W04-0813	P95-1026	o	The Decision List -LRB- DL -RRB- algorithm is described in -LRB- Yarowsky 1995b -RRB-	appos_Yarowsky_1995b prep_in_described_Yarowsky auxpass_described_is nsubjpass_described_algorithm nn_algorithm_List appos_List_DL nn_List_Decision det_List_The
W04-0813	P95-1026	p	4.1 Methods and Parameters DL On Senseval-2 data we observed that DL improved significantly its performance with a smoothing technique based on -LRB- Yarowsky 1995a -RRB-	appos_Yarowsky_1995a prep_on_based_Yarowsky vmod_technique_based nn_technique_smoothing det_technique_a prep_with_performance_technique poss_performance_its dobj_improved_performance advmod_improved_significantly nsubj_improved_DL mark_improved_that ccomp_observed_improved nsubj_observed_we prep_on_observed_data nn_data_Senseval-2 nn_DL_Parameters dep_Methods_observed conj_and_Methods_DL num_Methods_4.1 dep_``_DL dep_``_Methods
W04-0846	P95-1026	o	These include the bootstrapping approach -LSB- Yarowsky 1995 -RSB- and the context clustering approach -LSB- Schutze 1998 -RSB-	num_Schutze_1998 dep_approach_Schutze nn_approach_clustering nn_approach_context det_approach_the conj_and_Yarowsky_approach num_Yarowsky_1995 dep_approach_approach dep_approach_Yarowsky amod_approach_bootstrapping det_approach_the dobj_include_approach nsubj_include_These
W04-2312	P95-1026	o	Yarowsky -LRB- 1995 -RRB- used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis	nn_synthesis_speech prep_of_phonetizitation_words amod_phonetizitation_correct prep_in_supervised_synthesis prep_for_supervised_phonetizitation dep_supervised_WSD conj_and_supervised_unsupervised preconj_supervised_both dobj_used_unsupervised dobj_used_supervised nsubj_used_Yarowsky appos_Yarowsky_1995
W04-2312	P95-1026	o	2.1 Data-based Methods Data-based approaches extract their information directly from texts and are divided into supervised and unsupervised methods -LRB- Yarowsky 1995 Stevenson 2003 -RRB-	amod_Stevenson_2003 dep_Yarowsky_Stevenson dep_Yarowsky_1995 appos_methods_Yarowsky amod_methods_unsupervised amod_methods_supervised conj_and_supervised_unsupervised prep_into_divided_methods auxpass_divided_are poss_information_their prep_from_extract_texts advmod_extract_directly dobj_extract_information nsubj_extract_approaches amod_approaches_Data-based conj_and_Methods_divided rcmod_Methods_extract amod_Methods_Data-based num_Methods_2.1 dep_``_divided dep_``_Methods
W04-2402	P95-1026	p	We also note that there are a number of bootstrapping methods successfully applied to text e.g. word sense disambiguation -LRB- Yarowsky 1995 -RRB- named entity instance classification -LRB- Collins and Singer 1999 -RRB- and the extraction of parts word given the whole word -LRB- Berland and Charniak 1999 -RRB-	amod_Berland_1999 conj_and_Berland_Charniak dep_word_Charniak dep_word_Berland amod_word_whole det_word_the pobj_given_word prep_word_given nn_word_parts prep_of_extraction_word det_extraction_the amod_Collins_1999 conj_and_Collins_Singer conj_and_classification_extraction dep_classification_Singer dep_classification_Collins nn_classification_instance nn_classification_entity amod_classification_named dep_classification_disambiguation advmod_classification_e.g. amod_Yarowsky_1995 dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word dobj_applied_extraction dobj_applied_classification prep_to_applied_text advmod_applied_successfully nsubjpass_applied_number amod_methods_bootstrapping prep_of_number_methods det_number_a ccomp_are_applied expl_are_there mark_are_that ccomp_note_are advmod_note_also nsubj_note_We ccomp_``_note
W04-2808	P95-1026	o	Yarowsky -LRB- 1995 -RRB- used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis	nn_synthesis_speech prep_of_phonetizitation_words amod_phonetizitation_correct prep_in_supervised_synthesis prep_for_supervised_phonetizitation dep_supervised_WSD conj_and_supervised_unsupervised preconj_supervised_both dobj_used_unsupervised dobj_used_supervised nsubj_used_Yarowsky appos_Yarowsky_1995
W04-2808	P95-1026	o	Data-based Methods Data-based approaches extract their information directly from texts and are divided into supervised and unsupervised methods -LRB- Yarowsky 1995 Stevenson 2003 -RRB-	amod_Stevenson_2003 dep_Yarowsky_Stevenson dep_Yarowsky_1995 appos_methods_Yarowsky amod_methods_unsupervised amod_methods_supervised conj_and_supervised_unsupervised prep_into_divided_methods auxpass_divided_are poss_information_their prep_from_extract_texts advmod_extract_directly dobj_extract_information nsubj_extract_approaches amod_approaches_Data-based conj_and_Methods_divided rcmod_Methods_extract amod_Methods_Data-based
W04-2808	P95-1026	o	Many of these tasks have been addressed in other fields for example hypothesis verification in the field of machine translation -LRB- Tran et al. 1996 -RRB- sense disambiguation in speech synthesis -LRB- Yarowsky 1995 -RRB- and relation tagging in information retrieval -LRB- Marsh and Perzanowski 1999 -RRB-	amod_Marsh_1999 conj_and_Marsh_Perzanowski dep_retrieval_Perzanowski dep_retrieval_Marsh nn_retrieval_information nn_tagging_relation dep_Yarowsky_1995 nn_synthesis_speech appos_disambiguation_Yarowsky prep_in_disambiguation_synthesis nn_disambiguation_sense amod_Tran_1996 dep_Tran_al. nn_Tran_et nn_translation_machine prep_of_field_translation det_field_the prep_in_verification_retrieval conj_and_verification_tagging conj_and_verification_disambiguation dep_verification_Tran prep_in_verification_field nn_verification_hypothesis amod_fields_other parataxis_addressed_tagging parataxis_addressed_disambiguation parataxis_addressed_verification prep_for_addressed_example prep_in_addressed_fields auxpass_addressed_been aux_addressed_have nsubjpass_addressed_Many det_tasks_these prep_of_Many_tasks
W05-0605	P95-1026	o	These include the bootstrapping approach -LRB- Yarowsky 1995 -RRB- and the context clustering approach -LRB- Schtze 1998 -RRB-	num_Schtze_1998 dep_approach_Schtze nn_approach_clustering nn_approach_context det_approach_the num_Yarowsky_1995 conj_and_approach_approach appos_approach_Yarowsky amod_approach_bootstrapping det_approach_the dobj_include_approach dobj_include_approach nsubj_include_These
W05-0605	P95-1026	o	For example -LRB- Yarowsky 1995 -RRB- only requires sense number and a few seeds for each sense of an ambiguous word -LRB- hereafter called keyword -RRB-	dep_called_keyword advmod_called_hereafter dep_word_called amod_word_ambiguous det_word_an prep_of_sense_word det_sense_each amod_seeds_few det_seeds_a conj_and_number_seeds nn_number_sense prep_for_requires_sense dobj_requires_seeds dobj_requires_number advmod_requires_only nsubj_requires_Yarowsky prep_for_requires_example num_Yarowsky_1995
W05-1006	P95-1026	o	Even for semantically predictable phrases the fact that the words occur in fixed patterns can be very useful for the purposes of disambiguation as demonstrated by -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_by_Yarowsky prep_demonstrated_by mark_demonstrated_as prep_of_purposes_disambiguation det_purposes_the advcl_useful_demonstrated prep_for_useful_purposes advmod_useful_very cop_useful_be aux_useful_can nsubj_useful_fact prep_for_useful_phrases advmod_useful_Even amod_patterns_fixed prep_in_occur_patterns nsubj_occur_words mark_occur_that det_words_the ccomp_fact_occur det_fact_the amod_phrases_predictable advmod_predictable_semantically
W06-0208	P95-1026	p	7 Related Work Unannotated texts have been used successfully for a variety of NLP tasks including named entity recognition -LRB- Collins and Singer 1999 -RRB- subjectivity classification -LRB- Wiebe and Riloff 2005 -RRB- text classification -LRB- Nigam et al. 2000 -RRB- and word sense disambiguation -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word appos_Nigam_2000 dep_Nigam_al. nn_Nigam_et dep_classification_Nigam nn_classification_text dep_Wiebe_2005 conj_and_Wiebe_Riloff conj_and_classification_disambiguation conj_and_classification_classification dep_classification_Riloff dep_classification_Wiebe nn_classification_subjectivity amod_Collins_1999 conj_and_Collins_Singer dep_recognition_Singer dep_recognition_Collins nn_recognition_entity dep_named_disambiguation dep_named_classification dep_named_classification dep_named_recognition prepc_including_tasks_named nn_tasks_NLP prep_of_variety_tasks det_variety_a prep_for_used_variety advmod_used_successfully auxpass_used_been aux_used_have nsubjpass_used_texts amod_texts_Unannotated dep_Work_used amod_Work_Related num_Work_7
W06-0208	P95-1026	o	Many recent approaches in natural language processing -LRB- Yarowsky 1995 Collins and Singer 1999 Riloff and Jones 1999 Nigam et al. 2000 Wiebe and Riloff 2005 -RRB- have recognized the need to use unannotated data to improve performance	dobj_improve_performance aux_improve_to amod_data_unannotated vmod_use_improve dobj_use_data aux_use_to vmod_need_use det_need_the dobj_recognized_need aux_recognized_have nsubj_recognized_approaches dep_Wiebe_2005 conj_and_Wiebe_Riloff num_Nigam_2000 nn_Nigam_al. nn_Nigam_et dep_Riloff_Riloff dep_Riloff_Wiebe conj_and_Riloff_Nigam conj_and_Riloff_1999 conj_and_Riloff_Jones num_Collins_1999 conj_and_Collins_Singer dep_Yarowsky_Nigam dep_Yarowsky_1999 dep_Yarowsky_Jones dep_Yarowsky_Riloff conj_Yarowsky_Singer conj_Yarowsky_Collins appos_Yarowsky_1995 appos_processing_Yarowsky nn_processing_language amod_processing_natural prep_in_approaches_processing amod_approaches_recent amod_approaches_Many
W06-0505	P95-1026	o	This task is closely related to both named entity recognition -LRB- NER -RRB- which traditionally assigns nouns to a small number of categories and word sense disambiguation -LRB- Agirre and 1http / / class.inrialpes.fr / Rigau 1996 Yarowsky 1995 -RRB- where the sense for a word is chosen from a much larger inventory of word senses	nn_senses_word prep_of_inventory_senses amod_inventory_larger amod_inventory_much det_inventory_a prep_from_chosen_inventory auxpass_chosen_is nsubjpass_chosen_sense advmod_chosen_where det_word_a prep_for_sense_word det_sense_the amod_Yarowsky_1995 dep_Rigau_Yarowsky amod_Rigau_1996 amod_Rigau_class.inrialpes.fr dep_Agirre_Rigau conj_and_Agirre_1http nn_disambiguation_sense nn_disambiguation_word rcmod_categories_chosen dep_categories_1http dep_categories_Agirre conj_and_categories_disambiguation prep_of_number_disambiguation prep_of_number_categories amod_number_small det_number_a prep_to_assigns_number dobj_assigns_nouns advmod_assigns_traditionally nsubj_assigns_which appos_recognition_NER nn_recognition_entity dep_named_recognition rcmod_both_assigns vmod_both_named prep_to_related_both advmod_related_closely cop_related_is nsubj_related_task det_task_This
W06-1649	P95-1026	o	The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora -LRB- e.g. parallel corpora or untagged monolingual corpora in two languages -RRB- -LRB- Brown et al. 1991 Dagan and Itai 1994 -RRB- or sense-tagged seed examples -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_examples_Yarowsky nn_examples_seed amod_examples_sense-tagged dep_Dagan_1994 conj_and_Dagan_Itai dep_Brown_Itai dep_Brown_Dagan amod_Brown_1991 dep_Brown_al. nn_Brown_et num_languages_two amod_corpora_monolingual amod_corpora_untagged amod_corpora_corpora conj_or_corpora_untagged prep_in_parallel_languages dobj_parallel_corpora dep_parallel_e.g. conj_or_corpora_examples appos_corpora_Brown dep_corpora_parallel amod_corpora_bilingual prep_from_obtained_examples prep_from_obtained_corpora advmod_obtained_usually auxpass_obtained_is nsubjpass_obtained_information nn_disambiguation_sense amod_disambiguation_semi-supervised prep_for_information_disambiguation det_information_The
W06-1649	P95-1026	o	Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a tagged corpus with the instances of each possible sense e.g. supervised sense disambiguation -LRB- Leacock et al. 1998 -RRB- and semi-supervised sense disambiguation -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_disambiguation_Yarowsky nn_disambiguation_sense amod_disambiguation_semi-supervised amod_Leacock_1998 dep_Leacock_al. nn_Leacock_et dep_disambiguation_Leacock nn_disambiguation_sense amod_disambiguation_supervised amod_sense_possible det_sense_each prep_of_instances_sense det_instances_the prep_with_corpus_instances amod_corpus_tagged det_corpus_a conj_or_word_corpus nn_word_target det_word_a prep_of_sense_corpus prep_of_sense_word amod_sense_possible det_sense_each amod_nition_de dep_given_e.g. prep_for_given_sense dobj_given_nition advmod_given_when nn_problem_disambiguation nn_problem_sense det_problem_the advcl_deal_given prep_with_deal_problem aux_deal_to conj_and_proposed_disambiguation conj_and_proposed_disambiguation xcomp_proposed_deal auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_based nn_methods_corpus amod_corpus_Many
W06-1665	P95-1026	o	The principle of our approach is more similar to -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_to_Yarowsky prep_similar_to advmod_similar_more cop_similar_is nsubj_similar_principle poss_approach_our prep_of_principle_approach det_principle_The
W06-1665	P95-1026	o	The proposed approach follows the same principle as -LRB- Yarowsky 1995 -RRB- which tried to determine the appropriate word sense according to one relevant context word	nn_word_context amod_word_relevant num_word_one nn_sense_word amod_sense_appropriate det_sense_the pobj_determine_word prepc_according_to_determine_to dobj_determine_sense aux_determine_to xcomp_tried_determine nsubj_tried_which rcmod_Yarowsky_tried dep_Yarowsky_1995 amod_principle_same det_principle_the prep_as_follows_Yarowsky dobj_follows_principle nsubj_follows_approach amod_approach_proposed det_approach_The
W06-2204	P95-1026	o	Several approaches for learning from both labeled and unlabeled data have been proposed -LRB- Yarowsky 1995 Blum and Mitchell 1998 Collins and Singer 1999 -RRB- where the unlabeled data is utilised to boost the performance of the algorithm	det_algorithm_the prep_of_performance_algorithm det_performance_the dobj_boost_performance aux_boost_to xcomp_utilised_boost auxpass_utilised_is nsubjpass_utilised_data advmod_utilised_where amod_data_unlabeled det_data_the amod_Collins_1999 conj_and_Collins_Singer advcl_Blum_utilised dep_Blum_Singer dep_Blum_Collins conj_and_Blum_1998 conj_and_Blum_Mitchell dep_Yarowsky_1998 dep_Yarowsky_Mitchell dep_Yarowsky_Blum conj_Yarowsky_1995 dep_proposed_Yarowsky auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches amod_data_unlabeled conj_and_labeled_data preconj_labeled_both prep_from_learning_data prep_from_learning_labeled prepc_for_approaches_learning amod_approaches_Several
W06-2207	P95-1026	n	Although a rich literature covers bootstrapping methods applied to natural language problems -LRB- Yarowsky 1995 Riloff 1996 Collins and Singer 1999 Yangarber et al. 2000 Yangarber 2003 Abney 2004 -RRB- several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition	nn_acquisition_pattern amod_acquisition_semantic amod_acquisition_syntactic conj_or_syntactic_semantic prep_to_applied_acquisition auxpass_applied_are nsubjpass_applied_methods advmod_applied_when det_methods_these advcl_remain_applied acomp_remain_unanswered nsubj_remain_questions amod_questions_several dep_Abney_2004 num_Yangarber_2003 num_Yangarber_2000 nn_Yangarber_al. nn_Yangarber_et dep_Riloff_remain dep_Riloff_Abney conj_and_Riloff_Yangarber conj_and_Riloff_Yangarber num_Riloff_1999 conj_and_Riloff_Singer conj_and_Riloff_Collins conj_and_Riloff_1996 dep_Yarowsky_Yangarber dep_Yarowsky_Yangarber dep_Yarowsky_Singer dep_Yarowsky_Collins dep_Yarowsky_1996 dep_Yarowsky_Riloff appos_Yarowsky_1995 dep_problems_Yarowsky nn_problems_language amod_problems_natural prep_to_applied_problems nsubj_applied_methods nn_methods_bootstrapping ccomp_covers_applied nsubj_covers_literature mark_covers_Although amod_literature_rich det_literature_a advcl_``_covers
W06-2207	P95-1026	o	Several approaches have been proposed in the context of word sense disambiguation -LRB- Yarowsky 1995 -RRB- named entity -LRB- NE -RRB- classification -LRB- Collins and Singer 1999 -RRB- patternacquisitionforIE -LRB- Riloff ,1996 Yangarber 2003 -RRB- or dimensionality reduction for text categorization -LRB- TC -RRB- -LRB- Yang and Pedersen 1997 -RRB-	amod_Yang_1997 conj_and_Yang_Pedersen dep_categorization_Pedersen dep_categorization_Yang appos_categorization_TC nn_categorization_text prep_for_reduction_categorization nn_reduction_dimensionality dep_Yangarber_2003 dep_Riloff_Yangarber num_Riloff_,1996 appos_patternacquisitionforIE_Riloff amod_Collins_1999 conj_and_Collins_Singer conj_or_classification_reduction conj_or_classification_patternacquisitionforIE dep_classification_Singer dep_classification_Collins nn_classification_NE nn_classification_entity amod_classification_named amod_Yarowsky_1995 dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word prep_of_context_disambiguation det_context_the dep_proposed_reduction dep_proposed_patternacquisitionforIE dep_proposed_classification prep_in_proposed_context auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches amod_approaches_Several
W06-2207	P95-1026	o	Similarlyto -LRB- Collins and Singer 1999 Yarowsky 1995 -RRB- we define the strength of a pattern p in a category y as the precision of p in the set of documents labeled with category y estimated using Laplace smoothing strength -LRB- p y -RRB- = count -LRB- p y -RRB- + epsilon1count -LRB- p -RRB- + kepsilon1 -LRB- 3 -RRB- where count -LRB- p y -RRB- is the number of documents labeled y containing pattern p count -LRB- p -RRB- is the overall number of labeled documents containing p and k is the number of domains	prep_of_number_domains det_number_the cop_number_is nsubj_number_number conj_and_p_k dobj_containing_k dobj_containing_p vmod_documents_containing dobj_labeled_documents prepc_of_number_labeled amod_number_overall det_number_the cop_number_is nsubj_number_strength appos_count_p appos_p_count nn_p_pattern dobj_containing_p xcomp_y_containing xcomp_labeled_y vmod_documents_labeled prep_of_number_documents det_number_the cop_number_is nsubj_number_count advmod_number_where appos_p_y dep_count_p appos_kepsilon1_3 appos_epsilon1count_p appos_p_y rcmod_count_number conj_+_count_kepsilon1 conj_+_count_epsilon1count dep_count_p dobj_=_kepsilon1 dobj_=_epsilon1count dobj_=_count appos_p_y dep_strength_= dep_strength_p nn_smoothing_Laplace dobj_using_smoothing xcomp_estimated_using nn_y_category prep_with_labeled_y vmod_set_labeled prep_of_set_documents det_set_the prep_of_precision_p det_precision_the nn_y_category det_y_a prep_in_p_set prep_as_p_precision prep_in_p_y dep_pattern_p det_pattern_a prep_of_strength_pattern det_strength_the parataxis_define_number dep_define_estimated dobj_define_strength nsubj_define_we nsubj_define_Similarlyto dep_Yarowsky_1995 dep_Collins_Yarowsky amod_Collins_1999 conj_and_Collins_Singer appos_Similarlyto_Singer appos_Similarlyto_Collins
W07-2051	P95-1026	o	3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 agent_proposed_Yarowsky vmod_heuristic_proposed amod_heuristic_one-sense-per-collocation det_heuristic_the agent_inspired_heuristic auxpass_inspired_were nsubjpass_inspired_features nn_features_collocation det_features_The rcmod_Features_inspired nn_Features_Collocation num_Features_3.1
W09-1116	P95-1026	p	The notion that nouns have only one sense per discourse/collocation was also exploited by Yarowsky -LRB- 1995 -RRB- in his seminal work on bootstrapping for word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word prep_for_bootstrapping_disambiguation amod_work_seminal poss_work_his appos_Yarowsky_1995 prepc_on_exploited_bootstrapping prep_in_exploited_work agent_exploited_Yarowsky advmod_exploited_also auxpass_exploited_was nsubjpass_exploited_notion prep_per_sense_discourse/collocation num_sense_one quantmod_one_only dobj_have_sense nsubj_have_nouns mark_have_that ccomp_notion_have det_notion_The
W09-1705	P95-1026	o	The intuition is that the produced clusters will be less sense-conflating than those produced by other graph-based approaches since collocations provide strong and consistent clues to the senses of a target word -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_word_Yarowsky nn_word_target det_word_a prep_of_senses_word det_senses_the prep_to_clues_senses amod_clues_consistent amod_clues_strong conj_and_strong_consistent dobj_provide_clues nsubj_provide_collocations mark_provide_since amod_approaches_graph-based amod_approaches_other agent_produced_approaches vmod_those_produced advcl_sense-conflating_provide prep_than_sense-conflating_those advmod_sense-conflating_less cop_sense-conflating_be aux_sense-conflating_will nsubj_sense-conflating_clusters mark_sense-conflating_that amod_clusters_produced det_clusters_the ccomp_is_sense-conflating nsubj_is_intuition det_intuition_The ccomp_``_is
W09-1705	P95-1026	o	We observe that the tagging method exploits the one sense per collocation property -LRB- Yarowsky 1995 -RRB- which means that WSD based on collocations is probably finer than WSD based on simple words since ambiguity is reduced -LRB- Klapaftis and Manandhar 2008 -RRB-	amod_Klapaftis_2008 conj_and_Klapaftis_Manandhar dep_reduced_Manandhar dep_reduced_Klapaftis auxpass_reduced_is nsubjpass_reduced_ambiguity mark_reduced_since amod_words_simple prep_on_based_words vmod_WSD_based advcl_finer_reduced prep_than_finer_WSD advmod_finer_probably cop_finer_is nsubj_finer_WSD mark_finer_that prep_on_based_collocations vmod_WSD_based ccomp_means_finer nsubj_means_which dep_Yarowsky_1995 nn_property_collocation rcmod_sense_means appos_sense_Yarowsky prep_per_sense_property num_sense_one det_sense_the dobj_exploits_sense nsubj_exploits_method mark_exploits_that nn_method_tagging det_method_the ccomp_observe_exploits nsubj_observe_We ccomp_``_observe
W09-2207	P95-1026	o	1http / / www.nist.gov/speech/tests/ace/ 49 Bootstrapping techniques have been used for such diverse NLP problems as word sense disambiguation -LRB- Yarowsky 1995 -RRB- named entity classification -LRB- Collins and Singer 1999 -RRB- IE pattern acquisition -LRB- Riloff 1996 Yangarber et al. 2000 Yangarber 2003 Stevenson and Greenwood 2005 -RRB- document classification -LRB- Surdeanu et al. 2006 -RRB- fact extraction from the web -LRB- Pasca et al. 2006 -RRB- and hyponymy relation extraction -LRB- Kozareva et al. 2008 -RRB-	amod_Kozareva_2008 dep_Kozareva_al. nn_Kozareva_et nn_extraction_relation nn_extraction_hyponymy amod_Pasca_2006 dep_Pasca_al. nn_Pasca_et det_web_the dep_extraction_Pasca prep_from_extraction_web nn_extraction_fact amod_Surdeanu_2006 dep_Surdeanu_al. nn_Surdeanu_et dep_classification_Surdeanu nn_classification_document dep_Yangarber_2005 conj_and_Yangarber_Greenwood conj_and_Yangarber_Stevenson conj_and_Yangarber_2003 dep_Yangarber_Kozareva conj_and_Yangarber_extraction conj_and_Yangarber_extraction dep_Yangarber_classification dep_Yangarber_Greenwood dep_Yangarber_Stevenson dep_Yangarber_2003 dep_Yangarber_Yangarber num_Yangarber_2000 nn_Yangarber_al. nn_Yangarber_et dep_Riloff_extraction dep_Riloff_extraction dep_Riloff_Yangarber appos_Riloff_1996 dep_acquisition_Riloff nn_acquisition_pattern amod_acquisition_IE amod_Collins_1999 conj_and_Collins_Singer dep_classification_Singer dep_classification_Collins nn_classification_entity amod_classification_named amod_Yarowsky_1995 conj_disambiguation_acquisition conj_disambiguation_classification dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word dep_as_disambiguation prep_problems_as nn_problems_NLP amod_problems_diverse amod_problems_such prep_for_used_problems auxpass_used_been aux_used_have nsubjpass_used_techniques nn_techniques_Bootstrapping num_techniques_49 nn_techniques_www.nist.gov/speech/tests/ace/ parataxis_1http_used
W09-2207	P95-1026	o	-LRB- Yarowsky 1995 -RRB- used bootstrapping to train decision list classifiers to disambiguate between two senses of a word achieving impressive classification accuracy	nn_accuracy_classification amod_accuracy_impressive dobj_achieving_accuracy det_word_a prep_of_senses_word num_senses_two prep_between_disambiguate_senses aux_disambiguate_to nn_classifiers_list nn_classifiers_decision vmod_train_disambiguate dobj_train_classifiers aux_train_to xcomp_bootstrapping_train xcomp_used_achieving xcomp_used_bootstrapping dep_used_Yarowsky dep_Yarowsky_1995
W09-2208	P95-1026	o	Algorithms such as co-training -LRB- Blum and Mitchell 1998 -RRB- -LRB- Collins and Singer 1999 -RRB- -LRB- Pierce and Cardie 2001 -RRB- and the Yarowsky algorithm -LRB- Yarowsky 1995 -RRB- make assumptions about the data that permit such an approach	det_approach_an amod_approach_such dobj_permit_approach nsubj_permit_that rcmod_data_permit det_data_the prep_about_assumptions_data dobj_make_assumptions nsubj_make_Algorithms amod_Yarowsky_1995 dep_algorithm_Yarowsky nn_algorithm_Yarowsky det_algorithm_the dep_Pierce_2001 conj_and_Pierce_Cardie amod_Collins_1999 conj_and_Collins_Singer amod_Blum_1998 conj_and_Blum_Mitchell conj_and_co-training_algorithm dep_co-training_Cardie dep_co-training_Pierce dep_co-training_Singer dep_co-training_Collins dep_co-training_Mitchell dep_co-training_Blum prep_such_as_Algorithms_algorithm prep_such_as_Algorithms_co-training
W09-2208	P95-1026	o	2 Related Work The Yarowsky algorithm -LRB- Yarowsky 1995 -RRB- originally proposed for word sense disambiguation makes the assumption that it is very unlikely for two occurrences of a word in the same discourse to have different senses	amod_senses_different dobj_have_senses aux_have_to vmod_discourse_have amod_discourse_same det_discourse_the prep_in_word_discourse det_word_a prep_of_occurrences_word num_occurrences_two prep_for_unlikely_occurrences advmod_unlikely_very cop_unlikely_is nsubj_unlikely_it mark_unlikely_that ccomp_assumption_unlikely det_assumption_the dobj_makes_assumption nsubj_makes_algorithm nn_disambiguation_sense nn_disambiguation_word prep_for_proposed_disambiguation advmod_proposed_originally amod_Yarowsky_1995 vmod_algorithm_proposed dep_algorithm_Yarowsky nn_algorithm_Yarowsky det_algorithm_The rcmod_Work_makes amod_Work_Related num_Work_2
W09-2208	P95-1026	o	Collins et al. -LRB- Collins and Singer 1999 -RRB- proposed two algorithms for NER by modifying Yarowskys method -LRB- Yarowsky 1995 -RRB- and the framework suggested by -LRB- Blum and Mitchell 1998 -RRB-	amod_Blum_1998 conj_and_Blum_Mitchell dep_by_Mitchell dep_by_Blum prep_suggested_by vmod_framework_suggested det_framework_the amod_Yarowsky_1995 conj_and_method_framework dep_method_Yarowsky nn_method_Yarowskys dobj_modifying_framework dobj_modifying_method prep_for_algorithms_NER num_algorithms_two prepc_by_proposed_modifying dobj_proposed_algorithms nsubj_proposed_al. amod_Collins_1999 conj_and_Collins_Singer dep_al._Singer dep_al._Collins nn_al._et nn_al._Collins
W09-2404	P95-1026	p	Yarowsky -LRB- 1995 -RRB- successfully used this observation as an approximate annotation technique in an unsupervised WSD model	nn_model_WSD amod_model_unsupervised det_model_an prep_in_technique_model nn_technique_annotation amod_technique_approximate det_technique_an det_observation_this prep_as_used_technique dobj_used_observation advmod_used_successfully vmod_Yarowsky_used appos_Yarowsky_1995
W09-2404	P95-1026	p	-LRB- 1992b -RRB- has proved to be a simple yet powerful observation and has been successfully used in word sense disambiguation -LRB- WSD -RRB- and related tasks -LRB- e.g. Yarowsky -LRB- 1995 -RRB- Agirre and Rigau The author was partially funded by GALE DARPA Contract No	nn_No_Contract nn_No_DARPA nn_No_GALE agent_funded_No advmod_funded_partially auxpass_funded_was nsubjpass_funded_Rigau nsubjpass_funded_Agirre det_author_The dep_Agirre_author conj_and_Agirre_Rigau dep_Yarowsky_funded appos_Yarowsky_1995 dep_e.g._Yarowsky ccomp_-LRB-_e.g. amod_tasks_related conj_and_disambiguation_tasks appos_disambiguation_WSD nn_disambiguation_sense nn_disambiguation_word prep_in_used_tasks prep_in_used_disambiguation advmod_used_successfully auxpass_used_been aux_used_has nsubjpass_used_1992b amod_observation_powerful amod_observation_simple det_observation_a cop_observation_be aux_observation_to advmod_powerful_yet conj_and_proved_used xcomp_proved_observation aux_proved_has nsubj_proved_1992b
W96-0104	P95-1026	o	The original training set -LRB- before the addition of the feedback sets -RRB- consisted of a few dozen examples in comparison to thousands of examples needed in other corpus-based methods -LRB- Schutze 1992 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Schutze_Yarowsky appos_Schutze_1992 appos_methods_Schutze amod_methods_corpus-based amod_methods_other prep_in_needed_methods vmod_examples_needed prep_of_thousands_examples prep_to_comparison_thousands nn_examples_dozen amod_examples_few det_examples_a prep_in_consisted_comparison prep_of_consisted_examples nsubj_consisted_set nn_sets_feedback det_sets_the prep_of_addition_sets det_addition_the prep_before_set_addition nn_set_training amod_set_original det_set_The
W96-0104	P95-1026	o	In comparison -LRB- Yarowsky 1995 -RRB- achieved 48 Table 1 A summary of the experimental results on four polysemous words	amod_words_polysemous num_words_four prep_on_results_words amod_results_experimental det_results_the prep_of_summary_results det_summary_A dep_Table_summary num_Table_1 num_Table_48 dobj_achieved_Table nsubj_achieved_Yarowsky prep_in_achieved_comparison amod_Yarowsky_1995
W96-0104	P95-1026	o	Recently Yarowsky -LRB- 1995 -RRB- combined a MIlD and a corpus in a bootstrapping process	nn_process_bootstrapping det_process_a det_corpus_a prep_in_MIlD_process conj_and_MIlD_corpus amod_a_corpus amod_a_MIlD dobj_combined_a vmod_Yarowsky_combined appos_Yarowsky_1995 dep_Recently_Yarowsky advmod_``_Recently
W97-0108	P95-1026	o	Attempts to alleviate this tagbottleneck i ~ lude tmotstr ~ ias -LRB- Te ~ ot ill 1996 Hearst 1991 -RRB- and unsupervised algorith ~ -LRB- Yarowsky 199s -RRB- Dictionary-based approaches rely on linguistic knowledge sources such as ma ~ l ~ i ~ e-readable dictionaries -LRB- Luk 1995 Veronis and Ide 1990 -RRB- and WordNet -LRB- Agirre and Rigau 1996 Resnik 1995 -RRB- and e0 -LRB- ploit these for word sense disaznbiguation	nn_disaznbiguation_sense nn_disaznbiguation_word prep_for_ploit_disaznbiguation dep_ploit_these dep_e0_ploit dep_Resnik_1995 dep_Agirre_Resnik dep_Agirre_1996 conj_and_Agirre_Rigau dep_WordNet_Rigau dep_WordNet_Agirre dep_Veronis_1990 conj_and_Veronis_Ide dep_Luk_Ide dep_Luk_Veronis appos_Luk_1995 dep_dictionaries_Luk amod_dictionaries_e-readable nn_dictionaries_~ conj_and_i_e0 conj_and_i_WordNet conj_and_i_dictionaries nn_i_~ nn_i_l nn_i_~ nn_i_ma prep_such_as_sources_e0 prep_such_as_sources_WordNet prep_such_as_sources_dictionaries prep_such_as_sources_i nn_sources_knowledge amod_sources_linguistic prep_on_rely_sources nsubj_rely_approaches nsubj_rely_ias nsubj_rely_this amod_approaches_Dictionary-based nn_approaches_~ appos_Yarowsky_199s dep_~_Yarowsky nn_~_algorith amod_~_unsupervised dep_Hearst_1991 dep_ot_Hearst dep_ot_1996 advmod_ot_ill nn_ot_~ nn_ot_Te conj_and_ias_approaches dep_ias_ot nn_ias_~ nn_ias_tmotstr nn_ias_lude nn_ias_~ nn_ias_i nn_ias_tagbottleneck ccomp_alleviate_rely aux_alleviate_to vmod_Attempts_alleviate
W97-0108	P95-1026	p	Unsupervised algorit ~ m ~ such as -LRB- Yarowsky 1995 -RRB- have reported good accuracy that rivals that of supervised algorithms	amod_algorithms_supervised prep_of_that_algorithms dobj_rivals_that nsubj_rivals_that rcmod_accuracy_rivals amod_accuracy_good dobj_reported_accuracy aux_reported_have nsubj_reported_m amod_Yarowsky_1995 dep_as_Yarowsky mwe_as_such prep_~_as num_m_~ nn_m_~ nn_m_algorit amod_m_Unsupervised
W97-0201	P95-1026	o	Similarly if the task is to distinguish between binary coarse sense distinction then current WSD techniques can achieve very high accuracy -LRB- in excess of 96 % when tested on a dozen words in -LRB- Yarowsky 1995 -RRB- -RRB-	dep_Yarowsky_1995 prep_in_words_Yarowsky num_words_dozen quantmod_dozen_a prep_on_tested_words advmod_tested_when num_%_96 prep_of_excess_% advcl_in_tested pobj_in_excess amod_accuracy_high advmod_high_very dep_achieve_in dobj_achieve_accuracy aux_achieve_can nsubj_achieve_techniques nn_techniques_WSD amod_techniques_current advmod_techniques_then nn_distinction_sense amod_distinction_coarse rcmod_binary_achieve conj_binary_distinction prep_between_distinguish_binary aux_distinguish_to xcomp_is_distinguish nsubj_is_task mark_is_if det_task_the advcl_,_is dep_``_Similarly
W97-0201	P95-1026	o	Similarly -LRB- Yarowsky 1995 -RRB- tested his WSD algorithm on a dozen words	nn_words_dozen det_words_a nn_algorithm_WSD poss_algorithm_his prep_on_tested_words dobj_tested_algorithm dep_tested_Yarowsky advmod_tested_Similarly dep_Yarowsky_1995 ccomp_``_tested
W97-0208	P95-1026	p	The best examples of this approach has been the resent work of Yarowsky -LRB- Yarowsky 1992 -RRB- -LRB- Yarowsky 1993 -RRB- -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Yarowsky_1993 dep_Yarowsky_1992 appos_Yarowsky_Yarowsky appos_Yarowsky_Yarowsky appos_Yarowsky_Yarowsky prep_of_work_Yarowsky dobj_resent_work vmod_the_resent dep_been_the ccomp_has_been det_approach_this dep_examples_has prep_of_examples_approach amod_examples_best det_examples_The dep_``_examples
W97-0321	P95-1026	o	Recently some kinds of learning techniques have been applied to cumulatively acquire exemplars form large corpora -LRB- Yarowsky 1994 1995 -RRB-	amod_Yarowsky_1995 num_Yarowsky_1994 dep_corpora_Yarowsky amod_corpora_large dobj_form_corpora nsubj_form_exemplars ccomp_acquire_form advmod_acquire_cumulatively aux_acquire_to xcomp_applied_acquire auxpass_applied_been aux_applied_have nsubjpass_applied_kinds advmod_applied_Recently amod_techniques_learning prep_of_kinds_techniques det_kinds_some
W97-0321	P95-1026	o	Introduction Word sense disambiguation has long been one of the major concerns in natural language processing area -LRB- e.g. Bruce et al. 1994 Choueka et al. 1985 Gale et al. 1993 McRoy 1992 Yarowsky 1992 1994 1995 -RRB- whose aim is to identify the correct sense of a word in a particular context among all of its senses defined in a dictionary or a thesaurus	det_thesaurus_a conj_or_dictionary_thesaurus det_dictionary_a prep_in_defined_thesaurus prep_in_defined_dictionary poss_senses_its vmod_all_defined prep_of_all_senses amod_context_particular det_context_a prep_in_word_context det_word_a prep_of_sense_word amod_sense_correct det_sense_the prep_among_identify_all dobj_identify_sense aux_identify_to xcomp_is_identify nsubj_is_aim poss_aim_whose num_Yarowsky_1995 num_Yarowsky_1994 num_Yarowsky_1992 amod_McRoy_1992 nn_al._et nn_al._Gale nn_al._et nn_al._Choueka num_Bruce_1994 nn_Bruce_al. nn_Bruce_et dep_e.g._Yarowsky dep_e.g._McRoy dep_e.g._1993 dep_e.g._al. dep_e.g._1985 dep_e.g._al. dep_e.g._Bruce dep_area_e.g. nn_area_processing nn_area_language amod_area_natural rcmod_concerns_is prep_in_concerns_area amod_concerns_major det_concerns_the prep_of_one_concerns cop_one_been advmod_one_long aux_one_has nsubj_one_disambiguation nn_disambiguation_sense nn_disambiguation_Word nn_disambiguation_Introduction
W97-0322	P95-1026	o	In future work we will expand all of the above types of features and employ techniques to reduce dimensionality along the lines suggested in -LRB- Duda and Hart 1973 -RRB- and -LRB- Gale Church and Yarowsky 1995 -RRB-	amod_Yarowsky_1995 conj_and_Gale_Yarowsky appos_Gale_Church conj_and_Duda_Yarowsky conj_and_Duda_Gale num_Duda_1973 conj_and_Duda_Hart prep_in_suggested_Gale prep_in_suggested_Hart prep_in_suggested_Duda nsubj_suggested_lines mark_suggested_along det_lines_the advcl_reduce_suggested dobj_reduce_dimensionality aux_reduce_to xcomp_employ_reduce dobj_employ_techniques nsubj_employ_we prep_of_types_features amod_types_above det_types_the prep_of_all_types conj_and_expand_employ dobj_expand_all aux_expand_will nsubj_expand_we prep_in_expand_work amod_work_future
W97-0322	P95-1026	o	A more recent bootstrapping approach is described in -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_in_Yarowsky prep_described_in auxpass_described_is nsubjpass_described_approach nn_approach_bootstrapping amod_approach_recent det_approach_A advmod_recent_more ccomp_``_described
W97-0322	P95-1026	p	While -LRB- Yarowsky 1995 -RRB- does not discuss distinguishing more than 2 senses of a word there is no immediate reason to doubt that the one sense per collocation rule -LRB- Yarowsky 1993 -RRB- would still hold for a larger number of senses	prep_of_number_senses amod_number_larger det_number_a prep_for_hold_number advmod_hold_still aux_hold_would nsubj_hold_reason amod_Yarowsky_1993 dep_rule_Yarowsky nn_rule_sense mark_rule_that prep_per_sense_collocation num_sense_one det_sense_the ccomp_doubt_rule aux_doubt_to vmod_reason_doubt amod_reason_immediate neg_reason_no ccomp_is_hold expl_is_there advcl_is_discuss det_word_a prep_of_senses_word num_senses_2 quantmod_2_than mwe_than_more dobj_distinguishing_senses xcomp_discuss_distinguishing neg_discuss_not aux_discuss_does dep_discuss_Yarowsky mark_discuss_While amod_Yarowsky_1995 ccomp_``_is
W97-0322	P95-1026	p	-LRB- Yarowsky 1995 -RRB- compares his method to -LRB- Schiitze 1992 -RRB- and shows that for four words the former performs significantly better in distinguishing between two senses	num_senses_two prep_between_distinguishing_senses prepc_in_better_distinguishing advmod_better_significantly advmod_performs_better nsubj_performs_former det_former_the rcmod_words_performs num_words_four prep_for_that_words dep_shows_that nsubj_shows_Yarowsky amod_Schiitze_1992 dep_to_Schiitze poss_method_his conj_and_compares_shows prep_compares_to dobj_compares_method nsubj_compares_Yarowsky amod_Yarowsky_1995
W97-0322	P95-1026	o	7.3 EM algorithm The only other application of the EM algorithm to word-sense disambiguation is described in -LRB- Gale Church and Yarowsky 1995 -RRB-	amod_Yarowsky_1995 conj_and_Gale_Yarowsky conj_and_Gale_Church prep_in_described_Yarowsky prep_in_described_Church prep_in_described_Gale auxpass_described_is nsubjpass_described_application amod_disambiguation_word-sense nn_algorithm_EM det_algorithm_the prep_to_application_disambiguation prep_of_application_algorithm amod_application_other amod_application_only det_application_The rcmod_algorithm_described nn_algorithm_EM num_algorithm_7.3
W97-0808	P95-1026	o	The other approach selected was Yarowsky 's unsupervised algorithm -LRB- 1995 -RRB-	appos_algorithm_1995 amod_algorithm_unsupervised poss_algorithm_Yarowsky cop_algorithm_was nsubj_algorithm_approach vmod_approach_selected amod_approach_other det_approach_The
W97-0812	P95-1026	o	-LRB- 1992 -RRB- Yarowsky -LRB- 1995 -RRB- and Karol & Edelman -LRB- 1996 -RRB- where strong reliance on statistical techniques for the calculation of word and context similarity commands large source corpora	nn_corpora_source amod_corpora_large dobj_commands_corpora nsubj_commands_reliance advmod_commands_where nn_similarity_context nn_similarity_word conj_and_word_context prep_of_calculation_similarity det_calculation_the amod_techniques_statistical prep_for_reliance_calculation prep_on_reliance_techniques amod_reliance_strong rcmod_Karol_commands appos_Karol_1996 conj_and_Karol_Edelman conj_and_Yarowsky_Edelman conj_and_Yarowsky_Karol appos_Yarowsky_1995 dep_1992_Karol dep_1992_Yarowsky dep_''_1992
W97-1004	P95-1026	o	1 Introduction Word compositions have long been a concern in lexicography -LRB- Benson et al. 1986 Miller et al. 1995 -RRB- and now as a specific kind of lexical knowledge it has been shown that they have an important role in many areas in natural language processing e.g. parsing generation lexicon building word sense disambiguation and information retrieving etc. -LRB- e.g. Abney 1989 1990 Benson et al. 1986 Yarowsky 1995 Church and Hanks 1989 Church Gale Hans and Hindle 1989 -RRB-	num_Hindle_1989 conj_and_Church_Hindle conj_and_Church_Hans conj_and_Church_Gale num_Hanks_1989 conj_and_Church_Hanks dep_Yarowsky_Hindle dep_Yarowsky_Hans dep_Yarowsky_Gale dep_Yarowsky_Church conj_Yarowsky_Hanks conj_Yarowsky_Church num_Yarowsky_1995 nn_al._et nn_al._Benson num_Abney_1990 num_Abney_1989 dep_e.g._Yarowsky tmod_e.g._1986 dep_e.g._al. dep_e.g._Abney dep_etc._e.g. amod_retrieving_etc. nn_retrieving_information conj_and_disambiguation_retrieving nn_disambiguation_sense nn_disambiguation_word nn_building_lexicon dep_e.g._building dep_e.g._generation dep_e.g._parsing nn_processing_language amod_processing_natural prep_in_areas_processing amod_areas_many amod_role_important det_role_an prep_have_e.g. prep_in_have_areas dobj_have_role nsubj_have_they mark_have_that dep_shown_retrieving dep_shown_disambiguation ccomp_shown_have auxpass_shown_been aux_shown_has nsubjpass_shown_it prep_as_shown_kind advmod_shown_now amod_knowledge_lexical prep_of_kind_knowledge amod_kind_specific det_kind_a dep_al._1995 nn_al._et nn_al._Miller num_al._1986 dep_Benson_al. dep_Benson_al. nn_Benson_et conj_and_concern_shown dep_concern_Benson prep_in_concern_lexicography det_concern_a cop_concern_been advmod_concern_long aux_concern_have nsubj_concern_compositions nn_compositions_Word nn_compositions_Introduction num_compositions_1
W98-0701	P95-1026	o	6 Discourse Context -LRB- Yarowsky 1995 -RRB- pointed out that the sense of a target word is highly consistent within any given document -LRB- one sense per discourse -RRB-	prep_per_sense_discourse num_sense_one dep_document_sense amod_document_given det_document_any prep_within_consistent_document advmod_consistent_highly cop_consistent_is nsubj_consistent_sense mark_consistent_that nn_word_target det_word_a prep_of_sense_word det_sense_the ccomp_pointed_consistent prt_pointed_out nsubj_pointed_Context dep_Yarowsky_1995 appos_Context_Yarowsky nn_Context_Discourse num_Context_6 ccomp_``_pointed
W98-0701	P95-1026	p	-LRB- Yarowsky 1995 -RRB- whose training corpus for the noun drug was 9 times bigger than that of Karov and Edelman reports 91.4 % correct performance improved to impressive 93.9 % when using the one sense per discourse constraint	dep_constraint_sense det_constraint_the prep_per_sense_discourse num_sense_one dobj_using_constraint advmod_using_when num_%_93.9 amod_%_impressive prep_to_improved_% vmod_performance_improved amod_performance_correct dep_%_performance num_%_91.4 advcl_reports_using dobj_reports_% nsubj_reports_Yarowsky conj_and_Karov_Edelman prep_of_that_Edelman prep_of_that_Karov prep_than_bigger_that npadvmod_bigger_times cop_bigger_was nsubj_bigger_corpus num_times_9 nn_drug_noun det_drug_the prep_for_corpus_drug nn_corpus_training poss_corpus_whose rcmod_Yarowsky_bigger amod_Yarowsky_1995
W98-0703	P95-1026	o	WSD that use information gathered from raw corpora -LRB- unsupervised training methods -RRB- -LRB- Yarowsky 1995 -RRB- -LRB- Resnik 1997 -RRB-	num_Resnik_1997 num_Yarowsky_1995 nn_methods_training amod_methods_unsupervised appos_corpora_Resnik appos_corpora_Yarowsky appos_corpora_methods amod_corpora_raw prep_from_gathered_corpora vmod_information_gathered dobj_use_information nsubj_use_that rcmod_WSD_use
W99-0903	P95-1026	n	However our system is the unsupervised learning with small POS-tagged corpus and we do not restrict the word 's sense set within either binary senses -LRB- Yarowsky ,1995 Karov 1998 -RRB- or dictionary 's homograph level -LRB- Wilks 1997 -RRB-	amod_Wilks_1997 dep_level_Wilks nn_level_homograph poss_level_dictionary dep_Karov_1998 dep_Yarowsky_Karov num_Yarowsky_,1995 conj_or_senses_level appos_senses_Yarowsky amod_senses_binary preconj_senses_either prep_within_set_level prep_within_set_senses vmod_sense_set poss_sense_word det_word_the dobj_restrict_sense neg_restrict_not aux_restrict_do nsubj_restrict_we amod_corpus_POS-tagged amod_corpus_small conj_and_learning_restrict prep_with_learning_corpus amod_learning_unsupervised det_learning_the cop_learning_is nsubj_learning_system advmod_learning_However poss_system_our
W99-0903	P95-1026	o	Recently many works combined a MRD and a corpus for word sense disambiguation -LRB- Karov 1998 Luk 1995 Ng 1996 Yarowsky ,1995 -RRB-	num_Yarowsky_,1995 dep_Ng_Yarowsky num_Ng_1996 dep_Luk_Ng appos_Luk_1995 dep_Karov_Luk dep_Karov_1998 dep_disambiguation_Karov nn_disambiguation_sense nn_disambiguation_word det_corpus_a prep_for_MRD_disambiguation conj_and_MRD_corpus det_MRD_a dobj_combined_corpus dobj_combined_MRD vmod_works_combined amod_works_many advmod_works_Recently
W99-0903	P95-1026	o	In -LRB- Yarowsky ,1995 -RRB- the definition words were used as initial sense indicators automatically tagging the target word examples containing them	dobj_containing_them vmod_examples_containing nn_examples_word nn_examples_target det_examples_the dobj_tagging_examples advmod_tagging_automatically nn_indicators_sense amod_indicators_initial vmod_used_tagging prep_as_used_indicators auxpass_used_were nsubjpass_used_words prep_used_In nn_words_definition det_words_the num_Yarowsky_,1995 dep_In_Yarowsky
W99-0905	P95-1026	p	Among them the unsupervised algorithm using decisiontrees -LRB- Yarowsky 1995 -RRB- has achieved promising performance	amod_performance_promising dobj_achieved_performance aux_achieved_has nsubj_achieved_algorithm prep_among_achieved_them amod_Yarowsky_1995 dep_decisiontrees_Yarowsky dobj_using_decisiontrees vmod_algorithm_using amod_algorithm_unsupervised det_algorithm_the
W99-0908	P95-1026	o	The preliminary labeling by keyword matching used in this paper is similar to the seed collocations used by Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 agent_used_Yarowsky vmod_collocations_used nn_collocations_seed det_collocations_the prep_to_similar_collocations cop_similar_is nsubj_similar_labeling det_paper_this prep_in_used_paper vmod_matching_used nn_matching_keyword prep_by_labeling_matching amod_labeling_preliminary det_labeling_The
A00-2005	P97-1003	o	The parser induction algorithm used in all of the experiments in this paper was a distribution of Collins 's model 2 parser -LRB- Collins 1997 -RRB-	amod_Collins_1997 appos_parser_Collins num_parser_2 nn_parser_model poss_parser_Collins prep_of_distribution_parser det_distribution_a cop_distribution_was nsubj_distribution_algorithm det_paper_this prep_in_experiments_paper det_experiments_the prep_of_all_experiments prep_in_used_all vmod_algorithm_used nn_algorithm_induction nn_algorithm_parser det_algorithm_The
A00-2016	P97-1003	o	Training on about 40,000 sentences -LRB- Collins 1997 -RRB- achieves a crossing brackets rate of 1.07 a better value than our 1.63 value for regular parsing or the 1.13 value assuming perfect segmentation/tagging but even for similar text types comparisons across languages are of course problematic	acomp_are_problematic prep_of_are_course nsubj_are_comparisons prep_for_are_types advmod_are_even prep_across_comparisons_languages nn_types_text amod_types_similar amod_segmentation/tagging_perfect dobj_assuming_segmentation/tagging num_value_1.13 det_value_the conj_or_parsing_value amod_parsing_regular vmod_value_assuming prep_for_value_value prep_for_value_parsing num_value_1.63 poss_value_our prep_than_value_value amod_value_better det_value_a conj_but_rate_are appos_rate_value prep_of_rate_1.07 nn_rate_brackets amod_rate_crossing det_rate_a dobj_achieves_are dobj_achieves_rate csubj_achieves_Training amod_Collins_1997 num_sentences_40,000 quantmod_40,000_about dep_Training_Collins prep_on_Training_sentences
A00-2030	P97-1003	o	1 Introduction Since 1995 a few statistical parsing algorithms -LRB- Magerman 1995 Collins 1996 and 1997 Charniak 1997 Rathnaparki 1997 -RRB- demonstrated a breakthrough in parsing accuracy as measured against the University of Pennsylvania TREEBANK as a gold standard	amod_standard_gold det_standard_a prep_as_TREEBANK_standard dep_University_TREEBANK prep_of_University_Pennsylvania det_University_the prep_against_measured_University mark_measured_as amod_accuracy_parsing dep_breakthrough_measured prep_in_breakthrough_accuracy det_breakthrough_a dep_demonstrated_breakthrough nsubj_demonstrated_algorithms ccomp_demonstrated_Introduction dep_Rathnaparki_1997 num_Charniak_1997 conj_and_Collins_1997 conj_and_Collins_1996 dep_Magerman_Rathnaparki dep_Magerman_Charniak dep_Magerman_1997 dep_Magerman_1996 dep_Magerman_Collins dep_Magerman_1995 appos_algorithms_Magerman nn_algorithms_parsing amod_algorithms_statistical amod_algorithms_few det_algorithms_a prep_since_Introduction_1995 num_Introduction_1
A00-2030	P97-1003	o	Finally our newly constructed parser like that of -LRB- Collins 1997 -RRB- was based on a generative statistical model	amod_model_statistical amod_model_generative det_model_a prep_on_based_model auxpass_based_was nsubjpass_based_Collins advmod_based_of num_Collins_1997 rcmod_that_based prep_like_parser_that amod_parser_constructed poss_parser_our advmod_constructed_newly dep_Finally_parser dep_``_Finally
A00-2030	P97-1003	o	7 Model Structure In our statistical model trees are generated according to a process similar to that described in -LRB- Collins 1996 1997 -RRB-	amod_Collins_1997 num_Collins_1996 dep_in_Collins prep_described_in vmod_that_described prep_to_similar_that amod_process_similar det_process_a pobj_generated_process prepc_according_to_generated_to auxpass_generated_are nsubjpass_generated_trees advcl_generated_Structure amod_model_statistical poss_model_our prep_in_Structure_model nn_Structure_Model num_Structure_7 ccomp_``_generated
A00-2031	P97-1003	o	234 ADV Non-specific adverbial BNF Benefemtive CLF It-cleft CLR ` Closely related ' DIR Direction DTV Dative EXT Extent HLN Headline LGS Logical subject L0C Location MNI ~ Manner N0M Nominal PRD Predicate PRP Purpose PUT Locative complement of ` put ' SBJ Subject TMP Temporal TPC Topic TTL Title V0C Vocative Grammatical DTV 0.48 % LGS 3.0 % PRD 18 % PUT 0.26 % SBJ 78 % v0c 0.025 % Figure 1 Penn treebank function tags 53 % Form/Function 37 % Topicalisation 2.2 % 0.25 % NOM 6.8 % 2.5 % TPC 100 % 2.2 % 1.5 % ADV 11 % 4.2 % 9.3 % BN ' F 0.072 % 0.026 % 0.13 % DIR 8.3 % 3.0 % 41 % EXT 3.2 % 1.2 % 0.013 % LOC 25 % 9.2 % MNR 6.2 % 2.3 % PI ~ 5.2 % 1.9 % 33 % 12 % Miscellaneous 9.5 % CLR 94 % 8.8 % CLF 0.34 % 0.03 % HLN 2.6 % 0.25 % TTL 3.1 % 0.29 % Figure 2 Categories of function tags and their relative frequencies one project that used them at all -LRB- Collins 1997 -RRB- defines certain constituents as complements based on a combination of label and function tag information	nn_information_tag nn_information_function nn_information_label conj_and_label_function prep_of_combination_information det_combination_a prep_based_on_complements_combination amod_constituents_certain prepc_as_defines_complements dobj_defines_constituents nsubj_defines_project nsubj_defines_frequencies amod_Collins_1997 prep_at_used_all dobj_used_them nsubj_used_that dep_project_Collins rcmod_project_used num_project_one amod_frequencies_relative poss_frequencies_their nn_tags_function conj_and_Categories_defines prep_of_Categories_tags num_Figure_2 amod_Figure_% number_%_0.29 dep_%_Figure num_%_3.1 dep_TTL_% amod_TTL_% number_%_0.25 dep_%_TTL num_%_2.6 dep_HLN_% amod_HLN_% number_%_0.03 dep_%_HLN num_%_0.34 dep_CLF_% dep_%_defines dep_%_Categories dep_%_CLF number_%_8.8 dep_%_% num_CLR_94 amod_CLR_% amod_CLR_Miscellaneous number_%_9.5 dep_%_CLR dep_%_12 dep_%_33 num_%_1.9 dep_%_% num_%_5.2 dep_~_% nn_~_PI amod_~_% number_%_2.3 dep_%_~ num_%_6.2 dep_MNR_% dep_%_MNR number_%_9.2 dep_%_% dep_%_EXT dep_%_% num_LOC_25 dep_%_LOC number_%_0.013 amod_%_% num_%_1.2 dep_%_% num_%_3.2 dep_EXT_% dep_%_41 num_%_3.0 dep_%_% num_%_8.3 dep_DIR_% dep_%_DIR number_%_0.13 dep_%_% num_%_0.026 dep_%_% num_%_0.072 dep_F_% amod_BN_F amod_BN_% number_%_9.3 dep_%_BN num_%_4.2 dep_%_% dep_%_% dep_%_% dep_%_% dep_%_% dep_%_Topicalisation dep_%_% dep_%_Form/Function dep_%_% dep_%_Figure dep_%_% dep_%_% dep_%_CLR num_ADV_11 dep_%_ADV number_%_1.5 dep_%_% num_%_2.2 dep_%_% num_%_100 dep_TPC_% amod_TPC_% amod_TPC_% nn_TPC_NOM number_%_2.5 number_%_6.8 dep_%_TPC num_%_0.25 dep_%_% num_%_2.2 dep_Topicalisation_% num_Form/Function_37 dep_tags_53 nn_tags_function nn_tags_treebank nn_tags_Penn dep_Figure_tags num_Figure_1 amod_Figure_% nn_Figure_v0c number_%_0.025 num_SBJ_78 amod_SBJ_% nn_SBJ_PUT number_%_0.26 dep_%_SBJ num_PRD_18 amod_PRD_% nn_PRD_LGS amod_PRD_% nn_PRD_DTV nn_PRD_Grammatical nn_PRD_Vocative nn_PRD_V0C nn_PRD_Title nn_PRD_TTL nn_PRD_Topic nn_PRD_TPC amod_PRD_Temporal nn_PRD_TMP nn_PRD_Subject nn_PRD_SBJ number_%_3.0 number_%_0.48 dobj_put_PRD prepc_of_complement_put amod_complement_Locative nn_complement_PUT nn_complement_Purpose nn_complement_PRP dobj_Predicate_complement dep_PRD_Predicate nn_PRD_Nominal nn_PRD_N0M nn_PRD_Manner nn_PRD_~ nn_PRD_MNI nn_PRD_Location nn_PRD_L0C amod_PRD_subject amod_PRD_Logical nn_PRD_LGS nn_PRD_Headline nn_PRD_HLN nn_PRD_Extent nn_PRD_EXT nn_PRD_Dative nn_PRD_DTV nn_PRD_Direction nn_PRD_DIR amod_PRD_related advmod_related_Closely dep_CLR_PRD amod_CLR_It-cleft nn_CLR_CLF amod_CLR_Benefemtive nn_CLR_BNF amod_CLR_adverbial amod_CLR_Non-specific nn_CLR_ADV num_CLR_234
A00-2031	P97-1003	o	1 Introduction Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in -LRB- Chitrao and Grishman 1990 -RRB- and is by now a fairly well-studied problem -LRB- -LRB- Charniak 1997 -RRB- -LRB- Collins 1997 -RRB- -LRB- Ratnaparkhi 1997 -RRB- -RRB-	dep_Ratnaparkhi_1997 amod_Collins_1997 appos_Charniak_Ratnaparkhi appos_Charniak_Collins amod_Charniak_1997 dep_problem_Charniak amod_problem_well-studied det_problem_a advmod_problem_now advmod_well-studied_fairly prep_by_is_problem nsubj_is_sentences dep_Chitrao_1990 conj_and_Chitrao_Grishman npadvmod_ago_decade det_decade_a conj_and_examined_is prep_in_examined_Grishman prep_in_examined_Chitrao advmod_examined_ago advmod_examined_first auxpass_examined_was nsubjpass_examined_sentences det_treebank_a prep_from_gathered_treebank vmod_information_gathered amod_information_statistical dobj_using_information vmod_sentences_using amod_sentences_Parsing nn_sentences_Introduction num_Introduction_1
A00-2036	P97-1003	n	Bilexical context-free grammars have been presented in -LRB- Eisner and Satta 1999 -RRB- as an abstraction of language models that have been adopted in several recent real-world parsers improving state-of-the-art parsing accuracy -LRB- A1shawl 1996 Eisner 1996 Charniak 1997 Collins 1997 -RRB-	amod_Collins_1997 dep_Charniak_Collins conj_Charniak_1997 dep_Eisner_Charniak appos_Eisner_1996 dep_A1shawl_Eisner appos_A1shawl_1996 dep_accuracy_A1shawl nn_accuracy_parsing amod_accuracy_state-of-the-art dobj_improving_accuracy amod_parsers_real-world amod_parsers_recent amod_parsers_several prep_in_adopted_parsers auxpass_adopted_been aux_adopted_have nsubjpass_adopted_that rcmod_models_adopted nn_models_language prep_of_abstraction_models det_abstraction_an dep_Eisner_1999 conj_and_Eisner_Satta xcomp_presented_improving prep_as_presented_abstraction prep_in_presented_Satta prep_in_presented_Eisner auxpass_presented_been aux_presented_have nsubjpass_presented_grammars amod_grammars_context-free amod_grammars_Bilexical
C00-1011	P97-1003	o	40,000 sentences -RRB- and section 23 for testing -LRB- see Collins 1997 1999 Charniak 1997 2000 l ~ atnalmrkhi 1999 -RRB- we only tested on sentences _ < 40 words -LRB- 2245 sentences -RRB-	num_sentences_2245 appos_words_sentences num_words_40 amod_words_< dobj___words dep_tested__ prep_on_tested_sentences advmod_tested_only nsubj_tested_we num_atnalmrkhi_1999 appos_~_atnalmrkhi nn_~_l num_Charniak_2000 num_Charniak_1997 num_Collins_1999 num_Collins_1997 dep_see_~ dep_see_Charniak dobj_see_Collins dep_section_see prep_for_section_testing num_section_23 parataxis_sentences_tested conj_and_sentences_section num_sentences_40,000
C00-1011	P97-1003	o	As in other work we collapsed AI -RRB- VP and Pl?Jl to the same label when calculating these scores -LRB- see Collins 1997 I ~ atnaparkhi 1999 Charniak 1997 -RRB-	num_Charniak_1997 amod_1999_atnaparkhi parataxis_~_Charniak dobj_~_1999 nsubj_~_I num_Collins_1997 parataxis_see_~ dobj_see_Collins det_scores_these dep_calculating_see dobj_calculating_scores dep_when_calculating amod_label_same det_label_the conj_and_VP_Pl?Jl dep_AI_Pl?Jl dep_AI_VP advmod_collapsed_when prep_to_collapsed_label dobj_collapsed_AI nsubj_collapsed_we prep_collapsed_As amod_work_other pobj_in_work pcomp_As_in
C00-1011	P97-1003	n	These scores are higher than those of several other parsers -LRB- e.g. Collins 1997 99 Charniak 1997 -RRB- but remain behind tim scores of Charniak -LRB- 2000 -RRB- who obtains 90.1 % LP and 90.1 % LR for sentences _ < 40 words	num_words_40 amod_words_< dobj___words amod_LR_% number_%_90.1 conj_and_LP_LR amod_LP_% number_%_90.1 prep_for_obtains_sentences dobj_obtains_LR dobj_obtains_LP nsubj_obtains_who appos_Charniak_2000 rcmod_scores_obtains prep_of_scores_Charniak nn_scores_tim xcomp_remain__ prep_behind_remain_scores num_Charniak_1997 conj_but_Collins_remain dep_Collins_Charniak num_Collins_99 num_Collins_1997 pobj_e.g._remain pobj_e.g._Collins dep_parsers_e.g. amod_parsers_other amod_parsers_several prep_of_those_parsers prep_than_higher_those cop_higher_are nsubj_higher_scores det_scores_These
C00-1011	P97-1003	o	It also shows that DOP 's frontier lexicalization is a viable alternative to constituent lexicalization -LRB- as proposed in Charniak 1997 Collins 1997 99 Eisner 1997 -RRB-	num_Eisner_1997 dep_Collins_Eisner num_Collins_99 num_Collins_1997 num_Charniak_1997 dep_proposed_Collins prep_in_proposed_Charniak mark_proposed_as nn_lexicalization_constituent advcl_alternative_proposed prep_to_alternative_lexicalization amod_alternative_viable det_alternative_a cop_alternative_is nsubj_alternative_lexicalization mark_alternative_that nn_lexicalization_frontier poss_lexicalization_DOP ccomp_shows_alternative advmod_shows_also nsubj_shows_It
C00-1023	P97-1003	o	The Ino < lel 1 -RRB- 3 SI e l ina -LRB- 1.998 -RRB- was -LCB- rai \ -RSB- md tm SemCor that was merged with a flfll senl ential parse tree the determination of which is considered a difficult l -RRB- rolflem of its own -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_own_Collins poss_own_its prep_of_rolflem_own dep_l_rolflem amod_l_difficult det_l_a xcomp_considered_l auxpass_considered_is nsubjpass_considered_determination prep_of_determination_which det_determination_the rcmod_tree_considered nn_tree_parse amod_tree_ential nn_senl_flfll det_senl_a prep_with_merged_senl auxpass_merged_was nsubjpass_merged_that dep_SemCor_tree rcmod_SemCor_merged nn_SemCor_tm nn_SemCor_md nn_SemCor_\ nn_SemCor_rai dep_was_SemCor vmod_ina_was appos_ina_1.998 conj_l_ina dep_e_l dep_3_1 num_3_lel dep_<_3 dep_Ino_e appos_Ino_SI dep_Ino_< dep_The_Ino advcl_``_The
C00-1029	P97-1003	o	Further work will look at how to integrate probabilities such as p -LRB- clv r -RRB- into a model of dependency structure similar to that of Collins -LRB- 1996 -RRB- and Collins -LRB- 1997 -RRB- which can be used \ -LSB- ` or parse selection	conj_or_selection_parse amod_selection_\ dobj_used_selection dobj_used_parse auxpass_used_be aux_used_can nsubjpass_used_which appos_Collins_1997 conj_and_Collins_Collins appos_Collins_1996 rcmod_that_used prep_of_that_Collins prep_of_that_Collins prep_to_similar_that nn_structure_dependency prep_of_model_structure det_model_a appos_clv_r dep_p_clv prep_such_as_probabilities_p prep_into_integrate_model dobj_integrate_probabilities aux_integrate_to advmod_integrate_how advmod_look_similar prepc_at_look_integrate aux_look_will nsubj_look_work amod_work_Further
C00-1051	P97-1003	o	-LRB- 1998 -RRB- directly estimate DPs tbr ~ given int -RRB- ut whereas other models su -LRB- ' h as PCFOt -RRB- ased tel -RRB- down generation mod -LRB- ls P -LRB- H s -RRB- do not -LRB- Charnink 1997 Collins 1997 Shir ~ fi et ~ rl	nn_rl_~ nn_rl_et nn_rl_fi nn_rl_~ nn_rl_Shir amod_Collins_1997 appos_Charnink_1997 neg_do_not dep_H_s dep_P_rl dep_P_Collins dep_P_Charnink dep_P_do dep_P_H nn_P_ls nn_mod_generation dep_down_P pobj_down_mod dobj_ased_tel nsubj_ased_h prep_as_h_PCFOt nn_h_su rcmod_models_ased amod_models_other dep_ut_int dep_ut_given dep_ut_~ dep_tbr_ut nn_tbr_DPs dep_estimate_down dobj_estimate_models dep_estimate_whereas dobj_estimate_tbr advmod_estimate_directly dep_estimate_1998
C00-1052	P97-1003	o	The state of a left-corner parser does capture some linguistic generalizations -LRB- Mmming an < l Carpenter 1997 Roark a.nd Johnson 1999 -RRB- but one might still expect sparse-data problems	amod_problems_sparse-data dobj_expect_problems advmod_expect_still aux_expect_might nsubj_expect_one amod_Johnson_1999 nn_Johnson_a.nd nn_Johnson_Roark dep_Carpenter_Johnson amod_Carpenter_1997 nn_Carpenter_l dep_Carpenter_< det_Carpenter_an dobj_Mmming_Carpenter dep_generalizations_Mmming amod_generalizations_linguistic det_generalizations_some conj_but_capture_expect dobj_capture_generalizations aux_capture_does nsubj_capture_state amod_parser_left-corner det_parser_a prep_of_state_parser det_state_The ccomp_``_expect ccomp_``_capture
C00-1052	P97-1003	o	As reported previously the standard left-corner grmninar embeds sufficient non-local infornlation in its productions to significantly improve the labelled precision and recall of its MLPs with respect to MLPs of the PCFG estimated from the untransfornmd trees -LRB- Maiming and Carpenter 1997 ll.oark and Johnson 1999 -RRB-	amod_ll.oark_1999 conj_and_ll.oark_Johnson conj_and_Maiming_Johnson conj_and_Maiming_ll.oark conj_and_Maiming_1997 conj_and_Maiming_Carpenter dep_trees_ll.oark dep_trees_1997 dep_trees_Carpenter dep_trees_Maiming amod_trees_untransfornmd det_trees_the prep_from_estimated_trees vmod_PCFG_estimated det_PCFG_the prep_of_MLPs_PCFG poss_MLPs_its prep_of_precision_MLPs conj_and_precision_recall amod_precision_labelled det_precision_the prep_with_respect_to_improve_MLPs dobj_improve_recall dobj_improve_precision advmod_improve_significantly aux_improve_to poss_productions_its amod_infornlation_non-local amod_infornlation_sufficient vmod_embeds_improve prep_in_embeds_productions dobj_embeds_infornlation nsubj_embeds_grmninar advcl_embeds_reported nn_grmninar_left-corner amod_grmninar_standard det_grmninar_the advmod_reported_previously mark_reported_As
C00-1052	P97-1003	o	Ultinmtely however it seems that a more complex ai -RRB- t -RRB- roach incorporating back-off and smoothing is necessary ill order to achieve the parsing accuracy achieved by Charniak -LRB- 1997 -RRB- and Collins -LRB- 1997 -RRB-	appos_Collins_1997 conj_and_Charniak_Collins appos_Charniak_1997 agent_achieved_Collins agent_achieved_Charniak vmod_accuracy_achieved nn_accuracy_parsing det_accuracy_the dobj_achieve_accuracy aux_achieve_to vmod_order_achieve amod_order_ill amod_order_necessary cop_order_is nsubj_order_roach dep_order_t conj_and_back-off_smoothing dobj_incorporating_smoothing dobj_incorporating_back-off vmod_roach_incorporating dep_t_ai dep_t_complex dep_t_that advmod_complex_more det_complex_a ccomp_seems_order nsubj_seems_it advmod_seems_however advmod_seems_Ultinmtely
C00-1081	P97-1003	o	Recently in the area of parsers based oll a. stochastic context-fi ee grammar -LRB- SCFG -RRB- some researchers have pointed out the importance of t.he lexicon and proposed lexiealized models -LRB- Charniak 1997 Collins 1997 -RRB-	amod_Collins_1997 dep_Charniak_Collins appos_Charniak_1997 dep_models_Charniak amod_models_lexiealized dobj_proposed_models nsubj_proposed_researchers amod_lexicon_t.he prep_of_importance_lexicon det_importance_the conj_and_pointed_proposed dobj_pointed_importance prt_pointed_out aux_pointed_have nsubj_pointed_researchers det_researchers_some rcmod_grammar_proposed rcmod_grammar_pointed appos_grammar_SCFG nn_grammar_ee dep_context-fi_grammar amod_context-fi_stochastic nn_context-fi_a. nn_context-fi_oll amod_context-fi_based dep_area_context-fi prep_of_area_parsers det_area_the prep_in_,_area dep_``_Recently
C00-1081	P97-1003	p	Also in a sta.te-ofthe-a rt English pa.rser -LRB- Collins 1997 -RRB- only the words tha t occur more tha n d times in training data	nn_data_training prep_in_times_data nn_times_d nn_times_n appos_tha_times amod_tha_more dobj_occur_tha nsubj_occur_t dep_occur_tha nsubj_occur_words det_words_the advmod_words_only amod_Collins_1997 dep_pa.rser_occur dep_pa.rser_Collins nn_pa.rser_English nn_pa.rser_rt prep_in_pa.rser_sta.te-ofthe-a advmod_pa.rser_Also det_sta.te-ofthe-a_a
C00-2099	P97-1003	o	This is the same separation of arguments and adjuncts as that employed by -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_by_Collins prep_employed_by vmod_that_employed conj_and_arguments_adjuncts prep_as_separation_that prep_of_separation_adjuncts prep_of_separation_arguments amod_separation_same det_separation_the cop_separation_is nsubj_separation_This
C00-2105	P97-1003	o	~ The difl ` erent kinds of noun chunks covered by our grmnmar are listed below and illustrated with exmnples a combination of a non-obligatory deternfiner optional adjectives or cardinals and the noun 1Other types of lexicalised PCFGs have been -LRB- h!scrib -LRB- ' d in -LRB- Charniak 1997 -RRB- -LRB- Collins 1997 -RRB- -LRB- G'oodman 1997 -RRB- -LRB- Chcll -RRB- a and lelinek 1998 -RRB- mid -LRB- Eisner and Sat a 1999 -RRB-	det_1999_a dep_Eisner_1999 conj_and_Eisner_Sat amod_Eisner_mid dep_mid_lelinek dep_lelinek_1998 conj_and_a_Sat conj_and_a_Eisner amod_Collins_1997 amod_Charniak_1997 dep_in_Charniak num_d_1997 appos_d_G'oodman appos_d_Collins prep_d_in dep_h!scrib_Eisner dep_h!scrib_a appos_h!scrib_Chcll appos_h!scrib_d dep_been_h!scrib dep_have_been amod_PCFGs_lexicalised prep_of_types_PCFGs nn_types_1Other nn_types_noun det_types_the conj_and_adjectives_types conj_or_adjectives_cardinals amod_adjectives_optional amod_deternfiner_non-obligatory det_deternfiner_a vmod_combination_have appos_combination_types appos_combination_cardinals appos_combination_adjectives prep_of_combination_deternfiner det_combination_a prep_with_illustrated_exmnples nsubjpass_illustrated_kinds dep_listed_combination conj_and_listed_illustrated advmod_listed_below auxpass_listed_are nsubjpass_listed_kinds poss_grmnmar_our agent_covered_grmnmar vmod_chunks_covered nn_chunks_noun prep_of_kinds_chunks amod_kinds_erent nn_kinds_difl det_kinds_The num_kinds_~
C00-2109	P97-1003	o	There have been a lot of prol -RRB- OS ~ fls for statistical analysis in ninny languages in particular in English and Japanese -LRB- Magerman 1995 -RRB- -LRB- Sekine and Grishman 1995 -RRB- -LRB- Collins 1997 -RRB- -LRB- I/atnal -RRB- arkhi 1997 -RRB- -LRB- K.Shirai et.al 1998 -RRB- -LRB- Fujio and Matsnlnoto 1998 -RRB- -LRB- Itaruno ct.al 1997 -RRB- -LRB- Ehara 1998 -RRB-	amod_Ehara_1998 dep_ct.al_1997 nn_ct.al_Itaruno dep_Fujio_1998 conj_and_Fujio_Matsnlnoto dep_et.al_1998 nn_et.al_K.Shirai dep_arkhi_1997 dep_Collins_1997 dep_Sekine_1995 conj_and_Sekine_Grishman dep_Magerman_1995 conj_and_English_Japanese nn_languages_ninny amod_analysis_statistical nn_fls_~ nn_fls_OS nn_fls_prol dep_lot_Ehara dep_lot_ct.al dep_lot_Matsnlnoto dep_lot_Fujio dep_lot_et.al dep_lot_arkhi dep_lot_I/atnal dep_lot_Collins dep_lot_Grishman dep_lot_Sekine dep_lot_Magerman prep_in_lot_Japanese prep_in_lot_English prep_in_lot_particular prep_in_lot_languages prep_for_lot_analysis prep_of_lot_fls det_lot_a cop_lot_been aux_lot_have expl_lot_There
C00-2118	P97-1003	o	The last two counts -LRB- CAUS and ANIM -RRB- were performed on a 29-million word parsed corpus -LRB- \ gall Street Journal 1988 provided by Michael Collins -LRB- Collins 1997 -RRB- -RRB-	dep_Collins_1997 dep_Collins_Collins nn_Collins_Michael agent_provided_Collins vmod_Journal_provided num_Journal_1988 nn_Journal_Street nn_Journal_gall num_Journal_\ dep_corpus_Journal dobj_parsed_corpus vmod_word_parsed amod_word_29-million det_word_a prep_on_performed_word auxpass_performed_were nsubjpass_performed_counts conj_and_CAUS_ANIM dep_counts_ANIM dep_counts_CAUS num_counts_two amod_counts_last det_counts_The
C00-2135	P97-1003	o	The progress in parsing technology are noteworthy and in particular various statistical dependency models have been proposed -LRB- Collins 1997 -RRB- -LRB- Ratnaparkhi 1997 -RRB- -LRB- Charniak 2000 -RRB-	amod_Charniak_2000 dep_Ratnaparkhi_1997 dep_Collins_1997 dep_proposed_Charniak dep_proposed_Ratnaparkhi dep_proposed_Collins auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models prep_in_proposed_particular nn_models_dependency amod_models_statistical amod_models_various conj_and_noteworthy_proposed cop_noteworthy_are nsubj_noteworthy_progress nn_technology_parsing prep_in_progress_technology det_progress_The ccomp_``_proposed ccomp_``_noteworthy
C02-1003	P97-1003	p	Substantial improvements have been made to parse western language such as English and many powerful models have been proposed -LRB- Brill 1993 Collins 1997 -RRB-	num_Collins_1997 nn_Collins_Brill num_Brill_1993 dep_proposed_Collins auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models amod_models_powerful amod_models_many prep_such_as_language_English amod_language_western dobj_parse_language aux_parse_to conj_and_made_proposed xcomp_made_parse auxpass_made_been aux_made_have nsubjpass_made_improvements amod_improvements_Substantial
C02-1132	P97-1003	o	3TheData For our experiments we used a version of the British National Corpus parsed with the statistical parser of Collins -LRB- 1997 -RRB-	appos_Collins_1997 prep_of_parser_Collins amod_parser_statistical det_parser_the prep_with_parsed_parser nn_Corpus_National nn_Corpus_British det_Corpus_the prep_of_version_Corpus det_version_a dep_used_parsed dobj_used_version nsubj_used_we poss_experiments_our rcmod_3TheData_used prep_for_3TheData_experiments
C02-1143	P97-1003	o	In order to extract the linguistic features necessary for the model all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger -LRB- Ratnaparkhi 1998 -RRB- and parsed using the Collins parser -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_parser_Collins nn_parser_Collins det_parser_the dobj_using_parser xcomp_parsed_using amod_Ratnaparkhi_1998 dep_tagger_Ratnaparkhi amod_tagger_entropy nn_tagger_maximum det_tagger_a conj_and_using_parsed dobj_using_tagger dep_part-of-speech-tagged_parsed dep_part-of-speech-tagged_using advmod_part-of-speech-tagged_automatically ccomp_first_part-of-speech-tagged cop_first_were nsubj_first_sentences advcl_first_extract det_sentences_all det_model_the prep_for_necessary_model amod_features_necessary amod_features_linguistic det_features_the dobj_extract_features aux_extract_to dep_extract_order mark_extract_In
C02-2025	P97-1003	o	Most probabilistic parsing research including for example work by by Collins -LRB- 1997 -RRB- and Charniak -LRB- 1997 -RRB- is based on branching process models -LRB- Harris 1963 -RRB-	amod_Harris_1963 appos_models_Harris nn_models_process amod_models_branching prep_on_based_models auxpass_based_is nsubjpass_based_research appos_Charniak_1997 conj_and_Collins_Charniak appos_Collins_1997 pobj_by_Charniak pobj_by_Collins pcomp_by_by prep_work_by prep_for_work_example prep_including_research_work nn_research_parsing amod_research_probabilistic amod_research_Most
C02-2025	P97-1003	o	Abney -LRB- 1997 -RRB- notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations motivating the use of log-linear models -LRB- Agresti 1990 -RRB- for parse ranking that Johnson and colleagues further developed -LRB- Johnson Geman Canon Chi & Riezler 1999 -RRB-	amod_Johnson_1999 conj_and_Johnson_Riezler conj_and_Johnson_Chi conj_and_Johnson_Canon conj_and_Johnson_Geman dep_developed_Riezler dep_developed_Chi dep_developed_Canon dep_developed_Geman dep_developed_Johnson advmod_developed_further nsubj_developed_colleagues nsubj_developed_Johnson dobj_developed_that conj_and_Johnson_colleagues rcmod_ranking_developed nn_ranking_parse amod_Agresti_1990 dep_models_Agresti amod_models_log-linear prep_for_use_ranking prep_of_use_models det_use_the dobj_motivating_use det_derivations_the vmod_determining_motivating dobj_determining_derivations advmod_determining_actually aux_determining_is nsubj_determining_grammar advmod_determining_when amod_grammar_unification-based det_grammar_a det_approach_the prep_of_soundness_approach det_soundness_the prep_with_problems_soundness amod_problems_important advcl_notes_determining dobj_notes_problems nsubj_notes_Abney appos_Abney_1997
C02-2026	P97-1003	p	Several general-purpose off-the-shelf -LRB- OTS -RRB- parsers have become widely available -LRB- Lin 1994 Collins 1997 -RRB-	amod_Collins_1997 dep_Lin_Collins num_Lin_1994 advmod_available_widely dep_become_Lin acomp_become_available aux_become_have nsubj_become_parsers nn_parsers_off-the-shelf appos_off-the-shelf_OTS amod_off-the-shelf_general-purpose amod_off-the-shelf_Several
C04-1010	P97-1003	p	On the other hand the best available parsers trained on the Penn Treebank those of Collins -LRB- 1997 -RRB- and Charniak -LRB- 2000 -RRB- use statistical models for disambiguation that make crucial use of dependency relations	nn_relations_dependency prep_of_use_relations amod_use_crucial dobj_make_use nsubj_make_that rcmod_disambiguation_make prep_for_models_disambiguation amod_models_statistical nn_models_use appos_Charniak_2000 conj_and_Collins_Charniak appos_Collins_1997 prep_of_those_Charniak prep_of_those_Collins nn_Treebank_Penn det_Treebank_the prep_on_trained_Treebank appos_parsers_models dep_parsers_those vmod_parsers_trained amod_parsers_available amod_parsers_best det_parsers_the dep_,_parsers amod_hand_other det_hand_the pobj_On_hand dep_``_On
C04-1010	P97-1003	p	Moreover the deterministic dependency parser of Yamada and Matsumoto -LRB- 2003 -RRB- when trained on the Penn Treebank gives a dependency accuracy that is almost as good as that of Collins -LRB- 1997 -RRB- and Charniak -LRB- 2000 -RRB-	appos_Charniak_2000 conj_and_Collins_Charniak appos_Collins_1997 prep_of_that_Charniak prep_of_that_Collins prep_as_good_that advmod_good_as advmod_good_almost cop_good_is nsubj_good_that rcmod_accuracy_good nn_accuracy_dependency det_accuracy_a dobj_gives_accuracy nsubj_gives_parser advmod_gives_Moreover nn_Treebank_Penn det_Treebank_the prep_on_trained_Treebank advmod_trained_when appos_Matsumoto_2003 conj_and_Yamada_Matsumoto rcmod_parser_trained prep_of_parser_Matsumoto prep_of_parser_Yamada nn_parser_dependency amod_parser_deterministic det_parser_the
C04-1010	P97-1003	o	Table 2 shows the dependency accuracy root accuracy and complete match scores for our best parser -LRB- Model 2 with label set B -RRB- in comparison with Collins -LRB- 1997 -RRB- -LRB- Model 3 -RRB- Charniak -LRB- 2000 -RRB- and Yamada and Matsumoto -LRB- 2003 -RRB- .5 It is clear that with respect to unlabeled accuracy our parser does not quite reach state-of-the-art performance even if we limit the competition to deterministic methods such as that of Yamada and Matsumoto -LRB- 2003 -RRB-	appos_Matsumoto_2003 conj_and_Yamada_Matsumoto prep_of_that_Matsumoto prep_of_that_Yamada prep_such_as_methods_that amod_methods_deterministic prep_to_competition_methods det_competition_the dobj_limit_competition nsubj_limit_we mark_limit_if advmod_limit_even amod_performance_state-of-the-art dobj_reach_performance advmod_reach_quite neg_reach_not aux_reach_does nsubj_reach_parser prep_with_respect_to_reach_accuracy mark_reach_that poss_parser_our amod_accuracy_unlabeled ccomp_clear_reach cop_clear_is nsubj_clear_It dep_clear_Matsumoto dep_clear_Yamada num_It_.5 dep_Matsumoto_2003 conj_and_Yamada_Matsumoto appos_Charniak_2000 dep_3_Model conj_and_Collins_clear conj_and_Collins_Charniak dep_Collins_3 appos_Collins_1997 prep_with_comparison_clear prep_with_comparison_Charniak prep_with_comparison_Collins nn_B_set nn_B_label prep_with_Model_B num_Model_2 dep_parser_Model amod_parser_best poss_parser_our nn_scores_match amod_scores_complete nn_accuracy_root conj_and_accuracy_scores conj_and_accuracy_accuracy nn_accuracy_dependency det_accuracy_the advcl_shows_limit prep_in_shows_comparison prep_for_shows_parser dobj_shows_scores dobj_shows_accuracy dobj_shows_accuracy nsubj_shows_Table num_Table_2
C04-1010	P97-1003	o	This permits us to make exact comparisons with the parser of Yamada and Matsumoto -LRB- 2003 -RRB- but also the parsers of Collins -LRB- 1997 -RRB- and Charniak -LRB- 2000 -RRB- which are evaluated on the same data set in Yamada and Matsumoto -LRB- 2003 -RRB-	appos_Matsumoto_2003 conj_and_Yamada_Matsumoto prep_in_set_Matsumoto prep_in_set_Yamada vmod_data_set amod_data_same det_data_the prep_on_evaluated_data auxpass_evaluated_are nsubjpass_evaluated_which appos_Charniak_2000 conj_and_Collins_Charniak appos_Collins_1997 rcmod_parsers_evaluated prep_of_parsers_Charniak prep_of_parsers_Collins det_parsers_the dep_also_parsers appos_Matsumoto_2003 conj_and_Yamada_Matsumoto prep_of_parser_Matsumoto prep_of_parser_Yamada det_parser_the amod_comparisons_exact prep_with_make_parser dobj_make_comparisons aux_make_to conj_but_permits_also xcomp_permits_make dobj_permits_us nsubj_permits_This ccomp_``_also ccomp_``_permits
C04-1040	P97-1003	o	Here we extract part-of-speech tags from the Collins parsers output -LRB- Collins 1997 -RRB- for section 23 instead of reinventing a tagger	det_tagger_a dobj_reinventing_tagger prepc_instead_of_section_reinventing num_section_23 amod_Collins_1997 dep_output_Collins nn_output_parsers nn_output_Collins det_output_the amod_tags_part-of-speech prep_for_extract_section prep_from_extract_output dobj_extract_tags nsubj_extract_we advmod_extract_Here
C08-1025	P97-1003	o	As is common -LRB- Collins 1997 Johnson 1998 Klein and Manning 2003 Schmid 2006 -RRB- the treebank is first transformed in various ways in order to give an accurate PCFG	amod_PCFG_accurate det_PCFG_an dobj_give_PCFG aux_give_to dep_give_order mark_give_in dep_give_first aux_give_is nsubj_give_treebank dep_give_2003 dep_give_Manning dep_give_Klein amod_ways_various prep_in_transformed_ways dep_first_transformed det_treebank_the dep_Schmid_2006 dep_Klein_Schmid conj_and_Klein_2003 conj_and_Klein_Manning num_Johnson_1998 parataxis_Collins_give conj_Collins_Johnson amod_Collins_1997 dep_common_Collins cop_common_is advmod_common_As
C08-1050	P97-1003	o	By habit most systems for automatic role-semantic analysis have used Pennstyle constituents -LRB- Marcus et al. 1993 -RRB- produced by Collins -LRB- 1997 -RRB- or Charniaks -LRB- 2000 -RRB- parsers	nn_parsers_Charniaks appos_Charniaks_2000 conj_or_Collins_parsers appos_Collins_1997 prep_by_produced_parsers prep_by_produced_Collins amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_constituents_Pennstyle dep_used_produced dep_used_Marcus dobj_used_constituents aux_used_have nsubj_used_systems prep_by_used_habit amod_analysis_role-semantic amod_analysis_automatic prep_for_systems_analysis amod_systems_most
C08-1050	P97-1003	o	Typical approaches to conversion of constituent structures into dependencies are based on handconstructed head percolation rules an idea that has its roots in lexicalized constituent parsing -LRB- Magerman 1994 Collins 1997 -RRB-	amod_Collins_1997 dep_Magerman_Collins conj_Magerman_1994 dep_parsing_Magerman nn_parsing_constituent amod_parsing_lexicalized prep_in_roots_parsing poss_roots_its dobj_has_roots nsubj_has_that rcmod_idea_has det_idea_an dep_,_idea nn_rules_percolation nn_rules_head nn_rules_handconstructed prep_on_based_rules auxpass_based_are nsubjpass_based_approaches amod_structures_constituent prep_into_conversion_dependencies prep_of_conversion_structures prep_to_approaches_conversion amod_approaches_Typical
C08-1050	P97-1003	o	A constituent-based system using Collins parser -LRB- Collins 1997 -RRB-	amod_Collins_1997 appos_parser_Collins nn_parser_Collins dobj_using_parser vmod_system_using amod_system_constituent-based det_system_A
C08-1116	P97-1003	o	For our studies here the parser employed was that of Collins -LRB- 1997 -RRB- applied to the sentences of the British National Corpus -LRB- BNC Consortium 2001 -RRB-	dep_Consortium_2001 nn_Consortium_BNC appos_Corpus_Consortium nn_Corpus_National nn_Corpus_British det_Corpus_the prep_of_sentences_Corpus det_sentences_the prep_to_applied_sentences vmod_Collins_applied appos_Collins_1997 prep_of_that_Collins nsubj_was_that nsubj_was_parser prep_for_was_studies vmod_parser_employed det_parser_the advmod_studies_here poss_studies_our
D07-1058	P97-1003	o	Moreover as P-DOP is formulated as an enrichment of the treebank Probabilistic Context-free Grammar -LRB- PCFG -RRB- it allows for much easier comparison to alternative approaches to statistical parsing -LRB- Collins 1997 Charniak 1997 Johnson 1998 Klein and Manning 2003 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et num_Johnson_1998 conj_and_Charniak_Petrov conj_and_Charniak_2003 conj_and_Charniak_Manning conj_and_Charniak_Klein conj_and_Charniak_Johnson conj_and_Charniak_1997 dep_Collins_Petrov dep_Collins_2003 dep_Collins_Manning dep_Collins_Klein dep_Collins_Johnson dep_Collins_1997 dep_Collins_Charniak amod_Collins_1997 dep_parsing_Collins amod_parsing_statistical prep_to_approaches_parsing amod_approaches_alternative prep_to_comparison_approaches amod_comparison_easier advmod_easier_much prep_for_allows_comparison nsubj_allows_it advcl_allows_formulated advmod_allows_Moreover appos_Grammar_PCFG nn_Grammar_Context-free nn_Grammar_Probabilistic nn_Grammar_treebank det_Grammar_the prep_of_enrichment_Grammar det_enrichment_an prep_as_formulated_enrichment auxpass_formulated_is nsubjpass_formulated_P-DOP mark_formulated_as
D07-1058	P97-1003	p	All state-of-the-art wide-coverage parsers relax this assumption in some way for instance by -LRB- i -RRB- changing the parser in step -LRB- 3 -RRB- such that the application of rules is conditioned on other steps in the derivation process -LRB- Collins 1997 Charniak 1997 -RRB- or by -LRB- ii -RRB- enriching the nonterminal labels in step -LRB- 1 -RRB- with context-information -LRB- Johnson 1998 Klein and Manning 2003 -RRB- along with suitable backtransforms in step -LRB- 4 -RRB-	appos_step_4 prep_in_backtransforms_step amod_backtransforms_suitable dep_Klein_2003 conj_and_Klein_Manning dep_Johnson_Manning dep_Johnson_Klein amod_Johnson_1998 appos_context-information_Johnson prep_with_step_context-information appos_step_1 amod_labels_nonterminal det_labels_the pobj_enriching_backtransforms prepc_along_with_enriching_with prep_in_enriching_step dobj_enriching_labels dep_ii_enriching pobj_by_ii dep_Charniak_1997 dep_Collins_Charniak amod_Collins_1997 appos_process_Collins nn_process_derivation det_process_the prep_in_steps_process amod_steps_other conj_or_conditioned_by prep_on_conditioned_steps auxpass_conditioned_is nsubjpass_conditioned_application mark_conditioned_that dep_conditioned_such prep_of_application_rules det_application_the appos_step_3 det_parser_the parataxis_changing_by parataxis_changing_conditioned prep_in_changing_step dobj_changing_parser dep_i_changing dep_by_i det_way_some det_assumption_this prep_relax_by prep_for_relax_instance prep_in_relax_way dobj_relax_assumption nsubj_relax_parsers nn_parsers_wide-coverage amod_parsers_state-of-the-art det_parsers_All
D07-1077	P97-1003	o	presented in Collins -LRB- 1997 -RRB-	appos_Collins_1997 prep_in_presented_Collins
D07-1078	P97-1003	o	The parse trees on the English side of the bitexts were generated using a parser -LRB- Soricut 2004 -RRB- implementing the Collins parsing models -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_models_Collins nn_models_parsing nn_models_Collins det_models_the dobj_implementing_models amod_Soricut_2004 vmod_parser_implementing dep_parser_Soricut det_parser_a dobj_using_parser xcomp_generated_using auxpass_generated_were nsubjpass_generated_trees det_bitexts_the prep_of_side_bitexts amod_side_English det_side_the prep_on_trees_side nn_trees_parse det_trees_The
D07-1078	P97-1003	o	Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization -LRB- Collins 1997 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak amod_Collins_1997 appos_markovization_Collins nn_models_parsing prep_via_generalizing_markovization dobj_generalizing_models prepc_to_similar_generalizing prep_in_similar_spirit cop_similar_is csubj_similar_Binarizing nn_translation_machine amod_translation_syntax-based nn_trees_syntax det_trees_the prep_for_Binarizing_translation dobj_Binarizing_trees
D07-1096	P97-1003	o	Even before the 2006 shared task the parsers of Collins -LRB- 1997 -RRB- and Charniak -LRB- 2000 -RRB- originally developed for English had been adapted for dependency parsing of Czech and the parsing methodology proposed by Kudo and Matsumoto -LRB- 2002 -RRB- and Yamada and Matsumoto -LRB- 2003 -RRB- had been evaluated on both Japanese and English	conj_and_Japanese_English preconj_Japanese_both prep_on_evaluated_English prep_on_evaluated_Japanese auxpass_evaluated_been aux_evaluated_had nsubjpass_evaluated_methodology appos_Matsumoto_2003 appos_Matsumoto_2002 conj_and_Kudo_Matsumoto conj_and_Kudo_Yamada conj_and_Kudo_Matsumoto agent_proposed_Matsumoto agent_proposed_Yamada agent_proposed_Matsumoto agent_proposed_Kudo vmod_methodology_proposed nn_methodology_parsing det_methodology_the prep_of_parsing_Czech nn_parsing_dependency conj_and_adapted_evaluated prep_for_adapted_parsing auxpass_adapted_been aux_adapted_had nsubjpass_adapted_parsers prep_before_adapted_task advmod_adapted_Even prep_for_developed_English advmod_developed_originally appos_Charniak_2000 conj_and_Collins_Charniak appos_Collins_1997 vmod_parsers_developed prep_of_parsers_Charniak prep_of_parsers_Collins det_parsers_the amod_task_shared num_task_2006 det_task_the
D07-1116	P97-1003	o	Sometimes due to data sparseness and/or limitations in the machine learning paradigm used we need to extract features from the available representation in a manner that profoundly changes the representation -LRB- as is done in bilexical parsing -LRB- Collins 1997 -RRB- -RRB-	dep_Collins_1997 dep_parsing_Collins amod_parsing_bilexical prep_in_done_parsing auxpass_done_is advmod_done_as dep_representation_done det_representation_the dobj_changes_representation advmod_changes_profoundly nsubj_changes_that det_manner_a amod_representation_available det_representation_the ccomp_extract_changes prep_in_extract_manner prep_from_extract_representation dobj_extract_features aux_extract_to xcomp_need_extract nsubj_need_we prep_due_to_need_limitations prep_due_to_need_sparseness advmod_need_Sometimes vmod_paradigm_used nn_paradigm_learning nn_paradigm_machine det_paradigm_the prep_in_sparseness_paradigm conj_and/or_sparseness_limitations nn_sparseness_data
D08-1018	P97-1003	o	The last issue is how our binarization performs on a lexicalized parser like Collins -LRB- 1997 -RRB-	appos_Collins_1997 prep_like_parser_Collins amod_parser_lexicalized det_parser_a prep_on_performs_parser nsubj_performs_binarization advmod_performs_how poss_binarization_our ccomp_is_performs nsubj_is_issue amod_issue_last det_issue_The
D08-1018	P97-1003	o	Our intuition is that we can not apply our binarization to Collins -LRB- 1997 -RRB-	appos_Collins_1997 poss_binarization_our prep_to_apply_Collins dobj_apply_binarization neg_apply_not aux_apply_can nsubj_apply_we mark_apply_that ccomp_is_apply nsubj_is_intuition poss_intuition_Our
D08-1018	P97-1003	o	Therefore in Collins -LRB- 1997 -RRB- grammar rules are already factorized into a set of probabilities	prep_of_set_probabilities det_set_a prep_into_factorized_set advmod_factorized_already auxpass_factorized_are prep_in_factorized_rules advmod_factorized_Therefore nn_rules_grammar num_rules_1997 nn_rules_Collins
D08-1018	P97-1003	n	In order to capture the dependency relationship between lexcial heads Collins -LRB- 1997 -RRB- breaks down the rules from head outwards which prevents us from factorizing them in other ways	amod_ways_other prep_in_factorizing_ways dobj_factorizing_them prepc_from_prevents_factorizing dobj_prevents_us nsubj_prevents_which rcmod_head_prevents advmod_head_outwards det_rules_the prep_from_breaks_head prep_down_breaks_rules nn_breaks_Collins appos_Collins_1997 nn_Collins_heads amod_Collins_lexcial prep_between_relationship_breaks nn_relationship_dependency det_relationship_the dobj_capture_relationship aux_capture_to dep_capture_order mark_capture_In advcl_``_capture
D08-1018	P97-1003	o	It is equipped with head binarization to help improve parsing accuracy following the traditional linguistic insight that phrases are organized around the head -LRB- Collins 1997 Klein and Manning 2003b -RRB-	appos_Klein_2003b conj_and_Klein_Manning dep_Collins_Manning dep_Collins_Klein amod_Collins_1997 dep_head_Collins det_head_the prep_around_organized_head auxpass_organized_are nsubjpass_organized_phrases dobj_organized_that rcmod_insight_organized amod_insight_linguistic amod_insight_traditional det_insight_the pobj_following_insight ccomp_,_following amod_accuracy_parsing dobj_improve_accuracy ccomp_help_improve aux_help_to nn_binarization_head xcomp_equipped_help prep_with_equipped_binarization auxpass_equipped_is nsubjpass_equipped_It advcl_``_equipped
D08-1050	P97-1003	n	-LRB- 2006 -RRB- produced a corpus of 4,000 questions annotated with syntactic trees and obtained an improvement in parsing accuracy for Bikels reimplementation of the Collins parser -LRB- Collins 1997 -RRB- by training a new parser model with a combination of newspaper and question data	nn_data_question nn_data_newspaper conj_and_newspaper_question prep_of_combination_data det_combination_a nn_model_parser amod_model_new det_model_a prep_with_training_combination dobj_training_model dep_Collins_1997 appos_parser_Collins nn_parser_Collins det_parser_the prep_of_reimplementation_parser nn_reimplementation_Bikels prep_for_accuracy_reimplementation amod_accuracy_parsing prep_in_improvement_accuracy det_improvement_an prepc_by_obtained_training dobj_obtained_improvement amod_trees_syntactic prep_with_annotated_trees amod_questions_annotated num_questions_4,000 prep_of_corpus_questions det_corpus_a conj_and_produced_obtained dobj_produced_corpus dep_produced_2006
D09-1021	P97-1003	o	This procedure uses the head finding rules of -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_of_Collins prep_rules_of nn_rules_finding dep_head_rules det_head_the dobj_uses_head nsubj_uses_procedure det_procedure_This
D09-1085	P97-1003	o	The Stanford parser is representative of a large number of PTB parsers exemplified by Collins -LRB- 1997 -RRB- and Charniak -LRB- 2000 -RRB-	appos_Charniak_2000 conj_and_Collins_Charniak appos_Collins_1997 agent_exemplified_Charniak agent_exemplified_Collins nn_parsers_PTB prep_of_number_parsers amod_number_large det_number_a vmod_representative_exemplified prep_of_representative_number cop_representative_is nsubj_representative_parser nn_parser_Stanford det_parser_The
D09-1119	P97-1003	o	While it was initially believed that lexicalization of PCFG parsers -LRB- Collins 1997 Charniak 2000 -RRB- is crucial for obtaining good parsing results Gildea -LRB- 2001 -RRB- demonstrated that the lexicalized Model-1 parser of Collins -LRB- 1997 -RRB- does not benefit from bilexical information when tested on a new text domain and only marginally benefits from such information when tested on the same text domain as the training corpora	nn_corpora_training det_corpora_the nn_domain_text amod_domain_same det_domain_the prep_as_tested_corpora prep_on_tested_domain advmod_tested_when amod_information_such rcmod_benefits_tested prep_from_benefits_information advmod_benefits_marginally advmod_benefits_only nn_domain_text amod_domain_new det_domain_a prep_on_tested_domain advmod_tested_when amod_information_bilexical conj_and_benefit_benefits advcl_benefit_tested prep_from_benefit_information neg_benefit_not aux_benefit_does nsubj_benefit_parser mark_benefit_that appos_Collins_1997 prep_of_parser_Collins nn_parser_Model-1 amod_parser_lexicalized det_parser_the ccomp_demonstrated_benefits ccomp_demonstrated_benefit nsubj_demonstrated_Gildea advcl_demonstrated_believed appos_Gildea_2001 nn_results_parsing amod_results_good dobj_obtaining_results prepc_for_crucial_obtaining cop_crucial_is nsubj_crucial_lexicalization mark_crucial_that dep_Charniak_2000 dep_Collins_Charniak amod_Collins_1997 appos_parsers_Collins nn_parsers_PCFG prep_of_lexicalization_parsers ccomp_believed_crucial advmod_believed_initially auxpass_believed_was nsubjpass_believed_it mark_believed_While
D09-1135	P97-1003	p	Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree and get significant improvements for parsing -LRB- Collins 1997 Charniak 1997 Collins 1999 Charniak 2000 -RRB-	amod_Charniak_2000 num_Collins_1999 dep_Charniak_Charniak conj_Charniak_Collins conj_Charniak_1997 dep_Collins_Charniak amod_Collins_1997 dep_parsing_Collins prep_for_improvements_parsing amod_improvements_significant dobj_get_improvements nsubj_get_PCFGs det_tree_a amod_node_phrasal prep_of_head_node amod_head_lexical det_head_the amod_features_structural det_features_the conj_and_use_get prep_in_use_tree prep_on_use_head dobj_use_features nsubj_use_PCFGs amod_PCFGs_Lexicalized
D09-1161	P97-1003	o	In general they can be divided into two major categories namely lexicalized models -LRB- Collins 1997 1999 Charniak 1997 2000 -RRB- and un-lexicalized models -LRB- Klein and Manning 2003 Matsuzaki et al. 2005 Petrov et al. 2006 Petrov and Klein 2007 -RRB-	dep_Petrov_2007 conj_and_Petrov_Klein dep_al._2006 nn_al._et nn_al._Petrov dep_al._2005 nn_al._et nn_al._Matsuzaki num_Manning_2003 dep_Klein_Klein dep_Klein_Petrov conj_and_Klein_al. conj_and_Klein_al. conj_and_Klein_Manning appos_models_al. appos_models_al. appos_models_Manning appos_models_Klein amod_models_un-lexicalized num_Charniak_2000 num_Charniak_1997 dep_Collins_Charniak amod_Collins_1999 num_Collins_1997 conj_and_models_models appos_models_Collins amod_models_lexicalized advmod_models_namely appos_categories_models appos_categories_models amod_categories_major num_categories_two prep_into_divided_categories auxpass_divided_be aux_divided_can nsubjpass_divided_they prep_in_divided_general
E06-1012	P97-1003	n	-LRB- 1999 -RRB- applied the parser of Collins -LRB- 1997 -RRB- developed for English to Czech and found thatthe performance wassubstantially lower when compared to the results for English	prep_for_results_English det_results_the prep_to_compared_results advmod_compared_when advmod_lower_wassubstantially npadvmod_lower_performance nn_performance_thatthe acomp_found_lower prep_to_developed_Czech prep_for_developed_English nsubj_developed_parser appos_Collins_1997 prep_of_parser_Collins det_parser_the advcl_applied_compared conj_and_applied_found ccomp_applied_developed dep_applied_1999
E06-1015	P97-1003	o	Note that since the FrameNet data does not include deep syntactic tree annotation we processed the FrameNet data with Collins parser -LRB- Collins 1997 -RRB- consequently the experiments on FrameNet relate to automatic syntactic parse trees	nn_trees_parse amod_trees_syntactic amod_trees_automatic prep_to_relate_trees nsubj_relate_experiments prep_on_experiments_FrameNet det_experiments_the amod_Collins_1997 appos_parser_Collins nn_parser_Collins nn_data_FrameNet det_data_the parataxis_processed_relate advmod_processed_consequently prep_with_processed_parser dobj_processed_data nsubj_processed_we nn_annotation_tree amod_annotation_syntactic amod_annotation_deep dobj_include_annotation neg_include_not aux_include_does nsubj_include_data mark_include_since nn_data_FrameNet det_data_the parataxis_Note_processed advcl_Note_include dobj_Note_that
E09-1011	P97-1003	o	We parse the data using the Collins Parser -LRB- Collins 1997 -RRB- and then tag person location and organization names using the Stanford Named Entity Recognizer -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_Recognizer_Finkel nn_Recognizer_Entity dobj_Named_Recognizer det_Stanford_the dobj_using_Stanford nn_names_organization conj_and_person_Named vmod_person_using conj_and_person_names conj_and_person_location nn_person_tag advmod_person_then dep_Collins_1997 appos_Parser_Collins nn_Parser_Collins det_Parser_the dobj_using_Parser vmod_data_using det_data_the conj_and_parse_Named conj_and_parse_names conj_and_parse_location conj_and_parse_person dobj_parse_data nsubj_parse_We ccomp_``_person ccomp_``_parse
E09-1092	P97-1003	o	-LRB- -LRB- I -LRB- Q DET NAMED-ENTITY -RRB- ENTER -LSB- V -RSB- -LRB- Q THE ROOM -LSB- N -RSB- -RRB- -RRB- -LRB- I -LRB- Q DET FEMALE-INDIVIDUAL -RRB- HAVE -LSB- V -RSB- -LRB- Q DET ROOM -LSB- N -RSB- -RRB- -RRB- -LRB- I -LRB- Q DET FEMALE-INDIVIDUAL -RRB- SLEEP -LSB- V -RSB- -RRB- -LRB- I -LRB- Q DET FEMALE-INDIVIDUAL -RRB- HAVE -LSB- V -RSB- -LRB- Q DET -LRB- F PLUR CLOTHE -LSB- N -RSB- -RRB- -RRB- -RRB- -LRB- I -LRB- Q DET -LRB- F PLUR CLOTHE -LSB- N -RSB- -RRB- -RRB- WASHED -LSB- A -RSB- -RRB- -RRB- Here the upper-case sentences are automatically generated verbalizations of the abstracted LFs shown beneath them .1 The initial development of KNEXT was based on the hand-constructed parse trees in the Penn Treebank version of the Brown corpus but subsequently Schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers -LRB- e.g. that of Collins -LRB- 1997 -RRB- or Charniak -LRB- 2000 -RRB- -RRB- applied to larger corpora such as the British National Corpus -LRB- BNC -RRB- a 100 million-word mixed genre collection along with Web corpora of comparable size -LRB- see work of Van Durme et al.	nn_al._et dep_Durme_al. nn_Durme_Van prep_of_work_Durme dobj_see_work amod_size_comparable prep_of_corpora_size nn_corpora_Web nn_collection_genre amod_collection_mixed amod_collection_million-word num_collection_100 det_collection_a appos_Corpus_BNC nn_Corpus_National nn_Corpus_British det_Corpus_the dep_corpora_see pobj_corpora_corpora prepc_along_with_corpora_with appos_corpora_collection prep_such_as_corpora_Corpus amod_corpora_larger prep_to_applied_corpora appos_Charniak_2000 conj_or_Collins_Charniak appos_Collins_1997 prep_of_that_Charniak prep_of_that_Collins dep_e.g._that vmod_parsers_applied dep_parsers_e.g. amod_parsers_statistical prep_with_obtained_parsers vmod_trees_obtained nn_trees_parse prep_with_work_trees aux_work_to vmod_system_work det_system_the conj_and_refined_extended dep_Schubert_system dep_Schubert_extended dep_Schubert_refined conj_and_Schubert_collaborators advmod_Schubert_subsequently amod_corpus_Brown det_corpus_the prep_of_version_corpus nn_version_Treebank nn_version_Penn det_version_the prep_in_trees_version nn_trees_parse amod_trees_hand-constructed det_trees_the prep_on_based_trees auxpass_based_was nsubjpass_based_development dep_based_them mark_based_beneath prep_of_development_KNEXT amod_development_initial det_development_The num_development_.1 advcl_shown_based vmod_LFs_shown amod_LFs_abstracted det_LFs_the prep_of_verbalizations_LFs dobj_generated_verbalizations advmod_generated_automatically auxpass_generated_are nsubjpass_generated_sentences amod_sentences_upper-case det_sentences_the dep_WASHED_A dep_WASHED_V appos_CLOTHE_N nn_CLOTHE_PLUR nn_CLOTHE_F dep_DET_CLOTHE nn_DET_Q num_DET_I appos_CLOTHE_N nn_CLOTHE_PLUR nn_CLOTHE_F dep_DET_CLOTHE nn_DET_Q dep_HAVE_DET appos_HAVE_V num_HAVE_I nn_FEMALE-INDIVIDUAL_DET nn_FEMALE-INDIVIDUAL_Q appos_I_FEMALE-INDIVIDUAL appos_SLEEP_V num_SLEEP_I nn_FEMALE-INDIVIDUAL_DET nn_FEMALE-INDIVIDUAL_Q appos_I_FEMALE-INDIVIDUAL appos_ROOM_N nn_ROOM_DET nn_ROOM_Q dep_HAVE_ROOM appos_HAVE_V num_HAVE_I dep_HAVE_Q nn_FEMALE-INDIVIDUAL_DET nn_FEMALE-INDIVIDUAL_Q appos_I_FEMALE-INDIVIDUAL appos_ROOM_N det_ROOM_THE dep_Q_ROOM dep_V_DET dep_V_HAVE dep_V_SLEEP dep_V_HAVE conj_but_ENTER_collaborators conj_but_ENTER_Schubert ccomp_ENTER_generated advmod_ENTER_Here dep_ENTER_WASHED nsubj_ENTER_I nn_NAMED-ENTITY_DET nn_NAMED-ENTITY_Q appos_I_NAMED-ENTITY
E99-1007	P97-1003	o	First for each verb occurrence subjects and objects were extracted from a parsed corpus -LRB- Collins 1997 -RRB-	num_Collins_1997 appos_corpus_Collins amod_corpus_parsed det_corpus_a prep_from_extracted_corpus auxpass_extracted_were nsubjpass_extracted_verb conj_and_subjects_objects nn_subjects_occurrence dobj_verb_objects dobj_verb_subjects rcmod_each_extracted pobj_for_each ccomp_,_for dep_``_First
E99-1007	P97-1003	o	For causativity the same counting scripts were used for both groups of verbs but the input to the counting programs was determined by manual inspection of the corpus for verbs belonging to group 1 while it was extracted automatically from a parsed corpus for group 2 -LRB- WSJ 1988 parsed with the parser from -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_from_Collins prep_parser_from det_parser_the prep_with_parsed_parser vmod_WSJ_parsed num_WSJ_1988 dep_group_WSJ num_group_2 amod_corpus_parsed det_corpus_a prep_for_extracted_group prep_from_extracted_corpus advmod_extracted_automatically auxpass_extracted_was nsubjpass_extracted_it mark_extracted_while num_group_1 prep_to_belonging_group vmod_verbs_belonging det_corpus_the prep_for_inspection_verbs prep_of_inspection_corpus amod_inspection_manual advcl_determined_extracted agent_determined_inspection auxpass_determined_was nsubjpass_determined_input nn_programs_counting det_programs_the prep_to_input_programs det_input_the prep_of_groups_verbs det_groups_both conj_but_used_determined prep_for_used_groups auxpass_used_were nsubjpass_used_scripts prep_for_used_causativity nn_scripts_counting amod_scripts_same det_scripts_the
H05-1009	P97-1003	o	Dependency relations are produced using a version of the Collins parser -LRB- Collins 1997 -RRB- that has been adapted for building dependencies	dobj_building_dependencies prepc_for_adapted_building auxpass_adapted_been aux_adapted_has nsubjpass_adapted_that amod_Collins_1997 rcmod_parser_adapted appos_parser_Collins nn_parser_Collins det_parser_the prep_of_version_parser det_version_a dobj_using_version xcomp_produced_using auxpass_produced_are nsubjpass_produced_relations nn_relations_Dependency
H05-1024	P97-1003	o	In our experiments we used a dependency parser only in English -LRB- a version of the Collins parser -LRB- Collins 1997 -RRB- that has been adapted for building dependencies -RRB- but not in the other language	amod_language_other det_language_the dobj_building_dependencies prepc_for_adapted_building auxpass_adapted_been aux_adapted_has nsubjpass_adapted_that amod_Collins_1997 rcmod_parser_adapted appos_parser_Collins nn_parser_Collins det_parser_the prep_of_version_parser det_version_a conj_but_English_language dep_English_version nn_parser_dependency det_parser_a prep_in_used_language neg_used_not prep_in_used_English advmod_used_only dobj_used_parser nsubj_used_we prep_in_used_experiments poss_experiments_our
H05-1044	P97-1003	o	The modify features involve the dependency parse tree for the sentence obtained by first parsing the sentence -LRB- Collins 1997 -RRB- and then converting the tree into its dependency representation -LRB- Xia and Palmer 2001 -RRB-	dep_Xia_2001 conj_and_Xia_Palmer dep_representation_Palmer dep_representation_Xia nn_representation_dependency poss_representation_its det_tree_the prep_into_converting_representation dobj_converting_tree advmod_converting_then amod_Collins_1997 dep_sentence_Collins det_sentence_the dobj_parsing_sentence dep_first_parsing agent_obtained_first det_sentence_the conj_and_tree_converting vmod_tree_obtained prep_for_tree_sentence amod_tree_parse nn_tree_dependency det_dependency_the dobj_involve_converting dobj_involve_tree nsubj_involve_features dep_involve_modify dep_involve_The
H05-1091	P97-1003	o	In our experiments we used the full parse output from Collins parser -LRB- Collins 1997 -RRB- in which every non-terminal node is already annotated with head information	nn_information_head prep_with_annotated_information advmod_annotated_already cop_annotated_is nsubj_annotated_node prep_in_annotated_which amod_node_non-terminal det_node_every dep_Collins_1997 rcmod_parser_annotated appos_parser_Collins nn_parser_Collins amod_output_parse amod_output_full det_output_the prep_from_used_parser dobj_used_output nsubj_used_we prep_in_used_experiments poss_experiments_our
H05-1099	P97-1003	o	The output of a contextfree parser such as that of Collins -LRB- 1997 -RRB- or Charniak -LRB- 2000 -RRB- can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser	amod_parser_shallow det_parser_a prep_of_output_parser det_output_the prep_with_comparison_output amod_constituents_shallow prep_of_sequence_constituents det_sequence_a prep_for_transformed_comparison prep_into_transformed_sequence auxpass_transformed_be aux_transformed_can nsubjpass_transformed_output appos_Charniak_2000 conj_or_Collins_Charniak appos_Collins_1997 prep_of_that_Charniak prep_of_that_Collins prep_such_as_parser_that nn_parser_contextfree det_parser_a prep_of_output_parser det_output_The ccomp_``_transformed
H05-1099	P97-1003	p	To compare the output of their shallow parser with the output of the well-known Collins -LRB- 1997 -RRB- parser Li and Roth applied the chunklink conversion script to extract the shallow constituents from the output of the Collins parser on WSJ section 00	num_section_00 nn_section_WSJ nn_parser_Collins det_parser_the prep_of_output_parser det_output_the amod_constituents_shallow det_constituents_the prep_on_extract_section prep_from_extract_output dobj_extract_constituents aux_extract_to nn_script_conversion amod_script_chunklink det_script_the vmod_applied_extract dobj_applied_script csubj_applied_compare conj_and_parser_Roth conj_and_parser_Li nn_parser_Collins appos_Collins_1997 amod_Collins_well-known det_Collins_the prep_of_output_Roth prep_of_output_Li prep_of_output_parser det_output_the amod_parser_shallow poss_parser_their prep_of_output_parser det_output_the prep_with_compare_output dobj_compare_output aux_compare_To
H05-1099	P97-1003	o	Perhaps the most widely accepted convention is that of ignoring punctuation for the purposes of assigning constituent span under the perspective that fun788 Phrase Evaluation Scenario System Type -LRB- a -RRB- -LRB- b -RRB- -LRB- c -RRB- Modified All 98.37 99.72 99.72 Truth VP 92.14 98.70 98.70 Li and Roth All 94.64 -LRB- 2001 -RRB- VP 95.28 Collins -LRB- 1997 -RRB- All 92.16 93.42 94.28 VP 88.15 94.31 94.42 Charniak All 93.88 95.15 95.32 -LRB- 2000 -RRB- VP 88.92 95.11 95.19 Table 1 F-measure shallow bracketing accuracy under three different evaluation scenarios -LRB- a -RRB- baseline used in Li and Roth -LRB- 2001 -RRB- with original chunklink script converting treebank trees and context-free parser output -LRB- b -RRB- same as -LRB- a -RRB- except that empty subject NPs are inserted into every unary SVP production and -LRB- c -RRB- same as -LRB- b -RRB- except that punctuation is ignored for setting constituent span	amod_span_constituent dobj_setting_span prepc_for_ignored_setting auxpass_ignored_is nsubjpass_ignored_punctuation mark_ignored_except det_punctuation_that dep_as_b prep_same_as dep_same_c nn_production_SVP amod_production_unary det_production_every prep_into_inserted_production auxpass_inserted_are nsubjpass_inserted_NPs mark_inserted_that mark_inserted_except amod_NPs_subject amod_NPs_empty advcl_a_ignored conj_and_a_same advcl_a_inserted dep_as_same dep_as_a prep_same_as dep_same_b dep_same_baseline dep_same_a nn_output_parser amod_output_context-free conj_and_trees_output nn_trees_treebank dobj_converting_output dobj_converting_trees vmod_script_converting amod_script_chunklink amod_script_original appos_Li_2001 conj_and_Li_Roth prep_in_used_Roth prep_in_used_Li prep_with_baseline_script vmod_baseline_used nn_scenarios_evaluation amod_scenarios_different num_scenarios_three prep_under_accuracy_scenarios amod_accuracy_bracketing amod_accuracy_shallow nn_accuracy_F-measure num_Table_1 num_Table_95.19 num_Table_95.11 num_Table_88.92 dep_VP_Table nn_VP_All nn_VP_Charniak num_VP_94.42 num_95.32_95.15 number_95.15_93.88 appos_All_2000 num_All_95.32 dep_94.31_VP dep_88.15_94.31 dep_VP_88.15 dep_94.28_VP dep_93.42_accuracy dep_93.42_94.28 number_93.42_92.16 dep_All_93.42 appos_Collins_1997 num_Collins_95.28 nn_Collins_VP nn_Collins_All appos_All_2001 num_All_94.64 nn_All_Roth conj_and_Li_Collins num_Li_98.70 nn_Li_VP nn_Li_Truth num_Li_99.72 det_Li_All number_98.70_98.70 number_98.70_92.14 number_99.72_99.72 number_99.72_98.37 dep_Modified_All dobj_Modified_Collins dobj_Modified_Li dep_Type_same vmod_Type_Modified appos_Type_c appos_Type_b appos_Type_a nn_Type_System nn_Type_Scenario nn_Type_Evaluation nn_Type_Phrase nn_Type_fun788 dep_that_Type dep_perspective_that det_perspective_the prep_under_,_perspective amod_span_constituent amod_span_assigning prep_of_purposes_span det_purposes_the prep_for_ignoring_purposes dobj_ignoring_punctuation pcomp_of_ignoring dep_that_of prep_is_that nsubj_is_convention advmod_is_Perhaps amod_convention_accepted det_convention_the advmod_accepted_widely advmod_widely_most
H05-1105	P97-1003	o	-LRB- Collins parser -LRB- Collins 1997 -RRB- always predicts a flat NP for such configurations -RRB-	amod_configurations_such prep_for_NP_configurations amod_NP_flat det_NP_a dobj_predicts_NP advmod_predicts_always nsubj_predicts_parser amod_Collins_1997 appos_parser_Collins nn_parser_Collins
H05-1108	P97-1003	o	For the constituent-based models constituent information was obtained from the output of Collins parser -LRB- 1997 -RRB- for English and Dubeys parser -LRB- 2004 -RRB- for German	appos_parser_2004 nn_parser_Dubeys conj_and_English_parser prep_for_parser_parser prep_for_parser_English appos_parser_1997 nn_parser_Collins prep_for_output_German prep_of_output_parser det_output_the prep_from_obtained_output auxpass_obtained_was nsubjpass_obtained_information prep_for_obtained_models amod_information_constituent amod_models_constituent-based det_models_the
H05-2018	P97-1003	o	In batch mode OpinionFinder parses the data again this time to obtain constituency parse trees -LRB- Collins 1997 -RRB- which are then converted to dependency parse trees -LRB- Xia and Palmer 2001 -RRB-	dep_Xia_2001 conj_and_Xia_Palmer appos_trees_Palmer appos_trees_Xia nn_trees_parse nn_trees_dependency prep_to_converted_trees advmod_converted_then auxpass_converted_are nsubjpass_converted_which amod_Collins_1997 nn_trees_parse nn_trees_constituency dobj_obtain_trees aux_obtain_to rcmod_time_converted dep_time_Collins vmod_time_obtain det_time_this det_data_the dobj_parses_time advmod_parses_again dobj_parses_data nsubj_parses_OpinionFinder prep_in_parses_mode nn_mode_batch
J01-1004	P97-1003	o	recent advances in parsing technology are due to the explicit stochastic modeling of dependency information -LRB- Collins 1997 -RRB-	num_Collins_1997 nn_information_dependency prep_of_modeling_information amod_modeling_stochastic amod_modeling_explicit det_modeling_the dep_due_Collins prep_to_due_modeling cop_due_are nsubj_due_advances nn_technology_parsing prep_in_advances_technology amod_advances_recent
J01-2004	P97-1003	o	Top-Down Parsing and Language Modeling Statistically based heuristic best-first or beam-search strategies -LRB- Caraballo and Charniak 1998 Charniak Goldwater and Johnson 1998 Goodman 1997 -RRB- have yielded an enormous improvement in the quality and speed of parsers even without any guarantee that the parse returned is in fact that with the maximum likelihood for the probability model	nn_model_probability det_model_the prep_for_likelihood_model nn_likelihood_maximum det_likelihood_the prep_with_that_likelihood ccomp_,_that pobj_in_fact dep_,_in dep_returned_is nsubj_returned_parse mark_returned_that det_parse_the ccomp_guarantee_returned det_guarantee_any prep_of_quality_parsers conj_and_quality_speed det_quality_the prep_without_improvement_guarantee advmod_improvement_even prep_in_improvement_speed prep_in_improvement_quality amod_improvement_enormous det_improvement_an dobj_yielded_improvement aux_yielded_have nsubj_yielded_Modeling nsubj_yielded_Parsing num_Goodman_1997 num_Johnson_1998 nn_Goldwater_Charniak num_Charniak_1998 dep_Caraballo_Goodman conj_and_Caraballo_Johnson conj_and_Caraballo_Goldwater conj_and_Caraballo_Charniak amod_strategies_beam-search appos_best-first_Johnson appos_best-first_Goldwater appos_best-first_Charniak appos_best-first_Caraballo conj_or_best-first_strategies nn_best-first_heuristic dep_based_strategies dep_based_best-first advmod_based_Statistically nn_Modeling_Language vmod_Parsing_based conj_and_Parsing_Modeling amod_Parsing_Top-Down
J01-2004	P97-1003	p	The parsers with the highest published broad-coverage parsing accuracy which include Charniak -LRB- 1997 2000 -RRB- Collins -LRB- 1997 1999 -RRB- and Ratnaparkhi -LRB- 1997 -RRB- all utilize simple and straightforward statistically based search heuristics pruning the search-space quite dramatically	advmod_dramatically_quite advmod_search-space_dramatically det_search-space_the dep_pruning_search-space appos_heuristics_pruning nn_heuristics_search amod_heuristics_based amod_heuristics_straightforward amod_heuristics_simple advmod_based_statistically conj_and_simple_straightforward dobj_utilize_heuristics nsubj_utilize_all nsubj_utilize_parsers appos_Ratnaparkhi_1997 dep_1997_1999 dep_Collins_1997 dep_1997_2000 conj_and_Charniak_Ratnaparkhi conj_and_Charniak_Collins dep_Charniak_1997 dobj_include_Ratnaparkhi dobj_include_Collins dobj_include_Charniak nsubj_include_which rcmod_accuracy_include nn_accuracy_parsing amod_accuracy_broad-coverage amod_accuracy_published amod_accuracy_highest det_accuracy_the prep_with_parsers_accuracy det_parsers_The
J01-2004	P97-1003	o	It has been shown repeatedly -- e.g. Briscoe and Carroll -LRB- 1993 -RRB- Charniak -LRB- 1997 -RRB- Collins -LRB- 1997 -RRB- Inui et al.	nn_al._et nn_al._Inui dep_Collins_al. appos_Collins_1997 appos_Charniak_1997 appos_Carroll_1993 conj_and_e.g._Collins conj_and_e.g._Charniak conj_and_e.g._Carroll conj_and_e.g._Briscoe dep_shown_Collins dep_shown_Charniak dep_shown_Carroll dep_shown_Briscoe dep_shown_e.g. advmod_shown_repeatedly auxpass_shown_been aux_shown_has nsubjpass_shown_It
J01-2004	P97-1003	o	-LRB- 1997 -RRB- Johnson -LRB- 1998 -RRB- -- that conditioning the probabilities of structures on the context within which they appear for example on the lexical head of a constituent -LRB- Charniak 1997 Collins 1997 -RRB- on the label of its parent nonterrninal -LRB- Johnson 1998 -RRB- or ideally on both and many other things besides leads to a much better parsing model and results in higher parsing accuracies	nn_accuracies_parsing amod_accuracies_higher prep_in_results_accuracies nn_model_parsing amod_model_better det_model_a advmod_better_much prep_to_leads_model prep_on_leads_things advmod_leads_ideally dep_leads_parent nsubj_leads_its advmod_leads_of prep_things_besides amod_things_other preconj_things_many preconj_things_both conj_and_both_many num_Johnson_1998 appos_nonterrninal_Johnson cc_parent_or dep_parent_nonterrninal dep_label_leads det_label_the conj_and_on_results pobj_on_label ccomp_,_results ccomp_,_on num_Collins_1997 dep_1997_Collins amod_1997_Charniak dep_constituent_1997 det_constituent_a prep_of_head_constituent amod_head_lexical det_head_the prep_on_example_head pobj_for_example dep_,_for nsubj_appear_they prep_within_appear_which rcmod_context_appear det_context_the prep_on_probabilities_context prep_of_probabilities_structures det_probabilities_the dobj_conditioning_probabilities dep_that_conditioning dep_--_that appos_Johnson_1998 dep_,_Johnson dep_''_1997
J01-2004	P97-1003	o	Examples of this are bilexical grammars -- such as Eisner and Satta -LRB- 1999 -RRB- Charniak -LRB- 1997 -RRB- Collins -LRB- 1997 -RRB- -- where the lexical heads of each constituent are annotated on both the rightand left-hand sides of the context-free rules under the constraint that every constituent inherits the lexical head from exactly one of its children and the lexical head of a POS is its terminal item	amod_item_terminal poss_item_its cop_item_is nsubj_item_head det_POS_a prep_of_head_POS amod_head_lexical det_head_the poss_children_its prep_of_one_children advmod_one_exactly amod_head_lexical det_head_the conj_and_inherits_item prep_from_inherits_one dobj_inherits_head nsubj_inherits_constituent mark_inherits_that det_constituent_every ccomp_constraint_item ccomp_constraint_inherits det_constraint_the amod_rules_context-free det_rules_the prep_of_sides_rules amod_sides_left-hand amod_sides_rightand det_sides_the preconj_sides_both prep_under_annotated_constraint prep_on_annotated_sides cop_annotated_are nsubj_annotated_heads advmod_annotated_where det_constituent_each prep_of_heads_constituent amod_heads_lexical det_heads_the appos_Collins_1997 appos_Charniak_1997 appos_Satta_1999 rcmod_Eisner_annotated conj_and_Eisner_Collins conj_and_Eisner_Charniak conj_and_Eisner_Satta prep_such_as_grammars_Collins prep_such_as_grammars_Charniak prep_such_as_grammars_Satta prep_such_as_grammars_Eisner amod_grammars_bilexical cop_grammars_are nsubj_grammars_Examples prep_of_Examples_this
J01-2004	P97-1003	o	The differences between a k-best and a beam-search parser -LRB- not to mention the use of dynamic programming -RRB- make a running time difference unsur17 Our score of 85.8 average labeled precision and recall for sentences less than or equal to 100 on Section 23 compares to 86.7 in Charniak -LRB- 1997 -RRB- 86.9 in Ratnaparkhi -LRB- 1997 -RRB- 88.2 in Collins -LRB- 1999 -RRB- 89.6 in Charniak -LRB- 2000 -RRB- and 89.75 in Collins -LRB- 2000 -RRB-	appos_Collins_2000 appos_Charniak_2000 prep_in_89.6_Charniak appos_Collins_89.6 appos_Collins_1999 prep_in_88.2_Collins prep_in_Ratnaparkhi_Collins conj_and_Ratnaparkhi_89.75 appos_Ratnaparkhi_88.2 appos_Ratnaparkhi_1997 prep_in_86.9_89.75 prep_in_86.9_Ratnaparkhi appos_Charniak_86.9 appos_Charniak_1997 prep_in_86.7_Charniak dep_to_86.7 prep_compares_to nsubj_compares_recall nsubj_compares_score num_Section_23 pobj_to_100 pcomp_than_to conj_or_than_equal prep_less_equal prep_less_than amod_sentences_less prep_on_recall_Section prep_for_recall_sentences amod_precision_labeled amod_precision_average num_precision_85.8 conj_and_score_recall prep_of_score_precision poss_score_Our amod_score_unsur17 nn_score_difference nn_score_time amod_score_running det_score_a ccomp_make_compares nsubj_make_differences amod_programming_dynamic prep_of_use_programming det_use_the dobj_mention_use aux_mention_to neg_mention_not dep_parser_mention amod_parser_beam-search det_parser_a conj_and_k-best_parser det_k-best_a prep_between_differences_parser prep_between_differences_k-best det_differences_The
J01-3003	P97-1003	o	This corpus of 29 million words was provided to us by Michael Collins and was automatically parsed with the parser described in Collins -LRB- 1997 -RRB-	appos_Collins_1997 prep_in_described_Collins vmod_parser_described det_parser_the prep_with_parsed_parser advmod_parsed_automatically auxpass_parsed_was nsubjpass_parsed_corpus nn_Collins_Michael conj_and_provided_parsed agent_provided_Collins prep_to_provided_us auxpass_provided_was nsubjpass_provided_corpus num_words_million number_million_29 prep_of_corpus_words det_corpus_This
J02-2002	P97-1003	o	Statistical techniques developed for lexicalized grammars -LRB- e.g. Collins 1997 -RRB- readily apply to CCG to improve the average parsing performance in large-scale practical applications -LRB- Hockenmaier Bierner and Baldridge 2000 -RRB-	num_Baldridge_2000 conj_and_Hockenmaier_Baldridge conj_and_Hockenmaier_Bierner dep_applications_Baldridge dep_applications_Bierner dep_applications_Hockenmaier amod_applications_practical amod_applications_large-scale prep_in_performance_applications nn_performance_parsing amod_performance_average det_performance_the dobj_improve_performance aux_improve_to xcomp_apply_improve prep_to_apply_CCG advmod_apply_readily nsubj_apply_techniques num_Collins_1997 dep_e.g._Collins dep_grammars_e.g. amod_grammars_lexicalized prep_for_developed_grammars vmod_techniques_developed amod_techniques_Statistical
J02-3001	P97-1003	o	We used Collins -LRB- 1997 -RRB- statistical parser trained on examples from the Penn Treebank to generate parses of the same format for the sentences in our data	poss_data_our prep_in_sentences_data det_sentences_the prep_for_format_sentences amod_format_same det_format_the prep_of_parses_format dobj_generate_parses aux_generate_to nn_Treebank_Penn det_Treebank_the vmod_examples_generate prep_from_examples_Treebank prep_on_trained_examples vmod_parser_trained amod_parser_statistical num_parser_1997 dep_Collins_parser dobj_used_Collins nsubj_used_We
J02-3001	P97-1003	o	Collins -LRB- 1997 -RRB- reports 88 % labeled precision and recall on individual parse constituents on data from the Penn Treebank roughly consistent with our finding of at least 13 % error	nn_error_% num_error_13 quantmod_13_at mwe_at_least prep_of_finding_error poss_finding_our prep_with_consistent_finding advmod_consistent_roughly nn_Treebank_Penn det_Treebank_the prep_from_data_Treebank nn_constituents_parse amod_constituents_individual conj_and_precision_recall prep_on_labeled_data prep_on_labeled_constituents dep_labeled_recall dep_labeled_precision vmod_%_labeled num_%_88 dep_reports_% dep_1997_consistent dep_1997_reports dep_Collins_1997
J02-3001	P97-1003	o	Recent lexicalized stochastic parsers such as Collins -LRB- 1999 -RRB- Charniak -LRB- 1997 -RRB- and others add additional features to each constituent the most important being the head word of the parse constituent	nn_constituent_parse det_constituent_the prep_of_word_constituent nsubj_word_head cop_word_being det_head_the ccomp_important_word advmod_important_most det_important_the det_constituent_each amod_features_additional xcomp_add_important prep_to_add_constituent dobj_add_features nsubj_add_parsers appos_Charniak_1997 conj_and_Collins_others conj_and_Collins_Charniak appos_Collins_1999 prep_such_as_parsers_others prep_such_as_parsers_Charniak prep_such_as_parsers_Collins amod_parsers_stochastic amod_parsers_lexicalized amod_parsers_Recent
J03-4003	P97-1003	o	7 In the models described in Collins -LRB- 1997 -RRB- there was a third question concerning punctuation -LRB- 3 -RRB- Does the string contain 0 1 2 or more than 2 commas ?	num_commas_2 num_commas_2 num_commas_1 num_commas_0 quantmod_2_than mwe_than_more conj_or_0_2 conj_or_0_2 conj_or_0_1 dobj_contain_commas nsubj_contain_string det_string_the ccomp_Does_contain dep_Does_3 dobj_concerning_punctuation vmod_question_concerning amod_question_third det_question_a nsubj_was_question expl_was_there prep_in_was_models appos_Collins_1997 prep_in_described_Collins vmod_models_described det_models_the dep_7_Does rcmod_7_was ccomp_``_7
J03-4003	P97-1003	o	The models were originally introduced in Collins -LRB- 1997 -RRB- the current article 1 gives considerably more detail about the models and discusses them in greater depth	amod_depth_greater prep_in_discusses_depth dobj_discusses_them nsubj_discusses_article det_models_the advmod_detail_more advmod_more_considerably conj_and_gives_discusses prep_about_gives_models dobj_gives_detail nsubj_gives_article num_article_1 amod_article_current det_article_the appos_Collins_1997 parataxis_introduced_discusses parataxis_introduced_gives prep_in_introduced_Collins advmod_introduced_originally auxpass_introduced_were nsubjpass_introduced_models det_models_The
J03-4003	P97-1003	o	Previous workthe generative models described in Collins -LRB- 1996 -RRB- and the earlier version of these models described in Collins -LRB- 1997 -RRB- conditioned on punctuation as surface features of the string treating it quite differently from lexical items	amod_items_lexical advmod_differently_quite prep_from_treating_items advmod_treating_differently dobj_treating_it ccomp_,_treating det_string_the prep_of_features_string nn_features_surface prep_as_conditioned_features prep_on_conditioned_punctuation vmod_Collins_conditioned appos_Collins_1997 prep_in_described_Collins vmod_models_described det_models_these prep_of_version_models amod_version_earlier det_version_the conj_and_Collins_version appos_Collins_1996 prep_in_described_version prep_in_described_Collins vmod_models_described nn_models_generative nn_models_workthe amod_models_Previous dep_``_models
J03-4003	P97-1003	n	In particular the model in Collins -LRB- 1997 -RRB- failed to generate punctuation a deficiency of the model	det_model_the prep_of_deficiency_model det_deficiency_a appos_punctuation_deficiency dobj_generate_punctuation aux_generate_to xcomp_failed_generate nsubj_failed_model prep_in_failed_particular appos_Collins_1997 prep_in_model_Collins det_model_the
J03-4003	P97-1003	o	Goodman -LRB- 1997 -RRB- and Johnson -LRB- 1997 -RRB- both suggest this strategy	det_strategy_this dobj_suggest_strategy preconj_suggest_both nsubj_suggest_Johnson nsubj_suggest_Goodman appos_Johnson_1997 conj_and_Goodman_Johnson appos_Goodman_1997
J03-4003	P97-1003	o	Johnson -LRB- 1997 -RRB- considers conversion to a number of different representations and discusses how this influences accuracy for nonlexicalized PCFGs	amod_PCFGs_nonlexicalized prep_for_accuracy_PCFGs dobj_influences_accuracy nsubj_influences_this advmod_influences_how ccomp_discusses_influences nsubj_discusses_Johnson amod_representations_different prep_of_number_representations det_number_a prep_to_conversion_number conj_and_considers_discusses dobj_considers_conversion nsubj_considers_Johnson appos_Johnson_1997
J03-4003	P97-1003	o	-LRB- Johnson -LSB- 1997 -RSB- notes that this structure has a higher probability than the correct flat structure given counts taken from the treebank for a standard PCFG -RRB-	amod_PCFG_standard det_PCFG_a det_treebank_the prep_for_taken_PCFG prep_from_taken_treebank vmod_counts_taken amod_counts_given amod_structure_flat amod_structure_correct det_structure_the prep_than_probability_structure amod_probability_higher det_probability_a dobj_has_counts dobj_has_probability nsubj_has_structure mark_has_that det_structure_this ccomp_notes_has nsubj_notes_Johnson appos_Johnson_1997
J03-4003	P97-1003	o	Of particular relevance is other work on parsing the Penn WSJ Treebank -LRB- Jelinek et al. 1994 Magerman 1995 Eisner 1996a 1996b Collins 1996 Charniak 1997 Goodman 1997 Ratnaparkhi 1997 Chelba and Jelinek 1998 Roark 2001 -RRB-	num_Roark_2001 num_Jelinek_1998 conj_and_Chelba_Jelinek num_Ratnaparkhi_1997 num_Goodman_1997 num_Charniak_1997 num_Collins_1996 appos_1996a_1996b nn_1996a_Eisner num_Magerman_1995 num_al._1994 nn_al._et dep_Jelinek_Roark dep_Jelinek_Jelinek dep_Jelinek_Chelba dep_Jelinek_Ratnaparkhi dep_Jelinek_Goodman dep_Jelinek_Charniak dep_Jelinek_Collins dep_Jelinek_1996a dep_Jelinek_Magerman dep_Jelinek_al. appos_Treebank_Jelinek nn_Treebank_WSJ nn_Treebank_Penn det_Treebank_the dobj_parsing_Treebank prepc_on_work_parsing amod_work_other nsubj_is_work prep_of_is_relevance amod_relevance_particular
J03-4003	P97-1003	o	-LRB- Note that conditioning on the rules parent is needed to disallow the structure -LSB- NP -LSB- NP PP -RSB- PP -RSB- see Johnson -LSB- 1997 -RSB- for further discussion -RRB-	amod_discussion_further prep_for_Johnson_discussion appos_Johnson_1997 dobj_see_Johnson nn_PP_PP nn_PP_NP appos_NP_PP det_structure_the dobj_disallow_structure aux_disallow_to xcomp_needed_disallow auxpass_needed_is csubjpass_needed_conditioning nn_parent_rules det_parent_the prep_on_conditioning_parent nsubj_conditioning_that dep_Note_see dep_Note_NP ccomp_Note_needed
J04-3001	P97-1003	o	Moreover in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain we have performed the study on two parsing models one based on a context-free variant of tree-adjoining grammars -LRB- Joshi Levy and Takahashi 1975 -RRB- the Probabilistic Lexicalized Tree Insertion Grammar -LRB- PLTIG -RRB- formalism -LRB- Schabes and Waters 1993 Hwa 1998 -RRB- and Collinss Model 2 parser -LRB- 1997 -RRB-	appos_parser_1997 num_parser_2 nn_parser_Model nn_parser_Collinss num_Hwa_1998 num_Waters_1993 dep_Schabes_Hwa conj_and_Schabes_Waters appos_formalism_Waters appos_formalism_Schabes dep_Grammar_formalism appos_Grammar_PLTIG nn_Grammar_Insertion nn_Grammar_Tree nn_Grammar_Lexicalized nn_Grammar_Probabilistic det_Grammar_the num_Takahashi_1975 conj_and_Joshi_Takahashi conj_and_Joshi_Levy dep_grammars_Takahashi dep_grammars_Levy dep_grammars_Joshi amod_grammars_tree-adjoining prep_of_variant_grammars amod_variant_context-free det_variant_a prep_on_based_variant conj_and_one_parser conj_and_one_Grammar vmod_one_based nn_models_parsing num_models_two prep_on_study_models det_study_the dobj_performed_study aux_performed_have nsubj_performed_we amod_domain_same det_domain_the nn_models_learning amod_models_different prep_within_consistent_domain prep_across_consistent_models cop_consistent_are nsubj_consistent_performances mark_consistent_whether amod_criteria_predictive det_criteria_the prep_of_performances_criteria det_performances_the dep_determine_parser dep_determine_Grammar dep_determine_one parataxis_determine_performed ccomp_determine_consistent aux_determine_to dep_determine_order mark_determine_in advmod_determine_Moreover
J04-3001	P97-1003	o	In the second experiment the basic learning model is Collinss -LRB- 1997 -RRB- Model 2 parser which uses a history-based learning algorithm that takes statistics directly over the treebank	det_treebank_the prep_over_takes_treebank advmod_takes_directly dobj_takes_statistics nsubj_takes_that rcmod_algorithm_takes nn_algorithm_learning amod_algorithm_history-based det_algorithm_a dobj_uses_algorithm nsubj_uses_which rcmod_parser_uses num_parser_2 nn_parser_Model dep_parser_1997 dep_Collinss_parser cop_Collinss_is nsubj_Collinss_model prep_in_Collinss_experiment nn_model_learning amod_model_basic det_model_the amod_experiment_second det_experiment_the
J04-4004	P97-1003	n	Another consequence of not generating posthead conjunctions and punctuation as first-class words is that they 19 In fact if punctuation occurs before the head it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of Collins -LRB- 1997 -RRB-	appos_Collins_1997 prep_of_model_Collins det_model_the prep_in_handling_model nn_handling_punctuation amod_handling_deficient det_handling_the prep_from_holdover_handling det_holdover_a cop_holdover_be aux_holdover_to xcomp_appears_holdover nsubj_appears_that nn_model_parsing det_model_the prep_in_deficiency_model nn_deficiency_alla dep_generated_appears prep_at_generated_deficiency neg_generated_not auxpass_generated_is nsubjpass_generated_it advcl_generated_occurs prep_in_generated_fact dep_generated_19 nsubjpass_generated_they mark_generated_that det_head_the prep_before_occurs_head nsubj_occurs_punctuation mark_occurs_if ccomp_is_generated nsubj_is_consequence amod_words_first-class conj_and_conjunctions_punctuation amod_conjunctions_posthead prep_as_generating_words dobj_generating_punctuation dobj_generating_conjunctions neg_generating_not prepc_of_consequence_generating det_consequence_Another
J04-4004	P97-1003	o	503 Bikel Intricacies of Collins Parsing Model Table 4 Overall parsing results using only details found in Collins -LRB- 1997 1999 -RRB-	dep_1997_1999 dep_Collins_1997 prep_in_found_Collins vmod_details_found advmod_details_only dobj_using_details xcomp_results_using nsubj_results_Intricacies amod_parsing_Overall num_parsing_4 nn_parsing_Table nn_parsing_Model nn_parsing_Parsing nn_parsing_Collins prep_of_Intricacies_parsing nn_Intricacies_Bikel num_Intricacies_503
J04-4004	P97-1003	o	Evaluation 8.1 Effects of Unpublished Details In this section we present the results of effectively doing a clean-room implementation of Collins parsing model that is using only information available in -LRB- Collins 1997 1999 -RRB- as shown in Table 4	num_Table_4 prep_in_shown_Table mark_shown_as advcl_,_shown amod_Collins_1999 num_Collins_1997 prep_in_available_Collins amod_information_available advmod_information_only dobj_using_information ccomp_,_using dep_that_is amod_model_parsing nn_model_Collins prep_of_implementation_model amod_implementation_clean-room det_implementation_a dobj_doing_implementation advmod_doing_effectively prepc_of_results_doing det_results_the dobj_present_results nsubj_present_we det_section_this prep_in_Details_section nn_Details_Unpublished dep_Effects_that rcmod_Effects_present prep_of_Effects_Details num_Effects_8.1 nn_Effects_Evaluation
J04-4004	P97-1003	p	Introduction Michael Collins -LRB- 1996 1997 1999 -RRB- parsing models have been quite influential in the field of natural language processing	nn_processing_language amod_processing_natural prep_of_field_processing det_field_the prep_in_influential_field advmod_influential_quite cop_influential_been aux_influential_have nsubj_influential_models amod_models_parsing dep_models_1999 nn_models_Collins number_1999_1997 num_1999_1996 nn_Collins_Michael nn_Collins_Introduction
J05-1003	P97-1003	o	Collins and Koo Discriminative Reranking for NLP Della Pietra 1996 Della Pietra Della Pietra and Lafferty 1997 -RRB- or conjugate gradient methods -LRB- Malouf 2002 -RRB-	num_Malouf_2002 appos_methods_Malouf nn_methods_gradient nn_methods_conjugate num_Lafferty_1997 nn_Pietra_Della conj_or_Pietra_methods conj_and_Pietra_Lafferty appos_Pietra_Pietra nn_Pietra_Della num_Pietra_1996 nn_Pietra_Della nn_Pietra_NLP nn_Reranking_Discriminative nn_Reranking_Koo dep_Collins_methods dep_Collins_Lafferty dep_Collins_Pietra prep_for_Collins_Pietra conj_and_Collins_Reranking
J05-1003	P97-1003	o	-LRB- 1998 -RRB- we also introduce an approach related to the conditional log-linear models of Ratnaparkhi Roukos and Ward -LRB- 1994 -RRB- Papineni Roukos and Ward -LRB- 1997 1998 -RRB- Johnson et al.	nn_al._et nn_al._Johnson dep_1997_1998 dep_Ward_1997 appos_Ward_Roukos appos_Ward_Papineni appos_Ward_1994 conj_and_Ratnaparkhi_Ward conj_and_Ratnaparkhi_Roukos prep_of_models_Ward prep_of_models_Roukos prep_of_models_Ratnaparkhi amod_models_log-linear amod_models_conditional det_models_the prep_to_related_models amod_approach_related det_approach_an conj_and_introduce_Ward dobj_introduce_approach advmod_introduce_also nsubj_introduce_we dep_1998_al. parataxis_1998_Ward parataxis_1998_introduce dep_''_1998
J05-1003	P97-1003	o	In particular previous work -LRB- Ratnaparkhi Roukos and Ward 1994 Abney 1997 Della Pietra Della Pietra and Lafferty 1997 Johnson et al. 1999 Riezler et al. 2002 -RRB- has investigated the use of Markov random fields -LRB- MRFs -RRB- or log-linear models as probabilistic models with global features for parsing and other NLP tasks	nn_tasks_NLP amod_tasks_other conj_and_parsing_tasks prep_for_features_tasks prep_for_features_parsing amod_features_global prep_with_models_features amod_models_probabilistic amod_models_log-linear conj_or_fields_models appos_fields_MRFs amod_fields_random nn_fields_Markov prep_as_use_models prep_of_use_models prep_of_use_fields det_use_the dobj_investigated_use aux_investigated_has nsubj_investigated_work prep_in_investigated_particular dep_al._2002 nn_al._et nn_al._Riezler dep_al._1999 nn_al._et nn_al._Johnson num_Lafferty_1997 nn_Pietra_Della conj_and_Pietra_Lafferty conj_and_Pietra_Pietra nn_Pietra_Della num_Abney_1997 num_Ward_1994 dep_Ratnaparkhi_al. conj_and_Ratnaparkhi_al. conj_and_Ratnaparkhi_Lafferty conj_and_Ratnaparkhi_Pietra conj_and_Ratnaparkhi_Pietra conj_and_Ratnaparkhi_Abney conj_and_Ratnaparkhi_Ward conj_and_Ratnaparkhi_Roukos appos_work_al. appos_work_Pietra appos_work_Abney appos_work_Ward appos_work_Roukos appos_work_Ratnaparkhi amod_work_previous
J05-1003	P97-1003	p	First several of the best-performing parsers on the WSJ treebank -LRB- e.g. Ratnaparkhi 1997 Charniak 1997 2000 Collins 1997 1999 Henderson 2003 -RRB- are cases of history-based models	amod_models_history-based prep_of_cases_models cop_cases_are nsubj_cases_several advmod_cases_First num_Henderson_2003 num_Collins_1999 num_Collins_1997 num_Charniak_2000 num_Charniak_1997 num_Ratnaparkhi_1997 dep_e.g._Henderson dep_e.g._Collins dep_e.g._Charniak dep_e.g._Ratnaparkhi dep_treebank_e.g. nn_treebank_WSJ det_treebank_the prep_on_parsers_treebank amod_parsers_best-performing det_parsers_the prep_of_several_parsers
J05-3003	P97-1003	o	The extraction procedure utilizes a head percolation table as introduced by Magerman -LRB- 1995 -RRB- in combination with a variation of Collinss -LRB- 1997 -RRB- approach to the differentiation between complement and adjunct	conj_and_complement_adjunct prep_between_differentiation_adjunct prep_between_differentiation_complement det_differentiation_the prep_to_approach_differentiation nn_approach_Collinss appos_Collinss_1997 prep_of_variation_approach det_variation_a prep_with_combination_variation appos_Magerman_1995 prep_in_introduced_combination prep_by_introduced_Magerman mark_introduced_as dep_table_introduced nn_table_percolation nn_table_head det_table_a dobj_utilizes_table nsubj_utilizes_procedure nn_procedure_extraction det_procedure_The
J05-3003	P97-1003	o	The extraction procedure consists of three steps First the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman -LRB- 1994 -RRB- and Collins -LRB- 1997 -RRB-	appos_Collins_1997 conj_and_Magerman_Collins appos_Magerman_1994 prep_of_approaches_Collins prep_of_approaches_Magerman det_approaches_the prep_on_based_approaches vmod_extended_based nsubjpass_extended_First conj_and_corrected_extended auxpass_corrected_is nsubjpass_corrected_First nn_Treebank_Penn det_Treebank_the det_trees_the prep_in_bracketing_Treebank prep_of_bracketing_trees det_bracketing_the appos_First_bracketing dep_steps_extended dep_steps_corrected num_steps_three prep_of_consists_steps nsubj_consists_procedure nn_procedure_extraction det_procedure_The
J05-3003	P97-1003	o	The first step is to label each node as either a head complement or adjunct based on the approaches of Magerman -LRB- 1994 -RRB- and Collins -LRB- 1997 -RRB-	appos_Collins_1997 conj_and_Magerman_Collins appos_Magerman_1994 prep_of_approaches_Collins prep_of_approaches_Magerman det_approaches_the prep_based_on_head_approaches conj_or_head_adjunct conj_or_head_complement det_head_a preconj_head_either det_node_each prep_as_label_adjunct prep_as_label_complement prep_as_label_head dobj_label_node aux_label_to xcomp_is_label nsubj_is_step amod_step_first det_step_The ccomp_``_is
J05-3003	P97-1003	o	Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees Head/argument/modifier distinctions are made for each node in the tree based on Magerman -LRB- 1994 -RRB- and Collins -LRB- 1997 -RRB- 336 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources the whole tree is then converted to a binary tree heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category	nn_category_CFG poss_category_its det_tree_the det_node_each prep_in_accordance_with_assigned_category prep_in_assigned_tree prep_to_assigned_node auxpass_assigned_is nsubjpass_assigned_category nn_category_HPSG det_category_an advmod_category_finally det_treebank_the prep_in_errors_treebank det_errors_some dobj_correct_errors aux_correct_to nsubj_correct_heuristics conj_and_LDDs_coordination prep_such_as_phenomena_coordination prep_such_as_phenomena_LDDs prep_with_deal_phenomena aux_deal_to conj_and_applied_correct xcomp_applied_deal auxpass_applied_are nsubjpass_applied_heuristics amod_tree_binary det_tree_a prep_to_converted_tree advmod_converted_then auxpass_converted_is nsubjpass_converted_tree dep_converted_Evaluation dep_converted_Induction dep_converted_ODonovan amod_tree_whole det_tree_the nn_Resources_Lexical prep_of_Induction_Resources conj_and_Induction_Evaluation nn_Induction_Large-Scale nn_al._et advmod_ODonovan_al. num_ODonovan_336 appos_Collins_1997 conj_and_Magerman_Collins appos_Magerman_1994 prep_on_based_Collins prep_on_based_Magerman det_tree_the det_node_each xcomp_made_based prep_in_made_tree prep_for_made_node auxpass_made_are nsubjpass_made_distinctions nn_distinctions_Head/argument/modifier nn_trees_derivation nn_trees_HPSG amod_trees_specified advmod_specified_partially det_treebank_the det_tree_each prep_with_annotate_trees prep_in_annotate_treebank dobj_annotate_tree advmod_annotate_automatically aux_annotate_to conj_and_used_assigned parataxis_used_correct parataxis_used_applied parataxis_used_converted parataxis_used_made xcomp_used_annotate auxpass_used_are nsubjpass_used_heuristics amod_heuristics_defined advmod_defined_Manually
J07-3004	P97-1003	o	Typically frequency information for rare words in the training data is used to estimate parameters for unknown words -LRB- and when these rare or unknown words are encountered during parsing additional information may be obtained from a POS-tagger -LRB- Collins 1997 -RRB- -RRB-	num_Collins_1997 appos_POS-tagger_Collins det_POS-tagger_a prep_from_obtained_POS-tagger auxpass_obtained_be aux_obtained_may nsubjpass_obtained_information amod_information_additional parataxis_encountered_obtained prep_during_encountered_parsing auxpass_encountered_are nsubjpass_encountered_words advmod_encountered_when cc_encountered_and amod_words_unknown amod_words_rare det_words_these conj_or_rare_unknown amod_words_unknown prep_for_parameters_words dobj_estimate_parameters aux_estimate_to dep_used_encountered xcomp_used_estimate auxpass_used_is nsubjpass_used_information advmod_used_Typically nn_data_training det_data_the prep_in_words_data amod_words_rare prep_for_information_words nn_information_frequency
J08-3003	P97-1003	o	The results so far mainly come from studies where a parser originally developed for English such as the Collins parser -LRB- Collins 1997,1999 -RRB- is applied to a new language which often leads to a signicant decrease in the measured accuracy -LRB- Collins et al. 1999 Bikel and Chiang 2000 Dubey and Keller 2003 Levy and Manning 2003 Corazza et al. 2004 -RRB-	dep_al._2004 nn_al._et nn_al._Corazza num_Manning_2003 conj_and_Levy_Manning dep_Dubey_2003 conj_and_Dubey_Keller num_Chiang_2000 conj_and_Bikel_Chiang dep_Collins_al. dep_Collins_Manning dep_Collins_Levy dep_Collins_Keller dep_Collins_Dubey dep_Collins_Chiang dep_Collins_Bikel dep_Collins_1999 dep_Collins_al. nn_Collins_et amod_accuracy_measured det_accuracy_the prep_in_decrease_accuracy amod_decrease_signicant det_decrease_a prep_to_leads_decrease advmod_leads_often nsubj_leads_which rcmod_language_leads amod_language_new det_language_a dep_applied_Collins prep_to_applied_language auxpass_applied_is csubjpass_applied_come num_Collins_1997,1999 appos_parser_Collins nn_parser_Collins det_parser_the prep_such_as_English_parser prep_for_developed_English advmod_developed_originally nsubj_developed_parser advmod_developed_where det_parser_a rcmod_studies_developed prep_from_come_studies advmod_come_mainly advmod_come_far nsubj_come_results advmod_far_so det_results_The
J08-3003	P97-1003	o	History-based models for predicting the next parser action -LRB- Black et al. 1992 Magerman 1995 Ratnaparkhi 1997 Collins 1999 -RRB- 3	num_Collins_1999 num_Ratnaparkhi_1997 num_Magerman_1995 dep_al._Collins dep_al._Ratnaparkhi dep_al._Magerman num_al._1992 nn_al._et amod_al._Black nn_action_parser amod_action_next det_action_the dobj_predicting_action dep_models_3 dep_models_al. prepc_for_models_predicting amod_models_History-based
J08-3004	P97-1003	o	Recently specic probabilistic tree-based models have been proposed not only for machine translation -LRB- Wu 1997 Alshawi Bangalore and Douglas 2000 Yamada and Knight 2001 Eisner 2003 Gildea 2003 -RRB- but also for summarization -LRB- Knight and Marcu 2002 -RRB- paraphrasing -LRB- Pang Knight and Marcu 2003 -RRB- natural language generation -LRB- Langkilde and Knight 1998 Bangalore and Rambow 2000 Corston-Oliver et al. 2002 -RRB- parsing and language modeling -LRB- Baker 1979 Lari and Young 1990 Collins 1997 Chelba and Jelinek 2000 Charniak 2001 Klein Information Sciences Institute 4676 Admiralty Way Marina del Rey CA 90292	num_CA_90292 nn_Rey_del nn_Rey_Marina appos_Way_CA appos_Way_Rey nn_Way_Admiralty num_Way_4676 appos_Institute_Way nn_Institute_Sciences nn_Institute_Information nn_Institute_Klein num_Charniak_2001 num_Jelinek_2000 conj_and_Chelba_Jelinek num_Collins_1997 num_Young_1990 conj_and_Lari_Young dep_Baker_Institute dep_Baker_Charniak dep_Baker_Jelinek dep_Baker_Chelba dep_Baker_Collins dep_Baker_Young dep_Baker_Lari num_Baker_1979 nn_modeling_language dep_al._2002 nn_al._et nn_al._Corston-Oliver num_Rambow_2000 conj_and_Bangalore_Rambow num_Knight_1998 dep_Langkilde_al. conj_and_Langkilde_Rambow conj_and_Langkilde_Bangalore conj_and_Langkilde_Knight appos_generation_Bangalore appos_generation_Knight appos_generation_Langkilde nn_generation_language amod_generation_natural num_Marcu_2003 conj_and_Pang_Marcu conj_and_Pang_Knight dep_paraphrasing_Marcu dep_paraphrasing_Knight dep_paraphrasing_Pang num_Marcu_2002 conj_and_Knight_Marcu conj_and_summarization_modeling conj_and_summarization_parsing conj_and_summarization_generation appos_summarization_paraphrasing dep_summarization_Marcu dep_summarization_Knight num_Gildea_2003 num_Eisner_2003 dep_Yamada_2001 conj_and_Yamada_Knight num_Douglas_2000 conj_and_Alshawi_Douglas conj_and_Alshawi_Bangalore dep_Wu_Gildea dep_Wu_Eisner dep_Wu_Knight dep_Wu_Yamada dep_Wu_Douglas dep_Wu_Bangalore dep_Wu_Alshawi num_Wu_1997 conj_and_translation_modeling conj_and_translation_parsing conj_and_translation_generation conj_and_translation_summarization appos_translation_Wu nn_translation_machine neg_only_not dep_proposed_Baker prep_for_proposed_summarization prep_for_proposed_translation preconj_proposed_only auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models advmod_proposed_Recently amod_models_tree-based amod_models_probabilistic amod_models_specic
J09-1003	P97-1003	o	More specically they used a parser -LRB- Collins 1997 -RRB- to determine the constituent structure of the sentences from which the grammatical function for each NP was derived	auxpass_derived_was nsubjpass_derived_function prep_from_derived_which det_NP_each prep_for_function_NP amod_function_grammatical det_function_the rcmod_sentences_derived det_sentences_the prep_of_structure_sentences nn_structure_constituent det_structure_the dobj_determine_structure aux_determine_to num_Collins_1997 appos_parser_Collins det_parser_a vmod_used_determine dobj_used_parser nsubj_used_they advmod_used_specically dep_specically_More
M98-1009	P97-1003	o	Statistical Model In SIFTs statistical model augmented parse trees are generated according to a process similar to that described in Collins -LRB- 1996 1997 -RRB-	dep_1996_1997 dep_Collins_1996 prep_in_described_Collins vmod_that_described prep_to_similar_that amod_process_similar det_process_a pobj_generated_process prepc_according_to_generated_to auxpass_generated_are nn_trees_parse dep_augmented_generated dobj_augmented_trees nsubj_augmented_Model amod_model_statistical nn_model_SIFTs prep_in_Model_model amod_Model_Statistical
N03-1017	P97-1003	o	To identify these we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers -LSB- Collins 1997 Schmidt and Schulte im Walde 2000 -RSB-	appos_Walde_2000 nn_Walde_im nn_Walde_Schulte nn_Walde_Schmidt conj_and_Schmidt_Schulte dep_Collins_Walde amod_Collins_1997 dep_parsers_Collins nn_parsers_syntactic amod_parsers_statistical agent_generated_parsers vmod_trees_generated nn_trees_parse prep_with_annotated_trees npadvmod_annotated_corpus amod_corpus_word-aligned det_corpus_a acomp_use_annotated nsubj_use_we advcl_use_identify dobj_identify_these aux_identify_To
N03-1017	P97-1003	o	We then parse both sides of the corpus with syntactic parsers -LSB- Collins 1997 Schmidt and Schulte im Walde 2000 -RSB-	appos_Walde_2000 nn_Walde_im nn_Walde_Schulte nn_Walde_Schmidt conj_and_Schmidt_Schulte dep_Collins_Walde amod_Collins_1997 dep_parsers_Collins amod_parsers_syntactic det_corpus_the prep_of_sides_corpus det_sides_both prep_with_parse_parsers dobj_parse_sides advmod_parse_then nsubj_parse_We ccomp_``_parse
N03-1027	P97-1003	o	Model interpolation in this case perSystem Training Heldout LR LP MAP Brown T Brown H 76.0 75.4 MAP Brown T WSJ 24 76.9 77.1 Gildea WSJ 2-21 86.1 86.6 MAP WSJ 2-21 WSJ 24 86.9 87.1 Charniak -LRB- 1997 -RRB- WSJ 2-21 WSJ 24 86.7 86.6 Ratnaparkhi -LRB- 1999 -RRB- WSJ 2-21 86.3 87.5 Collins -LRB- 1999 -RRB- WSJ 2-21 88.1 88.3 Charniak -LRB- 2000 -RRB- WSJ 2-21 WSJ 24 89.6 89.5 Collins -LRB- 2000 -RRB- WSJ 2-21 89.6 89.9 Table 4 Parser performance on WSJ 23 baselines	num_baselines_23 prep_on_performance_WSJ nn_performance_Parser num_Table_4 num_Table_89.9 num_Table_89.6 num_Table_2-21 nn_WSJ_Collins num_WSJ_89.6 num_WSJ_24 appos_Collins_2000 num_Collins_89.5 num_WSJ_2-21 dep_Charniak_WSJ dep_Charniak_2000 num_Charniak_88.3 number_88.3_88.1 number_88.3_2-21 nn_WSJ_Collins num_WSJ_86.3 num_WSJ_2-21 appos_Collins_1999 num_Collins_87.5 dep_Ratnaparkhi_WSJ dep_Ratnaparkhi_1999 num_Ratnaparkhi_86.6 number_86.6_86.7 number_86.6_24 num_WSJ_2-21 dep_Charniak_WSJ dep_Charniak_1997 num_Charniak_87.1 number_87.1_86.9 number_87.1_24 num_WSJ_2-21 dep_MAP_WSJ num_MAP_86.6 number_86.6_86.1 number_86.6_2-21 dep_Gildea_WSJ num_Gildea_77.1 number_77.1_76.9 number_77.1_24 nn_WSJ_T nn_Brown_MAP num_Brown_75.4 num_Brown_76.0 nn_Brown_H npadvmod_Brown_T nn_Brown_MAP nn_Brown_LP nn_Brown_LR nn_Brown_Heldout nn_Brown_Training nn_Brown_perSystem det_case_this dep_interpolation_baselines dep_interpolation_performance dep_interpolation_Table dep_interpolation_WSJ dep_interpolation_WSJ dep_interpolation_Charniak dep_interpolation_WSJ dep_interpolation_Ratnaparkhi dep_interpolation_WSJ dep_interpolation_Charniak dep_interpolation_WSJ dep_interpolation_MAP dep_interpolation_Gildea dep_interpolation_WSJ dep_interpolation_Brown dep_interpolation_Brown dep_interpolation_Brown prep_in_interpolation_case nn_interpolation_Model
N03-1027	P97-1003	o	The PCFG is a Markov grammar -LRB- Collins 1997 Charniak 2000 -RRB- i.e. the production probabilities are estimated by decomposing the joint probability of the categories on the right-hand side into a product of conditionals via the chain rule and making a Markov assumption	nn_assumption_Markov det_assumption_a dobj_making_assumption nsubjpass_making_probabilities nn_rule_chain det_rule_the prep_of_product_conditionals det_product_a amod_side_right-hand det_side_the prep_on_categories_side det_categories_the prep_of_probability_categories amod_probability_joint det_probability_the prep_via_decomposing_rule prep_into_decomposing_product dobj_decomposing_probability conj_and_estimated_making agent_estimated_decomposing auxpass_estimated_are nsubjpass_estimated_probabilities advmod_estimated_i.e. nn_probabilities_production det_probabilities_the dep_Charniak_2000 dep_Collins_Charniak appos_Collins_1997 dep_grammar_making dep_grammar_estimated appos_grammar_Collins nn_grammar_Markov det_grammar_a cop_grammar_is nsubj_grammar_PCFG det_PCFG_The
N03-1029	P97-1003	o	In the sequel we use Collinss statistical parser -LRB- Collins 1997 -RRB- as our canonical automated approximation of the Treebank	det_Treebank_the prep_of_approximation_Treebank amod_approximation_automated amod_approximation_canonical poss_approximation_our amod_Collins_1997 dep_parser_Collins amod_parser_statistical nn_parser_Collinss prep_as_use_approximation dobj_use_parser nsubj_use_we prep_in_use_sequel det_sequel_the
N03-2008	P97-1003	o	We analyze our results using syntactic features extracted from a parse tree generated by Collins parser -LRB- Collins 1997 -RRB- and compare those to models built using features extracted from FrameNets human annotations	amod_annotations_human nn_annotations_FrameNets prep_from_extracted_annotations vmod_features_extracted dobj_using_features xcomp_built_using vmod_models_built prep_to_those_models dobj_compare_those amod_Collins_1997 appos_parser_Collins nn_parser_Collins agent_generated_parser vmod_tree_generated nn_tree_parse det_tree_a prep_from_extracted_tree vmod_features_extracted amod_features_syntactic conj_and_using_compare dobj_using_features dep_results_compare dep_results_using poss_results_our dobj_analyze_results nsubj_analyze_We ccomp_``_analyze
N03-3010	P97-1003	p	3.2 Statistical Learning Model 3.2.1 Nave Bayes Learning Nave Bayes learning has been widely used in natural language processing with good results such as statistical syntactic parsing -LRB- Collins 1997 Charniak 1997 -RRB- hidden language understanding -LRB- Miller et al. 1994 -RRB-	amod_Miller_1994 dep_Miller_al. nn_Miller_et dep_understanding_Miller nn_understanding_language amod_understanding_hidden dep_Charniak_1997 dep_Collins_Charniak amod_Collins_1997 appos_parsing_understanding appos_parsing_Collins nn_parsing_syntactic amod_parsing_statistical prep_such_as_results_parsing amod_results_good nn_processing_language amod_processing_natural prep_with_used_results prep_in_used_processing advmod_used_widely auxpass_used_been aux_used_has nsubjpass_used_learning nn_learning_Bayes nn_learning_Nave nn_learning_Learning nn_learning_Bayes nn_learning_Nave num_learning_3.2.1 nn_learning_Model nn_learning_Learning amod_learning_Statistical num_learning_3.2
N04-1014	P97-1003	o	summarization -LRB- Knight and Marcu 2002 -RRB- paraphrasing -LRB- Pang Knight and Marcu 2003 -RRB- natural language generation -LRB- Langkilde and Knight 1998 Bangalore and Rambow 2000 Corston-Oliver et al. 2002 -RRB- and language modeling -LRB- Baker 1979 Lari and Young 1990 Collins 1997 Chelba and Jelinek 2000 Charniak 2001 Klein and Manning 2003 -RRB-	dep_Charniak_2003 conj_and_Charniak_Manning conj_and_Charniak_Klein conj_and_Charniak_2001 dep_Chelba_Manning dep_Chelba_Klein dep_Chelba_2001 dep_Chelba_Charniak conj_and_Chelba_2000 conj_and_Chelba_Jelinek num_Collins_1997 dep_Lari_2000 dep_Lari_Jelinek dep_Lari_Chelba conj_and_Lari_Collins conj_and_Lari_1990 conj_and_Lari_Young dep_Baker_Collins dep_Baker_1990 dep_Baker_Young dep_Baker_Lari amod_Baker_1979 nn_modeling_language num_Corston-Oliver_2002 nn_Corston-Oliver_al. nn_Corston-Oliver_et dep_Bangalore_Baker conj_and_Bangalore_modeling conj_and_Bangalore_Corston-Oliver conj_and_Bangalore_2000 conj_and_Bangalore_Rambow conj_and_Langkilde_modeling conj_and_Langkilde_Corston-Oliver conj_and_Langkilde_2000 conj_and_Langkilde_Rambow conj_and_Langkilde_Bangalore conj_and_Langkilde_1998 conj_and_Langkilde_Knight dep_generation_Bangalore dep_generation_1998 dep_generation_Knight dep_generation_Langkilde nn_generation_language amod_generation_natural dep_Pang_2003 conj_and_Pang_Marcu conj_and_Pang_Knight appos_paraphrasing_generation dep_paraphrasing_Marcu dep_paraphrasing_Knight dep_paraphrasing_Pang dep_Knight_2002 conj_and_Knight_Marcu dep_summarization_paraphrasing appos_summarization_Marcu appos_summarization_Knight
N04-2004	P97-1003	o	Due to advances in statistical syntactic parsing techniques -LRB- Collins 1997 Charniak 2001 -RRB- attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences	nn_sentences_language amod_sentences_natural prep_of_meaning_sentences det_meaning_the dobj_analyzing_meaning prepc_of_question_analyzing amod_question_harder det_question_the prep_towards_shifted_question advmod_shifted_recently aux_shifted_has nsubj_shifted_attention prep_due_to_shifted_advances dep_Charniak_2001 dep_Collins_Charniak amod_Collins_1997 dep_techniques_Collins nn_techniques_parsing nn_techniques_syntactic amod_techniques_statistical prep_in_advances_techniques
N06-1022	P97-1003	n	-LRB- 2005 -RRB- have implemented a dependency parser with good accuracy -LRB- it is almost as good at dependency parsing as Charniak -LRB- 2000 -RRB- -RRB- and very impressive speed -LRB- it is about ten times faster than Collins -LRB- 1997 -RRB- and four times faster than Charniak -LRB- 2000 -RRB- -RRB-	appos_Charniak_2000 prep_than_faster_Charniak npadvmod_faster_times num_times_four advmod_Collins_faster cc_Collins_and appos_Collins_1997 prep_than_faster_Collins npadvmod_faster_times num_times_ten amod_times_about advmod_is_faster nsubj_is_it dep_speed_is amod_speed_impressive advmod_impressive_very dep_Charniak_2000 nn_parsing_dependency prep_as_good_Charniak prep_at_good_parsing advmod_good_as advmod_good_almost cop_good_is nsubj_good_it amod_accuracy_good nn_parser_dependency det_parser_a conj_and_implemented_speed parataxis_implemented_good prep_with_implemented_accuracy dobj_implemented_parser aux_implemented_have nsubj_implemented_2005
N06-1024	P97-1003	o	An early exception to this was -LRB- Collins 1997 -RRB- itself where Model 2 used function tags during the training process for heuristics to identify arguments -LRB- e.g. the TMP tag on the NP in Figure 1 disqualifies the NP-TMP from being treated as an argument -RRB-	det_argument_an prep_as_treated_argument auxpass_treated_being det_NP-TMP_the prepc_from_disqualifies_treated dobj_disqualifies_NP-TMP nsubj_disqualifies_tag num_Figure_1 det_NP_the prep_in_tag_Figure prep_on_tag_NP nn_tag_TMP det_tag_the dep_e.g._disqualifies dep_arguments_e.g. dobj_identify_arguments aux_identify_to vmod_heuristics_identify prep_for_process_heuristics nn_process_training det_process_the prep_during_tags_process nn_tags_function amod_tags_used num_tags_2 nn_tags_Model dep_where_tags appos_Collins_where npadvmod_Collins_itself dep_Collins_1997 dep_was_Collins vmod_exception_was prep_to_exception_this amod_exception_early det_exception_An
N06-1040	P97-1003	o	Probability estimates of the RHS given the LHS are often smoothed by making a Markov assumption regarding the conditional independence of a category on those more than k categories away -LRB- Collins 1997 Charniak 2000 -RRB- P -LRB- X Y1Yn -RRB- = P -LRB- Y1 | X -RRB- nY i = 2 P -LRB- Yi | X Y1 Yi1 -RRB- P -LRB- Y1 | X -RRB- nY i = 2 P -LRB- Yi | X Yik Yi1 -RRB-	nn_Yi1_Yik appos_X_Yi1 num_X_| nn_X_Yi dep_P_X num_P_2 dep_=_P dep_i_= dep_nY_i dep_nY_X dep_nY_Y1 num_X_| dep_P_nY nn_Yi1_Y1 appos_X_Yi1 num_X_| nn_X_Yi dep_P_P dep_P_X num_P_2 dep_=_P dep_i_= dep_nY_i dep_nY_X dep_nY_Y1 num_X_| dep_P_nY dep_=_P nn_Y1Yn_X amod_P_= appos_P_Y1Yn dep_Charniak_2000 dep_Collins_Charniak dep_Collins_1997 advmod_categories_away nn_categories_k prep_than_more_categories appos_those_Collins amod_those_more prep_on_category_those det_category_a prep_of_independence_category amod_independence_conditional det_independence_the nn_assumption_Markov det_assumption_a prep_regarding_making_independence dobj_making_assumption dep_smoothed_P agent_smoothed_making advmod_smoothed_often auxpass_smoothed_are nsubjpass_smoothed_estimates det_LHS_the pobj_given_LHS det_RHS_the prep_estimates_given prep_of_estimates_RHS nn_estimates_Probability
N06-2025	P97-1003	o	The sentences were processed with the Collins parser -LRB- Collins 1997 -RRB- to generate automatic parse trees	nn_trees_parse amod_trees_automatic dobj_generate_trees aux_generate_to amod_Collins_1997 appos_parser_Collins nn_parser_Collins det_parser_the xcomp_processed_generate prep_with_processed_parser auxpass_processed_were nsubjpass_processed_sentences det_sentences_The
N07-2024	P97-1003	o	Parse Parse score from Model 2 of the statistical parser -LRB- Collins 1997 -RRB- normalized by the number of words	prep_of_number_words det_number_the agent_normalized_number amod_Collins_1997 dep_parser_Collins amod_parser_statistical det_parser_the prep_of_Model_parser num_Model_2 vmod_score_normalized prep_from_score_Model amod_score_Parse dobj_Parse_score ccomp_``_Parse
N07-3002	P97-1003	o	For example smoothing methods have played a central role in probabilistic approaches -LRB- Collins 1997 Wang et al. 2005 -RRB- and yet they are not being used in current large margin training algorithms	nn_algorithms_training nn_algorithms_margin amod_algorithms_large amod_algorithms_current prep_in_used_algorithms auxpass_used_being neg_used_not aux_used_are nsubjpass_used_they advmod_used_yet num_Wang_2005 nn_Wang_al. nn_Wang_et dep_Collins_Wang appos_Collins_1997 appos_approaches_Collins amod_approaches_probabilistic amod_role_central det_role_a conj_and_played_used prep_in_played_approaches dobj_played_role aux_played_have nsubj_played_methods prep_for_played_example amod_methods_smoothing
N07-3002	P97-1003	o	Furthermore I based training on maximizing the conditional probability of a parse tree given a sentence unlike most previous generative models -LRB- Magerman 1995 Collins 1997 Charniak 1997 -RRB- which focus on maximizing the joint probability of the parse tree and the sentence	det_sentence_the nn_tree_parse det_tree_the prep_of_probability_tree amod_probability_joint det_probability_the conj_and_maximizing_sentence dobj_maximizing_probability prepc_on_focus_sentence prepc_on_focus_maximizing nsubj_focus_which dep_Charniak_1997 num_Collins_1997 dep_Magerman_Charniak conj_Magerman_Collins appos_Magerman_1995 rcmod_models_focus appos_models_Magerman amod_models_generative amod_models_previous amod_models_most det_sentence_a pobj_given_sentence prep_tree_given nn_tree_parse det_tree_a prep_of_probability_tree amod_probability_conditional det_probability_the dobj_maximizing_probability prep_unlike_based_models prepc_on_based_maximizing dobj_based_training nsubj_based_I advmod_based_Furthermore
N07-3002	P97-1003	o	1 Introduction Over the past decade there has been tremendous progress on learning parsing models from treebank data -LRB- Magerman 1995 Collins 1999 Charniak 1997 Ratnaparkhi 1999 Charniak 2000 Wang et al. 2005 McDonald et al. 2005 -RRB-	num_McDonald_2005 nn_McDonald_al. nn_McDonald_et num_Wang_2005 nn_Wang_al. nn_Wang_et num_Charniak_2000 appos_Ratnaparkhi_1999 num_Charniak_1997 num_Collins_1999 dep_Magerman_McDonald conj_Magerman_Wang conj_Magerman_Charniak conj_Magerman_Ratnaparkhi conj_Magerman_Charniak conj_Magerman_Collins appos_Magerman_1995 dep_data_Magerman amod_data_treebank nn_models_parsing prep_from_learning_data dobj_learning_models prepc_on_progress_learning amod_progress_tremendous cop_progress_been aux_progress_has expl_progress_there dep_progress_Introduction amod_decade_past det_decade_the prep_over_Introduction_decade num_Introduction_1
N07-3002	P97-1003	o	Most of the early work in this area was based on postulating generative probability models of language that included parse structures -LRB- Magerman 1995 Collins 1997 Charniak 1997 -RRB-	amod_Charniak_1997 dep_Collins_Charniak num_Collins_1997 dep_Magerman_Collins appos_Magerman_1995 dep_structures_Magerman amod_structures_parse dobj_included_structures nsubj_included_that rcmod_models_included prep_of_models_language nn_models_probability amod_models_generative dobj_postulating_models prepc_on_based_postulating auxpass_based_was nsubjpass_based_Most det_area_this prep_in_work_area amod_work_early det_work_the prep_of_Most_work
N07-3002	P97-1003	o	Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems -LRB- Collins 1997 Bikel 2004 -RRB-	amod_Bikel_2004 dep_Collins_Bikel amod_Collins_1997 appos_problems_Collins nn_problems_data amod_problems_sparse det_problems_the prep_with_cope_problems aux_cope_to nn_tricks_estimation amod_tricks_back-off conj_and_smoothing_tricks amod_smoothing_various vmod_incorporating_cope dobj_incorporating_tricks dobj_incorporating_smoothing nsubj_incorporating_Learning amod_techniques_based dep_techniques_likelihood amod_likelihood_simple det_model_the prep_of_parameters_model det_parameters_the prep_with_estimating_techniques dobj_estimating_parameters conj_but_consisted_incorporating prepc_of_consisted_estimating nsubj_consisted_Learning det_context_this prep_in_Learning_context
N09-1025	P97-1003	o	the syntax-based system we ran a reimplementation of the Collins parser -LRB- Collins 1997 -RRB- on the English half of the bitext to produce parse trees then restructured and relabeled them as described in Section 3.2	num_Section_3.2 prep_in_described_Section mark_described_as advcl_restructured_described dobj_restructured_them conj_and_restructured_relabeled advmod_restructured_then nn_trees_parse dobj_produce_trees aux_produce_to det_bitext_the prep_of_half_bitext nn_half_English det_half_the dep_Collins_1997 appos_parser_Collins nn_parser_Collins det_parser_the prep_of_reimplementation_parser det_reimplementation_a dep_ran_relabeled dep_ran_restructured vmod_ran_produce prep_on_ran_half dobj_ran_reimplementation nsubj_ran_we nsubj_ran_system amod_system_syntax-based det_system_the
N09-1037	P97-1003	o	6 Related Work A pioneering antecedent for our work is -LRB- Miller et al. 2000 -RRB- who trained a Collins-style generative parser -LRB- Collins 1997 -RRB- over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task	dobj_shared_task nsubj_shared_MUC-7 mark_shared_for det_MUC-7_the nn_annotations_relations nn_annotations_template conj_and_entity_annotations nn_entity_template det_entity_the advcl_augmented_shared prep_with_augmented_annotations prep_with_augmented_entity parataxis_augmented_trained auxpass_augmented_is nsubjpass_augmented_Work amod_structure_syntactic det_structure_a dep_Collins_1997 appos_parser_Collins amod_parser_generative amod_parser_Collins-style det_parser_a prep_over_trained_structure dobj_trained_parser nsubj_trained_who amod_Miller_2000 dep_Miller_al. nn_Miller_et dep_is_Miller poss_work_our prep_for_antecedent_work amod_antecedent_pioneering det_antecedent_A amod_Work_antecedent amod_Work_Related num_Work_6
N09-1039	P97-1003	o	Since this transform takes a probabilistic grammar as input it can also easily accommodate horizontal and vertical Markovisation -LRB- annotating grammar symbols with parent and sibling categories -RRB- as described by Collins -LRB- 1997 -RRB- and subsequently	advmod_Collins_subsequently cc_Collins_and appos_Collins_1997 prep_by_described_Collins mark_described_as nn_categories_sibling nn_categories_parent conj_and_parent_sibling nn_symbols_grammar prep_with_annotating_categories dobj_annotating_symbols dep_Markovisation_annotating amod_Markovisation_vertical amod_Markovisation_horizontal conj_and_horizontal_vertical advcl_accommodate_described dobj_accommodate_Markovisation advmod_accommodate_easily advmod_accommodate_also aux_accommodate_can nsubj_accommodate_it advcl_accommodate_transform amod_grammar_probabilistic det_grammar_a prep_as_takes_input dobj_takes_grammar ccomp_transform_takes nsubj_transform_this mark_transform_Since
N09-1063	P97-1003	o	For example the lexicalized grammars of Collins -LRB- 1997 -RRB- and Charniak -LRB- 1997 -RRB- and the statesplit grammars of Petrov et al.	nn_al._et nn_al._Petrov prep_of_grammars_al. amod_grammars_statesplit det_grammars_the appos_Charniak_1997 conj_and_Collins_Charniak appos_Collins_1997 conj_and_grammars_grammars prep_of_grammars_Charniak prep_of_grammars_Collins amod_grammars_lexicalized det_grammars_the dep_,_grammars dep_,_grammars pobj_For_example dep_``_For
P00-1060	P97-1003	o	Many probabilistic evaluation models have been published inspired by one or more of these feature types -LSB- Black 1992 -RSB- -LSB- Briscoe 1993 -RSB- -LSB- Charniak 1997 -RSB- -LSB- Collins 1996 -RSB- -LSB- Collins 1997 -RSB- -LSB- Magerman 1995 -RSB- -LSB- Eisner 1996 -RSB- but discrepancies between training sets algorithms and hardware environments make it difficult if not impossible to compare the models objectively	det_models_the advmod_compare_objectively dobj_compare_models aux_compare_to advcl_compare_impossible neg_impossible_not mark_impossible_if ccomp_difficult_compare nsubj_difficult_it ccomp_make_difficult nsubj_make_discrepancies nsubj_make_Briscoe nn_environments_hardware conj_and_sets_environments conj_and_sets_algorithms nn_sets_training prep_between_discrepancies_environments prep_between_discrepancies_algorithms prep_between_discrepancies_sets dep_Eisner_1996 dep_Magerman_1995 dep_Collins_1997 dep_Collins_1996 dep_Charniak_1997 conj_but_Briscoe_discrepancies dep_Briscoe_Eisner dep_Briscoe_Magerman dep_Briscoe_Collins dep_Briscoe_Collins dep_Briscoe_Charniak dep_Briscoe_1993 dep_Black_1992 nn_types_feature det_types_these dep_one_Black prep_of_one_types conj_or_one_more ccomp_inspired_make agent_inspired_more agent_inspired_one vmod_published_inspired auxpass_published_been aux_published_have nsubjpass_published_models nn_models_evaluation amod_models_probabilistic amod_models_Many
P00-1060	P97-1003	o	1 Introduction In the field of statistical parsing various probabilistic evaluation models have been proposed where different models use different feature types -LSB- Black 1992 -RSB- -LSB- Briscoe 1993 -RSB- -LSB- Brown 1991 -RSB- -LSB- Charniak 1997 -RSB- -LSB- Collins 1996 -RSB- -LSB- Collins 1997 -RSB- -LSB- Magerman 1991 -RSB- -LSB- Magerman 1992 -RSB- -LSB- Magerman 1995 -RSB- -LSB- Eisner 1996 -RSB-	amod_Eisner_1996 dep_Magerman_1995 dep_Magerman_1992 dep_Magerman_1991 dep_Collins_1997 dep_Collins_1996 dep_Charniak_1997 dep_Brown_Eisner dep_Brown_Magerman dep_Brown_Magerman dep_Brown_Magerman dep_Brown_Collins dep_Brown_Collins dep_Brown_Charniak dep_Brown_1991 dep_Briscoe_1993 dep_Black_1992 nn_types_feature amod_types_different dep_use_Brown dep_use_Briscoe dep_use_Black dobj_use_types nsubj_use_models advmod_use_where amod_models_different advcl_proposed_use auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models dep_proposed_Introduction nn_models_evaluation amod_models_probabilistic amod_models_various amod_parsing_statistical prep_of_field_parsing det_field_the prep_in_Introduction_field num_Introduction_1 ccomp_``_proposed
P00-1061	P97-1003	p	Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of e.g. Collins -LRB- 1997 -RRB- Charniak -LRB- 1997 -RRB- or Ratnaparkhi -LRB- 1997 -RRB-	appos_Ratnaparkhi_1997 appos_Charniak_1997 conj_or_Collins_Ratnaparkhi conj_or_Collins_Charniak appos_Collins_1997 pobj_of_Ratnaparkhi pobj_of_Charniak pobj_of_Collins conj_of_e.g. prep_models_of nn_models_parsing amod_models_statistical det_models_the prep_in_used_models advmod_used_successfully auxpass_used_are nsubjpass_used_lexicalizations nn_models_probability prep_of_lexicalizations_models amod_lexicalizations_word-based amod_lexicalizations_Such
P01-1010	P97-1003	p	Head-lexicalized stochastic grammars have recently become increasingly popular -LRB- see Collins 1997 1999 Charniak 1997 2000 -RRB-	num_Charniak_2000 num_Charniak_1997 dep_Collins_Charniak num_Collins_1999 num_Collins_1997 dobj_see_Collins advmod_popular_increasingly xcomp_become_see acomp_become_popular advmod_become_recently aux_become_have nsubj_become_grammars amod_grammars_stochastic amod_grammars_Head-lexicalized
P01-1010	P97-1003	o	We should note however that most other stochastic parsers do include counts of single nonheadwords they appear in the backed-off statistics of these parsers -LRB- see Collins 1997 1999 Charniak 1997 Goodman 1998 -RRB-	num_Goodman_1998 num_Charniak_1997 dep_Collins_Goodman dep_Collins_Charniak num_Collins_1999 num_Collins_1997 dobj_see_Collins det_parsers_these prep_of_statistics_parsers amod_statistics_backed-off det_statistics_the parataxis_appear_see prep_in_appear_statistics nsubj_appear_they amod_nonheadwords_single prep_of_counts_nonheadwords parataxis_include_appear dobj_include_counts aux_include_do nsubj_include_parsers mark_include_that amod_parsers_stochastic amod_parsers_other amod_parsers_most dep_note_include advmod_note_however aux_note_should nsubj_note_We
P01-1010	P97-1003	o	A major difference between our approach and most other models tested on the WSJ is that the DOP model uses frontier lexicalization while most other models use constituent lexicalization -LRB- in that they associate each constituent non terminal with its lexical head see Collins 1996 1999 Charniak 1997 Eisner 1997 -RRB-	num_Eisner_1997 num_Charniak_1997 dep_Collins_Eisner dep_Collins_Charniak num_Collins_1999 num_Collins_1996 dobj_see_Collins dep_head_see amod_head_lexical poss_head_its prep_with_terminal_head amod_terminal_non dep_constituent_terminal dep_each_constituent dobj_associate_each nsubj_associate_they mark_associate_that amod_lexicalization_constituent prepc_in_use_associate dobj_use_lexicalization nsubj_use_models mark_use_while amod_models_other amod_models_most nn_lexicalization_frontier advcl_uses_use dobj_uses_lexicalization nsubj_uses_model mark_uses_that nn_model_DOP det_model_the ccomp_is_uses nsubj_is_difference det_WSJ_the prep_on_tested_WSJ vmod_models_tested amod_models_other advmod_models_most conj_and_approach_models poss_approach_our prep_between_difference_models prep_between_difference_approach amod_difference_major det_difference_A
P01-1010	P97-1003	o	Charniak 1996 1997 -RRB- while most current stochastic parsing models use a markov grammar -LRB- e.g. Collins 1999 Charniak 2000 -RRB-	num_Charniak_2000 dep_Collins_Charniak num_Collins_1999 dep_e.g._Collins dep_''_e.g. nn_grammar_markov det_grammar_a dobj_use_grammar nsubj_use_models mark_use_while nn_models_parsing amod_models_stochastic amod_models_current amod_models_most advcl_,_use amod_Charniak_1997 num_Charniak_1996 dep_``_Charniak
P01-1010	P97-1003	o	context-free rules Charniak -LRB- 1996 -RRB- Collins -LRB- 1996 -RRB- Eisner -LRB- 1996 -RRB- context-free rules headwords Charniak -LRB- 1997 -RRB- context-free rules headwords grandparent nodes Collins -LRB- 2000 -RRB- context-free rules headwords grandparent nodes/rules bigrams two-level rules two-level bigrams nonheadwords Bod -LRB- 1992 -RRB- all fragments within parse trees Scope of Statistical Dependencies Model Figure 4	num_Figure_4 nn_Figure_Model nn_Figure_Dependencies amod_Figure_Statistical prep_of_Scope_Figure dep_trees_Scope nn_trees_parse prep_within_fragments_trees det_fragments_all nn_fragments_Bod appos_Bod_1992 nn_Bod_nonheadwords amod_bigrams_two-level amod_rules_two-level nn_nodes/rules_grandparent amod_rules_context-free nn_rules_Collins appos_Collins_2000 nn_Collins_nodes nn_Collins_grandparent amod_rules_context-free nn_rules_Charniak appos_Charniak_1997 nn_Charniak_headwords appos_rules_fragments conj_rules_bigrams conj_rules_rules conj_rules_bigrams conj_rules_nodes/rules conj_rules_headwords conj_rules_rules conj_rules_headwords conj_rules_rules amod_rules_context-free nn_rules_Eisner nn_rules_Collins dep_rules_1996 nsubj_rules_Charniak appos_Eisner_1996 appos_Collins_1996 nn_Charniak_rules amod_Charniak_context-free
P01-1010	P97-1003	o	While early head-lexicalized grammars restricted the fragments to the locality of headwords -LRB- e.g. Collins 1996 Eisner 1996 -RRB- later models showed the importance of including context from higher nodes in the tree -LRB- Charniak 1997 Johnson 1998 -RRB-	num_Johnson_1998 dep_Charniak_Johnson dep_Charniak_1997 dep_tree_Charniak det_tree_the prep_in_nodes_tree amod_nodes_higher prep_from_context_nodes pobj_including_context prepc_of_importance_including det_importance_the dobj_showed_importance nsubj_showed_models advcl_showed_restricted advmod_models_later num_Eisner_1996 prep_Eisner_e.g. num_Collins_1996 pobj_e.g._Collins appos_headwords_Eisner prep_of_locality_headwords det_locality_the det_fragments_the prep_to_restricted_locality dobj_restricted_fragments nsubj_restricted_grammars mark_restricted_While amod_grammars_head-lexicalized amod_grammars_early
P01-1010	P97-1003	o	The importance of including single nonheadwords is now also uncontroversial -LRB- e.g. Collins 1997 1999 Charniak 2000 -RRB- and the current paper has shown the importance of including two and more nonheadwords	num_nonheadwords_more num_nonheadwords_two conj_and_two_more pobj_including_nonheadwords prepc_of_importance_including det_importance_the dobj_shown_importance aux_shown_has nsubj_shown_paper amod_paper_current det_paper_the num_Charniak_2000 conj_and_Collins_shown conj_and_Collins_Charniak num_Collins_1999 num_Collins_1997 pobj_e.g._shown pobj_e.g._Charniak pobj_e.g._Collins dep_uncontroversial_e.g. advmod_uncontroversial_also advmod_uncontroversial_now cop_uncontroversial_is nsubj_uncontroversial_importance amod_nonheadwords_single pobj_including_nonheadwords prepc_of_importance_including det_importance_The ccomp_``_uncontroversial
P01-1010	P97-1003	o	As in most other statistical parsing systems we therefore use the pruning technique described in Goodman -LRB- 1997 -RRB- and Collins -LRB- 1999 263-264 -RRB- which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability	amod_probability_prior poss_probability_its det_item_the prep_of_probability_item amod_probability_inside det_probability_the prep_of_product_probability det_product_the prep_to_equal_product amod_chart_equal det_chart_the conj_and_in_probability pobj_in_chart det_item_each det_score_a prep_assigns_probability prep_assigns_in prep_to_assigns_item dobj_assigns_score nsubj_assigns_which dep_1999_263-264 dep_Collins_1999 conj_and_Goodman_Collins appos_Goodman_1997 prep_in_described_Collins prep_in_described_Goodman rcmod_technique_assigns vmod_technique_described nn_technique_pruning det_technique_the dobj_use_technique advmod_use_therefore nsubj_use_we prep_use_As nn_systems_parsing amod_systems_statistical amod_systems_other amod_systems_most pobj_in_systems pcomp_As_in
P01-1010	P97-1003	o	4.1 The base line For our base line parse accuracy we used the now standard division of the WSJ -LRB- see Collins 1997 1999 Charniak 1997 2000 Ratnaparkhi 1999 -RRB- with sections 2 through 21 for training -LRB- approx	advmod_training_approx num_sections_2 num_Ratnaparkhi_1999 dep_Charniak_Ratnaparkhi num_Charniak_2000 num_Charniak_1997 num_Collins_1999 num_Collins_1997 dep_see_Charniak dobj_see_Collins det_WSJ_the prep_of_division_WSJ amod_division_standard advmod_division_now det_division_the prep_for_used_training prep_through_used_21 prep_with_used_sections dep_used_see dobj_used_division nsubj_used_we nsubj_used_line nn_accuracy_parse nn_accuracy_line nn_accuracy_base poss_accuracy_our prep_for_line_accuracy nn_line_base det_line_The num_line_4.1
P01-1010	P97-1003	o	Many stochastic parsing models use linguistic intuitions to find this minimal set for example by restricting the statistical dependencies to the locality of headwords of constituents -LRB- Collins 1997 1999 Eisner 1997 -RRB- leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies	amod_dependencies_motivated advmod_motivated_linguistically prep_beyond_go_dependencies nsubj_go_that rcmod_dependencies_go amod_dependencies_statistical amod_dependencies_important dobj_exist_dependencies expl_exist_there mark_exist_whether amod_question_open det_question_an ccomp_leaving_exist prep_as_leaving_question dobj_leaving_it num_Eisner_1997 dep_Collins_Eisner amod_Collins_1999 num_Collins_1997 prep_of_headwords_constituents prep_of_locality_headwords det_locality_the amod_dependencies_statistical det_dependencies_the dep_restricting_Collins prep_to_restricting_locality dobj_restricting_dependencies amod_set_minimal det_set_this dobj_find_set aux_find_to amod_intuitions_linguistic vmod_use_leaving prepc_by_use_restricting prep_for_use_example vmod_use_find dobj_use_intuitions nsubj_use_models nn_models_parsing amod_models_stochastic amod_models_Many
P01-1010	P97-1003	o	Table 1 shows the LP and LR scores obtained with our base line subtree set and compares these scores with those of previous stochastic parsers tested on the WSJ -LRB- respectively Charniak 1997 Collins 1999 Ratnaparkhi 1999 and Charniak 2000 -RRB-	num_Charniak_2000 num_Ratnaparkhi_1999 num_Collins_1999 conj_and_Charniak_Charniak conj_and_Charniak_Ratnaparkhi conj_and_Charniak_Collins num_Charniak_1997 dep_respectively_Charniak dep_respectively_Ratnaparkhi dep_respectively_Collins dep_respectively_Charniak det_WSJ_the prep_on_tested_WSJ vmod_parsers_tested amod_parsers_stochastic amod_parsers_previous dep_those_respectively prep_of_those_parsers det_scores_these prep_with_compares_those dobj_compares_scores nsubj_compares_Table nn_set_subtree nn_set_line nn_set_base poss_set_our prep_with_obtained_set nn_scores_LR vmod_LP_obtained conj_and_LP_scores det_LP_the conj_and_shows_compares dobj_shows_scores dobj_shows_LP nsubj_shows_Table num_Table_1
P02-1018	P97-1003	o	-LRB- Collins -LRB- 1997 -RRB- discusses the recovery of one kind of empty node viz	amod_node_empty prep_of_kind_node num_kind_one appos_recovery_viz prep_of_recovery_kind det_recovery_the dobj_discusses_recovery nsubj_discusses_Collins appos_Collins_1997
P02-1043	P97-1003	o	Like the models of Goodman -LRB- 1997 -RRB- the additional features in our model are generated probabilistically whereas in the parser of Collins -LRB- 1997 -RRB- distance measures are assumed to be a function of the already generated structure and are not generated explicitly	advmod_generated_explicitly neg_generated_not auxpass_generated_are amod_structure_generated det_structure_the advmod_generated_already conj_and_function_generated prep_of_function_structure det_function_a cop_function_be aux_function_to xcomp_assumed_generated xcomp_assumed_function auxpass_assumed_are prep_in_assumed_parser mark_assumed_whereas nn_measures_distance num_measures_1997 nn_measures_Collins prep_of_parser_measures det_parser_the advcl_generated_assumed advmod_generated_probabilistically auxpass_generated_are nsubjpass_generated_features prep_like_generated_models poss_model_our prep_in_features_model amod_features_additional det_features_the appos_Goodman_1997 prep_of_models_Goodman det_models_the ccomp_``_generated
P02-1043	P97-1003	o	Distance measures for CCG Our distance measures are related to those proposed by Goodman -LRB- 1997 -RRB- which are appropriate for binary trees -LRB- unlike those of Collins -LRB- 1997 -RRB- -RRB-	appos_Collins_1997 prep_of_those_Collins amod_trees_binary prep_unlike_appropriate_those prep_for_appropriate_trees cop_appropriate_are nsubj_appropriate_which rcmod_Goodman_appropriate appos_Goodman_1997 agent_proposed_Goodman vmod_those_proposed prep_to_related_those cop_related_are csubj_related_measures nn_measures_distance poss_measures_Our nn_measures_CCG prep_for_measures_measures nsubj_measures_Distance
P02-1043	P97-1003	o	5.2 Adding lexical information Gildea -LRB- 2001 -RRB- shows that removing the lexical dependencies in Model 1 of Collins -LRB- 1997 -RRB- -LRB- that is not conditioning on w h when generating w s -RRB- decreases labeled precision and recall by only 0.5 %	num_%_0.5 quantmod_0.5_only conj_and_precision_recall prep_by_labeled_% dobj_labeled_recall dobj_labeled_precision dep_decreases_labeled nn_s_w dobj_generating_s advmod_generating_when nn_h_w dep_conditioning_decreases rcmod_conditioning_generating prep_on_conditioning_h neg_conditioning_not nsubj_is_conditioning dep_that_is dep_Collins_that appos_Collins_1997 prep_of_1_Collins dep_Model_1 amod_dependencies_lexical det_dependencies_the prep_in_removing_Model dobj_removing_dependencies dep_that_removing dep_shows_that nsubj_shows_5.2 appos_Gildea_2001 nn_Gildea_information amod_Gildea_lexical dobj_Adding_Gildea vmod_5.2_Adding
P02-1057	P97-1003	o	tactic parser -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_parser_Collins nn_parser_tactic
P03-1002	P97-1003	o	At last the dependency parser presented in -LRB- Collins 1997 -RRB- is used to generate the full parse	amod_parse_full det_parse_the dobj_generate_parse aux_generate_to xcomp_used_generate auxpass_used_is nsubjpass_used_parser prep_at_used_last amod_Collins_1997 prep_in_presented_Collins vmod_parser_presented nn_parser_dependency det_parser_the
P03-1002	P97-1003	o	This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_in_Collins prep_reported_in vmod_parser_reported amod_parser_probabilistic det_parser_the prep_of_output_parser det_output_the prep_on_operates_output nsubj_operates_technique nn_argument_predicate amod_argument_labeling prep_of_technique_argument amod_technique_statistical det_technique_This
P03-1013	P97-1003	o	The reader is referred to Schmid -LRB- 2000 -RRB- and Collins -LRB- 1997 -RRB- for details	appos_Collins_1997 conj_and_Schmid_Collins appos_Schmid_2000 prep_for_referred_details prep_to_referred_Collins prep_to_referred_Schmid auxpass_referred_is nsubjpass_referred_reader det_reader_The
P03-1013	P97-1003	p	Lexicalization can increase parsing performance dramatically for English -LRB- Carroll and Rooth 1998 Charniak 1997 2000 Collins 1997 -RRB- and the lexicalized model proposed by Collins -LRB- 1997 -RRB- has been successfully applied to Czech -LRB- Collins et al. 1999 -RRB- and Chinese -LRB- Bikel and Chiang 2000 -RRB-	amod_Bikel_2000 conj_and_Bikel_Chiang dep_Chinese_Chiang dep_Chinese_Bikel amod_Collins_1999 dep_Collins_al. nn_Collins_et conj_and_Czech_Chinese dep_Czech_Collins prep_to_applied_Chinese prep_to_applied_Czech advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_model appos_Collins_1997 agent_proposed_Collins vmod_model_proposed amod_model_lexicalized det_model_the dep_Collins_1997 dep_Charniak_2000 dep_Charniak_1997 dep_Carroll_Collins dep_Carroll_Charniak num_Carroll_1998 conj_and_Carroll_Rooth appos_English_Rooth appos_English_Carroll amod_performance_parsing conj_and_increase_applied prep_for_increase_English advmod_increase_dramatically dobj_increase_performance aux_increase_can nsubj_increase_Lexicalization
P03-1013	P97-1003	o	sister head tag X Table 4 Linguistic features in the current model compared to the models of Carroll and Rooth -LRB- 1998 -RRB- Collins -LRB- 1997 -RRB- and Charniak -LRB- 2000 -RRB- Negra based on Collinss -LRB- 1997 -RRB- model for nonrecursive NPs in the Penn Treebank -LRB- which are also flat -RRB-	advmod_flat_also cop_flat_are nsubj_flat_which rcmod_Treebank_flat nn_Treebank_Penn det_Treebank_the prep_in_NPs_Treebank amod_NPs_nonrecursive prep_for_model_NPs nn_model_Collinss appos_Collinss_1997 prep_on_based_model nn_Negra_Charniak appos_Charniak_2000 appos_Collins_1997 appos_Rooth_1998 conj_and_Carroll_Negra conj_and_Carroll_Collins conj_and_Carroll_Rooth prep_of_models_Negra prep_of_models_Collins prep_of_models_Rooth prep_of_models_Carroll det_models_the amod_model_current det_model_the vmod_features_based pobj_features_models prepc_compared_to_features_to prep_in_features_model amod_features_Linguistic dep_Table_features num_Table_4 nn_Table_X nn_Table_tag nn_Table_head nn_Table_sister
P03-1013	P97-1003	o	For non-recursive NPs Collins -LRB- 1997 -RRB- does not use the probability function in -LRB- 5 -RRB- but instead substitutes P r -LRB- and by analogy P l -RRB- by P r -LRB- R i t -LRB- R i -RRB- l -LRB- R i -RRB- jP R i1 t -LRB- R i1 -RRB- l -LRB- R i1 -RRB- ;d -LRB- i -RRB- -RRB- -LRB- 8 -RRB- Here the head H is substituted by the sister R i1 -LRB- and L i1 -RRB-	conj_and_i1_L appos_i1_i1 appos_i1_L nn_i1_R nn_i1_sister det_i1_the agent_substituted_i1 auxpass_substituted_is nsubjpass_substituted_H nn_H_head det_H_the npadvmod_Here_r dep_;d_i nn_;d_l nn_i1_R appos_l_i1 nn_i1_R appos_t_i1 nn_i1_R dep_jP_i nn_jP_l dep_i_R dep_i_R appos_t_i dep_R_;d dep_R_t dep_R_i1 dep_R_jP dep_R_t dep_R_i appos_r_8 dep_r_R rcmod_P_substituted advmod_P_Here nn_l_P pobj_by_analogy conj_and_r_l conj_and_r_by nn_r_P prep_by_substitutes_P dobj_substitutes_l dobj_substitutes_by dobj_substitutes_r advmod_substitutes_instead conj_but_5_substitutes nn_function_probability det_function_the prep_in_use_substitutes prep_in_use_5 dobj_use_function neg_use_not aux_use_does nsubj_use_Collins prep_for_use_NPs appos_Collins_1997 amod_NPs_non-recursive
P03-1013	P97-1003	o	Table 4 shows the linguistic features of the resulting model compared to the models of Carroll and Rooth -LRB- 1998 -RRB- Collins -LRB- 1997 -RRB- and Charniak -LRB- 2000 -RRB-	appos_Charniak_2000 appos_Collins_1997 appos_Rooth_1998 conj_and_Carroll_Charniak conj_and_Carroll_Collins conj_and_Carroll_Rooth prep_of_models_Charniak prep_of_models_Collins prep_of_models_Rooth prep_of_models_Carroll det_models_the amod_model_resulting det_model_the pobj_features_models prepc_compared_to_features_to prep_of_features_model amod_features_linguistic det_features_the dobj_shows_features nsubj_shows_Table num_Table_4
P03-1013	P97-1003	o	The Collins -LRB- 1997 -RRB- model does not use context-free rules but generates the next category using zeroth order Markov chains -LRB- see Section 3.3 -RRB- hence no information about the previous sisters is included	auxpass_included_is nsubjpass_included_information amod_sisters_previous det_sisters_the prep_about_information_sisters neg_information_no num_Section_3.3 dobj_see_Section nn_chains_Markov nn_chains_order nn_chains_zeroth dep_using_see dobj_using_chains vmod_category_using amod_category_next det_category_the dobj_generates_category nsubj_generates_model amod_rules_context-free ccomp_use_included advmod_use_hence conj_but_use_generates dobj_use_rules neg_use_not aux_use_does nsubj_use_model nn_model_Collins det_model_The appos_Collins_1997
P03-1013	P97-1003	o	Section 3 describes two standard lexicalized models -LRB- Carroll and Rooth 1998 Collins 1997 -RRB- as well as an unlexicalized baseline model	nn_model_baseline amod_model_unlexicalized det_model_an dep_Collins_1997 conj_and_Carroll_model conj_and_Carroll_Collins conj_and_Carroll_1998 conj_and_Carroll_Rooth dep_models_model dep_models_Collins dep_models_1998 dep_models_Rooth dep_models_Carroll amod_models_lexicalized amod_models_standard num_models_two dobj_describes_models nsubj_describes_Section num_Section_3
P03-1013	P97-1003	o	We first added sister-head dependencies for NPs -LRB- following Collinss -LRB- 1997 -RRB- original proposal -RRB- and then for PPs which are flat in Negra and thus similar in structure to NPs -LRB- see Section 2.2 -RRB-	num_Section_2.2 dobj_see_Section dep_similar_see prep_to_similar_NPs prep_in_similar_structure advmod_similar_thus nsubj_similar_We prep_in_flat_Negra cop_flat_are nsubj_flat_which rcmod_PPs_flat pobj_for_PPs advmod_for_then amod_proposal_original nn_proposal_Collinss dep_Collinss_1997 pobj_following_proposal conj_and_dependencies_for dep_dependencies_following prep_for_dependencies_NPs amod_dependencies_sister-head conj_and_added_similar dobj_added_for dobj_added_dependencies advmod_added_first nsubj_added_We
P03-1013	P97-1003	o	The progression in the probabilistic parsing literature has been to start with lexical head-head dependencies -LRB- Collins 1997 -RRB- and then add non-lexical sis2 This result generalizes to Ss which are also flat in Negra -LRB- see Section 2.2 -RRB-	num_Section_2.2 dobj_see_Section prep_in_flat_Negra advmod_flat_also cop_flat_are nsubj_flat_which dep_Ss_see rcmod_Ss_flat prep_to_generalizes_Ss nsubj_generalizes_result det_result_This amod_sis2_non-lexical dep_add_generalizes dobj_add_sis2 advmod_add_then amod_Collins_1997 appos_dependencies_Collins amod_dependencies_head-head amod_dependencies_lexical conj_and_start_add prep_with_start_dependencies aux_start_to xcomp_been_add xcomp_been_start aux_been_has nsubj_been_progression nn_literature_parsing amod_literature_probabilistic det_literature_the prep_in_progression_literature det_progression_The ccomp_``_been
P03-1013	P97-1003	n	Section 5 presents an error analysis for Collinss -LRB- 1997 -RRB- lexicalized model which shows that the head-head dependencies used in this model fail to cope well with the flat structures in Negra	amod_structures_flat det_structures_the prep_in_cope_Negra prep_with_cope_structures advmod_cope_well aux_cope_to xcomp_fail_cope nsubj_fail_dependencies mark_fail_that det_model_this prep_in_used_model vmod_dependencies_used amod_dependencies_head-head det_dependencies_the ccomp_shows_fail nsubj_shows_which rcmod_model_shows amod_model_lexicalized nn_model_Collinss appos_Collinss_1997 prep_for_analysis_model nn_analysis_error det_analysis_an dobj_presents_analysis nsubj_presents_Section num_Section_5
P03-1013	P97-1003	p	-LRB- 1999 -RRB- and Bikel and Chiang -LRB- 2000 -RRB- has demonstrated the applicability of the Collins -LRB- 1997 -RRB- model for Czech and Chinese	conj_and_Czech_Chinese nn_model_Collins det_model_the appos_Collins_1997 prep_for_applicability_Chinese prep_for_applicability_Czech prep_of_applicability_model det_applicability_the dobj_demonstrated_applicability aux_demonstrated_has nsubj_demonstrated_Bikel nsubj_demonstrated_1999 appos_Chiang_2000 conj_and_Bikel_Chiang conj_and_1999_Chiang conj_and_1999_Bikel
P03-1013	P97-1003	p	However the learning curve for Negra -LRB- see Figure 1 -RRB- indicates that the performance of the Collins -LRB- 1997 -RRB- model is stable even for small training sets	nn_sets_training amod_sets_small prep_for_stable_sets advmod_stable_even cop_stable_is nsubj_stable_performance mark_stable_that nn_model_Collins det_model_the appos_Collins_1997 prep_of_performance_model det_performance_the ccomp_indicates_stable nsubj_indicates_curve advmod_indicates_However dep_1_Figure dep_see_1 appos_curve_see prep_for_curve_Negra amod_curve_learning det_curve_the
P03-1013	P97-1003	o	In Experiment 1 we applied three standard parsing models from the literature to Negra an unlexicalized PCFG model -LRB- the baseline -RRB- Carroll and Rooths -LRB- 1998 -RRB- head-lexicalized model and Collinss -LRB- 1997 -RRB- model based on head-head dependencies	amod_dependencies_head-head prep_on_based_dependencies vmod_model_based nn_model_Collinss appos_Collinss_1997 amod_model_head-lexicalized nn_model_Rooths nn_model_Carroll appos_Rooths_1998 conj_and_Carroll_Rooths det_baseline_the conj_and_model_model conj_and_model_model appos_model_baseline nn_model_PCFG amod_model_unlexicalized det_model_an det_literature_the nn_models_parsing amod_models_standard num_models_three dep_applied_model dep_applied_model dep_applied_model prep_to_applied_Negra prep_from_applied_literature dobj_applied_models nsubj_applied_we prep_in_applied_Experiment num_Experiment_1
P03-1013	P97-1003	o	Prominent among these properties is the semi-free Language Size LR LP Source English 40,000 87.4 % 88.1 % -LRB- Collins 1997 -RRB- Chinese 3,484 69.0 % 74.8 % -LRB- Bikel and Chiang 2000 -RRB- Czech 19,000 80.0 % -LRB- Collins et al. 1999 -RRB- Table 1 Results for the Collins -LRB- 1997 -RRB- model for various languages -LRB- dependency precision for Czech -RRB- wordorder i.e. German wordorder is fixed in some respects but variable in others	prep_in_variable_others det_respects_some prep_in_fixed_respects auxpass_fixed_is nsubjpass_fixed_wordorder amod_wordorder_German conj_but_wordorder_variable conj_but_wordorder_fixed dep_wordorder_i.e. dep_wordorder_Czech prep_for_precision_variable prep_for_precision_fixed prep_for_precision_wordorder nn_precision_dependency amod_languages_various appos_model_precision prep_for_model_languages nn_model_Collins det_model_the appos_Collins_1997 prep_for_Results_model dep_Table_Results num_Table_1 amod_Collins_1999 dep_Collins_al. nn_Collins_et num_%_80.0 num_%_19,000 nn_%_Czech dep_Bikel_2000 conj_and_Bikel_Chiang dep_%_% dep_%_Chiang dep_%_Bikel num_%_74.8 amod_%_% amod_%_Chinese dep_%_Collins dep_%_% number_%_69.0 number_%_3,484 dep_Collins_1997 num_%_88.1 amod_%_% amod_%_English number_%_87.4 number_%_40,000 dep_Source_Table dep_Source_Collins dep_Source_% nn_Source_LP nn_Source_LR nn_Source_Size nn_Source_Language amod_Source_semi-free det_Source_the nsubj_is_Source det_properties_these dep_Prominent_is prep_among_Prominent_properties dep_``_Prominent
P03-1013	P97-1003	p	3 Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank -LRB- e.g. Carroll and Rooth 1998 Charniak 1997 2000 Collins 1997 -RRB-	num_Collins_1997 num_Charniak_2000 num_Charniak_1997 num_Rooth_1998 dep_Carroll_Collins conj_and_Carroll_Charniak conj_and_Carroll_Rooth dep_e.g._Charniak dep_e.g._Rooth dep_e.g._Carroll dep_Treebank_e.g. nn_Treebank_Penn det_Treebank_the prep_for_performance_Treebank nn_performance_parsing dobj_improve_performance aux_improve_to xcomp_shown_improve auxpass_shown_been aux_shown_has nsubjpass_shown_Lexicalization nn_Lexicalization_Grammars nn_Lexicalization_Context-Free nn_Lexicalization_Probabilistic num_Lexicalization_3.1 rcmod_Models_shown nn_Models_Parsing nn_Models_Probabilistic num_Models_3
P03-1013	P97-1003	o	3.3 Collinss Head-Lexicalized Model In contrast to Carroll and Rooths -LRB- 1998 -RRB- approach the model proposed by Collins -LRB- 1997 -RRB- does not compute rule probabilities directly	nn_probabilities_rule advmod_compute_directly dobj_compute_probabilities neg_compute_not aux_compute_does nsubj_compute_model ccomp_compute_Model appos_Collins_1997 agent_proposed_Collins vmod_model_proposed det_model_the nn_approach_Rooths nn_approach_Carroll appos_Rooths_1998 conj_and_Carroll_Rooths prep_to_contrast_approach prep_in_Model_contrast nn_Model_Head-Lexicalized nn_Model_Collinss num_Model_3.3
P03-1013	P97-1003	p	1 Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years resulting in parsing models that achieve both broad coverage and high parsing accuracy -LRB- e.g. Collins 1997 Charniak 2000 -RRB-	num_Charniak_2000 dep_Collins_Charniak num_Collins_1997 pobj_e.g._Collins prep_-LRB-_e.g. nn_accuracy_parsing amod_accuracy_high conj_and_coverage_accuracy amod_coverage_broad preconj_coverage_both dobj_achieve_accuracy dobj_achieve_coverage nsubj_achieve_that rcmod_models_achieve nn_models_parsing prep_in_resulting_models amod_years_few amod_years_past det_years_the amod_research_intensive vmod_subject_resulting prep_over_subject_years prep_of_subject_research det_subject_the cop_subject_been aux_subject_has nsubj_subject_parsing amod_parsing_probabilistic amod_parsing_Treebank-based nn_parsing_Introduction num_parsing_1
P03-1013	P97-1003	o	The lexicalized model proposed by Collins -LRB- 1997 -RRB- -LRB- henceforth Collins model -RRB- was re-implemented by one of the authors	det_authors_the prep_of_one_authors prep_by_re-implemented_one cop_re-implemented_was nsubj_re-implemented_model nn_model_Collins nn_model_henceforth nn_model_Collins appos_Collins_1997 agent_proposed_model vmod_model_proposed amod_model_lexicalized det_model_The
P03-1055	P97-1003	o	First we extend the mechanism of adding gap variables for nodes dominating a site of discontinuity -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_discontinuity_Collins prep_of_site_discontinuity det_site_a dobj_dominating_site vmod_nodes_dominating nn_variables_gap prep_for_adding_nodes dobj_adding_variables prepc_of_mechanism_adding det_mechanism_the dobj_extend_mechanism nsubj_extend_we advmod_extend_First
P03-1055	P97-1003	o	The idea of threading EEs to their antecedents in a stochastic parser was proposed by Collins -LRB- 1997 -RRB- following the GPSG tradition -LRB- Gazdar et al. 1985 -RRB-	amod_Gazdar_1985 dep_Gazdar_al. nn_Gazdar_et dep_tradition_Gazdar nn_tradition_GPSG det_tradition_the appos_Collins_1997 prep_following_proposed_tradition agent_proposed_Collins auxpass_proposed_was nsubjpass_proposed_idea amod_parser_stochastic det_parser_a poss_antecedents_their amod_EEs_threading prep_in_idea_parser prep_to_idea_antecedents prep_of_idea_EEs det_idea_The
P03-1055	P97-1003	o	controlled NP-traces -LRB- NPNP -RRB- we follow the standard technique of marking nodes dominating the empty element up to but not including the parent of the antecedent as defective -LRB- missing an argument -RRB- with a gap feature -LRB- Gazdar et al. 1985 Collins 1997 -RRB- .1 Furthermore to make antecedent co-indexation possible with many types of EEs we generalize Collins approach by enriching the annotation of non-terminals with the type of the EE in question -LRB- eg	det_EE_the prep_of_type_EE det_type_the prep_of_annotation_non-terminals det_annotation_the dep_enriching_eg prep_in_enriching_question prep_with_enriching_type dobj_enriching_annotation nn_approach_Collins prepc_by_generalize_enriching dobj_generalize_approach nsubj_generalize_we prep_of_types_EEs amod_types_many prep_with_possible_types nsubj_possible_co-indexation amod_co-indexation_antecedent xcomp_make_possible aux_make_to advmod_.1_Furthermore num_.1_1997 dep_.1_Collins rcmod_Gazdar_generalize vmod_Gazdar_make dep_Gazdar_.1 appos_Gazdar_1985 dep_Gazdar_al. nn_Gazdar_et dep_feature_Gazdar nn_feature_gap det_feature_a det_argument_an dobj_missing_argument prep_as_antecedent_defective det_antecedent_the prep_of_parent_antecedent det_parent_the pobj_including_parent neg_including_not conj_but_to_including dep_up_including dep_up_to amod_element_empty det_element_the prep_dominating_up dobj_dominating_element vmod_nodes_dominating dobj_marking_nodes prepc_of_technique_marking amod_technique_standard det_technique_the prep_with_follow_feature dep_follow_missing dobj_follow_technique nsubj_follow_we vmod_follow_controlled appos_NP-traces_NPNP dep_controlled_NP-traces
P03-1055	P97-1003	n	However such constructions prove to be difficult for stochastic parsers -LRB- Collins et al. 1999 -RRB- and they either avoid tackling the problem -LRB- Charniak 2000 Bod 2003 -RRB- or only deal with a subset of the problematic cases -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_cases_Collins amod_cases_problematic det_cases_the prep_of_subset_cases det_subset_a prep_with_deal_subset advmod_deal_only dep_Bod_2003 dep_Charniak_Bod dep_Charniak_2000 det_problem_the conj_or_tackling_deal dep_tackling_Charniak dobj_tackling_problem xcomp_avoid_deal xcomp_avoid_tackling advmod_avoid_either nsubj_avoid_they amod_Collins_1999 dep_Collins_al. nn_Collins_et amod_parsers_stochastic conj_and_difficult_avoid dep_difficult_Collins prep_for_difficult_parsers cop_difficult_be aux_difficult_to xcomp_prove_avoid xcomp_prove_difficult nsubj_prove_constructions advmod_prove_However amod_constructions_such
P04-1043	P97-1003	o	The sentences were processed using Collins parser -LRB- Collins 1997 -RRB- to generate parse-trees automatically	advmod_generate_automatically dobj_generate_parse-trees aux_generate_to amod_Collins_1997 appos_parser_Collins nn_parser_Collins vmod_using_generate dobj_using_parser xcomp_processed_using auxpass_processed_were nsubjpass_processed_sentences det_sentences_The
P04-1047	P97-1003	o	Both techniques implement variations on the approaches of -LRB- Magerman 1994 -RRB- and -LRB- Collins 1997 -RRB- for the purpose of differentiating between complement and adjunct	conj_and_complement_adjunct prep_between_differentiating_adjunct prep_between_differentiating_complement prepc_of_purpose_differentiating det_purpose_the dep_Collins_1997 conj_and_Magerman_Collins amod_Magerman_1994 prep_for_approaches_purpose prep_of_approaches_Collins prep_of_approaches_Magerman det_approaches_the prep_on_variations_approaches dobj_implement_variations nsubj_implement_techniques det_techniques_Both ccomp_``_implement
P04-1058	P97-1003	o	One attempt to implement this idea is lexicalization increasing the information in the POS tag by adding the lemma to it -LRB- Collins 1997 Simaan 2000 -RRB-	amod_Simaan_2000 dep_Collins_Simaan amod_Collins_1997 dep_it_Collins det_lemma_the prep_to_adding_it dobj_adding_lemma nn_tag_POS det_tag_the prep_in_information_tag det_information_the prepc_by_increasing_adding dobj_increasing_information dep_lexicalization_increasing cop_lexicalization_is nsubj_lexicalization_attempt det_idea_this dobj_implement_idea aux_implement_to vmod_attempt_implement num_attempt_One
P04-1058	P97-1003	o	N-grams have been used extensively for this purpose -LRB- Collins 1996 1997 Eisner 1996 -RRB-	amod_Eisner_1996 dep_Collins_Eisner num_Collins_1997 num_Collins_1996 det_purpose_this dep_used_Collins prep_for_used_purpose advmod_used_extensively auxpass_used_been aux_used_have nsubjpass_used_N-grams
P04-1058	P97-1003	p	As a side product we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques -LRB- Collins 1997 Simaan 2000 -RRB- and parent annotation techniques -LRB- Klein and Manning 2003 -RRB- is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora	nn_corpora_training prep_from_induced_corpora vmod_automata_induced det_automata_the prep_in_reduction_automata prep_in_reduction_perplexity det_reduction_a prep_to_lead_reduction preconj_lead_both nsubj_lead_that rcmod_fact_lead det_fact_the prep_to_due_fact cop_due_is nsubj_due_effectiveness mark_due_that dep_Klein_2003 conj_and_Klein_Manning appos_techniques_Manning appos_techniques_Klein nn_techniques_annotation nn_techniques_parent dep_Simaan_2000 dep_Collins_Simaan amod_Collins_1997 conj_and_techniques_techniques dep_techniques_Collins nn_techniques_lexicalization nn_techniques_rule prep_of_effectiveness_techniques prep_of_effectiveness_techniques det_effectiveness_the ccomp_suggest_due aux_suggest_to vmod_evidence_suggest amod_evidence_empirical dobj_find_evidence nsubj_find_we prep_as_find_product nn_product_side det_product_a
P04-1058	P97-1003	p	They are central to many parsing models -LRB- Charniak 1997 Collins 1997 2000 Eisner 1996 -RRB- and despite their simplicity n-gram models have been very successful	advmod_successful_very cop_successful_been aux_successful_have nsubj_successful_models mark_successful_despite nn_models_n-gram nn_models_simplicity poss_models_their dep_Eisner_1996 num_Collins_2000 num_Collins_1997 dep_Charniak_Eisner conj_Charniak_Collins appos_Charniak_1997 appos_models_Charniak nn_models_parsing amod_models_many conj_and_central_successful prep_to_central_models cop_central_are nsubj_central_They ccomp_``_successful ccomp_``_central
P04-1082	P97-1003	o	Previous approaches to the problem -LRB- Collins 1997 Johnson 2002 Dienes and Dubey 2003a b Higgins 2003 -RRB- have all been learning-based the primary difference between the present algorithm and earlier ones is that it is not learned but explicitly incorporates principles of GovernmentBinding theory -LRB- Chomsky 1981 -RRB- since that theory underlies the annotation	det_annotation_the dobj_underlies_annotation nsubj_underlies_theory mark_underlies_since det_theory_that dep_Chomsky_1981 appos_theory_Chomsky nn_theory_GovernmentBinding prep_of_principles_theory dobj_incorporates_principles advmod_incorporates_explicitly nsubj_incorporates_difference neg_learned_not auxpass_learned_is nsubjpass_learned_it mark_learned_that advcl_is_underlies conj_but_is_incorporates ccomp_is_learned nsubj_is_difference amod_ones_earlier conj_and_algorithm_ones amod_algorithm_present det_algorithm_the prep_between_difference_ones prep_between_difference_algorithm amod_difference_primary det_difference_the parataxis_learning-based_incorporates parataxis_learning-based_is cop_learning-based_been dep_learning-based_all aux_learning-based_have dep_learning-based_approaches dep_Higgins_2003 dep_Dienes_Higgins conj_and_Dienes_b conj_and_Dienes_2003a conj_and_Dienes_Dubey num_Johnson_2002 dep_Collins_b dep_Collins_2003a dep_Collins_Dubey dep_Collins_Dienes conj_Collins_Johnson amod_Collins_1997 appos_problem_Collins det_problem_the prep_to_approaches_problem amod_approaches_Previous
P04-1082	P97-1003	o	1 Empty categories however seem different in that for the most part their location and existence is determined not by observable data but by explicitly constructed linguistic principles which 1 Both Collins -LRB- 1997 19 -RRB- and Higgins -LRB- 2003 100 -RRB- are explicit about this predisposition	det_predisposition_this prep_about_explicit_predisposition cop_explicit_are nsubj_explicit_1 dep_explicit_which dep_2003_100 dep_Higgins_2003 dep_1997_19 conj_and_Collins_Higgins dep_Collins_1997 preconj_Collins_Both pobj_1_Higgins pobj_1_Collins rcmod_principles_explicit amod_principles_linguistic amod_principles_constructed advmod_constructed_explicitly pobj_by_principles amod_data_observable auxpass_determined_is nsubjpass_determined_existence nsubjpass_determined_location conj_and_location_existence poss_location_their amod_part_most det_part_the conj_but_seem_by prep_by_seem_data neg_seem_not parataxis_seem_determined prep_for_seem_part prep_in_seem_that acomp_seem_different advmod_seem_however nsubj_seem_categories amod_categories_Empty num_categories_1
P04-1082	P97-1003	o	Collins -LRB- 1997 -RRB- Model 3 integrates the detection and resolution of WH-traces in relative clauses into a lexicalized PCFG	amod_PCFG_lexicalized det_PCFG_a amod_clauses_relative prep_of_detection_WH-traces conj_and_detection_resolution det_detection_the prep_into_integrates_PCFG prep_in_integrates_clauses dobj_integrates_resolution dobj_integrates_detection nsubj_integrates_Model num_Model_3 nn_Model_Collins appos_Collins_1997
P05-1006	P97-1003	o	In order to extract the linguistic features necessary for the models all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger -LRB- Ratnaparkhi 1998 -RRB- and parsed using the Collins parser -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_parser_Collins nn_parser_Collins det_parser_the dobj_using_parser xcomp_parsed_using amod_Ratnaparkhi_1998 dep_tagger_Ratnaparkhi amod_tagger_entropy nn_tagger_maximum det_tagger_a conj_and_using_parsed dobj_using_tagger dep_part-of-speech-tagged_parsed dep_part-of-speech-tagged_using advmod_part-of-speech-tagged_automatically cop_part-of-speech-tagged_were nsubj_part-of-speech-tagged_sentences advcl_part-of-speech-tagged_extract nn_word_target det_word_the dobj_containing_word vmod_sentences_containing det_sentences_all det_models_the prep_for_necessary_models amod_features_necessary amod_features_linguistic det_features_the dobj_extract_features aux_extract_to dep_extract_order mark_extract_In
P05-1018	P97-1003	p	We employ a robust statistical parser -LRB- Collins 1997 -RRB- to determine the constituent structure for each sentence from which subjects -LRB- s -RRB- objects -LRB- o -RRB- and relations other than subject or object -LRB- x -RRB- are identified	auxpass_identified_are nsubjpass_identified_relations nsubjpass_identified_objects nsubjpass_identified_subjects prep_from_identified_which appos_subject_x conj_or_subject_object mark_subject_than dep_other_object dep_other_subject amod_relations_other appos_objects_o conj_and_subjects_relations conj_and_subjects_objects appos_subjects_s det_sentence_each nn_structure_constituent det_structure_the ccomp_determine_identified prep_for_determine_sentence dobj_determine_structure aux_determine_to amod_Collins_1997 appos_parser_Collins amod_parser_statistical amod_parser_robust det_parser_a vmod_employ_determine dobj_employ_parser nsubj_employ_We
P05-1022	P97-1003	o	The Collins parser -LRB- Collins 1997 -RRB- does use dynamic programming in its search	poss_search_its amod_programming_dynamic prep_in_use_search dobj_use_programming aux_use_does nsubj_use_parser amod_Collins_1997 appos_parser_Collins nn_parser_Collins det_parser_The
P05-1036	P97-1003	o	We use a program to label syntactic arguments with the roles they are playing -LRB- Blaheta and Charniak 2000 -RRB- and the rules for complement/adjunct distinction given by -LRB- Collins 1997 -RRB- to never allow deletion of the complement	det_complement_the prep_of_deletion_complement dobj_allow_deletion neg_allow_never aux_allow_to amod_Collins_1997 agent_given_Collins vmod_distinction_allow vmod_distinction_given nn_distinction_complement/adjunct prep_for_rules_distinction det_rules_the dep_Blaheta_2000 conj_and_Blaheta_Charniak aux_playing_are nsubj_playing_they appos_roles_Charniak appos_roles_Blaheta rcmod_roles_playing det_roles_the prep_with_arguments_roles amod_arguments_syntactic dobj_label_arguments aux_label_to det_program_a conj_and_use_rules vmod_use_label dobj_use_program nsubj_use_We
P05-1038	P97-1003	n	In general these authors have found that existing lexicalized parsing models for English -LRB- e.g. Collins 1997 -RRB- do not straightforwardly generalize to new languages this typically manifests itself in a severe reduction in parsing performance compared to the results for English	prep_for_results_English det_results_the amod_performance_parsing pobj_reduction_results prepc_compared_to_reduction_to prep_in_reduction_performance amod_reduction_severe det_reduction_a prep_in_manifests_reduction dobj_manifests_itself advmod_manifests_typically nsubj_manifests_this amod_languages_new prep_to_generalize_languages advmod_generalize_straightforwardly neg_generalize_not aux_generalize_do nsubj_generalize_models mark_generalize_that nsubj_do_Collins advmod_do_e.g. amod_Collins_1997 prep_for_models_English nn_models_parsing amod_models_lexicalized amod_models_existing parataxis_found_manifests ccomp_found_generalize aux_found_have nsubj_found_authors prep_in_found_general det_authors_these
P05-1038	P97-1003	o	The lexicalized parsing experiments were run using Dan Bikels probabilistic parsing engine -LRB- Bikel 2002 -RRB- which in addition to replicating the models described by Collins -LRB- 1997 -RRB- also provides a convenient interface to develop corresponding parsing models for other languages	amod_languages_other prep_for_models_languages nn_models_parsing amod_models_corresponding dobj_develop_models aux_develop_to vmod_interface_develop amod_interface_convenient det_interface_a dobj_provides_interface advmod_provides_also prepc_in_addition_to_provides_replicating nsubj_provides_which appos_Collins_1997 agent_described_Collins vmod_models_described det_models_the dobj_replicating_models amod_Bikel_2002 rcmod_engine_provides dep_engine_Bikel nn_engine_parsing amod_engine_probabilistic nn_engine_Bikels nn_engine_Dan dobj_using_engine xcomp_run_using auxpass_run_were nsubjpass_run_experiments nn_experiments_parsing amod_experiments_lexicalized det_experiments_The
P05-1038	P97-1003	o	Furthermore Bikel -LRB- 2004 -RRB- provides evidence that lexical information -LRB- in the form of bi-lexical dependencies -RRB- only makes a small contribution to the performance of parsing models such as Collinss -LRB- 1997 -RRB-	appos_Collinss_1997 prep_such_as_models_Collinss nn_models_parsing prep_of_performance_models det_performance_the prep_to_contribution_performance amod_contribution_small det_contribution_a dobj_makes_contribution advmod_makes_only nsubj_makes_information mark_makes_that amod_dependencies_bi-lexical prep_of_form_dependencies det_form_the prep_in_information_form amod_information_lexical ccomp_evidence_makes dobj_provides_evidence nsubj_provides_Bikel advmod_provides_Furthermore appos_Bikel_2004
P05-1038	P97-1003	o	Moreover as Collins -LRB- 1997 -RRB- mentions some of the benefits of Model 2 are already captured by inclusion of the distance measure	nn_measure_distance det_measure_the prep_of_inclusion_measure agent_captured_inclusion advmod_captured_already auxpass_captured_are nsubjpass_captured_some advcl_captured_mentions advmod_captured_Moreover num_Model_2 prep_of_benefits_Model det_benefits_the prep_of_some_benefits nsubj_mentions_Collins mark_mentions_as appos_Collins_1997
P05-1038	P97-1003	p	Previous work for English -LRB- e.g. Magerman 1995 Collins 1997 -RRB- has shown that lexicalization leads to a sizable improvement in parsing performance	amod_performance_parsing prep_in_improvement_performance amod_improvement_sizable det_improvement_a prep_to_leads_improvement nsubj_leads_lexicalization mark_leads_that ccomp_shown_leads aux_shown_has nsubj_shown_Magerman advmod_shown_e.g. dep_Collins_1997 dep_Magerman_Collins dep_Magerman_1995 vmod_work_shown prep_for_work_English amod_work_Previous
P05-3015	P97-1003	o	2 We used the Collins parser -LRB- 1997 -RRB- to generate the constituency parse and a dependency converter -LRB- Hwa and Lopez 2004 -RRB- to obtain the dependency parse of English sentences	amod_sentences_English prep_of_parse_sentences nn_parse_dependency det_parse_the dobj_obtain_parse aux_obtain_to dep_Hwa_2004 conj_and_Hwa_Lopez appos_converter_Lopez appos_converter_Hwa nn_converter_dependency det_converter_a conj_and_parse_converter nn_parse_constituency det_parse_the xcomp_generate_obtain dobj_generate_converter dobj_generate_parse aux_generate_to appos_parser_1997 nn_parser_Collins det_parser_the vmod_used_generate dobj_used_parser nsubj_used_We rcmod_2_used ccomp_``_2
P06-1033	P97-1003	n	This is well illustrated by the Collins parser -LRB- Collins 1997 Collins 1999 -RRB- scrutinized by Bikel -LRB- 2004 -RRB- where several transformations are applied in order to improve the analysis of noun phrases coordination and punctuation	conj_and_phrases_punctuation conj_and_phrases_coordination nn_phrases_noun prep_of_analysis_punctuation prep_of_analysis_coordination prep_of_analysis_phrases det_analysis_the dobj_improve_analysis aux_improve_to dep_improve_order mark_improve_in advcl_applied_improve auxpass_applied_are nsubjpass_applied_transformations advmod_applied_where amod_transformations_several rcmod_Bikel_applied appos_Bikel_2004 agent_scrutinized_Bikel dep_Collins_1999 dep_Collins_Collins amod_Collins_1997 appos_parser_Collins nn_parser_Collins det_parser_the vmod_illustrated_scrutinized agent_illustrated_parser advmod_illustrated_well auxpass_illustrated_is nsubjpass_illustrated_This ccomp_``_illustrated
P06-1072	P97-1003	o	This was done for supervised parsing in different ways by Collins -LRB- 1997 -RRB- Klein and Manning -LRB- 2003 -RRB- and McDonald et al.	nn_al._et nn_al._McDonald appos_Manning_2003 conj_and_Collins_Manning conj_and_Collins_Klein appos_Collins_1997 prep_by_ways_Manning prep_by_ways_Klein prep_by_ways_Collins amod_ways_different prep_in_parsing_ways amod_parsing_supervised conj_and_done_al. prep_for_done_parsing auxpass_done_was nsubjpass_done_This
P06-2004	P97-1003	o	The supervised component is Collins parser -LRB- Collins 1997 -RRB- trained on the Wall Street Journal	nn_Journal_Street nn_Journal_Wall det_Journal_the prep_on_trained_Journal amod_Collins_1997 vmod_parser_trained dep_parser_Collins nn_parser_Collins cop_parser_is nsubj_parser_component amod_component_supervised det_component_The
P06-2004	P97-1003	o	6 Related Work Other work combining supervised and unsupervised learning for parsing includes -LRB- Charniak 1997 -RRB- -LRB- Johnson and Riezler 2000 -RRB- and -LRB- Schmid 2002 -RRB-	amod_Schmid_2002 dep_Johnson_2000 conj_and_Johnson_Riezler conj_and_Charniak_Schmid appos_Charniak_Riezler appos_Charniak_Johnson dep_Charniak_1997 dep_includes_Schmid dep_includes_Charniak nsubj_includes_work prep_for_learning_parsing dep_supervised_learning conj_and_supervised_unsupervised dobj_combining_unsupervised dobj_combining_supervised vmod_work_combining amod_work_Other rcmod_Work_includes amod_Work_Related num_Work_6
P06-2004	P97-1003	o	unlabeled R 100 % 20/08/199605 / 08/1997 -LRB- 351 days -RRB- 50 % 20/08/199617 / 02/1997 -LRB- 182 days -RRB- 10 % 20/08/199624 / 09/1996 -LRB- 36 days -RRB- labeled WSJ 50 % sections 0012 -LRB- 23412 sentences -RRB- 25 % lines 1 292960 -LRB- 11637 sentences -RRB- 5 % lines 1 58284 -LRB- 2304 sentences -RRB- 1 % lines 1 11720 -LRB- 500 sentences -RRB- 0.05 % lines 1 611 -LRB- 23 sentences -RRB- Table 1 Corpora used for the experiments unlabeled Reuters -LRB- R -RRB- corpus for attachment statistics labeled Penn treebank -LRB- WSJ -RRB- for training the Collins parser	nn_parser_Collins det_parser_the dobj_training_parser appos_treebank_WSJ nn_treebank_Penn prepc_for_labeled_training dobj_labeled_treebank vmod_statistics_labeled nn_statistics_attachment prep_for_corpus_statistics nn_corpus_Reuters amod_corpus_unlabeled appos_Reuters_R det_experiments_the dep_used_corpus prep_for_used_experiments nsubj_used_Corpora dep_Table_used num_Table_1 num_sentences_23 number_611_1 dep_lines_Table appos_lines_sentences num_lines_611 nn_lines_% num_lines_0.05 num_sentences_500 number_11720_1 dep_lines_lines appos_lines_sentences num_lines_11720 nn_lines_% num_lines_1 num_sentences_2304 number_58284_1 dep_lines_lines appos_lines_sentences num_lines_58284 nn_lines_% num_lines_5 num_sentences_11637 number_292960_1 dep_lines_lines appos_lines_sentences num_lines_292960 nn_lines_% num_lines_25 num_sentences_23412 dep_0012_lines appos_0012_sentences dep_sections_0012 amod_sections_% nn_sections_WSJ amod_sections_labeled num_sections_09/1996 number_%_50 num_days_36 appos_09/1996_days num_%_20/08/199624 num_%_10 num_%_02/1997 num_days_182 appos_02/1997_days num_%_20/08/199617 num_%_50 num_%_08/1997 num_days_351 appos_08/1997_days dep_%_sections dep_%_% dep_%_% num_%_20/08/199605 num_%_100 dep_R_% amod_R_unlabeled
P06-2004	P97-1003	o	Our baseline method for ambiguity resolution is the Collins parser as implemented by Bikel -LRB- Collins 1997 Bikel 2004 -RRB-	amod_Bikel_2004 dep_Collins_Bikel amod_Collins_1997 dep_Bikel_Collins prep_by_implemented_Bikel mark_implemented_as dep_parser_implemented nn_parser_Collins det_parser_the cop_parser_is nsubj_parser_method nn_resolution_ambiguity prep_for_method_resolution nn_method_baseline poss_method_Our
P06-2034	P97-1003	o	SCISSOR is implemented by augmenting Collins -LRB- 1997 -RRB- head-driven parsing model II to incorporate the generation of semantic labels on internal nodes	amod_nodes_internal amod_labels_semantic prep_on_generation_nodes prep_of_generation_labels det_generation_the dobj_incorporate_generation aux_incorporate_to vmod_model_incorporate num_model_II nn_model_parsing amod_model_head-driven num_model_1997 dep_Collins_model dobj_augmenting_Collins agent_implemented_augmenting auxpass_implemented_is nsubjpass_implemented_SCISSOR
P06-2041	P97-1003	o	History-based models for predicting the next parser action -LRB- Black et al. 1992 Magerman 1995 Ratnaparkhi 1997 Collins 1999 -RRB- 3	dep_3_Collins dep_Collins_1999 dep_Ratnaparkhi_3 conj_Ratnaparkhi_1997 dep_Magerman_Ratnaparkhi num_Magerman_1995 dep_al._Magerman appos_al._1992 nn_al._et amod_al._Black nn_action_parser amod_action_next det_action_the dobj_predicting_action dep_models_al. prepc_for_models_predicting amod_models_History-based
P06-2041	P97-1003	o	1 Introduction Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques usually employing some kind of dynamic programming in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser -LRB- Collins 1997 Collins 1999 Charniak 2000 -RRB-	amod_Charniak_2000 num_Collins_1999 dep_Collins_Charniak conj_Collins_Collins amod_Collins_1997 dep_parser_Collins det_parser_the agent_derived_parser vmod_analyses_derived nn_analyses_candidate prep_of_set_analyses det_set_the prep_of_ranking_set amod_ranking_n-best det_ranking_an dobj_provide_ranking nsubj_provide_that rcmod_models_provide amod_models_probabilistic amod_models_generative prep_with_combination_models amod_programming_dynamic prep_of_kind_programming det_kind_some prep_in_employing_combination dobj_employing_kind advmod_employing_usually nn_techniques_parsing amod_techniques_nondeterministic dep_based_employing prep_on_based_techniques auxpass_based_are nsubjpass_based_approaches amod_parsing_statistical prep_in_approaches_parsing nn_approaches_Mainstream nn_approaches_Introduction num_approaches_1
P06-2089	P97-1003	o	In the probabilistic LR model probabilities are assigned to tree 696 Precision Recall F-score Time -LRB- min -RRB- Best-First Classifier-Based -LRB- this paper -RRB- 88.1 87.8 87.9 17 Deterministic -LRB- MaxEnt -RRB- -LRB- this paper -RRB- 85.4 84.8 85.1 < 1 Charniak & Johnson -LRB- 2005 -RRB- 91.3 90.6 91.0 Unk Bod -LRB- 2003 -RRB- 90.8 90.7 90.7 145 * Charniak -LRB- 2000 -RRB- 89.5 89.6 89.5 23 Collins -LRB- 1999 -RRB- 88.3 88.1 88.2 39 Ratnaparkhi -LRB- 1997 -RRB- 87.5 86.3 86.9 Unk Tsuruoka & Tsujii -LRB- 2005 -RRB- deterministic 86.5 81.2 83.8 < 1 * Tsuruoka & Tsujii -LRB- 2005 -RRB- search 86.8 85.0 85.9 2 * Sagae & Lavie -LRB- 2005 -RRB- 86.0 86.1 86.0 11 * Table 1 Summary of results on labeled precision and recall of constituents and time required to parse the test set	nn_set_test det_set_the dobj_parse_set aux_parse_to xcomp_required_parse vmod_time_required prep_of_precision_constituents conj_and_precision_recall dobj_labeled_recall dobj_labeled_precision prepc_on_Summary_labeled prep_of_Summary_results num_Table_1 dep_Table_* dep_Table_11 dep_Table_Lavie dep_Table_Sagae number_11_86.0 dep_11_86.1 number_86.1_86.0 dep_Sagae_2005 conj_and_Sagae_Lavie dep_Sagae_* dep_Sagae_2 dep_Sagae_85.9 dep_Sagae_search number_85.9_85.0 number_85.9_86.8 dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii dep_Tsuruoka_* dep_Tsuruoka_1 dep_Tsuruoka_81.2 dep_Tsuruoka_deterministic quantmod_1_< number_1_83.8 number_81.2_86.5 appos_Tsujii_2005 conj_and_Tsuruoka_Tsujii nn_Tsuruoka_Unk num_Tsuruoka_86.9 number_86.9_86.3 number_86.9_87.5 dep_Ratnaparkhi_Tsujii dep_Ratnaparkhi_Tsuruoka dep_Ratnaparkhi_1997 num_Ratnaparkhi_39 num_Ratnaparkhi_88.2 number_88.2_88.1 number_88.2_88.3 dep_Collins_Ratnaparkhi appos_Collins_1999 num_Collins_23 dep_89.5_Collins dep_89.6_89.5 number_89.6_89.5 amod_Charniak_89.6 appos_Charniak_2000 dep_Charniak_* num_Charniak_145 npadvmod_90.7_Charniak num_90.7_90.7 number_90.7_90.8 dep_Bod_90.7 dep_Bod_2003 nn_Bod_Unk dep_Bod_91.0 dep_91.0_90.6 number_90.6_91.3 conj_and_2005_time dep_2005_Summary dep_2005_Table dep_2005_Tsujii dep_2005_Tsuruoka dep_2005_Bod dep_Johnson_time dep_Johnson_2005 conj_and_Charniak_Johnson num_Charniak_1 num_Charniak_85.4 dep_1_< dep_<_85.1 number_85.1_84.8 dep_85.4_Deterministic dep_85.4_87.8 det_paper_this dep_Deterministic_paper dep_Deterministic_MaxEnt dep_Deterministic_17 number_17_87.9 number_87.8_88.1 det_paper_this dep_Classifier-Based_Johnson dep_Classifier-Based_Charniak dep_Classifier-Based_paper amod_Classifier-Based_Best-First dep_Time_Classifier-Based appos_Time_min nn_Time_F-score dobj_Recall_Time nsubj_Recall_Precision aux_Recall_to num_Precision_696 nn_Precision_tree xcomp_assigned_Recall auxpass_assigned_are nsubjpass_assigned_probabilities prep_in_assigned_model nn_model_LR amod_model_probabilistic det_model_the rcmod_``_assigned
P06-2089	P97-1003	p	Some of the more popular and more accurate of these approaches to data-driven parsing -LRB- Charniak 2000 Collins 1997 Klein and Manning 2002 -RRB- have been based on generative models that are closely related to probabilistic contextfree grammars	nn_grammars_contextfree amod_grammars_probabilistic prep_to_related_grammars advmod_related_closely cop_related_are nsubj_related_that rcmod_models_related amod_models_generative prep_on_based_models auxpass_based_been aux_based_have nsubjpass_based_Some dep_Klein_2002 conj_and_Klein_Manning num_Collins_1997 dep_Charniak_Manning dep_Charniak_Klein dep_Charniak_Collins dep_Charniak_2000 appos_parsing_Charniak amod_parsing_data-driven prep_to_approaches_parsing det_approaches_these advmod_accurate_more prep_of_popular_approaches conj_and_popular_accurate advmod_popular_more amod_the_accurate amod_the_popular prep_of_Some_the
P06-2105	P97-1003	o	In order to objectively evaluate our representation we derived it from two different sources constituency parse trees -LRB- generated with our implementation of -LRB- Collins 1997 -RRB- -RRB- and dependency parse trees -LRB- created using Minipar -LRB- Lin 1998 -RRB- -RRB- 1	dep_Lin_1998 num_Minipar_1 dep_Minipar_Lin dobj_using_Minipar xcomp_created_using vmod_trees_created dobj_parse_trees vmod_dependency_parse conj_and_Collins_dependency dep_Collins_1997 prep_of_implementation_dependency prep_of_implementation_Collins poss_implementation_our prep_with_generated_implementation vmod_trees_generated nn_trees_parse nn_trees_constituency dep_sources_trees amod_sources_different num_sources_two prep_from_derived_sources dobj_derived_it nsubj_derived_we advcl_derived_evaluate poss_representation_our dobj_evaluate_representation advmod_evaluate_objectively aux_evaluate_to dep_evaluate_order mark_evaluate_In
P06-3009	P97-1003	o	6 Conclusion Traditional approaches for devising parsing models smoothing techniques and evaluation metrics are not well suited for MH as they presuppose 13The lack of head marking for instance precludes the use of lexicalized models a la -LRB- Collins 1997 -RRB-	amod_Collins_1997 nn_Collins_la det_Collins_a dep_models_Collins amod_models_lexicalized prep_of_use_models det_use_the dobj_precludes_use amod_head_marking prep_of_lack_head nn_lack_13The dep_presuppose_precludes prep_for_presuppose_instance dobj_presuppose_lack nsubj_presuppose_they mark_presuppose_as prep_for_suited_MH advmod_suited_well neg_suited_not auxpass_suited_are nsubjpass_suited_metrics nsubjpass_suited_techniques nn_metrics_evaluation conj_and_techniques_metrics amod_techniques_smoothing nn_models_parsing dobj_devising_models advcl_approaches_presuppose rcmod_approaches_suited prepc_for_approaches_devising amod_approaches_Traditional nn_approaches_Conclusion num_approaches_6 dep_``_approaches
P06-3009	P97-1003	p	Parsing models have been developed for different languages and state-of-the-art results have been reported for e.g. English -LRB- Collins 1997 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak appos_Collins_1997 dep_English_Collins dep_e.g._English ccomp_,_e.g. dep_reported_for auxpass_reported_been aux_reported_have amod_results_state-of-the-art conj_and_languages_results amod_languages_different dep_developed_reported prep_for_developed_results prep_for_developed_languages auxpass_developed_been aux_developed_have nsubjpass_developed_models amod_models_Parsing
P07-1011	P97-1003	o	To measure the coherence of sentences we use a statistical parser Toolkit -LRB- Collins 1997 -RRB- to assign each sentence a parsers score that is the related log probability of parsing	prep_of_probability_parsing nn_probability_log amod_probability_related det_probability_the cop_probability_is nsubj_probability_that rcmod_score_probability amod_score_parsers det_score_a det_sentence_each dobj_assign_score iobj_assign_sentence aux_assign_to amod_Collins_1997 appos_Toolkit_Collins nn_Toolkit_parser amod_Toolkit_statistical det_Toolkit_a vmod_use_assign dobj_use_Toolkit nsubj_use_we advcl_use_measure prep_of_coherence_sentences det_coherence_the dobj_measure_coherence aux_measure_To
P07-1019	P97-1003	o	The data set is same as in Section 5.1 except that we also parsed the English-side using a variant of the Collins -LRB- 1997 -RRB- parser and then extracted 24.7 M tree-to-string rules using the algorithm of -LRB- Galley et al. 2006 -RRB-	amod_Galley_2006 dep_Galley_al. nn_Galley_et dep_of_Galley prep_algorithm_of det_algorithm_the dobj_using_algorithm vmod_rules_using amod_rules_tree-to-string nn_rules_M num_rules_24.7 amod_rules_extracted advmod_extracted_then nn_parser_Collins det_parser_the appos_Collins_1997 prep_of_variant_parser det_variant_a dobj_using_variant det_English-side_the conj_and_parsed_rules vmod_parsed_using dobj_parsed_English-side advmod_parsed_also nsubj_parsed_we mark_parsed_that num_Section_5.1 pobj_in_Section pcomp_as_in prepc_except_same_rules prepc_except_same_parsed prep_same_as cop_same_is nsubj_same_set nn_set_data det_set_The ccomp_``_same
P07-1019	P97-1003	o	These forest rescoring algorithms have potential applications to other computationally intensive tasks involving combinations of different models for example head-lexicalized parsing -LRB- Collins 1997 -RRB- joint parsing and semantic role labeling -LRB- Sutton and McCallum 2005 -RRB- or tagging and parsing with nonlocal features	amod_features_nonlocal prep_with_parsing_features conj_and_tagging_parsing dep_Sutton_2005 conj_and_Sutton_McCallum nn_labeling_role amod_labeling_semantic conj_or_parsing_parsing conj_or_parsing_tagging dep_parsing_McCallum dep_parsing_Sutton conj_and_parsing_labeling amod_parsing_joint amod_Collins_1997 dep_parsing_Collins amod_parsing_head-lexicalized amod_models_different prep_of_combinations_models dobj_involving_combinations vmod_tasks_involving amod_tasks_intensive advmod_tasks_computationally amod_tasks_other prep_to_applications_tasks amod_applications_potential conj_have_tagging conj_have_labeling conj_have_parsing conj_have_parsing prep_for_have_example dobj_have_applications nsubj_have_algorithms nn_algorithms_rescoring nn_algorithms_forest det_algorithms_These
P07-1028	P97-1003	o	To determine headwords of the semantic roles the corpus was parsed using the Collins -LRB- 1997 -RRB- parser	nn_parser_Collins det_parser_the appos_Collins_1997 dobj_using_parser xcomp_parsed_using auxpass_parsed_was nsubjpass_parsed_corpus advcl_parsed_determine det_corpus_the amod_roles_semantic det_roles_the prep_of_headwords_roles dobj_determine_headwords aux_determine_To ccomp_``_parsed
P07-3002	P97-1003	o	In particular Hockenmaier and Steedman -LRB- 2001 -RRB- report a generative model for CCG parsing roughly akin to the Collins parser -LRB- Collins 1997 -RRB- specific to CCG	prep_to_specific_CCG dep_Collins_1997 amod_parser_specific appos_parser_Collins nn_parser_Collins det_parser_the prep_to_akin_parser advmod_akin_roughly acomp_parsing_akin vmod_CCG_parsing prep_for_model_CCG amod_model_generative det_model_a dobj_report_model nsubj_report_Steedman nsubj_report_Hockenmaier prep_in_report_particular appos_Steedman_2001 conj_and_Hockenmaier_Steedman
P08-1006	P97-1003	o	We first determine lexical heads of nonterminal nodes by using Bikels implementation of Collins head detection algorithm9 -LRB- Bikel 2004 Collins 1997 -RRB-	amod_Collins_1997 dep_Bikel_Collins dep_Bikel_2004 appos_algorithm9_Bikel nn_algorithm9_detection nn_algorithm9_head nn_algorithm9_Collins prep_of_implementation_algorithm9 nn_implementation_Bikels dobj_using_implementation amod_nodes_nonterminal prep_of_heads_nodes amod_heads_lexical prepc_by_determine_using dobj_determine_heads advmod_determine_first nsubj_determine_We ccomp_``_determine
P08-1021	P97-1003	o	After parsing the corpus -LRB- Collins 1997 -RRB- we artificially introduced verb form errors into these sentences and observed the resulting disturbances to the parse trees	nn_trees_parse det_trees_the prep_to_disturbances_trees amod_disturbances_resulting det_disturbances_the dobj_observed_disturbances det_sentences_these nn_errors_form conj_and_verb_observed prep_into_verb_sentences dobj_verb_errors dobj_introduced_observed dobj_introduced_verb advmod_introduced_artificially nsubj_introduced_we prepc_after_introduced_parsing amod_Collins_1997 appos_corpus_Collins det_corpus_the dobj_parsing_corpus
P08-1021	P97-1003	o	Using these patterns we introduced verb form errors into AQUAINT then re-parsed the corpus -LRB- Collins 1997 -RRB- and compiled the changes in the disturbed trees into a catalog	det_catalog_a amod_trees_disturbed det_trees_the prep_in_changes_trees det_changes_the prep_into_compiled_catalog dobj_compiled_changes amod_Collins_1997 appos_corpus_Collins det_corpus_the dobj_re-parsed_corpus advmod_re-parsed_then nn_errors_form conj_and_verb_compiled conj_and_verb_re-parsed prep_into_verb_AQUAINT dobj_verb_errors dobj_introduced_compiled dobj_introduced_re-parsed dobj_introduced_verb nsubj_introduced_we vmod_introduced_Using det_patterns_these dobj_Using_patterns
P08-1021	P97-1003	o	For example the sentence My father is * work in the laboratory is parsed -LRB- Collins 1997 -RRB- as -LRB- S -LRB- NP My father -RRB- -LRB- VP is -LRB- NP work -RRB- -RRB- -LRB- PP in the laboratory -RRB- -RRB- 2The abbreviations s -LRB- is or has -RRB- and d -LRB- would or had -RRB- compound the ambiguities	det_ambiguities_the nn_ambiguities_compound dep_ambiguities_had dep_ambiguities_would dep_ambiguities_d dep_ambiguities_s dep_ambiguities_as conj_or_would_had conj_or_is_has conj_and_s_d dep_s_has dep_s_is nn_s_abbreviations nn_s_2The dep_s_S det_laboratory_the prep_in_PP_laboratory nn_work_NP dep_is_work dep_VP_is poss_father_My dep_NP_father appos_S_PP appos_S_VP appos_S_NP amod_Collins_1997 dep_parsed_ambiguities dep_parsed_Collins auxpass_parsed_is nsubjpass_parsed_sentence prep_for_parsed_example det_laboratory_the prep_in_work_laboratory dep_work_* cop_work_is nsubj_work_father poss_father_My rcmod_sentence_work det_sentence_the rcmod_``_parsed
P08-1087	P97-1003	o	For getting the syntax trees the latest version of Collins parser -LRB- Collins 1997 -RRB- was used	auxpass_used_was nsubjpass_used_version prepc_for_used_getting amod_Collins_1997 appos_parser_Collins nn_parser_Collins prep_of_version_parser amod_version_latest det_version_the nn_trees_syntax det_trees_the dobj_getting_trees
P08-2035	P97-1003	o	3 We then run Collins parser -LRB- 1997 -RRB- using just the sentence pairs where parsing succeeds with a negative log likelihood below 200	prep_below_likelihood_200 nn_likelihood_log amod_likelihood_negative det_likelihood_a prep_with_succeeds_likelihood nsubj_succeeds_parsing advmod_succeeds_where rcmod_pairs_succeeds nn_pairs_sentence det_pairs_the advmod_pairs_just dobj_using_pairs appos_parser_1997 nn_parser_Collins dobj_run_parser advmod_run_then nsubj_run_We vmod_3_using rcmod_3_run ccomp_``_3
P98-1091	P97-1003	o	Examples of formalisms using this approach include the work of Magerman -LRB- 1995 -RRB- Charniak -LRB- 1997 -RRB- Collins -LRB- 1997 -RRB- and Goodman -LRB- 1997 -RRB-	appos_Goodman_1997 appos_Collins_1997 appos_Charniak_1997 conj_and_Magerman_Goodman conj_and_Magerman_Collins conj_and_Magerman_Charniak appos_Magerman_1995 prep_of_work_Goodman prep_of_work_Collins prep_of_work_Charniak prep_of_work_Magerman det_work_the dobj_include_work nsubj_include_Examples det_approach_this dobj_using_approach vmod_Examples_using prep_of_Examples_formalisms
P98-1106	P97-1003	o	-LRB- owenOcogentex.com -RRB- 1 Introduction Dependency grammar has a long tradition in syntactic theory dating back to at least Tesni ~ re 's work from the thirties3 Recently it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words -LRB- see e.g. -LRB- Collins 1997 -RRB- -RRB- which is what dependency grammars model explicitly do but context-free phrasestructure grammars do not	neg_do_not nsubj_do_grammars nn_grammars_phrasestructure amod_grammars_context-free advmod_do_explicitly nsubj_do_model nn_model_grammars nn_model_dependency det_model_what ccomp_is_do nsubj_is_which dep_Collins_1997 dep_see_Collins dep_see_e.g. rcmod_words_is dep_words_see prep_between_relations_words prep_of_importance_relations det_importance_the conj_but_discovering_do dobj_discovering_importance aux_discovering_are csubj_discovering_has prep_in_methods_parsing amod_methods_empirical amod_attention_renewed prep_as_gained_methods dobj_gained_attention aux_gained_has nsubj_gained_it det_thirties3_the advmod_work_Recently prep_from_work_thirties3 poss_work_re num_work_Tesni nn_re_~ quantmod_Tesni_at mwe_at_least prep_to_dating_work advmod_dating_back nn_theory_syntactic amod_tradition_long det_tradition_a ccomp_has_gained vmod_has_dating prep_in_has_theory dobj_has_tradition nsubj_has_grammar nn_grammar_Dependency nn_grammar_Introduction num_grammar_1 dep_grammar_owenOcogentex.com
P98-2148	P97-1003	o	Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models -LRB- Jelinek et al. 1994 Collins 1997 -RRB-	amod_Collins_1997 dep_Jelinek_Collins appos_Jelinek_1994 dep_Jelinek_al. nn_Jelinek_et appos_models_Jelinek amod_models_lexicalized dobj_proposed_models nsubj_proposed_researchers det_lexicon_the prep_of_importance_lexicon det_importance_the conj_and_pointed_proposed dobj_pointed_importance prt_pointed_out aux_pointed_have nsubj_pointed_researchers advmod_pointed_Recently det_researchers_some ccomp_``_proposed ccomp_``_pointed
P98-2184	P97-1003	o	1 Introduction The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers -LRB- Charniak 1995 Collins 1996/1997 Joshi and Srinivas 1994 Kim Srinivas and Trueswell 1997 Stolcke et al. 1997 -RRB- and in psychological theories of language processing -LRB- Clifton et al. 1984 Ferfeira & McClure 1997 Gamsey et al. 1997 Jurafsky 1996 MacDonald 1994 Mitchell & Holmes 1985 Tanenhaus et al. 1990 Trueswell et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Trueswell num_al._1990 nn_al._et nn_al._Tanenhaus nn_1985_Holmes nn_1985_Mitchell conj_and_Mitchell_Holmes num_MacDonald_1994 num_Jurafsky_1996 num_al._1997 nn_al._et nn_al._Gamsey num_Ferfeira_1997 conj_and_Ferfeira_McClure appos_al._al. dep_al._al. num_al._1985 appos_al._MacDonald appos_al._Jurafsky dep_al._al. dep_al._McClure dep_al._Ferfeira num_al._1984 nn_al._et nn_al._Clifton nn_processing_language prep_of_theories_processing amod_theories_psychological dep_in_al. pobj_in_theories num_al._1997 nn_al._et nn_al._Stolcke num_Trueswell_1997 num_Srinivas_1994 num_Collins_1996/1997 dep_1995_al. conj_and_1995_Trueswell conj_and_1995_Srinivas conj_and_1995_Kim conj_and_1995_Srinivas conj_and_1995_Joshi conj_and_1995_Collins amod_1995_Charniak appos_supertaggers_Trueswell appos_supertaggers_Srinivas appos_supertaggers_Kim appos_supertaggers_Srinivas appos_supertaggers_Joshi appos_supertaggers_Collins appos_supertaggers_1995 conj_and_parsers_supertaggers amod_parsers_statistical amod_parsers_modern amod_role_important det_role_an conj_and_plays_in prep_in_plays_supertaggers prep_in_plays_parsers dobj_plays_role nsubj_plays_relation dep_plays_Introduction poss_arguments_their conj_and_verbs_arguments prep_between_relation_arguments prep_between_relation_verbs amod_relation_probabilistic det_relation_The num_Introduction_1
P98-2234	P97-1003	o	et al. 1994 Brill and Resnik 1994 Collins and Brooks 1995 Merlo et al. 1997 -RRB-	num_Merlo_1997 nn_Merlo_al. nn_Merlo_et num_Brill_1995 conj_and_Brill_Brooks conj_and_Brill_Collins conj_and_Brill_1994 conj_and_Brill_Resnik dep_1994_Merlo conj_1994_Brooks conj_1994_Collins conj_1994_1994 conj_1994_Resnik conj_1994_Brill dep_1994_al. nn_al._et
P98-2234	P97-1003	n	The 75.4 % results may seen low compared to parsing results like the 88 % precision and recall in -LRB- Collins 1997 -RRB- but those parsing results include many easier-to-parse constructs	amod_constructs_easier-to-parse amod_constructs_many dobj_include_constructs nsubj_include_results amod_results_parsing det_results_those amod_Collins_1997 prep_in_precision_Collins conj_and_precision_recall amod_precision_% det_precision_the number_%_88 prep_like_results_recall prep_like_results_precision amod_results_parsing pobj_low_results prepc_compared_to_low_to conj_but_seen_include acomp_seen_low aux_seen_may nsubj_seen_results amod_results_% det_results_The number_%_75.4 ccomp_``_include ccomp_``_seen
P99-1059	P97-1003	p	Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars -LRB- Alshawi 1996 Eisner 1996 Charniak 1997 Collins 1997 -RRB-	amod_Collins_1997 dep_Charniak_Collins conj_Charniak_1997 num_Eisner_1996 dep_Alshawi_Charniak conj_Alshawi_Eisner dep_Alshawi_1996 dep_grammars_Alshawi amod_grammars_bilexical prep_of_versions_grammars amod_versions_weighted amod_versions_probabilistic conj_or_probabilistic_weighted prep_on_relying_versions nn_accuracy_parsing amod_accuracy_state-of-the-art prepc_by_improved_relying dobj_improved_accuracy aux_improved_have nsubj_improved_parsers amod_parsers_real-world amod_parsers_recent amod_parsers_Several ccomp_``_improved
P99-1059	P97-1003	n	three models in -LRB- Collins 1997 -RRB- are susceptible to the O -LRB- n 3 -RRB- method -LRB- cf.	nn_method_O num_n_3 appos_O_n det_O_the dep_susceptible_cf. prep_to_susceptible_method cop_susceptible_are nsubj_susceptible_models amod_Collins_1997 prep_in_models_Collins num_models_three
W00-0740	P97-1003	o	Giving the increasing sophistication of probabilistic linguistic models -LRB- for example Collins -LRB- 1997 -RRB- has a statistical approach to learning gap-threading rules -RRB- a probabilistic extension of our work is attractive -- it will be interesting to see how far an integration of ` logical ' and statistical can go	aux_go_can nsubj_go_integration advmod_go_far conj_and_logical_statistical prep_of_integration_statistical prep_of_integration_logical det_integration_an advmod_far_how ccomp_see_go aux_see_to xcomp_interesting_see cop_interesting_be aux_interesting_will nsubj_interesting_it cop_attractive_is nsubj_attractive_extension dep_attractive_for poss_work_our prep_of_extension_work amod_extension_probabilistic det_extension_a amod_rules_gap-threading dobj_learning_rules prepc_to_approach_learning amod_approach_statistical det_approach_a dobj_has_approach nsubj_has_Collins appos_Collins_1997 dep_for_has pobj_for_example amod_models_linguistic amod_models_probabilistic prep_of_sophistication_models amod_sophistication_increasing det_sophistication_the parataxis_Giving_interesting advcl_Giving_attractive dobj_Giving_sophistication ccomp_``_Giving
W00-0905	P97-1003	o	Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications -LRB- e.g. Carroll Minnen and Briscoe 1998 Charniak 1997 Collins 1996/1997 Joshi and Srinivas 1994 Kim Srinivas and Tmeswell 1997 Stolcke et al. 1997 -RRB- and psycholinguisfic models of language processing -LRB- e.g. Boland 1997 Clifton et al. 1984 Ferreira & McClure 1997 Fodor 1978 Garnsey et al. 1997 Jurafsky 1996 MacDonald 1994 Mitchell & Holmes 1985 Tanenhaus et al. 1990 Trueswell et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Trueswell num_al._1990 nn_al._et nn_al._Tanenhaus num_MacDonald_1994 num_Jurafsky_1996 num_al._1997 nn_al._et nn_al._Garnsey num_Fodor_1978 num_McClure_1997 nn_1984_al. num_Clifton_1984 nn_Clifton_et dep_1997_al. dep_1997_al. dep_1997_1985 conj_and_1997_Holmes conj_and_1997_Mitchell conj_and_1997_MacDonald conj_and_1997_Jurafsky conj_and_1997_al. conj_and_1997_Fodor conj_and_1997_McClure conj_and_1997_Ferreira conj_and_1997_Clifton dep_1997_Boland dep_1997_e.g. nn_processing_language prep_of_models_processing amod_models_psycholinguisfic dep_al._1997 nn_al._et nn_al._Stolcke num_Tmeswell_1997 num_Srinivas_1994 num_Collins_1996/1997 num_Charniak_1997 num_Briscoe_1998 dep_Carroll_Holmes dep_Carroll_Mitchell dep_Carroll_MacDonald dep_Carroll_Jurafsky dep_Carroll_al. dep_Carroll_Fodor dep_Carroll_McClure dep_Carroll_Ferreira dep_Carroll_Clifton dep_Carroll_1997 conj_and_Carroll_models conj_and_Carroll_al. conj_and_Carroll_Tmeswell conj_and_Carroll_Srinivas conj_and_Carroll_Kim conj_and_Carroll_Srinivas conj_and_Carroll_Joshi conj_and_Carroll_Collins conj_and_Carroll_Charniak conj_and_Carroll_Briscoe conj_and_Carroll_Minnen pobj_e.g._models pobj_e.g._al. pobj_e.g._Tmeswell pobj_e.g._Srinivas pobj_e.g._Kim pobj_e.g._Srinivas pobj_e.g._Joshi pobj_e.g._Collins pobj_e.g._Charniak pobj_e.g._Briscoe pobj_e.g._Minnen pobj_e.g._Carroll dep_applications_e.g. amod_applications_linguistic amod_applications_computational preconj_applications_both amod_role_important det_role_an prep_in_play_applications dobj_play_role nsubj_play_probabilities nn_probabilities_subcategorizafion nn_probabilities_Verb nn_probabilities_Introduction
W00-1201	P97-1003	o	2.1 Model 2 of -LRB- Collins 1997 -RRB- Both parsing models discussed in this paper inherit a great deal from this model so we briefly describe its progenitive features here describing only how each of the two models of this paper differ in the subsequent two sections	num_sections_two amod_sections_subsequent det_sections_the prep_in_differ_sections nsubj_differ_each advmod_differ_how advmod_differ_only det_paper_this prep_of_models_paper num_models_two det_models_the prep_of_each_models ccomp_describing_differ advmod_features_here amod_features_progenitive poss_progenitive_its vmod_describe_describing dobj_describe_features advmod_describe_briefly nsubj_describe_we mark_describe_so det_model_this amod_deal_great det_deal_a advcl_inherit_describe prep_from_inherit_model dobj_inherit_deal nsubj_inherit_models dep_inherit_Model det_paper_this prep_in_discussed_paper vmod_models_discussed nn_models_parsing dep_models_Both amod_Collins_1997 prep_of_Model_Collins num_Model_2 num_Model_2.1
W00-1201	P97-1003	o	The lexicalized PCFG that sits behind Model 2 of -LRB- Collins 1997 -RRB- has rules of the form P ~ LnLn-I ' LIHRI Rn-IRn -LRB- 1 -RRB- S -LRB- will-MD -RRB- NP -LRB- AppI ~ NNP -RRB- VP -LRB- wilI-MD -RRB- NNP I Apple MD VP -LRB- buy-VB -RRB- VB PRT -LRB- out-RP -RRB- NP -LRB- Microsoft -- NNP -RRB- I \ -LSB- I buy RP NNP I I out Microsoft Figure 1 A sample sentence with parse tree	nn_tree_parse prep_with_sentence_tree nn_sentence_sample det_sentence_A dep_Figure_sentence num_Figure_1 nn_Figure_Microsoft dep_I_Figure prt_I_out dep_I_I dep_NNP_I dep_RP_NNP dobj_buy_RP nsubj_buy_I ccomp_\_buy nsubj_\_I dep_Microsoft_NNP rcmod_NP_\ dep_NP_Microsoft nn_NP_PRT appos_PRT_out-RP nn_PRT_VB dep_PRT_buy-VB dep_VP_NP nn_VP_MD nn_VP_Apple dep_I_VP dep_NNP_I dep_VP_NNP appos_VP_wilI-MD num_NNP_~ appos_AppI_NNP dep_S_VP dep_S_AppI dep_S_NP appos_S_will-MD num_S_1 dep_Rn-IRn_S dep_Rn-IRn_has nn_LIHRI_form det_LIHRI_the num_LnLn-I_~ dep_P_LnLn-I dep_form_P prep_of_rules_LIHRI dobj_has_rules nsubj_has_PCFG amod_Collins_1997 prep_of_Model_Collins num_Model_2 prep_behind_sits_Model nsubj_sits_that rcmod_PCFG_sits amod_PCFG_lexicalized det_PCFG_The
W00-1201	P97-1003	o	In the BB.N model as with Model 2 of -LRB- Collins 1997 -RRB- modifying nonterminals are generated conditioning both on the parent P and its head child H. Unlike Model 2 of -LRB- Collins 1997 -RRB- they are also generated conditioning on the previously generated modifying nonterminal L / -1 or Pq-1 and there is no subcat frame or distance feature	nn_feature_distance conj_or_frame_feature nn_frame_subcat neg_frame_no nsubj_is_feature nsubj_is_frame expl_is_there conj_or_-1_Pq-1 dep_L_Pq-1 dep_L_-1 conj_and_nonterminal_is conj_and_nonterminal_L dobj_modifying_is dobj_modifying_L dobj_modifying_nonterminal xcomp_generated_modifying advmod_generated_previously det_generated_the prep_on_generated_generated dobj_generated_conditioning advmod_generated_also auxpass_generated_are nsubjpass_generated_they prep_generated_as prep_in_generated_model amod_Collins_1997 prep_of_Model_Collins num_Model_2 nn_H._child nn_H._head poss_H._its conj_and_P_H. nn_P_parent det_P_the prep_on_conditioning_H. prep_on_conditioning_P dep_conditioning_both dobj_generated_conditioning auxpass_generated_are nsubjpass_generated_Collins dobj_modifying_nonterminals vmod_Collins_modifying amod_Collins_1997 prepc_of_Model_generated num_Model_2 pobj_with_Model prep_unlike_as_Model pcomp_as_with nn_model_BB.N det_model_the
W00-1201	P97-1003	o	While the BBN model does not perform at the level of Model 2 of -LRB- Collins 1997 -RRB- on Wall Street Journal text it is also less language-dependent eschewing the distance metric -LRB- which relied on specific features of the English Treebank -RRB- in favor of the bigrams on nonterminals model	nn_model_bigrams det_model_the prep_on_bigrams_nonterminals prep_of_favor_model nn_Treebank_English det_Treebank_the prep_of_features_Treebank amod_features_specific prep_on_relied_features nsubj_relied_which dep_metric_relied amod_distance_metric det_distance_the prep_in_eschewing_favor dobj_eschewing_distance xcomp_language-dependent_eschewing advmod_language-dependent_less advmod_language-dependent_also cop_language-dependent_is nsubj_language-dependent_it advcl_language-dependent_perform nn_text_Journal nn_text_Street nn_text_Wall prep_on_Collins_text dep_Collins_1997 prep_of_Model_Collins num_Model_2 prep_of_level_Model det_level_the prep_at_perform_level neg_perform_not aux_perform_does nsubj_perform_model mark_perform_While nn_model_BBN det_model_the
W00-1201	P97-1003	o	During training each example is broken into elementary trees using head rules and argument/adjunct rules similar to those of -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_of_Collins prep_those_of prep_to_similar_those amod_rules_argument/adjunct amod_rules_similar conj_and_rules_rules nn_rules_head dobj_using_rules dobj_using_rules amod_trees_elementary xcomp_broken_using prep_into_broken_trees auxpass_broken_is nsubjpass_broken_example prep_during_broken_training det_example_each rcmod_``_broken
W00-1306	P97-1003	p	The success of recent high-quality parsers -LRB- Charniak 1997 Collins 1997 -RRB- relies on the availability of such treebank corpora	amod_corpora_treebank amod_corpora_such prep_of_availability_corpora det_availability_the prep_on_relies_availability nsubj_relies_success dep_Collins_1997 dep_Charniak_Collins dep_Charniak_1997 appos_parsers_Charniak amod_parsers_high-quality amod_parsers_recent prep_of_success_parsers det_success_The
W00-1307	P97-1003	p	We choose those sections because several state-of-thwart parsers -LRB- Collins 1997 Ratnaparkhi 1998 Charniak 1997 -RRB- are trained on Section 2-21 and tested on Section 23	num_Section_23 prep_on_tested_Section nsubjpass_tested_Ratnaparkhi num_Section_2-21 conj_and_trained_tested prep_on_trained_Section auxpass_trained_are nsubjpass_trained_Ratnaparkhi amod_Charniak_1997 dep_Ratnaparkhi_Charniak conj_Ratnaparkhi_1998 parataxis_Collins_tested parataxis_Collins_trained amod_Collins_1997 dep_parsers_Collins amod_parsers_state-of-thwart amod_parsers_several det_sections_those prep_because_choose_parsers dobj_choose_sections nsubj_choose_We ccomp_``_choose
W00-1307	P97-1003	o	A Head Percolation Table has previously been used in several statistical parsers -LRB- Magerman 1995 Collins 1997 -RRB- to find heads of phrases	prep_of_heads_phrases dobj_find_heads aux_find_to dep_Collins_1997 dep_Magerman_Collins dep_Magerman_1995 appos_parsers_Magerman amod_parsers_statistical amod_parsers_several xcomp_used_find prep_in_used_parsers auxpass_used_been advmod_used_previously aux_used_has nsubjpass_used_Table nn_Table_Percolation nn_Table_Head det_Table_A
W00-1307	P97-1003	o	Our strategy for choosing heads is similar to the one in -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_in_Collins prep_one_in det_one_the prep_to_similar_one cop_similar_is nsubj_similar_strategy dobj_choosing_heads prepc_for_strategy_choosing poss_strategy_Our ccomp_``_similar
W00-1320	P97-1003	o	Related to this issue we note that the head rules which were nearly identical to those used in -LRB- Collins 1997 -RRB- have not been tuned at all to this task	det_task_this prep_to_tuned_task prep_at_tuned_all auxpass_tuned_been neg_tuned_not aux_tuned_have nsubjpass_tuned_Collins mark_tuned_in amod_Collins_1997 advcl_used_tuned vmod_those_used prep_to_identical_those advmod_identical_nearly cop_identical_were nsubj_identical_which rcmod_rules_identical nn_rules_head det_rules_the prep_that_note_rules nsubj_note_we vmod_note_Related det_issue_this prep_to_Related_issue
W00-1320	P97-1003	o	Models 2 and 3 of -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_of_Collins conj_and_2_3 prep_Models_of dep_Models_3 dep_Models_2
W00-1320	P97-1003	o	This includes both the parsers that attach probabilities to parser moves -LRB- Magerman 1995 Ratnaparkhi 1997 -RRB- but also those of the lexicalized PCFG variety -LRB- Collins 1997 Charniak 1997 -RRB-	amod_Charniak_1997 dep_Collins_Charniak dep_Collins_1997 nn_variety_PCFG amod_variety_lexicalized det_variety_the appos_those_Collins prep_of_those_variety dep_Ratnaparkhi_1997 dep_Magerman_Ratnaparkhi appos_Magerman_1995 appos_moves_Magerman nn_moves_parser prep_to_attach_moves dobj_attach_probabilities nsubj_attach_that rcmod_parsers_attach det_parsers_the preconj_parsers_both conj_and_includes_those dobj_includes_parsers nsubj_includes_This
W01-0706	P97-1003	p	However since work in this direction has started a significant progress has also been made in the research on statistical learning of full parsers both in terms of accuracy and processing time -LRB- Charniak 1997b Charniak 1997a Collins 1997 Ratnaparkhi 1997 -RRB-	amod_Ratnaparkhi_1997 num_Collins_1997 dep_Charniak_Ratnaparkhi conj_Charniak_Collins conj_Charniak_1997a dep_Charniak_Charniak appos_Charniak_1997b nn_time_processing conj_and_accuracy_time prep_of_terms_time prep_of_terms_accuracy amod_parsers_full prep_of_learning_parsers amod_learning_statistical prep_on_research_learning det_research_the dep_made_Charniak prep_in_made_terms preconj_made_both prep_in_made_research auxpass_made_been advmod_made_also aux_made_has nsubjpass_made_progress advcl_made_started advmod_made_However amod_progress_significant det_progress_a aux_started_has nsubj_started_work mark_started_since det_direction_this prep_in_work_direction
W01-0706	P97-1003	p	For the full parser we use the one developed by Michael Collins -LRB- Collins 1996 Collins 1997 -RRB- one of the most accurate full parsers around	advmod_parsers_around amod_parsers_full amod_parsers_accurate det_parsers_the advmod_accurate_most prep_of_one_parsers dep_Collins_1997 dep_Collins_Collins dep_Collins_1996 appos_Collins_Collins nn_Collins_Michael dep_developed_one agent_developed_Collins vmod_one_developed det_one_the dobj_use_one nsubj_use_we prep_for_use_parser amod_parser_full det_parser_the
W01-0706	P97-1003	o	The reported results for the full parse tree -LRB- on section 23 -RRB- are recall/precision of 88.1 / 87.5 -LRB- Collins 1997 -RRB-	amod_Collins_1997 appos_87.5_Collins dep_88.1_87.5 prep_of_recall/precision_88.1 cop_recall/precision_are nsubj_recall/precision_results num_section_23 nn_tree_parse amod_tree_full det_tree_the prep_on_results_section prep_for_results_tree amod_results_reported det_results_The
W01-0706	P97-1003	o	Thus over the past few years along with advances in the use of learning and statistical methods for acquisition of full parsers -LRB- Collins 1997 Charniak 1997a Charniak 1997b Ratnaparkhi 1997 -RRB- significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship -LRB- Church 1988 Ramshaw and Marcus 1995 Argamon et al. 1998 Cardie and Pierce 1998 Munoz et al. 1999 Punyakanok and Roth 2001 Buchholz et al. 1999 Tjong Kim Sang and Buchholz 2000 -RRB-	amod_Buchholz_2000 conj_and_Sang_Buchholz nn_Sang_Kim nn_Sang_Tjong num_Buchholz_1999 nn_Buchholz_al. nn_Buchholz_et conj_and_Punyakanok_Buchholz conj_and_Punyakanok_Sang conj_and_Punyakanok_Buchholz conj_and_Punyakanok_2001 conj_and_Punyakanok_Roth num_Munoz_1999 nn_Munoz_al. nn_Munoz_et num_Argamon_1998 nn_Argamon_al. nn_Argamon_et num_Ramshaw_1998 conj_and_Ramshaw_Pierce conj_and_Ramshaw_Cardie conj_and_Ramshaw_Argamon conj_and_Ramshaw_1995 conj_and_Ramshaw_Marcus dep_Church_Sang dep_Church_Buchholz dep_Church_2001 dep_Church_Roth dep_Church_Punyakanok conj_Church_Munoz conj_Church_Pierce conj_Church_Cardie conj_Church_Argamon conj_Church_1995 conj_Church_Marcus conj_Church_Ramshaw appos_Church_1988 dep_relationship_Church amod_relationship_syntactic det_relationship_a prep_in_participate_relationship nsubj_participate_that rcmod_words_participate conj_or_phrases_words amod_phrases_syntactic nn_phrases_patterns amod_phrases_parsing amod_phrases_shallow dobj_recognize_words dobj_recognize_phrases aux_recognize_to nn_methods_learning amod_methods_statistical vmod_use_recognize prep_of_use_methods det_use_the prep_on_made_use auxpass_made_been aux_made_has nsubjpass_made_Collins amod_progress_significant dep_Ratnaparkhi_1997 appos_Charniak_1997b appos_Charniak_1997a appos_Collins_progress dep_Collins_Ratnaparkhi conj_Collins_Charniak conj_Collins_Charniak amod_Collins_1997 rcmod_parsers_made amod_parsers_full prep_of_acquisition_parsers nn_methods_statistical nn_methods_learning conj_and_learning_statistical prep_for_use_acquisition prep_of_use_methods det_use_the prep_in_advances_use pobj_,_advances prepc_along_with_,_with amod_years_few amod_years_past det_years_the pobj_over_years dep_,_over dep_``_Thus
W01-0707	P97-1003	o	So the sequence with a fork which corresponds to only one nucleus is treated as a three word sequence in model C. Apart from this difference model C directly relies on a combination of equations -LRB- 10 -RRB- and -LRB- 12 -RRB- namely conditioning by a80a7a81a49a82a9a12 a74a61a8a65a75a57a12 and a74a61a8a65a75a57a14a61a86 both the probability of generating a74a61a8a65a75 a47 and the one of generating a80a7a81a49a82 a47 Thus model C uses a reduced version of equation -LRB- 12 -RRB- and an extended version of 2Other models as -LRB- Collins and Brooks 1995 Merlo et al. 1998 -RRB- for PP-attachment resolution or -LRB- Collins 1997 Samuelsson 2000 -RRB- for probabilistic parsing are somewhat related but their supervised nature makes any direct comparison impossible	nsubj_impossible_comparison amod_comparison_direct det_comparison_any xcomp_makes_impossible nsubj_makes_nature amod_nature_supervised poss_nature_their conj_but_related_makes advmod_related_somewhat cop_related_are nsubj_related_Collins nsubj_related_Brooks nsubj_related_Collins mark_related_as amod_parsing_probabilistic dep_Samuelsson_2000 dep_Collins_1997 nn_resolution_PP-attachment num_Merlo_1998 nn_Merlo_al. nn_Merlo_et prep_for_Collins_parsing dep_Collins_Samuelsson conj_or_Collins_Collins prep_for_Collins_resolution dep_Collins_Merlo num_Collins_1995 conj_and_Collins_Brooks nn_models_2Other dep_version_makes dep_version_related prep_of_version_models amod_version_extended det_version_an appos_equation_12 prep_of_version_equation amod_version_reduced det_version_a conj_and_uses_version dobj_uses_version nsubj_uses_C advmod_uses_Thus nn_C_model nn_a47_a80a7a81a49a82 dobj_generating_a47 prepc_of_one_generating det_one_the conj_and_a47_one nn_a47_a74a61a8a65a75 dobj_generating_one dobj_generating_a47 prepc_of_probability_generating det_probability_the preconj_probability_both conj_and_a80a7a81a49a82a9a12_a74a61a8a65a75a57a14a61a86 conj_and_a80a7a81a49a82a9a12_a74a61a8a65a75a57a12 prep_by_conditioning_a74a61a8a65a75a57a14a61a86 prep_by_conditioning_a74a61a8a65a75a57a12 prep_by_conditioning_a80a7a81a49a82a9a12 advmod_conditioning_namely conj_and_equations_12 appos_equations_10 appos_combination_probability appos_combination_conditioning prep_of_combination_12 prep_of_combination_equations det_combination_a parataxis_relies_version parataxis_relies_uses prep_on_relies_combination advmod_relies_directly nsubj_relies_C advcl_relies_treated nn_C_model det_difference_this nn_C._model prep_in_sequence_C. nn_sequence_word num_sequence_three det_sequence_a prep_apart_from_treated_difference prep_as_treated_sequence auxpass_treated_is nsubjpass_treated_sequence mark_treated_So num_nucleus_one quantmod_one_only prep_to_corresponds_nucleus nsubj_corresponds_which rcmod_fork_corresponds det_fork_a prep_with_sequence_fork det_sequence_the
W01-0714	P97-1003	o	Because of these kinds of results the vast majority of statistical parsing work has focused on parsing as a supervised learning problem -LRB- Collins 1997 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak amod_Collins_1997 appos_problem_Collins nn_problem_learning amod_problem_supervised det_problem_a prep_as_focused_problem prep_on_focused_parsing aux_focused_has nsubj_focused_majority prep_because_of_focused_kinds nn_work_parsing amod_work_statistical prep_of_majority_work amod_majority_vast det_majority_the prep_of_kinds_results det_kinds_these
W01-0812	P97-1003	o	A statistical language model a lexicalized PCFG -LRB- similar to that of Collins 1997 -RRB- is derived from the analysis grammar by processing a corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule	det_rule_each agent_built_rule vmod_substructures_built prep_of_frequencies_substructures nn_frequencies_recording conj_and_model_frequencies amod_model_statistical neg_model_no amod_grammar_same det_grammar_the prep_with_using_frequencies prep_with_using_model dobj_using_grammar det_corpus_a vmod_processing_using dobj_processing_corpus nn_grammar_analysis det_grammar_the agent_derived_processing prep_from_derived_grammar auxpass_derived_is nsubjpass_derived_model dep_Collins_1997 prep_of_that_Collins prep_to_similar_that dep_PCFG_similar amod_PCFG_lexicalized det_PCFG_a dep_model_PCFG nn_model_language amod_model_statistical det_model_A ccomp_``_derived
W01-1403	P97-1003	o	-LRB- 1997 -RRB- and the English parser developed by Collins -LRB- 1997 -RRB-	appos_Collins_1997 agent_developed_Collins vmod_parser_developed nn_parser_English det_parser_the conj_and_1997_parser
W01-1403	P97-1003	o	We automatically converted the phrase structure output of the Collins parser into the syntactic dependency representation used by our syntactic realizer RealPro -LRB- Lavoie and Rambow 1997 -RRB-	dep_Lavoie_1997 conj_and_Lavoie_Rambow dep_RealPro_Rambow dep_RealPro_Lavoie appos_realizer_RealPro amod_realizer_syntactic poss_realizer_our agent_used_realizer vmod_representation_used nn_representation_dependency amod_representation_syntactic det_representation_the nn_parser_Collins det_parser_the prep_of_output_parser nn_output_structure nn_output_phrase det_output_the prep_into_converted_representation dobj_converted_output advmod_converted_automatically nsubj_converted_We ccomp_``_converted
W02-0813	P97-1003	o	In order to extract the linguistic features necessary for the model all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger -LRB- Ratnaparkhi 1998 -RRB- and parsed using the Collins parser -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_parser_Collins nn_parser_Collins det_parser_the dobj_using_parser xcomp_parsed_using amod_Ratnaparkhi_1998 dep_tagger_Ratnaparkhi amod_tagger_entropy nn_tagger_maximum det_tagger_a conj_and_using_parsed dobj_using_tagger dep_part-of-speech-tagged_parsed dep_part-of-speech-tagged_using advmod_part-of-speech-tagged_automatically ccomp_first_part-of-speech-tagged cop_first_were nsubj_first_sentences advcl_first_extract det_sentences_all det_model_the prep_for_necessary_model amod_features_necessary amod_features_linguistic det_features_the dobj_extract_features aux_extract_to dep_extract_order mark_extract_In
W02-1014	P97-1003	o	This is similar to -LRB- Collins 1997 -RRB- s and Charniak97s definition of a separate category for auxiliary verbs	amod_verbs_auxiliary prep_for_category_verbs amod_category_separate det_category_a prep_of_s_category dep_s_definition conj_and_s_Charniak97s dep_s_Collins dep_Collins_1997 prep_to_similar_Charniak97s prep_to_similar_s cop_similar_is nsubj_similar_This ccomp_``_similar
W02-1014	P97-1003	o	are the labeled parsing recall and precision respectively as defined in -LRB- Collins 1997 -RRB- -LRB- slightly different from -LRB- Black et al. 1991 -RRB- -RRB-	num_al._1991 nn_al._et amod_al._Black dep_from_al. amod_different_from advmod_different_slightly dep_Collins_different dep_Collins_1997 prep_in_defined_Collins mark_defined_as dep_recall_defined advmod_recall_respectively conj_and_recall_precision nn_recall_parsing amod_recall_labeled det_recall_the cop_recall_are
W02-1504	P97-1003	o	The most common answer is component testing where the component is compared against a standard of goodness usually the Penn Treebank for English -LRB- Marcus et al. 1993 -RRB- allowing a numerical score of precision and recall -LRB- e.g. Collins 1997 -RRB-	num_Collins_1997 pobj_e.g._Collins conj_and_precision_recall prep_score_e.g. prep_of_score_recall prep_of_score_precision amod_score_numerical det_score_a dobj_allowing_score nsubj_allowing_Treebank advmod_allowing_usually amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus prep_for_Treebank_English nn_Treebank_Penn det_Treebank_the prep_of_standard_goodness det_standard_a prep_against_compared_standard auxpass_compared_is nsubjpass_compared_component advmod_compared_where det_component_the vmod_testing_allowing rcmod_testing_compared nn_testing_component cop_testing_is nsubj_testing_answer amod_answer_common det_answer_The advmod_common_most ccomp_``_testing
W02-2030	P97-1003	o	In most recent parsing work the history consists of a small number of manually selected features -LRB- Charniak 1997 Collins 1997 -RRB-	amod_Collins_1997 dep_Charniak_Collins appos_Charniak_1997 dep_features_Charniak amod_features_selected advmod_selected_manually prep_of_number_features amod_number_small det_number_a prep_of_consists_number nsubj_consists_history prep_in_consists_work det_history_the nn_work_parsing amod_work_recent amod_work_most
W02-2030	P97-1003	o	This model is very similar to the markovized rule models in Collins -LRB- 1997 -RRB-	appos_Collins_1997 prep_in_models_Collins nn_models_rule amod_models_markovized det_models_the prep_to_similar_models advmod_similar_very cop_similar_is nsubj_similar_model det_model_This
W03-0401	P97-1003	o	Using this model we can assign consistent probabilities to parsing results with complex structures such as ones represented with feature structures -LRB- Abney 1997 Johnson et al. 1999 -RRB-	num_Johnson_1999 nn_Johnson_al. nn_Johnson_et dep_Abney_Johnson dep_Abney_1997 appos_structures_Abney nn_structures_feature prep_with_represented_structures vmod_ones_represented prep_such_as_structures_ones amod_structures_complex prep_with_results_structures amod_results_parsing amod_probabilities_consistent prep_to_assign_results dobj_assign_probabilities aux_assign_can nsubj_assign_we vmod_assign_Using det_model_this dobj_Using_model
W03-0401	P97-1003	p	2.1 Lexicalized parse trees The first successful work on syntactic disambiguation was based on lexicalized probabilistic context-free grammar -LRB- LPCFG -RRB- -LRB- Collins 1997 Charniak 1997 -RRB-	amod_Charniak_1997 dep_Collins_Charniak appos_Collins_1997 dep_grammar_Collins appos_grammar_LPCFG amod_grammar_context-free amod_grammar_probabilistic amod_grammar_lexicalized prep_on_based_grammar auxpass_based_was nsubjpass_based_work amod_disambiguation_syntactic prep_on_work_disambiguation amod_work_successful amod_work_first det_work_The rcmod_trees_based nn_trees_parse amod_trees_Lexicalized num_trees_2.1 ccomp_``_trees
W03-0401	P97-1003	o	However existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars -LRB- LPCFG -RRB- -LRB- Collins 1996 Collins 1997 Charniak 1997 -RRB- which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies	det_dependencies_the prep_of_independence_dependencies prep_of_assumption_independence det_assumption_the prep_under_sentence_assumption det_sentence_a prep_in_words_sentence num_words_two prep_of_dependencies_words amod_dependencies_syntactic/semantic det_dependencies_the prep_into_results_dependencies nsubj_results_Collins prep_of_decomposition_parsing det_decomposition_the prep_on_based_decomposition auxpass_based_are nsubjpass_based_which dep_Charniak_1997 num_Collins_1997 rcmod_Collins_based dep_Collins_Charniak conj_Collins_Collins amod_Collins_1996 rcmod_grammars_results appos_grammars_LPCFG amod_grammars_context-free amod_grammars_probabilistic amod_grammars_lexicalized prep_of_extension_grammars amod_extension_mere det_extension_a cop_extension_are nsubj_extension_models advmod_extension_However amod_grammars_lexicalized prep_with_models_grammars prep_of_models_disambiguation amod_models_existing
W03-0401	P97-1003	o	Since an existing study incorporates these relations ad hoc -LRB- Collins 1997 -RRB- they are apparently crucial in accurate disambiguation	amod_disambiguation_accurate prep_in_crucial_disambiguation advmod_crucial_apparently cop_crucial_are nsubj_crucial_they advcl_crucial_incorporates num_Collins_1997 appos_hoc_Collins nn_hoc_ad dep_relations_hoc det_relations_these dobj_incorporates_relations nsubj_incorporates_study mark_incorporates_Since amod_study_existing det_study_an
W03-0401	P97-1003	o	what does student want to write your Figure 3 A derivation tree of lexicalized parse trees such as the distinction of arguments/modifiers and unbounded dependencies -LRB- Collins 1997 -RRB- are elegantly represented in derivation trees	nn_trees_derivation prep_in_represented_trees advmod_represented_elegantly auxpass_represented_are nsubjpass_represented_tree amod_Collins_1997 amod_dependencies_unbounded conj_and_arguments/modifiers_dependencies dep_distinction_Collins prep_of_distinction_dependencies prep_of_distinction_arguments/modifiers det_distinction_the nn_trees_parse amod_trees_lexicalized prep_such_as_tree_distinction prep_of_tree_trees nn_tree_derivation det_tree_A dep_tree_want num_Figure_3 poss_Figure_your dobj_write_Figure aux_write_to xcomp_want_write nsubj_want_student aux_want_does dobj_want_what
W03-0403	P97-1003	o	Most statistical parsing research such as Collins -LRB- 1997 -RRB- has centered on training probabilistic context-free grammars using the Penn Treebank	nn_Treebank_Penn det_Treebank_the dobj_using_Treebank amod_grammars_context-free amod_grammars_probabilistic nn_grammars_training xcomp_centered_using prep_on_centered_grammars aux_centered_has nsubj_centered_research appos_Collins_1997 prep_such_as_research_Collins nn_research_parsing amod_research_statistical amod_research_Most
W03-0501	P97-1003	o	-LRB- 1998 -RRB- the BBN parser builds augmented parse trees according to a process similar to that described in Collins -LRB- 1997 -RRB-	appos_Collins_1997 prep_in_described_Collins vmod_that_described prep_to_similar_that amod_process_similar det_process_a nn_trees_parse amod_trees_augmented pobj_builds_process prepc_according_to_builds_to dobj_builds_trees nsubj_builds_parser dep_builds_1998 nn_parser_BBN det_parser_the
W03-0501	P97-1003	o	2 Previous Work Other researchers have investigated the topic of automatic generation of abstracts but the focus has been different e.g. sentence extraction -LRB- Edmundson 1969 Johnson et al 1993 Kupiec et al. 1995 Mann et al. 1992 Teufel and Moens 1997 Zechner 1995 -RRB- processing of structured templates -LRB- Paice and Jones 1993 -RRB- sentence compression -LRB- Hori et al. 2002 Knight and Marcu 2001 Grefenstette 1998 Luhn 1958 -RRB- and generation of abstracts from multiple sources -LRB- Radev and McKeown 1998 -RRB-	dep_Radev_1998 conj_and_Radev_McKeown dep_sources_McKeown dep_sources_Radev amod_sources_multiple prep_from_abstracts_sources prep_of_generation_abstracts dep_Grefenstette_1958 appos_Grefenstette_Luhn amod_Grefenstette_1998 conj_and_Knight_generation conj_and_Knight_Grefenstette conj_and_Knight_2001 conj_and_Knight_Marcu dep_Hori_generation dep_Hori_Grefenstette dep_Hori_2001 dep_Hori_Marcu dep_Hori_Knight appos_Hori_2002 dep_Hori_al. nn_Hori_et dep_compression_Hori nn_compression_sentence dep_Paice_1993 conj_and_Paice_Jones dep_templates_Jones dep_templates_Paice amod_templates_structured prep_of_processing_templates dep_Zechner_1995 appos_Teufel_compression appos_Teufel_processing dep_Teufel_Zechner conj_and_Teufel_1997 conj_and_Teufel_Moens num_Mann_1992 nn_Mann_al. nn_Mann_et num_Kupiec_1995 nn_Kupiec_al. nn_Kupiec_et appos_al_1993 nn_al_et nn_al_Johnson dep_Edmundson_1997 dep_Edmundson_Moens dep_Edmundson_Teufel conj_Edmundson_Mann conj_Edmundson_Kupiec conj_Edmundson_al dep_Edmundson_1969 dep_extraction_Edmundson nn_extraction_sentence conj_different_extraction dep_different_e.g. cop_different_been aux_different_has nsubj_different_focus det_focus_the prep_of_generation_abstracts amod_generation_automatic prep_of_topic_generation det_topic_the dobj_investigated_topic aux_investigated_have nsubj_investigated_researchers amod_researchers_Other conj_but_Work_different rcmod_Work_investigated amod_Work_Previous num_Work_2 dep_``_different dep_``_Work
W03-1007	P97-1003	o	Features For each frame element features are extracted from the surface text of the sentence and from an automatically generated syntactic parse tree -LRB- Collins 1997 -RRB-	amod_Collins_1997 appos_tree_Collins nn_tree_parse amod_tree_syntactic amod_tree_generated det_tree_an advmod_generated_automatically pobj_from_tree det_sentence_the prep_of_text_sentence nn_text_surface det_text_the conj_and_extracted_from prep_from_extracted_text auxpass_extracted_are nsubjpass_extracted_features ccomp_extracted_Features nn_element_frame det_element_each prep_for_Features_element
W03-1025	P97-1003	o	Bikel and Chiang -LRB- 2000 -RRB- in fact contains two parsers one is a lexicalized probabilistic contextfree grammar -LRB- PCFG -RRB- similar to -LRB- Collins 1997 -RRB- the other is based on statistical TAG -LRB- Chiang 2000 -RRB-	dep_Chiang_2000 dep_TAG_Chiang amod_TAG_statistical prep_on_based_TAG auxpass_based_is nsubjpass_based_other det_other_the amod_Collins_1997 dep_to_Collins prep_similar_to parataxis_grammar_based amod_grammar_similar appos_grammar_PCFG nn_grammar_contextfree amod_grammar_probabilistic amod_grammar_lexicalized det_grammar_a cop_grammar_is nsubj_grammar_one dep_parsers_grammar num_parsers_two dobj_contains_parsers nsubj_contains_Chiang nsubj_contains_Bikel appos_Chiang_2000 prep_in_Bikel_fact conj_and_Bikel_Chiang
W03-1706	P97-1003	o	To make the model more practical in parameter estimation we assume the features in feature set FS are independent from each other thus = FSFi AFiPAFSP -RRB- | -LRB- -RRB- | -LRB- -LRB- 5 -RRB- Under this PCFG+PF model the goal of a parser is to choose a parse that maximizes the following score -RRB- | -LRB- maxarg -RRB- | -LRB- 1 AFS i i i n i T PSTScore = = -LRB- 6 -RRB- Our model is thus a simplification of more sophisticated models which integrate PCFGs with features such as those in Magerman -LRB- 1995 -RRB- Collins -LRB- 1997 -RRB- and Goodman -LRB- 1997 -RRB-	appos_Goodman_1997 appos_Collins_1997 conj_and_Magerman_Goodman conj_and_Magerman_Collins appos_Magerman_1995 prep_in_those_Goodman prep_in_those_Collins prep_in_those_Magerman prep_such_as_features_those prep_with_integrate_features dobj_integrate_PCFGs nsubj_integrate_which rcmod_models_integrate amod_models_sophisticated advmod_sophisticated_more prep_of_simplification_models det_simplification_a advmod_simplification_thus cop_simplification_is nsubj_simplification_PSTScore dep_simplification_T dep_simplification_i dep_simplification_n dep_simplification_i dep_simplification_i nn_simplification_i nn_simplification_AFS nn_simplification_| poss_model_Our dep_=_model dep_=_6 prep_=_= amod_PSTScore_= num_AFS_1 nn_|_maxarg dep_|_simplification amod_score_following det_score_the dobj_maximizes_score nsubj_maximizes_that rcmod_parse_maximizes det_parse_a dobj_choose_parse aux_choose_to xcomp_is_choose nsubj_is_goal det_parser_a prep_of_goal_parser det_goal_the nn_model_PCFG+PF det_model_this advmod_|_| rcmod_|_is prep_under_|_model appos_|_5 dep_|_| nn_AFiPAFSP_FSFi dep_=_| dep_=_AFiPAFSP det_other_each prep_from_independent_other cop_independent_are nsubj_independent_features nn_FS_set nn_FS_feature prep_in_features_FS det_features_the parataxis_assume_= advmod_assume_thus ccomp_assume_independent nsubj_assume_we advcl_assume_make nn_estimation_parameter advmod_practical_more nsubj_practical_model det_model_the prep_in_make_estimation xcomp_make_practical aux_make_To
W03-1707	P97-1003	p	The creation of the Penn English Treebank -LRB- Marcus et al. 1993 -RRB- a syntactically interpreted corpus played a crucial role in the advances in natural language parsing technology -LRB- Collins 1997 Collins 2000 Charniak 2000 -RRB- for English	dep_Charniak_2000 num_Collins_2000 prep_for_Collins_English dep_Collins_Charniak conj_Collins_Collins amod_Collins_1997 dep_technology_Collins nn_technology_parsing nn_technology_language amod_technology_natural prep_in_advances_technology det_advances_the amod_role_crucial det_role_a prep_in_played_advances dobj_played_role nsubj_played_creation amod_corpus_interpreted det_corpus_a advmod_interpreted_syntactically amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_English nn_Treebank_Penn det_Treebank_the appos_creation_corpus dep_creation_Marcus prep_of_creation_Treebank det_creation_The
W03-1711	P97-1003	p	Function P R F Speed Partial Parsing 85.1 82.5 83.8 4500 wps Full Parsing 77.1 70.3 73.7 2100 wps Table 3 Performances of 1 st level Partial Parsing and Full Parsing -LRB- wps words per second -RRB- Table 3 shows that the performances of partial parsing and full parsing are quite low compared to those of state-of-art partial parsing and full parsing for the English language -LRB- Zhou et al 2000a Collins 1997 -RRB-	num_Collins_1997 nn_2000a_al dep_Zhou_Collins dep_Zhou_2000a nn_Zhou_et nn_language_English det_language_the amod_parsing_full conj_and_parsing_parsing amod_parsing_partial amod_parsing_state-of-art prep_for_those_language prep_of_those_parsing prep_of_those_parsing dep_low_Zhou pobj_low_those prepc_compared_to_low_to advmod_low_quite cop_low_are nsubj_low_performances mark_low_that amod_parsing_full conj_and_parsing_parsing amod_parsing_partial prep_of_performances_parsing prep_of_performances_parsing det_performances_the num_shows_3 nn_shows_Table dep_shows_wps nn_shows_Parsing prep_per_words_second dep_wps_words amod_Parsing_Full dep_Parsing_low conj_and_Parsing_shows amod_Parsing_Partial nn_Parsing_level conj_1_shows conj_1_Parsing dep_1_st prep_of_Performances_1 dep_Table_Performances num_Table_3 nn_Table_wps num_Table_2100 num_Table_73.7 num_Table_70.3 number_70.3_77.1 dobj_Parsing_Table dep_Full_Parsing amod_wps_Full num_wps_4500 num_wps_83.8 num_wps_82.5 number_82.5_85.1 dobj_Parsing_wps xcomp_Partial_Parsing amod_Speed_Partial nn_Speed_F nn_Speed_R nn_Speed_P nn_Speed_Function
W04-0308	P97-1003	o	Parsers that attempt to disambiguate the input completely full parsing typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis -LRB- Collins 1997 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak appos_Collins_1997 dep_analysis_Collins amod_analysis_probable det_analysis_the advmod_probable_most dobj_select_analysis aux_select_to dep_select_order mark_select_in amod_model_top-down amod_model_probabilistic det_model_a advcl_applies_select dobj_applies_model advmod_applies_then nsubj_applies_that nn_forest_parse amod_forest_packed det_forest_a dobj_derive_forest aux_derive_to nn_algorithm_programming amod_algorithm_dynamic prep_of_kind_algorithm det_kind_some xcomp_employ_derive dobj_employ_kind advmod_employ_first advmod_employ_typically nsubj_employ_parsing advcl_employ_disambiguate amod_parsing_full advmod_full_completely det_input_the dobj_disambiguate_input aux_disambiguate_to conj_and_attempt_applies ccomp_attempt_employ nsubj_attempt_that rcmod_Parsers_applies rcmod_Parsers_attempt
W04-0819	P97-1003	o	These sentences were parsed with the Collins parser -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_parser_Collins nn_parser_Collins det_parser_the prep_with_parsed_parser auxpass_parsed_were nsubjpass_parsed_sentences det_sentences_These ccomp_``_parsed
W04-0845	P97-1003	o	Head word -LRB- and its part-of-speech tag -RRB- of the constituent After POS tagging a syntactic parser -LRB- Collins 1997 -RRB- was then used to obtain the parse tree for the sentence	det_sentence_the nn_tree_parse det_tree_the prep_for_obtain_sentence dobj_obtain_tree aux_obtain_to xcomp_used_obtain advmod_used_then auxpass_used_was nsubjpass_used_parser amod_Collins_1997 appos_parser_Collins amod_parser_syntactic det_parser_a nn_tagging_POS det_constituent_the amod_tag_part-of-speech poss_tag_its cc_tag_and rcmod_word_used prep_after_word_tagging prep_of_word_constituent appos_word_tag nn_word_Head
W04-1503	P97-1003	o	Recently it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words -LRB- see e.g. -LRB- Collins 1997 -RRB- -RRB- which is what dependency grammars model explicitly but context-free phrase-structure grammars do not	neg_do_not dep_grammars_do amod_grammars_phrase-structure amod_grammars_context-free conj_but_model_grammars advmod_model_explicitly nn_model_grammars nn_model_dependency det_model_what dobj_is_grammars dobj_is_model nsubj_is_which dep_Collins_1997 dep_see_Collins dep_see_e.g. rcmod_words_is dep_words_see prep_between_relations_words prep_of_importance_relations det_importance_the dobj_emphasized_importance aux_emphasized_have nsubj_emphasized_methods mark_emphasized_as prep_in_methods_parsing amod_methods_empirical amod_attention_renewed advcl_gained_emphasized dobj_gained_attention aux_gained_has nsubj_gained_it advmod_gained_Recently
W04-2002	P97-1003	p	Of particular interest are lexicalized parsing models such as the ones developed by Collins -LRB- 1996 1997 -RRB- and Carroll and Rooth -LRB- 1998 -RRB-	dep_Rooth_1998 dep_1996_1997 conj_and_Collins_Rooth conj_and_Collins_Carroll dep_Collins_1996 agent_developed_Rooth agent_developed_Carroll agent_developed_Collins vmod_ones_developed det_ones_the prep_such_as_models_ones nn_models_parsing amod_models_lexicalized cop_models_are prep_of_models_interest amod_interest_particular
W04-2002	P97-1003	o	These models include a standard unlexicalized PCFG parser a head-lexicalized parser -LRB- Collins 1997 -RRB- and a maximum-entropy inspired parser -LRB- Charniak 2000 -RRB-	amod_Charniak_2000 dep_parser_Charniak amod_parser_inspired amod_parser_maximum-entropy det_parser_a dep_Collins_1997 appos_parser_Collins amod_parser_head-lexicalized det_parser_a conj_and_parser_parser conj_and_parser_parser nn_parser_PCFG amod_parser_unlexicalized amod_parser_standard det_parser_a dobj_include_parser dobj_include_parser dobj_include_parser nsubj_include_models det_models_These
W04-2003	P97-1003	n	Statistical disambiguation such as -LRB- Collins and Brooks 1995 -RRB- for PP-attachment or -LRB- Collins 1997 Charniak 2000 -RRB- for generative parsing greatly improve disambiguation but as they model by imitation instead of by understanding complete soundness has to remain elusive	acomp_remain_elusive aux_remain_to xcomp_has_remain nsubj_has_soundness prep_by_has_understanding amod_soundness_complete conj_negcc_model_has prep_by_model_imitation nsubj_model_they mark_model_as dobj_improve_disambiguation advmod_improve_greatly nsubj_improve_Collins amod_parsing_generative dep_Charniak_2000 prep_for_Collins_parsing dep_Collins_Charniak dep_Collins_1997 conj_but_Collins_has conj_but_Collins_model conj_or_Collins_improve prep_for_Collins_PP-attachment num_Collins_1995 conj_and_Collins_Brooks prep_such_as_disambiguation_model prep_such_as_disambiguation_improve prep_such_as_disambiguation_Brooks prep_such_as_disambiguation_Collins amod_disambiguation_Statistical
W04-2504	P97-1003	o	Specifically the following information can be either automatically identified or manually annotated Syntactic structures automatically identified from a parser -LRB- Collins 1997 -RRB- Semantic roles of entities in the question -LRB- Gildea and Jurafsky 2002 Gildea and Palmer 2002 Surdeanu et al. 2003 -RRB- Discourse roles either manually annotated or identified by rules that map directly from semantic roles to discourse roles	nn_roles_discourse amod_roles_semantic prep_to_map_roles prep_from_map_roles advmod_map_directly nsubj_map_that rcmod_rules_map prep_by_identified_rules conj_or_annotated_identified advmod_annotated_manually preconj_annotated_either amod_roles_identified amod_roles_annotated nn_roles_Discourse num_Surdeanu_2003 nn_Surdeanu_al. nn_Surdeanu_et num_Gildea_2002 conj_and_Gildea_Palmer num_Jurafsky_2002 dep_Gildea_Surdeanu dep_Gildea_Palmer dep_Gildea_Gildea conj_and_Gildea_Jurafsky appos_question_Jurafsky appos_question_Gildea det_question_the dep_roles_roles prep_in_roles_question prep_of_roles_entities amod_roles_Semantic amod_Collins_1997 appos_parser_Collins det_parser_a prep_from_identified_parser advmod_identified_automatically dep_structures_roles vmod_structures_identified amod_structures_Syntactic advmod_annotated_manually nsubj_annotated_information dep_identified_structures conj_or_identified_annotated advmod_identified_automatically preconj_identified_either auxpass_identified_be aux_identified_can nsubjpass_identified_information advmod_identified_Specifically amod_information_following det_information_the
W04-2506	P97-1003	o	Step 3 -RRB- Answer Extraction We select the top 5 ranked sentences and return them as Collins 1997 -RRB- can be used to capture the binary dependencies between the head of each phrase	det_phrase_each prep_of_head_phrase det_head_the prep_between_dependencies_head amod_dependencies_binary det_dependencies_the dobj_capture_dependencies aux_capture_to xcomp_used_capture auxpass_used_be aux_used_can nsubjpass_used_Extraction amod_Collins_1997 prep_as_return_Collins dobj_return_them nsubj_return_We amod_sentences_ranked num_sentences_5 amod_5_top det_5_the conj_and_select_return dobj_select_sentences nsubj_select_We dep_Extraction_return dep_Extraction_select nn_Extraction_Answer dep_Extraction_3 dep_Extraction_Step
W05-0211	P97-1003	n	2.3 Collinss -LRB- Bikels -RRB- Parser Collinss statistical parser -LRB- CBP -LRB- Collins 1997 -RRB- -RRB- improved by Bikel -LRB- Bikel 2004 -RRB- is based on the probabilities between head-words in parse trees	nn_trees_parse prep_in_head-words_trees prep_between_probabilities_head-words det_probabilities_the prep_on_based_probabilities auxpass_based_is nsubjpass_based_parser amod_Bikel_2004 dep_Bikel_Bikel agent_improved_Bikel dep_Collins_1997 dep_CBP_Collins vmod_parser_improved appos_parser_CBP amod_parser_statistical nn_parser_Collinss nn_parser_Parser nn_parser_Collinss appos_Collinss_Bikels num_Collinss_2.3
W05-0305	P97-1003	o	The parser implementation in -LRB- Bikel 2002 -RRB- was used in this experiment and it was run in a mode which emulated the Collins -LRB- 1997 -RRB- parser	nn_parser_Collins det_parser_the appos_Collins_1997 dobj_emulated_parser nsubj_emulated_which rcmod_mode_emulated det_mode_a prep_in_run_mode auxpass_run_was nsubjpass_run_it det_experiment_this conj_and_used_run prep_in_used_experiment auxpass_used_was nsubjpass_used_implementation amod_Bikel_2002 prep_in_implementation_Bikel nn_implementation_parser det_implementation_The
W05-0602	P97-1003	o	The syntactic parameters are the same as in Section 5.1 and are smoothed as in -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_in_Collins pcomp_as_in prep_smoothed_as auxpass_smoothed_are nsubjpass_smoothed_parameters num_Section_5.1 pobj_in_Section pcomp_as_in conj_and_same_smoothed prep_same_as det_same_the cop_same_are nsubj_same_parameters amod_parameters_syntactic det_parameters_The
W05-0602	P97-1003	o	As in -LRB- Collins 1997 -RRB- the parameter C8 D0 B4C4 CX B4D0D8 CX BND0DB CX B5CYC8BNC0BNDBBND8BNA1BNC4BVB5 is further smoothed as follows C8 D0BD B4C4 CX CYC8BNC0BNDBBND8BNA1BNC4BVB5 A2 C8 D0BE B4D0D8 CX CYC8BNC0BNDBBND8BNA1BNC4BVBNC4 CX B5A2 C8 D0BF B4D0DB CX CYC8BNC0BNDBBND8BNA1BNC4BVBNC4 CX B4D0D8 CX B5B5 Note this smoothing is different from the syntactic counterpart	amod_counterpart_syntactic det_counterpart_the prep_from_different_counterpart cop_different_is csubj_different_Note det_smoothing_this dobj_Note_smoothing nsubj_Note_B5B5 nn_B5B5_CX nn_B5B5_B4D0D8 nn_B5B5_CX nn_B5B5_CYC8BNC0BNDBBND8BNA1BNC4BVBNC4 nn_B5B5_CX nn_B5B5_B4D0DB nn_B5B5_D0BF nn_B5B5_C8 nn_B5B5_B5A2 nn_B5B5_CX nn_B5B5_CYC8BNC0BNDBBND8BNA1BNC4BVBNC4 nn_B5B5_CX nn_B5B5_B4D0D8 nn_B5B5_D0BE nn_B5B5_C8 nn_B5B5_A2 nn_B5B5_CYC8BNC0BNDBBND8BNA1BNC4BVB5 nn_B5B5_CX nn_B5B5_B4C4 nn_B5B5_D0BD nn_B5B5_C8 mark_follows_as advcl_smoothed_follows dep_further_smoothed cop_further_is nsubj_further_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5 nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_CX nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_BND0DB nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_CX nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_B4D0D8 nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_CX nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_B4C4 nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_D0 nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_C8 nn_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_parameter det_B5CYC8BNC0BNDBBND8BNA1BNC4BVB5_the dep_Collins_different rcmod_Collins_further amod_Collins_1997 pobj_in_Collins pcomp_As_in prep_``_As
W05-0602	P97-1003	o	The probabilities from these back-off levels are interpolated using the techniques in -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_in_Collins prep_techniques_in det_techniques_the dobj_using_techniques xcomp_interpolated_using auxpass_interpolated_are nsubjpass_interpolated_probabilities amod_levels_back-off det_levels_these prep_from_probabilities_levels det_probabilities_The ccomp_``_interpolated
W05-0602	P97-1003	o	We augment Collins head-driven model 2 -LRB- Collins 1997 -RRB- to incorporate a semantic label on each internal node	amod_node_internal det_node_each prep_on_label_node amod_label_semantic det_label_a dobj_incorporate_label aux_incorporate_to amod_Collins_1997 appos_model_Collins num_model_2 amod_model_head-driven nn_model_Collins vmod_augment_incorporate dobj_augment_model nsubj_augment_We ccomp_``_augment
W05-0602	P97-1003	o	All words occurring less than 3 times in the training data and words in test data that were not seen in training are unknown words and are replaced with the UNKNOWN token Note this threshold is smaller than the one used in -LRB- Collins 1997 -RRB- since the corpora used in our experiments are smaller	cop_smaller_are nsubj_smaller_corpora mark_smaller_since dep_smaller_Collins poss_experiments_our prep_in_used_experiments vmod_corpora_used det_corpora_the amod_Collins_1997 prepc_in_used_smaller vmod_one_used det_one_the prep_than_smaller_one cop_smaller_is nsubj_smaller_threshold det_threshold_this ccomp_Note_smaller amod_UNKNOWN_token det_UNKNOWN_the prep_with_replaced_UNKNOWN auxpass_replaced_are nsubjpass_replaced_words dep_words_Note conj_and_words_replaced amod_words_unknown cop_words_are nsubj_words_words nsubj_words_words prep_in_seen_training neg_seen_not auxpass_seen_were nsubjpass_seen_that rcmod_data_seen nn_data_test prep_in_words_data nn_data_training det_data_the prep_in_times_data num_times_3 prep_than_occurring_times mwe_occurring_less conj_and_words_words vmod_words_occurring det_words_All
W05-0602	P97-1003	o	Otherwise they are generated along with the words using the same approach as in -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_in_Collins pcomp_as_in amod_approach_same det_approach_the prep_using_as dobj_using_approach vmod_words_using det_words_the prep_along_with_generated_words auxpass_generated_are nsubjpass_generated_they advmod_generated_Otherwise ccomp_``_generated
W05-0602	P97-1003	o	Only one word is labeled with the concept the syntactic head word -LRB- Collins 1997 -RRB- is preferred	auxpass_preferred_is nsubjpass_preferred_word amod_Collins_1997 dep_word_Collins nn_word_head amod_word_syntactic det_word_the det_concept_the parataxis_labeled_preferred prep_with_labeled_concept auxpass_labeled_is nsubjpass_labeled_word num_word_one quantmod_one_Only
W05-0602	P97-1003	o	In the following section we follow the notation in -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_in_Collins det_notation_the prep_follow_in dobj_follow_notation nsubj_follow_we prep_in_follow_section amod_section_following det_section_the
W05-1002	P97-1003	o	Both training and testing sentences were processed using Collins parser -LRB- Collins 1997 -RRB- to generate parse-tree automatically	advmod_generate_automatically dobj_generate_parse-tree aux_generate_to amod_Collins_1997 appos_parser_Collins nn_parser_Collins vmod_using_generate dobj_using_parser xcomp_processed_using auxpass_processed_were nsubjpass_processed_sentences nn_sentences_testing nn_sentences_training conj_and_training_testing preconj_training_Both
W05-1504	P97-1003	o	In parsing the most relevant previous work is due to Collins -LRB- 1997 -RRB- who considered three binary features of the intervening material did it contain -LRB- a -RRB- any word tokens at all -LRB- b -RRB- any verbs -LRB- c -RRB- any commas or colons ?	conj_or_commas_colons det_commas_any dep_verbs_colons dep_verbs_commas appos_verbs_c det_verbs_any dobj_b_verbs dep_tokens_b prep_at_tokens_all nn_tokens_word det_tokens_any det_tokens_a dobj_contain_tokens nsubj_contain_it aux_contain_did amod_material_intervening det_material_the prep_of_features_material amod_features_binary num_features_three dobj_considered_features nsubj_considered_who rcmod_Collins_considered appos_Collins_1997 parataxis_due_contain prep_to_due_Collins cop_due_is nsubj_due_work prep_in_due_parsing amod_work_previous amod_work_relevant det_work_the advmod_relevant_most rcmod_``_due
W05-1504	P97-1003	o	5 Related Work As discussed in footnote 3 Collins -LRB- 1997 -RRB- and McDonald et al.	nn_al._et nn_al._McDonald conj_and_Collins_al. dep_Collins_1997 num_footnote_3 prep_in_discussed_footnote mark_discussed_As dep_Work_al. dep_Work_Collins advcl_Work_discussed amod_Work_Related num_Work_5
W05-1507	P97-1003	o	Bilexical CFG is at the heart of most modern statistical parsers -LRB- Collins 1997 Charniak 1997 -RRB- because the statistics associated with word-specific rules are more informative for disambiguation purposes	nn_purposes_disambiguation prep_for_informative_purposes advmod_informative_more cop_informative_are nsubj_informative_statistics mark_informative_because amod_rules_word-specific prep_with_associated_rules vmod_statistics_associated det_statistics_the dep_Charniak_1997 dep_Collins_Charniak amod_Collins_1997 appos_parsers_Collins amod_parsers_statistical amod_parsers_modern amod_parsers_most prep_of_heart_parsers det_heart_the advcl_is_informative prep_at_is_heart nsubj_is_CFG amod_CFG_Bilexical ccomp_``_is
W05-1512	P97-1003	o	-LRB- 2005 -RRB- 86.6 86.7 1.19 Klein and Manning -LRB- 2003 -RRB- 86.9 85.7 86.3 30.9 1.10 Charniak -LRB- 1997 -RRB- 87.4 87.5 1.00 Collins -LRB- 1997 -RRB- 88.6 88.1 0.91 Table 3 Comparison with other parsers -LRB- sentences of length 40 -RRB- as head information	nn_information_head num_length_40 prep_of_sentences_length dep_parsers_sentences amod_parsers_other prep_as_Comparison_information prep_with_Comparison_parsers num_Table_3 num_Table_0.91 num_Table_88.1 num_Table_88.6 dep_Collins_Table appos_Collins_1997 num_Collins_1.00 dep_87.5_Collins dep_87.4_87.5 amod_Charniak_87.4 appos_Charniak_1997 num_Charniak_1.10 dep_30.9_Charniak number_30.9_86.3 dep_30.9_85.7 number_85.7_86.9 dep_2003_Comparison dep_2003_30.9 appos_Manning_2003 conj_and_Klein_Manning num_Klein_1.19 num_Klein_2005 number_1.19_86.7 number_1.19_86.6
W05-1512	P97-1003	o	This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data -LRB- Magerman -LRB- 1995 -RRB- Collins -LRB- 1996 -RRB- Charniak -LRB- 1997 -RRB- etc. -RRB-	appos_Charniak_1997 appos_Collins_1996 dep_Magerman_etc. appos_Magerman_Charniak conj_Magerman_Collins appos_Magerman_1995 appos_data_Magerman amod_data_sparse prep_from_derived_data vmod_models_derived nn_models_parsing amod_models_lexicalized amod_models_most prep_in_statistics_models amod_statistics_fixed-word amod_statistics_smoothed det_statistics_the prep_to_contrast_statistics amod_contrast_sharp prep_in_is_contrast nsubj_is_This ccomp_``_is
W05-1512	P97-1003	o	With automatic refinement it is harder to guarantee improved performance than with manual refinements -LRB- Klein and Manning 2003 -RRB- or with refinements based on direct lexicalization -LRB- Magerman -LRB- 1995 -RRB- Collins -LRB- 1996 -RRB- Charniak -LRB- 1997 -RRB- etc. -RRB-	appos_Charniak_1997 appos_Collins_1996 dep_Magerman_etc. appos_Magerman_Charniak appos_Magerman_Collins appos_Magerman_1995 appos_lexicalization_Magerman amod_lexicalization_direct pobj_with_refinements dep_Klein_2003 conj_and_Klein_Manning conj_or_refinements_with appos_refinements_Manning appos_refinements_Klein amod_refinements_manual pobj_with_with pobj_with_refinements pcomp_than_with amod_performance_improved pobj_guarantee_lexicalization prepc_based_on_guarantee_on prep_guarantee_than dobj_guarantee_performance aux_guarantee_to ccomp_harder_guarantee cop_harder_is nsubj_harder_it dep_harder_refinement mark_harder_With amod_refinement_automatic advcl_``_harder
W05-1512	P97-1003	p	2 Head Lexicalization As previously shown -LRB- Charniak -LRB- 1997 -RRB- Collins -LRB- 1997 -RRB- Carroll and Rooth -LRB- 1998 -RRB- etc. -RRB- ContextFree Grammars -LRB- CFGs -RRB- can be transformed to lexicalized CFGs provided that a head-marking scheme for rules is given	auxpass_given_is nsubjpass_given_scheme mark_given_that prep_for_scheme_rules amod_scheme_head-marking det_scheme_a ccomp_provided_given amod_CFGs_lexicalized ccomp_transformed_provided prep_to_transformed_CFGs auxpass_transformed_be aux_transformed_can nsubjpass_transformed_Lexicalization appos_Grammars_CFGs nn_Grammars_ContextFree appos_Rooth_1998 appos_Collins_1997 dep_Charniak_etc. conj_and_Charniak_Rooth conj_and_Charniak_Carroll conj_and_Charniak_Collins appos_Charniak_1997 dep_shown_Rooth dep_shown_Carroll dep_shown_Collins dep_shown_Charniak advmod_shown_previously mark_shown_As appos_Lexicalization_Grammars advcl_Lexicalization_shown nn_Lexicalization_Head num_Lexicalization_2 ccomp_``_transformed
W05-1512	P97-1003	o	By contrast alternative approaches like Collins -LRB- 1997 -RRB- apply an additional transformation to each tree in the tree-bank splitting each rule into small parts which finally results in a new grammar covering many more sentences than the explicit one	amod_one_explicit det_one_the prep_than_sentences_one amod_sentences_more amod_sentences_many dobj_covering_sentences vmod_grammar_covering amod_grammar_new det_grammar_a prep_in_results_grammar advmod_results_finally nsubj_results_which rcmod_parts_results amod_parts_small det_rule_each prep_into_splitting_parts dobj_splitting_rule det_tree-bank_the det_tree_each prep_to_transformation_tree amod_transformation_additional det_transformation_an conj_apply_splitting prep_in_apply_tree-bank dobj_apply_transformation nsubj_apply_approaches prep_by_apply_contrast appos_Collins_1997 prep_like_approaches_Collins amod_approaches_alternative
W05-1512	P97-1003	o	The most important tree-bank transformation in the literature is lexicalization Each node in a tree is labeled with its head word the most important word of the constituent under the node -LRB- Magerman -LRB- 1995 -RRB- Collins -LRB- 1996 -RRB- Charniak -LRB- 1997 -RRB- Collins -LRB- 1997 -RRB- Carroll and Rooth -LRB- 1998 -RRB- etc. -RRB-	appos_Rooth_1998 appos_Collins_1997 appos_Charniak_1997 appos_Collins_1996 dep_Magerman_etc. conj_and_Magerman_Rooth conj_and_Magerman_Carroll conj_and_Magerman_Collins conj_and_Magerman_Charniak conj_and_Magerman_Collins appos_Magerman_1995 appos_node_Rooth appos_node_Carroll appos_node_Collins appos_node_Charniak appos_node_Collins appos_node_Magerman det_node_the det_constituent_the prep_under_word_node prep_of_word_constituent amod_word_important det_word_the advmod_important_most appos_word_word nn_word_head poss_word_its prep_with_labeled_word auxpass_labeled_is nsubjpass_labeled_node det_tree_a prep_in_node_tree det_node_Each parataxis_lexicalization_labeled cop_lexicalization_is nsubj_lexicalization_transformation det_literature_the prep_in_transformation_literature amod_transformation_tree-bank amod_transformation_important det_transformation_The advmod_important_most
W05-1513	P97-1003	p	Although state-of-the-art statistical parsers -LRB- Collins 1997 Charniak 2000 -RRB- are more accurate the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast light-weight parsing or parsing of large amounts of data	prep_of_amounts_data amod_amounts_large prep_of_parsing_amounts conj_or_parsing_parsing amod_parsing_light-weight advmod_requiring_fast vmod_number_requiring prep_of_number_situations det_number_a prep_in_attractive_number nsubj_attractive_them dep_make_parsing dep_make_parsing xcomp_make_attractive nsubj_make_efficiency nsubj_make_simplicity advcl_make_accurate amod_parsers_deterministic prep_of_simplicity_parsers conj_and_simplicity_efficiency det_simplicity_the advmod_accurate_more cop_accurate_are nsubj_accurate_parsers mark_accurate_Although dep_Charniak_2000 dep_Collins_Charniak amod_Collins_1997 appos_parsers_Collins amod_parsers_statistical amod_parsers_state-of-the-art
W05-1513	P97-1003	o	Table 1 shows a summary of the results of our experiments with SVMpar and MBLpar and also results obtained with the Charniak -LRB- 2000 -RRB- parser the Bikel -LRB- 2003 -RRB- implementation of the Collins -LRB- 1997 -RRB- parser and the Ratnaparkhi -LRB- 1997 -RRB- parser	nn_parser_Ratnaparkhi det_parser_the appos_Ratnaparkhi_1997 nn_parser_Collins det_parser_the appos_Collins_1997 prep_of_implementation_parser nn_implementation_Bikel det_implementation_the appos_Bikel_2003 nn_parser_Charniak appos_Charniak_2000 det_Charniak_the prep_with_obtained_parser vmod_results_obtained advmod_results_also nsubj_results_Table conj_and_SVMpar_MBLpar prep_with_experiments_MBLpar prep_with_experiments_SVMpar poss_experiments_our prep_of_results_experiments det_results_the prep_of_summary_results det_summary_a conj_and_shows_parser conj_and_shows_implementation conj_and_shows_results dobj_shows_summary nsubj_shows_Table num_Table_1
W05-1516	P97-1003	o	Performance of Alternative Models 157 5 Related Work Previous parsing models -LRB- e.g. Collins 1997 Charniak 2000 -RRB- maximize the joint probability P -LRB- S T -RRB- of a sentence S and its parse tree T We maximize the conditional probability P -LRB- T | S -RRB-	num_S_| nn_S_T appos_P_S dep_probability_P amod_probability_conditional det_probability_the dobj_maximize_probability nsubj_maximize_We nsubj_maximize_Performance nn_T_tree amod_T_parse poss_T_its conj_and_S_T nn_S_sentence det_S_a appos_S_T prep_of_P_T prep_of_P_S dep_P_S nn_P_probability amod_P_joint det_P_the dobj_maximize_P nsubj_maximize_Collins advmod_maximize_e.g. dep_Charniak_2000 dep_Collins_Charniak amod_Collins_1997 rcmod_models_maximize nn_models_parsing amod_models_Previous dep_Work_models amod_Work_Related num_Work_5 number_5_157 dep_Models_Work nn_Models_Alternative prep_of_Performance_Models
W05-1516	P97-1003	p	1 Introduction There has been a great deal of progress in statistical parsing in the past decade -LRB- Collins 1996 Collins 1997 Chaniak 2000 -RRB-	amod_Chaniak_2000 num_Collins_1997 dep_Collins_Chaniak dep_Collins_Collins amod_Collins_1996 amod_decade_past det_decade_the amod_parsing_statistical dep_deal_Collins prep_in_deal_decade prep_in_deal_parsing prep_of_deal_progress amod_deal_great det_deal_a cop_deal_been aux_deal_has expl_deal_There dep_deal_Introduction num_Introduction_1
W06-1636	P97-1003	o	Instead researchers condition parsing decisions on many other features such as parent phrase-marker and famously the lexical-head of the phrase -LRB- Magerman 1995 Collins 1996 Collins 1997 Johnson 1998 Charniak 2000 Henderson 2003 Klein and Manning 2003 Matsuzaki et al. 2005 -RRB- -LRB- and others -RRB-	num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et num_Henderson_2003 conj_and_Charniak_others conj_and_Charniak_Matsuzaki conj_and_Charniak_2003 conj_and_Charniak_Manning conj_and_Charniak_Klein conj_and_Charniak_Henderson conj_and_Charniak_2000 num_Johnson_1998 num_Collins_1997 num_Collins_1996 dep_Magerman_others dep_Magerman_Matsuzaki dep_Magerman_2003 dep_Magerman_Manning dep_Magerman_Klein dep_Magerman_Henderson dep_Magerman_2000 dep_Magerman_Charniak conj_Magerman_Johnson conj_Magerman_Collins conj_Magerman_Collins conj_Magerman_1995 appos_phrase_Magerman det_phrase_the prep_of_lexical-head_phrase det_lexical-head_the nn_phrase-marker_parent appos_features_lexical-head advmod_features_famously cc_features_and prep_such_as_features_phrase-marker amod_features_other amod_features_many prep_on_decisions_features nn_decisions_parsing nn_decisions_condition nn_decisions_researchers advmod_decisions_Instead
W06-1638	P97-1003	o	Eisner -LRB- 1996 -RRB- Charniak -LRB- 1997 -RRB- Collins -LRB- 1997 -RRB- and many subsequent researchers1 annotated every node with lexical features passed up from its head child in order to more precisely reflect the nodes inside contents	prep_inside_nodes_contents det_nodes_the dobj_reflect_nodes advmod_reflect_precisely aux_reflect_to prep_in_reflect_order advmod_precisely_more nn_child_head poss_child_its xcomp_passed_reflect prep_from_passed_child prt_passed_up nsubj_passed_node amod_features_lexical prep_with_node_features det_node_every dep_annotated_passed amod_researchers1_annotated dobj_subsequent_researchers1 dep_many_subsequent appos_Collins_1997 appos_Charniak_1997 conj_and_Eisner_many conj_and_Eisner_Collins appos_Eisner_Charniak appos_Eisner_1996
W06-1638	P97-1003	o	Charniak -LRB- 1997 -RRB- and Johnson -LRB- 1998 -RRB- annotated each node with its parent and grandparent nonterminals to more precisely reflect its outside context	amod_context_outside poss_context_its dobj_reflect_context advmod_reflect_precisely aux_reflect_to advmod_precisely_more nn_nonterminals_grandparent conj_and_parent_nonterminals poss_parent_its prep_with_node_nonterminals prep_with_node_parent det_node_each amod_node_annotated nn_node_Johnson appos_Johnson_1998 vmod_Charniak_reflect conj_and_Charniak_node appos_Charniak_1997
W06-1638	P97-1003	o	317 Citation Observed data Hidden data Collins -LRB- 1997 -RRB- Treebank tree with head child annotated on each nonterminal No hidden data	amod_data_hidden neg_data_No amod_data_nonterminal det_data_each prep_on_annotated_data nsubj_annotated_child mark_annotated_with nn_child_head advcl_tree_annotated nn_tree_Treebank nsubj_tree_Collins dep_Collins_1997 nn_Collins_data amod_Collins_Hidden nn_Collins_data nn_Collins_Observed nn_Collins_Citation num_Collins_317
W06-1668	P97-1003	o	To avoid this problem generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution such as in the parsing models of -LRB- Collins 1997 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak amod_Collins_1997 prep_of_models_Collins nn_models_parsing det_models_the pobj_in_models prepc_such_as_distribution_in amod_distribution_joint det_distribution_the prep_of_representation_distribution amod_representation_appropriate det_representation_an dobj_achieve_representation aux_achieve_to xcomp_designed_achieve advmod_designed_manually auxpass_designed_been advmod_designed_often aux_designed_have nsubjpass_designed_models advcl_designed_avoid nn_tasks_NLP prep_for_models_tasks amod_models_generative det_problem_this dobj_avoid_problem aux_avoid_To ccomp_``_designed
W06-1668	P97-1003	o	This kind of smoothing has also been used in the generative parser of -LRB- Collins 1997 -RRB- and has been shown to have a relatively good performance for language modeling -LRB- Goodman 2001 -RRB-	amod_Goodman_2001 dep_modeling_Goodman nn_modeling_language prep_for_performance_modeling amod_performance_good det_performance_a advmod_good_relatively dobj_have_performance aux_have_to xcomp_shown_have auxpass_shown_been aux_shown_has conj_and_Collins_shown amod_Collins_1997 prep_of_parser_shown prep_of_parser_Collins amod_parser_generative det_parser_the prep_in_used_parser auxpass_used_been advmod_used_also aux_used_has nsubjpass_used_kind prep_of_kind_smoothing det_kind_This
W06-2902	P97-1003	o	In addition to portability experiments with the parsing model of -LRB- Collins 1997 -RRB- -LRB- Gildea 2001 -RRB- provided a comprehensive analysis of parser portability	nn_portability_parser prep_of_analysis_portability amod_analysis_comprehensive det_analysis_a dobj_provided_analysis nsubj_provided_Collins dep_Gildea_2001 appos_Collins_Gildea amod_Collins_1997 prepc_of_model_provided nn_model_parsing det_model_the prep_with_experiments_model nn_experiments_portability prep_to_addition_experiments pobj_In_addition dep_``_In
W06-2904	P97-1003	o	Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems -LRB- Collins 1997 Bikel 2004 -RRB-	amod_Bikel_2004 dep_Collins_Bikel amod_Collins_1997 appos_problems_Collins nn_problems_data amod_problems_sparse det_problems_the prep_with_cope_problems aux_cope_to nn_tricks_estimation amod_tricks_back-off conj_and_smoothing_tricks amod_smoothing_various vmod_incorporating_cope dobj_incorporating_tricks dobj_incorporating_smoothing nsubj_incorporating_Learning amod_techniques_based dep_techniques_likelihood amod_likelihood_simple det_model_the prep_of_parameters_model det_parameters_the prep_with_estimating_techniques dobj_estimating_parameters conj_but_consisted_incorporating prepc_of_consisted_estimating nsubj_consisted_Learning det_context_this prep_in_Learning_context
W06-2904	P97-1003	o	For example smoothing methods have played a central role in probabilistic approaches -LRB- Collins 1997 Wang et al. 2005 -RRB- and yet they are not being used in current large margin training algorithms	nn_algorithms_training nn_algorithms_margin amod_algorithms_large amod_algorithms_current prep_in_used_algorithms auxpass_used_being neg_used_not aux_used_are nsubjpass_used_they advmod_used_yet num_Wang_2005 nn_Wang_al. nn_Wang_et dep_Collins_Wang appos_Collins_1997 appos_approaches_Collins amod_approaches_probabilistic amod_role_central det_role_a conj_and_played_used prep_in_played_approaches dobj_played_role aux_played_have nsubj_played_methods prep_for_played_example amod_methods_smoothing
W06-2904	P97-1003	p	1 Introduction Over the past decade there has been tremendous progress on learning parsing models from treebank data -LRB- Collins 1997 Charniak 2000 Wang et al. 2005 McDonald et al. 2005 -RRB-	num_McDonald_2005 nn_McDonald_al. nn_McDonald_et num_Wang_2005 nn_Wang_al. nn_Wang_et num_Charniak_2000 dep_Collins_McDonald conj_Collins_Wang conj_Collins_Charniak amod_Collins_1997 dep_data_Collins amod_data_treebank nn_models_parsing prep_from_learning_data dobj_learning_models prepc_on_progress_learning amod_progress_tremendous cop_progress_been aux_progress_has expl_progress_there dep_progress_Introduction amod_decade_past det_decade_the prep_over_Introduction_decade num_Introduction_1
W06-2904	P97-1003	o	Most of the early work in this area was based on postulating generative probability models of language that included parse structure -LRB- Collins 1997 -RRB-	amod_Collins_1997 appos_structure_Collins nn_structure_parse dobj_included_structure nsubj_included_that rcmod_models_included prep_of_models_language nn_models_probability amod_models_generative dobj_postulating_models prepc_on_based_postulating auxpass_based_was nsubjpass_based_Most det_area_this prep_in_work_area amod_work_early det_work_the prep_of_Most_work
W06-2920	P97-1003	o	Collins -LRB- 1997 -RRB- s parser and its reimplementation and extension by Bikel -LRB- 2002 -RRB- have by now been applied to a variety of languages English -LRB- Collins 1999 -RRB- Czech -LRB- Collins et al. 1999 -RRB- German -LRB- Dubey and Keller 2003 -RRB- Spanish -LRB- Cowan and Collins 2005 -RRB- French -LRB- Arun and Keller 2005 -RRB- Chinese -LRB- Bikel 2002 -RRB- and according to Dan Bikels web page Arabic	amod_page_Arabic nn_page_web nn_page_Bikels nn_page_Dan pobj_to_page pcomp_according_to dep_Bikel_2002 dep_Chinese_Bikel num_Arun_2005 conj_and_Arun_Keller amod_Cowan_2005 conj_and_Cowan_Collins dep_Spanish_Collins dep_Spanish_Cowan dep_Dubey_2003 conj_and_Dubey_Keller amod_Collins_1999 dep_Collins_al. nn_Collins_et dep_Czech_Collins amod_Collins_1999 conj_and_English_according conj_and_English_Chinese dep_English_Keller dep_English_Arun conj_and_English_French conj_and_English_Spanish dep_English_Keller dep_English_Dubey amod_English_German appos_English_Czech dep_English_Collins prep_of_variety_languages det_variety_a prep_to_applied_variety auxpass_applied_been advmod_applied_now dep_have_according dep_have_Chinese dep_have_French dep_have_Spanish dep_have_English prepc_by_have_applied appos_Bikel_2002 dep_extension_have prep_by_extension_Bikel poss_reimplementation_its conj_and_parser_reimplementation conj_and_s_extension dobj_s_reimplementation dobj_s_parser nsubj_s_Collins appos_Collins_1997
W08-1112	P97-1003	p	It is interesting to note that while the study of how the granularity of context-free grammars -LRB- CFG -RRB- affects the performance of a parser -LRB- e.g. in the form 86 n1 IP -LSB- =] n2 NP -LSB- SUBJ =] n4 NR -LSB- =] GSC4ES JiangZemin n3 VP -LSB- =] n5 VV -LSB- =] ESDO interview n6 NP -LSB- OBJ =] n7 NR -LSB- ADJUNCT -RSB- AIC1 Thai n8 NN -LSB- =] D3D2 president f1 PRED ESDO SUBJ f2 PRED GSC4ESNTYPE proper NUM sg OBJ f3 PRED D3D2 NTYPE common NUM sg ADJUNCT f4 PRED AIC1NTYPE proper NUM sg N F -LRB- n1 -RRB- = -LRB- n3 -RRB- = -LRB- n5 -RRB- = f1 -LRB- n2 -RRB- = -LRB- n4 -RRB- = f2 -LRB- n6 -RRB- = -LRB- n8 -RRB- = f3 -LRB- n7 -RRB- = f4 Figure 1 Cand f-structures with links for the sentence GSC4ESESDOAIC1D3D2 of grammar transforms -LRB- Johnson 1998 -RRB- and lexicalisation -LRB- Collins 1997 -RRB- -RRB- has attracted substantial attention to our knowledge there has been a lot less research on this subject for surface realisation a process that is generally regarded as the reverse process of parsing	prep_of_process_parsing amod_process_reverse det_process_the prep_as_regarded_process advmod_regarded_generally auxpass_regarded_is nsubjpass_regarded_that rcmod_process_regarded det_process_a nn_realisation_surface prep_for_subject_realisation det_subject_this appos_research_process prep_on_research_subject amod_research_less cop_research_been aux_research_has expl_research_there npadvmod_less_lot det_lot_a poss_knowledge_our amod_attention_substantial parataxis_attracted_research prep_to_attracted_knowledge dobj_attracted_attention aux_attracted_has nsubj_attracted_f3 amod_Collins_1997 dep_lexicalisation_Collins amod_Johnson_1998 conj_and_transforms_lexicalisation dep_transforms_Johnson nsubj_transforms_f-structures prep_of_GSC4ESESDOAIC1D3D2_grammar dep_sentence_GSC4ESESDOAIC1D3D2 det_sentence_the prep_for_links_sentence prep_with_f-structures_links nn_f-structures_Cand dep_Figure_lexicalisation dep_Figure_transforms num_Figure_1 nn_Figure_f4 dep_=_Figure amod_f3_= appos_f3_n7 dep_=_attracted dep_=_= dep_=_n8 amod_f2_= appos_f2_n6 dobj_=_f2 dep_=_= dep_=_n4 amod_f1_= appos_f1_n2 dobj_=_f1 dep_=_= dep_=_n5 dep_=_= appos_=_n3 amod_F_= appos_F_n1 nn_F_N dep_sg_F nn_sg_NUM amod_sg_proper nn_sg_AIC1NTYPE dobj_PRED_sg advmod_PRED_f4 nsubj_PRED_ADJUNCT nn_ADJUNCT_sg nn_ADJUNCT_NUM amod_ADJUNCT_common nn_ADJUNCT_NTYPE nn_ADJUNCT_D3D2 ccomp_PRED_PRED nsubj_PRED_f3 nn_f3_OBJ nn_f3_sg nn_f3_NUM amod_f3_proper nn_f3_GSC4ESNTYPE ccomp_PRED_PRED nsubj_PRED_f2 nn_f2_SUBJ nn_f2_ESDO rcmod_PRED_PRED dep_f1_PRED nn_f1_president nn_f1_D3D2 num_f1_=] dep_NN_f1 dep_n8_NN nn_n8_Thai nn_n8_AIC1 nn_n8_NR appos_NR_ADJUNCT num_n7_=] dep_OBJ_n8 dep_OBJ_n7 dep_NP_OBJ dep_n6_NP nn_n6_interview nn_n6_ESDO num_n6_=] dep_VV_n6 dep_n5_VV num_n5_=] appos_VP_n5 dep_n3_VP nn_n3_JiangZemin nn_n3_GSC4ES num_n3_=] dep_NR_n3 num_n4_=] dep_SUBJ_NR dep_SUBJ_n4 appos_NP_SUBJ dep_n2_NP num_n2_=] dep_IP_n2 num_n1_86 nn_n1_form det_n1_the dep_e.g._IP prep_in_e.g._n1 dep_parser_e.g. det_parser_a prep_of_performance_parser det_performance_the dobj_affects_performance nsubj_affects_granularity advmod_affects_how appos_grammars_CFG amod_grammars_context-free prep_of_granularity_grammars det_granularity_the prepc_of_study_affects det_study_the pobj_while_study dep_,_while dep_note_that aux_note_to xcomp_interesting_note cop_interesting_is nsubj_interesting_It
W08-1112	P97-1003	p	Methodologies such as lexicalisation -LRB- Collins 1997 Charniak 2000 -RRB- and tree transformations -LRB- Johnson 1998 -RRB- weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs	amod_PCFGs_simple prep_over_improvements_PCFGs amod_improvements_significant dobj_shown_improvements nsubj_shown_Methodologies prep_to_applied_parsing advmod_applied_successfully auxpass_applied_been aux_applied_have nsubjpass_applied_Methodologies nn_assumptions_independence det_assumptions_the conj_and_weaken_shown conj_and_weaken_applied dobj_weaken_assumptions nsubj_weaken_Methodologies amod_Johnson_1998 nn_transformations_tree dep_Charniak_2000 dep_Collins_Charniak amod_Collins_1997 dep_lexicalisation_Johnson conj_and_lexicalisation_transformations appos_lexicalisation_Collins prep_such_as_Methodologies_transformations prep_such_as_Methodologies_lexicalisation
W08-1112	P97-1003	p	3.1 A History-Based Model The history-based -LRB- HB -RRB- approach which incorporates more context information has worked well in parsing -LRB- Collins 1997 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak amod_Collins_1997 appos_parsing_Collins prep_in_worked_parsing advmod_worked_well aux_worked_has nsubj_worked_approach nn_information_context amod_information_more dobj_incorporates_information nsubj_incorporates_which rcmod_approach_incorporates amod_approach_history-based det_approach_The dep_history-based_HB rcmod_Model_worked amod_Model_History-Based det_Model_A num_Model_3.1 dep_``_Model
W08-1112	P97-1003	o	As from a linguistic perspective it is the modifier 2We use a mechanism similar to -LRB- Collins 1997 -RRB- but adapted to Chinese data to find lexical heads in the treebank data	nn_data_treebank det_data_the prep_in_heads_data amod_heads_lexical dobj_find_heads aux_find_to amod_data_Chinese xcomp_adapted_find prep_to_adapted_data nsubj_adapted_2We amod_Collins_1997 prep_to_similar_Collins amod_mechanism_similar det_mechanism_a conj_but_use_adapted dobj_use_mechanism nsubj_use_2We nn_2We_modifier det_2We_the cop_2We_is nsubj_2We_it prep_from_2We_perspective advmod_2We_As amod_perspective_linguistic det_perspective_a
W08-2102	P97-1003	o	To achieve step -LRB- 1 -RRB- we first apply a set of headfinding rules which are similar to those described in -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_in_Collins prep_described_in vmod_those_described prep_to_similar_those cop_similar_are nsubj_similar_which rcmod_rules_similar amod_rules_headfinding prep_of_set_rules det_set_a dobj_apply_set advmod_apply_first nsubj_apply_we advcl_apply_achieve appos_step_1 dobj_achieve_step aux_achieve_To ccomp_``_apply
W08-2102	P97-1003	o	-LRB- For test or development data we used the part-of-speech tags generated by the parser of -LRB- Collins 1997 -RRB-	amod_Collins_1997 dep_of_Collins prep_parser_of det_parser_the agent_generated_parser vmod_tags_generated amod_tags_part-of-speech det_tags_the dobj_used_tags nsubj_used_we prep_for_used_data nn_data_development nn_data_test conj_or_test_development
W08-2219	P97-1003	o	Parse each sentence using a Treebank-trained parser -LRB- Collins 1997 Charniak 1999 -RRB-	amod_Charniak_1999 dep_Collins_Charniak appos_Collins_1997 dep_parser_Collins amod_parser_Treebank-trained det_parser_a dobj_using_parser vmod_sentence_using det_sentence_each dobj_Parse_sentence ccomp_``_Parse
W09-0429	P97-1003	o	2.4 GermanEnglish For GermanEnglish we additionally incorporated rule-based reordering We parse the input using the Collins parser -LRB- Collins 1997 -RRB- and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order -LRB- Collins et al. 2005 -RRB-	amod_Collins_2005 dep_Collins_al. nn_Collins_et dep_order_Collins nn_order_word amod_order_English advmod_closely_more dobj_corresponds_order advmod_corresponds_closely nsubj_corresponds_it mark_corresponds_that mark_corresponds_so amod_sentence_German det_sentence_the advcl_re-arrange_corresponds dobj_re-arrange_sentence aux_re-arrange_to nn_rules_reordering vmod_set_re-arrange prep_of_set_rules det_set_a dobj_apply_set amod_Collins_1997 appos_parser_Collins nn_parser_Collins det_parser_the conj_and_using_apply dobj_using_parser det_input_the dep_parse_apply dep_parse_using dobj_parse_input nsubj_parse_We rcmod_reordering_parse amod_reordering_rule-based dobj_incorporated_reordering advmod_incorporated_additionally nsubj_incorporated_we dep_incorporated_GermanEnglish prep_for_GermanEnglish_GermanEnglish num_GermanEnglish_2.4
W98-1115	P97-1003	o	One can imagine the same techniques coupled with more informative probability distributions such as lexicalized PCFGs -LRB- Charniak 1997 -RRB- or even grammars not based upon literal rules but probability distributions that describe how rules are built up from smaller components -LRB- Magerman 1995 Collins 1997 -RRB-	amod_Collins_1997 dep_Magerman_Collins appos_Magerman_1995 dep_components_Magerman amod_components_smaller prep_from_built_components prt_built_up auxpass_built_are nsubjpass_built_rules advmod_built_how ccomp_describe_built nsubj_describe_that rcmod_distributions_describe nn_distributions_probability amod_rules_literal prep_upon_based_rules neg_based_not dep_grammars_based advmod_grammars_even dep_Charniak_1997 appos_PCFGs_Charniak amod_PCFGs_lexicalized conj_or_distributions_grammars prep_such_as_distributions_PCFGs nn_distributions_probability amod_distributions_informative amod_distributions_more prep_with_coupled_grammars prep_with_coupled_distributions vmod_techniques_coupled amod_techniques_same det_techniques_the conj_but_imagine_distributions dobj_imagine_techniques aux_imagine_can nsubj_imagine_One
W98-1116	P97-1003	p	NJ 08903 U.S.A. suzanne ~ ruccs rutgers edu Empirically-induced models that learn a linguistically meaningflll grammar -LRB- Collins 1997 -RRB- seem to give tile best practical results in statistical natural language processing	nn_processing_language amod_processing_natural amod_processing_statistical prep_in_results_processing amod_results_practical amod_results_best dobj_give_results iobj_give_tile aux_give_to xcomp_seem_give nsubj_seem_ruccs amod_Collins_1997 appos_grammar_Collins amod_grammar_meaningflll advmod_grammar_linguistically det_grammar_a dobj_learn_grammar nsubj_learn_that rcmod_models_learn amod_models_Empirically-induced nn_models_edu appos_ruccs_models appos_ruccs_rutgers nn_ruccs_~ nn_ruccs_suzanne nn_ruccs_U.S.A. num_ruccs_08903 nn_ruccs_NJ
W98-1116	P97-1003	p	In agreement with recent results on parsing with lexicalised probabilistic grammars -LRB- Collins 1997 Srinivas 1997 Charniak 1997 -RRB- our main result is that statistics over lexical features best correspond to independently established truman intuitive preferences and experimental findings	amod_findings_experimental conj_and_preferences_findings amod_preferences_intuitive nn_preferences_truman amod_preferences_established advmod_established_independently prep_to_correspond_findings prep_to_correspond_preferences nsubj_correspond_statistics mark_correspond_that amod_features_best amod_features_lexical prep_over_statistics_features ccomp_is_correspond nsubj_is_result prep_in_is_agreement amod_result_main poss_result_our dep_Charniak_1997 appos_Srinivas_1997 dep_Collins_Charniak dep_Collins_Srinivas amod_Collins_1997 appos_grammars_Collins amod_grammars_probabilistic amod_grammars_lexicalised prep_with_parsing_grammars prepc_on_results_parsing amod_results_recent prep_with_agreement_results
W98-1116	P97-1003	p	In agreement with recent resuits on parsing with lexicalised probabilistic grammars -LRB- Collins 1997 Srinivas 1997 -RRB- we find that statistics over lexical as opposed to structural features best correspond to human intuitive.judgments and to experimental findings	amod_findings_experimental pobj_to_findings amod_intuitive.judgments_human conj_and_correspond_to prep_to_correspond_intuitive.judgments nsubj_correspond_best ccomp_features_to ccomp_features_correspond nsubj_features_statistics mark_features_that prep_to_opposed_structural mark_opposed_as dep_statistics_opposed prep_over_statistics_lexical ccomp_find_features nsubj_find_we prep_in_find_agreement dep_Srinivas_1997 dep_Collins_Srinivas amod_Collins_1997 appos_grammars_Collins amod_grammars_probabilistic amod_grammars_lexicalised prep_with_parsing_grammars amod_resuits_recent prepc_on_agreement_parsing prep_with_agreement_resuits
W98-1206	P97-1003	o	In addition many more sophisticated parsing models are elaborations of such PCFG models so understanding the properties of PCFGs is likely to be useful -LRB- Charniak 1997 Collins 1997 -RRB-	amod_Collins_1997 dep_Charniak_Collins dep_Charniak_1997 dep_useful_Charniak cop_useful_be aux_useful_to xcomp_likely_useful cop_likely_is nsubj_likely_elaborations prep_of_properties_PCFGs det_properties_the dobj_understanding_properties advmod_understanding_so nn_models_PCFG amod_models_such vmod_elaborations_understanding prep_of_elaborations_models cop_elaborations_are nsubj_elaborations_models prep_in_elaborations_addition nn_models_parsing amod_models_sophisticated amod_models_more amod_models_many
W99-0503	P97-1003	o	and CAUS ate slgmficantly different for unaccusattve and object-dtop verbs indicating that we need additional featules that have different values across these two classes In Section 2 1 we noted the differing semantic role asmgnments for the verb classes and hypothesized that these differences would affect the expression of syntactic features that ate countable in a corpus For example the c ~ bs feature approximates sen \ -RSB- antic role reformation b. ~ encoding the oxerlap beh ~ een nouns that can occur m the ~ ubject and object positions of a cau ~ ative xetb Here x ~ e suggest another feature that of ammacy of subject that is intended to distinguish nouns that receive an Agent role flora those that receive a Theme role Recall that objectdrop verbs assign Agent to their subject in both the transitive and intransitive alternations while unaccusattves assign Agent to their subject only in the transitive and Theme m the intransitive We expect then that object-drop verbs will occur more often with an animate subject Note again that ~ e are 20 II Features \ -LSB- Acc % SE % II I VBD ACT INTR CAUS I 63 7 0 6 \ -RSB- VBD ACT INTR CAUS PRO 70 7 0 4 Table 6 Percentage Accuracy -LRB- Acc % -RRB- and Standard Error -LRB- SE % -RRB- of C5 0 W ~ th and W ~ thout New PRO Feature All Verb Classes -LRB- 33 8 % basehne -RRB- making use of frequency dmtnbutmns -- the clatm ~ s not that only Agents can be ammate but rather that nouns that receive the Agent role will more often be ammate than nouns that receive the Theme role A problem w ~ th a feature hke ammacy ~ s that ~ t requires etther manual determmatmn of the antmacy of extracted subjects or reference to an on-hne resource such as WordNet for determining ammacy To approximate ammacy w ~ th a feature that can be extracted automatically and w ~ thout reference to a resource external to the corpus we instead count pronouns -LRB- other than ~ t -RRB- m subject positron The assumptmn ~ s that the words I we you she he and they most often refer to ammate ent ~ tms The values for the new feature P ~ O were determined by automatmally extracting all subject/verb tuples including our 59 examples verbs -LRB- from the WSJ88 parsed corpus -RRB- and computing the ratm of occurrences of pronouns to all subjects We again apply t-tests to our new data to determine whether the sets of PRo values d ~ ffer across the verb classes Interestingly we find that the Prto values for unaccusat ~ ve verbs -LRB- the only class to ass ~ gn Theme role to the sub tect m one of tts alternatmns -RRB- are s ~ gmficantly dtffe ~ ent from those for both unergatlve and object-drop verbs -LRB- p < 05 -RRB- Moreover the PRo values for unergat ~ ve and object-drop verbs -LRB- whose subjects are Agents m bo ~ h alternatmns -RRB- are not s ~ gmficantly d ~ fferent Th ~ s pattern confirms the abd ~ ty of the feature to capture the thematm d ~ stmctmn between unaccusat ~ ve verbs and the other two classes Table 6 shows the result of applying C5 0 -LRB- 10-fold eross-vahdatmn repeated 50 t ~ mes -RRB- to the three-x ~ ay classfficatmn task using the PRo feature m conjunctmn w ~ th the four previous features ~ ccuracy ranproves to over 70 % a teductmn m the error rate of almost 20 % due to th ~ s single nex ~ feature Moteover classifying the unaccusat ~ ve an2 object-drop verbs using the new feature m conjunctmn w ~ th the prevmus four leads to accuracy of over 68 % -LRB- compared to 58 % w ~ thout PRo -RRB- We conclude that this feature ~ s ~ mportant in d ~ stmgmshlng unaccusat ~ ve and object-drop verbs and hkely contributes to the tmprovement m the three-way classtficatton because of th ~ s Future work wdl examine the performance w ~ thm the verb classes of th ~ s new set of features to see whether accuracy has also tmproved for unergatire verbs 5 Conclusions In thin paper we have presented an m-depth case study m whmh we investigate varmus machine learnmg techmques to automatically classify a set of verbs based on dlstnbutmnal features extracted from a very large corpus Results show that a small number of hngmstlcally motivated grammatical features are sufficmnt to reduce the error rate by mote than 50 % over chance acluevmg a 70 % acctuacy rate m a three-way classfficatmn task Tins leads us to conclude that corpus data is a usable repository of verb class mformatmn On one hand ~ e observe that semantlc propemes of verb classes -LRB- such as causatlvlty or ammacy of subject -RRB- may be usefully approximated through countable syntactic features Even with some noise lexmal propertms are reflected m the corpus robustly enough to positively contribute m classlficatmn On the other hand however we remark that deep hngumtm analysis can not be ehmmated -- m our approach it is embedded m the selection of the features to count We also think that using hngumtlcally motivated features makes the approach very effective and easdy scalable we report a 56 % reductmn m error rate w ~ th only five features that are relatwely straightforward to count Acknowledgements This research was partly sponsored by the S ~ lss Natmnal Scmnce Foundatmn under fello ~ slup 821046569 to Paola Merlo by the US Natmnal Scmnce Foundatmn under grants # 9702331 and # 9818322 to $ uzanne Stevenson and by the Infotmatton Sciences Councd of Rutgers Umverslty ~ ~ e thank Martha Palmer for getting us started on tlus ~ ork and Mmhael Colhns for gwmg us access to the output of his parser We gratefully acknowledge the help of Ixlva Dickinson ~ ho calculated no ~ mahzatmns of the corpus data Appendix A The une ~ gatx ~ es are manner of morton ~ erbs jumptd rushed malched leaped floated laced huslwd uandered vaulted paraded galloped gl ded hzked hopped jogged scooted ncurlzed ~ kzpped hptoed trotted The unaccusau ~ es are verbs of change of state opened exploded flooded dzs ~ olved cracked hardened bozled melted fractured ol dzfied collapsed cooled folded w ~ dened changed clealed dzwded ~ mmered stabdzzed The object-dlop verbs are unspecffied object altelnatron verbs played painted k cked carved reaped washed danced yelled typed kmtted bolrowed mhet21 tted organtzed rented sketched cleaned packed studted swallowed called References Thomas G Bever 1970 The cogmtwe basis for hngmstlc structure In J R Hayes e & tor Cognttson and the Development of Language John Wdey New York Michael Brent 1993 From grammar to le ~ con Unsupervmed learmng of \ -LSB- ex ~ cal syntax Computational Linguistics 19 -LRB- 2 -RRB- 243-262 Edward Bnscoe and Ann Copestake 1995 Lex ~ cal rules m the TDFS framework Techmcal report AcquflexI I Working Papers Anne-Marm Brousseau and Ehzabeth R ~ tter 1991 A non-umfied analysis of agent ~ ve verbs In West Coast Conference on Formal Lmgutstzcs number 20 pages 53-64 M ~ chael John Colhns 1997 Three generaUve lexacahsed models for statistical parsmg In Proc of the ~ 5th Annual Meeting of the ACL pages 16-23 Hoa Trang Dang Kann K ~ pper Martha Palmer and Joseph Rosenzwe ~ g 1998 Investtgatmg regular sense extenmons based on mteresecttve Levm classes In Proc of the 361h Annual Meeting of the ACL and the 171h \ -LSB- nternatwnal Conference on Computatwnal L ngu st cs -LRB- COLING-A CL ' 98 -RRB- pages 293-299 Montreal Canada Umvers ~ t6 de Montreal Bonme Dorr and Doug Jones 1996 Role of word sense d ~ samb ~ guatmn m lexacal acqms ~ tmn Predmtmg semantics from syntactic cues In Proc of the 161h Internattonal Conference on Computat * onal Lmgutsttcs pages 322-327 Copenhagen Denmark COLING Bonnie Dorr 1997 Large-scale chctmnary constructmn for foreign language tutonng and mterhngual machine translatmn Machine Translatton 12 1-55 Hana Fd ~ p M ~ chael Tanenhaus Greg Carlson Paul AIlopenna and Joshua Blatt 1999 Reduced relatives judged hard require constraint-based analyses In P Merlo and S Stevenson echtors Sentence Processmg and the Lextcon Formal Computational and Ezpertmental Perspectives John Benjamms Holland Ken Hale and Jay Keyser 1993 On argument structure and the lexacal representatmn of s ~ ntact ~ c relatmns In K Hale and J Keyser editors The t ' ew from Budding ~ 0 pages 53-110 MIT Press Juchth L Ixlavans and Martin Chodorow 1992 Degrees of stat ~ vlty The lexacal representatmn of verb aspect In Proceedmg ~ of the Fourteenth International Conference on Computahonal Lmgmst cs Juchth Ixlavans and Mm-Yen Kan 1998 Role of ~ erbs m document analysis In Proc of the 361h Annual Meeting of the ACL and the 171h \ -LSB- nternatzonal Conference on Computational Lmgutsttcs -LRB- C O L L' v G4 C L ' 98 -RRB- pages 680-686 Montreal Canada Umvers ~ te de Montreal Beth Levm and/Vlalka Rappapti -LRB- t ` Hovav 1995 -LRB- Jnaccusatwlty MIT Press Cambridge MA Beth Le ~ m 1993 Enghsh Verb Clas ~ e ~ and 4lternatwns Chacago Umvers ~ ty Press Chicago IL Maryellen C MacDonald 1994 Probablhstlc constramts and syntactic amblgtuty resolution Language and Cognltzve Processes 9 -LRB- 2 -RRB- 157-201 Paola Merlo and Suzanne Stevenson 1998 What grammars tell us about corpora the case of reduced relative clauses In P1oceedmgs of the Slzth Workshop on Very Large Corpora pages 134-142 Montreal CA George Miller R Beckw ~ th C Fellbaum D Gross and Ix I ~ hller 1990 Fwe papers on Wordnet Techmcal report Cogmtzve Scmnce Lab Princeton Ual ~ erstt ~ Martha Palmer 1999 Coasmtent criteria for sense distmctmns Computmg \ -RSB- or the Hamamttes Fernando Perelra Naftah Tlshby and Ldhan Lee 1993 Dlstrabutmnal clustering of enghsh words \ -LSB- n Proc of the 31th 4nnual Meeting of the 4CL pages 183-190 Fernando Perexra Ido Dagan and Lalhan Lee 1997 Slmdanty-based methods for word sense dlsamblguatmn In Proc of the 35th Annual Meeting of the 4 CL and the 8th Conf of the E 4 CL -LRB- A CL/EA CL ' 97 -RRB- pages 56 -63 Geoffrey K Pullum 1996 Learnabthty hyperlearnrag and the poverty of the sttmulus In Jan Johnson Matthew L Jute and Jen L Moxley editors ~ nd Annual Meeting of the Berkeley Lmgutstzcs Soctety General Sesston and Parasesswn on the Role of Learnabdzty m Grammatzcal Theory pages 498-513 Berkeley Cahforma Berkeley Linguistics Socmty James Pustejovsky 1995 The Generatwe Lexicon MIT Press J Ross Qumlan 1992 C$ 5 Programs fo ~ Machine Learning Series m Machme Learning Morgan Ixaufmann San Mateo C 4	num_C_4 nn_Mateo_San appos_Ixaufmann_C appos_Ixaufmann_Mateo nn_Ixaufmann_Morgan nn_Ixaufmann_Learning nn_Ixaufmann_Machme nn_Ixaufmann_m nn_Ixaufmann_Series nn_Ixaufmann_Learning nn_Ixaufmann_Machine nn_Ixaufmann_~ nn_Ixaufmann_fo nn_Ixaufmann_Programs dep_Ixaufmann_C$ num_Ixaufmann_1992 number_C$_5 dep_Qumlan_Ixaufmann nn_Qumlan_Ross nn_Qumlan_J nn_Qumlan_Press nn_Qumlan_MIT dep_Lexicon_Qumlan dep_Generatwe_Lexicon dep_The_Generatwe dep_1995_The dep_Pustejovsky_1995 nn_Pustejovsky_James nn_Pustejovsky_Socmty nn_Pustejovsky_Linguistics nn_Pustejovsky_Berkeley nn_Pustejovsky_Cahforma num_pages_498-513 appos_Theory_Pustejovsky conj_Theory_Berkeley conj_Theory_pages nn_Theory_Grammatzcal nn_Theory_m nn_Theory_Learnabdzty prep_of_Role_Theory det_Role_the conj_and_Sesston_Parasesswn nn_Sesston_General nn_Sesston_Soctety nn_Sesston_Lmgutstzcs nn_Sesston_Berkeley det_Sesston_the prep_on_Meeting_Role prep_of_Meeting_Parasesswn prep_of_Meeting_Sesston amod_Meeting_Annual xcomp_nd_Meeting dep_~_nd amod_Moxley_~ appos_Moxley_editors nn_Moxley_L nn_Moxley_Jen nn_Jute_L nn_Jute_Matthew conj_and_Johnson_Moxley conj_and_Johnson_Jute nn_Johnson_Jan det_sttmulus_the prep_in_poverty_Moxley prep_in_poverty_Jute prep_in_poverty_Johnson prep_of_poverty_sttmulus det_poverty_the conj_and_Learnabthty_poverty conj_and_Learnabthty_hyperlearnrag num_Learnabthty_1996 dep_Pullum_poverty dep_Pullum_hyperlearnrag dep_Pullum_Learnabthty nn_Pullum_K nn_Pullum_Geoffrey num_Pullum_-63 number_-63_56 dep_pages_Pullum num_CL_97 nn_CL_CL/EA nn_CL_A dep_CL_pages appos_CL_CL num_CL_4 nn_CL_E det_CL_the amod_Conf_8th det_Conf_the prep_of_CL_CL conj_and_CL_Conf num_CL_4 det_CL_the prep_of_Meeting_Conf prep_of_Meeting_CL amod_Meeting_Annual amod_Meeting_35th det_Meeting_the prep_of_Proc_Meeting nn_dlsamblguatmn_sense nn_dlsamblguatmn_word prep_for_methods_dlsamblguatmn amod_methods_Slmdanty-based num_methods_1997 nn_methods_Lee nn_methods_Lalhan nn_Dagan_Ido prep_in_Perexra_Proc conj_and_Perexra_methods conj_and_Perexra_Dagan nn_Perexra_Fernando num_Perexra_183-190 dep_pages_methods dep_pages_Dagan dep_pages_Perexra det_4CL_the dobj_Meeting_pages prep_of_Meeting_4CL dep_4nnual_Meeting amod_4nnual_31th det_4nnual_the prep_of_Proc_4nnual nn_Proc_n dobj_\_Proc nsubj_\_clustering nsubj_\_Tlshby nsubj_\_Perelra nsubj_\_erstt nsubj_\_Lab nsubj_\_Ix nsubj_\_Gross nsubj_\_Fellbaum discourse_\_th nsubj_\_~ amod_words_enghsh prep_of_clustering_words nn_clustering_Dlstrabutmnal num_clustering_1993 nn_clustering_Lee nn_clustering_Ldhan nn_Tlshby_Naftah nn_Perelra_Fernando nn_Perelra_Hamamttes det_Perelra_the nn_\_Computmg nn_\_distmctmns nn_\_sense prep_for_criteria_\ nn_criteria_Coasmtent num_criteria_1999 nn_criteria_Palmer nn_criteria_Martha num_criteria_~ dep_erstt_criteria nn_erstt_~ nn_erstt_Ual nn_erstt_Princeton nn_Lab_Scmnce nn_Lab_Cogmtzve nn_report_Techmcal nn_report_Wordnet prep_on_papers_report nn_papers_Fwe num_papers_1990 amod_papers_hller dobj_~_papers nsubj_~_I rcmod_Ix_~ nn_Gross_D conj_and_Fellbaum_clustering conj_or_Fellbaum_Tlshby conj_or_Fellbaum_Perelra conj_and_Fellbaum_erstt conj_and_Fellbaum_Lab conj_and_Fellbaum_Ix conj_and_Fellbaum_Gross nn_Fellbaum_C nn_~_Beckw nn_~_R nn_Miller_George nn_Miller_CA num_pages_134-142 amod_Corpora_Large advmod_Large_Very nn_Workshop_Slzth det_Workshop_the prep_of_P1oceedmgs_Workshop prep_in_clauses_P1oceedmgs amod_clauses_relative amod_clauses_reduced dep_case_\ conj_case_Miller conj_case_Montreal conj_case_pages prep_on_case_Corpora prep_of_case_clauses det_case_the tmod_tell_case prep_about_tell_corpora dobj_tell_us nsubj_tell_grammars det_grammars_What num_Stevenson_1998 nn_Stevenson_Suzanne nn_Merlo_Paola num_Merlo_157-201 dep_157-201_2 number_157-201_9 nn_Processes_Cognltzve nn_Language_resolution nn_Language_amblgtuty amod_Language_syntactic nn_constramts_Probablhstlc num_constramts_1994 nn_constramts_MacDonald nn_MacDonald_C nn_MacDonald_Maryellen nn_MacDonald_IL nn_Press_ty nn_Press_~ nn_Press_Umvers nn_Press_Chacago nn_Press_4lternatwns rcmod_~_tell conj_and_~_Stevenson conj_and_~_Merlo conj_and_~_Processes conj_and_~_Language conj_and_~_constramts conj_and_~_Chicago conj_and_~_Press dep_~_e dep_~_Stevenson dep_~_Merlo dep_~_Processes dep_~_Language dep_~_constramts dep_~_Chicago dep_~_Press dep_~_~ nn_~_Clas nn_~_Verb nn_~_Enghsh num_~_1993 nn_~_m nn_~_~ nn_~_Le nn_~_Beth nn_~_MA appos_Press_~ appos_Press_Cambridge nn_Press_MIT nn_Press_Jnaccusatwlty num_Hovav_1995 dep_t_Press dep_t_Hovav dep_Rappapti_t nn_Rappapti_and/Vlalka nn_Rappapti_Levm nn_Rappapti_Beth nn_Rappapti_Montreal nn_Rappapti_de nn_Rappapti_te nn_Rappapti_~ nn_Rappapti_Umvers nn_Rappapti_Canada appos_pages_Rappapti conj_pages_Montreal num_pages_680-686 appos_L_pages num_L_98 nn_L_C nn_L_G4 dep_v_L amod_L'_v dep_L_L' dep_O_L dep_C_O dep_Lmgutsttcs_C amod_Lmgutsttcs_Computational prep_on_Conference_Lmgutsttcs amod_Conference_nternatzonal dep_\_Conference amod_\_171h det_\_the conj_and_ACL_\ det_ACL_the prep_of_Meeting_\ prep_of_Meeting_ACL amod_Meeting_Annual amod_Meeting_361h det_Meeting_the prep_of_Proc_Meeting nn_analysis_document nn_analysis_m nn_analysis_erbs nn_analysis_~ num_Role_1998 nn_Role_Kan nn_Role_Mm-Yen prep_of_Ixlavans_analysis conj_and_Ixlavans_Role nn_Ixlavans_Juchth prep_in_cs_Proc dobj_cs_Role dobj_cs_Ixlavans nsubj_cs_Keyser nsubj_cs_Hale mark_cs_In nn_Lmgmst_Computahonal prep_on_Conference_Lmgmst nn_Conference_International amod_Conference_Fourteenth det_Conference_the prep_of_~_Conference nn_~_Proceedmg prep_in_verb_~ dobj_verb_aspect prepc_of_representatmn_verb amod_representatmn_lexacal det_representatmn_The num_vlty_~ nn_vlty_stat dep_Degrees_representatmn prep_of_Degrees_vlty num_Degrees_1992 dep_Chodorow_Degrees nn_Chodorow_Martin conj_and_Ixlavans_Chodorow nn_Ixlavans_L nn_Ixlavans_Juchth nn_Ixlavans_Press nn_Ixlavans_MIT num_Ixlavans_53-110 dep_pages_Chodorow dep_pages_Ixlavans appos_~_pages num_~_0 dobj_Budding_~ prepc_from_ew_Budding det_t_The appos_Keyser_editors nn_Keyser_J appos_Hale_ew appos_Hale_t conj_and_Hale_Keyser nn_Hale_K dep_relatmns_cs nn_relatmns_c nn_relatmns_~ nn_relatmns_ntact nn_relatmns_~ prep_of_representatmn_s amod_representatmn_lexacal det_representatmn_the nn_structure_argument num_Keyser_1993 nn_Keyser_Jay prep_on_Hale_structure conj_and_Hale_Keyser nn_Hale_Ken nn_Hale_Holland nn_Benjamms_John amod_Perspectives_Ezpertmental conj_and_Formal_representatmn conj_and_Formal_Keyser conj_and_Formal_Hale conj_and_Formal_Benjamms conj_and_Formal_Perspectives conj_and_Formal_Computational nn_Formal_Lextcon det_Formal_the nn_Processmg_Sentence nn_Stevenson_S conj_and_Merlo_Stevenson nn_Merlo_P conj_and_analyses_representatmn conj_and_analyses_Hale conj_and_analyses_Benjamms conj_and_analyses_Perspectives conj_and_analyses_Computational conj_and_analyses_Formal conj_and_analyses_Processmg conj_and_analyses_echtors prep_in_analyses_Stevenson prep_in_analyses_Merlo amod_analyses_constraint-based dobj_require_Formal dobj_require_Processmg dobj_require_echtors dobj_require_analyses advmod_require_hard dep_require_judged nsubj_require_relatives nsubj_require_AIlopenna nsubj_require_Carlson nsubj_require_Tanenhaus nsubj_require_constructmn nsubj_require_Copenhagen nsubj_require_322-327 dep_require_pages amod_relatives_Reduced num_relatives_1999 nn_relatives_Blatt nn_Blatt_Joshua nn_AIlopenna_Paul nn_Carlson_Greg nn_Tanenhaus_chael nn_Tanenhaus_~ nn_Tanenhaus_M nn_Tanenhaus_p nn_Tanenhaus_~ nn_Tanenhaus_Fd nn_Tanenhaus_Hana num_Tanenhaus_1-55 number_1-55_12 nn_Translatton_Machine nn_Translatton_translatmn nn_Translatton_machine amod_Translatton_mterhngual conj_and_tutonng_Translatton nn_tutonng_language amod_tutonng_foreign prep_for_constructmn_Translatton prep_for_constructmn_tutonng amod_constructmn_chctmnary amod_constructmn_Large-scale num_constructmn_1997 nn_constructmn_Dorr nn_constructmn_Bonnie nn_constructmn_COLING nn_constructmn_Denmark conj_and_322-327_relatives conj_and_322-327_AIlopenna conj_and_322-327_Carlson conj_and_322-327_Tanenhaus conj_and_322-327_constructmn conj_and_322-327_Copenhagen rcmod_Lmgutsttcs_require amod_Lmgutsttcs_onal dep_Lmgutsttcs_* nn_Lmgutsttcs_Computat prep_on_Conference_Lmgutsttcs nn_Conference_Internattonal nn_Conference_161h det_Conference_the prep_of_Proc_Conference prep_in_cues_Proc amod_cues_syntactic nn_semantics_Predmtmg nn_semantics_tmn dobj_~_semantics amod_acqms_lexacal nn_acqms_m nn_acqms_guatmn nn_acqms_~ nn_acqms_samb num_acqms_~ nn_acqms_d nn_acqms_sense nn_acqms_word prep_of_Role_acqms num_Role_1996 dep_Jones_Role nn_Jones_Doug nn_Dorr_Bonme nn_Dorr_Montreal amod_Dorr_de nn_Dorr_t6 nn_Dorr_~ nn_Dorr_Umvers nn_Dorr_Canada num_pages_293-299 num_CL_98 nn_CL_COLING-A appos_cs_CL conj_and_L_Jones conj_and_L_Dorr conj_and_L_Montreal conj_and_L_pages conj_and_L_cs conj_and_L_st conj_and_L_ngu amod_L_Computatwnal dep_Conference_relatmns prep_from_Conference_cues dep_Conference_~ prep_on_Conference_Jones prep_on_Conference_Dorr prep_on_Conference_Montreal prep_on_Conference_pages prep_on_Conference_cs prep_on_Conference_st prep_on_Conference_ngu prep_on_Conference_L amod_Conference_nternatwnal appos_\_Conference amod_\_171h det_\_the conj_and_ACL_\ det_ACL_the prep_of_Meeting_\ prep_of_Meeting_ACL amod_Meeting_Annual amod_Meeting_361h det_Meeting_the prep_of_Proc_Meeting nn_classes_Levm nn_classes_mteresecttve prep_on_based_classes nn_extenmons_sense amod_extenmons_regular nn_extenmons_Investtgatmg nn_extenmons_Rosenzwe dep_1998_g number_1998_~ num_Rosenzwe_1998 nn_Rosenzwe_Joseph nn_Palmer_Martha nn_pper_~ nn_pper_K nn_pper_Kann conj_and_Dang_extenmons conj_and_Dang_Palmer conj_and_Dang_pper nn_Dang_Trang nn_Dang_Hoa num_Dang_16-23 prep_in_pages_Proc vmod_pages_based dep_pages_extenmons dep_pages_Palmer dep_pages_pper dep_pages_Dang det_ACL_the prep_of_Meeting_ACL amod_Meeting_Annual vmod_5th_Meeting nn_5th_~ det_5th_the prep_of_Proc_5th amod_parsmg_statistical prep_for_models_parsmg amod_models_lexacahsed prep_in_generaUve_Proc appos_generaUve_models num_generaUve_Three number_Three_1997 dep_Colhns_pages dep_Colhns_generaUve nn_Colhns_John dobj_chael_Colhns nsubj_chael_~ nn_~_M num_~_53-64 rcmod_pages_chael num_number_20 nn_Lmgutstzcs_Formal prep_on_Conference_Lmgutstzcs nn_Conference_Coast nn_Conference_West nn_verbs_ve num_verbs_~ nn_verbs_agent amod_analysis_non-umfied det_analysis_A num_analysis_1991 nn_analysis_tter nn_analysis_~ nn_analysis_R nn_analysis_Ehzabeth conj_and_Brousseau_analysis nn_Brousseau_Anne-Marm nn_Brousseau_Papers dep_Brousseau_Linguistics nsubj_Working_I rcmod_AcquflexI_Working amod_report_Techmcal nn_report_framework nn_report_TDFS det_report_the dep_m_AcquflexI dep_m_report nn_m_rules amod_m_cal nn_m_~ nn_m_Lex num_m_1995 dep_Copestake_m nn_Copestake_Ann conj_and_Bnscoe_Copestake nn_Bnscoe_Edward num_Bnscoe_243-262 dep_243-262_2 number_243-262_19 appos_Linguistics_Copestake appos_Linguistics_Bnscoe nn_Linguistics_Computational nn_Linguistics_syntax amod_Linguistics_cal nn_Linguistics_~ nn_Linguistics_ex prep_of_learmng_\ amod_learmng_Unsupervmed nn_learmng_con nn_learmng_~ det_learmng_le prep_to_Brent_learmng prep_from_Brent_grammar num_Brent_1993 nn_Brent_Michael nn_Brent_York nn_Brent_New appos_Wdey_Brent nn_Wdey_John nn_Wdey_Language prep_of_Development_Wdey det_Development_the conj_and_e_tor nn_Hayes_R nn_Hayes_J prep_in_structure_Hayes nn_structure_hngmstlc prep_for_basis_structure nn_basis_cogmtwe det_basis_The prep_in_Bever_Conference prep_of_Bever_verbs dep_Bever_analysis dep_Bever_Brousseau conj_and_Bever_Development conj_and_Bever_Cognttson conj_and_Bever_tor conj_and_Bever_e dep_Bever_basis num_Bever_1970 nn_Bever_G nn_Bever_Thomas nn_Bever_References dep_called_Development dep_called_Cognttson dep_called_e dep_called_Bever vmod_tted_called conj_tted_swallowed conj_tted_studted conj_tted_packed conj_tted_cleaned conj_tted_sketched conj_tted_rented conj_tted_organtzed dep_mhet21_tted amod_mhet21_bolrowed amod_mhet21_kmtted conj_played_pages conj_played_number conj_played_mhet21 conj_played_typed conj_played_yelled conj_played_danced conj_played_washed conj_played_reaped conj_played_carved conj_played_cked conj_played_k conj_played_painted dep_verbs_played nn_verbs_altelnatron nn_verbs_object amod_verbs_unspecffied cop_verbs_are nsubj_verbs_verbs amod_verbs_object-dlop det_verbs_The dep_stabdzzed_verbs amod_stabdzzed_mmered vmod_~_dened nn_~_w conj_folded_stabdzzed conj_folded_~ conj_folded_dzwded conj_folded_clealed conj_folded_changed conj_folded_~ dep_cooled_folded dep_collapsed_cooled appos_ol_collapsed vmod_ol_dzfied dep_fractured_ol dep_bozled_fractured conj_bozled_melted xcomp_hardened_bozled conj_olved_hardened conj_olved_cracked dep_~_olved dep_dzs_~ conj_opened_dzs conj_opened_flooded conj_opened_exploded prep_of_change_state dep_verbs_opened prep_of_verbs_change cop_verbs_are nsubj_verbs_help nn_es_~ amod_es_unaccusau det_es_The dobj_trotted_es nsubj_trotted_jogged dep_trotted_hopped dep_trotted_hzked nsubj_trotted_uandered dep_trotted_morton mark_trotted_of nsubj_kzpped_~ conj_jogged_hptoed conj_jogged_kzpped conj_jogged_ncurlzed conj_jogged_scooted conj_uandered_ded conj_uandered_gl conj_uandered_galloped conj_uandered_paraded conj_uandered_vaulted amod_uandered_huslwd nsubj_laced_erbs acomp_leaped_floated conj_rushed_leaped conj_rushed_malched dep_jumptd_rushed amod_erbs_jumptd num_erbs_~ rcmod_morton_laced vmod_manner_trotted cop_manner_are nsubj_manner_corpus det_manner_the num_es_~ nn_es_gatx nn_es_~ amod_es_une det_es_The det_es_A dep_Appendix_es dep_data_Appendix dobj_corpus_data prep_of_mahzatmns_manner nn_mahzatmns_~ neg_mahzatmns_no dobj_calculated_mahzatmns vmod_ho_calculated num_ho_~ appos_Dickinson_ho nn_Dickinson_Ixlva prep_of_help_Dickinson det_help_the ccomp_acknowledge_verbs advmod_acknowledge_gratefully nsubj_acknowledge_We rcmod_parser_acknowledge poss_parser_his prep_of_output_parser det_output_the prep_to_access_output dep_access_us nn_access_gwmg nn_Colhns_Mmhael conj_and_ork_Colhns nn_ork_~ amod_ork_tlus prep_for_started_access prep_on_started_Colhns prep_on_started_ork nsubj_started_us ccomp_getting_started nn_Palmer_Martha prepc_for_thank_getting dobj_thank_Palmer aux_thank_e nsubj_thank_Foundatmn aux_thank_lss appos_~_~ nn_~_Umverslty nn_~_Rutgers prep_of_Councd_~ nn_Councd_Sciences nn_Councd_Infotmatton det_Councd_the pobj_by_Councd conj_and_Stevenson_by amod_Stevenson_uzanne amod_Stevenson_$ dep_$_to dep_$_9818322 dep_$_9702331 dep_$_grants dep_$_under dep_9818322_# conj_and_9702331_9818322 dep_9702331_# nn_Foundatmn_Scmnce nn_Foundatmn_Natmnal nn_Foundatmn_US det_Foundatmn_the nn_Merlo_Paola num_slup_821046569 nn_slup_~ amod_slup_fello appos_Foundatmn_by appos_Foundatmn_Stevenson prep_by_Foundatmn_Foundatmn prep_to_Foundatmn_Merlo prep_under_Foundatmn_slup nn_Foundatmn_Scmnce nn_Foundatmn_Natmnal nn_~_S det_~_the agent_sponsored_~ advmod_sponsored_partly auxpass_sponsored_was nsubjpass_sponsored_features dep_sponsored_~ det_research_This nn_research_Acknowledgements dobj_count_research aux_count_to xcomp_straightforward_count advmod_straightforward_relatwely cop_straightforward_are nsubj_straightforward_that rcmod_features_straightforward num_features_five quantmod_five_only quantmod_five_th nn_~_w dep_rate_thank conj_rate_sponsored nn_rate_error nn_rate_m nn_rate_reductmn amod_rate_% det_rate_a number_%_56 dobj_report_rate nsubj_report_we amod_scalable_easdy nsubj_scalable_approach dep_effective_report conj_and_effective_scalable advmod_effective_very nsubj_effective_approach det_approach_the xcomp_makes_scalable xcomp_makes_effective csubj_makes_using mark_makes_that amod_features_motivated advmod_motivated_hngumtlcally dobj_using_features ccomp_think_makes advmod_think_also nsubj_think_selection dobj_count_We aux_count_to det_features_the vmod_selection_count prep_of_selection_features det_selection_the rcmod_m_think amod_m_embedded cop_m_is nsubj_m_it poss_approach_our nn_approach_m parataxis_ehmmated_m dep_ehmmated_approach auxpass_ehmmated_be neg_ehmmated_not aux_ehmmated_can nsubjpass_ehmmated_analysis mark_ehmmated_that nn_analysis_hngumtm amod_analysis_deep ccomp_remark_ehmmated nsubj_remark_we amod_hand_other det_hand_the nn_classlficatmn_m prep_on_contribute_hand dobj_contribute_classlficatmn advmod_contribute_positively aux_contribute_to advmod_enough_robustly vmod_corpus_contribute advmod_corpus_enough det_corpus_the rcmod_m_remark advmod_m_however dep_m_corpus dobj_reflected_m auxpass_reflected_are nsubjpass_reflected_propertms amod_propertms_lexmal dep_noise_reflected det_noise_some nn_features_syntactic amod_features_countable prep_with_approximated_noise advmod_approximated_Even prep_through_approximated_features advmod_approximated_usefully auxpass_approximated_be aux_approximated_may nsubjpass_approximated_verb mark_approximated_of prep_of_ammacy_subject conj_or_causatlvlty_ammacy prep_such_as_classes_ammacy prep_such_as_classes_causatlvlty dobj_verb_classes dep_propemes_approximated nn_propemes_semantlc dep_that_propemes dep_observe_that nsubj_observe_m dep_observe_acluevmg nn_~_hand num_~_one nn_mformatmn_class dep_verb_e prep_on_verb_~ dobj_verb_mformatmn prep_of_repository_verb amod_repository_usable det_repository_a cop_repository_is nsubj_repository_data mark_repository_that nn_data_corpus ccomp_conclude_repository aux_conclude_to xcomp_leads_conclude dobj_leads_us nsubj_leads_Tins nn_Tins_task nn_Tins_classfficatmn amod_Tins_three-way det_Tins_a rcmod_m_leads nn_m_rate nn_m_acctuacy amod_m_% det_m_a number_%_70 num_%_50 nn_rate_error det_rate_the prep_over_reduce_chance prep_than_reduce_% prep_by_reduce_mote dobj_reduce_rate aux_reduce_to dep_sufficmnt_observe xcomp_sufficmnt_reduce cop_sufficmnt_are nsubj_sufficmnt_number mark_sufficmnt_that amod_features_grammatical amod_features_motivated advmod_motivated_hngmstlcally prep_of_number_features amod_number_small det_number_a ccomp_show_sufficmnt nsubj_show_whmh nn_Results_corpus amod_Results_large det_Results_a advmod_large_very prep_from_extracted_Results vmod_features_extracted amod_features_dlstnbutmnal prep_on_based_features vmod_set_based prep_of_set_verbs det_set_a dobj_classify_set advmod_classify_automatically aux_classify_to nn_techmques_learnmg nn_techmques_machine nn_techmques_varmus xcomp_investigate_classify dobj_investigate_techmques nsubj_investigate_we rcmod_whmh_investigate nn_whmh_m nn_study_case amod_study_m-depth det_study_an dep_presented_show dobj_presented_study aux_presented_have nsubj_presented_we amod_paper_thin num_Conclusions_5 prep_in_verbs_paper dep_verbs_Conclusions amod_verbs_unergatire prep_for_tmproved_verbs advmod_tmproved_also aux_tmproved_has nsubj_tmproved_accuracy mark_tmproved_whether ccomp_see_tmproved aux_see_to prep_of_set_features amod_set_new amod_set_s det_set_th advmod_s_~ prep_of_classes_set amod_classes_verb det_classes_the prep_thm_~_classes nn_~_w nn_~_performance det_~_the xcomp_examine_see dobj_examine_~ nn_wdl_work amod_wdl_Future parataxis_s_presented vmod_s_examine dobj_s_wdl advmod_s_~ nsubj_s_th amod_classtficatton_three-way det_classtficatton_the dep_m_classtficatton nn_m_tmprovement det_m_the prepc_because_of_contributes_s prep_to_contributes_m advmod_contributes_hkely nsubj_contributes_s amod_verbs_object-drop conj_and_ve_verbs num_ve_~ nn_ve_unaccusat nn_ve_stmgmshlng nn_ve_~ nn_ve_d prep_in_mportant_verbs prep_in_mportant_ve conj_and_~_contributes acomp_~_mportant nsubj_~_s mark_~_that nn_s_~ nn_s_feature det_s_this ccomp_conclude_contributes ccomp_conclude_~ nsubj_conclude_We amod_PRo_thout nn_PRo_~ nn_PRo_w amod_PRo_% number_%_58 prep_compared_to_%_PRo num_%_68 quantmod_68_over prep_of_accuracy_% ccomp_leads_conclude prep_to_leads_accuracy nsubj_leads_ranproves num_prevmus_four det_prevmus_the pobj_th_prevmus dep_~_th dep_w_~ nn_w_conjunctmn nn_w_m nn_w_feature amod_w_new det_w_the dobj_using_w amod_verbs_object-drop nn_verbs_an2 nn_verbs_ve nn_verbs_~ amod_verbs_unaccusat det_verbs_the vmod_classifying_using dobj_classifying_verbs nn_Moteover_feature nn_Moteover_~ nn_Moteover_nex amod_Moteover_single dobj_s_Moteover det_~_th num_%_20 quantmod_20_almost vmod_rate_s prep_due_to_rate_~ prep_of_rate_% nn_rate_error det_rate_the dep_m_rate nn_m_teductmn det_m_a num_%_70 quantmod_70_over vmod_ranproves_classifying appos_ranproves_m prep_to_ranproves_% nn_ranproves_ccuracy nsubj_~_features amod_features_previous num_features_four det_features_the dep_features_th num_features_~ rcmod_w_~ nn_w_conjunctmn nn_w_m nn_w_feature nn_w_PRo det_w_the dobj_using_w vmod_task_using nn_task_classfficatmn nn_task_ay nn_task_~ amod_task_three-x det_task_the nn_mes_~ nn_mes_t num_mes_50 amod_mes_repeated amod_mes_eross-vahdatmn advmod_mes_10-fold num_C5_0 prep_to_applying_task dep_applying_mes dobj_applying_C5 rcmod_result_leads prepc_of_result_applying det_result_the dobj_shows_result nsubj_shows_Table dep_shows_classes dep_shows_two dep_shows_other dep_shows_the num_Table_6 conj_and_verbs_shows nn_verbs_ve nn_verbs_~ amod_verbs_unaccusat nn_stmctmn_~ nn_stmctmn_d nn_stmctmn_thematm det_stmctmn_the prep_between_capture_shows prep_between_capture_verbs dobj_capture_stmctmn aux_capture_to det_feature_the prep_of_ty_feature nn_ty_~ nn_ty_abd det_ty_the xcomp_confirms_capture dobj_confirms_ty nsubj_confirms_s amod_pattern_s amod_pattern_~ nn_pattern_s nn_pattern_ent amod_pattern_~ nsubj_s_~ advmod_s_gmficantly nn_~_Th amod_~_fferent nn_~_~ nn_~_d neg_s_not cop_s_are nsubj_s_values advmod_s_Moreover nn_alternatmns_h num_alternatmns_~ nn_alternatmns_bo nn_alternatmns_m nn_alternatmns_Agents cop_alternatmns_are nsubj_alternatmns_subjects poss_subjects_whose appos_verbs_alternatmns amod_verbs_object-drop conj_and_ve_verbs nn_ve_~ amod_ve_unergat prep_for_values_verbs prep_for_values_ve nn_values_PRo det_values_the dep_Moreover_p dep_<_05 amod_p_< amod_verbs_object-drop amod_verbs_unergatlve conj_and_unergatlve_object-drop preconj_unergatlve_both prep_for_those_verbs prep_from_ent_those nn_ent_~ amod_ent_dtffe advmod_ent_gmficantly dep_s_pattern cop_s_are nsubj_s_values mark_s_that nn_alternatmns_tts prep_of_one_alternatmns dep_m_one nn_m_tect nn_m_sub det_m_the nn_role_Theme nn_role_gn num_role_~ nn_role_ass prep_to_class_m prep_to_class_role advmod_class_only det_class_the appos_verbs_class nn_verbs_ve nn_verbs_~ amod_verbs_unaccusat prep_for_values_verbs nn_values_Prto det_values_the ccomp_find_confirms nsubj_find_we nsubj_find_sets mark_find_whether amod_classes_verb det_classes_the nn_ffer_~ nn_ffer_d nn_ffer_values nn_ffer_PRo advmod_sets_Interestingly prep_across_sets_classes prep_of_sets_ffer det_sets_the ccomp_determine_find aux_determine_to amod_data_new poss_data_our prep_to_apply_data dobj_apply_t-tests advmod_apply_again nsubj_apply_We rcmod_subjects_apply det_subjects_all prep_of_occurrences_pronouns prep_of_ratm_occurrences det_ratm_the vmod_computing_determine prep_to_computing_subjects dobj_computing_ratm amod_corpus_parsed dep_WSJ88_corpus det_WSJ88_the prep_from_verbs_WSJ88 dep_examples_verbs num_examples_59 poss_examples_our prep_including_tuples_examples amod_tuples_subject/verb det_tuples_all dobj_extracting_tuples advmod_extracting_automatmally agent_determined_extracting auxpass_determined_were nsubjpass_determined_refer nsubjpass_determined_he dep_determined_she nsubjpass_determined_you dep_determined_we nsubjpass_determined_I nn_O_~ nn_O_P appos_feature_O amod_feature_new det_feature_the prep_for_values_feature det_values_The dep_tms_values nn_tms_~ amod_tms_ent dobj_ammate_tms prepc_to_refer_ammate advmod_refer_often nsubj_refer_they advmod_often_most conj_and_he_refer conj_and_words_computing rcmod_words_determined det_words_the dep_that_computing dep_that_words prep_s_that nsubj_s_~ nn_~_assumptmn det_~_The rcmod_positron_s dobj_subject_positron npadvmod_subject_m dep_subject_other num_t_~ prep_than_other_t dep_pronouns_subject dobj_count_pronouns advmod_count_instead nsubj_count_we det_corpus_the prep_to_external_corpus amod_resource_external det_resource_a prep_to_reference_resource nn_reference_thout nn_reference_~ nn_reference_w conj_and_extracted_count conj_and_extracted_reference advmod_extracted_automatically auxpass_extracted_be aux_extracted_can nsubjpass_extracted_that det_feature_a det_feature_th num_feature_~ dep_w_feature nn_w_ammacy amod_w_approximate prep_to_determining_w dobj_determining_ammacy dep_resource_count dep_resource_reference dep_resource_extracted prepc_for_resource_determining prep_as_resource_WordNet mwe_resource_such amod_resource_on-hne det_resource_an prep_to_reference_resource amod_subjects_extracted prep_of_antmacy_subjects det_antmacy_the conj_or_determmatmn_reference prep_of_determmatmn_antmacy nn_determmatmn_manual nn_determmatmn_etther dobj_requires_reference dobj_requires_determmatmn nsubj_requires_t mark_requires_that nn_t_~ ccomp_s_requires nsubj_s_~ nn_~_ammacy nn_~_hke nn_~_feature det_~_a predet_~_th rcmod_~_s num_w_~ dep_problem_w dep_A_problem dep_role_A nn_role_Theme det_role_the dobj_receive_role nsubj_receive_that rcmod_nouns_receive prep_than_ammate_nouns cop_ammate_be advmod_ammate_often aux_ammate_will nsubj_ammate_nouns mark_ammate_that advmod_ammate_rather advmod_often_more nn_role_Agent det_role_the dobj_receive_role nsubj_receive_that rcmod_nouns_receive conj_but_ammate_ammate cop_ammate_be aux_ammate_can nsubj_ammate_Agents mark_ammate_that advmod_Agents_only dep_s_ammate dep_s_ammate neg_s_not nsubj_s_~ nn_~_clatm det_~_the nn_dmtnbutmns_frequency prep_of_use_dmtnbutmns dobj_making_use amod_basehne_% num_basehne_33 number_%_8 vmod_Classes_making appos_Classes_basehne nn_Classes_Verb det_Classes_All nn_Feature_PRO nn_Feature_New amod_Feature_thout num_Feature_~ nn_Feature_W num_th_~ dep_W_th num_C5_0 dep_%_SE appos_Error_% amod_Error_Standard nn_%_Acc appos_Accuracy_% nn_Accuracy_Percentage num_Accuracy_6 nn_Accuracy_Table num_Accuracy_4 num_Accuracy_0 number_0_7 number_0_70 prep_of_PRO_C5 conj_and_PRO_Error dep_PRO_Accuracy dep_CAUS_Error dep_CAUS_PRO conj_and_INTR_Feature conj_and_INTR_W amod_INTR_CAUS parataxis_ACT_s dep_ACT_Classes dep_ACT_Feature dep_ACT_W dep_ACT_INTR nn_ACT_VBD dep_ACT_\ dep_ACT_0 num_\_6 num_0_7 number_7_63 dep_I_ACT dep_CAUS_I nn_CAUS_INTR nn_CAUS_ACT dobj_VBD_CAUS nsubj_VBD_I rcmod_%_VBD num_%_II dep_SE_% dep_%_SE dep_Acc_% dep_\_Acc vmod_Features_\ num_Features_II cop_Features_are nsubj_Features_~ mark_Features_that number_II_20 dep_~_e amod_Note_subject nn_Note_animate det_Note_an advmod_often_more ccomp_occur_Features advmod_occur_again prep_with_occur_Note advmod_occur_often aux_occur_will nsubj_occur_verbs mark_occur_that amod_verbs_object-drop ccomp_expect_occur advmod_expect_then nsubj_expect_We rcmod_intransitive_expect det_intransitive_the dep_m_intransitive nn_m_Theme det_transitive_the prep_in_subject_transitive advmod_subject_only poss_subject_their prep_to_assign_subject dobj_assign_Agent nsubj_assign_unaccusattves mark_assign_while amod_alternations_intransitive amod_alternations_transitive det_alternations_the preconj_alternations_both conj_and_transitive_intransitive prep_in_subject_alternations poss_subject_their conj_and_assign_m advcl_assign_assign prep_to_assign_subject dobj_assign_Agent nsubj_assign_verbs mark_assign_that nn_verbs_objectdrop ccomp_Recall_m ccomp_Recall_assign nsubj_Recall_role nn_role_Theme det_role_a ccomp_receive_Recall nsubj_receive_that rcmod_those_receive nn_those_flora dep_role_those nn_role_Agent det_role_an dobj_receive_role nsubj_receive_that rcmod_nouns_receive dobj_distinguish_nouns aux_distinguish_to xcomp_intended_distinguish auxpass_intended_is nsubjpass_intended_that prep_of_ammacy_subject prep_of_that_ammacy rcmod_feature_intended appos_feature_that det_feature_another dobj_suggest_feature nsubj_suggest_e advmod_suggest_~ cc_suggest_x advmod_suggest_Here dep_xetb_suggest amod_xetb_ative nn_xetb_~ nn_xetb_cau det_xetb_a nn_positions_object prep_of_ubject_xetb conj_and_ubject_positions nn_ubject_~ det_ubject_the dep_m_positions dep_m_ubject dobj_occur_m aux_occur_can nsubj_occur_that rcmod_nouns_occur nn_nouns_een nn_nouns_~ nn_nouns_beh nn_nouns_oxerlap det_nouns_the dobj_encoding_nouns vmod_~_encoding nn_~_b. nn_~_reformation nn_~_role amod_~_antic dep_~_\ nn_\_sen dobj_approximates_~ dep_feature_approximates nsubj_feature_bs nn_bs_~ nn_bs_c det_bs_the det_corpus_a prep_for_ate_example prep_in_ate_corpus dobj_ate_countable nsubj_ate_that rcmod_features_feature rcmod_features_ate amod_features_syntactic prep_of_expression_features det_expression_the dobj_affect_expression aux_affect_would nsubj_affect_differences mark_affect_that det_differences_these ccomp_hypothesized_affect amod_classes_verb det_classes_the conj_and_asmgnments_hypothesized prep_for_asmgnments_classes nn_asmgnments_role amod_asmgnments_semantic amod_asmgnments_differing det_asmgnments_the dobj_noted_hypothesized dobj_noted_asmgnments nsubj_noted_we number_1_2 num_Section_1 num_classes_two det_classes_these amod_values_different prep_in_have_Section prep_across_have_classes dobj_have_values nsubj_have_that rcmod_featules_have amod_featules_additional dobj_need_featules nsubj_need_we mark_need_that ccomp_indicating_need amod_verbs_object-dtop amod_verbs_unaccusattve conj_and_unaccusattve_object-dtop prep_for_different_verbs advmod_different_slgmficantly parataxis_ate_noted vmod_ate_indicating dobj_ate_different nsubj_ate_CAUS cc_ate_and
W99-0622	P97-1003	o	Also adding a constituent size/distance effect as described by Schubert -LRB- 1986 -RRB- and as used by some researchers in parsing -LRB- e.g. Lesmo and Torasso -LRB- 1985 -RRB- and Collins -LRB- 1997 -RRB- -RRB- would almost certainly improve parsing	dobj_improve_parsing advmod_improve_certainly aux_improve_would dep_improve_used dep_improve_described dep_improve_adding advmod_improve_Also advmod_certainly_almost appos_Collins_1997 dep_Torasso_1985 conj_and_Lesmo_Collins conj_and_Lesmo_Torasso nn_Lesmo_e.g. prep_in_researchers_parsing det_researchers_some dep_used_Collins dep_used_Torasso dep_used_Lesmo prep_by_used_researchers mark_used_as appos_Schubert_1986 conj_and_described_used prep_by_described_Schubert mark_described_as nn_effect_size/distance nn_effect_constituent det_effect_a dobj_adding_effect
W99-0622	P97-1003	n	While these approaches have had som e success to date -LRB- Collins 1997 Charniak 1997a -RRB- their usability as parsers in systems for natural language understanding is suspect	cop_suspect_is nsubj_suspect_usability advcl_suspect_had nn_understanding_language amod_understanding_natural prep_for_systems_understanding prep_in_parsers_systems prep_as_usability_parsers poss_usability_their appos_Charniak_1997a dep_Collins_Charniak amod_Collins_1997 dep_success_e nn_success_som dep_had_Collins prep_to_had_date dobj_had_success aux_had_have nsubj_had_approaches mark_had_While det_approaches_these
W99-0623	P97-1003	p	The corpus-based statistical parsing community has many fast and accurate automated parsing systems including systems produced by Collins -LRB- 1997 -RRB- Charniak -LRB- 1997 -RRB- and Ratnaparkhi -LRB- 1997 -RRB-	appos_Ratnaparkhi_1997 appos_Charniak_1997 conj_and_Collins_Ratnaparkhi conj_and_Collins_Charniak appos_Collins_1997 agent_produced_Ratnaparkhi agent_produced_Charniak agent_produced_Collins vmod_systems_produced prep_including_systems_systems nn_systems_parsing amod_systems_automated amod_systems_accurate amod_systems_fast conj_and_fast_accurate amod_fast_many dobj_has_systems nsubj_has_community nn_community_parsing amod_community_statistical amod_community_corpus-based det_community_The
W99-0629	P97-1003	o	-LRB- Collins 1997 Ratnaparkhi 1997 -RRB- use cascaded processing for full parsing with good results	amod_results_good prep_with_parsing_results amod_full_parsing amod_processing_cascaded prep_for_use_full dobj_use_processing dep_Ratnaparkhi_1997 dep_Collins_use dep_Collins_Ratnaparkhi appos_Collins_1997 dep_''_Collins
X98-1014	P97-1003	o	Statistical Model In SIFT 's statistical model augmented parse trees are generated according to a process similar to that described in Collins -LRB- 1996 1997 -RRB-	dep_1996_1997 dep_Collins_1996 prep_in_described_Collins vmod_that_described prep_to_similar_that amod_process_similar det_process_a pobj_generated_process prepc_according_to_generated_to auxpass_generated_are nn_trees_parse dep_augmented_generated dobj_augmented_trees nsubj_augmented_Model amod_model_statistical poss_model_SIFT prep_in_Model_model amod_Model_Statistical
A00-1020	P99-1048	o	One such technique is bootstrapping which was recently presented in -LRB- Riloff and Jones 1999 -RRB- -LRB- Jones et a1 .1999 -RRB- as an ideal framework for text learning tasks that have knowledge seeds	nn_seeds_knowledge dobj_have_seeds nsubj_have_that rcmod_tasks_have nn_tasks_learning nn_tasks_text prep_for_framework_tasks amod_framework_ideal det_framework_an dep_a1_.1999 nn_a1_et advmod_Jones_a1 num_Jones_1999 prep_as_Riloff_framework appos_Riloff_Jones conj_and_Riloff_Jones prep_in_presented_Jones prep_in_presented_Riloff advmod_presented_recently auxpass_presented_was nsubjpass_presented_which rcmod_bootstrapping_presented aux_bootstrapping_is nsubj_bootstrapping_technique amod_technique_such num_technique_One ccomp_``_bootstrapping
A00-1020	P99-1048	o	-LRB- Riloff and Jones 1999 -RRB- note that the bootstrapping algorithm works well but its performance can deteriorate rapidly when non-coreferring data enter as candidate heuristics	nn_heuristics_candidate prep_as_enter_heuristics nsubj_enter_data advmod_enter_when amod_data_non-coreferring advcl_deteriorate_enter advmod_deteriorate_rapidly aux_deteriorate_can nsubj_deteriorate_performance poss_performance_its conj_but_works_deteriorate advmod_works_well nsubj_works_algorithm mark_works_that nn_algorithm_bootstrapping det_algorithm_the ccomp_note_deteriorate ccomp_note_works dep_note_Jones dep_note_Riloff num_Jones_1999 conj_and_Riloff_Jones
C00-2130	P99-1048	o	Our system attempts to recognize these syntactic patterus in addition it considers as unfamiliar some definites occurring in 4This list was developed by hand more recently Bean and Riloff -LRB- 1999 -RRB- proposed methods for autolnatically extracting fl ` om a corpus such special predicates i.e. heads that correlate well with discourse novelty	nn_novelty_discourse prep_with_correlate_novelty advmod_correlate_well nsubj_correlate_that ccomp_heads_correlate advmod_heads_i.e. nsubj_heads_predicates dep_heads_corpus dep_heads_om dep_heads_fl amod_predicates_special amod_predicates_such det_corpus_a ccomp_extracting_heads advmod_extracting_autolnatically prepc_for_methods_extracting amod_methods_proposed nsubj_proposed_Riloff nsubj_proposed_Bean advmod_proposed_recently appos_Riloff_1999 conj_and_Bean_Riloff advmod_recently_more agent_developed_hand auxpass_developed_was nsubjpass_developed_definites advmod_developed_unfamiliar mark_developed_as nn_list_4This prep_in_occurring_list vmod_definites_occurring det_definites_some advcl_considers_developed nsubj_considers_it prep_in_considers_addition amod_patterus_syntactic det_patterus_these dobj_recognize_patterus aux_recognize_to parataxis_attempts_methods parataxis_attempts_considers xcomp_attempts_recognize nsubj_attempts_system poss_system_Our
D09-1101	P99-1048	o	Non-anaphoric definite descriptions have been detected using heuristics -LRB- e.g. Vieira and Poesio -LRB- 2000 -RRB- -RRB- and unsupervised methods -LRB- e.g. Bean and Riloff -LRB- 1999 -RRB- -RRB-	appos_Riloff_1999 conj_and_Bean_Riloff dep_e.g._Riloff dep_e.g._Bean ccomp_-LRB-_e.g. amod_methods_unsupervised dep_Poesio_2000 conj_and_Vieira_methods conj_and_Vieira_Poesio advcl_,_methods advcl_,_Poesio advcl_,_Vieira dep_-LRB-_e.g. dobj_using_heuristics xcomp_detected_using auxpass_detected_been aux_detected_have nsubjpass_detected_descriptions amod_descriptions_definite amod_descriptions_Non-anaphoric
D09-1102	P99-1048	o	More recently the problem has been tackled using statistics-based -LRB- e.g. Bean and Riloff 1999 Bergsma et al 2008 -RRB- and learning-based -LRB- e.g. Evans 2001 Ng and Cardie 2002a Ng 2004 Yang et al 2005 Denis and Balbridge 2007 -RRB- methods	nn_methods_e.g. amod_methods_learning-based num_Balbridge_2007 dep_Yang_2005 dep_Yang_al nn_Yang_et num_Ng_2004 nn_2002a_Cardie nn_2002a_Ng conj_and_Ng_Cardie conj_and_Evans_Balbridge conj_and_Evans_Denis conj_and_Evans_Yang conj_and_Evans_Ng conj_and_Evans_2002a num_Evans_2001 dep_e.g._Balbridge dep_e.g._Denis dep_e.g._Yang dep_e.g._Ng dep_e.g._2002a dep_e.g._Evans num_al_2008 nn_al_et nn_al_Bergsma num_Riloff_1999 conj_and_Bean_Riloff dep_e.g._al dep_e.g._Riloff dep_e.g._Bean conj_and_statistics-based_methods dep_statistics-based_e.g. dobj_using_methods dobj_using_statistics-based xcomp_tackled_using auxpass_tackled_been aux_tackled_has nsubjpass_tackled_problem advmod_tackled_recently det_problem_the advmod_recently_More
D09-1102	P99-1048	o	2 Related Work Given its potential usefulness in coreference resolution anaphoricity determination has been studied fairly extensively in the literature and can be classified into three categories heuristic rule-based -LRB- e.g. Paice and Husk 1987 Lappin and Leass 1994 Kennedy and Boguraev 1996 Denber 1998 Vieira and Poesio 2000 -RRB- statistics-based -LRB- e.g. Bean and Riloff 1999 Cherry and Bergsma 2005 Bergsma et al 2008 -RRB- and learning-based -LRB- e.g. Evans 2001 Ng and Cardie 2002a Ng 2004 Yang et al 2005 Denis and Balbridge 2007 -RRB-	num_Balbridge_2007 dep_Yang_2005 dep_Yang_al nn_Yang_et num_Ng_2004 nn_2002a_Cardie nn_2002a_Ng conj_and_Ng_Cardie conj_and_Evans_Balbridge conj_and_Evans_Denis conj_and_Evans_Yang conj_and_Evans_Ng conj_and_Evans_2002a num_Evans_2001 pobj_e.g._Balbridge pobj_e.g._Denis pobj_e.g._Yang pobj_e.g._Ng pobj_e.g._2002a pobj_e.g._Evans prep_-LRB-_e.g. num_al_2008 nn_al_et nn_al_Bergsma num_Bergsma_2005 conj_and_Cherry_Bergsma num_Riloff_1999 conj_and_Bean_Riloff dep_e.g._al dep_e.g._Bergsma dep_e.g._Cherry dep_e.g._Riloff dep_e.g._Bean dep_statistics-based_e.g. num_Poesio_2000 conj_and_Vieira_Poesio num_Denber_1998 num_Boguraev_1996 conj_and_Kennedy_Boguraev num_Leass_1994 conj_and_Lappin_Leass num_Husk_1987 dep_Paice_Poesio dep_Paice_Vieira dep_Paice_Denber dep_Paice_Boguraev dep_Paice_Kennedy dep_Paice_Leass dep_Paice_Lappin conj_and_Paice_Husk nn_Paice_e.g. dep_rule-based_Husk dep_rule-based_Paice conj_and_heuristic_learning-based conj_and_heuristic_statistics-based amod_heuristic_rule-based num_categories_three prep_into_classified_categories auxpass_classified_be aux_classified_can nsubjpass_classified_determination det_literature_the advmod_extensively_fairly dep_studied_learning-based dep_studied_statistics-based dep_studied_heuristic conj_and_studied_classified prep_in_studied_literature advmod_studied_extensively auxpass_studied_been aux_studied_has nsubjpass_studied_determination dep_studied_Work nn_determination_anaphoricity nn_resolution_coreference prep_in_usefulness_resolution amod_usefulness_potential poss_usefulness_its pobj_Given_usefulness prep_Work_Given amod_Work_Related num_Work_2 advcl_``_classified advcl_``_studied
D09-1102	P99-1048	o	For the statistics-based approaches Bean and Riloff -LRB- 1999 -RRB- developed a statistics-based method for automatically identifying existential definite NPs which are non-anaphoric	cop_non-anaphoric_are nsubj_non-anaphoric_which rcmod_NPs_non-anaphoric amod_NPs_definite amod_NPs_existential dobj_identifying_NPs advmod_identifying_automatically prepc_for_method_identifying amod_method_statistics-based det_method_a dobj_developed_method nsubj_developed_Riloff nsubj_developed_Bean prep_for_developed_approaches appos_Riloff_1999 conj_and_Bean_Riloff amod_approaches_statistics-based det_approaches_the
E09-3006	P99-1048	o	-LRB- 2005 -RRB- for English but not identical to strictly anaphoric ones5 -LRB- Bean and Riloff 1999 Uryupina 2003 -RRB- since a non-anaphoric NP can corefer with a previous mention	amod_mention_previous det_mention_a prep_with_corefer_mention aux_corefer_can nsubj_corefer_NP mark_corefer_since amod_NP_non-anaphoric det_NP_a dep_Uryupina_2003 dep_Bean_Uryupina conj_and_Bean_1999 conj_and_Bean_Riloff appos_ones5_1999 appos_ones5_Riloff appos_ones5_Bean amod_ones5_anaphoric advmod_anaphoric_strictly prep_to_identical_ones5 neg_identical_not advcl_2005_corefer conj_but_2005_identical prep_for_2005_English dep_''_identical dep_''_2005
E09-3006	P99-1048	o	Bean and Riloff -LRB- 1999 -RRB- and Uryupina -LRB- 2003 -RRB- have already employed a definite probability measure in a similar way although the way the ratio is computed is slightly different	advmod_different_slightly cop_different_is nsubj_different_way mark_different_although auxpass_computed_is nsubjpass_computed_ratio det_ratio_the rcmod_way_computed det_way_the amod_way_similar det_way_a nn_measure_probability amod_measure_definite det_measure_a advcl_employed_different prep_in_employed_way dobj_employed_measure advmod_employed_already aux_employed_have nsubj_employed_Uryupina nsubj_employed_Riloff nsubj_employed_Bean appos_Uryupina_2003 appos_Riloff_1999 conj_and_Bean_Uryupina conj_and_Bean_Riloff
E09-3006	P99-1048	o	In contrast the latter computes four definite probabilities which are included as features within a machine-learning classifier from the Web in an attempt to overcome Bean and Riloffs -LRB- 1999 -RRB- data sparseness problem	nn_problem_sparseness nn_problem_data nn_problem_Riloffs nn_problem_Bean appos_Riloffs_1999 conj_and_Bean_Riloffs dobj_overcome_problem aux_overcome_to vmod_attempt_overcome det_attempt_an det_Web_the prep_from_classifier_Web amod_classifier_machine-learning det_classifier_a prep_in_features_attempt prep_within_features_classifier prep_as_included_features auxpass_included_are nsubjpass_included_which rcmod_probabilities_included amod_probabilities_definite num_probabilities_four dep_computes_probabilities amod_computes_latter det_computes_the ccomp_,_computes pobj_In_contrast dep_``_In
E09-3006	P99-1048	o	A more fine-grained distinction is made by Bean and Riloff -LRB- 1999 -RRB- and Vieira and Poesio -LRB- 2000 -RRB- to distinguish restrictive from non-restrictive postmodification by ommitting those modifiers that occur between commas which should not be classified as chain starting	vmod_chain_starting prep_as_classified_chain auxpass_classified_be neg_classified_not aux_classified_should nsubjpass_classified_which rcmod_commas_classified prep_between_occur_commas nsubj_occur_that rcmod_modifiers_occur det_modifiers_those dobj_ommitting_modifiers amod_postmodification_non-restrictive prep_from_restrictive_postmodification prepc_by_distinguish_ommitting acomp_distinguish_restrictive aux_distinguish_to appos_Poesio_2000 appos_Riloff_1999 conj_and_Bean_Poesio conj_and_Bean_Vieira conj_and_Bean_Riloff xcomp_made_distinguish agent_made_Poesio agent_made_Vieira agent_made_Riloff agent_made_Bean auxpass_made_is nsubjpass_made_distinction amod_distinction_fine-grained det_distinction_A advmod_fine-grained_more
E09-3006	P99-1048	o	We borrow the idea of classifying definites occurring in the first sentence as chain starting from Bean and Riloff -LRB- 1999 -RRB-	appos_Riloff_1999 conj_and_Bean_Riloff prep_from_starting_Riloff prep_from_starting_Bean vmod_chain_starting prep_as_sentence_chain amod_sentence_first det_sentence_the prep_in_occurring_sentence vmod_definites_occurring amod_definites_classifying prep_of_idea_definites det_idea_the dobj_borrow_idea nsubj_borrow_We
J00-4003	P99-1048	p	18 More recently Bean and Riloff -LRB- 1999 -RRB- have proposed methods for automatically extracting from a corpus heads that correlate well with discourse novelty	nn_novelty_discourse prep_with_correlate_novelty advmod_correlate_well nsubj_correlate_that ccomp_heads_correlate nsubj_heads_methods det_corpus_a prep_from_extracting_corpus advmod_extracting_automatically prepc_for_methods_extracting amod_methods_proposed num_methods_18 aux_proposed_have nsubj_proposed_Riloff nsubj_proposed_Bean advmod_proposed_recently appos_Riloff_1999 conj_and_Bean_Riloff advmod_recently_More
J00-4003	P99-1048	o	32 This problem is also a central concern in the work by Bean and Riloff -LRB- 1999 -RRB-	appos_Riloff_1999 conj_and_Bean_Riloff prep_by_work_Riloff prep_by_work_Bean det_work_the prep_in_concern_work amod_concern_central det_concern_a advmod_concern_also cop_concern_is nsubj_concern_problem det_problem_This num_problem_32
N04-1038	P99-1048	o	First a non-anaphoric NP classifier identifies definite noun phrases that are existential using both syntactic rules and our learned existential NP recognizer -LRB- Bean and Riloff 1999 -RRB- and removes them from the resolution process	nn_process_resolution det_process_the prep_from_removes_process dobj_removes_them nsubj_removes_classifier dep_Bean_1999 conj_and_Bean_Riloff appos_recognizer_Riloff appos_recognizer_Bean nn_recognizer_NP amod_recognizer_existential amod_recognizer_learned poss_recognizer_our conj_and_rules_recognizer amod_rules_syntactic preconj_rules_both dobj_using_recognizer dobj_using_rules xcomp_existential_using cop_existential_are nsubj_existential_that rcmod_phrases_existential nn_phrases_noun amod_phrases_definite conj_and_identifies_removes dobj_identifies_phrases nsubj_identifies_classifier advmod_identifies_First nn_classifier_NP amod_classifier_non-anaphoric det_classifier_a
N04-1038	P99-1048	o	In previous work -LRB- Bean and Riloff 1999 -RRB- we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood	advmod_understood_universally auxpass_understood_is nsubjpass_understood_meaning mark_understood_because poss_meaning_their amod_modification_syntactic advcl_existential_understood prep_without_existential_modification cop_existential_are nsubj_existential_that rcmod_NPs_existential amod_NPs_definite dobj_recognizes_NPs advmod_recognizes_automatically nsubj_recognizes_that rcmod_algorithm_recognizes nn_algorithm_learning amod_algorithm_unsupervised det_algorithm_an dobj_developed_algorithm nsubj_developed_we prep_in_developed_work amod_Bean_1999 conj_and_Bean_Riloff dep_work_Riloff dep_work_Bean amod_work_previous
N04-1038	P99-1048	o	Using this heuristic BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm -LRB- Bean and Riloff 1999 -RRB- and resolves all occurrences of the same existential NP with each another	det_another_each prep_with_NP_another amod_NP_existential amod_NP_same det_NP_the prep_of_occurrences_NP det_occurrences_all dobj_resolves_occurrences nsubj_resolves_BABAR amod_Bean_1999 conj_and_Bean_Riloff dep_algorithm_Riloff dep_algorithm_Bean nn_algorithm_learning amod_algorithm_previous poss_algorithm_our dobj_using_algorithm nn_corpus_training det_corpus_the vmod_NPs_using prep_in_NPs_corpus amod_NPs_definite amod_NPs_existential conj_and_identifies_resolves dobj_identifies_NPs nsubj_identifies_BABAR vmod_identifies_Using det_heuristic_this dobj_Using_heuristic
N07-1010	P99-1048	o	Bean and Riloff -LRB- 1999 -RRB- extracts rules from non-anaphoric noun phrases and noun phrases patterns which are then applied to test data to identify existential noun phrases	nn_phrases_noun amod_phrases_existential dobj_identify_phrases aux_identify_to vmod_test_identify dobj_test_data aux_test_to xcomp_applied_test advmod_applied_then auxpass_applied_are nsubjpass_applied_which nn_patterns_phrases nn_phrases_noun nn_phrases_noun amod_phrases_non-anaphoric nn_rules_extracts nn_rules_Riloff appos_Riloff_1999 rcmod_Bean_applied conj_and_Bean_patterns prep_from_Bean_phrases conj_and_Bean_rules
N09-1065	P99-1048	o	-LRB- 2008 -RRB- -RSB- -RRB- and others identifying non-anaphoric definite descriptions -LRB- using rule-based techniques -LSB- e.g. Vieira and Poesio -LRB- 2000 -RRB- -RSB- and unsupervised techniques -LSB- e.g. Bean and Riloff -LRB- 1999 -RRB- -RSB- -RRB-	dep_Riloff_1999 conj_and_Bean_Riloff dep_e.g._Riloff dep_e.g._Bean dep_techniques_e.g. amod_techniques_unsupervised dep_Poesio_2000 conj_and_Vieira_Poesio pobj_e.g._Poesio pobj_e.g._Vieira amod_techniques_rule-based conj_and_using_techniques dep_using_e.g. dobj_using_techniques amod_descriptions_definite amod_descriptions_non-anaphoric dobj_identifying_descriptions vmod_others_identifying dep_2008_techniques dep_2008_using conj_and_2008_others dep_''_others dep_''_2008
P03-1023	P99-1048	o	In our future work we intend to adopt a looser filter together with an anaphoricity determination module -LRB- Bean and Riloff 1999 Ng and Cardie 2002b -RRB-	appos_Bean_2002b conj_and_Bean_Cardie conj_and_Bean_Ng conj_and_Bean_1999 conj_and_Bean_Riloff dep_module_Cardie dep_module_Ng dep_module_1999 dep_module_Riloff dep_module_Bean nn_module_determination nn_module_anaphoricity det_module_an amod_filter_looser det_filter_a prep_together_with_adopt_module dobj_adopt_filter aux_adopt_to xcomp_intend_adopt nsubj_intend_we prep_in_intend_work amod_work_future poss_work_our
P03-2012	P99-1048	o	The system described in -LRB- Bean and Riloff 1999 -RRB- also makes use of syntactic heuristics	amod_heuristics_syntactic prep_of_use_heuristics dobj_makes_use advmod_makes_also nsubj_makes_system amod_Bean_1999 conj_and_Bean_Riloff prep_in_described_Riloff prep_in_described_Bean vmod_system_described det_system_The
P03-2013	P99-1048	o	The task of classifying several different uses of definite descriptions -LRB- Vieira and Poesio 2000 Bean and Riloff 1999 -RRB- is somewhat analogous to that for bare nouns	amod_nouns_bare prep_for_that_nouns prep_to_analogous_that advmod_analogous_somewhat cop_analogous_is nsubj_analogous_task dep_Bean_1999 conj_and_Bean_Riloff dep_Vieira_Riloff dep_Vieira_Bean conj_and_Vieira_2000 conj_and_Vieira_Poesio dep_descriptions_2000 dep_descriptions_Poesio dep_descriptions_Vieira amod_descriptions_definite prep_of_uses_descriptions amod_uses_different amod_uses_several dobj_classifying_uses prepc_of_task_classifying det_task_The
P04-1020	P99-1048	o	More recently the problem has been tackled using unsupervised -LRB- e.g. Bean and Riloff -LRB- 1999 -RRB- -RRB- and supervised -LRB- e.g. Evans -LRB- 2001 -RRB- Ng and Cardie -LRB- 2002a -RRB- -RRB- approaches	appos_approaches_Evans nn_approaches_e.g. amod_approaches_supervised appos_Ng_2002a conj_and_Ng_Cardie dep_Evans_Cardie dep_Evans_Ng appos_Evans_2001 appos_Riloff_1999 conj_and_Bean_Riloff dep_e.g._Riloff dep_e.g._Bean conj_and_unsupervised_approaches dep_unsupervised_e.g. dobj_using_approaches dobj_using_unsupervised xcomp_tackled_using auxpass_tackled_been aux_tackled_has nsubjpass_tackled_problem advmod_tackled_recently det_problem_the advmod_recently_More
P08-2011	P99-1048	p	3Bean and Riloff -LRB- 1999 -RRB- and Uryupina -LRB- 2003 -RRB- construct quite accurate classifiers to detect unique NPs	amod_NPs_unique dobj_detect_NPs aux_detect_to vmod_classifiers_detect amod_classifiers_accurate dep_classifiers_Uryupina dep_classifiers_Riloff dep_classifiers_3Bean advmod_accurate_quite appos_Uryupina_2003 appos_Riloff_1999 dep_3Bean_construct conj_and_3Bean_Uryupina conj_and_3Bean_Riloff
W04-0706	P99-1048	o	As resolving direct anaphoric descriptions -LRB- the ones where anaphor and antecedent have the same head noun -RRB- is a much simpler problem with high performance rates as shown in previous results -LRB- Vieira et al. 2000 Bean and Riloff 1999 -RRB- these heuristics should be applied first in a system that resolves definite descriptions	amod_descriptions_definite dobj_resolves_descriptions nsubj_resolves_that rcmod_system_resolves det_system_a prep_in_applied_system advmod_applied_first auxpass_applied_be aux_applied_should nsubjpass_applied_heuristics advcl_applied_is det_heuristics_these amod_Bean_1999 conj_and_Bean_Riloff dep_Vieira_Riloff dep_Vieira_Bean appos_Vieira_2000 dep_Vieira_al. nn_Vieira_et amod_results_previous prep_in_shown_results mark_shown_as nn_rates_performance amod_rates_high dep_problem_Vieira dep_problem_shown prep_with_problem_rates amod_problem_much det_problem_a dep_much_simpler nsubj_is_problem prepc_as_is_resolving nn_noun_head amod_noun_same det_noun_the dobj_have_noun nsubj_have_antecedent nsubj_have_anaphor advmod_have_where conj_and_anaphor_antecedent rcmod_ones_have det_ones_the dep_descriptions_ones amod_descriptions_anaphoric amod_descriptions_direct dobj_resolving_descriptions
W04-0707	P99-1048	o	Version of the System P R F Baseline 50.8 100 67.4 Discourse-new detection only 69 72 70 Hand-coded DT partial 62 85 71.7 Hand-coded DT total 77 77 77 ID3 75 75 75 Table 1 Overall results by Vieira and Poesio 2.2 Bean and Riloff Bean and Riloff -LRB- 1999 -RRB- developed a system for identifying discourse-new DDs1 that incorporates in addition to syntax-based heuristics aimed at recognizing predicative and established DDs using postmodification heuristics similar to those used by Vieira and Poesio additional techniques for mining from corpora unfamiliar DDs including proper names larger situation and semantically functional	advmod_functional_semantically amod_situation_larger conj_and_names_functional conj_and_names_situation amod_names_proper prep_including_DDs_functional prep_including_DDs_situation prep_including_DDs_names amod_DDs_unfamiliar nn_DDs_corpora prep_from_mining_DDs prep_for_techniques_mining amod_techniques_additional conj_and_Vieira_Poesio agent_used_Poesio agent_used_Vieira vmod_those_used prep_to_similar_those amod_heuristics_similar nn_heuristics_postmodification dobj_using_heuristics vmod_DDs_using amod_DDs_established amod_DDs_predicative conj_and_predicative_established dobj_recognizing_DDs prepc_at_aimed_recognizing vmod_heuristics_aimed amod_heuristics_syntax-based prep_in_addition_to_incorporates_heuristics nsubj_incorporates_that rcmod_DDs1_incorporates nn_DDs1_discourse-new dobj_identifying_DDs1 prepc_for_system_identifying det_system_a dep_developed_techniques dobj_developed_system nsubj_developed_results appos_Riloff_1999 nn_Bean_Riloff num_Bean_2.2 nn_Bean_Poesio conj_and_Vieira_Riloff conj_and_Vieira_Bean conj_and_Vieira_Bean prep_by_results_Riloff prep_by_results_Bean prep_by_results_Bean prep_by_results_Vieira amod_results_Overall num_Table_1 num_Table_75 dep_75_Table dep_75_75 dep_ID3_75 dep_77_ID3 dep_77_77 number_77_77 dep_total_77 dep_DT_developed dep_DT_total amod_DT_Hand-coded num_DT_71.7 amod_DT_partial dep_71.7_85 number_85_62 amod_DT_Hand-coded num_DT_70 num_70_72 number_72_69 quantmod_72_only dep_detection_DT amod_detection_Discourse-new num_detection_67.4 number_67.4_100 number_67.4_50.8 dep_Baseline_detection nn_Baseline_F nn_Baseline_R nn_Baseline_P nn_Baseline_System det_Baseline_the dep_Version_DT prep_of_Version_Baseline
C02-1085	W00-0405	o	More recently other approaches have investigated the use of machine learning to nd patterns in documents -LRB- Strzalkowski et al. 1998 -RRB- and the utility of parameterized modules so as to deal with dierent genres or corpora -LRB- Goldstein et al. 2000 -RRB-	amod_Goldstein_2000 dep_Goldstein_al. nn_Goldstein_et conj_or_genres_corpora amod_genres_dierent dep_deal_Goldstein prep_with_deal_corpora prep_with_deal_genres aux_deal_to amod_modules_parameterized prep_of_utility_modules det_utility_the amod_Strzalkowski_1998 dep_Strzalkowski_al. nn_Strzalkowski_et prep_in_patterns_documents nn_patterns_nd prep_to_learning_patterns vmod_machine_learning conj_and_use_utility dep_use_Strzalkowski prep_of_use_machine det_use_the prepc_as_investigated_deal advmod_investigated_so dobj_investigated_utility dobj_investigated_use aux_investigated_have nsubj_investigated_approaches advmod_investigated_recently amod_approaches_other advmod_recently_More
E09-1059	W00-0405	o	Alternatively we could have simply incorporated the DIVERSITY measure into the objective function or used an inference algorithm that specifically accounts for redundancy e.g. maximal marginal relevance -LRB- Goldstein et al. 2000 -RRB-	amod_Goldstein_2000 dep_Goldstein_al. nn_Goldstein_et dep_relevance_Goldstein amod_relevance_marginal amod_relevance_maximal advmod_relevance_e.g. conj_redundancy_relevance prep_for_accounts_redundancy advmod_accounts_specifically nsubj_accounts_that rcmod_algorithm_accounts nn_algorithm_inference det_algorithm_an dobj_used_algorithm conj_or_function_used amod_function_objective det_function_the nn_measure_DIVERSITY det_measure_the prep_into_incorporated_used prep_into_incorporated_function dobj_incorporated_measure advmod_incorporated_simply aux_incorporated_have aux_incorporated_could nsubj_incorporated_we advmod_incorporated_Alternatively ccomp_``_incorporated
E09-1059	W00-0405	o	The diversity function rewards summaries that cover many important aspects and plays the redundancy reducing role that is common in most extractive summarization frameworks -LRB- Goldstein et al. 2000 -RRB-	num_Goldstein_2000 nn_Goldstein_al. nn_Goldstein_et nn_frameworks_summarization amod_frameworks_extractive amod_frameworks_most prep_in_common_frameworks cop_common_is nsubj_common_that rcmod_role_common dobj_reducing_role vmod_redundancy_reducing det_redundancy_the dobj_plays_redundancy nsubj_plays_that amod_aspects_important amod_aspects_many conj_and_cover_plays dobj_cover_aspects nsubj_cover_that appos_summaries_Goldstein rcmod_summaries_plays rcmod_summaries_cover nn_summaries_rewards nn_summaries_function nn_summaries_diversity det_summaries_The ccomp_``_summaries
E09-1059	W00-0405	o	This is in contrast to standard summarization models that look to promote sentence diversity in order to cover as many important topics as possible -LRB- Goldstein et al. 2000 -RRB-	amod_Goldstein_2000 dep_Goldstein_al. nn_Goldstein_et dep_possible_Goldstein prep_as_topics_possible amod_topics_important amod_topics_many advmod_topics_as dobj_cover_topics aux_cover_to dep_cover_order mark_cover_in nn_diversity_sentence advcl_promote_cover dobj_promote_diversity aux_promote_to xcomp_look_promote nsubj_look_that rcmod_models_look nn_models_summarization amod_models_standard prep_to_contrast_models prep_in_is_contrast nsubj_is_This
H05-1090	W00-0405	p	In 2004 Conroy -LRB- Conroy 2004 -RRB- tested Maximal Marginal Relevance -LRB- Goldstein et al. 2000 -RRB- as well as QR decomposition	nn_decomposition_QR amod_Goldstein_2000 dep_Goldstein_al. nn_Goldstein_et conj_and_Relevance_decomposition dep_Relevance_Goldstein amod_Relevance_Marginal amod_Relevance_Maximal amod_Relevance_tested dep_Relevance_Conroy prep_in_Relevance_2004 dep_Conroy_2004 dep_Conroy_Conroy
N04-4001	W00-0405	o	While close attention has been paid to multi-document summarization technologies -LRB- Barzilay et al. 2002 Goldstein et al 2000 -RRB- the inherent properties of humanwritten multi-document summaries have not yet been quantified	auxpass_quantified_been advmod_quantified_yet neg_quantified_not aux_quantified_have nsubjpass_quantified_properties advcl_quantified_paid amod_summaries_multi-document amod_summaries_humanwritten prep_of_properties_summaries amod_properties_inherent det_properties_the nn_2000_al advmod_Goldstein_2000 nn_Goldstein_et dep_Barzilay_Goldstein dep_Barzilay_2002 dep_Barzilay_al. nn_Barzilay_et nn_technologies_summarization amod_technologies_multi-document dep_paid_Barzilay prep_to_paid_technologies auxpass_paid_been aux_paid_has nsubjpass_paid_attention mark_paid_While amod_attention_close
N07-1013	W00-0405	o	For example extractive text summarization generates a summary by selecting a few good sentences from one or more articles on the same topic -LRB- Goldstein et al. 2000 -RRB-	amod_Goldstein_2000 dep_Goldstein_al. nn_Goldstein_et amod_topic_same det_topic_the prep_on_articles_topic num_articles_more num_articles_one conj_or_one_more prep_from_sentences_articles amod_sentences_good amod_sentences_few det_sentences_a dobj_selecting_sentences det_summary_a dep_generates_Goldstein prepc_by_generates_selecting dobj_generates_summary nsubj_generates_summarization prep_for_generates_example nn_summarization_text amod_summarization_extractive
N07-1013	W00-0405	o	Perhaps the most well-known method is maximum marginal relevance -LRB- MMR -RRB- -LRB- Carbonell and Goldstein 1998 -RRB- as well as cross-sentence informational subsumption -LRB- Radev 2000 -RRB- mixture models -LRB- Zhang et al. 2002 -RRB- subtopic diversity -LRB- Zhai et al. 2003 -RRB- diversity penalty -LRB- Zhang et al. 2005 -RRB- and others	amod_Zhang_2005 dep_Zhang_al. nn_Zhang_et appos_penalty_Zhang nn_penalty_diversity amod_Zhai_2003 dep_Zhai_al. nn_Zhai_et dep_diversity_Zhai amod_diversity_subtopic amod_Zhang_2002 dep_Zhang_al. nn_Zhang_et appos_models_Zhang nn_models_mixture dep_Radev_2000 appos_subsumption_Radev amod_subsumption_informational nn_subsumption_cross-sentence dep_Carbonell_1998 conj_and_Carbonell_Goldstein conj_and_relevance_others conj_and_relevance_penalty conj_and_relevance_diversity conj_and_relevance_models conj_and_relevance_subsumption dep_relevance_Goldstein dep_relevance_Carbonell appos_relevance_MMR amod_relevance_marginal amod_relevance_maximum cop_relevance_is nsubj_relevance_method advmod_relevance_Perhaps amod_method_well-known det_method_the advmod_well-known_most
P09-1024	W00-0405	o	This strategy is commonly used in multi-document summarization -LRB- Barzilay et al. 1999 Goldstein et al. 2000 Radev et al. 2000 -RRB- where the combination step eliminates the redundancy across selected excerpts	amod_excerpts_selected det_redundancy_the prep_across_eliminates_excerpts dobj_eliminates_redundancy nsubj_eliminates_step advmod_eliminates_where nn_step_combination det_step_the num_Radev_2000 nn_Radev_al. nn_Radev_et num_Goldstein_2000 nn_Goldstein_al. nn_Goldstein_et dep_Barzilay_Radev dep_Barzilay_Goldstein dep_Barzilay_1999 dep_Barzilay_al. nn_Barzilay_et amod_summarization_multi-document advcl_used_eliminates dep_used_Barzilay prep_in_used_summarization advmod_used_commonly auxpass_used_is nsubjpass_used_strategy det_strategy_This
W02-0402	W00-0405	o	The marginal relevance systems -LRB- MR and MR+IE -RRB- used a simple selection mechanism which does not involve search inspired by the maximal marginal relevance -LRB- MMR -RRB- approach -LRB- Goldstein et al. 2000 -RRB-	amod_Goldstein_2000 dep_Goldstein_al. nn_Goldstein_et nn_approach_relevance appos_relevance_MMR amod_relevance_marginal amod_relevance_maximal det_relevance_the agent_inspired_approach dobj_involve_search neg_involve_not aux_involve_does nsubj_involve_which rcmod_mechanism_involve nn_mechanism_selection amod_mechanism_simple det_mechanism_a dep_used_Goldstein vmod_used_inspired dobj_used_mechanism nsubj_used_systems conj_and_MR_MR+IE dep_systems_MR+IE dep_systems_MR nn_systems_relevance amod_systems_marginal det_systems_The
W09-1802	W00-0405	o	both relevant and non-redundant -LRB- Goldstein et al. 2000 Nenkova and Vanderwende 2005 -RRB- some recent work focuses on improved search -LRB- McDonald 2007 Yih et al. 2007 -RRB-	num_Yih_2007 nn_Yih_al. nn_Yih_et dep_McDonald_Yih appos_McDonald_2007 appos_search_McDonald amod_search_improved prep_on_focuses_search nsubj_focuses_work amod_work_recent det_work_some dep_Nenkova_2005 conj_and_Nenkova_Vanderwende rcmod_Goldstein_focuses conj_Goldstein_Vanderwende conj_Goldstein_Nenkova conj_Goldstein_2000 dep_Goldstein_al. nn_Goldstein_et dep_relevant_Goldstein conj_and_relevant_non-redundant preconj_relevant_both ccomp_``_non-redundant ccomp_``_relevant
W09-1802	W00-0405	n	Methods like McDonalds including the wellknown Maximal Marginal Relevance -LRB- MMR -RRB- algorithm -LRB- Goldstein et al. 2000 -RRB- are subject to another problem Summary-level redundancy is not always well modeled by pairwise sentence-level redundancy	amod_redundancy_sentence-level amod_redundancy_pairwise agent_modeled_redundancy advmod_modeled_well advmod_modeled_always neg_modeled_not auxpass_modeled_is nsubjpass_modeled_redundancy amod_redundancy_Summary-level det_problem_another parataxis_subject_modeled prep_to_subject_problem cop_subject_are nsubj_subject_Methods amod_Goldstein_2000 dep_Goldstein_al. nn_Goldstein_et nn_algorithm_Relevance appos_Relevance_MMR amod_Relevance_Marginal amod_Relevance_Maximal amod_Relevance_wellknown det_Relevance_the prep_including_McDonalds_algorithm dep_Methods_Goldstein prep_like_Methods_McDonalds
C04-1080	W02-1001	o	The algorithms were trained and tested using version 3 of the Penn Treebank using the training development and test split described in Collins -LRB- 2002 -RRB- and also employed by Toutanova et al.	nn_al._et nn_al._Toutanova prep_by_employed_al. advmod_employed_also conj_and_Collins_employed appos_Collins_2002 prep_in_described_employed prep_in_described_Collins vmod_split_described nn_split_test conj_and_training_split conj_and_training_development det_training_the dobj_using_split dobj_using_development dobj_using_training nn_Treebank_Penn det_Treebank_the prep_of_version_Treebank num_version_3 vmod_using_using dobj_using_version nsubjpass_tested_algorithms xcomp_trained_using conj_and_trained_tested auxpass_trained_were nsubjpass_trained_algorithms det_algorithms_The
C08-1094	W02-1001	o	We trained log linear models with theperceptronalgorithm -LRB- Collins ,2002 -RRB- usingfea746 Markov order Classification Task 0 1 2 S1 -LRB- no multi-word constituent start -RRB- 96.7 96.9 96.9 E1 -LRB- no multi-word constituent end -RRB- 97.3 97.3 97.3 Table 2 Classification accuracy on development set for binary classes S1 and E1 for various Markov orders	nn_orders_Markov amod_orders_various conj_and_S1_E1 dep_classes_E1 dep_classes_S1 amod_classes_binary prep_for_set_classes nn_set_development prep_for_accuracy_orders prep_on_accuracy_set nn_accuracy_Classification num_Table_2 num_Table_97.3 dep_97.3_Table dep_97.3_97.3 nn_end_constituent amod_end_multi-word neg_end_no dep_E1_97.3 appos_E1_end num_E1_96.9 number_96.9_96.9 number_96.9_96.7 dep_start_E1 nn_start_constituent amod_start_multi-word neg_start_no dep_S1_start num_S1_2 number_2_1 number_2_0 dep_Task_S1 nn_Task_Classification nn_Task_order nn_Task_Markov nn_Task_usingfea746 nn_Task_theperceptronalgorithm num_Collins_,2002 appos_theperceptronalgorithm_Collins amod_models_linear nn_models_log dep_trained_accuracy prep_with_trained_Task dobj_trained_models nsubj_trained_We ccomp_``_trained
C08-2016	W02-1001	o	When we have a junction tree for each document we can efficiently perform belief propagation in order to compute argmax in Equation -LRB- 1 -RRB- or the marginal probabilities of cliques and labels necessary for the parameter estimation of machine learning classifiers including perceptrons -LRB- Collins 2002 -RRB- and maximum entropy models -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_models_entropy nn_models_maximum amod_Collins_2002 appos_perceptrons_Collins prep_including_classifiers_perceptrons nn_classifiers_learning nn_classifiers_machine prep_of_estimation_classifiers nn_estimation_parameter det_estimation_the prep_for_necessary_estimation conj_and_cliques_labels prep_of_probabilities_labels prep_of_probabilities_cliques amod_probabilities_marginal det_probabilities_the conj_or_Equation_probabilities appos_Equation_1 prep_in_argmax_probabilities prep_in_argmax_Equation dep_compute_Berger conj_and_compute_models conj_and_compute_necessary dobj_compute_argmax aux_compute_to dep_compute_order mark_compute_in nn_propagation_belief advcl_perform_models advcl_perform_necessary advcl_perform_compute dobj_perform_propagation advmod_perform_efficiently aux_perform_can nsubj_perform_we advcl_perform_have det_document_each prep_for_tree_document nn_tree_junction det_tree_a dobj_have_tree nsubj_have_we advmod_have_When
C08-2016	W02-1001	o	For BPM we run 100 averaged perceptrons -LRB- Collins 2002 -RRB- with 10 iterations for each	prep_for_iterations_each num_iterations_10 dep_Collins_2002 appos_perceptrons_Collins prep_with_averaged_iterations dobj_averaged_perceptrons nsubj_averaged_100 ccomp_run_averaged nsubj_run_we prep_for_run_BPM
D07-1009	W02-1001	o	The model weights are trained using the standard ranking perceptron -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_perceptron_Collins amod_perceptron_ranking amod_perceptron_standard det_perceptron_the dobj_using_perceptron xcomp_trained_using auxpass_trained_are nsubjpass_trained_weights nn_weights_model det_weights_The ccomp_``_trained
D07-1009	W02-1001	o	Our hierarchical training method yields significant improvement when compared to a similar nonhierarchical model which instead uses the standard 2Data and code used in this paper are available at http://people.csail.mit.edu/edc/emnlp07/ perceptron update of Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_of_update_Collins nsubj_update_yields nn_perceptron_http://people.csail.mit.edu/edc/emnlp07/ prep_at_available_perceptron cop_available_are nsubj_available_improvement det_paper_this prep_in_used_paper vmod_2Data_used conj_and_2Data_code amod_2Data_standard det_2Data_the dobj_uses_code dobj_uses_2Data advmod_uses_instead nsubj_uses_which rcmod_model_uses amod_model_nonhierarchical amod_model_similar det_model_a prep_to_compared_model advmod_compared_when rcmod_improvement_compared amod_improvement_significant rcmod_yields_available nn_yields_method nn_yields_training amod_yields_hierarchical poss_yields_Our
D07-1009	W02-1001	o	In fact when the perceptron update rule of -LRB- Dekel et al. 2004 -RRB- which modifies the weights of every divergent node along the predicted and true paths is used in the ranking framework it becomes virtually identical with the standard flat ranking perceptron of Collins -LRB- 2002 -RRB- .5 In contrast our approach shares the idea of -LRB- Cesa-Bianchi et al. 2006a -RRB- that if a parent class has been predicted wrongly then errors in the children should not be taken into account We also view this as one of the key ideas of the incremental perceptron algorithm of -LRB- Collins and Roark 2004 -RRB- which searches through a complex decision space step-by-step and is immediately updated at the first wrong move	amod_move_wrong amod_move_first det_move_the prep_at_updated_move advmod_updated_immediately auxpass_updated_is amod_space_step-by-step conj_and_decision_updated dep_decision_space amod_decision_complex det_decision_a prep_through_searches_updated prep_through_searches_decision nsubj_searches_which rcmod_Collins_searches dep_Collins_2004 conj_and_Collins_Roark prep_of_algorithm_Roark prep_of_algorithm_Collins nn_algorithm_perceptron amod_algorithm_incremental det_algorithm_the prep_of_ideas_algorithm amod_ideas_key det_ideas_the prep_of_one_ideas prep_as_view_one dobj_view_this advmod_view_also nsubj_view_We parataxis_taken_view prep_into_taken_account auxpass_taken_be neg_taken_not aux_taken_should nsubjpass_taken_errors advcl_taken_predicted mark_taken_that det_children_the prep_in_errors_children advmod_errors_then advmod_predicted_wrongly auxpass_predicted_been aux_predicted_has nsubjpass_predicted_class mark_predicted_if nn_class_parent det_class_a appos_Cesa-Bianchi_2006a dep_Cesa-Bianchi_al. nn_Cesa-Bianchi_et ccomp_idea_taken prep_of_idea_Cesa-Bianchi det_idea_the dep_shares_idea nn_shares_approach poss_shares_our ccomp_,_shares nn_.5_Collins appos_Collins_2002 prep_in_perceptron_contrast prep_of_perceptron_.5 amod_perceptron_ranking appos_standard_perceptron amod_standard_flat det_standard_the prep_with_identical_standard advmod_identical_virtually acomp_becomes_identical nsubj_becomes_it advcl_becomes_update prep_in_becomes_fact amod_framework_ranking det_framework_the prep_in_used_framework auxpass_used_is nsubjpass_used_paths mark_used_along amod_paths_true amod_paths_predicted det_paths_the conj_and_predicted_true amod_node_divergent det_node_every prep_of_weights_node det_weights_the advcl_modifies_used dobj_modifies_weights nsubj_modifies_which rcmod_Dekel_modifies amod_Dekel_2004 dep_Dekel_al. nn_Dekel_et prep_of_rule_Dekel dobj_update_rule nsubj_update_perceptron advmod_update_when det_perceptron_the
D07-1015	W02-1001	o	-LRB- 2005a -RRB- .5 6.2 Results We performed experiments using three training algorithms the averaged perceptron -LRB- Collins 2002 -RRB- log-linear training -LRB- via conjugate gradient descent -RRB- and max-margin training -LRB- via the EG algorithm -RRB-	nn_algorithm_EG det_algorithm_the prep_via_training_algorithm conj_and_training_max-margin dep_training_via nn_descent_gradient nn_descent_conjugate pobj_via_descent dep_training_training dep_training_max-margin amod_training_log-linear amod_Collins_2002 appos_perceptron_training appos_perceptron_Collins dobj_averaged_perceptron vmod_the_averaged nn_algorithms_training num_algorithms_three dobj_using_algorithms xcomp_performed_using dobj_performed_experiments nsubj_performed_We dep_Results_the rcmod_Results_performed num_Results_6.2 num_Results_.5 dep_Results_2005a
D07-1025	W02-1001	o	We describe a new sequence alignment model based on the averaged perceptron -LRB- Collins 2002 -RRB- which shares with the above approaches the ability to exploit arbitrary features of the input sequences but is distinguished from them by its relative simplicity and the incremental character of its training procedure	nn_procedure_training poss_procedure_its prep_of_character_procedure amod_character_incremental det_character_the amod_simplicity_relative poss_simplicity_its agent_distinguished_simplicity prep_from_distinguished_them auxpass_distinguished_is nn_sequences_input det_sequences_the prep_of_features_sequences amod_features_arbitrary dobj_exploit_features aux_exploit_to conj_and_ability_character conj_but_ability_distinguished vmod_ability_exploit det_ability_the dep_approaches_character dep_approaches_distinguished dep_approaches_ability amod_approaches_above det_approaches_the prep_with_shares_approaches nsubj_shares_which dep_Collins_2002 rcmod_perceptron_shares appos_perceptron_Collins amod_perceptron_averaged det_perceptron_the prep_on_based_perceptron vmod_model_based nn_model_alignment nn_model_sequence amod_model_new det_model_a dobj_describe_model nsubj_describe_We
D07-1025	W02-1001	o	2.2 A Perceptron-Based Edit Model In this section we present a general-purpose extension of perceptron training for sequence labeling due to Collins -LRB- 2002 -RRB- to the problem of sequence alignment	nn_alignment_sequence prep_of_problem_alignment det_problem_the appos_Collins_2002 nn_labeling_sequence nn_training_perceptron prep_for_extension_labeling prep_of_extension_training amod_extension_general-purpose det_extension_a prep_to_present_problem prep_due_to_present_Collins dobj_present_extension nsubj_present_we prep_in_present_section det_section_this rcmod_Model_present nn_Model_Edit amod_Model_Perceptron-Based det_Model_A num_Model_2.2 dep_``_Model
D07-1033	W02-1001	o	6.3 Comparison with re-ranking approach Finally we compared our algorithm with the reranking approach -LRB- Collins and Duffy 2002 Collins 2002b -RRB- where we rst generate the n-best candidates using a model with only local features -LRB- the rst model -RRB- and then re-rank the candidates using a model with non-local features -LRB- the second model -RRB-	amod_model_second det_model_the appos_features_model amod_features_non-local prep_with_model_features det_model_a dobj_using_model det_candidates_the xcomp_re-rank_using dobj_re-rank_candidates advmod_re-rank_then nsubj_re-rank_we nn_model_rst det_model_the appos_features_model amod_features_local amod_features_only prep_with_model_features det_model_a dobj_using_model amod_candidates_n-best det_candidates_the conj_and_generate_re-rank xcomp_generate_using dobj_generate_candidates aux_generate_rst nsubj_generate_we advmod_generate_where appos_Collins_2002b dep_Collins_Collins conj_and_Collins_2002 conj_and_Collins_Duffy rcmod_approach_re-rank rcmod_approach_generate appos_approach_2002 appos_approach_Duffy appos_approach_Collins nn_approach_reranking det_approach_the prep_with_algorithm_approach poss_algorithm_our dobj_compared_algorithm nsubj_compared_we ccomp_compared_Comparison amod_approach_re-ranking advmod_Comparison_Finally prep_with_Comparison_approach num_Comparison_6.3
D07-1033	W02-1001	o	re-ranking 1 uses the score of the rst model as a feature in addition to the non-local features as in Collins -LRB- 2002b -RRB-	appos_Collins_2002b pobj_in_Collins pcomp_as_in prep_features_as amod_features_non-local det_features_the prep_in_addition_to_feature_features det_feature_a nn_model_rst det_model_the prep_of_score_model det_score_the prep_as_uses_feature dobj_uses_score nsubj_uses_re-ranking num_re-ranking_1
D07-1033	W02-1001	p	To avoid this problem we adopt cross-validation training as used in Collins -LRB- 2002b -RRB-	appos_Collins_2002b prep_in_used_Collins mark_used_as amod_training_cross-validation advcl_adopt_used dobj_adopt_training nsubj_adopt_we advcl_adopt_avoid det_problem_this dobj_avoid_problem aux_avoid_To
D07-1033	W02-1001	o	To achieve robust training Daume III and Marcu -LRB- 2005 -RRB- employed the averaged perceptron -LRB- Collins 2002a -RRB- and ALMA -LRB- Gentile 2001 -RRB-	amod_Gentile_2001 dep_ALMA_Gentile appos_Collins_2002a conj_and_perceptron_ALMA dep_perceptron_Collins dobj_averaged_ALMA dobj_averaged_perceptron vmod_the_averaged dobj_employed_the vmod_Marcu_employed appos_Marcu_2005 conj_and_III_Marcu nn_III_Daume amod_training_robust dep_achieve_Marcu dep_achieve_III dobj_achieve_training aux_achieve_To ccomp_``_achieve
D07-1033	W02-1001	o	Collins and Roark -LRB- 2004 -RRB- used the averaged perceptron -LRB- Collins 2002a -RRB-	appos_Collins_2002a dep_perceptron_Collins amod_perceptron_averaged det_perceptron_the dobj_used_perceptron nsubj_used_Roark nsubj_used_Collins appos_Roark_2004 conj_and_Collins_Roark
D07-1033	W02-1001	p	With non-local features we can not use efcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs -LRB- Lafferty et al. 2001 -RRB- and perceptrons -LRB- Collins 2002a -RRB-	appos_Collins_2002a dep_perceptrons_Collins amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et nn_CRFs_training prep_in_required_CRFs auxpass_required_are nsubjpass_required_that nn_algorithm_Viterbi det_algorithm_the conj_and_procedures_algorithm amod_procedures_forward-backward rcmod_procedures_required prep_such_as_procedures_algorithm prep_such_as_procedures_procedures amod_procedures_efcient conj_and_use_perceptrons dep_use_Lafferty dobj_use_procedures neg_use_not aux_use_can nsubj_use_we prep_with_use_features amod_features_non-local
D07-1033	W02-1001	o	In this paper we follow this line of research and try to solve the problem by extending Collins perceptron algorithm -LRB- Collins 2002a -RRB-	appos_Collins_2002a dep_algorithm_Collins nn_algorithm_perceptron nn_algorithm_Collins dobj_extending_algorithm det_problem_the prepc_by_solve_extending dobj_solve_problem aux_solve_to xcomp_try_solve nsubj_try_we prep_of_line_research det_line_this conj_and_follow_try dobj_follow_line nsubj_follow_we prep_in_follow_paper det_paper_this
D07-1033	W02-1001	o	2 Perceptron Algorithm for Sequence Labeling Collins -LRB- 2002a -RRB- proposed an extension of the perceptron algorithm -LRB- Rosenblatt 1958 -RRB- to sequence labeling	nn_labeling_sequence dep_Rosenblatt_1958 dep_algorithm_Rosenblatt nn_algorithm_perceptron det_algorithm_the prep_to_extension_labeling prep_of_extension_algorithm det_extension_an dobj_proposed_extension nsubj_proposed_Collins appos_Collins_2002a ccomp_Labeling_proposed vmod_Sequence_Labeling prep_for_Algorithm_Sequence nn_Algorithm_Perceptron num_Algorithm_2 dep_``_Algorithm
D07-1033	W02-1001	o	The learning algorithm which is illustrated in Collins -LRB- 2002a -RRB- proceeds as follows	mark_follows_as advcl_proceeds_follows nsubj_proceeds_algorithm appos_Collins_2002a prep_in_illustrated_Collins auxpass_illustrated_is nsubjpass_illustrated_which rcmod_algorithm_illustrated nn_algorithm_learning det_algorithm_The
D07-1033	W02-1001	o	This algorithm is proved to converge -LRB- i.e. there are no more updates -RRB- in the separable case -LRB- Collins 2002a -RRB- .1 Thatis ifthereexistweightvectorU -LRB- with | | U | | = 1 -RRB- -LRB- > 0 -RRB- and R -LRB- > 0 -RRB- that satisfy i y Y | xi | -LRB- xi yi -RRB- U -LRB- xi y -RRB- U i y Y | xi | | | -LRB- xi yi -RRB- -LRB- xi y -RRB- | | R the number of updates is at most R2/2	amod_R2/2_most prep_at_is_R2/2 nsubj_is_xi dep_is_Y dep_is_y nsubj_is_U prep_of_number_updates det_number_the num_R_| num_R_| appos_xi_y appos_xi_yi dep_|_R dep_|_xi dep_|_xi num_|_| number_|_| appos_xi_number dep_xi_| num_xi_| appos_U_i nn_U_U appos_xi_y dep_U_xi nn_U_| appos_xi_yi dep_|_xi nn_|_xi num_|_| nn_|_Y amod_|_y ccomp_i_is nsubj_satisfy_that num_>_0 dep_R_> num_>_0 dep_=_1 amod_|_= nn_|_| nn_|_U num_|_| num_|_| prep_with_ifthereexistweightvectorU_| rcmod_Thatis_satisfy conj_and_Thatis_R appos_Thatis_> appos_Thatis_ifthereexistweightvectorU num_Thatis_.1 appos_Collins_2002a dep_case_R dep_case_Thatis dep_case_Collins amod_case_separable det_case_the prep_in_updates_case advmod_updates_more neg_updates_no nsubj_are_updates expl_are_there prep_are_i.e. ccomp_converge_are aux_converge_to parataxis_proved_i xcomp_proved_converge auxpass_proved_is nsubjpass_proved_algorithm det_algorithm_This
D07-1033	W02-1001	o	Thus Collins -LRB- 2002a -RRB- also proposed an averaged perceptron where the nal weight vector is 1Collins -LRB- 2002a -RRB- alsoprovidedproofthatguaranteedgood learning for the non-separable case	amod_case_non-separable det_case_the prep_for_learning_case prepc_alsoprovidedproofthatguaranteedgood_1Collins_learning appos_1Collins_2002a cop_1Collins_is nsubj_1Collins_vector advmod_1Collins_where nn_vector_weight amod_vector_nal det_vector_the rcmod_perceptron_1Collins dobj_averaged_perceptron vmod_an_averaged dobj_proposed_an advmod_proposed_also nsubj_proposed_Collins advmod_proposed_Thus appos_Collins_2002a
D07-1033	W02-1001	o	3 Margin Perceptron Algorithm for Sequence Labeling Weextendedaperceptronwithamargin -LRB- Krauthand Mezard 1987 -RRB- to sequence labeling in this study as Collins -LRB- 2002a -RRB- extended the perceptron algorithm to sequence labeling	nn_labeling_sequence nn_algorithm_perceptron det_algorithm_the prep_to_extended_labeling dobj_extended_algorithm nsubj_extended_Collins mark_extended_as appos_Collins_2002a det_study_this prep_in_labeling_study nn_labeling_sequence dep_Mezard_1987 nn_Mezard_Krauthand dep_Weextendedaperceptronwithamargin_Mezard prep_to_Labeling_labeling dobj_Labeling_Weextendedaperceptronwithamargin vmod_Sequence_Labeling advcl_Algorithm_extended prep_for_Algorithm_Sequence nn_Algorithm_Perceptron nn_Algorithm_Margin num_Algorithm_3 dep_``_Algorithm
D07-1033	W02-1001	o	Based on the proofs in Collins -LRB- 2002a -RRB- and Li et al.	nn_al._et nn_al._Li conj_and_Collins_al. dep_Collins_2002a prep_in_proofs_al. prep_in_proofs_Collins det_proofs_the pobj_on_proofs pcomp_Based_on
D07-1033	W02-1001	o	2 -LRB- Daume III and Marcu 2005 -RRB- also presents the method using the averaged perceptron -LRB- Collins 2002a -RRB- 3For re-ranking problems Shen and Joshi -LRB- 2004 -RRB- proposed a perceptron algorithm that also uses margins	dobj_uses_margins advmod_uses_also nsubj_uses_that rcmod_algorithm_uses nn_algorithm_perceptron det_algorithm_a dobj_proposed_algorithm nsubj_proposed_Joshi nsubj_proposed_Shen appos_Joshi_2004 conj_and_Shen_Joshi rcmod_problems_proposed nn_problems_re-ranking nn_problems_3For nn_problems_perceptron dep_Collins_2002a appos_perceptron_Collins amod_perceptron_averaged det_perceptron_the dobj_using_problems vmod_method_using det_method_the dobj_presents_method advmod_presents_also nsubj_presents_2 amod_III_2005 conj_and_III_Marcu nn_III_Daume appos_2_Marcu appos_2_III
D07-1033	W02-1001	p	Discriminative methods such as Conditional Random Fields -LRB- CRFs -RRB- -LRB- Lafferty et al. 2001 -RRB- Semi-Markov Random Fields -LRB- Sarawagi and Cohen 2004 -RRB- and perceptrons -LRB- Collins 2002a -RRB- have been popular approaches for sequence labeling because of their excellent performance which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features	amod_features_non-independent amod_features_overlapping conj_and_overlapping_non-independent prep_of_kinds_features amod_kinds_many dobj_incorporate_kinds aux_incorporate_to vmod_ability_incorporate poss_ability_their prep_to_due_ability advmod_due_mainly cop_due_is nsubj_due_which rcmod_performance_due amod_performance_excellent poss_performance_their nn_labeling_sequence prep_because_of_approaches_performance prep_for_approaches_labeling amod_approaches_popular cop_approaches_been aux_approaches_have nsubj_approaches_perceptrons nsubj_approaches_Fields nsubj_approaches_methods appos_Collins_2002a dep_perceptrons_Collins dep_Sarawagi_2004 conj_and_Sarawagi_Cohen appos_Fields_Cohen appos_Fields_Sarawagi nn_Fields_Random nn_Fields_Semi-Markov amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et dep_Fields_Lafferty appos_Fields_CRFs nn_Fields_Random amod_Fields_Conditional conj_and_methods_perceptrons conj_and_methods_Fields prep_such_as_methods_Fields amod_methods_Discriminative
D07-1033	W02-1001	o	This resembles the re-ranking approach -LRB- Collins and Duffy 2002 Collins 2002b -RRB-	appos_Collins_2002b dep_Collins_Collins conj_and_Collins_2002 conj_and_Collins_Duffy dep_approach_2002 dep_approach_Duffy dep_approach_Collins amod_approach_re-ranking det_approach_the dobj_resembles_approach nsubj_resembles_This
D07-1033	W02-1001	o	However by examining the Algorithm 4.2 Perceptron with local and non-local features -LRB- parameters n Ca Cl -RRB- 0 until no more updates do for i 1 to L do8 >> >> >> >> >> < >> >> >> >> >> -LCB- yn -RCB- = n-bestyl -LRB- xi y -RRB- y = argmaxy -LCB- yn -RCB- a -LRB- xi y -RRB- y = 2nd-besty -LCB- yn -RCB- a -LRB- xi y -RRB- if y = yi & a -LRB- xi yi -RRB- a -LRB- xi y -RRB- Ca then = + a -LRB- xi yi -RRB- a -LRB- xi y -RRB- -LRB- A -RRB- else if a -LRB- xi yi -RRB- a -LRB- xi y -RRB- Ca then = + a -LRB- xi yi -RRB- a -LRB- xi y -RRB- -LRB- A -RRB- else -LRB- B -RRB- 8 > < > if y1 = yi then -LRB- y1 represents the best in -LCB- yn -RCB- -RRB- = + l -LRB- xi yi -RRB- l -LRB- xi y1 -RRB- else if l -LRB- xi yi -RRB- l -LRB- xi y2 -RRB- Cl then = + l -LRB- xi yi -RRB- l -LRB- xi y2 -RRB- proofs in Collins -LRB- 2002a -RRB- we can see that the essential condition for convergence is that the weights are always updated using some y -LRB- = y -RRB- that satises -LRB- xi yi -RRB- -LRB- xi y -RRB- 0 -LRB- C in the case of a perceptron with a margin -RRB-	det_margin_a prep_with_perceptron_margin det_perceptron_a prep_of_case_perceptron det_case_the prep_in_C_case appos_xi_C num_xi_0 appos_xi_y appos_xi_yi nsubj_satises_that dep_=_y rcmod_y_satises dep_y_= det_y_some dobj_using_y xcomp_updated_using advmod_updated_always auxpass_updated_are nsubjpass_updated_weights mark_updated_that det_weights_the ccomp_is_updated nsubj_is_condition mark_is_that prep_for_condition_convergence amod_condition_essential det_condition_the ccomp_see_is aux_see_can nsubj_see_we appos_Collins_2002a prep_in_proofs_Collins nn_proofs_l appos_xi_y2 dep_l_xi appos_xi_yi dep_=_proofs dep_=_xi conj_+_=_l advmod_=_then nsubj_=_Cl mark_=_if dep_=_l nn_Cl_l appos_xi_y2 dep_l_xi nn_l_l appos_xi_yi dep_l_xi appos_xi_y1 advmod_l_else dep_l_xi appos_xi_yi dep_=_l dep_=_= dep_=_xi conj_+_=_l prepc_in_best_l prepc_in_best_= dep_best_yn det_best_the dep_represents_xi dep_represents_xi parataxis_represents_see dobj_represents_best nsubj_represents_y1 advmod_represents_then dep_represents_yi amod_yi_= nn_yi_y1 mark_yi_if amod_>_< dep_<_> dep_<_8 dep_else_a dep_else_a appos_xi_y appos_a_A dep_a_xi appos_xi_yi dep_a_xi dep_=_> appos_=_B conj_+_=_else advmod_=_then amod_Ca_else amod_Ca_= dep_Ca_xi det_Ca_a appos_xi_y appos_xi_yi dep_a_represents appos_a_Ca dep_a_xi prep_if_A_a advmod_A_else det_A_a appos_xi_y dep_a_xi appos_xi_yi amod_a_A dep_a_xi conj_+_=_a advmod_=_then amod_Ca_a amod_Ca_= det_Ca_a appos_xi_y dep_a_xi nn_a_a nn_a_yi appos_xi_yi dep_a_xi conj_and_yi_a dobj_=_Ca amod_y_= appos_xi_y dep_a_xi prep_if_2nd-besty_y dep_2nd-besty_a appos_2nd-besty_yn dobj_=_2nd-besty dep_y_= dep_y_xi det_y_a dep_xi_y dep_argmaxy_y appos_argmaxy_yn dobj_=_argmaxy dep_y_= nn_y_n-bestyl appos_xi_y dep_n-bestyl_xi dep_=_y num_>>_>> number_>>_>> dep_>>_>> num_>>_>> dep_<_>> amod_>>_< num_>>_>> number_>>_>> amod_>>_= appos_>>_yn dep_>>_>> dep_>>_>> dep_do8_>> dep_L_do8 prep_to_1_L dep_i_1 prep_for_do_i nsubj_do_updates mark_do_until dep_do_Perceptron amod_updates_more neg_more_no appos_n_Cl appos_n_Ca dep_parameters_n dep_features_parameters amod_features_non-local amod_features_local conj_and_local_non-local num_Perceptron_0 prep_with_Perceptron_features dep_Algorithm_do num_Algorithm_4.2 det_Algorithm_the dobj_examining_Algorithm pcomp_by_examining ccomp_,_by dep_``_However
D07-1064	W02-1001	o	As mentioned earlier both of these methods are based on Collinss averaged-perceptron algorithm for sequence labeling -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_labeling_Collins nn_labeling_sequence prep_for_algorithm_labeling amod_algorithm_averaged-perceptron nn_algorithm_Collinss prep_on_based_algorithm auxpass_based_are nsubjpass_based_both advcl_based_mentioned det_methods_these prep_of_both_methods advmod_mentioned_earlier mark_mentioned_As
D07-1064	W02-1001	o	This is contrastive to the one dimensional models used by Collinss perceptronbased sequence method -LRB- Collins 2002 -RRB- which our algorithms are based upon and by the linear-chain CRFs	amod_CRFs_linear-chain det_CRFs_the pobj_by_CRFs conj_and_based_by advmod_based_upon auxpass_based_are nsubjpass_based_algorithms dobj_based_which poss_algorithms_our amod_Collins_2002 rcmod_method_by rcmod_method_based dep_method_Collins nn_method_sequence amod_method_perceptronbased nn_method_Collinss agent_used_method vmod_models_used amod_models_dimensional num_models_one det_models_the prep_to_contrastive_models cop_contrastive_is nsubj_contrastive_This ccomp_``_contrastive
D07-1064	W02-1001	o	Our learning method is an extension of Collinss perceptron-based method for sequence labeling -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_labeling_Collins nn_labeling_sequence amod_method_perceptron-based nn_method_Collinss prep_for_extension_labeling prep_of_extension_method det_extension_an cop_extension_is nsubj_extension_method nn_method_learning poss_method_Our
D07-1071	W02-1001	o	In Step 3 a simple perceptron update -LRB- Collins 2002 -RRB- is performed	auxpass_performed_is nsubjpass_performed_Collins amod_Collins_2002 ccomp_update_performed nsubj_update_perceptron prep_in_update_Step amod_perceptron_simple det_perceptron_a num_Step_3
D07-1077	W02-1001	o	We trained a Chinese Treebank-style tokenizer and partof-speech tagger both using a tagging model based on a perceptron learning algorithm -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_algorithm_Collins amod_algorithm_learning nn_algorithm_perceptron det_algorithm_a prep_on_based_algorithm vmod_model_based amod_model_tagging det_model_a dobj_using_model vmod_both_using amod_tagger_partof-speech appos_tokenizer_both conj_and_tokenizer_tagger nn_tokenizer_Treebank-style amod_tokenizer_Chinese det_tokenizer_a dobj_trained_tagger dobj_trained_tokenizer nsubj_trained_We ccomp_``_trained
D07-1086	W02-1001	p	In our preliminary experiments we used a Support Vector Machine -LRB- SVM -RRB- ranker -LRB- Joachims 2002 -RRB- to learn the structured classi er .2 We also in1See e.g. Collins -LRB- 2002 -RRB- for a popular training algorithm	nn_algorithm_training amod_algorithm_popular det_algorithm_a appos_Collins_2002 pobj_e.g._Collins prep_for_in1See_algorithm prep_in1See_e.g. dep_also_in1See advmod_We_also dep_.2_We nn_.2_er nn_.2_classi amod_.2_structured det_.2_the dobj_learn_.2 aux_learn_to amod_Joachims_2002 dep_ranker_Joachims nn_ranker_Machine appos_Machine_SVM nn_Machine_Vector nn_Machine_Support det_Machine_a vmod_used_learn dobj_used_ranker nsubj_used_we prep_in_used_experiments amod_experiments_preliminary poss_experiments_our
D07-1101	W02-1001	o	The definition of 2 -LRB- x h m c -RRB- is dir cpos -LRB- xh -RRB- cpos -LRB- xm -RRB- cpos -LRB- xc -RRB- dir cpos -LRB- xh -RRB- cpos -LRB- xc -RRB- dir cpos -LRB- xm -RRB- cpos -LRB- xc -RRB- dir form -LRB- xh -RRB- form -LRB- xc -RRB- dir form -LRB- xm -RRB- form -LRB- xc -RRB- dir cpos -LRB- xh -RRB- form -LRB- xc -RRB- dir cpos -LRB- xm -RRB- form -LRB- xc -RRB- dir form -LRB- xh -RRB- cpos -LRB- xc -RRB- dir form -LRB- xm -RRB- cpos -LRB- xc -RRB- 3 Experiments and Results We report experiments with higher-order models for the ten languages in the multilingual track of the CoNLL-2007 shared task -LRB- Nivre et al. 2007 -RRB- .1 In all experiments we trained our models using the averaged perceptron -LRB- Freund and Schapire 1999 -RRB- following the extension of Collins -LRB- 2002 -RRB- for structured prediction problems	nn_problems_prediction amod_problems_structured appos_Collins_2002 prep_for_extension_problems prep_of_extension_Collins det_extension_the amod_Freund_1999 conj_and_Freund_Schapire dep_perceptron_Schapire dep_perceptron_Freund amod_perceptron_averaged det_perceptron_the dobj_using_perceptron poss_models_our xcomp_trained_using dobj_trained_models nsubj_trained_we det_experiments_all prep_following_Nivre_extension rcmod_Nivre_trained prep_in_Nivre_experiments num_Nivre_.1 amod_Nivre_2007 dep_Nivre_al. nn_Nivre_et dobj_shared_task nsubj_shared_experiments det_CoNLL-2007_the prep_of_track_CoNLL-2007 amod_track_multilingual det_track_the prep_in_languages_track num_languages_ten det_languages_the prep_for_models_languages amod_models_higher-order prep_with_experiments_models ccomp_report_shared nsubj_report_We rcmod_Results_report dep_Experiments_Nivre conj_and_Experiments_Results num_Experiments_3 nn_Experiments_cpos appos_cpos_xc nn_cpos_form appos_form_xm nn_form_dir nn_form_cpos appos_cpos_xc nn_cpos_form appos_form_xh nn_form_dir nn_form_form appos_form_xc nn_form_cpos appos_cpos_xm nn_cpos_dir nn_cpos_form appos_form_xc nn_form_cpos appos_cpos_xh nn_cpos_dir nn_cpos_form appos_form_xc nn_form_form appos_form_xm nn_form_dir nn_form_form appos_form_xc nn_form_form appos_form_xh nn_form_dir nn_form_cpos nn_form_cpos nn_form_cpos appos_cpos_xc nn_cpos_cpos appos_cpos_xm nn_cpos_dir appos_cpos_xc nn_cpos_cpos appos_cpos_xh nn_cpos_dir appos_cpos_xc nn_cpos_cpos appos_cpos_xm nn_cpos_cpos appos_cpos_xh nn_cpos_dir dep_is_Results dep_is_Experiments nsubj_is_definition appos_x_c appos_x_m appos_x_h dep_2_x prep_of_definition_2 det_definition_The ccomp_``_is
D07-1126	W02-1001	p	The Perceptron style for natural language processing problems as initially proposed by -LRB- Collins 2002 -RRB- can provide state of the art results on various domains including text chunking syntactic parsing etc. The main drawback of the Perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data	nn_data_training det_data_the prep_of_margin_data amod_margin_maximize det_margin_the dobj_attaining_margin prepc_for_mechanism_attaining det_mechanism_a dobj_have_mechanism neg_have_not aux_have_does nsubj_have_it mark_have_that ccomp_is_have nsubj_is_drawback nn_algorithm_style nn_algorithm_Perceptron det_algorithm_the prep_of_drawback_algorithm amod_drawback_main det_drawback_The amod_parsing_syntactic conj_chunking_etc. conj_chunking_parsing nn_chunking_text prep_including_domains_chunking amod_domains_various parataxis_results_is prep_on_results_domains nsubj_results_style det_art_the prep_of_state_art dobj_provide_state aux_provide_can nsubj_provide_Collins mark_provide_by amod_Collins_2002 ccomp_proposed_provide advmod_proposed_initially mark_proposed_as nn_problems_processing nn_problems_language amod_problems_natural dep_style_proposed prep_for_style_problems nn_style_Perceptron det_style_The
D07-1126	W02-1001	o	Averaging has been shown to help reduce overfitting -LRB- McDonald et al. 2005a Collins 2002 -RRB-	amod_Collins_2002 dep_McDonald_Collins appos_McDonald_2005a dep_McDonald_al. nn_McDonald_et appos_overfitting_McDonald dobj_reduce_overfitting ccomp_help_reduce aux_help_to xcomp_shown_help auxpass_shown_been aux_shown_has nsubjpass_shown_Averaging
D07-1126	W02-1001	o	It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm -LRB- PC -RRB- -LRB- Collins 2002 -RRB- as well as the MIRA algorithm -LRB- McDonald et al. 2005a -RRB- is in line 9	num_line_9 prep_in_is_line nsubj_is_difference mark_is_that appos_McDonald_2005a dep_McDonald_al. nn_McDonald_et dep_algorithm_McDonald nn_algorithm_MIRA det_algorithm_the amod_Collins_2002 dep_algorithm_Collins appos_algorithm_PC nn_algorithm_Perceptron det_algorithm_the conj_and_algorithms_algorithm conj_and_algorithms_algorithm nn_algorithms_PA det_algorithms_the prep_between_difference_algorithm prep_between_difference_algorithm prep_between_difference_algorithms amod_difference_main det_difference_the ccomp_see_is aux_see_to xcomp_easy_see cop_easy_is nsubj_easy_It
D07-1126	W02-1001	o	The learning methods using in discriminative parsing are Perceptron -LRB- Collins 2002 -RRB- and online large-margin learning -LRB- MIRA -RRB- -LRB- Crammer and Singer 2003 -RRB-	amod_Crammer_2003 conj_and_Crammer_Singer dep_learning_Singer dep_learning_Crammer appos_learning_MIRA amod_learning_large-margin amod_learning_online dep_Collins_2002 conj_and_Perceptron_learning dep_Perceptron_Collins cop_Perceptron_are nsubj_Perceptron_methods amod_parsing_discriminative prep_in_using_parsing vmod_methods_using nn_methods_learning det_methods_The
D07-1129	W02-1001	o	2.3 Online Learning Again following -LRB- McDonald et al. 2005 -RRB- we have used the single best MIRA -LRB- Crammer and Singer 2003 -RRB- which is a margin aware variant of perceptron -LRB- Collins 2002 Collins and Roark 2004 -RRB- for structured prediction	amod_prediction_structured prep_for_Collins_prediction appos_Collins_2004 conj_and_Collins_Roark dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 dep_perceptron_Collins prep_of_variant_perceptron amod_variant_aware nn_variant_margin det_variant_a cop_variant_is nsubj_variant_which dep_Crammer_2003 conj_and_Crammer_Singer rcmod_MIRA_variant appos_MIRA_Singer appos_MIRA_Crammer amod_MIRA_best amod_MIRA_single det_MIRA_the dobj_used_MIRA aux_used_have nsubj_used_we dep_used_Again amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et dep_following_McDonald vmod_Again_following nn_Again_Learning nn_Again_Online num_Again_2.3 advcl_``_used
D07-1129	W02-1001	o	We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron -LRB- Collins 2002 Collins and Roark 2004 Crammer and Singer 2003 -RRB-	appos_Crammer_2003 conj_and_Crammer_Singer num_Collins_2004 conj_and_Collins_Roark dep_Collins_Singer dep_Collins_Crammer dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 dep_perceptron_Collins amod_perceptron_voted det_perceptron_the prep_of_variant_perceptron det_variant_a dobj_using_variant amod_fashion_on-line det_fashion_an poss_parser_our vmod_trained_using prep_in_trained_fashion dobj_trained_parser advmod_trained_discriminatively nsubj_trained_We
D08-1024	W02-1001	p	The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training and has been shown to work well in practice -LRB- Freund and Schapire 1999 Collins 2002 -RRB-	amod_Collins_2002 dep_Freund_Collins conj_and_Freund_1999 conj_and_Freund_Schapire dep_practice_1999 dep_practice_Schapire dep_practice_Freund prep_in_work_practice advmod_work_well aux_work_to xcomp_shown_work auxpass_shown_been aux_shown_has nsubjpass_shown_technique prep_during_traversed_training vmod_models_traversed det_models_the predet_models_all det_vote_a prep_among_taking_models dobj_taking_vote prepc_to_approximation_taking det_approximation_an prep_of_context_perceptrons det_context_the conj_and_introduced_shown prep_as_introduced_approximation prep_in_introduced_context auxpass_introduced_was nsubjpass_introduced_technique prep_of_technique_averaging det_technique_The
D08-1052	W02-1001	o	Furthermore we use averaged weights -LRB- Collins 2002 Freund and Schapire 1999 -RRB- in Algorithm 1	num_Algorithm_1 dep_Freund_1999 conj_and_Freund_Schapire prep_in_Collins_Algorithm dep_Collins_Schapire dep_Collins_Freund appos_Collins_2002 dep_weights_Collins dobj_averaged_weights dep_use_averaged nsubj_use_we advmod_use_Furthermore
D08-1052	W02-1001	o	Our learning algorithm stems from Perceptron training in -LRB- Collins 2002 -RRB-	amod_Collins_2002 prep_in_training_Collins nn_training_Perceptron prep_from_stems_training nsubj_stems_algorithm nn_algorithm_learning poss_algorithm_Our
D08-1052	W02-1001	o	In Section 3 we will present a Perceptron like algorithm -LRB- Collins 2002 Daume III and Marcu 2005 -RRB- to obtain the parameters	det_parameters_the dobj_obtain_parameters aux_obtain_to dep_III_2005 conj_and_III_Marcu nn_III_Daume dep_Collins_Marcu dep_Collins_III amod_Collins_2002 appos_Perceptron_Collins prep_like_Perceptron_algorithm det_Perceptron_a vmod_present_obtain dobj_present_Perceptron aux_present_will nsubj_present_we prep_in_present_Section num_Section_3
D08-1059	W02-1001	o	Before parsing POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_from_POStagger_Collins det_POStagger_the prep_of_reimplementation_POStagger poss_reimplementation_our dobj_using_reimplementation nn_sentence_input det_sentence_the xcomp_assigned_using prep_to_assigned_sentence auxpass_assigned_are nsubjpass_assigned_tags prep_before_assigned_parsing nn_tags_POS
D08-1059	W02-1001	o	We use the discriminative perceptron learning algorithm -LRB- Collins 2002 McDonald et al. 2005 -RRB- to train the values of vectorw	prep_of_values_vectorw det_values_the dobj_train_values aux_train_to num_McDonald_2005 nn_McDonald_al. nn_McDonald_et dep_Collins_McDonald amod_Collins_2002 appos_algorithm_Collins amod_algorithm_learning nn_algorithm_perceptron amod_algorithm_discriminative det_algorithm_the vmod_use_train dobj_use_algorithm nsubj_use_We
D08-1059	W02-1001	p	Averaging parameters is a way to reduce overfitting for perceptron training -LRB- Collins 2002 -RRB- and is applied to all our experiments	poss_experiments_our predet_experiments_all prep_to_applied_experiments auxpass_applied_is nsubjpass_applied_parameters dep_Collins_2002 appos_training_Collins nn_training_perceptron prep_for_reduce_training dobj_reduce_overfitting aux_reduce_to conj_and_way_applied vmod_way_reduce det_way_a cop_way_is nsubj_way_parameters amod_parameters_Averaging
D08-1059	W02-1001	o	As with the graph-based parser we use the discriminative perceptron -LRB- Collins 2002 -RRB- to train the transition-based model -LRB- see Figure 5 -RRB-	num_Figure_5 dobj_see_Figure amod_model_transition-based det_model_the dobj_train_model aux_train_to amod_Collins_2002 appos_perceptron_Collins amod_perceptron_discriminative det_perceptron_the parataxis_use_see vmod_use_train dobj_use_perceptron nsubj_use_we prep_use_As amod_parser_graph-based det_parser_the pobj_with_parser pcomp_As_with
D08-1082	W02-1001	o	8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm -LRB- Collins 2002 -RRB- has previously been applied to various NLP tasks -LRB- Collins 2002 Collins 2001 -RRB- for discriminative reranking	amod_reranking_discriminative dep_Collins_2001 dep_Collins_Collins amod_Collins_2002 prep_for_tasks_reranking appos_tasks_Collins nn_tasks_NLP amod_tasks_various prep_to_applied_tasks auxpass_applied_been advmod_applied_previously aux_applied_has amod_Collins_2002 vmod_algorithm_applied dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_The nsubj_algorithm_Algorithm amod_Plane_Separating prep_with_Algorithm_Plane nn_Algorithm_Perceptron amod_Algorithm_Averaged det_Algorithm_The num_Algorithm_8.1
D08-1082	W02-1001	o	The detailed algorithm can be found in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins prep_found_in auxpass_found_be aux_found_can nsubjpass_found_algorithm amod_algorithm_detailed det_algorithm_The
D09-1013	W02-1001	o	Next using our feature vector we applied five different linear classifiers to extract PPI from AIMed L2-SVM 1-norm soft-margin SVM -LRB- L1-SVM -RRB- logistic regression -LRB- LR -RRB- -LRB- Fan et al. 2008 -RRB- averaged perceptron -LRB- AP -RRB- -LRB- Collins 2002 -RRB- and confidence weighted linear classification -LRB- CW -RRB- -LRB- Dredze et al. 2008 -RRB-	amod_Dredze_2008 dep_Dredze_al. nn_Dredze_et appos_classification_CW amod_classification_linear amod_classification_weighted nn_classification_confidence dep_Collins_2002 nn_Collins_perceptron appos_perceptron_AP dep_averaged_Dredze conj_and_averaged_classification dobj_averaged_Collins nsubj_averaged_L2-SVM amod_Fan_2008 dep_Fan_al. nn_Fan_et appos_regression_LR amod_regression_logistic appos_SVM_L1-SVM amod_SVM_soft-margin amod_SVM_1-norm dep_L2-SVM_Fan conj_L2-SVM_regression conj_L2-SVM_SVM parataxis_extract_classification parataxis_extract_averaged prep_from_extract_AIMed dobj_extract_PPI aux_extract_to amod_classifiers_linear amod_classifiers_different num_classifiers_five vmod_applied_extract dobj_applied_classifiers nsubj_applied_we vmod_applied_using advmod_applied_Next nn_vector_feature poss_vector_our dobj_using_vector
D09-1043	W02-1001	o	1 Introduction In this paper we show how discriminative training with averaged perceptron models -LRB- Collins 2002 -RRB- can be used to substantially improve surface realization with Combinatory Categorial Grammar -LRB- Steedman 2000 CCG -RRB-	appos_Steedman_CCG appos_Steedman_2000 dep_Grammar_Steedman amod_Grammar_Categorial amod_Grammar_Combinatory nn_realization_surface prep_with_improve_Grammar dobj_improve_realization advmod_improve_substantially aux_improve_to xcomp_used_improve auxpass_used_be aux_used_can nsubjpass_used_training amod_Collins_2002 dep_models_Collins nn_models_perceptron dobj_averaged_models prepc_with_training_averaged amod_training_discriminative advmod_training_how ccomp_show_used nsubj_show_we nsubj_show_Introduction det_paper_this prep_in_Introduction_paper num_Introduction_1
D09-1043	W02-1001	p	3 Perceptron Reranking As Collins -LRB- 2002 -RRB- observes perceptron training involves a simple on-line algorithm with few iterations typically required to achieve good performance	amod_performance_good dobj_achieve_performance aux_achieve_to xcomp_required_achieve advmod_required_typically vmod_iterations_required amod_iterations_few amod_algorithm_on-line amod_algorithm_simple det_algorithm_a prep_with_involves_iterations dobj_involves_algorithm nsubj_involves_training advcl_involves_observes nsubj_involves_Reranking nn_training_perceptron nsubj_observes_Collins mark_observes_As appos_Collins_2002 nn_Reranking_Perceptron num_Reranking_3
D09-1052	W02-1001	o	Online baselines include Top-1 Perceptron -LRB- Collins 2002 -RRB- Top-1 Passive-Aggressive -LRB- PA -RRB- and k-best PA -LRB- Crammer & Singer 2003 McDonald et al. 2004 -RRB-	num_McDonald_2004 nn_McDonald_al. nn_McDonald_et dep_Crammer_McDonald dep_Crammer_2003 conj_and_Crammer_Singer appos_PA_Singer appos_PA_Crammer amod_PA_k-best appos_Passive-Aggressive_PA dep_Top-1_Passive-Aggressive dep_Collins_2002 conj_and_Perceptron_PA appos_Perceptron_Top-1 appos_Perceptron_Collins nn_Perceptron_Top-1 dobj_include_PA dobj_include_Perceptron nsubj_include_baselines nn_baselines_Online
D09-1052	W02-1001	o	2 Problem Setting In the multi-class setting instances from an input spaceX take labels from a finite setY | Y | = K. 496 We use a standard approach -LRB- Collins 2002 -RRB- for generalizing binary classification and assume a feature function f -LRB- x y -RRB- Rd mapping instances xX and labels yY into a common space	amod_space_common det_space_a prep_into_yY_space nn_yY_labels nn_yY_xX nn_yY_instances nn_yY_mapping nn_yY_Rd nn_yY_x conj_and_xX_labels conj_x_y nn_x_f dep_function_yY nn_function_feature det_function_a dobj_assume_function amod_classification_binary dobj_generalizing_classification dep_Collins_2002 appos_approach_Collins amod_approach_standard det_approach_a prepc_for_use_generalizing dobj_use_approach nsubj_use_We dep_use_= nsubj_use_| dep_use_Y dep_use_| num_K._496 dep_=_K. conj_and_setY_assume conj_and_setY_use amod_setY_finite det_setY_a prep_from_take_assume prep_from_take_use prep_from_take_setY dobj_take_labels nsubj_take_spaceX nn_spaceX_input det_spaceX_an prepc_from_instances_take amod_setting_multi-class det_setting_the appos_Setting_instances prep_in_Setting_setting nn_Setting_Problem num_Setting_2 dep_``_Setting
D09-1105	W02-1001	o	We used the averaged perceptron algorithm -LRB- Freund and Schapire 1998 Collins 2002 -RRB- to train the parameters of the model	det_model_the prep_of_parameters_model det_parameters_the dobj_train_parameters aux_train_to amod_Collins_2002 vmod_Freund_train dep_Freund_Collins conj_and_Freund_1998 conj_and_Freund_Schapire dep_algorithm_1998 dep_algorithm_Schapire dep_algorithm_Freund nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the dobj_used_algorithm nsubj_used_We
D09-1111	W02-1001	o	A structured perceptron -LRB- Collins 2002 -RRB- learns weights for our transliteration features which are drawn from two broad classes indicator and hybrid generative features	nn_features_generative nn_features_hybrid conj_and_indicator_features amod_classes_broad num_classes_two prep_from_drawn_classes auxpass_drawn_are nsubjpass_drawn_which rcmod_features_drawn nn_features_transliteration poss_features_our prep_for_weights_features dep_learns_features dep_learns_indicator dobj_learns_weights nsubj_learns_perceptron amod_Collins_2002 appos_perceptron_Collins amod_perceptron_structured det_perceptron_A
D09-1111	W02-1001	o	Given an input training corpus of such derivations D = d1 dn a vector feature function on derivations vectorF -LRB- d -RRB- and an initial weight vector vectorw the perceptron performs two steps for each training example di D Decode d = argmaxdD -LRB- src -LRB- di -RRB- -RRB- parenleftBig vectorw vectorF -LRB- d -RRB- parenrightBig Update vectorw = vectorw + vectorF -LRB- di -RRB- vectorF -LRB- d -RRB- where D -LRB- src -LRB- d -RRB- -RRB- enumerates all possible derivations with the same source side as d. To improve generalization the final feature vector is the average of all vectors found during learning -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_learning_Collins prep_during_found_learning vmod_vectors_found det_vectors_all prep_of_average_vectors det_average_the cop_average_is nsubj_average_vector nn_vector_feature amod_vector_final det_vector_the dobj_improve_generalization aux_improve_To prep_as_side_d. nn_side_source amod_side_same det_side_the amod_derivations_possible det_derivations_all vmod_enumerates_improve prep_with_enumerates_side dobj_enumerates_derivations nsubj_enumerates_D advmod_enumerates_where appos_src_d dep_D_src appos_vectorF_d nn_vectorF_vectorF nn_vectorF_vectorw appos_vectorF_di conj_+_vectorw_vectorF rcmod_=_enumerates dep_=_vectorF rcmod_vectorw_average amod_vectorw_= nn_Update_parenrightBig dep_vectorF_vectorw dep_vectorF_Update appos_vectorF_d nn_vectorF_vectorw nn_vectorF_parenleftBig dep_vectorF_Decode dep_src_di nn_src_argmaxdD amod_src_= nn_src_d dep_Decode_src nn_D_di nn_D_example nn_D_training det_D_each dep_steps_vectorF prep_for_steps_D num_steps_two dobj_performs_steps nsubj_performs_perceptron det_perceptron_the nn_vectorw_vector nn_vectorw_weight amod_vectorw_initial det_vectorw_an appos_vectorF_d dep_derivations_vectorF prep_on_function_derivations nn_function_feature nn_function_vector det_function_a rcmod_dn_performs conj_and_dn_vectorw conj_and_dn_function nn_dn_d1 dobj_=_vectorw dobj_=_function dobj_=_dn nn_D_derivations amod_D_such prep_corpus_= prep_of_corpus_D nn_corpus_training nn_corpus_input det_corpus_an pobj_Given_corpus ccomp_``_Given
D09-1127	W02-1001	o	We evaluate its performance on the standard Penn English Treebank -LRB- PTB -RRB- dependency parsing task i.e. train on sections 02-21 and test on section 23 with automatically assigned POS tags -LRB- at 97.2 % accuracy -RRB- using a tagger similar to Collins -LRB- 2002 -RRB- and using the headrules of Yamada and Matsumoto -LRB- 2003 -RRB- for conversion into dependency trees	nn_trees_dependency prep_into_conversion_trees appos_Matsumoto_2003 conj_and_Yamada_Matsumoto prep_of_headrules_Matsumoto prep_of_headrules_Yamada det_headrules_the prep_for_using_conversion dobj_using_headrules appos_Collins_2002 prep_to_similar_Collins amod_tagger_similar det_tagger_a dobj_using_tagger amod_accuracy_% number_%_97.2 prep_at_tags_accuracy nn_tags_POS amod_tags_assigned advmod_assigned_automatically num_section_23 vmod_test_using prep_with_test_tags prep_on_test_section num_sections_02-21 conj_and_train_using conj_and_train_test prep_on_train_sections nn_task_parsing nn_task_dependency nn_task_Treebank appos_Treebank_PTB nn_Treebank_English nn_Treebank_Penn amod_Treebank_standard det_Treebank_the prep_on_performance_task poss_performance_its dep_evaluate_using dep_evaluate_test dep_evaluate_train dep_evaluate_i.e. dobj_evaluate_performance nsubj_evaluate_We ccomp_``_evaluate
D09-1127	W02-1001	p	We use the popular online learning algorithm of structured perceptron with parameter averaging -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_averaging_Collins nn_averaging_parameter amod_perceptron_structured prep_of_algorithm_perceptron nn_algorithm_learning amod_algorithm_online amod_algorithm_popular det_algorithm_the prep_with_use_averaging dobj_use_algorithm nsubj_use_We
D09-1161	W02-1001	p	The averaged 1555 perceptron has a solid theoretical fundamental and was proved to be effective across a variety of NLP tasks -LRB- Collins 2002 -RRB-	num_Collins_2002 nn_tasks_NLP prep_of_variety_tasks det_variety_a prep_across_effective_variety cop_effective_be aux_effective_to xcomp_proved_effective auxpass_proved_was csubjpass_proved_averaged dep_fundamental_Collins conj_and_fundamental_proved amod_fundamental_theoretical amod_fundamental_solid det_fundamental_a aux_fundamental_has csubj_fundamental_averaged dep_fundamental_The num_perceptron_1555 dobj_averaged_perceptron
D09-1161	W02-1001	o	In addition parsing re-ranking -LRB- Collins 2000 Riezler et al. 2002 Charniak and Johnson 2005 Huang 2008 -RRB- has also been shown to be another effective technique to improve parsing performance	nn_performance_parsing dobj_improve_performance aux_improve_to vmod_technique_improve amod_technique_effective det_technique_another cop_technique_be aux_technique_to xcomp_shown_technique auxpass_shown_been advmod_shown_also aux_shown_has nsubjpass_shown_re-ranking prep_in_shown_addition num_Huang_2008 dep_Charniak_2005 conj_and_Charniak_Johnson dep_al._2002 nn_al._et nn_al._Riezler dep_Collins_Huang dep_Collins_Johnson dep_Collins_Charniak dep_Collins_al. num_Collins_2000 appos_re-ranking_Collins amod_re-ranking_parsing
D09-1161	W02-1001	n	Our study also shows that the simulated-annealing algorithm -LRB- Kirkpatrick et al. 1983 -RRB- is more effective 1552 than the perceptron algorithm -LRB- Collins 2002 -RRB- for feature weight tuning	nn_tuning_weight nn_tuning_feature num_Collins_2002 prep_for_algorithm_tuning appos_algorithm_Collins nn_algorithm_perceptron det_algorithm_the prep_than_1552_algorithm dobj_effective_1552 advmod_effective_more cop_effective_is nsubj_effective_algorithm mark_effective_that nn_1983_al. num_Kirkpatrick_1983 nn_Kirkpatrick_et appos_algorithm_Kirkpatrick amod_algorithm_simulated-annealing det_algorithm_the ccomp_shows_effective advmod_shows_also nsubj_shows_study poss_study_Our
D09-1161	W02-1001	p	Collins -LRB- 2002 -RRB- improves the F1 score from 88.2 % to 89.7 % while Charniak and Johnson -LRB- 2005 -RRB- improve from 90.3 % to 91.4 %	num_%_91.4 num_%_90.3 prep_to_improve_% prep_from_improve_% nsubj_improve_Johnson nsubj_improve_Charniak mark_improve_while appos_Johnson_2005 conj_and_Charniak_Johnson num_%_89.7 num_%_88.2 nn_score_F1 det_score_the advcl_improves_improve prep_to_improves_% prep_from_improves_% dobj_improves_score nsubj_improves_Collins appos_Collins_2002
E06-1011	W02-1001	p	An online learning algorithm considers a single training instance for each update to the weight vector w We use the common method of setting the final weight vector as the average of the weight vectors after each iteration -LRB- Collins 2002 -RRB- which has been shown to alleviate overfitting	dobj_alleviate_overfitting aux_alleviate_to xcomp_shown_alleviate auxpass_shown_been aux_shown_has nsubjpass_shown_which dep_Collins_2002 det_iteration_each nn_vectors_weight det_vectors_the prep_of_average_vectors det_average_the nn_vector_weight amod_vector_final det_vector_the dep_setting_Collins prep_after_setting_iteration prep_as_setting_average dobj_setting_vector rcmod_method_shown prepc_of_method_setting amod_method_common det_method_the dobj_use_method nsubj_use_We nn_w_vector nn_w_weight det_w_the prep_to_update_w nsubj_update_instance prep_for_instance_each nn_instance_training amod_instance_single det_instance_a parataxis_considers_use ccomp_considers_update nsubj_considers_algorithm nn_algorithm_learning amod_algorithm_online det_algorithm_An
E06-1011	W02-1001	o	This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins -LRB- 2002 -RRB-	appos_Collins_2002 nn_Collins_outputs amod_Collins_structured prep_for_algorithm_Collins nn_algorithm_perceptron det_algorithm_the prep_of_version_algorithm amod_version_large-margin det_version_a prep_as_viewed_version auxpass_viewed_be advmod_viewed_thus aux_viewed_can nsubjpass_viewed_algorithm det_algorithm_This
E06-1038	W02-1001	p	Averaging has been shown to reduce overfitting -LRB- Collins 2002 -RRB- as well as reliance on the order of the examples during training	prep_during_examples_training det_examples_the prep_of_order_examples det_order_the prep_on_reliance_order dep_Collins_2002 conj_and_overfitting_reliance appos_overfitting_Collins dobj_reduce_reliance dobj_reduce_overfitting aux_reduce_to xcomp_shown_reduce auxpass_shown_been aux_shown_has nsubjpass_shown_Averaging
E09-1056	W02-1001	p	Online votedperceptrons have been reported to work well in a number of NLP tasks -LRB- Collins 2002 Liang et al. 2006 -RRB-	num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Collins_Liang amod_Collins_2002 nn_tasks_NLP prep_of_number_tasks det_number_a dep_work_Collins prep_in_work_number advmod_work_well aux_work_to xcomp_reported_work auxpass_reported_been aux_reported_have nsubjpass_reported_votedperceptrons nn_votedperceptrons_Online
E09-1087	W02-1001	o	0 10 20 30 40 5097.20 97.25 97.30 97.35 97.40 Iteration Accuracy on development data Every iterationEvery 4th iterationEvery 8th iteration Every 16th iterationOnce at the beginning No supervised data Figure 2 Dependence on the inclusion of the supervised training data -LRB- English -RRB- English Czech No supervised data 97.37 95.88 Once at the beginning 97.40 96.00 Every training iteration 97.44 96.21 Table 6 Dependence on the inclusion of the supervised training data 5.4 The morphological analyzers and the perceptron feature templates The whole experiment can be performed with the original perceptron feature set described in -LRB- Collins 2002 -RRB- instead of the feature set described in this article	det_article_this prep_in_described_article vmod_set_described vmod_feature_set det_feature_the conj_negcc_Collins_feature dep_Collins_2002 prep_in_described_feature prep_in_described_Collins dep_set_described nsubj_set_Dependence nn_feature_perceptron amod_feature_original det_feature_the prep_with_performed_feature auxpass_performed_be aux_performed_can nsubjpass_performed_experiment amod_experiment_whole det_experiment_The nn_templates_feature nn_templates_perceptron det_templates_the rcmod_analyzers_performed conj_and_analyzers_templates amod_analyzers_morphological det_analyzers_The num_analyzers_5.4 dep_data_templates dep_data_analyzers nn_data_training amod_data_supervised det_data_the prep_of_inclusion_data det_inclusion_the prep_on_Dependence_inclusion dep_Table_set num_Table_6 num_Table_96.21 num_Table_97.44 dep_iteration_Table nn_iteration_training det_iteration_Every num_iteration_96.00 number_96.00_97.40 prep_beginning_the_iteration number_95.88_97.37 num_data_95.88 prep_at_supervised_the advmod_supervised_Once dobj_supervised_data nsubj_supervised_No nn_No_Czech nn_No_English rcmod_data_supervised appos_data_English nn_data_training amod_data_supervised det_data_the prep_of_inclusion_data det_inclusion_the prep_on_Dependence_inclusion num_Figure_2 nn_Figure_data amod_Figure_supervised neg_Figure_No amod_Figure_beginning det_Figure_the dep_iterationOnce_Dependence prep_at_iterationOnce_Figure amod_iterationOnce_16th det_iterationOnce_Every dep_iteration_iterationOnce amod_iteration_8th nn_iteration_iterationEvery amod_iteration_4th nn_iteration_iterationEvery det_iteration_Every nn_data_development dep_Accuracy_iteration prep_on_Accuracy_data nn_Accuracy_Iteration num_Accuracy_97.40 num_Accuracy_0 dep_97.40_97.35 dep_97.35_97.30 number_97.30_97.25 dep_97.30_5097.20 number_5097.20_40 dep_5097.20_30 dep_30_20 number_20_10 dep_``_Accuracy
E09-1087	W02-1001	p	The state-of-the art taggers are using feature sets discribed in the corresponding articles -LRB- -LRB- Collins 2002 -RRB- -LRB- Gimenez and M`arquez 2004 -RRB- -LRB- Toutanova et al. 2003 -RRB- and -LRB- Shen et al. 2007 -RRB- -RRB- Morce supervised and Morce semi-supervised are using feature set desribed in section 4	num_section_4 prep_in_desribed_section dep_set_desribed dobj_using_feature aux_using_are amod_Morce_semi-supervised dep_supervised_set dep_supervised_using conj_and_supervised_Morce nsubj_supervised_Morce dep_supervised_Collins dep_Shen_2007 dep_Shen_al. nn_Shen_et amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_Gimenez_2004 conj_and_Gimenez_M`arquez dep_Collins_Shen cc_Collins_and appos_Collins_Toutanova appos_Collins_M`arquez appos_Collins_Gimenez amod_Collins_2002 dep_articles_Morce dep_articles_supervised amod_articles_corresponding det_articles_the prep_in_discribed_articles vmod_sets_discribed nn_sets_feature dobj_using_sets aux_using_are nsubj_using_taggers nn_taggers_art amod_taggers_state-of-the det_taggers_The ccomp_``_using
E09-1087	W02-1001	o	c2009 Association for Computational Linguistics Semi-supervised Training for the Averaged Perceptron POS Tagger Drahomra johanka Spoustova Jan Hajic Jan Raab Miroslav Spousta Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics Charles University Prague Czech Republic -LCB- johanka hajic raab spousta -RCB- @ ufal.mff.cuni.cz Abstract This paper describes POS tagging experiments with semi-supervised training as an extension to the -LRB- supervised -RRB- averaged perceptron algorithm first introduced for this task by -LRB- Collins 2002 -RRB-	amod_Collins_2002 det_task_this agent_introduced_Collins prep_for_introduced_task advmod_introduced_first nn_algorithm_perceptron vmod_averaged_introduced dobj_averaged_algorithm nsubj_averaged_extension mark_averaged_as dep_the_supervised prep_to_extension_the det_extension_an amod_training_semi-supervised prep_with_experiments_training nn_experiments_tagging nn_experiments_POS advcl_describes_averaged dobj_describes_experiments nsubj_describes_paper det_paper_This rcmod_Abstract_describes nn_Abstract_ufal.mff.cuni.cz dep_Abstract_@ appos_johanka_spousta appos_johanka_raab appos_johanka_hajic dep_Republic_Abstract dep_Republic_johanka nn_Republic_Czech appos_Prague_Republic nn_Prague_University nn_Prague_Charles dep_,_Prague conj_and_Mathematics_Physics nn_Faculty_Linguistics nn_Faculty_Applied prep_of_Formal_Physics prep_of_Formal_Mathematics conj_and_Formal_Faculty prep_of_Institute_Faculty prep_of_Institute_Formal nn_Institute_Spousta nn_Institute_Miroslav nn_Institute_Raab nn_Institute_Jan nn_Institute_Hajic nn_Institute_Jan nn_Institute_Spoustova nn_Institute_johanka nn_Institute_Drahomra nn_Institute_Tagger nn_Institute_POS nn_Institute_Perceptron nn_Institute_Averaged det_Institute_the amod_Training_Semi-supervised nn_Training_Linguistics nn_Training_Computational prep_for_Association_Institute prep_for_Association_Training nn_Association_c2009 dep_``_Association
E09-1087	W02-1001	o	To summarize we can describe our system as follows it is based on -LRB- Votrubec 2006 -RRB- s implementation of -LRB- Collins 2002 -RRB- which has been fed at each iteration by a different dataset consisting of the supervised and unsupervised part precisely by a concatenation of the manually tagged training data -LRB- WSJ portion of the PTB 3 for English morphologically disambiguated data from PDT 2.0 for Czech -RRB- and a chunk of automatically tagged unsupervised data	amod_data_unsupervised amod_data_tagged advmod_tagged_automatically prep_of_chunk_data det_chunk_a num_PDT_2.0 prep_for_data_Czech prep_from_data_PDT amod_data_disambiguated advmod_disambiguated_morphologically prep_for_PTB_English num_PTB_3 det_PTB_the appos_portion_data prep_of_portion_PTB nn_portion_WSJ conj_and_data_chunk dep_data_portion nn_data_training amod_data_tagged det_data_the advmod_tagged_manually prep_of_concatenation_chunk prep_of_concatenation_data det_concatenation_a pobj_by_concatenation ccomp_,_by amod_part_unsupervised amod_part_supervised det_part_the conj_and_supervised_unsupervised prep_of_consisting_part vmod_dataset_consisting amod_dataset_different det_dataset_a det_iteration_each agent_fed_dataset prep_at_fed_iteration auxpass_fed_been aux_fed_has nsubjpass_fed_which dep_Collins_precisely rcmod_Collins_fed dep_Collins_2002 prep_of_implementation_Collins amod_implementation_s dep_implementation_Votrubec amod_Votrubec_2006 prep_on_based_implementation auxpass_based_is nsubjpass_based_it parataxis_follows_based mark_follows_as poss_system_our advcl_describe_follows dobj_describe_system aux_describe_can nsubj_describe_we advcl_describe_summarize aux_summarize_To
E09-1087	W02-1001	o	It is a reimplementation of the averaged perceptron described in -LRB- Collins 2002 -RRB- which uses such features that it behaves like an HMM tagger and thus the standard Viterbi decoding is possible	cop_possible_is nsubj_possible_decoding advmod_possible_thus nn_decoding_Viterbi amod_decoding_standard det_decoding_the nn_tagger_HMM det_tagger_an conj_and_behaves_possible prep_like_behaves_tagger nsubj_behaves_it mark_behaves_that amod_features_such ccomp_uses_possible ccomp_uses_behaves dobj_uses_features nsubj_uses_which rcmod_Collins_uses dep_Collins_2002 prep_in_described_Collins vmod_perceptron_described amod_perceptron_averaged det_perceptron_the prep_of_reimplementation_perceptron det_reimplementation_a cop_reimplementation_is nsubj_reimplementation_It
E09-1087	W02-1001	o	10Our experiments have shown that using averaging helps tremendously confirming both the theoretical and practical results of -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_of_Collins prep_results_of amod_results_practical amod_results_theoretical det_results_the preconj_results_both conj_and_theoretical_practical dobj_confirming_results advmod_helps_tremendously csubj_helps_using mark_helps_that xcomp_using_averaging xcomp_shown_confirming ccomp_shown_helps aux_shown_have nsubj_shown_experiments nn_experiments_10Our
E09-1087	W02-1001	p	For English after a relatively big jump achieved by -LRB- Collins 2002 -RRB- we have seen two significant improvements -LRB- Toutanova et al. 2003 -RRB- and -LRB- Shen et al. 2007 -RRB- pushed the results by a significant amount each time .1 1In our final comparison we have also included the results of -LRB- Gimenez and M`arquez 2004 -RRB- because it has surpassed -LRB- Collins 2002 -RRB- as well and we have used this tagger in the data preparation phase	nn_phase_preparation nn_phase_data det_phase_the det_tagger_this prep_in_used_phase dobj_used_tagger aux_used_have nsubj_used_we advmod_well_as amod_Collins_2002 conj_and_surpassed_used advmod_surpassed_well dep_surpassed_Collins aux_surpassed_has nsubj_surpassed_it mark_surpassed_because advcl_Gimenez_used advcl_Gimenez_surpassed dep_Gimenez_2004 conj_and_Gimenez_M`arquez prep_of_results_M`arquez prep_of_results_Gimenez det_results_the dobj_included_results advmod_included_also aux_included_have nsubj_included_we amod_comparison_final poss_comparison_our dep_1In_comparison num_1In_.1 dep_time_1In det_time_each tmod_amount_time amod_amount_significant det_amount_a det_results_the parataxis_pushed_included prep_by_pushed_amount dobj_pushed_results nsubj_pushed_Shen nsubj_pushed_improvements amod_Shen_2007 dep_Shen_al. nn_Shen_et amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et conj_and_improvements_Shen dep_improvements_Toutanova amod_improvements_significant num_improvements_two dep_seen_pushed aux_seen_have nsubj_seen_we prep_after_seen_jump prep_for_seen_English amod_Collins_2002 agent_achieved_Collins vmod_jump_achieved amod_jump_big det_jump_a advmod_big_relatively
E09-1087	W02-1001	o	The supervised training described in -LRB- Collins 2002 -RRB- uses manually annotated data for the estimation of the weight coefficients	nn_coefficients_weight det_coefficients_the prep_of_estimation_coefficients det_estimation_the amod_data_annotated advmod_annotated_manually prep_for_uses_estimation dobj_uses_data nsubj_uses_Collins mark_uses_in amod_Collins_2002 advcl_described_uses vmod_training_described amod_training_supervised det_training_The dep_``_training
E09-1087	W02-1001	o	3 The data 3.1 The supervised data For English we use the same data division of Penn Treebank -LRB- PTB -RRB- parsed section -LRB- Marcus et al. 1994 -RRB- as all of -LRB- Collins 2002 -RRB- -LRB- Toutanova et al. 2003 -RRB- -LRB- Gimenez and M`arquez 2004 -RRB- and -LRB- Shen et al. 2007 -RRB- do for details see Table 1	num_Table_1 dobj_see_Table dep_do_see prep_for_do_details nsubj_do_division amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_Gimenez_2004 conj_and_Gimenez_M`arquez amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et conj_and_Collins_Shen appos_Collins_M`arquez appos_Collins_Gimenez appos_Collins_Toutanova amod_Collins_2002 prep_of_all_Shen prep_of_all_Collins dep_Marcus_1994 dep_Marcus_al. nn_Marcus_et prep_as_section_all appos_section_Marcus amod_section_parsed appos_Treebank_PTB nn_Treebank_Penn dep_division_section prep_of_division_Treebank nn_division_data amod_division_same det_division_the parataxis_use_do nsubj_use_we dep_use_data prep_for_data_English amod_data_supervised det_data_The num_data_3.1 nn_data_data det_data_The num_data_3
H05-1010	W02-1001	o	Another way of doing the parameter estimation for this matching task would have been to use an averaged perceptron method as in Collins -LRB- 2002 -RRB-	appos_Collins_2002 pobj_in_Collins pcomp_as_in nn_method_perceptron amod_method_averaged det_method_an dobj_use_method aux_use_to prep_been_as xcomp_been_use aux_been_have aux_been_would nsubj_been_way amod_task_matching det_task_this nn_estimation_parameter det_estimation_the prep_for_doing_task dobj_doing_estimation prepc_of_way_doing det_way_Another
H05-1010	W02-1001	o	At this point one can imagine estimating a linear matching model in multiple ways including using conditional likelihood estimation an averaged perceptron update -LRB- see which matchings are proposed and adjust the weights according to the dierence between the guessed and target structures -LRB- Collins 2002 -RRB- -RRB- or in large-margin fashion	amod_fashion_large-margin pobj_in_fashion dep_Collins_2002 dep_structures_Collins amod_structures_target amod_structures_guessed det_structures_the conj_and_guessed_target prep_between_dierence_structures det_dierence_the det_weights_the dobj_adjust_weights nsubj_adjust_matchings pobj_proposed_dierence prepc_according_to_proposed_to conj_and_proposed_adjust auxpass_proposed_are nsubjpass_proposed_matchings det_matchings_which ccomp_see_adjust ccomp_see_proposed conj_or_update_in parataxis_update_see nsubj_update_perceptron amod_perceptron_averaged det_perceptron_an nn_estimation_likelihood amod_estimation_conditional dobj_using_estimation rcmod_ways_in rcmod_ways_update prepc_including_ways_using amod_ways_multiple nn_model_matching amod_model_linear det_model_a prep_in_estimating_ways dobj_estimating_model xcomp_imagine_estimating aux_imagine_can nsubj_imagine_one prep_at_imagine_point det_point_this
H05-1011	W02-1001	o	4 Parameter Optimization We optimize the feature weights using a modified version of averaged perceptron learning as described by Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_by_described_Collins mark_described_as nn_learning_perceptron advcl_averaged_described dobj_averaged_learning prepc_of_version_averaged amod_version_modified det_version_a dobj_using_version nn_weights_feature det_weights_the xcomp_optimize_using dobj_optimize_weights nsubj_optimize_We rcmod_Optimization_optimize nn_Optimization_Parameter num_Optimization_4
H05-1011	W02-1001	o	We optimize the model weights using a modified version of averaged perceptron learning as described by Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_by_described_Collins mark_described_as nn_learning_perceptron advcl_averaged_described dobj_averaged_learning prepc_of_version_averaged amod_version_modified det_version_a dobj_using_version nn_weights_model det_weights_the xcomp_optimize_using dobj_optimize_weights nsubj_optimize_We
H05-1027	W02-1001	o	We also compared the MSR algorithm to two of the state-of-the-art discriminative training methods Boosting in Row 3 is an implementation of the improved algorithm for the boosting loss function proposed in -LRB- Collins 2000 -RRB- and Perceptron in Row 4 is an implementation of the averaged perceptron algorithm described in -LRB- Collins 2002 -RRB-	num_Collins_2002 dep_in_Collins prep_described_in vmod_algorithm_described nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the prep_of_implementation_algorithm det_implementation_an cop_implementation_is nsubj_implementation_implementation num_Row_4 prep_in_Perceptron_Row conj_and_Collins_Perceptron num_Collins_2000 prep_in_proposed_Perceptron prep_in_proposed_Collins vmod_function_proposed nn_function_loss amod_function_boosting det_function_the prep_for_algorithm_function amod_algorithm_improved det_algorithm_the prep_of_implementation_algorithm det_implementation_an cop_implementation_is nsubj_implementation_Boosting num_Row_3 prep_in_Boosting_Row nn_methods_training amod_methods_discriminative amod_methods_state-of-the-art det_methods_the prep_of_two_methods nn_algorithm_MSR det_algorithm_the parataxis_compared_implementation prep_to_compared_two dobj_compared_algorithm advmod_compared_also nsubj_compared_We ccomp_``_compared
H05-1027	W02-1001	p	5 Related Work Discriminative models have recently been proved to be more effective than generative models in some NLP tasks e.g. parsing -LRB- Collins 2000 -RRB- POS tagging -LRB- Collins 2002 -RRB- and LM for speech recognition -LRB- Roark et al. 2004 -RRB-	advmod_2004_al. nn_al._et num_Roark_2004 nn_recognition_speech num_Collins_2002 appos_tagging_Collins nn_tagging_POS num_Collins_2000 appos_parsing_Roark prep_for_parsing_recognition conj_and_parsing_LM conj_and_parsing_tagging appos_parsing_Collins dep_e.g._LM dep_e.g._tagging dep_e.g._parsing ccomp_,_e.g. nn_tasks_NLP det_tasks_some amod_models_generative prep_in_effective_tasks prep_than_effective_models advmod_effective_more cop_effective_be aux_effective_to xcomp_proved_effective auxpass_proved_been advmod_proved_recently aux_proved_have nsubjpass_proved_models amod_models_Discriminative rcmod_Work_proved amod_Work_Related num_Work_5 dep_``_Work
H05-1027	W02-1001	o	In the rest of the paper we use the following notation adapted from Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_from_adapted_Collins amod_notation_following det_notation_the vmod_use_adapted dobj_use_notation nsubj_use_we prep_in_use_rest det_paper_the prep_of_rest_paper det_rest_the
H05-1034	W02-1001	o	The four models we compare are a maximum a posteriori -LRB- MAP -RRB- method and three discriminative training methods namely the boosting algorithm -LRB- Collins 2000 -RRB- the average perceptron -LRB- Collins 2002 -RRB- and the minimum sample risk method -LRB- Gao et al. 2005 -RRB-	amod_Gao_2005 dep_Gao_al. nn_Gao_et nn_method_risk nn_method_sample amod_method_minimum det_method_the amod_Collins_2002 appos_perceptron_Collins amod_perceptron_average det_perceptron_the amod_Collins_2000 appos_algorithm_Collins amod_algorithm_boosting det_algorithm_the advmod_algorithm_namely nn_methods_training amod_methods_discriminative num_methods_three dep_method_Gao conj_and_method_method conj_and_method_perceptron conj_and_method_algorithm conj_and_method_methods nn_method_posteriori det_method_a nn_method_maximum det_method_a cop_method_are nsubj_method_models appos_posteriori_MAP nsubj_compare_we rcmod_models_compare num_models_four det_models_The
H05-1034	W02-1001	o	For a detailed description of each algorithm readers are referred to Collins -LRB- 2000 -RRB- for the boosting algorithm Collins -LRB- 2002 -RRB- for perceptron learning and Gao et al.	nn_al._et nn_al._Gao nn_learning_perceptron conj_and_Collins_al. prep_for_Collins_learning appos_Collins_2002 amod_algorithm_boosting det_algorithm_the appos_Collins_2000 parataxis_referred_al. parataxis_referred_Collins prep_for_referred_algorithm prep_to_referred_Collins auxpass_referred_are nsubjpass_referred_readers prep_for_referred_description det_algorithm_each prep_of_description_algorithm amod_description_detailed det_description_a
H05-1034	W02-1001	o	3.1 Definition The following set-up adapted from Collins -LRB- 2002 -RRB- was used for all three discriminative training methods 266 Training data is a set of input-output pairs	amod_pairs_input-output prep_of_set_pairs det_set_a cop_set_is nsubj_set_data nn_data_Training num_data_266 nn_methods_training amod_methods_discriminative num_methods_three det_methods_all prep_for_used_methods auxpass_used_was nsubjpass_used_set-up appos_Collins_2002 prep_from_adapted_Collins vmod_set-up_adapted amod_set-up_following det_set-up_The dep_Definition_set rcmod_Definition_used num_Definition_3.1 dep_``_Definition
H05-1034	W02-1001	p	We used the average perceptron algorithm of Collins -LRB- 2002 -RRB- in our experiments a variation that has been proven to be more effective than the standard algorithm shown in Figure 2	num_Figure_2 prep_in_shown_Figure vmod_algorithm_shown amod_algorithm_standard det_algorithm_the prep_than_effective_algorithm advmod_effective_more cop_effective_be aux_effective_to xcomp_proven_effective auxpass_proven_been aux_proven_has nsubjpass_proven_that rcmod_variation_proven det_variation_a poss_experiments_our appos_Collins_2002 prep_of_algorithm_Collins nn_algorithm_perceptron amod_algorithm_average det_algorithm_the dobj_used_variation prep_in_used_experiments dobj_used_algorithm nsubj_used_We
H05-1058	W02-1001	o	Previous authors have used numerous HMM-based models -LRB- Banko and Moore 2004 Collins 2002 Lee et al. 2000 Thede and Harper 1999 -RRB- and other types of networks including maximum entropy models -LRB- Ratnaparkhi 1996 -RRB- conditional Markov models -LRB- Klein and Manning 2002 McCallum et al. 2000 -RRB- conditional random elds -LRB- CRF -RRB- -LRB- Lafferty et al. 2001 -RRB- and cyclic dependency networks -LRB- Toutanova et al. 2003 -RRB-	amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_networks_Toutanova nn_networks_dependency amod_networks_cyclic amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et dep_elds_Lafferty appos_elds_CRF amod_elds_random amod_elds_conditional num_McCallum_2000 nn_McCallum_al. nn_McCallum_et dep_Klein_McCallum amod_Klein_2002 conj_and_Klein_Manning appos_models_Manning appos_models_Klein nn_models_Markov amod_models_conditional dep_Ratnaparkhi_1996 conj_and_models_networks conj_and_models_elds conj_and_models_models appos_models_Ratnaparkhi nn_models_entropy nn_models_maximum prep_including_networks_networks prep_including_networks_elds prep_including_networks_models prep_including_networks_models prep_of_types_networks amod_types_other conj_and_Thede_1999 conj_and_Thede_Harper num_Lee_2000 nn_Lee_al. nn_Lee_et num_Collins_2002 dep_Banko_1999 dep_Banko_Harper dep_Banko_Thede conj_and_Banko_Lee conj_and_Banko_Collins conj_and_Banko_2004 conj_and_Banko_Moore conj_and_models_types appos_models_Lee appos_models_Collins appos_models_2004 appos_models_Moore appos_models_Banko amod_models_HMM-based amod_models_numerous dobj_used_types dobj_used_models aux_used_have nsubj_used_authors amod_authors_Previous
H05-1059	W02-1001	o	4.1 Part-of-speech tagging experiments We split the Penn Treebank corpus -LRB- Marcus et al. 1994 -RRB- into training development and test sets as in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins pcomp_as_in nn_sets_test conj_and_training_sets conj_and_training_development amod_Marcus_1994 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn det_corpus_the dobj_split_corpus nsubj_split_We prep_experiments_as prep_into_experiments_sets prep_into_experiments_development prep_into_experiments_training dep_experiments_Marcus rcmod_experiments_split amod_experiments_tagging amod_experiments_Part-of-speech num_experiments_4.1 dep_``_experiments
H05-1059	W02-1001	o	Networks -LRB- Toutanova et al. 2003 -RRB- 97.24 Perceptron -LRB- Collins 2002 -RRB- 97.11 SVM -LRB- Gimenez and Marquez 2003 -RRB- 97.05 HMM -LRB- Brants 2000 -RRB- 96.48 Easiest-first 97.10 Full Bidirectional 97.15 Table 3 POS tagging accuracy on the test set -LRB- Sections 22-24 of the WSJ 5462 sentences -RRB-	num_sentences_5462 det_WSJ_the appos_Sections_sentences prep_of_Sections_WSJ num_Sections_22-24 dep_set_Sections nn_set_test det_set_the prep_on_accuracy_set amod_accuracy_tagging nn_accuracy_POS num_Table_3 num_Table_97.15 amod_Table_Bidirectional amod_Table_Full num_Table_97.10 amod_Table_Easiest-first num_Table_96.48 dep_Brants_2000 dep_HMM_Table dep_HMM_Brants num_HMM_97.05 dep_Gimenez_2003 conj_and_Gimenez_Marquez dep_SVM_HMM dep_SVM_Marquez dep_SVM_Gimenez num_SVM_97.11 dep_Collins_2002 dep_Perceptron_accuracy dep_Perceptron_SVM appos_Perceptron_Collins num_Perceptron_97.24 appos_Perceptron_Toutanova nn_Perceptron_Networks amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et
H05-1059	W02-1001	o	A perceptron algorithm gives 97.11 % -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_%_Collins num_%_97.11 dobj_gives_% nsubj_gives_algorithm nn_algorithm_perceptron det_algorithm_A
H05-1059	W02-1001	o	-LRB- Collins 2002 -RRB- and used POS-trigrams as well	advmod_well_as advmod_POS-trigrams_well amod_POS-trigrams_used conj_and_Collins_POS-trigrams amod_Collins_2002 dep_''_POS-trigrams dep_''_Collins
H05-1066	W02-1001	p	This averaging effect has been shown to help overfitting -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_overfitting_Collins dobj_help_overfitting aux_help_to xcomp_shown_help auxpass_shown_been aux_shown_has nsubjpass_shown_effect nn_effect_averaging det_effect_This
H05-1066	W02-1001	o	This model is related to the averaged perceptron algorithm of Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_of_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the prep_to_related_algorithm auxpass_related_is nsubjpass_related_model det_model_This
H05-1099	W02-1001	o	We use the averaged perceptron algorithm as presented in Collins -LRB- 2002 -RRB- to train the parser	det_parser_the dobj_train_parser aux_train_to appos_Collins_2002 prep_in_presented_Collins mark_presented_as nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the vmod_use_train advcl_use_presented dobj_use_algorithm nsubj_use_We
H05-1099	W02-1001	o	task originally introduced in Ramshaw and Marcus -LRB- 1995 -RRB- and also described in -LRB- Collins 2002 Sha and Pereira 2003 -RRB- brackets just base NP constituents5	nn_constituents5_NP amod_constituents5_base advmod_base_just dobj_brackets_constituents5 dep_Sha_2003 conj_and_Sha_Pereira dep_Collins_Pereira dep_Collins_Sha amod_Collins_2002 prep_in_described_Collins advmod_described_also nsubjpass_described_task appos_Marcus_1995 conj_and_Ramshaw_Marcus dep_introduced_brackets conj_and_introduced_described prep_in_introduced_Marcus prep_in_introduced_Ramshaw advmod_introduced_originally nsubjpass_introduced_task
H05-1099	W02-1001	o	We follow Collins -LRB- 2002 -RRB- and Sha and Pereira -LRB- 2003 -RRB- in using section 21 as a heldout set	amod_set_heldout det_set_a num_section_21 prep_as_using_set dobj_using_section appos_Pereira_2003 conj_and_Sha_Pereira conj_and_Collins_Pereira conj_and_Collins_Sha appos_Collins_2002 prepc_in_follow_using dobj_follow_Sha dobj_follow_Collins nsubj_follow_We
H05-1100	W02-1001	o	Prior to running the parsers we trained the POS tagger described in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins prep_described_in vmod_tagger_described nn_tagger_POS det_tagger_the dobj_trained_tagger nsubj_trained_we prepc_prior_to_trained_running det_parsers_the dobj_running_parsers
H05-1100	W02-1001	o	For instance work has been done in Chinese using the Penn Chinese Treebank -LRB- Levy and Manning 2003 Chiang and Bikel 2002 -RRB- in Czech using the Prague Dependency Treebank -LRB- Collins et al. 1999 -RRB- in French using the French Treebank -LRB- Arun and Keller 2005 -RRB- in German using the Negra Treebank -LRB- Dubey 2005 Dubey and Keller 2003 -RRB- and in Spanish using the UAM Spanish Treebank -LRB- Moreno et al. 2000 -RRB-	amod_Moreno_2000 dep_Moreno_al. nn_Moreno_et dep_Treebank_Moreno nn_Treebank_Spanish nn_Treebank_UAM det_Treebank_the dobj_using_Treebank prep_in_using_Spanish num_Dubey_2003 conj_and_Dubey_Keller dep_Dubey_Keller dep_Dubey_Dubey appos_Dubey_2005 conj_and_Treebank_using appos_Treebank_Dubey nn_Treebank_Negra det_Treebank_the dobj_using_using dobj_using_Treebank num_Arun_2005 conj_and_Arun_Keller appos_Treebank_Keller appos_Treebank_Arun amod_Treebank_French det_Treebank_the dep_using_using prep_in_using_German dobj_using_Treebank amod_Collins_1999 dep_Collins_al. nn_Collins_et dep_Treebank_Collins nn_Treebank_Dependency nn_Treebank_Prague det_Treebank_the dobj_using_Treebank dep_Chiang_2002 conj_and_Chiang_Bikel dep_Levy_using prep_in_Levy_French vmod_Levy_using prep_in_Levy_Czech dep_Levy_Bikel dep_Levy_Chiang conj_and_Levy_2003 conj_and_Levy_Manning dep_Treebank_2003 dep_Treebank_Manning dep_Treebank_Levy nn_Treebank_Chinese nn_Treebank_Penn det_Treebank_the dobj_using_Treebank xcomp_done_using prep_in_done_Chinese auxpass_done_been aux_done_has nsubjpass_done_work prep_for_done_instance
H05-1102	W02-1001	p	A = adjoin T = attach C = conjoin G = generate In this paper we use the perceptron-like algorithm proposed in -LRB- Collins 2002 -RRB- which does not suffer from the label bias problem and is fast in training	prep_in_fast_training cop_fast_is nn_problem_bias nn_problem_label det_problem_the prep_from_suffer_problem neg_suffer_not aux_suffer_does nsubj_suffer_which rcmod_Collins_suffer amod_Collins_2002 prep_in_proposed_Collins vmod_algorithm_proposed amod_algorithm_perceptron-like det_algorithm_the dobj_use_algorithm nsubj_use_we det_paper_this prep_in_generate_paper nsubj_generate_G dep_generate_conjoin dep_generate_= nsubj_generate_C amod_G_= conj_and_attach_fast parataxis_attach_use parataxis_attach_generate nsubj_attach_T dep_attach_adjoin dep_attach_= nsubj_attach_A amod_T_=
H05-1124	W02-1001	o	We compare our methods with both the averaged perceptron -LRB- Collins 2002 -RRB- and conditional random fields -LRB- Lafferty et al. 2001 -RRB- using identical predicate sets	nn_sets_predicate amod_sets_identical dobj_using_sets amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et amod_fields_random amod_fields_conditional dep_Collins_2002 conj_and_perceptron_fields appos_perceptron_Collins vmod_averaged_using dep_averaged_Lafferty dobj_averaged_fields dobj_averaged_perceptron vmod_the_averaged dep_both_the dep_with_both poss_methods_our prep_compare_with dobj_compare_methods nsubj_compare_We
I08-2078	W02-1001	p	A pioneer work in online training is the perceptron-like algorithm used in training a hidden Markov model -LRB- HMM -RRB- -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_model_HMM nn_model_Markov amod_model_hidden det_model_a dobj_training_model prepc_in_used_training dep_algorithm_Collins vmod_algorithm_used amod_algorithm_perceptron-like det_algorithm_the cop_algorithm_is nsubj_algorithm_work amod_training_online prep_in_work_training nn_work_pioneer det_work_A
I08-4025	W02-1001	o	The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm -LRB- Collins 2002a -RRB-	appos_Collins_2002a dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_an dobj_train_algorithm aux_train_to dep_train_order mark_train_in vmod_list_train amod_list_N-best det_list_the prep_with_compared_list auxpass_compared_be advmod_compared_now aux_compared_can nsubjpass_compared_segmentation amod_segmentation_true det_segmentation_The
I08-4025	W02-1001	o	However due to the computational issues with the voted perceptron the averaged perceptron algorithm -LRB- Collins 2002a -RRB- is used instead	advmod_used_instead auxpass_used_is nsubjpass_used_algorithm appos_Collins_2002a dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the prep_due_to_algorithm_issues advmod_algorithm_However amod_perceptron_voted det_perceptron_the prep_with_issues_perceptron amod_issues_computational det_issues_the
I08-4025	W02-1001	p	To reduce the time complexity we adapted the lazy update proposed in -LRB- Collins 2002b -RRB- which was also used in -LRB- Zhang and Clark 2007 -RRB-	num_Zhang_2007 conj_and_Zhang_Clark dep_in_Clark dep_in_Zhang prep_used_in advmod_used_also auxpass_used_was nsubjpass_used_which rcmod_Collins_used dep_Collins_2002b prep_in_proposed_Collins dep_update_proposed dep_lazy_update det_lazy_the dobj_adapted_lazy nsubj_adapted_we advcl_adapted_reduce nn_complexity_time det_complexity_the dobj_reduce_complexity aux_reduce_To
J05-1003	W02-1001	o	However there has recently been much work drawing connections between the two methods -LRB- Friedman Hastie and Tibshirani 2000 Lafferty 1999 Duffy and Helmbold 1999 Mason Bartlett and Baxter 1999 Lebanon and Lafferty 2001 Collins Schapire and Singer 2002 -RRB- in this section we review this work	det_work_this dobj_review_work nsubj_review_we prep_in_review_section det_section_this num_Singer_2002 conj_and_Collins_Singer conj_and_Collins_Schapire num_Lafferty_2001 conj_and_Lebanon_Lafferty num_Baxter_1999 conj_and_Mason_Baxter conj_and_Mason_Bartlett amod_1999_Helmbold conj_and_Duffy_1999 num_Lafferty_1999 num_Tibshirani_2000 dep_Friedman_Singer dep_Friedman_Schapire dep_Friedman_Collins conj_and_Friedman_Lafferty conj_and_Friedman_Lebanon conj_and_Friedman_Baxter conj_and_Friedman_Bartlett conj_and_Friedman_Mason conj_and_Friedman_1999 conj_and_Friedman_Duffy conj_and_Friedman_Lafferty conj_and_Friedman_Tibshirani conj_and_Friedman_Hastie appos_methods_Lebanon appos_methods_Mason appos_methods_Duffy appos_methods_Lafferty appos_methods_Tibshirani appos_methods_Hastie appos_methods_Friedman num_methods_two det_methods_the prep_between_drawing_methods dobj_drawing_connections parataxis_work_review vmod_work_drawing amod_work_much cop_work_been advmod_work_recently aux_work_has expl_work_there advmod_work_However
J05-1003	W02-1001	o	The central question in learning is how to set the parameters a given the training examples b x 1 y 1 x 2 y 2 x n y n Logistic regression and boosting involve different algorithms and criteria for training the parameters a but recent work -LRB- Friedman Hastie and Tibshirani 2000 Lafferty 1999 Duffy and Helmbold 1999 Mason Bartlett and Baxter 1999 Lebanon and Lafferty 2001 Collins Schapire and Singer 2002 -RRB- has shown that the methods have strong similarities	amod_similarities_strong dobj_have_similarities nsubj_have_methods mark_have_that det_methods_the ccomp_shown_have aux_shown_has nsubj_shown_work num_Singer_2002 conj_and_Collins_Singer conj_and_Collins_Schapire num_Lafferty_2001 conj_and_Lebanon_Lafferty num_Baxter_1999 conj_and_Mason_Baxter conj_and_Mason_Bartlett amod_1999_Helmbold conj_and_Duffy_1999 num_Lafferty_1999 num_Tibshirani_2000 dep_Friedman_Singer dep_Friedman_Schapire dep_Friedman_Collins conj_and_Friedman_Lafferty conj_and_Friedman_Lebanon conj_and_Friedman_Baxter conj_and_Friedman_Bartlett conj_and_Friedman_Mason conj_and_Friedman_1999 conj_and_Friedman_Duffy conj_and_Friedman_Lafferty conj_and_Friedman_Tibshirani conj_and_Friedman_Hastie appos_work_Lebanon appos_work_Mason appos_work_Duffy appos_work_Lafferty appos_work_Tibshirani appos_work_Hastie appos_work_Friedman amod_work_recent conj_but_a_shown det_parameters_the dep_training_shown dep_training_a dobj_training_parameters conj_and_algorithms_criteria amod_algorithms_different prepc_for_involve_training dobj_involve_criteria dobj_involve_algorithms nsubj_involve_a amod_regression_Logistic nn_regression_training det_regression_the nn_n_y appos_n_n pobj_x_n num_y_2 num_y_1 conj_x_x_y conj_x_x_2 appos_x_y num_x_1 nn_x_b dep_examples_y dep_examples_2 dep_examples_x prep_training_x dep_training_examples conj_and_given_boosting pobj_given_regression rcmod_a_boosting rcmod_a_given rcmod_parameters_involve det_parameters_the dobj_set_parameters aux_set_to advmod_set_how ccomp_is_set nsubj_is_question prep_in_question_learning amod_question_central det_question_The ccomp_``_is
J05-1003	W02-1001	o	Collins and Koo Discriminative Reranking for NLP Della Pietra 1996 Della Pietra Della Pietra and Lafferty 1997 -RRB- or conjugate gradient methods -LRB- Malouf 2002 -RRB-	num_Malouf_2002 appos_methods_Malouf nn_methods_gradient nn_methods_conjugate num_Lafferty_1997 nn_Pietra_Della conj_or_Pietra_methods conj_and_Pietra_Lafferty appos_Pietra_Pietra nn_Pietra_Della num_Pietra_1996 nn_Pietra_Della nn_Pietra_NLP nn_Reranking_Discriminative nn_Reranking_Koo dep_Collins_methods dep_Collins_Lafferty dep_Collins_Pietra prep_for_Collins_Pietra conj_and_Collins_Reranking
J05-1003	W02-1001	o	See Collins -LRB- 2002a -RRB- for an application of the boosting approach to named entity recognition and Walker Rambow and Rogati -LRB- 2001 -RRB- for the application of boosting techniques for ranking in the context of natural language generation	nn_generation_language amod_generation_natural prep_of_context_generation det_context_the prep_in_ranking_context prepc_for_techniques_ranking dobj_boosting_techniques prepc_of_application_boosting det_application_the appos_Rogati_2001 prep_for_Walker_application conj_and_Walker_Rogati conj_and_Walker_Rambow nn_recognition_entity amod_recognition_named prep_to_approach_recognition amod_approach_boosting det_approach_the prep_of_application_approach det_application_an prep_for_Collins_application appos_Collins_2002a conj_and_See_Rogati conj_and_See_Rambow conj_and_See_Walker dobj_See_Collins ccomp_``_Walker ccomp_``_See
J05-1003	W02-1001	o	Appendix B gives a sketch of one such approach which is based on results from Collins Schapire and Singer -LRB- 2002 -RRB-	appos_Singer_2002 conj_and_Collins_Singer conj_and_Collins_Schapire prep_from_results_Singer prep_from_results_Schapire prep_from_results_Collins prep_on_based_results auxpass_based_is nsubjpass_based_which rcmod_approach_based amod_approach_such num_approach_one prep_of_sketch_approach det_sketch_a dobj_gives_sketch nsubj_gives_B nn_B_Appendix
J05-1003	W02-1001	o	In particular previous work -LRB- Ratnaparkhi Roukos and Ward 1994 Abney 1997 Della Pietra Della Pietra and Lafferty 1997 Johnson et al. 1999 Riezler et al. 2002 -RRB- has investigated the use of Markov random fields -LRB- MRFs -RRB- or log-linear models as probabilistic models with global features for parsing and other NLP tasks	nn_tasks_NLP amod_tasks_other conj_and_parsing_tasks prep_for_features_tasks prep_for_features_parsing amod_features_global prep_with_models_features amod_models_probabilistic amod_models_log-linear conj_or_fields_models appos_fields_MRFs amod_fields_random nn_fields_Markov prep_as_use_models prep_of_use_models prep_of_use_fields det_use_the dobj_investigated_use aux_investigated_has nsubj_investigated_work prep_in_investigated_particular dep_al._2002 nn_al._et nn_al._Riezler dep_al._1999 nn_al._et nn_al._Johnson num_Lafferty_1997 nn_Pietra_Della conj_and_Pietra_Lafferty conj_and_Pietra_Pietra nn_Pietra_Della num_Abney_1997 num_Ward_1994 dep_Ratnaparkhi_al. conj_and_Ratnaparkhi_al. conj_and_Ratnaparkhi_Lafferty conj_and_Ratnaparkhi_Pietra conj_and_Ratnaparkhi_Pietra conj_and_Ratnaparkhi_Abney conj_and_Ratnaparkhi_Ward conj_and_Ratnaparkhi_Roukos appos_work_al. appos_work_Pietra appos_work_Abney appos_work_Ward appos_work_Roukos appos_work_Ratnaparkhi amod_work_previous
J05-1003	W02-1001	o	-LRB- 2002 -RRB- do not use a feature selection technique employing instead an objective function which includes a Table 4 Values of Savings -LRB- a b -RRB- for various values of a b. ab Savings -LRB- a b -RRB- 1100,000 2,692.7 110 48.6 11100 83.5 1011,000 280.0 1,00110,000 1,263.9 10,00150,000 2,920.2 50,001100,000 4,229.8 Collins and Koo Discriminative Reranking for NLP Gaussian prior on the parameter values thereby penalizing parameter values which become too large a C3 arg min a LogLossa X k0 m a 2 k 7 2 k 28 Closed-form updates under iterative scaling are not possible with this objective function instead optimization algorithms such as gradient descent or conjugate gradient methods are used to estimate parameter values	nn_values_parameter dobj_estimate_values aux_estimate_to xcomp_used_estimate auxpass_used_are nsubjpass_used_algorithms nn_methods_gradient nn_methods_conjugate conj_or_descent_methods nn_descent_gradient prep_such_as_algorithms_methods prep_such_as_algorithms_descent nn_algorithms_optimization amod_function_objective det_function_this prep_with_possible_function neg_possible_not cop_possible_are nsubj_possible_m amod_scaling_iterative prep_under_updates_scaling nn_updates_Closed-form num_updates_28 nn_updates_k nn_updates_k num_updates_2 det_updates_a number_2_7 num_k_2 dep_m_updates nn_k0_X nn_k0_LogLossa det_k0_a dep_min_k0 nn_min_arg nn_min_C3 det_min_a advmod_large_too acomp_become_large nsubj_become_which rcmod_values_become nn_values_parameter amod_values_penalizing advmod_penalizing_thereby nn_values_parameter det_values_the nn_Gaussian_NLP nn_Reranking_Discriminative nn_Reranking_Koo rcmod_Collins_used advmod_Collins_instead dep_Collins_possible dep_Collins_min appos_Collins_values prep_on_Collins_values advmod_Collins_prior prep_for_Collins_Gaussian conj_and_Collins_Reranking num_Collins_4,229.8 det_Collins_a dep_4,229.8_50,001100,000 dep_4,229.8_1,263.9 dep_4,229.8_83.5 dep_4,229.8_2,692.7 dep_4,229.8_b dep_50,001100,000_2,920.2 number_2,920.2_10,00150,000 number_1,263.9_1,00110,000 dep_1,263.9_280.0 number_280.0_1011,000 number_83.5_11100 dep_83.5_48.6 number_48.6_110 number_2,692.7_1100,000 dep_Savings_Reranking dep_Savings_Collins nn_Savings_ab nn_Savings_b. det_Savings_a prep_of_values_Savings amod_values_various det_b_a appos_Savings_b prep_for_Values_values prep_of_Values_Savings num_Values_4 nn_Values_Table det_Values_a dobj_includes_Values nsubj_includes_which rcmod_function_includes amod_function_objective det_function_an advmod_function_instead dobj_employing_function nn_technique_selection nn_technique_feature det_technique_a vmod_use_employing dobj_use_technique neg_use_not aux_use_do dep_use_2002
J05-1003	W02-1001	o	Section 3 describes previous work -LRB- Friedman Hastie and Tibshirani 2000 Duffy and Helmbold 1999 Mason Bartlett and Baxter 1999 Lebanon and Lafferty 2001 Collins Schapire and Singer 2002 -RRB- that derives connections between boosting and maximum-entropy models for the simpler case of classification problems this work forms the basis for the reranking methods	nn_methods_reranking det_methods_the prep_for_basis_methods det_basis_the dobj_forms_basis nsubj_forms_work det_work_this nn_problems_classification prep_of_case_problems amod_case_simpler det_case_the prep_for_boosting_case dobj_boosting_models conj_and_boosting_maximum-entropy prepc_between_connections_maximum-entropy prepc_between_connections_boosting dobj_derives_connections nsubj_derives_that num_Singer_2002 conj_and_Collins_Singer conj_and_Collins_Schapire num_Lafferty_2001 conj_and_Lebanon_Lafferty num_Baxter_1999 conj_and_Mason_Baxter conj_and_Mason_Bartlett amod_1999_Helmbold conj_and_Duffy_1999 num_Tibshirani_2000 dep_Friedman_Singer dep_Friedman_Schapire dep_Friedman_Collins conj_and_Friedman_Lafferty conj_and_Friedman_Lebanon conj_and_Friedman_Baxter conj_and_Friedman_Bartlett conj_and_Friedman_Mason conj_and_Friedman_1999 conj_and_Friedman_Duffy conj_and_Friedman_Tibshirani conj_and_Friedman_Hastie rcmod_work_derives appos_work_Lebanon appos_work_Mason appos_work_Duffy appos_work_Tibshirani appos_work_Hastie appos_work_Friedman amod_work_previous parataxis_describes_forms dobj_describes_work nsubj_describes_Section num_Section_3
J05-1003	W02-1001	o	The Gaussian prior -LRB- i.e. the P k a 2 k = 7 2 k penalty -RRB- has been found in practice to be very effective in combating overfitting of the parameters to the training data -LRB- Chen and Rosenfeld 1999 Johnson et al. 1999 Lafferty McCallum and Pereira 2001 Riezler et al. 2002 -RRB-	dep_al._2002 nn_al._et nn_al._Riezler num_Pereira_2001 conj_and_Lafferty_Pereira conj_and_Lafferty_McCallum dep_al._1999 nn_al._et nn_al._Johnson dep_Chen_al. dep_Chen_Pereira dep_Chen_McCallum dep_Chen_Lafferty dep_Chen_al. dep_Chen_1999 conj_and_Chen_Rosenfeld dep_data_Rosenfeld dep_data_Chen nn_data_training det_data_the prep_to_parameters_data det_parameters_the prep_of_overfitting_parameters dobj_combating_overfitting prepc_in_effective_combating advmod_effective_very cop_effective_be aux_effective_to xcomp_found_effective prep_in_found_practice auxpass_found_been aux_found_has nsubjpass_found_k advmod_found_i.e. nn_penalty_k num_penalty_2 num_penalty_7 dep_=_penalty amod_k_= num_k_2 det_k_a dep_k_k nn_k_P det_k_the dep_prior_found amod_prior_Gaussian det_prior_The dep_``_prior
J05-1003	W02-1001	o	See Collins -LRB- 2002a 2002b -RRB- and Collins and Duffy -LRB- 2001 2002 -RRB- for applications of the perceptron algorithm	nn_algorithm_perceptron det_algorithm_the prep_of_applications_algorithm num_2002_2001 appos_Duffy_2002 appos_2002a_2002b conj_and_Collins_Duffy conj_and_Collins_Collins dep_Collins_2002a prep_for_See_applications dobj_See_Duffy dobj_See_Collins dobj_See_Collins ccomp_``_See
J05-1003	W02-1001	o	Collins -LRB- 2002b -RRB- gives convergence proofs for the methods Collins -LRB- 2002a -RRB- directly compares the boosting and perceptron approaches on a named entity task and Collins and Duffy -LRB- 2001 2002 -RRB- use a reranking approach with kernels which allow representations of parse trees or labeled sequences in very-high-dimensional spaces	amod_spaces_very-high-dimensional prep_in_sequences_spaces amod_sequences_labeled nn_trees_parse conj_or_representations_sequences prep_of_representations_trees dobj_allow_sequences dobj_allow_representations nsubj_allow_which rcmod_kernels_allow prep_with_approach_kernels nn_approach_reranking det_approach_a dobj_use_approach nsubj_use_Duffy nsubj_use_Collins num_2002_2001 appos_Duffy_2002 conj_and_Collins_Duffy nn_task_entity amod_task_named det_task_a prep_on_approaches_task amod_approaches_perceptron amod_approaches_boosting det_approaches_the conj_and_boosting_perceptron dobj_compares_approaches advmod_compares_directly nsubj_compares_Collins appos_Collins_2002a det_methods_the nn_proofs_convergence conj_and_gives_use parataxis_gives_compares prep_for_gives_methods dobj_gives_proofs nsubj_gives_Collins appos_Collins_2002b
J05-1003	W02-1001	o	The boosting approach to ranking has been applied to named entity segmentation -LRB- Collins 2002a -RRB- and natural language generation -LRB- Walker Rambow and Rogati 2001 -RRB-	num_Rogati_2001 conj_and_Walker_Rogati conj_and_Walker_Rambow dep_generation_Rogati dep_generation_Rambow dep_generation_Walker nn_generation_language amod_generation_natural nn_2002a_Collins conj_and_segmentation_generation appos_segmentation_2002a nn_segmentation_entity amod_segmentation_named prep_to_applied_generation prep_to_applied_segmentation auxpass_applied_been aux_applied_has nsubjpass_applied_approach prep_to_approach_ranking amod_approach_boosting det_approach_The
J05-1003	W02-1001	o	Until now we have defined BestLossk a to be the minimum of the loss given that the kth feature is updated an optimal amount BestLossk amin d LogLossUpda k d In this section we sketch a different approach based on results from Collins Schapire and Singer -LRB- 2002 -RRB- which leads to an algorithm very similar to that for ExpLoss in Figures 3 and 4	conj_and_3_4 dep_Figures_4 dep_Figures_3 prep_in_ExpLoss_Figures prep_for_that_ExpLoss prep_to_similar_that advmod_similar_very amod_algorithm_similar det_algorithm_an prep_to_leads_algorithm nsubj_leads_which appos_Singer_2002 rcmod_Collins_leads conj_and_Collins_Singer conj_and_Collins_Schapire prep_from_results_Singer prep_from_results_Schapire prep_from_results_Collins amod_approach_different det_approach_a nn_approach_sketch dep_approach_we det_section_this pobj_d_results prepc_based_on_d_on dep_d_approach prep_in_d_section nn_d_BestLossk nn_LogLossUpda_d nn_LogLossUpda_amin conj_BestLossk_k conj_BestLossk_LogLossUpda dep_amount_d amod_amount_optimal det_amount_an dobj_updated_amount auxpass_updated_is nsubjpass_updated_feature mark_updated_that nn_feature_kth det_feature_the pcomp_given_updated prep_loss_given det_loss_the prep_of_minimum_loss det_minimum_the cop_minimum_be aux_minimum_to vmod_a_minimum appos_BestLossk_a dobj_defined_BestLossk aux_defined_have nsubj_defined_we prep_defined_Until pobj_Until_now
J05-1003	W02-1001	p	Results from Collins Schapire and Singer -LRB- 2002 -RRB- show that under these definitions the following guarantee holds LogLossUpda k BestWtk a C20 BestLossk a So it can be seen that the update from a to Upda k BestWtk a is guaranteed to decrease LogLoss by at least W k q C0 W C0 k qC16C17 2 From these results the algorithms in Figures 3 and 4 could be altered to take the revised definitions of W k and W C0 k into account	nn_k_C0 nn_k_W conj_and_k_k nn_k_W prep_of_definitions_k prep_of_definitions_k amod_definitions_revised det_definitions_the prep_into_take_account dobj_take_definitions aux_take_to xcomp_altered_take auxpass_altered_be aux_altered_could nsubjpass_altered_LogLossUpda conj_and_3_4 dep_Figures_4 dep_Figures_3 prep_in_algorithms_Figures det_algorithms_the det_results_these num_qC16C17_2 nn_qC16C17_k nn_qC16C17_C0 nn_qC16C17_W nn_qC16C17_C0 dobj_q_qC16C17 nsubj_q_Upda dep_q_to dep_W_k quantmod_W_at mwe_at_least prep_by_decrease_W dobj_decrease_LogLoss aux_decrease_to xcomp_guaranteed_decrease auxpass_guaranteed_is nsubjpass_guaranteed_a rcmod_Upda_guaranteed conj_Upda_BestWtk conj_Upda_k vmod_a_q prep_from_update_a nsubj_update_the mark_update_that ccomp_seen_update auxpass_seen_be aux_seen_can nsubjpass_seen_it advmod_seen_So det_So_a nn_BestLossk_C20 det_BestLossk_a appos_LogLossUpda_algorithms prep_from_LogLossUpda_results rcmod_LogLossUpda_seen appos_LogLossUpda_BestLossk conj_LogLossUpda_BestWtk conj_LogLossUpda_k parataxis_holds_altered nsubj_holds_guarantee prep_under_holds_definitions mark_holds_that amod_guarantee_following det_guarantee_the det_definitions_these ccomp_show_holds nsubj_show_Results appos_Singer_2002 conj_and_Collins_Singer conj_and_Collins_Schapire prep_from_Results_Singer prep_from_Results_Schapire prep_from_Results_Collins
J05-1003	W02-1001	p	For a full derivation of the modified updates and for quite technical convergence proofs see Collins Schapire and Singer -LRB- 2002 -RRB-	appos_Singer_2002 conj_and_Collins_Singer conj_and_Collins_Schapire dobj_see_Singer dobj_see_Schapire dobj_see_Collins ccomp_see_for ccomp_see_For nn_proofs_convergence amod_proofs_technical advmod_technical_quite pobj_for_proofs amod_updates_modified det_updates_the prep_of_derivation_updates amod_derivation_full det_derivation_a conj_and_For_for pobj_For_derivation
J05-4005	W02-1001	o	The algorithm is similar to the perceptron algorithm described in Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_in_described_Collins vmod_algorithm_described nn_algorithm_perceptron det_algorithm_the prep_to_similar_algorithm cop_similar_is nsubj_similar_algorithm det_algorithm_The
J05-4005	W02-1001	o	The key difference is that instead of using the delta rule of Equation -LRB- 8 -RRB- -LRB- as shown in line 5 of Figure 4 -RRB- Collins -LRB- 2002 -RRB- updates parameters using the rule t +1 d t d + f d -LRB- w R i -RRB- f d -LRB- w i -RRB-	dep_w_i dep_d_w nn_d_f nn_d_d nn_d_d dep_R_i nn_R_w nn_d_f appos_d_R conj_+_d_d nn_d_t nn_d_d num_d_+1 nn_d_t det_rule_the dobj_using_rule vmod_parameters_using nn_parameters_updates nn_parameters_Collins appos_Collins_2002 dep_Figure_d dep_Figure_parameters dep_Figure_4 prep_of_line_Figure num_line_5 prep_in_shown_line mark_shown_as appos_Equation_8 prep_of_rule_Equation nn_rule_delta det_rule_the advcl_using_shown dobj_using_rule pcomp_of_using advmod_of_instead ccomp_,_of dep_is_that nsubj_is_difference amod_difference_key det_difference_The
N03-1028	W02-1001	o	Minor variants support voted perceptron -LRB- Collins 2002 -RRB- and MEMMs -LRB- McCallum et al. 2000 -RRB- with the same ef cient feature encoding	nn_encoding_feature nn_encoding_cient nn_encoding_ef amod_encoding_same det_encoding_the amod_McCallum_2000 dep_McCallum_al. nn_McCallum_et amod_Collins_2002 dep_perceptron_McCallum conj_and_perceptron_MEMMs dep_perceptron_Collins prep_with_voted_encoding dobj_voted_MEMMs dobj_voted_perceptron ccomp_support_voted nsubj_support_variants amod_variants_Minor
N03-1028	W02-1001	o	The published F score for voted perceptron is 93.53 % with a different feature set -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_set_Collins nn_set_feature amod_set_different det_set_a prep_with_%_set num_%_93.53 cop_%_is nsubj_%_perceptron ccomp_voted_% prepc_for_score_voted nn_score_F amod_score_published det_score_The
N03-1028	W02-1001	o	On the machine-learning side it would be interesting to generalize the ideas of large-margin classi cation to sequence models strengthening the results of Collins -LRB- 2002 -RRB- and leading to new optimal training algorithms with stronger guarantees against over tting	pobj_over_tting pcomp_against_over prep_guarantees_against amod_guarantees_stronger prep_with_algorithms_guarantees nn_algorithms_training amod_algorithms_optimal amod_algorithms_new prep_to_leading_algorithms conj_and_Collins_leading appos_Collins_2002 prep_of_results_leading prep_of_results_Collins det_results_the dobj_strengthening_results nn_models_sequence nn_cation_classi amod_cation_large-margin prep_of_ideas_cation det_ideas_the xcomp_generalize_strengthening prep_to_generalize_models dobj_generalize_ideas aux_generalize_to xcomp_interesting_generalize cop_interesting_be aux_interesting_would nsubj_interesting_it prep_on_interesting_side amod_side_machine-learning det_side_the
N03-1028	W02-1001	n	However work in that direction has so far addressed only parse reranking -LRB- Collins and Duffy 2002 Riezler et al. 2002 -RRB-	num_Riezler_2002 nn_Riezler_al. nn_Riezler_et dep_Collins_Riezler num_Collins_2002 conj_and_Collins_Duffy appos_reranking_Duffy appos_reranking_Collins dobj_parse_reranking advmod_parse_only dep_addressed_parse advmod_addressed_far advmod_far_so dep_has_addressed nsubj_has_work advmod_has_However det_direction_that prep_in_work_direction
N03-1028	W02-1001	o	Full discriminative parser training faces signi cant algorithmic challenges in the relationship between parsing alternatives and feature values -LRB- Geman and Johnson 2002 -RRB- and in computing feature expectations	nn_expectations_feature amod_expectations_computing dep_Geman_2002 conj_and_Geman_Johnson dep_values_Johnson dep_values_Geman nn_values_feature conj_and_alternatives_values amod_alternatives_parsing conj_and_relationship_expectations prep_between_relationship_values prep_between_relationship_alternatives det_relationship_the amod_challenges_algorithmic amod_challenges_cant nn_challenges_signi prep_in_faces_expectations prep_in_faces_relationship dobj_faces_challenges nsubj_faces_training nn_training_parser amod_training_discriminative amod_training_Full
N03-1028	W02-1001	n	The generalized perceptron proposed by Collins -LRB- 2002 -RRB- is closely related to CRFs but the best CRF training methods seem to have a slight edge over the generalized perceptron	amod_perceptron_generalized det_perceptron_the amod_edge_slight det_edge_a prep_over_have_perceptron dobj_have_edge aux_have_to xcomp_seem_have nsubj_seem_methods nn_methods_training nn_methods_CRF amod_methods_best det_methods_the conj_but_related_seem prep_to_related_CRFs advmod_related_closely cop_related_is nsubj_related_perceptron appos_Collins_2002 agent_proposed_Collins vmod_perceptron_proposed amod_perceptron_generalized det_perceptron_The
N03-1028	W02-1001	o	We compare those algorithms to generalized iterative scaling -LRB- GIS -RRB- -LRB- Darroch and Ratcliff 1972 -RRB- non-preconditioned CG and voted perceptron training -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_training_Collins nn_training_perceptron dobj_voted_training nsubj_voted_We amod_CG_non-preconditioned dep_Darroch_1972 conj_and_Darroch_Ratcliff appos_scaling_CG appos_scaling_Ratcliff appos_scaling_Darroch appos_scaling_GIS amod_scaling_iterative amod_scaling_generalized det_algorithms_those conj_and_compare_voted prep_to_compare_scaling dobj_compare_algorithms nsubj_compare_We
N03-1028	W02-1001	o	3.3 Voted Perceptron Unlike other methods discussed so far voted perceptron training -LRB- Collins 2002 -RRB- attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model	amod_model_current det_model_the det_instance_that prep_of_labeling_instance amod_labeling_best-scoring det_labeling_the prep_for_vector_labeling nn_vector_feature amod_vector_same det_vector_the nn_instance_training det_instance_a conj_and_vector_vector prep_for_vector_instance nn_vector_feature amod_vector_global det_vector_the prep_between_difference_vector prep_between_difference_vector det_difference_the pobj_minimize_model prepc_according_to_minimize_to dobj_minimize_difference aux_minimize_to xcomp_attempts_minimize nsubj_attempts_training dep_Collins_2002 appos_training_Collins nn_training_perceptron ccomp_voted_attempts nsubj_voted_3.3 advmod_far_so advmod_discussed_far vmod_methods_discussed amod_methods_other prep_unlike_Perceptron_methods dobj_Voted_Perceptron vmod_3.3_Voted ccomp_``_voted
N03-1028	W02-1001	o	Instead of taking just the nal weight vector the voted perceptron algorithm takes the average of the t. Collins -LRB- 2002 -RRB- reported and we con rmed that this averaging reduces overtting considerably	advmod_overtting_considerably xcomp_reduces_overtting dep_reduces_averaging nsubj_reduces_this mark_reduces_that ccomp_rmed_reduces acomp_con_rmed nsubj_con_we conj_and_reported_con appos_Collins_2002 nn_Collins_t. det_Collins_the prep_of_average_Collins det_average_the dep_takes_con dep_takes_reported dobj_takes_average nsubj_takes_algorithm prepc_instead_of_takes_taking nn_algorithm_perceptron amod_algorithm_voted det_algorithm_the nn_vector_weight amod_vector_nal det_vector_the advmod_vector_just dobj_taking_vector
N03-1033	W02-1001	o	The Tagger Support cutoff Accuracy Collins -LRB- 2002 -RRB- 0 96.60 % 5 96.72 % Model 3W + TAGS variant 1 96.97 % 5 96.93 % Table 6 Effect of changing common word feature cutoffs -LRB- features with support cutoff are excluded from the model -RRB-	det_model_the prep_from_excluded_model auxpass_excluded_are nsubjpass_excluded_features nsubjpass_excluded_cutoffs nn_cutoff_support prep_with_features_cutoff nn_cutoffs_feature nn_cutoffs_word amod_cutoffs_common ccomp_changing_excluded prepc_of_Effect_changing num_Table_6 dep_%_Effect dep_%_Table num_%_96.93 number_96.93_5 dep_%_% dep_96.97_% dep_1_96.97 dep_variant_1 amod_TAGS_variant conj_+_3W_TAGS nn_3W_Model dep_3W_% dep_3W_% dep_%_96.72 number_96.72_5 number_%_96.60 number_%_0 dep_%_2002 dep_Collins_TAGS dep_Collins_3W nn_Collins_Accuracy nn_Collins_cutoff nn_Collins_Support nn_Collins_Tagger det_Collins_The
N03-1033	W02-1001	n	At any rate regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger Ratnaparkhi -LRB- 1996 -RRB- Toutanova and Manning -LRB- 2000 -RRB- and Collins -LRB- 2002 -RRB- all present unregularized models	amod_models_unregularized amod_models_present det_models_all nn_models_Collins appos_Collins_2002 appos_Manning_2000 conj_and_Ratnaparkhi_Manning conj_and_Ratnaparkhi_Toutanova appos_Ratnaparkhi_1996 dep_tagger_Manning dep_tagger_Toutanova dep_tagger_Ratnaparkhi amod_tagger_part-of-speech nn_tagger_quality amod_tagger_high det_tagger_a dobj_producing_tagger prepc_of_problem_producing det_problem_the conj_and_applied_models prep_to_applied_problem auxpass_applied_been advmod_applied_previously neg_applied_not aux_applied_have nsubjpass_applied_models prep_at_applied_rate nn_models_loglinear amod_models_conditional amod_models_regularized det_rate_any
N03-1033	W02-1001	o	Indeed the result of Collins -LRB- 2002 -RRB- that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized	auxpass_regularized_are nsubjpass_regularized_weights advmod_regularized_once nn_model_entropy nn_model_maximum det_model_the prep_of_weights_model det_weights_the ccomp_undone_regularized auxpass_undone_is nsubjpass_undone_harms nsubjpass_undone_result advmod_undone_Indeed nn_model_entropy nn_model_maximum det_model_a dobj_harms_model nn_model_perceptron amod_model_voted det_model_a dobj_helps_model nsubj_helps_that nn_features_support amod_features_low prep_including_that_features rcmod_Collins_helps appos_Collins_2002 conj_but_result_harms prep_of_result_Collins det_result_the
N03-1033	W02-1001	n	Whereas Ratnaparkhi -LRB- 1996 -RRB- used feature support cutoffs and early stopping to stop overfitting of the model and Collins -LRB- 2002 -RRB- contends that including low support features harms a maximum entropy model our results show that low support features are useful in a regularized maximum entropy model	nn_model_entropy nn_model_maximum amod_model_regularized det_model_a prep_in_useful_model cop_useful_are nsubj_useful_features mark_useful_that nn_features_support amod_features_low ccomp_show_useful nsubj_show_results tmod_show_model poss_results_our nn_model_entropy nn_model_maximum det_model_a ccomp_harms_show prep_including_harms_features mark_harms_that nn_features_support amod_features_low ccomp_contends_harms nsubj_contends_Collins nsubj_contends_early nsubj_contends_cutoffs mark_contends_Whereas appos_Collins_2002 det_model_the prep_of_overfitting_model dobj_stop_overfitting aux_stop_to xcomp_stopping_stop vmod_early_stopping conj_and_cutoffs_Collins conj_and_cutoffs_early nn_cutoffs_support nn_cutoffs_feature amod_cutoffs_used nn_cutoffs_Ratnaparkhi appos_Ratnaparkhi_1996 advcl_``_contends
N03-1033	W02-1001	o	Table 6 contrasts our results with those from Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_from_those_Collins poss_results_our prep_with_contrasts_those dobj_contrasts_results nsubj_contrasts_Table num_Table_6
N03-1033	W02-1001	p	Indeed as for the voted perceptron of Collins -LRB- 2002 -RRB- we can get performance gains by reducing the support threshold for features to be included in the model	det_model_the prep_in_included_model auxpass_included_be aux_included_to vmod_features_included prep_for_threshold_features nn_threshold_support det_threshold_the dobj_reducing_threshold nn_gains_performance prepc_by_get_reducing dobj_get_gains aux_get_can nsubj_get_we pobj_get_perceptron prepc_as_for_get_for advmod_get_Indeed appos_Collins_2002 prep_of_perceptron_Collins amod_perceptron_voted det_perceptron_the
N03-1033	W02-1001	o	This is the best automatically learned part-of-speech tagging result known to us representing an error reduction of 4.4 % on the model presented in Collins -LRB- 2002 -RRB- using the same data splits and a larger error reduction of 12.1 % from the more similar best previous loglinear model in Toutanova and Manning -LRB- 2000 -RRB-	appos_Manning_2000 conj_and_Toutanova_Manning prep_in_model_Manning prep_in_model_Toutanova amod_model_loglinear amod_model_previous amod_model_best amod_model_similar det_model_the advmod_similar_more num_%_12.1 prep_from_reduction_model prep_of_reduction_% nn_reduction_error amod_reduction_larger det_reduction_a conj_and_splits_reduction csubj_splits_using amod_data_same det_data_the dobj_using_data appos_Collins_2002 prep_in_presented_Collins vmod_model_presented det_model_the num_%_4.4 prep_on_reduction_model prep_of_reduction_% nn_reduction_error det_reduction_an dobj_representing_reduction dep_known_reduction dep_known_splits xcomp_known_representing prep_to_known_us prep_result_known nsubj_result_best amod_tagging_part-of-speech dobj_learned_tagging advmod_learned_automatically vmod_best_learned det_best_the cop_best_is nsubj_best_This
N03-1033	W02-1001	o	We extracted tagged sentences from the parse trees .5 We split the data into training development and test sets as in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins pcomp_as_in prep_sets_as nn_sets_test conj_and_training_sets conj_and_training_development det_data_the prep_into_split_sets prep_into_split_development prep_into_split_training dobj_split_data nsubj_split_We num_trees_.5 nn_trees_parse det_trees_the prep_from_sentences_trees amod_sentences_tagged dep_extracted_split dobj_extracted_sentences nsubj_extracted_We ccomp_``_extracted
N04-4006	W02-1001	o	2.2 Perceptron algorithm Our discriminative n-gram model training approach uses the perceptron algorithm as presented in -LRB- Roark et al. 2004 -RRB- which follows the general approach presented in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins prep_presented_in vmod_approach_presented amod_approach_general det_approach_the dobj_follows_approach nsubj_follows_which rcmod_Roark_follows amod_Roark_2004 dep_Roark_al. nn_Roark_et prep_in_presented_Roark mark_presented_as nn_algorithm_perceptron det_algorithm_the advcl_uses_presented dobj_uses_algorithm nsubj_uses_approach nn_approach_training nn_approach_model nn_approach_n-gram amod_approach_discriminative poss_approach_Our rcmod_algorithm_uses nn_algorithm_Perceptron num_algorithm_2.2
N04-4006	W02-1001	o	As suggested in -LRB- Collins 2002 -RRB- we use the averaged perceptron when applying the model to held-out or test data	amod_data_test amod_data_held-out conj_or_held-out_test det_model_the prep_to_applying_data dobj_applying_model advmod_applying_when amod_perceptron_averaged det_perceptron_the advcl_use_applying dobj_use_perceptron nsubj_use_we advcl_use_suggested amod_Collins_2002 prep_in_suggested_Collins mark_suggested_As
N06-1015	W02-1001	n	Moreover the parameters of the model must be estimated using averaged perceptron training -LRB- Collins 2002 -RRB- which can be unstable	cop_unstable_be aux_unstable_can nsubj_unstable_which dep_Collins_2002 rcmod_training_unstable appos_training_Collins nn_training_perceptron dobj_averaged_training dep_using_averaged xcomp_estimated_using auxpass_estimated_be aux_estimated_must nsubjpass_estimated_parameters advmod_estimated_Moreover det_model_the prep_of_parameters_model det_parameters_the ccomp_``_estimated
N06-1021	W02-1001	p	The averaged perceptron -LRB- Collins 2002 -RRB- is a variant which averages the w across all iterations it has demonstrated good generalization especially with data that is not linearly separable as in many natural language processing problems	nn_problems_processing nn_problems_language amod_problems_natural amod_problems_many pobj_in_problems pcomp_as_in prep_separable_as advmod_separable_linearly neg_separable_not cop_separable_is nsubj_separable_that rcmod_data_separable amod_generalization_good prep_with_demonstrated_data advmod_demonstrated_especially dobj_demonstrated_generalization aux_demonstrated_has nsubj_demonstrated_it det_iterations_all prep_across_w_iterations det_w_the parataxis_averages_demonstrated dobj_averages_w det_averages_which amod_averages_variant det_averages_a cop_averages_is csubj_averages_averaged nsubj_averages_The amod_Collins_2002 appos_perceptron_Collins dobj_averaged_perceptron
N06-1021	W02-1001	o	Also we chose to average each individual perceptron -LRB- Collins 2002 -RRB- prior to Bayesian averaging	amod_averaging_Bayesian dep_Collins_2002 appos_perceptron_Collins amod_perceptron_individual det_perceptron_each amod_perceptron_average prep_prior_to_chose_averaging prep_to_chose_perceptron nsubj_chose_we advmod_chose_Also
N06-2010	W02-1001	o	The weights are then averaged across all iterations of the perceptron as in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins pcomp_as_in det_perceptron_the prep_of_iterations_perceptron det_iterations_all prep_averaged_as prep_across_averaged_iterations advmod_averaged_then auxpass_averaged_are nsubjpass_averaged_weights det_weights_The
N06-2024	W02-1001	o	MUC-6 347 30 204,071 6.8 Enron 833143204 ,423 3.0 Mgmt-Groups 631128104 ,662 3.7 Table 1 Summary of the corpora used in the experiments We used an implementation of Collins votedpercepton method for discriminatively training HMMs -LRB- henceforth VP-HMM -RRB- -LRB- Collins 2002 -RRB- as well as CRF -LRB- Lafferty et al. 2001 -RRB- to learn a NER	det_NER_a dobj_learn_NER aux_learn_to amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et vmod_CRF_learn appos_CRF_Lafferty dep_Collins_2002 appos_henceforth_VP-HMM appos_HMMs_Collins dep_HMMs_henceforth conj_and_training_CRF dobj_training_HMMs advmod_training_discriminatively nn_method_votedpercepton nn_method_Collins prep_of_implementation_method det_implementation_an prep_for_used_CRF prep_for_used_training dobj_used_implementation nsubj_used_We rcmod_experiments_used det_experiments_the prep_in_used_experiments vmod_corpora_used det_corpora_the prep_of_Summary_corpora num_Table_1 num_Table_3.7 num_Table_,662 num_Table_631128104 dep_Mgmt-Groups_Table dep_3.0_Mgmt-Groups dep_,423_3.0 number_,423_833143204 num_Enron_,423 dep_6.8_Enron dep_204,071_6.8 dep_30_204,071 number_30_347 dep_MUC-6_Summary amod_MUC-6_30
N07-1009	W02-1001	o	Previous approaches for training CRFs have either -LRB- 1 -RRB- opted for a training method that no longer maximizes the likelihood -LRB- e.g. McCallum and Wellner -LRB- 2004 -RRB- Roth and Yih -LRB- 2005 -RRB- -RRB- 1 or -LRB- 2 -RRB- opted for a 1 Both McCallum and Wellner -LRB- 2004 -RRB- and Roth and Yih -LRB- 2005 -RRB- used the voted perceptron algorithm -LRB- Collins 2002 -RRB- to train intractable CRFs	amod_CRFs_intractable dobj_train_CRFs aux_train_to amod_Collins_2002 dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_voted det_algorithm_the vmod_used_train dobj_used_algorithm nsubj_used_Yih nsubj_used_Roth nsubj_used_Wellner nsubj_used_McCallum appos_Yih_2005 dep_Wellner_2004 conj_and_McCallum_Yih conj_and_McCallum_Roth conj_and_McCallum_Wellner preconj_McCallum_Both rcmod_1_used det_1_a prep_for_opted_1 dep_opted_2 conj_or_1_opted dep_Yih_2005 appos_Wellner_2004 dep_McCallum_opted dep_McCallum_1 conj_and_McCallum_Yih conj_and_McCallum_Roth conj_and_McCallum_Wellner dep_McCallum_e.g. det_likelihood_the dobj_maximizes_likelihood advmod_maximizes_longer nsubj_maximizes_that neg_longer_no rcmod_method_maximizes nn_method_training det_method_a dep_opted_Yih dep_opted_Roth dep_opted_Wellner dep_opted_McCallum prep_for_opted_method preconj_opted_either dep_either_1 dep_have_opted nsubj_have_approaches nn_CRFs_training prep_for_approaches_CRFs amod_approaches_Previous
N07-1009	W02-1001	o	Training via the voted perceptron algorithm -LRB- Collins 2002 -RRB- or using a max-margin criterion also correspond to the first option -LRB- e.g. McCallum and Wellner -LRB- 2004 -RRB- Finley and Joachims -LRB- 2005 -RRB- -RRB-	appos_Joachims_2005 appos_Wellner_2004 conj_and_McCallum_Joachims conj_and_McCallum_Finley conj_and_McCallum_Wellner dep_McCallum_e.g. dep_option_Joachims dep_option_Finley dep_option_Wellner dep_option_McCallum amod_option_first det_option_the prep_to_correspond_option advmod_correspond_also nsubj_correspond_using nsubj_correspond_Training amod_criterion_max-margin det_criterion_a dobj_using_criterion dep_Collins_2002 appos_algorithm_Collins nn_algorithm_perceptron amod_algorithm_voted det_algorithm_the conj_or_Training_using prep_via_Training_algorithm
N07-1049	W02-1001	o	For regularization purposes we adopt an average perceptron -LRB- Collins 2002 -RRB- which returns for each y y = 1T summationtextTt = 1 ty the average of all weight vectors ty posited during training	prep_during_posited_training vmod_ty_posited dep_vectors_ty nn_vectors_weight det_vectors_all prep_of_average_vectors det_average_the num_ty_1 dep_=_ty appos_summationtextTt_average amod_summationtextTt_= nn_summationtextTt_1T dep_=_summationtextTt amod_y_= det_y_each prep_for_returns_y nsubj_returns_which amod_Collins_2002 appos_perceptron_y rcmod_perceptron_returns appos_perceptron_Collins amod_perceptron_average det_perceptron_an dobj_adopt_perceptron nsubj_adopt_we prep_for_adopt_purposes nn_purposes_regularization
N09-1069	W02-1001	o	One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging -LRB- Collins 2002 -RRB- MIRA for dependency parsing -LRB- McDonald et al. 2005 -RRB- exponentiated gradient algorithms -LRB- Collins et al. 2008 -RRB- stochastic gradient for constituency parsing -LRB- Finkel et al. 2008 -RRB- just to name a few	det_few_a dobj_name_few aux_name_to advmod_name_just amod_Finkel_2008 dep_Finkel_al. nn_Finkel_et nn_parsing_constituency dep_gradient_Finkel prep_for_gradient_parsing amod_gradient_stochastic amod_Collins_2008 dep_Collins_al. nn_Collins_et appos_algorithms_Collins nn_algorithms_gradient amod_algorithms_exponentiated amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et nn_parsing_dependency vmod_MIRA_name conj_MIRA_gradient conj_MIRA_algorithms dep_MIRA_McDonald prep_for_MIRA_parsing amod_Collins_2002 dep_tagging_Collins appos_algorithm_MIRA prepc_for_algorithm_tagging nn_algorithm_Perceptron det_algorithm_the dobj_include_algorithm nn_examples_literature nn_examples_NLP amod_examples_supervised det_examples_the prep_in_trend_examples amod_trend_clear det_trend_this dep_sees_include dobj_sees_trend nsubj_sees_One
N09-3016	W02-1001	o	The phoneme prediction and sequence modeling are considered as tagging problems and a Perceptron HMM -LRB- Collins 2002 -RRB- is used to model it	dobj_model_it aux_model_to xcomp_used_model auxpass_used_is amod_Collins_2002 dep_HMM_Collins nn_HMM_Perceptron det_HMM_a conj_and_problems_HMM nn_problems_tagging dep_considered_used prep_as_considered_HMM prep_as_considered_problems auxpass_considered_are nsubjpass_considered_modeling nsubjpass_considered_prediction nn_modeling_sequence conj_and_prediction_modeling nn_prediction_phoneme det_prediction_The
P02-1034	W02-1001	o	We used a feature set which included the current next and previous word the previous two tags various capitalization and other features of the word being tagged -LRB- the full feature set is described in -LRB- Collins 2002a -RRB- -RRB-	nn_2002a_Collins dep_in_2002a prep_described_in auxpass_described_is nsubjpass_described_set nn_set_feature amod_set_full det_set_the auxpass_tagged_being rcmod_word_described vmod_word_tagged det_word_the prep_of_features_word amod_features_other conj_and_capitalization_features amod_capitalization_various dep_tags_features dep_tags_capitalization num_tags_two amod_tags_previous det_tags_the amod_word_previous cc_word_and amod_word_next amod_word_current det_word_the dobj_included_word nsubj_included_which rcmod_set_included nn_set_feature det_set_a dep_used_tags dobj_used_set nsubj_used_We
P02-1034	W02-1001	o	From a theoretical point of view it is difficult to find motivation for the parameter estimation methods used by -LRB- Bod 1998 -RRB- see -LRB- Johnson 2002 -RRB- for discussion	num_Johnson_2002 prep_for_see_discussion dep_see_Johnson num_Bod_1998 agent_used_Bod vmod_methods_used nn_methods_estimation nn_methods_parameter det_methods_the prep_for_motivation_methods dobj_find_motivation aux_find_to dep_difficult_see xcomp_difficult_find cop_difficult_is nsubj_difficult_it prep_from_difficult_point prep_of_point_view amod_point_theoretical det_point_a
P02-1034	W02-1001	o	For related work on the voted perceptron algorithm applied to NLP problems see -LRB- Collins 2002a -RRB- and -LRB- Collins 2002b -RRB-	nn_2002b_Collins nn_2002a_Collins dep_see_2002a nn_problems_NLP conj_and_applied_2002b conj_and_applied_see prep_to_applied_problems prep_for_applied_work nn_algorithm_perceptron amod_algorithm_voted det_algorithm_the prep_on_work_algorithm amod_work_related
P02-1034	W02-1001	o	-LRB- Collins 2002a -RRB- describes experiments on the same named-entity dataset as in this paper but using explicit features rather than kernels	conj_negcc_features_kernels amod_features_explicit dobj_using_kernels dobj_using_features nsubj_using_2002a det_paper_this pobj_in_paper pcomp_as_in nn_dataset_named-entity amod_dataset_same det_dataset_the prep_on_experiments_dataset conj_but_describes_using prep_describes_as dobj_describes_experiments nsubj_describes_2002a nn_2002a_Collins
P02-1034	W02-1001	o	-LRB- Collins 2002b -RRB- describes how the voted perceptron can be used to train maximum-entropy style taggers and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks	amod_tasks_ranking prep_to_applied_tasks nsubj_applied_discussion nn_algorithm_perceptron det_algorithm_the prep_behind_theory_algorithm det_theory_the prep_of_discussion_theory amod_discussion_thorough det_discussion_a advmod_thorough_more ccomp_gives_applied advmod_gives_also nsubj_gives_2002b nn_taggers_style amod_taggers_maximum-entropy dobj_train_taggers aux_train_to xcomp_used_train auxpass_used_be aux_used_can nsubjpass_used_perceptron advmod_used_how amod_perceptron_voted det_perceptron_the conj_and_describes_gives ccomp_describes_used nsubj_describes_2002b nn_2002b_Collins
P02-1062	W02-1001	o	Another attractive property of the voted perceptron is that it can be used with kernels for example the kernels over parse trees described in -LRB- Collins and Duffy 2001 Collins and Duffy 2002 -RRB-	dep_Collins_2002 conj_and_Collins_Duffy num_Duffy_2001 dep_Collins_Duffy dep_Collins_Collins conj_and_Collins_Duffy prep_in_described_Duffy prep_in_described_Collins vmod_trees_described nn_trees_parse prep_over_kernels_trees det_kernels_the dobj_used_kernels prep_for_used_example prep_with_used_kernels auxpass_used_be aux_used_can nsubjpass_used_it mark_used_that ccomp_is_used nsubj_is_property amod_perceptron_voted det_perceptron_the prep_of_property_perceptron amod_property_attractive det_property_Another
P02-1062	W02-1001	o	-LRB- Collins and Duffy 2002 -RRB- describe the voted perceptron applied to the named-entity data in this paper but using kernel-based features rather than the explicit features described in this paper	det_paper_this prep_in_described_paper vmod_features_described amod_features_explicit det_features_the conj_negcc_features_features amod_features_kernel-based dobj_using_features dobj_using_features nsubj_using_Collins det_paper_this prep_in_data_paper nn_data_named-entity det_data_the prep_to_applied_data nsubj_applied_perceptron amod_perceptron_voted det_perceptron_the conj_but_describe_using ccomp_describe_applied nsubj_describe_Duffy nsubj_describe_Collins num_Duffy_2002 conj_and_Collins_Duffy
P02-1062	W02-1001	o	See -LRB- Collins 2002 -RRB- for additional work using perceptron algorithms to train tagging models and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems	amod_problems_ranking prep_to_applied_problems nsubj_applied_description nn_algorithm_perceptron det_algorithm_the dobj_underlying_algorithm vmod_theory_underlying det_theory_the prep_of_description_theory amod_description_thorough det_description_a advmod_thorough_more amod_models_tagging dobj_train_models aux_train_to nn_algorithms_perceptron vmod_using_train dobj_using_algorithms amod_work_additional num_Collins_2002 conj_and_See_applied dep_See_using prep_for_See_work dep_See_Collins ccomp_``_applied ccomp_``_See
P03-1064	W02-1001	o	-LRB- Collins 2002 -RRB- proposed a new algorithm for parameter estimation as an alternate to CRF	prep_to_alternate_CRF det_alternate_an nn_estimation_parameter prep_for_algorithm_estimation amod_algorithm_new det_algorithm_a prep_as_proposed_alternate dobj_proposed_algorithm nsubj_proposed_Collins amod_Collins_2002
P03-1064	W02-1001	p	Many machine learning techniques have been successfully applied to chunking tasks such as Regularized Winnow -LRB- Zhang et al. 2001 -RRB- SVMs -LRB- Kudo and Matsumoto 2001 -RRB- CRFs -LRB- Sha and Pereira 2003 -RRB- Maximum Entropy Model -LRB- Collins 2002 -RRB- Memory Based Learning -LRB- Sang 2002 -RRB- and SNoW -LRB- Munoz et al. 1999 -RRB-	amod_Munoz_1999 dep_Munoz_al. nn_Munoz_et dep_SNoW_Munoz dep_Sang_2002 conj_and_Learning_SNoW dep_Learning_Sang pobj_Based_SNoW pobj_Based_Learning prep_Memory_Based dep_Collins_2002 appos_Model_Collins nn_Model_Entropy nn_Model_Maximum dep_Sha_2003 conj_and_Sha_Pereira appos_CRFs_Pereira appos_CRFs_Sha dep_Kudo_2001 conj_and_Kudo_Matsumoto appos_SVMs_Matsumoto appos_SVMs_Kudo amod_Zhang_2001 dep_Zhang_al. nn_Zhang_et appos_Winnow_Memory appos_Winnow_Model appos_Winnow_CRFs appos_Winnow_SVMs dep_Winnow_Zhang nn_Winnow_Regularized prep_such_as_tasks_Winnow amod_tasks_chunking prep_to_applied_tasks advmod_applied_successfully auxpass_applied_been aux_applied_have nsubjpass_applied_techniques nn_techniques_learning nn_techniques_machine amod_techniques_Many
P04-1007	W02-1001	o	2.1 Global linear models We follow the framework outlined in Collins -LRB- 2002 2004 -RRB-	dep_2002_2004 dep_Collins_2002 prep_in_outlined_Collins vmod_framework_outlined det_framework_the dobj_follow_framework nsubj_follow_We rcmod_models_follow amod_models_linear amod_models_Global num_models_2.1
P04-1007	W02-1001	o	Following Collins -LRB- 2002 -RRB- we used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments	poss_experiments_our nn_examples_test nn_examples_heldout conj_and_heldout_test prep_in_decoding_experiments dobj_decoding_examples nn_algorithm_training det_algorithm_the amod_parameters_averaged det_parameters_the prepc_in_used_decoding prep_from_used_algorithm dobj_used_parameters nsubj_used_we prep_following_used_Collins appos_Collins_2002
P04-1007	W02-1001	o	Freund and Schapire -LRB- 1999 -RRB- originally proposed the averaged parameter method it was shown to give substantial improvements in accuracy for tagging tasks in Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_in_tasks_Collins amod_tasks_tagging prep_in_improvements_accuracy amod_improvements_substantial prep_for_give_tasks dobj_give_improvements aux_give_to xcomp_shown_give auxpass_shown_was nsubjpass_shown_it nn_method_parameter amod_method_averaged det_method_the parataxis_proposed_shown dobj_proposed_method advmod_proposed_originally nsubj_proposed_Schapire nsubj_proposed_Freund appos_Schapire_1999 conj_and_Freund_Schapire
P04-1015	W02-1001	n	Collins -LRB- 2000 -RRB- and Collins and Duffy -LRB- 2002 -RRB- rerank the top N parses from an existing generative parser but this kind of approach 1Dynamic programming methods -LRB- Geman and Johnson 2002 Lafferty et al. 2001 -RRB- can sometimes be used for both training and decoding but this requires fairly strong restrictions on the features in the model	det_model_the prep_in_features_model det_features_the prep_on_restrictions_features amod_restrictions_strong advmod_strong_fairly dobj_requires_restrictions nsubj_requires_this conj_and_training_decoding preconj_training_both prep_for_used_decoding prep_for_used_training auxpass_used_be advmod_used_sometimes aux_used_can nsubjpass_used_kind num_Lafferty_2001 nn_Lafferty_al. nn_Lafferty_et dep_Geman_Lafferty conj_and_Geman_2002 conj_and_Geman_Johnson dep_methods_2002 dep_methods_Johnson dep_methods_Geman nn_methods_programming amod_methods_1Dynamic nn_methods_approach prep_of_kind_methods det_kind_this amod_parser_generative amod_parser_existing det_parser_an conj_but_parses_requires conj_but_parses_used prep_from_parses_parser nsubj_parses_N amod_N_top det_N_the dep_rerank_requires dep_rerank_used dep_rerank_parses nsubj_rerank_Collins nsubj_rerank_Collins appos_Duffy_2002 conj_and_Collins_Duffy conj_and_Collins_Duffy conj_and_Collins_Collins appos_Collins_2000
P04-1015	W02-1001	o	This paper explores an alternative approach to parsing based on the perceptron training algorithm introduced in Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_in_introduced_Collins vmod_algorithm_introduced nn_algorithm_training nn_algorithm_perceptron det_algorithm_the prep_on_based_algorithm vmod_approach_based prep_to_approach_parsing amod_approach_alternative det_approach_an dobj_explores_approach nsubj_explores_paper det_paper_This ccomp_``_explores
P04-1015	W02-1001	o	For this paper we used POS tags that were provided either by the Treebank itself -LRB- gold standard tags -RRB- or by the perceptron POS tagger3 presented in Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_in_presented_Collins vmod_tagger3_presented nn_tagger3_POS nn_tagger3_perceptron det_tagger3_the amod_tags_standard amod_tags_gold conj_or_Treebank_tagger3 appos_Treebank_tags npadvmod_Treebank_itself det_Treebank_the agent_provided_tagger3 agent_provided_Treebank preconj_provided_either auxpass_provided_were nsubjpass_provided_that rcmod_tags_provided nn_tags_POS dobj_used_tags nsubj_used_we prep_for_used_paper det_paper_this
P04-1015	W02-1001	o	2.1 Linear Models for NLP We follow the framework outlined in Collins -LRB- 2002 2004 -RRB-	dep_2002_2004 dep_Collins_2002 prep_in_outlined_Collins vmod_framework_outlined det_framework_the dobj_follow_framework nsubj_follow_We rcmod_NLP_follow prep_for_Models_NLP nn_Models_Linear num_Models_2.1
P04-1015	W02-1001	o	We will briefly review the perceptron algorithm and its convergence properties see Collins -LRB- 2002 -RRB- for a full description	amod_description_full det_description_a prep_for_Collins_description appos_Collins_2002 dobj_see_Collins nsubj_see_properties nn_properties_convergence poss_properties_its nn_algorithm_perceptron det_algorithm_the conj_and_review_see dobj_review_algorithm advmod_review_briefly aux_review_will nsubj_review_We
P04-1015	W02-1001	o	We can then state the following theorem -LRB- see -LRB- Collins 2002 -RRB- for a proof -RRB- Theorem 1 For any training sequence -LRB- xi yi -RRB- that is separable with margin for any value of T then for the perceptron algorithm in figure 1 Ne R 2 2 where R is a constant such that 8i 8z 2 GEN -LRB- xi -RRB- jj -LRB- xi yi -RRB- -LRB- xi z -RRB- jj R This theorem implies that if there is a parameter vector U which makes zero errors on the training set then after a finite number of iterations the training algorithm will converge to parameter values with zero training error	nn_error_training num_error_zero prep_with_values_error nn_values_parameter prep_to_converge_values aux_converge_will nsubj_converge_number mark_converge_after nn_algorithm_training det_algorithm_the nn_algorithm_iterations prep_of_number_algorithm amod_number_finite det_number_a advmod_after_then nn_set_training det_set_the prep_on_errors_set num_errors_zero dobj_makes_errors nsubj_makes_which rcmod_U_makes nn_U_vector nn_U_parameter det_U_a advcl_is_converge nsubj_is_U expl_is_there mark_is_if ccomp_that_is dep_implies_that nsubj_implies_R det_theorem_This appos_R_theorem nn_R_jj nn_R_jj dep_xi_z dep_xi_yi appos_jj_xi dep_jj_xi nn_jj_GEN appos_GEN_xi num_GEN_2 nn_GEN_8z parataxis_8i_implies dep_that_8i dep_such_that dep_constant_such amod_a_constant nsubj_is_a dep_R_is dep_where_R number_2_2 prep_R_where num_R_2 dep_Ne_R dep_1_Ne dep_figure_1 prep_in_algorithm_figure nn_algorithm_perceptron det_algorithm_the prep_of_value_T det_value_any prep_with_separable_margin cop_separable_is nsubj_separable_that dep_xi_yi rcmod_sequence_separable appos_sequence_xi nn_sequence_training det_sequence_any prep_for_Theorem_algorithm advmod_Theorem_then prep_for_Theorem_value prep_for_Theorem_sequence num_Theorem_1 det_proof_a dep_Collins_2002 prep_for_see_proof dep_see_Collins dep_theorem_see amod_theorem_following det_theorem_the dep_state_Theorem dobj_state_theorem advmod_state_then aux_state_can nsubj_state_We ccomp_``_state
P04-1015	W02-1001	o	All of the convergence and generalization results in Collins -LRB- 2002 -RRB- depend on notions of separability rather than the size of GEN. Two questions come to mind	aux_mind_to xcomp_come_mind num_questions_Two nn_questions_GEN. prep_of_size_questions det_size_the conj_negcc_notions_size prep_of_notions_separability dep_depend_come prep_on_depend_size prep_on_depend_notions nsubj_depend_All appos_Collins_2002 nn_results_generalization prep_in_convergence_Collins conj_and_convergence_results det_convergence_the prep_of_All_results prep_of_All_convergence
P04-1015	W02-1001	o	Freund and Schapire -LRB- 1999 -RRB- discuss how the theory for classification problems can be extended to deal with both of these questions Collins -LRB- 2002 -RRB- describes how these results apply to NLP problems	nn_problems_NLP prep_to_apply_problems nsubj_apply_results advmod_apply_how det_results_these ccomp_describes_apply nsubj_describes_Collins appos_Collins_2002 det_questions_these prep_of_both_questions prep_with_deal_both aux_deal_to xcomp_extended_deal auxpass_extended_be aux_extended_can nsubjpass_extended_theory advmod_extended_how nn_problems_classification prep_for_theory_problems det_theory_the parataxis_discuss_describes ccomp_discuss_extended nsubj_discuss_Schapire nsubj_discuss_Freund appos_Schapire_1999 conj_and_Freund_Schapire
P04-1015	W02-1001	o	As a final note following Collins -LRB- 2002 -RRB- we used the averaged parameters from the training algorithm in decoding test examples in our experiments	poss_experiments_our nn_examples_test prep_in_decoding_experiments dobj_decoding_examples nn_algorithm_training det_algorithm_the amod_parameters_averaged det_parameters_the prepc_in_used_decoding prep_from_used_algorithm dobj_used_parameters nsubj_used_we prep_following_used_Collins prep_as_used_note appos_Collins_2002 amod_note_final det_note_a
P04-1015	W02-1001	p	Freund and Schapire -LRB- 1999 -RRB- originally proposed the averaged parameter method it was shown to give substantial improvements in accuracy for tagging tasks in Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_in_tasks_Collins amod_tasks_tagging prep_in_improvements_accuracy amod_improvements_substantial prep_for_give_tasks dobj_give_improvements aux_give_to xcomp_shown_give auxpass_shown_was nsubjpass_shown_it nn_method_parameter amod_method_averaged det_method_the parataxis_proposed_shown dobj_proposed_method advmod_proposed_originally nsubj_proposed_Schapire nsubj_proposed_Freund appos_Schapire_1999 conj_and_Freund_Schapire
P04-1015	W02-1001	o	Examples of such techniques are Markov Random Fields -LRB- Ratnaparkhi et al. 1994 Abney 1997 Della Pietra et al. 1997 Johnson et al. 1999 -RRB- and boosting or perceptron approaches to reranking -LRB- Freund et al. 1998 Collins 2000 Collins and Duffy 2002 -RRB-	amod_Collins_2002 conj_and_Collins_Duffy num_Collins_2000 dep_Freund_Duffy dep_Freund_Collins conj_Freund_Collins appos_Freund_1998 dep_Freund_al. nn_Freund_et dep_reranking_Freund prep_to_approaches_reranking nn_approaches_perceptron num_Johnson_1999 nn_Johnson_al. nn_Johnson_et conj_al._Johnson conj_al._1997 nn_al._et dep_Pietra_al. nn_Pietra_Della conj_or_Abney_approaches conj_and_Abney_boosting conj_and_Abney_Pietra num_Abney_1997 dep_Ratnaparkhi_approaches dep_Ratnaparkhi_boosting dep_Ratnaparkhi_Pietra dep_Ratnaparkhi_Abney appos_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_Fields_Ratnaparkhi nn_Fields_Random nn_Fields_Markov cop_Fields_are nsubj_Fields_Examples amod_techniques_such prep_of_Examples_techniques
P04-1086	W02-1001	p	Discriminative learning methods such as Maximum Entropy Markov Models -LRB- McCallum et al. 2000 -RRB- Projection Based Markov Models -LRB- Punyakanok and Roth 2000 -RRB- Conditional Random Fields -LRB- Lafferty et al. 2001 -RRB- Sequence AdaBoost -LRB- Altun et al. 2003a -RRB- Sequence Perceptron -LRB- Collins 2002 -RRB- Hidden Markov Support Vector Machines -LRB- Altun et al. 2003b -RRB- and Maximum-Margin Markov Networks -LRB- Taskar et al. 2004 -RRB- overcome the limitations of HMMs	prep_of_limitations_HMMs det_limitations_the dobj_overcome_limitations nsubj_overcome_methods amod_Taskar_2004 dep_Taskar_al. nn_Taskar_et dep_Networks_Taskar nn_Networks_Markov nn_Networks_Maximum-Margin appos_Altun_2003b dep_Altun_al. nn_Altun_et nn_Machines_Vector nn_Machines_Support nn_Machines_Markov nn_Machines_Hidden amod_Collins_2002 dep_Perceptron_Collins nn_Perceptron_Sequence appos_Altun_2003a dep_Altun_al. nn_Altun_et dep_AdaBoost_Altun nn_AdaBoost_Sequence dep_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et appos_Fields_Lafferty nn_Fields_Random amod_Fields_Conditional dep_Punyakanok_2000 conj_and_Punyakanok_Roth conj_and_Models_Networks dep_Models_Altun conj_and_Models_Machines conj_and_Models_Perceptron conj_and_Models_AdaBoost conj_and_Models_Fields appos_Models_Roth appos_Models_Punyakanok nn_Models_Markov dobj_Based_Networks dobj_Based_Machines dobj_Based_Perceptron dobj_Based_AdaBoost dobj_Based_Fields dobj_Based_Models nsubj_Based_Projection amod_McCallum_2000 dep_McCallum_al. nn_McCallum_et rcmod_Models_Based dep_Models_McCallum nn_Models_Markov nn_Models_Entropy nn_Models_Maximum prep_such_as_methods_Models nn_methods_learning amod_methods_Discriminative
P04-1086	W02-1001	o	Among these methods CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging -LRB- Lafferty et al. 2001 -RRB- Named-Entity Recognition -LRB- Collins 2002 -RRB- and shallow parsing -LRB- Sha and Pereira 2003 McCallum 2003 -RRB-	amod_McCallum_2003 dep_Sha_McCallum conj_and_Sha_2003 conj_and_Sha_Pereira dep_parsing_2003 dep_parsing_Pereira dep_parsing_Sha amod_parsing_shallow amod_Collins_2002 dep_Recognition_Collins nn_Recognition_Named-Entity amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et conj_and_Tagging_parsing conj_and_Tagging_Recognition dep_Tagging_Lafferty nn_Tagging_Part-of-Speech prep_to_applied_parsing prep_to_applied_Recognition prep_to_applied_Tagging advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_CRFs prep_in_used_NLP conj_and_technique_applied vmod_technique_used amod_technique_common det_technique_the cop_technique_is nsubj_technique_CRFs prep_among_technique_methods advmod_common_most det_methods_these
P05-1012	W02-1001	o	We also implemented an averaged perceptron system -LRB- Collins 2002 -RRB- -LRB- another online learning algorithm -RRB- for comparison	amod_algorithm_learning nn_algorithm_online det_algorithm_another dep_Collins_2002 appos_system_algorithm appos_system_Collins nn_system_perceptron amod_system_averaged det_system_an prep_for_implemented_comparison dobj_implemented_system advmod_implemented_also nsubj_implemented_We ccomp_``_implemented
P05-1012	W02-1001	p	Averaging has been shown to help reduce overfitting -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_overfitting_Collins dobj_reduce_overfitting xcomp_help_reduce aux_help_to xcomp_shown_help auxpass_shown_been aux_shown_has nsubjpass_shown_Averaging
P05-1012	W02-1001	o	Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated -LRB- Riezler et al. 2002 Clark and Curran 2004 Collins and Roark 2004 Taskar et al. 2004 -RRB-	num_Taskar_2004 nn_Taskar_al. nn_Taskar_et num_Collins_2004 conj_and_Collins_Roark num_Clark_2004 conj_and_Clark_Curran dep_Riezler_Taskar conj_Riezler_Roark conj_Riezler_Collins conj_Riezler_Curran conj_Riezler_Clark amod_Riezler_2002 dep_Riezler_al. nn_Riezler_et dep_investigated_Riezler auxpass_investigated_been advmod_investigated_recently aux_investigated_have nsubjpass_investigated_parsers advmod_recently_only amod_sentence_given det_sentence_a amod_trees_entire prep_for_score_sentence dobj_score_trees nsubj_score_that rcmod_parsers_score amod_parsers_trained advmod_trained_Discriminatively
P05-1020	W02-1001	o	Following previous work on using global features of candidate structures to learn a ranking model -LRB- Collins 2002 -RRB- the global -LRB- i.e. partition-based -RRB- features we consider here are simple functions of the local features that capture the relationship between NP pairs	nn_pairs_NP prep_between_relationship_pairs det_relationship_the dobj_capture_relationship nsubj_capture_that rcmod_features_capture amod_features_local det_features_the prep_of_functions_features amod_functions_simple cop_functions_are nsubj_functions_features advmod_consider_here nsubj_consider_we ccomp_features_consider nsubj_features_the prep_following_features_work dep_i.e._partition-based dep_global_i.e. amod_the_global amod_Collins_2002 appos_model_Collins amod_model_ranking det_model_a dobj_learn_model aux_learn_to nn_structures_candidate prep_of_features_structures amod_features_global vmod_using_learn dobj_using_features prepc_on_work_using amod_work_previous
P05-1063	W02-1001	o	3 Parse Tree Features We tagged each candidate transcription with -LRB- 1 -RRB- part-of-speech tags using the tagger documented in Collins -LRB- 2002 -RRB- and -LRB- 2 -RRB- a full parse tree using the parser documented in Collins -LRB- 1999 -RRB-	appos_Collins_1999 prep_in_documented_Collins vmod_parser_documented det_parser_the dobj_using_parser vmod_tree_using nn_tree_parse amod_tree_full det_tree_a dep_tree_2 appos_Collins_2002 prep_in_documented_Collins vmod_tagger_documented det_tagger_the dobj_using_tagger vmod_tags_using nn_tags_part-of-speech dep_tags_1 nn_transcription_candidate det_transcription_each prep_with_tagged_tags dobj_tagged_transcription nsubj_tagged_We conj_and_Features_tree ccomp_Features_tagged nsubj_Features_Tree nn_Tree_Parse num_Tree_3 ccomp_``_tree ccomp_``_Features
P05-1063	W02-1001	o	For this paper we train the parameter vector using the perceptron algorithm -LRB- Collins 2004 Collins 2002 -RRB-	amod_Collins_2002 dep_Collins_Collins amod_Collins_2004 appos_algorithm_Collins nn_algorithm_perceptron det_algorithm_the dobj_using_algorithm nn_vector_parameter det_vector_the xcomp_train_using dobj_train_vector nsubj_train_we prep_for_train_paper det_paper_this
P05-1063	W02-1001	o	2.2 Global Linear Models We follow the framework of Collins -LRB- 2002 2004 -RRB- recently applied to language modeling in Roark et al.	nn_al._et nn_al._Roark nn_modeling_language prep_in_applied_al. prep_to_applied_modeling advmod_applied_recently nsubj_applied_Models dep_2002_2004 dep_Collins_2002 prep_of_framework_Collins det_framework_the dobj_follow_framework nsubj_follow_We rcmod_Models_follow nn_Models_Linear amod_Models_Global num_Models_2.2
P05-1063	W02-1001	o	For a full description of the algorithm see Collins -LRB- 2004 2002 -RRB-	dep_2004_2002 dep_Collins_2004 dobj_see_Collins prep_for_see_description det_algorithm_the prep_of_description_algorithm amod_description_full det_description_a
P05-1069	W02-1001	o	A related method is multi-category perceptron which explicitly finds a weight vector that separates correct labels from the incorrect ones in a mistake driven fashion -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_fashion_Collins amod_fashion_driven dep_mistake_fashion det_mistake_a amod_ones_incorrect det_ones_the amod_labels_correct prep_in_separates_mistake prep_from_separates_ones dobj_separates_labels nsubj_separates_that rcmod_vector_separates nn_vector_weight det_vector_a dobj_finds_vector advmod_finds_explicitly nsubj_finds_which rcmod_perceptron_finds amod_perceptron_multi-category cop_perceptron_is nsubj_perceptron_method amod_method_related det_method_A
P05-1069	W02-1001	o	Section 4 describes the online training procedure and compares it to the well known perceptron training algorithm -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_algorithm_Collins nn_algorithm_training nn_algorithm_perceptron amod_algorithm_known det_algorithm_the advmod_known_well prep_to_compares_algorithm dobj_compares_it nsubj_compares_Section nn_procedure_training amod_procedure_online det_procedure_the conj_and_describes_compares dobj_describes_procedure nsubj_describes_Section num_Section_4
P06-1065	W02-1001	o	The principal training method is an adaptation of averaged perceptron learning as described by Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_by_described_Collins mark_described_as nn_learning_perceptron advcl_averaged_described dobj_averaged_learning prepc_of_adaptation_averaged det_adaptation_an cop_adaptation_is nsubj_adaptation_method nn_method_training amod_method_principal det_method_The
P06-1065	W02-1001	o	5 Perceptron Training We optimize feature weights using a modification of averaged perceptron learning as described by Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_by_described_Collins mark_described_as nn_learning_perceptron advcl_averaged_described dobj_averaged_learning prepc_of_modification_averaged det_modification_a dobj_using_modification nn_weights_feature xcomp_optimize_using dobj_optimize_weights nsubj_optimize_We rcmod_Training_optimize nn_Training_Perceptron num_Training_5
P06-1088	W02-1001	p	1 Introduction State-of-the-art part of speech -LRB- POS -RRB- tagging accuracy is now above 97 % for newspaper text -LRB- Collins 2002 Toutanova et al. 2003 -RRB-	num_Toutanova_2003 nn_Toutanova_al. nn_Toutanova_et dep_Collins_Toutanova amod_Collins_2002 appos_text_Collins nn_text_newspaper num_%_97 prep_for_is_text prep_above_is_% advmod_is_now nsubj_is_part amod_accuracy_tagging nn_accuracy_speech appos_speech_POS prep_of_part_accuracy amod_part_State-of-the-art nn_part_Introduction num_part_1 ccomp_``_is
P06-1091	W02-1001	o	Moreover under this view SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task e.g. the on-line training algorithm presented in -LRB- McDonald et al. 2005 -RRB- and the perceptron training algorithm presented in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins prep_presented_in vmod_algorithm_presented nn_algorithm_training nn_algorithm_perceptron det_algorithm_the conj_and_McDonald_algorithm amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et prep_in_presented_algorithm prep_in_presented_McDonald vmod_algorithm_presented nn_algorithm_training amod_algorithm_on-line det_algorithm_the pobj_e.g._algorithm det_task_these prep_for_presented_task vmod_algorithms_presented nn_algorithms_training prep_on_work_algorithms aux_work_to xcomp_similar_work advmod_similar_most advmod_similar_actually cop_similar_is nsubj_similar_algorithm det_paper_this prep_in_presented_paper vmod_algorithm_presented nn_algorithm_training amod_algorithm_novel det_algorithm_the amod_parsing_shallow conj_and_tagging_parsing amod_tagging_part-of-speech prep_such_as_problems_parsing prep_such_as_problems_tagging nn_problems_annotation nn_problems_language amod_problems_natural amod_problems_sequential prep_to_similar_problems advmod_similar_quite prep_becomes_e.g. conj_and_becomes_similar acomp_becomes_similar nsubj_becomes_SMT prep_under_becomes_view advmod_becomes_Moreover det_view_this
P06-1096	W02-1001	p	2.2 Perceptron-based training To tune the parameters w of the model we use the averaged perceptron algorithm -LRB- Collins 2002 -RRB- because of its efficiency and past success on various NLP tasks -LRB- Collins and Roark 2004 Roark et al. 2004 -RRB-	num_Roark_2004 nn_Roark_al. nn_Roark_et dep_Collins_Roark amod_Collins_2004 conj_and_Collins_Roark appos_tasks_Roark appos_tasks_Collins nn_tasks_NLP amod_tasks_various amod_success_past prep_on_efficiency_tasks conj_and_efficiency_success poss_efficiency_its amod_Collins_2002 dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the prep_because_of_use_success prep_because_of_use_efficiency dobj_use_algorithm nsubj_use_we nsubj_use_training det_model_the prep_of_w_model dep_parameters_w det_parameters_the dobj_tune_parameters aux_tune_To vmod_training_tune amod_training_Perceptron-based num_training_2.2
P06-2034	W02-1001	p	Averaged perceptron -LRB- Collins 2002a -RRB- which has been successfully applied to several tagging and parsing reranking tasks -LRB- Collins 2002c Collins 2002a -RRB- was employed for training rerank267 CLANG GEOQUERY P R F P R F SCISSOR 89.5 73.7 80.8 98.5 74.4 84.8 SCISSOR + 87.0 78.0 82.3 95.5 77.2 85.4 Table 2 The performance of the baseline model SCISSOR + compared with SCISSOR -LRB- with the best result in bold -RRB- where P = precision R = recall and F = F-measure	dobj_=_F-measure npadvmod_=_F nsubj_=_P amod_recall_= npadvmod_=_R conj_and_=_= conj_and_=_recall dobj_=_precision nsubj_=_P advmod_=_where dep_in_= dep_in_recall dep_in_= dep_in_bold prep_result_in amod_result_best det_result_the prep_with_SCISSOR_result nn_SCISSOR_model nn_SCISSOR_baseline det_SCISSOR_the pobj_performance_SCISSOR prepc_compared_with_performance_with cc_performance_+ prep_of_performance_SCISSOR det_performance_The num_Table_2 num_Table_85.4 num_Table_77.2 dep_Table_95.5 number_95.5_82.3 dep_78.0_Table number_78.0_87.0 conj_+_SCISSOR_78.0 num_SCISSOR_84.8 dep_SCISSOR_74.4 dep_74.4_98.5 number_98.5_80.8 dep_73.7_78.0 dep_73.7_SCISSOR number_73.7_89.5 dep_SCISSOR_73.7 nn_SCISSOR_F nn_SCISSOR_R nn_SCISSOR_P nn_SCISSOR_F nn_SCISSOR_R nn_SCISSOR_P nn_SCISSOR_GEOQUERY nn_SCISSOR_CLANG nn_SCISSOR_rerank267 nn_SCISSOR_training dobj_employed_performance prep_for_employed_SCISSOR auxpass_employed_was nsubjpass_employed_perceptron appos_Collins_2002a dep_Collins_Collins appos_Collins_2002c nn_tasks_reranking nn_tasks_parsing appos_tagging_Collins conj_and_tagging_tasks amod_tagging_several prep_to_applied_tasks prep_to_applied_tagging advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_which dep_Collins_2002a rcmod_perceptron_applied appos_perceptron_Collins amod_perceptron_Averaged
P06-2034	W02-1001	o	While reranking has benefited many tagging and parsing tasks -LRB- Collins 2000 Collins 2002c Charniak and Johnson 2005 -RRB- including semantic role labeling -LRB- Toutanova et al. 2005 -RRB- it has not yet been applied to semantic parsing	amod_parsing_semantic prep_to_applied_parsing auxpass_applied_been advmod_applied_yet neg_applied_not aux_applied_has nsubjpass_applied_it amod_Toutanova_2005 dep_Toutanova_al. nn_Toutanova_et dep_labeling_Toutanova nn_labeling_role amod_labeling_semantic dep_Charniak_2005 conj_and_Charniak_Johnson appos_Collins_2002c rcmod_Collins_applied prep_including_Collins_labeling dep_Collins_Johnson dep_Collins_Charniak conj_Collins_Collins amod_Collins_2000 dep_tasks_Collins amod_tasks_parsing amod_tasks_tagging amod_tasks_many conj_and_tagging_parsing dobj_benefited_tasks aux_benefited_has nsubj_benefited_reranking mark_benefited_While advcl_``_benefited
P06-2034	W02-1001	o	We also plan to explore other types of reranking features such as the features used in semantic role labeling -LRB- SRL -RRB- -LRB- Gildea and Jurafsky 2002 Carreras and M`arquez 2005 -RRB- like the path between a target predicate and its argument and kernel methods -LRB- Collins 2002b -RRB-	appos_Collins_2002b dep_methods_Collins nn_methods_kernel poss_argument_its conj_and_predicate_argument nn_predicate_target det_predicate_a prep_between_path_argument prep_between_path_predicate det_path_the dep_Carreras_2005 conj_and_Carreras_M`arquez conj_and_Gildea_methods prep_like_Gildea_path dep_Gildea_M`arquez dep_Gildea_Carreras conj_and_Gildea_2002 conj_and_Gildea_Jurafsky dep_labeling_methods dep_labeling_2002 dep_labeling_Jurafsky dep_labeling_Gildea appos_labeling_SRL nn_labeling_role amod_labeling_semantic prep_in_used_labeling vmod_features_used det_features_the prep_such_as_features_features nn_features_reranking prep_of_types_features amod_types_other dobj_explore_types aux_explore_to xcomp_plan_explore advmod_plan_also nsubj_plan_We
P06-2034	W02-1001	o	Experimenting with other effective reranking algorithms such as SVMs -LRB- Joachims 2002 -RRB- and MaxEnt -LRB- Charniak and Johnson 2005 -RRB- is also a direction of our future research	amod_research_future poss_research_our prep_of_direction_research det_direction_a advmod_direction_also cop_direction_is vmod_direction_Experimenting dep_Charniak_2005 conj_and_Charniak_Johnson appos_MaxEnt_Johnson appos_MaxEnt_Charniak amod_Joachims_2002 conj_and_SVMs_MaxEnt dep_SVMs_Joachims prep_such_as_algorithms_MaxEnt prep_such_as_algorithms_SVMs nn_algorithms_reranking amod_algorithms_effective amod_algorithms_other prep_with_Experimenting_algorithms
P06-2034	W02-1001	p	2.3 The Averaged Perceptron Reranking Model Averaged perceptron -LRB- Collins 2002a -RRB- has been successfully applied to several tagging and parsing reranking tasks -LRB- Collins 2002c Collins 2002a -RRB- and in this paper we employed it in reranking semantic parses generated by the base semantic parser SCISSOR	nn_SCISSOR_parser amod_SCISSOR_semantic nn_SCISSOR_base det_SCISSOR_the agent_generated_SCISSOR vmod_parses_generated amod_parses_semantic dobj_reranking_parses prepc_in_employed_reranking dobj_employed_it nsubj_employed_we advcl_employed_in advcl_employed_applied det_paper_this pobj_in_paper appos_Collins_2002a dep_Collins_Collins appos_Collins_2002c nn_tasks_reranking nn_tasks_parsing appos_tagging_Collins conj_and_tagging_tasks amod_tagging_several conj_and_applied_in prep_to_applied_tasks prep_to_applied_tagging advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_2.3 appos_Collins_2002a dep_perceptron_Collins dobj_Averaged_perceptron nsubj_Averaged_Model nn_Model_Reranking nn_Model_Perceptron amod_Model_Averaged det_Model_The rcmod_2.3_Averaged
P06-2034	W02-1001	o	The model is composed of three parts -LRB- Collins 2002a -RRB- a set of candidate SAPTs GEN which is the top n SAPTs of a sentence from SCISSOR a function that maps a sentence Inputs A set of training examples -LRB- xi yi -RRB- i = 1n where xi is a sentence and yi is a candidate SAPT that has the highest similarity score with the gold-standard SAPT Initialization Set W = 0 Algorithm For t = 1T i = 1n Calculate yi = argmaxyGEN -LRB- xi -RRB- -LRB- xi y -RRB- W If -LRB- yi negationslash = yi -RRB- then W = W + -LRB- xi yi -RRB- -LRB- xi yi -RRB- Output The parameter vector W Figure 2 The perceptron training algorithm	nn_algorithm_training nn_algorithm_perceptron det_algorithm_The dep_Figure_algorithm num_Figure_2 nn_Figure_W nn_Figure_vector nn_Figure_parameter det_Figure_The nn_Output_yi dep_Output_Calculate nn_Output_1n dep_Output_= nn_Output_i nn_Output_1T amod_Output_= nn_Output_t nn_xi_xi nn_xi_W amod_xi_= nn_xi_W advmod_xi_then dep_xi_= mark_xi_If appos_xi_yi conj_+_W_xi dobj_=_yi nsubj_=_negationslash nn_negationslash_yi nn_W_argmaxyGEN amod_W_= nn_W_yi appos_xi_y dep_argmaxyGEN_xi appos_argmaxyGEN_xi advcl_Calculate_xi dobj_Calculate_W num_Algorithm_0 dep_=_Algorithm amod_W_= prep_for_Set_Output dobj_Set_W nn_Initialization_SAPT amod_Initialization_gold-standard det_Initialization_the prep_with_score_Initialization nn_score_similarity amod_score_highest det_score_the dobj_has_score nsubj_has_that rcmod_SAPT_has nn_SAPT_candidate det_SAPT_a cop_SAPT_is nsubj_SAPT_i det_sentence_a cop_sentence_is nsubj_sentence_xi advmod_sentence_where conj_and_1n_yi rcmod_1n_sentence dep_=_yi dep_=_1n dep_i_= appos_xi_yi dep_examples_xi nn_examples_training dep_set_Figure dep_set_Set rcmod_set_SAPT prep_of_set_examples det_set_A dep_Inputs_set nn_Inputs_sentence det_Inputs_a dobj_maps_Inputs nsubj_maps_that rcmod_function_maps det_function_a prep_from_sentence_SCISSOR det_sentence_a prep_of_SAPTs_sentence nn_SAPTs_n amod_SAPTs_top det_SAPTs_the cop_SAPTs_is nsubj_SAPTs_which rcmod_GEN_SAPTs nn_GEN_SAPTs nn_GEN_candidate dep_set_function prep_of_set_GEN det_set_a appos_Collins_2002a dep_parts_Collins num_parts_three dep_composed_set prep_of_composed_parts auxpass_composed_is nsubjpass_composed_model det_model_The rcmod_``_composed
P06-2034	W02-1001	o	For a full description of the algorithm see -LRB- Collins 2002a -RRB-	appos_Collins_2002a dep_see_Collins prep_for_see_description det_algorithm_the prep_of_description_algorithm amod_description_full det_description_a
P06-2034	W02-1001	o	In comparison with shallow semantic analysis tasks such as wordsense disambiguation -LRB- Ide and Jeaneronis 1998 -RRB- and semantic role labeling -LRB- Gildea and Jurafsky 2002 Carreras and M`arquez 2005 -RRB- which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates semantic parsing -LRB- Kate et al. 2005 Ge and Mooney 2005 Zettlemoyer and Collins 2005 -RRB- pursues a more ambitious goal mapping natural language sentences to complete formal meaning representations -LRB- MRs -RRB- where the meaning of each part of a sentence is analyzed including noun phrases verb phrases negation quantifiers and so on	advmod_on_so conj_and_phrases_on conj_and_phrases_quantifiers conj_and_phrases_negation dobj_verb_on dobj_verb_quantifiers dobj_verb_negation dobj_verb_phrases nn_phrases_noun dep_analyzed_verb prep_including_analyzed_phrases auxpass_analyzed_is nsubjpass_analyzed_meaning advmod_analyzed_where det_sentence_a prep_of_part_sentence det_part_each prep_of_meaning_part det_meaning_the rcmod_representations_analyzed appos_representations_MRs nn_representations_meaning amod_representations_formal dobj_complete_representations aux_complete_to nn_sentences_language amod_sentences_natural nn_sentences_mapping nn_sentences_goal amod_sentences_ambitious det_sentences_a advmod_ambitious_more vmod_pursues_complete dobj_pursues_sentences nsubj_pursues_Carreras nsubj_pursues_2002 nsubj_pursues_Jurafsky nsubj_pursues_Gildea dep_pursues_labeling num_Zettlemoyer_2005 conj_and_Zettlemoyer_Collins num_Ge_2005 conj_and_Ge_Mooney dep_Kate_Collins dep_Kate_Zettlemoyer dep_Kate_Mooney dep_Kate_Ge amod_Kate_2005 dep_Kate_al. nn_Kate_et appos_parsing_Kate amod_parsing_semantic appos_predicates_parsing prep_of_roles_predicates amod_roles_semantic dobj_finding_roles nn_words_target prep_of_meanings_words det_meanings_the conj_or_identifying_finding dobj_identifying_meanings det_problem_this prepc_by_tackle_finding prepc_by_tackle_identifying dobj_tackle_problem advmod_tackle_partially nsubj_tackle_which advmod_partially_only dep_Carreras_2005 conj_and_Carreras_M`arquez rcmod_Gildea_tackle conj_and_Gildea_M`arquez conj_and_Gildea_Carreras conj_and_Gildea_2002 conj_and_Gildea_Jurafsky nn_labeling_role amod_labeling_semantic dep_Ide_1998 conj_and_Ide_Jeaneronis conj_and_disambiguation_pursues dep_disambiguation_Jeaneronis dep_disambiguation_Ide nn_disambiguation_wordsense prep_such_as_tasks_pursues prep_such_as_tasks_disambiguation nn_tasks_analysis amod_tasks_semantic amod_tasks_shallow prep_with_comparison_tasks pobj_In_comparison dep_``_In
P06-2098	W02-1001	o	C0 C q 1 q xq xq1 xq1 xq xr xr +1 Table 6 Lexicalized Features for Joint Models aging of the weights suggested by -LRB- Collins 2002 -RRB-	amod_Collins_2002 agent_suggested_Collins vmod_weights_suggested det_weights_the prep_of_aging_weights dep_Models_aging nn_Models_Joint prep_for_Features_Models nsubj_Features_Lexicalized dep_Table_Features num_Table_6 num_Table_+1 nn_Table_xr nn_Table_xr nn_Table_xq nn_Table_xq1 nn_Table_xq1 nn_Table_xq dobj_q_Table vmod_q_q num_q_1 dep_C_q dep_C0_C
P06-2098	W02-1001	o	The final model V uses the weight vector w = summationtextk j = 1 -LRB- cjwj -RRB- Tn -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_Tn_Collins nn_Tn_cjwj num_Tn_1 dep_=_Tn amod_j_= nn_j_summationtextk dep_=_j amod_w_= nn_w_vector nn_w_weight det_w_the dobj_uses_w nsubj_uses_V nn_V_model amod_V_final det_V_The
P06-2113	W02-1001	o	3.3 CRFs and Perceptron Learning Perceptron training for conditional models -LRB- Collins 2002 -RRB- is an approximation to the SGD algorithm using feature counts from the Viterbi label sequence in lieu of expected feature counts	nn_counts_feature amod_counts_expected prep_in_lieu_of_sequence_counts nn_sequence_label nn_sequence_Viterbi det_sequence_the nn_counts_feature prep_from_using_sequence dobj_using_counts nn_algorithm_SGD det_algorithm_the vmod_approximation_using prep_to_approximation_algorithm det_approximation_an cop_approximation_is nsubj_approximation_training nsubj_approximation_CRFs amod_Collins_2002 amod_models_conditional nn_training_Perceptron nn_training_Learning nn_training_Perceptron appos_CRFs_Collins prep_for_CRFs_models conj_and_CRFs_training num_CRFs_3.3
P06-2113	W02-1001	o	In both perceptron and CRF training we average the parameters over training iterations -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_iterations_Collins nn_iterations_training prep_over_parameters_iterations det_parameters_the dobj_average_parameters nsubj_average_we prep_in_average_training prep_in_average_perceptron nn_training_CRF conj_and_perceptron_training preconj_perceptron_both
P07-1036	W02-1001	o	In many cases improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology -LRB- Cohen and Sarawagi 2004 Collins and Singer 1999 Haghighi and Klein 2006 Thelen and Riloff 2002 -RRB-	dep_Thelen_2002 conj_and_Thelen_Riloff dep_Haghighi_Riloff dep_Haghighi_Thelen conj_and_Haghighi_2006 conj_and_Haghighi_Klein num_Collins_1999 conj_and_Collins_Singer dep_Cohen_2006 dep_Cohen_Klein dep_Cohen_Haghighi conj_and_Cohen_Singer conj_and_Cohen_Collins conj_and_Cohen_2004 conj_and_Cohen_Sarawagi conj_or_dictionaries_ontology prep_from_taken_ontology prep_from_taken_dictionaries vmod_information_taken nn_information_domain det_models_these dep_seeding_Collins dep_seeding_2004 dep_seeding_Sarawagi dep_seeding_Cohen prep_with_seeding_information dobj_seeding_models agent_done_seeding auxpass_done_was nsubjpass_done_improving prep_in_done_cases amod_models_semi-supervised dobj_improving_models amod_cases_many
P07-1036	W02-1001	o	This was used for example by -LRB- Thelen and Riloff 2002 Collins and Singer 1999 -RRB- in information extraction and by -LRB- Smith and Eisner 2005 -RRB- in POS tagging	nn_tagging_POS prep_in_Smith_tagging amod_Smith_2005 conj_and_Smith_Eisner pobj_by_Eisner pobj_by_Smith nn_extraction_information amod_Collins_1999 conj_and_Collins_Singer conj_and_Thelen_by prep_in_Thelen_extraction dep_Thelen_Singer dep_Thelen_Collins conj_and_Thelen_2002 conj_and_Thelen_Riloff agent_used_by agent_used_2002 agent_used_Riloff agent_used_Thelen prep_for_used_example auxpass_used_was nsubjpass_used_This ccomp_``_used
P07-1036	W02-1001	o	This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model -LRB- for details see -LRB- Roth 1999 -RRB- for the classi cation case and -LRB- Collins 2002 -RRB- for the structured case -RRB-	amod_case_structured det_case_the prep_for_Collins_case amod_Collins_2002 nn_case_cation nn_case_classi det_case_the dep_Roth_1999 conj_and_see_Collins prep_for_see_case dep_see_Roth prep_for_see_details det_model_the nn_pair_input/output det_pair_the agent_assigned_model prep_to_assigned_pair vmod_likelihood_assigned amod_likelihood_log dep_corresponds_Collins dep_corresponds_see prep_to_corresponds_likelihood nsubj_corresponds_sum dep_corresponds_case prep_in_corresponds_which amod_sum_linear det_sum_the conj_and_HMMs_CRFs rcmod_models_corresponds prep_such_as_models_CRFs prep_such_as_models_HMMs amod_models_generative conj_and_models_models amod_models_linear amod_models_discriminative prep_to_applies_models prep_to_applies_models preconj_applies_both nsubj_applies_decomposition det_decomposition_This
P07-1036	W02-1001	o	Therefore an increasing attention has been recently given to semi-supervised learning where large amounts of unlabeled data are used to improve the models learned from a small training set -LRB- Collins and Singer 1999 Thelen and Riloff 2002 -RRB-	appos_Thelen_2002 conj_and_Thelen_Riloff dep_Collins_Riloff dep_Collins_Thelen amod_Collins_1999 conj_and_Collins_Singer dep_set_Singer dep_set_Collins nn_set_training amod_set_small det_set_a prep_from_learned_set nsubj_learned_models det_models_the ccomp_improve_learned aux_improve_to xcomp_used_improve auxpass_used_are nsubjpass_used_amounts advmod_used_where amod_data_unlabeled prep_of_amounts_data amod_amounts_large rcmod_learning_used amod_learning_semi-supervised prep_to_given_learning advmod_given_recently auxpass_given_been aux_given_has nsubjpass_given_attention advmod_given_Therefore amod_attention_increasing det_attention_an ccomp_``_given
P07-1055	W02-1001	p	Weight averaging was also employed -LRB- Collins 2002 -RRB- which helped improve performance	dobj_improve_performance ccomp_helped_improve nsubj_helped_which dep_Collins_2002 dep_employed_helped dep_employed_Collins advmod_employed_also auxpass_employed_was nsubjpass_employed_averaging nn_averaging_Weight
P07-1055	W02-1001	o	Hidden Markov models -LRB- Rabiner 1989 -RRB- are one of the earliest structured learning algorithms which have recently been followedbydiscriminativelearningapproachessuch as conditional random fields -LRB- CRFs -RRB- -LRB- Lafferty et al. 2001 Sutton and McCallum 2006 -RRB- the structured perceptron -LRB- Collins 2002 -RRB- and its large-margin variants -LRB- Taskar et al. 2003 Tsochantaridis et al. 2004 McDonald et al. 2005 Daume III et al. 2006 -RRB-	dep_al._2006 nn_al._et dep_III_al. nn_III_Daume num_McDonald_2005 nn_McDonald_al. nn_McDonald_et conj_Tsochantaridis_III conj_Tsochantaridis_McDonald num_Tsochantaridis_2004 nn_Tsochantaridis_al. nn_Tsochantaridis_et dep_Taskar_Tsochantaridis appos_Taskar_2003 dep_Taskar_al. nn_Taskar_et amod_variants_large-margin poss_variants_its amod_Collins_2002 dep_perceptron_Collins amod_perceptron_structured det_perceptron_the dep_Sutton_2006 conj_and_Sutton_McCallum dep_Lafferty_Taskar conj_and_Lafferty_variants conj_and_Lafferty_perceptron dep_Lafferty_McCallum dep_Lafferty_Sutton appos_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et appos_fields_CRFs amod_fields_random amod_fields_conditional dep_followedbydiscriminativelearningapproachessuch_variants dep_followedbydiscriminativelearningapproachessuch_perceptron dep_followedbydiscriminativelearningapproachessuch_Lafferty prep_as_followedbydiscriminativelearningapproachessuch_fields advmod_been_followedbydiscriminativelearningapproachessuch dep_recently_been advmod_have_recently nsubj_have_which rcmod_algorithms_have nn_algorithms_learning amod_algorithms_structured amod_algorithms_earliest det_algorithms_the prep_of_one_algorithms cop_one_are nsubj_one_models appos_Rabiner_1989 dep_models_Rabiner nn_models_Markov nn_models_Hidden
P07-1055	W02-1001	o	In this work we will use structured linear classifiers -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_classifiers_Collins amod_classifiers_linear amod_classifiers_structured dobj_use_classifiers aux_use_will nsubj_use_we prep_in_use_work det_work_this ccomp_``_use
P07-1060	W02-1001	o	Following the framework of global linear models in -LRB- Collins 2002 -RRB- we cast this task as learning a mapping F from input verses x X to a text-reuse hypothesis y Y -LCB- epsilon1 -RCB-	appos_Y_epsilon1 prep_y_hypothesis_Y amod_hypothesis_text-reuse det_hypothesis_a conj_x_verses_X nn_verses_input nn_F_mapping det_F_a prep_to_learning_hypothesis prep_from_learning_X prep_from_learning_verses dobj_learning_F det_task_this prepc_as_cast_learning dobj_cast_task nsubj_cast_we prep_following_cast_framework amod_Collins_2002 dep_in_Collins amod_models_linear amod_models_global prep_framework_in prep_of_framework_models det_framework_the
P07-1062	W02-1001	o	We used a loglinear model with no Markov dependency between adjacent tags ,3 and trained the parameters of the model with the perceptron algorithm with averaging to control for over-training -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_over-training_Collins prep_for_control_over-training aux_control_to xcomp_averaging_control nn_algorithm_perceptron det_algorithm_the prep_with_model_algorithm det_model_the prep_of_parameters_model det_parameters_the dobj_trained_parameters nsubj_trained_We advmod_tags_,3 amod_tags_adjacent prep_between_dependency_tags nn_dependency_Markov neg_dependency_no amod_model_loglinear det_model_a prepc_with_used_averaging conj_and_used_trained prep_with_used_dependency dobj_used_model nsubj_used_We
P07-1062	W02-1001	o	See Collins -LRB- 2002 -RRB- for more details on this approach	det_approach_this prep_on_details_approach amod_details_more prep_for_Collins_details appos_Collins_2002 dobj_See_Collins
P07-1096	W02-1001	p	In our experiments we have used Averaged Perceptron -LRB- Collins 2002 Freund and Schapire 1999 -RRB- and Perceptron with margin -LRB- Krauth and Mezard 1987 -RRB- to improve performance	dobj_improve_performance aux_improve_to dep_Krauth_1987 conj_and_Krauth_Mezard dep_margin_Mezard dep_margin_Krauth prep_with_Perceptron_margin dep_Freund_1999 conj_and_Freund_Schapire dep_Collins_Schapire dep_Collins_Freund amod_Collins_2002 conj_and_Perceptron_Perceptron appos_Perceptron_Collins amod_Perceptron_Averaged vmod_used_improve dobj_used_Perceptron dobj_used_Perceptron aux_used_have nsubj_used_we prep_in_used_experiments poss_experiments_our
P07-1096	W02-1001	o	In -LRB- Daume III and Marcu 2005 -RRB- as well as other similar works -LRB- Collins 2002 Collins and Roark 2004 Shen and Joshi 2005 -RRB- only left-toright search was employed	auxpass_employed_was nsubjpass_employed_search dep_employed_Collins amod_search_left-toright advmod_search_only amod_Shen_2005 conj_and_Shen_Joshi num_Collins_2004 conj_and_Collins_Roark dep_Collins_Joshi dep_Collins_Shen conj_Collins_Roark conj_Collins_Collins amod_Collins_2002 dep_works_employed amod_works_similar amod_works_other cc_works_well prep_works_In dep_III_2005 conj_and_III_Marcu nn_III_Daume dep_In_Marcu dep_In_III
P07-1096	W02-1001	o	Following -LRB- Ratnaparkhi 1996 Collins 2002 Toutanova et al. 2003 Tsuruoka and Tsujii 2005 -RRB- 765 Feature Sets Templates Error % A Ratnaparkhis 3.05 B A + -LSB- t0 t1 -RSB- -LSB- t0 t1 t1 -RSB- -LSB- t0 t1 t2 -RSB- 2.92 C B + -LSB- t0 t2 -RSB- -LSB- t0 t2 -RSB- -LSB- t0 t2 w0 -RSB- -LSB- t0 t1 w0 -RSB- -LSB- t0 t1 w0 -RSB- -LSB- t0 t2 w0 -RSB- -LSB- t0 t2 t1 w0 -RSB- -LSB- t0 t1 t1 w0 -RSB- -LSB- t0 t1 t2 w0 -RSB- 2.84 D C + -LSB- t0 w1 w0 -RSB- -LSB- t0 w1 w0 -RSB- 2.78 E D + -LSB- t0 X = prefix or suffix of w0 -RSB- ,4 < | X | 9 2.72 Table 2 Experiments on the development data with beam width of 3 we cut the PTB into the training development and test sets as shown in Table 1	num_Table_1 prep_in_shown_Table mark_shown_as nn_sets_test conj_and_training_sets conj_and_training_development det_training_the det_PTB_the advcl_cut_shown prep_into_cut_sets prep_into_cut_development prep_into_cut_training dobj_cut_PTB nsubj_cut_we prep_of_width_3 nn_width_beam prep_with_data_width nn_data_development det_data_the rcmod_Experiments_cut prep_on_Experiments_data num_Table_2 dep_Table_2.72 num_Table_| number_2.72_9 dep_X_Table num_X_| dep_<_X dep_,4_< prep_of_suffix_w0 conj_or_prefix_suffix amod_prefix_= npadvmod_=_X conj_+_D_t0 nn_D_E num_D_2.78 appos_t0_w0 appos_t0_w1 nn_C_D num_C_2.84 nn_C_w0 appos_t0_w0 conj_+_t0_w1 conj_+_t0_t0 conj_+_t0_C conj_+_t0_t2 conj_+_t0_t1 appos_t0_w0 conj_t0_t1 conj_t0_t1 appos_t0_w0 conj_t0_t1 conj_t0_t2 appos_t0_w0 appos_t0_t2 appos_t0_w0 appos_t0_t1 appos_t0_w0 appos_t0_t1 appos_t0_w0 appos_t0_t2 appos_t0_t2 appos_t0_t2 conj_+_B_t0 nn_B_C num_B_2.92 dep_t0_Experiments amod_t0_,4 dep_t0_suffix dep_t0_prefix dep_t0_t0 dep_t0_D dep_t0_t0 dep_t0_w1 dep_t0_t0 dep_t0_C dep_t0_t2 dep_t0_t1 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_B appos_t0_t2 appos_t0_t1 appos_t0_t1 appos_t0_t1 appos_t0_t1 conj_+_A_t0 nn_A_B num_A_3.05 nn_A_Ratnaparkhis nn_A_A nn_A_% nn_A_Error nn_A_Templates dep_Sets_t0 dep_Sets_t0 dobj_Sets_t0 dobj_Sets_A nsubj_Sets_Feature prep_Sets_Following num_Feature_765 dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii num_Toutanova_2003 nn_Toutanova_al. nn_Toutanova_et num_Collins_2002 dep_Ratnaparkhi_Tsujii dep_Ratnaparkhi_Tsuruoka dep_Ratnaparkhi_Toutanova dep_Ratnaparkhi_Collins dep_Ratnaparkhi_1996 dep_Following_Ratnaparkhi
P07-1096	W02-1001	o	Following -LRB- Collins 2002 -RRB- we do not distinguish rare words	amod_words_rare dobj_distinguish_words neg_distinguish_not aux_distinguish_do nsubj_distinguish_we vmod_distinguish_Following amod_Collins_2002 dep_Following_Collins
P07-1096	W02-1001	o	Collins -LRB- 2002 -RRB- proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order	amod_order_left-to-right amod_order_traditional det_order_the nn_classification_sequence prep_in_solve_order dobj_solve_classification aux_solve_to amod_algorithm_learning prep_like_Perceptron_algorithm det_Perceptron_a xcomp_proposed_solve dobj_proposed_Perceptron nsubj_proposed_Collins appos_Collins_2002
P07-1096	W02-1001	o	766 System Beam Error % -LRB- Ratnaparkhi 1996 -RRB- 5 3.37 -LRB- Tsuruoka and Tsujii 2005 -RRB- 1 2.90 -LRB- Collins 2002 -RRB- 2.89 Guided Learning feature B 3 2.85 -LRB- Tsuruoka and Tsujii 2005 -RRB- all 2.85 -LRB- Gimenez and M`arquez 2004 -RRB- 2.84 -LRB- Toutanova et al. 2003 -RRB- 2.76 Guided Learning feature E 1 2.73 Guided Learning feature E 3 2.67 Table 4 Comparison with the previous works According to the experiments shown above we build our best system by using feature set E with beam width B = 3	dobj_=_3 nsubj_=_B nn_B_width nn_B_beam nn_E_set nn_E_feature prepc_with_using_= dobj_using_E amod_system_best poss_system_our prepc_by_build_using dobj_build_system nsubj_build_we advmod_shown_above vmod_experiments_shown det_experiments_the pobj_works_experiments prepc_according_to_works_to amod_works_previous det_works_the rcmod_Comparison_build prep_with_Comparison_works num_Table_4 num_Table_2.67 num_Table_3 nn_Table_E nn_Table_feature amod_Learning_Guided nn_Learning_E nn_Learning_feature number_2.73_1 num_E_2.73 amod_Learning_Guided num_Learning_2.76 nn_Learning_B nn_Learning_feature amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_Gimenez_2004 conj_and_Gimenez_M`arquez det_2.85_all dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii number_2.85_3 dep_B_Toutanova num_B_2.84 dep_B_M`arquez dep_B_Gimenez num_B_2.85 appos_B_Tsujii appos_B_Tsuruoka num_B_2.85 amod_Learning_Guided num_Learning_2.89 dep_Collins_2002 num_Collins_2.90 number_2.90_1 dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii num_Tsuruoka_3.37 number_3.37_5 dep_Ratnaparkhi_1996 dep_%_Comparison appos_%_Table appos_%_Learning dep_%_Learning dep_%_Learning dep_%_Collins dep_%_Tsujii dep_%_Tsuruoka dep_%_Ratnaparkhi nn_%_Error nn_%_Beam nn_%_System num_%_766 dep_``_%
P07-1096	W02-1001	o	We use a bidirectional search strategy -LRB- Woods 1976 Satta and Stock 1994 -RRB- and our algorithm is based on Perceptron learning -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_learning_Collins nn_learning_Perceptron prep_on_based_learning auxpass_based_is nsubjpass_based_algorithm poss_algorithm_our dep_Satta_1994 conj_and_Satta_Stock dep_Woods_Stock dep_Woods_Satta dep_Woods_1976 appos_strategy_Woods nn_strategy_search amod_strategy_bidirectional det_strategy_a conj_and_use_based dobj_use_strategy nsubj_use_We
P07-1104	W02-1001	p	In our experiments we used the Averaged Perceptron algorithm of Freund and Schapire -LRB- 1999 -RRB- a variation that has been shown to be more effective than the standard algorithm -LRB- Collins 2002 -RRB-	num_Collins_2002 appos_algorithm_Collins amod_algorithm_standard det_algorithm_the prep_than_effective_algorithm advmod_effective_more cop_effective_be aux_effective_to xcomp_shown_effective auxpass_shown_been aux_shown_has nsubjpass_shown_that rcmod_variation_shown det_variation_a appos_Schapire_1999 conj_and_Freund_Schapire appos_algorithm_variation prep_of_algorithm_Schapire prep_of_algorithm_Freund nn_algorithm_Perceptron nn_algorithm_Averaged det_algorithm_the dobj_used_algorithm nsubj_used_we prep_in_used_experiments poss_experiments_our
P07-1104	W02-1001	p	In addition the perceptron algorithm and its variants e.g. the voted or averaged perceptron is becoming increasingly popular due to their competitive performance simplicity in implementation and low computational cost in training -LRB- e.g. Collins 2002 -RRB-	num_Collins_2002 pobj_e.g._Collins prep_-LRB-_e.g. amod_cost_computational amod_cost_low prep_in_implementation_training conj_and_implementation_cost prep_in_simplicity_cost prep_in_simplicity_implementation amod_performance_competitive poss_performance_their advmod_popular_increasingly dep_becoming_simplicity prep_due_to_becoming_performance acomp_becoming_popular aux_becoming_is nsubj_becoming_e.g. nsubj_becoming_variants nsubj_becoming_algorithm prep_in_becoming_addition nsubj_averaged_the dobj_voted_perceptron conj_or_voted_averaged nsubj_voted_the poss_variants_its rcmod_algorithm_averaged rcmod_algorithm_voted conj_and_algorithm_e.g. conj_and_algorithm_variants nn_algorithm_perceptron det_algorithm_the ccomp_``_becoming
P07-1106	W02-1001	o	Liang -LRB- 2005 -RRB- uses the discriminative perceptron algorithm -LRB- Collins 2002 -RRB- to score whole character tag sequences finding the best candidate by the global score	amod_score_global det_score_the prep_by_candidate_score amod_candidate_best det_candidate_the dobj_finding_candidate nn_sequences_tag nn_sequences_character amod_sequences_whole dobj_score_sequences aux_score_to amod_Collins_2002 dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_discriminative det_algorithm_the xcomp_uses_finding vmod_uses_score dobj_uses_algorithm nsubj_uses_Liang appos_Liang_2005
P07-1106	W02-1001	o	Collins -LRB- 2002 -RRB- proposed the perceptron as an alternative to the CRF method for HMM-style taggers	amod_taggers_HMM-style prep_for_method_taggers nn_method_CRF det_method_the prep_to_alternative_method det_alternative_an det_perceptron_the prep_as_proposed_alternative dobj_proposed_perceptron nsubj_proposed_Collins appos_Collins_2002
P07-1106	W02-1001	o	Given an input sentence x the correct output segmentation F -LRB- x -RRB- satisfies F -LRB- x -RRB- = argmax yGEN -LRB- x -RRB- Score -LRB- y -RRB- where GEN -LRB- x -RRB- denotes the set of possible segmentations for an input sentence x consistent with notation from Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_from_notation_Collins prep_with_consistent_notation nn_x_sentence nn_x_input det_x_an amod_segmentations_possible amod_set_consistent prep_for_set_x prep_of_set_segmentations det_set_the dobj_denotes_set nsubj_denotes_GEN advmod_denotes_where appos_GEN_x rcmod_Score_denotes appos_Score_y nn_Score_x dep_yGEN_Score nn_yGEN_argmax dobj_=_yGEN amod_F_= appos_F_x dobj_satisfies_F nsubj_satisfies_F prep_satisfies_Given appos_F_x nn_F_segmentation nn_F_output amod_F_correct det_F_the nn_x_sentence nn_x_input det_x_an pobj_Given_x
P07-1106	W02-1001	o	The term global feature vector is used by Collins -LRB- 2002 -RRB- to distinguish between feature count vectors for whole sequences and the local feature vectors in ME tagging models which are Boolean valued vectors containing the indicator features for one element in the sequence	det_sequence_the prep_in_element_sequence num_element_one prep_for_features_element nn_features_indicator det_features_the dobj_containing_features vmod_vectors_containing amod_vectors_valued amod_vectors_Boolean cop_vectors_are nsubj_vectors_which rcmod_models_vectors nn_models_tagging nn_models_ME nn_vectors_feature amod_vectors_local det_vectors_the prep_in_sequences_models conj_and_sequences_vectors amod_sequences_whole nn_vectors_count nn_vectors_feature prep_for_distinguish_vectors prep_for_distinguish_sequences prep_between_distinguish_vectors aux_distinguish_to appos_Collins_2002 xcomp_used_distinguish agent_used_Collins auxpass_used_is nsubjpass_used_vector nn_vector_feature amod_vector_global nn_vector_term det_vector_The
P07-1106	W02-1001	o	Denote the global feature vector for segmented sentence y with -LRB- y -RRB- Rd where d is the total number of features in the model then Score -LRB- y -RRB- is computed by the dot product of vector -LRB- y -RRB- and a parameter vector Rd where i is the weight for the ith feature Score -LRB- y -RRB- = -LRB- y -RRB- 841 Inputs training examples -LRB- xi yi -RRB- Initialization set = 0 Algorithm for t = 1T i = 1N calculate zi = argmaxyGEN -LRB- xi -RRB- -LRB- y -RRB- if zi negationslash = yi = + -LRB- yi -RRB- -LRB- zi -RRB- Outputs Figure 1 the perceptron learning algorithm adapted from Collins -LRB- 2002 -RRB- The perceptron training algorithm is used to determine the weight values	nn_values_weight det_values_the dobj_determine_values aux_determine_to xcomp_used_determine auxpass_used_is nsubjpass_used_algorithm nn_algorithm_training nn_algorithm_perceptron det_algorithm_The nn_algorithm_Collins appos_Collins_2002 prep_from_adapted_algorithm vmod_algorithm_adapted amod_algorithm_learning nn_algorithm_perceptron det_algorithm_the dep_Figure_used num_Figure_1 dep_=_Outputs appos_=_zi conj_+_=_yi amod_yi_yi amod_yi_= dep_=_yi amod_negationslash_= dep_zi_Figure dep_zi_negationslash nn_y_argmaxyGEN appos_argmaxyGEN_xi dep_=_y amod_zi_= prep_if_calculate_zi dobj_calculate_zi nsubj_calculate_1N dep_calculate_= dep_calculate_i dep_calculate_1T dep_calculate_= nsubj_calculate_t mark_calculate_for num_Algorithm_0 dep_=_Algorithm dep_set_calculate amod_set_= dep_Initialization_set appos_xi_yi dep_examples_Initialization dep_examples_xi nn_examples_training num_Inputs_841 dep_=_Inputs dep_=_y npadvmod_=_Score appos_Score_y nn_feature_ith det_feature_the prep_for_weight_feature det_weight_the cop_weight_is nsubj_weight_i advmod_weight_where dep_Rd_= rcmod_Rd_weight nn_Rd_vector nn_Rd_parameter det_Rd_a appos_vector_y prep_of_product_vector nn_product_dot det_product_the conj_and_computed_Rd agent_computed_product auxpass_computed_is nsubjpass_computed_Score appos_Score_y advmod_Score_then det_model_the prep_in_number_model prep_of_number_features amod_number_total det_number_the cop_number_is nsubj_number_d advmod_number_where rcmod_Rd_number appos_Rd_y prep_with_y_Rd nn_y_sentence amod_y_segmented prep_for_vector_y nn_vector_feature amod_vector_global det_vector_the dep_Denote_examples parataxis_Denote_Rd parataxis_Denote_computed dobj_Denote_vector
P07-1106	W02-1001	o	Note that the algorithm from Collins -LRB- 2002 -RRB- was designed for discriminatively training an HMM-style tagger	amod_tagger_HMM-style det_tagger_an dobj_training_tagger advmod_training_discriminatively prepc_for_designed_training auxpass_designed_was nsubjpass_designed_algorithm mark_designed_that appos_Collins_2002 prep_from_algorithm_Collins det_algorithm_the ccomp_Note_designed
P07-1106	W02-1001	o	Despite the above differences since the theorems of convergence and their proof -LRB- Collins 2002 -RRB- are only dependent on the feature vectors and not on the source of the feature definitions the perceptron algorithm is applicable to the training of our CWS model	nn_model_CWS poss_model_our prep_of_training_model det_training_the prep_to_applicable_training cop_applicable_is nsubj_applicable_algorithm advmod_applicable_on advmod_applicable_dependent nn_algorithm_perceptron det_algorithm_the nn_definitions_feature det_definitions_the prep_of_source_definitions det_source_the pobj_on_source neg_on_not nn_vectors_feature det_vectors_the conj_and_dependent_on prep_on_dependent_vectors advmod_dependent_only ccomp_are_applicable prep_since_are_proof prep_since_are_theorems prep_despite_are_differences amod_Collins_2002 dep_proof_Collins poss_proof_their conj_and_theorems_proof prep_of_theorems_convergence det_theorems_the amod_differences_above det_differences_the
P07-1106	W02-1001	p	2.1 The averaged perceptron The averaged perceptron algorithm -LRB- Collins 2002 -RRB- was proposed as a way of reducing overfitting on the training data	nn_data_training det_data_the prep_on_reducing_data dobj_reducing_overfitting prepc_of_way_reducing det_way_a prep_as_proposed_way auxpass_proposed_was nsubjpass_proposed_averaged nsubjpass_proposed_The amod_Collins_2002 nn_algorithm_perceptron dobj_averaged_algorithm nsubj_averaged_The dep_perceptron_Collins rcmod_perceptron_averaged dobj_averaged_perceptron rcmod_2.1_proposed ccomp_``_2.1
P07-2019	W02-1001	p	4 Evaluation The purpose of our evaluation is to contrast our proposed feature based approach with a state-ofthe-art sequential learning technique -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_technique_Collins nn_technique_learning amod_technique_sequential amod_technique_state-ofthe-art det_technique_a prep_with_approach_technique amod_approach_based dep_feature_approach amod_feature_proposed poss_feature_our dobj_contrast_feature aux_contrast_to xcomp_is_contrast nsubj_is_purpose poss_evaluation_our prep_of_purpose_evaluation det_purpose_The rcmod_Evaluation_is num_Evaluation_4
P07-2019	W02-1001	o	For a sequential learning algorithm we make use of the Collins Perceptron Learner -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_Learner_Collins nn_Learner_Perceptron nn_Learner_Collins det_Learner_the prep_of_use_Learner dobj_make_use nsubj_make_we prep_for_make_algorithm nn_algorithm_learning amod_algorithm_sequential det_algorithm_a
P08-1067	W02-1001	p	In this work we use the averaged perceptron algorithm -LRB- Collins 2002 -RRB- since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods	dep_Boosting_methods conj_and_Boosting_MaxEnt prepc_than_faster_MaxEnt prepc_than_faster_Boosting prep_of_orders_magnitude advmod_simpler_faster conj_and_simpler_orders amod_simpler_much dep_algorithm_orders dep_algorithm_simpler amod_algorithm_online det_algorithm_an cop_algorithm_is nsubj_algorithm_it mark_algorithm_since amod_Collins_2002 dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the advcl_use_algorithm dobj_use_algorithm nsubj_use_we prep_in_use_work det_work_this
P08-1067	W02-1001	p	This averaging effect has been shown to reduce overfitting and produce much more stable results -LRB- Collins 2002 -RRB-	amod_Collins_2002 amod_results_stable advmod_stable_more advmod_stable_much dobj_produce_results dep_reduce_Collins conj_and_reduce_produce dobj_reduce_overfitting aux_reduce_to xcomp_shown_produce xcomp_shown_reduce auxpass_shown_been aux_shown_has nsubjpass_shown_effect nn_effect_averaging det_effect_This
P08-1068	W02-1001	p	To facilitate comparisons with previous work -LRB- McDonald et al. 2005b McDonald and Pereira 2006 -RRB- we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus .10 Czech word clusters were derived from the raw text section of the PDT 1.0 which contains about 39 million words of newswire text .11 We trained the parsers using the averaged perceptron -LRB- Freund and Schapire 1999 Collins 2002 -RRB- which represents a balance between strong performance and fast training times	nn_times_training amod_times_fast conj_and_performance_times amod_performance_strong prep_between_balance_times prep_between_balance_performance det_balance_a dobj_represents_balance nsubj_represents_which dep_Collins_2002 dep_Freund_Collins conj_and_Freund_1999 conj_and_Freund_Schapire rcmod_perceptron_represents appos_perceptron_1999 appos_perceptron_Schapire appos_perceptron_Freund amod_perceptron_averaged det_perceptron_the dobj_using_perceptron det_parsers_the xcomp_trained_using dobj_trained_parsers nsubj_trained_We num_text_.11 nn_text_newswire rcmod_words_trained prep_of_words_text num_words_million number_million_39 quantmod_million_about dobj_contains_words nsubj_contains_which num_PDT_1.0 det_PDT_the rcmod_section_contains prep_of_section_PDT nn_section_text amod_section_raw det_section_the prep_from_derived_section auxpass_derived_were nn_clusters_word nn_clusters_Czech num_clusters_.10 nn_clusters_corpus det_clusters_the prep_in_provided_clusters vmod_tags_provided nn_tags_speech prep_of_part_tags amod_part_automatically-assigned det_part_the dep_used_derived dobj_used_part advmod_used_also nsubj_used_we det_corpus_the prep_in_defined_corpus vmod_partition_defined amod_partition_training/development/test det_partition_the conj_and_used_used dobj_used_partition nsubj_used_we advcl_used_facilitate dep_McDonald_2006 conj_and_McDonald_Pereira dep_McDonald_Pereira dep_McDonald_McDonald appos_McDonald_2005b dep_McDonald_al. nn_McDonald_et amod_work_previous prep_with_comparisons_work dep_facilitate_McDonald dobj_facilitate_comparisons aux_facilitate_To
P08-1101	W02-1001	o	The training is performed by a single generalized perceptron -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_perceptron_Collins amod_perceptron_generalized amod_perceptron_single det_perceptron_a agent_performed_perceptron auxpass_performed_is nsubjpass_performed_training det_training_The
P08-1101	W02-1001	o	1 word w 2 word bigram w1w2 3 single-character word w 4 a word of length l with starting character c 5 a word of length l with ending character c 6 space-separated characters c1 and c2 7 character bigram c1c2 in any word 8 the first / last characters c1 / c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l with previous word w 14 a word of length l with next word w Table 1 Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system using the perceptron segmentation model from our previous work -LRB- Zhang and Clark 2007 -RRB- and the perceptron POS tagging model from Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_from_model_Collins amod_model_tagging nn_model_POS nn_model_perceptron det_model_the dep_Zhang_2007 conj_and_Zhang_Clark conj_and_work_model dep_work_Clark dep_work_Zhang amod_work_previous poss_work_our nn_model_segmentation nn_model_perceptron det_model_the prep_from_using_model prep_from_using_work dobj_using_model nn_system_baseline amod_system_two-stage det_system_a dobj_built_system nsubj_built_We rcmod_System_built nn_System_Baseline det_System_The vmod_segmentor_using dep_segmentor_System num_segmentor_2 dep_baseline_segmentor dep_the_baseline prep_for_templates_the nn_templates_Feature num_Table_1 nn_Table_w nn_Table_word amod_Table_next nn_l_length prep_with_word_Table prep_of_word_l det_word_a dep_w_word num_w_14 nn_w_word amod_w_previous nn_l_length dep_word_templates prep_with_word_w prep_of_word_l det_word_a dep_13_word dep_words_13 amod_words_consecutive num_words_two prep_of_c1_words conj_and_c1_c2 nn_c1_characters amod_c1_ending det_c1_the num_c1_12 dep_words_c2 dep_words_c1 amod_words_consecutive num_words_two prep_of_c1_words conj_and_c1_c2 dep_characters_c2 dep_characters_c1 amod_characters_starting det_characters_the dep_w_characters num_w_11 nn_w_word nn_c_character num_c_10 nn_c_c nn_c_character nn_w_word num_w_9 nn_w_word det_w_any prep_before_c2_w advmod_c2_immediately prep_before_c2_c advmod_c2_immediately prep_of_c2_w dep_c1_c2 nsubj_c1_characters dep_c1_first dep_c1_the dep_c1_8 dep_c1_word amod_characters_last det_word_any nn_c1c2_bigram nn_c1c2_character num_c1c2_7 nn_c1c2_c2 conj_and_c1_c1c2 dep_characters_c1c2 dep_characters_c1 amod_characters_space-separated num_characters_6 dep_c_characters nn_c_character prepc_in_ending_c1 dobj_ending_c nn_l_length prep_of_word_l det_word_a npadvmod_c_word num_c_5 prepc_with_character_ending dobj_character_c dep_starting_character nn_l_length prep_of_word_l det_word_a npadvmod_w_word num_w_4 nn_w_word amod_w_single-character num_w_3 prepc_with_w1w2_starting dobj_w1w2_w dep_bigram_w1w2 dep_word_bigram dep_2_word dep_w_2 dep_word_w dep_1_word ccomp_``_1
P08-1101	W02-1001	o	The features used by the POS tagger some of which are different to those from Collins -LRB- 2002 -RRB- and are specific to Chinese are shown in Table 2	num_Table_2 prep_in_shown_Table auxpass_shown_are nsubjpass_shown_features prep_to_specific_Chinese cop_specific_are nsubj_specific_some appos_Collins_2002 prep_from_those_Collins conj_and_different_specific prep_to_different_those cop_different_are nsubj_different_some prep_of_some_which nn_tagger_POS det_tagger_the agent_used_tagger rcmod_features_specific rcmod_features_different vmod_features_used det_features_The
P08-1101	W02-1001	o	Each element in vectorw gives a weight to its corresponding element in -LRB- y -RRB- which is the count of a particular feature over the whole sentence y We calculate the vectorw value by supervised learning using the averaged perceptron algorithm -LRB- Collins 2002 -RRB- given in Figure 1	num_Figure_1 prep_in_given_Figure amod_Collins_2002 appos_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the vmod_using_given dobj_using_algorithm amod_learning_supervised nn_value_vectorw det_value_the xcomp_calculate_using prep_by_calculate_learning dobj_calculate_value nsubj_calculate_We nn_y_sentence amod_y_whole det_y_the prep_over_feature_y amod_feature_particular det_feature_a prep_of_count_feature det_count_the cop_count_is nsubj_count_which rcmod_y_count prep_in_element_y amod_element_corresponding poss_element_its det_weight_a parataxis_gives_calculate prep_to_gives_element dobj_gives_weight nsubj_gives_element prep_in_element_vectorw det_element_Each
P08-1102	W02-1001	p	Another widely used discriminative method is the perceptron algorithm -LRB- Collins 2002 -RRB- which achieves comparable performance to CRFs with much faster training so we base this work on the perceptron	det_perceptron_the det_work_this prep_on_base_perceptron dobj_base_work nsubj_base_we mark_base_so advmod_training_faster amod_training_much prep_to_performance_CRFs amod_performance_comparable prep_with_achieves_training dobj_achieves_performance nsubj_achieves_which dep_Collins_2002 advcl_algorithm_base rcmod_algorithm_achieves appos_algorithm_Collins nn_algorithm_perceptron det_algorithm_the cop_algorithm_is nsubj_algorithm_method amod_method_discriminative amod_method_used det_method_Another advmod_used_widely
P08-1102	W02-1001	p	3 The Perceptron The perceptron algorithm introduced into NLP by Collins -LRB- 2002 -RRB- is a simple but effective discriminative training method	nn_method_training amod_method_discriminative amod_method_effective amod_method_simple det_method_a cop_method_is nsubj_method_algorithm conj_but_simple_effective appos_Collins_2002 agent_introduced_Collins prep_into_introduced_NLP vmod_algorithm_introduced nn_algorithm_perceptron det_algorithm_The nn_algorithm_Perceptron det_algorithm_The num_algorithm_3
P08-1102	W02-1001	o	The perceptron has been used in many NLP tasks such as POS tagging -LRB- Collins 2002 -RRB- Chinese word segmentation -LRB- Ng and Low 2004 Zhang and Clark 2007 -RRB- and so on	advmod_on_so num_Ng_2007 conj_and_Ng_Clark conj_and_Ng_Zhang conj_and_Ng_2004 conj_and_Ng_Low appos_segmentation_Clark appos_segmentation_Zhang appos_segmentation_2004 appos_segmentation_Low appos_segmentation_Ng nn_segmentation_word amod_segmentation_Chinese amod_Collins_2002 conj_and_tagging_on conj_and_tagging_segmentation dep_tagging_Collins nn_tagging_POS prep_such_as_tasks_on prep_such_as_tasks_segmentation prep_such_as_tasks_tagging nn_tasks_NLP amod_tasks_many prep_in_used_tasks auxpass_used_been aux_used_has nsubjpass_used_perceptron det_perceptron_The
P08-1102	W02-1001	o	3.2 Training Algorithm We adopt the perceptron training algorithm of Collins -LRB- 2002 -RRB- to learn a discriminative model mapping from inputs xX to outputs yY where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results	amod_results_labelled amod_results_corresponding prep_of_set_results det_set_the cop_set_is nsubj_set_Y nn_corpus_training det_corpus_the conj_and_set_set prep_in_set_corpus prep_of_set_sentences det_set_the cop_set_is nsubj_set_X advmod_set_where rcmod_yY_set rcmod_yY_set nn_yY_outputs nn_xX_inputs prep_from_mapping_xX nn_mapping_model amod_mapping_discriminative det_mapping_a prep_to_learn_yY dobj_learn_mapping aux_learn_to appos_Collins_2002 prep_of_algorithm_Collins nn_algorithm_training nn_algorithm_perceptron det_algorithm_the xcomp_adopt_learn dobj_adopt_algorithm nsubj_adopt_We rcmod_Algorithm_adopt nn_Algorithm_Training num_Algorithm_3.2 dep_``_Algorithm
P08-1102	W02-1001	p	899 To alleviate overfitting on the training examples we use the refinement strategy called averaged parameters -LRB- Collins 2002 -RRB- to the algorithm in Algorithm 1	num_Algorithm_1 prep_in_algorithm_Algorithm det_algorithm_the amod_Collins_2002 prep_to_averaged_algorithm dep_averaged_Collins dobj_averaged_parameters nsubj_averaged_strategy vmod_strategy_called nn_strategy_refinement det_strategy_the ccomp_use_averaged nsubj_use_we ccomp_use_899 nn_examples_training det_examples_the prep_on_alleviate_examples dobj_alleviate_overfitting aux_alleviate_To vmod_899_alleviate
P08-1103	W02-1001	o	Perceptron Learning a discriminative structure prediction model with a perceptron update was first proposed by Collins -LRB- 2002 -RRB-	appos_Collins_2002 prep_by_proposed_Collins dep_first_proposed nsubj_was_first nsubj_update_model det_perceptron_a prep_with_model_perceptron nn_model_prediction nn_model_structure amod_model_discriminative det_model_a dep_Learning_was rcmod_Learning_update nn_Learning_Perceptron
P08-1103	W02-1001	o	We view L2P as a tagging task that can be performed with a discriminative learning method such as the Perceptron HMM -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_HMM_Collins nn_HMM_Perceptron det_HMM_the prep_such_as_method_HMM nn_method_learning amod_method_discriminative det_method_a prep_with_performed_method auxpass_performed_be aux_performed_can nsubjpass_performed_that rcmod_task_performed amod_task_tagging det_task_a prep_as_view_task dobj_view_L2P nsubj_view_We
P09-1014	W02-1001	o	Given the training pairs any sequence predictor can be used for example a Conditional Random Field -LRB- CRF -RRB- -LRB- Lafferty et al. 2001 -RRB- or a structured perceptron -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_perceptron_Collins amod_perceptron_structured det_perceptron_a dep_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et appos_Field_CRF nn_Field_Random amod_Field_Conditional det_Field_a auxpass_used_be aux_used_can nsubjpass_used_predictor nn_predictor_sequence det_predictor_any conj_or_pairs_perceptron dep_pairs_Lafferty dep_pairs_Field prep_for_pairs_example rcmod_pairs_used nn_pairs_training det_pairs_the pobj_Given_perceptron pobj_Given_pairs ccomp_``_Given
P09-1032	W02-1001	o	This algorithm and its many variants are widely used in the computational linguistics community -LRB- Collins 2002a Collins and Duffy 2002 Collins 2002b Collins and Roark 2004 Henderson and Titov 2005 Viola and Narasimhan 2005 Cohen et al. 2004 Carreras et al. 2005 Shen and Joshi 2005 Ciaramita and Johnson 2003 -RRB-	appos_Ciaramita_2003 conj_and_Ciaramita_Johnson num_Shen_2005 conj_and_Shen_Joshi num_Carreras_2005 nn_Carreras_al. nn_Carreras_et num_Cohen_2004 nn_Cohen_al. nn_Cohen_et conj_and_Viola_Johnson conj_and_Viola_Ciaramita conj_and_Viola_Joshi conj_and_Viola_Shen conj_and_Viola_Carreras conj_and_Viola_Cohen conj_and_Viola_2005 conj_and_Viola_Narasimhan dep_Collins_Ciaramita dep_Collins_Shen dep_Collins_Carreras dep_Collins_Cohen dep_Collins_2005 dep_Collins_Narasimhan dep_Collins_Viola conj_and_Collins_2005 conj_and_Collins_Titov conj_and_Collins_Henderson conj_and_Collins_2004 conj_and_Collins_Roark conj_Collins_2002b num_Collins_2002 conj_and_Collins_Duffy dep_Collins_2005 dep_Collins_Titov dep_Collins_Henderson dep_Collins_2004 dep_Collins_Roark dep_Collins_Collins conj_Collins_Collins conj_Collins_Duffy conj_Collins_Collins appos_Collins_2002a dep_community_Collins nn_community_linguistics amod_community_computational det_community_the prep_in_used_community advmod_used_widely auxpass_used_are nsubjpass_used_variants nsubjpass_used_algorithm amod_variants_many poss_variants_its conj_and_algorithm_variants det_algorithm_This
P09-1032	W02-1001	o	predict correctly the label of a test instance xN +1 is bounded by 2N +1 EN +1 bracketleftbigd + D bracketrightbig2 where D = D -LRB- w -RRB- = radicalBigsummationtext N i = 12i This result is used to explain the convergence of weighted or voted perceptron algorithms -LRB- Collins 2002a -RRB-	appos_Collins_2002a dep_algorithms_Collins nn_algorithms_perceptron amod_algorithms_voted amod_algorithms_weighted conj_or_weighted_voted prep_of_convergence_algorithms det_convergence_the dobj_explain_convergence aux_explain_to xcomp_used_explain auxpass_used_is nsubjpass_used_bracketrightbig2 advmod_used_EN det_result_This dep_result_N nn_result_radicalBigsummationtext amod_result_= nn_result_D advmod_result_where dep_=_12i amod_i_= dep_N_i appos_D_w amod_D_= nn_D_D rcmod_bracketrightbig2_result nn_bracketrightbig2_D nn_bracketrightbig2_bracketleftbigd num_bracketrightbig2_+1 conj_+_bracketleftbigd_D rcmod_+1_used dep_2N_+1 agent_bounded_2N auxpass_bounded_is nsubjpass_bounded_test num_xN_+1 dep_instance_xN dep_test_instance rcmod_a_bounded prep_of_label_a det_label_the pobj_correctly_label advmod_predict_correctly
P09-1054	W02-1001	o	Following -LRB- Collins 2002 -RRB- we used sections 0-18 of the Wall Street Journal -LRB- WSJ -RRB- corpus for training sections 19-21 for development and sections 22-24 for final evaluation	amod_evaluation_final prep_for_sections_evaluation num_sections_22-24 prep_for_sections_development num_sections_19-21 nn_corpus_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the conj_and_sections_sections conj_and_sections_sections prep_for_sections_training prep_of_sections_corpus num_sections_0-18 dobj_used_sections dobj_used_sections dobj_used_sections nsubj_used_we vmod_used_Following amod_Collins_2002 dep_Following_Collins
P09-1055	W02-1001	o	Linear weights are assigned to each of the transducers features using an averaged perceptron for structure prediction -LRB- Collins 2002 -RRB-	amod_Collins_2002 nn_prediction_structure dep_perceptron_Collins prep_for_perceptron_prediction dobj_averaged_perceptron vmod_an_averaged dobj_using_an nn_features_transducers det_features_the prep_of_each_features xcomp_assigned_using prep_to_assigned_each auxpass_assigned_are nsubjpass_assigned_weights amod_weights_Linear
P09-1058	W02-1001	o	As reported in -LRB- Collins 2002 McDonald et al. 2005 -RRB- parameter averaging can effectively avoid overfitting	dobj_avoid_overfitting advmod_avoid_effectively aux_avoid_can nsubj_avoid_averaging advcl_avoid_reported nn_averaging_parameter num_McDonald_2005 nn_McDonald_al. nn_McDonald_et dep_Collins_McDonald amod_Collins_2002 dep_in_Collins prep_reported_in mark_reported_As
P09-1059	W02-1001	p	Several classification models can be adopted here however we choose the averaged perceptron algorithm -LRB- Collins 2002 -RRB- because of its simplicity and high accuracy	amod_accuracy_high conj_and_simplicity_accuracy poss_simplicity_its amod_Collins_2002 dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the prep_because_of_choose_accuracy prep_because_of_choose_simplicity dobj_choose_algorithm nsubj_choose_we parataxis_adopted_choose advmod_adopted_however advmod_adopted_here auxpass_adopted_be aux_adopted_can nsubjpass_adopted_models nn_models_classification amod_models_Several
P09-1059	W02-1001	o	It is an online training algorithm and has been successfully used in many NLP tasks such as POS tagging -LRB- Collins 2002 -RRB- parsing -LRB- Collins and Roark 2004 -RRB- Chinese word segmentation -LRB- Zhang and Clark 2007 Jiang et al. 2008 -RRB- and so on	advmod_on_so num_Jiang_2008 nn_Jiang_al. nn_Jiang_et dep_Zhang_Jiang num_Zhang_2007 conj_and_Zhang_Clark appos_segmentation_Clark appos_segmentation_Zhang nn_segmentation_word amod_segmentation_Chinese amod_Collins_2004 conj_and_Collins_Roark appos_parsing_Roark appos_parsing_Collins amod_Collins_2002 conj_and_tagging_on conj_and_tagging_segmentation conj_and_tagging_parsing dep_tagging_Collins nn_tagging_POS prep_such_as_tasks_on prep_such_as_tasks_segmentation prep_such_as_tasks_parsing prep_such_as_tasks_tagging nn_tasks_NLP amod_tasks_many prep_in_used_tasks advmod_used_successfully auxpass_used_been aux_used_has nsubjpass_used_It conj_and_algorithm_used nn_algorithm_training amod_algorithm_online det_algorithm_an cop_algorithm_is nsubj_algorithm_It
P09-1059	W02-1001	p	In addition the averaged parameters technology -LRB- Collins 2002 -RRB- is used to alleviate overfitting and achieve stable performance	amod_performance_stable dobj_achieve_performance conj_and_alleviate_achieve dobj_alleviate_overfitting aux_alleviate_to xcomp_used_achieve xcomp_used_alleviate auxpass_used_is nsubjpass_used_technology prep_in_used_addition amod_Collins_2002 dep_technology_Collins nn_technology_parameters amod_technology_averaged det_technology_the
P09-1104	W02-1001	p	On the Hansards data the simple averaging technique described by Collins -LRB- 2002 -RRB- yields a reasonable model	amod_model_reasonable det_model_a dobj_yields_model nsubj_yields_technique prep_on_yields_data appos_Collins_2002 agent_described_Collins vmod_technique_described nn_technique_averaging amod_technique_simple det_technique_the nn_data_Hansards det_data_the
P09-1109	W02-1001	o	3.3 Perceptron learning of feature weights As we saw above our model is a linear model with the global weight vector w acting as the coefficient vector and hence various existing techniques can be exploited to optimize w In this paper we use the averaged perceptron learning -LRB- Collins 2002 Freund and Schapire 1999 -RRB- to optimize w on a training corpus so that the system assigns the highest score to the correct coordination tree among all possible trees for each training sentence	nn_sentence_training det_sentence_each amod_trees_possible det_trees_all prep_among_tree_trees nn_tree_coordination amod_tree_correct det_tree_the amod_score_highest det_score_the prep_for_assigns_sentence prep_to_assigns_tree dobj_assigns_score nsubj_assigns_system mark_assigns_that advmod_assigns_so det_system_the nn_corpus_training det_corpus_a advcl_optimize_assigns prep_on_optimize_corpus dobj_optimize_w aux_optimize_to dep_Freund_1999 conj_and_Freund_Schapire vmod_Collins_optimize dep_Collins_Schapire dep_Collins_Freund appos_Collins_2002 dep_learning_Collins nn_learning_perceptron amod_learning_averaged det_learning_the dobj_use_learning nsubj_use_we det_paper_this dobj_optimize_w aux_optimize_to parataxis_exploited_use prep_in_exploited_paper xcomp_exploited_optimize auxpass_exploited_be aux_exploited_can nsubjpass_exploited_techniques nsubjpass_exploited_model amod_techniques_existing amod_techniques_various advmod_techniques_hence nn_vector_coefficient det_vector_the prep_as_acting_vector nsubj_acting_w nn_w_vector nn_w_weight amod_w_global det_w_the conj_and_model_techniques prepc_with_model_acting amod_model_linear det_model_a cop_model_is nsubj_model_model advcl_model_saw dep_model_learning poss_model_our advmod_saw_above nsubj_saw_we mark_saw_As nn_weights_feature prep_of_learning_weights nn_learning_Perceptron num_learning_3.3
P09-1110	W02-1001	o	Learning We model the problem of selecting the best derivation as a structured prediction problem -LRB- Johnson et al. 1999 Lafferty et al. 2001 Collins 2002 Taskar et al. 2004 -RRB-	num_Taskar_2004 nn_Taskar_al. nn_Taskar_et num_Collins_2002 dep_Lafferty_Taskar conj_Lafferty_Collins num_Lafferty_2001 nn_Lafferty_al. nn_Lafferty_et dep_Johnson_Lafferty amod_Johnson_1999 dep_Johnson_al. nn_Johnson_et nn_problem_prediction amod_problem_structured det_problem_a prep_as_derivation_problem amod_derivation_best det_derivation_the dobj_selecting_derivation prepc_of_problem_selecting det_problem_the dobj_model_problem nsubj_model_We dep_Learning_Johnson rcmod_Learning_model
W03-0422	W02-1001	o	Collins -LRB- 2002 -RRB- adapted the perceptron learning algorithm to tagging tasks via sentence-based global feedback	amod_feedback_global amod_feedback_sentence-based amod_tasks_tagging amod_algorithm_learning nn_algorithm_perceptron det_algorithm_the prep_via_adapted_feedback prep_to_adapted_tasks dobj_adapted_algorithm nsubj_adapted_Collins appos_Collins_2002
W03-1022	W02-1001	p	The averaged version of the perceptron -LRB- Collins 2002 -RRB- like the voted perceptron -LRB- Freund and Schapire 1999 -RRB- reduces the effect of over-training	prep_of_effect_over-training det_effect_the dobj_reduces_effect prep_like_reduces_perceptron nsubj_reduces_version dep_Freund_1999 conj_and_Freund_Schapire appos_perceptron_Schapire appos_perceptron_Freund amod_perceptron_voted det_perceptron_the amod_Collins_2002 appos_perceptron_Collins det_perceptron_the prep_of_version_perceptron amod_version_averaged det_version_The
W04-0824	W02-1001	o	The classifier consists of two components based on the averaged multiclass perceptron -LRB- Collins 2002 Crammer and Singer 2003 -RRB-	appos_Crammer_2003 conj_and_Crammer_Singer dep_Collins_Singer dep_Collins_Crammer amod_Collins_2002 dep_perceptron_Collins amod_perceptron_multiclass amod_perceptron_averaged det_perceptron_the prep_on_based_perceptron vmod_components_based num_components_two prep_of_consists_components nsubj_consists_classifier det_classifier_The
W04-0824	W02-1001	o	Here we used the averaged perceptron -LRB- Collins 2002 -RRB- where the weight matrix used to classify the test data is the average of all of the matrices posited during training i.e. a1 a62 a52 a49 a62 a49 a42a51a50a53a52 a1 a42 4.2 Multicomponent architecture Task specific and external training data are integrated with a two-component perceptron	amod_perceptron_two-component det_perceptron_a prep_with_integrated_perceptron auxpass_integrated_are nsubjpass_integrated_data nn_data_training amod_data_external conj_and_specific_integrated npadvmod_specific_Task nn_Task_architecture amod_Task_Multicomponent num_Task_4.2 nn_a42_a1 nn_a42_a42a51a50a53a52 nn_a42_a49 nn_a42_a62 nn_a42_a49 nn_a42_a52 nn_a42_a62 nn_a42_a1 pobj_i.e._a42 prep_during_posited_training vmod_matrices_posited det_matrices_the prep_of_all_matrices prep_of_average_all det_average_the cop_average_is nsubj_average_matrix advmod_average_where nn_data_test det_data_the dobj_classify_data aux_classify_to xcomp_used_classify vmod_matrix_used nn_matrix_weight det_matrix_the amod_Collins_2002 rcmod_perceptron_average dep_perceptron_Collins amod_perceptron_averaged det_perceptron_the dep_used_integrated dep_used_specific prep_used_i.e. dobj_used_perceptron nsubj_used_we advmod_used_Here ccomp_``_used
W04-2415	W02-1001	o	The learning algorithm follows the global strategy introduced in -LRB- Collins 2002 -RRB- and adapted in -LRB- Carreras and M`arquez 2004b -RRB- for partial parsing tasks	nn_tasks_parsing amod_tasks_partial appos_Carreras_2004b conj_and_Carreras_M`arquez prep_for_adapted_tasks prep_in_adapted_M`arquez prep_in_adapted_Carreras conj_and_Collins_adapted amod_Collins_2002 prep_in_introduced_adapted prep_in_introduced_Collins vmod_strategy_introduced amod_strategy_global det_strategy_the dobj_follows_strategy nsubj_follows_algorithm nn_algorithm_learning det_algorithm_The
W04-2415	W02-1001	o	The algorithm is essentially the same as the one introduced in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins prep_introduced_in vmod_one_introduced det_one_the prep_as_same_one det_same_the advmod_same_essentially cop_same_is nsubj_same_algorithm det_algorithm_The
W05-1302	W02-1001	o	12 As such we resort to an approximation Voted Perceptron training -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_training_Collins nn_training_Perceptron dobj_Voted_training det_approximation_an dep_resort_Voted prep_to_resort_approximation nsubj_resort_we nsubj_resort_12 prep_such_as_12_such
W06-1608	W02-1001	o	To set the weight vector w we train twenty averaged perceptrons -LRB- Collins 2002 -RRB- on different shuffles of data drawn from sections 0221 of the Penn Treebank	nn_Treebank_Penn det_Treebank_the prep_of_sections_Treebank num_sections_0221 prep_from_drawn_sections vmod_data_drawn prep_of_shuffles_data amod_shuffles_different dep_Collins_2002 appos_perceptrons_Collins prep_on_averaged_shuffles dobj_averaged_perceptrons nsubj_averaged_twenty ccomp_train_averaged nsubj_train_we advcl_train_set nn_w_vector nn_w_weight det_w_the dobj_set_w aux_set_To
W06-1628	W02-1001	o	To train the model we use the averaged perceptron algorithm described by Collins -LRB- 2002 -RRB-	appos_Collins_2002 agent_described_Collins vmod_algorithm_described nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the dobj_use_algorithm nsubj_use_we advcl_use_train det_model_the dobj_train_model aux_train_To
W06-1628	W02-1001	p	This combination of the perceptron algorithm with beam-search is similar to that described by Collins and Roark -LRB- 2004 -RRB- .5 The perceptron algorithm is a convenient choice because it converges quickly usually taking only a few iterations over the training set -LRB- Collins 2002 Collins and Roark 2004 -RRB-	amod_Collins_2004 conj_and_Collins_Roark dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 appos_set_Collins nn_set_training det_set_the prep_over_iterations_set amod_iterations_few det_iterations_a advmod_iterations_only dobj_taking_iterations advmod_taking_usually advmod_taking_quickly xcomp_converges_taking nsubj_converges_it mark_converges_because advcl_choice_converges amod_choice_convenient det_choice_a cop_choice_is nsubj_choice_similar nn_algorithm_perceptron det_algorithm_The num_algorithm_.5 dep_algorithm_2004 nn_algorithm_Roark conj_and_Collins_algorithm agent_described_algorithm agent_described_Collins vmod_that_described prep_to_similar_that cop_similar_is nsubj_similar_combination prep_with_algorithm_beam-search nn_algorithm_perceptron det_algorithm_the prep_of_combination_algorithm det_combination_This
W06-1670	W02-1001	p	The limitations of the generative approach to sequence tagging i. e. Hidden Markov Models have been overcome by discriminative approaches proposed in recent years -LRB- McCallum et al. 2000 Lafferty et al. 2001 Collins 2002 Altun et al. 2003 -RRB-	num_Altun_2003 nn_Altun_al. nn_Altun_et num_Collins_2002 dep_Lafferty_Altun conj_Lafferty_Collins num_Lafferty_2001 nn_Lafferty_al. nn_Lafferty_et dep_McCallum_Lafferty appos_McCallum_2000 dep_McCallum_al. nn_McCallum_et amod_years_recent prep_in_proposed_years vmod_approaches_proposed amod_approaches_discriminative dep_overcome_McCallum agent_overcome_approaches auxpass_overcome_been aux_overcome_have nsubjpass_overcome_limitations nn_Models_Markov nn_Models_Hidden nn_Models_e. nn_Models_i. appos_tagging_Models nn_tagging_sequence prep_to_approach_tagging amod_approach_generative det_approach_the prep_of_limitations_approach det_limitations_The ccomp_``_overcome
W06-1670	W02-1001	o	In this paper we apply perceptron trained HMMs originally proposed in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins prep_proposed_in advmod_proposed_originally vmod_HMMs_proposed amod_HMMs_trained nn_HMMs_perceptron dobj_apply_HMMs nsubj_apply_we prep_in_apply_paper det_paper_this
W06-1670	W02-1001	o	We use the perceptron algorithm for sequence tagging -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_tagging_Collins nn_tagging_sequence nn_algorithm_perceptron det_algorithm_the prep_for_use_tagging dobj_use_algorithm nsubj_use_We
W06-1670	W02-1001	o	To this extent we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model -LRB- HMM -RRB- based on that of Collins -LRB- 2002 -RRB- on the manually annotated Semcor corpus -LRB- Miller et al. 1993 -RRB-	amod_Miller_1993 dep_Miller_al. nn_Miller_et nn_corpus_Semcor amod_corpus_annotated advmod_corpus_manually det_corpus_the appos_Collins_2002 prep_of_that_Collins prep_on_based_corpus prep_on_based_that dep_Model_Miller vmod_Model_based appos_Model_HMM nn_Model_Markov nn_Model_Hidden amod_Model_discriminative det_Model_a dobj_train_Model nsubj_train_we nn_task_labeling nn_task_sequence det_task_a nn_problem_tagging amod_problem_supersense det_problem_the conj_and_cast_train prep_as_cast_task dobj_cast_problem nsubj_cast_we prep_to_cast_extent det_extent_this
W06-1672	W02-1001	o	Finally our modeling approaches follow the recent work on both local classifier-based modeling of complex learning problems -LRB- McCallum et al. 2000 Punyakanok and Roth 2001 -RRB- as well as global discriminative approaches based on CRFs -LRB- Lafferty et al. 2001 -RRB- SVM -LRB- Taskar et al. 2005 -RRB- and the Perceptron algorithm -LRB- Collins 2002 -RRB- that we used in our experiments	poss_experiments_our prep_in_used_experiments nsubj_used_we mark_used_that num_Collins_2002 appos_algorithm_Collins nn_algorithm_Perceptron det_algorithm_the dep_2005_al. nn_al._et num_Taskar_2005 appos_SVM_Taskar dep_2001_al. nn_al._et num_Lafferty_2001 conj_and_CRFs_algorithm conj_and_CRFs_SVM appos_CRFs_Lafferty prep_on_based_algorithm prep_on_based_SVM prep_on_based_CRFs vmod_approaches_based amod_approaches_discriminative amod_approaches_global dep_Punyakanok_2001 conj_and_Punyakanok_Roth num_al._2000 nn_al._et dep_McCallum_Roth dep_McCallum_Punyakanok dep_McCallum_al. nn_problems_learning amod_problems_complex prep_of_modeling_problems amod_modeling_classifier-based amod_modeling_local det_modeling_both prep_on_work_modeling amod_work_recent det_work_the dep_follow_used conj_and_follow_approaches dep_follow_McCallum dobj_follow_work dep_approaches_approaches dep_approaches_follow nsubj_approaches_modeling advmod_approaches_Finally poss_modeling_our
W06-1672	W02-1001	o	4 Global Transliteration Modeling In global transliteration modeling we directly model the agreement function between f and e We follow -LRB- Collins 2002 -RRB- and consider the global feature representation F * E * R d 613 Each global feature corresponds to a condition on the pair of strings	prep_of_pair_strings det_pair_the prep_on_condition_pair det_condition_a prep_to_corresponds_condition nsubj_corresponds_d amod_feature_global det_feature_Each npadvmod_d_feature num_d_613 nn_d_R dep_d_* dep_d_E nn_E_* nn_E_F dep_representation_corresponds nn_representation_feature amod_representation_global det_representation_the dobj_consider_representation nsubj_consider_function num_Collins_2002 conj_and_follow_consider dep_follow_Collins nsubj_follow_We dep_follow_e dep_follow_f advmod_follow_between nsubj_follow_function conj_and_f_e nn_function_agreement det_function_the ccomp_model_consider ccomp_model_follow advmod_model_directly nsubj_model_we nsubj_model_Modeling nn_modeling_transliteration amod_modeling_global prep_in_Modeling_modeling nn_Modeling_Transliteration amod_Modeling_Global num_Modeling_4 ccomp_``_model
W06-1672	W02-1001	o	-LRB- Collins 2002 -RRB- showed how to use the Voted Perceptron algorithm for learning W and we use it for learning the global transliteration model	nn_model_transliteration amod_model_global det_model_the dobj_learning_model prepc_for_use_learning dobj_use_it nsubj_use_we dobj_learning_W nn_algorithm_Perceptron nn_algorithm_Voted det_algorithm_the prepc_for_use_learning dobj_use_algorithm aux_use_to advmod_use_how conj_and_showed_use ccomp_showed_use nsubj_showed_2002 dep_2002_Collins
W06-2925	W02-1001	o	2.3 Perceptron Learning As learning algorithm we use Perceptron tailored for structured scenarios proposed by Collins -LRB- 2002 -RRB-	appos_Collins_2002 agent_proposed_Collins amod_scenarios_structured vmod_tailored_proposed prep_for_tailored_scenarios dep_use_tailored dobj_use_Perceptron nsubj_use_we nsubj_use_Learning amod_algorithm_learning prep_as_Learning_algorithm nn_Learning_Perceptron num_Learning_2.3
W06-2936	W02-1001	o	Using a variant of the voted perceptron -LRB- Collins 2002 Collins and Roark 2004 Crammer and Singer 2003 -RRB- we discriminatively trained our parser in an on-line fashion	amod_fashion_on-line det_fashion_an poss_parser_our prep_in_trained_fashion dobj_trained_parser advmod_trained_discriminatively nsubj_trained_we vmod_trained_Using dep_Crammer_2003 conj_and_Crammer_Singer num_Collins_2004 conj_and_Collins_Roark dep_Collins_Singer dep_Collins_Crammer dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 appos_perceptron_Collins amod_perceptron_voted det_perceptron_the prep_of_variant_perceptron det_variant_a dobj_Using_variant
W06-2936	W02-1001	o	3 Online Learning Again following -LRB- McDonald et al. 2005 -RRB- we have used the single best MIRA -LRB- Crammer and Singer 2003 -RRB- which is a variant of the voted perceptron -LRB- Collins 2002 Collins and Roark 2004 -RRB- for structured prediction	amod_prediction_structured prep_for_Collins_prediction appos_Collins_2004 conj_and_Collins_Roark dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 dep_perceptron_Collins amod_perceptron_voted det_perceptron_the prep_of_variant_perceptron det_variant_a cop_variant_is nsubj_variant_which dep_Crammer_2003 conj_and_Crammer_Singer rcmod_MIRA_variant appos_MIRA_Singer appos_MIRA_Crammer amod_MIRA_best amod_MIRA_single det_MIRA_the dobj_used_MIRA aux_used_have nsubj_used_we dep_used_Again amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et dep_following_McDonald vmod_Again_following nn_Again_Learning nn_Again_Online num_Again_3 advcl_``_used
W07-1202	W02-1001	p	Collins -LRB- 2002 -RRB- introduced the averaged perceptron as a way of reducing overfitting and it has been shown to perform better than the non-averaged version on a number of tasks	prep_of_number_tasks det_number_a prep_on_version_number amod_version_non-averaged det_version_the prep_than_better_version dobj_perform_better aux_perform_to xcomp_shown_perform auxpass_shown_been aux_shown_has nsubjpass_shown_it dobj_reducing_overfitting prepc_of_way_reducing det_way_a amod_perceptron_averaged det_perceptron_the conj_and_introduced_shown prep_as_introduced_way dobj_introduced_perceptron nsubj_introduced_Collins appos_Collins_2002
W07-1202	W02-1001	o	In the tagging domain Collins -LRB- 2002 -RRB- compared log-linear and perceptron training for HMM-style tagging based on dynamic programming	amod_programming_dynamic prep_on_based_programming amod_tagging_HMM-style vmod_training_based prep_for_training_tagging nn_training_perceptron nn_training_log-linear conj_and_log-linear_perceptron pobj_compared_training nsubj_compared_Collins prep_in_compared_domain appos_Collins_2002 nn_domain_tagging det_domain_the
W07-1202	W02-1001	p	We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks in particular Collins -LRB- 2002 -RRB- reported good performance for a perceptron tagger compared to a Maximum Entropy tagger	nn_tagger_Entropy nn_tagger_Maximum det_tagger_a nn_tagger_perceptron det_tagger_a prep_for_performance_tagger amod_performance_good pobj_reported_tagger prepc_compared_to_reported_to dobj_reported_performance nsubj_reported_Collins prep_in_reported_particular appos_Collins_2002 nn_tasks_NLP amod_tasks_other prep_on_performance_tasks amod_performance_good dobj_shown_performance aux_shown_has nsubj_shown_it mark_shown_because nn_algorithm_training det_algorithm_the det_perceptron_the parataxis_chose_reported advcl_chose_shown prep_for_chose_algorithm dobj_chose_perceptron nsubj_chose_We
W07-1202	W02-1001	o	Like Collins -LRB- 2002 -RRB- the decoder is the same for both the perceptron and the log-linear parsing models the only change is the method for setting the weights	det_weights_the dobj_setting_weights prepc_for_method_setting det_method_the cop_method_is nsubj_method_change amod_change_only det_change_the nn_models_parsing amod_models_log-linear det_models_the conj_and_perceptron_models det_perceptron_the preconj_perceptron_both parataxis_same_method prep_for_same_models prep_for_same_perceptron det_same_the cop_same_is nsubj_same_decoder prep_like_same_Collins det_decoder_the appos_Collins_2002 ccomp_``_same
W07-1202	W02-1001	o	3 Perceptron Training The parsing problem is to find a mapping from a set of sentences x ?? X to a set of parses y ?? Y We assume that the mapping F is represented through a feature vector -LRB- x y -RRB- ?? Rd and a parameter vector ?? Rd in the following way -LRB- Collins 2002 -RRB- F -LRB- x -RRB- = argmax y?GEN -LRB- x -RRB- -LRB- x y -RRB- -LRB- 1 -RRB- where GEN -LRB- x -RRB- denotes the set of possible parses for sentence x and -LRB- x y -RRB- = summationtexti ii -LRB- x y -RRB- is the inner product	amod_product_inner det_product_the cop_product_is nsubj_product_x dep_product_ii nn_product_summationtexti appos_x_y dep_=_product appos_x_y nn_x_sentence amod_parses_possible amod_set_= conj_and_set_x prep_for_set_x prep_of_set_parses det_set_the dobj_denotes_x dobj_denotes_set nsubj_denotes_GEN advmod_denotes_where appos_GEN_x dep_1_denotes appos_x_y dep_y?GEN_x appos_y?GEN_x nn_y?GEN_argmax dep_=_y?GEN dep_F_1 amod_F_= appos_F_x amod_Collins_2002 amod_way_following det_way_the prep_in_Rd_way num_Rd_?? nn_Rd_vector nn_Rd_parameter det_Rd_a num_Rd_?? appos_x_y dep_vector_F dep_vector_Collins conj_and_vector_Rd dep_vector_Rd dep_vector_x nn_vector_feature det_vector_a prep_through_represented_Rd prep_through_represented_vector auxpass_represented_is nsubjpass_represented_F mark_represented_that nn_F_mapping det_F_the ccomp_assume_represented nsubj_assume_We nn_Y_?? nn_Y_y nn_Y_parses prep_of_set_Y det_set_a nn_X_?? conj_x_sentences_X prep_of_set_X prep_of_set_sentences det_set_a prep_to_mapping_set prep_from_mapping_set det_mapping_a dobj_find_mapping aux_find_to parataxis_is_assume xcomp_is_find nsubj_is_Perceptron nn_problem_parsing det_problem_The dobj_Training_problem vmod_Perceptron_Training num_Perceptron_3 ccomp_``_is
W07-1202	W02-1001	o	1 Introduction A recent development in data-driven parsing is the use of discriminative training methods -LRB- Riezler et al. 2002 Taskar et al. 2004 Collins and Roark 2004 Turian and Melamed 2006 -RRB-	num_Collins_2004 conj_and_Collins_Roark dep_Taskar_2006 conj_and_Taskar_Melamed conj_and_Taskar_Turian conj_and_Taskar_Roark conj_and_Taskar_Collins num_Taskar_2004 nn_Taskar_al. nn_Taskar_et dep_Riezler_Melamed dep_Riezler_Turian dep_Riezler_Collins dep_Riezler_Taskar appos_Riezler_2002 dep_Riezler_al. nn_Riezler_et nn_methods_training amod_methods_discriminative dep_use_Riezler prep_of_use_methods det_use_the cop_use_is nsubj_use_Introduction amod_parsing_data-driven prep_in_development_parsing amod_development_recent det_development_A amod_Introduction_development num_Introduction_1
W07-1202	W02-1001	o	One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function -LRB- Johnson et al. 1999 Riezler et al. 2002 Clark and Curran 2004b Malouf and van Noord 2004 Miyao and Tsujii 2005 -RRB-	nn_Noord_van num_Clark_2004 conj_and_Clark_Noord conj_and_Clark_Malouf conj_and_Clark_2004b conj_and_Clark_Curran amod_Riezler_2005 conj_and_Riezler_Tsujii conj_and_Riezler_Miyao conj_and_Riezler_Noord conj_and_Riezler_Malouf conj_and_Riezler_2004b conj_and_Riezler_Curran conj_and_Riezler_Clark num_Riezler_2002 nn_Riezler_al. nn_Riezler_et dep_Johnson_Tsujii dep_Johnson_Miyao dep_Johnson_Clark dep_Johnson_Riezler amod_Johnson_1999 dep_Johnson_al. nn_Johnson_et nn_function_likelihood amod_function_conditional det_function_the dobj_maximise_function nn_model_parsing amod_model_log-linear det_model_a dep_use_Johnson conj_and_use_maximise dobj_use_model aux_use_to xcomp_is_maximise xcomp_is_use nsubj_is_approach amod_approach_popular num_approach_One
W07-1427	W02-1001	o	We use the adaptation of this algorithm to structure prediction first proposed by -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_by_Collins prep_proposed_by dep_first_proposed amod_prediction_first nn_prediction_structure det_algorithm_this prep_of_adaptation_algorithm det_adaptation_the prep_to_use_prediction dobj_use_adaptation nsubj_use_We
W07-2090	W02-1001	o	The tagger is a Hidden Markov Model trained with the perceptron algorithm introduced in -LRB- Collins 2002 -RRB- which applies Viterbi decoding and is regularized using averaging	dobj_using_averaging xcomp_regularized_using auxpass_regularized_is nsubjpass_regularized_which nn_decoding_Viterbi conj_and_applies_regularized dobj_applies_decoding nsubj_applies_which rcmod_Collins_regularized rcmod_Collins_applies dep_Collins_2002 prep_in_introduced_Collins vmod_algorithm_introduced nn_algorithm_perceptron det_algorithm_the prep_with_trained_algorithm vmod_Model_trained nn_Model_Markov nn_Model_Hidden det_Model_a cop_Model_is nsubj_Model_tagger det_tagger_The
W07-2216	W02-1001	o	These include the perceptron -LRB- Collins 2002 -RRB- and its large-margin variants -LRB- Crammer and Singer 2003 McDonald et al. 2005a -RRB-	appos_McDonald_2005a nn_McDonald_al. nn_McDonald_et dep_Crammer_McDonald dep_Crammer_2003 conj_and_Crammer_Singer appos_variants_Singer appos_variants_Crammer amod_variants_large-margin poss_variants_its amod_Collins_2002 conj_and_perceptron_variants dep_perceptron_Collins det_perceptron_the dobj_include_variants dobj_include_perceptron nsubj_include_These ccomp_``_include
W07-2217	W02-1001	o	We briefly describe the tagger -LRB- see -LRB- Ciaramita & Altun 2006 -RRB- for more details -RRB- a Hidden Markov Model trained with the perceptron algorithm introduced in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins prep_introduced_in vmod_algorithm_introduced nn_algorithm_perceptron det_algorithm_the prep_with_trained_algorithm vmod_Model_trained nn_Model_Markov nn_Model_Hidden det_Model_a amod_details_more dep_Ciaramita_2006 conj_and_Ciaramita_Altun prep_for_see_details dep_see_Altun dep_see_Ciaramita appos_tagger_Model dep_tagger_see det_tagger_the dobj_describe_tagger advmod_describe_briefly nsubj_describe_We ccomp_``_describe
W07-2217	W02-1001	o	To regularize the model we take as the final model the average of all weight vectors posited during training -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_training_Collins prep_during_posited_training vmod_vectors_posited nn_vectors_weight det_vectors_all prep_of_average_vectors det_average_the dep_model_average amod_model_final det_model_the prep_as_take_model nsubj_take_we rcmod_model_take det_model_the dobj_regularize_model aux_regularize_To ccomp_``_regularize
W08-0306	W02-1001	o	We delete all links in the set -LCB- a an the -RCB- -LCB- DF GI -RCB- from Ainitial as a preprocessing step .7 2.4 Perceptron Training We set the feature weights using a modified version of averaged perceptron learning with structured outputs -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_outputs_Collins amod_outputs_structured prep_with_learning_outputs nn_learning_perceptron dobj_averaged_learning prepc_of_version_averaged amod_version_modified det_version_a dobj_using_version nn_weights_feature det_weights_the xcomp_set_using dobj_set_weights nsubj_set_We ccomp_Training_set vmod_Perceptron_Training num_Perceptron_2.4 num_Perceptron_.7 dep_step_Perceptron amod_step_preprocessing det_step_a prep_as_DF_step prep_from_DF_Ainitial appos_DF_GI det_DF_the dep_an_DF ccomp_,_an dep_-LCB-_a dep_the_set prep_in_links_the det_links_all dobj_delete_links nsubj_delete_We ccomp_``_delete
W08-0306	W02-1001	p	When alignment quality stops increasing on the discriminative training set perceptron training ends .10 The weight vector returned by perceptron training is the average over the training set of all weight vectors seen during all iterations averaging reduces overfitting on the training set -LRB- Collins 2002 -RRB-	amod_Collins_2002 appos_set_Collins nn_set_training det_set_the prep_on_reduces_set dobj_reduces_overfitting dep_reduces_averaging det_iterations_all prep_during_seen_iterations vmod_vectors_seen nn_vectors_weight det_vectors_all prep_of_set_vectors dep_training_set det_training_the parataxis_average_reduces prep_over_average_training det_average_the cop_average_is advcl_average_stops nn_training_perceptron agent_returned_training vmod_vector_returned nn_vector_weight det_vector_The num_vector_.10 dobj_ends_vector nsubj_ends_training nn_training_perceptron rcmod_set_ends nn_set_training amod_set_discriminative det_set_the prep_on_increasing_set xcomp_stops_increasing nsubj_stops_quality advmod_stops_When nn_quality_alignment
W08-2102	W02-1001	o	1 Introduction In global linear models -LRB- GLMs -RRB- for structured prediction -LRB- e.g. -LRB- Johnson et al. 1999 Lafferty et al. 2001 Collins 2002 Altun et al. 2003 Taskar et al. 2004 -RRB- -RRB- the optimal label y for an input x is y = arg max yY -LRB- x -RRB- w f -LRB- x y -RRB- -LRB- 1 -RRB- where Y -LRB- x -RRB- is the set of possible labels for the input x f -LRB- x y -RRB- Rd is a feature vector that represents the pair -LRB- x y -RRB- and w is a parameter vector	nn_vector_parameter det_vector_a cop_vector_is nsubj_vector_w appos_x_y dep_pair_x det_pair_the dobj_represents_pair nsubj_represents_that rcmod_vector_represents nn_vector_feature det_vector_a cop_vector_is nsubj_vector_Rd dep_Rd_x dep_Rd_f appos_x_y nn_x_input det_x_the amod_labels_possible conj_and_set_vector conj_and_set_vector prep_for_set_x prep_of_set_labels det_set_the cop_set_is nsubj_set_Y advmod_set_where appos_Y_x dep_1_vector dep_1_vector dep_1_set appos_x_y dep_f_x nn_f_w nn_f_yY appos_yY_x nn_yY_max nn_yY_arg dep_=_f dep_y_1 amod_y_= nsubj_is_y nn_x_input det_x_an dep_y_is prep_for_y_x nn_y_label amod_y_optimal det_y_the num_Taskar_2004 nn_Taskar_al. nn_Taskar_et num_Altun_2003 nn_Altun_al. nn_Altun_et num_Collins_2002 dep_Lafferty_Taskar conj_Lafferty_Altun conj_Lafferty_Collins num_Lafferty_2001 nn_Lafferty_al. nn_Lafferty_et dep_Johnson_y dep_Johnson_Lafferty amod_Johnson_1999 dep_Johnson_al. nn_Johnson_et dep_,_Johnson dep_-LRB-_e.g. amod_prediction_structured appos_models_GLMs amod_models_linear amod_models_global prep_for_Introduction_prediction prep_in_Introduction_models num_Introduction_1
W08-2103	W02-1001	o	22 Table 5 Comparison with previous best results -LRB- Top POS tagging Bottom Text Chunking -RRB- POS tagging F = 1 Perceptron -LRB- Collins 2002 -RRB- 97.11 Dep	num_Dep_97.11 nn_Dep_Perceptron dep_Collins_2002 appos_Perceptron_Collins num_Perceptron_1 dep_=_Dep amod_F_= dobj_tagging_F vmod_POS_tagging conj_Chunking_POS dobj_Text_Chunking appos_tagging_Bottom nn_tagging_POS dep_Top_Text dep_Top_tagging amod_results_best amod_results_previous prep_with_Comparison_results dep_Table_Top dep_Table_Comparison num_Table_5 num_Table_22 dep_``_Table
W08-2103	W02-1001	o	We split the treebank into training -LRB- sections 0-18 -RRB- development -LRB- sections 1921 -RRB- and test -LRB- sections 22-24 -RRB- as in -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_in_Collins pcomp_as_in num_sections_22-24 num_sections_1921 appos_development_sections num_sections_0-18 dep_training_sections conj_and_training_test conj_and_training_development appos_training_sections det_treebank_the prep_split_as prep_into_split_test prep_into_split_development prep_into_split_training dobj_split_treebank nsubj_split_We
W08-2118	W02-1001	o	Therefore other machine learning techniques such as perceptron -LRB- Collins 2002 -RRB- could also be applied for this problem	det_problem_this prep_for_applied_problem auxpass_applied_be advmod_applied_also aux_applied_could nsubjpass_applied_techniques advmod_applied_Therefore amod_Collins_2002 dep_perceptron_Collins prep_such_as_techniques_perceptron nn_techniques_learning nn_techniques_machine amod_techniques_other
W08-2124	W02-1001	o	It combines online Peceptron learning -LRB- Collins 2002 -RRB- with a parsing model based on the Eisner algorithm -LRB- Eisner 1996 -RRB- extended so as to jointly assign syntactic and semantic labels	amod_labels_semantic conj_and_syntactic_labels dobj_assign_labels dobj_assign_syntactic advmod_assign_jointly aux_assign_to prepc_as_extended_assign advmod_extended_so amod_Eisner_1996 dep_algorithm_Eisner nn_algorithm_Eisner det_algorithm_the prep_on_based_algorithm vmod_model_based nn_model_parsing det_model_a dep_Collins_2002 vmod_learning_extended prep_with_learning_model appos_learning_Collins nn_learning_Peceptron amod_learning_online dobj_combines_learning nsubj_combines_It ccomp_``_combines
W08-2124	W02-1001	o	Our proposal is a first order linear model that relies on an online averaged Perceptron for learning -LRB- Collins 2002 -RRB- and an extended Eisner algorithm for the joint parsing inference	nn_inference_parsing amod_inference_joint det_inference_the prep_for_algorithm_inference nn_algorithm_Eisner amod_algorithm_extended det_algorithm_an amod_Collins_2002 conj_and_learning_algorithm dep_learning_Collins prep_for_averaged_algorithm prep_for_averaged_learning dobj_averaged_Perceptron nsubj_averaged_online det_online_an prepc_on_relies_averaged nsubj_relies_that rcmod_model_relies amod_model_linear nn_model_order amod_model_first det_model_a cop_model_is nsubj_model_proposal poss_proposal_Our
W09-0715	W02-1001	o	Later taggers have managed to improve Brills figures a little bit to just above 97 % on the Wall Street Journal corpus using Hidden Markov Models HMM and Conditional Random Fields CRF e.g. Collins -LRB- 2002 -RRB- and Toutanova et al.	nn_al._et nn_al._Toutanova conj_and_Collins_al. appos_Collins_2002 pobj_e.g._al. pobj_e.g._Collins appos_Fields_CRF nn_Fields_Random nn_Fields_Conditional conj_and_Models_Fields conj_and_Models_HMM nn_Models_Markov nn_Models_Hidden dobj_using_Fields dobj_using_HMM dobj_using_Models nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the num_%_97 prep_on_above_corpus pobj_above_% advmod_above_just pobj_to_above dep_bit_e.g. dep_bit_using prep_bit_to amod_bit_little det_bit_a dep_figures_bit csubj_figures_managed dobj_improve_Brills aux_improve_to xcomp_managed_improve aux_managed_have nsubj_managed_taggers advmod_managed_Later
W09-1110	W02-1001	o	The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_Perceptron_Collins amod_Perceptron_structured det_Perceptron_the prep_of_variant_Perceptron amod_variant_regularized det_variant_a cop_variant_is nsubj_variant_algorithm nn_task_classification det_task_the prep_of_stage_task det_stage_each prep_for_used_stage vmod_algorithm_used nn_algorithm_learning det_algorithm_The
W09-1119	W02-1001	o	NER is typically viewed as a sequential prediction problem the typical models include HMM -LRB- Rabiner 1989 -RRB- CRF -LRB- Lafferty et al. 2001 -RRB- and sequential application of Perceptron or Winnow -LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_Perceptron_Collins conj_or_Perceptron_Winnow prep_of_application_Winnow prep_of_application_Perceptron amod_application_sequential amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et appos_CRF_Lafferty dep_Rabiner_1989 conj_and_HMM_application conj_and_HMM_CRF appos_HMM_Rabiner dobj_include_application dobj_include_CRF dobj_include_HMM nsubj_include_models amod_models_typical det_models_the nn_problem_prediction amod_problem_sequential det_problem_a parataxis_viewed_include prep_as_viewed_problem advmod_viewed_typically auxpass_viewed_is nsubjpass_viewed_NER
W09-1210	W02-1001	o	-LRB- Collins 2002 -RRB-	amod_Collins_2002 dep_''_Collins
W09-1212	W02-1001	o	The parser is coupled with an on-line averaged perceptron -LRB- Collins 2002 -RRB- as the learning method	nn_method_learning det_method_the amod_Collins_2002 dep_perceptron_Collins prep_as_averaged_method dobj_averaged_perceptron dep_on-line_averaged det_on-line_an prep_with_coupled_on-line auxpass_coupled_is nsubjpass_coupled_parser det_parser_The ccomp_``_coupled
W09-2501	W02-1001	o	4An adaptation of the averaged perceptron algorithm -LRB- Collins 2002 -RRB- is used to tune the model parameters	nn_parameters_model det_parameters_the dobj_tune_parameters aux_tune_to xcomp_used_tune auxpass_used_is nsubjpass_used_adaptation amod_Collins_2002 dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the prep_of_adaptation_algorithm nn_adaptation_4An
C04-1190	W02-1002	p	-LRB- 1999 -RRB- Pedersen -LRB- 2001 -RRB- Yarowsky and Florian -LRB- 2002 -RRB- -RRB- as well as maximum entropy models -LRB- e.g. Dang and Palmer -LRB- 2002 -RRB- Klein and Manning -LRB- 2002 -RRB- -RRB- in particular have shown a large degree of success for WSD and have established challenging state-of-the-art benchmarks	amod_benchmarks_state-of-the-art amod_benchmarks_challenging dobj_established_benchmarks aux_established_have nsubj_established_Dang prep_for_success_WSD prep_of_degree_success amod_degree_large det_degree_a conj_and_shown_established dobj_shown_degree aux_shown_have prep_in_shown_particular nsubj_shown_Manning nsubj_shown_Klein nsubj_shown_Palmer nsubj_shown_Dang advmod_shown_e.g. appos_Manning_2002 appos_Palmer_2002 conj_and_Dang_Manning conj_and_Dang_Klein conj_and_Dang_Palmer dep_models_established dep_models_shown amod_models_entropy amod_models_maximum appos_Florian_2002 conj_and_Yarowsky_Florian conj_and_Pedersen_models appos_Pedersen_Florian appos_Pedersen_Yarowsky appos_Pedersen_2001 dep_1999_models dep_1999_Pedersen dep_''_1999
D07-1007	W02-1002	o	The second model is a maximum entropy model -LRB- Jaynes 1978 -RRB- since Klein and Manning -LRB- Klein and Manning 2002 -RRB- found that this model yielded higher accuracy than nave Bayes in a subsequent comparison of WSD performance	nn_performance_WSD prep_of_comparison_performance amod_comparison_subsequent det_comparison_a nn_Bayes_nave prep_than_accuracy_Bayes amod_accuracy_higher prep_in_yielded_comparison dobj_yielded_accuracy nsubj_yielded_model mark_yielded_that det_model_this ccomp_found_yielded nsubj_found_Manning nsubj_found_Klein mark_found_since amod_Klein_2002 conj_and_Klein_Manning dep_Klein_Manning dep_Klein_Klein conj_and_Klein_Manning amod_Jaynes_1978 advcl_model_found dep_model_Jaynes nn_model_entropy nn_model_maximum det_model_a cop_model_is nsubj_model_model amod_model_second det_model_The
H05-1058	W02-1002	o	Previous authors have used numerous HMM-based models -LRB- Banko and Moore 2004 Collins 2002 Lee et al. 2000 Thede and Harper 1999 -RRB- and other types of networks including maximum entropy models -LRB- Ratnaparkhi 1996 -RRB- conditional Markov models -LRB- Klein and Manning 2002 McCallum et al. 2000 -RRB- conditional random elds -LRB- CRF -RRB- -LRB- Lafferty et al. 2001 -RRB- and cyclic dependency networks -LRB- Toutanova et al. 2003 -RRB-	amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_networks_Toutanova nn_networks_dependency amod_networks_cyclic amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et dep_elds_Lafferty appos_elds_CRF amod_elds_random amod_elds_conditional num_McCallum_2000 nn_McCallum_al. nn_McCallum_et dep_Klein_McCallum amod_Klein_2002 conj_and_Klein_Manning appos_models_Manning appos_models_Klein nn_models_Markov amod_models_conditional dep_Ratnaparkhi_1996 conj_and_models_networks conj_and_models_elds conj_and_models_models appos_models_Ratnaparkhi nn_models_entropy nn_models_maximum prep_including_networks_networks prep_including_networks_elds prep_including_networks_models prep_including_networks_models prep_of_types_networks amod_types_other conj_and_Thede_1999 conj_and_Thede_Harper num_Lee_2000 nn_Lee_al. nn_Lee_et num_Collins_2002 dep_Banko_1999 dep_Banko_Harper dep_Banko_Thede conj_and_Banko_Lee conj_and_Banko_Collins conj_and_Banko_2004 conj_and_Banko_Moore conj_and_models_types appos_models_Lee appos_models_Collins appos_models_2004 appos_models_Moore appos_models_Banko amod_models_HMM-based amod_models_numerous dobj_used_types dobj_used_models aux_used_have nsubj_used_authors amod_authors_Previous
I08-1036	W02-1002	o	However Klein and Manning -LRB- 2002 -RRB- showed that for natural language and text processing tasks conditional models are usually better than joint likelihood models	nn_models_likelihood amod_models_joint prep_than_better_models advmod_better_usually cop_better_are nsubj_better_models prep_for_better_tasks prep_for_better_language mark_better_that amod_models_conditional nn_tasks_processing nn_tasks_text conj_and_language_tasks amod_language_natural ccomp_showed_better nsubj_showed_Manning nsubj_showed_Klein advmod_showed_However appos_Manning_2002 conj_and_Klein_Manning
J03-4003	W02-1002	o	Similar observations have been made in the context of tagging problems using maximum-entropy models -LRB- Lafferty McCallum and Pereira 2001 Klein and Manning 2002 -RRB-	num_Manning_2002 conj_and_Klein_Manning num_Pereira_2001 dep_Lafferty_Manning dep_Lafferty_Klein conj_and_Lafferty_Pereira conj_and_Lafferty_McCallum appos_models_Pereira appos_models_McCallum appos_models_Lafferty amod_models_maximum-entropy dobj_using_models amod_problems_tagging prep_of_context_problems det_context_the xcomp_made_using prep_in_made_context auxpass_made_been aux_made_have nsubjpass_made_observations amod_observations_Similar
J03-4003	W02-1002	o	-LRB- 1994 -RRB- and Magerman -LRB- 1995 -RRB- can suffer from very similar problems to the label bias or observation bias problem observed in tagging models as described in Lafferty McCallum and Pereira -LRB- 2001 -RRB- and Klein and Manning -LRB- 2002 -RRB-	appos_Manning_2002 dep_Pereira_2001 conj_and_Lafferty_Manning conj_and_Lafferty_Klein conj_and_Lafferty_Pereira conj_and_Lafferty_McCallum prep_in_described_Manning prep_in_described_Klein prep_in_described_Pereira prep_in_described_McCallum prep_in_described_Lafferty mark_described_as amod_models_tagging prep_in_observed_models nn_problem_bias nn_problem_observation vmod_bias_observed conj_or_bias_problem nn_bias_label det_bias_the amod_problems_similar advmod_similar_very advcl_suffer_described prep_to_suffer_problem prep_to_suffer_bias prep_from_suffer_problems aux_suffer_can nsubj_suffer_Magerman nsubj_suffer_1994 appos_Magerman_1995 conj_and_1994_Magerman
J07-4003	W02-1002	o	That some model structures work better than others at real NLP tasks was discussed by Johnson -LRB- 2001 -RRB- and Klein and Manning -LRB- 2002 -RRB-	appos_Manning_2002 conj_and_Klein_Manning conj_and_Johnson_Manning conj_and_Johnson_Klein appos_Johnson_2001 agent_discussed_Klein agent_discussed_Johnson auxpass_discussed_was nsubjpass_discussed_That nn_tasks_NLP amod_tasks_real prep_than_better_others prep_at_work_tasks acomp_work_better nsubj_work_structures nn_structures_model det_structures_some rcmod_That_work
N03-1033	W02-1002	o	For example consider a case of observation bias -LRB- Klein and Manning 2002 -RRB- for a first-order left-toright CMM	amod_CMM_left-toright amod_CMM_first-order det_CMM_a dep_Klein_2002 conj_and_Klein_Manning prep_for_bias_CMM appos_bias_Manning appos_bias_Klein nn_bias_observation prep_of_case_bias det_case_a dobj_consider_case prep_for_consider_example
P04-1013	W02-1002	o	In the context of part-of-speech tagging Klein and Manning -LRB- 2002 -RRB- argue for the same distinctions made here between discriminative models and discriminative training criteria and come to the same conclusions	amod_conclusions_same det_conclusions_the prep_to_come_conclusions nn_criteria_training amod_criteria_discriminative conj_and_models_criteria amod_models_discriminative prep_between_made_criteria prep_between_made_models advmod_made_here vmod_distinctions_made amod_distinctions_same det_distinctions_the conj_and_argue_come prep_for_argue_distinctions prep_in_argue_context appos_Manning_2002 conj_and_tagging_Manning conj_and_tagging_Klein amod_tagging_part-of-speech prep_of_context_Manning prep_of_context_Klein prep_of_context_tagging det_context_the
P04-1013	W02-1002	n	While both -LRB- Johnson 2001 -RRB- and -LRB- Klein and Manning 2002 -RRB- propose models which use the parameters of the generative model but train to optimize a discriminative criteria neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing	nn_parsing_coverage amod_parsing_broad prep_for_used_parsing auxpass_used_be aux_used_to xcomp_enough_used dep_tractable_enough advmod_tractable_computationally cop_tractable_are nsubj_tractable_which rcmod_algorithms_tractable nn_algorithms_training dobj_proposes_algorithms dep_neither_proposes dep_,_neither amod_criteria_discriminative det_criteria_a dobj_optimize_criteria aux_optimize_to vmod_train_optimize amod_model_generative det_model_the prep_of_parameters_model det_parameters_the conj_but_use_train dobj_use_parameters nsubj_use_which rcmod_models_train rcmod_models_use dobj_propose_models nsubj_propose_Klein nsubj_propose_both mark_propose_While dep_Klein_2002 conj_and_Klein_Manning dep_Johnson_2001 conj_and_both_Manning conj_and_both_Klein appos_both_Johnson advcl_``_propose
P04-1013	W02-1002	o	In this form the distinction between our two models is sometimes referred to as \ joint versus conditional -LRB- Johnson 2001 Klein and Manning 2002 -RRB- rather than \ generative versus discriminative -LRB- Ng and Jordan 2002 -RRB-	num_Ng_2002 conj_and_Ng_Jordan conj_versus_generative_discriminative amod_\_discriminative amod_\_generative num_Klein_2002 conj_and_Klein_Manning dep_Johnson_Manning dep_Johnson_Klein amod_Johnson_2001 conj_versus_joint_conditional conj_negcc_\_\ appos_\_Johnson amod_\_conditional amod_\_joint dep_referred_Jordan dep_referred_Ng prep_as_to_referred_\ prep_as_to_referred_\ advmod_referred_sometimes auxpass_referred_is nsubjpass_referred_distinction prep_in_referred_form num_models_two poss_models_our prep_between_distinction_models det_distinction_the det_form_this
P04-1013	W02-1002	o	Klein and Manning -LRB- 2002 -RRB- argue that these results show a pattern where discriminative probability models are inferior to generative probability models but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria	nn_criteria_optimization amod_criteria_discriminative det_criteria_a conj_and_model_training nn_model_probability amod_model_generative det_model_a pobj_keeping_criteria prepc_according_to_keeping_to dobj_keeping_training dobj_keeping_model agent_achieved_keeping auxpass_achieved_be aux_achieved_can nsubjpass_achieved_improvements mark_achieved_that nn_models_probability amod_models_generative prep_to_inferior_models cop_inferior_are nsubj_inferior_models advmod_inferior_where nn_models_probability amod_models_discriminative rcmod_pattern_inferior det_pattern_a conj_but_show_achieved dobj_show_pattern nsubj_show_results mark_show_that det_results_these ccomp_argue_achieved ccomp_argue_show nsubj_argue_Manning nsubj_argue_Klein appos_Manning_2002 conj_and_Klein_Manning
P04-1081	W02-1002	o	-LRB- 1999 -RRB- Pedersen -LRB- 2001 -RRB- Yarowsky and Florian -LRB- 2002 -RRB- -RRB- as well as maximum entropy models -LRB- e.g. Dang and Palmer -LRB- 2002 -RRB- Klein and Manning -LRB- 2002 -RRB- -RRB-	appos_Manning_2002 appos_Palmer_2002 conj_and_Dang_Manning conj_and_Dang_Klein conj_and_Dang_Palmer dep_e.g._Manning dep_e.g._Klein dep_e.g._Palmer dep_e.g._Dang dep_models_e.g. amod_models_entropy amod_models_maximum appos_Florian_2002 conj_and_Yarowsky_Florian conj_and_Pedersen_models appos_Pedersen_Florian appos_Pedersen_Yarowsky appos_Pedersen_2001 dep_1999_models dep_1999_Pedersen dep_''_1999
P04-1081	W02-1002	o	However the maximum entropy -LRB- Jaynes 1978 -RRB- was found to yield higher accuracy than nave Bayes in a subsequent comparison by Klein and Manning -LRB- 2002 -RRB- who used a different subset of either Senseval-1 or Senseval-2 English lexical sample data	nn_data_sample amod_data_lexical amod_data_English nn_data_Senseval-2 conj_or_Senseval-1_data preconj_Senseval-1_either prep_of_subset_data prep_of_subset_Senseval-1 amod_subset_different det_subset_a dobj_used_subset nsubj_used_who appos_Manning_2002 rcmod_Klein_used conj_and_Klein_Manning prep_by_comparison_Manning prep_by_comparison_Klein amod_comparison_subsequent det_comparison_a nn_Bayes_nave prep_than_accuracy_Bayes amod_accuracy_higher prep_in_yield_comparison dobj_yield_accuracy aux_yield_to xcomp_found_yield auxpass_found_was nsubjpass_found_entropy advmod_found_However amod_Jaynes_1978 dep_entropy_Jaynes nn_entropy_maximum det_entropy_the
P04-1081	W02-1002	o	The models in the comparative study by Klein and Manning -LRB- 2002 -RRB- did not include such features and so again for consistency of comparison we experimentally verified that our maximum entropy model -LRB- a -RRB- consistently yielded higher scores than when the features were not used and -LRB- b -RRB- consistently yielded higher scores than nave Bayes using the same features in agreement with Klein and Manning -LRB- 2002 -RRB-	appos_Manning_2002 conj_and_Klein_Manning prep_with_agreement_Manning prep_with_agreement_Klein amod_features_same det_features_the dobj_using_features nn_Bayes_nave prep_than_scores_Bayes amod_scores_higher dobj_yielded_scores advmod_yielded_consistently dep_yielded_b nsubjpass_yielded_features xcomp_used_using conj_and_used_yielded neg_used_not auxpass_used_were nsubjpass_used_features advmod_used_when det_features_the prepc_than_scores_yielded prepc_than_scores_used amod_scores_higher prep_in_yielded_agreement dobj_yielded_scores advmod_yielded_consistently dep_yielded_a dep_model_yielded nn_model_entropy nn_model_maximum poss_model_our prep_that_verified_model advmod_verified_experimentally nsubj_verified_we prep_for_verified_consistency advmod_verified_again advmod_verified_so prep_of_consistency_comparison amod_features_such conj_and_include_verified dobj_include_features neg_include_not aux_include_did nsubj_include_models appos_Manning_2002 conj_and_Klein_Manning amod_study_comparative det_study_the prep_by_models_Manning prep_by_models_Klein prep_in_models_study det_models_The
P05-1044	W02-1002	o	Klein and Manning -LRB- 2002 -RRB- argue for CL on grounds of accuracy but see also Johnson -LRB- 2001 -RRB-	appos_Johnson_2001 dep_also_Johnson advmod_see_also nsubj_see_Klein prep_of_grounds_accuracy prep_on_CL_grounds conj_but_argue_see prep_for_argue_CL nsubj_argue_Manning nsubj_argue_Klein appos_Manning_2002 conj_and_Klein_Manning
P05-1044	W02-1002	o	joint likelihood -LRB- JL -RRB- productdisplay i p parenleftBig xi yi | vector parenrightBig conditional likelihood -LRB- CL -RRB- productdisplay i p parenleftBig yi | xi vector parenrightBig classification accuracy -LRB- Juang and Katagiri 1992 -RRB- summationdisplay i -LRB- yi y -LRB- xi -RRB- -RRB- expected classification accuracy -LRB- Klein and Manning 2002 -RRB- summationdisplay i p parenleftBig yi | xi vector parenrightBig negated boosting loss -LRB- Collins 2000 -RRB- summationdisplay i p parenleftBig yi | xi vector parenrightBig1 margin -LRB- Crammer and Singer 2001 -RRB- s.t. bardbl vectorbardbl 1 i y negationslash = yi vector -LRB- vectorf -LRB- xi yi -RRB- vectorf -LRB- xi y -RRB- -RRB- expected local accuracy -LRB- Altun et al. 2003 -RRB- productdisplay i productdisplay j p parenleftBig lscriptj -LRB- Y -RRB- = lscriptj -LRB- yi -RRB- | xi vector parenrightBig Table 1 Various supervised training criteria	nn_criteria_training amod_criteria_supervised amod_criteria_Various num_Table_1 nn_Table_parenrightBig nn_Table_vector appos_xi_Table num_xi_| nn_xi_lscriptj appos_lscriptj_yi dep_=_xi amod_lscriptj_= appos_lscriptj_Y nn_lscriptj_parenleftBig nn_lscriptj_p nn_lscriptj_j nn_lscriptj_productdisplay nn_lscriptj_i nn_lscriptj_productdisplay dep_Altun_criteria dep_Altun_lscriptj amod_Altun_2003 dep_Altun_al. nn_Altun_et dep_accuracy_Altun amod_accuracy_local dobj_expected_accuracy nsubj_expected_y advmod_expected_i appos_xi_y dep_vectorf_xi nn_vectorf_vectorf appos_xi_yi dep_vectorf_xi nn_vectorf_vector appos_yi_vectorf dep_=_yi amod_negationslash_= dep_y_negationslash num_vectorbardbl_1 nn_vectorbardbl_bardbl dobj_s.t._vectorbardbl nsubj_s.t._xi dep_Crammer_2001 conj_and_Crammer_Singer appos_margin_Singer appos_margin_Crammer nn_margin_parenrightBig1 nn_margin_vector appos_xi_margin num_xi_| rcmod_yi_s.t. nn_yi_parenleftBig nn_yi_p dep_i_yi dep_summationdisplay_i amod_Collins_summationdisplay dep_Collins_2000 appos_loss_Collins dobj_boosting_loss xcomp_negated_boosting nsubj_negated_xi nn_parenrightBig_vector appos_xi_parenrightBig num_xi_| dep_yi_xi nn_yi_parenleftBig nn_yi_p dep_i_yi dep_summationdisplay_i dep_summationdisplay_Manning dep_summationdisplay_Klein dep_Klein_2002 conj_and_Klein_Manning amod_accuracy_summationdisplay nn_accuracy_classification dobj_expected_accuracy advmod_expected_i nsubj_expected_summationdisplay appos_y_xi appos_yi_y dep_i_yi dep_Juang_1992 conj_and_Juang_Katagiri rcmod_accuracy_expected dep_accuracy_Katagiri dep_accuracy_Juang nn_accuracy_classification nn_accuracy_parenrightBig nn_accuracy_vector appos_xi_accuracy num_xi_| rcmod_yi_negated nn_yi_parenleftBig nn_yi_p dep_i_yi dep_productdisplay_i dep_likelihood_productdisplay appos_likelihood_CL amod_likelihood_conditional nn_likelihood_parenrightBig nn_likelihood_vector nn_likelihood_| nn_likelihood_yi dep_xi_expected conj_xi_likelihood nn_xi_parenleftBig nn_xi_p nn_xi_i nn_xi_productdisplay dep_likelihood_xi appos_likelihood_JL amod_likelihood_joint
P05-1048	W02-1002	o	The second voting model is a maximum entropy model -LRB- Jaynes 1978 -RRB- since Klein and Manning -LRB- 2002 -RRB- found that this model yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance	nn_performance_WSD prep_of_comparison_performance amod_comparison_subsequent det_comparison_a amod_Bayes_naive prep_than_accuracy_Bayes amod_accuracy_higher prep_in_yielded_comparison dobj_yielded_accuracy nsubj_yielded_model mark_yielded_that det_model_this ccomp_found_yielded nsubj_found_Manning nsubj_found_Klein mark_found_since appos_Manning_2002 conj_and_Klein_Manning amod_Jaynes_1978 advcl_model_found dep_model_Jaynes nn_model_entropy nn_model_maximum det_model_a cop_model_is nsubj_model_model nn_model_voting amod_model_second det_model_The
P07-1095	W02-1002	o	Generative and discriminative models have been comparedanddiscussedagreatdeal -LRB- NgandJordan 2002 -RRB- including for NLP models -LRB- Johnson 2001 Klein and Manning 2002 -RRB-	amod_Klein_2002 conj_and_Klein_Manning dep_Johnson_Manning dep_Johnson_Klein amod_Johnson_2001 nn_models_NLP pobj_for_models pcomp_including_for dep_NgandJordan_2002 dep_comparedanddiscussedagreatdeal_Johnson prep_comparedanddiscussedagreatdeal_including dep_comparedanddiscussedagreatdeal_NgandJordan cop_comparedanddiscussedagreatdeal_been aux_comparedanddiscussedagreatdeal_have nsubj_comparedanddiscussedagreatdeal_models amod_models_discriminative amod_models_Generative conj_and_Generative_discriminative
W03-1019	W02-1002	o	a22 a14 is the sufficient statistic of a16 a14 Then we can rewrite a2a24a3 a10a27 a42a7 a25 as a5a7a6a9a8a11a10 a23 a3 a10 a7 a15 a27 a25a18a17a26a25 a12a28a27 a5a7a6a29a8a30a10 a23 a3 a10 a7 a15 a27 a25a18a17 3 Loss Functions for Label Sequences Given the theoretical advantages of discriminative models over generative models and the empirical support by -LRB- Klein and Manning 2002 -RRB- and that CRFs are the state-of-the-art among discriminative models for label sequences we chose CRFs as our model and trained by optimizing various objective functions a31 a3 a10a36 a25 with respect to the corpus a36 The application of these models to the label sequence problems vary widely	advmod_vary_widely nsubj_vary_application nn_problems_sequence nn_problems_label det_problems_the det_models_these prep_to_application_problems prep_of_application_models det_application_The nn_a36_corpus det_a36_the prep_with_respect_to_a10a36_a36 num_a10a36_a25 nn_a10a36_a3 prep_a31_functions_a10a36 amod_functions_objective amod_functions_various rcmod_optimizing_vary dobj_optimizing_functions prepc_by_trained_optimizing poss_model_our prep_as_chose_model dobj_chose_CRFs nsubj_chose_we ccomp_chose_state-of-the-art nsubj_chose_the aux_chose_are nsubj_chose_CRFs mark_chose_that nn_sequences_label amod_models_discriminative prep_for_state-of-the-art_sequences prep_among_state-of-the-art_models conj_and_Klein_trained conj_and_Klein_chose dep_Klein_2002 conj_and_Klein_Manning amod_support_empirical det_support_the conj_and_models_support amod_models_generative amod_models_discriminative prep_by_advantages_trained prep_by_advantages_chose prep_by_advantages_Manning prep_by_advantages_Klein prep_over_advantages_support prep_over_advantages_models prep_of_advantages_models amod_advantages_theoretical det_advantages_the pobj_Given_advantages prep_Sequences_Given nn_Sequences_Label prep_for_Functions_Sequences nn_Functions_Loss num_Functions_3 dep_a25a18a17_Functions nn_a25a18a17_a27 nn_a25a18a17_a15 nn_a25a18a17_a7 nn_a25a18a17_a10 nn_a25a18a17_a3 nn_a25a18a17_a23 nn_a25a18a17_a5a7a6a29a8a30a10 nn_a25a18a17_a12a28a27 num_a25a18a17_a25a18a17a26a25 nn_a25a18a17_a27 nn_a25a18a17_a15 nn_a25a18a17_a7 nn_a25a18a17_a10 nn_a25a18a17_a3 nn_a25a18a17_a23 nn_a25a18a17_a5a7a6a9a8a11a10 num_a42a7_a25 nn_a42a7_a10a27 nn_a42a7_a2a24a3 prep_as_rewrite_a25a18a17 dobj_rewrite_a42a7 aux_rewrite_can nsubj_rewrite_we advmod_rewrite_Then nn_a14_a16 parataxis_statistic_rewrite prep_of_statistic_a14 amod_statistic_sufficient det_statistic_the cop_statistic_is nsubj_statistic_a14 nn_a14_a22
W03-1019	W02-1002	o	Discriminative models do not only have theoretical advantages over generative models as we discuss in Section 2 but they are also shown to be empirically favorable over generative models when features and objective functions are fixed -LRB- Klein and Manning 2002 -RRB-	amod_Klein_2002 conj_and_Klein_Manning dep_fixed_Manning dep_fixed_Klein auxpass_fixed_are nsubjpass_fixed_functions nsubjpass_fixed_features advmod_fixed_when amod_functions_objective conj_and_features_functions amod_models_generative prep_over_favorable_models advmod_favorable_empirically cop_favorable_be aux_favorable_to advcl_shown_fixed xcomp_shown_favorable advmod_shown_also auxpass_shown_are nsubjpass_shown_they num_Section_2 prep_in_discuss_Section nsubj_discuss_we mark_discuss_as amod_models_generative prep_over_advantages_models amod_advantages_theoretical conj_but_have_shown advcl_have_discuss dobj_have_advantages preconj_have_only aux_have_do nsubj_have_models neg_only_not amod_models_Discriminative
W04-0822	W02-1002	o	The second voting model a maximum entropy model -LRB- Jaynes 1978 -RRB- was built as Klein and Manning -LRB- 2002 -RRB- found that it yielded higher accuracy than nave Bayes in a subsequent comparison of WSD performance	nn_performance_WSD prep_of_comparison_performance amod_comparison_subsequent det_comparison_a nn_Bayes_nave prep_than_accuracy_Bayes amod_accuracy_higher prep_in_yielded_comparison dobj_yielded_accuracy nsubj_yielded_it mark_yielded_that ccomp_found_yielded appos_Manning_2002 conj_and_Klein_Manning dep_built_found prep_as_built_Manning prep_as_built_Klein auxpass_built_was nsubjpass_built_model amod_Jaynes_1978 dep_model_Jaynes nn_model_entropy nn_model_maximum det_model_a appos_model_model nn_model_voting amod_model_second det_model_The
W06-1668	W02-1002	o	3.2 Results In line with previous work -LRB- Ng and Jordan 2002 Klein and Manning 2002 -RRB- we first compare Naive Bayes and Logistic regression on the two NLP tasks	nn_tasks_NLP num_tasks_two det_tasks_the amod_regression_Logistic conj_and_Bayes_regression amod_Bayes_Naive prep_on_compare_tasks dobj_compare_regression dobj_compare_Bayes advmod_compare_first nsubj_compare_we advcl_compare_Results dep_Ng_2002 conj_and_Ng_Manning conj_and_Ng_Klein conj_and_Ng_2002 conj_and_Ng_Jordan appos_work_Manning appos_work_Klein appos_work_2002 appos_work_Jordan appos_work_Ng amod_work_previous prep_with_line_work prep_in_Results_line num_Results_3.2
W06-1668	W02-1002	o	4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure such as the Naive Bayes and Logistic regression models -LRB- Ng and Jordan 2002 Klein and Manning 2002 -RRB- and other models -LRB- Klein and Manning 2002 Johnson 2001 -RRB-	amod_Johnson_2001 dep_Klein_Johnson conj_and_Klein_2002 conj_and_Klein_Manning dep_models_2002 dep_models_Manning dep_models_Klein amod_models_other amod_Ng_2002 conj_and_Ng_Manning conj_and_Ng_Klein conj_and_Ng_2002 conj_and_Ng_Jordan dep_models_Manning dep_models_Klein dep_models_2002 dep_models_Jordan dep_models_Ng nn_models_regression nn_models_Logistic conj_and_Bayes_models conj_and_Bayes_models amod_Bayes_Naive det_Bayes_the amod_structure_same det_structure_the dobj_having_structure prep_such_as_models_models prep_such_as_models_models prep_such_as_models_Bayes vmod_models_having amod_models_discriminative amod_models_generative conj_and_generative_discriminative dobj_compared_models aux_compared_has nsubj_compared_work amod_work_Previous rcmod_Work_compared amod_Work_Related prep_to_Comparison_Work num_Comparison_4 ccomp_``_Comparison
W06-1668	W02-1002	o	The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure -LRB- Klein and Manning 2002 -RRB-	amod_Klein_2002 conj_and_Klein_Manning appos_structure_Manning appos_structure_Klein nn_structure_model amod_structure_same det_structure_the advmod_structure_exactly dobj_use_structure nsubj_use_models advmod_use_when amod_models_generative amod_models_discriminative det_models_the conj_and_discriminative_generative amod_tasks_many advcl_shown_use prep_on_shown_tasks auxpass_shown_been aux_shown_has nsubjpass_shown_superiority amod_models_discriminative prep_of_superiority_models det_superiority_The
W08-0617	W02-1002	o	It is believed that improvement can be achieved by training the generative model based on a discriminative optimization criteria -LRB- Klein and Manning 2002 -RRB- in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus	nn_corpus_training det_corpus_the prep_in_sentences_corpus det_sentences_the pobj_given_sentences prep_parses_given det_parses_the prep_of_probability_parses amod_probability_conditional det_probability_the dobj_maximize_probability aux_maximize_to xcomp_designed_maximize auxpass_designed_is nsubjpass_designed_procedure prep_in_designed_which nn_procedure_training det_procedure_the dep_Klein_2002 conj_and_Klein_Manning rcmod_criteria_designed appos_criteria_Manning appos_criteria_Klein nn_criteria_optimization amod_criteria_discriminative det_criteria_a amod_model_generative det_model_the pobj_training_criteria prepc_based_on_training_on dobj_training_model agent_achieved_training auxpass_achieved_be aux_achieved_can nsubjpass_achieved_improvement mark_achieved_that ccomp_believed_achieved auxpass_believed_is nsubjpass_believed_It ccomp_``_believed
C04-1071	W02-1011	o	2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the document-level classification of sentiment -LRB- Turney 2002 Pang et al. 2002 -RRB- where a document is assumed to have only a single sentiment thus these studies are not applicable to our goal	poss_goal_our prep_to_applicable_goal neg_applicable_not cop_applicable_are nsubj_applicable_studies advmod_applicable_thus det_studies_these amod_sentiment_single det_sentiment_a advmod_sentiment_only dobj_have_sentiment aux_have_to xcomp_assumed_have auxpass_assumed_is nsubjpass_assumed_document advmod_assumed_where det_document_a rcmod_Pang_assumed num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_applicable conj_Turney_Pang conj_Turney_2002 dep_sentiment_Turney prep_of_classification_sentiment amod_classification_document-level det_classification_the prep_on_focused_classification nsubj_focused_studies nn_analysis_sentiment prep_on_studies_analysis amod_studies_prior det_studies_Some rcmod_Analysis_focused nn_Analysis_Sentiment prep_on_work_Analysis amod_work_Previous num_work_2
C04-1200	W02-1011	o	Recent computational work either focuses on sentence subjectivity -LRB- Wiebe et al. 2002 Riloff et al. 2003 -RRB- concentrates just on explicit statements of evaluation such as of films -LRB- Turney 2002 Pang et al. 2002 -RRB- or focuses on just one aspect of opinion e.g. -LRB- Hatzivassiloglou and McKeown 1997 -RRB- on adjectives	num_McKeown_1997 conj_and_Hatzivassiloglou_McKeown prep_on_aspect_adjectives appos_aspect_McKeown appos_aspect_Hatzivassiloglou dep_aspect_e.g. prep_of_aspect_opinion num_aspect_one advmod_aspect_just prep_on_focuses_aspect dep_al._2002 nn_al._et nn_al._Pang conj_or_Turney_focuses dep_Turney_al. num_Turney_2002 dep_films_focuses dep_films_Turney pobj_statements_films prepc_as_of_statements_of mwe_statements_such prep_of_statements_evaluation amod_statements_explicit prep_on_concentrates_statements advmod_concentrates_just dep_al._2003 nn_al._et nn_al._Riloff dep_Wiebe_al. dep_Wiebe_2002 dep_Wiebe_al. nn_Wiebe_et appos_subjectivity_Wiebe nn_subjectivity_sentence dep_focuses_concentrates prep_on_focuses_subjectivity preconj_focuses_either nsubj_focuses_work amod_work_computational amod_work_Recent
C08-1031	W02-1011	o	One major focus is sentiment classification and opinion mining -LRB- e.g. Pang et al 2002 Turney 2002 Hu and Liu 2004 Wilson et al 2004 Kim and Hovy 2004 Popescu and Etzioni 2005 -RRB- 2008	dep_2008_Pang dep_2008_e.g. num_Etzioni_2005 conj_and_Popescu_Etzioni num_Hovy_2004 conj_and_Kim_Hovy nn_2004_al num_Wilson_2004 nn_Wilson_et num_Liu_2004 conj_and_Hu_Liu num_Turney_2002 dep_al_2002 nn_al_et dep_Pang_Etzioni dep_Pang_Popescu dep_Pang_Hovy dep_Pang_Kim dep_Pang_Wilson dep_Pang_Liu dep_Pang_Hu dep_Pang_Turney advmod_Pang_al dep_mining_2008 nn_mining_opinion conj_and_classification_mining nn_classification_sentiment cop_classification_is nsubj_classification_focus amod_focus_major num_focus_One
C08-1031	W02-1011	o	Sentiment classification at the document level investigates ways to classify each evaluative document -LRB- e.g. product review -RRB- as positive or negative -LRB- Pang et al 2002 Turney 2002 -RRB-	num_Turney_2002 num_al_2002 dep_Pang_Turney dep_Pang_al nn_Pang_et conj_or_positive_negative nn_review_product dep_review_e.g. dep_document_review amod_document_evaluative det_document_each prep_as_classify_negative prep_as_classify_positive dobj_classify_document aux_classify_to vmod_ways_classify dep_investigates_Pang dobj_investigates_ways nsubj_investigates_classification nn_level_document det_level_the prep_at_classification_level nn_classification_Sentiment
C08-1104	W02-1011	o	2 Related Work There has been extensive research in opinion mining at the document level for example on product and movie reviews -LRB- Pang et al. 2002 Pang and Lee 2004 Dave et al. 2003 Popescu and Etzioni 2005 -RRB-	num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_2005 conj_and_Pang_Etzioni conj_and_Pang_Popescu conj_and_Pang_Dave num_Pang_2004 conj_and_Pang_Lee dep_Pang_Etzioni dep_Pang_Popescu dep_Pang_Dave dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_reviews_movie nn_reviews_product conj_and_product_movie nn_level_document det_level_the nn_mining_opinion prep_on_research_reviews prep_for_research_example prep_at_research_level prep_in_research_mining amod_research_extensive cop_research_been aux_research_has expl_research_There dep_Work_Pang rcmod_Work_research amod_Work_Related num_Work_2 dep_``_Work
C08-1111	W02-1011	o	80 8.0 % Positive child education Positive cost Negative SUBJECT increase Figure 3 An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification ranging from the use of co-occurrence with typical positive and negative words -LRB- Turney 2002 -RRB- to bag of words -LRB- Pang et al. 2002 -RRB- and dependency structure -LRB- Kudo and Matsumoto 2004 -RRB-	dep_Kudo_2004 conj_and_Kudo_Matsumoto nn_structure_dependency amod_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_bag_words amod_Turney_2002 dep_words_Turney amod_words_negative amod_words_positive amod_words_typical conj_and_positive_negative prep_with_use_words prep_of_use_co-occurrence det_use_the prep_to_ranging_bag prep_from_ranging_use nn_classification_polarity nn_classification_sentiment prep_for_proposed_classification auxpass_proposed_been advmod_proposed_already aux_proposed_have nsubjpass_proposed_methods amod_methods_Various vmod_lattice_ranging rcmod_lattice_proposed amod_lattice_word-polarity det_lattice_a dep_example_Matsumoto dep_example_Kudo conj_and_example_structure dep_example_Pang prep_of_example_lattice det_example_An dep_Figure_structure dep_Figure_example num_Figure_3 nn_Figure_increase amod_Figure_SUBJECT amod_Figure_Negative nn_Figure_cost amod_Figure_Positive nn_Figure_education nn_Figure_child amod_Figure_Positive amod_Figure_% num_Figure_80 number_%_8.0
C08-1111	W02-1011	o	2 Related Work Recently several studies have reported about dialog systems that are capable of classifying emotions in a human-computer dialog -LRB- Batliner et al. 2004 Ang et al. 2002 Litman and Forbes-Riley 2004 Rotaru et al. 2005 -RRB-	num_Rotaru_2005 nn_Rotaru_al. nn_Rotaru_et dep_Litman_Rotaru conj_and_Litman_2004 conj_and_Litman_Forbes-Riley dep_Ang_2004 dep_Ang_Forbes-Riley dep_Ang_Litman num_Ang_2002 nn_Ang_al. nn_Ang_et dep_Batliner_Ang amod_Batliner_2004 dep_Batliner_al. nn_Batliner_et amod_dialog_human-computer det_dialog_a prep_in_emotions_dialog amod_emotions_classifying prep_of_capable_emotions cop_capable_are nsubj_capable_that rcmod_systems_capable nn_systems_dialog dep_reported_Batliner prep_about_reported_systems aux_reported_have nsubj_reported_studies ccomp_reported_Work amod_studies_several advmod_Work_Recently amod_Work_Related num_Work_2
D07-1035	W02-1011	o	1 Introduction In the past few years there has been an increasing interest in mining opinions from product reviews -LRB- Pang et al 2002 Liu et al 2004 Popescu and Etzioni 2005 -RRB-	amod_Popescu_2005 conj_and_Popescu_Etzioni nn_al_et dep_Liu_Etzioni dep_Liu_Popescu num_Liu_2004 appos_Liu_al nn_al_et dep_Pang_Liu appos_Pang_2002 appos_Pang_al dep_reviews_Pang nn_reviews_product prep_from_opinions_reviews nn_opinions_mining prep_in_interest_opinions amod_interest_increasing det_interest_an cop_interest_been aux_interest_has expl_interest_there ccomp_interest_Introduction amod_years_few amod_years_past det_years_the prep_in_Introduction_years num_Introduction_1
D07-1035	W02-1011	o	Pang et al -LRB- 2002 -RRB- considered the same problem and presented a set of supervised machine learning approaches to it	amod_approaches_learning nn_approaches_machine amod_machine_supervised prep_to_set_it prep_of_set_approaches det_set_a dobj_presented_set nsubj_presented_Pang amod_problem_same det_problem_the conj_and_considered_presented dobj_considered_problem nsubj_considered_Pang dep_al_2002 dep_Pang_al nn_Pang_et
D07-1114	W02-1011	o	This direction has been forming the mainstream of research on opinion-sensitive text processing -LRB- Pang et al. 2002 Turney 2002 etc. -RRB-	amod_Turney_etc. num_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_processing_text amod_processing_opinion-sensitive prep_of_mainstream_research det_mainstream_the dep_forming_Pang prep_on_forming_processing dobj_forming_mainstream aux_forming_been aux_forming_has nsubj_forming_direction det_direction_This ccomp_``_forming
D08-1004	W02-1011	o	We use the same set of binary features as in previous work on this dataset -LRB- Pang et al. 2002 Pang and Lee 2004 Zaidan et al. 2007 -RRB-	num_Zaidan_2007 nn_Zaidan_al. nn_Zaidan_et dep_Pang_Zaidan num_Pang_2004 conj_and_Pang_Lee dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et det_dataset_this amod_work_previous pobj_in_work pcomp_as_in amod_features_binary prep_of_set_features amod_set_same det_set_the dep_use_Pang prep_on_use_dataset prep_use_as dobj_use_set nsubj_use_We ccomp_``_use
D08-1013	W02-1011	o	Most of researchers focus on how to extract useful textual features -LRB- lexical syntactic punctuation etc. -RRB- for determining the semantic orientation of the sentences using machine learning algorithm -LRB- Bo et al. 2002 Kim and Hovy 2004 Bo et al. 2005 Hu et al. 2004 Alina et al 2008 Alistair et al 2006 -RRB-	num_al_2006 nn_al_et nn_al_Alistair num_al_2008 nn_al_et nn_al_Alina nn_al._et nn_al._Hu dep_Bo_2005 dep_Bo_al. nn_Bo_et num_Kim_2004 conj_and_Kim_Hovy dep_Bo_al conj_Bo_al dep_Bo_2004 dep_Bo_al. dep_Bo_Bo dep_Bo_Hovy dep_Bo_Kim num_Bo_2002 dep_Bo_al. nn_Bo_et nn_algorithm_learning nn_algorithm_machine dobj_using_algorithm det_sentences_the prep_of_orientation_sentences amod_orientation_semantic det_orientation_the vmod_determining_using dobj_determining_orientation dep_punctuation_etc. amod_punctuation_syntactic amod_punctuation_lexical appos_features_punctuation amod_features_textual amod_features_useful dep_extract_Bo prepc_for_extract_determining dobj_extract_features aux_extract_to advmod_extract_how prepc_on_focus_extract nsubj_focus_Most prep_of_Most_researchers
D08-1058	W02-1011	o	-LRB- 2002 -RRB- various classification models and linguistic features have been proposed to improve the classification performance -LRB- Pang and Lee 2004 Mullen and Collier 2004 Wilson et al. 2005a Read 2005 -RRB-	dep_Read_2005 appos_Wilson_2005a dep_Wilson_al. nn_Wilson_et dep_Mullen_Read conj_and_Mullen_Wilson conj_and_Mullen_2004 conj_and_Mullen_Collier conj_and_Pang_Wilson conj_and_Pang_2004 conj_and_Pang_Collier conj_and_Pang_Mullen conj_and_Pang_2004 conj_and_Pang_Lee dep_performance_Mullen dep_performance_2004 dep_performance_Lee dep_performance_Pang nn_performance_classification det_performance_the dobj_improve_performance aux_improve_to xcomp_proposed_improve auxpass_proposed_been aux_proposed_have nsubjpass_proposed_features nsubjpass_proposed_models amod_features_linguistic conj_and_models_features nn_models_classification amod_models_various dep_models_2002
D09-1017	W02-1011	o	Sentiment summarization has been well studied in the past decade -LRB- Turney 2002 Pang et al. 2002 Dave et al. 2003 Hu and Liu 2004a 2004b Carenini et al. 2006 Liu et al. 2007 -RRB-	num_Liu_2007 nn_Liu_al. nn_Liu_et num_Carenini_2006 nn_Carenini_al. nn_Carenini_et conj_and_Hu_2004b conj_and_Hu_2004a conj_and_Hu_Liu num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_Liu conj_Pang_Carenini conj_Pang_2004b conj_Pang_2004a conj_Pang_Liu conj_Pang_Hu conj_Pang_Dave num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang appos_Turney_2002 amod_decade_past det_decade_the dep_studied_Turney prep_in_studied_decade advmod_studied_well auxpass_studied_been aux_studied_has nsubjpass_studied_summarization nn_summarization_Sentiment
D09-1019	W02-1011	o	There are many research directions e.g. sentiment classification -LRB- classifying an opinion document as positive or negative -RRB- -LRB- e.g. Pang Lee and Vaithyanathan 2002 Turney 2002 -RRB- subjectivity classification -LRB- determining whether a sentence is subjective or objective and its associated opinion -RRB- -LRB- Wiebe and Wilson 2002 Yu and Hatzivassiloglou 2003 Wilson et al 2004 Kim and Hovy 2004 Riloff and Wiebe 2005 -RRB- feature/topic-based sentiment analysis -LRB- assigning positive or negative sentiments to topics or product features -RRB- -LRB- Hu and Liu 2004 Popescu and Etzioni 2005 Carenini et al. 2005 Ku et al. 2006 Kobayashi Inui and Matsumoto 2007 Titov and McDonald	num_Ku_2006 nn_Ku_al. nn_Ku_et num_Carenini_2005 nn_Carenini_al. nn_Carenini_et conj_and_Popescu_McDonald conj_and_Popescu_Titov conj_and_Popescu_2007 conj_and_Popescu_Matsumoto conj_and_Popescu_Inui conj_and_Popescu_Kobayashi conj_and_Popescu_Ku conj_and_Popescu_Carenini conj_and_Popescu_2005 conj_and_Popescu_Etzioni num_Liu_2004 dep_Hu_McDonald dep_Hu_Titov dep_Hu_2007 dep_Hu_Matsumoto dep_Hu_Inui dep_Hu_Kobayashi dep_Hu_Ku dep_Hu_Carenini dep_Hu_2005 dep_Hu_Etzioni dep_Hu_Popescu conj_and_Hu_Liu nn_features_product conj_or_topics_features amod_sentiments_negative amod_sentiments_positive conj_or_positive_negative prep_to_assigning_features prep_to_assigning_topics dobj_assigning_sentiments dep_analysis_Liu dep_analysis_Hu dep_analysis_assigning nn_analysis_sentiment amod_analysis_feature/topic-based amod_al_2004 nn_al_et nn_al_Wilson dep_Yu_2005 conj_and_Yu_Wiebe conj_and_Yu_Riloff conj_and_Yu_2004 conj_and_Yu_Hovy conj_and_Yu_Kim conj_and_Yu_al conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Wiebe_Wiebe dep_Wiebe_Riloff dep_Wiebe_2004 dep_Wiebe_Hovy dep_Wiebe_Kim dep_Wiebe_al dep_Wiebe_2003 dep_Wiebe_Hatzivassiloglou dep_Wiebe_Yu conj_and_Wiebe_2002 conj_and_Wiebe_Wilson dep_opinion_2002 dep_opinion_Wilson dep_opinion_Wiebe amod_opinion_associated poss_opinion_its nsubj_objective_sentence conj_or_subjective_objective cop_subjective_is nsubj_subjective_sentence mark_subjective_whether det_sentence_a dobj_determining_analysis conj_and_determining_opinion ccomp_determining_objective ccomp_determining_subjective dep_classification_opinion dep_classification_determining nn_classification_subjectivity dep_classification_2002 dep_classification_Vaithyanathan dep_classification_Lee dep_classification_Pang dep_classification_e.g. dep_Turney_2002 dep_Pang_Turney conj_and_Pang_2002 conj_and_Pang_Vaithyanathan conj_and_Pang_Lee conj_or_positive_negative nn_document_opinion det_document_an parataxis_classifying_classification prep_as_classifying_negative prep_as_classifying_positive dobj_classifying_document dep_classification_classifying nn_classification_sentiment pobj_e.g._classification prep_directions_e.g. nn_directions_research amod_directions_many nsubj_are_directions expl_are_There ccomp_``_are
D09-1019	W02-1011	o	One of the main directions is sentiment classification which classifies the whole opinion document -LRB- e.g. a product review -RRB- as positive or negative -LRB- e.g. Pang et al 2002 Turney 2002 Dave et al 2003 Ng et al. 2006 McDonald et al 2007 -RRB-	appos_al_2007 nn_al_et nn_al_McDonald num_al._2006 nn_al._et nn_al._Ng amod_al_2003 nn_al_et nn_al_Dave conj_Turney_al conj_Turney_al. conj_Turney_al appos_Turney_2002 dep_Pang_Turney num_Pang_2002 nn_Pang_al nn_Pang_et dep_e.g._Pang dep_positive_e.g. conj_or_positive_negative nn_review_product det_review_a dep_review_e.g. dep_document_review nn_document_opinion amod_document_whole det_document_the prep_as_classifies_negative prep_as_classifies_positive dobj_classifies_document nsubj_classifies_which rcmod_classification_classifies nn_classification_sentiment cop_classification_is nsubj_classification_One amod_directions_main det_directions_the prep_of_One_directions
D09-1061	W02-1011	p	We use five sentiment classification datasets including the widely-used movie review dataset -LSB- MOV -RSB- -LRB- Pang et al. 2002 -RRB- as well as four datasets containing reviews of four different types of products from Amazon -LSB- books -LRB- BOO -RRB- DVDs -LRB- DVD -RRB- electronics -LRB- ELE -RRB- and kitchen appliances -LRB- KIT -RRB- -RSB- -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et appos_appliances_KIT nn_appliances_kitchen appos_electronics_ELE appos_DVDs_DVD conj_and_books_appliances conj_and_books_electronics appos_books_DVDs appos_books_BOO prep_from_products_Amazon dep_types_appliances dep_types_electronics dep_types_books prep_of_types_products amod_types_different num_types_four prep_of_reviews_types dobj_containing_reviews vmod_datasets_containing num_datasets_four amod_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_dataset_datasets dep_dataset_Pang appos_dataset_MOV nn_dataset_review nn_dataset_movie amod_dataset_widely-used det_dataset_the dep_datasets_Blitzer prep_including_datasets_datasets prep_including_datasets_dataset nn_datasets_classification nn_datasets_sentiment num_datasets_five dobj_use_datasets nsubj_use_We
D09-1159	W02-1011	o	The former is a task of identifying positive and negative sentiments from a text which can be a passage a sentence a phrase and even a word -LRB- Somasundaran et al. 2008 Pang et al. 2002 Dave et al. 2003 Kim and Hovy 2004 Takamura et al. 2005 -RRB-	num_Takamura_2005 nn_Takamura_al. nn_Takamura_et num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_Takamura conj_and_Pang_2004 conj_and_Pang_Hovy conj_and_Pang_Kim conj_and_Pang_Dave amod_Pang_2002 dep_Pang_al. nn_Pang_et num_al._2008 nn_al._et dep_Somasundaran_al. amod_word_Somasundaran det_word_a advmod_word_even det_phrase_a conj_and_sentence_word conj_and_sentence_phrase det_sentence_a appos_passage_word appos_passage_phrase appos_passage_sentence det_passage_a cop_passage_be aux_passage_can nsubj_passage_which rcmod_text_passage det_text_a amod_sentiments_negative amod_sentiments_positive conj_and_positive_negative prep_from_identifying_text dobj_identifying_sentiments dep_task_2004 dep_task_Hovy dep_task_Kim dep_task_Dave dep_task_Pang prepc_of_task_identifying det_task_a cop_task_is nsubj_task_former det_former_The
D09-1159	W02-1011	o	Amount of works have been done on sentimental classification in different levels -LRB- Zhang et al. 2009 Somasundaran et al. 2008 Pang et al. 2002 Dave et al. 2003 Kim and Hovy 2004 Takamura et al. 2005 -RRB-	num_Takamura_2005 nn_Takamura_al. nn_Takamura_et num_Kim_2004 conj_and_Kim_Hovy num_Dave_2003 nn_Dave_al. nn_Dave_et num_Pang_2002 nn_Pang_al. nn_Pang_et num_Somasundaran_2008 nn_Somasundaran_al. nn_Somasundaran_et dep_Zhang_Takamura dep_Zhang_Hovy dep_Zhang_Kim dep_Zhang_Dave dep_Zhang_Pang dep_Zhang_Somasundaran amod_Zhang_2009 dep_Zhang_al. nn_Zhang_et amod_levels_different prep_in_classification_levels amod_classification_sentimental pobj_on_classification dep_done_Zhang prep_done_on auxpass_done_been aux_done_have nsubjpass_done_Amount prep_of_Amount_works
E09-1046	W02-1011	o	Such a lexicon can be used e.g. to classify individual sentences or phrases as subjective or not and as bearing positive or negative sentiments -LRB- Pang et al. 2002 Kim and Hovy 2004 Wilson et al. 2005a -RRB-	appos_Wilson_2005a nn_Wilson_al. nn_Wilson_et num_Kim_2004 conj_and_Kim_Hovy dep_Pang_Wilson dep_Pang_Hovy dep_Pang_Kim amod_Pang_2002 dep_Pang_al. nn_Pang_et amod_sentiments_negative amod_sentiments_positive conj_or_positive_negative dobj_bearing_sentiments dep_as_Pang pcomp_as_bearing conj_and_subjective_as conj_or_subjective_not conj_or_sentences_phrases amod_sentences_individual prep_as_classify_as prep_as_classify_not prep_as_classify_subjective dobj_classify_phrases dobj_classify_sentences aux_classify_to vmod_e.g._classify prep_used_e.g. auxpass_used_be aux_used_can nsubjpass_used_lexicon det_lexicon_a amod_lexicon_Such
H05-1043	W02-1011	o	Other systems -LRB- Morinaga et al. 2002 Kushal et al. 2003 -RRB- also look at Web product reviews but they do not extract 345 opinions about particular product features	nn_features_product amod_features_particular prep_about_opinions_features num_opinions_345 dobj_extract_opinions neg_extract_not aux_extract_do nsubj_extract_they nn_reviews_product nn_reviews_Web conj_but_look_extract prep_at_look_reviews advmod_look_also nsubj_look_systems num_Kushal_2003 nn_Kushal_al. nn_Kushal_et dep_Morinaga_Kushal appos_Morinaga_2002 dep_Morinaga_al. nn_Morinaga_et appos_systems_Morinaga amod_systems_Other
H05-1043	W02-1011	o	Subjective phrases are used by -LRB- Turney 2002 Pang and Vaithyanathan 2002 Kushal et al. 2003 Kim and Hovy 2004 -RRB- and others in order to classify reviews or sentences as positive or negative	conj_or_positive_negative prep_as_reviews_negative prep_as_reviews_positive conj_or_reviews_sentences dobj_classify_sentences dobj_classify_reviews aux_classify_to num_Kim_2004 conj_and_Kim_Hovy num_Kushal_2003 nn_Kushal_al. nn_Kushal_et vmod_Pang_classify prep_in_Pang_order conj_and_Pang_others conj_and_Pang_Hovy conj_and_Pang_Kim conj_and_Pang_Kushal conj_and_Pang_2002 conj_and_Pang_Vaithyanathan dep_Turney_others dep_Turney_Kim dep_Turney_Kushal dep_Turney_2002 dep_Turney_Vaithyanathan dep_Turney_Pang appos_Turney_2002 agent_used_Turney auxpass_used_are nsubjpass_used_phrases amod_phrases_Subjective
H05-1045	W02-1011	o	Errors from the sentence boundary detector in GATE -LRB- Cunningham et al. 2002 -RRB- were especially problematic because they caused the Collins parser to fail resulting in no dependency tree information	nn_information_tree nn_information_dependency neg_information_no prep_in_resulting_information xcomp_fail_resulting aux_fail_to nn_parser_Collins det_parser_the xcomp_caused_fail dobj_caused_parser nsubj_caused_they mark_caused_because advcl_problematic_caused advmod_problematic_especially cop_problematic_were nsubj_problematic_Errors amod_Cunningham_2002 dep_Cunningham_al. nn_Cunningham_et prep_in_detector_GATE nn_detector_boundary nn_detector_sentence det_detector_the dep_Errors_Cunningham prep_from_Errors_detector
H05-1045	W02-1011	o	Part-of-speech features Based on the lexical categories produced by GATE -LRB- Cunningham et al. 2002 -RRB- each token xi is classified into one of a set of coarse part-of-speech tags noun verb adverb wh-word determiner punctuation etc. We do the same for neighboring words in a -LSB- 2 +2 -RSB- window in order to assist noun phrase segmentation	nn_segmentation_phrase nn_segmentation_noun dobj_assist_segmentation aux_assist_to dep_assist_order mark_assist_in dep_window_2 det_window_a dep_2_+2 prep_in_words_window amod_words_neighboring det_same_the advcl_do_assist prep_for_do_words dobj_do_same nsubj_do_We dep_verb_etc. dep_verb_punctuation dep_verb_determiner dep_verb_wh-word dep_verb_adverb dep_verb_noun nn_tags_part-of-speech amod_tags_coarse prep_of_set_tags det_set_a parataxis_one_verb prep_of_one_set prep_into_classified_one auxpass_classified_is nsubjpass_classified_xi amod_xi_token det_xi_each amod_Cunningham_2002 dep_Cunningham_al. nn_Cunningham_et appos_GATE_Cunningham agent_produced_GATE vmod_categories_produced amod_categories_lexical det_categories_the prep_on_Based_categories dep_features_do dep_features_classified vmod_features_Based amod_features_Part-of-speech
I08-1039	W02-1011	p	2 Related Work Supervised machine learning methods including Support Vector Machines -LRB- SVM -RRB- are often used in sentiment analysis and shown to be very promising -LRB- Pang et al. 2002 Matsumoto et al. 2005 Kudo and Matsumoto 2004 Mullen and Collier 2004 Gamon 2004 -RRB-	amod_Gamon_2004 conj_and_Mullen_2004 conj_and_Mullen_Collier conj_and_Kudo_2004 conj_and_Kudo_Matsumoto num_Matsumoto_2005 nn_Matsumoto_al. nn_Matsumoto_et dep_Pang_Gamon dep_Pang_2004 dep_Pang_Collier dep_Pang_Mullen dep_Pang_2004 dep_Pang_Matsumoto dep_Pang_Kudo dep_Pang_Matsumoto amod_Pang_2002 dep_Pang_al. nn_Pang_et advmod_promising_very cop_promising_be aux_promising_to dep_shown_Pang xcomp_shown_promising nsubjpass_shown_methods nn_analysis_sentiment conj_and_used_shown prep_in_used_analysis advmod_used_often auxpass_used_are nsubjpass_used_methods appos_Machines_SVM nn_Machines_Vector nn_Machines_Support prep_including_methods_Machines amod_methods_learning nn_methods_machine amod_machine_Supervised rcmod_Work_shown rcmod_Work_used amod_Work_Related num_Work_2
I08-1039	W02-1011	o	One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated -LRB- Matsumoto et al. 2005 Kudo and Matsumoto 2004 Pang et al. 2002 -RRB-	num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Kudo_Pang conj_and_Kudo_2004 conj_and_Kudo_Matsumoto dep_Matsumoto_2004 dep_Matsumoto_Matsumoto dep_Matsumoto_Kudo appos_Matsumoto_2005 dep_Matsumoto_al. nn_Matsumoto_et dep_incorporated_Matsumoto auxpass_incorporated_be advmod_incorporated_easily aux_incorporated_can nsubjpass_incorporated_variety mark_incorporated_that prep_of_trees_words conj_and_trees_sequences nn_trees_dependency prep_such_as_features_sequences prep_such_as_features_trees prep_of_variety_features amod_variety_wide det_variety_a ccomp_is_incorporated nsubj_is_One det_methods_these prep_of_advantages_methods det_advantages_the prep_of_One_advantages ccomp_``_is
I08-1040	W02-1011	o	-LRB- Turney 2002 Pang et al. 2002 Dave at al. 2003 -RRB-	dep_al._2003 prep_at_Dave_al. dep_Pang_Dave num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang appos_Turney_2002 dep_''_Turney
I08-1040	W02-1011	o	2 RelatedWork 2.1 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment -LRB- Turney 2002 Pang et al. 2002 Dave at al. 2003 -RRB-	dep_al._2003 prep_at_Dave_al. num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Dave conj_Turney_Pang conj_Turney_2002 dep_sentiment_Turney amod_sentiment_negative amod_sentiment_positive conj_and_positive_negative prep_of_classification_sentiment amod_classification_binary det_classification_the prep_on_focused_classification aux_focused_has nsubj_focused_work amod_texts_opinionated amod_texts_categorizing prep_of_problem_texts det_problem_the prep_on_work_problem amod_work_previous amod_work_Most rcmod_Classification_focused nn_Classification_Sentiment num_Classification_2.1 nn_Classification_RelatedWork num_Classification_2
I08-1041	W02-1011	p	SVM has been shown to be useful for text classification tasks -LRB- Joachims 1998 -RRB- and has previously given good performance in sentiment classification experiments -LRB- Kennedy and Inkpen 2006 Mullen and Collier 2004 Pang and Lee 2004 Pang et al. 2002 -RRB-	num_Pang_2002 nn_Pang_al. nn_Pang_et num_Pang_2004 conj_and_Pang_Lee dep_Mullen_Pang conj_and_Mullen_Lee conj_and_Mullen_Pang conj_and_Mullen_2004 conj_and_Mullen_Collier dep_Kennedy_Pang dep_Kennedy_2004 dep_Kennedy_Collier dep_Kennedy_Mullen conj_and_Kennedy_2006 conj_and_Kennedy_Inkpen dep_experiments_2006 dep_experiments_Inkpen dep_experiments_Kennedy nn_experiments_classification nn_experiments_sentiment prep_in_performance_experiments amod_performance_good dobj_given_performance advmod_given_previously aux_given_has nsubjpass_given_SVM amod_Joachims_1998 dep_tasks_Joachims nn_tasks_classification nn_tasks_text prep_for_useful_tasks cop_useful_be aux_useful_to conj_and_shown_given xcomp_shown_useful auxpass_shown_been aux_shown_has nsubjpass_shown_SVM
I08-1041	W02-1011	p	Unigram models have been previously shown to give good results in sentiment classification tasks -LRB- Kennedy and Inkpen 2006 Pang et al. 2002 -RRB- unigram representations can capture a variety of lexical combinations and distributions including those of emotion words	nn_words_emotion prep_of_those_words prep_including_combinations_those conj_and_combinations_distributions amod_combinations_lexical prep_of_variety_distributions prep_of_variety_combinations det_variety_a dobj_capture_variety aux_capture_can nsubj_capture_representations amod_representations_unigram num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Kennedy_Pang amod_Kennedy_2006 conj_and_Kennedy_Inkpen appos_tasks_Inkpen appos_tasks_Kennedy nn_tasks_classification nn_tasks_sentiment amod_results_good prep_in_give_tasks dobj_give_results aux_give_to parataxis_shown_capture xcomp_shown_give advmod_shown_previously auxpass_shown_been aux_shown_have nsubjpass_shown_models nn_models_Unigram
J04-3002	W02-1011	o	Automatic subjectivity analysis would also be useful to perform flame recognition -LRB- Spertus 1997 Kaufer 2000 -RRB- e-mail classification -LRB- Aone Ramos-Santacruze and Niehaus 2000 -RRB- intellectual attribution in text -LRB- Teufel and Moens 2000 -RRB- recognition of speaker role in radio broadcasts -LRB- Barzialy et al. 2000 -RRB- review mining -LRB- Terveen et al. 1997 -RRB- review classification -LRB- Turney 2002 Pang Lee and Vaithyanathan 2002 -RRB- style in generation -LRB- Hovy 1987 -RRB- and clustering documents by ideological point of view -LRB- Sack 1995 -RRB-	num_Sack_1995 dep_point_Sack prep_of_point_view amod_point_ideological nn_documents_clustering dep_Hovy_1987 dep_generation_Hovy prep_in_style_generation num_Vaithyanathan_2002 conj_and_Pang_Vaithyanathan conj_and_Pang_Lee dep_Turney_Vaithyanathan dep_Turney_Lee dep_Turney_Pang num_Turney_2002 appos_classification_Turney nn_classification_review dep_al._1997 nn_al._et advmod_Terveen_al. appos_mining_Terveen nn_mining_review dep_al._2000 nn_al._et advmod_Barzialy_al. nn_broadcasts_radio nn_role_speaker prep_by_recognition_point conj_and_recognition_documents conj_and_recognition_style conj_and_recognition_classification conj_and_recognition_mining appos_recognition_Barzialy prep_in_recognition_broadcasts prep_of_recognition_role num_Moens_2000 conj_and_Teufel_Moens dep_text_Moens dep_text_Teufel prep_in_attribution_text amod_attribution_intellectual num_Niehaus_2000 conj_and_Aone_Niehaus conj_and_Aone_Ramos-Santacruze dep_classification_Niehaus dep_classification_Ramos-Santacruze dep_classification_Aone amod_classification_e-mail num_Kaufer_2000 dep_Spertus_Kaufer num_Spertus_1997 dep_recognition_documents dep_recognition_style dep_recognition_classification dep_recognition_mining dep_recognition_recognition appos_recognition_attribution appos_recognition_classification appos_recognition_Spertus nn_recognition_flame dobj_perform_recognition aux_perform_to xcomp_useful_perform cop_useful_be advmod_useful_also aux_useful_would nsubj_useful_analysis nn_analysis_subjectivity nn_analysis_Automatic
L08-1616	W02-1011	o	Workshop Towards Genre-Enabled Search Engines </booktitle> <pages> 13 -- 20 </pages> <editor> In G. Rehm and M. Santini editors </editor> <contexts> <context> ork on an intra-document or page segment level because a single document can contain instances of multiple genres e.g. contact information list of publications C.V. see -LRB- Rehm 2002 Rehm 2007 Mehler et al. 2007 -RRB-	num_Mehler_2007 nn_Mehler_al. nn_Mehler_et num_Rehm_2007 dep_Rehm_Mehler dep_Rehm_Rehm dep_Rehm_2002 dep_see_Rehm nsubj_see_information dep_see_Workshop prep_of_list_publications appos_information_C.V. appos_information_list nn_information_contact amod_genres_multiple prep_of_instances_genres dobj_contain_instances aux_contain_can nsubj_contain_document mark_contain_because amod_document_single det_document_a nn_level_segment amod_level_page amod_level_intra-document det_level_an conj_or_intra-document_page nn_ork_<context> amod_ork_<contexts> advcl_</editor>_contain prep_on_</editor>_level dobj_</editor>_ork nsubj_</editor>_editors nn_Santini_M. conj_and_Rehm_Santini nn_Rehm_G. prep_in_<editor>_Santini prep_in_<editor>_Rehm nn_<editor>_</pages> num_<editor>_20 rcmod_<pages>_</editor> dep_<pages>_<editor> num_<pages>_13 amod_<pages>_</booktitle> nn_<pages>_Engines prep_Search_e.g. dobj_Search_<pages> nsubj_Search_Genre-Enabled mark_Search_Towards advcl_Workshop_Search
L08-1616	W02-1011	o	-LRB- HICSS-35 </booktitle> <contexts> <context> documents genres also work on an intra-document or page segment level because a single document can contain instances of multiple genres e.g. contact information list of publications C.V. see -LRB- Rehm 2002 Rehm 2007 Mehler et al. 2007 -RRB-	num_Mehler_2007 nn_Mehler_al. nn_Mehler_et num_Rehm_2007 dep_Rehm_Mehler dep_Rehm_Rehm dep_Rehm_2002 dep_see_Rehm prep_of_list_publications conj_information_see conj_information_C.V. conj_information_list nn_information_contact dep_e.g._information amod_genres_multiple prep_of_instances_genres prep_contain_e.g. dobj_contain_instances aux_contain_can nsubj_contain_document mark_contain_because amod_document_single det_document_a nn_level_segment amod_level_page amod_level_intra-document det_level_an conj_or_intra-document_page advcl_work_contain prep_on_work_level advmod_work_also nsubj_work_genres rcmod_documents_work nn_documents_<context> nn_documents_<contexts> nn_documents_</booktitle> nn_documents_HICSS-35
N06-3005	W02-1011	o	So far research in automatic opinion recognition has primarily addressed learning subjective language -LRB- Wiebe et al. 2004 Riloff et al. 2003 Riloff and Wiebe 2003 -RRB- identifying opinionated documents -LRB- Yu and Hatzivassiloglou 2003 -RRB- and sentences -LRB- Yu and Hatzivassiloglou 2003 Riloff et al. 2003 Riloff and Wiebe 2003 -RRB- and discriminating between positive and negative language -LRB- Yu and Hatzivassiloglou 2003 Turney and Littman 2003 Pang et al. 2002 Dave et al. 2003 Nasukawa and Yi 2003 Morinaga et al. 2002 -RRB-	num_Morinaga_2002 nn_Morinaga_al. nn_Morinaga_et num_Dave_2003 nn_Dave_al. nn_Dave_et num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Morinaga conj_and_Turney_2003 conj_and_Turney_Yi conj_and_Turney_Nasukawa conj_and_Turney_Dave conj_and_Turney_Pang conj_and_Turney_2003 conj_and_Turney_Littman conj_and_Yu_2003 conj_and_Yu_Yi conj_and_Yu_Nasukawa conj_and_Yu_Dave conj_and_Yu_Pang conj_and_Yu_2003 conj_and_Yu_Littman conj_and_Yu_Turney conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_language_Turney dep_language_2003 dep_language_Hatzivassiloglou dep_language_Yu amod_language_negative amod_language_positive conj_and_positive_negative prep_between_discriminating_language dep_Riloff_2003 conj_and_Riloff_Wiebe num_Riloff_2003 nn_Riloff_al. nn_Riloff_et dep_Yu_Wiebe dep_Yu_Riloff conj_and_Yu_Riloff conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou appos_sentences_Riloff appos_sentences_2003 appos_sentences_Hatzivassiloglou appos_sentences_Yu conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou conj_and_documents_sentences dep_documents_2003 dep_documents_Hatzivassiloglou dep_documents_Yu amod_documents_opinionated dobj_identifying_sentences dobj_identifying_documents dep_Riloff_2003 conj_and_Riloff_Wiebe conj_and_Riloff_discriminating vmod_Riloff_identifying dep_Riloff_Wiebe dep_Riloff_Riloff num_Riloff_2003 nn_Riloff_al. nn_Riloff_et dep_Wiebe_discriminating dep_Wiebe_Riloff appos_Wiebe_2004 dep_Wiebe_al. nn_Wiebe_et amod_language_subjective dep_learning_Wiebe dobj_learning_language xcomp_addressed_learning advmod_addressed_primarily aux_addressed_has nsubj_addressed_research nn_recognition_opinion amod_recognition_automatic prep_in_research_recognition advmod_research_far advmod_research_So
N07-1033	W02-1011	p	We chose a dataset that would be enjoyable to reannotate the movie review dataset of -LRB- Pang et al. 2002 Pang and Lee 2004 -RRB- .3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database -LRB- IMDb -RRB- review archive all written before 2002 by a total of 312 authors with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable	advmod_valuable_equally cop_valuable_are nsubj_valuable_rationales mark_valuable_that det_rationales_all ccomp_means_valuable amod_means_constant cop_means_be aux_means_to nn_Ccontrast_2Taking prep_per_author_Ccontrast prep_per_reviews_author num_reviews_20 prep_of_cap_reviews det_cap_a num_authors_312 prep_of_total_authors det_total_a agent_written_total prep_before_written_2002 vmod_all_written vmod_archive_means prep_with_archive_cap appos_archive_all nn_archive_review nn_archive_IMDb dep_Database_archive nn_Database_Movie nn_Database_Internet det_Database_the prep_from_obtained_Database nn_reviews_movie amod_reviews_negative num_reviews_1000 dep_positive_obtained conj_and_positive_reviews amod_1000_reviews amod_1000_positive prep_of_consists_1000 nsubj_consists_dataset det_dataset_The rcmod_.3_consists dep_Pang_2004 conj_and_Pang_Lee dep_Pang_.3 dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_dataset_Pang nn_dataset_review nn_dataset_movie det_dataset_the prep_to_enjoyable_reannotate cop_enjoyable_be aux_enjoyable_would nsubj_enjoyable_that dep_dataset_dataset rcmod_dataset_enjoyable det_dataset_a dobj_chose_dataset nsubj_chose_We
N07-1038	W02-1011	o	2 Related Work Sentiment Classi cation Traditionally categorization of opinion texts has been cast as a binary classication task -LRB- Pang et al. 2002 Turney 2002 Yu and Hatzivassiloglou 2003 Dave et al. 2003 -RRB-	num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Yu_Dave conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Turney_2003 dep_Turney_Hatzivassiloglou dep_Turney_Yu num_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_task_classication amod_task_binary det_task_a dep_cast_Pang prep_as_cast_task auxpass_cast_been aux_cast_has nsubjpass_cast_categorization nn_texts_opinion prep_of_categorization_texts rcmod_cation_cast advmod_cation_Traditionally nn_cation_Classi nn_cation_Sentiment nn_cation_Work amod_cation_Related num_cation_2
N07-1038	W02-1011	o	1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text -LRB- Pang et al. 2002 Turney 2002 Yu and Hatzivassiloglou 2003 -RRB-	dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Turney_Hatzivassiloglou dep_Turney_Yu num_Turney_2002 nn_al._et dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_text_opinion det_text_an prep_of_polarity_text det_polarity_the dobj_express_polarity aux_express_can nsubj_express_score mark_express_that amod_score_single det_score_a ccomp_assumption_express amod_assumption_implicit det_assumption_an dep_makes_Pang dobj_makes_assumption nsubj_makes_Introduction nn_categorization_sentiment prep_on_work_categorization amod_work_Previous dep_Introduction_work num_Introduction_1
N07-1038	W02-1011	o	3.4 Feature Representation Ranking Models Following previous work on sentiment classi cation -LRB- Pang et al. 2002 -RRB- we represent each review as a vector of lexical features	amod_features_lexical prep_of_vector_features det_vector_a det_review_each prep_as_represent_vector dobj_represent_review nsubj_represent_we nsubj_represent_Models amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_cation_classi nn_cation_sentiment prep_on_work_cation amod_work_previous dep_Models_Pang prep_following_Models_work nn_Models_Ranking nn_Models_Representation nn_Models_Feature num_Models_3.4
N07-1039	W02-1011	o	Sentiment analysis includes a variety of different problems including sentiment classification techniques to classify reviews as positive or negative based on bag of words -LRB- Pang et al. 2002 -RRB- or positive and negative words -LRB- Turney 2002 Mullen and Collier 2004 -RRB- classifying sentences in a document as either subjective or objective -LRB- Riloff and Wiebe 2003 Pang and Lee 2004 -RRB- identifying or classifying appraisal targets -LRB- Nigam and Hurst 2004 -RRB- identifying the source of an opinion in a text -LRB- Choi et al. 2005 -RRB- whether the author is expressing the opinion or whether he is attributing the opinion to someone else and developing interactive and visual opinion mining methods -LRB- Gamon et al. 2005 Popescu and Etzioni 2005 -RRB-	dep_Popescu_2005 conj_and_Popescu_Etzioni dep_Gamon_Etzioni dep_Gamon_Popescu appos_Gamon_2005 dep_Gamon_al. nn_Gamon_et dep_methods_Gamon nn_methods_mining nn_methods_opinion amod_methods_visual amod_methods_interactive conj_and_interactive_visual dobj_developing_methods advmod_someone_else det_opinion_the prep_to_attributing_someone dobj_attributing_opinion aux_attributing_is nsubj_attributing_he mark_attributing_whether det_opinion_the conj_or_expressing_attributing dobj_expressing_opinion aux_expressing_is nsubj_expressing_author mark_expressing_whether det_author_the amod_Choi_2005 dep_Choi_al. nn_Choi_et det_text_a prep_in_opinion_text det_opinion_an prep_of_source_opinion det_source_the ccomp_identifying_attributing ccomp_identifying_expressing dep_identifying_Choi dobj_identifying_source dep_Nigam_2004 conj_and_Nigam_Hurst dep_targets_Hurst dep_targets_Nigam nn_targets_appraisal dobj_identifying_targets conj_or_identifying_classifying amod_Pang_2004 conj_and_Pang_Lee dep_Riloff_Lee dep_Riloff_Pang dep_Riloff_2003 conj_and_Riloff_Wiebe dep_subjective_Wiebe dep_subjective_Riloff conj_or_subjective_objective preconj_subjective_either det_document_a prep_as_sentences_objective prep_as_sentences_subjective prep_in_sentences_document amod_sentences_classifying dep_Mullen_2004 conj_and_Mullen_Collier dep_Turney_Collier dep_Turney_Mullen appos_Turney_2002 appos_words_Turney dep_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_bag_words conj_or_positive_negative prep_as_classify_negative prep_as_classify_positive dobj_classify_reviews aux_classify_to conj_and_techniques_developing conj_and_techniques_identifying conj_and_techniques_classifying conj_and_techniques_identifying conj_and_techniques_sentences dep_techniques_words conj_and_techniques_negative conj_or_techniques_positive dep_techniques_Pang prep_based_on_techniques_bag vmod_techniques_classify nn_techniques_classification nn_techniques_sentiment prep_including_problems_developing prep_including_problems_identifying prep_including_problems_identifying prep_including_problems_sentences prep_including_problems_negative prep_including_problems_positive prep_including_problems_techniques amod_problems_different prep_of_variety_problems det_variety_a dobj_includes_variety nsubj_includes_analysis nn_analysis_Sentiment
N09-1001	W02-1011	o	Automatic identification of subjective content often relies on word indicators such as unigrams -LRB- Pang et al. 2002 -RRB- or predetermined sentiment lexica -LRB- Wilson et al. 2005 -RRB-	amod_Wilson_2005 dep_Wilson_al. nn_Wilson_et nn_lexica_sentiment amod_lexica_predetermined dep_Pang_2002 dep_Pang_al. nn_Pang_et conj_or_unigrams_lexica dep_unigrams_Pang dep_indicators_Wilson prep_such_as_indicators_lexica prep_such_as_indicators_unigrams nn_indicators_word prep_on_relies_indicators advmod_relies_often nsubj_relies_identification amod_content_subjective prep_of_identification_content nn_identification_Automatic
N09-1001	W02-1011	o	2 Related Work There has been a large and diverse body of research in opinion mining with most research at the text -LRB- Pang et al. 2002 Pang and Lee 2004 Popescu and Etzioni 2005 Ounis et al. 2006 -RRB- sentence -LRB- Kim and Hovy 2005 Kudo and Matsumoto 2004 Riloff et al. 2003 Yu and Hatzivassiloglou 2003 -RRB- or word -LRB- Hatzivassiloglou and McKeown 1997 Turney and Littman 2003 Kim and Hovy 2004 Takamura et al. 2005 Andreevskaia and Bergler 2006 Kaji and Kitsuregawa 2007 -RRB- level	dep_level_Kitsuregawa dep_level_Kaji dep_level_2006 dep_level_Bergler dep_level_Andreevskaia dep_Andreevskaia_2007 conj_and_Andreevskaia_Kitsuregawa conj_and_Andreevskaia_Kaji conj_and_Andreevskaia_2006 conj_and_Andreevskaia_Bergler num_Takamura_2005 nn_Takamura_al. nn_Takamura_et num_Kim_2004 conj_and_Kim_Hovy dep_Turney_level conj_and_Turney_Takamura conj_and_Turney_Hovy conj_and_Turney_Kim conj_and_Turney_2003 conj_and_Turney_Littman dep_Hatzivassiloglou_Takamura dep_Hatzivassiloglou_Kim dep_Hatzivassiloglou_2003 dep_Hatzivassiloglou_Littman dep_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown dep_word_1997 dep_word_McKeown dep_word_Hatzivassiloglou dep_Yu_2003 conj_and_Yu_Hatzivassiloglou num_Riloff_2003 nn_Riloff_al. nn_Riloff_et num_Kudo_2004 conj_and_Kudo_Matsumoto dep_Kim_Hatzivassiloglou dep_Kim_Yu conj_and_Kim_Riloff conj_and_Kim_Matsumoto conj_and_Kim_Kudo conj_and_Kim_2005 conj_and_Kim_Hovy dep_sentence_Riloff dep_sentence_Kudo dep_sentence_2005 dep_sentence_Hovy dep_sentence_Kim num_Ounis_2006 nn_Ounis_al. nn_Ounis_et conj_or_Popescu_word conj_and_Popescu_sentence conj_and_Popescu_Ounis conj_and_Popescu_2005 conj_and_Popescu_Etzioni dep_Pang_word dep_Pang_sentence dep_Pang_Ounis dep_Pang_2005 dep_Pang_Etzioni dep_Pang_Popescu num_Pang_2004 conj_and_Pang_Lee dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et det_text_the amod_research_most nn_mining_opinion dep_body_Pang prep_at_body_text prep_with_body_research prep_in_body_mining prep_of_body_research amod_body_diverse amod_body_large det_body_a cop_body_been aux_body_has expl_body_There dep_body_Work conj_and_large_diverse amod_Work_Related num_Work_2
N09-1001	W02-1011	o	W -LRB- S T -RRB- = summationdisplay uS vT w -LRB- u v -RRB- Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice using the maximum flow algorithm -LRB- Pang and Lee 2004 Cormen et al. 2002 -RRB-	num_Cormen_2002 nn_Cormen_al. nn_Cormen_et dep_Pang_Cormen conj_and_Pang_2004 conj_and_Pang_Lee dep_algorithm_2004 dep_algorithm_Lee dep_algorithm_Pang nn_algorithm_flow nn_algorithm_maximum det_algorithm_the dobj_using_algorithm prep_in_time_practice amod_time_running amod_time_near-linear amod_time_polynomial conj_and_found_time prep_in_found_time auxpass_found_be aux_found_can nsubjpass_found_cuts nn_cuts_minimum amod_cuts_optimal advmod_cuts_Globally vmod_u_using rcmod_u_time rcmod_u_found appos_u_v dep_w_u nn_w_vT appos_uS_w nn_uS_summationdisplay dep_=_uS appos_S_T dep_W_= dep_W_S dep_``_W
N09-1031	W02-1011	o	Many researchers have focused the related problem of predicting sentiment and opinion in text -LRB- Pang et al. 2002 Wiebe and Riloff 2005 -RRB- sometimes connected to extrinsic values like prediction markets -LRB- Lerman et al. 2008 -RRB-	amod_Lerman_2008 dep_Lerman_al. nn_Lerman_et nn_markets_prediction prep_like_values_markets amod_values_extrinsic dep_connected_Lerman prep_to_connected_values advmod_connected_sometimes dep_Wiebe_2005 conj_and_Wiebe_Riloff dep_Pang_Riloff dep_Pang_Wiebe appos_Pang_2002 dep_Pang_al. nn_Pang_et prep_in_sentiment_text conj_and_sentiment_opinion dobj_predicting_opinion dobj_predicting_sentiment prepc_of_problem_predicting amod_problem_related det_problem_the dep_focused_connected dep_focused_Pang dobj_focused_problem aux_focused_have nsubj_focused_researchers amod_researchers_Many ccomp_``_focused
N09-1031	W02-1011	o	Applications have included the categorization of documents by topic -LRB- Joachims 1998 -RRB- language -LRB- Cavnar and Trenkle 1994 -RRB- genre -LRB- Karlgren and Cutting 1994 -RRB- author -LRB- Bosch and Smith 1998 -RRB- sentiment -LRB- Pang et al. 2002 -RRB- and desirability -LRB- Sahami et al. 1998 -RRB-	dep_al._1998 nn_al._et nn_al._Sahami dep_desirability_al. amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_sentiment_Pang amod_Bosch_1998 conj_and_Bosch_Smith appos_author_Smith appos_author_Bosch dep_Karlgren_1994 conj_and_Karlgren_Cutting appos_genre_Cutting appos_genre_Karlgren dep_Cavnar_1994 conj_and_Cavnar_Trenkle appos_language_Trenkle appos_language_Cavnar amod_Joachims_1998 conj_and_topic_desirability conj_and_topic_sentiment conj_and_topic_author appos_topic_genre dep_topic_language dep_topic_Joachims prep_of_categorization_documents det_categorization_the prep_by_included_desirability prep_by_included_sentiment prep_by_included_author prep_by_included_topic dobj_included_categorization aux_included_have nsubj_included_Applications
N09-1055	W02-1011	o	The research of opinion mining began in 1997 the early research results mainly focused on the polarity of opinion words -LRB- Hatzivassiloglou et al. 1997 -RRB- and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text -LRB- Turney et al. 2003 Pang et al. 2002 Zagibalov et al. 2008 ;-RRB-	num_;-RRB-_2008 appos_Zagibalov_;-RRB- dep_Zagibalov_al. nn_Zagibalov_et num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Zagibalov dep_Turney_Pang amod_Turney_2003 dep_Turney_al. nn_Turney_et num_text_one nn_words_opinion amod_words_negative amod_words_positive conj_or_positive_negative prep_of_number_words det_number_the conj_or_positive_negative preconj_positive_either prep_in_classification_text prep_on_classification_number prep_of_classification_negative prep_of_classification_positive det_classification_a nn_mining_opinion amod_mining_text-level det_mining_the dobj_treated_mining nsubj_treated_research amod_Hatzivassiloglou_1997 dep_Hatzivassiloglou_al. nn_Hatzivassiloglou_et nn_words_opinion prep_of_polarity_words det_polarity_the prep_on_focused_polarity advmod_focused_mainly dep_results_Turney prep_as_results_classification conj_and_results_treated dep_results_Hatzivassiloglou vmod_results_focused nsubj_results_research ccomp_results_began amod_research_early det_research_the prep_in_began_1997 nsubj_began_research nn_mining_opinion prep_of_research_mining det_research_The
N09-1056	W02-1011	o	Examples of such early work include -LRB- Turney 2002 Pang et al. 2002 Dave et al. 2003 Hu and Liu 2004 Popescu and Etzioni 2005 -RRB-	dep_Popescu_2005 conj_and_Popescu_Etzioni num_Hu_2004 conj_and_Hu_Liu num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_Etzioni dep_Pang_Popescu conj_Pang_Liu conj_Pang_Hu conj_Pang_Dave num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang dep_Turney_2002 dep_include_Turney nsubj_include_Examples amod_work_early amod_work_such prep_of_Examples_work
N09-2046	W02-1011	o	1 Introduction In the community of sentiment analysis -LRB- Turney 2002 Pang et al. 2002 Tang et al. 2009 -RRB- transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work because sentiment expression often behaves with strong domain-specific nature	amod_nature_domain-specific amod_nature_strong prep_with_behaves_nature advmod_behaves_often nsubj_behaves_expression mark_behaves_because nn_expression_sentiment amod_work_trivial det_work_a prep_from_far_work advmod_far_still cop_far_is nsubj_far_classifier nn_domain_target det_domain_another nn_domain_source num_domain_one prep_to_classifier_domain prep_from_classifier_domain rcmod_sentiment_far det_sentiment_a dobj_transferring_sentiment num_Tang_2009 nn_Tang_al. nn_Tang_et num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Tang conj_Turney_Pang num_Turney_2002 nn_analysis_sentiment appos_community_Turney prep_of_community_analysis det_community_the dep_Introduction_behaves vmod_Introduction_transferring prep_in_Introduction_community num_Introduction_1
P04-1034	W02-1011	o	a larger number of labeled documents its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_models_Entropy nn_models_Maximum conj_and_Machines_models nn_Machines_Vector nn_Machines_Support prep_of_that_models prep_of_that_Machines dep_comparable_Pang prep_to_comparable_that cop_comparable_is nsubj_comparable_performance dep_comparable_number det_corpus_this prep_on_performance_corpus poss_performance_its amod_documents_labeled prep_of_number_documents amod_number_larger det_number_a
P04-3025	W02-1011	n	Although such approaches have been employed effectively -LRB- Pang et al. 2002 -RRB- there appears to remain considerable room for improvement	prep_for_room_improvement amod_room_considerable xcomp_remain_room aux_remain_to xcomp_appears_remain expl_appears_there advcl_appears_employed amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_employed_Pang advmod_employed_effectively auxpass_employed_been aux_employed_have nsubjpass_employed_approaches mark_employed_Although amod_approaches_such
P06-1133	W02-1011	o	There are studies on learning subjective language -LRB- Wiebe et al. 2004 -RRB- identifying opinionated documents -LRB- Yu and Hatzivassiloglou 2003 -RRB- and sentences -LRB- Riloff et al. 2003 Riloff and Wiebe 2003 -RRB- and discriminating between positive and negative language -LRB- Turney and Littman 2003 Pang et al. 2002 Dave et al. 2003 Nasukawa and Yi 2003 Morinaga et al. 2002 -RRB-	num_Morinaga_2002 nn_Morinaga_al. nn_Morinaga_et dep_Nasukawa_Morinaga conj_and_Nasukawa_2003 conj_and_Nasukawa_Yi num_Dave_2003 nn_Dave_al. nn_Dave_et num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_2003 dep_Turney_Yi dep_Turney_Nasukawa conj_and_Turney_Dave conj_and_Turney_Pang conj_and_Turney_2003 conj_and_Turney_Littman dep_language_Dave dep_language_Pang dep_language_2003 dep_language_Littman dep_language_Turney amod_language_negative amod_language_positive conj_and_positive_negative prep_between_discriminating_language dep_Riloff_2003 conj_and_Riloff_Wiebe dep_Riloff_Wiebe dep_Riloff_Riloff appos_Riloff_2003 dep_Riloff_al. nn_Riloff_et conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou conj_and_documents_sentences dep_documents_2003 dep_documents_Hatzivassiloglou dep_documents_Yu amod_documents_opinionated dobj_identifying_sentences dobj_identifying_documents amod_Wiebe_2004 dep_Wiebe_al. nn_Wiebe_et amod_language_subjective dobj_learning_language conj_and_studies_discriminating dep_studies_Riloff vmod_studies_identifying dep_studies_Wiebe prepc_on_studies_learning nsubj_are_discriminating nsubj_are_studies expl_are_There ccomp_``_are
P06-2059	W02-1011	o	So far this approach has been taken by a lot of researchers -LRB- Pang et al. 2002 Dave et al. 2003 Wilson et al. 2005 -RRB-	num_Wilson_2005 nn_Wilson_al. nn_Wilson_et num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_Wilson dep_Pang_Dave amod_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_lot_researchers det_lot_a dep_taken_Pang agent_taken_lot auxpass_taken_been aux_taken_has nsubjpass_taken_approach advmod_taken_far det_approach_this advmod_far_So
P06-2059	W02-1011	o	Negation was processed in a similar way as previous works -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_works_Pang amod_works_previous prep_as_way_works amod_way_similar det_way_a prep_in_processed_way auxpass_processed_was nsubjpass_processed_Negation
P06-2059	W02-1011	o	Consider the following example -LRB- Pang et al. 2002 -RRB- This film should be brilliant	cop_brilliant_be aux_brilliant_should nsubj_brilliant_film det_film_This amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_example_brilliant dep_example_Pang amod_example_following det_example_the dobj_Consider_example
P06-2059	W02-1011	o	Pang et al. proposed a method of classifying movie reviews into positive and negative ones -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et amod_ones_negative amod_ones_positive conj_and_positive_negative nn_reviews_movie amod_reviews_classifying prep_of_method_reviews det_method_a dep_proposed_Pang prep_into_proposed_ones dobj_proposed_method advmod_proposed_al. nn_al._et vmod_Pang_proposed
P06-2063	W02-1011	o	Document level sentiment classification is mostly applied to reviews where systems assign a positive or negative sentiment for a whole review document -LRB- Pang et al. 2002 Turney 2002 -RRB-	amod_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_document_review amod_document_whole det_document_a amod_sentiment_negative amod_sentiment_positive det_sentiment_a conj_or_positive_negative dep_assign_Pang prep_for_assign_document dobj_assign_sentiment nsubj_assign_systems advmod_assign_where rcmod_reviews_assign prep_to_applied_reviews advmod_applied_mostly auxpass_applied_is nsubjpass_applied_classification nn_classification_sentiment nn_classification_level nn_classification_Document
P06-2079	W02-1011	o	As a sanity check we duplicated Pang et al. s -LRB- 2002 -RRB- baseline in which all unigrams that appear four or more times in the training documents are used as features	prep_as_used_features auxpass_used_are nsubjpass_used_times nn_documents_training det_documents_the prep_in_times_documents num_times_more num_times_four conj_or_four_more ccomp_appear_used nsubj_appear_that rcmod_unigrams_appear det_unigrams_all prep_in_baseline_unigrams prep_in_baseline_which nn_baseline_s appos_s_2002 nn_s_al. nn_s_et nn_s_Pang dobj_duplicated_baseline nsubj_duplicated_we prep_as_duplicated_check nn_check_sanity det_check_a
P06-2081	W02-1011	o	A richer set of features besides n-grams should be checked and we should not ignore the potential effectiveness of unigrams in this task -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et det_task_this prep_of_effectiveness_unigrams amod_effectiveness_potential det_effectiveness_the prep_in_ignore_task dobj_ignore_effectiveness neg_ignore_not aux_ignore_should nsubj_ignore_we dep_checked_Pang conj_and_checked_ignore auxpass_checked_be aux_checked_should nsubjpass_checked_set prep_besides_features_n-grams prep_of_set_features amod_set_richer det_set_A
P06-2081	W02-1011	o	Work focusses on analysing subjective features of text or speech such as sentiment opinion emotion or point of view -LRB- Pang et al. 2002 Turney 2002 Dave et al. 2003 Liu et al. 2003 Pang and Lee 2005 Shanahan et al. 2005 -RRB-	num_Shanahan_2005 nn_Shanahan_al. nn_Shanahan_et num_Liu_2003 nn_Liu_al. nn_Liu_et num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Turney_Shanahan num_Turney_2005 conj_and_Turney_Lee conj_and_Turney_Pang conj_and_Turney_Liu conj_and_Turney_Dave num_Turney_2002 dep_Pang_Lee dep_Pang_Pang dep_Pang_Liu dep_Pang_Dave dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_point_view conj_or_sentiment_point conj_or_sentiment_emotion conj_or_sentiment_opinion conj_or_text_speech prep_such_as_features_point prep_such_as_features_emotion prep_such_as_features_opinion prep_such_as_features_sentiment prep_of_features_speech prep_of_features_text amod_features_subjective dobj_analysing_features dep_focusses_Pang prepc_on_focusses_analysing nn_focusses_Work ccomp_``_focusses
P07-1053	W02-1011	o	Finally other approaches rely on reviews with numeric ratings from websites -LRB- Pang and Lee 2002 Dave et al. 2003 Pang and Lee 2004 Cui et al. 2006 -RRB- and train -LRB- semi -RRB- supervised learning algorithms to classify reviews as positive or negative or in more fine-grained scales -LRB- Pang and Lee 2005 Wilson et al. 2006 -RRB-	num_Wilson_2006 nn_Wilson_al. nn_Wilson_et dep_Pang_Wilson conj_and_Pang_2005 conj_and_Pang_Lee dep_scales_2005 dep_scales_Lee dep_scales_Pang amod_scales_fine-grained amod_scales_more pobj_in_scales conj_or_positive_negative prep_as_classify_negative prep_as_classify_positive dobj_classify_reviews aux_classify_to conj_or_learning_in vmod_learning_classify dobj_learning_algorithms dep_supervised_in dep_supervised_learning nsubj_supervised_train nsubj_supervised_websites appos_train_semi num_Cui_2006 nn_Cui_al. nn_Cui_et num_Pang_2004 conj_and_Pang_Lee num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_Cui conj_and_Pang_Lee conj_and_Pang_Pang conj_and_Pang_Dave conj_and_Pang_2002 conj_and_Pang_Lee conj_and_websites_train appos_websites_Pang appos_websites_Dave appos_websites_2002 appos_websites_Lee appos_websites_Pang prepc_from_ratings_supervised amod_ratings_numeric prep_with_reviews_ratings prep_on_rely_reviews nsubj_rely_approaches advmod_rely_Finally amod_approaches_other
P07-1055	W02-1011	o	Previous workonsentimentanalysishascoveredawiderange of tasks including polarity classification -LRB- Pang et al. 2002 Turney 2002 -RRB- opinion extraction -LRB- Pang and Lee 2004 -RRB- and opinion source assignment -LRB- Choi et al. 2005 Choi et al. 2006 -RRB-	num_Choi_2006 nn_Choi_al. nn_Choi_et dep_Choi_Choi appos_Choi_2005 dep_Choi_al. nn_Choi_et nn_assignment_source nn_assignment_opinion dep_Pang_2004 conj_and_Pang_Lee appos_extraction_Lee appos_extraction_Pang nn_extraction_opinion dep_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_classification_assignment conj_and_classification_extraction appos_classification_Pang nn_classification_polarity prep_including_tasks_assignment prep_including_tasks_extraction prep_including_tasks_classification dep_workonsentimentanalysishascoveredawiderange_Choi prep_of_workonsentimentanalysishascoveredawiderange_tasks amod_workonsentimentanalysishascoveredawiderange_Previous ccomp_``_workonsentimentanalysishascoveredawiderange
P07-1055	W02-1011	o	Furthermore these systems have tackled the problem at different levels of granularity from the document level -LRB- Pang et al. 2002 -RRB- sentence level -LRB- Pang and Lee 2004 Mao and Lebanon 2006 -RRB- phrase level -LRB- Turney 2002 Choi et al. 2005 -RRB- as well as the speaker level in debates -LRB- Thomas et al. 2006 -RRB-	amod_Thomas_2006 dep_Thomas_al. nn_Thomas_et prep_in_level_debates nn_level_speaker det_level_the num_Choi_2005 nn_Choi_al. nn_Choi_et dep_Turney_Choi appos_Turney_2002 appos_level_Turney nn_level_phrase num_Mao_2006 conj_and_Mao_Lebanon dep_Pang_Lebanon dep_Pang_Mao conj_and_Pang_2004 conj_and_Pang_Lee appos_level_2004 appos_level_Lee appos_level_Pang nn_level_sentence amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_level_document det_level_the prep_of_levels_granularity amod_levels_different prep_at_problem_levels det_problem_the dep_tackled_Thomas conj_and_tackled_level conj_and_tackled_level dep_tackled_level dep_tackled_Pang prep_from_tackled_level dobj_tackled_problem aux_tackled_have nsubj_tackled_systems advmod_tackled_Furthermore det_systems_these ccomp_``_level ccomp_``_level ccomp_``_tackled
P07-1056	W02-1011	o	After this conversion we had 1000 positive and 1000 negative examples for each domain the same balanced composition as the polarity dataset -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_dataset_Pang nn_dataset_polarity det_dataset_the prep_as_composition_dataset amod_composition_balanced amod_composition_same det_composition_the det_domain_each amod_examples_negative num_examples_1000 prep_for_positive_domain conj_and_positive_examples amod_1000_examples amod_1000_positive dobj_had_composition dobj_had_1000 nsubj_had_we prep_after_had_conversion det_conversion_this
P07-1056	W02-1011	o	1 Introduction Sentiment detection and classification has received considerable attention recently -LRB- Pang et al. 2002 Turney 2002 Goldberg and Zhu 2004 -RRB-	amod_Turney_2004 conj_and_Turney_Zhu conj_and_Turney_Goldberg conj_and_Turney_2002 dep_Pang_Zhu dep_Pang_Goldberg dep_Pang_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et amod_attention_considerable dep_received_Pang advmod_received_recently dobj_received_attention aux_received_has nsubj_received_classification nsubj_received_detection conj_and_detection_classification nn_detection_Sentiment nn_detection_Introduction num_detection_1 ccomp_``_received
P07-1123	W02-1011	o	This approach builds a subjectivity-annotated corpus for the target language through projection and then trains a statistical classifier on the resulting corpus -LRB- numerous statistical classifiers have been trained for subjectivity or sentiment classification e.g. -LRB- Pang et al. 2002 Yu and Hatzivassiloglou 2003 -RRB- -RRB-	appos_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Pang_Hatzivassiloglou dep_Pang_Yu appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_classification_sentiment nn_classification_subjectivity conj_or_subjectivity_sentiment dep_trained_Pang prep_trained_e.g. prep_for_trained_classification auxpass_trained_been aux_trained_have nsubjpass_trained_classifiers amod_classifiers_statistical amod_classifiers_numerous amod_corpus_resulting det_corpus_the rcmod_classifier_trained prep_on_classifier_corpus amod_classifier_statistical det_classifier_a dobj_trains_classifier advmod_trains_then nn_language_target det_language_the prep_for_corpus_language amod_corpus_subjectivity-annotated det_corpus_a conj_and_builds_trains prep_through_builds_projection dobj_builds_corpus nsubj_builds_approach det_approach_This
P07-1123	W02-1011	o	Similar to e.g. -LRB- Pang et al. 2002 -RRB- we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications	amod_classifications_objective det_classifications_the conj_and_subjective_classifications det_subjective_the prep_with_cooccurring_classifications prep_with_cooccurring_subjective vmod_features_cooccurring nn_features_word prep_on_trained_features vmod_algorithm_trained nn_algorithm_Bayes amod_algorithm_Naive det_algorithm_a dobj_use_algorithm nsubj_use_we dep_use_Pang advmod_use_e.g. dep_use_to dep_use_Similar nn_al._et amod_Pang_2002 dep_Pang_al.
P07-1123	W02-1011	o	2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications such as tracking sentiment timelines in online forums and news -LRB- Lloyd et al. 2005 Balog et al. 2006 -RRB- review classification -LRB- Turney 2002 Pang et al. 2002 -RRB- mining opinions from product reviews -LRB- Hu and Liu 2004 -RRB- automatic expressive text-to-speech synthesis -LRB- Alm et al. 2005 -RRB- text semantic analysis -LRB- Wiebe and Mihalcea 2006 Esuli and Sebastiani 2006 -RRB- and question answering -LRB- Yu and Hatzivassiloglou 2003 -RRB-	amod_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_answering_Hatzivassiloglou dep_answering_Yu dep_question_answering dep_Esuli_2006 conj_and_Esuli_Sebastiani dep_Wiebe_Sebastiani dep_Wiebe_Esuli conj_and_Wiebe_2006 conj_and_Wiebe_Mihalcea appos_analysis_2006 appos_analysis_Mihalcea appos_analysis_Wiebe amod_analysis_semantic nn_analysis_text amod_Alm_2005 dep_Alm_al. nn_Alm_et dep_synthesis_Alm amod_synthesis_text-to-speech amod_synthesis_expressive amod_synthesis_automatic dep_Hu_2004 conj_and_Hu_Liu dep_reviews_Liu dep_reviews_Hu nn_reviews_product prep_from_opinions_reviews nn_opinions_mining num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang appos_Turney_2002 dep_classification_Turney nn_classification_review num_Balog_2006 nn_Balog_al. nn_Balog_et dep_Lloyd_Balog amod_Lloyd_2005 dep_Lloyd_al. nn_Lloyd_et conj_and_forums_news amod_forums_online conj_and_timelines_question appos_timelines_analysis appos_timelines_synthesis appos_timelines_opinions appos_timelines_classification appos_timelines_Lloyd prep_in_timelines_news prep_in_timelines_forums nn_timelines_sentiment nn_timelines_tracking prep_such_as_applications_question prep_such_as_applications_timelines nn_applications_processing nn_applications_text prep_of_variety_applications amod_variety_wide det_variety_a prep_in_used_variety auxpass_used_been aux_used_have nsubjpass_used_methods nn_methods_analysis nn_methods_subjectivity nn_methods_Automatic nn_methods_Motivation num_methods_2
P07-2023	W02-1011	o	Researchers extracted opinions from words sentences and documents and both rule-based and statistical models are investigated -LRB- Wiebe et al. 2002 Pang et al. 2002 -RRB-	num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Wiebe_Pang appos_Wiebe_2002 dep_Wiebe_al. nn_Wiebe_et dep_investigated_Wiebe auxpass_investigated_are nsubjpass_investigated_models amod_models_statistical amod_models_rule-based conj_and_rule-based_statistical preconj_rule-based_both conj_and_words_documents conj_and_words_sentences conj_and_extracted_investigated prep_from_extracted_documents prep_from_extracted_sentences prep_from_extracted_words dobj_extracted_opinions nsubj_extracted_Researchers ccomp_``_investigated ccomp_``_extracted
P07-3012	W02-1011	o	-LSB- subjective -RSB- So far none of the studies in sentiment detection -LRB- e.g. Wilson et al. 2005 Pang et al. 2002 -RRB- or opinion extraction -LRB- e.g. Hu and Liu 2004 Popescu and Etzioni 2005 -RRB- have specifically looked at the role of superlatives in these areas	det_areas_these prep_in_role_areas prep_of_role_superlatives det_role_the prep_at_looked_role advmod_looked_specifically aux_looked_have nsubj_looked_none num_Popescu_2005 conj_and_Popescu_Etzioni num_Liu_2004 dep_Hu_Etzioni dep_Hu_Popescu conj_and_Hu_Liu nn_Hu_e.g. dep_extraction_Liu dep_extraction_Hu nn_extraction_opinion num_Pang_2002 nn_Pang_al. nn_Pang_et dep_2005_Pang dep_2005_al. nn_al._et nn_al._Wilson nn_al._e.g. conj_or_detection_extraction dep_detection_2005 nn_detection_sentiment prep_in_studies_extraction prep_in_studies_detection det_studies_the prep_of_none_studies dep_far_looked advmod_far_So dep_far_subjective
P08-1034	W02-1011	o	But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews -LRB- Pang et al. 2002 -RRB-	dep_Pang_2002 dep_Pang_al. nn_Pang_et nn_reviews_movie prep_of_sets_reviews nn_sets_training amod_sets_in-domain amod_sets_large advmod_large_sufficiently prep_on_built_sets vmod_models_built amod_models_statistical dep_worse_Pang prep_than_worse_models dobj_perform_worse aux_perform_to xcomp_shown_perform auxpass_shown_were nsubjpass_shown_lists cc_shown_But nn_lists_word amod_lists_general amod_lists_such
P08-1034	W02-1011	o	291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text -LRB- Aue and Gamon 2005 Pang et al. 2002 Pang and Lee 2004 Riloff et al. 2006 Turney 2002 Turney and Littman 2003 -RRB- or at the sentence levels -LRB- Gamon and Aue 2005 Hu and Liu 2004 Kim and Hovy 2005 Riloff et al. 2006 -RRB-	num_Riloff_2006 nn_Riloff_al. nn_Riloff_et num_Kim_2005 conj_and_Kim_Hovy num_Hu_2004 conj_and_Hu_Liu dep_Gamon_Riloff conj_and_Gamon_Hovy conj_and_Gamon_Kim conj_and_Gamon_Liu conj_and_Gamon_Hu conj_and_Gamon_2005 conj_and_Gamon_Aue nn_levels_sentence det_levels_the dep_Turney_2003 conj_and_Turney_Littman num_Turney_2002 num_Riloff_2006 nn_Riloff_al. nn_Riloff_et num_Pang_2004 conj_and_Pang_Lee num_Pang_2002 nn_Pang_al. nn_Pang_et conj_and_Aue_Littman conj_and_Aue_Turney conj_and_Aue_Turney conj_and_Aue_Riloff conj_and_Aue_Lee conj_and_Aue_Pang conj_and_Aue_Pang conj_and_Aue_2005 conj_and_Aue_Gamon conj_or_text_levels dep_text_Turney dep_text_Turney dep_text_Riloff dep_text_Pang dep_text_Pang dep_text_2005 dep_text_Gamon dep_text_Aue det_text_the dep_conducted_Kim dep_conducted_Hu dep_conducted_2005 dep_conducted_Aue dep_conducted_Gamon prep_at_conducted_levels prep_at_conducted_text advmod_conducted_usually auxpass_conducted_is nsubjpass_conducted_Level nn_annotation_sentiment nn_Research_Analysis prep_on_Level_annotation prep_of_Level_Research num_Level_3.1 num_Level_291
P08-1036	W02-1011	o	Sentiment classification is a well studied problem -LRB- Wiebe 2000 Pang et al. 2002 Turney 2002 -RRB- and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay -LRB- 2007 -RRB-	appos_Barzilay_2007 conj_and_Snyder_Barzilay pobj_in_Barzilay pobj_in_Snyder pcomp_as_in det_user_a prep_rated_as agent_rated_user auxpass_rated_be aux_rated_can nsubjpass_rated_that rcmod_object_rated det_object_an prep_of_properties_object dobj_denote_properties aux_denote_to nn_aspect_term det_aspect_the vmod_use_denote dobj_use_aspect ccomp_1We_use advmod_1We_explicitly nsubj_1We_in nsubj_1We_Pang nn_users_domains amod_users_many pobj_in_users dep_Turney_2002 conj_and_Pang_in dep_Pang_Turney num_Pang_2002 nn_Pang_al. nn_Pang_et parataxis_Wiebe_1We appos_Wiebe_2000 dep_problem_Wiebe amod_problem_studied det_problem_a cop_problem_is nsubj_problem_classification advmod_studied_well nn_classification_Sentiment
P08-2034	W02-1011	p	Three approaches are dominating i.e. knowledge-based approach -LRB- Kim and Hovy 2004 -RRB- information retrieval-based approach -LRB- Turney and Littman 2003 -RRB- and machine learning approach -LRB- Pang et al. 2002 -RRB- in which the last approach is found very popular	advmod_popular_very advmod_found_popular auxpass_found_is nsubjpass_found_approach prep_in_found_which amod_approach_last det_approach_the amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_approach_learning nn_approach_machine dep_Turney_2003 conj_and_Turney_Littman dep_approach_Littman dep_approach_Turney amod_approach_retrieval-based nn_approach_information dep_Kim_2004 conj_and_Kim_Hovy rcmod_approach_found dep_approach_Pang conj_and_approach_approach conj_and_approach_approach dep_approach_Hovy dep_approach_Kim amod_approach_knowledge-based advmod_approach_i.e. dobj_dominating_approach dobj_dominating_approach dobj_dominating_approach aux_dominating_are nsubj_dominating_approaches num_approaches_Three
P08-2065	W02-1011	o	1 Introduction Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of or sentiment toward a given subject -LRB- e.g. if an opinion is supported or not -RRB- -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et cc_not_or neg_supported_not auxpass_supported_is nsubjpass_supported_opinion mark_supported_if det_opinion_an advcl_e.g._supported dep_subject_e.g. amod_subject_given det_subject_a prep_toward_sentiment_subject conj_or_of_sentiment prep_opinion_sentiment prep_opinion_of poss_opinion_their pobj_classify_opinion prepc_according_to_classify_to dobj_classify_documents aux_classify_to xcomp_aims_classify nsubj_aims_that nn_categorization_text dep_task_Pang rcmod_task_aims prep_of_task_categorization amod_task_special det_task_a cop_task_is nsubj_task_classification nn_classification_Sentiment nn_classification_Introduction num_classification_1
P08-2065	W02-1011	o	Experiment Implementation We apply SVM algorithm to construct our classifiers which has been shown to perform better than many other classification algorithms -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et appos_algorithms_Pang nn_algorithms_classification amod_algorithms_other amod_algorithms_many prep_than_better_algorithms dobj_perform_better aux_perform_to xcomp_shown_perform auxpass_shown_been aux_shown_has nsubjpass_shown_which rcmod_classifiers_shown poss_classifiers_our dobj_construct_classifiers aux_construct_to nn_algorithm_SVM vmod_apply_construct dobj_apply_algorithm nsubj_apply_We nsubj_apply_Implementation nn_Implementation_Experiment
P09-1027	W02-1011	o	-LRB- 2002 -RRB- various classification models and linguistic features have been proposed to improve the classification performance -LRB- Pang and Lee 2004 Mullen and Collier 2004 Wilson et al. 2005 Read 2005 -RRB-	dep_Read_2005 num_Wilson_2005 nn_Wilson_al. nn_Wilson_et dep_Mullen_Read conj_and_Mullen_Wilson conj_and_Mullen_2004 conj_and_Mullen_Collier conj_and_Pang_Wilson conj_and_Pang_2004 conj_and_Pang_Collier conj_and_Pang_Mullen conj_and_Pang_2004 conj_and_Pang_Lee dep_performance_Mullen dep_performance_2004 dep_performance_Lee dep_performance_Pang nn_performance_classification det_performance_the dobj_improve_performance aux_improve_to xcomp_proposed_improve auxpass_proposed_been aux_proposed_have nsubjpass_proposed_features nsubjpass_proposed_models amod_features_linguistic conj_and_models_features nn_models_classification amod_models_various dep_models_2002
P09-1028	W02-1011	p	Movies Reviews This is a popular dataset in sentiment analysis literature -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_literature_analysis nn_literature_sentiment dep_dataset_Pang prep_in_dataset_literature amod_dataset_popular det_dataset_a cop_dataset_is nsubj_dataset_This dep_Reviews_dataset nn_Reviews_Movies
P09-1028	W02-1011	p	In particular the use of SVMs in -LRB- Pang et al. 2002 -RRB- initially sparked interest in using machine learning methods for sentiment classi cation	nn_cation_classi nn_cation_sentiment nn_methods_learning nn_methods_machine prep_for_using_cation dobj_using_methods prepc_in_sparked_using dobj_sparked_interest advmod_sparked_initially nsubj_sparked_use prep_in_sparked_particular amod_Pang_2002 dep_Pang_al. nn_Pang_et prep_in_use_Pang prep_of_use_SVMs det_use_the
P09-1028	W02-1011	p	In their seminal work -LRB- Pang et al. 2002 -RRB- demonstrated that supervised learning signi cantly outperformed a competing body of work where hand-crafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms	amod_terms_negative amod_terms_positive conj_and_positive_negative prep_of_frequencies_terms amod_frequencies_relative nn_labels_sentiment pobj_assign_frequencies prepc_based_on_assign_on dobj_assign_labels aux_assign_to xcomp_used_assign auxpass_used_are nsubjpass_used_dictionaries advmod_used_where amod_dictionaries_hand-crafted rcmod_work_used prep_of_body_work amod_body_competing det_body_a dobj_outperformed_body advmod_outperformed_cantly nsubj_outperformed_signi nn_signi_learning ccomp_supervised_outperformed dep_that_supervised prep_demonstrated_that nsubj_demonstrated_Pang prep_in_demonstrated_work nn_al._et amod_Pang_2002 dep_Pang_al. amod_work_seminal poss_work_their
P09-1028	W02-1011	p	Most semi-automated approaches have met with limited success -LRB- Ng et al. 2006 -RRB- and supervised learning models have tended to outperform dictionary-based classi cation schemes -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_schemes_cation nn_schemes_classi amod_schemes_dictionary-based dobj_outperform_schemes aux_outperform_to dep_tended_Pang xcomp_tended_outperform aux_tended_have nn_models_learning amod_models_supervised amod_Ng_2006 dep_Ng_al. nn_Ng_et conj_and_success_models dep_success_Ng amod_success_limited dep_met_tended prep_with_met_models prep_with_met_success aux_met_have nsubj_met_approaches amod_approaches_semi-automated amod_approaches_Most
P09-1028	W02-1011	o	Most work in machine learning literature on utilizing labeled features has focused on using them to generate weakly labeled examples that are then used for standard supervised learning -LRB- Schapire et al. 2002 -RRB- propose one such framework for boosting logistic regression -LRB- Wu and Srihari 2004 -RRB- build a modi ed SVM and -LRB- Liu et al. 2004 -RRB- use a combination of clustering and EM based methods to instantiate similar frameworks	amod_frameworks_similar dobj_instantiate_frameworks aux_instantiate_to amod_methods_based nn_methods_EM nn_methods_clustering conj_and_clustering_EM prep_of_combination_methods det_combination_a vmod_use_instantiate dobj_use_combination nsubj_use_Liu nsubj_use_SVM amod_Liu_2004 dep_Liu_al. nn_Liu_et conj_and_SVM_Liu amod_SVM_ed rcmod_modi_use det_modi_a dobj_build_modi nsubj_build_Srihari nsubj_build_Wu dep_Wu_2004 conj_and_Wu_Srihari amod_regression_logistic dobj_boosting_regression prepc_for_framework_boosting amod_framework_such num_framework_one parataxis_propose_build dobj_propose_framework dep_propose_al. dep_al._2002 nn_al._et nn_al._Schapire amod_learning_supervised amod_learning_standard prep_for_used_learning advmod_used_then auxpass_used_are nsubjpass_used_that rcmod_examples_used amod_examples_labeled advmod_labeled_weakly dobj_generate_examples aux_generate_to vmod_using_generate dobj_using_them parataxis_focused_propose prepc_on_focused_using aux_focused_has nsubj_focused_work amod_features_labeled dobj_utilizing_features nn_literature_learning nn_literature_machine prepc_on_work_utilizing prep_in_work_literature amod_work_Most
P09-1029	W02-1011	o	Specifically we explore the statistical term weighting features of the word generation model with Support Vector machine -LRB- SVM -RRB- faithfully reproducing previous work as closely as possible -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_possible_Pang pobj_as_possible advmod_as_closely pcomp_as_as amod_work_previous prep_reproducing_as dobj_reproducing_work advmod_reproducing_faithfully appos_machine_SVM nn_machine_Vector nn_machine_Support prep_with_model_machine nn_model_generation nn_model_word det_model_the prep_of_features_model dep_weighting_features nn_weighting_term amod_weighting_statistical det_weighting_the vmod_explore_reproducing dobj_explore_weighting nsubj_explore_we advmod_explore_Specifically
P09-1029	W02-1011	o	Movie-review dataset consists of positive and negative reviews from the Internet Movie Database -LRB- IMDb -RRB- archive -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_archive_Pang nn_archive_IMDb dep_Database_archive nn_Database_Movie nn_Database_Internet det_Database_the prep_from_reviews_Database amod_reviews_negative amod_reviews_positive conj_and_positive_negative prep_of_consists_reviews nsubj_consists_dataset nn_dataset_Movie-review
P09-1029	W02-1011	o	To closely reproduce the experiment with the best performance carried out in -LRB- Pang et al. 2002 -RRB- using SVM we use unigram with the presence feature	nn_feature_presence det_feature_the prep_with_use_feature dobj_use_unigram nsubj_use_we dobj_using_SVM amod_Pang_2002 dep_Pang_al. nn_Pang_et parataxis_carried_use xcomp_carried_using prep_in_carried_Pang prt_carried_out csubj_carried_reproduce amod_performance_best det_performance_the prep_with_experiment_performance det_experiment_the dobj_reproduce_experiment advmod_reproduce_closely aux_reproduce_To
P09-1078	W02-1011	o	Among these methods SVM is shown to perform better than other methods -LRB- Yang and Pedersen 1997 Pang et al. 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 2002 -RRB-	num_http://www.seas.upenn.edu/~mdredze/datasets/sentiment/_2002 num_http://www.seas.upenn.edu/~mdredze/datasets/sentiment/_3 dep_http://www.cs.cornell.edu/People/pabo/movie-review-data/_http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ num_http://www.cs.cornell.edu/People/pabo/movie-review-data/_2 dep_http://people.csail.mit.edu/~jrennie/20Newsgroups/_http://www.cs.cornell.edu/People/pabo/movie-review-data/ dep_1_http://people.csail.mit.edu/~jrennie/20Newsgroups/ appos_Pang_1 dep_Pang_al. nn_Pang_et dep_Yang_Pang conj_and_Yang_1997 conj_and_Yang_Pedersen dep_methods_1997 dep_methods_Pedersen dep_methods_Yang amod_methods_other prep_than_better_methods dobj_perform_better aux_perform_to xcomp_shown_perform auxpass_shown_is nsubjpass_shown_SVM prep_among_shown_methods det_methods_these
P09-1078	W02-1011	o	Recently sentiment classification has become popular because of its wide applications -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_applications_Pang amod_applications_wide poss_applications_its prep_because_of_become_applications acomp_become_popular aux_become_has nsubj_become_classification advmod_become_Recently nn_classification_sentiment
P09-1079	W02-1011	p	4 Evaluation 4.1 Experimental Setup For evaluation we use five sentiment classification datasets including the widely-used movie review dataset -LSB- MOV -RSB- -LRB- Pang et al. 2002 -RRB- as well as four datasets that contain reviews of four different types of product from Amazon -LSB- books -LRB- BOO -RRB- DVDs -LRB- DVD -RRB- electronics -LRB- ELE -RRB- and kitchen appliances -LRB- KIT -RRB- -RSB- -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et appos_appliances_KIT nn_appliances_kitchen appos_electronics_ELE appos_DVDs_DVD conj_and_books_appliances conj_and_books_electronics conj_and_books_DVDs appos_books_BOO nn_books_Amazon prep_from_product_appliances prep_from_product_electronics prep_from_product_DVDs prep_from_product_books prep_of_types_product amod_types_different num_types_four prep_of_reviews_types dobj_contain_reviews nsubj_contain_that rcmod_datasets_contain num_datasets_four amod_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_dataset_datasets dep_dataset_Pang appos_dataset_MOV nn_dataset_review nn_dataset_movie amod_dataset_widely-used det_dataset_the dep_datasets_Blitzer prep_including_datasets_datasets prep_including_datasets_dataset nn_datasets_classification nn_datasets_sentiment num_datasets_five dobj_use_datasets nsubj_use_we nsubj_use_Setup prep_for_Setup_evaluation amod_Setup_Experimental num_Setup_4.1 nn_Setup_Evaluation num_Setup_4
P09-1095	W02-1011	o	While the NASA researchers have applied a heuristic method for labeling a report with shapers -LRB- Posse 1http / / kdd.ics.uci.edu/databases/20newsgroups/ 2Of course the fact that sentiment classification requires a deeper understanding of a text also makes it more difficult than topic-based text classification -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_classification_text amod_classification_topic-based dep_difficult_Pang prep_than_difficult_classification advmod_difficult_more dep_it_difficult dobj_makes_it advmod_makes_also nsubj_makes_1http dep_makes_shapers mark_makes_with det_text_a prep_of_understanding_text amod_understanding_deeper det_understanding_a dobj_requires_understanding nsubj_requires_classification mark_requires_that nn_classification_sentiment ccomp_fact_requires det_fact_the nn_course_2Of nn_course_kdd.ics.uci.edu/databases/20newsgroups/ appos_1http_fact dep_1http_course nn_1http_Posse vmod_report_makes det_report_a dobj_labeling_report prepc_for_method_labeling nn_method_heuristic det_method_a dobj_applied_method aux_applied_have nsubj_applied_researchers mark_applied_While nn_researchers_NASA det_researchers_the advcl_``_applied
P09-2043	W02-1011	o	1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews product reviews news and blog reviews -LRB- Pang et al. 2002 Turney 2002 -RRB-	amod_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_reviews_blog dep_reviews_Pang conj_and_reviews_reviews conj_and_reviews_news nn_reviews_product nn_reviews_movie prep_such_as_domains_reviews amod_domains_several dep_conducted_reviews dep_conducted_news dep_conducted_reviews prep_in_conducted_domains advmod_conducted_widely auxpass_conducted_been aux_conducted_have nsubjpass_conducted_analysis nn_analysis_Sentiment nn_analysis_Introduction num_analysis_1 ccomp_``_conducted
P09-2080	W02-1011	o	In most cases supervised learning methods can perform well -LRB- Pang et al. 2002 -RRB-	amod_Pang_2002 dep_Pang_al. nn_Pang_et dep_perform_Pang advmod_perform_well aux_perform_can csubj_perform_supervised prep_in_perform_cases dobj_learning_methods xcomp_supervised_learning amod_cases_most
W03-0404	W02-1011	o	Some work identifies inflammatory texts -LRB- e.g. -LRB- Spertus 1997 -RRB- -RRB- or classifies reviews as positive or negative -LRB- -LRB- Turney 2002 Pang et al. 2002 -RRB- -RRB-	num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang conj_Turney_2002 dep_positive_Turney conj_or_positive_negative prep_as_classifies_negative prep_as_classifies_positive dobj_classifies_reviews dep_Spertus_1997 appos_e.g._Spertus conj_or_texts_classifies dep_texts_e.g. amod_texts_inflammatory dobj_identifies_classifies dobj_identifies_texts nsubj_identifies_work det_work_Some ccomp_``_identifies
W03-1014	W02-1011	o	For example -LRB- Spertus 1997 -RRB- developed a system to identify inflammatory texts and -LRB- Turney 2002 Pang et al. 2002 -RRB- developed methods for classifying reviews as positive or negative	conj_or_positive_negative prep_as_reviews_negative prep_as_reviews_positive amod_reviews_classifying prep_for_methods_reviews dobj_developed_methods dep_developed_Turney nsubj_developed_Spertus num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang appos_Turney_2002 amod_texts_inflammatory dobj_identify_texts aux_identify_to vmod_system_identify det_system_a conj_and_developed_developed dobj_developed_system nsubj_developed_Spertus prep_for_developed_example amod_Spertus_1997
W03-1014	W02-1011	o	For example -LRB- Pang et al. 2002 -RRB- collected reviews from a movie database and rated them as positive negative or neutral based on the rating -LRB- e.g. number of stars -RRB- given by the reviewer	det_reviewer_the agent_given_reviewer prep_of_number_stars appos_e.g._number vmod_rating_given dep_rating_e.g. det_rating_the pobj_positive_rating prepc_based_on_positive_on conj_or_positive_neutral conj_or_positive_negative prep_as_rated_neutral prep_as_rated_negative prep_as_rated_positive dobj_rated_them nn_database_movie det_database_a prep_from_reviews_database conj_and_collected_rated dobj_collected_reviews dep_collected_Pang prep_for_collected_example nn_al._et dep_Pang_2002 dep_Pang_al.
W03-1017	W02-1011	o	Our focus is on the sentence level unlike -LRB- Pang et al. 2002 -RRB- and -LRB- Turney 2002 -RRB- we employ a significantly larger set of seed words and we explore as indicators of orientation words from syntactic classes other than adjectives -LRB- nouns verbs and adverbs -RRB-	conj_and_nouns_adverbs conj_and_nouns_verbs dep_adjectives_adverbs dep_adjectives_verbs dep_adjectives_nouns prep_than_other_adjectives amod_classes_other amod_classes_syntactic prep_from_words_classes nn_words_orientation prep_of_indicators_words prep_as_explore_indicators nsubj_explore_we nn_words_seed prep_of_set_words amod_set_larger det_set_a advmod_larger_significantly conj_and_employ_explore dobj_employ_set nsubj_employ_we amod_Turney_2002 amod_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_unlike_Turney dep_unlike_Pang nn_level_sentence det_level_the parataxis_is_explore parataxis_is_employ dep_is_Turney dep_is_unlike prep_on_is_level nsubj_is_focus poss_focus_Our ccomp_``_is
W04-0509	W02-1011	o	Gildea and Jurafsky -LRB- 2002 -RRB- used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme target goal and the boundaries of the roles -LRB- Baker et al. 2003 -RRB-	amod_Baker_2003 dep_Baker_al. nn_Baker_et det_roles_the prep_of_boundaries_roles det_boundaries_the conj_and_theme_boundaries conj_and_theme_goal conj_and_theme_target prep_such_as_FrameNet_boundaries prep_such_as_FrameNet_goal prep_such_as_FrameNet_target prep_such_as_FrameNet_theme prep_in_defined_FrameNet vmod_roles_defined amod_roles_semantic det_roles_the appos_identifier_Baker prep_of_identifier_roles det_identifier_the preconj_identifier_both dobj_learn_identifier aux_learn_to vmod_method_learn nn_method_learning amod_method_supervised det_method_a dobj_used_method nsubj_used_Jurafsky nsubj_used_Gildea appos_Jurafsky_2002 conj_and_Gildea_Jurafsky
W05-0408	W02-1011	o	1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years -LRB- Pang and Lee 2002 Pang et al. 2004 Turney 2002 Turney and Littman 2002 Wiebe et al. 2001 Bai et al. 2004 Yu and Hatzivassiloglou 2003 and many others -RRB-	amod_others_many num_Hatzivassiloglou_2003 num_al._2004 dep_Bai_al. nn_Bai_et num_al._2001 dep_Wiebe_al. nn_Wiebe_et num_Littman_2002 num_Turney_2002 num_al._2004 dep_Pang_al. nn_Pang_et num_Lee_2002 conj_and_Pang_others conj_and_Pang_Hatzivassiloglou conj_and_Pang_Yu conj_and_Pang_Bai conj_and_Pang_Wiebe conj_and_Pang_Littman conj_and_Pang_Turney appos_Pang_Turney appos_Pang_Pang conj_and_Pang_Lee dep_years_others dep_years_Hatzivassiloglou dep_years_Yu dep_years_Bai dep_years_Wiebe dep_years_Littman dep_years_Turney dep_years_Lee dep_years_Pang amod_years_recent amod_attention_considerable prep_in_received_years prep_from_received_researchers dobj_received_attention aux_received_has nsubj_received_field nn_classification_sentiment prep_of_field_classification det_field_The rcmod_Introduction_received num_Introduction_1
W05-0408	W02-1011	o	Movie and product reviews have been the main focus of many of the recent studies in this area -LRB- Pang and Lee 2002 Pang et al. 2004 Turney 2002 Turney and Littman 2002 -RRB-	num_Littman_2002 num_Turney_2002 num_al._2004 dep_Pang_al. nn_Pang_et num_Lee_2002 conj_and_Pang_Littman conj_and_Pang_Turney conj_and_Pang_Turney conj_and_Pang_Pang conj_and_Pang_Lee det_area_this dep_studies_Littman dep_studies_Turney dep_studies_Turney dep_studies_Pang dep_studies_Lee dep_studies_Pang prep_in_studies_area amod_studies_recent det_studies_the prep_of_many_studies prep_of_focus_many amod_focus_main det_focus_the cop_focus_been aux_focus_have nsubj_focus_reviews nn_reviews_product nn_reviews_Movie conj_and_Movie_product
W06-0301	W02-1011	o	Identifying subjectivity helps separate opinions from fact which may be useful in question answering summarization etc. Sentiment detection is the task of determining positive or negative sentiment of words -LRB- Hatzivassiloglou and McKeown 1997 Turney 2002 Esuli and Sebastiani 2005 -RRB- phrases and sentences -LRB- Kim and Hovy 2004 Wilson et al. 2005 -RRB- or documents -LRB- Pang et al. 2002 Turney 2002 -RRB-	amod_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et appos_documents_Pang num_Wilson_2005 nn_Wilson_al. nn_Wilson_et dep_Kim_Wilson num_Kim_2004 conj_and_Kim_Hovy dep_Esuli_2005 conj_and_Esuli_Sebastiani num_Turney_2002 dep_Hatzivassiloglou_Sebastiani dep_Hatzivassiloglou_Esuli conj_and_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown conj_and_words_sentences conj_and_words_phrases appos_words_Turney appos_words_1997 appos_words_McKeown appos_words_Hatzivassiloglou prep_of_sentiment_sentences prep_of_sentiment_phrases prep_of_sentiment_words amod_sentiment_negative amod_sentiment_positive conj_or_positive_negative dobj_determining_sentiment conj_or_task_documents dep_task_Hovy dep_task_Kim prepc_of_task_determining det_task_the cop_task_is nsubj_task_detection nn_detection_Sentiment nn_etc._summarization dep_answering_etc. nn_answering_question prep_in_useful_answering cop_useful_be aux_useful_may nsubj_useful_which amod_opinions_separate parataxis_helps_documents parataxis_helps_task dep_helps_useful prep_from_helps_fact dobj_helps_opinions nsubj_helps_subjectivity amod_subjectivity_Identifying
W06-0305	W02-1011	o	Most of the annotation approaches tackling these issues however are aimed at performing classifications at either the document level -LRB- Pang et al. 2002 Turney 2002 -RRB- or the sentence or word level -LRB- Wiebe et al. 2004 Yu and Hatzivassiloglou 2003 -RRB-	dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Wiebe_Hatzivassiloglou dep_Wiebe_Yu amod_Wiebe_2004 dep_Wiebe_al. nn_Wiebe_et nn_level_word det_sentence_the dep_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et conj_or_level_level conj_or_level_sentence appos_level_Pang nn_level_document det_level_the preconj_level_either prep_at_performing_level prep_at_performing_sentence prep_at_performing_level dobj_performing_classifications dep_aimed_Wiebe prepc_at_aimed_performing auxpass_aimed_are advmod_aimed_however advcl_aimed_approaches det_issues_these dobj_tackling_issues xcomp_approaches_tackling nsubj_approaches_Most det_annotation_the prep_of_Most_annotation
W06-0306	W02-1011	o	In analyzing opinions -LRB- Cardie et al. 2003 Wilson et al. 2004 -RRB- judging document-level subjectivity -LRB- Pang et al. 2002 Turney 2002 -RRB- and answering opinion questions -LRB- Cardie et al. 2003 Yu and Hatzivassiloglou 2003 -RRB- the output of a sentence-level subjectivity classification can be used without modification	prep_without_used_modification auxpass_used_be aux_used_can nsubjpass_used_output vmod_used_answering vmod_used_judging dep_used_Cardie prepc_in_used_analyzing nn_classification_subjectivity amod_classification_sentence-level det_classification_a prep_of_output_classification det_output_the dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Cardie_Hatzivassiloglou dep_Cardie_Yu appos_Cardie_2003 dep_Cardie_al. nn_Cardie_et appos_questions_Cardie nn_questions_opinion dobj_answering_questions dep_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et appos_subjectivity_Pang amod_subjectivity_document-level conj_and_judging_answering dobj_judging_subjectivity num_Wilson_2004 nn_Wilson_al. nn_Wilson_et nn_al._et dep_Cardie_Wilson dep_Cardie_2003 dep_Cardie_al. dobj_analyzing_opinions
W06-1639	W02-1011	o	In particular since we treat each individual speech within a debate as a single document we are considering a version of document-level sentiment-polarity classification namely automatically distinguishing between positive and negative documents -LRB- Das and Chen 2001 Pang et al. 2002 Turney 2002 Dave et al. 2003 -RRB-	num_Dave_2003 nn_Dave_al. nn_Dave_et num_Turney_2002 num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Das_Dave conj_and_Das_Turney conj_and_Das_Pang conj_and_Das_2001 conj_and_Das_Chen dep_documents_Turney dep_documents_Pang dep_documents_2001 dep_documents_Chen dep_documents_Das amod_documents_negative amod_documents_positive conj_and_positive_negative prep_between_distinguishing_documents advmod_distinguishing_automatically nn_classification_sentiment-polarity amod_classification_document-level prep_of_version_classification det_version_a vmod_considering_distinguishing advmod_considering_namely dobj_considering_version aux_considering_are nsubj_considering_we advcl_considering_treat prep_in_considering_particular amod_document_single det_document_a det_debate_a amod_speech_individual det_speech_each prep_as_treat_document prep_within_treat_debate dobj_treat_speech nsubj_treat_we mark_treat_since ccomp_``_considering
W06-1639	W02-1011	o	3.2 Classifying speech segments in isolation In our experiments we employed the well-known classifier SVMlight to obtain individual-document classification scores treating Y as the positive class and using plain unigrams as features .5 Following standard practice in sentiment analysis -LRB- Pang et al. 2002 -RRB- the input to SVMlight consisted of normalized presence-of-feature -LRB- rather than frequency-of-feature -RRB- vectors	advmod_frequency-of-feature_rather mwe_rather_than dep_presence-of-feature_vectors dep_presence-of-feature_frequency-of-feature amod_presence-of-feature_normalized prep_of_consisted_presence-of-feature nsubj_consisted_input prep_to_input_SVMlight det_input_the amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_analysis_sentiment prep_in_practice_analysis amod_practice_standard amod_practice_Following num_practice_.5 dep_features_practice amod_unigrams_plain prep_as_using_features dobj_using_unigrams amod_class_positive det_class_the ccomp_treating_consisted dep_treating_Pang conj_and_treating_using prep_as_treating_class dobj_treating_Y nn_scores_classification amod_scores_individual-document dobj_obtain_scores aux_obtain_to nn_SVMlight_classifier amod_SVMlight_well-known det_SVMlight_the xcomp_employed_using xcomp_employed_treating vmod_employed_obtain dobj_employed_SVMlight nsubj_employed_we nsubj_employed_segments poss_experiments_our prep_in_segments_experiments prep_in_segments_isolation nn_segments_speech amod_segments_Classifying num_segments_3.2
W06-1641	W02-1011	o	This negation handling is similar to that used in -LRB- Das and Chen 2001 Pang et al. 2002 -RRB-	num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Das_Pang conj_and_Das_2001 conj_and_Das_Chen prep_in_used_2001 prep_in_used_Chen prep_in_used_Das vmod_that_used prep_to_similar_that cop_similar_is nsubj_similar_handling nn_handling_negation det_handling_This
W06-1641	W02-1011	n	A number of studies have investigated sentiment classification at document level e.g. -LRB- Pang et al. 2002 Dave et al. 2003 -RRB- and at sentence level e.g. -LRB- Hu and Liu 2004 Kim and Hovy 2004 Nigam and Hurst 2005 -RRB- however the accuracy is still less than desirable	prep_than_less_desirable advmod_less_still cop_less_is nsubj_less_accuracy advmod_less_however det_accuracy_the amod_Nigam_2005 conj_and_Nigam_Hurst num_Kim_2004 conj_and_Kim_Hovy dep_Hu_less dep_Hu_Hurst dep_Hu_Nigam dep_Hu_Hovy dep_Hu_Kim amod_Hu_2004 conj_and_Hu_Liu dep_,_Liu dep_,_Hu dep_,_e.g. nn_level_sentence pobj_at_level num_Dave_2003 nn_Dave_al. nn_Dave_et conj_and_Pang_at dep_Pang_Dave appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_level_document nn_classification_sentiment dep_investigated_at dep_investigated_Pang dep_investigated_e.g. prep_at_investigated_level dobj_investigated_classification aux_investigated_have nsubj_investigated_number prep_of_number_studies det_number_A ccomp_``_investigated
W06-1652	W02-1011	o	Lexical cues of differing complexities have been used including single words and Ngrams -LRB- e.g. -LRB- Mullen and Collier 2004 Pang et al. 2002 Turney 2002 Yu and Hatzivassiloglou 2003 Wiebe et al. 2004 -RRB- -RRB- as well as phrases and lexico-syntactic patterns -LRB- e.g -LRB- Kim and Hovy 2004 Hu and Liu 2004 Popescu and Etzioni 2005 Riloff and Wiebe 2003 Whitelaw et al. 2005 -RRB- -RRB-	num_Whitelaw_2005 nn_Whitelaw_al. nn_Whitelaw_et dep_Popescu_Whitelaw conj_and_Popescu_2003 conj_and_Popescu_Wiebe conj_and_Popescu_Riloff conj_and_Popescu_2005 conj_and_Popescu_Etzioni num_Hu_2004 conj_and_Hu_Liu dep_Kim_2003 dep_Kim_Wiebe dep_Kim_Riloff dep_Kim_2005 dep_Kim_Etzioni dep_Kim_Popescu conj_and_Kim_Liu conj_and_Kim_Hu conj_and_Kim_2004 conj_and_Kim_Hovy appos_e.g_Hu appos_e.g_2004 appos_e.g_Hovy appos_e.g_Kim dep_patterns_e.g amod_patterns_lexico-syntactic conj_and_phrases_patterns num_Wiebe_2004 nn_Wiebe_al. nn_Wiebe_et dep_Yu_Wiebe conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou num_Turney_2002 num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Mullen_2003 dep_Mullen_Hatzivassiloglou dep_Mullen_Yu conj_and_Mullen_Turney conj_and_Mullen_Pang conj_and_Mullen_2004 conj_and_Mullen_Collier conj_and_e.g._patterns conj_and_e.g._phrases appos_e.g._Turney appos_e.g._Pang appos_e.g._2004 appos_e.g._Collier appos_e.g._Mullen dep_Ngrams_phrases dep_Ngrams_e.g. conj_and_words_Ngrams amod_words_single prep_including_used_Ngrams prep_including_used_words auxpass_used_been aux_used_have nsubjpass_used_cues amod_complexities_differing prep_of_cues_complexities amod_cues_Lexical
W06-2915	W02-1011	o	So far research in automatic opinion recognition has primarily addressed learning subjective language -LRB- Wiebe et al. 2004 Riloff et al. 2003 -RRB- identifying opinionated documents -LRB- Yu and Hatzivassiloglou 2003 -RRB- and sentences -LRB- Yu and Hatzivassiloglou 2003 Riloff et al. 2003 -RRB- and discriminating between positive and negative language -LRB- Pang et al. 2002 Morinaga et al. 2002 Yu and Hatzivassiloglou 2003 Turney and Littman 2003 Dave et al. 2003 Nasukawa and Yi 2003 Popescu and Etzioni 2005 Wilson et al. 2005 -RRB-	num_Wilson_2005 nn_Wilson_al. nn_Wilson_et num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Yu_Wilson conj_and_Yu_2005 conj_and_Yu_Etzioni conj_and_Yu_Popescu conj_and_Yu_2003 conj_and_Yu_Yi conj_and_Yu_Nasukawa conj_and_Yu_Dave conj_and_Yu_2003 conj_and_Yu_Littman conj_and_Yu_Turney conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Morinaga_2005 dep_Morinaga_Etzioni dep_Morinaga_Popescu dep_Morinaga_2003 dep_Morinaga_Yi dep_Morinaga_Nasukawa dep_Morinaga_Dave dep_Morinaga_2003 dep_Morinaga_Littman dep_Morinaga_Turney dep_Morinaga_2003 dep_Morinaga_Hatzivassiloglou dep_Morinaga_Yu num_Morinaga_2002 nn_Morinaga_al. nn_Morinaga_et dep_Pang_Morinaga appos_Pang_2002 dep_Pang_al. nn_Pang_et dep_language_Pang amod_language_negative amod_language_positive conj_and_positive_negative prep_between_discriminating_language nsubj_discriminating_research num_Riloff_2003 nn_Riloff_al. nn_Riloff_et conj_and_Yu_Riloff conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_sentences_Riloff dep_sentences_2003 dep_sentences_Hatzivassiloglou dep_sentences_Yu conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou conj_and_documents_sentences dep_documents_2003 dep_documents_Hatzivassiloglou dep_documents_Yu amod_documents_opinionated dobj_identifying_sentences dobj_identifying_documents num_Riloff_2003 nn_Riloff_al. nn_Riloff_et dep_Wiebe_Riloff appos_Wiebe_2004 dep_Wiebe_al. nn_Wiebe_et amod_language_subjective dobj_learning_language conj_and_addressed_discriminating dep_addressed_identifying dep_addressed_Wiebe xcomp_addressed_learning advmod_addressed_primarily aux_addressed_has nsubj_addressed_research advmod_addressed_far nn_recognition_opinion amod_recognition_automatic prep_in_research_recognition advmod_far_So
W06-2915	W02-1011	o	Research on the automatic classification of movie or product reviews as positive or negative -LRB- e.g. -LRB- Pang et al. 2002 Morinaga et al. 2002 Turney and Littman 2003 Nasukawa and Yi 2003 Mullen and Collier 2004 Beineke et al. 2004 Hu and Liu 2004 -RRB- -RRB- is perhaps the most similar to our work	poss_work_our prep_to_similar_work advmod_similar_most det_similar_the advmod_similar_perhaps cop_similar_is nsubj_similar_e.g. dep_similar_Research amod_Hu_2004 conj_and_Hu_Liu num_Beineke_2004 nn_Beineke_al. nn_Beineke_et dep_Turney_Liu dep_Turney_Hu conj_and_Turney_Beineke conj_and_Turney_2004 conj_and_Turney_Collier conj_and_Turney_Mullen conj_and_Turney_2003 conj_and_Turney_Yi conj_and_Turney_Nasukawa conj_and_Turney_2003 conj_and_Turney_Littman dep_Morinaga_Beineke dep_Morinaga_2004 dep_Morinaga_Collier dep_Morinaga_Mullen dep_Morinaga_2003 dep_Morinaga_Yi dep_Morinaga_Nasukawa dep_Morinaga_2003 dep_Morinaga_Littman dep_Morinaga_Turney num_Morinaga_2002 nn_Morinaga_al. nn_Morinaga_et dep_Pang_Morinaga num_Pang_2002 dep_Pang_al. nn_Pang_et appos_e.g._Pang conj_or_positive_negative nn_reviews_product nn_reviews_movie conj_or_movie_product prep_of_classification_reviews amod_classification_automatic det_classification_the prep_as_Research_negative prep_as_Research_positive prep_on_Research_classification ccomp_``_similar
W07-1515	W02-1011	o	Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment -LRB- Pang et al. 2002 -RRB- andsubjectivityanalysis -LRB- Wiebeetal	dep_andsubjectivityanalysis_Wiebeetal appos_andsubjectivityanalysis_Pang amod_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_fields_sentiment det_fields_the prep_in_research_fields amod_research_existing dep_augment_andsubjectivityanalysis dobj_augment_research aux_augment_will nsubj_augment_analysis prep_of_aspects_language det_aspects_these prep_of_analysis_aspects amod_analysis_automatic amod_analysis_Accurate ccomp_``_augment
C08-1122	W03-1805	o	Tomokiyo and Hurst -LRB- 2003 -RRB- use pointwise KLdivergence between multiple language models for scoring both phraseness and informativeness of phrases	prep_of_phraseness_phrases conj_and_phraseness_informativeness preconj_phraseness_both dobj_scoring_informativeness dobj_scoring_phraseness nn_models_language amod_models_multiple amod_KLdivergence_pointwise prepc_for_use_scoring prep_between_use_models dobj_use_KLdivergence nsubj_use_Hurst nsubj_use_Tomokiyo appos_Hurst_2003 conj_and_Tomokiyo_Hurst
C08-1019	W04-1013	o	Responsiveness differs from other measures of summary content such as SEE coverage -LRB- Lin and Hovy 2002 -RRB- and Pyramid scores -LRB- Nenkova and Passonneau 2004 -RRB- in that it does not compare a peer summary against a set of known human summaries	amod_summaries_human amod_summaries_known prep_of_set_summaries det_set_a prep_against_summary_set amod_summary_peer det_summary_a dobj_compare_summary neg_compare_not aux_compare_does nsubj_compare_it mark_compare_that dep_Nenkova_2004 conj_and_Nenkova_Passonneau prepc_in_scores_compare appos_scores_Passonneau appos_scores_Nenkova nn_scores_Pyramid nn_scores_coverage dep_Lin_2002 conj_and_Lin_Hovy conj_and_coverage_Pyramid dep_coverage_Hovy dep_coverage_Lin dobj_SEE_scores prepc_such_as_content_SEE nn_content_summary prep_of_measures_content amod_measures_other prep_from_differs_measures nsubj_differs_Responsiveness
C08-1019	W04-1013	o	1 Introduction ROUGE -LRB- Lin 2004 -RRB- and its linguisticallymotivated descendent Basic Elements -LRB- BE -RRB- -LRB- Hovy et al. 2005 -RRB- evaluate a summary by computing its overlap with a set of model -LRB- human -RRB- summaries ROUGE considers lexical n-grams as the unit for comparing the overlap between summaries while Basic Elements uses larger units of comparison based on the output of syntactic parsers	amod_parsers_syntactic prep_of_output_parsers det_output_the prep_on_based_output vmod_units_based prep_of_units_comparison amod_units_larger dobj_uses_units nsubj_uses_Elements mark_uses_while amod_Elements_Basic advcl_overlap_uses prep_between_overlap_summaries nsubj_overlap_the ccomp_comparing_overlap det_unit_the prep_as_n-grams_unit amod_n-grams_lexical prepc_for_considers_comparing dobj_considers_n-grams nsubj_considers_ROUGE parataxis_summaries_considers amod_summaries_human dep_model_summaries prep_of_set_model det_set_a prep_with_overlap_set nsubj_overlap_its ccomp_computing_overlap det_summary_a prepc_by_evaluate_computing dobj_evaluate_summary nsubj_evaluate_Elements nsubj_evaluate_descendent nsubj_evaluate_ROUGE amod_Hovy_2005 dep_Hovy_al. nn_Hovy_et dep_Elements_BE amod_Elements_Basic amod_descendent_linguisticallymotivated poss_descendent_its amod_Lin_2004 dep_ROUGE_Hovy conj_and_ROUGE_Elements conj_and_ROUGE_descendent dep_ROUGE_Lin nn_ROUGE_Introduction num_ROUGE_1
C08-1023	W04-1013	o	We report on ROUGE-1 -LRB- unigrams -RRB- ROUGE-2 -LRB- bigrams -RRB- ROUGE W-1 .2 -LRB- weighted LCS -RRB- and ROUGE-S * -LRB- skip bigrams -RRB- as they appear to correlate well with human judgments for longer multi-document summaries particularly ROUGE-1 -LRB- Lin 2004 -RRB-	amod_Lin_2004 appos_ROUGE-1_Lin advmod_ROUGE-1_particularly appos_summaries_ROUGE-1 amod_summaries_multi-document amod_summaries_longer prep_for_judgments_summaries amod_judgments_human prep_with_correlate_judgments advmod_correlate_well aux_correlate_to xcomp_appear_correlate nsubj_appear_they mark_appear_as dobj_skip_bigrams dep_*_skip nn_*_ROUGE-S amod_LCS_weighted appos_.2_LCS nn_.2_W-1 nn_.2_ROUGE appos_ROUGE-2_bigrams conj_and_ROUGE-1_* conj_and_ROUGE-1_.2 conj_and_ROUGE-1_ROUGE-2 appos_ROUGE-1_unigrams advcl_report_appear prep_on_report_* prep_on_report_.2 prep_on_report_ROUGE-2 prep_on_report_ROUGE-1 nsubj_report_We
C08-2006	W04-1013	p	To evaluate the quality of our generated summaries we choose to use the ROUGE3 -LRB- Lin 2004 -RRB- evaluation toolkit that has been found to be highly correlated with human judgments	amod_judgments_human prep_with_correlated_judgments advmod_correlated_highly auxpass_correlated_be aux_correlated_to xcomp_found_correlated auxpass_found_been aux_found_has nsubjpass_found_that dep_evaluation_toolkit dep_evaluation_Lin dep_Lin_2004 dep_ROUGE3_evaluation det_ROUGE3_the dobj_use_ROUGE3 aux_use_to ccomp_choose_found xcomp_choose_use nsubj_choose_we advcl_choose_evaluate amod_summaries_generated poss_summaries_our prep_of_quality_summaries det_quality_the dobj_evaluate_quality aux_evaluate_To
D08-1032	W04-1013	o	We evaluate the summaries using the automatic evaluation tool ROUGE -LRB- Lin 2004 -RRB- -LRB- described in Section 6 -RRB- and the ROUGE value works as the feedback to our learning loop	nn_loop_learning poss_loop_our prep_to_feedback_loop det_feedback_the prep_as_works_feedback nsubj_works_value nn_value_ROUGE det_value_the num_Section_6 prep_in_described_Section dep_Lin_2004 dep_ROUGE_described appos_ROUGE_Lin nn_ROUGE_tool nn_ROUGE_evaluation amod_ROUGE_automatic det_ROUGE_the dobj_using_ROUGE conj_and_summaries_works vmod_summaries_using det_summaries_the dobj_evaluate_works dobj_evaluate_summaries nsubj_evaluate_We ccomp_``_evaluate
D08-1032	W04-1013	o	Some improvements on BOW are given by the use of dependency trees and syntactic parse trees -LRB- Hirao et al. 2004 -RRB- -LRB- Punyakanok et al. 2004 -RRB- -LRB- Zhang and Lee 2003 -RRB- but these too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs	advmod_paragraphs_even amod_sentences_articulated amod_sentences_long conj_and_long_articulated conj_or_expressed_paragraphs agent_expressed_sentences auxpass_expressed_are nsubjpass_expressed_answers poss_answers_whose rcmod_questions_paragraphs rcmod_questions_expressed amod_questions_complex prep_with_dealing_questions advmod_dealing_when advcl_adequate_dealing neg_adequate_not cop_adequate_are nsubj_adequate_these advmod_are_too num_Zhang_2003 conj_and_Zhang_Lee amod_Punyakanok_2004 dep_Punyakanok_al. nn_Punyakanok_et amod_Hirao_2004 dep_Hirao_al. nn_Hirao_et nn_trees_parse amod_trees_syntactic conj_and_trees_trees nn_trees_dependency prep_of_use_trees prep_of_use_trees det_use_the conj_but_given_adequate dep_given_Lee dep_given_Zhang dep_given_Punyakanok dep_given_Hirao agent_given_use auxpass_given_are nsubjpass_given_improvements prep_on_improvements_BOW det_improvements_Some ccomp_``_adequate ccomp_``_given
D08-1032	W04-1013	p	We carried out automatic evaluation of our summaries using ROUGE -LRB- Lin 2004 -RRB- toolkit which has been widely adopted by DUC for automatic summarization evaluation	nn_evaluation_summarization amod_evaluation_automatic prep_for_adopted_evaluation agent_adopted_DUC advmod_adopted_widely auxpass_adopted_been aux_adopted_has nsubjpass_adopted_which rcmod_toolkit_adopted nn_toolkit_ROUGE dep_Lin_2004 dep_ROUGE_Lin dobj_using_toolkit vmod_summaries_using poss_summaries_our prep_of_evaluation_summaries amod_evaluation_automatic dobj_carried_evaluation prt_carried_out nsubj_carried_We
D08-1032	W04-1013	o	The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_sequences_Lin nn_sequences_embedding poss_sequences_their prep_within_relations_sequences amod_relations_spatial amod_relations_different prep_of_LCSes_relations dobj_differentiate_LCSes neg_differentiate_not aux_differentiate_does nsubj_differentiate_it mark_differentiate_that ccomp_problem_differentiate det_problem_a dobj_has_problem nsubj_has_LCS amod_LCS_basic det_LCS_The
D08-1032	W04-1013	o	Given two sentences X and Y the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated in -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_in_Lin prep_stated_in mark_stated_as nn_procedure_programming amod_procedure_dynamic amod_procedure_similar det_procedure_the dobj_using_procedure advcl_computed_stated xcomp_computed_using auxpass_computed_be aux_computed_can nsubjpass_computed_Y nsubjpass_computed_X conj_and_X_Y prep_of_score_Y prep_of_score_X nn_score_WLCS det_score_the appos_X_score conj_and_X_Y rcmod_sentences_computed num_sentences_two pobj_Given_sentences ccomp_``_Given
D08-1032	W04-1013	o	We computed the LCS and WLCS-based F-measure following -LRB- Lin 2004 -RRB- using both the query pool and the sentence pool as in the previous section	amod_section_previous det_section_the pobj_in_section pcomp_as_in nn_pool_sentence det_pool_the conj_and_pool_pool nn_pool_query det_pool_the preconj_pool_both prep_using_as dobj_using_pool dobj_using_pool dep_2004_Lin amod_F-measure_WLCS-based conj_and_LCS_F-measure det_LCS_the prepc_following_computed_using dep_computed_2004 dobj_computed_F-measure dobj_computed_LCS nsubj_computed_We ccomp_``_computed
D08-1032	W04-1013	o	Following -LRB- Lin 2004 -RRB- we computed the skip bi-gram score using both the sentence pool and the query pool	nn_pool_query det_pool_the conj_and_pool_pool nn_pool_sentence det_pool_the preconj_pool_both dobj_using_pool dobj_using_pool amod_score_bi-gram dep_score_skip det_score_the vmod_computed_using dobj_computed_score nsubj_computed_we vmod_computed_Following dep_2004_Lin dep_Following_2004
D09-1132	W04-1013	o	-LRB- 2004 2005 -RRB- but its performance was worse than our centroid baseline	amod_baseline_centroid poss_baseline_our prep_than_worse_baseline cop_worse_was nsubj_worse_performance poss_performance_its conj_but_2004_worse dep_2004_2005 dep_''_worse dep_''_2004
D09-1132	W04-1013	o	-LRB- 2004 -RRB- use an information extraction engine to extract linguistic features from documents relevant to the target term	nn_term_target det_term_the prep_to_relevant_term amod_documents_relevant amod_features_linguistic prep_from_extract_documents dobj_extract_features aux_extract_to nn_engine_extraction nn_engine_information det_engine_an vmod_use_extract dobj_use_engine nsubj_use_2004
D09-1132	W04-1013	p	Here we use the more established ROUGE-W measure -LRB- Lin 2004 -RRB- instead	dep_Lin_2004 dep_measure_Lin nn_measure_ROUGE-W amod_measure_established amod_measure_more det_measure_the advmod_use_instead dobj_use_measure nsubj_use_we advmod_use_Here
D09-1132	W04-1013	o	WLCS -LRB- w d -RRB- = summationtextmi = 0 f -LRB- ki -RRB- We then compute the following quantities where | | is word length and f1 is the inverse of f. P -LRB- w d -RRB- = f1 -LRB- WLCS -LRB- w d -RRB- f -LRB- | w | -RRB- -RRB- R -LRB- w d -RRB- = f1 -LRB- WLCS -LRB- w d -RRB- f -LRB- | d | -RRB- -RRB- F -LRB- w d -RRB- = -LRB- 1 +2 -RRB- R -LRB- w d -RRB- P -LRB- w d -RRB- R -LRB- w d -RRB- +2 P -LRB- w d -RRB- In effect P -LRB- w d -RRB- examines how close the longest common substring is to w and R -LRB- w d -RRB- how close it is to d. Following Lin -LRB- 2004 -RRB- we use = 8 assigninggreaterimportancetoR -LRB- w d -RRB-	appos_w_d dep_assigninggreaterimportancetoR_w appos_8_assigninggreaterimportancetoR dep_=_8 acomp_use_= nsubj_use_we appos_Lin_2004 prep_following_d._Lin aux_d._to xcomp_is_d. nsubj_is_it dep_is_close advmod_close_how appos_w_d dep_R_w ccomp_w_is conj_and_w_R aux_w_to xcomp_is_R xcomp_is_w nsubj_is_substring advmod_is_close amod_substring_common amod_substring_longest det_substring_the advmod_close_how dep_examines_use ccomp_examines_is nsubj_examines_R appos_w_d dep_P_w appos_w_d dep_P_w appos_w_d appos_R_P prep_in_R_effect dep_R_P num_R_+2 dep_R_w nn_R_P appos_w_d dep_P_w nn_P_R appos_w_d dep_R_w amod_R_= nn_R_F dep_R_WLCS dep_R_inverse num_+2_1 dep_=_+2 appos_w_d dep_F_w nn_|_d num_|_| dep_f_| appos_w_d dep_WLCS_f dep_WLCS_w nn_WLCS_f1 amod_WLCS_= nn_WLCS_R appos_w_d dep_R_w nn_|_w num_|_| dep_f_| appos_w_d dep_WLCS_f dep_WLCS_w nn_WLCS_f1 amod_WLCS_= nn_WLCS_P appos_w_d dep_P_w nn_P_f. prep_of_inverse_WLCS det_inverse_the cop_inverse_is nsubj_inverse_WLCS nn_length_word cop_length_is nsubj_length_| advmod_length_where amod_|_| conj_and_quantities_f1 rcmod_quantities_length prep_following_the_f1 prep_following_the_quantities dobj_compute_the advmod_compute_then nsubj_compute_We appos_f_ki num_f_0 dep_=_f rcmod_summationtextmi_compute amod_summationtextmi_= dep_=_summationtextmi appos_w_d amod_WLCS_= dep_WLCS_w
E09-1048	W04-1013	o	ROUGE -LRB- Lin 2004 -RRB- a recall-oriented evaluation package for automatic summarization	amod_summarization_automatic prep_for_package_summarization nn_package_evaluation amod_package_recall-oriented det_package_a amod_Lin_2004 appos_ROUGE_package appos_ROUGE_Lin
E09-1048	W04-1013	o	In addition to tf.idf scores Hulth -LRB- 2004 -RRB- uses part-of-speech tags and NP chunks and complements this with machine learning the latter has been used to good results in similar cases -LRB- Turney 2000 Neto et al. 2002 -RRB-	num_Neto_2002 nn_Neto_al. nn_Neto_et dep_Turney_Neto appos_Turney_2000 appos_cases_Turney amod_cases_similar prep_in_results_cases amod_results_good prep_to_used_results auxpass_used_been aux_used_has nsubjpass_used_latter det_latter_the nn_learning_machine prep_with_complements_learning dobj_complements_this nsubj_complements_Hulth nn_chunks_NP conj_and_tags_chunks amod_tags_part-of-speech parataxis_uses_used conj_and_uses_complements dobj_uses_chunks dobj_uses_tags nsubj_uses_Hulth prep_in_addition_to_uses_scores appos_Hulth_2004 amod_scores_tf.idf
E09-1089	W04-1013	o	A pipage approach -LRB- Ageev and Sviridenko 2004 -RRB- has been proposed for MCKP but we do not use this algorithm since it requires costly partial enumeration and solutions to many linear relaxation problems	nn_problems_relaxation amod_problems_linear amod_problems_many conj_and_enumeration_solutions amod_enumeration_partial amod_enumeration_costly prep_to_requires_problems dobj_requires_solutions dobj_requires_enumeration nsubj_requires_it mark_requires_since det_algorithm_this dobj_use_algorithm neg_use_not aux_use_do nsubj_use_we advcl_proposed_requires conj_but_proposed_use prep_for_proposed_MCKP auxpass_proposed_been aux_proposed_has nsubjpass_proposed_approach dep_Ageev_2004 conj_and_Ageev_Sviridenko appos_approach_Sviridenko appos_approach_Ageev nn_approach_pipage det_approach_A ccomp_``_use ccomp_``_proposed
E09-1089	W04-1013	o	4.6 Weakly-constrained algorithms In evaluation with ROUGE -LRB- Lin 2004 -RRB- summaries are truncated to a target length K. Yih et al. -LRB- 2007 -RRB- usedastackdecodingwithaslightmodication which allows the last sentence in a summary to be truncated to a target length	nn_length_target det_length_a prep_to_truncated_length auxpass_truncated_be aux_truncated_to vmod_summary_truncated det_summary_a prep_in_sentence_summary amod_sentence_last det_sentence_the dobj_allows_sentence nsubj_allows_which nn_usedastackdecodingwithaslightmodication_al. appos_al._2007 rcmod_Yih_allows dep_Yih_usedastackdecodingwithaslightmodication nn_Yih_et nn_Yih_K. nn_Yih_length nn_Yih_target det_Yih_a prep_to_truncated_Yih auxpass_truncated_are nsubjpass_truncated_summaries ccomp_truncated_algorithms dep_Lin_2004 appos_ROUGE_Lin prep_with_evaluation_ROUGE prep_in_algorithms_evaluation amod_algorithms_Weakly-constrained num_algorithms_4.6
E09-1089	W04-1013	p	ROUGE version 1.5.5 -LRB- Lin 2004 -RRB- was used for evaluation .2 Among others we focus on ROUGE-1 in the discussion of the result because ROUGE-1 has proved to have strong correlation with human annotation -LRB- Lin 2004 Lin and Hovy 2003 -RRB-	num_Lin_2003 conj_and_Lin_Hovy dep_Lin_Hovy dep_Lin_Lin appos_Lin_2004 dep_annotation_Lin amod_annotation_human prep_with_correlation_annotation amod_correlation_strong dobj_have_correlation aux_have_to xcomp_proved_have aux_proved_has nsubj_proved_ROUGE-1 mark_proved_because det_result_the prep_of_discussion_result det_discussion_the advcl_focus_proved prep_in_focus_discussion prep_on_focus_ROUGE-1 nsubj_focus_we ccomp_focus_used prep_among_.2_others nn_.2_evaluation prep_for_used_.2 auxpass_used_was nsubjpass_used_version amod_Lin_2004 appos_version_Lin num_version_1.5.5 nn_version_ROUGE
E09-1089	W04-1013	o	We use only the words that are content words -LRB- nouns verbs or adjectives -RRB- and not in the stopword list used in ROUGE -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_ROUGE_Lin prep_in_used_ROUGE vmod_list_used nn_list_stopword det_list_the pobj_in_list neg_in_not conj_or_nouns_adjectives conj_or_nouns_verbs conj_and_words_in dep_words_adjectives dep_words_verbs dep_words_nouns amod_words_content cop_words_are nsubj_words_that rcmod_words_in rcmod_words_words det_words_the advmod_words_only dobj_use_words nsubj_use_We
H05-1019	W04-1013	o	ROUGE-N ROUGE-N is an N-gram-based evaluation measure defined as follows -LRB- Lin 2004b -RRB- ROUGE-Na59a61a146a31a62a90a147a49a65a68a67 a215 a77a83a216 a209a68a217a61a173 a172a27a218 a77 a215 a219a27a220a158a221a183a222a85a223 a172 a173a78a224a76a225a164a226 a59a136a227a158a228 a152a130a150a104a229 a65 a215 a77a29a216 a209a68a217a76a173 a172 a218 a77 a215 a219a27a220a159a221a183a222a85a223 a59a136a227a158a228 a152a130a150 a229 a65 -LRB- 10 -RRB- Here a230a66a231a37a232a21a233a27a234a118a28a78a235a37a236a25a237a37a238a11a239a168a36 is the number of an N-gram and a230a66a231a37a232a21a233a27a234 a196 a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36 denotes the number of ngram co-occurrences in a system output and the reference	det_reference_the conj_and_output_reference nn_output_system det_output_a nn_co-occurrences_ngram prep_of_number_co-occurrences det_number_the prep_in_denotes_reference prep_in_denotes_output dobj_denotes_number nsubj_denotes_a65 nn_a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36_a196 nn_a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36_a230a66a231a37a232a21a233a27a234 nn_a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36_N-gram det_a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36_an conj_and_N-gram_a230a66a231a37a232a21a233a27a234 prep_of_number_a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36 det_number_the cop_number_is nsubj_number_a230a66a231a37a232a21a233a27a234a118a28a78a235a37a236a25a237a37a238a11a239a168a36 rcmod_a65_number advmod_a65_Here appos_a65_10 nn_a65_a229 nn_a65_a152a130a150 nn_a65_a59a136a227a158a228 nn_a65_a219a27a220a159a221a183a222a85a223 num_a65_a215 nn_a65_a77 nn_a65_a218 nn_a65_a172 nn_a65_a209a68a217a76a173 nn_a65_a77a29a216 num_a65_a215 nn_a65_a65 nn_a65_a152a130a150a104a229 num_a65_a59a136a227a158a228 nn_a65_a173a78a224a76a225a164a226 nn_a65_a172 nn_a65_a219a27a220a158a221a183a222a85a223 num_a65_a215 nn_a65_a77 nn_a65_a172a27a218 nn_a65_a209a68a217a61a173 nn_a65_a77a83a216 num_a65_a215 nn_a65_ROUGE-Na59a61a146a31a62a90a147a49a65a68a67 appos_Lin_2004b dep_follows_Lin mark_follows_as advcl_defined_follows parataxis_measure_denotes vmod_measure_defined nn_measure_evaluation amod_measure_N-gram-based det_measure_an cop_measure_is nsubj_measure_ROUGE-N nn_ROUGE-N_ROUGE-N
H05-1019	W04-1013	o	ROUGE-S ROUGE-S is an extension of ROUGE-2 defined as follows -LRB- Lin 2004b -RRB- ROUGE-Sa59a61a146a31a62a98a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a161a163 a157 a134a61a135a93a245a246 a2 a59a61a146a31a62a98a147a49a65a161a163 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a164a147a49a65 a157 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a51a128a104a162 a2 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a98a147a49a65 -LRB- 11 -RRB- Where a166a168a169a78a170a248a247a250a249 a26 and a171a138a169a90a170a158a247a250a249 a26 are defined as follows a251 a134a61a135a89a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a248a253a85a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2a23a147 -LRB- 12 -RRB- a3 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a83a253a118a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2 a146 -LRB- 13 -RRB- Here function Skip2 returns the number of skipbi-grams that are common to a141 and a139 ROUGE-SU ROUGE-SU is an extension of ROUGE-S which includes unigrams as a feature defined as follows -LRB- Lin 2004b -RRB- ROUGE-SUa59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a117a163 a157 a134a5a4 a59a61a146a31a62a98a147a49a65a71a163 a145 a134a6a4 a59a61a146a31a62a98a147a49a65 a157 a134a5a4 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145 a134a5a4 a59a61a146a31a62a164a147a49a65 -LRB- 14 -RRB- Where a166 a169a8a7 and a171 a169a8a7 are defined as follows a251 a134a5a4 a59a61a146a31a62a98a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 -LRB- # of skip bigrams + # of unigrams -RRB- a2 a147 -LRB- 15 -RRB- a3 a134a5a4 a59a61a146a31a62a90a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 -LRB- # of skip bigrams + # of unigrams -RRB- a2 a146 -LRB- 16 -RRB- Here function SU returns the number of skip-bigrams and unigrams that are common to a141 and a139 ROUGE-L ROUGE-L is an LCS-based evaluation measure defined as follows -LRB- Lin 2004b -RRB- ROUGE-La59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a161a163 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a161a163 a145a12a10 a225a90a134 a59a61a146a31a62a98a147a49a65 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145a12a10 a225a98a134 a59a61a146a31a62a90a147a49a65 -LRB- 17 -RRB- where a166a14a13a250a241a132a169 and a171a15a13a250a241a130a169 are defined as follows a157a11a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a91 a16 a75 a77a29a216 LCSa17a244a59a61a156 a88 a62a90a146a21a65 -LRB- 18 -RRB- a145a18a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a95 a16 a75a78a77a83a216 LCSa17 a59a61a156a34a88a78a62a98a146a21a65 -LRB- 19 -RRB- Here LCSa19a244a28a78a144a183a114a93a32a93a139a102a36 is the LCS score of the union longest common subsequence between reference sentences a144a25a114 and a139 a115 and a122 are the number of words contained in a141 and a139 respectively	prep_in_contained_a141 vmod_words_contained prep_of_number_words det_number_the cop_number_are nsubj_number_a59a61a156a34a88a78a62a98a146a21a65 conj_and_a115_a122 dep_a144a25a114_a122 dep_a144a25a114_a115 conj_and_a144a25a114_a139 prep_sentences_a139 prep_sentences_a144a25a114 nn_sentences_reference amod_subsequence_common amod_subsequence_longest dep_union_subsequence det_union_the prep_between_score_sentences prep_of_score_union nn_score_LCS det_score_the cop_score_is nsubj_score_LCSa19a244a28a78a144a183a114a93a32a93a139a102a36 rcmod_a59a61a156a34a88a78a62a98a146a21a65_score advmod_a59a61a156a34a88a78a62a98a146a21a65_Here appos_a59a61a156a34a88a78a62a98a146a21a65_19 nn_a59a61a156a34a88a78a62a98a146a21a65_LCSa17 nn_a59a61a156a34a88a78a62a98a146a21a65_a75a78a77a83a216 nn_a59a61a156a34a88a78a62a98a146a21a65_a16 nn_a59a61a156a34a88a78a62a98a146a21a65_a95 nn_a59a61a156a34a88a78a62a98a146a21a65_a101 nn_a59a61a156a34a88a78a62a98a146a21a65_a59a61a146a31a62a98a147a49a65a68a67 nn_a59a61a156a34a88a78a62a98a146a21a65_a225a98a134 nn_a59a61a156a34a88a78a62a98a146a21a65_a145a18a10 num_a59a61a156a34a88a78a62a98a146a21a65_18 advmod_a62a90a146a21a65_respectively conj_and_a62a90a146a21a65_a139 rcmod_a62a90a146a21a65_number nn_a62a90a146a21a65_a88 nn_a62a90a146a21a65_LCSa17a244a59a61a156 nn_a62a90a146a21a65_a77a29a216 nn_a62a90a146a21a65_a75 nn_a62a90a146a21a65_a16 nn_a62a90a146a21a65_a91 nn_a62a90a146a21a65_a101 nn_a62a90a146a21a65_a59a61a146a31a62a98a147a49a65a68a67 nn_a62a90a146a21a65_a225a98a134 nn_a62a90a146a21a65_a157a11a10 mark_follows_as advcl_defined_follows auxpass_defined_are nsubjpass_defined_a171a15a13a250a241a130a169 nsubjpass_defined_a166a14a13a250a241a132a169 advmod_defined_where conj_and_a166a14a13a250a241a132a169_a171a15a13a250a241a130a169 dep_a59a61a146a31a62a90a147a49a65_a139 dep_a59a61a146a31a62a90a147a49a65_a62a90a146a21a65 rcmod_a59a61a146a31a62a90a147a49a65_defined appos_a59a61a146a31a62a90a147a49a65_17 nn_a59a61a146a31a62a90a147a49a65_a225a98a134 nn_a59a61a146a31a62a90a147a49a65_a145a12a10 nn_a59a61a146a31a62a90a147a49a65_a2 nn_a59a61a146a31a62a90a147a49a65_a59a61a146a31a62a90a147a49a65a47a128a49a162 nn_a59a61a146a31a62a90a147a49a65_a225a90a134 nn_a59a61a146a31a62a90a147a49a65_a157a11a10 nn_a59a61a146a31a62a90a147a49a65_a59a61a146a31a62a98a147a49a65 nn_a59a61a146a31a62a90a147a49a65_a225a90a134 nn_a59a61a146a31a62a90a147a49a65_a145a12a10 nn_a59a61a146a31a62a90a147a49a65_a59a61a146a31a62a90a147a49a65a161a163 nn_a59a61a146a31a62a90a147a49a65_a225a90a134 nn_a59a61a146a31a62a90a147a49a65_a157a11a10 nn_a59a61a146a31a62a90a147a49a65_a65a161a163 nn_a59a61a146a31a62a90a147a49a65_a2 nn_a59a61a146a31a62a90a147a49a65_a59a68a101a161a128a49a162 nn_a59a61a146a31a62a90a147a49a65_ROUGE-La59a61a146a31a62a90a147a49a65a68a67 appos_Lin_2004b dep_follows_Lin mark_follows_as advcl_defined_follows dep_measure_a59a61a146a31a62a90a147a49a65 vmod_measure_defined nn_measure_evaluation amod_measure_LCS-based det_measure_an cop_measure_is nsubj_measure_ROUGE-L nn_ROUGE-L_ROUGE-L conj_and_a141_a139 prep_to_common_a139 prep_to_common_a141 cop_common_are nsubj_common_that conj_and_skip-bigrams_unigrams rcmod_number_common prep_of_number_unigrams prep_of_number_skip-bigrams det_number_the dobj_returns_number nsubj_returns_SU nn_SU_function appos_a146_16 nn_a146_a2 nn_a146_unigrams amod_a146_of num_a146_# conj_+_bigrams_a146 dep_skip_measure conj_skip_returns advmod_skip_Here dobj_skip_a146 dobj_skip_bigrams prep_of_#_skip dep_-LRB-_# nn_a59a61a146a31a62a90a147a49a65_a9 nn_a59a61a146a31a62a90a147a49a65_a252 nn_a59a61a146a31a62a90a147a49a65_a59a61a146a31a62a90a147a49a65a68a67 nn_a59a61a146a31a62a90a147a49a65_a134a5a4 nn_a59a61a146a31a62a90a147a49a65_a3 num_a59a61a146a31a62a90a147a49a65_15 dep_a147_a59a61a146a31a62a90a147a49a65 nn_a147_a2 nn_a147_unigrams amod_a147_of num_a147_# conj_+_bigrams_a147 dobj_skip_a147 dobj_skip_bigrams prepc_of_#_skip advcl_-LRB-_# nn_a59a61a146a31a62a90a147a49a65_a9 nn_a59a61a146a31a62a90a147a49a65_a252 nn_a59a61a146a31a62a90a147a49a65_a59a61a146a31a62a98a147a49a65a68a67 nn_a59a61a146a31a62a90a147a49a65_a134a5a4 nn_a59a61a146a31a62a90a147a49a65_a251 mark_follows_as dep_defined_a59a61a146a31a62a90a147a49a65 advcl_defined_follows auxpass_defined_are nsubjpass_defined_a169a8a7 nsubjpass_defined_a169a8a7 advmod_defined_Where dep_defined_14 nn_a169a8a7_a171 conj_and_a169a8a7_a169a8a7 nn_a169a8a7_a166 dep_a59a61a146a31a62a164a147a49a65_defined nn_a59a61a146a31a62a164a147a49a65_a134a5a4 nn_a59a61a146a31a62a164a147a49a65_a145 nn_a59a61a146a31a62a164a147a49a65_a2 nn_a59a61a146a31a62a164a147a49a65_a59a61a146a31a62a90a147a49a65a47a128a49a162 nn_a59a61a146a31a62a164a147a49a65_a134a5a4 nn_a59a61a146a31a62a164a147a49a65_a157 nn_a59a61a146a31a62a164a147a49a65_a59a61a146a31a62a98a147a49a65 nn_a59a61a146a31a62a164a147a49a65_a134a6a4 nn_a59a61a146a31a62a164a147a49a65_a145 nn_a59a61a146a31a62a164a147a49a65_a59a61a146a31a62a98a147a49a65a71a163 nn_a59a61a146a31a62a164a147a49a65_a134a5a4 nn_a59a61a146a31a62a164a147a49a65_a157 nn_a59a61a146a31a62a164a147a49a65_a65a117a163 nn_a59a61a146a31a62a164a147a49a65_a2 nn_a59a61a146a31a62a164a147a49a65_a59a68a101a161a128a49a162 nn_a59a61a146a31a62a164a147a49a65_ROUGE-SUa59a61a146a31a62a90a147a49a65a68a67 appos_Lin_2004b dep_follows_Lin mark_follows_as advcl_defined_follows vmod_feature_defined det_feature_a prep_as_unigrams_feature dobj_includes_unigrams nsubj_includes_which rcmod_ROUGE-S_includes prep_of_extension_ROUGE-S det_extension_an cop_extension_is nsubj_extension_# nn_ROUGE-SU_ROUGE-SU conj_and_a141_a139 prep_to_common_a139 prep_to_common_a141 cop_common_are nsubj_common_that rcmod_skipbi-grams_common prep_of_number_skipbi-grams det_number_the dobj_returns_number nsubj_returns_Skip2 nn_Skip2_function advmod_a146_Here appos_a146_13 nn_a146_a2 amod_a146_bigram dep_skip_returns dobj_skip_a146 prep_of_#_ROUGE-SU dep_#_skip dep_a59a61a146a31a62a90a147a49a65_a59a61a146a31a62a164a147a49a65 rcmod_a59a61a146a31a62a90a147a49a65_extension nn_a59a61a146a31a62a90a147a49a65_a1 nn_a59a61a146a31a62a90a147a49a65_a252a83a253a118a254a255 nn_a59a61a146a31a62a90a147a49a65_a59a61a146a31a62a90a147a49a65a68a67 nn_a59a61a146a31a62a90a147a49a65_a2 nn_a59a61a146a31a62a90a147a49a65_a134a136a135a93a245a246 nn_a59a61a146a31a62a90a147a49a65_a3 num_a59a61a146a31a62a90a147a49a65_12 dep_a2a23a147_a59a61a146a31a62a90a147a49a65 amod_a2a23a147_bigram dobj_skip_a2a23a147 prepc_of_#_skip dep_a59a61a146a31a62a90a147a49a65_# nn_a59a61a146a31a62a90a147a49a65_a1 nn_a59a61a146a31a62a90a147a49a65_a252a248a253a85a254a255 nn_a59a61a146a31a62a90a147a49a65_a59a61a146a31a62a90a147a49a65a68a67 nn_a59a61a146a31a62a90a147a49a65_a2 nn_a59a61a146a31a62a90a147a49a65_a134a61a135a89a245a246 nn_a59a61a146a31a62a90a147a49a65_a251 mark_follows_as advcl_defined_follows auxpass_defined_are nsubjpass_defined_a26 nsubjpass_defined_a26 advmod_defined_Where nn_a26_a171a138a169a90a170a158a247a250a249 conj_and_a26_a26 nn_a26_a166a168a169a78a170a248a247a250a249 dep_a59a61a146a31a62a98a147a49a65_a59a61a146a31a62a90a147a49a65 rcmod_a59a61a146a31a62a98a147a49a65_defined appos_a59a61a146a31a62a98a147a49a65_11 nn_a59a61a146a31a62a98a147a49a65_a2 nn_a59a61a146a31a62a98a147a49a65_a134a61a135a89a245a246 nn_a59a61a146a31a62a98a147a49a65_a145 nn_a59a61a146a31a62a98a147a49a65_a2 nn_a59a61a146a31a62a98a147a49a65_a59a61a146a31a62a90a147a49a65a51a128a104a162 nn_a59a61a146a31a62a98a147a49a65_a2 nn_a59a61a146a31a62a98a147a49a65_a134a136a135a93a245a246 nn_a59a61a146a31a62a98a147a49a65_a157 nn_a59a61a146a31a62a98a147a49a65_a59a61a146a31a62a164a147a49a65 nn_a59a61a146a31a62a98a147a49a65_a2 nn_a59a61a146a31a62a98a147a49a65_a134a61a135a89a245a246 nn_a59a61a146a31a62a98a147a49a65_a145 nn_a59a61a146a31a62a98a147a49a65_a59a61a146a31a62a98a147a49a65a161a163 nn_a59a61a146a31a62a98a147a49a65_a2 nn_a59a61a146a31a62a98a147a49a65_a134a61a135a93a245a246 nn_a59a61a146a31a62a98a147a49a65_a157 nn_a59a61a146a31a62a98a147a49a65_a65a161a163 nn_a59a61a146a31a62a98a147a49a65_a2 nn_a59a61a146a31a62a98a147a49a65_a59a68a101a161a128a104a162 nn_a59a61a146a31a62a98a147a49a65_ROUGE-Sa59a61a146a31a62a98a147a49a65a68a67 appos_Lin_2004b dep_follows_Lin mark_follows_as advcl_defined_follows vmod_ROUGE-2_defined dep_extension_a59a61a146a31a62a98a147a49a65 prep_of_extension_ROUGE-2 det_extension_an cop_extension_is nsubj_extension_ROUGE-S nn_ROUGE-S_ROUGE-S
H05-1019	W04-1013	o	For ROUGE-S and ROUGE-SU we use three variations following -LRB- Lin 2004b -RRB- the maximum skip distances are 4 9 and infinity 7	num_infinity_7 conj_and_4_infinity conj_and_4_9 dep_are_infinity dep_are_9 dep_are_4 amod_distances_skip dep_maximum_are dep_maximum_distances det_maximum_the nn_2004b_Lin dep_following_2004b num_variations_three dep_use_maximum prep_use_following dobj_use_variations nsubj_use_we prep_for_use_ROUGE-SU prep_for_use_ROUGE-S conj_and_ROUGE-S_ROUGE-SU
H05-1019	W04-1013	o	Our method is based on the Extended String Subsequence Kernel -LRB- ESK -RRB- -LRB- Hirao et al. 2004b -RRB- which is a kind of convolution kernel -LRB- Collins and Duffy 2001 -RRB-	amod_Collins_2001 conj_and_Collins_Duffy nn_kernel_convolution dep_kind_Duffy dep_kind_Collins prep_of_kind_kernel det_kind_a cop_kind_is nsubj_kind_which nn_al._et rcmod_Hirao_kind appos_Hirao_2004b dep_Hirao_al. appos_Kernel_ESK nn_Kernel_Subsequence nn_Kernel_String nn_Kernel_Extended det_Kernel_the dep_based_Hirao prep_on_based_Kernel auxpass_based_is nsubjpass_based_method poss_method_Our ccomp_``_based
H05-1019	W04-1013	o	The results of the comparison with ROUGE-N -LRB- Lin and Hovy 2003 Lin 2004a Lin 2004b -RRB- ROUGE-S -LRB- U -RRB- -LRB- Lin 2004b Lin and Och 2004 -RRB- and ROUGE-L -LRB- Lin 2004a Lin 2004b -RRB- show that our method correlates more closely with human evaluations and is more robust	advmod_robust_more cop_robust_is nsubj_robust_method amod_evaluations_human advmod_closely_more conj_and_correlates_robust prep_with_correlates_evaluations advmod_correlates_closely nsubj_correlates_method mark_correlates_that poss_method_our ccomp_show_robust ccomp_show_correlates nsubj_show_results nn_2004b_Lin dep_Lin_2004b appos_Lin_2004a num_Lin_2004 conj_and_Lin_Och dep_Lin_Lin conj_and_Lin_ROUGE-L dep_Lin_Och dep_Lin_Lin appos_Lin_2004b appos_ROUGE-S_ROUGE-L appos_ROUGE-S_Lin appos_ROUGE-S_U nn_2004b_Lin appos_Lin_2004a dep_Lin_2004b conj_and_Lin_Lin conj_and_Lin_2003 conj_and_Lin_Hovy conj_ROUGE-N_ROUGE-S appos_ROUGE-N_Lin appos_ROUGE-N_2003 appos_ROUGE-N_Hovy appos_ROUGE-N_Lin prep_with_comparison_ROUGE-N det_comparison_the prep_of_results_comparison det_results_The
H05-1019	W04-1013	o	One is the longest common subsequence -LRB- LCS -RRB- based approach -LRB- Hori et al. 2003 Lin 2004a Lin 2004b Lin and Och 2004 -RRB-	dep_2004_Och dep_2004_Lin dep_2004_Lin dep_2004_Lin appos_Lin_2004b conj_and_Lin_Och conj_and_Lin_Lin conj_and_Lin_Lin appos_Lin_2004a nn_al._et dep_Hori_2004 appos_Hori_2003 dep_Hori_al. dep_approach_Hori amod_approach_based dep_subsequence_approach appos_subsequence_LCS amod_subsequence_common amod_subsequence_longest det_subsequence_the cop_subsequence_is nsubj_subsequence_One ccomp_``_subsequence
H05-1019	W04-1013	o	a0 subsequence S1 S2 a0 subsequence S1 S2 a0 subsequence S1 S2 Becoming 1 1 Becoming-is a1 a2 a1 a2 astronaut-DREAM 0 a1 a2 DREAM 1 1 Becoming-my a1a4a3a5a1a4a3 astronaut-ambition 0 a1 a2 SPACEMAN 1 1 SPACEMAN-DREAM a1a4a3a5a1 a2 astronaut-is 0 1 a 1 0 SPACEMAN-ambition 0 a1 a2 astronaut-my 0 a1 ambition 0 1 SPACEMAN-dream a1 a3 0 cosmonaut-DREAM a1 a3 0 1 an 0 1 SPACEMAN-great a1 a2 0 cosmonaut-dream a1 a3 0 astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great a1 a2 0 cosmonaut 1 0 SPACEMAN-my a1a6a1 cosmonaut-is 1 0 dream 1 0 a-DREAM a1 a7 0 cosmonaut-my a1 0great 1 0 a-SPACEMAN 1 0 great-DREAM 1 0 is 1 1 2 a-cosmonaut 1 0 2 great-dream 1 0 my 1 1 a-dream a1 a7 0 is-DREAM a1 a2 a1 Becoming-DREAM a1a4a8a5a1 a7 a-great a1 a3 0 is-ambition 0 a1 Becoming-SPACEMAN a1a6a1 a-is a1 0 is-dream a1 a2 0 Becoming-a 1 0 a-my a1 a2 0 is-great a1 0 Becoming-ambition 0 a1 a7 an-DREAM 0 a1 a3 is-my 1 1 2 Becoming-an 0 1 an-SPACEMAN 0 1 my-DREAM a1 1 Becoming-astronaut 0 a1 an-ambition 0 a1 a3 my-ambition 0 1 Becoming-cosmonaut a1 0 an-astronaut 0 1 my-dream a1 0 Becoming-dream a1a4a8 0 an-is 0 a1 my-great 1 0 Becoming-great a1 a7 0 an-my 0 a1 a2 2002 Lin and Hovy 2003 Lin 2004a Lin 2004b Soricut and Brill 2004 -RRB-	amod_Soricut_2004 conj_and_Soricut_Brill appos_Lin_2004b appos_Lin_2004a num_Lin_2003 conj_and_Lin_Hovy num_a2_2002 nn_a2_a1 num_a2_0 nn_a2_an-my num_a2_0 dep_a7_a2 nn_a7_a1 nn_a7_Becoming-great number_0_1 dep_my-great_a7 num_my-great_0 nn_my-great_a1 num_my-great_0 nn_my-great_an-is num_my-great_0 dep_a1a4a8_my-great nn_a1a4a8_Becoming-dream dep_a1_a1a4a8 num_a1_0 amod_a1_my-dream num_a1_1 dep_0_a1 amod_an-astronaut_0 num_an-astronaut_0 dep_a1_an-astronaut nn_a1_Becoming-cosmonaut num_a1_1 num_a1_0 dep_my-ambition_a1 nn_my-ambition_a3 nn_my-ambition_a1 num_my-ambition_0 dep_an-ambition_my-ambition nn_an-ambition_a1 num_an-ambition_0 nn_an-ambition_Becoming-astronaut num_an-ambition_1 dep_a1_an-ambition dep_my-DREAM_a1 appos_1_my-DREAM dep_0_1 amod_an-SPACEMAN_0 num_an-SPACEMAN_1 num_an-SPACEMAN_0 dep_Becoming-an_an-SPACEMAN num_Becoming-an_2 number_2_1 number_2_1 dep_is-my_Becoming-an nn_is-my_a3 nn_is-my_a1 num_is-my_0 dep_an-DREAM_is-my nn_an-DREAM_a7 nn_an-DREAM_a1 num_an-DREAM_0 nn_an-DREAM_Becoming-ambition num_an-DREAM_0 dep_a1_an-DREAM nn_a1_is-great num_a1_0 dep_a2_a1 nn_a2_a1 amod_a2_a-my num_a2_0 number_0_1 dep_Becoming-a_a2 num_Becoming-a_0 dep_a2_Becoming-a nn_a2_a1 amod_a2_is-dream num_a2_0 dep_a1_a2 nn_a1_a-is nn_a1_a1a6a1 amod_a1_Becoming-SPACEMAN nn_a1_a1 num_a1_0 nn_a1_is-ambition num_a1_0 dep_a3_a1 nn_a3_a1 nn_a3_a-great nn_a3_a7 nn_a3_a1a4a8a5a1 nn_a3_Becoming-DREAM nn_a3_a1 nn_a3_a2 nn_a3_a1 nn_a3_is-DREAM num_a3_0 dep_a7_Brill dep_a7_Soricut dep_a7_Lin dep_a7_Lin dep_a7_Hovy dep_a7_Lin dep_a7_a3 nn_a7_a1 nn_a7_a-dream amod_a7_1 number_1_1 dep_my_a7 number_0_1 amod_great-dream_my num_great-dream_0 num_great-dream_2 num_great-dream_0 number_0_1 dobj_a-cosmonaut_great-dream dep_2_a-cosmonaut number_2_1 number_2_1 dep_is_2 num_0_1 num_great-DREAM_0 num_great-DREAM_0 number_0_1 dep_a-SPACEMAN_great-DREAM number_0_1 prep_0great_a-SPACEMAN num_0great_0 nn_0great_a1 amod_0great_cosmonaut-my num_0great_0 dep_a7_0great nn_a7_a1 nn_a7_a-DREAM nn_a7_dream num_a7_0 number_0_1 num_dream_0 number_0_1 dep_cosmonaut-is_a7 nn_cosmonaut-is_a1a6a1 nn_cosmonaut-is_SPACEMAN-my num_cosmonaut-is_0 number_0_1 dep_cosmonaut_cosmonaut-is num_cosmonaut_0 dep_a2_cosmonaut nn_a2_a1 nn_a2_cosmonaut-great num_a2_1 number_1_1 dep_SPACEMAN-is_a2 appos_1_SPACEMAN-is dep_0_1 amod_astronaut_0 num_astronaut_0 dep_a3_astronaut nn_a3_a1 amod_a3_cosmonaut-dream prep_a2_a3 num_a2_0 nn_a2_a1 nn_a2_SPACEMAN-great num_a2_1 dep_0_a2 amod_an_is amod_an_0 dep_1_an number_1_0 num_a3_1 nn_a3_a1 nn_a3_cosmonaut-DREAM dep_a3_a3 num_a3_0 nn_a3_a1 nn_a3_SPACEMAN-dream num_a3_1 num_a3_0 dep_ambition_a3 nn_ambition_a1 num_ambition_0 nn_ambition_astronaut-my nn_astronaut-my_a2 nn_astronaut-my_a1 num_astronaut-my_0 dep_SPACEMAN-ambition_ambition amod_SPACEMAN-ambition_0 number_0_1 dep_a_SPACEMAN-ambition dep_1_a number_1_0 dep_astronaut-is_1 nn_astronaut-is_a2 nn_astronaut-is_a1a4a3a5a1 nn_astronaut-is_SPACEMAN-DREAM num_astronaut-is_1 number_1_1 dep_SPACEMAN_astronaut-is nn_SPACEMAN_a2 nn_SPACEMAN_a1 num_SPACEMAN_0 dep_astronaut-ambition_SPACEMAN nn_astronaut-ambition_a1a4a3a5a1a4a3 nn_astronaut-ambition_Becoming-my num_astronaut-ambition_1 number_1_1 dep_DREAM_astronaut-ambition nn_DREAM_a2 nn_DREAM_a1 num_DREAM_0 dep_astronaut-DREAM_DREAM nn_astronaut-DREAM_a2 nn_astronaut-DREAM_a1 nn_astronaut-DREAM_a2 nn_astronaut-DREAM_a1 nn_astronaut-DREAM_Becoming-is num_astronaut-DREAM_1 dep_1_astronaut-DREAM dobj_Becoming_1 vmod_S2_Becoming nn_S2_S1 nn_S2_subsequence nn_S2_a0 nn_S2_S2 nn_S2_S1 nn_S2_subsequence nn_S2_a0 nn_S2_S2 nn_S2_S1 nn_S2_subsequence nn_S2_a0
H05-1019	W04-1013	o	Lin -LRB- 2004a 2004b -RRB- and Lin and Och -LRB- 2004 -RRB- proposed an LCS-based automatic evaluation measure called ROUGE-L	dep_called_ROUGE-L vmod_measure_called nn_measure_evaluation amod_measure_automatic amod_measure_LCS-based det_measure_an dobj_proposed_measure nsubj_proposed_Och nsubj_proposed_Lin nsubj_proposed_Lin appos_Och_2004 dep_2004a_2004b conj_and_Lin_Och conj_and_Lin_Lin dep_Lin_2004a
H05-1019	W04-1013	o	Therefore Lin and Och -LRB- 2004 -RRB- introduced skip-bigram statistics for the evaluation of machine translation	nn_translation_machine prep_of_evaluation_translation det_evaluation_the prep_for_statistics_evaluation nn_statistics_skip-bigram dobj_introduced_statistics nsubj_introduced_Och nsubj_introduced_Lin advmod_introduced_Therefore appos_Och_2004 conj_and_Lin_Och
I05-2027	W04-1013	o	Since the DUC 2004 evaluation Lin -LRB- 2004 -RRB- has concluded that certain ROUGE metrics correlate better with human judgments than others depending on the summarisation task being evaluated i.e. single document headline or multi-document summarisation	amod_summarisation_multi-document conj_or_document_summarisation conj_or_document_headline amod_document_single pobj_i.e._summarisation pobj_i.e._headline pobj_i.e._document auxpass_evaluated_being vmod_task_evaluated nn_task_summarisation det_task_the prep_on_depending_task prep_than_judgments_others amod_judgments_human prep_correlate_i.e. vmod_correlate_depending prep_with_correlate_judgments advmod_correlate_better nsubj_correlate_metrics mark_correlate_that nn_metrics_ROUGE amod_metrics_certain ccomp_concluded_correlate aux_concluded_has nsubj_concluded_Lin prep_since_concluded_evaluation appos_Lin_2004 num_evaluation_2004 nn_evaluation_DUC det_evaluation_the
I05-2027	W04-1013	o	In this paper we compare the performance of this system HybridTrim with the Topiary system and a number of other baseline gisting systems on a collection of news documents from the DUC 2004 corpus -LRB- DUC 2003 -RRB-	amod_DUC_2003 appos_corpus_DUC num_corpus_2004 nn_corpus_DUC det_corpus_the nn_documents_news prep_from_collection_corpus prep_of_collection_documents det_collection_a amod_systems_gisting nn_systems_baseline amod_baseline_other prep_on_number_collection prep_of_number_systems det_number_a conj_and_system_number nn_system_Topiary det_system_the det_system_this appos_performance_HybridTrim prep_of_performance_system det_performance_the prep_with_compare_number prep_with_compare_system dobj_compare_performance nsubj_compare_we prep_in_compare_paper det_paper_this
I05-2027	W04-1013	o	These linguistically-motivated trimming rules -LRB- Dorr et al. 2003 Zajic et al. 2004 -RRB- iteratively remove constituents until a desired sentence compression rate is reached	auxpass_reached_is nsubjpass_reached_rate mark_reached_until nn_rate_compression nn_rate_sentence amod_rate_desired det_rate_a advcl_remove_reached dobj_remove_constituents advmod_remove_iteratively num_Zajic_2004 nn_Zajic_al. nn_Zajic_et dep_Dorr_remove dep_Dorr_Zajic appos_Dorr_2003 dep_Dorr_al. nn_Dorr_et dep_rules_Dorr amod_rules_trimming amod_rules_linguistically-motivated det_rules_These ccomp_``_rules
I05-2027	W04-1013	o	-LRB- 2004 -RRB- In this example we can see that after compression the lead sentence reads 156 more like a headline	det_headline_a prep_like_more_headline num_more_156 dobj_reads_more nsubj_reads_sentence dep_reads_compression mark_reads_after amod_sentence_lead det_sentence_the advcl_that_reads dep_see_that aux_see_can nsubj_see_we dep_see_2004 det_example_this prep_in_2004_example
I05-2027	W04-1013	o	It was also included in the DUC 2004 evaluation plan where summary quality was automatically judged using a set of n-gram word overlap metrics called ROUGE -LRB- Lin and Hovy 2003 -RRB-	amod_Lin_2003 conj_and_Lin_Hovy dep_ROUGE_Hovy dep_ROUGE_Lin dep_called_ROUGE vmod_metrics_called dobj_overlap_metrics nn_word_n-gram prep_of_set_word det_set_a dobj_using_set xcomp_judged_using advmod_judged_automatically auxpass_judged_was nsubjpass_judged_quality advmod_judged_where nn_quality_summary rcmod_plan_judged nn_plan_evaluation num_plan_2004 nn_plan_DUC det_plan_the dep_included_overlap prep_in_included_plan advmod_included_also auxpass_included_was nsubjpass_included_It
I05-2027	W04-1013	o	ROUGE-LCS calculated the longest common 2 Details of our official DUC 2004 headline generation system can be found in Doran et al.	nn_al._et nn_al._Doran prep_in_found_al. auxpass_found_be aux_found_can nsubjpass_found_Details nn_system_generation nn_system_headline num_system_2004 nn_system_DUC amod_system_official poss_system_our prep_of_Details_system num_Details_2 amod_Details_common amod_Details_longest det_Details_the ccomp_calculated_found nsubj_calculated_ROUGE-LCS
I08-1035	W04-1013	o	4.2 A ROUGE Based Approach ROUGE -LRB- Lin 2004 -RRB- is the standard automatic evaluation metric in the Summarization community	nn_community_Summarization det_community_the prep_in_metric_community nn_metric_evaluation amod_metric_automatic amod_metric_standard det_metric_the cop_metric_is nsubj_metric_ROUGE amod_Lin_2004 appos_ROUGE_Lin nn_ROUGE_Approach ccomp_Based_metric nsubj_Based_ROUGE det_ROUGE_A num_ROUGE_4.2
I08-1035	W04-1013	o	We experimented with two independent arguably complementary techniques for clustering and aligning a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE -LRB- Lin 2004 -RRB- based 265 approach that can extract templates containing multiple verbs	amod_verbs_multiple dobj_containing_verbs vmod_templates_containing dobj_extract_templates aux_extract_can nsubj_extract_that num_approach_265 amod_approach_based nn_approach_ROUGE dep_Lin_2004 appos_ROUGE_Lin det_ROUGE_a rcmod_predicate_extract conj_and_predicate_approach num_predicate_one dobj_containing_approach dobj_containing_predicate vmod_templates_containing amod_templates_general advmod_templates_more dobj_extracts_templates nsubj_extracts_that rcmod_approach_extracts amod_approach_based dep_argument_approach nn_argument_predicate det_argument_a dobj_aligning_argument conj_and_clustering_aligning prep_for_techniques_aligning prep_for_techniques_clustering amod_techniques_complementary amod_techniques_independent num_techniques_two advmod_complementary_arguably prep_with_experimented_techniques nsubj_experimented_We
I08-1063	W04-1013	o	We have also used ROUGE evaluation approach -LRB- Lin 2004 -RRB- which is based on n-gram co-occurrences between machine summaries and ideal human summaries	amod_summaries_human amod_summaries_ideal conj_and_summaries_summaries nn_summaries_machine prep_between_co-occurrences_summaries prep_between_co-occurrences_summaries nn_co-occurrences_n-gram prep_on_based_co-occurrences auxpass_based_is nsubjpass_based_which dep_Lin_2004 rcmod_approach_based appos_approach_Lin nn_approach_evaluation nn_approach_ROUGE dobj_used_approach advmod_used_also aux_used_have nsubj_used_We ccomp_``_used
L08-1063	W04-1013	o	ACM Transactions on Computer-Human Interaction -LRB- TOCHI -RRB- 11 -LRB- 3 -RRB- </rawString> </citation> < citation valid = true > <authors> <author> M E Pollack </author> </authors> <title> Intelligent technology for an aging population The use of AI to assist elders with cognitive impairment </title> <date> 2005 </date> <journal> AI Magazine </journal> <pages> 26 -- 2 </pages> <contexts> <context> rch on developing SDS for home-care and tele-care applications Examples include scheduling appointments over the phone -LRB- Zajicek et al. 2004 Wolters et al. submitted -RRB- interactive reminder systems -LRB- Pollack 2005 -RRB- symptom management systems -LRB- Black et al. 2005 -RRB- or environmental control systems -LRB- Clarke et al. 2005 -RRB-	dep_2005_al. num_Clarke_2005 nn_Clarke_et nn_systems_control amod_systems_environmental num_al._2005 nn_al._et amod_al._Black dep_systems_al. nn_systems_management nn_systems_symptom amod_Pollack_2005 dep_systems_Pollack nn_systems_reminder amod_systems_interactive nn_al._et nn_al._Wolters num_al._2004 amod_Zajicek_submitted dep_Zajicek_al. dep_Zajicek_al. nn_Zajicek_et det_phone_the conj_or_appointments_systems conj_or_appointments_systems conj_or_appointments_systems dep_appointments_Zajicek prep_over_appointments_phone nn_appointments_scheduling dobj_include_systems dobj_include_systems dobj_include_systems dobj_include_appointments nsubj_include_Examples dep_include_<pages> amod_applications_tele-care amod_applications_home-care conj_and_home-care_tele-care amod_SDS_developing prep_for_rch_applications prep_on_rch_SDS nn_rch_<context> nn_rch_<contexts> nn_rch_</pages> num_rch_2 dep_<pages>_rch num_<pages>_26 nn_<pages>_</journal> nn_<pages>_Magazine nn_<pages>_AI nn_<pages>_<journal> amod_<pages>_</date> num_<pages>_2005 dep_<date>_Clarke ccomp_<date>_include nsubj_<date>_use nn_</title>_impairment amod_</title>_cognitive prep_with_assist_</title> dobj_assist_elders aux_assist_to vmod_use_assist prep_of_use_AI det_use_The amod_population_aging det_population_an dep_technology_<date> prep_for_technology_population amod_technology_Intelligent nn_technology_<title> nn_technology_</authors> nn_technology_</author> nn_technology_Pollack nn_technology_E nn_technology_M nn_technology_<author> nn_technology_<authors> amod_technology_> amod_technology_true dep_true_= dep_true_valid dep_true_citation dep_true_< dep_true_</citation> dep_true_</rawString> dep_true_Transactions dep_11_3 appos_Interaction_11 appos_Interaction_TOCHI nn_Interaction_Computer-Human prep_on_Transactions_Interaction nn_Transactions_ACM
L08-1063	W04-1013	o	Oxford UK Oxford University Press </rawString> </citation> < citation valid = true > <authors> <author> L A Black </author> <author> C McMeel </author> <author> M McTear </author> <author> N Black </author> <author> R Harper </author> <author> M Lemon </author> </authors> <title> Implementing autonomy in a diabetes management system </title> <date> 2005 </date> <journal> J Telemed Telecare </journal> <volume> 11 </volume> <contexts> <context> care applications Examples include scheduling appointments over the phone -LRB- Zajicek et al. 2004 Wolters et al. submitted -RRB- interactive reminder systems -LRB- Pollack 2005 -RRB- symptom management systems -LRB- Black et al. 2005 -RRB- or environmental control systems -LRB- Clarke et al. 2005 -RRB-	dep_2005_al. num_Clarke_2005 nn_Clarke_et nn_systems_control amod_systems_environmental num_al._2005 nn_al._et amod_al._Black dep_systems_al. nn_systems_management nn_systems_symptom amod_Pollack_2005 appos_systems_Clarke conj_or_systems_systems conj_or_systems_systems dep_systems_Pollack nn_systems_reminder amod_systems_interactive nn_al._et nn_al._Wolters advmod_2004_al. nn_al._et vmod_Zajicek_submitted conj_Zajicek_al. num_Zajicek_2004 det_phone_the prep_over_appointments_phone nn_appointments_scheduling dep_include_Zajicek dobj_include_appointments nsubj_include_Examples nn_applications_care dep_<context>_systems dep_<context>_systems dep_<context>_systems rcmod_<context>_include dep_<context>_applications nn_<context>_<contexts> nn_<context>_</volume> num_<context>_11 nn_<context>_<volume> nn_<context>_</journal> nn_<context>_Telecare nn_<context>_Telemed nn_<context>_J nn_<context>_<journal> nn_<context>_</date> num_<context>_2005 amod_<context>_<date> amod_<context>_</title> dep_system_<context> nn_system_management nn_system_diabetes det_system_a prep_in_autonomy_system amod_autonomy_Implementing nn_autonomy_<title> nn_autonomy_</authors> nn_autonomy_</author> nn_autonomy_Lemon nn_autonomy_M nn_autonomy_<author> nn_autonomy_</author> nn_autonomy_Harper nn_autonomy_R nn_autonomy_<author> nn_autonomy_</author> amod_autonomy_Black nn_autonomy_N nn_autonomy_<author> nn_autonomy_</author> nn_autonomy_McTear nn_autonomy_M nn_autonomy_<author> nn_autonomy_</author> nn_autonomy_McMeel nn_autonomy_C nn_autonomy_<author> nn_autonomy_</author> amod_autonomy_Black nn_autonomy_A nn_autonomy_L nn_autonomy_<author> nn_autonomy_<authors> amod_autonomy_> amod_autonomy_true dep_true_= dep_true_valid dep_true_citation dep_true_< dep_true_</citation> dep_true_</rawString> dep_true_Press dep_true_Oxford nn_Press_University nn_Press_Oxford appos_Oxford_UK
N06-2006	W04-1013	o	3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm -LRB- Lin 2004 -RRB- is also used for evaluating results	dobj_evaluating_results prepc_for_used_evaluating advmod_used_also auxpass_used_is nsubjpass_used_Version num_Lin_2004 amod_algorithm_scoring nn_algorithm_ROUGE det_algorithm_the appos_1.5.5_Lin prep_of_1.5.5_algorithm dep_Version_1.5.5 nn_Version_ROUGE num_Version_3.2
N07-1005	W04-1013	o	Many methods for calculating the similarity have been proposed -LRB- Niessen et al. 2000 Akiba et al. 2001 Papineni et al. 2002 NIST 2002 Leusch et al. 2003 Turian et al. 2003 Babych and Hartley 2004 Lin and Och 2004 Banerjee and Lavie 2005 Gimenez et al. 2005 -RRB-	num_Gimenez_2005 nn_Gimenez_al. nn_Gimenez_et num_Lin_2004 conj_and_Lin_Och dep_Babych_Gimenez conj_and_Babych_2005 conj_and_Babych_Lavie conj_and_Babych_Banerjee conj_and_Babych_Och conj_and_Babych_Lin conj_and_Babych_2004 conj_and_Babych_Hartley num_Turian_2003 nn_Turian_al. nn_Turian_et num_Leusch_2003 nn_Leusch_al. nn_Leusch_et num_NIST_2002 num_Papineni_2002 nn_Papineni_al. nn_Papineni_et conj_Akiba_2005 conj_Akiba_Lavie conj_Akiba_Banerjee conj_Akiba_Lin conj_Akiba_2004 conj_Akiba_Hartley conj_Akiba_Babych conj_Akiba_Turian conj_Akiba_Leusch conj_Akiba_NIST conj_Akiba_Papineni num_Akiba_2001 nn_Akiba_al. nn_Akiba_et dep_Niessen_Akiba appos_Niessen_2000 dep_Niessen_al. nn_Niessen_et dep_proposed_Niessen auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods det_similarity_the dobj_calculating_similarity prepc_for_methods_calculating amod_methods_Many ccomp_``_proposed
N07-1005	W04-1013	o	In our research 23 scores namely BLEU -LRB- Papineni et al. 2002 -RRB- with maximum n-gram lengths of 1 2 3 and 4 NIST -LRB- NIST 2002 -RRB- with maximum n-gram lengths of 1 2 3 4 and 5 GTM -LRB- Turian et al. 2003 -RRB- with exponents of 1.0 2.0 and 3.0 METEOR -LRB- exact -RRB- -LRB- Banerjee and Lavie 2005 -RRB- WER -LRB- Niessen et al. 2000 -RRB- PER -LRB- Leusch et al. 2003 -RRB- and ROUGE -LRB- Lin 2004 -RRB- with n-gram lengths of 1 2 3 and 4 and 4 variants -LRB- LCS S SU W-1 .2 -RRB- were used to calculate each similarity S i Therefore the value of m in Eq	prep_in_value_Eq prep_of_value_m det_value_the dep_Therefore_value dep_i_Therefore nn_i_S dep_similarity_i det_similarity_each dobj_calculate_similarity aux_calculate_to xcomp_used_calculate auxpass_used_were nsubjpass_used_scores prep_in_used_research nn_.2_W-1 appos_LCS_.2 conj_LCS_SU conj_LCS_S dep_variants_LCS num_variants_4 num_variants_4 num_variants_3 num_variants_2 num_variants_1 conj_and_1_4 conj_and_1_4 conj_and_1_3 conj_and_1_2 prep_of_lengths_variants nn_lengths_n-gram num_Lin_2004 prep_with_ROUGE_lengths appos_ROUGE_Lin amod_Leusch_2003 dep_Leusch_al. nn_Leusch_et appos_PER_Leusch amod_Niessen_2000 dep_Niessen_al. nn_Niessen_et dep_WER_Niessen dep_Banerjee_2005 conj_and_Banerjee_Lavie nn_Banerjee_METEOR appos_METEOR_exact conj_and_1.0_3.0 conj_and_1.0_2.0 prep_of_exponents_3.0 prep_of_exponents_2.0 prep_of_exponents_1.0 amod_Turian_2003 dep_Turian_al. nn_Turian_et prep_with_GTM_exponents dep_GTM_Turian conj_and_1_5 conj_and_1_4 conj_and_1_3 conj_and_1_2 prep_of_lengths_5 prep_of_lengths_4 prep_of_lengths_3 prep_of_lengths_2 prep_of_lengths_1 nn_lengths_n-gram nn_lengths_maximum dep_NIST_2002 prep_with_NIST_lengths dep_NIST_NIST conj_and_1_4 conj_and_1_3 conj_and_1_2 prep_of_lengths_4 prep_of_lengths_3 prep_of_lengths_2 prep_of_lengths_1 nn_lengths_n-gram nn_lengths_maximum dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_ROUGE conj_and_BLEU_PER conj_and_BLEU_WER dep_BLEU_Lavie dep_BLEU_Banerjee dep_BLEU_GTM dep_BLEU_NIST prep_with_BLEU_lengths appos_BLEU_Papineni advmod_BLEU_namely appos_scores_ROUGE appos_scores_PER appos_scores_WER appos_scores_BLEU num_scores_23 poss_research_our
N07-1005	W04-1013	o	In recent years many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations -LRB- Niessen et al. 2000 Akiba et al. 2001 Papineni et al. 2002 NIST 2002 Leusch et al. 2003 Turian et al. 2003 Babych and Hartley 2004 Lin and Och 2004 Banerjee and Lavie 2005 Gimenez et al. 2005 -RRB- because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently	nn_systems_MT advmod_use_efficiently dobj_use_systems conj_and_use_improve aux_use_to xcomp_enable_improve xcomp_enable_use dobj_enable_us aux_enable_to xcomp_expected_enable auxpass_expected_is nsubjpass_expected_improving mark_expected_because nn_evaluation_MT amod_evaluation_automatic prep_of_performance_evaluation det_performance_the dobj_improving_performance num_Gimenez_2005 nn_Gimenez_al. nn_Gimenez_et conj_and_Banerjee_Lavie num_Lin_2004 conj_and_Lin_Och conj_and_Babych_2004 conj_and_Babych_Hartley num_Turian_2003 nn_Turian_al. nn_Turian_et num_Leusch_2003 nn_Leusch_al. nn_Leusch_et num_NIST_2002 num_Papineni_2002 nn_Papineni_al. nn_Papineni_et num_Akiba_2001 nn_Akiba_al. nn_Akiba_et dep_Niessen_Gimenez dep_Niessen_2005 dep_Niessen_Lavie dep_Niessen_Banerjee dep_Niessen_Och dep_Niessen_Lin dep_Niessen_2004 dep_Niessen_Hartley dep_Niessen_Babych dep_Niessen_Turian dep_Niessen_Leusch dep_Niessen_NIST dep_Niessen_Papineni dep_Niessen_Akiba dep_Niessen_2000 dep_Niessen_al. nn_Niessen_et nn_evaluations_MT amod_evaluations_automatic prep_of_performance_evaluations det_performance_the dobj_improve_performance prep_of_quality_MT det_quality_the conj_and_evaluate_improve dobj_evaluate_quality advmod_evaluate_automatically aux_evaluate_to advcl_tried_expected dep_tried_Niessen ccomp_tried_improve ccomp_tried_evaluate aux_tried_have nsubj_tried_researchers prep_in_tried_years amod_researchers_many amod_years_recent
N09-1041	W04-1013	o	2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content simple word frequency-based methods -LRB- Luhn 1958 Nenkova and Vanderwende 2005 -RRB- graph-based approaches -LRB- Radev 2004 Wan and Yang 2006 -RRB- as well as more linguistically motivated techniques -LRB- Mckeown et al. 1999 Leskovec et al. 2005 Harabagiu et al. 2007 -RRB-	num_Harabagiu_2007 nn_Harabagiu_al. nn_Harabagiu_et dep_Leskovec_Harabagiu num_Leskovec_2005 nn_Leskovec_al. nn_Leskovec_et dep_Mckeown_Leskovec appos_Mckeown_1999 dep_Mckeown_al. nn_Mckeown_et dep_techniques_Mckeown amod_techniques_motivated amod_techniques_more advmod_motivated_linguistically amod_Wan_2006 conj_and_Wan_Yang dep_Radev_Yang dep_Radev_Wan appos_Radev_2004 appos_approaches_Radev amod_approaches_graph-based dep_Nenkova_2005 conj_and_Nenkova_Vanderwende dep_Luhn_Vanderwende dep_Luhn_Nenkova appos_Luhn_1958 conj_and_methods_techniques appos_methods_approaches appos_methods_Luhn amod_methods_frequency-based nn_methods_word amod_methods_simple nn_content_document nn_content_modeling dep_approaches_techniques dep_approaches_methods prep_to_approaches_content amod_approaches_several nsubj_are_approaches expl_are_There amod_There_coherent rcmod_a_are dobj_form_a aux_form_to nn_sentences_summary xcomp_ordering_form dobj_ordering_sentences nsubj_ordering_extraction prep_of_problem_selecting det_problem_the conj_and_solve_ordering dobj_solve_problem neg_solve_not aux_solve_does nsubj_solve_extraction nn_extraction_sentence det_extraction_that rcmod_2Note_ordering rcmod_2Note_solve
N09-1041	W04-1013	o	All topic models utilize Gibbs sampling for inference -LRB- Griffiths 2002 Blei et al. 2004 -RRB-	num_Blei_2004 nn_Blei_al. nn_Blei_et dep_Griffiths_Blei dep_Griffiths_2002 appos_inference_Griffiths prep_for_sampling_inference nn_sampling_Gibbs dobj_utilize_sampling nsubj_utilize_models nn_models_topic det_models_All
N09-1041	W04-1013	o	Automated evaluation will utilize the standard DUC evaluation metric ROUGE -LRB- Lin 2004 -RRB- which representsrecallovervariousn-gramsstatisticsfrom asystem-generatedsummaryagainstasetofhumangenerated peer summaries .5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries	dobj_peer_summaries conj_and_peer_proposed prep_from_removed_proposed prep_from_removed_peer vmod_words_removed nn_words_stop nn_scores_ROUGE prep_without_compute_words prep_with_compute_words dobj_compute_scores conj_and_compute_compute nsubj_compute_We nsubj_compute_We rcmod_.5_compute rcmod_.5_compute dep_summaries_.5 dobj_peer_summaries dep_asystem-generatedsummaryagainstasetofhumangenerated_peer amod_asystem-generatedsummaryagainstasetofhumangenerated_representsrecallovervariousn-gramsstatisticsfrom dep_which_asystem-generatedsummaryagainstasetofhumangenerated amod_Lin_2004 dep_ROUGE_which dep_ROUGE_Lin amod_ROUGE_metric nn_ROUGE_evaluation nn_ROUGE_DUC amod_ROUGE_standard det_ROUGE_the dobj_utilize_ROUGE aux_utilize_will nsubj_utilize_evaluation amod_evaluation_Automated
N09-1041	W04-1013	o	Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_resampling_Lin dobj_bootstrapping_resampling xcomp_using_bootstrapping vmod_assesses_using dobj_assesses_significance nsubj_assesses_DUC nn_procedure_jackknife det_procedure_the conj_and_utilizes_assesses dobj_utilizes_procedure nsubj_utilizes_DUC vmod_DUC_scoring nn_DUC_Official
N09-1041	W04-1013	o	8This result is presented as 0.053 with the official ROUGE scorer -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_scorer_Lin nn_scorer_ROUGE amod_scorer_official det_scorer_the prep_with_presented_scorer prep_as_presented_0.053 auxpass_presented_is nsubjpass_presented_result nn_result_8This
N09-1066	W04-1013	o	6.1.1 Nugget-Based Pyramid Evaluation For our first approach we used a nugget-based evaluation methodology -LRB- Lin and Demner-Fushman 2006 Nenkova and Passonneau 2004 Hildebrandt et al. 2004 Voorhees 2003 -RRB-	amod_Voorhees_2003 num_Hildebrandt_2004 nn_Hildebrandt_al. nn_Hildebrandt_et num_Nenkova_2004 conj_and_Nenkova_Passonneau dep_Lin_Voorhees conj_and_Lin_Hildebrandt conj_and_Lin_Passonneau conj_and_Lin_Nenkova conj_and_Lin_2006 conj_and_Lin_Demner-Fushman dep_methodology_Hildebrandt dep_methodology_Nenkova dep_methodology_2006 dep_methodology_Demner-Fushman dep_methodology_Lin nn_methodology_evaluation amod_methodology_nugget-based det_methodology_a dobj_used_methodology nsubj_used_we amod_approach_first poss_approach_our rcmod_Evaluation_used prep_for_Evaluation_approach nn_Evaluation_Pyramid amod_Evaluation_Nugget-Based num_Evaluation_6.1.1
N09-1066	W04-1013	o	6.1.2 ROUGE evaluation Table 4 presents ROUGE scores -LRB- Lin 2004 -RRB- of each of human-generated 250-word surveys against each other	det_other_each prep_against_surveys_other amod_surveys_250-word amod_surveys_human-generated prep_of_each_surveys dep_Lin_2004 prep_of_scores_each appos_scores_Lin nn_scores_ROUGE dobj_presents_scores nsubj_presents_Table num_Table_4 nn_Table_evaluation nn_Table_ROUGE num_Table_6.1.2
N09-1066	W04-1013	o	Our aim is not only to determine the utility of citation texts for survey creation but also to examine the quality distinctions between this form of input and others such as abstracts and full textscomparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation -LRB- Lin and Demner-Fushman 2006 Nenkova and Passonneau 2004 Lin 2004 -RRB-	num_Lin_2004 num_Nenkova_2004 conj_and_Nenkova_Passonneau dep_Lin_Lin conj_and_Lin_Passonneau conj_and_Lin_Nenkova num_Lin_2006 conj_and_Lin_Demner-Fushman dep_evaluation_Nenkova dep_evaluation_Demner-Fushman dep_evaluation_Lin nn_evaluation_pyramid amod_evaluation_nugget-based amod_evaluation_automatic conj_and_automatic_nugget-based preconj_automatic_both dobj_using_evaluation vmod_surveys_using amod_surveys_human-generated prep_to_results_surveys det_results_the dobj_textscomparing_results vmod_full_textscomparing conj_and_abstracts_full conj_and_input_others prep_of_form_others prep_of_form_input det_form_this prep_such_as_distinctions_full prep_such_as_distinctions_abstracts prep_between_distinctions_form nn_distinctions_quality det_distinctions_the dobj_examine_distinctions aux_examine_to nn_creation_survey nn_texts_citation prep_for_utility_creation prep_of_utility_texts det_utility_the conj_and_determine_examine dobj_determine_utility aux_determine_to preconj_determine_only neg_only_not ccomp_is_examine ccomp_is_determine nsubj_is_aim poss_aim_Our ccomp_``_is
P04-1077	W04-1013	p	ROUGE-L ROUGE-W and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results -LRB- Lin 2004 -RRB-	num_Lin_2004 appos_results_Lin amod_results_promising advmod_promising_very dobj_achieved_results nsubjpass_achieved_ROUGE-L prep_of_evaluation_summarization amod_evaluation_automatic conj_and_applied_achieved prep_in_applied_evaluation auxpass_applied_been advmod_applied_also aux_applied_have nsubjpass_applied_ROUGE-S nsubjpass_applied_ROUGE-W nsubjpass_applied_ROUGE-L conj_and_ROUGE-L_ROUGE-S conj_and_ROUGE-L_ROUGE-W
P04-1077	W04-1013	o	In Lin and Och -LRB- 2004 -RRB- we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement	amod_involvement_human amod_involvement_further amod_translations_manual advmod_translations_only prep_without_using_involvement dobj_using_translations nn_metrics_evaluation nn_metrics_MT amod_metrics_automatic xcomp_evaluated_using dobj_evaluated_metrics advmod_evaluated_automatically nsubj_evaluated_that rcmod_framework_evaluated det_framework_a dobj_proposed_framework nsubj_proposed_we prep_in_proposed_Och prep_in_proposed_Lin appos_Och_2004 conj_and_Lin_Och
P06-1139	W04-1013	o	This evaluation shows that our WIDL-based approach to generation is capable of obtaining headlines that compare favorably in both content and fluency with extractive state-of-the-art results -LRB- Zajic et al. 2004 -RRB- while it outperforms a previously-proposed abstractive system by a wide margin -LRB- Zhou and Hovy 2003 -RRB-	amod_Zhou_2003 conj_and_Zhou_Hovy dep_margin_Hovy dep_margin_Zhou amod_margin_wide det_margin_a amod_system_abstractive amod_system_previously-proposed det_system_a prep_by_outperforms_margin dobj_outperforms_system nsubj_outperforms_it mark_outperforms_while amod_Zajic_2004 dep_Zajic_al. nn_Zajic_et amod_results_state-of-the-art amod_results_extractive conj_and_content_fluency preconj_content_both advmod_compare_favorably nsubj_compare_that rcmod_headlines_compare dobj_obtaining_headlines advcl_capable_outperforms dep_capable_Zajic prep_with_capable_results prep_in_capable_fluency prep_in_capable_content prepc_of_capable_obtaining cop_capable_is nsubj_capable_approach mark_capable_that prep_to_approach_generation amod_approach_WIDL-based poss_approach_our ccomp_shows_capable nsubj_shows_evaluation det_evaluation_This
P06-1139	W04-1013	o	When evaluated against the state-of-the-art phrase-based decoder Pharaoh -LRB- Koehn 2004 -RRB- using the same experimental conditions translation table trained on the FBIS corpus -LRB- 7.2 M Chinese words and 9.2 M English words of parallel text -RRB- trigram language model trained on 155M words of English newswire interpolation weights a65 -LRB- Equation 2 -RRB- trained using discriminative training -LRB- Och 2003 -RRB- -LRB- on the 2002 NIST MT evaluation set -RRB- probabilistic beam a90 set to 0.01 histogram beam a58 set to 10 and BLEU -LRB- Papineni et al. 2002 -RRB- as our metric the WIDL-NGLM-Aa86 a129 algorithm produces translations that have a BLEU score of 0.2570 while Pharaoh translations have a BLEU score of 0.2635	prep_of_score_0.2635 nn_score_BLEU det_score_a dobj_have_score nsubj_have_translations mark_have_while nn_translations_Pharaoh prep_of_score_0.2570 nn_score_BLEU det_score_a dobj_have_score nsubj_have_that rcmod_translations_have advcl_produces_have dobj_produces_translations nsubj_produces_algorithm nn_algorithm_a129 nn_algorithm_WIDL-NGLM-Aa86 det_algorithm_the poss_metric_our amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_10_BLEU prep_to_set_BLEU prep_to_set_10 prep_as_a58_metric appos_a58_Papineni vmod_a58_set nn_a58_beam nn_a58_histogram prep_to_set_0.01 vmod_a90_set nn_a90_beam amod_a90_probabilistic nn_set_evaluation nn_set_MT nn_set_NIST num_set_2002 det_set_the pobj_on_set dep_Och_2003 appos_training_Och amod_training_discriminative dep_using_produces dobj_using_a58 conj_using_a90 dep_using_on dobj_using_training prep_trained_using num_Equation_2 vmod_a65_trained appos_a65_Equation nn_a65_weights nn_a65_interpolation nn_newswire_English prep_of_words_newswire nn_words_155M prep_on_trained_words dep_model_a65 vmod_model_trained nn_model_language nn_model_trigram amod_text_parallel prep_of_words_text nn_words_English dep_words_M num_M_9.2 conj_and_words_words amod_words_Chinese nn_words_M num_words_7.2 dep_corpus_words dep_corpus_words nn_corpus_FBIS det_corpus_the prep_on_trained_corpus vmod_table_trained nn_table_translation nn_table_conditions amod_table_experimental amod_table_same det_table_the parataxis_using_model dobj_using_table ccomp_,_using dep_Koehn_2004 dep_Pharaoh_Koehn nn_Pharaoh_decoder amod_Pharaoh_phrase-based amod_Pharaoh_state-of-the-art det_Pharaoh_the prep_against_evaluated_Pharaoh advmod_evaluated_When advcl_``_evaluated
P06-1139	W04-1013	o	We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGEa129 -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_ROUGEa129_Lin dobj_using_ROUGEa129 xcomp_human_using amod_a_human agent_produced_a vmod_headline_produced nn_headline_reference num_headline_one amod_headlines_produced det_headlines_the prep_against_comparing_headline dobj_comparing_headlines prepc_by_measure_comparing dobj_measure_performance advmod_measure_automatically nsubj_measure_We ccomp_``_measure
P06-2078	W04-1013	o	using Spearmans rank correlation coefficient and Pearsons rank correlation coefficient -LRB- Lin et al. 2003 Lin 2004 Hirao et al. 2005 -RRB-	nn_Hirao_al. nn_Hirao_et amod_Lin_2005 appos_Lin_Hirao num_Lin_2004 appos_Lin_Lin num_Lin_2003 dep_Lin_al. nn_Lin_et appos_coefficient_Lin nn_coefficient_correlation dobj_rank_coefficient conj_and_coefficient_Pearsons nn_coefficient_correlation nn_coefficient_rank amod_coefficient_Spearmans dep_using_rank dobj_using_Pearsons dobj_using_coefficient ccomp_``_using
P06-2078	W04-1013	o	-LRB- Donaway et al. 2000 Hirao et al. 2005 Lin et al. 2003 Lin 2004 Hori et al. 2003 -RRB- and manual methods	amod_methods_manual nn_Hori_al. nn_Hori_et num_Lin_2003 conj_Lin_Hori num_Lin_2004 nn_Lin_al. nn_Lin_et nn_Hirao_al. nn_Hirao_et conj_and_Donaway_methods dep_Donaway_Lin amod_Donaway_2003 appos_Donaway_Lin appos_Donaway_2005 appos_Donaway_Hirao amod_Donaway_2000 dep_Donaway_al. nn_Donaway_et
P06-2078	W04-1013	o	We also tested other automatic methods content-based evaluation BLEU -LRB- Papineni et al. 2001 -RRB- and ROUGE-1 -LRB- Lin 2004 -RRB- and compared their results with that of evaluation by revision as reference	prep_of_that_evaluation prep_with_results_that poss_results_their pobj_compared_results dep_Lin_2004 appos_ROUGE-1_Lin amod_Papineni_2001 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_as_evaluation_reference prep_by_evaluation_revision conj_and_evaluation_compared conj_and_evaluation_ROUGE-1 conj_and_evaluation_BLEU amod_evaluation_content-based dep_methods_compared dep_methods_ROUGE-1 dep_methods_BLEU dep_methods_evaluation amod_methods_automatic amod_methods_other dobj_tested_methods advmod_tested_also nsubj_tested_We ccomp_``_tested
P06-2078	W04-1013	o	We tested several measures such as ROUGE -LRB- Lin 2004 -RRB- and the cosine distance	nn_distance_cosine det_distance_the amod_Lin_2004 conj_and_ROUGE_distance dep_ROUGE_Lin prep_such_as_measures_distance prep_such_as_measures_ROUGE amod_measures_several dobj_tested_measures nsubj_tested_We ccomp_``_tested
P06-2078	W04-1013	o	ROUGE-N -LRB- Lin 2004 -RRB- This measure compares n-grams of two summaries and counts the number of matches	prep_of_number_matches det_number_the dobj_counts_number nsubj_counts_measure num_summaries_two prep_of_n-grams_summaries conj_and_compares_counts dobj_compares_n-grams nsubj_compares_measure det_measure_This nn_measure_ROUGE-N amod_Lin_2004 dep_ROUGE-N_Lin
P06-2078	W04-1013	o	ROUGE-L -LRB- Lin 2004 -RRB- This measure evaluates summaries by longest common subsequence -LRB- LCS -RRB- defined by Equation 4	num_Equation_4 agent_defined_Equation vmod_subsequence_defined appos_subsequence_LCS amod_subsequence_common amod_subsequence_longest prep_by_summaries_subsequence dobj_evaluates_summaries nsubj_evaluates_measure dep_evaluates_ROUGE-L det_measure_This amod_Lin_2004 dep_ROUGE-L_Lin
P06-2078	W04-1013	o	605 ROUGE-S -LRB- Lin 2004 -RRB- Skip-bigram is any pair of words in their sentence order allowing for arbitrary gaps	amod_gaps_arbitrary prep_for_allowing_gaps nn_order_sentence poss_order_their vmod_pair_allowing prep_in_pair_order prep_of_pair_words det_pair_any cop_pair_is nsubj_pair_Skip-bigram nn_Skip-bigram_ROUGE-S dep_Lin_2004 appos_ROUGE-S_Lin num_ROUGE-S_605
P06-2078	W04-1013	o	In the following ROUGE-SN denotes ROUGE-S with maximum skip distance N. ROUGE-SU -LRB- Lin 2004 -RRB- This measure is an extension of ROUGE-S it adds a unigram as a counting unit	amod_unit_counting det_unit_a det_unigram_a prep_as_adds_unit dobj_adds_unigram nsubj_adds_it prep_of_extension_ROUGE-S det_extension_an parataxis_is_adds nsubj_is_extension prep_in_is_the det_measure_This nn_measure_ROUGE-SU amod_Lin_2004 appos_ROUGE-SU_Lin nn_ROUGE-SU_N. nn_ROUGE-SU_distance dobj_skip_measure nsubj_skip_ROUGE-S prep_with_ROUGE-S_maximum ccomp_denotes_skip nsubj_denotes_ROUGE-SN parataxis_following_denotes vmod_the_following
P06-2109	W04-1013	o	ROUGE -LRB- Lin 2004 -RRB- is a set of recall-based criteria that is mainly used for evaluating summarization tasks	nn_tasks_summarization dobj_evaluating_tasks prepc_for_used_evaluating advmod_used_mainly auxpass_used_is nsubjpass_used_that rcmod_criteria_used amod_criteria_recall-based prep_of_set_criteria det_set_a cop_set_is nsubj_set_ROUGE amod_Lin_2004 appos_ROUGE_Lin
P06-2109	W04-1013	o	ROUGE-L and ROUGE-1 are supposed to be appropriate for the headline gener853 ation task -LRB- Lin 2004 -RRB-	amod_Lin_2004 appos_task_Lin nn_task_ation nn_task_gener853 nn_task_headline det_task_the prep_for_appropriate_task cop_appropriate_be aux_appropriate_to xcomp_supposed_appropriate auxpass_supposed_are nsubjpass_supposed_ROUGE-1 nsubjpass_supposed_ROUGE-L conj_and_ROUGE-L_ROUGE-1
P07-2049	W04-1013	o	For evaluation we use ROUGE -LRB- Lin 2004 -RRB- SU4 recall metric1 which was among the official automatic evaluation metrics for DUC	prep_for_metrics_DUC nn_metrics_evaluation amod_metrics_automatic amod_metrics_official det_metrics_the prep_among_was_metrics nsubj_was_which rcmod_metric1_was nn_metric1_recall nn_metric1_SU4 nn_metric1_ROUGE dep_Lin_2004 dep_ROUGE_Lin dobj_use_metric1 nsubj_use_we prep_for_use_evaluation
P08-1094	W04-1013	o	The idea of topic signature terms was introduced by Lin and Hovy -LRB- Lin and Hovy 2000 -RRB- in the context of single document summarization and was later used in several multi-document summarization systems -LRB- Conroy et al. 2006 Lacatusu et al. 2004 Gupta et al. 2007 -RRB-	num_Gupta_2007 nn_Gupta_al. nn_Gupta_et num_Lacatusu_2004 nn_Lacatusu_al. nn_Lacatusu_et dep_Conroy_Gupta conj_Conroy_Lacatusu appos_Conroy_2006 dep_Conroy_al. nn_Conroy_et dep_systems_Conroy nn_systems_summarization amod_systems_multi-document amod_systems_several prep_in_used_systems advmod_used_later auxpass_used_was nsubjpass_used_idea nn_summarization_document amod_summarization_single prep_of_context_summarization det_context_the num_Lin_2000 conj_and_Lin_Hovy appos_Lin_Hovy appos_Lin_Lin conj_and_Lin_Hovy conj_and_introduced_used prep_in_introduced_context agent_introduced_Hovy agent_introduced_Lin auxpass_introduced_was nsubjpass_introduced_idea nn_terms_signature nn_terms_topic prep_of_idea_terms det_idea_The
P08-1094	W04-1013	p	The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores -LRB- Lin and Hovy 2003a Lin 2004 -RRB-	num_Lin_2004 dep_Lin_Lin conj_and_Lin_2003a conj_and_Lin_Hovy dep_scores_2003a dep_scores_Hovy dep_scores_Lin nn_scores_coverage nn_scores_DUC amod_scores_manual det_scores_the prep_with_correlated_scores advmod_correlated_highly auxpass_correlated_is nsubjpass_correlated_it ccomp_demonstrated_correlated auxpass_demonstrated_was nsubjpass_demonstrated_it mark_demonstrated_because advcl_adopted_demonstrated advmod_adopted_exactly auxpass_adopted_was csubjpass_adopted_used nn_ROUGE_evaluation amod_ROUGE_automatic prep_for_tool_ROUGE dobj_used_tool advmod_used_routinely nsubj_used_dif1The det_dif1The_The
P08-2003	W04-1013	p	We carried out automatic evaluation of our summaries using ROUGE -LRB- Lin 2004 -RRB- toolkit which has been widely adopted by DUC for automatic summarization evaluation	nn_evaluation_summarization amod_evaluation_automatic prep_for_adopted_evaluation agent_adopted_DUC advmod_adopted_widely auxpass_adopted_been aux_adopted_has nsubjpass_adopted_which rcmod_toolkit_adopted nn_toolkit_ROUGE dep_Lin_2004 dep_ROUGE_Lin dobj_using_toolkit vmod_summaries_using poss_summaries_our prep_of_evaluation_summaries amod_evaluation_automatic dobj_carried_evaluation prt_carried_out nsubj_carried_We
P08-2005	W04-1013	o	We use ROUGE -LRB- Lin 2004 -RRB- to assess summary quality using common n-gram counts and longest common subsequence -LRB- LCS -RRB- measures	nn_measures_subsequence appos_subsequence_LCS amod_subsequence_common amod_subsequence_longest conj_and_counts_measures nn_counts_n-gram amod_counts_common dobj_using_measures dobj_using_counts nn_quality_summary xcomp_assess_using dobj_assess_quality aux_assess_to dep_Lin_2004 appos_ROUGE_Lin vmod_use_assess dobj_use_ROUGE nsubj_use_We
P08-2005	W04-1013	p	We report on ROUGE-1 -LRB- unigrams -RRB- ROUGE-2 -LRB- bigrams -RRB- ROUGE W-1 .2 -LRB- weighted LCS -RRB- and ROUGE-S * -LRB- skip bigrams -RRB- as they have been shown to correlate well with human judgments for longer multidocument summaries -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_summaries_Lin nn_summaries_multidocument amod_summaries_longer prep_for_judgments_summaries amod_judgments_human prep_with_correlate_judgments advmod_correlate_well aux_correlate_to xcomp_shown_correlate auxpass_shown_been aux_shown_have nsubjpass_shown_they mark_shown_as dobj_skip_bigrams dep_*_skip nn_*_ROUGE-S amod_LCS_weighted appos_.2_LCS nn_.2_W-1 nn_.2_ROUGE appos_ROUGE-2_bigrams conj_and_ROUGE-1_* conj_and_ROUGE-1_.2 conj_and_ROUGE-1_ROUGE-2 appos_ROUGE-1_unigrams advcl_report_shown prep_on_report_* prep_on_report_.2 prep_on_report_ROUGE-2 prep_on_report_ROUGE-1 nsubj_report_We
P08-2051	W04-1013	o	Different approaches have been proposed to measure matches using words or more meaningful semantic units for example ROUGE -LRB- Lin 2004 -RRB- factoid analysis -LRB- Teufel and Halteren 2004 -RRB- pyramid method -LRB- Nenkova and Passonneau 2004 -RRB- and Basic Element -LRB- BE -RRB- -LRB- Hovy et al. 2006 -RRB-	amod_Hovy_2006 dep_Hovy_al. nn_Hovy_et dep_Element_BE amod_Element_Basic dep_Nenkova_2004 conj_and_Nenkova_Passonneau appos_method_Passonneau appos_method_Nenkova nn_method_pyramid dep_Teufel_2004 conj_and_Teufel_Halteren appos_analysis_Halteren appos_analysis_Teufel nn_analysis_factoid amod_Lin_2004 dep_ROUGE_Hovy conj_and_ROUGE_Element appos_ROUGE_method appos_ROUGE_analysis appos_ROUGE_Lin amod_units_semantic amod_units_meaningful amod_units_more conj_or_words_units dobj_using_units dobj_using_words xcomp_measure_using dobj_measure_matches aux_measure_to dep_proposed_Element dep_proposed_ROUGE prep_for_proposed_example xcomp_proposed_measure auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches amod_approaches_Different
P08-2051	W04-1013	o	3.2 Automatic ROUGE Evaluation ROUGE -LRB- Lin 2004 -RRB- measuresthen-grammatchbetween system generated summaries and human summaries	amod_summaries_human conj_and_summaries_summaries dobj_generated_summaries dobj_generated_summaries nsubj_generated_system amod_system_measuresthen-grammatchbetween nn_system_ROUGE dep_Lin_2004 dep_ROUGE_Lin nn_ROUGE_Evaluation nn_ROUGE_ROUGE nn_ROUGE_Automatic num_ROUGE_3.2
P08-2051	W04-1013	p	ROUGE -LRB- Lin 2004 -RRB- has been widely used for summarization evaluation	nn_evaluation_summarization prep_for_used_evaluation advmod_used_widely auxpass_used_been aux_used_has nsubjpass_used_ROUGE amod_Lin_2004 appos_ROUGE_Lin
P08-2051	W04-1013	p	In the news article domain ROUGE scores have been shown to be generally highly correlated with human evaluation in content match -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_match_Lin amod_match_content prep_in_evaluation_match amod_evaluation_human prep_with_correlated_evaluation advmod_correlated_highly advmod_correlated_generally auxpass_correlated_be aux_correlated_to xcomp_shown_correlated auxpass_shown_been aux_shown_have nsubjpass_shown_scores prep_in_shown_domain nn_scores_ROUGE nn_domain_article nn_domain_news det_domain_the
P08-2052	W04-1013	o	With our best performing features we get ROUGE-2 -LRB- Lin 2004 -RRB- scores of 0.11 and 0.0925 on 2007 and 2006 5This threshold was derived experimentally with previous data	amod_data_previous prep_with_derived_data advmod_derived_experimentally auxpass_derived_was nsubjpass_derived_scores nn_threshold_5This dep_2007_threshold conj_and_2007_2006 conj_and_0.11_0.0925 prep_on_scores_2006 prep_on_scores_2007 prep_of_scores_0.0925 prep_of_scores_0.11 nn_scores_ROUGE-2 dep_Lin_2004 appos_ROUGE-2_Lin ccomp_get_derived nsubj_get_we prep_with_get_features amod_features_performing amod_features_best poss_best_our
P09-1022	W04-1013	o	-LRB- 2004 -RRB- applied to the output of the reranking parser of Charniak and Johnson -LRB- 2005 -RRB- whereas in BE -LRB- in the version presented here -RRB- dependencies are generated by the Minipar parser -LRB- Lin 1995 -RRB-	amod_Lin_1995 dep_parser_Lin nn_parser_Minipar det_parser_the agent_generated_parser auxpass_generated_are nsubjpass_generated_dependencies dep_generated_in aux_generated_BE advmod_presented_here vmod_version_presented det_version_the pobj_in_version dep_in_generated pcomp_whereas_in appos_Johnson_2005 conj_and_Charniak_Johnson prep_of_parser_Johnson prep_of_parser_Charniak nn_parser_reranking det_parser_the prep_of_output_parser det_output_the prep_applied_whereas prep_to_applied_output dep_applied_2004
P09-1022	W04-1013	n	Despite relying on a the same concept our approach outperforms BE in most comparisons and it often achieves higher correlations with human judgments than the string-matching metric ROUGE -LRB- Lin 2004 -RRB-	amod_Lin_2004 appos_ROUGE_Lin amod_ROUGE_metric amod_ROUGE_string-matching det_ROUGE_the amod_judgments_human prep_than_correlations_ROUGE prep_with_correlations_judgments amod_correlations_higher dobj_achieves_correlations advmod_achieves_often nsubj_achieves_it amod_comparisons_most prep_in_BE_comparisons conj_and_outperforms_achieves dobj_outperforms_BE nsubj_outperforms_approach prepc_despite_outperforms_relying poss_approach_our amod_concept_same det_concept_the det_concept_a prep_on_relying_concept
P09-1022	W04-1013	o	In TAC 2008 Summarization track all submitted runs were scored with the ROUGE -LRB- Lin 2004 -RRB- and Basic Elements -LRB- BE -RRB- metrics -LRB- Hovy et al. 2005 -RRB-	amod_Hovy_2005 dep_Hovy_al. nn_Hovy_et dep_metrics_BE dep_metrics_Elements amod_Elements_Basic num_Lin_2004 conj_and_ROUGE_metrics appos_ROUGE_Lin det_ROUGE_the dep_scored_Hovy prep_with_scored_metrics prep_with_scored_ROUGE auxpass_scored_were nsubjpass_scored_runs prep_in_scored_track amod_runs_submitted det_runs_all nn_track_Summarization num_track_2008 nn_track_TAC
P09-1024	W04-1013	o	These domains have been commonly used in prior work on summarization -LRB- Weischedel et al. 2004 Zhou et al. 2004 Filatova and Prager 2005 DemnerFushman and Lin 2007 Biadsy et al. 2008 -RRB-	num_Biadsy_2008 nn_Biadsy_al. nn_Biadsy_et dep_DemnerFushman_Biadsy conj_and_DemnerFushman_2007 conj_and_DemnerFushman_Lin conj_and_Filatova_2007 conj_and_Filatova_Lin conj_and_Filatova_DemnerFushman conj_and_Filatova_2005 conj_and_Filatova_Prager dep_Zhou_DemnerFushman dep_Zhou_2005 dep_Zhou_Prager dep_Zhou_Filatova num_Zhou_2004 nn_Zhou_al. nn_Zhou_et dep_Weischedel_Zhou appos_Weischedel_2004 dep_Weischedel_al. nn_Weischedel_et prep_on_work_summarization amod_work_prior dep_used_Weischedel prep_in_used_work advmod_used_commonly auxpass_used_been aux_used_have nsubjpass_used_domains det_domains_These
P09-1024	W04-1013	o	We use the publicly available ROUGE toolkit -LRB- Lin 2004 -RRB- tocomputerecall precision andF-scorefor ROUGE-1	amod_ROUGE-1_andF-scorefor appos_tocomputerecall_ROUGE-1 conj_tocomputerecall_precision dep_Lin_2004 dep_toolkit_tocomputerecall dep_toolkit_Lin nn_toolkit_ROUGE amod_toolkit_available det_toolkit_the advmod_available_publicly dobj_use_toolkit nsubj_use_We
P09-1024	W04-1013	o	There has been a sizable amount of research on structure induction ranging fromlinearsegmentation -LRB- Hearst 1994 -RRB- tocontent modeling -LRB- Barzilay and Lee 2004 -RRB-	amod_Barzilay_2004 conj_and_Barzilay_Lee dep_modeling_Lee dep_modeling_Barzilay amod_modeling_tocontent nn_modeling_fromlinearsegmentation nn_modeling_induction dep_Hearst_1994 appos_fromlinearsegmentation_Hearst amod_fromlinearsegmentation_ranging nn_induction_structure prep_on_amount_modeling prep_of_amount_research amod_amount_sizable det_amount_a cop_amount_been aux_amount_has expl_amount_There
P09-1062	W04-1013	o	As such we quantify success based on ROUGE -LRB- Lin 2004 -RRB- scores	dep_scores_Lin nn_scores_ROUGE dep_Lin_2004 prep_on_based_scores vmod_success_based dobj_quantify_success nsubj_quantify_we prep_such_as_quantify_such
P09-1099	W04-1013	o	Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields widely used measures include BLEU -LRB- Papineni et al. 2002 -RRB- for machine translation and ROUGE -LRB- Lin 2004 -RRB- for summarisation for example	dep_Lin_2004 appos_ROUGE_Lin conj_and_translation_ROUGE nn_translation_machine amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_for_BLEU_example prep_for_BLEU_summarisation prep_for_BLEU_ROUGE prep_for_BLEU_translation dep_BLEU_Papineni dobj_include_BLEU nsubj_include_measures amod_measures_used advmod_used_widely amod_fields_other prep_of_number_fields det_number_a prep_in_developed_number auxpass_developed_been aux_developed_have nsubjpass_developed_properties advmod_developed_automatically amod_properties_computable prepc_on_based_developed dep_behaviour_include vmod_behaviour_based nn_behaviour_system nn_behaviour_rate dep_that_behaviour dep_metrics_that nn_metrics_evaluation amod_metrics_Automated
P09-2025	W04-1013	o	ROUGE -LRB- Lin 2004 -RRB- is an evaluation metric designed to evaluate automatically generated summaries	dobj_generated_summaries advmod_generated_automatically ccomp_evaluate_generated aux_evaluate_to xcomp_designed_evaluate vmod_metric_designed amod_evaluation_metric det_evaluation_an nsubj_is_evaluation amod_Lin_2004 dep_ROUGE_is appos_ROUGE_Lin
P09-2027	W04-1013	p	The ROUGE -LRB- Lin 2004 -RRB- suite of metrics are n-gram overlap based metrics that have been shown to highly correlate with human evaluations on content responsiveness	nn_responsiveness_content amod_evaluations_human prep_on_correlate_responsiveness prep_with_correlate_evaluations advmod_correlate_highly aux_correlate_to xcomp_shown_correlate auxpass_shown_been aux_shown_have nsubjpass_shown_that rcmod_metrics_shown amod_metrics_based dobj_overlap_metrics dep_n-gram_overlap cop_n-gram_are nsubj_n-gram_suite prep_of_suite_metrics nn_suite_ROUGE dep_Lin_2004 appos_ROUGE_Lin det_ROUGE_The
P09-2066	W04-1013	o	For a comparison we also include the ROUGE-1 Fscores -LRB- Lin 2004 -RRB- of each system output against the human compressed sentences	amod_sentences_compressed amod_sentences_human det_sentences_the nn_output_system det_output_each dep_Lin_2004 prep_against_Fscores_sentences prep_of_Fscores_output appos_Fscores_Lin nn_Fscores_ROUGE-1 det_Fscores_the dobj_include_Fscores advmod_include_also nsubj_include_we prep_for_include_comparison det_comparison_a
P09-2083	W04-1013	o	2 Automatic Annotation Schemes Using ROUGE Similarity Measures ROUGE -LRB- Recall-Oriented Understudy for Gisting Evaluation -RRB- is an automatic tool to determine the quality of a summary using a collection of measures ROUGE-N -LRB- N = 1,2,3,4 -RRB- ROUGE-L ROUGE-W and ROUGE-S which count the number of overlapping units such as n-gram word-sequences and word-pairs between the extract and the abstract summaries -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_summaries_Lin amod_summaries_abstract det_summaries_the conj_and_extract_summaries det_extract_the conj_and_n-gram_word-pairs conj_and_n-gram_word-sequences prep_such_as_units_word-pairs prep_such_as_units_word-sequences prep_such_as_units_n-gram amod_units_overlapping prep_between_number_summaries prep_between_number_extract prep_of_number_units det_number_the dobj_count_number nsubj_count_which rcmod_ROUGE-L_count conj_and_ROUGE-L_ROUGE-S conj_and_ROUGE-L_ROUGE-W dobj_=_1,2,3,4 nsubj_=_N dep_ROUGE-N_ROUGE-S dep_ROUGE-N_ROUGE-W dep_ROUGE-N_ROUGE-L dep_ROUGE-N_= dep_measures_ROUGE-N prep_of_collection_measures det_collection_a dobj_using_collection det_summary_a prep_of_quality_summary det_quality_the xcomp_determine_using dobj_determine_quality aux_determine_to amod_tool_automatic det_tool_an cop_tool_is nsubj_tool_Measures dep_tool_Similarity nn_tool_ROUGE nn_Evaluation_Gisting prep_for_Understudy_Evaluation amod_Understudy_Recall-Oriented dep_ROUGE_Understudy dep_Measures_ROUGE vmod_Using_determine dobj_Using_tool vmod_Schemes_Using nn_Schemes_Annotation nn_Schemes_Automatic num_Schemes_2 dep_``_Schemes
P09-2083	W04-1013	o	We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_ROUGE_Lin nn_ROUGE_toolkit nn_ROUGE_evaluation amod_ROUGE_automatic det_ROUGE_the dobj_using_ROUGE vmod_summaries_using dobj_generated_summaries nsubj_generated_system det_system_the ccomp_evaluate_generated nsubj_evaluate_We ccomp_``_evaluate
W05-0901	W04-1013	o	-LRB- Lin 2004 Lin and Och 2004 -RRB-	dep_2004_Och dep_2004_Lin conj_and_Lin_Och dep_2004_2004 dep_2004_Lin dep_''_2004
W05-0907	W04-1013	o	5 Related work The methodology which is closest to our framework is ORANGE -LRB- Lin 2004a -RRB- which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set	nn_set_baseline det_set_a prep_within_items_set nn_items_reference agent_obtained_items vmod_ranks_obtained amod_ranks_average det_ranks_the dobj_using_ranks vmod_similarity_using amod_similarity_metric det_similarity_a dobj_evaluates_similarity nsubj_evaluates_which appos_Lin_2004a rcmod_ORANGE_evaluates dep_ORANGE_Lin cop_ORANGE_is nsubj_ORANGE_methodology poss_framework_our prep_to_closest_framework cop_closest_is nsubj_closest_which rcmod_methodology_closest det_methodology_The rcmod_work_ORANGE amod_work_Related num_work_5
W05-0907	W04-1013	o	-LRB- Lin 2004b -RRB-	nn_2004b_Lin dep_''_2004b
W06-0707	W04-1013	o	Additionally automatic evaluation of content coverage using ROUGE -LRB- Lin 2004 -RRB- was explored in 2004	prep_in_explored_2004 auxpass_explored_was nsubjpass_explored_evaluation advmod_explored_Additionally amod_Lin_2004 appos_ROUGE_Lin dobj_using_ROUGE nn_coverage_content vmod_evaluation_using prep_of_evaluation_coverage amod_evaluation_automatic
W06-1401	W04-1013	p	We can credit DUC with the emergence of automatic methods for evaluation such as ROUGE -LRB- Lin and Hovy 2003 Lin 2004 -RRB- which allow quick measurement of systems during development and enable evaluation of larger amounts of data	prep_of_amounts_data amod_amounts_larger prep_of_evaluation_amounts dobj_enable_evaluation nsubj_enable_which prep_of_measurement_systems amod_measurement_quick conj_and_allow_enable prep_during_allow_development dobj_allow_measurement nsubj_allow_which num_Lin_2004 rcmod_Lin_enable rcmod_Lin_allow dep_Lin_Lin conj_and_Lin_2003 conj_and_Lin_Hovy dep_ROUGE_2003 dep_ROUGE_Hovy dep_ROUGE_Lin prep_such_as_evaluation_ROUGE amod_methods_automatic prep_for_emergence_evaluation prep_of_emergence_methods det_emergence_the prep_with_credit_emergence dobj_credit_DUC aux_credit_can nsubj_credit_We ccomp_``_credit
W06-1643	W04-1013	p	Two metrics have become quite popular in multi-document summarization namely the Pyramid method -LRB- Nenkova and Passonneau 2004b -RRB- and ROUGE -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_ROUGE_Lin conj_and_Nenkova_2004b conj_and_Nenkova_Passonneau conj_and_method_ROUGE dep_method_2004b dep_method_Passonneau dep_method_Nenkova nn_method_Pyramid det_method_the advmod_method_namely amod_summarization_multi-document advmod_popular_quite advcl_become_ROUGE advcl_become_method prep_in_become_summarization acomp_become_popular aux_become_have nsubj_become_metrics num_metrics_Two ccomp_``_become
W06-1643	W04-1013	o	Empirical evaluations using two standard summarization metricsthe Pyramid method -LRB- Nenkova and Passonneau 2004b -RRB- and ROUGE -LRB- Lin 2004 -RRB- show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies which achieves 91.3 % of human performance in Pyramid score and outperforms our best-performing non-sequential model by 3.9 %	num_%_3.9 amod_model_non-sequential amod_model_best-performing poss_model_our prep_by_outperforms_% dobj_outperforms_model nsubj_outperforms_system nn_score_Pyramid amod_performance_human prep_of_%_performance num_%_91.3 prep_in_achieves_score dobj_achieves_% nsubj_achieves_which nn_dependencies_skip-chain rcmod_dependencies_achieves conj_and_dependencies_dependencies nn_dependencies_Markov nn_dependencies_order-2 preconj_dependencies_both dobj_incorporating_dependencies dobj_incorporating_dependencies conj_and_CRF_outperforms vmod_CRF_incorporating det_CRF_a cop_CRF_is nsubj_CRF_system mark_CRF_that amod_system_performing amod_system_best det_system_the ccomp_show_outperforms ccomp_show_CRF nsubj_show_evaluations amod_Lin_2004 appos_ROUGE_Lin conj_and_Nenkova_2004b conj_and_Nenkova_Passonneau conj_and_method_ROUGE dep_method_2004b dep_method_Passonneau dep_method_Nenkova nn_method_Pyramid nn_method_metricsthe nn_method_summarization amod_method_standard num_method_two dobj_using_ROUGE dobj_using_method vmod_evaluations_using amod_evaluations_Empirical
W06-1643	W04-1013	o	To find these pairs automatically wetrainedanon-sequentiallog-linearmodel that achieves a .902 accuracy -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_accuracy_.902 det_accuracy_a dobj_achieves_accuracy nsubj_achieves_that dep_wetrainedanon-sequentiallog-linearmodel_Galley rcmod_wetrainedanon-sequentiallog-linearmodel_achieves det_pairs_these dep_find_wetrainedanon-sequentiallog-linearmodel advmod_find_automatically dobj_find_pairs aux_find_To ccomp_``_find
W06-1643	W04-1013	o	Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques including TRP -LRB- Sutton and McCallum 2004 -RRB- and Gibbs sampling -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et nn_sampling_Gibbs amod_Sutton_2004 conj_and_Sutton_McCallum conj_and_TRP_sampling dep_TRP_McCallum dep_TRP_Sutton dep_techniques_Finkel prep_including_techniques_sampling prep_including_techniques_TRP nn_techniques_inference amod_techniques_probabilistic amod_techniques_approximate amod_techniques_used dep_dependencies_techniques amod_dependencies_nonlocal dobj_containing_dependencies vmod_CRFs_containing prep_with_work_CRFs amod_work_previous amod_work_Most
W07-1411	W04-1013	o	We have implemented them as defined in -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_in_Lin prep_defined_in mark_defined_as advcl_implemented_defined dobj_implemented_them aux_implemented_have nsubj_implemented_We
W08-0127	W04-1013	o	e.g. BLEU -LRB- Papineni et al. 2001 -RRB- for machine translation ROUGE -LRB- Lin 2004 -RRB- for summarization	dep_Lin_2004 prep_for_ROUGE_summarization appos_ROUGE_Lin appos_translation_ROUGE nn_translation_machine num_Papineni_2001 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_for_e.g._translation dep_e.g._BLEU dep_``_e.g.
W08-1106	W04-1013	o	There are also automatic methods for summary evaluation such as ROUGE -LRB- Lin 2004 -RRB- which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary	nn_summary_machine det_summary_the conj_and_summary_summary nn_summary_model amod_summary_human-written det_summary_a prep_between_words_summary prep_between_words_summary prep_of_sequences_words det_sequences_the prep_in_similarity_sequences det_similarity_the det_score_a pobj_gives_similarity prepc_based_on_gives_on dobj_gives_score nsubj_gives_which dep_Lin_2004 rcmod_ROUGE_gives appos_ROUGE_Lin nn_evaluation_summary prep_such_as_methods_ROUGE prep_for_methods_evaluation amod_methods_automatic nsubj_are_methods advmod_are_also expl_are_There ccomp_``_are
W08-1113	W04-1013	o	Such metrics have been introduced in other fields including PARADISE -LRB- Walker et al. 1997 -RRB- for spoken dialogue systems BLEU -LRB- Papineni et al. 2002 -RRB- for machine translation ,1 and ROUGE -LRB- Lin 2004 -RRB- for summarisation	dep_Lin_2004 appos_ROUGE_Lin conj_and_,1_ROUGE nn_,1_translation nn_,1_machine amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_for_BLEU_summarisation prep_for_BLEU_ROUGE prep_for_BLEU_,1 dep_BLEU_Papineni nn_systems_dialogue amod_systems_spoken amod_Walker_1997 dep_Walker_al. nn_Walker_et dep_PARADISE_Walker prep_including_fields_PARADISE amod_fields_other dep_introduced_BLEU prep_for_introduced_systems prep_in_introduced_fields auxpass_introduced_been aux_introduced_have nsubjpass_introduced_metrics amod_metrics_Such
W08-1406	W04-1013	n	In what concerns the evaluation process although ROUGE -LRB- Lin 2004 -RRB- is the most common evaluation metric for the automatic evaluation of summarization since our approach might introduce in the summary information that it is not present in the original input source we found that a human evaluation was more adequate to assess the relevance of that additional information	amod_information_additional det_information_that prep_of_relevance_information det_relevance_the dobj_assess_relevance aux_assess_to xcomp_adequate_assess advmod_adequate_more cop_adequate_was nsubj_adequate_evaluation mark_adequate_that amod_evaluation_human det_evaluation_a ccomp_found_adequate nsubj_found_we nn_source_input amod_source_original det_source_the prep_in_present_source neg_present_not cop_present_is nsubj_present_it mark_present_that ccomp_information_present nn_information_summary det_information_the prep_in_introduce_information aux_introduce_might nsubj_introduce_approach mark_introduce_since poss_approach_our prep_of_evaluation_summarization amod_evaluation_automatic det_evaluation_the parataxis_metric_found advcl_metric_introduce prep_for_metric_evaluation npadvmod_metric_evaluation amod_metric_common det_metric_the cop_metric_is nsubj_metric_ROUGE mark_metric_although advmod_common_most amod_Lin_2004 appos_ROUGE_Lin advcl_process_metric nn_process_evaluation det_process_the det_concerns_what dep_In_process pobj_In_concerns dep_``_In
W08-1407	W04-1013	o	5 Results The model summaries were compared against 24 summaries generated automatically using SUMMA by calculating ROUGE-1 to ROUGE4 ROUGE-L and ROUGE-W-1 .2 recall metrics -LRB- Lin 2004 -RRB-	amod_Lin_2004 nn_metrics_recall nn_metrics_.2 nn_metrics_ROUGE-W-1 dep_ROUGE4_Lin conj_and_ROUGE4_metrics conj_and_ROUGE4_ROUGE-L prep_to_calculating_metrics prep_to_calculating_ROUGE-L prep_to_calculating_ROUGE4 dobj_calculating_ROUGE-1 prepc_by_using_calculating dobj_using_SUMMA advmod_using_automatically xcomp_generated_using vmod_summaries_generated num_summaries_24 prep_against_compared_summaries auxpass_compared_were nsubjpass_compared_summaries nn_summaries_model det_summaries_The rcmod_Results_compared num_Results_5
W08-1407	W04-1013	o	We use SUMMA -LRB- Saggion and Gaizauskas 2005 -RRB- to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics -LRB- Lin 2004 -RRB- relative to human generated summaries	amod_summaries_generated amod_summaries_human prep_to_relative_summaries dep_Lin_2004 amod_metrics_relative appos_metrics_Lin nn_metrics_evaluation nn_metrics_ROUGE dobj_using_metrics xcomp_evaluate_using dobj_evaluate_them amod_summaries_multi-document amod_summaries_query-based amod_summaries_generic conj_and_generic_query-based conj_and_generate_evaluate dobj_generate_summaries aux_generate_to dep_Saggion_2005 conj_and_Saggion_Gaizauskas appos_SUMMA_Gaizauskas appos_SUMMA_Saggion vmod_use_evaluate vmod_use_generate dobj_use_SUMMA nsubj_use_We
W08-1808	W04-1013	n	We considered a variety of tools like ROUGE -LRB- Lin 2004 -RRB- and METEOR -LRB- Lavie and Agarwal 2007 -RRB- but decided they were unsuitable for this task	det_task_this prep_for_unsuitable_task cop_unsuitable_were nsubj_unsuitable_they ccomp_decided_unsuitable nsubj_decided_We amod_Lavie_2007 conj_and_Lavie_Agarwal amod_Lin_2004 dep_ROUGE_Agarwal dep_ROUGE_Lavie conj_and_ROUGE_METEOR dep_ROUGE_Lin prep_like_tools_METEOR prep_like_tools_ROUGE prep_of_variety_tools det_variety_a conj_but_considered_decided dobj_considered_variety nsubj_considered_We ccomp_``_decided ccomp_``_considered
W08-2008	W04-1013	o	Finally in order to formally evaluate the method and the different heuristics a large-scale evaluation on the BioMed Corpus is under way based on computing the ROUGE measures -LRB- Lin 2004 -RRB-	dep_Lin_2004 appos_measures_Lin nn_measures_ROUGE det_measures_the dobj_computing_measures prepc_based_on_is_computing prep_under_is_way nsubj_is_evaluation advcl_is_evaluate advmod_is_Finally nn_Corpus_BioMed det_Corpus_the prep_on_evaluation_Corpus amod_evaluation_large-scale det_evaluation_a amod_heuristics_different det_heuristics_the conj_and_method_heuristics det_method_the dobj_evaluate_heuristics dobj_evaluate_method advmod_evaluate_formally aux_evaluate_to dep_evaluate_order mark_evaluate_in
W09-1607	W04-1013	o	The summaries from the above algorithm for the QF-MDS were evaluated based on ROUGE metrics -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_metrics_Lin nn_metrics_ROUGE pobj_evaluated_metrics prepc_based_on_evaluated_on auxpass_evaluated_were nsubjpass_evaluated_summaries det_QF-MDS_the prep_for_algorithm_QF-MDS amod_algorithm_above det_algorithm_the prep_from_summaries_algorithm det_summaries_The
W09-1802	W04-1013	o	In particular ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries -LRB- Lin 2004 -RRB-	amod_Lin_2004 dep_summaries_Lin amod_summaries_abstractive amod_summaries_human-written prep_of_set_summaries det_set_a prep_with_recall_set prep_in_recall_bigrams det_recall_the cop_recall_is nsubj_recall_ROUGE-2 prep_in_recall_particular
W09-2804	W04-1013	o	4.2 Building a Human Performance Model We adopt the evaluation approach that a good content selection strategy should perform similarly to humans which is the view taken by existing summarization evaluation schemes such as ROUGE -LRB- Lin 2004 -RRB- and the Pyramid method -LRB- Nenkova et al. 2007 -RRB-	amod_Nenkova_2007 dep_Nenkova_al. nn_Nenkova_et nn_method_Pyramid det_method_the amod_Lin_2004 dep_ROUGE_Nenkova conj_and_ROUGE_method dep_ROUGE_Lin prep_such_as_schemes_method prep_such_as_schemes_ROUGE nn_schemes_evaluation nn_schemes_summarization amod_schemes_existing agent_taken_schemes vmod_view_taken det_view_the cop_view_is nsubj_view_which rcmod_humans_view prep_to_perform_humans advmod_perform_similarly aux_perform_should nsubj_perform_strategy mark_perform_that nn_strategy_selection nn_strategy_content amod_strategy_good det_strategy_a ccomp_approach_perform nn_approach_evaluation det_approach_the dobj_adopt_approach nsubj_adopt_We rcmod_Model_adopt nn_Model_Performance amod_Model_Human det_Model_a dep_Building_Model num_Building_4.2 dep_``_Building
W09-2806	W04-1013	o	This view is supported by Lin -LRB- 2004a -RRB- who concludes that correlations to human judgments were increased by using multiple references but using single reference summary with enough number of samples was a valid alternative	amod_alternative_valid det_alternative_a cop_alternative_was prep_of_number_samples amod_number_enough nn_summary_reference amod_summary_single prep_with_using_number dobj_using_summary amod_references_multiple conj_but_using_using dobj_using_references agent_increased_using agent_increased_using auxpass_increased_were nsubjpass_increased_correlations mark_increased_that amod_judgments_human prep_to_correlations_judgments ccomp_concludes_increased nsubj_concludes_who rcmod_Lin_concludes appos_Lin_2004a dep_supported_alternative agent_supported_Lin auxpass_supported_is nsubjpass_supported_view det_view_This ccomp_``_supported
W09-2806	W04-1013	o	Interestingly similar conclusions were also reached in the area of Machine Translation evaluation in their experiments Zhang and Vogel -LRB- 2004 -RRB- show that adding an additional reference translation compensates the effects of removing 1015 % of the testing data and state that therefore it seems more cost effective to have more test sentences but fewer reference translations	nn_translations_reference amod_translations_fewer nn_sentences_test amod_sentences_more conj_but_have_translations dobj_have_sentences aux_have_to xcomp_effective_translations xcomp_effective_have npadvmod_effective_cost amod_cost_more acomp_seems_effective nsubj_seems_it advmod_seems_therefore nsubj_seems_that dep_seems_state nn_data_testing det_data_the prep_of_%_data num_%_1015 dobj_removing_% prepc_of_effects_removing det_effects_the conj_and_compensates_seems dobj_compensates_effects csubj_compensates_adding mark_compensates_that nn_translation_reference amod_translation_additional det_translation_an dobj_adding_translation ccomp_show_seems ccomp_show_compensates nsubj_show_Vogel nsubj_show_Zhang prep_in_show_experiments appos_Zhang_2004 conj_and_Zhang_Vogel poss_experiments_their nn_evaluation_Translation nn_evaluation_Machine prep_of_area_evaluation det_area_the parataxis_reached_show prep_in_reached_area advmod_reached_also auxpass_reached_were nsubjpass_reached_conclusions advmod_reached_Interestingly amod_conclusions_similar
W09-2806	W04-1013	o	All submitted runs were evaluated with the automatic metrics ROUGE -LRB- Lin 2004b -RRB- which calculates the proportion of n-grams shared between the candidate summary and the reference summaries and Basic Elements -LRB- Hovy et al. 2005 -RRB- which compares the candidate to the models in terms of head-modifier pairs	amod_pairs_head-modifier prep_of_terms_pairs det_models_the det_candidate_the prep_in_compares_terms prep_to_compares_models dobj_compares_candidate nsubj_compares_which amod_Hovy_2005 dep_Hovy_al. nn_Hovy_et amod_Elements_Basic nn_summaries_reference det_summaries_the conj_and_summary_summaries nn_summary_candidate det_summary_the prep_between_shared_summaries prep_between_shared_summary vmod_n-grams_shared prep_of_proportion_n-grams det_proportion_the dobj_calculates_proportion nsubj_calculates_which appos_Lin_2004b rcmod_ROUGE_compares dep_ROUGE_Hovy conj_and_ROUGE_Elements rcmod_ROUGE_calculates dep_ROUGE_Lin dep_metrics_Elements dep_metrics_ROUGE amod_metrics_automatic det_metrics_the prep_with_evaluated_metrics auxpass_evaluated_were nsubjpass_evaluated_runs amod_runs_submitted det_runs_All
W09-2806	W04-1013	o	2.2 Automatic metrics Similarly to the Pyramid method ROUGE -LRB- Lin 2004b -RRB- and Basic Elements -LRB- Hovy et al. 2005 -RRB- require multiple topics and model summaries to produce optimal results	amod_results_optimal dobj_produce_results aux_produce_to nn_summaries_model conj_and_topics_summaries amod_topics_multiple xcomp_require_produce dobj_require_summaries dobj_require_topics nsubj_require_metrics amod_Hovy_2005 dep_Hovy_al. nn_Hovy_et amod_Elements_Basic appos_Lin_2004b dep_ROUGE_Lin conj_and_method_Elements conj_and_method_ROUGE nn_method_Pyramid det_method_the dep_metrics_Hovy prep_to_metrics_Elements prep_to_metrics_ROUGE prep_to_metrics_method advmod_metrics_Similarly nn_metrics_Automatic num_metrics_2.2
W09-2806	W04-1013	o	Our question here is not only what this relation looks like -LRB- as it was examined on the basis of Document Understanding Conference data in Lin -LRB- 2004a -RRB- -RRB- but also how it compares to the reliability of other metrics	amod_metrics_other prep_of_reliability_metrics det_reliability_the prep_to_compares_reliability nsubj_compares_it advmod_compares_how advmod_compares_also appos_Lin_2004a prep_in_data_Lin nn_data_Conference dobj_Understanding_data vmod_Document_Understanding prep_of_basis_Document det_basis_the conj_but_examined_compares prep_on_examined_basis auxpass_examined_was nsubjpass_examined_it mark_examined_as dep_like_compares dep_like_examined prep_looks_like nsubj_looks_relation dobj_looks_what det_relation_this ccomp_only_looks neg_only_not cop_only_is advmod_only_here nsubj_only_question poss_question_Our ccomp_``_only
E09-1017	W04-1016	o	Considerations of sentence fluency are also key in sentence simplification -LRB- Siddharthan 2003 -RRB- sentence compression -LRB- Jing 2000 Knight and Marcu 2002 Clarke and Lapata 2006 McDonald 2006 Turner and Charniak 2005 Galley and McKeown 2007 -RRB- text re-generation for summarization -LRB- Daume III and Marcu 2004 Barzilay and McKeown 2005 Wan et al. 2005 -RRB- and headline generation -LRB- Banko et al. 2000 Zajic et al. 2007 Soricut and Marcu 2007 -RRB-	dep_Soricut_2007 conj_and_Soricut_Marcu dep_Zajic_Marcu dep_Zajic_Soricut num_Zajic_2007 nn_Zajic_al. nn_Zajic_et dep_Banko_Zajic appos_Banko_2000 dep_Banko_al. nn_Banko_et nn_generation_headline num_Wan_2005 nn_Wan_al. nn_Wan_et dep_Barzilay_Banko conj_and_Barzilay_generation conj_and_Barzilay_Wan conj_and_Barzilay_2005 conj_and_Barzilay_McKeown conj_and_Daume_generation conj_and_Daume_Wan conj_and_Daume_2005 conj_and_Daume_McKeown conj_and_Daume_Barzilay conj_and_Daume_2004 conj_and_Daume_Marcu num_Daume_III dep_summarization_Barzilay dep_summarization_2004 dep_summarization_Marcu dep_summarization_Daume prep_for_re-generation_summarization nn_re-generation_text dep_Galley_2007 conj_and_Galley_McKeown conj_and_Turner_2005 conj_and_Turner_Charniak conj_McDonald_2006 conj_and_Knight_McDonald conj_and_Knight_2006 conj_and_Knight_Lapata conj_and_Knight_Clarke conj_and_Knight_2002 conj_and_Knight_Marcu dep_Jing_McKeown dep_Jing_Galley conj_Jing_2005 conj_Jing_Charniak conj_Jing_Turner conj_Jing_McDonald conj_Jing_2006 conj_Jing_Lapata conj_Jing_Clarke conj_Jing_2002 conj_Jing_Marcu conj_Jing_Knight conj_Jing_2000 appos_compression_Jing nn_compression_sentence dep_Siddharthan_2003 nn_simplification_sentence dep_key_re-generation dep_key_compression dep_key_Siddharthan prep_in_key_simplification advmod_key_also cop_key_are nsubj_key_Considerations nn_fluency_sentence prep_of_Considerations_fluency ccomp_``_key
P08-2049	W04-1016	o	Daume III & Marcu -LRB- 2004 -RRB- argue that generic sentence fusion is an ill-defined task	amod_task_ill-defined det_task_an cop_task_is nsubj_task_fusion mark_task_that nn_fusion_sentence amod_fusion_generic ccomp_argue_task nsubj_argue_Marcu nsubj_argue_III appos_Marcu_2004 conj_and_III_Marcu nn_III_Daume
I08-1042	W05-0904	o	For instance we may find metrics based on full constituent parsing -LRB- Liu and Gildea 2005 -RRB- and on dependency parsing -LRB- Liu and Gildea 2005 Amigo et al. 2006 Mehay and Brew 2007 Owczarzak et al. 2007 -RRB-	num_Owczarzak_2007 nn_Owczarzak_al. nn_Owczarzak_et conj_and_Mehay_Brew num_Amigo_2006 nn_Amigo_al. nn_Amigo_et dep_Liu_Owczarzak dep_Liu_2007 dep_Liu_Brew dep_Liu_Mehay dep_Liu_Amigo num_Liu_2005 conj_and_Liu_Gildea appos_parsing_Gildea appos_parsing_Liu nn_parsing_dependency pobj_on_parsing dep_Liu_2005 conj_and_Liu_Gildea appos_parsing_Gildea appos_parsing_Liu nn_parsing_constituent amod_parsing_full prep_on_based_parsing vmod_metrics_based conj_and_find_on dobj_find_metrics aux_find_may nsubj_find_we prep_for_find_instance