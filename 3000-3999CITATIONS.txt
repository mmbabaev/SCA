P04-1018	J96-1002	o	We use maximum entropy model -LRB- Berger et al. 1996 -RRB- for both the mention-pair model -LRB- 9 -RRB- and the entity-mention model -LRB- 8 -RRB- a83a84a1a86a85a88a87 a43 a44 a71 a43 a16 a5a13a7 a55a35a34a23a36 a6a35a37 a6a39a38a40a6a42a41 a31a44a43a3a45a31 a6 a45a46a48a47a24a49 a50 a1 a43 a44 a71 a43 a16 a5 a71 -LRB- 10 -RRB- a83a84a1a4a85 a87 a55 a81 a71 a43 a16 a5a13a7 a55a35a34 a36 a6 a37 a6a39a38a40a6a42a41 a11a7a32 a45a31 a6 a45a46a48a47 a49 a50 a1 a55a39a81 a71 a43 a16 a5 a71 -LRB- 11 -RRB- wherea57 a16 a1a51a8 a71a52a8 a71a90a85a73a5 is a feature and a53 a16 is its weight a50 a1a33a8 a71a54a8a5 is a normalizing factor to ensure that -LRB- 10 -RRB- or -LRB- 11 -RRB- is a probability	det_probability_a cop_probability_is nsubj_probability_11 nsubj_probability_10 mark_probability_that conj_or_10_11 ccomp_ensure_probability aux_ensure_to vmod_factor_ensure nn_factor_normalizing det_factor_a cop_factor_is nsubj_factor_a71a54a8a5 nn_a71a54a8a5_a1a33a8 nn_a71a54a8a5_a50 poss_weight_its cop_weight_is nsubj_weight_a71 nn_a16_a53 conj_and_feature_a16 det_feature_a cop_feature_is nsubj_feature_a71a90a85a73a5 nn_a71a90a85a73a5_a71a52a8 num_a71a90a85a73a5_a1a51a8 nn_a71a90a85a73a5_a16 nn_a71a90a85a73a5_wherea57 num_a71a90a85a73a5_11 rcmod_a71_a16 rcmod_a71_feature nn_a71_a5 nn_a71_a16 nn_a71_a43 nn_a71_a71 nn_a71_a55a39a81 nn_a71_a1 nn_a71_a50 nn_a71_a49 nn_a71_a45a46a48a47 nn_a71_a6 nn_a71_a45a31 nn_a71_a11a7a32 nn_a71_a6a39a38a40a6a42a41 nn_a71_a37 nn_a71_a6 nn_a71_a36 nn_a71_a55a35a34 nn_a71_a5a13a7 nn_a71_a16 nn_a71_a43 nn_a71_a71 nn_a71_a81 nn_a71_a55 nn_a71_a87 nn_a71_a83a84a1a4a85 num_a71_10 dep_a71_factor rcmod_a71_weight nn_a71_a5 nn_a71_a16 nn_a71_a43 nn_a71_a71 nn_a71_a44 nn_a71_a43 nn_a71_a1 nn_a71_a50 nn_a71_a45a46a48a47a24a49 nn_a71_a6 nn_a71_a31a44a43a3a45a31 nn_a71_a6a39a38a40a6a42a41 nn_a71_a6a35a37 nn_a71_a55a35a34a23a36 nn_a71_a5a13a7 nn_a71_a16 nn_a71_a43 nn_a71_a71 nn_a71_a44 nn_a71_a43 nn_a71_a83a84a1a86a85a88a87 appos_model_8 amod_model_entity-mention det_model_the conj_and_model_model appos_model_9 amod_model_mention-pair det_model_the preconj_model_both amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_model_entropy amod_model_maximum dep_use_a71 prep_for_use_model prep_for_use_model dep_use_Berger dobj_use_model nsubj_use_We
P04-1018	J96-1002	p	Effective training algorithm exists -LRB- Berger et al. 1996 -RRB- once the set of features a42 a57 a16 a1a33a8 a71a54a8 a71a100a85a68a5 a53 is selected	auxpass_selected_is csubjpass_selected_a42 nn_a53_a71a100a85a68a5 nn_a53_a71a54a8 num_a53_a1a33a8 nn_a53_a16 nn_a53_a57 dobj_a42_a53 nsubj_a42_set prep_of_set_features det_set_the advmod_set_once amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_exists_selected dep_exists_Berger nsubj_exists_algorithm nn_algorithm_training amod_algorithm_Effective
P04-1020	J96-1002	o	-LRB- In our experiments we use maximum entropy classification -LRB- MaxEnt -RRB- -LRB- Berger et al. 1996 -RRB- to train this probability model -RRB-	nn_model_probability det_model_this dobj_train_model aux_train_to amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_classification_MaxEnt amod_classification_entropy amod_classification_maximum vmod_use_train dep_use_Berger dobj_use_classification nsubj_use_we prep_in_use_experiments poss_experiments_our
P04-1085	J96-1002	o	We use maximum entropy modeling -LRB- Berger et al. 1996 -RRB- to directly model the conditional probability a17a19a18a20a2a21a15a23a22a24a26a25 where each a27a5a15 in a24a29a28a30a18a31a27a32a4a33a6a7a8a9a8a9a8a9a6a23a27a34a11a14a25 is an observation associated with the corresponding speaker a2 a15 a27 a15 is represented here by only one variable for notational ease but it possibly represents several lexical durational structural and acoustic observations	amod_observations_acoustic conj_and_lexical_observations conj_and_lexical_structural conj_and_lexical_durational amod_lexical_several dobj_represents_observations dobj_represents_structural dobj_represents_durational dobj_represents_lexical advmod_represents_possibly nsubj_represents_it amod_ease_notational prep_for_variable_ease num_variable_one quantmod_one_only conj_but_represented_represents agent_represented_variable advmod_represented_here auxpass_represented_is nsubjpass_represented_a15 nn_a15_a27 nn_a15_a2 nn_a15_speaker amod_a15_corresponding det_a15_the prep_with_associated_a15 vmod_observation_associated det_observation_an cop_observation_is nsubj_observation_a27a5a15 advmod_observation_where prep_in_a27a5a15_a24a29a28a30a18a31a27a32a4a33a6a7a8a9a8a9a8a9a6a23a27a34a11a14a25 det_a27a5a15_each rcmod_a17a19a18a20a2a21a15a23a22a24a26a25_observation nn_a17a19a18a20a2a21a15a23a22a24a26a25_probability amod_a17a19a18a20a2a21a15a23a22a24a26a25_conditional det_a17a19a18a20a2a21a15a23a22a24a26a25_the dobj_model_a17a19a18a20a2a21a15a23a22a24a26a25 advmod_model_directly aux_model_to amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_modeling_Berger amod_modeling_entropy amod_modeling_maximum parataxis_use_represents parataxis_use_represented vmod_use_model dobj_use_modeling nsubj_use_We
P04-1085	J96-1002	o	Speaker ranking accuracy Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets the performance is 89.39 % when using all feature sets and reaches 90.2 % after applying Gaussian smoothing and using incremental feature selection as described in -LRB- Berger et al. 1996 -RRB- and implemented in the yasmetFS package .6 Note that restricting ourselves to only backward looking features decreases the performance significantly as we can see in Table 2	num_Table_2 prep_in_see_Table aux_see_can nsubj_see_we mark_see_as det_performance_the advmod_decreases_significantly dobj_decreases_performance nsubj_decreases_features ccomp_looking_decreases advmod_looking_backward advmod_backward_only advcl_restricting_see prepc_to_restricting_looking dobj_restricting_ourselves dep_that_restricting dep_Note_that num_package_.6 nn_package_yasmetFS det_package_the prep_in_implemented_package conj_and_Berger_implemented amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_in_described_implemented prep_in_described_Berger mark_described_as nn_selection_feature amod_selection_incremental advcl_using_described dobj_using_selection amod_smoothing_Gaussian conj_and_applying_using dobj_applying_smoothing num_%_90.2 prepc_after_reaches_using prepc_after_reaches_applying dobj_reaches_% nn_sets_feature det_sets_all dep_using_Note conj_and_using_reaches dobj_using_sets advmod_using_when rcmod_%_reaches rcmod_%_using num_%_89.39 cop_%_is nsubj_%_performance det_performance_the nn_sets_feature amod_sets_different prep_with_data_sets nn_data_test det_data_the amod_ranker_statistical poss_ranker_our prep_on_accuracy_data prep_of_accuracy_ranker det_accuracy_the parataxis_summarizes_% dobj_summarizes_accuracy nsubj_summarizes_Table num_Table_2 nn_Table_accuracy amod_Table_ranking nn_Table_Speaker
P05-1017	J96-1002	p	Another interesting point is the relation to maximum entropy model -LRB- Berger et al. 1996 -RRB- which is popular in the natural language processing community	nn_community_processing nn_community_language amod_community_natural det_community_the prep_in_popular_community cop_popular_is nsubj_popular_which amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum rcmod_relation_popular dep_relation_Berger prep_to_relation_model det_relation_the cop_relation_is nsubj_relation_point amod_point_interesting det_point_Another
P05-1020	J96-1002	o	We consider three learning algorithms namely the C4 .5 decision tree induction system -LRB- Quinlan 1993 -RRB- the RIPPER rule learning algorithm -LRB- Cohen 1995 -RRB- and maximum entropy classification -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_classification_entropy nn_classification_maximum amod_Cohen_1995 appos_algorithm_Cohen amod_algorithm_learning dep_rule_algorithm nn_rule_RIPPER det_rule_the amod_Quinlan_1993 dep_system_Berger conj_and_system_classification appos_system_rule dep_system_Quinlan nn_system_induction nn_system_tree nn_system_decision num_system_.5 nn_system_C4 det_system_the conj_algorithms_classification conj_algorithms_system advmod_algorithms_namely amod_algorithms_learning num_algorithms_three dobj_consider_algorithms nsubj_consider_We
P05-1027	J96-1002	o	216 The Maximum Entropy Principle -LRB- Berger et al. 1996 -RRB- is to nd a model p = argmax pC H -LRB- p -RRB- which means a probability model p -LRB- y | x -RRB- that maximizes entropy H -LRB- p -RRB-	appos_H_p nn_H_entropy dobj_maximizes_H nsubj_maximizes_that num_x_| nn_x_y rcmod_p_maximizes appos_p_x nn_p_model nn_p_probability det_p_a dobj_means_p nsubj_means_which appos_H_p nn_H_pC nn_H_argmax dep_=_H rcmod_p_means amod_p_= dep_model_p dep_a_model dobj_nd_a aux_nd_to xcomp_is_nd nsubj_is_Principle amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_Principle_Berger nn_Principle_Entropy nn_Principle_Maximum det_Principle_The num_Principle_216 ccomp_``_is
P05-1031	J96-1002	o	MAXENT Zhang Les C++ implementation8 of maximum entropy modelling -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_modelling_entropy nn_modelling_maximum dep_implementation8_Berger prep_of_implementation8_modelling nn_implementation8_C++ nn_implementation8_Les nn_implementation8_Zhang dep_MAXENT_implementation8
P05-1037	J96-1002	p	5.4 Maximum Entropy Maximum entropy has been proven to be an effective method in various natural language processing applications -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_applications_processing nn_applications_language amod_applications_natural amod_applications_various prep_in_method_applications amod_method_effective det_method_an cop_method_be aux_method_to dep_proven_Berger xcomp_proven_method auxpass_proven_been aux_proven_has nsubjpass_proven_entropy nn_entropy_Maximum nn_entropy_Entropy nn_entropy_Maximum num_entropy_5.4
P05-1057	J96-1002	o	Statistical approaches which depend on a set of unknown parameters that are learned from training data try to describe the relationship between a bilingual sentence pair -LRB- Brown et al. 1993 Vogel and Ney 1996 -RRB-	num_Vogel_1996 conj_and_Vogel_Ney dep_al._Ney dep_al._Vogel num_al._1993 nn_al._et amod_al._Brown dep_pair_al. nn_pair_sentence amod_pair_bilingual det_pair_a prep_between_relationship_pair det_relationship_the dobj_describe_relationship aux_describe_to xcomp_try_describe nsubj_try_approaches nn_data_training prep_from_learned_data auxpass_learned_are nsubjpass_learned_that rcmod_parameters_learned amod_parameters_unknown prep_of_set_parameters det_set_a prep_on_depend_set nsubj_depend_which rcmod_approaches_depend amod_approaches_Statistical
P05-1057	J96-1002	o	Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages -LRB- Smadja et al. 1996 Ker and Chang 1997 Melamed 2000 -RRB-	amod_Melamed_2000 dep_Ker_Melamed amod_Ker_1997 conj_and_Ker_Chang dep_Smadja_Chang dep_Smadja_Ker appos_Smadja_1996 dep_Smadja_al. nn_Smadja_et dep_languages_Smadja num_languages_two det_languages_the prep_of_types_languages det_types_the prep_between_functions_types nn_functions_similarity amod_functions_various dobj_using_functions nn_alignments_word prepc_by_obtain_using dobj_obtain_alignments nsubj_obtain_approaches amod_approaches_Heuristic
P05-1057	J96-1002	p	An especially well-founded framework is maximum entropy -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_entropy_Berger amod_entropy_maximum cop_entropy_is nsubj_entropy_framework amod_framework_well-founded det_framework_An advmod_well-founded_especially
P05-1061	J96-1002	o	We use a standard maximum entropy classifier -LRB- Berger et al. 1996 -RRB- implemented as part of MALLET -LRB- McCallum 2002 -RRB-	amod_McCallum_2002 dep_MALLET_McCallum prep_of_part_MALLET prep_as_implemented_part amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_classifier_Berger nn_classifier_entropy nn_classifier_maximum amod_classifier_standard det_classifier_a dep_use_implemented dobj_use_classifier nsubj_use_We
P05-1066	J96-1002	o	For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems -LRB- e.g. see -LRB- Alshawi 1996 Wu 1997 Yamada and Knight 2001 Gildea 2003 Melamed 2004 Graehl and Knight 2004 Och et al. 2004 Xia and McCord 2004 -RRB- -RRB-	dep_Xia_2004 conj_and_Xia_McCord nn_al._et nn_al._Och num_Melamed_2004 num_Gildea_2003 dep_Wu_McCord dep_Wu_Xia num_Wu_2004 dep_Wu_al. num_Wu_2004 conj_and_Wu_Knight conj_and_Wu_Graehl conj_and_Wu_Melamed conj_and_Wu_Gildea num_Wu_2001 conj_and_Wu_Knight conj_and_Wu_Yamada num_Wu_1997 dep_Alshawi_Knight dep_Alshawi_Graehl dep_Alshawi_Melamed dep_Alshawi_Gildea dep_Alshawi_Knight dep_Alshawi_Yamada dep_Alshawi_Wu appos_Alshawi_1996 dep_see_Alshawi dep_e.g._see dep_systems_e.g. nn_systems_translation nn_systems_machine amod_systems_statistical prep_within_information_systems amod_information_syntactic dobj_incorporate_information nsubj_incorporate_which rcmod_methods_incorporate prep_in_interest_methods prep_of_deal_interest amod_deal_great det_deal_a advmod_deal_currently nsubj_is_deal expl_is_there prep_for_is_reason det_reason_this ccomp_``_is
P05-1066	J96-1002	o	2.1.2 Research on Syntax-Based SMT A number of researchers -LRB- Alshawi 1996 Wu 1997 Yamada and Knight 2001 Gildea 2003 Melamed 2004 Graehl and Knight 2004 Galley et al. 2004 -RRB- have proposed models where the translation process involves syntactic representations of the source and/or target languages	nn_languages_target conj_and/or_source_languages det_source_the prep_of_representations_languages prep_of_representations_source amod_representations_syntactic dobj_involves_representations nsubj_involves_process advmod_involves_where nn_process_translation det_process_the rcmod_models_involves amod_models_proposed dobj_have_models num_Galley_2004 nn_Galley_al. nn_Galley_et num_Graehl_2004 conj_and_Graehl_Knight num_Melamed_2004 num_Gildea_2003 num_Wu_1997 dep_Alshawi_Galley conj_and_Alshawi_Knight conj_and_Alshawi_Graehl conj_and_Alshawi_Melamed conj_and_Alshawi_Gildea conj_and_Alshawi_2001 conj_and_Alshawi_Knight conj_and_Alshawi_Yamada conj_and_Alshawi_Wu appos_Alshawi_1996 appos_researchers_Graehl appos_researchers_Melamed appos_researchers_Gildea appos_researchers_2001 appos_researchers_Knight appos_researchers_Yamada appos_researchers_Wu appos_researchers_Alshawi prep_of_number_researchers det_number_A nn_number_SMT amod_number_Syntax-Based dep_Research_have prep_on_Research_number num_Research_2.1.2 dep_``_Research
P05-1066	J96-1002	o	A number of other re532 searchers -LRB- Berger et al. 1996 Niessen and Ney 2004 Xia and McCord 2004 -RRB- have described previous work on preprocessing methods	amod_methods_preprocessing amod_work_previous prep_on_described_methods dobj_described_work aux_described_have nsubj_described_2004 nsubj_described_Ney nsubj_described_Niessen dep_Xia_2004 conj_and_Xia_McCord dep_Niessen_McCord dep_Niessen_Xia conj_and_Niessen_2004 conj_and_Niessen_Ney parataxis_Berger_described appos_Berger_1996 dep_Berger_al. nn_Berger_et nn_searchers_re532 amod_searchers_other dep_number_Berger prep_of_number_searchers det_number_A ccomp_``_number
P05-1066	J96-1002	o	-LRB- Berger et al. 1996 -RRB- describe an approach that targets translation of French phrases of the form NOUN de NOUN -LRB- e.g. conflit dinteret -RRB-	nn_dinteret_conflit dep_dinteret_e.g. dep_NOUN_dinteret amod_NOUN_de nn_NOUN_NOUN nn_NOUN_form det_NOUN_the prep_of_phrases_NOUN amod_phrases_French prep_of_translation_phrases dobj_targets_translation nsubj_targets_that rcmod_approach_targets det_approach_an dobj_describe_approach nsubj_describe_Berger amod_Berger_1996 dep_Berger_al. nn_Berger_et
P05-2024	J96-1002	o	We employ loglinear models -LRB- Berger et al. 1996 -RRB- for the disambiguation	det_disambiguation_the amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_models_loglinear prep_for_employ_disambiguation dep_employ_Berger dobj_employ_models nsubj_employ_We
P06-1026	J96-1002	o	However in order to cope with the prediction errors of the classi er we approximate a74a51a18a77a76 a28 with an a80 gram language model on sequences of the re ned tag labels a38a58a39 a41 a81 a43a82a44a47a46a83a48a47a50a75a44a15a52 a53a9a54a49a84 a53a9a54a83a84a49a85a9a86a13a87a89a88a91a90 a55a57a56 a38a40a39 a81 a59a60a42a61 -LRB- 2 -RRB- a92 a44a47a46a83a48a47a50a75a44a15a52 a53a9a54 a84 a53a9a54a83a84a49a85a9a86a13a87a89a88a91a90 a93 a94a96a95 a55a57a56a98a97a66a99 a95 a59a100a27a61 -LRB- 3 -RRB- In order to estimate the conditional distribution a101 a18a20a19a15a21 a1 a68 a72 a28 we use the general technique of choosing the maximum entropy -LRB- maxent -RRB- distribution that properly estimates the average of each feature over the training data -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_data_training det_data_the det_feature_each prep_over_average_data prep_of_average_feature det_average_the dobj_estimates_average advmod_estimates_properly nsubj_estimates_that rcmod_distribution_estimates nn_distribution_entropy appos_entropy_maxent nn_entropy_maximum det_entropy_the dobj_choosing_distribution prepc_of_technique_choosing amod_technique_general det_technique_the dobj_use_technique nsubj_use_we rcmod_a28_use nn_a28_a72 nn_a28_a68 nn_a28_a1 nn_a28_a18a20a19a15a21 nn_a28_a101 nn_a28_distribution amod_a28_conditional det_a28_the dobj_estimate_a28 aux_estimate_to dep_estimate_order mark_estimate_In dep_a59a100a27a61_Berger dep_a59a100a27a61_estimate appos_a59a100a27a61_3 nn_a59a100a27a61_a95 nn_a59a100a27a61_a55a57a56a98a97a66a99 num_a59a100a27a61_a94a96a95 nn_a59a100a27a61_a93 nn_a59a100a27a61_a53a9a54a83a84a49a85a9a86a13a87a89a88a91a90 nn_a59a100a27a61_a84 nn_a59a100a27a61_a53a9a54 nn_a59a100a27a61_a44a47a46a83a48a47a50a75a44a15a52 nn_a59a100a27a61_a92 num_a59a100a27a61_2 dep_a59a60a42a61_a59a100a27a61 nn_a59a60a42a61_a81 nn_a59a60a42a61_a38a40a39 num_a59a60a42a61_a55a57a56 nn_a59a60a42a61_a53a9a54a83a84a49a85a9a86a13a87a89a88a91a90 nn_a59a60a42a61_a53a9a54a49a84 nn_a59a60a42a61_a43a82a44a47a46a83a48a47a50a75a44a15a52 nn_a59a60a42a61_a81 nn_a59a60a42a61_a41 nn_a59a60a42a61_a38a58a39 nn_labels_tag dobj_ned_labels nsubj_ned_a28 det_re_the prep_of_sequences_re prep_on_model_sequences nn_model_language nn_model_gram nn_model_a80 det_model_an prep_with_a28_model nn_a28_a74a51a18a77a76 amod_a28_approximate dep_we_a59a60a42a61 rcmod_we_ned nn_er_classi det_er_the prep_of_errors_er nn_errors_prediction det_errors_the dobj_cope_we prep_with_cope_errors aux_cope_to dep_cope_order mark_cope_in advmod_cope_However
P06-1042	J96-1002	o	7However the algorithms shares many common points with iterative algorithm that are known to converge and that have been proposed to find maximum entropy probability distributions under a set of constraints -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_of_set_constraints det_set_a nn_distributions_probability nn_distributions_entropy amod_distributions_maximum prep_under_find_set dobj_find_distributions aux_find_to xcomp_proposed_find auxpass_proposed_been aux_proposed_have nsubjpass_proposed_that dep_converge_Berger conj_and_converge_proposed aux_converge_to xcomp_known_proposed xcomp_known_converge auxpass_known_are nsubjpass_known_that rcmod_algorithm_known amod_algorithm_iterative prep_with_points_algorithm amod_points_common amod_points_many dep_shares_points nn_shares_algorithms det_shares_the dep_7However_shares
P06-1071	J96-1002	o	2.1 Conditional Maximum Entropy Model The goal of CME is to find the most uniform conditional distribution of y given observation x -LRB- -RRB- xyp subject to constraints specified by a set of features -LRB- -RRB- yxf i where features typically take the value of either 0 or 1 -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_0_Berger conj_or_0_1 preconj_0_either prep_of_value_1 prep_of_value_0 det_value_the dobj_take_value advmod_take_typically nsubj_take_features advmod_take_where rcmod_i_take nn_i_yxf appos_i_xyp prep_of_set_features det_set_a agent_specified_set vmod_constraints_specified prep_to_subject_constraints amod_xyp_subject nn_x_observation pobj_given_x prep_y_given prep_of_distribution_y amod_distribution_conditional amod_distribution_uniform det_distribution_the advmod_uniform_most dobj_find_distribution aux_find_to xcomp_is_find nsubj_is_goal prep_of_goal_CME det_goal_The dep_Model_i rcmod_Model_is nn_Model_Entropy nn_Model_Maximum amod_Model_Conditional num_Model_2.1 dep_``_Model
P06-1071	J96-1002	o	This leads to a good amount of work in this area -LRB- Ratnaparkhi et al. 1994 Berger et al. 1996 Pietra et al 1997 Zhou et al. 2003 Riezler and Vasserman 2004 -RRB- In the most basic approach such as Ratnaparkhi et al.	nn_al._et nn_al._Ratnaparkhi prep_such_as_approach_al. amod_approach_basic amod_approach_most det_approach_the num_Riezler_2004 conj_and_Riezler_Vasserman num_al._2003 nn_al._et nn_al._Zhou num_al_1997 nn_al_et nn_al_Pietra num_Berger_1996 nn_Berger_al. nn_Berger_et dep_al._Vasserman dep_al._Riezler conj_al._al. conj_al._al conj_al._Berger conj_al._1994 nn_al._et prep_in_Ratnaparkhi_approach dep_Ratnaparkhi_al. det_area_this prep_in_amount_area prep_of_amount_work amod_amount_good det_amount_a dep_leads_Ratnaparkhi prep_to_leads_amount nsubj_leads_This ccomp_``_leads
P06-1071	J96-1002	o	1 Introduction Conditional Maximum Entropy -LRB- CME -RRB- modeling has received a great amount of attention within natural language processing community for the past decade -LRB- e.g. Berger et al. 1996 Reynar and Ratnaparkhi 1997 Koeling 2000 Malouf 2002 Zhou et al. 2003 Riezler and Vasserman 2004 -RRB-	dep_Riezler_2004 conj_and_Riezler_Vasserman num_Zhou_2003 nn_Zhou_al. nn_Zhou_et num_Malouf_2002 num_Koeling_2000 dep_Reynar_Vasserman dep_Reynar_Riezler conj_and_Reynar_Zhou conj_and_Reynar_Malouf conj_and_Reynar_Koeling conj_and_Reynar_1997 conj_and_Reynar_Ratnaparkhi dep_Berger_Zhou dep_Berger_Malouf dep_Berger_Koeling dep_Berger_1997 dep_Berger_Ratnaparkhi dep_Berger_Reynar num_Berger_1996 nn_Berger_al. nn_Berger_et dep_e.g._Berger amod_decade_past det_decade_the nn_community_processing nn_community_language amod_community_natural prep_of_amount_attention amod_amount_great det_amount_a dep_received_e.g. prep_for_received_decade prep_within_received_community dobj_received_amount aux_received_has nsubj_received_modeling nn_modeling_Entropy appos_Entropy_CME nn_Entropy_Maximum amod_Entropy_Conditional nn_Entropy_Introduction num_Entropy_1
P06-1073	J96-1002	o	579 The MaxEnt algorithm associates a set of weights -LRB- ij -RRB- i = 1nj = 1m with the features which are estimated during the training phase to maximize the likelihood of the data -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et det_data_the prep_of_likelihood_data det_likelihood_the dobj_maximize_likelihood aux_maximize_to vmod_phase_maximize nn_phase_training det_phase_the prep_during_estimated_phase auxpass_estimated_are nsubjpass_estimated_which dep_features_Berger rcmod_features_estimated det_features_the prep_with_1m_features dep_=_1m amod_1nj_= dep_=_1nj dep_i_= dep_weights_i appos_weights_ij prep_of_set_weights det_set_a dobj_associates_set nsubj_associates_algorithm nn_algorithm_MaxEnt det_algorithm_The num_algorithm_579
P06-1073	J96-1002	o	Our appoach is based on Maximum Entropy -LRB- MaxEnt henceforth -RRB- technique -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_technique_Berger nn_technique_henceforth nn_technique_MaxEnt dep_Entropy_technique nn_Entropy_Maximum prep_on_based_Entropy auxpass_based_is nsubjpass_based_appoach poss_appoach_Our ccomp_``_based
P06-1089	J96-1002	o	There have been many studies on POS guessing of unknown words -LRB- Mori and Nagao 1996 Mikheev 1997 Chen et al. 1997 Nagata 1999 Orphanos and Christodoulakis 1999 -RRB-	amod_Orphanos_1999 conj_and_Orphanos_Christodoulakis num_Nagata_1999 num_Chen_1997 nn_Chen_al. nn_Chen_et dep_Mikheev_Christodoulakis dep_Mikheev_Orphanos conj_Mikheev_Nagata conj_Mikheev_Chen conj_Mikheev_1997 conj_and_Mori_Mikheev conj_and_Mori_1996 conj_and_Mori_Nagao dep_words_Mikheev dep_words_1996 dep_words_Nagao dep_words_Mori amod_words_unknown prep_of_guessing_words vmod_POS_guessing prep_on_studies_POS amod_studies_many cop_studies_been aux_studies_have expl_studies_There
P06-1089	J96-1002	o	p0 -LRB- t | w -RRB- is calculated by ME models as follows -LRB- Berger et al. 1996 -RRB- p0 -LRB- t | w -RRB- = 1Y -LRB- w -RRB- exp braceleftBigg Hsummationdisplay h = 1 hgh -LRB- w t -RRB- bracerightBigg -LRB- 20 -RRB- 709 Language Features English Prefixes of 0 up to four characters suffixes of 0 up to four characters 0 contains Arabic numerals 0 contains uppercase characters 0 contains hyphens	dobj_contains_hyphens nsubj_contains_0 rcmod_characters_contains amod_characters_uppercase dobj_contains_characters num_numerals_0 amod_numerals_Arabic dep_contains_contains dobj_contains_numerals nsubj_contains_p0 num_characters_four pobj_to_characters pcomp_up_to prep_suffixes_up prep_of_suffixes_0 num_characters_four pobj_to_characters pcomp_up_to prep_of_Prefixes_0 nn_Prefixes_English prep_Features_up dobj_Features_Prefixes nsubj_Features_Language num_Language_709 dep_bracerightBigg_Features appos_bracerightBigg_20 nn_bracerightBigg_hgh appos_w_t dep_hgh_w num_hgh_1 dep_=_bracerightBigg appos_h_0 appos_h_suffixes amod_h_= nn_h_Hsummationdisplay nn_h_braceleftBigg nn_h_exp nn_h_1Y appos_1Y_w dep_=_h num_w_| nn_w_t amod_p0_= appos_p0_w amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_follows_Berger mark_follows_as nn_models_ME parataxis_calculated_contains advcl_calculated_follows agent_calculated_models auxpass_calculated_is nsubjpass_calculated_p0 num_w_| nn_w_t appos_p0_w
P06-1089	J96-1002	o	The features we use are shown in Table 2 which are based on the features used by Ratnaparkhi -LRB- 1996 -RRB- and Uchimoto et al.	nn_al._et nn_al._Uchimoto conj_and_Ratnaparkhi_al. appos_Ratnaparkhi_1996 agent_used_al. agent_used_Ratnaparkhi vmod_features_used det_features_the prep_on_based_features auxpass_based_are nsubjpass_based_which rcmod_Table_based num_Table_2 prep_in_shown_Table auxpass_shown_are nsubjpass_shown_features nsubj_use_we rcmod_features_use det_features_The
P06-1112	J96-1002	p	-LRB- Berger et al. 1996 -RRB- gave a good description of ME model	nn_model_ME prep_of_description_model amod_description_good det_description_a dobj_gave_description nsubj_gave_Berger amod_Berger_1996 dep_Berger_al. nn_Berger_et
P06-1129	J96-1002	p	The maximum entropy model -LRB- Berger et al. 1996 -RRB- provides us with a well-founded framework for this purpose which has been extensively used in natural lan guage processing tasks ranging from part-ofspeech tagging to machine translation	nn_translation_machine amod_tagging_part-ofspeech prep_to_ranging_translation prep_from_ranging_tagging vmod_tasks_ranging nn_tasks_processing nn_tasks_guage nn_tasks_lan amod_tasks_natural prep_in_used_tasks advmod_used_extensively auxpass_used_been aux_used_has nsubjpass_used_which det_purpose_this rcmod_framework_used prep_for_framework_purpose amod_framework_well-founded det_framework_a prep_with_provides_framework dobj_provides_us nsubj_provides_model amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_model_Berger nn_model_entropy nn_model_maximum det_model_The
P06-2018	J96-1002	o	4.2 Cast3LB Function Tagging For the task of Cast3LB function tag assignment we experimented with three generic machine learning algorithms a memory-based learner -LRB- Daelemans and van den Bosch 2005 -RRB- a maximum entropy classifier -LRB- Berger et al. 1996 -RRB- and a Support Vector Machine classifier -LRB- Vapnik 1998 -RRB-	amod_Vapnik_1998 dep_classifier_Vapnik nn_classifier_Machine nn_classifier_Vector nn_classifier_Support det_classifier_a amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_classifier_classifier dep_classifier_Berger nn_classifier_entropy nn_classifier_maximum det_classifier_a nn_Bosch_den nn_Bosch_van dep_Daelemans_2005 conj_and_Daelemans_Bosch appos_learner_classifier appos_learner_classifier appos_learner_Bosch appos_learner_Daelemans amod_learner_memory-based det_learner_a dobj_learning_algorithms vmod_machine_learning amod_machine_generic num_machine_three prep_with_experimented_machine nsubj_experimented_we nn_assignment_tag nn_assignment_function nn_assignment_Cast3LB prep_of_task_assignment det_task_the dep_Tagging_learner rcmod_Tagging_experimented prep_for_Tagging_task nn_Tagging_Function nn_Tagging_Cast3LB num_Tagging_4.2 dep_``_Tagging
P06-2063	J96-1002	o	Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_possible_Berger dep_possible_as dep_is_possible prep_as_is_uniform advmod_is_otherwise nsubj_is_that det_evidence_the agent_imposed_evidence vmod_constraints_imposed prep_of_set_constraints det_set_the conj_but_consistent_is prep_with_consistent_set cop_consistent_is nsubj_consistent_that rcmod_one_is rcmod_one_consistent det_one_the cop_one_is nsubj_one_model mark_one_that amod_model_best det_model_the ccomp_intuition_one det_intuition_the dobj_implement_intuition nsubj_implement_models nn_models_Entropy nn_models_Maximum
P06-2089	J96-1002	o	One such approach is maximum entropy classification -LRB- Berger et al. 1996 -RRB- which we use in the form of a library implemented by Tsuruoka1 and used in his classifier-based parser -LRB- Tsuruoka and Tsujii 2005 -RRB-	amod_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii dep_parser_Tsujii dep_parser_Tsuruoka amod_parser_classifier-based poss_parser_his prep_in_used_parser conj_and_Tsuruoka1_used agent_implemented_used agent_implemented_Tsuruoka1 vmod_library_implemented det_library_a prep_of_form_library det_form_the prep_in_use_form nsubj_use_we dobj_use_which amod_Berger_1996 dep_Berger_al. nn_Berger_et rcmod_classification_use dep_classification_Berger amod_classification_entropy amod_classification_maximum cop_classification_is nsubj_classification_approach amod_approach_such num_approach_One
P06-2093	J96-1002	o	Several algorithms have been proposed in the literature that try to find the best splits see for instance -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_for_see_instance dep_splits_Berger conj_splits_see nsubj_splits_best det_best_the ccomp_find_splits aux_find_to xcomp_try_find nsubj_try_that rcmod_literature_try det_literature_the prep_in_proposed_literature auxpass_proposed_been aux_proposed_have nsubjpass_proposed_algorithms amod_algorithms_Several
P06-2109	J96-1002	o	2.2 Maximum Entropy Model The maximum entropy model -LRB- Berger et al. 1996 -RRB- estimates a probability distribution from training data	nn_data_training prep_from_distribution_data nn_distribution_probability det_distribution_a dobj_estimates_distribution amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_model_estimates dep_model_Berger nn_model_entropy nn_model_maximum nn_model_The nn_model_Model nn_model_Entropy nn_model_Maximum num_model_2.2
P07-1020	J96-1002	o	This logistic regression is also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_data_training det_data_the det_feature_each prep_over_average_data prep_of_average_feature det_average_the dobj_estimates_average advmod_estimates_properly nsubj_estimates_that rcmod_entropy_estimates nn_entropy_maximum prep_with_distribution_entropy det_distribution_the dobj_finds_distribution nsubj_finds_it mark_finds_as dep_Maxent_Berger dep_Maxent_finds dep_called_Maxent advmod_called_also auxpass_called_is nsubjpass_called_regression amod_regression_logistic det_regression_This ccomp_``_called
P07-1079	J96-1002	o	The disambiguation model of this parser is based on a maximum entropy model -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_a dep_based_Berger prep_on_based_model auxpass_based_is nsubjpass_based_model det_parser_this prep_of_model_parser nn_model_disambiguation det_model_The
P07-1079	J96-1002	o	1 Introduction Several efficient accurate and robust approaches to data-driven dependency parsing have been proposed recently -LRB- Nivre and Scholz 2004 McDonald et al. 2005 Buchholz and Marsi 2006 -RRB- for syntactic analysis of natural language using bilexical dependency relations -LRB- Eisner 1996 -RRB-	dep_Eisner_1996 dep_relations_Eisner nn_relations_dependency amod_relations_bilexical dobj_using_relations amod_language_natural prep_of_analysis_language nn_analysis_syntactic dep_Buchholz_2006 conj_and_Buchholz_Marsi num_McDonald_2005 nn_McDonald_al. nn_McDonald_et dep_Nivre_Marsi dep_Nivre_Buchholz conj_and_Nivre_McDonald conj_and_Nivre_2004 conj_and_Nivre_Scholz xcomp_proposed_using prep_for_proposed_analysis dep_proposed_McDonald dep_proposed_2004 dep_proposed_Scholz dep_proposed_Nivre advmod_proposed_recently auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Introduction nn_parsing_dependency amod_parsing_data-driven prep_to_approaches_parsing amod_approaches_robust amod_approaches_accurate amod_approaches_efficient conj_and_efficient_robust conj_and_efficient_accurate amod_efficient_Several amod_Introduction_approaches num_Introduction_1
P07-1096	J96-1002	o	Following -LRB- Ratnaparkhi 1996 Collins 2002 Toutanova et al. 2003 Tsuruoka and Tsujii 2005 -RRB- 765 Feature Sets Templates Error % A Ratnaparkhis 3.05 B A + -LSB- t0 t1 -RSB- -LSB- t0 t1 t1 -RSB- -LSB- t0 t1 t2 -RSB- 2.92 C B + -LSB- t0 t2 -RSB- -LSB- t0 t2 -RSB- -LSB- t0 t2 w0 -RSB- -LSB- t0 t1 w0 -RSB- -LSB- t0 t1 w0 -RSB- -LSB- t0 t2 w0 -RSB- -LSB- t0 t2 t1 w0 -RSB- -LSB- t0 t1 t1 w0 -RSB- -LSB- t0 t1 t2 w0 -RSB- 2.84 D C + -LSB- t0 w1 w0 -RSB- -LSB- t0 w1 w0 -RSB- 2.78 E D + -LSB- t0 X = prefix or suffix of w0 -RSB- ,4 < | X | 9 2.72 Table 2 Experiments on the development data with beam width of 3 we cut the PTB into the training development and test sets as shown in Table 1	num_Table_1 prep_in_shown_Table mark_shown_as nn_sets_test conj_and_training_sets conj_and_training_development det_training_the det_PTB_the advcl_cut_shown prep_into_cut_sets prep_into_cut_development prep_into_cut_training dobj_cut_PTB nsubj_cut_we prep_of_width_3 nn_width_beam prep_with_data_width nn_data_development det_data_the rcmod_Experiments_cut prep_on_Experiments_data num_Table_2 dep_Table_2.72 num_Table_| number_2.72_9 dep_X_Table num_X_| dep_<_X dep_,4_< prep_of_suffix_w0 conj_or_prefix_suffix amod_prefix_= npadvmod_=_X conj_+_D_t0 nn_D_E num_D_2.78 appos_t0_w0 appos_t0_w1 nn_C_D num_C_2.84 nn_C_w0 appos_t0_w0 conj_+_t0_w1 conj_+_t0_t0 conj_+_t0_C conj_+_t0_t2 conj_+_t0_t1 appos_t0_w0 conj_t0_t1 conj_t0_t1 appos_t0_w0 conj_t0_t1 conj_t0_t2 appos_t0_w0 appos_t0_t2 appos_t0_w0 appos_t0_t1 appos_t0_w0 appos_t0_t1 appos_t0_w0 appos_t0_t2 appos_t0_t2 appos_t0_t2 conj_+_B_t0 nn_B_C num_B_2.92 dep_t0_Experiments amod_t0_,4 dep_t0_suffix dep_t0_prefix dep_t0_t0 dep_t0_D dep_t0_t0 dep_t0_w1 dep_t0_t0 dep_t0_C dep_t0_t2 dep_t0_t1 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_t0 dep_t0_B appos_t0_t2 appos_t0_t1 appos_t0_t1 appos_t0_t1 appos_t0_t1 conj_+_A_t0 nn_A_B num_A_3.05 nn_A_Ratnaparkhis nn_A_A nn_A_% nn_A_Error nn_A_Templates dep_Sets_t0 dep_Sets_t0 dobj_Sets_t0 dobj_Sets_A nsubj_Sets_Feature prep_Sets_Following num_Feature_765 dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii num_Toutanova_2003 nn_Toutanova_al. nn_Toutanova_et num_Collins_2002 dep_Ratnaparkhi_Tsujii dep_Ratnaparkhi_Tsuruoka dep_Ratnaparkhi_Toutanova dep_Ratnaparkhi_Collins dep_Ratnaparkhi_1996 dep_Following_Ratnaparkhi
P07-1096	J96-1002	o	766 System Beam Error % -LRB- Ratnaparkhi 1996 -RRB- 5 3.37 -LRB- Tsuruoka and Tsujii 2005 -RRB- 1 2.90 -LRB- Collins 2002 -RRB- 2.89 Guided Learning feature B 3 2.85 -LRB- Tsuruoka and Tsujii 2005 -RRB- all 2.85 -LRB- Gimenez and M`arquez 2004 -RRB- 2.84 -LRB- Toutanova et al. 2003 -RRB- 2.76 Guided Learning feature E 1 2.73 Guided Learning feature E 3 2.67 Table 4 Comparison with the previous works According to the experiments shown above we build our best system by using feature set E with beam width B = 3	dobj_=_3 nsubj_=_B nn_B_width nn_B_beam nn_E_set nn_E_feature prepc_with_using_= dobj_using_E amod_system_best poss_system_our prepc_by_build_using dobj_build_system nsubj_build_we advmod_shown_above vmod_experiments_shown det_experiments_the pobj_works_experiments prepc_according_to_works_to amod_works_previous det_works_the rcmod_Comparison_build prep_with_Comparison_works num_Table_4 num_Table_2.67 num_Table_3 nn_Table_E nn_Table_feature amod_Learning_Guided nn_Learning_E nn_Learning_feature number_2.73_1 num_E_2.73 amod_Learning_Guided num_Learning_2.76 nn_Learning_B nn_Learning_feature amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_Gimenez_2004 conj_and_Gimenez_M`arquez det_2.85_all dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii number_2.85_3 dep_B_Toutanova num_B_2.84 dep_B_M`arquez dep_B_Gimenez num_B_2.85 appos_B_Tsujii appos_B_Tsuruoka num_B_2.85 amod_Learning_Guided num_Learning_2.89 dep_Collins_2002 num_Collins_2.90 number_2.90_1 dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii num_Tsuruoka_3.37 number_3.37_5 dep_Ratnaparkhi_1996 dep_%_Comparison appos_%_Table appos_%_Learning dep_%_Learning dep_%_Learning dep_%_Collins dep_%_Tsujii dep_%_Tsuruoka dep_%_Ratnaparkhi nn_%_Error nn_%_Beam nn_%_System num_%_766 dep_``_%
P07-1113	J96-1002	p	Weusemaximumentropy models -LRB- Berger et al. 1996 -RRB- which are particularly well-suited for tasks -LRB- like ours -RRB- with many overlapping features to harness these linguistic insights by using features in our models which encode directly or indirectly the linguistic correlates to SE types	nn_types_SE prep_to_correlates_types nsubj_correlates_linguistic det_linguistic_the conj_or_directly_indirectly advmod_encode_indirectly advmod_encode_directly nsubj_encode_which rcmod_models_encode poss_models_our prep_in_using_models dobj_using_features rcmod_insights_correlates prepc_by_insights_using amod_insights_linguistic det_insights_these dobj_harness_insights aux_harness_to amod_features_overlapping amod_features_many dep_-RRB-_harness prep_with_-RRB-_features prep_like_-LRB-_ours prep_for_well-suited_tasks advmod_well-suited_particularly cop_well-suited_are nsubj_well-suited_which amod_Berger_1996 dep_Berger_al. nn_Berger_et rcmod_models_well-suited appos_models_Berger amod_models_Weusemaximumentropy
P08-1002	J96-1002	o	For classi cation we use a maximum entropy model -LRB- Berger et al. 1996 -RRB- from the logistic regression package in Weka -LRB- Witten and Frank 2005 -RRB- with all default parameter settings	nn_settings_parameter nn_settings_default det_settings_all num_Witten_2005 conj_and_Witten_Frank appos_Weka_Frank appos_Weka_Witten prep_in_package_Weka nn_package_regression amod_package_logistic det_package_the amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_a prep_with_use_settings prep_from_use_package dep_use_Berger dobj_use_model nsubj_use_we prep_for_use_cation nn_cation_classi
P08-1033	J96-1002	o	2.4 Maximum Entropy Classifier Maximum Entropy Models -LRB- Berger et al. 1996 -RRB- seek to maximise the conditional probability of classes given certain observations -LRB- features -RRB-	appos_observations_features amod_observations_certain pobj_given_observations prep_of_probability_classes amod_probability_conditional det_probability_the dobj_maximise_probability aux_maximise_to xcomp_seek_maximise amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_Models_given vmod_Models_seek dep_Models_Berger nn_Models_Entropy nn_Models_Maximum nn_Models_Classifier nn_Models_Entropy nn_Models_Maximum num_Models_2.4
P08-1056	J96-1002	o	These belong to two main categories based on machine learning -LRB- Bikel et al. 1997 Borthwick 1999 McCallum and Li 2003 -RRB- and language or domain specific rules -LRB- Grishman 1995 Wakao et al. 1996 -RRB-	num_Wakao_1996 nn_Wakao_al. nn_Wakao_et dep_Grishman_Wakao dep_Grishman_1995 amod_rules_specific nn_rules_domain amod_McCallum_2003 conj_and_McCallum_Li dep_Borthwick_Grishman conj_or_Borthwick_rules conj_and_Borthwick_language dep_Borthwick_Li dep_Borthwick_McCallum num_Borthwick_1999 dep_Bikel_rules dep_Bikel_language dep_Bikel_Borthwick appos_Bikel_1997 dep_Bikel_al. nn_Bikel_et nn_learning_machine prep_on_based_learning vmod_categories_based amod_categories_main num_categories_two dep_belong_Bikel prep_to_belong_categories nsubj_belong_These ccomp_``_belong
P08-1056	J96-1002	o	Given a set of features and a training corpus the MaxEnt estimation process produces a model in which every feature fi has a weight i We can compute the conditional probability as -LRB- Berger et al. 1996 -RRB- p -LRB- o | h -RRB- = 1Z -LRB- h -RRB- productdisplay i ifi -LRB- h o -RRB- -LRB- 1 -RRB- Z -LRB- h -RRB- = summationdisplay o productdisplay i ifi -LRB- h o -RRB- -LRB- 2 -RRB- The conditional probability of the outcome is the product of the weights of all active features normalized over the products of all the features	det_features_the predet_features_all prep_of_products_features det_products_the prep_over_normalized_products amod_features_active det_features_all prep_of_weights_features det_weights_the vmod_product_normalized prep_of_product_weights det_product_the cop_product_is nsubj_product_p det_outcome_the prep_of_probability_outcome amod_probability_conditional det_probability_The dep_probability_2 nn_probability_ifi appos_h_o dep_ifi_h nn_ifi_i nn_ifi_productdisplay nn_ifi_o nn_ifi_summationdisplay dobj_=_probability nsubj_=_Z dep_=_1 appos_Z_h appos_h_o dep_ifi_h nn_ifi_i rcmod_productdisplay_= dep_productdisplay_ifi nn_productdisplay_1Z appos_1Z_h dobj_=_productdisplay num_h_| dep_o_h amod_p_= appos_p_o amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_as_Berger amod_probability_conditional det_probability_the parataxis_compute_product prep_compute_as dobj_compute_probability aux_compute_can nsubj_compute_We dep_weight_i det_weight_a dobj_has_weight nsubj_has_fi prep_in_has_which nn_fi_feature det_fi_every rcmod_model_has det_model_a parataxis_produces_compute dobj_produces_model nsubj_produces_process prep_produces_Given nn_process_estimation nn_process_MaxEnt det_process_the nn_corpus_training det_corpus_a conj_and_set_corpus prep_of_set_features det_set_a pobj_Given_corpus pobj_Given_set
P08-1115	J96-1002	o	Formally the approach we take can be thought of as a noisier channel where an observed signal o gives rise to a set of source-language strings fprime F -LRB- o -RRB- and we seek e = arg maxe max fprimeF -LRB- o -RRB- Pr -LRB- e fprime | o -RRB- -LRB- 2 -RRB- = arg maxe max fprimeF -LRB- o -RRB- Pr -LRB- e -RRB- Pr -LRB- fprime | e o -RRB- -LRB- 3 -RRB- = arg maxe max fprimeF -LRB- o -RRB- Pr -LRB- e -RRB- Pr -LRB- fprime | e -RRB- Pr -LRB- o | fprime -RRB- -LRB- 4 -RRB- Following Och and Ney -LRB- 2002 -RRB- we use the maximum entropy framework -LRB- Berger et al. 1996 -RRB- to directly model the posterior Pr -LRB- e fprime | o -RRB- with parameters tuned to minimize a loss function representing 1012 the quality only of the resulting translations	amod_translations_resulting det_translations_the prep_of_quality_translations advmod_quality_only det_quality_the num_quality_1012 dobj_representing_quality vmod_function_representing nn_function_loss det_function_a dobj_minimize_function aux_minimize_to xcomp_tuned_minimize vmod_parameters_tuned nn_o_| amod_o_fprime dep_o_e appos_Pr_o amod_Pr_posterior det_Pr_the prep_with_model_parameters dobj_model_Pr advmod_model_directly aux_model_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_framework_entropy nn_framework_maximum det_framework_the xcomp_use_model dep_use_Berger dobj_use_framework nsubj_use_we prep_following_use_Ney prep_following_use_Och dep_use_4 dep_use_Pr dep_use_= dep_use_3 dep_use_Pr appos_Ney_2002 conj_and_Och_Ney dep_4_fprime num_fprime_| dep_o_fprime appos_Pr_o dep_Pr_e num_Pr_| dep_fprime_Pr dep_Pr_e nn_Pr_Pr nn_Pr_fprimeF appos_fprimeF_o nn_fprimeF_max nn_fprimeF_maxe nn_fprimeF_arg appos_|_o dep_|_e amod_|_fprime dep_Pr_| dep_Pr_e nn_Pr_Pr nn_Pr_fprimeF appos_fprimeF_o nn_fprimeF_max nn_fprimeF_maxe nn_fprimeF_arg dep_=_use dep_=_2 nn_o_| amod_o_fprime dep_o_e rcmod_Pr_= appos_Pr_o nn_Pr_fprimeF appos_fprimeF_o nn_fprimeF_max nn_fprimeF_maxe nn_fprimeF_arg dobj_=_Pr dep_=_e ccomp_seek_= nsubj_seek_we appos_F_o conj_and_fprime_seek dobj_fprime_F amod_strings_source-language prep_of_set_strings det_set_a dep_gives_seek dep_gives_fprime prep_to_gives_set dobj_gives_rise nsubj_gives_o advmod_gives_where nn_o_signal amod_o_observed det_o_an rcmod_channel_gives amod_channel_noisier det_channel_a pobj_thought_channel prepc_as_of_thought_as auxpass_thought_be aux_thought_can nsubjpass_thought_approach advmod_thought_Formally nsubj_take_we rcmod_approach_take det_approach_the
P08-2001	J96-1002	o	Themodeling approachhere describedis discriminative and is based on maximum entropy -LRB- ME -RRB- models firstly applied to natural language problems in -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_in_Berger prep_problems_in nn_problems_language amod_problems_natural prep_to_applied_problems advmod_applied_firstly vmod_applied_based vmod_applied_Themodeling nn_models_entropy appos_entropy_ME nn_entropy_maximum prep_on_based_models auxpass_based_is amod_describedis_discriminative nn_describedis_approachhere conj_and_Themodeling_based dobj_Themodeling_describedis
P09-1005	J96-1002	o	For the identification and labeling steps we train a maximum entropy classifier -LRB- Berger et al. 1996 -RRB- over sections 02-21 of a version of the CCGbank corpus -LRB- Hockenmaier and Steedman 2007 -RRB- that has been augmented by projecting the Propbank semantic annotations -LRB- Boxwell and White 2008 -RRB-	amod_Boxwell_2008 conj_and_Boxwell_White dep_annotations_White dep_annotations_Boxwell amod_annotations_semantic nn_annotations_Propbank det_annotations_the dobj_projecting_annotations agent_augmented_projecting auxpass_augmented_been aux_augmented_has nsubjpass_augmented_that dep_Hockenmaier_2007 conj_and_Hockenmaier_Steedman appos_corpus_Steedman appos_corpus_Hockenmaier nn_corpus_CCGbank det_corpus_the rcmod_version_augmented prep_of_version_corpus det_version_a prep_of_sections_version num_sections_02-21 amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_classifier_Berger nn_classifier_entropy nn_classifier_maximum det_classifier_a prep_over_train_sections dobj_train_classifier nsubj_train_we prep_for_train_steps prep_for_train_identification nn_steps_labeling conj_and_identification_steps det_identification_the
P09-3007	J96-1002	o	4.1 Evaluation of Different Features and Models In pilot experiments on a subset of the features we provide a comparison of HM-SVM with other two learning models maximum entropy -LRB- MaxEnt -RRB- model -LRB- Berger et al. 1996 -RRB- and SVM model -LRB- Kudo 2001 -RRB- to test the effectiveness of HMSVM on function labeling task as well as the generality of our hypothesis on different learning 58 Table 3 Features used in each experiment round	nn_round_experiment det_round_each prep_in_used_round vmod_Features_used num_Table_3 num_Table_58 dep_learning_Table amod_learning_different prep_on_hypothesis_learning poss_hypothesis_our dep_generality_Features prep_of_generality_hypothesis det_generality_the nn_task_labeling nn_task_function prep_of_effectiveness_HMSVM det_effectiveness_the conj_and_test_generality prep_on_test_task dobj_test_effectiveness aux_test_to dep_test_model dep_test_model dep_test_provide amod_Kudo_2001 appos_model_Kudo nn_model_SVM amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_model_model dep_model_Berger nn_model_entropy appos_entropy_MaxEnt nn_entropy_maximum amod_models_learning num_models_two amod_models_other prep_with_comparison_models prep_of_comparison_HM-SVM det_comparison_a dobj_provide_comparison nsubj_provide_we nsubj_provide_Evaluation det_features_the prep_of_subset_features det_subset_a nn_experiments_pilot conj_and_Features_Models amod_Features_Different prep_on_Evaluation_subset prep_in_Evaluation_experiments prep_of_Evaluation_Models prep_of_Evaluation_Features num_Evaluation_4.1
P98-2140	J96-1002	o	We adopted the stop condition suggested in -LRB- Berger et al. 1996 -RRB- the maximization of the likelihood on a cross-validation set of samples which is unseen at the parameter estimation	nn_estimation_parameter det_estimation_the prep_at_unseen_estimation cop_unseen_is nsubj_unseen_which rcmod_samples_unseen prep_of_set_samples amod_set_cross-validation det_set_a det_likelihood_the prep_on_maximization_set prep_of_maximization_likelihood det_maximization_the dep_Berger_maximization amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_in_suggested_Berger nsubj_suggested_condition nn_condition_stop det_condition_the ccomp_adopted_suggested nsubj_adopted_We ccomp_``_adopted
P98-2140	J96-1002	o	To 848 make feature ranking computationally tractable in -LRB- Della Pietra et al. 1995 -RRB- and -LRB- Berger et al. 1996 -RRB- a simplified process proposed at the feature ranking stage when adding a new feature to the model all previously computed parameters are kept fixed and thus we have to fit only one new constraint imposed by the candidate feature	nn_feature_candidate det_feature_the agent_imposed_feature vmod_constraint_imposed amod_constraint_new num_constraint_one quantmod_one_only dobj_fit_constraint aux_fit_to xcomp_have_fit nsubj_have_we conj_and_fixed_have conj_and_fixed_thus dep_kept_have dep_kept_thus dep_kept_fixed auxpass_kept_are nsubjpass_kept_parameters advcl_kept_adding amod_parameters_computed advmod_parameters_previously det_parameters_all det_model_the amod_feature_new det_feature_a prep_to_adding_model dobj_adding_feature advmod_adding_when amod_stage_ranking nn_stage_feature det_stage_the ccomp_proposed_kept prep_at_proposed_stage vmod_process_proposed amod_process_simplified det_process_a dep_Berger_process amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_Pietra_Berger amod_Pietra_1995 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della prep_in_tractable_Berger prep_in_tractable_Pietra advmod_tractable_computationally amod_tractable_ranking nsubj_tractable_feature xcomp_make_tractable prep_to_make_848
P98-2140	J96-1002	o	We also do not require a newly added feature to be either atomic or a collocation of an atomic feature with a feature already included into the model as it was proposed in -LRB- Della Pietra et al. 1995 -RRB- -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_Pietra_1995 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della dep_in_Pietra dep_proposed_Berger prep_proposed_in auxpass_proposed_was nsubjpass_proposed_it mark_proposed_as det_model_the advmod_included_already nsubj_included_collocation nsubj_included_atomic det_feature_a prep_with_feature_feature amod_feature_atomic det_feature_an det_collocation_a prep_of_atomic_feature conj_or_atomic_collocation preconj_atomic_either cop_atomic_be aux_atomic_to rcmod_feature_included amod_feature_added det_feature_a advmod_added_newly advcl_require_proposed prep_into_require_model dobj_require_feature neg_require_not aux_require_do advmod_require_also nsubj_require_We ccomp_``_require
P98-2191	J96-1002	o	Therefore estimating a natural language model based on the maximum entropy -LRB- ME -RRB- method -LRB- Pietra et al. 1995 Berger et al. 1996 -RRB- has been highlighted recently	advmod_highlighted_recently auxpass_highlighted_been aux_highlighted_has nsubjpass_highlighted_estimating advmod_highlighted_Therefore num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Pietra_Berger appos_Pietra_1995 dep_Pietra_al. nn_Pietra_et nn_method_entropy appos_entropy_ME nn_entropy_maximum det_entropy_the prep_on_based_method vmod_model_based nn_model_language amod_model_natural det_model_a dep_estimating_Pietra dobj_estimating_model
P98-2191	J96-1002	o	Then to solve p. E C in equation -LRB- 8 -RRB- is equivalent to solve h. that maximize the loglikelihood = -LRB- x -RRB- log zj -LRB- z -RRB- + x i -LRB- 10 -RRB- h. = argmax kV -LRB- h -RRB- Such h. can be solved by one of the numerical algorithm called the Improved Iteratire Scaling Algorithm -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_Algorithm_Berger nn_Algorithm_Scaling nn_Algorithm_Iteratire nn_Algorithm_Improved det_Algorithm_the dep_called_Algorithm amod_algorithm_numerical det_algorithm_the prep_of_one_algorithm prep_solved_called agent_solved_one auxpass_solved_be aux_solved_can nsubjpass_solved_h. dep_solved_10 amod_h._Such nn_h._kV appos_kV_h nn_kV_argmax dep_=_h. amod_h._= dep_i_solved nn_zj_log conj_+_=_i conj_+_=_x appos_=_z dep_=_zj appos_=_x det_loglikelihood_the dobj_maximize_loglikelihood nsubj_maximize_that rcmod_h._maximize dobj_solve_h. aux_solve_to dep_equivalent_i dep_equivalent_x dep_equivalent_= xcomp_equivalent_solve cop_equivalent_is csubj_equivalent_solve appos_equation_8 nn_C_E nn_C_p. prep_in_solve_equation dobj_solve_C aux_solve_to advmod_solve_Then
P98-2191	J96-1002	o	We build a subset S C ~ incrementally by iterating to adjoin a feature f E ~ which maximizes loglikelihood of the model to S This algorithm is called the Basic Feature Selection -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_Selection_Berger nn_Selection_Feature amod_Selection_Basic det_Selection_the xcomp_called_Selection auxpass_called_is nsubjpass_called_algorithm det_algorithm_This det_model_the prep_of_loglikelihood_model prep_to_maximizes_S dobj_maximizes_loglikelihood nsubj_maximizes_which rcmod_~_maximizes nn_~_E nn_~_f nn_~_feature det_~_a dobj_adjoin_~ aux_adjoin_to xcomp_iterating_adjoin nn_~_C nn_~_S nn_~_subset det_~_a parataxis_build_called prepc_by_build_iterating advmod_build_incrementally dobj_build_~ nsubj_build_We ccomp_``_build
P98-2214	J96-1002	o	As a model learning method we adopt the maximum entropy model learning method -LRB- Della Pietra et al. 1997 Berger et al. 1996 -RRB-	num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Pietra_Berger appos_Pietra_1997 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della dep_method_Pietra nn_method_learning nn_method_model nn_method_entropy nn_method_maximum det_method_the dobj_adopt_method nsubj_adopt_we prep_as_adopt_method nn_method_learning nn_method_model det_method_a
W00-0704	J96-1002	o	We will provide a more detailed and systematic comparison between MAXIMUM ENTROPY MODELING -LRB- aatnaparkhi 1996 -RRB- and MEMORY BASED LEARNING -LRB- Daelemans et al. 1996 -RRB- for morpho-syntactic disambiguation and we investigate whether earlier observed differences in tagging accuracy can be attributed to algorithm bias information source issues or both	conj_or_issues_both nn_issues_source nn_issues_information ccomp_issues_investigate ccomp_issues_provide nn_bias_algorithm prep_to_attributed_bias auxpass_attributed_be aux_attributed_can nsubjpass_attributed_differences mark_attributed_whether amod_accuracy_tagging prep_in_differences_accuracy amod_differences_observed amod_differences_earlier ccomp_investigate_attributed nsubj_investigate_we amod_disambiguation_morpho-syntactic amod_Daelemans_1996 dep_Daelemans_al. nn_Daelemans_et dep_LEARNING_Daelemans nn_LEARNING_BASED nn_LEARNING_MEMORY amod_aatnaparkhi_1996 conj_and_MODELING_LEARNING dep_MODELING_aatnaparkhi nn_MODELING_ENTROPY nn_MODELING_MAXIMUM prep_for_comparison_disambiguation prep_between_comparison_LEARNING prep_between_comparison_MODELING amod_comparison_systematic amod_comparison_detailed det_comparison_a conj_and_detailed_systematic advmod_detailed_more conj_and_provide_investigate dobj_provide_comparison aux_provide_will nsubj_provide_We
W00-0704	J96-1002	o	A word is considered to be known when it has an ambiguous tag -LRB- henceforth ambitag -RRB- attributed to it in the LEXICON which is compiled in the same way as for the MBT-tagger -LRB- Daelemans et al. 1996 -RRB-	amod_Daelemans_1996 dep_Daelemans_al. nn_Daelemans_et det_MBT-tagger_the pobj_way_MBT-tagger prepc_as_for_way_for amod_way_same det_way_the prep_in_compiled_way auxpass_compiled_is nsubjpass_compiled_which dep_LEXICON_Daelemans rcmod_LEXICON_compiled det_LEXICON_the prep_in_attributed_LEXICON prep_to_attributed_it nsubj_attributed_tag nn_ambitag_henceforth appos_tag_ambitag amod_tag_ambiguous det_tag_an ccomp_has_attributed nsubj_has_it advmod_has_when auxpass_known_be aux_known_to advcl_considered_has xcomp_considered_known auxpass_considered_is nsubjpass_considered_word det_word_A ccomp_``_considered
W00-0707	J96-1002	o	In previous work -LRB- Foster 2000 -RRB- I described a Maximum Entropy/Minimum Divergence -LRB- MEMD -RRB- model -LRB- Berger et al. 1996 -RRB- for p -LRB- w \ -LSB- hi s -RRB- which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_translation nn_model_IBM amod_model_well-known det_model_the prep_of_analog_model det_analog_an cop_analog_is nsubj_analog_which rcmod_component_analog nn_component_translation det_component_a dep_model_Brown conj_and_model_component nn_model_language nn_model_trigram det_model_a dobj_incorporates_component dobj_incorporates_model nsubj_incorporates_which dep_hi_s nn_\_w nn_\_p amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_for_model_\ dep_model_Berger nn_model_Divergence appos_Divergence_MEMD nn_Divergence_Entropy/Minimum nn_Divergence_Maximum det_Divergence_a dep_described_incorporates dep_described_hi dobj_described_model nsubj_described_I dep_described_Foster prep_in_described_work dep_Foster_2000 amod_work_previous
W00-0707	J96-1002	o	For a given choice of q and f the IIS algorithm -LRB- Berger et al. 1996 -RRB- can be used to find maximum likelihood values for the parameters ~	nsubj_~_values det_parameters_the prep_for_values_parameters nn_values_likelihood amod_values_maximum ccomp_find_~ aux_find_to xcomp_used_find auxpass_used_be aux_used_can nsubjpass_used_algorithm dep_used_f amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_algorithm_Berger nn_algorithm_IIS det_algorithm_the prep_of_choice_q amod_choice_given det_choice_a conj_and_For_used pobj_For_choice dep_``_used dep_``_For
W00-0714	J96-1002	o	We have used the Improved Iterative Scaling algorithm -LRB- IIS -RRB- -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_algorithm_IIS amod_algorithm_Scaling nn_algorithm_Iterative nn_algorithm_Improved det_algorithm_the dep_used_Berger dobj_used_algorithm aux_used_have nsubj_used_We ccomp_``_used
W00-0729	J96-1002	o	In the last few years there has been an increasing interest in applying MaxEnt models for NLP applications -LRB- Ratnaparkhi 1998 Berger et al. 1996 Rosenfeld 1994 Ristad 1998 -RRB-	amod_Ristad_1998 num_Rosenfeld_1994 dep_Berger_Ristad conj_Berger_Rosenfeld num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Ratnaparkhi_Berger dep_Ratnaparkhi_1998 dep_applications_Ratnaparkhi nn_applications_NLP nn_models_MaxEnt prep_for_applying_applications dobj_applying_models prepc_in_interest_applying amod_interest_increasing det_interest_an cop_interest_been aux_interest_has nsubj_interest_there prep_in_interest_years amod_years_few amod_years_last det_years_the ccomp_``_interest
W01-0712	J96-1002	o	For every class the weights of the active features are combined and the best scoring class is chosen -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et auxpass_chosen_is nsubjpass_chosen_class amod_class_scoring amod_class_best det_class_the conj_and_combined_chosen auxpass_combined_are nsubjpass_combined_weights amod_features_active det_features_the prep_of_weights_features det_weights_the rcmod_class_chosen rcmod_class_combined det_class_every dep_For_Berger pobj_For_class dep_``_For
W02-0301	J96-1002	o	We use the maximum entropy tagging method described in -LRB- Kazama et al. 2001 -RRB- for the experiments which is a variant of -LRB- Ratnaparkhi 1996 -RRB- modified to use HMM state features	nn_features_state nn_features_HMM dobj_use_features aux_use_to xcomp_modified_use vmod_Ratnaparkhi_modified dep_Ratnaparkhi_1996 prep_of_variant_Ratnaparkhi det_variant_a cop_variant_is nsubj_variant_which rcmod_experiments_variant det_experiments_the prep_for_Kazama_experiments amod_Kazama_2001 dep_Kazama_al. nn_Kazama_et prep_in_described_Kazama vmod_method_described nn_method_tagging nn_method_entropy nn_method_maximum det_method_the dobj_use_method nsubj_use_We
W02-0301	J96-1002	p	Support Vector Machines -LRB- SVMs -RRB- -LRB- Vapnik 1995 -RRB- and Maximum Entropy -LRB- ME -RRB- method -LRB- Berger et al. 1996 -RRB- are powerful learning methods that satisfy such requirements and are applied successfully to other NLP tasks -LRB- Kudo and Matsumoto 2000 Nakagawa et al. 2001 Ratnaparkhi 1996 -RRB-	amod_Ratnaparkhi_1996 num_Nakagawa_2001 nn_Nakagawa_al. nn_Nakagawa_et dep_Kudo_Ratnaparkhi conj_and_Kudo_Nakagawa conj_and_Kudo_2000 conj_and_Kudo_Matsumoto dep_tasks_Nakagawa dep_tasks_2000 dep_tasks_Matsumoto dep_tasks_Kudo nn_tasks_NLP amod_tasks_other prep_to_applied_tasks advmod_applied_successfully auxpass_applied_are nsubjpass_applied_Machines amod_requirements_such dobj_satisfy_requirements nsubj_satisfy_that conj_and_methods_applied rcmod_methods_satisfy nn_methods_learning amod_methods_powerful cop_methods_are nsubj_methods_method nsubj_methods_Machines amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_method_Entropy appos_Entropy_ME nn_Entropy_Maximum dep_Vapnik_1995 dep_Machines_Berger conj_and_Machines_method appos_Machines_Vapnik appos_Machines_SVMs nn_Machines_Vector nn_Machines_Support
W02-0811	J96-1002	o	For the maximum entropy classifier we estimate the weights by maximizing the likelihood of a heldout set using the standard IIS algorithm -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_algorithm_IIS amod_algorithm_standard det_algorithm_the dep_using_Berger dobj_using_algorithm amod_set_heldout det_set_a prep_of_likelihood_set det_likelihood_the dobj_maximizing_likelihood det_weights_the xcomp_estimate_using prepc_by_estimate_maximizing dobj_estimate_weights nsubj_estimate_we prep_for_estimate_classifier nn_classifier_entropy nn_classifier_maximum det_classifier_the
W02-0813	J96-1002	o	Under the maximum entropy framework -LRB- Berger et al. 1996 -RRB- evidence from different features can be combined with no assumptions of feature independence	nn_independence_feature prep_of_assumptions_independence neg_assumptions_no prep_with_combined_assumptions auxpass_combined_be aux_combined_can nsubjpass_combined_evidence dep_combined_Berger prep_under_combined_framework amod_features_different prep_from_evidence_features amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_framework_entropy nn_framework_maximum det_framework_the rcmod_``_combined
W02-1002	J96-1002	o	Unconstrained CL corresponds exactly to a conditional maximum entropy model -LRB- Berger et al. 1996 Lafferty et al. 2001 -RRB-	num_Lafferty_2001 nn_Lafferty_al. nn_Lafferty_et dep_Berger_Lafferty appos_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum amod_model_conditional det_model_a dep_corresponds_Berger prep_to_corresponds_model advmod_corresponds_exactly nsubj_corresponds_CL amod_CL_Unconstrained
W02-1011	J96-1002	o	However feature/class functions are traditionally deflned as binary -LRB- Berger et al. 1996 -RRB- hence explicitly incorporating frequencies would require difierent functions for each count -LRB- or count bin -RRB- making training impractical	nsubj_impractical_training xcomp_making_impractical nn_bin_count cc_bin_or appos_count_bin det_count_each amod_functions_difierent xcomp_require_making prep_for_require_count dobj_require_functions aux_require_would csubj_require_incorporating advmod_require_hence dobj_incorporating_frequencies advmod_incorporating_explicitly amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_binary_Berger parataxis_deflned_require prep_as_deflned_binary advmod_deflned_traditionally auxpass_deflned_are nsubjpass_deflned_functions advmod_deflned_However amod_functions_feature/class ccomp_``_deflned
W02-1011	J96-1002	p	5.2 Maximum Entropy Maximum entropy classiflcation -LRB- MaxEnt or ME for short -RRB- is an alternative technique which has proven efiective in a number of natural language processing applications -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_applications_processing nn_applications_language amod_applications_natural prep_of_number_applications det_number_a prep_in_efiective_number acomp_proven_efiective aux_proven_has nsubj_proven_which dep_technique_Berger rcmod_technique_proven amod_technique_alternative det_technique_an cop_technique_is nsubj_technique_classiflcation prep_for_MaxEnt_short conj_or_MaxEnt_ME appos_classiflcation_ME appos_classiflcation_MaxEnt amod_classiflcation_entropy nn_classiflcation_Maximum nn_classiflcation_Entropy nn_classiflcation_Maximum num_classiflcation_5.2
W02-2018	J96-1002	o	A conditional maximum entropy model q -LRB- xjw -RRB- for p has the parametric form -LRB- Berger et al. 1996 Chi 1998 Johnson et al. 1999 -RRB- q -LRB- xjw -RRB- = exp T f -LRB- x -RRB- y2Y -LRB- w -RRB- exp -LRB- T f -LRB- y -RRB- -RRB- -LRB- 1 -RRB- where is a d-dimensional parameter vector and T f -LRB- x -RRB- is the inner product of the parameter vector and a feature vector	nn_vector_feature det_vector_a conj_and_vector_vector nn_vector_parameter det_vector_the prep_of_product_vector prep_of_product_vector amod_product_inner det_product_the cop_product_is nsubj_product_f nsubj_product_xjw dep_product_q appos_f_x nn_f_T appos_f_y nn_f_T dep_exp_f nn_exp_y2Y appos_y2Y_w nn_y2Y_f nn_y2Y_T appos_f_x nn_T_exp dep_=_vector dep_=_parameter dep_=_d-dimensional dep_=_a dep_=_is dep_=_where dep_=_1 dep_=_exp conj_and_xjw_f amod_xjw_= num_Johnson_1999 nn_Johnson_al. nn_Johnson_et num_Chi_1998 dep_Berger_Johnson dep_Berger_Chi appos_Berger_1996 dep_Berger_al. nn_Berger_et dep_form_product appos_form_Berger amod_form_parametric det_form_the dobj_has_form nsubj_has_q prep_for_q_p appos_q_xjw nn_q_model nn_q_entropy nn_q_maximum amod_q_conditional det_q_A
W02-2018	J96-1002	o	In natural language processing recent years have seen ME techniques used for sentence boundary detection part of speech tagging parse selection and ambiguity resolution and stochastic attribute-value grammars to name just a few applications -LRB- Abney 1997 Berger et al. 1996 Ratnaparkhi 1998 Johnson et al. 1999 -RRB-	num_Johnson_1999 nn_Johnson_al. nn_Johnson_et num_Ratnaparkhi_1998 num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Abney_Johnson conj_Abney_Ratnaparkhi conj_Abney_Berger dep_Abney_1997 dep_applications_Abney amod_applications_few det_applications_a advmod_applications_just dobj_name_applications aux_name_to vmod_grammars_name amod_grammars_attribute-value amod_grammars_stochastic nn_resolution_ambiguity conj_and_selection_resolution dobj_parse_resolution dobj_parse_selection nn_tagging_speech prep_of_part_tagging dep_detection_parse appos_detection_part nn_detection_boundary nn_detection_sentence prep_for_used_detection vmod_techniques_used nn_techniques_ME conj_and_seen_grammars dobj_seen_techniques aux_seen_have nsubj_seen_years prep_in_seen_processing amod_years_recent nn_processing_language amod_processing_natural
W02-2018	J96-1002	o	Finally it should be noted that in the current implementation we have not applied any of the possible optimizations that appear in the literature -LRB- Lafferty and Suhm 1996 Wu and Khudanpur 2000 Lafferty et al. 2001 -RRB- to speed up normalization of the probability distribution q These improvements take advantage of a models structure to simplify the evaluation of the denominator in -LRB- 1 -RRB-	dep_in_1 det_denominator_the prep_of_evaluation_denominator det_evaluation_the prep_simplify_in dobj_simplify_evaluation aux_simplify_to nn_structure_models det_structure_a prep_of_advantage_structure xcomp_take_simplify dobj_take_advantage nsubj_take_improvements det_improvements_These nn_q_distribution nn_q_probability det_q_the prep_of_normalization_q dobj_speed_normalization prt_speed_up aux_speed_to num_Lafferty_2001 nn_Lafferty_al. nn_Lafferty_et num_Wu_2000 conj_and_Wu_Khudanpur dep_Lafferty_Lafferty conj_and_Lafferty_Khudanpur conj_and_Lafferty_Wu conj_and_Lafferty_1996 conj_and_Lafferty_Suhm appos_literature_Wu appos_literature_1996 appos_literature_Suhm appos_literature_Lafferty det_literature_the xcomp_appear_speed prep_in_appear_literature nsubj_appear_that rcmod_optimizations_appear amod_optimizations_possible det_optimizations_the prep_of_any_optimizations dobj_applied_any neg_applied_not aux_applied_have nsubj_applied_we prep_in_applied_implementation mark_applied_that amod_implementation_current det_implementation_the parataxis_noted_take ccomp_noted_applied auxpass_noted_be aux_noted_should nsubjpass_noted_it advmod_noted_Finally
W02-2019	J96-1002	p	Maximum entropy models -LRB- Jaynes 1957 Berger et al. 1996 Della Pietra et al. 1997 -RRB- are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources	dobj_overlapping_sources advmod_overlapping_possibly conj_and_disparate_overlapping prep_from_integrating_overlapping prep_from_integrating_disparate dobj_integrating_information prepc_for_successful_integrating prep_in_successful_general advmod_successful_very cop_successful_be aux_successful_to xcomp_proven_successful aux_proven_have nsubj_proven_which nn_assumptions_independence amod_assumptions_unwarranted neg_assumptions_no conj_and_require_proven dobj_require_assumptions nsubj_require_which amod_models_exponential rcmod_class_proven rcmod_class_require prep_of_class_models det_class_a cop_class_are nsubj_class_Della num_Pietra_1997 nn_Pietra_al. nn_Pietra_et dep_Della_Pietra dep_Berger_class num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Jaynes_Berger appos_Jaynes_1957 dep_models_Jaynes nn_models_entropy nn_models_Maximum
W03-0401	J96-1002	o	A possible solution to this problem is to directly estimate p -LRB- A | w -RRB- by applying a maximum entropy model -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_a dobj_applying_model nn_w_| det_w_A appos_p_w prepc_by_estimate_applying dobj_estimate_p advmod_estimate_directly aux_estimate_to dep_is_Berger xcomp_is_estimate nsubj_is_solution det_problem_this prep_to_solution_problem amod_solution_possible det_solution_A ccomp_``_is
W03-0401	J96-1002	o	The parsing algorithm was CKY-style parsing with beam thresholding which was similar to ones used in -LRB- Collins 1996 Clark et al. 2002 -RRB-	num_Clark_2002 nn_Clark_al. nn_Clark_et dep_Collins_Clark amod_Collins_1996 dep_in_Collins prep_used_in vmod_ones_used prep_to_similar_ones cop_similar_was nsubj_similar_which rcmod_thresholding_similar nn_thresholding_beam prep_with_parsing_thresholding amod_parsing_CKY-style cop_parsing_was nsubj_parsing_algorithm nn_algorithm_parsing det_algorithm_The
W03-0401	J96-1002	o	Recently used machine learning methods including maximum entropy models -LRB- Berger et al. 1996 -RRB- and support vector machines -LRB- Vapnik 1995 -RRB- provide grounds for this type of modeling because it allows various dependent features to be incorporated into the model without the independence assumption	nn_assumption_independence det_assumption_the det_model_the prep_without_incorporated_assumption prep_into_incorporated_model auxpass_incorporated_be aux_incorporated_to amod_features_dependent amod_features_various xcomp_allows_incorporated dobj_allows_features nsubj_allows_it mark_allows_because prep_of_type_modeling det_type_this advcl_provide_allows prep_for_provide_type dobj_provide_grounds nsubj_provide_methods amod_Vapnik_1995 dep_machines_Vapnik nn_machines_vector nn_machines_support amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_models_machines dep_models_Berger nn_models_entropy nn_models_maximum prep_including_methods_machines prep_including_methods_models amod_methods_learning nn_methods_machine amod_methods_used advmod_used_Recently
W03-0417	J96-1002	p	State-of-theart machine learning techniques including Support Vector Machines -LRB- Vapnik 1995 -RRB- AdaBoost -LRB- Schapire and Singer 2000 -RRB- and Maximum Entropy Models -LRB- Ratnaparkhi 1998 Berger et al. 1996 -RRB- provide high performance classifiers if one has abundant correctly labeled examples	amod_examples_labeled amod_examples_abundant advmod_labeled_correctly dobj_has_examples nsubj_has_one mark_has_if nn_classifiers_performance amod_classifiers_high advcl_provide_has dobj_provide_classifiers nsubj_provide_techniques num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Ratnaparkhi_Berger dep_Ratnaparkhi_1998 appos_Models_Ratnaparkhi nn_Models_Entropy nn_Models_Maximum dep_Schapire_2000 conj_and_Schapire_Singer dep_AdaBoost_Singer dep_AdaBoost_Schapire amod_Vapnik_1995 conj_and_Machines_Models conj_and_Machines_AdaBoost dep_Machines_Vapnik nn_Machines_Vector nn_Machines_Support prep_including_techniques_Models prep_including_techniques_AdaBoost prep_including_techniques_Machines nn_techniques_learning nn_techniques_machine amod_techniques_State-of-theart
W03-0420	J96-1002	o	Thus we obtain the following second-order model a36a39a38a41a40 a17 a5a7 a42a4 a5a7 a44 a8 a5a57 a15a27a58 a7 a36a39a38a41a40 a17a20a15a59a42a17 a15a41a49 a7 a7 a60 a4 a5a7 a44 a8 ma61a63a62a65a64a33a66 a5a57 a15a27a58 a7a68a67 a40 a17 a15 a42a17 a15a50a49 a7 a15a50a49a51a48 a60 a4 a15a27a47a55a48 a15a50a49a54a48 a44 a11 A well-founded framework for directly modeling the posterior probability a67 a40 a17 a15 a42a17 a15a50a49 a7 a15a50a49a54a48 a60 a4 a15a12a47a55a48 a15a50a49a54a48 a44 is maximum entropy -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_entropy_maximum cop_entropy_is nsubj_entropy_a44 nn_a44_a15a50a49a54a48 nn_a44_a15a12a47a55a48 nn_a44_a4 nn_a44_a60 nn_a44_a15a50a49a54a48 nn_a44_a7 num_a44_a15a50a49 nn_a44_a42a17 nn_a44_a15 nn_a44_a17 nn_a44_a40 nn_a44_a67 nn_a44_probability amod_a44_posterior det_a44_the rcmod_modeling_entropy advmod_modeling_directly dep_framework_Berger prep_for_framework_modeling amod_framework_well-founded nn_framework_A nn_framework_a11 nn_framework_a44 nn_framework_a15a50a49a54a48 nn_framework_a15a27a47a55a48 nn_framework_a4 nn_framework_a60 nn_framework_a15a50a49a51a48 nn_framework_a7 num_framework_a15a50a49 nn_framework_a42a17 nn_framework_a15 nn_framework_a17 nn_framework_a40 nn_framework_a7a68a67 num_framework_a15a27a58 nn_framework_a5a57 nn_framework_ma61a63a62a65a64a33a66 nn_framework_a8 nn_framework_a44 nn_framework_a5a7 nn_framework_a4 nn_framework_a60 nn_framework_a7 nn_framework_a7 nn_framework_a15a41a49 nn_framework_a17a20a15a59a42a17 nn_framework_a36a39a38a41a40 nn_framework_a7 num_framework_a15a27a58 nn_framework_a5a57 nn_framework_a8 nn_framework_a44 nn_framework_a5a7 nn_framework_a42a4 nn_framework_a5a7 nn_framework_a17 nn_framework_a36a39a38a41a40 dep_model_framework nn_model_second-order amod_model_following det_model_the dobj_obtain_model nsubj_obtain_we advmod_obtain_Thus
W03-0420	J96-1002	o	1 Introduction In this paper we present an approach for extracting the named entities -LRB- NE -RRB- of natural language inputs which uses the maximum entropy -LRB- ME -RRB- framework -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_framework_Berger nn_framework_ME dep_entropy_framework nn_entropy_maximum det_entropy_the dobj_uses_entropy nsubj_uses_which rcmod_inputs_uses nn_inputs_language amod_inputs_natural prep_of_entities_inputs appos_entities_NE dobj_named_entities vmod_the_named dobj_extracting_the prepc_for_approach_extracting det_approach_an dobj_present_approach nsubj_present_we nsubj_present_Introduction det_paper_this prep_in_Introduction_paper num_Introduction_1
W03-0425	J96-1002	o	The model weights are trained using the improved iterative scaling algorithm -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_algorithm_scaling amod_algorithm_iterative amod_algorithm_improved det_algorithm_the dobj_using_algorithm dep_trained_Berger xcomp_trained_using auxpass_trained_are nsubjpass_trained_weights nn_weights_model det_weights_The ccomp_``_trained
W03-0425	J96-1002	o	-LRB- 1999 -RRB- a robust risk minimization classifier based on a regularized winnow method -LRB- Zhang et al. 2002 -RRB- -LRB- henceforth RRM -RRB- and a maximum entropy classifier -LRB- Darroch and Ratcliff 1972 Berger et al. 1996 Borthwick 1999 -RRB- -LRB- henceforth MaxEnt -RRB-	nn_MaxEnt_henceforth dep_Borthwick_1999 num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Darroch_Borthwick conj_and_Darroch_Berger conj_and_Darroch_1972 conj_and_Darroch_Ratcliff appos_classifier_MaxEnt dep_classifier_Berger dep_classifier_1972 dep_classifier_Ratcliff dep_classifier_Darroch nn_classifier_entropy nn_classifier_maximum det_classifier_a nn_RRM_henceforth num_Zhang_2002 dep_Zhang_al. nn_Zhang_et nn_method_winnow amod_method_regularized det_method_a prep_on_based_method conj_and_classifier_classifier appos_classifier_RRM dep_classifier_Zhang vmod_classifier_based nn_classifier_minimization nn_classifier_risk amod_classifier_robust det_classifier_a appos_1999_classifier appos_1999_classifier dep_''_1999
W03-0505	J96-1002	o	The first two phases are approached as straightforward classification in a maximum entropy framework -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_framework_entropy nn_framework_maximum det_framework_a amod_classification_straightforward dep_approached_Berger prep_in_approached_framework prep_as_approached_classification auxpass_approached_are nsubjpass_approached_phases num_phases_two amod_phases_first det_phases_The
W03-1007	J96-1002	o	3.2 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence but otherwise is as uniform as possible -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_possible_Berger dep_possible_as dep_is_possible prep_as_is_uniform advmod_is_otherwise nsubj_is_model det_evidence_the prep_by_imposed_evidence dep_constrains_imposed prep_of_set_constrains det_set_the prep_with_consistent_set cop_consistent_is nsubj_consistent_that conj_but_one_is rcmod_one_consistent det_one_the cop_one_be aux_one_will nsubj_one_model mark_one_that amod_model_best det_model_the ccomp_intuition_is ccomp_intuition_one det_intuition_the dobj_implement_intuition nsubj_implement_models nn_models_ME nn_models_Entropy nn_models_Maximum num_models_3.2
W03-1013	J96-1002	o	We have implemented a parallel version of our GIS code using the MPICH library -LRB- Gropp et al. 1996 -RRB- an open-source implementation of the Message Passing Interface -LRB- MPI -RRB- standard	dep_Interface_standard appos_Interface_MPI nn_Interface_Passing nn_Interface_Message det_Interface_the prep_of_implementation_Interface amod_implementation_open-source det_implementation_an amod_Gropp_1996 dep_Gropp_al. nn_Gropp_et appos_library_implementation dep_library_Gropp nn_library_MPICH det_library_the dobj_using_library nn_code_GIS poss_code_our prep_of_version_code amod_version_parallel det_version_a xcomp_implemented_using dobj_implemented_version aux_implemented_have nsubj_implemented_We ccomp_``_implemented
W03-1018	J96-1002	p	1 Introduction The maximum entropy model -LRB- Berger et al. 1996 Pietra et al. 1997 -RRB- has attained great popularity in the NLP field due to its power robustness and successful performance in various NLP tasks -LRB- Ratnaparkhi 1996 Nigam et al. 1999 Borthwick 1999 -RRB-	amod_Borthwick_1999 num_Nigam_1999 nn_Nigam_al. nn_Nigam_et dep_Ratnaparkhi_Borthwick conj_Ratnaparkhi_Nigam appos_Ratnaparkhi_1996 dep_tasks_Ratnaparkhi nn_tasks_NLP amod_tasks_various amod_performance_successful prep_in_power_tasks conj_and_power_performance conj_and_power_robustness poss_power_its nn_field_NLP det_field_the amod_popularity_great prep_due_to_attained_performance prep_due_to_attained_robustness prep_due_to_attained_power prep_in_attained_field dobj_attained_popularity aux_attained_has nsubj_attained_model num_Pietra_1997 nn_Pietra_al. nn_Pietra_et dep_Berger_Pietra appos_Berger_1996 dep_Berger_al. nn_Berger_et appos_model_Berger nn_model_entropy nn_model_maximum det_model_The rcmod_Introduction_attained num_Introduction_1
W03-1020	J96-1002	p	A more refined algorithm the incremental feature selection algorithm by Berger et al -LRB- 1996 -RRB- allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages	amod_stages_previous det_stages_the prep_in_selected_stages vmod_features_selected det_features_the prep_for_values_features nn_values_parameter amod_values_estimated dobj_keeps_values prep_at_keeps_time nsubj_keeps_feature amod_time_same det_time_the det_selection_each conj_and_added_keeps prep_at_added_selection auxpass_added_being nsubjpass_added_feature num_feature_one ccomp_allows_keeps ccomp_allows_added nsubj_allows_algorithm appos_al_1996 dep_Berger_al nn_Berger_et prep_by_algorithm_Berger nn_algorithm_selection nn_algorithm_feature amod_algorithm_incremental det_algorithm_the appos_algorithm_algorithm amod_algorithm_refined det_algorithm_A advmod_refined_more
W03-1020	J96-1002	o	In contrast to what is shown in Berger et al 1996s paper here is how the different values in this variant of the IFS algorithm are computed	auxpass_computed_are nsubjpass_computed_values advmod_computed_how nn_algorithm_IFS det_algorithm_the prep_of_variant_algorithm det_variant_this prep_in_values_variant amod_values_different det_values_the ccomp_is_computed advmod_is_here prep_in_is_contrast nn_paper_1996s nn_paper_al amod_paper_Berger nn_Berger_et prep_in_shown_paper auxpass_shown_is nsubjpass_shown_what prepc_to_contrast_shown
W03-1020	J96-1002	o	The goal of each selection stage is to select the feature f that maximizes the gain of the log likelihood where the a and gain of f are derived through following steps Let the log likelihood of the model be = yx xZysump pL -RRB- -LRB- / | log -LRB- -RRB- -LRB- ~ and the empirical expectation of feature f be E p -LRB- f -RRB- = p -LRB- x y -RRB- f -LRB- x y -RRB- x y With the approximation assumption in Berger et al -LRB- 1996 -RRB- s paper the un-normalized component and the normalization factor of the model have the following recursive forms -RRB- | -LRB- -RRB- | -LRB- aa exysumxysum SfS = | Z f + The approximate gain of the log likelihood is computed by G Sf -LRB- a -RRB- L -LRB- p Sf a -RRB- L -LRB- p S -RRB- = p -LRB- x -RRB- -LRB- logZ Sf a -LRB- x -RRB- x / Z S -LRB- x -RRB- -RRB- + aE p -LRB- f -RRB- -LRB- 1 -RRB- The maximum approximate gain and its corresponding a are represented as -RRB- -LRB- max -RRB- -LRB- ~ a fS GfSL =D maxarg f 3 A Fast Feature Selection Algorithm The inefficiency of the IFS algorithm is due to the following reasons	amod_reasons_following det_reasons_the prep_to_due_reasons cop_due_is nsubj_due_inefficiency nn_algorithm_IFS det_algorithm_the prep_of_inefficiency_algorithm det_inefficiency_The rcmod_Algorithm_due nn_Algorithm_Selection nn_Algorithm_Feature amod_Algorithm_Fast det_Algorithm_A num_Algorithm_3 dep_f_Algorithm dep_maxarg_f amod_=D_maxarg dep_GfSL_=D dep_fS_GfSL dep_a_fS dep_~_a dep_represented_~ dep_represented_max prep_represented_as auxpass_represented_are nsubjpass_represented_a dep_corresponding_represented vmod_its_corresponding amod_gain_approximate nn_gain_maximum det_gain_The dep_gain_1 nn_gain_p appos_p_f nn_p_aE appos_S_x nn_S_Z conj_x_a_S appos_a_x appos_Sf_S appos_Sf_a nn_Sf_logZ conj_+_p_gain appos_p_Sf appos_p_x nn_S_p appos_L_S dep_Sf_a nn_Sf_p conj_and_L_its dep_L_gain dep_L_p dep_L_= dep_L_L appos_L_Sf det_L_a dep_Sf_its dep_Sf_L nn_Sf_G agent_computed_Sf auxpass_computed_is nsubjpass_computed_SfS nn_likelihood_log det_likelihood_the prep_of_gain_likelihood amod_gain_approximate det_gain_The conj_+_f_gain dep_Z_gain dep_Z_f num_Z_| dep_=_Z amod_SfS_= nn_SfS_exysumxysum nn_SfS_aa rcmod_|_computed nn_|_| dep_forms_| amod_forms_recursive amod_forms_following det_forms_the dobj_have_forms nsubj_have_= nsubj_have_~ dep_have_log dep_have_select det_model_the prep_of_factor_model nn_factor_normalization det_factor_the amod_component_un-normalized det_component_the conj_and_paper_factor conj_and_paper_component amod_paper_s nn_paper_Berger appos_al_1996 dep_Berger_al nn_Berger_et prep_in_assumption_factor prep_in_assumption_component prep_in_assumption_paper nn_assumption_approximation det_assumption_the prep_with_y_assumption appos_x_y dep_f_x nn_f_p appos_x_y dep_p_x dep_=_f dep_=_f dep_=_p dep_=_be dep_=_f dep_=_feature dep_=_of dep_=_expectation nn_p_E amod_expectation_empirical det_expectation_the appos_~_y dep_~_x conj_and_~_= num_log_| nn_pL_xZysump nn_pL_yx dobj_=_pL dep_=_be dep_=_Let dep_=_the advmod_=_where det_model_the prep_of_likelihood_model nn_likelihood_log det_likelihood_the dobj_Let_likelihood amod_steps_following prep_through_derived_steps auxpass_derived_are nsubjpass_derived_gain nsubjpass_derived_a prep_of_gain_f conj_and_a_gain rcmod_the_derived rcmod_likelihood_= nn_likelihood_log det_likelihood_the prep_of_gain_likelihood det_gain_the dobj_maximizes_gain nsubj_maximizes_that rcmod_feature_maximizes dep_feature_f det_feature_the dobj_select_feature aux_select_to ccomp_is_have nsubj_is_goal nn_stage_selection det_stage_each prep_of_goal_stage det_goal_The
W03-1020	J96-1002	o	1 Introduction Maximum Entropy -LRB- ME -RRB- modeling has received a lot of attention in language modeling and natural language processing for the past few years -LRB- e.g. Rosenfeld 1994 Berger et al 1996 Ratnaparkhi 1998 Koeling 2000 -RRB-	amod_Koeling_2000 dep_Ratnaparkhi_Koeling appos_Ratnaparkhi_1998 nn_1996_al dep_Berger_1996 nn_Berger_et dep_Rosenfeld_Ratnaparkhi conj_Rosenfeld_Berger num_Rosenfeld_1994 pobj_e.g._Rosenfeld prep_-LRB-_e.g. amod_years_few amod_years_past det_years_the nn_processing_language amod_processing_natural conj_and_modeling_processing nn_modeling_language prep_of_lot_attention det_lot_a prep_for_received_years prep_in_received_processing prep_in_received_modeling dobj_received_lot aux_received_has nsubj_received_modeling nn_modeling_Entropy appos_Entropy_ME nn_Entropy_Maximum nn_Entropy_Introduction num_Entropy_1
W03-1021	J96-1002	o	We should note from equation 4 that the neural network model is similar in functional form to the maximum entropy model -LRB- Berger et al. 1996 -RRB- except that the neural network learns the feature functions by itself from the training data	nn_data_training det_data_the nn_functions_feature det_functions_the prep_from_learns_data prep_by_learns_itself dobj_learns_functions nsubj_learns_network mark_learns_that mark_learns_except amod_network_neural det_network_the amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_the amod_form_functional advcl_similar_learns dep_similar_Berger prep_to_similar_model prep_in_similar_form cop_similar_is nsubj_similar_model mark_similar_that nn_model_network amod_model_neural det_model_the ccomp_equation_similar num_equation_4 prep_from_note_equation aux_note_should nsubj_note_We
W03-1025	J96-1002	o	There are multiple studies -LRB- Wu and Fung 1994 Sproat et al. 1996 Luo and Roukos 1996 -RRB- showing that the agreement between two -LRB- untrained -RRB- native speakers is about upper a15 a12a14a7 to lower a0a4a12a14a7	amod_a0a4a12a14a7_lower prep_to_a12a14a7_a0a4a12a14a7 nn_a12a14a7_a15 amod_a12a14a7_upper prep_about_is_a12a14a7 amod_speakers_native num_speakers_two dep_two_untrained dep_agreement_is prep_between_agreement_speakers det_agreement_the dep_that_agreement prep_showing_that dep_Luo_1996 conj_and_Luo_Roukos nn_al._et nn_al._Sproat dep_Wu_Roukos dep_Wu_Luo num_Wu_1996 dep_Wu_al. num_Wu_1994 conj_and_Wu_Fung vmod_studies_showing appos_studies_Fung appos_studies_Wu amod_studies_multiple nsubj_are_studies expl_are_There ccomp_``_are
W03-1025	J96-1002	o	Chinese word segmentation is a well-known problem that has been studied extensively -LRB- Wu and Fung 1994 Sproat et al. 1996 Luo and Roukos 1996 -RRB- and it is known that human agreement is relatively low	advmod_low_relatively cop_low_is nsubj_low_agreement mark_low_that amod_agreement_human ccomp_known_low auxpass_known_is nsubjpass_known_it dep_Luo_1996 conj_and_Luo_Roukos nn_al._et nn_al._Sproat dep_Wu_Roukos dep_Wu_Luo num_Wu_1996 dep_Wu_al. amod_Wu_1994 conj_and_Wu_Fung conj_and_studied_known dep_studied_Fung dep_studied_Wu advmod_studied_extensively auxpass_studied_been aux_studied_has nsubjpass_studied_that rcmod_problem_known rcmod_problem_studied amod_problem_well-known det_problem_a cop_problem_is nsubj_problem_segmentation nn_segmentation_word amod_segmentation_Chinese
W03-1025	J96-1002	p	Each component model takes the exponential form a37a55a38a57a56 a51 a42a6a44a59a58a60a56 a61 a51a64a63a65a53a67a66 a53 a45a46a70 a71a16a72a21a73a75a74a77a76a79a78a81a80 a78a16a82a11a78 a38a83a44a59a58a60a56a84a61 a51a64a63a65a53a67a66 a53 a58a60a56 a51 a45a86a85 a87 a38a83a44a59a58a60a56a84a61 a51a64a63a65a53a67a66 a53 a45 a58 -LRB- 2 -RRB- where a87 a38a83a44a59a58a60a56 a61 a51a41a63a65a53a67a66 a53 a45 is a normalization term to ensure that a37a55a38a57a56 a51a42a6a44a88a58a60a56a62a61 a51a41a63a65a53a67a66 a53 a45 is a probability a82a11a78 a38a83a44a59a58a60a56 a61 a51a64a63a65a53a67a66 a53 a58a60a56 a51 a45 is a feature function -LRB- often binary -RRB- and a80 a78 is the weight ofa82a21a78 Given a set of features and a corpus of training data there exist ef cient training algorithms -LRB- Darroch and Ratcliff 1972 Berger et al. 1996 -RRB- to nd the optimal parameters a89 a80 a78a14a90 The art of building a maximum entropy parser then reduces to choosing good features	amod_features_good dobj_choosing_features prepc_to_reduces_choosing advmod_reduces_then nsubj_reduces_art dep_reduces_1972 dep_reduces_Ratcliff dep_reduces_Darroch nn_parser_entropy nn_parser_maximum det_parser_a dobj_building_parser prepc_of_art_building det_art_The nn_a78a14a90_a80 dobj_a89_a78a14a90 nsubj_a89_parameters amod_parameters_optimal det_parameters_the ccomp_nd_a89 aux_nd_to num_Berger_1996 nn_Berger_al. nn_Berger_et vmod_Darroch_nd dep_Darroch_Berger conj_and_Darroch_1972 conj_and_Darroch_Ratcliff dep_algorithms_reduces nn_algorithms_training amod_algorithms_cient nn_algorithms_ef dobj_exist_algorithms expl_exist_there prep_exist_Given nsubj_exist_a58 nn_data_training prep_of_corpus_data det_corpus_a conj_and_features_corpus prep_of_set_corpus prep_of_set_features det_set_a pobj_Given_set nn_ofa82a21a78_weight det_ofa82a21a78_the cop_ofa82a21a78_is nsubj_ofa82a21a78_term nn_a78_a80 advmod_binary_often dep_function_binary nn_function_feature det_function_a cop_function_is nsubj_function_a45 nn_a45_a51 num_a45_a58a60a56 nn_a45_a53 nn_a45_a51a64a63a65a53a67a66 nn_a45_a61 nn_a45_a38a83a44a59a58a60a56 nn_a45_a82a11a78 conj_and_probability_a78 conj_and_probability_function det_probability_a cop_probability_is nsubj_probability_a45 mark_probability_that nn_a45_a53 nn_a45_a51a41a63a65a53a67a66 nn_a45_a51a42a6a44a88a58a60a56a62a61 nn_a45_a37a55a38a57a56 ccomp_ensure_a78 ccomp_ensure_function ccomp_ensure_probability aux_ensure_to vmod_term_ensure nn_term_normalization det_term_a cop_term_is nsubj_term_a45 advmod_term_where dep_term_2 nn_a45_a53 nn_a45_a51a41a63a65a53a67a66 nn_a45_a61 nn_a45_a38a83a44a59a58a60a56 nn_a45_a87 rcmod_a58_ofa82a21a78 nn_a58_a45 nn_a58_a53 nn_a58_a51a64a63a65a53a67a66 nn_a58_a38a83a44a59a58a60a56a84a61 nn_a58_a87 nn_a58_a45a86a85 nn_a58_a51 num_a58_a58a60a56 nn_a58_a53 nn_a58_a51a64a63a65a53a67a66 nn_a58_a38a83a44a59a58a60a56a84a61 nn_a58_a78a16a82a11a78 nn_a58_a71a16a72a21a73a75a74a77a76a79a78a81a80 nn_a58_a45a46a70 nn_a58_a53 nn_a58_a51a64a63a65a53a67a66 nn_a58_a61 nn_a58_a42a6a44a59a58a60a56 nn_a58_a51 nn_a58_a37a55a38a57a56 amod_form_exponential det_form_the parataxis_takes_exist dobj_takes_form nsubj_takes_model nn_model_component det_model_Each
W03-1718	J96-1002	o	The training algorithm we used is the improved iterative scaling -LRB- IIS -RRB- described in -LRB- Berger et al 1996 -RRB- 3	num_Berger_3 amod_Berger_1996 dep_Berger_al nn_Berger_et prep_in_described_Berger vmod_scaling_described appos_scaling_IIS amod_scaling_iterative amod_scaling_improved det_scaling_the cop_scaling_is nsubj_scaling_algorithm nsubj_used_we rcmod_algorithm_used nn_algorithm_training det_algorithm_The
W04-0701	J96-1002	o	models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence but otherwise is as uniform as possible -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_possible_Berger dep_possible_as dep_is_possible prep_as_is_uniform advmod_is_otherwise nsubj_is_model det_evidence_the prep_by_imposed_evidence dep_constrains_imposed prep_of_set_constrains det_set_the prep_with_consistent_set cop_consistent_is nsubj_consistent_that conj_but_one_is rcmod_one_consistent det_one_the cop_one_be aux_one_will nsubj_one_model mark_one_that amod_model_best det_model_the ccomp_intuition_is ccomp_intuition_one det_intuition_the dobj_implement_intuition nsubj_implement_models
W04-0859	J96-1002	o	Our systems use both corpus-based and knowledge-based approaches Maximum Entropy -LRB- ME -RRB- -LRB- Lau et al. 1993 Berger et al. 1996 Ratnaparkhi 1998 -RRB- is a corpus-based and supervised method based on linguistic features ME is the core of a bootstrapping algorithm that we call re-training inspired This paper has been partially supported by the Spanish Government -LRB- CICyT -RRB- under project number TIC-2003-7180 and the Valencia Government -LRB- OCyT -RRB- under project number CTIDIB-2002-151 by co-training -LRB- Blum and Mitchell 1998 -RRB- Relevant Domains -LRB- RD -RRB- -LRB- Montoyo et al. 2003 -RRB- is a resource built from WordNet Domains -LRB- Magnini and Cavaglia 2000 -RRB- that is used in an unsupervised method that assigns domain and sense labels Specification Marks -LRB- SP -RRB- -LRB- Montoyo and Palomar 2000 -RRB- exploits the relations between synsets stored in WordNet -LRB- Miller et al. 1993 -RRB- and does not need any training corpora Commutative Test -LRB- CT -RRB- -LRB- Nica et al. 2003 -RRB- based on the Sense Discriminators device derived from EWN -LRB- Vossen 1998 -RRB- disambiguates nouns inside their syntactic patterns with the help of information extracted from raw corpus	amod_corpus_raw prep_from_extracted_corpus vmod_information_extracted prep_of_help_information det_help_the amod_patterns_syntactic poss_patterns_their prep_inside_nouns_patterns prep_with_disambiguates_help dobj_disambiguates_nouns amod_Vossen_1998 appos_EWN_Vossen prep_from_derived_EWN vmod_device_derived nn_device_Discriminators nn_device_Sense det_device_the prep_on_based_device amod_Nica_2003 dep_Nica_al. nn_Nica_et dep_Test_Nica appos_Test_CT amod_Test_Commutative nn_corpora_training det_corpora_any dobj_need_corpora neg_need_not aux_need_does nsubj_need_Marks amod_Miller_1993 dep_Miller_al. nn_Miller_et dep_WordNet_Miller prep_in_stored_WordNet vmod_synsets_stored prep_between_relations_synsets det_relations_the conj_and_exploits_need dobj_exploits_relations nsubj_exploits_Marks dep_Montoyo_2000 conj_and_Montoyo_Palomar appos_Marks_Palomar appos_Marks_Montoyo appos_Marks_SP nn_Marks_Specification nn_labels_sense nn_labels_domain conj_and_domain_sense dobj_assigns_labels nsubj_assigns_that rcmod_method_assigns amod_method_unsupervised det_method_an prep_in_used_method auxpass_used_is nsubjpass_used_that dep_Magnini_2000 conj_and_Magnini_Cavaglia rcmod_Domains_used appos_Domains_Cavaglia appos_Domains_Magnini nn_Domains_WordNet prep_from_built_Domains vmod_resource_built det_resource_a cop_resource_is nsubj_resource_Domains amod_Montoyo_2003 dep_Montoyo_al. nn_Montoyo_et dep_Domains_Montoyo appos_Domains_RD amod_Domains_Relevant amod_Blum_1998 conj_and_Blum_Mitchell dep_co-training_Mitchell dep_co-training_Blum nn_CTIDIB-2002-151_number nn_CTIDIB-2002-151_project appos_Government_OCyT nn_Government_Valencia det_Government_the conj_and_TIC-2003-7180_Government nn_TIC-2003-7180_number nn_TIC-2003-7180_project appos_Government_CICyT amod_Government_Spanish det_Government_the agent_supported_co-training prep_under_supported_CTIDIB-2002-151 prep_under_supported_Government prep_under_supported_TIC-2003-7180 agent_supported_Government advmod_supported_partially auxpass_supported_been aux_supported_has nsubjpass_supported_core det_paper_This dobj_inspired_paper vmod_call_inspired dobj_call_re-training nsubj_call_we mark_call_that ccomp_algorithm_call amod_algorithm_bootstrapping det_algorithm_a prep_of_core_algorithm det_core_the cop_core_is nsubj_core_ME amod_features_linguistic pobj_on_features pcomp_based_on prep_method_based amod_method_supervised amod_method_corpus-based det_method_a cop_method_is nsubj_method_Berger conj_and_corpus-based_supervised amod_Ratnaparkhi_1998 dep_Berger_Ratnaparkhi num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Lau_disambiguates vmod_Lau_based conj_Lau_Test parataxis_Lau_need parataxis_Lau_exploits parataxis_Lau_resource parataxis_Lau_supported dep_Lau_method appos_Lau_1993 dep_Lau_al. nn_Lau_et dep_Entropy_Lau appos_Entropy_ME nn_Entropy_Maximum amod_approaches_knowledge-based amod_approaches_corpus-based conj_and_corpus-based_knowledge-based preconj_corpus-based_both dep_use_Entropy dobj_use_approaches nsubj_use_systems poss_systems_Our
W04-0860	J96-1002	o	The supervised methods are based on Maximum Entropy -LRB- ME -RRB- -LRB- Lau et al. 1993 Berger et al. 1996 Ratnaparkhi 1998 -RRB- neural network using the Learning Vector Quantization algorithm -LRB- Kohonen 1995 -RRB- and Specialized Hidden Markov Models -LRB- Pla 2000 -RRB-	amod_Pla_2000 dep_Models_Pla nn_Models_Markov nn_Models_Hidden nn_Models_Specialized amod_Kohonen_1995 conj_and_algorithm_Models dep_algorithm_Kohonen nn_algorithm_Quantization nn_algorithm_Vector nn_algorithm_Learning det_algorithm_the dobj_using_Models dobj_using_algorithm vmod_network_using amod_network_neural dep_network_Berger dep_Ratnaparkhi_1998 dep_Berger_Ratnaparkhi num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Lau_network appos_Lau_1993 dep_Lau_al. nn_Lau_et dep_Entropy_Lau appos_Entropy_ME nn_Entropy_Maximum prep_on_based_Entropy auxpass_based_are nsubjpass_based_methods amod_methods_supervised det_methods_The ccomp_``_based
W04-1007	J96-1002	o	First two maximum entropy classifiers -LRB- Berger et al. 1996 -RRB- are applied where the first predicts clause start labels and the second predicts clause end labels	nn_labels_end nn_labels_clause dobj_predicts_labels dep_second_predicts amod_the_second conj_and_start_the dobj_start_labels dep_clause_the dep_clause_start dobj_predicts_clause dep_first_predicts amod_the_first dep_where_the prep_applied_where auxpass_applied_are nsubjpass_applied_classifiers advmod_applied_First amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_classifiers_Berger nn_classifiers_entropy amod_classifiers_maximum num_classifiers_two
W04-1802	J96-1002	o	Figures 1 and 2 present best results in the learning experiments for the complete set of patterns used in the collocation approach over two of our evaluation corpora .11 Type Positions Tags/Words Features Accuracy Precision Recall GIS 1 W 1254 0.97 0.96 0.98 IIS 1 T 136 0.95 0.96 0.94 NB 1 T 136 0.88 0.97 0.84 9 see Rish 2001 Ratnaparkhi 1997 and Berger et al 1996 for a formal description of these algorithms	det_algorithms_these prep_of_description_algorithms amod_description_formal det_description_a prep_for_1996_description nn_Berger_al nn_Berger_et appos_Rish_1996 conj_and_Rish_Berger conj_and_Rish_1997 conj_and_Rish_Ratnaparkhi amod_Rish_2001 dobj_see_Berger dobj_see_1997 dobj_see_Ratnaparkhi dobj_see_Rish nsubj_see_GIS num_9_0.84 number_0.84_0.97 dep_0.84_0.88 dep_0.84_T number_0.84_1 dep_0.84_NB dep_0.84_0.94 dep_0.84_T number_0.84_1 dep_0.84_IIS dep_0.84_0.98 number_0.88_136 number_0.94_0.96 dep_0.94_0.95 number_0.95_136 number_0.98_0.96 dep_0.98_0.97 number_0.97_1254 num_W_9 num_W_1 dep_GIS_W ccomp_Recall_see nsubj_Recall_Precision nn_Precision_Accuracy ccomp_Features_Recall nsubj_Features_results nn_Tags/Words_Positions nn_Tags/Words_Type num_Tags/Words_.11 nn_Tags/Words_corpora nn_Tags/Words_evaluation poss_Tags/Words_our prep_of_two_Tags/Words nn_approach_collocation det_approach_the prep_in_used_approach vmod_patterns_used prep_of_set_patterns amod_set_complete det_set_the prep_for_experiments_set nn_experiments_learning det_experiments_the prep_over_results_two prep_in_results_experiments amod_results_best amod_results_present num_results_2 num_results_1 nn_results_Figures conj_and_1_2
W05-0509	J96-1002	o	It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy is unique and has the following expone ntial form -LRB- Berger et al. 1996 -RRB- -LRB- 1 -RRB- = = k j cajf jcZcap 1 -RRB- -LRB- -RRB- -LRB- 1 -RRB- | -LRB- a where Z -LRB- c -RRB- is a normalization factor fj -LRB- a c -RRB- are the values of k features of the pair -LRB- a c -RRB- and correspond to the linguistic cues of c that are relevant to predict the outcome a. Features are extracted from the training data and define the constraints that the probabilistic model p must satisfy	aux_satisfy_must nsubj_satisfy_p mark_satisfy_that nn_p_model amod_p_probabilistic det_p_the ccomp_constraints_satisfy det_constraints_the dobj_define_constraints nsubj_define_factor nn_data_training det_data_the prep_from_extracted_data auxpass_extracted_are nsubjpass_extracted_Features nn_Features_a. nn_Features_outcome det_Features_the ccomp_predict_extracted aux_predict_to xcomp_relevant_predict cop_relevant_are nsubj_relevant_that rcmod_cues_relevant prep_of_cues_c amod_cues_linguistic det_cues_the prep_to_correspond_cues nsubj_correspond_factor det_c_a appos_pair_c det_pair_the prep_of_features_pair nn_features_k conj_and_values_define conj_and_values_correspond prep_of_values_features det_values_the cop_values_are nsubj_values_factor det_values_a det_c_a appos_fj_c conj_factor_fj nn_factor_normalization det_factor_a cop_factor_is nsubj_factor_Z advmod_factor_where appos_Z_c dep_|_define dep_|_correspond dep_|_values dep_|_1 num_jcZcap_1 nn_jcZcap_cajf nn_jcZcap_j nn_jcZcap_k amod_jcZcap_= dep_=_jcZcap dep_=_1 dep_al._1996 nn_al._et advmod_Berger_al. appos_form_Berger amod_form_ntial nn_form_expone amod_form_following det_form_the dobj_has_form dep_unique_| dep_unique_= conj_and_unique_has cop_unique_is ccomp_unique_proven amod_entropy_highest det_entropy_the prep_with_one_entropy det_one_the cop_one_is nsubj_one_p mark_one_that amod_assumption_above det_assumption_the dobj_satisfying_assumption vmod_p_satisfying nn_p_distribution nn_p_probability det_p_the ccomp_proven_one auxpass_proven_be aux_proven_can nsubjpass_proven_It
W05-0612	J96-1002	o	When labeled training data is available we can use the Maximum Entropy principle -LRB- Berger et al. 1996 -RRB- to optimize the weights	det_weights_the dobj_optimize_weights aux_optimize_to amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_principle_Berger nn_principle_Entropy nn_principle_Maximum det_principle_the vmod_use_optimize dobj_use_principle aux_use_can nsubj_use_we advcl_use_available cop_available_is nsubj_available_data advmod_available_When nn_data_training amod_data_labeled
W05-0627	J96-1002	o	In our SRL system we select maximum entropy -LRB- Berger et al. 1996 -RRB- as a classi er to implement the semantic role labeling system	nn_system_labeling nn_system_role amod_system_semantic det_system_the dobj_implement_system aux_implement_to vmod_er_implement nn_er_classi det_er_a amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_entropy_Berger amod_entropy_maximum prep_as_select_er dobj_select_entropy nsubj_select_we prep_in_select_system nn_system_SRL poss_system_our
W05-0709	J96-1002	o	The principle of maximum entropy states that when one searches among probability distributions that model the observed data -LRB- evidence -RRB- the preferred one is the one that maximizes the entropy -LRB- a measure of the uncertainty of the model -RRB- -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et det_model_the prep_of_uncertainty_model det_uncertainty_the prep_of_measure_uncertainty det_measure_a dep_entropy_Berger dep_entropy_measure det_entropy_the dobj_maximizes_entropy nsubj_maximizes_that rcmod_one_maximizes det_one_the cop_one_is nsubj_one_one amod_one_preferred det_one_the appos_data_evidence amod_data_observed det_data_the dobj_model_data nsubj_model_that rcmod_distributions_model nn_distributions_probability rcmod_searches_one prep_among_searches_distributions num_searches_one dep_when_searches dep_that_when dep_states_that amod_states_entropy nn_states_maximum prep_of_principle_states det_principle_The ccomp_``_principle
W05-0709	J96-1002	o	where mk is one mention in entity e and the basic model building block PL -LRB- L = 1je mk m -RRB- is an exponential or maximum entropy model -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_model_Berger amod_model_entropy amod_model_maximum amod_model_exponential det_model_an cop_model_is nsubj_model_PL conj_or_exponential_maximum dep_=_m dep_=_mk dobj_=_1je nsubj_=_L dep_PL_= nn_PL_block nn_PL_building nn_PL_model amod_PL_basic det_PL_the nn_e_entity conj_and_mention_model prep_in_mention_e num_mention_one cop_mention_is nsubj_mention_mk advmod_mention_where advcl_``_model advcl_``_mention
W05-0709	J96-1002	o	Both systems are built around from the maximum-entropy technique -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_technique_maximum-entropy det_technique_the prep_from_around_technique dep_built_Berger prep_built_around auxpass_built_are nsubjpass_built_systems det_systems_Both
W05-0709	J96-1002	o	SEP/epsilon a/A # epsilon / # a/epsilon a/epsilon b/epsilon b/B UNK/epsilon c/C b/epsilon c/BC e / + E epsilon / + d/epsilon d/epsilon epsilon/epsilon b/AB # b/A #B # e / + DE c/epsilon d/BCD e / + D+E Figure 1 Illustration of dictionary based segmentation finite state transducer 3.1 Bootstrapping In addition to the model based upon a dictionary of stems and words we also experimented with models based upon character n-grams similar to those used for Chinese segmentation -LRB- Sproat et al. 1996 -RRB-	amod_Sproat_1996 dep_Sproat_al. nn_Sproat_et amod_segmentation_Chinese prep_for_used_segmentation vmod_those_used prep_to_similar_those dep_n-grams_Sproat amod_n-grams_similar nn_n-grams_character prep_upon_based_n-grams vmod_models_based prep_with_experimented_models advmod_experimented_also nsubj_experimented_we advmod_experimented_Illustration conj_and_stems_words prep_of_dictionary_words prep_of_dictionary_stems det_dictionary_a prep_upon_based_dictionary vmod_model_based det_model_the prep_in_addition_to_Bootstrapping_model num_Bootstrapping_3.1 dep_transducer_Bootstrapping nn_transducer_state amod_transducer_finite nn_transducer_segmentation pobj_based_transducer prep_Illustration_based prep_of_Illustration_dictionary num_Figure_1 nn_Figure_D+E dep_d/BCD_e nn_d/BCD_c/epsilon nn_d/BCD_DE dep_d/BCD_+ dep_d/BCD_e dep_d/BCD_# nn_d/BCD_#B dep_d/BCD_# nn_d/BCD_b/AB nn_d/BCD_epsilon/epsilon nn_d/BCD_d/epsilon nn_d/BCD_d/epsilon nn_#B_b/A nn_epsilon_E dep_c/BC_experimented conj_+_c/BC_Figure conj_+_c/BC_d/BCD conj_+_c/BC_epsilon dep_c/BC_e nn_c/BC_b/epsilon nn_c/BC_c/C nn_c/BC_UNK/epsilon nn_c/BC_b/B nn_c/BC_b/epsilon nn_c/BC_a/epsilon nn_c/BC_a/epsilon dep_c/BC_# dep_c/BC_epsilon dep_c/BC_# dep_c/BC_a/A dep_c/BC_SEP/epsilon
W05-1304	J96-1002	o	In this paper we adopt a maximum entropy model -LRB- Berger et al. 1996 -RRB- to estimate the local probabilities a28 a14 a1 a25 a19a1 a25a30a29 a2 a9a22a21 since it can incorporate diverse types of features with reasonable computational cost	amod_cost_computational amod_cost_reasonable prep_of_types_features amod_types_diverse prep_with_incorporate_cost dobj_incorporate_types aux_incorporate_can nsubj_incorporate_it mark_incorporate_since nn_a9a22a21_a2 nn_a9a22a21_a25a30a29 nn_a9a22a21_a19a1 num_a9a22a21_a25 nn_a9a22a21_a1 nn_a9a22a21_a14 advcl_a28_incorporate dobj_a28_a9a22a21 nsubj_a28_probabilities amod_probabilities_local det_probabilities_the ccomp_estimate_a28 aux_estimate_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_a vmod_adopt_estimate dep_adopt_Berger dobj_adopt_model nsubj_adopt_we prep_in_adopt_paper det_paper_this
W05-1505	J96-1002	o	For a more detailed introduction to maximum entropy estimation see -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_see_Berger prep_for_see_introduction nn_estimation_entropy nn_estimation_maximum prep_to_introduction_estimation amod_introduction_detailed det_introduction_a advmod_detailed_more
W05-1510	J96-1002	o	The forest representation was obtained by adopting chart generation -LRB- Kay 1996 Car93 roll et al. 1999 -RRB- where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing	prep_as_way_parsing amod_way_same det_way_the det_forest_a det_chart_a prep_in_mapping_way prep_into_mapping_forest dobj_mapping_chart nsubjpass_mapping_candidates amod_class_equivalence det_class_an conj_and_packed_mapping prep_into_packed_class auxpass_packed_are nsubjpass_packed_candidates advmod_packed_where amod_candidates_ambiguous num_roll_1999 nn_roll_al. nn_roll_et nn_roll_Car93 dep_Kay_roll num_Kay_1996 rcmod_generation_mapping rcmod_generation_packed dep_generation_Kay nn_generation_chart dobj_adopting_generation agent_obtained_adopting auxpass_obtained_was nsubjpass_obtained_representation nn_representation_forest det_representation_The
W05-1510	J96-1002	o	2.3 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing -LRB- Malouf and van Noord 2004 Miyao and Tsujii 2005 -RRB- adopted log-linear models -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_models_log-linear dobj_adopted_models nsubj_adopted_studies dep_Miyao_2005 conj_and_Miyao_Tsujii nn_Noord_van dep_Malouf_Tsujii dep_Malouf_Miyao conj_and_Malouf_2004 conj_and_Malouf_Noord appos_parsing_2004 appos_parsing_Noord appos_parsing_Malouf nn_parsing_HPSG prep_for_models_parsing amod_models_probabilistic prep_on_studies_models amod_studies_existing det_studies_Some nn_studies_HPSG prepc_with_generation_adopted dep_models_Berger prep_for_models_generation nn_models_Probabilistic num_models_2.3 dep_``_models
W05-1511	J96-1002	o	Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed -LRB- Kasper et al. 1996 Briscoe and Carroll 1993 Kiefer et al. 2002 -RRB- and the most probable parse is found by PCFG parsing	nn_parsing_PCFG agent_found_parsing auxpass_found_is nsubjpass_found_parse nsubjpass_found_Kiefer nsubjpass_found_1993 nsubjpass_found_Carroll nsubjpass_found_Briscoe amod_parse_probable det_parse_the advmod_probable_most num_Kiefer_2002 nn_Kiefer_al. nn_Kiefer_et conj_and_Briscoe_parse conj_and_Briscoe_Kiefer conj_and_Briscoe_1993 conj_and_Briscoe_Carroll parataxis_Kasper_found appos_Kasper_1996 dep_Kasper_al. nn_Kasper_et dep_developed_Kasper auxpass_developed_been aux_developed_have nsubjpass_developed_models amod_grammar_unification-based det_grammar_the prep_of_backbone_grammar nn_backbone_CFG det_backbone_the prep_to_assigned_backbone auxpass_assigned_are nsubjpass_assigned_probabilities advmod_assigned_where rcmod_models_assigned nn_models_Probabilistic
W05-1511	J96-1002	o	Previous studies -LRB- Abney 1997 Johnson et al. 1999 Riezler et al. 2000 Miyao et al. 2003 Malouf and van Noord 2004 Kaplan et al. 2004 Miyao and Tsujii 2005 -RRB- defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum conj_or_model_model amod_model_log-linear det_model_a amod_grammars_unification-based prep_of_model_grammars amod_model_probabilistic det_model_a dep_defined_Berger prep_as_defined_model prep_as_defined_model dobj_defined_model nsubj_defined_studies dep_Miyao_2005 conj_and_Miyao_Tsujii num_Kaplan_2004 nn_Kaplan_al. nn_Kaplan_et nn_Noord_van conj_and_Malouf_2004 conj_and_Malouf_Noord num_Miyao_2003 nn_Miyao_al. nn_Miyao_et num_Riezler_2000 nn_Riezler_al. nn_Riezler_et num_Johnson_1999 nn_Johnson_al. nn_Johnson_et dep_Abney_Tsujii dep_Abney_Miyao dep_Abney_Kaplan dep_Abney_2004 dep_Abney_Noord dep_Abney_Malouf dep_Abney_Miyao dep_Abney_Riezler dep_Abney_Johnson dep_Abney_1997 dep_studies_Abney amod_studies_Previous
W05-1514	J96-1002	o	6 Phrase Recognition with a Maximum Entropy Classifier For the candidates which are not filtered out in the above two phases we perform classification with maximum entropy classifiers -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_classifiers_Berger nn_classifiers_entropy nn_classifiers_maximum prep_with_perform_classifiers dobj_perform_classification nsubj_perform_we dep_perform_Recognition num_phases_two amod_phases_above det_phases_the prep_in_filtered_phases prt_filtered_out neg_filtered_not auxpass_filtered_are nsubjpass_filtered_which rcmod_candidates_filtered det_candidates_the prep_for_Classifier_candidates nn_Classifier_Entropy nn_Classifier_Maximum det_Classifier_a prep_with_Recognition_Classifier nn_Recognition_Phrase num_Recognition_6
W05-1520	J96-1002	o	2.2 Maximum Entropy Our next approach is the Maximum Entropy -LRB- Berger et al. 1996 -RRB- classification approach	nn_approach_classification dep_approach_Berger dep_approach_Entropy dep_Berger_1996 dep_Berger_al. nn_Berger_et nn_Entropy_Maximum det_Entropy_the cop_Entropy_is nsubj_Entropy_Entropy amod_approach_next poss_approach_Our dep_Entropy_approach nn_Entropy_Maximum num_Entropy_2.2
W06-0301	J96-1002	o	As a learning algorithm for our classification model we used Maximum Entropy -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_Entropy_Berger nn_Entropy_Maximum dobj_used_Entropy nsubj_used_we prep_as_used_algorithm nn_model_classification poss_model_our prep_for_algorithm_model nn_algorithm_learning det_algorithm_a
W06-1314	J96-1002	o	96 Research on DA classification initially focused on two-party conversational speech -LRB- Mast et al. 1996 Stolcke et al. 1998 Shriberg et al. 1998 -RRB- and more recently has extended to multi-party audio recordings like the ICSI corpus -LRB- Shriberg et al. 2004 -RRB-	amod_Shriberg_2004 dep_Shriberg_al. nn_Shriberg_et nn_corpus_ICSI det_corpus_the prep_like_recordings_corpus nn_recordings_audio amod_recordings_multi-party prep_to_extended_recordings aux_extended_has advmod_extended_recently nsubj_extended_Research advmod_recently_more num_Shriberg_1998 nn_Shriberg_al. nn_Shriberg_et num_Stolcke_1998 nn_Stolcke_al. nn_Stolcke_et conj_Mast_Shriberg conj_Mast_Stolcke appos_Mast_1996 dep_Mast_al. nn_Mast_et amod_speech_conversational amod_speech_two-party dep_focused_Shriberg conj_and_focused_extended dep_focused_Mast prep_on_focused_speech advmod_focused_initially nsubj_focused_Research nn_classification_DA prep_on_Research_classification num_Research_96 ccomp_``_extended ccomp_``_focused
W06-1314	J96-1002	o	We apply a maximum entropy -LRB- maxent -RRB- model -LRB- Berger et al. 1996 -RRB- to this task	det_task_this amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_model_Berger nn_model_entropy appos_entropy_maxent nn_entropy_maximum det_entropy_a prep_to_apply_task dobj_apply_model nsubj_apply_We
W06-1617	J96-1002	p	Since its introduction to the Natural Language Processing -LRB- NLP -RRB- community -LRB- Berger et al. 1996 -RRB- ME-based classifiers have been shown to be effective in various NLP tasks	nn_tasks_NLP amod_tasks_various prep_in_effective_tasks cop_effective_be aux_effective_to xcomp_shown_effective auxpass_shown_been aux_shown_have nsubjpass_shown_community amod_classifiers_ME-based amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_community_classifiers appos_community_Berger nn_community_NLP rcmod_Processing_shown nn_Processing_Language amod_Processing_Natural det_Processing_the prep_to_introduction_Processing poss_introduction_its pobj_Since_introduction dep_``_Since
W06-1619	J96-1002	o	Previous studies -LRB- Abney 1997 Johnson et al. 1999 Riezler et al. 2000 Malouf and van Noord 2004 Kaplan et al. 2004 Miyao and Tsujii 2005 -RRB- defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum conj_or_model_model amod_model_log-linear det_model_a dep_HPSG_Berger prep_as_HPSG_model prep_as_HPSG_model prep_including_grammars_HPSG amod_grammars_unification-based prep_of_model_grammars amod_model_probabilistic det_model_a dobj_defined_model nsubj_defined_studies dep_Miyao_2005 conj_and_Miyao_Tsujii num_Kaplan_2004 nn_Kaplan_al. nn_Kaplan_et nn_Noord_van conj_and_Malouf_2004 conj_and_Malouf_Noord num_Riezler_2000 nn_Riezler_al. nn_Riezler_et num_Johnson_1999 nn_Johnson_al. nn_Johnson_et dep_Abney_Tsujii dep_Abney_Miyao dep_Abney_Kaplan dep_Abney_2004 dep_Abney_Noord dep_Abney_Malouf dep_Abney_Riezler dep_Abney_Johnson dep_Abney_1997 dep_studies_Abney amod_studies_Previous
W06-1633	J96-1002	o	Based on the data seen a maximum entropy model -LRB- Berger et al. 1996 -RRB- offers an expression -LRB- 1 -RRB- for the probability that there exists coreference C between a mention mi and a mention mj	nn_mj_mention det_mj_a conj_and_mi_mj nn_mi_mention det_mi_a prep_between_C_mj prep_between_C_mi nn_C_coreference nsubj_exists_C expl_exists_there mark_exists_that ccomp_probability_exists det_probability_the prep_for_expression_probability appos_expression_1 det_expression_an dobj_offers_expression nsubj_offers_model vmod_offers_Based amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_model_Berger nn_model_entropy nn_model_maximum det_model_a vmod_data_seen det_data_the prep_on_Based_data
W06-1643	J96-1002	o	We performed feature selection by incrementally growing a log-linear model with order0 features f -LRB- x yt -RRB- using a forward feature selection procedure similar to -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_to_Berger prep_similar_to amod_procedure_similar nn_procedure_selection nn_procedure_feature advmod_procedure_forward det_procedure_a dobj_using_procedure vmod_x_using appos_x_yt dep_f_x dep_features_f nn_features_order0 prep_with_model_features amod_model_log-linear det_model_a dobj_growing_model advmod_growing_incrementally nn_selection_feature prepc_by_performed_growing dobj_performed_selection nsubj_performed_We
W06-2601	J96-1002	n	Despite ME theory and its related training algorithm -LRB- Darroch and Ratcliff 1972 -RRB- do not set restrictions on the range of feature functions1 popular NLP text books -LRB- Manning and Schutze 1999 -RRB- and research papers -LRB- Berger et al. 1996 -RRB- seem to limit them to binary features	amod_features_binary prep_to_limit_features dobj_limit_them aux_limit_to xcomp_seem_limit nsubj_seem_papers nsubj_seem_books advcl_seem_set amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_papers_research conj_and_Manning_1999 conj_and_Manning_Schutze dep_books_Berger conj_and_books_papers dep_books_1999 dep_books_Schutze dep_books_Manning nn_books_text nn_books_NLP amod_books_popular nn_functions1_feature prep_of_range_functions1 det_range_the prep_on_restrictions_range dobj_set_restrictions neg_set_not aux_set_do prep_despite_set_algorithm prep_despite_set_theory dep_Darroch_1972 conj_and_Darroch_Ratcliff appos_algorithm_Ratcliff appos_algorithm_Darroch nn_algorithm_training amod_algorithm_related poss_algorithm_its conj_and_theory_algorithm nn_theory_ME
W06-2601	J96-1002	p	1 Introduction The Maximum Entropy -LRB- ME -RRB- statistical framework -LRB- Darroch and Ratcliff 1972 Berger et al. 1996 -RRB- has been successfully deployed in several NLP tasks	nn_tasks_NLP amod_tasks_several prep_in_deployed_tasks advmod_deployed_successfully auxpass_deployed_been aux_deployed_has nsubjpass_deployed_Introduction num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Darroch_Berger conj_and_Darroch_1972 conj_and_Darroch_Ratcliff dep_framework_1972 dep_framework_Ratcliff dep_framework_Darroch amod_framework_statistical nn_framework_Entropy appos_Entropy_ME nn_Entropy_Maximum det_Entropy_The vmod_Introduction_framework num_Introduction_1
W06-2601	J96-1002	o	6 Parameter Estimation From the duality of ME and maximum likelihood -LRB- Berger et al. 1996 -RRB- optimal parameters for model -LRB- 3 -RRB- can be found by maximizing the log-likelihood function over a training sample -LCB- -LRB- xt yt -RRB- t = 1 N -RCB- i.e. = argmax Nsummationdisplay t = 1 logp -LRB- yt | xt -RRB-	nn_xt_| nn_xt_yt appos_logp_xt num_logp_1 dep_=_logp amod_t_= nn_t_Nsummationdisplay nn_t_argmax dobj_=_t dep_=_1 dep_t_= dep_t_i.e. appos_t_N amod_t_= appos_xt_yt dep_sample_t dep_sample_xt nn_sample_training det_sample_a amod_function_log-likelihood det_function_the prep_over_maximizing_sample dobj_maximizing_function agent_found_maximizing auxpass_found_be aux_found_can nsubjpass_found_likelihood nsubjpass_found_Estimation appos_model_3 prep_for_parameters_model amod_parameters_optimal amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_likelihood_Berger nn_likelihood_maximum prep_of_duality_ME det_duality_the appos_Estimation_parameters conj_and_Estimation_likelihood prep_from_Estimation_duality nn_Estimation_Parameter num_Estimation_6
W06-2922	J96-1002	o	Using Maximum Entropy -LRB- Berger et al. 1996 -RRB- classifiers I built a parser that achieves a throughput of over 200 sentences per second with a small loss in accuracy of about 23 %	num_%_23 quantmod_23_about prep_of_accuracy_% prep_in_loss_accuracy amod_loss_small det_loss_a prep_per_sentences_second num_sentences_200 pobj_over_sentences pcomp_of_over prep_throughput_of det_throughput_a dobj_achieves_throughput nsubj_achieves_that rcmod_parser_achieves det_parser_a dobj_built_parser nsubj_built_I rcmod_classifiers_built dep_al._1996 nn_al._et advmod_Berger_al. dep_Entropy_classifiers appos_Entropy_Berger nn_Entropy_Maximum prep_with_Using_loss dobj_Using_Entropy ccomp_``_Using
W06-2928	J96-1002	o	4 The Dependency Labeler 4.1 Classifier We used a maximum entropy classifier -LRB- Berger et al. 1996 -RRB- to assign labels to the unlabeled dependencies produced by the Bayes Point Machine	nn_Machine_Point nn_Machine_Bayes det_Machine_the agent_produced_Machine vmod_dependencies_produced amod_dependencies_unlabeled det_dependencies_the prep_to_assign_dependencies dobj_assign_labels aux_assign_to nsubj_assign_Classifier amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_classifier_Berger nn_classifier_entropy nn_classifier_maximum det_classifier_a dobj_used_classifier nsubj_used_We rcmod_Classifier_used num_Classifier_4.1 nn_Classifier_Labeler nn_Classifier_Dependency det_Classifier_The rcmod_4_assign ccomp_``_4
W06-3108	J96-1002	p	In the case of two orientation classes cj j is defined as cj j = braceleftbigg left if j < j right if j > j -LRB- 4 -RRB- Then the reordering model has the form p -LRB- cj j | fJ1 eI1 i j -RRB- A well-founded framework for directly modeling the probability p -LRB- cj j | fJ1 eI1 i j -RRB- is maximum entropy -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_entropy_Berger amod_entropy_maximum cop_entropy_is nsubj_entropy_p num_fJ1_| nn_fJ1_j appos_cj_j conj_cj_i conj_cj_eI1 conj_cj_fJ1 dep_p_cj nn_p_probability det_p_the rcmod_modeling_entropy advmod_modeling_directly amod_framework_well-founded det_framework_A nn_framework_p num_fJ1_| nn_fJ1_j appos_cj_j conj_cj_i conj_cj_eI1 conj_cj_fJ1 dep_p_cj dep_form_framework det_form_the prep_for_has_modeling dobj_has_form nsubj_has_model nsubj_has_j mark_has_if nn_model_reordering det_model_the advmod_j_Then appos_j_4 amod_j_> nn_j_j num_right_j dep_<_right advcl_j_has amod_j_< prep_if_left_j nsubj_left_cj amod_braceleftbigg_= nn_braceleftbigg_j conj_cj_braceleftbigg dep_as_left prep_defined_as auxpass_defined_is prep_in_defined_case appos_cj_j appos_classes_cj nn_classes_orientation num_classes_two prep_of_case_classes det_case_the
W07-0401	J96-1002	o	Many reordering constraints have been used for word reorderings such as ITG constraints -LRB- Wu 1996 -RRB- IBM constraints -LRB- Berger et al. 1996 -RRB- and local constraints -LRB- Kanthak et al. 2005 -RRB-	amod_Kanthak_2005 dep_Kanthak_al. nn_Kanthak_et amod_constraints_local amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_constraints_Kanthak conj_and_constraints_constraints dep_constraints_Berger nn_constraints_IBM ccomp_,_constraints ccomp_,_constraints dep_Wu_1996 appos_constraints_Wu nn_constraints_ITG prep_such_as_reorderings_constraints nn_reorderings_word prep_for_used_reorderings auxpass_used_been aux_used_have nsubjpass_used_constraints nn_constraints_reordering amod_constraints_Many ccomp_``_used
W07-0413	J96-1002	o	The probability distributions of these binary classifiers are learnt using maximum entropy model -LRB- Berger et al. 1996 Haffner 2006 -RRB-	amod_Haffner_2006 dep_Berger_Haffner appos_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum dobj_using_model dep_learnt_Berger xcomp_learnt_using auxpass_learnt_are nsubjpass_learnt_distributions amod_classifiers_binary det_classifiers_these prep_of_distributions_classifiers nn_distributions_probability det_distributions_The ccomp_``_learnt
W07-0604	J96-1002	p	-LRB- 2006 -RRB- but we use a maximum entropy classifier -LRB- Berger et al. 1996 -RRB- to determine parser actions which makes parsing extremely fast	advmod_fast_extremely advmod_parsing_fast xcomp_makes_parsing nsubj_makes_which rcmod_actions_makes nn_actions_parser dobj_determine_actions aux_determine_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_classifier_entropy nn_classifier_maximum det_classifier_a dobj_use_classifier nsubj_use_we vmod_2006_determine dep_2006_Berger conj_but_2006_use dep_''_use dep_''_2006
W07-1027	J96-1002	o	Maximum Entropy Modeling -LRB- MaxEnt -RRB- -LRB- Berger et al. 1996 -RRB- and Support Vector Machine -LRB- SVM -RRB- -LRB- Vapnik 1995 -RRB- were used to build the classifiers in our solution	poss_solution_our det_classifiers_the prep_in_build_solution dobj_build_classifiers aux_build_to xcomp_used_build auxpass_used_were nsubjpass_used_Machine nsubjpass_used_Modeling amod_Vapnik_1995 appos_Machine_SVM nn_Machine_Vector nn_Machine_Support amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_Modeling_Vapnik conj_and_Modeling_Machine dep_Modeling_Berger appos_Modeling_MaxEnt nn_Modeling_Entropy nn_Modeling_Maximum
W07-1033	J96-1002	o	is the previous BIO tag S is the target sentence and fj and lj are feature functions and parameters of a log-linear model -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_model_log-linear det_model_a prep_of_parameters_model conj_and_functions_parameters nn_functions_feature cop_functions_are nsubj_functions_lj nsubj_functions_fj conj_and_fj_lj dep_sentence_Berger conj_and_sentence_parameters conj_and_sentence_functions nn_sentence_target det_sentence_the cop_sentence_is nsubj_sentence_S rcmod_tag_functions rcmod_tag_sentence nn_tag_BIO amod_tag_previous det_tag_the cop_tag_is
W07-1110	J96-1002	o	5.2 Maximum Entropy Model We use the Maximum Entropy -LRB- ME -RRB- Model -LRB- Berger et al. 1996 -RRB- for our classification task	nn_task_classification poss_task_our amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_for_Model_task dep_Model_Berger dep_Entropy_Model appos_Entropy_ME nn_Entropy_Maximum det_Entropy_the dobj_use_Entropy nsubj_use_We rcmod_Model_use nn_Model_Entropy nn_Model_Maximum num_Model_5.2 dep_``_Model
W07-1110	J96-1002	o	-LRB- Dahl et al. 1987 Hull and Gomez 1996 -RRB- use hand-coded slot-filling rules to determine the semantic roles of the arguments of a nominalization	det_nominalization_a prep_of_arguments_nominalization det_arguments_the prep_of_roles_arguments amod_roles_semantic det_roles_the dobj_determine_roles aux_determine_to nn_rules_slot-filling amod_rules_hand-coded vmod_use_determine dobj_use_rules nsubj_use_Dahl num_Hull_1996 conj_and_Hull_Gomez dep_Dahl_Gomez dep_Dahl_Hull appos_Dahl_1987 dep_Dahl_al. nn_Dahl_et
W07-2057	J96-1002	o	We utilize the OpenNLP MaxEnt implementation2 of the maximum entropy classification algorithm -LRB- Berger et al. 1996 -RRB- to train classification models for each lemma and part-of-speech combination in the training corpus	nn_corpus_training det_corpus_the amod_combination_part-of-speech conj_and_lemma_combination det_lemma_each nn_models_classification prep_in_train_corpus prep_for_train_combination prep_for_train_lemma dobj_train_models aux_train_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_algorithm_classification nn_algorithm_entropy nn_algorithm_maximum det_algorithm_the prep_of_implementation2_algorithm nn_implementation2_MaxEnt nn_implementation2_OpenNLP det_implementation2_the xcomp_utilize_train dep_utilize_Berger dobj_utilize_implementation2 nsubj_utilize_We
W07-2059	J96-1002	p	Exponential family models are a mainstay of modern statistical modeling -LRB- Brown 1986 -RRB- and they are widely and successfully used for example in text classification -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_classification_text prep_in_used_classification prep_for_used_example advmod_used_successfully advmod_used_widely auxpass_used_are nsubjpass_used_they conj_and_widely_successfully dep_Brown_1986 dep_modeling_Brown amod_modeling_statistical amod_modeling_modern dep_mainstay_Berger conj_and_mainstay_used prep_of_mainstay_modeling det_mainstay_a cop_mainstay_are nsubj_mainstay_models nn_models_family amod_models_Exponential
W07-2202	J96-1002	o	The disambiguation model of Enju is based on a feature forest model -LRB- Miyao and Tsujii 2002 -RRB- which is a log-linear model -LRB- Berger et al. 1996 -RRB- on packed forest structure	nn_structure_forest amod_structure_packed amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_on_model_structure dep_model_Berger amod_model_log-linear det_model_a cop_model_is nsubj_model_which dep_Miyao_2002 conj_and_Miyao_Tsujii rcmod_model_model appos_model_Tsujii appos_model_Miyao nn_model_forest nn_model_feature det_model_a prep_on_based_model auxpass_based_is nsubjpass_based_model prep_of_model_Enju nn_model_disambiguation det_model_The
W07-2208	J96-1002	o	This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model -LRB- Berger et al. 1996 -RRB- with many features for parse trees -LRB- Abney 1997 Johnson et al. 1999 Riezler et al. 2000 Malouf and van Noord 2004 Kaplan et al. 2004 Miyao and Tsujii 2005 -RRB-	dep_Miyao_2005 conj_and_Miyao_Tsujii num_Kaplan_2004 nn_Kaplan_al. nn_Kaplan_et nn_Noord_van dep_Malouf_Tsujii dep_Malouf_Miyao conj_and_Malouf_Kaplan conj_and_Malouf_2004 conj_and_Malouf_Noord num_Riezler_2000 nn_Riezler_al. nn_Riezler_et num_Johnson_1999 nn_Johnson_al. nn_Johnson_et dep_Abney_Kaplan dep_Abney_2004 dep_Abney_Noord dep_Abney_Malouf conj_Abney_Riezler conj_Abney_Johnson dep_Abney_1997 dep_trees_Abney nn_trees_parse prep_for_features_trees amod_features_many prep_with_Berger_features amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum conj_or_model_model amod_model_log-linear det_model_a nn_trees_parse prep_of_candidates_trees prep_among_tree_candidates nn_tree_parse amod_tree_correct det_tree_a prep_in_discriminating_model prep_in_discriminating_model dobj_discriminating_tree prepc_of_probabilities_discriminating dobj_provides_probabilities nsubj_provides_which rcmod_model_provides amod_model_probabilistic det_model_a dep_overcome_Berger agent_overcome_model auxpass_overcome_was nsubjpass_overcome_This ccomp_``_overcome
W07-2208	J96-1002	o	Previous studies -LRB- Abney 1997 Johnson et al. 1999 Riezler et al. 2000 Malouf and van Noord 2004 Kaplan et al. 2004 Miyao and Tsujii 2005 -RRB- defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum conj_or_model_model amod_model_log-linear det_model_a dep_HPSG_Berger prep_as_HPSG_model prep_as_HPSG_model prep_including_grammars_HPSG amod_grammars_unification-based prep_of_model_grammars amod_model_probabilistic det_model_a dobj_defined_model nsubj_defined_studies dep_Miyao_2005 conj_and_Miyao_Tsujii num_Kaplan_2004 nn_Kaplan_al. nn_Kaplan_et nn_Noord_van conj_and_Malouf_2004 conj_and_Malouf_Noord num_Riezler_2000 nn_Riezler_al. nn_Riezler_et num_Johnson_1999 nn_Johnson_al. nn_Johnson_et dep_Abney_Tsujii dep_Abney_Miyao dep_Abney_Kaplan dep_Abney_2004 dep_Abney_Noord dep_Abney_Malouf dep_Abney_Riezler dep_Abney_Johnson dep_Abney_1997 dep_studies_Abney amod_studies_Previous
W08-0206	J96-1002	o	For instance for Maximum Entropy I picked -LRB- Berger et al. 1996 Ratnaparkhi 1997 -RRB- for the basic theory -LRB- Ratnaparkhi 1996 -RRB- for an application -LRB- POS tagging in this case -RRB- and -LRB- Klein and Manning 2003 -RRB- for more advanced topics such as optimization and smoothing	conj_and_optimization_smoothing prep_such_as_topics_smoothing prep_such_as_topics_optimization amod_topics_advanced amod_topics_more prep_for_Klein_topics dep_Klein_2003 conj_and_Klein_Manning det_case_this prep_in_tagging_case nn_tagging_POS appos_application_tagging det_application_an dep_Ratnaparkhi_1996 amod_theory_basic det_theory_the dep_Ratnaparkhi_1997 conj_and_Berger_Manning conj_and_Berger_Klein prep_for_Berger_application appos_Berger_Ratnaparkhi prep_for_Berger_theory dep_Berger_Ratnaparkhi appos_Berger_1996 dep_Berger_al. nn_Berger_et dobj_picked_Klein dobj_picked_Berger nsubj_picked_I prep_for_picked_Entropy prep_for_picked_instance nn_Entropy_Maximum
W08-0404	J96-1002	o	Maximum entropy estimation for translation of individual words dates back to Berger et al -LRB- 1996 -RRB- and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings -LRB- Chan et al 2007 Carpuat and Wu 2007a Carpuat and Wu 2007b -RRB-	nn_2007b_Wu conj_and_Carpuat_2007b nn_2007a_Wu nn_2007a_Carpuat conj_and_Carpuat_Wu dep_Chan_2007b dep_Chan_Carpuat conj_Chan_2007a dep_Chan_2007 dep_Chan_al nn_Chan_et prep_to_generalized_substrings nsubj_generalized_estimation dep_reintroducedundertherubricofwordsensedisambiguation_Chan conj_and_reintroducedundertherubricofwordsensedisambiguation_generalized advmod_reintroducedundertherubricofwordsensedisambiguation_recently cop_reintroducedundertherubricofwordsensedisambiguation_been aux_reintroducedundertherubricofwordsensedisambiguation_has nsubj_reintroducedundertherubricofwordsensedisambiguation_idea nsubj_reintroducedundertherubricofwordsensedisambiguation_estimation nn_estimates_frequency amod_estimates_relative prep_through_made_estimates advmod_made_normally vmod_predictions_made dobj_sharpen_predictions aux_sharpen_to amod_classifiers_multi-class vmod_using_sharpen dobj_using_classifiers prepc_of_idea_using det_idea_the dep_al_1996 dep_Berger_al nn_Berger_et prep_to_back_Berger advmod_dates_back nn_dates_words amod_dates_individual prep_of_translation_dates conj_and_estimation_idea prep_for_estimation_translation amod_estimation_entropy nn_estimation_Maximum
W08-0504	J96-1002	p	-LRB- 2006 -RRB- but we use a maximum entropy classifier -LRB- Berger et al. 1996 -RRB- to determine parser actions which makes parsing considerably faster	advmod_faster_considerably advmod_parsing_faster xcomp_makes_parsing nsubj_makes_which rcmod_actions_makes nn_actions_parser dobj_determine_actions aux_determine_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_classifier_entropy nn_classifier_maximum det_classifier_a dobj_use_classifier nsubj_use_we vmod_2006_determine dep_2006_Berger conj_but_2006_use dep_''_use dep_''_2006
W08-1130	J96-1002	o	These feature functions fi were used to train a maximum entropy classifier -LRB- Berger et al. 1996 -RRB- -LRB- Le 2004 -RRB- thatassignsaprobabilitytoaREregiven context cx as follows p -LRB- re | cx -RRB- = Z -LRB- cx -RRB- exp nsummationdisplay i = 1 ifi -LRB- cx re -RRB- where Z -LRB- cx -RRB- is a normalizing sum and the i are the parameters -LRB- feature weights -RRB- learned	nsubj_learned_parameters dep_learned_= dep_learned_p nn_weights_feature appos_parameters_weights det_parameters_the cop_parameters_are nsubj_parameters_exp nn_parameters_Z det_i_the conj_and_sum_i amod_sum_normalizing det_sum_a cop_sum_is nsubj_sum_Z advmod_sum_where appos_Z_cx appos_cx_re rcmod_ifi_i rcmod_ifi_sum dep_ifi_cx num_ifi_1 dep_=_ifi amod_i_= dep_nsummationdisplay_i amod_exp_nsummationdisplay appos_Z_cx num_cx_| nn_cx_re appos_p_cx mark_follows_as nn_cx_context nn_cx_thatassignsaprobabilitytoaREregiven dep_Le_2004 amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_classifier_cx dep_classifier_Le dep_classifier_Berger nn_classifier_entropy nn_classifier_maximum det_classifier_a advcl_train_follows dobj_train_classifier aux_train_to parataxis_used_learned xcomp_used_train auxpass_used_were nsubjpass_used_fi nn_fi_functions amod_fi_feature det_fi_These
W08-1130	J96-1002	o	We use discourse-level feature predicates in a maximum entropy classifier -LRB- Berger et al. 1996 -RRB- with binary and n-class classification to select referring expressions from a list	det_list_a prep_from_expressions_list amod_expressions_referring dobj_select_expressions aux_select_to vmod_classification_select amod_classification_n-class amod_classification_binary conj_and_binary_n-class amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_classifier_entropy nn_classifier_maximum det_classifier_a prep_with_predicates_classification dep_predicates_Berger prep_in_predicates_classifier amod_feature_discourse-level dep_use_predicates dobj_use_feature nsubj_use_We
W08-1302	J96-1002	o	2 Background MaxEnt Models Maximum Entropy -LRB- MaxEnt -RRB- models are widely used in Natural Language Processing -LRB- Berger et al. 1996 Ratnaparkhi 1997 Abney 1997 -RRB-	amod_Abney_1997 dep_Ratnaparkhi_Abney amod_Ratnaparkhi_1997 dep_Berger_Ratnaparkhi appos_Berger_1996 dep_Berger_al. nn_Berger_et nn_Processing_Language amod_Processing_Natural dep_used_Berger prep_in_used_Processing advmod_used_widely auxpass_used_are nsubjpass_used_models nn_models_Entropy nn_models_Models nn_models_MaxEnt appos_Entropy_MaxEnt nn_Entropy_Maximum dep_Background_used num_Background_2 dep_``_Background
W08-2130	J96-1002	o	In this paper a discriminative parser is proposed to implement maximum entropy -LRB- ME -RRB- models -LRB- Berger et al. 1996 -RRB- to address the learning task	nn_task_learning det_task_the dobj_address_task aux_address_to nn_al._et num_Berger_1996 appos_Berger_al. nn_models_entropy appos_entropy_ME nn_entropy_maximum xcomp_implement_address dep_implement_Berger dobj_implement_models aux_implement_to xcomp_proposed_implement auxpass_proposed_is nsubjpass_proposed_parser prep_in_proposed_paper amod_parser_discriminative det_parser_a det_paper_this ccomp_``_proposed
W08-2139	J96-1002	o	The maximum entropy classier -LRB- Berger et al 1996 -RRB- used is Le Zhang 's Maximum Entropy Modeling Toolkit and the L-BFGS parameter estimation algorithm with gaussian prior smoothing -LRB- Chen and Rosenfeld 1999 -RRB-	amod_Chen_1999 conj_and_Chen_Rosenfeld dep_smoothing_Rosenfeld dep_smoothing_Chen amod_smoothing_prior amod_smoothing_gaussian prep_with_algorithm_smoothing nn_algorithm_estimation nn_algorithm_parameter nn_algorithm_L-BFGS det_algorithm_the conj_and_Toolkit_algorithm nn_Toolkit_Modeling nn_Toolkit_Entropy nn_Toolkit_Maximum poss_Toolkit_Zhang cop_Toolkit_is nsubj_Toolkit_classier nn_Zhang_Le amod_Berger_1996 dep_Berger_al nn_Berger_et vmod_classier_used dep_classier_Berger nn_classier_entropy nn_classier_maximum det_classier_The
W09-0435	J96-1002	o	Wu -LRB- 1996 -RRB- and Berger et al.	nn_al._et nn_al._Berger conj_and_Wu_al. appos_Wu_1996
W09-0706	J96-1002	o	1.2 Recent work A few publications so far deal with POS-tagging of Northern Sotho most prominently de Schryver and de Pauw -LRB- 2007 -RRB- have presented the MaxTag method a tagger based on Maximum Entropy 38 Learning -LRB- Berger et al. 1996 -RRB- as implemented in the machine learning package Maxent -LRB- Le 2004 -RRB-	amod_Le_2004 dep_Maxent_Le nn_Maxent_package dobj_learning_Maxent vmod_machine_learning det_machine_the prep_in_implemented_machine mark_implemented_as amod_Berger_1996 dep_Berger_al. nn_Berger_et num_Learning_38 nn_Learning_Entropy nn_Learning_Maximum prep_on_based_Learning vmod_tagger_based det_tagger_a dep_method_Berger appos_method_tagger nn_method_MaxTag det_method_the advcl_presented_implemented dobj_presented_method aux_presented_have nsubj_presented_Pauw nsubj_presented_Schryver advmod_presented_prominently dep_Pauw_2007 nn_Pauw_de conj_and_Schryver_Pauw nn_Schryver_de advmod_prominently_most nn_Sotho_Northern prep_of_POS-tagging_Sotho dep_deal_presented prep_with_deal_POS-tagging dep_far_deal advmod_far_so ccomp_,_far amod_publications_few det_publications_A nn_publications_work amod_publications_Recent num_publications_1.2 dep_``_publications
W09-1118	J96-1002	o	-LRB- 2007 -RRB- The committee consists of k = 3 Maximum Entropy -LRB- ME -RRB- classifiers -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_classifiers_Entropy appos_Entropy_ME nn_Entropy_Maximum num_Entropy_3 dep_=_classifiers dep_k_Berger amod_k_= prep_of_consists_k nsubj_consists_committee det_committee_The dep_2007_consists dep_''_2007
W09-1207	J96-1002	o	During the SRC stage a Maximum entropy -LRB- Berger et al. 1996 -RRB- classifier is used to predict the probabilities of a word in the sentence Language No-duplicated-roles Catalan arg0-agt arg0-cau arg1-pat arg2-atr arg2-loc Chinese A0 A1 A2 A3 A4 A5 Czech ACT ADDR CRIT LOC PAT DIR3 COND English A0 A1 A2 A3 A4 A5 German A0 A1 A2 A3 A4 A5 Japanese DE GA TMP WO Spanish arg0-agt arg0-cau arg1-pat arg1-tem arg2-atr arg2-loc arg2-null arg4-des argL-null argMcau argM-ext argM-fin Table 1 No-duplicated-roles for different languages to be each semantic role	amod_role_semantic det_role_each cop_role_be aux_role_to amod_languages_different vmod_No-duplicated-roles_role prep_for_No-duplicated-roles_languages num_Table_1 amod_Table_argM-fin nn_arg0-agt_Spanish nn_arg0-agt_WO amod_DE_Japanese amod_A0_German nn_A0_English nn_A0_COND nn_ACT_Czech amod_A0_Chinese amod_A0_arg2-loc appos_arg0-agt_Table conj_arg0-agt_argM-ext conj_arg0-agt_argMcau conj_arg0-agt_argL-null conj_arg0-agt_arg4-des conj_arg0-agt_arg2-null conj_arg0-agt_arg2-loc conj_arg0-agt_arg2-atr conj_arg0-agt_arg1-tem conj_arg0-agt_arg1-pat conj_arg0-agt_arg0-cau conj_arg0-agt_arg0-agt conj_arg0-agt_TMP conj_arg0-agt_GA conj_arg0-agt_DE conj_arg0-agt_A5 conj_arg0-agt_A4 conj_arg0-agt_A3 conj_arg0-agt_A2 conj_arg0-agt_A1 conj_arg0-agt_A0 conj_arg0-agt_A5 conj_arg0-agt_A4 conj_arg0-agt_A3 conj_arg0-agt_A2 conj_arg0-agt_A1 conj_arg0-agt_A0 conj_arg0-agt_DIR3 conj_arg0-agt_PAT conj_arg0-agt_LOC conj_arg0-agt_CRIT conj_arg0-agt_ADDR conj_arg0-agt_ACT conj_arg0-agt_A5 conj_arg0-agt_A4 conj_arg0-agt_A3 conj_arg0-agt_A2 conj_arg0-agt_A1 conj_arg0-agt_A0 conj_arg0-agt_arg2-atr conj_arg0-agt_arg1-pat conj_arg0-agt_arg0-cau amod_arg0-agt_Catalan nn_arg0-agt_No-duplicated-roles nn_arg0-agt_Language nn_arg0-agt_sentence det_arg0-agt_the prep_in_word_arg0-agt det_word_a prep_of_probabilities_word det_probabilities_the dobj_predict_probabilities aux_predict_to dep_used_No-duplicated-roles xcomp_used_predict auxpass_used_is nsubjpass_used_entropy prep_during_used_stage amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_entropy_classifier dep_entropy_Berger nn_entropy_Maximum det_entropy_a nn_stage_SRC det_stage_the
W09-2309	J96-1002	o	IBM constraints -LRB- Berger et al. 1996 -RRB- the lexical word reordering model -LRB- Tillmann 2004 -RRB- and inversion transduction grammar -LRB- ITG -RRB- constraints -LRB- Wu 1995 Wu 1997 -RRB- belong to this type of approach	prep_of_type_approach det_type_this prep_to_belong_type nsubj_belong_constraints nsubj_belong_model nsubj_belong_constraints dep_Wu_1997 dep_Wu_Wu appos_Wu_1995 appos_constraints_Wu nn_constraints_grammar appos_grammar_ITG nn_grammar_transduction nn_grammar_inversion dep_Tillmann_2004 appos_model_Tillmann nn_model_reordering nn_model_word amod_model_lexical det_model_the amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_constraints_constraints conj_and_constraints_model dep_constraints_Berger nn_constraints_IBM
W96-0213	J96-1002	o	Previous uses of this model include language modeling -LRB- Lau et al. 1993 -RRB- machine translation -LRB- Berger et al. 1996 -RRB- prepositional phrase attachment -LRB- Ratnaparkhi et al. 1994 -RRB- and word morphology -LRB- Della Pietra et al. 1995 -RRB-	amod_Pietra_1995 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della dep_morphology_Pietra nn_morphology_word amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et appos_attachment_Ratnaparkhi nn_attachment_phrase amod_attachment_prepositional amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_translation_Berger nn_translation_machine amod_Lau_1993 dep_Lau_al. nn_Lau_et conj_and_modeling_morphology conj_and_modeling_attachment conj_and_modeling_translation dep_modeling_Lau nn_modeling_language dobj_include_morphology dobj_include_attachment dobj_include_translation dobj_include_modeling nsubj_include_uses det_model_this prep_of_uses_model amod_uses_Previous
W97-0121	J96-1002	o	To make feature ranking computationally tractable in Della Pietra et al. 1995 and Berger et al. 1996 a simplified process proposed at the feature ranking stage when adding a new feature to the model all previously computed parameters are kept fixed and thus we have to fit only one new constraint imposed by a candidate feature	nn_feature_candidate det_feature_a agent_imposed_feature vmod_constraint_imposed amod_constraint_new num_constraint_one quantmod_one_only dobj_fit_constraint aux_fit_to xcomp_have_fit nsubj_have_we conj_and_fixed_have conj_and_fixed_thus dep_kept_have dep_kept_thus dep_kept_fixed auxpass_kept_are nsubjpass_kept_parameters amod_parameters_computed advmod_parameters_previously det_parameters_all rcmod_model_kept det_model_the amod_feature_new det_feature_a prep_to_adding_model dobj_adding_feature advmod_adding_when amod_stage_ranking nn_stage_feature det_stage_the dep_proposed_process dep_proposed_al. amod_process_simplified det_process_a num_process_1996 nn_al._et nn_al._Berger conj_and_1995_proposed dep_al._proposed dep_al._1995 nn_al._et nn_al._Pietra nn_al._Della prep_in_tractable_al. advmod_tractable_computationally amod_tractable_ranking nsubj_tractable_feature advcl_make_adding prep_at_make_stage xcomp_make_tractable aux_make_To
W97-0121	J96-1002	o	First as the configuration space we can use only the reference nodes -LRB- w -RRB- from the lattice which makes it similar to the method of Berger et al. 1996 described in section 2.1	num_section_2.1 prep_in_described_section vmod_1996_described pobj_al._1996 nn_al._et prep_Berger_al. prep_of_method_Berger det_method_the prep_to_similar_method amod_it_similar dobj_makes_it nsubj_makes_which rcmod_lattice_makes det_lattice_the appos_nodes_w nn_nodes_reference det_nodes_the advmod_nodes_only prep_from_use_lattice dobj_use_nodes aux_use_can nsubj_use_we rcmod_space_use nn_space_configuration det_space_the prep_as_First_space
W97-0121	J96-1002	o	We adopted the stop condition suggested in Berger et al. 1996 the maximization of the likelihood on a cross-validation set of samples which is unseen at the parameter esti ~ _ tion	nn_tion__ nn_tion_~ nn_tion_esti dep_parameter_tion det_parameter_the prep_at_unseen_parameter cop_unseen_is nsubj_unseen_which rcmod_samples_unseen prep_of_set_samples amod_set_cross-validation det_set_a det_likelihood_the prep_of_maximization_likelihood det_maximization_the num_maximization_1996 dep_al._maximization nn_al._et advmod_Berger_al. prep_on_suggested_set prep_in_suggested_Berger nsubj_suggested_condition nn_condition_stop det_condition_the ccomp_adopted_suggested nsubj_adopted_We
W97-0121	J96-1002	o	Our method uses assumptions similar to Berger et al. 1996 but is naturally suitable for distributed parallel computations	nn_computations_parallel amod_computations_distributed prep_for_suitable_computations advmod_suitable_naturally cop_suitable_is nsubj_suitable_method num_al._1996 dep_Berger_al. nn_Berger_et prep_to_similar_Berger amod_assumptions_similar conj_but_uses_suitable dobj_uses_assumptions nsubj_uses_method poss_method_Our
W97-0121	J96-1002	o	Berger et al. 1996 presented a way of computing conditional maximum entropy models directly by modifying equation 6 as follows -LRB- now instead of w we will explicitly use -LRB- x y -RRB- -RRB- i ~ Cx ~ -RRB- = ~ f ~ -LRB- ~ y -RRB- * ~ -LRB- ~ y -RRB- ~ ~ ~ -LRB- ~ y -RRB- * ~ -LRB- ~ -RRB- * pCy I ~ -RRB- = p -LRB- xk -RRB- -LRB- 9 -RRB- x6X yEY xEX yEY where ~ -LRB- x y -RRB- is an empirical probability of a joint configuration -LRB- w -RRB- of certain instantiated factor I variables with certain instantiated behavior variables	nn_variables_behavior amod_variables_instantiated amod_variables_certain prep_with_variables_variables dep_variables_I nn_variables_factor amod_variables_instantiated amod_variables_certain prep_of_configuration_variables appos_configuration_w amod_configuration_joint det_configuration_a prep_of_probability_configuration amod_probability_empirical det_probability_an cop_probability_is nsubj_probability_~ advmod_probability_where appos_x_y dep_~_x rcmod_yEY_probability nn_yEY_xEX nn_yEY_yEY nn_yEY_x6X num_yEY_9 dep_p_yEY appos_p_xk dep_=_p nsubj_~_I rcmod_pCy_~ dep_pCy_* dep_~_pCy dep_~_~ dep_~_* appos_~_y dep_~_~ dep_~_~ dep_~_~ num_~_~ nn_~_~ appos_~_y dep_~_* appos_~_y dep_~_~ num_~_~ dep_~_~ nn_~_f nn_~_~ dep_=_= dep_=_~ dep_~_= nn_~_Cx nn_~_~ nn_~_i appos_x_y dep_use_x advmod_use_explicitly aux_use_will nsubj_use_we prep_instead_of_use_w advmod_use_now dep_follows_use mark_follows_as num_equation_6 advcl_modifying_follows dobj_modifying_equation nn_models_entropy nn_models_maximum amod_models_conditional prepc_by_computing_modifying advmod_computing_directly dobj_computing_models prepc_of_way_computing det_way_a advmod_presented_~ dobj_presented_way nsubj_presented_1996 advmod_presented_al. nsubj_presented_Berger nn_al._et
W97-0301	J96-1002	o	6 Comparison With Previous Work The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St. Journal are the bigram parser described in -LRB- Collins 1996 -RRB- and the SPATTER parser described in -LRB- Jelinek et al. 1994 Magerman 1995 -RRB-	amod_Magerman_1995 dep_Jelinek_Magerman appos_Jelinek_1994 dep_Jelinek_al. nn_Jelinek_et prep_in_described_Jelinek vmod_parser_described nn_parser_SPATTER det_parser_the conj_and_Collins_parser amod_Collins_1996 prep_in_described_parser prep_in_described_Collins vmod_parser_described nn_parser_bigram det_parser_the nsubj_are_parser nn_Journal_St. nn_Journal_Wall nn_Journal_Treebank nn_Journal_Penn det_Journal_the prep_on_accuracies_Journal amod_accuracies_best det_accuracies_the dobj_reported_accuracies advmod_reported_previously aux_reported_have nsubj_reported_which dep_parsers_are rcmod_parsers_reported num_parsers_two det_parsers_The dep_Work_parsers amod_Work_Previous prep_with_Comparison_Work num_Comparison_6 dep_``_Comparison
W97-0319	J96-1002	o	164 and Itai 1990 Dagan et al. 1995 Kennedy and Boguraev 1996a Kennedy and Boguraev 1996b -RRB-	num_Dagan_1995 nn_Dagan_al. nn_Dagan_et dep_164_1996b conj_and_164_Boguraev conj_and_164_Kennedy conj_and_164_1996a conj_and_164_Boguraev conj_and_164_Kennedy conj_and_164_Dagan conj_and_164_1990 conj_and_164_Itai ccomp_``_Boguraev ccomp_``_Kennedy ccomp_``_1996a ccomp_``_Boguraev ccomp_``_Kennedy ccomp_``_Dagan ccomp_``_1990 ccomp_``_Itai ccomp_``_164
W97-0319	J96-1002	o	-LRB- 1996 -RRB- show that this model is a member of an exponential family with one parameter for each constraint specifically a model of the form 1 ~ I ~ -LRB- x ~ -RRB- p -LRB- yl -RRB- = E ' in which z -LRB- x -RRB- = eZ Y The parameters A1 An are Lagrange multipliers that impose the constraints corresponding to the chosen features fl fnThe term Z -LRB- x -RRB- normalizes the probabilities by summing over all possible outcomes y. Berger et al.	nn_al._et nn_al._Berger dep_y._al. amod_outcomes_possible det_outcomes_all prep_over_summing_outcomes det_probabilities_the dep_normalizes_y. prepc_by_normalizes_summing dobj_normalizes_probabilities nsubj_normalizes_multipliers appos_Z_x nn_Z_term amod_Z_fnThe appos_fl_Z dep_features_fl dep_chosen_features vmod_the_chosen prep_to_corresponding_the vmod_constraints_corresponding det_constraints_the dobj_impose_constraints nsubj_impose_that rcmod_multipliers_impose nn_multipliers_Lagrange cop_multipliers_are det_multipliers_An rcmod_A1_normalizes dep_parameters_A1 det_parameters_The dep_Y_parameters appos_eZ_Y dobj_=_eZ dep_z_= appos_z_x dep_=_z prep_in_=_which npadvmod_=_E npadvmod_=_p appos_p_yl appos_x_~ advmod_~_= dep_~_x nsubj_~_I rcmod_~_~ num_~_1 nn_~_form det_~_the prep_of_model_~ det_model_a advmod_model_specifically det_constraint_each num_parameter_one amod_family_exponential det_family_an appos_member_model prep_for_member_constraint prep_with_member_parameter prep_of_member_family det_member_a cop_member_is nsubj_member_model mark_member_that det_model_this ccomp_show_member dep_show_1996
W97-0319	J96-1002	o	Figure 1 exhibits this scenario with a typical IE system such as SRI 's FASTUS system -LRB- Hobbs et al. 1996 -RRB-	amod_Hobbs_1996 dep_Hobbs_al. nn_Hobbs_et nn_system_FASTUS poss_system_SRI prep_such_as_system_system amod_system_IE amod_system_typical det_system_a det_scenario_this dep_exhibits_Hobbs prep_with_exhibits_system dobj_exhibits_scenario nsubj_exhibits_Figure num_Figure_1 ccomp_``_exhibits
W97-1005	J96-1002	o	Statistical and information theoretic approaches -LRB- Hindle and Rooth 1993 -RRB- -LRB- Ratnaparkhi et al. 1994 -RRB- -LRB- Collins and Brooks 1995 -RRB- -LRB- Franz 1996 -RRB- Using lexical collocations to determine PPA with statistical techniques was first proposed by -LRB- Hindle and Rooth 1993 -RRB-	dep_Hindle_1993 conj_and_Hindle_Rooth dep_by_Rooth dep_by_Hindle prep_proposed_by vmod_first_proposed nsubj_was_first amod_techniques_statistical prep_with_PPA_techniques dobj_determine_PPA aux_determine_to amod_collocations_lexical vmod_Using_determine dobj_Using_collocations amod_Franz_1996 amod_Collins_1995 conj_and_Collins_Brooks amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_Hindle_1993 conj_and_Hindle_Rooth dep_approaches_was vmod_approaches_Using appos_approaches_Franz appos_approaches_Brooks appos_approaches_Collins dep_approaches_Ratnaparkhi dep_approaches_Rooth dep_approaches_Hindle amod_approaches_theoretic amod_approaches_information amod_approaches_Statistical conj_and_Statistical_information
W97-1005	J96-1002	o	The approach made use of a maximum entropy model -LRB- Berger et al. 1996 -RRB- formulated from frequency information for various combinations of the observed features	amod_features_observed det_features_the prep_of_combinations_features amod_combinations_various prep_for_information_combinations nn_information_frequency prep_from_formulated_information vmod_Berger_formulated amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_a prep_of_use_model dep_made_Berger dobj_made_use nsubj_made_approach det_approach_The
W98-0701	J96-1002	o	i.e. -LRB- ll -RRB- Lj = ~ maz -LRB- zi -LRB- j u -RRB- -RRB- i = I where xi -LRB- j u -RRB- E Qi and max -LRB- xi -LRB- j u -RRB- -RRB- is the highest score in the line of the matrix Qi which corresponds to the head word sense j. n is the number of modifiers of the head word h at the current tree level and k i Lj = j ~ l Lj where k is the number of senses of the head word h The reason why gj -LRB- I0 -RRB- is calculated as a sum of the best scores -LRB- ll -RRB- rather than by using the traditional maximum likelihood estimate -LRB- Berger et al. 1996 -RRB- -LRB- Gah eta \ -LSB-	nn_\_eta amod_\_Gah dep_Berger_\ amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_estimate_likelihood nn_estimate_maximum amod_estimate_traditional det_estimate_the dobj_using_estimate dep_by_Berger pcomp_by_using conj_negcc_scores_by appos_scores_ll amod_scores_best det_scores_the prep_of_sum_by prep_of_sum_scores det_sum_a prep_as_calculated_sum auxpass_calculated_is nsubjpass_calculated_gj advmod_calculated_why appos_gj_I0 rcmod_reason_calculated det_reason_The dep_reason_i.e. nn_h_word nn_h_head det_h_the prep_of_senses_h prep_of_number_senses det_number_the cop_number_is nsubj_number_k advmod_number_where rcmod_Lj_number nn_Lj_l nn_Lj_~ nn_Lj_j dobj_=_Lj dep_Lj_= nn_Lj_i nn_Lj_k nn_level_tree amod_level_current det_level_the nn_h_word nn_h_head det_h_the prep_of_modifiers_h conj_and_number_Lj prep_at_number_level prep_of_number_modifiers det_number_the cop_number_is nsubj_number_Lj dep_number_ll nn_n_j. nn_n_sense nn_n_word nn_n_head det_n_the prep_to_corresponds_n nsubj_corresponds_which rcmod_Qi_corresponds nn_Qi_matrix det_Qi_the prep_of_line_Qi det_line_the prep_in_score_line amod_score_highest det_score_the cop_score_is nsubj_score_max nsubj_score_Qi advmod_score_where appos_j_u dep_xi_j dep_max_xi conj_and_Qi_max nn_Qi_E nn_Qi_xi appos_j_u dep_xi_j rcmod_I_score dobj_=_I dep_i_= appos_j_u dep_zi_j dep_maz_i dep_maz_zi num_maz_~ dep_=_maz amod_Lj_= dep_i.e._Lj dep_i.e._number
W98-0701	J96-1002	o	To determine the tree head-word we used a set of rules similar to that described by -LRB- Magerman 1995 -RRB- -LRB- Jelinek et al. 1994 -RRB- and also used by -LRB- Collins 1996 -RRB- which we modified in the following way The head of a prepositional phrase -LRB- PP-IN NP -RRB- was substituted by a function the name of which corresponds to the preposition and its sole argument corresponds to the head of the noun phrase NP	nn_NP_phrase nn_NP_noun det_NP_the prep_of_head_NP det_head_the prep_to_corresponds_head nsubj_corresponds_argument amod_argument_sole poss_argument_its det_preposition_the prep_to_corresponds_preposition nsubj_corresponds_name prep_of_name_which det_name_the rcmod_function_corresponds det_function_a conj_and_substituted_corresponds agent_substituted_function auxpass_substituted_was nsubjpass_substituted_head nn_NP_PP-IN appos_phrase_NP amod_phrase_prepositional det_phrase_a prep_of_head_phrase det_head_The amod_way_following det_way_the prep_in_modified_way nsubj_modified_we dobj_modified_which dep_Collins_corresponds dep_Collins_substituted rcmod_Collins_modified dep_Collins_1996 prep_by_used_Collins advmod_used_also amod_Jelinek_1994 dep_Jelinek_al. nn_Jelinek_et conj_and_Magerman_used dep_Magerman_Jelinek dep_Magerman_1995 agent_described_used agent_described_Magerman vmod_that_described prep_to_similar_that amod_rules_similar prep_of_set_rules det_set_a dobj_used_set nsubj_used_we nn_head-word_tree det_head-word_the ccomp_determine_used dobj_determine_head-word aux_determine_To
W98-1117	J96-1002	o	Its applications range from sentence boundary disambiguation -LRB- Reynar and Ratnaparkhi 1997 -RRB- to part-of-speech tagging -LRB- Ratnaparkhi 1996 -RRB- parsing -LRB- Ratnaparkhi 1997 -RRB- and machine translation -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_translation_machine amod_Ratnaparkhi_1997 dep_parsing_Ratnaparkhi amod_Ratnaparkhi_1996 conj_and_tagging_translation conj_and_tagging_parsing dep_tagging_Ratnaparkhi amod_tagging_part-of-speech dep_Reynar_1997 conj_and_Reynar_Ratnaparkhi dep_disambiguation_Ratnaparkhi dep_disambiguation_Reynar nn_disambiguation_boundary nn_disambiguation_sentence dep_range_Berger prep_to_range_translation prep_to_range_parsing prep_to_range_tagging prep_from_range_disambiguation nsubj_range_applications poss_applications_Its
W98-1118	J96-1002	p	Clearly a more sophisticated feature selection routine such as the ones in -LRB- Berger et al. 1996 -RRB- or -LRB- Berger and Printz 1998 -RRB- would be required in this case	det_case_this prep_in_required_case auxpass_required_be aux_required_would nsubjpass_required_selection det_required_a advmod_required_Clearly amod_Berger_1998 conj_and_Berger_Printz conj_or_Berger_Printz conj_or_Berger_Berger amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_in_ones_Berger prep_in_ones_Berger det_ones_the prep_such_as_routine_ones amod_selection_routine nn_selection_feature amod_selection_sophisticated advmod_sophisticated_more advcl_``_required
W98-1118	J96-1002	o	Other recent work has applied M.E. to language modeling -LRB- Rosenfeld 1994 -RRB- machine translation -LRB- Berger et al. 1996 -RRB- and reference resolution -LRB- Kehler 1997 -RRB-	amod_Kehler_1997 dep_resolution_Kehler nn_resolution_reference amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_translation_resolution dep_translation_Berger nn_translation_machine amod_Rosenfeld_1994 dep_modeling_Rosenfeld nn_modeling_language parataxis_applied_resolution parataxis_applied_translation prep_to_applied_modeling dobj_applied_M.E. aux_applied_has nsubj_applied_work amod_work_recent amod_work_Other ccomp_``_applied
W98-1118	J96-1002	o	This allows us to compute the conditional probability as follows -LRB- Berger et al. 1996 -RRB- P -LRB- flh -RRB- = ~ i ~ ' -LRB- h ` I -RRB- -LRB- 2 -RRB- Z ~ -LRB- h -RRB- Z ~ -LRB- h -RRB- = ~ I ~ I ~ ' -LRB- h ' ~ -RRB- -LRB- a -RRB- ff i The maximum entropy estimation technique guarantees that for every feature gi the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus	nn_corpus_training det_corpus_the prep_of_expectation_gi amod_expectation_empirical det_expectation_the prep_in_equal_corpus dobj_equal_expectation aux_equal_will nsubj_equal_value prep_for_equal_gi nsubj_equal_that nn_model_M.E. det_model_the pobj_value_model prepc_according_to_value_to prep_of_value_gi amod_value_expected det_value_the nn_gi_feature det_gi_every rcmod_guarantees_equal nn_guarantees_technique nn_guarantees_estimation amod_guarantees_entropy nn_guarantees_maximum det_guarantees_The nn_guarantees_i dep_ff_guarantees det_ff_a dep_~_ff dep_~_h nn_~_~ dep_~_I dobj_~_~ nsubj_~_I rcmod_~_~ dobj_=_~ dep_~_= appos_~_h nn_~_Z nn_~_~ nn_~_Z dep_~_2 nn_~_P appos_~_h dep_h_I nn_~_i nn_~_~ dep_=_~ appos_P_h amod_P_= appos_P_flh amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_follows_Berger mark_follows_as amod_probability_conditional det_probability_the advcl_compute_follows dobj_compute_probability aux_compute_to parataxis_allows_~ xcomp_allows_compute dobj_allows_us nsubj_allows_This
W98-1118	J96-1002	p	More complete discussions of M.E. as applied to computational linguistics including a description of the M.E. estimation procedure can be found in -LRB- Berger et al. 1996 -RRB- and -LRB- Della Pietra et al. 1995 -RRB-	amod_Pietra_1995 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della conj_and_Berger_Pietra amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_in_found_Pietra prep_in_found_Berger auxpass_found_be aux_found_can nn_procedure_estimation nn_procedure_M.E. det_procedure_the prep_of_description_procedure det_description_a prep_including_linguistics_description amod_linguistics_computational dep_applied_found prep_to_applied_linguistics mark_applied_as dep_discussions_applied prep_of_discussions_M.E. amod_discussions_complete amod_discussions_More
A00-1007	J96-2004	o	1 Introduction on measures for inter-rater reliability -LRB- Carletta 1996 -RRB- on frameworks for evaluating spoken dialogue agents -LRB- Walker et al. 1998 -RRB- and on the use of different corpora in the development of a particular system -LRB- The Carnegie-Mellon Communicator Eskenazi et al.	nn_al._et nn_al._Eskenazi dep_Communicator_al. nn_Communicator_Carnegie-Mellon det_Communicator_The dep_system_Communicator amod_system_particular det_system_a prep_of_development_system det_development_the amod_corpora_different prep_in_use_development prep_of_use_corpora det_use_the pobj_on_use amod_Walker_1998 dep_Walker_al. nn_Walker_et nn_agents_dialogue amod_agents_spoken dobj_evaluating_agents dep_Carletta_1996 amod_reliability_inter-rater appos_measures_Carletta prep_for_measures_reliability conj_and_Introduction_on dep_Introduction_Walker prepc_for_Introduction_evaluating prep_on_Introduction_frameworks prep_on_Introduction_measures num_Introduction_1
A00-1012	J96-2004	o	Carletta -LRB- 1996 -RRB- argues that the kappa statistic -LRB- a -RRB- should be adopted to judge annotator consistency for classification tasks in the area of discourse and dialogue analysis	nn_analysis_dialogue nn_analysis_discourse conj_and_discourse_dialogue prep_of_area_analysis det_area_the prep_in_tasks_area nn_tasks_classification nn_consistency_annotator prep_for_judge_tasks dobj_judge_consistency aux_judge_to xcomp_adopted_judge auxpass_adopted_be aux_adopted_should nsubjpass_adopted_statistic mark_adopted_that appos_statistic_a nn_statistic_kappa det_statistic_the ccomp_argues_adopted nsubj_argues_Carletta appos_Carletta_1996
A00-1012	J96-2004	o	It has been claimed that content analysis researchers usually regard a > .8 to demonstrate good reliability and .67 < ~ < .8 alf16 lows tentative conclusions to be drawn -LRB- see Carletta -LRB- 1996 -RRB- -RRB-	appos_Carletta_1996 dobj_see_Carletta parataxis_drawn_see auxpass_drawn_be aux_drawn_to amod_conclusions_tentative nn_conclusions_lows nn_conclusions_alf16 num_conclusions_.8 amod_conclusions_< nn_conclusions_.67 quantmod_.8_< number_.8_~ conj_and_reliability_conclusions amod_reliability_good xcomp_demonstrate_drawn dobj_demonstrate_conclusions dobj_demonstrate_reliability aux_demonstrate_to quantmod_.8_> det_.8_a xcomp_regard_demonstrate dobj_regard_.8 advmod_regard_usually nsubj_regard_researchers mark_regard_that nn_researchers_analysis nn_researchers_content ccomp_claimed_regard auxpass_claimed_been aux_claimed_has nsubjpass_claimed_It
A00-1012	J96-2004	o	Carletta mentions this problem asking what the difference would be if the kappa statistic were computed across clause boundaries transcribed word boundaries and transcribed phoneme boundaries -LRB- Carletta 1996 p. 252 -RRB- rather than the sentence boundaries she suggested	nsubj_suggested_she rcmod_boundaries_suggested nn_boundaries_sentence det_boundaries_the num_p._252 dep_Carletta_p. dep_Carletta_1996 nn_boundaries_phoneme amod_boundaries_transcribed nn_boundaries_word amod_boundaries_transcribed conj_and_boundaries_boundaries conj_and_boundaries_boundaries nn_boundaries_clause prep_across_computed_boundaries prep_across_computed_boundaries prep_across_computed_boundaries auxpass_computed_were nsubjpass_computed_statistic mark_computed_if nn_statistic_kappa det_statistic_the advcl_be_computed aux_be_would nsubj_be_difference dobj_be_what det_difference_the conj_negcc_asking_boundaries dep_asking_Carletta ccomp_asking_be det_problem_this vmod_mentions_boundaries vmod_mentions_asking dobj_mentions_problem nsubj_mentions_Carletta
A00-2003	J96-2004	o	len. median length of sequences of co-specifying referring expressions with Cohen 's n -LRB- Cohen 1960 Carletta 1996 -RRB-	amod_Carletta_1996 dep_Cohen_Carletta amod_Cohen_1960 appos_n_Cohen poss_n_Cohen prep_with_expressions_n amod_expressions_referring prep_of_sequences_co-specifying dep_length_expressions prep_of_length_sequences amod_length_median dep_len._length dep_``_len.
A97-1050	J96-2004	o	4.5 Consistency of Annotations In order to assess the consistency of annotation we follow Carletta -LRB- 1996 -RRB- in using Cohen 's ~ a chancecorrected measure of inter-rater agreement	amod_agreement_inter-rater prep_of_measure_agreement amod_measure_chancecorrected det_measure_a appos_~_measure poss_~_Cohen dobj_using_~ appos_Carletta_1996 prepc_in_follow_using dobj_follow_Carletta nsubj_follow_we nsubj_follow_Consistency prep_of_consistency_annotation det_consistency_the dobj_assess_consistency aux_assess_to dep_assess_order mark_assess_In dep_Consistency_assess prep_of_Consistency_Annotations num_Consistency_4.5
C00-1039	J96-2004	o	It ewduato.s the pairwise agreement mnong a set of coders making category.iudgment correcting tbr expected chance agreement -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_agreement_Carletta nn_agreement_chance dobj_expected_agreement vmod_correcting_expected dobj_correcting_tbr dobj_making_category.iudgment vmod_coders_making det_set_a dep_set_mnong advmod_agreement_set amod_agreement_pairwise det_agreement_the xcomp_ewduato.s_correcting prep_of_ewduato.s_coders dobj_ewduato.s_agreement nsubj_ewduato.s_It ccomp_``_ewduato.s
C04-1020	J96-2004	o	In order to determine inter-annotator agreement for the database of annotated texts we computed kappa statistics -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_statistics_Carletta nn_statistics_kappa dobj_computed_statistics nsubj_computed_we advcl_computed_determine amod_texts_annotated prep_of_database_texts det_database_the prep_for_agreement_database amod_agreement_inter-annotator dobj_determine_agreement aux_determine_to dep_determine_order mark_determine_In
C04-1034	J96-2004	o	The reliability for the two annotation tasks -LRB- statistics -LRB- Carletta 1996 -RRB- -RRB- was of 0.94 and 0.90 respectively	advmod_0.94_respectively conj_and_0.94_0.90 prep_of_was_0.90 prep_of_was_0.94 nsubj_was_reliability dep_Carletta_1996 dep_statistics_Carletta dep_tasks_statistics nn_tasks_annotation num_tasks_two det_tasks_the prep_for_reliability_tasks det_reliability_The ccomp_``_was
C04-1035	J96-2004	o	-LSB- KD1 2371 -RSB- 2.3 Reliability To evaluate the reliability of the annotation we use the kappa coe cient -LRB- K -RRB- -LRB- Carletta 1996 -RRB- which measures pairwise agreement between a set of coders making category judgements correcting for expected chance agreement	nn_agreement_chance amod_agreement_expected prep_for_correcting_agreement nn_judgements_category dobj_making_judgements vmod_coders_making prep_of_set_coders det_set_a prep_between_agreement_set amod_agreement_pairwise dobj_measures_agreement nsubj_measures_which dep_Carletta_1996 appos_cient_Carletta appos_cient_K nn_cient_coe nn_cient_kappa det_cient_the dobj_use_cient nsubj_use_we det_annotation_the prep_of_reliability_annotation det_reliability_the dobj_evaluate_reliability aux_evaluate_To dep_Reliability_correcting rcmod_Reliability_measures rcmod_Reliability_use vmod_Reliability_evaluate num_Reliability_2.3 dep_Reliability_KD1 dep_KD1_2371
C04-1128	J96-2004	o	The kappa statistic -LRB- Carletta 1996 -RRB- for identifying question segments is 0.68 and for linking question and answer segments given a question segment is 0.81	cop_0.81_is prepc_for_0.81_linking nn_segment_question det_segment_a pobj_given_segment prep_segments_given nn_segments_answer nn_segments_question conj_and_question_answer dobj_linking_segments cop_0.68_is nsubj_0.68_segments rcmod_question_0.68 dobj_identifying_question dep_Carletta_1996 conj_and_statistic_0.81 prepc_for_statistic_identifying dep_statistic_Carletta nn_statistic_kappa det_statistic_The dep_``_0.81 dep_``_statistic
C04-1161	J96-2004	o	Carletta -LRB- 1996 -RRB- says that 0.67 a10a14a11a15a10 0.8 allows just tentative conclusions to be drawn	auxpass_drawn_be aux_drawn_to amod_conclusions_tentative advmod_tentative_just xcomp_allows_drawn dobj_allows_conclusions nsubj_allows_a10a14a11a15a10 mark_allows_that num_a10a14a11a15a10_0.8 num_a10a14a11a15a10_0.67 ccomp_says_allows nsubj_says_Carletta appos_Carletta_1996
C08-1109	J96-2004	o	While the need for annotation by multiple raters has been well established in NLP tasks -LRB- Carletta 1996 -RRB- most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors or to check the systems output	nn_output_systems det_output_the dobj_check_output aux_check_to nn_errors_learner prep_of_corpus_errors amod_corpus_annotated det_corpus_an conj_or_create_check dobj_create_corpus preconj_create_either aux_create_to num_rater_one advmod_rater_only ccomp_relied_check ccomp_relied_create prep_on_relied_rater advmod_relied_surprisingly aux_relied_has nsubj_relied_work advcl_relied_established nn_detection_error prep_in_work_detection amod_work_previous amod_work_most dep_Carletta_1996 appos_tasks_Carletta nn_tasks_NLP prep_in_established_tasks advmod_established_well auxpass_established_been aux_established_has nsubjpass_established_need mark_established_While amod_raters_multiple prep_by_need_raters prep_for_need_annotation det_need_the ccomp_``_relied
C96-1059	J96-2004	o	To support a more rigorous analysis however wc have followed Carletta 's suggestion -LRB- 1996 -RRB- of using the K coettMcnt -LRB- Siegel and Castellan 1988 -RRB- as a measure of coder agreement	nn_agreement_coder prep_of_measure_agreement det_measure_a amod_Siegel_1988 conj_and_Siegel_Castellan dep_coettMcnt_Castellan dep_coettMcnt_Siegel nn_coettMcnt_K det_coettMcnt_the prep_as_using_measure dobj_using_coettMcnt prepc_of_suggestion_using appos_suggestion_1996 poss_suggestion_Carletta dobj_followed_suggestion aux_followed_have nsubj_followed_wc advmod_followed_however advcl_followed_support amod_analysis_rigorous det_analysis_a advmod_rigorous_more dobj_support_analysis aux_support_To ccomp_``_followed
D08-1021	J96-2004	o	We measured inter-annotator agreement with the Kappa statistic -LRB- Carletta 1996 -RRB- using the 1,391 items that two annotators scored in common	prep_in_scored_common nsubj_scored_annotators dobj_scored_that num_annotators_two rcmod_items_scored num_items_1,391 det_items_the dobj_using_items amod_Carletta_1996 dep_statistic_Carletta nn_statistic_Kappa det_statistic_the prep_with_agreement_statistic amod_agreement_inter-annotator xcomp_measured_using dobj_measured_agreement nsubj_measured_We
D09-1150	J96-2004	o	3.1 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 det_purpose_this dep_measure_Carletta prep_for_measure_purpose amod_measure_standard det_measure_a nn_community_Linguistics nn_community_Computational det_community_the prep_as_adopted_measure agent_adopted_community vmod_statistic_adopted det_statistic_a cop_statistic_is nsubj_statistic_coefficient prep_of_coefficient_agreement nn_coefficient_kappa det_coefficient_The nn_Classes_Emotion rcmod_Agreement_statistic prep_for_Agreement_Classes num_Agreement_3.1
D09-1155	J96-2004	p	As agreement measure we choose the Kappa coefficient -LRB- Fleiss 1971 Siegel and Castellan 1988 -RRB- the agreement measure predominantly used in natural language processing research -LRB- Carletta 1996 -RRB-	dep_Carletta_1996 dep_research_Carletta nn_research_processing nn_research_language amod_research_natural prep_in_used_research advmod_used_predominantly vmod_measure_used nn_measure_agreement det_measure_the dep_Siegel_1988 conj_and_Siegel_Castellan dep_Fleiss_Castellan dep_Fleiss_Siegel conj_Fleiss_1971 appos_coefficient_measure appos_coefficient_Fleiss nn_coefficient_Kappa det_coefficient_the dobj_choose_coefficient nsubj_choose_we rcmod_measure_choose nn_measure_agreement pobj_As_measure dep_``_As
E06-1007	J96-2004	o	We then examined the inter-annotator reliability of the annotation by calculating the score -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_score_Carletta det_score_the dobj_calculating_score det_annotation_the prep_of_reliability_annotation amod_reliability_inter-annotator det_reliability_the prepc_by_examined_calculating dobj_examined_reliability advmod_examined_then nsubj_examined_We ccomp_``_examined
E99-1006	J96-2004	o	After each step the annotations were compared using the ~ statistic as reliability measure for all classification tasks -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_tasks_Carletta nn_tasks_classification det_tasks_all prep_for_measure_tasks nn_measure_reliability nn_statistic_~ det_statistic_the prep_as_using_measure dobj_using_statistic pcomp_compared_using auxpass_compared_were nsubjpass_compared_annotations prep_after_compared_step det_annotations_the det_step_each
E99-1015	J96-2004	o	Kappa is a better measurement of agreement than raw percentage agreement -LRB- Carletta 1996 -RRB- because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders	amod_coders_real det_coders_the prep_of_distribution_categories amod_distribution_same det_distribution_the prep_as_using_coders dobj_using_distribution amod_annotators_random xcomp_reached_using agent_reached_annotators auxpass_reached_be aux_reached_would nsubjpass_reached_which prep_of_level_agreement det_level_the rcmod_factors_reached prep_out_factors_level nsubj_factors_it mark_factors_because amod_Carletta_1996 dep_agreement_Carletta nn_agreement_percentage amod_agreement_raw advcl_measurement_factors prep_than_measurement_agreement prep_of_measurement_agreement amod_measurement_better det_measurement_a cop_measurement_is nsubj_measurement_Kappa
H05-1031	J96-2004	o	5.2 Results on the Newsblaster data We measured how well the models trained on DUC data perform with current news labeled using human 4http / / newsblaster.cs.columbia.edu 5a20 -LRB- kappa -RRB- is a measure of inter-annotator agreement over and above what might be expected by pure chance -LRB- See Carletta -LRB- 1996 -RRB- for discussion of its use in NLP -RRB- a20a22a21a24a23 if there is perfect agreement between annotators anda20a25a21a27a26 if the annotators agree only as much as you would expect by chance	prep_by_expect_chance aux_expect_would nsubj_expect_you mark_expect_as advmod_much_as advmod_much_only advcl_agree_expect dobj_agree_much nsubj_agree_annotators mark_agree_if det_annotators_the nn_anda20a25a21a27a26_annotators prep_between_agreement_anda20a25a21a27a26 amod_agreement_perfect advcl_is_agree nsubj_is_agreement expl_is_there mark_is_if prep_in_use_NLP poss_use_its prep_of_discussion_use prep_for_Carletta_discussion appos_Carletta_1996 dobj_See_Carletta amod_chance_pure agent_expected_chance auxpass_expected_be aux_expected_might nsubjpass_expected_what amod_agreement_inter-annotator advcl_measure_is dep_measure_a20a22a21a24a23 dep_measure_See prepc_above_measure_expected prepc_over_measure_expected prep_of_measure_agreement conj_and_measure_measure det_measure_a cop_measure_is nsubj_measure_5a20 appos_5a20_kappa nn_5a20_newsblaster.cs.columbia.edu amod_4http_human dobj_using_4http xcomp_labeled_using vmod_news_labeled amod_news_current prep_with_perform_news nsubj_perform_models advmod_perform_well nn_data_DUC prep_on_trained_data vmod_models_trained det_models_the advmod_well_how ccomp_measured_perform nsubj_measured_We rcmod_data_measured nn_data_Newsblaster det_data_the dep_Results_measure dep_Results_measure prep_on_Results_data num_Results_5.2
H05-1115	J96-2004	o	Once an acceptable rate of interjudge agreement was verified on the first nine clusters -LRB- Kappa -LRB- Carletta 1996 -RRB- of 0.68 -RRB- the remaining 11 clusters were annotated by one judge each	dep_judge_each num_judge_one prep_by_annotated_judge cop_annotated_were nsubj_annotated_clusters ccomp_annotated_verified num_clusters_11 amod_clusters_remaining det_clusters_the dep_Carletta_1996 prep_of_Kappa_0.68 dep_Kappa_Carletta appos_clusters_Kappa num_clusters_nine amod_clusters_first det_clusters_the prep_on_verified_clusters auxpass_verified_was nsubjpass_verified_rate amod_agreement_interjudge prep_of_rate_agreement amod_rate_acceptable det_rate_an advmod_rate_Once
J00-3003	J96-2004	o	As argued in Carletta -LRB- 1996 -RRB- Kappa values of 0.8 or higher are desirable for detecting associations between several coded variables we were thus satisfied with the level of agreement achieved	vmod_agreement_achieved prep_of_level_agreement det_level_the prep_with_satisfied_level advmod_satisfied_thus auxpass_satisfied_were nsubjpass_satisfied_we amod_variables_coded amod_variables_several prep_between_associations_variables dobj_detecting_associations parataxis_desirable_satisfied prepc_for_desirable_detecting cop_desirable_are csubj_desirable_argued conj_or_0.8_higher prep_of_values_higher prep_of_values_0.8 nn_values_Kappa appos_Carletta_values appos_Carletta_1996 prep_in_argued_Carletta mark_argued_As
J00-4003	J96-2004	o	Agreement among annotators was measured using the K statistic -LRB- Siegel and Castellan 1988 Carletta 1996 -RRB-	num_Carletta_1996 num_Castellan_1988 dep_Siegel_Carletta conj_and_Siegel_Castellan appos_statistic_Castellan appos_statistic_Siegel nn_statistic_K det_statistic_the dobj_using_statistic xcomp_measured_using auxpass_measured_was nsubjpass_measured_Agreement prep_among_Agreement_annotators
J01-3003	J96-2004	o	Carletta -LRB- 1996 -RRB- cites the convention from the domain of content analysis indicating that .67 K K < .8 indicates marginal agreement while K > .8 is an indication of good agreement	amod_agreement_good prep_of_indication_agreement det_indication_an cop_indication_is nsubj_indication_K mark_indication_while num_>_.8 amod_K_> amod_agreement_marginal advcl_indicates_indication dobj_indicates_agreement nsubj_indicates_K mark_indicates_that quantmod_.8_< num_K_.8 nn_K_K num_K_.67 ccomp_indicating_indicates nn_analysis_content prep_of_domain_analysis det_domain_the vmod_convention_indicating prep_from_convention_domain det_convention_the dobj_cites_convention nsubj_cites_Carletta appos_Carletta_1996
J02-3004	J96-2004	p	Although the Kappa coefficient has a number of advantages over percentage agreement -LRB- e.g. it takes into account the expected chance interrater agreement see Carletta -LRB- 1996 -RRB- for details -RRB- we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below whose performance will also be reported in terms of percentage agreement	nn_agreement_percentage prep_of_terms_agreement prep_in_reported_terms auxpass_reported_be advmod_reported_also aux_reported_will nsubjpass_reported_performance poss_performance_whose prepc_below_described_reported vmod_methods_described amod_methods_automatic det_methods_the conj_and_performance_methods amod_performance_human det_performance_the pobj_straightforwardly_methods pobj_straightforwardly_performance prep_compare_straightforwardly aux_compare_to xcomp_allows_compare dobj_allows_us nsubj_allows_it mark_allows_as nn_agreement_percentage dobj_report_agreement advmod_report_also nsubj_report_we prep_for_Carletta_details appos_Carletta_1996 dobj_see_Carletta nn_agreement_interrater nn_agreement_chance amod_agreement_expected det_agreement_the advcl_takes_allows parataxis_takes_report parataxis_takes_see dobj_takes_agreement prep_into_takes_account nsubj_takes_it advmod_takes_e.g. nsubj_takes_agreement mark_takes_over nn_agreement_percentage rcmod_advantages_takes prep_of_number_advantages det_number_a dobj_has_number nsubj_has_coefficient mark_has_Although nn_coefficient_Kappa det_coefficient_the advcl_``_has
J02-4001	J96-2004	o	Other commonly used measures include kappa -LRB- Carletta 1996 -RRB- and relative utility -LRB- Radev Jing and Budzikowska 2000 -RRB- both of which take into account the performance of a summarizer that randomly picks passages from the original document to produce an extract	det_extract_an dobj_produce_extract aux_produce_to amod_document_original det_document_the vmod_picks_produce prep_from_picks_document dobj_picks_passages advmod_picks_randomly nsubj_picks_that rcmod_summarizer_picks det_summarizer_a prep_of_performance_summarizer det_performance_the dobj_take_performance prep_into_take_account nsubj_take_both prep_of_both_which num_Budzikowska_2000 conj_and_Radev_Budzikowska conj_and_Radev_Jing dep_utility_Budzikowska dep_utility_Jing dep_utility_Radev amod_utility_relative num_Carletta_1996 rcmod_kappa_take conj_and_kappa_utility appos_kappa_Carletta dobj_include_utility dobj_include_kappa nsubj_include_measures amod_measures_used amod_measures_Other advmod_used_commonly
J02-4002	J96-2004	o	421 Teufel and Moens Summarizing Scientific Articles We use the kappa coefficient K -LRB- Siegel and Castellan 1988 -RRB- to measure stability and reproducibility following Carletta -LRB- 1996 -RRB-	appos_Carletta_1996 conj_and_stability_reproducibility dobj_measure_reproducibility dobj_measure_stability aux_measure_to num_Castellan_1988 conj_and_Siegel_Castellan dep_K_Castellan dep_K_Siegel nn_K_coefficient nn_K_kappa det_K_the prep_following_use_Carletta vmod_use_measure dobj_use_K nsubj_use_We nn_Articles_Scientific ccomp_Summarizing_use dobj_Summarizing_Articles vmod_Moens_Summarizing conj_and_Teufel_Moens num_Teufel_421 dep_``_Moens dep_``_Teufel
J03-1004	J96-2004	o	As Carletta -LRB- 1996 -RRB- notes many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff and according to Fleiss -LRB- 1981 -RRB- kappa values between .4 and .75 indicate fair to good agreement anyhow	advmod_agreement_anyhow amod_agreement_good prep_to_fair_agreement acomp_indicate_fair nsubj_indicate_Fleiss conj_and_.4_.75 prep_between_values_.75 prep_between_values_.4 nn_values_kappa appos_Fleiss_values appos_Fleiss_1981 pcomp_to_indicate pcomp_according_to nsubj_according_tasks agent_addressed_Krippendorff vmod_classifications_addressed nn_classifications_analysis nn_classifications_content det_classifications_the conj_and_difficult_according prep_than_difficult_classifications advmod_difficult_more advmod_difficult_simply cop_difficult_are nsubj_difficult_tasks prep_as_difficult_notes amod_linguistics_computational prep_in_tasks_linguistics amod_tasks_many nn_notes_Carletta appos_Carletta_1996
J04-1005	J96-2004	o	Carletta -LRB- 1996 -RRB- deserves the credit for bringing to the attention of computational linguists	amod_linguists_computational prep_of_attention_linguists det_attention_the prep_to_bringing_attention det_credit_the prepc_for_deserves_bringing dobj_deserves_credit nsubj_deserves_Carletta appos_Carletta_1996
J04-3003	J96-2004	o	One of our goals was to use for this study only information that could be annotated reliably -LRB- Passonneau and Litman 1993 Carletta 1996 -RRB- as we believe this will make our results easier to replicate	aux_replicate_to ccomp_easier_replicate nsubj_easier_results poss_results_our xcomp_make_easier aux_make_will nsubj_make_this ccomp_believe_make nsubj_believe_we mark_believe_as num_Carletta_1996 num_Litman_1993 dep_Passonneau_Carletta conj_and_Passonneau_Litman advmod_annotated_reliably cop_annotated_be aux_annotated_could nsubj_annotated_that appos_information_Litman appos_information_Passonneau rcmod_information_annotated advmod_information_only det_study_this dobj_use_information prep_for_use_study aux_use_to advcl_was_believe xcomp_was_use nsubj_was_One poss_goals_our prep_of_One_goals ccomp_``_was
J04-3003	J96-2004	o	The agreement on identifying the boundaries of units using the kappa statistic discussed in Carletta -LRB- 1996 -RRB- was = .9 -LRB- for two annotators and 500 units -RRB- the agreement on features -LRB- two annotators and at least 200 units -RRB- was as follows utype = .76 verbed = .9 nite = .81	dep_=_.81 dep_=_.9 dep_verbed_= dep_=_.76 dep_utype_= dep_utype_nite dep_utype_verbed dep_utype_= mark_follows_as dep_was_utype advcl_was_follows nsubj_was_agreement num_units_200 quantmod_200_at mwe_at_least conj_and_annotators_units num_annotators_two dep_features_units dep_features_annotators prep_on_agreement_features det_agreement_the num_units_500 conj_and_annotators_units num_annotators_two prep_for_.9_units prep_for_.9_annotators parataxis_=_was dobj_=_.9 cop_=_was nsubj_=_agreement appos_Carletta_1996 prep_in_discussed_Carletta vmod_statistic_discussed nn_statistic_kappa det_statistic_the dobj_using_statistic prep_of_boundaries_units det_boundaries_the dobj_identifying_boundaries vmod_agreement_using prepc_on_agreement_identifying det_agreement_The
J05-2005	J96-2004	o	In order to determine interannotator agreement for step 2 of the coding procedure for the database of annotated texts we calculated kappa statistics -LRB- Carletta 1996 -RRB-	num_Carletta_1996 appos_statistics_Carletta nn_statistics_kappa dobj_calculated_statistics nsubj_calculated_we advcl_calculated_determine amod_texts_annotated prep_of_database_texts det_database_the prep_for_procedure_database amod_procedure_coding det_procedure_the prep_of_step_procedure num_step_2 prep_for_agreement_step nn_agreement_interannotator dobj_determine_agreement aux_determine_to dep_determine_order mark_determine_In
J05-3001	J96-2004	o	For example the coding manual for the Switchboard DAMSL dialogue act annotation scheme -LRB- Jurafsky Shriberg and Biasca 1997 page 2 -RRB- states that kappa is used to assess labelling accuracy and Di Eugenio and Glass -LRB- 2004 -RRB- relate reliability to the objectivity of decisions whereas Carletta -LRB- 1996 -RRB- regards reliability as the degree to which we understand the judgments that annotators are asked to make	aux_make_to xcomp_asked_make auxpass_asked_are nsubjpass_asked_annotators dobj_asked_that rcmod_judgments_asked det_judgments_the dobj_understand_judgments nsubj_understand_we prep_to_understand_which rcmod_degree_understand det_degree_the prep_as_regards_degree dobj_regards_reliability nsubj_regards_Carletta appos_Carletta_1996 prep_of_objectivity_decisions det_objectivity_the prep_to_relate_objectivity dobj_relate_reliability nsubj_relate_Glass nsubj_relate_Eugenio appos_Glass_2004 conj_and_Eugenio_Glass nn_Eugenio_Di nn_accuracy_labelling dobj_assess_accuracy aux_assess_to parataxis_used_regards dep_used_whereas conj_and_used_relate xcomp_used_assess auxpass_used_is nsubjpass_used_kappa mark_used_that ccomp_states_relate ccomp_states_used nsubj_states_scheme num_page_2 num_Biasca_1997 appos_Jurafsky_page conj_and_Jurafsky_Biasca conj_and_Jurafsky_Shriberg dep_scheme_Biasca dep_scheme_Shriberg dep_scheme_Jurafsky nn_scheme_annotation ccomp_act_states nsubj_act_manual prep_for_act_example nn_dialogue_DAMSL nn_dialogue_Switchboard det_dialogue_the prep_for_manual_dialogue amod_manual_coding det_manual_the
J05-3001	J96-2004	o	This is an unsuitable measure for inferring reliability and it was the use of this measure that prompted Carletta -LRB- 1996 -RRB- to recommend chance-corrected measures	amod_measures_chance-corrected dobj_recommend_measures aux_recommend_to appos_Carletta_1996 xcomp_prompted_recommend dobj_prompted_Carletta nsubj_prompted_that det_measure_this rcmod_use_prompted prep_of_use_measure det_use_the cop_use_was nsubj_use_it dobj_inferring_reliability conj_and_measure_use prepc_for_measure_inferring amod_measure_unsuitable det_measure_an cop_measure_is nsubj_measure_This
J05-3001	J96-2004	p	Since Jean Carletta -LRB- 1996 -RRB- exposed computational linguists to the desirability of using chance-corrected agreement statistics to infer the reliability of data generated by applying coding schemes there has been a general acceptance of their use within the field	det_field_the prep_within_use_field poss_use_their prep_of_acceptance_use amod_acceptance_general det_acceptance_a cop_acceptance_been aux_acceptance_has expl_acceptance_there prep_since_acceptance_Carletta amod_schemes_coding dobj_applying_schemes agent_generated_applying vmod_data_generated prep_of_reliability_data det_reliability_the dobj_infer_reliability aux_infer_to nn_statistics_agreement amod_statistics_chance-corrected vmod_using_infer dobj_using_statistics prepc_of_desirability_using det_desirability_the prep_to_linguists_desirability amod_linguists_computational amod_linguists_exposed dep_Carletta_linguists appos_Carletta_1996 nn_Carletta_Jean
J05-3001	J96-2004	o	The prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies -LRB- Carletta 1996 Di Eugenio and Glass 2004 Krippendorff 2004a -RRB- is probably due to a desire for a simple system that can be easily applied to a scheme	det_scheme_a prep_to_applied_scheme advmod_applied_easily auxpass_applied_be aux_applied_can nsubjpass_applied_that rcmod_system_applied amod_system_simple det_system_a prep_for_desire_system det_desire_a prep_to_due_desire advmod_due_probably cop_due_is nsubj_due_use nn_2004a_Krippendorff num_Glass_2004 conj_and_Eugenio_Glass nn_Eugenio_Di dep_Carletta_2004a dep_Carletta_Glass dep_Carletta_Eugenio num_Carletta_1996 dep_studies_Carletta det_studies_all prep_for_suitable_studies cop_suitable_be aux_suitable_to xcomp_unlikely_suitable cop_unlikely_is nsubj_unlikely_it mark_unlikely_that ccomp_advice_unlikely amod_advice_repeated det_criterion_this prep_despite_use_advice prep_of_use_criterion amod_use_prevalent det_use_The
J07-1002	J96-2004	o	G-Theory and Agreement Indices Two well-known measures for capturing the quality of manual annotations are agreement percentages and the kappa statistic -LRB- Cohen 1960 Carletta 1996 Eugenio and Glass 2004 -RRB-	num_Eugenio_2004 conj_and_Eugenio_Glass num_Carletta_1996 dep_Cohen_Glass dep_Cohen_Eugenio dep_Cohen_Carletta num_Cohen_1960 appos_statistic_Cohen nn_statistic_kappa det_statistic_the conj_and_percentages_statistic nn_percentages_agreement cop_percentages_are nsubj_percentages_measures nsubj_percentages_G-Theory amod_annotations_manual prep_of_quality_annotations det_quality_the dobj_capturing_quality amod_measures_well-known num_measures_Two nn_measures_Indices nn_measures_Agreement prepc_for_G-Theory_capturing conj_and_G-Theory_measures
J08-3001	J96-2004	o	He uses a specic reliability statistic for his measurements but Carletta -LRB- 1996 -RRB- implicitly assumes kappa-like metrics are similar enough in practice for the rule of thumb to apply to them as well.A detailed discussion on the differences and similarities of these and other measures is provided by Krippendorff -LRB- 2004 -RRB- in this article we will use Cohens -LRB- 1960 -RRB- to investigate the value of the 0.8 reliability cut-off for computational linguistics	amod_linguistics_computational prep_for_cut-off_linguistics amod_reliability_cut-off num_reliability_0.8 det_reliability_the prep_of_value_reliability det_value_the dobj_investigate_value aux_investigate_to appos_Cohens_1960 vmod_use_investigate dobj_use_Cohens aux_use_will nsubj_use_we prep_in_use_article det_article_this appos_Krippendorff_2004 agent_provided_Krippendorff auxpass_provided_is nsubjpass_provided_measures nsubj_other_metrics prep_of_similarities_these conj_and_differences_similarities det_differences_the prep_on_discussion_similarities prep_on_discussion_differences amod_discussion_detailed nn_discussion_well.A prep_as_apply_discussion prep_to_apply_them aux_apply_to nsubj_apply_rule mark_apply_for prep_of_rule_thumb det_rule_the ccomp_enough_apply prep_in_enough_practice parataxis_similar_use conj_and_similar_provided conj_and_similar_other dep_similar_enough cop_similar_are nsubj_similar_metrics amod_metrics_kappa-like ccomp_assumes_provided ccomp_assumes_other ccomp_assumes_similar advmod_assumes_implicitly nsubj_assumes_Carletta appos_Carletta_1996 poss_measurements_his nn_statistic_reliability amod_statistic_specic det_statistic_a conj_but_uses_assumes prep_for_uses_measurements dobj_uses_statistic nsubj_uses_He
J97-1002	J96-2004	o	4.2 Interpreting reliability results It has been argued elsewhere -LRB- Carletta 1996 -RRB- that since the amount of agreement one would expect by chance depends on the number and relative frequencies of the categories under test reliability for category classifications should be measured using the kappa coefficient	nn_coefficient_kappa det_coefficient_the dobj_using_coefficient xcomp_measured_using auxpass_measured_be aux_measured_should nsubjpass_measured_reliability nn_classifications_category prep_for_reliability_classifications det_categories_the amod_frequencies_relative prep_of_number_categories conj_and_number_frequencies det_number_the prep_under_depends_test prep_on_depends_frequencies prep_on_depends_number nsubj_depends_Carletta prep_by_expect_chance aux_expect_would prep_since_expect_amount nsubj_expect_that num_agreement_one prep_of_amount_agreement det_amount_the rcmod_Carletta_expect num_Carletta_1996 ccomp_argued_measured ccomp_argued_depends advmod_argued_elsewhere auxpass_argued_been aux_argued_has nsubjpass_argued_It ccomp_results_argued nsubj_results_reliability amod_reliability_Interpreting num_reliability_4.2
J97-1003	J96-2004	o	6.1 Reader Judgments There is a growing concern surrounding issues of intercoder reliability when using human judgments to evaluate discourse-processing algorithms -LRB- Carletta 1996 Condon and Cech 1995 -RRB-	dep_Condon_1995 conj_and_Condon_Cech dep_Carletta_Cech dep_Carletta_Condon num_Carletta_1996 appos_algorithms_Carletta nn_algorithms_discourse-processing dobj_evaluate_algorithms aux_evaluate_to amod_judgments_human vmod_using_evaluate dobj_using_judgments advmod_using_when nn_reliability_intercoder advcl_issues_using prep_of_issues_reliability amod_issues_surrounding vmod_concern_issues amod_concern_growing det_concern_a nsubj_is_concern expl_is_There rcmod_Judgments_is nn_Judgments_Reader num_Judgments_6.1
J97-1003	J96-2004	o	Proposals have recently been made for protocols for the collection of human discourse segmentation data -LRB- Nakatani et al. 1995 -RRB- and for how to evaluate the validity of judgments so obtained -LRB- Carletta 1996 Isard and Carletta 1995 Ros6 1995 Passonneau and Litman 1993 Litman and Passonneau 1995 -RRB-	num_Passonneau_1995 conj_and_Litman_Passonneau num_Litman_1993 conj_and_Passonneau_Litman amod_1995_Ros6 num_Carletta_1995 conj_and_Isard_Carletta dep_Carletta_Passonneau dep_Carletta_Litman dep_Carletta_Litman dep_Carletta_Passonneau dep_Carletta_1995 dep_Carletta_Carletta dep_Carletta_Isard num_Carletta_1996 advmod_obtained_so appos_judgments_Carletta vmod_judgments_obtained prep_of_validity_judgments det_validity_the dobj_evaluate_validity aux_evaluate_to advmod_evaluate_how pcomp_for_evaluate nn_al._et dep_Nakatani_1995 advmod_Nakatani_al. nn_data_segmentation nn_data_discourse amod_data_human prep_of_collection_data det_collection_the prep_for_protocols_collection conj_and_made_for dep_made_Nakatani prep_for_made_protocols auxpass_made_been advmod_made_recently aux_made_have nsubjpass_made_Proposals
J97-1003	J96-2004	o	Carletta -LRB- 1996 -RRB- and Ros6 -LRB- 1995 -RRB- point out the importance of taking into account the expected chance agreement among judges when computing whether or not judges agree significantly	advmod_agree_significantly nsubj_agree_judges neg_judges_not cc_judges_or mark_judges_whether ccomp_computing_agree advmod_computing_when prep_among_agreement_judges nn_agreement_chance amod_agreement_expected det_agreement_the advcl_taking_computing dobj_taking_agreement prep_into_taking_account prepc_of_importance_taking det_importance_the nn_point_Ros6 appos_Ros6_1995 prep_out_Carletta_importance conj_and_Carletta_point appos_Carletta_1996
J97-1003	J96-2004	o	According to Carletta -LRB- 1996 -RRB- K measures pairwise agreement among a set of coders making category judgments correcting for expected chance agreement as follows KP -LRB- A -RRB- P -LRB- E -RRB- 1 P -LRB- E -RRB- where P -LRB- A -RRB- is the proportion of times that the coders agree and P -LRB- E -RRB- is the proportion of times that they would be expected to agree by chance	prep_by_agree_chance aux_agree_to xcomp_expected_agree auxpass_expected_be aux_expected_would nsubjpass_expected_they mark_expected_that ccomp_proportion_expected prep_of_proportion_times det_proportion_the cop_proportion_is nsubj_proportion_P appos_P_E conj_and_agree_proportion nsubj_agree_coders mark_agree_that det_coders_the ccomp_proportion_proportion ccomp_proportion_agree prep_of_proportion_times det_proportion_the cop_proportion_is nsubj_proportion_P advmod_proportion_where nn_proportion_P appos_P_A appos_P_E dep_1_proportion npadvmod_1_P appos_P_E dep_KP_1 appos_KP_A mark_follows_as dep_agreement_follows nn_agreement_chance amod_agreement_expected prep_for_correcting_agreement nn_judgments_category dobj_making_judgments vmod_coders_making prep_of_set_coders det_set_a prep_among_agreement_set amod_agreement_pairwise dep_measures_KP vmod_measures_correcting dobj_measures_agreement nsubj_measures_K pobj_measures_Carletta prepc_according_to_measures_to appos_Carletta_1996
J97-1003	J96-2004	o	Carletta -LRB- 1996 -RRB- also states that in the behavioral sciences K > .8 signals good replicability and .67 < K < .8 allows tentative conclusions to be drawn	auxpass_drawn_be aux_drawn_to amod_conclusions_tentative xcomp_allows_drawn dobj_allows_conclusions nsubj_allows_.67 nsubj_allows_signals prep_in_allows_sciences mark_allows_that num_<_.8 dep_K_< pobj_<_K prep_.67_< amod_replicability_good conj_and_signals_.67 dep_signals_replicability num_signals_.8 quantmod_.8_> dep_.8_K amod_sciences_behavioral det_sciences_the ccomp_states_allows advmod_states_also nsubj_states_Carletta appos_Carletta_1996
J97-1005	J96-2004	o	Reliability metrics -LRB- Krippendorff 1980 Carletta 1996 -RRB- are designed to give a robust measure of how well distinct sets of data agree with or replicate one another	dep_one_another conj_or_with_one conj_or_with_replicate prep_agree_one prep_agree_replicate prep_agree_with prep_of_sets_data amod_sets_distinct advmod_distinct_well advmod_well_how prep_of_measure_sets amod_measure_robust det_measure_a dobj_give_measure aux_give_to dep_designed_agree xcomp_designed_give auxpass_designed_are nsubjpass_designed_metrics num_Carletta_1996 dep_1980_Carletta amod_1980_Krippendorff dep_metrics_1980 nn_metrics_Reliability
J97-1005	J96-2004	o	In Hirschberg and Nakatani -LRB- 1996 -RRB- average reliability -LRB- measured using the kappa coefficient discussed in Carletta \ -LSB- 1996 \ -RSB- -RRB- of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker labeled using text and speech is .8 or above for both read and spontaneous speech values of at least .8 are typically viewed as representing high reliability -LRB- see Section 3.2 -RRB-	num_Section_3.2 dobj_see_Section amod_reliability_high dep_representing_see dobj_representing_reliability prepc_as_viewed_representing advmod_viewed_typically auxpass_viewed_are nsubjpass_viewed_values quantmod_.8_at mwe_at_least prep_of_values_.8 amod_speech_spontaneous conj_and_read_speech preconj_read_both parataxis_.8_viewed prep_for_.8_speech prep_for_.8_read conj_or_.8_above cop_.8_is nsubj_.8_reliability prep_in_.8_Nakatani prep_in_.8_Hirschberg conj_and_text_speech dobj_using_speech dobj_using_text xcomp_labeled_using vmod_speaker_labeled amod_speaker_same det_speaker_the agent_produced_speaker vmod_monologues_produced num_monologues_9 num_coders_3 amod_labels_segmentinitial num_\_1996 nn_\_Carletta prep_in_discussed_\ vmod_coefficient_discussed nn_coefficient_kappa det_coefficient_the dobj_using_coefficient dep_measured_\ xcomp_measured_using prep_on_reliability_monologues prep_among_reliability_coders prep_of_reliability_labels dep_reliability_measured amod_reliability_average appos_Nakatani_1996 conj_and_Hirschberg_Nakatani
J98-2001	J96-2004	o	Our study is also different from these previous ones in that measuring the agreement among annotators became an issue -LRB- Carletta 1996 -RRB-	num_Carletta_1996 appos_issue_Carletta det_issue_an xcomp_became_issue csubj_became_different prep_among_agreement_annotators det_agreement_the dobj_measuring_agreement vmod_that_measuring amod_ones_previous det_ones_these prep_in_different_that prep_from_different_ones advmod_different_also cop_different_is nsubj_different_study poss_study_Our
J98-2001	J96-2004	o	Idiom 0 0 1 1 0 2 V. Doubt 3 0 4 0 0 7 Total A 294160546 39 1 1,040 In order to measure the agreement in a more precise way we used the Kappa statistic -LRB- Siegel and Castellan 1988 -RRB- recently proposed by Carletta as a measure of agreement for discourse analysis -LRB- Carletta 1996 -RRB-	num_Carletta_1996 nn_analysis_discourse prep_for_agreement_analysis appos_measure_Carletta prep_of_measure_agreement det_measure_a prep_as_proposed_measure agent_proposed_Carletta advmod_proposed_recently num_Castellan_1988 conj_and_Siegel_Castellan dep_statistic_Castellan dep_statistic_Siegel nn_statistic_Kappa det_statistic_the vmod_used_proposed dobj_used_statistic nsubj_used_we ccomp_used_measure amod_way_precise det_way_a advmod_precise_more det_agreement_the prep_in_measure_way dobj_measure_agreement aux_measure_to dep_measure_order mark_measure_In dep_measure_Total dep_measure_7 dep_measure_0 dep_measure_0 nsubj_measure_Doubt dep_measure_V. dep_measure_2 dep_measure_0 dep_measure_0 nsubj_measure_Idiom number_1,040_1 num_1,040_39 number_39_294160546 num_A_1,040 dep_Total_A num_0_0 number_0_4 number_0_3 dep_0_1 number_1_1 number_0_0
J98-2001	J96-2004	o	And indeed the agreement figures went up from K = 0.63 to K = 0.68 -LRB- ignoring doubts -RRB- when we did so i.e. within the tentative margins of agreement according to Carletta -LRB- 1996 -RRB- -LRB- 0.68 < _ x < 0.8 -RRB-	quantmod_0.8_< dep_0.8_x dep_0.8_< dep_0.8_0.68 num_x__ dep_Carletta_0.8 appos_Carletta_1996 pobj_margins_Carletta prepc_according_to_margins_to prep_of_margins_agreement dep_tentative_margins amod_the_tentative pobj_within_the ccomp_,_within dep_did_i.e. advmod_did_so nsubj_did_we advmod_did_when advcl_ignoring_did dobj_ignoring_doubts dep_=_0.68 amod_K_= dep_=_0.63 amod_K_= xcomp_went_ignoring prep_to_went_K prep_from_went_K advmod_went_up nsubj_went_figures advmod_went_indeed cc_went_And nn_figures_agreement det_figures_the
N01-1010	J96-2004	o	11 This low agreement ratio is also re ected in a measure called the statistic -LRB- Carletta 1996 Bruce and Wiebe 1998 Ng et al. 1999 -RRB-	dep_al._1999 nn_al._et nn_al._Ng dep_Bruce_al. num_Bruce_1998 conj_and_Bruce_Wiebe dep_Carletta_Wiebe dep_Carletta_Bruce appos_Carletta_1996 dep_statistic_Carletta det_statistic_the dobj_called_statistic det_measure_a prep_in_ected_measure dep_re_called dep_re_ected advmod_re_also aux_re_is nsubj_re_ratio nn_ratio_agreement amod_ratio_low det_ratio_This num_ratio_11
N01-1010	J96-2004	o	Normally :8 is considered a good agreement -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_agreement_Carletta amod_agreement_good det_agreement_a dobj_considered_agreement auxpass_considered_is nsubjpass_considered_:8 advmod_considered_Normally
N01-1010	J96-2004	o	The results are quite promising our extraction method discovered 89 % of the WordNet cousins and the sense partitions in our lexicon yielded better values -LRB- Carletta 1996 -RRB- than arbitrary sense groupings on the agreement data	nn_data_agreement det_data_the prep_on_groupings_data nn_groupings_sense amod_groupings_arbitrary dep_Carletta_1996 prep_than_values_groupings appos_values_Carletta amod_values_better dobj_yielded_values nsubj_yielded_partitions nsubj_yielded_method poss_lexicon_our prep_in_partitions_lexicon nn_partitions_sense det_partitions_the nn_cousins_WordNet det_cousins_the prep_of_%_cousins num_%_89 dobj_discovered_% conj_and_method_partitions vmod_method_discovered nn_method_extraction poss_method_our parataxis_promising_yielded advmod_promising_quite cop_promising_are nsubj_promising_results det_results_The
N03-1012	J96-2004	o	The resulting Kappa statistics -LRB- Carletta 1996 -RRB- over the annotated data yields a0a2a1 a3a5a4a7a6 which seems to indicate that human annotators can reliably distinguish between coherent samples -LRB- as in Example -LRB- 1a -RRB- -RRB- and incoherent ones -LRB- as in Example -LRB- 1b -RRB- -RRB-	appos_Example_1b pobj_in_Example pcomp_as_in prep_ones_as amod_ones_incoherent appos_Example_1a pobj_in_Example pcomp_as_in conj_and_samples_ones prep_samples_as amod_samples_coherent prep_between_distinguish_ones prep_between_distinguish_samples advmod_distinguish_reliably aux_distinguish_can nsubj_distinguish_annotators mark_distinguish_that amod_annotators_human ccomp_indicate_distinguish aux_indicate_to xcomp_seems_indicate nsubj_seems_which rcmod_a3a5a4a7a6_seems dobj_a0a2a1_a3a5a4a7a6 nsubj_a0a2a1_statistics nn_yields_data amod_yields_annotated det_yields_the dep_Carletta_1996 prep_over_statistics_yields dep_statistics_Carletta nn_statistics_Kappa amod_statistics_resulting det_statistics_The
N04-1026	J96-2004	o	The two annotators agreed on the annotations of 385/453 turns achieving 84.99 % agreement with Kappa = 0.68.2 This inter-annotator agreement exceeds that of prior studies of emotion annotation in naturally occurring speech 2a3a5a4a7a6a8a6a9a4a11a10a13a12a15a14a17a16a19a18a21a20a22a12a23a14a25a24a26a18 a27 a20a22a12a23a14a25a24a26a18 -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_a20a22a12a23a14a25a24a26a18_Carletta nn_a20a22a12a23a14a25a24a26a18_a27 nn_a20a22a12a23a14a25a24a26a18_2a3a5a4a7a6a8a6a9a4a11a10a13a12a15a14a17a16a19a18a21a20a22a12a23a14a25a24a26a18 dep_speech_a20a22a12a23a14a25a24a26a18 dobj_occurring_speech advmod_occurring_naturally nn_annotation_emotion prep_of_studies_annotation amod_studies_prior prepc_in_that_occurring prep_of_that_studies dobj_exceeds_that amod_agreement_inter-annotator det_agreement_This num_agreement_0.68.2 dep_=_agreement amod_Kappa_= amod_agreement_% number_%_84.99 dobj_achieving_agreement num_turns_385/453 prep_of_annotations_turns det_annotations_the dep_agreed_exceeds prep_with_agreed_Kappa xcomp_agreed_achieving prep_on_agreed_annotations nsubj_agreed_annotators num_annotators_two det_annotators_The
N06-2040	J96-2004	o	The kappa -LRB- Carletta 1996 -RRB- obtained on this feature was 0.93	cop_0.93_was nsubj_0.93_Carletta det_feature_this prep_on_obtained_feature num_obtained_1996 vmod_Carletta_obtained nn_Carletta_kappa det_Carletta_The
N07-1072	J96-2004	o	The metric we used is the kappa statistic -LRB- Carletta 1996 -RRB- which factors out the agreement that is expected by chance -RRB- -LRB- 1 -RRB- -LRB- -RRB- -LRB- EP EPAP = where P -LRB- A -RRB- is the observed agreement among the raters and P -LRB- E -RRB- is the expected agreement i.e. the probability that the raters agree by chance	prep_by_agree_chance nsubj_agree_raters mark_agree_that det_raters_the ccomp_probability_agree det_probability_the appos_agreement_probability dep_agreement_i.e. amod_agreement_expected det_agreement_the cop_agreement_is nsubj_agreement_P nsubj_agreement_EPAP dep_agreement_statistic appos_P_E det_raters_the prep_among_agreement_raters amod_agreement_observed det_agreement_the cop_agreement_is nsubj_agreement_P advmod_agreement_where appos_P_A advcl_=_agreement conj_and_EPAP_P dep_EPAP_= nn_EPAP_EP dep_EPAP_1 agent_expected_chance auxpass_expected_is nsubjpass_expected_that rcmod_agreement_expected det_agreement_the prep_out_factors_agreement nsubj_factors_which dep_Carletta_1996 dep_statistic_factors dep_statistic_Carletta nn_statistic_kappa det_statistic_the cop_statistic_is nsubj_statistic_metric nsubj_used_we rcmod_metric_used det_metric_The
P00-1051	J96-2004	o	One of our goals was to use for our study only information that could be annotated reliably -LRB- Passonneau and Litman 1993 Carletta 1996 -RRB- as we believe this will make our results easier to replicate	aux_replicate_to ccomp_easier_replicate nsubj_easier_results poss_results_our xcomp_make_easier aux_make_will nsubj_make_this ccomp_believe_make nsubj_believe_we mark_believe_as dep_Carletta_1996 dep_Passonneau_Carletta conj_and_Passonneau_1993 conj_and_Passonneau_Litman advmod_annotated_reliably cop_annotated_be aux_annotated_could nsubj_annotated_that appos_information_1993 appos_information_Litman appos_information_Passonneau rcmod_information_annotated advmod_information_only dep_study_information poss_study_our advcl_use_believe prep_for_use_study aux_use_to xcomp_was_use nsubj_was_One poss_goals_our prep_of_One_goals ccomp_``_was
P00-1051	J96-2004	o	The agreement on identifying the boundaries of units using the AK statistic discussed in -LRB- Carletta 1996 -RRB- was AK BP BMBL -LRB- for two annotators and 500 units -RRB- the agreement on features -LRB- 2 annotators and at least 200 units -RRB- was follows Attribute AK Value utype .76 verbed .9 finite .81 subject .86 NPs Our instructions for identifying NP markables derive from those proposed in the MATE project scheme for annotating anaphoric relations -LRB- Poesio et al. 1999 -RRB-	amod_Poesio_1999 dep_Poesio_al. nn_Poesio_et amod_relations_anaphoric amod_relations_annotating prep_for_scheme_relations nn_scheme_project nn_scheme_MATE det_scheme_the prep_in_proposed_scheme dep_those_Poesio vmod_those_proposed prep_from_derive_those nsubj_derive_instructions nn_markables_NP dobj_identifying_markables prepc_for_instructions_identifying poss_instructions_Our rcmod_NPs_derive num_NPs_.86 quantmod_.86_subject number_.86_.81 dep_.86_finite number_.86_.9 dobj_verbed_NPs dep_.76_verbed dep_utype_.76 nn_utype_Value nn_utype_AK dobj_Attribute_utype dep_follows_Attribute auxpass_follows_was nsubjpass_follows_agreement num_units_200 quantmod_200_at mwe_at_least conj_and_annotators_units num_annotators_2 dep_features_units dep_features_annotators prep_on_agreement_features det_agreement_the num_units_500 conj_and_annotators_units num_annotators_two parataxis_BMBL_follows prep_for_BMBL_units prep_for_BMBL_annotators nn_BMBL_BP nn_BMBL_AK cop_BMBL_was nsubj_BMBL_agreement amod_Carletta_1996 dep_in_Carletta prep_discussed_in vmod_statistic_discussed nn_statistic_AK det_statistic_the dobj_using_statistic prep_of_boundaries_units det_boundaries_the dobj_identifying_boundaries vmod_agreement_using prepc_on_agreement_identifying det_agreement_The
P01-1032	J96-2004	o	On the one hand even the higher of the kappa coefficients mentioned above is significantly lower than the standard suggested for good reliability -LRB- a124a126a125a128a127a130a129 -RRB- or even the level where tentative conclusions may be drawn -LRB- a127a130a131a133a132a135a134a72a124 a134 a127a130a129 -RRB- -LRB- Carletta 1996 -RRB- -LRB- Krippendorff 1980 -RRB-	dep_Krippendorff_1980 amod_Carletta_1996 nn_a127a130a129_a134 nn_a127a130a129_a127a130a131a133a132a135a134a72a124 dep_drawn_a127a130a129 auxpass_drawn_be aux_drawn_may nsubjpass_drawn_conclusions advmod_drawn_where amod_conclusions_tentative appos_level_Carletta rcmod_level_drawn det_level_the advmod_level_even conj_or_reliability_level appos_reliability_a124a126a125a128a127a130a129 amod_reliability_good prep_for_suggested_level prep_for_suggested_reliability dep_standard_suggested det_standard_the dep_lower_Krippendorff prep_than_lower_standard advmod_lower_significantly cop_lower_is nsubj_lower_higher prep_on_lower_hand advmod_mentioned_above vmod_coefficients_mentioned nn_coefficients_kappa det_coefficients_the prep_of_higher_coefficients det_higher_the advmod_higher_even num_hand_one det_hand_the
P01-1038	J96-2004	o	This information can be annotated reliably -LRB- a1a3a2a5a4a7a6a9a8 a10a12a11a14a13a16a15 and a1a17a2a5a4a19a18a20a8 a10a12a11a14a13a16a21 -RRB- .4 4Following -LRB- Carletta 1996 -RRB- we use the a22 statistic to estimate reliability of annotation	prep_of_reliability_annotation dobj_estimate_reliability aux_estimate_to nn_statistic_a22 det_statistic_the vmod_use_estimate dobj_use_statistic nsubj_use_we amod_Carletta_1996 appos_4Following_Carletta num_4Following_.4 nn_a10a12a11a14a13a16a21_a1a17a2a5a4a19a18a20a8 rcmod_a10a12a11a14a13a16a15_use dep_a10a12a11a14a13a16a15_4Following conj_and_a10a12a11a14a13a16a15_a10a12a11a14a13a16a21 nn_a10a12a11a14a13a16a15_a1a3a2a5a4a7a6a9a8 dep_reliably_a10a12a11a14a13a16a21 dep_reliably_a10a12a11a14a13a16a15 prep_annotated_reliably cop_annotated_be aux_annotated_can nsubj_annotated_information det_information_This
P01-1051	J96-2004	o	Analyze resulting findings to determine a progression of competence In -LRB- Michaud et al. 2001 -RRB- we discuss the initial steps we took in this process including the development of a list of error codes documented by a coding manual the verification of our manual and coding scheme by testing inter-coder reliability in a subset of the corpus -LRB- where we achieved a Kappa agreement score -LRB- Carletta 1996 -RRB- of a0 a1a3a2a5a4a7a6 -RRB- 2 and the subsequent tagging of the entire corpus	amod_corpus_entire det_corpus_the prep_of_tagging_corpus amod_tagging_subsequent det_tagging_the num_a1a3a2a5a4a7a6_2 nn_a1a3a2a5a4a7a6_a0 dep_Carletta_1996 prep_of_score_a1a3a2a5a4a7a6 appos_score_Carletta nn_score_agreement nn_score_Kappa det_score_a conj_and_achieved_tagging dobj_achieved_score nsubj_achieved_we advmod_achieved_where dep_corpus_tagging dep_corpus_achieved det_corpus_the prep_of_subset_corpus det_subset_a nn_reliability_inter-coder nn_reliability_testing amod_scheme_coding poss_manual_our prep_in_verification_subset prep_by_verification_reliability conj_and_verification_scheme prep_of_verification_manual det_verification_the amod_manual_coding det_manual_a dobj_documented_scheme dobj_documented_verification prep_by_documented_manual dep_codes_documented prep_of_list_error det_list_a prep_of_development_list det_development_the dep_process_codes prep_including_process_development det_process_this dobj_took_process prt_took_in nsubj_took_we rcmod_steps_took amod_steps_initial det_steps_the dobj_discuss_steps nsubj_discuss_we mark_discuss_In nn_al._et amod_Michaud_2001 dep_Michaud_al. dep_In_Michaud prep_of_progression_competence det_progression_a advcl_determine_discuss dobj_determine_progression aux_determine_to vmod_findings_determine amod_findings_resulting dobj_Analyze_findings ccomp_``_Analyze
P02-1045	J96-2004	o	The reliability of the annotations was checked using the kappa statistic -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_statistic_Carletta nn_statistic_kappa det_statistic_the dobj_using_statistic xcomp_checked_using auxpass_checked_was nsubjpass_checked_reliability det_annotations_the prep_of_reliability_annotations det_reliability_The
P03-1008	J96-2004	o	The annotation can be considered reliable -LRB- Krippendorff 1980 -RRB- with 95 % agreement and a kappa -LRB- Carletta 1996 -RRB- of .88	dep_Carletta_1996 prep_of_kappa_.88 appos_kappa_Carletta det_kappa_a conj_and_agreement_kappa amod_agreement_% number_%_95 dep_Krippendorff_1980 prep_with_reliable_kappa prep_with_reliable_agreement dep_reliable_Krippendorff acomp_considered_reliable auxpass_considered_be aux_considered_can nsubjpass_considered_annotation det_annotation_The
P03-1048	J96-2004	o	Co-selection measures include precision and recall of co-selected sentences relative utility -LRB- Radev et al. 2000 -RRB- and Kappa -LRB- Siegel and Castellan 1988 Carletta 1996 -RRB-	num_Siegel_1996 conj_and_Siegel_Carletta conj_and_Siegel_1988 conj_and_Siegel_Castellan nn_Siegel_Kappa amod_Radev_2000 dep_Radev_al. nn_Radev_et dep_utility_Radev amod_utility_relative amod_sentences_co-selected conj_and_precision_Carletta conj_and_precision_1988 conj_and_precision_Castellan conj_and_precision_Siegel conj_and_precision_utility prep_of_precision_sentences conj_and_precision_recall dobj_include_Siegel dobj_include_utility dobj_include_recall dobj_include_precision nsubj_include_measures amod_measures_Co-selection
P03-1048	J96-2004	o	3.1.2 Kappa Kappa -LRB- Siegel and Castellan 1988 -RRB- is an evaluation measure which is increasingly used in NLP annotation work -LRB- Krippendorff 1980 Carletta 1996 -RRB-	amod_Carletta_1996 dep_Krippendorff_Carletta dep_Krippendorff_1980 appos_work_Krippendorff nn_work_annotation nn_work_NLP prep_in_used_work advmod_used_increasingly auxpass_used_is nsubjpass_used_which rcmod_measure_used nn_measure_evaluation det_measure_an cop_measure_is nsubj_measure_Kappa amod_Siegel_1988 conj_and_Siegel_Castellan appos_Kappa_Castellan appos_Kappa_Siegel nn_Kappa_Kappa num_Kappa_3.1.2
P04-1049	J96-2004	o	Inter-annotator agreement was determined for six pairs of two annotators each resulting in kappa values -LRB- Carletta -LRB- 1996 -RRB- -RRB- ranging from 0.62 to 0.82 for the whole database -LRB- Carlson et al.	dep_Carlson_al. nn_Carlson_et nn_database_whole det_database_the prep_to_ranging_0.82 prep_from_ranging_0.62 dep_Carletta_1996 vmod_values_ranging dep_values_Carletta nn_values_kappa dep_resulting_Carlson prep_for_resulting_database prep_in_resulting_values npadvmod_annotators_each num_annotators_two prep_of_pairs_annotators num_pairs_six xcomp_determined_resulting prep_for_determined_pairs auxpass_determined_was nsubjpass_determined_agreement amod_agreement_Inter-annotator
P04-1088	J96-2004	o	To support this claim first we used the coefficient -LRB- Krippendorff 1980 Carletta 1996 -RRB- to assess the agreement between the classification made by FLSA and the classification from the corpora see Table 8	num_Table_8 dobj_see_Table det_corpora_the det_classification_the conj_and_FLSA_classification prep_from_made_corpora agent_made_classification agent_made_FLSA vmod_classification_made det_classification_the prep_between_agreement_classification det_agreement_the ccomp_assess_see dobj_assess_agreement aux_assess_to dep_Carletta_1996 vmod_Krippendorff_assess dep_Krippendorff_Carletta appos_Krippendorff_1980 dep_coefficient_Krippendorff det_coefficient_the dobj_used_coefficient nsubj_used_we advmod_used_first dep_used_support det_claim_this dobj_support_claim aux_support_To
P05-1031	J96-2004	o	5To test the reliability of the annotation scheme we had a subset of the data annotated by two annotators and found a satisfactory agreement -LRB- Carletta 1996 -RRB- of = 0.81	dep_=_0.81 dep_Carletta_1996 prep_of_agreement_= dep_agreement_Carletta amod_agreement_satisfactory det_agreement_a dobj_found_agreement nsubj_found_we num_annotators_two prep_by_annotated_annotators amod_data_annotated det_data_the prep_of_subset_data det_subset_a conj_and_had_found dobj_had_subset nsubj_had_we dep_had_test nn_scheme_annotation det_scheme_the prep_of_reliability_scheme det_reliability_the dep_test_reliability nn_test_5To
P05-2010	J96-2004	o	We use the by now standard a0 statistic -LRB- Di Eugenio and Glass 2004 Carletta 1996 Marcu et al. 1999 Webber and Byron 2004 -RRB- to quantify the degree of above-chance agreement between multiple annotators and the a1 statistic for analysis of sources of unreliability -LRB- Krippendorff 1980 -RRB-	amod_Krippendorff_1980 dep_unreliability_Krippendorff prep_of_sources_unreliability prep_of_analysis_sources prep_for_statistic_analysis nn_statistic_a1 det_statistic_the amod_annotators_multiple prep_between_agreement_annotators amod_agreement_above-chance prep_of_degree_agreement det_degree_the conj_and_quantify_statistic dobj_quantify_degree aux_quantify_to num_Webber_2004 conj_and_Webber_Byron num_Marcu_1999 nn_Marcu_al. nn_Marcu_et vmod_Carletta_statistic vmod_Carletta_quantify dep_Carletta_Byron dep_Carletta_Webber conj_Carletta_Marcu num_Carletta_1996 dep_Eugenio_Carletta dep_Eugenio_2004 conj_and_Eugenio_Glass nn_Eugenio_Di dep_statistic_Glass dep_statistic_Eugenio nn_statistic_a0 amod_statistic_standard advmod_statistic_now prep_by_use_statistic dobj_use_the nsubj_use_We
P05-2014	J96-2004	o	Labelling was carried out by three computational linguistics graduate students with 89 % agreement resulting in a Kappa statistic of 0.87 which is a satisfactory indication that our corpus can be labelled with high reliability using our tag set -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_set_Carletta nn_set_tag poss_set_our dobj_using_set amod_reliability_high xcomp_labelled_using prep_with_labelled_reliability auxpass_labelled_be aux_labelled_can nsubjpass_labelled_corpus dobj_labelled_that poss_corpus_our rcmod_indication_labelled amod_indication_satisfactory det_indication_a cop_indication_is nsubj_indication_which rcmod_statistic_indication prep_of_statistic_0.87 nn_statistic_Kappa det_statistic_a prep_in_resulting_statistic vmod_agreement_resulting amod_agreement_% number_%_89 prep_with_students_agreement amod_students_graduate nn_students_linguistics amod_students_computational num_students_three agent_carried_students prt_carried_out auxpass_carried_was nsubjpass_carried_Labelling
P06-1050	J96-2004	o	The kappa statistic -LRB- Krippendorff 1980 Carletta 1996 -RRB- has become the de facto standard to assess inter-annotator agreement	amod_agreement_inter-annotator dobj_assess_agreement aux_assess_to vmod_standard_assess nn_standard_facto nn_standard_de det_standard_the xcomp_become_standard aux_become_has nsubj_become_statistic dep_Carletta_1996 dep_Krippendorff_Carletta appos_Krippendorff_1980 appos_statistic_Krippendorff nn_statistic_kappa det_statistic_The
P07-3006	J96-2004	o	Therefore the results are more informative than a simple agreement average -LRB- Cohen 1960 Carletta 1996 -RRB-	amod_Carletta_1996 dep_Cohen_Carletta amod_Cohen_1960 appos_average_Cohen nn_average_agreement amod_average_simple det_average_a prep_than_informative_average advmod_informative_more cop_informative_are nsubj_informative_results advmod_informative_Therefore det_results_the
P08-2064	J96-2004	o	Table 1 shows the percentage of agreement in classifying words as compounds or non-compounds -LRB- Compound Classification Agreement CCA -RRB- for each language and the Kappa score -LRB- Carletta 1996 -RRB- obtained from it and the percentage of words for which also the decomposition provided was identical -LRB- Decompounding Agreement DA -RRB-	appos_Agreement_DA amod_Agreement_Decompounding dep_identical_Agreement cop_identical_was nsubj_identical_decomposition advmod_identical_also vmod_decomposition_provided det_decomposition_the rcmod_words_identical prep_for_words_which prep_of_percentage_words det_percentage_the prep_from_obtained_it conj_and_Carletta_percentage vmod_Carletta_obtained dep_Carletta_1996 nn_score_Kappa det_score_the conj_and_language_score det_language_each nn_Agreement_Classification nn_Agreement_Compound nn_Agreement_non-compounds appos_compounds_CCA conj_or_compounds_Agreement prep_as_words_Agreement prep_as_words_compounds amod_words_classifying prep_of_percentage_agreement det_percentage_the dep_shows_percentage dep_shows_Carletta prep_for_shows_score prep_for_shows_language prep_in_shows_words dobj_shows_percentage nsubj_shows_Table num_Table_1
P09-1094	J96-2004	o	Kappa is defined as K = P -LRB- A -RRB- P -LRB- E -RRB- 1P -LRB- E -RRB- -LRB- Carletta 1996 -RRB- where P -LRB- A -RRB- is the proportion of times that the labels agree and P -LRB- E -RRB- is the proportion of times that they may agree by chance	prep_by_agree_chance aux_agree_may nsubj_agree_they dobj_agree_that rcmod_proportion_agree prep_of_proportion_times det_proportion_the cop_proportion_is appos_P_E nsubj_agree_labels mark_agree_that det_labels_the ccomp_proportion_agree prep_of_proportion_times det_proportion_the cop_proportion_is nsubj_proportion_P advmod_proportion_where appos_P_A amod_Carletta_1996 dep_1P_Carletta appos_1P_E nn_1P_P appos_P_E nn_P_P appos_P_A dep_=_1P conj_and_K_P rcmod_K_proportion amod_K_= dep_defined_proportion prep_as_defined_P prep_as_defined_K auxpass_defined_is nsubjpass_defined_Kappa
P09-1095	J96-2004	o	To measure interannotator agreement we compute Cohens Kappa -LRB- Carletta 1996 -RRB- from the two sets of annotations obtaining a Kappa value of only 0.43	advmod_0.43_only prep_of_value_0.43 nn_value_Kappa det_value_a dobj_obtaining_value prep_of_sets_annotations num_sets_two det_sets_the dep_Carletta_1996 appos_Kappa_Carletta nn_Kappa_Cohens xcomp_compute_obtaining prep_from_compute_sets dobj_compute_Kappa nsubj_compute_we advcl_compute_measure amod_agreement_interannotator dobj_measure_agreement aux_measure_To
P09-1101	J96-2004	o	-LRB- Carletta 1996 -RRB- is another method of comparing inter-annotator agreement 0 30 6090120150 1 2 3 4 5 6 7 8 9 10 11 > 11 120 25 10 32 3 4 3 1 2 0 17 2 Nu mb er of an not ators Number of dialogues completed Figure 2	num_Figure_2 dobj_completed_Figure vmod_dialogues_completed prep_of_Number_dialogues nn_Number_ators det_Number_an neg_ators_not prep_of_er_Number nn_er_mb nn_er_Nu num_er_2 num_er_0 dep_2_17 number_17_0 dep_17_2 number_2_1 number_2_3 dep_2_4 dep_2_25 number_4_3 dep_4_32 number_32_10 number_25_120 dep_25_11 dep_11_> dep_11_9 dep_>_11 number_11_10 number_9_8 number_9_7 dep_9_6 dep_9_4 number_6_5 number_4_3 dep_4_2 number_2_1 dep_2_6090120150 number_6090120150_30 dep_agreement_er amod_agreement_inter-annotator dobj_comparing_agreement prepc_of_method_comparing det_method_another cop_method_is nsubj_method_Carletta num_Carletta_1996
P09-2044	J96-2004	o	We then used Cohens Kappa -LRB- -RRB- to determine the level of agreement -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 prep_of_level_agreement det_level_the dep_determine_Carletta dobj_determine_level aux_determine_to dep_determine_Kappa nn_Kappa_Cohens ccomp_used_determine advmod_used_then nsubj_used_We ccomp_``_used
P97-1034	J96-2004	o	We then used the kappa statistic -LRB- Siegel and Castellan 1988 Carletta 1996 -RRB- to assess the level of agreement between the three coders with respect to the 2 An agent holds the task initiative during a turn as long as some utterance during the turn directly proposes how the agents should accomplish their goal as in utterance -LRB- 3c -RRB-	appos_utterance_3c pobj_in_utterance pcomp_as_in poss_goal_their dobj_accomplish_goal aux_accomplish_should nsubj_accomplish_agents advmod_accomplish_how det_agents_the ccomp_proposes_accomplish advmod_proposes_directly nsubj_proposes_utterance mark_proposes_as det_turn_the prep_during_utterance_turn det_utterance_some prep_long_as advcl_long_proposes advmod_long_as det_turn_a nn_initiative_task det_initiative_the prep_during_holds_turn dobj_holds_initiative nsubj_holds_1988 nsubj_holds_Castellan nsubj_holds_Siegel det_agent_An num_agent_2 det_agent_the prep_with_respect_to_coders_agent num_coders_three det_coders_the prep_between_agreement_coders prep_of_level_agreement det_level_the dobj_assess_level aux_assess_to dep_Carletta_1996 vmod_Siegel_assess dep_Siegel_Carletta conj_and_Siegel_1988 conj_and_Siegel_Castellan rcmod_statistic_holds nn_statistic_kappa det_statistic_the advmod_used_long dobj_used_statistic advmod_used_then nsubj_used_We ccomp_``_used
P97-1034	J96-2004	o	Carletta suggests that content analysis researchers consider K > .8 as good reliability with .67 < / ~ < .8 allowing tentative conclusions to be drawn -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_drawn_Carletta auxpass_drawn_be aux_drawn_to amod_conclusions_tentative xcomp_allowing_drawn dobj_allowing_conclusions vmod_.8_allowing quantmod_.8_< dep_~_< num_~_.67 amod_reliability_good num_>_.8 dep_K_> dobj_consider_.8 prep_with_consider_~ prep_as_consider_reliability dobj_consider_K nsubj_consider_researchers mark_consider_that nn_researchers_analysis nn_researchers_content ccomp_suggests_consider nsubj_suggests_Carletta
P98-1052	J96-2004	o	Table 1 reports values for the Kappa -LRB- K -RRB- coefficient of agreement -LRB- Carletta 1996 -RRB- for Forward and Backward Functions .6 The columns in the tables read as follows if utterance Ui has tag X do coders agree on the subtag ?	det_subtag_the prep_on_agree_subtag nsubj_agree_coders aux_agree_do advcl_agree_has nn_X_tag dobj_has_X nsubj_has_Ui mark_has_if nn_Ui_utterance parataxis_follows_agree mark_follows_as advcl_read_follows nsubj_read_values det_tables_the det_columns_The num_columns_.6 nn_columns_Functions advmod_columns_Backward prep_in_Forward_tables conj_and_Forward_columns pobj_for_columns pobj_for_Forward dep_Carletta_1996 prep_coefficient_for appos_coefficient_Carletta prep_of_coefficient_agreement nn_K_Kappa det_K_the dep_values_coefficient prep_for_values_K nn_values_reports num_values_1 nn_values_Table
P99-1032	J96-2004	o	We perform a statistical analysis that provides information that complements the information provided by Cohen 's Kappa -LRB- Cohen 1960 Carletta 1996 -RRB-	amod_Carletta_1996 dep_Cohen_Carletta amod_Cohen_1960 appos_Kappa_Cohen poss_Kappa_Cohen agent_provided_Kappa vmod_information_provided det_information_the dobj_complements_information nsubj_complements_that rcmod_information_complements dobj_provides_information nsubj_provides_that rcmod_analysis_provides amod_analysis_statistical det_analysis_a dobj_perform_analysis nsubj_perform_We
P99-1068	J96-2004	o	The table also shows Cohen 's to an agreement measure that corrects for chance agreement -LRB- Carletta 1996 -RRB- the most important t value in the table is the value of 0.7 for the two human judges which can be interpreted as sufficiently high to indicate that the task is reasonably well defined	advmod_defined_well auxpass_defined_is nsubjpass_defined_task mark_defined_that advmod_well_reasonably det_task_the ccomp_indicate_defined aux_indicate_to xcomp_high_indicate advmod_high_sufficiently prep_as_interpreted_high auxpass_interpreted_be aux_interpreted_can nsubjpass_interpreted_which rcmod_judges_interpreted amod_judges_human num_judges_two det_judges_the prep_for_value_judges prep_of_value_0.7 det_value_the cop_value_is nsubj_value_value det_table_the prep_in_value_table nn_value_t amod_value_important det_value_the advmod_important_most amod_Carletta_1996 dep_agreement_Carletta nn_agreement_chance prep_for_corrects_agreement nsubj_corrects_that parataxis_measure_value rcmod_measure_corrects nn_measure_agreement det_measure_an prep_to_Cohen_measure possessive_Cohen_'s dobj_shows_Cohen advmod_shows_also nsubj_shows_table det_table_The
W00-1302	J96-2004	o	We measured stability -LRB- the degree to which the same annotator will produce an annotation after 6 weeks -RRB- and reproducibility -LRB- the degree to which two unrelated annotators will produce the same annotation -RRB- using the Kappa coefficient K -LRB- Siegel and Castellan 1988 Carletta 1996 -RRB- which controls agreement P -LRB- A -RRB- for chance agreement P -LRB- E -RRB- K = PA -RRB- P -LRB- E -RRB- 1-P -LRB- Z -RRB- Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution and 1 for perfect agreement	amod_agreement_perfect prep_for_1_agreement amod_distribution_observed det_distribution_the prep_as_distribution_distribution amod_distribution_same det_distribution_the nn_annotation_chance prep_following_expected_distribution agent_expected_annotation auxpass_expected_be aux_expected_would advmod_expected_as auxpass_expected_is nsubjpass_expected_agreement mark_expected_if advmod_as_only cop_0_is nsubj_0_Kappa dep_0_1-P nn_Kappa_Z nn_1-P_P appos_P_E conj_and_PA_1 dep_PA_expected prep_PA_for num_PA_0 amod_PA_= npadvmod_=_K appos_P_E nn_P_agreement nn_P_chance appos_P_A nn_P_agreement prep_for_controls_P dobj_controls_P nsubj_controls_which dep_Carletta_1996 dep_Siegel_Carletta conj_and_Siegel_1988 conj_and_Siegel_Castellan rcmod_K_controls appos_K_1988 appos_K_Castellan appos_K_Siegel nn_K_coefficient nn_K_Kappa det_K_the dep_using_1 dep_using_PA dobj_using_K amod_annotation_same det_annotation_the dobj_produce_annotation aux_produce_will nsubj_produce_annotators prep_to_produce_which amod_annotators_unrelated num_annotators_two rcmod_degree_produce det_degree_the dep_reproducibility_degree num_weeks_6 det_annotation_an dobj_produce_annotation aux_produce_will nsubj_produce_annotator prep_to_produce_which amod_annotator_same det_annotator_the prep_after_degree_weeks rcmod_degree_produce det_degree_the conj_and_stability_reproducibility dep_stability_degree dep_measured_using dobj_measured_reproducibility dobj_measured_stability nsubj_measured_We
W00-1403	J96-2004	o	The agreement was statistically significant -LRB- Kappa = 0.65.0 > 0.01 for Japanese and Kappa = 0.748,0 > 0.01 for English -LRB- Carletta 1996 Siegel-and Castellan 1988 -RRB- -RRB-	dep_Castellan_1988 nn_Castellan_Siegel-and dep_Carletta_Castellan dep_Carletta_1996 appos_English_Carletta prep_for_0.01_English quantmod_0.01_> num_0.01_0.748,0 dobj_=_0.01 nsubj_=_Kappa nsubj_=_Japanese conj_and_Japanese_Kappa dep_for_= quantmod_0.01_> num_0.01_0.65.0 prep_=_for dobj_=_0.01 nsubj_=_Kappa dep_significant_= advmod_significant_statistically cop_significant_was nsubj_significant_agreement det_agreement_The
W00-1415	J96-2004	o	In other words -LRB- 4b -RRB- can be used in substitution of -LRB- 4a -RRB- whereas -LRB- 5b -RRB- can not so easily 41n -LRB- Carletta 1996 -RRB- a value of K between .8 and I indicates good agreement a value between .6 and .8 indicates some agreement	det_agreement_some dobj_indicates_agreement nsubj_indicates_value conj_and_.6_.8 prep_between_value_.8 prep_between_value_.6 det_value_a amod_agreement_good dobj_indicates_agreement nsubj_indicates_I conj_and_value_indicates conj_and_value_indicates prep_between_value_.8 prep_of_value_K det_value_a amod_Carletta_1996 appos_41n_Carletta dep_easily_indicates dep_easily_indicates dep_easily_value dep_easily_41n advmod_easily_so ccomp_,_easily dep_can_not dep_5b_can dep_whereas_5b dep_of_4a prep_substitution_of dep_used_whereas prep_in_used_substitution auxpass_used_be aux_used_can nsubjpass_used_4b prep_in_used_words amod_words_other
W02-0207	J96-2004	o	5 Reliability of Annotations 5.1 The Kappa Statistic To measure the reliability of annotations we used the Kappa statistic -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_statistic_Carletta nn_statistic_Kappa det_statistic_the dobj_used_statistic nsubj_used_we prep_of_reliability_annotations det_reliability_the ccomp_measure_used dobj_measure_reliability aux_measure_To vmod_Statistic_measure nn_Statistic_Kappa det_Statistic_The num_Statistic_5.1 dep_Annotations_Statistic prep_of_Reliability_Annotations num_Reliability_5 dep_``_Reliability
W02-0226	J96-2004	o	To test the reliability of group segmentation within GDM-IS we calculate the kappa coefficient -LRB- C3 -RRB- 8 -LRB- Carletta 1996 Carletta et al. 1997 Flammia 1998 -RRB- to measure pairwise agreement between the subject and the expert	det_expert_the conj_and_subject_expert det_subject_the prep_between_agreement_expert prep_between_agreement_subject amod_agreement_pairwise dobj_measure_agreement aux_measure_to amod_Flammia_1998 vmod_Carletta_measure dep_Carletta_Flammia num_Carletta_1997 nn_Carletta_al. nn_Carletta_et dep_Carletta_Carletta appos_Carletta_1996 dep_coefficient_Carletta num_coefficient_8 appos_coefficient_C3 nn_coefficient_kappa det_coefficient_the dobj_calculate_coefficient nsubj_calculate_we advcl_calculate_test nn_segmentation_group prep_of_reliability_segmentation det_reliability_the prep_within_test_GDM-IS dobj_test_reliability aux_test_To
W02-0226	J96-2004	o	From -LRB- Carletta 1996 -RRB- 9 Combined metric BY BP B4AC BE B7BDB5C8CABPB4AC BE C8 B7 CAB5 from -LRB- Jurafsky and Martin 2000 p. 578 -RRB- AC BPBD	nn_BPBD_AC num_p._578 appos_Jurafsky_p. conj_and_Jurafsky_2000 conj_and_Jurafsky_Martin prep_from_CAB5_BPBD dep_CAB5_2000 dep_CAB5_Martin dep_CAB5_Jurafsky nn_CAB5_B7 nn_CAB5_C8 cop_CAB5_BE nsubj_CAB5_B7BDB5C8CABPB4AC aux_CAB5_BE prep_from_CAB5_B4AC nn_B4AC_BP nn_B4AC_BY amod_B4AC_metric amod_B4AC_Combined num_B4AC_9 dep_B4AC_Carletta dep_Carletta_1996
W02-0806	J96-2004	o	A detailed discussion on the use of kappa in natural language processing is presented in -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_in_Carletta prep_presented_in auxpass_presented_is nsubjpass_presented_discussion nn_processing_language amod_processing_natural prep_in_use_processing prep_of_use_kappa det_use_the prep_on_discussion_use amod_discussion_detailed det_discussion_A
W02-0808	J96-2004	o	We chose nouns that occur a minimum of 10 times in the corpus have no undetermined translations and at least five different translations in the six nonEnglish languages and have the log likelihood score of at least 18 that is LL -LRB- T T T S -RRB- = = 2 1 ij n * j * j * i ij n log 18 where n ij stands for the number of times T T and T S have been seen together in aligned sentences n i * and n * j stand for the number occurrences of T T and T S respectively and n ** represents the total 4 We computed raw percentages only common measures of annotator agreement such as the Kappa statistic -LRB- Carletta 1996 -RRB- proved to be inappropriate for our two-category -LRB- yesno -RRB- classification scheme	nn_scheme_classification amod_scheme_two-category poss_scheme_our dep_two-category_yesno prep_for_inappropriate_scheme cop_inappropriate_be aux_inappropriate_to xcomp_proved_inappropriate nsubj_proved_measures amod_Carletta_1996 dep_statistic_Carletta nn_statistic_Kappa det_statistic_the prep_such_as_agreement_statistic nn_agreement_annotator prep_of_measures_agreement amod_measures_common amod_percentages_raw advmod_computed_only dobj_computed_percentages nsubj_computed_We rcmod_4_computed amod_4_total det_4_the dobj_represents_4 nsubj_represents_n nsubj_represents_LL dep_represents_is nsubj_represents_that dep_n_** nn_S_T conj_and_T_S nn_T_T prep_of_occurrences_S prep_of_occurrences_T nn_occurrences_number det_occurrences_the prep_for_stand_occurrences dep_stand_j nsubj_stand_n dep_j_* conj_and_i_stand dep_i_* nn_i_n amod_sentences_aligned prep_in_seen_sentences advmod_seen_together auxpass_seen_been aux_seen_have nsubjpass_seen_log dep_seen_* dep_seen_j dep_seen_j dep_seen_n nn_S_T conj_and_T_S nn_T_T nn_T_times prep_of_number_S prep_of_number_T det_number_the prep_for_stands_number nsubj_stands_ij advmod_stands_where nn_ij_n rcmod_log_stands num_log_18 nn_log_n nn_log_ij nn_log_i dep_j_* dep_j_* nn_n_ij num_n_1 num_n_2 dep_=_seen dep_=_= nn_S_T dep_T_S nn_T_T conj_and_LL_n advmod_LL_respectively appos_LL_stand appos_LL_i amod_LL_= dep_LL_T pobj_at_18 mwe_at_least pcomp_of_at prep_score_of nn_score_likelihood nn_score_log det_score_the dobj_have_score nsubj_have_We amod_languages_nonEnglish num_languages_six det_languages_the prep_in_translations_languages amod_translations_different num_translations_five quantmod_five_at mwe_at_least conj_and_translations_translations amod_translations_undetermined neg_translations_no dobj_have_translations dobj_have_translations nsubj_have_We det_corpus_the num_times_10 prep_in_minimum_corpus prep_of_minimum_times det_minimum_a dobj_occur_minimum nsubj_occur_that rcmod_nouns_occur parataxis_chose_proved parataxis_chose_represents conj_and_chose_have conj_and_chose_have dobj_chose_nouns nsubj_chose_We
W02-0904	J96-2004	o	The kappa value -LRB- Carletta 1996 -RRB- was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was	nsubj_was_task dep_was_difficult nn_task_evaluation det_task_the advmod_difficult_how ccomp_estimate_was aux_estimate_to det_judges_the prep_among_agreement_judges det_agreement_the conj_and_evaluate_estimate dobj_evaluate_agreement aux_evaluate_to xcomp_used_estimate xcomp_used_evaluate auxpass_used_was nsubjpass_used_value amod_Carletta_1996 dep_value_Carletta nn_value_kappa det_value_The
W02-1040	J96-2004	o	The reliability of the annotations was checked using the kappa statistic -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_statistic_Carletta nn_statistic_kappa det_statistic_the dobj_using_statistic xcomp_checked_using auxpass_checked_was nsubjpass_checked_reliability det_annotations_the prep_of_reliability_annotations det_reliability_The
W03-0201	J96-2004	o	Though inter-rater reliability using the kappa statistic -LRB- Carletta 1996 -RRB- may be calculated for each group the distribution of categories in the contribution group was highly skewed and warrants further discussion	advmod_discussion_further nn_discussion_warrants conj_and_skewed_discussion advmod_skewed_highly cop_skewed_was nsubj_skewed_distribution nn_group_contribution det_group_the prep_in_distribution_group prep_of_distribution_categories det_distribution_the det_group_each parataxis_calculated_discussion parataxis_calculated_skewed prep_for_calculated_group auxpass_calculated_be aux_calculated_may nsubjpass_calculated_reliability num_Carletta_1996 appos_statistic_Carletta nn_statistic_kappa det_statistic_the dobj_using_statistic vmod_reliability_using nn_reliability_inter-rater nn_reliability_Though
W03-0902	J96-2004	o	Overall % agreement among judges for 250 propositions 60.1 A commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_statistic_Carletta nn_statistic_kappa det_statistic_the cop_statistic_is nsubj_statistic_agreement prep_of_categorization_data nn_reliability_interrater prep_in_evaluating_categorization dobj_evaluating_reliability prepc_for_metric_evaluating amod_metric_used det_metric_A num_metric_60.1 advmod_used_commonly amod_propositions_metric num_propositions_250 prep_for_agreement_propositions prep_among_agreement_judges nn_agreement_% amod_agreement_Overall
W03-1010	J96-2004	o	The statistic -LRB- Carletta 1996 -RRB- is recast as -LRB- fs w -RRB- -LRB- sys sys -RRB- = agr -LRB- fs w -RRB- -LRB- sys sys -RRB- P agr -LRB- fs -RRB- -LRB- sys sys -RRB- N P agr -LRB- fs -RRB- -LRB- sys sys -RRB- N In this modified form -LRB- fs w -RRB- represents the divergence in relative agreement wrt f s for target noun w relative to the mean relative agreement wrt f s over all words	det_words_all prep_over_s_words nsubj_s_f rcmod_wrt_s nn_wrt_agreement amod_wrt_relative dep_mean_wrt det_mean_the prep_to_relative_mean nn_w_noun nn_w_target prep_for_s_w nn_s_f nn_s_wrt nn_s_agreement amod_s_relative prep_in_divergence_s det_divergence_the advmod_represents_relative dobj_represents_divergence nsubj_represents_sys appos_fs_w amod_form_modified det_form_this prep_in_N_form nn_N_agr appos_sys_sys dep_agr_sys appos_agr_fs nn_agr_P nn_agr_N nn_agr_agr appos_sys_sys dep_agr_sys appos_agr_fs nn_agr_P nn_agr_sys appos_sys_fs appos_sys_N nn_sys_agr amod_sys_= nn_sys_fs appos_fs_w dep_agr_fs appos_sys_sys dep_fs_sys appos_fs_w dep_as_represents prep_recast_as auxpass_recast_is nsubjpass_recast_statistic amod_Carletta_1996 dep_statistic_Carletta det_statistic_The dep_``_recast
W03-1903	J96-2004	o	In fact it has been shown that the agreement of subjects annotating bridging -LRB- Poesio and Vieira 1998 -RRB- or discourse -LRB- Cimiano 2003 -RRB- relations can be too low for tentative conclusion to be drawn -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_drawn_Carletta auxpass_drawn_be aux_drawn_to vmod_conclusion_drawn amod_conclusion_tentative prep_for_low_conclusion advmod_low_too cop_low_be aux_low_can nsubj_low_agreement mark_low_that dep_relations_Cimiano nn_relations_discourse dep_Cimiano_2003 dep_Poesio_1998 conj_and_Poesio_Vieira conj_or_bridging_relations dep_bridging_Vieira dep_bridging_Poesio xcomp_annotating_relations xcomp_annotating_bridging vmod_subjects_annotating prep_of_agreement_subjects det_agreement_the ccomp_shown_low auxpass_shown_been aux_shown_has nsubjpass_shown_it prep_in_shown_fact
W03-1903	J96-2004	o	In this sense instead of measuring only the categorial agreement between annotators with the kappa statistic -LRB- Carletta 1996 -RRB- or the performance of a system in terms of precision/recall we could take into account the hierarchical organization of the categories or concepts by making use of measures considering the hierarchical distance between two concepts such as proposed by -LRB- Hahn and Schnattinger 1998 -RRB- or -LRB- Madche et al. 2002 -RRB-	amod_Madche_2002 dep_Madche_al. nn_Madche_et conj_or_Hahn_Madche dep_Hahn_1998 conj_and_Hahn_Schnattinger prep_by_proposed_Madche prep_by_proposed_Schnattinger prep_by_proposed_Hahn prepc_such_as_concepts_proposed num_concepts_two prep_between_distance_concepts amod_distance_hierarchical det_distance_the dobj_considering_distance vmod_measures_considering prep_of_use_measures dobj_making_use conj_or_categories_concepts det_categories_the prep_of_organization_concepts prep_of_organization_categories amod_organization_hierarchical det_organization_the prepc_by_take_making dobj_take_organization prep_into_take_account aux_take_could nsubj_take_we prepc_instead_of_take_measuring prep_in_take_sense prep_of_terms_precision/recall det_system_a prep_of_performance_system det_performance_the dep_Carletta_1996 conj_or_statistic_performance dep_statistic_Carletta nn_statistic_kappa det_statistic_the prep_with_annotators_performance prep_with_annotators_statistic prep_between_agreement_annotators amod_agreement_categorial det_agreement_the advmod_agreement_only prep_in_measuring_terms dobj_measuring_agreement det_sense_this
W03-2802	J96-2004	o	With the help of the kappa coefficient -LRB- Carletta 1996 -RRB- proposes to represent the dialog success independently from the task intrinsic complexity thus opening the way to task generic comparative evaluation	amod_evaluation_comparative amod_evaluation_generic nn_evaluation_task det_way_the prep_to_opening_evaluation dobj_opening_way advmod_opening_thus amod_complexity_intrinsic dep_task_complexity det_task_the prep_from_success_task advmod_success_independently nn_success_dialog det_success_the dobj_represent_success aux_represent_to advcl_proposes_opening xcomp_proposes_represent prep_with_proposes_help dep_Carletta_1996 appos_coefficient_Carletta nn_coefficient_kappa det_coefficient_the prep_of_help_coefficient det_help_the
W04-0204	J96-2004	o	The intercoder reliability is a constant concern of everyone working with corpora to test linguistic hypotheses -LRB- Carletta 1996 -RRB- and the more so when one is coding for semanto-pragmatic interpretations as in the case of the analysis of connectives	prep_of_analysis_connectives det_analysis_the prep_of_case_analysis det_case_the pobj_in_case pcomp_as_in amod_interpretations_semanto-pragmatic prep_coding_as prep_for_coding_interpretations aux_coding_is nsubj_coding_one advmod_coding_when advmod_when_so advmod_so_more rcmod_the_coding dep_Carletta_1996 appos_hypotheses_Carletta amod_hypotheses_linguistic dobj_test_hypotheses aux_test_to xcomp_working_test prep_with_working_corpora vmod_everyone_working conj_and_concern_the prep_of_concern_everyone amod_concern_constant det_concern_a cop_concern_is nsubj_concern_reliability nn_reliability_intercoder det_reliability_The
W04-0210	J96-2004	o	The agreement on identifying the boundaries of units using the statistic discussed in -LRB- Carletta 1996 -RRB- was = .9 -LRB- for two annotators and 500 units -RRB- the agreement on features -LRB- 2 annotators and at least 200 units -RRB- was as follows UTYPE = .76 VERBED = .9 FINITE = .81	dep_=_.81 dep_=_.9 dep_VERBED_= dep_=_.76 dep_UTYPE_= dep_UTYPE_FINITE dep_UTYPE_VERBED dep_UTYPE_= mark_follows_as dep_was_UTYPE advcl_was_follows nsubj_was_agreement num_units_200 quantmod_200_at mwe_at_least conj_and_annotators_units num_annotators_2 dep_features_units dep_features_annotators prep_on_agreement_features det_agreement_the num_units_500 conj_and_annotators_units num_annotators_two prep_for_.9_units prep_for_.9_annotators parataxis_=_was dobj_=_.9 cop_=_was nsubj_=_agreement amod_Carletta_1996 dep_in_Carletta prep_discussed_in vmod_statistic_discussed det_statistic_the dobj_using_statistic prep_of_boundaries_units det_boundaries_the dobj_identifying_boundaries vmod_agreement_using prepc_on_agreement_identifying det_agreement_The
W04-0216	J96-2004	o	6 Coding reliability The reliability of the annotation was evaluated using the kappa statistic -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_statistic_Carletta nn_statistic_kappa det_statistic_the dobj_using_statistic xcomp_evaluated_using auxpass_evaluated_was nsubjpass_evaluated_reliability det_annotation_the prep_of_reliability_annotation det_reliability_The rcmod_reliability_evaluated nn_reliability_Coding num_reliability_6 dep_``_reliability
W04-0713	J96-2004	o	The reliability for the two annotation tasks -LRB- statistics -LRB- Carletta 1996 -RRB- -RRB- was of 0.94 and 0.90 respectively	advmod_0.94_respectively conj_and_0.94_0.90 prep_of_was_0.90 prep_of_was_0.94 nsubj_was_reliability dep_Carletta_1996 dep_statistics_Carletta dep_tasks_statistics nn_tasks_annotation num_tasks_two det_tasks_the prep_for_reliability_tasks det_reliability_The ccomp_``_was
W04-0807	J96-2004	o	In addition to raw inter-tagger agreement the kappa statistic which removes from the agreement rate the amount of agreement that is expected by chance -LRB- Carletta 1996 -RRB- was also determined	advmod_determined_also auxpass_determined_was nsubjpass_determined_statistic prep_in_addition_to_determined_agreement amod_Carletta_1996 dep_chance_Carletta agent_expected_chance auxpass_expected_is nsubjpass_expected_that rcmod_amount_expected prep_of_amount_agreement det_amount_the nn_rate_agreement det_rate_the dobj_removes_amount prep_from_removes_rate nsubj_removes_which rcmod_statistic_removes nn_statistic_kappa det_statistic_the nn_agreement_inter-tagger amod_agreement_raw
W04-1211	J96-2004	o	Kappa coefficient is given in -LRB- 1 -RRB- -LRB- Carletta 1996 -RRB- -LRB- 1 -RRB- -RRB- -LRB- 1 -RRB- -LRB- -RRB- -LRB- EP EPAP Kappa = where P -LRB- A -RRB- is the proportion of times the annotators actually agree and P -LRB- E -RRB- is the proportion of times the annotators are expected to agree due to chance 3	num_chance_3 prep_to_due_chance acomp_agree_due aux_agree_to xcomp_expected_agree auxpass_expected_are nsubjpass_expected_annotators det_annotators_the rcmod_proportion_expected prep_of_proportion_times det_proportion_the cop_proportion_is nsubj_proportion_P appos_P_E conj_and_agree_proportion advmod_agree_actually nsubj_agree_annotators dep_agree_proportion det_annotators_the prep_of_proportion_times det_proportion_the cop_proportion_is nsubj_proportion_P advmod_proportion_where appos_P_A dep_=_proportion dep_=_agree nsubj_=_Kappa dep_=_1 dep_=_in nn_Kappa_EPAP nn_Kappa_EP nn_Kappa_Carletta appos_Carletta_1 appos_Carletta_1 dep_Carletta_1996 dep_given_= auxpass_given_is nsubjpass_given_coefficient nn_coefficient_Kappa
W04-1211	J96-2004	o	An acceptable agreement for most NLP classification tasks lies between 0.7 and 0.8 -LRB- Carletta 1996 Poessio and Vieira 1988 -RRB-	num_Vieira_1988 conj_and_Carletta_Vieira conj_and_Carletta_Poessio num_Carletta_1996 appos_0.7_Vieira appos_0.7_Poessio appos_0.7_Carletta conj_and_0.7_0.8 prep_between_lies_0.8 prep_between_lies_0.7 nsubj_lies_agreement nn_tasks_classification nn_tasks_NLP amod_tasks_most prep_for_agreement_tasks amod_agreement_acceptable det_agreement_An
W04-2312	J96-2004	n	The class-based kappa statistic of -LRB- Cohen 1960 Carletta 1996 -RRB- can not be applied here as the classes vary depending on the number of ambiguities per entry in the lexicon	det_lexicon_the prep_in_entry_lexicon prep_per_ambiguities_entry prep_of_number_ambiguities det_number_the pobj_vary_number prepc_depending_on_vary_on nsubj_vary_classes mark_vary_as det_classes_the advcl_applied_vary advmod_applied_here auxpass_applied_be neg_applied_not aux_applied_can nsubjpass_applied_Cohen mark_applied_of dep_Carletta_1996 dep_Cohen_Carletta amod_Cohen_1960 rcmod_statistic_applied nn_statistic_kappa amod_statistic_class-based det_statistic_The dep_``_statistic
W04-2323	J96-2004	o	The a0 coefficient is computed as follows a0 a47 a1a32a2 a9 a1 a30 a68 a9 a1a32a30 Carletta -LRB- 1996 -RRB- reports that content analysis researchers generally think of a0a34a33 a49a36a35a37 as good reliability with a49a36a35a38a40a39a37a41 a0 a41a25a49a36a35a37 allowing tentative conclusions to be drawn All that remains is to define the chance agreement probability a1 a30 Let a1a32a41 a1 a30 a7 and a1a32a42 a1 a30 a7 be the fraction of utterances that begin or end one or more segments in segmentation a30 respectively	nn_a30_segmentation prep_in_segments_a30 num_segments_more num_segments_one conj_or_one_more nsubj_end_that advmod_begin_respectively dobj_begin_segments conj_or_begin_end nsubj_begin_that rcmod_utterances_end rcmod_utterances_begin prep_of_fraction_utterances det_fraction_the cop_fraction_be csubj_fraction_Let nn_a7_a30 nn_a7_a1 nn_a7_a1a32a42 conj_and_a7_a7 nn_a7_a30 nn_a7_a1 nn_a7_a1a32a41 dobj_Let_a7 dobj_Let_a7 nn_a30_a1 nn_a30_probability nn_a30_agreement nn_a30_chance det_a30_the dobj_define_a30 aux_define_to xcomp_is_define nsubj_is_All nsubj_remains_that rcmod_All_remains auxpass_drawn_be aux_drawn_to amod_conclusions_tentative parataxis_allowing_fraction parataxis_allowing_is xcomp_allowing_drawn dobj_allowing_conclusions nn_a41a25a49a36a35a37_a0 nn_a41a25a49a36a35a37_a49a36a35a38a40a39a37a41 amod_reliability_good nn_a49a36a35a37_a0a34a33 dep_think_allowing prep_with_think_a41a25a49a36a35a37 prep_as_think_reliability prep_of_think_a49a36a35a37 advmod_think_generally nsubj_think_researchers mark_think_that nn_researchers_analysis nn_researchers_content ccomp_reports_think nsubj_reports_Carletta appos_Carletta_1996 nn_Carletta_a1a32a30 nn_Carletta_a9 num_Carletta_a68 nn_Carletta_a30 nn_Carletta_a1 nn_Carletta_a9 nn_Carletta_a1a32a2 nn_Carletta_a47 nn_Carletta_a0 mark_follows_as parataxis_computed_reports advcl_computed_follows auxpass_computed_is nsubjpass_computed_coefficient nn_coefficient_a0 det_coefficient_The dep_``_computed
W04-2326	J96-2004	o	The two annotators agreed on the annotations of 385/453 turns achieving 84.99 % agreement -LRB- Kappa = 0.68 -LRB- Carletta 1996 -RRB- -RRB-	dep_Carletta_1996 dep_=_Carletta dobj_=_0.68 nsubj_=_Kappa dep_agreement_= nn_agreement_% num_agreement_84.99 dobj_achieving_agreement num_turns_385/453 prep_of_annotations_turns det_annotations_the xcomp_agreed_achieving prep_on_agreed_annotations nsubj_agreed_annotators num_annotators_two det_annotators_The
W04-2703	J96-2004	o	4 Data analysis To test the reliability of the annotation we first considered the kappa statistic -LRB- Siegel and Castellan 1988 -RRB- which is used extensively in empirical studies of discourse -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 prep_of_studies_discourse amod_studies_empirical dep_used_Carletta prep_in_used_studies advmod_used_extensively auxpass_used_is nsubjpass_used_which dep_Siegel_1988 conj_and_Siegel_Castellan rcmod_statistic_used appos_statistic_Castellan appos_statistic_Siegel nn_statistic_kappa det_statistic_the xcomp_considered_statistic advmod_considered_first nsubj_considered_we nsubj_considered_analysis det_annotation_the prep_of_reliability_annotation det_reliability_the dobj_test_reliability aux_test_To vmod_analysis_test nn_analysis_Data num_analysis_4
W04-2802	J96-2004	o	Much like kappa statistics proposed by Carletta -LRB- 1996 -RRB- existing employments of majority class baselines assume an equal set of identical potential mark-ups i.e. attributes and their values for all markables	det_markables_all poss_values_their prep_for_attributes_markables conj_and_attributes_values nn_attributes_i.e. amod_mark-ups_potential amod_mark-ups_identical appos_set_values appos_set_attributes prep_of_set_mark-ups amod_set_equal det_set_an dobj_assume_set nsubj_assume_employments advmod_assume_Much nn_baselines_class nn_baselines_majority prep_of_employments_baselines amod_employments_existing appos_Carletta_1996 agent_proposed_Carletta vmod_statistics_proposed nn_statistics_kappa prep_like_Much_statistics
W05-0307	J96-2004	o	We evaluated annotation reliability by using the Kappa statistic -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_statistic_Carletta nn_statistic_Kappa det_statistic_the dobj_using_statistic nn_reliability_annotation prepc_by_evaluated_using dobj_evaluated_reliability nsubj_evaluated_We ccomp_``_evaluated
W05-0901	J96-2004	o	In the SUMMAC experiments the Kappa score -LRB- Carletta 1996 Eugenio and Glass 2004 -RRB- for interannotator agreement was reported to be 0.38 -LRB- Mani et al. 2002 -RRB-	amod_Mani_2002 dep_Mani_al. nn_Mani_et dep_0.38_Mani cop_0.38_be aux_0.38_to xcomp_reported_0.38 auxpass_reported_was nsubjpass_reported_score prep_in_reported_experiments nn_agreement_interannotator amod_Eugenio_2004 conj_and_Eugenio_Glass dep_Carletta_Glass dep_Carletta_Eugenio appos_Carletta_1996 prep_for_score_agreement appos_score_Carletta nn_score_Kappa det_score_the nn_experiments_SUMMAC det_experiments_the
W05-0906	J96-2004	o	Computational linguistics research generally attaches great value to high kappa measures -LRB- Carletta 1996 -RRB- which indicate high human agreement on a particular task	amod_task_particular det_task_a prep_on_agreement_task amod_agreement_human amod_agreement_high dobj_indicate_agreement nsubj_indicate_which dep_Carletta_1996 rcmod_measures_indicate appos_measures_Carletta nn_measures_kappa amod_measures_high amod_value_great prep_to_attaches_measures dobj_attaches_value advmod_attaches_generally nsubj_attaches_research nn_research_linguistics amod_research_Computational ccomp_``_attaches
W05-1009	J96-2004	o	The judges had an acceptable 0.74 mean agreement -LRB- Carletta 1996 -RRB- for the assignment of the primary class but a meaningless 0.21 for the secondary class -LRB- they did not even agree on which lemmata were polysemous -RRB-	cop_polysemous_were nsubj_polysemous_lemmata prep_on_polysemous_which ccomp_agree_polysemous advmod_agree_even neg_agree_not aux_agree_did nsubj_agree_they amod_class_secondary det_class_the dep_0.21_agree prep_for_0.21_class amod_0.21_meaningless det_0.21_a amod_class_primary det_class_the prep_of_assignment_class det_assignment_the amod_Carletta_1996 dep_agreement_Carletta amod_agreement_mean num_agreement_0.74 amod_agreement_acceptable det_agreement_an conj_but_had_0.21 prep_for_had_assignment dobj_had_agreement nsubj_had_judges det_judges_The
W05-1612	J96-2004	o	The table also shows the score which is another commonly used measure for inter-annotator agreement -LSB- Carletta 1996 -RSB-	amod_Carletta_1996 amod_agreement_inter-annotator prep_for_measure_agreement amod_measure_used det_measure_another cop_measure_is nsubj_measure_which advmod_used_commonly dep_score_Carletta rcmod_score_measure det_score_the dobj_shows_score advmod_shows_also nsubj_shows_table det_table_The ccomp_``_shows
W06-0906	J96-2004	o	In the literature on the kappa statistic most authors address only category data some can handle more general data such as data in interval scales or ratio scales -LRB- Krippendorff 1980 Carletta 1996 -RRB-	amod_Carletta_1996 dep_Krippendorff_Carletta appos_Krippendorff_1980 nn_scales_ratio conj_or_scales_scales amod_scales_interval dep_data_Krippendorff prep_in_data_scales prep_in_data_scales prep_such_as_data_data amod_data_general advmod_data_more dobj_handle_data aux_handle_can nsubj_handle_some nn_data_category advmod_data_only parataxis_address_handle dobj_address_data nsubj_address_authors prep_in_address_literature amod_authors_most nn_statistic_kappa det_statistic_the prep_on_literature_statistic det_literature_the
W06-0906	J96-2004	o	The kappa statistic -LRB- Krippendorff 1980 Carletta 1996 -RRB- has become the de facto standard to assess inter-annotator agreement	amod_agreement_inter-annotator dobj_assess_agreement aux_assess_to vmod_standard_assess nn_standard_facto nn_standard_de det_standard_the xcomp_become_standard aux_become_has nsubj_become_statistic dep_Carletta_1996 dep_Krippendorff_Carletta appos_Krippendorff_1980 appos_statistic_Krippendorff nn_statistic_kappa det_statistic_The
W06-1203	J96-2004	o	4This was a straightforward task two annotators annotated independently with very high agreementkappa score of over 0.95 -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 quantmod_0.95_over dep_score_Carletta prep_of_score_0.95 amod_score_agreementkappa amod_score_high advmod_high_very advmod_annotated_independently prep_with_annotators_score amod_annotators_annotated num_annotators_two dep_task_annotators amod_task_straightforward det_task_a cop_task_was nsubj_task_4This
W06-1312	J96-2004	o	7Following Carletta -LRB- 1996 -RRB- we measure agreement in Kappa which follows the formula K = P -LRB- A -RRB- P -LRB- E -RRB- 1P -LRB- E -RRB- where P -LRB- A -RRB- is observed and P -LRB- E -RRB- expected agreement	dobj_expected_agreement nsubj_expected_P appos_P_E auxpass_observed_is nsubjpass_observed_P advmod_observed_where appos_P_A rcmod_1P_observed appos_1P_E nn_1P_P appos_P_E nn_P_P appos_P_A dep_=_1P amod_K_= nn_K_formula det_K_the conj_and_follows_expected dobj_follows_K nsubj_follows_which prep_in_agreement_Kappa dep_measure_expected dep_measure_follows dobj_measure_agreement nsubj_measure_we dep_Carletta_measure appos_Carletta_1996 nn_Carletta_7Following
W06-1314	J96-2004	o	Inter-annotator agreement is typically measured by the kappa statistic -LRB- Carletta 1996 -RRB- dekappa frequency 0.0 0.2 0.4 0.6 0.8 1.0 0 2 4 6 8 Figure 2 Distribution of -LRB- inter-annotator agreement -RRB- across the 54 ICSI meetings tagged by two annotators	num_annotators_two agent_tagged_annotators vmod_meetings_tagged nn_meetings_ICSI num_meetings_54 det_meetings_the prep_across_agreement_meetings amod_agreement_inter-annotator prep_of_Distribution_agreement num_Figure_2 num_Figure_8 dep_6_Figure dep_4_6 number_4_2 dep_4_0 dep_4_0.8 number_0_1.0 number_0.8_0.6 dep_0.8_0.4 number_0.4_0.2 dep_0.0_4 dep_frequency_Distribution dep_frequency_0.0 amod_frequency_dekappa amod_Carletta_1996 dep_statistic_Carletta nn_statistic_kappa det_statistic_the dep_measured_frequency agent_measured_statistic advmod_measured_typically auxpass_measured_is nsubjpass_measured_agreement amod_agreement_Inter-annotator ccomp_``_measured
W06-1318	J96-2004	o	Agreement is sometimes measured as percentage of the cases on which the annotators agree but more often expected agreement is taken into account in using the kappa statistic -LRB- Cohen 1960 Carletta 1996 -RRB- which is given by = po pe1 p e -LRB- 1 -RRB- where po is the observed proportion of agreement and pe is the proportion of agreement expected by chance	agent_expected_chance vmod_agreement_expected prep_of_proportion_agreement det_proportion_the cop_proportion_is nsubj_proportion_proportion dep_proportion_1 nn_proportion_p conj_and_agreement_pe prep_of_proportion_pe prep_of_proportion_agreement amod_proportion_observed det_proportion_the cop_proportion_is nsubj_proportion_po advmod_proportion_where dep_1_e nn_p_pe1 nn_p_po dep_=_proportion dep_by_= prep_given_by auxpass_given_is nsubjpass_given_which dep_Carletta_1996 dep_Cohen_Carletta amod_Cohen_1960 rcmod_statistic_given appos_statistic_Cohen nn_statistic_kappa det_statistic_the dobj_using_statistic prepc_in_taken_using prep_into_taken_account auxpass_taken_is nsubjpass_taken_agreement amod_agreement_expected advmod_expected_often advmod_often_more nsubj_agree_annotators prep_on_agree_which det_annotators_the rcmod_cases_agree det_cases_the prep_of_percentage_cases conj_but_measured_taken prep_as_measured_percentage advmod_measured_sometimes auxpass_measured_is nsubjpass_measured_Agreement
W06-1318	J96-2004	n	Ever since its introduction in general -LRB- Cohen 1960 -RRB- and in computational linguistics -LRB- Carletta 1996 -RRB- many researchers have pointed out that there are quite some problems in using -LRB- e.g.	dep_using_e.g. det_problems_some advmod_problems_quite prepc_in_are_using nsubj_are_problems expl_are_there mark_are_that ccomp_pointed_are prt_pointed_out aux_pointed_have nsubj_pointed_researchers advmod_pointed_Ever amod_researchers_many amod_Carletta_1996 dep_linguistics_Carletta amod_linguistics_computational dep_Cohen_1960 conj_and_general_linguistics dep_general_Cohen prep_in_introduction_linguistics prep_in_introduction_general poss_introduction_its prep_since_Ever_introduction ccomp_``_pointed
W06-1318	J96-2004	o	Following the suggestions in -LRB- Carletta 1996 -RRB- Core et al. consider kappa scores above 0.67 to indicate significant agreement and scores above 0.8 reliable agreement	amod_agreement_reliable num_agreement_0.8 prep_above_scores_agreement conj_and_agreement_scores amod_agreement_significant dobj_indicate_scores dobj_indicate_agreement aux_indicate_to prep_above_scores_0.67 nn_scores_kappa xcomp_consider_indicate dobj_consider_scores advmod_consider_al. nsubj_consider_Core nn_al._et rcmod_Carletta_consider amod_Carletta_1996 prep_in_suggestions_Carletta det_suggestions_the pobj_Following_suggestions ccomp_``_Following
W06-1602	J96-2004	o	Inter-annotator agreement was assessed mainly using f-score and percentage agreement as well as 11 Table 1 Annotation examples of superlative adjectives example sup span det num car mod comp set The third-largest thrift institution in Puerto Rico also -LSB- -RSB- 22 def sg no ord 37 The Agriculture Department reported that feedlots in the 13 biggest ranch states held -LSB- -RSB- 910 def pl yes no 1112 The failed takeover would have given UAL employees 75 % voting control of the nation s second-largest airline -LSB- -RSB- 1717 pos sg no ord 1418 the kappa statistics -LRB- K -RRB- where applicable -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_applicable_Carletta advmod_applicable_where appos_statistics_K nn_statistics_kappa det_statistics_the rcmod_ord_applicable dep_ord_statistics num_ord_1418 neg_ord_no dobj_sg_ord vmod_pos_sg num_pos_1717 appos_airline_pos amod_airline_second-largest amod_airline_s det_nation_the dep_control_airline prep_of_control_nation nn_control_voting amod_control_% number_%_75 nn_employees_UAL dobj_given_control iobj_given_employees aux_given_have aux_given_would nsubj_given_takeover amod_takeover_failed det_takeover_The num_takeover_1112 quantmod_1112_no quantmod_1112_yes rcmod_pl_given nn_pl_def num_pl_910 dep_held_pl nsubj_held_feedlots mark_held_that nn_states_ranch amod_states_biggest num_states_13 det_states_the prep_in_feedlots_states ccomp_reported_held dep_reported_sg nsubj_reported_institution nn_Department_Agriculture det_Department_The npadvmod_ord_Department num_ord_37 neg_ord_no dep_sg_ord nn_sg_def num_sg_22 nn_Rico_Puerto advmod_institution_also prep_in_institution_Rico nn_institution_thrift amod_institution_third-largest det_institution_The rcmod_set_reported amod_set_comp nn_set_mod nn_set_car nn_set_num nn_set_det nn_set_span nn_set_sup nn_set_example nn_set_adjectives amod_set_superlative prep_of_examples_set nn_examples_Annotation dep_Table_examples num_Table_1 num_Table_11 nn_agreement_percentage nn_agreement_f-score conj_and_f-score_percentage conj_and_using_Table dobj_using_agreement advmod_using_mainly xcomp_assessed_Table xcomp_assessed_using auxpass_assessed_was nsubjpass_assessed_agreement amod_agreement_Inter-annotator
W06-1612	J96-2004	o	Inter-annotator agreement was measured using the kappa -LRB- K -RRB- statistics -LRB- Cohen 1960 Carletta 1996 -RRB- on 1,502 instances -LRB- three Switchboard dialogues -RRB- marked by two annotators who followed specific written guidelines	amod_guidelines_written amod_guidelines_specific dobj_followed_guidelines nsubj_followed_who rcmod_annotators_followed num_annotators_two agent_marked_annotators prep_on_marked_instances nn_dialogues_Switchboard num_dialogues_three appos_instances_dialogues num_instances_1,502 dep_Carletta_1996 vmod_Cohen_marked dep_Cohen_Carletta appos_Cohen_1960 dep_statistics_Cohen nn_statistics_K nn_statistics_kappa det_statistics_the dobj_using_statistics xcomp_measured_using auxpass_measured_was nsubjpass_measured_agreement amod_agreement_Inter-annotator
W06-1613	J96-2004	o	4Following Carletta -LRB- 1996 -RRB- we measure agreement in Kappa which follows the formula K = P -LRB- A -RRB- P -LRB- E -RRB- 1P -LRB- E -RRB- where P -LRB- A -RRB- is observed and P -LRB- E -RRB- expected agreement	dobj_expected_agreement nsubj_expected_P appos_P_E auxpass_observed_is nsubjpass_observed_P advmod_observed_where appos_P_A rcmod_1P_observed appos_1P_E nn_1P_P appos_P_E nn_P_P appos_P_A dep_=_1P amod_K_= nn_K_formula det_K_the conj_and_follows_expected dobj_follows_K nsubj_follows_which prep_in_agreement_Kappa dep_measure_expected dep_measure_follows dobj_measure_agreement nsubj_measure_we dep_Carletta_measure appos_Carletta_1996 nn_Carletta_4Following
W06-2404	J96-2004	o	7 For the most frequent 184 expressions on the average the agreement rate between two human annotators is 0.93 and the Kappa value is 0.73 which means allowing tentative conclusions to be drawn -LRB- Carletta 1996 Ng et al. 1999 -RRB-	num_Ng_1999 nn_Ng_al. nn_Ng_et dep_Carletta_Ng dep_Carletta_1996 dep_drawn_Carletta auxpass_drawn_be aux_drawn_to amod_conclusions_tentative xcomp_allowing_drawn dobj_allowing_conclusions xcomp_means_allowing nsubj_means_which rcmod_0.73_means cop_0.73_is nsubj_0.73_value nsubj_0.73_0.93 nn_value_Kappa det_value_the conj_and_0.93_value cop_0.93_is nsubj_0.93_rate dep_0.93_7 amod_annotators_human num_annotators_two prep_between_rate_annotators nn_rate_agreement det_rate_the det_average_the num_expressions_184 amod_expressions_frequent det_expressions_the advmod_frequent_most prep_on_7_average prep_for_7_expressions
W06-2505	J96-2004	o	5.1 Agreement between translators In an attempt to quantify the agreement between the two groups of translators we computed the Kappa coefficient for annotation tasks as defined by Carletta -LRB- 1996 -RRB-	appos_Carletta_1996 prep_by_defined_Carletta mark_defined_as nn_tasks_annotation prep_for_coefficient_tasks nn_coefficient_Kappa det_coefficient_the advcl_computed_defined dobj_computed_coefficient nsubj_computed_we advcl_computed_Agreement prep_of_groups_translators num_groups_two det_groups_the prep_between_agreement_groups det_agreement_the dobj_quantify_agreement aux_quantify_to vmod_attempt_quantify det_attempt_an prep_in_Agreement_attempt prep_between_Agreement_translators num_Agreement_5.1
W06-3328	J96-2004	o	Secondly we used the Kappa coefficient -LRB- Carletta 1996 -RRB- which has become the standard evaluation metric and the score obtained was 0.905	cop_0.905_was nsubj_0.905_score vmod_score_obtained det_score_the nn_metric_evaluation amod_metric_standard det_metric_the conj_and_become_0.905 xcomp_become_metric aux_become_has nsubj_become_which dep_Carletta_1996 rcmod_coefficient_0.905 rcmod_coefficient_become appos_coefficient_Carletta nn_coefficient_Kappa det_coefficient_the dobj_used_coefficient nsubj_used_we advmod_used_Secondly
W06-3406	J96-2004	o	The Kappa statistic -LRB- Carletta 1996 -RRB- is typically used to measure the human interrater agreement	nn_agreement_interrater amod_agreement_human det_agreement_the dobj_measure_agreement aux_measure_to xcomp_used_measure advmod_used_typically auxpass_used_is nsubjpass_used_statistic amod_Carletta_1996 dep_statistic_Carletta nn_statistic_Kappa det_statistic_The
W07-0718	J96-2004	p	6.1 Interand Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient -LRB- K -RRB- whichiswidelyused in computational linguistics for measuring agreement in category judgments -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_judgments_Carletta nn_judgments_category prep_in_agreement_judgments dobj_measuring_agreement amod_linguistics_computational prepc_for_whichiswidelyused_measuring prep_in_whichiswidelyused_linguistics vmod_usingthekappacoefficient_whichiswidelyused appos_usingthekappacoefficient_K amod_annotators_usingthekappacoefficient prep_among_agreement_annotators amod_agreement_pairwise dobj_measured_agreement nsubj_measured_We rcmod_agreement_measured amod_agreement_Intra-annotator nn_agreement_Interand num_agreement_6.1
W07-1602	J96-2004	o	For these classications we calculated a kappa statistic of 0.528 -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_0.528_Carletta prep_of_statistic_0.528 nn_statistic_kappa det_statistic_a dobj_calculated_statistic nsubj_calculated_we prep_for_calculated_classications det_classications_these
W07-1707	J96-2004	o	Obtained percent agreement of 0.988 and coefficient -LRB- Carletta 1996 -RRB- of 0.975 suggest high convergence of both annotations	preconj_annotations_both prep_of_convergence_annotations amod_convergence_high dobj_suggest_convergence nsubj_suggest_agreement dep_Carletta_1996 prep_of_0.988_0.975 dep_0.988_Carletta conj_and_0.988_coefficient prep_of_agreement_coefficient prep_of_agreement_0.988 nn_agreement_percent amod_agreement_Obtained
W07-2007	J96-2004	o	Annotation was highly reliable with a kappa -LRB- Carletta 1996 -RRB- of 3https / / www.cia.gov/cia/publications/ factbook/index html 4Given that the task is not about standard Named Entity Recognition we assume that the general semantic class of the name is already known	advmod_known_already auxpass_known_is nsubjpass_known_class mark_known_that det_name_the prep_of_class_name amod_class_semantic amod_class_general det_class_the ccomp_assume_known nsubj_assume_we nsubj_assume_task mark_assume_that nn_Recognition_Entity amod_Recognition_Named amod_Recognition_standard prep_about_is_Recognition neg_is_not vmod_task_is det_task_the ccomp_4Given_assume nn_4Given_html dep_4Given_factbook/index amod_factbook/index_www.cia.gov/cia/publications/ dep_Carletta_1996 prep_of_kappa_3https appos_kappa_Carletta det_kappa_a parataxis_reliable_4Given prep_with_reliable_kappa advmod_reliable_highly cop_reliable_was nsubj_reliable_Annotation
W08-0112	J96-2004	o	3 Analysis Results 3.1 Kappa Statistic Kappa coefficient -LRB- Carletta 1996 -RRB- is commonly used as a standard to reflect inter-annotator agreement	amod_agreement_inter-annotator dobj_reflect_agreement aux_reflect_to vmod_standard_reflect det_standard_a prep_as_used_standard advmod_used_commonly auxpass_used_is nsubjpass_used_Results dep_used_3 amod_Carletta_1996 dep_coefficient_Carletta nn_coefficient_Kappa nn_coefficient_Statistic nn_coefficient_Kappa num_coefficient_3.1 dep_Results_coefficient nn_Results_Analysis ccomp_``_used
W08-0309	J96-2004	o	7.1 Interand Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient -LRB- K -RRB- whichiswidelyused in computational linguistics for measuring agreement in category judgments -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_judgments_Carletta nn_judgments_category prep_in_agreement_judgments dobj_measuring_agreement amod_linguistics_computational prepc_for_whichiswidelyused_measuring prep_in_whichiswidelyused_linguistics vmod_usingthekappacoefficient_whichiswidelyused appos_usingthekappacoefficient_K amod_annotators_usingthekappacoefficient prep_among_agreement_annotators amod_agreement_pairwise dobj_measured_agreement nsubj_measured_We rcmod_agreement_measured amod_agreement_Intra-annotator nn_agreement_Interand num_agreement_7.1
W09-2109	J96-2004	o	The resulting intercoder reliability measured with the Kappa statistic -LRB- Carletta ,1996 -RRB- is considered excellent -LRB- = 0.80 -RRB-	dobj_=_0.80 dep_excellent_= xcomp_considered_excellent auxpass_considered_is nsubjpass_considered_reliability num_Carletta_,1996 appos_statistic_Carletta nn_statistic_Kappa det_statistic_the prep_with_measured_statistic vmod_reliability_measured nn_reliability_intercoder amod_reliability_resulting det_reliability_The ccomp_``_considered
W96-0402	J96-2004	o	The percentage agreement for each of the features is shown in the following table feature percent agreement form 100 % intentionality 74.9 % awareness 93.5 % safety 90.7 % As advocated by Carletta -LRB- 1996 -RRB- we have used the Kappa coefficient -LRB- Siegel and Castellan 1988 -RRB- as a measure of coder agreement	nn_agreement_coder prep_of_measure_agreement det_measure_a amod_Siegel_1988 conj_and_Siegel_Castellan dep_coefficient_Castellan dep_coefficient_Siegel nn_coefficient_Kappa det_coefficient_the prep_as_used_measure dobj_used_coefficient aux_used_have nsubj_used_we appos_Carletta_1996 prep_by_advocated_Carletta mark_advocated_As dep_%_advocated num_%_90.7 nn_%_safety nn_%_% num_%_93.5 nn_%_awareness nn_%_% num_%_74.9 nn_%_intentionality amod_%_% number_%_100 rcmod_form_used dep_form_% nn_form_agreement nn_form_percent nn_form_feature amod_table_following det_table_the dep_shown_form prep_in_shown_table auxpass_shown_is nsubjpass_shown_agreement det_features_the prep_of_each_features prep_for_agreement_each nn_agreement_percentage det_agreement_The
W97-0113	J96-2004	o	We will do this by examining how humans perform on summary extraction and evaluating the reliability of their performance using the kappa statistic a metric standardly used in the behavioral sciences -LRB- Jean Carletta 1996 Sidney Siegel and N. John Castellan Jr. 1988 -RRB-	amod_Jr._1988 nn_Jr._Castellan nn_Jr._John nn_Jr._N. conj_and_Siegel_Jr. nn_Siegel_Sidney dep_Carletta_Jr. dep_Carletta_Siegel amod_Carletta_1996 nn_Carletta_Jean amod_sciences_behavioral det_sciences_the prep_in_used_sciences dep_standardly_Carletta vmod_standardly_used amod_standardly_metric det_standardly_a appos_statistic_standardly nn_statistic_kappa det_statistic_the dobj_using_statistic poss_performance_their prep_of_reliability_performance det_reliability_the dobj_evaluating_reliability nsubj_evaluating_humans nn_extraction_summary conj_and_perform_evaluating prep_on_perform_extraction nsubj_perform_humans advmod_perform_how ccomp_examining_evaluating ccomp_examining_perform xcomp_do_using prepc_by_do_examining dobj_do_this aux_do_will nsubj_do_We
W97-0113	J96-2004	o	Measurement of B.eliability The Kappa Statistic Following Jean Carletta -LRB- 1996 -RRB- we use the kappa statistic -LRB- Sidney Siegel and N. John Castellan Jr. 1988 -RRB- to measure degree of agreement among subjects	prep_among_agreement_subjects prep_of_degree_agreement dobj_measure_degree aux_measure_to nn_Jr._Castellan nn_Jr._John nn_Jr._N. dep_Siegel_1988 conj_and_Siegel_Jr. nn_Siegel_Sidney appos_statistic_Jr. appos_statistic_Siegel nn_statistic_kappa det_statistic_the vmod_use_measure dobj_use_statistic nsubj_use_we dep_use_Measurement appos_Carletta_1996 nn_Carletta_Jean prep_following_Statistic_Carletta nn_Statistic_Kappa det_Statistic_The dep_B.eliability_Statistic prep_of_Measurement_B.eliability
W97-0113	J96-2004	p	As aptly pointed out in Jean Carletta -LRB- 1996 -RRB- agreement measures proposed so far in the computational linguistics literature has failed to ask an important question of whether results obtained using agreement data are in any way different from random data	amod_data_random prep_from_different_data amod_way_different det_way_any prep_in_are_way nsubj_are_results mark_are_whether nn_data_agreement dobj_using_data xcomp_obtained_using vmod_results_obtained prepc_of_question_are amod_question_important det_question_an dobj_ask_question aux_ask_to xcomp_failed_ask aux_failed_has nsubj_failed_measures nn_literature_linguistics amod_literature_computational det_literature_the prep_in_far_literature advmod_far_so advmod_proposed_far vmod_measures_proposed nn_measures_agreement rcmod_Carletta_failed appos_Carletta_1996 nn_Carletta_Jean prep_in_pointed_Carletta prt_pointed_out advmod_pointed_aptly mark_pointed_As advcl_``_pointed
W97-0203	J96-2004	o	Its roots are the same as computational linguistics -LRB- CL -RRB- but it has been largely ignored in CL until recently -LRB- Dunning 1993 Carletta 1996 Kilgarriff 1996 -RRB-	amod_Kilgarriff_1996 dep_Carletta_Kilgarriff num_Carletta_1996 dep_Dunning_Carletta dep_Dunning_1993 dep_recently_Dunning advmod_until_recently prep_ignored_until prep_in_ignored_CL advmod_ignored_largely auxpass_ignored_been aux_ignored_has nsubjpass_ignored_it appos_linguistics_CL amod_linguistics_computational conj_but_same_ignored prep_as_same_linguistics det_same_the cop_same_are nsubj_same_roots poss_roots_Its
W97-0320	J96-2004	o	As in much recent empirical work in discourse processing -LRB- e.g. Arhenberg et al. 1995 Isard & Carletta 1995 Litman & Passonneau 1995 Moser & Moore 1995 Hirschberg & Nakatani 1996 -RRB- we performed an intercoder reliability study investigating agreement in annotating the times	det_times_the dobj_annotating_times prepc_in_agreement_annotating dobj_investigating_agreement vmod_study_investigating nn_study_reliability nn_study_intercoder det_study_an dobj_performed_study nsubj_performed_we dep_performed_Moser dep_performed_Litman dep_performed_Carletta dep_performed_Isard num_Hirschberg_1996 conj_and_Hirschberg_Nakatani num_Moser_1995 conj_and_Moser_Moore num_Litman_1995 conj_and_Litman_Passonneau dep_Isard_Nakatani dep_Isard_Hirschberg conj_and_Isard_Moore conj_and_Isard_Moser conj_and_Isard_Passonneau conj_and_Isard_Litman num_Isard_1995 conj_and_Isard_Carletta dep_al._performed num_al._1995 nn_al._et nn_al._Arhenberg dep_e.g._al. nn_processing_discourse prep_in_work_processing amod_work_empirical amod_work_recent amod_work_much pobj_in_work dep_As_e.g. pcomp_As_in prep_``_As
W97-0320	J96-2004	o	Intercoder reliability was assessed using Cohen 's Kappa statistic -LRB- ~ -RRB- -LRB- Siegel & Castellan 1988 Carletta 1996 -RRB-	num_Carletta_1996 dep_Siegel_Carletta num_Siegel_1988 conj_and_Siegel_Castellan appos_statistic_~ nn_statistic_Kappa poss_statistic_Cohen dep_using_Castellan dep_using_Siegel dobj_using_statistic xcomp_assessed_using auxpass_assessed_was nsubjpass_assessed_reliability nn_reliability_Intercoder
W97-0320	J96-2004	o	A ~ value of 0.8 or greater indicates a high level of reliability among raters with values between 0.67 and 0.8 indicating only moderate agreement -LRB- Hirschberg ~ Nakatani 1996 Carletta 1996 -RRB-	num_Carletta_1996 num_Nakatani_1996 num_Nakatani_~ dep_Hirschberg_Carletta dep_Hirschberg_Nakatani dep_agreement_Hirschberg amod_agreement_moderate advmod_agreement_only dobj_indicating_agreement conj_and_0.67_0.8 vmod_values_indicating prep_between_values_0.8 prep_between_values_0.67 prep_among_level_raters prep_of_level_reliability amod_level_high det_level_a prep_with_indicates_values dobj_indicates_level nsubj_indicates_value conj_or_0.8_greater prep_of_value_greater prep_of_value_0.8 nn_value_~ det_value_A
W98-0317	J96-2004	o	Cohen 's Kappa ~ -LRB- Bakeman and Gottman 1986 Carletta 1996 -RRB-	amod_Carletta_1996 dep_Bakeman_Carletta conj_and_Bakeman_1986 conj_and_Bakeman_Gottman dep_~_1986 dep_~_Gottman dep_~_Bakeman nn_~_Kappa poss_~_Cohen
W98-0319	J96-2004	o	The labeling agreement was 84 % -LRB- n = .80 -LRB- Carletta 1996 -RRB- -RRB-	dep_Carletta_1996 dep_=_.80 dep_n_Carletta amod_n_= dep_%_n num_%_84 cop_%_was nsubj_%_agreement nn_agreement_labeling det_agreement_The
W99-0305	J96-2004	o	Such a coding procedure covers for example how segmentation of a corpus is performed if multiple tagging is allowed and if so is it unlimited or are there just certain combinations of tags not allowed is look ahead permitted etc For further information on coding procedures we want to refer to \ -LSB- Dybkjmr et al. 1998 \ -RSB- and for good examples of coding books see for example \ -LSB- Carletta et al. 1996 \ -RSB- \ -LSB- Alexandersson et al. 1998 \ -RSB- or \ -LSB- Thym ~ Gobbel and Levin1998 \ -RSB-	num_Levin1998_\ conj_and_Gobbel_Levin1998 dep_Thym_Levin1998 dep_Thym_Gobbel dep_Thym_~ dep_\_Thym num_\_1998 nn_\_al. dep_Alexandersson_\ nn_Alexandersson_et dep_\_Alexandersson num_\_1996 nn_\_al. dep_Carletta_\ nn_Carletta_et conj_or_\_\ conj_or_\_\ dep_\_Carletta prep_for_see_examples nsubj_see_covers amod_books_coding prep_of_examples_books amod_examples_good num_\_1998 nn_\_al. dep_Dybkjmr_\ nn_Dybkjmr_et aux_\_to xcomp_refer_\ aux_refer_to xcomp_want_refer nsubj_want_we amod_procedures_coding prep_on_information_procedures amod_information_further rcmod_etc_want prep_for_etc_information dobj_permitted_etc vmod_look_permitted advmod_look_ahead cop_look_is nsubj_look_combinations neg_allowed_not vmod_tags_allowed prep_of_combinations_tags amod_combinations_certain advmod_certain_just expl_are_there nsubj_are_covers dep_unlimited_\ dep_unlimited_\ dep_unlimited_\ prep_for_unlimited_example conj_and_unlimited_see dep_unlimited_Dybkjmr dep_unlimited_look conj_or_unlimited_are nsubj_unlimited_it cop_unlimited_is ccomp_unlimited_performed prep_for_unlimited_example nsubj_unlimited_covers mark_so_if conj_and_allowed_so auxpass_allowed_is nsubjpass_allowed_tagging mark_allowed_if amod_tagging_multiple advcl_performed_so advcl_performed_allowed auxpass_performed_is nsubjpass_performed_segmentation advmod_performed_how det_corpus_a prep_of_segmentation_corpus nn_covers_procedure amod_covers_coding det_covers_a amod_covers_Such
W99-0305	J96-2004	o	A CHECK move requests the partner to confirm information that the speaker has some reason to believe but is not entirely sure about \ -LSB- Carletta et al. 1996 \ -RSB-	num_\_1996 nn_\_al. dep_Carletta_\ nn_Carletta_et prep_about_sure_\ advmod_sure_entirely neg_sure_not cop_sure_is aux_believe_to vmod_reason_believe det_reason_some dobj_has_reason nsubj_has_speaker mark_has_that det_speaker_the ccomp_information_has dobj_confirm_information aux_confirm_to appos_partner_Carletta conj_but_partner_sure vmod_partner_confirm det_partner_the dep_requests_sure dep_requests_partner nn_requests_move nn_requests_CHECK det_requests_A
W99-0305	J96-2004	o	However CHECK moves are almost always about some information which the speaker has been told \ -LSB- Carletta et al. 1996 \ -RSB- a description that models the backward looking functionality of a dialogue act	nn_act_dialogue det_act_a prep_of_functionality_act amod_functionality_looking det_functionality_the advmod_looking_backward dep_models_functionality det_models_that dep_description_models det_description_a num_\_1996 nn_\_al. dep_Carletta_\ nn_Carletta_et dep_\_description dep_\_Carletta dep_told_\ auxpass_told_been aux_told_has nsubjpass_told_speaker dobj_told_which det_speaker_the rcmod_information_told det_information_some pobj_about_information advmod_about_always aux_about_are nsubj_about_moves advmod_about_However advmod_always_almost nn_moves_CHECK ccomp_``_about
W99-0307	J96-2004	o	It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_coders_Carletta amod_coders_naive agent_made_coders vmod_judgments_made prep_of_basis_judgments det_basis_the prep_on_assessed_basis advmod_assessed_only auxpass_assessed_be aux_assessed_can nsubjpass_assessed_reliability mark_assessed_that amod_schema_coding det_schema_a prep_of_reliability_schema det_reliability_the ccomp_argued_assessed auxpass_argued_been aux_argued_has nsubjpass_argued_It
W99-0307	J96-2004	o	k ~ P -LRB- A -RRB- P -LRB- E -RRB- -LRB- 3 -RRB- 1P -LRB- E -RRB- Carletta -LRB- 1996 -RRB- suggests that the units over which the kappa statistic is computed affects the outcome	det_outcome_the dobj_affects_outcome nsubj_affects_units mark_affects_that auxpass_computed_is nsubjpass_computed_statistic prep_over_computed_which nn_statistic_kappa det_statistic_the rcmod_units_computed det_units_the ccomp_suggests_affects nsubj_suggests_Carletta dobj_suggests_1P appos_Carletta_1996 nn_Carletta_E dep_1P_3 nn_1P_P appos_P_E nn_P_P appos_P_A nn_P_~ nn_P_k
W99-0311	J96-2004	o	The rationale for using Kappa is explained in -LRB- Carletta 1996 -RRB-	amod_Carletta_1996 dep_in_Carletta prep_explained_in auxpass_explained_is nsubjpass_explained_rationale dobj_using_Kappa prepc_for_rationale_using det_rationale_The
W99-0502	J96-2004	o	It us widely acknowledged that word sense d ~ samblguatmn -LRB- WSD -RRB- us a central problem m natural language processing In order for computers to be able to understand and process natural language beyond simple keyword matching the problem of d ~ samblguatmg word sense or dlscermng the meamng of a word m context must be effectively dealt with Advances in WSD v ill have slgmficant Impact on apphcatlons hke information retrieval and machine translation For natural language subtasks hke part-of-speech tagging or s -RRB- ntactm parsing there are relatlvely well defined and agreed-upon cnterm of what it means to have the correct part of speech or syntactic structure assigned to a word or sentence For instance the Penn Treebank corpus -LRB- Marcus et al 1993 -RRB- pro ~ ide ~ t large repo ~ tory of texts annotated w ~ th partof-speech and s -RCB- ntactm structure mformatlon Tv.o independent human annotators can achieve a high rate of agreement on assigning part-of-speech tags to words m a g ~ ven sentence Unfortunately th ~ s us not the case for word sense assignment F ~ rstly it is rarely the case that any two dictionaries will have the same set of sense defimtmns for a g ~ ven word Different d ~ ctlonanes tend to carve up the semantic space m a different way so to speak Secondly the hst of senses for a word m a typical dmtmnar ~ tend to be rather refined and comprehensive This is especmlly so for the commonly used words which have a large number of senses The sense dustmctmn between the different senses for a commonly used word m a d ~ ctmnary hke WoRDNET -LRB- Miller 1990 -RRB- tend to be rather fine Hence two human annotators may genuinely dusagree m their sense assignment to a word m context The agreement rate between human annotators on word sense assignment us an Important concern for the evaluatmn of WSD algorithms One would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human annotators The agreement rate between human annotators will then form the upper ceiling against whmh to compare the performance of WSD algorithms For instance the SENSEVAL exerclse has performed a detaded study to find out the raterannotator agreement among ~ ts lexicographers taggrog the word senses -LRB- Kllgamff 1998c Kllgarnff 1998a Kflgarrlff 1998b -RRB- 2 A Case Study In this-paper we examine the ~ ssue of raterannotator agreement by comparing the agreement rate of human annotators on a large sense-tagged corpus of more than 30,000 instances of the most frequently occurring nouns and verbs of Enghsh This corpus is the intersection of the WORDNET Semcor corpus -LRB- Miller et al 1993 -RRB- and the DSO corpus -LRB- Ng and Lee 1996 Ng 1997 -RRB- which has been independently tagged wlth the refined senses of WORDNET by two separate groups of human annotators The Semcor corpus us a subset of the Brown corpus tagged with ~ VoRDNET senses and consists of more than 670,000 words from 352 text files Sense taggmg was done on the content words -LRB- nouns ~ erbs adjectives and adverbs -RRB- m this subset The DSO corpus consists of sentences drawn from the Brown corpus and the Wall Street Journal For each word w from a hst of 191 frequently occurring words of Enghsh -LRB- 121 nouns and 70 verbs -RRB- sentences containing w -LRB- m singular or plural form and m its various reflectional verb form -RRB- are selected and each word occurrence w ~ s tagged w ~ th a sense from WoRDNET There ~ s a total of about 192,800 sentences in the DSO corpus m which one word occurrence has been sense-tagged m each sentence The intersection of the Semcor corpus and the DSO corpus thus consists of Brown corpus sentences m which a word occurrence w is sense-tagged m each sentence where w Is one of.the 191 frequently oc currmg English nouns or verbs Since this common pomon has been sense-tagged by two independent groups of human annotators ~ t serves as our data set for investigating inter-annotator agreement in this paper 3 Sentence Matching To determine the extent of inter-annotator agreement the first step ~ s to match each sentence m Semcor to its corresponding counterpart In the DSO corpus This step ~ s comphcated by the following factors 1 Although the intersected portion of both corpora came from Brown corpus they adopted different tokemzatmn convention and segmentartan into sentences differed sometimes 2 The latest versmn of Semcor makes use of the senses from WORDNET 1 6 whereas the senses used m the DSO corpus were from WoRDNET 15 1 To match the sentences we first converted the senses m the DSO corpus to those of WORDNET 1 6 We ignored all sentences m the DSO corpus m which a word is tagged with sense 0 or -1 -LRB- A word is tagged with sense 0 or -1 ff none of the given senses m WoRDNFT applies -RRB- 4 sentence from Semcor is considered to match one from the DSO corpus ff both sentences are exactl -RRB- ldent ~ cal or ff the ~ differ only m the pre ~ ence or absence of the characters -LRB- permd -RRB- or ' -LRB- hyphen -RRB- For each remaining Semcor sentence taking into account word ordering ff 75 % or more of the words m the sentence match those in a DSO corpus sentence then a potential match ~ s recorded These i kctua \ -LSB- ly the WORD ~ q ` ET senses used m the DSO corpus were from a shght variant of the official WORDNE'I 1 5 release Th ~ s ssas brought to our attention after the pubhc release of the DSO corpus potential matches are then manually verffied to ensure that they are true matches and to ~ eed out any false matches Using this method of matching a total of 13,188 sentence-palrs contasnmg nouns and 17,127 sentence-pa ~ rs containing verbs are found to match from both corpora ymldmg 30,315 sentences which form the intersected corpus used m our present study 4 The Kappa Statistic Suppose there are N sentences m our corpus where each sentence contains the word w Assume that w has M senses Let 4 be the number of sentences which are assigned identical sense b ~ two human annotators Then a simple measure to quantify the agreement rate between two human annotators Is Pc where Pc = A/N The drawback of this simple measure is that it does not take into account chance agreement between two annotators The Kappa statistic a -LRB- Cohen 1960 -RRB- is a better measure of rater-annotator agreement which takes into account the effect of chance agreement It has been used recently w ~ thm computatmnal hngu ~ stlcs to measure raterannotator agreement -LRB- Bruce and Wmbe 1998 Carletta 1996 Veroms 1998 -RRB- Let Cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 Then P ~ P ~ 1-P ~ where M j = l and Pe measures the chance agreement between two annotators A Kappa ~ alue of 0 indicates that the agreement is purely due to chance agreement whereas a Kappa ~ alue of 1 indicates perfect agreement A Kappa ~ alue of 0 8 and above is considered as mdmatmg good agreement -LRB- Carletta 1996 -RRB- Table 1 summarizes the inter-annotator agreement on the mtersected corpus The first -LRB- becond -RRB- row denotes agreement on the nouns -LRB- xerbs -RRB- wh ~ le the lass row denotes agreement on all words combined The a ~ erage ~ reported m the table is a s ~ mpie average of the individual ~ value of each word The agreement rate on the 30,315 sentences as measured by P = is 57 % This tallies with the figure reported ~ n our earlier paper -LRB- Ng and Lee 1996 -RRB- where we performed a quick test on a subset of 5,317 sentences n the intersection of both the Semcor corpus and the DSO corpus 10 \ -LSB- \ -RSB- mm m m m m m mm m m m m mm m m m Type Num of v ords A N \ -LSB- P ~ Avg Nouns 121 7,676 13,188 I 0 582 0 300 Verbs 70 9,520 17,127 I 0 555 0 347 All I 191 I 17,196 30,315 I 056T 0317 Table 1 Raw inter-annotator agreement 5 Algorithm Since the rater-annotator agreement on the intersected corpus is not high we would like to find out how the agreement rate would be affected if different sense classes were in use In this section we present a greedy search algorithm that can automatmalb derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achmve a higher agreement rate but we still maintain as many of the original sense classes as possible The algorithm is given m Figure 1 The algorithm operates on a set of sentences where each sentence contains an occurrence of the word w whmh has been sense-tagged by two human annotators At each Iteration of the algorithm tt finds the pair of sense classes Ct and Cj such that merging these two sense classes results in the highest t ~ value for the resulting merged group of sense classes It then proceeds to merge Cz and C ~ Thin process Is repeated until the ~ value reaches a satisfactory value ~ ~ t ~ which we set as 0 8 Note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any NLP tasks in which prior human agreement rate may not be high enough Such NLP tasks could be discourse tagging speech-act categorization etc 6 Results For each word w from the list of 121 nouns and 70 verbs ~ e applied the greedy search algorithm to each set of sentences in the intersected corpus contaming w For a subset of 95 words -LRB- 53 nouns and 42 verbs -RRB- the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting Kappa ~ alue reaches 0 8 or higher For the other 96 words m order for the Kappa value to reach 0 8 or higher the algorithm collapses all senses of the ~ ord to a single -LRB- trivial -RRB- class Table 2 and 3 summarizes the results for the set of 53 nouns and 42 ~ erbs respectively Table 2 md ~ cates that before the collapse of sense classes these 53 nouns have an average of 7 6 senses per noun There is a total of 5,339 sentences in the intersected corpus containing these nouns of which 3,387 sentences were assigned the same sense by the two groups of human annotators The average Kappa statistic -LRB- computed as a simple average of the Kappa statistic of ~ he mdlwdual nouns -RRB- is 0 463 After the collapse of sense classes by the greedy search algorithm the average number of senses per noun for these 53 nouns drops to 40 Howe ~ er the number of sentences which have been asmgned the same coarse sense by the annotators increases to 5,033 That is about 94 3 % of the sentences have been assigned the same coarse sense and that the average Kappa statistic has improved to 0 862 mgmfymg high rater-annotator agreement on the derived coarse senses Table3 gl ~ es the analogous figures for the 42 verbs agmn mdmatmg that high agreement is achieved on the coarse sense classes den ~ ed for verbs 7 Discussion Our findings on rater-annotator agreement for word sense tagging indicate that for average language users it is quite dl ~ cult to achieve high agreement when they are asked to assign refned sense tags -LRB- such as those found in WORDNET -RRB- given only the scanty definition entries m the WORDNET dlctionary and a few or no example sentences for the usage of each word sense Thin observation agrees wlth that obtmned m a recent study done by -LRB- Veroms 1998 -RRB- where the agreement on sense-tagging by naive users was also not hlgh Thus It appears that an average language user is able to process language wlthout needing to perform the task of dlsamblguatmg word sense to a very fine-grained resolutmn as formulated m a tradltlonal dmtlonary In contrast expert lexicographers tagged the ~ ord sense in the sentences used m the SENSEVAL exerclse where high rater-annotator agreement was reported There are also fuller dlctlonary entries m the HECTOR dlctlonary used and more e < amples showing the usage of each word sense m HECTOR These factors are likely to have contributed to the difference in rater-annotator agreement observed m the three studies conducted We also examined the coarse sense classes derived by the greedy search algorithm Vv ' e found some interesting groupings of coarse senses for nouns which ~ e hst in Table 4 From Table 4 it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human mtmtwe judgment of sense graz -RCB- ulanty It Is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of WoRDNET As an example there is a total Ii loop let Ct C M denote the current M sense classes ~ * + -- oo for all z ,3 such that 1 < < 3 < M let C \ -LSB- C ~ w _ 1 denote the resulting M 1 sense classes by mergmg C and C 3 compute ~ -LRB- C \ -LSB- C ~ / _ t -RRB- ff ~ -LRB- C C ~ 4_x -RRB- > ~ * then ~ + ~ -LRB- C ~ C ~ _ t -RRB- z * + ~ ~ * + end for merge the sense class C	nn_C_class nn_C_sense det_C_the dobj_merge_C dep_for_merge prep_*_for conj_+_*_end num_*_~ appos_*_end appos_*_* conj_+_*_~ dep_z_~ dep_z_* num_t__ nn_t_~ nn_t_C nn_~_C num_~_~ quantmod_~_then quantmod_~_* num_*_~ quantmod_~_> nn_4_x_~ nn_4_x_C dep_C_4_x vmod_~_z dep_~_t conj_+_~_~ dep_~_~ dep_~_C nn_~_ff nn_~_~ num_t__ dep_~_t nn_~_C nn_\_C nn_\_~ dep_compute_~ dep_compute_~ dobj_compute_\ nsubj_compute_~ nn_C_mergmg nn_classes_sense num_classes_1 nn_classes_M amod_classes_resulting det_classes_the prep_by_denote_C dobj_denote_classes nsubj_denote__ num___1 nn___w nn___~ nn___C amod_C_\ dobj_let_C nsubj_let_M advmod_let_< nsubj_let_1 mark_let_that num_M_3 quantmod_3_< quantmod_3_< ccomp_such_let num_,3_3 conj_and_,3_C rcmod_,3_denote amod_,3_such dep_,3_z dep_,3_all prep_for_oo_C prep_for_oo_,3 cc_*_+ dep_~_oo dep_~_* nn_classes_sense nn_classes_M amod_classes_current det_classes_the parataxis_denote_compute dobj_denote_classes nsubj_denote_M nn_M_C parataxis_Ct_denote dep_let_Ct dep_loop_let nn_loop_Ii amod_loop_total det_loop_a nsubj_is_loop expl_is_there det_example_an prep_of_senses_WoRDNET amod_senses_refined det_senses_the advmod_refined_overly prep_as_attributed_example prep_to_attributed_senses advmod_attributed_solely auxpass_attributed_be aux_attributed_can nsubjpass_attributed_some mark_attributed_that amod_annotators_human prep_of_groups_annotators num_groups_two det_groups_the prep_between_disagreement_groups det_disagreement_the prep_of_some_disagreement ccomp_clear_attributed cop_clear_Is nsubj_clear_It dep_clear_ulanty nn_graz_sense prep_of_judgment_graz nn_judgment_mtmtwe amod_judgment_human prep_to_correspond_judgment nsubj_correspond_that rcmod_senses_correspond nn_senses_word prep_of_groupings_senses amod_groupings_interesting dobj_derive_groupings aux_derive_can nsubj_derive_algorithm mark_derive_that nn_algorithm_search amod_algorithm_greedy det_algorithm_the ccomp_apparent_derive cop_apparent_is nsubj_apparent_it dep_apparent_hst dep_apparent_e num_Table_4 prep_from_Table_Table num_Table_4 prep_in_hst_Table dep_~_apparent nsubj_~_which rcmod_nouns_~ prep_for_senses_nouns amod_senses_coarse prep_of_groupings_senses amod_groupings_interesting det_groupings_some dobj_found_groupings nsubj_found_e nsubj_found_Vv rcmod_algorithm_found nn_algorithm_search amod_algorithm_greedy det_algorithm_the agent_derived_algorithm vmod_classes_derived nn_classes_sense amod_classes_coarse det_classes_the dobj_examined_classes advmod_examined_also nsubj_examined_We ccomp_conducted_examined vmod_studies_conducted num_studies_three det_studies_the nn_studies_m amod_studies_observed nn_studies_agreement amod_agreement_rater-annotator prep_in_difference_studies det_difference_the prep_to_contributed_difference aux_contributed_have aux_contributed_to parataxis_likely_clear xcomp_likely_contributed cop_likely_are csubj_likely_e csubj_likely_used det_factors_These nn_factors_HECTOR nn_factors_m nn_factors_sense nn_factors_word det_factors_each prep_of_usage_factors det_usage_the dobj_showing_usage vmod_amples_showing amod_amples_< advmod_e_more dobj_used_amples conj_and_used_e dep_used_dlctlonary dep_used_HECTOR dep_used_the dep_used_m dep_used_entries dep_used_dlctlonary dep_used_fuller parataxis_are_likely advmod_are_also expl_are_There ccomp_reported_are auxpass_reported_was nsubjpass_reported_agreement advmod_reported_where amod_agreement_rater-annotator amod_agreement_high rcmod_exerclse_reported nn_exerclse_SENSEVAL det_exerclse_the dep_m_exerclse dobj_used_m vmod_sentences_used det_sentences_the nn_sense_ord nn_sense_~ det_sense_the dep_tagged_is prep_in_tagged_sentences dobj_tagged_sense nsubj_tagged_lexicographers ccomp_tagged_m dep_tagged_formulated mark_tagged_as nn_lexicographers_expert prep_in_dmtlonary_contrast amod_dmtlonary_tradltlonal det_dmtlonary_a dep_m_dmtlonary amod_resolutmn_fine-grained det_resolutmn_a advmod_fine-grained_very nn_sense_word nn_sense_dlsamblguatmg prep_of_task_sense det_task_the dep_perform_tagged prep_to_perform_resolutmn dobj_perform_task aux_perform_to xcomp_needing_perform vmod_wlthout_needing nn_wlthout_language dobj_process_wlthout aux_process_to xcomp_able_process cop_able_is nsubj_able_user mark_able_that nn_user_language amod_user_average det_user_an ccomp_appears_able nsubj_appears_It parataxis_hlgh_appears advmod_hlgh_Thus neg_hlgh_not advmod_hlgh_also aux_hlgh_was nsubj_hlgh_agreement advmod_hlgh_where amod_users_naive prep_by_agreement_users prep_on_agreement_sense-tagging det_agreement_the rcmod_Veroms_hlgh amod_Veroms_1998 agent_done_Veroms vmod_study_done amod_study_recent det_study_a dep_m_study dobj_obtmned_m nsubj_obtmned_that rcmod_wlth_obtmned dobj_agrees_wlth nsubj_agrees_observation amod_observation_Thin rcmod_sense_agrees nn_sense_word det_sense_each prep_of_usage_sense det_usage_the nn_sentences_example prep_for_few_usage dep_few_sentences conj_or_few_no det_few_a nsubj_few_WORDNET conj_and_dlctionary_no conj_and_dlctionary_few nsubj_dlctionary_WORDNET det_WORDNET_the rcmod_m_few rcmod_m_dlctionary dep_entries_m dep_definition_entries dep_scanty_definition amod_the_scanty dep_only_the advmod_given_only prep_in_found_WORDNET vmod_those_found prep_such_as_tags_those nn_tags_sense amod_tags_refned vmod_assign_given dobj_assign_tags aux_assign_to xcomp_asked_assign auxpass_asked_are nsubjpass_asked_they advmod_asked_when amod_agreement_high advcl_achieve_asked dobj_achieve_agreement aux_achieve_to vmod_cult_achieve nn_cult_~ amod_cult_dl cop_cult_is nsubj_cult_it prep_for_cult_users mark_cult_that advmod_dl_quite nn_users_language amod_users_average ccomp_indicate_cult nn_sense_word amod_agreement_tagging prep_for_agreement_sense amod_agreement_rater-annotator prep_on_findings_agreement poss_findings_Our dep_Discussion_findings num_Discussion_7 appos_verbs_Discussion prep_for_ed_verbs nsubj_ed_den num_den_~ rcmod_classes_ed nn_classes_sense amod_classes_coarse det_classes_the prep_on_achieved_classes auxpass_achieved_is nsubjpass_achieved_agreement amod_agreement_high det_agreement_that rcmod_mdmatmg_achieved amod_mdmatmg_agmn appos_verbs_mdmatmg num_verbs_42 det_verbs_the prep_for_figures_verbs amod_figures_analogous det_figures_the dep_es_indicate dobj_es_figures nsubj_es_agreement num_gl_~ nn_gl_Table3 nn_gl_senses amod_gl_coarse amod_gl_derived det_gl_the prep_on_agreement_gl amod_agreement_rater-annotator amod_agreement_high nn_agreement_mgmfymg number_862_0 parataxis_improved_es prep_to_improved_862 aux_improved_has nsubj_improved_statistic mark_improved_that nn_statistic_Kappa amod_statistic_average det_statistic_the amod_sense_coarse amod_sense_same det_sense_the dobj_assigned_sense auxpass_assigned_been aux_assigned_have nsubjpass_assigned_% det_sentences_the prep_of_%_sentences num_%_3 num_%_94 quantmod_94_about conj_and_is_improved conj_and_is_assigned num_That_5,033 dep_increases_improved dep_increases_assigned dep_increases_is prep_to_increases_That nsubj_increases_number det_annotators_the amod_sense_coarse amod_sense_same det_sense_the agent_asmgned_annotators dobj_asmgned_sense auxpass_asmgned_been aux_asmgned_have nsubjpass_asmgned_which rcmod_sentences_asmgned prep_of_number_sentences det_number_the num_er_~ nn_er_Howe num_er_40 prep_to_drops_er nsubj_drops_number num_nouns_53 det_nouns_these prep_per_senses_noun prep_for_number_nouns prep_of_number_senses amod_number_average det_number_the nn_algorithm_search amod_algorithm_greedy det_algorithm_the nn_classes_sense prep_by_collapse_algorithm prep_of_collapse_classes det_collapse_the prep_after_463_collapse number_463_0 cop_463_is parataxis_463_is csubj_463_have amod_nouns_mdlwdual dep_nouns_he nn_nouns_~ prep_of_statistic_nouns nn_statistic_Kappa det_statistic_the prep_of_average_statistic amod_average_simple det_average_a prep_as_computed_average dep_statistic_computed nn_statistic_Kappa amod_statistic_average det_statistic_The amod_annotators_human prep_of_groups_annotators num_groups_two det_groups_the amod_sense_same det_sense_the dep_assigned_statistic agent_assigned_groups dobj_assigned_sense auxpass_assigned_were nsubjpass_assigned_sentences prep_of_assigned_which num_sentences_3,387 rcmod_nouns_assigned det_nouns_these dobj_containing_nouns vmod_corpus_containing amod_corpus_intersected det_corpus_the prep_in_sentences_corpus num_sentences_5,339 prep_of_total_sentences det_total_a nsubj_is_total expl_is_There prep_per_senses_noun num_senses_6 number_6_7 prep_of_average_senses det_average_an dobj_have_average nsubj_have_nouns prep_before_have_collapse mark_have_that num_nouns_53 det_nouns_these nn_classes_sense prep_of_collapse_classes det_collapse_the parataxis_cates_increases ccomp_cates_drops ccomp_cates_463 nsubj_cates_~ advmod_cates_respectively nn_~_md num_~_2 nn_~_Table nn_erbs_~ num_erbs_42 conj_and_nouns_erbs num_nouns_53 prep_of_set_erbs prep_of_set_nouns det_set_the prep_for_results_set det_results_the parataxis_summarizes_cates dobj_summarizes_results num_Table_2 nn_Table_class amod_Table_single det_Table_a dep_single_trivial nn_ord_~ det_ord_the prep_of_senses_ord det_senses_all prep_to_collapses_Table dobj_collapses_senses nsubj_collapses_algorithm nsubj_collapses_Results advmod_collapses_etc det_algorithm_the conj_or_8_higher num_8_0 dobj_reach_higher dobj_reach_8 aux_reach_to nn_value_Kappa det_value_the vmod_order_reach prep_for_order_value nn_order_m ccomp_order_able num_words_96 amod_words_other det_words_the conj_or_8_higher num_8_0 prep_for_reaches_words dobj_reaches_higher dobj_reaches_8 nsubj_reaches_alue mark_reaches_that dep_reaches_such nn_alue_~ nn_alue_Kappa amod_alue_resulting det_alue_the num_words_95 det_words_these rcmod_each_reaches prep_of_each_words num_senses_more num_senses_2 conj_or_2_more prep_for_set_each prep_of_set_senses amod_set_coarser det_set_a dobj_derive_set aux_derive_to xcomp_able_derive cop_able_was nsubj_able_algorithm det_algorithm_the num_verbs_42 conj_and_nouns_verbs num_nouns_53 dep_words_verbs dep_words_nouns num_words_95 prep_of_subset_words det_subset_a nn_w_contaming nn_w_corpus amod_w_intersected det_w_the prep_in_set_w prep_of_set_sentences det_set_each nn_algorithm_search amod_algorithm_greedy det_algorithm_the prep_for_applied_subset prep_to_applied_set dobj_applied_algorithm nsubj_applied_~ dep_~_e num_verbs_70 conj_and_nouns_verbs num_nouns_121 prep_of_list_verbs prep_of_list_nouns det_list_the nn_w_word det_w_each appos_Results_order rcmod_Results_applied prep_from_Results_list prep_for_Results_w num_Results_6 amod_categorization_speech-act conj_and_tagging_3 conj_and_tagging_collapses appos_tagging_categorization nn_tagging_discourse cop_tagging_be aux_tagging_could nsubj_tagging_~ nn_tasks_NLP amod_tasks_Such amod_tasks_enough amod_tasks_high cop_tasks_be neg_tasks_not aux_tasks_may dep_tasks_rate prep_in_tasks_which nn_rate_agreement amod_rate_human amod_rate_prior rcmod_tasks_tasks nn_tasks_NLP det_tasks_any prep_for_set_tasks amod_set_refined det_set_a prep_from_set_set prep_of_set_classes amod_set_coarser det_set_any dobj_deriving_set prepc_to_applicable_deriving advmod_applicable_also cop_applicable_is nsubj_applicable_algorithm mark_applicable_that det_algorithm_this num_Note_8 num_Note_0 ccomp_set_applicable prep_as_set_Note nsubj_set_we dobj_set_which nn_t_~ rcmod_~_set conj_~_~ conj_~_t rcmod_value_3 rcmod_value_collapses rcmod_value_tagging amod_value_satisfactory det_value_a dobj_reaches_value nsubj_reaches_value mark_reaches_until nn_value_~ det_value_the dep_repeated_summarizes advcl_repeated_reaches auxpass_repeated_Is csubjpass_repeated_sense-tagged nn_process_Thin num_process_~ nn_process_C conj_and_Cz_process dobj_merge_process dobj_merge_Cz aux_merge_to xcomp_proceeds_merge advmod_proceeds_then nsubj_proceeds_It nn_classes_sense prep_of_group_classes amod_group_merged amod_group_resulting det_group_the prep_for_value_group nn_value_~ nn_value_t amod_value_highest det_value_the ccomp_results_proceeds prep_in_results_value csubj_results_merging mark_results_that dep_results_such nn_classes_sense num_classes_two det_classes_these dobj_merging_classes dep_Ct_results conj_and_Ct_Cj prep_classes_Cj prep_classes_Ct dobj_sense_classes prep_of_pair_sense det_pair_the dobj_finds_pair nsubj_finds_tt det_algorithm_the prep_of_Iteration_algorithm det_Iteration_each rcmod_annotators_finds prep_at_annotators_Iteration amod_annotators_human num_annotators_two agent_sense-tagged_annotators auxpass_sense-tagged_been aux_sense-tagged_has nn_whmh_w nn_whmh_word det_whmh_the prep_of_occurrence_whmh det_occurrence_an dobj_contains_occurrence nsubj_contains_sentence advmod_contains_where det_sentence_each rcmod_sentences_contains prep_of_set_sentences det_set_a ccomp_operates_repeated prep_on_operates_set nsubj_operates_Figure det_algorithm_The dep_Figure_algorithm num_Figure_1 nn_Figure_m ccomp_given_operates auxpass_given_is nsubjpass_given_algorithm det_algorithm_The nn_classes_sense amod_classes_original det_classes_the prep_as_many_possible prep_of_many_classes ccomp_maintain_given prep_as_maintain_many advmod_maintain_still nsubj_maintain_we nn_rate_agreement amod_rate_higher det_rate_a conj_but_achmve_maintain dobj_achmve_rate nsubj_achmve_classes nn_classes_sense amod_classes_coarse amod_classes_derived amod_classes_resulting det_classes_The dep_annotators_maintain dep_annotators_achmve amod_annotators_human num_annotators_two agent_assigned_annotators vmod_tags_assigned nn_tags_sense det_tags_the prep_on_based_tags vmod_classes_based nn_classes_sense amod_classes_coarser dobj_derive_classes dep_automatmalb_derive aux_automatmalb_can nsubj_automatmalb_that rcmod_algorithm_automatmalb nn_algorithm_search amod_algorithm_greedy det_algorithm_a dobj_present_algorithm nsubj_present_we det_section_this prep_in_were_section prep_in_were_use nsubj_were_classes mark_were_if nn_classes_sense amod_classes_different advcl_affected_were auxpass_affected_be aux_affected_would nsubjpass_affected_rate advmod_affected_how nn_rate_agreement det_rate_the ccomp_find_affected prt_find_out aux_find_to xcomp_like_find aux_like_would nsubj_like_we neg_high_not cop_high_is nsubj_high_agreement mark_high_Since amod_corpus_intersected det_corpus_the prep_on_agreement_corpus amod_agreement_rater-annotator det_agreement_the num_Algorithm_5 dep_agreement_high dep_agreement_Algorithm amod_agreement_inter-annotator nn_agreement_Raw num_agreement_1 nn_agreement_Table num_agreement_0317 dobj_056T_agreement nsubj_056T_I num_I_30,315 number_30,315_17,196 dep_30,315_I number_30,315_191 rcmod_I_056T dep_All_I dep_347_All number_347_0 dep_347_555 dep_347_Verbs dep_347_0 dep_347_582 number_555_0 dep_555_I number_555_17,127 dep_555_9,520 number_9,520_70 num_Verbs_300 number_582_0 dep_582_I number_582_13,188 dep_582_7,676 number_7,676_121 advmod_Nouns_347 nn_Nouns_Avg num_Nouns_~ rcmod_P_present rcmod_P_like dep_P_Nouns dep_\_P nn_\_N det_\_A dep_ords_\ dep_v_ords prep_of_Num_v nn_Num_Type nn_Num_m nn_Num_m nn_Num_m nn_Num_mm nn_Num_m nn_Num_m nn_Num_m nn_Num_m nn_Num_mm nn_Num_m nn_Num_m nn_Num_m nn_Num_m nn_Num_m nn_Num_mm num_Num_\ num_\_10 nn_\_corpus nn_\_DSO det_\_the conj_and_corpus_\ nn_corpus_Semcor det_corpus_the preconj_corpus_both appos_intersection_Num prep_of_intersection_\ prep_of_intersection_corpus det_intersection_the nn_intersection_n num_sentences_5,317 prep_of_subset_sentences det_subset_a prep_on_test_subset amod_test_quick det_test_a dobj_performed_test nsubj_performed_we advmod_performed_where dep_Ng_1996 conj_and_Ng_Lee appos_paper_intersection rcmod_paper_performed dep_paper_Lee dep_paper_Ng amod_paper_earlier poss_paper_our dep_n_paper num_n_~ dobj_reported_n nsubj_reported_tallies det_figure_the prep_with_tallies_figure det_tallies_This rcmod_%_reported num_%_57 cop_%_is nsubj_%_rate amod_P_= prep_by_measured_P mark_measured_as num_sentences_30,315 det_sentences_the dep_rate_measured prep_on_rate_sentences nn_rate_agreement det_rate_The rcmod_word_% det_word_each prep_of_value_word nn_value_~ amod_value_individual det_value_the prep_of_average_value amod_average_mpie dobj_~_average dep_s_~ det_s_a cop_s_is nsubj_s_table det_table_the nn_table_m ccomp_reported_s nsubj_reported_~ nn_~_erage nn_~_~ det_~_a det_~_The ccomp_combined_reported vmod_words_combined det_words_all prep_on_agreement_words dobj_denotes_agreement nsubj_denotes_row nn_row_lass det_row_the predet_row_le nn_row_~ nn_row_wh appos_nouns_xerbs det_nouns_the dep_agreement_denotes prep_on_agreement_nouns dep_denotes_agreement vmod_row_denotes amod_row_first det_row_The dep_first_becond amod_corpus_mtersected det_corpus_the appos_agreement_row prep_on_agreement_corpus amod_agreement_inter-annotator det_agreement_the dobj_summarizes_agreement nsubj_summarizes_Carletta dep_summarizes_agreement mark_summarizes_as num_Table_1 dep_Carletta_Table dep_Carletta_1996 amod_agreement_good nn_agreement_mdmatmg advcl_considered_summarizes auxpass_considered_is advmod_considered_above num_8_0 conj_and_alue_considered prep_of_alue_8 nn_alue_~ nn_alue_Kappa det_alue_A nn_alue_agreement amod_alue_perfect dobj_indicates_considered dobj_indicates_alue nsubj_indicates_alue mark_indicates_whereas prep_of_alue_1 nn_alue_~ nn_alue_Kappa det_alue_a nn_agreement_chance prep_to_due_agreement advmod_due_purely cop_due_is nsubj_due_agreement mark_due_that det_agreement_the ccomp_indicates_due prep_of_alue_0 nn_alue_~ nn_alue_Kappa det_alue_A nn_alue_annotators num_alue_two prep_between_agreement_alue nn_agreement_chance det_agreement_the dobj_measures_agreement nsubj_measures_j dep_measures_M advmod_measures_where conj_and_l_Pe dep_=_Pe dep_=_l amod_j_= rcmod_~_measures nn_~_1-P num_~_~ nn_~_P dep_~_P advmod_~_Then dep_P_~ num_annotator_2 num_sense_3 dep_assigned_indicates dep_assigned_~ agent_assigned_annotator dobj_assigned_sense auxpass_assigned_been aux_assigned_have nsubjpass_assigned_whmh prep_of_number_sentences det_number_the conj_and_annotator_number num_annotator_1 num_sense_3 agent_assigned_number agent_assigned_annotator dobj_assigned_sense auxpass_assigned_been aux_assigned_have nsubjpass_assigned_which rcmod_sentences_assigned prep_of_number_sentences det_number_the prep_of_sum_number det_sum_the cop_sum_be nsubj_sum_Cj ccomp_Let_sum nsubj_Let_agreement amod_Bruce_1998 appos_Bruce_Veroms amod_Bruce_1996 appos_Bruce_Carletta amod_Bruce_1998 conj_and_Bruce_Wmbe dep_agreement_Wmbe dep_agreement_Bruce amod_agreement_raterannotator ccomp_measure_Let aux_measure_to nn_stlcs_~ nn_stlcs_hngu amod_stlcs_computatmnal nn_stlcs_thm nn_stlcs_~ amod_stlcs_w advmod_w_recently advcl_used_indicates dep_used_assigned vmod_used_measure dobj_used_stlcs auxpass_used_been aux_used_has nsubjpass_used_It nn_agreement_chance prep_of_effect_agreement det_effect_the parataxis_takes_used dobj_takes_effect prep_into_takes_account nsubj_takes_which amod_agreement_rater-annotator rcmod_measure_takes prep_of_measure_agreement amod_measure_better det_measure_a cop_measure_is nsubj_measure_a amod_Cohen_1960 appos_a_Cohen rcmod_statistic_measure nn_statistic_Kappa det_statistic_The dep_annotators_statistic num_annotators_two prep_between_agreement_annotators nn_agreement_chance nn_agreement_account prep_into_take_agreement neg_take_not aux_take_does nsubj_take_it mark_take_that ccomp_is_take nsubj_is_Pc advmod_is_where amod_measure_simple det_measure_this prep_of_drawback_measure det_drawback_The nn_drawback_A/N amod_drawback_= appos_Pc_drawback rcmod_Pc_is cop_Pc_Is nsubj_Pc_number amod_annotators_human num_annotators_two prep_between_rate_annotators nn_rate_agreement det_rate_the dobj_quantify_rate aux_quantify_to amod_measure_simple det_measure_a advmod_measure_Then amod_annotators_human num_annotators_two num_annotators_~ dep_b_measure dep_b_annotators nn_b_sense amod_b_identical xcomp_assigned_quantify dobj_assigned_b auxpass_assigned_are nsubjpass_assigned_which rcmod_sentences_assigned prep_of_number_sentences det_number_the cop_number_be nsubj_number_4 dep_Let_Pc nsubj_Let_senses nn_senses_M ccomp_has_Let nsubj_has_w mark_has_that ccomp_Assume_has nsubj_Assume_corpora mark_Assume_from dep_word_w det_word_the nsubj_contains_sentence advmod_contains_where det_sentence_each appos_corpus_word rcmod_corpus_contains poss_corpus_our dobj_m_corpus dep_sentences_m nn_sentences_N nsubj_are_sentences expl_are_there ccomp_Suppose_are nsubj_Suppose_Statistic nn_Statistic_Kappa det_Statistic_The num_Statistic_4 rcmod_study_Suppose amod_study_present poss_study_our appos_m_study dobj_used_m vmod_corpus_used amod_corpus_intersected det_corpus_the dobj_form_corpus nsubj_form_which rcmod_sentences_form num_sentences_30,315 nn_sentences_ymldmg appos_corpora_sentences preconj_corpora_both advcl_match_Assume aux_match_to xcomp_found_match auxpass_found_are nsubjpass_found_ly dobj_containing_verbs vmod_rs_containing num_rs_~ nn_rs_sentence-pa num_rs_17,127 conj_and_nouns_rs nn_nouns_contasnmg nn_nouns_sentence-palrs num_nouns_13,188 prep_of_total_rs prep_of_total_nouns det_total_a appos_matching_total prep_of_method_matching det_method_this dobj_Using_method amod_matches_false det_matches_any prep_out_~_matches dobj_~_eed aux_~_to nsubj_~_they vmod_matches_Using conj_and_matches_~ amod_matches_true cop_matches_are nsubj_matches_they mark_matches_that ccomp_ensure_~ ccomp_ensure_matches aux_ensure_to xcomp_verffied_ensure advmod_verffied_manually advmod_verffied_then auxpass_verffied_are nsubjpass_verffied_release mark_verffied_after amod_matches_potential nn_matches_corpus nn_matches_DSO det_matches_the prep_of_release_matches nn_release_pubhc det_release_the poss_attention_our advcl_brought_verffied prep_to_brought_attention nn_ssas_s num_ssas_~ nn_ssas_Th nn_ssas_release num_ssas_5 num_ssas_1 nn_ssas_WORDNE'I amod_ssas_official det_ssas_the prep_of_variant_ssas nn_variant_shght det_variant_a prep_from_were_variant nn_corpus_DSO det_corpus_the nn_corpus_m dobj_used_corpus nsubj_used_senses dep_used_q dep_used_~ dep_used_WORD dep_used_the nn_senses_ET dep_ly_brought dep_ly_were dep_ly_used rcmod_\_found nn_\_kctua dep_\_i dep_\_These dobj_recorded_\ aux_recorded_s nsubj_recorded_~ advmod_recorded_then nn_~_match amod_~_potential det_~_a nn_sentence_corpus nn_sentence_DSO det_sentence_a prep_in_those_sentence dep_match_those nn_match_sentence det_match_the nn_match_m dep_words_match det_words_the prep_of_%_words conj_or_%_more num_%_75 nn_%_ff vmod_word_ordering nn_word_account prep_into_taking_word nn_sentence_Semcor amod_sentence_remaining det_sentence_each vmod_permd_recorded dep_permd_more dep_permd_% vmod_permd_taking prep_for_permd_sentence appos_permd_hyphen cc_permd_or det_characters_the prep_of_ence_characters conj_or_ence_absence nn_ence_~ amod_ence_pre det_ence_the dep_m_absence dep_m_ence advmod_m_only dep_differ_permd dobj_differ_m nsubj_differ_ff nsubj_differ_cal det_~_the dep_cal_~ conj_or_cal_ff nn_cal_~ amod_cal_ldent cop_exactl_are nsubj_exactl_sentences preconj_sentences_both nn_ff_corpus nn_ff_DSO det_ff_the ccomp_match_differ parataxis_match_exactl prep_from_match_ff dobj_match_one aux_match_to xcomp_considered_match auxpass_considered_is nsubjpass_considered_sense prep_from_sentence_Semcor nsubj_applies_m nn_WoRDNFT_m nn_WoRDNFT_senses amod_WoRDNFT_given det_WoRDNFT_the prep_of_none_WoRDNFT nn_none_ff nn_none_sense num_ff_-1 conj_or_sense_ff num_sense_0 prep_with_tagged_none auxpass_tagged_is nsubjpass_tagged_word det_word_A conj_or_sense_-1 num_sense_0 prep_with_tagged_-1 prep_with_tagged_sense auxpass_tagged_is nsubjpass_tagged_word dobj_tagged_which det_word_a rcmod_m_tagged rcmod_m_tagged nn_m_corpus nn_m_DSO det_m_the dep_m_m rcmod_sentences_applies det_sentences_all dobj_ignored_sentences nsubj_ignored_We tmod_ignored_corpus number_6_1 num_WORDNET_6 prep_of_those_WORDNET prep_to_corpus_those nn_corpus_DSO det_corpus_the rcmod_m_ignored nn_m_senses det_m_the dobj_converted_m advmod_converted_first nsubj_converted_we advcl_converted_were nsubj_converted_m det_sentences_the dobj_match_sentences aux_match_To num_1_15 num_WoRDNET_1 xcomp_were_match prep_from_were_WoRDNET nsubj_were_senses mark_were_whereas nn_corpus_DSO det_corpus_the nn_corpus_m amod_corpus_used dep_senses_corpus det_senses_the num_WORDNET_6 num_WORDNET_1 det_senses_the prep_from_use_WORDNET prep_of_use_senses dobj_makes_use nsubj_makes_versmn advmod_makes_sometimes prep_of_versmn_Semcor amod_versmn_latest det_versmn_The num_versmn_2 ccomp_differed_makes dep_differed_Is nsubj_differed_w advmod_differed_where nn_convention_tokemzatmn amod_convention_different dobj_adopted_convention nsubj_adopted_they nn_corpus_Brown prep_from_came_corpus nsubj_came_portion mark_came_Although preconj_corpora_both prep_of_portion_corpora amod_portion_intersected det_portion_the num_factors_1 amod_factors_following det_factors_the agent_comphcated_factors advcl_s_came vmod_s_comphcated nn_s_~ nn_s_step det_s_This nn_corpus_DSO det_corpus_the prep_in_counterpart_corpus amod_counterpart_corresponding poss_counterpart_its nn_Semcor_m nn_Semcor_sentence det_Semcor_each dobj_match_s prep_to_match_counterpart dobj_match_Semcor aux_match_to xcomp_s_match nsubj_s_~ nn_~_step amod_~_first det_~_the amod_agreement_inter-annotator prep_of_extent_agreement det_extent_the dobj_determine_extent aux_determine_To xcomp_Matching_determine nsubj_Matching_Sentence num_Sentence_3 rcmod_paper_Matching det_paper_this prep_in_agreement_paper amod_agreement_inter-annotator dobj_investigating_agreement prepc_for_set_investigating vmod_data_set poss_data_our prep_as_serves_data nsubj_serves_verbs nsubj_serves_nouns nn_t_~ appos_annotators_t amod_annotators_human prep_of_groups_annotators amod_groups_independent num_groups_two agent_sense-tagged_groups auxpass_sense-tagged_been aux_sense-tagged_has nsubjpass_sense-tagged_pomon mark_sense-tagged_Since amod_pomon_common det_pomon_this dep_nouns_sense-tagged conj_or_nouns_verbs nn_nouns_English amod_nouns_currmg advmod_oc_frequently dep_191_oc conj_and_of.the_segmentartan rcmod_of.the_adopted rcmod_of.the_s rcmod_of.the_serves dep_of.the_191 num_of.the_one prep_into_Is_sentences nsubj_Is_segmentartan nsubj_Is_of.the det_sentence_each nn_sentence_m amod_sentence_sense-tagged cop_sentence_is nsubj_sentence_w dobj_sentence_which nn_w_occurrence nn_w_word det_w_a rcmod_m_differed rcmod_m_sentence rcmod_sentences_converted nn_sentences_corpus nn_sentences_Brown prep_of_consists_sentences advmod_consists_thus nsubj_consists_corpus nsubj_consists_intersection nn_corpus_DSO det_corpus_the nn_corpus_Semcor det_corpus_the conj_and_intersection_corpus prep_of_intersection_corpus det_intersection_The dep_sentence_consists det_sentence_each nn_sentence_m amod_sentence_sense-tagged cop_sentence_been aux_sentence_has nsubj_sentence_occurrence dep_sentence_which nn_occurrence_word num_occurrence_one rcmod_m_sentence nn_m_corpus nn_m_DSO det_m_the prep_in_sentences_m num_sentences_192,800 quantmod_192,800_about prep_of_total_sentences det_total_a dobj_s_total advmod_s_~ expl_s_There appos_sense_sentence num_sense_4 dep_sense_s prep_from_sense_WoRDNET det_sense_a det_sense_th rcmod_~_considered nn_~_w dobj_tagged_~ nsubj_tagged_s nn_s_~ nn_s_w nn_s_occurrence nn_s_word det_s_each auxpass_selected_are dep_selected_tagged csubjpass_selected_acknowledged amod_form_verb amod_form_reflectional amod_form_various poss_form_its dep_m_form conj_and_form_m amod_form_plural amod_form_singular nn_form_m nn_form_w conj_or_singular_plural dobj_containing_m dobj_containing_form vmod_sentences_containing num_verbs_70 conj_and_nouns_verbs num_nouns_121 dep_Enghsh_verbs dep_Enghsh_nouns prep_of_words_Enghsh amod_words_occurring num_words_191 advmod_occurring_frequently prep_of_hst_words det_hst_a nn_w_word det_w_each appos_Journal_sentences prep_from_Journal_hst prep_for_Journal_w nn_Journal_Street nn_Journal_Wall det_Journal_the amod_corpus_Brown det_corpus_the prep_from_drawn_corpus vmod_sentences_drawn conj_and_consists_Journal prep_of_consists_sentences nsubj_consists_corpus nn_corpus_DSO det_corpus_The rcmod_subset_Journal rcmod_subset_consists det_subset_this dep_m_subset nn_erbs_~ conj_and_nouns_adverbs conj_and_nouns_adjectives conj_and_nouns_erbs dep_words_m dep_words_adverbs dep_words_adjectives dep_words_erbs dep_words_nouns nn_words_content det_words_the prep_on_done_words auxpass_done_was nsubjpass_done_senses mark_done_wlth nn_taggmg_Sense dep_files_taggmg nn_files_text num_files_352 num_words_670,000 quantmod_670,000_than mwe_than_more prep_from_consists_files prep_of_consists_words nsubj_consists_us nn_senses_VoRDNET nn_senses_~ prep_with_tagged_senses amod_corpus_Brown det_corpus_the conj_and_subset_consists vmod_subset_tagged prep_of_subset_corpus det_subset_a nsubj_subset_us nn_us_corpus nn_us_Semcor det_us_The amod_annotators_human prep_of_groups_annotators amod_groups_separate num_groups_two rcmod_senses_consists rcmod_senses_subset prep_by_senses_groups prep_of_senses_WORDNET amod_senses_refined det_senses_the ccomp_tagged_done advmod_tagged_independently auxpass_tagged_been aux_tagged_has nsubjpass_tagged_which num_Ng_1997 appos_Ng_Ng conj_and_Ng_1996 conj_and_Ng_Lee appos_corpus_1996 appos_corpus_Lee appos_corpus_Ng nn_corpus_DSO det_corpus_the amod_Miller_1993 dep_Miller_al nn_Miller_et nn_corpus_Semcor nn_corpus_WORDNET det_corpus_the conj_and_intersection_tagged conj_and_intersection_selected conj_and_intersection_corpus dep_intersection_Miller prep_of_intersection_corpus det_intersection_the cop_intersection_is csubj_intersection_acknowledged det_corpus_This nn_corpus_Enghsh prep_of_nouns_corpus conj_and_nouns_verbs amod_nouns_occurring det_nouns_the advmod_occurring_frequently advmod_frequently_most prep_of_instances_verbs prep_of_instances_nouns num_instances_30,000 quantmod_30,000_than mwe_than_more prep_of_corpus_instances amod_corpus_sense-tagged amod_corpus_large det_corpus_a amod_annotators_human prep_of_rate_annotators nn_rate_agreement det_rate_the prep_on_comparing_corpus dobj_comparing_rate amod_agreement_raterannotator prep_of_ssue_agreement nn_ssue_~ det_ssue_the prepc_by_examine_comparing dobj_examine_ssue nsubj_examine_we prep_in_Study_this-paper nn_Study_Case det_Study_A num_Study_2 dep_Kllgamff_1998b conj_Kllgamff_Kflgarrlff conj_Kllgamff_1998a conj_Kllgamff_Kllgarnff conj_Kllgamff_1998c dep_senses_Study dep_senses_Kllgamff nn_senses_word det_senses_the dobj_taggrog_senses nn_lexicographers_ts nn_lexicographers_~ prep_among_agreement_lexicographers nn_agreement_raterannotator det_agreement_the dobj_find_agreement prt_find_out aux_find_to vmod_study_find amod_study_detaded det_study_a dobj_performed_study aux_performed_has nsubj_performed_exerclse nn_exerclse_SENSEVAL det_exerclse_the nn_algorithms_WSD prep_for_performance_instance prep_of_performance_algorithms det_performance_the dobj_compare_performance aux_compare_to amod_ceiling_upper det_ceiling_the xcomp_form_compare prep_against_form_whmh dobj_form_ceiling advmod_form_then aux_form_will nsubj_form_rate amod_annotators_human prep_between_rate_annotators nn_rate_agreement det_rate_The rcmod_annotators_form amod_annotators_human prep_between_agreement_annotators dobj_hlgh_agreement advmod_hlgh_reasonably nsubj_hlgh_us expl_hlgh_there prep_for_hlgh_which rcmod_task_hlgh nn_task_dusamblguatlon det_task_a dobj_define_task aux_define_to xcomp_prefer_define aux_prefer_would num_algorithms_One nn_algorithms_WSD prep_of_evaluatmn_algorithms det_evaluatmn_the prep_for_concern_evaluatmn amod_concern_Important det_concern_an dep_concern_us dep_assignment_concern nn_assignment_sense nn_assignment_word amod_annotators_human prep_between_rate_annotators nn_rate_agreement det_rate_The nn_rate_context nn_rate_m nn_rate_word det_rate_a nn_assignment_sense poss_assignment_their nn_assignment_m dep_dusagree_prefer prep_on_dusagree_assignment prep_to_dusagree_rate dobj_dusagree_assignment advmod_dusagree_genuinely aux_dusagree_may nsubj_dusagree_annotators amod_annotators_human num_annotators_two advmod_fine_Hence advmod_fine_rather cop_fine_be aux_fine_to xcomp_tend_fine nsubj_tend_WoRDNET dep_Miller_1990 appos_WoRDNET_Miller nn_WoRDNET_hke amod_WoRDNET_ctmnary nn_WoRDNET_~ nn_WoRDNET_d det_WoRDNET_a rcmod_m_tend nn_m_word amod_m_used det_m_a advmod_used_commonly prep_for_senses_m amod_senses_different det_senses_the prep_between_sense_senses dep_sense_dustmctmn det_sense_The dep_senses_sense prep_of_number_senses amod_number_large det_number_a dobj_have_number nsubj_have_which rcmod_words_have amod_words_used det_words_the advmod_used_commonly prep_for_so_words advmod_so_especmlly cop_so_is nsubj_so_This amod_This_comprehensive advmod_refined_rather auxpass_refined_be aux_refined_to conj_and_tend_so xcomp_tend_refined nsubj_tend_~ nn_~_dmtmnar amod_~_typical det_~_a rcmod_m_so rcmod_m_tend nn_m_word det_m_a prep_for_hst_m prep_of_hst_senses det_hst_the advmod_speak_Secondly aux_speak_to advmod_speak_so amod_way_different det_way_a npadvmod_way_m dep_way_repo amod_space_semantic det_space_the dobj_carve_space prt_carve_up aux_carve_to xcomp_tend_carve nn_ctlonanes_~ nn_ctlonanes_d amod_ctlonanes_Different nn_ctlonanes_word nn_ctlonanes_ven nn_ctlonanes_~ nn_ctlonanes_g det_ctlonanes_a nn_defimtmns_sense prep_of_set_defimtmns amod_set_same det_set_the dep_have_tend prep_for_have_ctlonanes dobj_have_set aux_have_will nsubj_have_dictionaries mark_have_that num_dictionaries_two det_dictionaries_any ccomp_case_have det_case_the advmod_case_rarely cop_case_is nsubj_case_sentence tmod_case_words dep_case_to advmod_~_rstly nn_~_F nn_~_assignment nn_~_sense nn_~_word prep_for_case_~ det_case_the neg_case_not dep_s_case dobj_s_us nsubj_s_~ det_~_th appos_sentence_it rcmod_sentence_s advmod_sentence_Unfortunately nn_sentence_ven nn_sentence_~ nn_sentence_g det_sentence_a nn_sentence_m amod_tags_part-of-speech amod_tags_assigning prep_on_agreement_tags prep_of_rate_agreement amod_rate_high det_rate_a xcomp_achieve_case dobj_achieve_rate aux_achieve_can nsubj_achieve_tory amod_annotators_human amod_annotators_independent nn_annotators_Tv.o nn_annotators_mformatlon nn_annotators_structure nn_annotators_ntactm amod_annotators_s amod_annotators_partof-speech det_annotators_th nn_annotators_~ nn_annotators_w amod_annotators_annotated nn_annotators_texts conj_and_partof-speech_s prep_of_tory_annotators nn_tory_~ dep_repo_achieve amod_repo_large nn_repo_t nn_~_ide nn_~_~ nn_~_pro num_Marcus_1993 dep_Marcus_al nn_Marcus_et advmod_corpus_~ appos_corpus_Marcus nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_for_word_instance conj_or_word_sentence det_word_a prep_to_assigned_sentence prep_to_assigned_word nn_structure_syntactic nn_structure_speech conj_or_speech_syntactic vmod_part_assigned prep_of_part_structure dep_part_correct det_correct_the dobj_have_part aux_have_to xcomp_means_have nsubj_means_it dobj_means_what prepc_of_cnterm_means amod_cnterm_agreed-upon dep_defined_taggrog ccomp_defined_performed parataxis_defined_dusagree dep_defined_hst dep_defined_speak dep_defined_way conj_and_defined_corpus conj_and_defined_cnterm advmod_defined_well advmod_defined_relatlvely dep_are_corpus dep_are_cnterm dep_are_defined expl_are_there nn_parsing_ntactm nn_parsing_s conj_or_tagging_parsing amod_tagging_part-of-speech dobj_hke_parsing dobj_hke_tagging nsubj_hke_subtasks mark_hke_For nn_subtasks_language amod_subtasks_natural nn_translation_machine conj_and_retrieval_translation nn_retrieval_information parataxis_hke_examine parataxis_hke_are advcl_hke_hke dobj_hke_translation dobj_hke_retrieval nsubj_hke_Impact aux_hke_have dep_hke_v nsubj_hke_WSD mark_hke_in prep_on_Impact_apphcatlons amod_Impact_slgmficant advmod_have_ill rcmod_Advances_hke prep_with_dealt_Advances advmod_dealt_effectively auxpass_dealt_be aux_dealt_must nsubjpass_dealt_samblguatmn mark_dealt_that nn_context_m nn_context_word det_context_a prep_of_meamng_context det_meamng_the dobj_dlscermng_meamng nn_sense_word nn_sense_samblguatmg nn_sense_~ nn_sense_d conj_or_problem_dlscermng prep_of_problem_sense det_problem_the amod_matching_keyword amod_matching_simple prep_beyond_language_matching amod_language_natural dobj_process_language conj_and_understand_process aux_understand_to xcomp_able_process xcomp_able_understand cop_able_be aux_able_to nsubj_able_computers mark_able_for dep_able_order dep_able_In nn_processing_language amod_processing_natural nn_processing_m nn_processing_problem amod_processing_central det_processing_a dep_processing_us appos_samblguatmn_dlscermng appos_samblguatmn_problem dep_samblguatmn_able dep_samblguatmn_processing appos_samblguatmn_WSD nn_samblguatmn_~ nn_samblguatmn_d nn_samblguatmn_sense nn_samblguatmn_word ccomp_acknowledged_dealt advmod_acknowledged_widely nsubj_acknowledged_us nsubj_acknowledged_It
W99-0508	J96-2004	o	~ eqmvalent ot duty in a parallel French text the correct sense of the Enghsh word is identified These studies exploit th ~ s lnformatmn m order to gather co-occurrence data for the different senses which ts then used to dtsamb ~ guate new texts In related work Dywk -LRB- 1998 -RRB- used patterns of translational relatmns in an EnghshNorwegian paralle corpus -LRB- ENPC Oslo Umverslty -RRB- to define semantic propemes such as synonymy ambtgmty vagueness and semantic helds and suggested a derivation otsemantic representations for signs -LRB- eg lexemes -RRB- captunng semantm relatmnshlps such as hyponymy etc fiom such translatmnal relatmns Recently Resnlk and Yarowsky -LRB- 1997 -RRB- suggested that fol the purposes ot WSD the different senses of a wo ~ d could be detelmlned by considering only sense d ~ stmctmns that are lextcahzed cross-hngmstlcally In particular they propose that some set of target languages be ~ dent ~ fied and that the sense d ~ stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages This idea would seem to p ~ ovtde an answer at least m part to the problem of determining different senses of a word mtumvely one assumes that ff another language lexlcahzes a word m two or more ways there must be a conceptual monvatmn If we look at enough languages we would be likely to fred the s ~ gmficant lexlcal differences that dehmtt different senses of a word However th ~ s suggestmn raises several questions Fo ~ instance ~ t ~ s well known that many amb ~ gumes are preserved across languages -LRB- for example the French tntdrYt and the Enghsh interest -RRB- especmlly languages that are relatively closely related Assuming this problem can be overcome should differences found m closely related languages be given lesser -LRB- or greater -RRB- weight than those found m more distantly related languages 9 More generally which languages should be considered for this exermse 9 All languages 9 Closely related languages9 Languages from different language famlhes ' ~ A mixture of the two 9 How many languages and of which types would be enough to provide adequate lnfotmanon tot this purpose ~ There ts also the questmn ot the crlterm that would be used to estabhsh that a sense distinction is lexlcahzed cross-hngu ~ stmally How consistent must the d ~ stlnCtlOn be 9 Does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages or need tt only be the case that the option ot a different lexlcahzatlon exists m a certain percentage of cases 9 Another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from Using bdmgual dictionaries would be extremely tedmus and error-prone g ~ ven the substantial d ~ vergence among d ~ ctlonanes in terms of the kinds and degree of sense dlstmctmns they make Resmk and Yalowsky -LRB- 1997 -RRB- suggest EutoWordNet -LRB- Vossen 1998 -RRB- as a possible somce of mformatmn but given that EuroWordNet ts pttmatdy a lexmon and not a corpus ~ t is subject to many of the same objections as for bl-hngual dictionaries An alternative would be to gather the reformation from parallel ahgned corpma Unlike bilingual and muttt-hngual dictionaries translatmn eqmvalents xn parallel texts a ~ e determined by experienced translatols who evaluate each instance ot a word 's use m context rather than as a part of the meta-hngmst ~ c actlvlty of classifying senses for mclusmn in a dictionary However at present very few parallel ahgned corpora exist The vast majority ot these are bl-texts mvolwng only two languages one of which is very often English Ideally a serious 53 evaluation of Resnik and Yarowsky 's proposal would include parallel texts m languages from several different language families and to maximally ensure that the word m question is used in the exact same sense across languages ~ t would be preferable that the same text were used over all languages in the study The only currently avadable parallel corpora for more than two languages are Olwell 's Nmeteen Eighty-Four -LRB- Erjavec and Ide 1998 -RRB- Plato 's Repubhc -LRB- Erjavec et al 1998 -RRB- the MULTEXT Journal o / the Commt ~ ston corpus -LRB- Ide and V6roms 1994 -RRB- and the Bible -LRB- Resnlk et al m press -RRB- It is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions Also ~ t Is not clear how the lexlcahzatlon of sense distractions across languages Is affected by genre domain style etc Thls paper attempts to provide some prehmlnary answers to the questions outhned above In order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions and ff so the ways in which th ~ s reformation might be used Given the lack of lalge parallel texts across multiple languages the study is necessarily hmlted however close exammanon of a small sample of parallel data can as a first step provide the basis and dlrectmn for more extensive studies 1 Methodology I have conducted a small study using parallel aligned versmns ot George Orwell 's Nineteen Etghtv-Fo lr -LRB- Euavec and Ide 1998 -RRB- m five languages Enghsh Slovene Estonian Romanlan and Czech I The study therefole Involves languages from four language families The O ~ well parallel corpus also includes vers | ons o -RRB- Ntneteen-E ~ gho Four m Hungarian Bulgarmn Latwan Llthuaman Se ~ bmn and Russmn -LRB- Germanic Slavic Fmno-Ugrec and Romance -RRB- two languages from the same family -LRB- Czech and Slovene -RRB- as well as one non-Indo-European language -LRB- Estoman -RRB- Nmeteen Eighty-Four Is a text of about 100,000 words translated directly from the original English m each of the other languages The parallel versions of the text are sentence-aligned to the English and tagged for part of speech Although Nineteen Eighty-Four is a work of fiction Orwell 's prose IS not highly stylized and as such it provides a reasonable sample ot modern ordinary language that ~ s not tied to a given topic or sub-domain -LRB- such as newspapers technical reports etc -RRB- Furthermore the translations of the text seem to be relatively faithful to the original for instance over 95 % ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one -LRB- Prlest-Dorman et al 1997 -RRB- Nine ambiguous English words were considered hard head country hne promise shght seize scrap float The first four were chosen because they have been used in other dlsamb ~ guatlon studies the latter five were chosen from among the words used m the Senseval dlsamblguatlon exercise -LRB- Kllgamff and Palmer forthcoming -RRB- In all cases the study was necessarily hmlted to words that occurred frequently enough in the Orwell text to warrant consideration F ~ ve hundred forty-two sentences conta | nmg an occurrence or occurrences -LRB- Including morphological variants -RRB- of each of the nine words were extracted from the Enghsh text together w ~ th the parallel sentences m which they occur m the texts ot the four comparison languages -LRB- Czech Estonian Romantan Slovene -RRB- As Walks and Stevenson -LRB- 1998 -RRB- have pointed out pa ~ t-of-speech tagging accomplishes a good portion of the work ot semantic dlsamb ~ guatmn therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 The Enghsh occurrences were then grouped usmg the sense distinctions m WordNet -LRB- version 1 6 -RRB- \ -LSB- Miller et al 1990 Fellbaum 1998 \ -RSB- -RRB- The sense categonzatmn was performed by the author and two student assistants results from the three were compared and a final mutually agreeable set of sense assignments was estabhshed For each of the four comparison languages the corpus of sense-grouped parallel sentences were sent to a llngmst and natl ve speaker of the comparison language The hngmsts were asked to provide the lexlcal item m each parallel sentence that corresponds to the ambiguous Enghsh word If inflected they were asked to provide both the inflected form and the root form In addttmn the lmgmsts were asked to indicate the type of translatmn according to the dtstmctmns given m Table 1 For over 85 % of the Enghsh word occurrences -LRB- corresponding to types 1 and 2 m Table 1 -RRB- a specific lexlcal item or items could be identified as the translation equivalent for the corresponding Enghsh word For comparison purposes each translanon equivalent was represented by ~ ts lemma -LRB- or the lemma of the toot form in the case of derivatives -RRB- and associated w ~ th the WordNet sense to which it corresponds In order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents a coherence index -LRB- Cl -RRB- was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which a g ~ ven se ls z ~ s translated with the same word ~ Note that the z The adJective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used m the study Note that the CI ~ s similar to semanuc entropy -LRB- Melamed 1997 -RRB- However Melamed computes CIs do not determine whether or not a sense dtstmctton can be lextcahzed in the target language but only the degree to whmh they are lexicahzed differently m the translated text However tt can be assumed that the CIs provide a measure of the tendency to lex ~ cahze different WordNet senses differently which can m turn be seen as an mdtcatmn of the degree to which the distraction ts vahd For each ambiguous word the CI Is computed for each pair of senses as follows S <qt> Cl -LRB- sqS -RRB- = ' = 1 m rnrt where @ n ~ s the number of comparison languages under consideration nl ~ q and m are the nt ~ mber of occurrences olsense sqand sense s ~ m the Enghsh corpus respectively including occurrences that have no idenufiable translation s < ~ ~ > m ts the number of times that senses q and r are translated by the same lex ~ cal Item m language t i e x = y t ~ tJan ~ -LRB- q -RRB- r ~ oan ~ -LRB- r -RRB- The CI ts a value between 0 and 1 computed by examining clusters of occurrences translated by the same word In the othel languages If sense and sense -RRB- are consistently translated w ~ th the same wo ~ d in each comparison language then Cl -LRB- s s ~ -RRB- = 1 if they are translated with a different word m every occurrence Cl -LRB- s ~ -RRB- = 0 In general the CI for pans of different senses provides an index of thmr relatedness t e the greater the value of Cl -LRB- s sj -RRB- the more frequently occurrences of-sense t and sense j are translated with the same lextcal item When t = j we entropy tOl wold types lather than word senses 55 obtain a measure of the coherence of a ~ lven sense Type Meaning 1 A slngle lexlcal Item is used to translate the En@izsh equivalent -LRB- possibly a 2 The English word is translated by a phrase of two or more words or a compound meaning as the slngle English word 3 The En@izsh word is not lexzcalized in the translation 4 A pronoun is substituted for the English word In the translation An English phrase contalnmng the ambiguous word Is translated by a single language which has a broader or more specific meanlng or by a phrase in whl corresponding to the English word Is not explicltl ~ lexlcallzed Table 1 Translation types and their trequencles % dizen whl % h h 6 % 6 % 6 % of s p same Word # Description hard 1 1 difficult 2 head i i i 1 Table 2 1 2 _ meta ~ horlcally hard _ \ -RSB- 3 not yielding to pressure 1 4 very strong or ~ lgorous ar 2 I wlth force or vigor -LRB- adv -RRB- 3 earnestly intently -LRB- adv -RRB- i _ ~ art of the body 3 intellect 4 _ r ~ le _ r ch % ef 7 front front part WoldNet senses ot hard and head CIs were also computed for each language individually as well as for different language groupings Romaman Czech and Estonian -LRB- three different language families -RRB- Czech and Slovene -LRB- same family -RRB- Romaman Czech Slovene -LRB- Indo-European and Estonian -LRB- nonIndo-European -RRB- To better visualize the relationship between senses a hierarchical clustering algorithm was applied to the CI data to generate trees reflecting sense proximity 4 Finally in order to determine the degree to which the linguistic relaUon between languages may affect coherence a correlation was run among CIs for all pairs of the four target languages Fol example Table 2 gives the senses of hard and head that occurred in the data s The CI data s ` sobS ' hard and head are given in Tables 3 and 4 ~ uous CIs measuring the aff mty of a sense with itself -- that is the tendency for all occurrences of that sense to be translated wlth the same word -- show that all of the s x senses of ha d have greatel internal consistency tfian athmty with other senses with senses 1 1 -LRB- dlff | cult CI = 56 -RRB- and 13 -LRB- ` not soft ci = 63 -RRB- registenng the h ghest internal consistency 6 The same holds true for three of the four senses of head while the CI for senses 1 3 -LRB- Intellect -RRB- and 1 1 -LRB- part of the body -RRB- is higher than the CI for 13/1 3 WordNet Sense 2 1 2 3 1 4 1 3 1 1 1 2 21 23 1 4 13 0 50 o 13 i ool 0 O0 0 25 i O0 0 04 0 50 0 17 0 56 0 19 0 00 0 00 0 00 0 00 0 00 0 25 0 21 Table 3 CIs for hard I i 12 0 ,63 0 00 0 50 2 Results Although the data sample is small It gives some insight into ways m which a larger sample might contribute to sense discrimination 4 Developed by Andleas Stolcke Results tor all words m the study are avadable at http / / www cs vassar edu / ~ ~ de/wsd/cross-hng html 6 Senses 2 3 and 1 4 have CIs ot 1 because each ot these senses exists m a single occurrence m the corpus and have theretote been dlscarded horn consideration ot CIs to ~ individual senses We a ~ e currently mvesugatmg the use oI the Kappa staUst ~ c -LRB- Carletta 1996 -RRB- to normahze these sparse data 56 WordNet Sense 1 1 1 3 1 4 1 7 1 1 0 69 1 3 0 53 0 45 1 4 0 12 0 07 0 50 1 7 0 40 0 001 0 00 1 00 Table 4 CIs for head Figure 2 shows the sense clusters for hard generated from the CI data 7 The senses fall into two mare clusters w ~ th the two most internally consistent senses -LRB- 1 1 and 1 3 -RRB- at the deepest level of each ot the respecuve groups The two adverbml forms 8 are placed in separate groups leflectmg thmr semantic proximity to the different adjecuval meanings of hard The clusters for head -LRB- Figure 2 -RRB- stmdarly show two dlstmct groupings each anchored in the two senses with the h ~ ghest internal consistency and the lowest mutual CI -LRB- part of the body -LRB- 1 1 -RRB- and ruler chief -LRB- 1 4 -RRB- -RRB- The h ~ erarchtes apparent m the cluster graphs make intuitive sense Structured hke dictmnary enmes the clusters for hard and head might appeal as m F ~ gure 1 This ts not dissimilar to actual dlctLonary entries for hard and head for example the enmes for hard in four differently constructed dlctmnanes -LRB- Colhns Enghsh -LRB- CED -RRB- Longman 's -LRB- LDOCE -RRB- OxJotd Advanced Learner 's -LRB- OALD -RRB- and COBUILD -RRB- all hst the 'd ~ fficult and not soft senses first and second whmh since most dictionaries hst the most common Ol frequently used senses hrst reflects the gross dlwslon apparent m the clusters Beyond this ~ t ~ s difficult to assess the 7 Foi the purposes ot the cluster analys ~ s CIs of l 00 resulting from a single occurrrence were normahzed to 5 8 Because ~ oot to ms were used m the analysis no dzstlncUon m UanslaUon eqmvalents was made tor part ot speech correspondence between the senses In the dictionary entries and the clusters The remamlng WordNet senses are scattered at various places within the entries or m some cases split across various senses The h ~ erarchlcal relatmns apparent m the clusters are not reflected m the d ~ cttonary enmes smce the senses are for the most part presented in flat hnear hsts However It is interesting to note that the first five senses of hard In the COBUILD d ~ cuonary which is the only d ~ cttonary in the group constructed on the bas ~ s of colpus examples 9 and presents senses m ruder of frequency correspond to hve of the six WordNet senses in thls study WordNet 's metaphorically hard is spread over multiple senses in the COB UILD as it.is In the other d ~ ctlonarles HARD HEAD I 1 dlfflcult 2 vlgorously II 1 a not soft b strong 2 a earnestly b metaphorlcally hard I 1 a part of the body b zntellect 2 front front part II ruler chlef Flgme 1 Clusteis tol hard and head suuctured as dlcuonary entt ~ es The results tor dlftment language groupings show that the tendency to lextcahze senses differently is not aftected by language d ~ stance -LRB- Table 5 -RRB- In fact the mean CI fol Estonian the only non-Indo-European language m the study ~ s lower than that for any other group mdmatmg that WordNet sense dtstmctmns are slightly less hkely to be lexlcahzed differently m Estonian 9 Edmons ot the LDOCE -LRB- 1987 vexsmn -RRB- and OALD -LRB- 1985 version -RRB- dictlonalles consulted m this study ple-date edmons ol those same d ~ ctlonanes based on colpus evidence 57 Correlations of CIs for each language pair -LRB- Table 5 -RRB- also show no relationship between the degree to which sense d ~ stmcuons are lexlcahzed differently and language distance This is contrary to results obtained by Resmk and Yarowsky -LRB- subm tted -RRB- who using a memc slmdar to the one used in this study found that that non-Indo-European languages tended to lexlcallze English sense d ~ stmctlons more than Indo-European languages especially at finergrained levels However their translation data was generated by native speakers presented with Isolated sentences in English who were asked to provide the translation for a given word In the sentence It is not clear how this data compares to translations generated by trained translators working with full context Lanquaqe qroup Averaqe CI ALL 0 27 RO/ES/SL 0 28 SL/CS 0 28 RO/SL/CS 0 27 ES 0 26 Table 5 Average CI values Lanqs Hard Country Llne Head Ave ES/CS 0 86 0 72 0 68 0 69 0 74 RO/SL 0 73 0 78 0 68 1 00 0 80 RO/CS 0 83 0 66 0 67 0 72 0 72 SL/CS 0 88 0 51 0 72 0 71 0 71 RO/ES 0 97 0 26 0 70 0 98 0 73 ES/SL 0 73 0 59 0 90 0 99 0 80 Table 6 CI correlauon tor the tour target languages I I I I I m ~ nlmum dlstance = 0 249399 m ~ nlmum d ~ stance = 0 434856 mlnlmum dlstance = 0 555158 mlnlmum dlstance = 0 602972 m ~ nlmum dlstance = 0 761327 I > 21 I > ii I > 23 l > 13 l > 14 I > 12 -LRB- 13 -RRB- -LRB- 23 -RRB- -LRB- 12 -RRB- -LRB- 1,4 -RRB- -LRB- ii -RRB- -LRB- 21 -RRB- -LRB- 1412 -RRB- -LRB- 2313 -RRB- -LRB- 2 3 1 3 1 4 1 2 -RRB- -LRB- 2 111 -RRB- Figure 2 Cluster tree and distance measures tor the sm senses of hard I > 14 i I > i i I -- 1 J > i 3 I > 17 mlnlmum dlstance = 0 441022 mlnlmum dlstance = 0 619052 mln ~ mum dlstance = 0 723157 -LRB- 13 -RRB- -LRB- ll -RRB- -LRB- 17 -RRB- -LRB- 1113 -RRB- -LRB- 111317 -RRB- -LRB- 14 -RRB- F gure 3 Cluster tree and dmtance measures tot the tout senses ot head 58 Conclusion The small sample m this study suggests that cross-hngual lexlcahzat ~ on can be used to define and structure sense d ~ stmct ~ ons The cluster graphs above provide mformat ~ on about relations among WordNet senses that could be used for example to determine the granularity of sense differences whtch m turn could be used in tasks such as machine translatton mtormaUon retrieval etc For example it is hkely that as sense dtstmcttons become finer the degree of error ~ s less severe Resmk and Yarowsky -LRB- 1997 -RRB- suggest that confusing freer-grained sense dtstmctlons should be penahzed less severely than confusing grosser d ~ stmct ~ ons when evaluatmg the performance of sense dtsambtguatt0n systems The clusters also provide insight into the lexlcallzatlon of sense dtstmcttons related by various semantic relations -LRB- metonymy meronymy etc -RRB- across languages for instance the part of the body and intellect senses of head are lex ~ cahzed with the same ~ tem a s ~ gnlficant portion of the t ~ me across all languages reformation that could be used m machine translatton In addtt ~ on cluster data such as that presented here could be used m lexicography to determine a mole detaded hierarchy of relations among senses in dtct ~ onary entries It is less clear how cross-hngual reformation can be used to determine sense d ~ st ~ nctlons independent of a pre-deflned set such as the WordNet senses used here In an effort to explore how thts mlght be done I have used the small sample from thts study to create word groupmgs from back translations -LRB- l e additional translations m the original language ot the translations m the target language -RRB- and developed a metric that uses th ~ s mformatton to determine relatedness between occurrences whtch ~ s m turn used to cluster occurrences into sense groups I have also compared sets of back translations for words representing the various WordNet senses which provtde word groups s ~ mdar to WordNet synsets Interestingly there ts virtually no overlap between the WordNet synsets and word groups generated from back translations The results show however that sense dlstmctlons useful for natural language processing tasks such as machme translanon could potentsally be determined ot at least influenced by constdeHng this mformatton The automatically generated synsets themselves may also be useful m the same apphcatlons where WordNet synsets -LRB- and ontologtes -RRB- have been used tn the past More work needs to be done on the topic of cross-hngual sense determination utthzmg substantially larger parallel corpora that include a variety ot language types as well as texts fiom several genres This small study explores a possible methodology to apply when such resources become avatlable Acknowledgements The author would hke to gratefully acknowledge the contrtbut ~ on of those who provided the translatton mfotmat ~ on Tomaz Eua ~ ec -LRB- Slovene -RRB- Kadrt Muxschnek -LRB- Estonian -RRB- Vladtmlr Petkevtc -LRB- Czech -RRB- and Dan Tubs -LRB- Romanlan -RRB- as well as Dana Fleut and Darnel Khne who helped to transcrtbe and evaluate the data Special thanks to Dan Melamed and Hlnrtch Schutze for their helpful comments 59 \ -LSB- \ -RSB- \ -LSB- \ -RSB- in \ -LSB- \ -RSB- in i i Hg nn i an i am References Ca ~ letta Jean -LRB- 1996 -RRB- Assessing Agreement on Classthcatton Tasks The Kappa Stat ~ st ~ t. Computational Lmgulstlcs 22 -LRB- 2 -RRB- 249-254 Dagan Ido and Ita ~ Alon -LRB- 1994 -RRB- Wo ~ d sense dlsambxguat ~ on using a second language monohngual corpus Computattonal Ltngmsttcs 20 -LRB- 4 -RRB- 563-596 Dagan Ido Ital Alon and Schwall Ulnke -LRB- 1991 -RRB- Two languages a ~ e more mformattve than one Proceedings of the 29th Annual Meettng of the Assoctatton for Computattonal Ltngutsttcs 18-21 June 1991 Berkeley Cahfornm 130-137 Dyvtk Helge -LRB- 1998 -RRB- Translations as Semantic Mirrors Proceedmgs of Workshop W13 Multzlmguahty in the Lextcon II The 13th Biennial European Conference on Arttftctal lntelhgence -LRB- ECA198 -RRB- Brighton UK 24-44 Eqavec Tomaz and Ide Nancy -LRB- 1998 -RRB- The MULTEXT-EAST Corpus Proceedlng ~ of the Fltst International Conference on Language Resources and Evaluatton 27-30 May 1998 Granada 971-74 Erjavec Tomaz Lawson Ann and Romary Laurent -LRB- 1998 -RRB- East meets West Producing Multflmgual Resources m a European Context Pioceedtngs of the Ftrst Internattonal Conference on Language Resources and Evaluation 27-30 May 1998 Gtanada 981-86 Fellbaum Chttstmne -LRB- ed -RRB- -LRB- 1998 -RRB- WordNet An Electrontc Lexlcal Database MIT Press Cambridge Massachusetts Gale Wdham A Church Kenneth W and Yatowsky Davtd -LRB- 1993 -RRB- A method tor dlsamblguatmg word senses m a large cmpus Computers and the Humamtles 26 415-439 Hearst M'attl A -LRB- 1991 -RRB- Noun homograph ' dlsamblguatlon using local ' ~ ' 0ntext m large corpora Proceedtngs of the 7th Annual Conference of the Umver ~ lt ~ of Waterloo Centre for the New OED and Text ReaeaJch Oxford Umted Kingdom 1-19 Ide Nancy and V61oms Jean -LRB- 1998 -RRB- Word sense d ~ samb ~ guat ~ on The state of the alt Computational Lmgut ~ ttc ~ 24 1 1-40 Kdgar ~ ttt Adam and Palmer Ma ~ tha Eds -LRB- forthcoming -RRB- Proceedmgs ot the Senseval Word Sense D ~ samb ~ guatlon Workshop Specml double ~ ssue otComputer ~ and the Humamttes 33 4-5 Leacock Claudia Towell Geoffrey and Voorhees Ellen -LRB- 1993 -RRB- Corpus-based stattstlcal sense resolution Proceedtng ~ of the ARPA Human Language Technology Worsl ~ shop San Francisco Morgan Kautman Melamed I Dan -LRB- 1997 -RRB- Measuring Semantic Entropy ACL-SIGLEX Workshop Taggmg Tert wtth Lextcal Semanttcs Why What and How ~ April 4-5 1997 Washington D C 41-46 Mtllet George A Beckwlth Richard T Fellbaum	nn_Fellbaum_T nn_Fellbaum_Richard nn_A_George num_Mtllet_41-46 nn_C_D appos_April_Fellbaum appos_April_Beckwlth appos_April_A appos_April_Mtllet appos_April_C appos_April_Washington num_April_1997 num_April_4-5 dep_~_April advmod_~_How conj_and_What_~ dep_Why_~ dep_Why_What dep_Semanttcs_Why amod_Semanttcs_Lextcal dobj_wtth_Semanttcs nsubj_wtth_~ nsubj_wtth_Voorhees nsubj_wtth_Geoffrey nsubj_wtth_Towell nsubj_wtth_Claudia nsubj_wtth_Leacock nsubj_wtth_Humamttes nn_Tert_Taggmg nn_Tert_Workshop nn_Tert_ACL-SIGLEX nn_Tert_Entropy nn_Tert_Semantic amod_Tert_Measuring dep_Tert_1997 nn_Tert_Dan dep_Tert_I nn_Melamed_Kautman nn_Melamed_Morgan nn_Francisco_San appos_shop_Tert appos_shop_Melamed appos_shop_Francisco nn_shop_~ nn_shop_Worsl nn_shop_Technology nn_shop_Language nn_shop_Human nn_shop_ARPA det_shop_the prep_of_~_shop nn_~_Proceedtng nn_~_resolution nn_~_sense amod_~_stattstlcal amod_~_Corpus-based nn_~_Ellen appos_Ellen_1993 num_Leacock_4-5 number_4-5_33 conj_and_Humamttes_~ conj_and_Humamttes_Voorhees conj_and_Humamttes_Geoffrey conj_and_Humamttes_Towell conj_and_Humamttes_Claudia conj_and_Humamttes_Leacock det_Humamttes_the nn_~_otComputer nn_~_ssue nn_~_~ amod_~_double nn_~_Specml conj_and_Workshop_wtth conj_and_Workshop_~ nn_Workshop_guatlon nn_Workshop_~ nn_Workshop_samb num_Workshop_~ nn_Workshop_D nn_Workshop_Sense nn_Workshop_Word nn_Workshop_Senseval det_Workshop_the dobj_ot_wtth dobj_ot_~ dobj_ot_Workshop nsubj_ot_Proceedmgs dep_Proceedmgs_forthcoming nn_Proceedmgs_Eds num_tha_~ nn_tha_Ma nn_ttt_~ amod_ttt_Kdgar num_ttt_1-40 number_1_24 nn_~_ttc nn_~_~ nn_~_Lmgut amod_~_Computational nn_~_alt det_~_the prep_of_state_~ det_state_The prep_on_~_state nn_~_guat nn_~_~ nn_~_samb num_~_~ nn_~_d nn_~_sense nn_~_Word nn_~_Jean appos_Jean_1998 num_Ide_1-19 nn_Kingdom_Umted rcmod_ReaeaJch_ot conj_and_ReaeaJch_tha conj_and_ReaeaJch_Palmer conj_and_ReaeaJch_Adam conj_and_ReaeaJch_ttt conj_and_ReaeaJch_1 conj_and_ReaeaJch_~ conj_and_ReaeaJch_V61oms conj_and_ReaeaJch_Nancy conj_and_ReaeaJch_Ide conj_and_ReaeaJch_Kingdom conj_and_ReaeaJch_Oxford dobj_Text_tha dobj_Text_Palmer dobj_Text_Adam dobj_Text_ttt dobj_Text_1 dobj_Text_~ dobj_Text_V61oms dobj_Text_Nancy dobj_Text_Ide dobj_Text_Kingdom dobj_Text_Oxford dobj_Text_ReaeaJch nn_OED_New det_OED_the nn_Centre_Waterloo prep_of_~_Centre nn_~_lt nn_~_~ nn_~_Umver det_~_the prep_for_Conference_OED prep_of_Conference_~ amod_Conference_Annual amod_Conference_7th det_Conference_the prep_of_Proceedtngs_Conference nn_Proceedtngs_corpora amod_Proceedtngs_large nn_Proceedtngs_m nn_Proceedtngs_0ntext nn_Proceedtngs_l nn_Proceedtngs_translations dep_~_May dobj_using_local vmod_dlsamblguatlon_using nn_dlsamblguatlon_homograph num_dlsamblguatlon_26 nn_homograph_Noun nn_homograph_A appos_A_1991 nn_A_M'attl conj_26_Hearst conj_26_415-439 appos_Humamtles_dlsamblguatlon det_Humamtles_the conj_and_Computers_Humamtles nn_Computers_cmpus amod_Computers_large det_Computers_a dep_m_Humamtles dep_m_Computers nn_m_senses nn_m_word nn_m_dlsamblguatmg nn_m_tor nn_m_method det_m_A dep_m_1993 dep_m_Davtd dep_m_Yatowsky conj_and_W_m nn_W_Kenneth nn_A_Wdham nn_Gale_Massachusetts appos_Press_m appos_Press_W appos_Press_Church appos_Press_A appos_Press_Gale appos_Press_Cambridge nn_Press_MIT nn_Press_Database nn_Press_Lexlcal nn_Press_Electrontc det_Press_An dep_WordNet_Press nn_WordNet_Chttstmne appos_Chttstmne_1998 appos_Chttstmne_ed num_Fellbaum_981-86 appos_May_WordNet appos_May_Fellbaum appos_May_Gtanada num_May_1998 num_May_27-30 conj_and_Resources_Evaluation nn_Resources_Language prep_on_Conference_Evaluation prep_on_Conference_Resources nn_Conference_Internattonal nn_Conference_Ftrst det_Conference_the appos_Pioceedtngs_~ prep_of_Pioceedtngs_Conference nn_Pioceedtngs_Context amod_Pioceedtngs_European det_Pioceedtngs_a dep_m_Pioceedtngs nn_m_Resources nn_m_Multflmgual nn_m_Producing nn_m_West dobj_meets_m nsubj_meets_East nn_East_Laurent appos_Laurent_1998 num_Erjavec_971-74 conj_and_May_Romary appos_May_Ann appos_May_Lawson appos_May_Tomaz appos_May_Erjavec appos_May_Granada num_May_1998 num_May_27-30 conj_and_Resources_Evaluatton nn_Resources_Language prep_on_Conference_Evaluatton prep_on_Conference_Resources nn_Conference_International nn_Conference_Fltst det_Conference_the prep_of_~_Conference nn_~_Proceedlng nn_~_Corpus nn_~_MULTEXT-EAST det_~_The nn_~_Nancy appos_Nancy_1998 num_Eqavec_24-44 conj_and_Brighton_Ide conj_and_Brighton_Tomaz conj_and_Brighton_Eqavec appos_Brighton_UK rcmod_lntelhgence_meets appos_lntelhgence_Romary appos_lntelhgence_May appos_lntelhgence_~ appos_lntelhgence_Ide appos_lntelhgence_Tomaz appos_lntelhgence_Eqavec appos_lntelhgence_Brighton appos_lntelhgence_ECA198 amod_lntelhgence_Arttftctal prep_on_Conference_lntelhgence amod_Conference_European amod_Conference_Biennial amod_Conference_13th det_Conference_The nn_II_Lextcon det_II_the nn_Multzlmguahty_W13 nn_Multzlmguahty_Workshop prep_of_Proceedmgs_Multzlmguahty nn_Proceedmgs_Mirrors nn_Proceedmgs_Semantic nn_Translations_Helge appos_Helge_1998 num_Dyvtk_130-137 prep_in_June_II prep_as_June_Proceedmgs appos_June_Translations appos_June_Dyvtk appos_June_Cahfornm appos_June_Berkeley num_June_1991 num_June_18-21 nn_Ltngutsttcs_Computattonal prep_for_Assoctatton_Ltngutsttcs det_Assoctatton_the prep_of_Meettng_Assoctatton amod_Meettng_Annual amod_Meettng_29th det_Meettng_the appos_Proceedings_Conference appos_Proceedings_June prep_of_Proceedings_Meettng num_Proceedings_one prep_than_mformattve_Proceedings advmod_mformattve_more dep_mformattve_e dep_mformattve_~ dep_mformattve_a amod_languages_mformattve num_languages_Two nn_languages_Ulnke dep_Ulnke_1991 conj_and_Ido_Schwall conj_and_Ido_Alon conj_and_Ido_Ital num_Dagan_563-596 num_4_20 appos_Ltngmsttcs_languages appos_Ltngmsttcs_Schwall appos_Ltngmsttcs_Alon appos_Ltngmsttcs_Ital appos_Ltngmsttcs_Ido appos_Ltngmsttcs_Dagan appos_Ltngmsttcs_4 nn_Ltngmsttcs_Computattonal nn_Ltngmsttcs_corpus amod_Ltngmsttcs_monohngual nn_Ltngmsttcs_language amod_Ltngmsttcs_second det_Ltngmsttcs_a dobj_using_Ltngmsttcs nn_~_dlsambxguat nn_~_sense nn_~_d prepc_on_~_using dobj_~_~ aux_~_Wo nsubj_~_Lmgulstlcs appos_Alon_1994 appos_~_Alon nn_~_Ita conj_and_Dagan_~ conj_and_Dagan_Ido num_Dagan_249-254 appos_22_2 appos_Lmgulstlcs_~ appos_Lmgulstlcs_Ido appos_Lmgulstlcs_Dagan appos_Lmgulstlcs_22 amod_Lmgulstlcs_Computational ccomp_t._~ nn_~_st num_~_~ nn_~_Stat nn_~_Kappa det_~_The nn_~_Tasks nn_~_Classthcatton prep_on_Agreement_~ amod_Agreement_Assessing nn_Agreement_Jean appos_Jean_1996 dep_letta_t. appos_letta_Agreement dobj_~_letta aux_~_Ca nsubj_~_References dep_References_am nn_References_i det_References_an rcmod_i_~ nn_i_nn nn_i_Hg nn_i_i nn_i_i dep_\_\ prep_in_\_i dep_\_\ prep_in_\_\ dep_\_\ number_\_59 dep_comments_\ num_comments_\ amod_comments_helpful poss_comments_their nn_Schutze_Hlnrtch conj_and_Melamed_Schutze nn_Melamed_Dan prep_for_thanks_comments prep_to_thanks_Schutze prep_to_thanks_Melamed amod_thanks_Special nn_thanks_data det_thanks_the dobj_evaluate_thanks nsubj_evaluate_who conj_and_helped_evaluate prep_to_helped_transcrtbe nsubj_helped_who nn_Khne_Darnel rcmod_Fleut_evaluate rcmod_Fleut_helped conj_and_Fleut_Khne nn_Fleut_Dana appos_Tubs_Romanlan nn_Tubs_Dan appos_Petkevtc_Czech nn_Petkevtc_Vladtmlr appos_Muxschnek_Estonian nn_Muxschnek_Kadrt conj_and_ec_Tubs conj_and_ec_Petkevtc conj_and_ec_Muxschnek appos_ec_Slovene nn_ec_~ nn_ec_Eua nn_ec_Tomaz nn_~_mfotmat nn_~_translatton det_~_the prep_on_provided_Tubs prep_on_provided_Petkevtc prep_on_provided_Muxschnek prep_on_provided_ec dobj_provided_~ nsubj_provided_who rcmod_those_provided pobj_of_those pcomp_on_of conj_and_~_Khne conj_and_~_Fleut prep_~_on nn_~_contrtbut det_~_the dobj_acknowledge_Fleut dobj_acknowledge_~ advmod_acknowledge_gratefully aux_acknowledge_to xcomp_hke_acknowledge aux_hke_would nsubj_hke_author det_author_The rcmod_Acknowledgements_hke amod_Acknowledgements_avatlable dobj_become_Acknowledgements nsubj_become_resources advmod_become_when amod_resources_such advcl_apply_become aux_apply_to vmod_methodology_apply amod_methodology_possible det_methodology_a dobj_explores_methodology nsubj_explores_study amod_study_small det_study_This rcmod_genres_explores amod_genres_several dobj_fiom_genres nsubj_fiom_corpora conj_and_types_texts nn_types_language nn_types_ot nn_types_variety det_types_a dobj_include_texts dobj_include_types nsubj_include_that rcmod_corpora_include nn_corpora_parallel amod_corpora_larger nn_corpora_utthzmg advmod_larger_substantially nn_determination_sense amod_determination_cross-hngual prep_of_topic_determination det_topic_the prep_on_done_topic auxpass_done_be aux_done_to ccomp_needs_fiom xcomp_needs_done nsubj_needs_work mark_needs_tn amod_work_More amod_work_past det_work_the ccomp_used_needs auxpass_used_been aux_used_have nsubjpass_used_synsets advmod_used_where cc_ontologtes_and appos_synsets_ontologtes nn_synsets_WordNet rcmod_apphcatlons_used amod_apphcatlons_same det_apphcatlons_the dep_m_apphcatlons amod_m_useful cop_m_be advmod_m_also aux_m_may nsubj_m_synsets npadvmod_synsets_themselves amod_synsets_generated det_synsets_The advmod_generated_automatically rcmod_mformatton_m det_mformatton_this dep_constdeHng_mformatton advmod_influenced_at nsubj_influenced_ot pobj_at_least agent_determined_constdeHng ccomp_determined_influenced auxpass_determined_be advmod_determined_potentsally aux_determined_could nsubjpass_determined_dlstmctlons mark_determined_that nn_translanon_machme prep_such_as_tasks_translanon nn_tasks_processing nn_tasks_language amod_tasks_natural prep_for_useful_tasks amod_dlstmctlons_useful nn_dlstmctlons_sense ccomp_show_determined advmod_show_however nsubj_show_results det_results_The rcmod_translations_show advmod_translations_back prep_from_generated_translations nn_groups_word vmod_synsets_generated conj_and_synsets_groups nn_synsets_WordNet det_synsets_the prep_between_overlap_groups prep_between_overlap_synsets neg_overlap_no advmod_no_virtually dobj_ts_overlap expl_ts_there advmod_synsets_Interestingly nn_mdar_~ dep_s_synsets prep_to_s_WordNet dobj_s_mdar nsubj_s_groups nn_groups_word ccomp_provtde_s nsubj_provtde_which rcmod_senses_provtde nn_senses_WordNet amod_senses_various det_senses_the dobj_representing_senses vmod_words_representing prep_for_translations_words advmod_translations_back prep_of_sets_translations dobj_compared_sets advmod_compared_also aux_compared_have nsubj_compared_I rcmod_groups_compared nn_groups_sense prep_into_cluster_groups dobj_cluster_occurrences aux_cluster_to xcomp_used_cluster vmod_turn_used nn_turn_m nn_turn_s nn_turn_~ dobj_whtch_turn prep_between_relatedness_occurrences dobj_determine_relatedness aux_determine_to vmod_s_determine dobj_s_mformatton det_~_th vmod_uses_s dobj_uses_~ nsubj_uses_that dep_metric_uses det_metric_a dobj_developed_metric nsubj_developed_language nn_language_target det_language_the dep_m_language dep_translations_m det_translations_the conj_and_ot_developed dobj_ot_translations nsubj_ot_language amod_language_original det_language_the rcmod_m_developed rcmod_m_ot nn_m_translations amod_m_additional rcmod_l_ts dep_l_whtch appos_l_m dep_l_e advmod_translations_back nn_groupmgs_word conj_and_create_Text prep_from_create_Proceedtngs dobj_create_groupmgs aux_create_to nn_study_thts amod_sample_small det_sample_the dep_used_Text dep_used_create prep_from_used_study dobj_used_sample aux_used_have nsubj_used_I auxpass_done_be nsubjpass_done_mlght advmod_done_how amod_mlght_thts ccomp_explore_done aux_explore_to vmod_effort_explore det_effort_an prep_in_used_effort advmod_used_here vmod_senses_used nn_senses_WordNet det_senses_the prep_as_set_senses mwe_set_such amod_set_pre-deflned det_set_a prep_of_independent_set amod_nctlons_independent nn_nctlons_~ nn_nctlons_st num_nctlons_~ nn_nctlons_d nn_nctlons_sense dobj_determine_nctlons aux_determine_to xcomp_used_determine auxpass_used_be aux_used_can nsubjpass_used_reformation advmod_used_how amod_reformation_cross-hngual ccomp_clear_used advmod_clear_less cop_clear_is nsubj_clear_It amod_entries_onary nn_entries_~ amod_entries_dtct prep_in_senses_entries prep_among_relations_senses prep_of_hierarchy_relations dep_detaded_clear dobj_detaded_hierarchy vmod_mole_detaded det_mole_a dobj_determine_mole aux_determine_to nn_lexicography_m dobj_used_lexicography auxpass_used_be aux_used_could nsubjpass_used_reformation advmod_presented_here vmod_that_presented prep_as_data_that mwe_data_such nn_data_cluster prep_on_~_data amod_~_addtt prep_in_translatton_~ nn_translatton_machine nn_translatton_m dobj_used_translatton auxpass_used_be aux_used_could nsubjpass_used_that rcmod_reformation_used det_languages_all parataxis_~_used xcomp_~_determine ccomp_~_used prep_across_~_languages dobj_~_me det_t_the prep_of_portion_t amod_portion_gnlficant dep_~_~ dobj_~_portion nsubj_~_s det_s_a rcmod_tem_~ nn_tem_~ amod_tem_same det_tem_the prep_with_cahzed_tem vmod_~_cahzed amod_~_lex cop_~_are nsubj_~_senses dep_~_intellect prep_of_senses_head dep_intellect_hkely dep_intellect_etc dep_intellect_retrieval dep_intellect_translatton det_body_the prep_of_part_body det_part_the appos_metonymy_etc appos_metonymy_meronymy prep_across_relations_languages dep_relations_metonymy amod_relations_semantic amod_relations_various agent_related_relations vmod_dtstmcttons_related nn_dtstmcttons_sense prep_of_lexlcallzatlon_dtstmcttons det_lexlcallzatlon_the prep_into_insight_lexlcallzatlon dobj_provide_insight advmod_provide_also nsubj_provide_clusters det_clusters_The rcmod_systems_provide nn_systems_dtsambtguatt0n nn_systems_sense prep_of_performance_systems det_performance_the conj_evaluatmg_part prep_for_evaluatmg_instance dobj_evaluatmg_performance advmod_evaluatmg_when nn_ons_~ nn_ons_stmct nn_ons_~ nn_ons_d nn_ons_grosser amod_ons_confusing prep_than_severely_ons advmod_severely_less dep_penahzed_evaluatmg advmod_penahzed_severely auxpass_penahzed_be aux_penahzed_should nsubjpass_penahzed_dtstmctlons mark_penahzed_that nn_dtstmctlons_sense amod_dtstmctlons_freer-grained amod_dtstmctlons_confusing ccomp_suggest_penahzed nsubj_suggest_degree advcl_suggest_become mark_suggest_that appos_Yarowsky_1997 conj_and_Resmk_Yarowsky amod_Resmk_severe advmod_severe_less nn_s_~ nn_s_error appos_degree_Yarowsky appos_degree_Resmk prep_of_degree_s det_degree_the xcomp_become_finer nsubj_become_dtstmcttons mark_become_as nn_dtstmcttons_sense ccomp_hkely_suggest cop_hkely_is nsubj_hkely_it prep_for_etc_example nn_retrieval_mtormaUon conj_and_translatton_hkely conj_and_translatton_etc conj_and_translatton_retrieval nn_translatton_machine prep_as_tasks_~ mwe_tasks_such prep_in_used_tasks auxpass_used_be aux_used_could nsubjpass_used_turn nn_turn_m nn_turn_whtch nn_differences_sense prep_of_granularity_differences det_granularity_the parataxis_determine_used dobj_determine_granularity aux_determine_to dep_used_determine prep_for_used_example auxpass_used_be aux_used_could nsubjpass_used_that nn_senses_WordNet rcmod_relations_used prep_among_relations_senses prep_about_on_relations nn_~_mformat prep_provide_on dobj_provide_~ prepc_above_graphs_provide nn_graphs_cluster det_graphs_The dep_ons_graphs nn_ons_~ nn_ons_stmct nn_ons_~ nn_ons_d nn_ons_sense dobj_structure_ons conj_and_define_structure aux_define_to xcomp_used_structure xcomp_used_define auxpass_used_be aux_used_can prepc_on_~_used nn_~_lexlcahzat amod_~_cross-hngual prep_that_suggests_~ nsubj_suggests_measures nsubj_suggests_tree nsubj_suggests_F dep_suggests_723157 dep_suggests_= nsubj_suggests_dlstance dep_suggests_= nsubj_suggests_dlstance dep_suggests_= dep_suggests_dlstance dep_suggests_> nsubj_suggests_I dep_suggests_i det_study_this dep_m_study nn_m_sample amod_m_small det_m_The dep_Conclusion_m num_Conclusion_58 nn_Conclusion_head nn_Conclusion_ot nn_Conclusion_senses dobj_tout_Conclusion amod_the_tout dep_tot_the dep_measures_tot nn_measures_dmtance nn_tree_Cluster num_tree_3 nn_tree_gure conj_and_F_measures conj_and_F_tree num_F_14 appos_723157_111317 appos_723157_1113 appos_723157_17 appos_723157_ll appos_723157_13 num_723157_0 nn_dlstance_mum nn_dlstance_~ nn_dlstance_mln num_dlstance_619052 number_619052_0 nn_dlstance_mlnlmum num_dlstance_441022 num_dlstance_0 nn_dlstance_mlnlmum num_dlstance_17 nn_dlstance_i amod_dlstance_> nn_dlstance_J dep_dlstance_I nn_dlstance_i quantmod_17_> dep_17_I number_17_3 num_J_1 nn_i_i parataxis_14_suggests quantmod_14_> dep_14_I dep_14_hard prep_of_senses_14 nn_senses_sm det_senses_the dobj_tor_senses dep_tree_tor dep_tree_measures conj_and_tree_distance nn_tree_Cluster num_tree_2 nn_tree_Figure dep_tree_1 dep_tree_3 num_111_2 dep_1_111 dep_1_2 num_1_4 number_4_1 number_3_1 dep_3_3 number_3_2 dep_21_distance dep_21_tree appos_21_2313 dep_21_1412 dep_ii_21 quantmod_12_> dep_12_I number_12_14 dep_>_23 dep_>_13 num_>_12 appos_l_1,4 appos_l_12 dep_l_> num_l_13 dep_>_l dep_l_ii amod_l_> num_l_23 quantmod_23_> dep_I_l dep_ii_I dep_>_ii amod_I_> num_I_21 quantmod_21_> dep_I_I num_I_761327 number_761327_0 dobj_=_I amod_dlstance_= nn_dlstance_nlmum nn_dlstance_~ dep_dlstance_m num_dlstance_0 number_m_602972 dep_=_dlstance amod_dlstance_= nn_dlstance_mlnlmum num_dlstance_555158 num_dlstance_0 dep_=_dlstance amod_dlstance_= nn_dlstance_mlnlmum num_dlstance_434856 num_dlstance_0 dep_=_dlstance amod_stance_= nn_stance_~ nn_stance_d nn_stance_nlmum nn_stance_~ amod_stance_m num_stance_0 number_m_249399 dep_=_stance amod_dlstance_= nn_dlstance_nlmum nn_dlstance_~ nn_dlstance_m dep_dlstance_I dep_dlstance_I dep_dlstance_I dep_dlstance_I dep_dlstance_I dep_languages_dlstance nn_languages_target nn_languages_tour det_languages_the dep_tor_languages nn_tor_correlauon nn_tor_CI num_tor_6 nn_tor_Table num_tor_80 dep_tor_0 number_0_99 dep_0_0 dep_0_0 number_0_90 number_0_59 dep_0_tor number_0_73 number_0_0 dep_ES/SL_0 dep_73_ES/SL number_73_0 dep_73_98 dep_73_70 number_98_0 num_70_0 number_0_26 dep_0_73 number_0_97 number_0_0 dep_RO/ES_0 dep_RO/ES_71 dep_71_0 dep_71_0 number_0_71 number_0_72 dep_0_0 number_0_51 dep_0_RO/ES number_0_88 number_0_0 dep_SL/CS_0 dep_SL/CS_72 dep_72_0 dep_72_0 number_0_72 number_0_67 dep_0_0 number_0_66 dep_0_SL/CS number_0_83 number_0_0 dep_RO/CS_0 dep_80_RO/CS number_80_0 dep_80_00 number_00_1 dep_00_68 number_68_0 dep_68_78 number_78_0 dep_73_80 number_73_0 dep_RO/SL_73 num_RO/SL_74 dep_0_RO/SL number_0_69 dep_0_0 number_0_68 dep_0_0 number_0_72 dep_0_0 number_0_86 number_0_0 dep_ES/CS_0 nn_ES/CS_Ave nn_ES/CS_Head nn_ES/CS_Llne nn_ES/CS_Country nn_ES/CS_Hard nn_ES/CS_Lanqs dep_values_ES/CS nn_values_CI amod_values_Average num_values_5 dep_Table_values appos_26_Table dep_0_26 num_ES_0 dep_27_ES number_27_0 dep_RO/SL/CS_27 num_RO/SL/CS_28 dep_0_RO/SL/CS amod_SL/CS_0 num_SL/CS_28 dep_0_SL/CS dep_RO/ES/SL_0 num_RO/ES/SL_27 number_27_0 dep_ALL_RO/ES/SL nn_CI_Averaqe nn_CI_qroup nn_CI_Lanquaqe nn_CI_context amod_CI_full dobj_working_ALL prep_with_working_CI vmod_translators_working amod_translators_trained agent_generated_translators vmod_translations_generated prep_to_compares_translations nsubj_compares_data advmod_compares_how det_data_this ccomp_clear_compares neg_clear_not cop_clear_is nsubj_clear_It det_sentence_the rcmod_word_clear prep_in_word_sentence amod_word_given det_word_a det_translation_the prep_for_provide_word dobj_provide_translation aux_provide_to xcomp_asked_provide auxpass_asked_were nsubjpass_asked_who rcmod_sentences_asked prep_in_sentences_English amod_sentences_Isolated prep_with_presented_sentences vmod_speakers_presented amod_speakers_native agent_generated_speakers auxpass_generated_was nsubjpass_generated_data ccomp_generated_tended mark_generated_that nn_data_translation poss_data_their amod_levels_finergrained amod_languages_Indo-European prep_than_more_languages nn_stmctlons_~ nn_stmctlons_d nn_stmctlons_sense amod_stmctlons_English advmod_lexlcallze_However prep_at_lexlcallze_levels advmod_lexlcallze_especially advmod_lexlcallze_more dobj_lexlcallze_stmctlons aux_lexlcallze_to xcomp_tended_lexlcallze nsubj_tended_languages mark_tended_that amod_languages_non-Indo-European ccomp_found_generated det_study_this prep_in_used_study vmod_one_used det_one_the nn_slmdar_memc det_slmdar_a dep_using_found prep_to_using_one dobj_using_slmdar ccomp_,_using dep_subm_tted dep_Resmk_who appos_Resmk_subm conj_and_Resmk_Yarowsky agent_obtained_Yarowsky agent_obtained_Resmk vmod_results_obtained prep_to_contrary_results cop_contrary_is nsubj_contrary_This rcmod_distance_contrary nn_distance_language conj_and_lexlcahzed_distance advmod_lexlcahzed_differently auxpass_lexlcahzed_are nsubjpass_lexlcahzed_stmcuons nn_stmcuons_~ nn_stmcuons_d ccomp_sense_distance ccomp_sense_lexlcahzed prep_to_sense_which vmod_degree_sense det_degree_the prep_between_relationship_degree neg_relationship_no dobj_show_relationship advmod_show_also nsubj_show_study tmod_show_m num_Table_5 appos_pair_Table nn_pair_language det_pair_each prep_for_Correlations_pair prep_of_Correlations_CIs num_Correlations_57 dep_evidence_Correlations nn_evidence_colpus prep_on_based_evidence nn_ctlonanes_~ nn_ctlonanes_d amod_ctlonanes_same det_ctlonanes_those nn_ctlonanes_ol vmod_edmons_based dobj_edmons_ctlonanes nsubj_edmons_ple-date rcmod_study_edmons det_study_this ccomp_consulted_show nsubj_consulted_dictlonalles nsubj_consulted_LDOCE nn_dictlonalles_OALD num_version_1985 appos_OALD_version num_vexsmn_1987 conj_and_LDOCE_dictlonalles appos_LDOCE_vexsmn det_LDOCE_the rcmod_ot_consulted nn_ot_Edmons num_ot_9 amod_ot_Estonian nn_ot_m advmod_ot_differently dobj_lexlcahzed_ot auxpass_lexlcahzed_be aux_lexlcahzed_to xcomp_hkely_lexlcahzed advmod_hkely_less cop_hkely_are nsubj_hkely_dtstmctmns dobj_hkely_that advmod_less_slightly nn_dtstmctmns_sense nn_dtstmctmns_WordNet rcmod_mdmatmg_hkely amod_group_other det_group_any prep_for_that_group prep_than_lower_that amod_s_lower nn_s_~ appos_study_mdmatmg appos_study_s det_study_the dep_m_study nn_m_language amod_m_non-Indo-European amod_m_only det_m_the appos_fol_m amod_fol_Estonian nn_fol_CI nn_fol_mean det_fol_the num_Table_5 appos_stance_Table nn_stance_~ nn_stance_d nn_stance_language dobj_aftected_fol prep_in_aftected_fact agent_aftected_stance neg_aftected_not auxpass_aftected_is nsubjpass_aftected_tendency mark_aftected_that advmod_lextcahze_differently dobj_lextcahze_senses aux_lextcahze_to vmod_tendency_lextcahze det_tendency_the ccomp_show_aftected nsubj_show_groupings nn_groupings_language nn_groupings_dlftment nn_groupings_tor rcmod_results_show det_results_The dobj_es_results nsubj_es_~ mark_es_as nn_~_entt amod_~_dlcuonary advcl_suuctured_es nsubj_suuctured_part amod_tol_hard nn_tol_Clusteis num_tol_1 nn_tol_Flgme nn_tol_chlef num_ruler_II nn_ruler_part amod_ruler_front conj_and_front_head conj_and_front_tol conj_and_front_ruler num_front_2 nn_front_zntellect nn_front_b nn_front_body det_front_the prep_of_part_head prep_of_part_tol prep_of_part_ruler prep_of_part_front det_part_a num_part_1 dep_part_I amod_part_hard nn_part_b advmod_part_earnestly det_part_a advmod_hard_metaphorlcally amod_2_strong rcmod_b_suuctured num_b_2 amod_b_soft neg_b_not det_b_a num_b_1 number_1_II quantmod_1_vlgorously number_1_2 dep_dlfflcult_b num_dlfflcult_1 dep_I_dlfflcult dep_HEAD_I dep_HARD_HEAD amod_ctlonarles_HARD dep_~_ctlonarles dep_d_~ dep_other_d amod_the_other nn_UILD_COB det_UILD_the prep_in_senses_UILD amod_senses_multiple prep_in_spread_the prep_as_spread_it.is prep_over_spread_senses auxpass_spread_is nsubjpass_spread_oot mark_spread_Because advmod_hard_metaphorically dep_WordNet_hard possessive_WordNet_'s nn_WordNet_study nn_WordNet_thls prep_in_senses_WordNet nn_senses_WordNet num_senses_six det_senses_the prep_of_hve_senses prep_to_correspond_hve nsubj_correspond_senses mark_correspond_that prep_of_ruder_frequency nn_ruder_m nn_ruder_senses dobj_presents_ruder nsubj_presents_which num_examples_9 nn_examples_colpus prep_of_s_examples nn_s_~ nn_s_bas det_s_the prep_on_constructed_s vmod_group_constructed det_group_the conj_and_cttonary_presents prep_in_cttonary_group nn_cttonary_~ nn_cttonary_d amod_cttonary_only det_cttonary_the cop_cttonary_is nsubj_cttonary_which rcmod_~_presents rcmod_~_cttonary amod_~_cuonary nn_~_d nn_~_COBUILD det_~_the prep_in_senses_~ prep_of_senses_hard num_senses_five amod_senses_first det_senses_the ccomp_note_correspond aux_note_to xcomp_interesting_note cop_interesting_is nsubj_interesting_It parataxis_interesting_split amod_hsts_hnear amod_hsts_flat prep_in_presented_hsts vmod_part_presented amod_part_most det_part_the advmod_are_However prep_for_are_part nsubj_are_senses det_senses_the ccomp_smce_are amod_enmes_cttonary nn_enmes_~ nn_enmes_d det_enmes_the nn_enmes_m xcomp_reflected_enmes neg_reflected_not auxpass_reflected_are nsubjpass_reflected_clusters det_clusters_the rcmod_m_reflected amod_m_apparent dep_relatmns_m amod_relatmns_erarchlcal nn_relatmns_~ nn_relatmns_h det_relatmns_The amod_senses_various vmod_split_smce dobj_split_relatmns prep_across_split_senses nsubj_split_cases det_cases_some nn_cases_m det_entries_the prep_within_places_entries amod_places_various conj_or_scattered_interesting prep_at_scattered_places auxpass_scattered_are nsubjpass_scattered_senses nn_senses_WordNet nn_senses_remamlng det_senses_The dep_clusters_interesting dep_clusters_scattered det_clusters_the conj_and_entries_clusters nn_entries_dictionary det_entries_the prep_in_senses_clusters prep_in_senses_entries det_senses_the prep_between_correspondence_senses nn_correspondence_speech nn_correspondence_ot nn_correspondence_part nn_correspondence_tor dobj_made_correspondence auxpass_made_was nsubjpass_made_eqmvalents nn_eqmvalents_UanslaUon nn_eqmvalents_m nn_eqmvalents_dzstlncUon neg_eqmvalents_no rcmod_analysis_made det_analysis_the nn_analysis_m dobj_used_analysis auxpass_used_were nsubjpass_used_ms rcmod_oot_used dep_oot_to num_oot_~ number_8_5 prep_to_normahzed_8 auxpass_normahzed_were nsubjpass_normahzed_clusters amod_occurrrence_single det_occurrrence_a prep_from_resulting_occurrrence vmod_l_resulting num_l_00 prep_of_CIs_l appos_s_CIs dobj_~_s vmod_analys_~ dep_cluster_analys det_cluster_the dobj_ot_cluster nsubj_ot_purposes det_purposes_the rcmod_Foi_ot num_Foi_7 det_Foi_the dobj_assess_Foi aux_assess_to xcomp_difficult_assess acomp_s_difficult nsubj_s_~ nn_~_t nn_~_~ rcmod_clusters_s prep_beyond_clusters_this det_clusters_the rcmod_m_normahzed amod_m_apparent nn_m_dlwslon amod_m_gross det_m_the advcl_reflects_spread dobj_reflects_m nsubj_hrst_senses amod_senses_used advmod_used_frequently rcmod_Ol_hrst amod_Ol_common det_Ol_the advmod_common_most dep_hst_reflects dobj_hst_Ol nsubj_hst_dictionaries mark_hst_since amod_dictionaries_most conj_and_first_second dep_senses_hst appos_senses_whmh advmod_senses_second advmod_senses_first neg_soft_not advmod_fficult_~ aux_fficult_'d det_fficult_the cc_hst_and dobj_hst_fficult dep_hst_all nsubj_hst_enmes appos_Learner_OALD nn_Learner_Advanced nn_Learner_OxJotd dep_Longman_LDOCE possessive_Longman_'s conj_and_Enghsh_COBUILD conj_and_Enghsh_Learner appos_Enghsh_Longman appos_Enghsh_CED nn_Enghsh_Colhns dep_dlctmnanes_COBUILD dep_dlctmnanes_Learner dep_dlctmnanes_Enghsh amod_dlctmnanes_constructed num_dlctmnanes_four advmod_constructed_differently prep_in_enmes_dlctmnanes prep_for_enmes_hard det_enmes_the conj_and_hard_head prep_for_entries_head prep_for_entries_hard amod_entries_dlctLonary amod_entries_actual prep_to_dissimilar_entries neg_dissimilar_not dep_ts_dissimilar nsubj_ts_This rcmod_gure_ts num_gure_1 nn_gure_~ nn_gure_F nn_gure_m prep_as_appeal_gure aux_appeal_might nsubj_appeal_We conj_and_hard_head prep_for_clusters_head prep_for_clusters_hard det_clusters_the amod_enmes_dictmnary nn_enmes_hke amod_enmes_Structured dep_sense_enmes amod_sense_intuitive dobj_make_sense nsubj_make_cluster tmod_make_m dep_make_apparent nsubj_make_erarchtes dep_make_4 dep_cluster_graphs det_cluster_the nn_erarchtes_~ nn_erarchtes_h det_erarchtes_The dep_4_1 nn_chief_ruler dep_chief_part dep_1_1 det_body_the cc_part_and dep_part_1 prep_of_part_body dep_CI_chief amod_CI_mutual amod_CI_lowest det_CI_the conj_and_consistency_CI amod_consistency_internal nn_consistency_ghest nn_consistency_~ nn_consistency_h det_consistency_the prep_with_senses_CI prep_with_senses_consistency num_senses_two det_senses_the prep_in_anchored_senses vmod_each_anchored appos_groupings_each amod_groupings_dlstmct num_groupings_two dobj_show_groupings advmod_show_stmdarly nsubj_show_groups mark_show_in num_Figure_2 appos_head_Figure prep_for_clusters_head det_clusters_The amod_clusters_hard prep_of_meanings_clusters amod_meanings_adjecuval amod_meanings_different det_meanings_the prep_to_proximity_meanings amod_proximity_semantic nn_proximity_thmr nn_proximity_leflectmg appos_groups_proximity amod_groups_separate advcl_placed_show auxpass_placed_are nsubjpass_placed_groups num_forms_8 amod_forms_adverbml num_forms_two det_forms_The dep_groups_forms nn_groups_respecuve det_groups_the rcmod_ot_placed det_ot_each rcmod_level_make prep_of_level_ot amod_level_deepest det_level_the dep_1_3 conj_and_1_1 num_1_1 prep_at_senses_level appos_senses_1 appos_senses_1 amod_senses_consistent num_senses_two det_senses_th advmod_consistent_internally advmod_consistent_most det_two_the dep_~_senses nn_~_w appos_clusters_clusters appos_clusters_~ nn_clusters_mare num_clusters_two prep_into_fall_clusters nsubj_fall_~ det_senses_The num_senses_7 dep_data_senses nn_data_CI det_data_the prep_from_generated_data dep_hard_generated prep_for_clusters_hard nn_clusters_sense det_clusters_the dobj_shows_clusters nsubj_shows_Sense nsubj_shows_data num_Figure_2 nn_Figure_head prep_for_CIs_Figure num_CIs_4 dep_CIs_Table dep_CIs_00 num_Table_00 number_00_1 number_00_0 dep_00_001 number_001_0 dep_001_40 number_40_0 dep_40_7 number_7_1 dep_7_50 number_50_0 dep_07_CIs dep_07_0 dep_07_7 dep_0_12 number_12_0 dep_12_4 number_4_1 dep_4_45 number_45_0 dep_45_53 number_53_0 dep_53_3 number_3_1 dep_3_69 number_69_0 dep_69_1 number_1_1 number_7_1 dep_7_4 dep_7_1 number_4_1 dep_4_3 number_3_1 number_1_1 prep_Sense_07 nn_Sense_WordNet num_Sense_56 amod_data_sparse det_data_these ccomp_normahze_shows aux_normahze_to amod_Carletta_1996 vmod_c_normahze appos_c_Carletta nn_c_~ nn_c_staUst nn_c_Kappa det_c_the dep_oI_c dep_use_oI det_use_the dobj_mvesugatmg_use advmod_mvesugatmg_currently dep_mvesugatmg_e vmod_~_mvesugatmg det_~_a rcmod_We_fall rcmod_senses_appeal amod_senses_individual dep_~_senses dep_~_soft parataxis_~_hst prep_for_~_example dobj_~_senses aux_~_to vmod_CIs_~ nn_CIs_ot nn_CIs_consideration nn_CIs_horn amod_CIs_dlscarded cop_CIs_been advmod_CIs_theretote aux_CIs_have det_corpus_the dep_m_corpus nn_m_occurrence amod_m_single det_m_a dep_m_m conj_and_exists_CIs dobj_exists_m nsubj_exists_senses det_senses_these rcmod_ot_CIs rcmod_ot_exists det_ot_each dobj_ot_1 nsubj_ot_CIs aux_ot_have num_4_1 num_4_3 conj_and_3_1 number_3_2 dep_Senses_ot dobj_Senses_4 nsubj_Senses_html num_html_6 amod_html_de/wsd/cross-hng num_html_~ number_~_~ dep_edu_Senses prep_because_vassar_ot dobj_vassar_edu nsubj_vassar_cs nn_cs_www prep_at_avadable_http cop_avadable_are nsubj_avadable_study det_study_the nn_study_m rcmod_words_avadable det_words_all dobj_tor_words nsubj_tor_m nn_Results_Stolcke nn_Results_Andleas agent_Developed_Results vmod_discrimination_Developed num_discrimination_4 nn_discrimination_sense prep_to_contribute_discrimination aux_contribute_might nsubj_contribute_sample dep_contribute_which amod_sample_larger det_sample_a rcmod_m_contribute rcmod_ways_tor det_insight_some parataxis_gives_vassar prep_into_gives_ways dobj_gives_insight nsubj_gives_It advcl_gives_small nsubj_gives_O0 dep_gives_i nsubj_gives_O0 dep_gives_i dep_gives_13 dep_gives_o dep_gives_4 dep_gives_1 dep_gives_4 nsubj_gives_1 dep_gives_Sense nsubj_gives_13/1 mark_gives_for cop_small_is nsubj_small_sample mark_small_Although nn_sample_data det_sample_the num_Results_2 num_Results_50 amod_Results_hard number_50_0 number_50_00 dep_50_0 dep_50_i dep_50_I number_0_,63 number_0_12 num_i_0 prep_for_CIs_Results dep_CIs_Table num_CIs_0 dep_CIs_25 dep_CIs_19 dep_CIs_0 dep_CIs_0 num_Table_3 num_Table_21 number_25_0 dep_25_00 number_00_0 dep_00_00 number_00_0 dep_00_00 number_00_0 dep_00_00 number_00_0 dep_00_00 number_00_0 num_19_0 number_0_56 number_0_17 dep_0_0 number_0_50 number_0_04 number_0_0 dep_O0_CIs number_25_0 num_O0_25 num_O0_0 nn_O0_ool num_o_50 num_o_0 number_0_13 number_4_1 dep_4_23 dep_23_21 number_21_2 number_1_1 number_1_1 dep_1_3 number_3_1 number_4_1 dep_4_3 number_3_2 number_1_2 nn_Sense_WordNet num_Sense_3 vmod_CI_gives det_CI_the prep_than_higher_CI cop_higher_is nsubj_higher_word mark_higher_wlth det_body_the prep_of_part_body number_1_1 number_3_1 num_senses_3 dep_CI_part conj_and_CI_1 dep_CI_Intellect prep_for_CI_senses det_CI_the prep_of_senses_head num_senses_four det_senses_the prep_of_three_senses prep_for_true_three acomp_holds_true nsubj_holds_same det_same_The rcmod_6_holds dep_consistency_6 amod_consistency_internal nn_consistency_ghest det_h_the dobj_registenng_h dep_=_63 amod_ci_= dep_soft_ci neg_soft_not dep_13_registenng dep_13_soft dep_=_56 nsubj_=_CI dep_=_cult nn_cult_| nn_cult_dlff number_1_1 num_senses_1 amod_senses_other amod_athmty_tfian nn_athmty_consistency amod_athmty_internal nn_athmty_greatel prep_while_have_1 prep_while_have_CI conj_and_have_consistency conj_and_have_13 dep_have_= prep_with_have_senses prep_with_have_senses dobj_have_athmty nsubj_have_d dep_have_senses dep_have_all mark_have_that prep_of_senses_ha det_s_the conj_x_all_senses prep_of_all_s dep_show_consistency dep_show_13 dep_show_have dep_word_show amod_word_same det_word_the advcl_translated_higher auxpass_translated_be aux_translated_to vmod_sense_translated det_sense_that prep_of_occurrences_sense det_occurrences_all prep_for_tendency_occurrences det_tendency_the nsubj_is_tendency dep_that_is prep_with_sense_itself det_sense_a prep_of_mty_sense det_aff_the dobj_measuring_aff amod_CIs_uous nn_CIs_~ num_CIs_4 dep_Tables_that appos_Tables_mty vmod_Tables_measuring conj_and_Tables_CIs num_Tables_3 prep_in_given_CIs prep_in_given_Tables auxpass_given_are nsubjpass_given_head nsubjpass_given_sobS dep_given_Table advmod_given_hard dep_given_Description dep_given_# conj_and_sobS_head amod_sobS_hard nn_sobS_s nn_data_CI det_data_The dep_s_data nn_s_data det_s_the prep_in_occurred_s nsubj_occurred_that conj_and_hard_head prep_of_senses_head prep_of_senses_hard det_senses_the dep_gives_occurred dobj_gives_senses nsubj_gives_Table nsubj_gives_Czech nsubj_gives_Czech nsubj_gives_Romaman dep_gives_groupings dep_gives_for dep_gives_computed nsubj_gives_3 dep_gives_vigor dep_gives_force dep_gives_wlth dep_gives_2 nsubj_gives_ar num_Table_2 nn_example_Fol nn_example_languages nn_example_target num_example_four det_example_the prep_of_pairs_example det_pairs_all prep_for_CIs_pairs prep_among_run_CIs auxpass_run_was nsubjpass_run_correlation dep_run_4 det_correlation_a dobj_affect_coherence aux_affect_may nsubj_affect_relaUon prep_to_affect_which prep_between_relaUon_languages amod_relaUon_linguistic det_relaUon_the rcmod_degree_affect det_degree_the dobj_determine_degree aux_determine_to dep_determine_order mark_determine_in dep_4_determine advmod_4_Finally rcmod_proximity_run nn_proximity_sense dobj_reflecting_proximity dobj_generate_trees aux_generate_to vmod_data_generate nn_data_CI det_data_the xcomp_applied_reflecting prep_to_applied_data auxpass_applied_was nsubjpass_applied_Estonian nsubjpass_applied_Indo-European advmod_applied_Slovene nn_algorithm_clustering amod_algorithm_hierarchical det_algorithm_a appos_senses_algorithm prep_between_relationship_senses det_relationship_the dobj_visualize_relationship advmod_visualize_better aux_visualize_To dep_Estonian_nonIndo-European xcomp_Indo-European_visualize conj_and_Indo-European_Estonian amod_family_same appos_Slovene_family dep_Czech_applied conj_and_Czech_Czech conj_and_Czech_Romaman conj_and_Czech_Slovene amod_Czech_Estonian nn_families_language amod_families_different num_families_three dep_Estonian_families conj_and_Romaman_Table conj_and_Romaman_Czech conj_and_Romaman_Romaman conj_and_Romaman_Slovene conj_and_Romaman_Czech conj_and_Romaman_Czech amod_language_different pobj_for_language det_language_each conj_and_computed_for prep_for_computed_language advmod_computed_also auxpass_computed_were nsubjpass_computed_art advmod_computed_earnestly amod_CIs_head amod_CIs_hard conj_and_hard_head dobj_ot_CIs nsubj_ot_r dep_ot__ nn_senses_WoldNet nn_senses_part amod_senses_front num_front_7 nn_front_ef nn_front_% conj_r_senses conj_r_front conj_r_ch dep___le num___~ dep___r dep____ dep___4 dep_intellect_ot num_intellect_3 nn_intellect_body det_intellect_the prep_of_art_intellect nn_art_~ nn_art__ nn_art_i dep_art_adv advmod_art_intently appos_force_adv conj_or_force_vigor nsubj_wlth_I dep_strong_lgorous conj_or_strong_~ advmod_strong_very amod_4_~ amod_4_strong number_4_1 neg_yielding_not nn_\__ amod_\_hard advmod_hard_horlcally dep_~_gives appos_~_4 prep_to_~_pressure dep_~_yielding num_~_3 dep_~_\ nn_~_meta num_~__ num_~_2 number_2_1 number_2_2 dep_Table_~ num_Table_1 nn_Table_i nn_Table_i nn_Table_i dep_Table_head num_head_2 amod_head_difficult dep_head_1 number_1_1 vmod_Word_given amod_Word_same nn_Word_p dep_s_Word prep_of_%_s num_%_6 dep_%_% num_%_6 dep_%_% num_%_6 dep_h_% nn_h_h amod_h_% nn_h_dizen number_%_whl dep_%_h dep_trequencles_% dep_their_trequencles conj_and_types_their nn_types_Translation num_types_1 nn_types_Table amod_types_lexlcallzed nn_types_~ amod_types_explicltl cop_types_Is csubj_types_translated neg_explicltl_not nn_word_English det_word_the prep_to_corresponding_word vmod_whl_corresponding prep_in_phrase_whl det_phrase_a pobj_by_phrase conj_or_meanlng_by amod_meanlng_specific amod_meanlng_more amod_meanlng_broader det_meanlng_a conj_or_broader_more dobj_has_by dobj_has_meanlng nsubj_has_which rcmod_language_has amod_language_single det_language_a agent_translated_language auxpass_translated_Is nsubjpass_translated_= dep_translated_~ dobj_translated_s amod_word_ambiguous det_word_the dobj_contalnmng_word vmod_phrase_contalnmng amod_phrase_English det_phrase_An det_translation_the dep_word_phrase prep_in_word_translation nn_word_English det_word_the prep_for_substituted_word auxpass_substituted_is nsubjpass_substituted_word mark_substituted_as det_pronoun_A num_pronoun_4 nn_pronoun_translation det_pronoun_the prep_in_lexzcalized_pronoun neg_lexzcalized_not auxpass_lexzcalized_is nsubjpass_lexzcalized_word amod_word_En@izsh det_word_The rcmod_word_lexzcalized num_word_3 nn_word_English nn_word_slngle det_word_the ccomp_meaning_substituted det_compound_a conj_or_words_compound num_words_more num_words_two conj_or_two_more prep_of_phrase_compound prep_of_phrase_words det_phrase_a agent_translated_phrase auxpass_translated_is nsubjpass_translated_word amod_word_English det_word_The num_word_2 det_word_a advmod_word_possibly rcmod_equivalent_translated amod_equivalent_En@izsh det_equivalent_the dobj_translate_equivalent aux_translate_to xcomp_used_translate auxpass_used_is nsubjpass_used_t advmod_used_When amod_Item_lexlcal nn_Item_slngle det_Item_A num_Item_1 nn_Item_Meaning nn_Item_Type nn_Item_sense nn_Item_lven nn_Item_~ det_Item_a prep_of_coherence_Item det_coherence_the prep_of_measure_coherence det_measure_a dobj_obtain_measure nsubj_obtain_senses mark_obtain_than num_senses_55 nn_senses_word ccomp_lather_obtain appos_types_lather amod_types_wold nn_types_tOl dobj_entropy_types nsubj_entropy_we dep_=_j rcmod_t_entropy amod_t_= amod_item_lextcal amod_item_same det_item_the advcl_translated_used prep_with_translated_item auxpass_translated_are nsubjpass_translated_occurrences nn_j_sense conj_and_t_j amod_t_of-sense dep_occurrences_j dep_occurrences_t advmod_occurrences_frequently det_occurrences_the advmod_frequently_more appos_s_sj dep_Cl_s prep_of_value_Cl det_value_the dep_greater_value det_greater_the nn_e_t nn_relatedness_thmr prep_of_index_relatedness det_index_an dobj_provides_index nsubj_provides_CI amod_senses_different prep_of_pans_senses prep_for_CI_pans det_CI_the prep_in_=_general dobj_=_0 nsubj_=_s advcl_=_translated appos_s_~ nn_s_Cl det_occurrence_every nn_m_word amod_m_different det_m_a tmod_translated_occurrence prep_with_translated_m auxpass_translated_are nsubjpass_translated_they mark_translated_if vmod_=_meaning conj_=_translated conj_=_greater conj_=_e dep_=_provides dep_=_= dobj_=_1 nsubj_~_s nn_s_Cl advmod_s_then nn_language_comparison det_language_each prep_in_~_language dobj_~_d aux_~_wo nsubj_~_same det_same_the rcmod_th_~ dep_~_th nn_~_w dobj_translated_~ advmod_translated_consistently auxpass_translated_are conj_and_sense_sense nn_languages_othel det_languages_the prep_in_word_languages amod_word_same det_word_the prep_if_translated_sense prep_if_translated_sense agent_translated_word vmod_occurrences_translated prep_of_clusters_occurrences dobj_examining_clusters prepc_by_computed_examining conj_and_0_1 prep_between_value_1 prep_between_value_0 det_value_a dobj_ts_value nsubj_ts_CI det_CI_The appos_~_r nn_~_oan nn_~_~ nn_~_r appos_~_q nn_~_tJan num_~_~ dep_t_~ nn_t_y dep_=_computed dep_=_ts dep_=_~ dep_=_t npadvmod_=_x dep_=_e advmod_=_i nn_t_language nn_t_m nn_t_Item amod_t_cal nn_t_~ nn_t_lex amod_t_same det_t_the agent_translated_t auxpass_translated_are nsubjpass_translated_r conj_and_q_translated nsubj_q_senses det_senses_that rcmod_times_translated rcmod_times_q prep_of_number_times det_number_the parataxis_ts_translated parataxis_ts_= dobj_ts_number nsubj_ts_m dep_ts_~ dep_ts_< dep_ts_s nsubj_ts_rnrt amod_m_> num_~_~ amod_translation_idenufiable neg_translation_no dobj_have_translation nsubj_have_that rcmod_occurrences_have nn_corpus_Enghsh det_corpus_the nn_corpus_m dep_corpus_~ nn_s_sense nn_s_sqand amod_s_olsense nn_s_occurrences advmod_mber_respectively dep_mber_corpus prep_of_mber_s nn_mber_~ nn_mber_nt det_mber_the cop_mber_are nsubj_mber_m nsubj_mber_q conj_and_q_m nn_q_~ nn_q_nl dep_q_s advmod_q_where nn_languages_comparison prep_of_number_languages det_number_the prep_under_s_consideration dobj_s_number nsubj_s_~ mark_s_@ nn_~_n prep_including_rnrt_occurrences rcmod_rnrt_mber nn_rnrt_m num_rnrt_1 amod_rnrt_= amod_rnrt_= nn_rnrt_Cl appos_Cl_sqS nn_Cl_<qt> nn_Cl_S dep_follows_ts mark_follows_as prep_of_pair_senses det_pair_each prep_for_computed_pair auxpass_computed_Is nsubjpass_computed_CI ccomp_computed_distraction prep_to_computed_which det_CI_the amod_word_ambiguous det_word_each prep_for_ts_word dobj_ts_vahd dep_distraction_ts det_distraction_the vmod_degree_computed det_degree_the prep_of_mdtcatmn_degree det_mdtcatmn_an prep_as_seen_mdtcatmn auxpass_seen_be parataxis_turn_their parataxis_turn_types advcl_turn_follows acomp_turn_seen nsubj_turn_m aux_turn_can nsubj_turn_which nn_senses_WordNet amod_senses_different dep_cahze_turn advmod_cahze_differently dobj_cahze_senses nsubj_cahze_m nn_~_lex prep_to_tendency_~ det_tendency_the prep_of_measure_tendency det_measure_a dobj_provide_measure nsubj_provide_CIs mark_provide_that det_CIs_the ccomp_assumed_provide auxpass_assumed_be aux_assumed_can nsubjpass_assumed_text appos_text_tt advmod_text_However amod_text_translated det_text_the rcmod_m_assumed advmod_m_differently parataxis_lexicahzed_cahze auxpass_lexicahzed_are nsubjpass_lexicahzed_they parataxis_whmh_lexicahzed aux_whmh_to vmod_degree_whmh det_degree_the advmod_degree_only nn_language_target det_language_the prep_in_lextcahzed_language auxpass_lextcahzed_be aux_lextcahzed_can nsubjpass_lextcahzed_dtstmctton rcmod_sense_lextcahzed det_sense_a conj_but_not_degree dep_not_sense mark_not_whether dep_determine_degree dep_determine_not neg_determine_not aux_determine_do nsubj_determine_CIs parataxis_computes_determine nsubj_computes_Melamed ccomp_computes_z mark_computes_that dep_Melamed_1997 appos_entropy_Melamed amod_entropy_semanuc prep_to_similar_entropy advmod_s_However acomp_s_similar nsubj_s_~ mark_s_that nn_~_CI det_~_the ccomp_Note_s nsubj_Note_study tmod_Note_m dep_Note_used nsubj_Note_translations mark_Note_across det_study_the det_translations_the ccomp_consistent_Note neg_consistent_not cop_consistent_is nsubj_consistent_distinction mark_consistent_because det_distinction_the advcl_consadeied_consistent advmod_consadeied_together auxpass_consadeied_are nsubjpass_consadeied_senses nsubjpass_consadeied_adJective nn_senses_adverb prep_of_adJective_hard conj_and_adJective_senses det_adJective_The rcmod_z_consadeied det_z_the ccomp_Note_computes nsubj_Note_~ prep_with_Note_which nsubj_Note_consistency nn_~_word amod_~_same det_~_the prep_with_translated_~ vmod_s_translated nn_s_~ dep_z_s nn_se_ven prep_~_z appos_~_ls advmod_~_se nn_~_g det_~_a det_consistency_the conj_and_word_Note amod_word_same det_word_the prep_usmg_translated_Note prep_usmg_translated_word auxpass_translated_is nsubjpass_translated_pmr advmod_translated_often prep_of_pmr_senses det_pmr_each advmod_often_how ccomp_measures_translated nsubj_measures_that ccomp_computed_measures auxpass_computed_was nsubjpass_computed_index advcl_computed_determine appos_index_Cl nn_index_coherence det_index_a nn_eqmvalents_translation prep_to_correspond_eqmvalents nsubj_correspond_dlstlncttons prep_to_correspond_which nn_dlstlncttons_sense amod_dlstlncttons_assigned det_dlstlncttons_the rcmod_degree_correspond det_degree_the dobj_determine_degree aux_determine_to dep_determine_order mark_determine_In ccomp_corresponds_computed nsubj_corresponds_it prep_to_corresponds_which rcmod_sense_corresponds nn_sense_WordNet det_sense_the det_sense_th dep_~_sense nn_~_w dobj_associated_~ nsubjpass_associated_equivalent prep_of_case_derivatives det_case_the nn_form_toot det_form_the prep_of_lemma_form det_lemma_the conj_or_lemma_lemma nn_lemma_ts nn_lemma_~ conj_and_represented_associated prep_in_represented_case agent_represented_lemma agent_represented_lemma auxpass_represented_was nsubjpass_represented_equivalent nn_equivalent_translanon det_equivalent_each nn_purposes_comparison prep_for_word_purposes nn_word_Enghsh amod_word_corresponding det_word_the prep_for_equivalent_word nn_equivalent_translation det_equivalent_the parataxis_identified_associated parataxis_identified_represented prep_as_identified_equivalent auxpass_identified_be aux_identified_could amod_item_lexlcal amod_item_specific det_item_a dep_Table_1 nn_Table_m num_Table_2 conj_or_types_items conj_and_types_item conj_and_types_Table num_types_1 prep_to_corresponding_items prep_to_corresponding_item prep_to_corresponding_Table prep_to_corresponding_types vmod_occurrences_corresponding nn_occurrences_word nn_occurrences_Enghsh det_occurrences_the prep_of_%_occurrences num_%_85 quantmod_85_over prep_for_Table_% num_Table_1 nn_Table_m pobj_given_Table prep_dtstmctmns_given det_dtstmctmns_the prep_of_type_translatmn det_type_the dobj_indicate_type aux_indicate_to xcomp_asked_indicate auxpass_asked_were nsubjpass_asked_lmgmsts det_lmgmsts_the nn_form_root det_form_the prep_in_form_addttmn conj_and_form_form amod_form_inflected det_form_the preconj_form_both dobj_provide_form dobj_provide_form aux_provide_to xcomp_asked_provide auxpass_asked_were nsubjpass_asked_they mark_inflected_If nn_word_Enghsh amod_word_ambiguous det_word_the advcl_corresponds_inflected prep_to_corresponds_word nsubj_corresponds_that rcmod_sentence_corresponds amod_sentence_parallel det_sentence_each dep_m_sentence nn_m_item amod_m_lexlcal det_m_the dobj_provide_m aux_provide_to xcomp_asked_provide auxpass_asked_were nsubjpass_asked_hngmsts det_hngmsts_The nn_language_comparison det_language_the rcmod_speaker_asked prep_of_speaker_language nn_speaker_ve conj_and_llngmst_natl det_llngmst_a dep_sent_identified pobj_sent_dtstmctmns prepc_according_to_sent_to parataxis_sent_asked parataxis_sent_asked dobj_sent_speaker prep_to_sent_natl prep_to_sent_llngmst auxpass_sent_were nsubjpass_sent_corpus amod_sentences_parallel amod_sentences_sense-grouped prep_of_corpus_sentences det_corpus_the nn_languages_comparison num_languages_four det_languages_the prep_of_each_languages prep_for_estabhshed_each auxpass_estabhshed_was nsubjpass_estabhshed_set nn_assignments_sense prep_of_set_assignments amod_set_agreeable amod_set_final det_set_a advmod_agreeable_mutually parataxis_compared_sent conj_and_compared_estabhshed auxpass_compared_were nsubjpass_compared_results det_three_the prep_from_results_three nn_assistants_student num_assistants_two conj_and_author_assistants det_author_the parataxis_performed_estabhshed parataxis_performed_compared agent_performed_assistants agent_performed_author auxpass_performed_was nsubjpass_performed_categonzatmn nn_categonzatmn_sense det_categonzatmn_The num_\_1998 dep_Miller_\ dep_Miller_Fellbaum amod_Miller_1990 dep_Miller_al nn_Miller_et dep_\_Miller appos_\_version number_6_1 num_version_6 nn_WordNet_m nn_WordNet_distinctions nn_WordNet_sense det_WordNet_the conj_grouped_\ dobj_grouped_WordNet iobj_grouped_usmg advmod_grouped_then auxpass_grouped_were nsubjpass_grouped_w amod_occurrences_Enghsh det_occurrences_The num_occurrences_2 quantmod_2_separately dobj_grouped_occurrences auxpass_grouped_were nsubjpass_grouped_occmrences advmod_grouped_therefore prep_of_part_speech num_part_one num_part_54 quantmod_54_than mwe_than_more det_data_the prep_in_appemed_part prep_in_appemed_data nsubj_appemed_that rcmod_wolds_appemed prep_of_occmrences_wolds nn_guatmn_~ nn_guatmn_dlsamb amod_guatmn_semantic nn_guatmn_ot nn_guatmn_work det_guatmn_the prep_of_portion_guatmn amod_portion_good det_portion_a dobj_accomplishes_portion nsubj_accomplishes_tagging amod_tagging_t-of-speech nn_tagging_~ nn_tagging_pa ccomp_pointed_grouped ccomp_pointed_accomplishes prt_pointed_out aux_pointed_have nsubj_pointed_m dep_pointed_sentences appos_Stevenson_1998 conj_and_Walks_Stevenson appos_Czech_Slovene appos_Czech_Romantan appos_Czech_Estonian dep_languages_Czech nn_languages_comparison num_languages_four det_languages_the prep_as_ot_Stevenson prep_as_ot_Walks dobj_ot_languages nsubj_ot_texts det_texts_the rcmod_m_ot dobj_occur_m nsubj_occur_they dobj_occur_which rcmod_m_occur amod_sentences_parallel det_sentences_the dep_sentences_th number_sentences_~ ccomp_w_pointed advmod_w_together amod_text_Enghsh det_text_the parataxis_extracted_performed parataxis_extracted_grouped prep_from_extracted_text auxpass_extracted_were nsubjpass_extracted_conta num_words_nine det_words_the prep_of_each_words amod_variants_morphological pobj_Including_variants prep_of_occurrence_each dep_occurrence_Including conj_or_occurrence_occurrences det_occurrence_an dep_nmg_occurrences dep_nmg_occurrence nn_nmg_| dobj_conta_nmg num_sentences_forty-two num_sentences_hundred nn_sentences_ve num_sentences_~ nn_sentences_F nn_sentences_consideration dobj_warrant_sentences aux_warrant_to nn_text_Orwell det_text_the prep_in_enough_text advmod_enough_frequently xcomp_occurred_warrant advmod_occurred_enough nsubj_occurred_that rcmod_words_occurred dep_hmlted_extracted prep_to_hmlted_words advmod_hmlted_necessarily auxpass_hmlted_was nsubjpass_hmlted_study det_study_the det_cases_all amod_Kllgamff_forthcoming conj_and_Kllgamff_Palmer dep_exercise_Palmer dep_exercise_Kllgamff nn_exercise_dlsamblguatlon amod_exercise_Senseval det_exercise_the dep_m_exercise prep_in_used_cases dobj_used_m vmod_words_used det_words_the pobj_among_words pcomp_from_among parataxis_chosen_hmlted prep_chosen_from auxpass_chosen_were nsubjpass_chosen_five amod_five_latter det_five_the nn_studies_guatlon nn_studies_~ nn_studies_dlsamb amod_studies_other prep_in_used_studies auxpass_used_been aux_used_have nsubjpass_used_they mark_used_because parataxis_chosen_chosen advcl_chosen_used auxpass_chosen_were nsubjpass_chosen_four amod_four_first det_four_The dep_float_chosen conj_seize_float conj_seize_scrap nsubj_seize_head appos_head_shght conj_head_promise conj_head_hne conj_head_country parataxis_considered_seize advmod_considered_hard auxpass_considered_were nsubjpass_considered_words amod_words_English amod_words_ambiguous num_words_Nine dep_words_Prlest-Dorman nn_al_et amod_Prlest-Dorman_1997 appos_Prlest-Dorman_al parataxis_one-to-one_considered cop_one-to-one_are nsubj_one-to-one_sentence num_languages_seven prep_of_corpus_languages nn_corpus_pmallel amod_corpus_full det_corpus_the prep_in_alignments_corpus dep_sentence_alignments det_sentence_the amod_ot_% number_%_95 prep_for_original_instance det_original_the prep_to_faithful_original advmod_faithful_relatively cop_faithful_be aux_faithful_to xcomp_seem_faithful nsubj_seem_translations advmod_seem_Furthermore nsubj_seem_reports prep_as_seem_newspapers mwe_seem_such dep_seem_sub-domain det_text_the prep_of_translations_text det_translations_the dep_reports_etc amod_reports_technical conj_or_topic_seem amod_topic_given det_topic_a prep_to_tied_seem prep_to_tied_topic neg_tied_not dep_s_one-to-one prep_over_s_ot vmod_s_tied dobj_~_s dep_that_~ dep_language_that amod_language_ordinary appos_ot_language amod_ot_modern nn_ot_sample amod_ot_reasonable det_ot_a dobj_provides_ot nsubj_provides_it prep_as_provides_such conj_and_stylized_provides advmod_stylized_highly neg_stylized_not cop_stylized_IS nsubj_stylized_prose advcl_stylized_work poss_prose_Orwell prep_of_work_fiction det_work_a cop_work_is nsubj_work_Eighty-Four mark_work_Although num_Eighty-Four_Nineteen prep_of_part_speech dep_tagged_provides dep_tagged_stylized prep_for_tagged_part nsubjpass_tagged_versions det_English_the conj_and_sentence-aligned_tagged prep_to_sentence-aligned_English auxpass_sentence-aligned_are nsubjpass_sentence-aligned_versions det_text_the prep_of_versions_text amod_versions_parallel det_versions_The amod_languages_other det_languages_the rcmod_each_tagged rcmod_each_sentence-aligned prep_of_each_languages dep_m_each nn_m_English amod_m_original det_m_the prep_from_translated_m advmod_translated_directly vmod_words_translated num_words_100,000 quantmod_100,000_about prep_of_text_words det_text_a cop_text_Is nsubj_text_study dep_text_I nn_text_Czech num_Nmeteen_Eighty-Four dep_language_Nmeteen appos_language_Estoman amod_language_non-Indo-European num_language_one conj_and_Czech_Slovene appos_family_Slovene appos_family_Czech amod_family_same det_family_the prep_from_languages_family num_languages_two conj_and_Fmno-Ugrec_Romance amod_Fmno-Ugrec_Slavic amod_Fmno-Ugrec_Germanic dep_Russmn_Romance dep_Russmn_Fmno-Ugrec num_bmn_~ nn_bmn_Se npadvmod_Hungarian_m num_m_Four amod_gho_Hungarian num_gho_~ conj_and_Ntneteen-E_Russmn conj_and_Ntneteen-E_bmn conj_and_Ntneteen-E_Llthuaman conj_and_Ntneteen-E_Latwan conj_and_Ntneteen-E_Bulgarmn dep_Ntneteen-E_gho dep_o_Russmn dep_o_bmn dep_o_Llthuaman dep_o_Latwan dep_o_Bulgarmn dep_o_Ntneteen-E nn_o_ons num_o_| conj_and_vers_language appos_vers_languages dep_vers_o dobj_includes_language dobj_includes_vers advmod_includes_also nsubj_includes_corpus advmod_includes_~ dep_includes_O dep_includes_The amod_corpus_parallel advmod_parallel_well nn_families_language num_families_four dep_Involves_includes prep_from_Involves_families dobj_Involves_languages nsubj_Involves_therefole rcmod_study_Involves det_study_The conj_and_Enghsh_text conj_and_Enghsh_Romanlan conj_and_Enghsh_Estonian conj_and_Enghsh_Slovene dep_languages_text dep_languages_Romanlan dep_languages_Estonian dep_languages_Slovene dep_languages_Enghsh num_languages_five dep_m_languages dep_Euavec_1998 conj_and_Euavec_Ide dep_lr_m dep_lr_Ide dep_lr_Euavec appos_Etghtv-Fo_lr num_Etghtv-Fo_Nineteen poss_Etghtv-Fo_Orwell nn_Orwell_George dobj_ot_Etghtv-Fo nsubj_ot_versmns amod_versmns_aligned rcmod_parallel_ot dobj_using_parallel amod_study_small det_study_a xcomp_conducted_using dobj_conducted_study aux_conducted_have nsubj_conducted_I rcmod_Methodology_conducted num_Methodology_1 dep_studies_Methodology amod_studies_extensive advmod_extensive_more prep_for_basis_studies conj_and_basis_dlrectmn det_basis_the dobj_provide_dlrectmn dobj_provide_basis prep_as_provide_step aux_provide_can nsubj_provide_exammanon parataxis_provide_hmlted nsubj_provide_ways advmod_provide_ff amod_step_first det_step_a amod_data_parallel prep_of_sample_data amod_sample_small det_sample_a prep_of_exammanon_sample amod_exammanon_close advmod_hmlted_however advmod_hmlted_necessarily auxpass_hmlted_is nsubjpass_hmlted_study det_study_the amod_languages_multiple nn_texts_parallel nn_texts_lalge prep_across_lack_languages prep_of_lack_texts det_lack_the prep_given_used_lack auxpass_used_be aux_used_might nsubjpass_used_reformation prep_in_used_which amod_reformation_s nn_reformation_~ det_~_th rcmod_ways_used det_ways_the advmod_ff_so nn_distinctions_sense dobj_determine_distinctions aux_determine_to xcomp_vmble_determine nsubj_vmble_use prep_to_vmble_which nn_ts_data amod_ts_parallel prep_of_use_ts det_use_the rcmod_degree_vmble det_degree_the conj_and_determine_provide dobj_determine_degree advmod_determine_eventually aux_determine_to dep_determine_order mark_determine_In dep_outhned_provide dep_outhned_determine advmod_outhned_above dep_questions_outhned det_questions_the amod_answers_prehmlnary det_answers_some prep_to_provide_questions dobj_provide_answers aux_provide_to xcomp_attempts_provide nsubj_attempts_paper nn_paper_Thls nn_paper_etc rcmod_genre_attempts conj_genre_style conj_genre_domain agent_affected_genre auxpass_affected_Is nsubjpass_affected_lexlcahzatlon advmod_affected_how nn_distractions_sense prep_across_lexlcahzatlon_languages prep_of_lexlcahzatlon_distractions det_lexlcahzatlon_the neg_clear_not cop_clear_Is nsubj_clear_t advmod_clear_Also nsubj_clear_distinctions nn_t_~ nn_distinctions_sense ccomp_determine_affected ccomp_determine_clear advmod_determine_reliably aux_determine_to amod_data_appropriate amod_data_enough xcomp_provide_determine dobj_provide_data neg_provide_not aux_provide_do nsubj_provide_corpora mark_provide_that det_corpora_these ccomp_likely_provide cop_likely_is nsubj_likely_It dep_likely_Bible nn_press_m nn_al_et appos_Resnlk_press dep_Resnlk_al dep_Bible_Resnlk det_Bible_the dep_Ide_1994 conj_and_Ide_V6roms dep_corpus_V6roms dep_corpus_Ide nn_corpus_ston nn_corpus_~ dep_corpus_o dep_corpus_Journal det_Commt_the dep_o_Commt nn_Journal_MULTEXT det_Journal_the nn_al_et amod_Erjavec_1998 appos_Erjavec_al dep_Repubhc_Erjavec poss_Repubhc_Plato dep_Erjavec_1998 conj_and_Erjavec_Ide conj_and_Eighty-Four_likely conj_and_Eighty-Four_corpus conj_and_Eighty-Four_Repubhc dep_Eighty-Four_Ide dep_Eighty-Four_Erjavec nn_Eighty-Four_Nmeteen poss_Eighty-Four_Olwell cop_Eighty-Four_are nsubj_Eighty-Four_corpora num_languages_two quantmod_two_than mwe_than_more prep_for_corpora_languages nn_corpora_parallel amod_corpora_avadable amod_corpora_only det_corpora_The advmod_avadable_currently det_study_the det_languages_all parataxis_used_likely parataxis_used_corpus parataxis_used_Repubhc parataxis_used_Eighty-Four prep_in_used_study prep_over_used_languages auxpass_used_were nsubjpass_used_text mark_used_that amod_text_same det_text_the ccomp_preferable_used cop_preferable_be aux_preferable_would nsubj_preferable_t nn_t_~ prep_across_sense_languages amod_sense_same amod_sense_exact det_sense_the prep_in_used_sense auxpass_used_is nsubjpass_used_question mark_used_that nn_question_m nn_question_word det_question_the ccomp_ensure_used advmod_ensure_maximally aux_ensure_to nn_families_language amod_families_different amod_families_several conj_and_languages_preferable conj_and_languages_ensure prep_from_languages_families nn_languages_m nn_languages_texts amod_languages_parallel dobj_include_preferable dobj_include_ensure dobj_include_languages aux_include_would nsubj_include_evaluation advmod_include_Ideally poss_proposal_Yarowsky conj_and_Resnik_proposal prep_of_evaluation_proposal prep_of_evaluation_Resnik num_evaluation_53 amod_evaluation_serious det_evaluation_a dep_English_include advmod_English_often cop_English_is nsubj_English_one advmod_often_very prep_of_one_which rcmod_languages_English num_languages_two advmod_languages_only nn_languages_mvolwng cop_bl-texts_are nsubj_bl-texts_these appos_ot_languages rcmod_ot_bl-texts nn_ot_majority amod_ot_vast det_ot_The dobj_exist_ot nsubj_exist_ot amod_corpora_ahgned amod_corpora_parallel amod_corpora_few amod_corpora_present advmod_few_very det_dictionary_a amod_senses_classifying prep_of_actlvlty_senses nn_actlvlty_c nn_actlvlty_~ amod_actlvlty_meta-hngmst det_actlvlty_the prep_for_part_mclusmn prep_of_part_actlvlty det_part_a pobj_as_part conj_negcc_context_as nn_context_m nn_context_use poss_context_word det_word_a prep_at_ot_corpora advmod_ot_However prep_in_ot_dictionary dep_ot_as dep_ot_context rcmod_instance_exist det_instance_each dobj_evaluate_instance nsubj_evaluate_who rcmod_translatols_evaluate amod_translatols_experienced agent_determined_translatols dep_determined_e vmod_~_determined det_~_a dep_texts_~ nn_texts_parallel nn_texts_xn nn_texts_eqmvalents nn_texts_translatmn amod_dictionaries_muttt-hngual amod_dictionaries_bilingual conj_and_bilingual_muttt-hngual appos_corpma_texts prep_unlike_corpma_dictionaries amod_corpma_ahgned det_reformation_the prep_from_gather_parallel dobj_gather_reformation aux_gather_to xcomp_be_gather aux_be_would nsubj_be_alternative det_alternative_An rcmod_dictionaries_be amod_dictionaries_bl-hngual pobj_objections_dictionaries prepc_as_for_objections_for amod_objections_same det_objections_the prep_of_many_objections conj_subject_corpma prep_to_subject_many cop_subject_is csubj_subject_ts mark_subject_given nn_t_~ det_corpus_a neg_corpus_not appos_lexmon_t conj_and_lexmon_corpus det_lexmon_a dep_pttmatdy_corpus dep_pttmatdy_lexmon dobj_ts_pttmatdy nsubj_ts_EuroWordNet mark_ts_that prep_of_somce_mformatmn amod_somce_possible det_somce_a amod_Vossen_1998 dep_EutoWordNet_Vossen conj_but_suggest_subject prep_as_suggest_somce dobj_suggest_EutoWordNet nsubj_suggest_Yalowsky nsubj_suggest_Resmk appos_Yalowsky_1997 conj_and_Resmk_Yalowsky ccomp_make_subject ccomp_make_suggest nsubj_make_they rcmod_dlstmctmns_make nn_dlstmctmns_sense prep_of_kinds_dlstmctmns conj_and_kinds_degree det_kinds_the prep_of_terms_degree prep_of_terms_kinds nn_ctlonanes_~ nn_ctlonanes_d prep_among_vergence_ctlonanes nn_vergence_~ nn_vergence_d amod_vergence_substantial det_vergence_the prep_in_ven_terms dobj_ven_vergence nsubj_ven_~ nn_~_g nsubj_error-prone_ts parataxis_tedmus_ven conj_and_tedmus_error-prone advmod_tedmus_extremely cop_tedmus_be aux_tedmus_would nsubj_tedmus_ts amod_dictionaries_bdmgual dobj_Using_dictionaries prepc_from_come_Using aux_come_would nsubj_come_mformatlon advmod_come_where det_questmns_these dobj_answer_questmns aux_answer_to vmod_mformatlon_answer amod_mformatlon_cross-hngual det_mformatlon_the rcmod_ts_come nn_ts_conslderatmn det_ts_Another num_ts_9 rcmod_cases_error-prone rcmod_cases_tedmus prep_of_percentage_cases amod_percentage_certain det_percentage_a dep_m_percentage dep_exists_m nsubj_exists_option mark_exists_that amod_lexlcahzatlon_different det_lexlcahzatlon_a dep_ot_lexlcahzatlon dep_option_ot det_option_the ccomp_case_exists det_case_the cop_case_be advmod_case_only vmod_need_case dobj_need_tt nsubj_need_concepts amod_languages_other nn_languages_ot nn_languages_number amod_languages_slgmficant det_languages_some prep_in_items_languages amod_items_lexmal amod_items_non-lntetchangeable advmod_non-lntetchangeable_mutually conj_or_expressed_need agent_expressed_items auxpass_expressed_are nsubjpass_expressed_concepts mark_expressed_that num_concepts_two ccomp_mean_need ccomp_mean_expressed nsubj_mean_it aux_mean_Does dobj_mean_9 nsubj_mean_~ cop_9_be nsubj_9_stlnCtlOn aux_9_must amod_9_consistent nn_stlnCtlOn_~ nn_stlnCtlOn_d det_stlnCtlOn_the advmod_consistent_How advmod_~_stmally nn_~_cross-hngu amod_~_lexlcahzed cop_~_is nsubj_~_distinction mark_~_that nn_distinction_sense det_distinction_a ccomp_estabhsh_mean aux_estabhsh_to xcomp_used_estabhsh auxpass_used_be aux_used_would nsubjpass_used_that rcmod_crlterm_used det_crlterm_the dep_ot_crlterm nn_ot_questmn det_ot_the dobj_ts_ot advmod_ts_also expl_ts_There num_purpose_~ det_purpose_this nn_tot_lnfotmanon amod_tot_adequate dobj_provide_purpose dobj_provide_tot aux_provide_to dep_enough_be aux_be_would det_types_which pobj_of_types amod_languages_many num_languages_9 det_languages_the advmod_many_How number_9_two prep_of_mixture_languages nn_mixture_A nn_mixture_~ poss_mixture_famlhes nn_famlhes_language amod_famlhes_different prep_from_Languages_mixture amod_languages9_related num_languages9_9 nn_languages9_languages det_languages9_All num_languages9_9 nn_languages9_exermse det_languages9_this advmod_related_Closely dep_considered_ts xcomp_considered_provide dep_considered_enough conj_and_considered_of dep_considered_Languages prep_for_considered_languages9 auxpass_considered_be aux_considered_should nsubjpass_considered_languages advmod_considered_generally det_languages_which dep_generally_More vmod_9_of vmod_9_considered dep_languages_9 amod_languages_related nn_languages_m advmod_related_distantly advmod_distantly_more dobj_found_languages vmod_those_found prep_than_weight_those amod_weight_lesser cc_greater_or dep_lesser_greater dobj_given_weight auxpass_given_be nsubjpass_given_languages amod_languages_related nn_languages_m advmod_related_closely ccomp_found_given nsubj_found_differences aux_found_should nsubj_found_languages dep_found_interest dep_found_tntdrYt prep_for_found_example auxpass_overcome_be aux_overcome_can nsubjpass_overcome_problem det_problem_this ccomp_Assuming_overcome xcomp_related_Assuming advmod_related_closely cop_related_are nsubj_related_that advmod_closely_relatively rcmod_languages_related amod_languages_especmlly amod_interest_Enghsh det_interest_the conj_and_tntdrYt_interest amod_tntdrYt_French det_tntdrYt_the parataxis_preserved_found prep_across_preserved_languages auxpass_preserved_are nsubjpass_preserved_gumes mark_preserved_that nn_gumes_~ nn_gumes_amb amod_gumes_many ccomp_known_preserved advmod_known_well acomp_s_known nsubj_s_~ nn_~_t nn_~_~ rcmod_instance_s nn_instance_~ nn_instance_Fo dep_questions_instance amod_questions_several dobj_raises_questions dep_s_raises dobj_s_suggestmn nsubj_s_~ det_~_th det_word_a prep_of_senses_word amod_senses_different parataxis_dehmtt_s advmod_dehmtt_However dobj_dehmtt_senses nsubj_dehmtt_that rcmod_differences_dehmtt amod_differences_lexlcal amod_differences_gmficant dobj_~_differences nsubj_~_s det_s_the ccomp_fred_~ aux_fred_to xcomp_likely_fred cop_likely_be aux_likely_would nsubj_likely_we amod_languages_enough prep_at_look_languages nsubj_look_we mark_look_If advcl_monvatmn_look amod_monvatmn_conceptual det_monvatmn_a cop_monvatmn_be aux_monvatmn_must expl_monvatmn_there num_ways_more num_ways_two conj_or_two_more dep_m_ways nn_m_word det_m_a dobj_lexlcahzes_m nsubj_lexlcahzes_language mark_lexlcahzes_that det_language_another nn_language_ff ccomp_assumes_lexlcahzes nsubj_assumes_one advmod_word_mtumvely det_word_a prep_of_senses_word amod_senses_different dobj_determining_senses prepc_of_problem_determining det_problem_the nn_part_m amod_part_least det_answer_an parataxis_ovtde_likely parataxis_ovtde_monvatmn rcmod_ovtde_assumes prep_to_ovtde_problem prep_at_ovtde_part dobj_ovtde_answer nn_~_p dep_seem_ovtde prep_to_seem_~ aux_seem_would nsubj_seem_idea det_idea_This det_languages_those prep_of_subset_languages amod_subset_minimum det_subset_some ccomp_reahzed_seem prep_in_reahzed_subset advmod_reahzed_lexlcally auxpass_reahzed_are nsubjpass_reahzed_that rcmod_those_reahzed prep_to_restricted_those auxpass_restricted_be nsubjpass_restricted_stmctmns mark_restricted_that conj_and_appllcatmns_evaluatmn nn_appllcatmns_processing nn_appllcatmns_language prep_for_considered_evaluatmn prep_for_considered_appllcatmns auxpass_considered_be aux_considered_to vmod_stmctmns_considered nn_stmctmns_~ nn_stmctmns_d nn_stmctmns_sense det_stmctmns_the conj_and_~_restricted dep_~_fied nn_~_dent amod_~_~ cop_~_be nsubj_~_set mark_~_that nn_languages_target prep_of_set_languages det_set_some ccomp_propose_restricted ccomp_propose_~ nsubj_propose_they prep_in_lextcahzed_particular advmod_lextcahzed_cross-hngmstlcally auxpass_lextcahzed_are nsubjpass_lextcahzed_that rcmod_stmctmns_lextcahzed nn_stmctmns_~ nn_stmctmns_d nn_stmctmns_sense advmod_sense_only dobj_considering_stmctmns parataxis_detelmlned_propose agent_detelmlned_considering auxpass_detelmlned_be aux_detelmlned_could nsubjpass_detelmlned_~ det_detelmlned_a dobj_~_d aux_~_wo prep_of_senses_detelmlned amod_senses_different det_senses_the dobj_ot_WSD nsubj_ot_purposes det_purposes_the appos_fol_senses rcmod_fol_ot prep_that_suggested_fol nsubj_suggested_relatmns appos_Yarowsky_1997 conj_and_Resnlk_Yarowsky appos_relatmns_Yarowsky appos_relatmns_Resnlk advmod_relatmns_Recently amod_relatmns_translatmnal amod_relatmns_such rcmod_fiom_suggested appos_etc_fiom nn_etc_hyponymy prep_as_relatmnshlps_etc mwe_relatmnshlps_such nn_relatmnshlps_semantm nn_relatmnshlps_captunng dep_eg_lexemes dep_signs_eg appos_representations_relatmnshlps prep_for_representations_signs amod_representations_otsemantic nn_representations_derivation det_representations_a dobj_suggested_representations amod_helds_semantic conj_and_synonymy_helds conj_and_synonymy_vagueness conj_and_synonymy_ambtgmty prep_as_propemes_helds prep_as_propemes_vagueness prep_as_propemes_ambtgmty prep_as_propemes_synonymy mwe_propemes_such amod_propemes_semantic dobj_define_propemes aux_define_to nn_Umverslty_Oslo appos_ENPC_Umverslty nn_ENPC_corpus dep_paralle_ENPC amod_paralle_EnghshNorwegian det_paralle_an amod_relatmns_translational vmod_patterns_define prep_in_patterns_paralle prep_of_patterns_relatmns amod_patterns_used nn_patterns_Dywk appos_Dywk_1998 amod_work_related amod_texts_new prep_in_guate_work dobj_guate_texts conj_and_~_suggested conj_and_~_patterns vmod_~_guate dobj_dtsamb_suggested dobj_dtsamb_patterns dobj_dtsamb_~ aux_dtsamb_to xcomp_used_dtsamb advmod_used_then ccomp_ts_used nsubj_ts_which rcmod_senses_ts amod_senses_different det_senses_the nn_data_co-occurrence prep_for_gather_senses dobj_gather_data aux_gather_to vmod_order_gather nn_order_m nn_order_lnformatmn dobj_s_order nsubj_s_~ det_~_th dep_exploit_s nsubj_exploit_studies det_studies_These ccomp_identified_exploit auxpass_identified_is nsubjpass_identified_duty nn_word_Enghsh det_word_the prep_of_sense_word amod_sense_correct det_sense_the amod_text_French amod_text_parallel det_text_a appos_duty_sense prep_in_duty_text nn_duty_ot nn_duty_eqmvalent parataxis_~_identified
C02-1003	J97-3002	o	1 A bilingual language model ITG Wu -LRB- 1997 -RRB- has proposed a bilingual language model called Inversion Transduction Grammar -LRB- ITG -RRB- which can be used to parse bilingual sentence pairs simultaneously	nn_pairs_sentence amod_pairs_bilingual advmod_parse_simultaneously dobj_parse_pairs aux_parse_to xcomp_used_parse auxpass_used_be aux_used_can nsubjpass_used_which appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion dep_called_Grammar rcmod_model_used vmod_model_called nn_model_language amod_model_bilingual det_model_a dobj_proposed_model aux_proposed_has nsubj_proposed_Wu appos_Wu_1997 nn_Wu_ITG nn_Wu_model nn_Wu_language amod_Wu_bilingual det_Wu_A num_Wu_1 ccomp_``_proposed
C02-1003	J97-3002	o	For details please refer to -LRB- Wu 1995 Wu 1997 -RRB-	num_Wu_1997 nn_Wu_Wu num_Wu_1995 prep_to_refer_Wu dep_please_refer dep_For_please pobj_For_details dep_``_For
C02-1003	J97-3002	o	S BNP VP PP VP Mr. / g1820g10995 Wu/g2568 plays/g6183 basketball/g12738g10711 on/e Sunday/g7155g7411g3837 S / g452 Figure 1 Inversion transduction Grammar parsing Any ITG can be converted to a normal form where all productions are either lexical productions or binary-fanout nonterminal productions -LRB- Wu 1997 -RRB-	num_Wu_1997 amod_productions_nonterminal amod_productions_binary-fanout dep_productions_Wu conj_or_productions_productions amod_productions_lexical advmod_productions_either cop_productions_are nsubj_productions_productions advmod_productions_where det_productions_all rcmod_form_productions rcmod_form_productions amod_form_normal det_form_a prep_to_converted_form auxpass_converted_be aux_converted_can nsubjpass_converted_S det_ITG_Any dobj_parsing_ITG vmod_Grammar_parsing nn_Grammar_transduction nn_Grammar_Inversion num_Grammar_1 dep_Figure_Grammar nn_Figure_g452 dep_S_Figure nn_S_Sunday/g7155g7411g3837 amod_S_on/e nn_S_basketball/g12738g10711 nn_S_plays/g6183 nn_S_Wu/g2568 nn_S_g1820g10995 nn_S_Mr. nn_S_VP nn_S_PP nn_S_VP nn_S_BNP nn_S_S
C02-1003	J97-3002	p	Because the expressiveness characteristics of ITG naturally constrain the space of possible matching in a highly appropriate fashion BTG achieves encouraging results for bilingual bracketing using a word-translation lexicon alone -LRB- Wu 1997 -RRB-	num_Wu_1997 advmod_lexicon_alone nn_lexicon_word-translation det_lexicon_a dobj_using_lexicon amod_bracketing_bilingual dep_results_Wu vmod_results_using prep_for_results_bracketing amod_results_encouraging dobj_achieves_results nsubj_achieves_BTG advcl_achieves_constrain amod_fashion_appropriate det_fashion_a advmod_appropriate_highly amod_matching_possible prep_of_space_matching det_space_the prep_in_constrain_fashion dobj_constrain_space advmod_constrain_naturally nsubj_constrain_characteristics mark_constrain_Because prep_of_characteristics_ITG nn_characteristics_expressiveness det_characteristics_the
C02-1003	J97-3002	o	The optimal bilingual parsing tree for a given sentence-pair can be computed using dynamic programming -LRB- DP -RRB- algorithm -LRB- Wu 1997 -RRB-	num_Wu_1997 appos_algorithm_Wu nn_algorithm_DP dep_programming_algorithm amod_programming_dynamic dobj_using_programming xcomp_computed_using auxpass_computed_be aux_computed_can nsubjpass_computed_tree amod_sentence-pair_given det_sentence-pair_a prep_for_tree_sentence-pair nn_tree_parsing amod_tree_bilingual amod_tree_optimal det_tree_The
C02-1010	J97-3002	o	415-458 Wu Dekai -LRB- 1997 -RRB- Stochastic inversion transduction grammars and bilingual parsing of parallel corpora	amod_corpora_parallel prep_of_parsing_corpora amod_parsing_bilingual nn_grammars_transduction nn_grammars_inversion nn_grammars_Stochastic nn_grammars_Dekai appos_Dekai_1997 conj_and_Wu_parsing conj_and_Wu_grammars dep_415-458_parsing dep_415-458_grammars dep_415-458_Wu ccomp_``_415-458
C02-1010	J97-3002	o	To deal with the difficulties in parse-to-parse matching Wu -LRB- 1997 -RRB- utilizes inversion transduction grammar -LRB- ITG -RRB- for bilingual parsing	amod_parsing_bilingual prep_for_grammar_parsing appos_grammar_ITG nn_grammar_transduction nn_grammar_inversion dobj_utilizes_grammar csubj_utilizes_deal appos_Wu_1997 appos_matching_Wu amod_matching_parse-to-parse prep_in_difficulties_matching det_difficulties_the prep_with_deal_difficulties aux_deal_To ccomp_``_utilizes
C02-1010	J97-3002	o	2.2 The Crossing Constraint According to -LRB- Wu 1997 -RRB- crossing constraint can be defined in the following	amod_the_following prep_in_defined_the auxpass_defined_be aux_defined_can nsubjpass_defined_constraint amod_constraint_crossing rcmod_Wu_defined amod_Wu_1997 pobj_Constraint_Wu prepc_according_to_Constraint_to nn_Constraint_Crossing det_Constraint_The num_Constraint_2.2 dep_``_Constraint
C04-1005	J97-3002	o	In addition Wu -LRB- 1997 -RRB- used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments	nn_alignments_phrase conj_or_word_alignments det_word_the dobj_get_alignments dobj_get_word aux_get_to nn_pairs_sentence det_pairs_the vmod_parse_get dobj_parse_pairs advmod_parse_simultaneously aux_parse_to nn_grammar_transduction det_grammar_a nn_transduction_inversion amod_transduction_stochastic xcomp_used_parse dobj_used_grammar nsubj_used_Wu prep_in_used_addition appos_Wu_1997
C04-1006	J97-3002	o	Bilingual bracketing methods were used to produce a word alignment in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu prep_alignment_in nn_alignment_word det_alignment_a dobj_produce_alignment aux_produce_to xcomp_used_produce auxpass_used_were nsubjpass_used_methods nn_methods_bracketing amod_methods_Bilingual
C04-1030	J97-3002	o	3.2 ITG Constraints In this section we describe the ITG constraints -LRB- Wu 1995 Wu 1997 -RRB-	amod_Wu_1997 dep_Wu_Wu appos_Wu_1995 appos_constraints_Wu nn_constraints_ITG det_constraints_the dobj_describe_constraints nsubj_describe_we nsubj_describe_Constraints det_section_this prep_in_Constraints_section nn_Constraints_ITG num_Constraints_3.2
C04-1032	J97-3002	o	Bilingual bracketing methods were used to produce a word alignment in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu prep_alignment_in nn_alignment_word det_alignment_a dobj_produce_alignment aux_produce_to xcomp_used_produce auxpass_used_were nsubjpass_used_methods nn_methods_bracketing amod_methods_Bilingual
C04-1060	J97-3002	o	Wu -LRB- 1997 -RRB- modeled the reordering process with binary branching trees where each production could be either in the same or in reverse order going from source to target language	dobj_target_language aux_target_to prep_from_going_source nsubj_going_order amod_order_reverse conj_or_same_going det_same_the xcomp_be_target prep_in_be_going prep_in_be_same preconj_be_either aux_be_could nsubj_be_production advmod_be_where det_production_each rcmod_trees_be amod_trees_branching amod_trees_binary prep_with_process_trees nn_process_reordering det_process_the dobj_modeled_process nsubj_modeled_Wu appos_Wu_1997
C04-1060	J97-3002	o	Zens and Ney -LRB- 2003 -RRB- compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5 and then measure how many of the resulting alignments fall within the hard constraints of both Wu -LRB- 1997 -RRB- and Berger et al.	nn_al._et dep_Wu_al. conj_and_Wu_Berger appos_Wu_1997 dep_both_Berger dep_both_Wu prep_of_constraints_both amod_constraints_hard det_constraints_the prep_within_fall_constraints nsubj_fall_many amod_alignments_resulting det_alignments_the prep_of_many_alignments advmod_many_how ccomp_measure_fall advmod_measure_then num_Model_5 nn_Model_IBM conj_and_using_measure dobj_using_Model nn_pairs_sentences amod_pairs_French-English amod_pairs_German-English conj_and_German-English_French-English nn_alignments_viterbi det_alignments_the dep_compute_measure dep_compute_using prep_for_compute_pairs dobj_compute_alignments nsubj_compute_Ney nsubj_compute_Zens appos_Ney_2003 conj_and_Zens_Ney
C04-1060	J97-3002	o	This gives the translation model more information about the structure of the source language and further constrains the reorderings to match not just a possible bracketing as in Wu -LRB- 1997 -RRB- but the specific bracketing of the parse tree provided	vmod_tree_provided nn_tree_parse det_tree_the prep_of_bracketing_tree amod_bracketing_specific det_bracketing_the appos_Wu_1997 pobj_in_Wu pcomp_as_in amod_bracketing_possible det_bracketing_a advmod_bracketing_just neg_bracketing_not prep_match_as dobj_match_bracketing aux_match_to det_reorderings_the vmod_constrains_match dobj_constrains_reorderings advmod_constrains_further nsubj_constrains_This nn_language_source det_language_the prep_of_structure_language det_structure_the prep_about_information_structure amod_information_more nn_model_translation det_model_the conj_but_gives_bracketing conj_and_gives_constrains dobj_gives_information iobj_gives_model nsubj_gives_This
C04-1060	J97-3002	o	In this paper we make a direct comparison of a syntactically unsupervised alignment model based on Wu -LRB- 1997 -RRB- with a syntactically supervised model based on Yamada and Knight -LRB- 2001 -RRB-	appos_Knight_2001 conj_and_Yamada_Knight prep_on_based_Knight prep_on_based_Yamada vmod_model_based amod_model_supervised det_model_a advmod_supervised_syntactically appos_Wu_1997 prep_with_based_model prep_on_based_Wu nn_model_alignment amod_model_unsupervised advmod_model_syntactically det_model_a vmod_comparison_based prep_of_comparison_model amod_comparison_direct det_comparison_a dobj_make_comparison nsubj_make_we prep_in_make_paper det_paper_this
C04-1060	J97-3002	o	2 The Inversion Transduction Grammar The Inversion Transduction Grammar of Wu -LRB- 1997 -RRB- can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions	nn_productions_grammar amod_productions_context-free amod_productions_synchronous prep_of_series_productions det_series_a det_languages_both prep_in_strings_languages prep_through_produces_series dobj_produces_strings advmod_produces_simultaneously nsubj_produces_which rcmod_process_produces amod_process_generative det_process_a dep_a_process prep_as_thought_a auxpass_thought_be aux_thought_can nsubjpass_thought_Grammar appos_Wu_1997 prep_of_Grammar_Wu nn_Grammar_Transduction nn_Grammar_Inversion nn_Grammar_The rcmod_Grammar_thought dep_Transduction_Grammar dep_Inversion_Transduction dep_The_Inversion dep_2_The
C04-1060	J97-3002	o	In our experiments we use a grammar with a start symbol S a single preterminal C and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment -LRB- ignoring insertions and deletions -RRB- -LRB- Wu 1997 Zens and Ney 2003 -RRB-	amod_Zens_2003 conj_and_Zens_Ney dep_Wu_Ney dep_Wu_Zens appos_Wu_1997 conj_and_insertions_deletions dobj_ignoring_deletions dobj_ignoring_insertions amod_alignment_word-level amod_alignment_given det_alignment_any dobj_generate_alignment aux_generate_can nsubj_generate_parse mark_generate_that num_parse_one quantmod_one_only ccomp_ensure_generate aux_ensure_to xcomp_used_ensure conj_and_A_B vmod_nonterminals_used dep_nonterminals_B dep_nonterminals_A num_nonterminals_two amod_C_preterminal amod_C_single det_C_a conj_and_S_nonterminals appos_S_C nn_S_symbol nn_S_start det_S_a det_grammar_a dep_use_Wu parataxis_use_ignoring prep_with_use_nonterminals prep_with_use_S dobj_use_grammar nsubj_use_we prep_in_use_experiments poss_experiments_our
C04-1060	J97-3002	o	The trees may be learned directly from parallel corpora -LRB- Wu 1997 -RRB- or provided by a parser trained on hand-annotated treebanks -LRB- Yamada and Knight 2001 -RRB-	amod_Yamada_2001 conj_and_Yamada_Knight dep_treebanks_Knight dep_treebanks_Yamada amod_treebanks_hand-annotated prep_on_trained_treebanks vmod_parser_trained det_parser_a prep_by_provided_parser dep_Wu_1997 conj_or_corpora_provided appos_corpora_Wu amod_corpora_parallel prep_from_learned_provided prep_from_learned_corpora advmod_learned_directly auxpass_learned_be aux_learned_may nsubjpass_learned_trees det_trees_The
C04-1060	J97-3002	o	Inversion Transduction Grammar -LRB- ITG -RRB- is the model of Wu -LRB- 1997 -RRB- Tree-to-String is the model of Yamada and Knight -LRB- 2001 -RRB- and Tree-to-String Clone allows the node cloning operation described above	advmod_described_above vmod_operation_described nn_operation_cloning nn_operation_node det_operation_the dobj_allows_operation nsubj_allows_Clone appos_Knight_2001 conj_and_Yamada_Knight rcmod_model_allows conj_and_model_Tree-to-String prep_of_model_Knight prep_of_model_Yamada det_model_the cop_model_is nsubj_model_model appos_Wu_Tree-to-String appos_Wu_1997 prep_of_model_Wu det_model_the cop_model_is nsubj_model_Grammar appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion
C04-1134	J97-3002	o	Inspired by previous work on syntax-driven semantic parsing -LRB- Gildea and Jurafsky 2002 Fleischman et al. 2003 -RRB- and syntax-based machine translation -LRB- Wu 1997 Cuerzan and Yarowsky 2002 -RRB- we postulate that syntactically similar sentences with the same predicate also share similar semantic roles	amod_roles_semantic amod_roles_similar dobj_share_roles advmod_share_also nsubj_share_sentences mark_share_that amod_predicate_same det_predicate_the prep_with_sentences_predicate amod_sentences_similar advmod_sentences_syntactically ccomp_postulate_share nsubj_postulate_we dep_Cuerzan_2002 conj_and_Cuerzan_Yarowsky dep_Wu_Yarowsky dep_Wu_Cuerzan appos_Wu_1997 dep_translation_Wu nn_translation_machine amod_translation_syntax-based cc_translation_and num_Fleischman_2003 nn_Fleischman_al. nn_Fleischman_et dep_Gildea_Fleischman dep_Gildea_2002 conj_and_Gildea_Jurafsky appos_parsing_Jurafsky appos_parsing_Gildea amod_parsing_semantic amod_parsing_syntax-driven prep_on_work_parsing amod_work_previous ccomp_Inspired_postulate dep_Inspired_translation prep_by_Inspired_work ccomp_``_Inspired
C08-1127	J97-3002	o	The straight-forward way is to first generate the best BTG tree for each sentence pair using the way of -LRB- Wu 1997 -RRB- then annotate each BTG node with linguistic elements by projecting source-side syntax tree to BTG tree and finally extract rules from these annotated BTG trees	nn_trees_BTG amod_trees_annotated det_trees_these prep_from_rules_trees nn_rules_extract advmod_rules_finally nn_tree_BTG nn_tree_syntax amod_tree_source-side prep_to_projecting_tree dobj_projecting_tree amod_elements_linguistic nn_node_BTG det_node_each conj_and_annotate_rules prepc_by_annotate_projecting prep_with_annotate_elements nsubj_annotate_node advmod_annotate_then ccomp_annotate_is amod_Wu_1997 dep_of_Wu prep_way_of det_way_the dobj_using_way nn_pair_sentence det_pair_each prep_for_tree_pair nn_tree_BTG amod_tree_best det_tree_the xcomp_generate_using dobj_generate_tree advmod_generate_first aux_generate_to xcomp_is_generate nsubj_is_way amod_way_straight-forward det_way_The
C08-1127	J97-3002	o	1 Introduction Formal grammar used in statistical machine translation -LRB- SMT -RRB- such as Bracketing Transduction Grammar -LRB- BTG -RRB- proposed by -LRB- Wu 1997 -RRB- and the synchronous CFG presented by -LRB- Chiang 2005 -RRB- provides a natural platform for integrating linguistic knowledge into SMT because hierarchical structures produced by the formal grammar resemble linguistic structures	amod_structures_linguistic dobj_resemble_structures nsubj_resemble_structures mark_resemble_because amod_grammar_formal det_grammar_the agent_produced_grammar vmod_structures_produced amod_structures_hierarchical amod_knowledge_linguistic advcl_integrating_resemble prep_into_integrating_SMT dobj_integrating_knowledge amod_platform_natural det_platform_a prepc_for_provides_integrating dobj_provides_platform nsubj_provides_grammar amod_Chiang_2005 dep_by_Chiang prep_presented_by vmod_CFG_presented amod_CFG_synchronous det_CFG_the conj_and_Wu_CFG num_Wu_1997 agent_proposed_CFG agent_proposed_Wu vmod_Grammar_proposed appos_Grammar_BTG nn_Grammar_Transduction dobj_Bracketing_Grammar appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_used_translation prepc_such_as_grammar_Bracketing vmod_grammar_used amod_grammar_Formal nn_grammar_Introduction num_grammar_1
C08-1138	J97-3002	o	Many grammars such as finite-state grammars -LRB- FSG -RRB- bracket/inversion transduction grammars -LRB- BTG/ITG -RRB- -LRB- Wu 1997 -RRB- context-free grammar -LRB- CFG -RRB- tree substitution grammar -LRB- TSG -RRB- -LRB- Comon et al. 2007 -RRB- and their synchronous versions have been explored in SMT	prep_in_explored_SMT auxpass_explored_been aux_explored_have nsubjpass_explored_grammars amod_versions_synchronous poss_versions_their amod_Comon_2007 dep_Comon_al. nn_Comon_et appos_grammar_TSG nn_grammar_substitution nn_grammar_tree appos_grammar_CFG amod_grammar_context-free dep_Wu_1997 appos_grammars_Wu appos_grammars_BTG/ITG nn_grammars_transduction nn_grammars_bracket/inversion conj_and_grammars_versions dep_grammars_Comon conj_and_grammars_grammar conj_and_grammars_grammar conj_and_grammars_grammars appos_grammars_FSG amod_grammars_finite-state prep_such_as_grammars_versions prep_such_as_grammars_grammar prep_such_as_grammars_grammar prep_such_as_grammars_grammars prep_such_as_grammars_grammars amod_grammars_Many
C08-2026	J97-3002	o	Coling 2008 Companion volume Posters and Demonstrations pages 103106 Manchester August 2008 Range concatenation grammars for translation Anders Sgaard University of Potsdam soegaard@ling.uni-potsdam.de Abstract Positive and bottom-up non-erasing binary range concatenation grammars -LRB- Boullier 1998 -RRB- with at most binary predicates -LRB- -LRB- 2,2 -RRB- BRCGs -RRB- is a O -LRB- | G | n6 -RRB- time strict extension of inversion transduction grammars -LRB- Wu 1997 -RRB- -LRB- ITGs -RRB-	dep_Wu_1997 appos_grammars_ITGs appos_grammars_Wu nn_grammars_transduction nn_grammars_inversion prep_of_extension_grammars amod_extension_strict nn_extension_time num_n6_| nn_n6_G num_n6_| dep_O_extension appos_O_n6 det_O_a cop_O_is nsubj_O_2008 dep_predicates_BRCGs appos_predicates_2,2 amod_predicates_binary amod_predicates_most pobj_at_predicates pcomp_with_at amod_Boullier_1998 dep_grammars_Boullier nn_grammars_concatenation nn_grammars_range amod_grammars_binary amod_grammars_non-erasing amod_grammars_bottom-up amod_grammars_Positive conj_and_Positive_bottom-up dep_Abstract_grammars nn_Abstract_soegaard@ling.uni-potsdam.de appos_University_Abstract prep_of_University_Potsdam nn_University_Sgaard nn_University_Anders nn_University_translation prep_for_grammars_University nn_grammars_concatenation nn_grammars_Range num_grammars_2008 nn_grammars_August prep_Manchester_with appos_Manchester_grammars num_Manchester_103106 dep_pages_Manchester appos_Posters_pages conj_and_Posters_Demonstrations nn_Posters_volume nn_Posters_Companion dep_2008_Demonstrations dep_2008_Posters amod_2008_Coling ccomp_``_O
C08-2026	J97-3002	o	It is shown that -LRB- 2,2 -RRB- BRCGs induce inside-out alignments -LRB- Wu 1997 -RRB- and cross-serial discontinuous translation units -LRB- CDTUs -RRB- both phenomena can be shown to occur frequently in many hand-aligned parallel corpora	nn_corpora_parallel amod_corpora_hand-aligned amod_corpora_many prep_in_occur_corpora advmod_occur_frequently aux_occur_to xcomp_shown_occur auxpass_shown_be aux_shown_can nsubjpass_shown_phenomena preconj_phenomena_both appos_units_CDTUs nn_units_translation amod_units_discontinuous amod_units_cross-serial dep_Wu_1997 conj_and_alignments_units appos_alignments_Wu amod_alignments_inside-out parataxis_induce_shown dobj_induce_units dobj_induce_alignments nsubj_induce_BRCGs advmod_induce_that dep_that_2,2 dep_shown_induce auxpass_shown_is nsubjpass_shown_It
C08-2026	J97-3002	o	ITGs translate into simple -LRB- 2,2 -RRB- BRCGs in the following way see Wu -LRB- 1997 -RRB- for a definition of ITGs	prep_of_definition_ITGs det_definition_a prep_for_Wu_definition appos_Wu_1997 dobj_see_Wu amod_way_following det_way_the dep_simple_BRCGs dep_simple_2,2 parataxis_translate_see prep_in_translate_way prep_into_translate_simple nsubj_translate_ITGs
C08-2026	J97-3002	n	Inside-out alignments -LRB- Wu 1997 -RRB- such as the one in Example 1.3 can not be induced by any of these theories in fact there seems to be no useful synchronous grammar formalisms available that handle inside-out alignments with the possible exceptions of synchronous tree-adjoining grammars -LRB- Shieber and Schabes 1990 -RRB- Bertsch and Nederhof -LRB- 2001 -RRB- and generalized multitext grammars -LRB- Melamed et al. 2004 -RRB- which are all way more complex than ITG STSG and -LRB- 2,2 -RRB- BRCG	dep_ITG_BRCG conj_and_ITG_2,2 conj_and_ITG_STSG prep_than_complex_2,2 prep_than_complex_STSG prep_than_complex_ITG advmod_complex_more amod_way_complex det_way_all cop_way_are nsubj_way_which amod_Melamed_2004 dep_Melamed_al. nn_Melamed_et dep_grammars_Melamed amod_grammars_multitext amod_grammars_generalized appos_Nederhof_2001 conj_and_Bertsch_Nederhof dep_Shieber_1990 conj_and_Shieber_Schabes rcmod_grammars_way conj_and_grammars_grammars appos_grammars_Nederhof appos_grammars_Bertsch appos_grammars_Schabes appos_grammars_Shieber amod_grammars_tree-adjoining amod_grammars_synchronous prep_of_exceptions_grammars prep_of_exceptions_grammars amod_exceptions_possible det_exceptions_the amod_alignments_inside-out prep_with_handle_exceptions dobj_handle_alignments nsubj_handle_that dep_available_handle amod_formalisms_available nn_formalisms_grammar amod_formalisms_synchronous amod_formalisms_useful neg_formalisms_no cop_formalisms_be aux_formalisms_to xcomp_seems_formalisms expl_seems_there prep_in_seems_fact det_theories_these prep_of_any_theories parataxis_induced_seems agent_induced_any auxpass_induced_be neg_induced_not aux_induced_can nsubjpass_induced_alignments num_Example_1.3 prep_in_one_Example det_one_the dep_Wu_1997 prep_such_as_alignments_one appos_alignments_Wu amod_alignments_Inside-out
D07-1006	J97-3002	o	In designing LEAF we were also inspired by dependency-based alignment models -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knight 2001 Cherry and Lin 2003 Zhang and Gildea 2004 -RRB-	amod_Zhang_2004 conj_and_Zhang_Gildea conj_and_Cherry_2003 conj_and_Cherry_Lin nn_al._et nn_al._Alshawi dep_Wu_Gildea dep_Wu_Zhang dep_Wu_2003 dep_Wu_Lin dep_Wu_Cherry num_Wu_2001 conj_and_Wu_Knight conj_and_Wu_Yamada num_Wu_2000 dep_Wu_al. amod_Wu_1997 appos_models_Knight appos_models_Yamada appos_models_Wu nn_models_alignment amod_models_dependency-based agent_inspired_models advmod_inspired_also auxpass_inspired_were nsubjpass_inspired_we prepc_in_inspired_designing dobj_designing_LEAF
D07-1030	J97-3002	o	SMT has evolved from the original word-based approach -LRB- Brown et al. 1993 -RRB- into phrase-based approaches -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and syntax-based approaches -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knignt 2001 Chiang 2005 -RRB-	amod_Chiang_2005 dep_Yamada_Chiang conj_and_Yamada_2001 conj_and_Yamada_Knignt nn_al._et nn_al._Alshawi dep_Wu_2001 dep_Wu_Knignt dep_Wu_Yamada num_Wu_2000 dep_Wu_al. num_Wu_1997 dep_approaches_Wu amod_approaches_syntax-based dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_approaches_phrase-based dep_al._1993 nn_al._et amod_al._Brown dep_approach_al. amod_approach_word-based amod_approach_original det_approach_the conj_and_evolved_approaches dep_evolved_Koehn prep_into_evolved_approaches prep_from_evolved_approach aux_evolved_has nsubj_evolved_SMT ccomp_``_approaches ccomp_``_evolved
D07-1038	J97-3002	o	Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree similar to the bilingual parsing of Wu -LRB- 1997 -RRB- 1	dep_1_1997 num_Wu_1 prep_of_parsing_Wu amod_parsing_bilingual det_parsing_the prep_to_similar_parsing amod_tree_English amod_string_similar conj_and_string_tree amod_string_foreign det_string_the prep_on_constrained_tree prep_on_constrained_string advmod_constrained_simultaneously vmod_parser_constrained nn_parser_CKY det_parser_a nn_chart_parse det_chart_a prep_with_Construct_parser dobj_Construct_chart
D07-1091	J97-3002	o	The goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models -LRB- Wu 1997 Alshawi et al. 1998 Yamada and Knight 2001 Melamed 2004 Menezes and Quirk 2005 Galley et al. 2006 -RRB- with increasingly encouraging results	amod_results_encouraging advmod_encouraging_increasingly nn_al._et nn_al._Galley conj_and_Menezes_Quirk num_Melamed_2004 conj_and_Yamada_Knight nn_al._et nn_al._Alshawi amod_Wu_2006 dep_Wu_al. amod_Wu_2005 dep_Wu_Quirk dep_Wu_Menezes dep_Wu_Melamed amod_Wu_2001 dep_Wu_Knight dep_Wu_Yamada num_Wu_1998 dep_Wu_al. amod_Wu_1997 appos_models_Wu nn_models_transfer amod_models_tree-based dobj_pursue_models aux_pursue_to amod_researchers_many prep_with_prompted_results xcomp_prompted_pursue dobj_prompted_researchers aux_prompted_has nsubj_prompted_goal nn_model_translation det_model_the amod_information_syntactic prep_into_integrating_model dobj_integrating_information prepc_of_goal_integrating det_goal_The
D08-1012	J97-3002	o	108 To follow related work and to focus on the effects of the language model we present translation resultsunderaninversiontransductiongrammar -LRB- ITG -RRB- translation model -LRB- Wu 1997 -RRB- trained on the Europarl corpus -LRB- Koehn 2005 -RRB- described in detail in Section 3 and using a trigram language model	nn_model_language nn_model_trigram det_model_a dobj_using_model nsubj_using_we num_Section_3 prep_in_described_Section prep_in_described_detail amod_Koehn_2005 appos_corpus_Koehn nn_corpus_Europarl det_corpus_the prep_on_trained_corpus dep_Wu_1997 vmod_model_trained appos_model_Wu nn_model_translation nn_model_resultsunderaninversiontransductiongrammar appos_resultsunderaninversiontransductiongrammar_ITG nn_resultsunderaninversiontransductiongrammar_translation conj_and_present_using vmod_present_described dobj_present_model nsubj_present_we ccomp_present_108 nn_model_language det_model_the prep_of_effects_model det_effects_the prep_on_focus_effects aux_focus_to amod_work_related conj_and_follow_focus dobj_follow_work aux_follow_To vmod_108_focus vmod_108_follow
D08-1012	J97-3002	o	3 Inversion Transduction Grammars While our approach applies in principle to a variety of machine translation systems -LRB- phrase-based or syntactic -RRB- we will use the inversion transduction grammar -LRB- ITG -RRB- approach of Wu -LRB- 1997 -RRB- to facilitate comparison with previous work -LRB- Zens and Ney 2003 ZhangandGildea ,2008 -RRB- aswellastofocuson language model complexity	nn_complexity_model nn_complexity_language nn_complexity_aswellastofocuson nn_complexity_work num_ZhangandGildea_,2008 dep_Zens_ZhangandGildea dep_Zens_2003 conj_and_Zens_Ney dep_work_Ney dep_work_Zens amod_work_previous prep_with_comparison_complexity dobj_facilitate_comparison aux_facilitate_to appos_Wu_1997 vmod_approach_facilitate prep_of_approach_Wu nn_approach_grammar appos_grammar_ITG nn_grammar_transduction nn_grammar_inversion det_grammar_the dobj_use_approach aux_use_will nsubj_use_we dep_use_Grammars conj_or_phrase-based_syntactic dep_systems_syntactic dep_systems_phrase-based nn_systems_translation nn_systems_machine prep_of_variety_systems det_variety_a prep_to_applies_variety prep_in_applies_principle nsubj_applies_approach mark_applies_While poss_approach_our advcl_Grammars_applies nn_Grammars_Transduction nn_Grammars_Inversion num_Grammars_3
D08-1016	J97-3002	n	String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG -LRB- Wu 1997 -RRB- but Duchi et al.	nn_al._et nn_al._Duchi num_Wu_1997 conj_but_ITG_al. appos_ITG_Wu prep_like_formalisms_al. prep_like_formalisms_ITG amod_formalisms_synchronous amod_formalisms_simple prep_for_expensive_formalisms advmod_expensive_even advmod_expensive_quite cop_expensive_is nsubj_expensive_alignment amod_grammars_synchronous prep_with_alignment_grammars nn_alignment_String ccomp_``_expensive
D08-1060	J97-3002	o	-LRB- 2007 -RRB- are appealing as they have rather simple structure modeling only NP VP and LCP via one-level sub-tree structure with two children in the source parse-tree -LRB- a special case of ITG -LRB- Wu 1997 -RRB- -RRB-	dep_Wu_1997 dep_ITG_Wu prep_of_case_ITG amod_case_special det_case_a nn_parse-tree_source det_parse-tree_the num_children_two prep_with_structure_children amod_structure_sub-tree amod_structure_one-level dep_NP_case prep_in_NP_parse-tree prep_via_NP_structure conj_and_NP_LCP conj_and_NP_VP advmod_NP_only dep_modeling_LCP dep_modeling_VP dep_modeling_NP amod_structure_simple advmod_simple_rather dobj_have_structure nsubj_have_they mark_have_as dobj_appealing_modeling advcl_appealing_have aux_appealing_are nsubj_appealing_2007
D08-1066	J97-3002	o	We use binary Synchronous ContextFree Grammar -LRB- bSCFG -RRB- based on Inversion Transduction Grammar -LRB- ITG -RRB- -LRB- Wu 1997 Chiang 2005a -RRB- to define the set of eligible segmentations for an aligned sentence pair	nn_pair_sentence amod_pair_aligned det_pair_an prep_for_segmentations_pair amod_segmentations_eligible prep_of_set_segmentations det_set_the dobj_define_set aux_define_to appos_Chiang_2005a dep_Wu_Chiang appos_Wu_1997 appos_Grammar_Wu appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion prep_on_based_Grammar vmod_Grammar_based appos_Grammar_bSCFG nn_Grammar_ContextFree amod_Grammar_Synchronous amod_Grammar_binary vmod_use_define dobj_use_Grammar nsubj_use_We
D08-1066	J97-3002	o	In particular this holds for the SCFG implementing Inversion 3For two sequences of numbers the notation y < z stands for y y z z y < z. Transduction Grammar -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_Grammar_Wu nn_Grammar_Transduction num_Grammar_z. dep_Grammar_< nn_Grammar_y dep_z_z nn_y_y dep_stands_Grammar prep_stands_z prep_for_stands_y nsubj_stands_y dep_<_z amod_y_< nn_y_notation det_y_the prep_of_sequences_numbers num_sequences_two nn_sequences_3For nn_sequences_Inversion dobj_implementing_sequences vmod_SCFG_implementing det_SCFG_the parataxis_holds_stands prep_for_holds_SCFG nsubj_holds_this prep_in_holds_particular
D08-1089	J97-3002	p	Coming from the other direction such observations about phrase reordering between different languages are precisely thekindsoffactsthatparsingapproachestomachine translation are designed to handle and do successfully handle -LRB- Wu 1997 Melamed 2003 Chiang 2005 -RRB-	amod_Chiang_2005 num_Melamed_2003 dep_Wu_Chiang conj_Wu_Melamed num_Wu_1997 advmod_handle_successfully aux_handle_do dep_handle_Wu conj_and_handle_handle aux_handle_to xcomp_designed_handle xcomp_designed_handle auxpass_designed_are nsubjpass_designed_translation amod_translation_thekindsoffactsthatparsingapproachestomachine cop_translation_are nsubj_translation_observations vmod_translation_Coming advmod_thekindsoffactsthatparsingapproachestomachine_precisely amod_languages_different prep_between_reordering_languages nn_reordering_phrase prep_about_observations_reordering amod_observations_such amod_direction_other det_direction_the prep_from_Coming_direction
D08-1089	J97-3002	o	To be able identify that adjacent blocks -LRB- e.g. the development and and progress -RRB- can be merged into larger blocks our model infers binary -LRB- non-linguistic -RRB- trees reminiscent of -LRB- Wu 1997 Chiang 2005 -RRB-	amod_Chiang_2005 dep_Wu_Chiang appos_Wu_1997 prep_of_reminiscent_Wu amod_trees_reminiscent amod_trees_binary dep_binary_non-linguistic dobj_infers_trees nsubj_infers_model advcl_infers_able poss_model_our amod_blocks_larger prep_into_merged_blocks auxpass_merged_be aux_merged_can nsubjpass_merged_blocks mark_merged_that det_development_the conj_and_e.g._progress conj_and_e.g._development dep_blocks_progress dep_blocks_development dep_blocks_e.g. amod_blocks_adjacent ccomp_identify_merged dep_able_identify cop_able_be aux_able_To
D09-1021	J97-3002	o	Early examples of this work include -LRB- Alshawi 1996 Wu 1997 -RRB- more recent models include -LRB- Yamada and Knight 2001 Eisner 2003 Melamed 2004 Zhang and Gildea 2005 Chiang 2005 Quirk et al. 2005 Marcu et al. 2006 Zollmann and Venugopal 2006 Nesson et al. 2006 Cherry 2008 Mi et al. 2008 Shen et al. 2008 -RRB-	num_Shen_2008 nn_Shen_al. nn_Shen_et num_Mi_2008 nn_Mi_al. nn_Mi_et num_Cherry_2008 num_Nesson_2006 nn_Nesson_al. nn_Nesson_et num_Marcu_2006 nn_Marcu_al. nn_Marcu_et num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Chiang_2005 dep_Zhang_Shen conj_and_Zhang_Mi conj_and_Zhang_Cherry conj_and_Zhang_Nesson num_Zhang_2006 conj_and_Zhang_Venugopal conj_and_Zhang_Zollmann conj_and_Zhang_Marcu conj_and_Zhang_Quirk conj_and_Zhang_Chiang conj_and_Zhang_2005 conj_and_Zhang_Gildea num_Melamed_2004 num_Eisner_2003 dep_Yamada_Mi dep_Yamada_Cherry dep_Yamada_Nesson dep_Yamada_Venugopal dep_Yamada_Zollmann dep_Yamada_Marcu dep_Yamada_Quirk dep_Yamada_Chiang dep_Yamada_2005 dep_Yamada_Gildea dep_Yamada_Zhang conj_and_Yamada_Melamed conj_and_Yamada_Eisner conj_and_Yamada_2001 conj_and_Yamada_Knight dep_include_Melamed dep_include_Eisner dep_include_2001 dep_include_Knight dep_include_Yamada nsubj_include_models amod_models_recent advmod_recent_more dep_Wu_1997 parataxis_Alshawi_include dep_Alshawi_Wu conj_Alshawi_1996 dep_include_Alshawi nsubj_include_examples det_work_this prep_of_examples_work advmod_examples_Early ccomp_``_include
D09-1039	J97-3002	o	Each linked fragment pair consists of a source-language side and a target-language side similar to -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_to_Wu prep_similar_to amod_side_target-language det_side_a amod_side_similar conj_and_side_side amod_side_source-language det_side_a prep_of_consists_side prep_of_consists_side nsubj_consists_pair nn_pair_fragment amod_pair_linked det_pair_Each
D09-1050	J97-3002	o	Since many concepts are expressed by idiomatic multiword expressions instead of single words and different languages may realize the same concept using different numbers of words -LRB- Ma et al. 2007 Wu 1997 -RRB- word alignment based methods which are highly dependent on the probability information at the lexical level are not well suited for this type of translation	prep_of_type_translation det_type_this prep_for_suited_type advmod_suited_well neg_suited_not auxpass_suited_are nsubjpass_suited_methods amod_level_lexical det_level_the prep_at_information_level nn_information_probability det_information_the prep_on_dependent_information advmod_dependent_highly cop_dependent_are nsubj_dependent_which rcmod_methods_dependent amod_methods_based rcmod_alignment_suited nn_alignment_word dep_Wu_1997 dep_Ma_Wu num_Ma_2007 dep_Ma_al. nn_Ma_et prep_of_numbers_words amod_numbers_different dobj_using_numbers vmod_concept_using amod_concept_same det_concept_the dobj_realize_alignment dep_realize_Ma dobj_realize_concept aux_realize_may nsubj_realize_languages cc_realize_and advcl_realize_expressed amod_languages_different amod_words_single amod_expressions_multiword amod_expressions_idiomatic prep_instead_of_expressed_words agent_expressed_expressions auxpass_expressed_are nsubjpass_expressed_concepts mark_expressed_Since amod_concepts_many
D09-1073	J97-3002	o	In this paper we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG -LRB- Bracketing Transduction Grammar -RRB- constraints -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_constraints_Wu dep_constraints_Bracketing nn_constraints_BTG nn_Grammar_Transduction dobj_Bracketing_Grammar prep_with_system_constraints nn_system_SMT amod_system_phrase-based det_system_a prep_in_reordering_system nn_reordering_phrase nn_features_syntactic amod_features_structured prep_for_utilize_reordering dobj_utilize_features aux_utilize_to advmod_utilize_how prepc_of_issue_utilize det_issue_the dobj_studying_issue amod_idea_first det_idea_the prepc_by_bring_studying dobj_bring_idea advmod_bring_forward nsubj_bring_we prep_in_bring_paper det_paper_this
D09-1073	J97-3002	o	Recently many phrase reordering methods have been proposed ranging from simple distancebased distortion model -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- flat reordering model -LRB- Wu 1997 Zens et al. 2004 -RRB- lexicalized reordering model -LRB- Tillmann 2004 Kumar and Byrne 2005 -RRB- to hierarchical phrase-based model -LRB- Chiang 2005 Setiawan et al. 2007 -RRB- and classifier-based reordering model with linear features -LRB- Zens and Ney 2006 Xiong et al. 2006 Zhang et al. 2007a Xiong et al. 2008 -RRB-	num_Xiong_2008 nn_Xiong_al. nn_Xiong_et appos_Zhang_2007a dep_Zhang_al. nn_Zhang_et num_Xiong_2006 nn_Xiong_al. nn_Xiong_et dep_Zens_Xiong conj_and_Zens_Zhang conj_and_Zens_Xiong conj_and_Zens_2006 conj_and_Zens_Ney appos_features_Zhang appos_features_Xiong appos_features_2006 appos_features_Ney appos_features_Zens amod_features_linear prep_with_model_features nn_model_reordering amod_model_classifier-based num_Setiawan_2007 nn_Setiawan_al. nn_Setiawan_et dep_Chiang_Setiawan appos_Chiang_2005 dep_model_Chiang amod_model_phrase-based amod_model_hierarchical dep_Kumar_2005 conj_and_Kumar_Byrne dep_Tillmann_Byrne dep_Tillmann_Kumar appos_Tillmann_2004 appos_model_Tillmann nn_model_reordering amod_model_lexicalized nn_al._et nn_al._Zens amod_Wu_2004 dep_Wu_al. appos_Wu_1997 appos_model_Wu nn_model_reordering amod_model_flat dep_Och_2004 conj_and_Och_Ney conj_and_Koehn_model prep_to_Koehn_model conj_and_Koehn_model dep_Koehn_model dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_model_distortion amod_model_distancebased amod_model_simple prep_from_ranging_model dep_proposed_model dep_proposed_model dep_proposed_Koehn xcomp_proposed_ranging auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods advmod_proposed_Recently nn_methods_reordering nn_methods_phrase amod_methods_many
D09-1073	J97-3002	p	1 Introduction Phrase-based method -LRB- Koehn et al. 2003 Och and Ney 2004 Koehn et al. 2007 -RRB- and syntaxbased method -LRB- Wu 1997 Yamada and Knight 2001 Eisner 2003 Chiang 2005 Cowan et al. 2006 Marcu et al. 2006 Liu et al. 2007 Zhang et al. 2007c 2008a 2008b Shen et al. 2008 Mi and Huang 2008 -RRB- represent the state-of-the-art technologies in statistical machine translation -LRB- SMT -RRB-	appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_technologies_translation amod_technologies_state-of-the-art det_technologies_the dobj_represent_technologies nsubj_represent_method nsubj_represent_Koehn nsubj_represent_2004 nsubj_represent_Ney nsubj_represent_Och dep_Mi_2008 conj_and_Mi_Huang nn_al._et nn_al._Shen appos_Zhang_2008b appos_Zhang_2008a appos_Zhang_2007c dep_Zhang_al. nn_Zhang_et num_Liu_2007 nn_Liu_al. nn_Liu_et dep_al._2006 nn_al._et nn_al._Marcu num_Cowan_2006 nn_Cowan_al. nn_Cowan_et num_Chiang_2005 num_Eisner_2003 num_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Huang dep_Wu_Mi num_Wu_2008 dep_Wu_al. conj_Wu_Zhang conj_Wu_Liu conj_Wu_al. conj_Wu_Cowan conj_Wu_Chiang conj_Wu_Eisner conj_Wu_Knight conj_Wu_Yamada appos_Wu_1997 appos_method_Wu amod_method_syntaxbased num_Koehn_2007 nn_Koehn_al. nn_Koehn_et conj_and_Och_method conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney parataxis_Koehn_represent appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn amod_method_Phrase-based nn_method_Introduction num_method_1
D09-1105	J97-3002	o	S S0 n Si k Si j Sj k Si1 i pii Figure 1 A grammar for a large neighborhood of permutations given one permutation pi of length n The Si k rules are instantiated for each 0 i < j < k n and the Si1 i rules for each 0 < in We say that two permutations are neighbors iff they can be aligned by an Inversion Transduction Grammar -LRB- ITG -RRB- -LRB- Wu 1997 -RRB- which is a familiar reordering device in machine translation	nn_translation_machine prep_in_device_translation nn_device_reordering amod_device_familiar det_device_a cop_device_is nsubj_device_which dep_Wu_1997 rcmod_Grammar_device appos_Grammar_Wu appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion det_Grammar_an agent_aligned_Grammar auxpass_aligned_be aux_aligned_can nsubjpass_aligned_they mark_aligned_iff dep_neighbors_aligned cop_neighbors_are nsubj_neighbors_permutations mark_neighbors_that num_permutations_two ccomp_say_neighbors nsubj_say_We nsubj_say_S0 num_<_0 quantmod_0_each prep_for_rules_< nn_rules_i prep_Si1_in appos_Si1_rules det_Si1_the nn_n_k amod_n_< nn_n_j dep_n_< nn_n_i num_n_0 det_n_each conj_and_instantiated_Si1 prep_for_instantiated_n auxpass_instantiated_are nsubjpass_instantiated_Si nn_rules_k appos_Si_rules det_Si_The nn_n_length prep_of_pi_n nn_pi_permutation num_pi_one pobj_given_pi prep_of_neighborhood_permutations amod_neighborhood_large det_neighborhood_a rcmod_grammar_Si1 rcmod_grammar_instantiated prep_grammar_given prep_for_grammar_neighborhood det_grammar_A dep_Figure_grammar num_Figure_1 nn_Figure_pii nn_Figure_i nn_Si1_k nn_Sj_j nn_Si_k nn_Si_n appos_S0_Figure conj_S0_Si1 conj_S0_Sj conj_S0_Si conj_S0_Si nn_S0_S
D09-1127	J97-3002	o	Joint parsing with a simplest synchronous context-free grammar -LRB- Wu 1997 -RRB- is O -LRB- n6 -RRB- as opposed to the monolingual O -LRB- n3 -RRB- time	dep_O_time appos_O_n3 amod_O_monolingual det_O_the prep_to_opposed_O mark_opposed_as advcl_O_opposed appos_O_n6 cop_O_is nsubj_O_Joint amod_Wu_1997 appos_grammar_Wu amod_grammar_context-free amod_grammar_synchronous amod_grammar_simplest det_grammar_a prep_with_parsing_grammar vmod_Joint_parsing
E06-1019	J97-3002	o	Someworkwithintheframework of synchronous grammars -LRB- Wu 1997 Melamed 2003 -RRB- while others create a generative story that includes a parse tree provided for one of the sentences -LRB- Yamada and Knight 2001 -RRB-	amod_Yamada_2001 conj_and_Yamada_Knight det_sentences_the dep_one_Knight dep_one_Yamada prep_of_one_sentences prep_for_provided_one vmod_tree_provided amod_tree_parse det_tree_a dobj_includes_tree nsubj_includes_that rcmod_story_includes amod_story_generative det_story_a dobj_create_story nsubj_create_others mark_create_while advcl_,_create dep_Melamed_2003 dep_Wu_Melamed appos_Wu_1997 appos_grammars_Wu amod_grammars_synchronous prep_of_Someworkwithintheframework_grammars dep_``_Someworkwithintheframework
E06-1019	J97-3002	o	To enable such techniques we bring the cohesion constraint inside the ITG framework -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_framework_Wu nn_framework_ITG det_framework_the prep_inside_constraint_framework nn_constraint_cohesion det_constraint_the dobj_bring_constraint nsubj_bring_we advcl_bring_enable amod_techniques_such dobj_enable_techniques aux_enable_To
E06-1019	J97-3002	o	Alignment spaces can emerge from generative stories -LRB- Brown et al. 1993 -RRB- from syntactic notions -LRB- Wu 1997 -RRB- or they can be imposed to create competition between links -LRB- Melamed 2000 -RRB-	amod_Melamed_2000 dep_links_Melamed prep_between_competition_links dobj_create_competition aux_create_to xcomp_imposed_create auxpass_imposed_be aux_imposed_can nsubjpass_imposed_they dep_Wu_1997 appos_notions_Wu amod_notions_syntactic conj_or_from_imposed pobj_from_notions amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_stories_generative dep_emerge_imposed dep_emerge_from dep_emerge_Brown prep_from_emerge_stories aux_emerge_can nsubj_emerge_spaces nn_spaces_Alignment
E06-1019	J97-3002	p	2.2 ITG Space Inversion Transduction Grammars or ITGs -LRB- Wu 1997 -RRB- provide an efficient formalism to synchronously parse bitext	dobj_parse_bitext advmod_parse_synchronously aux_parse_to amod_formalism_efficient det_formalism_an xcomp_provide_parse dobj_provide_formalism nsubj_provide_ITGs nsubj_provide_Grammars dep_Wu_1997 appos_ITGs_Wu conj_or_Grammars_ITGs nn_Grammars_Transduction nn_Grammars_Inversion nn_Grammars_Space nn_Grammars_ITG num_Grammars_2.2
E06-1019	J97-3002	o	This results in two forbidden alignment structures shown in Figure 1 called inside-out transpositions in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu prep_transpositions_in amod_transpositions_inside-out dep_called_transpositions vmod_Figure_called num_Figure_1 prep_in_shown_Figure vmod_structures_shown nn_structures_alignment amod_structures_forbidden num_structures_two prep_in_results_structures nsubj_results_This ccomp_``_results
E06-1019	J97-3002	o	Zens and Ney -LRB- 2003 -RRB- explore the re-orderings allowed by ITGs and provide a formulation for the number of structures that can be built for a sentence pair of size n. ITGs explore almost all of permutation space when n is small but their coverage of permutation space falls off quickly for n > 5 -LRB- Wu 1997 -RRB-	amod_Wu_1997 quantmod_5_> appos_n_Wu num_n_5 prep_for_falls_n advmod_falls_quickly prt_falls_off nsubj_falls_coverage nn_space_permutation prep_of_coverage_space poss_coverage_their conj_but_small_falls cop_small_is nsubj_small_n advmod_small_when nn_space_permutation prep_of_all_space advmod_all_almost advcl_explore_falls advcl_explore_small dobj_explore_all nn_ITGs_n. nn_ITGs_size prep_of_pair_ITGs nn_pair_sentence det_pair_a prep_for_built_pair auxpass_built_be aux_built_can nsubjpass_built_that rcmod_structures_built prep_of_number_structures det_number_the prep_for_formulation_number det_formulation_a dobj_provide_formulation nsubj_provide_Zens agent_allowed_ITGs vmod_re-orderings_allowed det_re-orderings_the dep_explore_explore conj_and_explore_provide dobj_explore_re-orderings nsubj_explore_Ney nsubj_explore_Zens appos_Ney_2003 conj_and_Zens_Ney
E06-1019	J97-3002	o	3.1 A simple solution Wu -LRB- 1997 -RRB- suggests that in order to have an ITG take advantage of a known partial structure one can simply stop the parser from using any spans that would violate the structure	det_structure_the dobj_violate_structure aux_violate_would nsubj_violate_that rcmod_spans_violate det_spans_any dobj_using_spans det_parser_the prepc_from_stop_using dobj_stop_parser advmod_stop_simply aux_stop_can nsubj_stop_one amod_structure_partial amod_structure_known det_structure_a prep_of_advantage_structure parataxis_take_stop dobj_take_advantage nsubj_take_ITG aux_take_have mark_take_that det_ITG_an aux_have_to dep_have_order mark_have_in ccomp_suggests_take nsubj_suggests_Wu appos_Wu_1997 nn_Wu_solution amod_Wu_simple det_Wu_A num_Wu_3.1
E06-1019	J97-3002	o	Having a single canonical tree structure for each possible alignment can help when flattening binary trees as it indicates arbitrary binarization decisions -LRB- Wu 1997 -RRB-	dep_Wu_1997 appos_decisions_Wu nn_decisions_binarization amod_decisions_arbitrary dobj_indicates_decisions nsubj_indicates_it mark_indicates_as amod_trees_binary advcl_flattening_indicates dobj_flattening_trees advmod_flattening_when advcl_help_flattening aux_help_can csubj_help_Having amod_alignment_possible det_alignment_each prep_for_structure_alignment nn_structure_tree amod_structure_canonical amod_structure_single det_structure_a dobj_Having_structure
E06-1019	J97-3002	o	Normally one would eliminate the redundant structures produced by the grammar in -LRB- 1 -RRB- by replacing it with the canonical form grammar -LRB- Wu 1997 -RRB- which has the following form S A | B | C A -LSB- AB -RSB- | -LSB- BB -RSB- | -LSB- CB -RSB- | -LSB- AC -RSB- | -LSB- BC -RSB- | -LSB- CC -RSB- B AA | BA | CA | AC | BC | CC C e/f -LRB- 2 -RRB- By design this grammar allows only one struc147 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a2 a8 a10 a8 a1 a2 a3 a6 a8 a4 a7 a8 a6 a8 a9 a8 a8 a11 a12 a11 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a13 a11 Figure 3 An example of how dependency trees interact with ITGs	prep_with_interact_ITGs nsubj_interact_trees advmod_interact_how nn_trees_dependency prepc_of_example_interact det_example_An num_Figure_3 nn_Figure_a11 nn_Figure_a13 nn_Figure_a8 nn_Figure_a9 nn_Figure_a8 nn_Figure_a6 nn_Figure_a8 nn_Figure_a7 nn_Figure_a6 nn_Figure_a1 nn_Figure_a5 nn_Figure_a2 nn_Figure_a4 nn_Figure_a3 nn_Figure_a0 nn_Figure_a2 nn_Figure_a1 nn_Figure_a0 nn_Figure_a8 nn_Figure_a9 nn_Figure_a8 nn_Figure_a6 nn_Figure_a8 nn_Figure_a7 nn_Figure_a6 nn_Figure_a1 nn_Figure_a5 nn_Figure_a2 nn_Figure_a4 nn_Figure_a3 nn_Figure_a0 nn_Figure_a2 nn_Figure_a1 nn_Figure_a0 nn_Figure_a8 nn_Figure_a9 nn_Figure_a8 nn_Figure_a6 nn_Figure_a8 nn_Figure_a7 nn_Figure_a6 nn_Figure_a1 nn_Figure_a5 nn_Figure_a2 nn_Figure_a4 nn_Figure_a3 nn_Figure_a0 nn_Figure_a2 nn_Figure_a1 nn_Figure_a0 nn_Figure_a11 nn_Figure_a12 nn_Figure_a11 nn_Figure_a8 nn_Figure_a8 nn_Figure_a9 nn_Figure_a8 nn_Figure_a6 nn_Figure_a8 nn_Figure_a7 nn_Figure_a4 nn_Figure_a8 nn_Figure_a6 nn_Figure_a3 nn_Figure_a2 nn_Figure_a1 nn_Figure_a8 nn_Figure_a10 nn_Figure_a8 nn_Figure_a2 nn_Figure_a8 nn_Figure_a9 nn_Figure_a8 nn_Figure_a6 nn_Figure_a8 nn_Figure_a7 nn_Figure_a6 nn_Figure_a1 nn_Figure_a5 nn_Figure_a2 nn_Figure_a4 nn_Figure_a3 nn_Figure_a0 nn_Figure_a2 nn_Figure_a1 nn_Figure_a0 nn_Figure_struc147 num_Figure_one quantmod_one_only dobj_allows_Figure nsubj_allows_grammar det_grammar_this appos_e/f_2 nn_e/f_C nn_e/f_CC num_e/f_| nn_e/f_BC num_e/f_| nn_e/f_AC nn_e/f_| nn_e/f_CA num_e/f_| dep_BA_e/f num_BA_| dep_AA_BA nn_AA_B dep_AA_CC num_|_| appos_|_AC dep_A_example rcmod_A_allows prep_by_A_design dep_A_AA num_A_| appos_A_BC num_A_| appos_A_CB num_A_| appos_A_BB num_A_| appos_A_AB nn_A_C nn_A_| nn_A_B nn_A_| det_A_A nn_A_S amod_form_following det_form_the dobj_has_form nsubj_has_which dep_Wu_1997 dep_grammar_A rcmod_grammar_has appos_grammar_Wu nn_grammar_form amod_grammar_canonical det_grammar_the prep_with_replacing_grammar dobj_replacing_it prepc_by_1_replacing det_grammar_the prep_in_produced_1 agent_produced_grammar vmod_structures_produced amod_structures_redundant det_structures_the dobj_eliminate_structures aux_eliminate_would nsubj_eliminate_one advmod_eliminate_Normally
H01-1035	J97-3002	p	Wu -LRB- 1995 1997 -RRB- investigated the use of concurrent parsing of parallel corpora in a transduction inversion framework helping to resolve attachment ambiguities in one language by the coupled parsing state in the second language	amod_language_second det_language_the nn_state_parsing amod_state_coupled det_state_the num_language_one prep_in_ambiguities_language nn_ambiguities_attachment prep_in_resolve_language prep_by_resolve_state dobj_resolve_ambiguities aux_resolve_to xcomp_helping_resolve nn_framework_inversion nn_framework_transduction det_framework_a amod_corpora_parallel prep_of_parsing_corpora amod_parsing_concurrent prep_in_use_framework prep_of_use_parsing det_use_the vmod_investigated_helping dobj_investigated_use nsubj_investigated_Wu amod_1995_1997 dep_Wu_1995
H05-1023	J97-3002	o	In this respect it resembles bilingual bracketing -LRB- Wu 1997 -RRB- but our model has more lexical items in the blocks with many-to-many word alignment freedom in both inner and outer parts	amod_parts_outer amod_parts_inner conj_and_inner_outer preconj_inner_both prep_in_freedom_parts nn_freedom_alignment nn_freedom_word amod_freedom_many-to-many prep_with_blocks_freedom det_blocks_the prep_in_items_blocks amod_items_lexical amod_items_more dobj_has_items nsubj_has_model poss_model_our dep_Wu_1997 appos_bracketing_Wu amod_bracketing_bilingual conj_but_resembles_has dobj_resembles_bracketing nsubj_resembles_it prep_in_resembles_respect det_respect_this
H05-1036	J97-3002	o	These techniques included unweighted FS morphology conditional random fields -LRB- Lafferty et al. 2001 -RRB- synchronous parsers -LRB- Wu 1997 Melamed 2003 -RRB- lexicalized parsers -LRB- Eisner and Satta 1999 -RRB- ,22 partially supervised training ` a la -LRB- Pereira and Schabes 1992 -RRB- ,23 and grammar induction -LRB- Klein and Manning 2002 -RRB-	amod_Klein_2002 conj_and_Klein_Manning nn_induction_grammar dep_Pereira_1992 conj_and_Pereira_Schabes dep_la_Manning dep_la_Klein conj_and_la_induction num_la_,23 dep_la_Schabes dep_la_Pereira det_la_a dep_training_induction dep_training_la dobj_supervised_training advmod_supervised_partially vmod_,22_supervised dep_Eisner_1999 conj_and_Eisner_Satta dep_parsers_,22 dep_parsers_Satta dep_parsers_Eisner amod_parsers_lexicalized dep_Melamed_2003 dep_Wu_Melamed appos_Wu_1997 appos_parsers_Wu amod_parsers_synchronous amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et dep_fields_Lafferty amod_fields_random amod_fields_conditional appos_morphology_parsers conj_morphology_parsers conj_morphology_fields nn_morphology_FS amod_morphology_unweighted dobj_included_morphology nsubj_included_techniques det_techniques_These ccomp_``_included
H05-1098	J97-3002	o	Since one of these filters restricts the number of nonterminal symbols to two our extracted grammar is equivalent to an inversion transduction grammar -LRB- Wu 1997 -RRB-	dep_Wu_1997 appos_grammar_Wu nn_grammar_transduction nn_grammar_inversion det_grammar_an prep_to_equivalent_grammar cop_equivalent_is nsubj_equivalent_grammar advcl_equivalent_restricts amod_grammar_extracted poss_grammar_our amod_symbols_nonterminal prep_of_number_symbols det_number_the prep_to_restricts_two dobj_restricts_number nsubj_restricts_one mark_restricts_Since det_filters_these prep_of_one_filters
H05-1101	J97-3002	o	Among the several proposals we mention here the models presented in -LRB- Wu 1997 Wu and Wong 1998 -RRB- -LRB- Alshawi et al. 2000 -RRB- -LRB- Yamada and Knight 2001 -RRB- -LRB- Gildea 2003 -RRB- and -LRB- Melamed 2003 -RRB-	amod_Melamed_2003 dep_Gildea_2003 conj_and_Yamada_Knight amod_Alshawi_2000 dep_Alshawi_al. nn_Alshawi_et amod_Wu_1998 conj_and_Wu_Wong conj_and_Wu_Melamed appos_Wu_Gildea dep_Wu_2001 appos_Wu_Knight appos_Wu_Yamada appos_Wu_Alshawi dep_Wu_Wong dep_Wu_Wu appos_Wu_1997 prep_in_presented_Melamed prep_in_presented_Wu vmod_models_presented det_models_the dobj_mention_models advmod_mention_here nsubj_mention_we prep_among_mention_proposals amod_proposals_several det_proposals_the
H05-1101	J97-3002	o	This problem has been considered for instance in -LRB- Wu 1997 -RRB- for his inversion transduction grammars and has applications in the support of several tasks of automatic annotation of parallel corpora as for instance segmentation bracketing phrasal and word alignment	dep_bracketing_alignment conj_and_bracketing_word conj_and_bracketing_phrasal nn_segmentation_instance amod_corpora_parallel prep_of_annotation_corpora amod_annotation_automatic prep_of_tasks_annotation amod_tasks_several prep_of_support_tasks det_support_the prep_in_has_support dobj_has_applications nsubj_has_problem nn_grammars_transduction nn_grammars_inversion poss_grammars_his dep_Wu_1997 prep_in_instance_Wu xcomp_considered_word xcomp_considered_phrasal xcomp_considered_bracketing pobj_considered_segmentation prepc_as_for_considered_for conj_and_considered_has prep_for_considered_grammars prep_for_considered_instance auxpass_considered_been aux_considered_has nsubjpass_considered_problem det_problem_This
I05-1023	J97-3002	o	We present a new implication of Wus -LRB- 1997 -RRB- Inversion Transduction Grammar -LRB- ITG -RRB- Hypothesis on the problem of retrieving truly parallel sentence translations from large collections of highly non-parallel documents	amod_documents_non-parallel advmod_non-parallel_highly prep_of_collections_documents amod_collections_large prep_from_translations_collections nn_translations_sentence amod_translations_parallel amod_translations_retrieving advmod_parallel_truly prep_of_problem_translations det_problem_the dep_Grammar_Hypothesis appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion nn_Grammar_Wus appos_Wus_1997 prep_of_implication_Grammar amod_implication_new det_implication_a prep_on_present_problem dobj_present_implication nsubj_present_We ccomp_``_present
I08-2087	J97-3002	o	However formally syntax-based methods propose simple but efficient ways to parse and translate sentences -LRB- Wu 1997 Chiang 2005 -RRB-	num_Chiang_2005 dep_Wu_Chiang num_Wu_1997 appos_sentences_Wu dobj_parse_sentences conj_and_parse_translate aux_parse_to vmod_ways_translate vmod_ways_parse amod_ways_efficient amod_ways_simple conj_but_simple_efficient dobj_propose_ways nsubj_propose_methods advmod_propose_However amod_methods_syntax-based advmod_syntax-based_formally
I08-8001	J97-3002	p	Some methods which can offer powerful reordering policies have been proposed like syntax based machine translation -LRB- Yamada and Knight 2001 -RRB- and Inversion Transduction Grammar -LRB- Wu 1997 -RRB-	dep_Wu_1997 appos_Grammar_Wu nn_Grammar_Transduction nn_Grammar_Inversion amod_Yamada_2001 conj_and_Yamada_Knight conj_and_translation_Grammar dep_translation_Knight dep_translation_Yamada nn_translation_machine amod_translation_based nn_translation_syntax prep_like_proposed_Grammar prep_like_proposed_translation auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods nn_policies_reordering amod_policies_powerful dobj_offer_policies aux_offer_can nsubj_offer_which rcmod_methods_offer det_methods_Some ccomp_``_proposed
J00-1004	J97-3002	o	Concluding Remarks Formalisms for finite-state and context-free transduction have a long history -LRB- e.g. Lewis and Stearns 1968 Aho and Ullman 1972 -RRB- and such formalisms have been applied to the machine translation problem both in the finite-state case -LRB- e.g. Vilar et al. 1996 -RRB- and the context-free case -LRB- e.g. Wu 1997 -RRB-	num_Wu_1997 pobj_e.g._Wu ccomp_-LRB-_e.g. amod_case_context-free det_case_the num_al._1996 nn_al._et nn_al._Vilar dep_al._e.g. dep_case_al. amod_case_finite-state det_case_the conj_and_in_case pobj_in_case preconj_in_both nn_problem_translation nn_problem_machine det_problem_the dep_applied_case dep_applied_in prep_to_applied_problem auxpass_applied_been aux_applied_have nsubjpass_applied_formalisms nsubjpass_applied_Stearns nsubjpass_applied_Lewis advmod_applied_e.g. amod_formalisms_such num_Ullman_1972 conj_and_Aho_Ullman conj_and_Lewis_formalisms dep_Lewis_Ullman dep_Lewis_Aho num_Lewis_1968 conj_and_Lewis_Stearns dep_history_applied amod_history_long det_history_a dobj_have_history nsubj_have_Formalisms amod_transduction_context-free amod_transduction_finite-state conj_and_finite-state_context-free prep_for_Formalisms_transduction nn_Formalisms_Remarks amod_Formalisms_Concluding
J07-2003	J97-3002	o	This also makes our grammar weakly equivalent to an inversion transduction grammar -LRB- Wu 1997 -RRB- although the conversion would create a very large number of new nonterminal symbols	amod_symbols_nonterminal amod_symbols_new prep_of_number_symbols amod_number_large det_number_a advmod_large_very dobj_create_number aux_create_would nsubj_create_conversion mark_create_although det_conversion_the num_Wu_1997 appos_grammar_Wu nn_grammar_transduction nn_grammar_inversion det_grammar_an prep_to_equivalent_grammar advmod_equivalent_weakly nsubj_equivalent_grammar poss_grammar_our advcl_makes_create xcomp_makes_equivalent advmod_makes_also nsubj_makes_This ccomp_``_makes
J07-2003	J97-3002	o	Because our system uses a synchronous CFG it could be thought of as an example of syntax-based statistical machine translation -LRB- MT -RRB- joining a line of research -LRB- Wu 1997 Alshawi Bangalore and Douglas 2000 Yamada and Knight 2001 -RRB- that has been fruitful but has not previously produced systems that can compete with phrase-based systems in large-scale translation tasks such as the evaluations held by NIST	agent_held_NIST vmod_evaluations_held det_evaluations_the prep_such_as_tasks_evaluations nn_tasks_translation amod_tasks_large-scale amod_systems_phrase-based prep_in_compete_tasks prep_with_compete_systems aux_compete_can nsubj_compete_that rcmod_systems_compete dobj_produced_systems advmod_produced_previously neg_produced_not aux_produced_has nsubj_produced_that conj_but_fruitful_produced cop_fruitful_been aux_fruitful_has nsubj_fruitful_that dep_Yamada_2001 conj_and_Yamada_Knight num_Douglas_2000 conj_and_Alshawi_Douglas conj_and_Alshawi_Bangalore dep_Wu_Knight dep_Wu_Yamada dep_Wu_Douglas dep_Wu_Bangalore dep_Wu_Alshawi num_Wu_1997 rcmod_line_produced rcmod_line_fruitful appos_line_Wu prep_of_line_research det_line_a dobj_joining_line appos_translation_MT nn_translation_machine amod_translation_statistical amod_translation_syntax-based prep_of_example_translation det_example_an xcomp_thought_joining pobj_thought_example prepc_as_of_thought_as auxpass_thought_be aux_thought_could nsubjpass_thought_it advcl_thought_uses amod_CFG_synchronous det_CFG_a dobj_uses_CFG nsubj_uses_system mark_uses_Because poss_system_our
J07-2003	J97-3002	o	At one extreme are those exemplified by that of Wu -LRB- 1997 -RRB- that have no dependence on syntactic theory beyond the idea that natural language is hierarchical	cop_hierarchical_is nsubj_hierarchical_language mark_hierarchical_that amod_language_natural ccomp_idea_hierarchical det_idea_the nn_theory_syntactic prep_beyond_dependence_idea prep_on_dependence_theory neg_dependence_no dobj_have_dependence nsubj_have_that appos_Wu_1997 prep_of_that_Wu agent_exemplified_that rcmod_those_have vmod_those_exemplified cop_those_are prep_at_those_extreme num_extreme_one
N03-1017	J97-3002	o	Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models -LSB- Yamada and Knight 2001 Wu 1997 -RSB-	dep_Wu_1997 dep_Yamada_Wu conj_and_Yamada_2001 conj_and_Yamada_Knight nn_models_translation amod_models_syntactic dobj_built_models aux_built_to vmod_efforts_built amod_efforts_recent dep_comes_2001 dep_comes_Knight dep_comes_Yamada prep_from_comes_efforts nsubj_comes_motivation amod_phrases_syntactic advmod_phrases_only dobj_contains_phrases nsubj_contains_that rcmod_model_contains nn_model_translation nn_model_phrase det_model_a prep_of_performance_model det_performance_the dobj_evaluate_performance aux_evaluate_to vmod_motivation_evaluate det_motivation_Another
N03-2017	J97-3002	o	Methods such as -LRB- Wu 1997 -RRB- -LRB- Alshawi et al. 2000 -RRB- and -LRB- Lopez et al. 2002 -RRB- employ a synchronous parsing procedure to constrain a statistical alignment	amod_alignment_statistical det_alignment_a dobj_constrain_alignment aux_constrain_to nn_procedure_parsing amod_procedure_synchronous det_procedure_a vmod_employ_constrain dobj_employ_procedure nsubj_employ_Lopez nsubj_employ_Wu mark_employ_as amod_Lopez_2002 dep_Lopez_al. nn_Lopez_et amod_Alshawi_2000 dep_Alshawi_al. nn_Alshawi_et conj_and_Wu_Lopez appos_Wu_Alshawi amod_Wu_1997 mwe_as_such ccomp_Methods_employ
N03-2017	J97-3002	o	More recently there have been many proposals to introduce syntactic knowledge into SMT models -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knight 2001 Lopez et al. 2002 -RRB-	nn_al._et nn_al._Lopez nn_al._et nn_al._Alshawi amod_Wu_2002 dep_Wu_al. num_Wu_2001 conj_and_Wu_Knight conj_and_Wu_Yamada num_Wu_2000 dep_Wu_al. amod_Wu_1997 appos_models_Knight appos_models_Yamada appos_models_Wu nn_models_SMT amod_knowledge_syntactic prep_into_introduce_models dobj_introduce_knowledge aux_introduce_to vmod_proposals_introduce amod_proposals_many cop_proposals_been aux_proposals_have expl_proposals_there advmod_proposals_recently advmod_recently_More
N04-1014	J97-3002	o	Recently specific probabilistic tree-based models have been proposed not only for machine translation -LRB- Wu 1997 Alshawi Bangalore and Douglas 2000 Yamada and Knight 2001 Gildea 2003 Eisner 2003 -RRB- but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450	nn_MDA904-02-C-0450_contract nn_MDA904-02-C-0450_ARDA conj_and_F49620-001-0337_MDA904-02-C-0450 nn_F49620-001-0337_contract nn_F49620-001-0337_DARPA agent_supported_MDA904-02-C-0450 agent_supported_F49620-001-0337 auxpass_supported_was det_work_This amod_Eisner_2003 num_Gildea_2003 conj_and_Yamada_Knight num_Douglas_2000 conj_and_Alshawi_Douglas conj_and_Alshawi_Bangalore dep_Wu_Eisner dep_Wu_Gildea dep_Wu_2001 dep_Wu_Knight dep_Wu_Yamada dep_Wu_Douglas dep_Wu_Bangalore dep_Wu_Alshawi appos_Wu_1997 conj_and_translation_work appos_translation_Wu nn_translation_machine neg_only_not dep_proposed_supported prep_for_proposed_work prep_for_proposed_translation preconj_proposed_only auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models advmod_proposed_Recently amod_models_tree-based amod_models_probabilistic amod_models_specific
N04-1023	J97-3002	o	Wu -LRB- 1997 -RRB- introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form	amod_form_Chomskynormal prep_to_restricted_form amod_grammar_restricted amod_grammar_context-free amod_grammar_synchronous amod_grammar_probabilistic det_grammar_a dobj_using_grammar prep_on_constraints_alignments xcomp_introduced_using dobj_introduced_constraints nsubj_introduced_Wu appos_Wu_1997
N04-1023	J97-3002	o	-LRB- Wu 1997 -RRB- was an implicit or selforganizing syntax model as it did not use a Treebank	det_Treebank_a dobj_use_Treebank neg_use_not aux_use_did nsubj_use_it mark_use_as advcl_model_use nn_model_syntax amod_model_selforganizing amod_model_implicit det_model_an cop_model_was nsubj_model_Wu conj_or_implicit_selforganizing amod_Wu_1997
N04-1035	J97-3002	o	One approach here is that of Wu -LRB- 1997 -RRB- in which word-movement is modeled by rotations at unlabeled binary-branching nodes	amod_nodes_binary-branching appos_unlabeled_nodes prep_at_modeled_unlabeled agent_modeled_rotations auxpass_modeled_is nsubjpass_modeled_word-movement prep_in_modeled_which rcmod_Wu_modeled appos_Wu_1997 prep_of_that_Wu dep_is_that nsubj_is_approach advmod_approach_here num_approach_One
N06-1031	J97-3002	o	Some approaches have used syntax at the core -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knight 2001 Gildea 2003 Eisner 2003 Hearne and Way 2003 Melamed 2004 -RRB- while others have integrated syntax into existing phrase-based frameworks -LRB- Xia and McCord 2004 Chiang 2005 Collins et al. 2005 Quirk et al. 2005 -RRB-	num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Collins_2005 nn_Collins_al. nn_Collins_et num_Chiang_2005 dep_Xia_Quirk conj_and_Xia_Collins conj_and_Xia_Chiang conj_and_Xia_2004 conj_and_Xia_McCord dep_frameworks_Collins dep_frameworks_Chiang dep_frameworks_2004 dep_frameworks_McCord dep_frameworks_Xia amod_frameworks_phrase-based amod_frameworks_existing prep_into_integrated_frameworks dobj_integrated_syntax aux_integrated_have nsubj_integrated_others mark_integrated_while amod_Melamed_2004 advcl_Hearne_integrated dep_Hearne_Melamed conj_and_Hearne_2003 conj_and_Hearne_Way num_Eisner_2003 num_Gildea_2003 nn_al._et nn_al._Alshawi dep_Wu_2003 dep_Wu_Way dep_Wu_Hearne conj_and_Wu_Eisner conj_and_Wu_Gildea num_Wu_2001 conj_and_Wu_Knight conj_and_Wu_Yamada num_Wu_2000 dep_Wu_al. num_Wu_1997 dep_core_Eisner dep_core_Gildea dep_core_Knight dep_core_Yamada dep_core_Wu det_core_the prep_at_used_core dobj_used_syntax aux_used_have nsubj_used_approaches det_approaches_Some ccomp_``_used
N06-1033	J97-3002	o	We use -LSB- -RSB- and for straight and inverted combinations respectively following the ITG notation -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_notation_Wu nn_notation_ITG det_notation_the pobj_following_notation ccomp_,_following amod_combinations_inverted amod_combinations_straight conj_and_straight_inverted advmod_for_respectively pobj_for_combinations cc_-RSB-_and nsubj_use_We
N06-1033	J97-3002	o	One way around this dif culty is to stipulate that all rules must be binary from the outset as in inversion-transduction grammar -LRB- ITG -RRB- -LRB- Wu 1997 -RRB- and the binary synchronous context-free grammar -LRB- SCFG -RRB- employed by the Hiero system -LRB- Chiang 2005 -RRB- to model the hierarchical phrases	amod_phrases_hierarchical det_phrases_the dobj_model_phrases aux_model_to amod_Chiang_2005 dep_system_Chiang nn_system_Hiero det_system_the agent_employed_system vmod_grammar_employed appos_grammar_SCFG amod_grammar_context-free amod_grammar_synchronous amod_grammar_binary det_grammar_the amod_Wu_1997 dep_grammar_Wu appos_grammar_ITG amod_grammar_inversion-transduction conj_and_in_grammar pobj_in_grammar pobj_as_grammar pobj_as_in det_outset_the xcomp_binary_model prep_binary_as prep_from_binary_outset cop_binary_be aux_binary_must nsubj_binary_rules mark_binary_that det_rules_all ccomp_stipulate_binary aux_stipulate_to xcomp_is_stipulate nsubj_is_way nn_culty_dif det_culty_this prep_around_way_culty num_way_One
N06-1033	J97-3002	o	It has been shown by Shapiro and Stephens -LRB- 1991 -RRB- and Wu -LRB- 1997 Sec	appos_1997_Sec dep_Wu_1997 appos_Stephens_1991 conj_and_Shapiro_Wu conj_and_Shapiro_Stephens agent_shown_Wu agent_shown_Stephens agent_shown_Shapiro auxpass_shown_been aux_shown_has nsubjpass_shown_It
N06-1033	J97-3002	o	Wu -LRB- 1997 -RRB- shows that parsing a binary SCFG is in O -LRB- | w | 6 -RRB- while parsing SCFG is NP-hard in general -LRB- Satta and Peserico 2005 -RRB-	dep_Satta_2005 conj_and_Satta_Peserico dep_general_Peserico dep_general_Satta prep_in_NP-hard_general cop_NP-hard_is nsubj_NP-hard_SCFG mark_NP-hard_while amod_SCFG_parsing number_6_| num_w_6 num_w_| appos_O_w advcl_is_NP-hard prep_in_is_O csubj_is_parsing mark_is_that amod_SCFG_binary det_SCFG_a dobj_parsing_SCFG ccomp_shows_is nsubj_shows_Wu appos_Wu_1997
N06-1033	J97-3002	o	This problem can be cast as an instance of synchronous ITG parsing -LRB- Wu 1997 -RRB-	amod_Wu_1997 appos_parsing_Wu nn_parsing_ITG amod_parsing_synchronous prep_of_instance_parsing det_instance_an prep_as_cast_instance auxpass_cast_be aux_cast_can nsubjpass_cast_problem det_problem_This
N07-1052	J97-3002	o	Then h -LRB- s -RRB- h -LRB- s -RRB- + Lmax s S This epsilon1-admissible heuristic -LRB- Ghallab and Allard 1982 -RRB- bounds our search error by Lmax .3 3 Bitext Parsing In bitext parsing one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt -LRB- Melamed et al. 2004 Wu 1997 -RRB-	dep_Wu_1997 dep_Melamed_Wu appos_Melamed_2004 dep_Melamed_al. nn_Melamed_et dep_wt_Melamed amod_translation_wt dep_its_translation conj_and_ws_its nn_ws_sentence det_ws_a nn_tree_structure nn_tree_phrase amod_tree_synchronous det_tree_a prep_over_infers_its prep_over_infers_ws dobj_infers_tree advmod_infers_jointly nsubj_infers_one amod_parsing_bitext nn_Parsing_Bitext num_Parsing_3 nn_Parsing_.3 nn_Parsing_Lmax rcmod_error_infers prep_in_error_parsing prep_by_error_Parsing nn_error_search poss_error_our dobj_bounds_error nsubj_bounds_heuristic dep_Ghallab_1982 conj_and_Ghallab_Allard appos_heuristic_Allard appos_heuristic_Ghallab amod_heuristic_epsilon1-admissible det_heuristic_This parataxis_s_bounds dobj_s_S nsubj_s_Lmax nsubj_s_h advmod_s_Then conj_+_h_Lmax appos_h_s nn_h_h appos_h_s
N07-1052	J97-3002	o	We can however produce a useful surrogate a pair of monolingual WCFGs with structures projected by G and weights that when combined underestimate the costs of G. Parsing optimally relative to a synchronous grammar using a dynamic program requires time O -LRB- n6 -RRB- in the length of the sentence -LRB- Wu 1997 -RRB-	amod_Wu_1997 appos_sentence_Wu det_sentence_the prep_of_length_sentence det_length_the appos_O_n6 dep_time_O prep_in_requires_length dobj_requires_time nsubj_requires_pair amod_program_dynamic det_program_a dobj_using_program vmod_grammar_using amod_grammar_synchronous det_grammar_a prep_to_relative_grammar advmod_relative_optimally amod_Parsing_relative nn_Parsing_G. prep_of_costs_Parsing det_costs_the dobj_underestimate_costs dep_underestimate_combined nsubj_underestimate_that advmod_combined_when conj_and_G_weights agent_projected_weights agent_projected_G vmod_structures_projected amod_WCFGs_monolingual rcmod_pair_underestimate prep_with_pair_structures prep_of_pair_WCFGs det_pair_a dep_surrogate_requires amod_surrogate_useful det_surrogate_a dobj_produce_surrogate advmod_produce_however aux_produce_can nsubj_produce_We
N09-1009	J97-3002	o	They are most commonly used for parsing and linguistic analysis -LRB- Charniak and Johnson 2005 Collins 2003 -RRB- but are now commonly seen in applications like machine translation -LRB- Wu 1997 -RRB- and question answering -LRB- Wang et al. 2007 -RRB-	amod_Wang_2007 dep_Wang_al. nn_Wang_et dep_answering_Wang nn_answering_question num_Wu_1997 conj_and_translation_answering appos_translation_Wu nn_translation_machine prep_like_applications_answering prep_like_applications_translation prep_in_seen_applications advmod_seen_commonly advmod_seen_now auxpass_seen_are nsubjpass_seen_They dep_Collins_2003 dep_Charniak_Collins conj_and_Charniak_2005 conj_and_Charniak_Johnson appos_analysis_2005 appos_analysis_Johnson appos_analysis_Charniak nn_analysis_linguistic nn_analysis_parsing conj_and_parsing_linguistic conj_but_used_seen prep_for_used_analysis advmod_used_commonly auxpass_used_are nsubjpass_used_They advmod_commonly_most
N09-1026	J97-3002	o	Meanwhile translation grammars have grown in complexity from simple inversion transduction grammars -LRB- Wu 1997 -RRB- to general tree-to-string transducers -LRB- Galley et al. 2004 -RRB- and have increased in size by including more synchronous tree fragments -LRB- Galley et al. 2006 Marcuetal. ,2006 DeNeefeetal. ,2007 -RRB-	num_DeNeefeetal._,2007 num_Marcuetal._,2006 dep_Galley_DeNeefeetal. dep_Galley_Marcuetal. appos_Galley_2006 dep_Galley_al. nn_Galley_et appos_fragments_Galley nn_fragments_tree amod_fragments_synchronous advmod_fragments_more pobj_including_fragments prepc_by_increased_including prep_in_increased_size aux_increased_have nsubj_increased_grammars amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_transducers_tree-to-string amod_transducers_general dep_Wu_1997 appos_grammars_Wu nn_grammars_transduction nn_grammars_inversion amod_grammars_simple prep_to_complexity_transducers prep_from_complexity_grammars conj_and_grown_increased dep_grown_Galley prep_in_grown_complexity aux_grown_have nsubj_grown_grammars advmod_grown_Meanwhile nn_grammars_translation ccomp_``_increased ccomp_``_grown
P01-1067	J97-3002	o	Wu -LRB- 1997 -RRB- and Alshawi et al.	nn_al._et nn_al._Alshawi conj_and_Wu_al. appos_Wu_1997
P02-1039	J97-3002	n	Other statistical machine translation systems such as -LRB- Wu 1997 -RRB- and -LRB- Alshawi et al. 2000 -RRB- also produce a tree a15 given a sentence a16 Their models are based on mechanisms that generate two languages at the same time so an English tree a15 is obtained as a subproduct of parsing a16 However their use of the LM is not mathematically motivated since their models do not decompose into Pa4a5a2a9a8a3a10a6 and a12a14a4a5a3a7a6 unlike the noisy channel model	nn_model_channel amod_model_noisy det_model_the conj_and_Pa4a5a2a9a8a3a10a6_a12a14a4a5a3a7a6 prep_unlike_decompose_model prep_into_decompose_a12a14a4a5a3a7a6 prep_into_decompose_Pa4a5a2a9a8a3a10a6 neg_decompose_not aux_decompose_do nsubj_decompose_models mark_decompose_since poss_models_their advcl_,_decompose advmod_motivated_mathematically neg_motivated_not auxpass_motivated_is nsubjpass_motivated_use advmod_motivated_However nsubjpass_motivated_systems det_LM_the prep_of_use_LM poss_use_their dobj_parsing_a16 prepc_of_subproduct_parsing det_subproduct_a prep_as_obtained_subproduct auxpass_obtained_is nsubjpass_obtained_a15 mark_obtained_so nn_a15_tree amod_a15_English det_a15_an amod_time_same det_time_the num_languages_two prep_at_generate_time dobj_generate_languages nsubj_generate_that rcmod_mechanisms_generate advcl_based_obtained prep_on_based_mechanisms auxpass_based_are nsubjpass_based_models poss_models_Their nn_a16_sentence det_a16_a pobj_given_a16 prep_a15_given nn_a15_tree det_a15_a dobj_produce_a15 advmod_produce_also nsubj_produce_Alshawi nsubj_produce_Wu mark_produce_as amod_Alshawi_2000 dep_Alshawi_al. nn_Alshawi_et conj_and_Wu_Alshawi amod_Wu_1997 mwe_as_such rcmod_systems_based dep_systems_produce nn_systems_translation nn_systems_machine amod_systems_statistical amod_systems_Other ccomp_``_motivated
P03-1011	J97-3002	p	Wu -LRB- 1997 -RRB- showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution	amod_solution_polynomial-time det_solution_a dobj_allows_solution nn_problem_alignment det_problem_the prep_of_complexity_problem det_complexity_the conj_and_reduces_allows dobj_reduces_complexity advmod_reduces_significantly nn_constraints_bracketing amod_constraints_syntactic dep_observe_allows dep_observe_reduces dobj_observe_constraints aux_observe_to nn_pairs_sentence prep_between_alignments_pairs amod_alignments_word-level vmod_restricting_observe dobj_restricting_alignments ccomp_that_restricting dobj_showed_that nsubj_showed_Wu appos_Wu_1997
P03-1012	J97-3002	o	Methods such as -LRB- Wu 1997 -RRB- -LRB- Alshawi et al. 2000 -RRB- and -LRB- Lopez et al. 2002 -RRB- employ a synchronous parsing procedure to constrain a statistical alignment	amod_alignment_statistical det_alignment_a dobj_constrain_alignment aux_constrain_to nn_procedure_parsing amod_procedure_synchronous det_procedure_a vmod_employ_constrain dobj_employ_procedure nsubj_employ_Lopez nsubj_employ_Wu mark_employ_as amod_Lopez_2002 dep_Lopez_al. nn_Lopez_et amod_Alshawi_2000 dep_Alshawi_al. nn_Alshawi_et conj_and_Wu_Lopez appos_Wu_Alshawi amod_Wu_1997 mwe_as_such ccomp_Methods_employ
P03-1019	J97-3002	o	For this purpose we adopt the view of the ITG constraints as a bilingual grammar as e.g. in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu conj_as_in conj_as_e.g. amod_grammar_bilingual det_grammar_a nn_constraints_ITG det_constraints_the prep_of_view_constraints det_view_the prep_adopt_as prep_as_adopt_grammar dobj_adopt_view nsubj_adopt_we prep_for_adopt_purpose det_purpose_this
P03-1019	J97-3002	o	Obviously these productions are not in the normal form of an ITG but with the method described in -LRB- Wu 1997 -RRB- they can be normalized	auxpass_normalized_be aux_normalized_can nsubjpass_normalized_they prep_with_normalized_method amod_Wu_1997 prep_in_described_Wu vmod_method_described det_method_the det_ITG_an prep_of_form_ITG amod_form_normal det_form_the conj_but_are_normalized prep_in_are_form neg_are_not nsubj_are_productions advmod_are_Obviously det_productions_these
P03-1019	J97-3002	o	The first constraints are based on inversion transduction grammars -LRB- ITG -RRB- -LRB- Wu 1995 Wu 1997 -RRB-	amod_Wu_1997 dep_Wu_Wu appos_Wu_1995 dep_grammars_Wu appos_grammars_ITG nn_grammars_transduction nn_grammars_inversion prep_on_based_grammars auxpass_based_are nsubjpass_based_constraints amod_constraints_first det_constraints_The
P03-1019	J97-3002	o	the parse trees of the simple grammar in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu amod_grammar_simple det_grammar_the prep_trees_in prep_of_trees_grammar nn_trees_parse det_trees_the dep_``_trees
P03-1019	J97-3002	o	With this constraint each of these binary trees is unique and equivalent to a parse tree of the canonical-form grammar in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu amod_grammar_canonical-form det_grammar_the prep_of_tree_grammar nn_tree_parse det_tree_a prep_to_equivalent_tree nsubj_equivalent_each prep_unique_in conj_and_unique_equivalent cop_unique_is nsubj_unique_each prep_with_unique_constraint amod_trees_binary det_trees_these prep_of_each_trees det_constraint_this
P03-1019	J97-3002	o	In -LRB- Wu 1997 -RRB- these forbidden subsequences are called inside-out transpositions	amod_transpositions_inside-out dep_called_transpositions auxpass_called_are nsubjpass_called_subsequences prep_called_In amod_subsequences_forbidden det_subsequences_these amod_Wu_1997 dep_In_Wu
P03-2041	J97-3002	o	3However the binary-branching SCFGs used by Wu -LRB- 1997 -RRB- and Alshawi et al.	nn_al._et nn_al._Alshawi conj_and_Wu_al. appos_Wu_1997 agent_used_al. agent_used_Wu vmod_SCFGs_used amod_SCFGs_binary-branching det_SCFGs_the dep_3However_SCFGs
P03-2041	J97-3002	o	Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knight 2001 -RRB-	amod_Yamada_2001 conj_and_Yamada_Knight nn_al._et nn_al._Alshawi dep_Wu_Knight dep_Wu_Yamada appos_Wu_2000 dep_Wu_al. appos_Wu_1997 appos_grammar_Wu amod_grammar_context-free amod_grammar_synchronous prep_of_forms_grammar prep_to_limited_forms auxpass_limited_been aux_limited_has nsubjpass_limited_work amod_grammars_synchronous amod_grammars_statistical prep_in_work_grammars amod_work_Previous
P04-1060	J97-3002	o	-LRB- Wu 1997 -RRB- also includes a brief discussion of crossing constraints that can be derived from phrase structure correspondences	nn_correspondences_structure nn_correspondences_phrase prep_from_derived_correspondences auxpass_derived_be aux_derived_can nsubjpass_derived_that rcmod_constraints_derived amod_constraints_crossing prep_of_discussion_constraints amod_discussion_brief det_discussion_a dobj_includes_discussion advmod_includes_also nsubj_includes_Wu amod_Wu_1997
P04-1083	J97-3002	o	This normal form allows simpler algorithm descriptions than the normal forms used by Wu -LRB- 1997 -RRB- and Melamed -LRB- 2003 -RRB-	appos_Melamed_2003 conj_and_Wu_Melamed appos_Wu_1997 agent_used_Melamed agent_used_Wu vmod_forms_used amod_forms_normal det_forms_the prep_than_descriptions_forms nn_descriptions_algorithm amod_descriptions_simpler dobj_allows_descriptions nsubj_allows_form amod_form_normal det_form_This
P04-1083	J97-3002	o	Item Form a32 a2 a49a51 a15 a52 a49 a51a16a33 Goal a32a35a34 a49 a51 a15 a23a4a3 a12 a0a36a5 a24 a49 a51a37a33 Inference Rules Scan component d a10a38a8 a7 a8 a0 a39a41a40a43a42a44 a44a45 a23a25a24 a49 a5a47a46 a49 a2 a23a25a24 a5a49a48 a49 a51 a50 a23a25a24 a49 a5a47a46 a49 a20a43a5 a3a22 a23a25a24 a5a49a48 a49 a51 a51a14a52 a52 a53 a54a55 a55 a56 a23a25a24 a49 a5a47a46 a49 a2 a23a25a24 a5a49a48 a49 a51 a50 a23a25a24 a49 a5a47a46 a49a23 a19a57a24 a10a13a12 a19 a24 a23a25a24 a5a49a48 a49 a51 a58a59 a59 a60 Compose a61a63a62a65a64 a66a68a67a69 a64 a66a71a70 a61a35a72a37a64 a66a68a67a73 a64 a66a71a70a36a74a76a75 a32a78a77 a64 a66a76a67a69 a64 a66a80a79a81a73 a64 a66 a14 a62a82a64 a66 a14 a72a37a64 a66 a33 a10 a77 a64 a66 a67a69 a64 a66a37a83 a73 a64 a66 a18 Figure 3 Logic C -LRB- C for CKY -RRB- These constraints are enforced by the d-span operators a84 and a85 Parser C is conceptually simpler than the synchronous parsers of Wu -LRB- 1997 -RRB- Alshawi et al.	nn_al._et nn_al._Alshawi dep_Wu_al. appos_Wu_1997 prep_of_parsers_Wu amod_parsers_synchronous det_parsers_the prep_than_simpler_parsers advmod_simpler_conceptually cop_simpler_is nsubj_simpler_C nn_C_Parser conj_and_a84_a85 dep_operators_a85 dep_operators_a84 amod_operators_d-span det_operators_the parataxis_enforced_simpler agent_enforced_operators auxpass_enforced_are nsubjpass_enforced_constraints det_constraints_These nn_constraints_C prep_for_C_CKY appos_C_C nn_C_Logic dep_Figure_enforced num_Figure_3 nn_Figure_a18 nn_Figure_a66 nn_Figure_a64 nn_Figure_a73 nn_Figure_a66a37a83 nn_Figure_a64 nn_Figure_a67a69 nn_Figure_a66 nn_Figure_a64 nn_Figure_a77 nn_Figure_a10 nn_Figure_a33 nn_Figure_a66 nn_Figure_a72a37a64 nn_Figure_a14 nn_Figure_a66 nn_Figure_a62a82a64 nn_Figure_a14 nn_Figure_a66 nn_Figure_a64 nn_Figure_a66a80a79a81a73 nn_Figure_a64 nn_Figure_a66a76a67a69 nn_Figure_a64 num_Figure_a32a78a77 nn_Figure_a66a71a70a36a74a76a75 nn_Figure_a64 nn_Figure_a66a68a67a73 nn_Figure_a61a35a72a37a64 nn_Figure_a66a71a70 nn_Figure_a64 nn_Figure_a66a68a67a69 nn_Figure_a61a63a62a65a64 dep_Compose_Figure nn_Compose_a60 nn_Compose_a59 nn_Compose_a58a59 nn_Compose_a51 nn_Compose_a49 num_Compose_a5a49a48 nn_Compose_a23a25a24 nn_Compose_a24 nn_Compose_a19 nn_Compose_a10a13a12 nn_Compose_a19a57a24 nn_Compose_a49a23 nn_Compose_a5a47a46 nn_Compose_a49 nn_Compose_a23a25a24 nn_Compose_a50 nn_Compose_a51 nn_Compose_a49 num_Compose_a5a49a48 nn_Compose_a23a25a24 nn_Compose_a2 nn_Compose_a49 nn_Compose_a5a47a46 nn_Compose_a49 nn_Compose_a23a25a24 nn_Compose_a56 nn_Compose_a55 nn_Compose_a54a55 nn_Compose_a53 nn_Compose_a52 nn_Compose_a51a14a52 nn_Compose_a51 nn_Compose_a49 num_Compose_a5a49a48 nn_Compose_a23a25a24 nn_Compose_a3a22 nn_Compose_a20a43a5 nn_Compose_a49 nn_Compose_a5a47a46 nn_Compose_a49 nn_Compose_a23a25a24 nn_Compose_a50 nn_Compose_a51 nn_Compose_a49 num_Compose_a5a49a48 nn_Compose_a23a25a24 nn_Compose_a2 nn_Compose_a49 nn_Compose_a5a47a46 nn_Compose_a49 nn_Compose_a23a25a24 nn_Compose_a44a45 nn_Compose_a39a41a40a43a42a44 nn_a0_a8 nn_a0_a7 nn_a0_a10a38a8 appos_d_a0 nn_d_component dep_Scan_Compose dobj_Scan_d nsubj_Scan_Rules nn_Rules_Inference num_Rules_a51a37a33 nn_Rules_a49 nn_Rules_a24 nn_Rules_a0a36a5 nn_Rules_a12 nn_Rules_a23a4a3 nn_Rules_a15 nn_Rules_a51 nn_Rules_a49 nn_Rules_a32a35a34 dep_Goal_Scan nn_Goal_a51a16a33 nn_Goal_a49 nn_Goal_a52 nn_Goal_a15 nn_Goal_a49a51 nn_Goal_a2 nn_Goal_a32 dep_Form_Goal nn_Form_Item
P04-1084	J97-3002	o	Thus GCNF is a more restrictive normal form than those used by Wu -LRB- 1997 -RRB- and Melamed -LRB- 2003 -RRB-	appos_Melamed_2003 conj_and_Wu_Melamed appos_Wu_1997 agent_used_Melamed agent_used_Wu vmod_those_used prep_than_form_those amod_form_normal amod_form_restrictive det_form_a cop_form_is nsubj_form_GCNF advmod_form_Thus advmod_restrictive_more
P04-1084	J97-3002	o	Inversion Transduction Grammar -LRB- ITG -RRB- -LRB- Wu 1997 -RRB- and Syntax-Directed Translation Schema -LRB- SDTS -RRB- -LRB- Aho and Ullman 1969 -RRB- lack both of these properties	det_properties_these prep_of_both_properties dobj_lack_both nsubj_lack_Schema nsubj_lack_Grammar dep_Aho_1969 conj_and_Aho_Ullman appos_Schema_Ullman appos_Schema_Aho appos_Schema_SDTS nn_Schema_Translation amod_Schema_Syntax-Directed num_Wu_1997 conj_and_Grammar_Schema appos_Grammar_Wu appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion
P04-3002	J97-3002	o	In previous alignment methods some researchers modeled the alignments with different statistical models -LRB- Wu 1997 Och and Ney 2000 Cherry and Lin 2003 -RRB-	amod_Cherry_2003 conj_and_Cherry_Lin dep_Och_Lin dep_Och_Cherry conj_and_Och_2000 conj_and_Och_Ney dep_Wu_2000 dep_Wu_Ney dep_Wu_Och appos_Wu_1997 dep_models_Wu amod_models_statistical amod_models_different det_alignments_the prep_with_modeled_models dobj_modeled_alignments nsubj_modeled_researchers prep_in_modeled_methods det_researchers_some nn_methods_alignment amod_methods_previous
P04-3032	J97-3002	o	The simplest -LRB- Wu 1997 -RRB- uses constit -LRB- np ,3,5 np ,4,8 -RRB- to denote a NP spanning positions 35 in the English string that is aligned with an NP spanning positions 48 in the Chinese string	amod_string_Chinese det_string_the prep_in_positions_string num_positions_48 dobj_spanning_positions vmod_NP_spanning det_NP_an prep_with_aligned_NP auxpass_aligned_is nsubjpass_aligned_that rcmod_string_aligned nn_string_English det_string_the prep_in_positions_string num_positions_35 dobj_spanning_positions vmod_NP_spanning det_NP_a dobj_denote_NP aux_denote_to num_np_,4,8 appos_np_np num_np_,3,5 dep_constit_np vmod_uses_denote dobj_uses_constit dep_uses_simplest nsubj_uses_The amod_Wu_1997 dep_simplest_Wu
P05-1033	J97-3002	o	In this respect it resembles Wus 264 bilingual bracketer -LRB- Wu 1997 -RRB- but ours uses a different extraction method that allows more than one lexical item in a rule in keeping with the phrasebased philosophy	amod_philosophy_phrasebased det_philosophy_the prep_with_keeping_philosophy det_rule_a amod_item_lexical num_item_one quantmod_one_than mwe_than_more prepc_in_allows_keeping prep_in_allows_rule dobj_allows_item nsubj_allows_that rcmod_method_allows nn_method_extraction amod_method_different det_method_a dobj_uses_method nsubj_uses_ours dep_Wu_1997 appos_bracketer_Wu amod_bracketer_bilingual num_bracketer_264 nn_bracketer_Wus conj_but_resembles_uses dobj_resembles_bracketer nsubj_resembles_it prep_in_resembles_respect det_respect_this
P05-1058	J97-3002	o	In recent years many researchers have employed statistical models -LRB- Wu 1997 Och and Ney 2003 Cherry and Lin 2003 -RRB- or association measures -LRB- Smadja et al. 1996 Ahrenberg et al. 1998 Tufis and Barbu 2002 -RRB- to build alignment links	nn_links_alignment dobj_build_links aux_build_to nsubj_build_measures nsubj_build_Cherry nsubj_build_2003 nsubj_build_Ney nsubj_build_Och dep_Tufis_2002 conj_and_Tufis_Barbu num_Ahrenberg_1998 nn_Ahrenberg_al. nn_Ahrenberg_et dep_Smadja_Barbu dep_Smadja_Tufis conj_Smadja_Ahrenberg appos_Smadja_1996 dep_Smadja_al. nn_Smadja_et dep_measures_Smadja nn_measures_association dep_Cherry_2003 conj_and_Cherry_Lin conj_or_Och_measures conj_and_Och_Lin conj_and_Och_Cherry conj_and_Och_2003 conj_and_Och_Ney parataxis_Wu_build appos_Wu_1997 dep_models_Wu amod_models_statistical dobj_employed_models aux_employed_have nsubj_employed_researchers prep_in_employed_years amod_researchers_many amod_years_recent
P05-1059	J97-3002	o	Wu -LRB- 1997 -RRB- demonstrated that for pairs of sentences that are less than 16 words the ITG alignment space has a good coverage over all possibilities	det_possibilities_all prep_over_coverage_possibilities amod_coverage_good det_coverage_a dobj_has_coverage nsubj_has_space prep_for_has_pairs mark_has_that nn_space_alignment nn_space_ITG det_space_the num_words_16 prep_than_less_words cop_less_are nsubj_less_that rcmod_sentences_less prep_of_pairs_sentences ccomp_demonstrated_has nsubj_demonstrated_Wu appos_Wu_1997
P05-1059	J97-3002	o	The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar it has a start symbol S a single preterminal C and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment as discussed by Wu -LRB- 1997 -RRB- and Zens and Ney -LRB- 2003 -RRB-	appos_Ney_2003 conj_and_Wu_Ney conj_and_Wu_Zens appos_Wu_1997 prep_by_discussed_Ney prep_by_discussed_Zens prep_by_discussed_Wu mark_discussed_as amod_alignment_word-level amod_alignment_given det_alignment_any advcl_generate_discussed dobj_generate_alignment aux_generate_can nsubj_generate_parse mark_generate_that num_parse_one quantmod_one_only ccomp_ensure_generate aux_ensure_to xcomp_used_ensure vmod_A_used conj_and_A_B dep_nonterminals_B dep_nonterminals_A amod_nonterminals_intermediate num_nonterminals_two amod_C_preterminal amod_C_single det_C_a conj_and_S_nonterminals appos_S_C nn_S_symbol nn_S_start det_S_a dobj_has_nonterminals dobj_has_S nsubj_has_it nn_grammar_bracketing amod_grammar_primitive det_grammar_the prep_than_labels_grammar amod_labels_structural amod_labels_more parataxis_has_has dobj_has_labels nsubj_has_ITG poss_experiments_our prep_in_apply_experiments nsubj_apply_we rcmod_ITG_apply det_ITG_The
P05-1059	J97-3002	o	1 Introduction The Inversion Transduction Grammar -LRB- ITG -RRB- of Wu -LRB- 1997 -RRB- is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages	num_languages_two amod_sentences_equivalent advmod_sentences_translationally prep_of_pairs_sentences prep_of_alignments_pairs amod_alignments_word-level prep_in_producing_languages dobj_producing_alignments prepc_for_algorithm_producing amod_algorithm_motivated det_algorithm_a cop_algorithm_is nsubj_algorithm_Grammar advmod_motivated_syntactically appos_Wu_1997 prep_of_Grammar_Wu appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion det_Grammar_The rcmod_Introduction_algorithm num_Introduction_1
P05-1066	J97-3002	o	For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems -LRB- e.g. see -LRB- Alshawi 1996 Wu 1997 Yamada and Knight 2001 Gildea 2003 Melamed 2004 Graehl and Knight 2004 Och et al. 2004 Xia and McCord 2004 -RRB- -RRB-	dep_Xia_2004 conj_and_Xia_McCord nn_al._et nn_al._Och num_Melamed_2004 num_Gildea_2003 dep_Wu_McCord dep_Wu_Xia num_Wu_2004 dep_Wu_al. num_Wu_2004 conj_and_Wu_Knight conj_and_Wu_Graehl conj_and_Wu_Melamed conj_and_Wu_Gildea num_Wu_2001 conj_and_Wu_Knight conj_and_Wu_Yamada num_Wu_1997 dep_Alshawi_Knight dep_Alshawi_Graehl dep_Alshawi_Melamed dep_Alshawi_Gildea dep_Alshawi_Knight dep_Alshawi_Yamada dep_Alshawi_Wu appos_Alshawi_1996 dep_see_Alshawi dep_e.g._see dep_systems_e.g. nn_systems_translation nn_systems_machine amod_systems_statistical prep_within_information_systems amod_information_syntactic dobj_incorporate_information nsubj_incorporate_which rcmod_methods_incorporate prep_in_interest_methods prep_of_deal_interest amod_deal_great det_deal_a advmod_deal_currently nsubj_is_deal expl_is_there prep_for_is_reason det_reason_this ccomp_``_is
P05-1066	J97-3002	o	2.1.2 Research on Syntax-Based SMT A number of researchers -LRB- Alshawi 1996 Wu 1997 Yamada and Knight 2001 Gildea 2003 Melamed 2004 Graehl and Knight 2004 Galley et al. 2004 -RRB- have proposed models where the translation process involves syntactic representations of the source and/or target languages	nn_languages_target conj_and/or_source_languages det_source_the prep_of_representations_languages prep_of_representations_source amod_representations_syntactic dobj_involves_representations nsubj_involves_process advmod_involves_where nn_process_translation det_process_the rcmod_models_involves amod_models_proposed dobj_have_models num_Galley_2004 nn_Galley_al. nn_Galley_et num_Graehl_2004 conj_and_Graehl_Knight num_Melamed_2004 num_Gildea_2003 num_Wu_1997 dep_Alshawi_Galley conj_and_Alshawi_Knight conj_and_Alshawi_Graehl conj_and_Alshawi_Melamed conj_and_Alshawi_Gildea conj_and_Alshawi_2001 conj_and_Alshawi_Knight conj_and_Alshawi_Yamada conj_and_Alshawi_Wu appos_Alshawi_1996 appos_researchers_Graehl appos_researchers_Melamed appos_researchers_Gildea appos_researchers_2001 appos_researchers_Knight appos_researchers_Yamada appos_researchers_Wu appos_researchers_Alshawi prep_of_number_researchers det_number_A nn_number_SMT amod_number_Syntax-Based dep_Research_have prep_on_Research_number num_Research_2.1.2 dep_``_Research
P05-1067	J97-3002	o	-LRB- Wu 1997 -RRB- introduced a polynomial-time solution for the alignment problem based on synchronous binary trees	amod_trees_binary amod_trees_synchronous prep_on_based_trees vmod_problem_based nn_problem_alignment det_problem_the prep_for_solution_problem amod_solution_polynomial-time det_solution_a dobj_introduced_solution nsubj_introduced_Wu amod_Wu_1997
P06-1062	J97-3002	o	For example -LRB- Wu 1997 Alshawi Bangalore and Douglas 2000 Yamada and Knight 2001 -RRB- have studied synchronous context free grammar	amod_grammar_free nn_grammar_context amod_grammar_synchronous dobj_studied_grammar aux_studied_have nsubj_studied_Wu prep_for_studied_example amod_Yamada_2001 conj_and_Yamada_Knight num_Douglas_2000 conj_and_Alshawi_Douglas conj_and_Alshawi_Bangalore dep_Wu_Knight dep_Wu_Yamada dep_Wu_Douglas dep_Wu_Bangalore dep_Wu_Alshawi num_Wu_1997
P06-1066	J97-3002	o	Here under the ITG constraint -LRB- Wu 1997 Zens et al. 2004 -RRB- we need to consider just two kinds of reorderings straight and inverted between two consecutive blocks	amod_blocks_consecutive num_blocks_two prep_between_straight_blocks conj_and_straight_inverted amod_reorderings_inverted amod_reorderings_straight prep_of_kinds_reorderings num_kinds_two quantmod_two_just dobj_consider_kinds aux_consider_to xcomp_need_consider nsubj_need_we prep_under_need_constraint advmod_need_Here dep_al._2004 nn_al._et nn_al._Zens dep_Wu_al. appos_Wu_1997 appos_constraint_Wu nn_constraint_ITG det_constraint_the
P06-1077	J97-3002	o	Wu -LRB- 1997 -RRB- proposes Inversion Transduction Grammars treating translation as a process of parallel parsing of the source and target language via a synchronized grammar	amod_grammar_synchronized det_grammar_a nn_language_target conj_and_source_language det_source_the prep_of_parsing_language prep_of_parsing_source amod_parsing_parallel prep_of_process_parsing det_process_a prep_via_treating_grammar prep_as_treating_process dobj_treating_translation vmod_Grammars_treating nn_Grammars_Transduction nn_Grammars_Inversion dobj_proposes_Grammars nsubj_proposes_Wu appos_Wu_1997
P06-1098	J97-3002	p	In the hierarchical phrase-based model -LRB- Chiang 2005 -RRB- and an inversion transduction grammar -LRB- ITG -RRB- -LRB- Wu 1997 -RRB- the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side	amod_side_righthand det_side_the prep_in_allowed_side auxpass_allowed_are prep_at_allowed_non-terminals advmod_allowed_where num_non-terminals_two amod_non-terminals_most rcmod_form_allowed amod_form_binarized det_form_a prep_to_restricting_form agent_resolved_restricting auxpass_resolved_is nsubjpass_resolved_grammar cc_resolved_and prep_in_resolved_model det_problem_the dep_Wu_1997 appos_grammar_problem appos_grammar_Wu appos_grammar_ITG nn_grammar_transduction nn_grammar_inversion det_grammar_an dep_Chiang_2005 appos_model_Chiang amod_model_phrase-based amod_model_hierarchical det_model_the
P06-1121	J97-3002	o	7 Related work Similarly to -LRB- Poutsma 2000 Wu 1997 Yamada and Knight 2001 Chiang 2005 -RRB- the rules discussed in this paper are equivalent to productions of synchronous tree substitution grammars	nn_grammars_substitution nn_grammars_tree amod_grammars_synchronous prep_of_productions_grammars prep_to_equivalent_productions cop_equivalent_are nsubj_equivalent_Poutsma mark_equivalent_to det_paper_this prep_in_discussed_paper vmod_rules_discussed det_rules_the dep_rules_Knight dep_rules_Yamada dep_rules_Wu dep_Chiang_2005 dep_Wu_Chiang num_Wu_2001 conj_and_Wu_Knight conj_and_Wu_Yamada num_Wu_1997 dep_Poutsma_rules appos_Poutsma_2000 advmod_to_Similarly rcmod_work_equivalent amod_work_Related num_work_7 dep_``_work
P06-1123	J97-3002	p	Wu -LRB- 1997 -RRB- has been unable to find real examples of cases where hierarchical alignment would fail under these conditions at least in fixed-word-order languages that are lightly inflected such as English and Chinese -LRB- p. 385 -RRB-	num_p._385 conj_and_English_Chinese prep_such_as_inflected_Chinese prep_such_as_inflected_English advmod_inflected_lightly auxpass_inflected_are nsubjpass_inflected_that rcmod_languages_inflected nn_languages_fixed-word-order pobj_at_least det_conditions_these prep_under_fail_conditions aux_fail_would nsubj_fail_alignment advmod_fail_where amod_alignment_hierarchical rcmod_cases_fail prep_of_examples_cases amod_examples_real dobj_find_examples aux_find_to dep_unable_p. prep_in_unable_languages advmod_unable_at xcomp_unable_find cop_unable_been aux_unable_has nsubj_unable_Wu appos_Wu_1997
P06-1123	J97-3002	o	Following Wu -LRB- 1997 -RRB- the prevailing opinion in the research community has been that more complex patterns of word alignment in real bitexts are mostly attributable to alignment errors	nn_errors_alignment prep_to_attributable_errors advmod_attributable_mostly cop_attributable_are nsubj_attributable_patterns mark_attributable_that amod_bitexts_real nn_alignment_word prep_in_patterns_bitexts prep_of_patterns_alignment nn_patterns_complex amod_patterns_more ccomp_been_attributable aux_been_has nsubj_been_Wu dep_been_Following nn_community_research det_community_the prep_in_opinion_community amod_opinion_prevailing det_opinion_the appos_Wu_opinion appos_Wu_1997 advcl_``_been
P06-1123	J97-3002	o	A hierarchical alignment algorithm is a type of synchronous parser where instead of constraining inferences by the production rules of a grammar the constraints come from word alignments and possibly other sources -LRB- Wu 1997 Melamed and Wang 2005 -RRB-	amod_Melamed_2005 conj_and_Melamed_Wang dep_Wu_Wang dep_Wu_Melamed appos_Wu_1997 appos_sources_Wu amod_sources_other advmod_sources_possibly nn_alignments_word conj_and_come_sources prep_from_come_alignments nsubj_come_constraints prep_instead_of_come_inferences advmod_come_where det_constraints_the det_grammar_a prep_of_rules_grammar nn_rules_production det_rules_the prep_by_inferences_rules amod_inferences_constraining rcmod_parser_sources rcmod_parser_come amod_parser_synchronous prep_of_type_parser det_type_a cop_type_is nsubj_type_algorithm nn_algorithm_alignment amod_algorithm_hierarchical det_algorithm_A
P06-2014	J97-3002	o	Some methods parse two flat strings at once using a bitext grammar -LRB- Wu 1997 -RRB-	dep_Wu_1997 appos_grammar_Wu amod_grammar_bitext det_grammar_a dobj_using_grammar advmod_using_once amod_strings_flat num_strings_two prepc_at_parse_using dobj_parse_strings nsubj_parse_methods det_methods_Some ccomp_``_parse
P06-2014	J97-3002	p	The Inversion Transduction Grammar or ITG formalism described in -LRB- Wu 1997 -RRB- is well suited for our purposes	poss_purposes_our prep_for_suited_purposes advmod_suited_well auxpass_suited_is nsubjpass_suited_Wu mark_suited_in amod_Wu_1997 advcl_described_suited nn_formalism_ITG vmod_Grammar_described conj_or_Grammar_formalism nn_Grammar_Transduction nn_Grammar_Inversion det_Grammar_The dep_``_formalism dep_``_Grammar
P06-2014	J97-3002	n	Wu -LRB- 1997 -RRB- provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints	nn_constraints_ITG agent_eliminated_constraints auxpass_eliminated_are nsubjpass_eliminated_alignments mark_eliminated_that amod_alignments_incorrect advmod_incorrect_only ccomp_evidence_eliminated amod_evidence_anecdotal dobj_provides_evidence nsubj_provides_Wu appos_Wu_1997
P06-2014	J97-3002	p	Fortunately Wu -LRB- 1997 -RRB- provides a method to have an ITG respect a known partial structure	amod_structure_partial amod_structure_known det_structure_a dep_respect_structure nn_respect_ITG det_respect_an dobj_have_respect aux_have_to vmod_method_have det_method_a dobj_provides_method nsubj_provides_Wu advmod_provides_Fortunately appos_Wu_1997
P06-2036	J97-3002	o	Variations of SCFGs go back to Aho and Ullman -LRB- 1972 -RRB- s Syntax-Directed Translation Schemata but also include the Inversion Transduction Grammars in Wu -LRB- 1997 -RRB- which restrict grammar rules to be binary the synchronous grammars in Chiang -LRB- 2005 -RRB- which use only a single nonterminal symbol and the Multitext Grammars in Melamed -LRB- 2003 -RRB- which allow independent rewriting as well as other tree-based models such as Yamada and Knight -LRB- 2001 -RRB- and Galley et al.	nn_al._et nn_al._Galley appos_Knight_2001 conj_and_Yamada_al. conj_and_Yamada_Knight prep_such_as_models_al. prep_such_as_models_Knight prep_such_as_models_Yamada amod_models_tree-based amod_models_other conj_and_rewriting_models dep_independent_models dep_independent_rewriting acomp_allow_independent nsubj_allow_which appos_Melamed_2003 prep_in_Grammars_Melamed nn_Grammars_Multitext det_Grammars_the amod_symbol_nonterminal amod_symbol_single det_symbol_a advmod_symbol_only dobj_use_symbol nsubj_use_which conj_and_Chiang_Grammars rcmod_Chiang_use appos_Chiang_2005 rcmod_grammars_allow prep_in_grammars_Grammars prep_in_grammars_Chiang amod_grammars_synchronous det_grammars_the cop_binary_be aux_binary_to nn_rules_grammar xcomp_restrict_binary dobj_restrict_rules nsubj_restrict_which appos_Wu_1997 appos_Grammars_grammars rcmod_Grammars_restrict prep_in_Grammars_Wu nn_Grammars_Transduction nn_Grammars_Inversion det_Grammars_the dobj_include_Grammars advmod_include_also nn_Schemata_Translation amod_Schemata_Syntax-Directed conj_but_s_include dobj_s_Schemata appos_Ullman_1972 conj_and_Aho_Ullman dep_go_include dep_go_s prep_to_go_Ullman prep_to_go_Aho advmod_go_back nsubj_go_Variations prep_of_Variations_SCFGs
P06-2112	J97-3002	o	Many researchers build alignment links with bilingual corpora -LRB- Wu 1997 Och and Ney 2003 Cherry and Lin 2003 Zhang and Gildea 2005 -RRB-	amod_Cherry_2005 conj_and_Cherry_Gildea conj_and_Cherry_Zhang conj_and_Cherry_2003 conj_and_Cherry_Lin dep_Och_Gildea dep_Och_Zhang dep_Och_2003 dep_Och_Lin dep_Och_Cherry conj_and_Och_2003 conj_and_Och_Ney dep_Wu_2003 dep_Wu_Ney dep_Wu_Och appos_Wu_1997 dep_corpora_Wu amod_corpora_bilingual prep_with_links_corpora nn_links_alignment dobj_build_links nsubj_build_researchers amod_researchers_Many ccomp_``_build
P06-2117	J97-3002	o	In recent years many researchers build alignment links with bilingual corpora -LRB- Wu 1997 Och and Ney 2003 Cherry and Lin 2003 Wu et al. 2005 Zhang and Gildea 2005 -RRB-	num_Wu_2005 nn_Wu_al. nn_Wu_et amod_Cherry_2005 conj_and_Cherry_Gildea conj_and_Cherry_Zhang conj_and_Cherry_Wu conj_and_Cherry_2003 conj_and_Cherry_Lin dep_Och_Gildea dep_Och_Zhang dep_Och_Wu dep_Och_2003 dep_Och_Lin dep_Och_Cherry conj_and_Och_2003 conj_and_Och_Ney dep_Wu_2003 dep_Wu_Ney dep_Wu_Och appos_Wu_1997 dep_corpora_Wu amod_corpora_bilingual prep_with_links_corpora nn_links_alignment dobj_build_links nsubj_build_researchers prep_in_build_years amod_researchers_many amod_years_recent
P06-2122	J97-3002	o	953 2 Bilexicalization of Inversion Transduction Grammar The Inversion Transduction Grammar of Wu -LRB- 1997 -RRB- models word alignment between a translation pair of sentences by assuming a binary synchronous tree on top of both sides	det_sides_both prep_on_top_of_tree_sides amod_tree_synchronous amod_tree_binary det_tree_a dobj_assuming_tree prep_of_pair_sentences nn_pair_translation det_pair_a prep_between_alignment_pair nn_alignment_word nn_models_Wu appos_Wu_1997 dep_Grammar_alignment prep_of_Grammar_models nn_Grammar_Transduction nn_Grammar_Inversion nn_Grammar_The prepc_by_Grammar_assuming dobj_Grammar_Grammar dep_Transduction_Grammar dep_Inversion_Transduction prep_of_Bilexicalization_Inversion num_Bilexicalization_2 num_Bilexicalization_953
P06-2122	J97-3002	n	Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years -LRB- Aho and Ullman 1972 Wu 1997 Yamada and Knight 2001 Melamed 2003 Chiang 2005 -RRB- but have not been scaled to large corpora and long sentences until recently	pobj_until_recently amod_sentences_long amod_corpora_large prep_to_scaled_corpora auxpass_scaled_been neg_scaled_not aux_scaled_have num_Chiang_2005 conj_but_Melamed_scaled conj_but_Melamed_Chiang conj_but_Melamed_2003 num_Yamada_2001 conj_and_Yamada_Knight num_Wu_1997 prep_Aho_until conj_and_Aho_sentences conj_and_Aho_scaled conj_and_Aho_Chiang conj_and_Aho_2003 conj_and_Aho_Melamed conj_and_Aho_Knight conj_and_Aho_Yamada conj_and_Aho_Wu conj_and_Aho_1972 conj_and_Aho_Ullman amod_years_many dep_proposed_sentences dep_proposed_Melamed dep_proposed_Yamada dep_proposed_Wu dep_proposed_1972 dep_proposed_Ullman dep_proposed_Aho prep_for_proposed_years auxpass_proposed_been aux_proposed_have nsubjpass_proposed_formalisms det_language_each amod_property_context-free det_property_the prep_in_maintaining_language dobj_maintaining_property mark_maintaining_while nn_relationships_complex amod_relationships_such nn_relationships_modeling advcl_capable_maintaining prep_of_capable_relationships cop_capable_are nsubj_capable_that rcmod_formalisms_capable nn_formalisms_grammar amod_formalisms_Synchronous ccomp_``_proposed
P06-2122	J97-3002	o	In this paper we focus on the second issue constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu -LRB- 1997 -RRB-	appos_Wu_1997 prep_of_Grammar_Wu nn_Grammar_Transduction nn_Grammar_Inversion amod_Grammar_binary-branching det_Grammar_the det_grammar_the prep_to_constraining_Grammar dobj_constraining_grammar amod_issue_second det_issue_the xcomp_focus_constraining prep_on_focus_issue nsubj_focus_we prep_in_focus_paper det_paper_this
P07-1002	J97-3002	o	Alternatively order is modelled in terms of movement of automatically induced hierarchical structure of sentences -LRB- Chiang 2005 Wu 1997 -RRB-	amod_Wu_1997 dep_Chiang_Wu appos_Chiang_2005 appos_structure_Chiang prep_of_structure_sentences amod_structure_hierarchical amod_structure_induced advmod_induced_automatically prep_of_movement_structure prep_of_terms_movement prep_in_modelled_terms auxpass_modelled_is nsubjpass_modelled_order advmod_modelled_Alternatively
P07-1003	J97-3002	o	1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation being both algorithmically appealing -LRB- Melamed 2004 Wu 1997 -RRB- and empirically successful -LRB- Chiang 2005 Galley et al. 2006 -RRB-	num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Chiang_Galley dep_Chiang_2005 dep_successful_Chiang advmod_successful_empirically num_Wu_1997 dep_Melamed_Wu appos_Melamed_2004 conj_and_appealing_successful dep_appealing_Melamed advmod_appealing_algorithmically preconj_appealing_both cop_appealing_being nn_translation_machine amod_translation_statistical dep_approach_successful dep_approach_appealing prep_to_approach_translation amod_approach_promising det_approach_an cop_approach_are nsubj_approach_methods advmod_promising_increasingly amod_methods_Syntactic nn_methods_Introduction num_methods_1
P07-1020	J97-3002	o	A few exceptions are the hierarchical -LRB- possibly syntax-based -RRB- transduction models -LRB- Wu 1997 Alshawi et al. 1998 Yamada and Knight 2001 Chiang 2005 -RRB- and the string transduction models -LRB- Kanthak et al. 2005 -RRB-	amod_Kanthak_2005 dep_Kanthak_al. nn_Kanthak_et dep_models_Kanthak nn_models_transduction nn_models_string det_models_the num_Chiang_2005 conj_and_Yamada_Knight nn_al._et nn_al._Alshawi dep_Wu_Chiang amod_Wu_2001 dep_Wu_Knight dep_Wu_Yamada num_Wu_1998 dep_Wu_al. amod_Wu_1997 conj_and_models_models dep_models_Wu nn_models_transduction amod_models_hierarchical det_models_the cop_models_are nsubj_models_exceptions advmod_syntax-based_possibly dep_hierarchical_syntax-based amod_exceptions_few det_exceptions_A
P07-1039	J97-3002	o	-LRB- Wu 1997 -RRB- -RRB-	dep_Wu_1997 dep_''_Wu
P07-1039	J97-3002	o	We use a bootstrap approach in which we first extract 1-to-n word alignments using an existing word aligner and then estimate the confidence of those alignments to decide whether or not the n words have to be grouped if so this group is conwould thus be completely driven by the bilingual alignment process -LRB- see also -LRB- Wu 1997 Tiedemann 2003 -RRB- for related considerations -RRB-	amod_considerations_related dep_Tiedemann_2003 dep_Wu_Tiedemann appos_Wu_1997 prep_for_see_considerations dep_see_Wu advmod_see_also nn_process_alignment amod_process_bilingual det_process_the dep_driven_see agent_driven_process advmod_driven_completely auxpass_driven_be advmod_driven_thus dep_conwould_driven cop_conwould_is nsubj_conwould_group det_group_this mark_so_if auxpass_grouped_be aux_grouped_to xcomp_have_grouped nsubj_have_words dep_have_not nn_words_n det_words_the cc_not_or mark_not_whether ccomp_decide_have aux_decide_to det_alignments_those prep_of_confidence_alignments det_confidence_the vmod_estimate_decide dobj_estimate_confidence advmod_estimate_then nn_aligner_word amod_aligner_existing det_aligner_an dobj_using_aligner conj_and_alignments_estimate vmod_alignments_using nn_alignments_word amod_alignments_1-to-n nn_alignments_extract advmod_alignments_first rcmod_we_conwould dep_we_so dep_we_estimate dep_we_alignments prep_in_approach_we prep_in_approach_which nn_approach_bootstrap det_approach_a dobj_use_approach nsubj_use_We
P07-1039	J97-3002	o	Note that the need to consider segmentation and alignment at the same time is also mentioned in -LRB- Tiedemann 2003 -RRB- and related issues are reported in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu prep_reported_in auxpass_reported_are nsubjpass_reported_Note amod_issues_related conj_and_Tiedemann_issues dep_Tiedemann_2003 prep_in_mentioned_issues prep_in_mentioned_Tiedemann advmod_mentioned_also auxpass_mentioned_is nsubjpass_mentioned_need mark_mentioned_that amod_time_same det_time_the conj_and_segmentation_alignment prep_at_consider_time dobj_consider_alignment dobj_consider_segmentation aux_consider_to vmod_need_consider det_need_the ccomp_Note_mentioned
P07-1090	J97-3002	o	Instead of using Inversion Transduction Grammar -LRB- ITG -RRB- -LRB- Wu 1997 -RRB- directly we will discuss an ITG extension to accommodate gapping	dobj_accommodate_gapping aux_accommodate_to vmod_extension_accommodate nn_extension_ITG det_extension_an dobj_discuss_extension aux_discuss_will nsubj_discuss_we prepc_instead_of_discuss_using dep_Wu_1997 appos_Grammar_Wu appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion advmod_using_directly dobj_using_Grammar
P07-1090	J97-3002	n	The utility of ITG as a reordering constraint for most language pairs is well-known both empirically -LRB- Zens and Ney 2003 -RRB- and analytically -LRB- Wu 1997 -RRB- howeverITGsstraight -LRB- monotone -RRB- andinverted -LRB- reverse -RRB- rules exhibit strong cohesiveness which is inadequate to express orientations that require gaps	dobj_require_gaps nsubj_require_that rcmod_orientations_require dobj_express_orientations aux_express_to xcomp_inadequate_express cop_inadequate_is nsubj_inadequate_which rcmod_cohesiveness_inadequate amod_cohesiveness_strong dobj_exhibit_cohesiveness nsubj_exhibit_rules advmod_exhibit_analytically advmod_exhibit_empirically amod_rules_andinverted nn_rules_howeverITGsstraight dep_andinverted_reverse appos_howeverITGsstraight_monotone amod_Wu_1997 dep_analytically_Wu amod_Zens_2003 conj_and_Zens_Ney conj_and_empirically_analytically dep_empirically_Ney dep_empirically_Zens preconj_empirically_both ccomp_well-known_exhibit cop_well-known_is nsubj_well-known_utility nn_pairs_language amod_pairs_most prep_for_constraint_pairs nn_constraint_reordering det_constraint_a prep_as_utility_constraint prep_of_utility_ITG det_utility_The ccomp_``_well-known
P07-1108	J97-3002	p	1 Introduction For statistical machine translation -LRB- SMT -RRB- phrasebased methods -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and syntax-based methods -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knignt 2001 Melamed 2004 Chiang 2005 Quick et al. 2005 Mellebeek et al. 2006 -RRB- outperform word-based methods -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_methods_Brown amod_methods_word-based dobj_outperform_methods nsubj_outperform_methods nsubj_outperform_methods ccomp_outperform_Introduction nn_al._et nn_al._Mellebeek dep_al._2005 nn_al._et nn_al._Quick num_Chiang_2005 num_Melamed_2004 conj_and_Yamada_al. conj_and_Yamada_Chiang conj_and_Yamada_Melamed conj_and_Yamada_2001 conj_and_Yamada_Knignt num_al._2000 nn_al._et nn_al._Alshawi amod_Wu_2006 dep_Wu_al. dep_Wu_al. dep_Wu_Chiang dep_Wu_Melamed dep_Wu_2001 dep_Wu_Knignt dep_Wu_Yamada dep_Wu_al. amod_Wu_1997 dep_methods_Wu amod_methods_syntax-based dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_methods_methods dep_methods_Koehn amod_methods_phrasebased appos_translation_SMT nn_translation_machine amod_translation_statistical prep_for_Introduction_translation num_Introduction_1
P07-1121	J97-3002	o	Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars -LRB- SCFG -RRB- -LRB- Wu 1997 -RRB- and synchronous treesubstitutiongrammars -LRB- STSG -RRB- -LRB- YamadaandKnight 2001 -RRB-	amod_YamadaandKnight_2001 dep_treesubstitutiongrammars_YamadaandKnight appos_treesubstitutiongrammars_STSG amod_treesubstitutiongrammars_synchronous num_Wu_1997 dep_grammars_Wu appos_grammars_SCFG amod_grammars_context-free amod_grammars_synchronous cop_grammars_are amod_SMT_syntaxbased prep_in_use_SMT conj_and_put_treesubstitutiongrammars conj_and_put_grammars prep_into_put_use advmod_put_successfully prep_among_put_formalisms nn_formalisms_grammar det_formalisms_the
P08-1009	J97-3002	o	methods for syntactic SMT held to this assumption in its entirety -LRB- Wu 1997 Yamada and Knight 2001 -RRB-	amod_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Knight dep_Wu_Yamada num_Wu_1997 appos_entirety_Wu poss_entirety_its prep_in_assumption_entirety det_assumption_this prep_to_held_assumption vmod_SMT_held amod_SMT_syntactic prep_for_methods_SMT
P08-1012	J97-3002	o	2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar -LRB- Wu 1997 -RRB- as the generative framework	amod_framework_generative det_framework_the dep_Wu_1997 appos_Grammar_Wu nn_Grammar_Transduction nn_Grammar_Inversion prep_of_extension_Grammar amod_extension_phrasal det_extension_a prep_as_use_framework dobj_use_extension nsubj_use_We rcmod_Grammar_use nn_Grammar_Transduction nn_Grammar_Inversion nn_Grammar_Phrasal num_Grammar_2 dep_``_Grammar
P08-1023	J97-3002	o	Depending on the type of input these efforts can be divided into two broad categories the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar -LRB- Wu 1997 Chiang 2005 Galley et al. 2006 -RRB- and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string -LRB- Lin 2004 Ding and Palmer 2005 Quirk et al. 2005 Liu et al. 2006 Huang et al. 2006 -RRB-	num_Huang_2006 nn_Huang_al. nn_Huang_et num_Liu_2006 nn_Liu_al. nn_Liu_et num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Ding_2005 conj_and_Ding_Palmer dep_Lin_Huang dep_Lin_Liu dep_Lin_Quirk dep_Lin_Palmer dep_Lin_Ding num_Lin_2004 appos_tree_Lin conj_or_tree_string nn_tree_target det_tree_a prep_into_converted_string prep_into_converted_tree advmod_converted_directly auxpass_converted_be aux_converted_to vmod_tree_converted nn_tree_parse det_tree_a advmod_tree_already cop_tree_is nsubj_tree_input poss_input_whose rcmod_systems_tree amod_systems_tree-based det_systems_the num_Galley_2006 nn_Galley_al. nn_Galley_et conj_and_Chiang_systems dep_Chiang_Galley num_Chiang_2005 dep_Wu_systems dep_Wu_Chiang appos_Wu_1997 dep_grammar_Wu amod_grammar_synchronous det_grammar_a prep_by_translated_grammar conj_and_parsed_translated advmod_parsed_simultaneously auxpass_parsed_be aux_parsed_to vmod_string_translated vmod_string_parsed det_string_a cop_string_is nsubj_string_input poss_input_whose rcmod_systems_string amod_systems_string-based det_systems_the amod_categories_broad num_categories_two dep_divided_systems prep_into_divided_categories auxpass_divided_be aux_divided_can nsubjpass_divided_efforts pobj_divided_type prepc_depending_on_divided_on det_efforts_these prep_of_type_input det_type_the
P08-1024	J97-3002	o	This is an instance of the ITG alignment algorithm -LRB- Wu 1997 -RRB-	amod_Wu_1997 appos_algorithm_Wu nn_algorithm_alignment nn_algorithm_ITG det_algorithm_the prep_of_instance_algorithm det_instance_an cop_instance_is nsubj_instance_This
P08-1025	J97-3002	o	Thus we are focusing on Inversion Transduction Grammars -LRB- Wu 1997 -RRB- which are an important subclass of SCFG	prep_of_subclass_SCFG amod_subclass_important det_subclass_an cop_subclass_are nsubj_subclass_which dep_Wu_1997 rcmod_Grammars_subclass appos_Grammars_Wu nn_Grammars_Transduction nn_Grammars_Inversion prep_on_focusing_Grammars aux_focusing_are nsubj_focusing_we advmod_focusing_Thus
P08-1064	J97-3002	p	Recently many syntax-based models have been proposed to address the above deficiencies -LRB- Wu 1997 Chiang 2005 Eisner 2003 Ding and Palmer 2005 Quirk et al 2005 Cowan et al. 2006 Zhang et al. 2007 Bod 2007 Yamada and Knight 2001 Liu et al. 2006 Liu et al. 2007 Gildea 2003 Poutsma 2000 Hearne and Way 2003 -RRB-	num_Poutsma_2000 num_Gildea_2003 num_Liu_2007 nn_Liu_al. nn_Liu_et num_Liu_2006 nn_Liu_al. nn_Liu_et num_Bod_2007 num_Zhang_2007 nn_Zhang_al. nn_Zhang_et nn_Cowan_al. nn_Cowan_et num_al_2005 nn_al_et nn_al_Quirk num_Eisner_2003 amod_Chiang_2003 conj_and_Chiang_Way conj_and_Chiang_Hearne conj_and_Chiang_Poutsma conj_and_Chiang_Gildea conj_and_Chiang_Liu conj_and_Chiang_Liu num_Chiang_2001 conj_and_Chiang_Knight conj_and_Chiang_Yamada conj_and_Chiang_Bod conj_and_Chiang_Zhang num_Chiang_2006 conj_and_Chiang_Cowan conj_and_Chiang_al num_Chiang_2005 conj_and_Chiang_Palmer conj_and_Chiang_Ding conj_and_Chiang_Eisner num_Chiang_2005 dep_Wu_Way dep_Wu_Hearne dep_Wu_Poutsma dep_Wu_Gildea dep_Wu_Liu dep_Wu_Liu dep_Wu_Knight dep_Wu_Yamada dep_Wu_Bod dep_Wu_Zhang dep_Wu_Cowan dep_Wu_al dep_Wu_Palmer dep_Wu_Ding dep_Wu_Eisner dep_Wu_Chiang appos_Wu_1997 appos_deficiencies_Wu amod_deficiencies_above det_deficiencies_the dobj_address_deficiencies aux_address_to xcomp_proposed_address auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models advmod_proposed_Recently amod_models_syntax-based amod_models_many
P08-1064	J97-3002	o	The formally syntax-based model for SMT was first advocated by Wu -LRB- 1997 -RRB-	appos_Wu_1997 agent_advocated_Wu advmod_advocated_first auxpass_advocated_was nsubjpass_advocated_model prep_for_model_SMT amod_model_syntax-based det_model_The advmod_syntax-based_formally
P08-1064	J97-3002	o	-LRB- 2006 -RRB- propose a MaxEnt-based reordering model for BTG -LRB- Wu 1997 -RRB- while Setiawan et al.	nn_al._et nn_al._Setiawan mark_al._while num_Wu_1997 appos_BTG_Wu prep_for_model_BTG nn_model_reordering amod_model_MaxEnt-based det_model_a advcl_propose_al. dobj_propose_model nsubj_propose_2006
P08-1114	J97-3002	o	-LRB- Chiang 2005 Chiang 2007 Wu 1997 -RRB- -RRB-	dep_Wu_1997 dep_Chiang_Wu num_Chiang_2007 dep_Chiang_Chiang appos_Chiang_2005 dep_''_Chiang
P08-2021	J97-3002	o	An alternative to tercom considered in this paper is to use the Inversion Transduction Grammar -LRB- ITG -RRB- formalism -LRB- Wu 1997 -RRB- which allows one to view the problem of alignment as a problem of bilingual parsing	amod_parsing_bilingual prep_of_problem_parsing det_problem_a prep_of_problem_alignment det_problem_the prep_as_view_problem dobj_view_problem aux_view_to xcomp_allows_view dobj_allows_one nsubj_allows_which amod_Wu_1997 rcmod_formalism_allows appos_formalism_Wu dep_Grammar_formalism appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion det_Grammar_the dobj_use_Grammar aux_use_to xcomp_is_use nsubj_is_alternative det_paper_this prep_in_considered_paper vmod_alternative_considered prep_to_alternative_tercom det_alternative_An
P08-2038	J97-3002	p	1 Introduction In recent years Bracketing Transduction Grammar -LRB- BTG -RRB- proposed by -LRB- Wu 1997 -RRB- has been widely used in statistical machine translation -LRB- SMT -RRB-	appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_used_translation advmod_used_widely auxpass_used_been aux_used_has nsubjpass_used_Introduction dep_Wu_1997 agent_proposed_Wu vmod_Grammar_proposed appos_Grammar_BTG nn_Grammar_Transduction dobj_Bracketing_Grammar amod_years_recent vmod_Introduction_Bracketing prep_in_Introduction_years num_Introduction_1
P09-1009	J97-3002	p	Research in this direction was pioneered by -LRB- Wu 1997 -RRB- who developed Inversion Transduction Grammars to capture crosslingual grammar variations such as phrase reorderings	nn_reorderings_phrase prep_such_as_variations_reorderings nn_variations_grammar amod_variations_crosslingual dobj_capture_variations aux_capture_to nn_Grammars_Transduction nn_Grammars_Inversion xcomp_developed_capture dobj_developed_Grammars nsubj_developed_who rcmod_Wu_developed dep_Wu_1997 agent_pioneered_Wu auxpass_pioneered_was nsubjpass_pioneered_Research det_direction_this prep_in_Research_direction
P09-1036	J97-3002	o	In this paper we implement the SDB model in a state-of-the-art phrase-based system which adapts a binary bracketing transduction grammar -LRB- BTG -RRB- -LRB- Wu 1997 -RRB- to phrase translation and reordering described in -LRB- Xiong et al. 2006 -RRB-	amod_Xiong_2006 dep_Xiong_al. nn_Xiong_et dep_in_Xiong prep_described_in conj_and_translation_reordering nn_translation_phrase dep_Wu_1997 dep_grammar_Wu appos_grammar_BTG nn_grammar_transduction nn_grammar_bracketing amod_grammar_binary det_grammar_a prep_to_adapts_reordering prep_to_adapts_translation dobj_adapts_grammar nsubj_adapts_which rcmod_system_adapts amod_system_phrase-based amod_system_state-of-the-art det_system_a nn_model_SDB det_model_the conj_implement_described prep_in_implement_system dobj_implement_model nsubj_implement_we prep_in_implement_paper det_paper_this
P09-1053	J97-3002	o	Most related to our approach Wu -LRB- 2005 -RRB- used inversion transduction grammarsa synchronous context-free formalism -LRB- Wu 1997 -RRB- for this task	det_task_this dep_Wu_1997 prep_for_formalism_task appos_formalism_Wu amod_formalism_context-free amod_formalism_synchronous nn_formalism_grammarsa nn_formalism_transduction nn_formalism_inversion amod_formalism_used nn_formalism_Wu appos_Wu_2005 conj_approach_formalism poss_approach_our prep_to_related_approach amod_Most_related dep_``_Most
P09-1065	J97-3002	o	-LRB- 2006 -RRB- develop a bottom-up decoder for BTG -LRB- Wu 1997 -RRB- that uses only phrase pairs	nn_pairs_phrase advmod_pairs_only dobj_uses_pairs nsubj_uses_that amod_Wu_1997 dep_BTG_Wu rcmod_decoder_uses prep_for_decoder_BTG amod_decoder_bottom-up det_decoder_a dobj_develop_decoder nsubj_develop_2006
P09-1088	J97-3002	o	Moreover the inference procedure for each sentence pair is non-trivial proving NP-complete for learning phrase based models -LRB- DeNero and Klein 2008 -RRB- or a high order polynomial -LRB- O -LRB- | f | 3 | e | 3 -RRB- -RRB- 1 for a sub-class of weighted synchronous context free grammars -LRB- Wu 1997 -RRB-	amod_Wu_1997 appos_grammars_Wu amod_grammars_free nn_grammars_context amod_grammars_synchronous amod_grammars_weighted prep_of_sub-class_grammars det_sub-class_a dep_3_| dep_3_e dep_3_| dep_3_| num_|_3 nn_|_f nn_|_| prep_for_O_sub-class num_O_1 dep_O_3 dep_polynomial_O nn_polynomial_order amod_polynomial_high det_polynomial_a conj_and_DeNero_2008 conj_and_DeNero_Klein conj_or_models_polynomial dep_models_2008 dep_models_Klein dep_models_DeNero amod_models_based nn_models_phrase amod_models_learning prep_for_NP-complete_polynomial prep_for_NP-complete_models dobj_proving_NP-complete dep_non-trivial_proving cop_non-trivial_is nsubj_non-trivial_procedure advmod_non-trivial_Moreover nn_pair_sentence det_pair_each prep_for_procedure_pair nn_procedure_inference det_procedure_the
P09-1088	J97-3002	o	Following the broad shift in the field from finite state transducers to grammar transducers -LRB- Chiang 2007 -RRB- recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference -LRB- Wu 1997 783 Cherry and Lin 2007 Zhang et al. 2008b Blunsom et al. 2008 -RRB-	nn_al._et nn_al._Blunsom dep_Zhang_al. nn_Zhang_et amod_Cherry_2007 conj_and_Cherry_Lin num_Cherry_783 amod_Wu_2008 dep_Wu_al. appos_Wu_2008b dep_Wu_Zhang dep_Wu_Lin dep_Wu_Cherry dep_Wu_1997 appos_inference_Wu nn_inference_time amod_inference_polynomial dobj_permitting_inference vmod_formalisms_permitting nn_formalisms_grammar amod_formalisms_synchronous dobj_used_formalisms aux_used_have dep_used_Following amod_alignment_phrase-based prep_to_approaches_alignment amod_approaches_recent dep_Chiang_2007 appos_transducers_approaches appos_transducers_Chiang nn_transducers_grammar nn_transducers_state amod_transducers_finite det_field_the prep_to_shift_transducers prep_from_shift_transducers prep_in_shift_field amod_shift_broad det_shift_the dobj_Following_shift
P09-1104	J97-3002	p	This source of overcounting is considered and fixed by Wu -LRB- 1997 -RRB- and Zens and Ney -LRB- 2003 -RRB- which we briefly review here	advmod_review_here advmod_review_briefly nsubj_review_we dobj_review_which appos_Ney_2003 conj_and_Wu_Ney conj_and_Wu_Zens appos_Wu_1997 prep_by_fixed_Ney prep_by_fixed_Zens prep_by_fixed_Wu nsubjpass_fixed_source dep_considered_review conj_and_considered_fixed auxpass_considered_is nsubjpass_considered_source prep_of_source_overcounting det_source_This
P09-1104	J97-3002	o	Null productions are also a source of double counting as there are many possible orders in 926 N I 2 + N IN N I -RCB- N IN I I I N N N -LRB- a -RRB- Normal Domain Rules -RCB- I squigglerightN 2 + I squigglerightNI I squigglerightNI I squigglerightN N N N I I I -LRB- b -RRB- Inverted Domain Rules N 11 fN 11 N 11 N 10 N 10 N 10 e N 10 N 00 -RCB- N 11 f N 10 -RCB- N 10 N 00 e -RCB- N 00 I 11 N NI 11 N NI 00 N 00 I + 11 I 00 N 00 N 10 N 10 N 11 N N I 11 I 11 I 00 N 00 N 11 -LRB- c -RRB- Normal Domain with Null Rules -RCB- -RCB- -RCB- I 11 squiggleright fI 11 I 11 squigglerightI 10 I 11 squiggleright f I 10 I 10 squiggleright I 10 e I 10 squigglerightI 00 I 10 squiggleright I 00 e I 00 squigglerightN + 11 N 00 I I N 00 N 11 N 11 I 00 squigglerightN 11 I I squigglerightN 11 I I squigglerightN 00 I 00 I 00 I 10 I 10 I 11 I 11 -LRB- d -RRB- Inverted Domain with Null Rules Figure 2 Illustration of two unambiguous forms of ITG grammars In -LRB- a -RRB- and -LRB- b -RRB- we illustrate the normal grammar without nulls -LRB- presented in Wu -LRB- 1997 -RRB- and Zens and Ney -LRB- 2003 -RRB- -RRB-	appos_Ney_2003 conj_and_Wu_Ney conj_and_Wu_Zens appos_Wu_1997 prep_in_presented_Ney prep_in_presented_Zens prep_in_presented_Wu dep_nulls_presented amod_grammar_normal det_grammar_the prep_without_illustrate_nulls dobj_illustrate_grammar nsubj_illustrate_we prep_illustrate_b prep_illustrate_In conj_and_In_b dep_In_a nn_grammars_ITG prep_of_forms_grammars amod_forms_unambiguous num_forms_two dep_Illustration_illustrate prep_of_Illustration_forms num_Figure_2 nn_Figure_Rules nn_Figure_Null prep_with_Domain_Figure amod_Domain_Inverted nn_Domain_d num_Domain_11 dep_Domain_I num_Domain_11 dep_Domain_I num_Domain_10 dep_Domain_I num_Domain_10 dep_Domain_I num_Domain_00 dep_Domain_I num_Domain_00 dep_Domain_I num_Domain_00 dep_squigglerightN_Domain nsubj_squigglerightN_I dep_squigglerightN_I rcmod_11_squigglerightN dobj_squigglerightN_11 nsubj_squigglerightN_I dep_squigglerightN_I dep_11_squigglerightN num_squigglerightN_11 num_squigglerightN_00 dep_I_squigglerightN dep_11_I num_N_11 dep_N_N num_N_11 dep_00_N dep_N_Illustration amod_N_00 dep_I_N dep_I_I dep_N_I num_N_00 tmod_11_N conj_+_squigglerightN_11 num_squigglerightN_00 dep_squigglerightN_I dep_squigglerightN_e num_e_00 dep_e_I dep_e_squiggleright dep_e_10 dep_e_I dep_e_00 dep_e_squigglerightI dep_e_I num_squigglerightI_10 dep_e_10 nsubj_e_I dep_e_squiggleright dep_e_10 dep_e_I dep_e_10 dep_e_I dep_e_f appos_squiggleright_11 appos_squiggleright_squigglerightN advmod_squiggleright_e num_squiggleright_11 dep_squiggleright_I num_squiggleright_10 dep_squiggleright_squigglerightI nn_squiggleright_fI num_squigglerightI_11 dep_squigglerightI_I dep_squigglerightI_11 num_squiggleright_11 dep_squiggleright_I nn_squiggleright_squigglerightNI dep_squiggleright_I amod_Rules_Null amod_Domain_Normal nn_Domain_N nn_Domain_N num_Domain_00 dep_Domain_I num_Domain_11 dep_Domain_I num_Domain_11 dep_Domain_I nn_Domain_N nn_Domain_N num_Domain_11 nn_Domain_N num_Domain_10 nn_Domain_N num_Domain_10 nn_Domain_N nn_Domain_N num_Domain_00 dep_Domain_I num_Domain_11 appos_N_c num_N_11 num_N_00 num_N_00 conj_+_I_Domain npadvmod_N_Domain npadvmod_N_I num_N_00 num_N_00 prep_with_NI_Rules dobj_NI_N nn_NI_N num_NI_11 nn_NI_NI nn_NI_N num_NI_11 dep_NI_I nn_NI_N dep_NI_N dep_NI_N dep_NI_e num_N_00 amod_N_e num_N_00 num_N_10 nn_N_N dep_N_10 nn_N_f appos_N_N num_N_11 dep_N_N dep_N_N num_N_00 num_N_10 npadvmod_e_N num_N_10 num_N_10 nn_N_N num_N_10 nn_N_N num_N_11 nn_N_N num_N_11 nn_N_fN dep_N_NI num_N_11 dep_Rules_N nn_Rules_Domain amod_Rules_Inverted nn_Rules_b dep_Rules_I dep_Rules_I dep_Rules_I nn_Rules_N nn_Rules_N nn_Rules_N nn_Rules_squigglerightN dep_Rules_I dobj_squigglerightNI_Rules nsubj_squigglerightNI_I rcmod_squigglerightNI_squigglerightNI conj_+_2_squiggleright dep_squigglerightN_squiggleright dobj_squigglerightN_squiggleright dobj_squigglerightN_2 nsubj_squigglerightN_I dep_squigglerightN_N dep_squigglerightN_IN nsubj_squigglerightN_N nsubj_squigglerightN_2 dep_squigglerightN_I dep_squigglerightN_N dep_squigglerightN_926 dep_squigglerightN_in nsubj_squigglerightN_orders nn_Rules_Domain amod_Rules_Normal det_Rules_a appos_N_Rules nn_N_N nn_N_N dep_N_I dep_N_I dep_N_I num_N_I nn_N_N prep_in_2_N conj_+_2_N amod_orders_possible amod_orders_many ccomp_are_squigglerightN expl_are_there mark_are_as amod_counting_double advcl_source_are prep_of_source_counting det_source_a advmod_source_also cop_source_are nsubj_source_productions amod_productions_Null
P09-1104	J97-3002	o	Because of this Wu -LRB- 1997 -RRB- and Zens and Ney -LRB- 2003 -RRB- introduced a normal form ITG which avoids this over-counting	det_over-counting_this dobj_avoids_over-counting nsubj_avoids_which rcmod_ITG_avoids dep_form_ITG amod_form_normal det_form_a dobj_introduced_form prep_because_of_introduced_Ney prep_because_of_introduced_Zens prep_because_of_introduced_Wu prep_because_of_introduced_this appos_Ney_2003 appos_Wu_1997 conj_and_this_Ney conj_and_this_Zens conj_and_this_Wu
P09-1104	J97-3002	o	2.2 Inversion Transduction Grammar Wu -LRB- 1997 -RRB- s inversion transduction grammar -LRB- ITG -RRB- is a synchronous grammar formalism in which derivations of sentence pairs correspond to alignments	prep_to_correspond_alignments nsubj_correspond_derivations prep_in_correspond_which nn_pairs_sentence prep_of_derivations_pairs rcmod_formalism_correspond nn_formalism_grammar amod_formalism_synchronous det_formalism_a cop_formalism_is csubj_formalism_s appos_grammar_ITG nn_grammar_transduction nn_grammar_inversion dobj_s_grammar nsubj_s_Wu appos_Wu_1997 nn_Wu_Grammar nn_Wu_Transduction nn_Wu_Inversion num_Wu_2.2
P09-1104	J97-3002	o	The set of such ITG alignments AITG are a strict subset of A1-1 -LRB- Wu 1997 -RRB-	amod_Wu_1997 appos_A1-1_Wu prep_of_subset_A1-1 amod_subset_strict det_subset_a cop_subset_are nsubj_subset_set appos_alignments_AITG nn_alignments_ITG amod_alignments_such prep_of_set_alignments det_set_The
P09-1104	J97-3002	o	1 Introduction Inversion transduction grammar -LRB- ITG -RRB- constraints -LRB- Wu 1997 -RRB- provide coherent structural constraints on the relationship between a sentence and its translation	poss_translation_its conj_and_sentence_translation det_sentence_a prep_between_relationship_translation prep_between_relationship_sentence det_relationship_the amod_constraints_structural amod_constraints_coherent prep_on_provide_relationship dobj_provide_constraints nsubj_provide_constraints amod_Wu_1997 dep_constraints_Wu nn_constraints_grammar appos_grammar_ITG nn_grammar_transduction nn_grammar_Inversion nn_grammar_Introduction num_grammar_1
P09-2032	J97-3002	o	1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation -LRB- SMT -RRB- -LRB- Wu 1997 Eisner 2003 Galley et al. 2006 Chiang 2007 Zhang et al. 2008 -RRB-	num_Zhang_2008 nn_Zhang_al. nn_Zhang_et dep_Chiang_Zhang num_Chiang_2007 nn_al._et nn_al._Galley num_Eisner_2003 dep_Wu_Chiang appos_Wu_2006 dep_Wu_al. dep_Wu_Eisner appos_Wu_1997 dep_translation_Wu appos_translation_SMT nn_translation_machine amod_translation_statistical prep_for_trend_translation det_trend_a cop_trend_been aux_trend_has nsubj_trend_formalisms amod_formalisms_based amod_grammar_synchronous amod_grammar_various rcmod_use_trend prep_of_use_grammar det_use_The dep_Introduction_use num_Introduction_1
P09-2036	J97-3002	o	There are rules though rare that can not be binarized synchronously at all -LRB- Wu 1997 -RRB- but can be incorporated in two-stage decoding with asynchronous binarization	amod_binarization_asynchronous prep_with_decoding_binarization vmod_two-stage_decoding prep_in_incorporated_two-stage auxpass_incorporated_be aux_incorporated_can dep_Wu_1997 appos_all_Wu prep_at_binarized_all advmod_binarized_synchronously auxpass_binarized_be neg_binarized_not aux_binarized_can nsubjpass_binarized_that conj_but_rare_incorporated ccomp_rare_binarized mark_rare_though advcl_are_incorporated advcl_are_rare nsubj_are_rules expl_are_There ccomp_``_are
P98-1006	J97-3002	n	The work reported in Wu -LRB- 1997 -RRB- which uses an inside-outside type of training algorithm to learn statistical contextfree transduction has a similar motivation to the current work but the models we describe here being fully lexical are more suitable for direct statistical modelling	amod_modelling_statistical amod_modelling_direct prep_for_suitable_modelling advmod_suitable_more cop_suitable_are dep_suitable_lexical advmod_lexical_fully cop_lexical_being advmod_describe_here nsubj_describe_we amod_models_suitable rcmod_models_describe det_models_the amod_work_current det_work_the prep_to_motivation_work amod_motivation_similar det_motivation_a conj_but_has_models dobj_has_motivation nsubj_has_work nn_transduction_contextfree amod_transduction_statistical dobj_learn_transduction aux_learn_to nn_algorithm_training prep_of_type_algorithm amod_type_inside-outside det_type_an vmod_uses_learn dobj_uses_type nsubj_uses_which rcmod_Wu_uses appos_Wu_1997 prep_in_reported_Wu vmod_work_reported det_work_The
P98-1074	J97-3002	o	Unfortunately this is not always the case and the above methodology suffers from the weaknesses pointed out by -LRB- Wu 1997 -RRB- concerning parse-parse-match procedures	amod_procedures_parse-parse-match prep_concerning_Wu_procedures num_Wu_1997 agent_pointed_Wu prt_pointed_out vmod_weaknesses_pointed det_weaknesses_the prep_from_suffers_weaknesses nsubj_suffers_methodology amod_methodology_above det_methodology_the conj_and_case_suffers det_case_the advmod_case_always neg_case_not cop_case_is nsubj_case_this advmod_case_Unfortunately
P98-1076	J97-3002	o	Consequently the mainstream research in the literature has been focused on the modeling and utilization of local and sentential contexts either linguistically in a rule-based framework or statistically in a searching and optimization set-up -LRB- Gan Palmer and Lua 1996 Sproat Shih Gale and Chang 1996 Wu 1997 Gut 1997 -RRB-	num_Gut_1997 num_Wu_1997 num_Chang_1996 conj_and_Sproat_Chang conj_and_Sproat_Gale conj_and_Sproat_Shih num_Lua_1996 dep_Gan_Gut conj_and_Gan_Wu conj_and_Gan_Chang conj_and_Gan_Gale conj_and_Gan_Shih conj_and_Gan_Sproat conj_and_Gan_Lua conj_and_Gan_Palmer appos_set-up_Wu appos_set-up_Sproat appos_set-up_Lua appos_set-up_Palmer appos_set-up_Gan nn_set-up_optimization conj_and_searching_set-up amod_a_set-up amod_a_searching pobj_in_a advmod_in_statistically amod_framework_rule-based det_framework_a conj_or_linguistically_in prep_in_linguistically_framework dep_either_in dep_either_linguistically amod_contexts_sentential amod_contexts_local conj_and_local_sentential prep_of_modeling_contexts conj_and_modeling_utilization det_modeling_the dep_focused_either prep_on_focused_utilization prep_on_focused_modeling auxpass_focused_been aux_focused_has nsubjpass_focused_research advmod_focused_Consequently det_literature_the prep_in_research_literature nn_research_mainstream det_research_the
P98-2230	J97-3002	o	The model employs a stochastic version of an inversion transduction grammar or ITG -LRB- Wu 1995c Wu 1995d Wu 1997 -RRB-	amod_Wu_1997 dep_Wu_Wu num_Wu_1995d dep_Wu_Wu appos_Wu_1995c dep_ITG_Wu conj_or_grammar_ITG nn_grammar_transduction nn_grammar_inversion det_grammar_an prep_of_version_ITG prep_of_version_grammar amod_version_stochastic det_version_a dobj_employs_version nsubj_employs_model det_model_The
P98-2230	J97-3002	o	1409 cally and experimentally -LRB- Wu 1995b Wu 1997 -RRB-	amod_Wu_1997 dep_Wu_Wu appos_Wu_1995b dep_cally_Wu conj_and_cally_experimentally dep_1409_experimentally dep_1409_cally ccomp_``_1409
P98-2230	J97-3002	o	If the target CFG is purely binary branching then the previous theoretical and linguistic analyses -LRB- Wu 1997 -RRB- suggest that much of the requisite constituent and word order transposition may be accommodated without change to the mirrored ITG	amod_ITG_mirrored det_ITG_the prep_to_change_ITG prep_without_accommodated_change auxpass_accommodated_be aux_accommodated_may nsubjpass_accommodated_much mark_accommodated_that nn_transposition_order nn_transposition_word conj_and_constituent_transposition amod_constituent_requisite det_constituent_the prep_of_much_transposition prep_of_much_constituent ccomp_suggest_accommodated nsubj_suggest_analyses amod_Wu_1997 dep_analyses_Wu amod_analyses_linguistic amod_analyses_theoretical det_analyses_the advmod_analyses_then advcl_analyses_binary conj_and_theoretical_linguistic amod_theoretical_previous xcomp_binary_branching advmod_binary_purely cop_binary_is nsubj_binary_CFG mark_binary_If nn_CFG_target det_CFG_the
W00-0508	J97-3002	o	There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure -LRB- Alshawi et al. 1998b Wu 1997 -RRB-	dep_Wu_1997 dep_Alshawi_Wu appos_Alshawi_1998b dep_Alshawi_al. nn_Alshawi_et nn_structure_language dobj_target_structure aux_target_to nn_structure_language nn_structure_source vmod_transduction_target prep_of_transduction_structure prep_through_achieved_transduction auxpass_achieved_is nsubjpass_achieved_translation advmod_achieved_where rcmod_translation_achieved nn_translation_machine amod_translation_statistical prep_to_approaches_translation amod_approaches_other dep_are_Alshawi nsubj_are_approaches expl_are_There ccomp_``_are
W02-1039	J97-3002	p	Several studies have reported alignment or translation performance for syntactically augmented translation models -LRB- Wu 1997 Wang 1998 Alshawi et al. 2000 Yamada and Knight 2001 Jones and Havrilla 1998 -RRB- and these results have been promising	cop_promising_been aux_promising_have nsubj_promising_results det_results_these amod_Jones_1998 conj_and_Jones_Havrilla nn_al._et nn_al._Alshawi num_Wang_1998 dep_Wu_Havrilla dep_Wu_Jones amod_Wu_2001 conj_and_Wu_Knight conj_and_Wu_Yamada num_Wu_2000 dep_Wu_al. dep_Wu_Wang appos_Wu_1997 appos_models_Knight appos_models_Yamada appos_models_Wu nn_models_translation amod_models_augmented advmod_augmented_syntactically nn_performance_translation conj_and_alignment_promising prep_for_alignment_models conj_or_alignment_performance dobj_reported_promising dobj_reported_performance dobj_reported_alignment aux_reported_have nsubj_reported_studies amod_studies_Several
W03-0303	J97-3002	o	2 Bilingual Bracketing In -LSB- Wu 1997 -RSB- the Bilingual Bracketing PCFG was introduced which can be simplified as the following production rules A -LSB- AA -RSB- -LRB- 1 -RRB- A < AA > -LRB- 2 -RRB- A f = e -LRB- 3 -RRB- A f = null -LRB- 4 -RRB- A null = e -LRB- 5 -RRB- Where f and e are words in the target vocabulary Vf and source vocabulary Ve respectively	nn_Ve_vocabulary nn_Ve_source conj_and_Vf_Ve nn_Vf_vocabulary nn_Vf_target det_Vf_the advmod_words_respectively prep_in_words_Ve prep_in_words_Vf cop_words_are nsubj_words_e nsubj_words_f dep_words_e conj_and_f_e advmod_f_Where dep_f_5 dep_=_words amod_null_= nn_A_null appos_null_4 dep_=_A dep_A_3 dep_3_e dobj_=_null prep_=_= dep_=_f dep_=_A advmod_=_f nsubj_=_AA dep_A_2 dep_AA_A dep_AA_> amod_AA_< det_AA_A dep_A_1 det_A_A appos_A_AA dep_rules_= nn_rules_production amod_rules_following det_rules_the prep_as_simplified_rules auxpass_simplified_be aux_simplified_can nsubjpass_simplified_which dep_introduced_simplified auxpass_introduced_was nsubjpass_introduced_PCFG nn_PCFG_Bracketing nn_PCFG_Bilingual det_PCFG_the num_Wu_1997 rcmod_Bracketing_introduced prep_in_Bracketing_Wu nn_Bracketing_Bilingual num_Bracketing_2 dep_``_Bracketing
W03-0303	J97-3002	o	However instead of estimating the probabilities for the production rules via EM as described in -LSB- Wu 1997 -RSB- we assign the probabilities to the rules using the Model-1 statistical translation lexicon -LSB- Brown et al. 1993 -RSB-	num_al._1993 nn_al._et amod_al._Brown nn_lexicon_translation amod_lexicon_statistical nn_lexicon_Model-1 det_lexicon_the dep_using_al. dobj_using_lexicon det_rules_the det_probabilities_the dep_assign_using prep_to_assign_rules dobj_assign_probabilities nsubj_assign_we rcmod_Wu_assign num_Wu_1997 prep_in_described_Wu mark_described_as nn_rules_production det_rules_the prep_for_probabilities_rules det_probabilities_the advcl_estimating_described prep_via_estimating_EM dobj_estimating_probabilities pcomp_of_estimating advmod_of_instead ccomp_,_of dep_``_However
W03-0303	J97-3002	o	Bilingual Bracketing -LSB- Wu 1997 -RSB- is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment	nn_alignment_word amod_alignment_Chinese-English prep_for_studied_alignment vmod_approaches_studied amod_approaches_parsing amod_approaches_shallow amod_approaches_bilingual det_approaches_the prep_of_one_approaches cop_one_is nsubj_one_Bracketing num_Wu_1997 appos_Bracketing_Wu amod_Bracketing_Bilingual
W03-0303	J97-3002	p	More suitable ways could be bilingual chunk parsing and refining the bracketing grammar as described in -LSB- Wu 1997 -RSB-	num_Wu_1997 prep_in_described_Wu mark_described_as amod_grammar_bracketing det_grammar_the advcl_refining_described dobj_refining_grammar nsubj_refining_ways conj_and_parsing_refining nn_parsing_chunk amod_parsing_bilingual cop_parsing_be aux_parsing_could nsubj_parsing_ways amod_ways_suitable amod_ways_More
W03-0304	J97-3002	o	This contrasts with alternative alignment models such as those of Melamed -LRB- 1998 -RRB- and Wu -LRB- 1997 -RRB- which impose a one-to-one constraint on alignments	prep_on_constraint_alignments amod_constraint_one-to-one det_constraint_a dobj_impose_constraint nsubj_impose_which appos_Wu_1997 conj_and_Melamed_Wu appos_Melamed_1998 rcmod_those_impose prep_of_those_Wu prep_of_those_Melamed prep_such_as_models_those nn_models_alignment amod_models_alternative prep_with_contrasts_models nsubj_contrasts_This ccomp_``_contrasts
W03-0313	J97-3002	o	This contrasts with alternative alignment models such as those of Melamed -LRB- 1998 -RRB- and Wu -LRB- 1997 -RRB- which impose a one-to-one constraint on alignments	prep_on_constraint_alignments amod_constraint_one-to-one det_constraint_a dobj_impose_constraint nsubj_impose_which appos_Wu_1997 conj_and_Melamed_Wu appos_Melamed_1998 rcmod_those_impose prep_of_those_Wu prep_of_those_Melamed prep_such_as_models_those nn_models_alignment amod_models_alternative prep_with_contrasts_models nsubj_contrasts_This ccomp_``_contrasts
W03-1002	J97-3002	o	Wu -LRB- 1997 -RRB- and Jones and Havrilla -LRB- 1998 -RRB- have sought to more closely tie the allowed motion of constituents between languages to those syntactic transductions supported by the independent rotation of parse tree constituents	nn_constituents_tree nn_constituents_parse prep_of_rotation_constituents amod_rotation_independent det_rotation_the agent_supported_rotation vmod_transductions_supported amod_transductions_syntactic det_transductions_those prep_between_constituents_languages prep_of_motion_constituents amod_motion_allowed det_motion_the prep_to_tie_transductions dobj_tie_motion advmod_tie_closely aux_tie_to advmod_closely_more xcomp_sought_tie aux_sought_have nsubj_sought_Havrilla nsubj_sought_Jones nsubj_sought_Wu appos_Havrilla_1998 conj_and_Wu_Havrilla conj_and_Wu_Jones appos_Wu_1997
W03-1807	J97-3002	o	Related Works Generally speaking approaches to MWE extraction proposed so far can be divided into three categories a -RRB- statistical approaches based on frequency and co-occurrence affinity b -RRB- knowledgebased or symbolic approaches using parsers lexicons and language filters and c -RRB- hybrid approaches combining different methods -LRB- Smadja 1993 Dagan and Church 1994 Daille 1995 McEnery et al. 1997 Wu 1997 Wermter et al. 1997 Michiels and Dufour 1998 Merkel and Andersson 2000 Piao and McEnery 2001 Sag et al. 2001a 2001b Biber et al. 2003 -RRB-	dep_al._2003 nn_al._et nn_al._Biber advmod_2001a_al. nn_2001a_Sag nn_al._et num_McEnery_2001 conj_and_Piao_McEnery num_Merkel_2000 conj_and_Merkel_Andersson num_Dufour_1998 conj_and_Michiels_Dufour dep_al._1997 nn_al._et nn_al._Wermter num_Wu_1997 dep_al._1997 nn_al._et nn_al._McEnery num_Daille_1995 num_Church_1994 conj_and_Dagan_Church dep_Smadja_al. appos_Smadja_2001b dep_Smadja_2001a dep_Smadja_McEnery dep_Smadja_Piao dep_Smadja_Andersson dep_Smadja_Merkel dep_Smadja_Dufour dep_Smadja_Michiels dep_Smadja_al. dep_Smadja_Wu dep_Smadja_al. dep_Smadja_Daille dep_Smadja_Church dep_Smadja_Dagan num_Smadja_1993 dep_methods_Smadja amod_methods_different dobj_combining_methods xcomp_approaches_combining nsubj_approaches_hybrid nsubj_approaches_approaches nn_hybrid_c nn_filters_language conj_and_parsers_filters conj_and_parsers_lexicons dobj_using_filters dobj_using_lexicons dobj_using_parsers vmod_approaches_using amod_approaches_symbolic amod_approaches_knowledgebased nn_approaches_affinity conj_or_knowledgebased_symbolic appos_affinity_b nn_affinity_co-occurrence nn_affinity_frequency conj_and_frequency_co-occurrence prep_on_based_approaches conj_and_approaches_hybrid vmod_approaches_based amod_approaches_statistical det_approaches_a dep_categories_approaches num_categories_three prep_into_divided_categories auxpass_divided_be aux_divided_can csubjpass_divided_speaking nsubjpass_divided_Works advmod_far_so advmod_proposed_far vmod_extraction_proposed nn_extraction_MWE prep_to_approaches_extraction conj_speaking_approaches advmod_speaking_Generally amod_Works_Related
W03-1807	J97-3002	o	For example Wu -LRB- 1997 -RRB- used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms including multiword expressions	amod_expressions_multiword prep_including_terms_expressions dobj_identify_terms aux_identify_to nn_grammars_transduction amod_grammars_stochastic xcomp_based_identify prep_on_based_grammars amod_parser_bilingual amod_parser_English-Chinese det_parser_an vmod_used_based dobj_used_parser nsubj_used_Wu prep_for_used_example appos_Wu_1997
W04-1513	J97-3002	o	5 Synchronous DIG 5.1 Definition -LRB- Wu 1997 -RRB- introduced synchronous binary trees and -LRB- Shieber 1990 -RRB- introduced synchronous tree adjoining grammars both of which view the translation process as a synchronous derivation process of parallel trees	amod_trees_parallel prep_of_process_trees nn_process_derivation amod_process_synchronous det_process_a nn_process_translation det_process_the prep_as_view_process dobj_view_process nsubj_view_both prep_of_both_which amod_grammars_adjoining nn_grammars_tree amod_grammars_introduced amod_tree_synchronous dep_introduced_Shieber dep_Shieber_1990 conj_and_trees_grammars amod_trees_binary amod_trees_synchronous parataxis_introduced_view dobj_introduced_grammars dobj_introduced_trees nsubj_introduced_Definition dep_Wu_1997 appos_Definition_Wu num_Definition_5.1 nn_Definition_DIG amod_Definition_Synchronous num_Definition_5 ccomp_``_introduced
W04-1513	J97-3002	o	Syntax based statistical MT approaches began with -LRB- Wu 1997 -RRB- who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees	amod_trees_binary amod_trees_synchronous prep_on_based_trees vmod_problem_based nn_problem_alignment det_problem_the prep_for_solution_problem amod_solution_polynomial-time det_solution_a dobj_introduced_solution nsubj_introduced_who rcmod_Wu_introduced num_Wu_1997 prep_with_began_Wu nsubj_began_approaches nn_approaches_MT amod_approaches_statistical amod_approaches_based nn_approaches_Syntax
W04-1513	J97-3002	o	At the same time grammar theoreticians have proposed various generative synchronous grammar formalisms for MT such as Synchronous Context Free Grammars -LRB- S-CFG -RRB- -LRB- Wu 1997 -RRB- or Synchronous Tree Adjoining Grammars -LRB- S-TAG -RRB- -LRB- Shieber and Schabes 1990 -RRB-	dep_Shieber_1990 conj_and_Shieber_Schabes appos_Grammars_S-TAG amod_Grammars_Adjoining nn_Grammars_Tree amod_Grammars_Synchronous dep_Wu_1997 dep_Grammars_Schabes dep_Grammars_Shieber conj_or_Grammars_Grammars dep_Grammars_Wu appos_Grammars_S-CFG nn_Grammars_Free nn_Grammars_Context amod_Grammars_Synchronous prep_such_as_MT_Grammars prep_such_as_MT_Grammars prep_for_formalisms_MT nn_formalisms_grammar amod_formalisms_synchronous amod_formalisms_generative amod_formalisms_various dobj_proposed_formalisms aux_proposed_have nsubj_proposed_theoreticians prep_at_proposed_time nn_theoreticians_grammar amod_time_same det_time_the rcmod_``_proposed
W05-0803	J97-3002	o	The only requirement will be that a parallel corpus exist for the language under consideration and one or more other languages .2 Induction of grammars from parallel corpora is rarely viewed as a promising task in its own right in work that has addressed the issue directly -LRB- Wu 1997 Melamed 2003 Melamed 2004 -RRB- the synchronous grammar is mainly viewed as instrumental in the process of improving the translation model in a noisy channel approach to statistical MT. 3 In the present paper we provide an important prerequisite for parallel corpus-based grammar induction work an efficient algorithm for synchronous parsing of sentence pairs given a word alignment	nn_alignment_word det_alignment_a pobj_given_alignment nn_pairs_sentence prep_of_parsing_pairs amod_parsing_synchronous prep_algorithm_given prep_for_algorithm_parsing amod_algorithm_efficient det_algorithm_an nn_work_induction nn_work_grammar amod_work_corpus-based amod_work_parallel dep_prerequisite_algorithm prep_for_prerequisite_work amod_prerequisite_important det_prerequisite_an dobj_provide_prerequisite nsubj_provide_we amod_paper_present det_paper_the num_MT._3 amod_MT._statistical prep_in_approach_paper prep_to_approach_MT. nn_approach_channel amod_approach_noisy det_approach_a nn_model_translation det_model_the prep_in_improving_approach dobj_improving_model prepc_of_process_improving det_process_the prep_in_instrumental_process prep_as_viewed_instrumental advmod_viewed_mainly auxpass_viewed_is nsubjpass_viewed_grammar amod_grammar_synchronous det_grammar_the amod_Melamed_2004 num_Melamed_2003 dep_Wu_Melamed conj_Wu_Melamed appos_Wu_1997 det_issue_the parataxis_addressed_provide ccomp_addressed_viewed dep_addressed_Wu advmod_addressed_directly dobj_addressed_issue aux_addressed_has nsubj_addressed_that prep_in_addressed_work amod_right_own poss_right_its prep_in_task_right amod_task_promising det_task_a parataxis_viewed_addressed prep_as_viewed_task advmod_viewed_rarely auxpass_viewed_is nsubjpass_viewed_be amod_corpora_parallel nn_Induction_.2 nn_Induction_languages amod_Induction_other num_Induction_more num_Induction_one conj_or_one_more prep_of_consideration_grammars conj_and_consideration_Induction prep_from_language_corpora prep_under_language_Induction prep_under_language_consideration det_language_the prep_for_exist_language nsubj_exist_corpus mark_exist_that amod_corpus_parallel det_corpus_a ccomp_be_exist aux_be_will nsubj_be_requirement amod_requirement_only det_requirement_The
W05-0803	J97-3002	o	Graphically speaking parsing amounts to identifying rectangular crosslinguistic constituents by assembling smaller rectangles that will together cover the full string spans in both dimensions -LRB- compare -LRB- Wu 1997 Melamed 2003 -RRB- -RRB-	dep_Melamed_2003 dep_Wu_Melamed appos_Wu_1997 dep_compare_Wu det_dimensions_both prep_in_spans_dimensions nn_spans_string amod_spans_full det_spans_the dobj_cover_spans advmod_cover_together aux_cover_will nsubj_cover_that rcmod_rectangles_cover amod_rectangles_smaller dobj_assembling_rectangles amod_constituents_crosslinguistic amod_constituents_rectangular dep_identifying_compare prepc_by_identifying_assembling dobj_identifying_constituents prepc_to_amounts_identifying amod_amounts_parsing dobj_speaking_amounts advmod_speaking_Graphically ccomp_``_speaking
W05-0803	J97-3002	o	-LRB- 2 -RRB- X1/X2 Y1 r1/Y2 r2 -LSB- i1 j1 i2 j2 -RSB- Y1/Y2 -LSB- j1 k1 j2 k2 -RSB- X1/X2 Y1 r1/Y2 r2 -LSB- i1 k1 i2 k2 -RSB- -LRB- 3 -RRB- X1/X2 Y1 r1/Y2 r2 -LSB- i1 j1 j2 k2 -RSB- Y1/Y2 -LSB- j1 k1 i2 j2 -RSB- X1/X2 Y1 r1/Y2 r2 -LSB- i1 k1 i2 k2 -RSB- Since each inference rule contains six free variables over string positions -LRB- i1 j1 k1 i2 j2 k2 -RRB- we get a parsing complexity of order O -LRB- n6 -RRB- for unlexicalized grammars -LRB- where n is the number of words in the longer of the two strings from language L1 and L2 -RRB- -LRB- Wu 1997 Melamed 2003 -RRB-	amod_Melamed_2003 dep_Wu_Melamed appos_Wu_1997 conj_and_L1_L2 nn_L1_language prep_from_strings_L2 prep_from_strings_L1 num_strings_two det_strings_the prep_of_longer_strings det_longer_the prep_in_number_longer prep_of_number_words det_number_the cop_number_is nsubj_number_n advmod_number_where appos_grammars_Wu appos_grammars_number amod_grammars_unlexicalized appos_O_n6 nn_O_order prep_for_complexity_grammars prep_of_complexity_O nn_complexity_parsing det_complexity_a dobj_get_complexity nsubj_get_we appos_i1_k2 conj_i1_j2 conj_i1_i2 conj_i1_k1 conj_i1_j1 dep_positions_i1 nn_positions_string amod_variables_free num_variables_six prep_over_contains_positions dobj_contains_variables nsubj_contains_rule mark_contains_Since nn_rule_inference det_rule_each appos_i1_k2 conj_i1_i2 conj_i1_k1 advcl_r2_contains appos_r2_i1 dep_r1/Y2_r2 dep_Y1_r1/Y2 nn_Y1_X1/X2 nn_Y1_r2 dep_j1_j2 conj_j1_i2 conj_j1_k1 appos_i1_k2 conj_i1_j2 conj_i1_j1 appos_r2_j1 appos_r2_Y1/Y2 appos_r2_i1 dep_r1/Y2_Y1 rcmod_Y1_get dep_Y1_r1/Y2 nn_Y1_X1/X2 dep_Y1_3 nn_Y1_r2 appos_i1_k2 conj_i1_i2 conj_i1_k1 appos_r2_i1 dep_r1/Y2_Y1 dep_Y1_r1/Y2 nn_Y1_X1/X2 nn_Y1_r2 dep_j1_k2 conj_j1_j2 conj_j1_k1 appos_i1_j2 conj_i1_i2 conj_i1_j1 appos_r2_j1 appos_r2_Y1/Y2 appos_r2_i1 dep_r1/Y2_Y1 dep_Y1_r1/Y2 nn_Y1_X1/X2 dep_Y1_2
W05-0815	J97-3002	o	This model shares some similarities with the stochastic inversion transduction grammars -LRB- SITG -RRB- presented by Wu in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu agent_presented_Wu vmod_grammars_presented appos_grammars_SITG nn_grammars_transduction nn_grammars_inversion amod_grammars_stochastic det_grammars_the prep_with_similarities_grammars det_similarities_some prep_shares_in dobj_shares_similarities nsubj_shares_model det_model_This
W05-0830	J97-3002	p	Whereas language generation has benefited from syntax -LSB- Wu 1997 Alshawi et al. 2000 -RSB- the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor -LSB- Koehn et al. 2003 -RSB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et cop_poor_be aux_poor_to dep_reported_Koehn xcomp_reported_poor auxpass_reported_been aux_reported_has nsubjpass_reported_performance advcl_reported_benefited amod_phrases_syntactic prep_on_relying_phrases advmod_relying_solely advmod_relying_when nn_translation_machine amod_translation_phrase-based amod_translation_statistical vmod_performance_relying prep_of_performance_translation det_performance_the nn_al._et nn_al._Alshawi amod_Wu_2000 dep_Wu_al. num_Wu_1997 appos_syntax_Wu prep_from_benefited_syntax aux_benefited_has nsubj_benefited_generation mark_benefited_Whereas nn_generation_language
W05-0831	J97-3002	o	4.5 ITG Constraints Another type of reordering can be obtained using Inversion Transduction Grammars -LRB- ITG -RRB- -LRB- Wu 1997 -RRB-	dep_Wu_1997 appos_Grammars_Wu appos_Grammars_ITG nn_Grammars_Transduction nn_Grammars_Inversion dobj_using_Grammars xcomp_obtained_using auxpass_obtained_be aux_obtained_can nsubjpass_obtained_type prep_of_type_reordering det_type_Another nn_type_Constraints nn_type_ITG num_type_4.5
W05-0835	J97-3002	o	This model shares some similarities with the stochastic inversion transduction grammars -LRB- SITG -RRB- presented by Wu in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu agent_presented_Wu vmod_grammars_presented appos_grammars_SITG nn_grammars_transduction nn_grammars_inversion amod_grammars_stochastic det_grammars_the prep_with_similarities_grammars det_similarities_some prep_shares_in dobj_shares_similarities nsubj_shares_model det_model_This
W05-1205	J97-3002	o	c2005 Association for Computational Linguistics Recognizing Paraphrases and Textual Entailment using Inversion Transduction Grammars Dekai Wu1 Human Language Technology Center HKUST Department of Computer Science University of Science and Technology Clear Water Bay Hong Kong dekai@cs.ust.hk Abstract We present first results using paraphrase as well as textual entailment data to test the language universal constraint posited by Wus -LRB- 1995 1997 -RRB- Inversion Transduction Grammar -LRB- ITG -RRB- hypothesis	nn_hypothesis_Grammar appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion nn_Grammar_Wus dep_1995_1997 dep_Wus_1995 agent_posited_hypothesis vmod_constraint_posited amod_constraint_universal nn_constraint_language det_constraint_the dobj_test_constraint aux_test_to nn_data_entailment amod_data_textual conj_and_paraphrase_data vmod_using_test dobj_using_data dobj_using_paraphrase vmod_results_using amod_results_first dobj_present_results nsubj_present_We rcmod_Abstract_present nn_Abstract_dekai@cs.ust.hk nn_Abstract_Kong nn_Abstract_Hong nn_Bay_Water nn_Bay_Clear conj_and_Science_Technology prep_of_University_Technology prep_of_University_Science nn_University_Science nn_University_Computer prep_of_Department_University nn_Department_HKUST nn_Department_Center nn_Department_Technology nn_Department_Language nn_Department_Human nn_Department_Wu1 nn_Department_Dekai nn_Department_Grammars nn_Department_Transduction nn_Department_Inversion dobj_using_Department nn_Entailment_Textual vmod_Paraphrases_using conj_and_Paraphrases_Entailment amod_Paraphrases_Recognizing appos_Linguistics_Abstract conj_Linguistics_Bay dep_Linguistics_Entailment dep_Linguistics_Paraphrases nn_Linguistics_Computational prep_for_Association_Linguistics nn_Association_c2005
W05-1205	J97-3002	o	Let W1 W2 be the vocabulary sizes of the two languages and N = -LCB- A1 AN -RCB- be the set of nonterminals with indices 1 N. Wu -LRB- 1997 -RRB- also showed that ITGs can be equivalently be defined in two other ways	amod_ways_other num_ways_two prep_in_defined_ways auxpass_defined_be advmod_defined_equivalently auxpass_defined_be aux_defined_can nsubjpass_defined_ITGs mark_defined_that ccomp_showed_defined advmod_showed_also nsubj_showed_Wu appos_Wu_1997 nn_Wu_N. num_indices_1 prep_with_set_indices prep_of_set_nonterminals det_set_the cop_set_be nsubj_set_= dep_set_N dep_A1_AN appos_=_A1 num_languages_two det_languages_the prep_of_sizes_languages nn_sizes_vocabulary det_sizes_the cop_sizes_be nsubj_sizes_W2 conj_and_W1_set conj_and_W1_sizes parataxis_Let_showed dobj_Let_set dobj_Let_sizes dobj_Let_W1
W05-1205	J97-3002	p	Moreover for reasons discussed by Wu -LRB- 1997 -RRB- ITGs possess an interesting intrinsic combinatorial property of permitting roughly up to four arguments of any frame to be transposed freely but not more	dep_not_more cc_not_but advmod_transposed_freely auxpass_transposed_be aux_transposed_to vmod_frame_transposed det_frame_any prep_of_arguments_frame num_arguments_four advmod_up_roughly prep_to_permitting_arguments advmod_permitting_up prepc_of_property_permitting amod_property_combinatorial amod_property_intrinsic amod_property_interesting det_property_an dep_possess_not dobj_possess_property prep_for_possess_reasons advmod_possess_Moreover appos_Wu_ITGs appos_Wu_1997 agent_discussed_Wu vmod_reasons_discussed
W05-1205	J97-3002	o	The result in Wu -LRB- 1997 -RRB- implies that for the special case of Bracketing ITGs the time complexity of the algorithm is parenleftbigT3V 3parenrightbig where T and V are the lengths of the two sentences	num_sentences_two det_sentences_the prep_of_lengths_sentences det_lengths_the cop_lengths_are nsubj_lengths_V nsubj_lengths_T advmod_lengths_where conj_and_T_V rcmod_3parenrightbig_lengths nn_3parenrightbig_parenleftbigT3V cop_3parenrightbig_is nsubj_3parenrightbig_complexity prep_for_3parenrightbig_case mark_3parenrightbig_that det_algorithm_the prep_of_complexity_algorithm nn_complexity_time det_complexity_the amod_ITGs_Bracketing prep_of_case_ITGs amod_case_special det_case_the ccomp_implies_3parenrightbig nsubj_implies_result appos_Wu_1997 prep_in_result_Wu det_result_The
W05-1205	J97-3002	o	1 Introduction The Inversion Transduction Grammar or ITG formalism which historically was developed in the context of translation and alignment hypothesizes strong expressiveness restrictions that constrain paraphrases to vary word order only in certain allowable nested permutations of arguments -LRB- Wu 1997 -RRB-	amod_Wu_1997 appos_permutations_Wu prep_of_permutations_arguments amod_permutations_nested amod_permutations_allowable amod_permutations_certain nn_order_word prep_in_vary_permutations advmod_vary_only dobj_vary_order aux_vary_to xcomp_constrain_vary dobj_constrain_paraphrases nsubj_constrain_that rcmod_restrictions_constrain nn_restrictions_expressiveness amod_restrictions_strong dobj_hypothesizes_restrictions nsubj_hypothesizes_formalism nsubj_hypothesizes_Grammar conj_and_translation_alignment prep_of_context_alignment prep_of_context_translation det_context_the prep_in_developed_context auxpass_developed_was advmod_developed_historically nsubjpass_developed_which nn_formalism_ITG rcmod_Grammar_developed conj_or_Grammar_formalism nn_Grammar_Transduction nn_Grammar_Inversion det_Grammar_The rcmod_Introduction_hypothesizes num_Introduction_1
W05-1507	J97-3002	o	This is the same complexity as the ITG alignment algorithm used by Wu -LRB- 1997 -RRB- and others meaning complete Viterbi decoding is possible without pruning for realistic-length sentences	amod_sentences_realistic-length prep_for_pruning_sentences prep_without_possible_pruning cop_possible_is nsubj_possible_complexity nn_decoding_Viterbi amod_decoding_complete dobj_meaning_decoding conj_and_Wu_others appos_Wu_1997 agent_used_others agent_used_Wu vmod_algorithm_used nn_algorithm_alignment nn_algorithm_ITG det_algorithm_the vmod_complexity_meaning prep_as_complexity_algorithm amod_complexity_same det_complexity_the cop_complexity_is nsubj_complexity_This ccomp_``_possible
W05-1507	J97-3002	o	2 Machine Translation using Inversion Transduction Grammar The Inversion Transduction Grammar -LRB- ITG -RRB- of Wu -LRB- 1997 -RRB- is a type of context-free grammar -LRB- CFG -RRB- for generating two languages synchronously	advmod_languages_synchronously num_languages_two dobj_generating_languages appos_grammar_CFG amod_grammar_context-free prepc_for_type_generating prep_of_type_grammar det_type_a cop_type_is nsubj_type_Translation appos_Wu_1997 prep_of_Grammar_Wu appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion nn_Grammar_The nn_Grammar_Grammar nn_Grammar_Transduction nn_Grammar_Inversion dobj_using_Grammar vmod_Translation_using nn_Translation_Machine num_Translation_2
W06-1504	J97-3002	o	A related example would be a version of synchronous CFG that allows only one pair of linked nonterminals and any number of unlinked nonterminals which could be bitextparsed in O -LRB- n5 -RRB- time whereas inversion transduction grammar -LRB- Wu 1997 -RRB- takes O -LRB- n6 -RRB-	appos_O_n6 dobj_takes_O nsubj_takes_grammar mark_takes_whereas dep_Wu_1997 appos_grammar_Wu nn_grammar_transduction nn_grammar_inversion nn_time_O appos_O_n5 prep_in_bitextparsed_time auxpass_bitextparsed_be aux_bitextparsed_could nsubjpass_bitextparsed_which rcmod_nonterminals_bitextparsed amod_nonterminals_unlinked prep_of_number_nonterminals det_number_any amod_nonterminals_linked conj_and_pair_number prep_of_pair_nonterminals num_pair_one quantmod_one_only dobj_allows_number dobj_allows_pair nsubj_allows_that rcmod_CFG_allows amod_CFG_synchronous advcl_version_takes prep_of_version_CFG det_version_a cop_version_be aux_version_would nsubj_version_example amod_example_related det_example_A
W06-1627	J97-3002	o	Alignment whether for training a translation model using EM or for nding the Viterbi alignment of test data is O -LRB- n6 -RRB- -LRB- Wu 1997 -RRB- while translation -LRB- decoding -RRB- is O -LRB- n7 -RRB- using a bigram language model and O -LRB- n11 -RRB- with trigrams	prep_with_O_trigrams appos_O_n11 nn_model_language nn_model_bigram det_model_a dobj_using_model conj_and_O_O vmod_O_using dep_O_n7 cop_O_is nsubj_O_translation mark_O_while appos_translation_decoding amod_Wu_1997 dep_O_Wu appos_O_n6 cop_O_is prepc_for_O_training mark_O_whether nn_data_test prep_of_alignment_data nn_alignment_Viterbi det_alignment_the dobj_nding_alignment pcomp_for_nding conj_or_using_for dobj_using_EM prep_model_for prep_model_using nn_model_translation det_model_a dobj_training_model dep_Alignment_O dep_Alignment_O conj_Alignment_O
W06-1627	J97-3002	o	1 Introduction The Inversion Transduction Grammar -LRB- ITG -RRB- of Wu -LRB- 1997 -RRB- is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages	num_languages_two amod_sentences_equivalent advmod_sentences_translationally prep_of_pairs_sentences prep_of_alignments_pairs amod_alignments_word-level prep_in_producing_languages dobj_producing_alignments prepc_for_algorithm_producing amod_algorithm_motivated det_algorithm_a cop_algorithm_is nsubj_algorithm_Grammar advmod_motivated_syntactically appos_Wu_1997 prep_of_Grammar_Wu appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion det_Grammar_The rcmod_Introduction_algorithm num_Introduction_1
W06-1628	J97-3002	o	Wu -LRB- 1997 -RRB- and Alshawi -LRB- 1996 -RRB- describe early work on formalisms that make use of transductive grammars Graehl and Knight -LRB- 2004 -RRB- describe methods for training tree transducers	nn_transducers_tree nn_transducers_training prep_for_methods_transducers dobj_describe_methods nsubj_describe_Knight nsubj_describe_Graehl appos_Knight_2004 conj_and_Graehl_Knight amod_grammars_transductive prep_of_use_grammars dobj_make_use nsubj_make_that rcmod_formalisms_make prep_on_work_formalisms amod_work_early parataxis_describe_describe dobj_describe_work nsubj_describe_Alshawi nsubj_describe_Wu appos_Alshawi_1996 conj_and_Wu_Alshawi appos_Wu_1997
W06-2403	J97-3002	o	2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing -LRB- NLP -RRB- community including Smadja 1993 Dagan and Church 1994 Daille 1995 1995 McEnery et al. 1997 Wu 1997 Michiels and Dufour 1998 Maynard and Ananiadou 2000 Merkel and Andersson 2000 Piao and McEnery 2001 Sag et al. 2001 Tanaka and Baldwin 2003 Dias 2003 Baldwin et al. 2003 Nivre and Nilsson 2004 Pereira et al	dep_Pereira_al nn_Pereira_et num_Pereira_2004 appos_Nivre_Pereira conj_and_Nivre_Nilsson num_Baldwin_2003 nn_Baldwin_al. nn_Baldwin_et num_Dias_2003 num_Baldwin_2003 num_Sag_2001 nn_Sag_al. nn_Sag_et dep_Piao_Nilsson dep_Piao_Nivre conj_and_Piao_Baldwin conj_and_Piao_Dias conj_and_Piao_Baldwin conj_and_Piao_Tanaka conj_and_Piao_Sag conj_and_Piao_2001 conj_and_Piao_McEnery appos_Andersson_2000 num_Wu_1997 num_McEnery_1997 nn_McEnery_al. nn_McEnery_et appos_Daille_1995 conj_and_Smadja_Andersson conj_and_Smadja_Merkel conj_and_Smadja_2000 conj_and_Smadja_Ananiadou conj_and_Smadja_Maynard conj_and_Smadja_1998 conj_and_Smadja_Dufour conj_and_Smadja_Michiels conj_and_Smadja_Wu conj_and_Smadja_McEnery conj_and_Smadja_1995 conj_and_Smadja_Daille conj_and_Smadja_1994 conj_and_Smadja_Church conj_and_Smadja_Dagan conj_and_Smadja_1993 prep_including_community_Andersson prep_including_community_Merkel prep_including_community_2000 prep_including_community_Ananiadou prep_including_community_Maynard prep_including_community_1998 prep_including_community_Dufour prep_including_community_Michiels prep_including_community_Wu prep_including_community_McEnery prep_including_community_1995 prep_including_community_Daille prep_including_community_1994 prep_including_community_Church prep_including_community_Dagan prep_including_community_1993 prep_including_community_Smadja nn_community_Processing appos_Processing_NLP nn_Processing_Language amod_Processing_Natural det_Processing_the amod_attention_much prep_from_attracted_community dobj_attracted_attention aux_attracted_has nsubj_attracted_issue nn_processing_MWE prep_of_issue_processing det_issue_The dep_Work_Baldwin dep_Work_Dias dep_Work_Baldwin dep_Work_Tanaka dep_Work_Sag dep_Work_2001 dep_Work_McEnery dep_Work_Piao rcmod_Work_attracted amod_Work_Related num_Work_2
W06-3104	J97-3002	o	It differs from the many approaches where -LRB- 1 -RRB- is defined by a stochastic synchronous grammar -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knight 2001 Eisner 2003 Gildea 2003 Melamed 2004 -RRB- and from transfer-based systems defined by context-free grammars -LRB- Lavie et al. 2003 -RRB-	amod_Lavie_2003 dep_Lavie_al. nn_Lavie_et dep_grammars_Lavie amod_grammars_context-free agent_defined_grammars vmod_systems_defined amod_systems_transfer-based pobj_from_systems dep_Melamed_2004 num_Gildea_2003 num_Eisner_2003 num_Yamada_2001 conj_and_Yamada_Knight nn_al._et nn_al._Alshawi dep_Wu_Melamed conj_Wu_Gildea conj_Wu_Eisner conj_Wu_Knight conj_Wu_Yamada appos_Wu_2000 dep_Wu_al. appos_Wu_1997 appos_grammar_Wu amod_grammar_synchronous amod_grammar_stochastic det_grammar_a conj_and_defined_from agent_defined_grammar auxpass_defined_is dep_defined_1 advmod_defined_where rcmod_approaches_from rcmod_approaches_defined amod_approaches_many det_approaches_the prep_from_differs_approaches nsubj_differs_It
W06-3108	J97-3002	o	The approach presented here has some resemblance to the bracketing transduction grammars -LRB- BTG -RRB- of -LRB- Wu 1997 -RRB- which have been applied to a phrase-based machine translation system in -LRB- Zens et al. 2004 -RRB-	amod_Zens_2004 dep_Zens_al. nn_Zens_et dep_in_Zens prep_system_in nn_system_translation nn_system_machine amod_system_phrase-based det_system_a prep_to_applied_system auxpass_applied_been aux_applied_have nsubjpass_applied_which rcmod_Wu_applied dep_Wu_1997 prep_of_grammars_Wu appos_grammars_BTG nn_grammars_transduction nn_grammars_bracketing det_grammars_the prep_to_resemblance_grammars det_resemblance_some dobj_has_resemblance nsubj_has_approach advmod_presented_here vmod_approach_presented det_approach_The ccomp_``_has
W06-3111	J97-3002	n	An alternative method -LRB- Wu 1997 -RRB- makes decisions at the end but has a high computational requirement	amod_requirement_computational amod_requirement_high det_requirement_a dobj_has_requirement nsubj_has_method det_end_the conj_but_makes_has prep_at_makes_end dobj_makes_decisions nsubj_makes_method amod_Wu_1997 dep_method_Wu nn_method_alternative det_method_An
W06-3117	J97-3002	o	In this work we focus on learning bilingual word phrases by using Stochastic Inversion Transduction Grammars -LRB- SITGs -RRB- -LRB- Wu 1997 -RRB-	dep_Wu_1997 appos_Grammars_Wu appos_Grammars_SITGs nn_Grammars_Transduction nn_Grammars_Inversion nn_Grammars_Stochastic dobj_using_Grammars nn_phrases_word amod_phrases_bilingual prepc_by_learning_using dobj_learning_phrases prepc_on_focus_learning nsubj_focus_we prep_in_focus_work det_work_this
W06-3117	J97-3002	o	3 Stochastic Inversion Transduction Grammars Stochastic Inversion Transduction Grammars -LRB- SITGs -RRB- -LRB- Wu 1997 -RRB- can be viewed as a restricted subset of Stochastic Syntax-Directed Transduction Grammars	nn_Grammars_Transduction nn_Grammars_Syntax-Directed nn_Grammars_Stochastic prep_of_subset_Grammars amod_subset_restricted det_subset_a prep_as_viewed_subset auxpass_viewed_be aux_viewed_can nsubjpass_viewed_Grammars dep_Wu_1997 appos_Grammars_Wu appos_Grammars_SITGs nn_Grammars_Transduction nn_Grammars_Inversion nn_Grammars_Stochastic nn_Grammars_Grammars nn_Grammars_Transduction nn_Grammars_Inversion nn_Grammars_Stochastic num_Grammars_3
W06-3117	J97-3002	p	An efficient Viterbi-like parsing algorithm that is based on a Dynamic Programing Scheme is proposed in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu prep_proposed_in auxpass_proposed_is nsubjpass_proposed_algorithm nn_Scheme_Programing nn_Scheme_Dynamic det_Scheme_a prep_on_based_Scheme auxpass_based_is nsubjpass_based_that rcmod_algorithm_based nn_algorithm_parsing amod_algorithm_Viterbi-like amod_algorithm_efficient det_algorithm_An
W06-3117	J97-3002	o	1A Normal Form for SITGs can be defined -LRB- Wu 1997 -RRB- by analogy to the Chomsky Normal Form for Stochastic ContextFree Grammars	nn_Grammars_ContextFree nn_Grammars_Stochastic prep_for_Form_Grammars amod_Form_Normal nn_Form_Chomsky det_Form_the prep_to_analogy_Form dep_Wu_1997 agent_defined_analogy dep_defined_Wu auxpass_defined_be aux_defined_can nsubjpass_defined_Form prep_for_Form_SITGs amod_Form_Normal nn_Form_1A
W06-3117	J97-3002	o	In this work we study a method for obtaining word phrases that is based on Stochastic Inversion Transduction Grammars that was proposed in -LRB- Wu 1997 -RRB-	amod_Wu_1997 dep_in_Wu prep_proposed_in auxpass_proposed_was nsubjpass_proposed_that rcmod_Grammars_proposed nn_Grammars_Transduction nn_Grammars_Inversion nn_Grammars_Stochastic prep_on_based_Grammars auxpass_based_is nsubjpass_based_that rcmod_phrases_based nn_phrases_word dobj_obtaining_phrases prepc_for_method_obtaining det_method_a dobj_study_method nsubj_study_we prep_in_study_work det_work_this
W06-3601	J97-3002	n	Besides our model as being linguistically motivated is also more expressive than the formally syntax-based models of Chiang -LRB- 2005 -RRB- and Wu -LRB- 1997 -RRB-	appos_Wu_1997 conj_and_Chiang_Wu appos_Chiang_2005 prep_of_models_Wu prep_of_models_Chiang amod_models_syntax-based det_models_the advmod_syntax-based_formally prep_than_expressive_models advmod_expressive_more advmod_expressive_also cop_expressive_is prepc_as_expressive_motivated nsubj_expressive_model dep_expressive_Besides advmod_motivated_linguistically auxpass_motivated_being poss_model_our
W06-3602	J97-3002	p	The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in -LRB- Wu 1997 -RRB- in both cases the number of alignments is drastically reduced by introducing appropriate re-ordering restrictions	amod_restrictions_re-ordering amod_restrictions_appropriate dobj_introducing_restrictions agent_reduced_introducing advmod_reduced_drastically auxpass_reduced_is nsubjpass_reduced_number prep_in_reduced_cases prep_of_number_alignments det_number_the det_cases_both dep_Wu_1997 prep_in_described_Wu vmod_parsing_described amod_parsing_bilingual prep_to_approach_parsing nn_approach_grammar nn_approach_transduction nn_approach_inversion det_approach_the parataxis_related_reduced prep_to_related_approach cop_related_is nsubj_related_algorithm num_Section_4 prep_in_algorithm_Section nn_algorithm_alignment nn_algorithm_block amod_algorithm_efficient det_algorithm_The
W07-0401	J97-3002	o	-LRB- Wu 1997 Yamada and Knight 2001 Gildea 2003 Melamed 2004 Graehl and Knight 2004 Galley et al. 2006 -RRB-	num_Galley_2006 nn_Galley_al. nn_Galley_et num_Graehl_2004 conj_and_Graehl_Knight num_Melamed_2004 num_Gildea_2003 dep_Yamada_Galley conj_and_Yamada_Knight conj_and_Yamada_Graehl conj_and_Yamada_Melamed conj_and_Yamada_Gildea num_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Graehl dep_Wu_Melamed dep_Wu_Gildea dep_Wu_Knight dep_Wu_Yamada appos_Wu_1997 dep_''_Wu
W07-0403	J97-3002	o	In the meantime synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents but continue to be employed primarily for word-to-word analysis -LRB- Wu 1997 -RRB-	dep_Wu_1997 appos_analysis_Wu amod_analysis_word-to-word prep_for_employed_analysis advmod_employed_primarily auxpass_employed_be aux_employed_to xcomp_continue_employed nsubj_continue_methods amod_constituents_bilingual poss_constituents_their dobj_building_constituents mark_building_while nn_phrases_bitext amod_phrases_same det_phrases_the conj_but_process_continue advcl_process_building dobj_process_phrases advmod_process_efficiently nsubj_process_methods prep_in_process_meantime nn_methods_parsing amod_methods_synchronous det_meantime_the
W07-0403	J97-3002	p	Inversion transduction grammar -LRB- Wu 1997 -RRB- or ITG is a wellstudied synchronous grammar formalism	nn_formalism_grammar amod_formalism_synchronous amod_formalism_wellstudied det_formalism_a cop_formalism_is nsubj_formalism_ITG nsubj_formalism_grammar dep_Wu_1997 conj_or_grammar_ITG appos_grammar_Wu nn_grammar_transduction nn_grammar_Inversion
W07-0403	J97-3002	o	This ITG constraint is characterized by the two forbidden structures shown in Figure 1 -LRB- Wu 1997 -RRB-	dep_Wu_1997 appos_Figure_Wu num_Figure_1 prep_in_shown_Figure vmod_structures_shown amod_structures_forbidden num_structures_two det_structures_the agent_characterized_structures auxpass_characterized_is nsubjpass_characterized_constraint nn_constraint_ITG det_constraint_This
W07-0403	J97-3002	o	Stochastic ITGs are parameterized like their PCFG counterparts -LRB- Wu 1997 -RRB- productions A X are assigned probability Pr -LRB- X | A -RRB-	num_A_| nn_A_X appos_Pr_A nn_Pr_probability dobj_assigned_Pr auxpass_assigned_are nsubjpass_assigned_X det_X_A nn_X_productions appos_Wu_1997 dep_counterparts_Wu nn_counterparts_PCFG poss_counterparts_their parataxis_parameterized_assigned prep_like_parameterized_counterparts auxpass_parameterized_are nsubjpass_parameterized_ITGs amod_ITGs_Stochastic
W07-0403	J97-3002	o	Wu -LRB- 1997 -RRB- used a binary bracketing ITG to segment a sen19 tence while simultaneously word-aligning it to its translation but the model was trained heuristically with a fixed segmentation	amod_segmentation_fixed det_segmentation_a prep_with_trained_segmentation advmod_trained_heuristically auxpass_trained_was nsubjpass_trained_model det_model_the poss_translation_its prep_to_word-aligning_translation dobj_word-aligning_it advmod_word-aligning_simultaneously mark_word-aligning_while advcl_tence_word-aligning nn_tence_sen19 det_tence_a dep_segment_tence amod_ITG_bracketing amod_ITG_binary det_ITG_a conj_but_used_trained prep_to_used_segment dobj_used_ITG nsubj_used_Wu appos_Wu_1997
W07-0403	J97-3002	o	The similarities become moreapparentwhenweconsiderthecanonical-form binary-bracketing ITG -LRB- Wu 1997 -RRB- shown here S A | B | C A -LSB- AB -RSB- | -LSB- BB -RSB- | -LSB- CB -RSB- | -LSB- AC -RSB- | -LSB- BC -RSB- | -LSB- CC -RSB- B AA | BA | CA | AC | BC | CC C e / f -LRB- 3 -RRB- -LRB- 3 -RRB- is employed in place of -LRB- 2 -RRB- to reduce redundant alignments and clean up EM expectations .1 More importantly for our purposes it introduces a preterminal C which generates all phrase pairs or cepts	conj_or_pairs_cepts nn_pairs_phrase det_pairs_all dobj_generates_cepts dobj_generates_pairs nsubj_generates_which rcmod_C_generates amod_C_preterminal det_C_a dobj_introduces_C nsubj_introduces_it advcl_introduces_clean advcl_introduces_reduce dep_introduces_2 prep_introduces_of dep_introduces_place mark_introduces_in poss_purposes_our dep_importantly_More prep_for_.1_purposes advmod_.1_importantly dep_expectations_.1 nn_expectations_EM dobj_clean_expectations prt_clean_up amod_alignments_redundant conj_and_reduce_clean dobj_reduce_alignments aux_reduce_to advcl_employed_introduces auxpass_employed_is dep_employed_3 dep_employed_3 dep_employed_f amod_C_e nn_C_CC num_C_| nn_C_BC num_C_| nn_C_AC nn_C_| nn_C_CA num_C_| nn_C_BA num_BA_| dep_AA_C nn_AA_B dep_AA_CC dep_A_employed dep_A_AA num_A_| appos_A_BC num_A_| appos_A_AC num_A_| appos_A_CB num_A_| appos_A_BB num_A_| appos_A_AB nn_A_C nn_A_| nn_A_B nn_A_| det_A_A nn_A_S advmod_shown_here dep_Wu_1997 vmod_ITG_shown appos_ITG_Wu amod_ITG_binary-bracketing amod_ITG_moreapparentwhenweconsiderthecanonical-form dep_become_A xcomp_become_ITG nsubj_become_similarities det_similarities_The
W07-0404	J97-3002	o	Wu -LRB- 1997 -RRB- demonstrates the case of binary SCFG parsing where six string boundary variables three for each language as in monolingual CFG parsing interact with each other yielding an O -LRB- N6 -RRB- dynamic programming algorithm where N is the string length assuming the two paired strings are comparable in length	prep_in_comparable_length cop_comparable_are nsubj_comparable_strings amod_strings_paired num_strings_two det_strings_the ccomp_assuming_comparable nn_length_string det_length_the cop_length_is nsubj_length_N advmod_length_where rcmod_algorithm_length nn_algorithm_programming amod_algorithm_dynamic nn_algorithm_O appos_O_N6 det_O_an vmod_yielding_assuming dobj_yielding_algorithm det_other_each prep_with_interact_other nsubj_interact_variables advmod_interact_where nn_parsing_CFG amod_parsing_monolingual pobj_in_parsing advmod_in_as det_language_each dep_three_in prep_for_three_language appos_variables_three nn_variables_boundary nn_variables_string num_variables_six rcmod_parsing_interact nn_parsing_SCFG amod_parsing_binary prep_of_case_parsing det_case_the dep_demonstrates_yielding dobj_demonstrates_case nsubj_demonstrates_Wu appos_Wu_1997
W07-0404	J97-3002	o	Wu -LRB- 1997 -RRB- s Inversion Transduction Grammar as well as tree-transformation models of translation such as Yamada and Knight -LRB- 2001 -RRB- Galley et al.	nn_al._et nn_al._Galley appos_Knight_2001 dep_Yamada_al. conj_and_Yamada_Knight prep_such_as_models_Knight prep_such_as_models_Yamada prep_of_models_translation nn_models_tree-transformation conj_and_Grammar_models nn_Grammar_Transduction nn_Grammar_Inversion dobj_s_models dobj_s_Grammar nsubj_s_Wu appos_Wu_1997
W07-0406	J97-3002	o	Machine translation based on a deeper analysis of the syntactic structure of a sentence has long been identified as a desirable objective in principle -LRB- consider -LRB- Wu 1997 Yamada and Knight 2001 -RRB- -RRB-	amod_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Knight dep_Wu_Yamada amod_Wu_1997 dep_consider_Wu prep_in_objective_principle amod_objective_desirable det_objective_a dep_identified_consider prep_as_identified_objective auxpass_identified_been advmod_identified_long aux_identified_has nsubjpass_identified_translation det_sentence_a prep_of_structure_sentence amod_structure_syntactic det_structure_the prep_of_analysis_structure amod_analysis_deeper det_analysis_a prep_on_based_analysis vmod_translation_based nn_translation_Machine ccomp_``_identified
W07-0408	J97-3002	p	Synchronous parsing models have been explored with moderate success -LRB- Wu 1997 Quirk et al. 2005 -RRB-	nn_al._et nn_al._Quirk amod_Wu_2005 dep_Wu_al. amod_Wu_1997 dep_success_Wu amod_success_moderate prep_with_explored_success auxpass_explored_been aux_explored_have nsubjpass_explored_models nn_models_parsing amod_models_Synchronous
W07-0410	J97-3002	o	Actually now that SMT has reached some maturity we see several attempts to integrate more structure into these systems ranging from simple hierarchical alignment models -LRB- Wu 1997 Chiang 2005 -RRB- to syntax-based statistical systems -LRB- Yamada and Knight 2001 Zollmann and Venugopal 2006 -RRB-	num_Venugopal_2006 num_Knight_2001 conj_and_Yamada_Venugopal conj_and_Yamada_Zollmann conj_and_Yamada_Knight dep_systems_Venugopal dep_systems_Zollmann dep_systems_Knight dep_systems_Yamada amod_systems_statistical amod_systems_syntax-based num_Chiang_2005 appos_Wu_Chiang num_Wu_1997 dep_models_Wu nn_models_alignment amod_models_hierarchical amod_models_simple prep_to_ranging_systems prep_from_ranging_models det_systems_these amod_structure_more prep_into_integrate_systems dobj_integrate_structure aux_integrate_to vmod_attempts_ranging vmod_attempts_integrate amod_attempts_several dobj_see_attempts nsubj_see_we advcl_see_reached advmod_see_Actually det_maturity_some dobj_reached_maturity aux_reached_has nsubj_reached_SMT mark_reached_that advmod_reached_now
W07-0413	J97-3002	o	A few exceptions are the hierarchical -LRB- possibly syntaxbased -RRB- transduction models -LRB- Wu 1997 Alshawi et al. 1998 Yamada and Knight 2001 Chiang 2005 -RRB- and the string transduction models -LRB- Kanthak et al. 2005 -RRB-	amod_Kanthak_2005 dep_Kanthak_al. nn_Kanthak_et dep_models_Kanthak nn_models_transduction nn_models_string det_models_the num_Chiang_2005 conj_and_Yamada_Knight nn_al._et nn_al._Alshawi dep_Wu_Chiang amod_Wu_2001 dep_Wu_Knight dep_Wu_Yamada num_Wu_1998 dep_Wu_al. amod_Wu_1997 conj_and_models_models dep_models_Wu nn_models_transduction amod_models_hierarchical det_models_the cop_models_are nsubj_models_exceptions advmod_syntaxbased_possibly dep_hierarchical_syntaxbased amod_exceptions_few det_exceptions_A
W07-0414	J97-3002	o	Other models -LRB- Wu -LRB- 1997 -RRB- Xiong et al.	nn_al._et nn_al._Xiong dep_Wu_al. appos_Wu_1997 dep_models_Wu amod_models_Other
W07-0414	J97-3002	o	2.3 ITG Constraints The Inversion Transduction Grammar -LRB- ITG -RRB- -LRB- Wu 1997 -RRB- a derivative of the Syntax Directed Transduction Grammars -LRB- Aho and Ullman 1972 -RRB- constrains the possible permutations of the input string by defining rewrite rules that indicate permutations of the string	det_string_the prep_of_permutations_string dobj_indicate_permutations nsubj_indicate_that rcmod_rules_indicate dobj_rewrite_rules xcomp_defining_rewrite nn_string_input det_string_the prep_of_permutations_string amod_permutations_possible det_permutations_the prepc_by_constrains_defining dobj_constrains_permutations nsubj_constrains_Constraints dep_Aho_1972 conj_and_Aho_Ullman appos_Grammars_Ullman appos_Grammars_Aho nn_Grammars_Transduction nn_Grammars_Directed nn_Grammars_Syntax det_Grammars_the prep_of_derivative_Grammars det_derivative_a dep_Wu_1997 appos_Grammar_derivative appos_Grammar_Wu appos_Grammar_ITG nn_Grammar_Transduction nn_Grammar_Inversion det_Grammar_The dep_Constraints_Grammar nn_Constraints_ITG num_Constraints_2.3
W07-0709	J97-3002	o	Syntax-based MT approaches began with Wu -LRB- 1997 -RRB- who introduced the Inversion Transduction Grammars	nn_Grammars_Transduction nn_Grammars_Inversion det_Grammars_the dobj_introduced_Grammars nsubj_introduced_who rcmod_Wu_introduced appos_Wu_1997 prep_with_began_Wu nsubj_began_approaches nn_approaches_MT amod_approaches_Syntax-based
W07-1205	J97-3002	o	The other form of hybridization ?? a statistical MT model that is based on a deeper analysis of the syntactic 33 structure of a sentence ?? has also long been identified as a desirable objective in principle -LRB- consider -LRB- Wu 1997 Yamada and Knight 2001 -RRB- -RRB-	amod_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Knight dep_Wu_Yamada amod_Wu_1997 dep_consider_Wu prep_in_objective_principle amod_objective_desirable det_objective_a prep_as_identified_objective auxpass_identified_been advmod_identified_long advmod_identified_also aux_identified_has nsubjpass_identified_model nn_??_sentence det_??_a prep_of_structure_?? num_structure_33 nn_structure_syntactic det_structure_the prep_of_analysis_structure amod_analysis_deeper det_analysis_a prep_on_based_analysis auxpass_based_is nsubjpass_based_that rcmod_model_based nn_model_MT amod_model_statistical det_model_a rcmod_hybridization_identified num_hybridization_?? dep_form_consider prep_of_form_hybridization amod_form_other det_form_The ccomp_``_form
W08-0308	J97-3002	o	The idea of synchronous SSMT can be traced back to Wu -LRB- 1997 -RRB- s Stochastic Inversion Transduction Grammars	nn_Grammars_Transduction nn_Grammars_Inversion nn_Grammars_Stochastic dobj_s_Grammars appos_Wu_1997 dep_traced_s prep_to_traced_Wu advmod_traced_back auxpass_traced_be aux_traced_can nsubjpass_traced_idea amod_SSMT_synchronous prep_of_idea_SSMT det_idea_The
W08-0401	J97-3002	o	IBM constraints -LRB- Berger et al. 1996 -RRB- lexical word reordering model -LRB- Tillmann 2004 -RRB- and inversion transduction grammar -LRB- ITG -RRB- constraints -LRB- Wu 1995 Wu 1997 -RRB- belong to this type of approach	prep_of_type_approach det_type_this prep_to_belong_type nsubj_belong_constraints nsubj_belong_model nsubj_belong_constraints dep_Wu_1997 dep_Wu_Wu appos_Wu_1995 appos_constraints_Wu nn_constraints_grammar appos_grammar_ITG nn_grammar_transduction nn_grammar_inversion dep_Tillmann_2004 appos_model_Tillmann nn_model_reordering nn_model_word amod_model_lexical amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_constraints_constraints conj_and_constraints_model dep_constraints_Berger nn_constraints_IBM
W08-0403	J97-3002	o	Examples include Wus -LRB- Wu 1997 -RRB- ITG and Chiangs hierarchical models -LRB- Chiang 2007 -RRB-	amod_Chiang_2007 dep_models_Chiang amod_models_hierarchical nn_models_Chiangs conj_and_ITG_models dep_ITG_Wu nn_ITG_Wus dep_Wu_1997 dobj_include_models dobj_include_ITG nsubj_include_Examples
W08-0403	J97-3002	o	2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature -LRB- Wu 1997 Yamada and Knight 2001 Gildea 2003 Galley et al. 2004 Satta and Peserico 2005 -RRB-	amod_Satta_2005 conj_and_Satta_Peserico num_al._2004 nn_al._et nn_al._Galley num_Gildea_2003 num_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Peserico dep_Wu_Satta conj_Wu_al. conj_Wu_Gildea conj_Wu_Knight conj_Wu_Yamada appos_Wu_1997 dep_literature_Wu det_literature_the prep_in_investigated_literature advmod_investigated_actively auxpass_investigated_been aux_investigated_have nsubjpass_investigated_models prep_with_engaged_SCFG vmod_models_engaged nn_models_translation amod_models_Syntax-based rcmod_Work_investigated amod_Work_Related num_Work_2
W08-0408	J97-3002	o	Recent work on reordering has been on trying to find smart ways to decide word order using syntactic features such as POS tags -LRB- Lee and Ge 2005 -RRB- parse trees -LRB- Zhang et.al 2007 Wang et.al 2007 Collins et.al 2005 Yamada and Knight 2001 -RRB- to name just a few and synchronized CFG -LRB- Wu 1997 Chiang 2005 -RRB- again to name just a few	det_few_a advmod_few_just acomp_name_few aux_name_to advmod_name_again nsubj_name_work num_Chiang_2005 appos_Wu_Chiang num_Wu_1997 dep_CFG_Wu dobj_synchronized_CFG nsubj_synchronized_work det_few_a advmod_few_just dobj_name_few aux_name_to num_Knight_2001 num_et.al_2005 nn_et.al_Collins nn_et.al_Wang conj_and_et.al_Knight conj_and_et.al_Yamada conj_and_et.al_et.al dep_et.al_2007 appos_et.al_et.al amod_et.al_2007 nn_et.al_Zhang appos_trees_Knight appos_trees_Yamada appos_trees_et.al appos_trees_et.al vmod_parse_name dobj_parse_trees num_Ge_2005 conj_and_Lee_Ge vmod_tags_parse dep_tags_Ge dep_tags_Lee nn_tags_POS prep_such_as_features_tags amod_features_syntactic dobj_using_features nn_order_word dobj_decide_order aux_decide_to vmod_ways_decide amod_ways_smart dobj_find_ways aux_find_to xcomp_trying_find conj_and_been_name conj_and_been_synchronized xcomp_been_using prepc_on_been_trying aux_been_has nsubj_been_work prep_on_work_reordering amod_work_Recent ccomp_``_name ccomp_``_synchronized ccomp_``_been
W08-0409	J97-3002	o	Deeper syntax e.g. phrase or dependency structures has been shown useful in generative models -LRB- Wang and Zhou 2004 Lopez and Resnik 2005 -RRB- heuristic-based models -LRB- Ayan et al. 2004 Ozdowska 2004 -RRB- and even for syntactically motivated models such as ITG -LRB- Wu 1997 Cherry and Lin 2006 -RRB-	amod_Cherry_2006 conj_and_Cherry_Lin dep_Wu_Lin dep_Wu_Cherry dep_Wu_1997 appos_ITG_Wu prep_such_as_models_ITG amod_models_motivated advmod_motivated_syntactically pobj_for_models advmod_for_even dep_Ozdowska_2004 dep_Ayan_Ozdowska appos_Ayan_2004 dep_Ayan_al. nn_Ayan_et appos_models_Ayan amod_models_heuristic-based dep_Wang_2005 conj_and_Wang_Resnik conj_and_Wang_Lopez conj_and_Wang_2004 conj_and_Wang_Zhou conj_and_models_for conj_and_models_models appos_models_Resnik appos_models_Lopez appos_models_2004 appos_models_Zhou appos_models_Wang amod_models_generative prep_in_useful_for prep_in_useful_models prep_in_useful_models acomp_shown_useful auxpass_shown_been aux_shown_has prep_shown_e.g. nsubjpass_shown_syntax nn_structures_dependency conj_or_phrase_structures pobj_e.g._structures pobj_e.g._phrase amod_syntax_Deeper
W08-0411	J97-3002	o	The underlying formalisms used has been quite broad and include simple formalisms such as ITGs -LRB- Wu 1997 -RRB- hierarchicalsynchronousrules -LRB- Chiang 2005 -RRB- string to tree models by -LRB- Galley et al. 2004 -RRB- and -LRB- Galley et al. 2006 -RRB- synchronous CFG models such -LRB- Xia and McCord 2004 -RRB- -LRB- Yamada and Knight 2001 -RRB- synchronous Lexical Functional Grammar inspired approaches -LRB- Probst et al. 2002 -RRB- and others	amod_Probst_2002 dep_Probst_al. nn_Probst_et conj_and_approaches_others dep_approaches_Probst dobj_inspired_others dobj_inspired_approaches nsubj_inspired_Grammar amod_Grammar_Functional nn_Grammar_Lexical amod_Grammar_synchronous amod_Yamada_2001 conj_and_Yamada_Knight rcmod_Xia_inspired dep_Xia_Knight dep_Xia_Yamada appos_Xia_2004 conj_and_Xia_McCord amod_Xia_such dep_models_McCord dep_models_Xia nn_models_CFG amod_models_synchronous amod_Galley_2006 dep_Galley_al. nn_Galley_et appos_Galley_models conj_and_Galley_Galley amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_models_tree prep_by_string_Galley prep_by_string_Galley prep_to_string_models dep_Chiang_2005 appos_hierarchicalsynchronousrules_Chiang dep_Wu_1997 conj_ITGs_string conj_ITGs_hierarchicalsynchronousrules appos_ITGs_Wu prep_such_as_formalisms_ITGs amod_formalisms_simple dobj_include_formalisms nsubj_include_formalisms conj_and_broad_include advmod_broad_quite cop_broad_been aux_broad_has nsubj_broad_formalisms vmod_formalisms_used amod_formalisms_underlying det_formalisms_The
W09-0434	J97-3002	o	Modeling reordering as the inversion in order of two adjacent blocks is similar to the approach taken by the Inverse Transduction Model -LRB- ITG -RRB- -LRB- Wu 1997 -RRB- except that here we are not limited to a binary tree	amod_tree_binary det_tree_a prep_to_limited_tree neg_limited_not auxpass_limited_are nsubjpass_limited_we advmod_limited_here amod_that_limited dep_Wu_1997 appos_Model_Wu appos_Model_ITG nn_Model_Transduction nn_Model_Inverse det_Model_the prep_except_taken_that agent_taken_Model vmod_approach_taken det_approach_the prep_to_similar_approach cop_similar_is nsubj_similar_reordering amod_blocks_adjacent num_blocks_two prep_of_inversion_blocks dep_inversion_order dep_inversion_in det_inversion_the prep_as_reordering_inversion amod_reordering_Modeling
W09-1804	J97-3002	o	Probabilistic generative models like IBM 1-5 -LRB- Brown et al. 1993 -RRB- HMM -LRB- Vogel et al. 1996 -RRB- ITG -LRB- Wu 1997 -RRB- and LEAF -LRB- Fraser and Marcu 2007 -RRB- define formulas for P -LRB- f | e -RRB- or P -LRB- e f -RRB- with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1 Word alignment exercise -LRB- Knight 1997 -RRB-	dep_Knight_1997 appos_exercise_Knight nn_exercise_alignment nn_exercise_Word num_Figure_1 nn_Figure_gat nn_Figure_vat nn_Figure_arrat nn_Figure_forat nn_Figure_nnat dobj_wat_Figure nsubj_wat_mok nn_mok_hihok nn_mok_izok nn_mok_nok nn_mok_rarok nn_mok_lalok nn_mok_zanzanat nn_mok_mat nn_mok_arrat nsubj_wat_zanzanok nn_zanzanok_yorok nn_zanzanok_hihok nn_zanzanok_crrrok nn_zanzanok_nok nn_zanzanok_lalok nn_zanzanok_hilat nn_zanzanok_bat nn_zanzanok_mat nn_zanzanok_gat nn_zanzanok_nnat amod_zanzanok_wat nn_clok_ghirok nn_clok_yorok nn_clok_nok nn_clok_mok nn_clok_lalok nn_clok_at-yurp nn_clok_oloat nn_clok_quat nn_clok_nnat nn_clok_totat nn_clok_ok-yurp nn_clok_kantok nn_clok_izok nn_clok_nok nn_clok_wiwok nn_clok_nnat nn_clok_rrat nn_clok_pippat nn_clok_lat nn_clok_iat nn_clok_nok nn_clok_plok nn_clok_anok nn_clok_brok nn_clok_lalok nn_clok_eneat nn_clok_vat nn_clok_dat dobj_wat_clok nsubj_wat_bichat nn_bichat_jjat amod_bichat_wat nn_enemok_izok nn_enemok_sprok nn_enemok_lalok nn_enemok_ororok nn_enemok_farok nn_enemok_lalok nn_enemok_cat nn_enemok_quat nn_enemok_krat nn_enemok_dat dep_wat_exercise parataxis_wat_wat dep_wat_nnat dep_wat_wat dep_wat_wat dobj_wat_enemok nsubj_wat_stok nn_stok_jok nn_stok_izok nn_stok_sprok nn_stok_lalok nn_stok_cat nn_stok_quat nn_stok_jjat nn_stok_totat nn_stok_stok nn_stok_izok nn_stok_farok nn_stok_wiwok nn_stok_lat dep_sat_wat nn_pippat_krat nn_pippat_at-voon nn_pippat_jok nn_pippat_brok nn_pippat_drok nn_pippat_anok nn_pippat_ok-voon nn_pippat_dat nn_pippat_rrat nn_pippat_pippat nn_pippat_at-voon nn_pippat_at-drubel nn_pippat_sprok nn_pippat_plok nn_pippat_anok nn_pippat_ok-voon amod_pippat_ok-drubel nn_pippat_hilat nn_pippat_vat nn_pippat_arrat nn_pippat_dat nn_pippat_totat nn_pippat_ghirok nn_pippat_hihok nn_pippat_izok nn_pippat_sprok nn_pippat_erok nn_pippat_dat nn_pippat_bichat nn_pippat_at-voon nn_pippat_sprok nn_pippat_ororok amod_pippat_ok-voon dep_|_f dep_|_e conj_or_|_P dep_|_e nn_|_f dep_P_P dep_P_| prep_for_formulas_P dep_define_sat prep_with_define_pippat dobj_define_formulas nsubj_define_models dep_Fraser_2007 conj_and_Fraser_Marcu appos_LEAF_Marcu appos_LEAF_Fraser dep_Wu_1997 appos_ITG_Wu amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et conj_and_HMM_LEAF conj_and_HMM_ITG dep_HMM_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et num_IBM_1-5 appos_models_LEAF appos_models_ITG appos_models_HMM dep_models_Brown prep_like_models_IBM amod_models_generative nn_models_Probabilistic
W09-1804	J97-3002	o	4 Related Work -LRB- Zhang et al. 2003 -RRB- and -LRB- Wu 1997 -RRB- tackle the problem of segmenting Chinese while aligning it to English	prep_to_aligning_English dobj_aligning_it mark_aligning_while advcl_segmenting_aligning dobj_segmenting_Chinese prepc_of_problem_segmenting det_problem_the dobj_tackle_problem nsubj_tackle_Wu nsubj_tackle_Work amod_Wu_1997 amod_Zhang_2003 dep_Zhang_al. nn_Zhang_et conj_and_Work_Wu dep_Work_Zhang amod_Work_Related num_Work_4
W09-1908	J97-3002	o	While traditional approaches to syntax based MT were dependent on availability of manual grammar more recent approaches operate within the resources of PB-SMT and induce hierarchical or linguistic grammars from existing phrasal units to provide better generality and structure for reordering -LRB- Yamada and Knight 2001 Chiang 2005 Wu 1997 -RRB-	amod_Wu_1997 dep_Chiang_Wu num_Chiang_2005 dep_Yamada_Chiang conj_and_Yamada_2001 conj_and_Yamada_Knight dep_reordering_2001 dep_reordering_Knight dep_reordering_Yamada conj_and_generality_structure amod_generality_better prep_for_provide_reordering dobj_provide_structure dobj_provide_generality aux_provide_to amod_units_phrasal amod_units_existing amod_grammars_linguistic amod_grammars_hierarchical conj_or_hierarchical_linguistic prep_from_induce_units dobj_induce_grammars nsubj_induce_approaches prep_of_resources_PB-SMT det_resources_the xcomp_operate_provide conj_and_operate_induce prep_within_operate_resources nsubj_operate_approaches advcl_operate_dependent amod_approaches_recent advmod_recent_more amod_grammar_manual prep_of_availability_grammar prep_on_dependent_availability cop_dependent_were nsubj_dependent_MT dep_dependent_approaches mark_dependent_While amod_MT_based prep_to_approaches_syntax amod_approaches_traditional
W09-2303	J97-3002	o	production rules are typically learned from alignment structures -LRB- Wu 1997 Zhang and Gildea 2004 Chiang 2007 -RRB- or from alignment structures and derivation trees for the source string -LRB- Yamada and Knight 2001 Zhang and Gildea 2004 -RRB-	dep_Zhang_2004 conj_and_Zhang_Gildea dep_Yamada_Gildea dep_Yamada_Zhang conj_and_Yamada_2001 conj_and_Yamada_Knight dep_string_2001 dep_string_Knight dep_string_Yamada nn_string_source det_string_the nn_trees_derivation conj_and_structures_trees nn_structures_alignment dep_Chiang_2007 num_Zhang_2004 conj_and_Zhang_Gildea dep_Wu_Chiang conj_Wu_Gildea conj_Wu_Zhang appos_Wu_1997 conj_or_structures_trees conj_or_structures_structures appos_structures_Wu nn_structures_alignment prep_for_learned_string prep_from_learned_structures prep_from_learned_structures advmod_learned_typically auxpass_learned_are nsubjpass_learned_rules nn_rules_production rcmod_``_learned
W09-2303	J97-3002	o	Related work includes Wu -LRB- 1997 -RRB- Zens and Ney -LRB- 2003 -RRB- and Wellington et al.	nn_al._et nn_al._Wellington appos_Ney_2003 conj_and_Wu_al. conj_and_Wu_Ney conj_and_Wu_Zens appos_Wu_1997 dobj_includes_al. dobj_includes_Ney dobj_includes_Zens dobj_includes_Wu nsubj_includes_work amod_work_Related
W09-2303	J97-3002	o	They are also used for inducing alignments -LRB- Wu 1997 Zhang and Gildea 2004 -RRB-	amod_Zhang_2004 conj_and_Zhang_Gildea dep_Wu_Gildea dep_Wu_Zhang appos_Wu_1997 appos_alignments_Wu dobj_inducing_alignments prepc_for_used_inducing advmod_used_also auxpass_used_are nsubjpass_used_They
W09-2303	J97-3002	o	The production rules in ITGs are of the following form -LRB- Wu 1997 -RRB- with a notation similar to what is typically used for SDTSs and SCFGs in the right column A -LSB- BC -RSB- A B1C2 B1C2 A BC A B1C2 C2B1 A e | f A e f A e | A e A | f A f It is important to note that RHSs of production rules have at most one source-side and one targetside terminal symbol	amod_symbol_terminal nn_symbol_targetside num_symbol_one conj_and_source-side_symbol num_source-side_one amod_source-side_most prep_at_have_symbol prep_at_have_source-side nsubj_have_RHSs mark_have_that nn_rules_production prep_of_RHSs_rules ccomp_note_have aux_note_to dep_note_important dep_note_e dep_note_A dep_note_f dep_note_| dep_note_e nsubj_note_A dep_note_B1C2 dep_note_A cop_important_is nsubj_important_It advmod_important_f nsubj_important_A dep_important_e dep_important_A dep_important_| nn_A_f nn_A_| det_A_A dep_|_e quantmod_|_A dep_|_f nn_A_C2B1 nn_B1C2_A nn_B1C2_BC nn_B1C2_A nn_B1C2_B1C2 conj_B1C2_B1C2 det_B1C2_A dep_B1C2_BC amod_column_right det_column_the conj_and_SDTSs_SCFGs prep_in_used_column prep_for_used_SCFGs prep_for_used_SDTSs advmod_used_typically auxpass_used_is nsubjpass_used_what prepc_to_similar_used amod_notation_similar det_notation_a amod_Wu_1997 dep_form_Wu amod_form_following det_form_the dep_are_note prep_with_are_notation prep_of_are_form rcmod_rules_are prep_in_rules_ITGs nn_rules_production det_rules_The
W09-2303	J97-3002	o	In this paper it is shown that the synchronous grammars used in Wu -LRB- 1997 -RRB- Zhang et al.	nn_al._et nn_al._Zhang dep_Wu_al. appos_Wu_1997 prep_in_used_Wu vmod_grammars_used amod_grammars_synchronous det_grammars_the prep_that_shown_grammars auxpass_shown_is nsubjpass_shown_it prep_in_shown_paper det_paper_this
W09-2303	J97-3002	o	of Linguistics University of Potsdam kuhn@ling.uni-potsdam.de Abstract The empirical adequacy of synchronous context-free grammars of rank two -LRB- 2-SCFGs -RRB- -LRB- Satta and Peserico 2005 -RRB- used in syntaxbased machine translation systems such as Wu -LRB- 1997 -RRB- Zhang et al.	nn_al._et nn_al._Zhang dep_Wu_al. appos_Wu_1997 prep_such_as_systems_Wu nn_systems_translation nn_systems_machine amod_systems_syntaxbased prep_in_used_systems dep_Satta_2005 conj_and_Satta_Peserico vmod_two_used appos_two_Peserico appos_two_Satta appos_two_2-SCFGs dep_rank_two prep_of_grammars_rank amod_grammars_context-free amod_grammars_synchronous prep_of_adequacy_grammars amod_adequacy_empirical nn_adequacy_The nn_adequacy_Abstract nn_adequacy_kuhn@ling.uni-potsdam.de dep_University_adequacy prep_of_University_Potsdam nn_University_Linguistics prep_of_``_University
W09-2303	J97-3002	o	-LRB- 2006 -RRB- and Chiang -LRB- 2007 -RRB- in terms of what alignments they induce has been discussed in Wu -LRB- 1997 -RRB- and Wellington et al.	nn_al._et nn_al._Wellington conj_and_Wu_al. appos_Wu_1997 prep_in_discussed_al. prep_in_discussed_Wu auxpass_discussed_been aux_discussed_has nsubjpass_discussed_Chiang nsubjpass_discussed_2006 nsubj_induce_they rcmod_alignments_induce det_alignments_what prep_of_terms_alignments appos_Chiang_2007 prep_in_2006_terms conj_and_2006_Chiang
W09-2303	J97-3002	o	2 Inside-out alignments Wu -LRB- 1997 -RRB- identified so-called inside-out alignments two alignment configurations that can not be induced by binary synchronous context-free grammars these alignment configurations while infrequent in language pairs such as EnglishFrench -LRB- Cherry and Lin 2006 Wellington et al. 2006 -RRB- have been argued to be frequent in other language pairs incl	appos_pairs_incl nn_pairs_language amod_pairs_other prep_in_frequent_pairs cop_frequent_be aux_frequent_to xcomp_argued_frequent auxpass_argued_been aux_argued_have nsubjpass_argued_configurations num_Wellington_2006 nn_Wellington_al. nn_Wellington_et dep_Cherry_Wellington amod_Cherry_2006 conj_and_Cherry_Lin appos_EnglishFrench_Lin appos_EnglishFrench_Cherry prep_such_as_pairs_EnglishFrench nn_pairs_language prep_in_infrequent_pairs mark_infrequent_while advcl_configurations_infrequent nn_configurations_alignment det_configurations_these amod_grammars_context-free amod_grammars_synchronous amod_grammars_binary agent_induced_grammars auxpass_induced_be neg_induced_not aux_induced_can nsubjpass_induced_that rcmod_configurations_induced nn_configurations_alignment num_configurations_two appos_alignments_configurations amod_alignments_inside-out amod_alignments_so-called parataxis_identified_argued dobj_identified_alignments nsubj_identified_Wu appos_Wu_1997 nn_Wu_alignments amod_Wu_Inside-out num_Wu_2
W09-2306	J97-3002	o	To overcome these limitations many syntaxbased SMT models have been proposed -LRB- Wu 1997 Chiang 2007 Ding et al. 2005 Eisner 2003 Quirk et al. 2005 Liu et al. 2007 Zhang et al. 2007 Zhang et al. 2008a Zhang et al. 2008b Gildea 2003 Galley et al. 2004 Marcu et al. 2006 Bod 2007 -RRB-	amod_Bod_2007 nn_al._et nn_al._Marcu nn_al._et nn_al._Galley num_Gildea_2003 dep_Zhang_al. nn_Zhang_et dep_Zhang_al. nn_Zhang_et num_Zhang_2007 nn_Zhang_al. nn_Zhang_et num_Liu_2007 nn_Liu_al. nn_Liu_et num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Eisner_2003 num_Ding_2005 nn_Ding_al. nn_Ding_et appos_Chiang_2008a conj_Chiang_Zhang conj_Chiang_Zhang conj_Chiang_Liu conj_Chiang_Quirk conj_Chiang_Eisner conj_Chiang_Ding num_Chiang_2007 dep_Wu_Bod num_Wu_2006 dep_Wu_al. num_Wu_2004 dep_Wu_al. dep_Wu_Gildea appos_Wu_2008b dep_Wu_Zhang dep_Wu_Chiang appos_Wu_1997 dep_proposed_Wu auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models advcl_proposed_overcome nn_models_SMT amod_models_syntaxbased amod_models_many det_limitations_these dobj_overcome_limitations aux_overcome_To ccomp_``_proposed
W09-2308	J97-3002	o	In this paper two synchronous grammar formalisms are discussed inversion transduction grammars -LRB- ITGs -RRB- -LRB- Wu 1997 -RRB- and two-variable binary bottom-up non-erasing range concatenation grammars -LRB- -LRB- 2,2 -RRB- BRCGs -RRB- -LRB- Sgaard 2008 -RRB-	amod_Sgaard_2008 dep_grammars_BRCGs appos_grammars_2,2 nn_grammars_concatenation nn_grammars_range amod_grammars_non-erasing amod_grammars_bottom-up amod_grammars_binary amod_grammars_two-variable dep_Wu_1997 dep_grammars_Sgaard conj_and_grammars_grammars appos_grammars_Wu appos_grammars_ITGs nn_grammars_transduction nn_grammars_inversion dobj_discussed_grammars dobj_discussed_grammars auxpass_discussed_are nsubjpass_discussed_formalisms prep_in_discussed_paper nn_formalisms_grammar amod_formalisms_synchronous num_formalisms_two det_paper_this
W09-2308	J97-3002	o	It is known that ITGs do not induce the class of inside-out alignments discussed in Wu -LRB- 1997 -RRB-	appos_Wu_1997 prep_in_discussed_Wu amod_alignments_inside-out vmod_class_discussed prep_of_class_alignments det_class_the dobj_induce_class neg_induce_not aux_induce_do nsubj_induce_ITGs mark_induce_that ccomp_known_induce auxpass_known_is nsubjpass_known_It
W09-2308	J97-3002	o	The complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation inversion transduction grammars -LRB- ITGs -RRB- -LRB- Wu 1997 -RRB- and a restricted form of range concatenation grammars -LRB- -LRB- 2,2 -RRB- BRCGs -RRB- -LRB- Sgaard 2008 -RRB- are investigated	auxpass_investigated_are nsubjpass_investigated_form nsubjpass_investigated_grammars nsubjpass_investigated_complexities amod_Sgaard_2008 nn_grammars_concatenation nn_grammars_range prep_of_form_grammars amod_form_restricted det_form_a num_Wu_1997 appos_grammars_Wu appos_grammars_ITGs nn_grammars_transduction nn_grammars_inversion nn_translation_machine amod_translation_syntax-based prep_of_formalisms_translation nn_formalisms_grammar amod_formalisms_synchronous amod_formalisms_different num_formalisms_two advmod_different_very prep_in_problems_formalisms nn_problems_alignment amod_problems_restricted num_problems_15 dep_complexities_Sgaard dep_complexities_BRCGs dep_complexities_2,2 conj_and_complexities_form conj_and_complexities_grammars prep_of_complexities_problems det_complexities_The
W09-2308	J97-3002	o	2 Inversion transduction grammars Inversion transduction grammars -LRB- ITGs -RRB- -LRB- Wu 1997 -RRB- are a notational variant of binary syntax-directed translation schemas -LRB- Aho and Ullman 1972 -RRB- and are usually presented with a normal form A -LSB- BC -RSB- A BC A e | f A e | A | f where A B C N and e f T The first production rule intuitively says that the subtree -LSB- -LSB- -RSB- B -LSB- -RSB- C -RSB- A in the source language translates into 62 a subtree -LSB- -LSB- -RSB- B -LSB- -RSB- C -RSB- A whereas the second production rule inverts the order in the target language i.e. -LSB- -LSB- -RSB- C -LSB- -RSB- B -RSB- A The universal recognition problem of ITGs can be solved in time O -LRB- n6 | G | -RRB- by a CYKstyle parsing algorithm with two charts	num_charts_two prep_with_algorithm_charts nn_algorithm_parsing nn_algorithm_CYKstyle det_algorithm_a nn_|_G num_|_| nn_|_n6 appos_O_| nn_O_time agent_solved_algorithm prep_in_solved_O auxpass_solved_be aux_solved_can nsubjpass_solved_problem prep_of_problem_ITGs nn_problem_recognition amod_problem_universal det_problem_The appos_A_B nn_A_C pobj_i.e._A nn_language_target det_language_the prep_in_order_language det_order_the prep_inverts_i.e. dobj_inverts_order nsubj_inverts_rule nn_rule_production amod_rule_second det_rule_the appos_A_C nn_A_B det_subtree_a num_subtree_62 parataxis_translates_inverts dep_translates_whereas dobj_translates_A prep_into_translates_subtree nsubj_translates_A mark_translates_that nn_language_source det_language_the prep_in_A_language nn_A_B nn_A_subtree appos_B_C det_subtree_the ccomp_says_translates nsubj_says_rule dep_says_T dep_says_e appos_rule_intuitively nn_rule_production amod_rule_first det_rule_The nn_T_f dep_N_solved conj_and_N_says nn_N_C nn_N_B det_N_A dep_where_says dep_where_N dep_f_where nn_f_| det_f_A num_f_| dep_f_e vmod_A_f amod_f_A nn_f_| dep_e_f amod_A_e nn_A_BC det_A_A dep_BC_A dep_A_BC dep_form_A amod_form_normal det_form_a prep_with_presented_form advmod_presented_usually auxpass_presented_are nsubjpass_presented_grammars dep_Aho_1972 conj_and_Aho_Ullman appos_schemas_Ullman appos_schemas_Aho nn_schemas_translation amod_schemas_syntax-directed amod_schemas_binary conj_and_variant_presented prep_of_variant_schemas amod_variant_notational det_variant_a cop_variant_are nsubj_variant_grammars amod_Wu_1997 appos_grammars_Wu appos_grammars_ITGs nn_grammars_transduction nn_grammars_Inversion nn_grammars_grammars nn_grammars_transduction nn_grammars_Inversion num_grammars_2
W09-2309	J97-3002	o	IBM constraints -LRB- Berger et al. 1996 -RRB- the lexical word reordering model -LRB- Tillmann 2004 -RRB- and inversion transduction grammar -LRB- ITG -RRB- constraints -LRB- Wu 1995 Wu 1997 -RRB- belong to this type of approach	prep_of_type_approach det_type_this prep_to_belong_type nsubj_belong_constraints nsubj_belong_model nsubj_belong_constraints dep_Wu_1997 dep_Wu_Wu appos_Wu_1995 appos_constraints_Wu nn_constraints_grammar appos_grammar_ITG nn_grammar_transduction nn_grammar_inversion dep_Tillmann_2004 appos_model_Tillmann nn_model_reordering nn_model_word amod_model_lexical det_model_the amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_constraints_constraints conj_and_constraints_model dep_constraints_Berger nn_constraints_IBM
A00-1005	J99-3003	p	However the only known work which automates part of a customer service center using natural language dialogue is the one by Chu-Carroll and Carpenter -LRB- 1999 -RRB-	appos_Carpenter_1999 conj_and_Chu-Carroll_Carpenter prep_by_one_Carpenter prep_by_one_Chu-Carroll det_one_the cop_one_is nsubj_one_work advmod_one_However nn_dialogue_language amod_dialogue_natural dobj_using_dialogue nn_center_service nn_center_customer det_center_a prep_of_part_center xcomp_automates_using dobj_automates_part nsubj_automates_which rcmod_work_automates amod_work_known det_work_the advmod_known_only
A00-1014	J99-3003	o	Using a vector-based topic identification process -LRB- Salton 1971 Chu-Carroll and Carpenter 1999 -RRB- these keywords are used to determine a set of likely values -LRB- including null -RRB- for that attribute	det_attribute_that prep_including_values_null amod_values_likely prep_for_set_attribute prep_of_set_values det_set_a dobj_determine_set aux_determine_to xcomp_used_determine auxpass_used_are nsubjpass_used_keywords vmod_used_Using det_keywords_these amod_Chu-Carroll_1999 conj_and_Chu-Carroll_Carpenter dep_Salton_Carpenter dep_Salton_Chu-Carroll amod_Salton_1971 appos_process_Salton nn_process_identification nn_process_topic amod_process_vector-based det_process_a dobj_Using_process
A00-2028	J99-3003	o	Research prototypes exist for applications such as personal email and calendars travel and restaurant information and personal banking -LRB- Baggia et al. 1998 Walker et al. 1998 Seneff et al. 1995 Sanderman et al. 1998 Chu-Carroll and Carpenter 1999 -RRB- inter alia	nn_alia_inter nn_alia_banking dep_Chu-Carroll_1999 conj_and_Chu-Carroll_Carpenter num_Sanderman_1998 nn_Sanderman_al. nn_Sanderman_et num_Seneff_1995 nn_Seneff_al. nn_Seneff_et num_Walker_1998 nn_Walker_al. nn_Walker_et dep_Baggia_Carpenter dep_Baggia_Chu-Carroll dep_Baggia_Sanderman dep_Baggia_Seneff dep_Baggia_Walker dep_Baggia_1998 dep_Baggia_al. nn_Baggia_et appos_banking_Baggia amod_banking_personal nn_information_restaurant conj_and_email_information conj_and_email_travel conj_and_email_calendars amod_email_personal prep_such_as_applications_information prep_such_as_applications_travel prep_such_as_applications_calendars prep_such_as_applications_email conj_and_exist_alia prep_for_exist_applications nsubj_exist_prototypes nn_prototypes_Research
A00-2028	J99-3003	o	2 Experimental System and Data HMIHY is a spoken dialogue system based on the notion of call routing -LRB- Gorin et al. 1997 Chu-Carroll and Carpenter 1999 -RRB-	appos_Chu-Carroll_1999 conj_and_Chu-Carroll_Carpenter dep_Gorin_Carpenter dep_Gorin_Chu-Carroll appos_Gorin_1997 dep_Gorin_al. nn_Gorin_et dep_routing_Gorin vmod_call_routing prep_of_notion_call det_notion_the prep_on_based_notion vmod_system_based nn_system_dialogue amod_system_spoken det_system_a cop_system_is nsubj_system_HMIHY nsubj_system_System nn_HMIHY_Data conj_and_System_HMIHY amod_System_Experimental num_System_2
P04-1010	J99-3003	o	Adapting a vectorbased approach reported by Chu-Carroll and Carpenter -LRB- 1999 -RRB- the Task ID Frame Agent is domain-independent and automatically trained	advmod_trained_automatically nsubj_trained_Agent conj_and_domain-independent_trained cop_domain-independent_is nsubj_domain-independent_Agent ccomp_domain-independent_Adapting nn_Agent_Frame nn_Agent_ID nn_Agent_Task det_Agent_the appos_Carpenter_1999 conj_and_Chu-Carroll_Carpenter agent_reported_Carpenter agent_reported_Chu-Carroll vmod_approach_reported amod_approach_vectorbased det_approach_a dobj_Adapting_approach
W00-0310	J99-3003	o	1 Specifically MIMIC uses an n-dimensional call router front-end -LRB- Chu-Carroll 2000 -RRB- which is a generalization of the vector-based call-routing paradigm of semantic interpretation -LRB- Chu-CarroU and Carpenter 1999 -RRB- that is instead of detecting one concept per utterance MIMIC 's semantic interpretation engine detects multiple -LRB- n -RRB- concepts or classes conveyed by a single utterance by using n call touters in parallel	nn_touters_call nn_touters_n prep_in_using_parallel dobj_using_touters amod_utterance_single det_utterance_a agent_conveyed_utterance vmod_concepts_conveyed conj_or_concepts_classes amod_concepts_multiple appos_multiple_n prepc_by_detects_using dobj_detects_classes dobj_detects_concepts nsubj_detects_engine prepc_instead_of_detects_detecting aux_detects_is nsubj_detects_that nn_engine_interpretation amod_engine_semantic poss_engine_MIMIC prep_per_concept_utterance num_concept_one dobj_detecting_concept dep_Chu-CarroU_1999 conj_and_Chu-CarroU_Carpenter appos_interpretation_Carpenter appos_interpretation_Chu-CarroU amod_interpretation_semantic prep_of_paradigm_interpretation amod_paradigm_call-routing amod_paradigm_vector-based det_paradigm_the prep_of_generalization_paradigm det_generalization_a cop_generalization_is nsubj_generalization_which dep_Chu-Carroll_2000 rcmod_front-end_generalization appos_front-end_Chu-Carroll nn_front-end_router nn_front-end_call amod_front-end_n-dimensional det_front-end_an parataxis_uses_detects dobj_uses_front-end nsubj_uses_MIMIC advmod_uses_Specifically nsubj_uses_1
W01-1602	J99-3003	o	Our approach thus provides an even more extreme version of automatic con rmation generation than that used byChu-Carroll and Carpenter -LRB- 1999 -RRB- where only a small eort is required by the developer	det_developer_the agent_required_developer auxpass_required_is nsubjpass_required_eort advmod_required_where amod_eort_small det_eort_a advmod_eort_only appos_Carpenter_1999 rcmod_byChu-Carroll_required conj_and_byChu-Carroll_Carpenter amod_byChu-Carroll_used dep_that_Carpenter dep_that_byChu-Carroll nn_generation_rmation nn_generation_con amod_generation_automatic prep_than_version_that prep_of_version_generation amod_version_extreme det_version_an advmod_extreme_more advmod_more_even dobj_provides_version advmod_provides_thus nsubj_provides_approach poss_approach_Our
W03-2126	J99-3003	o	Their approaches include the use of a vector-based information retrieval technique -LRB- Lee et al. 2000 Chu-Carroll and Carpenter 1999 -RRB- / bin/bash line 1 a command not found Our do mains are more varied which may results in more recognition errors	nn_errors_recognition amod_errors_more prep_in_results_errors aux_results_may nsubj_results_which dep_varied_results advmod_varied_more cop_varied_are nsubj_varied_mains dep_do_varied poss_do_Our dep_found_do neg_found_not vmod_command_found det_command_a dep_line_command num_line_1 dep_bin/bash_line num_Chu-Carroll_1999 conj_and_Chu-Carroll_Carpenter dep_Lee_Carpenter dep_Lee_Chu-Carroll amod_Lee_2000 dep_Lee_al. nn_Lee_et nn_technique_retrieval nn_technique_information amod_technique_vector-based det_technique_a prep_of_use_technique det_use_the dep_include_bin/bash dep_include_Lee dobj_include_use nsubj_include_approaches poss_approaches_Their
W05-0404	J99-3003	o	An alternative would be using a vector space model for classi cation where calltypes and utterances are represented as vectors including word a2 grams -LRB- Chu-Carroll and Carpenter 1999 -RRB-	amod_Chu-Carroll_1999 conj_and_Chu-Carroll_Carpenter dep_grams_Carpenter dep_grams_Chu-Carroll nn_grams_a2 nn_grams_word prep_including_vectors_grams prep_as_represented_vectors auxpass_represented_are nsubjpass_represented_utterances nsubjpass_represented_calltypes advmod_represented_where conj_and_calltypes_utterances rcmod_cation_represented nn_cation_classi prep_for_model_cation nn_model_space nn_model_vector det_model_a dobj_using_model aux_using_be aux_using_would nsubj_using_alternative det_alternative_An
W05-0404	J99-3003	o	This step can be seen as a multi-label multi-class call classi cation problem for customer care applications -LRB- Gorin et al. 1997 Chu-Carroll and Carpenter 1999 Gupta et al. To appear among others -RRB-	prep_among_appear_others aux_appear_To nn_al._et nn_al._Gupta num_Chu-Carroll_1999 conj_and_Chu-Carroll_Carpenter dep_Gorin_appear dep_Gorin_al. dep_Gorin_Carpenter dep_Gorin_Chu-Carroll amod_Gorin_1997 dep_Gorin_al. nn_Gorin_et nn_applications_care nn_applications_customer prep_for_problem_applications nn_problem_cation nn_problem_classi nn_problem_call amod_problem_multi-class amod_problem_multi-label det_problem_a dep_seen_Gorin prep_as_seen_problem auxpass_seen_be aux_seen_can nsubjpass_seen_step det_step_This ccomp_``_seen
W06-1303	J99-3003	o	An alternative is to create an automatic system that uses a set of training question-answer pairs to learn the appropriate question-answer matching algorithm -LRB- Chu-Carroll and Carpenter 1999 -RRB-	amod_Chu-Carroll_1999 conj_and_Chu-Carroll_Carpenter dep_algorithm_Carpenter dep_algorithm_Chu-Carroll amod_algorithm_matching nn_algorithm_question-answer amod_algorithm_appropriate det_algorithm_the dobj_learn_algorithm aux_learn_to nn_pairs_question-answer nn_pairs_training prep_of_set_pairs det_set_a vmod_uses_learn dobj_uses_set nsubj_uses_that rcmod_system_uses amod_system_automatic det_system_an dobj_create_system aux_create_to xcomp_is_create nsubj_is_alternative det_alternative_An
W07-0310	J99-3003	o	Chu-Carroll and Carpenter -LRB- 1999 -RRB- describe a method of disambiguation where disambiguation questions are dynamically constructed on the basis of an analysis of the differences among the closest routing destination vectors	nn_vectors_destination amod_vectors_routing amod_vectors_closest det_vectors_the prep_among_differences_vectors det_differences_the prep_of_analysis_differences det_analysis_an prep_of_basis_analysis det_basis_the prep_on_constructed_basis advmod_constructed_dynamically auxpass_constructed_are nsubjpass_constructed_questions advmod_constructed_where nn_questions_disambiguation rcmod_disambiguation_constructed prep_of_method_disambiguation det_method_a dobj_describe_method nsubj_describe_Carpenter nsubj_describe_Chu-Carroll appos_Carpenter_1999 conj_and_Chu-Carroll_Carpenter
P09-1032	L08-1329	o	While this is certainly a daunting task it is possible that for annotation studies that do not require expert annotators and extensive annotator training the newly available access to a large pool of inexpensive annotators such as the Amazon Mechanical Turk scheme -LRB- Snow et al. 2008 -RRB- ,4 or embedding the task in an online game played by volunteers -LRB- Poesio et al. 2008 von Ahn 2006 -RRB- could provide some solutions	det_solutions_some dobj_provide_solutions aux_provide_could nsubj_provide_access prep_for_provide_studies mark_provide_that amod_Ahn_2006 nn_Ahn_von dep_Poesio_Ahn appos_Poesio_2008 dep_Poesio_al. nn_Poesio_et agent_played_volunteers vmod_game_played amod_game_online det_game_an det_task_the dobj_embedding_task conj_or_,4_embedding amod_Snow_2008 dep_Snow_al. nn_Snow_et dep_scheme_embedding dep_scheme_,4 dep_scheme_Snow nn_scheme_Turk amod_scheme_Mechanical nn_scheme_Amazon det_scheme_the prep_such_as_annotators_scheme amod_annotators_inexpensive prep_of_pool_annotators amod_pool_large det_pool_a dep_access_Poesio prep_in_access_game prep_to_access_pool amod_access_available det_access_the advmod_available_newly nn_training_annotator amod_training_extensive conj_and_annotators_training amod_annotators_expert dobj_require_training dobj_require_annotators neg_require_not aux_require_do nsubj_require_that rcmod_studies_require nn_studies_annotation ccomp_possible_provide cop_possible_is nsubj_possible_it advcl_possible_task amod_task_daunting det_task_a advmod_task_certainly cop_task_is nsubj_task_this mark_task_While
C04-1051	N03-1003	o	Lee & Barzilay -LRB- 2003 -RRB- for example use MultiSequence Alignment -LRB- MSA -RRB- to build a corpus of paraphrases involving terrorist acts	amod_acts_terrorist dobj_involving_acts vmod_paraphrases_involving prep_of_corpus_paraphrases det_corpus_a dobj_build_corpus aux_build_to vmod_Alignment_build appos_Alignment_MSA nn_Alignment_MultiSequence nn_Alignment_use dep_Lee_Alignment prep_for_Lee_example appos_Lee_2003 conj_and_Lee_Barzilay
C04-1051	N03-1003	o	Mean number of instances of paraphrase phenomena per sentence -LRB- such as Multiple Sequence Alignment as employed by Barzilay & Lee 2003 -RRB-	dep_Barzilay_2003 conj_and_Barzilay_Lee prep_by_employed_Lee prep_by_employed_Barzilay mark_employed_as nn_Alignment_Sequence amod_Alignment_Multiple dep_phenomena_employed prep_such_as_phenomena_Alignment prep_per_phenomena_sentence nn_phenomena_paraphrase prep_of_instances_phenomena prep_of_number_instances nn_number_Mean
C04-1051	N03-1003	n	While the idea of exploiting multiple news reports for paraphrase acquisition is not new previous efforts -LRB- for example Shinyama et al. 2002 Barzilay and Lee 2003 -RRB- have been restricted to at most two news sources	nn_sources_news num_sources_two amod_sources_most pobj_at_sources pcomp_to_at prep_restricted_to cop_restricted_been aux_restricted_have nsubj_restricted_Lee nsubj_restricted_Barzilay num_Lee_2003 conj_and_Barzilay_Lee num_al._2002 nn_al._et nn_al._Shinyama conj_for_restricted conj_for_al. pobj_for_example prep_efforts_for amod_efforts_previous advcl_efforts_new neg_new_not cop_new_is nsubj_new_idea mark_new_While nn_acquisition_paraphrase prep_for_reports_acquisition nn_reports_news amod_reports_multiple dobj_exploiting_reports prepc_of_idea_exploiting det_idea_the
C04-1051	N03-1003	o	1 Introduction The importance of learning to manipulate monolingual paraphrase relationships for applications like summarization search and dialog has been highlighted by a number of recent efforts -LRB- Barzilay & McKeown 2001 Shinyama et al. 2002 Lee & Barzilay 2003 Lin & Pantel 2001 -RRB-	num_Lin_2001 conj_and_Lin_Pantel num_Lee_2003 conj_and_Lee_Barzilay num_al._2002 nn_al._et nn_al._Shinyama dep_Barzilay_Pantel dep_Barzilay_Lin dep_Barzilay_Barzilay dep_Barzilay_Lee dep_Barzilay_al. dep_Barzilay_2001 conj_and_Barzilay_McKeown appos_efforts_McKeown appos_efforts_Barzilay amod_efforts_recent prep_of_number_efforts det_number_a agent_highlighted_number auxpass_highlighted_been aux_highlighted_has nsubjpass_highlighted_importance conj_and_summarization_dialog conj_and_summarization_search prep_like_applications_dialog prep_like_applications_search prep_like_applications_summarization nn_relationships_paraphrase amod_relationships_monolingual prep_for_manipulate_applications dobj_manipulate_relationships aux_manipulate_to xcomp_learning_manipulate prepc_of_importance_learning det_importance_The rcmod_Introduction_highlighted num_Introduction_1
C08-1029	N03-1003	o	Multiple translations of the same text -LRB- Barzilay and McKeown 2001 -RRB- corresponding articles from multiple news sources -LRB- Barzilay and Lee 2003 Quirk et al. 2004 Dolan et al. 2004 -RRB- and bilingual corpus -LRB- Bannard and Callison-Burch 2005 -RRB- have been utilized	auxpass_utilized_been aux_utilized_have nsubjpass_utilized_corpus nsubjpass_utilized_articles nsubjpass_utilized_translations dep_Bannard_2005 conj_and_Bannard_Callison-Burch appos_corpus_Callison-Burch appos_corpus_Bannard amod_corpus_bilingual num_Dolan_2004 nn_Dolan_al. nn_Dolan_et num_Quirk_2004 nn_Quirk_al. nn_Quirk_et dep_Barzilay_Dolan conj_and_Barzilay_Quirk conj_and_Barzilay_2003 conj_and_Barzilay_Lee appos_sources_Quirk appos_sources_2003 appos_sources_Lee appos_sources_Barzilay nn_sources_news amod_sources_multiple prep_from_articles_sources amod_articles_corresponding dep_Barzilay_2001 conj_and_Barzilay_McKeown appos_text_McKeown appos_text_Barzilay amod_text_same det_text_the conj_and_translations_corpus conj_and_translations_articles prep_of_translations_text amod_translations_Multiple
C08-1107	N03-1003	o	Some works focused on learning rules from comparable corpora containing comparable documents such as different news articles from the same date on the same topic -LRB- Barzilay and Lee 2003 Ibrahim et al. 2003 -RRB-	num_Ibrahim_2003 nn_Ibrahim_al. nn_Ibrahim_et dep_Barzilay_Ibrahim conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_topic_2003 dep_topic_Lee dep_topic_Barzilay amod_topic_same det_topic_the prep_on_date_topic amod_date_same det_date_the prep_from_articles_date nn_articles_news amod_articles_different prep_such_as_documents_articles amod_documents_comparable dobj_containing_documents vmod_corpora_containing amod_corpora_comparable prep_from_rules_corpora amod_rules_learning prep_on_focused_rules nsubj_focused_works det_works_Some ccomp_``_focused
C08-1110	N03-1003	o	This is related to the wellstudied problem of identifying paraphrases -LRB- Barzilay and Lee 2003 Pang et al. 2003 -RRB- and the more general variant of recognizing textual entailment which explores whether information expressed in a hypothesis can be inferred from a given premise	amod_premise_given det_premise_a prep_from_inferred_premise auxpass_inferred_be aux_inferred_can nsubjpass_inferred_information mark_inferred_whether det_hypothesis_a prep_in_expressed_hypothesis vmod_information_expressed ccomp_explores_inferred nsubj_explores_which rcmod_entailment_explores amod_entailment_textual dobj_recognizing_entailment prepc_of_variant_recognizing amod_variant_general amod_variant_more det_variant_the num_Pang_2003 nn_Pang_al. nn_Pang_et dep_Barzilay_Pang num_Barzilay_2003 conj_and_Barzilay_Lee conj_and_paraphrases_variant appos_paraphrases_Lee appos_paraphrases_Barzilay dobj_identifying_variant dobj_identifying_paraphrases prepc_of_problem_identifying amod_problem_wellstudied det_problem_the prep_to_related_problem cop_related_is nsubj_related_This ccomp_``_related
D09-1122	N03-1003	o	2 Related Work Previous studies on entailment inference rules and paraphrase acquisition are roughly classified into those that require comparable corpora -LRB- Shinyama et al. 2002 Barzilay and Lee 2003 Ibrahim et al. 2003 -RRB- and those that do not -LRB- Lin and Pantel 2001 Weeds and Weir 2003 Geffet and Dagan 2005 Pekar 2006 Bhagat et al. 2007 Szpektor and Dagan 2008 -RRB-	amod_Szpektor_2008 conj_and_Szpektor_Dagan num_Bhagat_2007 nn_Bhagat_al. nn_Bhagat_et num_Pekar_2006 dep_Lin_Dagan dep_Lin_Szpektor conj_and_Lin_Bhagat conj_and_Lin_Pekar conj_and_Lin_2005 conj_and_Lin_Dagan conj_and_Lin_Geffet conj_and_Lin_2003 conj_and_Lin_Weir conj_and_Lin_Weeds conj_and_Lin_2001 conj_and_Lin_Pantel neg_do_not nsubj_do_that appos_those_Bhagat appos_those_Pekar appos_those_2005 appos_those_Dagan appos_those_Geffet appos_those_2003 appos_those_Weir appos_those_Weeds appos_those_2001 appos_those_Pantel appos_those_Lin rcmod_those_do num_Ibrahim_2003 nn_Ibrahim_al. nn_Ibrahim_et num_Barzilay_2003 conj_and_Barzilay_Lee dep_Shinyama_Ibrahim conj_Shinyama_Lee conj_Shinyama_Barzilay appos_Shinyama_2002 dep_Shinyama_al. nn_Shinyama_et conj_and_corpora_those appos_corpora_Shinyama amod_corpora_comparable dobj_require_those dobj_require_corpora nsubj_require_that rcmod_those_require prep_into_classified_those advmod_classified_roughly auxpass_classified_are nsubjpass_classified_studies nn_acquisition_paraphrase nn_rules_inference conj_and_entailment_acquisition conj_and_entailment_rules prep_on_studies_acquisition prep_on_studies_rules prep_on_studies_entailment amod_studies_Previous rcmod_Work_classified amod_Work_Related num_Work_2
D09-1122	N03-1003	o	Barzilay and Lee -LRB- 2003 -RRB- also used newspaper articles on the same event as comparable corpora to acquire paraphrases	dobj_acquire_paraphrases aux_acquire_to vmod_corpora_acquire amod_corpora_comparable prep_as_event_corpora amod_event_same det_event_the nn_articles_newspaper prep_on_used_event dobj_used_articles advmod_used_also nsubj_used_Lee nsubj_used_Barzilay appos_Lee_2003 conj_and_Barzilay_Lee
E09-1082	N03-1003	o	-LRB- 2006 -RRB- propose using a statistical word alignment algorithm as a more robust way of aligning -LRB- monolingual -RRB- outputs into a confusion network for system com2Barzilay and Lee -LRB- 2003 -RRB- construct lattices over paraphrases using an iterative pairwise multiple sequence alignment -LRB- MSA -RRB- algorithm	nn_algorithm_alignment appos_alignment_MSA nn_alignment_sequence amod_alignment_multiple amod_alignment_pairwise amod_alignment_iterative det_alignment_an dobj_using_algorithm nn_lattices_construct nn_lattices_Lee appos_Lee_2003 conj_and_com2Barzilay_lattices nn_com2Barzilay_system prep_for_network_lattices prep_for_network_com2Barzilay nn_network_confusion det_network_a dep_outputs_monolingual amod_outputs_aligning prep_over_way_paraphrases prep_into_way_network prep_of_way_outputs amod_way_robust det_way_a advmod_robust_more nn_algorithm_alignment nn_algorithm_word amod_algorithm_statistical det_algorithm_a dep_using_using prep_as_using_way dobj_using_algorithm ccomp_propose_using dep_propose_2006
I05-5001	N03-1003	o	Barzilay & Lee -LRB- 2003 -RRB- employ Multiple Sequence Alignment -LRB- MSA e.g. Durbin et al. 1998 -RRB- to align strings extracted from closely related news articles	nn_articles_news amod_articles_related advmod_related_closely prep_from_extracted_articles vmod_strings_extracted dobj_align_strings aux_align_to num_Durbin_1998 nn_Durbin_al. nn_Durbin_et vmod_MSA_align appos_MSA_Durbin conj_MSA_e.g. dep_Alignment_MSA nn_Alignment_Sequence amod_Alignment_Multiple dobj_employ_Alignment nsubj_employ_Lee nsubj_employ_Barzilay appos_Barzilay_2003 conj_and_Barzilay_Lee
I05-5001	N03-1003	o	The word-based edit distance heuristic yields pairs that are relatively clean but offer relatively minor rewrites in generation especially when compared to the MSA model of -LRB- Barzilay & Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_of_Lee dep_of_Barzilay prep_model_of nn_model_MSA det_model_the prep_to_compared_model advmod_compared_when advmod_when_especially amod_rewrites_minor advmod_minor_relatively dobj_offer_rewrites nsubj_offer_that prep_in_clean_generation conj_but_clean_offer advmod_clean_relatively cop_clean_are nsubj_clean_that rcmod_pairs_compared rcmod_pairs_offer rcmod_pairs_clean dep_yields_pairs nn_yields_heuristic nn_yields_distance nn_yields_edit amod_yields_word-based det_yields_The dep_``_yields
I05-5001	N03-1003	o	A growing body of recent research has focused on the problems of identifying and generating paraphrases e.g. Barzilay & McKeown -LRB- 2001 -RRB- Lin & Pantel -LRB- 2002 -RRB- Shinyama et al -LRB- 2002 -RRB- Barzilay & Lee -LRB- 2003 -RRB- and Pang et al.	nn_al._et nn_al._Pang dep_Shinyama_al nn_Shinyama_et appos_Lin_2003 conj_and_Lin_Lee conj_and_Lin_Barzilay appos_Lin_2002 appos_Lin_Shinyama appos_Lin_2002 conj_and_Lin_Pantel conj_and_Barzilay_al. conj_and_Barzilay_Lee conj_and_Barzilay_Barzilay conj_and_Barzilay_Pantel conj_and_Barzilay_Lin appos_Barzilay_2001 conj_and_Barzilay_McKeown appos_e.g._al. appos_e.g._Lin appos_e.g._McKeown appos_e.g._Barzilay dobj_identifying_paraphrases conj_and_identifying_generating prepc_of_problems_generating prepc_of_problems_identifying det_problems_the prep_focused_e.g. prep_on_focused_problems aux_focused_has nsubj_focused_body amod_research_recent prep_of_body_research amod_body_growing det_body_A
I05-5001	N03-1003	o	Barzilay & Lee -LRB- 2003 -RRB- and Quirk et al.	nn_al._et nn_al._Quirk conj_and_Barzilay_al. dep_Barzilay_2003 conj_and_Barzilay_Lee
I05-5002	N03-1003	p	g2 2 Motivation The success of Statistical Machine Translation -LRB- SMT -RRB- has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem -LRB- e.g. Barzilay & Lee 2003 Pang et al. 2003 Quirk et al. 2004 Finch et al. 2004 -RRB-	num_Finch_2004 nn_Finch_al. nn_Finch_et num_Quirk_2004 nn_Quirk_al. nn_Quirk_et dep_Pang_Finch conj_Pang_Quirk num_Pang_2003 nn_Pang_al. nn_Pang_et dep_Barzilay_Pang num_Barzilay_2003 conj_and_Barzilay_Lee dep_e.g._Lee dep_e.g._Barzilay ccomp_-LRB-_e.g. nn_problem_translation nn_problem_machine amod_problem_monolingual det_problem_a conj_and_acquisition_generation nn_acquisition_paraphrase prep_as_treats_problem advmod_treats_essentially dobj_treats_generation dobj_treats_acquisition nsubj_treats_that rcmod_line_treats prep_of_line_investigation amod_line_successful det_line_a dobj_sparked_line aux_sparked_has nsubj_sparked_success appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical prep_of_success_Translation det_success_The rcmod_Motivation_sparked num_Motivation_2 nn_Motivation_g2
I05-5004	N03-1003	o	Some studies exploit topically related articles derived from multiple news sources -LRB- Barzilay and Lee 2003 Shinyama and Sekine 2003 Quirk et al. 2004 Dolan et al. 2004 -RRB-	num_Dolan_2004 nn_Dolan_al. nn_Dolan_et num_Quirk_2004 nn_Quirk_al. nn_Quirk_et dep_Shinyama_Dolan conj_and_Shinyama_Quirk conj_and_Shinyama_2003 conj_and_Shinyama_Sekine conj_and_Barzilay_Quirk conj_and_Barzilay_2003 conj_and_Barzilay_Sekine conj_and_Barzilay_Shinyama conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_sources_Shinyama dep_sources_2003 dep_sources_Lee dep_sources_Barzilay nn_sources_news amod_sources_multiple prep_from_derived_sources vmod_articles_derived amod_articles_related advmod_articles_topically dobj_exploit_articles nsubj_exploit_studies det_studies_Some ccomp_``_exploit
I05-5007	N03-1003	o	Generation of paraphrase examples was also investigated -LRB- Barzilay and Lee 2003 Quirk et al. 2004 -RRB-	num_Quirk_2004 nn_Quirk_al. nn_Quirk_et dep_Barzilay_Quirk conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_investigated_2003 dep_investigated_Lee dep_investigated_Barzilay advmod_investigated_also auxpass_investigated_was nsubjpass_investigated_Generation nn_examples_paraphrase prep_of_Generation_examples
I05-5008	N03-1003	p	Such a method alleviates the problem of creating templates from examples which would be used in an ulterior phase of generation -LRB- BARZILAY and LEE 2003 -RRB-	amod_BARZILAY_2003 conj_and_BARZILAY_LEE dep_generation_LEE dep_generation_BARZILAY prep_of_phase_generation amod_phase_ulterior det_phase_an prep_in_used_phase auxpass_used_be aux_used_would nsubjpass_used_which rcmod_examples_used prep_from_creating_examples dobj_creating_templates prepc_of_problem_creating det_problem_the dobj_alleviates_problem nsubj_alleviates_method det_method_a amod_method_Such ccomp_``_alleviates
I08-1070	N03-1003	o	The other utilizes a sort of parallel texts such as multiple translation of the same text -LRB- Barzilay and McKeown 2001 Pang et al. 2003 -RRB- corresponding articles from multiple news sources -LRB- Barzilay and Lee 2003 Dolan et al. 2004 -RRB- and bilingual corpus -LRB- Wu and Zhou 2003 Bannard and Callison-Burch 2005 -RRB-	amod_Bannard_2005 conj_and_Bannard_Callison-Burch dep_Wu_Callison-Burch dep_Wu_Bannard num_Wu_2003 conj_and_Wu_Zhou appos_corpus_Zhou appos_corpus_Wu amod_corpus_bilingual num_Dolan_2004 nn_Dolan_al. nn_Dolan_et conj_and_Barzilay_Dolan conj_and_Barzilay_2003 conj_and_Barzilay_Lee appos_sources_Dolan appos_sources_2003 appos_sources_Lee appos_sources_Barzilay nn_sources_news amod_sources_multiple prep_from_articles_sources amod_articles_corresponding num_Pang_2003 nn_Pang_al. nn_Pang_et conj_and_Barzilay_Pang conj_and_Barzilay_2001 conj_and_Barzilay_McKeown dep_text_Pang dep_text_2001 dep_text_McKeown dep_text_Barzilay amod_text_same det_text_the conj_and_translation_corpus conj_and_translation_articles prep_of_translation_text amod_translation_multiple prep_such_as_texts_corpus prep_such_as_texts_articles prep_such_as_texts_translation amod_texts_parallel prep_of_sort_texts det_sort_a dobj_utilizes_sort nsubj_utilizes_other det_other_The ccomp_``_utilizes
I08-2110	N03-1003	o	Recently some work has been done on corpusbased paraphrase extraction -LRB- Lin and Pantel 2001 Barzilay and Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_Lin_Lee dep_Lin_Barzilay num_Lin_2001 conj_and_Lin_Pantel appos_extraction_Pantel appos_extraction_Lin nn_extraction_paraphrase amod_extraction_corpusbased prep_on_done_extraction auxpass_done_been aux_done_has nsubjpass_done_work advmod_done_Recently det_work_some
N03-1024	N03-1003	o	Its still possible to use MSA if for example the input is pre-clustered to have the same constituent ordering -LRB- Barzilay and Lee -LRB- 2003 -RRB- -RRB-	appos_Lee_2003 conj_and_Barzilay_Lee dep_ordering_Lee dep_ordering_Barzilay vmod_constituent_ordering amod_constituent_same det_constituent_the dobj_have_constituent aux_have_to xcomp_pre-clustered_have cop_pre-clustered_is nsubj_pre-clustered_input prep_for_pre-clustered_example mark_pre-clustered_if det_input_the advcl_use_pre-clustered dobj_use_MSA aux_use_to xcomp_possible_use advmod_possible_still poss_possible_Its dep_``_possible
N04-1015	N03-1003	o	But because we want the insertion state a1a16a20 to model digressions or unseen topics we take the novel step of forcing its language model to be complementary to those of the other states by setting a2 a3a27a38 a21 a8 a8 a4 a8 a24 a26a11a28a30a29a6 a39a41a40a43a42a45a44a16a46 a1a48a47a1a50a49 a20 a2 a3 a26a17a21 a8a9a8 a4 a8 a24 a51a53a52a55a54a57a56 a21 a39a58a40a43a42a45a44a16a46 a1a59a47a1a50a49 a20 a2 a3a27a26a11a21a50a60 a4 a8 a24a30a24 a17 4Following Barzilay and Lee -LRB- 2003 -RRB- proper names numbers and dates are -LRB- temporarily -RRB- replaced with generic tokens to help ensure that clusters contain sentences describing the same event type rather than same actual event	amod_event_actual amod_event_same conj_negcc_type_event nn_type_event amod_type_same det_type_the dobj_describing_event dobj_describing_type vmod_sentences_describing dobj_contain_sentences nsubj_contain_clusters mark_contain_that ccomp_ensure_contain ccomp_help_ensure aux_help_to amod_tokens_generic xcomp_replaced_help prep_with_replaced_tokens dep_replaced_temporarily auxpass_replaced_are nsubjpass_replaced_dates nsubjpass_replaced_numbers nsubjpass_replaced_names conj_and_names_dates conj_and_names_numbers amod_names_proper appos_Lee_2003 conj_and_Barzilay_Lee nn_Barzilay_4Following nn_Barzilay_a17 nn_Barzilay_a24a30a24 nn_Barzilay_a8 nn_Barzilay_a4 nn_Barzilay_a3a27a26a11a21a50a60 nn_Barzilay_a2 nn_Barzilay_a20 nn_Barzilay_a1a59a47a1a50a49 nn_Barzilay_a39a58a40a43a42a45a44a16a46 nn_Barzilay_a21 nn_Barzilay_a51a53a52a55a54a57a56 nn_Barzilay_a24 nn_Barzilay_a8 nn_Barzilay_a4 num_Barzilay_a8a9a8 nn_Barzilay_a26a17a21 nn_Barzilay_a3 nn_Barzilay_a2 nn_Barzilay_a20 nn_Barzilay_a1a48a47a1a50a49 nn_Barzilay_a39a41a40a43a42a45a44a16a46 nn_Barzilay_a26a11a28a30a29a6 nn_Barzilay_a24 nn_Barzilay_a8 nn_Barzilay_a4 nn_Barzilay_a8 nn_Barzilay_a8 nn_Barzilay_a21 nn_Barzilay_a3a27a38 nn_Barzilay_a2 dobj_setting_Lee dobj_setting_Barzilay amod_states_other det_states_the prep_of_those_states prep_to_complementary_those cop_complementary_be aux_complementary_to nn_model_language poss_model_its prepc_by_forcing_setting xcomp_forcing_complementary dobj_forcing_model prepc_of_step_forcing amod_step_novel det_step_the parataxis_take_replaced dobj_take_step nsubj_take_we advcl_take_want cc_take_But amod_topics_unseen conj_or_digressions_topics nn_digressions_model prep_to_a1a16a20_topics prep_to_a1a16a20_digressions nn_a1a16a20_state nn_a1a16a20_insertion det_a1a16a20_the dobj_want_a1a16a20 nsubj_want_we mark_want_because
N04-1031	N03-1003	o	Although a large number of studies have been made on learning paraphrases for example -LRB- Barzilay and Lee 2003 -RRB- there are only a few studies which address the connotational difference of paraphrases	prep_of_difference_paraphrases amod_difference_connotational det_difference_the dobj_address_difference nsubj_address_which rcmod_studies_address amod_studies_few det_studies_a advmod_studies_only nsubj_are_studies expl_are_there dep_are_Lee dep_are_Barzilay prep_for_are_example advcl_are_made appos_Barzilay_2003 conj_and_Barzilay_Lee dobj_learning_paraphrases prepc_on_made_learning auxpass_made_been aux_made_have nsubjpass_made_number mark_made_Although prep_of_number_studies amod_number_large det_number_a
N04-1031	N03-1003	o	There are several works that try to learn paraphrase pairs from parallel or comparable corpora -LRB- Barzilay and McKeown 2001 Shinyama et al. 2002 Barzilay and Lee 2003 Pang et al. 2003 -RRB-	num_Pang_2003 nn_Pang_al. nn_Pang_et num_Barzilay_2003 conj_and_Barzilay_Lee num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et dep_Barzilay_Pang conj_and_Barzilay_Lee conj_and_Barzilay_Barzilay conj_and_Barzilay_Shinyama conj_and_Barzilay_2001 conj_and_Barzilay_McKeown dep_corpora_Barzilay dep_corpora_Shinyama dep_corpora_2001 dep_corpora_McKeown dep_corpora_Barzilay amod_corpora_comparable conj_or_parallel_corpora nn_pairs_paraphrase prep_from_learn_corpora prep_from_learn_parallel dobj_learn_pairs aux_learn_to xcomp_try_learn nsubj_try_that rcmod_works_try amod_works_several nsubj_are_works expl_are_There ccomp_``_are
N06-1008	N03-1003	o	Previous attempts have used for instance the similarities between case frames -LRB- Lin and Pan57 tel 2001 -RRB- anchor words -LRB- Barzilay and Lee 2003 Shinyama et al. 2002 Szepektor et al. 2004 -RRB- and a web-based method -LRB- Szepektor et al. 2004 Geffet and Dagan 2005 -RRB-	dep_Geffet_2005 conj_and_Geffet_Dagan dep_Szepektor_Dagan dep_Szepektor_Geffet appos_Szepektor_2004 dep_Szepektor_al. nn_Szepektor_et dep_method_Szepektor amod_method_web-based det_method_a num_Szepektor_2004 nn_Szepektor_al. nn_Szepektor_et num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et dep_Barzilay_Szepektor conj_and_Barzilay_Shinyama conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_words_Shinyama dep_words_2003 dep_words_Lee dep_words_Barzilay nn_words_anchor nn_tel_Pan57 num_Lin_2001 conj_and_Lin_tel conj_and_frames_method appos_frames_words appos_frames_tel appos_frames_Lin nn_frames_case prep_between_similarities_method prep_between_similarities_frames det_similarities_the dobj_used_similarities prep_for_used_instance aux_used_have nsubj_used_attempts amod_attempts_Previous
N06-1058	N03-1003	n	2This can explain why previous attempts to use WordNet for generating sentence-level paraphrases -LRB- Barzilay and Lee 2003 Quirk et al. 2004 -RRB- were unsuccessful	cop_unsuccessful_were nsubj_unsuccessful_attempts advmod_unsuccessful_why num_Quirk_2004 nn_Quirk_al. nn_Quirk_et dep_Barzilay_Quirk num_Barzilay_2003 conj_and_Barzilay_Lee appos_paraphrases_Lee appos_paraphrases_Barzilay amod_paraphrases_sentence-level dobj_generating_paraphrases prepc_for_use_generating dobj_use_WordNet aux_use_to vmod_attempts_use amod_attempts_previous advcl_explain_unsuccessful aux_explain_can nsubj_explain_2This
N06-1058	N03-1003	o	2 Related Work Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing in particular to sentence level paraphrasing -LRB- Barzilay and Lee 2003 Pang et al. 2003 Quirk et al. 2004 -RRB-	num_Quirk_2004 nn_Quirk_al. nn_Quirk_et num_Pang_2003 nn_Pang_al. nn_Pang_et dep_Barzilay_Quirk conj_and_Barzilay_Pang conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_paraphrasing_Pang dep_paraphrasing_2003 dep_paraphrasing_Lee dep_paraphrasing_Barzilay nn_paraphrasing_level nn_paraphrasing_sentence amod_paraphrasing_automatic prep_in_research_paraphrasing prep_to_related_paraphrasing prep_in_related_particular prep_to_related_research advmod_related_closely cop_related_is nsubj_related_work nsubj_related_Paraphrasing poss_work_Our nn_work_Entailment conj_and_Paraphrasing_work nn_Paraphrasing_Automatic nn_Paraphrasing_Work amod_Paraphrasing_Related num_Paraphrasing_2 ccomp_``_related
N09-3008	N03-1003	o	The use of Profile HMMs for multiple sequence alignment also presents applications to the acquisition of mapping dictionaries -LRB- Barzilay and Lee 2002 -RRB- and sentence-level paraphrasing -LRB- Barzilay and Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_paraphrasing_Lee dep_paraphrasing_Barzilay amod_paraphrasing_sentence-level amod_Barzilay_2002 conj_and_Barzilay_Lee conj_and_dictionaries_paraphrasing dep_dictionaries_Lee dep_dictionaries_Barzilay dobj_mapping_paraphrasing dobj_mapping_dictionaries prepc_of_acquisition_mapping det_acquisition_the prep_to_applications_acquisition dobj_presents_applications advmod_presents_also nsubj_presents_use nn_alignment_sequence amod_alignment_multiple nn_HMMs_Profile prep_for_use_alignment prep_of_use_HMMs det_use_The ccomp_``_presents
P04-1077	N03-1003	o	Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee -LRB- 2003 -RRB-	appos_Lee_2003 conj_and_Barzilay_Lee prep_by_shown_Lee prep_by_shown_Barzilay mark_shown_as amod_methods_statistical advcl_using_shown dobj_using_methods xcomp_acquired_using advmod_acquired_automatically auxpass_acquired_be advmod_acquired_also aux_acquired_can nsubjpass_acquired_Paraphrases
P04-2006	N03-1003	o	Barzilay & Lee -LRB- 2003 -RRB- also identify paraphrases in their paraphrased sentence generation system	nn_system_generation nn_system_sentence amod_system_paraphrased poss_system_their prep_in_identify_system dobj_identify_paraphrases advmod_identify_also nsubj_identify_Lee nsubj_identify_Barzilay appos_Barzilay_2003 conj_and_Barzilay_Lee
P05-1074	N03-1003	o	Past work -LRB- Barzilay and McKeown 2001 Barzilay and Lee 2003 Pang et al. 2003 Ibrahim et al. 2003 -RRB- has examined the use of monolingual parallel corpora for paraphrase extraction	nn_extraction_paraphrase amod_corpora_parallel amod_corpora_monolingual prep_for_use_extraction prep_of_use_corpora det_use_the dobj_examined_use aux_examined_has nsubj_examined_work num_Ibrahim_2003 nn_Ibrahim_al. nn_Ibrahim_et num_Pang_2003 nn_Pang_al. nn_Pang_et num_Barzilay_2003 conj_and_Barzilay_Lee dep_Barzilay_Ibrahim conj_and_Barzilay_Pang conj_and_Barzilay_Lee conj_and_Barzilay_Barzilay conj_and_Barzilay_2001 conj_and_Barzilay_McKeown dep_work_Pang dep_work_Barzilay dep_work_2001 dep_work_McKeown dep_work_Barzilay amod_work_Past ccomp_``_examined
P05-1074	N03-1003	o	2 Extracting paraphrases Much previous work on extracting paraphrases -LRB- Barzilay and McKeown 2001 Barzilay and Lee 2003 Pang et al. 2003 -RRB- has focused on finding identifying contexts within aligned monolingual sentences from which divergent text can be extracted and treated as paraphrases	prep_as_treated_paraphrases nsubjpass_treated_text conj_and_extracted_treated auxpass_extracted_be aux_extracted_can nsubjpass_extracted_text prep_from_extracted_which amod_text_divergent rcmod_sentences_treated rcmod_sentences_extracted amod_sentences_monolingual amod_sentences_aligned prep_within_contexts_sentences dobj_identifying_contexts xcomp_finding_identifying prepc_on_focused_finding aux_focused_has nsubj_focused_work num_Pang_2003 nn_Pang_al. nn_Pang_et num_Barzilay_2003 conj_and_Barzilay_Lee dep_Barzilay_Pang conj_and_Barzilay_Lee conj_and_Barzilay_Barzilay conj_and_Barzilay_2001 conj_and_Barzilay_McKeown appos_paraphrases_Barzilay appos_paraphrases_2001 appos_paraphrases_McKeown appos_paraphrases_Barzilay dobj_extracting_paraphrases prepc_on_work_extracting amod_work_previous amod_work_Much nn_work_paraphrases amod_work_Extracting num_work_2 ccomp_``_focused
P06-1034	N03-1003	o	5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corporawith multiple descriptionsof the same events -LRB- Barzilay and McKeown 2001 Barzilay and Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_Barzilay_Lee dep_Barzilay_Barzilay conj_and_Barzilay_2001 conj_and_Barzilay_McKeown dep_events_2001 dep_events_McKeown dep_events_Barzilay amod_events_same det_events_the amod_descriptionsof_multiple nn_descriptionsof_corporawith dep_corpora_events conj_and_corpora_descriptionsof amod_corpora_parallel dobj_using_descriptionsof dobj_using_corpora amod_paraphrasing_automatic prep_of_field_paraphrasing det_field_the xcomp_studied_using prep_in_studied_field advmod_studied_extensively auxpass_studied_been aux_studied_has nsubjpass_studied_Work amod_meaning_same det_meaning_the prep_with_finding_meaning dobj_finding_sentences advmod_finding_Automatically vmod_Work_finding amod_Work_Related num_Work_5 ccomp_``_studied
P06-1114	N03-1003	o	Classi er Training Set Precision Recall F-Measure Linear 10K pairs 0.837 0.774 0.804 Maximum Entropy 10K pairs 0.881 0.851 0.866 Maximum Entropy 450K pairs 0.902 0.944 0.922 Table 4 Performance of Alignment Classi er 3.2 Paraphrase Acquisition Much recent work on automatic paraphrasing -LRB- Barzilay and Lee 2003 -RRB- has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora	amod_corpora_parallel prep_from_information_corpora amod_information_same det_information_the dobj_contain_information nsubj_contain_that rcmod_passages_contain nn_passages_text dobj_identify_passages aux_identify_to amod_techniques_statistical amod_techniques_simple advmod_simple_relatively vmod_used_identify dobj_used_techniques aux_used_has nsubj_used_work amod_Barzilay_2003 conj_and_Barzilay_Lee appos_paraphrasing_Lee appos_paraphrasing_Barzilay amod_paraphrasing_automatic prep_on_work_paraphrasing amod_work_recent amod_work_Much nn_work_Performance nn_Acquisition_Paraphrase num_Acquisition_3.2 nn_Acquisition_er nn_Acquisition_Classi nn_Acquisition_Alignment prep_of_Performance_Acquisition num_Table_4 num_Table_0.922 num_Table_0.944 num_Table_0.902 dep_pairs_Table nn_pairs_450K nn_pairs_Entropy nn_pairs_Maximum dep_pairs_0.866 dep_0.866_0.851 number_0.851_0.881 dep_pairs_pairs nn_pairs_10K nn_pairs_Entropy nn_pairs_Maximum dep_pairs_0.804 dep_0.804_0.774 number_0.774_0.837 dep_pairs_pairs nn_pairs_10K amod_pairs_Linear nn_pairs_F-Measure dobj_Recall_pairs nsubj_Recall_Precision parataxis_Set_used ccomp_Set_Recall nsubj_Set_Training dep_Set_er dep_Set_Classi
P06-1114	N03-1003	o	In order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example extracted sentences were clustered using complete-link clustering using a technique proposed in -LRB- Barzilay and Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_in_Lee dep_in_Barzilay prep_proposed_in vmod_technique_proposed det_technique_a dobj_using_technique amod_clustering_complete-link xcomp_using_using dobj_using_clustering xcomp_clustered_using auxpass_clustered_were nsubjpass_clustered_sentences amod_sentences_extracted det_example_an prep_for_alternations_example amod_alternations_phraselevel prep_as_considered_alternations auxpass_considered_were nsubjpass_considered_paraphrases dobj_considered_that amod_paraphrases_true num_paraphrases_909 advmod_true_only rcmod_likelihood_considered det_likelihood_the parataxis_increase_clustered dobj_increase_likelihood prep_in_increase_order
P06-2027	N03-1003	o	The procedure of substituting named entities with their respective tags previously proved to be useful for various tasks -LRB- Barzilay and Lee 2003 Sudo et al. 2003 Filatova and Prager 2005 -RRB-	dep_Filatova_2005 conj_and_Filatova_Prager num_Sudo_2003 nn_Sudo_al. nn_Sudo_et dep_Barzilay_Prager dep_Barzilay_Filatova conj_and_Barzilay_Sudo conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_tasks_Sudo dep_tasks_2003 dep_tasks_Lee dep_tasks_Barzilay amod_tasks_various prep_for_useful_tasks cop_useful_be aux_useful_to xcomp_proved_useful advmod_proved_previously nsubj_proved_entities amod_tags_respective poss_tags_their prep_with_entities_tags ccomp_named_proved dep_substituting_named prepc_of_procedure_substituting det_procedure_The ccomp_``_procedure
P06-2027	N03-1003	o	Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus -LRB- Collier 1998 Barzilay and Lee 2003 Sudo et al. 2003 -RRB-	num_Sudo_2003 nn_Sudo_al. nn_Sudo_et dep_Barzilay_Sudo num_Barzilay_2003 conj_and_Barzilay_Lee dep_Collier_Lee dep_Collier_Barzilay appos_Collier_1998 dep_corpus_Collier amod_corpus_generalized det_corpus_this prep_on_based_corpus vmod_domain_based det_domain_a prep_for_important_domain cop_important_is nsubj_important_Many det_information_what prep_on_decision_information det_decision_the dobj_make_decision conj_and_instances_make amod_instances_different nn_collapse_modeling nn_collapse_domain dep_approaches_make dep_approaches_instances advmod_approaches_together prep_of_approaches_collapse amod_approaches_current det_approaches_the prep_of_Many_approaches ccomp_``_important
P06-2070	N03-1003	n	If we consider these probabilities as a vector the similarities of two English words can be obtained by computing the dot product of their corresponding vectors .2 The formula is described below similarity -LRB- ei ej -RRB- = Nsummationdisplay k = 1 p -LRB- ei | fk -RRB- p -LRB- ej | fk -RRB- -LRB- 3 -RRB- Paraphrasing methods based on monolingual parallel corpora such as -LRB- Pang et al. 2003 Barzilay and Lee 2003 -RRB- can also be used to compute the similarity ratio of two words but they dont have as rich training resources as the bilingual methods do	nsubj_do_methods mark_do_as amod_methods_bilingual det_methods_the advcl_resources_do nn_resources_training amod_resources_rich prep_as_have_resources ccomp_dont_have nsubj_dont_they num_words_two prep_of_ratio_words nn_ratio_similarity det_ratio_the dobj_compute_ratio aux_compute_to xcomp_used_compute auxpass_used_be advmod_used_also aux_used_can nsubjpass_used_Pang mark_used_as num_Barzilay_2003 conj_and_Barzilay_Lee dep_Pang_Lee dep_Pang_Barzilay amod_Pang_2003 dep_Pang_al. nn_Pang_et mwe_as_such dep_corpora_used amod_corpora_parallel amod_corpora_monolingual conj_but_methods_dont pobj_methods_corpora prepc_based_on_methods_on amod_methods_Paraphrasing dep_3_dont dep_3_methods num_fk_| nn_fk_ej dep_p_3 appos_p_fk nn_p_p dep_fk_| dep_fk_ei appos_p_fk num_p_1 tmod_=_p amod_k_= nn_k_Nsummationdisplay dobj_=_k appos_ei_ej dep_similarity_= dep_similarity_ei advmod_described_below auxpass_described_is nsubjpass_described_formula det_formula_The nn_formula_.2 rcmod_vectors_described amod_vectors_corresponding poss_vectors_their prep_of_product_vectors nn_product_dot det_product_the dobj_computing_product dep_obtained_similarity agent_obtained_computing auxpass_obtained_be aux_obtained_can nsubjpass_obtained_similarities advcl_obtained_consider amod_words_English num_words_two prep_of_similarities_words det_similarities_the det_vector_a det_probabilities_these prep_as_consider_vector dobj_consider_probabilities nsubj_consider_we mark_consider_If
P06-2096	N03-1003	o	Previous work aligns a group of sentences into a compact word lattice -LRB- Barzilay and Lee 2003 -RRB- a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases	dobj_generate_paraphrases nsubj_generate_work amod_texts_comparable conj_or_commonality_variability prep_among_identify_texts dobj_identify_variability dobj_identify_commonality aux_identify_to xcomp_used_identify auxpass_used_be aux_used_can nsubjpass_used_that rcmod_representation_used nn_representation_automaton nn_representation_state amod_representation_finite det_representation_a amod_Barzilay_2003 conj_and_Barzilay_Lee appos_lattice_representation appos_lattice_Lee appos_lattice_Barzilay nn_lattice_word amod_lattice_compact det_lattice_a prep_of_group_sentences det_group_a conj_and_aligns_generate prep_into_aligns_lattice dobj_aligns_group nsubj_aligns_work amod_work_Previous ccomp_``_generate ccomp_``_aligns
P06-2096	N03-1003	o	2 Related work Our work is closest in spirit to the two papers that inspired us -LRB- Barzilay and Lee 2003 -RRB- and -LRB- Pang et al. 2003 -RRB-	amod_Pang_2003 dep_Pang_al. nn_Pang_et dep_Barzilay_2003 conj_and_Barzilay_Lee dobj_inspired_us nsubj_inspired_that rcmod_papers_inspired num_papers_two det_papers_the prep_to_spirit_papers prep_in_closest_spirit cop_closest_is nsubj_closest_work poss_work_Our conj_and_work_Pang dep_work_Lee dep_work_Barzilay rcmod_work_closest amod_work_Related num_work_2
P07-1058	N03-1003	o	2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years ranging from distributional similarity to finding shared contexts -LRB- Lin and Pantel 2001 Ravichandran and Hovy 2002 Shinyama et al. 2002 Barzilay and Lee 2003 Szpektor et al. 2004 Sekine 2005 -RRB-	amod_Sekine_2005 num_Szpektor_2004 nn_Szpektor_al. nn_Szpektor_et num_Barzilay_2003 conj_and_Barzilay_Lee num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et dep_Lin_Sekine conj_and_Lin_Szpektor conj_and_Lin_Lee conj_and_Lin_Barzilay conj_and_Lin_Shinyama num_Lin_2002 conj_and_Lin_Hovy conj_and_Lin_Ravichandran conj_and_Lin_2001 conj_and_Lin_Pantel dep_contexts_Szpektor dep_contexts_Barzilay dep_contexts_Shinyama dep_contexts_Hovy dep_contexts_Ravichandran dep_contexts_2001 dep_contexts_Pantel dep_contexts_Lin amod_contexts_shared dobj_finding_contexts prepc_to_similarity_finding amod_similarity_distributional prep_from_ranging_similarity amod_years_recent xcomp_suggested_ranging prep_in_suggested_years auxpass_suggested_been aux_suggested_have nsubjpass_suggested_Evaluation prep_of_acquisition_rules amod_acquisition_automatic amod_methods_Many nn_methods_Algorithms nn_methods_Acquisition prep_for_Evaluation_acquisition prep_of_Evaluation_methods num_Evaluation_2.2
P07-1058	N03-1003	o	Indeed the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules -LRB- Lin and Pantel 2001 Shinyama et al. 2002 Barzilay and Lee 2003 Pang et al. 2003 Szpektor et al. 2004 Sekine 2005 -RRB-	amod_Sekine_2005 num_Szpektor_2004 nn_Szpektor_al. nn_Szpektor_et num_Pang_2003 nn_Pang_al. nn_Pang_et num_Barzilay_2003 conj_and_Barzilay_Lee num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et dep_Lin_Sekine conj_and_Lin_Szpektor conj_and_Lin_Pang conj_and_Lin_Lee conj_and_Lin_Barzilay conj_and_Lin_Shinyama num_Lin_2001 conj_and_Lin_Pantel dep_rules_Szpektor dep_rules_Pang dep_rules_Barzilay dep_rules_Shinyama dep_rules_Pantel dep_rules_Lin amod_rules_learned det_rules_the prep_of_judgment_rules amod_judgment_human prep_by_is_judgment nsubj_is_approach advmod_is_Indeed nn_algorithms_acquisition nn_algorithms_rule prep_of_quality_algorithms det_quality_the dobj_evaluating_quality prepc_for_approach_evaluating amod_approach_prominent det_approach_the
P07-1058	N03-1003	o	Indeed only few earlier works reported inter-judge agreement level and those that did reported rather low Kappa values such as 0.54 -LRB- Barzilay and Lee 2003 -RRB- and 0.55 0.63 -LRB- Szpektor et al. 2004 -RRB-	amod_Szpektor_2004 dep_Szpektor_al. nn_Szpektor_et number_0.63_0.55 dep_Barzilay_2003 conj_and_Barzilay_Lee dep_0.54_Szpektor conj_and_0.54_0.63 dep_0.54_Lee dep_0.54_Barzilay prep_such_as_values_0.63 prep_such_as_values_0.54 nn_values_Kappa amod_values_low advmod_low_rather dobj_reported_values aux_reported_did nsubj_reported_that rcmod_those_reported nn_level_agreement amod_level_inter-judge conj_and_reported_those dobj_reported_level nsubj_reported_works advmod_reported_Indeed amod_works_earlier amod_works_few advmod_few_only
P08-1077	N03-1003	o	-LRB- 2004 -RRB- and Barzilay and Lee -LRB- 2003 -RRB- used comparable news articles to obtain sentence level paraphrases	nn_level_sentence dobj_obtain_level aux_obtain_to vmod_articles_obtain nn_articles_news amod_articles_comparable amod_articles_used nn_articles_Lee nn_articles_Barzilay appos_Lee_2003 conj_and_Barzilay_Lee dep_2004_paraphrases conj_and_2004_articles
P08-1089	N03-1003	o	Some methods only extract paraphrase patternsusingnewsarticlesoncertaintopics -LRB- Shinyama et al. 2002 Barzilay and Lee 2003 -RRB- while some others need seeds as initial input -LRB- Ravichandran and Hovy 2002 -RRB-	amod_Ravichandran_2002 conj_and_Ravichandran_Hovy dep_input_Hovy dep_input_Ravichandran amod_input_initial prep_as_need_input dobj_need_seeds nsubj_need_others mark_need_while det_others_some num_Barzilay_2003 conj_and_Barzilay_Lee dep_Shinyama_Lee dep_Shinyama_Barzilay appos_Shinyama_2002 dep_Shinyama_al. nn_Shinyama_et nn_patternsusingnewsarticlesoncertaintopics_paraphrase advcl_extract_need dep_extract_Shinyama dobj_extract_patternsusingnewsarticlesoncertaintopics advmod_extract_only nsubj_extract_methods det_methods_Some ccomp_``_extract
P08-1089	N03-1003	o	In paraphrase generation a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. Avarietyofmethodshavebeenproposedonparaphrase patterns extraction -LRB- Lin and Pantel 2001 Ravichandran and Hovy 2002 Shinyama et al. 2002 Barzilay and Lee 2003 Ibrahim et al. 2003 Pang et al. 2003 Szpektor et al. 2004 -RRB-	num_Szpektor_2004 nn_Szpektor_al. nn_Szpektor_et num_Pang_2003 nn_Pang_al. nn_Pang_et num_Ibrahim_2003 nn_Ibrahim_al. nn_Ibrahim_et num_Barzilay_2003 conj_and_Barzilay_Lee num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et num_Ravichandran_2002 conj_and_Ravichandran_Hovy dep_Lin_Szpektor dep_Lin_Pang dep_Lin_Ibrahim dep_Lin_Lee dep_Lin_Barzilay dep_Lin_Shinyama dep_Lin_Hovy dep_Lin_Ravichandran num_Lin_2001 conj_and_Lin_Pantel appos_extraction_Pantel appos_extraction_Lin nn_extraction_patterns nn_extraction_Avarietyofmethodshavebeenproposedonparaphrase nn_extraction_P. prep_of_patterns_extraction nn_patterns_paraphrase det_patterns_the dobj_using_patterns xcomp_rewritten_using auxpass_rewritten_be aux_rewritten_can nsubjpass_rewritten_unit prep_in_rewritten_generation nn_P_pattern det_P_a dobj_matches_P nsubj_matches_that rcmod_unit_matches nn_unit_text det_unit_a nn_generation_paraphrase
P08-1089	N03-1003	o	The preci781 start Palestinian suicide bomberblew himself up in SLOT1 on SLOT2 killing SLOT3 other people and injuring wounding SLOT4 end detroit the * e * a s * e * building buildingin detroit flattened ground levelled to blasted leveled * e * was reduced razed leveled to down rubble into ashes * e * to * e * -LRB- 1 -RRB- -LRB- 2 -RRB- Figure 1 Examples of paraphrase patterns extracted by Barzilay and Lee -LRB- 2003 -RRB- and Pang et al.	nn_al._et nn_al._Pang appos_Lee_2003 conj_and_Barzilay_al. conj_and_Barzilay_Lee agent_extracted_al. agent_extracted_Lee agent_extracted_Barzilay vmod_patterns_extracted nn_patterns_paraphrase prep_of_Examples_patterns num_Figure_1 dep_Figure_2 advmod_Figure_* advmod_Figure_down dep_*_1 dep_*_e dep_*_* dep_*_to conj_*_down_* pobj_down_rubble prep_to_leveled_Figure dep_razed_leveled dobj_reduced_Examples prep_reduced_razed auxpass_reduced_was nsubjpass_reduced_* advmod_reduced_* dep_*_e ccomp_leveled_reduced aux_blasted_to xcomp_levelled_blasted vmod_ground_levelled vmod_flattened_leveled dobj_flattened_ground nsubj_flattened_detroit nsubj_flattened_bomberblew nn_detroit_buildingin dobj_building_detroit dep_building_* dep_building_e dep_building_* vmod_s_building det_s_a dep_s_* det_s_the dep_*_e dep_*_* dobj_detroit_s csubj_detroit_injuring nn_end_SLOT4 amod_end_wounding dobj_injuring_end amod_people_other nn_people_SLOT3 nn_people_killing nn_people_SLOT2 prep_in_up_SLOT1 prep_on_himself_people advmod_himself_up conj_and_bomberblew_detroit dep_bomberblew_himself nn_bomberblew_suicide amod_bomberblew_Palestinian nn_bomberblew_start nn_bomberblew_preci781 det_bomberblew_The
P08-1089	N03-1003	o	Barzilay and Lee -LRB- 2003 -RRB- applied multi-sequence alignment -LRB- MSA -RRB- to parallel news sentences and induced paraphrase patterns for generating new sentences -LRB- Figure 1 -LRB- 1 -RRB- -RRB-	num_Figure_1 num_Figure_1 appos_sentences_Figure amod_sentences_new dobj_generating_sentences nn_patterns_paraphrase prepc_for_induced_generating dobj_induced_patterns nsubj_induced_Barzilay nn_sentences_news dobj_parallel_sentences aux_parallel_to appos_alignment_MSA amod_alignment_multi-sequence conj_and_applied_induced xcomp_applied_parallel dobj_applied_alignment nsubj_applied_Lee nsubj_applied_Barzilay appos_Lee_2003 conj_and_Barzilay_Lee
P08-1116	N03-1003	o	3 Monolingual comparable corpus Similar to the methods in -LRB- Shinyama et al. 2002 Barzilay and Lee 2003 -RRB- we construct a corpus of comparable documents from a large corpus D of news articles	nn_articles_news prep_of_D_articles nn_D_corpus amod_D_large det_D_a amod_documents_comparable prep_of_corpus_documents det_corpus_a prep_from_construct_D dobj_construct_corpus nsubj_construct_we dep_construct_corpus num_Barzilay_2003 conj_and_Barzilay_Lee dep_Shinyama_Lee dep_Shinyama_Barzilay appos_Shinyama_2002 dep_Shinyama_al. nn_Shinyama_et prep_in_methods_Shinyama det_methods_the prep_to_Similar_methods dep_corpus_Similar amod_corpus_comparable amod_corpus_Monolingual num_corpus_3
P08-1116	N03-1003	o	Different news articles reporting on the same event are commonly used as monolingual comparable corpora from which both paraphrase patterns and phrasal paraphrases can be derived -LRB- Shinyama et al. 2002 Barzilay and Lee 2003 Quirk et al. 2004 -RRB-	num_Quirk_2004 nn_Quirk_al. nn_Quirk_et dep_Barzilay_Quirk num_Barzilay_2003 conj_and_Barzilay_Lee dep_Shinyama_Lee dep_Shinyama_Barzilay appos_Shinyama_2002 dep_Shinyama_al. nn_Shinyama_et dep_derived_Shinyama auxpass_derived_be aux_derived_can nsubjpass_derived_paraphrases nsubjpass_derived_patterns prep_from_derived_which amod_paraphrases_phrasal conj_and_patterns_paraphrases nn_patterns_paraphrase preconj_patterns_both rcmod_corpora_derived amod_corpora_comparable amod_corpora_monolingual prep_as_used_corpora advmod_used_commonly auxpass_used_are nsubjpass_used_articles amod_event_same det_event_the prep_on_reporting_event vmod_articles_reporting nn_articles_news amod_articles_Different
P08-1116	N03-1003	o	For example Barzilay and Lee -LRB- 2003 -RRB- applied multiple-sequence alignment -LRB- MSA -RRB- to parallel news sentences and induced paraphrasing patterns for generating new sentences	amod_sentences_new dobj_generating_sentences nn_patterns_paraphrasing prepc_for_induced_generating dobj_induced_patterns nsubj_induced_Barzilay nn_sentences_news dobj_parallel_sentences aux_parallel_to appos_alignment_MSA amod_alignment_multiple-sequence conj_and_applied_induced xcomp_applied_parallel dobj_applied_alignment nsubj_applied_Lee nsubj_applied_Barzilay prep_for_applied_example appos_Lee_2003 conj_and_Barzilay_Lee
P09-1053	N03-1003	o	For natural language engineers the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences -LRB- Barzilay and Lee 2003 -RRB- question answering modules -LRB- Marsi and Krahmer 2005 -RRB- and machine translation -LRB- Callison-Burch et al. 2006 -RRB-	dep_al._2006 nn_al._et amod_al._Callison-Burch dep_translation_al. nn_translation_machine dep_Marsi_2005 conj_and_Marsi_Krahmer dep_modules_Krahmer dep_modules_Marsi nn_modules_answering nn_modules_question dep_Barzilay_2003 conj_and_Barzilay_Lee dep_sentences_Lee dep_sentences_Barzilay prep_between_overlap_sentences nsubj_overlap_semantic ccomp_measure_overlap aux_measure_must nsubj_measure_that rcmod_summarizers_measure amod_summarizers_abstractive prep_like_systems_summarizers nn_systems_management nn_systems_information conj_and_bears_translation conj_and_bears_modules prep_on_bears_systems nsubj_bears_problem prep_for_bears_engineers det_problem_the nn_engineers_language amod_engineers_natural
P09-1094	N03-1003	o	Some researchers then tried to automatically extract paraphrase rules -LRB- Lin and Pantel 2001 Barzilay and Lee 2003 Zhao et al. 2008b -RRB- which facilitates the rule-based PG methods	nn_methods_PG amod_methods_rule-based det_methods_the dobj_facilitates_methods nsubj_facilitates_which amod_2008b_Zhao dep_Zhao_al. nn_Zhao_et num_Barzilay_2003 conj_and_Barzilay_Lee dep_Lin_2008b dep_Lin_Lee dep_Lin_Barzilay num_Lin_2001 conj_and_Lin_Pantel rcmod_rules_facilitates appos_rules_Pantel appos_rules_Lin nn_rules_paraphrase dobj_extract_rules advmod_extract_automatically aux_extract_to xcomp_tried_extract advmod_tried_then nsubj_tried_researchers det_researchers_Some ccomp_``_tried
P09-2063	N03-1003	o	For instance automatic summary can be seen as a particular paraphrasing task -LRB- Barzilay and Lee 2003 -RRB- with the aim of selecting the shortest paraphrase	amod_paraphrase_shortest det_paraphrase_the dobj_selecting_paraphrase prepc_of_aim_selecting det_aim_the amod_Barzilay_2003 conj_and_Barzilay_Lee prep_with_task_aim dep_task_Lee dep_task_Barzilay nn_task_paraphrasing amod_task_particular det_task_a prep_as_seen_task auxpass_seen_be aux_seen_can nsubjpass_seen_summary prep_for_seen_instance amod_summary_automatic
P09-3004	N03-1003	o	In another generation approach Barzilay and Lee -LRB- 2002 2003 -RRB- look for pairs of slotted word lattices that share many common slot fillers the lattices are generated by applying a multiplesequence alignment algorithm to a corpus of multiple news articles about the same events	amod_events_same det_events_the nn_articles_news amod_articles_multiple prep_about_corpus_events prep_of_corpus_articles det_corpus_a nn_algorithm_alignment nn_algorithm_multiplesequence det_algorithm_a prep_to_applying_corpus dobj_applying_algorithm agent_generated_applying auxpass_generated_are nsubjpass_generated_lattices det_lattices_the nn_fillers_slot amod_fillers_common amod_fillers_many dobj_share_fillers nsubj_share_that rcmod_lattices_share nn_lattices_word amod_lattices_slotted prep_of_pairs_lattices parataxis_look_generated prep_for_look_pairs nsubj_look_Lee nsubj_look_Barzilay prep_in_look_approach dep_2002_2003 dep_Lee_2002 conj_and_Barzilay_Lee nn_approach_generation det_approach_another
W03-1602	N03-1003	o	-LRB- Barzilay and McKeown 2001 Shinyama et al. 2002 Barzilay and Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et dep_Barzilay_Lee dep_Barzilay_Barzilay dep_Barzilay_Shinyama dep_Barzilay_2001 conj_and_Barzilay_McKeown dep_''_McKeown dep_''_Barzilay
W03-1605	N03-1003	o	For this reason paraphrase poses a great challenge for many Natural Language Processing -LRB- NLP -RRB- tasks just as ambiguity does notably in text summarization and NL generation -LRB- Barzilay and Lee 2003 Pang et al. 2003 -RRB-	num_Pang_2003 nn_Pang_al. nn_Pang_et dep_Barzilay_Pang conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_generation_2003 dep_generation_Lee dep_generation_Barzilay nn_generation_NL conj_and_summarization_generation nn_summarization_text pobj_in_generation pobj_in_summarization advmod_in_notably ccomp_,_in dep_ambiguity_does prep_as_just_ambiguity nn_tasks_Processing appos_Processing_NLP nn_Processing_Language amod_Processing_Natural amod_Processing_many prep_for_challenge_tasks amod_challenge_great det_challenge_a advmod_poses_just dobj_poses_challenge nsubj_poses_paraphrase prep_for_poses_reason det_reason_this
W03-1608	N03-1003	o	Similar to the work of Barzilay and Lee -LRB- 2003 -RRB- who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event we are currently attempting to solve the data sparseness problem by extending our approach to non-parallel corpora	amod_corpora_non-parallel poss_approach_our prep_to_extending_corpora dobj_extending_approach nn_problem_sparseness nn_problem_data det_problem_the prepc_by_solve_extending dobj_solve_problem aux_solve_to xcomp_attempting_solve advmod_attempting_currently aux_attempting_are nsubj_attempting_we amod_event_same det_event_the prep_about_articles_event nn_articles_newspaper amod_articles_different prep_of_consisting_articles vmod_corpora_consisting amod_corpora_comparable nn_techniques_generation nn_techniques_paraphrase parataxis_applied_attempting prep_to_applied_corpora dobj_applied_techniques aux_applied_have nsubj_applied_who advmod_applied_Similar appos_Lee_2003 conj_and_Barzilay_Lee prep_of_work_Lee prep_of_work_Barzilay det_work_the prep_to_Similar_work
W04-0910	N03-1003	o	Similarly -LRB- Barzilay and Lee 2003 -RRB- and -LRB- Shinyanma et al. 2002 -RRB- learn sentence level paraphrase templates from a corpus of news articles stemming from different news source	nn_source_news amod_source_different prep_from_stemming_source vmod_articles_stemming nn_articles_news prep_of_corpus_articles det_corpus_a nn_templates_paraphrase nn_templates_level nn_templates_sentence prep_from_learn_corpus dobj_learn_templates nsubj_learn_Shinyanma nsubj_learn_Lee nsubj_learn_Barzilay advmod_learn_Similarly amod_Shinyanma_2002 dep_Shinyanma_al. nn_Shinyanma_et conj_and_Barzilay_Shinyanma amod_Barzilay_2003 conj_and_Barzilay_Lee
W05-1210	N03-1003	o	Such transformations are typically denoted as paraphrases in the literature where a wealth of methods for their automatic acquisition were proposed -LRB- Lin and Pantel 2001 Shinyama et al. 2002 Barzilay and Lee 2003 Szpektor et al. 2004 -RRB-	num_Szpektor_2004 nn_Szpektor_al. nn_Szpektor_et num_Barzilay_2003 conj_and_Barzilay_Lee num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et dep_Lin_Szpektor dep_Lin_Lee dep_Lin_Barzilay dep_Lin_Shinyama num_Lin_2001 conj_and_Lin_Pantel dep_proposed_Pantel dep_proposed_Lin auxpass_proposed_were nsubjpass_proposed_wealth advmod_proposed_where amod_acquisition_automatic poss_acquisition_their prep_for_wealth_acquisition prep_of_wealth_methods det_wealth_a rcmod_literature_proposed det_literature_the prep_in_paraphrases_literature prep_as_denoted_paraphrases advmod_denoted_typically auxpass_denoted_are nsubjpass_denoted_transformations amod_transformations_Such
W06-1403	N03-1003	o	Our experience suggests that disjunctive LFs are an important capability especially as one seeks to make grammars reusable across applications and to employ domain-specific sentence-level paraphrases -LRB- Barzilay and Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_paraphrases_Lee dep_paraphrases_Barzilay amod_paraphrases_sentence-level conj_domain-specific_paraphrases dobj_employ_domain-specific aux_employ_to nsubj_employ_LFs prep_across_reusable_applications nsubj_reusable_grammars xcomp_make_reusable aux_make_to xcomp_seeks_make nsubj_seeks_one conj_and_capability_employ prepc_as_capability_seeks advmod_capability_especially amod_capability_important det_capability_an cop_capability_are nsubj_capability_LFs mark_capability_that amod_LFs_disjunctive ccomp_suggests_employ ccomp_suggests_capability nsubj_suggests_experience poss_experience_Our ccomp_``_suggests
W06-1603	N03-1003	o	Barzilay and Lee -LRB- 2003 -RRB- proposed to apply multiple-sequence alignment -LRB- MSA -RRB- for traditional sentence-level PR	amod_PR_sentence-level dep_traditional_PR prep_for_alignment_traditional appos_alignment_MSA amod_alignment_multiple-sequence dobj_apply_alignment aux_apply_to xcomp_proposed_apply appos_Lee_2003 vmod_Barzilay_proposed conj_and_Barzilay_Lee
W07-0716	N03-1003	o	At the sentence level -LRB- Barzilay and Lee 2003 -RRB- employed an unsupervised learning approach to cluster sentences and extract lattice pairs from comparable monolingual corpora	amod_corpora_monolingual amod_corpora_comparable nn_pairs_lattice nn_pairs_extract conj_and_sentences_pairs prep_from_cluster_corpora dobj_cluster_pairs dobj_cluster_sentences aux_cluster_to vmod_approach_cluster nn_approach_learning amod_approach_unsupervised det_approach_an dobj_employed_approach dep_employed_Lee dep_employed_Barzilay prep_at_employed_level dep_Barzilay_2003 conj_and_Barzilay_Lee nn_level_sentence det_level_the
W07-0716	N03-1003	o	Most previous work on paraphrase has focused on high quality rather than coverage -LRB- Barzilay and Lee 2003 Quirk et al. 2004 -RRB- but generating artificial references for MT parameter tuning in our setting has two unique properties compared to other paraphrase applications	nn_applications_paraphrase amod_applications_other pobj_properties_applications prepc_compared_to_properties_to amod_properties_unique num_properties_two dobj_has_properties poss_setting_our prep_in_tuning_setting nn_tuning_parameter nn_tuning_MT prep_for_references_tuning amod_references_artificial dep_generating_has dobj_generating_references nsubj_generating_work num_Quirk_2004 nn_Quirk_al. nn_Quirk_et conj_and_Barzilay_Quirk conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_quality_Quirk dep_quality_2003 dep_quality_Lee dep_quality_Barzilay conj_negcc_quality_coverage amod_quality_high conj_but_focused_generating prep_on_focused_coverage prep_on_focused_quality aux_focused_has nsubj_focused_work prep_on_work_paraphrase amod_work_previous amod_work_Most
W07-0909	N03-1003	o	Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years -LRB- Lin and Pantel 2001 1http / / jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy 2002 Shinyama et al. 2002 Barzilay and Lee 2003 Sudo et al. 2003 Szpektor et al. 2004 Satoshi 2005 -RRB-	amod_Satoshi_2005 num_Szpektor_2004 nn_Szpektor_al. nn_Szpektor_et num_Sudo_2003 nn_Sudo_al. nn_Sudo_et num_Barzilay_2003 conj_and_Barzilay_Lee num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et conj_and_Ravichandran_Hovy num_Ravichandran_67 nn_Ravichandran_jakarta.apache.org/lucene/docs/index.html dep_Lin_Satoshi conj_and_Lin_Szpektor conj_and_Lin_Sudo conj_and_Lin_Lee conj_and_Lin_Barzilay conj_and_Lin_Shinyama amod_Lin_2002 dep_Lin_Hovy dep_Lin_Ravichandran dep_Lin_1http num_Lin_2001 conj_and_Lin_Pantel amod_years_recent dep_explored_Szpektor dep_explored_Sudo dep_explored_Barzilay dep_explored_Shinyama dep_explored_Pantel dep_explored_Lin prep_in_explored_years auxpass_explored_been aux_explored_have nsubjpass_explored_algorithms nn_rules_entailment nn_rules_paraphrases conj_and_paraphrases_entailment dobj_learning_rules advmod_learning_automatically prepc_for_algorithms_learning amod_algorithms_Many det_Web_the dep_Rules_explored prep_from_Rules_Web nn_Rules_Entailment nn_Rules_Learning advmod_Rules_Automatically
W07-1424	N03-1003	o	Most of the reported work on paraphrase generation from arbitrary input sentences uses machine learning techniques trained on sentences that are known or can be inferred to be paraphrases of each other -LRB- Bannard and Callison-Burch 2005 Barzilay and Lee 2003 Barzilay and McKeown 2001 Callison-Burch et al. 2006 Dolan et al. 2004 Ibrahim et al. 2003 Lin and Pantel 2001 Pang et al. 2003 Quirk et al. 2004 Shinyama et al. 2002 -RRB-	num_Shinyama_2002 nn_Shinyama_al. nn_Shinyama_et num_Quirk_2004 nn_Quirk_al. nn_Quirk_et num_Pang_2003 nn_Pang_al. nn_Pang_et num_Lin_2001 conj_and_Lin_Pantel num_Ibrahim_2003 nn_Ibrahim_al. nn_Ibrahim_et num_Dolan_2004 nn_Dolan_al. nn_Dolan_et num_Callison-Burch_2006 nn_Callison-Burch_al. nn_Callison-Burch_et dep_Barzilay_Shinyama conj_and_Barzilay_Quirk conj_and_Barzilay_Pang conj_and_Barzilay_Pantel conj_and_Barzilay_Lin conj_and_Barzilay_Ibrahim conj_and_Barzilay_Dolan conj_and_Barzilay_Callison-Burch conj_and_Barzilay_2001 conj_and_Barzilay_McKeown conj_and_Barzilay_Barzilay conj_and_Barzilay_2003 conj_and_Barzilay_Lee conj_and_Bannard_Quirk conj_and_Bannard_Pang conj_and_Bannard_Lin conj_and_Bannard_Ibrahim conj_and_Bannard_Dolan conj_and_Bannard_Callison-Burch conj_and_Bannard_2001 conj_and_Bannard_McKeown conj_and_Bannard_Barzilay conj_and_Bannard_2003 conj_and_Bannard_Lee conj_and_Bannard_Barzilay conj_and_Bannard_2005 conj_and_Bannard_Callison-Burch dep_other_Barzilay dep_other_2005 dep_other_Callison-Burch dep_other_Bannard amod_each_other prep_of_paraphrases_each cop_paraphrases_be aux_paraphrases_to xcomp_inferred_paraphrases auxpass_inferred_be aux_inferred_can nsubjpass_inferred_that conj_or_known_inferred auxpass_known_are nsubjpass_known_that rcmod_sentences_inferred rcmod_sentences_known prep_on_trained_sentences vmod_techniques_trained nn_techniques_learning nn_techniques_machine dobj_uses_techniques nsubj_uses_Most nn_sentences_input amod_sentences_arbitrary nn_generation_paraphrase prep_on_work_generation amod_work_reported det_work_the prep_from_Most_sentences prep_of_Most_work
W07-1425	N03-1003	o	The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text -LRB- Barzilay and Lee 2003 Brockett and Dolan 2005 -RRB-	num_Brockett_2005 conj_and_Brockett_Dolan dep_Barzilay_Dolan dep_Barzilay_Brockett conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_text_2003 dep_text_Lee dep_text_Barzilay amod_text_parallel prep_of_set_text det_set_a prep_from_derived_set nn_probabilities_translation vmod_templates_derived conj_or_templates_probabilities dobj_using_probabilities dobj_using_templates xcomp_composed_using vmod_alignment_composed nn_alignment_word prep_on_based_alignment vmod_equivalence_based det_equivalence_the dep_estimates_equivalence amod_estimates_third det_estimates_The
W07-1429	N03-1003	o	Second we will discuss the work done by -LRB- Barzilay & Lee 2003 -RRB- who use clustering of paraphrases to induce rewriting rules	dobj_rewriting_rules xcomp_induce_rewriting aux_induce_to prep_of_clustering_paraphrases vmod_use_induce dobj_use_clustering nsubj_use_who rcmod_Barzilay_use amod_Barzilay_2003 conj_and_Barzilay_Lee agent_done_Lee agent_done_Barzilay vmod_work_done det_work_the dobj_discuss_work aux_discuss_will nsubj_discuss_we advmod_discuss_Second
W07-1429	N03-1003	o	2 Related Work Two different approaches have been proposed for Sentence Compression purely statistical methodologies -LRB- Barzilay & Lee 2003 Le Nguyen & Ho 2004 -RRB- and hybrid linguistic/statistic methodologies -LRB- Knight & Marcu 2002 Shinyama et al. 2002 Daelemans et al. 2004 Marsi & Krahmer 2005 Unno et al. 2006 -RRB-	appos_al._2006 nn_al._et nn_al._Unno num_Marsi_2005 conj_and_Marsi_Krahmer dep_al._2004 nn_al._et nn_al._Daelemans dep_al._2002 nn_al._et nn_al._Shinyama dep_Knight_al. conj_and_Knight_Krahmer conj_and_Knight_Marsi conj_and_Knight_al. conj_and_Knight_al. conj_and_Knight_2002 conj_and_Knight_Marcu amod_methodologies_linguistic/statistic nn_methodologies_hybrid num_Nguyen_2004 conj_and_Nguyen_Ho nn_Nguyen_Le conj_and_Barzilay_Ho conj_and_Barzilay_Nguyen conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_methodologies_Marsi dep_methodologies_al. dep_methodologies_al. dep_methodologies_2002 dep_methodologies_Marcu dep_methodologies_Knight conj_and_methodologies_methodologies dep_methodologies_Nguyen dep_methodologies_2003 dep_methodologies_Lee dep_methodologies_Barzilay amod_methodologies_statistical advmod_statistical_purely nn_Compression_Sentence prep_for_proposed_Compression auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches amod_approaches_different num_approaches_Two dep_Work_methodologies dep_Work_methodologies rcmod_Work_proposed amod_Work_Related num_Work_2
W07-1429	N03-1003	n	Experiments by using 4 algorithms and through visualization techniques revealed that clustering is a worthless effort for paraphrase corpora construction contrary to the literature claims -LRB- Barzilay & Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_claims_Lee dep_claims_Barzilay nn_claims_literature det_claims_the nn_construction_corpora nn_construction_paraphrase prep_contrary_to_effort_claims prep_for_effort_construction amod_effort_worthless det_effort_a cop_effort_is nsubj_effort_clustering mark_effort_that ccomp_revealed_effort nsubj_revealed_Experiments nsubj_revealed_Experiments nn_techniques_visualization num_algorithms_4 dobj_using_algorithms prep_through_Experiments_techniques prepc_by_Experiments_using conj_and_Experiments_Experiments
W07-1429	N03-1003	o	As our work is based on the first paradigm we will focus on the works proposed by -LRB- Barzilay & Lee 2003 -RRB- and -LRB- Le Nguyen & Ho 2004 -RRB-	dep_Nguyen_2004 conj_and_Nguyen_Ho nn_Nguyen_Le conj_and_Barzilay_Ho conj_and_Barzilay_Nguyen dep_Barzilay_2003 conj_and_Barzilay_Lee agent_proposed_Nguyen agent_proposed_Lee agent_proposed_Barzilay vmod_works_proposed det_works_the prep_on_focus_works aux_focus_will nsubj_focus_we advcl_focus_based amod_paradigm_first det_paradigm_the prep_on_based_paradigm auxpass_based_is nsubjpass_based_work mark_based_As poss_work_our
W07-1429	N03-1003	o	-LRB- Barzilay & Lee 2003 -RRB- present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 learn generate sentence-level paraphrases essentially from unannotated corpus data alone	advmod_data_alone nn_data_corpus amod_data_unannotated prep_from_paraphrases_data advmod_paraphrases_essentially nsubj_paraphrases_sentence-level ccomp_generate_paraphrases ccomp_learn_generate amod_alignment_multiple-sequence prep_to_uses_177 dobj_uses_alignment nsubj_uses_that rcmod_algorithm_uses amod_algorithm_knowledge-lean det_algorithm_a vmod_present_learn dobj_present_algorithm nsubj_present_Lee nsubj_present_Barzilay amod_Barzilay_2003 conj_and_Barzilay_Lee
W07-1429	N03-1003	o	Comparatively -LRB- Barzilay & Lee 2003 -RRB- propose to use the N-gram Overlap metric to capture similarities between sentences and automatically create paraphrase corpora	nn_corpora_paraphrase dobj_create_corpora advmod_create_automatically prep_between_similarities_sentences dobj_capture_similarities aux_capture_to xcomp_metric_capture amod_Overlap_metric conj_and_N-gram_create dep_N-gram_Overlap dep_the_create dep_the_N-gram dobj_use_the aux_use_to xcomp_propose_use nsubj_propose_Lee nsubj_propose_Barzilay advmod_propose_Comparatively amod_Barzilay_2003 conj_and_Barzilay_Lee
W07-1429	N03-1003	o	Unlike -LRB- Le Nguyen & Ho 2004 -RRB- one interesting idea proposed by -LRB- Barzilay & Lee 2003 -RRB- is to cluster similar pairs of paraphrases to apply multiplesequence alignment	nn_alignment_multiplesequence dobj_apply_alignment aux_apply_to prep_of_pairs_paraphrases amod_pairs_similar vmod_cluster_apply dobj_cluster_pairs aux_cluster_to aux_cluster_is nsubj_cluster_Ho nsubj_cluster_Nguyen mark_cluster_Unlike amod_Barzilay_2003 conj_and_Barzilay_Lee agent_proposed_Lee agent_proposed_Barzilay vmod_idea_proposed amod_idea_interesting num_idea_one appos_Nguyen_idea dep_Nguyen_2004 conj_and_Nguyen_Ho nn_Nguyen_Le advcl_``_cluster
W07-1429	N03-1003	o	Second we discuss the work done by -LRB- Barzilay & Lee 2003 -RRB- who use clustering of paraphrases to induce rewriting rules	dobj_rewriting_rules xcomp_induce_rewriting aux_induce_to prep_of_clustering_paraphrases vmod_use_induce dobj_use_clustering nsubj_use_who rcmod_Barzilay_use amod_Barzilay_2003 conj_and_Barzilay_Lee agent_done_Lee agent_done_Barzilay vmod_work_done det_work_the dobj_discuss_work nsubj_discuss_we advmod_discuss_Second
W07-1429	N03-1003	o	3.1 Paraphrase Identification A few unsupervised metrics have been applied to automatic paraphrase identification and extraction -LRB- Barzilay & Lee 2003 Dolan & Brockett 2004 -RRB-	appos_Dolan_2004 conj_and_Dolan_Brockett conj_and_Barzilay_Brockett conj_and_Barzilay_Dolan conj_and_Barzilay_2003 conj_and_Barzilay_Lee dep_extraction_Dolan dep_extraction_2003 dep_extraction_Lee dep_extraction_Barzilay conj_and_identification_extraction nn_identification_paraphrase amod_identification_automatic prep_to_applied_extraction prep_to_applied_identification auxpass_applied_been aux_applied_have nsubjpass_applied_metrics amod_metrics_unsupervised amod_metrics_few det_metrics_A nn_metrics_Identification nn_metrics_Paraphrase num_metrics_3.1
W07-1429	N03-1003	o	However these unsupervised methodologies show a major drawback by extracting quasi-exact2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of -LRB- Dolan & Brockett 2004 -RRB- and word N-gram overlap for -LRB- Barzilay & Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_for_Lee dep_for_Barzilay prep_overlap_for nsubj_overlap_N-gram nsubj_overlap_Brockett nsubj_overlap_Dolan nn_N-gram_word conj_and_Dolan_N-gram dep_Dolan_2004 conj_and_Dolan_Brockett prepc_of_case_overlap det_case_the prep_in_Distance_case nn_Distance_Edit det_Distance_the prep_such_as_measures_Distance nn_measures_similarity nn_measures_string amod_measures_classical prep_on_rely_measures nsubj_rely_they mark_rely_as advcl_pairs_rely prep_of_pairs_sentences nn_pairs_match amod_pairs_exact advmod_pairs_even conj_or_extracting_pairs dobj_extracting_quasi-exact2 prepc_by_drawback_pairs prepc_by_drawback_extracting amod_drawback_major det_drawback_a dobj_show_drawback nsubj_show_methodologies advmod_show_However amod_methodologies_unsupervised det_methodologies_these
W07-1429	N03-1003	o	In particular it shows systematically better F-Measure and Accuracy measures over all other metrics showing an improvement of -LRB- 1 -RRB- at least 2.86 % in terms of F-Measure and 3.96 % in terms of Accuracy and -LRB- 2 -RRB- at most 6.61 % in terms of FMeasure and 6.74 % in terms of Accuracy compared to the second best metric which is also systematically the word N-gram overlap similarity measure used by -LRB- Barzilay & Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_by_Lee dep_by_Barzilay prep_used_by vmod_measure_used nn_measure_similarity dobj_overlap_measure nn_N-gram_word det_N-gram_the advmod_N-gram_systematically advmod_N-gram_also cop_N-gram_is nsubj_N-gram_which rcmod_metric_N-gram amod_metric_best amod_metric_second det_metric_the prep_of_terms_Accuracy num_%_6.74 conj_and_FMeasure_% prep_of_terms_% prep_of_terms_FMeasure num_%_6.61 amod_%_most dep_at_overlap pobj_at_metric prepc_compared_to_at_to prep_in_at_terms prep_in_at_terms pobj_at_% dep_2_at prep_of_terms_Accuracy num_%_3.96 conj_and_F-Measure_% prep_of_terms_% prep_of_terms_F-Measure conj_and_%_2 prep_in_%_terms prep_in_%_terms num_%_2.86 quantmod_2.86_at mwe_at_least dep_1_2 dep_1_% prep_of_improvement_1 det_improvement_an dobj_showing_improvement vmod_metrics_showing amod_metrics_other det_metrics_all nn_measures_Accuracy prep_over_F-Measure_metrics conj_and_F-Measure_measures amod_F-Measure_better advmod_better_systematically dobj_shows_measures dobj_shows_F-Measure nsubj_shows_it prep_in_shows_particular
W07-1429	N03-1003	o	On one hand as -LRB- Barzilay & Lee 2003 -RRB- evidence clusters of paraphrases can lead to better learning of text-totext rewriting rules compared to just pairs of paraphrases	prep_of_pairs_paraphrases advmod_pairs_just amod_rules_rewriting amod_rules_text-totext pobj_learning_pairs prepc_compared_to_learning_to prep_of_learning_rules amod_learning_better prep_to_lead_learning aux_lead_can nsubj_lead_clusters advcl_lead_evidence prep_on_lead_hand prep_of_clusters_paraphrases dep_evidence_Lee dep_evidence_Barzilay mark_evidence_as dep_Barzilay_2003 conj_and_Barzilay_Lee num_hand_one
W07-1429	N03-1003	o	However as -LRB- Barzilay & Lee 2003 -RRB- do not propose any evaluation of which clustering algorithm should be used we experiment a set of clustering algorithms and present the comparative results	amod_results_comparative det_results_the dobj_present_results nn_algorithms_clustering conj_and_set_present prep_of_set_algorithms det_set_a dep_experiment_present dep_experiment_set nsubj_experiment_we advcl_experiment_propose advmod_experiment_However auxpass_used_be aux_used_should nsubjpass_used_algorithm prep_of_used_which nn_algorithm_clustering rcmod_evaluation_used det_evaluation_any dobj_propose_evaluation neg_propose_not aux_propose_do nsubj_propose_Lee nsubj_propose_Barzilay mark_propose_as dep_Barzilay_2003 conj_and_Barzilay_Lee
W07-1429	N03-1003	n	Table 2 Figures about clustering algorithms Algorithm # Sentences / # Clusters S-HAC 6,23 C-HAC 2,17 QT 2,32 EM 4,16 In fact table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by -LRB- Barzilay & Lee 2003 -RRB- who only keep the clusters that contain more than 10 sentences	num_sentences_10 quantmod_10_than mwe_than_more dobj_contain_sentences nsubj_contain_that rcmod_clusters_contain det_clusters_the dobj_keep_clusters advmod_keep_only nsubj_keep_who rcmod_Barzilay_keep amod_Barzilay_2003 conj_and_Barzilay_Lee agent_presented_Lee agent_presented_Barzilay vmod_results_presented det_results_the dobj_question_results aux_question_to xcomp_leads_question nsubj_leads_which rcmod_sentences_leads num_sentences_6 quantmod_6_than mwe_than_less dobj_have_sentences nsubj_have_most mark_have_that det_clusters_the prep_of_most_clusters ccomp_shows_have nsubj_shows_S-HAC dep_shows_Clusters dep_shows_# num_table_2 num_EM_4,16 num_EM_2,32 nn_EM_QT num_EM_2,17 prep_in_C-HAC_fact dep_C-HAC_EM dep_6,23_C-HAC appos_S-HAC_table dep_S-HAC_6,23 dep_Sentences_shows dep_Sentences_# nn_Sentences_Algorithm nn_Sentences_algorithms nn_Sentences_clustering prep_about_Figures_Sentences dep_Table_Figures num_Table_2
W07-1429	N03-1003	o	Sentence Compression takes an important place for Natural Language Processing -LRB- NLP -RRB- tasks where specific constraints must be satisfied such as length in summarization -LRB- Barzilay & Lee 2002 Knight & Marcu 2002 Shinyama et al. 2002 Barzilay & Lee 2003 Le Nguyen & Ho 2004 Unno et al. 2006 -RRB- style in text simplification -LRB- Marsi & Krahmer 2005 -RRB- or sentence simplification for subtitling -LRB- Daelemans et al. 2004 -RRB-	amod_Daelemans_2004 dep_Daelemans_al. nn_Daelemans_et dep_subtitling_Daelemans nn_simplification_sentence dep_Marsi_2005 conj_and_Marsi_Krahmer nn_simplification_text dep_style_Krahmer dep_style_Marsi prep_in_style_simplification dep_style_Nguyen dep_style_Barzilay dep_style_al. dep_style_2002 dep_style_Marcu dep_style_Knight dep_al._2006 nn_al._et nn_al._Unno num_Nguyen_2004 conj_and_Nguyen_Ho nn_Nguyen_Le num_Barzilay_2003 conj_and_Barzilay_Lee dep_al._2002 nn_al._et nn_al._Shinyama dep_Knight_al. conj_and_Knight_Ho conj_and_Knight_Nguyen conj_and_Knight_Lee conj_and_Knight_Barzilay conj_and_Knight_al. conj_and_Knight_2002 conj_and_Knight_Marcu prepc_for_Barzilay_subtitling conj_or_Barzilay_simplification conj_and_Barzilay_style conj_and_Barzilay_2002 conj_and_Barzilay_Lee dep_summarization_simplification dep_summarization_style dep_summarization_2002 dep_summarization_Lee dep_summarization_Barzilay prep_in_length_summarization cop_satisfied_be aux_satisfied_must nsubj_satisfied_constraints advmod_satisfied_where amod_constraints_specific prep_such_as_tasks_length rcmod_tasks_satisfied nn_tasks_Processing appos_Processing_NLP nn_Processing_Language amod_Processing_Natural amod_place_important det_place_an prep_for_takes_tasks dobj_takes_place nsubj_takes_Compression nn_Compression_Sentence
W07-1429	N03-1003	o	These results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction contrarily to what -LRB- Barzilay & Lee 2003 -RRB- suggest	nsubj_suggest_Lee nsubj_suggest_Barzilay dobj_suggest_what amod_Barzilay_2003 conj_and_Barzilay_Lee prepc_to_contrarily_suggest nn_construction_corpora nn_construction_paraphrase amod_construction_automatic prep_for_effort_construction amod_effort_worthless det_effort_a cop_effort_is nsubj_effort_clustering dobj_effort_that rcmod_sight_effort det_sight_the dobj_reinforce_sight amod_subsection_previous det_subsection_the advmod_figures_contrarily conj_and_figures_reinforce prep_in_figures_subsection amod_figures_observed det_figures_the dobj_confirm_reinforce dobj_confirm_figures nsubj_confirm_results det_results_These
W08-0906	N03-1003	o	In order to be able to compare the edit distance with the other metrics we have used the following formula -LRB- Wen et al. 2002 -RRB- whichnormalisesthe minimum edit distance by the length of the longest questionand transformsit into a similaritymetric normalisededitdistance = 1 edit dist -LRB- q1 q2 -RRB- max -LRB- | q 1 | | q2 | -RRB- Word Ngram Overlap This metric compares the word n-gramsin both questions ngramoverlap = 1N Nsummationdisplay n = 1 | Gn -LRB- q1 -RRB- Gn -LRB- q2 -RRB- | min -LRB- | Gn -LRB- q1 -RRB- | | Gn -LRB- q2 -RRB- | -RRB- where Gn -LRB- q -RRB- is the set of n-grams of length n in question q and N usually equals 4 -LRB- Barzilay and Lee 2003 Cordeiroet al. 2007 -RRB-	nn_al._Cordeiroet amod_Barzilay_2007 dep_Barzilay_al. num_Barzilay_2003 conj_and_Barzilay_Lee dep_4_Lee dep_4_Barzilay dobj_equals_4 advmod_equals_usually nsubj_equals_ngramoverlap conj_and_q_N dep_question_N dep_question_q nn_n_length prep_of_n-grams_n prep_in_set_question prep_of_set_n-grams det_set_the cop_set_is nsubj_set_Gn advmod_set_where appos_Gn_q nn_|_Gn appos_Gn_q2 num_Gn_| appos_Gn_| num_Gn_| appos_Gn_q1 num_Gn_| rcmod_min_set dep_min_Gn num_min_| nn_min_Gn nn_min_Gn appos_Gn_q2 appos_Gn_q1 num_Gn_| number_|_1 dobj_=_min dep_n_= nn_n_Nsummationdisplay nn_n_1N dep_=_n amod_ngramoverlap_= dep_questions_equals preconj_questions_both nn_n-gramsin_word det_n-gramsin_the dobj_compares_questions dobj_compares_n-gramsin nsubj_compares_metric det_metric_This rcmod_Overlap_compares dep_Ngram_Overlap dep_Word_Ngram nn_|_q2 num_|_| number_|_1 appos_q_| num_q_| num_q_| appos_q1_q2 dep_dist_q1 nn_dist_edit num_dist_1 dep_=_q dep_=_max dep_=_dist nsubj_=_normalisededitdistance dep_=_similaritymetric dep_=_a nn_transformsit_questionand amod_transformsit_longest det_transformsit_the prep_of_length_transformsit det_length_the dobj_edit_distance dep_minimum_edit nn_minimum_whichnormalisesthe dep_Wen_2002 dep_Wen_al. nn_Wen_et dep_formula_minimum dep_formula_Wen amod_formula_following det_formula_the prep_into_used_Word dep_used_= prep_by_used_length dobj_used_formula aux_used_have nsubj_used_we advcl_used_able amod_metrics_other det_metrics_the nn_distance_edit det_distance_the prep_with_compare_metrics dobj_compare_distance aux_compare_to xcomp_able_compare cop_able_be aux_able_to dep_able_order mark_able_In
W08-0906	N03-1003	o	While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy sentencelevel paraphrasingis moredifficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence -LRB- Barzilay and Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_sentence_Lee dep_sentence_Barzilay det_sentence_the prep_of_structure_sentence det_structure_the prep_in_changes_structure dobj_entail_changes aux_entail_might nsubj_entail_it mark_entail_since amod_substitution_phrase-by-phrase amod_substitution_word-for-word conj_or_word-for-word_phrase-by-phrase prep_with_equated_substitution auxpass_equated_be neg_equated_not aux_equated_can advcl_grasp_entail conj_and_grasp_equated aux_grasp_to vmod_moredifficult_equated vmod_moredifficult_grasp nn_moredifficult_paraphrasingis nn_moredifficult_sentencelevel appos_synonymy_moredifficult prep_of_notion_synonymy amod_notion_well-studied det_notion_the prep_to_assimilated_notion auxpass_assimilated_be aux_assimilated_can nsubjpass_assimilated_phrasal nsubjpass_assimilated_word mark_assimilated_While dep_word_paraphrases conj_and_word_phrasal advcl_``_assimilated
W08-0906	N03-1003	o	There exist many different string similarity measures word overlap -LRB- Tomuro and Lytinen 2004 -RRB- longest common subsequence -LRB- Islamand Inkpen ,2007 -RRB- Levenshteinedit distance -LRB- Dolan et al. 2004 -RRB- word n-gramoverlap -LRB- Barzilay and Lee 2003 -RRB- etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words containedin the sentencesbeing compared	vmod_sentencesbeing_compared det_sentencesbeing_the dobj_containedin_sentencesbeing det_words_the prep_of_similarity_words amod_similarity_semantic det_similarity_the dobj_computing_similarity vmod_first_computing dep_obtained_containedin agent_obtained_first auxpass_obtained_are nsubjpass_obtained_measures nn_measures_similarity amod_measures_Semantic dep_Barzilay_2003 conj_and_Barzilay_Lee dep_n-gramoverlap_etc. dep_n-gramoverlap_Lee dep_n-gramoverlap_Barzilay nn_n-gramoverlap_word amod_Dolan_2004 dep_Dolan_al. nn_Dolan_et appos_distance_Dolan nn_distance_Levenshteinedit amod_,2007_Inkpen num_Islamand_,2007 rcmod_subsequence_obtained conj_subsequence_n-gramoverlap conj_subsequence_distance appos_subsequence_Islamand amod_subsequence_common amod_subsequence_longest dep_Tomuro_2004 conj_and_Tomuro_Lytinen dobj_overlap_subsequence dep_overlap_Lytinen dep_overlap_Tomuro nsubj_overlap_word dep_measures_overlap nn_measures_similarity nn_measures_string amod_measures_different amod_measures_many dobj_exist_measures expl_exist_There
W08-1911	N03-1003	o	Barzilay and Lee -LRB- Barzilay and Lee 2003 -RRB- learned paraphrasing patterns as pairs of word lattices which are then used to produce sentence level paraphrases	nn_level_sentence dobj_produce_level aux_produce_to dep_used_paraphrases xcomp_used_produce advmod_used_then auxpass_used_are nsubjpass_used_which rcmod_lattices_used nn_lattices_word prep_of_pairs_lattices amod_patterns_paraphrasing prep_as_learned_pairs dobj_learned_patterns nsubj_learned_Lee nsubj_learned_Barzilay dep_Barzilay_2003 conj_and_Barzilay_Lee dep_Barzilay_Lee dep_Barzilay_Barzilay conj_and_Barzilay_Lee
W09-0604	N03-1003	o	3.4 Perspectives for automatic paraphrase extraction There is a growing amount of work on automatic extraction of paraphrases from text corpora -LRB- Lin and Pantel 2001 Barzilay and Lee 2003 Ibrahim et al. 2003 Dolan et al. 2004 -RRB-	num_Dolan_2004 nn_Dolan_al. nn_Dolan_et num_Ibrahim_2003 nn_Ibrahim_al. nn_Ibrahim_et num_Barzilay_2003 conj_and_Barzilay_Lee dep_Lin_Dolan dep_Lin_Ibrahim dep_Lin_Lee dep_Lin_Barzilay num_Lin_2001 conj_and_Lin_Pantel appos_corpora_Pantel appos_corpora_Lin nn_corpora_text prep_from_extraction_corpora prep_of_extraction_paraphrases amod_extraction_automatic prep_on_amount_extraction prep_of_amount_work amod_amount_growing det_amount_a nsubj_is_amount expl_is_There nn_extraction_paraphrase amod_extraction_automatic dep_Perspectives_is prep_for_Perspectives_extraction num_Perspectives_3.4 dep_``_Perspectives
W09-2805	N03-1003	o	A few unsupervised metrics have been applied to automatic paraphrase identification and extraction -LRB- Barzilay & Lee 2003 Dolan et al. 2004 -RRB-	num_Dolan_2004 nn_Dolan_al. nn_Dolan_et dep_Barzilay_Dolan appos_Barzilay_2003 conj_and_Barzilay_Lee dep_extraction_Lee dep_extraction_Barzilay conj_and_identification_extraction nn_identification_paraphrase amod_identification_automatic prep_to_applied_extraction prep_to_applied_identification auxpass_applied_been aux_applied_have nsubjpass_applied_metrics amod_metrics_unsupervised amod_metrics_few det_metrics_A
W09-2805	N03-1003	o	However these unsupervised methodologies show a major drawback by extracting quasi-exact or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of -LRB- Dolan et al. 2004 -RRB- and Word N-gram Overlap for -LRB- Barzilay & Lee 2003 -RRB-	amod_Barzilay_2003 conj_and_Barzilay_Lee dep_for_Lee dep_for_Barzilay prep_Overlap_for nn_Overlap_N-gram nn_Overlap_Word conj_and_Dolan_Overlap amod_Dolan_2004 dep_Dolan_al. nn_Dolan_et prep_of_case_Overlap prep_of_case_Dolan det_case_the prep_in_Distance_case nn_Distance_Edit det_Distance_the prep_such_as_measures_Distance nn_measures_similarity nn_measures_string amod_measures_classical prep_on_rely_measures nsubj_rely_they mark_rely_as prep_of_pairs_sentences nn_pairs_match amod_pairs_exact amod_pairs_quasi-exact advmod_exact_even conj_or_quasi-exact_exact advcl_extracting_rely dobj_extracting_pairs prepc_by_drawback_extracting amod_drawback_major det_drawback_a dobj_show_drawback nsubj_show_methodologies advmod_show_However amod_methodologies_unsupervised det_methodologies_these
C04-1090	N03-1017	o	However -LRB- Koehn et al 2003 -RRB- found that it is actually harmful to restrict phrases to constituents in parse trees because the restriction would cause the system to miss many reliable translations such as the correspondence between there is in English and es gibt -LRB- it gives -RRB- in German	nsubj_gives_it prep_in_es_German dep_es_gives dobj_es_gibt conj_and_is_es prep_in_is_English expl_is_there mark_is_between ccomp_correspondence_es ccomp_correspondence_is det_correspondence_the prep_such_as_translations_correspondence amod_translations_reliable amod_translations_many dobj_miss_translations aux_miss_to det_system_the vmod_cause_miss dobj_cause_system aux_cause_would nsubj_cause_restriction mark_cause_because det_restriction_the nn_trees_parse prep_in_constituents_trees prep_to_restrict_constituents dobj_restrict_phrases aux_restrict_to advcl_harmful_cause xcomp_harmful_restrict advmod_harmful_actually cop_harmful_is nsubj_harmful_it mark_harmful_that ccomp_found_harmful nsubj_found_Koehn advmod_found_However advmod_2003_al nn_al_et num_Koehn_2003
C08-1005	N03-1017	o	grow-diagfinal -LRB- Koehn et al. 2003 -RRB- -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diagfinal_Koehn dep_``_grow-diagfinal
C08-1014	N03-1017	o	Our MT baseline system is based on Moses decoder -LRB- Koehn et al. 2007 -RRB- with word alignment obtained from GIZA + + -LRB- Och et al. 2003 -RRB-	amod_Och_2003 dep_Och_al. nn_Och_et dep_GIZA_Och conj_+_GIZA_+ prep_from_obtained_+ prep_from_obtained_GIZA vmod_alignment_obtained nn_alignment_word amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_decoder_Koehn nn_decoder_Moses prep_with_based_alignment prep_on_based_decoder auxpass_based_is nsubjpass_based_system nn_system_baseline nn_system_MT poss_system_Our
C08-1014	N03-1017	p	1 Introduction State-of-the-art Statistical Machine Translation -LRB- SMT -RRB- systems usually adopt a two-pass search strategy -LRB- Och 2003 Koehn et al. 2003 -RRB- as shown in Figure 1	num_Figure_1 prep_in_shown_Figure mark_shown_as nn_al._et amod_Koehn_2003 dep_Koehn_al. dep_Och_Koehn appos_Och_2003 dep_strategy_Och nn_strategy_search amod_strategy_two-pass det_strategy_a advcl_adopt_shown dobj_adopt_strategy advmod_adopt_usually nsubj_adopt_systems nn_systems_Translation appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical amod_Translation_State-of-the-art nn_Translation_Introduction num_Translation_1
C08-1017	N03-1017	o	However Moores Law the driving force of change in computing since then has opened the way for recent progress in the field such as Statistical Machine Translation -LRB- SMT -RRB- -LRB- Koehn et al. 2003 -RRB-	advmod_2003_al. nn_al._et num_Koehn_2003 appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical det_field_the prep_in_progress_field amod_progress_recent dep_way_Koehn prep_such_as_way_Translation prep_for_way_progress det_way_the dobj_opened_way aux_opened_has nsubj_opened_Law advmod_opened_However pobj_since_then prep_force_since prep_in_force_computing prep_of_force_change amod_force_driving det_force_the appos_Law_force nn_Law_Moores
C08-1027	N03-1017	p	1 Introduction The emergence of phrase-based statistical machine translation -LRB- PSMT -RRB- -LRB- Koehn et al. 2003 -RRB- has been one of the major developments in statistical approaches to translation	prep_to_approaches_translation amod_approaches_statistical prep_in_developments_approaches amod_developments_major det_developments_the prep_of_one_developments cop_one_been aux_one_has nsubj_one_emergence amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_translation_PSMT nn_translation_machine amod_translation_statistical amod_translation_phrase-based dep_emergence_Koehn prep_of_emergence_translation det_emergence_The rcmod_Introduction_one num_Introduction_1
C08-1041	N03-1017	o	Then the word alignment is refined by performing growdiag-final method -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn amod_method_growdiag-final amod_method_performing agent_refined_method auxpass_refined_is nsubjpass_refined_alignment advmod_refined_Then nn_alignment_word det_alignment_the
C08-1064	N03-1017	o	Except where noted each system was trained on 27 million words of newswire data aligned with GIZA + + -LRB- Och and Ney 2003 -RRB- and symmetrized with the grow-diag-final-and heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_grow-diag-final-and det_heuristic_the prep_with_symmetrized_heuristic amod_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ conj_and_aligned_symmetrized prep_with_aligned_+ prep_with_aligned_GIZA amod_data_newswire vmod_words_symmetrized vmod_words_aligned prep_of_words_data num_words_million number_million_27 prep_on_trained_words auxpass_trained_was nsubjpass_trained_system advcl_trained_noted mark_trained_Except det_system_each advmod_noted_where advcl_``_trained
C08-1064	N03-1017	o	Sum of logarithms of source-to-target lexical weighting -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_weighting_lexical amod_weighting_source-to-target prep_of_logarithms_weighting dep_Sum_Koehn prep_of_Sum_logarithms
C08-1064	N03-1017	o	4.3 Relaxing Length Restrictions Increasing the maximum phrase length in standard phrase-based translation does not improve BLEU -LRB- Koehn et al. 2003 Zens and Ney 2007 -RRB-	dep_Zens_2007 conj_and_Zens_Ney dep_Koehn_Ney dep_Koehn_Zens appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_improve_Koehn dobj_improve_BLEU neg_improve_not aux_improve_does nsubj_improve_Restrictions amod_translation_phrase-based amod_translation_standard prep_in_length_translation nn_length_phrase nn_length_maximum det_length_the dobj_Increasing_length vmod_Restrictions_Increasing nn_Restrictions_Length amod_Restrictions_Relaxing num_Restrictions_4.3 ccomp_``_improve
C08-1064	N03-1017	o	Our results are similar to those for conventional phrase-based models -LRB- Koehn et al. 2003 Zens and Ney 2007 -RRB-	dep_Zens_2007 conj_and_Zens_Ney dep_Koehn_Ney dep_Koehn_Zens appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_models_phrase-based amod_models_conventional prep_for_those_models dep_similar_Koehn prep_to_similar_those cop_similar_are nsubj_similar_results poss_results_Our
C08-1064	N03-1017	o	It compares favorably 505 with conventional phrase-based translation -LRB- Koehn et al. 2003 -RRB- on Chinese-English news translation -LRB- Chiang 2007 -RRB-	dep_Chiang_2007 appos_translation_Chiang nn_translation_news amod_translation_Chinese-English dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_translation_phrase-based amod_translation_conventional dep_505_Koehn prep_with_505_translation advmod_505_favorably prep_on_compares_translation dobj_compares_505 nsubj_compares_It
C08-1064	N03-1017	o	Our baseline uses Giza + + alignments -LRB- Och and Ney 2003 -RRB- symmetrized with the grow-diag-final-and heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_grow-diag-final-and det_heuristic_the prep_with_symmetrized_heuristic dep_Och_2003 conj_and_Och_Ney conj_+_Giza_alignments vmod_uses_symmetrized dep_uses_Ney dep_uses_Och dobj_uses_alignments dobj_uses_Giza nsubj_uses_baseline poss_baseline_Our ccomp_``_uses
C08-1127	N03-1017	n	With these linguistic annotations we expect the LABTG to address two traditional issues of standard phrase-based SMT -LRB- Koehn et al. 2003 -RRB- in a more effective manner	amod_manner_effective det_manner_a advmod_effective_more amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_SMT_Koehn amod_SMT_phrase-based amod_SMT_standard prep_of_issues_SMT amod_issues_traditional num_issues_two prep_in_address_manner dobj_address_issues aux_address_to det_LABTG_the xcomp_expect_address dobj_expect_LABTG nsubj_expect_we prep_with_expect_annotations amod_annotations_linguistic det_annotations_these
C08-1127	N03-1017	o	2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems either from the target side -LRB- Marcu et al. 2006 Hassan et al. 2007 Zollmann and Venugopal 2006 -RRB- the source side -LRB- Quirk et al. 2005 Liu et al. 2006 Huang et al. 2006 -RRB- or both sides -LRB- Eisner 2003 Ding et al. 2005 Koehn and Hoang 2007 -RRB- just to name a few	det_few_a dobj_name_few aux_name_to advmod_name_just dep_Koehn_2007 conj_and_Koehn_Hoang num_Ding_2005 nn_Ding_al. nn_Ding_et dep_Eisner_Hoang dep_Eisner_Koehn dep_Eisner_Ding dep_Eisner_2003 det_sides_both num_Huang_2006 nn_Huang_al. nn_Huang_et num_Liu_2006 nn_Liu_al. nn_Liu_et dep_Quirk_Huang conj_Quirk_Liu appos_Quirk_2005 dep_Quirk_al. nn_Quirk_et dep_side_Quirk nn_side_source det_side_the dep_Zollmann_2006 conj_and_Zollmann_Venugopal vmod_Hassan_name appos_Hassan_Eisner conj_or_Hassan_sides conj_or_Hassan_side dep_Hassan_Venugopal dep_Hassan_Zollmann num_Hassan_2007 nn_Hassan_al. nn_Hassan_et dep_Marcu_sides dep_Marcu_side dep_Marcu_Hassan appos_Marcu_2006 dep_Marcu_al. nn_Marcu_et nn_side_target det_side_the nn_systems_SMT amod_knowledge_linguistic prep_from_integrate_side preconj_integrate_either prep_into_integrate_systems dobj_integrate_knowledge aux_integrate_to dep_efforts_Marcu vmod_efforts_integrate amod_efforts_various cop_efforts_been aux_efforts_have expl_efforts_There rcmod_Work_efforts amod_Work_Related num_Work_2
C08-1127	N03-1017	o	Firstly we run GIZA + + -LRB- Och and Ney 2000 -RRB- on the training corpus in both directions and then apply the ogrow-diag-finalprefinement rule -LRB- Koehn et al. 2003 -RRB- to obtain many-to-many word alignments	nn_alignments_word amod_alignments_many-to-many dobj_obtain_alignments aux_obtain_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_rule_Koehn nn_rule_ogrow-diag-finalprefinement det_rule_the vmod_apply_obtain dobj_apply_rule advmod_apply_then nsubj_apply_we preconj_directions_both nn_corpus_training det_corpus_the num_Och_2000 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ conj_and_run_apply prep_in_run_directions dobj_run_+ dobj_run_GIZA nsubj_run_we advmod_run_Firstly
C08-1138	N03-1017	o	Based on these grammars a great number of SMT models have been recently proposed including string-to-string model -LRB- Synchronous FSG -RRB- -LRB- Brown et al. 1993 Koehn et al. 2003 -RRB- tree-to-string model -LRB- TSG-string -RRB- -LRB- Huang et al. 2006 Liu et al. 2006 Liu et al. 2007 -RRB- string-totree model -LRB- string-CFG/TSG -RRB- -LRB- Yamada and Knight 2001 Galley et al. 2006 Marcu et al. 2006 -RRB- tree-to-tree model -LRB- Synchronous CFG/TSG Data-Oriented Translation -RRB- -LRB- Chiang 2005 Cowan et al. 2006 Eisner 2003 Ding and Palmer 2005 Zhang et al. 2007 Bod 2007 Quirk wt al. 2005 Poutsma 2000 Hearne and Way 2003 -RRB- and so on	advmod_on_so dep_Hearne_2003 conj_and_Hearne_Way num_Poutsma_2000 amod_al._wt nn_al._Quirk num_Bod_2007 num_Zhang_2007 nn_Zhang_al. nn_Zhang_et conj_and_Ding_2005 conj_and_Ding_Palmer num_Eisner_2003 num_Cowan_2006 nn_Cowan_al. nn_Cowan_et dep_Chiang_Way dep_Chiang_Hearne conj_Chiang_Poutsma appos_Chiang_2005 dep_Chiang_al. dep_Chiang_Bod conj_Chiang_Zhang conj_Chiang_2005 conj_Chiang_Palmer conj_Chiang_Ding conj_Chiang_Eisner conj_Chiang_Cowan appos_Chiang_2005 amod_Translation_Data-Oriented appos_CFG/TSG_Translation amod_CFG/TSG_Synchronous dep_model_Chiang dep_model_CFG/TSG amod_model_tree-to-tree num_Marcu_2006 nn_Marcu_al. nn_Marcu_et conj_and_Galley_on conj_and_Galley_model dep_Galley_Marcu num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Yamada_on dep_Yamada_model dep_Yamada_Galley appos_Yamada_2001 conj_and_Yamada_Knight dep_model_Knight dep_model_Yamada appos_model_string-CFG/TSG amod_model_string-totree num_Liu_2007 nn_Liu_al. nn_Liu_et dep_Liu_Liu num_Liu_2006 nn_Liu_al. nn_Liu_et dep_Huang_Liu appos_Huang_2006 dep_Huang_al. nn_Huang_et dep_model_model appos_model_Huang appos_model_TSG-string amod_model_tree-to-string nn_al._et nn_al._Koehn amod_Brown_2003 dep_Brown_al. amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_FSG_Synchronous dep_model_Brown appos_model_FSG amod_model_string-to-string dep_proposed_model prep_including_proposed_model advmod_proposed_recently auxpass_proposed_been aux_proposed_have nsubjpass_proposed_number pobj_proposed_grammars prepc_based_on_proposed_on nn_models_SMT prep_of_number_models amod_number_great det_number_a det_grammars_these
C08-1144	N03-1017	o	Starting with bilingualphrasepairsextractedfromautomatically aligned parallel text -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- these PSCFG approaches augment each contiguous -LRB- in source and target words -RRB- phrase pair with a left-hand-side symbol -LRB- like the VP in the example above -RRB- and perform a generalization procedure to form rules that include nonterminal symbols	amod_symbols_nonterminal dobj_include_symbols nsubj_include_that rcmod_rules_include dobj_form_rules aux_form_to nn_procedure_generalization det_procedure_a vmod_perform_form dobj_perform_procedure prep_example_above det_example_the prep_in_VP_example det_VP_the conj_and_like_perform pobj_like_VP dep_symbol_perform dep_symbol_like amod_symbol_left-hand-side det_symbol_a nn_pair_phrase amod_pair_contiguous nn_words_target nn_words_source conj_and_source_target pobj_in_words dep_contiguous_in det_contiguous_each prep_with_augment_symbol dobj_augment_pair dep_approaches_augment nsubj_approaches_PSCFG vmod_approaches_Starting det_PSCFG_these num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney appos_text_Koehn appos_text_2004 appos_text_Ney appos_text_Och amod_text_parallel amod_text_aligned advmod_aligned_bilingualphrasepairsextractedfromautomatically prep_with_Starting_text ccomp_``_approaches
C08-1144	N03-1017	o	Phrase pairs are extracted up to a fixed maximum length since very long phrases rarely have a tangible impact during translation -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_impact_tangible det_impact_a prep_during_have_translation dobj_have_impact advmod_have_rarely nsubj_have_phrases mark_have_since amod_phrases_long advmod_long_very nn_length_maximum amod_length_fixed det_length_a dep_extracted_Koehn advcl_extracted_have prep_to_extracted_length prt_extracted_up auxpass_extracted_are nsubjpass_extracted_pairs nn_pairs_Phrase
C08-2005	N03-1017	o	1 Introduction In phrase-based statistical machine translation -LRB- Koehn et al. 2003 -RRB- phrases extracted from word-aligned parallel data are the fundamental unit of translation	prep_of_unit_translation amod_unit_fundamental det_unit_the cop_unit_are nsubj_unit_Koehn dep_unit_Introduction amod_data_parallel amod_data_word-aligned prep_from_extracted_data vmod_phrases_extracted dep_Koehn_phrases amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_machine amod_translation_statistical amod_translation_phrase-based prep_in_Introduction_translation num_Introduction_1
C08-2032	N03-1017	o	This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation -LRB- SMT -RRB- -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_translation_SMT nn_translation_machine amod_translation_statistical amod_translation_phrase-based dobj_using_translation amod_language_pivot det_language_a amod_lexicon_bilingual det_lexicon_a prepc_by_building_using prep_through_building_language dobj_building_lexicon dep_method_Koehn prepc_for_method_building det_method_a dobj_proposes_method nsubj_proposes_paper det_paper_This ccomp_``_proposes
C08-2032	N03-1017	o	Let us suppose that we have two bilingual lexicons L f L p and L p L e We obtain word alignments of these lexicons by applying GIZA + + -LRB- Och and Ney 2003 -RRB- and grow-diag-final heuristics -LRB- Koehn et al. 2007 -RRB-	amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et amod_heuristics_grow-diag-final num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_heuristics conj_+_GIZA_+ dobj_applying_heuristics dobj_applying_+ dobj_applying_GIZA det_lexicons_these prep_of_alignments_lexicons nn_alignments_word prepc_by_obtain_applying dobj_obtain_alignments nsubj_obtain_We dep_L_Koehn dep_L_obtain amod_L_e nn_L_p nn_L_L nn_L_p conj_and_p_L dep_L_L nn_L_f nn_L_L nn_L_lexicons amod_L_bilingual num_L_two dobj_have_L nsubj_have_we mark_have_that ccomp_suppose_have nsubj_suppose_us ccomp_Let_suppose
D07-1006	N03-1017	o	This operation does not change the collection of phrases or rules extracted from a hypothesized alignment see for instance -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_see_Koehn prep_for_see_instance amod_alignment_hypothesized det_alignment_a prep_from_extracted_alignment vmod_phrases_extracted conj_or_phrases_rules prep_of_collection_rules prep_of_collection_phrases det_collection_the prep_change_see dobj_change_collection neg_change_not aux_change_does nsubj_change_operation det_operation_This
D07-1006	N03-1017	p	For French/English translation we use a state of the art phrase-based MT system similar to -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney prep_to_similar_2004 prep_to_similar_Ney prep_to_similar_Och amod_system_similar nn_system_MT amod_system_phrase-based nn_system_art det_system_the prep_of_state_system det_state_a dobj_use_state nsubj_use_we prep_for_use_translation amod_translation_French/English
D07-1006	N03-1017	o	-LRB- Och and Ney 2003 -RRB- invented heuristic symmetriza57 FRENCH/ENGLISH ARABIC/ENGLISH SYSTEM F-MEASURE -LRB- = 0.4 -RRB- BLEU F-MEASURE -LRB- = 0.1 -RRB- BLEU GIZA + + 73.5 30.63 75.8 51.55 -LRB- FRASER AND MARCU 2006B -RRB- 74.1 31.40 79.1 52.89 LEAF UNSUPERVISED 74.5 72.3 LEAF SEMI-SUPERVISED 76.3 31.86 84.5 54.34 Table 3 Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment this was extended in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_extended_in auxpass_extended_was nsubjpass_extended_this amod_alignment_M-to-N det_alignment_a prep_in_resulting_alignment vmod_model_resulting nn_model_M-to-1 det_model_a conj_and_model_model amod_model_1-to-N det_model_a prep_of_output_model prep_of_output_model det_output_the prep_of_tion_output dep_Results_extended dep_Results_tion amod_Results_Experimental num_Table_3 num_Table_54.34 num_Table_84.5 appos_31.86_Table dep_76.3_31.86 dep_SEMI-SUPERVISED_76.3 dep_LEAF_SEMI-SUPERVISED dep_72.3_LEAF number_72.3_74.5 dep_UNSUPERVISED_72.3 amod_LEAF_UNSUPERVISED num_LEAF_52.89 dep_79.1_LEAF dep_79.1_31.40 number_31.40_74.1 appos_FRASER_2006B conj_and_FRASER_MARCU dep_51.55_79.1 dep_51.55_MARCU dep_51.55_FRASER dep_51.55_75.8 cc_51.55_+ dep_75.8_30.63 number_30.63_73.5 dep_GIZA_Results conj_+_GIZA_51.55 nn_GIZA_BLEU dep_=_51.55 dep_=_GIZA dep_=_0.1 nn_F-MEASURE_BLEU dep_=_0.4 dep_F-MEASURE_F-MEASURE dep_F-MEASURE_= nn_F-MEASURE_SYSTEM nn_F-MEASURE_ARABIC/ENGLISH nn_F-MEASURE_FRENCH/ENGLISH nn_F-MEASURE_symmetriza57 nn_F-MEASURE_heuristic dep_invented_= dobj_invented_F-MEASURE nsubj_invented_Ney nsubj_invented_Och amod_Och_2003 conj_and_Och_Ney
D07-1008	N03-1017	o	Our corpora were automatically aligned with Giza + + -LRB- Och et al. 1999 -RRB- in both directions between source and target and symmetrised using the intersection heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_heuristic_Koehn nn_heuristic_intersection det_heuristic_the dobj_using_heuristic xcomp_symmetrised_using conj_and_source_symmetrised conj_and_source_target prep_between_directions_symmetrised prep_between_directions_target prep_between_directions_source preconj_directions_both amod_Och_1999 dep_Och_al. nn_Och_et dep_Giza_Och conj_+_Giza_+ prep_in_aligned_directions prep_with_aligned_+ prep_with_aligned_Giza advmod_aligned_automatically auxpass_aligned_were nsubjpass_aligned_corpora poss_corpora_Our
D07-1030	N03-1017	o	SMT has evolved from the original word-based approach -LRB- Brown et al. 1993 -RRB- into phrase-based approaches -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and syntax-based approaches -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knignt 2001 Chiang 2005 -RRB-	amod_Chiang_2005 dep_Yamada_Chiang conj_and_Yamada_2001 conj_and_Yamada_Knignt nn_al._et nn_al._Alshawi dep_Wu_2001 dep_Wu_Knignt dep_Wu_Yamada num_Wu_2000 dep_Wu_al. num_Wu_1997 dep_approaches_Wu amod_approaches_syntax-based dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_approaches_phrase-based dep_al._1993 nn_al._et amod_al._Brown dep_approach_al. amod_approach_word-based amod_approach_original det_approach_the conj_and_evolved_approaches dep_evolved_Koehn prep_into_evolved_approaches prep_from_evolved_approach aux_evolved_has nsubj_evolved_SMT ccomp_``_approaches ccomp_``_evolved
D07-1030	N03-1017	o	3.1 Phrase-Based Models According to the translation model presented in -LRB- Koehn et al. 2003 -RRB- given a source sentence f the best target translation can be obtained using the following model best e 288 -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- maxarg -RRB- -LRB- maxarg | | e e e eef fee length LM best pp p = = -LRB- 1 -RRB- Where the translation model can be decomposed into -RRB- -LRB- | efp = = I i i iii i i II aefpbadef efp 1 1 1 1 -RRB- | -LRB- -RRB- -LRB- -RRB- | -LRB- -RRB- | -LRB- w -LRB- 2 -RRB- Where -RRB- | -LRB- i i ef is the phrase translation probability	nn_probability_translation nn_probability_phrase det_probability_the cop_probability_is nsubj_probability_ef dep_probability_| nn_ef_i nn_ef_i dep_|_w nn_|_| dep_w_Where appos_w_2 nn_|_| nn_|_| dep_1_probability dep_1_1 num_1_1 number_1_1 dep_efp_1 nn_efp_aefpbadef num_efp_II nn_efp_i nn_efp_i dep_iii_efp advmod_iii_i dep_iii_I dep_iii_= dep_iii_= dep_iii_efp nn_i_i num_efp_| dep_decomposed_iii dep_decomposed_into auxpass_decomposed_be aux_decomposed_can nsubjpass_decomposed_model advmod_decomposed_Where nn_model_translation det_model_the dep_=_decomposed dep_=_1 dep_=_= amod_p_= dep_pp_p dep_best_pp amod_LM_best dep_length_LM dep_fee_length dep_eef_fee dep_e_eef amod_e_e dep_e_e prep_|_e num_|_| dep_maxarg_| dep_maxarg_maxarg dep_288_e amod_288_best dep_model_288 dep_the_maxarg prep_following_the_model dobj_using_the xcomp_obtained_using auxpass_obtained_be aux_obtained_can nsubjpass_obtained_translation nn_translation_target amod_translation_best det_translation_the rcmod_f_obtained dep_sentence_f nn_sentence_source det_sentence_a pobj_given_sentence prep_Koehn_given amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_presented_Koehn vmod_model_presented nn_model_translation det_model_the pobj_Models_model prepc_according_to_Models_to amod_Models_Phrase-Based num_Models_3.1 dep_``_Models
D07-1036	N03-1017	o	In training process we use GIZA + + 4 toolkit for word alignment in both translation directions and apply grow-diag-final method to refine it -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dobj_refine_it aux_refine_to amod_method_grow-diag-final vmod_apply_refine dobj_apply_method nsubj_apply_we nn_directions_translation preconj_directions_both prep_in_alignment_directions nn_alignment_word prep_for_toolkit_alignment num_toolkit_4 pobj_+_toolkit conj_+_GIZA_+ dep_use_Koehn conj_and_use_apply dobj_use_+ dobj_use_GIZA nsubj_use_we prep_in_use_process nn_process_training
D07-1056	N03-1017	o	In phrase-based SMT systems -LRB- Koehn et al. 2003 Koehn 2004 -RRB- foreign sentences are firstly segmented into phrases which consists of adjacent words	amod_words_adjacent prep_of_consists_words nsubj_consists_which rcmod_phrases_consists prep_into_segmented_phrases advmod_segmented_firstly cop_segmented_are nsubj_segmented_sentences dep_segmented_Koehn prep_in_segmented_systems amod_sentences_foreign dep_Koehn_2004 nn_al._et dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_systems_SMT amod_systems_phrase-based
D07-1056	N03-1017	o	There have been considerable amount of efforts to improve the reordering model in SMT systems ranging from the fundamental distance-based distortion model -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- flat reordering model -LRB- Wu 1996 Zens et al. 2004 Kumar et al. 2005 -RRB- to lexicalized reordering model -LRB- Tillmann 2004 Kumar et al. 2005 Koehn et al. 2005 -RRB- hierarchical phrase-based model -LRB- Chiang 2005 -RRB- and maximum entropy-based phrase reordering model -LRB- Xiong et al. 2006 -RRB-	amod_Xiong_2006 dep_Xiong_al. nn_Xiong_et nn_model_reordering nn_model_phrase amod_model_entropy-based nn_model_maximum dep_Chiang_2005 appos_model_Chiang amod_model_phrase-based amod_model_hierarchical num_Koehn_2005 nn_Koehn_al. nn_Koehn_et dep_Kumar_Xiong conj_and_Kumar_model conj_and_Kumar_model dep_Kumar_Koehn num_Kumar_2005 nn_Kumar_al. nn_Kumar_et dep_Tillmann_model dep_Tillmann_model dep_Tillmann_Kumar appos_Tillmann_2004 dep_model_Tillmann nn_model_reordering amod_model_lexicalized nn_al._et nn_al._Kumar nn_al._et nn_al._Zens amod_Wu_2005 dep_Wu_al. num_Wu_2004 dep_Wu_al. appos_Wu_1996 appos_model_Wu nn_model_reordering amod_model_flat num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney prep_to_model_model conj_model_model appos_model_2004 appos_model_Ney appos_model_Och nn_model_distortion amod_model_distance-based amod_model_fundamental det_model_the prep_from_ranging_model nn_systems_SMT prep_in_model_systems nn_model_reordering det_model_the dobj_improve_model aux_improve_to vmod_efforts_improve vmod_amount_ranging prep_of_amount_efforts amod_amount_considerable cop_amount_been aux_amount_have expl_amount_There
D07-1079	N03-1017	o	Approaches include word substitution systems -LRB- Brown et al. 1993 -RRB- phrase substitution systems -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and synchronous context-free grammar systems -LRB- Wu and Wong 1998 Chiang 2005 -RRB- all of which train on string pairs and seek to establish connections between source and target strings	nn_strings_target conj_and_source_strings prep_between_connections_strings prep_between_connections_source dobj_establish_connections aux_establish_to xcomp_seek_establish nn_pairs_string conj_and_train_seek prep_on_train_pairs dep_all_seek dep_all_train prep_of_all_which num_Chiang_2005 dep_Wu_Chiang num_Wu_1998 conj_and_Wu_Wong appos_systems_Wong appos_systems_Wu nn_systems_grammar amod_systems_context-free amod_systems_synchronous dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_systems_Koehn nn_systems_substitution nn_systems_phrase amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_systems_all conj_and_systems_systems conj_and_systems_systems dep_systems_Brown nn_systems_substitution nn_systems_word dobj_include_all dobj_include_systems dobj_include_systems dobj_include_systems nsubj_include_Approaches
D07-1080	N03-1017	o	Second the word alignment is refined by a grow-diag-final heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_grow-diag-final det_heuristic_a agent_refined_heuristic auxpass_refined_is nsubjpass_refined_alignment advmod_refined_Second nn_alignment_word det_alignment_the ccomp_``_refined
D07-1080	N03-1017	n	Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_approach_phrase-based amod_approach_conventional det_approach_a agent_modeled_approach advmod_modeled_directly neg_modeled_not auxpass_modeled_is nsubjpass_modeled_that rcmod_phrases_modeled prep_of_reordering_phrases det_reordering_the dep_capture_Koehn dobj_capture_reordering advmod_capture_naturally aux_capture_can nsubj_capture_structure amod_structure_quasi-syntactic det_structure_a amod_structure_Such
D07-1080	N03-1017	p	1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on -LRB- hierarchical -RRB- phrase-based translation -LRB- Och and Ney 2004 Koehn et al. 2003 Chiang 2005 -RRB- or syntax-based translation -LRB- Galley et al. 2006 -RRB-	amod_Galley_2006 dep_Galley_al. nn_Galley_et amod_translation_syntax-based dep_Chiang_2005 num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Chiang conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_translation_Galley conj_or_translation_translation appos_translation_Koehn appos_translation_2004 appos_translation_Ney appos_translation_Och amod_translation_phrase-based amod_translation_hierarchical prep_on_based_translation prep_on_based_translation preconj_based_either amod_features_real-valued prep_of_number_features amod_number_small det_number_a vmod_training_based dobj_training_number advmod_training_discriminatively agent_achieved_training auxpass_achieved_been aux_achieved_have nsubjpass_achieved_advances nn_translation_machine amod_translation_statistical prep_in_advances_translation amod_advances_recent det_advances_The rcmod_Introduction_achieved num_Introduction_1
D07-1103	N03-1017	o	These joint counts are estimated using the phrase induction algorithm described in -LRB- Koehn et al. 2003 -RRB- with symmetrized word alignments generated using IBM model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_IBM dobj_using_model vmod_generated_using vmod_alignments_generated nn_alignments_word amod_alignments_symmetrized dep_Koehn_Brown prep_with_Koehn_alignments amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn vmod_algorithm_described nn_algorithm_induction nn_algorithm_phrase det_algorithm_the dobj_using_algorithm xcomp_estimated_using auxpass_estimated_are nsubjpass_estimated_counts amod_counts_joint det_counts_These
D07-1103	N03-1017	o	The features used are the length of t a single-parameter distortion penalty on phrase reordering in a as described in -LRB- Koehn et al. 2003 -RRB- phrase translation model probabilities and 4-gram language model probabilities logp -LRB- t -RRB- using Kneser-Ney smoothing as implemented in the SRILM toolkit -LRB- Stolcke 2002 -RRB-	amod_Stolcke_2002 dep_toolkit_Stolcke nn_toolkit_SRILM det_toolkit_the prep_in_implemented_toolkit mark_implemented_as nn_smoothing_Kneser-Ney advcl_using_implemented dobj_using_smoothing appos_logp_t dep_probabilities_logp nn_probabilities_model nn_probabilities_language amod_probabilities_4-gram nn_probabilities_model nn_probabilities_translation nn_probabilities_phrase amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in mark_described_as nn_reordering_phrase prep_in_penalty_a prep_on_penalty_reordering nn_penalty_distortion amod_penalty_single-parameter det_penalty_a vmod_length_using conj_and_length_probabilities conj_and_length_probabilities dep_length_described dep_length_penalty prep_of_length_t det_length_the dep_are_probabilities dep_are_probabilities dep_are_length nsubj_are_features vmod_features_used det_features_The
D07-1104	N03-1017	o	So far these techniques have focused on phrasebased models using contiguous phrases -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_phrases_contiguous dobj_using_phrases vmod_models_using amod_models_phrasebased dep_focused_Koehn prep_on_focused_models aux_focused_have nsubj_focused_techniques advmod_focused_far det_techniques_these advmod_far_So
D07-1104	N03-1017	o	We symmetrized bidirectional alignments using the growdiag-final heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_growdiag-final det_heuristic_the dobj_using_heuristic amod_alignments_bidirectional vmod_symmetrized_using dobj_symmetrized_alignments nsubj_symmetrized_We ccomp_``_symmetrized
D07-1105	N03-1017	o	For instance word alignment models are often trained using the GIZA + + toolkit -LRB- Och and Ney 2003 -RRB- error minimizing training criteria such as the Minimum Error Rate Training -LRB- Och 2003 -RRB- are employed in order to learn feature function weights for log-linear models and translation candidates are produced using phrase-based decoders -LRB- Koehn et al. 2003 -RRB- in combination with n-gram language models -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_models_Brants nn_models_language amod_models_n-gram prep_with_combination_models amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_decoders_Koehn amod_decoders_phrase-based prep_in_using_combination dobj_using_decoders xcomp_produced_using auxpass_produced_are csubjpass_produced_trained nn_candidates_translation amod_models_log-linear nn_weights_function nn_weights_feature prep_for_learn_models dobj_learn_weights aux_learn_to dep_learn_order mark_learn_in advcl_employed_learn auxpass_employed_are nsubjpass_employed_error amod_Och_2003 dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum det_Training_the prep_such_as_criteria_Training nn_criteria_training dobj_minimizing_criteria vmod_error_minimizing dep_Och_2003 conj_and_Och_Ney appos_toolkit_Ney appos_toolkit_Och pobj_+_toolkit conj_and_GIZA_candidates conj_+_GIZA_employed conj_+_GIZA_+ det_GIZA_the dobj_using_candidates dobj_using_employed dobj_using_+ dobj_using_GIZA xcomp_trained_using advmod_trained_often auxpass_trained_are nsubjpass_trained_models prep_for_trained_instance nn_models_alignment nn_models_word
D08-1010	N03-1017	o	Thenthewordalignment is refined by performing grow-diag-final method -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn amod_method_grow-diag-final amod_method_performing agent_refined_method auxpass_refined_is nsubjpass_refined_Thenthewordalignment
D08-1021	N03-1017	o	They give a probabilistic formation of paraphrasing which naturally falls out of the fact that they use techniques from phrase-based statistical machine translation e2 = argmax e2 e2negationslash = e1 p -LRB- e2 | e1 -RRB- -LRB- 1 -RRB- where p -LRB- e2 | e1 -RRB- = summationdisplay f p -LRB- f | e1 -RRB- p -LRB- e2 | f e1 -RRB- -LRB- 2 -RRB- summationdisplay f p -LRB- f | e1 -RRB- p -LRB- e2 | f -RRB- -LRB- 3 -RRB- Phrase translation probabilities p -LRB- f | e1 -RRB- and p -LRB- e2 | f -RRB- are commonly calculated using maximum likelihood estimation -LRB- Koehn et al. 2003 -RRB- p -LRB- f | e -RRB- = count -LRB- e f -RRB- summationtext f count -LRB- e f -RRB- -LRB- 4 -RRB- where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the 197 conseguido opportunitiesequalcreatetofailedhasprojecteuropeanthe oportunidadesdeigualdadlahanoeuropeoproyectoel Figure 1 The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish phrase la igualdad aligns with equal create equal and to create equal	dobj_create_equal aux_create_to dep_equal_create conj_and_equal_create conj_and_equal_equal prep_with_aligns_create prep_with_aligns_equal prep_with_aligns_equal nsubj_aligns_igualdad mark_aligns_that nn_igualdad_la nn_igualdad_phrase amod_igualdad_Spanish det_igualdad_the ccomp_means_aligns nsubj_means_interaction amod_words_English amod_words_unaligned prep_with_heuristic_words nn_heuristic_extraction nn_heuristic_phrase det_heuristic_the prep_of_interaction_heuristic det_interaction_The num_Figure_1 nn_Figure_oportunidadesdeigualdadlahanoeuropeoproyectoel nn_Figure_opportunitiesequalcreatetofailedhasprojecteuropeanthe num_conseguido_197 det_conseguido_the prep_with_consistent_conseguido cop_consistent_are nsubj_consistent_that rcmod_pairs_consistent nn_pairs_phrase amod_pairs_bilingual det_pairs_all dobj_enumerating_pairs parataxis_collected_means dep_collected_Figure agent_collected_enumerating auxpass_collected_are nsubjpass_collected_counts advmod_collected_where det_counts_the dep_4_collected dep_e_f dep_count_4 appos_count_e nn_count_f nn_count_summationtext nn_count_count dep_e_f dep_count_e dobj_=_count dep_e_= advmod_e_| nn_|_f dep_p_e amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_estimation_likelihood nn_estimation_maximum dep_using_p dep_using_Koehn dobj_using_estimation xcomp_calculated_using advmod_calculated_commonly auxpass_calculated_are nsubjpass_calculated_probabilities dep_calculated_3 num_f_| dep_f_e2 dep_p_f nn_e1_| nn_e1_f conj_and_p_p appos_p_e1 dep_probabilities_p dep_probabilities_p nn_probabilities_translation nn_probabilities_Phrase num_f_| dep_f_e2 dep_p_calculated dep_p_f nn_p_p nn_e1_| nn_e1_f appos_p_e1 nn_p_f nn_p_summationdisplay dep_2_p dep_e1_f num_e1_| nn_e1_e2 dep_p_2 appos_p_e1 nn_p_p nn_e1_| nn_e1_f appos_p_e1 nn_p_f nn_p_summationdisplay dobj_=_p nsubj_=_p advmod_=_where num_e1_| nn_e1_e2 appos_p_e1 dep_1_= num_e1_| nn_e1_e2 dep_p_1 appos_p_e1 nn_p_e1 tmod_=_p amod_e2negationslash_= dep_e2_e2negationslash nn_e2_argmax dobj_=_e2 dep_e2_= nn_translation_machine amod_translation_statistical amod_translation_phrase-based prep_from_use_translation dobj_use_techniques nsubj_use_they mark_use_that ccomp_fact_use det_fact_the pobj_falls_fact prepc_out_of_falls_of advmod_falls_naturally nsubj_falls_which rcmod_paraphrasing_falls prep_of_formation_paraphrasing amod_formation_probabilistic det_formation_a dep_give_e2 dobj_give_formation nsubj_give_They
D08-1024	N03-1017	o	5.1 Experimental setup The baseline model was Hiero with the following baseline features -LRB- Chiang 2005 Chiang 2007 -RRB- two language models phrase translation probabilities p -LRB- f | e -RRB- and p -LRB- e | f -RRB- lexical weighting in both directions -LRB- Koehn et al. 2003 -RRB- word penalty penalties for automatically extracted rules identity rules -LRB- translating a word into itself -RRB- two classes of number/name translation rules glue rules The probability features are base-100 logprobabilities	amod_logprobabilities_base-100 cop_logprobabilities_are nsubj_logprobabilities_features nn_features_probability det_features_The nn_rules_glue nn_rules_rules nn_rules_translation amod_rules_number/name rcmod_classes_logprobabilities prep_of_classes_rules num_classes_two det_word_a prep_into_translating_itself dobj_translating_word dep_rules_classes dep_rules_translating nn_rules_identity nn_rules_rules amod_rules_extracted advmod_extracted_automatically nn_penalties_penalty nn_penalties_word dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_directions_penalties dep_directions_Koehn preconj_directions_both amod_weighting_lexical dep_weighting_f nn_weighting_p dep_f_| dep_|_e prep_for_|_rules prep_in_|_directions conj_and_|_weighting dep_|_e nn_|_f dep_p_weighting dep_p_| amod_probabilities_p nn_probabilities_translation nn_probabilities_phrase nn_probabilities_models nn_probabilities_language num_probabilities_two dep_Chiang_2007 dep_Chiang_Chiang appos_Chiang_2005 appos_features_Chiang nn_features_baseline prep_following_the_features prep_with_Hiero_the cop_Hiero_was nsubj_Hiero_model nn_model_baseline det_model_The dep_setup_probabilities rcmod_setup_Hiero amod_setup_Experimental num_setup_5.1
D08-1051	N03-1017	p	486 One of the most popular instantiations of loglinear models is that including phrase-based -LRB- PB -RRB- models -LRB- Zens et al. 2002 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Zens_Koehn appos_Zens_2002 dep_Zens_al. nn_Zens_et appos_models_Zens nn_models_PB amod_models_phrase-based pobj_including_models prepc_that_is_including nsubj_is_One amod_models_loglinear prep_of_instantiations_models amod_instantiations_popular det_instantiations_the advmod_popular_most prep_of_One_instantiations num_One_486 ccomp_``_is
D08-1059	N03-1017	p	Beam-search has been successful in many NLP tasks -LRB- Koehn et al. 2003 562 Inputs training examples -LRB- xi yi -RRB- Initialization set vectorw = 0 Algorithm / / R training iterations N examples for t = 1R i = 1N zi = argmaxyGEN -LRB- xi -RRB- -LRB- y -RRB- vectorw if zi negationslash = yi vectorw = vectorw + -LRB- yi -RRB- -LRB- zi -RRB- Outputs vectorw Figure 1 The perceptron learning algorithm Collins and Roark 2004 -RRB- and can achieve accuracy that is close to exact inference	amod_inference_exact prep_to_close_inference cop_close_is nsubj_close_that rcmod_accuracy_close dobj_achieve_accuracy aux_achieve_can dep_Roark_2004 conj_and_Collins_Roark nn_Collins_algorithm dobj_learning_Roark dobj_learning_Collins conj_and_perceptron_achieve vmod_perceptron_learning dep_The_achieve dep_The_perceptron dep_Figure_The num_Figure_1 dobj_vectorw_Figure dep_Outputs_vectorw nn_Outputs_yi nn_Outputs_vectorw dep_yi_zi conj_+_vectorw_yi dep_=_Outputs amod_vectorw_= dep_yi_vectorw dobj_=_yi dep_negationslash_= nn_negationslash_zi nn_vectorw_argmaxyGEN appos_argmaxyGEN_y appos_argmaxyGEN_xi prep_if_=_negationslash dobj_=_vectorw dep_zi_= dep_1N_zi dep_=_1N amod_i_= dep_1R_i dep_=_1R amod_t_= prep_for_examples_t nn_examples_N nn_iterations_training nn_iterations_R num_Algorithm_0 dep_=_Algorithm amod_vectorw_= dobj_set_vectorw dep_Initialization_iterations dep_Initialization_set appos_xi_yi dep_examples_examples dep_examples_Initialization dep_examples_xi nn_examples_training dep_Inputs_examples num_Inputs_562 dep_Koehn_Inputs dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_tasks_NLP amod_tasks_many dep_successful_Koehn prep_in_successful_tasks cop_successful_been aux_successful_has nsubj_successful_Beam-search ccomp_``_successful
D08-1066	N03-1017	o	From this aligned training corpus we extract the phrase pairs according to the heuristics in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_heuristics_in det_heuristics_the nn_pairs_phrase det_pairs_the pobj_extract_heuristics prepc_according_to_extract_to dobj_extract_pairs nsubj_extract_we prep_from_extract_corpus nn_corpus_training amod_corpus_aligned det_corpus_this
D08-1066	N03-1017	o	-LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- -RRB-	appos_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_''_Koehn
D08-1066	N03-1017	o	These heuristics define a phrase pair to consist of a source and target ngrams of a word-aligned source-target sentence pair such that if one end of an alignment is in the one ngram the other end is in the other ngram -LRB- and there is at least one such alignment -RRB- -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_alignment_2004 dep_alignment_Ney dep_alignment_Och amod_alignment_such num_alignment_one quantmod_one_at mwe_at_least dobj_is_alignment expl_is_there cc_is_and amod_ngram_other det_ngram_the dep_is_is prep_in_is_ngram nsubj_is_end advcl_is_is mark_is_that dep_is_such amod_end_other det_end_the num_ngram_one det_ngram_the prep_in_is_ngram nsubj_is_end mark_is_if det_alignment_an prep_of_end_alignment num_end_one dep_pair_is nn_pair_sentence amod_pair_source-target amod_pair_word-aligned det_pair_a nn_ngrams_target prep_of_source_pair conj_and_source_ngrams det_source_a prep_of_consist_ngrams prep_of_consist_source aux_consist_to vmod_pair_consist nn_pair_phrase det_pair_a dobj_define_pair nsubj_define_heuristics det_heuristics_These
D08-1066	N03-1017	o	1 Motivation A major component in phrase-based statistical Machine translation -LRB- PBSMT -RRB- -LRB- Zens et al. 2002 Koehn et al. 2003 -RRB- is the table of conditional probabilities of phrase translation pairs	nn_pairs_translation nn_pairs_phrase prep_of_probabilities_pairs amod_probabilities_conditional prep_of_table_probabilities det_table_the cop_table_is nsubj_table_component num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Zens_Koehn appos_Zens_2002 dep_Zens_al. nn_Zens_et appos_translation_PBSMT nn_translation_Machine amod_translation_statistical amod_translation_phrase-based appos_component_Zens prep_in_component_translation amod_component_major det_component_A nn_component_Motivation num_component_1
D08-1066	N03-1017	o	The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multi-set of the phrase pairs extracted from the word-aligned corpus -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_corpus_word-aligned det_corpus_the prep_from_extracted_corpus vmod_pairs_extracted nn_pairs_phrase det_pairs_the prep_of_multi-set_pairs det_multi-set_the nn_pair_phrase det_pair_the prep_in_frequency_multi-set prep_of_frequency_pair amod_frequency_relative det_frequency_the prep_on_based_frequency dep_heuristic_Koehn vmod_heuristic_based amod_heuristic_simple det_heuristic_a cop_heuristic_is nsubj_heuristic_method det_probabilities_these dobj_estimating_probabilities prepc_for_method_estimating nn_method_pervading det_method_The
D08-1078	N03-1017	o	The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA + + -LRB- Och and Ney 2003 -RRB- and the growfinal-diag algorithm -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_algorithm_growfinal-diag det_algorithm_the num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_algorithm conj_+_GIZA_+ dobj_using_algorithm dobj_using_+ dobj_using_GIZA vmod_aligning_using dobj_aligning_them nn_corpora_v3 nn_corpora_Europarl amod_corpora_respective det_corpora_the pobj_to_corpora pcomp_on_to amod_sentences_aligned det_sentences_the advmod_aligned_manually dep_appending_Koehn conj_and_appending_aligning prep_appending_on dobj_appending_sentences agent_extracted_aligning agent_extracted_appending auxpass_extracted_were nsubjpass_extracted_alignments amod_alignments_automatic det_alignments_The
D08-1089	N03-1017	n	1 Introduction Statistical phrase-based systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- have consistently delivered state-of-the-art performance in recent machine translation evaluations yet these systems remain weak at handling word order changes	nn_changes_order nn_changes_word dobj_handling_changes prepc_at_remain_handling acomp_remain_weak nsubj_remain_systems advmod_remain_yet det_systems_these nn_evaluations_translation nn_evaluations_machine amod_evaluations_recent amod_performance_state-of-the-art ccomp_delivered_remain prep_in_delivered_evaluations dobj_delivered_performance advmod_delivered_consistently aux_delivered_have nsubj_delivered_systems num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney appos_systems_2004 appos_systems_Ney appos_systems_Och amod_systems_phrase-based amod_systems_Statistical nn_systems_Introduction num_systems_1 ccomp_``_delivered
D09-1006	N03-1017	p	1 Introduction Many state-of-the-art machine translation -LRB- MT -RRB- systems over the past few years -LRB- Och and Ney 2002 Koehn et al. 2003 Chiang 2007 Koehn et al. 2007 Li et al. 2009 -RRB- rely on several models to evaluate the goodness of a given candidate translation in the target language	nn_language_target det_language_the nn_translation_candidate amod_translation_given det_translation_a prep_of_goodness_translation det_goodness_the prep_in_evaluate_language dobj_evaluate_goodness aux_evaluate_to amod_models_several xcomp_rely_evaluate prep_on_rely_models nsubj_rely_systems num_Li_2009 nn_Li_al. nn_Li_et num_Koehn_2007 nn_Koehn_al. nn_Koehn_et num_Chiang_2007 num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Li conj_and_Och_Koehn conj_and_Och_Chiang conj_and_Och_Koehn conj_and_Och_2002 conj_and_Och_Ney amod_years_few amod_years_past det_years_the appos_systems_Koehn appos_systems_Chiang appos_systems_Koehn appos_systems_2002 appos_systems_Ney appos_systems_Och prep_over_systems_years nn_systems_translation appos_translation_MT nn_translation_machine amod_translation_state-of-the-art amod_translation_Many nn_translation_Introduction num_translation_1
D09-1021	N03-1017	o	The method thereby retains the full set of lexical entries of phrase-based systems -LRB- e.g. -LRB- Koehn et al. 2003 -RRB- -RRB- .1 The model allows a straightforward integration of lexicalized syntactic language modelsfor example the models of -LRB- Charniak 2001 -RRB- in addition to a surface language model	nn_model_language nn_model_surface det_model_a prep_to_addition_model pobj_in_addition dep_Charniak_2001 pcomp_of_in dep_of_Charniak prep_models_of det_models_the nn_example_modelsfor nn_example_language amod_example_syntactic amod_example_lexicalized dep_integration_models prep_of_integration_example amod_integration_straightforward det_integration_a dobj_allows_integration nsubj_allows_set det_model_The num_model_.1 amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_e.g._Koehn dep_systems_model dep_systems_e.g. amod_systems_phrase-based prep_of_entries_systems amod_entries_lexical prep_of_set_entries amod_set_full det_set_the ccomp_retains_allows advmod_retains_thereby nsubj_retains_method det_method_The ccomp_``_retains
D09-1021	N03-1017	o	The future score is based on the source-language words that are still to be translatedthis can be directly inferred from the items bit-stringthis is similar to the use of future scores in Pharoah -LRB- Koehn et al. 2003 -RRB- and in fact we use Pharoahs future scores in our model	poss_model_our amod_scores_future nn_scores_Pharoahs prep_in_use_model dobj_use_scores nsubj_use_we prep_in_use_fact amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharoah_Koehn prep_in_scores_Pharoah amod_scores_future prep_of_use_scores det_use_the prep_to_similar_use cop_similar_is nsubj_similar_bit-stringthis rcmod_items_similar det_items_the conj_and_from_use pobj_from_items prep_inferred_use prep_inferred_from advmod_inferred_directly auxpass_inferred_be aux_inferred_can cop_translatedthis_be aux_translatedthis_to advmod_translatedthis_still cop_translatedthis_are nsubj_translatedthis_that rcmod_words_translatedthis amod_words_source-language det_words_the advcl_based_inferred prep_on_based_words auxpass_based_is nsubjpass_based_score amod_score_future det_score_The
D09-1021	N03-1017	o	We used Pharoah -LRB- Koehn et al. 2003 -RRB- as a baseline system for comparison the s-phrases used in our system include all phrases with the same scores as those used by Pharoah allowing a direct comparison	amod_comparison_direct det_comparison_a dobj_allowing_comparison agent_used_Pharoah vmod_those_used amod_scores_same det_scores_the det_phrases_all xcomp_include_allowing prep_as_include_those prep_with_include_scores dobj_include_phrases nsubj_include_s-phrases poss_system_our prep_in_used_system vmod_s-phrases_used det_s-phrases_the nn_system_baseline det_system_a amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharoah_Koehn parataxis_used_include prep_for_used_comparison prep_as_used_system dobj_used_Pharoah nsubj_used_We
D09-1021	N03-1017	o	In our experiments we use standard methods in phrase-based systems -LRB- Koehn et al. 2003 -RRB- to define the set of phrase entries for each sentence in training data	nn_data_training prep_in_sentence_data det_sentence_each nn_entries_phrase prep_for_set_sentence prep_of_set_entries det_set_the dobj_define_set aux_define_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_systems_phrase-based amod_methods_standard xcomp_use_define dep_use_Koehn prep_in_use_systems dobj_use_methods nsubj_use_we prep_in_use_experiments poss_experiments_our
D09-1023	N03-1017	o	These estimates are usually heuristic and inconsistent -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nsubj_inconsistent_estimates dep_heuristic_Koehn conj_and_heuristic_inconsistent advmod_heuristic_usually cop_heuristic_are nsubj_heuristic_estimates det_estimates_These
D09-1023	N03-1017	o	220 -LRB- Koehn et al. 2003 -RRB- they can overlap .5 Additionally since phrase features can be any function of words and alignments we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase as well as phrase pairs with gaps -LRB- Chiang 2005 Ittycheriah and Roukos 2007 -RRB-	amod_Ittycheriah_2007 conj_and_Ittycheriah_Roukos dep_Chiang_Roukos dep_Chiang_Ittycheriah appos_Chiang_2005 appos_gaps_Chiang prep_with_pairs_gaps nn_pairs_phrase nn_phrase_source det_phrase_the conj_and_word_pairs prep_inside_word_phrase nn_word_source det_word_a prep_to_aligns_pairs prep_to_aligns_word nsubj_aligns_word prep_in_aligns_which nn_phrase_target det_phrase_the prep_outside_word_phrase nn_word_target det_word_a rcmod_pairs_aligns nn_pairs_phrase dobj_consider_pairs nsubj_consider_that rcmod_features_consider dobj_permit_features nsubj_permit_we conj_and_words_alignments prep_of_function_alignments prep_of_function_words det_function_any cop_function_be aux_function_can nsubj_function_features mark_function_since nn_features_phrase dep_overlap_permit advcl_overlap_function advmod_overlap_Additionally dobj_overlap_.5 aux_overlap_can nsubj_overlap_they amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et parataxis_220_overlap appos_220_Koehn ccomp_``_220
D09-1023	N03-1017	o	2.4 Reordering Reordering features take many forms in MT. In phrase-based systems reordering is accomplished both within phrase pairs -LRB- local reordering -RRB- as well as through distance-based distortion models -LRB- Koehn et al. 2003 -RRB- and lexicalized reordering models -LRB- Koehn et al. 2007 -RRB-	amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_models_Koehn nn_models_reordering dobj_lexicalized_models nsubjpass_lexicalized_reordering amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_models_distortion amod_models_distance-based dep_through_Koehn pobj_through_models pcomp_as_through advmod_well_as dep_well_both amod_reordering_local nn_pairs_phrase appos_both_reordering prep_within_both_pairs conj_and_accomplished_lexicalized prep_accomplished_as advmod_accomplished_well auxpass_accomplished_is nsubjpass_accomplished_reordering ccomp_accomplished_take amod_systems_phrase-based amod_forms_many prep_in_take_systems prep_in_take_MT. dobj_take_forms nsubj_take_features nn_features_Reordering nn_features_Reordering num_features_2.4
D09-1023	N03-1017	p	1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms often based on grammatical formalisms .1 If we view MT as a machine learning problem features and formalisms imply structural independence assumptions which are in turn exploited by efficient inference algorithms including decoders -LRB- Koehn et al. 2003 Yamada and Knight 2001 -RRB-	amod_Yamada_2001 conj_and_Yamada_Knight dep_Koehn_Knight dep_Koehn_Yamada appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_decoders_Koehn prep_including_algorithms_decoders nn_algorithms_inference amod_algorithms_efficient agent_exploited_algorithms vmod_turn_exploited prep_in_are_turn nsubj_are_which rcmod_assumptions_are nn_assumptions_independence amod_assumptions_structural dobj_imply_assumptions nsubj_imply_formalisms nsubj_imply_features dep_imply_Introduction conj_and_features_formalisms nn_problem_learning nn_problem_machine det_problem_a prep_as_view_problem advmod_view_MT nsubj_view_we mark_view_If num_formalisms_.1 amod_formalisms_grammatical advcl_based_view prep_on_based_formalisms advmod_based_often amod_algorithms_decoding amod_algorithms_improved prep_of_development_algorithms det_development_the amod_features_rich conj_and_use_development prep_of_use_features det_use_the nn_translation_machine prep_in_progress_translation amod_progress_recent amod_progress_rapid prep_through_seen_development prep_through_seen_use dobj_seen_progress aux_seen_have nsubj_seen_We vmod_Introduction_based rcmod_Introduction_seen num_Introduction_1 ccomp_``_imply
D09-1037	N03-1017	o	These heuristics are extensions of those developed for phrase-based models -LRB- Koehn et al. 2003 -RRB- and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_trees_parse nn_trees_target det_trees_the prep_in_words_trees conj_and_words_nodes nn_words_source prep_between_mapping_nodes prep_between_mapping_words det_mapping_a dobj_find_mapping aux_find_to det_alignments_the vmod_uses_find dobj_uses_alignments nsubj_uses_which rcmod_step_uses nn_step_projection det_step_a agent_followed_step vmod_alignments_followed nn_alignments_word amod_alignments_directional num_alignments_two dobj_symmetrising_alignments dep_involve_Galley xcomp_involve_symmetrising nsubj_involve_heuristics amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_models_phrase-based prep_for_developed_models vmod_those_developed conj_and_extensions_involve dep_extensions_Koehn prep_of_extensions_those cop_extensions_are nsubj_extensions_heuristics det_heuristics_These
D09-1037	N03-1017	o	The rules are then treated as events in a relative frequency estimate .4 We used Giza + + Model 4 to obtain word alignments -LRB- Och and Ney 2003 -RRB- using the grow-diag-final-and heuristic to symmetrise the two directional predictions -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_predictions_Koehn amod_predictions_directional num_predictions_two det_predictions_the dobj_symmetrise_predictions aux_symmetrise_to amod_heuristic_grow-diag-final-and det_heuristic_the vmod_using_symmetrise dobj_using_heuristic dep_Och_2003 conj_and_Och_Ney dep_alignments_Ney dep_alignments_Och nn_alignments_word dobj_obtain_alignments aux_obtain_to num_Giza_4 conj_+_Giza_Model vmod_used_obtain dobj_used_Model dobj_used_Giza nsubj_used_We rcmod_.4_used dep_estimate_.4 nn_estimate_frequency amod_estimate_relative det_estimate_a xcomp_treated_using prep_in_treated_estimate prep_as_treated_events advmod_treated_then auxpass_treated_are nsubjpass_treated_rules det_rules_The ccomp_``_treated
D09-1037	N03-1017	o	2.1 Heuristic Grammar Induction Grammar based SMT models almost exclusively follow the same two-stage approach to grammar induction developed for phrase-based methods -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_methods_phrase-based dep_developed_Koehn prep_for_developed_methods nn_induction_grammar prep_to_approach_induction amod_approach_two-stage amod_approach_same det_approach_the dep_follow_developed dobj_follow_approach advmod_follow_exclusively nsubj_follow_models dep_follow_Grammar advmod_exclusively_almost nn_models_SMT amod_models_based nn_Grammar_Induction nn_Grammar_Grammar nn_Grammar_Heuristic num_Grammar_2.1
D09-1037	N03-1017	o	The production weights are estimated either by heuristic counting -LRB- Koehn et al. 2003 -RRB- or using the EM algorithm	nn_algorithm_EM det_algorithm_the dobj_using_algorithm amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_counting_heuristic conj_or_either_using dep_either_Koehn prep_by_either_counting dep_estimated_using dep_estimated_either auxpass_estimated_are nsubjpass_estimated_weights nn_weights_production det_weights_The
D09-1037	N03-1017	n	In contrast standard phrase-based models -LRB- Koehn et al. 2003 -RRB- assume a mostly monotone mapping between source and target and therefore can not adequately model these phenomena	det_phenomena_these dobj_model_phenomena advmod_model_adequately neg_model_not aux_model_can advmod_model_therefore nsubj_model_models conj_and_source_target prep_between_mapping_target prep_between_mapping_source nn_mapping_monotone det_mapping_a advmod_monotone_mostly conj_and_assume_model dobj_assume_mapping nsubj_assume_models prep_in_assume_contrast amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_models_Koehn amod_models_phrase-based amod_models_standard
D09-1040	N03-1017	n	1 Introduction Phrase-based systems flat and hierarchical alike -LRB- Koehn et al. 2003 Koehn 2004b Koehn et al. 2007 Chiang 2005 Chiang 2007 -RRB- have achieved a much better translation coverage than wordbased ones -LRB- Brown et al. 1993 -RRB- but untranslated words remain a major problem in SMT	prep_in_problem_SMT amod_problem_major det_problem_a xcomp_remain_problem nsubj_remain_words amod_words_untranslated amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_ones_wordbased prep_than_coverage_ones nn_coverage_translation amod_coverage_better det_coverage_a advmod_better_much conj_but_achieved_remain dep_achieved_Brown dobj_achieved_coverage aux_achieved_have nsubj_achieved_systems dep_Chiang_2007 num_Chiang_2005 num_Koehn_2007 nn_Koehn_al. nn_Koehn_et appos_Koehn_2004b dep_Koehn_Chiang dep_Koehn_Chiang dep_Koehn_Koehn dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et advmod_flat_alike conj_and_flat_hierarchical dep_systems_Koehn amod_systems_hierarchical amod_systems_flat amod_systems_Phrase-based nn_systems_Introduction num_systems_1 ccomp_``_remain ccomp_``_achieved
D09-1050	N03-1017	o	-LRB- Och et al. 1999 Koehn et al. 2003 Liang et al. 2006 -RRB-	num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Koehn_Liang num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn appos_Och_1999 dep_Och_al. nn_Och_et dep_''_Och
D09-1050	N03-1017	o	Computing the phrase translation probability is trivial in the training corpora but lexical weighting -LRB- Koehn et al. 2003 -RRB- needs lexical-level alignment	amod_alignment_lexical-level dobj_needs_alignment nsubj_needs_weighting amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_weighting_Koehn amod_weighting_lexical nn_corpora_training det_corpora_the conj_but_trivial_needs prep_in_trivial_corpora cop_trivial_is nsubj_trivial_probability dep_trivial_Computing nn_probability_translation nn_probability_phrase det_probability_the
D09-1073	N03-1017	o	Recently many phrase reordering methods have been proposed ranging from simple distancebased distortion model -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- flat reordering model -LRB- Wu 1997 Zens et al. 2004 -RRB- lexicalized reordering model -LRB- Tillmann 2004 Kumar and Byrne 2005 -RRB- to hierarchical phrase-based model -LRB- Chiang 2005 Setiawan et al. 2007 -RRB- and classifier-based reordering model with linear features -LRB- Zens and Ney 2006 Xiong et al. 2006 Zhang et al. 2007a Xiong et al. 2008 -RRB-	num_Xiong_2008 nn_Xiong_al. nn_Xiong_et appos_Zhang_2007a dep_Zhang_al. nn_Zhang_et num_Xiong_2006 nn_Xiong_al. nn_Xiong_et dep_Zens_Xiong conj_and_Zens_Zhang conj_and_Zens_Xiong conj_and_Zens_2006 conj_and_Zens_Ney appos_features_Zhang appos_features_Xiong appos_features_2006 appos_features_Ney appos_features_Zens amod_features_linear prep_with_model_features nn_model_reordering amod_model_classifier-based num_Setiawan_2007 nn_Setiawan_al. nn_Setiawan_et dep_Chiang_Setiawan appos_Chiang_2005 dep_model_Chiang amod_model_phrase-based amod_model_hierarchical dep_Kumar_2005 conj_and_Kumar_Byrne dep_Tillmann_Byrne dep_Tillmann_Kumar appos_Tillmann_2004 appos_model_Tillmann nn_model_reordering amod_model_lexicalized nn_al._et nn_al._Zens amod_Wu_2004 dep_Wu_al. appos_Wu_1997 appos_model_Wu nn_model_reordering amod_model_flat dep_Och_2004 conj_and_Och_Ney conj_and_Koehn_model prep_to_Koehn_model conj_and_Koehn_model dep_Koehn_model dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_model_distortion amod_model_distancebased amod_model_simple prep_from_ranging_model dep_proposed_model dep_proposed_model dep_proposed_Koehn xcomp_proposed_ranging auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods advmod_proposed_Recently nn_methods_reordering nn_methods_phrase amod_methods_many
D09-1073	N03-1017	p	1 Introduction Phrase-based method -LRB- Koehn et al. 2003 Och and Ney 2004 Koehn et al. 2007 -RRB- and syntaxbased method -LRB- Wu 1997 Yamada and Knight 2001 Eisner 2003 Chiang 2005 Cowan et al. 2006 Marcu et al. 2006 Liu et al. 2007 Zhang et al. 2007c 2008a 2008b Shen et al. 2008 Mi and Huang 2008 -RRB- represent the state-of-the-art technologies in statistical machine translation -LRB- SMT -RRB-	appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_technologies_translation amod_technologies_state-of-the-art det_technologies_the dobj_represent_technologies nsubj_represent_method nsubj_represent_Koehn nsubj_represent_2004 nsubj_represent_Ney nsubj_represent_Och dep_Mi_2008 conj_and_Mi_Huang nn_al._et nn_al._Shen appos_Zhang_2008b appos_Zhang_2008a appos_Zhang_2007c dep_Zhang_al. nn_Zhang_et num_Liu_2007 nn_Liu_al. nn_Liu_et dep_al._2006 nn_al._et nn_al._Marcu num_Cowan_2006 nn_Cowan_al. nn_Cowan_et num_Chiang_2005 num_Eisner_2003 num_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Huang dep_Wu_Mi num_Wu_2008 dep_Wu_al. conj_Wu_Zhang conj_Wu_Liu conj_Wu_al. conj_Wu_Cowan conj_Wu_Chiang conj_Wu_Eisner conj_Wu_Knight conj_Wu_Yamada appos_Wu_1997 appos_method_Wu amod_method_syntaxbased num_Koehn_2007 nn_Koehn_al. nn_Koehn_et conj_and_Och_method conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney parataxis_Koehn_represent appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn amod_method_Phrase-based nn_method_Introduction num_method_1
D09-1106	N03-1017	o	Word-aligned corpora have been found to be an excellent source for translation-related knowledge not only for phrase-based models -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- but also for syntax-based models -LRB- e.g. -LRB- Chiang 2007 Galley et al. 2006 Shen et al. 2008 Liu et al. 2006 -RRB- -RRB-	num_Liu_2006 nn_Liu_al. nn_Liu_et num_Shen_2008 nn_Shen_al. nn_Shen_et num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Chiang_Liu dep_Chiang_Shen dep_Chiang_Galley num_Chiang_2007 dep_e.g._Chiang dep_models_e.g. amod_models_syntax-based num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney conj_and_models_models dep_models_Koehn dep_models_2004 dep_models_Ney dep_models_Och amod_models_phrase-based neg_only_not amod_knowledge_translation-related prep_for_source_models prep_for_source_models preconj_source_only prep_for_source_knowledge amod_source_excellent det_source_an cop_source_be aux_source_to xcomp_found_source auxpass_found_been aux_found_have nsubjpass_found_corpora amod_corpora_Word-aligned
D09-1106	N03-1017	o	Then we used the refinement technique grow-diag-final-and -LRB- Koehn et al. 2003 -RRB- to all 50 50 bidirectional alignment pairs	nn_pairs_alignment amod_pairs_bidirectional num_pairs_50 det_pairs_all number_50_50 amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diag-final-and_Koehn nn_grow-diag-final-and_technique nn_grow-diag-final-and_refinement det_grow-diag-final-and_the prep_to_used_pairs dobj_used_grow-diag-final-and nsubj_used_we advmod_used_Then
D09-1106	N03-1017	o	The methods for calculating relative frequencies -LRB- Och and Ney 2004 -RRB- and lexical weights -LRB- Koehn et al. 2003 -RRB- are also adapted for the weighted matrix case	nn_case_matrix amod_case_weighted det_case_the prep_for_adapted_case advmod_adapted_also auxpass_adapted_are nsubjpass_adapted_methods amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_weights_lexical conj_and_Och_2004 conj_and_Och_Ney conj_and_frequencies_weights dep_frequencies_2004 dep_frequencies_Ney dep_frequencies_Och amod_frequencies_relative dobj_calculating_weights dobj_calculating_frequencies dep_methods_Koehn prepc_for_methods_calculating det_methods_The ccomp_``_adapted
D09-1106	N03-1017	p	Besides relative frequencies lexical weights -LRB- Koehn et al. 2003 -RRB- are widely used to estimate how well the words in f translate the words in e. To do this one needs first to estimate a lexical translation probability distribution w -LRB- e | f -RRB- by relative frequency from the same word alignments in the training corpus w -LRB- e | f -RRB- = count -LRB- f e -RRB- summationtext e count -LRB- f e -RRB- -LRB- 3 -RRB- Note that a special source NULL token is added to each source sentence and aligned to each unaligned target word	nn_word_target amod_word_unaligned det_word_each prep_to_aligned_word nsubjpass_aligned_that nn_sentence_source det_sentence_each conj_and_added_aligned prep_to_added_sentence auxpass_added_is nsubjpass_added_NULL dep_added_source dep_added_special dep_added_a nsubjpass_added_that amod_NULL_token ccomp_Note_aligned ccomp_Note_added dep_Note_3 dep_Note_= dep_Note_f nsubj_Note_w dep_f_e dep_count_f dep_count_e dep_count_summationtext dep_count_f nn_count_count dep_summationtext_e dep_=_count dep_f_| dep_|_e nn_corpus_training det_corpus_the prep_in_alignments_corpus nn_alignments_word amod_alignments_same det_alignments_the prep_from_frequency_alignments amod_frequency_relative parataxis_f_Note prep_by_f_frequency dep_f_| dep_|_e dep_w_f nn_w_distribution nn_w_probability nn_w_translation amod_w_lexical det_w_a dobj_estimate_w aux_estimate_to xcomp_needs_estimate acomp_needs_first nsubj_needs_one dobj_do_this aux_do_To xcomp_e._do det_words_the prepc_in_translate_e. dobj_translate_words advmod_translate_well prep_in_words_f det_words_the pobj_well_words advmod_well_how ccomp_estimate_translate aux_estimate_to parataxis_used_needs xcomp_used_estimate advmod_used_widely auxpass_used_are nsubjpass_used_weights prep_besides_used_frequencies amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_weights_Koehn amod_weights_lexical amod_frequencies_relative
D09-1107	N03-1017	o	While theoretically sound this approach is computationally challenging both in practice -LRB- DeNero et al. 2008 -RRB- and in theory -LRB- DeNero and Klein 2008 -RRB- may suffer from reference reachability problems -LRB- DeNero et al. 2006 -RRB- and in the end may lead to inferior translation quality -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_quality_Koehn nn_quality_translation amod_quality_inferior prep_to_lead_quality aux_lead_may prep_in_lead_end nsubj_lead_approach det_end_the amod_DeNero_2006 dep_DeNero_al. nn_DeNero_et dep_problems_DeNero nn_problems_reachability nn_problems_reference prep_from_suffer_problems aux_suffer_may nsubj_suffer_approach amod_DeNero_2008 conj_and_DeNero_Klein dep_in_Klein dep_in_DeNero pobj_in_theory amod_DeNero_2008 dep_DeNero_al. nn_DeNero_et conj_and_challenging_lead conj_and_challenging_suffer conj_and_challenging_in dep_challenging_DeNero prep_in_challenging_practice preconj_challenging_both advmod_challenging_computationally aux_challenging_is nsubj_challenging_approach advcl_challenging_sound det_approach_this advmod_sound_theoretically mark_sound_While
D09-1111	N03-1017	o	Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT -LRB- Koehn et al. 2003 -RRB- operating on characters rather than words	conj_negcc_operating_words prep_on_operating_characters amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_SMT_phrasal prep_for_solutions_SMT amod_solutions_existing dep_similar_words dep_similar_operating dep_similar_Koehn prep_to_similar_solutions advmod_similar_very cop_similar_is nsubj_similar_transliteration nn_model_hybrid amod_model_generative det_model_a prep_with_transliteration_model amod_transliteration_Substring-based
D09-1115	N03-1017	o	Then we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction -LRB- Koehn et al. 2003 -RRB- to monolingual alignments	amod_alignments_monolingual amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_extraction_phrase amod_extraction_bilingual prep_in_used_extraction advmod_used_widely auxpass_used_is nsubjpass_used_which rcmod_algorithm_used amod_algorithm_grow-diag-final det_algorithm_a prep_to_apply_alignments dep_apply_Koehn dobj_apply_algorithm nsubj_apply_we advmod_apply_Then
D09-1123	N03-1017	o	The prior probability P0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional Phrasebased SMT systems e.g. -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_systems_SMT amod_systems_Phrasebased amod_systems_conventional prep_in_used_systems advmod_used_commonly vmod_counts_used appos_normalized_Koehn dep_normalized_e.g. dobj_normalized_counts det_phrase_the dobj_using_phrase xcomp_estimated_using auxpass_estimated_is nsubjpass_estimated_which rcmod_probability_estimated nn_probability_phrase det_probability_the dep_distribution_normalized prep_for_distribution_probability amod_distribution_prior det_distribution_the cop_distribution_is nsubj_distribution_P0 nn_P0_probability amod_P0_prior det_P0_The
D09-1136	N03-1017	o	1313 E2C C2E Union Heuristic w / Big 13.37 12.66 14.55 14.28 w/o Big 13.20 12.62 14.53 14.21 Table 3 BLEU-4 scores -LRB- test set -RRB- of systems based on GIZA + + word alignments 5 6 7 8 BLEU-4 14.27 14.42 14.43 14.45 14.55 Table 4 BLEU-4 scores -LRB- test set -RRB- of the union alignment using TTS templates up to a certain size in terms of the number of leaves in their LHSs 4.1 Baseline Systems GHKM -LRB- Galley et al. 2004 -RRB- is used to generate the baseline TTS templates based on the word alignments computed using GIZA + + and different combination methods including union and the diagonal growing heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_heuristic_growing amod_heuristic_diagonal det_heuristic_the conj_and_union_heuristic nn_methods_combination amod_methods_different dep_GIZA_Koehn prep_including_GIZA_heuristic prep_including_GIZA_union conj_and_GIZA_methods conj_+_GIZA_+ dobj_using_methods dobj_using_+ dobj_using_GIZA xcomp_computed_using vmod_alignments_computed nn_alignments_word det_alignments_the prep_on_based_alignments vmod_templates_based nn_templates_TTS nn_templates_baseline det_templates_the dobj_generate_templates aux_generate_to xcomp_used_generate auxpass_used_is nsubjpass_used_scores amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_GHKM_Galley nn_GHKM_Systems nn_GHKM_Baseline num_GHKM_4.1 dep_LHSs_GHKM poss_LHSs_their prep_in_leaves_LHSs prep_of_number_leaves det_number_the prep_of_terms_number amod_size_certain det_size_a pobj_to_size pcomp_up_to nn_templates_TTS prep_using_up dobj_using_templates nn_alignment_union det_alignment_the vmod_test_set prep_in_scores_terms vmod_scores_using prep_of_scores_alignment dep_scores_test nn_scores_BLEU-4 num_Table_4 num_Table_14.55 num_Table_14.45 dep_14.43_Table dep_14.42_14.43 number_14.42_14.27 dep_BLEU-4_14.42 dep_8_BLEU-4 dep_7_8 number_7_6 number_7_5 nn_alignments_word dep_GIZA_7 conj_+_GIZA_alignments prep_on_based_alignments prep_on_based_GIZA vmod_test_set vmod_scores_based prep_of_scores_systems dep_scores_test nn_scores_BLEU-4 num_Table_3 num_Table_14.21 num_Table_14.53 num_Table_12.62 number_12.62_13.20 parataxis_Big_used dep_Big_scores dep_Big_Table dep_w/o_Big dep_14.28_w/o number_14.28_14.55 dep_14.28_12.66 dep_14.28_Big number_12.66_13.37 dep_w_14.28 nn_w_Heuristic nn_w_Union nn_w_C2E nn_w_E2C num_w_1313 dep_``_w
D09-1136	N03-1017	o	The word alignment is computed using GIZA + +2 for the selected 73,597 sentence pairs in the FBIS corpus in both directions and then combined using union and heuristic diagonal growing -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_growing_Koehn dep_diagonal_growing dep_union_diagonal conj_and_union_heuristic dobj_using_heuristic dobj_using_union xcomp_combined_using advmod_combined_then conj_and_directions_combined preconj_directions_both nn_corpus_FBIS det_corpus_the prep_in_pairs_corpus nn_pairs_sentence num_pairs_73,597 amod_pairs_selected det_pairs_the conj_+_GIZA_+2 prep_in_using_combined prep_in_using_directions prep_for_using_pairs dobj_using_+2 dobj_using_GIZA xcomp_computed_using auxpass_computed_is nsubjpass_computed_alignment nn_alignment_word det_alignment_The dep_``_computed
E06-2002	N03-1017	o	By introducing the hidden word alignment variable a the following approximate optimization criterion can be applied for that purpose e = argmaxe Pr -LRB- e | f -RRB- = argmaxe summationdisplay a Pr -LRB- e a | f -RRB- argmaxe a Pr -LRB- e a | f -RRB- Exploiting the maximum entropy -LRB- Berger et al. 1996 -RRB- framework the conditional distribution Pr -LRB- e a | f -RRB- can be determined through suitable real valued functions -LRB- called features -RRB- hr -LRB- e f a -RRB- r = 1R and takes the parametric form p -LRB- e a | f -RRB- exp Rsummationdisplay r = 1 rhr -LRB- e f a -RRB- -RCB- The ITC-irst system -LRB- Chen et al. 2005 -RRB- is based on a log-linear model which extends the original IBM Model 4 -LRB- Brown et al. 1993 -RRB- to phrases -LRB- Koehn et al. 2003 Federico and Bertoldi 2005 -RRB-	amod_Federico_2005 conj_and_Federico_Bertoldi dep_Koehn_Bertoldi dep_Koehn_Federico appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Koehn prep_to_Model_phrases dep_Model_Brown num_Model_4 dep_IBM_Model dep_original_IBM amod_the_original dobj_extends_the nsubj_extends_which rcmod_model_extends amod_model_log-linear det_model_a amod_Chen_2005 dep_Chen_al. nn_Chen_et dep_system_Chen amod_system_ITC-irst det_system_The dep_system_e nn_system_p dep_e_a dep_e_f num_rhr_1 dep_=_rhr dep_r_e amod_r_= nn_r_Rsummationdisplay dep_exp_r dep_exp_f dep_|_exp det_|_a appos_e_| amod_form_parametric det_form_the nsubj_takes_system dobj_takes_form dobj_=_1R npadvmod_=_r appos_e_a appos_e_f appos_hr_e prep_based_on_called_model auxpass_called_is nsubjpass_called_system conj_and_called_takes dep_called_= dep_called_hr dep_called_features dep_functions_takes dep_functions_called amod_functions_valued amod_functions_real amod_functions_suitable prep_through_determined_functions auxpass_determined_be aux_determined_can nsubjpass_determined_| dep_determined_e dep_|_f det_|_a vmod_Pr_determined nn_Pr_distribution amod_Pr_conditional det_Pr_the appos_framework_Pr dep_framework_Berger dep_framework_entropy dep_framework_Exploiting dep_framework_| dep_framework_e dep_Berger_1996 dep_Berger_al. nn_Berger_et nn_entropy_maximum det_entropy_the dep_Exploiting_f det_|_a dep_Pr_framework det_Pr_a appos_argmaxe_Pr dep_argmaxe_f dep_|_argmaxe det_|_a dep_e_| appos_Pr_e det_Pr_a nn_summationdisplay_argmaxe dobj_=_Pr iobj_=_summationdisplay dep_=_f dep_f_| dep_|_e dep_Pr_= amod_Pr_argmaxe dep_=_Pr dep_=_e prepc_by_=_introducing det_purpose_that prep_for_applied_purpose auxpass_applied_be aux_applied_can nsubjpass_applied_criterion dep_applied_a nn_criterion_optimization amod_criterion_approximate amod_criterion_following det_criterion_the dep_variable_applied nn_variable_alignment nn_variable_word amod_variable_hidden det_variable_the dobj_introducing_variable
E09-1003	N03-1017	o	It is today common practice to use phrases as translation units -LRB- Koehn et al. 2003 Och and Ney 2003 -RRB- instead of the original word-based approach	amod_approach_word-based amod_approach_original det_approach_the dep_Och_2003 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_negcc_units_approach appos_units_Koehn nn_units_translation prep_as_use_approach prep_as_use_units dobj_use_phrases aux_use_to vmod_practice_use amod_practice_common tmod_practice_today cop_practice_is nsubj_practice_It
E09-1033	N03-1017	o	These were combined using the Grow Diag Final And symmetrization heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_heuristic_symmetrization dep_Final_Koehn conj_and_Final_heuristic nn_Final_Diag nn_Final_Grow det_Final_the dobj_using_heuristic dobj_using_Final xcomp_combined_using auxpass_combined_were nsubjpass_combined_These ccomp_``_combined
E09-1043	N03-1017	n	The problem is typically presented in log-space which simplifies computations but otherwise does not change the problem due to the monotonicity of the log function -LRB- hm = log hprimem -RRB- log p -LRB- t | s -RRB- = summationdisplay m m hm -LRB- t s -RRB- -LRB- 3 -RRB- Phrase-based models -LRB- Koehn et al. 2003 -RRB- are limited to the mapping of small contiguous chunks of text	prep_of_chunks_text amod_chunks_contiguous amod_chunks_small prep_of_mapping_chunks det_mapping_the prep_to_limited_mapping auxpass_limited_are nsubjpass_limited_change amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_models_Koehn amod_models_Phrase-based dep_models_3 dep_models_p dep_models_monotonicity dep_models_to dep_models_due appos_t_s dep_hm_t nn_hm_m nn_hm_m nn_hm_summationdisplay dep_=_hm num_s_| nn_s_t amod_p_= appos_p_s nn_p_log dep_p_hm nn_hprimem_log dep_=_hprimem amod_hm_= nn_function_log det_function_the prep_of_monotonicity_function det_monotonicity_the rcmod_problem_models det_problem_the dobj_change_problem neg_change_not aux_change_does advmod_change_otherwise dobj_simplifies_computations nsubj_simplifies_which rcmod_log-space_simplifies conj_but_presented_limited prep_in_presented_log-space advmod_presented_typically auxpass_presented_is nsubjpass_presented_problem det_problem_The
E09-1049	N03-1017	o	-LRB- 2003 -RRB- or in more recent implementation the MOSES MT system1 -LRB- Koehn et al. 2007 -RRB-	amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_system1_Koehn nn_system1_MT nn_system1_MOSES det_system1_the amod_implementation_recent advmod_recent_more pobj_in_implementation conj_or_2003_system1 conj_or_2003_in dep_''_system1 dep_''_in dep_''_2003
E09-1063	N03-1017	o	5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model GIZA + + implementation of IBM word alignment model 4 -LRB- Och and Ney 2003 -RRB- the refinement and phrase-extraction heuristics described in -LRB- Koehn et al. 2003 -RRB- minimum-errorrate training -LRB- Och 2003 -RRB- a 5-gram language model with Kneser-Ney smoothing trained with SRILM -LRB- Stolcke 2002 -RRB- on the English side of the training data and Moses -LRB- Koehn et al. 2007 Dyer et al. 2008 -RRB- to translate both single best segmentation and word lattices	nn_lattices_word conj_and_segmentation_lattices amod_segmentation_best amod_segmentation_single preconj_segmentation_both dobj_translate_lattices dobj_translate_segmentation aux_translate_to num_Dyer_2008 nn_Dyer_al. nn_Dyer_et dep_Koehn_Dyer appos_Koehn_2007 dep_Koehn_al. nn_Koehn_et vmod_Moses_translate appos_Moses_Koehn nn_data_training det_data_the prep_of_side_data amod_side_English det_side_the dep_Stolcke_2002 appos_SRILM_Stolcke prep_on_trained_side prep_with_trained_SRILM vmod_smoothing_trained amod_smoothing_Kneser-Ney prep_with_model_smoothing nn_model_language amod_model_5-gram det_model_a dep_Och_2003 appos_training_Och amod_training_minimum-errorrate conj_and_Koehn_Moses conj_and_Koehn_model conj_and_Koehn_training amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Moses prep_in_described_model prep_in_described_training prep_in_described_Koehn amod_heuristics_phrase-extraction vmod_refinement_described conj_and_refinement_heuristics det_refinement_the dep_Och_2003 conj_and_Och_Ney appos_model_Ney appos_model_Och num_model_4 nn_model_alignment nn_model_word appos_GIZA_heuristics appos_GIZA_refinement dep_GIZA_model prep_of_GIZA_IBM conj_+_GIZA_implementation nn_model_PB-SMT amod_model_log-linear amod_model_standard det_model_a amod_segmenters_different prep_with_using_model dobj_using_segmenters vmod_experiments_using dobj_conducted_experiments nsubj_conducted_We dep_System_implementation dep_System_GIZA rcmod_System_conducted nn_System_Baseline num_System_5.3
H05-1009	N03-1017	o	We computed precision recall and error rate on the entire set of sentence pairs for each data set .5 To evaluate NeurAlign we used GIZA + + in both directions -LRB- E-to-F and F-to-E where F is either Chinese -LRB- C -RRB- or Spanish -LRB- S -RRB- -RRB- as input and a refined alignment approach -LRB- Och and Ney 2000 -RRB- that uses a heuristic combination method called grow-diagfinal -LRB- Koehn et al. 2003 -RRB- for comparison	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diagfinal_Koehn prep_for_called_comparison dep_called_grow-diagfinal vmod_method_called nn_method_combination nn_method_heuristic det_method_a dobj_uses_method nsubj_uses_that amod_Och_2000 conj_and_Och_Ney nn_approach_alignment amod_approach_refined det_approach_a rcmod_input_uses dep_input_Ney dep_input_Och conj_and_input_approach appos_Spanish_S nsubj_Spanish_F conj_or_Chinese_Spanish dep_Chinese_C preconj_Chinese_either cop_Chinese_is nsubj_Chinese_F advmod_Chinese_where prep_as_E-to-F_approach prep_as_E-to-F_input dep_E-to-F_Spanish dep_E-to-F_Chinese conj_and_E-to-F_F-to-E det_directions_both prep_in_+_directions conj_+_GIZA_F-to-E conj_+_GIZA_E-to-F conj_+_GIZA_+ dobj_used_E-to-F dobj_used_+ dobj_used_GIZA nsubj_used_we ccomp_used_computed dobj_evaluate_NeurAlign aux_evaluate_To vmod_set_evaluate dobj_set_.5 det_data_each nn_pairs_sentence prep_for_set_data prep_of_set_pairs amod_set_entire det_set_the nn_rate_error conj_and_precision_rate conj_and_precision_recall dep_computed_set prep_on_computed_set dobj_computed_rate dobj_computed_recall dobj_computed_precision nsubj_computed_We
H05-1021	N03-1017	o	Phrase-pairs are then extracted from the word alignments -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_alignments_Koehn nn_alignments_word det_alignments_the prep_from_extracted_alignments advmod_extracted_then auxpass_extracted_are nsubjpass_extracted_Phrase-pairs
H05-1022	N03-1017	o	5 Phrase Pair Induction A common approach to phrase-based translation is to extract an inventory of phrase pairs -LRB- PPI -RRB- from bitext -LRB- Koehn et al. 2003 -RRB- For example in the phraseextract algorithm -LRB- Och 2002 -RRB- a word alignment am1 is generated over the bitext and all word subsequences ei2i1 and fj2j1 are found that satisfy am1 aj -LSB- i1 i2 -RSB- iff j -LSB- j1 j2 -RSB-	appos_j1_j2 dep_j_j1 nn_j_iff dep_j_i1 nn_j_aj appos_i1_i2 dep_am1_j dep_satisfy_am1 nsubj_satisfy_that ccomp_found_satisfy auxpass_found_are nsubjpass_found_subsequences conj_and_ei2i1_fj2j1 dobj_subsequences_fj2j1 dobj_subsequences_ei2i1 nsubj_subsequences_word det_word_all det_bitext_the conj_and_generated_found prep_over_generated_bitext auxpass_generated_is nsubjpass_generated_am1 nn_am1_alignment nn_am1_word det_am1_a amod_Och_2002 dep_algorithm_Och amod_algorithm_phraseextract det_algorithm_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_pairs_PPI nn_pairs_phrase prep_from_inventory_bitext prep_of_inventory_pairs det_inventory_an dobj_extract_inventory aux_extract_to parataxis_is_found parataxis_is_generated prep_in_is_algorithm prep_for_is_example dep_is_Koehn xcomp_is_extract nsubj_is_approach amod_translation_phrase-based prep_to_approach_translation amod_approach_common det_approach_A nn_approach_Induction nn_approach_Pair nn_approach_Phrase num_approach_5
H05-1023	N03-1017	p	1 Introduction Todays statistical machine translation systems rely on high quality phrase translation pairs to acquire state-of-the-art performance see -LRB- Koehn et al. 2003 Zens and Ney 2004 Och and Ney 2003 -RRB-	dep_Zens_2003 conj_and_Zens_Ney conj_and_Zens_Och conj_and_Zens_2004 conj_and_Zens_Ney dep_Koehn_Ney dep_Koehn_Och dep_Koehn_2004 dep_Koehn_Ney dep_Koehn_Zens appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_see_Koehn amod_performance_state-of-the-art dobj_acquire_performance aux_acquire_to nn_pairs_translation nn_pairs_phrase nn_pairs_quality amod_pairs_high xcomp_rely_acquire prep_on_rely_pairs nsubj_rely_systems nn_systems_translation nn_systems_machine amod_systems_statistical dep_Todays_see ccomp_Todays_rely nsubj_Todays_Introduction num_Introduction_1 ccomp_``_Todays
H05-1024	N03-1017	o	We computed precision recall and error rate on the entire set for each data set .6 For an initial alignment we used GIZA + + in both directions -LRB- E-to-F and F-to-E where F is either Chinese -LRB- C -RRB- or Spanish -LRB- S -RRB- -RRB- and also two different combined alignments intersection of E-to-F and F-to-E and RA using a heuristic combination approach called grow-diag-final -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diag-final_Koehn acomp_called_grow-diag-final nsubj_called_RA nn_approach_combination nn_approach_heuristic det_approach_a dobj_using_approach vmod_RA_using conj_and_E-to-F_F-to-E conj_and_intersection_called prep_of_intersection_F-to-E prep_of_intersection_E-to-F dep_alignments_called dep_alignments_intersection amod_alignments_combined amod_alignments_different num_alignments_two advmod_alignments_also appos_Spanish_S nsubj_Spanish_F conj_or_Chinese_Spanish dep_Chinese_C preconj_Chinese_either cop_Chinese_is nsubj_Chinese_F advmod_Chinese_where rcmod_E-to-F_Spanish rcmod_E-to-F_Chinese conj_and_E-to-F_F-to-E det_directions_both appos_+_F-to-E appos_+_E-to-F prep_in_+_directions conj_and_used_alignments conj_+_used_+ dobj_used_GIZA nsubj_used_we ccomp_used_set nsubj_used_computed amod_alignment_initial det_alignment_an prep_for_set_alignment dobj_set_.6 det_data_each prep_for_set_data amod_set_entire det_set_the nn_rate_error conj_and_precision_rate conj_and_precision_recall prep_on_computed_set dobj_computed_rate dobj_computed_recall dobj_computed_precision nsubj_computed_We
H05-1024	N03-1017	o	The standard method to overcome this problem to use the model in both directions -LRB- interchanging the source and target languages -RRB- and applying heuristic-based combination techniques to produce a refined alignment -LRB- Och and Ney 2000 Koehn et al. 2003 -RRB- henceforth referred to as RA Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models	nn_models_alignment amod_models_different dobj_combining_models amod_knowledge_additional conj_or_injecting_combining dobj_injecting_knowledge nn_systems_alignment nn_systems_word prepc_by_improving_combining prepc_by_improving_injecting dobj_improving_systems prepc_for_proposed_improving dobj_proposed_algorithms aux_proposed_have nsubj_proposed_researchers amod_researchers_Several pobj_referred_RA prepc_as_to_referred_as nsubj_referred_henceforth num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2000 conj_and_Och_Ney rcmod_alignment_referred appos_alignment_2000 appos_alignment_Ney appos_alignment_Och amod_alignment_refined det_alignment_a dobj_produce_alignment aux_produce_to nn_techniques_combination amod_techniques_heuristic-based vmod_applying_produce dobj_applying_techniques nn_languages_target conj_and_source_languages det_source_the dobj_interchanging_languages dobj_interchanging_source preconj_directions_both det_model_the conj_and_use_applying dep_use_interchanging prep_in_use_directions dobj_use_model aux_use_to det_problem_this xcomp_overcome_applying xcomp_overcome_use dobj_overcome_problem aux_overcome_to rcmod_method_proposed vmod_method_overcome nn_method_standard det_method_The
H05-1024	N03-1017	p	For our experiments we chose GIZA + + -LRB- Och and Ney 2000 -RRB- and the RA approach -LRB- Koehn et al. 2003 -RRB- the best known alignment combination technique as our initial aligners .1 4.2 TBL Templates Our templates consider consecutive words -LRB- of size 1 2 or 3 -RRB- in both languages	preconj_languages_both conj_or_size_3 num_size_2 num_size_1 prep_of_words_3 prep_of_words_size amod_words_consecutive prep_in_consider_languages dobj_consider_words nsubj_consider_aligners mark_consider_as poss_templates_Our dep_Templates_templates nn_Templates_TBL num_Templates_4.2 number_4.2_.1 dep_aligners_Templates amod_aligners_initial poss_aligners_our advcl_technique_consider nn_technique_combination nn_technique_alignment amod_technique_known amod_technique_best det_technique_the dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_approach_RA det_approach_the num_Och_2000 conj_and_Och_Ney appos_+_Ney appos_+_Och dep_GIZA_technique dep_GIZA_Koehn conj_and_GIZA_approach conj_+_GIZA_+ dobj_chose_approach dobj_chose_+ dobj_chose_GIZA nsubj_chose_we prep_for_chose_experiments poss_experiments_our
H05-1096	N03-1017	p	Nowadays most of the state-of-the-art SMT systems are based on bilingual phrases -LRB- Bertoldi et al. 2004 Koehn et al. 2003 Och and Ney 2004 Tillmann 2003 Vogel et al. 2004 Zens and Ney 2004 -RRB-	dep_Zens_2004 conj_and_Zens_Ney num_Vogel_2004 nn_Vogel_al. nn_Vogel_et num_Tillmann_2003 dep_Och_Ney dep_Och_Zens conj_and_Och_Vogel conj_and_Och_Tillmann conj_and_Och_2004 conj_and_Och_Ney dep_Koehn_Vogel dep_Koehn_Tillmann dep_Koehn_2004 dep_Koehn_Ney dep_Koehn_Och num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Bertoldi_Koehn amod_Bertoldi_2004 dep_Bertoldi_al. nn_Bertoldi_et amod_phrases_bilingual dep_based_Bertoldi prep_on_based_phrases auxpass_based_are nsubjpass_based_most advmod_based_Nowadays nn_systems_SMT amod_systems_state-of-the-art det_systems_the prep_of_most_systems ccomp_``_based
H05-1098	N03-1017	o	The basic model uses the following features analogous to Pharaohs default feature set P -LRB- | -RRB- and P -LRB- | -RRB- the lexical weights Pw -LRB- | -RRB- and Pw -LRB- | -RRB- -LRB- Koehn et al. 2003 -RRB- 1 a phrase penalty exp -LRB- 1 -RRB- a word penalty exp -LRB- l -RRB- where l is the number of terminals in	prep_number_in prep_of_number_terminals det_number_the cop_number_is nsubj_number_l advmod_number_where rcmod_exp_number appos_exp_l nn_exp_penalty nn_exp_word det_exp_a appos_exp_1 nn_exp_penalty nn_exp_phrase det_exp_a num_exp_1 amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_Pw_| conj_and_Pw_Pw appos_Pw_| dep_weights_Pw dep_weights_Pw amod_weights_lexical det_weights_the appos_P_| dep_P_exp dep_P_exp dep_P_Koehn dep_P_weights conj_and_P_P appos_P_| nn_set_feature nn_set_default nn_set_Pharaohs prep_to_analogous_set amod_features_following det_features_the dep_uses_P dep_uses_P dep_uses_analogous dobj_uses_features nsubj_uses_model amod_model_basic det_model_The ccomp_``_uses
H05-1098	N03-1017	o	The feature weights are learned by maximizing the BLEU score -LRB- Papineni et al. 2002 -RRB- on held-out data,usingminimum-error-ratetraining -LRB- Och ,2003 -RRB- as implemented by Koehn	prep_by_implemented_Koehn mark_implemented_as num_Och_,2003 appos_data,usingminimum-error-ratetraining_Och amod_data,usingminimum-error-ratetraining_held-out dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dobj_maximizing_score advcl_learned_implemented prep_on_learned_data,usingminimum-error-ratetraining dep_learned_Papineni agent_learned_maximizing auxpass_learned_are nsubjpass_learned_weights nn_weights_feature det_weights_The ccomp_``_learned
H05-1098	N03-1017	o	The need for some way to model aspects of syntactic behavior such as the tendency of constituents to move together as a unit is widely recognizedthe role of syntactic units is well attested in recent systematic studies of translation -LRB- Fox 2002 Hwa et al. 2002 Koehn and Knight 2003 -RRB- and their absence in phrase-based models is quite evident when looking at MT system output	nn_output_system nn_output_MT prep_at_looking_output advmod_looking_when advcl_evident_looking advmod_evident_quite cop_evident_is nsubj_evident_absence amod_models_phrase-based prep_in_absence_models poss_absence_their amod_Koehn_2003 conj_and_Koehn_Knight num_Hwa_2002 nn_Hwa_al. nn_Hwa_et dep_Fox_Knight dep_Fox_Koehn dep_Fox_Hwa dep_Fox_2002 prep_of_studies_translation amod_studies_systematic amod_studies_recent conj_and_attested_evident dep_attested_Fox prep_in_attested_studies advmod_attested_well auxpass_attested_is nsubjpass_attested_role amod_units_syntactic prep_of_role_units amod_role_recognizedthe cop_role_is nsubj_role_need advmod_recognizedthe_widely det_unit_a prep_as_move_unit advmod_move_together aux_move_to vmod_tendency_move prep_of_tendency_constituents det_tendency_the amod_behavior_syntactic prep_such_as_aspects_tendency prep_of_aspects_behavior nn_aspects_model prep_to_way_aspects det_way_some prep_for_need_way det_need_The
I08-1033	N03-1017	o	The phrasebased machine translation -LRB- Koehn et al. 2003 -RRB- uses the grow-diag-final heuristic to extend the word alignment to phrase alignment by using the intersection result	nn_result_intersection det_result_the dobj_using_result nn_alignment_phrase nn_alignment_word det_alignment_the prepc_by_extend_using prep_to_extend_alignment dobj_extend_alignment aux_extend_to amod_heuristic_grow-diag-final det_heuristic_the vmod_uses_extend dobj_uses_heuristic nsubj_uses_translation amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_translation_Koehn nn_translation_machine amod_translation_phrasebased det_translation_The
I08-1033	N03-1017	o	However for remedy many of the current word alignment methods combine the results of both alignment directions via intersection or 249 grow-diag-final heuristic to improve the alignment reliability -LRB- Koehn et al. 2003 Liang et al. 2006 Ayan et al. 2006 DeNero et al. 2007 -RRB-	num_DeNero_2007 nn_DeNero_al. nn_DeNero_et num_Ayan_2006 nn_Ayan_al. nn_Ayan_et dep_Liang_DeNero conj_Liang_Ayan num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Koehn_Liang appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_reliability_alignment det_reliability_the dep_improve_Koehn dobj_improve_reliability aux_improve_to amod_heuristic_grow-diag-final num_heuristic_249 conj_or_intersection_heuristic nn_directions_alignment det_directions_both prep_of_results_directions det_results_the xcomp_combine_improve prep_via_combine_heuristic prep_via_combine_intersection dobj_combine_results nsubj_combine_many prep_for_combine_remedy advmod_combine_However nn_methods_alignment nn_methods_word amod_methods_current det_methods_the prep_of_many_methods
I08-1064	N03-1017	p	Although bi-alignments are known to exhibit high precision -LRB- Koehn et al. 2003 -RRB- in the face of sparse annotations we use unidirectional alignments as a fallback as has been proposed in the context of phrase-based machine translation -LRB- Koehn et al. 2003 Tillmann 2003 -RRB-	amod_Tillmann_2003 dep_Koehn_Tillmann appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_machine amod_translation_phrase-based prep_of_context_translation det_context_the dep_proposed_Koehn prep_in_proposed_context auxpass_proposed_been aux_proposed_has mark_proposed_as det_fallback_a amod_alignments_unidirectional advcl_use_proposed prep_as_use_fallback dobj_use_alignments nsubj_use_we prep_in_use_face advcl_use_known amod_annotations_sparse prep_of_face_annotations det_face_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_precision_Koehn amod_precision_high dobj_exhibit_precision aux_exhibit_to xcomp_known_exhibit auxpass_known_are nsubjpass_known_bi-alignments mark_known_Although
I08-1067	N03-1017	p	Phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases the joint model and IBM model 4 -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et num_model_4 nn_model_IBM dep_model_Koehn conj_and_model_model amod_model_joint det_model_the dep_,_model dep_,_model amod_phrases_motivated advmod_motivated_syntactically prep_than_better_phrases dobj_perform_better aux_perform_to xcomp_shown_perform advmod_shown_also auxpass_shown_are nsubjpass_shown_Phrases det_heuristics_these dobj_using_heuristics xcomp_extracted_using vmod_Phrases_extracted ccomp_``_shown
I08-1067	N03-1017	o	The phrase translation table is learnt in the following manner The parallel corpus is word-aligned bidirectionally and using various heuristics -LRB- see -LRB- Koehn et al. 2003 -RRB- for details -RRB- phrase correspondences are established	auxpass_established_are nsubjpass_established_using nn_correspondences_phrase nn_correspondences_heuristics amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_for_see_details dep_see_Koehn dep_heuristics_see amod_heuristics_various dobj_using_correspondences conj_and_word-aligned_established advmod_word-aligned_bidirectionally cop_word-aligned_is nsubj_word-aligned_corpus amod_corpus_parallel det_corpus_The amod_manner_following det_manner_the parataxis_learnt_established parataxis_learnt_word-aligned prep_in_learnt_manner auxpass_learnt_is nsubjpass_learnt_table nn_table_translation nn_table_phrase det_table_The
I08-2088	N03-1017	o	We used the preprocessed data to train the phrase-based translation model by using GIZA + + -LRB- Och and Ney 2003 -RRB- and the Pharaoh tool kit -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_kit_tool nn_kit_Pharaoh det_kit_the num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och dep_GIZA_Koehn conj_and_GIZA_kit conj_+_GIZA_+ dobj_using_kit dobj_using_+ dobj_using_GIZA nn_model_translation amod_model_phrase-based det_model_the prepc_by_train_using dobj_train_model aux_train_to amod_data_preprocessed det_data_the xcomp_used_train dobj_used_data nsubj_used_We
I08-2088	N03-1017	o	3.2.2 Features We used eight features -LRB- Och and Ney 2003 Koehn et al. 2003 -RRB- and their weights for the translations	det_translations_the prep_for_weights_translations poss_weights_their num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2003 conj_and_Och_Ney dep_features_Koehn dep_features_2003 dep_features_Ney dep_features_Och num_features_eight conj_and_used_weights dobj_used_features nsubj_used_We rcmod_Features_weights rcmod_Features_used num_Features_3.2.2 dep_``_Features
I08-8001	N03-1017	n	However reordering models in traditional phrase-based systems are not sufficient to treat such complex cases when we translate long sentences -LRB- Koehn et al 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al nn_Koehn_et amod_sentences_long dobj_translate_sentences nsubj_translate_we advmod_translate_when amod_cases_complex amod_cases_such dep_treat_Koehn advcl_treat_translate dobj_treat_cases aux_treat_to xcomp_sufficient_treat neg_sufficient_not cop_sufficient_are nsubj_sufficient_models advmod_sufficient_However amod_systems_phrase-based amod_systems_traditional prep_in_models_systems nn_models_reordering
J07-1003	N03-1017	p	Nowadays most state-of-the-art SMT systems are based on bilingual phrases -LRB- Och Tillmann and Ney 1999 Koehn Och and Marcu 2003 Tillmann 2003 Bertoldi et al. 2004 Vogel et al. 2004 Zens and Ney 2004 Chiang 2005 -RRB-	num_Chiang_2005 num_Ney_2004 conj_and_Zens_Ney nn_2004_al. num_Vogel_2004 nn_Vogel_et num_al._2004 nn_al._et nn_al._Bertoldi num_Tillmann_2003 num_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och num_Ney_1999 dep_Och_Chiang conj_and_Och_Ney conj_and_Och_Zens conj_and_Och_Vogel conj_and_Och_al. conj_and_Och_Tillmann conj_and_Och_Marcu conj_and_Och_Och conj_and_Och_Koehn conj_and_Och_Ney conj_and_Och_Tillmann dep_phrases_Zens dep_phrases_Vogel dep_phrases_al. dep_phrases_Tillmann dep_phrases_Koehn dep_phrases_Ney dep_phrases_Tillmann dep_phrases_Och amod_phrases_bilingual prep_on_based_phrases auxpass_based_are nsubjpass_based_systems advmod_based_Nowadays nn_systems_SMT amod_systems_state-of-the-art advmod_state-of-the-art_most ccomp_``_based
J07-2003	N03-1017	o	Above the phrase level some models perform no reordering -LRB- Zens and Ney 2004 Kumar Deng and Byrne 2006 -RRB- some have a simple distortion model that reorders phrases independently of their content -LRB- Koehn Och and Marcu 2003 Och and Ney 2004 -RRB- and some for example the Alignment Template System -LRB- Och et al. 2004 Thayer et al. 2004 -RRB- hereafter ATS and the IBM phrase-based system -LRB- Tillmann 2004 Tillmann and Zhang 2005 -RRB- have phrase-reordering models that add some lexical sensitivity	amod_sensitivity_lexical det_sensitivity_some dobj_add_sensitivity nsubj_add_that rcmod_models_add amod_models_phrase-reordering dobj_have_models nsubj_have_system nsubj_have_System prep_for_have_example nsubj_have_some num_Zhang_2005 conj_and_Tillmann_Zhang dep_Tillmann_Zhang dep_Tillmann_Tillmann num_Tillmann_2004 appos_system_Tillmann amod_system_phrase-based nn_system_IBM det_system_the advmod_ATS_hereafter dep_al._2004 nn_al._et nn_al._Thayer num_al._2004 nn_al._et dep_Och_al. dep_Och_al. conj_and_System_system appos_System_ATS appos_System_Och nn_System_Template nn_System_Alignment det_System_the num_Ney_2004 conj_and_Och_Ney num_Marcu_2003 dep_Koehn_Ney dep_Koehn_Och conj_and_Koehn_Marcu conj_and_Koehn_Och dep_content_Marcu dep_content_Och dep_content_Koehn poss_content_their prep_of_independently_content advmod_reorders_independently dobj_reorders_phrases nsubj_reorders_that rcmod_model_reorders nn_model_distortion amod_model_simple det_model_a conj_and_have_have dobj_have_model nsubj_have_some num_Byrne_2006 conj_and_Kumar_Byrne conj_and_Kumar_Deng num_Ney_2004 dep_Zens_Byrne dep_Zens_Deng dep_Zens_Kumar conj_and_Zens_Ney dep_reordering_Ney dep_reordering_Zens neg_reordering_no dep_perform_have dep_perform_have dobj_perform_reordering nsubj_perform_models prep_above_perform_level det_models_some nn_level_phrase det_level_the
N04-1033	N03-1017	o	In -LRB- Koehn et al. 2003 -RRB- various aspects of phrase-based systems are compared e.g. the phrase extraction method the underlying word alignment model or the maximum phrase length	nn_length_phrase nn_length_maximum det_length_the nn_model_alignment nn_model_word amod_model_underlying det_model_the conj_or_method_length conj_or_method_model nn_method_extraction nn_method_phrase det_method_the pobj_e.g._length pobj_e.g._model pobj_e.g._method prep_compared_e.g. auxpass_compared_are nsubjpass_compared_aspects prep_compared_In amod_systems_phrase-based prep_of_aspects_systems amod_aspects_various nn_al._et amod_Koehn_2003 dep_Koehn_al. dep_In_Koehn
N04-1035	N03-1017	p	Along this line -LRB- Koehn et al. 2003 -RRB- present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance the ability to translate nonconstituent phrases -LRB- such as there are note that and according to -RRB- turns out to be critical and pervasive	conj_and_critical_pervasive cop_critical_be aux_critical_to xcomp_turns_pervasive xcomp_turns_critical prt_turns_out nsubj_turns_that prepc_according_to_that_to cc_that_and ccomp_note_turns dep_are_note expl_are_there mark_are_as mwe_as_such rcmod_phrases_are amod_phrases_nonconstituent dobj_translate_phrases aux_translate_to vmod_ability_translate det_ability_the dep_performance_ability nn_performance_translation amod_performance_poor dep_yields_performance dep_constituents_yields dep_syntactic_constituents amod_translation_phrasal prep_to_restricting_syntactic dobj_restricting_translation dep_that_restricting prep_evidence_that amod_evidence_convincing amod_evidence_present dep_evidence_Koehn prep_along_evidence_line dep_al._2003 nn_al._et advmod_Koehn_al. det_line_this
N04-4026	N03-1017	p	1 Introduction In recent years phrase-based systems for statistical machine translation -LRB- Och et al. 1999 Koehn et al. 2003 Venugopal et al. 2003 -RRB- have delivered state-of-the-art performance on standard translation tasks	nn_tasks_translation amod_tasks_standard amod_performance_state-of-the-art prep_on_delivered_tasks dobj_delivered_performance aux_delivered_have nsubj_delivered_systems num_Venugopal_2003 nn_Venugopal_al. nn_Venugopal_et num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Venugopal conj_Och_Koehn appos_Och_1999 dep_Och_al. nn_Och_et nn_translation_machine amod_translation_statistical dep_systems_Och prep_for_systems_translation amod_systems_phrase-based amod_years_recent rcmod_Introduction_delivered prep_in_Introduction_years num_Introduction_1
N06-1002	N03-1017	o	As an additional baseline we compare against a phrasal SMT decoder Pharaoh -LRB- Koehn et al. 2003 -RRB-	advmod_2003_al. nn_al._et num_Koehn_2003 appos_Pharaoh_Koehn appos_decoder_Pharaoh nn_decoder_SMT amod_decoder_phrasal det_decoder_a prep_against_compare_decoder nsubj_compare_we prep_as_compare_baseline amod_baseline_additional det_baseline_an
N06-1002	N03-1017	o	We used the heuristic combination described in -LRB- Och and Ney 2003 -RRB- and extracted phrasal translation pairs from this combined alignment as described in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in mark_described_as amod_alignment_combined det_alignment_this nn_pairs_translation amod_pairs_phrasal amod_pairs_extracted num_Ney_2003 prep_from_Och_alignment conj_and_Och_pairs conj_and_Och_Ney advcl_described_described prep_in_described_pairs prep_in_described_Ney prep_in_described_Och vmod_combination_described nn_combination_heuristic det_combination_the dobj_used_combination nsubj_used_We
N06-1003	N03-1017	p	2 The Problem of Coverage in SMT Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn appos_Marcu_2002 conj_and_Marcu_Wong dep_translation_Wong dep_translation_Marcu amod_translation_phrase-based prep_of_introduction_translation det_introduction_the nn_quality_translation prep_in_advances_quality amod_advances_considerable prep_with_made_introduction dobj_made_advances nsubj_made_Problem nn_translation_machine amod_translation_Statistical nn_translation_SMT prep_in_Coverage_translation prep_of_Problem_Coverage det_Problem_The num_Problem_2
N06-1004	N03-1017	o	Phrase tables were learned from the training corpus using the diag-and method -LRB- Koehn et al. 2003 -RRB- and using IBM model 2 to produce initial word alignments -LRB- these authors found this worked as well as IBM4 -RRB-	conj_and_worked_IBM4 nsubj_worked_this ccomp_found_IBM4 ccomp_found_worked nsubj_found_authors det_authors_these nn_alignments_word amod_alignments_initial dobj_produce_alignments aux_produce_to num_model_2 nn_model_IBM parataxis_using_found vmod_using_produce dobj_using_model nsubjpass_using_tables amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn nn_method_diag-and det_method_the dobj_using_method nn_corpus_training det_corpus_the conj_and_learned_using xcomp_learned_using prep_from_learned_corpus auxpass_learned_were nsubjpass_learned_tables nn_tables_Phrase
N06-1004	N03-1017	o	1 Introduction Defining SCMs The work presented here was done in the context of phrase-based MT -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_MT_phrase-based prep_of_context_MT det_context_the prep_in_done_context auxpass_done_was nsubjpass_done_work advmod_presented_here vmod_work_presented det_work_The rcmod_SCMs_done dep_Defining_Koehn dobj_Defining_SCMs dep_Introduction_Defining num_Introduction_1
N06-1013	N03-1017	o	Based on the observations in -LRB- Koehn et al. 2003 -RRB- we also limited the phrase length to 3 for computational reasons	amod_reasons_computational nn_length_phrase det_length_the prep_for_limited_reasons prep_to_limited_3 dobj_limited_length advmod_limited_also nsubj_limited_we pobj_limited_observations prepc_based_on_limited_on amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_observations_in det_observations_the
N06-1013	N03-1017	n	For comparison purposes three additional heuristically-induced alignments are generated for each system -LRB- 1 -RRB- Intersection of both directions -LRB- Aligner -LRB- int -RRB- -RRB- -LRB- 2 -RRB- Union of both directions -LRB- Aligner -LRB- union -RRB- -RRB- and -LRB- 3 -RRB- The previously bestknown heuristic combination approach called growdiag-final -LRB- Koehn et al. 2003 -RRB- -LRB- Aligner -LRB- gdf -RRB- -RRB-	appos_Aligner_gdf dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_growdiag-final_Aligner dep_growdiag-final_Koehn acomp_called_growdiag-final nsubj_called_approach dep_called_3 nn_approach_combination nn_approach_heuristic amod_approach_bestknown det_approach_The advmod_bestknown_previously appos_Aligner_union det_directions_both dep_Union_Aligner prep_of_Union_directions dep_Union_2 appos_Aligner_int dep_directions_Aligner preconj_directions_both conj_and_Intersection_called conj_and_Intersection_Union prep_of_Intersection_directions dep_1_called dep_1_Union dep_1_Intersection det_system_each dep_generated_1 prep_for_generated_system auxpass_generated_are nsubjpass_generated_alignments prep_for_generated_purposes amod_alignments_heuristically-induced amod_alignments_additional num_alignments_three nn_purposes_comparison
N06-1013	N03-1017	o	1 Introduction Word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation -LRB- MT -RRB- -LRB- Brown et al. 1993 Och and Ney 2003 Koehn et al. 2003 -RRB- but also has been shown useful for other applications such as construction of bilingual lexicons word-sense disambiguation projection of resources and crosslanguage information retrieval	nn_retrieval_information nn_retrieval_crosslanguage prep_of_projection_resources amod_disambiguation_word-sense conj_and_lexicons_retrieval conj_and_lexicons_projection conj_and_lexicons_disambiguation amod_lexicons_bilingual prep_of_construction_retrieval prep_of_construction_projection prep_of_construction_disambiguation prep_of_construction_lexicons prep_such_as_applications_construction amod_applications_other prep_for_useful_applications acomp_shown_useful auxpass_shown_been aux_shown_has advmod_shown_also nn_al._et nn_al._Koehn conj_and_Och_2003 conj_and_Och_Ney amod_Brown_2003 dep_Brown_al. dep_Brown_2003 dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_translation_MT nn_translation_machine amod_translation_statistical dep_step_Brown prep_of_step_translation amod_step_intermediate det_step_an advmod_step_usually nsubj_step_otheris det_otheris_each prep_of_translations_step cop_translations_are nsubj_translations_that rcmod_sentences_translations num_sentences_two prep_between_words_sentences amod_words_corresponding conj_but_alignmentdetection_shown prep_of_alignmentdetection_words nn_alignmentdetection_Word nn_alignmentdetection_Introduction num_alignmentdetection_1 dep_``_shown dep_``_alignmentdetection
N06-1014	N03-1017	o	Using GIZA + + model 4 alignments and Pharaoh -LRB- Koehn et al. 2003 -RRB- we achieved a BLEU score of 0.3035	prep_of_score_0.3035 nn_score_BLEU det_score_a dobj_achieved_score nsubj_achieved_we vmod_achieved_Using amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et num_alignments_4 nn_alignments_model dep_alignments_+ conj_and_GIZA_Pharaoh conj_+_GIZA_alignments dep_Using_Koehn dobj_Using_Pharaoh dobj_Using_alignments dobj_Using_GIZA
N06-1014	N03-1017	o	1 Introduction Word alignment is an important component of a complete statistical machine translation pipeline -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_pipeline_translation nn_pipeline_machine amod_pipeline_statistical amod_pipeline_complete det_pipeline_a dep_component_Koehn prep_of_component_pipeline amod_component_important det_component_an cop_component_is nsubj_component_alignment nn_alignment_Word nn_alignment_Introduction num_alignment_1
N06-1015	N03-1017	p	We view this as a particularly promising aspect of our work given that phrase-based systems such as Pharaoh -LRB- Koehn et al. 2003 -RRB- perform better with higher recall alignments	nn_alignments_recall amod_alignments_higher prep_with_better_alignments dobj_perform_better nsubj_perform_systems mark_perform_that amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharaoh_Koehn prep_such_as_systems_Pharaoh amod_systems_phrase-based pcomp_given_perform poss_work_our prep_of_aspect_work amod_aspect_promising det_aspect_a advmod_promising_particularly prep_view_given prep_as_view_aspect dobj_view_this nsubj_view_We
N06-1031	N03-1017	o	1 Introduction Recent work in statistical machine translation -LRB- MT -RRB- has sought to overcome the limitations of phrasebased models -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB- by making use of syntactic information	amod_information_syntactic prep_of_use_information dobj_making_use dep_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong appos_models_Koehn appos_models_Wong appos_models_Marcu amod_models_phrasebased prep_of_limitations_models det_limitations_the prepc_by_overcome_making dobj_overcome_limitations aux_overcome_to xcomp_sought_overcome aux_sought_has nsubj_sought_work appos_translation_MT nn_translation_machine amod_translation_statistical prep_in_work_translation amod_work_Recent nn_work_Introduction num_work_1 ccomp_``_sought
N06-1032	N03-1017	o	1 Introduction Recent approaches to statistical machine translation -LRB- SMT -RRB- piggyback on the central concepts of phrasebased SMT -LRB- Och et al. 1999 Koehn et al. 2003 -RRB- and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process	nn_process_translation det_process_the prep_in_knowledge_process amod_knowledge_syntactic dobj_incorporating_knowledge poss_shortcomings_its prep_of_some_shortcomings prepc_by_improve_incorporating dobj_improve_some aux_improve_to vmod_attempt_improve nn_attempt_time amod_attempt_same det_attempt_the num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn appos_Och_1999 dep_Och_al. nn_Och_et appos_SMT_Och amod_SMT_phrasebased prep_of_concepts_SMT amod_concepts_central det_concepts_the nn_piggyback_translation appos_translation_SMT nn_translation_machine amod_translation_statistical prep_at_approaches_attempt prep_on_approaches_concepts prep_to_approaches_piggyback conj_and_approaches_approaches amod_approaches_Recent dep_Introduction_approaches dep_Introduction_approaches num_Introduction_1
N07-1007	N03-1017	p	Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words and rely on general-purpose phrasal translations and target language models to generate these elements -LRB- e.g. Och and Ney 2002 Koehn et al. 2003 Quirk et al. 2005 Chiang 2005 Galley et al. 2006 -RRB-	num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Chiang_Galley num_Chiang_2005 num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Chiang conj_and_Och_Quirk conj_and_Och_Koehn conj_and_Och_2002 conj_and_Och_Ney dep_e.g._Quirk dep_e.g._Koehn dep_e.g._2002 dep_e.g._Ney dep_e.g._Och ccomp_-LRB-_e.g. det_elements_these dobj_generate_elements aux_generate_to nn_models_language nn_models_target conj_and_translations_models amod_translations_phrasal amod_translations_general-purpose xcomp_rely_generate prep_on_rely_models prep_on_rely_translations nsubj_rely_systems amod_words_content amod_way_same det_way_the advmod_way_exactly amod_elements_grammatical conj_and_treat_rely prep_as_treat_words prep_in_treat_way dobj_treat_elements nsubj_treat_systems nn_systems_SMT amod_systems_stateof-the-art amod_systems_Most
N07-1022	N03-1017	o	In this paper we present results on using a recent phrase-based SMT system PHARAOH -LRB- Koehn et al. 2003 -RRB- for NLG .1 Although moderately effec1We also tried IBM Model 4/REWRITE -LRB- Germann 2003 -RRB- a word-based SMT system but it gave much worse results	amod_results_worse advmod_worse_much dobj_gave_results nsubj_gave_it conj_but_system_gave nn_system_SMT amod_system_word-based det_system_a amod_Germann_2003 dep_4/REWRITE_Germann nn_4/REWRITE_Model nn_4/REWRITE_IBM dep_tried_gave dep_tried_system dobj_tried_4/REWRITE advmod_tried_also nsubj_tried_effec1We mark_tried_Although advmod_effec1We_moderately num_NLG_.1 amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et advcl_PHARAOH_tried prep_for_PHARAOH_NLG dep_PHARAOH_Koehn ccomp_,_PHARAOH nn_system_SMT amod_system_phrase-based amod_system_recent det_system_a dobj_using_system prepc_on_present_using dobj_present_results nsubj_present_we prep_in_present_paper det_paper_this ccomp_``_present
N07-1022	N03-1017	n	Like WASP1 the phrase extraction algorithm of PHARAOH is based on the output of a word alignment model such as GIZA + + -LRB- Koehn et al. 2003 -RRB- which performs poorly when applied directly to MRLs -LRB- Section 3.2 -RRB-	num_Section_3.2 appos_MRLs_Section prep_to_applied_MRLs advmod_applied_directly advmod_applied_when advcl_performs_applied advmod_performs_poorly nsubj_performs_which amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_GIZA_Koehn conj_+_GIZA_+ rcmod_model_performs prep_such_as_model_+ prep_such_as_model_GIZA nn_model_alignment nn_model_word det_model_a prep_of_output_model det_output_the prep_on_based_output auxpass_based_is nsubjpass_based_algorithm prep_like_based_WASP1 prep_of_algorithm_PHARAOH nn_algorithm_extraction nn_algorithm_phrase det_algorithm_the
N07-1022	N03-1017	o	Toremedythis situation we can borrow the probabilistic model of PHARAOH and define the parsing model as Pr -LRB- d | e -LRB- d -RRB- -RRB- = productdisplay dd w -LRB- r -LRB- d -RRB- -RRB- -LRB- 4 -RRB- which is the product of the weights of the rules used in a derivation d The rule weight w -LRB- X -RRB- is in turn defined as P -LRB- | -RRB- 1P -LRB- | -RRB- 2Pw -LRB- | -RRB- 3Pw -LRB- | -RRB- 4 exp -LRB- | | -RRB- 5 where P -LRB- | -RRB- and P -LRB- | -RRB- are the relative frequencies of and and Pw -LRB- | -RRB- and Pw -LRB- | -RRB- are 176 the lexical weights -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_weights_Koehn amod_weights_lexical det_weights_the num_weights_176 cop_weights_are nsubj_weights_3Pw appos_Pw_| appos_Pw_| conj_and_of_Pw conj_and_of_Pw prep_frequencies_Pw prep_frequencies_Pw prep_frequencies_of amod_frequencies_relative det_frequencies_the cop_frequencies_are nsubj_frequencies_P nsubj_frequencies_P advmod_frequencies_where appos_P_| conj_and_P_P appos_P_| rcmod_5_frequencies num_|_| amod_exp_5 appos_exp_| num_exp_4 dep_3Pw_exp appos_3Pw_| nn_3Pw_2Pw appos_2Pw_| nn_2Pw_1P appos_1P_| nn_1P_P appos_P_| dep_as_weights prep_defined_as vmod_turn_defined prep_in_is_turn appos_w_X dep_weight_is appos_weight_w nn_weight_rule det_weight_The nn_d_derivation det_d_a prep_in_used_d vmod_rules_used det_rules_the prep_of_weights_rules det_weights_the dep_product_weight prep_of_product_weights det_product_the cop_product_is nsubj_product_which ccomp_4_product nn_d_r appos_w_d nn_w_dd nn_w_productdisplay dep_=_w appos_d_d dep_d_e num_d_| amod_Pr_= dep_Pr_d dep_as_4 dep_as_Pr nn_model_parsing det_model_the prep_define_as dobj_define_model nsubj_define_we prep_of_model_PHARAOH amod_model_probabilistic det_model_the conj_and_borrow_define dobj_borrow_model aux_borrow_can nsubj_borrow_we dep_borrow_situation nn_situation_Toremedythis
N07-1022	N03-1017	o	Following the phrase extraction phase in PHARAOH we eliminate word gaps by incorporating unaligned words as part of the extracted NL phrases -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_phrases_NL amod_phrases_extracted det_phrases_the prep_of_part_phrases amod_words_unaligned prep_as_incorporating_part dobj_incorporating_words nn_gaps_word dep_eliminate_Koehn prepc_by_eliminate_incorporating dobj_eliminate_gaps nsubj_eliminate_we prep_following_eliminate_phase prep_in_phase_PHARAOH nn_phase_extraction nn_phase_phrase det_phase_the
N07-1022	N03-1017	o	3.1 Generation using PHARAOH PHARAOH -LRB- Koehn et al. 2003 -RRB- is an SMT system that uses phrases as basic translation units	nn_units_translation amod_units_basic prep_as_uses_units dobj_uses_phrases nsubj_uses_that rcmod_system_uses nn_system_SMT det_system_an cop_system_is nsubj_system_Generation amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_PHARAOH_Koehn nn_PHARAOH_PHARAOH dobj_using_PHARAOH vmod_Generation_using num_Generation_3.1
N07-1022	N03-1017	o	These rules are learned using a word alignment model which finds an optimal mapping from words to MR predicates given a set of training sentences and their correct MRs. Word alignment models have been widely used for lexical acquisition in SMT -LRB- Brown et al. 1993 Koehn et al. 2003 -RRB-	nn_al._et nn_al._Koehn amod_Brown_2003 dep_Brown_al. amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_SMT_Brown prep_in_acquisition_SMT amod_acquisition_lexical prep_for_used_acquisition advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_model nn_models_alignment nn_models_Word nn_models_MRs. amod_models_correct poss_models_their nn_sentences_training conj_and_set_models prep_of_set_sentences det_set_a pobj_given_models pobj_given_set prep_predicates_given prep_to_mapping_MR prep_from_mapping_words amod_mapping_optimal det_mapping_an dep_finds_predicates dobj_finds_mapping nsubj_finds_which rcmod_model_finds nn_model_alignment nn_model_word det_model_a ccomp_using_used xcomp_learned_using auxpass_learned_are nsubjpass_learned_rules det_rules_These ccomp_``_learned
N07-1061	N03-1017	o	2 Phrase-based SMT We use a phrase-based SMT system Pharaoh -LRB- Koehn et al. 2003 Koehn 2004 -RRB- which is based on a log-linear formulation -LRB- Och and Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_formulation_Ney dep_formulation_Och amod_formulation_log-linear det_formulation_a prep_on_based_formulation auxpass_based_is nsubjpass_based_which dep_Koehn_2004 dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et rcmod_system_based appos_system_Koehn appos_system_Pharaoh nn_system_SMT amod_system_phrase-based det_system_a dobj_use_system nsubj_use_We rcmod_SMT_use amod_SMT_Phrase-based num_SMT_2 dep_``_SMT
N07-1061	N03-1017	o	For details on these feature functions please refer to -LRB- Koehn et al. 2003 Koehn 2004 Koehn et al. 2005 -RRB-	num_Koehn_2005 nn_Koehn_al. nn_Koehn_et dep_Koehn_Koehn num_Koehn_2004 dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_to_refer_Koehn aux_refer_please prep_for_refer_details nn_functions_feature det_functions_these prep_on_details_functions
N07-1061	N03-1017	o	That is phrases are heuristically extracted from word-level alignments produced by doing GIZA + + training on the corresponding parallel corpora -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_corpora_parallel amod_corpora_corresponding det_corpora_the prep_on_training_corpora pobj_+_training dep_GIZA_Koehn conj_+_GIZA_+ dobj_doing_+ dobj_doing_GIZA agent_produced_doing vmod_alignments_produced amod_alignments_word-level prep_from_extracted_alignments advmod_extracted_heuristically auxpass_extracted_are nsubjpass_extracted_phrases parataxis_extracted_is nsubj_is_That
N07-1061	N03-1017	o	The definitions of the phrase and lexical translation probabilities are as follows -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_follows_Koehn mark_follows_as advcl_are_follows nsubj_are_definitions nn_probabilities_translation amod_probabilities_lexical conj_and_phrase_probabilities det_phrase_the prep_of_definitions_probabilities prep_of_definitions_phrase det_definitions_The ccomp_``_are
N07-1062	N03-1017	o	Even a length limit of 3 as proposed by -LRB- Koehn et al. 2003 -RRB- would result in almost optimal translation quality	nn_quality_translation amod_quality_optimal advmod_optimal_almost prep_in_result_quality aux_result_would dep_result_proposed nsubj_result_limit amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_by_proposed_Koehn mark_proposed_as prep_of_limit_3 nn_limit_length det_limit_a advmod_limit_Even
N07-1062	N03-1017	o	We have investigated this and our results are in line with -LRB- Koehn et al. 2003 -RRB- showing that the translation quality does not improve if we utilize phrases beyond a certain length	amod_length_certain det_length_a prep_beyond_phrases_length dobj_utilize_phrases nsubj_utilize_we mark_utilize_if advcl_improve_utilize neg_improve_not aux_improve_does nsubj_improve_quality mark_improve_that nn_quality_translation det_quality_the ccomp_showing_improve dep_showing_Koehn amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prepc_with_line_showing prep_in_are_line poss_results_our conj_and_this_results dep_investigated_are dobj_investigated_results dobj_investigated_this aux_investigated_have nsubj_investigated_We ccomp_``_investigated
N07-1063	N03-1017	o	Grammar rules were induced with the syntaxbased SMT system SAMT described in -LRB- Zollmann and Venugopal 2006 -RRB- which requires initial phrase alignments that we generated with GIZA + + -LRB- Koehn et al. 2003 -RRB- and syntactic parse trees of the target training sentences generated by the Stanford Parser -LRB- D. Klein 2003 -RRB- pre-trained on the Penn Treebank	nn_Treebank_Penn det_Treebank_the prep_on_pre-trained_Treebank amod_Klein_2003 nn_Klein_D. amod_Parser_pre-trained dep_Parser_Klein nn_Parser_Stanford det_Parser_the agent_generated_Parser nn_sentences_training nn_sentences_target det_sentences_the prep_of_trees_sentences nn_trees_parse amod_trees_syntactic amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_GIZA_Koehn conj_+_GIZA_+ prep_with_generated_+ prep_with_generated_GIZA nsubj_generated_we mark_generated_that nn_alignments_phrase amod_alignments_initial ccomp_requires_generated dobj_requires_alignments nsubj_requires_which vmod_Zollmann_generated conj_and_Zollmann_trees rcmod_Zollmann_requires dep_Zollmann_2006 conj_and_Zollmann_Venugopal prep_in_described_trees prep_in_described_Venugopal prep_in_described_Zollmann vmod_SAMT_described nn_SAMT_system nn_SAMT_SMT amod_SAMT_syntaxbased det_SAMT_the prep_with_induced_SAMT auxpass_induced_were nsubjpass_induced_rules nn_rules_Grammar
N07-2007	N03-1017	o	The baseline we measure against in all of these experiments is the state-of-the-art grow-diag-final -LRB- gdf -RRB- alignment refinement heuristic commonly used in phrase-based SMT -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_SMT_phrase-based prep_in_used_SMT advmod_used_commonly dep_heuristic_Koehn vmod_heuristic_used nn_heuristic_refinement nn_heuristic_alignment amod_heuristic_grow-diag-final amod_heuristic_state-of-the-art det_heuristic_the cop_heuristic_is nsubj_heuristic_baseline dep_grow-diag-final_gdf det_experiments_these prep_of_all_experiments pobj_in_all pcomp_against_in prep_measure_against nsubj_measure_we rcmod_baseline_measure det_baseline_The
N07-2008	N03-1017	o	They have been employed in word sense disambiguation -LRB- Diab and Resnik 2002 -RRB- automatic construction of bilingual dictionaries -LRB- McEwan et al. 2002 -RRB- and inducing statistical machine translation models -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_models_translation nn_models_machine amod_models_statistical dobj_inducing_models amod_McEwan_2002 dep_McEwan_al. nn_McEwan_et amod_dictionaries_bilingual dep_construction_McEwan prep_of_construction_dictionaries amod_construction_automatic dep_Diab_2002 conj_and_Diab_Resnik conj_and_disambiguation_inducing conj_and_disambiguation_construction appos_disambiguation_Resnik appos_disambiguation_Diab nn_disambiguation_sense nn_disambiguation_word dep_employed_Koehn prep_in_employed_inducing prep_in_employed_construction prep_in_employed_disambiguation auxpass_employed_been aux_employed_have nsubjpass_employed_They
N07-2009	N03-1017	o	For comparison we use the MT training program GIZA + + -LRB- Och and Ney 2003 -RRB- the phrase-base decoder Pharaoh -LRB- Koehn et al. 2003 -RRB- and the wordbased decoder Rewrite -LRB- Germann 2003 -RRB-	amod_Germann_2003 dep_Rewrite_Germann amod_decoder_wordbased det_decoder_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharaoh_Koehn amod_decoder_phrase-base det_decoder_the num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_decoder conj_+_GIZA_Pharaoh conj_+_GIZA_decoder conj_+_GIZA_+ appos_program_Rewrite appos_program_decoder appos_program_Pharaoh appos_program_decoder appos_program_+ appos_program_GIZA nn_program_training nn_program_MT det_program_the dobj_use_program nsubj_use_we prep_for_use_comparison
N07-2015	N03-1017	o	Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_paradigm_Koehn amod_paradigm_main prep_as_incorporating_paradigm dobj_incorporating_phrases vmod_approach_incorporating amod_approach_log-linear det_approach_a prep_on_based_approach vmod_decoder_based det_decoder_a dobj_use_decoder nsubj_use_groups nn_groups_research amod_groups_Many
N07-2053	N03-1017	o	-LRB- 2006 -RRB- modified from -LRB- Koehn et al. 2003 -RRB- which is an average of pairwise word translation probabilities	nn_probabilities_translation nn_probabilities_word amod_probabilities_pairwise prep_of_average_probabilities det_average_an cop_average_is nsubj_average_which amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_from_Koehn prep_modified_from rcmod_2006_average vmod_2006_modified dep_''_2006
N07-2053	N03-1017	p	They provide pairs of phrases that are used to construct a large set of potential translations for each input sentence along with feature values associated with each phrase pair that are used to select the best translation from this set .1 The most widely used method for building phrase translation tables -LRB- Koehn et al. 2003 -RRB- selects from a word alignment of a parallel bilingual training corpus all pairs of phrases -LRB- up to a given length -RRB- that are consistent with the alignment	det_alignment_the prep_with_consistent_alignment cop_consistent_are nsubj_consistent_that advmod_consistent_up amod_length_given det_length_a prep_to_up_length dep_pairs_consistent prep_of_pairs_phrases det_pairs_all nn_corpus_training amod_corpus_bilingual amod_corpus_parallel det_corpus_a prep_of_alignment_corpus nn_alignment_word det_alignment_a dobj_selects_pairs prep_from_selects_alignment amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_tables_translation nn_tables_phrase dobj_building_tables dep_method_selects dep_method_Koehn prepc_for_method_building amod_method_used det_method_The advmod_used_widely advmod_widely_most num_set_.1 det_set_this amod_translation_best det_translation_the dep_select_method prep_from_select_set dobj_select_translation aux_select_to xcomp_used_select auxpass_used_are nsubjpass_used_that rcmod_pair_used nn_pair_phrase det_pair_each prep_with_associated_pair vmod_values_associated nn_values_feature nn_sentence_input det_sentence_each prep_for_translations_sentence amod_translations_potential prep_of_set_translations amod_set_large det_set_a pobj_construct_values prepc_along_with_construct_with dobj_construct_set aux_construct_to xcomp_used_construct auxpass_used_are nsubjpass_used_that rcmod_phrases_used prep_of_pairs_phrases dobj_provide_pairs nsubj_provide_They
N09-1013	N03-1017	o	3.2.2 Alignment Error Rate Since MT systems are usually built on the union of the two sets of alignments -LRB- Koehn et al. 2003 -RRB- we consider the union of alignments in the two directions as well as those in each direction	det_direction_each prep_in_those_direction num_directions_two det_directions_the conj_and_union_those prep_in_union_directions prep_of_union_alignments det_union_the dobj_consider_those dobj_consider_union nsubj_consider_we ccomp_consider_Rate amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_of_sets_alignments num_sets_two det_sets_the prep_of_union_sets det_union_the prep_on_built_union advmod_built_usually auxpass_built_are nsubjpass_built_systems mark_built_Since nn_systems_MT dep_Rate_Koehn advcl_Rate_built nn_Rate_Error nn_Rate_Alignment num_Rate_3.2.2
N09-1021	N03-1017	o	In particular we adopt the approach of phrase-based statistical machine translation -LRB- Koehn et al. 2003 Koehn and Hoang 2007 -RRB-	dep_Koehn_2007 conj_and_Koehn_Hoang dep_Koehn_Hoang dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_machine amod_translation_statistical amod_translation_phrase-based prep_of_approach_translation det_approach_the dep_adopt_Koehn dobj_adopt_approach nsubj_adopt_we prep_in_adopt_particular
N09-1029	N03-1017	o	We obtain aligned parallel sentences and the phrase table after the training of Moses which includes running GIZA + + -LRB- Och and Ney 2003 -RRB- grow-diagonal-final symmetrization and phrase extraction -LRB- Koehn et al. 2005 -RRB-	amod_Koehn_2005 dep_Koehn_al. nn_Koehn_et nn_extraction_phrase amod_symmetrization_grow-diagonal-final num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och dep_GIZA_Koehn conj_and_GIZA_extraction conj_+_GIZA_symmetrization conj_+_GIZA_+ dobj_running_extraction dobj_running_symmetrization dobj_running_+ dobj_running_GIZA xcomp_includes_running nsubj_includes_which rcmod_Moses_includes prep_of_training_Moses det_training_the nn_table_phrase det_table_the prep_after_sentences_training conj_and_sentences_table amod_sentences_parallel amod_sentences_aligned dep_sentences_obtain nsubj_sentences_We
N09-1029	N03-1017	o	For example in phrase-based SMT systems -LRB- Koehn et al. 2003 Koehn 2004 -RRB- distortion model is used in which reordering probabilities depend on relative positions of target side phrases between adjacent blocks	amod_blocks_adjacent nn_phrases_side nn_phrases_target prep_between_positions_blocks prep_of_positions_phrases amod_positions_relative prep_on_depend_positions nsubj_depend_probabilities prep_in_depend_which nn_probabilities_reordering rcmod_used_depend auxpass_used_is nsubjpass_used_model dep_used_Koehn prep_in_used_systems prep_for_used_example nn_model_distortion dep_Koehn_2004 dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_systems_SMT amod_systems_phrase-based
N09-1029	N03-1017	o	Therefore while phrase-based SMT moves from words to phrases as the basic unit of translation implying effective local reordering within phrases it suffers when determining phrase reordering especially when phrases are longer than three words -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et num_words_three dep_longer_Koehn prep_than_longer_words cop_longer_are nsubj_longer_phrases advmod_longer_when advmod_longer_especially dep_reordering_longer nn_reordering_phrase dobj_determining_reordering advmod_determining_when advcl_suffers_determining nsubj_suffers_it prep_within_reordering_phrases amod_reordering_local amod_reordering_effective dobj_implying_reordering prep_of_unit_translation amod_unit_basic det_unit_the rcmod_moves_suffers vmod_moves_implying prep_as_moves_unit prep_to_moves_phrases prep_from_moves_words nn_moves_SMT amod_moves_phrase-based pobj_while_moves dep_,_while dep_``_Therefore
N09-1046	N03-1017	o	Word alignment was carried out by running Giza + + implementation of IBM Model 4 initialized with 5 iterations of Model 1 5 of the HMM aligner and 3 iterations of Model 4 -LRB- Och and Ney 2003 -RRB- in both directions and then symmetrizing using the grow-diag-final-and heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_grow-diag-final-and det_heuristic_the dobj_using_heuristic xcomp_symmetrizing_using advmod_symmetrizing_then preconj_directions_both dep_Och_2003 conj_and_Och_Ney dep_Model_Ney dep_Model_Och num_Model_4 prep_of_iterations_Model num_iterations_3 nn_aligner_HMM det_aligner_the prep_of_5_aligner conj_and_Model_iterations amod_Model_5 num_Model_1 prep_of_iterations_iterations prep_of_iterations_Model num_iterations_5 prep_in_initialized_directions prep_with_initialized_iterations vmod_4_initialized conj_and_Model_symmetrizing num_Model_4 dep_IBM_symmetrizing dep_IBM_Model prep_of_implementation_IBM pobj_+_implementation conj_+_running_+ dobj_running_Giza agent_carried_+ agent_carried_running prt_carried_out auxpass_carried_was nsubjpass_carried_alignment nn_alignment_Word
N09-1046	N03-1017	o	Unfortunately determining the optimal segmentation is challenging typically requiring extensive experimentation -LRB- Koehn and Knight 2003 Habash and Sadat 2006 Chang et al. 2008 -RRB-	num_Chang_2008 nn_Chang_al. nn_Chang_et dep_Habash_Chang conj_and_Habash_2006 conj_and_Habash_Sadat dep_Koehn_2006 dep_Koehn_Sadat dep_Koehn_Habash appos_Koehn_2003 conj_and_Koehn_Knight dep_experimentation_Knight dep_experimentation_Koehn amod_experimentation_extensive dobj_requiring_experimentation advmod_requiring_typically ccomp_requiring_challenging cop_challenging_is csubj_challenging_determining advmod_challenging_Unfortunately amod_segmentation_optimal det_segmentation_the dobj_determining_segmentation
N09-1046	N03-1017	o	The features used by the decoder were the English language model log probability logf -LRB- e | f -RRB- the lexical translation log probabilities in both directions -LRB- Koehn et al. 2003 -RRB- and a word count feature	nn_feature_count nn_feature_word det_feature_a amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et det_directions_both dep_probabilities_Koehn prep_in_probabilities_directions nn_probabilities_log nn_probabilities_translation amod_probabilities_lexical det_probabilities_the dep_f_| dep_|_e conj_and_logf_feature conj_and_logf_probabilities dep_logf_f dep_probability_feature dep_probability_probabilities dep_probability_logf nn_probability_log nn_probability_model nn_probability_language amod_probability_English det_probability_the cop_probability_were nsubj_probability_features det_decoder_the agent_used_decoder vmod_features_used det_features_The
N09-2024	N03-1017	o	Typically a phrase-based SMT system includes a feature that scores phrase pairs using lexical weights -LRB- Koehn et al. 2003 -RRB- which are computed for two directions source to target and target to source	conj_and_target_target prep_to_source_source prep_to_source_target prep_to_source_target num_directions_two prep_for_computed_directions auxpass_computed_are nsubjpass_computed_which amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_weights_lexical dobj_using_weights dep_pairs_source rcmod_pairs_computed dep_pairs_Koehn vmod_pairs_using nn_pairs_phrase nn_pairs_scores det_pairs_that dep_feature_pairs det_feature_a dobj_includes_feature nsubj_includes_system advmod_includes_Typically nn_system_SMT amod_system_phrase-based det_system_a
N09-2055	N03-1017	o	Each model can represent an important feature for the translation such as phrase-based language or lexical models -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_models_Koehn amod_models_lexical amod_models_language amod_models_phrase-based conj_or_phrase-based_lexical conj_or_phrase-based_language det_translation_the prep_such_as_feature_models prep_for_feature_translation amod_feature_important det_feature_an dobj_represent_feature aux_represent_can nsubj_represent_model det_model_Each ccomp_``_represent
N09-3016	N03-1017	o	The transcription probabilities can then be easily learnt from the alignments induced by GIZA + + using a scoring function -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_function_scoring det_function_a dep_using_Koehn dobj_using_function conj_+_GIZA_+ agent_induced_+ agent_induced_GIZA vmod_alignments_induced det_alignments_the xcomp_learnt_using prep_from_learnt_alignments advmod_learnt_easily auxpass_learnt_be advmod_learnt_then aux_learnt_can nsubjpass_learnt_probabilities nn_probabilities_transcription det_probabilities_The
N09-3016	N03-1017	o	We used minimum error rate training -LRB- Och 2003 -RRB- and the A * beam search decoder implemented by Koehn -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Koehn_Koehn agent_implemented_Koehn vmod_decoder_implemented nn_decoder_search nn_decoder_beam dep_decoder_* nn_decoder_A det_decoder_the amod_Och_2003 conj_and_training_decoder dep_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_used_decoder dobj_used_training nsubj_used_We
N09-3016	N03-1017	o	5.1 ExploringtheParameters Theparameterswhichhaveamajorinuenceonthe performance of a phrase-based SMT model are the alignment heuristics the maximum phrase length -LRB- MPR -RRB- and the order of the language model -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_model_language det_model_the prep_of_order_model det_order_the appos_length_MPR nn_length_phrase nn_length_maximum det_length_the dep_heuristics_Koehn conj_and_heuristics_order conj_and_heuristics_length nn_heuristics_alignment det_heuristics_the cop_heuristics_are nsubj_heuristics_performance nn_model_SMT amod_model_phrase-based det_model_a prep_of_performance_model nn_performance_Theparameterswhichhaveamajorinuenceonthe nn_performance_ExploringtheParameters num_performance_5.1
P04-1023	N03-1017	o	The phrase-based decoder extracts phrases from the word alignments produced by GIZA + + and computes translation probabilities based on the frequency of one phrase being aligned with another -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_another_Koehn prep_with_aligned_another auxpass_aligned_being vmod_phrase_aligned num_phrase_one prep_of_frequency_phrase det_frequency_the pobj_probabilities_frequency prepc_based_on_probabilities_on nn_probabilities_translation dobj_computes_probabilities conj_+_GIZA_+ agent_produced_+ agent_produced_GIZA vmod_alignments_produced nn_alignments_word det_alignments_the conj_and_phrases_computes prep_from_phrases_alignments nn_phrases_extracts nn_phrases_decoder amod_phrases_phrase-based det_phrases_The dep_``_computes dep_``_phrases
P04-1060	N03-1017	o	For each span in the chart we get a weight factor that is multiplied with the parameter-based expectations .9 4 Experiments We applied GIZA + + -LRB- Al-Onaizan et al. 1999 Och and Ney 2003 -RRB- to word-align parts of the Europarl corpus -LRB- Koehn 2002 -RRB- for English and all other 10 languages	num_languages_10 amod_languages_other det_languages_all dep_Koehn_2002 appos_corpus_Koehn nn_corpus_Europarl det_corpus_the prep_for_parts_English prep_of_parts_corpus amod_parts_word-align dep_Och_2003 conj_and_Och_Ney prep_to_Al-Onaizan_parts dep_Al-Onaizan_Ney dep_Al-Onaizan_Och appos_Al-Onaizan_1999 dep_Al-Onaizan_al. nn_Al-Onaizan_et conj_and_GIZA_languages conj_+_GIZA_Al-Onaizan conj_+_GIZA_+ dobj_applied_languages dobj_applied_Al-Onaizan dobj_applied_+ dobj_applied_GIZA nsubj_applied_We rcmod_Experiments_applied num_Experiments_4 num_Experiments_.9 dep_expectations_Experiments amod_expectations_parameter-based det_expectations_the prep_with_multiplied_expectations auxpass_multiplied_is nsubjpass_multiplied_that rcmod_factor_multiplied nn_factor_weight det_factor_a dobj_get_factor nsubj_get_we prep_in_get_chart prep_for_get_span det_chart_the det_span_each
P04-1060	N03-1017	o	We use the Europarl corpus -LRB- Koehn 2002 -RRB- and the statistical word alignment was performed with the GIZA + + toolkit -LRB- Al-Onaizan et al. 1999 Och and Ney 2003 -RRB- .1 For the current experiments we assume no preexisting parser for any of the languages contrary to the information projection scenario	nn_scenario_projection nn_scenario_information det_scenario_the det_languages_the prep_of_any_languages prep_contrary_to_parser_scenario prep_for_parser_any amod_parser_preexisting neg_parser_no dobj_assume_parser nsubj_assume_we amod_experiments_current det_experiments_the rcmod_.1_assume prep_for_.1_experiments dep_.1_Ney dep_.1_Och num_Och_2003 conj_and_Och_Ney conj_al._.1 conj_al._1999 nn_al._et dep_Al-Onaizan_al. cc_toolkit_+ amod_GIZA_Al-Onaizan conj_+_GIZA_toolkit det_GIZA_the prep_with_performed_toolkit prep_with_performed_GIZA auxpass_performed_was nsubjpass_performed_alignment nn_alignment_word amod_alignment_statistical det_alignment_the dep_Koehn_2002 appos_corpus_Koehn nn_corpus_Europarl det_corpus_the conj_and_use_performed dobj_use_corpus nsubj_use_We
P04-1060	N03-1017	o	-LRB- Koehn et al. 2003 -RRB- show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only	amod_constituents_syntactic advmod_focusing_only prep_on_focusing_constituents prepc_than_better_focusing cop_better_is csubj_better_show amod_alignment_phrase-based prep_in_blocks_alignment csubj_blocks_exploiting mark_blocks_that amod_word_contiguous det_word_all dobj_exploiting_word ccomp_show_blocks nsubj_show_Koehn dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et
P04-1064	N03-1017	o	It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation -LRB- Och et al. 1999 -RRB- -LRB- Tillmann and Xia 2003 -RRB- -LRB- Koehn et al. 2003 sec	appos_Koehn_sec amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Tillmann_2003 conj_and_Tillmann_Xia amod_Och_1999 dep_Och_al. nn_Och_et nn_Translation_Machine amod_Translation_phrase-based conj_or_phrases_templates dep_identify_Koehn dep_identify_Xia dep_identify_Tillmann dep_identify_Och prep_in_identify_Translation dobj_identify_templates dobj_identify_phrases aux_identify_to dep_identify_order mark_identify_in dep_step_identify amod_step_first det_step_a prep_as_used_step advmod_used_typically auxpass_used_is nsubjpass_used_corpus mark_used_because amod_corpus_wordaligned det_corpus_a advcl_important_used cop_important_is nsubj_important_It
P05-1033	N03-1017	p	We compared a baseline system the state-of-the-art phrase-based system Pharaoh -LRB- Koehn et al. 2003 Koehn 2004a -RRB- against our system	poss_system_our appos_Koehn_2004a dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_against_Pharaoh_system appos_Pharaoh_Koehn dep_system_Pharaoh amod_system_phrase-based amod_system_state-of-the-art det_system_the nn_system_baseline det_system_a dep_compared_system dobj_compared_system nsubj_compared_We ccomp_``_compared
P05-1033	N03-1017	o	5.1 Baseline The baseline system we used for comparison was Pharaoh -LRB- Koehn et al. 2003 Koehn 2004a -RRB- as publicly distributed	advmod_distributed_publicly mark_distributed_as appos_Koehn_2004a dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharaoh_Koehn cop_Pharaoh_was nsubj_Pharaoh_system prep_for_used_comparison nsubj_used_we rcmod_system_used nn_system_baseline det_system_The dep_Baseline_distributed rcmod_Baseline_Pharaoh num_Baseline_5.1 dep_``_Baseline
P05-1033	N03-1017	o	Above the phrase level these models typically have a simple distortion model that reorders phrases independently of their content -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- or not at all -LRB- Zens and Ney 2004 Kumar et al. 2005 -RRB-	num_Kumar_2005 nn_Kumar_al. nn_Kumar_et dep_Zens_Kumar dep_Zens_2004 conj_and_Zens_Ney appos_all_Ney appos_all_Zens pobj_at_all neg_at_not num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney conj_or_content_at dep_content_Koehn dep_content_2004 dep_content_Ney dep_content_Och poss_content_their prep_of_independently_at prep_of_independently_content advmod_reorders_independently dobj_reorders_phrases nsubj_reorders_that rcmod_model_reorders nn_model_distortion amod_model_simple det_model_a dobj_have_model advmod_have_typically nsubj_have_models prep_above_have_level det_models_these nn_level_phrase det_level_the
P05-1033	N03-1017	o	When we run a phrase-based system Pharaoh -LRB- Koehn et al. 2003 Koehn 2004a -RRB- on this sentence -LRB- using the experimental setup described below -RRB- we get the following phrases with translations -LRB- 4 -RRB- -LSB- Aozhou -RSB- -LSB- shi -RSB- -LSB- yu -RSB- -LSB- Bei Han -RSB- -LSB- you -RSB- -LSB- bangjiao -RSB- 1 -LSB- de shaoshu guojia zhiyi -RSB- -LSB- Australia -RSB- -LSB- is -RSB- -LSB- dipl	nn_zhiyi_guojia nn_zhiyi_shaoshu nn_zhiyi_de dep_bangjiao_dipl dep_bangjiao_is appos_bangjiao_Australia dep_bangjiao_zhiyi num_bangjiao_1 nn_Han_Bei dep_Aozhou_bangjiao dep_Aozhou_you dep_Aozhou_Han dep_Aozhou_yu dep_Aozhou_shi dep_4_Aozhou prep_with_phrases_translations prep_following_the_phrases dep_get_4 dobj_get_the nsubj_get_we ccomp_get_Pharaoh advcl_get_run advmod_described_below vmod_setup_described amod_setup_experimental det_setup_the dobj_using_setup det_sentence_this appos_Koehn_2004a dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharaoh_using prep_on_Pharaoh_sentence appos_Pharaoh_Koehn amod_system_phrase-based det_system_a dobj_run_system nsubj_run_we advmod_run_When
P05-1033	N03-1017	o	For our experiments we used the following features analogous to Pharaohs default feature set P -LRB- | -RRB- and P -LRB- | -RRB- the latter of which is not found in the noisy-channel model but has been previously found to be a helpful feature -LRB- Och and Ney 2002 -RRB- the lexical weights Pw -LRB- | -RRB- and Pw -LRB- | -RRB- -LRB- Koehn et al. 2003 -RRB- which estimate how well the words in translate the words in 2 a phrase penalty exp -LRB- 1 -RRB- which allows the model to learn a preference for longer or shorter derivations analogous to Koehns phrase penalty -LRB- Koehn 2003 -RRB-	amod_Koehn_2003 appos_penalty_Koehn nn_penalty_phrase nn_penalty_Koehns prep_to_analogous_penalty amod_derivations_analogous amod_derivations_shorter amod_derivations_longer conj_or_longer_shorter prep_for_preference_derivations det_preference_a dobj_learn_preference aux_learn_to det_model_the xcomp_allows_learn dobj_allows_model nsubj_allows_which rcmod_exp_allows appos_exp_1 nn_exp_penalty nn_exp_phrase det_exp_a dep_2_exp det_words_the prep_translate_in dobj_translate_words prepc_in_words_translate det_words_the pobj_well_words advmod_well_how prep_estimate_well nsubj_estimate_which dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_Pw_| conj_and_Pw_Pw appos_Pw_| rcmod_weights_estimate appos_weights_Koehn dep_weights_Pw dep_weights_Pw amod_weights_lexical det_weights_the dep_Och_2002 conj_and_Och_Ney dep_feature_Ney dep_feature_Och amod_feature_helpful det_feature_a cop_feature_be aux_feature_to xcomp_found_feature advmod_found_previously auxpass_found_been aux_found_has amod_model_noisy-channel det_model_the prep_in_found_model neg_found_not auxpass_found_is nsubjpass_found_latter prep_of_latter_which det_latter_the appos_P_| conj_but_P_2 conj_but_P_weights conj_but_P_found conj_and_P_found conj_and_P_P appos_P_| nn_set_feature nn_set_default nn_set_Pharaohs prep_to_analogous_set amod_features_following det_features_the dep_used_2 dep_used_weights dep_used_found dep_used_found dep_used_P dep_used_P dep_used_analogous dobj_used_features nsubj_used_we prep_for_used_experiments poss_experiments_our rcmod_``_used
P05-1033	N03-1017	o	To do this we first identify initial phrase pairs using the same criterion as previous systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- Definition 1	num_Definition_1 num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_systems_Koehn dep_systems_2004 dep_systems_Ney dep_systems_Och amod_systems_previous prep_as_criterion_systems amod_criterion_same det_criterion_the dobj_using_criterion nn_pairs_phrase amod_pairs_initial dep_identify_Definition xcomp_identify_using dobj_identify_pairs advmod_identify_first nsubj_identify_we advcl_identify_do dobj_do_this aux_do_To
P05-1066	N03-1017	o	In experiments with the system of -LRB- Koehn et al. 2003 -RRB- we have found that in practice a large number of complete translations are completely monotonic -LRB- i.e. have a0 skips -RRB- suggesting that the system has difficulty learning exactly what points in the translation should allow reordering	dobj_allow_reordering aux_allow_should nsubj_allow_monotonic det_translation_the det_points_what advmod_points_exactly prep_in_learning_translation dobj_learning_points vmod_difficulty_learning dobj_has_difficulty nsubj_has_system mark_has_that det_system_the ccomp_suggesting_has nn_skips_a0 dobj_have_skips dep_i.e._have vmod_monotonic_suggesting dep_monotonic_i.e. advmod_monotonic_completely cop_monotonic_are nsubj_monotonic_number prep_in_monotonic_practice mark_monotonic_that amod_translations_complete prep_of_number_translations amod_number_large det_number_a ccomp_found_allow aux_found_have nsubj_found_we prep_in_found_experiments amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_of_system_Koehn det_system_the prep_with_experiments_system
P05-1066	N03-1017	o	Our baseline is the phrase-based MT system of -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_of_Koehn prep_system_of nn_system_MT amod_system_phrase-based det_system_the cop_system_is nsubj_system_baseline poss_baseline_Our
P05-1066	N03-1017	p	Results using the method show an improvement from 25.2 % Bleu score to 26.8 % Bleu score -LRB- a statistically significant improvement -RRB- using a phrase-based system -LRB- Koehn et al. 2003 -RRB- which has been shown in the past to be a highly competitive SMT system	nn_system_SMT amod_system_competitive det_system_a cop_system_be aux_system_to advmod_competitive_highly det_past_the xcomp_shown_system prep_in_shown_past auxpass_shown_been aux_shown_has nsubjpass_shown_which amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_system_phrase-based det_system_a ccomp_using_shown dep_using_Koehn dobj_using_system amod_improvement_significant det_improvement_a advmod_significant_statistically appos_score_improvement nn_score_Bleu amod_score_% number_%_26.8 nn_score_Bleu amod_score_% number_%_25.2 prep_to_improvement_score prep_from_improvement_score det_improvement_an vmod_show_using dobj_show_improvement nsubj_show_Results det_method_the dobj_using_method vmod_Results_using
P05-1066	N03-1017	p	More recently phrase-based models -LRB- Och et al. 1999 Marcu and Wong 2002 Koehn et al. 2003 -RRB- have been proposed as a highly successful alternative to the IBM models	nn_models_IBM det_models_the prep_to_alternative_models amod_alternative_successful det_alternative_a advmod_successful_highly prep_as_proposed_alternative auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models advmod_proposed_recently num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong dep_Och_Wong dep_Och_Marcu appos_Och_1999 dep_Och_al. nn_Och_et appos_models_Och amod_models_phrase-based advmod_recently_More
P05-1066	N03-1017	o	In this paper we use the phrase-based system of -LRB- Koehn et al. 2003 -RRB- as our underlying model	amod_model_underlying poss_model_our amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_of_Koehn prep_system_of amod_system_phrase-based det_system_the prep_as_use_model dobj_use_system nsubj_use_we prep_in_use_paper det_paper_this
P05-1066	N03-1017	o	Reranking methods have also been proposed as a method for using syntactic information -LRB- Koehn and Knight 2003 Och et al. 2004 Shen et al. 2004 -RRB-	num_Shen_2004 nn_Shen_al. nn_Shen_et num_Och_2004 nn_Och_al. nn_Och_et dep_Koehn_Shen conj_and_Koehn_Och conj_and_Koehn_2003 conj_and_Koehn_Knight appos_information_Och appos_information_2003 appos_information_Knight appos_information_Koehn amod_information_syntactic dobj_using_information prepc_for_method_using det_method_a prep_as_proposed_method auxpass_proposed_been advmod_proposed_also aux_proposed_have nsubjpass_proposed_methods nn_methods_Reranking ccomp_``_proposed
P05-1066	N03-1017	o	1 Introduction Recent research on statistical machine translation -LRB- SMT -RRB- has lead to the development of phrasebased systems -LRB- Och et al. 1999 Marcu and Wong 2002 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong dep_Och_Wong dep_Och_Marcu appos_Och_1999 dep_Och_al. nn_Och_et amod_systems_phrasebased prep_of_development_systems det_development_the dep_lead_Och prep_to_lead_development aux_lead_has nsubj_lead_research appos_translation_SMT nn_translation_machine amod_translation_statistical prep_on_research_translation amod_research_Recent nn_research_Introduction num_research_1 ccomp_``_lead
P05-1068	N03-1017	p	Recently various works have improved the quality of statistical machine translation systems by using phrase translation -LRB- Koehn et al. 2003 Marcu et al. 2002 Och et al. 1999 Och and Ney 2000 Zens et al. 2004 -RRB-	num_Zens_2004 nn_Zens_al. nn_Zens_et conj_and_Och_2000 conj_and_Och_Ney num_Och_1999 nn_Och_al. nn_Och_et num_Marcu_2002 nn_Marcu_al. nn_Marcu_et dep_Koehn_Zens dep_Koehn_2000 dep_Koehn_Ney dep_Koehn_Och dep_Koehn_Och dep_Koehn_Marcu appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_phrase dobj_using_translation nn_systems_translation nn_systems_machine amod_systems_statistical prep_of_quality_systems det_quality_the dep_improved_Koehn prepc_by_improved_using dobj_improved_quality aux_improved_have nsubj_improved_works advmod_improved_Recently amod_works_various
P05-1069	N03-1017	o	3.4 Lexical Weighting The lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to -LRB- Koehn et al. 2003 -RRB- but the lexical translation probability a27 a14a12a94 a29 a97a100a21 is derived from the block set itself rather than from a word alignment resulting in a simplified training	amod_training_simplified det_training_a prep_in_resulting_training nn_alignment_word det_alignment_a pobj_from_alignment conj_negcc_set_from dobj_set_itself prep_block_from prep_block_set det_block_the xcomp_derived_resulting prep_from_derived_block auxpass_derived_is nsubjpass_derived_a97a100a21 nn_a97a100a21_a29 nn_a97a100a21_a14a12a94 nn_a97a100a21_a27 nn_a97a100a21_probability nn_a97a100a21_translation amod_a97a100a21_lexical det_a97a100a21_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_to_Koehn conj_but_computed_derived prep_computed_to advmod_computed_similarly auxpass_computed_is nsubjpass_computed_a92a93a21 nn_a19a86a92a93a21_a14a12a91 nn_a19a86a92a93a21_a72 nn_a19a86a92a93a21_a9 nn_a19a86a92a93a21_block det_a19a86a92a93a21_the prep_of_a92a93a21_a19a86a92a93a21 nn_a92a93a21_a29 nn_a92a93a21_a14a12a91 nn_a92a93a21_a27 nn_a92a93a21_weight amod_a92a93a21_lexical det_a92a93a21_The rcmod_Weighting_derived rcmod_Weighting_computed amod_Weighting_Lexical num_Weighting_3.4 dep_``_Weighting
P05-1069	N03-1017	o	Two block sets are derived for each of the training sets using a phrase-pair selection algorithm similar to -LRB- Koehn et al. 2003 Tillmann and Xia 2003 -RRB-	dep_Tillmann_2003 conj_and_Tillmann_Xia dep_Koehn_Xia dep_Koehn_Tillmann appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_to_similar_Koehn amod_algorithm_similar nn_algorithm_selection amod_algorithm_phrase-pair det_algorithm_a dobj_using_algorithm xcomp_sets_using det_training_the prep_of_each_training dep_derived_sets prep_for_derived_each auxpass_derived_are nsubjpass_derived_sets nn_sets_block num_sets_Two ccomp_``_derived
P05-1069	N03-1017	o	2 Block Orientation Bigrams This section describes a phrase-based model for SMT similar to the models presented in -LRB- Koehn et al. 2003 Och et al. 1999 Tillmann and Xia 2003 -RRB-	dep_Tillmann_2003 conj_and_Tillmann_Xia dep_Och_Xia dep_Och_Tillmann num_Och_1999 nn_Och_al. nn_Och_et dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_presented_Koehn vmod_models_presented det_models_the prep_to_similar_models amod_SMT_similar prep_for_model_SMT amod_model_phrase-based det_model_a dobj_describes_model nsubj_describes_section det_section_This nn_section_Bigrams nn_section_Orientation nn_section_Block num_section_2 ccomp_``_describes
P05-1069	N03-1017	o	Lexical Weighting -LRB- e -RRB- the lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to -LRB- Koehn et al. 2003 -RRB- details are given in Section 3.4	num_Section_3.4 prep_in_given_Section auxpass_given_are nsubjpass_given_details amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_to_Koehn parataxis_computed_given prep_computed_to advmod_computed_similarly auxpass_computed_is nsubjpass_computed_a92a93a21 nn_a19a86a92a93a21_a14a12a91 nn_a19a86a92a93a21_a72 nn_a19a86a92a93a21_a9 nn_a19a86a92a93a21_block det_a19a86a92a93a21_the prep_of_a92a93a21_a19a86a92a93a21 nn_a92a93a21_a29 nn_a92a93a21_a14a12a91 nn_a92a93a21_a27 nn_a92a93a21_weight amod_a92a93a21_lexical det_a92a93a21_the dep_a92a93a21_e dep_Weighting_computed amod_Weighting_Lexical
P05-1074	N03-1017	o	Our method for identifying paraphrases is an extension of recent work in phrase-based statistical machine translation -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_machine amod_translation_statistical amod_translation_phrase-based amod_work_recent dep_extension_Koehn prep_in_extension_translation prep_of_extension_work det_extension_an cop_extension_is nsubj_extension_method dobj_identifying_paraphrases prepc_for_method_identifying poss_method_Our
P05-1074	N03-1017	o	Koehn -LRB- 2004 -RRB- Tillmann -LRB- 2003 -RRB- and Vogel et al.	nn_al._et nn_al._Vogel appos_Tillmann_2003 conj_and_Koehn_al. appos_Koehn_Tillmann appos_Koehn_2004 dep_``_al. dep_``_Koehn
P05-2016	N03-1017	o	Statistical Phrase-based Translation -LRB- Koehn et al. 2003 -RRB- Here phrase-based means subsequence-based as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases	amod_phrases_syntactic pobj_think_phrases prepc_as_of_think_as aux_think_would nsubj_think_we dobj_think_what prepc_to_relation_think det_relation_any dobj_have_relation aux_have_will nsubj_have_phrases mark_have_that det_model_the agent_learned_model vmod_phrases_learned det_phrases_the ccomp_guarantee_have neg_guarantee_no nsubj_is_guarantee expl_is_there mark_is_as advcl_subsequence-based_is dep_means_subsequence-based amod_means_phrase-based advmod_means_Here amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Translation_means appos_Translation_Koehn amod_Translation_Phrase-based amod_Translation_Statistical
P06-1009	N03-1017	o	Most current SMT systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- use a generative model for word alignment such as the freely available GIZA + + -LRB- Och and Ney 2003 -RRB- an implementation of the IBM alignment models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_alignment nn_models_IBM det_models_the dep_implementation_Brown prep_of_implementation_models det_implementation_an num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_implementation conj_+_GIZA_+ amod_GIZA_available det_GIZA_the advmod_available_freely prep_such_as_alignment_implementation prep_such_as_alignment_+ prep_such_as_alignment_GIZA nn_alignment_word amod_model_generative det_model_a prep_for_use_alignment dobj_use_model nsubj_use_systems num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney appos_systems_2004 appos_systems_Ney appos_systems_Och nn_systems_SMT amod_systems_current amod_systems_Most
P06-1066	N03-1017	o	One is distortion model -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- which penalizes translations according to their jump distance instead of their content	poss_content_their prep_instead_of_distance_content nn_distance_jump poss_distance_their pobj_penalizes_distance prepc_according_to_penalizes_to dobj_penalizes_translations nsubj_penalizes_which num_Koehn_2003 nn_Koehn_al. nn_Koehn_et rcmod_Och_penalizes dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_model_2004 dep_model_Ney dep_model_Och nn_model_distortion cop_model_is nsubj_model_One
P06-1067	N03-1017	o	However their decoder is outperformed by phrase-based decoders such as -LRB- Koehn 2004 -RRB- -LRB- Och et al. 1999 -RRB- and -LRB- Tillmann and Ney 2003 -RRB-	dep_Tillmann_2003 conj_and_Tillmann_Ney amod_Och_1999 dep_Och_al. nn_Och_et conj_and_Koehn_Ney conj_and_Koehn_Tillmann appos_Koehn_Och amod_Koehn_2004 prep_such_as_decoders_Tillmann prep_such_as_decoders_Koehn amod_decoders_phrase-based agent_outperformed_decoders auxpass_outperformed_is nsubjpass_outperformed_decoder advmod_outperformed_However poss_decoder_their
P06-1067	N03-1017	o	Similarly -LRB- Koehn et al. 2003 -RRB- propose a relative distortion model to be used with a phrase decoder	nn_decoder_phrase det_decoder_a prep_with_used_decoder auxpass_used_be aux_used_to vmod_model_used nn_model_distortion amod_model_relative det_model_a dobj_propose_model nsubj_propose_Koehn advmod_propose_Similarly dep_2003_al. nn_al._et num_Koehn_2003
P06-1077	N03-1017	o	5.1 Pharaoh The baseline system we used for comparison was Pharaoh -LRB- Koehn et al. 2003 Koehn 2004 -RRB- a freely available decoder for phrase-based translation models p -LRB- e | f -RRB- = p -LRB- f | e -RRB- pLM -LRB- e -RRB- LM pD -LRB- e f -RRB- D length -LRB- e -RRB- W -LRB- e -RRB- -LRB- 10 -RRB- We ran GIZA + + -LRB- Och and Ney 2000 -RRB- on the training corpus in both directions using its default setting and then applied the refinement rule diagand described in -LRB- Koehn et al. 2003 -RRB- to obtain a single many-to-many word alignment for each sentence pair	nn_pair_sentence det_pair_each prep_for_alignment_pair nn_alignment_word nn_alignment_many-to-many amod_alignment_single det_alignment_a dobj_obtain_alignment aux_obtain_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn xcomp_described_obtain prep_described_in vmod_diagand_described nn_diagand_rule nn_diagand_refinement det_diagand_the dobj_applied_diagand advmod_applied_then nn_setting_default poss_setting_its dobj_using_setting preconj_directions_both nn_corpus_training det_corpus_the num_Och_2000 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ prep_in_ran_directions dobj_ran_+ dobj_ran_GIZA nsubj_ran_We dep_W_e nn_W_length nn_length_D nn_length_pD dep_length_e nn_length_| dep_D_e dep_e_f nn_pD_LM dep_pD_e nn_pD_pLM nn_|_f conj_and_=_applied dep_=_using dep_=_ran dep_=_10 dep_=_e dep_=_W dep_=_p dep_=_f dep_=_p dep_f_| dep_|_e nn_models_translation amod_models_phrase-based prep_for_decoder_models amod_decoder_available det_decoder_a advmod_available_freely dep_Koehn_2004 dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et parataxis_Pharaoh_applied parataxis_Pharaoh_= appos_Pharaoh_decoder dep_Pharaoh_Koehn cop_Pharaoh_was nsubj_Pharaoh_Pharaoh prep_for_used_comparison nsubj_used_we rcmod_system_used nn_system_baseline det_system_The dep_Pharaoh_system num_Pharaoh_5.1
P06-1077	N03-1017	o	h1 -LRB- eI1 fJ1 -RRB- = log Kproductdisplay k = 1 N -LRB- z -RRB- -LRB- T -LRB- z -RRB- Tk -RRB- N -LRB- T -LRB- z -RRB- -RRB- h2 -LRB- eI1 fJ1 -RRB- = log Kproductdisplay k = 1 N -LRB- z -RRB- -LRB- T -LRB- z -RRB- Tk -RRB- N -LRB- S -LRB- z -RRB- -RRB- h3 -LRB- eI1 fJ1 -RRB- = log Kproductdisplay k = 1 lex -LRB- T -LRB- z -RRB- | S -LRB- z -RRB- -RRB- -LRB- T -LRB- z -RRB- Tk -RRB- h4 -LRB- eI1 fJ1 -RRB- = log Kproductdisplay k = 1 lex -LRB- S -LRB- z -RRB- | T -LRB- z -RRB- -RRB- -LRB- T -LRB- z -RRB- Tk -RRB- h5 -LRB- eI1 fJ1 -RRB- = K h6 -LRB- eI1 fJ1 -RRB- = log Iproductdisplay i = 1 p -LRB- ei | ei2 ei1 -RRB- h7 -LRB- eI1 fJ1 -RRB- = I 4When computing lexical weighting features -LRB- Koehn et al. 2003 -RRB- we take only terminals into account	advmod_terminals_only prep_into_take_account dobj_take_terminals nsubj_take_we amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_features_Koehn nn_features_weighting amod_features_lexical nn_features_computing nn_features_4When num_features_I dep_=_features appos_eI1_fJ1 rcmod_h7_take amod_h7_= dep_h7_eI1 amod_h7_p dep_ei2_ei1 nn_ei2_| nn_ei2_ei dep_p_ei2 num_p_1 dep_=_h7 dep_i_= dep_Iproductdisplay_i nn_Iproductdisplay_log dep_=_Iproductdisplay appos_eI1_fJ1 prep_h6_= dep_h6_eI1 nn_h6_K amod_h6_= amod_h6_= nn_h6_h4 appos_eI1_fJ1 dep_h5_eI1 nn_h5_k appos_T_Tk appos_T_z appos_T_z num_T_| dep_S_T appos_S_z dep_lex_S num_lex_1 dep_=_lex dep_k_T amod_k_= nn_k_Kproductdisplay nn_k_log dep_=_h5 appos_eI1_fJ1 dep_h4_eI1 nn_h4_k appos_T_Tk appos_T_z appos_S_z num_S_| dep_T_S appos_T_z dep_lex_T num_lex_1 dep_=_lex dep_k_T amod_k_= nn_k_Kproductdisplay nn_k_log dep_=_h6 appos_eI1_fJ1 appos_S_z nn_S_N nn_N_k appos_T_Tk appos_T_z appos_N_z num_N_1 dep_=_N dep_k_T amod_k_= nn_k_Kproductdisplay nn_k_log dep_=_h3 dep_=_S appos_eI1_fJ1 dep_h2_= dep_h2_eI1 amod_h2_= dep_h2_eI1 nn_h2_N appos_T_z appos_N_T nn_N_k appos_T_Tk appos_T_z appos_N_z num_N_1 dep_=_N dep_k_T amod_k_= nn_k_Kproductdisplay nn_k_log dep_=_h2 appos_eI1_fJ1 amod_h1_= dep_h1_eI1
P06-1077	N03-1017	p	1 Introduction Phrase-based translation models -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB- which go beyond the original IBM translation models -LRB- Brown et al. 1993 -RRB- 1 by modeling translations of phrases rather than individual words have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations	amod_evaluations_empirical nn_translation_machine amod_translation_statistical prep_by_state-of-theart_evaluations prep_in_state-of-theart_translation det_state-of-theart_the cop_state-of-theart_be aux_state-of-theart_to xcomp_suggested_state-of-theart auxpass_suggested_been aux_suggested_have nsubjpass_suggested_models amod_words_individual conj_negcc_translations_words prep_of_translations_phrases nn_translations_modeling prep_by_1_words prep_by_1_translations amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation nn_models_IBM amod_models_original det_models_the prep_beyond_go_models nsubj_go_which dep_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong dep_models_1 dep_models_Brown rcmod_models_go appos_models_Koehn appos_models_Wong appos_models_Marcu nn_models_translation amod_models_Phrase-based nn_models_Introduction num_models_1 ccomp_``_suggested
P06-1090	N03-1017	o	Here ppicker shows the accuracy when phrases are extracted by using the N-best phrase alignment method described in Section 4.1 while growdiag-final shows the accuracy when phrases are extracted using the standard phrase extraction algorithm described in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in vmod_algorithm_described nn_algorithm_extraction nn_algorithm_phrase amod_algorithm_standard det_algorithm_the dobj_using_algorithm xcomp_extracted_using auxpass_extracted_are nsubjpass_extracted_phrases advmod_extracted_when rcmod_accuracy_extracted det_accuracy_the dep_shows_accuracy amod_shows_growdiag-final num_Section_4.1 prep_in_described_Section vmod_method_described nn_method_alignment nn_method_phrase amod_method_N-best det_method_the dobj_using_method agent_extracted_using auxpass_extracted_are nsubjpass_extracted_phrases advmod_extracted_when det_accuracy_the prep_while_shows_shows advcl_shows_extracted dobj_shows_accuracy nsubj_shows_ppicker advmod_shows_Here
P06-1090	N03-1017	o	The translation model used in -LRB- Koehn et al. 2003 -RRB- is the product of translation probability a34a35a4 a29 a0 a33 a6 a29 a2 a33 a8 and distortion probability a36a37a4a39a38 a33a41a40a43a42a44a33a46a45 a32 a8 a3a5a4a35a29 a0 a30 a32 a6 a29 a2 a30 a32 a8 a10 a30 a47 a33a49a48 a32 a34a35a4 a29 a0a22a33 a6 a29 a2 a33a50a8 a36a51a4a39a38 a33 a40a52a42 a33a53a45 a32 a8 -LRB- 1 -RRB- where a38 a33 denotes the start position of the source phrase translated into the a54 th target phrase and a42 a33a53a45 a32 denotes the end position of the source phrase translated into the a4a53a54 a40a56a55 a8 th target phrase	nn_phrase_target det_phrase_th dep_a8_phrase nn_a8_a40a56a55 nn_a8_a4a53a54 det_a8_the prep_into_translated_a8 vmod_phrase_translated nn_phrase_source det_phrase_the prep_of_position_phrase nn_position_end det_position_the dobj_denotes_position nsubj_denotes_a32 nsubj_denotes_phrase nn_a32_a33a53a45 nn_a32_a42 conj_and_phrase_a32 nn_phrase_target det_phrase_th dep_a54_denotes det_a54_the prep_into_translated_a54 vmod_phrase_translated nn_phrase_source det_phrase_the prep_of_position_phrase nn_position_start det_position_the dobj_denotes_position nsubj_denotes_a33 advmod_denotes_where dep_denotes_1 nn_a33_a38 dep_a8_denotes nn_a8_a32 num_a8_a33a53a45 nn_a8_a40a52a42 nn_a8_a33 nn_a8_a36a51a4a39a38 num_a8_a33a50a8 nn_a8_a2 nn_a8_a29 nn_a8_a6 nn_a8_a0a22a33 nn_a8_a29 nn_a8_a34a35a4 nn_a8_a32 num_a8_a33a49a48 nn_a8_a47 nn_a8_a30 nn_a8_a10 nn_a8_a8 nn_a8_a32 nn_a8_a30 nn_a8_a2 nn_a8_a29 nn_a8_a6 nn_a8_a32 nn_a8_a30 nn_a8_a0 nn_a8_a3a5a4a35a29 nn_a8_a32 nn_a8_a33a41a40a43a42a44a33a46a45 nn_a8_a36a37a4a39a38 nn_a8_probability nn_a8_distortion conj_and_a8_a8 nn_a8_a33 nn_a8_a2 nn_a8_a29 nn_a8_a6 nn_a8_a33 nn_a8_a0 nn_a8_a29 nn_a8_a34a35a4 nn_a8_probability nn_a8_translation appos_product_a8 prep_of_product_a8 prep_of_product_a8 det_product_the cop_product_is nsubj_product_model amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_used_Koehn vmod_model_used nn_model_translation det_model_The
P06-1090	N03-1017	o	-LRB- Koehn et al. 2003 -RRB- used the following distortion model which simply penalizes nonmonotonic phrase alignments based on the word distance of successively translated source phrases with an appropriate value for the parameter a71 a36a51a4a39a38 a33 a40a52a42 a33a53a45 a32 a8 a10 a71a26a72a73a25a74 a45a62a75 a74a77a76a24a78 a45 a32 a72 -LRB- 3 -RRB- a79a17a80a82a81a84a83a85a15a86a88a87a70a89a91a90 languageis a means communication of MG RA RA b1 b2 b3 b4 Figure 1 Phrase alignment and reordering bi-1 bi fi-1 fi ei-1 ei bi-1 bi fi-1 fi ei-1 ei bi-1 bi fi-1 fi ei-1 ei bi-1 bi fi-1 fi ei-1 ei source target target source target target source source d = MA d = MG d = RA d = RG Figure 2 Four types of reordering patterns 3 The Global Phrase Reordering Model Figure 1 shows an example of Japanese-English phrase alignment that consists of four phrase pairs	nn_pairs_phrase num_pairs_four prep_of_consists_pairs nsubj_consists_that rcmod_alignment_consists nn_alignment_phrase amod_alignment_Japanese-English prep_of_example_alignment det_example_an dobj_shows_example nsubj_shows_Figure num_Figure_1 nn_Figure_Model nn_Figure_Reordering nn_Figure_Phrase amod_Figure_Global det_Figure_The rcmod_3_shows dep_patterns_3 nn_patterns_reordering prep_of_types_patterns num_types_Four num_Figure_2 nn_Figure_RG amod_Figure_= nn_Figure_d amod_Figure_= nn_Figure_d amod_Figure_= nn_Figure_d amod_Figure_= nn_Figure_d nn_Figure_source nn_Figure_source nn_Figure_target nn_Figure_target nn_Figure_source nn_Figure_target nn_Figure_target nn_Figure_source nn_Figure_ei nn_Figure_ei-1 nn_Figure_fi nn_Figure_fi-1 nn_Figure_bi nn_Figure_bi-1 nn_Figure_ei nn_Figure_ei-1 nn_Figure_fi nn_Figure_fi-1 nn_Figure_bi nn_Figure_bi-1 nn_Figure_ei nn_Figure_ei-1 nn_Figure_fi nn_Figure_fi-1 nn_Figure_bi nn_Figure_bi-1 nn_Figure_ei nn_Figure_ei-1 nn_Figure_fi nn_Figure_fi-1 nn_Figure_bi nn_Figure_bi-1 nn_Figure_reordering nn_d_RA nn_d_MG nn_d_MA dep_alignment_types conj_and_alignment_Figure nn_alignment_Phrase num_Figure_1 nn_Figure_b4 nn_Figure_b3 nn_Figure_b2 nn_Figure_b1 nn_Figure_RA nn_Figure_RA nn_Figure_MG prep_of_communication_Figure nn_communication_means det_communication_a dobj_languageis_communication nn_a79a17a80a82a81a84a83a85a15a86a88a87a70a89a91a90_a72 appos_a72_3 nn_a72_a32 nn_a72_a45 nn_a72_a74a77a76a24a78 num_a72_a45a62a75 nn_a72_a71a26a72a73a25a74 nn_a72_a10 nn_a72_a8 nn_a72_a32 num_a72_a33a53a45 nn_a72_a40a52a42 nn_a72_a33 nn_a72_a36a51a4a39a38 appos_a71_a79a17a80a82a81a84a83a85a15a86a88a87a70a89a91a90 nn_a71_parameter det_a71_the prep_for_value_a71 amod_value_appropriate det_value_an prep_with_phrases_value nn_phrases_source amod_phrases_translated advmod_translated_successively prep_of_distance_phrases nn_distance_word det_distance_the prep_on_based_distance vmod_alignments_based nn_alignments_phrase amod_alignments_nonmonotonic dobj_penalizes_alignments advmod_penalizes_simply nsubj_penalizes_which rcmod_model_penalizes nn_model_distortion amod_model_following det_model_the dep_used_Figure dep_used_alignment dep_used_languageis dobj_used_model nsubj_used_Koehn amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et
P06-1090	N03-1017	o	For comparison we also implemented a different N-best phrase alignment method where _ _ _ _ the_light_was_red _ _ _ the_light was_red _ _ the_light was red -LRB- 1 -RRB- -LRB- 2 -RRB- -LRB- 3 -RRB- Figure 4 N-best phrase alignments phrase pairs are extracted using the standard phrase extraction method described in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in vmod_method_described nn_method_extraction nn_method_phrase amod_method_standard det_method_the dobj_using_method xcomp_extracted_using auxpass_extracted_are nsubjpass_extracted_pairs nn_pairs_phrase nn_pairs_alignments nn_pairs_phrase amod_pairs_N-best dep_Figure_extracted num_Figure_4 dep_3_Figure dep_2_3 dep_2_1 ccomp_red_2 cop_red_was nsubj_red__ advmod_red_where num_the_light__ number____ dobj_was_red_the_light vmod_the_light_was_red num_the_light__ num_the_light__ amod_the_light_the_light_was_red num_the_light__ num_the_light__ number____ number____ dep___the_light rcmod_method_red nn_method_alignment nn_method_phrase nn_method_N-best amod_method_different det_method_a dobj_implemented_method advmod_implemented_also nsubj_implemented_we prep_for_implemented_comparison
P06-1090	N03-1017	o	a1 Graduated in March 2006 Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_phrases_target conj_and_source_phrases det_source_the prep_of_identities_phrases prep_of_identities_source det_identities_the conj_or_alignment_identities nn_alignment_phrase det_alignment_the prep_of_orientation_identities prep_of_orientation_alignment det_orientation_the dobj_considering_orientation nn_phrases_source amod_phrases_translated advmod_translated_successively prep_between_distance_phrases nn_distance_word det_distance_the prep_on_based_distance prepc_without_penalized_considering dobj_penalized_based auxpass_penalized_is nsubjpass_penalized_alignment prep_in_penalized_which nn_alignment_phrase amod_alignment_non-monotonic rcmod_model_penalized nn_model_reordering amod_model_distance-based nn_model_word det_model_a dep_use_Koehn dobj_use_model nn_systems_translation amod_systems_phrase-based nn_systems_Standard num_systems_2006 nn_systems_March dep_Graduated_use prep_in_Graduated_systems nsubj_Graduated_a1
P06-1091	N03-1017	o	The block set is generated using a phrase-pair selection algorithm similar to -LRB- Koehn et al. 2003 Al-Onaizan et al. 2004 -RRB- which includes some heuristic filtering to mal statement here	advmod_mal_here dobj_mal_statement aux_mal_to xcomp_filtering_mal vmod_heuristic_filtering det_heuristic_some dobj_includes_heuristic nsubj_includes_which num_Al-Onaizan_2004 nn_Al-Onaizan_al. nn_Al-Onaizan_et rcmod_Koehn_includes dep_Koehn_Al-Onaizan appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_to_similar_Koehn amod_algorithm_similar nn_algorithm_selection amod_algorithm_phrase-pair det_algorithm_a dobj_using_algorithm xcomp_generated_using auxpass_generated_is nsubjpass_generated_set nn_set_block det_set_The ccomp_``_generated
P06-1091	N03-1017	o	Word-based features are used as well e.g. feature a75 a11a39a99a78a99a18a11 captures word-to-word translation de4On our test set -LRB- Tillmann and Zhang 2005 -RRB- reports a BLEU score of a100a63a101a63a102a43a103 and -LRB- Ittycheriah and Roukos 2005 -RRB- reports a BLEU score of a104a89a103a63a102 a105 pendencies similar to the use of Model a98 probabilities in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_probabilities_in nn_probabilities_a98 nn_probabilities_Model prep_of_use_probabilities det_use_the prep_to_similar_use amod_pendencies_similar nn_a105_a104a89a103a63a102 prep_of_score_a105 nn_score_BLEU det_score_a dobj_reports_score nsubj_reports_Roukos nsubj_reports_Ittycheriah dep_Ittycheriah_2005 conj_and_Ittycheriah_Roukos prep_of_score_a100a63a101a63a102a43a103 nn_score_BLEU det_score_a dobj_reports_score nsubj_reports_set amod_Tillmann_2005 conj_and_Tillmann_Zhang appos_set_Zhang appos_set_Tillmann nn_set_test poss_set_our rcmod_de4On_reports nn_de4On_translation amod_de4On_word-to-word dobj_captures_de4On conj_and_a11a39a99a78a99a18a11_reports vmod_a11a39a99a78a99a18a11_captures nn_a11a39a99a78a99a18a11_a75 nn_a11a39a99a78a99a18a11_feature pobj_e.g._reports pobj_e.g._a11a39a99a78a99a18a11 advmod_well_as dobj_used_pendencies prep_used_e.g. advmod_used_well auxpass_used_are nsubjpass_used_features amod_features_Word-based
P06-1096	N03-1017	n	The process of phrase extraction is difficult to optimize in a non-discriminative setting many heuristics have been proposed -LRB- Koehn et al. 2003 -RRB- but it is not obvious which one should be chosen for a given language pair	nn_pair_language amod_pair_given det_pair_a prep_for_chosen_pair auxpass_chosen_be aux_chosen_should nsubjpass_chosen_one dobj_chosen_which ccomp_obvious_chosen neg_obvious_not cop_obvious_is nsubj_obvious_it amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_but_proposed_obvious dep_proposed_Koehn auxpass_proposed_been aux_proposed_have nsubjpass_proposed_heuristics amod_heuristics_many amod_setting_non-discriminative det_setting_a prep_in_optimize_setting aux_optimize_to parataxis_difficult_obvious parataxis_difficult_proposed xcomp_difficult_optimize cop_difficult_is nsubj_difficult_process nn_extraction_phrase prep_of_process_extraction det_process_The
P06-1096	N03-1017	o	The discrepancy between DEV performance and TEST performance is due to temporal distance from TRAIN and high variance in BLEU score .11 We also compared our model with Pharaoh -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharaoh_Koehn prep_with_model_Pharaoh poss_model_our dobj_compared_model advmod_compared_also nsubj_compared_We rcmod_.11_compared dobj_score_.11 nsubj_score_variance prep_in_variance_BLEU amod_variance_high prep_from_distance_TRAIN amod_distance_temporal conj_and_due_score prep_to_due_distance cop_due_is nsubj_due_discrepancy nn_performance_TEST conj_and_performance_performance nn_performance_DEV prep_between_discrepancy_performance prep_between_discrepancy_performance det_discrepancy_The ccomp_``_score ccomp_``_due
P06-1096	N03-1017	o	At the end we ran our models once on TEST to get final numbers .2 4 Models Our experiments used phrase-based models -LRB- Koehn et al. 2003 -RRB- which require a translation table and language model for decoding and feature computation	nn_computation_feature conj_and_decoding_computation nn_model_language conj_and_table_model nn_table_translation det_table_a prep_for_require_computation prep_for_require_decoding dobj_require_model dobj_require_table nsubj_require_which amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et rcmod_models_require appos_models_Koehn amod_models_phrase-based amod_models_used dep_experiments_models poss_experiments_Our dep_Models_experiments num_Models_4 nn_Models_.2 nn_Models_numbers amod_Models_final dobj_get_Models aux_get_to poss_models_our xcomp_ran_get prep_on_ran_TEST advmod_ran_once dobj_ran_models nsubj_ran_we prep_at_ran_end det_end_the
P06-1096	N03-1017	o	In the future we plan to explore our discriminative framework on a full distortion model -LRB- Koehn et al. 2003 -RRB- or even a hierarchical model -LRB- Chiang 2005 -RRB-	amod_Chiang_2005 appos_model_Chiang amod_model_hierarchical det_model_a advmod_model_even amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_model_distortion amod_model_full det_model_a amod_framework_discriminative poss_framework_our prep_on_explore_model dobj_explore_framework aux_explore_to conj_or_plan_model dep_plan_Koehn xcomp_plan_explore nsubj_plan_we prep_in_plan_future det_future_the
P06-1098	N03-1017	o	A phrase-based translation model is one of the modern approaches which exploits a phrase a contiguous sequence of words as a unit of translation -LRB- Koehn et al. 2003 Zens and Ney 2003 Tillman 2004 -RRB-	amod_Tillman_2004 dep_Zens_Tillman conj_and_Zens_2003 conj_and_Zens_Ney dep_Koehn_2003 dep_Koehn_Ney dep_Koehn_Zens amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_of_unit_translation det_unit_a prep_of_sequence_words amod_sequence_contiguous det_sequence_a appos_phrase_sequence det_phrase_a prep_as_exploits_unit dobj_exploits_phrase nsubj_exploits_which rcmod_approaches_exploits amod_approaches_modern det_approaches_the dep_one_Koehn prep_of_one_approaches cop_one_is nsubj_one_model nn_model_translation amod_model_phrase-based det_model_A ccomp_``_one
P06-1098	N03-1017	o	Many-to-many word alignments are induced by running a one-to-many word alignment model such as GIZA + + -LRB- Och and Ney 2003 -RRB- in both directions and by combining the results based on a heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn det_heuristic_a prep_on_based_heuristic vmod_results_based det_results_the dobj_combining_results pcomp_by_combining det_directions_both pobj_in_directions num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_by conj_+_GIZA_in conj_+_GIZA_+ prep_such_as_model_by prep_such_as_model_in prep_such_as_model_+ prep_such_as_model_GIZA nn_model_alignment nn_model_word amod_model_one-to-many det_model_a dobj_running_model agent_induced_running auxpass_induced_are nsubjpass_induced_alignments nn_alignments_word amod_alignments_Many-to-many
P06-1098	N03-1017	o	Second phrase translation pairs are extracted from the word alignment corpus -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_corpus_Koehn nn_corpus_alignment nn_corpus_word det_corpus_the prep_from_extracted_corpus auxpass_extracted_are nsubjpass_extracted_pairs advmod_extracted_Second nn_pairs_translation nn_pairs_phrase ccomp_``_extracted
P06-1122	N03-1017	p	4 Experiments Phrase-based SMT systems have been shown to outperform word-based approaches -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_approaches_Koehn amod_approaches_word-based dobj_outperform_approaches aux_outperform_to xcomp_shown_outperform auxpass_shown_been aux_shown_have nsubjpass_shown_systems nn_systems_SMT amod_systems_Phrase-based nn_systems_Experiments num_systems_4
P06-1122	N03-1017	o	4.1 Applications to phrase-based SMT Aphrase-basedtranslationmodelcanbeestimated in two stages first a parallel corpus is aligned at the word-level and then phrase pairs are extracted -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_extracted_Koehn auxpass_extracted_are csubjpass_extracted_aligned nn_pairs_phrase amod_pairs_then amod_pairs_word-level det_pairs_the conj_and_word-level_then prep_at_aligned_pairs auxpass_aligned_is nsubjpass_aligned_corpus amod_corpus_parallel det_corpus_a advmod_corpus_first num_stages_two prep_in_SMT_stages amod_SMT_Aphrase-basedtranslationmodelcanbeestimated dep_phrase-based_SMT parataxis_Applications_extracted prep_to_Applications_phrase-based num_Applications_4.1 dep_``_Applications
P06-1123	N03-1017	o	is relevant to finite-state phrase-based models that use no parse trees -LRB- Koehn et al. 2003 -RRB- tree-tostring models that rely on one parse tree -LRB- Yamada and Knight 2001 -RRB- and tree-to-tree models that rely on two parse trees -LRB- Groves et al. 2004 e.g. -RRB-	dep_Groves_e.g. appos_Groves_2004 dep_Groves_al. nn_Groves_et appos_trees_Groves nn_trees_parse num_trees_two prep_on_rely_trees nsubj_rely_that rcmod_models_rely amod_models_tree-to-tree dep_Yamada_2001 conj_and_Yamada_Knight appos_tree_Knight appos_tree_Yamada nn_tree_parse num_tree_one prep_on_rely_tree nsubj_rely_that rcmod_models_rely amod_models_tree-tostring amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_trees_parse neg_trees_no dobj_use_trees nsubj_use_that rcmod_models_use amod_models_phrase-based amod_models_finite-state conj_and_relevant_models conj_and_relevant_models dep_relevant_Koehn prep_to_relevant_models cop_relevant_is
P06-1139	N03-1017	o	Automatic Creation of WIDL-expressions for MT. We generate WIDL-expressions from Chinese strings by exploiting a phrase-based translation table -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_table_translation amod_table_phrase-based det_table_a dobj_exploiting_table amod_strings_Chinese prep_from_WIDL-expressions_strings prepc_by_generate_exploiting dobj_generate_WIDL-expressions nsubj_generate_We rcmod_MT._generate dep_Creation_Koehn prep_for_Creation_MT. prep_of_Creation_WIDL-expressions nn_Creation_Automatic dep_``_Creation
P06-1139	N03-1017	o	When evaluated against the state-of-the-art phrase-based decoder Pharaoh -LRB- Koehn 2004 -RRB- using the same experimental conditions translation table trained on the FBIS corpus -LRB- 7.2 M Chinese words and 9.2 M English words of parallel text -RRB- trigram language model trained on 155M words of English newswire interpolation weights a65 -LRB- Equation 2 -RRB- trained using discriminative training -LRB- Och 2003 -RRB- -LRB- on the 2002 NIST MT evaluation set -RRB- probabilistic beam a90 set to 0.01 histogram beam a58 set to 10 and BLEU -LRB- Papineni et al. 2002 -RRB- as our metric the WIDL-NGLM-Aa86 a129 algorithm produces translations that have a BLEU score of 0.2570 while Pharaoh translations have a BLEU score of 0.2635	prep_of_score_0.2635 nn_score_BLEU det_score_a dobj_have_score nsubj_have_translations mark_have_while nn_translations_Pharaoh prep_of_score_0.2570 nn_score_BLEU det_score_a dobj_have_score nsubj_have_that rcmod_translations_have advcl_produces_have dobj_produces_translations nsubj_produces_algorithm nn_algorithm_a129 nn_algorithm_WIDL-NGLM-Aa86 det_algorithm_the poss_metric_our amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_10_BLEU prep_to_set_BLEU prep_to_set_10 prep_as_a58_metric appos_a58_Papineni vmod_a58_set nn_a58_beam nn_a58_histogram prep_to_set_0.01 vmod_a90_set nn_a90_beam amod_a90_probabilistic nn_set_evaluation nn_set_MT nn_set_NIST num_set_2002 det_set_the pobj_on_set dep_Och_2003 appos_training_Och amod_training_discriminative dep_using_produces dobj_using_a58 conj_using_a90 dep_using_on dobj_using_training prep_trained_using num_Equation_2 vmod_a65_trained appos_a65_Equation nn_a65_weights nn_a65_interpolation nn_newswire_English prep_of_words_newswire nn_words_155M prep_on_trained_words dep_model_a65 vmod_model_trained nn_model_language nn_model_trigram amod_text_parallel prep_of_words_text nn_words_English dep_words_M num_M_9.2 conj_and_words_words amod_words_Chinese nn_words_M num_words_7.2 dep_corpus_words dep_corpus_words nn_corpus_FBIS det_corpus_the prep_on_trained_corpus vmod_table_trained nn_table_translation nn_table_conditions amod_table_experimental amod_table_same det_table_the parataxis_using_model dobj_using_table ccomp_,_using dep_Koehn_2004 dep_Pharaoh_Koehn nn_Pharaoh_decoder amod_Pharaoh_phrase-based amod_Pharaoh_state-of-the-art det_Pharaoh_the prep_against_evaluated_Pharaoh advmod_evaluated_When advcl_``_evaluated
P06-2005	N03-1017	o	The normalization is visualized as a translation problem where messages in the SMS language are to be translated to normal English using a similar phrase-based statistical MT method -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn nn_method_MT amod_method_statistical amod_method_phrase-based amod_method_similar det_method_a dobj_using_method amod_English_normal xcomp_translated_using prep_to_translated_English auxpass_translated_be aux_translated_to xcomp_are_translated nsubj_are_messages advmod_are_where nn_language_SMS det_language_the prep_in_messages_language rcmod_problem_are nn_problem_translation det_problem_a prep_as_visualized_problem auxpass_visualized_is nsubjpass_visualized_normalization det_normalization_The
P06-2101	N03-1017	o	791 and score the alignment template models phrases -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_models_phrases nn_models_template nn_models_alignment det_models_the appos_791_Koehn dep_791_models conj_and_791_score ccomp_``_score ccomp_``_791
P06-2107	N03-1017	o	The second one is heuristic and tries to use a wordaligned corpus -LRB- Zens et al. 2002 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Zens_Koehn appos_Zens_2002 dep_Zens_al. nn_Zens_et amod_corpus_wordaligned det_corpus_a dobj_use_corpus aux_use_to xcomp_tries_use nsubj_tries_one dep_heuristic_Zens conj_and_heuristic_tries cop_heuristic_is nsubj_heuristic_one amod_one_second det_one_The
P07-1001	N03-1017	o	Our decoder is a phrase-based multi-stack imple5 mentation of the log-linear model similar to Pharaoh -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_to_similar_Pharaoh amod_model_similar amod_model_log-linear det_model_the dep_mentation_Koehn prep_of_mentation_model nn_mentation_imple5 amod_mentation_multi-stack amod_mentation_phrase-based det_mentation_a cop_mentation_is nsubj_mentation_decoder poss_decoder_Our
P07-1005	N03-1017	p	To perform translation state-of-the-art MT systems use a statistical phrase-based approach -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB- by treating phrases as the basic units of translation	prep_of_units_translation amod_units_basic det_units_the prep_as_treating_units dobj_treating_phrases dep_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_Koehn conj_and_Marcu_2002 conj_and_Marcu_Wong dep_approach_Koehn dep_approach_2002 dep_approach_Wong dep_approach_Marcu amod_approach_phrase-based amod_approach_statistical det_approach_a prepc_by_use_treating dobj_use_approach nsubj_use_systems advcl_use_perform nn_systems_MT amod_systems_state-of-the-art dobj_perform_translation aux_perform_To
P07-1005	N03-1017	p	Recently Cabezas and Resnik -LRB- 2005 -RRB- experimented with incorporating WSD translations into Pharaoh a state-of-the-art phrase-based MT system -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_system_Koehn nn_system_MT amod_system_phrase-based amod_system_state-of-the-art det_system_a appos_Pharaoh_system prep_into_translations_Pharaoh nn_translations_WSD dobj_incorporating_translations prepc_with_experimented_incorporating nsubj_experimented_Resnik nsubj_experimented_Cabezas advmod_experimented_Recently appos_Resnik_2005 conj_and_Cabezas_Resnik
P07-1039	N03-1017	o	4.3 Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline GIZA + + implementation of IBM word alignment model 4 -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- ,8 the refinement and phrase-extraction heuristics described in -LRB- Koehn et al. 2003 -RRB- minimum-error-rate training 7More specifically we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs	nn_pairs_sentence amod_pairs_new dobj_construct_pairs aux_construct_to amod_sentence_Chinese det_sentence_the vmod_references_construct conj_and_references_sentence num_references_7 det_references_the prep_from_reference_sentence prep_from_reference_references nn_reference_English amod_reference_first det_reference_the dobj_choose_reference nsubj_choose_we advmod_7More_specifically nn_7More_training amod_7More_minimum-error-rate rcmod_Koehn_choose appos_Koehn_7More amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn amod_heuristics_phrase-extraction vmod_refinement_described conj_and_refinement_heuristics det_refinement_the dep_,8_heuristics dep_,8_refinement dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_model_,8 dep_model_Brown num_model_4 nn_model_alignment nn_model_word dep_GIZA_model prep_of_GIZA_IBM conj_+_GIZA_implementation det_baseline_a nn_system_translation nn_system_machine amod_system_statistical amod_system_phrase-based amod_system_log-linear amod_system_standard det_system_a prep_as_use_baseline dobj_use_system nsubj_use_We dep_Baseline_implementation dep_Baseline_GIZA rcmod_Baseline_use num_Baseline_4.3 dep_``_Baseline
P07-1039	N03-1017	o	Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2 ChineseEnglish corpus statistics -LRB- Och 2003 -RRB- using Phramer -LRB- Olteanu et al. 2006 -RRB- a 3-gram language model with Kneser-Ney smoothing trained with SRILM -LRB- Stolcke 2002 -RRB- on the English side of the training data and Pharaoh -LRB- Koehn 2004 -RRB- with default settings to decode	aux_decode_to nn_settings_default dep_Koehn_2004 dep_Pharaoh_Koehn conj_and_data_Pharaoh nn_data_training det_data_the prep_of_side_Pharaoh prep_of_side_data amod_side_English det_side_the dep_Stolcke_2002 appos_SRILM_Stolcke xcomp_trained_decode prep_with_trained_settings prep_on_trained_side prep_with_trained_SRILM vmod_smoothing_trained amod_smoothing_Kneser-Ney prep_with_model_smoothing nn_model_language amod_model_3-gram det_model_a amod_Olteanu_2006 dep_Olteanu_al. nn_Olteanu_et appos_Phramer_model dep_Phramer_Olteanu dobj_using_Phramer dep_Och_2003 vmod_statistics_using dep_statistics_Och nn_statistics_corpus nn_statistics_ChineseEnglish num_Table_2 num_Table_1,081 num_Table_569 dep_size_Table nn_size_Vocabulary num_size_14,437 num_size_1,864 dep_words_size dep_Running_statistics dobj_Running_words ccomp_``_Running
P07-1039	N03-1017	o	To quickly -LRB- and approximately -RRB- evaluate this phenomenon we trained the statistical IBM wordalignment model 4 -LRB- Brown et al. 1993 -RRB- ,1 using the GIZA + + software -LRB- Och and Ney 2003 -RRB- for the following language pairs ChineseEnglish Italian English and DutchEnglish using the IWSLT-2006 corpus -LRB- Takezawa et al. 2002 Paul 2006 -RRB- for the first two language pairs and the Europarl corpus -LRB- Koehn 2005 -RRB- for the last one	amod_one_last det_one_the dep_Koehn_2005 prep_for_corpus_one appos_corpus_Koehn nn_corpus_Europarl det_corpus_the nn_pairs_language num_pairs_two amod_pairs_first det_pairs_the dep_Paul_2006 conj_and_Takezawa_corpus prep_for_Takezawa_pairs dep_Takezawa_Paul appos_Takezawa_2002 dep_Takezawa_al. nn_Takezawa_et nn_corpus_IWSLT-2006 det_corpus_the dobj_using_corpus amod_English_Italian dep_ChineseEnglish_corpus dep_ChineseEnglish_Takezawa vmod_ChineseEnglish_using conj_and_ChineseEnglish_DutchEnglish appos_ChineseEnglish_English nn_pairs_language amod_pairs_following det_pairs_the dep_Och_2003 conj_and_Och_Ney prep_for_software_pairs appos_software_Ney appos_software_Och pobj_+_software dep_GIZA_DutchEnglish dep_GIZA_ChineseEnglish conj_+_GIZA_+ det_GIZA_the dobj_using_+ dobj_using_GIZA advmod_,1_quickly num_Brown_1993 nn_Brown_al. nn_Brown_et num_model_4 nn_model_wordalignment nn_model_IBM amod_model_statistical det_model_the dobj_trained_model nsubj_trained_we det_phenomenon_this dobj_evaluate_Brown parataxis_evaluate_trained dobj_evaluate_phenomenon advmod_evaluate_approximately cc_evaluate_and dep_quickly_evaluate dep_To_using pobj_To_,1 dep_``_To
P07-1059	N03-1017	o	We present two approaches to SMT-based query expansion both of which are implemented in the framework of phrase-based SMT -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_SMT_2004 dep_SMT_Ney dep_SMT_Och amod_SMT_phrase-based prep_of_framework_SMT det_framework_the prep_in_implemented_framework auxpass_implemented_are nsubjpass_implemented_both prep_of_both_which nn_expansion_query amod_expansion_SMT-based prep_to_approaches_expansion num_approaches_two parataxis_present_implemented dobj_present_approaches nsubj_present_We
P07-1059	N03-1017	o	4 SMT-Based Query Expansion Our SMT-based query expansion techniques are based on a recent implementation of the phrasebased SMT framework -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_framework_SMT amod_framework_phrasebased det_framework_the prep_of_implementation_framework amod_implementation_recent det_implementation_a prep_on_based_implementation auxpass_based_are nsubjpass_based_techniques nn_techniques_expansion nn_techniques_query amod_techniques_SMT-based poss_techniques_Our dep_Expansion_Koehn rcmod_Expansion_based nn_Expansion_Query amod_Expansion_SMT-Based num_Expansion_4 ccomp_``_Expansion
P07-1083	N03-1017	o	A similar use of the term phrase exists in machine translation where phrases are often pairs of word sequences consistent with word-based alignments -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_alignments_word-based prep_with_consistent_alignments amod_sequences_consistent nn_sequences_word prep_of_pairs_sequences advmod_pairs_often cop_pairs_are nsubj_pairs_phrases advmod_pairs_where rcmod_translation_pairs nn_translation_machine dep_exists_Koehn prep_in_exists_translation nsubj_exists_use nn_phrase_term det_phrase_the prep_of_use_phrase amod_use_similar det_use_A
P07-1089	N03-1017	o	We ran GIZA + + -LRB- Och and Ney 2000 -RRB- on the training corpus in both directions using its default setting and then applied the refinement rule diagand described in -LRB- Koehn et al. 2003 -RRB- to obtain a single many-to-many word alignment for each sentence pair	nn_pair_sentence det_pair_each prep_for_alignment_pair nn_alignment_word nn_alignment_many-to-many amod_alignment_single det_alignment_a dobj_obtain_alignment aux_obtain_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn xcomp_described_obtain prep_described_in vmod_diagand_described nn_diagand_rule nn_diagand_refinement det_diagand_the dobj_applied_diagand advmod_applied_then nn_setting_default poss_setting_its conj_and_using_applied dobj_using_setting preconj_directions_both nn_corpus_training det_corpus_the num_Och_2000 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ dep_ran_applied dep_ran_using prep_in_ran_directions dobj_ran_+ dobj_ran_GIZA nsubj_ran_We
P07-1089	N03-1017	o	We compared our system Lynx against a freely available phrase-based decoder Pharaoh -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_Pharaoh_decoder amod_Pharaoh_phrase-based amod_Pharaoh_available det_Pharaoh_a advmod_available_freely dep_Lynx_Koehn prep_against_Lynx_Pharaoh dep_system_Lynx poss_system_our dobj_compared_system nsubj_compared_We ccomp_``_compared
P07-1090	N03-1017	o	The basic phrase reordering model is a simple unlexicalized context-insensitive distortion penalty model -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_model_Koehn nn_model_penalty nn_model_distortion amod_model_unlexicalized det_model_a cop_model_is nsubj_model_model dep_unlexicalized_context-insensitive amod_unlexicalized_simple nn_model_reordering nn_model_phrase amod_model_basic det_model_The
P07-1090	N03-1017	o	However the pb features yields no noticeable improvement unlike in prefect lexical choice scenario this is similar to the findings in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_findings_in det_findings_the prep_to_similar_findings cop_similar_is nsubj_similar_this nn_scenario_choice amod_scenario_lexical amod_scenario_prefect pobj_in_scenario pcomp_unlike_in prep_improvement_unlike amod_improvement_noticeable neg_improvement_no dep_yields_improvement parataxis_features_similar dobj_features_yields nsubj_features_pb advmod_features_However det_pb_the
P07-1091	N03-1017	o	The translation table is obtained as described in -LRB- Koehn et al. 2003 -RRB- i.e. the alignment tool GIZA + + is run over the training data in both translation directions and the two alignTest Setting BLEU B1 standard phrase-based SMT 29.22 B2 -LRB- B1 -RRB- + clause splitting 29.13 Table 2 Experiment Baseline Test Setting BLEU BLEU 2-ary 2,3-ary 1 rule 29.77 30.31 2 ME -LRB- phrase label -RRB- 29.93 30.49 3 ME -LRB- left right -RRB- 30.10 30.53 4 ME -LRB- -LRB- 3 -RRB- + head -RRB- 30.24 30.71 5 ME -LRB- -LRB- 3 -RRB- + phrase label -RRB- 30.12 30.30 6 ME -LRB- -LRB- 4 -RRB- + context -RRB- 30.24 30.76 Table 3 Tests on Various Reordering Models The 3rd column comprises the BLEU scores obtained by reordering binary nodes only the 4th column the scores by reordering both binary and 3-ary nodes	dep_binary_nodes conj_and_binary_3-ary preconj_binary_both dobj_reordering_3-ary dobj_reordering_binary prep_by_scores_reordering det_scores_the dep_column_scores amod_column_4th det_column_the dep_only_column amod_nodes_binary nn_nodes_reordering agent_obtained_nodes vmod_scores_obtained nn_scores_BLEU det_scores_the prep_comprises_only dobj_comprises_scores nsubj_comprises_column amod_column_3rd det_column_The nn_Models_Reordering amod_Models_Various rcmod_Tests_comprises prep_on_Tests_Models num_Table_3 num_Table_30.76 nn_Table_context number_30.76_30.24 dep_4_Tests conj_+_4_Table dep_ME_Table dep_ME_4 num_ME_6 num_ME_30.12 number_6_30.30 nn_label_phrase dep_3_ME conj_+_3_label dep_ME_label dep_ME_3 num_ME_5 num_ME_30.24 dep_ME_rule number_5_30.71 cc_head_+ dep_head_3 num_ME_4 number_4_30.53 number_4_30.10 appos_left_right dep_ME_ME dep_ME_left num_ME_3 num_ME_30.49 number_30.49_29.93 nn_label_phrase dep_ME_ME appos_ME_label num_ME_2 num_ME_29.77 number_2_30.31 dep_rule_head dep_rule_ME num_rule_1 amod_rule_2,3-ary amod_rule_2-ary nn_rule_BLEU nn_rule_BLEU nn_rule_Setting nn_rule_Test nn_rule_Baseline nn_rule_Experiment num_Table_2 num_Table_29.13 dep_splitting_Table nn_splitting_clause dep_B2_ME conj_+_B2_splitting appos_B2_B1 num_B2_29.22 nn_B2_SMT amod_B2_phrase-based amod_B2_standard nn_B2_B1 nn_B2_BLEU nn_B2_Setting nn_B2_alignTest num_B2_two det_B2_the nn_directions_translation det_directions_both prep_in_data_directions nn_data_training det_data_the conj_and_run_splitting conj_and_run_B2 prep_over_run_data auxpass_run_is prep_run_i.e. nn_GIZA_tool nn_GIZA_alignment det_GIZA_the cc_i.e._+ cc_i.e._+ pobj_i.e._GIZA rcmod_Koehn_B2 rcmod_Koehn_run amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn mark_described_as advcl_obtained_described auxpass_obtained_is nsubjpass_obtained_table nn_table_translation det_table_The
P07-1091	N03-1017	o	The implementation is similar to the idea of lexical weight in -LRB- Koehn et al. 2003 -RRB- all points in the alignment matrices of the entire training corpus are collected to calculate the probabilistic distribution P -LRB- t | s -RRB- of some TL word 3Some readers may prefer the expression the subtree rooted at node N to node N The latter term is used in this paper for simplicity	det_paper_this prep_for_used_simplicity prep_in_used_paper auxpass_used_is nsubjpass_used_term amod_term_latter det_term_The nn_N_node nn_N_node prep_to_rooted_N prep_at_rooted_N dep_subtree_used vmod_subtree_rooted det_subtree_the dep_expression_subtree det_expression_the dobj_prefer_expression aux_prefer_may nn_readers_3Some nn_readers_word nn_readers_TL det_readers_some num_s_| nn_s_t nn_s_P amod_distribution_probabilistic det_distribution_the dobj_calculate_distribution aux_calculate_to dep_collected_prefer prep_of_collected_readers parataxis_collected_s xcomp_collected_calculate auxpass_collected_are nsubjpass_collected_points nn_corpus_training amod_corpus_entire det_corpus_the prep_of_matrices_corpus nn_matrices_alignment det_matrices_the prep_in_points_matrices det_points_all amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn amod_weight_lexical prep_idea_in prep_of_idea_weight det_idea_the parataxis_similar_collected prep_to_similar_idea cop_similar_is nsubj_similar_implementation det_implementation_The
P07-1091	N03-1017	o	For example the distancebased reordering model -LRB- Koehn et al. 2003 -RRB- allows a decoder to translate in non-monotonous order under the constraint that the distance between two phrases translated consecutively does not exceed a limit known as distortion limit	nn_limit_distortion prep_as_known_limit vmod_limit_known det_limit_a dobj_exceed_limit neg_exceed_not aux_exceed_does nsubj_exceed_distance mark_exceed_that advmod_translated_consecutively vmod_phrases_translated num_phrases_two prep_between_distance_phrases det_distance_the ccomp_constraint_exceed det_constraint_the amod_order_non-monotonous prep_in_translate_order aux_translate_to det_decoder_a prep_under_allows_constraint xcomp_allows_translate dobj_allows_decoder nsubj_allows_model prep_for_allows_example amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_model_Koehn nn_model_reordering amod_model_distancebased det_model_the
P07-1092	N03-1017	o	The translation models and lexical scores were estimated on the training corpus whichwasautomaticallyalignedusingGiza + + -LRB- Och et al. 1999 -RRB- in both directions between source and target and symmetrised using the growing heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_heuristic_growing det_heuristic_the dobj_using_heuristic vmod_source_using conj_and_source_symmetrised conj_and_source_target prep_between_directions_symmetrised prep_between_directions_target prep_between_directions_source preconj_directions_both amod_Och_1999 dep_Och_al. nn_Och_et dep_whichwasautomaticallyalignedusingGiza_Och conj_+_whichwasautomaticallyalignedusingGiza_+ nn_whichwasautomaticallyalignedusingGiza_corpus nn_whichwasautomaticallyalignedusingGiza_training det_whichwasautomaticallyalignedusingGiza_the dep_estimated_Koehn prep_in_estimated_directions prep_on_estimated_+ prep_on_estimated_whichwasautomaticallyalignedusingGiza auxpass_estimated_were nsubjpass_estimated_scores nsubjpass_estimated_models amod_scores_lexical conj_and_models_scores nn_models_translation det_models_The ccomp_``_estimated
P07-1092	N03-1017	o	This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps -LRB- Koehn et al. 2003 -RRB- and has proven a very effective feature -LRB- Zens and Ney 2004 Foster et al. 2006 -RRB-	num_Foster_2006 nn_Foster_al. nn_Foster_et dep_Zens_Foster dep_Zens_2004 conj_and_Zens_Ney appos_feature_Ney appos_feature_Zens amod_feature_effective det_feature_a advmod_effective_very dobj_proven_feature aux_proven_has nsubj_proven_This amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_steps_translation amod_steps_word-for-word amod_steps_independent prep_of_series_steps det_series_a prep_into_decomposed_series auxpass_decomposed_is nsubjpass_decomposed_it advmod_decomposed_when det_phrase_a prep_of_probability_phrase nn_probability_translation det_probability_the conj_and_represents_proven dep_represents_Koehn advcl_represents_decomposed dobj_represents_probability nsubj_represents_This
P07-1092	N03-1017	o	As with conventional smoothing methods -LRB- Koehn et al. 2003 Foster et al. 2006 -RRB- triangulation increases the robustness of phrase translation estimates	nn_estimates_translation nn_estimates_phrase prep_of_robustness_estimates det_robustness_the dobj_increases_robustness nsubj_increases_triangulation num_Foster_2006 nn_Foster_al. nn_Foster_et rcmod_Koehn_increases dep_Koehn_Foster appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_methods_smoothing amod_methods_conventional pobj_with_methods dep_As_Koehn pcomp_As_with prep_``_As
P07-1092	N03-1017	p	1 Introduction Statistical machine translation -LRB- Brown et al. 1993 -RRB- has seen many improvements in recent years most notably the transition from wordto phrase-based models -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_models_phrase-based amod_models_wordto dep_transition_Koehn prep_from_transition_models det_transition_the advmod_transition_notably advmod_notably_most amod_years_recent prep_in_improvements_years amod_improvements_many dep_seen_transition dobj_seen_improvements aux_seen_has nsubj_seen_translation num_al._1993 nn_al._et amod_al._Brown appos_translation_al. nn_translation_machine amod_translation_Statistical nn_translation_Introduction num_translation_1 ccomp_``_seen
P07-1108	N03-1017	o	3 Phrase-Based SMT According to the translation model presented in -LRB- Koehn et al. 2003 -RRB- given a source sentence f the best target translation best e can be obtained according to the following model -RRB- -LRB- -RRB- -LRB- -RRB- | -LRB- maxarg -RRB- | -LRB- maxarg e e e eef fee length LM best pp p = = -LRB- 1 -RRB- Where the translation model -RRB- | -LRB- efp can be decomposed into = = I i i iii i i II aefpbadef efp 1 1 1 1 -RRB- | -LRB- -RRB- -LRB- -RRB- | -LRB- -RRB- | -LRB- w -LRB- 2 -RRB- Where -RRB- | -LRB- i i ef and -RRB- -LRB- 1 ii bad denote phrase translation probability and distortion probability respectively	nn_probability_distortion advmod_probability_respectively conj_and_probability_probability nn_probability_translation nn_probability_phrase nn_probability_denote amod_probability_bad amod_probability_ii num_probability_1 nn_ef_i cc_i_and dep_i_ef dep_|_probability dep_|_probability appos_|_i dep_|_w nn_|_| dep_w_Where appos_w_2 nn_|_| nn_|_| dep_1_| dep_1_1 num_1_1 number_1_1 dep_efp_1 nn_efp_aefpbadef num_efp_II nn_efp_i nn_efp_i dep_iii_efp advmod_iii_i dep_iii_I dep_iii_= dep_iii_= dep_iii_into nn_i_i advcl_decomposed_iii auxpass_decomposed_be aux_decomposed_can nsubjpass_decomposed_efp rcmod_|_decomposed dep_model_| nn_model_translation det_model_the dep_Where_model dep_=_Where dep_=_1 dep_=_= amod_p_= dep_pp_p dep_best_pp amod_LM_best dep_length_LM dep_fee_length dep_eef_fee dep_e_eef dep_maxarg_e dep_maxarg_e dep_maxarg_e appos_|_maxarg nn_|_| appos_|_maxarg amod_|_best amod_model_following det_model_the pobj_obtained_model prepc_according_to_obtained_to auxpass_obtained_be aux_obtained_can nsubjpass_obtained_e dep_best_obtained dep_translation_| nn_translation_target amod_translation_best det_translation_the appos_f_translation dep_sentence_f nn_sentence_source det_sentence_a pobj_given_sentence prep_Koehn_given amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_presented_Koehn vmod_model_presented nn_model_translation det_model_the pobj_to_model pcomp_According_to prep_SMT_According amod_SMT_Phrase-Based num_SMT_3 dep_``_SMT
P07-1108	N03-1017	o	Thus equation -LRB- 3 -RRB- can be rewritten as = i p i iii i i eppfef -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -LRB- 4 -RRB- 4.2 Lexical Weight Given a phrase pair -RRB- -LRB- ef and a word alignment a between the source word positions ni ,1 = and the target word positions mj ,1 = the lexical weight can be estimated according to the following method -LRB- Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et amod_method_following det_method_the pobj_estimated_method prepc_according_to_estimated_to auxpass_estimated_be aux_estimated_can nsubjpass_estimated_weight amod_weight_lexical det_weight_the npadvmod_=_,1 dep_mj_Koehn rcmod_mj_estimated amod_mj_= nn_mj_positions nn_mj_word nn_mj_target det_mj_the conj_and_=_mj npadvmod_=_,1 amod_ni_mj amod_ni_= nn_ni_positions nn_ni_word nn_ni_source det_ni_the prep_between_a_ni dep_alignment_a nn_alignment_word det_alignment_a conj_and_ef_alignment nn_pair_phrase det_pair_a pobj_Given_pair appos_Weight_alignment appos_Weight_ef prep_Weight_Given amod_Weight_Lexical num_Weight_4.2 dep_4_Weight dep_|_4 nn_|_| nn_|_| nn_eppfef_i nn_eppfef_i dep_eppfef_iii dep_eppfef_i dep_eppfef_p nn_p_i dep_=_eppfef mark_=_as xcomp_rewritten_| advcl_rewritten_= auxpass_rewritten_be aux_rewritten_can nsubjpass_rewritten_equation advmod_rewritten_Thus appos_equation_3
P07-1108	N03-1017	o	1 Introduction For statistical machine translation -LRB- SMT -RRB- phrasebased methods -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and syntax-based methods -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knignt 2001 Melamed 2004 Chiang 2005 Quick et al. 2005 Mellebeek et al. 2006 -RRB- outperform word-based methods -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_methods_Brown amod_methods_word-based dobj_outperform_methods nsubj_outperform_methods nsubj_outperform_methods ccomp_outperform_Introduction nn_al._et nn_al._Mellebeek dep_al._2005 nn_al._et nn_al._Quick num_Chiang_2005 num_Melamed_2004 conj_and_Yamada_al. conj_and_Yamada_Chiang conj_and_Yamada_Melamed conj_and_Yamada_2001 conj_and_Yamada_Knignt num_al._2000 nn_al._et nn_al._Alshawi amod_Wu_2006 dep_Wu_al. dep_Wu_al. dep_Wu_Chiang dep_Wu_Melamed dep_Wu_2001 dep_Wu_Knignt dep_Wu_Yamada dep_Wu_al. amod_Wu_1997 dep_methods_Wu amod_methods_syntax-based dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_methods_methods dep_methods_Koehn amod_methods_phrasebased appos_translation_SMT nn_translation_machine amod_translation_statistical prep_for_Introduction_translation num_Introduction_1
P07-1119	N03-1017	o	The phrase-based approach developed for statistical machine translation -LRB- Koehn et al. 2003 -RRB- is designed to overcome the restrictions on many-tomany mappings in word-based translation models	nn_models_translation amod_models_word-based amod_mappings_many-tomany prep_in_restrictions_models prep_on_restrictions_mappings det_restrictions_the dobj_overcome_restrictions aux_overcome_to xcomp_designed_overcome auxpass_designed_is amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_machine amod_translation_statistical dep_developed_designed dep_developed_Koehn prep_for_developed_translation nsubj_developed_approach amod_approach_phrase-based det_approach_The ccomp_``_developed
P07-1119	N03-1017	o	Starting from a word-based alignment for each pair of sentences the training for the algorithm accepts all contiguous bilingual phrase pairs -LRB- up to a predetermined maximum length -RRB- whose words are only aligned with each other -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_other_Koehn det_other_each prep_with_aligned_other advmod_aligned_only auxpass_aligned_are nsubjpass_aligned_words advmod_aligned_up poss_words_whose nn_length_maximum amod_length_predetermined det_length_a prep_to_up_length vmod_pairs_aligned nn_pairs_phrase amod_pairs_bilingual amod_pairs_contiguous det_pairs_all dobj_accepts_pairs nsubj_accepts_training vmod_accepts_Starting det_algorithm_the prep_for_training_algorithm det_training_the prep_of_pair_sentences det_pair_each prep_for_alignment_pair amod_alignment_word-based det_alignment_a prep_from_Starting_alignment
P07-2045	N03-1017	p	1 Motivation Phrase-based statistical machine translation -LRB- Koehn et al. 2003 -RRB- has emerged as the dominant paradigm in machine translation research	nn_research_translation nn_research_machine prep_in_paradigm_research amod_paradigm_dominant det_paradigm_the prep_as_emerged_paradigm aux_emerged_has nsubj_emerged_translation dep_2003_al. nn_al._et num_Koehn_2003 appos_translation_Koehn nn_translation_machine amod_translation_statistical amod_translation_Phrase-based nn_translation_Motivation num_translation_1
P07-2046	N03-1017	o	It is an extension of Pharaoh -LRB- Koehn et al. 2003 -RRB- and supports factor training and decoding	conj_and_training_decoding nn_training_factor dobj_supports_decoding dobj_supports_training nsubj_supports_It amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_extension_supports dep_extension_Koehn prep_of_extension_Pharaoh det_extension_an cop_extension_is nsubj_extension_It
P08-1009	N03-1017	p	Phrase-based decoding -LRB- Koehn et al. 2003 -RRB- is a dominant formalism in statistical machine translation	nn_translation_machine amod_translation_statistical prep_in_formalism_translation amod_formalism_dominant det_formalism_a cop_formalism_is nsubj_formalism_decoding amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_decoding_Koehn amod_decoding_Phrase-based
P08-1009	N03-1017	o	Early experiments with syntactically-informed phrases -LRB- Koehn et al. 2003 -RRB- and syntactic reranking of K-best lists -LRB- Och et al. 2004 -RRB- produced mostly negative results	amod_results_negative advmod_negative_mostly dobj_produced_results nsubj_produced_reranking nsubj_produced_experiments amod_Och_2004 dep_Och_al. nn_Och_et amod_lists_K-best prep_of_reranking_lists nn_reranking_syntactic amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_phrases_syntactically-informed dep_experiments_Och conj_and_experiments_reranking dep_experiments_Koehn prep_with_experiments_phrases advmod_experiments_Early ccomp_``_produced
P08-1009	N03-1017	o	Restricting phrases to syntactic constituents has been shown to harm performance -LRB- Koehn et al. 2003 -RRB- so we tighten our definition of a violation to disregard cases where the only point of overlap is obscured by our phrasal resolution	amod_resolution_phrasal poss_resolution_our agent_obscured_resolution auxpass_obscured_is csubjpass_obscured_tighten nsubj_overlap_point advmod_overlap_where prep_point_of amod_point_only det_point_the rcmod_cases_overlap nn_cases_disregard prep_to_violation_cases det_violation_a prep_of_definition_violation poss_definition_our dobj_tighten_definition nsubj_tighten_we amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dobj_harm_performance aux_harm_to parataxis_shown_obscured dep_shown_so dep_shown_Koehn xcomp_shown_harm auxpass_shown_been aux_shown_has nsubjpass_shown_phrases amod_constituents_syntactic prep_to_phrases_constituents amod_phrases_Restricting
P08-1010	N03-1017	p	The most widely used approach derives phrase pairs from word alignment matrix -LRB- Och and Ney 2003 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2003 conj_and_Och_Ney dep_matrix_2003 dep_matrix_Ney dep_matrix_Och nn_matrix_alignment nn_matrix_word nn_pairs_phrase prep_from_derives_matrix dobj_derives_pairs nsubj_derives_approach amod_approach_used det_approach_The advmod_used_widely advmod_widely_most ccomp_``_derives
P08-1010	N03-1017	o	4.1 Training and Translation Setup Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_to_similar_Pharaoh amod_model_similar amod_model_log-linear det_model_the dep_implementation_Koehn prep_of_implementation_model amod_implementation_multi-stack amod_implementation_phrase-based det_implementation_a cop_implementation_is nsubj_implementation_Setup nsubj_implementation_Training poss_decoder_Our nn_Setup_Translation dep_Training_decoder conj_and_Training_Setup num_Training_4.1
P08-1010	N03-1017	o	Since most phrases appear only a few times in training data a phrase pair translation is also evaluated by lexical weights -LRB- Koehn et al. 2003 -RRB- or term weighting -LRB- Zhao et al. 2004 -RRB- as additional features to avoid overestimation	dobj_avoid_overestimation aux_avoid_to vmod_features_avoid amod_features_additional amod_Zhao_2004 dep_Zhao_al. nn_Zhao_et nn_weighting_term dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_weights_lexical prep_as_evaluated_features dep_evaluated_Zhao conj_or_evaluated_weighting dep_evaluated_Koehn agent_evaluated_weights advmod_evaluated_also auxpass_evaluated_is nsubjpass_evaluated_translation advcl_evaluated_appear nn_translation_pair nn_translation_phrase det_translation_a nn_data_training prep_in_times_data amod_times_few det_times_a advmod_times_only dobj_appear_times nsubj_appear_phrases mark_appear_Since amod_phrases_most
P08-1010	N03-1017	o	The commonly used phrase extraction approach based on word alignment heuristics -LRB- referred as ViterbiExtract algorithm for comparison in this paper -RRB- as described in -LRB- Och 2002 Koehn et al. 2003 -RRB- is a special case of the algorithm where candidate phrase pairs are restricted to those that respect word alignment boundaries	nn_boundaries_alignment nn_boundaries_word dobj_respect_boundaries nsubj_respect_that rcmod_those_respect prep_to_restricted_those cop_restricted_are nsubj_restricted_pairs advmod_restricted_where nn_pairs_phrase nn_pairs_candidate rcmod_algorithm_restricted det_algorithm_the prep_of_case_algorithm amod_case_special det_case_a cop_case_is nsubj_case_approach num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn appos_Och_2002 prep_in_described_Och mark_described_as det_paper_this prep_in_comparison_paper nn_algorithm_ViterbiExtract prep_for_referred_comparison prep_as_referred_algorithm dep_heuristics_referred nn_heuristics_alignment nn_heuristics_word advcl_based_described prep_on_based_heuristics vmod_approach_based nn_approach_extraction nn_approach_phrase amod_approach_used det_approach_The advmod_used_commonly
P08-1011	N03-1017	o	In most statistical machine translation -LRB- SMT -RRB- models -LRB- Och et al. 2004 Koehn et al. 2003 Chiang 2005 -RRB- some of measure words can be generated without modification or additional processing	amod_processing_additional conj_or_modification_processing prep_without_generated_processing prep_without_generated_modification auxpass_generated_be aux_generated_can nsubjpass_generated_Koehn nn_words_measure prep_of_some_words dep_Chiang_2005 appos_Koehn_some dep_Koehn_Chiang num_Koehn_2003 nn_Koehn_al. nn_Koehn_et parataxis_Och_generated appos_Och_2004 dep_Och_al. nn_Och_et dep_models_Och nn_models_SMT nn_translation_machine amod_translation_statistical amod_translation_most dep_In_models pobj_In_translation dep_``_In
P08-1011	N03-1017	o	We ran GIZA + + -LRB- Och and Ney 2000 -RRB- on the training corpus in both directions with IBM model 4 and then applied the refinement rule described in -LRB- Koehn et al. 2003 -RRB- to obtain a many-to-many word alignment for each sentence pair	nn_pair_sentence det_pair_each prep_for_alignment_pair nn_alignment_word amod_alignment_many-to-many det_alignment_a dobj_obtain_alignment aux_obtain_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn xcomp_described_obtain prep_described_in vmod_rule_described nn_rule_refinement det_rule_the dobj_applied_rule advmod_applied_then nsubj_applied_We num_model_4 nn_model_IBM preconj_directions_both nn_corpus_training det_corpus_the num_Och_2000 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ conj_and_ran_applied prep_with_ran_model prep_in_ran_directions dobj_ran_+ dobj_ran_GIZA nsubj_ran_We
P08-1024	N03-1017	o	The standard solution is to approximate the maximum probability translation using a single derivation -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_derivation_Koehn amod_derivation_single det_derivation_a dobj_using_derivation vmod_translation_using nn_translation_probability nn_translation_maximum det_translation_the dobj_approximate_translation aux_approximate_to xcomp_is_approximate nsubj_is_solution amod_solution_standard det_solution_The ccomp_``_is
P08-1024	N03-1017	o	-LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_''_Koehn
P08-1049	N03-1017	o	While the research in statistical machine translation -LRB- SMT -RRB- has made significant progress most SMT systems -LRB- Koehn et al. 2003 Chiang 2007 Galleyetal. 2006 -RRB- relyonparallel corpora toextract translation entries	nn_entries_translation nn_entries_toextract nn_entries_corpora nn_entries_relyonparallel dep_entries_Koehn dep_entries_systems advcl_entries_made dep_Galleyetal._2006 dep_Chiang_Galleyetal. num_Chiang_2007 dep_Koehn_Chiang appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_systems_SMT amod_systems_most amod_progress_significant dobj_made_progress aux_made_has nsubj_made_research mark_made_While appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_research_translation det_research_the
P08-1049	N03-1017	p	However since most of statistical translation models -LRB- Koehn et al. 2003 Chiang 2007 Galley et al. 2006 -RRB- are symmetrical it is relatively easy to train a translation system to translate from English to Chinese except that weneed to train aChinese language model from the Chinese monolingual data	amod_data_monolingual amod_data_Chinese det_data_the nn_model_language amod_model_aChinese prep_from_train_data dobj_train_model aux_train_to vmod_weneed_train det_weneed_that prep_to_translate_Chinese prep_from_translate_English aux_translate_to nn_system_translation det_system_a vmod_train_translate dobj_train_system aux_train_to prep_except_easy_weneed xcomp_easy_train advmod_easy_relatively cop_easy_is nsubj_easy_it advcl_easy_symmetrical cop_symmetrical_are dep_symmetrical_Koehn prep_since_symmetrical_most advmod_symmetrical_However num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Chiang_Galley num_Chiang_2007 nn_al._et dep_Koehn_Chiang appos_Koehn_2003 dep_Koehn_al. nn_models_translation amod_models_statistical prep_of_most_models
P08-1064	N03-1017	o	However most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_methods_phrase-based det_methods_the prep_in_useful_methods acomp_proven_useful cop_proven_are nsubj_proven_that amod_phrases_non-syntactic dep_utilize_Koehn ccomp_utilize_proven advmod_utilize_well dobj_utilize_phrases aux_utilize_to xcomp_fail_utilize nsubj_fail_most advmod_fail_However prep_of_most_them
P08-1064	N03-1017	p	1 Introduction Phrase-based modeling method -LRB- Koehn et al. 2003 Och and Ney 2004a -RRB- is a simple but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well	amod_expressions_multiword prep_of_translations_expressions conj_and_reorderings_translations amod_reorderings_local advmod_model_well dobj_model_translations dobj_model_reorderings aux_model_can nsubj_model_it mark_model_since nn_translation_machine advcl_mechanism_model prep_to_mechanism_translation amod_mechanism_powerful amod_mechanism_simple det_mechanism_a cop_mechanism_is nsubj_mechanism_method conj_but_simple_powerful appos_Och_2004a conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_method_Koehn nn_method_modeling amod_method_Phrase-based nn_method_Introduction num_method_1
P08-1089	N03-1017	o	LW was originally used to validate the quality of a phrase translation pair in MT -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_pair_MT nn_pair_translation nn_pair_phrase det_pair_a prep_of_quality_pair det_quality_the dobj_validate_quality aux_validate_to dep_used_Koehn xcomp_used_validate advmod_used_originally auxpass_used_was nsubjpass_used_LW
P08-1114	N03-1017	o	Rules have the form X e f where e and f are phrases containing terminal symbols -LRB- words -RRB- and possibly co-indexed instances of the nonterminal symbol X. 2 Associated with each rule is a set of translation model features i -LRB- f e -RRB- for example one intuitively natural feature of a rule is the phrase translation -LRB- log -RRB- probability -LRB- f e -RRB- = log p -LRB- e | f -RRB- directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharaoh_Koehn prep_like_models_Pharaoh amod_models_phrase-based amod_models_non-hierarchical prep_in_feature_models amod_feature_corresponding det_feature_the prep_to_analogous_feature advmod_analogous_directly dep_f_| dep_|_e amod_p_analogous appos_p_f nn_p_log dep_=_p amod_f_= appos_f_e dep_probability_f nn_probability_translation appos_translation_log nn_translation_phrase det_translation_the cop_translation_is nsubj_translation_feature prep_for_translation_example det_rule_a prep_of_feature_rule amod_feature_natural advmod_feature_intuitively num_feature_one dep_f_e appos_i_f nn_features_model nn_features_translation prep_of_set_features det_set_a cop_set_is nsubj_set_symbol dep_set_nonterminal det_set_the det_rule_each prep_with_Associated_rule vmod_X._Associated num_X._2 dep_symbol_X. prep_of_instances_set amod_instances_co-indexed advmod_co-indexed_possibly conj_and_symbols_instances appos_symbols_words amod_symbols_terminal dobj_containing_instances dobj_containing_symbols dep_phrases_probability advmod_phrases_i vmod_phrases_containing cop_phrases_are nsubj_phrases_f nsubj_phrases_e advmod_phrases_where conj_and_e_f dep_f_phrases dep_e_f nn_e_X nn_e_form det_e_the dobj_have_e nsubj_have_Rules
P08-1114	N03-1017	o	In addition to this phrase translation probability feature Hieros feature set includes the inverse phrase translation probability log p -LRB- f | e -RRB- lexical weights lexwt -LRB- f | e -RRB- and lexwt -LRB- e | f -RRB- which are estimates of translation quality based on word-level correspondences -LRB- Koehn et al. 2003 -RRB- and a rule penalty allowing the model to learn a preference for longer or shorter derivations see -LRB- Chiang 2007 -RRB- for details	dep_Chiang_2007 prep_for_see_details dep_see_Chiang amod_derivations_shorter amod_derivations_longer conj_or_longer_shorter prep_for_preference_derivations det_preference_a dobj_learn_preference aux_learn_to det_model_the xcomp_allowing_learn dobj_allowing_model vmod_penalty_allowing nn_penalty_rule det_penalty_a amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_correspondences_Koehn amod_correspondences_word-level nn_quality_translation pobj_estimates_correspondences prepc_based_on_estimates_on prep_of_estimates_quality cop_estimates_are nsubj_estimates_which num_f_| dep_f_e dep_f_lexwt dep_f_| dep_f_lexwt conj_and_|_lexwt dep_|_e nn_|_f nn_lexwt_weights amod_lexwt_lexical dep_|_f dep_|_e nn_|_f conj_and_p_penalty rcmod_p_estimates dep_p_| nn_p_log nn_p_probability nn_p_translation nn_p_phrase amod_p_inverse det_p_the parataxis_includes_see dobj_includes_penalty dobj_includes_p nsubj_includes_set prep_in_addition_to_includes_feature nn_set_feature nn_set_Hieros nn_feature_probability nn_feature_translation nn_feature_phrase det_feature_this
P08-1115	N03-1017	o	Models that support non-monotonic decoding generally include a distortion cost such as | aibi11 | where ai is the starting position of the foreign phrasefi andbi1 is the ending position of phrase fi1 -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_fi1_phrase dep_position_Koehn prep_of_position_fi1 amod_position_ending det_position_the cop_position_is nn_andbi1_phrasefi amod_andbi1_foreign det_andbi1_the dep_position_position prep_of_position_andbi1 amod_position_starting det_position_the cop_position_is nsubj_position_ai advmod_position_where num_aibi11_| num_aibi11_| prep_such_as_cost_aibi11 nn_cost_distortion det_cost_a advcl_include_position dobj_include_cost advmod_include_generally nsubj_include_Models amod_decoding_non-monotonic dobj_support_decoding nsubj_support_that rcmod_Models_support
P08-1116	N03-1017	o	Given phrase p1 and its paraphrase p2 we compute Score3 -LRB- p1 p2 -RRB- by relative frequency -LRB- Koehn et al. 2003 -RRB- Score3 -LRB- p1 p2 -RRB- = p -LRB- p2 | p1 -RRB- = count -LRB- p2 p1 -RRB- P pprime count -LRB- pprime p1 -RRB- -LRB- 7 -RRB- People may wonder why we do not use the same method on the monolingual parallel and comparable corpora	amod_corpora_comparable conj_and_parallel_corpora amod_parallel_monolingual det_parallel_the amod_method_same det_method_the prep_on_use_corpora prep_on_use_parallel dobj_use_method neg_use_not aux_use_do nsubj_use_we advmod_use_why ccomp_wonder_use aux_wonder_may nsubj_wonder_People nn_People_count appos_pprime_p1 appos_count_7 dep_count_pprime nn_count_pprime nn_count_P nn_count_count appos_p2_p1 dep_count_p2 ccomp_=_wonder npadvmod_=_p num_p1_| nn_p1_p2 appos_p_p1 amod_=_= appos_p1_p2 amod_Score3_= dep_Score3_p1 amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_frequency_relative appos_p1_p2 dep_Score3_p1 dep_compute_Score3 dep_compute_Koehn prep_by_compute_frequency dobj_compute_Score3 nsubj_compute_we prep_compute_Given nn_p2_paraphrase poss_p2_its conj_and_p1_p2 nn_p1_phrase pobj_Given_p2 pobj_Given_p1
P08-2041	N03-1017	n	1 Introduction Currently most of the phrase-based statistical machine translation -LRB- PBSMT -RRB- models -LRB- Marcu and Wong 2002 Koehn et al. 2003 -RRB- adopt full matching strategy for phrase translation which means that a phrase pair -LRB- tildewidef tildewidee -RRB- can be used for translating a source phrase f only if tildewidef = f. Due to lack of generalization ability the full matching strategy has some limitations	det_limitations_some dobj_has_limitations nsubj_has_strategy nn_strategy_matching amod_strategy_full det_strategy_the nn_ability_generalization prep_of_lack_ability rcmod_f._has prep_due_to_f._lack dep_=_f. amod_tildewidef_= prep_if_only_tildewidef appos_f_only nn_f_phrase nn_f_source det_f_a dobj_translating_f prepc_for_used_translating auxpass_used_be aux_used_can nsubjpass_used_pair mark_used_that appos_tildewidef_tildewidee dep_pair_tildewidef nn_pair_phrase det_pair_a ccomp_means_used nsubj_means_which rcmod_translation_means nn_translation_phrase prep_for_strategy_translation nn_strategy_matching amod_strategy_full dobj_adopt_strategy nsubj_adopt_most advmod_adopt_Currently nsubj_adopt_Introduction num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong appos_models_Wong appos_models_Marcu nn_models_translation appos_translation_PBSMT nn_translation_machine amod_translation_statistical amod_translation_phrase-based det_translation_the prep_of_most_models num_Introduction_1 ccomp_``_adopt
P09-1036	N03-1017	n	In such a process original phrase-based decoding -LRB- Koehn et al. 2003 -RRB- does not take advantage of any linguistic analysis which however is broadly used in rule-based approaches	amod_approaches_rule-based prep_in_used_approaches advmod_used_broadly auxpass_used_is advmod_used_however nsubjpass_used_which rcmod_analysis_used amod_analysis_linguistic det_analysis_any prep_of_advantage_analysis dobj_take_advantage neg_take_not aux_take_does nsubj_take_decoding prep_in_take_process amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_decoding_Koehn amod_decoding_phrase-based amod_decoding_original det_process_a predet_process_such
P09-1036	N03-1017	o	This unfortunately significantly jeopardizes performance -LRB- Koehn et al. 2003 Xiong et al. 2008 -RRB- because by integrating syntactic constraint into decoding as a hard constraint it simply prohibits any other useful non-syntactic translations which violate constituent boundaries	amod_boundaries_constituent dobj_violate_boundaries nsubj_violate_which rcmod_translations_violate amod_translations_non-syntactic amod_translations_useful amod_translations_other det_translations_any dobj_prohibits_translations advmod_prohibits_simply nsubj_prohibits_it prepc_by_prohibits_integrating mwe_prohibits_because amod_constraint_hard det_constraint_a prep_as_decoding_constraint nn_constraint_syntactic prepc_into_integrating_decoding dobj_integrating_constraint num_Xiong_2008 nn_Xiong_al. nn_Xiong_et dep_Koehn_Xiong appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et parataxis_jeopardizes_prohibits dep_jeopardizes_Koehn dobj_jeopardizes_performance advmod_jeopardizes_significantly advmod_jeopardizes_unfortunately nsubj_jeopardizes_This ccomp_``_jeopardizes
P09-1038	N03-1017	o	5.2 Translation experiments with a bigram language model In this section we consider two real translation tasks namely translation from English to French trained on Europarl -LRB- Koehn et al. 2003 -RRB- and translation from German to Spanish training on the NewsCommentary corpus	nn_corpus_NewsCommentary det_corpus_the prep_on_training_corpus amod_training_Spanish amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_on_trained_Europarl dep_French_Koehn vmod_French_trained prep_to_translation_training prep_from_translation_German conj_and_translation_translation prep_to_translation_French prep_from_translation_English dep_tasks_translation dep_tasks_translation advmod_tasks_namely nn_tasks_translation amod_tasks_real num_tasks_two dobj_consider_tasks nsubj_consider_we nsubj_consider_experiments det_section_this prep_in_model_section nn_model_language nn_model_bigram det_model_a prep_with_experiments_model nn_experiments_Translation num_experiments_5.2
P09-1038	N03-1017	p	1 Introduction Phrase-based systems -LRB- Koehn et al. 2003 -RRB- are probably the most widespread class of Statistical Machine Translation systems and arguably one of the most successful	advmod_successful_most det_successful_the prep_of_one_successful advmod_one_arguably nn_systems_Translation nn_systems_Machine amod_systems_Statistical conj_and_class_one prep_of_class_systems amod_class_widespread det_class_the advmod_class_probably cop_class_are nsubj_class_systems advmod_widespread_most amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_systems_Koehn amod_systems_Phrase-based nn_systems_Introduction num_systems_1
P09-1063	N03-1017	o	We obtained word alignments of the training data by first running GIZA + + -LRB- Och and Ney 2003 -RRB- and then applying the refinement rule grow-diagfinal-and -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diagfinal-and_Koehn nn_grow-diagfinal-and_rule nn_grow-diagfinal-and_refinement det_grow-diagfinal-and_the dobj_applying_grow-diagfinal-and nsubj_applying_then nsubj_applying_+ nsubj_applying_GIZA num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_then conj_+_GIZA_+ ccomp_running_applying xcomp_first_running nn_data_training det_data_the prep_of_alignments_data nn_alignments_word prepc_by_obtained_first dobj_obtained_alignments nsubj_obtained_We
P09-1065	N03-1017	o	We obtained word alignments of training data by first running GIZA + + -LRB- Och and Ney 2003 -RRB- and then applying the refinement rule grow-diag-final-and -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diag-final-and_Koehn nn_grow-diag-final-and_rule nn_grow-diag-final-and_refinement det_grow-diag-final-and_the dobj_applying_grow-diag-final-and nsubj_applying_then nsubj_applying_+ nsubj_applying_GIZA num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_then conj_+_GIZA_+ ccomp_running_applying xcomp_first_running nn_data_training prep_of_alignments_data nn_alignments_word prepc_by_obtained_first dobj_obtained_alignments nsubj_obtained_We
P09-1065	N03-1017	o	On the other hand other authors -LRB- e.g. -LRB- Och and Ney 2004 Koehn et al. 2003 Chiang 2007 -RRB- -RRB- do use the expression phrase-based models	amod_models_phrase-based nn_models_expression det_models_the dobj_use_models aux_use_do nsubj_use_Ney nsubj_use_Och advmod_use_e.g. dep_Chiang_2007 dep_Koehn_Chiang num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn dep_Och_2004 conj_and_Och_Ney dep_authors_use amod_authors_other dep_,_authors amod_hand_other det_hand_the pobj_On_hand dep_``_On
P09-1067	N03-1017	o	They recover additional latent variables so-called nuisance variablesthat are not of interest to the user .1 For example though machine translation -LRB- MT -RRB- seeks to output a string typical MT systems -LRB- Koehn et al. 2003 Chiang 2007 -RRB- 1These nuisance variables may be annotated in training data but it is more common for them to be latent even there i.e. there is no supervision as to their correct values	amod_values_correct poss_values_their pobj_supervision_values prepc_as_to_supervision_to neg_supervision_no nsubj_is_supervision expl_is_there advmod_there_even parataxis_latent_is dep_latent_i.e. advmod_latent_there cop_latent_be aux_latent_to xcomp_common_latent prep_for_common_them advmod_common_more cop_common_is nsubj_common_it nn_data_training conj_but_annotated_common prep_in_annotated_data cop_annotated_be aux_annotated_may nn_variables_nuisance nn_variables_1These nn_variables_string nn_variables_output num_Chiang_2007 dep_Koehn_Chiang appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_systems_MT amod_systems_typical appos_string_Koehn conj_string_systems det_string_a dep_seeks_common dep_seeks_annotated prep_to_seeks_variables nsubj_seeks_translation mark_seeks_though appos_translation_MT nn_translation_machine nn_.1_user det_.1_the prep_to_interest_.1 prep_for_are_example prep_of_are_interest neg_are_not nsubj_are_variablesthat nn_variablesthat_nuisance amod_variablesthat_so-called rcmod_variables_are amod_variables_latent amod_variables_additional advcl_recover_seeks dobj_recover_variables nsubj_recover_They ccomp_``_recover
P09-1067	N03-1017	o	594 2.3 Viterbi Approximation To approximate the intractable decoding problem of -LRB- 2 -RRB- most MT systems -LRB- Koehn et al. 2003 Chiang 2007 -RRB- use a simple Viterbi approximation y = argmax yT -LRB- x -RRB- pViterbi -LRB- y | x -RRB- -LRB- 4 -RRB- = argmax yT -LRB- x -RRB- max dD -LRB- x y -RRB- p -LRB- y d | x -RRB- -LRB- 5 -RRB- = Y parenleftBigg argmax dD -LRB- x -RRB- p -LRB- y d | x -RRB- parenrightBigg -LRB- 6 -RRB- Clearly -LRB- 5 -RRB- replaces the sum in -LRB- 2 -RRB- with a max	det_max_a prep_with_2_max det_sum_the prep_in_replaces_2 dobj_replaces_sum nsubj_replaces_dD dep_replaces_= dep_replaces_4 dep_replaces_pViterbi dep_replaces_= nsubj_replaces_y advmod_parenrightBigg_Clearly appos_parenrightBigg_6 dep_parenrightBigg_y nn_parenrightBigg_p num_x_| nn_x_d dep_y_x nn_p_dD appos_dD_x nn_dD_argmax nn_dD_parenleftBigg nn_dD_Y dep_=_parenrightBigg npadvmod_=_p num_x_| nn_x_d appos_y_x appos_p_5 dep_p_y appos_x_y appos_dD_5 amod_dD_= dep_dD_x nn_dD_max nn_dD_yT appos_yT_x nn_yT_argmax num_x_| nn_x_y appos_pViterbi_x nn_pViterbi_yT appos_yT_x nn_yT_argmax rcmod_approximation_replaces nn_approximation_Viterbi amod_approximation_simple det_approximation_a dobj_use_approximation nsubj_use_systems dep_Chiang_2007 dep_Koehn_Chiang appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_systems_Koehn amod_systems_MT advmod_MT_most ccomp_2_use prep_of_problem_2 amod_problem_decoding amod_problem_intractable det_problem_the prep_to_problem_approximate nsubj_problem_Approximation nn_Approximation_Viterbi num_Approximation_2.3 num_Approximation_594
P09-1087	N03-1017	p	1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years -LRB- Chiang 2005 Marcu et al. 2006 Shen et al. 2008 -RRB- and often outperform phrase-based systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- on target-language fluency and adequacy	conj_and_fluency_adequacy nn_fluency_target-language num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney prep_on_systems_adequacy prep_on_systems_fluency appos_systems_2004 appos_systems_Ney appos_systems_Och amod_systems_phrase-based amod_systems_outperform advmod_outperform_often num_Shen_2008 nn_Shen_al. nn_Shen_et num_Marcu_2006 nn_Marcu_al. nn_Marcu_et dep_Chiang_Shen dep_Chiang_Marcu appos_Chiang_2005 appos_years_Chiang amod_years_recent advmod_successful_increasingly conj_and_proven_systems prep_in_proven_years acomp_proven_successful aux_proven_have nsubj_proven_approaches nn_translation_machine prep_to_approaches_translation amod_approaches_Hierarchical nn_approaches_Introduction num_approaches_1 ccomp_``_systems ccomp_``_proven
P09-1088	N03-1017	o	These wordbased models are used to find the latent wordalignments between bilingual sentence pairs from which a weighted string transducer can be induced -LRB- either finite state -LRB- Koehn et al. 2003 -RRB- or synchronous context free grammar -LRB- Chiang 2007 -RRB- -RRB-	dep_Chiang_2007 amod_grammar_free nn_grammar_context amod_grammar_synchronous dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_state_Chiang conj_or_state_grammar dep_state_Koehn amod_state_finite preconj_state_either ccomp_-LRB-_grammar ccomp_-LRB-_state auxpass_induced_be aux_induced_can nsubjpass_induced_transducer prep_from_induced_which nn_transducer_string amod_transducer_weighted det_transducer_a dep_pairs_induced nn_pairs_sentence amod_pairs_bilingual prep_between_wordalignments_pairs amod_wordalignments_latent det_wordalignments_the dobj_find_wordalignments aux_find_to xcomp_used_find auxpass_used_are nsubjpass_used_models amod_models_wordbased det_models_These
P09-1088	N03-1017	o	We use the GIZA + + implementation of IBM Model 4 -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- coupled with the phrase extraction heuristics of Koehn et al.	nn_al._et nn_al._Koehn prep_of_heuristics_al. nn_heuristics_extraction nn_heuristics_phrase det_heuristics_the prep_with_coupled_heuristics dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM prep_of_implementation_Model pobj_+_implementation dep_GIZA_Brown conj_+_GIZA_+ det_GIZA_the dep_use_coupled dobj_use_+ dobj_use_GIZA nsubj_use_We
P09-1088	N03-1017	p	1 Introduction The field of machine translation has seen many advances in recent years most notably the shift from word-based -LRB- Brown et al. 1993 -RRB- to phrasebased models which use token n-grams as translation units -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_units_Koehn nn_units_translation amod_n-grams_token prep_as_use_units dobj_use_n-grams nsubj_use_which rcmod_models_use amod_models_phrasebased dep_al._1993 nn_al._et advmod_Brown_al. dep_word-based_Brown prep_to_shift_models prep_from_shift_word-based det_shift_the advmod_shift_notably advmod_notably_most amod_years_recent amod_advances_many prep_in_seen_years dobj_seen_advances aux_seen_has nsubj_seen_field nn_translation_machine prep_of_field_translation det_field_The dep_Introduction_shift rcmod_Introduction_seen num_Introduction_1
P09-1094	N03-1017	o	Actually it is defined similarly to the translation model in SMT -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_model_SMT nn_model_translation det_model_the dep_defined_Koehn prep_to_defined_model advmod_defined_similarly auxpass_defined_is nsubjpass_defined_it advmod_defined_Actually
P09-1103	N03-1017	o	Between them the phrase-based approach -LRB- Marcu and Wong 2002 Koehn et al 2003 Och and Ney 2004 -RRB- allows local reordering and contiguous phrase translation	nn_translation_phrase amod_translation_contiguous conj_and_reordering_translation amod_reordering_local dobj_allows_translation dobj_allows_reordering nsubj_allows_approach prep_between_allows_them dep_Och_2004 conj_and_Och_Ney appos_al_2003 nn_al_et nn_al_Koehn dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_al conj_and_Marcu_2002 conj_and_Marcu_Wong dep_approach_al dep_approach_2002 dep_approach_Wong dep_approach_Marcu amod_approach_phrase-based det_approach_the
P09-2031	N03-1017	o	1LDC2002E18 -LRB- 4,000 sentences -RRB- LDC2002T01 LDC2003E07 LDC2003E14 LDC2004T07 LDC2005T10 LDC2004T08 HK Hansards -LRB- 500,000 sentences -RRB- 2http / / www.statmt.org/wmt07/shared-task.html For both the tasks the word alignment were trained by GIZA + + in two translation directions and refined by grow-diag-final method -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_method_grow-diag-final prep_by_refined_method cc_refined_and nn_directions_translation num_directions_two prep_in_+_directions conj_+_by_refined conj_+_by_+ pobj_by_GIZA dep_trained_Koehn prep_trained_refined prep_trained_+ prep_trained_by auxpass_trained_were nsubjpass_trained_alignment nn_alignment_word det_alignment_the det_tasks_the preconj_tasks_both parataxis_www.statmt.org/wmt07/shared-task.html_trained prep_for_www.statmt.org/wmt07/shared-task.html_tasks dep_www.statmt.org/wmt07/shared-task.html_1LDC2002E18 num_sentences_500,000 num_Hansards_2http appos_Hansards_sentences nn_Hansards_HK nn_Hansards_LDC2004T08 conj_LDC2003E07_Hansards conj_LDC2003E07_LDC2005T10 conj_LDC2003E07_LDC2004T07 conj_LDC2003E07_LDC2003E14 num_sentences_4,000 dep_1LDC2002E18_LDC2003E07 appos_1LDC2002E18_LDC2002T01 appos_1LDC2002E18_sentences
P09-2037	N03-1017	o	-LRB- Koehn et al. 2003 -RRB- -RRB- in which translation and language models are trainable separately too	advmod_trainable_too advmod_trainable_separately cop_trainable_are nsubj_trainable_models prep_in_trainable_which nn_models_language nn_models_translation conj_and_translation_language rcmod_Koehn_trainable amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_''_Koehn
P09-2058	N03-1017	o	This is applied to maximize coverage which is similar as the final in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_final_in det_final_the prep_as_similar_final cop_similar_is nsubj_similar_which rcmod_coverage_similar dobj_maximize_coverage aux_maximize_to xcomp_applied_maximize auxpass_applied_is nsubjpass_applied_This ccomp_``_applied
P09-2058	N03-1017	o	Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_to_similar_Pharaoh amod_model_similar amod_model_log-linear det_model_the dep_implementation_Koehn prep_of_implementation_model amod_implementation_multi-stack amod_implementation_phrase-based det_implementation_a cop_implementation_is nsubj_implementation_decoder poss_decoder_Our
P09-2058	N03-1017	o	The next two methods are heuristic -LRB- H -RRB- in -LRB- Och and Ney 2003 -RRB- and grow-diagonal -LRB- GD -RRB- proposed in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_proposed_in vmod_grow-diagonal_proposed appos_grow-diagonal_GD conj_and_Och_grow-diagonal conj_and_Och_2003 conj_and_Och_Ney prep_in_heuristic_grow-diagonal prep_in_heuristic_2003 prep_in_heuristic_Ney prep_in_heuristic_Och appos_heuristic_H cop_heuristic_are nsubj_heuristic_methods num_methods_two amod_methods_next det_methods_The
P09-2058	N03-1017	o	It is a fundamental and often a necessary step before linguistic knowledge acquisitions such as training a phrase translation table in phrasal machine translation -LRB- MT -RRB- system -LRB- Koehn et al. 2003 -RRB- or extracting hierarchial phrase rules or synchronized grammars in syntax-based translation framework	nn_framework_translation amod_framework_syntax-based prep_in_grammars_framework amod_grammars_synchronized conj_or_rules_grammars nn_rules_phrase amod_rules_hierarchial dobj_extracting_grammars dobj_extracting_rules amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_or_system_extracting dep_system_Koehn nn_system_MT dep_translation_extracting dep_translation_system nn_translation_machine amod_translation_phrasal nn_table_translation nn_table_phrase det_table_a prep_in_training_translation dobj_training_table prepc_such_as_acquisitions_training nn_acquisitions_knowledge amod_acquisitions_linguistic prep_before_step_acquisitions amod_step_necessary det_step_a dep_fundamental_step conj_and_fundamental_often amod_a_often amod_a_fundamental nsubj_is_a dep_It_is dep_``_It
P09-2060	N03-1017	o	1 Introduction Phrase-based translation -LRB- Koehn et al. 2003 -RRB- and hierarchical phrase-based translation -LRB- Chiang 2005 -RRB- are the state of the art in statistical machine translation -LRB- SMT -RRB- techniques	nn_techniques_translation appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_art_techniques det_art_the prep_of_state_art det_state_the cop_state_are nsubj_state_translation nsubj_state_translation amod_Chiang_2005 appos_translation_Chiang amod_translation_phrase-based amod_translation_hierarchical amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_translation_translation dep_translation_Koehn amod_translation_Phrase-based nn_translation_Introduction num_translation_1
P09-4005	N03-1017	o	4 Options from the Translation Table Phrase-based statistical machine translation methods acquire their translation knowledge in form of large phrase translation tables automatically from large amounts of translated texts -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_texts_translated prep_of_amounts_texts amod_amounts_large nn_tables_translation nn_tables_phrase amod_tables_large prep_of_form_tables nn_knowledge_translation poss_knowledge_their dep_acquire_Koehn prep_from_acquire_amounts advmod_acquire_automatically prep_in_acquire_form dobj_acquire_knowledge nsubj_acquire_Options nn_methods_translation nn_methods_machine amod_methods_statistical amod_methods_Phrase-based nn_methods_Table nn_methods_Translation det_methods_the prep_from_Options_methods num_Options_4 ccomp_``_acquire
W03-1001	N03-1017	o	A word link extension algorithm similar to the one presented in this paper is given in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_given_in auxpass_given_is nsubjpass_given_algorithm det_paper_this prep_in_presented_paper vmod_one_presented det_one_the prep_to_similar_one amod_algorithm_similar nn_algorithm_extension nn_algorithm_link nn_algorithm_word det_algorithm_A ccomp_``_given
W05-0820	N03-1017	o	-LRB- 2004 -RRB- -RRB- better language-specific preprocessing -LRB- Koehn and Knight 2003 -RRB- and restructuring -LRB- Collins et al. 2005 -RRB- additional feature functions such as word class language models and minimum error rate training -LRB- Och 2003 -RRB- to optimize parameters	dobj_optimize_parameters aux_optimize_to dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum nn_models_language nn_models_class nn_models_word prep_such_as_functions_models nn_functions_feature amod_functions_additional amod_Collins_2005 dep_Collins_al. nn_Collins_et amod_Koehn_2003 conj_and_Koehn_Knight vmod_preprocessing_optimize conj_and_preprocessing_training conj_and_preprocessing_functions dep_preprocessing_Collins conj_and_preprocessing_restructuring dep_preprocessing_Knight dep_preprocessing_Koehn amod_preprocessing_language-specific amod_preprocessing_better dep_preprocessing_2004
W05-0823	N03-1017	o	1 Introduction During the last decade statistical machine translation -LRB- SMT -RRB- systems have evolved from the original word-based approach -LRB- Brown et al. 1993 -RRB- into phrase-based translation systems -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_systems_translation amod_systems_phrase-based dep_al._1993 nn_al._et amod_al._Brown dep_approach_al. amod_approach_word-based amod_approach_original det_approach_the dep_evolved_Koehn prep_into_evolved_systems prep_from_evolved_approach aux_evolved_have nsubj_evolved_systems ccomp_evolved_Introduction nn_systems_translation appos_translation_SMT nn_translation_machine amod_translation_statistical amod_decade_last det_decade_the prep_during_Introduction_decade num_Introduction_1
W05-0826	N03-1017	o	See -LRB- Och and Ney 2000 -RRB- -LRB- Yamada and Knight 2001 -RRB- -LRB- Koehn and Knight 2002 -RRB- -LRB- Koehn et al. 2003 -RRB- -LRB- Schafer and Yarowsky 2003 -RRB- and -LRB- Gildea 2003 -RRB-	dep_Gildea_2003 dep_Schafer_2003 conj_and_Schafer_Yarowsky amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_Koehn_2002 conj_and_Koehn_Knight amod_Yamada_2001 conj_and_Yamada_Knight dep_Och_2000 conj_and_Och_Ney conj_and_See_Gildea dep_See_Yarowsky dep_See_Schafer dep_See_Koehn dep_See_Knight dep_See_Koehn dep_See_Knight dep_See_Yamada dep_See_Ney dep_See_Och ccomp_``_Gildea ccomp_``_See
W05-0829	N03-1017	p	1 Introduction In recent years various phrase translation approaches -LRB- Marcu and Wong 2002 Och et al. 1999 Koehn et al. 2003 -RRB- have been shown to outperform word-to-word translation models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_translation amod_models_word-to-word dobj_outperform_models aux_outperform_to xcomp_shown_outperform auxpass_shown_been aux_shown_have nsubjpass_shown_approaches dep_shown_Introduction num_Koehn_2003 nn_Koehn_al. nn_Koehn_et num_Och_1999 nn_Och_al. nn_Och_et dep_Marcu_Koehn dep_Marcu_Och num_Marcu_2002 conj_and_Marcu_Wong appos_approaches_Wong appos_approaches_Marcu nn_approaches_translation nn_approaches_phrase amod_approaches_various amod_years_recent prep_in_Introduction_years num_Introduction_1
W05-0830	N03-1017	o	Whereas language generation has benefited from syntax -LSB- Wu 1997 Alshawi et al. 2000 -RSB- the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor -LSB- Koehn et al. 2003 -RSB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et cop_poor_be aux_poor_to dep_reported_Koehn xcomp_reported_poor auxpass_reported_been aux_reported_has nsubjpass_reported_performance advcl_reported_benefited amod_phrases_syntactic prep_on_relying_phrases advmod_relying_solely advmod_relying_when nn_translation_machine amod_translation_phrase-based amod_translation_statistical vmod_performance_relying prep_of_performance_translation det_performance_the nn_al._et nn_al._Alshawi amod_Wu_2000 dep_Wu_al. num_Wu_1997 appos_syntax_Wu prep_from_benefited_syntax aux_benefited_has nsubj_benefited_generation mark_benefited_Whereas nn_generation_language
W05-0830	N03-1017	o	The inclusion of phrases longer than three words in translation resources has been avoided as it has been shown not to have a strong impact on translation performance -LSB- Koehn et al. 2003 -RSB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_performance_translation prep_on_impact_performance amod_impact_strong det_impact_a dobj_have_impact aux_have_to neg_have_not dep_shown_Koehn xcomp_shown_have auxpass_shown_been aux_shown_has nsubjpass_shown_it mark_shown_as advcl_,_shown auxpass_avoided_been aux_avoided_has nsubjpass_avoided_words mark_avoided_than nn_resources_translation prep_in_words_resources num_words_three ccomp_longer_avoided amod_phrases_longer prep_of_inclusion_phrases det_inclusion_The dep_``_inclusion
W05-0833	N03-1017	o	-LRB- Koehn et al. 2003 -RRB- -LRB- Och 2003 -RRB- -RRB-	dep_Och_2003 dep_Koehn_Och amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_''_Koehn
W05-0833	N03-1017	o	Accordingly in this section we describe a set of experiments which extends the work of -LRB- Way and Gough 2005 -RRB- by evaluating the Marker-based EBMT system of -LRB- Gough & Way 2004b -RRB- against a phrase-based SMT system built using the following components Giza + + to extract the word-level correspondences The Giza + + word alignments are then refined and used to extract phrasal alignments -LRB- -LRB- Och & Ney 2003 -RRB- or -LRB- Koehn et al. 2003 -RRB- for a more recent implementation -RRB- Probabilities of the extracted phrases are calculated from relative frequencies The resulting phrase translation table is passed to the Pharaoh phrase-based SMT decoder which along with SRI language modelling toolkit5 performs translation	dobj_performs_translation pobj_performs_toolkit5 prepc_along_with_performs_with nsubj_performs_which amod_toolkit5_modelling nn_toolkit5_language nn_language_SRI rcmod_decoder_performs nn_decoder_SMT amod_decoder_phrase-based nn_decoder_Pharaoh det_decoder_the prep_to_passed_decoder auxpass_passed_is nsubjpass_passed_table nn_table_translation nn_table_phrase amod_table_resulting det_table_The amod_frequencies_relative parataxis_calculated_passed prep_from_calculated_frequencies auxpass_calculated_are nsubjpass_calculated_Probabilities amod_phrases_extracted det_phrases_the prep_of_Probabilities_phrases amod_implementation_recent det_implementation_a advmod_recent_more prep_for_Koehn_implementation amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Och_calculated conj_or_Och_Koehn dep_Och_2003 conj_and_Och_Ney dep_-LRB-_Koehn dep_-LRB-_Ney dep_-LRB-_Och amod_alignments_phrasal dobj_extract_alignments aux_extract_to xcomp_used_extract nsubjpass_used_Giza conj_and_refined_used advmod_refined_then auxpass_refined_are nsubjpass_refined_+ nsubjpass_refined_Giza nn_alignments_word pobj_+_alignments conj_+_Giza_+ det_Giza_The amod_correspondences_word-level det_correspondences_the dobj_extract_correspondences aux_extract_to conj_+_Giza_used conj_+_Giza_refined conj_+_Giza_extract conj_+_Giza_+ dep_components_refined dep_components_extract dep_components_+ dep_components_Giza amod_components_following det_components_the dobj_using_components xcomp_built_using vmod_system_built nn_system_SMT amod_system_phrase-based det_system_a dep_Gough_2004b conj_and_Gough_Way prep_of_system_Way prep_of_system_Gough nn_system_EBMT amod_system_Marker-based det_system_the prep_against_evaluating_system dobj_evaluating_system dep_Way_2005 conj_and_Way_Gough prep_of_work_Gough prep_of_work_Way det_work_the prepc_by_extends_evaluating dobj_extends_work nsubj_extends_which rcmod_experiments_extends prep_of_set_experiments det_set_a dobj_describe_set nsubj_describe_we prep_in_describe_section advmod_describe_Accordingly det_section_this
W05-0836	N03-1017	o	Under a phrase based translation model -LRB- Koehn et al. 2003 Marcu and Wong 2002 -RRB- this distinction is important and will be discussed in more detail	amod_detail_more prep_in_discussed_detail auxpass_discussed_be aux_discussed_will nsubjpass_discussed_distinction conj_and_important_discussed cop_important_is nsubj_important_distinction det_distinction_this amod_Marcu_2002 conj_and_Marcu_Wong dep_Koehn_Wong dep_Koehn_Marcu appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et rcmod_model_discussed rcmod_model_important appos_model_Koehn nn_model_translation amod_model_based dep_phrase_model det_phrase_a pobj_Under_phrase dep_``_Under
W05-0836	N03-1017	o	The first system is the Pharaoh decoder provided by -LRB- Koehn et al. 2003 -RRB- for the shared data task	nn_task_data amod_task_shared det_task_the prep_for_Koehn_task amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et agent_provided_Koehn vmod_decoder_provided nn_decoder_Pharaoh det_decoder_the cop_decoder_is nsubj_decoder_system amod_system_first det_system_The
W05-0836	N03-1017	o	For further information on these parameter settings confer -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_confer_Koehn prep_for_confer_information nn_settings_parameter det_settings_these prep_on_information_settings amod_information_further
W05-0908	N03-1017	o	In the area of statistical machine translation -LRB- SMT -RRB- recently a combination of the BLEU evaluation metric -LRB- Papineni et al. 2001 -RRB- and the bootstrap method for statistical significance testing -LRB- Efron and Tibshirani 1993 -RRB- has become popular -LRB- Och 2003 Kumar and Byrne 2004 Koehn 2004b Zhang et al. 2004 -RRB-	num_Zhang_2004 nn_Zhang_al. nn_Zhang_et conj_Koehn_2004b dep_Kumar_Zhang conj_and_Kumar_Koehn conj_and_Kumar_2004 conj_and_Kumar_Byrne conj_Och_Koehn conj_Och_2004 conj_Och_Byrne conj_Och_Kumar conj_Och_2003 dep_become_Och acomp_become_popular aux_become_has nsubj_become_method nsubj_become_Papineni advmod_become_metric dep_Efron_1993 conj_and_Efron_Tibshirani appos_testing_Tibshirani appos_testing_Efron nn_testing_significance amod_testing_statistical prep_for_method_testing nn_method_bootstrap det_method_the conj_al._2001 nn_al._et conj_and_Papineni_method dep_Papineni_al. dep_evaluation_become amod_BLEU_evaluation dep_the_BLEU prep_of_combination_the det_combination_a advmod_combination_recently prep_in_combination_area appos_translation_SMT nn_translation_machine amod_translation_statistical prep_of_area_translation det_area_the
W06-1606	N03-1017	p	1 Introduction During the last four years various implementations and extentions to phrase-based statistical models -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB- have led to significant increases in machine translation accuracy	nn_accuracy_translation nn_accuracy_machine prep_in_increases_accuracy amod_increases_significant prep_to_led_increases aux_led_have nsubj_led_extentions nsubj_led_implementations dep_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong appos_models_Koehn appos_models_Wong appos_models_Marcu amod_models_statistical amod_models_phrase-based prep_to_implementations_models conj_and_implementations_extentions amod_implementations_various num_years_four amod_years_last det_years_the rcmod_Introduction_led prep_during_Introduction_years num_Introduction_1
W06-1607	N03-1017	o	Traditionally maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities -LRB- Koehn et al. 2003 -RRB- eg p -LRB- s | t -RRB- = c -LRB- s t -RRB- / summationtexts c -LRB- s t -RRB- -LRB- since the estimation problems for p -LRB- s | t -RRB- and p -LRB- t | s -RRB- are symmetrical we will usually refer only to p -LRB- s | t -RRB- for brevity -RRB-	prep_for_|_brevity dobj_|_t nsubj_|_s dep_p_| prep_to_refer_p advmod_refer_only advmod_refer_usually aux_refer_will nsubj_refer_we cop_symmetrical_are nsubj_symmetrical_problems mark_symmetrical_since num_s_| nn_s_t nn_s_p dobj_|_t nsubj_|_s conj_and_p_s dep_p_| prep_for_problems_s prep_for_problems_p nn_problems_estimation det_problems_the appos_s_t dep_c_s nn_c_summationtexts rcmod_s_refer dep_s_symmetrical dep_s_c appos_s_t nn_s_c amod_s_= dep_s_| nn_s_p dobj_|_t nsubj_|_s amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_probabilities_conditional dobj_obtain_probabilities aux_obtain_to dobj_used_s dep_used_eg dep_used_Koehn xcomp_used_obtain auxpass_used_is nsubjpass_used_estimation advmod_used_Traditionally amod_frequencies_relative prep_from_estimation_frequencies amod_estimation_maximum-likelihood
W06-1607	N03-1017	o	The features used in this study are the length of t a single-parameter distortion penalty on phrase reordering in a as described in -LRB- Koehn et al. 2003 -RRB- phrase translation model probabilities and trigram language model probabilities logp -LRB- t -RRB- using Kneser-Ney smoothing as implemented in the SRILM toolkit -LRB- Stolcke 2002 -RRB-	amod_Stolcke_2002 dep_toolkit_Stolcke nn_toolkit_SRILM det_toolkit_the prep_in_implemented_toolkit mark_implemented_as nn_smoothing_Kneser-Ney advcl_using_implemented dobj_using_smoothing appos_logp_t dep_probabilities_logp nn_probabilities_model nn_probabilities_language nn_probabilities_trigram nn_probabilities_model nn_probabilities_translation nn_probabilities_phrase amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in mark_described_as nn_reordering_phrase prep_in_penalty_a prep_on_penalty_reordering nn_penalty_distortion amod_penalty_single-parameter det_penalty_a vmod_length_using conj_and_length_probabilities conj_and_length_probabilities dep_length_described dep_length_penalty prep_of_length_t det_length_the dep_are_probabilities dep_are_probabilities dep_are_length nsubj_are_features det_study_this prep_in_used_study vmod_features_used det_features_The
W06-1607	N03-1017	o	To derive the joint counts c -LRB- s t -RRB- from which p -LRB- s | t -RRB- and p -LRB- t | s -RRB- are estimated we use the phrase induction algorithm described in -LRB- Koehn et al. 2003 -RRB- with symmetrized word alignments generated using IBM model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_IBM dobj_using_model vmod_generated_using vmod_alignments_generated nn_alignments_word amod_alignments_symmetrized dep_Koehn_Brown prep_with_Koehn_alignments amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn vmod_algorithm_described nn_algorithm_induction nn_algorithm_phrase det_algorithm_the dobj_use_algorithm nsubj_use_we advcl_use_derive auxpass_estimated_are nsubjpass_estimated_s nsubjpass_estimated_p prep_from_estimated_which num_s_| nn_s_t nn_s_p dobj_|_t nsubj_|_s conj_and_p_s dep_p_| appos_s_t nn_s_c nn_s_counts amod_s_joint det_s_the dep_derive_estimated dobj_derive_s aux_derive_To
W06-1607	N03-1017	o	This is the traditional approach for glass-box smoothing -LRB- Koehn et al. 2003 Zens and Ney 2004 -RRB-	dep_Zens_2004 conj_and_Zens_Ney dep_Koehn_Ney dep_Koehn_Zens appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_smoothing_glass-box dep_approach_Koehn prep_for_approach_smoothing amod_approach_traditional det_approach_the cop_approach_is nsubj_approach_This
W06-1608	N03-1017	o	In Englishto-German this result produces results very comparable to a phrasal SMT system -LRB- Koehn et al. 2003 -RRB- trained on the same data	amod_data_same det_data_the prep_on_trained_data amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_system_SMT amod_system_phrasal det_system_a prep_to_comparable_system advmod_comparable_very amod_results_comparable vmod_produces_trained dep_produces_Koehn dobj_produces_results nsubj_produces_result prep_in_produces_Englishto-German det_result_this
W06-1608	N03-1017	o	This dependency graph is partitioned into treelets like -LRB- Koehn et al. 2003 -RRB- we assume a uniform probability distribution over all partitions	det_partitions_all prep_over_distribution_partitions nn_distribution_probability amod_distribution_uniform det_distribution_a dobj_assume_distribution nsubj_assume_we prep_assume_like amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_like_Koehn parataxis_partitioned_assume prep_into_partitioned_treelets auxpass_partitioned_is nsubjpass_partitioned_graph nn_graph_dependency det_graph_This
W06-1608	N03-1017	o	It has been shown that phrasal machine translation systems are not affected by the quality of the input word alignments -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_alignments_word nn_alignments_input det_alignments_the prep_of_quality_alignments det_quality_the agent_affected_quality neg_affected_not auxpass_affected_are nsubjpass_affected_systems mark_affected_that nn_systems_translation nn_systems_machine amod_systems_phrasal dep_shown_Koehn ccomp_shown_affected auxpass_shown_been aux_shown_has nsubjpass_shown_It
W06-1609	N03-1017	p	1 Introduction During the last few years SMT systems have evolved from the original word-based approach -LRB- Brown et al. 1993 -RRB- to phrase-based translation systems -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_systems_translation amod_systems_phrase-based dep_al._1993 nn_al._et amod_al._Brown prep_to_approach_systems dep_approach_al. amod_approach_word-based amod_approach_original det_approach_the dep_evolved_Koehn prep_from_evolved_approach aux_evolved_have nsubj_evolved_systems ccomp_evolved_Introduction nn_systems_SMT amod_years_few amod_years_last det_years_the prep_during_Introduction_years num_Introduction_1
W06-3102	N03-1017	o	Table 2 The set of tags used to mark explicit morphemes in English Tag Meaning JJR Adjective comparative JJS Adjective superlative NNS Noun plural POS Possessive ending RBR Adverb comparative RBS Adverb superlative VB Verb base form VBD Verb past tense VBG Verb gerund or present participle VBN Verb past participle VBP Verb non3rd person singular present VBZ Verb 3rd person singular present Figure 2 Morpheme alignment between a Turkish and an English sentence 4 Experiments We proceeded with the following sequence of experiments -LRB- 1 -RRB- Baseline As a baseline system we used a pure word-based approach and used Pharaoh Training tool -LRB- 2004 -RRB- to train on the 22,500 sentences and decoded using Pharaoh -LRB- Koehn et al. 2003 -RRB- to obtain translations for a test set of 50 sentences	num_sentences_50 prep_of_set_sentences nn_set_test det_set_a prep_for_translations_set dobj_obtain_translations aux_obtain_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Pharaoh_Koehn xcomp_using_obtain dobj_using_Pharaoh xcomp_decoded_using num_sentences_22,500 det_sentences_the prep_on_train_sentences aux_train_to appos_tool_2004 amod_tool_Training nn_tool_Pharaoh dep_used_tool conj_and_approach_used amod_approach_word-based amod_approach_pure det_approach_a dobj_used_used dobj_used_approach nsubj_used_we nn_system_baseline det_system_a prep_as_Baseline_system dep_Baseline_1 prep_of_sequence_experiments amod_sequence_following det_sequence_the prep_with_proceeded_sequence nsubj_proceeded_We rcmod_Experiments_proceeded num_Experiments_4 dep_sentence_Experiments amod_sentence_English det_sentence_an conj_and_Turkish_sentence dep_a_sentence dep_a_Turkish prep_between_alignment_a nn_alignment_Morpheme num_Figure_2 amod_Figure_present amod_Figure_singular dep_person_Figure amod_person_3rd appos_Verb_person nn_Verb_VBZ amod_Verb_present amod_Verb_singular dep_person_Verb amod_person_non3rd nn_Verb_VBP nn_Verb_participle amod_Verb_past nn_Verb_VBN nn_Verb_participle amod_Verb_present nn_Verb_VBG amod_Verb_tense amod_Verb_past nn_Verb_VBD nn_Verb_form amod_Verb_base nn_Verb_VB amod_Verb_superlative nn_Adverb_RBS amod_Adverb_comparative appos_Adverb_person conj_or_Adverb_Verb conj_or_Adverb_Verb conj_or_Adverb_gerund conj_or_Adverb_Verb conj_or_Adverb_Verb conj_or_Adverb_Verb conj_or_Adverb_Adverb nn_Adverb_RBR dobj_ending_Verb dobj_ending_Verb dobj_ending_gerund dobj_ending_Verb dobj_ending_Verb dobj_ending_Verb dobj_ending_Adverb dobj_ending_Adverb dep_Possessive_ending amod_POS_Possessive dep_plural_POS nn_Noun_NNS amod_Noun_superlative nn_Adjective_JJS amod_Adjective_comparative appos_Adjective_plural conj_Adjective_Noun conj_Adjective_Adjective nn_Adjective_JJR nn_Adjective_Meaning nn_Adjective_Tag nn_Adjective_English amod_morphemes_explicit prep_in_mark_Adjective dobj_mark_morphemes aux_mark_to xcomp_used_mark vmod_tags_used conj_and_set_decoded vmod_set_train rcmod_set_used dep_set_Baseline dep_set_alignment prep_of_set_tags det_set_The dep_Table_decoded dep_Table_set num_Table_2
W06-3106	N03-1017	o	PP-model WecollectedthePPparametersbysimply reading the alignment matrices resulting from the word alignment in a way similar to the one described in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in vmod_one_described det_one_the prep_to_similar_one amod_way_similar det_way_a pobj_in_way ccomp_,_in nn_alignment_word det_alignment_the prep_from_resulting_alignment vmod_matrices_resulting nn_matrices_alignment det_matrices_the dobj_reading_matrices vmod_WecollectedthePPparametersbysimply_reading amod_WecollectedthePPparametersbysimply_PP-model advcl_``_WecollectedthePPparametersbysimply
W06-3106	N03-1017	o	This includes the standard notion of phrase popular with phrasedbased SMT -LRB- Koehn et al. 2003 Vogel et al. 2003 -RRB- aswellassequencesofwordsthatcontaingaps -LRB- possibly of arbitrary size -RRB-	amod_size_arbitrary prep_of_aswellassequencesofwordsthatcontaingaps_size advmod_aswellassequencesofwordsthatcontaingaps_possibly amod_aswellassequencesofwordsthatcontaingaps_popular num_Vogel_2003 nn_Vogel_al. nn_Vogel_et dep_Koehn_Vogel dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_SMT_phrasedbased dep_popular_Koehn prep_with_popular_SMT appos_notion_aswellassequencesofwordsthatcontaingaps prep_of_notion_phrase amod_notion_standard det_notion_the dobj_includes_notion nsubj_includes_This
W06-3106	N03-1017	n	It has the advantage of naturally capturing local reorderings and is shown to outperform word-based machine translation -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_machine amod_translation_word-based dobj_outperform_translation aux_outperform_to xcomp_shown_outperform auxpass_shown_is nsubjpass_shown_It amod_reorderings_local dobj_capturing_reorderings advmod_capturing_naturally prepc_of_advantage_capturing det_advantage_the dep_has_Koehn conj_and_has_shown dobj_has_advantage nsubj_has_It