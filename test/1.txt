2.1 Categories and Granularity of SentimentVarious researchers have addressed the problem of classifying sentiment into di↵erent categories. The simplest example is the work by Pang et al. (2002), who proposed methods to automatically classify the movie reviews into two categories: positive and negative. This categorisation is also known as sentiment polarity detection or simply polarity detection.While Pang et al. (2002) restricted themselves to only two categories, such a categorisation is not suitable for a review which does not carry a distinctly negative or positive attitude towards a movie. This issue can be solved by introducing a neutral category to label text that does not carry any polarity, expanding the classification scheme to three basic categories: positive, negative and neutral. I use this three-class scheme in my research because it is simple as well as su ciently general to cover most cases of expression of sentiment in scientific citations.Another line of research revolves around distinguishing between polar and neutral senti- ment. This task is often referred to as subjectivity detection, and a subjective sentiment can either be positive of negative. Text with a neutral sentiment is thus also called ob- jective text. An example of this task is the work by Wiebe et al. (1999), who used this subjective vs objective categorisation scheme to automatically classify newspaper text.As in the case of movie reviews, annotation is generally performed by assigning a la- bel from a list of labels to each instance of the training data. However, utilising any hierarchical relation which may be present amongst the labels may lead to improved performance. Pang and Lee (2004) provide an example of this work when they used a two-step hierarchical approach for sentiment classification of movie reviews. In the first step, the system labelled text as either objective or subjective, and in the second step, the extracted subjective text was used to detect polarity.Some categorisation schemes use finer divisions for assigning subjectivity or sentiment. For example, Wilson et al. (2004) used a four-class scheme for detecting the strength of sentiment in subjective clauses. Their proposed categories were neutral, low, medium, and high. More recently, Thelwall et al. (2011) used a scale of one to five for determining positive and negative sentiment in short informal text.Sentiment categories in text are closely linked to the granularity of the observed text. By granularity, I refer to the di↵erent units of length of text to be examined, which can range from a word or a phrase to a sentence or even a whole document. For instance, the word-level method presented by Hatzivassiloglou and McKeown (1997) retrieves positive or negative semantic orientation information only for adjectives. Wiebe and Mihalcea (2006) addressed the task of automatic assignment of subjectivity labels to word senses. For phrase-level categorisation, Turney (2002) worked on predicting the polarity of phrases containing adjectives and adverbs which were extracted from text using POS tag patterns. Wilson et al. (2009) proposed a system for automatically distinguishing between prior and contextual polarity of phrases. At a sentence level, Pang and Lee (2004) used classifiers to detect subjective sentences from movie reviews. The subjective sentences were then used to calculate the overall sentiment of a whole movie review document. Yu and Hatzivas- siloglou (2003) presented a solution for three subtasks of sentiment detection at di↵erent levels of granularity: detecting subjectivity of documents, detecting subjectivity of sen- tences, and assigning polarity to sentences. Keeping the methods described above for13
sentiment classification of citations in mind, labelling phrases would be too fine-grained to provide any useful information about the target citations. On the other hand, labelling the whole citing document does not capture much of the sentiment present in the paper because of the inherent objective nature of scientific writing. Therefore, I use a middle approach by choosing sentences as units of classification.A whole body of work exists on sentence based sentiment classification in social media, in particular, the micro-blogging platform Twitter 1. Twitter users post sentences, also called tweets, which consist of at most 140 characters. Such short sentences are mostly about a particular subject or event and applications of detecting sentiment from these sentences range from predicting elections (Tumasjan et al., 2010) results to forecasting stock market trends (Bollen et al., 2011). For this level of granularity, certain features become very important in predicting the sentiment, for example, Read (2005) and Go et al. (2009) explore the use of emoticons for sentiment classification of tweets. Simi- larly, Brody and Diakopoulos (2011) leverage word lengthening of polar words (such as cooooooooooooooollllllllllllll!!) to build a Twitter-specific sentiment classifier.Granularity becomes particularly important in the case of product reviews, where di↵erent features of a product have to be examined. In such cases, sentiment is determined with respect to the target objects. Product features can be thought of as individual target objects towards which the sentiment is to be detected. For example, a camera with an overall positive review may have low battery life. This task has been tackled by many researchers with varying degrees of success (Hu and Liu, 2004; Liu et al., 2005; Wu et al., 2009; Dasgupta and Ng, 2009). This can be mapped to my research problem, where a single sentence can contain citations to many di↵erent papers. In such cases, the sentiment toward only the target paper is to be determined while the rest of the sentiment, however strongly expressed, needs to be ignored.In the next section, I describe various supervised learning methods for the prediction of sentiment, i.e. those methods which use data that has been labelled with target categories, in order to predict sentiment.2.2 Supervised MethodsThe increasing availability of labelled data has played an important role in the application of supervised machine learning methods to sentiment analysis. These methods represent the labelled data in the form of a set of features. The features are then used to learn a function for classification of unseen data. In this dissertation, I approach the problem of sentiment analysis as a classification task.The foremost example of an approach utilising labelled data is the work by Pang et al. (2002). The authors used Naive Bayes (NB), Maximum Entropy (MaxEnt) and Support Vector Machines (SVMs) classifiers (Berger et al., 1996; Joachims, 1998) for determining if a movie review is positive or negative. The IMDB archive of rec.arts.movies.reviews newsgroup2 was used as the source of data, and only those reviews were selected which1http://twitter.com 2http://www.imdb.com/reviews/ 14
had a rating expressed as a number or as stars. These ratings were used as an automatic indication of the sentiment associated with the review text, and thus converted to two labels: positive and negative. For obtaining a balanced corpus, 700 documents of each label were selected. N-grams, part-of-speech (POS) tags and their combinations were used as features, and three-fold cross validation was used. Their best system achieved an accuracy of 82.9% when using the unigram presence feature set with SVMs. It should be noted that the corpus used in this work was balanced artificially, thus avoiding the problem of data sparsity for under-represented classes. However, it is possible to learn more from real-life balance between classes, which is what I do in my research.Supervised learning can also be performed using multiple classifiers, particularly if the labelling scheme allows for hierarchical relations. As described earlier, one example of this is the work by Pang and Lee (2004). They represented sentences in the given document as graph nodes and calculated the minimal cut on that graph to identify the subjec- tive sentences. Afterwards, standard machine learning classification algorithms (NB and SVMs) were applied only on the extracted subjective sentences to predict their polarity. On a balanced polarity corpus of 2,000 reviews, the minimum-cut framework resulted in an accuracy of 86.4%, which represents a statistically significant improvement in polarity classification accuracy over previous attempts.Another example of supervised sentiment classification at a finer granularity is the work by Wilson et al. (2009), who proposed a system for automatically distinguishing between prior and contextual polarity at the sentence level. The key idea is that the polarity of words can change on the basis of other words present in their context. For instance, in the phrase National Environment Trust, the word trust has positive priors but is used here in a neutral context. Starting with a collection of clues marked with prior polarity, the so-called contextual polarity in the corpus was identified as positive, negative, neutral or both. A two-step approach was taken by first classifying each phrase containing a clue as either neutral or polar. In the second step, only the phrases marked as polar were considered further. A corpus of 8,984 sentences was constructed from 425 documents. Unlike movie reviews, these documents did not contain any information for the automatic extraction of labels. Therefore, this corpus was annotated manually by examining each sentence. As a result, 15,991 subjective expressions were found in the 8,984 sentences. 66 documents (1,373 sentences/2,808 expressions) were separated as the development test and the remaining data was used as the test set. For the task of neutral-polar classification, 28 di↵erent features based on word context, dependency parse tree, structural relationship, sentence subjectivity and document topic we used in a machine learning framework. For the task of polarity classification, ten features based on word tokens, word prior polarity, as well as presence of negation, modification relationships and polarity shifters were used. The BoosTexter AdaBoost.HM (Schapire and Singer, 2000) achieved 75.9% accuracy on the former tasks and 65.7% on the latter.When the units of classification, whether words, phrases or sentences, are present in a sequence, identification of sentiment can also be viewed as a tagging task. Breck et al. (2007) used a simple tagging scheme which tagged each term in a sentence as either being ‘in’ a polar expression (/I), or ‘out’ of it (/O). They used various lexical, syntactic and dictionary-based features to identify both /I and /O types of subjective expressions. The MPQA data corpus of 535 newswire articles was used, 135 of which were put aside for