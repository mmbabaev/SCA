W09-1606	P02-1053	o	Turney -LRB- 2002 -RRB- suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon	nn_lexicon_sentiment det_lexicon_the agent_predetermined_lexicon vmod_words_predetermined nn_co-occurrences_phrase prep_of_frequency_co-occurrences det_frequency_the prep_with_comparing_words dobj_comparing_frequency xcomp_suggested_comparing nsubj_suggested_Turney appos_Turney_2002
W09-1703	P02-1053	o	Our work builds upon Turneys work on semantic orientation -LRB- Turney 2002 -RRB- and synonym learning -LRB- Turney 2001 -RRB- in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries	nn_queries_Web prep_on_based_queries conj_and_words_phrases vmod_similarity_based prep_of_similarity_phrases prep_of_similarity_words det_similarity_the dobj_measure_similarity aux_measure_to nn_algorithm_PMI-IR det_algorithm_a vmod_used_measure dobj_used_algorithm nsubj_used_he prep_in_used_which appos_Turney_2001 dep_learning_Turney nn_learning_synonym dep_Turney_2002 conj_and_orientation_learning dep_orientation_Turney amod_orientation_semantic parataxis_work_used prep_on_work_learning prep_on_work_orientation nsubj_work_Turneys mark_work_upon advcl_builds_work nsubj_builds_work poss_work_Our ccomp_``_builds
W09-1703	P02-1053	o	Turney -LRB- Turney 2001 Turney 2002 -RRB- reported that the NEAR operator outperformed simple page co-occurrence for his purposes our early experiments informally showed the same for this work	det_work_this prep_for_same_work det_same_the dobj_showed_same advmod_showed_informally nsubj_showed_experiments amod_experiments_early poss_experiments_our poss_purposes_his nn_co-occurrence_page amod_co-occurrence_simple parataxis_outperformed_showed prep_for_outperformed_purposes dobj_outperformed_co-occurrence nsubj_outperformed_operator mark_outperformed_that nn_operator_NEAR det_operator_the ccomp_reported_outperformed nsubj_reported_Turney amod_Turney_2002 conj_Turney_Turney conj_Turney_2001 nn_Turney_Turney
W09-1904	P02-1053	o	Previous research has focused on classifying subjective-versus-objective expressions -LRB- Wiebe et al. 2004 -RRB- and also on accurate sentiment polarity assignment -LRB- Turney 2002 Yi et al. 2003 Pang and Lee 2004 Sindhwani and Melville 2008 Melville et al. 2009 -RRB-	num_Melville_2009 nn_Melville_al. nn_Melville_et dep_Sindhwani_Melville conj_and_Sindhwani_2008 conj_and_Sindhwani_Melville num_Pang_2004 conj_and_Pang_Lee dep_Yi_2008 dep_Yi_Melville dep_Yi_Sindhwani conj_Yi_Lee conj_Yi_Pang num_Yi_2003 nn_Yi_al. nn_Yi_et dep_Turney_Yi appos_Turney_2002 dep_assignment_Turney nn_assignment_polarity nn_assignment_sentiment amod_assignment_accurate pobj_on_assignment advmod_on_also amod_Wiebe_2004 dep_Wiebe_al. nn_Wiebe_et appos_expressions_Wiebe amod_expressions_subjective-versus-objective amod_expressions_classifying conj_and_focused_on prep_on_focused_expressions aux_focused_has nsubj_focused_research amod_research_Previous
C08-1018	P02-1057	o	Finally we plan to apply the model to other paraphrasing tasks including fully abstractive document summarisation -LRB- Daume III and Marcu 2002 -RRB-	dep_Daume_2002 conj_and_Daume_Marcu num_Daume_III dep_summarisation_Marcu dep_summarisation_Daume nn_summarisation_document amod_summarisation_abstractive advmod_abstractive_fully prep_including_tasks_summarisation amod_tasks_paraphrasing amod_tasks_other det_model_the prep_to_apply_tasks dobj_apply_model aux_apply_to xcomp_plan_apply nsubj_plan_we advmod_plan_Finally
D08-1057	P02-1057	o	For content selection discourse-level considerations were proposed by Daume III and Marcu -LRB- 2002 -RRB- who explored the use of Rhetorical Structure Theory -LRB- Mann and Thompson 1988 -RRB-	dep_Mann_1988 conj_and_Mann_Thompson appos_Theory_Thompson appos_Theory_Mann nn_Theory_Structure amod_Theory_Rhetorical prep_of_use_Theory det_use_the dobj_explored_use nsubj_explored_who appos_Marcu_2002 rcmod_III_explored conj_and_III_Marcu nn_III_Daume agent_proposed_Marcu agent_proposed_III auxpass_proposed_were nsubjpass_proposed_considerations prep_for_proposed_selection nn_considerations_discourse-level amod_selection_content rcmod_``_proposed
J05-4004	P02-1057	o	Between these two extremes there has been a relatively modest amount of work in sentence simplification -LRB- Chandrasekar Doran and Bangalore 1996 Mahesh 1997 Carroll et al. 1998 Grefenstette 1998 Jing 2000 Knight and Marcu 2002 -RRB- and document compression -LRB- Daume III and Marcu 2002 Daume III and Marcu 2004 Zajic Dorr and Schwartz 2004 -RRB- in which words phrases and sentences are selected in an extraction process	nn_process_extraction det_process_an prep_in_selected_process auxpass_selected_are nsubjpass_selected_sentences nsubjpass_selected_phrases nsubjpass_selected_words prep_in_selected_which conj_and_words_sentences conj_and_words_phrases num_Schwartz_2004 rcmod_Zajic_selected conj_and_Zajic_Schwartz conj_and_Zajic_Dorr num_Marcu_2004 conj_and_III_Marcu nn_III_Daume num_Marcu_2002 dep_Daume_Schwartz dep_Daume_Dorr dep_Daume_Zajic conj_and_Daume_Marcu conj_and_Daume_III conj_and_Daume_Marcu num_Daume_III dep_compression_III dep_compression_Marcu dep_compression_Daume nn_compression_document num_Knight_2002 conj_and_Knight_Marcu dobj_Jing_2000 num_Grefenstette_1998 num_al._1998 dep_Carroll_al. nn_Carroll_et num_Mahesh_1997 num_Bangalore_1996 dep_Chandrasekar_Marcu dep_Chandrasekar_Knight dep_Chandrasekar_Jing dep_Chandrasekar_Grefenstette dep_Chandrasekar_Carroll conj_and_Chandrasekar_Mahesh conj_and_Chandrasekar_Bangalore conj_and_Chandrasekar_Doran nn_simplification_sentence conj_and_amount_compression appos_amount_Mahesh appos_amount_Bangalore appos_amount_Doran appos_amount_Chandrasekar prep_in_amount_simplification prep_of_amount_work amod_amount_modest det_amount_a cop_amount_been aux_amount_has expl_amount_there prep_between_amount_extremes advmod_modest_relatively num_extremes_two det_extremes_these
J05-4004	P02-1057	o	In our own work on document compression models -LRB- Daume III and Marcu 2002 Daume III and Marcu 2004 -RRB- both of which extend the sentence compression model of Knight and Marcu -LRB- 2002 -RRB- we assume that sentences and documents can be summarized exclusively through deletion of contiguous text segments	nn_segments_text amod_segments_contiguous prep_of_deletion_segments prep_through_summarized_deletion advmod_summarized_exclusively auxpass_summarized_be aux_summarized_can nsubjpass_summarized_documents nsubjpass_summarized_sentences mark_summarized_that conj_and_sentences_documents ccomp_assume_summarized nsubj_assume_we appos_Marcu_2002 conj_and_Knight_Marcu prep_of_model_Marcu prep_of_model_Knight nn_model_compression nn_model_sentence det_model_the parataxis_extend_assume dobj_extend_model nsubj_extend_both prep_in_extend_work prep_of_both_which num_Marcu_2004 conj_and_III_Marcu nn_III_Daume num_Marcu_2002 dep_III_Marcu dep_III_III conj_and_III_Marcu nn_III_Daume appos_models_Marcu appos_models_III nn_models_compression nn_models_document prep_on_work_models amod_work_own poss_work_our
W04-1016	P02-1057	o	It has been further observed that simply compressing sentences individually and concatenating the results leads to suboptimal summaries -LRB- Daume III and Marcu 2002 -RRB-	dep_III_2002 conj_and_III_Marcu nn_III_Daume appos_summaries_Marcu appos_summaries_III amod_summaries_suboptimal prep_to_leads_summaries csubj_leads_concatenating csubj_leads_compressing mark_leads_that det_results_the dobj_concatenating_results conj_and_compressing_concatenating advmod_compressing_individually dobj_compressing_sentences advmod_compressing_simply ccomp_observed_leads advmod_observed_further auxpass_observed_been aux_observed_has nsubjpass_observed_It
W04-1016	P02-1057	o	The third baseline COMP is the document compression system developed by Daume III and Marcu -LRB- 2002 -RRB- which compresses documents by cutting out constituents in a combined syntax and discourse tree	nn_tree_discourse conj_and_syntax_tree amod_syntax_combined det_syntax_a prep_in_cutting_tree prep_in_cutting_syntax dobj_cutting_constituents prt_cutting_out prepc_by_compresses_cutting dobj_compresses_documents nsubj_compresses_which appos_Marcu_2002 rcmod_III_compresses conj_and_III_Marcu nn_III_Daume agent_developed_Marcu agent_developed_III vmod_system_developed nn_system_compression nn_system_document det_system_the cop_system_is nsubj_system_COMP dep_baseline_system amod_baseline_third det_baseline_The dep_``_baseline
W04-1016	P02-1057	o	A few researchers have focused on other aspects of summarization including single sentence -LRB- Knight and Marcu 2002 -RRB- paragraph or short document -LRB- Daume III and Marcu 2002 -RRB- query-focused -LRB- Berger and Mittal 2000 -RRB- or speech -LRB- Hori et al. 2003 -RRB-	amod_Hori_2003 dep_Hori_al. nn_Hori_et dep_Berger_2000 conj_and_Berger_Mittal dep_query-focused_Hori conj_or_query-focused_speech dep_query-focused_Mittal dep_query-focused_Berger dep_Daume_2002 conj_and_Daume_Marcu num_Daume_III amod_document_short dep_paragraph_Marcu dep_paragraph_Daume conj_or_paragraph_document dep_Knight_2002 conj_and_Knight_Marcu dep_sentence_speech dep_sentence_query-focused appos_sentence_document appos_sentence_paragraph appos_sentence_Marcu appos_sentence_Knight amod_sentence_single prep_including_aspects_sentence prep_of_aspects_summarization amod_aspects_other prep_on_focused_aspects aux_focused_have nsubj_focused_researchers amod_researchers_few det_researchers_A ccomp_``_focused
C04-1135	P03-1001	o	1 Introduction Hyponymy relations can play a crucial role in various NLP systems and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora -LRB- Hearst 1992 Caraballo 1999 Imasumi 2001 Fleischman et al. 2003 Morin and Jacquemin 2003 Ando et al. 2003 -RRB-	num_Ando_2003 nn_Ando_al. nn_Ando_et dep_Morin_Ando conj_and_Morin_2003 conj_and_Morin_Jacquemin num_Fleischman_2003 nn_Fleischman_al. nn_Fleischman_et conj_Imasumi_2003 conj_Imasumi_Jacquemin conj_Imasumi_Morin conj_Imasumi_Fleischman num_Imasumi_2001 dep_Caraballo_Imasumi appos_Caraballo_1999 dep_Hearst_Caraballo appos_Hearst_1992 dep_corpora_Hearst nn_corpora_text prep_from_relations_corpora amod_relations_hyponymy dobj_acquire_relations aux_acquire_to amod_methods_automatic xcomp_develop_acquire dobj_develop_methods aux_develop_to vmod_attempts_develop amod_attempts_many cop_attempts_been aux_attempts_have expl_attempts_there nn_systems_NLP amod_systems_various amod_role_crucial det_role_a conj_and_play_attempts prep_in_play_systems dobj_play_role aux_play_can nsubj_play_relations nn_relations_Hyponymy nn_relations_Introduction num_relations_1
C04-1188	P03-1001	o	The row labelled Precision shows the precision of the extracted information -LRB- i.e. how many entries are correct according to a human annotator -RRB- estimated by random sampling and manual evaluation of 1 % of the data for each table similar to -LRB- Fleischman et al. 2003 -RRB-	amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et dep_to_Fleischman prep_similar_to det_table_each prep_for_data_table det_data_the prep_of_%_data num_%_1 amod_evaluation_manual amod_sampling_similar prep_of_sampling_% conj_and_sampling_evaluation amod_sampling_random agent_estimated_evaluation agent_estimated_sampling amod_annotator_human det_annotator_a pobj_correct_annotator prepc_according_to_correct_to cop_correct_are nsubj_correct_entries advmod_correct_i.e. amod_entries_many advmod_many_how dep_information_correct amod_information_extracted det_information_the vmod_precision_estimated prep_of_precision_information det_precision_the dobj_shows_precision nsubj_shows_Precision amod_Precision_labelled dep_Precision_row det_row_The
C04-1188	P03-1001	o	-LRB- Fleischman et al. 2003 Jijkoun et al. 2003 -RRB-	num_Jijkoun_2003 nn_Jijkoun_al. nn_Jijkoun_et dep_Fleischman_Jijkoun appos_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et dep_''_Fleischman
C04-1188	P03-1001	p	In our future work we plan to investigate the effect of more sophisticated and probably more accurate filtering methods -LRB- Fleischman et al. 2003 -RRB- on the QA results	det_QA_the appos_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et amod_methods_filtering amod_methods_accurate amod_methods_more dep_sophisticated_Fleischman conj_and_sophisticated_methods conj_and_sophisticated_probably advmod_sophisticated_more prep_on_effect_QA prep_of_effect_methods prep_of_effect_probably prep_of_effect_sophisticated det_effect_the dobj_investigate_effect aux_investigate_to dep_plan_results xcomp_plan_investigate nsubj_plan_we prep_in_plan_work amod_work_future poss_work_our
C04-1188	P03-1001	o	The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise -LRB- Fleischman et al. 2003 -RRB-	amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et nn_methods_learning nn_methods_machine prep_out_using_noise prep_to_using_filter dobj_using_methods dep_improved_Fleischman agent_improved_using advmod_improved_significantly auxpass_improved_be aux_improved_can nsubjpass_improved_precision amod_information_extracted det_information_the prep_of_precision_information det_precision_The
C04-1188	P03-1001	o	The recall problem is usually addressed by increasing the amount of text data for extraction -LRB- taking larger collections -LRB- Fleischman et al. 2003 -RRB- -RRB- or by developing more surface patterns -LRB- Soubbotin and Soubbotin 2002 -RRB-	amod_Soubbotin_2002 conj_and_Soubbotin_Soubbotin dep_patterns_Soubbotin dep_patterns_Soubbotin nn_patterns_surface amod_patterns_more dobj_developing_patterns pcomp_by_developing amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et amod_collections_larger conj_or_taking_by dep_taking_Fleischman dobj_taking_collections prep_for_data_extraction nn_data_text prep_of_amount_data det_amount_the dep_increasing_by dep_increasing_taking dobj_increasing_amount agent_addressed_increasing advmod_addressed_usually auxpass_addressed_is nsubjpass_addressed_problem nn_problem_recall det_problem_The
C04-1188	P03-1001	o	The usual recall and precision metrics -LRB- e.g. how many of the interesting bits of information were detected and how many of the found bits were actually correct -RRB- require either a test corpus previously annotated with the required information or manual evaluation -LRB- Fleischman et al. 2003 -RRB-	amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et dep_evaluation_Fleischman amod_evaluation_manual conj_or_information_evaluation amod_information_required det_information_the prep_with_annotated_evaluation prep_with_annotated_information advmod_annotated_previously npadvmod_annotated_corpus preconj_annotated_either nn_corpus_test det_corpus_a acomp_require_annotated dep_require_correct nsubj_require_many advmod_correct_actually cop_correct_were nsubj_correct_many amod_bits_found det_bits_the prep_of_many_bits advmod_many_how conj_and_detected_require auxpass_detected_were nsubjpass_detected_many prep_of_bits_information amod_bits_interesting det_bits_the prep_of_many_bits advmod_many_how dep_e.g._require dep_e.g._detected nn_metrics_precision dep_recall_e.g. conj_and_recall_metrics amod_recall_usual det_recall_The dep_``_metrics dep_``_recall
C04-1188	P03-1001	o	3.2 Questions and Corpus To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases similarly to -LRB- Fleischman et al. 2003 -RRB- we focused only on questions about persons taken from the TREC8 through TREC 2003 question sets	nn_sets_question num_sets_2003 nn_sets_TREC det_TREC8_the prep_through_taken_sets prep_from_taken_TREC8 vmod_questions_taken prep_about_questions_persons prep_on_focused_questions advmod_focused_only nsubj_focused_we nsubj_focused_Corpus nsubj_focused_Questions amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et dep_to_Fleischman advmod_to_similarly nn_bases_knowledge prep_of_construction_bases amod_construction_offline det_construction_the prep_for_methods_construction nn_methods_extraction nn_methods_information amod_methods_different dobj_using_methods prepc_of_impact_using det_impact_the prep_of_picture_impact amod_picture_clear det_picture_a dobj_get_picture aux_get_To prep_Questions_to vmod_Questions_get conj_and_Questions_Corpus num_Questions_3.2
H05-1013	P03-1001	o	In particular we use the name/instance lists described by -LRB- Fleischman et al. 2003 -RRB- and available on Fleischmans web page to generate features between names and nominals -LRB- this list contains a110a111a85 pairs mined from a112a73a96 GBs of news data -RRB-	nn_data_news prep_of_GBs_data num_GBs_a112a73a96 prep_from_mined_GBs vmod_pairs_mined nn_pairs_a110a111a85 dobj_contains_pairs nsubj_contains_list det_list_this conj_and_names_nominals prep_between_features_nominals prep_between_features_names parataxis_generate_contains dobj_generate_features aux_generate_to nn_page_web nn_page_Fleischmans xcomp_available_generate prep_on_available_page conj_and_Fleischman_available amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et prep_by_described_available prep_by_described_Fleischman dep_lists_described dep_name/instance_lists det_name/instance_the dobj_use_name/instance nsubj_use_we prep_in_use_particular
H05-1075	P03-1001	o	2 Related Work Question Answering has attracted much attention from the areas of Natural Language Processing Information Retrieval and Data Mining -LRB- Fleischman et al. 2003 Echihabi et al. 2003 Yang et al. 2003 Hermjakob et al. 2002 Dumais et al. 2002 Hermjakob et al. 2000 -RRB-	num_Hermjakob_2000 nn_Hermjakob_al. nn_Hermjakob_et num_Dumais_2002 nn_Dumais_al. nn_Dumais_et num_Hermjakob_2002 nn_Hermjakob_al. nn_Hermjakob_et num_Yang_2003 nn_Yang_al. nn_Yang_et dep_Echihabi_Hermjakob conj_Echihabi_Dumais conj_Echihabi_Hermjakob conj_Echihabi_Yang num_Echihabi_2003 nn_Echihabi_al. nn_Echihabi_et dep_Fleischman_Echihabi appos_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et nn_Mining_Data dep_Retrieval_Fleischman conj_and_Retrieval_Mining nn_Retrieval_Information nn_Processing_Language amod_Processing_Natural prep_of_areas_Processing det_areas_the amod_attention_much prep_from_attracted_areas dobj_attracted_attention aux_attracted_has nsubj_attracted_Answering nn_Answering_Question dep_Work_Mining dep_Work_Retrieval rcmod_Work_attracted amod_Work_Related num_Work_2
H05-1075	P03-1001	o	1 Motivation Question Answering has emerged as a key area in natural language processing -LRB- NLP -RRB- to apply question parsing information extraction summarization and language generation techniques -LRB- Clark et al. 2004 Fleischman et al. 2003 Echihabi et al. 2003 Yang et al. 2003 Hermjakob et al. 2002 Dumais et al. 2002 -RRB-	num_Dumais_2002 nn_Dumais_al. nn_Dumais_et num_Hermjakob_2002 nn_Hermjakob_al. nn_Hermjakob_et num_Yang_2003 nn_Yang_al. nn_Yang_et num_Echihabi_2003 nn_Echihabi_al. nn_Echihabi_et num_Fleischman_2003 nn_Fleischman_al. nn_Fleischman_et dep_Clark_Dumais dep_Clark_Hermjakob dep_Clark_Yang dep_Clark_Echihabi dep_Clark_Fleischman amod_Clark_2004 dep_Clark_al. nn_Clark_et nn_techniques_generation nn_techniques_language nn_extraction_information conj_and_parsing_techniques conj_and_parsing_summarization conj_and_parsing_extraction nn_parsing_question dobj_apply_techniques dobj_apply_summarization dobj_apply_extraction dobj_apply_parsing aux_apply_to appos_processing_NLP nn_processing_language amod_processing_natural prep_in_area_processing amod_area_key det_area_a dep_emerged_Clark xcomp_emerged_apply prep_as_emerged_area aux_emerged_has nsubj_emerged_Answering nn_Answering_Question nn_Answering_Motivation num_Answering_1 ccomp_``_emerged
I08-2126	P03-1001	o	1 Introduction The goal of this study has been to automatically extract a large set of hyponymy relations which play a critical role in many NLP applications such as Q&A systems -LRB- Fleischman et al. 2003 -RRB-	amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et dep_systems_Fleischman nn_systems_Q&A prep_such_as_applications_systems nn_applications_NLP amod_applications_many amod_role_critical det_role_a prep_in_play_applications dobj_play_role nsubj_play_which rcmod_relations_play amod_relations_hyponymy prep_of_set_relations amod_set_large det_set_a dobj_extract_set advmod_extract_automatically aux_extract_to xcomp_been_extract aux_been_has nsubj_been_goal det_study_this prep_of_goal_study det_goal_The rcmod_Introduction_been num_Introduction_1
N04-1010	P03-1001	o	We do not use particular lexicosyntactic patterns as previous attempts have -LRB- Hearst 1992 Caraballo 1999 Imasumi 2001 Fleischman et al. 2003 Morin and Jacquemin 2003 Ando et al. 2003 -RRB-	num_Ando_2003 nn_Ando_al. nn_Ando_et conj_and_Morin_2003 conj_and_Morin_Jacquemin num_Fleischman_2003 nn_Fleischman_al. nn_Fleischman_et num_Imasumi_2001 appos_Caraballo_1999 dep_Hearst_Ando dep_Hearst_2003 dep_Hearst_Jacquemin dep_Hearst_Morin dep_Hearst_Fleischman dep_Hearst_Imasumi dep_Hearst_Caraballo amod_Hearst_1992 dep_have_Hearst rcmod_attempts_have amod_attempts_previous nn_patterns_lexicosyntactic amod_patterns_particular prep_as_use_attempts dobj_use_patterns neg_use_not aux_use_do nsubj_use_We
N04-1041	P03-1001	o	We compared our system with the concepts in WordNet and Fleischman et al. s instance/concept relations -LRB- Fleischman et al. 2003 -RRB-	dep_2003_al. nn_al._et num_Fleischman_2003 amod_relations_instance/concept nn_relations_s nn_s_al. nn_s_et dep_WordNet_relations conj_and_WordNet_Fleischman prep_in_concepts_Fleischman prep_in_concepts_WordNet det_concepts_the appos_system_Fleischman prep_with_system_concepts poss_system_our dobj_compared_system nsubj_compared_We
N04-1041	P03-1001	o	2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text -LRB- Hearst 1992 Riloff and Shepherd 1997 Riloff and Jones 1999 Berland and Charniak 1999 Pantel and Lin 2002 Fleischman et al. 2003 Girju et al. 2003 -RRB-	dep_al._2003 nn_al._et nn_al._Girju dep_al._2003 nn_al._et nn_al._Fleischman num_Lin_2002 conj_and_Pantel_Lin num_Charniak_1999 conj_and_Berland_Charniak num_Jones_1999 conj_and_Riloff_Jones num_Shepherd_1997 conj_and_Riloff_Shepherd dep_Hearst_al. dep_Hearst_al. dep_Hearst_Lin dep_Hearst_Pantel dep_Hearst_Charniak dep_Hearst_Berland dep_Hearst_Jones dep_Hearst_Riloff dep_Hearst_Shepherd dep_Hearst_Riloff num_Hearst_1992 dep_text_Hearst prep_from_information_text amod_information_lexico-semantic dobj_discovering_information advmod_discovering_automatically prepc_to_approaches_discovering amod_approaches_several cop_approaches_been aux_approaches_have expl_approaches_There dep_Work_approaches amod_Work_Previous num_Work_2
P06-1102	P03-1001	p	In contrast the idea of bootstrapping for relation and information extraction was first proposed in -LRB- Riloff and Jones 1999 -RRB- and successfully applied to the construction of semantic lexicons -LRB- Thelen and Riloff 2002 -RRB- named entity recognition -LRB- Collins and Singer 1999 -RRB- extraction of binary relations -LRB- Agichtein and Gravano 2000 -RRB- and acquisition of structured data for tasks such as Question Answering -LRB- Lita and Carbonell 2004 Fleischman et al. 2003 -RRB-	num_Fleischman_2003 nn_Fleischman_al. nn_Fleischman_et dep_Lita_Fleischman dep_Lita_2004 conj_and_Lita_Carbonell appos_Answering_Carbonell appos_Answering_Lita nn_Answering_Question prep_such_as_tasks_Answering prep_for_data_tasks amod_data_structured prep_of_acquisition_data dep_Agichtein_2000 conj_and_Agichtein_Gravano dep_relations_Gravano dep_relations_Agichtein amod_relations_binary prep_of_extraction_relations amod_Collins_1999 conj_and_Collins_Singer dep_recognition_Singer dep_recognition_Collins nn_recognition_entity dep_named_recognition dep_Thelen_2002 conj_and_Thelen_Riloff appos_lexicons_extraction vmod_lexicons_named appos_lexicons_Riloff appos_lexicons_Thelen amod_lexicons_semantic prep_of_construction_lexicons det_construction_the prep_to_applied_construction advmod_applied_successfully dep_Riloff_1999 conj_and_Riloff_Jones dep_in_Jones dep_in_Riloff conj_and_proposed_applied prep_proposed_in conj_and_first_acquisition dep_first_applied dep_first_proposed nsubj_was_acquisition nsubj_was_first nn_extraction_information conj_and_relation_extraction prep_for_bootstrapping_extraction prep_for_bootstrapping_relation dep_idea_was prepc_of_idea_bootstrapping det_idea_the dep_,_idea pobj_In_contrast dep_``_In
W04-2709	P03-1001	o	After that several million instances of people locations and other facts were added -LRB- Fleischman et al. 2003 -RRB-	amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et dep_added_Fleischman auxpass_added_were nsubjpass_added_instances prep_after_added_that amod_facts_other conj_and_people_facts conj_and_people_locations prep_of_instances_facts prep_of_instances_locations prep_of_instances_people num_instances_million quantmod_million_several
C04-1030	P03-1021	o	Alternatively one can train them with respect to the final translation quality measured by some error criterion -LRB- Och 2003 -RRB-	amod_Och_2003 appos_criterion_Och nn_criterion_error det_criterion_some agent_measured_criterion vmod_quality_measured nn_quality_translation amod_quality_final det_quality_the prep_with_respect_to_train_quality dobj_train_them aux_train_can nsubj_train_one advmod_train_Alternatively
C04-1072	P03-1021	o	To simulate real world scenario we use n-best lists from ISIs state-of-the-art statistical machine translation system AlTemp -LRB- Och 2003 -RRB- and the 2002 NIST Chinese-English evaluation corpus as the test corpus	nn_corpus_test det_corpus_the prep_as_corpus_corpus nn_corpus_evaluation amod_corpus_Chinese-English nn_corpus_NIST num_corpus_2002 det_corpus_the num_Och_2003 appos_AlTemp_Och conj_and_system_corpus conj_and_system_AlTemp nn_system_translation nn_system_machine amod_system_statistical amod_system_state-of-the-art nn_system_ISIs prep_from_lists_corpus prep_from_lists_AlTemp prep_from_lists_system amod_lists_n-best dobj_use_lists nsubj_use_we advcl_use_simulate nn_scenario_world amod_scenario_real dobj_simulate_scenario aux_simulate_To
C04-1072	P03-1021	o	For example a statistical machine translation system such as ISIs AlTemp SMT system -LRB- Och 2003 -RRB- can generate a list of n-best alternative translations given a source sentence	nn_sentence_source det_sentence_a pobj_given_sentence prep_translations_given amod_translations_alternative amod_translations_n-best prep_of_list_translations det_list_a dobj_generate_list aux_generate_can nsubj_generate_system prep_for_generate_example num_Och_2003 appos_system_Och nn_system_SMT nn_system_AlTemp nn_system_ISIs prep_such_as_system_system nn_system_translation nn_system_machine amod_system_statistical det_system_a
C04-1072	P03-1021	o	A natural fit to the existing statistical machine translation framework A metric that ranks a good translation high in an nbest list could be easily integrated in a minimal error rate statistical machine translation training framework -LRB- Och 2003 -RRB-	num_Och_2003 appos_framework_Och nn_framework_training nn_framework_translation nn_framework_machine amod_framework_statistical nn_framework_rate nn_framework_error amod_framework_minimal det_framework_a prep_in_integrated_framework advmod_integrated_easily auxpass_integrated_be aux_integrated_could nsubjpass_integrated_A amod_list_nbest det_list_an prep_in_high_list amod_translation_high amod_translation_good det_translation_a dobj_ranks_translation nsubj_ranks_that rcmod_A_ranks amod_A_metric rcmod_framework_integrated dep_translation_framework dep_machine_translation dobj_statistical_machine dep_existing_statistical vmod_the_existing prep_to_fit_the amod_fit_natural det_fit_A
C04-1168	P03-1021	o	The training of IBM model 4 was implemented by the GIZA + + package -LRB- Och and Ney 2003 -RRB-	nn_Och_package preconj_Och_+ dep_GIZA_2003 conj_and_GIZA_Ney conj_+_GIZA_Och det_GIZA_the agent_implemented_Ney agent_implemented_Och agent_implemented_GIZA auxpass_implemented_was nsubjpass_implemented_training num_model_4 nn_model_IBM prep_of_training_model det_training_The
C04-1168	P03-1021	o	We adopted an N-best hypothesis approach -LRB- Och 2003 -RRB- to train	aux_train_to amod_Och_2003 vmod_approach_train dep_approach_Och nn_approach_hypothesis amod_approach_N-best det_approach_an dobj_adopted_approach nsubj_adopted_We ccomp_``_adopted
C04-1168	P03-1021	o	Indeed the proposed speech translation paradigm of log-linear models have been shown e ective in many applications -LRB- Beyerlein 1998 -RRB- -LRB- Vergyri 2000 -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Vergyri_2000 amod_Beyerlein_1998 dep_applications_Och appos_applications_Vergyri dep_applications_Beyerlein amod_applications_many prep_in_ective_applications dep_ective_e dep_shown_ective auxpass_shown_been aux_shown_have nsubjpass_shown_paradigm advmod_shown_Indeed amod_models_log-linear prep_of_paradigm_models nn_paradigm_translation nn_paradigm_speech amod_paradigm_proposed det_paradigm_the
C04-1168	P03-1021	o	The Powells algorithm used in this work is similar as the one from -LRB- Press et al. 2000 -RRB- but we modi ed the line optimization codes a subroutine of Powells algorithm with reference to -LRB- Och 2003 -RRB-	amod_Och_2003 dep_to_Och prep_reference_to nn_algorithm_Powells prep_of_subroutine_algorithm det_subroutine_a appos_codes_subroutine nn_codes_optimization nn_codes_line det_codes_the prep_with_ed_reference dobj_ed_codes ccomp_modi_ed nsubj_modi_we amod_Press_2000 dep_Press_al. nn_Press_et dep_from_Press prep_one_from det_one_the conj_but_similar_modi prep_as_similar_one cop_similar_is nsubj_similar_algorithm det_work_this prep_in_used_work vmod_algorithm_used nn_algorithm_Powells det_algorithm_The
C08-1005	P03-1021	o	imum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- to maximize BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU dep_maximize_Papineni dobj_maximize_score aux_maximize_to amod_Och_2003 vmod_training_maximize dep_training_Och appos_training_MERT nn_training_rate nn_training_error nn_training_imum
C08-1014	P03-1021	o	By introducing the hidden word alignment variable a -LRB- Brown et al. 1993 -RRB- the optimal translation can be searched for based on the following criterion * 1 arg max -LRB- -LRB- -RRB- -RRB- M mm m ea eh = = efa -LRB- 1 -RRB- where is a string of phrases in the target language e f fa is the source language string of phrases he are feature functions weights -LRB- -RRB- m m are typically optimized to maximize the scoring function -LRB- Och 2003 -RRB-	amod_Och_2003 dep_function_Och amod_function_scoring det_function_the dobj_maximize_function aux_maximize_to xcomp_optimized_maximize advmod_optimized_typically auxpass_optimized_are nsubjpass_optimized_m nn_m_m rcmod_weights_optimized nn_functions_feature cop_functions_are nsubj_functions_he appos_string_weights rcmod_string_functions prep_of_string_phrases nn_string_language nn_string_source det_string_the cop_string_is nsubj_string_1 dep_is_fa nn_fa_f dep_fa_e nn_language_target det_language_the prep_in_string_language prep_of_string_phrases det_string_a cop_string_is advmod_string_where num_string_1 nn_string_efa amod_string_= amod_string_= discourse_string_eh dep_string_ea dep_string_max nn_ea_m nn_ea_mm nn_ea_M nn_max_arg conj_1_string dep_1_* prepc_by_1_introducing amod_criterion_following det_criterion_the prep_on_based_criterion prepc_for_searched_based auxpass_searched_be aux_searched_can nsubjpass_searched_translation dep_searched_a amod_translation_optimal det_translation_the amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_a_Brown rcmod_variable_searched nn_variable_alignment nn_variable_word amod_variable_hidden det_variable_the dobj_introducing_variable
C08-1014	P03-1021	o	Our MT baseline system is based on Moses decoder -LRB- Koehn et al. 2007 -RRB- with word alignment obtained from GIZA + + -LRB- Och et al. 2003 -RRB-	amod_Och_2003 dep_Och_al. nn_Och_et dep_GIZA_Och conj_+_GIZA_+ prep_from_obtained_+ prep_from_obtained_GIZA vmod_alignment_obtained nn_alignment_word amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_decoder_Koehn nn_decoder_Moses prep_with_based_alignment prep_on_based_decoder auxpass_based_is nsubjpass_based_system nn_system_baseline nn_system_MT poss_system_Our
C08-1014	P03-1021	p	1 Introduction State-of-the-art Statistical Machine Translation -LRB- SMT -RRB- systems usually adopt a two-pass search strategy -LRB- Och 2003 Koehn et al. 2003 -RRB- as shown in Figure 1	num_Figure_1 prep_in_shown_Figure mark_shown_as nn_al._et amod_Koehn_2003 dep_Koehn_al. dep_Och_Koehn appos_Och_2003 dep_strategy_Och nn_strategy_search amod_strategy_two-pass det_strategy_a advcl_adopt_shown dobj_adopt_strategy advmod_adopt_usually nsubj_adopt_systems nn_systems_Translation appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical amod_Translation_State-of-the-art nn_Translation_Introduction num_Translation_1
C08-1041	P03-1021	o	We use minimum error rate training -LRB- Och 2003 -RRB- to tune the feature weights for the log-linear model	amod_model_log-linear det_model_the prep_for_weights_model nn_weights_feature det_weights_the dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_use_tune dobj_use_training nsubj_use_We
C08-1064	P03-1021	o	Except where noted each system was trained on 27 million words of newswire data aligned with GIZA + + -LRB- Och and Ney 2003 -RRB- and symmetrized with the grow-diag-final-and heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_grow-diag-final-and det_heuristic_the prep_with_symmetrized_heuristic amod_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ conj_and_aligned_symmetrized prep_with_aligned_+ prep_with_aligned_GIZA amod_data_newswire vmod_words_symmetrized vmod_words_aligned prep_of_words_data num_words_million number_million_27 prep_on_trained_words auxpass_trained_was nsubjpass_trained_system advcl_trained_noted mark_trained_Except det_system_each advmod_noted_where advcl_``_trained
C08-1064	P03-1021	o	In all experiments that follow each system configuration was independently optimized on the NIST 2003 Chinese-English test set -LRB- 919 sentences -RRB- using minimum error rate training -LRB- Och 2003 -RRB- and tested on the NIST 2005 Chinese-English task -LRB- 1082 sentences -RRB-	num_sentences_1082 appos_task_sentences amod_task_Chinese-English num_task_2005 nn_task_NIST det_task_the prep_on_tested_task dep_Och_2003 conj_and_training_tested appos_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_tested dobj_using_training num_sentences_919 appos_set_sentences nn_set_test amod_set_Chinese-English num_set_2003 nn_set_NIST det_set_the xcomp_optimized_using prep_on_optimized_set advmod_optimized_independently auxpass_optimized_was nsubjpass_optimized_configuration prep_in_optimized_experiments nn_configuration_system det_configuration_each nsubj_follow_that rcmod_experiments_follow det_experiments_all
C08-1064	P03-1021	o	This may be because their system was not tuned using minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training xcomp_tuned_using neg_tuned_not auxpass_tuned_was nsubjpass_tuned_system mark_tuned_because poss_system_their advcl_be_tuned aux_be_may nsubj_be_This ccomp_``_be
C08-1064	P03-1021	o	5We use deterministic sampling which is useful for reproducibility and for minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum pobj_for_training conj_and_useful_for prep_for_useful_reproducibility cop_useful_is nsubj_useful_which rcmod_sampling_for rcmod_sampling_useful amod_sampling_deterministic nn_sampling_use nn_sampling_5We
C08-1064	P03-1021	o	Our baseline uses Giza + + alignments -LRB- Och and Ney 2003 -RRB- symmetrized with the grow-diag-final-and heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_grow-diag-final-and det_heuristic_the prep_with_symmetrized_heuristic dep_Och_2003 conj_and_Och_Ney conj_+_Giza_alignments vmod_uses_symmetrized dep_uses_Ney dep_uses_Och dobj_uses_alignments dobj_uses_Giza nsubj_uses_baseline poss_baseline_Our ccomp_``_uses
C08-1074	P03-1021	o	Proceedings of the 22nd International Conference on Computational Linguistics -LRB- Coling 2008 -RRB- pages 585592 Manchester August 2008 Random Restarts in Minimum Error Rate Training for Statistical Machine Translation Robert C. Moore and Chris Quirk Microsoft Research Redmond WA 98052 USA bobmoore@microsoft.com, chrisq@microsoft.com Abstract Ochs -LRB- 2003 -RRB- minimum error rate training -LRB- MERT -RRB- procedure is the most commonly used method for training feature weights in statistical machine translation -LRB- SMT -RRB- models	nn_models_translation appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_weights_models nn_weights_feature nn_weights_training prep_for_method_weights amod_method_used det_method_the cop_method_is nsubj_method_Manchester advmod_used_commonly advmod_used_most nn_procedure_training appos_training_MERT nn_training_rate nn_training_error amod_training_minimum nn_training_Ochs appos_Ochs_2003 nn_Ochs_Abstract nn_Ochs_chrisq@microsoft.com nn_Ochs_bobmoore@microsoft.com, nn_Ochs_USA num_WA_98052 nn_Redmond_Research nn_Redmond_Microsoft nn_Redmond_Quirk nn_Redmond_Chris appos_Moore_procedure conj_and_Moore_WA conj_and_Moore_Redmond nn_Moore_C. nn_Moore_Robert nn_Moore_Translation nn_Moore_Machine nn_Moore_Statistical prep_for_Training_WA prep_for_Training_Redmond prep_for_Training_Moore vmod_Rate_Training nn_Rate_Error nn_Rate_Minimum prep_in_Restarts_Rate nn_Restarts_Random num_Restarts_2008 nn_Restarts_August appos_Manchester_Restarts num_Manchester_585592 rcmod_pages_method dep_pages_Proceedings num_Coling_2008 appos_Linguistics_Coling nn_Linguistics_Computational prep_on_Conference_Linguistics nn_Conference_International amod_Conference_22nd det_Conference_the prep_of_Proceedings_Conference
C08-1074	P03-1021	p	1 Introduction Och -LRB- 2003 -RRB- introduced minimum error rate training -LRB- MERT -RRB- for optimizing feature weights in statistical machine translation -LRB- SMT -RRB- models and demonstrated that it produced higher translation quality scores than maximizing the conditional likelihood of a maximum entropy model using the same features	amod_features_same det_features_the dobj_using_features nn_model_entropy nn_model_maximum det_model_a prep_of_likelihood_model amod_likelihood_conditional det_likelihood_the vmod_maximizing_using dobj_maximizing_likelihood prepc_than_scores_maximizing nn_scores_quality nn_scores_translation amod_scores_higher dobj_produced_scores nsubj_produced_it mark_produced_that ccomp_demonstrated_produced nn_models_translation appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_weights_models nn_weights_feature amod_weights_optimizing conj_and_training_demonstrated prep_for_training_weights appos_training_MERT nn_training_rate nn_training_error amod_training_minimum amod_training_introduced dep_training_Och appos_Och_2003 nn_Och_Introduction num_Och_1
C08-1125	P03-1021	o	Then we use both Moses decoder and its suppo We run the decoder with its d then use Moses ' implementation of minimum error rate training -LRB- Och 2003 -RRB- to tune the feature weights on the development set	nn_set_development det_set_the prep_on_weights_set nn_weights_feature det_weights_the dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_of_implementation_training poss_implementation_Moses vmod_use_tune dobj_use_implementation poss_d_its det_decoder_the dep_run_use advmod_run_then prep_with_run_d dobj_run_decoder nsubj_run_We rcmod_suppo_run poss_suppo_its conj_and_decoder_suppo nn_decoder_Moses preconj_decoder_both dobj_use_suppo dobj_use_decoder nsubj_use_we advmod_use_Then
C08-1125	P03-1021	o	3 Baseline MT System The phrase-based SMT system used in our experiments is Moses phrase translation pro ing probabilities and languag ties are combined in the log-linear model to obtain the best translation best e of the source sentence f = = M p | -RRB- -LRB- maxarg fee ebest -LRB- 2 -RRB- m mm h 1 -LRB- maxarg f -RRB- e e The weights are set by a discriminative training method using a held-out data set as describ in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_set_in prep_as_set_describ vmod_data_set amod_data_held-out det_data_a dobj_using_data vmod_method_using nn_method_training amod_method_discriminative det_method_a agent_set_method auxpass_set_are nsubjpass_set_ebest dep_set_source dep_set_the det_weights_The dep_weights_e dep_e_e nn_f_maxarg dep_h_weights dep_h_f num_h_1 nn_h_mm nn_h_m num_h_2 appos_ebest_h nn_ebest_fee nn_ebest_maxarg nn_|_p nn_|_M amod_|_= amod_|_= dep_sentence_f dep_source_| dep_source_sentence dep_best_e prepc_of_translation_set amod_translation_best amod_translation_best det_translation_the dobj_obtain_translation aux_obtain_to amod_model_log-linear det_model_the xcomp_combined_obtain prep_in_combined_model auxpass_combined_are nsubjpass_combined_ties nn_ties_languag nn_probabilities_ing amod_probabilities_pro nn_probabilities_translation nn_probabilities_phrase conj_and_Moses_combined conj_and_Moses_probabilities cop_Moses_is nsubj_Moses_system poss_experiments_our prep_in_used_experiments vmod_system_used nn_system_SMT amod_system_phrase-based nn_system_The nn_system_System nn_system_MT nn_system_Baseline num_system_3
C08-1127	P03-1021	o	For the efficiency of minimum-error-rate training -LRB- Och 2003 -RRB- we built our development set -LRB- 580 sentences -RRB- using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data	nn_data_test nn_data_evaluation nn_data_MT-02 nn_data_NIST det_data_the num_characters_50 prep_from_exceeding_data dobj_exceeding_characters neg_exceeding_not vmod_sentences_exceeding dobj_using_sentences num_sentences_580 appos_set_sentences nn_set_development poss_set_our xcomp_built_using dobj_built_set nsubj_built_we prep_for_built_efficiency appos_Och_2003 dep_training_Och amod_training_minimum-error-rate prep_of_efficiency_training det_efficiency_the
C08-1127	P03-1021	o	This wrong translation of content words is similar to the incorrect omission reported in -LRB- Och et al. 2003 -RRB- which both hurt translation adequacy	nn_adequacy_translation dobj_hurt_adequacy preconj_hurt_both nsubj_hurt_which rcmod_Och_hurt amod_Och_2003 dep_Och_al. nn_Och_et prep_in_reported_Och vmod_omission_reported amod_omission_incorrect det_omission_the prep_to_similar_omission cop_similar_is nsubj_similar_translation nn_words_content prep_of_translation_words amod_translation_wrong det_translation_This
C08-1127	P03-1021	o	Firstly we run GIZA + + -LRB- Och and Ney 2000 -RRB- on the training corpus in both directions and then apply the ogrow-diag-finalprefinement rule -LRB- Koehn et al. 2003 -RRB- to obtain many-to-many word alignments	nn_alignments_word amod_alignments_many-to-many dobj_obtain_alignments aux_obtain_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_rule_Koehn nn_rule_ogrow-diag-finalprefinement det_rule_the vmod_apply_obtain dobj_apply_rule advmod_apply_then nsubj_apply_we preconj_directions_both nn_corpus_training det_corpus_the num_Och_2000 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ conj_and_run_apply prep_in_run_directions dobj_run_+ dobj_run_GIZA nsubj_run_we advmod_run_Firstly
C08-1144	P03-1021	o	Starting with bilingualphrasepairsextractedfromautomatically aligned parallel text -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- these PSCFG approaches augment each contiguous -LRB- in source and target words -RRB- phrase pair with a left-hand-side symbol -LRB- like the VP in the example above -RRB- and perform a generalization procedure to form rules that include nonterminal symbols	amod_symbols_nonterminal dobj_include_symbols nsubj_include_that rcmod_rules_include dobj_form_rules aux_form_to nn_procedure_generalization det_procedure_a vmod_perform_form dobj_perform_procedure prep_example_above det_example_the prep_in_VP_example det_VP_the conj_and_like_perform pobj_like_VP dep_symbol_perform dep_symbol_like amod_symbol_left-hand-side det_symbol_a nn_pair_phrase amod_pair_contiguous nn_words_target nn_words_source conj_and_source_target pobj_in_words dep_contiguous_in det_contiguous_each prep_with_augment_symbol dobj_augment_pair dep_approaches_augment nsubj_approaches_PSCFG vmod_approaches_Starting det_PSCFG_these num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney appos_text_Koehn appos_text_2004 appos_text_Ney appos_text_Och amod_text_parallel amod_text_aligned advmod_aligned_bilingualphrasepairsextractedfromautomatically prep_with_Starting_text ccomp_``_approaches
C08-1144	P03-1021	o	2 Summary of approaches Given a source language sentence f statistical machine translation defines the translation task as selecting the most likely target translation e under a model P -LRB- e | f -RRB- i.e. e -LRB- f -RRB- = argmax e P -LRB- e | f -RRB- = argmax e msummationdisplay i = 1 hi -LRB- e f -RRB- i where the argmax operation denotes a search through a structured space of translation ouputs in the target language hi -LRB- e f -RRB- are bilingual features of e and f and monolingual features of e and weights i are trained discriminitively to maximize translation quality -LRB- based on automatic metrics -RRB- on held out data -LRB- Och 2003 -RRB-	amod_Och_2003 dep_data_Och dep_held_data prt_held_out amod_metrics_automatic nn_quality_translation prepc_on_maximize_held pobj_maximize_metrics prepc_based_on_maximize_on dobj_maximize_quality aux_maximize_to dep_discriminitively_maximize advmod_trained_discriminitively auxpass_trained_are nsubjpass_trained_i nn_i_weights conj_and_e_trained prep_of_features_trained prep_of_features_e dep_features_monolingual dep_features_f dep_features_e conj_and_e_monolingual conj_and_e_f prep_of_features_features amod_features_bilingual cop_features_are dep_features_= dep_are_e dep_e_f nn_language_target det_language_the nn_ouputs_translation prep_of_space_ouputs amod_space_structured det_space_a prep_in_search_language prep_through_search_space det_search_a dobj_denotes_search nsubj_denotes_operation advmod_denotes_where nn_operation_argmax det_operation_the rcmod_i_denotes dep_i_e dep_e_f dobj_=_1 advmod_=_i dep_=_e dep_=_argmax dep_=_= nn_i_msummationdisplay dep_=_f dep_=_P dep_f_| dep_|_e dep_P_e nn_P_argmax discourse_=_hi dep_=_i dep_=_hi xcomp_=_= dep_=_f dep_=_e dep_f_| dep_|_e appos_P_i.e. appos_P_f nn_P_model det_P_a prep_under_translation_P dep_translation_e nn_translation_target amod_translation_likely det_translation_the advmod_likely_most dobj_selecting_translation nn_task_translation det_task_the dobj_defines_task nsubj_defines_translation nn_translation_machine amod_translation_statistical prepc_as_f_selecting rcmod_f_defines dep_sentence_f nn_sentence_language nn_sentence_source det_sentence_a pobj_Given_sentence prep_approaches_Given dep_Summary_features prep_of_Summary_approaches num_Summary_2 dep_``_Summary
C08-5001	P03-1021	o	The k-best list is also frequently used in discriminative learning to approximate the whole set of candidates which is usually exponentially large -LRB- Och 2003 McDonald et al. 2005 -RRB-	num_McDonald_2005 nn_McDonald_al. nn_McDonald_et dep_Och_McDonald dep_Och_2003 dep_large_Och advmod_large_exponentially advmod_large_usually cop_large_is nsubj_large_which rcmod_candidates_large prep_of_set_candidates amod_set_whole det_set_the amod_set_approximate prep_to_learning_set amod_discriminative_learning prep_in_used_discriminative advmod_used_frequently advmod_used_also auxpass_used_is nsubjpass_used_list amod_list_k-best det_list_The
D07-1005	P03-1021	o	Minimum Error Rate Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- under BLEU criterion is used to estimate 20 feature function weights over the larger development set -LRB- dev1 -RRB-	appos_set_dev1 nn_set_development amod_set_larger det_set_the prep_over_weights_set nn_weights_function nn_weights_feature num_weights_20 dobj_estimate_weights aux_estimate_to xcomp_used_estimate auxpass_used_is nsubjpass_used_Training nn_criterion_BLEU dep_Och_2003 prep_under_Training_criterion dep_Training_Och appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum
D07-1005	P03-1021	o	Our human word alignments do not distinguish between Sure and Probable links -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney nn_links_Probable appos_Sure_Ney appos_Sure_Och conj_and_Sure_links prep_between_distinguish_links prep_between_distinguish_Sure neg_distinguish_not aux_distinguish_do nsubj_distinguish_alignments nn_alignments_word amod_alignments_human poss_alignments_Our
D07-1005	P03-1021	o	Such an approach contrasts with the log-linear HMM/Model -4 combination proposed by Och and Ney -LRB- 2003 -RRB-	appos_Ney_2003 conj_and_Och_Ney agent_proposed_Ney agent_proposed_Och vmod_combination_proposed num_combination_-4 nn_combination_HMM/Model amod_combination_log-linear det_combination_the prep_with_contrasts_combination nsubj_contrasts_approach det_approach_an predet_approach_Such
D07-1005	P03-1021	o	2 Word Alignment Framework A statistical translation model -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- describes the relationship between a pair of sentences in the source and target languages -LRB- f = fJ1 e = eI1 -RRB- using a translation probability P -LRB- f | e -RRB-	dep_|_e nn_|_f dep_P_| nn_P_probability nn_P_translation det_P_a dobj_using_P dobj_=_eI1 dep_=_e dep_=_= dobj_=_fJ1 dep_=_f dep_languages_= nn_languages_target conj_and_source_languages det_source_the prep_of_pair_sentences det_pair_a prep_in_relationship_languages prep_in_relationship_source prep_between_relationship_pair det_relationship_the xcomp_describes_using dobj_describes_relationship nsubj_describes_model dep_Och_2003 conj_and_Och_Ney dep_al._Ney dep_al._Och num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation amod_model_statistical nn_model_A nn_model_Framework nn_model_Alignment nn_model_Word num_model_2
D07-1005	P03-1021	p	-LRB- 2 -RRB- We note that these posterior probabilities can be computed efficiently for some alignment models such as the HMM -LRB- Vogel et al. 1996 Och and Ney 2003 -RRB- Models 1 and 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_1_2 dep_Models_2 dep_Models_1 dep_Och_2003 conj_and_Och_Ney dep_Vogel_Ney dep_Vogel_Och amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_HMM_Brown appos_HMM_Models appos_HMM_Vogel det_HMM_the prep_such_as_models_HMM nn_models_alignment det_models_some prep_for_computed_models advmod_computed_efficiently auxpass_computed_be aux_computed_can nsubjpass_computed_probabilities mark_computed_that amod_probabilities_posterior det_probabilities_these ccomp_note_computed nsubj_note_We dep_note_2
D07-1005	P03-1021	o	High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system -LRB- Och and Ney 2003 Fraser and Marcu 2006b -RRB-	appos_Och_2006b conj_and_Och_Marcu conj_and_Och_Fraser conj_and_Och_2003 conj_and_Och_Ney dep_system_Marcu dep_system_Fraser dep_system_2003 dep_system_Ney dep_system_Och nn_system_SMT amod_system_phrase-based det_system_a prep_of_quality_system dobj_improve_quality nsubj_improve_which rcmod_phrase-pairs_improve amod_phrase-pairs_accurate advmod_accurate_more dobj_yield_phrase-pairs aux_yield_can nsubj_yield_alignments nn_alignments_word nn_alignments_quality amod_alignments_High ccomp_``_yield
D07-1005	P03-1021	o	Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling -LRB- Och and Ney 2003 Deng and Byrne 2005 Martin et al. 2005 -RRB- or alternative approaches to training -LRB- Fraser and Marcu 2006b Moore 2005 Ittycheriah and Roukos 2005 -RRB-	appos_Ittycheriah_2005 conj_and_Ittycheriah_Roukos num_Moore_2005 dep_Fraser_Roukos dep_Fraser_Ittycheriah conj_and_Fraser_Moore conj_and_Fraser_2006b conj_and_Fraser_Marcu dep_training_Moore dep_training_2006b dep_training_Marcu dep_training_Fraser prep_to_approaches_training amod_approaches_alternative num_Martin_2005 nn_Martin_al. nn_Martin_et conj_and_Och_Martin conj_and_Och_2005 conj_and_Och_Byrne conj_and_Och_Deng conj_and_Och_2003 conj_and_Och_Ney conj_or_modeling_approaches dep_modeling_Martin dep_modeling_2005 dep_modeling_Byrne dep_modeling_Deng dep_modeling_2003 dep_modeling_Ney dep_modeling_Och amod_modeling_better nn_quality_alignment nn_quality_word det_quality_the prep_through_improving_approaches prep_through_improving_modeling dobj_improving_quality prepc_on_focussed_improving aux_focussed_has nsubj_focussed_Much nn_alignment_word prep_in_work_alignment amod_work_recent det_work_the prep_of_Much_work ccomp_``_focussed
D07-1006	P03-1021	o	4.2 Experiments To build all alignment systems we start with 5 iterations of Model 1 followed by 4 iterations of HMM -LRB- Vogel et al. 1996 -RRB- as implemented in GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ prep_in_implemented_+ prep_in_implemented_GIZA mark_implemented_as amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et prep_of_iterations_HMM num_iterations_4 prep_by_followed_iterations nsubj_followed_iterations mark_followed_with num_Model_1 prep_of_iterations_Model num_iterations_5 advcl_start_implemented dep_start_Vogel advcl_start_followed nsubj_start_we nsubj_start_Experiments nn_systems_alignment det_systems_all dobj_build_systems aux_build_To vmod_Experiments_build num_Experiments_4.2
D07-1006	P03-1021	o	For all non-LEAF systems we take the best performing of the union refined and intersection symmetrization heuristics -LRB- Och and Ney 2003 -RRB- to combine the 1-to-N and M-to-1 directions resulting in a M-to-N alignment	amod_alignment_M-to-N det_alignment_a prep_in_resulting_alignment nn_directions_M-to-1 vmod_1-to-N_resulting conj_and_1-to-N_directions det_1-to-N_the dobj_combine_directions dobj_combine_1-to-N aux_combine_to dep_Och_2003 conj_and_Och_Ney appos_heuristics_Ney appos_heuristics_Och nn_heuristics_symmetrization xcomp_refined_combine dep_refined_heuristics conj_and_refined_intersection det_union_the conj_performing_intersection conj_performing_refined prep_of_performing_union nsubj_performing_best det_best_the dep_take_performing nsubj_take_we prep_for_take_systems amod_systems_non-LEAF det_systems_all
D07-1006	P03-1021	o	For French/English translation we use a state of the art phrase-based MT system similar to -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney prep_to_similar_2004 prep_to_similar_Ney prep_to_similar_Och amod_system_similar nn_system_MT amod_system_phrase-based nn_system_art det_system_the prep_of_state_system det_state_a dobj_use_state nsubj_use_we prep_for_use_translation amod_translation_French/English
D07-1006	P03-1021	o	-LRB- Och and Ney 2003 -RRB- invented heuristic symmetriza57 FRENCH/ENGLISH ARABIC/ENGLISH SYSTEM F-MEASURE -LRB- = 0.4 -RRB- BLEU F-MEASURE -LRB- = 0.1 -RRB- BLEU GIZA + + 73.5 30.63 75.8 51.55 -LRB- FRASER AND MARCU 2006B -RRB- 74.1 31.40 79.1 52.89 LEAF UNSUPERVISED 74.5 72.3 LEAF SEMI-SUPERVISED 76.3 31.86 84.5 54.34 Table 3 Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment this was extended in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_extended_in auxpass_extended_was nsubjpass_extended_this amod_alignment_M-to-N det_alignment_a prep_in_resulting_alignment vmod_model_resulting nn_model_M-to-1 det_model_a conj_and_model_model amod_model_1-to-N det_model_a prep_of_output_model prep_of_output_model det_output_the prep_of_tion_output dep_Results_extended dep_Results_tion amod_Results_Experimental num_Table_3 num_Table_54.34 num_Table_84.5 appos_31.86_Table dep_76.3_31.86 dep_SEMI-SUPERVISED_76.3 dep_LEAF_SEMI-SUPERVISED dep_72.3_LEAF number_72.3_74.5 dep_UNSUPERVISED_72.3 amod_LEAF_UNSUPERVISED num_LEAF_52.89 dep_79.1_LEAF dep_79.1_31.40 number_31.40_74.1 appos_FRASER_2006B conj_and_FRASER_MARCU dep_51.55_79.1 dep_51.55_MARCU dep_51.55_FRASER dep_51.55_75.8 cc_51.55_+ dep_75.8_30.63 number_30.63_73.5 dep_GIZA_Results conj_+_GIZA_51.55 nn_GIZA_BLEU dep_=_51.55 dep_=_GIZA dep_=_0.1 nn_F-MEASURE_BLEU dep_=_0.4 dep_F-MEASURE_F-MEASURE dep_F-MEASURE_= nn_F-MEASURE_SYSTEM nn_F-MEASURE_ARABIC/ENGLISH nn_F-MEASURE_FRENCH/ENGLISH nn_F-MEASURE_symmetriza57 nn_F-MEASURE_heuristic dep_invented_= dobj_invented_F-MEASURE nsubj_invented_Ney nsubj_invented_Och amod_Och_2003 conj_and_Och_Ney
D07-1006	P03-1021	o	Our work is most similar to work using discriminative log-linear models for alignment which is similar to discriminative log-linear models used for the SMT decoding -LRB- translation -RRB- problem -LRB- Och and Ney 2002 Och 2003 -RRB-	amod_Och_2003 dep_Och_Och conj_and_Och_2002 conj_and_Och_Ney dep_problem_2002 dep_problem_Ney dep_problem_Och nn_problem_decoding nn_problem_SMT det_problem_the appos_decoding_translation prep_for_used_problem vmod_models_used amod_models_log-linear amod_models_discriminative prep_to_similar_models cop_similar_is nsubj_similar_which rcmod_alignment_similar amod_models_log-linear amod_models_discriminative prep_for_using_alignment dobj_using_models xcomp_work_using aux_work_to xcomp_similar_work advmod_similar_most cop_similar_is nsubj_similar_work poss_work_Our ccomp_``_similar
D07-1006	P03-1021	o	-LRB- Och and Ney 2003 -RRB- presented results suggesting that the additional parameters required to ensure that a model is not deficient result in inferior performance but we plan to study whether this is the case for our generative model in future work	amod_work_future prep_in_model_work amod_model_generative poss_model_our prep_for_case_model det_case_the cop_case_is nsubj_case_this mark_case_whether ccomp_study_case aux_study_to xcomp_plan_study nsubj_plan_we amod_performance_inferior prep_in_result_performance dobj_deficient_result neg_deficient_not cop_deficient_is nsubj_deficient_model mark_deficient_that det_model_a ccomp_ensure_deficient aux_ensure_to xcomp_required_ensure conj_but_parameters_plan vmod_parameters_required amod_parameters_additional det_parameters_the prep_that_suggesting_plan prep_that_suggesting_parameters vmod_results_suggesting amod_results_presented dep_results_Ney dep_results_Och dep_Och_2003 conj_and_Och_Ney
D07-1006	P03-1021	p	2.2 Unsupervised Parameter Estimation We can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of Model 4 -LRB- Brown et al. 1993 -RRB- described thoroughly in -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och prep_described_in advmod_described_thoroughly amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 prep_of_that_Model amod_fashion_similar det_fashion_a det_model_this prep_of_parameters_model det_parameters_the prep_of_estimation_parameters nn_estimation_likelihood amod_estimation_maximum prep_to_perform_that prep_in_perform_fashion dobj_perform_estimation aux_perform_can nsubj_perform_We vmod_Estimation_described dep_Estimation_Brown rcmod_Estimation_perform nn_Estimation_Parameter amod_Estimation_Unsupervised num_Estimation_2.2 dep_``_Estimation
D07-1006	P03-1021	o	We use Viterbi training -LRB- Brown et al. 1993 -RRB- but neighborhood estimation -LRB- Al-Onaizan et al. 1999 Och and Ney 2003 -RRB- or pegging -LRB- Brown et al. 1993 -RRB- could also be used	auxpass_used_be advmod_used_also aux_used_could nsubjpass_used_al. num_Brown_1993 dep_Brown_al. nn_Brown_et conj_or_Och_pegging appos_Och_2003 conj_and_Och_Ney appos_al._Brown dep_al._pegging dep_al._Ney dep_al._Och num_al._1999 nn_al._et amod_al._Al-Onaizan rcmod_estimation_used nn_estimation_neighborhood num_al._1993 nn_al._et amod_al._Brown nn_training_Viterbi conj_but_use_estimation dep_use_al. dobj_use_training nsubj_use_We
D07-1006	P03-1021	p	-LRB- Och and Ney 2003 -RRB- discussed efficient implementation	amod_implementation_efficient dobj_discussed_implementation dep_discussed_Ney dep_discussed_Och dep_Och_2003 conj_and_Och_Ney
D07-1007	P03-1021	o	The phrase bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments obtained with GIZA + + -LRB- Och and Ney 2003 -RRB- augmented to improve recall using the grow-diag-final heuristic	amod_heuristic_grow-diag-final det_heuristic_the dobj_using_heuristic xcomp_improve_using dobj_improve_recall aux_improve_to xcomp_augmented_improve dep_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ prep_with_obtained_+ prep_with_obtained_GIZA num_alignments_4 nn_alignments_Model nn_alignments_IBM amod_alignments_bidirectional prep_of_intersection_alignments det_intersection_the dep_derived_augmented ccomp_derived_obtained prep_from_derived_intersection auxpass_derived_is nsubjpass_derived_bilexicon nn_bilexicon_phrase det_bilexicon_The ccomp_``_derived
D07-1007	P03-1021	o	The loglinear model weights are learned using Chiangs implementation of the maximum BLEU training algorithm -LRB- Och 2003 -RRB- both for the baseline and the WSD-augmented system	amod_system_WSD-augmented det_system_the det_baseline_the amod_Och_2003 dep_algorithm_Och nn_algorithm_training nn_algorithm_BLEU nn_algorithm_maximum det_algorithm_the prep_of_implementation_algorithm nn_implementation_Chiangs dobj_using_implementation conj_and_learned_system prep_for_learned_baseline preconj_learned_both xcomp_learned_using auxpass_learned_are nsubjpass_learned_weights nn_weights_model amod_weights_loglinear det_weights_The ccomp_``_system ccomp_``_learned
D07-1029	P03-1021	o	-LRB- 3 -RRB- s in Equation 1 are the weights of different feature functions learned to maximize development set BLEU scores using a method similar to -LRB- Och 2003 -RRB-	amod_Och_2003 dep_to_Och prep_similar_to amod_method_similar det_method_a dobj_using_method nn_scores_BLEU amod_scores_set nn_scores_development xcomp_maximize_using dobj_maximize_scores aux_maximize_to xcomp_learned_maximize nn_functions_feature amod_functions_different vmod_weights_learned prep_of_weights_functions det_weights_the cop_weights_are nsubj_weights_s num_Equation_1 prep_in_s_Equation dep_s_3
D07-1030	P03-1021	o	SMT has evolved from the original word-based approach -LRB- Brown et al. 1993 -RRB- into phrase-based approaches -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and syntax-based approaches -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knignt 2001 Chiang 2005 -RRB-	amod_Chiang_2005 dep_Yamada_Chiang conj_and_Yamada_2001 conj_and_Yamada_Knignt nn_al._et nn_al._Alshawi dep_Wu_2001 dep_Wu_Knignt dep_Wu_Yamada num_Wu_2000 dep_Wu_al. num_Wu_1997 dep_approaches_Wu amod_approaches_syntax-based dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_approaches_phrase-based dep_al._1993 nn_al._et amod_al._Brown dep_approach_al. amod_approach_word-based amod_approach_original det_approach_the conj_and_evolved_approaches dep_evolved_Koehn prep_into_evolved_approaches prep_from_evolved_approach aux_evolved_has nsubj_evolved_SMT ccomp_``_approaches ccomp_``_evolved
D07-1030	P03-1021	o	We run the decoder with its default settings -LRB- maximum phrase length 7 -RRB- and then use Koehn 's implementation of minimum error rate training -LRB- Och 2003 -RRB- to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China also named as 863 Program	num_Program_863 prep_as_named_Program advmod_named_also nn_Program_Development vmod_Research_named prep_of_Research_China conj_and_Research_Program nn_Research_Technology nn_Research_High nn_Research_National cop_Research_is nsubj_Research_name prep_of_name_HTRDP amod_name_full det_name_The det_de2_the prep_on_weights_de2 nn_weights_feature det_weights_the ccomp_tune_Program ccomp_tune_Research iobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_of_implementation_training poss_implementation_Koehn vmod_use_tune dobj_use_implementation advmod_use_then nsubj_use_We num_length_7 nn_length_phrase nn_length_maximum nn_settings_default poss_settings_its det_decoder_the conj_and_run_use dep_run_length prep_with_run_settings dobj_run_decoder nsubj_run_We
D07-1036	P03-1021	o	For the log-linear model training we take minimum-error-rate training method as described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in mark_described_as nn_method_training amod_method_minimum-error-rate advcl_take_described dobj_take_method nsubj_take_we prep_for_take_training nn_training_model amod_training_log-linear det_training_the
D07-1038	P03-1021	o	We obtain weights for the combinations of the features by performing minimum error rate training -LRB- Och 2003 -RRB- on held-out data	amod_data_held-out dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum prep_on_performing_data dobj_performing_training det_features_the prep_of_combinations_features det_combinations_the prepc_by_obtain_performing prep_for_obtain_combinations dobj_obtain_weights nsubj_obtain_We ccomp_``_obtain
D07-1054	P03-1021	o	The translation models were pharse-based -LRB- Zen et al. 2002 -RRB- created using the GIZA + + toolkit -LRB- Och et al. 2003 -RRB-	amod_Och_2003 dep_Och_al. nn_Och_et cc_toolkit_+ dep_GIZA_Och conj_+_GIZA_toolkit det_GIZA_the dobj_using_toolkit dobj_using_GIZA xcomp_created_using dep_Zen_2002 dep_Zen_al. nn_Zen_et dep_pharse-based_created dep_pharse-based_Zen cop_pharse-based_were nsubj_pharse-based_models nn_models_translation det_models_The
D07-1054	P03-1021	o	For tuning of the decoders parameters including the language model weight minimum error training -LRB- Och 2003 -RRB- with respect to the BLEU score using was conducted using the development corpus	nn_corpus_development det_corpus_the dobj_using_corpus xcomp_conducted_using auxpass_conducted_was nsubjpass_conducted_using nn_score_BLEU det_score_the num_Och_2003 prep_with_respect_to_training_score appos_training_Och nn_training_error amod_training_minimum rcmod_weight_conducted conj_weight_training nn_weight_model nn_weight_language det_weight_the pobj_including_weight ccomp_,_including nn_parameters_decoders det_parameters_the prep_of_tuning_parameters pobj_For_tuning dep_``_For
D07-1055	P03-1021	n	Note that the minimum error rate training -LRB- Och 2003 -RRB- uses only the target sentence with the maximum posterior probability whereas here the whole probability distribution is taken into account	prep_into_taken_account auxpass_taken_is nsubjpass_taken_distribution advmod_taken_here mark_taken_whereas nn_distribution_probability amod_distribution_whole det_distribution_the amod_probability_posterior nn_probability_maximum det_probability_the prep_with_sentence_probability nn_sentence_target det_sentence_the advmod_sentence_only advcl_uses_taken dobj_uses_sentence nsubj_uses_training mark_uses_that appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum det_training_the ccomp_Note_uses
D07-1055	P03-1021	n	We will show that some achieve significantly better results than the standard minimum error rate training of -LRB- Och 2003 -RRB-	amod_Och_2003 dep_of_Och prep_training_of nn_training_rate nn_training_error nn_training_minimum amod_training_standard det_training_the prep_than_results_training amod_results_better advmod_better_significantly dobj_achieve_results nsubj_achieve_some mark_achieve_that ccomp_show_achieve aux_show_will nsubj_show_We
D07-1055	P03-1021	p	The current state-of-the-art is to optimize these parameters with respect to the final evaluation criterion this is the so-called minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error nn_training_minimum amod_training_so-called det_training_the cop_training_is nsubj_training_this nn_criterion_evaluation amod_criterion_final det_criterion_the det_parameters_these prep_with_respect_to_optimize_criterion dobj_optimize_parameters aux_optimize_to parataxis_is_training xcomp_is_optimize nsubj_is_state-of-the-art amod_state-of-the-art_current det_state-of-the-art_The ccomp_``_is
D07-1055	P03-1021	p	The current state-of-the-art is to use minimum error rate training -LRB- MERT -RRB- as described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in mark_described_as appos_training_MERT nn_training_rate nn_training_error amod_training_minimum advcl_use_described dobj_use_training aux_use_to xcomp_is_use nsubj_is_state-of-the-art amod_state-of-the-art_current det_state-of-the-art_The ccomp_``_is
D07-1055	P03-1021	o	Therefore -LRB- Och and Ney 2002 Och 2003 -RRB- defined the translation candidate with the minimum word-error rate as pseudo reference translation	nn_translation_reference nn_translation_pseudo prep_as_rate_translation amod_rate_word-error amod_rate_minimum det_rate_the prep_with_candidate_rate nn_candidate_translation det_candidate_the dobj_defined_candidate dep_defined_Ney dep_defined_Och advmod_defined_Therefore dep_Och_2003 dep_Och_Och dep_Och_2002 conj_and_Och_Ney
D07-1055	P03-1021	o	However as pointed out in -LRB- Och 2003 -RRB- there is no reason to believe that the resulting parameters are optimal with respect to translation quality measured with the Bleu score	nn_score_Bleu det_score_the prep_with_measured_score vmod_quality_measured nn_quality_translation prep_with_respect_to_optimal_quality cop_optimal_are nsubj_optimal_parameters mark_optimal_that amod_parameters_resulting det_parameters_the ccomp_believe_optimal aux_believe_to vmod_reason_believe neg_reason_no nsubj_is_reason expl_is_there advcl_is_pointed advmod_is_However appos_Och_2003 prep_in_pointed_Och prt_pointed_out mark_pointed_as
D07-1056	P03-1021	o	3.3 Features Similar to the default features in Pharaoh -LRB- Koehn Och and Marcu 2003 -RRB- we used following features to estimate the weight of our grammar rules	nn_rules_grammar poss_rules_our prep_of_weight_rules det_weight_the dobj_estimate_weight aux_estimate_to vmod_features_estimate prep_following_used_features nsubj_used_we dep_used_Features num_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och dep_Pharaoh_Marcu dep_Pharaoh_Och dep_Pharaoh_Koehn prep_in_features_Pharaoh nn_features_default det_features_the prep_to_Similar_features amod_Features_Similar num_Features_3.3
D07-1056	P03-1021	o	We just assign these rules a constant score trained using our implementation of Minimum Error Rate Training -LRB- Och 2003b -RRB- which is 0.7 in our system	poss_system_our prep_in_0.7_system cop_0.7_is nsubj_0.7_which appos_Och_2003b rcmod_Training_0.7 dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum prep_of_implementation_Training poss_implementation_our dobj_using_implementation xcomp_trained_using vmod_score_trained amod_score_constant det_score_a det_rules_these dobj_assign_score iobj_assign_rules advmod_assign_just nsubj_assign_We ccomp_``_assign
D07-1056	P03-1021	o	6 Training Similar to most state-of-the-art phrase-based SMT systems we use the SRI toolkit -LRB- Stolcke 2002 -RRB- for language model training and Giza + + toolkit -LRB- Och and Ney 2003 -RRB- for word alignment	nn_alignment_word dep_Och_2003 conj_and_Och_Ney prep_for_toolkit_alignment appos_toolkit_Ney appos_toolkit_Och pobj_+_toolkit conj_and_training_Giza nn_training_model nn_training_language conj_+_for_+ pobj_for_Giza pobj_for_training amod_Stolcke_2002 dep_toolkit_Stolcke nn_toolkit_SRI det_toolkit_the prep_use_+ prep_use_for dobj_use_toolkit nsubj_use_we dep_use_Training nn_systems_SMT amod_systems_phrase-based amod_systems_state-of-the-art amod_systems_most prep_to_Similar_systems amod_Training_Similar num_Training_6
D07-1056	P03-1021	o	Based on the word alignment results if the aligned target words of any two adjacent foreign linguistic phrases can also be formed into two valid adjacent phrase according to constraints proposed in the phrase extraction algorithm by Och -LRB- 2003a -RRB- they will be extracted as a reordering training sample	nn_sample_training nn_sample_reordering det_sample_a prep_as_extracted_sample auxpass_extracted_be aux_extracted_will nsubjpass_extracted_they advcl_extracted_formed appos_Och_2003a nn_algorithm_extraction nn_algorithm_phrase det_algorithm_the agent_proposed_Och prep_in_proposed_algorithm vmod_constraints_proposed amod_phrase_adjacent amod_phrase_valid num_phrase_two pobj_formed_constraints prepc_according_to_formed_to prep_into_formed_phrase auxpass_formed_be advmod_formed_also aux_formed_can nsubjpass_formed_words mark_formed_if amod_phrases_linguistic amod_phrases_foreign amod_phrases_adjacent num_phrases_two det_phrases_any prep_of_words_phrases nn_words_target amod_words_aligned det_words_the conj_results_extracted csubj_results_Based nn_alignment_word det_alignment_the prep_on_Based_alignment
D07-1056	P03-1021	o	There have been considerable amount of efforts to improve the reordering model in SMT systems ranging from the fundamental distance-based distortion model -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- flat reordering model -LRB- Wu 1996 Zens et al. 2004 Kumar et al. 2005 -RRB- to lexicalized reordering model -LRB- Tillmann 2004 Kumar et al. 2005 Koehn et al. 2005 -RRB- hierarchical phrase-based model -LRB- Chiang 2005 -RRB- and maximum entropy-based phrase reordering model -LRB- Xiong et al. 2006 -RRB-	amod_Xiong_2006 dep_Xiong_al. nn_Xiong_et nn_model_reordering nn_model_phrase amod_model_entropy-based nn_model_maximum dep_Chiang_2005 appos_model_Chiang amod_model_phrase-based amod_model_hierarchical num_Koehn_2005 nn_Koehn_al. nn_Koehn_et dep_Kumar_Xiong conj_and_Kumar_model conj_and_Kumar_model dep_Kumar_Koehn num_Kumar_2005 nn_Kumar_al. nn_Kumar_et dep_Tillmann_model dep_Tillmann_model dep_Tillmann_Kumar appos_Tillmann_2004 dep_model_Tillmann nn_model_reordering amod_model_lexicalized nn_al._et nn_al._Kumar nn_al._et nn_al._Zens amod_Wu_2005 dep_Wu_al. num_Wu_2004 dep_Wu_al. appos_Wu_1996 appos_model_Wu nn_model_reordering amod_model_flat num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney prep_to_model_model conj_model_model appos_model_2004 appos_model_Ney appos_model_Och nn_model_distortion amod_model_distance-based amod_model_fundamental det_model_the prep_from_ranging_model nn_systems_SMT prep_in_model_systems nn_model_reordering det_model_the dobj_improve_model aux_improve_to vmod_efforts_improve vmod_amount_ranging prep_of_amount_efforts amod_amount_considerable cop_amount_been aux_amount_have expl_amount_There
D07-1079	P03-1021	o	Tuning was done using Maximum BLEU hill-climbing -LRB- Och 2003 -RRB-	amod_Och_2003 appos_hill-climbing_Och nn_hill-climbing_BLEU nn_hill-climbing_Maximum dobj_using_hill-climbing xcomp_done_using auxpass_done_was nsubjpass_done_Tuning
D07-1079	P03-1021	o	A superset of the parallel data was word aligned by GIZA union -LRB- Och and Ney 2003 -RRB- and EMD -LRB- Fraser and Marcu 2006 -RRB-	amod_Fraser_2006 conj_and_Fraser_Marcu dep_EMD_Marcu dep_EMD_Fraser num_Och_2003 conj_and_Och_Ney conj_and_union_EMD appos_union_Ney appos_union_Och nn_union_GIZA agent_aligned_EMD agent_aligned_union vmod_word_aligned cop_word_was nsubj_word_superset amod_data_parallel det_data_the prep_of_superset_data det_superset_A
D07-1079	P03-1021	o	Approaches include word substitution systems -LRB- Brown et al. 1993 -RRB- phrase substitution systems -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and synchronous context-free grammar systems -LRB- Wu and Wong 1998 Chiang 2005 -RRB- all of which train on string pairs and seek to establish connections between source and target strings	nn_strings_target conj_and_source_strings prep_between_connections_strings prep_between_connections_source dobj_establish_connections aux_establish_to xcomp_seek_establish nn_pairs_string conj_and_train_seek prep_on_train_pairs dep_all_seek dep_all_train prep_of_all_which num_Chiang_2005 dep_Wu_Chiang num_Wu_1998 conj_and_Wu_Wong appos_systems_Wong appos_systems_Wu nn_systems_grammar amod_systems_context-free amod_systems_synchronous dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_systems_Koehn nn_systems_substitution nn_systems_phrase amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_systems_all conj_and_systems_systems conj_and_systems_systems dep_systems_Brown nn_systems_substitution nn_systems_word dobj_include_all dobj_include_systems dobj_include_systems dobj_include_systems nsubj_include_Approaches
D07-1080	P03-1021	o	The hierarchical phrase translation pairs are extracted in a standard way -LRB- Chiang 2005 -RRB- First the bilingual data are word alignment annotated by running GIZA + + -LRB- Och and Ney 2003 -RRB- in two directions	num_directions_two prep_in_Och_directions num_Och_2003 conj_and_Och_Ney pobj_+_Ney pobj_+_Och conj_+_GIZA_+ dep_running_+ dep_running_GIZA prepc_by_annotated_running amod_alignment_annotated nn_alignment_word nsubj_are_alignment dep_data_are amod_data_bilingual det_data_the appos_First_data dep_Chiang_2005 dep_way_First appos_way_Chiang amod_way_standard det_way_a prep_in_extracted_way auxpass_extracted_are nsubjpass_extracted_pairs nn_pairs_translation nn_pairs_phrase amod_pairs_hierarchical det_pairs_The ccomp_``_extracted
D07-1080	P03-1021	o	The baseline hierarchical phrase-based system is trained using standard max-BLEU training -LRB- MERT -RRB- without sparse features -LRB- Och 2003 -RRB-	amod_Och_2003 dep_features_Och amod_features_sparse appos_training_MERT nn_training_max-BLEU amod_training_standard prep_without_using_features dobj_using_training xcomp_trained_using auxpass_trained_is nsubjpass_trained_system amod_system_phrase-based amod_system_hierarchical nn_system_baseline det_system_The
D07-1080	P03-1021	o	2 Statistical Machine Translation We use a log-linear approach -LRB- Och 2003 -RRB- in which a foreign language sentence f is translated into another language for example English e by seeking a maximum solution e = argmax e wT h -LRB- f e -RRB- -LRB- 1 -RRB- where h -LRB- f e -RRB- is a large-dimension feature vector	nn_vector_feature amod_vector_large-dimension det_vector_a cop_vector_is nsubj_vector_h advmod_vector_where nn_vector_h dep_f_e appos_h_f dep_f_1 appos_f_e appos_h_f nn_h_wT dep_h_e nn_h_argmax dep_=_vector dep_=_e nn_solution_maximum det_solution_a dobj_seeking_solution dep_e_= prepc_by_e_seeking nn_English_example det_language_another dep_translated_e prep_for_translated_English prep_into_translated_language auxpass_translated_is nsubjpass_translated_sentence prep_in_translated_which dep_sentence_f nn_sentence_language amod_sentence_foreign det_sentence_a dep_Och_2003 rcmod_approach_translated dep_approach_Och amod_approach_log-linear det_approach_a dobj_use_approach nsubj_use_We rcmod_Translation_use nn_Translation_Machine amod_Translation_Statistical num_Translation_2
D07-1080	P03-1021	o	1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on -LRB- hierarchical -RRB- phrase-based translation -LRB- Och and Ney 2004 Koehn et al. 2003 Chiang 2005 -RRB- or syntax-based translation -LRB- Galley et al. 2006 -RRB-	amod_Galley_2006 dep_Galley_al. nn_Galley_et amod_translation_syntax-based dep_Chiang_2005 num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Chiang conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_translation_Galley conj_or_translation_translation appos_translation_Koehn appos_translation_2004 appos_translation_Ney appos_translation_Och amod_translation_phrase-based amod_translation_hierarchical prep_on_based_translation prep_on_based_translation preconj_based_either amod_features_real-valued prep_of_number_features amod_number_small det_number_a vmod_training_based dobj_training_number advmod_training_discriminatively agent_achieved_training auxpass_achieved_been aux_achieved_have nsubjpass_achieved_advances nn_translation_machine amod_translation_statistical prep_in_advances_translation amod_advances_recent det_advances_The rcmod_Introduction_achieved num_Introduction_1
D07-1091	P03-1021	o	The feature weights i in the log-linear model are determined using a minimum error rate training method typically Powells method -LRB- Och 2003 -RRB-	amod_Och_2003 dep_method_Och nn_method_Powells advmod_method_typically ccomp_method_determined nn_method_training nn_method_rate nn_method_error amod_method_minimum det_method_a dobj_using_method xcomp_determined_using auxpass_determined_are nsubjpass_determined_weights amod_model_log-linear det_model_the prep_in_i_model dep_weights_i nn_weights_feature det_weights_The
D07-1103	P03-1021	o	To model p -LRB- t a | s -RRB- we use a standard loglinear approach p -LRB- t a | s -RRB- exp bracketleftBiggsummationdisplay i ifi -LRB- s t a -RRB- bracketrightBigg where each fi -LRB- s t a -RRB- is a feature function and weights i are set using Ochs algorithm -LRB- Och 2003 -RRB- to maximize the systems BLEU score -LRB- Papineni et aal	nn_aal_et advmod_Papineni_aal nn_score_BLEU nn_score_systems det_score_the dep_maximize_Papineni dobj_maximize_score aux_maximize_to amod_Och_2003 dep_algorithm_Och nn_algorithm_Ochs vmod_using_maximize dobj_using_algorithm xcomp_set_using auxpass_set_are nsubjpass_set_weights dep_weights_i nn_function_feature det_function_a nsubj_is_function nsubj_is_s advmod_is_where appos_s_a appos_s_t nn_s_fi det_s_each rcmod_bracketrightBigg_is nn_bracketrightBigg_ifi dep_s_a appos_s_t appos_ifi_s nn_ifi_i nn_ifi_bracketleftBiggsummationdisplay conj_and_exp_set dobj_exp_bracketrightBigg nsubj_exp_p nn_s_| det_s_a appos_t_s dep_p_t nn_approach_loglinear amod_approach_standard det_approach_a parataxis_use_set parataxis_use_exp dobj_use_approach nsubj_use_we advcl_use_model nn_s_| det_s_a appos_t_s nn_t_p dobj_model_t aux_model_To
D07-1105	P03-1021	o	on test BLEU BP BLEU BP pair-CI 95 % BLEU BP 3 01 03 32.98 0.92 33.03 0.93 -LSB- -0.23 +0.34 -RSB- 33.60 0.93 4 01 04 33.44 0.93 33.46 0.93 -LSB- -0.26 +0.29 -RSB- 34.97 0.94 5 01 05 33.07 0.92 33.14 0.93 -LSB- -0.29 +0.43 -RSB- 34.33 0.93 6 01 06 32.86 0.92 33.53 0.93 -LSB- +0.26 +1.08 -RSB- 34.43 0.93 7 01 07 33.08 0.93 33.51 0.93 -LSB- +0.04 +0.82 -RSB- 34.49 0.93 8 01 08 33.12 0.93 33.47 0.93 -LSB- -0.06 +0.75 -RSB- 34.50 0.94 9 01 09 33.15 0.93 33.22 0.93 -LSB- -0.35 +0.51 -RSB- 34.68 0.93 10 01 10 33.01 0.93 33.59 0.94 -LSB- +0.18 +0.96 -RSB- 34.79 0.94 11 01 11 32.84 0.94 33.40 0.94 -LSB- +0.13 +0.98 -RSB- 34.76 0.94 12 01 12 32.73 0.93 33.49 0.94 -LSB- +0.34 +1.18 -RSB- 34.83 0.94 13 01 13 32.71 0.93 33.54 0.94 -LSB- +0.39 +1.26 -RSB- 34.91 0.94 14 01 14 32.66 0.93 33.69 0.94 -LSB- +0.58 +1.47 -RSB- 34.97 0.94 15 01 15 32.47 0.93 33.57 0.94 -LSB- +0.63 +1.57 -RSB- 34.99 0.94 16 01 16 32.51 0.93 33.62 0.94 -LSB- +0.62 +1.59 -RSB- 35.00 0.94 3.2 Non-Uniform System Prior Weights As pointed out in Section 2.1 a useful property of the MBR-like system selection method is that system prior weights can easily be trained using the Minimum Error Rate Training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum det_Training_the dobj_using_Training xcomp_trained_using auxpass_trained_be advmod_trained_easily aux_trained_can nsubjpass_trained_weights mark_trained_that amod_weights_prior nn_weights_system ccomp_is_trained nsubj_is_property dep_is_pointed dep_is_Weights dep_is_0.94 prep_on_is_BP nn_method_selection nn_method_system amod_method_MBR-like det_method_the prep_of_property_method amod_property_useful det_property_a num_Section_2.1 prep_in_pointed_Section prt_pointed_out mark_pointed_As amod_Weights_Prior nn_Weights_System num_Weights_0.94 num_Weights_35.00 nn_System_Non-Uniform num_System_3.2 dep_0.94_33.62 dep_0.94_01 number_33.62_0.93 dep_33.62_32.51 number_32.51_16 number_01_16 dep_01_0.94 number_0.94_34.99 dep_+0.63_+1.57 dep_0.94_+0.63 dep_0.94_33.57 dep_0.94_01 number_33.57_0.93 dep_33.57_32.47 number_32.47_15 number_01_15 dep_01_0.94 dep_01_+0.58 number_01_0.94 number_0.94_34.97 dep_+0.58_+1.47 number_33.69_0.93 dep_33.69_32.66 dep_33.69_01 number_32.66_14 number_01_14 dep_01_0.94 number_0.94_34.91 dep_+0.39_+1.26 dep_0.94_+0.39 dep_0.94_33.54 dep_0.94_01 number_33.54_0.93 dep_33.54_32.71 number_32.71_13 number_01_13 dep_01_0.94 number_0.94_34.83 dep_+0.34_+1.18 dep_33.49_0.94 dep_33.49_+0.34 num_33.49_0.94 number_33.49_0.93 dep_33.49_32.73 dep_33.49_01 number_32.73_12 number_01_12 dep_01_0.94 number_0.94_34.76 dep_+0.13_+0.98 dep_0.94_+1.59 dep_0.94_+0.62 dep_0.94_0.94 dep_0.94_0.94 dep_0.94_33.69 dep_0.94_33.49 dep_0.94_+0.13 dep_0.94_33.40 dep_0.94_0.93 dep_0.94_-0.23 number_33.40_0.94 dep_33.40_32.84 dep_33.40_0.94 dep_33.40_01 number_33.40_9 dep_33.40_0.94 number_32.84_11 dep_32.84_01 number_01_11 number_0.94_34.79 dep_+0.18_+0.96 num_33.59_0.94 dep_33.59_0.93 dep_33.59_01 dep_0.93_33.01 number_33.01_10 number_01_10 dep_01_0.93 number_0.93_34.68 dep_-0.35_+0.51 dep_0.93_33.59 dep_0.93_-0.35 num_0.93_33.22 num_0.93_33.15 number_33.22_0.93 number_33.15_09 dep_01_+0.18 dep_01_0.93 number_0.94_34.50 dep_-0.06_+0.75 dep_0.93_-0.06 dep_0.93_33.47 number_33.47_0.93 dep_33.47_33.12 dep_33.47_0.93 dep_33.47_01 number_33.47_6 dep_33.47_0.93 number_33.12_08 dep_33.12_01 number_01_8 number_0.93_34.49 dep_+0.04_+0.82 num_33.51_0.93 number_33.51_0.93 dep_33.51_33.08 dep_33.51_01 number_33.08_07 number_01_7 dep_01_0.93 number_0.93_34.43 dep_+0.26_+1.08 dep_0.93_33.51 dep_0.93_+0.26 num_0.93_33.53 num_0.93_32.86 number_33.53_0.92 number_32.86_06 dep_01_+0.04 dep_01_0.93 number_0.93_34.33 num_33.14_0.93 number_33.14_0.92 dep_33.14_33.07 dep_33.14_01 number_33.07_05 number_01_5 dep_01_0.94 number_0.94_34.97 num_33.46_0.93 number_33.46_0.93 dep_33.46_33.44 dep_33.46_01 number_33.44_04 number_01_4 dep_01_0.93 number_0.93_33.60 dep_-0.23_+0.43 dep_-0.23_-0.29 dep_-0.23_33.14 dep_-0.23_+0.29 dep_-0.23_-0.26 dep_-0.23_33.46 dep_-0.23_+0.34 dep_0.93_33.03 dep_0.93_01 number_33.03_0.92 dep_33.03_32.98 number_32.98_03 number_01_3 num_BP_0.93 nn_BP_BLEU amod_BP_% nn_BP_pair-CI nn_BP_BP nn_BP_BLEU nn_BP_BP nn_BP_BLEU nn_BP_test number_%_95
D07-1105	P03-1021	o	Note that all systems were optimized using a non-deterministic implementation of the Minimum Error Rate Training described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_Training_described vmod_Rate_Training nn_Rate_Error nn_Rate_Minimum det_Rate_the prep_of_implementation_Rate amod_implementation_non-deterministic det_implementation_a dobj_using_implementation xcomp_optimized_using auxpass_optimized_were nsubjpass_optimized_systems mark_optimized_that det_systems_all ccomp_Note_optimized
D07-1105	P03-1021	o	For instance word alignment models are often trained using the GIZA + + toolkit -LRB- Och and Ney 2003 -RRB- error minimizing training criteria such as the Minimum Error Rate Training -LRB- Och 2003 -RRB- are employed in order to learn feature function weights for log-linear models and translation candidates are produced using phrase-based decoders -LRB- Koehn et al. 2003 -RRB- in combination with n-gram language models -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_models_Brants nn_models_language amod_models_n-gram prep_with_combination_models amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_decoders_Koehn amod_decoders_phrase-based prep_in_using_combination dobj_using_decoders xcomp_produced_using auxpass_produced_are csubjpass_produced_trained nn_candidates_translation amod_models_log-linear nn_weights_function nn_weights_feature prep_for_learn_models dobj_learn_weights aux_learn_to dep_learn_order mark_learn_in advcl_employed_learn auxpass_employed_are nsubjpass_employed_error amod_Och_2003 dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum det_Training_the prep_such_as_criteria_Training nn_criteria_training dobj_minimizing_criteria vmod_error_minimizing dep_Och_2003 conj_and_Och_Ney appos_toolkit_Ney appos_toolkit_Och pobj_+_toolkit conj_and_GIZA_candidates conj_+_GIZA_employed conj_+_GIZA_+ det_GIZA_the dobj_using_candidates dobj_using_employed dobj_using_+ dobj_using_GIZA xcomp_trained_using advmod_trained_often auxpass_trained_are nsubjpass_trained_models prep_for_trained_instance nn_models_alignment nn_models_word
D07-1105	P03-1021	p	For instance changing the training procedure for word alignment models turned out to be most beneficial for details see -LRB- Och and Ney 2003 -RRB-	dep_Och_2003 conj_and_Och_Ney dep_see_Ney dep_see_Och advmod_beneficial_most cop_beneficial_be aux_beneficial_to dep_turned_see prep_for_turned_details xcomp_turned_beneficial prt_turned_out csubj_turned_changing prep_for_turned_instance nn_models_alignment nn_models_word prep_for_procedure_models nn_procedure_training det_procedure_the dobj_changing_procedure
D07-1105	P03-1021	p	Using the components of the row-vector bm as feature function values for the candidate translation em -LRB- m a16 1 M -RRB- the system prior weights can easily be trained using the Minimum Error Rate Training described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_Training_described vmod_Rate_Training nn_Rate_Error nn_Rate_Minimum det_Rate_the dobj_using_Rate xcomp_trained_using auxpass_trained_be advmod_trained_easily aux_trained_can nsubjpass_trained_weights amod_weights_prior ccomp_system_trained det_system_the dep_a16_M num_a16_1 nn_a16_m dep_em_a16 nn_em_translation nn_em_candidate det_em_the prep_for_values_em nn_values_function nn_values_feature nn_bm_row-vector det_bm_the prep_of_components_bm det_components_the parataxis_Using_system prep_as_Using_values dobj_Using_components ccomp_''_Using
D08-1010	P03-1021	o	We perform minimum error rate training -LRB- Och 2003 -RRB- to tune the feature weights for the log-linear modeltomaximizethesystemssBLEUscoreonthe development set	nn_set_development nn_set_modeltomaximizethesystemssBLEUscoreonthe amod_set_log-linear det_set_the prep_for_weights_set nn_weights_feature det_weights_the dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_perform_tune dobj_perform_training nsubj_perform_We
D08-1012	P03-1021	o	When different decoder settings are applied to the same model MERT weights -LRB- Och 2003 -RRB- from the unprojected single pass setup are used and are kept constant across runs	prep_across_kept_runs acomp_kept_constant auxpass_kept_are nsubjpass_kept_weights conj_and_used_kept auxpass_used_are nsubjpass_used_weights advcl_used_applied nn_setup_pass amod_setup_single amod_setup_unprojected det_setup_the appos_Och_2003 prep_from_weights_setup dep_weights_Och nn_weights_MERT amod_model_same det_model_the prep_to_applied_model auxpass_applied_are nsubjpass_applied_settings advmod_applied_When nn_settings_decoder amod_settings_different ccomp_``_kept ccomp_``_used
D08-1022	P03-1021	o	These parameters 1 8 are tuned by minimum error rate training -LRB- Och 2003 -RRB- on the dev sets	nn_sets_dev det_sets_the appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_on_tuned_sets agent_tuned_training auxpass_tuned_are nsubjpass_tuned_8 number_8_1 rcmod_parameters_tuned det_parameters_These ccomp_``_parameters
D08-1023	P03-1021	o	We benchmark our results against a model -LRB- Hiero -RRB- which was directly trained to optimise BLEUNIST using the standard MERT algorithm -LRB- Och 2003 -RRB- and the full set of translation and lexical weight features described for the Hiero model -LRB- Chiang 2007 -RRB-	amod_Chiang_2007 dep_model_Chiang nn_model_Hiero det_model_the prep_for_described_model nn_features_weight amod_features_lexical vmod_translation_described conj_and_translation_features prep_of_set_features prep_of_set_translation amod_set_full det_set_the amod_Och_2003 conj_and_algorithm_set dep_algorithm_Och nn_algorithm_MERT amod_algorithm_standard det_algorithm_the dobj_using_set dobj_using_algorithm xcomp_optimise_using dobj_optimise_BLEUNIST aux_optimise_to xcomp_trained_optimise advmod_trained_directly auxpass_trained_was nsubjpass_trained_which rcmod_model_trained appos_model_Hiero det_model_a prep_against_results_model poss_results_our amod_results_benchmark nsubj_results_We
D08-1023	P03-1021	o	Most work on discriminative training for SMT has focussed on linear models often with margin based algorithms -LRB- Liang et al. 2006 Watanabe et al. 2006 -RRB- or rescaling a product of sub-models -LRB- Och 2003 Ittycheriah and Roukos 2007 -RRB-	amod_Ittycheriah_2007 conj_and_Ittycheriah_Roukos dep_Och_Roukos dep_Och_Ittycheriah conj_Och_2003 dep_sub-models_Och prep_of_product_sub-models det_product_a dobj_rescaling_product num_Watanabe_2006 nn_Watanabe_al. nn_Watanabe_et dep_Liang_Watanabe appos_Liang_2006 dep_Liang_al. nn_Liang_et conj_or_algorithms_rescaling appos_algorithms_Liang dobj_based_rescaling dobj_based_algorithms vmod_margin_based amod_models_linear prep_with_focussed_margin advmod_focussed_often prep_on_focussed_models aux_focussed_has nsubj_focussed_work prep_for_training_SMT amod_training_discriminative prep_on_work_training amod_work_Most ccomp_``_focussed
D08-1024	P03-1021	o	2 Learning algorithm The translation model is a standard linear model -LRB- Och and Ney 2002 -RRB- which we train using MIRA -LRB- Crammer and Singer 2003 Crammer et al. 2006 -RRB- following Watanabe et al.	nn_al._et nn_al._Watanabe dep_following_al. num_Crammer_2006 nn_Crammer_al. nn_Crammer_et dep_Crammer_Crammer num_Crammer_2003 conj_and_Crammer_Singer appos_MIRA_Singer appos_MIRA_Crammer dobj_using_MIRA xcomp_train_using nsubj_train_we dobj_train_which dep_Och_2002 conj_and_Och_Ney dep_model_Ney dep_model_Och amod_model_linear amod_model_standard det_model_a cop_model_is nsubj_model_model nn_model_translation det_model_The appos_algorithm_following rcmod_algorithm_train rcmod_algorithm_model nn_algorithm_Learning num_algorithm_2
D08-1024	P03-1021	p	1 Introduction Since its introduction by Och -LRB- 2003 -RRB- minimum error rate training -LRB- MERT -RRB- has been widely adopted for training statistical machine translation -LRB- MT -RRB- systems	nn_systems_translation appos_translation_MT nn_translation_machine amod_translation_statistical nn_translation_training prep_for_adopted_systems advmod_adopted_widely auxpass_adopted_been aux_adopted_has nsubjpass_adopted_introduction mark_adopted_Since appos_training_MERT nn_training_rate nn_training_error amod_training_minimum appos_Och_training appos_Och_2003 prep_by_introduction_Och poss_introduction_its advcl_Introduction_adopted num_Introduction_1
D08-1033	P03-1021	o	5.1 Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 -LRB- 177k sentences -RRB- using GIZA + + Model 4 word alignments and the growdiag-final-and combination heuristic -LRB- Koehn et al. 2007 Och and Ney 2003 Koehn 2002 -RRB- which performed better than any alternative combination heuristic .13 The baseline estimates -LRB- Heuristic -RRB- come fromextractingphrasesuptolength7fromtheword alignment	nn_alignment_fromextractingphrasesuptolength7fromtheword dobj_come_alignment nsubj_come_2003 nsubj_come_Ney nsubj_come_Och dep_estimates_Heuristic nn_estimates_baseline det_estimates_The dep_heuristic_estimates num_heuristic_.13 vmod_combination_heuristic dep_alternative_combination amod_any_alternative prep_than_better_any acomp_performed_better nsubj_performed_which dep_Koehn_2002 rcmod_Och_performed dep_Och_Koehn conj_and_Och_2003 conj_and_Och_Ney parataxis_Koehn_come appos_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn nn_heuristic_combination amod_heuristic_growdiag-final-and det_heuristic_the nn_alignments_word num_alignments_4 nn_alignments_Model preconj_alignments_+ conj_and_GIZA_heuristic conj_+_GIZA_alignments dobj_using_heuristic dobj_using_alignments dobj_using_GIZA num_sentences_177k num_length_20 pobj_to_length pcomp_up_to nn_sentences_Europarl amod_sentences_Spanish-English det_sentences_all xcomp_trained_using dep_trained_sentences prep_trained_up prep_on_trained_sentences dobj_trained_Moses nsubj_trained_We rcmod_System_trained nn_System_Baseline num_System_5.1 dep_``_System
D08-1033	P03-1021	o	The parameters for each phrase table were tuned separately using minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training xcomp_tuned_using advmod_tuned_separately auxpass_tuned_were nsubjpass_tuned_parameters nn_table_phrase det_table_each prep_for_parameters_table det_parameters_The ccomp_``_tuned
D08-1051	P03-1021	p	An important contribution to interactive CAT technology was carried out around the TransType -LRB- TT -RRB- project -LRB- Langlais et al. 2002 Foster et al. 2002 Foster 2002 Och et al. 2003 -RRB-	num_Och_2003 nn_Och_al. nn_Och_et num_Foster_2002 num_Foster_2002 nn_Foster_al. nn_Foster_et dep_Langlais_Och conj_Langlais_Foster conj_Langlais_Foster appos_Langlais_2002 dep_Langlais_al. nn_Langlais_et nn_project_TransType det_project_the appos_TransType_TT dep_carried_Langlais prep_around_carried_project prt_carried_out auxpass_carried_was nsubjpass_carried_contribution nn_technology_CAT amod_technology_interactive prep_to_contribution_technology amod_contribution_important det_contribution_An
D08-1051	P03-1021	o	In the present work we decided to use WSR instead of Key Stroke Ratio -LRB- KSR -RRB- which is used in other works on IMT such as -LRB- Och et al. 2003 -RRB-	amod_Och_2003 dep_Och_al. nn_Och_et dep_as_Och mwe_as_such prep_IMT_as prep_on_works_IMT amod_works_other prep_in_used_works auxpass_used_is nsubjpass_used_which rcmod_Ratio_used appos_Ratio_KSR nn_Ratio_Stroke nn_Ratio_Key prep_instead_of_use_Ratio dobj_use_WSR aux_use_to xcomp_decided_use nsubj_decided_we prep_in_decided_work amod_work_present det_work_the
D08-1051	P03-1021	o	EsEn 63.00.9 59.20.9 6.01.4 EnEs 63.80.9 60.51.0 5.21.6 DeEn 71.60.8 69.00.9 3.61.3 EnDe 75.90.8 73.50.9 3.21.2 FrEn 62.90.9 59.21.0 5.91.6 EnFr 63.40.9 60.00.9 5.41.4 bined in a log-linear fashion by adjusting a weight for each of them by means of the MERT -LRB- Och 2003 -RRB- procedure optimising the BLEU -LRB- Papineni et al. 2002 -RRB- score obtained on the development partition	nn_partition_development det_partition_the prep_on_obtained_partition vmod_score_obtained dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_score dep_BLEU_Papineni det_BLEU_the dobj_optimising_BLEU dep_procedure_Och nn_procedure_MERT dep_Och_2003 det_MERT_the prep_of_each_them prep_for_weight_each det_weight_a prep_by_means_of_adjusting_procedure dobj_adjusting_weight amod_fashion_log-linear det_fashion_a agent_bined_adjusting prep_in_bined_fashion vmod_5.41.4_bined num_5.41.4_60.00.9 number_60.00.9_63.40.9 vmod_EnFr_optimising dep_EnFr_5.41.4 dep_5.91.6_EnFr dep_59.21.0_5.91.6 number_59.21.0_62.90.9 dep_FrEn_59.21.0 dep_3.21.2_FrEn dep_73.50.9_3.21.2 number_73.50.9_75.90.8 dep_EnDe_73.50.9 dep_3.61.3_EnDe number_3.61.3_69.00.9 dep_71.60.8_3.61.3 dep_DeEn_71.60.8 dep_5.21.6_DeEn dep_60.51.0_5.21.6 number_60.51.0_63.80.9 dep_EnEs_60.51.0 dep_6.01.4_EnEs dep_59.20.9_6.01.4 number_59.20.9_63.00.9 dep_EsEn_59.20.9
D08-1051	P03-1021	o	In -LRB- Och et al. 2003 -RRB- the use of a word graph is proposed as interface between an alignment-template SMT model and the IMT engine	nn_engine_IMT det_engine_the conj_and_model_engine nn_model_SMT amod_model_alignment-template det_model_an prep_between_interface_engine prep_between_interface_model prep_as_proposed_interface auxpass_proposed_is nsubjpass_proposed_use prep_proposed_In nn_graph_word det_graph_a prep_of_use_graph det_use_the nn_al._et amod_Och_2003 dep_Och_al. dep_In_Och rcmod_``_proposed
D08-1051	P03-1021	o	This tolerant search uses the well known concept of Levenshtein distance in order to obtain the most similar string for the given prefix -LRB- see -LRB- Och et al. 2003 -RRB- for more details -RRB-	amod_details_more dep_Och_2003 dep_Och_al. nn_Och_et prep_for_see_details dep_see_Och amod_prefix_given det_prefix_the amod_string_similar det_string_the advmod_similar_most dep_obtain_see prep_for_obtain_prefix dobj_obtain_string aux_obtain_to dep_obtain_order mark_obtain_in nn_distance_Levenshtein prep_of_concept_distance amod_concept_known det_concept_the advmod_known_well advcl_uses_obtain dobj_uses_concept nsubj_uses_search amod_search_tolerant det_search_This
D08-1060	P03-1021	o	The standard Minimum Error Rate training -LRB- Och 2003 -RRB- was applied to tune the weights for all feature types	nn_types_feature det_types_all prep_for_weights_types det_weights_the dobj_tune_weights aux_tune_to xcomp_applied_tune auxpass_applied_was nsubjpass_applied_training dep_Och_2003 appos_training_Och nn_training_Rate nn_training_Error nn_training_Minimum amod_training_standard det_training_The
D08-1060	P03-1021	o	We use MER -LRB- Och 2003 -RRB- to tune the decoders parameters using a development data set	nn_set_data nn_set_development det_set_a dobj_using_set nn_parameters_decoders det_parameters_the vmod_tune_using dobj_tune_parameters aux_tune_to appos_Och_2003 dep_MER_Och vmod_use_tune dobj_use_MER nsubj_use_We
D08-1065	P03-1021	o	For each language pair we use two development sets one for Minimum Error Rate Training -LRB- Och 2003 Macherey et al. 2008 -RRB- and the other for tuning the scale factor for MBR decoding	nn_decoding_MBR prep_for_factor_decoding nn_factor_scale det_factor_the dep_tuning_factor prep_for_other_tuning det_other_the num_Macherey_2008 nn_Macherey_al. nn_Macherey_et dep_Och_Macherey appos_Och_2003 appos_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum conj_and_one_other prep_for_one_Training dep_sets_other dep_sets_one nn_sets_development num_sets_two dobj_use_sets nsubj_use_we prep_for_use_pair nn_pair_language det_pair_each
D08-1065	P03-1021	o	We then train word alignment models -LRB- Och and Ney 2003 -RRB- using 6 Model-1 iterations and 6 HMM iterations	nn_iterations_HMM num_iterations_6 conj_and_iterations_iterations nn_iterations_Model-1 num_iterations_6 dobj_using_iterations dobj_using_iterations dep_Och_2003 conj_and_Och_Ney appos_models_Ney appos_models_Och nn_models_alignment nn_models_word xcomp_train_using dobj_train_models advmod_train_then nsubj_train_We ccomp_``_train
D08-1066	P03-1021	o	The heuristic estimator employs word-alignment -LRB- Giza + + -RRB- -LRB- Och and Ney 2003 -RRB- and a few thumb rules for defining phrase pairs and then extracts a multi-set of phrase pairs and estimates their conditional probabilities based on the counts in the multi-set	det_multi-set_the prep_in_counts_multi-set det_counts_the prep_on_based_counts vmod_probabilities_based amod_probabilities_conditional poss_probabilities_their dep_estimates_probabilities nn_pairs_phrase conj_and_multi-set_estimates prep_of_multi-set_pairs det_multi-set_a dobj_extracts_estimates dobj_extracts_multi-set advmod_extracts_then nsubj_extracts_estimator nn_pairs_phrase dobj_defining_pairs prepc_for_rules_defining nn_rules_thumb amod_rules_few det_rules_a conj_and_Och_2003 conj_and_Och_Ney cc_Giza_+ cc_Giza_+ dep_word-alignment_2003 dep_word-alignment_Ney dep_word-alignment_Och appos_word-alignment_Giza conj_and_employs_extracts conj_and_employs_rules dobj_employs_word-alignment nsubj_employs_estimator nn_estimator_heuristic det_estimator_The
D08-1066	P03-1021	o	The f are optimized by Minimum-Error Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och appos_Training_MERT nn_Training_Minimum-Error agent_optimized_Training auxpass_optimized_are nsubjpass_optimized_f det_f_The
D08-1066	P03-1021	o	For evaluation we use a state-of-the-art baseline system -LRB- Moses -RRB- -LRB- Hoang and Koehn 2008 -RRB- which works with a log-linear interpolation of feature functions optimized by MERT -LRB- Och 2003 -RRB-	amod_Och_2003 dep_MERT_Och agent_optimized_MERT vmod_functions_optimized nn_functions_feature prep_of_interpolation_functions amod_interpolation_log-linear det_interpolation_a prep_with_works_interpolation nsubj_works_which dep_Hoang_2008 conj_and_Hoang_Koehn rcmod_system_works appos_system_Koehn appos_system_Hoang appos_system_Moses nn_system_baseline amod_system_state-of-the-art det_system_a dobj_use_system nsubj_use_we prep_for_use_evaluation
D08-1066	P03-1021	o	-LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- -RRB-	appos_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_''_Koehn
D08-1066	P03-1021	o	These heuristics define a phrase pair to consist of a source and target ngrams of a word-aligned source-target sentence pair such that if one end of an alignment is in the one ngram the other end is in the other ngram -LRB- and there is at least one such alignment -RRB- -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_alignment_2004 dep_alignment_Ney dep_alignment_Och amod_alignment_such num_alignment_one quantmod_one_at mwe_at_least dobj_is_alignment expl_is_there cc_is_and amod_ngram_other det_ngram_the dep_is_is prep_in_is_ngram nsubj_is_end advcl_is_is mark_is_that dep_is_such amod_end_other det_end_the num_ngram_one det_ngram_the prep_in_is_ngram nsubj_is_end mark_is_if det_alignment_an prep_of_end_alignment num_end_one dep_pair_is nn_pair_sentence amod_pair_source-target amod_pair_word-aligned det_pair_a nn_ngrams_target prep_of_source_pair conj_and_source_ngrams det_source_a prep_of_consist_ngrams prep_of_consist_source aux_consist_to vmod_pair_consist nn_pair_phrase det_pair_a dobj_define_pair nsubj_define_heuristics det_heuristics_These
D08-1076	P03-1021	o	A class of training criteria that provides a tighter connection between the decision rule and the final error metric is known as Minimum Error Rate Training -LRB- MERT -RRB- and has been suggested for SMT in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_SMT_in prep_for_suggested_SMT auxpass_suggested_been aux_suggested_has nsubjpass_suggested_class appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum conj_and_known_suggested prep_as_known_Training auxpass_known_is nsubjpass_known_class nn_metric_error amod_metric_final det_metric_the conj_and_rule_metric nn_rule_decision det_rule_the prep_between_connection_metric prep_between_connection_rule amod_connection_tighter det_connection_a dobj_provides_connection nsubj_provides_that nn_criteria_training rcmod_class_provides prep_of_class_criteria det_class_A
D08-1076	P03-1021	o	6 Related Work As suggested in -LRB- Och 2003 -RRB- an alternative method for the optimization of the unsmoothed error count is Powells algorithm combined with a grid-based line optimization -LRB- Press et al. 2007 p. 509 -RRB-	num_p._509 dep_Press_p. amod_Press_2007 dep_Press_al. nn_Press_et appos_optimization_Press nn_optimization_line amod_optimization_grid-based det_optimization_a prep_with_combined_optimization vmod_algorithm_combined nn_algorithm_Powells cop_algorithm_is nsubj_algorithm_method nn_count_error amod_count_unsmoothed det_count_the prep_of_optimization_count det_optimization_the prep_for_method_optimization amod_method_alternative det_method_an rcmod_Och_algorithm dep_Och_2003 prep_in_suggested_Och mark_suggested_As advcl_Work_suggested amod_Work_Related num_Work_6
D08-1076	P03-1021	o	Assuming that the corpusbased error count for some translations eS1 is additively decomposable into the error counts of the individual sentences i.e. ED4rS1 eS1D5 AG EWSs AG1 ED4rs esD5 the MERT criterion is given as M1 AG argmin M1 AZ S F4 sAG1 EA0rs eD4fs M1 D5A8 B7 -LRB- 3 -RRB- AG argmin M1 AZ S F4 sAG1 K F4 kAG1 ED4rs es kD5A0eD4fs M1 D5 es kA8 B7 with e D4fs M1 D5 AG argmaxe AZ M F4 mAG1 mhmD4e fsD5 B7 -LRB- 4 -RRB- In -LRB- Och 2003 -RRB- it was shown that linear models can effectively be trained under the MERT criterion using a special line optimization algorithm	nn_algorithm_optimization nn_algorithm_line amod_algorithm_special det_algorithm_a dobj_using_algorithm nn_criterion_MERT det_criterion_the xcomp_trained_using prep_under_trained_criterion auxpass_trained_be advmod_trained_effectively aux_trained_can nsubjpass_trained_models mark_trained_that amod_models_linear ccomp_shown_trained auxpass_shown_was nsubjpass_shown_it appos_Och_2003 prep_in_B7_Och appos_B7_4 nn_B7_fsD5 nn_mhmD4e_mAG1 nn_mhmD4e_F4 nn_mhmD4e_M nn_mhmD4e_AZ nn_mhmD4e_argmaxe nn_mhmD4e_AG nn_mhmD4e_D5 nn_mhmD4e_M1 dep_D4fs_e prep_with_B7_D4fs nn_B7_kA8 rcmod_D5_shown conj_D5_B7 conj_D5_mhmD4e conj_D5_B7 conj_D5_es nn_D5_M1 conj_ED4rs_D5 conj_ED4rs_kD5A0eD4fs conj_ED4rs_es nn_ED4rs_kAG1 nn_ED4rs_F4 nn_ED4rs_K nn_ED4rs_sAG1 nn_ED4rs_F4 nn_ED4rs_S nn_ED4rs_AZ nn_ED4rs_M1 nn_ED4rs_argmin nn_ED4rs_AG dep_ED4rs_3 nn_ED4rs_B7 nn_B7_D5A8 nn_B7_M1 dep_EA0rs_ED4rs appos_EA0rs_eD4fs nn_EA0rs_sAG1 nn_EA0rs_F4 nn_EA0rs_S nn_EA0rs_AZ nn_EA0rs_M1 nn_EA0rs_argmin nn_EA0rs_AG nn_EA0rs_M1 dep_as_EA0rs prep_given_as auxpass_given_is nsubjpass_given_ED4rS1 cc_given_i.e. nn_criterion_MERT det_criterion_the nn_ED4rs_AG1 nn_ED4rs_EWSs nn_ED4rs_AG nn_ED4rs_eS1D5 conj_ED4rS1_criterion conj_ED4rS1_esD5 conj_ED4rS1_ED4rs amod_sentences_individual det_sentences_the parataxis_counts_given prep_of_counts_sentences nsubj_counts_count mark_counts_that det_error_the prep_into_decomposable_error advmod_decomposable_additively cop_decomposable_is nsubj_decomposable_eS1 rcmod_translations_decomposable det_translations_some prep_for_count_translations nn_count_error amod_count_corpusbased det_count_the ccomp_Assuming_counts ccomp_``_Assuming
D08-1076	P03-1021	o	Starting from an initial point M1 computing the most probable sentence hypothesis out of a set of K candidate translations Cs AG D8e1 eKD9 along the line M1 A0 A4 dM1 results in the following optimization problem -LRB- Och 2003 -RRB- e D4fs D5 AG argmax eC8Cs AX D4 M 1 A0 A4 d M 1 D5 C2 A4 hM1 D4e fsD5 B5 AG argmax eC8Cs AY F4 m mhmD4e fsD5 D0D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D2 AGaD4e fsD5 A0 A4 F4 m dmhmD4e fsD5 D0D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D2 AGbD4e fsD5 B6 AG argmax eC8Cs AWa D4e fsD5 A0 A4 bD4e fsD5 D0D3D3D3D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D3D3D3D2 D4A6D5 B4 -LRB- 5 -RRB- Hence the total score D4A6D5 for any candidate translation corresponds to a line in the plane with as the independent variable	amod_variable_independent det_variable_the pobj_as_variable pcomp_with_as det_plane_the prep_line_with prep_in_line_plane det_line_a prep_to_corresponds_line nsubj_corresponds_D4e nn_translation_candidate det_translation_any prep_for_D4A6D5_translation nn_D4A6D5_score amod_D4A6D5_total det_D4A6D5_the advmod_B4_Hence appos_B4_5 nn_B4_D4A6D5 nn_B4_D0D3D3D3D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D3D3D3D2 nn_B4_fsD5 nn_bD4e_A4 nn_bD4e_A0 nn_bD4e_fsD5 nn_D4e_AWa nn_D4e_eC8Cs nn_D4e_argmax nn_D4e_AG nn_D4e_B6 nn_D4e_fsD5 nn_AGbD4e_D0D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D2 nn_AGbD4e_fsD5 nn_dmhmD4e_m nn_dmhmD4e_F4 nn_dmhmD4e_A4 nn_dmhmD4e_A0 nn_dmhmD4e_fsD5 nn_AGaD4e_D0D3D3D3D3D3D3D3D3D1D3D3D3D3D3D3D3D3D2 nn_AGaD4e_fsD5 nn_mhmD4e_m nn_mhmD4e_F4 nn_mhmD4e_AY nn_mhmD4e_eC8Cs nn_mhmD4e_argmax nn_mhmD4e_AG nn_mhmD4e_B5 nn_mhmD4e_fsD5 appos_D4e_D4A6D5 conj_D4e_B4 conj_D4e_bD4e conj_D4e_D4e conj_D4e_AGbD4e conj_D4e_dmhmD4e conj_D4e_AGaD4e conj_D4e_mhmD4e nn_D4e_hM1 nn_D4e_A4 nn_D4e_C2 nn_D4e_D5 num_D4e_1 nn_D4e_M nn_D4e_d nn_D4e_A4 nn_D4e_A0 num_D4e_1 nn_D4e_M nn_D4e_D4 nn_D4e_AX nn_D4e_eC8Cs nn_D4e_argmax nn_D4e_AG nn_D4e_D5 parataxis_D4fs_corresponds dep_D4fs_e appos_Och_2003 dep_problem_Och nn_problem_optimization amod_problem_following det_problem_the dep_results_D4fs prep_in_results_problem nsubj_results_eKD9 vmod_results_computing vmod_results_Starting nn_dM1_A4 nn_dM1_A0 nn_dM1_M1 nn_dM1_line det_dM1_the prep_along_eKD9_dM1 nn_D8e1_AG nn_D8e1_Cs nn_D8e1_translations nn_D8e1_candidate nn_D8e1_K prep_of_set_D8e1 det_set_a pobj_hypothesis_set prepc_out_of_hypothesis_of nn_hypothesis_sentence amod_hypothesis_probable det_hypothesis_the advmod_probable_most dobj_computing_hypothesis nn_M1_point amod_M1_initial det_M1_an prep_from_Starting_M1
D08-1076	P03-1021	o	The upper envelope is a convex hull and can be inscribed with a convex polygon whose edges are the segments of a piecewise linear function in -LRB- Papineni 1999 Och 2003 -RRB- EnvD4fD5 AG max eC8C AWa D4e fD5 A0 A4 bD4e fD5 C8 RB4 -LRB- 6 -RRB- 726 Score Error count 0 0 e1 e2 e5 e6 e8 e1e 2 e3 e4 e5e6e 7 e8 Figure 1 The upper envelope -LRB- bold red curve -RRB- for a set of lines is the convex hull which consists of the topmost line segments	nn_segments_line amod_segments_topmost det_segments_the prep_of_consists_segments nsubj_consists_which rcmod_hull_consists nn_hull_convex det_hull_the cop_hull_is prep_of_set_lines det_set_a amod_curve_red amod_curve_bold prep_for_envelope_set appos_envelope_curve amod_envelope_upper det_envelope_The num_Figure_1 nn_Figure_e8 num_Figure_7 nn_Figure_e5e6e nn_Figure_e4 nn_Figure_e3 num_Figure_2 nn_Figure_e1e nn_Figure_e8 nn_Figure_e6 nn_Figure_e5 nn_Figure_e2 nn_Figure_e1 num_Figure_0 number_0_0 dep_count_hull dep_count_envelope dep_count_Figure nn_count_Error nn_count_Score num_count_726 num_count_6 dep_RB4_count nn_RB4_C8 nn_bD4e_A4 nn_bD4e_A0 nn_bD4e_fD5 dep_D4e_RB4 appos_D4e_fD5 appos_D4e_bD4e nn_D4e_AWa nn_D4e_eC8C nn_D4e_max nn_D4e_AG nn_D4e_EnvD4fD5 dep_Och_2003 dep_Papineni_Och appos_Papineni_1999 dep_in_Papineni prep_function_in amod_function_linear amod_function_piecewise det_function_a prep_of_segments_function det_segments_the cop_segments_are nsubj_segments_edges poss_edges_whose rcmod_polygon_segments nn_polygon_convex det_polygon_a prep_with_inscribed_polygon auxpass_inscribed_be aux_inscribed_can nsubjpass_inscribed_envelope dep_hull_D4e conj_and_hull_inscribed nn_hull_convex det_hull_a cop_hull_is nsubj_hull_envelope amod_envelope_upper det_envelope_The
D08-1088	P03-1021	o	This operation can be used in applications like Minimum Error Rate Training -LRB- Och 2003 -RRB- or optimizing system combination as described by Hillard et al.	nn_al._et nn_al._Hillard prep_by_described_al. mark_described_as nn_combination_system amod_combination_optimizing dep_Och_2003 conj_or_Training_combination appos_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum prep_like_applications_combination prep_like_applications_Training advcl_used_described prep_in_used_applications auxpass_used_be aux_used_can nsubjpass_used_operation det_operation_This
D08-1089	P03-1021	o	Parameters were tuned with minimum error-rate training -LRB- Och 2003 -RRB- on the NIST evaluation set of 2006 -LRB- MT06 -RRB- for both C-E and A-E	conj_and_C-E_A-E preconj_C-E_both appos_2006_MT06 prep_of_set_2006 nn_set_evaluation nn_set_NIST det_set_the appos_Och_2003 prep_on_training_set dep_training_Och amod_training_error-rate amod_training_minimum prep_for_tuned_A-E prep_for_tuned_C-E prep_with_tuned_training auxpass_tuned_were nsubjpass_tuned_Parameters
D08-1089	P03-1021	o	1 Introduction Statistical phrase-based systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- have consistently delivered state-of-the-art performance in recent machine translation evaluations yet these systems remain weak at handling word order changes	nn_changes_order nn_changes_word dobj_handling_changes prepc_at_remain_handling acomp_remain_weak nsubj_remain_systems advmod_remain_yet det_systems_these nn_evaluations_translation nn_evaluations_machine amod_evaluations_recent amod_performance_state-of-the-art ccomp_delivered_remain prep_in_delivered_evaluations dobj_delivered_performance advmod_delivered_consistently aux_delivered_have nsubj_delivered_systems num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney appos_systems_2004 appos_systems_Ney appos_systems_Och amod_systems_phrase-based amod_systems_Statistical nn_systems_Introduction num_systems_1 ccomp_``_delivered
D08-1093	P03-1021	o	Moreover rather than predicting an intrinsic metric such as the PARSEVAL Fscore the metric that the predictor learns to predict can be chosen to better fit the final metric on which an end-to-end system is measured in the style of -LRB- Och 2003 -RRB-	amod_Och_2003 dep_of_Och prep_style_of det_style_the auxpass_measured_is nsubjpass_measured_system prep_on_measured_which amod_system_end-to-end det_system_an rcmod_metric_measured amod_metric_final det_metric_the dobj_fit_metric advmod_fit_better aux_fit_to prep_in_chosen_style xcomp_chosen_fit auxpass_chosen_be aux_chosen_can prepc_than_chosen_predicting advmod_chosen_rather advmod_chosen_Moreover aux_predict_to xcomp_learns_predict nsubj_learns_predictor mark_learns_that det_predictor_the ccomp_metric_learns det_metric_the amod_Fscore_metric nn_Fscore_PARSEVAL det_Fscore_the prep_such_as_metric_Fscore amod_metric_intrinsic det_metric_an dobj_predicting_metric
D09-1005	P03-1021	o	7.2 Minimum-Risk Training Adjusting or changes the distribution p. Minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- tries to tune to minimize the BLEU loss of a decoder that chooses the most probable output according to p.	amod_output_probable det_output_the advmod_probable_most pobj_chooses_p. prepc_according_to_chooses_to dobj_chooses_output nsubj_chooses_that rcmod_decoder_chooses det_decoder_a prep_of_loss_decoder nn_loss_BLEU det_loss_the dobj_minimize_loss aux_minimize_to xcomp_tune_minimize aux_tune_to xcomp_tries_tune nsubj_tries_training dep_Och_2003 appos_training_Och appos_training_MERT nn_training_rate nn_training_error nn_training_Minimum nn_training_p. nn_training_distribution det_training_the nsubj_changes_Training dep_Adjusting_tries conj_or_Adjusting_changes nsubj_Adjusting_Training nn_Training_Minimum-Risk num_Training_7.2
D09-1006	P03-1021	o	Och -LRB- 2003 -RRB- shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged	auxpass_judged_be advmod_judged_eventually aux_judged_will nsubjpass_judged_system agent_judged_which nn_system_MT det_system_the rcmod_metric_judged amod_evaluation_metric det_evaluation_the dobj_take_evaluation prep_into_take_account aux_take_should csubj_take_setting mark_take_that det_weights_those dobj_setting_weights ccomp_shows_take nsubj_shows_Och appos_Och_2003
D09-1006	P03-1021	o	-LRB- 1 -RRB- Och -LRB- 2003 -RRB- provides evidence that should be chosen by optimizing an objective function basd on the evaluation metric of interest rather than likelihood	prep_of_metric_interest nn_metric_evaluation det_metric_the nn_basd_function amod_basd_objective det_basd_an prep_than_optimizing_likelihood advmod_optimizing_rather prep_on_optimizing_metric dobj_optimizing_basd agent_chosen_optimizing auxpass_chosen_be aux_chosen_should nsubjpass_chosen_that ccomp_evidence_chosen dobj_provides_evidence nsubj_provides_Och dep_provides_1 appos_Och_2003
D09-1006	P03-1021	o	1 Introduction Many state-of-the-art machine translation -LRB- MT -RRB- systems over the past few years -LRB- Och and Ney 2002 Koehn et al. 2003 Chiang 2007 Koehn et al. 2007 Li et al. 2009 -RRB- rely on several models to evaluate the goodness of a given candidate translation in the target language	nn_language_target det_language_the nn_translation_candidate amod_translation_given det_translation_a prep_of_goodness_translation det_goodness_the prep_in_evaluate_language dobj_evaluate_goodness aux_evaluate_to amod_models_several xcomp_rely_evaluate prep_on_rely_models nsubj_rely_systems num_Li_2009 nn_Li_al. nn_Li_et num_Koehn_2007 nn_Koehn_al. nn_Koehn_et num_Chiang_2007 num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Li conj_and_Och_Koehn conj_and_Och_Chiang conj_and_Och_Koehn conj_and_Och_2002 conj_and_Och_Ney amod_years_few amod_years_past det_years_the appos_systems_Koehn appos_systems_Chiang appos_systems_Koehn appos_systems_2002 appos_systems_Ney appos_systems_Och prep_over_systems_years nn_systems_translation appos_translation_MT nn_translation_machine amod_translation_state-of-the-art amod_translation_Many nn_translation_Introduction num_translation_1
D09-1021	P03-1021	o	10Both Pharoah and our system have weights trained using MERT -LRB- Och 2003 -RRB- on sentences of length 30 words or less to ensure that training and test conditions are matched	auxpass_matched_are nsubjpass_matched_conditions mark_matched_that nn_conditions_test nn_conditions_training conj_and_training_test ccomp_ensure_matched aux_ensure_to conj_or_words_less num_words_30 nn_words_length prep_of_sentences_less prep_of_sentences_words dep_Och_2003 dep_MERT_Och prep_on_using_sentences dobj_using_MERT xcomp_trained_using vmod_weights_trained vmod_have_ensure dobj_have_weights poss_system_our dep_Pharoah_have conj_and_Pharoah_system nn_Pharoah_10Both
D09-1021	P03-1021	o	However the approach raises two major challenges 7In practice MERT training -LRB- Och 2003 -RRB- will be used to train relative weights for the different model components	nn_components_model amod_components_different det_components_the amod_weights_relative prep_for_train_components dobj_train_weights aux_train_to xcomp_used_train auxpass_used_be aux_used_will nsubjpass_used_practice dep_Och_2003 appos_training_Och nn_training_MERT conj_practice_training nn_practice_7In dep_challenges_used amod_challenges_major num_challenges_two dobj_raises_challenges nsubj_raises_approach advmod_raises_However det_approach_the
D09-1023	P03-1021	o	Our approach permits an alternative to minimum error-rate training -LRB- MERT Och 2003 -RRB- it is discriminativebuthandleslatentstructureandregularization in more principled ways	amod_ways_principled advmod_principled_more prep_in_discriminativebuthandleslatentstructureandregularization_ways cop_discriminativebuthandleslatentstructureandregularization_is nsubj_discriminativebuthandleslatentstructureandregularization_it amod_Och_2003 dep_MERT_Och appos_training_MERT amod_training_error-rate amod_training_minimum prep_to_alternative_training det_alternative_an parataxis_permits_discriminativebuthandleslatentstructureandregularization dobj_permits_alternative nsubj_permits_approach poss_approach_Our ccomp_``_permits
D09-1023	P03-1021	o	We perform word alignment using GIZA + + -LRB- Och and Ney 2003 -RRB- symmetrize the alignments using the grow-diag-final-and heuristic and extract phrases up to length 3	num_length_3 pobj_to_length pcomp_up_to prep_phrases_up nn_phrases_extract amod_heuristic_grow-diag-final-and det_heuristic_the dobj_using_heuristic vmod_alignments_using det_alignments_the dobj_symmetrize_alignments num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_phrases conj_+_GIZA_symmetrize conj_+_GIZA_+ dobj_using_phrases dobj_using_symmetrize dobj_using_+ dobj_using_GIZA vmod_alignment_using nn_alignment_word dobj_perform_alignment nsubj_perform_We
D09-1023	P03-1021	o	The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA + + mkcls utility -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_utility_Ney dep_utility_Och nn_utility_mkcls pobj_+_utility conj_+_GIZA_+ det_GIZA_the dobj_using_+ dobj_using_GIZA amod_corpus_parallel det_corpus_the xcomp_derived_using prep_from_derived_corpus vmod_classes_derived nn_classes_word amod_classes_hard num_classes_50 dobj_using_classes xcomp_included_using advmod_included_also auxpass_included_are nsubjpass_included_probabilities amod_probabilities_same det_probabilities_The
D09-1037	P03-1021	o	The rules are then treated as events in a relative frequency estimate .4 We used Giza + + Model 4 to obtain word alignments -LRB- Och and Ney 2003 -RRB- using the grow-diag-final-and heuristic to symmetrise the two directional predictions -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_predictions_Koehn amod_predictions_directional num_predictions_two det_predictions_the dobj_symmetrise_predictions aux_symmetrise_to amod_heuristic_grow-diag-final-and det_heuristic_the vmod_using_symmetrise dobj_using_heuristic dep_Och_2003 conj_and_Och_Ney dep_alignments_Ney dep_alignments_Och nn_alignments_word dobj_obtain_alignments aux_obtain_to num_Giza_4 conj_+_Giza_Model vmod_used_obtain dobj_used_Model dobj_used_Giza nsubj_used_We rcmod_.4_used dep_estimate_.4 nn_estimate_frequency amod_estimate_relative det_estimate_a xcomp_treated_using prep_in_treated_estimate prep_as_treated_events advmod_treated_then auxpass_treated_are nsubjpass_treated_rules det_rules_The ccomp_``_treated
D09-1037	P03-1021	o	No artificial glue-rules or rule span limits were employed .7 The parameters of the translation system were trained to maximize BLEU on the MT02 test set -LRB- Och 2003 -RRB-	amod_Och_2003 dep_set_Och nn_set_test nn_set_MT02 det_set_the prep_on_maximize_set dobj_maximize_BLEU aux_maximize_to xcomp_trained_maximize auxpass_trained_were csubjpass_trained_employed nn_system_translation det_system_the prep_of_parameters_system det_parameters_The num_parameters_.7 dobj_employed_parameters auxpass_employed_were nsubjpass_employed_limits nsubjpass_employed_glue-rules nn_limits_span nn_limits_rule conj_or_glue-rules_limits amod_glue-rules_artificial neg_glue-rules_No
D09-1039	P03-1021	o	There has been some previous work on accuracy-driven training techniques for SMT such as MERT -LRB- Och 2003 -RRB- and the Simplex Armijo Downhill method -LRB- Zhao and Chen 2009 -RRB- which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set	nn_set_tuning amod_set_held-out det_set_a nn_scores_phrase amod_scores_various prep_of_combination_scores amod_combination_linear det_combination_a det_parameters_the pobj_tune_set prepc_according_to_tune_to prep_in_tune_combination dobj_tune_parameters nsubj_tune_which num_Zhao_2009 conj_and_Zhao_Chen appos_method_Chen appos_method_Zhao nn_method_Downhill nn_method_Armijo nn_method_Simplex det_method_the amod_Och_2003 conj_and_MERT_method dep_MERT_Och rcmod_SMT_tune prep_such_as_SMT_method prep_such_as_SMT_MERT prep_for_techniques_SMT nn_techniques_training amod_techniques_accuracy-driven prep_on_work_techniques amod_work_previous det_work_some cop_work_been aux_work_has expl_work_There
D09-1040	P03-1021	o	Feature weights were set with minimum error rate training -LRB- Och 2003 -RRB- on a development set using BLEU -LRB- Papineni et al. 2002 -RRB- as the objective function	amod_function_objective det_function_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_as_using_function dobj_using_BLEU xcomp_set_using vmod_development_set det_development_a appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_on_set_development prep_with_set_training auxpass_set_were nsubjpass_set_weights nn_weights_Feature
D09-1042	P03-1021	o	Furthermore WASP1 + + employs minimum error rate training -LRB- Och 2003 -RRB- to directly optimize the evaluation metrics	nn_metrics_evaluation det_metrics_the dobj_optimize_metrics advmod_optimize_directly aux_optimize_to dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_employs_optimize dobj_employs_training nsubj_employs_+ nsubj_employs_WASP1 advmod_employs_Furthermore conj_+_WASP1_+
D09-1073	P03-1021	o	For the MER training -LRB- Och 2003 -RRB- we modify Koehns MER trainer -LRB- Koehn 2004 -RRB- to train our system	poss_system_our dobj_train_system aux_train_to amod_Koehn_2004 appos_trainer_Koehn nn_trainer_MER nn_trainer_Koehns vmod_modify_train dobj_modify_trainer nsubj_modify_we prep_for_modify_training appos_Och_2003 dep_training_Och nn_training_MER det_training_the
D09-1073	P03-1021	o	Recently many phrase reordering methods have been proposed ranging from simple distancebased distortion model -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- flat reordering model -LRB- Wu 1997 Zens et al. 2004 -RRB- lexicalized reordering model -LRB- Tillmann 2004 Kumar and Byrne 2005 -RRB- to hierarchical phrase-based model -LRB- Chiang 2005 Setiawan et al. 2007 -RRB- and classifier-based reordering model with linear features -LRB- Zens and Ney 2006 Xiong et al. 2006 Zhang et al. 2007a Xiong et al. 2008 -RRB-	num_Xiong_2008 nn_Xiong_al. nn_Xiong_et appos_Zhang_2007a dep_Zhang_al. nn_Zhang_et num_Xiong_2006 nn_Xiong_al. nn_Xiong_et dep_Zens_Xiong conj_and_Zens_Zhang conj_and_Zens_Xiong conj_and_Zens_2006 conj_and_Zens_Ney appos_features_Zhang appos_features_Xiong appos_features_2006 appos_features_Ney appos_features_Zens amod_features_linear prep_with_model_features nn_model_reordering amod_model_classifier-based num_Setiawan_2007 nn_Setiawan_al. nn_Setiawan_et dep_Chiang_Setiawan appos_Chiang_2005 dep_model_Chiang amod_model_phrase-based amod_model_hierarchical dep_Kumar_2005 conj_and_Kumar_Byrne dep_Tillmann_Byrne dep_Tillmann_Kumar appos_Tillmann_2004 appos_model_Tillmann nn_model_reordering amod_model_lexicalized nn_al._et nn_al._Zens amod_Wu_2004 dep_Wu_al. appos_Wu_1997 appos_model_Wu nn_model_reordering amod_model_flat dep_Och_2004 conj_and_Och_Ney conj_and_Koehn_model prep_to_Koehn_model conj_and_Koehn_model dep_Koehn_model dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_model_distortion amod_model_distancebased amod_model_simple prep_from_ranging_model dep_proposed_model dep_proposed_model dep_proposed_Koehn xcomp_proposed_ranging auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods advmod_proposed_Recently nn_methods_reordering nn_methods_phrase amod_methods_many
D09-1073	P03-1021	p	1 Introduction Phrase-based method -LRB- Koehn et al. 2003 Och and Ney 2004 Koehn et al. 2007 -RRB- and syntaxbased method -LRB- Wu 1997 Yamada and Knight 2001 Eisner 2003 Chiang 2005 Cowan et al. 2006 Marcu et al. 2006 Liu et al. 2007 Zhang et al. 2007c 2008a 2008b Shen et al. 2008 Mi and Huang 2008 -RRB- represent the state-of-the-art technologies in statistical machine translation -LRB- SMT -RRB-	appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_technologies_translation amod_technologies_state-of-the-art det_technologies_the dobj_represent_technologies nsubj_represent_method nsubj_represent_Koehn nsubj_represent_2004 nsubj_represent_Ney nsubj_represent_Och dep_Mi_2008 conj_and_Mi_Huang nn_al._et nn_al._Shen appos_Zhang_2008b appos_Zhang_2008a appos_Zhang_2007c dep_Zhang_al. nn_Zhang_et num_Liu_2007 nn_Liu_al. nn_Liu_et dep_al._2006 nn_al._et nn_al._Marcu num_Cowan_2006 nn_Cowan_al. nn_Cowan_et num_Chiang_2005 num_Eisner_2003 num_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Huang dep_Wu_Mi num_Wu_2008 dep_Wu_al. conj_Wu_Zhang conj_Wu_Liu conj_Wu_al. conj_Wu_Cowan conj_Wu_Chiang conj_Wu_Eisner conj_Wu_Knight conj_Wu_Yamada appos_Wu_1997 appos_method_Wu amod_method_syntaxbased num_Koehn_2007 nn_Koehn_al. nn_Koehn_et conj_and_Och_method conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney parataxis_Koehn_represent appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn amod_method_Phrase-based nn_method_Introduction num_method_1
D09-1075	P03-1021	o	Default parameters were used for all experiments except for the numberofiterationsforGIZA + + -LRB- OchandNey 2003 -RRB-	amod_OchandNey_2003 dep_numberofiterationsforGIZA_OchandNey conj_+_numberofiterationsforGIZA_+ det_numberofiterationsforGIZA_the det_experiments_all pobj_used_+ pobj_used_numberofiterationsforGIZA prepc_except_for_used_for prep_for_used_experiments auxpass_used_were nsubjpass_used_parameters nn_parameters_Default
D09-1075	P03-1021	o	For practical reasons the maximum size of a token was set at three for Chinese andfour forKorean .2 Minimum error rate training -LRB- Och 2003 -RRB- was run on each system afterwardsand BLEU score -LRB- Papineni et al. 2002 -RRB- was calculated on the test sets	nn_sets_test det_sets_the prep_on_calculated_sets auxpass_calculated_was csubjpass_calculated_set amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU amod_score_afterwardsand nn_score_system det_score_each prep_on_run_score auxpass_run_was nsubjpass_run_Chinese mark_run_for dep_Och_2003 appos_training_Och nn_training_rate nn_training_error nn_training_Minimum nn_training_.2 nn_training_forKorean amod_training_andfour conj_Chinese_training appos_three_Papineni rcmod_three_run prep_at_set_three auxpass_set_was nsubjpass_set_size prep_for_set_reasons det_token_a prep_of_size_token nn_size_maximum det_size_the amod_reasons_practical
D09-1076	P03-1021	o	We train our feature weights using max-BLEU -LRB- Och 2003 -RRB- and decode with a CKY-based decoder that supports language model scoring directly integrated into the search	det_search_the prep_into_integrated_search advmod_integrated_directly dep_scoring_integrated vmod_model_scoring nn_model_language dobj_supports_model nsubj_supports_that rcmod_decoder_supports amod_decoder_CKY-based det_decoder_a prep_with_decode_decoder amod_Och_2003 appos_max-BLEU_Och conj_and_using_decode dobj_using_max-BLEU nn_weights_feature poss_weights_our dep_train_decode dep_train_using dobj_train_weights nsubj_train_We
D09-1079	P03-1021	o	We held out 300 sentences for minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- and optimised the parameters of the feature functions of the decoder for each experimental run	amod_run_experimental det_run_each det_decoder_the prep_of_functions_decoder nn_functions_feature det_functions_the prep_of_parameters_functions det_parameters_the prep_for_optimised_run dobj_optimised_parameters nsubj_optimised_We amod_Och_2003 dep_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum prep_for_sentences_training num_sentences_300 conj_and_held_optimised dobj_held_sentences prt_held_out nsubj_held_We
D09-1105	P03-1021	o	We used GIZA + + -LRB- Och and Ney 2003 -RRB- to align approximately 751,000 sentences from the German-English portion of the Europarl corpus -LRB- Koehn 2005 -RRB- in both the German-to-English and English-to-German directions	amod_directions_English-to-German amod_directions_German-to-English det_directions_the preconj_directions_both conj_and_German-to-English_English-to-German dep_Koehn_2005 appos_corpus_Koehn nn_corpus_Europarl det_corpus_the prep_of_portion_corpus amod_portion_German-English det_portion_the num_sentences_751,000 quantmod_751,000_approximately prep_from_align_portion dobj_align_sentences aux_align_to num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ prep_in_used_directions vmod_used_align dobj_used_+ dobj_used_GIZA nsubj_used_We
D09-1105	P03-1021	o	Moses used the development data for minimum error-rate training -LRB- Och 2003 -RRB- of its small number of parameters	prep_of_number_parameters amod_number_small poss_number_its dep_Och_2003 prep_of_training_number appos_training_Och amod_training_error-rate amod_training_minimum nn_data_development det_data_the prep_for_used_training dobj_used_data nsubj_used_Moses
D09-1108	P03-1021	o	We use GIZA + + -LRB- Och and Ney 2003 -RRB- to do m-to-n word-alignment and adopt heuristic grow-diag-final-and to do refinement	dobj_do_refinement aux_do_to nn_grow-diag-final-and_heuristic vmod_adopt_do dobj_adopt_grow-diag-final-and amod_word-alignment_m-to-n conj_and_do_adopt dobj_do_word-alignment aux_do_to num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ vmod_use_adopt vmod_use_do dobj_use_+ dobj_use_GIZA nsubj_use_We
D09-1108	P03-1021	o	The feature weights are tuned by the modified Koehns MER -LRB- Och 2003 Koehn 2007 -RRB- trainer	dep_trainer_Och dep_Och_2007 appos_Och_Koehn amod_Och_2003 dep_MER_trainer nn_MER_Koehns amod_MER_modified det_MER_the agent_tuned_MER auxpass_tuned_are nsubjpass_tuned_weights nn_weights_feature det_weights_The ccomp_``_tuned
D09-1111	P03-1021	o	Their transliteration probability is P -LRB- t | s -RRB- PE -LRB- s | t -RRB- max -LSB- PT -LRB- t -RRB- PL -LRB- t -RRB- -RSB- -LRB- 1 -RRB- Inspired by the linear models used in SMT -LRB- Och 2003 -RRB- we can discriminatively weight the components of this generative model producing wE logPE -LRB- s | t -RRB- + wT logPT -LRB- t -RRB- + wL logPL -LRB- t -RRB- with weights w learned by perceptron training	nn_training_perceptron prep_by_learned_training dep_w_learned appos_logPL_t nn_logPL_wL appos_logPT_t nn_logPT_wT dobj_|_t nsubj_|_s dep_logPE_w prep_with_logPE_weights conj_+_logPE_logPL conj_+_logPE_logPT dep_logPE_| nn_logPE_wE dobj_producing_logPL dobj_producing_logPT dobj_producing_logPE amod_model_generative det_model_this prep_of_components_model det_components_the vmod_weight_producing dobj_weight_components advmod_weight_discriminatively aux_weight_can nsubj_weight_we amod_Och_2003 appos_SMT_Och prep_in_used_SMT vmod_models_used amod_models_linear det_models_the ccomp_Inspired_weight agent_Inspired_models dep_Inspired_1 dep_Inspired_max appos_PL_t appos_PT_PL appos_PT_t dep_max_PT dobj_|_t nsubj_|_s vmod_PE_Inspired dep_PE_| nn_PE_P num_s_| nn_s_t appos_P_s dep_is_PE nsubj_is_probability nn_probability_transliteration poss_probability_Their ccomp_``_is
D09-1111	P03-1021	p	However this is not unprecedented discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks -LRB- Raina et al. 2004 Toutanova 2006 -RRB- and remain the standard approach in statistical translation modeling -LRB- Och 2003 -RRB-	amod_Och_2003 dep_modeling_Och nn_modeling_translation amod_modeling_statistical prep_in_approach_modeling amod_approach_standard det_approach_the xcomp_remain_approach nsubj_remain_models dep_Toutanova_2006 dep_Raina_Toutanova appos_Raina_2004 dep_Raina_al. nn_Raina_et nn_tasks_classification nn_tasks_NLP amod_tasks_various amod_competitors_discriminative advmod_discriminative_purely dep_outperform_Raina prep_in_outperform_tasks dobj_outperform_competitors aux_outperform_to conj_and_shown_remain xcomp_shown_outperform auxpass_shown_been aux_shown_have nsubjpass_shown_models amod_models_generative amod_models_weighted advmod_weighted_discriminatively parataxis_unprecedented_remain parataxis_unprecedented_shown neg_unprecedented_not cop_unprecedented_is nsubj_unprecedented_this advmod_unprecedented_However ccomp_``_unprecedented
D09-1111	P03-1021	o	Note that generative hybrids are the norm in SMT where translation scores are provided by a discriminative combination of generative models -LRB- Och 2003 -RRB-	amod_Och_2003 dep_models_Och amod_models_generative prep_of_combination_models amod_combination_discriminative det_combination_a agent_provided_combination auxpass_provided_are nsubjpass_provided_scores advmod_provided_where nn_scores_translation rcmod_SMT_provided prep_in_norm_SMT det_norm_the cop_norm_are nsubj_norm_hybrids mark_norm_that amod_hybrids_generative ccomp_Note_norm
D09-1114	P03-1021	o	Since we also adopt a linear scoring function in Equation -LRB- 3 -RRB- the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training -LRB- MERT -RRB- algorithm -LRB- Och 2003 -RRB-	num_Och_2003 appos_algorithm_Och nn_algorithm_Training appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum amod_Training_standard det_Training_the dobj_using_algorithm nn_metrics_evaluation amod_metrics_specified det_metrics_the dobj_optimize_metrics aux_optimize_to vmod_set_optimize nn_set_data nn_set_development det_set_a xcomp_tuned_using prep_on_tuned_set auxpass_tuned_be advmod_tuned_also aux_tuned_can nsubjpass_tuned_weights advcl_tuned_adopt nn_model_combination poss_model_our prep_of_weights_model nn_weights_feature det_weights_the appos_Equation_3 amod_function_scoring amod_function_linear det_function_a prep_in_adopt_Equation dobj_adopt_function advmod_adopt_also nsubj_adopt_we mark_adopt_Since ccomp_``_tuned
D09-1114	P03-1021	o	Parameters were tuned with MERT algorithm -LRB- Och 2003 -RRB- on the NIST evaluation set of 2003 -LRB- MT03 -RRB- for both the baseline systems and the system combination model	nn_model_combination nn_model_system det_model_the conj_and_systems_model nn_systems_baseline det_systems_the preconj_systems_both appos_2003_MT03 prep_for_set_model prep_for_set_systems prep_of_set_2003 nn_set_evaluation nn_set_NIST det_set_the appos_Och_2003 dep_algorithm_Och nn_algorithm_MERT prep_on_tuned_set prep_with_tuned_algorithm auxpass_tuned_were nsubjpass_tuned_Parameters
D09-1114	P03-1021	o	GIZA + + toolkit -LRB- Och and Ney 2003 -RRB- is used to perform word alignment in both directions with default settings and the intersect-diag-grow method is used to generate symmetric word alignment refinement	nn_refinement_alignment nn_refinement_word amod_refinement_symmetric dobj_generate_refinement aux_generate_to xcomp_used_generate auxpass_used_is nsubjpass_used_method nn_method_intersect-diag-grow det_method_the nn_settings_default prep_with_directions_settings preconj_directions_both nn_alignment_word prep_in_perform_directions dobj_perform_alignment aux_perform_to conj_and_used_used xcomp_used_perform auxpass_used_is nsubjpass_used_+ nsubjpass_used_GIZA dep_Och_2003 conj_and_Och_Ney appos_toolkit_Ney appos_toolkit_Och pobj_+_toolkit conj_+_GIZA_+
D09-1117	P03-1021	o	The system was trained in a standard manner using a minimum error-rate training -LRB- MERT -RRB- procedure -LRB- Och 2003 -RRB- with respect to the BLEU score -LRB- Papineni et al. 2001 -RRB- on held-out development data to optimize the loglinear model weights	nn_weights_model amod_weights_loglinear det_weights_the dobj_optimize_weights aux_optimize_to nn_data_development amod_data_held-out amod_Papineni_2001 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the appos_Och_2003 vmod_procedure_optimize prep_on_procedure_data dep_procedure_Papineni prep_with_respect_to_procedure_score dep_procedure_Och nn_procedure_MERT dep_training_procedure amod_training_error-rate amod_training_minimum det_training_a dobj_using_training amod_manner_standard det_manner_a xcomp_trained_using prep_in_trained_manner auxpass_trained_was nsubjpass_trained_system det_system_The
D09-1125	P03-1021	o	Then the same system weights are applied to both IncHMM and Joint Decoding based approaches and the feature weights of them are trained using the max-BLEU training method proposed by Och -LRB- 2003 -RRB- and refined by Moore and Quirk -LRB- 2008 -RRB-	appos_Quirk_2008 conj_and_Moore_Quirk prep_by_refined_Quirk prep_by_refined_Moore appos_Och_2003 conj_and_proposed_refined prep_by_proposed_Och dep_method_refined dep_method_proposed nn_method_training amod_method_max-BLEU det_method_the dobj_using_method xcomp_trained_using auxpass_trained_are prep_of_weights_them nn_weights_feature det_weights_the amod_approaches_based nn_Decoding_Joint conj_and_IncHMM_weights conj_and_IncHMM_approaches conj_and_IncHMM_Decoding preconj_IncHMM_both dep_applied_trained prep_to_applied_weights prep_to_applied_approaches prep_to_applied_Decoding prep_to_applied_IncHMM auxpass_applied_are nsubjpass_applied_weights advmod_applied_Then nn_weights_system amod_weights_same det_weights_the ccomp_``_applied
D09-1141	P03-1021	o	We then built separate directed word alignments for EnglishX andXEnglish -LRB- X -LCB- Indonesian Spanish -RCB- -RRB- using IBM model 4 -LRB- Brown et al. 1993 -RRB- combined them using the intersect + grow heuristic -LRB- Och and Ney 2003 -RRB- and extracted phrase-level translation pairs of maximum length seven using the alignment template approach -LRB- Och and Ney 2004 -RRB-	amod_Och_2004 conj_and_Och_Ney dep_approach_Ney dep_approach_Och nn_approach_template nn_approach_alignment det_approach_the dobj_using_approach num_length_seven nn_length_maximum vmod_pairs_using prep_of_pairs_length nn_pairs_translation amod_pairs_phrase-level amod_pairs_extracted conj_and_Och_2003 conj_and_Och_Ney nn_Och_heuristic dep_intersect_2003 dep_intersect_Ney dep_intersect_Och conj_+_intersect_grow det_intersect_the dobj_using_grow dobj_using_intersect conj_and_combined_pairs xcomp_combined_using dobj_combined_them csubj_combined_using dep_combined_X amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_IBM dep_using_Brown dobj_using_model dep_Indonesian_Spanish dep_X_Indonesian dep_andXEnglish_pairs dep_andXEnglish_combined nn_andXEnglish_EnglishX prep_for_alignments_andXEnglish nn_alignments_word amod_alignments_directed amod_alignments_separate dobj_built_alignments advmod_built_then nsubj_built_We ccomp_``_built
D09-1141	P03-1021	o	We set all weights by optimizing Bleu -LRB- Papineni et al. 2002 -RRB- using minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- on a separate development set of 2,000 sentences -LRB- Indonesian or Spanish -RRB- and we used them in a beam search decoder -LRB- Koehn et al. 2007 -RRB- to translate 2,000 test sentences -LRB- Indonesian or Spanish -RRB- into English	conj_or_Indonesian_Spanish dep_sentences_Spanish dep_sentences_Indonesian nn_sentences_test num_sentences_2,000 prep_into_translate_English dobj_translate_sentences aux_translate_to nn_al._et amod_Koehn_2007 dep_Koehn_al. nn_decoder_search nn_decoder_beam det_decoder_a prep_in_used_decoder dobj_used_them nsubj_used_we conj_or_Indonesian_Spanish dep_sentences_Spanish dep_sentences_Indonesian num_sentences_2,000 prep_of_set_sentences nn_set_development amod_set_separate det_set_a dep_Och_2003 appos_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum prep_on_using_set dobj_using_training amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni dobj_optimizing_Bleu det_weights_all vmod_set_translate dep_set_Koehn conj_and_set_used xcomp_set_using prepc_by_set_optimizing dobj_set_weights nsubj_set_We
D09-1147	P03-1021	n	The ubiquitous minimum error rate training -LRB- MERT -RRB- approach optimizes Viterbi predictions but does not explicitly boost the aggregated posterior probability of desirable n-grams -LRB- Och 2003 -RRB-	amod_Och_2003 dep_n-grams_Och amod_n-grams_desirable prep_of_probability_n-grams nn_probability_posterior amod_probability_aggregated det_probability_the dobj_boost_probability advmod_boost_explicitly neg_boost_not aux_boost_does nsubj_boost_approach nn_predictions_Viterbi conj_but_optimizes_boost dobj_optimizes_predictions nsubj_optimizes_approach nn_approach_training appos_training_MERT nn_training_rate nn_training_error nn_training_minimum amod_training_ubiquitous det_training_The
D09-1147	P03-1021	o	We extract a phrase table using the Moses pipeline based on Model 4 word alignments generated from GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ prep_from_generated_+ prep_from_generated_GIZA vmod_alignments_generated nn_alignments_word num_alignments_4 nn_alignments_Model prep_on_based_alignments nn_pipeline_Moses det_pipeline_the vmod_using_based dobj_using_pipeline nn_table_phrase det_table_a xcomp_extract_using dobj_extract_table nsubj_extract_We ccomp_``_extract
E06-1006	P03-1021	o	Phrases are then extracted from the word alignments using the method described in -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och prep_described_in vmod_method_described det_method_the dobj_using_method nn_alignments_word det_alignments_the xcomp_extracted_using prep_from_extracted_alignments advmod_extracted_then auxpass_extracted_are nsubjpass_extracted_Phrases
E06-1006	P03-1021	o	The score combination weights are trained by a minimum error rate training procedure similar to -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_to_Ney dep_to_Och prep_similar_to amod_procedure_similar nn_procedure_training nn_procedure_rate nn_procedure_error amod_procedure_minimum det_procedure_a agent_trained_procedure auxpass_trained_are nsubjpass_trained_weights nn_weights_combination nn_weights_score det_weights_The ccomp_``_trained
E06-1032	P03-1021	p	The remaining six entries were all fully automatic machine translation systems in fact they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training -LRB- Och 2003 -RRB- to optimize the weights of their log linear models feature functions -LRB- Och and Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_functions_Ney dep_functions_Och dobj_feature_functions nsubj_feature_system amod_models_linear nn_models_log poss_models_their prep_of_weights_models det_weights_the dobj_optimize_weights aux_optimize_to dep_Och_2003 appos_training_Och nn_training_rate nn_training_error nn_training_minimum amod_training_Bleubased vmod_used_optimize dobj_used_training nsubj_used_most nn_corpus_parallel amod_corpus_same det_corpus_the conj_and_trained_used prep_on_trained_corpus auxpass_trained_been aux_trained_had nsubjpass_trained_that rcmod_system_used rcmod_system_trained nn_system_translation nn_system_machine amod_system_statistical amod_system_phrase-based det_system_all cop_system_were nsubj_system_they prep_in_system_fact parataxis_systems_feature nn_systems_translation nn_systems_machine amod_systems_automatic det_systems_all cop_systems_were nsubj_systems_entries advmod_automatic_fully num_entries_six amod_entries_remaining det_entries_The
E06-1032	P03-1021	o	For example work which failed to detect improvements in translation quality with the integration of word sense disambiguation -LRB- Carpuat and Wu 2005 -RRB- or work which attempted to integrate syntactic information but which failed to improve Bleu -LRB- Charniak et al. 2003 Och et al. 2004 -RRB- may deserve a second look with a more targeted manual evaluation	amod_evaluation_manual amod_evaluation_targeted det_evaluation_a advmod_targeted_more prep_with_look_evaluation amod_look_second det_look_a dobj_deserve_look aux_deserve_may nsubj_deserve_work prep_for_deserve_example num_Och_2004 nn_Och_al. nn_Och_et dep_Charniak_Och appos_Charniak_2003 dep_Charniak_al. nn_Charniak_et appos_Bleu_Charniak dobj_improve_Bleu aux_improve_to xcomp_failed_improve nsubj_failed_which amod_information_syntactic dobj_integrate_information aux_integrate_to xcomp_attempted_integrate nsubj_attempted_which rcmod_work_attempted amod_Carpuat_2005 conj_and_Carpuat_Wu conj_or_disambiguation_work dep_disambiguation_Wu dep_disambiguation_Carpuat nn_disambiguation_sense nn_disambiguation_word conj_but_integration_failed prep_of_integration_work prep_of_integration_disambiguation det_integration_the nn_quality_translation prep_with_improvements_failed prep_with_improvements_integration prep_in_improvements_quality dobj_detect_improvements aux_detect_to xcomp_failed_detect nsubj_failed_which rcmod_work_failed
E06-1032	P03-1021	p	The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error nn_training_minimum prep_through_optimizing_training dobj_optimizing_systems conj_and_changes_optimizing nn_changes_system amod_changes_incremental dobj_evaluating_optimizing dobj_evaluating_changes prepc_of_purposes_evaluating det_purposes_the nn_metric_Bleu det_metric_the prep_for_relies_purposes prep_on_relies_metric nsubj_relies_community nn_community_translation nn_community_machine amod_community_statistical det_community_The
E06-2002	P03-1021	o	This preprocessing step can be accomplished by applying the GIZA + + toolkit -LRB- Och and Ney 2003 -RRB- that provides Viterbi alignments based on IBM Model-4	nn_Model-4_IBM nn_alignments_Viterbi prep_based_on_provides_Model-4 dobj_provides_alignments nsubj_provides_that amod_Och_2003 conj_and_Och_Ney rcmod_toolkit_provides appos_toolkit_Ney appos_toolkit_Och pobj_+_toolkit conj_+_GIZA_+ det_GIZA_the dobj_applying_+ dobj_applying_GIZA agent_accomplished_applying auxpass_accomplished_be aux_accomplished_can nsubjpass_accomplished_step amod_step_preprocessing det_step_This
E06-2002	P03-1021	o	Starting from the parallel training corpus provided with direct and inverted alignments the socalled union alignment -LRB- Och and Ney 2003 -RRB- is computed	auxpass_computed_is nsubjpass_computed_alignment vmod_computed_Starting dep_Och_2003 conj_and_Och_Ney appos_alignment_Ney appos_alignment_Och nn_alignment_union amod_alignment_socalled det_alignment_the amod_alignments_inverted amod_alignments_direct conj_and_direct_inverted prep_with_provided_alignments nn_corpus_training amod_corpus_parallel det_corpus_the vmod_Starting_provided prep_from_Starting_corpus
E09-1011	P03-1021	o	We tune using Ochs algorithm -LRB- Och 2003 -RRB- to optimize weights for the distortion model language model phrase translation model and word penalty over the BLEU metric -LRB- Papineni et al. 2001 -RRB-	dep_al._2001 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric det_BLEU_the nn_penalty_word nn_model_translation nn_model_phrase prep_over_model_BLEU conj_and_model_penalty conj_and_model_model nn_model_language nn_model_distortion det_model_the prep_for_weights_model dobj_optimize_weights aux_optimize_to amod_Och_2003 dep_algorithm_Och nn_algorithm_Ochs dobj_using_penalty dobj_using_model dobj_using_model vmod_using_optimize dobj_using_algorithm xcomp_tune_using nsubj_tune_We
E09-1033	P03-1021	p	Och -LRB- 2003 -RRB- has described an efficient exact onedimensional accuracy maximization technique for a similar search problem in machine translation	nn_translation_machine prep_in_problem_translation nn_problem_search amod_problem_similar det_problem_a prep_for_technique_problem nn_technique_maximization nn_technique_accuracy amod_technique_onedimensional amod_technique_exact amod_technique_efficient det_technique_an dobj_described_technique aux_described_has nsubj_described_Och appos_Och_2003
E09-1033	P03-1021	o	Due to space we do not describe step 8 in detail -LRB- see -LRB- Och 2003 -RRB- -RRB-	dep_Och_2003 dep_see_Och prep_in_step_detail num_step_8 dep_describe_see dobj_describe_step neg_describe_not aux_describe_do nsubj_describe_we prep_due_to_describe_space
E09-1033	P03-1021	o	287 System Train + base Test + base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 -LRB- 5 trials/fold -RRB- 3 Contrastive 88.82 0.93 88.55 0.66 -LRB- greedy selection -RRB- Table 1 Average F1 of 7-way cross-validation To generate the alignments we used Model 4 -LRB- Brown et al. 1993 -RRB- as implemented in GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ prep_in_implemented_+ prep_in_implemented_GIZA mark_implemented_as amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_4 advcl_used_implemented dobj_used_Model nsubj_used_we nsubj_used_base nsubj_used_Test nsubj_used_Train det_alignments_the dobj_generate_alignments aux_generate_To amod_cross-validation_7-way vmod_F1_generate prep_of_F1_cross-validation amod_F1_Average num_Table_1 dep_Table_0.66 amod_selection_greedy appos_0.66_selection dep_0.66_88.55 dep_88.55_0.93 number_0.93_88.82 dep_Contrastive_F1 dep_Contrastive_Table dep_3_Contrastive num_trials/fold_5 dep_0.56_3 dep_0.56_trials/fold number_0.56_88.45 dep_0.56_0.82 number_0.82_88.70 dep_Contrastive_0.56 amod_2_Contrastive dep_87.89_2 number_87.89_87.89 amod_Baseline_87.89 num_Baseline_1 dep_base_Baseline nn_Test_base conj_+_Train_base conj_+_Train_Test nn_Train_System num_Train_287
E09-1044	P03-1021	o	MET -LRB- Och 2003 -RRB- iterative parameter estimation under IBM BLEU is performed on the development set	nn_set_development det_set_the prep_on_performed_set auxpass_performed_is nsubjpass_performed_estimation nn_BLEU_IBM prep_under_estimation_BLEU nn_estimation_parameter amod_estimation_iterative nn_estimation_MET dep_Och_2003 dep_MET_Och
E09-1063	P03-1021	o	5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model GIZA + + implementation of IBM word alignment model 4 -LRB- Och and Ney 2003 -RRB- the refinement and phrase-extraction heuristics described in -LRB- Koehn et al. 2003 -RRB- minimum-errorrate training -LRB- Och 2003 -RRB- a 5-gram language model with Kneser-Ney smoothing trained with SRILM -LRB- Stolcke 2002 -RRB- on the English side of the training data and Moses -LRB- Koehn et al. 2007 Dyer et al. 2008 -RRB- to translate both single best segmentation and word lattices	nn_lattices_word conj_and_segmentation_lattices amod_segmentation_best amod_segmentation_single preconj_segmentation_both dobj_translate_lattices dobj_translate_segmentation aux_translate_to num_Dyer_2008 nn_Dyer_al. nn_Dyer_et dep_Koehn_Dyer appos_Koehn_2007 dep_Koehn_al. nn_Koehn_et vmod_Moses_translate appos_Moses_Koehn nn_data_training det_data_the prep_of_side_data amod_side_English det_side_the dep_Stolcke_2002 appos_SRILM_Stolcke prep_on_trained_side prep_with_trained_SRILM vmod_smoothing_trained amod_smoothing_Kneser-Ney prep_with_model_smoothing nn_model_language amod_model_5-gram det_model_a dep_Och_2003 appos_training_Och amod_training_minimum-errorrate conj_and_Koehn_Moses conj_and_Koehn_model conj_and_Koehn_training amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Moses prep_in_described_model prep_in_described_training prep_in_described_Koehn amod_heuristics_phrase-extraction vmod_refinement_described conj_and_refinement_heuristics det_refinement_the dep_Och_2003 conj_and_Och_Ney appos_model_Ney appos_model_Och num_model_4 nn_model_alignment nn_model_word appos_GIZA_heuristics appos_GIZA_refinement dep_GIZA_model prep_of_GIZA_IBM conj_+_GIZA_implementation nn_model_PB-SMT amod_model_log-linear amod_model_standard det_model_a amod_segmenters_different prep_with_using_model dobj_using_segmenters vmod_experiments_using dobj_conducted_experiments nsubj_conducted_We dep_System_implementation dep_System_GIZA rcmod_System_conducted nn_System_Baseline num_System_5.3
E09-3008	P03-1021	o	The tools used are the Moses toolkit -LRB- Koehn et al. 2007 -RRB- for decoding and training GIZA + + for word alignment -LRB- Och and Ney 2003 -RRB- and SRILM -LRB- Stolcke 2002 -RRB- for language models	nn_models_language dep_Stolcke_2002 prep_for_SRILM_models dep_SRILM_Stolcke dep_Och_2003 conj_and_Och_Ney dep_alignment_Ney dep_alignment_Och nn_alignment_word pobj_for_alignment conj_+_GIZA_for conj_and_decoding_training amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et conj_and_toolkit_SRILM conj_and_toolkit_for conj_and_toolkit_GIZA prep_for_toolkit_training prep_for_toolkit_decoding dep_toolkit_Koehn nn_toolkit_Moses det_toolkit_the cop_toolkit_are nsubj_toolkit_tools vmod_tools_used det_tools_The
E09-3008	P03-1021	o	To tune feature weights minimum error rate training is used -LRB- Och 2003 -RRB- optimized against the Neva metric -LRB- Forsbom 2003 -RRB-	amod_Forsbom_2003 dep_metric_Forsbom amod_Neva_metric dep_the_Neva prep_against_optimized_the amod_Och_2003 dep_used_Och auxpass_used_is nsubjpass_used_training nn_training_rate nn_training_error amod_training_minimum nn_training_weights nn_training_feature vmod_tune_optimized ccomp_tune_used aux_tune_To
H05-1012	P03-1021	p	Current state of the art machine translation systems -LRB- Och 2003 -RRB- use phrasal -LRB- n-gram -RRB- features extracted automatically from parallel corpora	amod_corpora_parallel prep_from_extracted_corpora advmod_extracted_automatically dep_features_extracted appos_phrasal_n-gram nn_phrasal_use dep_Och_2003 dep_systems_phrasal dep_systems_Och nn_systems_translation nn_systems_machine nn_systems_art det_systems_the dep_state_features prep_of_state_systems amod_state_Current
H05-1012	P03-1021	o	Although there is a modest cost associated with annotating data we show that a reduction of 40 % relative in alignment error -LRB- AER -RRB- is possible over the GIZA + + aligner -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_aligner_Ney dep_aligner_Och pobj_+_aligner conj_+_GIZA_+ det_GIZA_the prep_over_possible_+ prep_over_possible_GIZA cop_possible_is nsubj_possible_reduction mark_possible_that appos_error_AER nn_error_alignment prep_in_relative_error num_%_40 amod_reduction_relative prep_of_reduction_% det_reduction_a ccomp_show_possible nsubj_show_we advcl_show_is amod_data_annotating prep_with_associated_data vmod_cost_associated amod_cost_modest det_cost_a nsubj_is_cost expl_is_there mark_is_Although
H05-1021	P03-1021	o	For the combined set -LRB- ALL -RRB- we also show the 95 % BLEU confidence interval computed using bootstrap resampling -LRB- Och 2003 -RRB-	amod_Och_2003 dep_resampling_Och nn_resampling_bootstrap dobj_using_resampling xcomp_computed_using vmod_interval_computed nn_interval_confidence nn_interval_BLEU amod_interval_% det_interval_the number_%_95 dobj_show_interval advmod_show_also nsubj_show_we prep_for_show_set appos_set_ALL amod_set_combined det_set_the
H05-1021	P03-1021	o	Finally we use Minimum Error Training -LRB- MET -RRB- -LRB- Och 2003 -RRB- to train log-linear scaling factors that are applied to the WFSTs in Equation 1	num_Equation_1 prep_in_WFSTs_Equation det_WFSTs_the prep_to_applied_WFSTs auxpass_applied_are nsubjpass_applied_that rcmod_factors_applied nn_factors_scaling amod_factors_log-linear dobj_train_factors aux_train_to amod_Och_2003 appos_Training_Och appos_Training_MET nn_Training_Error nn_Training_Minimum vmod_use_train dobj_use_Training nsubj_use_we advmod_use_Finally
H05-1022	P03-1021	o	5 Phrase Pair Induction A common approach to phrase-based translation is to extract an inventory of phrase pairs -LRB- PPI -RRB- from bitext -LRB- Koehn et al. 2003 -RRB- For example in the phraseextract algorithm -LRB- Och 2002 -RRB- a word alignment am1 is generated over the bitext and all word subsequences ei2i1 and fj2j1 are found that satisfy am1 aj -LSB- i1 i2 -RSB- iff j -LSB- j1 j2 -RSB-	appos_j1_j2 dep_j_j1 nn_j_iff dep_j_i1 nn_j_aj appos_i1_i2 dep_am1_j dep_satisfy_am1 nsubj_satisfy_that ccomp_found_satisfy auxpass_found_are nsubjpass_found_subsequences conj_and_ei2i1_fj2j1 dobj_subsequences_fj2j1 dobj_subsequences_ei2i1 nsubj_subsequences_word det_word_all det_bitext_the conj_and_generated_found prep_over_generated_bitext auxpass_generated_is nsubjpass_generated_am1 nn_am1_alignment nn_am1_word det_am1_a amod_Och_2002 dep_algorithm_Och amod_algorithm_phraseextract det_algorithm_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_pairs_PPI nn_pairs_phrase prep_from_inventory_bitext prep_of_inventory_pairs det_inventory_an dobj_extract_inventory aux_extract_to parataxis_is_found parataxis_is_generated prep_in_is_algorithm prep_for_is_example dep_is_Koehn xcomp_is_extract nsubj_is_approach amod_translation_phrase-based prep_to_approach_translation amod_approach_common det_approach_A nn_approach_Induction nn_approach_Pair nn_approach_Phrase num_approach_5
H05-1022	P03-1021	o	Pooling the sets to form two large CE and AE test sets the AE system improvements are significant at a 95 % level -LRB- Och 2003 -RRB- the CE systems are only equivalent	advmod_equivalent_only cop_equivalent_are nsubj_equivalent_systems nn_systems_CE det_systems_the appos_Och_2003 dep_level_Och amod_level_% det_level_a number_%_95 parataxis_significant_equivalent prep_at_significant_level cop_significant_are nsubj_significant_improvements vmod_significant_Pooling nn_improvements_system nn_improvements_AE det_improvements_the nn_sets_test nn_sets_AE conj_and_CE_sets amod_CE_large num_CE_two dobj_form_sets dobj_form_CE aux_form_to vmod_sets_form det_sets_the dobj_Pooling_sets
H05-1022	P03-1021	o	The hallucination process is motivated by the use of NULL alignments into Markov alignment models as done by -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_by_Ney dep_by_Och prep_done_by mark_done_as nn_models_alignment nn_models_Markov amod_alignments_NULL dep_use_done prep_into_use_models prep_of_use_alignments det_use_the agent_motivated_use auxpass_motivated_is nsubjpass_motivated_process nn_process_hallucination det_process_The
H05-1022	P03-1021	o	Alignment performance is measured by the Alignment Error Rate -LRB- AER -RRB- -LRB- Och and Ney 2003 -RRB- AER -LRB- B B -RRB- = 12 | B B | / -LRB- | B | + | B | -RRB- where B is a set reference word links and B are the word links generated automatically	advmod_generated_automatically vmod_links_generated nn_links_word det_links_the cop_links_are nsubj_links_B nsubj_links_links conj_and_links_B nn_links_word nn_links_reference nn_links_set det_links_a cop_links_is nsubj_links_B advmod_links_where nn_|_B nn_|_| conj_+_|_| nn_|_B nn_|_| dep_B_links dep_B_| dep_B_| num_B_| nn_B_B nn_B_| num_B_12 dobj_=_B dep_B_B dep_AER_= dep_AER_B dep_Och_2003 conj_and_Och_Ney dep_Rate_AER appos_Rate_Ney appos_Rate_Och appos_Rate_AER nn_Rate_Error nn_Rate_Alignment det_Rate_the agent_measured_Rate auxpass_measured_is nsubjpass_measured_performance nn_performance_Alignment
H05-1027	P03-1021	o	The line search is an extension of that described in -LRB- Och 2003 Quirk et al. 2005	num_al._2005 nn_al._et nn_al._Quirk dep_2003_al. amod_2003_Och prep_in_described_2003 vmod_that_described prep_of_extension_that det_extension_an cop_extension_is nsubj_extension_search nn_search_line det_search_The
H05-1027	P03-1021	o	3.3 Grid Line Search Our implementation of a grid search is a modified version of that proposed in -LRB- Och 2003 -RRB-	dep_Och_2003 dep_in_Och prep_proposed_in vmod_that_proposed prep_of_version_that amod_version_modified det_version_a cop_version_is csubj_version_Search nn_search_grid det_search_a prep_of_implementation_search poss_implementation_Our dobj_Search_implementation nsubj_Search_Line nn_Line_Grid num_Line_3.3
H05-1027	P03-1021	o	The modifications are made to deal with the efficiency issue due to the fact that there is a very large number of features and training samples in our task compared to only 8 features used in -LRB- Och 2003 -RRB-	dep_Och_2003 dep_in_Och prep_used_in vmod_features_used num_features_8 quantmod_8_only poss_task_our nn_samples_training conj_and_features_samples prep_in_number_task prep_of_number_samples prep_of_number_features amod_number_large det_number_a advmod_large_very pobj_is_features prepc_compared_to_is_to nsubj_is_number expl_is_there mark_is_that ccomp_fact_is det_fact_the nn_issue_efficiency det_issue_the prep_due_to_deal_fact prep_with_deal_issue aux_deal_to xcomp_made_deal auxpass_made_are nsubjpass_made_modifications det_modifications_The
H05-1034	P03-1021	o	MSR thus adopts the method proposed by Och -LRB- 2003 -RRB-	appos_Och_2003 agent_proposed_Och vmod_method_proposed det_method_the dobj_adopts_method advmod_adopts_thus nsubj_adopts_MSR
H05-1087	P03-1021	o	This is analogous and in a certain sense equivalent to empirical risk minimization which has been used successfully in related areas such as speech recognition -LRB- Rahim and Lee 1997 -RRB- language modeling -LRB- Paciorek and Rosenfeld 2000 -RRB- and machine translation -LRB- Och 2003 -RRB-	amod_Och_2003 dep_translation_Och nn_translation_machine dep_Paciorek_2000 conj_and_Paciorek_Rosenfeld appos_modeling_Rosenfeld appos_modeling_Paciorek nn_modeling_language dep_Rahim_1997 conj_and_Rahim_Lee conj_and_recognition_translation conj_and_recognition_modeling dep_recognition_Lee dep_recognition_Rahim nn_recognition_speech prep_such_as_areas_translation prep_such_as_areas_modeling prep_such_as_areas_recognition amod_areas_related prep_in_used_areas advmod_used_successfully auxpass_used_been aux_used_has nsubjpass_used_which rcmod_minimization_used nn_minimization_risk amod_minimization_empirical nn_equivalent_sense amod_equivalent_certain det_equivalent_a prep_to_in_minimization pobj_in_equivalent conj_and_analogous_in cop_analogous_is nsubj_analogous_This
H05-1095	P03-1021	o	A first family of libraries was based on a word alignment A produced using the Refined method described in -LRB- Och and Ney 2003 -RRB- -LRB- combination of two IBM-Viterbi alignments -RRB- we call these the A libraries	nn_libraries_A det_libraries_the predet_libraries_these dobj_call_libraries nsubj_call_we amod_alignments_IBM-Viterbi num_alignments_two prep_of_combination_alignments dep_Och_combination num_Och_2003 conj_and_Och_Ney prep_in_described_Ney prep_in_described_Och vmod_method_described amod_method_Refined det_method_the dobj_using_method xcomp_produced_using vmod_A_produced nn_A_alignment nn_A_word det_A_a parataxis_based_call prep_on_based_A auxpass_based_was nsubjpass_based_family prep_of_family_libraries amod_family_first det_family_A
H05-1095	P03-1021	o	The first is to align the words using a standard word alignement technique such as the Refined Method described in -LRB- Och and Ney 2003 -RRB- -LRB- the intersection of two IBM Viterbi alignments forward and reverse enriched with alignments from the union -RRB- and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences	prep_of_pair_sentences amod_pair_same det_pair_the prep_in_co-occur_pair nsubj_co-occur_that rcmod_alignments_co-occur amod_alignments_individual dobj_combining_alignments advmod_combining_together prepc_by_generate_combining dobj_generate_bi-phrases advmod_generate_then det_union_the prep_from_alignments_union prep_with_enriched_alignments conj_and_forward_reverse nn_alignments_Viterbi nn_alignments_IBM num_alignments_two amod_intersection_enriched advmod_intersection_reverse advmod_intersection_forward prep_of_intersection_alignments det_intersection_the dep_Och_intersection num_Och_2003 conj_and_Och_Ney prep_in_described_Ney prep_in_described_Och vmod_Method_described amod_Method_Refined det_Method_the prep_such_as_technique_Method nn_technique_alignement nn_technique_word amod_technique_standard det_technique_a dobj_using_technique det_words_the conj_and_align_generate xcomp_align_using dobj_align_words aux_align_to xcomp_is_generate xcomp_is_align nsubj_is_first det_first_The rcmod_``_is
H05-1095	P03-1021	o	This is the strategy that is usually adopted in other phrase-based MT approaches -LRB- Zens and Ney 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney num_Ney_2003 dep_Zens_Ney dep_Zens_Och conj_and_Zens_Ney dep_approaches_Ney dep_approaches_Zens nn_approaches_MT amod_approaches_phrase-based amod_approaches_other prep_in_adopted_approaches advmod_adopted_usually auxpass_adopted_is nsubjpass_adopted_that rcmod_strategy_adopted det_strategy_the cop_strategy_is nsubj_strategy_This
H05-1095	P03-1021	o	Instead and as suggested by Och -LRB- 2003 -RRB- we chose to maximize directly the quality of the translations produced by the system as measured with a machine translation evaluation metric	amod_evaluation_metric dep_translation_evaluation dep_machine_translation dep_a_machine prep_with_measured_a mark_measured_as det_system_the agent_produced_system vmod_translations_produced det_translations_the prep_of_quality_translations det_quality_the advcl_maximize_measured dobj_maximize_quality advmod_maximize_directly aux_maximize_to xcomp_chose_maximize nsubj_chose_we advcl_chose_suggested cc_chose_and advmod_chose_Instead appos_Och_2003 prep_by_suggested_Och mark_suggested_as
H05-1095	P03-1021	o	1 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models -LRB- Och et al. 1999 Marcu and Wong 2002 Yamada and Knight 2002 Tillmann and Xia 2003 -RRB-	dep_Tillmann_2003 conj_and_Tillmann_Xia num_Yamada_2002 conj_and_Yamada_Knight dep_Marcu_Xia dep_Marcu_Tillmann conj_and_Marcu_Knight conj_and_Marcu_Yamada num_Marcu_2002 conj_and_Marcu_Wong dep_Och_Yamada dep_Och_Wong dep_Och_Marcu appos_Och_1999 dep_Och_al. nn_Och_et amod_models_phrase-based amod_models_word-based dep_step_Och prep_to_step_models prep_from_step_models det_step_the cop_step_is nsubj_step_evolution nn_translation_machine amod_translation_statistical amod_years_recent prep_in_evolution_translation prep_of_evolution_years amod_evolution_remarkable det_evolution_the advmod_evolution_Possibly dep_evolution_Introduction advmod_remarkable_most num_Introduction_1
H05-1096	P03-1021	o	Nowadays most of the state-of-the-art SMT systems are based on bilingual phrases -LRB- Bertoldi et al. 2004 Koehn et al. 2003 Och and Ney 2004 Tillmann 2003 Vogel et al. 2004 Zens and Ney 2004 -RRB-	dep_Zens_2004 conj_and_Zens_Ney num_Vogel_2004 nn_Vogel_al. nn_Vogel_et num_Tillmann_2003 dep_Och_Ney dep_Och_Zens conj_and_Och_Vogel conj_and_Och_Tillmann conj_and_Och_2004 conj_and_Och_Ney dep_Koehn_Vogel dep_Koehn_Tillmann dep_Koehn_2004 dep_Koehn_Ney dep_Koehn_Och num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Bertoldi_Koehn amod_Bertoldi_2004 dep_Bertoldi_al. nn_Bertoldi_et amod_phrases_bilingual dep_based_Bertoldi prep_on_based_phrases auxpass_based_are nsubjpass_based_most advmod_based_Nowadays nn_systems_SMT amod_systems_state-of-the-art det_systems_the prep_of_most_systems ccomp_``_based
H05-1096	P03-1021	o	The model scaling factors 1 ,5 and the word and phrase penalties are optimized with respect to some evaluation criterion -LRB- Och 2003 -RRB- e.g. BLEU score	nn_score_BLEU nn_score_e.g. dep_Och_2003 appos_criterion_Och nn_criterion_evaluation det_criterion_some dobj_optimized_score prep_with_respect_to_optimized_criterion auxpass_optimized_are nsubjpass_optimized_factors num_penalties_phrase num_penalties_word num_penalties_,5 num_penalties_1 det_word_the conj_and_1_phrase conj_and_1_word conj_and_1_,5 dep_factors_penalties nn_factors_scaling nn_factors_model det_factors_The
H05-1098	P03-1021	o	The feature weights are learned by maximizing the BLEU score -LRB- Papineni et al. 2002 -RRB- on held-out data,usingminimum-error-ratetraining -LRB- Och ,2003 -RRB- as implemented by Koehn	prep_by_implemented_Koehn mark_implemented_as num_Och_,2003 appos_data,usingminimum-error-ratetraining_Och amod_data,usingminimum-error-ratetraining_held-out dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dobj_maximizing_score advcl_learned_implemented prep_on_learned_data,usingminimum-error-ratetraining dep_learned_Papineni agent_learned_maximizing auxpass_learned_are nsubjpass_learned_weights nn_weights_feature det_weights_The ccomp_``_learned
H05-1098	P03-1021	o	5 Analysis Over the last few years several automatic metrics for machine translation evaluation have been introduced largely to reduce the human cost of iterative system evaluation during the development cycle -LRB- Lin and Och 2004 Melamed et al. 2003 Papineni et al. 2002 -RRB-	num_Papineni_2002 nn_Papineni_al. nn_Papineni_et num_Melamed_2003 nn_Melamed_al. nn_Melamed_et dep_Lin_Papineni dep_Lin_Melamed num_Lin_2004 conj_and_Lin_Och dep_cycle_Och dep_cycle_Lin nn_cycle_development det_cycle_the nn_evaluation_system amod_evaluation_iterative prep_of_cost_evaluation amod_cost_human det_cost_the prep_during_reduce_cycle dobj_reduce_cost aux_reduce_to advmod_reduce_largely xcomp_introduced_reduce auxpass_introduced_been aux_introduced_have nsubjpass_introduced_metrics dep_introduced_Analysis nn_evaluation_translation nn_evaluation_machine prep_for_metrics_evaluation amod_metrics_automatic amod_metrics_several amod_years_few amod_years_last det_years_the prep_over_Analysis_years num_Analysis_5
I05-2039	P03-1021	o	It has a lower bound of 0 no upper bound better scores indicate better translations and it tends to be highly correlated with the adequacy of outputs mWER -LRB- Och 2003 -RRB- or Multiple Word Error Rate is the edit distance in words between the system output and the closest reference translation in a set	det_set_a prep_in_translation_set nn_translation_reference amod_translation_closest det_translation_the conj_and_output_translation nn_output_system det_output_the prep_between_words_translation prep_between_words_output prep_in_distance_words nn_distance_edit det_distance_the cop_distance_is nsubj_distance_Rate nsubj_distance_mWER nn_Rate_Error nn_Rate_Word amod_Rate_Multiple num_Och_2003 conj_or_mWER_Rate appos_mWER_Och prep_of_adequacy_outputs det_adequacy_the prep_with_correlated_adequacy advmod_correlated_highly auxpass_correlated_be aux_correlated_to xcomp_tends_correlated nsubj_tends_it amod_translations_better parataxis_indicate_distance conj_and_indicate_tends dobj_indicate_translations nsubj_indicate_scores advcl_indicate_bound mark_indicate_a amod_scores_better amod_bound_upper neg_bound_no dep_bound_bound prep_of_bound_0 dep_bound_lower dep_has_tends dep_has_indicate nsubj_has_It
I08-1030	P03-1021	o	2 Phrase-based statistical machine translation Phrase-based SMT uses a framework of log-linear models -LRB- Och 2003 -RRB- to integrate multiple features	amod_features_multiple dobj_integrate_features aux_integrate_to appos_Och_2003 dep_models_Och amod_models_log-linear prep_of_framework_models det_framework_a vmod_uses_integrate dobj_uses_framework nsubj_uses_SMT amod_SMT_Phrase-based nn_SMT_translation nn_SMT_machine amod_SMT_statistical amod_SMT_Phrase-based num_SMT_2
I08-1030	P03-1021	o	In the training phase bilingual parallel sentences are preprocessed and aligned using alignment algorithms or tools such as GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ prep_such_as_tools_+ prep_such_as_tools_GIZA conj_or_algorithms_tools nn_algorithms_alignment dobj_using_tools dobj_using_algorithms xcomp_aligned_using nsubj_aligned_sentences conj_and_preprocessed_aligned cop_preprocessed_are nsubj_preprocessed_sentences prep_in_preprocessed_phase amod_sentences_parallel amod_sentences_bilingual nn_phase_training det_phase_the
I08-1067	P03-1021	o	The weights for the various components of the model -LRB- phrase translation model language model distortion model etc. -RRB- are set by minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum agent_set_training auxpass_set_are nsubjpass_set_weights nn_etc._model nn_etc._distortion nn_model_language appos_model_etc. appos_model_model nn_model_translation nn_model_phrase det_model_the prep_of_components_model amod_components_various det_components_the dep_weights_model prep_for_weights_components det_weights_The
I08-2087	P03-1021	o	The corresponding weight is trained through minimum error rate method -LRB- Och 2003 -RRB-	num_Och_2003 appos_method_Och nn_method_rate nn_method_error nn_method_minimum prep_through_trained_method auxpass_trained_is nsubjpass_trained_weight amod_weight_corresponding det_weight_The
I08-2087	P03-1021	o	-LRB- 2003 -RRB- bilingual sentences are trained by GIZA + + -LRB- Och and Ney 2003 -RRB- in two directions -LRB- from source to target and target to source -RRB-	conj_and_target_target prep_to_source_target prep_to_source_target num_directions_two num_Ney_2003 conj_and_Och_Ney dep_+_Ney dep_+_Och prep_in_GIZA_directions conj_+_GIZA_+ prep_to_trained_source prep_from_trained_source agent_trained_+ agent_trained_GIZA auxpass_trained_are nsubjpass_trained_sentences amod_sentences_bilingual dep_sentences_2003
I08-2088	P03-1021	o	We used the preprocessed data to train the phrase-based translation model by using GIZA + + -LRB- Och and Ney 2003 -RRB- and the Pharaoh tool kit -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_kit_tool nn_kit_Pharaoh det_kit_the num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och dep_GIZA_Koehn conj_and_GIZA_kit conj_+_GIZA_+ dobj_using_kit dobj_using_+ dobj_using_GIZA nn_model_translation amod_model_phrase-based det_model_the prepc_by_train_using dobj_train_model aux_train_to amod_data_preprocessed det_data_the xcomp_used_train dobj_used_data nsubj_used_We
I08-2088	P03-1021	o	3.2.2 Features We used eight features -LRB- Och and Ney 2003 Koehn et al. 2003 -RRB- and their weights for the translations	det_translations_the prep_for_weights_translations poss_weights_their num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2003 conj_and_Och_Ney dep_features_Koehn dep_features_2003 dep_features_Ney dep_features_Och num_features_eight conj_and_used_weights dobj_used_features nsubj_used_We rcmod_Features_weights rcmod_Features_used num_Features_3.2.2 dep_``_Features
I08-2088	P03-1021	p	Target language model probability -LRB- weight = 0.5 -RRB- According to a previous study the minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- which is the optimization of feature weights by maximizing the BLEU score on the development set can improve the performance of a system	det_system_a prep_of_performance_system det_performance_the dobj_improve_performance aux_improve_can nsubj_improve_training nn_set_development det_set_the nn_score_BLEU det_score_the prep_on_maximizing_set dobj_maximizing_score nn_weights_feature prepc_by_optimization_maximizing prep_of_optimization_weights det_optimization_the cop_optimization_is nsubj_optimization_which dep_Och_2003 rcmod_training_optimization appos_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum det_training_the amod_study_previous det_study_a dobj_=_0.5 nsubj_=_weight rcmod_probability_improve pobj_probability_study prepc_according_to_probability_to dep_probability_= nn_probability_model nn_probability_language nn_probability_Target
I08-4028	P03-1021	o	The decision rule here is W 0 = argmax W -LCB- Pr -LRB- W | C -RRB- -RCB- = argmax W -LCB- M summationdisplay m = 1 m h m -LRB- W C -RRB- -RCB- -LRB- 3 -RRB- The parameters M 1 of this model can be optimized by standard approaches such as the Minimum Error Rate Training used in machine translation -LRB- Och 2003 -RRB-	amod_Och_2003 dep_translation_Och nn_translation_machine prep_in_used_translation vmod_Training_used nn_Training_Rate nn_Training_Error nn_Training_Minimum det_Training_the prep_such_as_approaches_Training amod_approaches_standard agent_optimized_approaches auxpass_optimized_be aux_optimized_can nsubjpass_optimized_M det_model_this prep_of_M_model num_M_1 rcmod_parameters_optimized det_parameters_The dep_3_parameters appos_W_C dep_m_W nn_m_h nn_m_m num_m_1 dep_=_m amod_m_= nn_m_summationdisplay nn_m_M dep_W_m nn_W_argmax dep_=_W num_C_| nn_C_W appos_Pr_C dep_W_3 amod_W_= dep_W_Pr nn_W_argmax amod_W_= num_W_0 nn_W_W dep_is_W nsubj_is_rule advmod_rule_here nn_rule_decision det_rule_The ccomp_``_is
J04-4002	P03-1021	o	A comparison of the two approaches can be found in Koehn Och and Marcu -LRB- 2003 -RRB-	appos_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och prep_in_found_Marcu prep_in_found_Och prep_in_found_Koehn auxpass_found_be aux_found_can nsubjpass_found_comparison num_approaches_two det_approaches_the prep_of_comparison_approaches det_comparison_A
J04-4002	P03-1021	p	An efficient algorithm for performing this tuning for a larger number of model parameters can be found in Och -LRB- 2003 -RRB-	appos_Och_2003 prep_in_found_Och auxpass_found_be aux_found_can nsubjpass_found_algorithm nn_parameters_model prep_of_number_parameters amod_number_larger det_number_a det_tuning_this prep_for_performing_number dobj_performing_tuning prepc_for_algorithm_performing amod_algorithm_efficient det_algorithm_An
J04-4002	P03-1021	o	Looking at the results of the recent machine translation evaluations this approach seems currently to give the best results and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes -LRB- Marcu and Wong 2002 Venugopal Vogel and Waibel 2003 Tillmann 2003 Koehn Och and Marcu 2003 -RRB-	num_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och num_Tillmann_2003 num_Waibel_2003 conj_and_Venugopal_Waibel conj_and_Venugopal_Vogel num_Wong_2002 dep_Marcu_Marcu dep_Marcu_Och dep_Marcu_Koehn dep_Marcu_Tillmann dep_Marcu_Waibel dep_Marcu_Vogel dep_Marcu_Venugopal conj_and_Marcu_Wong appos_purposes_Wong appos_purposes_Marcu nn_purposes_translation nn_purposes_machine nn_lexica_translation nn_lexica_phrase prep_for_learning_purposes dobj_learning_lexica amod_methods_different prepc_for_working_learning prep_on_working_methods aux_working_are nsubj_working_number prep_of_number_researchers amod_number_increasing det_number_an amod_results_best det_results_the dobj_give_results aux_give_to conj_and_seems_working xcomp_seems_give advmod_seems_currently nsubj_seems_approach vmod_seems_Looking det_approach_this nn_evaluations_translation nn_evaluations_machine amod_evaluations_recent det_evaluations_the prep_of_results_evaluations det_results_the prep_at_Looking_results
J04-4002	P03-1021	o	An alternative training criterion therefore directly optimizes translation quality as measured by an automatic evaluation criterion -LRB- Och 2003 -RRB-	num_Och_2003 appos_criterion_Och nn_criterion_evaluation amod_criterion_automatic det_criterion_an prep_by_measured_criterion mark_measured_as nn_quality_translation advcl_optimizes_measured dobj_optimizes_quality advmod_optimizes_directly advmod_optimizes_therefore nsubj_optimizes_criterion nn_criterion_training amod_criterion_alternative det_criterion_An
J04-4002	P03-1021	o	-LRB- 1993 -RRB- and Och and Ney -LRB- 2003 -RRB-	appos_Ney_2003 conj_and_Och_Ney conj_and_1993_Ney conj_and_1993_Och
J04-4002	P03-1021	o	The alignment a J 1 that has the highest probability -LRB- under a certain model -RRB- is also called the Viterbi alignment -LRB- of that model -RRB- a J 1 = argmax a J 1 p -LRB- f J 1 a J 1 | e I 1 -RRB- -LRB- 8 -RRB- A detailed comparison of the quality of these Viterbi alignments for various statistical alignment models compared to human-made word alignments can be found in Och and Ney -LRB- 2003 -RRB-	appos_Ney_2003 conj_and_Och_Ney prep_in_found_Ney prep_in_found_Och auxpass_found_be aux_found_can nsubjpass_found_J dobj_found_p dep_found_= dep_found_1 dep_found_J dep_found_a nn_alignments_word amod_alignments_human-made nn_models_alignment amod_models_statistical amod_models_various nn_alignments_Viterbi det_alignments_these prep_for_quality_models prep_of_quality_alignments det_quality_the prep_of_comparison_quality amod_comparison_detailed det_comparison_A num_comparison_8 dep_comparison_1 dep_comparison_I dep_comparison_e nn_comparison_| num_|_1 nn_|_J det_|_a pobj_J_alignments prepc_compared_to_J_to appos_J_comparison num_J_1 nn_J_f num_p_1 nn_p_J det_p_a nn_p_argmax det_model_that dep_alignment_found prep_of_alignment_model nn_alignment_Viterbi det_alignment_the dep_called_alignment advmod_called_also auxpass_called_is nsubjpass_called_J amod_model_certain det_model_a prep_under_probability_model amod_probability_highest det_probability_the dobj_has_probability nsubj_has_that rcmod_J_has num_J_1 det_J_a rcmod_alignment_called det_alignment_The dep_``_alignment
J05-4003	P03-1021	o	Using this alignment strategy we follow -LRB- Och and Ney 2003 -RRB- and compute one alignment for each translation direction -LRB- f e and e f -RRB- and then combine them	dobj_combine_them advmod_combine_then nsubj_combine_we dep_f_f conj_and_f_e dep_f_e nn_direction_translation det_direction_each appos_alignment_e appos_alignment_f prep_for_alignment_direction num_alignment_one dobj_compute_alignment nsubj_compute_we num_Ney_2003 conj_and_Och_Ney conj_and_follow_combine conj_and_follow_compute dep_follow_Ney dep_follow_Och nsubj_follow_we vmod_follow_Using nn_strategy_alignment det_strategy_this dobj_Using_strategy
J05-4003	P03-1021	o	All our MT systems were trained using a variant of the alignment template model described in -LRB- Och 2003 -RRB-	dep_Och_2003 dep_in_Och prep_described_in vmod_model_described nn_model_template nn_model_alignment det_model_the prep_of_variant_model det_variant_a dobj_using_variant xcomp_trained_using auxpass_trained_were nsubjpass_trained_systems nn_systems_MT poss_systems_our predet_systems_All
J05-4005	P03-1021	o	It is also related to -LRB- log -RRB- linear models described in Berger Della Pietra and Della Pietra -LRB- 1996 -RRB- Xue -LRB- 2003 -RRB- Och -LRB- 2003 -RRB- and Peng Feng and McCallum -LRB- 2004 -RRB-	appos_McCallum_2004 conj_and_Peng_McCallum conj_and_Peng_Feng appos_Och_2003 appos_Xue_2003 appos_Pietra_1996 nn_Pietra_Della nn_Pietra_Della conj_and_Berger_McCallum conj_and_Berger_Feng conj_and_Berger_Peng conj_and_Berger_Och conj_and_Berger_Xue conj_and_Berger_Pietra conj_and_Berger_Pietra prep_in_described_Peng prep_in_described_Och prep_in_described_Xue prep_in_described_Pietra prep_in_described_Pietra prep_in_described_Berger vmod_models_described amod_models_linear dep_models_log prep_to_related_models advmod_related_also auxpass_related_is nsubjpass_related_It
J06-4002	P03-1021	o	Furthermore statistical generation systems -LRB- Lapata 2003 Barzilay and Lee 2004 Karamanis and Manurung 2002 Mellish et al. 1998 -RRB- could use as a means of directly optimizing information ordering much in the same way MT systems optimize model parameters using BLEU as a measure of translation quality -LRB- Och 2003 -RRB-	num_Och_2003 appos_quality_Och nn_quality_translation prep_of_measure_quality det_measure_a prep_as_using_measure dobj_using_BLEU nn_parameters_model vmod_optimize_using dobj_optimize_parameters nsubj_optimize_systems advmod_optimize_much nn_systems_MT amod_way_same det_way_the prep_in_much_way ccomp_ordering_optimize vmod_information_ordering amod_information_optimizing advmod_optimizing_directly prep_of_means_information det_means_a prep_as_use_means aux_use_could nsubj_use_systems advmod_use_Furthermore num_al._1998 nn_al._et nn_al._Mellish num_Karamanis_2002 conj_and_Karamanis_Manurung dep_Barzilay_2004 conj_and_Barzilay_Lee dep_2003_al. conj_2003_Manurung conj_2003_Karamanis conj_2003_Lee conj_2003_Barzilay dep_2003_Lapata dep_systems_2003 nn_systems_generation amod_systems_statistical
J07-1003	P03-1021	o	These weights or scaling factors can be optimized with respect to some evaluation criterion -LRB- Och 2003 -RRB-	num_Och_2003 appos_criterion_Och nn_criterion_evaluation det_criterion_some prep_with_respect_to_optimized_criterion auxpass_optimized_be aux_optimized_can nsubjpass_optimized_factors nsubjpass_optimized_weights amod_factors_scaling conj_or_weights_factors det_weights_These
J07-1003	P03-1021	o	Nowadays most state-of-the-art SMT systems are based on bilingual phrases -LRB- Och Tillmann and Ney 1999 Koehn Och and Marcu 2003 Tillmann 2003 Bertoldi et al. 2004 Vogel et al. 2004 Zens and Ney 2004 Chiang 2005 -RRB-	num_Chiang_2005 num_Ney_2004 conj_and_Zens_Ney nn_2004_al. num_Vogel_2004 nn_Vogel_et num_al._2004 nn_al._et nn_al._Bertoldi num_Tillmann_2003 num_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och num_Ney_1999 dep_Och_Chiang conj_and_Och_Ney conj_and_Och_Zens conj_and_Och_Vogel conj_and_Och_al. conj_and_Och_Tillmann conj_and_Och_Marcu conj_and_Och_Och conj_and_Och_Koehn conj_and_Och_Ney conj_and_Och_Tillmann dep_phrases_Zens dep_phrases_Vogel dep_phrases_al. dep_phrases_Tillmann dep_phrases_Koehn dep_phrases_Ney dep_phrases_Tillmann dep_phrases_Och amod_phrases_bilingual prep_on_based_phrases auxpass_based_are nsubjpass_based_systems advmod_based_Nowadays nn_systems_SMT amod_systems_state-of-the-art advmod_state-of-the-art_most ccomp_``_based
J07-1003	P03-1021	o	The model scaling factors 1 5 and the word and phrase penalties are optimized with respect to some evaluation criterion -LRB- Och 2003 -RRB- such as BLEU score	nn_score_BLEU num_Och_2003 prep_such_as_criterion_score appos_criterion_Och nn_criterion_evaluation det_criterion_some prep_with_respect_to_optimized_criterion auxpass_optimized_are nsubjpass_optimized_factors num_penalties_phrase num_penalties_word num_penalties_5 num_penalties_1 det_word_the conj_and_1_phrase conj_and_1_word conj_and_1_5 dep_factors_penalties nn_factors_scaling nn_factors_model det_factors_The
J07-2003	P03-1021	o	4.2 Features For our experiments we use a feature set analogous to the default feature set of Pharaoh -LRB- Koehn Och and Marcu 2003 -RRB-	num_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och dep_Pharaoh_Marcu dep_Pharaoh_Och dep_Pharaoh_Koehn prep_of_set_Pharaoh nn_set_feature nn_set_default det_set_the prep_to_analogous_set acomp_set_analogous vmod_feature_set det_feature_a dobj_use_feature nsubj_use_we dep_use_Features poss_experiments_our prep_for_Features_experiments num_Features_4.2
J07-2003	P03-1021	o	The rules extracted from the training bitext have the following features a114 P -LRB- | -RRB- andP -LRB- | -RRB- the latter of which is not found in the noisy-channel model but has been previously found to be a helpful feature -LRB- Och and Ney 2002 -RRB- 210 Chiang Hierarchical Phrase-Based Translation a114 the lexical weights P w -LRB- | -RRB- andP w -LRB- | -RRB- which estimate how well the words in translate the words in -LRB- Koehn Och and Marcu 2003 -RRB- 4 a114 a penalty exp -LRB- 1 -RRB- for extracted rules analogous to Koehns phrase penalty -LRB- Koehn 2003 -RRB- which allows the model to learn a preference for longer or shorter derivations	amod_derivations_shorter amod_derivations_longer conj_or_longer_shorter prep_for_preference_derivations det_preference_a dobj_learn_preference aux_learn_to det_model_the xcomp_allows_learn dobj_allows_model nsubj_allows_which num_Koehn_2003 rcmod_penalty_allows appos_penalty_Koehn nn_penalty_phrase nn_penalty_Koehns prep_to_analogous_penalty amod_rules_extracted prep_for_exp_rules appos_exp_1 nn_exp_penalty det_exp_a amod_a114_analogous dep_a114_exp num_a114_4 num_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och dep_in_Marcu dep_in_Och dep_in_Koehn prep_words_in det_words_the dobj_translate_words prepc_in_words_translate det_words_the pobj_well_words advmod_well_how prep_estimate_well nsubj_estimate_which rcmod_w_estimate appos_w_| nn_w_andP nn_w_w appos_w_| nn_w_P dep_weights_w amod_weights_lexical det_weights_the prep_a114_Translation_weights nn_Translation_Phrase-Based nn_Translation_Hierarchical nn_Translation_Chiang num_Translation_210 num_Ney_2002 conj_and_Och_Ney dep_feature_Ney dep_feature_Och amod_feature_helpful det_feature_a cop_feature_be aux_feature_to xcomp_found_feature advmod_found_previously auxpass_found_been aux_found_has amod_model_noisy-channel det_model_the prep_in_found_model neg_found_not auxpass_found_is nsubjpass_found_latter prep_of_latter_which det_latter_the appos_andP_| conj_but_P_a114 conj_but_P_Translation conj_but_P_found conj_but_P_found dep_P_andP appos_P_| nn_P_a114 amod_features_following det_features_the dep_have_a114 dep_have_Translation dep_have_found dep_have_found dep_have_P dobj_have_features nsubj_have_rules dep_training_bitext det_training_the prep_from_extracted_training vmod_rules_extracted det_rules_The rcmod_``_have
J07-2003	P03-1021	o	Finally the parameters i of the log-linear model -LRB- 18 -RRB- are learned by minimumerror-rate training -LRB- Och 2003 -RRB- which tries to set the parameters so as to maximize the BLEU score -LRB- Papineni et al. 2002 -RRB- of a development set	nn_set_development det_set_a dep_2002_al. nn_al._et num_Papineni_2002 nn_score_BLEU det_score_the prep_of_maximize_set dep_maximize_Papineni dobj_maximize_score aux_maximize_to det_parameters_the prepc_as_set_maximize advmod_set_so dobj_set_parameters aux_set_to xcomp_tries_set nsubj_tries_which num_Och_2003 rcmod_training_tries appos_training_Och amod_training_minimumerror-rate agent_learned_training auxpass_learned_are dep_learned_i nsubjpass_learned_parameters advmod_learned_Finally appos_model_18 amod_model_log-linear det_model_the prep_of_i_model det_parameters_the
J07-2003	P03-1021	o	But Koehn Och and Marcu -LRB- 2003 -RRB- find that phrases longer than three words improve performance little for training corpora of up to 20 million words suggesting that the data may be too sparse to learn longer phrases	amod_phrases_longer dobj_learn_phrases aux_learn_to xcomp_sparse_learn advmod_sparse_too cop_sparse_be aux_sparse_may nsubj_sparse_data mark_sparse_that det_data_the ccomp_suggesting_sparse num_words_million number_million_20 dep_million_to quantmod_million_up prep_of_corpora_words nn_corpora_training prep_for_little_corpora amod_performance_little dobj_improve_performance vmod_words_improve num_words_three prep_than_longer_words vmod_phrases_suggesting amod_phrases_longer dep_that_phrases dobj_find_that nsubj_find_Marcu nsubj_find_Och nsubj_find_Koehn cc_find_But appos_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och
J07-2003	P03-1021	o	Above the phrase level some models perform no reordering -LRB- Zens and Ney 2004 Kumar Deng and Byrne 2006 -RRB- some have a simple distortion model that reorders phrases independently of their content -LRB- Koehn Och and Marcu 2003 Och and Ney 2004 -RRB- and some for example the Alignment Template System -LRB- Och et al. 2004 Thayer et al. 2004 -RRB- hereafter ATS and the IBM phrase-based system -LRB- Tillmann 2004 Tillmann and Zhang 2005 -RRB- have phrase-reordering models that add some lexical sensitivity	amod_sensitivity_lexical det_sensitivity_some dobj_add_sensitivity nsubj_add_that rcmod_models_add amod_models_phrase-reordering dobj_have_models nsubj_have_system nsubj_have_System prep_for_have_example nsubj_have_some num_Zhang_2005 conj_and_Tillmann_Zhang dep_Tillmann_Zhang dep_Tillmann_Tillmann num_Tillmann_2004 appos_system_Tillmann amod_system_phrase-based nn_system_IBM det_system_the advmod_ATS_hereafter dep_al._2004 nn_al._et nn_al._Thayer num_al._2004 nn_al._et dep_Och_al. dep_Och_al. conj_and_System_system appos_System_ATS appos_System_Och nn_System_Template nn_System_Alignment det_System_the num_Ney_2004 conj_and_Och_Ney num_Marcu_2003 dep_Koehn_Ney dep_Koehn_Och conj_and_Koehn_Marcu conj_and_Koehn_Och dep_content_Marcu dep_content_Och dep_content_Koehn poss_content_their prep_of_independently_content advmod_reorders_independently dobj_reorders_phrases nsubj_reorders_that rcmod_model_reorders nn_model_distortion amod_model_simple det_model_a conj_and_have_have dobj_have_model nsubj_have_some num_Byrne_2006 conj_and_Kumar_Byrne conj_and_Kumar_Deng num_Ney_2004 dep_Zens_Byrne dep_Zens_Deng dep_Zens_Kumar conj_and_Zens_Ney dep_reordering_Ney dep_reordering_Zens neg_reordering_no dep_perform_have dep_perform_have dobj_perform_reordering nsubj_perform_models prep_above_perform_level det_models_some nn_level_phrase det_level_the
J07-2003	P03-1021	o	Phrases of up to 10 in length on the French side were extracted from the parallel text and minimum-error-rate training -LRB- Och 2003 -RRB- was 8 We can train on the full training data shown if tighter constraints are placed on rule extraction for the United Nations data	nn_data_Nations nn_data_United det_data_the prep_for_extraction_data nn_extraction_rule prep_on_placed_extraction auxpass_placed_are nsubjpass_placed_constraints mark_placed_if amod_constraints_tighter advcl_shown_placed vmod_data_shown nn_data_training amod_data_full det_data_the prep_on_train_data aux_train_can nsubj_train_We rcmod_8_train cop_8_was nsubj_8_training num_Och_2003 appos_training_Och amod_training_minimum-error-rate amod_text_parallel det_text_the conj_and_extracted_8 prep_from_extracted_text auxpass_extracted_were nsubjpass_extracted_Phrases amod_side_French det_side_the prep_on_up_side prep_in_up_length prep_to_up_10 pobj_of_up prep_Phrases_of
J07-2003	P03-1021	p	Other insights borrowed from the current state of the art include minimum-error-rate training of log-linear models -LRB- Och and Ney 2002 Och 2003 -RRB- and use of an m-gram language model	nn_model_language amod_model_m-gram det_model_an prep_of_use_model num_Och_2003 num_Ney_2002 dep_Och_Och conj_and_Och_Ney appos_models_Ney appos_models_Och amod_models_log-linear conj_and_training_use prep_of_training_models amod_training_minimum-error-rate dobj_include_use dobj_include_training nsubj_include_insights det_art_the prep_of_state_art amod_state_current det_state_the prep_from_borrowed_state vmod_insights_borrowed amod_insights_Other
J07-3002	P03-1021	o	Some of the alignment sets also have links which are not Sure links but are Possible links -LRB- Och and Ney 2003 -RRB-	num_Ney_2003 conj_and_Och_Ney dep_links_Ney dep_links_Och amod_links_Possible cop_links_are conj_but_links_links amod_links_Sure neg_links_not cop_links_are nsubj_links_which rcmod_links_links rcmod_links_links dobj_have_links advmod_have_also nsubj_have_Some nn_sets_alignment det_sets_the prep_of_Some_sets
J07-3002	P03-1021	o	We also have an additional held-out translation set the development set which is employed by the MT system to train the weights of its log-linear model to maximize BLEU -LRB- Och 2003 -RRB-	num_Och_2003 appos_BLEU_Och dobj_maximize_BLEU aux_maximize_to amod_model_log-linear poss_model_its prep_of_weights_model det_weights_the vmod_train_maximize dobj_train_weights aux_train_to nn_system_MT det_system_the xcomp_employed_train agent_employed_system auxpass_employed_is nsubjpass_employed_which rcmod_set_employed nn_set_development det_set_the nn_set_translation amod_set_held-out amod_set_additional det_set_an dep_have_set dobj_have_set advmod_have_also nsubj_have_We ccomp_``_have
J07-3002	P03-1021	o	The training data for the French/English data set is taken from the LDC Canadian Hansard data set from which the word aligned data -LRB- presented in Och and Ney 2003 -RRB- was also taken	advmod_taken_also auxpass_taken_was nsubjpass_taken_word prep_from_taken_which num_Ney_2003 conj_and_Och_Ney prep_in_presented_Ney prep_in_presented_Och dep_data_presented amod_data_aligned dep_word_data det_word_the rcmod_set_taken nn_set_data nn_set_Hansard nn_set_Canadian nn_set_LDC det_set_the prep_from_taken_set auxpass_taken_is nsubjpass_taken_data nn_set_data nn_set_French/English det_set_the prep_for_data_set nn_data_training det_data_The
J07-3002	P03-1021	o	294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 2.2 Measuring Translation Performance Changes Caused By Alignment In phrased-based SMT -LRB- Koehn Och and Marcu 2003 -RRB- the knowledge sources which vary with the word alignment are the phrase translation lexicon -LRB- which maps source phrases to target phrases using counts from the word alignment -RRB- and some of the word level translation parameters -LRB- sometimes called lexical smoothing -RRB-	amod_smoothing_lexical dep_called_smoothing advmod_called_sometimes dep_parameters_called nn_parameters_translation nn_parameters_level nn_parameters_word det_parameters_the prep_of_some_parameters nn_alignment_word det_alignment_the prep_from_using_alignment dobj_using_counts vmod_target_using dobj_target_phrases aux_target_to nn_phrases_source vmod_maps_target dobj_maps_phrases nsubj_maps_which rcmod_lexicon_maps nn_lexicon_translation nn_lexicon_phrase det_lexicon_the conj_and_are_some dobj_are_lexicon nsubj_are_Quality nsubj_are_Fraser nn_alignment_word det_alignment_the prep_with_vary_alignment nsubj_vary_which rcmod_sources_vary nn_sources_knowledge det_sources_the num_Marcu_2003 conj_and_Koehn_Marcu conj_and_Koehn_Och dep_SMT_Marcu dep_SMT_Och dep_SMT_Koehn amod_SMT_phrased-based prep_in_Alignment_SMT dobj_Caused_sources prep_by_Caused_Alignment nsubj_Caused_Changes nn_Changes_Performance nn_Changes_Translation amod_Changes_Measuring num_Changes_2.2 rcmod_Translation_Caused nn_Translation_Machine nn_Translation_Statistical nn_Quality_Alignment nn_Quality_Word nn_Quality_Measuring nn_Quality_Marcu prep_for_Fraser_Translation conj_and_Fraser_Quality num_Fraser_294 ccomp_``_some ccomp_``_are
J07-3002	P03-1021	o	The weights of the different knowledge sources in the log-linear model used by our system are trained using Maximum BLEU -LRB- Och 2003 -RRB- which we run for 25 iterations individually for each system	det_system_each num_iterations_25 prep_for_run_system advmod_run_individually prep_for_run_iterations nsubj_run_we dobj_run_which num_Och_2003 rcmod_BLEU_run appos_BLEU_Och nn_BLEU_Maximum dobj_using_BLEU xcomp_trained_using auxpass_trained_are nsubjpass_trained_weights poss_system_our agent_used_system vmod_model_used amod_model_log-linear det_model_the nn_sources_knowledge amod_sources_different det_sources_the prep_in_weights_model prep_of_weights_sources det_weights_The ccomp_``_trained
J07-3002	P03-1021	o	To generate word alignments we use GIZA + + -LRB- Och and Ney 2003 -RRB- which implements both the IBM Models of Brown et al.	nn_al._et nn_al._Brown prep_of_Models_al. nn_Models_IBM det_Models_the preconj_Models_both dobj_implements_Models nsubj_implements_which num_Ney_2003 conj_and_Och_Ney dep_+_Ney dep_+_Och rcmod_GIZA_implements conj_+_GIZA_+ dobj_use_+ dobj_use_GIZA nsubj_use_we nn_alignments_word ccomp_generate_use dobj_generate_alignments aux_generate_To
J07-3002	P03-1021	o	The output of GIZA + + is then post-processed using the three symmetrization heuristics described in Och and Ney -LRB- 2003 -RRB-	appos_Ney_2003 conj_and_Och_Ney prep_in_described_Ney prep_in_described_Och vmod_heuristics_described nn_heuristics_symmetrization num_heuristics_three det_heuristics_the dobj_using_heuristics xcomp_post-processed_using advmod_post-processed_then cop_post-processed_is nsubj_post-processed_output conj_+_GIZA_+ prep_of_output_+ prep_of_output_GIZA det_output_The
J07-3002	P03-1021	o	Word Alignment Quality Metrics 3.1 Alignment Error Rate is Not a Useful Measure We begin our study of metrics for word alignment quality by testing AER -LRB- Och and Ney 2003 -RRB-	num_Ney_2003 conj_and_Och_Ney dep_AER_Ney dep_AER_Och dobj_testing_AER nn_quality_alignment nn_quality_word prep_for_metrics_quality prep_of_study_metrics poss_study_our prepc_by_begin_testing dobj_begin_study nsubj_begin_We ccomp_Measure_begin nn_Measure_Useful det_Measure_a neg_Measure_Not cop_Measure_is nsubj_Measure_Rate nsubj_Measure_Metrics nn_Rate_Error nn_Rate_Alignment num_Rate_3.1 nn_Metrics_Quality nn_Metrics_Alignment nn_Metrics_Word
J07-3002	P03-1021	o	Och and Ney -LRB- 2003 -RRB- state that AER is derived from F-Measure	prep_from_derived_F-Measure auxpass_derived_is nsubjpass_derived_AER dobj_derived_that rcmod_state_derived num_state_2003 dep_Ney_state conj_and_Och_Ney
N04-1008	P03-1021	p	4.4.1 N-gram Co-Occurrence Statistics for Answer Extraction N-gram co-occurrence statistics have been successfully used in automatic evaluation -LRB- Papineni et al. 2002 Lin and Hovy 2003 -RRB- and more recently as training criteria in statistical machine translation -LRB- Och 2003 -RRB-	num_Och_2003 appos_translation_Och nn_translation_machine amod_translation_statistical prep_in_criteria_translation nn_criteria_training prep_as_recently_criteria advmod_recently_more num_Hovy_2003 conj_and_Papineni_Hovy conj_and_Papineni_Lin dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_evaluation_automatic advmod_used_recently cc_used_and dep_used_Hovy dep_used_Lin dep_used_Papineni prep_in_used_evaluation advmod_used_successfully auxpass_used_been aux_used_have nsubjpass_used_Statistics nn_statistics_co-occurrence nn_statistics_N-gram nn_statistics_Extraction nn_statistics_Answer prep_for_Statistics_statistics nn_Statistics_Co-Occurrence nn_Statistics_N-gram num_Statistics_4.4.1
N04-1021	P03-1021	o	However certain properties of the BLEU metric can be exploited to speed up search as described in detail by Och -LRB- 2003 -RRB-	appos_Och_2003 prep_by_described_Och prep_in_described_detail mark_described_as dobj_speed_search prt_speed_up aux_speed_to advcl_exploited_described xcomp_exploited_speed auxpass_exploited_be aux_exploited_can nsubjpass_exploited_properties advmod_exploited_However nn_metric_BLEU det_metric_the prep_of_properties_metric amod_properties_certain ccomp_``_exploited
N04-1022	P03-1021	o	For all performance metrics we show the 70 % confidence interval with respect to the MAP baseline computed using bootstrap resampling -LRB- Press et al. 2002 Och 2003 -RRB-	dep_Och_2003 dep_Press_Och appos_Press_2002 dep_Press_al. nn_Press_et dep_resampling_Press nn_resampling_bootstrap dobj_using_resampling xcomp_computed_using nn_baseline_MAP det_baseline_the nn_interval_confidence amod_interval_% det_interval_the number_%_70 dep_show_computed prep_with_respect_to_show_baseline dobj_show_interval nsubj_show_we prep_for_show_metrics nn_metrics_performance det_metrics_all
N04-1022	P03-1021	o	Och -LRB- 2003 -RRB- developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models	nn_models_MT amod_models_log-linear prep_of_procedure_models nn_procedure_training det_procedure_the prep_in_criteria_procedure nn_criteria_evaluation nn_criteria_MT amod_criteria_various dobj_incorporates_criteria nsubj_incorporates_that rcmod_procedure_incorporates nn_procedure_training det_procedure_a dobj_developed_procedure nsubj_developed_Och appos_Och_2003
N04-1023	P03-1021	p	Recently so-called reranking techniques such as maximum entropy models -LRB- Och and Ney 2002 -RRB- and gradient methods -LRB- Och 2003 -RRB- have been applied to machine translation -LRB- MT -RRB- and have provided significant improvements	amod_improvements_significant dobj_provided_improvements aux_provided_have nsubjpass_provided_techniques appos_translation_MT nn_translation_machine conj_and_applied_provided prep_to_applied_translation auxpass_applied_been aux_applied_have nsubjpass_applied_techniques appos_Och_2003 dep_methods_Och nn_methods_gradient amod_Och_2002 conj_and_Och_Ney conj_and_models_methods dep_models_Ney dep_models_Och amod_models_entropy amod_models_maximum prep_such_as_techniques_methods prep_such_as_techniques_models nn_techniques_reranking amod_techniques_so-called advmod_techniques_Recently
N04-1023	P03-1021	o	The minimum error training -LRB- Och 2003 -RRB- was used on the development data for parameter estimation	nn_estimation_parameter nn_data_development det_data_the prep_for_used_estimation prep_on_used_data auxpass_used_was nsubjpass_used_training dep_Och_2003 dep_training_Och nn_training_error amod_training_minimum det_training_The
N04-1023	P03-1021	o	Six features from -LRB- Och 2003 -RRB- were used as baseline features	nn_features_baseline prep_as_used_features auxpass_used_were nsubjpass_used_features appos_Och_2003 prep_from_features_Och num_features_Six
N04-1023	P03-1021	o	In our experiments we will use 4 different kinds of feature combinations a157 Baseline The 6 baseline features used in -LRB- Och 2003 -RRB- such as cost of word penalty cost of aligned template penalty	nn_penalty_template amod_penalty_aligned prep_of_cost_penalty nn_penalty_word prep_of_cost_penalty appos_Och_cost prep_such_as_Och_cost dep_Och_2003 prep_in_used_Och vmod_features_used nn_features_baseline num_features_6 det_features_The dep_Baseline_features num_Baseline_a157 nn_combinations_feature prep_of_kinds_combinations amod_kinds_different num_kinds_4 dep_use_Baseline dobj_use_kinds aux_use_will nsubj_use_we prep_in_use_experiments poss_experiments_our
N04-1023	P03-1021	o	Och -LRB- 2003 -RRB- described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU	prep_such_as_metrics_BLEU nn_metrics_evaluation nn_metrics_MT amod_metrics_automatic nn_rate_error det_rate_the prep_on_optimizing_metrics dobj_optimizing_rate advmod_optimizing_directly nn_training_error amod_training_minimum vmod_use_optimizing prep_of_use_training det_use_the dobj_described_use nsubj_described_Och appos_Och_2003
N04-1023	P03-1021	o	SMT Team -LRB- 2003 -RRB- also used minimum error training as in Och -LRB- 2003 -RRB- but used a large number of feature functions	nn_functions_feature prep_of_number_functions amod_number_large det_number_a dobj_used_number nsubj_used_Team appos_Och_2003 pobj_in_Och pcomp_as_in nn_training_error amod_training_minimum conj_but_used_used prep_used_as dobj_used_training advmod_used_also nsubj_used_Team dep_Team_2003 nn_Team_SMT
N04-1023	P03-1021	o	By reranking a 1000-best list generated by the baseline MT system from Och -LRB- 2003 -RRB- the BLEU -LRB- Papineni et al. 2001 -RRB- score on the test dataset was improved from 31.6 % to 32.9 %	num_%_32.9 num_%_31.6 prep_to_improved_% prep_from_improved_% auxpass_improved_was nsubjpass_improved_BLEU agent_improved_reranking nn_dataset_test det_dataset_the prep_on_score_dataset dep_Papineni_2001 dep_Papineni_al. nn_Papineni_et dep_BLEU_score dep_BLEU_Papineni det_BLEU_the appos_Och_2003 prep_from_system_Och nn_system_MT nn_system_baseline det_system_the agent_generated_system vmod_list_generated amod_list_1000-best det_list_a dobj_reranking_list rcmod_``_improved
N04-1033	P03-1021	o	The model scaling factors are optimized on the development corpus with respect to mWER similar to -LRB- Och 2003 -RRB-	amod_Och_2003 dep_to_Och prep_similar_to amod_mWER_similar nn_corpus_development det_corpus_the prep_with_respect_to_optimized_mWER prep_on_optimized_corpus auxpass_optimized_are nsubjpass_optimized_factors nn_factors_scaling nn_factors_model det_factors_The
N04-1033	P03-1021	n	This method has the advantage that it is not limited to the model scaling factors as the method described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_method_described det_method_the nn_factors_scaling nn_factors_model det_factors_the prep_as_limited_method prep_to_limited_factors neg_limited_not auxpass_limited_is nsubjpass_limited_it mark_limited_that ccomp_advantage_limited det_advantage_the dobj_has_advantage nsubj_has_method det_method_This
N04-1033	P03-1021	o	Alternatively one can train them with respect to the final translation quality measured by some error criterion -LRB- Och 2003 -RRB-	amod_Och_2003 appos_criterion_Och nn_criterion_error det_criterion_some agent_measured_criterion vmod_quality_measured nn_quality_translation amod_quality_final det_quality_the prep_with_respect_to_train_quality dobj_train_them aux_train_can nsubj_train_one advmod_train_Alternatively
N06-1002	P03-1021	o	Word alignments were produced by GIZA + + -LRB- Och and Ney 2003 -RRB- with a standard training regimen of five iterations of Model 1 five iterations of the HMM Model and five iterations of Model 4 in both directions	det_directions_both num_Model_4 prep_of_iterations_Model num_iterations_five nn_Model_HMM det_Model_the prep_of_iterations_Model num_iterations_five conj_and_Model_iterations conj_and_Model_iterations num_Model_1 prep_of_iterations_iterations prep_of_iterations_iterations prep_of_iterations_Model num_iterations_five prep_of_regimen_iterations nn_regimen_training amod_regimen_standard det_regimen_a num_Ney_2003 conj_and_Och_Ney dep_+_Ney dep_+_Och prep_in_GIZA_directions prep_with_GIZA_regimen conj_+_GIZA_+ agent_produced_+ agent_produced_GIZA auxpass_produced_were nsubjpass_produced_alignments nn_alignments_Word
N06-1002	P03-1021	o	Finally we trained model weights by maximizing BLEU -LRB- Och 2003 -RRB- and set decoder optimization parameters -LRB- n-best list size timeouts 14 etc -RRB- on a development test set of 200 held-out sentences each with a single reference translation	nn_translation_reference amod_translation_single det_translation_a prep_with_each_translation amod_sentences_held-out num_sentences_200 dep_set_each prep_of_set_sentences nn_set_test nn_set_development det_set_a num_etc_14 dep_etc_timeouts appos_size_etc nn_size_list amod_size_n-best dep_parameters_size nn_parameters_optimization nn_parameters_decoder amod_parameters_set num_Och_2003 appos_BLEU_Och prep_on_maximizing_set conj_and_maximizing_parameters dobj_maximizing_BLEU nn_weights_model prepc_by_trained_parameters prepc_by_trained_maximizing dobj_trained_weights nsubj_trained_we advmod_trained_Finally
N06-1002	P03-1021	o	We used the heuristic combination described in -LRB- Och and Ney 2003 -RRB- and extracted phrasal translation pairs from this combined alignment as described in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in mark_described_as amod_alignment_combined det_alignment_this nn_pairs_translation amod_pairs_phrasal amod_pairs_extracted num_Ney_2003 prep_from_Och_alignment conj_and_Och_pairs conj_and_Och_Ney advcl_described_described prep_in_described_pairs prep_in_described_Ney prep_in_described_Och vmod_combination_described nn_combination_heuristic det_combination_the dobj_used_combination nsubj_used_We
N06-1002	P03-1021	o	Model weights were also trained following Och -LRB- 2003 -RRB-	appos_Och_2003 prep_following_trained_Och advmod_trained_also auxpass_trained_were nsubjpass_trained_weights nn_weights_Model
N06-1003	P03-1021	o	2 The Problem of Coverage in SMT Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn appos_Marcu_2002 conj_and_Marcu_Wong dep_translation_Wong dep_translation_Marcu amod_translation_phrase-based prep_of_introduction_translation det_introduction_the nn_quality_translation prep_in_advances_quality amod_advances_considerable prep_with_made_introduction dobj_made_advances nsubj_made_Problem nn_translation_machine amod_translation_Statistical nn_translation_SMT prep_in_Coverage_translation prep_of_Problem_Coverage det_Problem_The num_Problem_2
N06-1003	P03-1021	o	To set the weights m we performed minimum error rate training -LRB- Och 2003 -RRB- on the development set using Bleu -LRB- Papineni et al. 2002 -RRB- as the objective function	amod_function_objective det_function_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni prep_as_using_function dobj_using_Bleu xcomp_set_using vmod_development_set det_development_the appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_on_performed_development dobj_performed_training nsubj_performed_we tmod_performed_m advcl_performed_set det_weights_the dobj_set_weights aux_set_To
N06-1004	P03-1021	o	Weights on the components were assigned using the -LRB- Och 2003 -RRB- method for max-BLEU training on the development set	nn_set_development det_set_the prep_on_training_set amod_training_max-BLEU prep_for_method_training dep_method_Och det_method_the dep_Och_2003 dobj_using_method xcomp_assigned_using auxpass_assigned_were nsubjpass_assigned_Weights det_components_the prep_on_Weights_components
N06-1004	P03-1021	o	1 Introduction Defining SCMs The work presented here was done in the context of phrase-based MT -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_MT_phrase-based prep_of_context_MT det_context_the prep_in_done_context auxpass_done_was nsubjpass_done_work advmod_presented_here vmod_work_presented det_work_The rcmod_SCMs_done dep_Defining_Koehn dobj_Defining_SCMs dep_Introduction_Defining num_Introduction_1
N06-1013	P03-1021	o	The parameters of the MT system were optimized on MTEval02 data using minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training nn_data_MTEval02 xcomp_optimized_using prep_on_optimized_data auxpass_optimized_were nsubjpass_optimized_parameters nn_system_MT det_system_the prep_of_parameters_system det_parameters_The
N06-1013	P03-1021	p	In a later study Och and Ney -LRB- 2003 -RRB- present a loglinear combination of the HMM and IBM Model 4 that produces better alignments than either of those	prep_of_either_those prep_than_alignments_either amod_alignments_better dobj_produces_alignments nsubj_produces_that num_Model_4 nn_Model_IBM rcmod_HMM_produces conj_and_HMM_Model det_HMM_the prep_of_combination_Model prep_of_combination_HMM amod_combination_loglinear det_combination_a dobj_present_combination nsubj_present_Ney nsubj_present_Och prep_in_present_study appos_Ney_2003 conj_and_Och_Ney amod_study_later det_study_a
N06-1013	P03-1021	o	1 Introduction Word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation -LRB- MT -RRB- -LRB- Brown et al. 1993 Och and Ney 2003 Koehn et al. 2003 -RRB- but also has been shown useful for other applications such as construction of bilingual lexicons word-sense disambiguation projection of resources and crosslanguage information retrieval	nn_retrieval_information nn_retrieval_crosslanguage prep_of_projection_resources amod_disambiguation_word-sense conj_and_lexicons_retrieval conj_and_lexicons_projection conj_and_lexicons_disambiguation amod_lexicons_bilingual prep_of_construction_retrieval prep_of_construction_projection prep_of_construction_disambiguation prep_of_construction_lexicons prep_such_as_applications_construction amod_applications_other prep_for_useful_applications acomp_shown_useful auxpass_shown_been aux_shown_has advmod_shown_also nn_al._et nn_al._Koehn conj_and_Och_2003 conj_and_Och_Ney amod_Brown_2003 dep_Brown_al. dep_Brown_2003 dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_translation_MT nn_translation_machine amod_translation_statistical dep_step_Brown prep_of_step_translation amod_step_intermediate det_step_an advmod_step_usually nsubj_step_otheris det_otheris_each prep_of_translations_step cop_translations_are nsubj_translations_that rcmod_sentences_translations num_sentences_two prep_between_words_sentences amod_words_corresponding conj_but_alignmentdetection_shown prep_of_alignmentdetection_words nn_alignmentdetection_Word nn_alignmentdetection_Introduction num_alignmentdetection_1 dep_``_shown dep_``_alignmentdetection
N06-1013	P03-1021	o	Maximum entropy -LRB- ME -RRB- models have been used in bilingual sense disambiguation word reordering and sentence segmentation -LRB- Berger et al. 1996 -RRB- parsing POS tagging and PP attachment -LRB- Ratnaparkhi 1998 -RRB- machine translation -LRB- Och and Ney 2002 -RRB- and FrameNet classification -LRB- Fleischman et al. 2003 -RRB-	amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et nn_classification_FrameNet dep_Och_2002 conj_and_Och_Ney appos_translation_Ney appos_translation_Och nn_translation_machine amod_Ratnaparkhi_1998 nn_attachment_PP nn_tagging_POS dep_parsing_Fleischman conj_and_parsing_classification conj_and_parsing_translation dep_parsing_Ratnaparkhi conj_and_parsing_attachment conj_and_parsing_tagging amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_segmentation_Berger nn_segmentation_sentence nn_reordering_word nn_disambiguation_sense amod_disambiguation_bilingual conj_and_used_classification conj_and_used_translation conj_and_used_attachment conj_and_used_tagging conj_and_used_parsing conj_and_used_segmentation conj_and_used_reordering prep_in_used_disambiguation auxpass_used_been aux_used_have nsubjpass_used_models nn_models_entropy appos_entropy_ME nn_entropy_Maximum
N06-1032	P03-1021	o	number of words in target string These statistics are combined into a log-linear model whose parameters are adjusted by minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum agent_adjusted_training auxpass_adjusted_are nsubjpass_adjusted_parameters poss_parameters_whose rcmod_model_adjusted amod_model_log-linear det_model_a prep_into_combined_model auxpass_combined_are nsubjpass_combined_number det_statistics_These nn_statistics_string nn_statistics_target prep_in_number_statistics prep_of_number_words
N06-1032	P03-1021	o	Minimum-error-rate training was done using Koehns implementation of Ochs -LRB- 2003 -RRB- minimum-error-rate model	amod_model_minimum-error-rate num_model_2003 nn_model_Ochs prep_of_implementation_model nn_implementation_Koehns dobj_using_implementation xcomp_done_using auxpass_done_was nsubjpass_done_training amod_training_Minimum-error-rate
N06-1032	P03-1021	o	-LRB- 2003 -RRB- and component weights are adjusted by minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum agent_adjusted_training auxpass_adjusted_are nsubjpass_adjusted_weights nsubjpass_adjusted_2003 nn_weights_component conj_and_2003_weights
N06-1032	P03-1021	o	1 Introduction Recent approaches to statistical machine translation -LRB- SMT -RRB- piggyback on the central concepts of phrasebased SMT -LRB- Och et al. 1999 Koehn et al. 2003 -RRB- and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process	nn_process_translation det_process_the prep_in_knowledge_process amod_knowledge_syntactic dobj_incorporating_knowledge poss_shortcomings_its prep_of_some_shortcomings prepc_by_improve_incorporating dobj_improve_some aux_improve_to vmod_attempt_improve nn_attempt_time amod_attempt_same det_attempt_the num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn appos_Och_1999 dep_Och_al. nn_Och_et appos_SMT_Och amod_SMT_phrasebased prep_of_concepts_SMT amod_concepts_central det_concepts_the nn_piggyback_translation appos_translation_SMT nn_translation_machine amod_translation_statistical prep_at_approaches_attempt prep_on_approaches_concepts prep_to_approaches_piggyback conj_and_approaches_approaches amod_approaches_Recent dep_Introduction_approaches dep_Introduction_approaches num_Introduction_1
N06-2013	P03-1021	o	Decoding weights are optimized using Ochs algorithm -LRB- Och 2003 -RRB- to set weights for the four components of the log-linear model language model phrase translation model distortion model and word-length feature	amod_feature_word-length nn_model_distortion nn_model_translation nn_model_phrase conj_and_model_feature conj_and_model_model appos_model_model nn_model_language amod_model_log-linear det_model_the prep_of_components_model num_components_four det_components_the prep_for_set_components dobj_set_weights aux_set_to amod_Och_2003 dep_algorithm_Och nn_algorithm_Ochs dep_using_feature dep_using_model dep_using_model vmod_using_set dobj_using_algorithm xcomp_optimized_using auxpass_optimized_are nsubjpass_optimized_weights amod_weights_Decoding
N06-3004	P03-1021	o	This is also true for reranking and discriminative training where the k-best list of candidates serves as an approximation of the full set -LRB- Collins 2000 Och 2003 McDonald et al. 2005 -RRB-	num_McDonald_2005 nn_McDonald_al. nn_McDonald_et dep_Och_McDonald num_Och_2003 dep_Collins_Och appos_Collins_2000 dep_set_Collins amod_set_full det_set_the prep_of_approximation_set det_approximation_an prep_as_serves_approximation nsubj_serves_list advmod_serves_where prep_of_list_candidates amod_list_k-best det_list_the rcmod_training_serves amod_training_discriminative amod_training_reranking conj_and_reranking_discriminative prep_for_true_training advmod_true_also cop_true_is nsubj_true_This ccomp_``_true
N07-1005	P03-1021	o	Many methods for calculating the similarity have been proposed -LRB- Niessen et al. 2000 Akiba et al. 2001 Papineni et al. 2002 NIST 2002 Leusch et al. 2003 Turian et al. 2003 Babych and Hartley 2004 Lin and Och 2004 Banerjee and Lavie 2005 Gimenez et al. 2005 -RRB-	num_Gimenez_2005 nn_Gimenez_al. nn_Gimenez_et num_Lin_2004 conj_and_Lin_Och dep_Babych_Gimenez conj_and_Babych_2005 conj_and_Babych_Lavie conj_and_Babych_Banerjee conj_and_Babych_Och conj_and_Babych_Lin conj_and_Babych_2004 conj_and_Babych_Hartley num_Turian_2003 nn_Turian_al. nn_Turian_et num_Leusch_2003 nn_Leusch_al. nn_Leusch_et num_NIST_2002 num_Papineni_2002 nn_Papineni_al. nn_Papineni_et conj_Akiba_2005 conj_Akiba_Lavie conj_Akiba_Banerjee conj_Akiba_Lin conj_Akiba_2004 conj_Akiba_Hartley conj_Akiba_Babych conj_Akiba_Turian conj_Akiba_Leusch conj_Akiba_NIST conj_Akiba_Papineni num_Akiba_2001 nn_Akiba_al. nn_Akiba_et dep_Niessen_Akiba appos_Niessen_2000 dep_Niessen_al. nn_Niessen_et dep_proposed_Niessen auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods det_similarity_the dobj_calculating_similarity prepc_for_methods_calculating amod_methods_Many ccomp_``_proposed
N07-1005	P03-1021	o	In recent years many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations -LRB- Niessen et al. 2000 Akiba et al. 2001 Papineni et al. 2002 NIST 2002 Leusch et al. 2003 Turian et al. 2003 Babych and Hartley 2004 Lin and Och 2004 Banerjee and Lavie 2005 Gimenez et al. 2005 -RRB- because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently	nn_systems_MT advmod_use_efficiently dobj_use_systems conj_and_use_improve aux_use_to xcomp_enable_improve xcomp_enable_use dobj_enable_us aux_enable_to xcomp_expected_enable auxpass_expected_is nsubjpass_expected_improving mark_expected_because nn_evaluation_MT amod_evaluation_automatic prep_of_performance_evaluation det_performance_the dobj_improving_performance num_Gimenez_2005 nn_Gimenez_al. nn_Gimenez_et conj_and_Banerjee_Lavie num_Lin_2004 conj_and_Lin_Och conj_and_Babych_2004 conj_and_Babych_Hartley num_Turian_2003 nn_Turian_al. nn_Turian_et num_Leusch_2003 nn_Leusch_al. nn_Leusch_et num_NIST_2002 num_Papineni_2002 nn_Papineni_al. nn_Papineni_et num_Akiba_2001 nn_Akiba_al. nn_Akiba_et dep_Niessen_Gimenez dep_Niessen_2005 dep_Niessen_Lavie dep_Niessen_Banerjee dep_Niessen_Och dep_Niessen_Lin dep_Niessen_2004 dep_Niessen_Hartley dep_Niessen_Babych dep_Niessen_Turian dep_Niessen_Leusch dep_Niessen_NIST dep_Niessen_Papineni dep_Niessen_Akiba dep_Niessen_2000 dep_Niessen_al. nn_Niessen_et nn_evaluations_MT amod_evaluations_automatic prep_of_performance_evaluations det_performance_the dobj_improve_performance prep_of_quality_MT det_quality_the conj_and_evaluate_improve dobj_evaluate_quality advmod_evaluate_automatically aux_evaluate_to advcl_tried_expected dep_tried_Niessen ccomp_tried_improve ccomp_tried_evaluate aux_tried_have nsubj_tried_researchers prep_in_tried_years amod_researchers_many amod_years_recent
N07-1005	P03-1021	o	For example Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system -LRB- Och 2003 -RRB-	amod_Och_2003 dep_system_Och nn_system_MT det_system_an prep_of_tuning_system nn_tuning_parameter det_tuning_the prep_for_measures_tuning nn_measures_evaluation nn_measures_MT amod_measures_automatic dobj_using_measures agent_improved_using auxpass_improved_was nsubjpass_improved_quality mark_improved_that nn_results_MT prep_of_quality_results det_quality_the ccomp_reported_improved nsubj_reported_Och prep_for_reported_example
N07-1006	P03-1021	p	This type of direct optimization is known as Minimum Error Rate Training -LRB- Och 2003 -RRB- in the MT community and is an essential component in building the stateof-art MT systems	nn_systems_MT amod_systems_stateof-art det_systems_the dobj_building_systems prepc_in_component_building amod_component_essential det_component_an cop_component_is nn_community_MT det_community_the appos_Och_2003 dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum conj_and_known_component prep_in_known_community prep_as_known_Training auxpass_known_is nsubjpass_known_type amod_optimization_direct prep_of_type_optimization det_type_This
N07-1007	P03-1021	o	-LRB- 2003 -RRB- a trigram target language model an order model word count phrase count average phrase size functions and whole-sentence IBM Model 1 logprobabilities in both directions -LRB- Och et al. 2004 -RRB-	advmod_2004_al. nn_al._et num_Och_2004 det_directions_both prep_in_logprobabilities_directions num_logprobabilities_1 nn_logprobabilities_Model nn_Model_IBM nn_Model_whole-sentence nn_functions_size nn_functions_phrase amod_functions_average nn_count_phrase nn_count_word nn_model_order det_model_an nn_model_language nn_model_target nn_model_trigram det_model_a appos_2003_Och conj_and_2003_logprobabilities appos_2003_functions appos_2003_count appos_2003_count appos_2003_model appos_2003_model dep_''_logprobabilities dep_''_2003
N07-1007	P03-1021	o	The weights of these models are determined using the max-BLEU method described in Och -LRB- 2003 -RRB-	appos_Och_2003 prep_in_described_Och vmod_method_described nn_method_max-BLEU det_method_the dobj_using_method xcomp_determined_using auxpass_determined_are nsubjpass_determined_weights det_models_these prep_of_weights_models det_weights_The
N07-1007	P03-1021	o	Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words and rely on general-purpose phrasal translations and target language models to generate these elements -LRB- e.g. Och and Ney 2002 Koehn et al. 2003 Quirk et al. 2005 Chiang 2005 Galley et al. 2006 -RRB-	num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Chiang_Galley num_Chiang_2005 num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Chiang conj_and_Och_Quirk conj_and_Och_Koehn conj_and_Och_2002 conj_and_Och_Ney dep_e.g._Quirk dep_e.g._Koehn dep_e.g._2002 dep_e.g._Ney dep_e.g._Och ccomp_-LRB-_e.g. det_elements_these dobj_generate_elements aux_generate_to nn_models_language nn_models_target conj_and_translations_models amod_translations_phrasal amod_translations_general-purpose xcomp_rely_generate prep_on_rely_models prep_on_rely_translations nsubj_rely_systems amod_words_content amod_way_same det_way_the advmod_way_exactly amod_elements_grammatical conj_and_treat_rely prep_as_treat_words prep_in_treat_way dobj_treat_elements nsubj_treat_systems nn_systems_SMT amod_systems_stateof-the-art amod_systems_Most
N07-1008	P03-1021	o	Unlike MaxEnt training the method -LRB- Och 2003 -RRB- used for estimating the weight vector for BLEU maximization are not computationally scalable for a large number of feature functions	nn_functions_feature prep_of_number_functions amod_number_large det_number_a prep_for_scalable_number advmod_scalable_computationally neg_scalable_not cop_scalable_are nsubj_scalable_method prep_unlike_scalable_training nn_maximization_BLEU prep_for_vector_maximization nn_vector_weight det_vector_the dobj_estimating_vector prepc_for_used_estimating dep_Och_2003 vmod_method_used dep_method_Och det_method_the nn_training_MaxEnt
N07-1008	P03-1021	o	The f are trained using a held-out corpus using maximum BLEU training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_BLEU nn_training_maximum dobj_using_training amod_corpus_held-out det_corpus_a vmod_using_using dobj_using_corpus xcomp_trained_using auxpass_trained_are nsubjpass_trained_f det_f_The
N07-1022	P03-1021	o	The model parameters are trained using minimum error-rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och amod_training_error-rate amod_training_minimum dobj_using_training xcomp_trained_using auxpass_trained_are nsubjpass_trained_parameters nn_parameters_model det_parameters_The
N07-1022	P03-1021	o	In WASP GIZA + + -LRB- Och and Ney 2003 -RRB- is used to obtain the best alignments from the training examples	nn_examples_training det_examples_the amod_alignments_best det_alignments_the prep_from_obtain_examples dobj_obtain_alignments aux_obtain_to xcomp_used_obtain auxpass_used_is nsubjpass_used_+ nsubjpass_used_GIZA prep_in_used_WASP num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+
N07-1029	P03-1021	p	The modified Powells method has been previously used in optimizing the weights of a standard feature-based MT decoder in -LRB- Och 2003 -RRB- where a more efficient algorithm for log-linear models was proposed	auxpass_proposed_was nsubjpass_proposed_algorithm advmod_proposed_where amod_models_log-linear prep_for_algorithm_models amod_algorithm_efficient det_algorithm_a advmod_efficient_more rcmod_Och_proposed dep_Och_2003 nn_decoder_MT amod_decoder_feature-based amod_decoder_standard det_decoder_a prep_of_weights_decoder det_weights_the prep_in_optimizing_Och dobj_optimizing_weights prepc_in_used_optimizing advmod_used_previously auxpass_used_been aux_used_has nsubjpass_used_method nn_method_Powells amod_method_modified det_method_The
N07-1029	P03-1021	o	If the alignments are not available they can be automatically generated e.g. using GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ dobj_using_+ dobj_using_GIZA ccomp_,_using dep_generated_e.g. advmod_generated_automatically auxpass_generated_be aux_generated_can nsubjpass_generated_they advcl_generated_available neg_available_not cop_available_are nsubj_available_alignments mark_available_If det_alignments_the
N07-1061	P03-1021	o	This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation -LRB- Koehn and Monz 2006 -RRB- and consists of the Pharaoh decoder -LRB- Koehn 2004 -RRB- SRILM -LRB- Stolcke 2002 -RRB- GIZA + + -LRB- Och and Ney 2003 -RRB- mkcls -LRB- Och 1999 -RRB- Carmel ,1 and a phrase model training code	nn_code_training nn_code_model nn_code_phrase det_code_a num_Carmel_,1 amod_Och_1999 dep_mkcls_Och num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och amod_Stolcke_2002 dep_SRILM_Stolcke amod_Koehn_2004 conj_and_decoder_code conj_+_decoder_Carmel conj_+_decoder_mkcls conj_+_decoder_+ conj_+_decoder_GIZA conj_+_decoder_SRILM dep_decoder_Koehn nn_decoder_Pharaoh det_decoder_the prep_of_consists_code prep_of_consists_Carmel prep_of_consists_mkcls prep_of_consists_+ prep_of_consists_GIZA prep_of_consists_SRILM prep_of_consists_decoder nsubj_consists_This conj_and_Koehn_2006 conj_and_Koehn_Monz dep_translation_2006 dep_translation_Monz dep_translation_Koehn nn_translation_machine amod_translation_statistical prep_on_workshop_translation nn_workshop_NAACL/HLT num_workshop_2006 det_workshop_the conj_and_system_consists prep_for_system_workshop nn_system_baseline nn_system_task amod_system_shared det_system_the cop_system_is nsubj_system_This
N07-1061	P03-1021	o	2 Phrase-based SMT We use a phrase-based SMT system Pharaoh -LRB- Koehn et al. 2003 Koehn 2004 -RRB- which is based on a log-linear formulation -LRB- Och and Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_formulation_Ney dep_formulation_Och amod_formulation_log-linear det_formulation_a prep_on_based_formulation auxpass_based_is nsubjpass_based_which dep_Koehn_2004 dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et rcmod_system_based appos_system_Koehn appos_system_Pharaoh nn_system_SMT amod_system_phrase-based det_system_a dobj_use_system nsubj_use_We rcmod_SMT_use amod_SMT_Phrase-based num_SMT_2 dep_``_SMT
N07-1061	P03-1021	o	To set the weights m we carried out minimum error rate training -LRB- Och 2003 -RRB- using BLEU -LRB- Papineni et al. 2002 -RRB- as the objective function	amod_function_objective det_function_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_as_using_function dobj_using_BLEU dep_Och_2003 vmod_training_using appos_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_carried_training prt_carried_out nsubj_carried_we tmod_carried_m advcl_carried_set det_weights_the dobj_set_weights aux_set_To
N07-1062	P03-1021	o	The model scaling factors are optimized using minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training xcomp_optimized_using auxpass_optimized_are nsubjpass_optimized_factors nn_factors_scaling nn_factors_model det_factors_The
N07-1063	P03-1021	o	Parameters used to calculate P -LRB- D -RRB- are trained using MER training -LRB- Och 2003 -RRB- on development data	nn_data_development dep_Och_2003 appos_training_Och nn_training_MER prep_on_using_data dobj_using_training xcomp_trained_using auxpass_trained_are nsubjpass_trained_Parameters appos_P_D dobj_calculate_P aux_calculate_to xcomp_used_calculate vmod_Parameters_used
N07-1064	P03-1021	o	Feature function weights in the loglinear model are set using Ochs minium error rate algorithm -LRB- Och 2003 -RRB-	amod_Och_2003 appos_algorithm_Och nn_algorithm_rate nn_algorithm_error nn_algorithm_minium nn_algorithm_Ochs dobj_using_algorithm xcomp_set_using auxpass_set_are nsubjpass_set_weights amod_model_loglinear det_model_the prep_in_weights_model nn_weights_function nn_weights_Feature
N07-2022	P03-1021	o	In order to improve translation quality this tuning can be effectively performed by minimizing translation error over a development corpus for which manually translated references are available -LRB- Och 2003 -RRB-	amod_Och_2003 cop_available_are nsubj_available_references prep_for_available_which amod_references_translated advmod_translated_manually dep_corpus_Och rcmod_corpus_available nn_corpus_development det_corpus_a nn_error_translation prep_over_minimizing_corpus dobj_minimizing_error agent_performed_minimizing advmod_performed_effectively auxpass_performed_be aux_performed_can nsubjpass_performed_tuning advcl_performed_improve det_tuning_this nn_quality_translation dobj_improve_quality aux_improve_to dep_improve_order mark_improve_In
N07-2022	P03-1021	o	Unsupervised systems -LRB- Och and Ney 2003 Liang et al. 2006 -RRB- are based on generative models trained with the EM algorithm	nn_algorithm_EM det_algorithm_the prep_with_trained_algorithm vmod_models_trained amod_models_generative prep_on_based_models auxpass_based_are nsubjpass_based_systems num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Och_Liang conj_and_Och_2003 conj_and_Och_Ney dep_systems_2003 dep_systems_Ney dep_systems_Och amod_systems_Unsupervised
N07-2047	P03-1021	o	Whilst the parameters for the maximum entropy model are developed based on the minimum error rate training method -LRB- Och 2003 -RRB-	amod_Och_2003 dep_method_Och nn_method_training nn_method_rate nn_method_error amod_method_minimum det_method_the pobj_developed_method prepc_based_on_developed_on auxpass_developed_are nsubjpass_developed_parameters advmod_developed_Whilst nn_model_entropy nn_model_maximum det_model_the prep_for_parameters_model det_parameters_the advcl_``_developed
N07-2053	P03-1021	p	Finally to estimate the parameters i of the weighted linear model we adopt the popular minimum error rate training procedure -LRB- Och 2003 -RRB- which directly optimizes translation quality as measured by the BLEU metric	nn_metric_BLEU det_metric_the prep_by_measured_metric mark_measured_as nn_quality_translation advcl_optimizes_measured dobj_optimizes_quality advmod_optimizes_directly nsubj_optimizes_which appos_Och_2003 rcmod_procedure_optimizes dep_procedure_Och nn_procedure_training nn_procedure_rate nn_procedure_error nn_procedure_minimum amod_procedure_popular det_procedure_the dobj_adopt_procedure nsubj_adopt_we dep_adopt_i nsubj_adopt_parameters amod_model_linear amod_model_weighted det_model_the prep_of_i_model det_parameters_the ccomp_estimate_adopt aux_estimate_to dep_Finally_estimate dep_``_Finally
N09-1013	P03-1021	o	MET -LRB- Och 2003 -RRB- was carried out using a development set and the BLEU score evaluated on two test sets	nn_sets_test num_sets_two prep_on_evaluated_sets vmod_score_evaluated nn_score_BLEU det_score_the nn_set_development det_set_a dobj_using_set conj_and_carried_score xcomp_carried_using prt_carried_out auxpass_carried_was nsubjpass_carried_MET amod_Och_2003 appos_MET_Och
N09-1015	P03-1021	o	The way a decoder constructs translation hypotheses is directly related to the weights for different model features in a SMT system which are usually optimized for a given set of models with minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- to achieve better translation performance	nn_performance_translation amod_performance_better dobj_achieve_performance aux_achieve_to dep_Och_2003 dep_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum vmod_set_achieve prep_with_set_training prep_of_set_models amod_set_given det_set_a prep_for_optimized_set advmod_optimized_usually auxpass_optimized_are nsubjpass_optimized_which rcmod_system_optimized nn_system_SMT det_system_a nn_features_model amod_features_different prep_in_weights_system prep_for_weights_features det_weights_the prep_to_related_weights advmod_related_directly cop_related_is nsubj_related_hypotheses nn_hypotheses_translation nn_hypotheses_constructs nn_hypotheses_decoder det_hypotheses_a rcmod_way_related det_way_The dep_``_way
N09-1025	P03-1021	o	The models are trained using the Margin Infused Relaxed Algorithm or MIRA -LRB- Crammer et al. 2006 -RRB- instead of the standard minimum-error-rate training or MERT algorithm -LRB- Och 2003 -RRB-	amod_Och_2003 dep_algorithm_Och nn_algorithm_MERT conj_or_training_algorithm amod_training_minimum-error-rate amod_training_standard det_training_the dep_Crammer_2006 dep_Crammer_al. nn_Crammer_et conj_or_Algorithm_MIRA amod_Algorithm_Relaxed amod_Algorithm_Infused dep_Margin_MIRA dep_Margin_Algorithm det_Margin_the dobj_using_Margin prep_instead_of_trained_algorithm prep_instead_of_trained_training dep_trained_Crammer xcomp_trained_using auxpass_trained_are nsubjpass_trained_models det_models_The
N09-1027	P03-1021	o	Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU -LRB- Papineni et al. 2002 -RRB- automatic evaluation metric via Minimum Error Rate Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum prep_via_metric_Training nn_metric_evaluation amod_metric_automatic dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_BLEU_metric dep_BLEU_Papineni det_BLEU_the dobj_maximize_BLEU aux_maximize_to nn_weight_model nn_weight_language det_weight_the prep_with_concert_weight xcomp_trained_maximize prep_in_trained_concert advmod_trained_discriminatively auxpass_trained_are nsubjpass_trained_vector nn_vector_weights nn_vector_Feature
N09-1029	P03-1021	o	We obtain aligned parallel sentences and the phrase table after the training of Moses which includes running GIZA + + -LRB- Och and Ney 2003 -RRB- grow-diagonal-final symmetrization and phrase extraction -LRB- Koehn et al. 2005 -RRB-	amod_Koehn_2005 dep_Koehn_al. nn_Koehn_et nn_extraction_phrase amod_symmetrization_grow-diagonal-final num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och dep_GIZA_Koehn conj_and_GIZA_extraction conj_+_GIZA_symmetrization conj_+_GIZA_+ dobj_running_extraction dobj_running_symmetrization dobj_running_+ dobj_running_GIZA xcomp_includes_running nsubj_includes_which rcmod_Moses_includes prep_of_training_Moses det_training_the nn_table_phrase det_table_the prep_after_sentences_training conj_and_sentences_table amod_sentences_parallel amod_sentences_aligned dep_sentences_obtain nsubj_sentences_We
N09-1029	P03-1021	o	To tune all lambda weights above we perform minimum error rate training -LRB- Och 2003 -RRB- on the development set described in Section 7	num_Section_7 prep_in_described_Section vmod_set_described vmod_development_set det_development_the appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_on_perform_development dobj_perform_training nsubj_perform_we advcl_perform_tune prep_weights_above nn_weights_lambda det_weights_all dobj_tune_weights aux_tune_To
N09-1047	P03-1021	o	Their weights are optimized w.r.t. BLEU score using the algorithm described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_algorithm_described det_algorithm_the dobj_using_algorithm vmod_score_using nn_score_BLEU nn_score_w.r.t. dobj_optimized_score auxpass_optimized_are nsubjpass_optimized_weights poss_weights_Their ccomp_``_optimized
N09-1049	P03-1021	o	Standard MET -LRB- Och 2003 -RRB- iterative parameter estimation under IBM BLEU -LRB- Papineni et al. 2001 -RRB- is performed on the corresponding development set	nn_set_development amod_set_corresponding det_set_the prep_on_performed_set auxpass_performed_is nsubjpass_performed_MET amod_Papineni_2001 dep_Papineni_al. nn_Papineni_et nn_BLEU_IBM dep_estimation_Papineni prep_under_estimation_BLEU nn_estimation_parameter amod_estimation_iterative dep_Och_2003 dep_MET_estimation dep_MET_Och nn_MET_Standard
N09-2001	P03-1021	o	The component features are weighted to minimize a translation error criterion on a development set -LRB- Och 2003 -RRB-	amod_Och_2003 dep_set_Och nn_set_development det_set_a prep_on_criterion_set nn_criterion_error nn_criterion_translation det_criterion_a dobj_minimize_criterion aux_minimize_to xcomp_weighted_minimize auxpass_weighted_are nsubjpass_weighted_features nn_features_component det_component_The
N09-2001	P03-1021	o	3 Experiments We built baseline systems using GIZA + + -LRB- Och and Ney 2003 -RRB- Moses phrase extraction with grow-diag-finalend heuristic -LRB- Koehn et al. 2007 -RRB- a standard phrasebased decoder -LRB- Vogel 2003 -RRB- the SRI LM toolkit -LRB- Stolcke 2002 -RRB- the suffix-array language model -LRB- Zhang and Vogel 2005 -RRB- a distance-based word reordering model Algorithm 5 Rich Interruption Constraints -LRB- Coh5 -RRB- Input Source tree T previous phrase fh current phrase fh +1 coverage vector HC 1 Interruption False 2 ICount VerbCount NounCount 0 3 F the left and right-most tokens of fh 4 for each of f F do 5 Climb the dependency tree from f until you reach the highest node n such that fh +1 / T -LRB- n -RRB-	appos_T_n dep_fh_T num_fh_+1 dep_that_fh dep_such_that amod_n_such nn_n_node amod_n_highest det_n_the dobj_reach_n nsubj_reach_you mark_reach_until nn_tree_dependency det_tree_the advcl_Climb_reach prep_from_Climb_f dobj_Climb_tree dep_do_Climb dobj_do_5 nn_F_f prep_of_each_F dep_for_do pobj_for_each num_fh_4 amod_tokens_right-most prep_of_left_fh conj_and_left_tokens det_left_the dep_F_for dep_F_tokens dep_F_left number_3_0 num_NounCount_3 dep_ICount_F conj_ICount_NounCount conj_ICount_VerbCount dep_False_ICount num_False_2 nn_False_Interruption num_HC_1 nn_HC_vector nn_HC_coverage num_fh_+1 nn_fh_phrase amod_fh_current nn_fh_phrase amod_fh_previous dep_T_False appos_T_HC conj_T_fh conj_T_fh nn_T_tree nn_T_Source dep_Input_T dep_Constraints_Input appos_Constraints_Coh5 nn_Constraints_Interruption nn_Constraints_Rich num_Constraints_5 nn_Constraints_Algorithm nn_Constraints_model nn_Constraints_reordering nn_Constraints_word amod_Constraints_distance-based det_Constraints_a amod_Zhang_2005 conj_and_Zhang_Vogel dep_model_Vogel dep_model_Zhang nn_model_language amod_model_suffix-array det_model_the amod_Stolcke_2002 dep_toolkit_Stolcke nn_toolkit_LM nn_toolkit_SRI det_toolkit_the amod_Vogel_2003 appos_decoder_Vogel amod_decoder_phrasebased amod_decoder_standard det_decoder_a amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et amod_heuristic_grow-diag-finalend prep_with_extraction_heuristic nn_extraction_phrase amod_extraction_Moses num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och dep_GIZA_Constraints appos_GIZA_model appos_GIZA_toolkit appos_GIZA_decoder dep_GIZA_Koehn conj_+_GIZA_extraction conj_+_GIZA_+ dobj_using_extraction dobj_using_+ dobj_using_GIZA nn_systems_baseline xcomp_built_using dobj_built_systems nsubj_built_We rcmod_Experiments_built num_Experiments_3
N09-2001	P03-1021	o	All model weights were trained on development sets via minimum-error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- with 200 unique n-best lists and optimizing toward BLEU	prep_toward_optimizing_BLEU nsubjpass_optimizing_weights nn_lists_n-best amod_lists_unique num_lists_200 dep_Och_2003 prep_with_training_lists dep_training_Och appos_training_MERT nn_training_rate amod_training_minimum-error nn_sets_development conj_and_trained_optimizing prep_via_trained_training prep_on_trained_sets auxpass_trained_were nsubjpass_trained_weights nn_weights_model det_weights_All ccomp_``_optimizing ccomp_``_trained
N09-2006	P03-1021	o	Starting from a N-Best list generated from a translation decoder an optimizer such as Minimum Error Rate -LRB- MER -RRB- -LRB- Och 2003 -RRB- training proposes directions to search for a better weight-vector to combine feature functions	nn_functions_feature dobj_combine_functions aux_combine_to vmod_weight-vector_combine amod_weight-vector_better det_weight-vector_a prep_for_search_weight-vector aux_search_to vmod_directions_search dobj_proposes_directions vmod_proposes_Starting dep_training_Och nn_training_Rate dep_Och_2003 appos_Rate_MER nn_Rate_Error nn_Rate_Minimum det_optimizer_an prep_such_as_decoder_training appos_decoder_optimizer nn_decoder_translation det_decoder_a prep_from_generated_decoder vmod_list_generated nn_list_N-Best det_list_a prep_from_Starting_list
P04-1059	P03-1021	o	An alternative to linear models is the log-linear models suggested by Och -LRB- 2003 -RRB-	appos_Och_2003 agent_suggested_Och vmod_models_suggested amod_models_log-linear det_models_the cop_models_is nsubj_models_alternative amod_models_linear prep_to_alternative_models det_alternative_An
P04-1059	P03-1021	o	It is also related to loglinear models for machine translation -LRB- Och 2003 -RRB-	amod_Och_2003 dep_translation_Och nn_translation_machine prep_for_models_translation amod_models_loglinear prep_to_related_models advmod_related_also auxpass_related_is nsubjpass_related_It
P04-1059	P03-1021	o	For each feature function there is a model parameter i The best word segmentation W * is determined by the decision rule as = == M i ii W M W WSfWSScoreW 0 0 * -RRB- -LRB- maxarg -RRB- -LRB- maxarg -LRB- 2 -RRB- Below we describe how to optimize s Our method is a discriminative approach inspired by the Minimum Error Rate Training method proposed in Och -LRB- 2003 -RRB-	appos_Och_2003 prep_in_proposed_Och vmod_method_proposed nn_method_Training nn_method_Rate nn_method_Error nn_method_Minimum det_method_the agent_inspired_method vmod_approach_inspired amod_approach_discriminative det_approach_a cop_approach_is nsubj_approach_method poss_method_Our dobj_optimize_s aux_optimize_to advmod_optimize_how ccomp_describe_optimize nsubj_describe_we dep_describe_Below dep_describe_maxarg dep_describe_i nsubj_describe_M dep_describe_= mark_describe_as appos_maxarg_2 number_0_0 dep_WSfWSScoreW_* num_WSfWSScoreW_0 nn_WSfWSScoreW_W nn_WSfWSScoreW_M nn_WSfWSScoreW_W appos_ii_maxarg dep_ii_WSfWSScoreW dep_i_ii num_M_== nn_rule_decision det_rule_the advcl_determined_describe agent_determined_rule auxpass_determined_is nsubjpass_determined_* dep_determined_i nn_*_W nn_*_segmentation nn_*_word amod_*_best det_*_The ccomp_parameter_determined nn_parameter_model det_parameter_a parataxis_is_approach nsubj_is_parameter expl_is_there prep_for_is_function nn_function_feature det_function_each
P04-1078	P03-1021	o	1 Introduction With the introduction of the BLEU metric for machine translation evaluation -LRB- Papineni et al 2002 -RRB- the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated they allow for faster implement-evaluate cycles -LRB- by by-passing the human evaluation bottleneck -RRB- less variation in evaluation performance due to errors in human assessor judgment and not least the possibility of hill-climbing on such metrics in order to improve system performance -LRB- Och 2003 -RRB-	num_Och_2003 appos_performance_Och nn_performance_system dobj_improve_performance aux_improve_to dep_improve_order mark_improve_in dep_metrics_improve amod_metrics_such prep_on_possibility_metrics prep_of_possibility_hill-climbing det_possibility_the neg_least_not nn_judgment_assessor amod_judgment_human prep_in_errors_judgment nn_performance_evaluation appos_variation_possibility conj_and_variation_least prep_due_to_variation_errors prep_in_variation_performance amod_variation_less appos_bottleneck_least appos_bottleneck_variation nn_bottleneck_evaluation amod_bottleneck_human det_bottleneck_the dobj_by-passing_bottleneck amod_cycles_implement-evaluate advmod_cycles_faster prepc_by_allow_by-passing prep_for_allow_cycles nsubj_allow_they advmod_appreciated_increasingly parataxis_become_allow acomp_become_appreciated aux_become_have nsubj_become_Introduction nn_applications_NLP amod_applications_various amod_evaluation_automatic prep_for_doing_applications dobj_doing_evaluation prepc_of_advantages_doing det_advantages_the amod_Papineni_2002 dep_Papineni_al nn_Papineni_et nn_evaluation_translation nn_evaluation_machine nn_metric_BLEU det_metric_the prep_for_introduction_evaluation prep_of_introduction_metric det_introduction_the appos_Introduction_advantages dep_Introduction_Papineni prep_with_Introduction_introduction num_Introduction_1 ccomp_``_become
P05-1033	P03-1021	o	We ran the trainer with its default settings -LRB- maximum phrase length 7 -RRB- and then used Koehns implementation of minimumerror-rate training -LRB- Och 2003 -RRB- to tune the feature weights to maximize the systems BLEU score on our development set yielding the values shown in Table 2	num_Table_2 prep_in_shown_Table vmod_values_shown det_values_the dobj_yielding_values nn_set_development poss_set_our prep_on_score_set nn_score_BLEU nn_score_systems det_score_the dobj_maximize_score aux_maximize_to nn_weights_feature det_weights_the vmod_tune_maximize dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och amod_training_minimumerror-rate prep_of_implementation_training nn_implementation_Koehns vmod_used_tune dobj_used_implementation advmod_used_then nsubj_used_We num_length_7 nn_length_phrase nn_length_maximum dep_settings_length nn_settings_default poss_settings_its det_trainer_the vmod_ran_yielding conj_and_ran_used prep_with_ran_settings dobj_ran_trainer nsubj_ran_We
P05-1033	P03-1021	o	Above the phrase level these models typically have a simple distortion model that reorders phrases independently of their content -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- or not at all -LRB- Zens and Ney 2004 Kumar et al. 2005 -RRB-	num_Kumar_2005 nn_Kumar_al. nn_Kumar_et dep_Zens_Kumar dep_Zens_2004 conj_and_Zens_Ney appos_all_Ney appos_all_Zens pobj_at_all neg_at_not num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney conj_or_content_at dep_content_Koehn dep_content_2004 dep_content_Ney dep_content_Och poss_content_their prep_of_independently_at prep_of_independently_content advmod_reorders_independently dobj_reorders_phrases nsubj_reorders_that rcmod_model_reorders nn_model_distortion amod_model_simple det_model_a dobj_have_model advmod_have_typically nsubj_have_models prep_above_have_level det_models_these nn_level_phrase det_level_the
P05-1033	P03-1021	o	For our experiments we used the following features analogous to Pharaohs default feature set P -LRB- | -RRB- and P -LRB- | -RRB- the latter of which is not found in the noisy-channel model but has been previously found to be a helpful feature -LRB- Och and Ney 2002 -RRB- the lexical weights Pw -LRB- | -RRB- and Pw -LRB- | -RRB- -LRB- Koehn et al. 2003 -RRB- which estimate how well the words in translate the words in 2 a phrase penalty exp -LRB- 1 -RRB- which allows the model to learn a preference for longer or shorter derivations analogous to Koehns phrase penalty -LRB- Koehn 2003 -RRB-	amod_Koehn_2003 appos_penalty_Koehn nn_penalty_phrase nn_penalty_Koehns prep_to_analogous_penalty amod_derivations_analogous amod_derivations_shorter amod_derivations_longer conj_or_longer_shorter prep_for_preference_derivations det_preference_a dobj_learn_preference aux_learn_to det_model_the xcomp_allows_learn dobj_allows_model nsubj_allows_which rcmod_exp_allows appos_exp_1 nn_exp_penalty nn_exp_phrase det_exp_a dep_2_exp det_words_the prep_translate_in dobj_translate_words prepc_in_words_translate det_words_the pobj_well_words advmod_well_how prep_estimate_well nsubj_estimate_which dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_Pw_| conj_and_Pw_Pw appos_Pw_| rcmod_weights_estimate appos_weights_Koehn dep_weights_Pw dep_weights_Pw amod_weights_lexical det_weights_the dep_Och_2002 conj_and_Och_Ney dep_feature_Ney dep_feature_Och amod_feature_helpful det_feature_a cop_feature_be aux_feature_to xcomp_found_feature advmod_found_previously auxpass_found_been aux_found_has amod_model_noisy-channel det_model_the prep_in_found_model neg_found_not auxpass_found_is nsubjpass_found_latter prep_of_latter_which det_latter_the appos_P_| conj_but_P_2 conj_but_P_weights conj_but_P_found conj_and_P_found conj_and_P_P appos_P_| nn_set_feature nn_set_default nn_set_Pharaohs prep_to_analogous_set amod_features_following det_features_the dep_used_2 dep_used_weights dep_used_found dep_used_found dep_used_P dep_used_P dep_used_analogous dobj_used_features nsubj_used_we prep_for_used_experiments poss_experiments_our rcmod_``_used
P05-1033	P03-1021	o	-LRB- 2003 -RRB- which is based on that of Och and Ney -LRB- 2004 -RRB-	appos_Ney_2004 conj_and_Och_Ney prep_of_that_Ney prep_of_that_Och prep_on_based_that auxpass_based_is nsubjpass_based_which rcmod_2003_based dep_''_2003
P05-1033	P03-1021	o	To do this we first identify initial phrase pairs using the same criterion as previous systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- Definition 1	num_Definition_1 num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_systems_Koehn dep_systems_2004 dep_systems_Ney dep_systems_Och amod_systems_previous prep_as_criterion_systems amod_criterion_same det_criterion_the dobj_using_criterion nn_pairs_phrase amod_pairs_initial dep_identify_Definition xcomp_identify_using dobj_identify_pairs advmod_identify_first nsubj_identify_we advcl_identify_do dobj_do_this aux_do_To
P05-1057	P03-1021	o	We used GIZA + + package -LRB- Och and Ney 2003 -RRB- to train IBM translation models	nn_models_translation nn_models_IBM dobj_train_models aux_train_to nn_Och_package preconj_Och_+ amod_GIZA_2003 conj_and_GIZA_Ney conj_+_GIZA_Och xcomp_used_train dobj_used_Ney dobj_used_Och dobj_used_GIZA nsubj_used_We
P05-1057	P03-1021	o	After that we used three types of methods for performing a symmetrization of IBM models intersection union and refined methods -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_methods_Ney dep_methods_Och amod_methods_refined conj_and_intersection_methods conj_and_intersection_union nn_models_IBM prep_of_symmetrization_models det_symmetrization_a dobj_performing_symmetrization prep_of_types_methods num_types_three dep_used_methods dep_used_union dep_used_intersection prepc_for_used_performing dobj_used_types nsubj_used_we prep_after_used_that
P05-1057	P03-1021	p	Studies reveal that statistical alignment models outperform the simple Dice coefficient -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_coefficient_Ney dep_coefficient_Och nn_coefficient_Dice amod_coefficient_simple det_coefficient_the dobj_outperform_coefficient nsubj_outperform_models mark_outperform_that nn_models_alignment amod_models_statistical ccomp_reveal_outperform nsubj_reveal_Studies
P05-1057	P03-1021	p	It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation -LRB- Och 2003 -RRB-	amod_Och_2003 dep_translation_Och nn_translation_machine amod_translation_statistical prep_in_suggested_translation mark_suggested_as nn_parameters_model det_parameters_the advcl_optimize_suggested prep_with_respect_to_optimize_AER advmod_optimize_directly dobj_optimize_parameters aux_optimize_to xcomp_promising_optimize aux_promising_is nsubj_promising_It ccomp_``_promising
P05-1057	P03-1021	o	Och and Ney -LRB- 2003 -RRB- proposed Model 6 a log-linear combination of IBM translation models and HMM model	nn_model_HMM conj_and_models_model nn_models_translation nn_models_IBM prep_of_combination_model prep_of_combination_models amod_combination_log-linear det_combination_a appos_Model_combination num_Model_6 amod_Model_proposed appos_Ney_2003 dep_Och_Model conj_and_Och_Ney
P05-1057	P03-1021	o	In order to incorporate a new dependency which contains extra information other than the bilingual sentence pair we modify Eq .2 by adding a new variable v Pr -LRB- a | e f v -RRB- = exp -LSB- summationtextM m = 1 mhm -LRB- a e f v -RRB- -RSB- summationtext aprime exp -LSB- summationtextM m = 1 mhm -LRB- aprime e f v -RRB- -RSB- -LRB- 4 -RRB- Accordingly we get a new decision rule a = argmax a braceleftbigg Msummationdisplay m = 1 mhm -LRB- a e f v -RRB- bracerightbigg -LRB- 5 -RRB- Note that our log-linear models are different from Model 6 proposed by Och and Ney -LRB- 2003 -RRB- which defines the alignment problem as finding the alignment a that maximizes Pr -LRB- f a | e -RRB- given e. 3 Feature Functions In this paper we use IBM translation Model 3 as the base feature of our log-linear models	amod_models_log-linear poss_models_our prep_of_feature_models nn_feature_base det_feature_the num_Model_3 nn_Model_translation nn_Model_IBM prep_as_use_feature dobj_use_Model nsubj_use_we det_paper_this prep_in_Functions_paper nn_Functions_Feature num_Functions_3 pobj_e._Functions pcomp_given_e. prep_|_given dep_|_e det_|_a rcmod_f_use appos_f_| dep_Pr_f dobj_maximizes_Pr nsubj_maximizes_that rcmod_a_maximizes dep_alignment_a det_alignment_the dobj_finding_alignment nn_problem_alignment det_problem_the prepc_as_defines_finding dobj_defines_problem nsubj_defines_which appos_Ney_2003 conj_and_Och_Ney agent_proposed_Ney agent_proposed_Och rcmod_Model_defines vmod_Model_proposed num_Model_6 prep_from_different_Model cop_different_are nsubj_different_models mark_different_that amod_models_log-linear poss_models_our ccomp_Note_different dep_Note_bracerightbigg dep_Note_v dep_Note_e dep_Note_mhm dep_Note_= nsubj_Note_m dep_Note_argmax appos_bracerightbigg_5 dep_e_f dep_e_a num_mhm_1 nn_m_Msummationdisplay nn_m_braceleftbigg det_m_a amod_argmax_= det_argmax_a parataxis_rule_Note nn_rule_decision amod_rule_new det_rule_a dobj_get_rule nsubj_get_we parataxis_4_get advmod_4_Accordingly dep_v_f dep_v_e dep_aprime_v dep_mhm_4 appos_mhm_aprime num_mhm_1 dobj_=_mhm nsubj_=_m nn_m_summationtextM dep_exp_= nn_exp_aprime nn_exp_summationtext dep_v_e dep_v_mhm dep_e_f dep_e_a num_mhm_1 dep_=_exp dep_=_v nsubj_=_m nn_m_summationtextM dep_exp_= dep_=_exp amod_|_= appos_|_v appos_|_f dep_|_e det_|_a dep_Pr_| dep_v_Pr dep_variable_v amod_variable_new amod_a_variable dobj_adding_a nn_.2_Eq prepc_by_modify_adding dobj_modify_.2 nsubj_modify_we advcl_modify_incorporate nn_pair_sentence amod_pair_bilingual det_pair_the prep_than_other_pair amod_information_other amod_information_extra dobj_contains_information nsubj_contains_which rcmod_dependency_contains amod_dependency_new det_dependency_a dobj_incorporate_dependency aux_incorporate_to dep_incorporate_order mark_incorporate_In
P05-1066	P03-1021	o	In practice when training the parameters of an SMT system for example using the discriminative methods of -LRB- Och 2003 -RRB- the cost for skips of this kind is typically set to a very high value	amod_value_high det_value_a advmod_high_very prep_to_set_value advmod_set_typically auxpass_set_is nsubjpass_set_cost det_kind_this prep_of_skips_kind prep_for_cost_skips det_cost_the rcmod_Och_set dep_Och_2003 prep_of_methods_Och amod_methods_discriminative det_methods_the dobj_using_methods prep_for_using_example advcl_using_training prep_in_using_practice nn_system_SMT det_system_an prep_of_parameters_system det_parameters_the dobj_training_parameters advmod_training_when rcmod_``_using
P05-1066	P03-1021	o	For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems -LRB- e.g. see -LRB- Alshawi 1996 Wu 1997 Yamada and Knight 2001 Gildea 2003 Melamed 2004 Graehl and Knight 2004 Och et al. 2004 Xia and McCord 2004 -RRB- -RRB-	dep_Xia_2004 conj_and_Xia_McCord nn_al._et nn_al._Och num_Melamed_2004 num_Gildea_2003 dep_Wu_McCord dep_Wu_Xia num_Wu_2004 dep_Wu_al. num_Wu_2004 conj_and_Wu_Knight conj_and_Wu_Graehl conj_and_Wu_Melamed conj_and_Wu_Gildea num_Wu_2001 conj_and_Wu_Knight conj_and_Wu_Yamada num_Wu_1997 dep_Alshawi_Knight dep_Alshawi_Graehl dep_Alshawi_Melamed dep_Alshawi_Gildea dep_Alshawi_Knight dep_Alshawi_Yamada dep_Alshawi_Wu appos_Alshawi_1996 dep_see_Alshawi dep_e.g._see dep_systems_e.g. nn_systems_translation nn_systems_machine amod_systems_statistical prep_within_information_systems amod_information_syntactic dobj_incorporate_information nsubj_incorporate_which rcmod_methods_incorporate prep_in_interest_methods prep_of_deal_interest amod_deal_great det_deal_a advmod_deal_currently nsubj_is_deal expl_is_there prep_for_is_reason det_reason_this ccomp_``_is
P05-1066	P03-1021	o	More recently phrase-based models -LRB- Och et al. 1999 Marcu and Wong 2002 Koehn et al. 2003 -RRB- have been proposed as a highly successful alternative to the IBM models	nn_models_IBM det_models_the prep_to_alternative_models amod_alternative_successful det_alternative_a advmod_successful_highly prep_as_proposed_alternative auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models advmod_proposed_recently num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong dep_Och_Wong dep_Och_Marcu appos_Och_1999 dep_Och_al. nn_Och_et appos_models_Och amod_models_phrase-based advmod_recently_More
P05-1066	P03-1021	o	Reranking methods have also been proposed as a method for using syntactic information -LRB- Koehn and Knight 2003 Och et al. 2004 Shen et al. 2004 -RRB-	num_Shen_2004 nn_Shen_al. nn_Shen_et num_Och_2004 nn_Och_al. nn_Och_et dep_Koehn_Shen conj_and_Koehn_Och conj_and_Koehn_2003 conj_and_Koehn_Knight appos_information_Och appos_information_2003 appos_information_Knight appos_information_Koehn amod_information_syntactic dobj_using_information prepc_for_method_using det_method_a prep_as_proposed_method auxpass_proposed_been advmod_proposed_also aux_proposed_have nsubjpass_proposed_methods nn_methods_Reranking ccomp_``_proposed
P05-1066	P03-1021	o	1 Introduction Recent research on statistical machine translation -LRB- SMT -RRB- has lead to the development of phrasebased systems -LRB- Och et al. 1999 Marcu and Wong 2002 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong dep_Och_Wong dep_Och_Marcu appos_Och_1999 dep_Och_al. nn_Och_et amod_systems_phrasebased prep_of_development_systems det_development_the dep_lead_Och prep_to_lead_development aux_lead_has nsubj_lead_research appos_translation_SMT nn_translation_machine amod_translation_statistical prep_on_research_translation amod_research_Recent nn_research_Introduction num_research_1 ccomp_``_lead
P05-1069	P03-1021	o	Instead of directly minimizing error as in earlier work -LRB- Och 2003 -RRB- we decompose the decoding process into a sequence of local decision steps based on Eq	prep_on_based_Eq vmod_steps_based nn_steps_decision amod_steps_local prep_of_sequence_steps det_sequence_a nn_process_decoding det_process_the prep_into_decompose_sequence dobj_decompose_process nsubj_decompose_we prepc_instead_of_decompose_minimizing appos_Och_2003 dep_work_Och amod_work_earlier pobj_in_work pcomp_as_in prep_minimizing_as dobj_minimizing_error advmod_minimizing_directly
P05-1069	P03-1021	o	As far as the log-linear combination of float features is concerned similar training procedures have been proposed in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_proposed_in auxpass_proposed_been aux_proposed_have nsubjpass_proposed_procedures advcl_proposed_concerned nn_procedures_training amod_procedures_similar auxpass_concerned_is nsubjpass_concerned_combination mark_concerned_as advmod_concerned_far mark_concerned_As nn_features_float prep_of_combination_features amod_combination_log-linear det_combination_the ccomp_``_proposed
P05-1069	P03-1021	o	2 Block Orientation Bigrams This section describes a phrase-based model for SMT similar to the models presented in -LRB- Koehn et al. 2003 Och et al. 1999 Tillmann and Xia 2003 -RRB-	dep_Tillmann_2003 conj_and_Tillmann_Xia dep_Och_Xia dep_Och_Tillmann num_Och_1999 nn_Och_al. nn_Och_et dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_presented_Koehn vmod_models_presented det_models_the prep_to_similar_models amod_SMT_similar prep_for_model_SMT amod_model_phrase-based det_model_a dobj_describes_model nsubj_describes_section det_section_This nn_section_Bigrams nn_section_Orientation nn_section_Block num_section_2 ccomp_``_describes
P06-1001	P03-1021	o	Decoding weights are optimized using Ochs algorithm -LRB- Och 2003 -RRB- to set weights for the four components of the loglinear model language model phrase translation model distortion model and word-length feature	amod_feature_word-length nn_model_distortion nn_model_translation nn_model_phrase conj_and_model_feature conj_and_model_model appos_model_model nn_model_language amod_model_loglinear det_model_the prep_of_components_model num_components_four det_components_the prep_for_set_components dobj_set_weights aux_set_to amod_Och_2003 dep_algorithm_Och nn_algorithm_Ochs dep_using_feature dep_using_model dep_using_model vmod_using_set dobj_using_algorithm xcomp_optimized_using auxpass_optimized_are nsubjpass_optimized_weights amod_weights_Decoding
P06-1002	P03-1021	o	2 Related Work Starting with the IBM models -LRB- Brown et al. 1993 -RRB- researchers have developed various statistical word alignment systems based on different models such as hidden Markov models -LRB- HMM -RRB- -LRB- Vogel et al. 1996 -RRB- log-linear models -LRB- Och and Ney 2003 -RRB- and similarity-based heuristic methods -LRB- Melamed 2000 -RRB-	amod_Melamed_2000 dep_methods_Melamed nn_methods_heuristic amod_methods_similarity-based dep_Och_2003 conj_and_Och_Ney appos_models_Ney appos_models_Och amod_models_log-linear amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et appos_models_HMM nn_models_Markov amod_models_hidden prep_such_as_models_models amod_models_different prep_on_based_models vmod_systems_based nn_systems_alignment nn_systems_word amod_systems_statistical amod_systems_various conj_and_developed_methods conj_and_developed_models dep_developed_Vogel dobj_developed_systems aux_developed_have nsubj_developed_researchers dep_developed_Work amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the prep_with_Starting_models dep_Work_Brown vmod_Work_Starting amod_Work_Related num_Work_2 ccomp_``_methods ccomp_``_models ccomp_``_developed
P06-1002	P03-1021	o	MT output was evaluated using the standard evaluation metric BLEU -LRB- Papineni et al. 2002 -RRB- .2 The parameters of the MT System were optimized for BLEU metric on NIST MTEval2002 test sets using minimum error rate training -LRB- Och 2003 -RRB- and the systems were tested on NIST MTEval2003 test sets for both languages	det_languages_both prep_for_sets_languages nn_sets_test nn_sets_MTEval2003 nn_sets_NIST prep_on_tested_sets auxpass_tested_were nsubjpass_tested_systems det_systems_the dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training nn_sets_test nn_sets_MTEval2002 nn_sets_NIST prep_on_BLEU_sets amod_BLEU_metric conj_and_optimized_tested xcomp_optimized_using prep_for_optimized_BLEU auxpass_optimized_were nn_System_MT det_System_the prep_of_parameters_System det_parameters_The nn_parameters_.2 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_parameters dep_BLEU_Papineni amod_BLEU_metric nn_BLEU_evaluation amod_BLEU_standard det_BLEU_the dobj_using_BLEU dep_evaluated_tested dep_evaluated_optimized xcomp_evaluated_using auxpass_evaluated_was nsubjpass_evaluated_output amod_output_MT
P06-1028	P03-1021	o	Several non-linear objective functions such as F-score for text classification -LRB- Gao et al. 2003 -RRB- and BLEU-score and some other evaluation measures for statistical machine translation -LRB- Och 2003 -RRB- have been introduced with reference to the framework of MCE criterion training	nn_training_criterion nn_training_MCE prep_of_framework_training det_framework_the prep_to_reference_framework prep_with_introduced_reference auxpass_introduced_been aux_introduced_have nsubjpass_introduced_BLEU-score nsubjpass_introduced_functions appos_Och_2003 dep_translation_Och nn_translation_machine amod_translation_statistical nn_measures_evaluation amod_measures_other det_measures_some prep_for_BLEU-score_translation conj_and_BLEU-score_measures amod_Gao_2003 dep_Gao_al. nn_Gao_et nn_classification_text prep_for_F-score_classification conj_and_functions_measures conj_and_functions_BLEU-score dep_functions_Gao prep_such_as_functions_F-score amod_functions_objective amod_functions_non-linear amod_functions_Several
P06-1032	P03-1021	o	N-best results for phrasal alignment and ordering models in the decoder were optimized by lambda training via Maximum Bleu along the lines described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_lines_described det_lines_the prep_along_Bleu_lines nn_Bleu_Maximum nn_training_lambda prep_via_optimized_Bleu agent_optimized_training auxpass_optimized_were nsubjpass_optimized_ordering nsubjpass_optimized_results det_decoder_the prep_in_ordering_decoder dobj_ordering_models amod_alignment_phrasal conj_and_results_ordering prep_for_results_alignment amod_results_N-best
P06-1066	P03-1021	o	One is distortion model -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- which penalizes translations according to their jump distance instead of their content	poss_content_their prep_instead_of_distance_content nn_distance_jump poss_distance_their pobj_penalizes_distance prepc_according_to_penalizes_to dobj_penalizes_translations nsubj_penalizes_which num_Koehn_2003 nn_Koehn_al. nn_Koehn_et rcmod_Och_penalizes dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_model_2004 dep_model_Ney dep_model_Och nn_model_distortion cop_model_is nsubj_model_One
P06-1066	P03-1021	o	Line 4 and 5 are similar to the phrase extraction algorithm by Och -LRB- 2003b -RRB-	appos_Och_2003b prep_by_algorithm_Och nn_algorithm_extraction nn_algorithm_phrase det_algorithm_the prep_to_similar_algorithm cop_similar_are nsubj_similar_5 nsubj_similar_Line conj_and_Line_5 num_Line_4
P06-1066	P03-1021	o	The k-best list is very important for the minimum error rate training -LRB- Och 2003a -RRB- which is used for tuning the weights for our model	poss_model_our prep_for_weights_model det_weights_the dep_tuning_weights prep_for_used_tuning auxpass_used_is nsubjpass_used_which appos_Och_2003a rcmod_training_used dep_training_Och nn_training_rate nn_training_error amod_training_minimum det_training_the prep_for_important_training advmod_important_very cop_important_is nsubj_important_list amod_list_k-best det_list_The
P06-1077	P03-1021	o	5.1 Pharaoh The baseline system we used for comparison was Pharaoh -LRB- Koehn et al. 2003 Koehn 2004 -RRB- a freely available decoder for phrase-based translation models p -LRB- e | f -RRB- = p -LRB- f | e -RRB- pLM -LRB- e -RRB- LM pD -LRB- e f -RRB- D length -LRB- e -RRB- W -LRB- e -RRB- -LRB- 10 -RRB- We ran GIZA + + -LRB- Och and Ney 2000 -RRB- on the training corpus in both directions using its default setting and then applied the refinement rule diagand described in -LRB- Koehn et al. 2003 -RRB- to obtain a single many-to-many word alignment for each sentence pair	nn_pair_sentence det_pair_each prep_for_alignment_pair nn_alignment_word nn_alignment_many-to-many amod_alignment_single det_alignment_a dobj_obtain_alignment aux_obtain_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn xcomp_described_obtain prep_described_in vmod_diagand_described nn_diagand_rule nn_diagand_refinement det_diagand_the dobj_applied_diagand advmod_applied_then nn_setting_default poss_setting_its dobj_using_setting preconj_directions_both nn_corpus_training det_corpus_the num_Och_2000 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ prep_in_ran_directions dobj_ran_+ dobj_ran_GIZA nsubj_ran_We dep_W_e nn_W_length nn_length_D nn_length_pD dep_length_e nn_length_| dep_D_e dep_e_f nn_pD_LM dep_pD_e nn_pD_pLM nn_|_f conj_and_=_applied dep_=_using dep_=_ran dep_=_10 dep_=_e dep_=_W dep_=_p dep_=_f dep_=_p dep_f_| dep_|_e nn_models_translation amod_models_phrase-based prep_for_decoder_models amod_decoder_available det_decoder_a advmod_available_freely dep_Koehn_2004 dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et parataxis_Pharaoh_applied parataxis_Pharaoh_= appos_Pharaoh_decoder dep_Pharaoh_Koehn cop_Pharaoh_was nsubj_Pharaoh_Pharaoh prep_for_used_comparison nsubj_used_we rcmod_system_used nn_system_baseline det_system_The dep_Pharaoh_system num_Pharaoh_5.1
P06-1077	P03-1021	o	To perform minimum error rate training -LRB- Och 2003 -RRB- to tune the feature weights to maximize the systems BLEU score on development set we used optimizeV5IBMBLEU.m -LRB- Venugopal and Vogel 2005 -RRB-	dep_Venugopal_2005 conj_and_Venugopal_Vogel dep_optimizeV5IBMBLEU.m_Vogel dep_optimizeV5IBMBLEU.m_Venugopal dobj_used_optimizeV5IBMBLEU.m nsubj_used_we advcl_used_perform nn_set_development prep_on_score_set nn_score_BLEU nn_score_systems det_score_the dobj_maximize_score aux_maximize_to nn_weights_feature det_weights_the vmod_tune_maximize dobj_tune_weights aux_tune_to dep_Och_2003 nn_training_rate nn_training_error amod_training_minimum vmod_perform_tune dep_perform_Och dobj_perform_training aux_perform_To
P06-1077	P03-1021	p	1 Introduction Phrase-based translation models -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB- which go beyond the original IBM translation models -LRB- Brown et al. 1993 -RRB- 1 by modeling translations of phrases rather than individual words have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations	amod_evaluations_empirical nn_translation_machine amod_translation_statistical prep_by_state-of-theart_evaluations prep_in_state-of-theart_translation det_state-of-theart_the cop_state-of-theart_be aux_state-of-theart_to xcomp_suggested_state-of-theart auxpass_suggested_been aux_suggested_have nsubjpass_suggested_models amod_words_individual conj_negcc_translations_words prep_of_translations_phrases nn_translations_modeling prep_by_1_words prep_by_1_translations amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation nn_models_IBM amod_models_original det_models_the prep_beyond_go_models nsubj_go_which dep_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong dep_models_1 dep_models_Brown rcmod_models_go appos_models_Koehn appos_models_Wong appos_models_Marcu nn_models_translation amod_models_Phrase-based nn_models_Introduction num_models_1 ccomp_``_suggested
P06-1091	P03-1021	p	While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems -LRB- Chiang 2005 Och 2003 -RRB- this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems 727 such as part-of-speech tagging or shallow parsing both in modeling and parameter training	nn_training_parameter conj_and_modeling_training amod_parsing_shallow conj_or_tagging_parsing amod_tagging_part-of-speech prep_such_as_727_parsing prep_such_as_727_tagging dep_problems_727 nn_problems_annotation nn_problems_language amod_problems_natural amod_problems_sequential prep_in_similar_training prep_in_similar_modeling preconj_similar_both prep_to_similar_problems cop_similar_is nsubj_similar_that rcmod_approach_similar prep_to_approach_SMT nn_approach_translation nn_approach_sequence nn_approach_block amod_approach_novel det_approach_a dobj_presents_approach nsubj_presents_paper advcl_presents_used det_paper_this amod_Och_2003 dep_Chiang_Och appos_Chiang_2005 appos_systems_Chiang nn_systems_translation amod_systems_phrasebased prep_of_performance_systems det_performance_the dobj_improve_performance aux_improve_to xcomp_used_improve advmod_used_commonly auxpass_used_are nsubjpass_used_techniques mark_used_While nn_techniques_training amod_techniques_error-driven
P06-1091	P03-1021	o	The current approach does not use specialized probability features as in -LRB- Och 2003 -RRB- in any stage during decoder parameter training	nn_training_parameter nn_training_decoder det_stage_any appos_Och_2003 pobj_in_Och pcomp_as_in nn_features_probability amod_features_specialized prep_during_use_training prep_in_use_stage prep_use_as dobj_use_features neg_use_not aux_use_does nsubj_use_approach amod_approach_current det_approach_The ccomp_``_use
P06-1091	P03-1021	o	The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT -LRB- Och 2003 -RRB- as follows a90 No computationally expensive a57 best lists are generated during training for each input sentence a single block sequence is generated on each iteration over the training data	nn_data_training det_data_the det_iteration_each prep_over_generated_data prep_on_generated_iteration auxpass_generated_is nsubjpass_generated_sequence nn_sequence_block amod_sequence_single det_sequence_a rcmod_sentence_generated nn_sentence_input det_sentence_each prep_for_generated_sentence prep_during_generated_training auxpass_generated_are nsubjpass_generated_lists amod_lists_best dep_lists_a57 num_lists_a90 amod_a57_expensive advmod_a57_computationally neg_a57_No mark_follows_as appos_Och_2003 dep_SMT_Och nn_algorithms_training amod_algorithms_discriminative prep_for_work_SMT prep_in_work_algorithms amod_work_earlier parataxis_differs_generated advcl_differs_follows prep_from_differs_work advmod_differs_computationally nsubj_differs_algorithm amod_algorithm_novel det_algorithm_The
P06-1091	P03-1021	o	Although the training algorithm can handle realvalued features as used in -LRB- Och 2003 Tillmann and Zhang 2005 -RRB- the current paper intentionally excludes them	dobj_excludes_them advmod_excludes_intentionally nsubj_excludes_paper dep_excludes_Och mark_excludes_in amod_paper_current det_paper_the amod_Tillmann_2005 conj_and_Tillmann_Zhang dep_Och_Zhang dep_Och_Tillmann appos_Och_2003 advcl_used_excludes mark_used_as amod_features_realvalued advcl_handle_used dobj_handle_features aux_handle_can nsubj_handle_algorithm mark_handle_Although nn_algorithm_training det_algorithm_the advcl_``_handle
P06-1096	P03-1021	n	Unlike minimum error rate training -LRB- Och 2003 -RRB- our system is able to exploit large numbers of specific features in the same manner as static reranking systems -LRB- Shen et al. 2004 Och et al. 2004 -RRB-	num_Och_2004 nn_Och_al. nn_Och_et dep_Shen_Och appos_Shen_2004 dep_Shen_al. nn_Shen_et amod_systems_reranking amod_systems_static amod_manner_same det_manner_the amod_features_specific prep_of_numbers_features amod_numbers_large prep_as_exploit_systems prep_in_exploit_manner dobj_exploit_numbers aux_exploit_to dep_able_Shen xcomp_able_exploit cop_able_is nsubj_able_system prep_unlike_able_training poss_system_our dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum
P06-1096	P03-1021	o	We tuned Pharaohs four parameters using minimum error rate training -LRB- Och 2003 -RRB- on DEV .12 We obtained an increase of 0.8 9As in the POS features we map each phrase pair to its majority constellation	nn_constellation_majority poss_constellation_its nn_pair_phrase det_pair_each prep_to_map_constellation dobj_map_pair nsubj_map_we ccomp_map_tuned nn_features_POS det_features_the num_9As_0.8 prep_in_increase_features prep_of_increase_9As det_increase_an dobj_obtained_increase nsubj_obtained_We num_DEV_.12 dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum ccomp_using_obtained prep_on_using_DEV dobj_using_training num_parameters_four nn_parameters_Pharaohs vmod_tuned_using dobj_tuned_parameters nsubj_tuned_We
P06-1096	P03-1021	o	The first approach is to reuse the components of a generative model but tune their relative weights in a discriminative fashion -LRB- Och and Ney 2002 Och 2003 Chiang 2005 -RRB-	amod_Chiang_2005 appos_Och_2003 dep_Och_Chiang conj_and_Och_Och conj_and_Och_2002 conj_and_Och_Ney dep_fashion_Och dep_fashion_2002 dep_fashion_Ney dep_fashion_Och amod_fashion_discriminative det_fashion_a amod_weights_relative poss_weights_their prep_in_tune_fashion dobj_tune_weights nsubj_tune_approach amod_model_generative det_model_a prep_of_components_model det_components_the dobj_reuse_components aux_reuse_to conj_but_is_tune xcomp_is_reuse nsubj_is_approach amod_approach_first det_approach_The ccomp_``_tune ccomp_``_is
P06-1097	P03-1021	o	We run Maximum BLEU -LRB- Och 2003 -RRB- for 25 iterations individually for each system	det_system_each num_iterations_25 dep_Och_2003 appos_BLEU_Och nn_BLEU_Maximum prep_for_run_system advmod_run_individually prep_for_run_iterations dobj_run_BLEU nsubj_run_We
P06-1097	P03-1021	o	However union and rened alignments which are many-to-many are what are used to build competitive phrasal SMT systems because intersection performs poorly despite having been shown to have the best AER scores for the French/English corpus we are using -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_using_Ney dep_using_Och aux_using_are nsubj_using_we nn_corpus_French/English det_corpus_the prep_for_scores_corpus nn_scores_AER amod_scores_best det_scores_the dobj_have_scores aux_have_to ccomp_shown_using xcomp_shown_have auxpass_shown_been aux_shown_having prepc_despite_performs_shown advmod_performs_poorly nsubj_performs_intersection mark_performs_because nn_systems_SMT amod_systems_phrasal amod_systems_competitive dobj_build_systems aux_build_to advcl_used_performs xcomp_used_build auxpass_used_are nsubjpass_used_what ccomp_are_used nsubj_are_alignments advmod_are_However cop_many-to-many_are nsubj_many-to-many_which rcmod_alignments_many-to-many nn_alignments_rened nn_alignments_union conj_and_union_rened
P06-1097	P03-1021	o	An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model -LRB- Och 2003 -RRB-	amod_Och_2003 dep_model_Och amod_model_log-linear poss_model_its prep_of_components_model det_components_the prep_with_associated_components vmod_weights_associated det_weights_the dobj_train_weights aux_train_to nn_system_SMT det_system_the xcomp_employed_train agent_employed_system auxpass_employed_is nsubjpass_employed_set nn_set_BLEU nn_set_Maximum det_set_the ccomp_called_employed nsubj_called_set nn_set_translation amod_set_additional det_set_An
P06-1097	P03-1021	o	For each training direction we run GIZA + + -LRB- Och and Ney 2003 -RRB- specifying 5 iterations of Model 1 4 iterations of the HMM model -LRB- Vogel et al. 1996 -RRB- and 4 iterations of Model 4	num_Model_4 prep_of_iterations_Model num_iterations_4 amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et nn_model_HMM det_model_the dep_iterations_Vogel prep_of_iterations_model num_iterations_4 conj_and_Model_iterations conj_and_Model_iterations num_Model_1 prep_of_iterations_iterations prep_of_iterations_iterations prep_of_iterations_Model num_iterations_5 dobj_specifying_iterations num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_specifying conj_+_GIZA_+ dobj_run_specifying dobj_run_+ dobj_run_GIZA nsubj_run_we prep_for_run_direction nn_direction_training det_direction_each
P06-1097	P03-1021	o	We use the union re ned and intersection heuristics de ned in -LRB- Och and Ney 2003 -RRB- which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment	nn_alignment_word amod_work_recent amod_work_all advmod_all_virtually det_baseline_the num_Model_4 nn_Model_IBM prep_with_conjunction_Model prep_on_used_alignment prep_in_used_work prep_as_used_baseline prep_in_used_conjunction auxpass_used_are nsubjpass_used_which rcmod_Och_used num_Och_2003 conj_and_Och_Ney prep_in_ned_Ney prep_in_ned_Och dep_de_ned dep_heuristics_de dep_intersection_heuristics conj_and_ned_intersection nsubj_ned_re det_union_the parataxis_use_intersection parataxis_use_ned dobj_use_union nsubj_use_We
P06-1097	P03-1021	n	1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4 -LRB- Brown et al. 1993 -RRB- unsupervised training followed by post-processing with symmetrization heuristics -LRB- Och and Ney 2003 -RRB- yields low quality word alignments	nn_alignments_word nn_alignments_quality amod_alignments_low dep_yields_alignments dep_yields_Ney dep_yields_Och dep_Och_2003 conj_and_Och_Ney dep_heuristics_yields nn_heuristics_symmetrization prep_with_post-processing_heuristics agent_followed_post-processing vmod_training_followed amod_training_unsupervised dep_training_Introduction amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_IBM nn_model_translation nn_model_machine amod_model_statistical prep_for_procedure_model nn_procedure_training amod_procedure_applied det_procedure_The advmod_applied_widely advmod_widely_most dep_Introduction_Brown dep_Introduction_procedure num_Introduction_1
P06-1097	P03-1021	p	Och -LRB- 2003 -RRB- has described an ef cient exact one-dimensional error minimization technique for a similar search problem in machine translation	nn_translation_machine prep_in_problem_translation nn_problem_search amod_problem_similar det_problem_a prep_for_technique_problem nn_technique_minimization nn_technique_error amod_technique_one-dimensional amod_technique_exact amod_technique_cient nn_technique_ef det_technique_an dobj_described_technique aux_described_has nsubj_described_Och appos_Och_2003
P06-1098	P03-1021	o	Feature function scaling factors m are optimized based on a maximum likely approach -LRB- Och and Ney 2002 -RRB- or on a direct error minimization approach -LRB- Och 2003 -RRB-	amod_Och_2003 dep_approach_Och nn_approach_minimization nn_approach_error amod_approach_direct det_approach_a pobj_on_approach conj_and_Och_2002 conj_and_Och_Ney dep_approach_2002 dep_approach_Ney dep_approach_Och amod_approach_likely nn_approach_maximum det_approach_a conj_or_optimized_on prep_based_on_optimized_approach auxpass_optimized_are nsubjpass_optimized_function nn_m_factors dobj_scaling_m vmod_function_scaling nn_function_Feature
P06-1098	P03-1021	o	Many-to-many word alignments are induced by running a one-to-many word alignment model such as GIZA + + -LRB- Och and Ney 2003 -RRB- in both directions and by combining the results based on a heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn det_heuristic_a prep_on_based_heuristic vmod_results_based det_results_the dobj_combining_results pcomp_by_combining det_directions_both pobj_in_directions num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_by conj_+_GIZA_in conj_+_GIZA_+ prep_such_as_model_by prep_such_as_model_in prep_such_as_model_+ prep_such_as_model_GIZA nn_model_alignment nn_model_word amod_model_one-to-many det_model_a dobj_running_model agent_induced_running auxpass_induced_are nsubjpass_induced_alignments nn_alignments_word amod_alignments_Many-to-many
P06-1139	P03-1021	o	When evaluated against the state-of-the-art phrase-based decoder Pharaoh -LRB- Koehn 2004 -RRB- using the same experimental conditions translation table trained on the FBIS corpus -LRB- 7.2 M Chinese words and 9.2 M English words of parallel text -RRB- trigram language model trained on 155M words of English newswire interpolation weights a65 -LRB- Equation 2 -RRB- trained using discriminative training -LRB- Och 2003 -RRB- -LRB- on the 2002 NIST MT evaluation set -RRB- probabilistic beam a90 set to 0.01 histogram beam a58 set to 10 and BLEU -LRB- Papineni et al. 2002 -RRB- as our metric the WIDL-NGLM-Aa86 a129 algorithm produces translations that have a BLEU score of 0.2570 while Pharaoh translations have a BLEU score of 0.2635	prep_of_score_0.2635 nn_score_BLEU det_score_a dobj_have_score nsubj_have_translations mark_have_while nn_translations_Pharaoh prep_of_score_0.2570 nn_score_BLEU det_score_a dobj_have_score nsubj_have_that rcmod_translations_have advcl_produces_have dobj_produces_translations nsubj_produces_algorithm nn_algorithm_a129 nn_algorithm_WIDL-NGLM-Aa86 det_algorithm_the poss_metric_our amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_10_BLEU prep_to_set_BLEU prep_to_set_10 prep_as_a58_metric appos_a58_Papineni vmod_a58_set nn_a58_beam nn_a58_histogram prep_to_set_0.01 vmod_a90_set nn_a90_beam amod_a90_probabilistic nn_set_evaluation nn_set_MT nn_set_NIST num_set_2002 det_set_the pobj_on_set dep_Och_2003 appos_training_Och amod_training_discriminative dep_using_produces dobj_using_a58 conj_using_a90 dep_using_on dobj_using_training prep_trained_using num_Equation_2 vmod_a65_trained appos_a65_Equation nn_a65_weights nn_a65_interpolation nn_newswire_English prep_of_words_newswire nn_words_155M prep_on_trained_words dep_model_a65 vmod_model_trained nn_model_language nn_model_trigram amod_text_parallel prep_of_words_text nn_words_English dep_words_M num_M_9.2 conj_and_words_words amod_words_Chinese nn_words_M num_words_7.2 dep_corpus_words dep_corpus_words nn_corpus_FBIS det_corpus_the prep_on_trained_corpus vmod_table_trained nn_table_translation nn_table_conditions amod_table_experimental amod_table_same det_table_the parataxis_using_model dobj_using_table ccomp_,_using dep_Koehn_2004 dep_Pharaoh_Koehn nn_Pharaoh_decoder amod_Pharaoh_phrase-based amod_Pharaoh_state-of-the-art det_Pharaoh_the prep_against_evaluated_Pharaoh advmod_evaluated_When advcl_``_evaluated
P06-1139	P03-1021	o	The interpolation weights a65 -LRB- Equation 2 -RRB- are trained using discriminative training -LRB- Och 2003 -RRB- using ROUGEa129 as the objective function on the development set	nn_set_development det_set_the amod_function_objective det_function_the prep_on_using_set prep_as_using_function dobj_using_ROUGEa129 dep_Och_2003 appos_training_Och amod_training_discriminative xcomp_using_using dobj_using_training xcomp_trained_using auxpass_trained_are nsubjpass_trained_weights num_Equation_2 appos_a65_Equation dep_weights_a65 nn_weights_interpolation det_weights_The ccomp_``_trained
P06-2061	P03-1021	o	A statistical prediction engine provides the completions to what a human translator types -LRB- Foster et al. 1997 Och et al. 2003 -RRB-	num_Och_2003 nn_Och_al. nn_Och_et dep_Foster_Och amod_Foster_1997 dep_Foster_al. nn_Foster_et appos_types_Foster nn_types_translator amod_types_human det_types_a nsubj_what_types det_completions_the prep_to_provides_what dobj_provides_completions nsubj_provides_engine nn_engine_prediction amod_engine_statistical det_engine_A
P06-2061	P03-1021	o	In the post-editing step a prediction engine helps to decrease the amount of human interaction -LRB- Och et al. 2003 -RRB-	amod_Och_2003 dep_Och_al. nn_Och_et amod_interaction_human prep_of_amount_interaction det_amount_the dep_decrease_Och dobj_decrease_amount aux_decrease_to xcomp_helps_decrease nsubj_helps_engine prep_in_helps_step nn_engine_prediction det_engine_a amod_step_post-editing det_step_the
P06-2061	P03-1021	o	For instance the resulting word graph can be used in the prediction engine of a CAT system -LRB- Och et al. 2003 -RRB-	amod_Och_2003 dep_Och_al. nn_Och_et nn_system_CAT det_system_a prep_of_engine_system nn_engine_prediction det_engine_the dep_used_Och prep_in_used_engine auxpass_used_be aux_used_can nsubjpass_used_graph prep_for_used_instance nn_graph_word amod_graph_resulting det_graph_the
P06-2061	P03-1021	o	The model scaling factors M1 are trained on a development corpus according to the final recognition quality measured by the word error rate -LRB- WER -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_rate_Och appos_rate_WER nn_rate_error nn_rate_word det_rate_the agent_measured_rate vmod_quality_measured nn_quality_recognition amod_quality_final det_quality_the nn_corpus_development det_corpus_a pobj_trained_quality prepc_according_to_trained_to prep_on_trained_corpus auxpass_trained_are nsubjpass_trained_M1 nn_M1_factors nn_M1_scaling nn_M1_model det_M1_The
P06-2101	P03-1021	o	To find the optimal coefficients for a loglinear combination of these experts we use separate development data using the following procedure due to Och -LRB- 2003 -RRB- 1	appos_Och_2003 prep_due_to_procedure_Och amod_procedure_following det_procedure_the dobj_using_procedure nn_data_development amod_data_separate dep_use_1 vmod_use_using dobj_use_data nsubj_use_we advcl_use_find det_experts_these prep_of_combination_experts amod_combination_loglinear det_combination_a prep_for_coefficients_combination amod_coefficients_optimal det_coefficients_the dobj_find_coefficients aux_find_To
P06-2101	P03-1021	o	Despite these difficulties some work has shown it worthwhile to minimize error directly -LRB- Och 2003 Bahl et al. 1988 -RRB-	num_Bahl_1988 nn_Bahl_al. nn_Bahl_et dep_Och_Bahl dep_Och_2003 dep_minimize_Och advmod_minimize_directly dobj_minimize_error aux_minimize_to dep_minimize_worthwhile nsubj_minimize_it xcomp_shown_minimize aux_shown_has nsubj_shown_work prep_despite_shown_difficulties det_work_some det_difficulties_these
P06-2101	P03-1021	o	Och -LRB- 2003 -RRB- observed however that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space and hence to minimize it globally along that line	det_line_that prep_along_minimize_line advmod_minimize_globally dobj_minimize_it aux_minimize_to advmod_minimize_hence nsubj_minimize_property nn_space_parameter prep_in_line_space det_line_any prep_along_function_line advmod_function_exhaustively det_function_the dobj_characterize_function aux_characterize_to conj_and_exploited_minimize xcomp_exploited_characterize auxpass_exploited_be aux_exploited_could nsubjpass_exploited_property mark_exploited_that amod_property_piecewiseconstant det_property_the ccomp_,_minimize ccomp_,_exploited dep_,_however vmod_Och_observed appos_Och_2003 dep_``_Och
P06-2101	P03-1021	o	Och -LRB- 2003 -RRB- found that such smoothing during training gives almost identical results on translation metrics	nn_metrics_translation prep_on_results_metrics amod_results_identical advmod_identical_almost dobj_gives_results nsubj_gives_smoothing mark_gives_that prep_during_smoothing_training amod_smoothing_such ccomp_found_gives nsubj_found_Och appos_Och_2003
P06-2103	P03-1021	o	The solution we employ here is the discriminative training procedure of Och -LRB- 2003 -RRB-	appos_Och_2003 prep_of_procedure_Och nn_procedure_training amod_procedure_discriminative det_procedure_the cop_procedure_is nsubj_procedure_solution advmod_employ_here nsubj_employ_we rcmod_solution_employ det_solution_The
P06-2103	P03-1021	o	There are two necessary ingredients to implement Ochs -LRB- 2003 -RRB- training procedure	nn_procedure_training nn_procedure_Ochs appos_Ochs_2003 dobj_implement_procedure aux_implement_to vmod_ingredients_implement amod_ingredients_necessary num_ingredients_two nsubj_are_ingredients expl_are_There
P06-2103	P03-1021	o	In contrast more recent research has focused on stochastic approaches that model discourse coherence at the local lexical -LRB- Lapata 2003 -RRB- and global levels -LRB- Barzilay and Lee 2004 -RRB- while preserving regularities recognized by classic discourse theories -LRB- Barzilay and Lapata 2005 -RRB-	dep_Barzilay_2005 conj_and_Barzilay_Lapata dep_theories_Lapata dep_theories_Barzilay nn_theories_discourse amod_theories_classic agent_recognized_theories vmod_regularities_recognized dobj_preserving_regularities amod_Barzilay_2004 conj_and_Barzilay_Lee dep_levels_Lee dep_levels_Barzilay amod_levels_global amod_Lapata_2003 conj_and_lexical_levels dep_lexical_Lapata amod_lexical_local det_lexical_the nn_coherence_discourse prepc_while_model_preserving prep_at_model_levels prep_at_model_lexical dobj_model_coherence nsubj_model_that rcmod_approaches_model amod_approaches_stochastic prep_on_focused_approaches aux_focused_has nsubj_focused_research prep_in_focused_contrast amod_research_recent advmod_recent_more
P07-1004	P03-1021	o	Their weights are optimized w.r.t. BLEU score using the algorithm described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_algorithm_described det_algorithm_the dobj_using_algorithm vmod_score_using nn_score_BLEU nn_score_w.r.t. dobj_optimized_score auxpass_optimized_are nsubjpass_optimized_weights poss_weights_Their ccomp_``_optimized
P07-1005	P03-1021	o	To perform translation state-of-the-art MT systems use a statistical phrase-based approach -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB- by treating phrases as the basic units of translation	prep_of_units_translation amod_units_basic det_units_the prep_as_treating_units dobj_treating_phrases dep_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_Koehn conj_and_Marcu_2002 conj_and_Marcu_Wong dep_approach_Koehn dep_approach_2002 dep_approach_Wong dep_approach_Marcu amod_approach_phrase-based amod_approach_statistical det_approach_a prepc_by_use_treating dobj_use_approach nsubj_use_systems advcl_use_perform nn_systems_MT amod_systems_state-of-the-art dobj_perform_translation aux_perform_To
P07-1005	P03-1021	o	6.1 Hiero Results Using the MT 2002 test set we ran the minimumerror rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- with the decoder to tune the weights for each feature	det_feature_each prep_for_weights_feature det_weights_the dobj_tune_weights aux_tune_to det_decoder_the dep_Och_2003 dep_training_Och appos_training_MERT nn_training_rate nn_training_minimumerror det_training_the vmod_ran_tune prep_with_ran_decoder dobj_ran_training nsubj_ran_we nsubj_ran_Results nn_set_test num_set_2002 nn_set_MT det_set_the dobj_Using_set vmod_Results_Using nn_Results_Hiero num_Results_6.1
P07-1024	P03-1021	o	This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs and we employ an optimization technique similar to that used by Och -LRB- 2003 -RRB- for machine translation	nn_translation_machine appos_Och_2003 prep_for_used_translation agent_used_Och vmod_that_used prep_to_similar_that amod_technique_similar nn_technique_optimization det_technique_an dobj_employ_technique nsubj_employ_we nn_outputs_translation nn_outputs_machine nn_outputs_candidate prep_of_reranking_outputs prep_for_weights_reranking nn_weights_feature amod_weights_optimizing prep_of_problem_weights det_problem_the conj_and_reminiscent_employ prep_of_reminiscent_problem cop_reminiscent_is nsubj_reminiscent_setting det_setting_This
P07-1037	P03-1021	o	The NIST MT03 test set is used for development particularly for optimizing the interpolation weights using Minimum Error Rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_Rate nn_training_Error nn_training_Minimum dobj_using_training nn_weights_interpolation det_weights_the vmod_optimizing_using dobj_optimizing_weights prepc_for_used_optimizing advmod_used_particularly prep_for_used_development auxpass_used_is nsubjpass_used_set nn_set_test nn_set_MT03 nn_set_NIST det_set_The
P07-1037	P03-1021	o	Firstly rather than induce millions of xRS rules from parallel data we extract phrase pairs in the standard way -LRB- Och & Ney 2003 -RRB- and associate with each phrase-pair a set of target language syntactic structures based on supertag sequences	nn_sequences_supertag amod_structures_syntactic nn_structures_language nn_structures_target pobj_set_sequences prepc_based_on_set_on prep_of_set_structures det_set_a dep_phrase-pair_set det_phrase-pair_each prep_with_associate_phrase-pair dep_Och_2003 conj_and_Och_Ney dep_way_Ney dep_way_Och amod_way_standard det_way_the nn_pairs_phrase conj_and_extract_associate prep_in_extract_way dobj_extract_pairs nsubj_extract_we ccomp_extract_induce amod_data_parallel nn_rules_xRS prep_of_millions_rules prep_from_induce_data dobj_induce_millions advmod_induce_rather advmod_induce_Firstly mwe_rather_than
P07-1037	P03-1021	o	The bidirectional word alignment is used to obtain phrase translation pairs using heuristics presented in 2http / / www.fjoch.com/GIZA++.html 289 -LRB- Och & Ney 2003 -RRB- and -LRB- Koehn et al. 2003 -RRB- and the Moses decoder was used for phrase extraction and decoding .3 Let t and s be the target and source language sentences respectively	advmod_sentences_respectively nn_sentences_language nn_sentences_source nn_sentences_target conj_and_target_source det_target_the cop_target_be csubj_target_Let aux_target_decoding conj_and_t_s dobj_Let_s dobj_Let_t nsubj_Let_.3 nn_extraction_phrase conj_and_used_sentences prep_for_used_extraction auxpass_used_was nsubjpass_used_decoder nsubjpass_used_Koehn nsubjpass_used_www.fjoch.com/GIZA++.html nn_decoder_Moses det_decoder_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Och_2003 conj_and_Och_Ney conj_and_www.fjoch.com/GIZA++.html_decoder conj_and_www.fjoch.com/GIZA++.html_Koehn appos_www.fjoch.com/GIZA++.html_Ney appos_www.fjoch.com/GIZA++.html_Och num_www.fjoch.com/GIZA++.html_289 prep_in_presented_2http vmod_heuristics_presented dobj_using_heuristics nn_pairs_translation nn_pairs_phrase xcomp_obtain_using dobj_obtain_pairs aux_obtain_to parataxis_used_sentences parataxis_used_used xcomp_used_obtain auxpass_used_is nsubjpass_used_alignment nn_alignment_word amod_alignment_bidirectional det_alignment_The
P07-1037	P03-1021	o	The bidirectional word alignmentisusedtoobtainlexicalphrasetranslationpairs using heuristics presented in -LRB- Och & Ney 2003 -RRB- and -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_Och_Koehn dep_Och_2003 conj_and_Och_Ney prep_in_presented_Koehn prep_in_presented_Ney prep_in_presented_Och vmod_heuristics_presented dobj_using_heuristics vmod_alignmentisusedtoobtainlexicalphrasetranslationpairs_using nn_alignmentisusedtoobtainlexicalphrasetranslationpairs_word amod_alignmentisusedtoobtainlexicalphrasetranslationpairs_bidirectional det_alignmentisusedtoobtainlexicalphrasetranslationpairs_The dep_``_alignmentisusedtoobtainlexicalphrasetranslationpairs
P07-1039	P03-1021	o	4.3 Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline GIZA + + implementation of IBM word alignment model 4 -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- ,8 the refinement and phrase-extraction heuristics described in -LRB- Koehn et al. 2003 -RRB- minimum-error-rate training 7More specifically we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs	nn_pairs_sentence amod_pairs_new dobj_construct_pairs aux_construct_to amod_sentence_Chinese det_sentence_the vmod_references_construct conj_and_references_sentence num_references_7 det_references_the prep_from_reference_sentence prep_from_reference_references nn_reference_English amod_reference_first det_reference_the dobj_choose_reference nsubj_choose_we advmod_7More_specifically nn_7More_training amod_7More_minimum-error-rate rcmod_Koehn_choose appos_Koehn_7More amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn amod_heuristics_phrase-extraction vmod_refinement_described conj_and_refinement_heuristics det_refinement_the dep_,8_heuristics dep_,8_refinement dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_model_,8 dep_model_Brown num_model_4 nn_model_alignment nn_model_word dep_GIZA_model prep_of_GIZA_IBM conj_+_GIZA_implementation det_baseline_a nn_system_translation nn_system_machine amod_system_statistical amod_system_phrase-based amod_system_log-linear amod_system_standard det_system_a prep_as_use_baseline dobj_use_system nsubj_use_We dep_Baseline_implementation dep_Baseline_GIZA rcmod_Baseline_use num_Baseline_4.3 dep_``_Baseline
P07-1039	P03-1021	o	Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2 ChineseEnglish corpus statistics -LRB- Och 2003 -RRB- using Phramer -LRB- Olteanu et al. 2006 -RRB- a 3-gram language model with Kneser-Ney smoothing trained with SRILM -LRB- Stolcke 2002 -RRB- on the English side of the training data and Pharaoh -LRB- Koehn 2004 -RRB- with default settings to decode	aux_decode_to nn_settings_default dep_Koehn_2004 dep_Pharaoh_Koehn conj_and_data_Pharaoh nn_data_training det_data_the prep_of_side_Pharaoh prep_of_side_data amod_side_English det_side_the dep_Stolcke_2002 appos_SRILM_Stolcke xcomp_trained_decode prep_with_trained_settings prep_on_trained_side prep_with_trained_SRILM vmod_smoothing_trained amod_smoothing_Kneser-Ney prep_with_model_smoothing nn_model_language amod_model_3-gram det_model_a amod_Olteanu_2006 dep_Olteanu_al. nn_Olteanu_et appos_Phramer_model dep_Phramer_Olteanu dobj_using_Phramer dep_Och_2003 vmod_statistics_using dep_statistics_Och nn_statistics_corpus nn_statistics_ChineseEnglish num_Table_2 num_Table_1,081 num_Table_569 dep_size_Table nn_size_Vocabulary num_size_14,437 num_size_1,864 dep_words_size dep_Running_statistics dobj_Running_words ccomp_``_Running
P07-1039	P03-1021	o	To quickly -LRB- and approximately -RRB- evaluate this phenomenon we trained the statistical IBM wordalignment model 4 -LRB- Brown et al. 1993 -RRB- ,1 using the GIZA + + software -LRB- Och and Ney 2003 -RRB- for the following language pairs ChineseEnglish Italian English and DutchEnglish using the IWSLT-2006 corpus -LRB- Takezawa et al. 2002 Paul 2006 -RRB- for the first two language pairs and the Europarl corpus -LRB- Koehn 2005 -RRB- for the last one	amod_one_last det_one_the dep_Koehn_2005 prep_for_corpus_one appos_corpus_Koehn nn_corpus_Europarl det_corpus_the nn_pairs_language num_pairs_two amod_pairs_first det_pairs_the dep_Paul_2006 conj_and_Takezawa_corpus prep_for_Takezawa_pairs dep_Takezawa_Paul appos_Takezawa_2002 dep_Takezawa_al. nn_Takezawa_et nn_corpus_IWSLT-2006 det_corpus_the dobj_using_corpus amod_English_Italian dep_ChineseEnglish_corpus dep_ChineseEnglish_Takezawa vmod_ChineseEnglish_using conj_and_ChineseEnglish_DutchEnglish appos_ChineseEnglish_English nn_pairs_language amod_pairs_following det_pairs_the dep_Och_2003 conj_and_Och_Ney prep_for_software_pairs appos_software_Ney appos_software_Och pobj_+_software dep_GIZA_DutchEnglish dep_GIZA_ChineseEnglish conj_+_GIZA_+ det_GIZA_the dobj_using_+ dobj_using_GIZA advmod_,1_quickly num_Brown_1993 nn_Brown_al. nn_Brown_et num_model_4 nn_model_wordalignment nn_model_IBM amod_model_statistical det_model_the dobj_trained_model nsubj_trained_we det_phenomenon_this dobj_evaluate_Brown parataxis_evaluate_trained dobj_evaluate_phenomenon advmod_evaluate_approximately cc_evaluate_and dep_quickly_evaluate dep_To_using pobj_To_,1 dep_``_To
P07-1039	P03-1021	o	there is want to need not in front of as soon as look at Figure 2 Examples of entries from the manually developed dictionary 4 Experimental Setting 4.1 Evaluation The intrinsic quality of word alignment can be assessed using the Alignment Error Rate -LRB- AER -RRB- metric -LRB- Och and Ney 2003 -RRB- that compares a systems alignment output to a set of gold-standard alignment	amod_alignment_gold-standard prep_of_set_alignment det_set_a nn_output_alignment nn_output_systems det_output_a prep_to_compares_set dobj_compares_output nsubj_compares_that dep_Och_2003 conj_and_Och_Ney dep_Rate_Ney dep_Rate_Och amod_Rate_metric appos_Rate_AER nn_Rate_Error nn_Rate_Alignment det_Rate_the dobj_using_Rate xcomp_assessed_using auxpass_assessed_be aux_assessed_can nsubjpass_assessed_quality nn_alignment_word prep_of_quality_alignment amod_quality_intrinsic det_quality_The rcmod_Evaluation_assessed num_Evaluation_4.1 dobj_Setting_Evaluation xcomp_Experimental_Setting amod_4_Experimental num_dictionary_4 dep_developed_dictionary advmod_developed_manually vmod_the_developed prep_from_entries_the rcmod_Examples_compares prep_of_Examples_entries num_Figure_2 dep_look_Examples prep_at_look_Figure dep_as_look dep_soon_as advmod_soon_as prep_front_of dep_in_soon pobj_in_front dep_not_in dep_need_not dep_to_need prep_want_to dep_is_want expl_is_there
P07-1040	P03-1021	o	The same Powells method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och nn_decoder_MT amod_decoder_phrasal amod_decoder_feature-based amod_decoder_standard det_decoder_a prep_of_weights_decoder nn_weights_feature prep_estimate_in dobj_estimate_weights aux_estimate_to xcomp_used_estimate auxpass_used_been aux_used_has nsubjpass_used_method nn_method_Powells amod_method_same det_method_The
P07-1040	P03-1021	o	In -LRB- Matusov et al. 2006 -RRB- different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ dobj_using_+ dobj_using_GIZA amod_corpus_parallel det_corpus_a nn_pairs_hypothesis det_pairs_all vmod_considering_using prep_as_considering_corpus dobj_considering_pairs nn_models_alignment nn_models_training agent_taken_considering agent_taken_models prep_into_taken_account auxpass_taken_are nsubjpass_taken_orderings prep_taken_In nn_orderings_word amod_orderings_different nn_al._et amod_Matusov_2006 dep_Matusov_al. dep_In_Matusov
P07-1059	P03-1021	o	We present two approaches to SMT-based query expansion both of which are implemented in the framework of phrase-based SMT -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_SMT_2004 dep_SMT_Ney dep_SMT_Och amod_SMT_phrase-based prep_of_framework_SMT det_framework_the prep_in_implemented_framework auxpass_implemented_are nsubjpass_implemented_both prep_of_both_which nn_expansion_query amod_expansion_SMT-based prep_to_approaches_expansion num_approaches_two parataxis_present_implemented dobj_present_approaches nsubj_present_We
P07-1059	P03-1021	o	4 SMT-Based Query Expansion Our SMT-based query expansion techniques are based on a recent implementation of the phrasebased SMT framework -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_framework_SMT amod_framework_phrasebased det_framework_the prep_of_implementation_framework amod_implementation_recent det_implementation_a prep_on_based_implementation auxpass_based_are nsubjpass_based_techniques nn_techniques_expansion nn_techniques_query amod_techniques_SMT-based poss_techniques_Our dep_Expansion_Koehn rcmod_Expansion_based nn_Expansion_Query amod_Expansion_SMT-Based num_Expansion_4 ccomp_``_Expansion
P07-1059	P03-1021	o	as follows p -LRB- synI1 | trgI1 -RRB- = -LRB- Iproductdisplay i = 1 p -LRB- syni | trgi -RRB- -LRB- 4 -RRB- pprime -LRB- trgi | syni -RRB- prime pw -LRB- syni | trgi -RRB- w pwprime -LRB- trgi | syni -RRB- wprime pd -LRB- syni trgi -RRB- d -RRB- lw -LRB- synI1 -RRB- l c -LRB- synI1 -RRB- c pLM -LRB- synI1 -RRB- LM For estimation of the feature weights vector defined in equation -LRB- 4 -RRB- we employed minimum error rate -LRB- MER -RRB- training under the BLEU measure -LRB- Och 2003 -RRB-	amod_Och_2003 dep_measure_Och nn_measure_BLEU det_measure_the nn_training_rate appos_rate_MER nn_rate_error amod_rate_minimum prep_under_employed_measure dobj_employed_training nsubj_employed_we appos_equation_4 prep_in_defined_equation vmod_vector_defined nn_vector_weights nn_vector_feature det_vector_the prep_of_estimation_vector nn_LM_pLM dep_LM_d appos_pLM_synI1 nn_pLM_c nn_pLM_c appos_c_synI1 nn_c_l nn_c_lw appos_lw_synI1 dep_d_syni nn_d_pd appos_syni_trgi dep_wprime_employed prep_for_wprime_estimation dobj_wprime_LM num_syni_| nn_syni_trgi dobj_|_trgi nsubj_|_syni dep_pw_| amod_pw_prime num_syni_| dep_trgi_syni dobj_|_trgi nsubj_|_syni dep_p_| num_p_1 dep_=_4 dep_=_p nsubj_=_i dep_=_Iproductdisplay dep_=_wprime dep_=_syni dep_=_pwprime dep_=_w dep_=_pw dep_=_trgi dep_=_pprime dep_=_= nsubj_=_p dep_=_follows num_trgI1_| nn_trgI1_synI1 appos_p_trgI1 mark_follows_as
P07-1089	P03-1021	o	To perform minimum error rate training -LRB- Och 2003 -RRB- to tune the feature weights to maximize the systems BLEU score on development set we used the script optimizeV5IBMBLEU.m -LRB- Venugopal and Vogel 2005 -RRB-	dep_Venugopal_2005 conj_and_Venugopal_Vogel dep_optimizeV5IBMBLEU.m_Vogel dep_optimizeV5IBMBLEU.m_Venugopal nn_optimizeV5IBMBLEU.m_script det_optimizeV5IBMBLEU.m_the dobj_used_optimizeV5IBMBLEU.m nsubj_used_we advcl_used_perform nn_set_development prep_on_score_set nn_score_BLEU nn_score_systems det_score_the dobj_maximize_score aux_maximize_to nn_weights_feature det_weights_the vmod_tune_maximize dobj_tune_weights aux_tune_to dep_Och_2003 nn_training_rate nn_training_error amod_training_minimum vmod_perform_tune dep_perform_Och dobj_perform_training aux_perform_To
P07-1089	P03-1021	o	We ran GIZA + + -LRB- Och and Ney 2000 -RRB- on the training corpus in both directions using its default setting and then applied the refinement rule diagand described in -LRB- Koehn et al. 2003 -RRB- to obtain a single many-to-many word alignment for each sentence pair	nn_pair_sentence det_pair_each prep_for_alignment_pair nn_alignment_word nn_alignment_many-to-many amod_alignment_single det_alignment_a dobj_obtain_alignment aux_obtain_to amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn xcomp_described_obtain prep_described_in vmod_diagand_described nn_diagand_rule nn_diagand_refinement det_diagand_the dobj_applied_diagand advmod_applied_then nn_setting_default poss_setting_its conj_and_using_applied dobj_using_setting preconj_directions_both nn_corpus_training det_corpus_the num_Och_2000 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ dep_ran_applied dep_ran_using prep_in_ran_directions dobj_ran_+ dobj_ran_GIZA nsubj_ran_We
P07-1091	P03-1021	o	All the feature weights -LRB- s -RRB- were trained using our implementation of Minimum Error Rate Training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum prep_of_implementation_Training poss_implementation_our dobj_using_implementation xcomp_trained_using auxpass_trained_were nsubjpass_trained_weights appos_weights_s nn_weights_feature det_weights_the predet_weights_All ccomp_``_trained
P07-1091	P03-1021	o	We use the Stanford parser -LRB- Klein and Manning 2003 -RRB- with its default Chinese grammar the GIZA + + -LRB- Och and Ney 2000 -RRB- alignment package with its default settings and the ME tool developed by -LRB- Zhang 2004 -RRB-	amod_Zhang_2004 dep_by_Zhang prep_developed_by vmod_tool_developed nn_tool_ME det_tool_the nn_settings_default poss_settings_its nn_package_alignment dep_package_Ney dep_package_Och cc_package_+ num_Och_2000 conj_and_Och_Ney conj_and_GIZA_tool prep_with_GIZA_settings conj_+_GIZA_package det_GIZA_the amod_grammar_Chinese nn_grammar_default poss_grammar_its dep_Klein_2003 conj_and_Klein_Manning appos_parser_Manning appos_parser_Klein nn_parser_Stanford det_parser_the dobj_use_tool dobj_use_package dobj_use_GIZA prep_with_use_grammar dobj_use_parser nsubj_use_We
P07-1092	P03-1021	o	The parameters j were trained using minimum error rate training -LRB- Och 2003 -RRB- to maximise the BLEU score -LRB- Papineni et al. 2002 -RRB- on a 150 sentence development set	nn_set_development nn_set_sentence num_set_150 det_set_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dobj_maximise_score aux_maximise_to dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_using_maximise dobj_using_training prep_on_trained_set dep_trained_Papineni xcomp_trained_using auxpass_trained_were nsubjpass_trained_parameters appos_parameters_j det_parameters_The
P07-1092	P03-1021	o	The translation models and lexical scores were estimated on the training corpus whichwasautomaticallyalignedusingGiza + + -LRB- Och et al. 1999 -RRB- in both directions between source and target and symmetrised using the growing heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_heuristic_growing det_heuristic_the dobj_using_heuristic vmod_source_using conj_and_source_symmetrised conj_and_source_target prep_between_directions_symmetrised prep_between_directions_target prep_between_directions_source preconj_directions_both amod_Och_1999 dep_Och_al. nn_Och_et dep_whichwasautomaticallyalignedusingGiza_Och conj_+_whichwasautomaticallyalignedusingGiza_+ nn_whichwasautomaticallyalignedusingGiza_corpus nn_whichwasautomaticallyalignedusingGiza_training det_whichwasautomaticallyalignedusingGiza_the dep_estimated_Koehn prep_in_estimated_directions prep_on_estimated_+ prep_on_estimated_whichwasautomaticallyalignedusingGiza auxpass_estimated_were nsubjpass_estimated_scores nsubjpass_estimated_models amod_scores_lexical conj_and_models_scores nn_models_translation det_models_The ccomp_``_estimated
P07-1092	P03-1021	o	A single translation is then selected by finding the candidate that yields the best overall score -LRB- Och and Ney 2001 Utiyama and Isahara 2007 -RRB- or by cotraining -LRB- Callison-Burch and Osborne 2003 -RRB-	amod_Osborne_2003 conj_and_Callison-Burch_Osborne dep_cotraining_Osborne dep_cotraining_Callison-Burch pcomp_by_cotraining dep_Utiyama_2007 conj_and_Utiyama_Isahara conj_or_Och_by conj_and_Och_Isahara conj_and_Och_Utiyama conj_and_Och_2001 conj_and_Och_Ney dep_score_by dep_score_Utiyama dep_score_2001 dep_score_Ney dep_score_Och amod_score_overall amod_score_best det_score_the dobj_yields_score nsubj_yields_that rcmod_candidate_yields det_candidate_the dobj_finding_candidate agent_selected_finding advmod_selected_then auxpass_selected_is nsubjpass_selected_translation amod_translation_single det_translation_A
P07-1092	P03-1021	o	As an alternative to linear interpolation we also employ a weighted product for phrase-table combination p -LRB- s | t -RRB- productdisplay j pj -LRB- s | t -RRB- j -LRB- 3 -RRB- This has the same form used for log-linear training of SMT decoders -LRB- Och 2003 -RRB- which allows us to treateachdistributionasafeature andlearnthemixing weights automatically	advmod_weights_automatically amod_weights_andlearnthemixing prep_to_allows_treateachdistributionasafeature dobj_allows_us nsubj_allows_which dep_Och_2003 appos_decoders_weights rcmod_decoders_allows appos_decoders_Och nn_decoders_SMT prep_of_training_decoders amod_training_log-linear prep_for_used_training vmod_form_used amod_form_same det_form_the dobj_has_form nsubj_has_This rcmod_j_has appos_j_3 dobj_|_t nsubj_|_s dep_pj_j dep_pj_| nn_pj_j nn_pj_productdisplay nn_pj_p dobj_|_t nsubj_|_s dep_p_| amod_combination_phrase-table prep_for_product_combination amod_product_weighted det_product_a dep_employ_pj dobj_employ_product advmod_employ_also nsubj_employ_we prep_as_employ_alternative amod_interpolation_linear prep_to_alternative_interpolation det_alternative_an
P07-1108	P03-1021	o	1 Introduction For statistical machine translation -LRB- SMT -RRB- phrasebased methods -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and syntax-based methods -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knignt 2001 Melamed 2004 Chiang 2005 Quick et al. 2005 Mellebeek et al. 2006 -RRB- outperform word-based methods -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_methods_Brown amod_methods_word-based dobj_outperform_methods nsubj_outperform_methods nsubj_outperform_methods ccomp_outperform_Introduction nn_al._et nn_al._Mellebeek dep_al._2005 nn_al._et nn_al._Quick num_Chiang_2005 num_Melamed_2004 conj_and_Yamada_al. conj_and_Yamada_Chiang conj_and_Yamada_Melamed conj_and_Yamada_2001 conj_and_Yamada_Knignt num_al._2000 nn_al._et nn_al._Alshawi amod_Wu_2006 dep_Wu_al. dep_Wu_al. dep_Wu_Chiang dep_Wu_Melamed dep_Wu_2001 dep_Wu_Knignt dep_Wu_Yamada dep_Wu_al. amod_Wu_1997 dep_methods_Wu amod_methods_syntax-based dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_methods_methods dep_methods_Koehn amod_methods_phrasebased appos_translation_SMT nn_translation_machine amod_translation_statistical prep_for_Introduction_translation num_Introduction_1
P07-1108	P03-1021	o	We run the decoder with its default settings and then use Koehn 's implementation of minimum error rate training -LRB- Och 2003 -RRB- to tune the feature weights on the development set	nn_set_development det_set_the prep_on_weights_set nn_weights_feature det_weights_the dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_of_implementation_training poss_implementation_Koehn vmod_use_tune dobj_use_implementation advmod_use_then nsubj_use_We nn_settings_default poss_settings_its det_decoder_the conj_and_run_use prep_with_run_settings dobj_run_decoder nsubj_run_We
P07-1111	P03-1021	o	We want to avoid training a metric that as5Or in a less adversarial setting a system may be performing minimum error-rate training -LRB- Och 2003 -RRB- signs a higher than deserving score to a sentence that just happens to have many n-gram matches against the target-language reference corpus	nn_corpus_reference amod_corpus_target-language det_corpus_the prep_against_matches_corpus amod_matches_n-gram amod_matches_many dobj_have_matches aux_have_to xcomp_happens_have advmod_happens_just nsubj_happens_that rcmod_sentence_happens det_sentence_a amod_score_deserving prep_than_higher_score prep_to_a_sentence amod_a_higher dep_signs_a nn_signs_training dep_Och_2003 appos_training_Och amod_training_error-rate amod_training_minimum dobj_performing_signs aux_performing_be aux_performing_may nsubj_performing_system prep_in_performing_setting nsubj_performing_as5Or mark_performing_that det_system_a amod_setting_adversarial amod_setting_less det_setting_a ccomp_metric_performing det_metric_a amod_training_metric dobj_avoid_training aux_avoid_to xcomp_want_avoid nsubj_want_We
P07-1111	P03-1021	o	Metrics in the Rouge family allow for skip n-grams -LRB- Lin and Och 2004a -RRB- Kauchak and Barzilay -LRB- 2006 -RRB- take paraphrasing into account metrics such as METEOR -LRB- Banerjee and Lavie 2005 -RRB- and GTM -LRB- Melamed et al. 2003 -RRB- calculate both recall and precision METEOR is also similar to SIA -LRB- Liu and Gildea 2006 -RRB- in that word class information is used	auxpass_used_is nsubjpass_used_similar nn_information_class nn_information_word det_information_that dep_Liu_2006 conj_and_Liu_Gildea appos_SIA_Gildea appos_SIA_Liu prep_in_similar_information prep_to_similar_SIA advmod_similar_also cop_similar_is nsubj_similar_METEOR parataxis_recall_used conj_and_recall_precision preconj_recall_both dep_calculate_precision dep_calculate_recall nsubj_calculate_n-grams amod_Melamed_2003 dep_Melamed_al. nn_Melamed_et dep_Banerjee_2005 conj_and_Banerjee_Lavie conj_and_METEOR_GTM dep_METEOR_Lavie dep_METEOR_Banerjee dep_metrics_Melamed prep_such_as_metrics_GTM prep_such_as_metrics_METEOR prep_into_paraphrasing_account xcomp_take_paraphrasing nsubj_take_Barzilay nsubj_take_Kauchak appos_Barzilay_2006 conj_and_Kauchak_Barzilay appos_Lin_2004a conj_and_Lin_Och conj_n-grams_metrics conj_n-grams_take dep_n-grams_Och dep_n-grams_Lin ccomp_skip_calculate prepc_for_allow_skip nsubj_allow_Metrics nn_family_Rouge det_family_the prep_in_Metrics_family
P07-2026	P03-1021	o	The model scaling factors M1 are optimized with respect to the BLEU score as described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in mark_described_as nn_score_BLEU det_score_the advcl_optimized_described prep_with_respect_to_optimized_score auxpass_optimized_are nsubjpass_optimized_M1 nn_M1_factors nn_M1_scaling nn_M1_model det_M1_The
P07-2045	P03-1021	o	It also contains tools for tuning these models using minimum error rate training -LRB- Och 2003 -RRB- and evaluating the resulting translations using the BLEU score -LRB- Papineni et al. 2002 -RRB-	dep_2002_al. nn_al._et num_Papineni_2002 nn_score_BLEU det_score_the dobj_using_score amod_translations_resulting det_translations_the vmod_evaluating_using dobj_evaluating_translations nsubj_evaluating_It num_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training vmod_models_using det_models_these prep_for_tools_tuning dep_contains_Papineni conj_and_contains_evaluating dobj_contains_models dobj_contains_tools advmod_contains_also nsubj_contains_It
P07-2045	P03-1021	o	Moses uses standard external tools for some of the tasks to avoid duplication such as GIZA + + -LRB- Och and Ney 2003 -RRB- for word alignments and SRILM for language modeling	nn_modeling_language nn_alignments_word pobj_for_alignments num_Ney_2003 conj_and_Och_Ney dep_+_Ney dep_+_Och conj_and_GIZA_SRILM conj_+_GIZA_for conj_+_GIZA_+ prep_for_duplication_modeling prep_such_as_duplication_SRILM prep_such_as_duplication_for prep_such_as_duplication_+ prep_such_as_duplication_GIZA dobj_avoid_duplication aux_avoid_to det_tasks_the prep_of_some_tasks amod_tools_external amod_tools_standard vmod_uses_avoid prep_for_uses_some dobj_uses_tools nsubj_uses_Moses
P07-2046	P03-1021	o	The weighting parameters of these features were optimized in terms of BLEU by the approach of minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_of_approach_training det_approach_the prep_of_terms_BLEU agent_optimized_approach prep_in_optimized_terms auxpass_optimized_were nsubjpass_optimized_parameters det_features_these prep_of_parameters_features nn_parameters_weighting det_parameters_The
P07-2046	P03-1021	p	1 Introduction Raw parallel data need to be preprocessed in the modern phrase-based SMT before they are aligned by alignment algorithms one of which is the wellknown tool GIZA + + -LRB- Och and Ney 2003 -RRB- for training IBM models -LRB- 1-4 -RRB-	appos_models_1-4 nn_models_IBM nn_models_training pobj_for_models num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_for conj_+_GIZA_+ appos_tool_for appos_tool_+ appos_tool_GIZA amod_tool_wellknown det_tool_the cop_tool_is nsubj_tool_one prep_of_one_which nn_algorithms_alignment agent_aligned_algorithms auxpass_aligned_are nsubjpass_aligned_they mark_aligned_before amod_SMT_phrase-based amod_SMT_modern det_SMT_the prep_in_preprocessed_SMT auxpass_preprocessed_be aux_preprocessed_to parataxis_need_tool advcl_need_aligned xcomp_need_preprocessed nsubj_need_data nn_data_parallel nn_data_Raw nn_data_Introduction num_data_1
P08-1009	P03-1021	o	Word alignments are provided by GIZA + + -LRB- Och and Ney 2003 -RRB- with grow-diag-final combination with infrastructure for alignment combination and phrase extraction provided by the shared task	amod_task_shared det_task_the agent_provided_task nn_extraction_phrase vmod_combination_provided conj_and_combination_extraction nn_combination_alignment amod_combination_grow-diag-final prep_for_Och_extraction prep_for_Och_combination prep_with_Och_infrastructure prep_with_Och_combination num_Och_2003 conj_and_Och_Ney pobj_+_Ney pobj_+_Och conj_+_GIZA_+ agent_provided_+ agent_provided_GIZA auxpass_provided_are nsubjpass_provided_alignments nn_alignments_Word
P08-1009	P03-1021	o	Candidate translations are scored by a linear combination of models weighted according to Minimum Error Rate Training or MERT -LRB- Och 2003 -RRB-	amod_Och_2003 dep_MERT_Och conj_or_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum pobj_weighted_MERT pobj_weighted_Training prepc_according_to_weighted_to prep_of_combination_models amod_combination_linear det_combination_a dep_scored_weighted agent_scored_combination auxpass_scored_are nsubjpass_scored_translations nn_translations_Candidate
P08-1009	P03-1021	o	Early experiments with syntactically-informed phrases -LRB- Koehn et al. 2003 -RRB- and syntactic reranking of K-best lists -LRB- Och et al. 2004 -RRB- produced mostly negative results	amod_results_negative advmod_negative_mostly dobj_produced_results nsubj_produced_reranking nsubj_produced_experiments amod_Och_2004 dep_Och_al. nn_Och_et amod_lists_K-best prep_of_reranking_lists nn_reranking_syntactic amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_phrases_syntactically-informed dep_experiments_Och conj_and_experiments_reranking dep_experiments_Koehn prep_with_experiments_phrases advmod_experiments_Early ccomp_``_produced
P08-1012	P03-1021	o	We also trained a baseline model with GIZA + + -LRB- Och and Ney 2003 -RRB- following a regimen of 5 iterations of Model 1 5 iterations of HMM and 5 iterations of Model 4	num_Model_4 prep_of_iterations_Model num_iterations_5 prep_of_iterations_HMM num_iterations_5 conj_and_Model_iterations appos_Model_iterations num_Model_1 prep_of_iterations_iterations prep_of_iterations_Model num_iterations_5 prep_of_regimen_iterations det_regimen_a amod_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ prep_with_model_+ prep_with_model_GIZA nn_model_baseline det_model_a prep_following_trained_regimen dobj_trained_model advmod_trained_also nsubj_trained_We ccomp_``_trained
P08-1012	P03-1021	o	Minimum Error Rate training -LRB- Och 2003 -RRB- over BLEU was used to optimize the weights for each of these models over the development test data	nn_data_test nn_data_development det_data_the det_models_these prep_of_each_models det_weights_the prep_over_optimize_data prep_for_optimize_each dobj_optimize_weights aux_optimize_to xcomp_used_optimize auxpass_used_was nsubjpass_used_training dep_Och_2003 prep_over_training_BLEU appos_training_Och nn_training_Rate nn_training_Error nn_training_Minimum
P08-1023	P03-1021	o	We use the standard minimum error-rate training -LRB- Och 2003 -RRB- to tune the feature weights to maximize the systems BLEU score on the dev set	amod_set_dev det_set_the prep_on_score_set nn_score_BLEU nn_score_systems det_score_the dobj_maximize_score aux_maximize_to nn_weights_feature det_weights_the vmod_tune_maximize dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och amod_training_error-rate nn_training_minimum amod_training_standard det_training_the vmod_use_tune dobj_use_training nsubj_use_We
P08-1024	P03-1021	o	However while discriminative models promise much they have not been shown to deliver significant gains 1We class approaches using minimum error rate training -LRB- Och 2003 -RRB- frequency count based as these systems re-scale a handful of generative features estimated from frequency counts and do not support large sets of non-independent features	amod_features_non-independent prep_of_sets_features amod_sets_large dobj_support_sets neg_support_not aux_support_do nsubj_support_systems nn_counts_frequency prep_from_estimated_counts vmod_features_estimated amod_features_generative prep_of_handful_features det_handful_a conj_and_re-scale_support dobj_re-scale_handful nsubj_re-scale_systems mark_re-scale_as det_systems_these advcl_based_support advcl_based_re-scale nn_count_frequency nn_count_training dep_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_using_based dobj_using_count nn_approaches_class nn_approaches_1We nn_approaches_gains amod_approaches_significant vmod_deliver_using dobj_deliver_approaches aux_deliver_to xcomp_shown_deliver auxpass_shown_been neg_shown_not aux_shown_have nsubjpass_shown_they advcl_shown_promise advmod_shown_However advmod_promise_much nsubj_promise_models mark_promise_while amod_models_discriminative
P08-1049	P03-1021	o	Once we obtain the augmented phrase table we should run the minimum-error-rate training -LRB- Och 2003 -RRB- with the augmented phrase table such that the model parameters are properly adjusted	advmod_adjusted_properly auxpass_adjusted_are nsubjpass_adjusted_parameters mark_adjusted_that dep_adjusted_such nn_parameters_model det_parameters_the vmod_table_adjusted nn_table_phrase amod_table_augmented det_table_the appos_Och_2003 dep_training_Och amod_training_minimum-error-rate det_training_the prep_with_run_table dobj_run_training aux_run_should nsubj_run_we advcl_run_obtain nn_table_phrase amod_table_augmented det_table_the dobj_obtain_table nsubj_obtain_we advmod_obtain_Once
P08-1049	P03-1021	o	The feature functions are combined under a log-linear framework andtheweights aretuned bytheminimum-error-rate training -LRB- Och 2003 -RRB- using BLEU -LRB- Papineni et al. 2002 -RRB- as the optimization metric	nn_metric_optimization det_metric_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_as_using_metric dobj_using_BLEU dep_Och_2003 vmod_training_using appos_training_Och amod_training_bytheminimum-error-rate dobj_aretuned_training nsubj_aretuned_andtheweights ccomp_aretuned_combined amod_framework_log-linear det_framework_a prep_under_combined_framework auxpass_combined_are nsubjpass_combined_functions nn_functions_feature det_functions_The
P08-1049	P03-1021	o	4.5.2 BLEU on NIST MT Test Sets We use MT02 as the development set4 for minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 appos_training_MERT nn_training_rate nn_training_error amod_training_minimum dep_set4_Och prep_for_set4_training nn_set4_development det_set4_the prep_as_use_set4 dobj_use_MT02 nsubj_use_We ccomp_Sets_use nsubj_Sets_BLEU nn_Test_MT nn_Test_NIST prep_on_BLEU_Test num_BLEU_4.5.2 ccomp_``_Sets
P08-1049	P03-1021	o	Moreover our approach integrates the abbreviation translation component into the baseline system in a natural way and thus is able to make use of the minimum-error-rate training -LRB- Och 2003 -RRB- to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system	nn_system_baseline det_system_the amod_system_integrated det_system_the prep_over_change_system prep_of_change_system det_change_the dobj_reflect_change aux_reflect_to nn_parameters_model det_parameters_the vmod_adjust_reflect dobj_adjust_parameters advmod_adjust_automatically aux_adjust_to dep_Och_2003 appos_training_Och amod_training_minimum-error-rate det_training_the prep_of_use_training xcomp_make_adjust dobj_make_use aux_make_to xcomp_able_make cop_able_is advmod_able_thus nsubj_able_approach amod_way_natural det_way_a prep_in_system_way nn_system_baseline det_system_the nn_component_translation nn_component_abbreviation det_component_the conj_and_integrates_able prep_into_integrates_system dobj_integrates_component nsubj_integrates_approach advmod_integrates_Moreover poss_approach_our
P08-1059	P03-1021	o	The features are similar to the ones used in phrasal systems and their weights are trained using max-BLEU training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och amod_training_max-BLEU dobj_using_training xcomp_trained_using auxpass_trained_are nsubjpass_trained_weights poss_weights_their amod_systems_phrasal prep_in_used_systems vmod_ones_used det_ones_the conj_and_similar_trained prep_to_similar_ones cop_similar_are nsubj_similar_features det_features_The
P08-1064	P03-1021	o	For the MER training -LRB- Och 2003 -RRB- we modified Koehns MER trainer -LRB- Koehn 2004 -RRB- for our tree sequence-based system	amod_system_sequence-based nn_system_tree poss_system_our amod_Koehn_2004 prep_for_trainer_system appos_trainer_Koehn nn_trainer_MER nn_trainer_Koehns dobj_modified_trainer nsubj_modified_we prep_for_modified_training appos_Och_2003 dep_training_Och nn_training_MER det_training_the
P08-1064	P03-1021	o	1 Introduction Phrase-based modeling method -LRB- Koehn et al. 2003 Och and Ney 2004a -RRB- is a simple but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well	amod_expressions_multiword prep_of_translations_expressions conj_and_reorderings_translations amod_reorderings_local advmod_model_well dobj_model_translations dobj_model_reorderings aux_model_can nsubj_model_it mark_model_since nn_translation_machine advcl_mechanism_model prep_to_mechanism_translation amod_mechanism_powerful amod_mechanism_simple det_mechanism_a cop_mechanism_is nsubj_mechanism_method conj_but_simple_powerful appos_Och_2004a conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_method_Koehn nn_method_modeling amod_method_Phrase-based nn_method_Introduction num_method_1
P08-1066	P03-1021	o	Given sentence-aligned bi-lingual training data we first use GIZA + + -LRB- Och and Ney 2003 -RRB- to generate word level alignment	nn_alignment_level nn_alignment_word dobj_generate_alignment aux_generate_to num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ vmod_use_generate dobj_use_+ dobj_use_GIZA advmod_use_first nsubj_use_we prep_use_Given nn_data_training amod_data_bi-lingual amod_data_sentence-aligned pobj_Given_data
P08-1066	P03-1021	o	Following -LRB- Och 2003 -RRB- the k-best results are accumulated as the input of the optimizer	det_optimizer_the prep_of_input_optimizer det_input_the prep_as_accumulated_input auxpass_accumulated_are nsubjpass_accumulated_results prep_accumulated_Following amod_results_k-best det_results_the amod_Och_2003 dep_Following_Och rcmod_``_accumulated
P08-1066	P03-1021	o	Hierarchical rules were extracted from a subset which has about 35M/41M words5 and the rest of the training data were used to extract phrasal rules as in -LRB- Och 2003 Chiang 2005 -RRB-	amod_Chiang_2005 dep_Och_Chiang conj_Och_2003 pobj_in_Och pcomp_as_in amod_rules_phrasal prep_extract_as dobj_extract_rules aux_extract_to xcomp_used_extract auxpass_used_were nsubjpass_used_rest nn_data_training det_data_the prep_of_rest_data det_rest_the amod_words5_35M/41M advmod_words5_about dobj_has_words5 nsubj_has_which rcmod_subset_has det_subset_a conj_and_extracted_used prep_from_extracted_subset auxpass_extracted_were nsubjpass_extracted_rules amod_rules_Hierarchical
P08-1086	P03-1021	o	The weights are trained using minimum error rate training -LRB- Och 2003 -RRB- with BLEU score as the objective function	amod_function_objective det_function_the nn_score_BLEU dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum prep_with_using_score dobj_using_training prep_as_trained_function xcomp_trained_using auxpass_trained_are nsubjpass_trained_weights det_weights_The ccomp_``_trained
P08-1087	P03-1021	o	A Greek model was trained on 440,082 aligned sentences of Europarl v. 3 tuned with Minimum Error Training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och nn_Training_Error nn_Training_Minimum prep_with_tuned_Training conj_v._Europarl_3 prep_of_sentences_3 prep_of_sentences_Europarl amod_sentences_aligned num_sentences_440,082 dep_trained_tuned prep_on_trained_sentences auxpass_trained_was nsubjpass_trained_model amod_model_Greek det_model_A
P08-1102	P03-1021	o	To obtain their corresponding weights we adapted the minimum-error-rate training algorithm -LRB- Och 2003 -RRB- to train the outside-layer model	nn_model_outside-layer det_model_the dobj_train_model aux_train_to amod_Och_2003 appos_algorithm_Och nn_algorithm_training amod_algorithm_minimum-error-rate det_algorithm_the vmod_adapted_train dobj_adapted_algorithm nsubj_adapted_we advcl_adapted_obtain amod_weights_corresponding poss_weights_their dobj_obtain_weights aux_obtain_To
P08-1114	P03-1021	o	Each i is a weight associated with feature i and these weights are typically optimized using minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training xcomp_optimized_using advmod_optimized_typically auxpass_optimized_are nsubjpass_optimized_weight det_weights_these conj_and_i_weights nn_i_feature prep_with_associated_weights prep_with_associated_i vmod_weight_associated det_weight_a ccomp_is_optimized dep_i_is det_i_Each dep_``_i
P08-2010	P03-1021	o	This shows that hypothesis features are either not discriminative enough or that the reranking model is too weak This performance gap can be mainly attributed to two problems optimization error and modeling error -LRB- see Figure 1 -RRB- .1 Much work has focused on developing better algorithms to tackle the optimization problem -LRB- e.g. MERT -LRB- Och 2003 -RRB- -RRB- since MT evaluation metrics such as BLEU and PER are riddled with local minima and are difficult to differentiate with respect to re-ranker parameters	nn_parameters_re-ranker prep_with_respect_to_differentiate_parameters aux_differentiate_to xcomp_difficult_differentiate cop_difficult_are nsubj_difficult_metrics amod_minima_local conj_and_riddled_difficult prep_with_riddled_minima cop_riddled_are nsubj_riddled_metrics mark_riddled_since conj_and_BLEU_PER prep_such_as_metrics_PER prep_such_as_metrics_BLEU nn_metrics_evaluation nn_metrics_MT advcl_,_difficult advcl_,_riddled dep_Och_2003 dep_MERT_Och pobj_e.g._MERT dep_-LRB-_e.g. nn_problem_optimization det_problem_the dobj_tackle_problem aux_tackle_to vmod_algorithms_tackle amod_algorithms_better dobj_developing_algorithms prepc_on_focused_developing aux_focused_has nsubj_focused_work amod_work_Much num_work_.1 parataxis_1_focused nsubj_1_Figure dep_see_1 nn_error_modeling dep_error_see conj_and_error_error nn_error_optimization dep_problems_error dep_problems_error num_problems_two prep_to_attributed_problems advmod_attributed_mainly auxpass_attributed_be aux_attributed_can nsubjpass_attributed_gap nn_gap_performance det_gap_This ccomp_weak_attributed advmod_weak_too cop_weak_is nsubj_weak_model mark_weak_that nn_model_reranking det_model_the conj_or_discriminative_weak advmod_discriminative_enough neg_discriminative_not preconj_discriminative_either cop_discriminative_are nsubj_discriminative_features mark_discriminative_that nn_features_hypothesis ccomp_shows_weak ccomp_shows_discriminative nsubj_shows_This
P08-2038	P03-1021	o	For the efficiency of minimum-errorrate training -LRB- Och 2003 -RRB- we built our development set -LRB- 580 sentences -RRB- using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data	nn_data_test nn_data_evaluation nn_data_MT-02 nn_data_NIST det_data_the num_characters_50 prep_from_exceeding_data dobj_exceeding_characters neg_exceeding_not vmod_sentences_exceeding dobj_using_sentences num_sentences_580 appos_set_sentences nn_set_development poss_set_our xcomp_built_using dobj_built_set nsubj_built_we prep_for_built_efficiency appos_Och_2003 dep_training_Och amod_training_minimum-errorrate prep_of_efficiency_training det_efficiency_the
P08-2041	P03-1021	o	We perform minimum-error-rate training -LRB- Och 2003 -RRB- to tune the feature weights of the translation model to maximize the BLEU score on development set	nn_set_development prep_on_score_set nn_score_BLEU det_score_the dobj_maximize_score aux_maximize_to nn_model_translation det_model_the prep_of_weights_model nn_weights_feature det_weights_the vmod_tune_maximize dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och amod_training_minimum-error-rate vmod_perform_tune dobj_perform_training nsubj_perform_We
P09-1018	P03-1021	o	We ran the decoder with its default settings and then used Moses implementation of minimum error rate training -LRB- Och 2003 -RRB- to tune the feature weights on the development set	nn_set_development det_set_the prep_on_weights_set nn_weights_feature det_weights_the dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_of_implementation_training nn_implementation_Moses vmod_used_tune dobj_used_implementation advmod_used_then nsubj_used_We nn_settings_default poss_settings_its det_decoder_the conj_and_ran_used prep_with_ran_settings dobj_ran_decoder nsubj_ran_We
P09-1018	P03-1021	o	sp and pt are feature weights set by performing minimum error rate training as described in Och -LRB- 2003 -RRB-	appos_Och_2003 prep_in_described_Och mark_described_as nn_training_rate nn_training_error amod_training_minimum advcl_performing_described dobj_performing_training agent_set_performing vmod_weights_set nn_weights_feature cop_weights_are nsubj_weights_pt nsubj_weights_sp conj_and_sp_pt
P09-1019	P03-1021	p	Two popular techniques that incorporate the error criterion are Minimum Error Rate Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- and Minimum BayesRisk -LRB- MBR -RRB- decoding -LRB- Kumar and Byrne 2004 -RRB-	num_Kumar_2004 conj_and_Kumar_Byrne appos_decoding_Byrne appos_decoding_Kumar nn_decoding_BayesRisk appos_BayesRisk_MBR nn_BayesRisk_Minimum dep_Och_2003 conj_and_Training_decoding dep_Training_Och appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum nsubj_are_decoding nsubj_are_Training nn_criterion_error det_criterion_the dobj_incorporate_criterion nsubj_incorporate_that dep_techniques_are rcmod_techniques_incorporate amod_techniques_popular num_techniques_Two dep_``_techniques
P09-1019	P03-1021	o	A path in a translation hypergraph induces a translation hypothesis E along with its sequence of SCFG rules D = r1 r2 rK which if applied to the start symbol derives E The sequence of SCFG rules induced by a path is also called a derivation tree for E. 3 Minimum Error Rate Training Given a set of source sentences FS1 with corresponding reference translations RS1 the objective of MERT is to find a parameter set M1 which minimizes an automated evaluation criterion under a linear model M1 = argmin M1 SX s = 1 Err ` Rs E -LRB- Fs M1 -RRB- ff E -LRB- Fs M1 -RRB- = argmax E SX s = 1 mhm -LRB- E Fs -RRB- ff In the context of statistical machine translation the optimization procedure was first described in Och -LRB- 2003 -RRB- for N-best lists and later extended to phrase-lattices in Macherey et al.	nn_al._et nn_al._Macherey prep_in_phrase-lattices_al. prep_to_extended_phrase-lattices advmod_extended_later nn_lists_N-best prep_for_Och_lists appos_Och_2003 prep_in_described_Och conj_and_first_extended dep_first_described nsubj_was_extended nsubj_was_first dep_procedure_was nn_procedure_optimization det_procedure_the nn_translation_machine amod_translation_statistical prep_of_context_translation det_context_the dep_ff_E dep_ff_mhm dep_ff_= dep_ff_s amod_ff_= appos_E_Fs num_mhm_1 nsubj_s_SX nn_SX_E nn_SX_argmax npadvmod_=_E dep_Fs_M1 appos_E_Fs nn_E_ff nn_E_M1 nn_Fs_E nn_Fs_Rs dep_Fs_Err num_Err_1 dep_=_Fs acomp_s_= nsubj_s_SX nn_SX_M1 nn_SX_argmin dep_=_s appos_M1_procedure prep_in_M1_context dep_M1_ff amod_M1_= dep_model_M1 amod_model_linear det_model_a nn_criterion_evaluation amod_criterion_automated det_criterion_an prep_under_minimizes_model dobj_minimizes_criterion nsubj_minimizes_which rcmod_M1_minimizes amod_M1_set dep_parameter_M1 det_parameter_a dobj_find_parameter aux_find_to xcomp_is_find nsubj_is_RS1 prep_of_objective_MERT det_objective_the appos_RS1_objective rcmod_translations_is nn_translations_reference amod_translations_corresponding prep_with_FS1_translations dep_sentences_FS1 dep_source_sentences prep_of_set_source det_set_a pobj_Given_set prep_Training_Given nn_Training_Rate nn_Training_Error nn_Training_Minimum num_Training_3 nn_Training_E. prep_for_tree_Training nn_tree_derivation det_tree_a xcomp_called_tree advmod_called_also auxpass_called_is nsubjpass_called_sequence dep_called_E det_path_a agent_induced_path vmod_rules_induced nn_rules_SCFG prep_of_sequence_rules det_sequence_The ccomp_derives_called advcl_derives_applied dep_derives_which nn_symbol_start det_symbol_the prep_to_applied_symbol mark_applied_if rcmod_rK_derives appos_r1_rK conj_r1_r2 dobj_=_r1 amod_D_= dep_rules_D dep_SCFG_rules prep_of_sequence_SCFG poss_sequence_its pobj_E_sequence prepc_along_with_E_with dep_hypothesis_E nn_hypothesis_translation det_hypothesis_a dobj_induces_hypothesis nsubj_induces_path nn_hypergraph_translation det_hypergraph_a prep_in_path_hypergraph det_path_A
P09-1020	P03-1021	o	GIZA + + -LRB- Och and Ney 2003 -RRB- and the heuristics grow-diag-final-and are used to generate m-ton word alignments	nn_alignments_word nn_alignments_m-ton dobj_generate_alignments aux_generate_to xcomp_used_generate auxpass_used_are nsubjpass_used_grow-diag-final-and nsubjpass_used_+ nsubjpass_used_GIZA nn_grow-diag-final-and_heuristics det_grow-diag-final-and_the num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_grow-diag-final-and conj_+_GIZA_+
P09-1020	P03-1021	o	For the MER training -LRB- Och 2003 -RRB- Koehns MER trainer -LRB- Koehn 2007 -RRB- is modified for our system	poss_system_our prep_for_modified_system auxpass_modified_is nsubjpass_modified_trainer prep_for_modified_training amod_Koehn_2007 appos_trainer_Koehn nn_trainer_MER nn_trainer_Koehns dep_Och_2003 appos_training_Och nn_training_MER det_training_the rcmod_``_modified
P09-1021	P03-1021	o	The number of weights wi is 3 plus the number of source languages and they are trained using minimum error-rate training -LRB- MERT -RRB- to maximize the BLEU score -LRB- Och 2003 -RRB- on a development set	nn_set_development det_set_a dep_Och_2003 appos_score_Och nn_score_BLEU det_score_the prep_on_maximize_set dobj_maximize_score aux_maximize_to appos_training_MERT amod_training_error-rate amod_training_minimum vmod_using_maximize dobj_using_training xcomp_trained_using auxpass_trained_are nsubjpass_trained_they nn_languages_source prep_of_number_languages det_number_the conj_and_3_trained conj_plus_3_number cop_3_is nsubj_3_number nn_wi_weights prep_of_number_wi det_number_The
P09-1021	P03-1021	o	-LRB- Ueffing et al. 2007 Haffari et al. 2009 -RRB- show that treating U + as a source for a new feature function in a loglinear model for SMT -LRB- Och and Ney 2004 -RRB- allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och appos_training_MERT amod_training_error-rate amod_training_minimum dobj_using_training det_feature_this prep_for_weight_feature det_weight_a vmod_finding_using dobj_finding_weight amod_data_unlabeled prep_of_advantage_data prepc_by_take_finding dobj_take_advantage advmod_take_maximally aux_take_to dobj_allows_us dep_Och_2004 conj_and_Och_Ney appos_SMT_Ney appos_SMT_Och prep_for_model_SMT amod_model_loglinear det_model_a prep_in_function_model nn_function_feature amod_function_new det_function_a prep_for_source_function det_source_a prep_as_+_source conj_+_U_allows xcomp_treating_take dobj_treating_allows dobj_treating_U dep_that_treating dep_show_that nsubj_show_Ueffing num_Haffari_2009 nn_Haffari_al. nn_Haffari_et dep_Ueffing_Haffari appos_Ueffing_2007 dep_Ueffing_al. nn_Ueffing_et
P09-1034	P03-1021	o	We are currently investigating caching and optimizations that will enable the use of our metric for MT parameter tuning in a Minimum Error Rate Training setup -LRB- Och 2003 -RRB-	amod_Och_2003 dep_setup_Och amod_setup_Training nn_setup_Rate nn_setup_Error nn_setup_Minimum det_setup_a nn_tuning_parameter nn_tuning_MT poss_metric_our prep_for_use_tuning prep_of_use_metric det_use_the prep_in_enable_setup dobj_enable_use aux_enable_will nsubj_enable_that rcmod_caching_enable conj_and_caching_optimizations dobj_investigating_optimizations dobj_investigating_caching advmod_investigating_currently aux_investigating_are nsubj_investigating_We ccomp_``_investigating
P09-1036	P03-1021	o	These constituent matching/violation counts are used as a feature in the decoders log-linear model and their weights are tuned via minimal error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimal prep_via_tuned_training auxpass_tuned_are poss_weights_their amod_model_log-linear nn_model_decoders det_model_the conj_and_feature_weights prep_in_feature_model det_feature_a dep_used_tuned prep_as_used_weights prep_as_used_feature auxpass_used_are nsubjpass_used_counts nn_counts_matching/violation amod_counts_constituent det_counts_These
P09-1064	P03-1021	o	The model was trained using minimum error rate training for Arabic -LRB- Och 2003 -RRB- and MIRA for Chinese -LRB- Chiang et al. 2008 -RRB-	num_Chiang_2008 dep_Chiang_al. nn_Chiang_et prep_for_MIRA_Chinese amod_Och_2003 dep_Arabic_Och conj_and_training_MIRA prep_for_training_Arabic nn_training_rate nn_training_error amod_training_minimum dobj_using_MIRA dobj_using_training dep_trained_Chiang xcomp_trained_using auxpass_trained_was nsubjpass_trained_model det_model_The
P09-1065	P03-1021	p	4 Extended Minimum Error Rate Training Minimum error rate training -LRB- Och 2003 -RRB- is widely used to optimize feature weights for a linear model -LRB- Och and Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_model_Ney dep_model_Och amod_model_linear det_model_a nn_weights_feature prep_for_optimize_model dobj_optimize_weights aux_optimize_to xcomp_used_optimize advmod_used_widely auxpass_used_is nsubjpass_used_training dep_Och_2003 appos_training_Och nn_training_rate nn_training_error nn_training_Minimum nn_training_Training nn_training_Rate nn_training_Error nn_training_Minimum nn_training_Extended num_training_4 ccomp_``_used
P09-1065	P03-1021	o	Instead of computing all intersections Och -LRB- 2003 -RRB- only computes critical intersections where highest-score translations will change	aux_change_will nsubj_change_translations advmod_change_where amod_translations_highest-score rcmod_intersections_change amod_intersections_critical dobj_computes_intersections advmod_computes_only nsubj_computes_Och prepc_instead_of_computes_computing appos_Och_2003 det_intersections_all dobj_computing_intersections
P09-1065	P03-1021	o	We obtained word alignments of training data by first running GIZA + + -LRB- Och and Ney 2003 -RRB- and then applying the refinement rule grow-diag-final-and -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diag-final-and_Koehn nn_grow-diag-final-and_rule nn_grow-diag-final-and_refinement det_grow-diag-final-and_the dobj_applying_grow-diag-final-and nsubj_applying_then nsubj_applying_+ nsubj_applying_GIZA num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_then conj_+_GIZA_+ ccomp_running_applying xcomp_first_running nn_data_training prep_of_alignments_data nn_alignments_word prepc_by_obtained_first dobj_obtained_alignments nsubj_obtained_We
P09-1065	P03-1021	o	As multiple derivations are used for finding optimal translations we extend the minimum error rate training -LRB- MERT -RRB- algorithm -LRB- Och 2003 -RRB- to tune feature weights with respect to BLEU score for max-translation decoding -LRB- Section 4 -RRB-	num_Section_4 appos_decoding_Section nn_decoding_max-translation nn_score_BLEU nn_weights_feature prep_for_tune_decoding prep_with_respect_to_tune_score dobj_tune_weights aux_tune_to amod_Och_2003 dep_algorithm_Och nn_algorithm_training appos_training_MERT nn_training_rate nn_training_error amod_training_minimum det_training_the vmod_extend_tune dobj_extend_algorithm nsubj_extend_we advcl_extend_used amod_translations_optimal dobj_finding_translations prepc_for_used_finding auxpass_used_are nsubjpass_used_derivations mark_used_As amod_derivations_multiple
P09-1065	P03-1021	o	While they train the parameters using a maximum a posteriori estimator we extend the MERT algorithm -LRB- Och 2003 -RRB- to take the evaluation metric into account	nn_metric_evaluation det_metric_the prep_into_take_account dobj_take_metric aux_take_to amod_Och_2003 dep_algorithm_Och nn_algorithm_MERT det_algorithm_the xcomp_extend_take dobj_extend_algorithm nsubj_extend_we advcl_extend_train amod_estimator_posteriori det_estimator_a dep_maximum_estimator det_maximum_a dobj_using_maximum det_parameters_the vmod_train_using dobj_train_parameters nsubj_train_they mark_train_While
P09-1065	P03-1021	o	On the other hand other authors -LRB- e.g. -LRB- Och and Ney 2004 Koehn et al. 2003 Chiang 2007 -RRB- -RRB- do use the expression phrase-based models	amod_models_phrase-based nn_models_expression det_models_the dobj_use_models aux_use_do nsubj_use_Ney nsubj_use_Och advmod_use_e.g. dep_Chiang_2007 dep_Koehn_Chiang num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn dep_Och_2004 conj_and_Och_Ney dep_authors_use amod_authors_other dep_,_authors amod_hand_other det_hand_the pobj_On_hand dep_``_On
P09-1066	P03-1021	o	2.5 Model Training We adapt the Minimum Error Rate Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- algorithm to estimate parameters for each member model in co-decoding	prep_in_model_co-decoding nn_model_member det_model_each prep_for_parameters_model dobj_estimate_parameters aux_estimate_to vmod_algorithm_estimate dep_Och_2003 dep_Training_algorithm dep_Training_Och appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum det_Training_the dobj_adapt_Training nsubj_adapt_We ccomp_Training_adapt vmod_Model_Training num_Model_2.5 dep_``_Model
P09-1067	P03-1021	o	In the geometric interpolation above the weight n controls the relative veto power of the n-gram approximation and can be tuned using MERT -LRB- Och 2003 -RRB- or a minimum risk procedure -LRB- Smith and Eisner 2006 -RRB-	amod_Smith_2006 conj_and_Smith_Eisner dep_procedure_Eisner dep_procedure_Smith nn_procedure_risk amod_procedure_minimum det_procedure_a dep_Och_2003 conj_or_MERT_procedure appos_MERT_Och dobj_using_procedure dobj_using_MERT xcomp_tuned_using auxpass_tuned_be aux_tuned_can nsubjpass_tuned_n nn_approximation_n-gram det_approximation_the prep_of_power_approximation nn_power_veto amod_power_relative det_power_the conj_and_controls_tuned dobj_controls_power nsubj_controls_n prep_in_controls_interpolation nn_n_weight det_n_the advmod_interpolation_above amod_interpolation_geometric det_interpolation_the
P09-1067	P03-1021	p	The NIST MT03 set is used to tune model weights -LRB- e.g. those of -LRB- 16 -RRB- -RRB- and the scaling factor 17We have also experimented with MERT -LRB- Och 2003 -RRB- and found that the deterministic annealing gave results that were more consistent across runs and often better	advmod_better_often conj_and_consistent_better prep_across_consistent_runs dep_consistent_more cop_consistent_were nsubj_consistent_that rcmod_results_better rcmod_results_consistent dobj_gave_results nsubj_gave_annealing mark_gave_that amod_annealing_deterministic det_annealing_the ccomp_found_gave nsubjpass_found_set dep_Och_2003 appos_MERT_Och prep_with_experimented_MERT advmod_experimented_also aux_experimented_have nsubj_experimented_17We nsubj_experimented_those advmod_experimented_e.g. nn_17We_factor nn_17We_scaling det_17We_the dep_of_16 conj_and_those_17We prep_those_of vmod_weights_experimented nn_weights_model dobj_tune_weights aux_tune_to conj_and_used_found xcomp_used_tune auxpass_used_is nsubjpass_used_set nn_set_MT03 nn_set_NIST det_set_The
P09-1087	P03-1021	o	Parametertuningwasdonewithminimum error rate training -LRB- Och 2003 -RRB- which was used to maximize BLEU -LRB- Papineni et al. 2001 -RRB-	amod_Papineni_2001 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dobj_maximize_BLEU aux_maximize_to xcomp_used_maximize auxpass_used_was nsubjpass_used_which dep_Och_2003 rcmod_training_used appos_training_Och nn_training_rate nn_training_error nn_training_Parametertuningwasdonewithminimum
P09-1087	P03-1021	n	1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years -LRB- Chiang 2005 Marcu et al. 2006 Shen et al. 2008 -RRB- and often outperform phrase-based systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- on target-language fluency and adequacy	conj_and_fluency_adequacy nn_fluency_target-language num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney prep_on_systems_adequacy prep_on_systems_fluency appos_systems_2004 appos_systems_Ney appos_systems_Och amod_systems_phrase-based amod_systems_outperform advmod_outperform_often num_Shen_2008 nn_Shen_al. nn_Shen_et num_Marcu_2006 nn_Marcu_al. nn_Marcu_et dep_Chiang_Shen dep_Chiang_Marcu appos_Chiang_2005 appos_years_Chiang amod_years_recent advmod_successful_increasingly conj_and_proven_systems prep_in_proven_years acomp_proven_successful aux_proven_have nsubj_proven_approaches nn_translation_machine prep_to_approaches_translation amod_approaches_Hierarchical nn_approaches_Introduction num_approaches_1 ccomp_``_systems ccomp_``_proven
P09-1088	P03-1021	o	We use the GIZA + + implementation of IBM Model 4 -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- coupled with the phrase extraction heuristics of Koehn et al.	nn_al._et nn_al._Koehn prep_of_heuristics_al. nn_heuristics_extraction nn_heuristics_phrase det_heuristics_the prep_with_coupled_heuristics dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM prep_of_implementation_Model pobj_+_implementation dep_GIZA_Brown conj_+_GIZA_+ det_GIZA_the dep_use_coupled dobj_use_+ dobj_use_GIZA nsubj_use_We
P09-1088	P03-1021	o	The parameters of the NIST systems were tuned using Ochs algorithm to maximize BLEU on the MT02 test set -LRB- Och 2003 -RRB-	amod_Och_2003 dep_set_Och nn_set_test nn_set_MT02 det_set_the prep_on_BLEU_set dobj_maximize_BLEU aux_maximize_to nn_algorithm_Ochs vmod_using_maximize dobj_using_algorithm xcomp_tuned_using auxpass_tuned_were nsubjpass_tuned_parameters nn_systems_NIST det_systems_the prep_of_parameters_systems det_parameters_The ccomp_``_tuned
P09-1090	P03-1021	o	Tuning -LRB- learning the values discussed in section 4.1 -RRB- was done using minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training xcomp_done_using auxpass_done_was nsubjpass_done_Tuning num_section_4.1 prep_in_discussed_section vmod_values_discussed det_values_the dobj_learning_values dep_Tuning_learning
P09-1094	P03-1021	p	3.6 Parameter Estimation To estimate parameters k -LRB- 1 k K -RRB- lm and um we adopt the approach of minimum error rate training -LRB- MERT -RRB- that is popular in SMT -LRB- Och 2003 -RRB-	amod_Och_2003 dep_SMT_Och prep_in_popular_SMT cop_popular_is nsubj_popular_that appos_training_MERT nn_training_rate nn_training_error amod_training_minimum rcmod_approach_popular prep_of_approach_training det_approach_the dobj_adopt_approach nsubj_adopt_we nsubj_adopt_Estimation nn_K_k num_K_1 conj_and_k_um conj_and_k_lm appos_k_K nn_k_parameters dobj_estimate_um dobj_estimate_lm dobj_estimate_k aux_estimate_To vmod_Estimation_estimate nn_Estimation_Parameter num_Estimation_3.6
P09-1104	P03-1021	o	The pipeline extracts a Hiero-style synchronous context-free grammar -LRB- Chiang 2007 -RRB- employs suffix-array based rule extraction -LRB- Lopez 2007 -RRB- and tunes model parameters with minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_with_parameters_training nn_parameters_model nn_parameters_tunes amod_Lopez_2007 dep_extraction_Lopez nn_extraction_rule amod_extraction_based amod_extraction_suffix-array dobj_employs_extraction nsubj_employs_pipeline dep_Chiang_2007 appos_grammar_Chiang amod_grammar_context-free amod_grammar_synchronous amod_grammar_Hiero-style det_grammar_a conj_and_extracts_parameters conj_and_extracts_employs dobj_extracts_grammar nsubj_extracts_pipeline det_pipeline_The ccomp_``_parameters ccomp_``_employs ccomp_``_extracts
P09-1106	P03-1021	o	The weights of feature functions are optimized to maximize the scoring measure -LRB- Och 2003 -RRB-	amod_Och_2003 dep_measure_Och amod_measure_scoring det_measure_the dobj_maximize_measure aux_maximize_to xcomp_optimized_maximize auxpass_optimized_are nsubjpass_optimized_weights nn_functions_feature prep_of_weights_functions det_weights_The
P09-1106	P03-1021	o	-LRB- 2006 2008 -RRB- proposed using GIZA + + -LRB- Och and Ney 2003 -RRB- to align words between the backbone and hypothesis	conj_and_backbone_hypothesis det_backbone_the prep_between_words_hypothesis prep_between_words_backbone dobj_align_words aux_align_to num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och vmod_GIZA_align conj_+_GIZA_+ dobj_using_+ dobj_using_GIZA xcomp_proposed_using vmod_2008_proposed num_2008_2006 dep_''_2008
P09-1108	P03-1021	o	Uses for k-best lists include minimum Bayes risk decoding -LRB- Goodman 1998 Kumar and Byrne 2004 -RRB- discriminative reranking -LRB- Collins 2000 Charniak and Johnson 2005 -RRB- and discriminative training -LRB- Och 2003 McClosky et al. 2006 -RRB-	num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Och_McClosky appos_Och_2003 appos_training_Och amod_training_discriminative dep_Charniak_2005 conj_and_Charniak_Johnson dep_Collins_Johnson dep_Collins_Charniak amod_Collins_2000 appos_reranking_Collins amod_reranking_discriminative dep_Kumar_2004 conj_and_Kumar_Byrne dep_Goodman_Byrne dep_Goodman_Kumar amod_Goodman_1998 conj_and_decoding_training conj_and_decoding_reranking appos_decoding_Goodman nn_decoding_risk nn_decoding_Bayes amod_decoding_minimum dobj_include_training dobj_include_reranking dobj_include_decoding nsubj_include_Uses nn_lists_k-best prep_for_Uses_lists
P09-2035	P03-1021	o	We also use minimum error-rate training -LRB- Och 2003 -RRB- to tune our feature weights	nn_weights_feature poss_weights_our dobj_tune_weights aux_tune_to appos_Och_2003 dep_training_Och amod_training_error-rate amod_training_minimum vmod_use_tune dobj_use_training advmod_use_also nsubj_use_We ccomp_``_use
P09-2058	P03-1021	o	-LRB- 2003 -RRB- grow the set of word links by appending neighboring points while Och and Hey -LRB- 2003 -RRB- try to avoid both horizontal and vertical neighbors	amod_neighbors_vertical amod_neighbors_horizontal conj_and_horizontal_vertical preconj_horizontal_both dobj_avoid_neighbors aux_avoid_to xcomp_try_avoid nsubj_try_Hey nsubj_try_Och mark_try_while dep_Och_2003 conj_and_Och_Hey amod_points_neighboring dobj_appending_points nn_links_word prepc_by_set_appending prep_of_set_links det_set_the advcl_grow_try dobj_grow_set dep_grow_2003
P09-2058	P03-1021	o	We train IBM Model-4 using GIZA + + toolkit -LRB- Och and Ney 2003 -RRB- in two translation directions and perform different word alignment combination	nn_combination_alignment nn_combination_word amod_combination_different dobj_perform_combination nn_directions_translation num_directions_two dep_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_toolkit conj_and_using_perform prep_in_using_directions dobj_using_toolkit dobj_using_GIZA nn_Model-4_IBM dep_train_perform dep_train_using dobj_train_Model-4 nsubj_train_We
P09-2058	P03-1021	o	The next two methods are heuristic -LRB- H -RRB- in -LRB- Och and Ney 2003 -RRB- and grow-diagonal -LRB- GD -RRB- proposed in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_proposed_in vmod_grow-diagonal_proposed appos_grow-diagonal_GD conj_and_Och_grow-diagonal conj_and_Och_2003 conj_and_Och_Ney prep_in_heuristic_grow-diagonal prep_in_heuristic_2003 prep_in_heuristic_Ney prep_in_heuristic_Och appos_heuristic_H cop_heuristic_are nsubj_heuristic_methods num_methods_two amod_methods_next det_methods_The
P09-2058	P03-1021	o	We tune all feature weights automatically -LRB- Och 2003 -RRB- to maximize the BLEU -LRB- Papineni et al. 2002 -RRB- score on the dev set	amod_set_dev det_set_the prep_on_score_set dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_score dep_BLEU_Papineni det_BLEU_the dobj_maximize_BLEU aux_maximize_to amod_Och_2003 nn_weights_feature det_weights_all xcomp_tune_maximize dep_tune_Och advmod_tune_automatically dobj_tune_weights nsubj_tune_We
W04-1513	P03-1021	p	By having the advantage of leveraging large parallel corpora the statistical MT approach outperforms the traditional transfer based approaches in tasks for which adequate parallel corpora is available -LRB- Och 2003 -RRB-	amod_Och_2003 cop_available_is nsubj_available_corpora prep_for_available_which nn_corpora_parallel amod_corpora_adequate dep_tasks_Och rcmod_tasks_available prep_in_approaches_tasks amod_approaches_based dep_transfer_approaches amod_transfer_traditional det_transfer_the dobj_outperforms_transfer nsubj_outperforms_approach prepc_by_outperforms_having nn_approach_MT amod_approach_statistical det_approach_the amod_corpora_parallel amod_corpora_large dobj_leveraging_corpora prepc_of_advantage_leveraging det_advantage_the dobj_having_advantage
W05-0814	P03-1021	o	We applied the union intersection and refined symmetrization metrics -LRB- Och and Ney 2003 -RRB- to the final alignments output from training as well as evaluating the two final alignments directly	amod_alignments_final num_alignments_two det_alignments_the advmod_evaluating_directly dobj_evaluating_alignments nsubj_evaluating_We prep_from_output_training nn_output_alignments amod_output_final det_output_the dep_Och_2003 conj_and_Och_Ney nn_metrics_symmetrization amod_metrics_refined conj_and_union_metrics conj_and_union_intersection det_union_the conj_and_applied_evaluating prep_to_applied_output dep_applied_Ney dep_applied_Och dobj_applied_metrics dobj_applied_intersection dobj_applied_union nsubj_applied_We ccomp_``_evaluating ccomp_``_applied
W05-0814	P03-1021	p	We wish to minimize this error function so we select accordingly argmin summationdisplay a E -LRB- a -RRB- -LRB- a -LRB- argmax a p -LRB- a f | e -RRB- -RRB- -RRB- -LRB- 4 -RRB- Maximizing performance for all of the weights at once is not computationally tractable but -LRB- Och 2003 -RRB- has described an efficient one-dimensional search for a similar problem	amod_problem_similar det_problem_a prep_for_search_problem amod_search_one-dimensional amod_search_efficient det_search_an dobj_described_search aux_described_has nsubj_described_Och amod_Och_2003 conj_but_tractable_described advmod_tractable_computationally neg_tractable_not cop_tractable_is nsubj_tractable_summationdisplay pobj_at_once det_weights_the prep_of_all_weights prep_for_performance_all amod_performance_Maximizing dep_performance_4 nn_performance_p dep_4_f dep_4_a dep_|_e dep_f_| det_p_a dep_argmax_performance prep_E_at appos_E_argmax dep_E_a appos_E_a det_E_a dep_summationdisplay_E amod_summationdisplay_argmin parataxis_select_described parataxis_select_tractable advmod_select_accordingly nsubj_select_we nn_function_error det_function_this dobj_minimize_function aux_minimize_to parataxis_wish_select dep_wish_so xcomp_wish_minimize nsubj_wish_We
W05-0814	P03-1021	o	The discriminative training regimen is otherwise similar to -LRB- Och 2003 -RRB-	amod_Och_2003 dep_to_Och prep_similar_to advmod_similar_otherwise cop_similar_is nsubj_similar_regimen nn_regimen_training amod_regimen_discriminative det_regimen_The
W05-0814	P03-1021	o	The system used for baseline experiments is two runs of IBM Model 4 -LRB- Brown et al. 1993 -RRB- in the GIZA + + -LRB- Och and Ney 2003 -RRB- implementation which includes smoothing extensions to Model 4	num_Model_4 prep_to_extensions_Model dobj_smoothing_extensions xcomp_includes_smoothing nsubj_includes_which appos_implementation_Ney appos_implementation_Och num_Och_2003 conj_and_Och_Ney pobj_+_implementation rcmod_GIZA_includes conj_+_GIZA_+ det_GIZA_the amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM prep_in_runs_+ prep_in_runs_GIZA dep_runs_Brown prep_of_runs_Model num_runs_two cop_runs_is nsubj_runs_system nn_experiments_baseline prep_for_used_experiments vmod_system_used det_system_The
W05-0814	P03-1021	p	For symmetrization we found that Och and Neys refined technique described in -LRB- Och and Ney 2003 -RRB- produced the best AER for this data set under all experimental conditions	amod_conditions_experimental det_conditions_all nn_set_data det_set_this prep_for_AER_set amod_AER_best det_AER_the prep_under_produced_conditions dobj_produced_AER dep_produced_Ney dep_produced_Och mark_produced_in num_Och_2003 conj_and_Och_Ney advcl_described_produced vmod_technique_described amod_technique_refined nn_technique_Neys nn_technique_Och conj_and_Och_Neys prep_that_found_technique nsubj_found_we prep_for_found_symmetrization
W05-0820	P03-1021	p	The field of statistical machine translation has been blessed with a long tradition of freely available software tools such as GIZA + + -LRB- Och and Ney 2003 -RRB- and parallel corpora such as the Canadian Hansards2	amod_Hansards2_Canadian det_Hansards2_the num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_such_as_GIZA_Hansards2 dep_GIZA_corpora conj_and_GIZA_parallel conj_+_GIZA_+ prep_such_as_tools_parallel prep_such_as_tools_+ prep_such_as_tools_GIZA nn_tools_software amod_tools_available advmod_available_freely prep_of_tradition_tools amod_tradition_long det_tradition_a prep_with_blessed_tradition auxpass_blessed_been aux_blessed_has nsubjpass_blessed_field nn_translation_machine amod_translation_statistical prep_of_field_translation det_field_The
W05-0820	P03-1021	o	In addition we also made a word alignment available which was derived using a variant of the current default method for word alignment Och and Ney -LRB- 2003 -RRB- s refined method	amod_method_refined nn_method_s nn_s_Ney appos_Ney_2003 conj_and_Och_method nn_Och_alignment nn_Och_word prep_for_method_method prep_for_method_Och nn_method_default amod_method_current det_method_the prep_of_variant_method det_variant_a dobj_using_variant xcomp_derived_using auxpass_derived_was nsubjpass_derived_which rcmod_alignment_derived amod_alignment_available nn_alignment_word det_alignment_a dobj_made_alignment advmod_made_also nsubj_made_we prep_in_made_addition
W05-0820	P03-1021	o	-LRB- 2004 -RRB- -RRB- better language-specific preprocessing -LRB- Koehn and Knight 2003 -RRB- and restructuring -LRB- Collins et al. 2005 -RRB- additional feature functions such as word class language models and minimum error rate training -LRB- Och 2003 -RRB- to optimize parameters	dobj_optimize_parameters aux_optimize_to dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum nn_models_language nn_models_class nn_models_word prep_such_as_functions_models nn_functions_feature amod_functions_additional amod_Collins_2005 dep_Collins_al. nn_Collins_et amod_Koehn_2003 conj_and_Koehn_Knight vmod_preprocessing_optimize conj_and_preprocessing_training conj_and_preprocessing_functions dep_preprocessing_Collins conj_and_preprocessing_restructuring dep_preprocessing_Knight dep_preprocessing_Koehn amod_preprocessing_language-specific amod_preprocessing_better dep_preprocessing_2004
W05-0822	P03-1021	o	s To set weights on the components of the loglinear model we implemented Ochs algorithm -LRB- Och 2003 -RRB-	amod_Och_2003 dep_algorithm_Och nn_algorithm_Ochs dobj_implemented_algorithm nsubj_implemented_we dep_implemented_s amod_model_loglinear det_model_the prep_of_components_model det_components_the prep_on_set_components dobj_set_weights aux_set_To vmod_s_set
W05-0833	P03-1021	o	-LRB- Koehn et al. 2003 -RRB- -LRB- Och 2003 -RRB- -RRB-	dep_Och_2003 dep_Koehn_Och amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_''_Koehn
W05-0833	P03-1021	o	In order to create the necessary SMT language and translation models they used Giza + + -LRB- Och & Ney 2003 -RRB- 2 the CMU-Cambridge statistical toolkit 3 the ISI ReWrite Decoder .4 Translation was performed from EnglishFrench and FrenchEnglish and the resulting translations were evaluated using a range of automatic metrics BLEU -LRB- Papineni et al. 2002 -RRB- Precision and Recall 2http / / www.isi.edu/och/Giza++.html 3http / / mi.eng.cam.ac.uk / prc14/toolkit html 4http / / www.isi.edu/licensed-sw/rewrite-decoder/ 185 -LRB- Turian et al. 2003 -RRB- and Wordand Sentence Error Rates	nn_Rates_Error nn_Rates_Sentence nn_Rates_Wordand dep_al._2003 nn_al._et amod_al._Turian appos_www.isi.edu/licensed-sw/rewrite-decoder/_al. num_www.isi.edu/licensed-sw/rewrite-decoder/_185 nn_4http_html dep_prc14/toolkit_4http amod_prc14/toolkit_mi.eng.cam.ac.uk nn_3http_www.isi.edu/och/Giza++.html dobj_Recall_2http amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_Rates conj_and_BLEU_www.isi.edu/licensed-sw/rewrite-decoder/ dep_BLEU_prc14/toolkit dep_BLEU_3http conj_and_BLEU_Recall conj_and_BLEU_Precision dep_BLEU_Papineni amod_metrics_automatic prep_of_range_metrics det_range_a dobj_using_range xcomp_evaluated_using auxpass_evaluated_were nsubjpass_evaluated_translations amod_translations_resulting det_translations_the conj_and_EnglishFrench_FrenchEnglish dep_performed_Rates dep_performed_www.isi.edu/licensed-sw/rewrite-decoder/ dep_performed_Recall dep_performed_Precision dep_performed_BLEU conj_and_performed_evaluated prep_from_performed_FrenchEnglish prep_from_performed_EnglishFrench auxpass_performed_was nsubjpass_performed_Translation dep_performed_3 num_Translation_.4 nn_Translation_Decoder nn_Translation_ReWrite nn_Translation_ISI det_Translation_the amod_toolkit_statistical amod_toolkit_CMU-Cambridge det_toolkit_the pobj_2_toolkit dep_Och_2003 conj_and_Och_Ney appos_Giza_Ney appos_Giza_Och conj_+_Giza_+ parataxis_used_evaluated parataxis_used_performed dep_used_2 dobj_used_+ dobj_used_Giza nsubj_used_they advcl_used_create nn_models_translation conj_and_language_models nn_language_SMT amod_language_necessary det_language_the dobj_create_models dobj_create_language aux_create_to dep_create_order mark_create_In
W05-0833	P03-1021	o	Accordingly in this section we describe a set of experiments which extends the work of -LRB- Way and Gough 2005 -RRB- by evaluating the Marker-based EBMT system of -LRB- Gough & Way 2004b -RRB- against a phrase-based SMT system built using the following components Giza + + to extract the word-level correspondences The Giza + + word alignments are then refined and used to extract phrasal alignments -LRB- -LRB- Och & Ney 2003 -RRB- or -LRB- Koehn et al. 2003 -RRB- for a more recent implementation -RRB- Probabilities of the extracted phrases are calculated from relative frequencies The resulting phrase translation table is passed to the Pharaoh phrase-based SMT decoder which along with SRI language modelling toolkit5 performs translation	dobj_performs_translation pobj_performs_toolkit5 prepc_along_with_performs_with nsubj_performs_which amod_toolkit5_modelling nn_toolkit5_language nn_language_SRI rcmod_decoder_performs nn_decoder_SMT amod_decoder_phrase-based nn_decoder_Pharaoh det_decoder_the prep_to_passed_decoder auxpass_passed_is nsubjpass_passed_table nn_table_translation nn_table_phrase amod_table_resulting det_table_The amod_frequencies_relative parataxis_calculated_passed prep_from_calculated_frequencies auxpass_calculated_are nsubjpass_calculated_Probabilities amod_phrases_extracted det_phrases_the prep_of_Probabilities_phrases amod_implementation_recent det_implementation_a advmod_recent_more prep_for_Koehn_implementation amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Och_calculated conj_or_Och_Koehn dep_Och_2003 conj_and_Och_Ney dep_-LRB-_Koehn dep_-LRB-_Ney dep_-LRB-_Och amod_alignments_phrasal dobj_extract_alignments aux_extract_to xcomp_used_extract nsubjpass_used_Giza conj_and_refined_used advmod_refined_then auxpass_refined_are nsubjpass_refined_+ nsubjpass_refined_Giza nn_alignments_word pobj_+_alignments conj_+_Giza_+ det_Giza_The amod_correspondences_word-level det_correspondences_the dobj_extract_correspondences aux_extract_to conj_+_Giza_used conj_+_Giza_refined conj_+_Giza_extract conj_+_Giza_+ dep_components_refined dep_components_extract dep_components_+ dep_components_Giza amod_components_following det_components_the dobj_using_components xcomp_built_using vmod_system_built nn_system_SMT amod_system_phrase-based det_system_a dep_Gough_2004b conj_and_Gough_Way prep_of_system_Way prep_of_system_Gough nn_system_EBMT amod_system_Marker-based det_system_the prep_against_evaluating_system dobj_evaluating_system dep_Way_2005 conj_and_Way_Gough prep_of_work_Gough prep_of_work_Way det_work_the prepc_by_extends_evaluating dobj_extends_work nsubj_extends_which rcmod_experiments_extends prep_of_set_experiments det_set_a dobj_describe_set nsubj_describe_we prep_in_describe_section advmod_describe_Accordingly det_section_this
W05-0834	P03-1021	o	More details on these standard criteria can be found for instance in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_instance_in prep_for_found_instance auxpass_found_be aux_found_can nsubjpass_found_details amod_criteria_standard det_criteria_these prep_on_details_criteria amod_details_More
W05-0834	P03-1021	o	-LRB- Och et al. 2003 -RRB-	amod_Och_2003 dep_Och_al. nn_Och_et dep_''_Och
W05-0834	P03-1021	o	The model scaling factors are optimized with respect to some evaluation criterion -LRB- Och 2003 -RRB-	amod_Och_2003 appos_criterion_Och nn_criterion_evaluation det_criterion_some prep_with_respect_to_optimized_criterion auxpass_optimized_are nsubjpass_optimized_factors nn_factors_scaling nn_factors_model det_factors_The
W05-0836	P03-1021	o	In this paper we will compare and evaluate several aspects of these techniques focusing on Minimum Error Rate -LRB- MER -RRB- training -LRB- Och 2003 -RRB- and Minimum Bayes Risk -LRB- MBR -RRB- decision rules within a novel training environment that isolates the impact of each component of these methods	det_methods_these prep_of_component_methods det_component_each prep_of_impact_component det_impact_the dobj_isolates_impact nsubj_isolates_that rcmod_environment_isolates nn_environment_training amod_environment_novel det_environment_a nn_rules_decision nn_rules_Risk appos_Risk_MBR nn_Risk_Bayes nn_Risk_Minimum dep_Och_2003 conj_and_training_rules dep_training_Och nn_training_Rate appos_Rate_MER nn_Rate_Error nn_Rate_Minimum prep_on_focusing_rules prep_on_focusing_training det_techniques_these prep_of_aspects_techniques amod_aspects_several nsubj_evaluate_we prep_within_compare_environment dep_compare_focusing dobj_compare_aspects conj_and_compare_evaluate aux_compare_will nsubj_compare_we prep_in_compare_paper det_paper_this
W05-0836	P03-1021	o	2.1 Minimum Error Rate Training The predominant approach to reconciling the mismatch between the MAP decision rule and the evaluation metric has been to train the parameters of the exponential model to correlate the MAP choice with the maximum score as indicated by the evaluation metric on a development set with known references -LRB- Och 2003 -RRB-	amod_Och_2003 dep_references_Och amod_references_known prep_with_set_references vmod_development_set det_development_a nn_metric_evaluation det_metric_the prep_on_indicated_development prep_by_indicated_metric mark_indicated_as amod_score_maximum det_score_the nn_choice_MAP det_choice_the advcl_correlate_indicated prep_with_correlate_score dobj_correlate_choice aux_correlate_to amod_model_exponential det_model_the prep_of_parameters_model det_parameters_the vmod_train_correlate dobj_train_parameters aux_train_to xcomp_been_train aux_been_has nsubj_been_Rate nn_metric_evaluation det_metric_the conj_and_rule_metric nn_rule_decision nn_rule_MAP det_rule_the prep_between_mismatch_metric prep_between_mismatch_rule det_mismatch_the dobj_reconciling_mismatch prepc_to_approach_reconciling amod_approach_predominant det_approach_The dobj_Training_approach vmod_Rate_Training nn_Rate_Error nn_Rate_Minimum num_Rate_2.1 ccomp_``_been
W05-0836	P03-1021	o	In the following we summarize the optimization algorithm for the unsmoothed error counts presented in -LRB- Och 2003 -RRB- and the implementation detailed in -LRB- Venugopal and Vogel 2005 -RRB-	amod_Venugopal_2005 conj_and_Venugopal_Vogel dep_in_Vogel dep_in_Venugopal prep_detailed_in vmod_implementation_detailed det_implementation_the appos_Och_2003 prep_in_presented_Och vmod_counts_presented conj_and_error_implementation dep_error_counts amod_error_unsmoothed det_error_the nn_algorithm_optimization det_algorithm_the prep_for_summarize_implementation prep_for_summarize_error dobj_summarize_algorithm nsubj_summarize_we prep_in_summarize_the amod_the_following
W05-0836	P03-1021	o	As discussed in -LRB- Och 2003 -RRB- the direct translation model represents the probability of target sentence English e = e1eI being the translation for a source sentence French f = f1 fJ through an exponential or log-linear model p -LRB- e | f -RRB- = exp -LRB- summationtextm k = 1 k hk -LRB- e f -RRB- -RRB- summationtext eprimeE exp -LRB- summationtextm k = 1 k hk -LRB- eprime f -RRB- -RRB- -LRB- 1 -RRB- where e is a single candidate translation for f from the set of all English translations E is the parameter vector for the model and each hk is a feature function of e and f In practice we restrict E to the set Gen -LRB- f -RRB- which is a set of highly likely translations discovered by a decoder -LRB- Vogel et al. 2003 -RRB-	amod_Vogel_2003 dep_Vogel_al. nn_Vogel_et det_decoder_a agent_discovered_decoder vmod_translations_discovered amod_translations_likely advmod_likely_highly prep_of_set_translations det_set_a cop_set_is nsubj_set_which dep_Gen_Vogel rcmod_Gen_set dep_Gen_f amod_Gen_set det_Gen_the prep_to_restrict_Gen dobj_restrict_E nsubj_restrict_we conj_and_e_f prep_of_function_f prep_of_function_e nn_function_feature det_function_a cop_function_is nsubj_function_hk det_hk_each det_model_the prep_for_vector_model nn_vector_parameter det_vector_the cop_vector_is nsubj_vector_translation dep_vector_1 nn_E_translations amod_E_English det_E_all prep_of_set_E det_set_the prep_from_f_set prep_for_translation_f nn_translation_candidate amod_translation_single det_translation_a cop_translation_is nsubj_translation_e advmod_translation_where appos_eprime_f appos_hk_eprime nn_hk_k num_hk_1 dep_=_hk rcmod_k_restrict prep_in_k_practice conj_and_k_function dep_k_vector amod_k_= nn_k_summationtextm dep_exp_function dep_exp_k nn_exp_eprimeE nn_exp_summationtext dep_e_f dep_hk_e nn_hk_k num_hk_1 dep_=_hk amod_k_= nn_k_summationtextm nn_k_exp dep_=_exp dep_=_k acomp_f_= dep_f_| dep_|_e dep_p_f nn_p_model amod_p_log-linear amod_p_exponential det_p_an conj_or_exponential_log-linear nn_fJ_f1 amod_fJ_= amod_fJ_French nn_fJ_sentence nn_fJ_source det_fJ_a advmod_=_f prep_through_translation_p prep_for_translation_fJ det_translation_the cop_translation_being nsubj_translation_e1eI dep_=_translation dep_=_e amod_=_English prep_sentence_= dep_target_sentence prep_of_probability_target det_probability_the dobj_represents_probability nsubj_represents_model advcl_represents_discussed nn_model_translation amod_model_direct det_model_the appos_Och_2003 prep_in_discussed_Och mark_discussed_As
W05-0904	P03-1021	o	The translations were generated by the alignment template system of Och -LRB- 2003 -RRB-	appos_Och_2003 prep_of_system_Och nn_system_template nn_system_alignment det_system_the agent_generated_system auxpass_generated_were nsubjpass_generated_translations det_translations_The
W05-0908	P03-1021	o	In the area of statistical machine translation -LRB- SMT -RRB- recently a combination of the BLEU evaluation metric -LRB- Papineni et al. 2001 -RRB- and the bootstrap method for statistical significance testing -LRB- Efron and Tibshirani 1993 -RRB- has become popular -LRB- Och 2003 Kumar and Byrne 2004 Koehn 2004b Zhang et al. 2004 -RRB-	num_Zhang_2004 nn_Zhang_al. nn_Zhang_et conj_Koehn_2004b dep_Kumar_Zhang conj_and_Kumar_Koehn conj_and_Kumar_2004 conj_and_Kumar_Byrne conj_Och_Koehn conj_Och_2004 conj_Och_Byrne conj_Och_Kumar conj_Och_2003 dep_become_Och acomp_become_popular aux_become_has nsubj_become_method nsubj_become_Papineni advmod_become_metric dep_Efron_1993 conj_and_Efron_Tibshirani appos_testing_Tibshirani appos_testing_Efron nn_testing_significance amod_testing_statistical prep_for_method_testing nn_method_bootstrap det_method_the conj_al._2001 nn_al._et conj_and_Papineni_method dep_Papineni_al. dep_evaluation_become amod_BLEU_evaluation dep_the_BLEU prep_of_combination_the det_combination_a advmod_combination_recently prep_in_combination_area appos_translation_SMT nn_translation_machine amod_translation_statistical prep_of_area_translation det_area_the
W05-0908	P03-1021	o	Our system is a re-implementation of the phrase-based system described in Koehn -LRB- 2003 -RRB- and uses publicly available components for word alignment -LRB- Och and Ney 2003 -RRB- 1 decoding -LRB- Koehn 2004a -RRB- 2 language modeling -LRB- Stolcke 2002 -RRB- 3 and finite-state processing -LRB- Knight and Al-Onaizan 1999 -RRB- 4	dep_Knight_1999 conj_and_Knight_Al-Onaizan amod_processing_finite-state dep_Stolcke_4 dep_Stolcke_Al-Onaizan dep_Stolcke_Knight conj_and_Stolcke_processing num_Stolcke_3 dep_Stolcke_2002 dep_modeling_processing dep_modeling_Stolcke nn_modeling_language appos_Koehn_modeling num_Koehn_2 appos_Koehn_2004a dep_decoding_Koehn dep_Och_2003 conj_and_Och_Ney num_alignment_1 dep_alignment_Ney dep_alignment_Och nn_alignment_word amod_components_available advmod_available_publicly prep_for_uses_alignment dobj_uses_components nsubj_uses_system appos_Koehn_2003 prep_in_described_Koehn vmod_system_described amod_system_phrase-based det_system_the vmod_re-implementation_decoding conj_and_re-implementation_uses prep_of_re-implementation_system det_re-implementation_a cop_re-implementation_is nsubj_re-implementation_system poss_system_Our
W05-1506	P03-1021	o	For example Och -LRB- 2003 -RRB- shows how to train a log-linear translation model not by maximizing the likelihood of training data but maximizing the BLEU score -LRB- among other metrics -RRB- of the model on 53 the data	det_data_the num_data_53 det_model_the amod_metrics_other prep_of_score_model prep_among_score_metrics nn_score_BLEU det_score_the prep_on_maximizing_data dobj_maximizing_score nn_data_training prep_of_likelihood_data det_likelihood_the dobj_maximizing_likelihood nn_model_translation amod_model_log-linear det_model_a conj_but_train_maximizing prepc_by_train_maximizing neg_train_not dobj_train_model aux_train_to advmod_train_how ccomp_shows_maximizing ccomp_shows_train nsubj_shows_Och prep_for_shows_example appos_Och_2003
W06-1606	P03-1021	o	The weights of the models are computed automatically using a variant of the Maximum Bleu training procedure proposed by Och -LRB- 2003 -RRB-	appos_Och_2003 agent_proposed_Och vmod_procedure_proposed nn_procedure_training nn_procedure_Bleu nn_procedure_Maximum det_procedure_the prep_of_variant_procedure det_variant_a dobj_using_variant xcomp_computed_using advmod_computed_automatically auxpass_computed_are nsubjpass_computed_weights det_models_the prep_of_weights_models det_weights_The
W06-1606	P03-1021	o	The decoder is capable of producing nbest derivations and nbest lists -LRB- Knight and Graehl 2005 -RRB- which are used for Maximum Bleu training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_Bleu nn_training_Maximum prep_for_used_training auxpass_used_are nsubjpass_used_which dep_Knight_2005 conj_and_Knight_Graehl nn_lists_nbest conj_and_derivations_lists amod_derivations_nbest dobj_producing_lists dobj_producing_derivations ccomp_capable_used dep_capable_Graehl dep_capable_Knight prepc_of_capable_producing cop_capable_is nsubj_capable_decoder det_decoder_The
W06-1606	P03-1021	o	We concatenate the lists and we learn a new combination of weights that maximizes the Bleu score of the combined nbest list using the same development corpus we used for tuning the individual systems -LRB- Och 2003 -RRB-	amod_Och_2003 dep_systems_Och amod_systems_individual det_systems_the dep_tuning_systems prep_for_used_tuning nsubj_used_we nn_corpus_development amod_corpus_same det_corpus_the dep_using_used dobj_using_corpus nn_list_nbest amod_list_combined det_list_the prep_of_score_list nn_score_Bleu det_score_the dobj_maximizes_score nsubj_maximizes_that rcmod_weights_maximizes prep_of_combination_weights amod_combination_new det_combination_a xcomp_learn_using dobj_learn_combination nsubj_learn_we det_lists_the conj_and_concatenate_learn dobj_concatenate_lists nsubj_concatenate_We ccomp_``_learn ccomp_``_concatenate
W06-1606	P03-1021	p	1 Introduction During the last four years various implementations and extentions to phrase-based statistical models -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB- have led to significant increases in machine translation accuracy	nn_accuracy_translation nn_accuracy_machine prep_in_increases_accuracy amod_increases_significant prep_to_led_increases aux_led_have nsubj_led_extentions nsubj_led_implementations dep_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong appos_models_Koehn appos_models_Wong appos_models_Marcu amod_models_statistical amod_models_phrase-based prep_to_implementations_models conj_and_implementations_extentions amod_implementations_various num_years_four amod_years_last det_years_the rcmod_Introduction_led prep_during_Introduction_years num_Introduction_1
W06-1607	P03-1021	o	To model p -LRB- t a | s -RRB- we use a standard loglinear approach p -LRB- t a | s -RRB- exp bracketleftBiggsummationdisplay i ifi -LRB- s t a -RRB- bracketrightBigg where each fi -LRB- s t a -RRB- is a feature function and weights i are set using Ochs algorithm -LRB- Och 2003 -RRB- to maximize the systems BLEU score -LRB- Papineni et al. 2001 -RRB- on a development corpus	nn_corpus_development det_corpus_a amod_Papineni_2001 dep_Papineni_al. nn_Papineni_et nn_score_BLEU nn_score_systems det_score_the prep_on_maximize_corpus dep_maximize_Papineni dobj_maximize_score aux_maximize_to amod_Och_2003 dep_algorithm_Och nn_algorithm_Ochs vmod_using_maximize dobj_using_algorithm xcomp_set_using auxpass_set_are nsubjpass_set_weights dep_weights_i nn_function_feature det_function_a nsubj_is_function nsubj_is_s advmod_is_where appos_s_a appos_s_t nn_s_fi det_s_each rcmod_bracketrightBigg_is nn_bracketrightBigg_ifi dep_s_a appos_s_t appos_ifi_s nn_ifi_i nn_ifi_bracketleftBiggsummationdisplay conj_and_exp_set dobj_exp_bracketrightBigg nsubj_exp_p nn_s_| det_s_a appos_t_s dep_p_t nn_approach_loglinear amod_approach_standard det_approach_a parataxis_use_set parataxis_use_exp dobj_use_approach nsubj_use_we advcl_use_model nn_s_| det_s_a appos_t_s nn_t_p dobj_model_t aux_model_To
W06-1607	P03-1021	o	In fact a limitation of the experiments described in this paper is that the loglinear weights for the glass-box techniques were optimized for BLEU using Ochs algorithm -LRB- Och 2003 -RRB- while the linear weights for 55 black-box techniques were set heuristically	advmod_set_heuristically auxpass_set_were nsubjpass_set_weights mark_set_while amod_techniques_black-box num_techniques_55 prep_for_weights_techniques amod_weights_linear det_weights_the amod_Och_2003 appos_algorithm_Och nn_algorithm_Ochs dobj_using_algorithm advcl_optimized_set xcomp_optimized_using prep_for_optimized_BLEU auxpass_optimized_were nsubjpass_optimized_weights mark_optimized_that nn_techniques_glass-box det_techniques_the prep_for_weights_techniques amod_weights_loglinear det_weights_the ccomp_is_optimized nsubj_is_limitation prep_in_is_fact det_paper_this prep_in_described_paper vmod_experiments_described det_experiments_the prep_of_limitation_experiments det_limitation_a
W06-1608	P03-1021	o	The weights for these models are determined using the method described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_method_described det_method_the dobj_using_method xcomp_determined_using auxpass_determined_are nsubjpass_determined_weights det_models_these prep_for_weights_models det_weights_The
W06-1615	P03-1021	p	Furthermore end-to-end systems like speech recognizers -LRB- Roark et al. 2004 -RRB- and automatic translators -LRB- Och 2003 -RRB- use increasingly sophisticated discriminative models which generalize well to new data that is drawn from the same distribution as the training data	nn_data_training det_data_the prep_as_distribution_data amod_distribution_same det_distribution_the prep_from_drawn_distribution auxpass_drawn_is nsubjpass_drawn_that rcmod_data_drawn amod_data_new prep_to_generalize_data advmod_generalize_well nsubj_generalize_which rcmod_models_generalize amod_models_discriminative amod_models_sophisticated advmod_sophisticated_increasingly dobj_use_models nsubj_use_translators nsubj_use_systems advmod_use_Furthermore amod_Och_2003 appos_translators_Och amod_translators_automatic amod_Roark_2004 dep_Roark_al. nn_Roark_et nn_recognizers_speech conj_and_systems_translators dep_systems_Roark prep_like_systems_recognizers amod_systems_end-to-end
W06-2606	P03-1021	o	Alternatively one can train them with respect to the final translation quality measured by an error criterion -LRB- Och 2003 -RRB-	amod_Och_2003 appos_criterion_Och nn_criterion_error det_criterion_an agent_measured_criterion vmod_quality_measured nn_quality_translation amod_quality_final det_quality_the prep_with_respect_to_train_quality dobj_train_them aux_train_can nsubj_train_one advmod_train_Alternatively
W06-3103	P03-1021	o	The model scaling factors M1 are trained with respect to the final translation quality measured by an error criterion -LRB- Och 2003 -RRB-	amod_Och_2003 appos_criterion_Och nn_criterion_error det_criterion_an agent_measured_criterion vmod_quality_measured nn_quality_translation amod_quality_final det_quality_the prep_with_respect_to_trained_quality auxpass_trained_are nsubjpass_trained_M1 nn_M1_factors nn_M1_scaling nn_M1_model det_M1_The
W06-3108	P03-1021	o	The model scaling factors M1 are trained with respect to the final translation quality measured by an error criterion -LRB- Och 2003 -RRB-	amod_Och_2003 appos_criterion_Och nn_criterion_error det_criterion_an agent_measured_criterion vmod_quality_measured nn_quality_translation amod_quality_final det_quality_the prep_with_respect_to_trained_quality auxpass_trained_are nsubjpass_trained_M1 nn_M1_factors nn_M1_scaling nn_M1_model det_M1_The
W06-3108	P03-1021	o	We train IBM Model 4 with GIZA + + -LRB- Och and Ney 2003 -RRB- in both translation directions	nn_directions_translation preconj_directions_both dep_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ num_Model_4 nn_Model_IBM prep_in_train_directions prep_with_train_+ prep_with_train_GIZA dobj_train_Model nsubj_train_We
W06-3108	P03-1021	o	Then the alignments are symmetrized using a refined heuristic as described in -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och prep_described_in mark_described_as dep_heuristic_described amod_heuristic_refined det_heuristic_a dobj_using_heuristic xcomp_symmetrized_using auxpass_symmetrized_are nsubjpass_symmetrized_alignments advmod_symmetrized_Then det_alignments_the ccomp_``_symmetrized
W06-3110	P03-1021	o	The model scaling factors M1 are trained with respect to the final translation quality measured by an error criterion -LRB- Och 2003 -RRB-	amod_Och_2003 appos_criterion_Och nn_criterion_error det_criterion_an agent_measured_criterion vmod_quality_measured nn_quality_translation amod_quality_final det_quality_the prep_with_respect_to_trained_quality auxpass_trained_are nsubjpass_trained_M1 nn_M1_factors nn_M1_scaling nn_M1_model det_M1_The
W06-3115	P03-1021	o	Feature function scaling factors m are optimized based on a maximum likelihood approach -LRB- Och and Ney 2002 -RRB- or on a direct error minimization approach -LRB- Och 2003 -RRB-	amod_Och_2003 dep_approach_Och nn_approach_minimization nn_approach_error amod_approach_direct det_approach_a pobj_on_approach conj_and_Och_2002 conj_and_Och_Ney dep_approach_2002 dep_approach_Ney dep_approach_Och nn_approach_likelihood nn_approach_maximum det_approach_a conj_or_optimized_on prep_based_on_optimized_approach auxpass_optimized_are nsubjpass_optimized_function nn_m_factors dobj_scaling_m vmod_function_scaling nn_function_Feature
W06-3115	P03-1021	o	First manyto-many word alignments are induced by running a one-to-many word alignment model such as GIZA + + -LRB- Och and Ney 2003 -RRB- in both directions and by combining the results based on a heuristic -LRB- Och and Ney 2004 -RRB-	amod_Och_2004 conj_and_Och_Ney dep_heuristic_Ney dep_heuristic_Och det_heuristic_a pobj_results_heuristic prepc_based_on_results_on det_results_the dobj_combining_results pcomp_by_combining det_directions_both pobj_in_directions num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_by conj_+_GIZA_in conj_+_GIZA_+ prep_such_as_model_by prep_such_as_model_in prep_such_as_model_+ prep_such_as_model_GIZA nn_model_alignment nn_model_word amod_model_one-to-many det_model_a dobj_running_model agent_induced_running auxpass_induced_are nsubjpass_induced_alignments advmod_induced_First nn_alignments_word amod_alignments_manyto-many
W06-3115	P03-1021	o	For each differently tokenized corpus we computed word alignments by a HMM translation model -LRB- Och and Ney 2003 -RRB- and by a word alignment refinement heuristic of grow-diagfinal -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diagfinal_Koehn prep_of_heuristic_grow-diagfinal nn_heuristic_refinement nn_heuristic_alignment nn_heuristic_word det_heuristic_a pobj_by_heuristic num_Och_2003 conj_and_Och_Ney appos_model_Ney appos_model_Och nn_model_translation nn_model_HMM det_model_a nn_alignments_word conj_and_computed_by prep_by_computed_model dobj_computed_alignments nsubj_computed_we prep_for_computed_corpus amod_corpus_tokenized advmod_corpus_differently det_corpus_each
W06-3119	P03-1021	o	Given a source sentence f the preferred translation output is determined by computing the lowest-cost derivation -LRB- combination of hierarchical and glue rules -RRB- yielding f as its source side where the cost of a derivation R1 Rn with respective feature vectors v1 vn Rm is given by msummationdisplay i = 1 i nsummationdisplay j = 1 -LRB- vj -RRB- i. Here 1 m are the parameters of the loglinear model which we optimize on a held-out portion of the training set -LRB- 2005 development data -RRB- using minimum-error-rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och amod_training_minimum-error-rate dobj_using_training nn_data_development num_data_2005 appos_set_data nn_set_training det_set_the prep_of_portion_set amod_portion_held-out det_portion_a xcomp_optimize_using prep_on_optimize_portion nsubj_optimize_we dobj_optimize_which rcmod_model_optimize amod_model_loglinear det_model_the prep_of_parameters_model det_parameters_the cop_parameters_are nsubj_parameters_m dobj_i._1 advmod_i._Here num_vj_1 dep_=_i. dep_=_vj amod_j_= nn_j_nsummationdisplay nn_j_i appos_1_j dep_=_1 amod_i_= rcmod_msummationdisplay_parameters dep_msummationdisplay_i agent_given_msummationdisplay auxpass_given_is nsubjpass_given_cost advmod_given_where nn_Rm_vn appos_v1_Rm dep_vectors_v1 nn_vectors_feature amod_vectors_respective prep_with_Rn_vectors nn_Rn_R1 nn_Rn_derivation det_Rn_a prep_of_cost_Rn det_cost_the rcmod_side_given nn_side_source poss_side_its prep_as_yielding_side dobj_yielding_f amod_rules_glue amod_rules_hierarchical conj_and_hierarchical_glue prep_of_combination_rules vmod_derivation_yielding dep_derivation_combination amod_derivation_lowest-cost det_derivation_the dobj_computing_derivation agent_determined_computing auxpass_determined_is nsubjpass_determined_output dep_determined_f nn_output_translation amod_output_preferred det_output_the dep_sentence_determined nn_sentence_source det_sentence_a pobj_Given_sentence ccomp_``_Given
W06-3121	P03-1021	p	The MERT module is a highly modular efficient and customizable implementation of the algorithm described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_algorithm_described det_algorithm_the prep_of_implementation_algorithm amod_implementation_customizable amod_implementation_efficient amod_implementation_modular det_implementation_a cop_implementation_is nsubj_implementation_module conj_and_efficient_customizable advmod_modular_highly nn_module_MERT det_module_The
W06-3121	P03-1021	o	The software also required GIZA + + word alignment tool -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_tool_Ney dep_tool_Och nn_tool_alignment nn_tool_word pobj_+_tool conj_+_GIZA_+ dobj_required_+ dobj_required_GIZA advmod_required_also nsubj_required_software det_software_The ccomp_``_required
W06-3121	P03-1021	o	In this paper we present Phramer an open-source system that embeds a phrase-based decoder a minimum error rate training -LRB- Och 2003 -RRB- module and various tools related to Machine Translation -LRB- MT -RRB-	appos_Translation_MT nn_Translation_Machine prep_to_related_Translation vmod_tools_related amod_tools_various dep_module_Och nn_module_training dep_Och_2003 nn_training_rate nn_training_error amod_training_minimum det_training_a conj_and_decoder_tools conj_and_decoder_module amod_decoder_phrase-based det_decoder_a dobj_embeds_tools dobj_embeds_module dobj_embeds_decoder nsubj_embeds_that rcmod_system_embeds nn_system_open-source det_system_an appos_Phramer_system dobj_present_Phramer nsubj_present_we prep_in_present_paper det_paper_this
W06-3122	P03-1021	o	It generates a vector of 5 numeric values for each phrase pair phrase translation probability -LRB- f | e -RRB- = count -LRB- f e -RRB- count -LRB- e -RRB- -LRB- e | f -RRB- = count -LRB- f e -RRB- count -LRB- f -RRB- 2http / / www.phramer.org / Java-based open-source phrase based SMT system 3http / / www.isi.edu/licensed-sw/carmel/ 4http / / www.speech.sri.com/projects/srilm/ 5http / / www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 150 lexical weighting -LRB- Koehn et al. 2003 -RRB- lex -LRB- f | e a -RRB- = nproductdisplay i = 1 1 | -LCB- j | -LRB- i j -RRB- a -RCB- | summationdisplay -LRB- i j -RRB- a w -LRB- fi | ej -RRB- lex -LRB- e | f a -RRB- = mproductdisplay j = 1 1 | -LCB- i | -LRB- i j -RRB- a -RCB- | summationdisplay -LRB- i j -RRB- a w -LRB- ej | fi -RRB- phrase penalty -LRB- f | e -RRB- = e log -LRB- -LRB- f | e -RRB- -RRB- = 1 2.2 Decoding We used the Pharaoh decoder for both the Minimum Error Rate Training -LRB- Och 2003 -RRB- and test dataset decoding	nn_decoding_dataset nn_decoding_test amod_Och_2003 conj_and_Training_decoding dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum det_Training_the preconj_Training_both nn_decoder_Pharaoh det_decoder_the prep_for_used_decoding prep_for_used_Training dobj_used_decoder nsubj_used_We rcmod_Decoding_used num_Decoding_2.2 number_2.2_1 dep_=_Decoding dep_=_log dep_e_| dep_e_f dep_log_e dep_=_e dep_=_e nsubj_=_| dep_=_f parataxis_penalty_= dep_penalty_= nn_penalty_phrase dep_penalty_fi nn_penalty_ej num_fi_| dep_w_penalty det_w_a appos_i_j dep_summationdisplay_w dep_summationdisplay_i num_summationdisplay_| det_summationdisplay_a appos_i_j dobj_|_summationdisplay dep_|_i nn_|_i dep_|_| num_|_1 number_1_1 dep_=_| nsubj_=_j nn_j_mproductdisplay dep_=_= dep_=_a advmod_=_f dep_=_e dep_=_lex dep_=_w nn_f_| nn_lex_fi num_ej_| appos_fi_ej det_w_a appos_i_j amod_summationdisplay_= dep_summationdisplay_i num_summationdisplay_| det_summationdisplay_a amod_summationdisplay_| appos_i_j dep_|_i nn_|_j num_|_| num_|_1 number_1_1 dobj_=_summationdisplay advmod_=_i nsubj_=_nproductdisplay dep_=_= det_=_a dep_=_e advmod_=_| nn_|_f dep_lex_= amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_weighting_lex appos_weighting_Koehn amod_weighting_lexical num_weighting_150 nn_weighting_www.iccs.inf.ed.ac.uk/pkoehn/training.tgz amod_5http_www.speech.sri.com/projects/srilm/ amod_4http_www.isi.edu/licensed-sw/carmel/ num_system_3http nn_system_SMT amod_system_based dep_phrase_system amod_phrase_open-source amod_phrase_Java-based nn_phrase_www.phramer.org dep_2http_weighting dep_2http_5http dep_2http_4http dep_2http_phrase dep_2http_f nn_2http_count dep_count_e dep_count_f nn_count_count dobj_=_2http dep_=_f dep_=_| dep_=_e conj_count_= appos_count_e dep_e_count dep_f_e dep_count_f dep_=_count dep_=_e dep_=_| dep_f_= dep_probability_f nn_probability_translation nn_probability_phrase nn_pair_phrase det_pair_each amod_values_numeric num_values_5 prep_for_vector_pair prep_of_vector_values det_vector_a dep_generates_probability dobj_generates_vector nsubj_generates_It
W06-3122	P03-1021	n	The size of the development set used to generate 1 and 2 -LRB- 1000 sentences -RRB- compensates the tendency of the unsmoothed MERT algorithm to overfit -LRB- Och 2003 -RRB- by providing a high ratio between number of variables and number of parameters to be estimated	auxpass_estimated_be aux_estimated_to vmod_parameters_estimated prep_of_variables_parameters conj_and_variables_number prep_of_number_number prep_of_number_variables prep_between_ratio_number amod_ratio_high det_ratio_a dobj_providing_ratio appos_Och_2003 dep_overfit_Och nn_algorithm_MERT amod_algorithm_unsmoothed det_algorithm_the prep_to_tendency_overfit prep_of_tendency_algorithm det_tendency_the prepc_by_compensates_providing dobj_compensates_tendency nsubj_compensates_size num_sentences_1000 appos_1_sentences conj_and_1_2 dobj_generate_2 dobj_generate_1 aux_generate_to xcomp_used_generate vmod_set_used nn_set_development det_set_the prep_of_size_set det_size_The
W06-3601	P03-1021	o	Feature weights of both systems are tuned on the same data set .3 For Pharaoh we use the standard minimum error-rate training -LRB- Och 2003 -RRB- and for our system since there are only two independent features -LRB- as we always fix = 1 -RRB- we use a simple grid-based line-optimization along the language-model weight axis	nn_axis_weight amod_axis_language-model det_axis_the amod_line-optimization_grid-based amod_line-optimization_simple det_line-optimization_a prep_along_use_axis dobj_use_line-optimization nsubj_use_we advcl_use_are prep_for_use_system dep_=_1 dep_fix_= advmod_fix_always nsubj_fix_we mark_fix_as amod_features_independent num_features_two quantmod_two_only parataxis_are_fix nsubj_are_features expl_are_there mark_are_since poss_system_our appos_Och_2003 dep_training_Och amod_training_error-rate nn_training_minimum amod_training_standard det_training_the conj_and_use_use dobj_use_training nsubj_use_we ccomp_use_tuned prep_for_.3_Pharaoh amod_.3_set dep_data_.3 amod_data_same det_data_the prep_on_tuned_data auxpass_tuned_are nsubjpass_tuned_weights det_systems_both prep_of_weights_systems nn_weights_Feature
W06-3601	P03-1021	o	2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- are good at learning local translations that are pairs of -LRB- consecutive -RRB- sub-strings but often insufficient in modeling the reorderings of phrases themselves especially between language pairs with very different word-order	amod_word-order_different advmod_different_very nn_pairs_language npadvmod_phrases_themselves prep_with_reorderings_word-order prep_between_reorderings_pairs advmod_reorderings_especially prep_of_reorderings_phrases det_reorderings_the dep_insufficient_reorderings prep_in_insufficient_modeling advmod_insufficient_often nsubj_insufficient_Koehn dep_sub-strings_consecutive prep_of_pairs_sub-strings cop_pairs_are nsubj_pairs_that rcmod_translations_pairs amod_translations_local dobj_learning_translations conj_but_good_insufficient prepc_at_good_learning cop_good_are nsubj_good_Koehn dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_models_Phrase-based nn_models_MT. amod_models_statistical prep_in_efforts_models amod_efforts_recent det_approach_this dep_compare_insufficient dep_compare_good prep_with_compare_efforts dobj_compare_approach aux_compare_to xcomp_helpful_compare cop_helpful_is nsubj_helpful_It rcmod_Work_helpful amod_Work_Previous num_Work_2
W06-3602	P03-1021	o	The real-valued features include the following a block translation score derived from phrase occurrence statistics a4a9a113a77a11 a trigram language model to predict target words a4a179a112a229 a78a204a11 a lexical weighting score for the block internal words a4a127a202a204a11 a distortion model a4a0a207a229 a218a147a11 as well as the negative target phrase length a4a60a36a87a11 The transition cost is computed as a19 a4a20a6 a23 a6 a39 a11a224a15 a27 a28 a30a89a32 a4a7a6 a83 a6a20a39a34a11 where a27 a199a230a227 a228 is a weight vector that sums up to a113a89a35a116 a228 a13a26a17 a10 a27 a13a217a15a231a113a25a35a116 The weights are trained using a procedure similar to -LRB- Och 2003 -RRB- on held-out test data	nn_data_test amod_data_held-out prep_on_Och_data dep_Och_2003 prep_to_similar_Och amod_procedure_similar det_procedure_a dobj_using_procedure xcomp_trained_using auxpass_trained_are nsubjpass_trained_weights det_weights_The dep_a13a217a15a231a113a25a35a116_trained nn_a13a217a15a231a113a25a35a116_a27 nn_a13a217a15a231a113a25a35a116_a10 nn_a13a217a15a231a113a25a35a116_a13a26a17 num_a13a217a15a231a113a25a35a116_a228 prep_to_sums_a113a89a35a116 prt_sums_up nsubj_sums_that rcmod_vector_sums nn_vector_weight det_vector_a cop_vector_is nsubj_vector_a228 advmod_vector_where nn_a228_a199a230a227 nn_a228_a27 rcmod_a6a20a39a34a11_vector nn_a6a20a39a34a11_a83 nn_a6a20a39a34a11_a4a7a6 nn_a6a20a39a34a11_a30a89a32 num_a6a20a39a34a11_a28 nn_a6a20a39a34a11_a27 nn_a6a20a39a34a11_a11a224a15 nn_a6a20a39a34a11_a39 nn_a6a20a39a34a11_a6 nn_a6a20a39a34a11_a23 nn_a6a20a39a34a11_a4a20a6 nn_a6a20a39a34a11_a19 dep_computed_a13a217a15a231a113a25a35a116 prep_as_computed_a6a20a39a34a11 auxpass_computed_is nsubjpass_computed_cost nn_cost_transition det_cost_The nn_a4a60a36a87a11_length nn_a4a60a36a87a11_phrase nn_a4a60a36a87a11_target amod_a4a60a36a87a11_negative det_a4a60a36a87a11_the conj_and_a218a147a11_a4a60a36a87a11 nn_a218a147a11_a4a0a207a229 nn_a218a147a11_model nn_a218a147a11_distortion det_a218a147a11_a appos_a4a127a202a204a11_a4a60a36a87a11 appos_a4a127a202a204a11_a218a147a11 nn_a4a127a202a204a11_words amod_a4a127a202a204a11_internal dep_block_a4a127a202a204a11 det_block_the prep_for_score_block nn_score_weighting amod_score_lexical det_score_a appos_a78a204a11_score amod_a78a204a11_a4a179a112a229 nn_a78a204a11_words nn_a78a204a11_target dobj_predict_a78a204a11 aux_predict_to vmod_model_predict nn_model_language nn_model_trigram det_model_a appos_a4a9a113a77a11_model nn_a4a9a113a77a11_statistics nn_a4a9a113a77a11_occurrence nn_a4a9a113a77a11_phrase prep_from_derived_a4a9a113a77a11 parataxis_score_computed vmod_score_derived nn_score_translation nn_score_block det_score_a dep_following_score det_following_the dobj_include_following nsubj_include_features amod_features_real-valued det_features_The
W07-0401	P03-1021	o	Alternatively one can train them with respect to the final translation quality measured by an error criterion -LRB- Och 2003 -RRB-	amod_Och_2003 appos_criterion_Och nn_criterion_error det_criterion_an agent_measured_criterion vmod_quality_measured nn_quality_translation amod_quality_final det_quality_the prep_with_respect_to_train_quality dobj_train_them aux_train_can nsubj_train_one advmod_train_Alternatively
W07-0401	P03-1021	o	Here we train word alignments in both directions with GIZA + + -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ prep_with_directions_+ prep_with_directions_GIZA det_directions_both nn_alignments_word prep_in_train_directions dobj_train_alignments nsubj_train_we advmod_train_Here
W07-0403	P03-1021	o	We report precision recall and balanced F-measure -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_F-measure_Ney dep_F-measure_Och amod_F-measure_balanced conj_and_precision_F-measure conj_and_precision_recall dobj_report_F-measure dobj_report_recall dobj_report_precision nsubj_report_We
W07-0403	P03-1021	o	Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training -LRB- Och 2003 -RRB- as implemented by Venugopal and Vogel -LRB- 2005 -RRB-	appos_Vogel_2005 conj_and_Venugopal_Vogel prep_by_implemented_Vogel prep_by_implemented_Venugopal mark_implemented_as dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum dep_task_implemented prep_with_task_training amod_task_shared det_task_the prep_for_provided_task dep_set_provided vmod_tuning_set amod_tuning_500-sentence det_tuning_the dobj_using_tuning xcomp_set_using auxpass_set_are nsubjpass_set_Weights amod_model_log-linear det_model_the prep_for_Weights_model
W07-0403	P03-1021	o	The surface heuristic can define consistency according to any word alignment but most often the alignment is provided by GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ agent_provided_+ agent_provided_GIZA auxpass_provided_is nsubjpass_provided_alignment advmod_provided_often det_alignment_the advmod_often_most cc_often_but nn_alignment_word det_alignment_any parataxis_define_provided pobj_define_alignment prepc_according_to_define_to dobj_define_consistency aux_define_can nsubj_define_heuristic nn_heuristic_surface det_heuristic_The
W07-0403	P03-1021	o	Many-to-many alignments can be created by combining two GIZA + + alignments one where English generates Foreign and another with those roles reversed -LRB- Och and Ney 2003 -RRB-	dep_Och_2003 conj_and_Och_Ney dep_reversed_Ney dep_reversed_Och nsubj_reversed_another nsubj_reversed_Foreign det_roles_those prep_with_another_roles conj_and_Foreign_another ccomp_generates_reversed nsubj_generates_English advmod_generates_where rcmod_one_generates conj_+_+_alignments appos_GIZA_one dep_GIZA_alignments dep_GIZA_+ num_GIZA_two dobj_combining_GIZA agent_created_combining auxpass_created_be aux_created_can nsubjpass_created_alignments amod_alignments_Many-to-many
W07-0410	P03-1021	o	Different optimization techniques are available like the Simplex algorithm or the special Minimum Error Training as described in -LRB- Och 2003 -RRB-	dep_Och_2003 dep_in_Och prep_described_in mark_described_as advcl_Training_described vmod_Error_Training nn_Error_Minimum amod_Error_special det_Error_the conj_or_algorithm_Error nn_algorithm_Simplex det_algorithm_the prep_like_available_Error prep_like_available_algorithm cop_available_are nsubj_available_techniques nn_techniques_optimization amod_techniques_Different
W07-0701	P03-1021	o	The comparison phrasal system was constructed using the same GIZA + + alignments and the heuristic combination described in -LRB- Och & Ney 2003 -RRB-	dep_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och prep_described_in vmod_combination_described nn_combination_heuristic det_combination_the dep_alignments_+ conj_and_GIZA_combination conj_+_GIZA_alignments amod_GIZA_same det_GIZA_the dobj_using_combination dobj_using_alignments dobj_using_GIZA xcomp_constructed_using auxpass_constructed_was nsubjpass_constructed_system amod_system_phrasal nn_system_comparison det_system_The
W07-0701	P03-1021	o	Model weights were trained separately for all 3 systems using minimum error rate training to maximize BLEU -LRB- Och 2003 -RRB- on the development set -LRB- dev -RRB-	appos_set_dev nn_set_development det_set_the dep_Och_2003 appos_BLEU_Och prep_on_maximize_set dobj_maximize_BLEU aux_maximize_to nn_training_rate nn_training_error amod_training_minimum vmod_using_maximize dobj_using_training num_systems_3 det_systems_all xcomp_trained_using prep_for_trained_systems advmod_trained_separately auxpass_trained_were nsubjpass_trained_weights nn_weights_Model
W07-0702	P03-1021	o	The factored translation model combines features in a log-linear fashion -LRB- Och 2003 -RRB-	amod_Och_2003 dep_fashion_Och amod_fashion_log-linear det_fashion_a prep_in_combines_fashion dobj_combines_features nsubj_combines_model nn_model_translation amod_model_factored det_model_The ccomp_``_combines
W07-0703	P03-1021	o	Weights on the loglinear features are set using Och 's algorithm -LRB- Och 2003 -RRB- to maximize the system 's BLEU score on a development corpus	nn_corpus_development det_corpus_a prep_on_score_corpus nn_score_BLEU poss_score_system det_system_the dobj_maximize_score aux_maximize_to amod_Och_2003 dep_algorithm_Och poss_algorithm_Och vmod_using_maximize dobj_using_algorithm xcomp_set_using auxpass_set_are nsubjpass_set_Weights nn_features_loglinear det_features_the prep_on_Weights_features
W07-0706	P03-1021	o	We selected 580 short sentences of length at most 50 characters from the 2002 NIST MT Evaluation test set as our development corpus and used it to tune s by maximizing the BLEU score -LRB- Och 2003 -RRB- and used the 2005 NIST MT Evaluation test set as our test corpus	nn_corpus_test poss_corpus_our prep_as_set_corpus vmod_test_set nn_test_Evaluation nn_test_MT nn_test_NIST num_test_2005 det_test_the dobj_used_test nsubj_used_We dep_Och_2003 appos_score_Och nn_score_BLEU det_score_the dobj_maximizing_score prepc_by_tune_maximizing dobj_tune_s aux_tune_to xcomp_used_tune dobj_used_it nsubj_used_We nn_corpus_development poss_corpus_our prep_as_set_corpus vmod_test_set nn_test_Evaluation nn_test_MT nn_test_NIST num_test_2002 det_test_the prep_from_characters_test num_characters_50 amod_characters_most prep_of_sentences_length amod_sentences_short num_sentences_580 conj_and_selected_used conj_and_selected_used prep_at_selected_characters dobj_selected_sentences nsubj_selected_We
W07-0710	P03-1021	o	We use the n-best generation scheme interleaved with optimization as described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in mark_described_as advcl_interleaved_described prep_with_interleaved_optimization vmod_scheme_interleaved nn_scheme_generation amod_scheme_n-best det_scheme_the dobj_use_scheme nsubj_use_We
W07-0710	P03-1021	p	73 2.2.4 Minimum Error Rate Training A good way of training is to minimize empirical top-1 error on training data -LRB- Och 2003 -RRB-	amod_Och_2003 dep_data_Och nn_data_training prep_on_error_data nn_error_top-1 amod_error_empirical dobj_minimize_error aux_minimize_to xcomp_is_minimize nsubj_is_73 prep_of_way_training amod_way_good det_way_A dobj_Training_way vmod_Rate_Training nn_Rate_Error nn_Rate_Minimum num_Rate_2.2.4 dep_73_Rate
W07-0710	P03-1021	p	1 Introduction In recent years statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation -LRB- Papineni et al. 2002 -RRB- and errorbased optimization -LRB- Och 2003 -RRB-	amod_Och_2003 dep_optimization_Och amod_optimization_errorbased amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_evaluation_automatic prep_to_thanks_evaluation nn_thanks_quality prep_in_leap_thanks nn_leap_quantum det_leap_a conj_and_experienced_optimization dep_experienced_Papineni dobj_experienced_leap aux_experienced_have nsubj_experienced_translation dep_experienced_Introduction nn_translation_machine amod_translation_statistical amod_years_recent prep_in_Introduction_years num_Introduction_1 ccomp_``_optimization ccomp_``_experienced
W07-0711	P03-1021	o	In the experiment only the first 500 sentences were used to train the log-linear model weight vector where minimum error rate -LRB- MER -RRB- training was used -LRB- Och 2003 -RRB-	amod_Och_2003 dep_used_Och auxpass_used_was nsubjpass_used_training advmod_used_where nn_training_rate appos_rate_MER nn_rate_error amod_rate_minimum rcmod_vector_used nn_vector_weight nn_vector_model amod_vector_log-linear det_vector_the dobj_train_vector aux_train_to xcomp_used_train auxpass_used_were nsubjpass_used_sentences prep_in_used_experiment num_sentences_500 amod_sentences_first det_sentences_the advmod_sentences_only det_experiment_the
W07-0713	P03-1021	o	Still a confidence range for BLEU can be estimated by bootstrapping -LRB- Och 2003 Zhang and Vogel 2004 -RRB-	amod_Zhang_2004 conj_and_Zhang_Vogel dep_Och_Vogel dep_Och_Zhang conj_Och_2003 dep_bootstrapping_Och agent_estimated_bootstrapping auxpass_estimated_be aux_estimated_can nsubjpass_estimated_range advmod_estimated_Still prep_for_range_BLEU nn_range_confidence det_range_a ccomp_``_estimated
W07-0715	P03-1021	o	The feature weights for the overall translation models were trained using Och?s -LRB- 2003 -RRB- minimum-error-rate training procedure	nn_procedure_training amod_procedure_minimum-error-rate nn_procedure_Och?s appos_Och?s_2003 dobj_using_procedure xcomp_trained_using auxpass_trained_were nsubjpass_trained_weights nn_models_translation amod_models_overall det_models_the prep_for_weights_models nn_weights_feature det_weights_The
W07-0716	P03-1021	o	Och -LRB- 2003 -RRB- introduced minimum error rate training -LRB- MERT -RRB- a technique for optimizing log-linear modelparametersrelativetoameasureoftranslation quality	nn_quality_modelparametersrelativetoameasureoftranslation amod_quality_log-linear amod_quality_optimizing prep_for_technique_quality det_technique_a appos_training_technique appos_training_MERT nn_training_rate nn_training_error amod_training_minimum amod_training_introduced nn_training_Och appos_Och_2003
W07-0716	P03-1021	o	?? Initial phrase pairs are identified following the procedure typically employed in phrase based systems -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_systems_based nn_systems_phrase prep_in_employed_systems advmod_employed_typically vmod_procedure_employed det_procedure_the dep_identified_Koehn prep_following_identified_procedure auxpass_identified_are nsubjpass_identified_pairs nn_pairs_phrase amod_pairs_Initial num_pairs_??
W07-0716	P03-1021	o	Oncetraininghastakenplace minimumerrorrate training -LRB- Och 2003 -RRB- is used to tune the parameters i. Finally decoding in Hiero takes place using a CKY synchronous parser with beam search augmented to permit efficient incorporation of language model scores -LRB- Chiang 2007 -RRB-	amod_Chiang_2007 appos_scores_Chiang nn_scores_model nn_scores_language prep_of_incorporation_scores amod_incorporation_efficient dobj_permit_incorporation aux_permit_to xcomp_augmented_permit nn_search_beam amod_parser_synchronous nn_parser_CKY det_parser_a prep_with_using_search dobj_using_parser xcomp_takes_using dobj_takes_place csubj_takes_decoding prep_in_decoding_Hiero ccomp_i._takes advmod_i._Finally vmod_parameters_i. det_parameters_the dobj_tune_parameters aux_tune_to dep_used_augmented xcomp_used_tune auxpass_used_is nsubjpass_used_Oncetraininghastakenplace dep_Och_2003 appos_training_Och amod_training_minimumerrorrate appos_Oncetraininghastakenplace_training
W07-0717	P03-1021	o	To model p -LRB- t a | s -RRB- we use a standard loglinear approach p -LRB- t a | s -RRB- ?? exp bracketleftBiggsummationdisplay i ifi -LRB- s t a -RRB- bracketrightBigg -LRB- 1 -RRB- where each fi -LRB- s t a -RRB- is a feature function and weights i are set using Och?s algorithm -LRB- Och 2003 -RRB- to maximize the system?s BLEU score -LRB- Papineni et al. 2001 -RRB- on a development corpus	nn_corpus_development det_corpus_a amod_Papineni_2001 dep_Papineni_al. nn_Papineni_et nn_score_BLEU nn_score_system?s det_score_the prep_on_maximize_corpus dep_maximize_Papineni dobj_maximize_score aux_maximize_to amod_Och_2003 dep_algorithm_Och nn_algorithm_Och?s vmod_using_maximize dobj_using_algorithm xcomp_set_using auxpass_set_are nsubjpass_set_weights dep_weights_i nn_function_feature det_function_a nsubj_is_function nsubj_is_s advmod_is_where appos_s_a appos_s_t nn_s_fi det_s_each rcmod_bracketrightBigg_is appos_bracketrightBigg_1 nn_bracketrightBigg_ifi nn_bracketrightBigg_exp appos_s_a appos_s_t appos_ifi_s nn_ifi_i nn_ifi_bracketleftBiggsummationdisplay conj_and_??_set dobj_??_bracketrightBigg nsubj_??_p nn_s_| det_s_a appos_t_s dep_p_t nn_approach_loglinear amod_approach_standard det_approach_a parataxis_use_set parataxis_use_?? dobj_use_approach nsubj_use_we advcl_use_model nn_s_| det_s_a appos_t_s nn_t_p dobj_model_t aux_model_To
W07-0724	P03-1021	o	Their weights are optimized w.r.t. BLEU score using the algorithm described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in vmod_algorithm_described det_algorithm_the dobj_using_algorithm vmod_score_using nn_score_BLEU nn_score_w.r.t. dobj_optimized_score auxpass_optimized_are nsubjpass_optimized_weights poss_weights_Their ccomp_``_optimized
W07-0726	P03-1021	o	3see http://www.statmt.org/moses/ 194 4 Implementation Details 4.1 Alignment of MT output The input text and the output text of the MT systems was aligned by means of GIZA + + -LRB- Och and Ney 2003 -RRB- a tool with which statistical models for alignment of parallel texts can be trained	auxpass_trained_be aux_trained_can nsubjpass_trained_models prep_with_trained_which amod_texts_parallel prep_of_alignment_texts prep_for_models_alignment amod_models_statistical rcmod_tool_trained det_tool_a num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och appos_GIZA_tool conj_+_GIZA_+ prep_by_means_of_aligned_+ prep_by_means_of_aligned_GIZA auxpass_aligned_was nsubjpass_aligned_text nsubjpass_aligned_text nn_systems_MT det_systems_the nn_text_output det_text_the prep_of_text_systems conj_and_text_text nn_text_input det_text_The amod_output_MT rcmod_Alignment_aligned prep_of_Alignment_output num_Alignment_4.1 dep_Details_Alignment nn_Details_Implementation num_Details_4 nn_Details_http://www.statmt.org/moses/ amod_Details_3see number_4_194
W07-0726	P03-1021	o	The optimal weights for the different columns can then be assigned with the help of minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_of_help_training det_help_the prep_with_assigned_help auxpass_assigned_be advmod_assigned_then aux_assigned_can nsubjpass_assigned_weights amod_columns_different det_columns_the prep_for_weights_columns amod_weights_optimal det_weights_The
W07-0727	P03-1021	o	To optimize the system towards a maximal BLEU or NIST score we use Minimum Error Rate -LRB- MER -RRB- Training as described in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_described_in mark_described_as advcl_Training_described vmod_Rate_Training appos_Rate_MER nn_Rate_Error nn_Rate_Minimum dobj_use_Rate nsubj_use_we advcl_use_optimize nn_score_NIST conj_or_BLEU_score amod_BLEU_maximal det_BLEU_a det_system_the prep_towards_optimize_score prep_towards_optimize_BLEU dobj_optimize_system aux_optimize_To
W07-0729	P03-1021	o	Feature weight tuning was carried out using minimum error rate training maximizing BLEU scores on a held-out development set -LRB- Och 2003 -RRB-	amod_Och_2003 dep_set_Och nn_set_development amod_set_held-out det_set_a nn_scores_BLEU prep_on_maximizing_set dobj_maximizing_scores nn_training_rate nn_training_error amod_training_minimum dobj_using_training xcomp_carried_maximizing xcomp_carried_using prt_carried_out auxpass_carried_was nsubjpass_carried_tuning nn_tuning_weight nn_tuning_Feature
W07-0730	P03-1021	n	Unfortunately longer sentences -LRB- up to 100 tokens rather than 40 -RRB- longer phrases -LRB- up to 10 tokens rather than 7 -RRB- two LMs -LRB- rather than just one -RRB- higher-order LMs -LRB- order 7 rather than 3 -RRB- multiple higher-order lexicalized re-ordering models -LRB- up to 3 -RRB- etc. all contributed to increased system?s complexity and as a result time limitations prevented us from performing minimum-error-rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- for ucb3 ucb4 and ucb5	conj_and_ucb3_ucb5 conj_and_ucb3_ucb4 dep_Och_2003 appos_training_Och appos_training_MERT amod_training_minimum-error-rate prep_for_performing_ucb5 prep_for_performing_ucb4 prep_for_performing_ucb3 dobj_performing_training prepc_from_prevented_performing dobj_prevented_us nsubj_prevented_limitations prep_as_prevented_result nn_limitations_time det_result_a nn_complexity_system?s amod_complexity_increased conj_and_contributed_prevented prep_to_contributed_complexity dep_contributed_all nsubj_contributed_etc. dep_contributed_3 dep_3_to advmod_3_up rcmod_models_prevented rcmod_models_contributed nn_models_re-ordering amod_models_lexicalized amod_models_higher-order amod_models_multiple prep_than_order_3 advmod_order_rather num_order_7 appos_LMs_order amod_LMs_higher-order advmod_one_just pobj_than_one advmod_than_rather appos_LMs_models conj_LMs_LMs dep_LMs_than num_LMs_two conj_negcc_tokens_7 num_tokens_10 dep_up_LMs prep_to_up_7 prep_to_up_tokens advmod_phrases_up advmod_phrases_longer conj_negcc_tokens_40 num_tokens_100 pobj_up_phrases prep_to_up_40 prep_to_up_tokens advcl_-LRB-_up amod_sentences_longer dep_Unfortunately_sentences
W07-0731	P03-1021	o	The feature weights i are trained in concert with the LM weight via minimum error rate -LRB- MER -RRB- training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate appos_rate_MER nn_rate_error amod_rate_minimum nn_weight_LM det_weight_the prep_with_concert_weight prep_via_trained_training prep_in_trained_concert auxpass_trained_are nsubjpass_trained_i rcmod_weights_trained nn_weights_feature det_weights_The dep_``_weights
W07-0733	P03-1021	o	are combined in a log-linear model to obtainthescoreforthetranslationeforaninputsentence f score -LRB- e f -RRB- = exp summationdisplay i i hi -LRB- e f -RRB- -LRB- 1 -RRB- The weights of the components i are set by a discriminative training method on held-out development data -LRB- Och 2003 -RRB-	amod_Och_2003 dep_data_Och nn_data_development amod_data_held-out prep_on_method_data nn_method_training amod_method_discriminative det_method_a agent_set_method auxpass_set_are nsubjpass_set_weights dep_set_1 dep_set_f dep_components_i det_components_the prep_of_weights_components det_weights_The dep_f_e dep_e_hi dep_e_i dep_e_= dep_e_score nn_i_i nn_i_summationdisplay nn_summationdisplay_exp dep_e_f appos_score_e nn_f_obtainthescoreforthetranslationeforaninputsentence prep_to_model_f amod_model_log-linear det_model_a dep_combined_set prep_in_combined_model auxpass_combined_are
W07-0734	P03-1021	p	Bleu is fast and easy to run and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems -LRB- Och 2003 -RRB-	amod_Och_2003 dep_systems_Och nn_systems_MT amod_systems_statistical amod_systems_state-of-the-art prep_in_used_systems advmod_used_commonly auxpass_used_are nsubjpass_used_that rcmod_procedures_used nn_procedures_training nn_procedures_optimization nn_procedures_parameter prep_in_function_procedures nn_function_target det_function_a prep_as_used_function auxpass_used_be aux_used_can nsubjpass_used_it aux_run_to xcomp_easy_run nsubj_easy_Bleu conj_and_fast_used conj_and_fast_easy cop_fast_is nsubj_fast_Bleu
W07-0735	P03-1021	o	In all experiments word alignment was obtained using the grow-diag-final heuristic for symmetrizing GIZA + + -LRB- Och and Ney 2003 -RRB- alignments	appos_alignments_Ney appos_alignments_Och num_Och_2003 conj_and_Och_Ney pobj_+_alignments conj_+_GIZA_+ dep_symmetrizing_+ dep_symmetrizing_GIZA amod_heuristic_grow-diag-final det_heuristic_the prepc_for_using_symmetrizing dobj_using_heuristic xcomp_obtained_using auxpass_obtained_was nsubjpass_obtained_alignment prep_in_obtained_experiments nn_alignment_word det_experiments_all
W07-0735	P03-1021	o	3.1 Evaluation Measure and MERT We evaluate our experiments using the -LRB- lowercase tokenized -RRB- BLEU metric and estimate the empirical confidence using the bootstrapping method described in Koehn -LRB- 2004b -RRB- .6 We report the scores obtained on the test section with model parameters tuned using the tuning section for minimum error rate training -LRB- MERT -LRB- Och 2003 -RRB- -RRB-	dep_Och_2003 appos_MERT_Och dep_training_MERT nn_training_rate nn_training_error amod_training_minimum nn_section_tuning det_section_the prep_for_using_training dobj_using_section xcomp_tuned_using vmod_parameters_tuned nn_parameters_model nn_section_test det_section_the prep_with_obtained_parameters prep_on_obtained_section vmod_scores_obtained det_scores_the dobj_report_scores nsubj_report_We rcmod_.6_report dep_Koehn_.6 dep_Koehn_2004b prep_in_described_Koehn vmod_method_described nn_method_bootstrapping det_method_the dobj_using_method vmod_confidence_using amod_confidence_empirical det_confidence_the dobj_estimate_confidence amod_BLEU_metric conj_and_tokenized_estimate dep_tokenized_BLEU amod_tokenized_lowercase dep_the_estimate dep_the_tokenized dobj_using_the poss_experiments_our xcomp_evaluate_using dobj_evaluate_experiments nsubj_evaluate_We rcmod_Measure_evaluate conj_and_Measure_MERT nn_Measure_Evaluation num_Measure_3.1
W08-0127	P03-1021	o	We also plan to employ this evaluation metric as feedback in building dialogue coherence models as is done in machine translation -LRB- Och 2003 -RRB-	amod_Och_2003 dep_translation_Och nn_translation_machine prep_in_done_translation auxpass_done_is mark_done_as nn_models_coherence nn_models_dialogue nn_models_building prep_in_feedback_models nn_metric_evaluation det_metric_this advcl_employ_done prep_as_employ_feedback dobj_employ_metric aux_employ_to xcomp_plan_employ advmod_plan_also nsubj_plan_We
W08-0302	P03-1021	o	Baseline We use the Moses MT system -LRB- Koehn et al. 2007 -RRB- as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks .4 In particular we perform word alignment in each direction using GIZA + + -LRB- Och and Ney 2003 -RRB- apply the grow-diag-finaland heuristic for symmetrization and use a maximum phrase length of 7	prep_of_length_7 nn_length_phrase nn_length_maximum det_length_a dobj_use_length nsubj_use_GIZA prep_for_heuristic_symmetrization amod_heuristic_grow-diag-finaland det_heuristic_the conj_and_apply_use dobj_apply_heuristic nsubj_apply_+ nsubj_apply_GIZA num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ ccomp_using_use ccomp_using_apply det_direction_each nn_alignment_word vmod_perform_using prep_in_perform_direction dobj_perform_alignment nsubj_perform_we num_tasks_.4 parataxis_shared_perform prep_in_shared_particular dobj_shared_tasks nsubj_shared_follow nsubj_shared_Baseline conj_and_WMT-07_WMT-08 det_WMT-07_the prep_for_given_WMT-08 prep_for_given_WMT-07 vmod_procedure_given nn_procedure_training nn_procedure_example det_procedure_the dobj_follow_procedure advmod_follow_closely det_baseline_a amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et nn_system_MT nn_system_Moses det_system_the dobj_use_system nsubj_use_We conj_and_Baseline_follow prep_as_Baseline_baseline dep_Baseline_Koehn rcmod_Baseline_use
W08-0302	P03-1021	o	Minimum error-rate -LRB- MER -RRB- training -LRB- Och 2003 -RRB- was applied to obtain weights -LRB- m in Equation 2 -RRB- for these features	det_features_these num_Equation_2 prep_in_m_Equation appos_weights_m prep_for_obtain_features dobj_obtain_weights aux_obtain_to xcomp_applied_obtain auxpass_applied_was nsubjpass_applied_training dep_Och_2003 appos_training_Och appos_training_MER amod_training_error-rate nn_training_Minimum
W08-0302	P03-1021	o	The weights 1 M are typically learned to directly minimize a standard evaluation criterion on development data -LRB- e.g. the BLEU score Papineni et al. -LRB- 2002 -RRB- -RRB- using numerical search -LRB- Och 2003 -RRB-	amod_Och_2003 dep_search_Och amod_search_numerical dobj_using_search nn_al._et nn_al._Papineni nn_score_BLEU det_score_the appos_e.g._2002 dep_e.g._al. appos_e.g._score dep_data_e.g. nn_data_development prep_on_criterion_data nn_criterion_evaluation amod_criterion_standard det_criterion_a dobj_minimize_criterion advmod_minimize_directly aux_minimize_to xcomp_learned_using xcomp_learned_minimize advmod_learned_typically auxpass_learned_are nsubjpass_learned_M rcmod_weights_learned num_weights_1 det_weights_The dep_``_weights
W08-0302	P03-1021	o	The mixture coefficients are trained in the usual way -LRB- minimum error-rate training Och 2003 -RRB- so that the additional context is exploited when it is useful and ignored when it isnt The paper proceeds as follows	mark_follows_as advcl_proceeds_follows nsubj_proceeds_paper det_paper_The nsubj_isnt_it advmod_isnt_when nsubj_ignored_it advcl_useful_isnt conj_and_useful_ignored cop_useful_is nsubj_useful_it advmod_useful_when advcl_exploited_ignored advcl_exploited_useful auxpass_exploited_is nsubjpass_exploited_context mark_exploited_that advmod_exploited_so amod_context_additional det_context_the dep_training_2003 appos_training_Och amod_training_error-rate nn_training_minimum dep_way_training amod_way_usual det_way_the parataxis_trained_proceeds advcl_trained_exploited prep_in_trained_way auxpass_trained_are nsubjpass_trained_coefficients nn_coefficients_mixture det_coefficients_The
W08-0302	P03-1021	o	To combine the many differently-conditioned features into a single model we provide them as features to the linear model -LRB- Equation 2 -RRB- and use minimum error-rate training -LRB- Och 2003 -RRB- to obtain interpolation weights m This is similar to an interpolation of backed-off estimates if we imagine that all of the different contextsaredifferently-backedoffestimatesofthe complete context	amod_context_complete nn_context_contextsaredifferently-backedoffestimatesofthe amod_context_different det_context_the prep_of_all_context dep_that_all advmod_imagine_that nsubj_imagine_we mark_imagine_if amod_estimates_backed-off prep_of_interpolation_estimates det_interpolation_an advcl_similar_imagine prep_to_similar_interpolation cop_similar_is nsubj_similar_This nn_m_weights nn_m_interpolation dobj_obtain_m aux_obtain_to dep_Och_2003 appos_training_Och amod_training_error-rate amod_training_minimum vmod_use_obtain dobj_use_training nsubj_use_we num_Equation_2 appos_model_Equation amod_model_linear det_model_the prep_to_features_model parataxis_provide_similar conj_and_provide_use prep_as_provide_features dobj_provide_them nsubj_provide_we advcl_provide_combine amod_model_single det_model_a amod_features_differently-conditioned amod_features_many det_features_the prep_into_combine_model dobj_combine_features aux_combine_To
W08-0304	P03-1021	o	Och -LRB- 2003 -RRB- claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective O = lscript	dep_=_lscript amod_O_= det_objective_the det_loss_the prep_as_using_objective dobj_using_loss advmod_using_directly advmod_using_when dep_obtained_O pcomp_obtained_using vmod_that_obtained amod_performance_equivalent advmod_equivalent_essentially prep_to_achieved_that dobj_achieved_performance nsubj_achieved_approximation mark_achieved_that det_approximation_this ccomp_claimed_achieved nsubj_claimed_Och appos_Och_2003
W08-0304	P03-1021	o	-LRB- 2003 -RRB- of running GIZA + + -LRB- Och & Ney 2003 -RRB- in both directions and then merging the alignments using the grow-diag-final heuristic	amod_heuristic_grow-diag-final det_heuristic_the dobj_using_heuristic det_alignments_the xcomp_merging_using dobj_merging_alignments advmod_merging_then preconj_directions_both dep_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ prep_in_running_directions dobj_running_+ dobj_running_GIZA conj_and_2003_merging prepc_of_2003_running dep_''_merging dep_''_2003
W08-0304	P03-1021	o	The first is a novel stochastic search strategy that appears to make better use of Och -LRB- 2003 -RRB- s algorithm for finding the global minimum along any given search direction than either coordinate descent or Powells method	nn_method_Powells conj_or_descent_method amod_descent_coordinate preconj_descent_either nn_direction_search amod_direction_given det_direction_any amod_minimum_global det_minimum_the prep_along_finding_direction dobj_finding_minimum prep_than_algorithm_method prep_than_algorithm_descent prepc_for_algorithm_finding amod_algorithm_s appos_Och_2003 prep_of_use_Och amod_use_better dobj_make_use aux_make_to dep_appears_algorithm xcomp_appears_make nsubj_appears_that rcmod_strategy_appears nn_strategy_search amod_strategy_stochastic amod_strategy_novel det_strategy_a cop_strategy_is nsubj_strategy_first det_first_The
W08-0304	P03-1021	o	However by exploiting the fact that the underlying scores assigned to competing hypotheses w -LRB- e h f -RRB- vary linearly w.r.t. changes in the weight vector w Och -LRB- 2003 -RRB- proposed a strategy for finding the global minimum along any given search direction	nn_direction_search amod_direction_given det_direction_any amod_minimum_global det_minimum_the prep_along_finding_direction dobj_finding_minimum prepc_for_strategy_finding det_strategy_a dobj_proposed_strategy nsubj_proposed_changes appos_Och_2003 appos_vector_Och appos_vector_w nn_vector_weight det_vector_the prep_in_changes_vector amod_changes_w.r.t. advmod_changes_linearly ccomp_vary_proposed nsubj_vary_scores mark_vary_that dep_h_f dep_h_e dep_w_h amod_hypotheses_competing prep_to_assigned_hypotheses appos_scores_w vmod_scores_assigned amod_scores_underlying det_scores_the ccomp_fact_vary det_fact_the dobj_exploiting_fact pcomp_by_exploiting ccomp_,_by dep_``_However
W08-0304	P03-1021	o	1 Introduction Och -LRB- 2003 -RRB- introduced minimum error rate training -LRB- MERT -RRB- as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models -LRB- Och & Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_models_Ney dep_models_Och nn_models_translation amod_models_log-linear prep_with_used_models advmod_used_previously vmod_objective_used nn_objective_likelihood amod_objective_conditional det_objective_the nn_regime_training amod_regime_alternative det_regime_an prep_to_training_objective prep_as_training_regime appos_training_MERT nn_training_rate nn_training_error amod_training_minimum amod_training_introduced dep_training_Och appos_Och_2003 nn_Och_Introduction num_Och_1
W08-0304	P03-1021	o	This is seen in that each time we check for the nearest intersection to the current 1-best for some n-best list l we Algorithm 1 Och -LRB- 2003 -RRB- s line search method to find the global minimum in the loss lscript when starting at the point w and searching along the direction d using the candidate translations given in the collection of n-best lists L. Input L w d lscript I -LCB- -RCB- for l L do for e l do m -LCB- e -RCB- e.features d b -LCB- e -RCB- e.features w end for bestn argmaxel m -LCB- e -RCB- -LCB- b -LCB- e -RCB- breaks ties -RCB- loop bestn +1 = argminel max parenleftBig 0 b -LCB- bestn -RCB- b -LCB- e -RCB- m -LCB- e -RCB- m -LCB- bestn -RCB- parenrightBig intercept max parenleftBig 0 b -LCB- bestn -RCB- b -LCB- bestn +1 -RCB- m -LCB- bestn +1 -RCB- m -LCB- bestn -RCB- parenrightBig if intercept > 0 then add -LRB- I intercept -RRB- else break end if end loop end for add -LRB- I max -LRB- I -RRB- +2 epsilon1 -RRB- ibest = argminiI evallscript -LRB- L w + -LRB- iepsilon1 -RRB- d -RRB- return w + -LRB- ibest epsilon1 -RRB- d must calculate its intersection with all other candidate translations that have yet to be selected as the 1-best	det_1-best_the prep_as_selected_1-best auxpass_selected_be aux_selected_to xcomp_have_selected advmod_have_yet nsubj_have_that rcmod_translations_have nn_translations_candidate amod_translations_other det_translations_all poss_intersection_its prep_with_calculate_translations dobj_calculate_intersection aux_calculate_must nsubj_calculate_ibest nsubj_calculate_max dep_calculate_I dep_d_epsilon1 amod_epsilon1_ibest conj_+_w_d nn_w_return nn_w_evallscript dep_d_iepsilon1 conj_+_L_d conj_+_L_w dep_evallscript_d dep_evallscript_w dep_evallscript_L nn_evallscript_argminiI dobj_=_d dobj_=_w amod_ibest_= num_epsilon1_+2 dep_max_epsilon1 appos_max_I dep_add_calculate prep_for_end_add nn_end_loop nn_end_end prep_if_break_end dobj_break_end advmod_break_else dep_I_intercept dep_add_break dobj_add_I dep_add_b nsubj_add_m mark_add_for dep_add_end dep_add_w dep_add_e.features nsubj_add_b dep_add_e.features dep_add_m aux_add_do nsubj_add_l mark_add_for quantmod_0_> dobj_intercept_0 mark_intercept_if nn_parenrightBig_m nn_parenrightBig_m appos_m_bestn num_bestn_+1 appos_m_bestn nn_m_b num_bestn_+1 nn_bestn_b nn_bestn_bestn appos_b_bestn num_parenleftBig_0 nn_parenleftBig_max dobj_intercept_parenleftBig nsubj_intercept_parenrightBig nn_parenrightBig_bestn appos_m_parenrightBig rcmod_m_intercept nn_m_m appos_m_e tmod_b_m dep_b_e nn_b_bestn nn_b_b appos_parenleftBig_b num_parenleftBig_0 nn_parenleftBig_max nn_parenleftBig_argminel dobj_=_parenleftBig dep_+1_= nn_+1_bestn nn_+1_loop nn_+1_ties nn_+1_breaks dep_b_then dep_b_intercept dep_b_+1 appos_b_e appos_m_e nn_m_argmaxel nn_m_bestn dep_e.features_e nn_b_d appos_m_e dep_l_e advcl_do_add prep_for_do_L dep_do_I dep_do_lscript nsubj_do_L nn_L_l conj_L_d conj_L_w parataxis_Input_do nn_Input_L. dep_lists_Input amod_lists_n-best prep_of_collection_lists det_collection_the prep_in_given_collection vmod_translations_given nn_translations_candidate det_translations_the dobj_using_translations nn_d_direction det_d_the prep_along_searching_d nn_w_point det_w_the xcomp_starting_using conj_and_starting_searching prep_at_starting_w advmod_starting_when det_loss_the appos_minimum_lscript prep_in_minimum_loss amod_minimum_global det_minimum_the advcl_find_searching advcl_find_starting dobj_find_minimum aux_find_to xcomp_method_find nn_method_search nn_method_line dep_method_s nsubj_method_we nsubj_s_Och appos_Och_2003 num_Och_1 nn_Och_Algorithm dep_list_l amod_list_n-best det_list_some amod_1-best_current det_1-best_the prep_to_intersection_1-best amod_intersection_nearest det_intersection_the parataxis_check_method prep_for_check_list prep_for_check_intersection nsubj_check_we det_time_each det_time_that parataxis_seen_check prep_in_seen_time auxpass_seen_is nsubjpass_seen_This ccomp_``_seen
W08-0304	P03-1021	o	The first Powells method was advocated by Och -LRB- 2003 -RRB- when MERT was first introduced for statistical machine translation	nn_translation_machine amod_translation_statistical prep_for_introduced_translation advmod_introduced_first auxpass_introduced_was nsubjpass_introduced_MERT advmod_introduced_when appos_Och_2003 advcl_advocated_introduced agent_advocated_Och auxpass_advocated_was nsubjpass_advocated_method nn_method_Powells amod_method_first det_method_The
W08-0304	P03-1021	p	While the former is piecewise constant and thus can not be optimized using gradient techniques Och -LRB- 2003 -RRB- provides an approach that performs such training efficiently	advmod_training_efficiently amod_training_such dobj_performs_training nsubj_performs_that rcmod_approach_performs det_approach_an dobj_provides_approach nsubj_provides_Och appos_Och_2003 rcmod_techniques_provides nn_techniques_gradient dobj_using_techniques xcomp_optimized_using auxpass_optimized_be neg_optimized_not aux_optimized_can advmod_optimized_thus nsubjpass_optimized_former conj_and_constant_optimized amod_constant_piecewise cop_constant_is nsubj_constant_former mark_constant_While det_former_the advcl_``_optimized advcl_``_constant
W08-0305	P03-1021	o	The de-facto answer came during the 1990s from the research community on Statistical Machine Translation who made use of statistical tools based on a noisy channel model originally developed for speech recognition -LRB- Brown et al. 1994 Och and Weber 1998 R.Zens et al. 2002 Och and Ney 2001 Koehn et al. 2003 -RRB-	nn_al._et nn_al._Koehn num_Och_2001 conj_and_Och_Ney nn_al._et nn_al._R.Zens num_Och_1998 conj_and_Och_Weber dep_al._2003 dep_al._al. dep_al._Ney dep_al._Och num_al._2002 dep_al._al. dep_al._Weber dep_al._Och num_al._1994 nn_al._et amod_al._Brown nn_recognition_speech dep_developed_al. prep_for_developed_recognition advmod_developed_originally nsubj_developed_model mark_developed_on nn_model_channel amod_model_noisy det_model_a pcomp_based_developed amod_tools_statistical prep_of_use_tools prep_made_based dobj_made_use nsubj_made_who rcmod_Translation_made nn_Translation_Machine amod_Translation_Statistical nn_community_research det_community_the det_1990s_the prep_on_came_Translation prep_from_came_community prep_during_came_1990s nsubj_came_answer amod_answer_de-facto det_answer_The
W08-0305	P03-1021	o	These models can be tuned using minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training xcomp_tuned_using auxpass_tuned_be aux_tuned_can nsubjpass_tuned_models det_models_These
W08-0305	P03-1021	o	Moses uses standard external tools for some of these tasks such as GIZA + + -LRB- Och and Ney 2003 -RRB- for word alignments and SRILM -LRB- Stolcke 2002 -RRB- for language modeling	nn_modeling_language amod_Stolcke_2002 appos_SRILM_Stolcke conj_and_alignments_SRILM nn_alignments_word num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_for_GIZA_modeling prep_for_GIZA_SRILM prep_for_GIZA_alignments conj_+_GIZA_+ det_tasks_these prep_such_as_some_+ prep_such_as_some_GIZA prep_of_some_tasks amod_tools_external amod_tools_standard prep_for_uses_some dobj_uses_tools nsubj_uses_Moses
W08-0306	P03-1021	o	We show that link 1For a complete discussion of alignment symmetrization heuristics including union intersection and refined refer to -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_to_Ney dep_to_Och prep_refer_to conj_and_union_refined conj_and_union_intersection prep_including_heuristics_refined prep_including_heuristics_intersection prep_including_heuristics_union nn_heuristics_symmetrization nn_heuristics_alignment vmod_discussion_refer prep_of_discussion_heuristics amod_discussion_complete det_discussion_a dep_1For_discussion nn_1For_link prep_that_show_1For nsubj_show_We
W08-0306	P03-1021	o	GIZA + + -LRB- Och and Ney 2003 -RRB- an implementation of the IBM -LRB- Brown et al. 1993 -RRB- and HMM -LRB- ? -RRB-	dep_HMM_-LRB- amod_Brown_1993 dep_Brown_al. nn_Brown_et det_IBM_the dep_implementation_Brown prep_of_implementation_IBM det_implementation_an num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_HMM conj_+_GIZA_implementation conj_+_GIZA_+
W08-0306	P03-1021	o	After maximum BLEU tuning -LRB- Och 2003a -RRB- on a held-out tuning set we evaluate translation quality on a held-out test set	nn_set_test amod_set_held-out det_set_a nn_quality_translation prep_on_evaluate_set dobj_evaluate_quality nsubj_evaluate_we prep_after_evaluate_tuning nn_set_tuning amod_set_held-out det_set_a appos_Och_2003a prep_on_tuning_set dep_tuning_Och nn_tuning_BLEU amod_tuning_maximum
W08-0306	P03-1021	n	3.2 Evaluation Metrics AER -LRB- Alignment Error Rate -RRB- -LRB- Och and Ney 2003 -RRB- is the most widely used metric of alignment quality but requires gold-standard alignments labelled with sure/possible annotations to compute lacking such annotations we can compute alignment fmeasure instead	nn_fmeasure_alignment advmod_compute_instead dobj_compute_fmeasure aux_compute_can nsubj_compute_we amod_annotations_such dobj_lacking_annotations aux_compute_to amod_annotations_sure/possible xcomp_labelled_compute prep_with_labelled_annotations vmod_alignments_labelled amod_alignments_gold-standard dobj_requires_alignments nsubj_requires_the nn_quality_alignment prep_of_metric_quality dobj_used_metric advmod_used_widely advmod_widely_most vmod_the_used conj_but_is_requires nsubj_is_the dep_Och_2003 conj_and_Och_Ney nn_Rate_Error nn_Rate_Alignment dep_AER_compute dep_AER_lacking dep_AER_requires dep_AER_is appos_AER_Ney appos_AER_Och appos_AER_Rate nn_AER_Metrics nn_AER_Evaluation num_AER_3.2 dep_``_AER
W08-0306	P03-1021	o	GIZA + + refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as -LRB- Och 2004 -RRB- variations on the refined heuristic have been used by -LRB- Koehn et al. 2003 -RRB- -LRB- diag and diag-and -RRB- and by the phrase-based system Moses -LRB- grow-diag-final -RRB- -LRB- Koehn et al. 2007 -RRB-	amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_Moses_Koehn dep_Moses_grow-diag-final dep_system_Moses amod_system_phrase-based det_system_the conj_and_diag_diag-and conj_and_Koehn_system dep_Koehn_diag-and dep_Koehn_diag dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et agent_used_system agent_used_Koehn auxpass_used_been aux_used_have nsubjpass_used_variations amod_heuristic_refined det_heuristic_the prep_on_variations_heuristic amod_Och_2004 dep_as_Och mwe_as_such prep_systems_as nn_systems_MT amod_systems_statistical amod_systems_phrase-based amod_systems_state-of-the-art parataxis_used_used prep_in_used_systems auxpass_used_been aux_used_have nsubjpass_used_+ nsubjpass_used_GIZA amod_alignments_refined pobj_+_alignments conj_+_GIZA_+
W08-0306	P03-1021	o	The feature weights are tuned using minimum error rate training -LRB- Och and Ney 2003 -RRB- to optimize BLEU score on a held-out development set	nn_set_development amod_set_held-out det_set_a nn_score_BLEU prep_on_optimize_set dobj_optimize_score aux_optimize_to dep_Och_2003 conj_and_Och_Ney appos_training_Ney appos_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_using_optimize dobj_using_training xcomp_tuned_using auxpass_tuned_are nsubjpass_tuned_weights nn_weights_feature det_weights_The ccomp_``_tuned
W08-0309	P03-1021	o	The word alignments were created with Giza + + -LRB- Och and Ney 2003 -RRB- applied to a parallel corpus containing the complete Europarl training data plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations and the test sentences paired with each of the system translations	nn_translations_system det_translations_the prep_of_each_translations prep_with_paired_each vmod_sentences_paired nn_sentences_test det_sentences_the nn_translations_reference det_translations_the nn_sentences_test det_sentences_the prep_with_pairing_translations dobj_pairing_sentences agent_created_pairing vmod_pairs_created nn_pairs_sentence num_pairs_4,051 prep_of_sets_pairs conj_plus_data_sets nn_data_training nn_data_Europarl amod_data_complete det_data_the dobj_containing_sets dobj_containing_data vmod_corpus_containing amod_corpus_parallel det_corpus_a prep_to_applied_corpus vmod_Och_applied num_Och_2003 conj_and_Och_Ney pobj_+_Ney pobj_+_Och conj_and_Giza_sentences conj_+_Giza_+ prep_with_created_sentences prep_with_created_+ prep_with_created_Giza auxpass_created_were nsubjpass_created_alignments nn_alignments_word det_alignments_The
W08-0309	P03-1021	o	A large database of human judgments might also be useful as an objective function for minimum error rate training -LRB- Och 2003 -RRB- or in other system development tasks	nn_tasks_development nn_tasks_system amod_tasks_other appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_in_function_tasks prep_for_function_training conj_or_function_function amod_function_objective det_function_an prep_as_useful_function prep_as_useful_function cop_useful_be advmod_useful_also aux_useful_might nsubj_useful_database amod_judgments_human prep_of_database_judgments amod_database_large det_database_A
W08-0310	P03-1021	o	translation systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- and use Moses -LRB- Koehn et al. 2007 -RRB- to search for the best target sentence	nn_sentence_target amod_sentence_best det_sentence_the prep_for_search_sentence aux_search_to amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et appos_Moses_Koehn dobj_use_Moses num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney vmod_systems_search conj_and_systems_use dep_systems_Koehn dep_systems_2004 dep_systems_Ney dep_systems_Och nn_systems_translation
W08-0310	P03-1021	o	These fourteen scores are weighted and linearly combined -LRB- Och and Ney 2002 Och 2003 -RRB- their respective weights are learned on development data so as to maximize the BLEU score	nn_score_BLEU det_score_the dobj_maximize_score aux_maximize_to nn_data_development prepc_as_learned_maximize advmod_learned_so prep_on_learned_data auxpass_learned_are nsubjpass_learned_weights amod_weights_respective poss_weights_their dep_Och_2003 dep_Och_Och conj_and_Och_2002 conj_and_Och_Ney dep_combined_2002 dep_combined_Ney dep_combined_Och advmod_combined_linearly nsubj_combined_scores parataxis_weighted_learned conj_and_weighted_combined cop_weighted_are nsubj_weighted_scores num_scores_fourteen det_scores_These
W08-0312	P03-1021	p	Bleu is fast and easy to run and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems -LRB- Och 2003 -RRB-	amod_Och_2003 dep_systems_Och nn_systems_MT amod_systems_statistical amod_systems_state-of-the-art prep_in_used_systems advmod_used_commonly auxpass_used_are nsubjpass_used_that rcmod_procedures_used nn_procedures_training nn_procedures_optimization nn_procedures_parameter prep_in_function_procedures nn_function_target det_function_a prep_as_used_function auxpass_used_be aux_used_can nsubjpass_used_it aux_run_to xcomp_easy_run nsubj_easy_Bleu conj_and_fast_used conj_and_fast_easy cop_fast_is nsubj_fast_Bleu
W08-0316	P03-1021	o	Word alignments were generated using GIZA + + -LRB- Och and Ney 2003 -RRB- over a stemmed version of the parallel text	amod_text_parallel det_text_the prep_of_version_text amod_version_stemmed det_version_a num_Och_2003 conj_and_Och_Ney pobj_+_Ney pobj_+_Och prep_over_GIZA_version conj_+_GIZA_+ dobj_using_+ dobj_using_GIZA xcomp_generated_using auxpass_generated_were nsubjpass_generated_alignments nn_alignments_Word
W08-0316	P03-1021	o	3.1 System Tuning Minimum error training -LRB- Och 2003 -RRB- under BLEU -LRB- Papineni et al. 2001 -RRB- was used to optimise the feature weights of the decoder with respect to the dev2006 development set	nn_set_development amod_set_dev2006 det_set_the det_decoder_the prep_of_weights_decoder nn_weights_feature det_weights_the prep_with_respect_to_optimise_set dobj_optimise_weights aux_optimise_to xcomp_used_optimise auxpass_used_was nsubjpass_used_training amod_Papineni_2001 dep_Papineni_al. nn_Papineni_et dep_Och_2003 dep_training_Papineni prep_under_training_BLEU appos_training_Och nn_training_error nn_training_Minimum nn_training_Tuning nn_training_System num_training_3.1
W08-0319	P03-1021	o	We use the minimum-error rate training procedure by Och -LRB- 2003 -RRB- as implemented in the Moses toolkit to set the weights of the various translation and language models optimizing for BLEU	prep_for_optimizing_BLEU nn_models_language conj_and_translation_models amod_translation_various det_translation_the prep_of_weights_models prep_of_weights_translation det_weights_the vmod_set_optimizing dobj_set_weights aux_set_to nn_toolkit_Moses det_toolkit_the xcomp_implemented_set prep_in_implemented_toolkit mark_implemented_as appos_Och_2003 nn_procedure_training nn_procedure_rate amod_procedure_minimum-error det_procedure_the advcl_use_implemented prep_by_use_Och dobj_use_procedure nsubj_use_We
W08-0320	P03-1021	o	We set the feature weights by optimizing the Bleu score directly using minimum error rate training -LRB- Och 2003 -RRB- on the development set	nn_set_development det_set_the appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_on_using_set dobj_using_training advmod_using_directly nn_score_Bleu det_score_the vmod_optimizing_using dobj_optimizing_score nn_weights_feature det_weights_the prepc_by_set_optimizing dobj_set_weights nsubj_set_We
W08-0321	P03-1021	o	Following the guidelines of the workshop we built baseline systems using the lower-cased Europarl parallel corpus -LRB- restricting sentence length to 40 words -RRB- GIZA + + -LRB- Och and Ney 2003 -RRB- Moses -LRB- Koehn et al. 2007 -RRB- and the SRI LM toolkit -LRB- Stolcke 2002 -RRB- to build 5-gram LMs	nn_LMs_5-gram dobj_build_LMs aux_build_to amod_Stolcke_2002 vmod_toolkit_build dep_toolkit_Stolcke nn_toolkit_LM nn_toolkit_SRI det_toolkit_the amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_toolkit dep_GIZA_Koehn conj_+_GIZA_Moses conj_+_GIZA_+ num_words_40 nn_length_sentence prep_to_restricting_words dobj_restricting_length nn_corpus_parallel nn_corpus_Europarl amod_corpus_lower-cased det_corpus_the dobj_using_toolkit dobj_using_Moses dobj_using_+ dobj_using_GIZA dep_using_restricting dobj_using_corpus nn_systems_baseline vmod_built_using dobj_built_systems nsubj_built_we prep_following_built_guidelines det_workshop_the prep_of_guidelines_workshop det_guidelines_the ccomp_``_built
W08-0321	P03-1021	o	Instead of interpolating the two language models we explicitly used them in the decoder and optimized their weights via minimumerror-rate -LRB- MER -RRB- training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_minimumerror-rate appos_minimumerror-rate_MER poss_weights_their prep_via_optimized_training dobj_optimized_weights nsubj_optimized_we det_decoder_the conj_and_used_optimized prep_in_used_decoder dobj_used_them advmod_used_explicitly nsubj_used_we prepc_instead_of_used_interpolating nn_models_language num_models_two det_models_the dobj_interpolating_models
W08-0321	P03-1021	o	For example in IBM Model 1 the lexicon probability of source word f given target word e is calculated as -LRB- Och and Ney 2003 -RRB- p -LRB- f | e -RRB- = summationtext k c -LRB- f | e e k fk -RRB- summationtext k f c -LRB- f | e e k fk -RRB- -LRB- 1 -RRB- c -LRB- f | e ek fk -RRB- = summationdisplay ek fk P -LRB- ek fk -RRB- summationdisplay a P -LRB- a | ek fk -RRB- -LRB- 2 -RRB- summationdisplay j -LRB- f fkj -RRB- -LRB- e ekaj -RRB- Therefore the distribution of P -LRB- ek fk -RRB- will affect the alignment results	det_alignment_the dep_affect_results dobj_affect_alignment aux_affect_will nsubj_affect_distribution appos_ek_fk dep_P_ek prep_of_distribution_P det_distribution_the dep_Therefore_e dep_e_ekaj nn_fkj_f dep_j_fkj nn_j_summationdisplay dep_j_2 appos_ek_fk nn_ek_| det_ek_a dep_P_Therefore dep_P_j dep_P_ek det_P_a dep_summationdisplay_P appos_ek_fk dep_P_summationdisplay dep_P_ek nn_P_fk rcmod_ek_affect appos_ek_P nn_ek_summationdisplay dobj_=_ek appos_ek_fk dep_|_ek dep_|_e dep_|_f dep_c_= dep_c_| dep_1_c nn_fk_k dep_fk_e dep_|_e dep_|_f dep_c_| nn_c_f dep_k_1 parataxis_k_fk appos_k_c nn_k_summationtext nn_k_fk nn_k_k dep_k_e dep_|_e dep_|_f dep_c_| nn_c_k nn_c_summationtext dep_=_c dep_=_e dep_=_p advmod_e_| nn_|_f dep_Och_2003 conj_and_Och_Ney dep_as_Ney dep_as_Och parataxis_calculated_k parataxis_calculated_= prep_calculated_as auxpass_calculated_is prep_in_calculated_IBM prep_for_calculated_example dep_word_e nn_word_target pobj_given_word prep_f_given dep_word_f amod_source_word prep_of_probability_source nn_probability_lexicon det_probability_the num_probability_1 dep_Model_probability appos_IBM_Model
W08-0326	P03-1021	o	For example our system configuration for the shared task incorporates a wrapper around GIZA + + -LRB- Och and Ney 2003 -RRB- for word alignment and a wrapper around Moses -LRB- Koehn et al. 2007 -RRB- for decoding	amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et prep_around_wrapper_Moses det_wrapper_a nn_alignment_word num_Och_2003 conj_and_Och_Ney prep_for_+_alignment appos_+_Ney appos_+_Och conj_and_GIZA_wrapper conj_+_GIZA_+ dep_wrapper_Koehn prep_around_wrapper_wrapper prep_around_wrapper_+ prep_around_wrapper_GIZA det_wrapper_a prepc_for_incorporates_decoding dobj_incorporates_wrapper nsubj_incorporates_configuration prep_for_incorporates_example amod_task_shared det_task_the prep_for_configuration_task nn_configuration_system poss_configuration_our
W08-0326	P03-1021	o	Assuming that the parameters P -LRB- etk | fsk -RRB- are known the most likely alignment is computed by a simple dynamic-programming algorithm .1 Instead of using an Expectation-Maximization algorithm to estimate these parameters as commonly done when performing word alignment -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- we directly compute these parameters by relying on the information contained within the chunks	det_chunks_the prep_within_contained_chunks nsubj_contained_parameters mark_contained_that det_information_the prep_on_relying_information det_parameters_these prepc_by_compute_relying dobj_compute_parameters advmod_compute_directly nsubj_compute_we num_Och_2003 conj_and_Och_Ney dep_al._Ney dep_al._Och num_al._1993 nn_al._et amod_al._Brown dep_alignment_al. nn_alignment_word dobj_performing_alignment advmod_performing_when advcl_done_performing advmod_done_commonly mark_done_as det_parameters_these dobj_estimate_parameters aux_estimate_to nn_algorithm_Expectation-Maximization det_algorithm_an vmod_using_estimate dobj_using_algorithm num_algorithm_.1 amod_algorithm_dynamic-programming amod_algorithm_simple det_algorithm_a advcl_computed_done prepc_instead_of_computed_using agent_computed_algorithm auxpass_computed_is nsubjpass_computed_alignment amod_alignment_likely det_alignment_the advmod_likely_most auxpass_known_are nsubjpass_known_P num_fsk_| nn_fsk_etk appos_P_fsk rcmod_parameters_compute rcmod_parameters_computed rcmod_parameters_known det_parameters_the ccomp_Assuming_contained ccomp_``_Assuming
W08-0326	P03-1021	o	We tuned our system on the development set devtest2006 for the EuroParl tasks and on nc-test2007 for CzechEnglish using minimum error-rate training -LRB- Och 2003 -RRB- to optimise BLEU score	nn_score_BLEU dobj_optimise_score aux_optimise_to dep_Och_2003 appos_training_Och amod_training_error-rate amod_training_minimum vmod_using_optimise dobj_using_training prep_for_nc-test2007_CzechEnglish nn_tasks_EuroParl det_tasks_the prep_for_devtest2006_tasks amod_devtest2006_set conj_and_development_nc-test2007 dep_development_devtest2006 det_development_the poss_system_our vmod_tuned_using prep_on_tuned_nc-test2007 prep_on_tuned_development dobj_tuned_system nsubj_tuned_We ccomp_``_tuned
W08-0328	P03-1021	o	This set of 800 sentences was used for Minimum Error Rate Training -LRB- Och 2003 -RRB- to tune the weights of our system with respect to BLEU score	nn_score_BLEU poss_system_our prep_of_weights_system det_weights_the prep_with_respect_to_tune_score dobj_tune_weights aux_tune_to appos_Och_2003 dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum xcomp_used_tune prep_for_used_Training auxpass_used_was nsubjpass_used_set num_sentences_800 prep_of_set_sentences det_set_This
W08-0328	P03-1021	o	This setup provides an elegant solution to the fairly complex task of integrating multiple MT results that may differ in word order using only standard software modules in particular GIZA + + -LRB- Och and Ney 2003 -RRB- for the identification of building blocks and Moses for the recombination but the authors were not able to observe improvements in 1see http://www.statmt.org/moses/ terms of BLEU score	nn_score_BLEU prep_of_terms_score nn_terms_http://www.statmt.org/moses/ amod_terms_1see prep_in_improvements_terms dobj_observe_improvements aux_observe_to xcomp_able_observe neg_able_not cop_able_were nsubj_able_authors det_authors_the det_recombination_the conj_and_blocks_Moses nn_blocks_building prep_for_identification_recombination prep_of_identification_Moses prep_of_identification_blocks det_identification_the pobj_for_identification preconj_for_+ num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_for amod_GIZA_particular nn_modules_software amod_modules_standard advmod_modules_only dobj_using_modules nn_order_word xcomp_differ_using prep_in_differ_order aux_differ_may nsubj_differ_that conj_but_results_able prep_in_results_for prep_in_results_GIZA ccomp_results_differ amod_MT_multiple dobj_integrating_MT prepc_of_task_integrating amod_task_complex det_task_the advmod_complex_fairly amod_solution_elegant det_solution_an dep_provides_able dep_provides_results prep_to_provides_task dobj_provides_solution nsubj_provides_setup det_setup_This
W08-0334	P03-1021	o	Decoding Conditions For tuning of the decoder 's parameters minimum error training -LRB- Och 2003 -RRB- with respect to the BLEU score using was conducted using the respective development corpus	nn_corpus_development amod_corpus_respective det_corpus_the dobj_using_corpus xcomp_conducted_using auxpass_conducted_was nsubjpass_conducted_using nn_score_BLEU det_score_the num_Och_2003 rcmod_training_conducted prep_with_respect_to_training_score appos_training_Och nn_training_error amod_training_minimum poss_parameters_decoder det_decoder_the prep_of_tuning_parameters dep_Conditions_training prep_for_Conditions_tuning amod_Conditions_Decoding ccomp_``_Conditions
W08-0335	P03-1021	o	The feature weights were optimized against the BLEU scores -LRB- Och 2003 -RRB-	amod_Och_2003 dep_scores_Och nn_scores_BLEU det_scores_the prep_against_optimized_scores auxpass_optimized_were nsubjpass_optimized_weights nn_weights_feature det_weights_The
W08-0336	P03-1021	o	We build phrase translations by first acquiring bidirectional GIZA + + -LRB- Och and Ney 2003 -RRB- alignments and using Moses grow-diag alignment symmetrization heuristic .1 We set the maximum phrase length to a large value -LRB- 10 -RRB- because some segmenters described later in this paper will result in shorter 1In our experiments this heuristic consistently performed better than the default grow-diag-final	amod_default_grow-diag-final det_default_the prep_than_better_default advmod_performed_better advmod_performed_consistently nsubj_performed_heuristic advcl_performed_using advcl_performed_build det_heuristic_this poss_experiments_our nn_experiments_1In amod_experiments_shorter prep_in_result_experiments aux_result_will nsubj_result_segmenters mark_result_because det_paper_this prep_in_described_paper advmod_described_later vmod_segmenters_described det_segmenters_some appos_value_10 amod_value_large det_value_a nn_length_phrase nn_length_maximum det_length_the advcl_set_result prep_to_set_value dobj_set_length nsubj_set_We num_heuristic_.1 nn_heuristic_symmetrization nn_heuristic_alignment nn_heuristic_grow-diag nn_heuristic_Moses ccomp_using_set dobj_using_heuristic nsubj_using_We appos_alignments_Ney appos_alignments_Och num_Och_2003 conj_and_Och_Ney pobj_+_alignments conj_+_GIZA_+ amod_GIZA_bidirectional dobj_acquiring_+ dobj_acquiring_GIZA dep_first_acquiring nn_translations_phrase conj_and_build_using prep_by_build_first dobj_build_translations nsubj_build_We
W08-0336	P03-1021	o	We tuned the parameters of these features with Minimum Error Rate Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- on the NIST MT03 Evaluation data set -LRB- 919 sentences -RRB- and then test the MT performance on NIST MT03 and MT05 Evaluation data -LRB- 878 and 1082 sentences respectively -RRB-	advmod_878_respectively dep_878_sentences conj_and_878_1082 dep_data_1082 dep_data_878 nn_data_Evaluation nn_data_MT05 conj_and_MT03_data nn_MT03_NIST prep_on_performance_data prep_on_performance_MT03 nn_performance_MT det_performance_the dobj_test_performance advmod_test_then nsubj_test_We num_sentences_919 appos_set_sentences nn_set_data nn_set_Evaluation nn_set_MT03 nn_set_NIST det_set_the dep_Och_2003 appos_Training_Och appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum det_features_these prep_of_parameters_features det_parameters_the conj_and_tuned_test prep_on_tuned_set prep_with_tuned_Training dobj_tuned_parameters nsubj_tuned_We ccomp_``_test ccomp_``_tuned
W08-0401	P03-1021	o	For phrase-based translation model training we used the GIZA + + toolkit -LRB- Och et al. 2003 -RRB-	amod_Och_2003 dep_Och_al. nn_Och_et cc_toolkit_+ dep_GIZA_Och conj_+_GIZA_toolkit det_GIZA_the dobj_used_toolkit dobj_used_GIZA nsubj_used_we prep_for_used_training nn_training_model nn_training_translation amod_training_phrase-based
W08-0401	P03-1021	o	For tuning of decoder parameters we conducted minimum error training -LRB- Och 2003 -RRB- with respect to the BLEU score using 916 development sentence pairs	nn_pairs_sentence nn_pairs_development num_pairs_916 dobj_using_pairs nn_score_BLEU det_score_the num_Och_2003 appos_training_Och nn_training_error amod_training_minimum vmod_conducted_using prep_with_respect_to_conducted_score dobj_conducted_training nsubj_conducted_we prep_for_conducted_tuning nn_parameters_decoder prep_of_tuning_parameters
W08-0401	P03-1021	o	One of the popular statistical machine translation paradigms is the phrase-based model -LRB- PBSMT -RRB- -LRB- Marcu et al. 2002 Koehn et al. 2003 Och et al. 2004 -RRB-	num_Och_2004 nn_Och_al. nn_Och_et dep_Koehn_Och num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn appos_Marcu_2002 dep_Marcu_al. nn_Marcu_et dep_model_Marcu appos_model_PBSMT amod_model_phrase-based det_model_the cop_model_is nsubj_model_One nn_paradigms_translation nn_paradigms_machine amod_paradigms_statistical amod_paradigms_popular det_paradigms_the prep_of_One_paradigms
W08-0402	P03-1021	o	We use the GIZA toolkit -LRB- Och and Ney 2000 -RRB- a suffix-array architecture -LRB- Lopez 2007 -RRB- the SRILM toolkit -LRB- Stolcke 2002 -RRB- and minimum error rate training -LRB- Och et al. 2003 -RRB- to obtain wordalignments a translation model language models and the optimal weights for combining these models respectively	det_models_these dobj_combining_models prepc_for_weights_combining amod_weights_optimal det_weights_the nn_models_language nn_model_translation det_model_a conj_and_wordalignments_weights conj_and_wordalignments_models appos_wordalignments_model dobj_obtain_weights dobj_obtain_models dobj_obtain_wordalignments aux_obtain_to amod_Och_2003 dep_Och_al. nn_Och_et dep_training_Och nn_training_rate nn_training_error amod_training_minimum dep_Stolcke_2002 appos_toolkit_Stolcke nn_toolkit_SRILM det_toolkit_the amod_Lopez_2007 appos_architecture_Lopez amod_architecture_suffix-array det_architecture_a dep_Och_2000 conj_and_Och_Ney conj_and_toolkit_training appos_toolkit_toolkit appos_toolkit_architecture appos_toolkit_Ney appos_toolkit_Och nn_toolkit_GIZA det_toolkit_the advmod_use_respectively vmod_use_obtain dobj_use_training dobj_use_toolkit nsubj_use_We
W08-0402	P03-1021	o	Furthermore techniques such as iterative minimum errorrate training -LRB- Och et al. 2003 -RRB- as well as web-based MT services require the decoder to translate a large number of source-language sentences per unit time	nn_time_unit prep_per_sentences_time amod_sentences_source-language prep_of_number_sentences amod_number_large det_number_a dobj_translate_number aux_translate_to det_decoder_the xcomp_require_translate dobj_require_decoder nsubj_require_techniques advmod_require_Furthermore nn_services_MT amod_services_web-based appos_Och_2003 dep_Och_al. nn_Och_et conj_and_training_services dep_training_Och amod_training_errorrate amod_training_minimum amod_training_iterative prep_such_as_techniques_services prep_such_as_techniques_training
W08-0403	P03-1021	o	Minimum-error-rate training -LRB- Och 2003 -RRB- are conducted on dev-set to optimize feature weights maximizing the BLEU score up to 4grams and the obtained feature weights are blindly applied on the test-set	det_test-set_the prep_on_applied_test-set advmod_applied_blindly auxpass_applied_are nsubjpass_applied_weights nn_weights_feature amod_weights_obtained det_weights_the pobj_to_4grams pcomp_up_to nn_score_BLEU det_score_the prep_maximizing_up dobj_maximizing_score vmod_weights_maximizing nn_weights_feature dobj_optimize_weights aux_optimize_to conj_and_conducted_applied xcomp_conducted_optimize prep_on_conducted_dev-set auxpass_conducted_are nsubjpass_conducted_training dep_Och_2003 appos_training_Och amod_training_Minimum-error-rate
W08-0404	P03-1021	o	The decision rule was based on the standard loglinear interpolation of several models with weights tunedbyMERTonthedevelopmentset -LRB- Och ,2003 -RRB-	num_Och_,2003 appos_tunedbyMERTonthedevelopmentset_Och nn_tunedbyMERTonthedevelopmentset_weights amod_models_several prep_of_interpolation_models nn_interpolation_loglinear amod_interpolation_standard det_interpolation_the prep_with_based_tunedbyMERTonthedevelopmentset prep_on_based_interpolation auxpass_based_was nsubjpass_based_rule nn_rule_decision det_rule_The
W08-0404	P03-1021	n	While minimum error training -LRB- Och 2003 -RRB- has by now become a standard tool for interpolating a small number of aggregate scores it is not well suited for learning in high-dimensional feature spaces	nn_spaces_feature amod_spaces_high-dimensional prep_in_learning_spaces prepc_for_suited_learning advmod_suited_well neg_suited_not auxpass_suited_is nsubjpass_suited_it advcl_suited_become amod_scores_aggregate prep_of_number_scores amod_number_small det_number_a dobj_interpolating_number prepc_for_tool_interpolating amod_tool_standard det_tool_a xcomp_become_tool nsubj_become_training mark_become_While pobj_by_now prep_has_by appos_Och_2003 vmod_training_has dep_training_Och nn_training_error amod_training_minimum
W08-0409	P03-1021	o	4.3 Baselines 4.3.1 Word Alignment We used the GIZA + + implementation of IBM word alignment model 4 -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- for word alignment and the heuristics described in -LRB- Och and Ney 2003 -RRB- to derive the intersection and refined alignment	amod_alignment_refined conj_and_intersection_alignment det_intersection_the dobj_derive_alignment dobj_derive_intersection aux_derive_to num_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och xcomp_described_derive prep_described_in vmod_heuristics_described det_heuristics_the nn_alignment_word dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_alignment nn_model_word nn_model_IBM prep_of_implementation_model pobj_+_implementation appos_GIZA_Brown conj_+_GIZA_+ det_GIZA_the prep_for_used_alignment dobj_used_+ dobj_used_GIZA nsubj_used_We conj_and_Alignment_heuristics rcmod_Alignment_used nn_Alignment_Word num_Alignment_4.3.1 nn_Alignment_Baselines num_Alignment_4.3
W08-0409	P03-1021	o	73 ment and phrase-extraction heuristics described in -LRB- Koehn et al. 2003 -RRB- minimum-error-rate training -LRB- Och 2003 -RRB- a trigram language model with KneserNey smoothing trained with SRILM -LRB- Stolcke 2002 -RRB- on the English side of the training data and Moses -LRB- Koehn et al. 2007 -RRB- to decode	aux_decode_to amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_Moses_Koehn nn_data_training det_data_the prep_of_side_data amod_side_English det_side_the dep_Stolcke_2002 appos_SRILM_Stolcke prep_on_trained_side prep_with_trained_SRILM vmod_smoothing_trained nn_smoothing_KneserNey prep_with_model_smoothing nn_model_language nn_model_trigram det_model_a dep_Och_2003 appos_training_Och amod_training_minimum-error-rate vmod_Koehn_decode conj_and_Koehn_Moses conj_and_Koehn_model conj_and_Koehn_training amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Moses prep_in_described_model prep_in_described_training prep_in_described_Koehn amod_heuristics_phrase-extraction vmod_ment_described conj_and_ment_heuristics num_ment_73 dep_``_heuristics dep_``_ment
W08-0409	P03-1021	o	Slightly differently from -LRB- Och and Ney 2003 -RRB- we use possible alignments in computing recall	amod_recall_computing amod_alignments_possible prep_in_use_recall dobj_use_alignments nsubj_use_we prep_use_from num_Och_2003 conj_and_Och_Ney dep_from_Ney dep_from_Och advmod_from_differently advmod_differently_Slightly
W08-0409	P03-1021	o	Since manual word alignment is an ambiguous task we also explicitly allow for ambiguous alignments i.e. the links are marked as sure -LRB- S -RRB- or possible -LRB- P -RRB- -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_possible_P conj_or_sure_possible dep_sure_S dep_marked_Ney dep_marked_Och prep_as_marked_possible prep_as_marked_sure auxpass_marked_are nsubjpass_marked_links advmod_marked_i.e. det_links_the amod_alignments_ambiguous parataxis_allow_marked prep_for_allow_alignments advmod_allow_explicitly advmod_allow_also nsubj_allow_we advcl_allow_task amod_task_ambiguous det_task_an cop_task_is nsubj_task_alignment mark_task_Since nn_alignment_word amod_alignment_manual
W08-0510	P03-1021	o	These include scripts for creating alignments from a parallel corpus creating phrase tables and language models binarizing phrase tables scripts for weight optimization using MERT -LRB- Och 2003 -RRB- and testing scripts	nn_scripts_testing num_Och_2003 appos_MERT_Och dobj_using_MERT vmod_optimization_using nn_optimization_weight prep_for_scripts_optimization conj_and_tables_scripts conj_and_tables_scripts nn_tables_phrase amod_tables_binarizing nn_models_language conj_and_tables_models nn_tables_phrase conj_creating_scripts conj_creating_scripts conj_creating_tables dobj_creating_models dobj_creating_tables amod_corpus_parallel det_corpus_a dep_creating_creating prep_from_creating_corpus dobj_creating_alignments prepc_for_scripts_creating dobj_include_scripts nsubj_include_These ccomp_``_include
W08-0510	P03-1021	p	GIZA + + -LRB- Och and Ney 2003 -RRB- is a very popular system within SMT for creating word alignment from parallel corpus in fact the Moses training scripts uses it	dobj_uses_it nsubj_uses_scripts nn_scripts_training nn_scripts_Moses det_scripts_the amod_corpus_parallel nn_alignment_word prep_from_creating_corpus dobj_creating_alignment rcmod_system_uses prep_in_system_fact prepc_for_system_creating prep_within_system_SMT amod_system_popular det_system_a cop_system_is nsubj_system_+ nsubj_system_GIZA advmod_popular_very num_Ney_2003 conj_and_Och_Ney pobj_+_Ney pobj_+_Och conj_+_GIZA_+
W09-0404	P03-1021	o	However this may still be too expensive as part of an MT model that directly optimizes some performance measure e.g. minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum nn_measure_performance det_measure_some dobj_optimizes_measure advmod_optimizes_directly nsubj_optimizes_that rcmod_model_optimizes nn_model_MT det_model_an prep_of_part_model conj_expensive_training dep_expensive_e.g. prep_as_expensive_part advmod_expensive_too cop_expensive_be advmod_expensive_still aux_expensive_may nsubj_expensive_this advmod_expensive_However
W09-0405	P03-1021	o	Then we run GIZA + + -LRB- Och and Ney 2003 -RRB- on the corpus to obtain word alignments in both directions	det_directions_both nn_alignments_word prep_in_obtain_directions dobj_obtain_alignments aux_obtain_to det_corpus_the num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ xcomp_run_obtain dobj_run_+ dobj_run_GIZA nsubj_run_we advmod_run_Then
W09-0405	P03-1021	o	We use Minimal Error Rate Training -LRB- Och 2003 -RRB- to maximize BLEU on the complete development data	nn_data_development amod_data_complete det_data_the prep_on_maximize_data dobj_maximize_BLEU aux_maximize_to amod_Och_2003 appos_Training_Och nn_Training_Rate nn_Training_Error amod_Training_Minimal vmod_use_maximize dobj_use_Training nsubj_use_We
W09-0412	P03-1021	o	We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 -LRB- Brown et al. 1993 -RRB- combined them using the intersect + grow heuristic -LRB- Och and Ney 2003 -RRB- and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach -LRB- Och and Ney 2004 -RRB-	amod_Och_2004 conj_and_Och_Ney dep_approach_Ney dep_approach_Och nn_approach_template nn_approach_alignment det_approach_the dobj_using_approach num_length_7 nn_length_maximum vmod_pairs_using prep_of_pairs_length nn_pairs_translation amod_pairs_phrase-level amod_pairs_extracted conj_and_Och_2003 conj_and_Och_Ney nn_Och_heuristic dep_intersect_2003 dep_intersect_Ney dep_intersect_Och conj_+_intersect_grow det_intersect_the dobj_using_grow dobj_using_intersect xcomp_combined_using dobj_combined_them amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_IBM dobj_using_model nn_alignments_word amod_alignments_directed amod_alignments_Spanish-to-English conj_and_English-to-Spanish_alignments amod_English-to-Spanish_separate conj_and_built_pairs dep_built_combined dep_built_Brown vmod_built_using dobj_built_alignments dobj_built_English-to-Spanish advmod_built_then nsubj_built_We ccomp_``_pairs ccomp_``_built
W09-0412	P03-1021	o	We set all feature weights by optimizing Bleu -LRB- Papineni et al. 2002 -RRB- directly using minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- on the tuning part of the development set -LRB- dev-test2009a -RRB-	appos_set_dev-test2009a nn_set_development det_set_the prep_of_part_set nn_part_tuning det_part_the dep_Och_2003 appos_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum prep_on_using_part dobj_using_training advmod_using_directly amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni dobj_optimizing_Bleu nn_weights_feature det_weights_all dep_set_using prepc_by_set_optimizing dobj_set_weights nsubj_set_We
W09-0416	P03-1021	o	The features we used are as follows word posterior probability -LRB- Fiscus 1997 -RRB- 3 4-gram target language model word length penalty Null word length penalty Also we use MERT -LRB- Och 2003 -RRB- to tune the weights of confusion network	nn_network_confusion prep_of_weights_network det_weights_the dobj_tune_weights aux_tune_to dep_Och_2003 dep_MERT_Och vmod_use_tune dobj_use_MERT nsubj_use_we advmod_use_Also nn_penalty_length nn_penalty_word nn_penalty_Null nn_penalty_length nn_penalty_word conj_model_use conj_model_penalty conj_model_penalty nn_model_language nn_model_target amod_model_4-gram num_model_3 appos_Fiscus_1997 dep_probability_model dep_probability_Fiscus nn_probability_posterior nn_probability_word dep_:_probability mark_follows_as dep_are_follows nsubj_are_features nsubj_used_we rcmod_features_used det_features_The
W09-0417	P03-1021	o	2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- distributed with the Moses decoder using the development corpus -LRB- dev2009a -RRB-	appos_corpus_dev2009a nn_corpus_development det_corpus_the dobj_using_corpus nn_decoder_Moses det_decoder_the prep_with_distributed_decoder dep_Och_2003 appos_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum vmod_implementation_distributed prep_of_implementation_training det_implementation_the dobj_using_implementation xcomp_tuned_using auxpass_tuned_were nsubjpass_tuned_systems amod_systems_Moses-based det_systems_The vmod_procedure_using rcmod_procedure_tuned nn_procedure_Tuning num_procedure_2.6
W09-0418	P03-1021	o	4.1 Baseline Our baseline system is a fairly typical phrasebased machine translation system -LRB- Finch and Sumita 2008a -RRB- built within the framework of a feature-based exponential model containing the following features Table 1 Language Resources Corpus Train Dev Eval NC Spanish sentences 74K 2,001 2,007 words 2,048 K 49,116 56,081 vocab 61K 9,047 8,638 length 27.6 24.5 27.9 OOV -LRB- % -RRB- 5.2 / 2.9 1.4 / 0.9 English sentences 74K 2,001 2,007 words 1,795 K 46,524 49,693 vocab 47K 8,110 7,541 length 24.2 23.2 24.8 OOV -LRB- % -RRB- 5.2 / 2.9 1.2 / 0.9 perplexity 349 / 381 348 / 458 EP Spanish sentences 1,404 K 1,861 2,000 words 41,003 K 50,216 61,293 vocab 170K 7,422 8,251 length 29.2 27.0 30.6 OOV -LRB- % -RRB- 2.4 / 0.1 2.4 / 0.2 English sentences 1,404 K 1,861 2,000 words 39,354 K 48,663 59,145 vocab 121K 5,869 6,428 length 28.0 26.1 29.6 OOV -LRB- % -RRB- 1.8 / 0.1 1.9 / 0.1 perplexity 210 / 72 305 / 125 Table 2 Testset 2009 Corpus Test NC Spanish sentences 3,027 words 80,591 vocab 12,616 length 26.6 Source-target phrase translation probability Inverse phrase translation probability Source-target lexical weighting probability Inverse lexical weighting probability Phrase penalty Language model probability Lexical reordering probability Simple distance-based distortion model Word penalty For the training of the statistical models standard word alignment -LRB- GIZA + + -LRB- Och and Ney 2003 -RRB- -RRB- and language modeling -LRB- SRILM -LRB- Stolcke 2002 -RRB- -RRB- tools were used	auxpass_used_were nsubjpass_used_K dep_tools_SRILM nn_tools_modeling dep_Stolcke_2002 dep_SRILM_Stolcke nn_modeling_language num_Och_2003 conj_and_Och_Ney dep_+_Ney dep_+_Och conj_+_GIZA_+ conj_and_alignment_tools dep_alignment_+ dep_alignment_GIZA nn_alignment_word amod_alignment_standard dep_alignment_K num_alignment_39,354 amod_models_statistical det_models_the prep_of_training_models det_training_the prep_for_penalty_training nn_penalty_Word nn_penalty_model nn_penalty_distortion amod_penalty_distance-based amod_penalty_Simple nn_penalty_probability nn_penalty_reordering amod_penalty_Lexical nn_penalty_probability nn_penalty_model nn_penalty_Language nn_penalty_penalty nn_penalty_Phrase nn_penalty_probability nn_penalty_weighting amod_penalty_lexical amod_penalty_Inverse nn_penalty_probability nn_penalty_weighting amod_penalty_lexical amod_penalty_Source-target nn_penalty_probability nn_penalty_translation nn_penalty_phrase amod_penalty_Inverse nn_penalty_probability nn_penalty_translation nn_penalty_phrase nn_penalty_Source-target num_penalty_26.6 nn_penalty_length num_penalty_12,616 amod_penalty_vocab num_penalty_80,591 dep_words_penalty num_words_3,027 dep_sentences_words nn_sentences_Spanish nn_sentences_NC nn_sentences_Test nn_sentences_Corpus num_sentences_2009 nn_sentences_Testset num_Table_2 num_Table_125 number_305_72 num_perplexity_210 num_perplexity_0.1 number_1.9_0.1 num_OOV_1.8 appos_OOV_% num_OOV_29.6 num_OOV_26.1 number_26.1_28.0 dep_length_sentences dep_length_Table dep_length_305 dep_length_perplexity dep_length_1.9 dep_length_OOV dep_6,428_length dep_5,869_6,428 number_5,869_121K num_vocab_5,869 number_59,145_48,663 dep_K_vocab num_K_59,145 dep_words_tools dep_words_alignment num_words_2,000 number_2,000_1,861 dep_K_words rcmod_1,404_used dep_sentences_1,404 amod_sentences_English num_sentences_0.2 number_2.4_0.1 dep_OOV_sentences num_OOV_2.4 num_OOV_2.4 appos_OOV_% num_OOV_30.6 num_OOV_27.0 number_27.0_29.2 dep_length_OOV dep_8,251_length dep_7,422_8,251 dep_170K_7,422 nn_170K_vocab nn_170K_K num_170K_41,003 number_61,293_50,216 num_K_61,293 dep_words_170K num_words_2,000 number_2,000_1,861 dep_K_words dep_1,404_K dep_sentences_1,404 amod_sentences_Spanish nn_sentences_EP num_sentences_458 number_348_381 num_perplexity_349 num_perplexity_0.9 number_1.2_2.9 num_OOV_5.2 appos_OOV_% num_OOV_24.8 num_OOV_23.2 number_23.2_24.2 dep_length_sentences dep_length_348 dep_length_perplexity dep_length_1.2 dep_length_OOV dep_7,541_length dep_8,110_7,541 dep_47K_8,110 nn_47K_vocab nn_47K_K num_47K_1,795 number_49,693_46,524 num_K_49,693 dep_words_47K num_words_2,007 num_words_2,001 number_2,001_74K dep_sentences_words amod_sentences_English num_sentences_0.9 number_1.4_2.9 dep_OOV_sentences num_OOV_1.4 num_OOV_5.2 appos_OOV_% num_OOV_27.9 num_OOV_24.5 number_24.5_27.6 dep_length_OOV dep_8,638_length dep_9,047_8,638 number_9,047_61K dep_vocab_9,047 number_56,081_49,116 dep_K_vocab num_K_56,081 dep_2,048_K dep_words_2,048 num_words_2,007 num_words_2,001 dep_74K_words dep_sentences_74K dep_Spanish_sentences dep_NC_Spanish dep_Eval_NC dep_Dev_Eval dep_Train_Dev dep_Corpus_Train dep_Resources_Corpus dep_Language_Resources dep_Table_Language num_Table_1 amod_features_following det_features_the dobj_containing_features vmod_model_containing amod_model_exponential amod_model_feature-based det_model_a prep_of_framework_model det_framework_the prep_within_built_framework dep_Finch_2008a conj_and_Finch_Sumita dep_system_Table vmod_system_built dep_system_Sumita dep_system_Finch nn_system_translation nn_system_machine amod_system_phrasebased amod_system_typical det_system_a cop_system_is nsubj_system_system advmod_typical_fairly nn_system_baseline poss_system_Our nn_system_Baseline num_system_4.1
W09-0418	P03-1021	o	Minimum error rate training -LRB- MERT -RRB- with respect to BLEU score was used to tune the decoders parameters and performed using the technique proposed in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_proposed_in vmod_technique_proposed det_technique_the dobj_using_technique xcomp_performed_using nsubjpass_performed_training nn_parameters_decoders det_parameters_the dobj_tune_parameters aux_tune_to conj_and_used_performed xcomp_used_tune auxpass_used_was nsubjpass_used_training nn_score_BLEU prep_with_respect_to_training_score appos_training_MERT nn_training_rate nn_training_error nn_training_Minimum
W09-0421	P03-1021	o	The translation system is a factored phrasebased translation system that uses the Moses toolkit -LRB- Koehn et al. 2007 -RRB- for decoding and training GIZA + + for word alignment -LRB- Och and Ney 2003 -RRB- and SRILM -LRB- Stolcke 2002 -RRB- for language models	nn_models_language dep_Stolcke_2002 dep_SRILM_Stolcke dep_Och_2003 conj_and_Och_Ney dep_alignment_Ney dep_alignment_Och nn_alignment_word pobj_for_alignment prep_for_GIZA_models conj_and_GIZA_SRILM conj_+_GIZA_for conj_and_decoding_training amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et nn_toolkit_Moses det_toolkit_the dobj_uses_toolkit nsubj_uses_that dep_system_SRILM dep_system_for dep_system_GIZA prep_for_system_training prep_for_system_decoding dep_system_Koehn rcmod_system_uses nn_system_translation amod_system_phrasebased amod_system_factored det_system_a cop_system_is nsubj_system_system nn_system_translation det_system_The
W09-0421	P03-1021	o	Minimum error rate training was used to tune the model feature weights -LRB- Och 2003 -RRB-	amod_Och_2003 dep_weights_Och nn_weights_feature nn_weights_model det_weights_the dobj_tune_weights aux_tune_to xcomp_used_tune auxpass_used_was nsubjpass_used_training nn_training_rate nn_training_error nn_training_Minimum
W09-0424	P03-1021	o	Deterministic Annealing In this system instead of using the regular MERT -LRB- Och 2003 -RRB- whose training objective is to minimize the onebest error we use the deterministic annealing training procedure described in Smith and Eisner -LRB- 2006 -RRB- whose objective is to minimize the expected error -LRB- together with the entropy regularization technique -RRB-	nn_technique_regularization nn_technique_entropy det_technique_the prep_together_with_error_technique amod_error_expected det_error_the dobj_minimize_error aux_minimize_to xcomp_is_minimize nsubj_is_objective poss_objective_whose appos_Eisner_2006 rcmod_Smith_is conj_and_Smith_Eisner prep_in_described_Eisner prep_in_described_Smith vmod_procedure_described nn_procedure_training nn_procedure_annealing amod_procedure_deterministic det_procedure_the dobj_use_procedure nsubj_use_we prepc_instead_of_use_using prep_in_use_system amod_error_onebest det_error_the dobj_minimize_error aux_minimize_to xcomp_is_minimize nsubj_is_objective nn_objective_training poss_objective_whose dep_Och_2003 rcmod_MERT_is dep_MERT_Och amod_MERT_regular det_MERT_the dobj_using_MERT det_system_this dep_Annealing_use amod_Annealing_Deterministic
W09-0424	P03-1021	o	The toolkit also implements suffixarray grammar extraction -LRB- Callison-Burch et al. 2005 Lopez 2007 -RRB- and minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum num_Lopez_2007 dep_al._Lopez dep_al._2005 nn_al._et amod_al._Callison-Burch conj_and_extraction_training dep_extraction_al. nn_extraction_grammar amod_extraction_suffixarray dobj_implements_training dobj_implements_extraction advmod_implements_also nsubj_implements_toolkit det_toolkit_The ccomp_``_implements
W09-0424	P03-1021	o	The toolkit also implements suffix-array grammar extraction -LRB- Lopez 2007 -RRB- and minimum error rate training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum amod_Lopez_2007 conj_and_extraction_training dep_extraction_Lopez nn_extraction_grammar amod_extraction_suffix-array dobj_implements_training dobj_implements_extraction advmod_implements_also nsubj_implements_toolkit det_toolkit_The ccomp_``_implements
W09-0424	P03-1021	p	The search across a dimension uses the efficient method of Och -LRB- 2003 -RRB-	appos_Och_2003 prep_of_method_Och amod_method_efficient det_method_the dobj_uses_method nsubj_uses_search det_dimension_a prep_across_search_dimension det_search_The
W09-0424	P03-1021	o	3.2 Translation Scores The translation scores for four different systems are reported in Table 1.5 Baseline In this system we use the GIZA + + toolkit -LRB- Och and Ney 2003 -RRB- a suffix-array architecture -LRB- Lopez 2007 -RRB- the SRILM toolkit -LRB- Stolcke 2002 -RRB- and minimum error rate training -LRB- Och 2003 -RRB- to obtain word-alignments a translation model language models and the optimal weights for combining these models respectively	det_models_these dobj_combining_models prepc_for_weights_combining amod_weights_optimal det_weights_the nn_models_language nn_model_translation det_model_a conj_and_word-alignments_weights conj_and_word-alignments_models appos_word-alignments_model dobj_obtain_weights dobj_obtain_models dobj_obtain_word-alignments aux_obtain_to dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum dep_Stolcke_2002 appos_toolkit_Stolcke nn_toolkit_SRILM det_toolkit_the amod_Lopez_2007 appos_architecture_Lopez amod_architecture_suffix-array det_architecture_a dep_Och_2003 conj_and_Och_Ney conj_and_toolkit_training appos_toolkit_toolkit appos_toolkit_architecture appos_toolkit_Ney appos_toolkit_Och pobj_+_training pobj_+_toolkit conj_+_GIZA_+ det_GIZA_the advmod_use_respectively vmod_use_obtain dobj_use_+ dobj_use_GIZA nsubj_use_we prep_in_use_system det_system_this num_Baseline_1.5 nn_Baseline_Table prep_in_reported_Baseline auxpass_reported_are nsubjpass_reported_scores amod_systems_different num_systems_four prep_for_scores_systems nn_scores_translation det_scores_The parataxis_Scores_use rcmod_Scores_reported nn_Scores_Translation num_Scores_3.2
W09-0426	P03-1021	o	The preprocessed training data was filtered for length and aligned using the GIZA + + implementation of IBM Model 4 -LRB- Och and Ney 2003 -RRB- in both directions and symmetrized using the grow-diag-final-and heuristic	amod_heuristic_grow-diag-final-and det_heuristic_the dobj_using_heuristic xcomp_symmetrized_using nsubjpass_symmetrized_data preconj_directions_both dep_Och_2003 conj_and_Och_Ney dep_Model_Ney dep_Model_Och num_Model_4 nn_Model_IBM prep_of_implementation_Model pobj_+_implementation prep_in_GIZA_directions conj_+_GIZA_+ det_GIZA_the dobj_using_+ dobj_using_GIZA xcomp_aligned_using nsubjpass_aligned_data conj_and_filtered_symmetrized conj_and_filtered_aligned prep_for_filtered_length auxpass_filtered_was nsubjpass_filtered_data nn_data_training amod_data_preprocessed det_data_The ccomp_``_symmetrized ccomp_``_aligned ccomp_``_filtered
W09-0426	P03-1021	o	2.3 Forest minimum error training To tune the feature weights of our system we used a variant of the minimum error training algorithm -LRB- Och 2003 -RRB- that computes the error statistics from the target sentences from the translation search space -LRB- represented by a packed forest -RRB- that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space -LRB- Macherey et al. 2008 -RRB-	amod_Macherey_2008 dep_Macherey_al. nn_Macherey_et nn_space_feature det_space_the prep_of_dimensions_space det_dimensions_the amod_vector_single det_vector_a prep_along_weights_vector nn_weights_feature det_weights_the prep_in_changing_dimensions dobj_changing_weights prepc_by_discriminable_changing advmod_discriminable_minimally cop_discriminable_are nsubj_discriminable_that dep_those_Macherey rcmod_those_discriminable advmod_those_exactly cop_those_are nsubj_those_that amod_forest_packed det_forest_a prep_by_represented_forest nn_space_search nn_space_translation det_space_the rcmod_sentences_those dep_sentences_represented prep_from_sentences_space nn_sentences_target det_sentences_the nn_statistics_error det_statistics_the prep_from_computes_sentences dobj_computes_statistics nsubj_computes_that amod_Och_2003 dep_algorithm_Och nn_algorithm_training nn_algorithm_error amod_algorithm_minimum det_algorithm_the rcmod_variant_computes prep_of_variant_algorithm det_variant_a dobj_used_variant nsubj_used_we nsubj_used_training poss_system_our prep_of_weights_system nn_weights_feature det_weights_the dobj_tune_weights aux_tune_To vmod_training_tune nn_training_error nn_training_minimum nn_training_Forest num_training_2.3
W09-0427	P03-1021	o	The loglinear model feature weights were learned using minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- with BLEU score -LRB- Papineni et al. 2002 -RRB- as the objective function	amod_function_objective det_function_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU dep_Och_2003 dep_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum prep_with_using_score dobj_using_training prep_as_learned_function dep_learned_Papineni xcomp_learned_using auxpass_learned_were nsubjpass_learned_weights nn_weights_feature nn_weights_model amod_weights_loglinear det_weights_The ccomp_``_learned
W09-0431	P03-1021	o	We use GIZA + + -LRB- Och and Ney 2003 -RRB- for 5 http://iit-iti.nrc-cnrc.gc.ca/projects-projets/portage_e.html 176 word alignment and the Pharaoh system suite to build the phrase table and decode -LRB- Koehn 2004 -RRB-	amod_Koehn_2004 dep_decode_Koehn nn_table_phrase det_table_the conj_and_build_decode dobj_build_table aux_build_to vmod_suite_decode vmod_suite_build nn_suite_system nn_suite_Pharaoh det_suite_the nn_alignment_word num_alignment_176 nn_alignment_http://iit-iti.nrc-cnrc.gc.ca/projects-projets/portage_e.html num_alignment_5 conj_and_Och_suite prep_for_Och_alignment num_Och_2003 conj_and_Och_Ney pobj_+_suite pobj_+_Ney pobj_+_Och conj_+_GIZA_+ dobj_use_+ dobj_use_GIZA nsubj_use_We
W09-0431	P03-1021	o	Tuning is done for each experimental condition using Ochs Minimum Error Training -LRB- Och 2003 -RRB-	amod_Och_2003 appos_Training_Och nn_Training_Error nn_Training_Minimum nn_Training_Ochs dobj_using_Training amod_condition_experimental det_condition_each xcomp_done_using prep_for_done_condition auxpass_done_is nsubjpass_done_Tuning
W09-0433	P03-1021	o	4 Experiment Our baseline system is a popular phrase-based SMT system Moses -LRB- Koehn et al. 2007 -RRB- with 5-gram SRILM language model -LRB- Stolcke 2002 -RRB- tuned with Minimum Error Training -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och nn_Training_Error nn_Training_Minimum prep_with_tuned_Training dep_Stolcke_2002 appos_model_Stolcke nn_model_language nn_model_SRILM amod_model_5-gram amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et vmod_Moses_tuned prep_with_Moses_model dep_Moses_Koehn dep_system_Moses nn_system_SMT amod_system_phrase-based amod_system_popular det_system_a cop_system_is nsubj_system_system nn_system_baseline poss_system_Our nn_system_Experiment num_system_4
W09-0436	P03-1021	o	Parameter tuning is done with Minimum Error Rate Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum prep_with_done_Training auxpass_done_is nsubjpass_done_tuning nn_tuning_Parameter
W09-0437	P03-1021	o	The corpus was aligned with GIZA + + -LRB- Och and Ney 2003 -RRB- and symmetrized with the grow-diag-finaland heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_grow-diag-finaland det_heuristic_the prep_with_symmetrized_heuristic nsubjpass_symmetrized_corpus amod_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ conj_and_aligned_symmetrized prep_with_aligned_+ prep_with_aligned_GIZA auxpass_aligned_was nsubjpass_aligned_corpus det_corpus_The
W09-0437	P03-1021	o	Systems were optimized on the WMT08 French-English development data -LRB- 2000 sentences -RRB- using minimum error rate training -LRB- Och 2003 -RRB- and tested on the WMT08 test data -LRB- 2000 sentences -RRB-	num_sentences_2000 appos_data_sentences nn_data_test nn_data_WMT08 det_data_the prep_on_tested_data dep_Och_2003 conj_and_training_tested appos_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_tested dobj_using_training num_sentences_2000 appos_data_sentences nn_data_development amod_data_French-English nn_data_WMT08 det_data_the xcomp_optimized_using prep_on_optimized_data auxpass_optimized_were nsubjpass_optimized_Systems
W09-0439	P03-1021	p	Ochs procedure is the most widely-used version of MERT for SMT -LRB- Och 2003 -RRB-	amod_Och_2003 dep_SMT_Och prep_for_version_SMT prep_of_version_MERT amod_version_widely-used det_version_the cop_version_is nsubj_version_procedure advmod_widely-used_most nn_procedure_Ochs
W09-0439	P03-1021	p	Although they obtained consistent and stable performance gains for MT these were inferior to the gains yielded by Ochs procedure in -LRB- Och 2003 -RRB-	amod_Och_2003 dep_in_Och prep_procedure_in nn_procedure_Ochs agent_yielded_procedure vmod_gains_yielded det_gains_the prep_to_inferior_gains cop_inferior_were nsubj_inferior_these advcl_inferior_obtained prep_for_gains_MT nn_gains_performance amod_gains_stable amod_gains_consistent conj_and_consistent_stable dobj_obtained_gains nsubj_obtained_they mark_obtained_Although
W09-1114	P03-1021	o	3.2 Translation performance For the experiments reported in this section we used feature weights trained with minimum error rate training -LRB- MERT Och 2003 -RRB- Because MERT ignores the denominator in Equation 1 it is invariant with respect to the scale of the weight vector the Moses implementation simply normalises the weight vector it finds by its lscript1-norm	poss_lscript1-norm_its prep_by_finds_lscript1-norm nsubj_finds_it rcmod_vector_finds nn_vector_weight det_vector_the dobj_normalises_vector advmod_normalises_simply nsubj_normalises_implementation nn_implementation_Moses det_implementation_the rcmod_vector_normalises nn_vector_weight det_vector_the prep_of_scale_vector det_scale_the prep_with_respect_to_invariant_scale cop_invariant_is nsubj_invariant_it advcl_invariant_ignores num_Equation_1 det_denominator_the prep_in_ignores_Equation dobj_ignores_denominator nsubj_ignores_MERT mark_ignores_Because dep_Och_2003 dep_MERT_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum prep_with_trained_training vmod_weights_trained nn_weights_feature parataxis_used_invariant dobj_used_weights nsubj_used_we ccomp_used_performance det_section_this prep_in_reported_section vmod_experiments_reported det_experiments_the prep_for_performance_experiments nn_performance_Translation num_performance_3.2
W09-2307	P03-1021	o	Parameter tuning is done with Minimum Error Rate Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum prep_with_done_Training auxpass_done_is nsubjpass_done_tuning nn_tuning_Parameter
W09-2309	P03-1021	o	A popular statistical machine translation paradigms is the phrase-based model -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_model_Koehn amod_model_phrase-based det_model_the cop_model_is nsubj_model_paradigms nn_paradigms_translation nn_paradigms_machine amod_paradigms_statistical amod_paradigms_popular det_paradigms_A
W09-2309	P03-1021	o	For phrase-based translation model training we used the GIZA + + toolkit -LRB- Och and Ney 2003 -RRB- and 1.0 M bilingual sentences	amod_sentences_bilingual nn_sentences_M num_sentences_1.0 dep_Och_2003 conj_and_Och_Ney appos_toolkit_Ney appos_toolkit_Och pobj_+_toolkit conj_and_GIZA_sentences conj_+_GIZA_+ det_GIZA_the dobj_used_sentences dobj_used_+ dobj_used_GIZA nsubj_used_we prep_for_used_training nn_training_model nn_training_translation amod_training_phrase-based
W09-2309	P03-1021	o	To tune the decoder parameters we conducted minimum error rate training -LRB- Och 2003 -RRB- with respect to the word BLEU score -LRB- Papineni et al. 2002 -RRB- using 2.0 K development sentence pairs	nn_pairs_sentence nn_pairs_development nn_pairs_K num_pairs_2.0 dobj_using_pairs amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU nn_score_word det_score_the dep_Och_2003 prep_with_respect_to_training_score appos_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_conducted_using dep_conducted_Papineni dobj_conducted_training nsubj_conducted_we advcl_conducted_tune nn_parameters_decoder det_parameters_the dobj_tune_parameters aux_tune_To ccomp_``_conducted
W09-2309	P03-1021	o	This involves running GIZA + + -LRB- Och and Ney 2003 -RRB- on the corpus in both directions and applying renement rules -LRB- the variant they designate is nal-and -RRB- to obtain a single many-tomany word alignment for each sentence	det_sentence_each prep_for_alignment_sentence nn_alignment_word nn_alignment_many-tomany amod_alignment_single det_alignment_a dobj_obtain_alignment aux_obtain_to cop_nal-and_is nsubj_nal-and_variant nsubj_designate_they rcmod_variant_designate det_variant_the dep_rules_nal-and nn_rules_renement vmod_applying_obtain dobj_applying_rules preconj_directions_both det_corpus_the num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och prep_on_GIZA_corpus conj_+_GIZA_+ conj_and_running_applying prep_in_running_directions dobj_running_+ dobj_running_GIZA ccomp_involves_applying ccomp_involves_running nsubj_involves_This ccomp_``_involves
C08-2012	P04-1015	o	As to analysis of NPs there have been a lot of work on statistical techniques for lexical dependency parsing of sentences -LRB- Collins and Roark 2004 McDonald et al. 2005 -RRB- and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available	cop_available_are nsubj_available_resources mark_available_if prep_for_resources_NPs amod_resources_appropriate prep_of_analysis_NPs advcl_used_available prep_for_used_analysis auxpass_used_be aux_used_can advmod_used_potentially nsubjpass_used_techniques det_techniques_these num_McDonald_2005 nn_McDonald_al. nn_McDonald_et dep_Collins_McDonald amod_Collins_2004 conj_and_Collins_Roark appos_parsing_Roark appos_parsing_Collins prep_of_parsing_sentences nn_parsing_dependency amod_parsing_lexical prep_for_techniques_parsing amod_techniques_statistical conj_and_lot_used prep_on_lot_techniques prep_of_lot_work det_lot_a cop_lot_been aux_lot_have expl_lot_there pobj_lot_analysis prepc_as_to_lot_to prep_of_analysis_NPs
D07-1009	P04-1015	o	In fact when the perceptron update rule of -LRB- Dekel et al. 2004 -RRB- which modifies the weights of every divergent node along the predicted and true paths is used in the ranking framework it becomes virtually identical with the standard flat ranking perceptron of Collins -LRB- 2002 -RRB- .5 In contrast our approach shares the idea of -LRB- Cesa-Bianchi et al. 2006a -RRB- that if a parent class has been predicted wrongly then errors in the children should not be taken into account We also view this as one of the key ideas of the incremental perceptron algorithm of -LRB- Collins and Roark 2004 -RRB- which searches through a complex decision space step-by-step and is immediately updated at the first wrong move	amod_move_wrong amod_move_first det_move_the prep_at_updated_move advmod_updated_immediately auxpass_updated_is amod_space_step-by-step conj_and_decision_updated dep_decision_space amod_decision_complex det_decision_a prep_through_searches_updated prep_through_searches_decision nsubj_searches_which rcmod_Collins_searches dep_Collins_2004 conj_and_Collins_Roark prep_of_algorithm_Roark prep_of_algorithm_Collins nn_algorithm_perceptron amod_algorithm_incremental det_algorithm_the prep_of_ideas_algorithm amod_ideas_key det_ideas_the prep_of_one_ideas prep_as_view_one dobj_view_this advmod_view_also nsubj_view_We parataxis_taken_view prep_into_taken_account auxpass_taken_be neg_taken_not aux_taken_should nsubjpass_taken_errors advcl_taken_predicted mark_taken_that det_children_the prep_in_errors_children advmod_errors_then advmod_predicted_wrongly auxpass_predicted_been aux_predicted_has nsubjpass_predicted_class mark_predicted_if nn_class_parent det_class_a appos_Cesa-Bianchi_2006a dep_Cesa-Bianchi_al. nn_Cesa-Bianchi_et ccomp_idea_taken prep_of_idea_Cesa-Bianchi det_idea_the dep_shares_idea nn_shares_approach poss_shares_our ccomp_,_shares nn_.5_Collins appos_Collins_2002 prep_in_perceptron_contrast prep_of_perceptron_.5 amod_perceptron_ranking appos_standard_perceptron amod_standard_flat det_standard_the prep_with_identical_standard advmod_identical_virtually acomp_becomes_identical nsubj_becomes_it advcl_becomes_update prep_in_becomes_fact amod_framework_ranking det_framework_the prep_in_used_framework auxpass_used_is nsubjpass_used_paths mark_used_along amod_paths_true amod_paths_predicted det_paths_the conj_and_predicted_true amod_node_divergent det_node_every prep_of_weights_node det_weights_the advcl_modifies_used dobj_modifies_weights nsubj_modifies_which rcmod_Dekel_modifies amod_Dekel_2004 dep_Dekel_al. nn_Dekel_et prep_of_rule_Dekel dobj_update_rule nsubj_update_perceptron advmod_update_when det_perceptron_the
D07-1033	P04-1015	o	7 Discussion As we mentioned there are some algorithms similar to ours -LRB- Collins and Roark 2004 Daume III and Marcu 2005 McDonald and Pereira 2006 Liang et al. 2006 -RRB-	num_Liang_2006 nn_Liang_al. nn_Liang_et dep_McDonald_Liang conj_and_McDonald_2006 conj_and_McDonald_Pereira dep_III_2006 dep_III_Pereira dep_III_McDonald conj_and_III_2005 conj_and_III_Marcu nn_III_Daume dep_Collins_2005 dep_Collins_Marcu dep_Collins_III amod_Collins_2004 conj_and_Collins_Roark dep_ours_Roark dep_ours_Collins prep_to_similar_ours amod_algorithms_similar det_algorithms_some nsubj_are_algorithms expl_are_there dep_are_Discussion nsubj_mentioned_we mark_mentioned_As dep_Discussion_mentioned num_Discussion_7
D07-1033	P04-1015	o	CollinsandRoark -LRB- 2004 -RRB- proposedanapproximate incremental method for parsing	prep_for_method_parsing amod_method_incremental nn_method_proposedanapproximate nn_method_CollinsandRoark appos_CollinsandRoark_2004
D07-1033	P04-1015	o	Collins and Roark -LRB- 2004 -RRB- used the averaged perceptron -LRB- Collins 2002a -RRB-	appos_Collins_2002a dep_perceptron_Collins amod_perceptron_averaged det_perceptron_the dobj_used_perceptron nsubj_used_Roark nsubj_used_Collins appos_Roark_2004 conj_and_Collins_Roark
D07-1033	P04-1015	o	With regard to the local update -LRB- B -RRB- in Algorithm 4.2 early updates -LRB- Collins and Roark 2004 -RRB- and y-good requirement in -LRB- Daume III and Marcu 2005 -RRB- resemble our local update in that they tried to avoid the situation where the correct answer can not be output	cop_output_be neg_output_not aux_output_can nsubj_output_answer advmod_output_where amod_answer_correct det_answer_the rcmod_situation_output det_situation_the dobj_avoid_situation aux_avoid_to xcomp_tried_avoid nsubj_tried_they mark_tried_that prepc_in_update_tried nsubj_update_local poss_local_our ccomp_resemble_update nsubj_resemble_requirement nsubj_resemble_updates dep_III_2005 conj_and_III_Marcu nn_III_Daume nn_requirement_y-good amod_Collins_2004 conj_and_Collins_Roark prep_in_updates_Marcu prep_in_updates_III conj_and_updates_requirement dep_updates_Roark dep_updates_Collins advmod_updates_early num_Algorithm_4.2 parataxis_update_resemble prep_in_update_Algorithm appos_update_B prep_with_regard_to_update_local det_local_the
D07-1033	P04-1015	o	Recently severalmethods -LRB- Collins and Roark 2004 Daume III and Marcu 2005 McDonald and Pereira 2006 -RRB- have been proposed with similar motivation to ours	prep_to_motivation_ours amod_motivation_similar prep_with_proposed_motivation auxpass_proposed_been aux_proposed_have nsubjpass_proposed_2005 nsubjpass_proposed_Marcu nsubjpass_proposed_III dep_McDonald_2006 conj_and_McDonald_Pereira dep_III_Pereira dep_III_McDonald conj_and_III_2005 conj_and_III_Marcu nn_III_Daume dep_Collins_proposed amod_Collins_2004 conj_and_Collins_Roark dep_severalmethods_Roark dep_severalmethods_Collins dep_Recently_severalmethods dep_``_Recently
D07-1129	P04-1015	o	2.3 Online Learning Again following -LRB- McDonald et al. 2005 -RRB- we have used the single best MIRA -LRB- Crammer and Singer 2003 -RRB- which is a margin aware variant of perceptron -LRB- Collins 2002 Collins and Roark 2004 -RRB- for structured prediction	amod_prediction_structured prep_for_Collins_prediction appos_Collins_2004 conj_and_Collins_Roark dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 dep_perceptron_Collins prep_of_variant_perceptron amod_variant_aware nn_variant_margin det_variant_a cop_variant_is nsubj_variant_which dep_Crammer_2003 conj_and_Crammer_Singer rcmod_MIRA_variant appos_MIRA_Singer appos_MIRA_Crammer amod_MIRA_best amod_MIRA_single det_MIRA_the dobj_used_MIRA aux_used_have nsubj_used_we dep_used_Again amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et dep_following_McDonald vmod_Again_following nn_Again_Learning nn_Again_Online num_Again_2.3 advcl_``_used
D07-1129	P04-1015	o	We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron -LRB- Collins 2002 Collins and Roark 2004 Crammer and Singer 2003 -RRB-	appos_Crammer_2003 conj_and_Crammer_Singer num_Collins_2004 conj_and_Collins_Roark dep_Collins_Singer dep_Collins_Crammer dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 dep_perceptron_Collins amod_perceptron_voted det_perceptron_the prep_of_variant_perceptron det_variant_a dobj_using_variant amod_fashion_on-line det_fashion_an poss_parser_our vmod_trained_using prep_in_trained_fashion dobj_trained_parser advmod_trained_discriminatively nsubj_trained_We
D08-1052	P04-1015	p	Some recent work on incremental parsing -LRB- Collins and Roark 2004 Shen and Joshi 2005 -RRB- showed another way to handle this problem	det_problem_this dobj_handle_problem aux_handle_to vmod_way_handle det_way_another dobj_showed_way nsubj_showed_work num_Shen_2005 conj_and_Shen_Joshi dep_Collins_Joshi dep_Collins_Shen amod_Collins_2004 conj_and_Collins_Roark appos_parsing_Roark appos_parsing_Collins amod_parsing_incremental prep_on_work_parsing amod_work_recent det_work_Some ccomp_``_showed
D08-1052	P04-1015	p	Variants of this method have been successfully used in many NLP tasks like shallow processing -LRB- Daume III and Marcu 2005 -RRB- parsing -LRB- Collins and Roark 2004 Shen and Joshi 2005 -RRB- and word alignment -LRB- Moore 2005 -RRB-	amod_Moore_2005 dep_alignment_Moore nn_alignment_word num_Shen_2005 conj_and_Shen_Joshi dep_Collins_Joshi dep_Collins_Shen amod_Collins_2004 conj_and_Collins_Roark appos_parsing_Roark appos_parsing_Collins dep_Daume_2005 conj_and_Daume_Marcu num_Daume_III conj_and_processing_alignment conj_and_processing_parsing appos_processing_Marcu appos_processing_Daume amod_processing_shallow nn_tasks_NLP amod_tasks_many prep_like_used_alignment prep_like_used_parsing prep_like_used_processing prep_in_used_tasks advmod_used_successfully auxpass_used_been aux_used_have nsubjpass_used_Variants det_method_this prep_of_Variants_method
D08-1052	P04-1015	o	We still use complex structures to represent the partial analyses so as to employ both top-down and bottom-up information as in -LRB- Collins and Roark 2004 Shen and Joshi 2005 -RRB-	amod_Shen_2005 conj_and_Shen_Joshi dep_Collins_Joshi dep_Collins_Shen conj_and_Collins_2004 conj_and_Collins_Roark pobj_in_2004 pobj_in_Roark pobj_in_Collins pcomp_as_in amod_information_bottom-up amod_information_top-down conj_and_top-down_bottom-up preconj_top-down_both prep_employ_as dobj_employ_information aux_employ_to amod_analyses_partial det_analyses_the dobj_represent_analyses aux_represent_to amod_structures_complex prepc_as_use_employ advmod_use_so vmod_use_represent dobj_use_structures advmod_use_still nsubj_use_We
D08-1059	P04-1015	o	Beam-search has been successful in many NLP tasks -LRB- Koehn et al. 2003 562 Inputs training examples -LRB- xi yi -RRB- Initialization set vectorw = 0 Algorithm / / R training iterations N examples for t = 1R i = 1N zi = argmaxyGEN -LRB- xi -RRB- -LRB- y -RRB- vectorw if zi negationslash = yi vectorw = vectorw + -LRB- yi -RRB- -LRB- zi -RRB- Outputs vectorw Figure 1 The perceptron learning algorithm Collins and Roark 2004 -RRB- and can achieve accuracy that is close to exact inference	amod_inference_exact prep_to_close_inference cop_close_is nsubj_close_that rcmod_accuracy_close dobj_achieve_accuracy aux_achieve_can dep_Roark_2004 conj_and_Collins_Roark nn_Collins_algorithm dobj_learning_Roark dobj_learning_Collins conj_and_perceptron_achieve vmod_perceptron_learning dep_The_achieve dep_The_perceptron dep_Figure_The num_Figure_1 dobj_vectorw_Figure dep_Outputs_vectorw nn_Outputs_yi nn_Outputs_vectorw dep_yi_zi conj_+_vectorw_yi dep_=_Outputs amod_vectorw_= dep_yi_vectorw dobj_=_yi dep_negationslash_= nn_negationslash_zi nn_vectorw_argmaxyGEN appos_argmaxyGEN_y appos_argmaxyGEN_xi prep_if_=_negationslash dobj_=_vectorw dep_zi_= dep_1N_zi dep_=_1N amod_i_= dep_1R_i dep_=_1R amod_t_= prep_for_examples_t nn_examples_N nn_iterations_training nn_iterations_R num_Algorithm_0 dep_=_Algorithm amod_vectorw_= dobj_set_vectorw dep_Initialization_iterations dep_Initialization_set appos_xi_yi dep_examples_examples dep_examples_Initialization dep_examples_xi nn_examples_training dep_Inputs_examples num_Inputs_562 dep_Koehn_Inputs dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_tasks_NLP amod_tasks_many dep_successful_Koehn prep_in_successful_tasks cop_successful_been aux_successful_has nsubj_successful_Beam-search ccomp_``_successful
D08-1059	P04-1015	o	During training the early update strategy of Collins and Roark -LRB- 2004 -RRB- is used when the correct state item falls out of the beam at any stage parsing is stopped immediately and the model is updated using the current best partial item	amod_item_partial amod_item_best amod_item_current det_item_the dobj_using_item xcomp_updated_using auxpass_updated_is nsubjpass_updated_model det_model_the conj_and_stopped_updated advmod_stopped_immediately auxpass_stopped_is nsubjpass_stopped_parsing advcl_stopped_falls det_stage_any det_beam_the prep_at_falls_stage pobj_falls_beam prepc_out_of_falls_of nsubj_falls_item advmod_falls_when nn_item_state amod_item_correct det_item_the parataxis_used_updated parataxis_used_stopped auxpass_used_is nsubjpass_used_the prep_during_used_training amod_is_early appos_Roark_2004 conj_and_Collins_Roark prep_of_strategy_Roark prep_of_strategy_Collins dobj_update_strategy dep_early_update
D09-1034	P04-1015	p	Incremental top-down and left-corner parsers have been shown to effectively -LRB- and efficiently -RRB- make use of non-local features from the left-context to yield very high accuracy syntactic parses -LRB- Roark 2001 Henderson 2003 Collins and Roark 2004 -RRB- and we will use such rich models to derive our scores	poss_scores_our dobj_derive_scores aux_derive_to amod_models_rich amod_models_such vmod_use_derive dobj_use_models aux_use_will nsubj_use_we dep_Collins_2004 conj_and_Collins_Roark num_Henderson_2003 dep_Roark_Roark dep_Roark_Collins dep_Roark_Henderson dep_Roark_2001 conj_and_parses_use dep_parses_Roark nn_syntactic_accuracy amod_syntactic_high advmod_high_very dobj_yield_syntactic aux_yield_to det_left-context_the amod_features_non-local vmod_use_yield prep_from_use_left-context prep_of_use_features dep_make_use dep_make_parses dobj_make_use advmod_make_effectively aux_make_to cc_efficiently_and dep_effectively_efficiently xcomp_shown_make auxpass_shown_been aux_shown_have nsubjpass_shown_left-corner nsubjpass_shown_top-down dep_top-down_parsers conj_and_top-down_left-corner amod_top-down_Incremental
D09-1043	P04-1015	o	These findings are in line with Collins & Roarks -LRB- 2004 -RRB- results with incremental parsing with perceptrons where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning	prep_for_point_learning amod_point_starting amod_point_better amod_better_much det_better_a nn_algorithm_perceptron det_algorithm_the prep_with_provides_point dobj_provides_algorithm nsubj_provides_feature mark_provides_that nn_feature_baseline amod_feature_generative det_feature_a ccomp_suggested_provides auxpass_suggested_is nsubjpass_suggested_it advmod_suggested_where rcmod_perceptrons_suggested prep_with_parsing_perceptrons xcomp_incremental_parsing prep_with_results_incremental appos_Collins_2004 conj_and_Collins_Roarks prep_with_line_Roarks prep_with_line_Collins dep_are_results prep_in_are_line nsubj_are_findings det_findings_These ccomp_``_are
D09-1127	P04-1015	o	In shift-reduce parsing further mistakes are often caused by previous ones so only the first mistake in each sentence -LRB- if there is one -RRB- is easily identifiable 7 this is also the argument for early update in applying perceptron learning to these incremental parsing algorithms -LRB- Collins and Roark 2004 -RRB- -LRB- see also Section 2 -RRB-	num_Section_2 dep_also_Section advmod_see_also dep_Collins_2004 conj_and_Collins_Roark appos_algorithms_Roark appos_algorithms_Collins nn_algorithms_parsing amod_algorithms_incremental det_algorithms_these prep_to_learning_algorithms vmod_perceptron_learning dep_applying_see dobj_applying_perceptron prepc_in_update_applying dep_argument_update prep_for_argument_early det_argument_the advmod_argument_also cop_argument_is nsubj_argument_this num_this_7 parataxis_identifiable_argument advmod_identifiable_easily cop_identifiable_is dep_identifiable_is nsubj_is_one expl_is_there mark_is_if det_sentence_each dep_mistake_identifiable prep_in_mistake_sentence amod_mistake_first det_mistake_the advmod_mistake_only advmod_only_so amod_ones_previous dobj_caused_mistake agent_caused_ones advmod_caused_often auxpass_caused_are nsubjpass_caused_mistakes prep_in_caused_parsing amod_mistakes_further amod_parsing_shift-reduce
D09-1127	P04-1015	o	Following Collins and Roark -LRB- 2004 -RRB- we also use the early-update strategy where an update happens whenever the goldstandard action-sequence falls off the beam with the rest of the sequence neglected	vmod_sequence_neglected det_sequence_the prep_of_rest_sequence det_rest_the det_beam_the prep_with_falls_rest dobj_falls_beam prt_falls_off nsubj_falls_action-sequence advmod_falls_whenever nn_action-sequence_goldstandard det_action-sequence_the advcl_happens_falls dep_update_happens nsubj_update_an advmod_update_where rcmod_strategy_update amod_strategy_early-update det_strategy_the dobj_use_strategy advmod_use_also nsubj_use_we prep_following_use_Roark prep_following_use_Collins appos_Roark_2004 conj_and_Collins_Roark
E06-1011	P04-1015	o	Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment -LRB- Moore 2005 -RRB- sequence analysis -LRB- Daume and Marcu 2005 McDonald et al. 2005a -RRB- and phrase-structure parsing -LRB- Collins and Roark 2004 -RRB-	amod_Collins_2004 conj_and_Collins_Roark appos_parsing_Roark appos_parsing_Collins amod_parsing_phrase-structure appos_McDonald_2005a nn_McDonald_al. nn_McDonald_et conj_and_Daume_McDonald conj_and_Daume_2005 conj_and_Daume_Marcu conj_and_analysis_parsing dep_analysis_McDonald dep_analysis_2005 dep_analysis_Marcu dep_analysis_Daume nn_analysis_sequence amod_Moore_2005 dep_alignment_Moore nn_alignment_word prep_such_as_problems_alignment prep_in_inference_problems amod_inference_exact prep_than_approximate_inference advmod_approximate_rather prep_with_robust_approximate advmod_robust_even cop_robust_be aux_robust_to dep_shown_parsing dep_shown_analysis xcomp_shown_robust auxpass_shown_been aux_shown_have nsubjpass_shown_algorithms nn_algorithms_learning nn_algorithms_Online
H05-1102	P04-1015	o	We also employ the voted perceptron algorithm -LRB- Freund and Schapire 1999 -RRB- and the early update technique as in -LRB- Collins and Roark 2004 -RRB-	amod_Collins_2004 conj_and_Collins_Roark dep_in_Roark dep_in_Collins pcomp_as_in prep_technique_as amod_technique_update amod_technique_early det_technique_the conj_and_Freund_1999 conj_and_Freund_Schapire dep_algorithm_1999 dep_algorithm_Schapire dep_algorithm_Freund nn_algorithm_perceptron amod_algorithm_voted det_algorithm_the conj_and_employ_technique dobj_employ_algorithm advmod_employ_also nsubj_employ_We
H05-1102	P04-1015	o	Both left-corner strategy -LRB- Ratnaparkhi 1997 Roark 2001 Prolo 2003 Henderson 2003 Collins and Roark 2004 -RRB- and head-corner strategy -LRB- Henderson 2000 Yamada and Matsumoto 2003 -RRB- were employed in incremental parsing	amod_parsing_incremental prep_in_employed_parsing auxpass_employed_were nsubjpass_employed_strategy nsubjpass_employed_Roark dep_Yamada_2003 conj_and_Yamada_Matsumoto dep_Henderson_Matsumoto dep_Henderson_Yamada amod_Henderson_2000 nn_strategy_head-corner num_Henderson_2003 dep_Prolo_2004 conj_and_Prolo_Roark conj_and_Prolo_Collins conj_and_Prolo_Henderson conj_and_Prolo_2003 dep_Roark_Henderson conj_and_Roark_strategy dep_Roark_Roark dep_Roark_Collins dep_Roark_Henderson dep_Roark_2003 dep_Roark_Prolo num_Roark_2001 parataxis_Ratnaparkhi_employed appos_Ratnaparkhi_1997 dep_strategy_Ratnaparkhi amod_strategy_left-corner det_strategy_Both dep_``_strategy
J07-4004	P04-1015	o	Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark -LRB- 2004 -RRB- which allows more global features	amod_features_global advmod_global_more dobj_allows_features nsubj_allows_which appos_Roark_2004 conj_and_Collins_Roark rcmod_approach_allows prep_of_approach_Roark prep_of_approach_Collins amod_approach_beam-search det_approach_the prep_with_taken_approach advmod_taken_here vmod_approach_taken nn_approach_programming amod_approach_dynamic det_approach_the dobj_compare_approach aux_compare_to xcomp_is_compare nsubj_is_alternative amod_work_future prep_for_alternative_work det_alternative_Another
N06-1045	P04-1015	o	Thelistsmaybeused withannotation and a tuning process such as in -LRB- Collins and Roark 2004 -RRB- to iteratively alter feature weights and improve results	dobj_improve_results nn_weights_feature conj_and_alter_improve dobj_alter_weights advmod_alter_iteratively aux_alter_to vmod_Collins_improve vmod_Collins_alter amod_Collins_2004 conj_and_Collins_Roark pobj_in_Roark pobj_in_Collins nn_process_tuning det_process_a prepc_such_as_withannotation_in conj_and_withannotation_process amod_withannotation_Thelistsmaybeused
N07-1011	P04-1015	o	Collins and Roark -LRB- 2004 -RRB- present an incremental perceptron algorithm for parsing that uses early update to update the parameters when an error is encountered	auxpass_encountered_is nsubjpass_encountered_error advmod_encountered_when det_error_an det_parameters_the advcl_update_encountered dobj_update_parameters aux_update_to amod_update_early xcomp_uses_update dobj_uses_update nsubj_uses_that ccomp_parsing_uses prepc_for_algorithm_parsing nn_algorithm_perceptron amod_algorithm_incremental det_algorithm_an dobj_present_algorithm nsubj_present_Roark nsubj_present_Collins appos_Roark_2004 conj_and_Collins_Roark
P05-1012	P04-1015	o	Our approach is related to those of Collins and Roark -LRB- 2004 -RRB- and Taskar et al.	nn_al._et nn_al._Taskar dep_Roark_2004 conj_and_Collins_al. conj_and_Collins_Roark prep_of_those_al. prep_of_those_Roark prep_of_those_Collins prep_to_related_those auxpass_related_is nsubjpass_related_approach poss_approach_Our
P05-1012	P04-1015	o	Collins and Roark -LRB- 2004 -RRB- presented a linear parsing model trained with an averaged perceptron algorithm	nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_an prep_with_trained_algorithm vmod_model_trained nn_model_parsing amod_model_linear det_model_a dobj_presented_model nsubj_presented_Roark nsubj_presented_Collins appos_Roark_2004 conj_and_Collins_Roark
P05-1012	P04-1015	o	Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated -LRB- Riezler et al. 2002 Clark and Curran 2004 Collins and Roark 2004 Taskar et al. 2004 -RRB-	num_Taskar_2004 nn_Taskar_al. nn_Taskar_et num_Collins_2004 conj_and_Collins_Roark num_Clark_2004 conj_and_Clark_Curran dep_Riezler_Taskar conj_Riezler_Roark conj_Riezler_Collins conj_Riezler_Curran conj_Riezler_Clark amod_Riezler_2002 dep_Riezler_al. nn_Riezler_et dep_investigated_Riezler auxpass_investigated_been advmod_investigated_recently aux_investigated_have nsubjpass_investigated_parsers advmod_recently_only amod_sentence_given det_sentence_a amod_trees_entire prep_for_score_sentence dobj_score_trees nsubj_score_that rcmod_parsers_score amod_parsers_trained advmod_trained_Discriminatively
P05-1023	P04-1015	o	In particular most of the work on parsing with kernel methods has focussed on kernels over parse trees -LRB- Collins and Duffy 2002 Shen and Joshi 2003 Shen et al. 2003 Collins and Roark 2004 -RRB-	amod_Collins_2004 conj_and_Collins_Roark num_Shen_2003 nn_Shen_al. nn_Shen_et dep_Collins_Roark dep_Collins_Collins conj_and_Collins_Shen num_Collins_2003 conj_and_Collins_Joshi conj_and_Collins_Shen conj_and_Collins_2002 conj_and_Collins_Duffy dep_trees_Shen dep_trees_Joshi dep_trees_Shen dep_trees_2002 dep_trees_Duffy dep_trees_Collins nn_trees_parse prep_over_kernels_trees prep_on_focussed_kernels aux_focussed_has nsubj_focussed_most prep_in_focussed_particular nn_methods_kernel prep_with_parsing_methods det_work_the prepc_on_most_parsing prep_of_most_work ccomp_``_focussed
P05-1023	P04-1015	o	For comparison to previous results table 2 lists the results on the testing set for our best model -LRB- TOP-Efficient-Freq20 -RRB- and several other statistical parsers -LRB- Collins 1999 Collins and Duffy 2002 Collins and Roark 2004 Henderson 2003 Charniak 2000 Collins 2000 Shen and Joshi 2004 Shen et al. 2003 Henderson 2004 Bod 2003 -RRB-	amod_Bod_2003 num_Henderson_2004 num_Shen_2003 nn_Shen_al. nn_Shen_et num_Shen_2004 conj_and_Shen_Joshi num_Collins_2000 conj_Charniak_2000 num_Henderson_2003 dep_Collins_Bod conj_and_Collins_Henderson conj_and_Collins_Shen conj_and_Collins_Joshi conj_and_Collins_Shen conj_and_Collins_Collins conj_and_Collins_Charniak conj_and_Collins_Henderson conj_and_Collins_2004 conj_and_Collins_Roark conj_and_Collins_Duffy dep_Collins_Henderson dep_Collins_Shen dep_Collins_Shen dep_Collins_Collins dep_Collins_Charniak dep_Collins_Henderson dep_Collins_2004 dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 dep_Collins_Duffy dep_Collins_Collins amod_Collins_1999 dep_parsers_Collins amod_parsers_statistical amod_parsers_other amod_parsers_several conj_and_model_parsers appos_model_TOP-Efficient-Freq20 amod_model_best poss_model_our prep_for_set_parsers prep_for_set_model vmod_testing_set det_testing_the prep_on_results_testing det_results_the dobj_lists_results nsubj_lists_table prep_for_lists_comparison num_table_2 amod_results_previous prep_to_comparison_results
P05-1023	P04-1015	n	When compared to other kernel methods our approach performs better than those based on the Tree kernel -LRB- Collins and Duffy 2002 Collins and Roark 2004 -RRB- and is only 0.2 % worse than the best results achieved by a kernel method for parsing -LRB- Shen et al. 2003 Shen and Joshi 2004 -RRB-	amod_Shen_2004 conj_and_Shen_Joshi dep_Shen_Joshi dep_Shen_Shen appos_Shen_2003 dep_Shen_al. nn_Shen_et prep_for_method_parsing nn_method_kernel det_method_a agent_achieved_method vmod_results_achieved amod_results_best det_results_the prep_than_worse_results npadvmod_worse_% cop_worse_is nsubj_worse_approach num_%_0.2 quantmod_0.2_only dep_Collins_2004 conj_and_Collins_Roark dep_Collins_Roark dep_Collins_Collins num_Collins_2002 conj_and_Collins_Duffy appos_kernel_Duffy appos_kernel_Collins nn_kernel_Tree det_kernel_the prep_on_based_kernel vmod_those_based prep_than_better_those dep_performs_Shen conj_and_performs_worse dobj_performs_better nsubj_performs_approach advcl_performs_compared poss_approach_our nn_methods_kernel amod_methods_other prep_to_compared_methods advmod_compared_When
P06-1096	P04-1015	p	2.2 Perceptron-based training To tune the parameters w of the model we use the averaged perceptron algorithm -LRB- Collins 2002 -RRB- because of its efficiency and past success on various NLP tasks -LRB- Collins and Roark 2004 Roark et al. 2004 -RRB-	num_Roark_2004 nn_Roark_al. nn_Roark_et dep_Collins_Roark amod_Collins_2004 conj_and_Collins_Roark appos_tasks_Roark appos_tasks_Collins nn_tasks_NLP amod_tasks_various amod_success_past prep_on_efficiency_tasks conj_and_efficiency_success poss_efficiency_its amod_Collins_2002 dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_averaged det_algorithm_the prep_because_of_use_success prep_because_of_use_efficiency dobj_use_algorithm nsubj_use_we nsubj_use_training det_model_the prep_of_w_model dep_parameters_w det_parameters_the dobj_tune_parameters aux_tune_To vmod_training_tune amod_training_Perceptron-based num_training_2.2
P06-1110	P04-1015	p	Collins and Roark -LRB- 2004 -RRB- saw a LFMS improvement of 0.8 % over their baseline discriminative parser after adding punctuation features one of which encoded the sentence-final punctuation	amod_punctuation_sentence-final det_punctuation_the dobj_encoded_punctuation nsubj_encoded_one prep_of_one_which nn_features_punctuation dobj_adding_features amod_parser_discriminative nn_parser_baseline poss_parser_their num_%_0.8 prep_over_improvement_parser prep_of_improvement_% nn_improvement_LFMS det_improvement_a parataxis_saw_encoded prepc_after_saw_adding dobj_saw_improvement nsubj_saw_Roark nsubj_saw_Collins appos_Roark_2004 conj_and_Collins_Roark
P06-1110	P04-1015	o	The left-to-right parser would likely improve if we were to use a left-corner transform -LRB- Collins & Roark 2004 -RRB-	amod_Collins_2004 conj_and_Collins_Roark nsubj_transform_left-corner det_left-corner_a ccomp_use_transform aux_use_to xcomp_were_use nsubj_were_we mark_were_if dep_improve_Roark dep_improve_Collins advcl_improve_were advmod_improve_likely aux_improve_would nsubj_improve_parser amod_parser_left-to-right det_parser_The ccomp_``_improve
P06-1110	P04-1015	o	Collins and Roark -LRB- 2004 -RRB- and Taskar et al.	nn_al._et nn_al._Taskar dep_Roark_2004 conj_and_Collins_al. conj_and_Collins_Roark
P06-1110	P04-1015	n	Although generating training examples in advance without a working parser -LRB- Turian & Melamed 2005 -RRB- is much faster than using inference -LRB- Collins & Roark 2004 Henderson 2004 Taskar et al. 2004 -RRB- our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor	amod_factor_branching amod_factor_lower det_factor_a prep_with_strategy_factor nn_strategy_parsing det_strategy_a dobj_choosing_strategy agent_decreased_choosing advmod_decreased_further auxpass_decreased_be advmod_decreased_probably aux_decreased_can nsubjpass_decreased_time advcl_decreased_faster nn_time_training poss_time_our num_Taskar_2004 nn_Taskar_al. nn_Taskar_et num_Henderson_2004 dep_Collins_Taskar dep_Collins_Henderson amod_Collins_2004 conj_and_Collins_Roark appos_inference_Roark appos_inference_Collins dobj_using_inference prepc_than_faster_using advmod_faster_much cop_faster_is csubj_faster_generating mark_faster_Although dep_Turian_2005 conj_and_Turian_Melamed appos_parser_Melamed appos_parser_Turian amod_parser_working det_parser_a prep_in_examples_advance nn_examples_training prep_without_generating_parser dobj_generating_examples ccomp_``_decreased
P06-1110	P04-1015	o	Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines -LRB- Collins & Roark 2004 Henderson 2004 Taskar et al. 2004 -RRB-	num_Taskar_2004 nn_Taskar_al. nn_Taskar_et num_Henderson_2004 dep_Collins_Taskar conj_and_Collins_Henderson conj_and_Collins_2004 conj_and_Collins_Roark dep_baselines_Henderson dep_baselines_2004 dep_baselines_Roark dep_baselines_Collins amod_baselines_generative prep_above_accuracy_baselines dobj_raise_accuracy nn_time_training conj_and_reduce_raise dobj_reduce_time aux_reduce_to amod_models_generative xcomp_relied_raise xcomp_relied_reduce prep_on_relied_models aux_relied_have nsubj_relied_parsers amod_parsers_discriminative amod_parsers_Successful ccomp_``_relied
P07-1069	P04-1015	o	3For decoding loc is averaged over the training iterations as in Collins and Roark -LRB- 2004 -RRB-	appos_Roark_2004 conj_and_Collins_Roark pobj_in_Roark pobj_in_Collins pcomp_as_in nn_iterations_training det_iterations_the prep_averaged_as prep_over_averaged_iterations auxpass_averaged_is nsubjpass_averaged_loc dep_averaged_decoding nn_decoding_3For
P07-1069	P04-1015	p	Similar models have been successfully applied in the past to other tasks including parsing -LRB- Collins and Roark 2004 -RRB- chunking -LRB- Daume and Marcu 2005 -RRB- and machine translation -LRB- Cowan et al. 2006 -RRB-	amod_Cowan_2006 dep_Cowan_al. nn_Cowan_et nn_translation_machine dep_Daume_2005 conj_and_Daume_Marcu dep_chunking_Marcu dep_chunking_Daume amod_Collins_2004 conj_and_Collins_Roark conj_and_parsing_translation conj_and_parsing_chunking appos_parsing_Roark appos_parsing_Collins prep_including_tasks_translation prep_including_tasks_chunking prep_including_tasks_parsing amod_tasks_other det_past_the dep_applied_Cowan prep_to_applied_tasks prep_in_applied_past advmod_applied_successfully auxpass_applied_been aux_applied_have nsubjpass_applied_models amod_models_Similar
P07-1069	P04-1015	o	This linear model is learned using a variant of the incremental perceptron algorithm -LRB- Collins and Roark 2004 Daume and Marcu 2005 -RRB-	appos_Daume_2005 conj_and_Daume_Marcu dep_Collins_Marcu dep_Collins_Daume amod_Collins_2004 conj_and_Collins_Roark dep_algorithm_Roark dep_algorithm_Collins nn_algorithm_perceptron amod_algorithm_incremental det_algorithm_the prep_of_variant_algorithm det_variant_a dobj_using_variant xcomp_learned_using auxpass_learned_is nsubjpass_learned_model amod_model_linear det_model_This
P07-1096	P04-1015	o	In -LRB- Daume III and Marcu 2005 -RRB- as well as other similar works -LRB- Collins 2002 Collins and Roark 2004 Shen and Joshi 2005 -RRB- only left-toright search was employed	auxpass_employed_was nsubjpass_employed_search dep_employed_Collins amod_search_left-toright advmod_search_only amod_Shen_2005 conj_and_Shen_Joshi num_Collins_2004 conj_and_Collins_Roark dep_Collins_Joshi dep_Collins_Shen conj_Collins_Roark conj_Collins_Collins amod_Collins_2002 dep_works_employed amod_works_similar amod_works_other cc_works_well prep_works_In dep_III_2005 conj_and_III_Marcu nn_III_Daume dep_In_Marcu dep_In_III
P07-1096	P04-1015	o	In -LRB- Collins and Roark 2004 Shen and Joshi 2005 -RRB- a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates	prep_of_queue_candidates det_queue_the prep_in_standard_queue amod_standard_gold det_standard_the prep_with_compatible_standard amod_hypothesis_compatible neg_hypothesis_no nsubj_is_hypothesis expl_is_there mark_is_if advcl_stops_is nsubj_stops_search prep_in_stops_Roark prep_in_stops_Collins det_search_a amod_Shen_2005 conj_and_Shen_Joshi dep_Collins_Joshi dep_Collins_Shen amod_Collins_2004 conj_and_Collins_Roark
P07-1096	P04-1015	o	We proposed a Perceptron like learning algorithm -LRB- Collins and Roark 2004 Daume III and Marcu 2005 -RRB- for guided learning	amod_learning_guided prep_for_III_learning appos_III_2005 conj_and_III_Marcu nn_III_Daume dep_Collins_Marcu dep_Collins_III amod_Collins_2004 conj_and_Collins_Roark dep_algorithm_Roark dep_algorithm_Collins amod_algorithm_learning prep_like_Perceptron_algorithm det_Perceptron_a dobj_proposed_Perceptron nsubj_proposed_We ccomp_``_proposed
P07-1106	P04-1015	o	Hence we use a beam-search decoder during training and testing our idea is similar to that of Collins and Roark -LRB- 2004 -RRB- who used a beam-search decoder as part of a perceptron parsing model	nn_model_parsing nn_model_perceptron det_model_a prep_of_part_model amod_decoder_beam-search det_decoder_a prep_as_used_part dobj_used_decoder nsubj_used_who dep_Roark_2004 conj_and_Collins_Roark rcmod_that_used prep_of_that_Roark prep_of_that_Collins prep_to_similar_that cop_similar_is nsubj_similar_idea poss_idea_our conj_and_training_testing amod_decoder_beam-search det_decoder_a parataxis_use_similar prep_during_use_testing prep_during_use_training dobj_use_decoder nsubj_use_we advmod_use_Hence
P09-1032	P04-1015	o	This algorithm and its many variants are widely used in the computational linguistics community -LRB- Collins 2002a Collins and Duffy 2002 Collins 2002b Collins and Roark 2004 Henderson and Titov 2005 Viola and Narasimhan 2005 Cohen et al. 2004 Carreras et al. 2005 Shen and Joshi 2005 Ciaramita and Johnson 2003 -RRB-	appos_Ciaramita_2003 conj_and_Ciaramita_Johnson num_Shen_2005 conj_and_Shen_Joshi num_Carreras_2005 nn_Carreras_al. nn_Carreras_et num_Cohen_2004 nn_Cohen_al. nn_Cohen_et conj_and_Viola_Johnson conj_and_Viola_Ciaramita conj_and_Viola_Joshi conj_and_Viola_Shen conj_and_Viola_Carreras conj_and_Viola_Cohen conj_and_Viola_2005 conj_and_Viola_Narasimhan dep_Collins_Ciaramita dep_Collins_Shen dep_Collins_Carreras dep_Collins_Cohen dep_Collins_2005 dep_Collins_Narasimhan dep_Collins_Viola conj_and_Collins_2005 conj_and_Collins_Titov conj_and_Collins_Henderson conj_and_Collins_2004 conj_and_Collins_Roark conj_Collins_2002b num_Collins_2002 conj_and_Collins_Duffy dep_Collins_2005 dep_Collins_Titov dep_Collins_Henderson dep_Collins_2004 dep_Collins_Roark dep_Collins_Collins conj_Collins_Collins conj_Collins_Duffy conj_Collins_Collins appos_Collins_2002a dep_community_Collins nn_community_linguistics amod_community_computational det_community_the prep_in_used_community advmod_used_widely auxpass_used_are nsubjpass_used_variants nsubjpass_used_algorithm amod_variants_many poss_variants_its conj_and_algorithm_variants det_algorithm_This
P09-1059	P04-1015	p	It is an online training algorithm and has been successfully used in many NLP tasks such as POS tagging -LRB- Collins 2002 -RRB- parsing -LRB- Collins and Roark 2004 -RRB- Chinese word segmentation -LRB- Zhang and Clark 2007 Jiang et al. 2008 -RRB- and so on	advmod_on_so num_Jiang_2008 nn_Jiang_al. nn_Jiang_et dep_Zhang_Jiang num_Zhang_2007 conj_and_Zhang_Clark appos_segmentation_Clark appos_segmentation_Zhang nn_segmentation_word amod_segmentation_Chinese amod_Collins_2004 conj_and_Collins_Roark appos_parsing_Roark appos_parsing_Collins amod_Collins_2002 conj_and_tagging_on conj_and_tagging_segmentation conj_and_tagging_parsing dep_tagging_Collins nn_tagging_POS prep_such_as_tasks_on prep_such_as_tasks_segmentation prep_such_as_tasks_parsing prep_such_as_tasks_tagging nn_tasks_NLP amod_tasks_many prep_in_used_tasks advmod_used_successfully auxpass_used_been aux_used_has nsubjpass_used_It conj_and_algorithm_used nn_algorithm_training amod_algorithm_online det_algorithm_an cop_algorithm_is nsubj_algorithm_It
P09-2011	P04-1015	o	To tackle this problem we defined 2The best results of Collins and Roark -LRB- 2004 -RRB- -LRB- LR = 88.4 % LP = 89.1 % and F = 88.8 % -RRB- are achieved when the parser utilizes the information about the final punctuation and the look-ahead	det_look-ahead_the conj_and_punctuation_look-ahead amod_punctuation_final det_punctuation_the prep_about_information_look-ahead prep_about_information_punctuation det_information_the dobj_utilizes_information nsubj_utilizes_parser advmod_utilizes_when det_parser_the advcl_achieved_utilizes auxpass_achieved_are nsubjpass_achieved_results num_%_88.8 dep_=_% amod_%_= conj_and_%_F num_%_89.1 dep_=_F dep_=_% amod_LP_= num_%_88.4 dep_=_LP dobj_=_% nsubj_=_LR appos_Roark_2004 conj_and_Collins_Roark dep_results_= prep_of_results_Roark prep_of_results_Collins amod_results_best nn_results_2The ccomp_defined_achieved nsubj_defined_we advcl_defined_tackle det_problem_this dobj_tackle_problem aux_tackle_To
P09-2011	P04-1015	o	2 Incremental Parsing This section gives a description of Collins and Roarks incremental parser -LRB- Collins and Roark 2004 -RRB- and discusses its problem	poss_problem_its dobj_discusses_problem nsubj_discusses_2 amod_Collins_2004 conj_and_Collins_Roark amod_parser_incremental nn_parser_Roarks dep_Collins_Roark dep_Collins_Collins conj_and_Collins_parser prep_of_description_parser prep_of_description_Collins det_description_a conj_and_gives_discusses dobj_gives_description nsubj_gives_2 det_section_This dobj_Parsing_section xcomp_Incremental_Parsing amod_2_Incremental
P09-2011	P04-1015	o	3 Incremental Parsing Method Based on Adjoining Operation In order to avoid the problem of infinite local ambiguity the previous works have adopted the following approaches -LRB- 1 -RRB- a beam search strategy -LRB- Collins and Roark 2004 Roark 2001 Roark 2004 -RRB- -LRB- 2 -RRB- limiting the allowable chains to those actually observed in the treebank -LRB- Collins and Roark 2004 -RRB- and -LRB- 3 -RRB- transforming the parse trees with a selective left-corner transformation -LRB- Johnson and Roark 2000 -RRB- before inducing the allowable chains and allowable triples -LRB- Collins and Roark 2004 -RRB-	amod_Collins_2004 conj_and_Collins_Roark dep_triples_Roark dep_triples_Collins amod_triples_allowable conj_and_chains_triples amod_chains_allowable det_chains_the dobj_inducing_triples dobj_inducing_chains dep_Johnson_2000 conj_and_Johnson_Roark prepc_before_transformation_inducing appos_transformation_Roark appos_transformation_Johnson amod_transformation_left-corner amod_transformation_selective det_transformation_a nn_trees_parse det_trees_the prep_with_transforming_transformation dobj_transforming_trees dep_3_transforming amod_Collins_2004 conj_and_Collins_Roark appos_treebank_Roark appos_treebank_Collins det_treebank_the prep_in_observed_treebank advmod_observed_actually vmod_those_observed amod_chains_allowable det_chains_the conj_and_limiting_3 prep_to_limiting_those dobj_limiting_chains dep_2_3 dep_2_limiting dep_Roark_2004 appos_Roark_2001 dep_Collins_Roark dep_Collins_Roark dep_Collins_2004 conj_and_Collins_Roark appos_strategy_2 appos_strategy_Roark appos_strategy_Collins nn_strategy_search nn_strategy_beam det_strategy_a dep_1_strategy amod_approaches_following det_approaches_the dep_adopted_1 dep_adopted_approaches aux_adopted_have nsubj_adopted_works ccomp_adopted_Based amod_works_previous det_works_the amod_ambiguity_local amod_ambiguity_infinite prep_of_problem_ambiguity det_problem_the dobj_avoid_problem aux_avoid_to dep_avoid_order mark_avoid_In nn_Operation_Adjoining advcl_Based_avoid prep_on_Based_Operation nsubj_Based_Method nn_Method_Parsing amod_Method_Incremental num_Method_3
P09-2011	P04-1015	o	Several incremental parsing methods have been proposed so far -LRB- Collins and Roark 2004 Roark 2001 Roark 2004 -RRB-	amod_Roark_2004 appos_Roark_2001 dep_Collins_Roark conj_and_Collins_Roark conj_and_Collins_2004 conj_and_Collins_Roark dep_far_Roark dep_far_2004 dep_far_Roark dep_far_Collins advmod_far_so advmod_proposed_far auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods nn_methods_parsing amod_methods_incremental amod_methods_Several ccomp_``_proposed
P09-2011	P04-1015	o	The limited contexts used in this model are similar to the previous methods -LRB- Collins and Roark 2004 Roark 2001 Roark 2004 -RRB-	amod_Roark_2004 appos_Roark_2001 dep_Collins_Roark dep_Collins_Roark amod_Collins_2004 conj_and_Collins_Roark appos_methods_Roark appos_methods_Collins amod_methods_previous det_methods_the prep_to_similar_methods cop_similar_are nsubj_similar_contexts det_model_this prep_in_used_model vmod_contexts_used amod_contexts_limited det_contexts_The
P09-2011	P04-1015	p	To achieve efficient parsing we use a beam search strategy like the previous methods -LRB- Collins and Roark 2004 Roark 2001 Roark 2004 -RRB-	amod_Roark_2004 appos_Roark_2001 dep_Collins_Roark dep_Collins_Roark amod_Collins_2004 conj_and_Collins_Roark appos_methods_Roark appos_methods_Collins amod_methods_previous det_methods_the nn_strategy_search nn_strategy_beam det_strategy_a prep_like_use_methods dobj_use_strategy nsubj_use_we advcl_use_achieve amod_parsing_efficient dobj_achieve_parsing aux_achieve_To
P09-2011	P04-1015	o	Each queue Hi stores the only N-best 43 Table 1 Parsing results LR -LRB- % -RRB- LP -LRB- % -RRB- F -LRB- % -RRB- Roark -LRB- 2004 -RRB- 86.4 86.8 86.6 Collins and Roark -LRB- 2004 -RRB- 86.5 86.8 86.7 No adjoining 86.3 86.8 86.6 Non-monotonic adjoining 86.1 87.1 86.6 Monotonic adjoining 87.2 87.7 87.4 partial parse trees	nn_trees_parse amod_trees_partial num_trees_87.4 amod_trees_adjoining nn_trees_Monotonic num_trees_86.6 num_trees_87.1 amod_trees_adjoining amod_trees_Non-monotonic num_trees_86.6 num_trees_86.8 amod_trees_adjoining neg_trees_No dep_trees_86.7 dep_trees_2004 nn_trees_Roark dep_87.4_87.7 number_87.7_87.2 number_87.1_86.1 number_86.8_86.3 dep_86.7_86.8 number_86.8_86.5 conj_and_Collins_trees num_Collins_86.6 dep_86.6_86.8 dep_86.6_2004 dep_86.6_Roark number_86.8_86.4 dep_F_trees dep_F_Collins appos_F_% nn_F_LP appos_LP_% nn_LP_LR nn_LP_results amod_LP_Parsing appos_LR_% dep_Table_F num_Table_1 num_Table_43 nn_Table_N-best amod_Table_only det_Table_the dep_stores_Table nn_stores_Hi nn_stores_queue det_stores_Each
W04-0303	P04-1015	p	This approach has been shown to be accurate relatively efficient and robust using both generative and discriminative models -LRB- Roark 2001 Roark 2004 Collins and Roark 2004 -RRB-	dep_Roark_2004 conj_and_Roark_Roark conj_and_Roark_Collins conj_and_Roark_2004 dep_Roark_Roark dep_Roark_Collins dep_Roark_2004 dep_Roark_Roark appos_Roark_2001 appos_models_Roark amod_models_discriminative amod_models_generative conj_and_generative_discriminative preconj_generative_both dobj_using_models xcomp_robust_using advmod_efficient_relatively conj_and_accurate_robust conj_and_accurate_efficient cop_accurate_be aux_accurate_to xcomp_shown_robust xcomp_shown_efficient xcomp_shown_accurate auxpass_shown_been aux_shown_has nsubjpass_shown_approach det_approach_This
W04-0303	P04-1015	o	Beam-search parsing using an unnormalized discriminative model as in Collins and Roark -LRB- 2004 -RRB- requires a slightly different search strategy than the original generative model described in Roark -LRB- 2001 2004 -RRB-	dep_2001_2004 dep_Roark_2001 prep_in_described_Roark vmod_model_described amod_model_generative amod_model_original det_model_the prep_than_strategy_model nn_strategy_search amod_strategy_different det_strategy_a advmod_different_slightly dobj_requires_strategy nsubj_requires_parsing appos_Roark_2004 conj_and_Collins_Roark pobj_in_Roark pobj_in_Collins pcomp_as_in amod_model_discriminative amod_model_unnormalized det_model_an dobj_using_model prep_parsing_as vmod_parsing_using amod_parsing_Beam-search
W04-0303	P04-1015	o	A generative parsing model can be used on its own and it was shown in Collins and Roark -LRB- 2004 -RRB- that a discriminative parsing model can be used on its own	poss_own_its prep_on_used_own auxpass_used_be aux_used_can nsubjpass_used_model mark_used_that nn_model_parsing amod_model_discriminative det_model_a appos_Roark_2004 conj_and_Collins_Roark ccomp_shown_used prep_in_shown_Roark prep_in_shown_Collins auxpass_shown_was nsubjpass_shown_it poss_own_its conj_and_used_shown prep_on_used_own auxpass_used_be aux_used_can nsubjpass_used_model nn_model_parsing amod_model_generative det_model_A
W05-1505	P04-1015	p	1 Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies -LRB- Collins 2003 Charniak and Johnson 2005 Roark and Collins 2004 -RRB- and have also been shown to be adequate in recovering dependency relationships -LRB- Collins et al. 1999 Levy and Manning 2004 Dubey and Keller 2003 -RRB-	appos_Dubey_2003 conj_and_Dubey_Keller conj_and_Levy_Keller conj_and_Levy_Dubey conj_and_Levy_2004 conj_and_Levy_Manning dep_Collins_Dubey dep_Collins_2004 dep_Collins_Manning dep_Collins_Levy amod_Collins_1999 dep_Collins_al. nn_Collins_et nn_relationships_dependency amod_relationships_recovering prep_in_adequate_relationships cop_adequate_be aux_adequate_to xcomp_shown_adequate auxpass_shown_been advmod_shown_also aux_shown_have num_Roark_2004 conj_and_Roark_Collins dep_Charniak_Collins conj_and_Charniak_shown conj_and_Charniak_Collins conj_and_Charniak_Roark conj_and_Charniak_2005 conj_and_Charniak_Johnson dep_Collins_shown dep_Collins_Roark dep_Collins_2005 dep_Collins_Johnson dep_Collins_Charniak appos_Collins_2003 dep_constituencies_Collins amod_constituencies_labeled amod_constituencies_recovering prep_in_successful_constituencies cop_successful_be aux_successful_to xcomp_shown_successful auxpass_shown_been aux_shown_have nsubjpass_shown_models nn_models_parsing amod_models_Statistical nn_models_Introduction num_models_1
W05-1515	P04-1015	p	Its also worth noting that Collins and Roark -LRB- 2004 -RRB- saw a LFMS improvement of 0.8 % over their baseline discriminative parser after adding punctuation features one of which encoded the sentence-final punctuation	amod_punctuation_sentence-final det_punctuation_the dobj_encoded_punctuation nsubj_encoded_one prep_of_one_which nn_features_punctuation dobj_adding_features amod_parser_discriminative nn_parser_baseline poss_parser_their num_%_0.8 prep_over_improvement_parser prep_of_improvement_% nn_improvement_LFMS det_improvement_a dep_saw_encoded prepc_after_saw_adding dobj_saw_improvement nsubj_saw_Roark nsubj_saw_Collins mark_saw_that appos_Roark_2004 conj_and_Collins_Roark ccomp_noting_saw vmod_worth_noting advmod_worth_also poss_worth_Its ccomp_``_worth
W05-1515	P04-1015	o	Training discriminative parsers is notoriously slow especially if it requires generating examples by repeatedly parsing the treebank -LRB- Collins & Roark 2004 Taskar et al. 2004 -RRB-	num_Taskar_2004 nn_Taskar_al. nn_Taskar_et dep_Collins_Taskar amod_Collins_2004 conj_and_Collins_Roark appos_treebank_Roark appos_treebank_Collins det_treebank_the dobj_parsing_treebank advmod_parsing_repeatedly prepc_by_generating_parsing dobj_generating_examples xcomp_requires_generating nsubj_requires_it mark_requires_if advmod_requires_especially advcl_slow_requires advmod_slow_notoriously cop_slow_is nsubj_slow_parsers amod_parsers_discriminative amod_parsers_Training
W06-1628	P04-1015	o	This combination of the perceptron algorithm with beam-search is similar to that described by Collins and Roark -LRB- 2004 -RRB- .5 The perceptron algorithm is a convenient choice because it converges quickly usually taking only a few iterations over the training set -LRB- Collins 2002 Collins and Roark 2004 -RRB-	amod_Collins_2004 conj_and_Collins_Roark dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 appos_set_Collins nn_set_training det_set_the prep_over_iterations_set amod_iterations_few det_iterations_a advmod_iterations_only dobj_taking_iterations advmod_taking_usually advmod_taking_quickly xcomp_converges_taking nsubj_converges_it mark_converges_because advcl_choice_converges amod_choice_convenient det_choice_a cop_choice_is nsubj_choice_similar nn_algorithm_perceptron det_algorithm_The num_algorithm_.5 dep_algorithm_2004 nn_algorithm_Roark conj_and_Collins_algorithm agent_described_algorithm agent_described_Collins vmod_that_described prep_to_similar_that cop_similar_is nsubj_similar_combination prep_with_algorithm_beam-search nn_algorithm_perceptron det_algorithm_the prep_of_combination_algorithm det_combination_This
W06-2936	P04-1015	o	Using a variant of the voted perceptron -LRB- Collins 2002 Collins and Roark 2004 Crammer and Singer 2003 -RRB- we discriminatively trained our parser in an on-line fashion	amod_fashion_on-line det_fashion_an poss_parser_our prep_in_trained_fashion dobj_trained_parser advmod_trained_discriminatively nsubj_trained_we vmod_trained_Using dep_Crammer_2003 conj_and_Crammer_Singer num_Collins_2004 conj_and_Collins_Roark dep_Collins_Singer dep_Collins_Crammer dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 appos_perceptron_Collins amod_perceptron_voted det_perceptron_the prep_of_variant_perceptron det_variant_a dobj_Using_variant
W06-2936	P04-1015	o	3 Online Learning Again following -LRB- McDonald et al. 2005 -RRB- we have used the single best MIRA -LRB- Crammer and Singer 2003 -RRB- which is a variant of the voted perceptron -LRB- Collins 2002 Collins and Roark 2004 -RRB- for structured prediction	amod_prediction_structured prep_for_Collins_prediction appos_Collins_2004 conj_and_Collins_Roark dep_Collins_Roark dep_Collins_Collins amod_Collins_2002 dep_perceptron_Collins amod_perceptron_voted det_perceptron_the prep_of_variant_perceptron det_variant_a cop_variant_is nsubj_variant_which dep_Crammer_2003 conj_and_Crammer_Singer rcmod_MIRA_variant appos_MIRA_Singer appos_MIRA_Crammer amod_MIRA_best amod_MIRA_single det_MIRA_the dobj_used_MIRA aux_used_have nsubj_used_we dep_used_Again amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et dep_following_McDonald vmod_Again_following nn_Again_Learning nn_Again_Online num_Again_3 advcl_``_used
W06-3603	P04-1015	n	Although generating training examples in advance without a working parser -LRB- Sagae & Lavie 2005 -RRB- is much faster than using inference -LRB- Collins & Roark 2004 Henderson 2004 Taskar et al. 2004 -RRB- our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor	amod_factor_branching amod_factor_lower det_factor_a prep_with_strategy_factor nn_strategy_parsing det_strategy_a dobj_choosing_strategy agent_decreased_choosing advmod_decreased_further auxpass_decreased_be advmod_decreased_probably aux_decreased_can nsubjpass_decreased_time advcl_decreased_faster nn_time_training poss_time_our num_Taskar_2004 nn_Taskar_al. nn_Taskar_et num_Henderson_2004 dep_Collins_Taskar dep_Collins_Henderson amod_Collins_2004 conj_and_Collins_Roark appos_inference_Roark appos_inference_Collins dobj_using_inference prepc_than_faster_using advmod_faster_much cop_faster_is csubj_faster_generating mark_faster_Although dep_Sagae_2005 conj_and_Sagae_Lavie appos_parser_Lavie appos_parser_Sagae amod_parser_working det_parser_a prep_in_examples_advance nn_examples_training prep_without_generating_parser dobj_generating_examples ccomp_``_decreased
W06-3603	P04-1015	o	Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines -LRB- Collins & Roark 2004 Henderson 2004 Taskar et al. 2004 -RRB-	num_Taskar_2004 nn_Taskar_al. nn_Taskar_et num_Henderson_2004 dep_Collins_Taskar conj_and_Collins_Henderson conj_and_Collins_2004 conj_and_Collins_Roark dep_baselines_Henderson dep_baselines_2004 dep_baselines_Roark dep_baselines_Collins amod_baselines_generative prep_above_accuracy_baselines dobj_raise_accuracy nn_time_training conj_and_reduce_raise dobj_reduce_time aux_reduce_to amod_models_generative vmod_used_raise vmod_used_reduce dobj_used_models aux_used_have nsubj_used_parsers amod_parsers_discriminative amod_parsers_Successful ccomp_``_used
W07-1202	P04-1015	o	Parsing research has also begun to adopt discriminative methods from the Machine Learning literature such as the perceptron -LRB- Freund and Schapire 1999 Collins and Roark 2004 -RRB- and the largemargin methods underlying Support Vector Machines -LRB- Taskar et al. 2004 McDonald 2006 -RRB-	amod_McDonald_2006 dep_Taskar_McDonald appos_Taskar_2004 dep_Taskar_al. nn_Taskar_et appos_Machines_Taskar nn_Machines_Vector nn_Machines_Support dobj_underlying_Machines vmod_methods_underlying nn_methods_largemargin det_methods_the dep_Freund_2004 conj_and_Freund_Roark conj_and_Freund_Collins conj_and_Freund_1999 conj_and_Freund_Schapire conj_and_perceptron_methods dep_perceptron_Roark dep_perceptron_Collins dep_perceptron_1999 dep_perceptron_Schapire dep_perceptron_Freund det_perceptron_the prep_such_as_literature_methods prep_such_as_literature_perceptron nn_literature_Learning nn_literature_Machine det_literature_the amod_methods_discriminative prep_from_adopt_literature dobj_adopt_methods aux_adopt_to xcomp_begun_adopt advmod_begun_also aux_begun_has nsubj_begun_research amod_research_Parsing
W07-1202	P04-1015	o	The existing work most similar to ours is Collins and Roark -LRB- 2004 -RRB-	appos_Roark_2004 conj_and_Collins_Roark cop_Collins_is nsubj_Collins_work prep_to_similar_ours advmod_similar_most amod_work_similar amod_work_existing det_work_The
W07-1202	P04-1015	o	1 Introduction A recent development in data-driven parsing is the use of discriminative training methods -LRB- Riezler et al. 2002 Taskar et al. 2004 Collins and Roark 2004 Turian and Melamed 2006 -RRB-	num_Collins_2004 conj_and_Collins_Roark dep_Taskar_2006 conj_and_Taskar_Melamed conj_and_Taskar_Turian conj_and_Taskar_Roark conj_and_Taskar_Collins num_Taskar_2004 nn_Taskar_al. nn_Taskar_et dep_Riezler_Melamed dep_Riezler_Turian dep_Riezler_Collins dep_Riezler_Taskar appos_Riezler_2002 dep_Riezler_al. nn_Riezler_et nn_methods_training amod_methods_discriminative dep_use_Riezler prep_of_use_methods det_use_the cop_use_is nsubj_use_Introduction amod_parsing_data-driven prep_in_development_parsing amod_development_recent det_development_A amod_Introduction_development num_Introduction_1
W07-2211	P04-1015	o	5.1 Relationship to supervised training To illustrate the relationship between the above symbolic training method for preference scoring and corpus-based methods perhaps the easiest way is to compare it to an adaptation -LRB- Collins and Roark 2004 -RRB- of the perceptron training method to the problem of obtaining a best parse -LRB- either directly or for parse reranking -RRB- because the two methods are analogous in a number of ways	prep_of_number_ways det_number_a prep_in_analogous_number cop_analogous_are nsubj_analogous_methods mark_analogous_because num_methods_two det_methods_the nn_reranking_parse pobj_for_reranking conj_or_directly_for dep_directly_either dep_parse_for dep_parse_directly amod_parse_best det_parse_a dobj_obtaining_parse prepc_of_problem_obtaining det_problem_the nn_method_training nn_method_perceptron det_method_the amod_Collins_2004 conj_and_Collins_Roark prep_to_adaptation_problem prep_of_adaptation_method appos_adaptation_Roark appos_adaptation_Collins det_adaptation_an prep_to_compare_adaptation dobj_compare_it aux_compare_to xcomp_is_compare nsubj_is_way amod_way_easiest det_way_the advmod_way_perhaps dobj_scoring_methods conj_and_scoring_corpus-based vmod_preference_corpus-based vmod_preference_scoring prep_for_method_preference nn_method_training amod_method_symbolic amod_method_above det_method_the prep_between_relationship_method det_relationship_the dobj_illustrate_relationship aux_illustrate_To vmod_training_illustrate dobj_supervised_training aux_supervised_to dep_Relationship_analogous rcmod_Relationship_is vmod_Relationship_supervised num_Relationship_5.1 dep_``_Relationship
W08-2129	P04-1015	o	Here it might be useful to relax the strict linear control regime by exploring beam search strategies e.g. along the lines of Collins and Roark -LRB- 2004 -RRB-	appos_Roark_2004 conj_and_Collins_Roark prep_of_lines_Roark prep_of_lines_Collins det_lines_the pobj_along_lines dep_e.g._along nn_strategies_search nn_strategies_beam dobj_exploring_strategies nn_regime_control amod_regime_linear amod_regime_strict det_regime_the prepc_by_relax_exploring dobj_relax_regime aux_relax_to prep_useful_e.g. xcomp_useful_relax cop_useful_be aux_useful_might nsubj_useful_it advmod_useful_Here
W09-0508	P04-1015	o	It is possible to prove that provided the training set -LRB- xi zi -RRB- is separable with margin > 0 the algorithm is assured to converge after a finite number of iterations to a model with zero training errors -LRB- Collins and Roark 2004 -RRB-	amod_Collins_2004 conj_and_Collins_Roark appos_errors_Roark appos_errors_Collins nn_errors_training num_errors_zero prep_with_model_errors det_model_a prep_to_number_model prep_of_number_iterations amod_number_finite det_number_a prep_after_converge_number aux_converge_to xcomp_assured_converge auxpass_assured_is nsubjpass_assured_algorithm det_algorithm_the num_>_0 dep_margin_> ccomp_separable_assured prep_with_separable_margin cop_separable_is nsubj_separable_possible appos_xi_zi dep_set_xi nn_set_training det_set_the dobj_provided_set mark_provided_that ccomp_prove_provided aux_prove_to xcomp_possible_prove cop_possible_is nsubj_possible_It
W09-0508	P04-1015	o	Albeit simple the algorithm has proven to be very efficient and accurate for the task of parse selection -LRB- Collins and Roark 2004 Collins 2004 Zettlemoyer and Collins 2005 Zettlemoyer and Collins 2007 -RRB-	num_Zettlemoyer_2007 conj_and_Zettlemoyer_Collins conj_and_Zettlemoyer_Collins conj_and_Zettlemoyer_Zettlemoyer conj_and_Zettlemoyer_2005 conj_and_Zettlemoyer_Collins num_Collins_2004 dep_Collins_Zettlemoyer dep_Collins_2005 dep_Collins_Collins dep_Collins_Zettlemoyer conj_and_Collins_Collins conj_and_Collins_2004 conj_and_Collins_Roark nn_selection_parse prep_of_task_selection det_task_the prep_for_efficient_task conj_and_efficient_accurate advmod_efficient_very cop_efficient_be aux_efficient_to dep_proven_Collins dep_proven_2004 dep_proven_Roark dep_proven_Collins xcomp_proven_accurate xcomp_proven_efficient aux_proven_has nsubj_proven_algorithm ccomp_proven_simple det_algorithm_the advmod_simple_Albeit
C08-1101	P04-1035	o	6 Related work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective -LRB- Riloff et al. 2003 Pang and Lee 2004 -RRB- and adjacency pair information has been used to predict congressional votes -LRB- Thomas et al. 2006 -RRB-	amod_Thomas_2006 dep_Thomas_al. nn_Thomas_et amod_votes_congressional dobj_predict_votes aux_predict_to xcomp_used_predict auxpass_used_been aux_used_has nsubjpass_used_information nn_information_pair nn_information_adjacency num_Pang_2004 conj_and_Pang_Lee dep_al._Lee dep_al._Pang num_al._2003 nn_al._et amod_al._Riloff cop_subjective/objective_be aux_subjective/objective_should nsubj_subjective/objective_sentence mark_subjective/objective_if amod_sentence_current det_sentence_the conj_and_determine_used dep_determine_al. advcl_determine_subjective/objective aux_determine_to dep_used_Thomas ccomp_used_used ccomp_used_determine advmod_used_previously auxpass_used_been aux_used_has nsubjpass_used_Evidence amod_context_surrounding det_context_the prep_from_Evidence_context nn_Evidence_work amod_Evidence_Related num_Evidence_6
C08-1104	P04-1035	o	Movie-domainSubjectivityDataSet -LRB- Movie -RRB- Pang and Lee -LRB- 2004 -RRB- used a collection of labeled subjective and objective sentences in their work on review classification .5 The data set contains 5000 subjective sentences extracted from movie reviews collected from the Rotten Tomatoes web formed best	dobj_formed_best nn_web_Tomatoes nn_web_Rotten det_web_the prep_from_collected_web vmod_reviews_collected nn_reviews_movie prep_from_extracted_reviews amod_sentences_subjective num_sentences_5000 dobj_contains_sentences nsubj_contains_set nn_set_data det_set_The num_set_.5 rcmod_classification_contains nn_classification_review poss_work_their prep_in_sentences_work amod_sentences_objective amod_sentences_subjective amod_sentences_labeled conj_and_subjective_objective prep_of_collection_sentences det_collection_a dep_used_formed vmod_used_extracted prep_on_used_classification dobj_used_collection nsubj_used_Lee nsubj_used_Pang appos_Lee_2004 conj_and_Pang_Lee dep_Movie-domainSubjectivityDataSet_used appos_Movie-domainSubjectivityDataSet_Movie
C08-1104	P04-1035	o	2 Related Work There has been extensive research in opinion mining at the document level for example on product and movie reviews -LRB- Pang et al. 2002 Pang and Lee 2004 Dave et al. 2003 Popescu and Etzioni 2005 -RRB-	num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_2005 conj_and_Pang_Etzioni conj_and_Pang_Popescu conj_and_Pang_Dave num_Pang_2004 conj_and_Pang_Lee dep_Pang_Etzioni dep_Pang_Popescu dep_Pang_Dave dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_reviews_movie nn_reviews_product conj_and_product_movie nn_level_document det_level_the nn_mining_opinion prep_on_research_reviews prep_for_research_example prep_at_research_level prep_in_research_mining amod_research_extensive cop_research_been aux_research_has expl_research_There dep_Work_Pang rcmod_Work_research amod_Work_Related num_Work_2 dep_``_Work
C08-1135	P04-1035	o	Pang and Lee -LRB- 2004 -RRB- use a graph-based technique to identify and analyze only subjective parts of texts	prep_of_parts_texts amod_parts_subjective advmod_subjective_only dobj_identify_parts conj_and_identify_analyze aux_identify_to amod_technique_graph-based det_technique_a vmod_use_analyze vmod_use_identify dobj_use_technique nsubj_use_Lee nsubj_use_Pang appos_Lee_2004 conj_and_Pang_Lee
C08-2004	P04-1035	o	Within NLP applications include sentiment-analysis problems -LRB- Pang and Lee 2004 Agarwal and Bhattacharyya 2005 Thomas et al. 2006 -RRB- and content selection for text generation -LRB- Barzilay and Lapata 2005 -RRB-	amod_Barzilay_2005 conj_and_Barzilay_Lapata dep_generation_Lapata dep_generation_Barzilay nn_generation_text prep_for_selection_generation nn_selection_content num_Thomas_2006 nn_Thomas_al. nn_Thomas_et conj_and_Agarwal_2005 conj_and_Agarwal_Bhattacharyya conj_and_Pang_Thomas conj_and_Pang_2005 conj_and_Pang_Bhattacharyya conj_and_Pang_Agarwal conj_and_Pang_2004 conj_and_Pang_Lee conj_and_problems_selection dep_problems_Thomas dep_problems_Agarwal dep_problems_2004 dep_problems_Lee dep_problems_Pang nn_problems_sentiment-analysis dobj_include_selection dobj_include_problems nsubj_include_applications prep_within_include_NLP
D07-1035	P04-1035	o	-LRB- 2003 -RRB- Pang and Lee -LRB- 2004 2005 -RRB-	dep_2004_2005 dep_Lee_2004 conj_and_Pang_Lee dep_Pang_2003
D08-1004	P04-1035	o	-LRB- 2007 -RRB- we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales .8 It is based on the dataset of Pang and Lee -LRB- 2004 -RRB- ,9 which consists of 1000 positive and 1000 negative movie reviews tokenized and divided into 10 folds -LRB- F0F9 -RRB-	dep_folds_F0F9 nn_reviews_movie amod_reviews_negative num_reviews_1000 prep_into_positive_10 conj_and_positive_divided conj_and_positive_tokenized conj_and_positive_reviews amod_1000_divided amod_1000_tokenized amod_1000_reviews amod_1000_positive dep_consists_folds prep_of_consists_1000 nsubj_consists_which dep_,9_2004 num_Lee_,9 rcmod_Pang_consists conj_and_Pang_Lee prep_of_dataset_Lee prep_of_dataset_Pang det_dataset_the prep_on_based_dataset auxpass_based_is nsubjpass_based_It rcmod_Rationales_based num_Rationales_.8 dep_Annotator_Rationales prep_with_Enriched_Annotator amod_Dataset_Enriched dep_Polarity_Dataset dep_Review_Polarity dep_Movie_Review dep_the_Movie dobj_introduced_the nsubj_introduced_we dep_introduced_2007
D08-1004	P04-1035	o	We use the same set of binary features as in previous work on this dataset -LRB- Pang et al. 2002 Pang and Lee 2004 Zaidan et al. 2007 -RRB-	num_Zaidan_2007 nn_Zaidan_al. nn_Zaidan_et dep_Pang_Zaidan num_Pang_2004 conj_and_Pang_Lee dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et det_dataset_this amod_work_previous pobj_in_work pcomp_as_in amod_features_binary prep_of_set_features amod_set_same det_set_the dep_use_Pang prep_on_use_dataset prep_use_as dobj_use_set nsubj_use_We ccomp_``_use
D08-1004	P04-1035	o	We collect substring rationales for a sentiment classification task -LRB- Pang and Lee 2004 -RRB- and use them to obtain significant accuracy improvements for each annotator	det_annotator_each prep_for_improvements_annotator nn_improvements_accuracy amod_improvements_significant dobj_obtain_improvements aux_obtain_to vmod_use_obtain dobj_use_them nsubj_use_We num_Pang_2004 conj_and_Pang_Lee appos_task_Lee appos_task_Pang nn_task_classification nn_task_sentiment det_task_a amod_rationales_substring conj_and_collect_use prep_for_collect_task dobj_collect_rationales nsubj_collect_We
D08-1058	P04-1035	o	-LRB- 2002 -RRB- various classification models and linguistic features have been proposed to improve the classification performance -LRB- Pang and Lee 2004 Mullen and Collier 2004 Wilson et al. 2005a Read 2005 -RRB-	dep_Read_2005 appos_Wilson_2005a dep_Wilson_al. nn_Wilson_et dep_Mullen_Read conj_and_Mullen_Wilson conj_and_Mullen_2004 conj_and_Mullen_Collier conj_and_Pang_Wilson conj_and_Pang_2004 conj_and_Pang_Collier conj_and_Pang_Mullen conj_and_Pang_2004 conj_and_Pang_Lee dep_performance_Mullen dep_performance_2004 dep_performance_Lee dep_performance_Pang nn_performance_classification det_performance_the dobj_improve_performance aux_improve_to xcomp_proposed_improve auxpass_proposed_been aux_proposed_have nsubjpass_proposed_features nsubjpass_proposed_models amod_features_linguistic conj_and_models_features nn_models_classification amod_models_various dep_models_2002
D09-1017	P04-1035	o	With this model we can provide not only qualitative textual summarization such as good food and bad service but also a numerical scoring of sentiment i.e. how good the food is and how bad the service is 2 Related Work There have been many studies on sentiment classification and opinion summarization -LRB- Pang and Lee 2004 2005 Gamon et al. 2005 Popescu and Etzioni 2005 Liu et al. 2005 Zhuang et al. 2006 Kim and Hovy 2006 -RRB-	amod_Kim_2006 conj_and_Kim_Hovy num_Zhuang_2006 nn_Zhuang_al. nn_Zhuang_et num_Liu_2005 nn_Liu_al. nn_Liu_et dep_Popescu_Hovy dep_Popescu_Kim conj_and_Popescu_Zhuang conj_and_Popescu_Liu conj_and_Popescu_2005 conj_and_Popescu_Etzioni num_Gamon_2005 nn_Gamon_al. nn_Gamon_et dep_Pang_Zhuang dep_Pang_Liu dep_Pang_2005 dep_Pang_Etzioni dep_Pang_Popescu conj_and_Pang_Gamon num_Pang_2005 num_Pang_2004 conj_and_Pang_Lee dep_summarization_Gamon dep_summarization_Lee dep_summarization_Pang nn_summarization_opinion conj_and_classification_summarization nn_classification_sentiment prep_on_studies_summarization prep_on_studies_classification amod_studies_many cop_studies_been aux_studies_have expl_studies_There dep_Work_studies amod_Work_Related num_Work_2 dep_Work_i.e. nsubj_is_service dep_is_bad det_service_the advmod_bad_how conj_and_is_is nsubj_is_food dep_is_good det_food_the advmod_good_how dep_i.e._is dep_i.e._is dobj_scoring_Work prep_of_scoring_sentiment amod_scoring_numerical det_scoring_a nsubj_scoring_we amod_service_bad conj_and_food_service amod_food_good prep_such_as_summarization_service prep_such_as_summarization_food amod_summarization_textual amod_summarization_qualitative advmod_qualitative_only neg_qualitative_not conj_and_provide_scoring dobj_provide_summarization aux_provide_can nsubj_provide_we prep_with_provide_model det_model_this
D09-1018	P04-1035	o	Others use sentence cohesion -LRB- Pang and Lee 2004 -RRB- agreement/disagreement between speakers -LRB- Thomas et al. 2006 Bansal et al. 2008 -RRB- or structural adjacency	amod_adjacency_structural num_Bansal_2008 nn_Bansal_al. nn_Bansal_et dep_Thomas_Bansal num_Thomas_2006 dep_Thomas_al. nn_Thomas_et conj_or_agreement/disagreement_adjacency appos_agreement/disagreement_Thomas prep_between_agreement/disagreement_speakers dep_Pang_2004 conj_and_Pang_Lee dep_cohesion_Lee dep_cohesion_Pang nn_cohesion_sentence dep_use_adjacency dep_use_agreement/disagreement dobj_use_cohesion nsubj_use_Others
D09-1018	P04-1035	o	et al. 2007 -RRB- -RRB- and unigrams -LRB- used by many researchers e.g. -LRB- Pang and Lee 2004 -RRB- -RRB-	dep_Pang_2004 conj_and_Pang_Lee amod_researchers_many dep_used_Lee dep_used_Pang dep_used_e.g. prep_by_used_researchers dep_unigrams_used conj_and_2007_unigrams dep_2007_al. nn_al._et
D09-1020	P04-1035	p	Second benefits for sentiment analysis can be realized by decomposing the problem into S/O -LRB- or neutral versus polar -RRB- and polarity classification -LRB- Yu and Hatzivassiloglou 2003 Pang and Lee 2004 Wilson et al. 2005a Kim and Hovy 2006 -RRB-	amod_Kim_2006 conj_and_Kim_Hovy appos_Wilson_2005a dep_Wilson_al. nn_Wilson_et num_Pang_2004 conj_and_Pang_Lee dep_Yu_Hovy dep_Yu_Kim conj_and_Yu_Wilson conj_and_Yu_Lee conj_and_Yu_Pang conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_classification_Wilson dep_classification_Pang dep_classification_2003 dep_classification_Hatzivassiloglou dep_classification_Yu nn_classification_polarity conj_versus_neutral_polar conj_and_S/O_classification dep_S/O_polar dep_S/O_neutral det_problem_the prep_into_decomposing_classification prep_into_decomposing_S/O dobj_decomposing_problem agent_realized_decomposing auxpass_realized_be aux_realized_can nsubjpass_realized_benefits advmod_realized_Second nn_analysis_sentiment prep_for_benefits_analysis ccomp_``_realized
E06-1025	P04-1035	o	This amounts to performing binary text categorization under categories Objective and Subjective -LRB- Pang and Lee 2004 Yu and Hatzivassiloglou 2003 -RRB- 2	dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Pang_Hatzivassiloglou dep_Pang_Yu conj_and_Pang_2004 conj_and_Pang_Lee dep_Objective_2004 dep_Objective_Lee dep_Objective_Pang conj_and_Objective_Subjective dep_categories_Subjective dep_categories_Objective prep_under_categorization_categories nn_categorization_text amod_categorization_binary dobj_performing_categorization conj_amounts_2 prepc_to_amounts_performing nsubj_amounts_This
E06-1025	P04-1035	o	determining document orientation -LRB- or polarity -RRB- as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter -LRB- Pang and Lee 2004 Turney 2002 -RRB- 3	amod_Turney_2002 dep_Pang_Turney conj_and_Pang_2004 conj_and_Pang_Lee amod_matter_subject poss_matter_its amod_opinion_Negative det_opinion_a dep_Positive_2004 dep_Positive_Lee dep_Positive_Pang prep_on_Positive_matter conj_or_Positive_opinion amod_a_opinion amod_a_Positive dobj_expresses_a nsubj_expresses_text mark_expresses_if amod_text_Subjective amod_text_given det_text_a advcl_deciding_expresses pcomp_in_deciding pcomp_as_in cc_polarity_or appos_orientation_polarity nn_orientation_document conj_determining_3 prep_determining_as dobj_determining_orientation dep_``_determining
E09-1004	P04-1035	o	2 Literature Survey The task of sentiment analysis has evolved from document level analysis -LRB- e.g. -LRB- Turney. 2002 -RRB- -LRB- Pang and Lee 2004 -RRB- -RRB- to sentence level analysis -LRB- e.g. -LRB- Hu and Liu. 2004 -RRB- -LRB- Kim and Hovy. 2004 -RRB- -LRB- Yu and Hatzivassiloglou 2003 -RRB- -RRB-	dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Kim_2004 conj_and_Kim_Hovy. dep_Hu_2004 conj_and_Hu_Liu. dep_e.g._Hatzivassiloglou dep_e.g._Yu dep_e.g._Hovy. dep_e.g._Kim appos_e.g._Liu. appos_e.g._Hu nn_analysis_level nn_analysis_sentence dep_Pang_2004 conj_and_Pang_Lee dep_Turney._e.g. prep_to_Turney._analysis dep_Turney._Lee dep_Turney._Pang amod_Turney._2002 dep_,_Turney. dep_-LRB-_e.g. nn_analysis_level nn_analysis_document prep_from_evolved_analysis aux_evolved_has nsubj_evolved_task nn_analysis_sentiment prep_of_task_analysis det_task_The rcmod_Survey_evolved nn_Survey_Literature num_Survey_2
E09-1077	P04-1035	o	Mincuts have been used 4As of this writing WordNet is available for more than 40 world languages -LRB- http://www.globalwordnet.org -RRB- Figure 2 Semi-supervised classification using mincuts in semi-supervised learning for various tasks including document level sentiment analysis -LRB- Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_analysis_Lee dep_analysis_Pang nn_analysis_sentiment nn_analysis_level nn_analysis_document amod_tasks_various prep_for_learning_tasks amod_learning_semi-supervised prep_in_mincuts_learning dobj_using_mincuts prep_including_classification_analysis vmod_classification_using amod_classification_Semi-supervised num_Figure_2 dep_languages_Figure appos_languages_http://www.globalwordnet.org nn_languages_world num_languages_40 quantmod_40_than mwe_than_more dep_available_classification prep_for_available_languages cop_available_is nsubj_available_WordNet advcl_available_used det_writing_this prep_of_4As_writing dobj_used_4As auxpass_used_been aux_used_have nsubjpass_used_Mincuts
H05-1042	P04-1035	o	This formulation is similar to the energy minimization framework which is commonly used in image analysis -LRB- Besag 1986 Boykov et al. 1999 -RRB- and has been recently applied in natural language processing -LRB- Pang and Lee 2004 -RRB-	num_Pang_2004 conj_and_Pang_Lee appos_processing_Lee appos_processing_Pang nn_processing_language amod_processing_natural prep_in_applied_processing advmod_applied_recently auxpass_applied_been aux_applied_has nsubjpass_applied_which num_Boykov_1999 nn_Boykov_al. nn_Boykov_et dep_Besag_Boykov appos_Besag_1986 dep_analysis_Besag nn_analysis_image conj_and_used_applied prep_in_used_analysis advmod_used_commonly auxpass_used_is nsubjpass_used_which rcmod_framework_applied rcmod_framework_used nn_framework_minimization nn_framework_energy det_framework_the prep_to_similar_framework cop_similar_is nsubj_similar_formulation det_formulation_This
H05-1044	P04-1035	o	7 Related Work Much work on sentiment analysis classifies documents by their overall sentiment for example determining whether a review is positive or negative -LRB- e.g. -LRB- Turney 2002 Dave et al. 2003 Pang and Lee 2004 Beineke et al. 2004 -RRB- -RRB-	num_Beineke_2004 nn_Beineke_al. nn_Beineke_et num_Pang_2004 conj_and_Pang_Lee dep_Dave_Beineke dep_Dave_Lee dep_Dave_Pang num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Turney_Dave dep_Turney_2002 dep_e.g._Turney nsubj_negative_review dep_positive_e.g. conj_or_positive_negative cop_positive_is nsubj_positive_review mark_positive_whether det_review_a ccomp_determining_negative ccomp_determining_positive vmod_example_determining pobj_for_example ccomp_,_for amod_sentiment_overall poss_sentiment_their prep_by_classifies_sentiment dobj_classifies_documents nsubj_classifies_work nn_analysis_sentiment prep_on_work_analysis amod_work_Much rcmod_Work_classifies amod_Work_Related num_Work_7 dep_``_Work
H05-1045	P04-1035	o	-LRB- 2003 -RRB- Pang and Lee -LRB- 2004 -RRB- -RRB-	appos_Lee_2004 conj_and_Pang_Lee dep_Pang_2003
H05-1045	P04-1035	o	-LRB- 2003 -RRB- Pang and Lee -LRB- 2004 -RRB- Wilson et al.	nn_al._et nn_al._Wilson appos_Lee_2004 dep_Pang_al. conj_and_Pang_Lee dep_Pang_2003
H05-1073	P04-1035	o	-LRB- Turney 2002 -RRB- -LRB- Bai Padman and Airoldi 2004 -RRB- -LRB- Beineke Hastie and Vaithyanathan 2003 -RRB- -LRB- Mullen and Collier 2003 -RRB- -LRB- Pang and Lee 2003 -RRB-	amod_Pang_2003 conj_and_Pang_Lee dep_Mullen_2003 conj_and_Mullen_Collier dep_Beineke_2003 conj_and_Beineke_Vaithyanathan conj_and_Beineke_Hastie dep_Bai_2004 conj_and_Bai_Airoldi conj_and_Bai_Padman appos_Turney_Lee appos_Turney_Pang appos_Turney_Collier appos_Turney_Mullen appos_Turney_Vaithyanathan appos_Turney_Hastie appos_Turney_Beineke appos_Turney_Airoldi appos_Turney_Padman appos_Turney_Bai amod_Turney_2002 dep_''_Turney
H05-1115	P04-1035	p	Recently graph-based methods have proved useful for a number of NLP and IR tasks such as document re-ranking in ad hoc IR -LRB- Kurland and Lee 2005 -RRB- and analyzing sentiments in text -LRB- Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_text_Lee dep_text_Pang prep_in_sentiments_text dobj_analyzing_sentiments nsubj_analyzing_methods num_Kurland_2005 conj_and_Kurland_Lee dep_IR_Lee dep_IR_Kurland nn_IR_hoc nn_IR_ad prep_in_re-ranking_IR nn_re-ranking_document prep_such_as_tasks_re-ranking nn_tasks_IR nn_tasks_NLP conj_and_NLP_IR prep_of_number_tasks det_number_a prep_for_useful_number conj_and_proved_analyzing acomp_proved_useful aux_proved_have nsubj_proved_methods advmod_proved_Recently amod_methods_graph-based
H05-1116	P04-1035	o	-LRB- 2003 -RRB- Pang and Lee -LRB- 2004 -RRB- -RRB-	appos_Lee_2004 conj_and_Pang_Lee dep_Pang_2003
H05-1116	P04-1035	o	-LRB- 2004 -RRB- Pang and Lee -LRB- 2004 -RRB- Wilson et al.	nn_al._et nn_al._Wilson appos_Lee_2004 dep_Pang_al. conj_and_Pang_Lee dep_Pang_2004
I05-2030	P04-1035	o	-LRB- Dave et al. 2003 Pang and Lee 2004 Turney 2002 -RRB- -RRB-	dep_Turney_2002 dep_Pang_Turney num_Pang_2004 conj_and_Pang_Lee dep_Dave_Lee dep_Dave_Pang appos_Dave_2003 dep_Dave_al. nn_Dave_et dep_''_Dave
I08-1039	P04-1035	o	5 Evaluation 5.1 Datasets We used two datasets customer reviews 1 -LRB- Hu and Liu 2004 -RRB- and movie reviews 2 -LRB- Pang and Lee 2005 -RRB- to evaluate sentiment classification of sentences	prep_of_classification_sentences nn_classification_sentiment dobj_evaluate_classification aux_evaluate_to dep_Pang_2005 conj_and_Pang_Lee dep_reviews_Lee dep_reviews_Pang num_reviews_2 nn_reviews_movie dep_Hu_2004 conj_and_Hu_Liu appos_reviews_Liu appos_reviews_Hu num_reviews_1 nn_reviews_customer conj_and_datasets_reviews conj_and_datasets_reviews num_datasets_two vmod_used_evaluate dobj_used_reviews dobj_used_reviews dobj_used_datasets nsubj_used_We rcmod_Datasets_used num_Datasets_5.1 nn_Datasets_Evaluation num_Datasets_5 dep_``_Datasets
I08-1039	P04-1035	o	Pang and Lee -LRB- 2004 -RRB- proposed to eliminate objective sentences before the sentiment classification of documents	prep_of_classification_documents nn_classification_sentiment det_classification_the amod_sentences_objective prep_before_eliminate_classification dobj_eliminate_sentences aux_eliminate_to xcomp_proposed_eliminate appos_Lee_2004 vmod_Pang_proposed conj_and_Pang_Lee
I08-1040	P04-1035	o	There has also been previous work on determining whether a given text is factual or expresses opinion -LRB- Yu & Hatzivassiloglu 2003 Pang & Lee 2004 -RRB- again this work uses a binary distinction and supervised rather than unsupervised approaches	amod_approaches_unsupervised conj_negcc_supervised_approaches nsubj_supervised_work amod_distinction_binary det_distinction_a conj_and_uses_approaches conj_and_uses_supervised dobj_uses_distinction nsubj_uses_work advmod_uses_again det_work_this dep_Pang_2004 conj_and_Pang_Lee dep_Yu_Lee dep_Yu_Pang conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglu appos_opinion_2003 appos_opinion_Hatzivassiloglu appos_opinion_Yu nsubj_expresses_text dep_factual_opinion conj_or_factual_expresses cop_factual_is nsubj_factual_text mark_factual_whether amod_text_given det_text_a ccomp_determining_expresses ccomp_determining_factual parataxis_work_supervised parataxis_work_uses prepc_on_work_determining amod_work_previous cop_work_been advmod_work_also aux_work_has expl_work_There
I08-1041	P04-1035	p	SVM has been shown to be useful for text classification tasks -LRB- Joachims 1998 -RRB- and has previously given good performance in sentiment classification experiments -LRB- Kennedy and Inkpen 2006 Mullen and Collier 2004 Pang and Lee 2004 Pang et al. 2002 -RRB-	num_Pang_2002 nn_Pang_al. nn_Pang_et num_Pang_2004 conj_and_Pang_Lee dep_Mullen_Pang conj_and_Mullen_Lee conj_and_Mullen_Pang conj_and_Mullen_2004 conj_and_Mullen_Collier dep_Kennedy_Pang dep_Kennedy_2004 dep_Kennedy_Collier dep_Kennedy_Mullen conj_and_Kennedy_2006 conj_and_Kennedy_Inkpen dep_experiments_2006 dep_experiments_Inkpen dep_experiments_Kennedy nn_experiments_classification nn_experiments_sentiment prep_in_performance_experiments amod_performance_good dobj_given_performance advmod_given_previously aux_given_has nsubjpass_given_SVM amod_Joachims_1998 dep_tasks_Joachims nn_tasks_classification nn_tasks_text prep_for_useful_tasks cop_useful_be aux_useful_to conj_and_shown_given xcomp_shown_useful auxpass_shown_been aux_shown_has nsubjpass_shown_SVM
N06-1027	P04-1035	o	Inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graph-based approaches on keyword selection -LRB- Mihalcea and Tarau 2004 -RRB- text summarization -LRB- Erkan and Radev 2004 Mihalcea 2004 -RRB- word sense disambiguation -LRB- Mihalcea et al. 2004 Mihalcea 2005 -RRB- sentiment analysis -LRB- Pang and Lee 2004 -RRB- and sentence retrieval for question answering -LRB- Otterbacher et al. 2005 -RRB-	amod_Otterbacher_2005 dep_Otterbacher_al. nn_Otterbacher_et dep_answering_Otterbacher dep_question_answering nn_retrieval_sentence dep_Pang_2004 conj_and_Pang_Lee appos_analysis_Lee appos_analysis_Pang nn_analysis_sentiment dep_Mihalcea_2005 dep_Mihalcea_Mihalcea appos_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et appos_disambiguation_Mihalcea nn_disambiguation_sense nn_disambiguation_word dep_Mihalcea_2004 dep_Erkan_Mihalcea conj_and_Erkan_2004 conj_and_Erkan_Radev appos_summarization_2004 appos_summarization_Radev appos_summarization_Erkan nn_summarization_text dep_Mihalcea_2004 conj_and_Mihalcea_Tarau dep_selection_Tarau dep_selection_Mihalcea amod_selection_keyword conj_and_approaches_retrieval conj_and_approaches_analysis conj_and_approaches_disambiguation conj_and_approaches_summarization prep_on_approaches_selection amod_approaches_graph-based prep_for_applied_question dobj_applied_retrieval dobj_applied_analysis dobj_applied_disambiguation dobj_applied_summarization dobj_applied_approaches aux_applied_have nsubj_applied_candidate nn_community_language amod_community_natural det_community_the prep_in_efforts_community nn_efforts_research appos_candidate_efforts amod_candidate_best det_candidate_the ccomp_select_applied conj_and_rank_select advmod_rank_collectively aux_rank_to vmod_algorithms_select vmod_algorithms_rank dobj_based_algorithms vmod_idea_based prep_of_idea_graph det_idea_the prep_by_Inspired_idea ccomp_``_Inspired
N07-1013	P04-1035	o	For examples see -LRB- Erkan and Radev 2004 Mihalcea and Tarau 2004 Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_Mihalcea_Lee dep_Mihalcea_Pang conj_and_Mihalcea_2004 conj_and_Mihalcea_Tarau dep_Erkan_2004 dep_Erkan_Tarau dep_Erkan_Mihalcea conj_and_Erkan_2004 conj_and_Erkan_Radev dep_see_2004 dep_see_Radev dep_see_Erkan prep_for_see_examples
N07-1026	P04-1035	o	2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems minimum cuts -LRB- Pang and Lee 2004 -RRB- random walks -LRB- Mihalcea 2005 Otterbacher et al. 2005 -RRB- graph matching -LRB- Haghighi et al. 2005 -RRB- and label propagation -LRB- Niu et al. 2005 -RRB-	amod_Niu_2005 dep_Niu_al. nn_Niu_et dep_propagation_Niu nn_propagation_label amod_Haghighi_2005 dep_Haghighi_al. nn_Haghighi_et conj_and_matching_propagation dep_matching_Haghighi nn_matching_graph num_Otterbacher_2005 nn_Otterbacher_al. nn_Otterbacher_et dep_Mihalcea_Otterbacher dep_Mihalcea_2005 dep_walks_propagation dep_walks_matching dep_walks_Mihalcea dep_walks_random nsubj_walks_cuts dep_Pang_2004 conj_and_Pang_Lee appos_cuts_Lee appos_cuts_Pang amod_cuts_minimum nn_problems_NLP prep_to_applied_problems nsubjpass_applied_techniques parataxis_developed_walks conj_and_developed_applied auxpass_developed_been advmod_developed_recently aux_developed_have nsubjpass_developed_techniques nn_techniques_learning amod_techniques_graph-based amod_techniques_Several nn_techniques_Background num_techniques_2
N07-1033	P04-1035	o	-LRB- 2002 -RRB- and Pang and Lee -LRB- 2004 -RRB- in merely using binary unigram features corresponding to the 17,744 unstemmed word or punctuation types with count 4 in the full 2000-document corpus	amod_corpus_2000-document amod_corpus_full det_corpus_the num_count_4 nn_types_punctuation prep_in_word_corpus prep_with_word_count conj_or_word_types amod_word_unstemmed num_word_17,744 det_word_the prep_to_corresponding_types prep_to_corresponding_word nn_features_unigram amod_features_binary dobj_using_features advmod_using_merely appos_Lee_2004 vmod_2002_corresponding prepc_in_2002_using conj_and_2002_Lee conj_and_2002_Pang dep_''_Lee dep_''_Pang dep_''_2002
N07-1033	P04-1035	p	We chose a dataset that would be enjoyable to reannotate the movie review dataset of -LRB- Pang et al. 2002 Pang and Lee 2004 -RRB- .3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database -LRB- IMDb -RRB- review archive all written before 2002 by a total of 312 authors with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable	advmod_valuable_equally cop_valuable_are nsubj_valuable_rationales mark_valuable_that det_rationales_all ccomp_means_valuable amod_means_constant cop_means_be aux_means_to nn_Ccontrast_2Taking prep_per_author_Ccontrast prep_per_reviews_author num_reviews_20 prep_of_cap_reviews det_cap_a num_authors_312 prep_of_total_authors det_total_a agent_written_total prep_before_written_2002 vmod_all_written vmod_archive_means prep_with_archive_cap appos_archive_all nn_archive_review nn_archive_IMDb dep_Database_archive nn_Database_Movie nn_Database_Internet det_Database_the prep_from_obtained_Database nn_reviews_movie amod_reviews_negative num_reviews_1000 dep_positive_obtained conj_and_positive_reviews amod_1000_reviews amod_1000_positive prep_of_consists_1000 nsubj_consists_dataset det_dataset_The rcmod_.3_consists dep_Pang_2004 conj_and_Pang_Lee dep_Pang_.3 dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_dataset_Pang nn_dataset_review nn_dataset_movie det_dataset_the prep_to_enjoyable_reannotate cop_enjoyable_be aux_enjoyable_would nsubj_enjoyable_that dep_dataset_dataset rcmod_dataset_enjoyable det_dataset_a dobj_chose_dataset nsubj_chose_We
N07-1039	P04-1035	o	Sentiment analysis includes a variety of different problems including sentiment classification techniques to classify reviews as positive or negative based on bag of words -LRB- Pang et al. 2002 -RRB- or positive and negative words -LRB- Turney 2002 Mullen and Collier 2004 -RRB- classifying sentences in a document as either subjective or objective -LRB- Riloff and Wiebe 2003 Pang and Lee 2004 -RRB- identifying or classifying appraisal targets -LRB- Nigam and Hurst 2004 -RRB- identifying the source of an opinion in a text -LRB- Choi et al. 2005 -RRB- whether the author is expressing the opinion or whether he is attributing the opinion to someone else and developing interactive and visual opinion mining methods -LRB- Gamon et al. 2005 Popescu and Etzioni 2005 -RRB-	dep_Popescu_2005 conj_and_Popescu_Etzioni dep_Gamon_Etzioni dep_Gamon_Popescu appos_Gamon_2005 dep_Gamon_al. nn_Gamon_et dep_methods_Gamon nn_methods_mining nn_methods_opinion amod_methods_visual amod_methods_interactive conj_and_interactive_visual dobj_developing_methods advmod_someone_else det_opinion_the prep_to_attributing_someone dobj_attributing_opinion aux_attributing_is nsubj_attributing_he mark_attributing_whether det_opinion_the conj_or_expressing_attributing dobj_expressing_opinion aux_expressing_is nsubj_expressing_author mark_expressing_whether det_author_the amod_Choi_2005 dep_Choi_al. nn_Choi_et det_text_a prep_in_opinion_text det_opinion_an prep_of_source_opinion det_source_the ccomp_identifying_attributing ccomp_identifying_expressing dep_identifying_Choi dobj_identifying_source dep_Nigam_2004 conj_and_Nigam_Hurst dep_targets_Hurst dep_targets_Nigam nn_targets_appraisal dobj_identifying_targets conj_or_identifying_classifying amod_Pang_2004 conj_and_Pang_Lee dep_Riloff_Lee dep_Riloff_Pang dep_Riloff_2003 conj_and_Riloff_Wiebe dep_subjective_Wiebe dep_subjective_Riloff conj_or_subjective_objective preconj_subjective_either det_document_a prep_as_sentences_objective prep_as_sentences_subjective prep_in_sentences_document amod_sentences_classifying dep_Mullen_2004 conj_and_Mullen_Collier dep_Turney_Collier dep_Turney_Mullen appos_Turney_2002 appos_words_Turney dep_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_bag_words conj_or_positive_negative prep_as_classify_negative prep_as_classify_positive dobj_classify_reviews aux_classify_to conj_and_techniques_developing conj_and_techniques_identifying conj_and_techniques_classifying conj_and_techniques_identifying conj_and_techniques_sentences dep_techniques_words conj_and_techniques_negative conj_or_techniques_positive dep_techniques_Pang prep_based_on_techniques_bag vmod_techniques_classify nn_techniques_classification nn_techniques_sentiment prep_including_problems_developing prep_including_problems_identifying prep_including_problems_identifying prep_including_problems_sentences prep_including_problems_negative prep_including_problems_positive prep_including_problems_techniques amod_problems_different prep_of_variety_problems det_variety_a dobj_includes_variety nsubj_includes_analysis nn_analysis_Sentiment
N09-1001	P04-1035	o	2 Related Work There has been a large and diverse body of research in opinion mining with most research at the text -LRB- Pang et al. 2002 Pang and Lee 2004 Popescu and Etzioni 2005 Ounis et al. 2006 -RRB- sentence -LRB- Kim and Hovy 2005 Kudo and Matsumoto 2004 Riloff et al. 2003 Yu and Hatzivassiloglou 2003 -RRB- or word -LRB- Hatzivassiloglou and McKeown 1997 Turney and Littman 2003 Kim and Hovy 2004 Takamura et al. 2005 Andreevskaia and Bergler 2006 Kaji and Kitsuregawa 2007 -RRB- level	dep_level_Kitsuregawa dep_level_Kaji dep_level_2006 dep_level_Bergler dep_level_Andreevskaia dep_Andreevskaia_2007 conj_and_Andreevskaia_Kitsuregawa conj_and_Andreevskaia_Kaji conj_and_Andreevskaia_2006 conj_and_Andreevskaia_Bergler num_Takamura_2005 nn_Takamura_al. nn_Takamura_et num_Kim_2004 conj_and_Kim_Hovy dep_Turney_level conj_and_Turney_Takamura conj_and_Turney_Hovy conj_and_Turney_Kim conj_and_Turney_2003 conj_and_Turney_Littman dep_Hatzivassiloglou_Takamura dep_Hatzivassiloglou_Kim dep_Hatzivassiloglou_2003 dep_Hatzivassiloglou_Littman dep_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown dep_word_1997 dep_word_McKeown dep_word_Hatzivassiloglou dep_Yu_2003 conj_and_Yu_Hatzivassiloglou num_Riloff_2003 nn_Riloff_al. nn_Riloff_et num_Kudo_2004 conj_and_Kudo_Matsumoto dep_Kim_Hatzivassiloglou dep_Kim_Yu conj_and_Kim_Riloff conj_and_Kim_Matsumoto conj_and_Kim_Kudo conj_and_Kim_2005 conj_and_Kim_Hovy dep_sentence_Riloff dep_sentence_Kudo dep_sentence_2005 dep_sentence_Hovy dep_sentence_Kim num_Ounis_2006 nn_Ounis_al. nn_Ounis_et conj_or_Popescu_word conj_and_Popescu_sentence conj_and_Popescu_Ounis conj_and_Popescu_2005 conj_and_Popescu_Etzioni dep_Pang_word dep_Pang_sentence dep_Pang_Ounis dep_Pang_2005 dep_Pang_Etzioni dep_Pang_Popescu num_Pang_2004 conj_and_Pang_Lee dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et det_text_the amod_research_most nn_mining_opinion dep_body_Pang prep_at_body_text prep_with_body_research prep_in_body_mining prep_of_body_research amod_body_diverse amod_body_large det_body_a cop_body_been aux_body_has expl_body_There dep_body_Work conj_and_large_diverse amod_Work_Related num_Work_2
N09-1001	P04-1035	o	Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level -LRB- Pang and Lee 2004 Agarwal and Bhattacharyya 2005 Thomas et al. 2006 -RRB- instead of aiming at dictionary annotation as we do	nsubj_do_we mark_do_as nn_annotation_dictionary advcl_aiming_do prep_at_aiming_annotation num_Thomas_2006 nn_Thomas_al. nn_Thomas_et conj_negcc_Agarwal_aiming conj_and_Agarwal_Thomas conj_and_Agarwal_2005 conj_and_Agarwal_Bhattacharyya dep_Pang_aiming dep_Pang_Thomas dep_Pang_2005 dep_Pang_Bhattacharyya dep_Pang_Agarwal conj_and_Pang_2004 conj_and_Pang_Lee nn_level_document dep_sentence_2004 dep_sentence_Lee dep_sentence_Pang conj_and_sentence_level det_sentence_the prep_at_used_level prep_at_used_sentence advmod_used_mostly auxpass_used_been aux_used_have nsubjpass_used_algorithms nn_units_language amod_units_positive/negative amod_units_subjective/objective conj_or_subjective/objective_positive/negative prep_into_classification_units prep_for_algorithms_classification amod_algorithms_Graph-based
N09-1001	P04-1035	o	We also can not use prior graph construction methods for the document level -LRB- such as physical proximity of sentences used in Pang and Lee -LRB- 2004 -RRB- -RRB- at the word sense level	nn_level_sense nn_level_word det_level_the appos_Lee_2004 conj_and_Pang_Lee prep_at_used_level prep_in_used_Lee prep_in_used_Pang ccomp_,_used prep_of_proximity_sentences amod_proximity_physical prep_such_as_level_proximity nn_level_document det_level_the nn_methods_construction nn_methods_graph amod_methods_prior prep_for_use_level dobj_use_methods neg_use_not aux_use_can advmod_use_also nsubj_use_We ccomp_``_use
N09-1001	P04-1035	o	W -LRB- S T -RRB- = summationdisplay uS vT w -LRB- u v -RRB- Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice using the maximum flow algorithm -LRB- Pang and Lee 2004 Cormen et al. 2002 -RRB-	num_Cormen_2002 nn_Cormen_al. nn_Cormen_et dep_Pang_Cormen conj_and_Pang_2004 conj_and_Pang_Lee dep_algorithm_2004 dep_algorithm_Lee dep_algorithm_Pang nn_algorithm_flow nn_algorithm_maximum det_algorithm_the dobj_using_algorithm prep_in_time_practice amod_time_running amod_time_near-linear amod_time_polynomial conj_and_found_time prep_in_found_time auxpass_found_be aux_found_can nsubjpass_found_cuts nn_cuts_minimum amod_cuts_optimal advmod_cuts_Globally vmod_u_using rcmod_u_time rcmod_u_found appos_u_v dep_w_u nn_w_vT appos_uS_w nn_uS_summationdisplay dep_=_uS appos_S_T dep_W_= dep_W_S dep_``_W
N09-1002	P04-1035	o	In fact researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification -LRB- Yu and Hatzivassiloglou 2003 Pang and Lee 2004 Wilson et al. 2005 Kim and Hovy 2006 -RRB-	amod_Kim_2006 conj_and_Kim_Hovy num_Wilson_2005 nn_Wilson_al. nn_Wilson_et num_Pang_2004 conj_and_Pang_Lee dep_Yu_Hovy dep_Yu_Kim conj_and_Yu_Wilson conj_and_Yu_Lee conj_and_Yu_Pang conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou nn_classification_polarity dep_S/O_Wilson dep_S/O_Pang dep_S/O_2003 dep_S/O_Hatzivassiloglou dep_S/O_Yu conj_and_S/O_classification det_problem_the prep_into_decomposing_classification prep_into_decomposing_S/O dobj_decomposing_problem prepc_by_benefits_decomposing dobj_realized_benefits aux_realized_have nsubj_realized_researchers prep_in_realized_fact nn_analysis_sentiment prep_in_researchers_analysis ccomp_``_realized
N09-1065	P04-1035	o	The description of the minimum cut framework in Section 4.1 was inspired by Pang and Lee -LRB- 2004 -RRB-	appos_Lee_2004 conj_and_Pang_Lee agent_inspired_Lee agent_inspired_Pang auxpass_inspired_was num_Section_4.1 vmod_cut_inspired prep_in_cut_Section dobj_cut_framework nsubj_cut_description det_minimum_the prep_of_description_minimum det_description_The
N09-3010	P04-1035	o	3.1 Data and Experimental Setup The data set by Pang and Lee -LRB- 2004 -RRB- consists of 2000 movie reviews -LRB- 1000-pos 1000-neg -RRB- from the IMDb review archive	nn_archive_review nn_archive_IMDb det_archive_the amod_1000-pos_1000-neg prep_from_reviews_archive dep_reviews_1000-pos nn_reviews_movie num_reviews_2000 prep_of_consists_reviews nsubj_consists_Setup nsubj_consists_Data appos_Lee_2004 conj_and_Pang_Lee agent_set_Lee agent_set_Pang vmod_data_set det_data_The amod_Setup_Experimental dep_Data_data conj_and_Data_Setup num_Data_3.1
P05-1015	P04-1035	p	All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences the motivation for the latter step is that it has previously aided positive vs. negative classi cation -LRB- Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_cation_Lee dep_cation_Pang nn_cation_classi dep_positive_cation conj_vs._positive_negative dobj_aided_negative dobj_aided_positive advmod_aided_previously aux_aided_has nsubj_aided_it mark_aided_that ccomp_is_aided nsubj_is_motivation amod_step_latter det_step_the prep_for_motivation_step det_motivation_the amod_sentences_objective conj_and_indicators_sentences nn_indicators_rating amod_indicators_explicit preconj_indicators_both dobj_remove_sentences dobj_remove_indicators aux_remove_to parataxis_preprocessed_is xcomp_preprocessed_remove advmod_preprocessed_automatically auxpass_preprocessed_were nsubjpass_preprocessed_reviews det_reviews_All
P05-1015	P04-1035	p	Interestingly previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results -LRB- Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_results_Lee dep_results_Pang amod_results_good dobj_yielded_results nsubj_yielded_formulation mark_yielded_that amod_distinction_subjective/objective amod_distinction_binary det_distinction_the prep_for_formulation_distinction amod_formulation_minimum-cut det_formulation_a ccomp_found_yielded nsubj_found_research advmod_found_Interestingly nn_research_analysis nn_research_sentiment amod_research_previous
P05-2008	P04-1035	p	A later study -LRB- Pang and Lee 2004 -RRB- found that performance increased to 87.2 % when considering only those portions of the text deemed to be subjective	cop_subjective_be aux_subjective_to xcomp_deemed_subjective nsubj_deemed_portions det_text_the prep_of_portions_text det_portions_those advmod_portions_only ccomp_considering_deemed advmod_considering_when num_%_87.2 advcl_increased_considering prep_to_increased_% nsubj_increased_performance mark_increased_that ccomp_found_increased nsubj_found_study num_Pang_2004 conj_and_Pang_Lee appos_study_Lee appos_study_Pang amod_study_later det_study_A ccomp_``_found
P06-1133	P04-1035	o	There are also research work on automatically classifying movie or product reviews as positive or negative -LRB- Nasukawa and Yi 2003 Mullen and Collier 2004 Beineke et al. 2004 Pang and Lee 2004 Hu and Liu 2004 -RRB-	amod_Hu_2004 conj_and_Hu_Liu num_Pang_2004 conj_and_Pang_Lee num_Beineke_2004 nn_Beineke_al. nn_Beineke_et dep_Nasukawa_Liu dep_Nasukawa_Hu conj_and_Nasukawa_Lee conj_and_Nasukawa_Pang conj_and_Nasukawa_Beineke conj_and_Nasukawa_2004 conj_and_Nasukawa_Collier conj_and_Nasukawa_Mullen conj_and_Nasukawa_2003 conj_and_Nasukawa_Yi dep_positive_Pang dep_positive_Beineke dep_positive_2004 dep_positive_Collier dep_positive_Mullen dep_positive_2003 dep_positive_Yi dep_positive_Nasukawa conj_or_positive_negative prep_as_reviews_negative prep_as_reviews_positive nn_reviews_product nn_reviews_movie amod_reviews_classifying conj_or_movie_product advmod_classifying_automatically prep_on_work_reviews nn_work_research nsubj_are_work advmod_are_also expl_are_There ccomp_``_are
P06-1134	P04-1035	o	The third exploits automatic subjectivity analysis in applications such as review classification -LRB- e.g. -LRB- Turney 2002 Pang and Lee 2004 -RRB- -RRB- mining texts for product reviews -LRB- e.g. -LRB- Yi et al. 2003 Hu and Liu 2004 Popescu and Etzioni 2005 -RRB- -RRB- summarization -LRB- e.g. -LRB- Kim and Hovy 2004 -RRB- -RRB- information extraction -LRB- e.g. -LRB- Riloff et al. 2005 -RRB- -RRB- 1Note that sentiment the focus of much recent work in the area is a type of subjectivity specifically involving positive or negative opinion emotion or evaluation	conj_or_opinion_evaluation conj_or_opinion_emotion amod_opinion_negative amod_opinion_positive conj_or_positive_negative dobj_involving_evaluation dobj_involving_emotion dobj_involving_opinion advmod_involving_specifically vmod_type_involving prep_of_type_subjectivity det_type_a cop_type_is nsubj_type_sentiment dobj_type_that det_area_the prep_in_work_area amod_work_recent amod_work_much prep_of_focus_work det_focus_the appos_sentiment_focus rcmod_1Note_type amod_Riloff_2005 dep_Riloff_al. nn_Riloff_et dep_e.g._Riloff dep_extraction_e.g. nn_extraction_information dep_Kim_2004 conj_and_Kim_Hovy appos_e.g._Hovy appos_e.g._Kim dep_summarization_e.g. appos_Popescu_2005 conj_and_Popescu_Etzioni num_Hu_2004 conj_and_Hu_Liu nn_al._et appos_Yi_1Note conj_Yi_extraction amod_Yi_summarization dep_Yi_Etzioni dep_Yi_Popescu dep_Yi_Liu dep_Yi_Hu dep_Yi_2003 dep_Yi_al. dep_,_Yi dep_-LRB-_e.g. nn_reviews_product prep_for_texts_reviews nn_texts_mining appos_Pang_2004 conj_and_Pang_Lee dep_Turney_Lee dep_Turney_Pang amod_Turney_2002 dep_e.g._texts appos_e.g._Turney dep_classification_e.g. nn_classification_review prep_such_as_applications_classification prep_in_analysis_applications nn_analysis_subjectivity amod_analysis_automatic dep_exploits_analysis amod_exploits_third det_exploits_The ccomp_``_exploits
P06-2079	P04-1035	o	4.1 Experimental Setup Like several previous work -LRB- e.g. Mullen and Collier -LRB- 2004 -RRB- Pang and Lee -LRB- 2004 -RRB- Whitelaw et al.	nn_al._et nn_al._Whitelaw appos_Lee_2004 appos_Collier_2004 dep_Mullen_al. conj_and_Mullen_Lee conj_and_Mullen_Pang conj_and_Mullen_Collier dep_e.g._Lee dep_e.g._Pang dep_e.g._Collier dep_e.g._Mullen ccomp_-LRB-_e.g. amod_work_previous amod_work_several prep_like_Setup_work amod_Setup_Experimental num_Setup_4.1
P06-2079	P04-1035	p	Note that our result on Dataset A is as strong as that obtained by Pang and Lee -LRB- 2004 -RRB- via their subjectivity summarization algorithm which retains only the subjective portions of a document	det_document_a prep_of_portions_document amod_portions_subjective det_portions_the advmod_portions_only dobj_retains_portions nsubj_retains_which rcmod_algorithm_retains nn_algorithm_summarization nn_algorithm_subjectivity poss_algorithm_their appos_Lee_2004 conj_and_Pang_Lee prep_via_obtained_algorithm agent_obtained_Lee agent_obtained_Pang vmod_that_obtained prep_as_is_that prep_as_is_strong nn_A_Dataset dep_result_is prep_on_result_A poss_result_our prep_that_Note_result
P06-2079	P04-1035	o	Next we learn our polarity classifier using positive and negative reviews taken from two movie 611 review datasets one assembled by Pang and Lee -LRB- 2004 -RRB- and the other by ourselves	prep_by_other_ourselves det_other_the appos_Lee_2004 conj_and_Pang_Lee agent_assembled_Lee agent_assembled_Pang conj_and_one_other vmod_one_assembled appos_datasets_other appos_datasets_one nn_datasets_review num_datasets_611 nn_datasets_movie num_datasets_two prep_from_taken_datasets vmod_reviews_taken amod_reviews_negative amod_reviews_positive conj_and_positive_negative dobj_using_reviews vmod_classifier_using dep_polarity_classifier poss_polarity_our dobj_learn_polarity nsubj_learn_we advmod_learn_Next
P06-2079	P04-1035	p	Indeed recent work has shown that benefits can be made by first separating facts from opinions in a document -LRB- e.g Yu and Hatzivassiloglou -LRB- 2003 -RRB- -RRB- and classifying the polarity based solely on the subjective portions of the document -LRB- e.g. Pang and Lee -LRB- 2004 -RRB- -RRB-	appos_Lee_2004 conj_and_Pang_Lee dep_e.g._Lee dep_e.g._Pang dep_document_e.g. det_document_the prep_of_portions_document amod_portions_subjective det_portions_the prep_on_based_portions advmod_based_solely vmod_polarity_based det_polarity_the dobj_classifying_polarity nsubjpass_classifying_benefits appos_Hatzivassiloglou_2003 conj_and_e.g_Hatzivassiloglou conj_and_e.g_Yu dep_document_Hatzivassiloglou dep_document_Yu dep_document_e.g det_document_a prep_in_opinions_document prep_from_facts_opinions dobj_separating_facts vmod_first_separating conj_and_made_classifying agent_made_first auxpass_made_be aux_made_can nsubjpass_made_benefits mark_made_that ccomp_shown_classifying ccomp_shown_made aux_shown_has nsubj_shown_work advmod_shown_Indeed amod_work_recent
P07-1053	P04-1035	o	Finally other approaches rely on reviews with numeric ratings from websites -LRB- Pang and Lee 2002 Dave et al. 2003 Pang and Lee 2004 Cui et al. 2006 -RRB- and train -LRB- semi -RRB- supervised learning algorithms to classify reviews as positive or negative or in more fine-grained scales -LRB- Pang and Lee 2005 Wilson et al. 2006 -RRB-	num_Wilson_2006 nn_Wilson_al. nn_Wilson_et dep_Pang_Wilson conj_and_Pang_2005 conj_and_Pang_Lee dep_scales_2005 dep_scales_Lee dep_scales_Pang amod_scales_fine-grained amod_scales_more pobj_in_scales conj_or_positive_negative prep_as_classify_negative prep_as_classify_positive dobj_classify_reviews aux_classify_to conj_or_learning_in vmod_learning_classify dobj_learning_algorithms dep_supervised_in dep_supervised_learning nsubj_supervised_train nsubj_supervised_websites appos_train_semi num_Cui_2006 nn_Cui_al. nn_Cui_et num_Pang_2004 conj_and_Pang_Lee num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_Cui conj_and_Pang_Lee conj_and_Pang_Pang conj_and_Pang_Dave conj_and_Pang_2002 conj_and_Pang_Lee conj_and_websites_train appos_websites_Pang appos_websites_Dave appos_websites_2002 appos_websites_Lee appos_websites_Pang prepc_from_ratings_supervised amod_ratings_numeric prep_with_reviews_ratings prep_on_rely_reviews nsubj_rely_approaches advmod_rely_Finally amod_approaches_other
P07-1055	P04-1035	o	In both cases there 1Alternatively decisions from the sentence classifier can guide which input is seen by the document level classifier -LRB- Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_classifier_Lee dep_classifier_Pang nn_classifier_level nn_classifier_document det_classifier_the agent_seen_classifier auxpass_seen_is nsubjpass_seen_input det_input_which ccomp_guide_seen aux_guide_can nsubj_guide_decisions advmod_guide_1Alternatively advmod_guide_there nsubj_guide_cases mark_guide_In nn_classifier_sentence det_classifier_the prep_from_decisions_classifier det_cases_both advcl_``_guide
P07-1055	P04-1035	o	In fact it has already been established that sentence level classification can improve document level analysis -LRB- Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_analysis_Lee dep_analysis_Pang nn_analysis_level nn_analysis_document dobj_improve_analysis aux_improve_can nsubj_improve_classification mark_improve_that nn_classification_level nn_classification_sentence ccomp_established_improve auxpass_established_been advmod_established_already aux_established_has nsubjpass_established_it prep_in_established_fact
P07-1055	P04-1035	o	Cascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee -LRB- 2004 -RRB-	appos_Lee_2004 conj_and_Pang_Lee agent_studied_Lee agent_studied_Pang auxpass_studied_were nsubjpass_studied_models nn_analysis_sentiment amod_analysis_fine-to-coarse prep_for_models_analysis amod_models_Cascaded
P07-1055	P04-1035	o	For instance in Pang and Lee -LRB- 2004 -RRB- yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective	nsubj_objective_si conj_or_subjective_objective cop_subjective_is nsubj_subjective_si mark_subjective_whether nn_si_sentence ccomp_indicate_objective ccomp_indicate_subjective aux_indicate_would nsubj_indicate_ysi det_document_the conj_and_polarity_indicate prep_of_polarity_document det_polarity_the cop_polarity_be aux_polarity_would nsubj_polarity_yd prep_in_polarity_Lee prep_in_polarity_Pang prep_for_polarity_instance appos_Lee_2004 conj_and_Pang_Lee
P07-1055	P04-1035	o	The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee -LRB- 2004 -RRB- where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a min-cut algorithm	amod_algorithm_min-cut det_algorithm_a dobj_using_algorithm xcomp_wassolved_using conj_and_adocument_inference prep_in_sentence_inference prep_in_sentence_adocument det_sentence_every prep_between_created_sentence auxpass_created_were nsubjpass_created_constraints advmod_created_where nn_constraints_consistency amod_constraints_local amod_constraints_soft appos_Lee_2004 rcmod_Pang_created conj_and_Pang_Lee prep_of_work_Lee prep_of_work_Pang det_work_the dep_similar_wassolved prep_to_similar_work cop_similar_is nsubj_similar_dependencies nn_labels_sentiment prep_on_dependencies_sentences prep_between_dependencies_labels amod_dependencies_local det_dependencies_The
P07-1055	P04-1035	o	Previous workonsentimentanalysishascoveredawiderange of tasks including polarity classification -LRB- Pang et al. 2002 Turney 2002 -RRB- opinion extraction -LRB- Pang and Lee 2004 -RRB- and opinion source assignment -LRB- Choi et al. 2005 Choi et al. 2006 -RRB-	num_Choi_2006 nn_Choi_al. nn_Choi_et dep_Choi_Choi appos_Choi_2005 dep_Choi_al. nn_Choi_et nn_assignment_source nn_assignment_opinion dep_Pang_2004 conj_and_Pang_Lee appos_extraction_Lee appos_extraction_Pang nn_extraction_opinion dep_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_classification_assignment conj_and_classification_extraction appos_classification_Pang nn_classification_polarity prep_including_tasks_assignment prep_including_tasks_extraction prep_including_tasks_classification dep_workonsentimentanalysishascoveredawiderange_Choi prep_of_workonsentimentanalysishascoveredawiderange_tasks amod_workonsentimentanalysishascoveredawiderange_Previous ccomp_``_workonsentimentanalysishascoveredawiderange
P07-1055	P04-1035	o	Furthermore these systems have tackled the problem at different levels of granularity from the document level -LRB- Pang et al. 2002 -RRB- sentence level -LRB- Pang and Lee 2004 Mao and Lebanon 2006 -RRB- phrase level -LRB- Turney 2002 Choi et al. 2005 -RRB- as well as the speaker level in debates -LRB- Thomas et al. 2006 -RRB-	amod_Thomas_2006 dep_Thomas_al. nn_Thomas_et prep_in_level_debates nn_level_speaker det_level_the num_Choi_2005 nn_Choi_al. nn_Choi_et dep_Turney_Choi appos_Turney_2002 appos_level_Turney nn_level_phrase num_Mao_2006 conj_and_Mao_Lebanon dep_Pang_Lebanon dep_Pang_Mao conj_and_Pang_2004 conj_and_Pang_Lee appos_level_2004 appos_level_Lee appos_level_Pang nn_level_sentence amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_level_document det_level_the prep_of_levels_granularity amod_levels_different prep_at_problem_levels det_problem_the dep_tackled_Thomas conj_and_tackled_level conj_and_tackled_level dep_tackled_level dep_tackled_Pang prep_from_tackled_level dobj_tackled_problem aux_tackled_have nsubj_tackled_systems advmod_tackled_Furthermore det_systems_these ccomp_``_level ccomp_``_level ccomp_``_tackled
P07-1123	P04-1035	p	First even when sentiment is the desired focus researchers in sentiment analysis have shown that a two-stage approach is often beneficial in which subjective instances are distinguished from objective ones and then the subjective instances are further classified according to polarity -LRB- Yu and Hatzivassiloglou 2003 Pang and Lee 2004 Wilson et al. 2005 Kim and Hovy 2006 -RRB-	num_Wilson_2005 nn_Wilson_al. nn_Wilson_et num_Pang_2004 conj_and_Pang_Lee amod_Yu_2006 conj_and_Yu_Hovy conj_and_Yu_Kim conj_and_Yu_Wilson conj_and_Yu_Lee conj_and_Yu_Pang conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_polarity_Hovy dep_polarity_Kim dep_polarity_Wilson dep_polarity_Pang dep_polarity_2003 dep_polarity_Hatzivassiloglou dep_polarity_Yu pobj_classified_polarity prepc_according_to_classified_to dep_further_classified nsubj_are_further dep_instances_are amod_instances_subjective det_instances_the dep_then_instances amod_ones_objective conj_and_distinguished_then prep_from_distinguished_ones auxpass_distinguished_are nsubjpass_distinguished_instances prep_in_distinguished_which amod_instances_subjective parataxis_beneficial_then parataxis_beneficial_distinguished advmod_beneficial_often cop_beneficial_is nsubj_beneficial_approach mark_beneficial_that amod_approach_two-stage det_approach_a ccomp_shown_beneficial aux_shown_have nsubj_shown_researchers advcl_shown_focus advmod_shown_First nn_analysis_sentiment prep_in_researchers_analysis amod_focus_desired det_focus_the cop_focus_is nsubj_focus_sentiment advmod_focus_when advmod_focus_even
P07-3007	P04-1035	o	It is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by Pang and Lee -LRB- 2004 -RRB- and Eriksson -LRB- 2006 -RRB-	appos_Eriksson_2006 appos_Lee_2004 conj_and_Pang_Eriksson conj_and_Pang_Lee prep_by_described_Eriksson prep_by_described_Lee prep_by_described_Pang mark_described_as nn_accuracy_classification nn_accuracy_polarity conj_and_detection_accuracy nn_detection_subjectivity prep_between_relation_accuracy prep_between_relation_detection amod_relation_same det_relation_the advcl_observed_described dobj_observed_relation nsubj_observed_we mark_observed_that ccomp_noting_observed xcomp_worth_noting cop_worth_is nsubj_worth_It
P08-1034	P04-1035	o	Pang and Lee -LRB- 2004 -RRB- applied two different classifiers to perform sentiment annotation in two sequential steps the first classifier separated subjective -LRB- sentiment-laden -RRB- texts from objective -LRB- neutral -RRB- ones and then they used the second classifier to classify the subjective texts into positive and negative	conj_and_positive_negative amod_texts_subjective det_texts_the prep_into_classify_negative prep_into_classify_positive dobj_classify_texts aux_classify_to amod_classifier_second det_classifier_the vmod_used_classify dobj_used_classifier nsubj_used_they advmod_used_then nn_ones_objective dep_objective_neutral conj_and_texts_used prep_from_texts_ones amod_texts_subjective amod_texts_separated dep_subjective_sentiment-laden dep_classifier_used dep_classifier_texts amod_classifier_first det_classifier_the dep_steps_classifier amod_steps_sequential num_steps_two nn_annotation_sentiment prep_in_perform_steps dobj_perform_annotation aux_perform_to amod_classifiers_different num_classifiers_two xcomp_applied_perform dobj_applied_classifiers nsubj_applied_Lee nsubj_applied_Pang appos_Lee_2004 conj_and_Pang_Lee
P08-1034	P04-1035	o	291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text -LRB- Aue and Gamon 2005 Pang et al. 2002 Pang and Lee 2004 Riloff et al. 2006 Turney 2002 Turney and Littman 2003 -RRB- or at the sentence levels -LRB- Gamon and Aue 2005 Hu and Liu 2004 Kim and Hovy 2005 Riloff et al. 2006 -RRB-	num_Riloff_2006 nn_Riloff_al. nn_Riloff_et num_Kim_2005 conj_and_Kim_Hovy num_Hu_2004 conj_and_Hu_Liu dep_Gamon_Riloff conj_and_Gamon_Hovy conj_and_Gamon_Kim conj_and_Gamon_Liu conj_and_Gamon_Hu conj_and_Gamon_2005 conj_and_Gamon_Aue nn_levels_sentence det_levels_the dep_Turney_2003 conj_and_Turney_Littman num_Turney_2002 num_Riloff_2006 nn_Riloff_al. nn_Riloff_et num_Pang_2004 conj_and_Pang_Lee num_Pang_2002 nn_Pang_al. nn_Pang_et conj_and_Aue_Littman conj_and_Aue_Turney conj_and_Aue_Turney conj_and_Aue_Riloff conj_and_Aue_Lee conj_and_Aue_Pang conj_and_Aue_Pang conj_and_Aue_2005 conj_and_Aue_Gamon conj_or_text_levels dep_text_Turney dep_text_Turney dep_text_Riloff dep_text_Pang dep_text_Pang dep_text_2005 dep_text_Gamon dep_text_Aue det_text_the dep_conducted_Kim dep_conducted_Hu dep_conducted_2005 dep_conducted_Aue dep_conducted_Gamon prep_at_conducted_levels prep_at_conducted_text advmod_conducted_usually auxpass_conducted_is nsubjpass_conducted_Level nn_annotation_sentiment nn_Research_Analysis prep_on_Level_annotation prep_of_Level_Research num_Level_3.1 num_Level_291
P08-1034	P04-1035	p	Table 1 Datasets 3.3 Establishing a Baseline for a Corpus-based System -LRB- CBS -RRB- Supervised statistical methods have been very successful in sentiment tagging of texts on movie review texts they reach accuracies of 85-90 % -LRB- Aue and Gamon 2005 Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_Aue_Lee dep_Aue_Pang conj_and_Aue_2005 conj_and_Aue_Gamon dep_%_2005 dep_%_Gamon dep_%_Aue num_%_85-90 prep_of_accuracies_% dobj_reach_accuracies nsubj_reach_they rcmod_texts_reach nn_texts_review nn_texts_movie prep_of_tagging_texts nn_tagging_sentiment prep_on_successful_texts prep_in_successful_tagging advmod_successful_very cop_successful_been aux_successful_have nsubj_successful_Datasets amod_methods_statistical amod_methods_Supervised nn_methods_System appos_System_CBS amod_System_Corpus-based det_System_a det_Baseline_a prep_for_Establishing_methods dobj_Establishing_Baseline vmod_Datasets_Establishing num_Datasets_3.3 dep_Table_successful num_Table_1
P08-1034	P04-1035	o	It has been shown that both Nave Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks -LRB- Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_tasks_Lee dep_tasks_Pang amod_tasks_tagging nn_tasks_sentiment amod_sentiment_different prep_on_accuracy_tasks amod_accuracy_similar prep_with_perform_accuracy nsubj_perform_SVMs nsubj_perform_Bayes mark_perform_that conj_and_Bayes_SVMs nn_Bayes_Nave preconj_Bayes_both ccomp_shown_perform auxpass_shown_been aux_shown_has nsubjpass_shown_It
P08-1041	P04-1035	o	In many applications it has been shown that sentences with subjective meanings are paid more attention than factual ones -LRB- Pang and Lee 2004 -RRB- -LRB- Esuli and Sebastiani 2006 -RRB-	dep_Esuli_2006 conj_and_Esuli_Sebastiani dep_Pang_2004 conj_and_Pang_Lee dep_ones_Sebastiani dep_ones_Esuli dep_ones_Lee dep_ones_Pang amod_ones_factual prep_than_attention_ones amod_attention_more dobj_paid_attention auxpass_paid_are nsubjpass_paid_sentences mark_paid_that amod_meanings_subjective prep_with_sentences_meanings ccomp_shown_paid auxpass_shown_been aux_shown_has nsubjpass_shown_it prep_in_shown_applications amod_applications_many
P08-2004	P04-1035	o	-LRB- Wilson et al. 2005 Pang and Lee 2004 -RRB- -RRB- and emotion studies -LRB- e.g.	dep_studies_e.g. nn_studies_emotion dep_Pang_2004 conj_and_Pang_Lee conj_and_Wilson_studies dep_Wilson_Lee dep_Wilson_Pang amod_Wilson_2005 dep_Wilson_al. nn_Wilson_et nsubj_''_studies nsubj_''_Wilson
P09-1027	P04-1035	o	-LRB- 2002 -RRB- various classification models and linguistic features have been proposed to improve the classification performance -LRB- Pang and Lee 2004 Mullen and Collier 2004 Wilson et al. 2005 Read 2005 -RRB-	dep_Read_2005 num_Wilson_2005 nn_Wilson_al. nn_Wilson_et dep_Mullen_Read conj_and_Mullen_Wilson conj_and_Mullen_2004 conj_and_Mullen_Collier conj_and_Pang_Wilson conj_and_Pang_2004 conj_and_Pang_Collier conj_and_Pang_Mullen conj_and_Pang_2004 conj_and_Pang_Lee dep_performance_Mullen dep_performance_2004 dep_performance_Lee dep_performance_Pang nn_performance_classification det_performance_the dobj_improve_performance aux_improve_to xcomp_proposed_improve auxpass_proposed_been aux_proposed_have nsubjpass_proposed_features nsubjpass_proposed_models amod_features_linguistic conj_and_models_features nn_models_classification amod_models_various dep_models_2002
P09-1028	P04-1035	p	A two-tier scheme -LRB- Pang and Lee 2004 -RRB- where sentences are rst classi ed as subjective versus objective and then applying the sentiment classi er on only the subjective sentences further improves performance	dobj_improves_performance advmod_improves_further csubj_improves_applying amod_sentences_subjective det_sentences_the advmod_sentences_only nn_er_classi nn_er_sentiment det_er_the prep_on_applying_sentences dobj_applying_er advmod_applying_then conj_versus_subjective_objective prep_as_ed_objective prep_as_ed_subjective vmod_classi_ed amod_classi_rst cop_classi_are nsubj_classi_sentences advmod_classi_where dep_Pang_2004 conj_and_Pang_Lee conj_and_scheme_improves rcmod_scheme_classi appos_scheme_Lee appos_scheme_Pang amod_scheme_two-tier det_scheme_A
P09-1078	P04-1035	p	And 20NG is a collection of approximately 20,000 20-category documents 1 In sentiment text classification we also use two data sets one is the widely used Cornell movie-review dataset2 -LRB- Pang and Lee 2004 -RRB- and one dataset from product reviews of domain DVD3 -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et nn_DVD3_domain prep_of_reviews_DVD3 nn_reviews_product prep_from_dataset_reviews num_dataset_one dep_Pang_2004 conj_and_Pang_Lee dep_dataset2_Blitzer conj_and_dataset2_dataset dep_dataset2_Lee dep_dataset2_Pang nn_dataset2_movie-review nn_dataset2_Cornell amod_dataset2_used det_dataset2_the cop_dataset2_is nsubj_dataset2_one advmod_used_widely nn_sets_data num_sets_two dobj_use_sets advmod_use_also nsubj_use_we nn_classification_text nn_classification_sentiment num_documents_1 amod_documents_20-category num_documents_20,000 quantmod_20,000_approximately dep_collection_dataset dep_collection_dataset2 parataxis_collection_use prep_in_collection_classification prep_of_collection_documents det_collection_a cop_collection_is nsubj_collection_20NG cc_collection_And
P09-1078	P04-1035	n	-LRB- 2006 -RRB- examine the FS of the weighted log-likelihood ratio -LRB- WLLR -RRB- on the movie review dataset and achieves an accuracy of 87.1 % which is higher than the result reported by Pang and Lee -LRB- 2004 -RRB- with the same dataset	amod_dataset_same det_dataset_the appos_Lee_2004 conj_and_Pang_Lee prep_with_reported_dataset agent_reported_Lee agent_reported_Pang vmod_result_reported det_result_the prep_than_higher_result cop_higher_is nsubj_higher_which rcmod_%_higher num_%_87.1 prep_of_accuracy_% det_accuracy_an dobj_achieves_accuracy nsubj_achieves_2006 nn_dataset_review nn_dataset_movie det_dataset_the prep_on_ratio_dataset appos_ratio_WLLR amod_ratio_log-likelihood amod_ratio_weighted det_ratio_the prep_of_FS_ratio det_FS_the conj_and_examine_achieves dobj_examine_FS nsubj_examine_2006
P09-1079	P04-1035	o	For instance Pang and Lee -LRB- 2004 -RRB- train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification	nn_classification_polarity prep_prior_to_review_classification det_review_a amod_sentences_objective prep_from_identify_review dobj_identify_sentences conj_and_identify_remove aux_identify_to nn_classifier_subjectivity amod_classifier_independent det_classifier_an vmod_train_remove vmod_train_identify dobj_train_classifier nsubj_train_Lee nsubj_train_Pang prep_for_train_instance appos_Lee_2004 conj_and_Pang_Lee
W05-0408	P04-1035	o	1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years -LRB- Pang and Lee 2002 Pang et al. 2004 Turney 2002 Turney and Littman 2002 Wiebe et al. 2001 Bai et al. 2004 Yu and Hatzivassiloglou 2003 and many others -RRB-	amod_others_many num_Hatzivassiloglou_2003 num_al._2004 dep_Bai_al. nn_Bai_et num_al._2001 dep_Wiebe_al. nn_Wiebe_et num_Littman_2002 num_Turney_2002 num_al._2004 dep_Pang_al. nn_Pang_et num_Lee_2002 conj_and_Pang_others conj_and_Pang_Hatzivassiloglou conj_and_Pang_Yu conj_and_Pang_Bai conj_and_Pang_Wiebe conj_and_Pang_Littman conj_and_Pang_Turney appos_Pang_Turney appos_Pang_Pang conj_and_Pang_Lee dep_years_others dep_years_Hatzivassiloglou dep_years_Yu dep_years_Bai dep_years_Wiebe dep_years_Littman dep_years_Turney dep_years_Lee dep_years_Pang amod_years_recent amod_attention_considerable prep_in_received_years prep_from_received_researchers dobj_received_attention aux_received_has nsubj_received_field nn_classification_sentiment prep_of_field_classification det_field_The rcmod_Introduction_received num_Introduction_1
W05-0408	P04-1035	o	Movie and product reviews have been the main focus of many of the recent studies in this area -LRB- Pang and Lee 2002 Pang et al. 2004 Turney 2002 Turney and Littman 2002 -RRB-	num_Littman_2002 num_Turney_2002 num_al._2004 dep_Pang_al. nn_Pang_et num_Lee_2002 conj_and_Pang_Littman conj_and_Pang_Turney conj_and_Pang_Turney conj_and_Pang_Pang conj_and_Pang_Lee det_area_this dep_studies_Littman dep_studies_Turney dep_studies_Turney dep_studies_Pang dep_studies_Lee dep_studies_Pang prep_in_studies_area amod_studies_recent det_studies_the prep_of_many_studies prep_of_focus_many amod_focus_main det_focus_the cop_focus_been aux_focus_have nsubj_focus_reviews nn_reviews_product nn_reviews_Movie conj_and_Movie_product
W05-0408	P04-1035	o	accuracy Training data Turney -LRB- 2002 -RRB- 66 % unsupervised Pang & Lee -LRB- 2004 -RRB- 87.15 % supervised Aue & Gamon -LRB- 2005 -RRB- 91.4 % supervised SO 73.95 % unsupervised SM+SO to increase seed words then SO 74.85 % weakly supervised Table 7 Classification accuracy on the movie review domain Turney -LRB- 2002 -RRB- achieves 66 % accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web	det_web_the prep_from_scores_web nn_scores_association dobj_gather_scores aux_gather_to nn_algorithm_PMI-IR det_algorithm_the vmod_using_gather dobj_using_algorithm nn_domain_review nn_domain_movie det_domain_the prep_on_accuracy_domain amod_accuracy_% number_%_66 xcomp_achieves_using dobj_achieves_accuracy nsubj_achieves_accuracy appos_Turney_2002 nn_Turney_domain nn_Turney_review nn_Turney_movie det_Turney_the prep_on_accuracy_Turney nn_accuracy_Classification num_Table_7 dobj_supervised_Table advmod_supervised_weakly nsubj_supervised_% advmod_supervised_then num_%_74.85 advmod_%_SO nn_words_seed dobj_increase_words aux_increase_to amod_SM+SO_unsupervised amod_SM+SO_% nn_SM+SO_SO number_%_73.95 vmod_supervised_increase dobj_supervised_SM+SO vmod_%_supervised num_%_91.4 dep_Aue_% dep_Aue_2005 conj_and_Aue_Gamon amod_Aue_supervised npadvmod_%_Gamon npadvmod_%_Aue num_%_87.15 dep_Pang_2004 conj_and_Pang_Lee amod_Pang_unsupervised parataxis_%_achieves rcmod_%_supervised dep_%_% dep_%_Lee dep_%_Pang num_%_66 num_%_2002 dep_Turney_% nn_Turney_data nn_Turney_Training nn_Turney_accuracy
W05-0408	P04-1035	o	Pang and Lee -LRB- 2004 -RRB- report 87.15 % accuracy using a unigram-based SVM classifier combined with subjectivity detection	nn_detection_subjectivity prep_with_combined_detection vmod_classifier_combined nn_classifier_SVM amod_classifier_unigram-based det_classifier_a dobj_using_classifier vmod_accuracy_using amod_accuracy_% nn_accuracy_report nn_accuracy_Pang number_%_87.15 dep_report_2004 nn_report_Lee conj_and_Pang_report
W06-0302	P04-1035	o	-LRB- 2003 -RRB- Pang and Lee -LRB- 2004 -RRB- -RRB-	appos_Lee_2004 conj_and_Pang_Lee dep_Pang_2003
W06-0302	P04-1035	o	-LRB- 2004 -RRB- Pang and Lee -LRB- 2004 -RRB- Wilson et al.	nn_al._et nn_al._Wilson appos_Lee_2004 dep_Pang_al. conj_and_Pang_Lee dep_Pang_2004
W06-0303	P04-1035	o	For process -LRB- 2 -RRB- existing methods aim to distinguish between subjective and objective descriptions in texts -LRB- Kim and Hovy 2004 Pang and Lee 2004 Riloff and Wiebe 2003 -RRB-	amod_Riloff_2003 conj_and_Riloff_Wiebe num_Pang_2004 conj_and_Pang_Lee dep_Kim_Wiebe dep_Kim_Riloff conj_and_Kim_Lee conj_and_Kim_Pang conj_and_Kim_2004 conj_and_Kim_Hovy prep_in_descriptions_texts amod_descriptions_objective amod_descriptions_subjective conj_and_subjective_objective prep_between_distinguish_descriptions aux_distinguish_to dep_aim_Pang dep_aim_2004 dep_aim_Hovy dep_aim_Kim vmod_aim_distinguish nn_aim_methods amod_aim_existing appos_process_2 dep_``_aim prep_for_``_process
W06-0303	P04-1035	o	For process -LRB- 3 -RRB- machine-learning methods are usually used to classify subjective descriptions into bipolar categories -LRB- Dave et al. 2003 Beineke et al. 2004 Hu and Liu 2004 Pang and Lee 2004 -RRB- or multipoint scale categories -LRB- Kim and Hovy 2004 Pang and Lee 2005 -RRB-	amod_Pang_2005 conj_and_Pang_Lee dep_Kim_Lee dep_Kim_Pang conj_and_Kim_2004 conj_and_Kim_Hovy dep_categories_2004 dep_categories_Hovy dep_categories_Kim nn_categories_scale amod_categories_multipoint conj_and_Pang_2004 conj_and_Pang_Lee conj_and_Hu_Liu num_Beineke_2004 nn_Beineke_al. nn_Beineke_et dep_Dave_2004 dep_Dave_Lee dep_Dave_Pang dep_Dave_2004 dep_Dave_Liu dep_Dave_Hu dep_Dave_Beineke amod_Dave_2003 dep_Dave_al. nn_Dave_et amod_categories_bipolar amod_descriptions_subjective prep_into_classify_categories dobj_classify_descriptions aux_classify_to conj_or_used_categories dep_used_Dave xcomp_used_classify advmod_used_usually auxpass_used_are nsubjpass_used_methods prep_for_used_process amod_methods_machine-learning appos_process_3
W06-0304	P04-1035	o	The focus of much of the automatic sentiment analysis research is on identifying the affect bearing words -LRB- words with emotional content -RRB- and on measurement approaches for sentiment -LRB- Turney & Littman 2003 Pang & Lee 2004 Wilson et al. 2005 -RRB-	num_Wilson_2005 nn_Wilson_al. nn_Wilson_et num_Pang_2004 conj_and_Pang_Lee dep_Turney_Wilson conj_and_Turney_Lee conj_and_Turney_Pang conj_and_Turney_2003 conj_and_Turney_Littman dep_sentiment_Pang dep_sentiment_2003 dep_sentiment_Littman dep_sentiment_Turney prep_for_approaches_sentiment nn_approaches_measurement pobj_on_approaches amod_content_emotional prep_with_words_content dep_words_words dobj_bearing_words vmod_affect_bearing det_affect_the conj_and_identifying_on dobj_identifying_affect prepc_on_is_on prepc_on_is_identifying nsubj_is_focus nn_research_analysis nn_research_sentiment amod_research_automatic det_research_the prep_of_much_research prep_of_focus_much det_focus_The ccomp_``_is
W06-0304	P04-1035	o	-LRB- Pang & Lee 2004 Aue & Gamon 2005 -RRB-	dep_Aue_2005 conj_and_Aue_Gamon dep_Pang_Gamon dep_Pang_Aue amod_Pang_2004 conj_and_Pang_Lee dep_''_Lee dep_''_Pang
W06-1639	P04-1035	p	As has been previously observed and exploited in the NLP literature -LRB- Pang and Lee 2004 Agarwal and Bhattacharyya 2005 Barzilay and Lapata 2005 -RRB- the above optimization function unlike many others that have been proposed for graph or set partitioning can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs	amod_cuts_minimum prep_in_finding_graphs dobj_finding_cuts prepc_for_methods_finding amod_manner_efficient det_manner_an advmod_efficient_provably prep_via_solved_methods prep_in_solved_manner advmod_solved_exactly auxpass_solved_be aux_solved_can prep_unlike_solved_others nsubjpass_solved_function advcl_solved_exploited advcl_solved_observed amod_partitioning_set conj_or_graph_partitioning prep_for_proposed_partitioning prep_for_proposed_graph auxpass_proposed_been aux_proposed_have nsubjpass_proposed_that rcmod_others_proposed amod_others_many nn_function_optimization amod_function_above det_function_the dep_Agarwal_2005 conj_and_Agarwal_Lapata conj_and_Agarwal_Barzilay conj_and_Agarwal_2005 conj_and_Agarwal_Bhattacharyya dep_Pang_Lapata dep_Pang_Barzilay dep_Pang_2005 dep_Pang_Bhattacharyya dep_Pang_Agarwal conj_and_Pang_2004 conj_and_Pang_Lee appos_literature_2004 appos_literature_Lee appos_literature_Pang nn_literature_NLP det_literature_the prep_in_exploited_literature conj_and_observed_exploited advmod_observed_previously auxpass_observed_been aux_observed_has mark_observed_As
W06-1640	P04-1035	o	In contrast to the opinion extracts produced by Pang and Lee -LRB- 2004 -RRB- our summaries are not text extracts but rather explicitly identify and 337 characterize the relations between opinions and their sources	poss_sources_their conj_and_opinions_sources prep_between_relations_sources prep_between_relations_opinions det_relations_the dobj_characterize_relations nsubj_characterize_identify nsubj_characterize_extracts conj_and_identify_337 advmod_identify_explicitly advmod_identify_rather nsubj_identify_summaries conj_but_extracts_337 conj_but_extracts_identify nn_extracts_text cop_extracts_are nsubj_extracts_summaries prep_in_extracts_contrast neg_text_not poss_summaries_our appos_Lee_2004 conj_and_Pang_Lee agent_produced_Lee agent_produced_Pang vmod_extracts_produced nn_extracts_opinion det_extracts_the prep_to_contrast_extracts
W06-1642	P04-1035	o	Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis -LRB- Riloff and Wiebe 2003 Pang and Lee 2004 -RRB- which is two-fold classification into subjective and objective sentences	amod_sentences_objective amod_sentences_subjective conj_and_subjective_objective prep_into_classification_sentences amod_classification_two-fold cop_classification_is nsubj_classification_which dep_Pang_2004 conj_and_Pang_Lee dep_Riloff_Lee dep_Riloff_Pang conj_and_Riloff_2003 conj_and_Riloff_Wiebe rcmod_analysis_classification appos_analysis_2003 appos_analysis_Wiebe appos_analysis_Riloff nn_analysis_subjectivity prep_for_clue_analysis advmod_clue_also det_clue_a prep_as_used_clue auxpass_used_were nsubjpass_used_contexts poss_approach_our pobj_in_approach pcomp_as_in prep_contexts_as amod_contexts_Inter-sentential
W06-1652	P04-1035	o	3 Data Sets We used three opinion-related data sets for our analyses and experiments the OP data set created by -LRB- Wiebe et al. 2004 -RRB- the Polarity data set5 created by -LRB- Pang and Lee 2004 -RRB- and the MPQA data set created by -LRB- Wiebe et al. 2005 -RRB- .6 The OP and Polarity data sets involve document-level opinion classi cation while the MPQA data set involves 5Version v2 .0 which is available at http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classi cation	nn_cation_classi nn_cation_sentence-level nn_cation_http://www.cs.pitt.edu/mpqa/databaserelease/ prep_at_6Available_cation amod_6Available_http://www.cs.cornell.edu/people/pabo/movie-review-data/ dep_at_6Available prep_available_at cop_available_is nsubj_available_which rcmod_v2_available num_v2_.0 nn_v2_5Version dobj_involves_v2 nsubj_involves_set mark_involves_while nn_set_data nn_set_MPQA det_set_the nn_cation_classi nn_cation_opinion amod_cation_document-level dobj_involve_cation nsubj_involve_set5 nn_sets_data dep_OP_sets conj_and_OP_Polarity det_OP_The num_OP_.6 dep_Wiebe_Polarity dep_Wiebe_OP amod_Wiebe_2005 dep_Wiebe_al. nn_Wiebe_et agent_created_Wiebe vmod_set_created vmod_data_set nn_data_MPQA det_data_the conj_and_Pang_data dep_Pang_2004 conj_and_Pang_Lee agent_created_data agent_created_Lee agent_created_Pang vmod_set5_created nn_set5_data nn_set5_Polarity det_set5_the rcmod_Wiebe_involve amod_Wiebe_2004 dep_Wiebe_al. nn_Wiebe_et agent_created_Wiebe advcl_set_involves vmod_set_created nn_set_data nn_set_OP det_set_the conj_and_analyses_experiments poss_analyses_our nn_sets_data amod_sets_opinion-related num_sets_three prep_for_used_experiments prep_for_used_analyses dobj_used_sets nsubj_used_We dep_Sets_set ccomp_Sets_used nsubj_Sets_Data num_Data_3 ccomp_``_Sets
W07-2013	P04-1035	o	Unlike previous annotations of sentiment or subjectivity -LRB- Wiebe et al. 2005 Pang and Lee 2004 -RRB- which typically relied on binary 0/1 annotations we decided to use a finer-grained scale hence allowing the annotators to select different degrees of emotional load	amod_load_emotional prep_of_degrees_load amod_degrees_different dobj_select_degrees aux_select_to det_annotators_the xcomp_allowing_select dobj_allowing_annotators advmod_allowing_hence amod_scale_finer-grained det_scale_a dobj_use_scale aux_use_to xcomp_decided_allowing xcomp_decided_use nsubj_decided_we num_annotations_0/1 amod_annotations_binary prep_on_relied_annotations advmod_relied_typically nsubj_relied_which num_Pang_2004 conj_and_Pang_Lee rcmod_Wiebe_decided rcmod_Wiebe_relied dep_Wiebe_Lee dep_Wiebe_Pang appos_Wiebe_2005 dep_Wiebe_al. nn_Wiebe_et conj_or_sentiment_subjectivity prep_of_annotations_subjectivity prep_of_annotations_sentiment amod_annotations_previous dep_Unlike_Wiebe pobj_Unlike_annotations dep_``_Unlike
W07-2022	P04-1035	p	Sentence-level subjectivity detection where training data is easier to obtain than for positive vs. negative classification has been successfully performed using supervised statistical methods alone -LRB- Pang and Lee 2004 -RRB- or in combination with a knowledgebased approach -LRB- Riloff et al. 2006 -RRB-	amod_Riloff_2006 dep_Riloff_al. nn_Riloff_et dep_approach_Riloff amod_approach_knowledgebased det_approach_a prep_with_combination_approach pobj_in_combination dep_Pang_2004 conj_and_Pang_Lee conj_or_methods_in appos_methods_Lee appos_methods_Pang advmod_methods_alone amod_methods_statistical amod_methods_supervised dobj_using_in dobj_using_methods xcomp_performed_using advmod_performed_successfully auxpass_performed_been aux_performed_has nsubjpass_performed_detection amod_classification_negative amod_classification_positive conj_vs._positive_negative pobj_for_classification pcomp_than_for aux_obtain_to prep_easier_than xcomp_easier_obtain cop_easier_is nsubj_easier_data advmod_easier_where nn_data_training rcmod_detection_easier nn_detection_subjectivity amod_detection_Sentence-level
W07-2022	P04-1035	p	3 CLaC-NB System Nave Bayes Supervised statistical methods have been very successful in sentiment tagging of texts and in subjectivity detection at sentence level on movie review texts they reach an accuracy of 85-90 % -LRB- Aue and Gamon 2005 Pang and Lee 2004 -RRB- and up to 92 % accuracy on classifying movie review snippets into subjective and objective using both Nave Bayes and SVM -LRB- Pang and Lee 2004 -RRB-	amod_Pang_2004 conj_and_Pang_Lee dep_SVM_Lee dep_SVM_Pang conj_and_Bayes_SVM nn_Bayes_Nave preconj_Bayes_both dobj_using_SVM dobj_using_Bayes conj_and_subjective_objective nn_snippets_review nn_snippets_movie prep_into_classifying_objective prep_into_classifying_subjective dobj_classifying_snippets vmod_accuracy_using prepc_on_accuracy_classifying nn_accuracy_% num_accuracy_92 dep_92_to quantmod_92_up dep_Pang_2004 conj_and_Pang_Lee dep_Aue_Lee dep_Aue_Pang conj_and_Aue_2005 conj_and_Aue_Gamon appos_%_2005 appos_%_Gamon appos_%_Aue num_%_85-90 conj_and_accuracy_accuracy prep_of_accuracy_% det_accuracy_an dobj_reach_accuracy dobj_reach_accuracy nsubj_reach_they prep_on_reach_texts nn_texts_review nn_texts_movie nn_level_sentence nn_detection_subjectivity conj_and_tagging_detection prep_of_tagging_texts nn_tagging_sentiment prep_at_successful_level prep_in_successful_detection prep_in_successful_tagging advmod_successful_very cop_successful_been aux_successful_have amod_methods_statistical parataxis_Supervised_reach dep_Supervised_successful dobj_Supervised_methods nsubj_Supervised_Bayes nn_Bayes_Nave dep_System_Supervised nn_System_CLaC-NB num_System_3
W08-0122	P04-1035	o	5 Related Work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective -LRB- Riloff et al. 2003 Pang and Lee 2004 -RRB- -RRB- and adjacency pair information has been used to predict congressional votes -LRB- Thomas et al. 2006 -RRB-	amod_Thomas_2006 dep_Thomas_al. nn_Thomas_et amod_votes_congressional dobj_predict_votes aux_predict_to xcomp_used_predict auxpass_used_been aux_used_has nsubjpass_used_information nn_information_pair nn_information_adjacency num_Pang_2004 conj_and_Pang_Lee dep_al._Lee dep_al._Pang num_al._2003 nn_al._et amod_al._Riloff cop_subjective/objective_be aux_subjective/objective_should nsubj_subjective/objective_sentence mark_subjective/objective_if amod_sentence_current det_sentence_the dep_determine_al. advcl_determine_subjective/objective aux_determine_to dep_used_Thomas conj_and_used_used xcomp_used_determine advmod_used_previously auxpass_used_been aux_used_has nsubjpass_used_Evidence amod_context_surrounding det_context_the prep_from_Evidence_context nn_Evidence_Work amod_Evidence_Related num_Evidence_5
W09-1606	P04-1035	o	3.3 Language Model -LRB- LM -RRB- As a second baseline we use the classification based on the language model using overlapping ngram sequences -LRB- n was set to 8 -RRB- as suggested by Pang & Lee -LRB- 2004 2005 -RRB- for the English language	nn_language_English det_language_the dep_2004_2005 dep_Pang_2004 conj_and_Pang_Lee prep_for_suggested_language prep_by_suggested_Lee prep_by_suggested_Pang mark_suggested_as prep_to_set_8 auxpass_set_was nsubjpass_set_n rcmod_sequences_set nn_sequences_ngram amod_sequences_overlapping dobj_using_sequences nn_model_language det_model_the det_classification_the advcl_use_suggested pcomp_use_using prep_based_on_use_model dobj_use_classification nsubj_use_we nsubj_use_Model amod_baseline_second det_baseline_a prep_as_Model_baseline appos_Model_LM nn_Model_Language num_Model_3.3 ccomp_``_use
W09-1606	P04-1035	o	Pang & Lee -LRB- 2004 -RRB- propose the use of language models for sentiment analysis task and subjectivity extraction	nn_extraction_subjectivity conj_and_task_extraction nn_task_analysis nn_task_sentiment nn_models_language prep_for_use_extraction prep_for_use_task prep_of_use_models det_use_the dobj_propose_use nsubj_propose_Lee nsubj_propose_Pang appos_Pang_2004 conj_and_Pang_Lee
W09-1904	P04-1035	o	Previous research has focused on classifying subjective-versus-objective expressions -LRB- Wiebe et al. 2004 -RRB- and also on accurate sentiment polarity assignment -LRB- Turney 2002 Yi et al. 2003 Pang and Lee 2004 Sindhwani and Melville 2008 Melville et al. 2009 -RRB-	num_Melville_2009 nn_Melville_al. nn_Melville_et dep_Sindhwani_Melville conj_and_Sindhwani_2008 conj_and_Sindhwani_Melville num_Pang_2004 conj_and_Pang_Lee dep_Yi_2008 dep_Yi_Melville dep_Yi_Sindhwani conj_Yi_Lee conj_Yi_Pang num_Yi_2003 nn_Yi_al. nn_Yi_et dep_Turney_Yi appos_Turney_2002 dep_assignment_Turney nn_assignment_polarity nn_assignment_sentiment amod_assignment_accurate pobj_on_assignment advmod_on_also amod_Wiebe_2004 dep_Wiebe_al. nn_Wiebe_et appos_expressions_Wiebe amod_expressions_subjective-versus-objective amod_expressions_classifying conj_and_focused_on prep_on_focused_expressions aux_focused_has nsubj_focused_research amod_research_Previous
W09-2804	P04-1035	o	Pang and Lee -LRB- 2004 -RRB- frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences	det_sentences_the prep_of_representation_sentences nn_representation_graph det_representation_a prep_in_cut_representation nn_cut_minimum det_cut_the dobj_finding_cut amod_sentences_subjective prepc_as_detecting_finding dobj_detecting_sentences prepc_of_problem_detecting det_problem_the nn_problem_frame nn_problem_Pang nn_frame_Lee appos_Lee_2004 conj_and_Pang_frame
C08-1038	P04-1041	o	-LRB- 2007 -RRB- present a chart generator using wide-coverage PCFG-based LFG approximations automatically acquired from treebanks -LRB- Cahill et al. 2004 -RRB-	amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et prep_from_acquired_treebanks advmod_acquired_automatically vmod_approximations_acquired nn_approximations_LFG amod_approximations_PCFG-based amod_approximations_wide-coverage dobj_using_approximations vmod_generator_using nn_generator_chart det_generator_a dep_present_Cahill dobj_present_generator nsubj_present_2007
C08-1038	P04-1041	o	Our approach is data-driven following the methodology in -LRB- Cahill et al. 2004 Guo et al. 2007 -RRB- we automatically convert the English PennII treebank and the Chinese Penn Treebank -LRB- Xue et al. 2005 -RRB- into f-structure banks	amod_banks_f-structure amod_Xue_2005 dep_Xue_al. nn_Xue_et prep_into_Treebank_banks dep_Treebank_Xue nn_Treebank_Penn amod_Treebank_Chinese det_Treebank_the nn_treebank_PennII nn_treebank_English det_treebank_the conj_and_convert_Treebank dobj_convert_treebank advmod_convert_automatically nsubj_convert_we num_Guo_2007 nn_Guo_al. nn_Guo_et dep_Cahill_Guo appos_Cahill_2004 dep_Cahill_al. nn_Cahill_et dep_in_Cahill prep_methodology_in det_methodology_the parataxis_data-driven_Treebank parataxis_data-driven_convert prep_following_data-driven_methodology cop_data-driven_is nsubj_data-driven_approach poss_approach_Our ccomp_``_data-driven
C08-1038	P04-1041	o	1999 -RRB- OpenCCG -LRB- White 2004 -RRB- and XLE -LRB- Crouch et al. 2007 -RRB- or created semi-automatically -LRB- Belz 2007 -RRB- or fully automatically extracted from annotated corpora like the HPSG -LRB- Nakanishi et al. 2005 -RRB- LFG -LRB- Cahill and van Genabith 2006 Hogan et al. 2007 -RRB- and CCG -LRB- White et al. 2007 -RRB- resources derived from the Penn-II Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn-II det_Treebank_the prep_from_derived_Treebank dep_resources_Marcus vmod_resources_derived nn_resources_CCG nn_resources_LFG nn_resources_created nn_resources_XLE nn_resources_White dep_White_2007 dep_White_al. nn_White_et num_Hogan_2007 nn_Hogan_al. nn_Hogan_et nn_Genabith_van dep_Cahill_Hogan dep_Cahill_2006 conj_and_Cahill_Genabith dep_LFG_Genabith dep_LFG_Cahill amod_Nakanishi_2005 dep_Nakanishi_al. nn_Nakanishi_et det_HPSG_the amod_corpora_annotated prep_from_extracted_corpora advmod_extracted_automatically advmod_automatically_fully amod_Belz_2007 dep_semi-automatically_Nakanishi prep_like_semi-automatically_HPSG conj_or_semi-automatically_extracted dep_semi-automatically_Belz advmod_created_extracted advmod_created_semi-automatically amod_Crouch_2007 dep_Crouch_al. nn_Crouch_et appos_XLE_Crouch dep_White_White conj_and_White_CCG conj_or_White_LFG conj_or_White_created conj_and_White_XLE num_White_2004 nn_White_OpenCCG conj_1999_resources
D07-1027	P04-1041	o	In addition to CFG-oriented approaches a number of richer treebank-based grammar acquisition and parsing methods based on HPSG -LRB- Miyao et al. 2003 -RRB- CCG -LRB- Clark and Hockenmaier 2002 -RRB- LFG -LRB- Riezler et al. 2002 Cahill et al. 2004 -RRB- and Dependency Grammar -LRB- Nivre and Nilsson 2005 -RRB- incorporate non-local dependencies into their deep syntactic or semantic representations	amod_representations_semantic conj_or_syntactic_representations amod_syntactic_deep poss_syntactic_their amod_dependencies_non-local prep_into_incorporate_representations prep_into_incorporate_syntactic dobj_incorporate_dependencies dep_Nivre_2005 conj_and_Nivre_Nilsson appos_Grammar_Nilsson appos_Grammar_Nivre nn_Grammar_Dependency num_Cahill_2004 nn_Cahill_al. nn_Cahill_et dep_Riezler_incorporate conj_and_Riezler_Grammar dep_Riezler_Cahill appos_Riezler_2002 dep_Riezler_al. nn_Riezler_et dep_LFG_Grammar dep_LFG_Riezler amod_Clark_2002 conj_and_Clark_Hockenmaier appos_CCG_Hockenmaier appos_CCG_Clark amod_Miyao_2003 dep_Miyao_al. nn_Miyao_et appos_HPSG_LFG conj_HPSG_CCG dep_HPSG_Miyao prep_on_based_HPSG nn_methods_parsing conj_and_acquisition_methods nn_acquisition_grammar amod_acquisition_treebank-based amod_acquisition_richer vmod_number_based prep_of_number_methods prep_of_number_acquisition det_number_a amod_approaches_CFG-oriented prep_to_addition_approaches dep_In_number pobj_In_addition ccomp_``_In
D07-1027	P04-1041	o	F -LRB- Cahill et al. 2004 -RRB- overall 95.98 57.86 72.20 73.00 40.28 51.91 90.16 54.35 67.82 65.54 36.16 46.61 args only 98.64 42.03 58.94 82.69 30.54 44.60 86.36 36.80 51.61 66.08 24.40 35.64 Basic Model overall 92.44 91.28 91.85 63.87 62.15 63.00 63.12 62.33 62.72 42.69 41.54 42.10 args only 89.42 92.95 91.15 60.89 63.45 62.15 47.92 49.81 48.84 31.41 32.73 32.06 Basic Model with Subject Path Constraint overall 92.16 91.36 91.76 63.72 62.20 62.95 75.96 75.30 75.63 50.82 49.61 50.21 args only 89.04 93.08 91.02 60.69 63.52 62.07 66.15 69.15 67.62 42.77 44.76 44.76 Table 7 Evaluation of trace insertion and antecedent recovery for C04 algorithm our basic algorithm and basic algorithm with the subject path constraint	nn_constraint_path amod_constraint_subject det_constraint_the amod_algorithm_basic prep_with_algorithm_constraint conj_and_algorithm_algorithm amod_algorithm_basic poss_algorithm_our nn_algorithm_C04 amod_recovery_antecedent conj_and_insertion_recovery nn_insertion_trace dep_Evaluation_algorithm dep_Evaluation_algorithm prep_for_Evaluation_algorithm prep_of_Evaluation_recovery prep_of_Evaluation_insertion num_Table_7 num_Table_44.76 num_Table_44.76 dep_42.77_Table number_42.77_67.62 dep_42.77_69.15 number_69.15_66.15 dep_62.07_42.77 number_62.07_63.52 dep_62.07_60.69 number_60.69_91.02 dep_93.08_62.07 number_93.08_89.04 quantmod_93.08_only dep_args_50.21 amod_args_overall nn_args_Constraint nn_args_Path nn_args_Subject number_50.21_49.61 dep_50.21_50.82 dep_50.21_62.95 dep_50.21_91.36 number_50.82_75.63 dep_50.82_75.30 number_75.30_75.96 number_62.95_62.20 dep_62.95_63.72 number_63.72_91.76 number_91.36_92.16 prep_with_Model_args amod_Model_Basic num_Model_32.06 num_Model_32.73 dep_31.41_93.08 dep_31.41_Model number_31.41_48.84 dep_31.41_49.81 number_49.81_47.92 dep_62.15_31.41 number_62.15_63.45 dep_62.15_60.89 number_60.89_91.15 dep_92.95_Evaluation dep_92.95_62.15 number_92.95_89.42 dep_only_92.95 amod_args_only dep_args_42.10 amod_args_overall nn_args_Model amod_args_Basic num_args_35.64 num_args_24.40 dep_args_66.08 dep_args_44.60 dep_args_42.03 advmod_args_only number_42.10_41.54 dep_42.10_42.69 dep_42.10_63.00 dep_42.10_91.28 number_42.69_62.72 dep_42.69_62.33 number_62.33_63.12 number_63.00_62.15 dep_63.00_63.87 number_63.87_91.85 number_91.28_92.44 number_66.08_51.61 dep_66.08_36.80 number_36.80_86.36 number_44.60_30.54 dep_44.60_82.69 number_82.69_58.94 number_42.03_98.64 dep_args_args dep_args_46.61 amod_args_overall number_46.61_36.16 dep_46.61_65.54 dep_46.61_51.91 dep_46.61_57.86 number_65.54_67.82 dep_65.54_54.35 number_54.35_90.16 number_51.91_40.28 dep_51.91_73.00 number_73.00_72.20 number_57.86_95.98 amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et dep_F_args dep_F_Cahill
D07-1027	P04-1041	o	We also combine our basic algorithm -LRB- Section 4.2 -RRB- with -LRB- Cahill et al. 2004 -RRB- s algorithm in order to resolve the modifier-function traces	amod_traces_modifier-function det_traces_the dobj_resolve_traces aux_resolve_to dep_resolve_order mark_resolve_in vmod_algorithm_resolve amod_algorithm_s amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et num_Section_4.2 prep_with_algorithm_algorithm dep_algorithm_Cahill appos_algorithm_Section amod_algorithm_basic poss_algorithm_our dobj_combine_algorithm advmod_combine_also nsubj_combine_We ccomp_``_combine
D07-1027	P04-1041	o	Inspired by -LRB- Cahill et al. 2004 -RRB- s methodology which was originally designed for English and Penn-II treebank our approach to Chinese non-local dependency recovery is based on Lexical-Functional Grammar -LRB- LFG -RRB- a formalism that involves both phrase structure trees and predicate-argument structures	amod_structures_predicate-argument conj_and_trees_structures nn_trees_structure nn_trees_phrase preconj_trees_both dobj_involves_structures dobj_involves_trees nsubj_involves_that rcmod_formalism_involves det_formalism_a appos_Grammar_formalism appos_Grammar_LFG amod_Grammar_Lexical-Functional prep_on_based_Grammar auxpass_based_is nsubjpass_based_approach vmod_based_Inspired nn_recovery_dependency amod_recovery_non-local amod_recovery_Chinese prep_to_approach_recovery poss_approach_our amod_treebank_Penn-II conj_and_English_treebank prep_for_designed_treebank prep_for_designed_English advmod_designed_originally auxpass_designed_was nsubjpass_designed_which rcmod_methodology_designed amod_methodology_s dep_methodology_Cahill amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et agent_Inspired_methodology
D07-1027	P04-1041	o	Our method revises and considerably extends the approach of -LRB- Cahill et al. 2004 -RRB- originally designed for English and to the best of our knowledge is the first NLD recovery algorithm for Chinese	prep_for_algorithm_Chinese nn_algorithm_recovery nn_algorithm_NLD amod_algorithm_first det_algorithm_the cop_algorithm_is poss_knowledge_our prep_of_best_knowledge det_best_the prep_for_designed_English advmod_designed_originally prep_to_Cahill_best cc_Cahill_and vmod_Cahill_designed amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et prep_of_approach_Cahill det_approach_the dobj_extends_approach advmod_extends_considerably nsubj_extends_method dep_revises_algorithm conj_and_revises_extends nsubj_revises_method poss_method_Our ccomp_``_extends ccomp_``_revises
D07-1027	P04-1041	n	The evaluation shows that our algorithm considerably outperforms -LRB- Cahill et al. 2004 -RRB- s with respect to Chinese data	amod_data_Chinese prep_with_respect_to_s_data nsubj_s_Cahill amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et ccomp_outperforms_s advmod_outperforms_considerably nsubj_outperforms_algorithm mark_outperforms_that poss_algorithm_our ccomp_shows_outperforms nsubj_shows_evaluation det_evaluation_The
D07-1027	P04-1041	o	In Section 3 we review -LRB- Cahill et al. 2004 -RRB- s method for recovering English NLDs in treebank-based LFG approximations	nn_approximations_LFG amod_approximations_treebank-based prep_in_NLDs_approximations amod_NLDs_English dobj_recovering_NLDs prepc_for_method_recovering amod_method_s amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et dobj_review_method dep_review_Cahill nsubj_review_we prep_in_review_Section num_Section_3
D07-1027	P04-1041	o	3.2 F-Structure Based NLD Recovery -LRB- Cahill et al. 2004 -RRB- presented a NLD recovery algorithm operating at LFG f-structure for treebankbased LFG approximations	nn_approximations_LFG amod_approximations_treebankbased nn_f-structure_LFG prep_for_operating_approximations prep_at_operating_f-structure nn_operating_algorithm nn_operating_recovery nn_operating_NLD det_operating_a dobj_presented_operating nsubj_presented_Recovery amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et dep_Recovery_Cahill nn_Recovery_NLD ccomp_Based_presented nsubj_Based_F-Structure num_F-Structure_3.2
D07-1027	P04-1041	o	-LRB- Cahill et al. 2004 -RRB- s approach for English resolves three LDD types in parser output trees without traces and coindexation -LRB- Figure 2 -LRB- b -RRB- -RRB- i.e. topicalisation -LRB- TOPIC -RRB- wh-movement in relative clauses -LRB- TOPIC REL -RRB- and interrogatives -LRB- FOCUS -RRB-	appos_interrogatives_FOCUS nn_REL_TOPIC conj_and_clauses_interrogatives appos_clauses_REL amod_clauses_relative prep_in_wh-movement_interrogatives prep_in_wh-movement_clauses appos_topicalisation_wh-movement appos_topicalisation_TOPIC pobj_i.e._topicalisation appos_Figure_b num_Figure_2 dep_coindexation_Figure conj_and_traces_coindexation nn_trees_output nn_trees_parser nn_types_LDD num_types_three prep_resolves_i.e. prep_without_resolves_coindexation prep_without_resolves_traces prep_in_resolves_trees dobj_resolves_types nsubj_resolves_s prep_for_s_English dobj_s_approach nsubj_s_Cahill dep_Cahill_2004 dep_Cahill_al. nn_Cahill_et
D07-1027	P04-1041	o	Inspired by -LRB- Cahill et al. 2004 Burke et al. 2004 -RRB- we have implemented an f-structure annotation algorithm to automatically obtain f-structures from CFG-trees in the CTB5 .1	nn_.1_CTB5 det_.1_the prep_in_CFG-trees_.1 prep_from_obtain_CFG-trees dobj_obtain_f-structures advmod_obtain_automatically aux_obtain_to nn_algorithm_annotation amod_algorithm_f-structure det_algorithm_an vmod_implemented_obtain dobj_implemented_algorithm aux_implemented_have nsubj_implemented_we vmod_implemented_Inspired num_Burke_2004 nn_Burke_al. nn_Burke_et dep_Cahill_Burke appos_Cahill_2004 dep_Cahill_al. nn_Cahill_et dep_by_Cahill prep_Inspired_by
D07-1027	P04-1041	o	4.2 Adaptation to Chinese -LRB- Cahill et al. 2004 -RRB- s algorithm -LRB- Section 3.2 -RRB- only resolves certain NLDs with known types of antecedents -LRB- TOPIC TOPIC REL and FOCUS -RRB- at fstructures	nn_REL_TOPIC conj_and_TOPIC_FOCUS conj_and_TOPIC_REL dep_antecedents_FOCUS dep_antecedents_REL dep_antecedents_TOPIC prep_of_types_antecedents amod_types_known prep_with_NLDs_types amod_NLDs_certain prep_at_resolves_fstructures dobj_resolves_NLDs advmod_resolves_only num_Section_3.2 appos_algorithm_Section dep_s_resolves dobj_s_algorithm nsubj_s_Adaptation amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et dep_Chinese_Cahill prep_to_Adaptation_Chinese num_Adaptation_4.2
D07-1027	P04-1041	o	In order to resolve all Chinese NLDs represented in the CTB we modify and substantially extend the -LRB- Cahill et al. 2004 -RRB- -LRB- henceforth C04 for short -RRB- algorithm as follows Given the set of subcat frames s for the word w and a set of paths p for the trace t the algorithm traverses the f-structure f to predict a dislocated argument t at a sub-fstructure h by comparing the local PRED w to ws subcat frames s t can be inserted at h if h together with t is complete and coherent relative to subcat frame s traverse f starting from t along the path p link t to its antecedent a if ps ending GF a exists in a sub-f-structure within f or leave t without an antecedent if an empty path for t exists In the modified algorithm we condition the probability of NLD path p -LRB- including the empty path without an antecedent -RRB- on the GF associated of the trace t rather than the antecedent a as in C04	prep_in_as_C04 det_as_a advmod_antecedent_as det_antecedent_the conj_negcc_t_antecedent nn_t_trace det_t_the prep_of_associated_antecedent prep_of_associated_t vmod_GF_associated det_GF_the det_antecedent_an prep_without_path_antecedent amod_path_empty det_path_the prep_including_p_path nn_p_path nn_p_NLD prep_on_probability_GF prep_of_probability_p det_probability_the dobj_condition_probability nsubj_condition_we amod_algorithm_modified det_algorithm_the prep_in_exists_algorithm nsubj_exists_path mark_exists_if prep_for_path_t amod_path_empty det_path_an det_antecedent_an advcl_leave_exists prep_without_leave_antecedent dobj_leave_t nsubj_leave_w prep_within_sub-f-structure_f det_sub-f-structure_a prep_in_exists_sub-f-structure det_exists_a nsubj_exists_GF ccomp_ending_exists vmod_ps_ending prep_if_a_ps amod_a_antecedent poss_a_its nn_t_link nn_t_p nn_t_path det_t_the prep_to_starting_a prep_along_starting_t prep_from_starting_t amod_f_starting nn_f_traverse dobj_s_f advmod_s_relative nn_frame_subcat prep_to_relative_frame conj_coherent_s nsubj_coherent_w cop_complete_is nsubj_complete_h mark_complete_if prep_together_with_h_t parataxis_inserted_condition conj_or_inserted_leave conj_and_inserted_coherent advcl_inserted_complete prep_at_inserted_h auxpass_inserted_be aux_inserted_can nsubjpass_inserted_w amod_t_s nn_frames_subcat dobj_ws_frames aux_ws_to dobj_w_t xcomp_w_ws amod_PRED_local det_PRED_the dobj_comparing_PRED amod_h_sub-fstructure det_h_a nn_t_argument amod_t_dislocated det_t_a ccomp_predict_leave ccomp_predict_coherent ccomp_predict_inserted prepc_by_predict_comparing prep_at_predict_h dobj_predict_t dep_f-structure_f det_f-structure_the dep_traverses_predict prep_traverses_to dobj_traverses_f-structure nsubj_traverses_algorithm vmod_traverses_set vmod_traverses_Given det_algorithm_the nn_t_trace det_t_the prep_for_p_t dep_paths_p prep_of_set_paths det_set_a nn_w_word det_w_the nn_s_frames nn_s_subcat prep_for_set_w prep_of_set_s det_set_the conj_and_Given_set pobj_Given_set mark_follows_as amod_algorithm_short amod_algorithm_for nn_algorithm_C04 det_algorithm_the nn_C04_henceforth dep_Cahill_2004 dep_Cahill_al. nn_Cahill_et dep_the_Cahill advcl_extend_follows dobj_extend_algorithm advmod_extend_substantially nsubj_extend_we parataxis_modify_traverses conj_and_modify_extend nsubj_modify_we advcl_modify_resolve det_CTB_the prep_in_represented_CTB vmod_NLDs_represented amod_NLDs_Chinese det_NLDs_all dobj_resolve_NLDs aux_resolve_to dep_resolve_order mark_resolve_In
D07-1028	P04-1041	o	The LFG annotation algorithm of -LRB- Cahill et al. 2004 -RRB- was used to produce the f-structures for development test and training sets	nn_sets_training nn_sets_test conj_and_test_training appos_f-structures_sets prep_for_f-structures_development det_f-structures_the dobj_produce_f-structures aux_produce_to xcomp_used_produce auxpass_used_was nsubjpass_used_algorithm amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et prep_of_algorithm_Cahill nn_algorithm_annotation nn_algorithm_LFG det_algorithm_The
D09-1085	P04-1041	o	It has also obtained competitive scores on general GR evaluation corpora -LRB- Cahill et al. 2004 -RRB-	amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et nn_corpora_evaluation nn_corpora_GR amod_corpora_general amod_scores_competitive dep_obtained_Cahill prep_on_obtained_corpora dobj_obtained_scores advmod_obtained_also aux_obtained_has nsubj_obtained_It ccomp_``_obtained
D09-1085	P04-1041	o	3.2 The parsers The parsers that we chose to evaluate are the C&C CCG parser -LRB- Clark and Curran 2007 -RRB- the Enju HPSG parser -LRB- Miyao and Tsujii 2005 -RRB- the RASP parser -LRB- Briscoe et al. 2006 -RRB- the Stanford parser -LRB- Klein and Manning 2003 -RRB- and the DCU postprocessor of PTB parsers -LRB- Cahill et al. 2004 -RRB- based on LFG and applied to the output of the Charniak and Johnson reranking parser	amod_parser_reranking nn_parser_Johnson conj_and_Charniak_parser det_Charniak_the prep_of_output_parser prep_of_output_Charniak det_output_the prep_to_applied_output conj_and_LFG_applied prep_on_based_applied prep_on_based_LFG amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et nn_parsers_PTB vmod_postprocessor_based dep_postprocessor_Cahill prep_of_postprocessor_parsers nn_postprocessor_DCU det_postprocessor_the dep_Klein_2003 conj_and_Klein_Manning appos_parser_Manning appos_parser_Klein nn_parser_Stanford det_parser_the amod_Briscoe_2006 dep_Briscoe_al. nn_Briscoe_et appos_parser_Briscoe nn_parser_RASP det_parser_the dep_Miyao_2005 conj_and_Miyao_Tsujii appos_parser_Tsujii appos_parser_Miyao nn_parser_HPSG nn_parser_Enju det_parser_the dep_Clark_2007 conj_and_Clark_Curran conj_and_parser_postprocessor conj_and_parser_parser conj_and_parser_parser conj_and_parser_parser dep_parser_Curran dep_parser_Clark nn_parser_CCG nn_parser_C&C det_parser_the cop_parser_are nsubj_parser_3.2 aux_evaluate_to xcomp_chose_evaluate nsubj_chose_we mark_chose_that ccomp_parsers_chose det_parsers_The dep_parsers_parsers det_parsers_The dep_3.2_parsers
E06-1010	P04-1041	o	Most of this work has so far focused either on post-processing to recover non-local dependencies from context-free parse trees -LRB- Johnson 2002 Jijkoun and De Rijke 2004 Levy and Manning 2004 Campbell 2004 -RRB- or on incorporating nonlocal dependency information in nonterminal categories in constituency representations -LRB- Dienes and Dubey 2003 Hockenmaier 2003 Cahill et al. 2004 -RRB- or in the categories used to label arcs in dependency representations -LRB- Nivre and Nilsson 2005 -RRB-	amod_Nivre_2005 conj_and_Nivre_Nilsson dep_representations_Nilsson dep_representations_Nivre nn_representations_dependency prep_in_arcs_representations dobj_label_arcs aux_label_to xcomp_used_label vmod_categories_used det_categories_the num_Cahill_2004 nn_Cahill_al. nn_Cahill_et num_Hockenmaier_2003 conj_and_Dienes_Cahill conj_and_Dienes_Hockenmaier conj_and_Dienes_2003 conj_and_Dienes_Dubey conj_or_representations_categories dep_representations_Cahill dep_representations_Hockenmaier dep_representations_2003 dep_representations_Dubey dep_representations_Dienes nn_representations_constituency prep_in_categories_categories prep_in_categories_representations amod_categories_nonterminal prep_in_information_categories nn_information_dependency amod_information_nonlocal dobj_incorporating_information pcomp_on_incorporating dep_Campbell_2004 conj_and_Levy_2004 conj_and_Levy_Manning appos_Rijke_2004 nn_Rijke_De conj_and_Jijkoun_Rijke conj_or_Johnson_on dep_Johnson_Campbell dep_Johnson_2004 dep_Johnson_Manning dep_Johnson_Levy dep_Johnson_Rijke dep_Johnson_Jijkoun amod_Johnson_2002 dep_trees_on dep_trees_Johnson nn_trees_parse amod_trees_context-free amod_dependencies_non-local prep_from_recover_trees dobj_recover_dependencies aux_recover_to xcomp_focused_recover prep_on_focused_post-processing advmod_focused_either advmod_focused_far advmod_far_so vmod_has_focused nsubj_has_Most det_work_this prep_of_Most_work
J05-3003	P04-1041	o	However more recent work -LRB- Cahill et al. 2002 Cahill McCarthy et al. 2004 -RRB- has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank -LRB- Marcus et al. 1994 -RRB- containing more than 1,000,000 words and 49,000 sentences	num_sentences_49,000 conj_and_words_sentences num_words_1,000,000 quantmod_1,000,000_than mwe_than_more dobj_containing_sentences dobj_containing_words dep_1994_al. nn_al._et num_Marcus_1994 nn_Treebank_Penn-II det_Treebank_the nn_techniques_annotation dobj_scaling_techniques prt_scaling_up conj_and_evolving_scaling prep_to_efforts_Treebank prepc_in_efforts_scaling prepc_in_efforts_evolving xcomp_presented_containing dep_presented_Marcus dobj_presented_efforts aux_presented_has nsubj_presented_work advmod_presented_However dep_2004_al. nn_al._et num_Cahill_2004 appos_Cahill_McCarthy num_al._2002 nn_al._et dep_Cahill_Cahill dep_Cahill_al. appos_work_Cahill amod_work_recent advmod_work_more
J05-3003	P04-1041	o	Our approach is based on earlier work on LFG semantic form extraction -LRB- van Genabith Sadler and Way 1999 -RRB- and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures -LRB- Cahill et al. 2002 Cahill McCarthy et al. 2004 -RRB-	advmod_2004_al. nn_al._et num_Cahill_2004 appos_Cahill_McCarthy num_al._2002 nn_al._et dep_Cahill_Cahill dep_Cahill_al. nn_f-structures_LFG nn_Treebanks_Penn-III conj_and_Penn-II_Treebanks det_Penn-II_the prep_with_annotating_f-structures dobj_annotating_Treebanks dobj_annotating_Penn-II advmod_annotating_automatically prepc_in_progress_annotating amod_progress_recent num_Way_1999 conj_and_Genabith_Way conj_and_Genabith_Sadler nn_Genabith_van conj_and_extraction_progress dep_extraction_Way dep_extraction_Sadler dep_extraction_Genabith nn_extraction_form amod_extraction_semantic nn_extraction_LFG prep_on_work_progress prep_on_work_extraction amod_work_earlier dep_based_Cahill prep_on_based_work auxpass_based_is nsubjpass_based_approach poss_approach_Our ccomp_``_based
J05-3003	P04-1041	o	We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank -LRB- Brants et al. 2002 -RRB- and Penn Chinese Treebank -LRB- Xue Chiou and Palmer 2002 -RRB- extracting wide-coverage probabilistic LFG grammar 361 Computational Linguistics Volume 31 Number 3 approximations and lexical resources for German -LRB- Cahill et al. 2003 -RRB- and Chinese -LRB- Burke Lam et al. 2004 -RRB-	advmod_2004_al. nn_al._et dep_Lam_2004 nn_Lam_Burke dep_2003_al. nn_al._et num_Cahill_2003 conj_and_German_Chinese dep_German_Cahill amod_resources_lexical num_approximations_3 nn_approximations_Number num_Volume_31 nn_Volume_Linguistics nn_Volume_Computational num_Volume_361 nn_Volume_grammar nn_Volume_LFG amod_Volume_probabilistic conj_and_wide-coverage_resources conj_and_wide-coverage_approximations conj_and_wide-coverage_Volume dep_extracting_Lam prep_for_extracting_Chinese prep_for_extracting_German dobj_extracting_resources dobj_extracting_approximations dobj_extracting_Volume dobj_extracting_wide-coverage num_Palmer_2002 conj_and_Xue_Palmer conj_and_Xue_Chiou dep_Treebank_Palmer dep_Treebank_Chiou dep_Treebank_Xue nn_Treebank_Chinese nn_Treebank_Penn dep_2002_al. nn_al._et num_Brants_2002 conj_and_Treebank_Treebank appos_Treebank_Brants nn_Treebank_TIGER det_Treebank_the nn_methodology_acquisition nn_methodology_grammar nn_methodology_unification amod_methodology_general amod_methodology_more poss_methodology_our xcomp_applied_extracting prep_to_applied_Treebank prep_to_applied_Treebank dobj_applied_methodology advmod_applied_also aux_applied_have nsubj_applied_We ccomp_``_applied
J07-3004	P04-1041	o	Because treebank annotation for individual formalisms is prohibitively expensive there have been a number of efforts to extract TAGs LFGs and more recently HPSGs from the Penn Treebank -LRB- Xia 1999 Chen and Vijay-Shanker 2000 Xia Palmer and Joshi 2000 Xia 2001 Cahill et al. 2002 Miyao Ninomiya and Tsujii 2004 ODonovan et al. 2005 Shen and Joshi 2005 Chen Bangalore and Vijay-Shanker 2006 -RRB-	num_Vijay-Shanker_2006 conj_and_Chen_Vijay-Shanker conj_and_Chen_Bangalore dep_Shen_2005 conj_and_Shen_Joshi num_al._2005 nn_al._et nn_al._ODonovan num_Tsujii_2004 conj_and_Miyao_Tsujii conj_and_Miyao_Ninomiya dep_al._2002 nn_al._et nn_al._Cahill num_Xia_2001 num_Joshi_2000 conj_and_Xia_Joshi conj_and_Xia_Palmer num_Chen_2000 conj_and_Chen_Vijay-Shanker dep_Xia_Vijay-Shanker dep_Xia_Bangalore dep_Xia_Chen dep_Xia_Joshi dep_Xia_Shen dep_Xia_al. dep_Xia_Tsujii dep_Xia_Ninomiya dep_Xia_Miyao dep_Xia_al. dep_Xia_Xia dep_Xia_Joshi dep_Xia_Palmer dep_Xia_Xia dep_Xia_Vijay-Shanker dep_Xia_Chen num_Xia_1999 appos_Treebank_Xia nn_Treebank_Penn det_Treebank_the advmod_recently_more prep_from_TAGs_Treebank conj_and_TAGs_HPSGs conj_and_TAGs_recently conj_and_TAGs_LFGs dobj_extract_HPSGs dobj_extract_recently dobj_extract_LFGs dobj_extract_TAGs aux_extract_to vmod_efforts_extract prep_of_number_efforts det_number_a cop_number_been aux_number_have expl_number_there advcl_number_expensive advmod_expensive_prohibitively cop_expensive_is nsubj_expensive_annotation mark_expensive_Because amod_formalisms_individual prep_for_annotation_formalisms amod_annotation_treebank
J07-3004	P04-1041	o	For the Penn Treebank our research and the work of others -LRB- Xia 1999 Chen and Vijay-Shanker 2004 Chiang 2000 Cahill et al. 2002 -RRB- have shown that such a correspondence exists in most cases	amod_cases_most prep_in_exists_cases nsubj_exists_correspondence mark_exists_that det_correspondence_a predet_correspondence_such ccomp_shown_exists aux_shown_have nsubj_shown_work nsubj_shown_research prep_for_shown_Treebank dep_al._2002 nn_al._et nn_al._Cahill num_Chiang_2000 num_Vijay-Shanker_2004 conj_and_Chen_Vijay-Shanker dep_Xia_al. dep_Xia_Chiang dep_Xia_Vijay-Shanker dep_Xia_Chen num_Xia_1999 appos_others_Xia prep_of_work_others det_work_the conj_and_research_work poss_research_our nn_Treebank_Penn det_Treebank_the
J07-4004	P04-1041	o	Statistical parsers have been developed for TAG -LRB- Chiang 2000 Sarkar and Joshi 2003 -RRB- LFG -LRB- Riezler et al. 2002 Kaplan et al. 2004 Cahill et al. 2004 -RRB- and HPSG -LRB- Toutanova et al. 2002 Toutanova Markova and Manning 2004 Miyao and Tsujii 2004 Malouf and van Noord 2004 -RRB- among others	num_Noord_2004 nn_Noord_van conj_and_Malouf_Noord num_Tsujii_2004 conj_and_Miyao_Tsujii num_Manning_2004 conj_and_Toutanova_Manning conj_and_Toutanova_Markova dep_Toutanova_Noord dep_Toutanova_Malouf dep_Toutanova_Tsujii dep_Toutanova_Miyao dep_Toutanova_Manning dep_Toutanova_Markova dep_Toutanova_Toutanova dep_Toutanova_2002 dep_Toutanova_al. nn_Toutanova_et appos_HPSG_Toutanova dep_al._2004 nn_al._et nn_al._Cahill num_al._2004 nn_al._et nn_al._Kaplan num_al._2002 nn_al._et dep_Riezler_al. dep_Riezler_al. dep_Riezler_al. appos_LFG_Riezler dep_Sarkar_2003 conj_and_Sarkar_Joshi dep_Chiang_Joshi dep_Chiang_Sarkar num_Chiang_2000 conj_and_TAG_HPSG conj_and_TAG_LFG appos_TAG_Chiang prep_among_developed_others prep_for_developed_HPSG prep_for_developed_LFG prep_for_developed_TAG auxpass_developed_been aux_developed_have nsubjpass_developed_parsers amod_parsers_Statistical
N06-1019	P04-1041	o	Even robust parsers using linguistically sophisticated formalisms such as TAG -LRB- Chiang 2000 -RRB- CCG -LRB- Clark and Curran 2004b Hockenmaier 2003 -RRB- HPSG -LRB- Miyao et al. 2004 -RRB- and LFG -LRB- Riezler et al. 2002 Cahill et al. 2004 -RRB- often use training data derived from the Penn Treebank	nn_Treebank_Penn det_Treebank_the prep_from_derived_Treebank vmod_data_derived nn_data_training dobj_use_data advmod_use_often nsubj_use_LFG nsubj_use_HPSG nsubj_use_2004b nsubj_use_Curran nsubj_use_Clark tmod_use_CCG num_Cahill_2004 nn_Cahill_al. nn_Cahill_et dep_Riezler_Cahill appos_Riezler_2002 dep_Riezler_al. nn_Riezler_et amod_Miyao_2004 dep_Miyao_al. nn_Miyao_et dep_HPSG_Miyao dep_Hockenmaier_2003 dep_Clark_Riezler conj_and_Clark_LFG conj_and_Clark_HPSG dep_Clark_Hockenmaier conj_and_Clark_2004b conj_and_Clark_Curran dep_Chiang_2000 rcmod_TAG_use dep_TAG_Chiang prep_such_as_formalisms_TAG amod_formalisms_sophisticated advmod_formalisms_linguistically dobj_using_formalisms vmod_parsers_using amod_parsers_robust advmod_robust_Even dep_``_parsers
N07-2031	P04-1041	o	The feasibility of such post-parse deepening -LRB- for a statistical parser -RRB- is demonstrated by Cahill et al -LRB- 2004 -RRB-	dep_al_2004 dep_Cahill_al nn_Cahill_et agent_demonstrated_Cahill auxpass_demonstrated_is nsubjpass_demonstrated_feasibility amod_parser_statistical det_parser_a prep_for_deepening_parser amod_post-parse_deepening amod_post-parse_such prep_of_feasibility_post-parse det_feasibility_The
P04-1047	P04-1041	o	Our approach is based on earlier work on LFG semantic form extraction -LRB- van Genabith et al. 1999 -RRB- and recent progress in automatically annotating the Penn-II treebank with LFG f-structures -LRB- Cahill et al. 2004b -RRB-	nn_al._et appos_Cahill_2004b dep_Cahill_al. nn_f-structures_LFG nn_treebank_Penn-II det_treebank_the prep_with_annotating_f-structures dobj_annotating_treebank advmod_annotating_automatically prepc_in_progress_annotating amod_progress_recent amod_Genabith_1999 dep_Genabith_al. nn_Genabith_et nn_Genabith_van conj_and_extraction_progress dep_extraction_Genabith nn_extraction_form amod_extraction_semantic nn_extraction_LFG prep_on_work_progress prep_on_work_extraction amod_work_earlier dep_based_Cahill prep_on_based_work auxpass_based_is nsubjpass_based_approach poss_approach_Our ccomp_``_based
P04-1047	P04-1041	o	In this paper we show how the extraction process can be scaled to the complete Wall Street Journal -LRB- WSJ -RRB- section of the Penn-II treebank with about 1 million words in 50,000 sentences based on the automatic LFG f-structure annotation algorithm described in -LRB- Cahill et al. 2004b -RRB-	appos_Cahill_2004b dep_Cahill_al. nn_Cahill_et prep_in_described_Cahill vmod_algorithm_described nn_algorithm_annotation nn_algorithm_f-structure nn_algorithm_LFG amod_algorithm_automatic det_algorithm_the num_sentences_50,000 pobj_words_algorithm prepc_based_on_words_on prep_in_words_sentences num_words_million number_million_1 quantmod_million_about nn_treebank_Penn-II det_treebank_the prep_of_section_treebank nn_section_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall amod_Journal_complete det_Journal_the prep_with_scaled_words prep_to_scaled_section auxpass_scaled_be aux_scaled_can nsubjpass_scaled_process advmod_scaled_how nn_process_extraction det_process_the ccomp_show_scaled nsubj_show_we prep_in_show_paper det_paper_this
P04-1047	P04-1041	o	We are already using the extracted semantic forms in parsing new text with robust wide-coverage PCFG-based LFG grammar approximations automatically acquired from the f-structure annotated Penn-II treebank -LRB- Cahill et al. 2004a -RRB-	appos_Cahill_2004a dep_Cahill_al. nn_Cahill_et dep_treebank_Cahill nn_treebank_Penn-II amod_treebank_annotated nn_treebank_f-structure det_treebank_the prep_from_acquired_treebank advmod_acquired_automatically vmod_approximations_acquired nn_approximations_grammar nn_approximations_LFG amod_approximations_PCFG-based amod_approximations_wide-coverage appos_robust_approximations prep_with_text_robust amod_text_new dobj_parsing_text amod_forms_semantic amod_forms_extracted det_forms_the prepc_in_using_parsing dobj_using_forms advmod_using_already aux_using_are nsubj_using_We ccomp_``_using
P04-1047	P04-1041	o	We utilise the automatic annotation algorithm of -LRB- Cahill et al. 2004b -RRB- to derive a version of Penn-II where each node in each tree is annotated with an LFG functional annotation -LRB- i.e. an attribute value structure equation -RRB-	nn_equation_structure nn_equation_value nn_equation_attribute det_equation_an advmod_equation_i.e. appos_annotation_equation amod_annotation_functional nn_annotation_LFG det_annotation_an prep_with_annotated_annotation cop_annotated_is nsubj_annotated_node advmod_annotated_where det_tree_each prep_in_node_tree det_node_each rcmod_Penn-II_annotated prep_of_version_Penn-II det_version_a dobj_derive_version aux_derive_to appos_Cahill_2004b dep_Cahill_al. nn_Cahill_et prep_of_algorithm_Cahill nn_algorithm_annotation amod_algorithm_automatic det_algorithm_the vmod_utilise_derive dobj_utilise_algorithm nsubj_utilise_We ccomp_``_utilise
P04-1047	P04-1041	o	-LRB- Cahill et al. 2004b -RRB- provide four sets of annotation principles one for non-coordinate configurations one for coordinate configurations one for traces -LRB- long distance dependencies -RRB- and a final catch all and clean up phase	dep_up_phase prep_all_up conj_and_all_clean dep_catch_clean dep_catch_all amod_catch_final det_catch_a nn_dependencies_distance amod_dependencies_long conj_and_traces_catch appos_traces_dependencies prep_for_one_catch prep_for_one_traces amod_configurations_coordinate prep_for_one_configurations amod_configurations_non-coordinate appos_one_one appos_one_one prep_for_one_configurations nn_principles_annotation prep_of_sets_principles num_sets_four parataxis_provide_one dobj_provide_sets nsubj_provide_Cahill nn_al._et appos_Cahill_2004b dep_Cahill_al.
P04-1047	P04-1041	o	The algorithm of -LRB- Cahill et al. 2004b -RRB- translates the traces into corresponding re-entrancies in the f-structure representation -LRB- Figure 1 -RRB-	num_Figure_1 nn_representation_f-structure det_representation_the appos_re-entrancies_Figure prep_in_re-entrancies_representation amod_re-entrancies_corresponding prep_into_traces_re-entrancies det_traces_the dobj_translates_traces nsubj_translates_algorithm appos_Cahill_2004b dep_Cahill_al. nn_Cahill_et prep_of_algorithm_Cahill det_algorithm_The ccomp_``_translates
P04-1047	P04-1041	o	-LRB- Cahill et al. 2004b -RRB- measure annotation quality in terms of precision and recall against manually constructed gold-standard f-structures for 105 randomly selected trees from section 23 of the WSJ section of Penn-II	prep_of_section_Penn-II nn_section_WSJ det_section_the prep_of_section_section num_section_23 prep_from_trees_section amod_trees_selected num_trees_105 advmod_selected_randomly prep_for_f-structures_trees amod_f-structures_gold-standard conj_constructed_f-structures advmod_constructed_manually conj_and_precision_recall prep_of_terms_recall prep_of_terms_precision nn_quality_annotation prepc_against_measure_constructed prep_in_measure_terms dep_measure_quality dep_measure_Cahill nn_al._et dep_Cahill_2004b dep_Cahill_al.
P05-1013	P04-1041	o	Finally since non-projective constructions often involve long-distance dependencies the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing -LRB- Johnson 2002 Dienes and Dubey 2003 Jijkoun and de Rijke 2004 Cahill et al. 2004 Levy and Manning 2004 Campbell 2004 -RRB-	amod_Campbell_2004 num_Cahill_2004 nn_Cahill_al. nn_Cahill_et amod_Rijke_de dep_Jijkoun_Campbell conj_and_Jijkoun_2004 conj_and_Jijkoun_Manning conj_and_Jijkoun_Levy conj_and_Jijkoun_Cahill conj_and_Jijkoun_2004 conj_and_Jijkoun_Rijke conj_and_Dienes_2004 conj_and_Dienes_Manning conj_and_Dienes_Levy conj_and_Dienes_Cahill conj_and_Dienes_2004 conj_and_Dienes_Rijke conj_and_Dienes_Jijkoun conj_and_Dienes_2003 conj_and_Dienes_Dubey dep_Johnson_Jijkoun dep_Johnson_2003 dep_Johnson_Dubey dep_Johnson_Dienes amod_Johnson_2002 dep_parsing_Johnson amod_parsing_constituency-based amod_dependencies_non-local conj_and_categories_dependencies amod_categories_empty prep_in_recovery_parsing prep_of_recovery_dependencies prep_of_recovery_categories det_recovery_the prep_to_related_recovery advmod_related_closely cop_related_is nsubj_related_problem advcl_related_involve advmod_related_Finally det_problem_the amod_dependencies_long-distance dobj_involve_dependencies advmod_involve_often nsubj_involve_constructions mark_involve_since amod_constructions_non-projective
P06-1130	P04-1041	o	Recent work on the automatic acquisition of multilingual LFG resources from treebanks for Chinese German and Spanish -LRB- Burke et al. 2004 Cahill et al. 2005 ODonovan et al. 2005 -RRB- has shown that given a suitable treebank it is possible to automatically acquire high quality LFG resources in a very short space of time	prep_of_space_time amod_space_short det_space_a advmod_short_very nn_resources_LFG nn_resources_quality amod_resources_high prep_in_acquire_space dobj_acquire_resources advmod_acquire_automatically aux_acquire_to xcomp_possible_acquire cop_possible_is nsubj_possible_it prep_possible_given mark_possible_that amod_treebank_suitable det_treebank_a pobj_given_treebank ccomp_shown_possible aux_shown_has nsubj_shown_work num_ODonovan_2005 nn_ODonovan_al. nn_ODonovan_et num_Cahill_2005 nn_Cahill_al. nn_Cahill_et dep_Burke_ODonovan dep_Burke_Cahill amod_Burke_2004 dep_Burke_al. nn_Burke_et conj_and_Chinese_Spanish conj_and_Chinese_German nn_resources_LFG amod_resources_multilingual prep_for_acquisition_Spanish prep_for_acquisition_German prep_for_acquisition_Chinese prep_from_acquisition_treebanks prep_of_acquisition_resources amod_acquisition_automatic det_acquisition_the appos_work_Burke prep_on_work_acquisition amod_work_Recent
P06-1130	P04-1041	o	c2006 Association for Computational Linguistics Robust PCFG-Based Generation using Automatically Acquired LFG Approximations Aoife Cahill1 and Josef van Genabith1 ,2 1 National Centre for Language Technology -LRB- NCLT -RRB- School of Computing Dublin City University Dublin 9 Ireland 2 Center for Advanced Studies IBM Dublin Ireland -LCB- acahill josef -RCB- @computing dcu.ie Abstract We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations -LRB- Cahill et al. 2004 -RRB- automatically extracted from treebanks maximising the probability of a tree given an f-structure	det_f-structure_an pobj_given_f-structure prep_tree_given det_tree_a prep_of_probability_tree det_probability_the dobj_maximising_probability prep_from_extracted_treebanks advmod_extracted_automatically amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et vmod_approximations_extracted appos_approximations_Cahill nn_approximations_LFG amod_approximations_wide-coverage prep_on_based_approximations vmod_generation_based amod_generation_probabilistic amod_generation_robust prep_for_architecture_generation amod_architecture_PCFG-based amod_architecture_novel det_architecture_a vmod_present_maximising dobj_present_architecture nsubj_present_We dep_present_Abstract dep_present_dcu.ie nsubj_present_Association dep_acahill_josef vmod_Ireland_@computing dep_Ireland_acahill nn_Dublin_IBM nn_Studies_Advanced prep_for_Center_Studies num_Center_2 nn_Center_Ireland num_Dublin_9 appos_University_Ireland appos_University_Dublin appos_University_Center appos_University_Dublin nn_University_City nn_University_Dublin prep_of_School_Computing dep_Technology_School appos_Technology_NCLT nn_Technology_Language appos_Centre_University prep_for_Centre_Technology nn_Centre_National num_Centre_1 num_Centre_,2 dep_Genabith1_Centre nn_Genabith1_van nn_Genabith1_Josef conj_and_Cahill1_Genabith1 nn_Cahill1_Aoife nn_Cahill1_Approximations nn_Cahill1_LFG dep_Acquired_Genabith1 dep_Acquired_Cahill1 advmod_Acquired_Automatically ccomp_using_Acquired amod_Generation_PCFG-Based nn_Generation_Robust nn_Generation_Linguistics nn_Generation_Computational vmod_Association_using prep_for_Association_Generation nn_Association_c2006
P06-1130	P04-1041	o	In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage robust Lexical Functional Grammar -LRB- LFG -RRB- approximations automatically extracted from treebanks -LRB- Cahill et al. 2004 -RRB-	amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et prep_from_extracted_treebanks advmod_extracted_automatically vmod_approximations_extracted nn_approximations_wide-coverage appos_Grammar_LFG amod_Grammar_Functional amod_Grammar_Lexical amod_Grammar_robust appos_wide-coverage_Grammar prep_on_based_approximations vmod_generation_based amod_generation_probabilistic prep_for_architecture_generation amod_architecture_PCFG-based amod_architecture_novel det_architecture_a dep_present_Cahill dobj_present_architecture nsubj_present_we prep_in_present_paper det_paper_this rcmod_``_present
P06-2018	P04-1041	o	The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English -LRB- Cahill et al. 2004 -RRB- uses configurational categorial function tag and trace information	nn_information_trace nn_tag_function conj_and_configurational_information conj_and_configurational_tag conj_and_configurational_categorial dobj_uses_information dobj_uses_tag dobj_uses_categorial dobj_uses_configurational nsubj_uses_algorithm amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et prep_for_treebank_English nn_treebank_Penn-II det_treebank_the nn_resources_LFG prep_from_inducing_treebank dobj_inducing_resources prepc_for_used_inducing dep_algorithm_Cahill vmod_algorithm_used nn_algorithm_annotation amod_algorithm_f-structure det_algorithm_The
P06-2018	P04-1041	o	It has been shown that the methods can be ported to other languages and treebanks -LRB- Burke et al. 2004 Cahill et al. 2003 -RRB- including Cast3LB -LRB- ODonovan et al. 2005 -RRB-	amod_ODonovan_2005 dep_ODonovan_al. nn_ODonovan_et dep_Cast3LB_ODonovan num_Cahill_2003 nn_Cahill_al. nn_Cahill_et dep_Burke_Cahill amod_Burke_2004 dep_Burke_al. nn_Burke_et conj_and_languages_treebanks amod_languages_other prep_to_ported_treebanks prep_to_ported_languages auxpass_ported_be aux_ported_can nsubjpass_ported_methods mark_ported_that det_methods_the prep_including_shown_Cast3LB dep_shown_Burke ccomp_shown_ported auxpass_shown_been aux_shown_has nsubjpass_shown_It
P06-2018	P04-1041	o	1 Introduction The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual LexicalFunctional Grammar -LRB- LFG -RRB- -LRB- Bresnan 2001 -RRB- resources from treebanks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers -LRB- Cahill et al. 2004 -RRB-	amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et amod_parsers_probabilistic agent_produced_parsers vmod_trees_produced nn_trees_constituency prep_with_information_trees amod_information_f-structure nn_information_LFG dobj_associating_information advmod_associating_automatically dep_resources_Bresnan nn_resources_Grammar dep_Bresnan_2001 appos_Grammar_LFG nn_Grammar_LexicalFunctional amod_Grammar_multilingual amod_Grammar_wide-coverage prepc_by_means_of_induce_associating prep_from_induce_treebanks dobj_induce_resources aux_induce_to vmod_develop_induce dobj_develop_methods aux_develop_to vmod_effort_develop amod_effort_ongoing det_effort_an prep_of_part_effort dobj_forms_part nsubj_forms_research det_paper_this prep_in_presented_paper vmod_research_presented det_research_The dep_Introduction_Cahill rcmod_Introduction_forms num_Introduction_1
P07-1032	P04-1041	o	1 Introduction Parsers have been developed for a variety of grammar formalisms for example HPSG -LRB- Toutanova et al. 2002 Malouf and van Noord 2004 -RRB- LFG -LRB- Kaplan et al. 2004 Cahill et al. 2004 -RRB- TAG -LRB- Sarkar and Joshi 2003 -RRB- CCG -LRB- Hockenmaier and Steedman 2002 Clark and Curran 2004b -RRB- and variants of phrase-structure grammar -LRB- Briscoe et al. 2006 -RRB- including the phrase-structure grammar implicit in the Penn Treebank -LRB- Collins 2003 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak appos_Collins_2003 dep_Treebank_Collins nn_Treebank_Penn det_Treebank_the prep_in_implicit_Treebank amod_grammar_implicit amod_grammar_phrase-structure det_grammar_the amod_Briscoe_2006 dep_Briscoe_al. nn_Briscoe_et amod_grammar_phrase-structure prep_of_variants_grammar appos_Clark_2004b conj_and_Clark_Curran dep_Hockenmaier_Curran dep_Hockenmaier_Clark conj_and_Hockenmaier_2002 conj_and_Hockenmaier_Steedman appos_CCG_2002 appos_CCG_Steedman appos_CCG_Hockenmaier amod_Sarkar_2003 conj_and_Sarkar_Joshi dep_TAG_Joshi dep_TAG_Sarkar num_Cahill_2004 nn_Cahill_al. nn_Cahill_et dep_Kaplan_Cahill num_Kaplan_2004 dep_Kaplan_al. nn_Kaplan_et prep_including_LFG_grammar dep_LFG_Briscoe conj_and_LFG_variants appos_LFG_CCG appos_LFG_TAG appos_LFG_Kaplan ccomp_,_variants ccomp_,_LFG nn_Noord_van dep_Malouf_2004 conj_and_Malouf_Noord dep_Toutanova_Noord dep_Toutanova_Malouf appos_Toutanova_2002 dep_Toutanova_al. nn_Toutanova_et nn_HPSG_example nn_formalisms_grammar prep_of_variety_formalisms det_variety_a dep_developed_Toutanova prep_for_developed_HPSG prep_for_developed_variety auxpass_developed_been aux_developed_have nsubjpass_developed_Parsers nn_Parsers_Introduction num_Parsers_1 dep_``_developed
W04-2003	P04-1041	o	Our approach is to use finite-state approximations of long-distance dependencies as they are described in -LRB- Schneider 2003a -RRB- for Dependency Grammar -LRB- DG -RRB- and -LRB- Cahill et al. 2004 -RRB- for Lexical Functional Grammar -LRB- LFG -RRB-	appos_Grammar_LFG amod_Grammar_Functional amod_Grammar_Lexical prep_for_Cahill_Grammar amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et appos_Grammar_DG nn_Grammar_Dependency conj_and_Schneider_Cahill prep_for_Schneider_Grammar appos_Schneider_2003a prep_in_described_Cahill prep_in_described_Schneider auxpass_described_are nsubjpass_described_they mark_described_as amod_dependencies_long-distance prep_of_approximations_dependencies amod_approximations_finite-state advcl_use_described dobj_use_approximations aux_use_to xcomp_is_use nsubj_is_approach poss_approach_Our ccomp_``_is
W07-0411	P04-1041	o	The translation and reference files are analyzed by a treebank-based probabilistic Lexical-Functional Grammar -LRB- LFG -RRB- parser -LRB- Cahill et al. 2004 -RRB- which produces a set of dependency triples for each input	det_input_each nn_triples_dependency prep_for_set_input prep_of_set_triples det_set_a dobj_produces_set nsubj_produces_which amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et nn_parser_Grammar amod_parser_treebank-based det_parser_a appos_Grammar_LFG amod_Grammar_Lexical-Functional amod_Grammar_probabilistic dep_analyzed_produces dep_analyzed_Cahill agent_analyzed_parser auxpass_analyzed_are nsubjpass_analyzed_files nsubjpass_analyzed_translation nn_files_reference conj_and_translation_files det_translation_The
W07-0714	P04-1041	o	The translation and reference files are analyzed by a treebank-based probabilistic LFG parser -LRB- Cahill et al. 2004 -RRB- which produces a set of dependency triples for each input	det_input_each nn_triples_dependency prep_for_set_input prep_of_set_triples det_set_a dobj_produces_set nsubj_produces_which amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et nn_parser_LFG amod_parser_probabilistic amod_parser_treebank-based det_parser_a dep_analyzed_produces dep_analyzed_Cahill agent_analyzed_parser auxpass_analyzed_are nsubjpass_analyzed_files nsubjpass_analyzed_translation nn_files_reference conj_and_translation_files det_translation_The
W07-2206	P04-1041	o	1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars for example LFG -LRB- Kaplan et al. 2004 Cahill et al. 2004 -RRB- HPSG -LRB- Toutanova et al. 2002 Malouf and van Noord 2004 -RRB- TAG -LRB- Sarkar and Joshi 2003 -RRB- and CCG -LRB- Hockenmaier andSteedman ,2002 ClarkandCurran ,2004 b -RRB-	num_b_,2004 nn_b_ClarkandCurran dep_andSteedman_b num_andSteedman_,2002 nn_andSteedman_Hockenmaier dep_CCG_andSteedman amod_Sarkar_2003 conj_and_Sarkar_Joshi dep_TAG_Joshi dep_TAG_Sarkar nn_Noord_van dep_Malouf_2004 conj_and_Malouf_Noord dep_Toutanova_Noord dep_Toutanova_Malouf appos_Toutanova_2002 dep_Toutanova_al. nn_Toutanova_et conj_and_HPSG_CCG conj_and_HPSG_TAG appos_HPSG_Toutanova num_Cahill_2004 nn_Cahill_al. nn_Cahill_et dep_Kaplan_Cahill num_Kaplan_2004 dep_Kaplan_al. nn_Kaplan_et nn_LFG_example amod_grammars_motivated advmod_motivated_linguistically amod_methods_statistical appos_application_CCG appos_application_TAG appos_application_HPSG dep_application_Kaplan prep_for_application_LFG prep_to_application_grammars prep_of_application_methods det_application_the cop_application_been aux_application_has nsubj_application_theme amod_research_parsing prep_in_theme_research amod_theme_recent det_theme_A nn_theme_Introduction num_theme_1
W07-2211	P04-1041	o	Methods for doing so for stochastic parser output are described by Johnson -LRB- 2002 -RRB- and Cahill et al -LRB- 2004 -RRB-	dep_al_2004 dep_Cahill_al nn_Cahill_et conj_and_Johnson_Cahill appos_Johnson_2002 agent_described_Cahill agent_described_Johnson auxpass_described_are prep_for_described_output nsubjpass_described_Methods nn_output_parser amod_output_stochastic advmod_doing_so prepc_for_Methods_doing
W08-1122	P04-1041	o	The f-structures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver -LRB- Cahill et al. 2004 -RRB-	amod_Cahill_2004 dep_Cahill_al. nn_Cahill_et nn_solver_constraint det_solver_a det_equations_these prep_through_passing_solver dobj_passing_equations advmod_passing_then nsubjpass_passing_f-structures amod_equations_functional nn_equations_LFG nn_trees_WSJ amod_trees_standard amod_standard_gold det_standard_the prep_in_nodes_trees amod_nodes_annotating dep_created_Cahill conj_and_created_passing prep_with_created_equations agent_created_nodes advmod_created_automatically auxpass_created_are nsubjpass_created_f-structures det_f-structures_The
W08-1306	P04-1041	o	-LRB- Cahill et al. 2004 -RRB- managed to extract LFG subcategorisation frames and paths linking long distance dependencies reentrancies from f-structures generated automatically for the PennII treebank trees and used them in an long distance dependency resolution algorithm to parse new text	amod_text_new dobj_parse_text aux_parse_to vmod_algorithm_parse nn_algorithm_resolution nn_algorithm_dependency nn_algorithm_distance amod_algorithm_long det_algorithm_an prep_in_used_algorithm dobj_used_them nsubj_used_Cahill nn_trees_treebank nn_trees_PennII det_trees_the prep_for_generated_trees advmod_generated_automatically vmod_f-structures_generated nn_reentrancies_dependencies nn_reentrancies_distance amod_reentrancies_long prep_from_linking_f-structures dobj_linking_reentrancies vmod_paths_linking conj_and_frames_paths nn_frames_subcategorisation nn_frames_LFG dobj_extract_paths dobj_extract_frames aux_extract_to conj_and_managed_used xcomp_managed_extract nsubj_managed_Cahill dep_Cahill_2004 dep_Cahill_al. nn_Cahill_et
W09-2605	P04-1041	o	-LRB- 2004 -RRB- and Cahill et al.	nn_al._et nn_al._Cahill conj_and_2004_al.
P06-2031	P04-1048	o	6 Related Work The most relevant previous works include word sense translation and translation disambiguation -LRB- Li & Li 2003 Cao & Li 2002 Koehn and Knight 2000 Kikui 1999 Fung et al. 1999 -RRB- frame semantic induction -LRB- Green et al. 2004 Fung & Chen 2004 -RRB- and bilingual semantic mapping -LRB- Fung & Chen 2004 Huang et al. 2004 Ploux & Ji 2003 Ngai et al. 2002 Palmer & Wu 1995 -RRB-	nn_1995_Wu nn_1995_Palmer conj_and_Palmer_Wu nn_Ngai_al. nn_Ngai_et num_Ploux_2002 conj_and_Ploux_Ngai conj_and_Ploux_2003 conj_and_Ploux_Ji num_al._2004 nn_al._et nn_al._Huang dep_Fung_1995 conj_and_Fung_Ngai conj_and_Fung_2003 conj_and_Fung_Ji conj_and_Fung_Ploux dep_Fung_al. num_Fung_2004 conj_and_Fung_Chen appos_mapping_Ploux appos_mapping_Chen appos_mapping_Fung amod_mapping_semantic amod_mapping_bilingual nn_2004_Chen nn_2004_Fung conj_and_Fung_Chen dep_Green_2004 appos_Green_2004 dep_Green_al. nn_Green_et appos_induction_Green amod_induction_semantic nn_induction_frame num_Fung_1999 nn_Fung_al. nn_Fung_et num_Kikui_1999 num_Koehn_2000 conj_and_Koehn_Knight num_Cao_2002 conj_and_Cao_Li dep_Li_Fung dep_Li_Kikui dep_Li_Knight dep_Li_Koehn dep_Li_Li dep_Li_Cao num_Li_2003 conj_and_Li_Li nn_disambiguation_translation conj_and_translation_mapping conj_and_translation_induction dep_translation_Li dep_translation_Li conj_and_translation_disambiguation nn_translation_sense nn_translation_word dobj_include_mapping dobj_include_induction dobj_include_disambiguation dobj_include_translation nsubj_include_works amod_works_previous amod_works_relevant det_works_The advmod_relevant_most rcmod_Work_include amod_Work_Related num_Work_6
P06-2031	P04-1048	o	It would be necessary to apply either semiautomatic or automatic methods such as those in -LRB- Burchardt et al. 2005 Green et al 2004 -RRB- to extend FrameNet coverage for final application to machine translation tasks	nn_tasks_translation nn_tasks_machine prep_to_application_tasks amod_application_final nn_coverage_FrameNet prep_for_extend_application dobj_extend_coverage aux_extend_to nn_2004_al dep_Green_2004 nn_Green_et advmod_2005_al. nn_al._et dep_Burchardt_Green num_Burchardt_2005 vmod_those_extend prep_in_those_Burchardt prep_such_as_methods_those amod_methods_automatic amod_methods_semiautomatic conj_or_semiautomatic_automatic preconj_semiautomatic_either dobj_apply_methods aux_apply_to xcomp_necessary_apply cop_necessary_be aux_necessary_would nsubj_necessary_It
W05-1007	P04-1048	p	This paper demonstrates several of the characteristics and benefits of SemFrame -LRB- Green et al. 2004 Green and Dorr 2004 -RRB- a system that produces such a resource	det_resource_a predet_resource_such dobj_produces_resource nsubj_produces_that rcmod_system_produces det_system_a dep_Green_2004 conj_and_Green_Dorr dep_Green_Dorr dep_Green_Green amod_Green_2004 dep_Green_al. nn_Green_et appos_SemFrame_system appos_SemFrame_Green prep_of_characteristics_SemFrame conj_and_characteristics_benefits det_characteristics_the prep_of_several_benefits prep_of_several_characteristics dobj_demonstrates_several nsubj_demonstrates_paper det_paper_This ccomp_``_demonstrates
C08-1106	P05-1010	o	When the data has distinct sub-structures models that exploit hidden state variables are advantageous in learning -LRB- Matsuzaki et al. 2005 Petrov et al. 2007 -RRB-	dep_al._2007 nn_al._et nn_al._Petrov num_al._2005 nn_al._et dep_Matsuzaki_al. dep_Matsuzaki_al. dobj_learning_Matsuzaki prepc_in_advantageous_learning cop_advantageous_are nsubj_advantageous_models advcl_advantageous_has nn_variables_state amod_variables_hidden dobj_exploit_variables nsubj_exploit_that rcmod_models_exploit amod_sub-structures_distinct dobj_has_sub-structures nsubj_has_data advmod_has_When det_data_the
D07-1014	P05-1010	o	More recently EM has been used to learn hidden variables in parse trees these can be head-childannotations -LRB- ChiangandBikel 2002 -RRB- latent head features -LRB- Matsuzaki et al. 2005 Prescher 2005 Dreyer and Eisner 2006 -RRB- or hierarchicallysplit nonterminal states -LRB- Petrov et al. 2006 -RRB-	amod_Petrov_2006 dep_Petrov_al. nn_Petrov_et amod_states_nonterminal nn_states_hierarchicallysplit dep_Dreyer_Petrov conj_or_Dreyer_states appos_Dreyer_2006 conj_and_Dreyer_Eisner conj_Prescher_states conj_Prescher_Eisner conj_Prescher_Dreyer num_Prescher_2005 dep_Matsuzaki_Prescher appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_features_Matsuzaki nn_features_head amod_features_latent dep_ChiangandBikel_2002 appos_head-childannotations_features dep_head-childannotations_ChiangandBikel cop_head-childannotations_be aux_head-childannotations_can nsubj_head-childannotations_these nn_trees_parse amod_variables_hidden prep_in_learn_trees dobj_learn_variables aux_learn_to parataxis_used_head-childannotations xcomp_used_learn auxpass_used_been aux_used_has nsubjpass_used_EM advmod_used_recently advmod_recently_More
D07-1014	P05-1010	p	6 Discussion Noting that adding latent features to nonterminals in unlexicalized context-free parsing has been very successful -LRB- Chiang and Bikel 2002 Matsuzaki et al. 2005 Prescher 2005 Dreyer and Eisner 2006 Petrov et al. 2006 -RRB- we were surprised not to see a 3Czech experiments were not done since the number of features -LRB- more than 14 million -RRB- was too high to multiply out by clusters	prep_by_multiply_clusters prt_multiply_out aux_multiply_to xcomp_high_multiply advmod_high_too cop_high_was nsubj_high_number mark_high_since number_million_14 prep_than_more_million appos_features_more prep_of_number_features det_number_the advcl_done_high neg_done_not auxpass_done_were nsubjpass_done_experiments nn_experiments_3Czech det_experiments_a ccomp_see_done aux_see_to neg_see_not xcomp_surprised_see auxpass_surprised_were nsubjpass_surprised_we num_Petrov_2006 nn_Petrov_al. nn_Petrov_et num_Dreyer_2006 conj_and_Dreyer_Eisner num_Prescher_2005 num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Chiang_Petrov conj_and_Chiang_Eisner conj_and_Chiang_Dreyer conj_and_Chiang_Prescher conj_and_Chiang_Matsuzaki dep_Chiang_2002 conj_and_Chiang_Bikel dep_successful_Dreyer dep_successful_Prescher dep_successful_Matsuzaki dep_successful_Bikel dep_successful_Chiang advmod_successful_very cop_successful_been aux_successful_has csubj_successful_adding mark_successful_that amod_parsing_context-free amod_parsing_unlexicalized prep_in_nonterminals_parsing amod_features_latent prep_to_adding_nonterminals dobj_adding_features parataxis_Noting_surprised ccomp_Noting_successful nsubj_Noting_Discussion num_Discussion_6 ccomp_``_Noting
D07-1072	P05-1010	o	We compare an ordinary PCFG estimated with maximum likelihood -LRB- Matsuzaki et al. 2005 -RRB- and the HDP-PCFG estimated using the variational inference algorithm described in Section 2.6	num_Section_2.6 prep_in_described_Section vmod_algorithm_described nn_algorithm_inference amod_algorithm_variational det_algorithm_the dobj_using_algorithm xcomp_estimated_using vmod_HDP-PCFG_estimated det_HDP-PCFG_the amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et nn_likelihood_maximum prep_with_estimated_likelihood vmod_PCFG_estimated amod_PCFG_ordinary det_PCFG_an conj_and_compare_HDP-PCFG dep_compare_Matsuzaki dobj_compare_PCFG nsubj_compare_We
D07-1072	P05-1010	o	Unlexicalized methods refine the grammar in a more conservative fashion splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols -LRB- Klein and Manning 2003 Matsuzaki et al. 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Klein_Petrov dep_Klein_Matsuzaki num_Klein_2003 conj_and_Klein_Manning appos_subsymbols_Manning appos_subsymbols_Klein prep_of_number_subsymbols amod_number_smaller det_number_a advmod_smaller_much amod_symbol_pre-terminal amod_symbol_non-terminal det_symbol_each conj_or_non-terminal_pre-terminal prep_into_splitting_number dobj_splitting_symbol amod_fashion_conservative det_fashion_a advmod_conservative_more det_grammar_the dep_refine_splitting prep_in_refine_fashion dobj_refine_grammar nsubj_refine_methods amod_methods_Unlexicalized ccomp_``_refine
D08-1016	P05-1010	o	We could also introduce new variables e.g. nonterminal refinements -LRB- Matsuzaki et al. 2005 -RRB- or secondary linksMij -LRB- not constrained by TREE/PTREE -RRB- that augment the parse with representations of control binding etc.	conj_control_etc. conj_control_binding prep_of_representations_control det_parse_the prep_with_augment_representations dobj_augment_parse nsubj_augment_that prep_by_constrained_TREE/PTREE neg_constrained_not rcmod_linksMij_augment dep_linksMij_constrained amod_linksMij_secondary amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_refinements_Matsuzaki amod_refinements_nonterminal conj_or_e.g._linksMij conj_or_e.g._refinements amod_variables_new dep_introduce_linksMij dep_introduce_refinements dep_introduce_e.g. dobj_introduce_variables advmod_introduce_also aux_introduce_could nsubj_introduce_We ccomp_``_introduce
D08-1091	P05-1010	o	The parameters of the refined productions Ax By Cz where Ax is a subcategory of A By of B and Cz of C can then be estimated in various ways past work has included both generative -LRB- Matsuzaki et al. 2005 Liang et al. 2007 -RRB- and discriminative approaches -LRB- Petrov and Klein 2008 -RRB-	amod_Petrov_2008 conj_and_Petrov_Klein dep_approaches_Klein dep_approaches_Petrov amod_approaches_discriminative num_Liang_2007 nn_Liang_al. nn_Liang_et dep_Matsuzaki_Liang appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et conj_and_generative_approaches dep_generative_Matsuzaki preconj_generative_both dobj_included_approaches dobj_included_generative aux_included_has nsubj_included_work amod_work_past amod_ways_various parataxis_estimated_included prep_in_estimated_ways auxpass_estimated_be advmod_estimated_then aux_estimated_can nsubjpass_estimated_Cz nsubjpass_estimated_parameters prep_of_Cz_C pobj_of_B pcomp_By_of prep_of_subcategory_A det_subcategory_a cop_subcategory_is nsubj_subcategory_Ax advmod_subcategory_where rcmod_Cz_subcategory nn_Ax_productions amod_Ax_refined det_Ax_the conj_and_parameters_Cz prep_parameters_By prep_by_parameters_Cz prep_of_parameters_Ax det_parameters_The
D08-1091	P05-1010	o	The resulting memory limitations alone can prevent the practical learning of highly split grammars -LRB- Matsuzaki et al. 2005 -RRB-	amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et amod_grammars_split advmod_split_highly prep_of_learning_grammars amod_learning_practical det_learning_the dep_prevent_Matsuzaki dobj_prevent_learning aux_prevent_can nsubj_prevent_limitations advmod_limitations_alone nn_limitations_memory amod_limitations_resulting det_limitations_The ccomp_``_prevent
D08-1091	P05-1010	o	1 Introduction In latent variable approaches to parsing -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB- one models an observed treebank of coarse parse trees using a grammar over more refined but unobserved derivation trees	nn_trees_derivation amod_trees_unobserved advmod_refined_more advmod_refined_over dep_refined_using det_grammar_a dobj_using_grammar nn_trees_parse amod_trees_coarse conj_but_treebank_trees amod_treebank_refined prep_of_treebank_trees amod_treebank_observed det_treebank_an dep_models_trees dep_models_treebank num_models_one num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Matsuzaki_Petrov appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et prep_to_approaches_parsing amod_approaches_variable amod_approaches_latent appos_Introduction_models appos_Introduction_Matsuzaki prep_in_Introduction_approaches num_Introduction_1
D09-1087	P05-1010	p	2 Parsing Model The Berkeley parser -LRB- Petrov et al. 2006 Petrov and Klein 2007 -RRB- is an efficient and effective parser that introduces latent annotations -LRB- Matsuzaki et al. 2005 -RRB- to refine syntactic categories to learn better PCFG grammars	nn_grammars_PCFG amod_grammars_better dobj_learn_grammars aux_learn_to amod_categories_syntactic vmod_refine_learn dobj_refine_categories aux_refine_to amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_annotations_Matsuzaki amod_annotations_latent xcomp_introduces_refine dobj_introduces_annotations nsubj_introduces_that rcmod_parser_introduces amod_parser_effective amod_parser_efficient det_parser_an cop_parser_is nsubj_parser_parser conj_and_efficient_effective dep_Petrov_2007 conj_and_Petrov_Klein dep_Petrov_Klein dep_Petrov_Petrov appos_Petrov_2006 dep_Petrov_al. nn_Petrov_et appos_parser_Petrov nn_parser_Berkeley nn_parser_The nn_parser_Model nn_parser_Parsing num_parser_2
D09-1119	P05-1010	p	This was recently followed by -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB- who introduce state-of-the-art nearly unlexicalized PCFG parsers	nn_parsers_PCFG amod_parsers_unlexicalized amod_parsers_state-of-the-art advmod_unlexicalized_nearly dobj_introduce_parsers nsubj_introduce_who num_Petrov_2006 nn_Petrov_al. nn_Petrov_et rcmod_Matsuzaki_introduce dep_Matsuzaki_Petrov appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et agent_followed_Matsuzaki advmod_followed_recently auxpass_followed_was nsubjpass_followed_This
D09-1135	P05-1010	o	Then some manual and automatic symbol splitting methods are presented which get comparable performance with lexicalized parsers -LRB- Klein and Manning 2003 Matsuzaki et al. 2005 -RRB-	num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Klein_Matsuzaki num_Klein_2003 conj_and_Klein_Manning appos_parsers_Manning appos_parsers_Klein amod_parsers_lexicalized prep_with_performance_parsers amod_performance_comparable dobj_get_performance nsubj_get_which ccomp_presented_get auxpass_presented_are nsubjpass_presented_methods nsubjpass_presented_manual advmod_presented_Then nn_methods_splitting nn_methods_symbol amod_methods_automatic conj_and_manual_methods det_manual_some
D09-1161	P05-1010	p	The latent-annotation model -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB- is one of the most effective un-lexicalized models	amod_models_un-lexicalized amod_models_effective det_models_the advmod_effective_most prep_of_one_models cop_one_is nsubj_one_model dep_al._2006 nn_al._et nn_al._Petrov dep_Matsuzaki_al. dep_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et appos_model_Matsuzaki amod_model_latent-annotation det_model_The
D09-1161	P05-1010	o	In general they can be divided into two major categories namely lexicalized models -LRB- Collins 1997 1999 Charniak 1997 2000 -RRB- and un-lexicalized models -LRB- Klein and Manning 2003 Matsuzaki et al. 2005 Petrov et al. 2006 Petrov and Klein 2007 -RRB-	dep_Petrov_2007 conj_and_Petrov_Klein dep_al._2006 nn_al._et nn_al._Petrov dep_al._2005 nn_al._et nn_al._Matsuzaki num_Manning_2003 dep_Klein_Klein dep_Klein_Petrov conj_and_Klein_al. conj_and_Klein_al. conj_and_Klein_Manning appos_models_al. appos_models_al. appos_models_Manning appos_models_Klein amod_models_un-lexicalized num_Charniak_2000 num_Charniak_1997 dep_Collins_Charniak amod_Collins_1999 num_Collins_1997 conj_and_models_models appos_models_Collins amod_models_lexicalized advmod_models_namely appos_categories_models appos_categories_models amod_categories_major num_categories_two prep_into_divided_categories auxpass_divided_be aux_divided_can nsubjpass_divided_they prep_in_divided_general
E09-1088	P05-1010	p	1 Introduction When data have distinct sub-structures models exploiting latent variables are advantageous in learning -LRB- Matsuzaki et al. 2005 Petrov and Klein 2007 Blunsom et al. 2008 -RRB-	num_Blunsom_2008 nn_Blunsom_al. nn_Blunsom_et dep_Petrov_Blunsom conj_and_Petrov_2007 conj_and_Petrov_Klein dep_Matsuzaki_2007 dep_Matsuzaki_Klein dep_Matsuzaki_Petrov appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dobj_learning_Matsuzaki prepc_in_advantageous_learning cop_advantageous_are nsubj_advantageous_sub-structures aux_advantageous_have nsubj_advantageous_data advmod_advantageous_When amod_variables_latent dobj_exploiting_variables vmod_models_exploiting appos_sub-structures_models amod_sub-structures_distinct rcmod_Introduction_advantageous num_Introduction_1
N07-1051	P05-1010	o	The refined grammar is estimated using a variant of the forward-backward algorithm -LRB- Matsuzaki et al. 2005 -RRB-	amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et amod_algorithm_forward-backward det_algorithm_the prep_of_variant_algorithm det_variant_a dobj_using_variant dep_estimated_Matsuzaki xcomp_estimated_using auxpass_estimated_is nsubjpass_estimated_grammar amod_grammar_refined det_grammar_The ccomp_``_estimated
N07-1051	P05-1010	p	Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank either by manual annotation -LRB- Klein and Manning 2003 -RRB- or automatic state splitting -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Matsuzaki_Petrov appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et nn_splitting_state amod_splitting_automatic dep_Klein_2003 conj_and_Klein_Manning conj_or_annotation_splitting appos_annotation_Manning appos_annotation_Klein amod_annotation_manual dep_either_Matsuzaki prep_by_either_splitting prep_by_either_annotation det_treebank_a dep_learned_either prep_from_learned_treebank auxpass_learned_be aux_learned_can nsubjpass_learned_PCFGs mark_learned_that amod_PCFGs_unlexicalized amod_PCFGs_high-quality ccomp_shown_learned aux_shown_has nsubj_shown_work amod_work_Previous
N09-2054	P05-1010	o	Rather than explicit annotation we could use latent annotations to split the POS tags similarly to the introduction of latent annotations to PCFG grammars -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Matsuzaki_Petrov appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et nn_grammars_PCFG amod_annotations_latent prep_to_introduction_grammars prep_of_introduction_annotations det_introduction_the nn_tags_POS det_tags_the dobj_split_tags aux_split_to amod_annotations_latent dep_use_Matsuzaki prep_to_use_introduction advmod_use_similarly vmod_use_split dobj_use_annotations aux_use_could nsubj_use_we advmod_use_Rather amod_annotation_explicit prep_than_Rather_annotation
N09-2054	P05-1010	o	Building upon the large body of research to improve tagging performance for various languages using various models -LRB- e.g. -LRB- Thede and Harper 1999 Brants 2000 Tseng et al. 2005b Huang et al. 2007 -RRB- -RRB- and the recent work on PCFG grammars with latent annotations -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB- we will investigate the use of fine-grained latent annotations for Chinese POS tagging	nn_tagging_POS amod_tagging_Chinese amod_annotations_latent amod_annotations_fine-grained prep_for_use_tagging prep_of_use_annotations det_use_the dobj_investigate_use aux_investigate_will nsubj_investigate_we num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Matsuzaki_Petrov num_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et amod_annotations_latent nn_grammars_PCFG prep_with_work_annotations prep_on_work_grammars amod_work_recent det_work_the num_Huang_2007 nn_Huang_al. nn_Huang_et appos_Tseng_2005b dep_Tseng_al. nn_Tseng_et num_Brants_2000 dep_Thede_Huang conj_and_Thede_Tseng conj_and_Thede_Brants conj_and_Thede_1999 conj_and_Thede_Harper dep_e.g._investigate dep_e.g._Matsuzaki conj_and_e.g._work appos_e.g._Tseng appos_e.g._Brants appos_e.g._1999 appos_e.g._Harper appos_e.g._Thede dep_models_work dep_models_e.g. amod_models_various dobj_using_models amod_languages_various prep_for_performance_languages amod_performance_tagging dobj_improve_performance aux_improve_to prep_of_body_research amod_body_large det_body_the vmod_Building_using vmod_Building_improve prep_upon_Building_body
P06-1055	P05-1010	n	As one can see in Table 4 the resulting parser ranks among the best lexicalized parsers beating those of Collins -LRB- 1999 -RRB- and Charniak and Johnson -LRB- 2005 -RRB- .8 Its F1 performance is a 27 % reduction in error over Matsuzaki et al.	nn_al._et nn_al._Matsuzaki prep_over_reduction_al. prep_in_reduction_error amod_reduction_% det_reduction_a cop_reduction_is nsubj_reduction_ranks advcl_reduction_see number_%_27 nn_performance_F1 poss_performance_Its num_performance_.8 dep_performance_2005 nn_performance_Johnson conj_and_Collins_performance conj_and_Collins_Charniak appos_Collins_1999 prep_of_those_performance prep_of_those_Charniak prep_of_those_Collins dobj_beating_those amod_parsers_lexicalized amod_parsers_best det_parsers_the vmod_ranks_beating prep_among_ranks_parsers nn_ranks_parser amod_ranks_resulting det_ranks_the num_Table_4 prep_in_see_Table aux_see_can nsubj_see_one mark_see_As
P06-1055	P05-1010	o	-LRB- 2005 -RRB- 86.6 86.7 1.19 61.1 Collins -LRB- 1999 -RRB- 88.7 88.5 0.92 66.7 Charniak and Johnson -LRB- 2005 -RRB- 90.1 90.1 0.74 70.1 This Paper 90.3 90.0 0.78 68.5 all sentences LP LR CB 0CB Klein and Manning -LRB- 2003 -RRB- 86.3 85.1 1.31 57.2 Matsuzaki et al.	nn_al._et nn_al._Matsuzaki dep_al._57.2 dep_al._1.31 dep_al._2003 dep_al._Manning dep_1.31_85.1 number_85.1_86.3 conj_and_Klein_al. nn_Klein_0CB nn_Klein_CB nn_Klein_LR nn_Klein_LP dep_sentences_al. dep_sentences_Klein det_sentences_all dep_68.5_sentences number_68.5_0.78 dep_68.5_90.0 number_90.0_90.3 dep_Paper_68.5 det_Paper_This dep_70.1_Paper dep_70.1_0.74 dep_0.74_90.1 number_90.1_90.1 dep_2005_70.1 dep_Johnson_2005 conj_and_Charniak_Johnson dep_Charniak_66.7 dep_Charniak_Collins dep_Charniak_1.19 dep_Charniak_86.7 dep_Charniak_2005 number_66.7_0.92 dep_66.7_88.5 number_88.5_88.7 appos_Collins_1999 num_Collins_61.1 number_86.7_86.6
P07-1022	P05-1010	o	For example incremental CFG parsing algorithms can be used with the CFGs produced by this transform as can the Inside-Outside estimation algorithm -LRB- Lari and Young 1990 -RRB- and more exotic methods such as estimating adjoined hidden states -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Matsuzaki_Petrov appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_states_Matsuzaki amod_states_hidden amod_states_adjoined dobj_estimating_states prepc_such_as_methods_estimating amod_methods_exotic advmod_exotic_more conj_and_Lari_1990 conj_and_Lari_Young conj_and_algorithm_methods dep_algorithm_1990 dep_algorithm_Young dep_algorithm_Lari nn_algorithm_estimation nn_algorithm_Inside-Outside det_algorithm_the dobj_can_methods dobj_can_algorithm prepc_as_transform_can nsubj_transform_this mark_transform_by advcl_produced_transform vmod_CFGs_produced det_CFGs_the prep_with_used_CFGs auxpass_used_be aux_used_can nsubjpass_used_algorithms prep_for_used_example nn_algorithms_parsing nn_algorithms_CFG amod_algorithms_incremental
P07-1080	P05-1010	o	5 Related Work There has not been much previous work on graphical models for full parsing although recently several latent variable models for parsing have been proposed -LRB- Koo and Collins 2005 Matsuzaki et al. 2005 Riezler et al. 2002 -RRB-	num_Riezler_2002 nn_Riezler_al. nn_Riezler_et num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Koo_Riezler conj_and_Koo_Matsuzaki conj_and_Koo_2005 conj_and_Koo_Collins dep_proposed_Matsuzaki dep_proposed_2005 dep_proposed_Collins dep_proposed_Koo auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models mark_proposed_although prepc_for_models_parsing amod_models_variable amod_models_latent amod_models_several advmod_models_recently amod_parsing_full amod_models_graphical prep_for_work_parsing prep_on_work_models amod_work_previous amod_work_much cop_work_been neg_work_not aux_work_has expl_work_There advcl_Work_proposed rcmod_Work_work amod_Work_Related num_Work_5 dep_``_Work
P07-1080	P05-1010	o	-LRB- Koo and Collins 2005 Matsuzaki et al. 2005 Riezler et al. 2002 -RRB- -RRB-	num_Riezler_2002 nn_Riezler_al. nn_Riezler_et dep_Matsuzaki_Riezler num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Koo_Matsuzaki appos_Koo_2005 conj_and_Koo_Collins dep_''_Collins dep_''_Koo
P07-2052	P05-1010	o	splitting tags -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Matsuzaki_Petrov appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_tags_Matsuzaki nn_tags_splitting
P07-2052	P05-1010	o	Unlexicalized parsers on the other hand achieved accuracies almost equivalent to those of lexicalized parsers -LRB- Klein and Manning 2003 Matsuzaki et al. 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Klein_Petrov dep_Klein_Matsuzaki num_Klein_2003 conj_and_Klein_Manning appos_parsers_Manning appos_parsers_Klein amod_parsers_lexicalized prep_of_those_parsers prep_to_equivalent_those advmod_equivalent_almost acomp_achieved_equivalent dobj_achieved_accuracies amod_hand_other det_hand_the vmod_parsers_achieved prep_on_parsers_hand amod_parsers_Unlexicalized
P08-1006	P05-1010	o	5http / / nlp.cs.berkeley.edu/Main.html#Parsing 47 Figure 3 Predicate argument structure timized automatically by assigning latent variables to each nonterminal node and estimating the parameters of the latent variables by the EM algorithm -LRB- Matsuzaki et al. 2005 -RRB-	nn_al._et nn_al._Matsuzaki nn_algorithm_EM det_algorithm_the amod_variables_latent det_variables_the prep_of_parameters_variables det_parameters_the prep_by_estimating_algorithm dobj_estimating_parameters amod_node_nonterminal det_node_each amod_variables_latent conj_and_assigning_estimating prep_to_assigning_node dobj_assigning_variables agent_timized_estimating agent_timized_assigning advmod_timized_automatically vmod_structure_timized nn_structure_argument amod_structure_Predicate amod_Figure_2005 dep_Figure_al. dep_Figure_structure num_Figure_3 num_Figure_47 nn_Figure_nlp.cs.berkeley.edu/Main.html#Parsing dep_5http_Figure
P08-1068	P05-1010	o	Previous research in this area includes several models which incorporate hidden variables -LRB- Matsuzaki et al. 2005 Koo and Collins 2005 Petrov et al. 2006 Titov and Henderson 2007 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et appos_Koo_2007 conj_and_Koo_Henderson conj_and_Koo_Titov conj_and_Koo_Petrov num_Koo_2005 conj_and_Koo_Collins dep_Matsuzaki_Henderson dep_Matsuzaki_Titov dep_Matsuzaki_Petrov dep_Matsuzaki_Collins dep_Matsuzaki_Koo appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et amod_variables_hidden dobj_incorporate_variables nsubj_incorporate_which rcmod_models_incorporate amod_models_several dep_includes_Matsuzaki dobj_includes_models nsubj_includes_research det_area_this prep_in_research_area amod_research_Previous
P08-2054	P05-1010	o	CFGs extracted from such structures were then annotated with hidden variables encoding the constraints described in the previous section and trained until convergence by means of the Inside-Outside algorithm defined in -LRB- Pereira and Schabes 1992 -RRB- and applied in -LRB- Matsuzaki et al. 2005 -RRB-	amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_in_Matsuzaki prep_applied_in conj_and_Pereira_applied dep_Pereira_1992 conj_and_Pereira_Schabes prep_in_defined_applied prep_in_defined_Schabes prep_in_defined_Pereira vmod_algorithm_defined nn_algorithm_Inside-Outside det_algorithm_the prep_by_means_of_convergence_algorithm prep_until_trained_convergence nsubj_trained_CFGs amod_section_previous det_section_the prep_in_described_section vmod_constraints_described det_constraints_the dobj_encoding_constraints vmod_variables_encoding amod_variables_hidden conj_and_annotated_trained prep_with_annotated_variables advmod_annotated_then cop_annotated_were nsubj_annotated_CFGs amod_structures_such prep_from_extracted_structures vmod_CFGs_extracted
P08-2054	P05-1010	o	Such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finer-grained than those that can be read off the parsed sentences in treebanks -LRB- Henderson 2003 Matsuzaki et al. 2005 Prescher 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Prescher_Petrov num_Prescher_2005 dep_Matsuzaki_Prescher num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Henderson_Matsuzaki amod_Henderson_2003 dep_treebanks_Henderson prep_in_sentences_treebanks amod_sentences_parsed det_sentences_the dobj_read_sentences prt_read_off auxpass_read_be aux_read_can nsubjpass_read_that rcmod_those_read prep_than_finer-grained_those cop_finer-grained_are nsubj_finer-grained_that rcmod_representations_finer-grained amod_representations_grammatical amod_representations_hidden dobj_induce_representations aux_induce_to xcomp_proposed_induce auxpass_proposed_been advmod_proposed_recently aux_proposed_have nsubjpass_proposed_that rcmod_techniques_proposed amod_techniques_supervised advmod_supervised_partially amod_contrast_sharp prep_to_stand_techniques prep_in_stand_contrast nsubj_stand_methods amod_methods_Such
P09-1067	P05-1010	o	Indeed our methods were inspired by past work on variational decoding for DOP -LRB- Goodman 1996 -RRB- and for latent-variable parsing -LRB- Matsuzaki et al. 2005 -RRB-	amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et amod_parsing_latent-variable amod_Goodman_1996 conj_and_DOP_parsing dep_DOP_Goodman amod_decoding_variational prep_on_work_decoding amod_work_past dep_inspired_Matsuzaki prep_for_inspired_parsing prep_for_inspired_DOP agent_inspired_work auxpass_inspired_were nsubjpass_inspired_methods advmod_inspired_Indeed poss_methods_our
W06-1636	P05-1010	o	Instead researchers condition parsing decisions on many other features such as parent phrase-marker and famously the lexical-head of the phrase -LRB- Magerman 1995 Collins 1996 Collins 1997 Johnson 1998 Charniak 2000 Henderson 2003 Klein and Manning 2003 Matsuzaki et al. 2005 -RRB- -LRB- and others -RRB-	num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et num_Henderson_2003 conj_and_Charniak_others conj_and_Charniak_Matsuzaki conj_and_Charniak_2003 conj_and_Charniak_Manning conj_and_Charniak_Klein conj_and_Charniak_Henderson conj_and_Charniak_2000 num_Johnson_1998 num_Collins_1997 num_Collins_1996 dep_Magerman_others dep_Magerman_Matsuzaki dep_Magerman_2003 dep_Magerman_Manning dep_Magerman_Klein dep_Magerman_Henderson dep_Magerman_2000 dep_Magerman_Charniak conj_Magerman_Johnson conj_Magerman_Collins conj_Magerman_Collins conj_Magerman_1995 appos_phrase_Magerman det_phrase_the prep_of_lexical-head_phrase det_lexical-head_the nn_phrase-marker_parent appos_features_lexical-head advmod_features_famously cc_features_and prep_such_as_features_phrase-marker amod_features_other amod_features_many prep_on_decisions_features nn_decisions_parsing nn_decisions_condition nn_decisions_researchers advmod_decisions_Instead
W06-1636	P05-1010	o	In retrospect however there are perhaps even greater similarities to that of -LRB- Magerman 1995 Henderson 2003 Matsuzaki et al. 2005 -RRB-	num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et num_Henderson_2003 dep_Magerman_Matsuzaki dep_Magerman_Henderson dep_Magerman_1995 prep_of_that_Magerman prep_to_similarities_that amod_similarities_greater advmod_greater_even advmod_greater_perhaps nsubj_are_similarities expl_are_there advmod_are_however prep_in_are_retrospect
W06-2902	P05-1010	o	-LRB- Matsuzaki et al. 2005 Koo and Collins 2005 -RRB- -RRB-	dep_Koo_2005 conj_and_Koo_Collins dep_Matsuzaki_Collins dep_Matsuzaki_Koo appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_''_Matsuzaki
W06-2903	P05-1010	p	Compared to a basic treebank grammar -LRB- Charniak 1996 -RRB- the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical -LRB- Charniak 2000 Collins 1999 -RRB- or nonlexical -LRB- Klein and Manning 2003 Matsuzaki et al. 2005 -RRB- conditioning information	nn_information_conditioning dep_information_Matsuzaki num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et conj_and_Klein_information conj_and_Klein_2003 conj_and_Klein_Manning dep_Collins_1999 dep_Charniak_Collins dep_Charniak_2000 dep_lexical_information dep_lexical_2003 dep_lexical_Manning dep_lexical_Klein conj_or_lexical_nonlexical dep_lexical_Charniak preconj_lexical_either prep_with_rules_nonlexical prep_with_rules_lexical conj_and_symbols_rules nn_symbols_grammar nn_symbols_splitting nn_assumptions_independence prep_by_weaken_rules prep_by_weaken_symbols dobj_weaken_assumptions nsubj_weaken_grammars vmod_weaken_Compared nn_parsers_highaccuracy prep_of_grammars_parsers det_grammars_the dep_Charniak_1996 appos_grammar_Charniak amod_grammar_treebank amod_grammar_basic det_grammar_a prep_to_Compared_grammar
W07-2218	P05-1010	o	Recently several latent variable models for constituent parsing have been proposed -LRB- Koo and Collins 2005 Matsuzaki et al. 2005 Prescher 2005 Riezler et al. 2002 -RRB-	num_Riezler_2002 nn_Riezler_al. nn_Riezler_et num_Prescher_2005 num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Koo_Riezler conj_and_Koo_Prescher conj_and_Koo_Matsuzaki conj_and_Koo_2005 conj_and_Koo_Collins dep_proposed_Prescher dep_proposed_Matsuzaki dep_proposed_2005 dep_proposed_Collins dep_proposed_Koo auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models nn_parsing_constituent prep_for_models_parsing amod_models_variable amod_models_latent amod_models_several advmod_models_Recently
W07-2218	P05-1010	o	In -LRB- Matsuzaki et al. 2005 -RRB- non-terminals in a standard PCFG model are augmented with latent variables	amod_variables_latent prep_with_augmented_variables cop_augmented_are prep_in_augmented_non-terminals nn_model_PCFG amod_model_standard det_model_a prep_in_non-terminals_model dep_non-terminals_Matsuzaki nn_al._et amod_Matsuzaki_2005 dep_Matsuzaki_al.
W07-2218	P05-1010	n	While the model of -LRB- Matsuzaki et al. 2005 -RRB- significantly outperforms the constrained model of -LRB- Prescher 2005 -RRB- they both are well below the state-of-the-art in constituent parsing	amod_parsing_constituent prep_in_state-of-the-art_parsing amod_the_state-of-the-art prep_below_are_the advmod_are_well nsubj_are_they advcl_are_outperforms det_they_both amod_Prescher_2005 dep_of_Prescher prep_model_of amod_model_constrained det_model_the dobj_outperforms_model advmod_outperforms_significantly nsubj_outperforms_model mark_outperforms_While amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et prep_of_model_Matsuzaki det_model_the
W07-2219	P05-1010	o	3.1 A Note on State-Splits Recent studies -LRB- Klein and Manning 2003 Matsuzaki et al. 2005 Prescher 2005 Petrov et al. 2006 -RRB- suggest that category-splits help in enhancing the performance of treebank grammars and a previous study on MH -LRB- Tsarfaty 2006 -RRB- outlines specific POS-tags splits that improve MH parsing accuracy	nn_accuracy_parsing nn_accuracy_MH dobj_improve_accuracy nsubj_improve_that ccomp_splits_improve amod_POS-tags_specific dep_outlines_splits dobj_outlines_POS-tags nsubj_outlines_study amod_Tsarfaty_2006 dep_MH_Tsarfaty prep_on_study_MH amod_study_previous det_study_a amod_grammars_treebank prep_of_performance_grammars det_performance_the dobj_enhancing_performance conj_and_help_outlines prepc_in_help_enhancing nsubj_help_category-splits mark_help_that ccomp_suggest_outlines ccomp_suggest_help nsubj_suggest_Note num_Petrov_2006 nn_Petrov_al. nn_Petrov_et num_Prescher_2005 num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Klein_Petrov conj_and_Klein_Prescher conj_and_Klein_Matsuzaki conj_and_Klein_2003 conj_and_Klein_Manning appos_studies_Prescher appos_studies_Matsuzaki appos_studies_2003 appos_studies_Manning appos_studies_Klein amod_studies_Recent nn_studies_State-Splits prep_on_Note_studies det_Note_A num_Note_3.1
W08-1005	P05-1010	o	2 Latent Variable Parsing In latent variable parsing -LRB- Matsuzaki et al. 2005 Prescher 2005 Petrov et al. 2006 -RRB- we learn rule probabilities on latent annotations that when marginalized out maximize the likelihood of the unannotated training trees	nn_trees_training amod_trees_unannotated det_trees_the prep_of_likelihood_trees det_likelihood_the dobj_maximize_likelihood advcl_maximize_marginalized mark_maximize_that prt_marginalized_out advmod_marginalized_when ccomp_annotations_maximize amod_annotations_latent nn_probabilities_rule prep_on_learn_annotations dobj_learn_probabilities nsubj_learn_we dep_learn_Prescher num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Prescher_Petrov num_Prescher_2005 parataxis_Matsuzaki_learn appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et amod_parsing_variable amod_parsing_latent dep_Parsing_Matsuzaki prep_in_Parsing_parsing amod_Parsing_Variable amod_Parsing_Latent num_Parsing_2 dep_``_Parsing
W09-1008	P05-1010	o	This leads to 49 methods that use semi-supervised techniques on a treebank-infered grammar backbone such as -LRB- Matsuzaki et al. 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et dep_Matsuzaki_Petrov amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_as_Matsuzaki mwe_as_such prep_backbone_as nn_backbone_grammar amod_backbone_treebank-infered det_backbone_a amod_techniques_semi-supervised prep_on_use_backbone dobj_use_techniques nsubj_use_that rcmod_methods_use num_methods_49 prep_to_leads_methods nsubj_leads_This ccomp_``_leads
W09-1008	P05-1010	o	Solving this first methodological issue has led to solutions dubbed hereafter as unlexicalized statistical parsing -LRB- Johnson 1998 Klein and Manning 2003a Matsuzaki et al. 2005 Petrov et al. 2006 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et conj_and_Klein_2003a conj_and_Klein_Manning dep_Johnson_Petrov dep_Johnson_Matsuzaki dep_Johnson_2003a dep_Johnson_Manning dep_Johnson_Klein amod_Johnson_1998 appos_parsing_Johnson amod_parsing_statistical amod_parsing_unlexicalized prep_as_dubbed_parsing advmod_dubbed_hereafter vmod_solutions_dubbed prep_to_led_solutions aux_led_has vmod_led_Solving amod_issue_methodological amod_issue_first det_issue_this dobj_Solving_issue
W09-1008	P05-1010	o	A further development has been first introduced by -LRB- Matsuzaki et al. 2005 -RRB- who recasts the problem of adding latent annotations as an unsupervised learning problem given an observed PCFG induced from the treebank the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols	amod_symbols_latent prep_of_H_symbols nn_H_set amod_H_predefined det_H_a amod_grammar_observed det_grammar_the prep_of_terminal_grammar amod_terminal_non det_terminal_every prep_to_combining_H dobj_combining_terminal agent_generated_combining auxpass_generated_is nsubjpass_generated_grammar amod_grammar_latent det_grammar_the det_treebank_the prep_from_induced_treebank rcmod_PCFG_generated vmod_PCFG_induced amod_PCFG_observed det_PCFG_an pobj_given_PCFG nn_problem_learning amod_problem_unsupervised det_problem_an amod_annotations_latent prep_as_adding_problem dobj_adding_annotations prepc_of_problem_adding det_problem_the dobj_recasts_problem nsubj_recasts_who rcmod_Matsuzaki_recasts amod_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et prep_introduced_given agent_introduced_Matsuzaki advmod_introduced_first auxpass_introduced_been aux_introduced_has nsubjpass_introduced_development amod_development_further det_development_A
C08-1061	P05-1045	o	In this paper Stanford Named Entity Recognizer -LRB- Finkel et al. 2005 -RRB- is used to classify noun phrases into four semantic categories PERSON LOCATION ORGANIZARION and MISC	conj_and_PERSON_MISC conj_and_PERSON_ORGANIZARION conj_and_PERSON_LOCATION dep_categories_MISC dep_categories_ORGANIZARION dep_categories_LOCATION dep_categories_PERSON amod_categories_semantic num_categories_four nn_phrases_noun prep_into_classify_categories dobj_classify_phrases aux_classify_to xcomp_used_classify auxpass_used_is nsubjpass_used_Recognizer dep_2005_al. nn_al._et num_Finkel_2005 appos_Recognizer_Finkel nn_Recognizer_Entity ccomp_Named_used nsubj_Named_Stanford prep_in_Named_paper det_paper_this
D07-1033	P05-1045	o	For example non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition -LRB- Sutton and McCallum 2004 Bunescu and Mooney 2004 Finkel et al. 2005 Krishnan and Manning 2006 -RRB-	appos_Krishnan_2006 conj_and_Krishnan_Manning num_Finkel_2005 nn_Finkel_al. nn_Finkel_et num_Bunescu_2004 conj_and_Bunescu_Mooney dep_Sutton_Manning dep_Sutton_Krishnan conj_and_Sutton_Finkel conj_and_Sutton_Mooney conj_and_Sutton_Bunescu conj_and_Sutton_2004 conj_and_Sutton_McCallum dep_recognition_Finkel dep_recognition_Bunescu dep_recognition_2004 dep_recognition_McCallum dep_recognition_Sutton nn_recognition_entity dobj_named_recognition prepc_in_useful_named cop_useful_be aux_useful_to xcomp_shown_useful auxpass_shown_were nn_classes_entity amod_classes_different dep_have_shown dobj_have_classes neg_have_not aux_have_do nsubj_have_features prep_for_have_example det_document_a prep_in_phrases_document amod_phrases_same prep_such_as_features_phrases amod_features_non-local
D07-1033	P05-1045	n	Although several methods have already been proposed to incorporate non-local features -LRB- Sutton and McCallum 2004 Bunescu and Mooney 2004 Finkel et al. 2005 Roth and Yih 2005 Krishnan and Manning 2006 Nakagawa and Matsumoto 2006 -RRB- these present a problem that the types of non-local features are somewhat constrained	advmod_constrained_somewhat auxpass_constrained_are nsubjpass_constrained_types mark_constrained_that amod_features_non-local prep_of_types_features det_types_the ccomp_problem_constrained det_problem_a amod_problem_present det_problem_these dep_Nakagawa_2006 conj_and_Nakagawa_Matsumoto appos_Krishnan_problem conj_and_Krishnan_Matsumoto conj_and_Krishnan_Nakagawa conj_and_Krishnan_2006 conj_and_Krishnan_Manning num_Yih_2005 num_Finkel_2005 nn_Finkel_al. nn_Finkel_et num_Mooney_2004 dep_Sutton_Nakagawa dep_Sutton_2006 dep_Sutton_Manning dep_Sutton_Krishnan conj_and_Sutton_Yih conj_and_Sutton_Roth conj_and_Sutton_Finkel conj_and_Sutton_Mooney conj_and_Sutton_Bunescu conj_and_Sutton_2004 conj_and_Sutton_McCallum dep_features_Yih dep_features_Roth dep_features_Finkel dep_features_Mooney dep_features_Bunescu dep_features_2004 dep_features_McCallum dep_features_Sutton amod_features_non-local dobj_incorporate_features aux_incorporate_to xcomp_proposed_incorporate auxpass_proposed_been advmod_proposed_already aux_proposed_have nsubjpass_proposed_methods mark_proposed_Although amod_methods_several advcl_``_proposed
D07-1033	P05-1045	o	The performance of the related work -LRB- Finkel et al. 2005 Krishnan and Manning 2006 -RRB- is listed in Table4	prep_in_listed_Table4 auxpass_listed_is nsubjpass_listed_performance dep_Krishnan_2006 conj_and_Krishnan_Manning dep_Finkel_Manning dep_Finkel_Krishnan appos_Finkel_2005 dep_Finkel_al. nn_Finkel_et amod_work_related det_work_the appos_performance_Finkel prep_of_performance_work det_performance_The ccomp_``_listed
D07-1033	P05-1045	o	Method dev test Finkel et al. 2005 -LRB- Finkel et al. 2005 -RRB- baseline CRF 85.51 + non-local features 86.86 Krishnan and Manning 2006 -LRB- Krishnan and Manning 2006 -RRB- baseline CRF 85.29 + non-local features 87.24 Table 5 Summary of performance with POS/chunk tags by TagChunk	nn_tags_POS/chunk prep_with_performance_tags prep_by_Summary_TagChunk prep_of_Summary_performance num_Table_5 num_Table_87.24 dep_features_Table amod_features_non-local conj_+_CRF_features num_CRF_85.29 nn_CRF_baseline dep_Krishnan_2006 conj_and_Krishnan_Manning dep_2006_features dep_2006_CRF dep_2006_Manning dep_2006_Krishnan appos_Manning_2006 conj_and_Krishnan_Manning num_Krishnan_86.86 dep_features_Manning dep_features_Krishnan dep_85.51_features conj_+_85.51_non-local dep_CRF_non-local dep_CRF_85.51 nn_CRF_baseline dep_Finkel_Summary dep_Finkel_CRF num_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_al._Finkel num_al._2005 nn_al._et dep_Finkel_al. nn_Finkel_test nn_Finkel_dev nn_Finkel_Method
D07-1033	P05-1045	o	However the achieved accuracy was not better than that of related work -LRB- Finkel et al. 2005 Krishnan and Manning 2006 -RRB- based on CRFs	dep_Krishnan_2006 conj_and_Krishnan_Manning dep_Finkel_Manning dep_Finkel_Krishnan appos_Finkel_2005 dep_Finkel_al. nn_Finkel_et amod_work_related dep_that_Finkel prep_of_that_work prep_based_on_better_CRFs prep_than_better_that neg_better_not cop_better_was nsubj_better_accuracy advmod_better_However amod_accuracy_achieved det_accuracy_the
D09-1016	P05-1045	o	The named-entity features are generated by the freely available Stanford NER tagger -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_tagger_Finkel dep_NER_tagger nn_NER_Stanford amod_NER_available det_NER_the advmod_available_freely agent_generated_NER auxpass_generated_are nsubjpass_generated_features nn_features_named-entity det_features_The
D09-1057	P05-1045	n	However due to the lack of a fine grained NER tool at hand we employ the Stanford NER package -LRB- Finkel et al. 2005 -RRB- which identifies only four types of named entities	dep_named_entities prepc_of_types_named num_types_four quantmod_four_only dobj_identifies_types nsubj_identifies_which amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et nn_package_NER nn_package_Stanford det_package_the dep_employ_identifies dep_employ_Finkel dobj_employ_package nsubj_employ_we prep_due_to_employ_lack advmod_employ_However prep_at_tool_hand nn_tool_NER amod_tool_grained amod_tool_fine det_tool_a prep_of_lack_tool det_lack_the
D09-1057	P05-1045	o	5.1 CoNLL named entities presence feature We use Stanford named entity recognizer -LRB- NER -RRB- -LRB- Finkel et al. 2005 -RRB- to identify CoNLL style NEs7 as possible answer strings in a candidate sentence for a given type of question	prep_of_type_question amod_type_given det_type_a prep_for_sentence_type nn_sentence_candidate det_sentence_a nn_strings_answer amod_strings_possible nn_NEs7_style nn_NEs7_CoNLL prep_in_identify_sentence prep_as_identify_strings dobj_identify_NEs7 aux_identify_to amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_recognizer_Finkel appos_recognizer_NER nn_recognizer_entity vmod_named_identify dobj_named_recognizer nsubj_named_Stanford ccomp_use_named nsubj_use_We rcmod_feature_use nn_feature_presence nn_feature_entities amod_feature_named nn_feature_CoNLL num_feature_5.1
D09-1101	P05-1045	o	Semantic -LRB- 1 -RRB- The named entity -LRB- NE -RRB- tag of wi obtained using the Stanford CRF-based NE recognizer -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et nn_recognizer_NE amod_recognizer_CRF-based nn_recognizer_Stanford det_recognizer_the dobj_using_recognizer xcomp_obtained_using vmod_wi_obtained dep_tag_Finkel prep_of_tag_wi nn_tag_NE nn_tag_entity amod_tag_named det_tag_The dep_Semantic_tag dep_Semantic_1 dep_``_Semantic
D09-1119	P05-1045	p	As discussed above all state-of-the-art published methods rely on lexical features for such tasks -LRB- Zhang et al. 2001 Sha and Pereira 2003 Finkel et al. 2005 Ratinov and Roth 2009 -RRB-	amod_Ratinov_2009 conj_and_Ratinov_Roth num_Finkel_2005 nn_Finkel_al. nn_Finkel_et num_Sha_2003 conj_and_Sha_Pereira dep_Zhang_Roth dep_Zhang_Ratinov dep_Zhang_Finkel dep_Zhang_Pereira dep_Zhang_Sha amod_Zhang_2001 dep_Zhang_al. nn_Zhang_et amod_tasks_such prep_for_features_tasks amod_features_lexical dep_rely_Zhang prep_on_rely_features nsubj_rely_methods advcl_rely_discussed amod_methods_published amod_methods_state-of-the-art det_methods_all advmod_discussed_above mark_discussed_As
D09-1120	P05-1045	o	Instead we opt to utilize the Stanford NER tagger -LRB- Finkel et al. 2005 -RRB- over the sentences in a document and annotate each NP with the NER label assigned to that mention head	nn_head_mention det_head_that prep_to_assigned_head vmod_label_assigned nn_label_NER det_label_the det_NP_each prep_with_annotate_label dobj_annotate_NP nsubj_annotate_we det_document_a det_sentences_the amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_NER_tagger nn_NER_Stanford det_NER_the dobj_utilize_NER aux_utilize_to conj_and_opt_annotate prep_in_opt_document prep_over_opt_sentences dep_opt_Finkel xcomp_opt_utilize nsubj_opt_we advmod_opt_Instead
D09-1158	P05-1045	o	4.1 NER features We used the features generated by the CRF package -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et nn_package_CRF det_package_the agent_generated_package vmod_features_generated det_features_the dobj_used_features nsubj_used_We dep_features_Finkel rcmod_features_used nn_features_NER num_features_4.1
E09-1007	P05-1045	o	F-me 1 CBC-NER system M 71.67 23.47 35.36CBC-NER system A 70.66 32.86 44.86 2 XIP NER 77.77 56.55 65.48 XIP + CBC M 78.41 60.26 68.15 XIP + CBC A 76.31 60.48 67.48 3 Stanford NER 67.94 68.01 67.97 Stanford + CBC M 69.40 71.07 70.23 Stanford + CBC A 70.09 72.93 71.48 4 GATE NER 63.30 56.88 59.92 GATE + CBC M 66.43 61.79 64.03 GATE + CBC A 66.51 63.10 64.76 5 Stanford + XIP 72.85 75.87 74.33 Stanford + XIP + CBC M 72.94 77.70 75.24 Stanford + XIP + CBC A 73.55 78.93 76.15 6 GATE + XIP 69.38 66.04 67.67 GATE + XIP + CBC M 69.62 67.79 68.69 GATE + XIP + CBC A 69.87 69.10 69.48 7 GATE + Stanford 63.12 69.32 66.07 GATE + Stanford + CBC M 65.09 72.05 68.39 GATE + Stanford + CBC A 65.66 73.25 69.25 Table 1 Results given by different hybrid NER systems and coupled with the CBC-NER system corpora -LRB- CoNLL MUC6 MUC7 and ACE -RRB- ner-eng-ie crf-3-all2008-distsim ser.gz -LRB- Finkel et al. 2005 -RRB- -LRB- line 3 in Table 1 -RRB- GATE NER or in short GATE -LRB- Cunningham et al. 2002 -RRB- -LRB- line 4 in Table 1 -RRB- and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned NER systems -LRB- lines 5 to 7 in Table 1 -RRB-	dep_Table_1 dep_7_to number_7_5 prep_in_lines_Table num_lines_7 dep_systems_lines nn_systems_NER amod_systems_last-mentioned num_systems_three det_systems_the prep_of_set_systems det_set_the prep_among_taken_set vmod_pairs_taken prep_of_combination_pairs det_combination_the agent_given_combination auxpass_given_are nsubjpass_given_which rcmod_systems_given nn_systems_hybrid amod_systems_several conj_and_1_systems dep_1_Table dep_1_in dep_1_line dep_1_line dep_1_ser.gz dep_1_crf-3-all2008-distsim num_line_4 amod_Cunningham_2002 dep_Cunningham_al. nn_Cunningham_et amod_GATE_short pobj_in_GATE nn_NER_GATE conj_or_Table_in conj_or_Table_NER dep_Table_1 dep_line_Cunningham prep_in_line_in prep_in_line_NER prep_in_line_Table num_line_3 amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_ser.gz_Finkel nn_crf-3-all2008-distsim_ner-eng-ie conj_and_CoNLL_ACE conj_and_CoNLL_MUC7 conj_and_CoNLL_MUC6 dep_corpora_ACE dep_corpora_MUC7 dep_corpora_MUC6 dep_corpora_CoNLL nn_corpora_system nn_corpora_CBC-NER det_corpora_the prep_with_coupled_corpora nn_systems_NER nn_systems_hybrid amod_systems_different agent_given_systems dep_Results_systems dep_Results_1 conj_and_Results_coupled vmod_Results_given num_Table_69.25 num_Table_73.25 num_Table_65.66 nn_Table_A nn_Table_CBC num_GATE_1 conj_+_GATE_Table conj_+_GATE_Stanford dep_GATE_68.39 dep_68.39_72.05 number_72.05_65.09 dep_M_Table dep_M_Stanford dep_M_GATE nn_M_CBC dep_GATE_66.07 nn_GATE_Stanford dep_66.07_69.32 number_69.32_63.12 conj_+_GATE_M conj_+_GATE_Stanford conj_+_GATE_GATE dep_7_M dep_7_Stanford dep_7_GATE dep_7_GATE number_7_69.48 dep_7_69.10 number_69.10_69.87 dep_A_7 dep_CBC_A conj_+_GATE_CBC conj_+_GATE_XIP dep_GATE_68.69 dep_68.69_67.79 number_67.79_69.62 dep_M_CBC dep_M_XIP dep_M_GATE nn_M_CBC conj_+_XIP_M conj_+_GATE_M conj_+_GATE_XIP num_GATE_67.67 num_GATE_66.04 number_66.04_69.38 dep_XIP_XIP dep_XIP_GATE num_GATE_6 dep_GATE_76.15 nn_GATE_A nn_GATE_CBC dep_76.15_78.93 number_78.93_73.55 conj_+_Stanford_XIP conj_+_Stanford_GATE conj_+_Stanford_XIP dep_75.24_XIP dep_75.24_GATE dep_75.24_XIP dep_75.24_Stanford num_75.24_77.70 number_77.70_72.94 dep_M_75.24 dep_CBC_M conj_+_Stanford_CBC conj_+_Stanford_XIP dep_74.33_CBC dep_74.33_XIP dep_74.33_Stanford num_74.33_75.87 number_75.87_72.85 dep_XIP_74.33 conj_+_Stanford_XIP dep_5_XIP dep_5_Stanford number_5_64.76 dep_5_63.10 number_63.10_66.51 dep_A_5 dep_GATE_A conj_+_GATE_CBC dep_64.03_CBC dep_64.03_GATE dep_64.03_61.79 number_61.79_66.43 dep_M_64.03 nn_M_CBC conj_+_GATE_M num_GATE_59.92 num_GATE_56.88 number_56.88_63.30 dep_NER_M dep_NER_GATE nn_NER_GATE num_NER_4 dep_71.48_NER dep_71.48_72.93 number_72.93_70.09 dep_A_coupled dep_A_Results num_A_71.48 dep_CBC_A conj_+_Stanford_CBC dep_70.23_CBC dep_70.23_Stanford num_70.23_71.07 number_71.07_69.40 dep_M_70.23 dep_CBC_M conj_+_Stanford_CBC dep_67.97_CBC dep_67.97_Stanford dep_67.97_68.01 number_68.01_67.94 dep_NER_67.97 nn_NER_Stanford num_NER_3 dep_67.48_NER dep_67.48_60.48 number_60.48_76.31 dep_A_67.48 dep_XIP_A conj_+_XIP_CBC dep_68.15_CBC dep_68.15_XIP dep_68.15_60.26 number_60.26_78.41 dep_M_68.15 nn_M_CBC conj_+_XIP_M num_XIP_65.48 num_XIP_56.55 number_56.55_77.77 dep_NER_M dep_NER_XIP nn_NER_XIP num_NER_2 num_NER_32.86 number_2_44.86 number_32.86_70.66 dep_A_NER dep_system_A nn_system_35.36CBC-NER num_system_23.47 num_system_71.67 nn_system_M nn_system_system nn_system_CBC-NER num_system_1 nn_system_F-me
E09-1011	P05-1045	o	We parse the data using the Collins Parser -LRB- Collins 1997 -RRB- and then tag person location and organization names using the Stanford Named Entity Recognizer -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_Recognizer_Finkel nn_Recognizer_Entity dobj_Named_Recognizer det_Stanford_the dobj_using_Stanford nn_names_organization conj_and_person_Named vmod_person_using conj_and_person_names conj_and_person_location nn_person_tag advmod_person_then dep_Collins_1997 appos_Parser_Collins nn_Parser_Collins det_Parser_the dobj_using_Parser vmod_data_using det_data_the conj_and_parse_Named conj_and_parse_names conj_and_parse_location conj_and_parse_person dobj_parse_data nsubj_parse_We ccomp_``_person ccomp_``_parse
E09-1037	P05-1045	o	Some stem from work on graphical models includingloopybeliefpropagation -LRB- Suttonand McCallum 2004 Smith and Eisner 2008 -RRB- Gibbs sampling -LRB- Finkel et al. 2005 -RRB- sequential Monte Carlo methods such as particle filtering -LRB- Levy et al. 2008 -RRB- and variational inference -LRB- Jordan et al. 1999 MacKay 1997 Kurihara and Sato 2006 -RRB-	amod_Kurihara_2006 conj_and_Kurihara_Sato num_MacKay_1997 dep_Jordan_Sato dep_Jordan_Kurihara dep_Jordan_MacKay num_Jordan_1999 dep_Jordan_al. nn_Jordan_et amod_inference_variational amod_Levy_2008 dep_Levy_al. nn_Levy_et vmod_particle_filtering dep_methods_Levy prep_such_as_methods_particle nn_methods_Carlo nn_methods_Monte amod_methods_sequential amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et nn_sampling_Gibbs dep_Smith_2008 conj_and_Smith_Eisner dep_McCallum_Eisner dep_McCallum_Smith dep_McCallum_2004 nn_McCallum_Suttonand appos_includingloopybeliefpropagation_McCallum dep_models_Finkel conj_models_sampling conj_models_includingloopybeliefpropagation amod_models_graphical appos_stem_Jordan conj_and_stem_inference conj_and_stem_methods prep_on_stem_models prep_from_stem_work det_stem_Some ccomp_``_inference ccomp_``_methods ccomp_``_stem
E09-1091	P05-1045	o	In all the experiments our source side language is English and the Stanford Named Entity Recognizer -LRB- Finkel et al 2005 -RRB- was used to extract NEs from the source side article	nn_article_side nn_article_source det_article_the prep_from_extract_article dobj_extract_NEs aux_extract_to xcomp_used_extract auxpass_used_was nsubjpass_used_Named nsubjpass_used_English amod_Finkel_2005 dep_Finkel_al nn_Finkel_et dep_Recognizer_Finkel nn_Recognizer_Entity dobj_Named_Recognizer nsubj_Named_Stanford det_Stanford_the conj_and_English_Named cop_English_is nsubj_English_language prep_in_English_experiments nn_language_side nn_language_source poss_language_our det_experiments_the predet_experiments_all
I08-4013	P05-1045	o	In the first approach heuristic rules are used to find the dependencies -LRB- Bunescu and Mooney 2004 -RRB- or penalties for label inconsistency are required to handset ad-hoc -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_ad-hoc_Finkel dobj_handset_ad-hoc aux_handset_to xcomp_required_handset auxpass_required_are csubjpass_required_used nn_inconsistency_label prep_for_penalties_inconsistency conj_and_Bunescu_2004 conj_and_Bunescu_Mooney conj_or_dependencies_penalties dep_dependencies_2004 dep_dependencies_Mooney dep_dependencies_Bunescu det_dependencies_the dobj_find_penalties dobj_find_dependencies aux_find_to xcomp_used_find auxpass_used_are nsubjpass_used_rules prep_in_used_approach nn_rules_heuristic amod_approach_first det_approach_the
I08-6004	P05-1045	o	Corpus Time Period Size Articles Words New Indian Express -LRB- English -RRB- 2007.01.01 to 2007.08.31 2,359 347,050 Dinamani -LRB- Tamil -RRB- 2007.01.01 to 2007.08.31 2,359 256,456 Table 1 Statistics on Comparable Corpora From the above corpora we first extracted all the NEs from the English side using the Stanford NER tool -LSB- Finkel et al 2005 -RSB-	amod_Finkel_2005 dep_Finkel_al nn_Finkel_et nn_tool_NER nn_tool_Stanford det_tool_the dobj_using_tool amod_side_English det_side_the det_NEs_the predet_NEs_all dep_extracted_Finkel vmod_extracted_using prep_from_extracted_side dobj_extracted_NEs advmod_extracted_first nsubj_extracted_we nsubj_extracted_Express amod_corpora_above det_corpora_the prep_from_Corpora_corpora amod_Corpora_Comparable prep_on_Statistics_Corpora num_Table_1 num_Table_256,456 dep_2,359_Table dep_2007.08.31_2,359 prep_to_2007.01.01_2007.08.31 dep_Dinamani_2007.01.01 appos_Dinamani_Tamil num_Dinamani_347,050 number_347,050_2,359 number_347,050_2007.08.31 dep_347,050_to number_347,050_2007.01.01 dep_Express_Statistics dep_Express_Dinamani appos_Express_English nn_Express_Indian nn_Express_New nn_Express_Words nn_Express_Articles nn_Express_Size nn_Express_Period nn_Express_Time nn_Express_Corpus
N06-1054	P05-1045	o	That is a significant shortcoming because in many domains hard or soft global constraints on the label sequence are motivated by common sense For named entity recognition a phrase that appears multiple times should tend to get the same label each time -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_time_Finkel det_time_each dep_label_time amod_label_same det_label_the dobj_get_label aux_get_to xcomp_tend_get aux_tend_should nsubj_tend_times amod_times_multiple ccomp_appears_tend nsubj_appears_that rcmod_phrase_appears det_phrase_a appos_recognition_phrase nn_recognition_entity dep_named_recognition amod_sense_common prepc_for_motivated_named agent_motivated_sense auxpass_motivated_are nsubjpass_motivated_constraints prep_in_motivated_domains mark_motivated_because nn_sequence_label det_sequence_the prep_on_constraints_sequence amod_constraints_global amod_constraints_soft amod_constraints_hard conj_or_hard_soft amod_domains_many advcl_shortcoming_motivated amod_shortcoming_significant det_shortcoming_a cop_shortcoming_is nsubj_shortcoming_That
N06-1054	P05-1045	o	should appear with at most one value in each announcement although the field and value may be repeated -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_repeated_Finkel auxpass_repeated_be aux_repeated_may nsubjpass_repeated_value nsubjpass_repeated_field mark_repeated_although conj_and_field_value det_field_the det_announcement_each num_value_one amod_value_most pobj_at_value pcomp_with_at advcl_appear_repeated prep_in_appear_announcement prep_appear_with aux_appear_should
N06-1054	P05-1045	o	Such techniques include Gibbs sampling -LRB- Finkel et al. 2005 -RRB- a general-purpose Monte Carlo method and integer linear programming -LRB- ILP -RRB- -LRB- Roth and Yih 2005 -RRB- a general-purpose exact framework for NP-complete problems	amod_problems_NP-complete prep_for_framework_problems amod_framework_exact amod_framework_general-purpose det_framework_a dep_Roth_2005 conj_and_Roth_Yih appos_programming_ILP amod_programming_linear nn_programming_integer nn_method_Carlo nn_method_Monte amod_method_general-purpose det_method_a amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et appos_sampling_framework appos_sampling_Yih appos_sampling_Roth conj_and_sampling_programming conj_and_sampling_method dep_sampling_Finkel nn_sampling_Gibbs dobj_include_programming dobj_include_method dobj_include_sampling nsubj_include_techniques amod_techniques_Such
N09-1037	P05-1045	o	For the named entity features we used a fairly standard feature set similar to those described in -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_in_Finkel prep_described_in vmod_those_described prep_to_similar_those amod_set_similar nn_set_feature amod_set_standard det_set_a advmod_standard_fairly dobj_used_set nsubj_used_we prep_for_used_features nn_features_entity amod_features_named det_features_the
N09-1068	P05-1045	o	Our features were based on those in -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_in_Finkel prep_those_in prep_on_based_those auxpass_based_were nsubjpass_based_features poss_features_Our
P06-1059	P05-1045	o	Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models -LRB- HMMs -RRB- -LRB- Bikel et al. 1997 -RRB- the dictionary HMM model -LRB- Kou et al. 2005 -RRB- and Maximum Entropy Markov Models -LRB- MEMMs -RRB- -LRB- Finkel et al. 2004 -RRB-	amod_Finkel_2004 dep_Finkel_al. nn_Finkel_et appos_Models_MEMMs nn_Models_Markov nn_Models_Entropy nn_Models_Maximum amod_Kou_2005 dep_Kou_al. nn_Kou_et dep_model_Finkel conj_and_model_Models dep_model_Kou nn_model_HMM nn_model_dictionary det_model_the dep_,_Models dep_,_model amod_Bikel_1997 dep_Bikel_al. nn_Bikel_et dep_Models_Bikel appos_Models_HMMs nn_Models_Markov nn_Models_Hidden prep_including_techniques_Models nn_techniques_learning nn_techniques_machine nn_tasks_Bio-NER prep_of_studies_tasks amod_studies_previous det_studies_the prep_of_Many_studies prep_based_on_``_techniques auxpass_``_been aux_``_have nsubjpass_``_Many
P06-1059	P05-1045	o	However other types of nonlocal information have also been shown to be effective -LRB- Finkel et al. 2005 -RRB- and we will examine the effectiveness of other non-local information which can be embedded into label information	nn_information_label prep_into_embedded_information auxpass_embedded_be aux_embedded_can nsubjpass_embedded_which rcmod_information_embedded amod_information_non-local amod_information_other prep_of_effectiveness_information det_effectiveness_the dobj_examine_effectiveness aux_examine_will nsubj_examine_we amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_effective_Finkel cop_effective_be aux_effective_to conj_and_shown_examine xcomp_shown_effective auxpass_shown_been advmod_shown_also aux_shown_have nsubjpass_shown_types advmod_shown_However amod_information_nonlocal prep_of_types_information amod_types_other
P06-1059	P05-1045	o	information about the previous state -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et amod_state_previous det_state_the dep_information_Finkel prep_about_information_state
P06-1059	P05-1045	n	In a recent study by Finkel et al. -LRB- 2005 -RRB- nonlocal information is encoded using an independence model and the inference is performed by Gibbs sampling which enables us to use a stateof-the-art factored model and carry out training efficiently but inference still incurs a considerable computational cost	amod_cost_computational amod_cost_considerable det_cost_a dobj_incurs_cost advmod_incurs_still nsubj_incurs_inference advmod_carry_efficiently dobj_carry_training prt_carry_out amod_model_factored amod_model_stateof-the-art det_model_a conj_and_use_carry dobj_use_model aux_use_to xcomp_enables_carry xcomp_enables_use dobj_enables_us nsubj_enables_which conj_but_sampling_incurs rcmod_sampling_enables nn_sampling_Gibbs agent_performed_incurs agent_performed_sampling auxpass_performed_is nsubjpass_performed_inference det_inference_the nn_model_independence det_model_an dobj_using_model conj_and_encoded_performed xcomp_encoded_using auxpass_encoded_is nsubjpass_encoded_information amod_information_nonlocal dep_information_2005 prep_in_information_study dep_Finkel_al. nn_Finkel_et prep_by_study_Finkel amod_study_recent det_study_a
P06-1089	P05-1045	o	Global information is known to be useful in other NLP tasks especially in the named entity recognition task and several studies successfully used global features -LRB- Chieu and Ng 2002 Finkel et al. 2005 -RRB-	num_Finkel_2005 nn_Finkel_al. nn_Finkel_et dep_Chieu_Finkel conj_and_Chieu_2002 conj_and_Chieu_Ng dep_features_2002 dep_features_Ng dep_features_Chieu amod_features_global amod_features_used advmod_used_successfully dep_studies_features amod_studies_several nn_task_recognition nn_task_entity amod_task_named det_task_the nn_tasks_NLP amod_tasks_other conj_and_useful_studies prep_in_useful_task advmod_useful_especially prep_in_useful_tasks cop_useful_be aux_useful_to xcomp_known_studies xcomp_known_useful auxpass_known_is nsubjpass_known_information amod_information_Global
P06-1141	P05-1045	o	Most existing work to capture labelconsistency has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity -LRB- Finkel et al. 2005 Sutton and McCallum 2004 -RRB- where n is the number of occurrences of the given entity	amod_entity_given det_entity_the prep_of_occurrences_entity prep_of_number_occurrences det_number_the cop_number_is nsubj_number_n advmod_number_where amod_Sutton_2004 conj_and_Sutton_McCallum dep_Finkel_McCallum dep_Finkel_Sutton dep_Finkel_2005 dep_Finkel_al. nn_Finkel_et det_entity_an prep_of_occurrences_entity amod_occurrences_different det_occurrences_the prep_between_dependencies_occurrences amod_dependencies_pairwise nn_dependencies_parenleftbign2parenrightbig det_dependencies_all dobj_create_dependencies aux_create_to advcl_attempted_number dep_attempted_Finkel xcomp_attempted_create aux_attempted_has nsubj_attempted_work dobj_capture_labelconsistency aux_capture_to vmod_work_capture amod_work_existing amod_work_Most
P06-1141	P05-1045	o	Most work has looked to model non-local dependencies only within a document -LRB- Finkel 1125 et al. 2005 Chieu and Ng 2002 Sutton and McCallum 2004 Bunescu and Mooney 2004 -RRB-	dep_Chieu_2004 conj_and_Chieu_Mooney conj_and_Chieu_Bunescu conj_and_Chieu_2004 conj_and_Chieu_McCallum conj_and_Chieu_Sutton conj_and_Chieu_2002 conj_and_Chieu_Ng nn_al._et num_al._1125 dep_Finkel_Mooney dep_Finkel_Bunescu dep_Finkel_2004 dep_Finkel_McCallum dep_Finkel_Sutton dep_Finkel_2002 dep_Finkel_Ng dep_Finkel_Chieu appos_Finkel_2005 dep_Finkel_al. dep_document_Finkel det_document_a amod_dependencies_non-local prep_within_model_document advmod_model_only dobj_model_dependencies aux_model_to xcomp_looked_model aux_looked_has nsubj_looked_work amod_work_Most
P06-1141	P05-1045	n	The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus which would be relatively much harder to incorporate in approaches like -LRB- Bunescu and Mooney 2004 -RRB- and -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_Bunescu_2004 conj_and_Bunescu_Mooney conj_and_like_Finkel dep_like_Mooney dep_like_Bunescu prep_approaches_Finkel prep_approaches_like prep_in_incorporate_approaches aux_incorporate_to xcomp_harder_incorporate amod_harder_much cop_harder_be aux_harder_would nsubj_harder_which advmod_much_relatively amod_corpus_whole det_corpus_the dep_incorporate_harder prep_across_incorporate_corpus dobj_incorporate_dependencies aux_incorporate_to dep_incorporate_easy nsubj_incorporate_it xcomp_makes_incorporate nsubj_makes_simplicity poss_approach_our prep_of_simplicity_approach det_simplicity_The
P06-1141	P05-1045	o	Additionally our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF in contrast approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30 -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et prep_of_factor_30 det_factor_a nn_time_inference prep_by_increase_factor dobj_increase_time aux_increase_can advmod_increase_directly nsubj_increase_dependencies det_dependencies_the appos_model_Finkel rcmod_model_increase dep_that_model prep_Sampling_that nsubj_Sampling_Gibbs prepc_like_approaches_Sampling amod_CRF_sequential amod_CRF_single det_CRF_a prep_with_time_CRF nn_time_inference det_time_the num_time_about quantmod_about_twice advmod_about_just prep_in_do_time dobj_do_inference aux_do_to ccomp_possible_do amod_it_possible dep_makes_approaches prep_in_makes_contrast dobj_makes_it nsubj_makes_approach advmod_makes_Additionally poss_approach_our
P06-1141	P05-1045	n	We also compare our performance against -LRB- Bunescu and Mooney 2004 -RRB- and -LRB- Finkel et al. 2005 -RRB- and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF	nn_CRF_baseline amod_CRF_competitive det_CRF_a advmod_competitive_very prep_from_starting_CRF amod_work_existing prep_than_improvement_work amod_improvement_relative amod_improvement_higher prepc_despite_manage_starting dobj_manage_improvement nsubj_manage_we mark_manage_that ccomp_find_manage amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et conj_and_Bunescu_find conj_and_Bunescu_Finkel conj_and_Bunescu_2004 conj_and_Bunescu_Mooney prep_against_performance_find prep_against_performance_Finkel prep_against_performance_2004 prep_against_performance_Mooney prep_against_performance_Bunescu poss_performance_our dobj_compare_performance advmod_compare_also nsubj_compare_We ccomp_``_compare
P06-1141	P05-1045	o	A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily a newspaper and China the country -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_country_Finkel det_country_the det_newspaper_a appos_Daily_country conj_and_Daily_China appos_Daily_newspaper nn_Daily_China det_Daily_The preconj_Daily_both prep_to_references_China prep_to_references_Daily dobj_containing_references vmod_documents_containing prep_of_that_documents prep_is_that nsubj_is_case nn_dataset_CoNLL det_dataset_the prep_in_case_dataset prep_of_case_this amod_case_common det_case_A advmod_common_very ccomp_``_is
P06-2054	P05-1045	o	An additional consistent edge of a linear-chain conditional random field -LRB- CRF -RRB- explicitly models the dependencies between distant occurrences of similar words -LRB- Sutton and McCallum 2004 Finkel et al. 2005 -RRB-	num_Finkel_2005 nn_Finkel_al. nn_Finkel_et dep_Sutton_Finkel amod_Sutton_2004 conj_and_Sutton_McCallum appos_words_McCallum appos_words_Sutton amod_words_similar prep_of_occurrences_words amod_occurrences_distant prep_between_dependencies_occurrences det_dependencies_the dep_models_dependencies advmod_models_explicitly appos_field_CRF amod_field_random amod_field_conditional amod_field_linear-chain det_field_a dep_edge_models prep_of_edge_field amod_edge_consistent amod_edge_additional det_edge_An
P08-4003	P05-1045	o	Starting out with a chunking pipeline which uses a classical combination of tagger and chunker with the Stanford POS tagger -LRB- Toutanova et al. 2003 -RRB- the YamCha chunker -LRB- Kudoh and Matsumoto 2000 -RRB- and the Stanford Named Entity Recognizer -LRB- Finkel et al. 2005 -RRB- the desire to use richer syntactic representations led to the development of a parsing pipeline which uses Charniak and Johnsons reranking parser -LRB- Charniak and Johnson 2005 -RRB- to assign POS tags and uses base NPs as chunk equivalents while also providing syntactic trees that can be used by feature extractors	nn_extractors_feature agent_used_extractors auxpass_used_be aux_used_can nsubjpass_used_that rcmod_trees_used amod_trees_syntactic dobj_providing_trees advmod_providing_also mark_providing_while nn_equivalents_chunk nn_NPs_base prep_as_uses_equivalents dobj_uses_NPs nsubj_uses_which nn_tags_POS dobj_assign_tags aux_assign_to dep_Charniak_2005 conj_and_Charniak_Johnson amod_parser_reranking nn_parser_Johnsons dep_Charniak_Johnson dep_Charniak_Charniak conj_and_Charniak_parser conj_and_uses_uses xcomp_uses_assign dobj_uses_parser dobj_uses_Charniak nsubj_uses_which rcmod_pipeline_uses rcmod_pipeline_uses nn_pipeline_parsing det_pipeline_a prep_of_development_pipeline det_development_the prep_to_led_development nsubj_led_desire nn_representations_syntactic amod_representations_richer dobj_use_representations aux_use_to vmod_desire_use det_desire_the amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et rcmod_Recognizer_led dep_Recognizer_Finkel nn_Recognizer_Entity advcl_Named_providing dobj_Named_Recognizer det_Stanford_the dep_Kudoh_2000 conj_and_Kudoh_Matsumoto appos_chunker_Matsumoto appos_chunker_Kudoh nn_chunker_YamCha det_chunker_the amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_tagger_Toutanova nn_tagger_POS nn_tagger_Stanford det_tagger_the conj_and_tagger_chunker prep_of_combination_chunker prep_of_combination_tagger amod_combination_classical det_combination_a dobj_uses_combination nsubj_uses_which conj_and_pipeline_Stanford conj_and_pipeline_chunker prep_with_pipeline_tagger rcmod_pipeline_uses amod_pipeline_chunking det_pipeline_a dep_Starting_Named prep_with_Starting_Stanford prep_with_Starting_chunker prep_with_Starting_pipeline prt_Starting_out ccomp_``_Starting
P09-1113	P05-1045	o	We perform named entity tagging using the Stanford four-class named entity tagger -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_tagger_Finkel nn_tagger_entity dep_named_tagger dep_four-class_named amod_Stanford_four-class dep_the_Stanford dobj_using_the xcomp_tagging_using nsubj_tagging_entity dep_named_tagging dep_perform_named dep_We_perform dep_``_We
P09-2041	P05-1045	o	To implement this method we rst use the Stanford Named Entity Recognizer4 -LRB- Finkel et al. 2005 -RRB- toidentifythesetofpersonandorganisation entities E from each article in the corpus	det_corpus_the prep_in_article_corpus det_article_each appos_entities_E nn_entities_toidentifythesetofpersonandorganisation nn_entities_Recognizer4 amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_Recognizer4_Finkel nn_Recognizer4_Entity prep_from_Named_article dobj_Named_entities det_Stanford_the dep_use_Named dobj_use_Stanford ccomp_rst_use nsubj_rst_we advcl_rst_implement det_method_this dobj_implement_method aux_implement_To ccomp_``_rst
W06-1643	P05-1045	o	Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques including TRP -LRB- Sutton and McCallum 2004 -RRB- and Gibbs sampling -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et nn_sampling_Gibbs amod_Sutton_2004 conj_and_Sutton_McCallum conj_and_TRP_sampling dep_TRP_McCallum dep_TRP_Sutton dep_techniques_Finkel prep_including_techniques_sampling prep_including_techniques_TRP nn_techniques_inference amod_techniques_probabilistic amod_techniques_approximate amod_techniques_used dep_dependencies_techniques amod_dependencies_nonlocal dobj_containing_dependencies vmod_CRFs_containing prep_with_work_CRFs amod_work_previous amod_work_Most
W06-1655	P05-1045	o	4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks including named-entity recognition gene prediction shallow parsing and others -LRB- Finkel et al. 2005 Culotta et al. 2005 Sha and Pereira 2003 -RRB-	dep_Sha_2003 conj_and_Sha_Pereira dep_Culotta_Pereira dep_Culotta_Sha num_Culotta_2005 nn_Culotta_al. nn_Culotta_et dep_Finkel_Culotta appos_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_others_Finkel amod_parsing_shallow nn_prediction_gene conj_and_recognition_others conj_and_recognition_parsing conj_and_recognition_prediction amod_recognition_named-entity amod_tasks_chunking prep_including_variety_others prep_including_variety_parsing prep_including_variety_prediction prep_including_variety_recognition prep_of_variety_tasks det_variety_a prep_for_use_variety prep_of_use_CRFs det_use_the dobj_exploring_use vmod_volume_exploring prep_of_volume_work amod_volume_significant det_volume_a nsubj_is_volume expl_is_There dep_is_Relation amod_Work_Previous prep_to_Relation_Work num_Relation_4 ccomp_``_is
W07-2058	P05-1045	o	We extract the named entities from the web pages using the Stanford Named Entity Recognizer -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_Recognizer_Finkel nn_Recognizer_Entity dobj_Named_Recognizer det_Stanford_the dep_using_Named dobj_using_Stanford nn_pages_web det_pages_the amod_entities_named det_entities_the vmod_extract_using prep_from_extract_pages dobj_extract_entities nsubj_extract_We ccomp_``_extract
W09-0422	P05-1045	o	One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_Recognizer_Finkel nn_Recognizer_Entity dobj_Named_Recognizer nsubj_Named_recognition dobj_using_Stanford vmod_recognition_using nn_recognition_entity ccomp_named_Named auxpass_named_is nsubjpass_named_One prep_of_analysis_English det_analysis_the prep_in_steps_analysis det_steps_the prep_of_One_steps ccomp_``_named
W09-1119	P05-1045	o	The results we obtained on the CoNLL03 test set were consistent with what was reported in -LRB- Finkel et al. 2005 -RRB-	amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et dep_in_Finkel prep_reported_in auxpass_reported_was nsubjpass_reported_what prepc_with_consistent_reported cop_consistent_were nsubj_consistent_results vmod_test_set nn_test_CoNLL03 det_test_the prep_on_obtained_test nsubj_obtained_we rcmod_results_obtained det_results_The
W09-1119	P05-1045	o	NER proves to be a knowledgeintensive task and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia Nonlocal Features Word-class Model 90.80 -LRB- Suzuki and Isozaki 2008 -RRB- Semi-supervised on 1Gword unlabeled data 89.92 -LRB- Ando and Zhang 2005 -RRB- Semi-supervised on 27Mword unlabeled data 89.31 -LRB- Kazama and Torisawa 2007a -RRB- Wikipedia 88.02 -LRB- Krishnan and Manning 2006 -RRB- Non-local Features 87.24 -LRB- Kazama and Torisawa 2007b -RRB- Non-local Features 87.17 + -LRB- Finkel et al. 2005 -RRB- Non-local Features 86.86 Table 7 Results for CoNLL03 data reported in the literature	det_literature_the prep_in_reported_literature vmod_data_reported nn_data_CoNLL03 prep_for_Results_data num_Table_7 num_Table_86.86 nn_Table_Features amod_Table_Non-local num_Table_Finkel num_Table_87.17 amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et conj_+_87.17_Finkel dep_Features_Table amod_Features_Non-local conj_and_Kazama_2007b conj_and_Kazama_Torisawa dep_Features_Features dep_Features_2007b dep_Features_Torisawa dep_Features_Kazama num_Features_87.24 amod_Features_Non-local dep_Features_Manning dep_Features_Krishnan dep_Krishnan_2006 conj_and_Krishnan_Manning dep_88.02_Features nn_88.02_Wikipedia nn_88.02_89.31 dep_Kazama_2007a conj_and_Kazama_Torisawa appos_89.31_Torisawa appos_89.31_Kazama nn_89.31_data amod_89.31_unlabeled nn_89.31_27Mword amod_Ando_2005 conj_and_Ando_Zhang amod_89.92_Semi-supervised appos_89.92_Zhang appos_89.92_Ando nn_89.92_data amod_89.92_unlabeled nn_89.92_1Gword dep_Suzuki_2008 conj_and_Suzuki_Isozaki prep_on_Model_88.02 prep_on_Model_89.92 amod_Model_Semi-supervised appos_Model_Isozaki appos_Model_Suzuki num_Model_90.80 amod_Model_Word-class amod_Features_Nonlocal appos_Wikipedia_Model appos_Wikipedia_Features nn_Wikipedia_LBJ-NER nn_Wikipedia_F1 conj_+_F1_LBJ-NER dobj_Used_Wikipedia nsubj_Used_Resources mark_Used_that nn_Resources_System ccomp_observe_Used aux_observe_to xcomp_reassuring_observe aux_reassuring_was nsubj_reassuring_it amod_task_knowledgeintensive det_task_a cop_task_be aux_task_to dep_proves_Results conj_and_proves_reassuring xcomp_proves_task nsubj_proves_NER rcmod_``_reassuring rcmod_``_proves
W09-1218	P05-1045	o	Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling -LRB- Finkel et al. 2005 Krishnan and Manning 2006 Kazama and Torisawa 2007 -RRB- and dependency parsing -LRB- Nakagawa 2007 -RRB- with a great deal of success	prep_of_deal_success amod_deal_great det_deal_a appos_Nakagawa_2007 dep_parsing_Nakagawa nn_parsing_dependency dep_Krishnan_2007 conj_and_Krishnan_Torisawa conj_and_Krishnan_Kazama conj_and_Krishnan_2006 conj_and_Krishnan_Manning dep_Finkel_Torisawa dep_Finkel_Kazama dep_Finkel_2006 dep_Finkel_Manning dep_Finkel_Krishnan appos_Finkel_2005 dep_Finkel_al. nn_Finkel_et prep_with_labeling_deal conj_and_labeling_parsing dep_labeling_Finkel amod_labeling_sequential prep_such_as_applications_parsing prep_such_as_applications_labeling nn_applications_NLP amod_applications_several agent_explored_applications auxpass_explored_been aux_explored_has nsubjpass_explored_Use nn_problem_prediction amod_problem_structured amod_features_global prep_for_Use_problem prep_of_Use_features
N07-2005	P05-3024	o	K-best suffix arrays have been used in autocomplete applications -LRB- Church and Thiesson 2005 -RRB-	amod_Church_2005 conj_and_Church_Thiesson dep_applications_Thiesson dep_applications_Church nn_applications_autocomplete prep_in_used_applications auxpass_used_been aux_used_have nsubjpass_used_arrays nn_arrays_suffix amod_arrays_K-best
D07-1107	P06-1014	o	Finally we compare against the mapping from WordNet to the Oxford English Dictionary constructed in -LRB- Navigli 2006 -RRB- equivalent to clustering based solely on the OED feature	nn_feature_OED det_feature_the prep_on_based_feature advmod_based_solely vmod_clustering_based prep_to_equivalent_clustering amod_Navigli_equivalent dep_Navigli_2006 prep_in_constructed_Navigli vmod_Dictionary_constructed nn_Dictionary_English nn_Dictionary_Oxford det_Dictionary_the prep_to_mapping_Dictionary prep_from_mapping_WordNet det_mapping_the prep_against_compare_mapping nsubj_compare_we advmod_compare_Finally
D07-1107	P06-1014	o	Of the methods we compare against only the WordNet-based similarity measures -LRB- Mihalcea and Moldovan 2001 -RRB- and -LRB- Navigli 2006 -RRB- provide a method for predicting verb similarities our learned measure widely outperforms these methods achieving a 13.6 % F-score improvement over the LESK similarity measure	nn_measure_similarity nn_measure_LESK det_measure_the prep_over_improvement_measure nn_improvement_F-score amod_improvement_% det_improvement_a number_%_13.6 dobj_achieving_improvement det_methods_these vmod_outperforms_achieving dobj_outperforms_methods advmod_outperforms_widely nsubj_outperforms_measure amod_measure_learned poss_measure_our dobj_verb_similarities ccomp_predicting_verb prepc_for_method_predicting det_method_a dobj_provide_method nsubj_provide_Navigli nsubj_provide_measures mark_provide_against dep_Navigli_2006 dep_Mihalcea_2001 conj_and_Mihalcea_Moldovan conj_and_measures_Navigli appos_measures_Moldovan appos_measures_Mihalcea nn_measures_similarity amod_measures_WordNet-based det_measures_the advmod_measures_only parataxis_compare_outperforms advcl_compare_provide nsubj_compare_we prep_of_compare_methods det_methods_the
D07-1107	P06-1014	o	Only the measures provided by LESK HSO VEC -LRB- Mihalcea and Moldovan 2001 -RRB- and -LRB- Navigli 2006 -RRB- provide a method for predicting adjective similarities of these only LESK and VEC outperform the uninformed baseline on adjectives while our learned measure achieves a 4.0 % improvement over the LESK measure on adjectives	prep_on_measure_adjectives nn_measure_LESK det_measure_the prep_over_improvement_measure amod_improvement_% det_improvement_a number_%_4.0 dobj_achieves_improvement nsubj_achieves_measure mark_achieves_while amod_measure_learned poss_measure_our amod_baseline_uninformed det_baseline_the advcl_outperform_achieves prep_on_outperform_adjectives dobj_outperform_baseline nsubj_outperform_VEC nsubj_outperform_LESK prep_of_outperform_these conj_and_LESK_VEC advmod_LESK_only amod_similarities_adjective dobj_predicting_similarities prepc_for_method_predicting det_method_a parataxis_provide_outperform dobj_provide_method nsubj_provide_Navigli nsubj_provide_measures dep_Navigli_2006 dep_Mihalcea_2001 conj_and_Mihalcea_Moldovan appos_LESK_Moldovan appos_LESK_Mihalcea conj_LESK_VEC conj_LESK_HSO agent_provided_LESK conj_and_measures_Navigli vmod_measures_provided det_measures_the advmod_measures_Only
D07-1107	P06-1014	o	-LRB- Navigli 2006 -RRB- presents an automatic approach for mapping between sense inventories here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary	nn_Dictionary_English nn_Dictionary_Oxford amod_Dictionary_coarser-grained det_Dictionary_the prep_within_made_Dictionary vmod_distinctions_made conj_and_senses_distinctions nn_senses_WordNet prep_between_map_distinctions prep_between_map_senses aux_map_to dep_map_order mark_map_in advcl_exploited_map auxpass_exploited_are nsubjpass_exploited_similarities advmod_exploited_here nn_inventories_sense num_inventories_two det_inventories_the amod_relations_structured conj_and_definition_relations prep_between_gloss_inventories dobj_gloss_relations dobj_gloss_definition prepc_in_similarities_gloss nn_inventories_sense prep_between_mapping_inventories prep_for_approach_mapping amod_approach_automatic det_approach_an parataxis_presents_exploited dobj_presents_approach nsubj_presents_Navigli amod_Navigli_2006
D07-1107	P06-1014	o	Finally we use as a feature the mappings produced in -LRB- Navigli 2006 -RRB- of WordNet senses to Oxford English Dictionary senses	nn_senses_Dictionary nn_senses_English nn_senses_Oxford nn_senses_WordNet prep_to_Navigli_senses prep_of_Navigli_senses dep_Navigli_2006 prep_in_produced_Navigli vmod_mappings_produced det_mappings_the dep_feature_mappings det_feature_a prep_as_use_feature nsubj_use_we advmod_use_Finally
D09-1020	P06-1014	o	Several researchers -LRB- e.g. -LRB- Palmer et al. 2004 Navigli 2006 Snow et al. 2007 Hovy et al. 2006 -RRB- -RRB- work on reducing the granularity of sense inventories for WSD	prep_for_inventories_WSD nn_inventories_sense prep_of_granularity_inventories det_granularity_the dobj_reducing_granularity prepc_on_work_reducing num_Hovy_2006 nn_Hovy_al. nn_Hovy_et num_Snow_2007 nn_Snow_al. nn_Snow_et dep_Navigli_Hovy conj_Navigli_Snow num_Navigli_2006 dep_Palmer_work dep_Palmer_Navigli num_Palmer_2004 dep_Palmer_al. nn_Palmer_et dep_e.g._Palmer dep_researchers_e.g. amod_researchers_Several dep_``_researchers
D09-1046	P06-1014	o	Such coarse-grained inventories can be produced manually from scratch -LRB- Hovy et al. 2006 -RRB- or by automatically relating -LRB- McCarthy 2006 -RRB- or clustering -LRB- Navigli 2006 Navigli et al. 2007 -RRB- existing word senses	nn_senses_word amod_senses_existing nn_senses_clustering num_Navigli_2007 nn_Navigli_al. nn_Navigli_et dep_Navigli_Navigli appos_Navigli_2006 dep_clustering_Navigli conj_or_McCarthy_senses amod_McCarthy_2006 dobj_relating_senses dobj_relating_McCarthy advmod_relating_automatically pcomp_by_relating dep_Hovy_2006 dep_Hovy_al. nn_Hovy_et conj_or_produced_by dep_produced_Hovy prep_from_produced_scratch advmod_produced_manually auxpass_produced_be aux_produced_can nsubjpass_produced_inventories amod_inventories_coarse-grained amod_inventories_Such
D09-1046	P06-1014	o	WordNet has been criticized for being overly finegrained -LRB- Navigli et al. 2007 Ide and Wilks 2006 -RRB- we are using it here because it is the sense inventory used by Erk et al.	nn_al._et nn_al._Erk agent_used_al. vmod_inventory_used nn_inventory_sense det_inventory_the cop_inventory_is nsubj_inventory_it mark_inventory_because advmod_using_here dobj_using_it aux_using_are nsubj_using_we advmod_using_al. num_Ide_2006 conj_and_Ide_Wilks conj_al._Wilks conj_al._Ide conj_al._2007 nn_al._et advcl_Navigli_inventory xcomp_Navigli_using dep_finegrained_Navigli advmod_finegrained_overly cop_finegrained_being prepc_for_criticized_finegrained auxpass_criticized_been aux_criticized_has nsubjpass_criticized_WordNet
D09-1081	P06-1014	o	WordNet sense information has been criticized to be too fine grained -LRB- Agirre and Lopez de Lacalle Lekuona 2003 Navigli 2006 -RRB-	amod_Navigli_2006 nn_Lekuona_Lacalle amod_Lekuona_de nn_Lekuona_Lopez dep_Agirre_Navigli conj_and_Agirre_2003 conj_and_Agirre_Lekuona dep_grained_2003 dep_grained_Lekuona dep_grained_Agirre amod_grained_fine cop_grained_be aux_grained_to advmod_fine_too xcomp_criticized_grained auxpass_criticized_been aux_criticized_has nsubjpass_criticized_information nn_information_sense amod_information_WordNet
E09-1045	P06-1014	o	Thus some research has been focused on deriving different word-sense groupings to overcome the finegrained distinctions of WN -LRB- Hearst and Schutze 1993 -RRB- -LRB- Peters et al. 1998 -RRB- -LRB- Mihalcea and Moldovan 2001 -RRB- -LRB- Agirre and LopezDeLaCalle 2003 -RRB- -LRB- Navigli 2006 -RRB- and -LRB- Snow et al. 2007 -RRB-	amod_Snow_2007 dep_Snow_al. nn_Snow_et dep_Navigli_2006 dep_Agirre_2003 conj_and_Agirre_LopezDeLaCalle dep_Mihalcea_2001 conj_and_Mihalcea_Moldovan dep_Peters_Snow cc_Peters_and appos_Peters_Navigli appos_Peters_LopezDeLaCalle appos_Peters_Agirre appos_Peters_Moldovan appos_Peters_Mihalcea amod_Peters_1998 dep_Peters_al. nn_Peters_et dep_,_Peters amod_Hearst_1993 conj_and_Hearst_Schutze appos_WN_Schutze appos_WN_Hearst prep_of_distinctions_WN amod_distinctions_finegrained det_distinctions_the dobj_overcome_distinctions aux_overcome_to amod_groupings_word-sense amod_groupings_different vmod_deriving_overcome dobj_deriving_groupings prepc_on_focused_deriving auxpass_focused_been aux_focused_has nsubjpass_focused_research advmod_focused_Thus det_research_some advcl_``_focused
E09-1092	P06-1014	o	The first-sense heuristic can be thought of as striving for maximal specificity at the risk of precluding some admissible senses -LRB- reduced recall -RRB- 7Allowing for multiple fine-grained senses to be judged as appropriate in a given context goes back at least to Sussna -LRB- 1993 -RRB- discussed more recently by e.g. Navigli -LRB- 2006 -RRB-	appos_Navigli_2006 dep_e.g._Navigli dep_by_e.g. advmod_recently_more prep_discussed_by advmod_discussed_recently appos_Sussna_1993 pobj_at_least prep_to_goes_Sussna advmod_goes_at advmod_goes_back csubj_goes_striving mark_goes_as amod_context_given det_context_a prep_in_judged_context prep_as_judged_appropriate auxpass_judged_be aux_judged_to amod_senses_fine-grained amod_senses_multiple vmod_7Allowing_judged prep_for_7Allowing_senses amod_recall_reduced appos_senses_7Allowing appos_senses_recall amod_senses_admissible det_senses_some dobj_precluding_senses prepc_of_risk_precluding det_risk_the amod_specificity_maximal prep_at_striving_risk prep_for_striving_specificity dep_thought_discussed prepc_of_thought_goes auxpass_thought_be aux_thought_can nsubjpass_thought_heuristic amod_heuristic_first-sense det_heuristic_The
P08-2063	P06-1014	o	Navigli -LRB- 2006 -RRB- has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource	amod_resource_lexical amod_resource_coarse-grained det_resource_a advmod_coarse-grained_more nn_senses_WordNet prep_to_mapping_resource dobj_mapping_senses prepc_by_induced_mapping dobj_induced_clusters aux_induced_has nsubj_induced_Navigli appos_Navigli_2006
P08-2063	P06-1014	n	Although ITA rates and system performance both significantly improve with coarse-grained senses -LRB- Duffield et al. 2007 Navigli 2006 -RRB- the question about what level of granularity is needed remains	dobj_needed_remains auxpass_needed_is nsubjpass_needed_question advcl_needed_improve prep_of_level_granularity det_level_what prep_about_question_level det_question_the dep_Navigli_2006 dep_Duffield_Navigli amod_Duffield_2007 dep_Duffield_al. nn_Duffield_et appos_senses_Duffield amod_senses_coarse-grained prep_with_improve_senses advmod_improve_significantly preconj_improve_both nsubj_improve_performance nsubj_improve_rates mark_improve_Although nn_performance_system conj_and_rates_performance nn_rates_ITA ccomp_``_needed
P08-2063	P06-1014	n	WSD systems have been far more successful in distinguishing coarsegrained senses than fine-grained ones -LRB- Navigli 2006 -RRB- but does that approach neglect necessary meaning differences ?	nn_differences_meaning amod_differences_necessary nsubj_differences_neglect mark_differences_that nn_neglect_approach ccomp_does_differences nsubj_does_systems appos_Navigli_2006 dep_ones_Navigli amod_ones_fine-grained prep_than_senses_ones amod_senses_coarsegrained dobj_distinguishing_senses conj_but_successful_does prepc_in_successful_distinguishing advmod_successful_more advmod_successful_far cop_successful_been aux_successful_have nsubj_successful_systems nn_systems_WSD
W07-1404	P06-1014	o	This clustering was created automatically with the aid of a methodology described in -LRB- Navigli 2006 -RRB-	amod_Navigli_2006 dep_in_Navigli prep_described_in vmod_methodology_described det_methodology_a prep_of_aid_methodology det_aid_the prep_with_created_aid advmod_created_automatically auxpass_created_was nsubjpass_created_clustering det_clustering_This
W07-2006	P06-1014	o	2.2 Creation of a Coarse-Grained Sense Inventory To tackle the granularity issue we produced a coarser-grained version of the WordNet sense inventory3 based on the procedure described by Navigli -LRB- 2006 -RRB-	appos_Navigli_2006 agent_described_Navigli vmod_procedure_described det_procedure_the nn_inventory3_sense nn_inventory3_WordNet det_inventory3_the pobj_version_procedure prepc_based_on_version_on prep_of_version_inventory3 amod_version_coarser-grained det_version_a dobj_produced_version nsubj_produced_we nsubj_produced_Creation nn_issue_granularity det_issue_the dobj_tackle_issue aux_tackle_To vmod_Inventory_tackle nn_Inventory_Sense nn_Inventory_Coarse-Grained det_Inventory_a prep_of_Creation_Inventory num_Creation_2.2
W07-2059	P06-1014	o	However in the coarse-grained task the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses -LRB- Navigli 2006 -RRB-	amod_Navigli_2006 dep_senses_Navigli prep_over_class_senses amod_class_equivalence det_class_an dobj_representing_class vmod_cluster_representing det_cluster_each prep_with_clustered_cluster advmod_clustered_semi-automatically vmod_first_clustered nsubj_was_first dep_inventory_was nn_inventory_sense det_inventory_the ccomp_,_inventory amod_task_coarse-grained det_task_the pobj_in_task dep_,_in dep_``_However
D07-1070	P06-1027	o	In particular Abney defines a function K that is an upper bound on the negative log-likelihood and shows his bootstrapping algorithms locally minimize K We now present a generalization of Abneys K function and relate it to another semi-supervised learning technique entropy regularization -LRB- Brand 1999 Grandvalet and Bengio 2005 Jiao et al. 2006 -RRB-	num_Jiao_2006 nn_Jiao_al. nn_Jiao_et dep_Grandvalet_Jiao conj_and_Grandvalet_2005 conj_and_Grandvalet_Bengio conj_Brand_2005 conj_Brand_Bengio conj_Brand_Grandvalet conj_Brand_1999 dep_regularization_Brand nn_regularization_entropy appos_technique_regularization nn_technique_learning amod_technique_semi-supervised det_technique_another prep_to_relate_technique dobj_relate_it nsubj_relate_We nn_function_K nn_function_Abneys prep_of_generalization_function det_generalization_a conj_and_present_relate dobj_present_generalization advmod_present_now nsubj_present_We dobj_minimize_K advmod_minimize_locally nn_algorithms_bootstrapping poss_algorithms_his dep_shows_minimize dobj_shows_algorithms nsubj_shows_Abney amod_log-likelihood_negative det_log-likelihood_the prep_on_bound_log-likelihood amod_bound_upper det_bound_an auxpass_bound_is nsubjpass_bound_that rcmod_K_bound nn_K_function det_K_a parataxis_defines_relate parataxis_defines_present conj_and_defines_shows dobj_defines_K nsubj_defines_Abney prep_in_defines_particular
D07-1070	P06-1027	o	We thus introduce a multiplier to form the actual objective function that we minimize with respect to :4 summationdisplay iL logp i -LRB- yi -RRB- + Nsummationdisplay inegationslashL H -LRB- p i -RRB- -LRB- 4 -RRB- One may regard as a Lagrange multiplier that is used to constrain the classifiers uncertainty H to be low as presented in the work on entropy regularization -LRB- Brand 1999 Grandvalet and Bengio 2005 Jiao et al. 2006 -RRB-	num_Jiao_2006 nn_Jiao_al. nn_Jiao_et dep_Grandvalet_Jiao conj_and_Grandvalet_2005 conj_and_Grandvalet_Bengio conj_Brand_2005 conj_Brand_Bengio conj_Brand_Grandvalet conj_Brand_1999 dep_regularization_Brand nn_regularization_entropy prep_on_work_regularization det_work_the prep_in_presented_work mark_presented_as cop_low_be aux_low_to nn_H_uncertainty nn_H_classifiers det_H_the xcomp_constrain_low dobj_constrain_H aux_constrain_to xcomp_used_constrain auxpass_used_is nsubjpass_used_that rcmod_multiplier_used nn_multiplier_Lagrange det_multiplier_a pobj_as_multiplier aux_regard_may nsubj_regard_One dep_regard_4 dep_regard_H dep_p_i appos_H_p nn_H_inegationslashL nn_H_Nsummationdisplay dep_i_as conj_+_i_regard appos_i_yi appos_logp_regard appos_logp_i nn_logp_iL nn_logp_summationdisplay num_logp_:4 advcl_minimize_presented prep_with_respect_to_minimize_logp nsubj_minimize_we mark_minimize_that dep_function_minimize amod_function_objective amod_function_actual det_function_the dobj_form_function aux_form_to det_multiplier_a vmod_introduce_form dobj_introduce_multiplier advmod_introduce_thus nsubj_introduce_We ccomp_``_introduce
D07-1083	P06-1027	o	In fact many attempts have recently been made to develop semi-supervised SOL methods -LRB- Zhu et al. 2003 Li and McCallum 2005 Altun et al. 2005 Jiao et al. 2006 Brefeld and Scheffer 2006 -RRB-	amod_Brefeld_2006 conj_and_Brefeld_Scheffer num_Jiao_2006 nn_Jiao_al. nn_Jiao_et num_Altun_2005 nn_Altun_al. nn_Altun_et dep_Li_Scheffer dep_Li_Brefeld conj_and_Li_Jiao conj_and_Li_Altun num_Li_2005 conj_and_Li_McCallum dep_Zhu_Jiao dep_Zhu_Altun dep_Zhu_McCallum dep_Zhu_Li appos_Zhu_2003 dep_Zhu_al. nn_Zhu_et nn_methods_SOL amod_methods_semi-supervised dobj_develop_methods aux_develop_to dep_made_Zhu xcomp_made_develop auxpass_made_been advmod_made_recently aux_made_have nsubjpass_made_attempts prep_in_made_fact amod_attempts_many
D07-1083	P06-1027	o	5.3 Comparison with SS-CRF-MER When we consider semi-supervised SOL methods SS-CRF-MER -LRB- Jiao et al. 2006 -RRB- is the most competitive with HySOL since both methods are defined based on CRFs	prep_based_on_defined_CRFs auxpass_defined_are nsubjpass_defined_methods mark_defined_since det_methods_both prep_with_competitive_HySOL advmod_competitive_most det_competitive_the cop_competitive_is nsubj_competitive_SS-CRF-MER advcl_competitive_consider amod_Jiao_2006 dep_Jiao_al. nn_Jiao_et dep_SS-CRF-MER_Jiao nn_methods_SOL amod_methods_semi-supervised dobj_consider_methods nsubj_consider_we advmod_consider_When dep_Comparison_defined dep_Comparison_competitive prep_with_Comparison_SS-CRF-MER num_Comparison_5.3 dep_``_Comparison
D07-1083	P06-1027	o	In fact we still have a question as to whether SS-CRF-MER is really scalable in practical time for such a large amount of unlabeled data as used in our experiments which is about 680 times larger than that of -LRB- Jiao et al. 2006 -RRB-	amod_Jiao_2006 dep_Jiao_al. nn_Jiao_et dep_of_Jiao prep_that_of prep_than_larger_that num_larger_times cop_larger_is nsubj_larger_which num_times_680 quantmod_times_about rcmod_experiments_larger poss_experiments_our prep_in_used_experiments mark_used_as amod_data_unlabeled prep_of_amount_data amod_amount_large det_amount_a amod_amount_such prep_for_time_amount amod_time_practical advcl_scalable_used prep_in_scalable_time advmod_scalable_really cop_scalable_is nsubj_scalable_SS-CRF-MER mark_scalable_whether det_question_a pcomp_have_scalable prepc_as_to_have_to dobj_have_question advmod_have_still nsubj_have_we prep_in_have_fact
D07-1083	P06-1027	o	Semi-supervised conditional random fields -LRB- CRFs -RRB- based on a minimum entropy regularizer -LRB- SS-CRF-MER -RRB- have been proposed in -LRB- Jiao et al. 2006 -RRB-	amod_Jiao_2006 dep_Jiao_al. nn_Jiao_et dep_in_Jiao prep_proposed_in auxpass_proposed_been aux_proposed_have nsubjpass_proposed_fields appos_regularizer_SS-CRF-MER amod_regularizer_entropy amod_regularizer_minimum det_regularizer_a prep_on_based_regularizer vmod_fields_based appos_fields_CRFs amod_fields_random amod_fields_conditional amod_fields_Semi-supervised
D07-1088	P06-1027	p	Recent work includes improved model variants -LRB- e.g. Jiao et al. 2006 Okanohara et al. 2006 -RRB- and applications such as web data extraction -LRB- Pinto et al. 2003 -RRB- scientific citation extraction -LRB- Peng and McCallum 2004 -RRB- and word alignment -LRB- Blunsom and Cohn 2006 -RRB-	dep_Blunsom_2006 conj_and_Blunsom_Cohn dep_alignment_Cohn dep_alignment_Blunsom nn_alignment_word dep_Peng_2004 conj_and_Peng_McCallum dep_extraction_McCallum dep_extraction_Peng nn_extraction_citation amod_extraction_scientific amod_Pinto_2003 dep_Pinto_al. nn_Pinto_et conj_and_extraction_alignment conj_and_extraction_extraction dep_extraction_Pinto nn_extraction_data nn_extraction_web prep_such_as_applications_alignment prep_such_as_applications_extraction prep_such_as_applications_extraction dep_al._2006 nn_al._et nn_al._Okanohara nn_al._et nn_al._Jiao dep_e.g._al. appos_e.g._2006 dep_e.g._al. conj_and_variants_applications dep_variants_e.g. nn_variants_model amod_variants_improved dobj_includes_applications dobj_includes_variants nsubj_includes_work amod_work_Recent
D09-1005	P06-1027	o	The variance semiring is essential for many interesting training paradigms such as deterministic 40 annealing -LRB- Rose 1998 -RRB- minimum risk -LRB- Smith and Eisner 2006 -RRB- active and semi-supervised learning -LRB- Grandvalet and Bengio 2004 Jiao et al. 2006 -RRB-	num_Jiao_2006 nn_Jiao_al. nn_Jiao_et dep_Grandvalet_Jiao conj_and_Grandvalet_2004 conj_and_Grandvalet_Bengio dep_learning_2004 dep_learning_Bengio dep_learning_Grandvalet amod_learning_semi-supervised amod_learning_active conj_and_active_semi-supervised amod_Smith_2006 conj_and_Smith_Eisner appos_risk_Eisner appos_risk_Smith amod_risk_minimum amod_Rose_1998 appos_annealing_learning conj_annealing_risk dep_annealing_Rose num_annealing_40 amod_annealing_deterministic prep_such_as_paradigms_annealing nn_paradigms_training amod_paradigms_interesting amod_paradigms_many prep_for_essential_paradigms cop_essential_is nsubj_essential_semiring nn_semiring_variance det_semiring_The ccomp_``_essential
D09-1009	P06-1027	o	We use Entropy Regularization -LRB- ER -RRB- -LRB- Jiao et al. 2006 -RRB- to leverage unlabeled instances .7 We weight the ER term by choosing the best8 weight in -LCB- 103,102,101,1,10 -RCB- multiplied by #labeled #unlabeled for each data set and query selection method	nn_method_selection nn_method_query conj_and_set_method nn_set_data det_set_each prep_for_#unlabeled_method prep_for_#unlabeled_set vmod_#labeled_#unlabeled prep_by_multiplied_#labeled nsubjpass_multiplied_103,102,101,1,10 prepc_in_weight_multiplied amod_weight_best8 det_weight_the dobj_choosing_weight nn_term_ER det_term_the prepc_by_weight_choosing dobj_weight_term nsubj_weight_We rcmod_.7_weight dep_instances_.7 amod_instances_unlabeled nn_instances_leverage amod_Jiao_2006 dep_Jiao_al. nn_Jiao_et dep_Regularization_Jiao appos_Regularization_ER nn_Regularization_Entropy prep_to_use_instances dobj_use_Regularization nsubj_use_We
D09-1134	P06-1027	o	For example minimum entropy regularization -LRB- Grandvalet and Bengio 2004 Jiao et al. 2006 -RRB- aims to maximize the conditional likelihood of labeled data while minimizing the conditional entropy of unlabeled data summationdisplay i logp -LRB- y -LRB- i -RRB- | x -LRB- i -RRB- -RRB- 122bardblbardbl2H -LRB- y | x -RRB- -LRB- 3 -RRB- This approach generally would result in sharper models which can be data-sensitive in practice	prep_in_data-sensitive_practice cop_data-sensitive_be aux_data-sensitive_can nsubj_data-sensitive_which rcmod_models_data-sensitive amod_models_sharper prep_in_result_models aux_result_would advmod_result_generally nsubj_result_approach dep_result_3 dep_result_122bardblbardbl2H nsubj_result_logp det_approach_This num_x_| nn_x_y appos_122bardblbardbl2H_x dep_122bardblbardbl2H_x dep_122bardblbardbl2H_i dep_122bardblbardbl2H_y dep_x_i num_x_| nn_logp_i nn_logp_summationdisplay amod_data_unlabeled prep_of_entropy_data amod_entropy_conditional det_entropy_the dobj_minimizing_entropy mark_minimizing_while amod_data_labeled prep_of_likelihood_data amod_likelihood_conditional det_likelihood_the advcl_maximize_minimizing dobj_maximize_likelihood aux_maximize_to parataxis_aims_result xcomp_aims_maximize nsubj_aims_regularization prep_for_aims_example num_Jiao_2006 nn_Jiao_al. nn_Jiao_et dep_Grandvalet_Jiao conj_and_Grandvalet_2004 conj_and_Grandvalet_Bengio appos_regularization_2004 appos_regularization_Bengio appos_regularization_Grandvalet amod_regularization_entropy amod_regularization_minimum
I08-2124	P06-1027	o	Pattern-based IE approaches employ seed data to learn useful patterns to pinpoint required fields values -LRB- e.g. Ravichandran and Hovy 2002 Mann and Yarowsky 2005 Feng et al. 2006 -RRB-	num_Feng_2006 nn_Feng_al. nn_Feng_et dep_Mann_Feng num_Mann_2005 conj_and_Mann_Yarowsky num_Hovy_2002 dep_Ravichandran_Yarowsky dep_Ravichandran_Mann conj_and_Ravichandran_Hovy nn_Ravichandran_e.g. nn_values_fields amod_values_required dep_pinpoint_Hovy dep_pinpoint_Ravichandran dobj_pinpoint_values aux_pinpoint_to amod_patterns_useful xcomp_learn_pinpoint dobj_learn_patterns aux_learn_to nn_data_seed vmod_employ_learn dobj_employ_data nsubj_employ_approaches amod_approaches_IE amod_approaches_Pattern-based
I08-2124	P06-1027	o	Reported work includes improved model variants -LRB- e.g. Jiao et al. 2006 -RRB- and applications such as web data extraction -LRB- Pinto et al. 2003 -RRB- scientific citation extraction -LRB- Peng and McCallum 2004 -RRB- word alignment -LRB- Blunsom and Cohn 2006 -RRB- and discourselevel chunking -LRB- Feng et al. 2007 -RRB-	amod_Feng_2007 dep_Feng_al. nn_Feng_et dep_chunking_Feng nn_chunking_discourselevel dep_Blunsom_2006 conj_and_Blunsom_Cohn appos_alignment_Cohn appos_alignment_Blunsom nn_alignment_word dep_Peng_2004 conj_and_Peng_McCallum dep_extraction_McCallum dep_extraction_Peng nn_extraction_citation amod_extraction_scientific amod_Pinto_2003 dep_Pinto_al. nn_Pinto_et conj_and_extraction_chunking conj_and_extraction_alignment conj_and_extraction_extraction dep_extraction_Pinto nn_extraction_data nn_extraction_web prep_such_as_applications_chunking prep_such_as_applications_alignment prep_such_as_applications_extraction prep_such_as_applications_extraction nn_al._et nn_al._Jiao dep_e.g._2006 dep_e.g._al. conj_and_variants_applications dep_variants_e.g. nn_variants_model amod_variants_improved dobj_includes_applications dobj_includes_variants nsubj_includes_work amod_work_Reported
P08-1099	P06-1027	o	High values of fall into the minimal entropy trap while low values ofhave no effect on the model -LRB- see -LRB- Jiao et al. 2006 -RRB- for an example -RRB-	det_example_an amod_Jiao_2006 dep_Jiao_al. nn_Jiao_et prep_for_see_example dep_see_Jiao det_model_the neg_effect_no dep_ofhave_see prep_on_ofhave_model dobj_ofhave_effect nsubj_ofhave_values mark_ofhave_while amod_values_low nn_trap_entropy amod_trap_minimal det_trap_the advcl_values_ofhave prep_into_values_trap prep_of_values_fall amod_values_High
W09-2208	P06-1027	o	Jiao et al. propose semi-supervised conditional random fields -LRB- Jiao et al. 2006 -RRB- that try to maximize the conditional log-likelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data	amod_data_unlabeled det_data_the nn_labels_class det_labels_the prep_on_entropy_data prep_of_entropy_labels amod_entropy_conditional det_entropy_the dobj_minimize_entropy advmod_minimize_simultaneously nn_data_training det_data_the prep_on_log-likelihood_data amod_log-likelihood_conditional det_log-likelihood_the conj_and_maximize_minimize dobj_maximize_log-likelihood aux_maximize_to xcomp_try_minimize xcomp_try_maximize nsubj_try_that amod_Jiao_2006 dep_Jiao_al. nn_Jiao_et rcmod_fields_try dep_fields_Jiao amod_fields_random amod_fields_conditional amod_fields_semi-supervised dobj_propose_fields nsubj_propose_al. nn_al._et nn_al._Jiao
D07-1083	P06-1028	o	-LRB- Suzuki et al. 2006 -RRB- 88.02 -LRB- +0.82 -RRB- + unlabeled data -LRB- 17M 27M words -RRB- 88.41 -LRB- +0.39 -RRB- + supplied gazetters 88.90 -LRB- +0.49 -RRB- + add dev	dobj_add_dev appos_gazetters_+0.49 num_gazetters_88.90 amod_gazetters_supplied conj_+_88.41_gazetters dep_88.41_+0.39 nn_words_27M nn_words_17M dep_data_gazetters dep_data_88.41 appos_data_words amod_data_unlabeled conj_+_Suzuki_add conj_+_Suzuki_data dep_Suzuki_+0.82 num_Suzuki_88.02 amod_Suzuki_2006 dep_Suzuki_al. nn_Suzuki_et dep_''_add dep_''_data dep_''_Suzuki
D07-1083	P06-1028	o	-LRB- Suzuki et al. 2006 -RRB- 94.36 -LRB- +0.06 -RRB- Table 8 The HySOL performance with the F-score optimization technique on Chunking -LRB- CoNLL-2000 -RRB- experiments from unlabeled data appear different from each other	det_other_each prep_from_different_other acomp_appear_different nsubj_appear_Suzuki amod_data_unlabeled prep_from_experiments_data nn_experiments_Chunking appos_Chunking_CoNLL-2000 prep_on_technique_experiments nn_technique_optimization nn_technique_F-score det_technique_the prep_with_performance_technique nn_performance_HySOL det_performance_The num_Table_8 dep_Suzuki_performance dep_Suzuki_Table dep_Suzuki_+0.06 num_Suzuki_94.36 amod_Suzuki_2006 dep_Suzuki_al. nn_Suzuki_et
D07-1083	P06-1028	o	5.5 Applying F-score Optimization Technique In addition we can simply apply the F-score optimization technique for the sequence labeling tasks proposed in -LRB- Suzuki et al. 2006 -RRB- to boost the HySOL performance since the base discriminative models pD -LRB- y | x -RRB- and discriminative combination namely Equation -LRB- 3 -RRB- in our hybrid model basically uses the same optimization procedure as CRFs	nn_procedure_optimization amod_procedure_same det_procedure_the prep_as_uses_CRFs dobj_uses_procedure advmod_uses_basically nsubj_uses_Equation advmod_uses_namely nn_model_hybrid poss_model_our prep_in_Equation_model appos_Equation_3 amod_combination_discriminative num_x_| nn_x_y conj_and_pD_combination appos_pD_x dep_models_combination dep_models_pD amod_models_discriminative nn_models_base det_models_the nn_performance_HySOL det_performance_the parataxis_boost_uses prep_since_boost_models dobj_boost_performance aux_boost_to amod_Suzuki_2006 dep_Suzuki_al. nn_Suzuki_et xcomp_proposed_boost prep_in_proposed_Suzuki vmod_tasks_proposed nn_tasks_labeling nn_tasks_sequence det_tasks_the prep_for_technique_tasks nn_technique_optimization nn_technique_F-score det_technique_the dobj_apply_technique advmod_apply_simply aux_apply_can nsubj_apply_we ccomp_apply_5.5 nn_Technique_Optimization nn_Technique_F-score prep_in_Applying_addition dobj_Applying_Technique vmod_5.5_Applying
P07-1093	P06-1028	o	More specialized methods also exist for example for support vector machines -LRB- Musicant et al. 2003 -RRB- and for conditional random fields -LRB- Gross et al. 2007 Suzuki et al. 2006 -RRB-	num_Suzuki_2006 nn_Suzuki_al. nn_Suzuki_et dep_Gross_Suzuki amod_Gross_2007 dep_Gross_al. nn_Gross_et amod_fields_random amod_fields_conditional dep_for_Gross pobj_for_fields amod_Musicant_2003 dep_Musicant_al. nn_Musicant_et nn_machines_vector nn_machines_support prep_for_example_machines conj_and_exist_for dep_exist_Musicant prep_for_exist_example advmod_exist_also nsubj_exist_methods amod_methods_specialized advmod_specialized_More
W08-0303	P06-1028	o	We follow -LRB- Gao et al. 2006 Suzuki et al. 2006 -RRB- and approximate the metrics using the sigmoid function	amod_function_sigmoid det_function_the dobj_using_function vmod_metrics_using det_metrics_the amod_metrics_approximate num_Suzuki_2006 nn_Suzuki_al. nn_Suzuki_et dep_Gao_Suzuki dep_Gao_2006 dep_Gao_al. nn_Gao_et conj_and_follow_metrics dep_follow_Gao nsubj_follow_We
D07-1032	P06-1053	o	Dubey et al. proposed an unlexicalized PCFG parser that modied PCFG probabilities to condition the existence of syntactic parallelism -LRB- Dubey et al. 2006 -RRB-	amod_Dubey_2006 dep_Dubey_al. nn_Dubey_et amod_parallelism_syntactic dep_existence_Dubey prep_of_existence_parallelism det_existence_the nn_probabilities_PCFG dep_modied_existence prep_to_modied_condition dobj_modied_probabilities nsubj_modied_that rcmod_parser_modied nn_parser_PCFG amod_parser_unlexicalized det_parser_an dobj_proposed_parser nsubj_proposed_al. nn_al._et nn_al._Dubey
W06-1637	P06-1053	o	The results have demonstrated the existence of priming effects in corpus data they occur for specific syntactic constructions -LRB- Gries 2005 Szmrecsanyi 2005 -RRB- consistent with the experimental literature but also generalize to syntactic rules across the board which repeated more often than expected by chance -LRB- Reitter et al. 2006b Dubey et al. 2006 -RRB-	num_Dubey_2006 nn_Dubey_al. nn_Dubey_et dep_Reitter_Dubey appos_Reitter_2006b dep_Reitter_al. nn_Reitter_et prep_by_expected_chance mark_expected_than advcl_often_expected advmod_often_more advmod_repeated_often nsubj_repeated_which det_board_the rcmod_rules_repeated prep_across_rules_board amod_rules_syntactic prep_to_generalize_rules advmod_generalize_also nsubj_generalize_they amod_literature_experimental det_literature_the prep_with_consistent_literature nsubj_consistent_they dep_Szmrecsanyi_2005 dep_Gries_Szmrecsanyi appos_Gries_2005 appos_constructions_Gries amod_constructions_syntactic amod_constructions_specific dep_occur_Reitter conj_but_occur_generalize conj_but_occur_consistent prep_for_occur_constructions nsubj_occur_they nn_data_corpus prep_in_effects_data amod_effects_priming prep_of_existence_effects det_existence_the parataxis_demonstrated_generalize parataxis_demonstrated_consistent parataxis_demonstrated_occur dobj_demonstrated_existence aux_demonstrated_have nsubj_demonstrated_results det_results_The
C08-1001	P06-1079	o	The other intriguing issue is how our anchor-based method for shared argument identification can benefit from recent advances in coreference and zero-anaphora resolution -LRB- Iida et al. 2006 Komachi et al. 2007 etc. -RRB-	amod_Komachi_etc. num_Komachi_2007 nn_Komachi_al. nn_Komachi_et dep_Iida_Komachi appos_Iida_2006 dep_Iida_al. nn_Iida_et nn_resolution_zero-anaphora conj_and_coreference_resolution prep_in_advances_resolution prep_in_advances_coreference amod_advances_recent prep_from_benefit_advances aux_benefit_can nsubj_benefit_method advmod_benefit_how nn_identification_argument amod_identification_shared prep_for_method_identification amod_method_anchor-based poss_method_our dep_is_Iida ccomp_is_benefit nsubj_is_issue amod_issue_intriguing amod_issue_other det_issue_The
C08-1121	P06-1079	o	We follow -LRB- Yang et al. 2006 Iida et al. 2006 -RRB- in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate	amod_candidate_antecedent poss_candidate_its conj_and_pronoun_candidate det_pronoun_a dobj_covers_candidate dobj_covers_pronoun nsubj_covers_that rcmod_subtree_covers det_subtree_the dobj_using_subtree vmod_information_using amod_information_structural dobj_represent_information aux_represent_to nn_kernel_tree det_kernel_a vmod_using_represent dobj_using_kernel dep_al._2006 nn_al._et nn_al._Iida dep_Yang_al. dep_Yang_2006 dep_Yang_al. nn_Yang_et prepc_in_follow_using dobj_follow_Yang nsubj_follow_We
D08-1055	P06-1079	o	There have been many studies of zero-pronoun identification -LRB- Walker et al. 1994 -RRB- -LRB- Nakaiwa 1997 -RRB- -LRB- Iida et al. 2006 -RRB-	amod_Iida_2006 dep_Iida_al. nn_Iida_et dep_Nakaiwa_1997 dep_Walker_Iida dep_Walker_Nakaiwa dep_Walker_1994 dep_Walker_al. nn_Walker_et nn_identification_zero-pronoun dep_studies_Walker prep_of_studies_identification amod_studies_many cop_studies_been aux_studies_have expl_studies_There
D08-1055	P06-1079	o	We divided these case roles into four types by location in the article as in -LRB- Iida et al. 2006 -RRB- i -RRB- the case role depends on the predicate or the predicate depends on the case role in the intra-sentence -LRB- dependency relations -RRB- ii -RRB- the case role does not depend on the predicate and the predicate does not depend on the case role in the intra-sentence -LRB- zeroanaphoric -LRB- intra-sentential -RRB- -RRB- iii -RRB- the case role is not in the sentence containing the predicate -LRB- zeroanaphoric -LRB- inter-sentential -RRB- -RRB- and iv -RRB- the case role and the predicate are in the same phrase -LRB- in same phrase -RRB-	amod_phrase_same prep_in_-LRB-_phrase amod_phrase_same det_phrase_the prep_in_are_phrase nsubj_are_predicate det_predicate_the nn_role_case det_role_the nn_role_iv appos_zeroanaphoric_inter-sentential dep_predicate_zeroanaphoric det_predicate_the dobj_containing_predicate vmod_sentence_containing det_sentence_the prep_in_is_sentence neg_is_not nsubj_is_role nn_role_case det_role_the dep_iii_intra-sentence dep_iii_the dep_iii_in dep_zeroanaphoric_intra-sentential dep_intra-sentence_zeroanaphoric dep_role_iii nn_role_case det_role_the ccomp_depend_is prep_on_depend_role neg_depend_not aux_depend_does det_predicate_the conj_and_predicate_predicate det_predicate_the ccomp_depend_depend prep_on_depend_predicate prep_on_depend_predicate neg_depend_not aux_depend_does nsubj_depend_role dep_depend_ii nn_role_case det_role_the nn_relations_dependency amod_relations_intra-sentence det_relations_the prep_in_role_relations nn_role_case det_role_the prep_on_depends_role det_predicate_the conj_or_predicate_predicate det_predicate_the prep_on_depends_predicate prep_on_depends_predicate nsubj_depends_role nn_role_case det_role_the dep_role_i amod_Iida_2006 dep_Iida_al. nn_Iida_et dep_in_Iida pcomp_as_in det_article_the prep_in_location_article num_types_four nn_roles_case det_roles_these conj_and_divided_are conj_and_divided_role conj_and_divided_depend dep_divided_depends ccomp_divided_depends prep_divided_as prep_by_divided_location prep_into_divided_types dobj_divided_roles nsubj_divided_We