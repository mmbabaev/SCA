W06-3112	N03-1017	o	Word alignment and phrase extraction We used the GIZA + + word alignment software 3 to produce initial word alignments for our miniature bilingual corpus consisting of the source French file and the English reference file and the refined word alignment strategy of -LRB- Och and Ney 2003 Koehn et al. 2003 Tiedemann 2004 -RRB- to obtain improved word and phrase alignments	nn_alignments_phrase conj_and_word_alignments amod_word_improved dobj_obtain_alignments dobj_obtain_word aux_obtain_to dep_Tiedemann_2004 num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Tiedemann conj_and_Och_Koehn conj_and_Och_2003 conj_and_Och_Ney vmod_strategy_obtain prep_of_strategy_Koehn prep_of_strategy_2003 prep_of_strategy_Ney prep_of_strategy_Och nn_strategy_alignment nn_strategy_word amod_strategy_refined det_strategy_the nn_file_reference nn_file_English det_file_the conj_and_file_file amod_file_French dep_source_file dep_source_file det_source_the prep_of_consisting_source nsubj_consisting_corpus amod_corpus_bilingual amod_corpus_miniature poss_corpus_our nn_alignments_word amod_alignments_initial prepc_for_produce_consisting dobj_produce_alignments aux_produce_to vmod_software_produce num_software_3 nn_software_alignment nn_software_word pobj_+_software conj_+_GIZA_+ det_GIZA_the dobj_used_+ dobj_used_GIZA nsubj_used_We nn_extraction_phrase conj_and_alignment_strategy rcmod_alignment_used conj_and_alignment_extraction nn_alignment_Word
W06-3113	N03-1017	p	The current state of the art is represented by the so-called phrase-based translation approach -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney dep_approach_2004 dep_approach_Ney dep_approach_Och nn_approach_translation amod_approach_phrase-based amod_approach_so-called det_approach_the agent_represented_approach auxpass_represented_is nsubjpass_represented_state det_art_the prep_of_state_art amod_state_current det_state_The
W06-3115	N03-1017	o	In a phrase-based statistical translation -LRB- Koehn et al. 2003 -RRB- a bilingual text is decomposed as K phrase translation pairs -LRB- e1 fa1 -RRB- -LRB- e2 fa2 -RRB- The input foreign sentence is segmented into phrases f K1 122 mapped into corresponding English eK1 then reordered to form the output English sentence according to a phrase alignment index mapping a In a hierarchical phrase-based translation -LRB- Chiang 2005 -RRB- translation is modeled after a weighted synchronous-CFG consisting of production rules whose right-hand side is paired -LRB- Aho and Ullman 1969 -RRB- X where X is a non-terminal and are strings of terminals and non-terminals	conj_and_terminals_non-terminals prep_of_strings_non-terminals prep_of_strings_terminals cop_strings_are conj_and_non-terminal_strings det_non-terminal_a cop_non-terminal_is nsubj_non-terminal_X advmod_non-terminal_where rcmod_X_strings rcmod_X_non-terminal dep_Aho_1969 conj_and_Aho_Ullman dep_paired_Ullman dep_paired_Aho cop_paired_is nsubj_paired_side amod_side_right-hand poss_side_whose rcmod_rules_paired nn_rules_production prep_of_consisting_rules dep_synchronous-CFG_X vmod_synchronous-CFG_consisting amod_synchronous-CFG_weighted det_synchronous-CFG_a prep_after_modeled_synchronous-CFG auxpass_modeled_is nsubjpass_modeled_translation prep_in_modeled_translation dep_Chiang_2005 appos_translation_Chiang amod_translation_phrase-based amod_translation_hierarchical det_translation_a dobj_mapping_a vmod_index_mapping nn_index_alignment nn_index_phrase det_index_a amod_sentence_English nn_sentence_output det_sentence_the dobj_form_sentence aux_form_to xcomp_reordered_form vmod_eK1_reordered advmod_eK1_then amod_eK1_English amod_eK1_corresponding pobj_mapped_index prepc_according_to_mapped_to prep_into_mapped_eK1 nsubj_mapped_122 dep_mapped_K1 dep_mapped_f dep_phrases_mapped parataxis_segmented_modeled prep_into_segmented_phrases cop_segmented_is nsubj_segmented_sentence amod_sentence_foreign nn_sentence_input det_sentence_The appos_e2_fa2 appos_e1_fa1 dep_pairs_e1 nn_pairs_translation nn_pairs_phrase nn_pairs_K parataxis_decomposed_segmented dep_decomposed_e2 prep_as_decomposed_pairs auxpass_decomposed_is nsubjpass_decomposed_text dep_decomposed_Koehn prep_in_decomposed_translation amod_text_bilingual det_text_a nn_al._et amod_Koehn_2003 dep_Koehn_al. amod_translation_statistical amod_translation_phrase-based det_translation_a ccomp_``_decomposed
W06-3115	N03-1017	o	Second phrase translation pairs are extracted from the word aligned corpus -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_corpus_Koehn amod_corpus_aligned dep_word_corpus det_word_the prep_from_extracted_word auxpass_extracted_are nsubjpass_extracted_pairs advmod_extracted_Second nn_pairs_translation nn_pairs_phrase ccomp_``_extracted
W06-3115	N03-1017	o	The decoding process is very similar to those described in -LRB- Koehn et al. 2003 -RRB- It starts from an initial empty hypothesis	amod_hypothesis_empty amod_hypothesis_initial det_hypothesis_an prep_from_starts_hypothesis nsubj_starts_It amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in vmod_those_described parataxis_similar_starts prep_to_similar_those advmod_similar_very cop_similar_is nsubj_similar_process nn_process_decoding det_process_The
W06-3115	N03-1017	o	2.3 Feature Functions Our phrase-based model uses a standard pharaoh feature functions listed as follows -LRB- Koehn et al. 2003 -RRB- Relative-count based phrase translation probabilities in both directions	det_directions_both prep_in_probabilities_directions nn_probabilities_translation nn_probabilities_phrase amod_probabilities_based nn_probabilities_Relative-count amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_follows_probabilities dep_follows_Koehn mark_follows_as dep_follows_listed dep_functions_follows nn_functions_feature nn_functions_pharaoh amod_functions_standard det_functions_a dobj_uses_functions nsubj_uses_model amod_model_phrase-based poss_model_Our rcmod_Functions_uses nn_Functions_Feature num_Functions_2.3 dep_``_Functions
W06-3115	N03-1017	o	For each differently tokenized corpus we computed word alignments by a HMM translation model -LRB- Och and Ney 2003 -RRB- and by a word alignment refinement heuristic of grow-diagfinal -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_grow-diagfinal_Koehn prep_of_heuristic_grow-diagfinal nn_heuristic_refinement nn_heuristic_alignment nn_heuristic_word det_heuristic_a pobj_by_heuristic num_Och_2003 conj_and_Och_Ney appos_model_Ney appos_model_Och nn_model_translation nn_model_HMM det_model_a nn_alignments_word conj_and_computed_by prep_by_computed_model dobj_computed_alignments nsubj_computed_we prep_for_computed_corpus amod_corpus_tokenized advmod_corpus_differently det_corpus_each
W06-3115	N03-1017	o	One is a phrase-based translation in which a phrasal unit is employed for translation -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_for_employed_translation auxpass_employed_is nsubjpass_employed_unit prep_in_employed_which amod_unit_phrasal det_unit_a dep_translation_Koehn rcmod_translation_employed amod_translation_phrase-based det_translation_a cop_translation_is nsubj_translation_One
W06-3119	N03-1017	o	138 2 Rule Generation We start with phrase translations on the parallel training data using the techniques and implementation described in -LRB- Koehn et al. 2003a -RRB-	appos_Koehn_2003a dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn vmod_techniques_described conj_and_techniques_implementation det_techniques_the dobj_using_implementation dobj_using_techniques nn_data_training amod_data_parallel det_data_the prep_on_translations_data nn_translations_phrase xcomp_start_using prep_with_start_translations nsubj_start_We rcmod_Generation_start nn_Generation_Rule num_Generation_2 num_Generation_138 dep_``_Generation
W06-3119	N03-1017	o	We use the following features for our rules sourceand target-conditioned neg-log lexical weights as described in -LRB- Koehn et al. 2003b -RRB- neg-log relative frequencies left-handside-conditioned target-phrase-conditioned source-phrase-conditioned Counters n.o. rule applications n.o. target words Flags IsPurelyLexical -LRB- i.e. contains only terminals -RRB- IsPurelyAbstract -LRB- i.e. contains only nonterminals -RRB- IsXRule -LRB- i.e. non-syntactical span -RRB- IsGlueRule 139 Penalties rareness penalty exp -LRB- 1 RuleFrequency -RRB- unbalancedness penalty | MeanTargetSourceRatio n.o. source words n.o. target words | 4 Parsing Our SynCFG rules are equivalent to a probabilistic context-free grammar and decoding is therefore an application of chart parsing	nn_parsing_chart prep_of_application_parsing det_application_an advmod_application_therefore cop_application_is nsubj_application_words conj_and_grammar_decoding amod_grammar_context-free amod_grammar_probabilistic det_grammar_a prep_to_equivalent_decoding prep_to_equivalent_grammar cop_equivalent_are nsubj_equivalent_rules amod_rules_SynCFG poss_rules_Our rcmod_Parsing_equivalent num_Parsing_4 dobj_|_Parsing nsubj_|_words nn_words_target ccomp_n.o._| vmod_words_n.o. nn_words_source nn_words_n.o. nn_words_MeanTargetSourceRatio num_words_| nn_words_penalty nn_penalty_unbalancedness num_RuleFrequency_1 parataxis_exp_application appos_exp_RuleFrequency nn_exp_penalty nn_exp_rareness num_Penalties_139 nn_Penalties_IsGlueRule amod_span_non-syntactical advmod_span_i.e. dep_IsXRule_span dep_nonterminals_exp conj_nonterminals_Penalties conj_nonterminals_IsXRule advmod_nonterminals_only dobj_contains_nonterminals dep_i.e._contains dep_IsPurelyAbstract_i.e. advmod_terminals_only dobj_contains_terminals dep_i.e._contains conj_IsPurelyLexical_IsPurelyAbstract dep_IsPurelyLexical_i.e. nn_Flags_words nn_Flags_target nn_Flags_n.o. dep_applications_IsPurelyLexical conj_applications_Flags nn_applications_rule nn_applications_n.o. dep_left-handside-conditioned_applications dep_left-handside-conditioned_Counters conj_left-handside-conditioned_source-phrase-conditioned conj_left-handside-conditioned_target-phrase-conditioned amod_frequencies_relative nn_frequencies_neg-log dep_frequencies_Koehn appos_Koehn_2003b dep_Koehn_al. nn_Koehn_et prep_in_described_frequencies mark_described_as dep_weights_left-handside-conditioned dep_weights_described amod_weights_lexical amod_weights_neg-log amod_weights_target-conditioned nn_weights_sourceand poss_rules_our amod_features_following det_features_the dep_use_weights prep_for_use_rules dobj_use_features nsubj_use_We
W06-3119	N03-1017	o	5 Results We present results that compare our system against the baseline Pharaoh implementation -LRB- Koehn et al. 2003a -RRB- and MER training scripts provided for this workshop	det_workshop_this prep_for_provided_workshop vmod_scripts_provided nn_scripts_training nn_scripts_MER appos_Koehn_2003a dep_Koehn_al. nn_Koehn_et nn_implementation_Pharaoh nn_implementation_baseline det_implementation_the poss_system_our prep_against_compare_implementation dobj_compare_system nsubj_compare_that conj_and_results_scripts dep_results_Koehn rcmod_results_compare amod_results_present dep_results_We dep_results_Results num_Results_5
W06-3119	N03-1017	o	Baseline Pharaoh with phrases extracted from IBM Model 4 training with maximum phrase length 7 and extraction method diag-growthfinal -LRB- Koehn et al. 2003a -RRB- Lex Phrase-decoder simulation using only the initial lexical rules from the phrase table all with LHS X the Glue rule and a binary reordering rule with its own reordering-feature XCat All nonterminals merged into a single X nonterminal simulation of the system Hiero -LRB- Chiang 2005 -RRB-	amod_Chiang_2005 appos_Hiero_Chiang nn_Hiero_system det_Hiero_the prep_of_simulation_Hiero npadvmod_nonterminal_X amod_nonterminal_single det_nonterminal_a prep_into_merged_nonterminal nsubj_merged_nonterminals det_nonterminals_All rcmod_XCat_merged amod_XCat_reordering-feature amod_XCat_own poss_XCat_its prep_with_rule_XCat nn_rule_reordering amod_rule_binary det_rule_a conj_and_Glue_rule dobj_Glue_rule dep_the_simulation dep_the_rule dep_the_Glue nn_X_LHS prep_with_all_X nn_table_phrase det_table_the amod_rules_lexical amod_rules_initial det_rules_the advmod_rules_only conj_using_the dobj_using_all prep_from_using_table dobj_using_rules nn_simulation_Phrase-decoder nn_simulation_Lex amod_simulation_diag-growthfinal nn_simulation_method dep_Koehn_2003a dep_Koehn_al. nn_Koehn_et dep_diag-growthfinal_Koehn nn_method_extraction conj_and_length_simulation num_length_7 nn_length_phrase nn_length_maximum num_training_4 nn_training_Model nn_training_IBM prep_with_extracted_simulation prep_with_extracted_length prep_from_extracted_training vmod_phrases_extracted dep_Pharaoh_using prep_with_Pharaoh_phrases nn_Pharaoh_Baseline poss_''_Pharaoh
W06-3119	N03-1017	o	1 Introduction Recent work in machine translation has evolved from the traditional word -LRB- Brown et al. 1993 -RRB- and phrase based -LRB- Koehn et al. 2003a -RRB- models to include hierarchical phrase models -LRB- Chiang 2005 -RRB- and bilingual synchronous grammars -LRB- Melamed 2004 -RRB-	amod_Melamed_2004 dep_grammars_Melamed amod_grammars_synchronous amod_grammars_bilingual amod_Chiang_2005 conj_and_models_grammars dep_models_Chiang nn_models_phrase amod_models_hierarchical dobj_include_grammars dobj_include_models aux_include_to dep_models_Koehn amod_models_based nn_models_phrase nn_al._et dep_Koehn_2003a dep_Koehn_al. conj_al._1993 nn_al._et vmod_Brown_include conj_and_Brown_models dep_Brown_al. dep_word_models dep_word_Brown amod_word_traditional det_word_the prep_from_evolved_word aux_evolved_has nsubj_evolved_work nn_translation_machine prep_in_work_translation amod_work_Recent nn_work_Introduction num_work_1 ccomp_``_evolved
W06-3119	N03-1017	o	The hierarchical translation operations introduced in these methods call for extensions to the traditional beam decoder -LRB- Koehn et al. 2003a -RRB-	appos_Koehn_2003a dep_Koehn_al. nn_Koehn_et dep_decoder_Koehn nn_decoder_beam amod_decoder_traditional det_decoder_the prep_to_extensions_decoder prep_for_call_extensions nsubj_call_operations det_methods_these prep_in_introduced_methods vmod_operations_introduced nn_operations_translation amod_operations_hierarchical det_operations_The
W06-3120	N03-1017	o	The huge increase in computational and storage cost of including longer phrases does not provide a signi cant improvement in quality -LRB- Koehn et al. 2003 -RRB- as the probability of reappearance of larger phrases decreases	nsubj_decreases_probability mark_decreases_as amod_phrases_larger prep_of_reappearance_phrases prep_of_probability_reappearance det_probability_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_improvement_quality amod_improvement_cant nn_improvement_signi det_improvement_a advcl_provide_decreases dep_provide_Koehn dobj_provide_improvement neg_provide_not aux_provide_does nsubj_provide_increase amod_phrases_longer pobj_including_phrases prepc_of_cost_including amod_cost_storage amod_cost_computational conj_and_computational_storage prep_in_increase_cost amod_increase_huge det_increase_The
W06-3121	N03-1017	o	We generated for each phrase pair in the translation table 5 features phrase translation probability -LRB- both directions -RRB- lexical weighting -LRB- Koehn et al. 2003 -RRB- -LRB- both directions -RRB- and phrase penalty -LRB- constant value -RRB-	amod_value_constant appos_penalty_value nn_penalty_phrase preconj_directions_both dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_weighting_penalty appos_weighting_directions dep_weighting_Koehn amod_weighting_lexical det_directions_both appos_probability_penalty appos_probability_weighting appos_probability_directions nn_probability_translation nn_probability_phrase num_features_5 dep_table_features nn_table_translation det_table_the nn_pair_phrase det_pair_each dep_generated_probability prep_in_generated_table prep_for_generated_pair nsubj_generated_We ccomp_``_generated
W06-3122	N03-1017	o	It generates a vector of 5 numeric values for each phrase pair phrase translation probability -LRB- f | e -RRB- = count -LRB- f e -RRB- count -LRB- e -RRB- -LRB- e | f -RRB- = count -LRB- f e -RRB- count -LRB- f -RRB- 2http / / www.phramer.org / Java-based open-source phrase based SMT system 3http / / www.isi.edu/licensed-sw/carmel/ 4http / / www.speech.sri.com/projects/srilm/ 5http / / www.iccs.inf.ed.ac.uk/pkoehn/training.tgz 150 lexical weighting -LRB- Koehn et al. 2003 -RRB- lex -LRB- f | e a -RRB- = nproductdisplay i = 1 1 | -LCB- j | -LRB- i j -RRB- a -RCB- | summationdisplay -LRB- i j -RRB- a w -LRB- fi | ej -RRB- lex -LRB- e | f a -RRB- = mproductdisplay j = 1 1 | -LCB- i | -LRB- i j -RRB- a -RCB- | summationdisplay -LRB- i j -RRB- a w -LRB- ej | fi -RRB- phrase penalty -LRB- f | e -RRB- = e log -LRB- -LRB- f | e -RRB- -RRB- = 1 2.2 Decoding We used the Pharaoh decoder for both the Minimum Error Rate Training -LRB- Och 2003 -RRB- and test dataset decoding	nn_decoding_dataset nn_decoding_test amod_Och_2003 conj_and_Training_decoding dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum det_Training_the preconj_Training_both nn_decoder_Pharaoh det_decoder_the prep_for_used_decoding prep_for_used_Training dobj_used_decoder nsubj_used_We rcmod_Decoding_used num_Decoding_2.2 number_2.2_1 dep_=_Decoding dep_=_log dep_e_| dep_e_f dep_log_e dep_=_e dep_=_e nsubj_=_| dep_=_f parataxis_penalty_= dep_penalty_= nn_penalty_phrase dep_penalty_fi nn_penalty_ej num_fi_| dep_w_penalty det_w_a appos_i_j dep_summationdisplay_w dep_summationdisplay_i num_summationdisplay_| det_summationdisplay_a appos_i_j dobj_|_summationdisplay dep_|_i nn_|_i dep_|_| num_|_1 number_1_1 dep_=_| nsubj_=_j nn_j_mproductdisplay dep_=_= dep_=_a advmod_=_f dep_=_e dep_=_lex dep_=_w nn_f_| nn_lex_fi num_ej_| appos_fi_ej det_w_a appos_i_j amod_summationdisplay_= dep_summationdisplay_i num_summationdisplay_| det_summationdisplay_a amod_summationdisplay_| appos_i_j dep_|_i nn_|_j num_|_| num_|_1 number_1_1 dobj_=_summationdisplay advmod_=_i nsubj_=_nproductdisplay dep_=_= det_=_a dep_=_e advmod_=_| nn_|_f dep_lex_= amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_weighting_lex appos_weighting_Koehn amod_weighting_lexical num_weighting_150 nn_weighting_www.iccs.inf.ed.ac.uk/pkoehn/training.tgz amod_5http_www.speech.sri.com/projects/srilm/ amod_4http_www.isi.edu/licensed-sw/carmel/ num_system_3http nn_system_SMT amod_system_based dep_phrase_system amod_phrase_open-source amod_phrase_Java-based nn_phrase_www.phramer.org dep_2http_weighting dep_2http_5http dep_2http_4http dep_2http_phrase dep_2http_f nn_2http_count dep_count_e dep_count_f nn_count_count dobj_=_2http dep_=_f dep_=_| dep_=_e conj_count_= appos_count_e dep_e_count dep_f_e dep_count_f dep_=_count dep_=_e dep_=_| dep_f_= dep_probability_f nn_probability_translation nn_probability_phrase nn_pair_phrase det_pair_each amod_values_numeric num_values_5 prep_for_vector_pair prep_of_vector_values det_vector_a dep_generates_probability dobj_generates_vector nsubj_generates_It
W06-3123	N03-1017	o	For the future the joint model would benefit from lexical weighting like that used in the standard model -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_model_standard det_model_the prep_in_used_model vmod_that_used prep_like_weighting_that amod_weighting_lexical dep_benefit_Koehn prep_from_benefit_weighting aux_benefit_would nsubj_benefit_model prep_for_benefit_future amod_model_joint det_model_the det_future_the
W06-3123	N03-1017	o	154 2 Translation Models 2.1 Standard Phrase-based Model Most phrase-based translation models -LRB- Och 2003 Koehn et al. 2003 Vogel et al. 2003 -RRB- rely on a pre-existing set of word-based alignments from which they induce their parameters	poss_parameters_their dobj_induce_parameters nsubj_induce_they prep_from_induce_which rcmod_alignments_induce amod_alignments_word-based prep_of_set_alignments amod_set_pre-existing det_set_a prep_on_rely_set nsubj_rely_models num_Vogel_2003 nn_Vogel_al. nn_Vogel_et num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Vogel conj_Och_Koehn appos_Och_2003 appos_models_Och nn_models_translation amod_models_phrase-based amod_models_Most nn_models_Model amod_models_Phrase-based amod_models_Standard num_models_2.1 rcmod_Models_rely nn_Models_Translation num_Models_2 num_Models_154 dep_``_Models
W06-3123	N03-1017	o	On smaller data sets -LRB- Koehn et al. 2003 -RRB- the joint model shows performance comparable to the standard model however the joint model does not reach the level of performance of the stan156 EN-ES ES-EN Joint 3-gram dl4 20.51 26.64 5-gram dl6 26.34 27.17 + lex	conj_+_27.17_lex number_27.17_26.34 dep_dl6_lex dep_dl6_27.17 num_5-gram_26.64 nn_5-gram_dl4 number_26.64_20.51 appos_3-gram_dl6 appos_3-gram_5-gram nn_3-gram_Joint nn_3-gram_ES-EN nn_3-gram_EN-ES nn_3-gram_stan156 det_3-gram_the prep_of_performance_3-gram prep_of_level_performance det_level_the dobj_reach_level neg_reach_not aux_reach_does nsubj_reach_model advmod_reach_however amod_model_joint det_model_the amod_model_standard det_model_the prep_to_comparable_model amod_performance_comparable parataxis_shows_reach dobj_shows_performance nsubj_shows_model prep_on_shows_sets amod_model_joint det_model_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_sets_Koehn nn_sets_data amod_sets_smaller
W06-3125	N03-1017	p	This translation model differs from the well known phrase-based translation approach -LRB- Koehn et al. 2003 -RRB- in two basic issues rst training data is monotonously segmented into bilingual units and second the model considers n-gram probabilities instead of relative frequencies	amod_frequencies_relative prep_instead_of_probabilities_frequencies amod_probabilities_n-gram dobj_considers_probabilities nsubj_considers_model advmod_considers_second det_model_the amod_units_bilingual conj_and_segmented_considers prep_into_segmented_units advmod_segmented_monotonously cop_segmented_is nsubj_segmented_rst nn_data_training conj_rst_data dep_issues_considers dep_issues_segmented amod_issues_basic num_issues_two amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_approach_Koehn nn_approach_translation amod_approach_phrase-based amod_approach_known det_approach_the advmod_known_well prep_in_differs_issues prep_from_differs_approach nsubj_differs_model nn_model_translation det_model_This
W06-3601	N03-1017	n	2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- are good at learning local translations that are pairs of -LRB- consecutive -RRB- sub-strings but often insufficient in modeling the reorderings of phrases themselves especially between language pairs with very different word-order	amod_word-order_different advmod_different_very nn_pairs_language npadvmod_phrases_themselves prep_with_reorderings_word-order prep_between_reorderings_pairs advmod_reorderings_especially prep_of_reorderings_phrases det_reorderings_the dep_insufficient_reorderings prep_in_insufficient_modeling advmod_insufficient_often nsubj_insufficient_Koehn dep_sub-strings_consecutive prep_of_pairs_sub-strings cop_pairs_are nsubj_pairs_that rcmod_translations_pairs amod_translations_local dobj_learning_translations conj_but_good_insufficient prepc_at_good_learning cop_good_are nsubj_good_Koehn dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_models_Phrase-based nn_models_MT. amod_models_statistical prep_in_efforts_models amod_efforts_recent det_approach_this dep_compare_insufficient dep_compare_good prep_with_compare_efforts dobj_compare_approach aux_compare_to xcomp_helpful_compare cop_helpful_is nsubj_helpful_It rcmod_Work_helpful amod_Work_Previous num_Work_2
W07-0403	N03-1017	o	4.1 Translation Modeling We can test our models utility for translation by transforming its parameters into a phrase table for the phrasal decoder Pharaoh -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_Pharaoh_decoder amod_Pharaoh_phrasal det_Pharaoh_the prep_for_table_Pharaoh nn_table_phrase det_table_a poss_parameters_its prep_into_transforming_table dobj_transforming_parameters nn_utility_models poss_utility_our prepc_by_test_transforming prep_for_test_translation dobj_test_utility aux_test_can nsubj_test_We ccomp_Modeling_test dep_Translation_Koehn vmod_Translation_Modeling num_Translation_4.1 dep_``_Translation
W07-0403	N03-1017	o	Pharaoh also includes lexical weighting parameters that are derived from the alignments used to induce its phrase pairs -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_pairs_phrase poss_pairs_its dobj_induce_pairs aux_induce_to xcomp_used_induce vmod_alignments_used det_alignments_the prep_from_derived_alignments auxpass_derived_are nsubjpass_derived_that rcmod_parameters_derived nn_parameters_weighting amod_parameters_lexical dep_includes_Koehn dobj_includes_parameters advmod_includes_also nsubj_includes_Pharaoh ccomp_``_includes
W07-0403	N03-1017	o	2 Background 2.1 Phrase Table Extraction Phrasal decoders require a phrase table -LRB- Koehn et al. 2003 -RRB- which contains bilingual phrase pairs and 17 scores indicating their utility	poss_utility_their dobj_indicating_utility vmod_scores_indicating num_scores_17 conj_and_pairs_scores nn_pairs_phrase amod_pairs_bilingual dobj_contains_scores dobj_contains_pairs nsubj_contains_which dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et rcmod_table_contains appos_table_Koehn nn_table_phrase det_table_a dobj_require_table nsubj_require_decoders nn_decoders_Phrasal nn_decoders_Extraction nn_decoders_Table nn_decoders_Phrase num_decoders_2.1 nn_decoders_Background num_decoders_2 ccomp_``_require
W07-0403	N03-1017	o	Two are conditionalized phrasal models each EM trained until performance degrades C-JPTM3 as described in -LRB- Birch et al. 2006 -RRB- Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic GIZA + + with grow-diag-final -LRB- GDF -RRB- Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder -LRB- Koehn et al. 2003 -RRB- with the SMT Shared Task baseline system -LRB- Koehn and Monz 2006 -RRB-	amod_Koehn_2006 conj_and_Koehn_Monz dep_system_Monz dep_system_Koehn nn_system_baseline nn_system_Task dobj_Shared_system nsubj_Shared_SMT mark_Shared_with det_SMT_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_decoder_Koehn nn_decoder_Pharaoh det_decoder_the advcl_use_Shared dobj_use_decoder nsubj_use_We rcmod_constraint_use amod_constraint_noncompositional det_constraint_the nn_ITG_Phrasal nn_ITG_Viterbi amod_ITG_grow-diag-final dep_grow-diag-final_GDF prep_without_with_constraint prep_with_with_constraint conj_and_with_with prep_with_+_ITG conj_+_GIZA_+ nn_heuristic_surface det_heuristic_the dep_provide_+ dep_provide_GIZA prep_for_provide_heuristic dobj_provide_alignments nsubj_provide_C-JPTM3 num_Section_Three num_Section_4.1 prep_in_described_Section mark_described_as nn_ITG_Phrasal dep_ITG_Birch amod_Birch_2006 dep_Birch_al. nn_Birch_et advcl_described_described prep_in_described_ITG mark_described_as dep_C-JPTM3_described nsubj_degrades_performance mark_degrades_until advcl_trained_degrades vmod_EM_trained det_EM_each dep_models_provide appos_models_EM amod_models_phrasal dobj_conditionalized_models auxpass_conditionalized_are nsubjpass_conditionalized_Two ccomp_``_conditionalized
W07-0403	N03-1017	o	It extracts all consistent phrase pairs from word-aligned bitext -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_bitext_Koehn amod_bitext_word-aligned nn_pairs_phrase amod_pairs_consistent det_pairs_all prep_from_extracts_bitext dobj_extracts_pairs nsubj_extracts_It ccomp_``_extracts
W07-0403	N03-1017	o	The grow-diag-final -LRB- GDF -RRB- combination heuristic -LRB- Koehn et al. 2003 -RRB- adds links so that each new link connects a previously unlinked token	amod_token_unlinked det_token_a advmod_unlinked_previously dobj_connects_token nsubj_connects_link mark_connects_that mark_connects_so amod_link_new det_link_each advcl_links_connects dobj_adds_links nsubj_adds_heuristic amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn nn_heuristic_combination amod_heuristic_grow-diag-final det_heuristic_The dep_grow-diag-final_GDF
W07-0406	N03-1017	p	However attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success -LRB- Koehn et al. 2003 Och et al. 2003 -RRB- 1 and purely phrase-based machine translation systems continue to outperform these syntax/phrase-based hybrids	amod_hybrids_syntax/phrase-based det_hybrids_these dobj_outperform_hybrids aux_outperform_to xcomp_continue_outperform nsubj_continue_systems nsubj_continue_Koehn nn_systems_translation nn_systems_machine amod_systems_phrase-based advmod_phrase-based_purely num_Och_2003 nn_Och_al. nn_Och_et conj_and_Koehn_systems num_Koehn_1 dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_success_enormous ccomp_met_continue prep_with_met_success neg_met_not aux_met_have amod_paradigm_phrase-based det_paradigm_the amod_information_syntactic prep_into_retrofit_paradigm dobj_retrofit_information aux_retrofit_to vmod_attempts_met xcomp_attempts_retrofit advmod_attempts_However
W07-0409	N03-1017	p	1 Introduction Recent works in statistical machine translation -LRB- SMT -RRB- shows how phrase-based modeling -LRB- Och and Ney 2000a Koehn et al. 2003 -RRB- significantly outperform the historical word-based modeling -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_modeling_al. amod_modeling_word-based amod_modeling_historical det_modeling_the dobj_outperform_modeling advmod_outperform_significantly nsubj_outperform_modeling num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2000a conj_and_Och_Ney dep_modeling_2000a dep_modeling_Ney dep_modeling_Och amod_modeling_phrase-based advmod_modeling_how ccomp_shows_outperform nsubj_shows_works appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_works_translation amod_works_Recent nn_works_Introduction num_works_1
W07-0412	N03-1017	p	And again we see this insight informing statistical machine translation systems for instance in the phrase-based approaches of Och -LRB- 2003 -RRB- and Koehn et al.	nn_al._et nn_al._Koehn conj_and_Och_al. appos_Och_2003 prep_of_approaches_al. prep_of_approaches_Och amod_approaches_phrase-based det_approaches_the nn_systems_translation nn_systems_machine amod_systems_statistical dobj_informing_systems vmod_insight_informing det_insight_this prep_in_see_approaches prep_for_see_instance dobj_see_insight nsubj_see_we advmod_see_again cc_see_And
W07-0701	N03-1017	o	We compared our system to Pharaoh a leading phrasal SMT decoder -LRB- Koehn et al. 2003 -RRB- and our treelet system	amod_system_treelet poss_system_our amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_decoder_Koehn nn_decoder_SMT amod_decoder_phrasal amod_decoder_leading det_decoder_a conj_and_Pharaoh_system appos_Pharaoh_decoder poss_system_our prep_to_compared_system prep_to_compared_Pharaoh dobj_compared_system nsubj_compared_We ccomp_``_compared
W07-0701	N03-1017	p	1 Introduction Modern phrasal SMT systems such as -LRB- Koehn et al. 2003 -RRB- derive much of their power from being able to memorize and use long phrases	amod_phrases_long dobj_use_phrases conj_and_memorize_use aux_memorize_to xcomp_able_use xcomp_able_memorize cop_able_being poss_power_their prep_of_much_power prepc_from_derive_able dobj_derive_much nsubj_derive_systems amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_such_as_systems_Koehn nn_systems_SMT amod_systems_phrasal nn_systems_Modern nn_systems_Introduction num_systems_1
W07-0703	N03-1017	o	Portage is a statistical phrase-based SMT system similar to Pharaoh -LRB- Koehn et al 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al nn_Koehn_et prep_to_similar_Pharaoh dep_system_Koehn amod_system_similar nn_system_SMT amod_system_phrase-based amod_system_statistical det_system_a cop_system_is nsubj_system_Portage
W07-0703	N03-1017	o	To generate phrase pairs from a parallel corpus we use the diag-and phrase induction algorithm described in -LRB- Koehn et al 2003 -RRB- with symmetrized word alignments generated using IBM model 2 -LRB- Brown et al 1993 -RRB-	dep_al_1993 nn_al_et amod_al_Brown dep_model_al num_model_2 nn_model_IBM dobj_using_model xcomp_generated_using vmod_alignments_generated nn_alignments_word amod_alignments_symmetrized prep_with_Koehn_alignments amod_Koehn_2003 dep_Koehn_al nn_Koehn_et prep_in_described_Koehn vmod_algorithm_described nn_algorithm_induction nn_algorithm_phrase amod_algorithm_diag-and det_algorithm_the dobj_use_algorithm nsubj_use_we advcl_use_generate amod_corpus_parallel det_corpus_a nn_pairs_phrase prep_from_generate_corpus dobj_generate_pairs aux_generate_To
W07-0704	N03-1017	o	We employ the phrase-based SMT framework -LRB- Koehn et al. 2003 -RRB- and use the Moses toolkit -LRB- Koehn et al. 2007 -RRB- and the SRILM language modelling toolkit -LRB- Stolcke 2002 -RRB- and evaluate our decoded translations using the BLEU measure -LRB- Papineni et al. 2002 -RRB- using a single reference translation	nn_translation_reference amod_translation_single det_translation_a dobj_using_translation nsubj_using_We amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_measure_Papineni nn_measure_BLEU det_measure_the dobj_using_measure amod_translations_decoded poss_translations_our xcomp_evaluate_using dobj_evaluate_translations nsubj_evaluate_We dep_Stolcke_2002 appos_toolkit_Stolcke amod_toolkit_modelling dep_language_toolkit nn_language_SRILM det_language_the amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_toolkit_Koehn nn_toolkit_Moses det_toolkit_the dobj_use_toolkit nsubj_use_We amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_framework_SMT amod_framework_phrase-based det_framework_the conj_and_employ_using conj_and_employ_evaluate conj_and_employ_language conj_and_employ_use dep_employ_Koehn dobj_employ_framework nsubj_employ_We
W07-0709	N03-1017	o	5.1 The baseline System used for comparison was Pharaoh -LRB- Koehn et al. 2003 Koehn 2004 -RRB- which uses a beam search algorithm for decoding	prep_for_algorithm_decoding nn_algorithm_search nn_algorithm_beam det_algorithm_a dobj_uses_algorithm nsubj_uses_which dep_Koehn_2004 dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et rcmod_Pharaoh_uses dep_Pharaoh_Koehn cop_Pharaoh_was nsubj_Pharaoh_System prep_for_used_comparison vmod_System_used nn_System_baseline det_System_The num_System_5.1
W07-0711	N03-1017	o	1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_machine amod_translation_statistical amod_approaches_modern amod_approaches_most dep_step_Koehn prep_to_step_translation prep_of_step_approaches amod_step_important det_step_an cop_step_is nsubj_step_alignment nn_alignment_Word nn_alignment_Introduction num_alignment_1
W07-0715	N03-1017	o	2 Previous Approaches Koehn et al. ? s -LRB- 2003 -RRB- method of estimating phrasetranslation probabilities is very simple	advmod_simple_very cop_simple_is nsubj_simple_Koehn nn_probabilities_phrasetranslation dobj_estimating_probabilities prepc_of_method_estimating nn_method_s appos_s_2003 nn_s_al. nn_al._et appos_Koehn_method nn_Koehn_Approaches amod_Koehn_Previous num_Koehn_2
W07-0716	N03-1017	o	?? Initial phrase pairs are identified following the procedure typically employed in phrase based systems -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB-	dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_systems_based nn_systems_phrase prep_in_employed_systems advmod_employed_typically vmod_procedure_employed det_procedure_the dep_identified_Koehn prep_following_identified_procedure auxpass_identified_are nsubjpass_identified_pairs nn_pairs_phrase amod_pairs_Initial num_pairs_??
W07-0716	N03-1017	o	1 Introduction Viewed at a very high level statistical machine translationinvolvesfourphases languageandtranslation model training parameter tuning decoding and evaluation -LRB- Lopez 2007 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Lopez_Koehn appos_Lopez_2007 appos_evaluation_Lopez nn_tuning_parameter conj_and_training_evaluation conj_and_training_decoding conj_and_training_tuning nn_training_model nn_training_languageandtranslation dep_translationinvolvesfourphases_evaluation dep_translationinvolvesfourphases_decoding dep_translationinvolvesfourphases_tuning dep_translationinvolvesfourphases_training nn_translationinvolvesfourphases_machine amod_translationinvolvesfourphases_statistical dep_,_translationinvolvesfourphases amod_level_high det_level_a advmod_high_very prep_at_Viewed_level vmod_Introduction_Viewed num_Introduction_1 dep_``_Introduction
W07-0716	N03-1017	o	We use the following features in our induced English-to-English grammar :3 3Hiero also uses lexical weights -LRB- Koehn et al. 2003 -RRB- in both 122 ?? The joint probability of the two English hierarchical paraphrases conditioned on the nonterminal symbol as defined by this formula p -LRB- e1 e2 | x -RRB- = c -LRB- X ??? e1 e2 ?? summationtext e1prime e2prime c -LRB- X ??? e1prime e2prime ??	amod_??_e2prime appos_e1prime_?? num_e1prime_??? nn_e1prime_X dep_c_e1prime nn_c_e2prime nn_e1prime_summationtext nn_e1prime_?? nn_e1prime_e2 appos_e1_c conj_e1_e1prime num_e1_??? nn_e1_X nn_e1_c amod_e1_= nn_e1_p num_x_| nn_x_e2 appos_e1_x dep_p_e1 det_formula_this prep_by_defined_formula mark_defined_as amod_symbol_nonterminal det_symbol_the prep_on_conditioned_symbol amod_paraphrases_hierarchical nn_paraphrases_English num_paraphrases_two det_paraphrases_the prep_of_probability_paraphrases amod_probability_joint det_probability_The num_probability_?? preconj_probability_both number_??_122 amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_weights_lexical dobj_uses_weights advmod_uses_also nsubj_uses_3Hiero num_3Hiero_:3 rcmod_grammar_uses amod_grammar_English-to-English amod_grammar_induced poss_grammar_our amod_features_following det_features_the parataxis_use_e1 advcl_use_defined vmod_use_conditioned prep_in_use_probability dep_use_Koehn prep_in_use_grammar dobj_use_features nsubj_use_We
W07-0717	N03-1017	o	2 Phrase-based Statistical MT Our baseline is a standard phrase-based SMT system -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_system_Koehn nn_system_SMT amod_system_phrase-based amod_system_standard det_system_a cop_system_is nsubj_system_MT poss_baseline_Our dep_MT_baseline amod_MT_Statistical amod_MT_Phrase-based num_MT_2
W07-0717	N03-1017	o	The features used in this study are the length of t a single-parameter distortion penalty on phrase reordering in a as described in -LRB- Koehn et al. 2003 -RRB- phrase translation model probabilities and 4-gram language model probabilities logp -LRB- t -RRB- using Kneser-Ney smoothing as implemented in the SRILM toolkit	nn_toolkit_SRILM det_toolkit_the prep_in_implemented_toolkit mark_implemented_as nn_smoothing_Kneser-Ney advcl_using_implemented dobj_using_smoothing appos_logp_t dep_probabilities_logp nn_probabilities_model nn_probabilities_language amod_probabilities_4-gram nn_probabilities_model nn_probabilities_translation nn_probabilities_phrase amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_described_in mark_described_as nn_reordering_phrase prep_in_penalty_a prep_on_penalty_reordering nn_penalty_distortion amod_penalty_single-parameter det_penalty_a vmod_length_using conj_and_length_probabilities conj_and_length_probabilities dep_length_described dep_length_penalty prep_of_length_t det_length_the dep_are_probabilities dep_are_probabilities dep_are_length nsubj_are_features det_study_this prep_in_used_study vmod_features_used det_features_The
W07-0717	N03-1017	o	To derive the joint counts c -LRB- ? s ? t -RRB- from which p -LRB- ? s | ? t -RRB- and p -LRB- ? t | ? s -RRB- are estimated we use the phrase induction algorithm described in -LRB- Koehn et al. 2003 -RRB- with symmetrized word alignments generated using IBM model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_IBM dobj_using_model vmod_generated_using vmod_alignments_generated nn_alignments_word amod_alignments_symmetrized dep_Koehn_Brown prep_with_Koehn_alignments amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn vmod_algorithm_described nn_algorithm_induction nn_algorithm_phrase det_algorithm_the dobj_use_algorithm nsubj_use_we advcl_use_derive auxpass_estimated_are nsubjpass_estimated_s nsubjpass_estimated_s prep_from_estimated_which nn_s_| nn_s_t nn_s_p conj_and_s_s dep_s_t num_s_| nn_s_p dep_s_t rcmod_c_estimated dep_c_s nn_c_counts amod_c_joint det_c_the dobj_derive_c aux_derive_To
W07-0719	N03-1017	o	159 2.1 Baseline System The baseline system is a phrase-based SMT system -LRB- Koehn et al. 2003 -RRB- built almost entirely using freely available components	amod_components_available advmod_available_freely dobj_using_components advmod_using_entirely advmod_entirely_almost xcomp_built_using amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et vmod_system_built dep_system_Koehn nn_system_SMT amod_system_phrase-based det_system_a cop_system_is nsubj_system_system nn_system_baseline nn_system_The nn_system_System nn_system_Baseline num_system_2.1 num_system_159
W07-0719	N03-1017	n	1 Introduction Translations tables in Phrase-based Statistical Machine Translation -LRB- SMT -RRB- are often built on the basis of Maximum-likelihood Estimation -LRB- MLE -RRB- being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_ignored_Koehn advmod_ignored_completely auxpass_ignored_is nsubjpass_ignored_sentence mark_ignored_that nsubj_occur_phrases prep_in_occur_which rcmod_sentence_occur dep_sentence_context nn_sentence_source det_sentence_the det_approach_this ccomp_limitations_ignored prep_of_limitations_approach amod_limitations_major det_limitations_the prep_of_one_limitations cop_one_being appos_Estimation_MLE amod_Estimation_Maximum-likelihood prep_of_basis_Estimation det_basis_the ccomp_built_one prep_on_built_basis advmod_built_often auxpass_built_are nsubjpass_built_tables appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical amod_Translation_Phrase-based prep_in_tables_Translation nn_tables_Translations nn_tables_Introduction num_tables_1 ccomp_``_built
W07-0721	N03-1017	o	1 Introduction Nowadays statistical machine translation is mainly based on phrases -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_on_based_phrases advmod_based_mainly auxpass_based_is nsubjpass_based_translation advmod_based_Nowadays nn_translation_machine amod_translation_statistical dep_Introduction_Koehn vmod_Introduction_based num_Introduction_1
W07-0724	N03-1017	o	They are generated from the training corpus via the ? diag-and ?? method -LRB- Koehn et al. 2003 -RRB- and smoothed using Kneser-Ney smoothing -LRB- Foster et al. 2006 -RRB- ?? one or several n-gram language model -LRB- s -RRB- trained with the SRILM toolkit -LRB- Stolcke 2002 -RRB- in the baseline experiments reported here we used a trigram model ?? a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase ?? a word penalty	nn_penalty_word det_penalty_a dobj_??_penalty nn_phrase_target amod_phrase_new det_phrase_a dobj_generating_phrase advmod_generating_when advcl_skipped_generating auxpass_skipped_are nsubjpass_skipped_which rcmod_words_skipped nn_words_source prep_of_number_words det_number_the prep_on_based_number vmod_penalty_based det_penalty_a dobj_assigns_penalty nsubj_assigns_which rcmod_model_assigns nn_model_distortion det_model_a dobj_??_model nn_model_trigram det_model_a conj_used_?? conj_used_?? dobj_used_model nsubj_used_we prep_in_used_experiments advmod_reported_here vmod_experiments_reported nn_experiments_baseline det_experiments_the amod_Stolcke_2002 appos_toolkit_Stolcke nn_toolkit_SRILM det_toolkit_the prep_with_trained_toolkit appos_model_s nn_model_language amod_model_n-gram amod_model_several vmod_one_trained conj_or_one_model num_one_?? amod_Foster_2006 dep_Foster_al. nn_Foster_et nn_smoothing_Kneser-Ney dobj_using_smoothing xcomp_smoothed_using nsubjpass_smoothed_They amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn nn_method_?? amod_method_diag-and det_method_the nn_corpus_training det_corpus_the parataxis_generated_used dobj_generated_model dobj_generated_one dep_generated_Foster conj_and_generated_smoothed prep_via_generated_method prep_from_generated_corpus auxpass_generated_are nsubjpass_generated_They
W07-0725	N03-1017	o	2 Architecture of the system The goal of statistical machine translation -LRB- SMT -RRB- is to produce a target sentence e from a source sentence f It is today common practice to use phrases as translation units -LRB- Koehn et al. 2003 Och and Ney 2003 -RRB- and a log linear framework in order to introduce several models explaining the translation process e ?? = argmaxp -LRB- e | f -RRB- = argmaxe -LCB- exp -LRB- summationdisplay i ihi -LRB- e f -RRB- -RRB- -RCB- -LRB- 1 -RRB- The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set -LRB- Och and Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_set_Ney dep_set_Och nn_set_development det_set_a prep_on_function_set amod_function_scoring det_function_a dobj_maximize_function aux_maximize_to xcomp_optimized_maximize advmod_optimized_typically auxpass_optimized_are nsubjpass_optimized_= nn_weights_i det_weights_the conj_and_models_weights nn_models_system det_models_the cop_models_are nsubj_models_summationdisplay dobj_models_exp nsubj_functions_feature det_feature_The dep_e_f nn_ihi_i rcmod_summationdisplay_functions dep_summationdisplay_1 dep_summationdisplay_e dep_summationdisplay_ihi nn_exp_argmaxe prep_=_weights prep_=_models dep_=_f dep_=_argmaxp dep_=_= dep_=_?? dep_=_e dep_f_| dep_|_e nn_process_translation det_process_the dobj_explaining_process amod_models_several dep_introduce_explaining dobj_introduce_models aux_introduce_to dep_introduce_order mark_introduce_in dep_framework_introduce amod_framework_linear nn_framework_log det_framework_a dep_Och_2003 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_units_framework appos_units_Koehn nn_units_translation prep_as_use_framework prep_as_use_units dobj_use_phrases aux_use_to amod_practice_common tmod_practice_today cop_practice_is nsubj_practice_It dep_practice_source det_practice_a dep_sentence_f amod_source_sentence prep_from_sentence_practice dep_sentence_e nn_sentence_target det_sentence_a vmod_produce_use dobj_produce_sentence aux_produce_to xcomp_is_produce nsubj_is_goal appos_translation_SMT nn_translation_machine amod_translation_statistical prep_of_goal_translation det_goal_The det_system_the dep_Architecture_optimized rcmod_Architecture_is prep_of_Architecture_system num_Architecture_2 dep_``_Architecture
W07-0731	N03-1017	o	As in phrasebased translation model estimation ? also contains two lexical weights -LRB- Koehn et al. 2003 -RRB- counters for number of target terminals generated	vmod_terminals_generated nn_terminals_target prep_of_number_terminals prep_for_counters_number amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_weights_counters dep_weights_Koehn amod_weights_lexical num_weights_two dobj_contains_weights advmod_contains_also prep_contains_As nn_estimation_model nn_estimation_translation amod_estimation_phrasebased pobj_in_estimation pcomp_As_in
W07-0731	N03-1017	o	Table 1 shows the impact of increasing reordering window length -LRB- Koehn et al. 2003 -RRB- on translation quality for the ? dev06 ?? data .2 Increasing the reordering window past 2 has minimal impact on translation quality implying that most of the reordering effects across Spanish and English are well modeled at the local or phrase level	amod_level_phrase amod_level_local det_level_the conj_or_local_phrase prep_at_modeled_level advmod_modeled_well auxpass_modeled_are nsubjpass_modeled_most mark_modeled_that conj_and_Spanish_English prep_across_effects_English prep_across_effects_Spanish nn_effects_reordering det_effects_the prep_of_most_effects ccomp_implying_modeled nn_quality_translation prep_on_impact_quality amod_impact_minimal vmod_has_implying dobj_has_impact nsubj_has_impact num_past_2 dep_window_past nn_window_reordering det_window_the dobj_Increasing_window vmod_.2_Increasing nn_.2_data num_.2_?? det_.2_the number_??_dev06 nn_quality_translation dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_length_window nn_length_reordering amod_length_increasing prep_for_impact_.2 prep_on_impact_quality dep_impact_Koehn prep_of_impact_length det_impact_the ccomp_shows_has nsubj_shows_Table num_Table_1
W07-1512	N03-1017	o	However many of these models are not applicable to parallel treebanks because they assume translation units where either the source text the target text or both are represented as word sequences without any syntactic structure -LRB- Galley et al. 2004 Marcu et al. 2006 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn num_Marcu_2006 nn_Marcu_al. nn_Marcu_et dep_Galley_Marcu appos_Galley_2004 dep_Galley_al. nn_Galley_et nn_structure_syntactic det_structure_any nn_sequences_word prep_without_represented_structure prep_as_represented_sequences auxpass_represented_are nsubjpass_represented_text advmod_represented_where conj_or_text_both nn_text_target det_text_the appos_text_both appos_text_text nn_text_source det_text_the preconj_text_either dep_units_Galley rcmod_units_represented nn_units_translation dobj_assume_units nsubj_assume_they mark_assume_because dobj_parallel_treebanks aux_parallel_to advcl_applicable_assume xcomp_applicable_parallel neg_applicable_not cop_applicable_are nsubj_applicable_many advmod_applicable_However det_models_these prep_of_many_models
W08-0301	N03-1017	o	4 Experiments 4.1 Experiment Settings A series of experiments were run to compare the performance of the three SWD models against the baseline which is the standard phrase-based approach to SMT as elaborated in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_elaborated_in mark_elaborated_as advcl_approach_elaborated prep_to_approach_SMT amod_approach_phrase-based amod_approach_standard det_approach_the cop_approach_is nsubj_approach_which rcmod_baseline_approach det_baseline_the nn_models_SWD num_models_three det_models_the prep_against_performance_baseline prep_of_performance_models det_performance_the dobj_compare_performance aux_compare_to xcomp_run_compare auxpass_run_were nsubjpass_run_series prep_of_series_experiments det_series_A nn_series_Settings nn_series_Experiment num_series_4.1 nn_series_Experiments num_series_4
W08-0301	N03-1017	o	The subsequent construction of translation table was done in exactly the same way as explained 4 in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_explained_in dobj_explained_4 mark_explained_as advcl_way_explained amod_way_same det_way_the advmod_way_exactly prep_in_done_way auxpass_done_was nsubjpass_done_construction nn_table_translation prep_of_construction_table amod_construction_subsequent det_construction_The
W08-0302	N03-1017	o	Baseline We use the Moses MT system -LRB- Koehn et al. 2007 -RRB- as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks .4 In particular we perform word alignment in each direction using GIZA + + -LRB- Och and Ney 2003 -RRB- apply the grow-diag-finaland heuristic for symmetrization and use a maximum phrase length of 7	prep_of_length_7 nn_length_phrase nn_length_maximum det_length_a dobj_use_length nsubj_use_GIZA prep_for_heuristic_symmetrization amod_heuristic_grow-diag-finaland det_heuristic_the conj_and_apply_use dobj_apply_heuristic nsubj_apply_+ nsubj_apply_GIZA num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ ccomp_using_use ccomp_using_apply det_direction_each nn_alignment_word vmod_perform_using prep_in_perform_direction dobj_perform_alignment nsubj_perform_we num_tasks_.4 parataxis_shared_perform prep_in_shared_particular dobj_shared_tasks nsubj_shared_follow nsubj_shared_Baseline conj_and_WMT-07_WMT-08 det_WMT-07_the prep_for_given_WMT-08 prep_for_given_WMT-07 vmod_procedure_given nn_procedure_training nn_procedure_example det_procedure_the dobj_follow_procedure advmod_follow_closely det_baseline_a amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et nn_system_MT nn_system_Moses det_system_the dobj_use_system nsubj_use_We conj_and_Baseline_follow prep_as_Baseline_baseline dep_Baseline_Koehn rcmod_Baseline_use
W08-0302	N03-1017	o	-LRB- 2003 -RRB- in which we translate a source-language sentence f into the target-language sentence e that maximizes a linear combination of features and weights :1 e a = argmax e a score -LRB- e a f -RRB- -LRB- 1 -RRB- = argmax e a Msummationdisplay m = 1 mhm -LRB- e a f -RRB- -LRB- 2 -RRB- where a represents the segmentation of e and f into phrases and a correspondence between phrases and each hm is a R-valued feature with learned weight m The translation is typically found using beam search -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_search_beam dobj_using_search dep_found_Koehn xcomp_found_using advmod_found_typically auxpass_found_is nsubjpass_found_translation dep_found_2 det_translation_The nn_m_weight amod_m_learned prep_with_feature_m amod_feature_R-valued det_feature_a cop_feature_is nsubj_feature_hm det_hm_each prep_between_correspondence_phrases det_correspondence_a conj_and_phrases_correspondence conj_and_e_f prep_into_segmentation_correspondence prep_into_segmentation_phrases prep_of_segmentation_f prep_of_segmentation_e det_segmentation_the conj_and_represents_feature dobj_represents_segmentation nsubj_represents_a advmod_represents_where dep_2_feature dep_2_represents dep_f_e dep_f_m dep_f_Msummationdisplay dep_f_a appos_e_a num_mhm_1 dep_=_mhm amod_m_= nn_e_argmax dep_=_e dep_=_1 dep_=_f dep_f_e dep_f_score dep_f_a dep_f_e dep_f_weights appos_e_a det_score_a nn_e_argmax dep_=_e amod_a_= num_e_:1 conj_and_features_= prep_of_combination_= prep_of_combination_features amod_combination_linear det_combination_a dobj_maximizes_combination nsubj_maximizes_that rcmod_e_maximizes dep_sentence_e amod_sentence_target-language det_sentence_the prep_into_f_sentence rcmod_sentence_found appos_sentence_f dep_sentence_f amod_sentence_source-language det_sentence_a dobj_translate_sentence nsubj_translate_we prep_in_translate_which rcmod_2003_translate dep_''_2003
W08-0302	N03-1017	p	Phrase-based MT systems are straightforward to train from parallel corpora -LRB- Koehn et al. 2003 -RRB- and like the original IBM models -LRB- Brown et al. 1990 -RRB- benefit from standard language models built on large monolingual target-language corpora -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_corpora_Brants amod_corpora_target-language appos_monolingual_corpora amod_monolingual_large prep_on_built_monolingual vmod_models_built nn_models_language amod_models_standard prep_from_benefit_models amod_Brown_1990 dep_Brown_al. nn_Brown_et nn_models_IBM amod_models_original det_models_the dep_like_Brown pobj_like_models amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_corpora_Koehn amod_corpora_parallel conj_and_train_benefit conj_and_train_like prep_from_train_corpora aux_train_to xcomp_straightforward_benefit xcomp_straightforward_like xcomp_straightforward_train cop_straightforward_are nsubj_straightforward_systems nn_systems_MT amod_systems_Phrase-based
W08-0303	N03-1017	o	For the first two tasks all heuristics of the Pharaoh-Toolkit -LRB- Koehn et al. 2003 -RRB- as well as the refined heuristic -LRB- Och and Ney 2003 -RRB- to combine both IBM4-alignments were tested and the best ones are shown in the tables	det_tables_the prep_in_shown_tables auxpass_shown_are nsubjpass_shown_ones amod_ones_best det_ones_the conj_and_tested_shown auxpass_tested_were nsubjpass_tested_heuristic nsubjpass_tested_heuristics prep_for_tested_tasks preconj_IBM4-alignments_both dobj_combine_IBM4-alignments aux_combine_to dep_Och_2003 conj_and_Och_Ney vmod_heuristic_combine appos_heuristic_Ney appos_heuristic_Och amod_heuristic_refined det_heuristic_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et det_Pharaoh-Toolkit_the conj_and_heuristics_heuristic dep_heuristics_Koehn prep_of_heuristics_Pharaoh-Toolkit det_heuristics_all num_tasks_two amod_tasks_first det_tasks_the
W08-0305	N03-1017	o	The de-facto answer came during the 1990s from the research community on Statistical Machine Translation who made use of statistical tools based on a noisy channel model originally developed for speech recognition -LRB- Brown et al. 1994 Och and Weber 1998 R.Zens et al. 2002 Och and Ney 2001 Koehn et al. 2003 -RRB-	nn_al._et nn_al._Koehn num_Och_2001 conj_and_Och_Ney nn_al._et nn_al._R.Zens num_Och_1998 conj_and_Och_Weber dep_al._2003 dep_al._al. dep_al._Ney dep_al._Och num_al._2002 dep_al._al. dep_al._Weber dep_al._Och num_al._1994 nn_al._et amod_al._Brown nn_recognition_speech dep_developed_al. prep_for_developed_recognition advmod_developed_originally nsubj_developed_model mark_developed_on nn_model_channel amod_model_noisy det_model_a pcomp_based_developed amod_tools_statistical prep_of_use_tools prep_made_based dobj_made_use nsubj_made_who rcmod_Translation_made nn_Translation_Machine amod_Translation_Statistical nn_community_research det_community_the det_1990s_the prep_on_came_Translation prep_from_came_community prep_during_came_1990s nsubj_came_answer amod_answer_de-facto det_answer_The
W08-0306	N03-1017	o	GIZA + + refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as -LRB- Och 2004 -RRB- variations on the refined heuristic have been used by -LRB- Koehn et al. 2003 -RRB- -LRB- diag and diag-and -RRB- and by the phrase-based system Moses -LRB- grow-diag-final -RRB- -LRB- Koehn et al. 2007 -RRB-	amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_Moses_Koehn dep_Moses_grow-diag-final dep_system_Moses amod_system_phrase-based det_system_the conj_and_diag_diag-and conj_and_Koehn_system dep_Koehn_diag-and dep_Koehn_diag dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et agent_used_system agent_used_Koehn auxpass_used_been aux_used_have nsubjpass_used_variations amod_heuristic_refined det_heuristic_the prep_on_variations_heuristic amod_Och_2004 dep_as_Och mwe_as_such prep_systems_as nn_systems_MT amod_systems_statistical amod_systems_phrase-based amod_systems_state-of-the-art parataxis_used_used prep_in_used_systems auxpass_used_been aux_used_have nsubjpass_used_+ nsubjpass_used_GIZA amod_alignments_refined pobj_+_alignments conj_+_GIZA_+
W08-0309	N03-1017	o	The phrases in the translations were located using standard phrase extraction techniques -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_techniques_Koehn nn_techniques_extraction nn_techniques_phrase amod_techniques_standard dobj_using_techniques xcomp_located_using auxpass_located_were nsubjpass_located_phrases det_translations_the prep_in_phrases_translations det_phrases_The
W08-0310	N03-1017	o	translation systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- and use Moses -LRB- Koehn et al. 2007 -RRB- to search for the best target sentence	nn_sentence_target amod_sentence_best det_sentence_the prep_for_search_sentence aux_search_to amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et appos_Moses_Koehn dobj_use_Moses num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney vmod_systems_search conj_and_systems_use dep_systems_Koehn dep_systems_2004 dep_systems_Ney dep_systems_Och nn_systems_translation
W08-0313	N03-1017	o	2 Architecture of the system The goal of statistical machine translation -LRB- SMT -RRB- is to produce a target sentence e from a source sentence f It is today common practice to use phrases as translation units -LRB- Koehn et al. 2003 Och and Ney 2003 -RRB- and a log linear framework in order to introduce several models explaining the translation process e = argmaxp -LRB- e | f -RRB- = argmaxe -LCB- exp -LRB- summationdisplay i ihi -LRB- e f -RRB- -RRB- -RCB- -LRB- 1 -RRB- The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set -LRB- Och and Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_set_Ney dep_set_Och nn_set_development det_set_a prep_on_function_set amod_function_scoring det_function_a dobj_maximize_function aux_maximize_to xcomp_optimized_maximize advmod_optimized_typically auxpass_optimized_are nsubjpass_optimized_weights nsubjpass_optimized_models nn_weights_i det_weights_the conj_and_models_weights nn_models_system det_models_the cop_models_are nsubj_models_summationdisplay dobj_models_exp nsubj_functions_feature det_feature_The dep_e_f nn_ihi_i rcmod_summationdisplay_functions dep_summationdisplay_1 dep_summationdisplay_e dep_summationdisplay_ihi nn_exp_argmaxe dep_=_optimized dep_=_f dep_=_argmaxp dep_=_= dep_=_e dep_=_Architecture dep_f_| dep_|_e nn_process_translation det_process_the dobj_explaining_process amod_models_several dep_introduce_explaining dobj_introduce_models aux_introduce_to dep_introduce_order mark_introduce_in dep_framework_introduce amod_framework_linear nn_framework_log det_framework_a dep_Och_2003 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_units_framework appos_units_Koehn nn_units_translation prep_as_use_framework prep_as_use_units dobj_use_phrases aux_use_to amod_practice_common tmod_practice_today cop_practice_is nsubj_practice_It dep_practice_source det_practice_a dep_sentence_f amod_source_sentence prep_from_sentence_practice dep_sentence_e nn_sentence_target det_sentence_a vmod_produce_use dobj_produce_sentence aux_produce_to xcomp_is_produce nsubj_is_goal appos_translation_SMT nn_translation_machine amod_translation_statistical prep_of_goal_translation det_goal_The det_system_the dep_Architecture_is prep_of_Architecture_system num_Architecture_2 ccomp_``_=
W08-0314	N03-1017	p	3 System Overview 3.1 Translation model The system developed for this years shared task is a state-of-the-art two-pass phrase-based statistical machine translation system based on a log-linear translation model -LRB- Koehn et al 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al nn_Koehn_et nn_model_translation amod_model_log-linear det_model_a prep_on_based_model vmod_system_based nn_system_translation nn_system_machine amod_system_statistical amod_system_phrase-based amod_system_two-pass amod_system_state-of-the-art det_system_a cop_system_is nsubj_system_task tmod_system_years amod_task_shared det_years_this prepc_for_developed_system nsubj_developed_system det_system_The rcmod_model_developed nn_model_Translation num_model_3.1 dep_Overview_Koehn dep_Overview_model dep_System_Overview dep_3_System ccomp_``_3
W08-0316	N03-1017	o	After unioning the Viterbi alignments the stems were replaced with their original words and phrase-pairs of up to five foreign words in length were extracted in the usual fashion -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_fashion_usual det_fashion_the prep_in_extracted_fashion auxpass_extracted_were nsubjpass_extracted_phrase-pairs prep_in_words_length amod_words_foreign num_words_five prep_to_up_words pobj_of_up prep_phrase-pairs_of amod_words_original poss_words_their dep_replaced_Koehn conj_and_replaced_extracted prep_with_replaced_words auxpass_replaced_were nsubjpass_replaced_stems prepc_after_replaced_unioning det_stems_the nn_alignments_Viterbi det_alignments_the dobj_unioning_alignments
W08-0318	N03-1017	o	Foralllanguagepairs weusedtheMosesdecoder -LRB- Koehnetal. ,2007 -RRB- whichfollowsthephrase-based statistical machine translation approach -LRB- Koehn et al. 2003 -RRB- with default settings as a starting point	amod_point_starting det_point_a nn_settings_default amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_approach_Koehn nn_approach_translation nn_approach_machine amod_approach_statistical amod_approach_whichfollowsthephrase-based num_Koehnetal._,2007 prep_as_weusedtheMosesdecoder_point prep_with_weusedtheMosesdecoder_settings appos_weusedtheMosesdecoder_approach appos_weusedtheMosesdecoder_Koehnetal. dep_Foralllanguagepairs_weusedtheMosesdecoder
W08-0322	N03-1017	o	Our system is actually designed as a hybrid of the classic phrase-based SMT model -LRB- Koehn et al. 2003 -RRB- and the kernel regression model as follows First for each source sentence a small relevant set of sentence pairs are retrieved from the large-scale parallel corpus	nn_corpus_parallel amod_corpus_large-scale det_corpus_the prep_from_retrieved_corpus auxpass_retrieved_are nsubjpass_retrieved_set nn_pairs_sentence prep_of_set_pairs amod_set_relevant amod_set_small det_set_a rcmod_sentence_retrieved nn_sentence_source det_sentence_each prep_for_First_sentence mark_follows_as dep_model_First dep_model_follows nn_model_regression nn_model_kernel det_model_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_model_SMT amod_model_phrase-based amod_model_classic det_model_the prep_of_hybrid_model det_hybrid_a conj_and_designed_model dep_designed_Koehn prep_as_designed_hybrid advmod_designed_actually auxpass_designed_is nsubjpass_designed_system poss_system_Our
W08-0326	N03-1017	o	For example our system configuration for the shared task incorporates a wrapper around GIZA + + -LRB- Och and Ney 2003 -RRB- for word alignment and a wrapper around Moses -LRB- Koehn et al. 2007 -RRB- for decoding	amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et prep_around_wrapper_Moses det_wrapper_a nn_alignment_word num_Och_2003 conj_and_Och_Ney prep_for_+_alignment appos_+_Ney appos_+_Och conj_and_GIZA_wrapper conj_+_GIZA_+ dep_wrapper_Koehn prep_around_wrapper_wrapper prep_around_wrapper_+ prep_around_wrapper_GIZA det_wrapper_a prepc_for_incorporates_decoding dobj_incorporates_wrapper nsubj_incorporates_configuration prep_for_incorporates_example amod_task_shared det_task_the prep_for_configuration_task nn_configuration_system poss_configuration_our
W08-0333	N03-1017	o	In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistical MT today a phrasebased translation model -LRB- Koehn et al. 2003 -RRB- and word alignment models based on pairwise lexical translation trained using expectation maximization -LRB- Dempster et al. 1977 -RRB-	amod_Dempster_1977 dep_Dempster_al. nn_Dempster_et dep_maximization_Dempster nn_maximization_expectation dobj_using_maximization xcomp_trained_using vmod_translation_trained amod_translation_lexical amod_translation_pairwise prep_on_based_translation nn_models_alignment nn_models_word amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et vmod_model_based conj_and_model_models dep_model_Koehn nn_model_translation amod_model_phrasebased det_model_a amod_MT_statistical tmod_used_today prep_in_used_MT advmod_used_commonly vmod_models_used dep_kinds_models dep_kinds_model prep_of_kinds_models num_kinds_two nn_algorithms_training prep_of_implementations_algorithms nn_implementations_MapReduce prep_for_present_kinds dobj_present_implementations nsubj_present_we prep_in_present_paper det_paper_this rcmod_``_present
W08-0333	N03-1017	o	4 Phrase-Based Translation In phrase-based translation the translation process is modeled by splitting the source sentence into phrases -LRB- a contiguous string of words -RRB- and translating the phrases as a unit -LRB- Och et al. 1999 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn appos_Och_1999 dep_Och_al. nn_Och_et appos_unit_Och det_unit_a det_phrases_the prep_as_translating_unit dobj_translating_phrases prep_of_string_words amod_string_contiguous det_string_a dep_phrases_string prep_into_sentence_phrases nn_sentence_source det_sentence_the conj_and_splitting_translating dobj_splitting_sentence agent_modeled_translating agent_modeled_splitting auxpass_modeled_is nsubjpass_modeled_process tmod_modeled_Translation nn_process_translation det_process_the amod_translation_phrase-based prep_in_Translation_translation amod_Translation_Phrase-Based num_Translation_4
W08-0335	N03-1017	o	The training and decoding system of our SMT used the publicly available Pharaoh -LRB- Koehn et al. 2003 -RRB- 2	dep_Koehn_2 dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_Pharaoh_available det_Pharaoh_the advmod_available_publicly dep_used_Koehn dobj_used_Pharaoh nsubj_used_system poss_SMT_our prep_of_system_SMT nn_system_decoding nn_system_training det_system_The conj_and_training_decoding
W08-0336	N03-1017	p	2.2 Phrase-based Chinese-to-English MT The MT system used in this paper is Moses a stateof-the-art phrase-based system -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_system_Koehn amod_system_phrase-based amod_system_stateof-the-art det_system_a appos_Moses_system cop_Moses_is nsubj_Moses_system det_paper_this prep_in_used_paper vmod_system_used nn_system_MT det_system_The rcmod_MT_Moses amod_MT_Chinese-to-English amod_MT_Phrase-based num_MT_2.2 dep_``_MT
W08-0403	N03-1017	o	Our baseline model follows Chiangs hierarchical model -LRB- Chiang 2007 -RRB- in conjunction with additional features conditional probabilities in both directions P -LRB- | -RRB- and P -LRB- | -RRB- lexical weights -LRB- Koehn et al. 2003 -RRB- in both directions Pw -LRB- | -RRB- and Pw -LRB- | -RRB- 21 word counts | e | rule counts | D | target n-gram language model PLM -LRB- e -RRB- glue rule penalty to learn preference of nonterminal rewriting over serial combination through Eq	prep_through_combination_Eq amod_combination_serial prep_over_rewriting_combination amod_nonterminal_rewriting prep_of_preference_nonterminal dobj_learn_preference aux_learn_to vmod_penalty_learn nn_penalty_rule nn_penalty_glue appos_PLM_e nn_PLM_model nn_PLM_language nn_PLM_n-gram nn_PLM_target nn_|_D nn_|_| dobj_counts_| nsubj_counts_rule dep_|_e number_|_| dobj_counts_| nsubj_counts_word num_word_21 appos_Pw_| conj_and_Pw_penalty conj_and_Pw_PLM conj_and_Pw_counts conj_and_Pw_counts conj_and_Pw_Pw appos_Pw_| preconj_directions_both amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_weights_directions dep_weights_Koehn amod_weights_lexical appos_P_| dep_P_penalty dep_P_PLM dep_P_counts dep_P_counts dep_P_Pw dep_P_Pw conj_and_P_weights conj_and_P_P appos_P_| det_directions_both dep_probabilities_weights dep_probabilities_P dep_probabilities_P prep_in_probabilities_directions amod_probabilities_conditional amod_features_additional prep_with_conjunction_features dep_Chiang_2007 prep_in_model_conjunction appos_model_Chiang amod_model_hierarchical nn_model_Chiangs dep_follows_probabilities dobj_follows_model nsubj_follows_model nn_model_baseline poss_model_Our
W08-0404	N03-1017	o	by diag-and symmetrization -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_symmetrization_diag-and dep_by_Koehn pobj_by_symmetrization dep_``_by
W08-0404	N03-1017	o	Consider the lexical model pw -LRB- ry | rx -RRB- defined following Koehn et al -LRB- 2003 -RRB- with a denoting the most frequent word alignment observed for the rule in the training set	nn_set_training det_set_the prep_in_rule_set det_rule_the prep_for_observed_rule vmod_alignment_observed nn_alignment_word amod_alignment_frequent det_alignment_the advmod_frequent_most dobj_denoting_alignment vmod_a_denoting appos_al_2003 prep_with_Koehn_a dep_Koehn_al nn_Koehn_et prep_following_defined_Koehn nn_rx_| nn_rx_ry vmod_pw_defined appos_pw_rx nn_pw_model amod_pw_lexical det_pw_the dobj_Consider_pw
W08-0405	N03-1017	o	2 Baseline DP Decoder The translation model used in this paper is a phrasebased model -LRB- Koehn et al. 2003 -RRB- where the translation units are so-called blocks a block b is a pair consisting of a source phrase s and a target phrase t which are translations of each other	det_other_each prep_of_translations_other cop_translations_are nsubj_translations_which rcmod_t_translations nn_t_phrase nn_t_target det_t_a conj_and_s_t nn_s_phrase nn_s_source det_s_a prep_of_consisting_t prep_of_consisting_s vmod_pair_consisting det_pair_a cop_pair_is nsubj_pair_b nn_b_block det_b_a amod_blocks_so-called cop_blocks_are nsubj_blocks_units advmod_blocks_where nn_units_translation det_units_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_model_phrasebased det_model_a cop_model_is nsubj_model_model det_paper_this prep_in_used_paper vmod_model_used nn_model_translation det_model_The dep_Decoder_pair rcmod_Decoder_blocks dep_Decoder_Koehn rcmod_Decoder_model nn_Decoder_DP nn_Decoder_Baseline num_Decoder_2 dep_``_Decoder
W08-0406	N03-1017	p	1 Introduction The emergence of phrase-based statistical machine translation -LRB- PSMT -RRB- -LRB- Koehn et al. 2003 -RRB- has been one of the major developments in statistical approaches to translation	prep_to_approaches_translation amod_approaches_statistical prep_in_developments_approaches amod_developments_major det_developments_the prep_of_one_developments cop_one_been aux_one_has nsubj_one_emergence amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_translation_PSMT nn_translation_machine amod_translation_statistical amod_translation_phrase-based dep_emergence_Koehn prep_of_emergence_translation det_emergence_The rcmod_Introduction_one num_Introduction_1
W08-0409	N03-1017	o	73 ment and phrase-extraction heuristics described in -LRB- Koehn et al. 2003 -RRB- minimum-error-rate training -LRB- Och 2003 -RRB- a trigram language model with KneserNey smoothing trained with SRILM -LRB- Stolcke 2002 -RRB- on the English side of the training data and Moses -LRB- Koehn et al. 2007 -RRB- to decode	aux_decode_to amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_Moses_Koehn nn_data_training det_data_the prep_of_side_data amod_side_English det_side_the dep_Stolcke_2002 appos_SRILM_Stolcke prep_on_trained_side prep_with_trained_SRILM vmod_smoothing_trained nn_smoothing_KneserNey prep_with_model_smoothing nn_model_language nn_model_trigram det_model_a dep_Och_2003 appos_training_Och amod_training_minimum-error-rate vmod_Koehn_decode conj_and_Koehn_Moses conj_and_Koehn_model conj_and_Koehn_training amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Moses prep_in_described_model prep_in_described_training prep_in_described_Koehn amod_heuristics_phrase-extraction vmod_ment_described conj_and_ment_heuristics num_ment_73 dep_``_heuristics dep_``_ment
W08-0411	N03-1017	p	1 Introduction Phrase-based Statistical MT -LRB- PB-SMT -RRB- -LRB- Koehn et al. 2003 -RRB- has become the predominant approach to Machine Translation in recent years	amod_years_recent nn_Translation_Machine prep_to_approach_Translation amod_approach_predominant det_approach_the prep_in_become_years xcomp_become_approach aux_become_has nsubj_become_MT amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_MT_Koehn appos_MT_PB-SMT amod_MT_Statistical amod_MT_Phrase-based nn_MT_Introduction num_MT_1 ccomp_``_become
W08-1501	N03-1017	o	To generate the n-best lists a phrase based SMT -LRB- Koehn et al. 2003 -RRB- was used	auxpass_used_was nsubjpass_used_phrase advcl_used_generate amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_SMT_Koehn pobj_based_SMT prep_phrase_based det_phrase_a amod_lists_n-best det_lists_the dobj_generate_lists aux_generate_To
W08-1911	N03-1017	o	4 Experiments and evaluation We carried out an evaluation on the local rephrasing of French sentences using English as the pivot language .2 We extracted phrase alignments of up to 7 word forms using the Giza + + alignment tool -LRB- Och and Ney 2003 -RRB- and the grow-diag-final-and heuristics described in -LRB- Koehn et al. 2003 -RRB- on 948,507 sentences of the French-English part of the Europarl corpus -LRB- Koehn 2005 -RRB- and obtained some 42 million phrase pairs for which probabilities were estimated using maximum likelihood estimation	nn_estimation_likelihood nn_estimation_maximum dobj_using_estimation xcomp_estimated_using auxpass_estimated_were nsubjpass_estimated_probabilities prep_for_estimated_which rcmod_pairs_estimated nn_pairs_phrase num_pairs_million det_pairs_some number_million_42 dobj_obtained_pairs amod_Koehn_2005 appos_corpus_Koehn nn_corpus_Europarl det_corpus_the prep_of_part_corpus amod_part_French-English det_part_the prep_of_sentences_part num_sentences_948,507 conj_and_Koehn_obtained prep_on_Koehn_sentences amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_obtained prep_in_described_Koehn vmod_heuristics_described amod_heuristics_grow-diag-final-and det_heuristics_the num_Och_2003 conj_and_Och_Ney conj_and_tool_heuristics appos_tool_Ney appos_tool_Och nn_tool_alignment pobj_+_heuristics pobj_+_tool det_Giza_the conj_+_using_+ dobj_using_Giza nn_forms_word num_forms_7 dep_7_to quantmod_7_up prep_of_alignments_forms nn_alignments_phrase dobj_extracted_alignments nsubj_extracted_We dep_.2_+ dep_.2_using rcmod_.2_extracted nn_.2_language amod_.2_pivot det_.2_the prep_as_using_.2 dobj_using_English amod_sentences_French prep_of_rephrasing_sentences amod_rephrasing_local det_rephrasing_the prep_on_evaluation_rephrasing det_evaluation_an vmod_carried_using dobj_carried_evaluation prt_carried_out nsubj_carried_We rcmod_Experiments_carried conj_and_Experiments_evaluation num_Experiments_4
W08-1911	N03-1017	o	-LRB- Och and Ney 2003 -RRB- -RRB- and the phrase-based approach to Statistical Machine Translation -LRB- Koehn et al. 2003 -RRB- has led to the development of heuristics for obtaining alignments between phrases of any number of words	prep_of_number_words det_number_any prep_of_phrases_number prep_between_obtaining_phrases dobj_obtaining_alignments prep_of_development_heuristics det_development_the prepc_for_led_obtaining prep_to_led_development aux_led_has nsubj_led_approach nsubj_led_Ney nsubj_led_Och amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_Translation_Machine nn_Translation_Statistical prep_to_approach_Translation amod_approach_phrase-based det_approach_the dep_Och_Koehn conj_and_Och_approach dep_Och_2003 conj_and_Och_Ney
W08-2119	N03-1017	o	We use the same featuresas -LRB- Koehnet al. 2003 -RRB-	dep_al._2003 nn_al._Koehnet dep_featuresas_al. amod_featuresas_same det_featuresas_the dobj_use_featuresas nsubj_use_We
W08-2119	N03-1017	o	Our method does not suppose a uniform distribution over all possible phrase segmentationsas -LRB- Koehn et al. 2003 -RRB- since each phrase tree has a probability	det_probability_a dobj_has_probability nsubj_has_distribution nn_tree_phrase det_tree_each amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_segmentationsas_phrase amod_segmentationsas_possible det_segmentationsas_all prep_since_distribution_tree dep_distribution_Koehn prep_over_distribution_segmentationsas amod_distribution_uniform det_distribution_a ccomp_suppose_has neg_suppose_not aux_suppose_does nsubj_suppose_method poss_method_Our ccomp_``_suppose
W08-2119	N03-1017	o	This system uses all featuresof conventionalphrase-basedSMT as in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn pcomp_as_in amod_conventionalphrase-basedSMT_featuresof det_conventionalphrase-basedSMT_all prep_uses_as dobj_uses_conventionalphrase-basedSMT nsubj_uses_system det_system_This
W09-0423	N03-1017	o	4 Architecture of the SMT system The goal of statistical machine translation -LRB- SMT -RRB- is to produce a target sentence e from a source sentence f It is today common practice to use phrases as translation units -LRB- Koehn et al. 2003 Och and Ney 2003 -RRB- and a log linear framework in order to introduce several models explaining the translation process e = argmaxp -LRB- e | f -RRB- = argmaxe -LCB- exp -LRB- summationdisplay i ihi -LRB- e f -RRB- -RRB- -RCB- -LRB- 1 -RRB- The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set -LRB- Och and Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_set_Ney dep_set_Och nn_set_development det_set_a prep_on_function_set amod_function_scoring det_function_a dobj_maximize_function aux_maximize_to xcomp_optimized_maximize advmod_optimized_typically auxpass_optimized_are nsubjpass_optimized_weights nsubjpass_optimized_models nn_weights_i det_weights_the conj_and_models_weights nn_models_system det_models_the cop_models_are nsubj_models_summationdisplay dobj_models_exp nsubj_functions_feature det_feature_The dep_e_f nn_ihi_i rcmod_summationdisplay_functions dep_summationdisplay_1 dep_summationdisplay_e dep_summationdisplay_ihi nn_exp_argmaxe dep_=_optimized dep_=_f dep_=_argmaxp dep_=_= dep_=_e dep_=_Architecture dep_f_| dep_|_e nn_process_translation det_process_the dobj_explaining_process amod_models_several dep_introduce_explaining dobj_introduce_models aux_introduce_to dep_introduce_order mark_introduce_in dep_framework_introduce amod_framework_linear nn_framework_log det_framework_a dep_Och_2003 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_units_framework appos_units_Koehn nn_units_translation prep_as_use_framework prep_as_use_units dobj_use_phrases aux_use_to amod_practice_common tmod_practice_today cop_practice_is nsubj_practice_It dep_practice_source det_practice_a dep_sentence_f amod_source_sentence prep_from_sentence_practice dep_sentence_e nn_sentence_target det_sentence_a vmod_produce_use dobj_produce_sentence aux_produce_to xcomp_is_produce nsubj_is_goal appos_translation_SMT nn_translation_machine amod_translation_statistical prep_of_goal_translation det_goal_The nn_system_SMT det_system_the dep_Architecture_is prep_of_Architecture_system num_Architecture_4 ccomp_``_=
W09-0424	N03-1017	n	In such tasks feature calculation is also very expensive in terms of time required huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p -LRB- f | e -RRB- and reverse translation probability p -LRB- e | f -RRB- -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_f_| dep_|_e appos_p_Koehn dep_p_f nn_p_probability nn_p_translation dobj_reverse_p conj_and_|_reverse dep_|_e nn_|_f dep_p_reverse dep_p_| nn_p_probability nn_p_translation det_p_the amod_features_such prep_of_calculation_features nn_calculation_frequency amod_calculation_relative prep_for_directions_calculation num_directions_two prep_as_sorted_p prep_in_sorted_directions auxpass_sorted_be aux_sorted_must nsubjpass_sorted_sets amod_rules_extracted prep_of_sets_rules amod_sets_huge vmod_time_required prep_of_terms_time parataxis_expensive_sorted prep_in_expensive_terms advmod_expensive_very advmod_expensive_also cop_expensive_is nsubj_expensive_calculation prep_in_expensive_tasks nn_calculation_feature amod_tasks_such
W09-0430	N03-1017	o	Then the two models and a search module are used to decode the best translation -LRB- Brown et al. 1993 Koehn et al. 2003 -RRB-	appos_al._2003 nn_al._et nn_al._Koehn dep_al._al. num_al._1993 nn_al._et amod_al._Brown dep_translation_al. amod_translation_best det_translation_the dobj_decode_translation aux_decode_to xcomp_used_decode auxpass_used_are nsubjpass_used_module nsubjpass_used_models advmod_used_Then nn_module_search det_module_a conj_and_models_module num_models_two det_models_the
W09-0434	N03-1017	p	Phrase-based models -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- have been a major paradigm in statistical machine translation in the last few years showing state-of-the-art performance for many language pairs	nn_pairs_language amod_pairs_many amod_performance_state-of-the-art prep_for_showing_pairs dobj_showing_performance amod_years_few amod_years_last det_years_the nn_translation_machine amod_translation_statistical vmod_paradigm_showing prep_in_paradigm_years prep_in_paradigm_translation amod_paradigm_major det_paradigm_a cop_paradigm_been aux_paradigm_have nsubj_paradigm_models num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney appos_models_2004 appos_models_Ney appos_models_Och amod_models_Phrase-based
W09-0436	N03-1017	p	4 Machine Translation Experiments 4.1 Experimental Setting For our MT experiments we used a reimplementation of Moses -LRB- Koehn et al. 2003 -RRB- a state-of-the-art phrase-based system	amod_system_phrase-based amod_system_state-of-the-art det_system_a amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_of_reimplementation_Moses det_reimplementation_a dobj_used_system dep_used_Koehn dobj_used_reimplementation nsubj_used_we nsubj_used_Experiments nn_experiments_MT poss_experiments_our prep_for_Setting_experiments amod_Setting_Experimental num_Setting_4.1 dep_Experiments_Setting nn_Experiments_Translation nn_Experiments_Machine num_Experiments_4 ccomp_``_used
W09-0437	N03-1017	o	The corpus was aligned with GIZA + + -LRB- Och and Ney 2003 -RRB- and symmetrized with the grow-diag-finaland heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_heuristic_Koehn amod_heuristic_grow-diag-finaland det_heuristic_the prep_with_symmetrized_heuristic nsubjpass_symmetrized_corpus amod_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ conj_and_aligned_symmetrized prep_with_aligned_+ prep_with_aligned_GIZA auxpass_aligned_was nsubjpass_aligned_corpus det_corpus_The
W09-0439	N03-1017	o	1 Introduction Most recent approaches in SMT eg -LRB- Koehn et al. 2003 Chiang 2005 -RRB- use a log-linear model to combine probabilistic features	amod_features_probabilistic dobj_combine_features aux_combine_to amod_model_log-linear det_model_a vmod_use_combine dobj_use_model num_Chiang_2005 dep_Koehn_use dep_Koehn_Chiang amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_eg_Koehn dep_approaches_eg prep_in_approaches_SMT amod_approaches_recent advmod_recent_Most dep_Introduction_approaches num_Introduction_1
W09-0809	N03-1017	o	Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system -LRB- Koehn et al. 2003b -RRB- or a hierarchical phrase system -LRB- Chiang 2005 -RRB-	amod_Chiang_2005 dep_system_Chiang nn_system_phrase amod_system_hierarchical det_system_a appos_Koehn_2003b dep_Koehn_al. nn_Koehn_et conj_or_system_system dep_system_Koehn nn_system_PSMT amod_system_traditional det_system_a prep_such_as_approaches_system prep_such_as_approaches_system amod_approaches_other prep_with_compatible_approaches cop_compatible_is nsubj_compatible_approach mark_compatible_that prep_in_approach_essence det_approach_the ccomp_advantage_compatible det_advantage_the dobj_has_advantage advmod_has_also nsubj_has_Separating nn_reordering_language nn_reordering_source det_reordering_the prep_from_scoring_reordering vmod_the_scoring dobj_Separating_the ccomp_``_has
W09-0809	N03-1017	o	In addition to the manual alignment supplied with these data we create an automatic word alignment for them using GIZA + + -LRB- Och and Ney 2003 -RRB- and the grow-diagfinal -LRB- GDF -RRB- symmetrization algorithm -LRB- Koehn et al. 2005 -RRB-	amod_Koehn_2005 dep_Koehn_al. nn_Koehn_et nn_algorithm_symmetrization amod_algorithm_grow-diagfinal det_algorithm_the dep_grow-diagfinal_GDF num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_algorithm conj_+_GIZA_+ dobj_using_algorithm dobj_using_+ dobj_using_GIZA vmod_alignment_using prep_for_alignment_them nn_alignment_word amod_alignment_automatic det_alignment_an dep_create_Koehn dobj_create_alignment nsubj_create_we prep_in_addition_to_create_alignment det_data_these prep_with_supplied_data vmod_alignment_supplied amod_alignment_manual det_alignment_the
W09-0809	N03-1017	o	We are also interested in examining the approach within a standard phrase-based decoder such as Moses -LRB- Koehn et al. 2003b -RRB- or a hierarchical phrase system -LRB- Chiang 2005 -RRB-	amod_Chiang_2005 dep_system_Chiang nn_system_phrase amod_system_hierarchical det_system_a appos_Koehn_2003b dep_Koehn_al. nn_Koehn_et conj_or_Moses_system dep_Moses_Koehn prep_such_as_decoder_system prep_such_as_decoder_Moses amod_decoder_phrase-based amod_decoder_standard det_decoder_a prep_within_approach_decoder det_approach_the dobj_examining_approach prepc_in_interested_examining advmod_interested_also cop_interested_are nsubj_interested_We
W09-0809	N03-1017	p	1 Introduction The emergence of phrase-based statistical machine translation -LRB- PSMT -RRB- -LRB- Koehn et al. 2003a -RRB- has been one of the major developments in statistical approaches to translation	prep_to_approaches_translation amod_approaches_statistical prep_in_developments_approaches amod_developments_major det_developments_the prep_of_one_developments cop_one_been aux_one_has nsubj_one_emergence appos_Koehn_2003a dep_Koehn_al. nn_Koehn_et appos_translation_PSMT nn_translation_machine amod_translation_statistical amod_translation_phrase-based dep_emergence_Koehn prep_of_emergence_translation det_emergence_The rcmod_Introduction_one num_Introduction_1
W09-1104	N03-1017	o	By using only the bidirectional word alignment links one can implement a very robust such filter as the bidirectional links are generally reliable even though they have low recall for overall translational correspondences -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_correspondences_translational amod_correspondences_overall prep_for_recall_correspondences amod_recall_low dep_have_Koehn dobj_have_recall nsubj_have_they mark_have_though advmod_have_even advcl_reliable_have advmod_reliable_generally cop_reliable_are nsubj_reliable_links mark_reliable_as amod_links_bidirectional det_links_the amod_filter_such amod_filter_robust det_filter_a advmod_robust_very advcl_implement_reliable dobj_implement_filter aux_implement_can nsubj_implement_one prepc_by_implement_using nn_links_alignment nn_links_word amod_links_bidirectional det_links_the advmod_links_only dobj_using_links
W09-1114	N03-1017	o	Our technique is based on a novel Gibbs sampler that draws samples from the posterior distributionofaphrase-basedtranslationmodel -LRB- Koehn et al. 2003 -RRB- but operates in linear time with respect to the number of input words -LRB- Section 2 -RRB-	num_Section_2 appos_words_Section nn_words_input prep_of_number_words det_number_the amod_time_linear prep_with_respect_to_operates_number prep_in_operates_time nsubj_operates_that amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_distributionofaphrase-basedtranslationmodel_Koehn amod_distributionofaphrase-basedtranslationmodel_posterior det_distributionofaphrase-basedtranslationmodel_the conj_but_draws_operates prep_from_draws_distributionofaphrase-basedtranslationmodel dobj_draws_samples nsubj_draws_that rcmod_sampler_operates rcmod_sampler_draws nn_sampler_Gibbs amod_sampler_novel det_sampler_a prep_on_based_sampler auxpass_based_is nsubjpass_based_technique poss_technique_Our ccomp_``_based
W09-1117	N03-1017	o	1 Introduction Recent trends in machine translation illustrate that highly accurate word and phrase translations can be learned automatically given enough parallel training data -LRB- Koehn et al. 2003 Chiang 2007 -RRB-	amod_Chiang_2007 dep_Koehn_Chiang appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_data_Koehn nn_data_training amod_data_parallel dep_enough_data advmod_given_enough advmod_given_automatically dep_learned_given auxpass_learned_be aux_learned_can nsubjpass_learned_translations nsubjpass_learned_word mark_learned_that nn_translations_phrase conj_and_word_translations amod_word_accurate advmod_accurate_highly ccomp_illustrate_learned nsubj_illustrate_trends nn_translation_machine prep_in_trends_translation amod_trends_Recent nn_trends_Introduction num_trends_1
W09-1908	N03-1017	n	While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems -LRB- Koehn et al. 2003 -RRB- the variety of linguistic annotation required is greater	cop_greater_is nsubj_greater_phrase mark_greater_than amod_annotation_linguistic vmod_variety_required prep_of_variety_annotation det_variety_the amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_systems_Koehn amod_systems_statistical amod_systems_based appos_phrase_variety dep_phrase_systems amod_phrase_corresponding advcl_smaller_greater amod_magnitude_smaller prep_of_orders_magnitude cop_orders_is nsubj_orders_amount mark_orders_While amod_systems_such dobj_build_systems aux_build_to xcomp_required_build vmod_data_required amod_data_parallel prep_of_amount_data det_amount_the advcl_``_orders
W09-1908	N03-1017	p	We conclude with some challenges that still remain in applying proactive learning for MT. 2 Syntax Based Machine Translation In recent years corpus based approaches to machine translation have become predominant with Phrase Based Statistical Machine Translation -LRB- PBSMT -RRB- -LRB- Koehn et al. 2003 -RRB- being the most actively progressing area	amod_area_progressing det_area_the cop_area_being nsubj_area_Translation advmod_progressing_actively advmod_progressing_most amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_Translation_Koehn appos_Translation_PBSMT nn_Translation_Machine amod_Translation_Statistical pcomp_Based_area acomp_become_predominant aux_become_have nsubj_become_approaches nn_translation_machine prep_to_approaches_translation amod_approaches_based nn_approaches_corpus amod_years_recent nn_Translation_Machine prep_Based_Based prep_with_Based_Phrase parataxis_Based_become prep_in_Based_years dobj_Based_Translation num_Syntax_2 nn_Syntax_MT. prep_for_learning_Syntax amod_learning_proactive dobj_applying_learning prepc_in_remain_applying advmod_remain_still nsubj_remain_that rcmod_challenges_remain det_challenges_some dep_conclude_Based prep_with_conclude_challenges nsubj_conclude_We
W09-2301	N03-1017	n	1 Introduction The dominance of traditional phrase-based statistical machine translation -LRB- PBSMT -RRB- models -LRB- Koehn et al. 2003 -RRB- has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated	auxpass_translated_being vmod_sentences_translated det_sentences_the prep_of_syntax_sentences det_syntax_the dobj_take_syntax prep_into_take_account advmod_take_explicity nsubj_take_that rcmod_models_take amod_models_new prep_of_number_models det_number_a prep_of_development_number conj_and_development_improvement det_development_the agent_challenged_improvement agent_challenged_development auxpass_challenged_been advmod_challenged_recently aux_challenged_has nsubjpass_challenged_dominance amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_models_translation appos_translation_PBSMT nn_translation_machine amod_translation_statistical amod_translation_phrase-based amod_translation_traditional dep_dominance_Koehn prep_of_dominance_models det_dominance_The rcmod_Introduction_challenged num_Introduction_1
W09-2306	N03-1017	p	1 Introduction Phrase-based statistical machine translation models -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 Koehn 2004 Koehn et al. 2007 -RRB- have achieved significant improvements in translation accuracy over the original IBM word-based model	amod_model_word-based nn_model_IBM amod_model_original det_model_the nn_accuracy_translation amod_improvements_significant prep_over_achieved_model prep_in_achieved_accuracy dobj_achieved_improvements aux_achieved_have nsubj_achieved_Koehn nsubj_achieved_2004 nsubj_achieved_Ney nsubj_achieved_Och num_Koehn_2007 nn_Koehn_al. nn_Koehn_et num_Koehn_2004 dep_Och_Koehn conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et parataxis_Marcu_achieved conj_and_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong dep_models_Koehn dep_models_Wong dep_models_Marcu nn_models_translation nn_models_machine amod_models_statistical amod_models_Phrase-based nn_models_Introduction num_models_1
W09-2307	N03-1017	o	Our MT experiments use a re-implementation of Moses -LRB- Koehn et al. 2003 -RRB- called Phrasal which provides an easier API for adding features	dobj_adding_features amod_API_easier det_API_an prepc_for_provides_adding dobj_provides_API nsubj_provides_which dep_called_provides dep_called_Phrasal dep_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_of_re-implementation_Moses det_re-implementation_a dep_use_called dep_use_Koehn dobj_use_re-implementation nsubj_use_experiments nn_experiments_MT poss_experiments_Our ccomp_``_use
W09-2307	N03-1017	o	2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_movements_phrase prep_for_cost_movements det_cost_the amod_distance_linear dep_use_Koehn prep_as_use_cost dobj_use_distance nsubj_use_models amod_systems_phrase-based prep_in_models_systems nn_models_reordering amod_models_Basic nn_models_Model nn_models_Reordering amod_models_Discriminative num_models_2 ccomp_``_use
W09-2310	N03-1017	p	The state-of-the-art SMT system Moses implements a distance-based reordering model -LRB- Koehn et al. 2003 -RRB- and a distortion model operating with rewrite patterns extracted from a phrase alignment table -LRB- Tillman 2004 -RRB-	dep_Tillman_2004 appos_table_Tillman nn_table_alignment nn_table_phrase det_table_a prep_from_extracted_table vmod_patterns_extracted dobj_rewrite_patterns prepc_with_operating_rewrite vmod_model_operating nn_model_distortion det_model_a amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_model_model dep_model_Koehn nn_model_reordering amod_model_distance-based det_model_a dobj_implements_model dobj_implements_model dep_Moses_implements nsubj_Moses_system nn_system_SMT amod_system_state-of-the-art det_system_The
C04-1112	N03-5008	o	4.2 Smoothing Gaussian Priors Since NLP maximum entropy models usually have lots of features and lots of sparseness -LRB- e.g. features seen in testing not occurring in training -RRB- smoothing is essential as a way to optimize the feature weights -LRB- Chen and Rosenfeld 2000 Klein and Manning 2003 -RRB-	dep_Chen_2003 conj_and_Chen_Manning conj_and_Chen_Klein conj_and_Chen_2000 conj_and_Chen_Rosenfeld nn_weights_feature det_weights_the dobj_optimize_weights aux_optimize_to vmod_way_optimize det_way_a dep_essential_Manning dep_essential_Klein dep_essential_2000 dep_essential_Rosenfeld dep_essential_Chen prep_as_essential_way cop_essential_is nsubj_essential_smoothing advmod_essential_e.g. prep_in_occurring_training neg_occurring_not vmod_testing_occurring prep_in_seen_testing dep_features_seen dep_e.g._features prep_of_lots_sparseness conj_and_features_lots prep_of_lots_lots prep_of_lots_features dep_have_essential dobj_have_lots advmod_have_usually nsubj_have_models mark_have_Since nn_models_entropy nn_models_maximum nn_models_NLP rcmod_Priors_have nn_Priors_Gaussian dep_Smoothing_Priors num_Smoothing_4.2 dep_``_Smoothing
C08-1145	N03-5008	o	As machine learners we used SVM-light1 -LRB- Joachims 1998 -RRB- and the MaxEnt decider from the Stanford Classifier2 -LRB- Manning and Klein 2003 -RRB-	amod_Manning_2003 conj_and_Manning_Klein dep_Classifier2_Klein dep_Classifier2_Manning nn_Classifier2_Stanford det_Classifier2_the prep_from_decider_Classifier2 nn_decider_MaxEnt det_decider_the amod_Joachims_1998 conj_and_SVM-light1_decider dep_SVM-light1_Joachims dobj_used_decider dobj_used_SVM-light1 nsubj_used_we rcmod_learners_used nn_learners_machine pobj_As_learners dep_``_As
D08-1097	N03-5008	p	2.2 Maximum Entropy Models Maximum entropy -LRB- ME -RRB- models -LRB- Berger et al. 1996 Manning and Klein 2003 -RRB- also known as 928 log-linear and exponential learning models provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification	prep_for_sources_classification nn_sources_information amod_sources_heterogeneous amod_sources_many prep_from_integrate_sources dobj_integrate_features aux_integrate_can nsubj_integrate_models amod_models_entropy nn_models_Maximum nn_etc._recognition nn_etc._entity dep_named_etc. nn_tagging_speech prep_of_part_tagging prep_including_processing_part nn_processing_language amod_processing_natural prep_to_applied_processing advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_which conj_and_classification_prediction prep_for_technique_prediction prep_for_technique_classification amod_technique_learning rcmod_machine_applied dep_machine_technique nn_machine_purpose amod_machine_general det_machine_a ccomp_provide_integrate vmod_provide_named dobj_provide_machine nsubj_provide_models nn_models_learning amod_models_exponential amod_models_log-linear num_models_928 conj_and_log-linear_exponential prep_as_known_models advmod_known_also dep_Manning_2003 conj_and_Manning_Klein dep_Berger_Klein dep_Berger_Manning appos_Berger_1996 dep_Berger_al. nn_Berger_et vmod_models_known appos_models_Berger nn_models_entropy nn_models_Models nn_models_Entropy nn_models_Maximum num_models_2.2 appos_entropy_ME nn_entropy_Maximum
D08-1097	N03-5008	o	In this paper we adopt Stanford Maximum Entropy -LRB- Manning and Klein 2003 -RRB- implementation in our experiments	poss_experiments_our dep_implementation_Klein dep_implementation_Manning nn_implementation_Entropy dep_Manning_2003 conj_and_Manning_Klein nn_Entropy_Maximum nn_Entropy_Stanford prep_in_adopt_experiments dobj_adopt_implementation nsubj_adopt_we prep_in_adopt_paper det_paper_this
D08-1097	N03-5008	o	There are accurate parsers available such as Chaniak parser -LRB- Charniak and Johnson 2005 -RRB- Stanford parser -LRB- Klein and Manning 2003 -RRB- and Berkeley parser -LRB- Petrov and Klein 2007 -RRB- among which we use the Berkeley parser 2 to help identify the head word	nn_word_head det_word_the dobj_identify_word ccomp_help_identify aux_help_to num_parser_2 nn_parser_Berkeley det_parser_the vmod_use_help dobj_use_parser nsubj_use_we prep_among_use_which amod_Petrov_2007 conj_and_Petrov_Klein dep_parser_Klein dep_parser_Petrov nn_parser_Berkeley amod_Klein_2003 conj_and_Klein_Manning dep_parser_Manning dep_parser_Klein nn_parser_Stanford dep_Charniak_2005 conj_and_Charniak_Johnson conj_and_parser_parser conj_and_parser_parser dep_parser_Johnson dep_parser_Charniak nn_parser_Chaniak prep_such_as_available_parser prep_such_as_available_parser prep_such_as_available_parser rcmod_parsers_use amod_parsers_available amod_parsers_accurate nsubj_are_parsers expl_are_There ccomp_``_are
D08-1097	N03-5008	o	Collins head words finder rules have been modified to extract semantic head word -LRB- Klein and Manning 2003 -RRB-	amod_Klein_2003 conj_and_Klein_Manning dep_word_Manning dep_word_Klein nn_word_head amod_word_semantic dobj_extract_word aux_extract_to xcomp_modified_extract auxpass_modified_been aux_modified_have nsubjpass_modified_rules amod_rules_finder nn_rules_words nn_rules_head nn_rules_Collins
D09-1057	N03-5008	o	5.5 Dependency validity features Like -LRB- Cui et al. 2004 -RRB- we extract the dependency path from the question word to the common word -LRB- existing in both question and sentence -RRB- and the path from candidate answer -LRB- such as CoNLL NE and numerical entity -RRB- to the common word for each pair of question and candidate sentence using Stanford dependency parser -LRB- Klein and Manning 2003 Marneffe et al. 2006 -RRB-	num_Marneffe_2006 nn_Marneffe_al. nn_Marneffe_et dep_Klein_Marneffe conj_and_Klein_2003 conj_and_Klein_Manning dep_parser_2003 dep_parser_Manning dep_parser_Klein nn_parser_dependency nn_parser_Stanford dobj_using_parser nn_sentence_candidate prep_of_pair_question det_pair_each prep_for_word_pair amod_word_common det_word_the amod_entity_numerical conj_and_NE_entity nn_NE_CoNLL prep_such_as_answer_entity prep_such_as_answer_NE nn_answer_candidate vmod_path_using conj_and_path_sentence prep_to_path_word prep_from_path_answer det_path_the conj_and_question_sentence preconj_question_both prep_in_existing_sentence prep_in_existing_question dep_word_existing amod_word_common det_word_the prep_to_word_word dep_question_word det_question_the nn_path_dependency det_path_the conj_and_extract_sentence conj_and_extract_path prep_from_extract_question dobj_extract_path nsubj_extract_we vmod_extract_features amod_Cui_2004 dep_Cui_al. nn_Cui_et dep_Like_Cui prep_features_Like nsubj_features_validity nn_validity_Dependency num_validity_5.5
D09-1057	N03-5008	p	2 Maximum Entropy Models Maximum entropy -LRB- ME -RRB- models -LRB- Berger et al. 1996 Manning and Klein 2003 -RRB- also known as log-linear and exponential learning models provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification	prep_for_sources_classification nn_sources_information amod_sources_heterogeneous amod_sources_many prep_from_integrate_sources dobj_integrate_features aux_integrate_can nsubj_integrate_models dep_integrate_provideageneralpurposemachinelearningtechnique amod_models_entropy nn_models_Maximum nn_etc._recognition nn_etc._entity dep_named_etc. nn_tagging_speech prep_of_part_tagging prep_including_processing_part nn_processing_language amod_processing_natural prep_to_applied_processing advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_which conj_and_classification_prediction vmod_provideageneralpurposemachinelearningtechnique_named rcmod_provideageneralpurposemachinelearningtechnique_applied prep_for_provideageneralpurposemachinelearningtechnique_prediction prep_for_provideageneralpurposemachinelearningtechnique_classification nn_models_learning amod_models_exponential amod_models_log-linear conj_and_log-linear_exponential parataxis_known_integrate prep_as_known_models advmod_known_also dep_Manning_2003 conj_and_Manning_Klein dep_Berger_Klein dep_Berger_Manning appos_Berger_1996 dep_Berger_al. nn_Berger_et dep_models_known appos_models_Berger nn_models_entropy nn_models_Models nn_models_Entropy nn_models_Maximum num_models_2 appos_entropy_ME nn_entropy_Maximum
D09-1158	N03-5008	o	We used the implementation of MaxEnt classifier described in -LRB- Manning and Klein 2003 -RRB-	dep_Manning_2003 conj_and_Manning_Klein dep_in_Klein dep_in_Manning prep_described_in vmod_classifier_described nn_classifier_MaxEnt prep_of_implementation_classifier det_implementation_the dobj_used_implementation nsubj_used_We
W04-2328	N03-5008	o	Correspondences between MALTUS and other tagsets -LRB- Klein and Soria 1998 -RRB- were also provided -LRB- Popescu-Belis 2003 -RRB-	amod_Popescu-Belis_2003 dep_provided_Popescu-Belis advmod_provided_also auxpass_provided_were nsubjpass_provided_Correspondences amod_Klein_1998 conj_and_Klein_Soria appos_tagsets_Soria appos_tagsets_Klein amod_tagsets_other conj_and_MALTUS_tagsets prep_between_Correspondences_tagsets prep_between_Correspondences_MALTUS
W04-2328	N03-5008	p	5.2 Results We use a Maximum Entropy -LRB- ME -RRB- classi er -LRB- Manning and Klein 2003 -RRB- which allows an e cient combination of many overlapping features	amod_features_overlapping amod_features_many prep_of_combination_features amod_combination_cient dep_combination_e det_combination_an dobj_allows_combination nsubj_allows_which amod_Manning_2003 conj_and_Manning_Klein dep_er_Klein dep_er_Manning nn_er_classi rcmod_Entropy_allows dep_Entropy_er appos_Entropy_ME nn_Entropy_Maximum det_Entropy_a dobj_use_Entropy nsubj_use_We rcmod_Results_use num_Results_5.2
W08-0206	N03-5008	o	For instance for Maximum Entropy I picked -LRB- Berger et al. 1996 Ratnaparkhi 1997 -RRB- for the basic theory -LRB- Ratnaparkhi 1996 -RRB- for an application -LRB- POS tagging in this case -RRB- and -LRB- Klein and Manning 2003 -RRB- for more advanced topics such as optimization and smoothing	conj_and_optimization_smoothing prep_such_as_topics_smoothing prep_such_as_topics_optimization amod_topics_advanced amod_topics_more prep_for_Klein_topics dep_Klein_2003 conj_and_Klein_Manning det_case_this prep_in_tagging_case nn_tagging_POS appos_application_tagging det_application_an dep_Ratnaparkhi_1996 amod_theory_basic det_theory_the dep_Ratnaparkhi_1997 conj_and_Berger_Manning conj_and_Berger_Klein prep_for_Berger_application appos_Berger_Ratnaparkhi prep_for_Berger_theory dep_Berger_Ratnaparkhi appos_Berger_1996 dep_Berger_al. nn_Berger_et dobj_picked_Klein dobj_picked_Berger nsubj_picked_I prep_for_picked_Entropy prep_for_picked_instance nn_Entropy_Maximum
C08-1136	N04-1035	o	From wordlevel alignments such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages -LRB- Galley et al. 2004 -RRB- or with the the word-level alignments alone without reference to external syntactic analysis -LRB- Chiang 2005 -RRB- which is the scenario we address here	advmod_address_here nsubj_address_we rcmod_scenario_address det_scenario_the cop_scenario_is nsubj_scenario_which dep_Chiang_2005 rcmod_analysis_scenario appos_analysis_Chiang amod_analysis_syntactic amod_analysis_external prep_to_reference_analysis advmod_alignments_alone amod_alignments_word-level det_alignments_the det_alignments_the pobj_with_alignments amod_Galley_2004 dep_Galley_al. nn_Galley_et prep_of_one_languages prep_without_parse_reference conj_or_parse_with dep_parse_Galley prep_for_parse_one dobj_parse_trees nsubj_parse_systems det_alignments_the prep_with_consistent_alignments preconj_consistent_either amod_rules_consistent nn_rules_grammar det_rules_the conj_and_extract_with conj_and_extract_parse dobj_extract_rules nsubj_extract_systems prep_from_extract_alignments amod_systems_such amod_alignments_wordlevel
C08-1138	N04-1035	o	It reconfirms that only allowing sibling nodes reordering as done in SCFG may be inadequate for translational equivalence modeling -LRB- Galley et al. 2004 -RRB- 4 3 -RRB- All the three models on the FBIS corpus show much lower performance than that on the other two corpora	num_corpora_two amod_corpora_other det_corpora_the prep_on_that_corpora prep_than_performance_that amod_performance_lower advmod_lower_much dep_show_performance nn_show_corpus nn_show_FBIS det_show_the prep_on_models_show num_models_three det_models_the predet_models_All dep_models_3 dep_Galley_models num_Galley_4 amod_Galley_2004 dep_Galley_al. nn_Galley_et amod_modeling_equivalence amod_modeling_translational prep_for_inadequate_modeling cop_inadequate_be aux_inadequate_may csubj_inadequate_allowing mark_inadequate_that prep_in_done_SCFG mark_done_as dep_reordering_done nn_reordering_nodes nn_reordering_sibling dobj_allowing_reordering advmod_allowing_only dep_reconfirms_Galley ccomp_reconfirms_inadequate nsubj_reconfirms_It ccomp_``_reconfirms
C08-1138	N04-1035	n	This implies that the complexity of structure divergence between two languages is higher than suggested in literature -LRB- Fox 2002 Galley et al. 2004 -RRB-	dep_al._2004 nn_al._et nn_al._Galley dep_2002_Fox prep_in_suggested_literature mark_suggested_than dep_higher_2002 ccomp_higher_suggested cop_higher_is nsubj_higher_complexity mark_higher_that num_languages_two prep_between_divergence_languages nn_divergence_structure prep_of_complexity_divergence det_complexity_the dep_implies_al. ccomp_implies_higher nsubj_implies_This ccomp_``_implies
C08-1138	N04-1035	o	However as discussed in prior arts -LRB- Galley et al. 2004 -RRB- and this paper linguistically-informed SCFG is an inadequate model for parallel corpora due to its nature that only allowing child-node reorderings	amod_reorderings_child-node dobj_allowing_reorderings advmod_allowing_only prepc_that_nature_allowing poss_nature_its prep_due_to_corpora_nature amod_corpora_parallel prep_for_model_corpora amod_model_inadequate det_model_an cop_model_is nsubj_model_paper amod_SCFG_linguistically-informed appos_paper_SCFG det_paper_this amod_Galley_2004 dep_Galley_al. nn_Galley_et amod_arts_prior conj_and_discussed_model dep_discussed_Galley prep_in_discussed_arts mark_discussed_as advcl_,_model advcl_,_discussed dep_``_However
C08-1138	N04-1035	o	Fox -LRB- 2002 -RRB- Galley et al -LRB- 2004 -RRB- and Wellington et al.	nn_al._et nn_al._Wellington dep_al_2004 dep_Galley_al nn_Galley_et conj_and_Fox_al. conj_and_Fox_Galley appos_Fox_2002 dep_``_al. dep_``_Galley dep_``_Fox
D08-1021	N04-1035	o	Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems -LRB- Galley et al. 2004 Quirk et al. 2005 -RRB-	num_Quirk_2005 nn_Quirk_al. nn_Quirk_et dep_Galley_Quirk amod_Galley_2004 dep_Galley_al. nn_Galley_et amod_systems_phrase-based agent_used_systems vmod_rules_used nn_rules_translation det_rules_the nn_subtrees_dependency nn_subtrees_constituent conj_and_constituent_dependency dep_incorporated_Galley prep_into_incorporated_rules dobj_incorporated_subtrees aux_incorporated_has nsubj_incorporated_work amod_work_recent amod_work_Other ccomp_``_incorporated
D08-1064	N04-1035	o	We trained three Arabic-English syntax-based statistical MT systems -LRB- Galley et al. 2004 Galley et al. 2006 -RRB- using max-B training -LRB- Och 2003 -RRB- one on a newswire development set one on a weblog development set and one on a combined development set containing documents from both genres	det_genres_both prep_from_documents_genres dobj_containing_documents vmod_set_containing nn_set_development amod_set_combined det_set_a prep_on_one_set nn_set_development nn_set_weblog det_set_a prep_on_one_set nn_set_development amod_set_newswire det_set_a conj_and_one_one appos_one_one prep_on_one_set appos_Och_2003 dep_training_Och amod_training_max-B dobj_using_training num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Galley_Galley appos_Galley_2004 dep_Galley_al. nn_Galley_et appos_systems_Galley nn_systems_MT amod_systems_statistical amod_systems_syntax-based amod_systems_Arabic-English num_systems_three dep_trained_one dep_trained_one vmod_trained_using dobj_trained_systems nsubj_trained_We
D08-1093	N04-1035	o	As a result they are being used in a variety of applications such as question answering -LRB- Hermjakob 2001 -RRB- speech recognition -LRB- Chelba and Jelinek 1998 -RRB- language modeling -LRB- Roark 2001 -RRB- language generation -LRB- Soricut 2006 -RRB- and most notably machine translation -LRB- Charniak et al. 2003 Galley et al. 2004 Collins et al. 2005 Marcu et al. 2006 Huang et al. 2006 Avramidis and Koehn 2008 -RRB-	dep_Avramidis_2008 conj_and_Avramidis_Koehn num_Huang_2006 nn_Huang_al. nn_Huang_et num_Marcu_2006 nn_Marcu_al. nn_Marcu_et num_Collins_2005 nn_Collins_al. nn_Collins_et dep_Galley_Koehn dep_Galley_Avramidis conj_Galley_Huang conj_Galley_Marcu conj_Galley_Collins num_Galley_2004 nn_Galley_al. nn_Galley_et dep_Charniak_Galley appos_Charniak_2003 dep_Charniak_al. nn_Charniak_et dep_translation_Charniak nn_translation_machine dep_notably_translation advmod_notably_most dep_Soricut_2006 appos_generation_Soricut nn_generation_language dep_Roark_2001 appos_modeling_Roark nn_modeling_language dep_Chelba_1998 conj_and_Chelba_Jelinek dep_recognition_Jelinek dep_recognition_Chelba nn_recognition_speech dep_Hermjakob_2001 conj_and_answering_notably conj_and_answering_generation conj_and_answering_modeling conj_and_answering_recognition dep_answering_Hermjakob dep_question_notably dep_question_generation dep_question_modeling dep_question_recognition dep_question_answering prep_such_as_applications_question prep_of_variety_applications det_variety_a prep_in_used_variety auxpass_used_being aux_used_are nsubjpass_used_they prep_as_used_result det_result_a advcl_``_used
D09-1076	N04-1035	n	Current tree-based models that integrate linguistics and statistics such as GHKM -LRB- Galley et al. 2004 -RRB- are not able to generalize well from a single phrase pair	nn_pair_phrase amod_pair_single det_pair_a prep_from_generalize_pair advmod_generalize_well aux_generalize_to xcomp_able_generalize neg_able_not cop_able_are nsubj_able_models amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_GHKM_Galley conj_and_linguistics_statistics dobj_integrate_statistics dobj_integrate_linguistics nsubj_integrate_that prep_such_as_models_GHKM rcmod_models_integrate amod_models_tree-based amod_models_Current
D09-1108	N04-1035	o	The tree-to-string model -LRB- Galley et al. 2004 Liu et al. 2006 -RRB- views the translation as a structure mapping process which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules finally combines these target translations into a complete sentence	amod_sentence_complete det_sentence_a nn_translations_target det_translations_these prep_into_combines_sentence dobj_combines_translations advmod_combines_finally nsubj_combines_process mark_combines_as nn_rules_translation dobj_using_rules nn_translation_target amod_translation_corresponding poss_translation_its nn_fragment_tree det_fragment_each vmod_maps_using prep_into_maps_translation dobj_maps_fragment advmod_maps_then nsubj_maps_which nn_fragments_tree amod_fragments_many nn_tree_syntax nn_tree_source det_tree_the conj_and_breaks_maps prep_into_breaks_fragments dobj_breaks_tree advmod_breaks_first nsubj_breaks_which rcmod_process_maps rcmod_process_breaks nn_process_mapping nn_process_structure det_process_a det_translation_the advcl_views_combines dobj_views_translation nsubj_views_model num_al._2006 nn_al._et nn_al._Liu dep_Galley_al. dep_Galley_2004 dep_Galley_al. nn_Galley_et appos_model_Galley amod_model_tree-to-string det_model_The
D09-1108	N04-1035	p	1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation -LRB- SMT -RRB- -LRB- Galley et al. 2004 Liu et al. 2006 2007 Zhang et al. 2007 2008a Mi et al. 2008 Mi and Huang 2008 Zhang et al. 2009 -RRB-	num_Zhang_2009 nn_Zhang_al. nn_Zhang_et num_Huang_2008 num_Mi_2008 nn_Mi_al. nn_Mi_et appos_Zhang_2008a num_Zhang_2007 nn_Zhang_al. nn_Zhang_et conj_and_Liu_Huang conj_and_Liu_Mi conj_and_Liu_Mi conj_and_Liu_Zhang num_Liu_2007 num_Liu_2006 nn_Liu_al. nn_Liu_et dep_Galley_Zhang dep_Galley_Huang dep_Galley_Mi dep_Galley_Mi dep_Galley_Zhang dep_Galley_Liu appos_Galley_2004 dep_Galley_al. nn_Galley_et appos_translation_SMT nn_translation_machine amod_translation_statistical amod_success_great dep_achieved_Galley prep_in_achieved_translation dobj_achieved_success aux_achieved_has nsubj_achieved_method nn_method_translation amod_method_syntax-based amod_method_linguistically-motivated advmod_method_Recently dep_method_Introduction num_Introduction_1
D09-1127	N04-1035	o	We envision the use of a clever datastructure would reduce the complexity but leave this to future work as the experiments -LRB- Table 8 -RRB- show that 5Our definition implies that we only consider faithful spans to be contiguous -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_contiguous_Galley cop_contiguous_be aux_contiguous_to vmod_spans_contiguous amod_spans_faithful dobj_consider_spans advmod_consider_only nsubj_consider_we mark_consider_that ccomp_implies_consider nsubj_implies_definition mark_implies_that nn_definition_5Our ccomp_show_implies nsubj_show_experiments mark_show_as num_Table_8 appos_experiments_Table det_experiments_the amod_work_future prep_to_leave_work dobj_leave_this nsubj_leave_We det_complexity_the dobj_reduce_complexity aux_reduce_would nsubj_reduce_use amod_datastructure_clever det_datastructure_a prep_of_use_datastructure det_use_the advcl_envision_show conj_but_envision_leave ccomp_envision_reduce nsubj_envision_We ccomp_``_leave ccomp_``_envision
D09-1127	N04-1035	o	For example Smith and Smith -LRB- 2004 -RRB- and Burkett and Klein -LRB- 2008 -RRB- show that joint parsing -LRB- or reranking -RRB- on a bitext improves accuracies on either or both sides by leveraging bilingual constraints which is very promising for syntax-based machine translation which requires -LRB- good-quality -RRB- parse trees for rule extraction -LRB- Galley et al. 2004 Mi and Huang 2008 -RRB-	dep_Mi_2008 conj_and_Mi_Huang dep_Galley_Huang dep_Galley_Mi appos_Galley_2004 dep_Galley_al. nn_Galley_et nn_extraction_rule dep_trees_Galley prep_for_trees_extraction dobj_parse_trees dep_parse_good-quality dep_requires_parse nsubj_requires_which rcmod_translation_requires nn_translation_machine amod_translation_syntax-based prep_for_promising_translation advmod_promising_very cop_promising_is nsubj_promising_which rcmod_constraints_promising amod_constraints_bilingual amod_constraints_leveraging det_sides_both conj_or_either_sides prep_on_accuracies_sides prep_on_accuracies_either prep_by_improves_constraints dobj_improves_accuracies nsubj_improves_parsing mark_improves_that det_bitext_a cc_reranking_or prep_on_parsing_bitext appos_parsing_reranking amod_parsing_joint ccomp_show_improves nsubj_show_Klein nsubj_show_Burkett nsubj_show_Smith nsubj_show_Smith prep_for_show_example appos_Klein_2008 appos_Smith_2004 conj_and_Smith_Klein conj_and_Smith_Burkett conj_and_Smith_Smith
D09-1127	N04-1035	o	To make things worse languages are non-isomorphic i.e. there is no 1to-1 mapping between tree nodes thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars -LRB- Eisner 2003 Galley et al. 2004 -RRB-	num_Galley_2004 nn_Galley_al. nn_Galley_et dep_Eisner_Galley dep_Eisner_2003 appos_grammars_Eisner nn_grammars_tree-substitution amod_grammars_synchronous prep_such_as_formalisms_grammars amod_formalisms_expressive amod_formalisms_more dobj_use_formalisms aux_use_to aux_use_has num_practice_one nn_nodes_tree prep_between_mapping_nodes nn_mapping_1to-1 neg_mapping_no advcl_is_use prep_in_is_practice advmod_is_thus nsubj_is_mapping expl_is_there parataxis_non-isomorphic_is dep_non-isomorphic_i.e. cop_non-isomorphic_are nsubj_non-isomorphic_languages advcl_non-isomorphic_make nsubj_worse_things xcomp_make_worse aux_make_To ccomp_``_non-isomorphic
D09-1136	N04-1035	o	1313 E2C C2E Union Heuristic w / Big 13.37 12.66 14.55 14.28 w/o Big 13.20 12.62 14.53 14.21 Table 3 BLEU-4 scores -LRB- test set -RRB- of systems based on GIZA + + word alignments 5 6 7 8 BLEU-4 14.27 14.42 14.43 14.45 14.55 Table 4 BLEU-4 scores -LRB- test set -RRB- of the union alignment using TTS templates up to a certain size in terms of the number of leaves in their LHSs 4.1 Baseline Systems GHKM -LRB- Galley et al. 2004 -RRB- is used to generate the baseline TTS templates based on the word alignments computed using GIZA + + and different combination methods including union and the diagonal growing heuristic -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_heuristic_growing amod_heuristic_diagonal det_heuristic_the conj_and_union_heuristic nn_methods_combination amod_methods_different dep_GIZA_Koehn prep_including_GIZA_heuristic prep_including_GIZA_union conj_and_GIZA_methods conj_+_GIZA_+ dobj_using_methods dobj_using_+ dobj_using_GIZA xcomp_computed_using vmod_alignments_computed nn_alignments_word det_alignments_the prep_on_based_alignments vmod_templates_based nn_templates_TTS nn_templates_baseline det_templates_the dobj_generate_templates aux_generate_to xcomp_used_generate auxpass_used_is nsubjpass_used_scores amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_GHKM_Galley nn_GHKM_Systems nn_GHKM_Baseline num_GHKM_4.1 dep_LHSs_GHKM poss_LHSs_their prep_in_leaves_LHSs prep_of_number_leaves det_number_the prep_of_terms_number amod_size_certain det_size_a pobj_to_size pcomp_up_to nn_templates_TTS prep_using_up dobj_using_templates nn_alignment_union det_alignment_the vmod_test_set prep_in_scores_terms vmod_scores_using prep_of_scores_alignment dep_scores_test nn_scores_BLEU-4 num_Table_4 num_Table_14.55 num_Table_14.45 dep_14.43_Table dep_14.42_14.43 number_14.42_14.27 dep_BLEU-4_14.42 dep_8_BLEU-4 dep_7_8 number_7_6 number_7_5 nn_alignments_word dep_GIZA_7 conj_+_GIZA_alignments prep_on_based_alignments prep_on_based_GIZA vmod_test_set vmod_scores_based prep_of_scores_systems dep_scores_test nn_scores_BLEU-4 num_Table_3 num_Table_14.21 num_Table_14.53 num_Table_12.62 number_12.62_13.20 parataxis_Big_used dep_Big_scores dep_Big_Table dep_w/o_Big dep_14.28_w/o number_14.28_14.55 dep_14.28_12.66 dep_14.28_Big number_12.66_13.37 dep_w_14.28 nn_w_Heuristic nn_w_Union nn_w_C2E nn_w_E2C num_w_1313 dep_``_w
D09-1136	N04-1035	p	This algorithm is referred to as GHKM -LRB- Galley et al. 2004 -RRB- and is widely used in SSMT systems -LRB- Galley et al. 2006 Liu et al. 2006 Huang et al. 2006 -RRB-	num_Huang_2006 nn_Huang_al. nn_Huang_et dep_Liu_Huang num_Liu_2006 nn_Liu_al. nn_Liu_et dep_Galley_Liu appos_Galley_2006 dep_Galley_al. nn_Galley_et nn_systems_SSMT prep_in_used_systems advmod_used_widely auxpass_used_is nsubjpass_used_algorithm amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_GHKM_Galley dep_referred_Galley conj_and_referred_used pobj_referred_GHKM prepc_as_to_referred_as auxpass_referred_is nsubjpass_referred_algorithm det_algorithm_This
N06-1001	N04-1035	o	To this end the translational correspondence is described within a translation rule i.e. -LRB- Galley et al. 2004 -RRB- -LRB- or a synchronous production -RRB- rather than a translational phrase pair and the training data will be derivation forests instead of the phrase-aligned bilingual corpus	amod_corpus_bilingual amod_corpus_phrase-aligned det_corpus_the conj_negcc_forests_corpus nn_forests_derivation cop_forests_be aux_forests_will nsubj_forests_data nn_data_training det_data_the nn_pair_phrase amod_pair_translational det_pair_a amod_production_synchronous det_production_a conj_or_Galley_production dep_Galley_2004 dep_Galley_al. nn_Galley_et nn_rule_translation det_rule_a conj_and_described_corpus conj_and_described_forests conj_negcc_described_pair dep_described_production dep_described_Galley dep_described_i.e. prep_within_described_rule auxpass_described_is nsubjpass_described_correspondence prep_to_described_end amod_correspondence_translational det_correspondence_the det_end_this
N06-1031	N04-1035	o	Step 2 involves extracting minimal xRS rules -LRB- Galley et al. 2004 -RRB- from the set of string/tree/alignments triplets	nn_triplets_string/tree/alignments prep_of_set_triplets det_set_the amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_rules_xRS amod_rules_minimal dobj_extracting_rules prep_from_involves_set dep_involves_Galley xcomp_involves_extracting nsubj_involves_Step num_Step_2
N06-1031	N04-1035	o	In this work we employ a syntax-based model that applies a series of tree/string -LRB- xRS -RRB- rules -LRB- Galley et al. 2004 Graehl and Knight 2004 -RRB- to a source language string to produce a target language phrase structure tree	nn_tree_structure nn_tree_phrase nn_tree_language nn_tree_target det_tree_a dobj_produce_tree aux_produce_to vmod_string_produce nn_string_language nn_string_source det_string_a amod_Graehl_2004 conj_and_Graehl_Knight dep_Galley_Knight dep_Galley_Graehl appos_Galley_2004 dep_Galley_al. nn_Galley_et appos_rules_Galley nn_rules_xRS nn_rules_tree/string prep_of_series_rules det_series_a prep_to_applies_string dobj_applies_series nsubj_applies_that rcmod_model_applies amod_model_syntax-based det_model_a dobj_employ_model nsubj_employ_we prep_in_employ_work det_work_this
N06-1033	N04-1035	o	1 Introduction Several recent syntax-based models for machine translation -LRB- Chiang 2005 Galley et al. 2004 -RRB- can be seen as instances of the general framework of synchronous grammars and tree transducers	nn_transducers_tree conj_and_grammars_transducers amod_grammars_synchronous prep_of_framework_transducers prep_of_framework_grammars amod_framework_general det_framework_the prep_of_instances_framework prep_as_seen_instances auxpass_seen_be aux_seen_can nsubjpass_seen_models dep_seen_Introduction num_Galley_2004 nn_Galley_al. nn_Galley_et dep_Chiang_Galley appos_Chiang_2005 appos_translation_Chiang nn_translation_machine prep_for_models_translation amod_models_syntax-based amod_models_recent amod_models_Several num_Introduction_1
N06-3004	N04-1035	p	However to be more expressive and flexible it is often easier to start with a general SCFG or tree-transducer -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et conj_or_SCFG_tree-transducer amod_SCFG_general det_SCFG_a prep_with_start_tree-transducer prep_with_start_SCFG aux_start_to dep_easier_Galley xcomp_easier_start advmod_easier_often cop_easier_is nsubj_easier_it ccomp_easier_flexible ccomp_easier_expressive conj_and_expressive_flexible advmod_expressive_more cop_expressive_be aux_expressive_to advmod_expressive_However
N06-3004	N04-1035	p	Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization -LRB- see Table 1 -RRB- in a stateof-the-art syntax-based machine translation system -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_system_translation nn_system_machine amod_system_syntax-based amod_system_stateof-the-art det_system_a dep_1_Table dep_see_Galley prep_in_see_system dep_see_1 amod_binarization_monolingual prep_over_speed_binarization conj_and_speed_accuracy det_speed_the dep_improves_see dobj_improves_accuracy dobj_improves_speed advmod_improves_significantly dep_set_improves nsubj_set_rule mark_set_that amod_rule_resulting det_rule_the ccomp_show_set nsubj_show_Experiments
N06-3004	N04-1035	o	These rules can be learned from a parallel corpus using English parsetrees Chinese strings and word alignment -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_alignment_Galley nn_alignment_word amod_strings_Chinese conj_and_parsetrees_alignment conj_and_parsetrees_strings nn_parsetrees_English dobj_using_alignment dobj_using_strings dobj_using_parsetrees amod_corpus_parallel det_corpus_a xcomp_learned_using prep_from_learned_corpus auxpass_learned_be aux_learned_can nsubjpass_learned_rules det_rules_These ccomp_``_learned
N09-1025	N04-1035	o	From this data we use the the GHKM minimal-rule extraction algorithm of -LRB- Galley et al. 2004 -RRB- to yield rules like NP-C -LRB- x0 NPB PP -LRB- IN -LRB- of x1 NPB -RRB- -RRB- $ x1 de x0 Though this rule can be used in either direction here we use it right-to-left -LRB- Chinese to English -RRB-	prep_to_Chinese_English dep_right-to-left_Chinese amod_it_right-to-left dobj_use_it nsubj_use_we advmod_use_here det_direction_either prep_in_used_direction auxpass_used_be aux_used_can nsubjpass_used_rule mark_used_Though det_rule_this prep_de_$_x0 num_$_x1 dep_of_NPB pobj_of_x1 rcmod_IN_use advcl_IN_used pobj_IN_$ dep_IN_of dep_PP_IN nn_PP_NPB dep_x0_PP dep_NP-C_x0 dep_like_NP-C prep_rules_like dobj_yield_rules aux_yield_to amod_Galley_2004 dep_Galley_al. nn_Galley_et prep_of_algorithm_Galley nn_algorithm_extraction nn_algorithm_minimal-rule nn_algorithm_GHKM det_algorithm_the det_algorithm_the vmod_use_yield dobj_use_algorithm nsubj_use_we prep_from_use_data det_data_this
N09-1026	N04-1035	o	Meanwhile translation grammars have grown in complexity from simple inversion transduction grammars -LRB- Wu 1997 -RRB- to general tree-to-string transducers -LRB- Galley et al. 2004 -RRB- and have increased in size by including more synchronous tree fragments -LRB- Galley et al. 2006 Marcuetal. ,2006 DeNeefeetal. ,2007 -RRB-	num_DeNeefeetal._,2007 num_Marcuetal._,2006 dep_Galley_DeNeefeetal. dep_Galley_Marcuetal. appos_Galley_2006 dep_Galley_al. nn_Galley_et appos_fragments_Galley nn_fragments_tree amod_fragments_synchronous advmod_fragments_more pobj_including_fragments prepc_by_increased_including prep_in_increased_size aux_increased_have nsubj_increased_grammars amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_transducers_tree-to-string amod_transducers_general dep_Wu_1997 appos_grammars_Wu nn_grammars_transduction nn_grammars_inversion amod_grammars_simple prep_to_complexity_transducers prep_from_complexity_grammars conj_and_grown_increased dep_grown_Galley prep_in_grown_complexity aux_grown_have nsubj_grown_grammars advmod_grown_Meanwhile nn_grammars_translation ccomp_``_increased ccomp_``_grown
N09-1026	N04-1035	o	stituent alignments -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_alignments_Galley amod_alignments_stituent
N09-1058	N04-1035	o	Language modeling -LRB- Chen and Goodman 1996 -RRB- noun-clustering -LRB- Ravichandran et al. 2005 -RRB- constructing syntactic rules for SMT -LRB- Galley et al. 2004 -RRB- and finding analogies -LRB- Turney 2008 -RRB- are examples of some of the problems where we need to compute relative frequencies	amod_frequencies_relative dobj_compute_frequencies aux_compute_to xcomp_need_compute nsubj_need_we advmod_need_where rcmod_problems_need det_problems_the prep_of_some_problems prep_of_examples_some cop_examples_are nsubj_examples_finding nsubj_examples_modeling amod_Turney_2008 appos_analogies_Turney dobj_finding_analogies amod_Galley_2004 dep_Galley_al. nn_Galley_et prep_for_rules_SMT amod_rules_syntactic dobj_constructing_rules dep_al._2005 nn_al._et advmod_Ravichandran_al. dep_noun-clustering_Ravichandran dep_Chen_1996 conj_and_Chen_Goodman conj_and_modeling_finding dep_modeling_Galley vmod_modeling_constructing appos_modeling_noun-clustering appos_modeling_Goodman appos_modeling_Chen nn_modeling_Language
P05-1066	N04-1035	o	2.1.2 Research on Syntax-Based SMT A number of researchers -LRB- Alshawi 1996 Wu 1997 Yamada and Knight 2001 Gildea 2003 Melamed 2004 Graehl and Knight 2004 Galley et al. 2004 -RRB- have proposed models where the translation process involves syntactic representations of the source and/or target languages	nn_languages_target conj_and/or_source_languages det_source_the prep_of_representations_languages prep_of_representations_source amod_representations_syntactic dobj_involves_representations nsubj_involves_process advmod_involves_where nn_process_translation det_process_the rcmod_models_involves amod_models_proposed dobj_have_models num_Galley_2004 nn_Galley_al. nn_Galley_et num_Graehl_2004 conj_and_Graehl_Knight num_Melamed_2004 num_Gildea_2003 num_Wu_1997 dep_Alshawi_Galley conj_and_Alshawi_Knight conj_and_Alshawi_Graehl conj_and_Alshawi_Melamed conj_and_Alshawi_Gildea conj_and_Alshawi_2001 conj_and_Alshawi_Knight conj_and_Alshawi_Yamada conj_and_Alshawi_Wu appos_Alshawi_1996 appos_researchers_Graehl appos_researchers_Melamed appos_researchers_Gildea appos_researchers_2001 appos_researchers_Knight appos_researchers_Yamada appos_researchers_Wu appos_researchers_Alshawi prep_of_number_researchers det_number_A nn_number_SMT amod_number_Syntax-Based dep_Research_have prep_on_Research_number num_Research_2.1.2 dep_``_Research
P05-3025	N04-1035	o	-LRB- 2004 -RRB- describe how to learn hundreds of millions of treetransformation rules from a parsed aligned Chinese/English corpus and Galley et al.	nn_al._et nn_al._Galley nn_corpus_Chinese/English amod_corpus_aligned amod_corpus_parsed det_corpus_a nn_rules_treetransformation prep_of_millions_rules prep_of_hundreds_millions conj_and_learn_al. prep_from_learn_corpus dobj_learn_hundreds aux_learn_to advmod_learn_how ccomp_describe_al. ccomp_describe_learn nsubj_describe_2004
P06-1077	N04-1035	o	Similarly to -LRB- Galley et al. 2004 -RRB- the tree-to-string alignment templates discussed in this paper are actually transformation rules	nn_rules_transformation advmod_rules_actually cop_rules_are nsubj_rules_templates prep_rules_to det_paper_this prep_in_discussed_paper vmod_templates_discussed nn_templates_alignment amod_templates_tree-to-string det_templates_the amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_to_Galley advmod_to_Similarly
P06-1121	N04-1035	o	We contrast our work with -LRB- Galley et al. 2004 -RRB- highlight some severe limitations of probability estimates computed from single derivations and demonstrate that it is critical to account for many derivations for each sentence pair	nn_pair_sentence det_pair_each prep_for_derivations_pair amod_derivations_many prep_for_account_derivations aux_account_to xcomp_critical_account cop_critical_is nsubj_critical_it mark_critical_that ccomp_demonstrate_critical amod_derivations_single conj_and_computed_demonstrate prep_from_computed_derivations dep_estimates_demonstrate dep_estimates_computed nsubj_estimates_limitations prep_of_limitations_probability amod_limitations_severe det_limitations_some ccomp_highlight_estimates amod_Galley_highlight amod_Galley_2004 dep_Galley_al. nn_Galley_et prep_with_work_Galley poss_work_our dobj_contrast_work nsubj_contrast_We
P06-1121	N04-1035	n	Finally we show that our contextually richer rules provide a 3.63 BLEU point increase over those of -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_of_Galley prep_those_of prep_over_increase_those nn_increase_point nn_increase_BLEU num_increase_3.63 det_increase_a dobj_provide_increase nsubj_provide_rules mark_provide_that amod_rules_richer poss_rules_our advmod_richer_contextually ccomp_show_provide nsubj_show_we advmod_show_Finally
P06-1121	N04-1035	o	8 Conclusions In this paper we developed probability models for the multi-level transfer rules presented in -LRB- Galley et al. 2004 -RRB- showed how to acquire larger rules that crucially condition on more syntactic context and how to pack multiple derivations including interpretations of unaligned words into derivation forests	nn_forests_derivation amod_words_unaligned prep_of_interpretations_words amod_derivations_multiple prep_into_pack_forests prep_including_pack_interpretations dobj_pack_derivations aux_pack_to advmod_pack_how nsubj_pack_we amod_context_syntactic advmod_syntactic_more prep_on_condition_context advmod_condition_crucially nsubj_condition_that rcmod_rules_condition amod_rules_larger dobj_acquire_rules aux_acquire_to advmod_acquire_how ccomp_showed_acquire nsubj_showed_we amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_in_Galley prep_presented_in vmod_rules_presented nn_rules_transfer amod_rules_multi-level det_rules_the prep_for_models_rules nn_models_probability conj_and_developed_pack conj_and_developed_showed dobj_developed_models nsubj_developed_we dep_developed_Conclusions det_paper_this prep_in_Conclusions_paper num_Conclusions_8
P06-1121	N04-1035	n	We presented some theoretical arguments for not limiting extraction to minimal rules validated them on concrete examples and presented experiments showing that contextually richer rules provide a 3.63 BLEU point increase over the minimal rules of -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_of_Galley prep_rules_of amod_rules_minimal det_rules_the prep_over_increase_rules nn_increase_point nn_increase_BLEU num_increase_3.63 det_increase_a dobj_provide_increase nsubj_provide_rules mark_provide_that amod_rules_richer advmod_richer_contextually ccomp_showing_provide vmod_experiments_showing dobj_presented_experiments nsubj_presented_We amod_examples_concrete prep_on_validated_examples dobj_validated_them nsubj_validated_We amod_rules_minimal prep_to_limiting_rules dobj_limiting_extraction neg_limiting_not prepc_for_arguments_limiting amod_arguments_theoretical det_arguments_some conj_and_presented_presented conj_and_presented_validated dobj_presented_arguments nsubj_presented_We
P06-1121	N04-1035	o	Formally transformational rules ri presented in -LRB- Galley et al. 2004 -RRB- are equivalent to 1-state xRs transducers mapping a given pattern -LRB- subtree to match in pi -RRB- to a right hand side string	nn_string_side nn_string_hand amod_string_right det_string_a prep_in_match_pi aux_match_to vmod_subtree_match appos_pattern_subtree amod_pattern_given det_pattern_a prep_to_mapping_string dobj_mapping_pattern vmod_transducers_mapping nn_transducers_xRs amod_transducers_1-state prep_to_equivalent_transducers cop_equivalent_are nsubj_equivalent_rules advmod_equivalent_Formally amod_Galley_2004 dep_Galley_al. nn_Galley_et prep_in_presented_Galley dep_ri_presented dep_rules_ri amod_rules_transformational
P06-1121	N04-1035	o	In this paper we take the framework for acquiring multi-level syntactic translation rules of -LRB- Galley et al. 2004 -RRB- from aligned tree-string pairs and present two main extensions of their approach first instead of merely computing a single derivation that minimally explains a sentence pair we construct a large number of derivations that include contextually richer rules and account for multiple interpretations of unaligned words	amod_words_unaligned prep_of_interpretations_words amod_interpretations_multiple prep_for_account_interpretations amod_rules_richer advmod_rules_contextually dobj_include_rules nsubj_include_that rcmod_derivations_include prep_of_number_derivations amod_number_large det_number_a conj_and_construct_account dobj_construct_number nsubj_construct_we prepc_instead_of_construct_computing advmod_construct_first nn_pair_sentence det_pair_a dobj_explains_pair advmod_explains_minimally nsubj_explains_that rcmod_derivation_explains amod_derivation_single det_derivation_a dobj_computing_derivation advmod_computing_merely poss_approach_their dep_extensions_account dep_extensions_construct prep_of_extensions_approach amod_extensions_main num_extensions_two amod_extensions_present amod_pairs_tree-string amod_pairs_aligned amod_Galley_2004 dep_Galley_al. nn_Galley_et prep_of_rules_Galley nn_rules_translation nn_rules_syntactic amod_rules_multi-level prep_from_acquiring_pairs dobj_acquiring_rules det_framework_the conj_and_take_extensions prepc_for_take_acquiring dobj_take_framework nsubj_take_we prep_in_take_paper det_paper_this
P06-1123	N04-1035	o	We would expect the opposite effect with hand-aligned data -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et amod_data_hand-aligned prep_with_effect_data amod_effect_opposite det_effect_the dep_expect_Galley dobj_expect_effect aux_expect_would nsubj_expect_We
P06-1123	N04-1035	o	Analogous techniques for tree-structured translation models involve either allowing each nonterminal to generate both terminals and other nonterminals -LRB- Groves et al. 2004 Chiang 2005 -RRB- or given a constraining parse tree to flatten it -LRB- Fox 2002 Zens and Ney 2003 Galley et al. 2004 -RRB-	num_Galley_2004 nn_Galley_al. nn_Galley_et conj_and_Zens_Galley conj_and_Zens_2003 conj_and_Zens_Ney dep_Fox_Galley dep_Fox_2003 dep_Fox_Ney dep_Fox_Zens appos_Fox_2002 dep_flatten_Fox dobj_flatten_it aux_flatten_to nn_tree_parse amod_tree_constraining det_tree_a pobj_given_tree num_Chiang_2005 vmod_Groves_flatten conj_or_Groves_given dep_Groves_Chiang appos_Groves_2004 dep_Groves_al. nn_Groves_et amod_nonterminals_other conj_and_terminals_nonterminals preconj_terminals_both dep_generate_given dep_generate_Groves dobj_generate_nonterminals dobj_generate_terminals aux_generate_to det_nonterminal_each vmod_allowing_generate dobj_allowing_nonterminal preconj_allowing_either xcomp_involve_allowing nsubj_involve_techniques nn_models_translation amod_models_tree-structured prep_for_techniques_models amod_techniques_Analogous
P08-1023	N04-1035	o	Depending on the type of input these efforts can be divided into two broad categories the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar -LRB- Wu 1997 Chiang 2005 Galley et al. 2006 -RRB- and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string -LRB- Lin 2004 Ding and Palmer 2005 Quirk et al. 2005 Liu et al. 2006 Huang et al. 2006 -RRB-	num_Huang_2006 nn_Huang_al. nn_Huang_et num_Liu_2006 nn_Liu_al. nn_Liu_et num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Ding_2005 conj_and_Ding_Palmer dep_Lin_Huang dep_Lin_Liu dep_Lin_Quirk dep_Lin_Palmer dep_Lin_Ding num_Lin_2004 appos_tree_Lin conj_or_tree_string nn_tree_target det_tree_a prep_into_converted_string prep_into_converted_tree advmod_converted_directly auxpass_converted_be aux_converted_to vmod_tree_converted nn_tree_parse det_tree_a advmod_tree_already cop_tree_is nsubj_tree_input poss_input_whose rcmod_systems_tree amod_systems_tree-based det_systems_the num_Galley_2006 nn_Galley_al. nn_Galley_et conj_and_Chiang_systems dep_Chiang_Galley num_Chiang_2005 dep_Wu_systems dep_Wu_Chiang appos_Wu_1997 dep_grammar_Wu amod_grammar_synchronous det_grammar_a prep_by_translated_grammar conj_and_parsed_translated advmod_parsed_simultaneously auxpass_parsed_be aux_parsed_to vmod_string_translated vmod_string_parsed det_string_a cop_string_is nsubj_string_input poss_input_whose rcmod_systems_string amod_systems_string-based det_systems_the amod_categories_broad num_categories_two dep_divided_systems prep_into_divided_categories auxpass_divided_be aux_divided_can nsubjpass_divided_efforts pobj_divided_type prepc_depending_on_divided_on det_efforts_these prep_of_type_input det_type_the
P09-1020	N04-1035	o	4 Training This section discusses how to extract our translation rules given a triple nullnull null null nullnull As we know the traditional tree-to-string rules can be easily extracted from nullnull null null nullnull using the algorithm of Mi and Huang -LRB- 2008 -RRB- 2 We would like 2 Mi and Huang -LRB- 2008 -RRB- extend the tree-based rule extraction algorithm -LRB- Galley et al. 2004 -RRB- to forest-based by introducing non-deterministic mechanism	amod_mechanism_non-deterministic dobj_introducing_mechanism amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_algorithm_extraction nn_algorithm_rule amod_algorithm_tree-based det_algorithm_the prepc_by_extend_introducing prep_to_extend_forest-based dep_extend_Galley dobj_extend_algorithm appos_Huang_2008 conj_and_Mi_Huang num_Mi_2 dep_like_extend dobj_like_Huang dobj_like_Mi aux_like_would nsubj_like_We dep_2_2008 num_Huang_2 conj_and_Mi_Huang prep_of_algorithm_Huang prep_of_algorithm_Mi det_algorithm_the dobj_using_algorithm vmod_nullnull_using parataxis_null_like appos_null_nullnull amod_null_null prep_from_extracted_nullnull advmod_extracted_easily auxpass_extracted_be aux_extracted_can nsubjpass_extracted_rules advcl_extracted_know amod_rules_tree-to-string amod_rules_traditional det_rules_the nsubj_know_we mark_know_As amod_null_null appos_nullnull_nullnull conj_nullnull_null amod_nullnull_triple det_nullnull_a pobj_given_nullnull prep_rules_given nn_rules_translation poss_rules_our dobj_extract_rules aux_extract_to advmod_extract_how dobj_discusses_null parataxis_discusses_extracted ccomp_discusses_extract nsubj_discusses_section det_section_This amod_section_Training num_section_4
P09-1020	N04-1035	o	From the above discussion we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string -LRB- Galley et al. 2004 Liu et al. 2006 -RRB- rules	amod_rules_tree-to-string num_Liu_2006 nn_Liu_al. nn_Liu_et dep_Galley_Liu dep_Galley_2004 dep_Galley_al. nn_Galley_et dep_tree-to-string_Galley dobj_learn_rules advmod_learn_only aux_learn_can nsubj_learn_that rcmod_unit_learn nn_unit_translation amod_unit_basic det_unit_the amod_sub-tree_single prep_as_uses_unit dobj_uses_sub-tree nsubj_uses_model mark_uses_while amod_model_forestbased det_model_the nn_input_translation amod_tree_single advcl_uses_uses prep_as_uses_input dobj_uses_tree nsubj_uses_method mark_uses_that amod_method_sequence-based nn_method_tree amod_method_traditional ccomp_see_uses aux_see_can nsubj_see_we prep_from_see_discussion amod_discussion_above det_discussion_the
P09-1063	N04-1035	o	While Galley -LRB- 2004 -RRB- describes extracting treeto-string rules from 1-best trees Mi and Huang et al.	dep_Huang_al. nn_Huang_et conj_and_trees_Huang conj_and_trees_Mi amod_trees_1-best prep_from_rules_Huang prep_from_rules_Mi prep_from_rules_trees amod_rules_treeto-string dobj_extracting_rules xcomp_describes_extracting nsubj_describes_Galley mark_describes_While appos_Galley_2004 advcl_``_describes
P09-1064	N04-1035	o	The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_parse_Galley amod_parse_syntactic det_parse_a prep_with_annotated_parse cop_annotated_is nsubj_annotated_sentence advmod_annotated_where nn_sentence_target det_sentence_the rcmod_pairs_annotated nn_pairs_sentence amod_pairs_aligned nn_pairs_word prep_from_extracted_pairs auxpass_extracted_are nsubjpass_extracted_rules nn_rules_grammar amod_rules_synchronous det_rules_The ccomp_``_extracted
P09-1108	N04-1035	o	3.3 Tree Transducer Grammars Syntactic machine translation -LRB- Galley et al. 2004 -RRB- uses tree transducer grammars to translate sentences	dobj_translate_sentences aux_translate_to nn_grammars_transducer nn_grammars_tree vmod_uses_translate dobj_uses_grammars nsubj_uses_translation amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_translation_Galley nn_translation_machine nn_translation_Syntactic nn_translation_Grammars nn_translation_Transducer nn_translation_Tree num_translation_3.3
W05-0803	N04-1035	o	-LRB- Gildea 2003 -RRB- and -LRB- Galley et al. 2004 -RRB- discuss different ways of generalizing the tree-level crosslinguistic correspondence relation so it is not confined to single tree nodes thereby avoiding a continuity assumption	nn_assumption_continuity det_assumption_a dobj_avoiding_assumption advmod_avoiding_thereby nn_nodes_tree amod_nodes_single xcomp_confined_avoiding prep_to_confined_nodes neg_confined_not auxpass_confined_is nsubjpass_confined_it mark_confined_so nn_relation_correspondence amod_relation_crosslinguistic amod_relation_tree-level det_relation_the dobj_generalizing_relation prepc_of_ways_generalizing amod_ways_different advcl_discuss_confined dobj_discuss_ways nsubj_discuss_Galley nsubj_discuss_Gildea amod_Galley_2004 dep_Galley_al. nn_Galley_et conj_and_Gildea_Galley amod_Gildea_2003
W06-3601	N04-1035	o	Besides being linguistically motivated the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate -LRB- Fox 2002 Galley et al. 2004 -RRB-	num_Galley_2004 nn_Galley_al. nn_Galley_et dep_Fox_Galley dep_Fox_2002 dep_inadequate_Fox advmod_inadequate_often cop_inadequate_are nsubj_inadequate_rules amod_rules_one-level det_rules_that rcmod_MT_inadequate prep_in_findings_MT amod_findings_empirical agent_supported_findings advmod_supported_also auxpass_supported_is nsubjpass_supported_need prepc_besides_supported_motivated prep_for_need_EDL det_need_the advmod_motivated_linguistically auxpass_motivated_being
W07-1512	N04-1035	n	However many of these models are not applicable to parallel treebanks because they assume translation units where either the source text the target text or both are represented as word sequences without any syntactic structure -LRB- Galley et al. 2004 Marcu et al. 2006 Koehn et al. 2003 -RRB-	num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Koehn num_Marcu_2006 nn_Marcu_al. nn_Marcu_et dep_Galley_Marcu appos_Galley_2004 dep_Galley_al. nn_Galley_et nn_structure_syntactic det_structure_any nn_sequences_word prep_without_represented_structure prep_as_represented_sequences auxpass_represented_are nsubjpass_represented_text advmod_represented_where conj_or_text_both nn_text_target det_text_the appos_text_both appos_text_text nn_text_source det_text_the preconj_text_either dep_units_Galley rcmod_units_represented nn_units_translation dobj_assume_units nsubj_assume_they mark_assume_because dobj_parallel_treebanks aux_parallel_to advcl_applicable_assume xcomp_applicable_parallel neg_applicable_not cop_applicable_are nsubj_applicable_many advmod_applicable_However det_models_these prep_of_many_models
W08-0411	N04-1035	o	-LRB- Chiang 2005 -RRB- -LRB- Imamura et al. 2004 -RRB- -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et amod_Imamura_2004 dep_Imamura_al. nn_Imamura_et dep_Chiang_Galley dep_Chiang_Imamura dep_Chiang_2005 dep_''_Chiang
W08-0411	N04-1035	o	The underlying formalisms used has been quite broad and include simple formalisms such as ITGs -LRB- Wu 1997 -RRB- hierarchicalsynchronousrules -LRB- Chiang 2005 -RRB- string to tree models by -LRB- Galley et al. 2004 -RRB- and -LRB- Galley et al. 2006 -RRB- synchronous CFG models such -LRB- Xia and McCord 2004 -RRB- -LRB- Yamada and Knight 2001 -RRB- synchronous Lexical Functional Grammar inspired approaches -LRB- Probst et al. 2002 -RRB- and others	amod_Probst_2002 dep_Probst_al. nn_Probst_et conj_and_approaches_others dep_approaches_Probst dobj_inspired_others dobj_inspired_approaches nsubj_inspired_Grammar amod_Grammar_Functional nn_Grammar_Lexical amod_Grammar_synchronous amod_Yamada_2001 conj_and_Yamada_Knight rcmod_Xia_inspired dep_Xia_Knight dep_Xia_Yamada appos_Xia_2004 conj_and_Xia_McCord amod_Xia_such dep_models_McCord dep_models_Xia nn_models_CFG amod_models_synchronous amod_Galley_2006 dep_Galley_al. nn_Galley_et appos_Galley_models conj_and_Galley_Galley amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_models_tree prep_by_string_Galley prep_by_string_Galley prep_to_string_models dep_Chiang_2005 appos_hierarchicalsynchronousrules_Chiang dep_Wu_1997 conj_ITGs_string conj_ITGs_hierarchicalsynchronousrules appos_ITGs_Wu prep_such_as_formalisms_ITGs amod_formalisms_simple dobj_include_formalisms nsubj_include_formalisms conj_and_broad_include advmod_broad_quite cop_broad_been aux_broad_has nsubj_broad_formalisms vmod_formalisms_used amod_formalisms_underlying det_formalisms_The
W08-0411	N04-1035	o	Our process of extraction of rules as synchronous trees and then converting them to synchronous CFG rules is most similar to that of -LRB- Galley et al. 2004 -RRB-	amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_of_Galley prep_that_of prep_to_similar_that advmod_similar_most cop_similar_is nsubj_similar_converting nsubj_similar_process nn_rules_CFG amod_rules_synchronous prep_to_converting_rules dobj_converting_them advmod_converting_then amod_trees_synchronous prep_of_extraction_rules conj_and_process_converting prep_as_process_trees prep_of_process_extraction poss_process_Our
W09-2306	N04-1035	o	To overcome these limitations many syntaxbased SMT models have been proposed -LRB- Wu 1997 Chiang 2007 Ding et al. 2005 Eisner 2003 Quirk et al. 2005 Liu et al. 2007 Zhang et al. 2007 Zhang et al. 2008a Zhang et al. 2008b Gildea 2003 Galley et al. 2004 Marcu et al. 2006 Bod 2007 -RRB-	amod_Bod_2007 nn_al._et nn_al._Marcu nn_al._et nn_al._Galley num_Gildea_2003 dep_Zhang_al. nn_Zhang_et dep_Zhang_al. nn_Zhang_et num_Zhang_2007 nn_Zhang_al. nn_Zhang_et num_Liu_2007 nn_Liu_al. nn_Liu_et num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Eisner_2003 num_Ding_2005 nn_Ding_al. nn_Ding_et appos_Chiang_2008a conj_Chiang_Zhang conj_Chiang_Zhang conj_Chiang_Liu conj_Chiang_Quirk conj_Chiang_Eisner conj_Chiang_Ding num_Chiang_2007 dep_Wu_Bod num_Wu_2006 dep_Wu_al. num_Wu_2004 dep_Wu_al. dep_Wu_Gildea appos_Wu_2008b dep_Wu_Zhang dep_Wu_Chiang appos_Wu_1997 dep_proposed_Wu auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models advcl_proposed_overcome nn_models_SMT amod_models_syntaxbased amod_models_many det_limitations_these dobj_overcome_limitations aux_overcome_To ccomp_``_proposed
W09-2306	N04-1035	o	Firstly they classify all the GHKM2 rules -LRB- Galley et al. 2004 Galley et al. 2006 -RRB- into two categories lexical rules and non-lexical rules	amod_rules_non-lexical conj_and_rules_rules amod_rules_lexical dep_categories_rules dep_categories_rules num_categories_two num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Galley_Galley appos_Galley_2004 dep_Galley_al. nn_Galley_et appos_rules_Galley nn_rules_GHKM2 det_rules_the predet_rules_all prep_into_classify_categories dobj_classify_rules nsubj_classify_they advmod_classify_Firstly
W09-2306	N04-1035	o	We guess it is an acronym for the authors of -LRB- Galley et al. 2004 -RRB- Michel Galley Mark Hopkins Kevin Knight and Daniel Marcu	nn_Marcu_Daniel conj_and_Knight_Marcu nn_Knight_Kevin nn_Hopkins_Mark appos_Galley_Marcu appos_Galley_Knight appos_Galley_Hopkins nn_Galley_Michel amod_Galley_2004 dep_Galley_al. nn_Galley_et dep_of_Galley prep_authors_of det_authors_the prep_for_acronym_authors det_acronym_an cop_acronym_is nsubj_acronym_it parataxis_guess_Galley ccomp_guess_acronym nsubj_guess_We
H05-1003	N04-1038	o	Recently Bean and Riloff -LRB- 2004 -RRB- have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution using techniques adapted from information extraction	nn_extraction_information prep_from_adapted_extraction vmod_techniques_adapted dobj_using_techniques nn_resolution_reference dobj_improve_resolution aux_improve_to vmod_information_improve amod_information_contextual prep_as_used_information auxpass_used_be aux_used_can nsubjpass_used_that rcmod_patterns_used amod_patterns_semantic det_patterns_some dobj_acquire_patterns advmod_acquire_automatically aux_acquire_to xcomp_sought_using xcomp_sought_acquire aux_sought_have nsubj_sought_Riloff nsubj_sought_Bean advmod_sought_Recently appos_Riloff_2004 conj_and_Bean_Riloff ccomp_``_sought
P05-1020	N04-1038	o	-LRB- 2001 -RRB- -RRB- and unsupervised approaches -LRB- e.g. Cardie and Wagstaff -LRB- 1999 -RRB- Bean and Riloff -LRB- 2004 -RRB- -RRB-	appos_Riloff_2004 appos_Wagstaff_1999 conj_and_Cardie_Riloff conj_and_Cardie_Bean conj_and_Cardie_Wagstaff dep_e.g._Riloff dep_e.g._Bean dep_e.g._Wagstaff dep_e.g._Cardie dep_approaches_e.g. amod_approaches_unsupervised conj_and_2001_approaches dep_''_approaches dep_''_2001
P05-1020	N04-1038	o	3.1 Selecting Coreference Systems A learning-based coreference system can be defined by four elements the learning algorithm used to train the coreference classifier the method of creating training instances for the learner the feature set 2Examples of such scoring functions include the DempsterShafer rule -LRB- see Kehler -LRB- 1997 -RRB- and Bean and Riloff -LRB- 2004 -RRB- -RRB- and its variants -LRB- see Harabagiu et al.	nn_al._et nn_al._Harabagiu dep_see_al. dep_variants_see poss_variants_its dep_Riloff_2004 conj_and_Kehler_Riloff conj_and_Kehler_Bean appos_Kehler_1997 dobj_see_Riloff dobj_see_Bean dobj_see_Kehler conj_and_rule_variants dep_rule_see nn_rule_DempsterShafer det_rule_the dobj_include_variants dobj_include_rule nsubj_include_algorithm amod_functions_scoring amod_functions_such prep_of_2Examples_functions amod_2Examples_set dep_feature_2Examples det_feature_the det_learner_the nn_instances_training prep_for_creating_learner dobj_creating_instances prepc_of_method_creating det_method_the nn_classifier_coreference det_classifier_the dobj_train_classifier aux_train_to xcomp_used_train appos_algorithm_feature appos_algorithm_method vmod_algorithm_used nn_algorithm_learning det_algorithm_the num_elements_four parataxis_defined_include agent_defined_elements auxpass_defined_be aux_defined_can nsubjpass_defined_system nn_system_coreference amod_system_learning-based det_system_A nn_system_Systems nn_system_Coreference amod_system_Selecting num_system_3.1
P05-1021	N04-1038	o	Recently Bean and Riloff -LRB- 2004 -RRB- presented an unsupervised approach to coreference resolution which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method	nn_method_bootstrapping det_method_a dobj_using_method amod_corpus_large det_corpus_a vmod_predicatearguments_using prep_from_predicatearguments_corpus amod_predicatearguments_similar nn_pairs_NP amod_pairs_co-referring det_pairs_the prep_with_mined_predicatearguments dobj_mined_pairs nsubj_mined_which rcmod_resolution_mined nn_resolution_coreference prep_to_approach_resolution amod_approach_unsupervised det_approach_an dobj_presented_approach nsubj_presented_Riloff nsubj_presented_Bean advmod_presented_Recently appos_Riloff_2004 conj_and_Bean_Riloff
P06-1005	N04-1038	o	Since no such corpus exists researchers have used coarser features learned from smaller sets through supervised learning -LRB- Soon et al. 2001 Ng and Cardie 2002 -RRB- manually-de ned coreference patterns to mine speci c kinds of data -LRB- Bean and Riloff 2004 Bergsma 2005 -RRB- or accepted the noise inherent in unsupervised schemes -LRB- Ge et al. 1998 Cherry and Bergsma 2005 -RRB-	num_Cherry_2005 conj_and_Cherry_Bergsma dep_Ge_Bergsma dep_Ge_Cherry dep_Ge_1998 dep_Ge_al. nn_Ge_et amod_schemes_unsupervised prep_in_inherent_schemes amod_noise_inherent det_noise_the dobj_accepted_noise amod_Bergsma_2005 dep_Bean_Ge conj_or_Bean_accepted conj_and_Bean_Bergsma conj_and_Bean_2004 conj_and_Bean_Riloff dep_data_accepted dep_data_Bergsma dep_data_2004 dep_data_Riloff dep_data_Bean prep_of_kinds_data nn_kinds_c nn_kinds_speci amod_kinds_mine prep_to_patterns_kinds nn_patterns_coreference amod_patterns_ned nn_patterns_manually-de num_Ng_2002 conj_and_Ng_Cardie appos_al._patterns dep_al._Cardie dep_al._Ng appos_al._2001 nn_al._et advmod_al._Soon amod_learning_supervised amod_sets_smaller dep_learned_al. prep_through_learned_learning prep_from_learned_sets nsubj_learned_features amod_features_coarser ccomp_used_learned aux_used_have nsubj_used_researchers parataxis_exists_used prep_since_exists_corpus amod_corpus_such neg_corpus_no
P06-1005	N04-1038	o	Bean and Riloff -LRB- 2004 -RRB- used bootstrapping to extend their semantic compatibility model which they called contextual-role knowledge by identifying certain cases of easily-resolved anaphors and antecedents	conj_and_anaphors_antecedents amod_anaphors_easily-resolved prep_of_cases_antecedents prep_of_cases_anaphors amod_cases_certain dobj_identifying_cases amod_knowledge_contextual-role dep_called_knowledge nsubj_called_they dobj_called_which rcmod_model_called nn_model_compatibility amod_model_semantic poss_model_their dobj_extend_model aux_extend_to xcomp_bootstrapping_extend prepc_by_used_identifying xcomp_used_bootstrapping nsubj_used_Riloff nsubj_used_Bean appos_Riloff_2004 conj_and_Bean_Riloff
P07-1067	N04-1038	o	Bean and Riloff -LRB- 2004 -RRB- present a system called BABAR that uses contextual role knowledge to do coreference resolution	nn_resolution_coreference dobj_do_resolution aux_do_to nn_knowledge_role amod_knowledge_contextual vmod_uses_do dobj_uses_knowledge nsubj_uses_that dep_called_BABAR rcmod_system_uses vmod_system_called det_system_a dobj_present_system nsubj_present_Riloff nsubj_present_Bean appos_Riloff_2004 conj_and_Bean_Riloff
P07-1068	N04-1038	o	-LRB- 2004 -RRB- -RRB- or Wikipedia -LRB- Ponzetto and Strube 2006 -RRB- and the contextual role played by an NP -LRB- see Bean and Riloff -LRB- 2004 -RRB- -RRB-	appos_Riloff_2004 conj_and_see_Riloff dobj_see_Bean det_NP_an agent_played_NP vmod_role_played amod_role_contextual det_role_the dep_Ponzetto_2006 conj_and_Ponzetto_Strube nn_Ponzetto_Wikipedia dep_2004_Riloff dep_2004_see conj_and_2004_role conj_or_2004_Strube conj_or_2004_Ponzetto dep_''_role dep_''_Ponzetto dep_''_2004
P08-1090	N04-1038	o	Bean and Riloff -LRB- 2004 -RRB- proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution	nn_resolution_anaphora nn_knoweldge_role amod_knoweldge_contextual prep_for_kind_resolution prep_of_kind_knoweldge det_kind_a nn_networks_caseframe prep_of_use_networks det_use_the prep_as_proposed_kind dobj_proposed_use nsubj_proposed_Riloff nsubj_proposed_Bean appos_Riloff_2004 conj_and_Bean_Riloff
P09-1068	N04-1038	o	In this paper we extend this work to represent sets of situation-specific events not unlike scripts caseframes -LRB- Bean and Riloff 2004 -RRB- and FrameNet frames -LRB- Baker et al. 1998 -RRB-	amod_Baker_1998 dep_Baker_al. nn_Baker_et nn_frames_FrameNet dep_Bean_2004 conj_and_Bean_Riloff appos_caseframes_Riloff appos_caseframes_Bean conj_and_scripts_frames conj_and_scripts_caseframes amod_events_situation-specific prep_of_sets_events prep_unlike_represent_frames prep_unlike_represent_caseframes prep_unlike_represent_scripts neg_represent_not dobj_represent_sets aux_represent_to det_work_this dep_extend_Baker vmod_extend_represent dobj_extend_work nsubj_extend_we prep_in_extend_paper det_paper_this
P09-1074	N04-1038	o	3.5 Anaphoricity Determination Finally several coreference systems have successfully incorporated anaphoricity determination 660 modules -LRB- e.g. Ng and Cardie -LRB- 2002a -RRB- and Bean and Riloff -LRB- 2004 -RRB- -RRB-	appos_Riloff_2004 appos_Cardie_2002a conj_and_Ng_Riloff conj_and_Ng_Bean conj_and_Ng_Cardie pobj_e.g._Riloff pobj_e.g._Bean pobj_e.g._Cardie pobj_e.g._Ng prep_-LRB-_e.g. num_modules_660 dep_determination_modules nn_determination_anaphoricity dobj_incorporated_determination advmod_incorporated_successfully aux_incorporated_have nsubj_incorporated_systems nn_systems_coreference amod_systems_several dep_Determination_incorporated advmod_Determination_Finally nn_Determination_Anaphoricity num_Determination_3.5 dep_``_Determination
W05-0612	N04-1038	o	There are also approaches to anaphora resolution using unsupervised methods to extract useful information such as gender and number -LRB- Ge et al. 1998 -RRB- or contextual role-knowledge -LRB- Bean and Riloff 2004 -RRB-	amod_Bean_2004 conj_and_Bean_Riloff dep_role-knowledge_Riloff dep_role-knowledge_Bean amod_role-knowledge_contextual amod_Ge_1998 dep_Ge_al. nn_Ge_et conj_and_gender_number prep_such_as_information_number prep_such_as_information_gender amod_information_useful dobj_extract_information aux_extract_to amod_methods_unsupervised conj_or_using_role-knowledge dep_using_Ge vmod_using_extract dobj_using_methods nn_resolution_anaphora xcomp_approaches_role-knowledge xcomp_approaches_using prep_to_approaches_resolution advmod_approaches_also ccomp_are_approaches expl_are_There ccomp_``_are
W06-0206	N04-1038	o	It has shown promise in improving the performance of many tasks such as name tagging -LRB- Miller et al. 2004 -RRB- semantic class extraction -LRB- Lin et al. 2003 -RRB- chunking -LRB- Ando and Zhang 2005 -RRB- coreference resolution -LRB- Bean and Riloff 2004 -RRB- and text classification -LRB- Blum and Mitchell 1998 -RRB-	amod_Blum_1998 conj_and_Blum_Mitchell nn_classification_text dep_Bean_2004 conj_and_Bean_Riloff dep_resolution_Riloff dep_resolution_Bean nn_resolution_coreference amod_Ando_2005 conj_and_Ando_Zhang dep_chunking_Zhang dep_chunking_Ando amod_Lin_2003 dep_Lin_al. nn_Lin_et appos_extraction_Lin nn_extraction_class amod_extraction_semantic amod_Miller_2004 dep_Miller_al. nn_Miller_et dep_tagging_Mitchell dep_tagging_Blum conj_and_tagging_classification conj_and_tagging_resolution conj_and_tagging_chunking conj_and_tagging_extraction dep_tagging_Miller nn_tagging_name prep_such_as_tasks_classification prep_such_as_tasks_resolution prep_such_as_tasks_chunking prep_such_as_tasks_extraction prep_such_as_tasks_tagging amod_tasks_many prep_of_performance_tasks det_performance_the dobj_improving_performance prepc_in_promise_improving dobj_shown_promise aux_shown_has nsubj_shown_It
D09-1160	N04-1039	o	lscript1-regularized log-linear models -LRB- lscript1-LLMs -RRB- on the other hand provide sparse solutions in which weights of irrelevant features are exactly zero by assumingaLaplacianpriorontheweights -LRB- Tibshirani 1996 Kazama and Tsujii 2003 Goodman 2004 Gao et al. 2007 -RRB-	num_Gao_2007 nn_Gao_al. nn_Gao_et num_Goodman_2004 dep_Kazama_Gao conj_and_Kazama_Goodman conj_and_Kazama_2003 conj_and_Kazama_Tsujii dep_Tibshirani_Goodman dep_Tibshirani_2003 dep_Tibshirani_Tsujii dep_Tibshirani_Kazama appos_Tibshirani_1996 dep_assumingaLaplacianpriorontheweights_Tibshirani advmod_zero_exactly cop_zero_are nsubj_zero_weights prep_in_zero_which amod_features_irrelevant prep_of_weights_features rcmod_solutions_zero amod_solutions_sparse prep_by_provide_assumingaLaplacianpriorontheweights dobj_provide_solutions nsubj_provide_models amod_hand_other det_hand_the prep_on_models_hand appos_models_lscript1-LLMs amod_models_log-linear amod_models_lscript1-regularized ccomp_``_provide
N09-1051	N04-1039	o	Goodman 2004 -RRB- and lscript22 regularization -LRB- Lau 1994 Chen and Rosenfeld 2000 Lebanon and Lafferty 2001 -RRB-	amod_Lebanon_2001 conj_and_Lebanon_Lafferty num_Chen_2000 conj_and_Chen_Rosenfeld dep_Lau_Lafferty dep_Lau_Lebanon conj_Lau_Rosenfeld conj_Lau_Chen appos_Lau_1994 appos_regularization_Lau nn_regularization_lscript22 conj_and_Goodman_regularization amod_Goodman_2004
P06-1124	N04-1039	n	Our interpretation is more useful than past interpretations involving marginal constraints -LRB- Kneser and Ney 1995 Chen and Goodman 1998 -RRB- or maximum-entropy models -LRB- Goodman 2004 -RRB- as it can recover the exact formulation of interpolated Kneser-Ney and actually produces superior results	amod_results_superior dobj_produces_results advmod_produces_actually nsubj_produces_it amod_Kneser-Ney_interpolated prep_of_formulation_Kneser-Ney amod_formulation_exact det_formulation_the conj_and_recover_produces dobj_recover_formulation aux_recover_can nsubj_recover_it mark_recover_as amod_Goodman_2004 appos_models_Goodman amod_models_maximum-entropy dep_1998_Goodman dep_1998_Chen conj_and_Chen_Goodman dep_Kneser_1998 conj_and_Kneser_1995 conj_and_Kneser_Ney advcl_constraints_produces advcl_constraints_recover conj_or_constraints_models dep_constraints_1995 dep_constraints_Ney dep_constraints_Kneser amod_constraints_marginal dobj_involving_models dobj_involving_constraints vmod_interpretations_involving amod_interpretations_past prep_than_useful_interpretations advmod_useful_more cop_useful_is nsubj_useful_interpretation poss_interpretation_Our
P07-1104	N04-1039	o	L1 or Lasso regularization of linear models introduced by Tibshirani -LRB- 1996 -RRB- embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework and has generated a large amount of interest in the NLP community recently -LRB- e.g. Goodman 2003 Riezler and Vasserman 2004 -RRB-	num_Vasserman_2004 conj_and_Riezler_Vasserman dep_Goodman_Vasserman dep_Goodman_Riezler num_Goodman_2003 pobj_e.g._Goodman prep_-LRB-_e.g. nn_community_NLP det_community_the prep_of_amount_interest amod_amount_large det_amount_a dep_generated_recently prep_in_generated_community dobj_generated_amount aux_generated_has amod_framework_same det_framework_the prep_in_done_framework auxpass_done_are nsubjpass_done_it ccomp_remove_done aux_remove_to mark_remove_whether prepc_about_decision_remove det_decision_the conj_and_feature_decision det_feature_a prep_of_reliability_decision prep_of_reliability_feature det_reliability_the conj_and_assessment_generated prep_of_assessment_reliability det_assessment_an preconj_assessment_both dep_that_generated dep_that_assessment dep_so_that nn_selection_feature prep_embeds_so prep_into_embeds_regularization dobj_embeds_selection nsubj_embeds_Lasso nsubj_embeds_L1 appos_Tibshirani_1996 agent_introduced_Tibshirani amod_models_linear vmod_L1_introduced prep_of_L1_models dep_L1_regularization conj_or_L1_Lasso
D07-1061	N04-3012	o	We used the WordNet Similarity package -LRB- Pedersen et al. 2004 -RRB- to compute baseline scores for several existing measures noting that one word pair was not processed in WS-353 because one of the words was missing from WordNet	prep_from_missing_WordNet aux_missing_was nsubj_missing_one mark_missing_because det_words_the prep_of_one_words advcl_processed_missing prep_in_processed_WS-353 neg_processed_not auxpass_processed_was nsubjpass_processed_pair mark_processed_that nn_pair_word num_pair_one ccomp_noting_processed amod_measures_existing amod_measures_several prep_for_scores_measures nn_scores_baseline dobj_compute_scores aux_compute_to amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et vmod_package_noting vmod_package_compute appos_package_Pedersen nn_package_Similarity det_WordNet_the dobj_used_package dobj_used_WordNet nsubj_used_We
D07-1107	N04-3012	o	We consider the outputs of the top 3 allwords WSD systems that participated in Senseval-3 Gambl -LRB- Decadt et al. 2004 -RRB- SenseLearner -LRB- Mihalcea and Faruque 2004 -RRB- and KOC University -LRB- Yuret Nouns Verbs Adjectives F-SCORE 0.4228 0.4319 0.4727 Feature F-Score Ablation Difference TOPSIG 0.0403 OED 0.0355 0.0126 -0.0124 DERIV 0.0351 0.0977 0.0352 RES 0.0287 0.0147 TWIN 0.0285 0.0109 -0.0130 MN 0.0188 0.0358 LESK 0.0183 0.0541 -0.0250 SENSENUM 0.0155 0.0146 -0.0147 SENSECNT 0.0121 0.0160 0.0168 DOMAIN 0.0119 0.0082 -0.0265 LCH 0.0099 0.0068 WUP 0.0036 0.0168 JCN 0.0025 0.0190 ANTONYM 0.0000 0.0295 0.0000 MAXMN -0.0013 0.0179 VEC -0.0024 0.0371 -0.0062 HSO -0.0073 0.0112 -0.0246 LIN -0.0086 0.0742 COUSIN -0.0094 VERBGRP 0.0327 VERBFRM 0.0102 PERTAINYM -0.0029 Table 4 Feature ablation study F-score difference obtained by removal of the single feature 2004 -RRB-	num_feature_2004 amod_feature_single det_feature_the prep_of_removal_feature agent_obtained_removal vmod_difference_obtained nn_difference_F-score nn_study_ablation nn_study_Feature num_Table_4 num_Table_-0.0029 nn_Table_PERTAINYM num_Table_0.0102 nn_Table_VERBFRM num_Table_0.0327 nn_Table_VERBGRP num_Table_-0.0094 nn_Table_COUSIN num_Table_0.0742 nn_Table_LIN num_Table_-0.0246 num_Table_0.0112 num_Table_-0.0073 nn_Table_HSO num_Table_-0.0062 num_Table_0.0371 number_0.0742_-0.0086 number_0.0371_-0.0024 dep_VEC_study dep_VEC_Table dep_0.0179_VEC dep_-0.0013_0.0179 dep_MAXMN_-0.0013 dep_0.0000_MAXMN dep_0.0295_0.0000 number_0.0295_0.0000 dep_ANTONYM_0.0295 dep_0.0190_ANTONYM dep_0.0025_0.0190 dep_JCN_0.0025 dep_0.0168_JCN dep_0.0036_0.0168 dep_WUP_0.0036 dep_0.0068_WUP dep_0.0099_0.0068 dep_LCH_0.0099 dep_-0.0265_LCH dep_0.0082_-0.0265 number_0.0082_0.0119 dep_DOMAIN_0.0082 dep_0.0168_DOMAIN dep_0.0160_0.0168 number_0.0160_0.0121 dep_SENSECNT_0.0160 amod_-0.0147_SENSECNT dep_0.0146_-0.0147 number_0.0146_0.0155 num_SENSENUM_0.0146 dep_-0.0250_SENSENUM dep_0.0541_-0.0250 number_0.0541_0.0183 dep_LESK_0.0541 dep_0.0358_LESK dep_0.0188_0.0358 dep_MN_0.0188 appos_-0.0130_MN dep_0.0109_-0.0130 number_0.0109_0.0285 dep_TWIN_0.0109 dep_0.0147_TWIN dep_0.0287_0.0147 dep_RES_0.0287 dep_0.0352_RES dep_0.0977_0.0352 number_0.0977_0.0351 dep_DERIV_0.0977 dep_-0.0124_DERIV dep_0.0126_-0.0124 number_0.0126_0.0355 dep_OED_0.0126 num_OED_0.0403 dep_TOPSIG_OED dep_Difference_TOPSIG dep_Ablation_Difference dep_F-Score_Ablation dep_Feature_F-Score dep_0.4727_Feature dep_0.4319_0.4727 number_0.4319_0.4228 dep_F-SCORE_0.4319 dep_Adjectives_F-SCORE dep_Verbs_Adjectives dep_Nouns_Verbs dep_Yuret_difference appos_Yuret_Nouns dep_University_Yuret nn_University_KOC dep_Mihalcea_2004 conj_and_Mihalcea_Faruque appos_SenseLearner_Faruque appos_SenseLearner_Mihalcea amod_Decadt_2004 dep_Decadt_al. nn_Decadt_et conj_and_Gambl_University conj_and_Gambl_SenseLearner dep_Gambl_Decadt prep_in_participated_Senseval-3 nsubj_participated_that rcmod_systems_participated dobj_WSD_systems nsubj_WSD_outputs num_allwords_3 amod_allwords_top det_allwords_the prep_of_outputs_allwords det_outputs_the dep_consider_University dep_consider_SenseLearner dep_consider_Gambl ccomp_consider_WSD nsubj_consider_We
D07-1107	N04-3012	o	A variety of synset similarity measures based on properties of WordNet itself have been proposed nine such measures are discussed in -LRB- Pedersen et al. 2004 -RRB- including gloss-based heuristics -LRB- Lesk 1986 Banerjee and Pedersen 2003 -RRB- information-content based measures -LRB- Resnik 1995 Lin 1998 Jiang and Conrath 1997 -RRB- and others	dep_Jiang_1997 conj_and_Jiang_Conrath conj_and_Lin_others dep_Lin_Conrath dep_Lin_Jiang num_Lin_1998 dep_Resnik_others dep_Resnik_Lin appos_Resnik_1995 dep_measures_Resnik amod_measures_based amod_measures_information-content amod_Banerjee_2003 conj_and_Banerjee_Pedersen dep_Lesk_Pedersen dep_Lesk_Banerjee appos_Lesk_1986 appos_heuristics_measures appos_heuristics_Lesk amod_heuristics_gloss-based prep_including_Pedersen_heuristics amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et prep_in_discussed_Pedersen auxpass_discussed_are nsubjpass_discussed_measures amod_measures_such num_measures_nine parataxis_proposed_discussed auxpass_proposed_been aux_proposed_have nsubjpass_proposed_variety npadvmod_WordNet_itself prep_of_properties_WordNet prep_on_based_properties vmod_measures_based nn_measures_similarity nn_measures_synset prep_of_variety_measures det_variety_A
D07-1107	N04-3012	o	We use eight similarity measures implemented within the WordNet Similarity package5 described in -LRB- Pedersen et al. 2004 -RRB- these include three measures derived from the paths between the synsets in WordNet HSO -LRB- Hirst and St-Onge 1998 -RRB- LCH -LRB- Leacock and Chodorow 1998 -RRB- and WUP -LRB- Wu and Palmer 1994 -RRB- three measures based on information content RES -LRB- Resnik 1995 -RRB- LIN -LRB- Lin 1998 -RRB- and JCN -LRB- Jiang and Conrath 1997 -RRB- the gloss-based Extended Lesk Measure LESK -LRB- Banerjee and Pedersen 2003 -RRB- and finally the gloss vector similarity measure VECTOR -LRB- Patwardan 2003 -RRB-	amod_Patwardan_2003 appos_VECTOR_Patwardan nn_VECTOR_measure nn_VECTOR_similarity nn_VECTOR_vector dobj_gloss_VECTOR vmod_the_gloss dep_finally_the dep_Banerjee_2003 conj_and_Banerjee_Pedersen appos_LESK_Pedersen appos_LESK_Banerjee nn_LESK_Measure nn_LESK_Lesk nn_LESK_Extended amod_LESK_gloss-based det_LESK_the dep_Jiang_1997 conj_and_Jiang_Conrath appos_JCN_Conrath appos_JCN_Jiang num_Lin_1998 nn_Lin_LIN amod_Resnik_1995 conj_and_RES_JCN appos_RES_Lin dep_RES_Resnik dep_content_JCN dep_content_RES nn_content_information pobj_measures_content prepc_based_on_measures_on num_measures_three dep_Wu_1994 conj_and_Wu_Palmer appos_WUP_Palmer appos_WUP_Wu dep_Leacock_1998 conj_and_Leacock_Chodorow appos_LCH_Chodorow appos_LCH_Leacock amod_Hirst_1998 conj_and_Hirst_St-Onge conj_and_HSO_finally conj_and_HSO_LESK conj_and_HSO_measures conj_and_HSO_WUP conj_and_HSO_LCH dep_HSO_St-Onge dep_HSO_Hirst prep_in_synsets_WordNet det_synsets_the prep_between_paths_synsets det_paths_the prep_from_derived_paths dep_measures_finally dep_measures_LESK dep_measures_measures dep_measures_WUP dep_measures_LCH dep_measures_HSO vmod_measures_derived num_measures_three dobj_include_measures nsubj_include_these amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et dep_in_Pedersen prep_described_in parataxis_package5_include vmod_package5_described nn_package5_Similarity det_WordNet_the prep_within_implemented_WordNet vmod_measures_implemented nn_measures_similarity num_measures_eight dep_use_package5 dobj_use_measures nsubj_use_We
E09-1077	N04-3012	o	is a WordNet based relatedness measure -LRB- Pedersen et al. 2004 -RRB-	amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et dep_measure_Pedersen nn_measure_relatedness amod_measure_based nn_measure_WordNet det_measure_a cop_measure_is
I08-1055	N04-3012	o	We could also use the value of semantic similarity and relatedness measures -LRB- Pedersen et al. 2004 -RRB- or the existence of hypernym or hyponym relations as features	nn_relations_hyponym nn_relations_hypernym conj_or_hypernym_hyponym prep_of_existence_relations det_existence_the amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et nn_measures_relatedness conj_and_similarity_measures amod_similarity_semantic prep_of_value_measures prep_of_value_similarity det_value_the prep_as_use_features conj_or_use_existence dep_use_Pedersen dobj_use_value advmod_use_also aux_use_could nsubj_use_We
I08-2105	N04-3012	o	Selectional preferences are estimated using grammatical collocation information from the British National Corpus -LRB- BNC -RRB- obtained with the Word Sketch Engine -LRB- WSE -RRB- -LRB- Kilgarriff et al. 2004 -RRB-	amod_Kilgarriff_2004 dep_Kilgarriff_al. nn_Kilgarriff_et dep_Engine_Kilgarriff appos_Engine_WSE dobj_Sketch_Engine vmod_Word_Sketch dep_the_Word prep_with_obtained_the appos_Corpus_BNC nn_Corpus_National nn_Corpus_British det_Corpus_the prep_from_information_Corpus nn_information_collocation amod_information_grammatical dobj_using_information dep_estimated_obtained xcomp_estimated_using auxpass_estimated_are nsubjpass_estimated_preferences amod_preferences_Selectional ccomp_``_estimated
I08-2105	N04-3012	o	Relatedness scores are computed for each pair of senses of the grammatically linked pair of words -LRB- w1 w2 GR -RRB- using the WordNet-Similarity-1 .03 package and the lesk 759 option -LRB- Pedersen et al. 2004 -RRB-	amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et num_option_759 nn_option_lesk det_option_the conj_and_package_option num_package_.03 amod_package_WordNet-Similarity-1 det_package_the dep_using_Pedersen dobj_using_option dobj_using_package dep_w1_GR dep_w1_w2 dep_words_w1 prep_of_pair_words amod_pair_linked advmod_pair_grammatically det_pair_the prep_of_senses_pair prep_of_pair_senses det_pair_each vmod_computed_using prep_for_computed_pair auxpass_computed_are nsubjpass_computed_scores nn_scores_Relatedness
N06-1025	N04-3012	o	In addition we use the measure from Resnik -LRB- 1995 -RRB- which is computed using an intrinsic information content measure relying on the hierarchical structure of the category tree -LRB- Seco et al. 2004 -RRB-	amod_Seco_2004 dep_Seco_al. nn_Seco_et nn_tree_category det_tree_the prep_of_structure_tree amod_structure_hierarchical det_structure_the prep_on_relying_structure vmod_measure_relying nn_measure_content nn_measure_information amod_measure_intrinsic det_measure_an dobj_using_measure xcomp_computed_using auxpass_computed_is nsubjpass_computed_which dep_Resnik_Seco rcmod_Resnik_computed appos_Resnik_1995 det_measure_the prep_from_use_Resnik dobj_use_measure nsubj_use_we prep_in_use_addition
N06-1025	N04-3012	o	1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems -LRB- Soon et al. 2001 Ng & Cardie 2002 Yang et al. 2003 Luo et al. 2004 inter alia -RRB-	nn_alia_inter appos_Luo_alia num_Luo_2004 nn_Luo_al. nn_Luo_et num_Yang_2003 nn_Yang_al. nn_Yang_et dep_Ng_Luo conj_and_Ng_Yang num_Ng_2002 conj_and_Ng_Cardie dep_al._Yang dep_al._Cardie dep_al._Ng appos_al._2001 nn_al._et advmod_al._Soon dep_systems_al. nn_systems_resolution nn_systems_coreference pobj_based_systems nn_learning_machine prep_development_based prep_of_development_learning det_development_the prep_to_devoted_development vmod_boost_devoted prep_of_boost_work det_boost_a dobj_seen_boost aux_seen_have nsubj_seen_Introduction amod_years_last det_years_The tmod_Introduction_years num_Introduction_1 ccomp_``_seen
N06-1025	N04-3012	o	We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy -LRB- Pedersen et al. 2004 -RRB-	amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et appos_taxonomy_Pedersen nn_taxonomy_WordNet det_taxonomy_the prep_on_based_taxonomy vmod_measures_based nn_measures_similarity amod_measures_semantic dobj_using_measures det_classifier_the prep_to_available_classifier amod_information_available amod_information_semantic det_information_the prepc_by_enrich_using dobj_enrich_information nsubj_enrich_We ccomp_``_enrich
N06-2004	N04-3012	o	1 Introduction Estimating the degree of semantic relatedness between words in a text is deemed important in numerous applications word-sense disambiguation -LRB- Banerjee and Pedersen 2003 -RRB- story segmentation -LRB- Stokes et al. 2004 -RRB- error correction -LRB- Hirst and Budanitsky 2005 -RRB- summarization -LRB- Barzilay and Elhadad 1997 Gurevych and Strube 2004 -RRB-	amod_Gurevych_2004 conj_and_Gurevych_Strube dep_Barzilay_Strube dep_Barzilay_Gurevych conj_and_Barzilay_1997 conj_and_Barzilay_Elhadad dep_summarization_1997 dep_summarization_Elhadad dep_summarization_Barzilay dep_Hirst_2005 conj_and_Hirst_Budanitsky appos_correction_Budanitsky appos_correction_Hirst nn_correction_error amod_Stokes_2004 dep_Stokes_al. nn_Stokes_et dep_segmentation_Stokes nn_segmentation_story dep_Banerjee_2003 conj_and_Banerjee_Pedersen appos_disambiguation_summarization conj_disambiguation_correction conj_disambiguation_segmentation appos_disambiguation_Pedersen appos_disambiguation_Banerjee amod_disambiguation_word-sense amod_applications_numerous prep_in_important_applications parataxis_deemed_disambiguation acomp_deemed_important auxpass_deemed_is nsubjpass_deemed_Introduction det_text_a prep_in_words_text amod_relatedness_semantic prep_between_degree_words prep_of_degree_relatedness det_degree_the dobj_Estimating_degree vmod_Introduction_Estimating num_Introduction_1
N06-2004	N04-3012	o	We use the default configuration of the measure in WordNet Similarity-0 .12 package -LRB- Pedersen et al. 2004 -RRB- and with a single exception the measure performed below Gic see BP in table 1	num_table_1 prep_in_see_table dobj_see_BP prep_below_performed_Gic vmod_measure_performed det_measure_the amod_exception_single det_exception_a pobj_with_exception amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et dep_package_see appos_package_measure conj_and_package_with appos_package_Pedersen num_package_.12 nn_package_Similarity-0 prep_in_measure_WordNet det_measure_the prep_of_configuration_measure nn_configuration_default det_configuration_the dep_use_with dep_use_package dobj_use_configuration nsubj_use_We
N06-2004	N04-3012	o	8See formula in appendix B We use -LRB- Pedersen et al. 2004 -RRB- implementation with a minor alteration see Beigman Klebanov -LRB- 2006 -RRB-	appos_Klebanov_2006 nn_Klebanov_Beigman dep_see_Klebanov vmod_alteration_see amod_alteration_minor det_alteration_a prep_with_implementation_alteration amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et dobj_use_implementation dep_use_Pedersen nsubj_use_We dep_use_formula nn_B_appendix prep_in_formula_B nn_formula_8See
N09-5005	N04-3012	o	SR-AW finds the sense of each word that is most relatedormostsimilartothoseofitsneighborsinthe sentence according to any of the ten measures available in WordNet Similarity -LRB- Pedersen et al. 2004 -RRB-	amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et dep_Similarity_Pedersen prep_in_available_WordNet amod_measures_available num_measures_ten det_measures_the prep_of_any_measures amod_sentence_relatedormostsimilartothoseofitsneighborsinthe cop_sentence_is nsubj_sentence_that advmod_relatedormostsimilartothoseofitsneighborsinthe_most rcmod_word_sentence det_word_each prep_of_sense_word det_sense_the dep_finds_Similarity pobj_finds_any prepc_according_to_finds_to dobj_finds_sense nsubj_finds_SR-AW
P05-3002	N04-3012	p	Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet Similarity package -LRB- Pedersen et al. 2004 -RRB- .1 In its turn the development of the WordNet based semantic similarity software has been facilitated by the availability of tools to easily retrieve 1http / / www.d.umn.edu/a0 tpederse/similarity html data from WordNet e.g. WordNet QueryData ,2 jwnl .3 Research integrating semantic knowledge into NLP for languages other than English is scarce	cop_scarce_is nsubj_scarce_English mark_scarce_than advcl_other_scarce amod_languages_other prep_for_NLP_languages amod_knowledge_semantic prep_into_integrating_NLP dobj_integrating_knowledge vmod_Research_integrating nn_Research_.3 nn_Research_jwnl num_Research_,2 nn_Research_QueryData pobj_e.g._WordNet prep_data_e.g. prep_from_data_WordNet nn_data_html num_tpederse/similarity_www.d.umn.edu/a0 dobj_retrieve_1http advmod_retrieve_easily aux_retrieve_to vmod_tools_retrieve prep_of_availability_tools det_availability_the agent_facilitated_availability auxpass_facilitated_been aux_facilitated_has nsubjpass_facilitated_software nn_software_similarity amod_software_semantic amod_software_based det_WordNet_the rcmod_development_facilitated prep_of_development_WordNet det_development_the poss_turn_its dep_Pedersen_Research dep_Pedersen_data dep_Pedersen_tpederse/similarity dep_Pedersen_development prep_in_Pedersen_turn num_Pedersen_.1 amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et dep_package_Pedersen nn_package_Similarity prep_of_emergence_WordNet det_emergence_the dobj_fostered_package agent_fostered_emergence advmod_fostered_arguably auxpass_fostered_been aux_fostered_has nsubjpass_fostered_research nn_language_English det_language_the amod_knowledge_semantic prep_for_integration_language prep_into_integration_NLP prep_of_integration_knowledge det_integration_the prep_concerning_research_integration amod_research_Extensive
P05-3019	N04-3012	o	The disambiguation algorithms also require that the semantic relatedness measures WordNet Similarity -LRB- Pedersen et al. 2004 -RRB- be installed	auxpass_installed_be nsubjpass_installed_Similarity amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et appos_Similarity_Pedersen dep_WordNet_installed nn_WordNet_measures nn_WordNet_relatedness amod_WordNet_semantic det_WordNet_the dep_that_WordNet dobj_require_that advmod_require_also nsubj_require_algorithms nn_algorithms_disambiguation det_algorithms_The
P06-1051	N04-3012	o	Unfortunately a counterexample illustrated in -LRB- Boughorbel et al. 2004 -RRB- shows that the max function does not produce valid kernels in general	amod_kernels_valid prep_in_produce_general dobj_produce_kernels neg_produce_not aux_produce_does nsubj_produce_function mark_produce_that nn_function_max det_function_the ccomp_shows_produce nsubj_shows_Boughorbel mark_shows_in amod_Boughorbel_2004 dep_Boughorbel_al. nn_Boughorbel_et advcl_illustrated_shows nsubj_illustrated_counterexample advmod_illustrated_Unfortunately det_counterexample_a
P06-1051	N04-3012	o	The wn similarity package -LRB- Pedersen et al. 2004 -RRB- to compute the Jiang & Conrath -LRB- J&C -RRB- distance -LRB- Jiang and Conrath 1997 -RRB- as in -LRB- Corley and Mihalcea 2005 -RRB-	dep_Corley_2005 conj_and_Corley_Mihalcea dep_in_Mihalcea dep_in_Corley pcomp_as_in dep_Jiang_1997 conj_and_Jiang_Conrath appos_distance_Conrath appos_distance_Jiang dep_Jiang_distance appos_Jiang_J&C conj_and_Jiang_Conrath det_Jiang_the prep_compute_as dobj_compute_Conrath dobj_compute_Jiang aux_compute_to amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et vmod_package_compute appos_package_Pedersen nn_package_similarity dep_wn_package det_wn_The dep_``_wn
P07-1070	N04-3012	o	The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction relative path and density such as vector lesk hso lch wup path res lin and jcn -LRB- Pedersen et al. 2004 -RRB-	amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et conj_and_vector_jcn conj_and_vector_lin conj_and_vector_res conj_and_vector_path conj_and_vector_wup conj_and_vector_lch conj_and_vector_hso conj_and_vector_lesk dep_path_Pedersen prep_such_as_path_jcn prep_such_as_path_lin prep_such_as_path_res prep_such_as_path_path prep_such_as_path_wup prep_such_as_path_lch prep_such_as_path_hso prep_such_as_path_lesk prep_such_as_path_vector conj_and_path_density amod_path_relative nn_direction_link dobj_considering_direction nn_structure_network det_structure_the prep_of_peculiarities_structure dobj_attempt_density dobj_attempt_path prepc_by_attempt_considering prep_in_attempt_peculiarities prep_to_attempt_factor aux_attempt_to amod_edge-counting_simple xcomp_vary_attempt prep_from_vary_edge-counting nsubj_vary_measures det_measures_The
P07-1126	N04-3012	o	The WordNet Similarity package -LRB- Pedersen et al. 2004 -RRB- implements this distance measure and was used by the authors	det_authors_the agent_used_authors auxpass_used_was nsubjpass_used_package nn_measure_distance det_measure_this conj_and_implements_used dobj_implements_measure nsubj_implements_package amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et appos_package_Pedersen nn_package_Similarity dep_WordNet_used dep_WordNet_implements det_WordNet_The dep_``_WordNet
P07-2013	N04-3012	o	The implementation includes path-length -LRB- Rada et al. 1989 Wu & Palmer 1994 Leacock & Chodorow 1998 -RRB- information-content -LRB- Resnik 1995 Seco et al. 2004 -RRB- and text-overlap -LRB- Lesk 1986 Banerjee & Pedersen 2003 -RRB- measures as described in Strube & Ponzetto -LRB- 2006 -RRB-	appos_Strube_2006 conj_and_Strube_Ponzetto prep_in_described_Ponzetto prep_in_described_Strube mark_described_as num_measures_2003 dep_Banerjee_described appos_Banerjee_measures conj_and_Banerjee_Pedersen dep_Lesk_Pedersen dep_Lesk_Banerjee dep_Lesk_1986 dep_text-overlap_Lesk num_Seco_2004 nn_Seco_al. nn_Seco_et dep_Resnik_Seco dep_Resnik_1995 dep_information-content_Resnik amod_Leacock_1998 conj_and_Leacock_Chodorow dep_Wu_Chodorow dep_Wu_Leacock num_Wu_1994 conj_and_Wu_Palmer dep_Rada_Palmer dep_Rada_Wu appos_Rada_1989 dep_Rada_al. nn_Rada_et conj_and_path-length_text-overlap conj_and_path-length_information-content appos_path-length_Rada dobj_includes_text-overlap dobj_includes_information-content dobj_includes_path-length nsubj_includes_implementation det_implementation_The
P07-2013	N04-3012	p	We believe that the extensive usage of such measures derives also from the availability of robust and freely availablesoftwarethatallowstocomputethem -LRB- Pedersen et al. 2004 WordNet Similarity -RRB-	dep_Pedersen_Similarity appos_Pedersen_WordNet amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et advmod_availablesoftwarethatallowstocomputethem_freely conj_and_robust_availablesoftwarethatallowstocomputethem prep_of_availability_availablesoftwarethatallowstocomputethem prep_of_availability_robust det_availability_the prep_from_derives_availability advmod_derives_also nsubj_derives_usage mark_derives_that amod_measures_such prep_of_usage_measures amod_usage_extensive det_usage_the dep_believe_Pedersen ccomp_believe_derives nsubj_believe_We ccomp_``_believe
P07-2033	N04-3012	o	2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by -LRB- Pedersen et al. 2004 -RRB- are used	auxpass_used_are nsubjpass_used_Pedersen mark_used_by amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et advcl_written_used vmod_WordNet-Similarity_written nn_WordNet-Similarity_package nn_WordNet-Similarity_Perl det_WordNet-Similarity_the prep_from_measures_WordNet-Similarity nn_measures_similarity/distance num_measures_Two dep_measures_measures amod_measures_Basic num_measures_2.1 dep_measures_measures nn_measures_relatedness amod_measures_semantic amod_measures_WordNet-based num_measures_2
W06-1659	N04-3012	o	The approaches proposed to the ACE RDC task such as kernel methods -LRB- Zelenko et al. 2002 -RRB- and Maximum Entropy methods -LRB- Kambhatla 2004 -RRB- required the availability of large set of human annotated corpora which are tagged with relation instances	nn_instances_relation prep_with_tagged_instances auxpass_tagged_are nsubjpass_tagged_which rcmod_corpora_tagged amod_corpora_annotated amod_corpora_human prep_of_set_corpora amod_set_large prep_of_availability_set det_availability_the dobj_required_availability nsubj_required_methods nsubj_required_approaches dep_Kambhatla_2004 appos_methods_Kambhatla nn_methods_Entropy nn_methods_Maximum amod_Zelenko_2002 dep_Zelenko_al. nn_Zelenko_et nn_methods_kernel prep_such_as_task_methods nn_task_RDC nn_task_ACE det_task_the prep_to_proposed_task conj_and_approaches_methods dep_approaches_Zelenko vmod_approaches_proposed det_approaches_The
W06-3806	N04-3012	o	We also used the following resources the Charniak parser -LRB- Charniak 2000 -RRB- to carry out the syntactic analysis the wn similaritypackage -LRB- Pedersen et al. 2004 -RRB- to compute the Jiang & Conrath -LRB- J&C -RRB- distance -LRB- Jiang and Conrath 1997 -RRB- needed to implement the lexical similarity siml -LRB- T H -RRB- as defined in -LRB- Corley and Mihalcea 2005 -RRB- SVM-lightTK -LRB- Moschitti 2004 -RRB- to encode the basic tree kernel function KT in SVM-light -LRB- Joachims 1999 -RRB-	amod_Joachims_1999 dep_SVM-light_Joachims nn_function_kernel nn_function_tree amod_function_basic det_function_the dobj_encode_function aux_encode_to amod_Moschitti_2004 vmod_SVM-lightTK_encode dep_SVM-lightTK_Moschitti dep_Corley_2005 conj_and_Corley_Mihalcea dep_in_Mihalcea dep_in_Corley prep_defined_in mark_defined_as appos_T_H dep_siml_T nn_siml_similarity amod_siml_lexical det_siml_the advcl_implement_defined dobj_implement_siml aux_implement_to xcomp_needed_implement nn_needed_distance dep_Jiang_1997 conj_and_Jiang_Conrath appos_distance_Conrath appos_distance_Jiang vmod_Jiang_needed appos_Jiang_J&C conj_and_Jiang_Conrath det_Jiang_the dobj_compute_Conrath dobj_compute_Jiang aux_compute_to amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et prep_in_similaritypackage_SVM-light conj_similaritypackage_KT conj_similaritypackage_SVM-lightTK vmod_similaritypackage_compute appos_similaritypackage_Pedersen dep_wn_similaritypackage det_wn_the nn_analysis_syntactic det_analysis_the dobj_carry_analysis prt_carry_out aux_carry_to amod_Charniak_2000 dep_parser_wn vmod_parser_carry appos_parser_Charniak nn_parser_Charniak det_parser_the amod_resources_following det_resources_the dep_used_parser dobj_used_resources advmod_used_also nsubj_used_We ccomp_``_used
W07-0211	N04-3012	o	Such a similarity is calculated by using the WordNet Similarity tool -LRB- Pedersen et al. 2004 -RRB- and concretely the Wu-Palmer measure as defined in Equation1 -LRB- Wu and Palmer 1994 -RRB-	dep_Wu_1994 conj_and_Wu_Palmer appos_Equation1_Palmer appos_Equation1_Wu prep_in_defined_Equation1 mark_defined_as nn_measure_Wu-Palmer det_measure_the amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et dep_tool_defined appos_tool_measure conj_and_tool_concretely appos_tool_Pedersen nn_tool_Similarity det_WordNet_the dobj_using_WordNet parataxis_calculated_concretely parataxis_calculated_tool agent_calculated_using auxpass_calculated_is nsubjpass_calculated_similarity det_similarity_a amod_similarity_Such
W07-2086	N04-3012	o	The system uses WordNet-based 1http / / senserelate.sourceforge.net measures of semantic relatedness2 -LRB- Pedersen et al. 2004 -RRB- to measure the relatedness between the different senses of the target word and the words in its context	poss_context_its prep_in_words_context det_words_the conj_and_word_words nn_word_target det_word_the prep_of_senses_words prep_of_senses_word amod_senses_different det_senses_the prep_between_relatedness_senses det_relatedness_the dobj_measure_relatedness aux_measure_to amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et amod_relatedness2_semantic vmod_measures_measure dep_measures_Pedersen prep_of_measures_relatedness2 nn_measures_senserelate.sourceforge.net amod_1http_WordNet-based dep_uses_measures dobj_uses_1http nsubj_uses_system det_system_The
W07-2086	N04-3012	o	The relatedness between two word senses is computed using a measure of semantic relatedness defined in the WordNet Similarity software package -LRB- Pedersen et al. 2004 -RRB- which is a suite of Perl modules implementing a number WordNet-based measures of semantic relatedness	amod_relatedness_semantic prep_of_measures_relatedness amod_measures_WordNet-based nn_measures_number det_measures_a dobj_implementing_measures nn_modules_Perl vmod_suite_implementing prep_of_suite_modules det_suite_a cop_suite_is nsubj_suite_which amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et rcmod_package_suite appos_package_Pedersen nn_package_software nn_package_Similarity det_WordNet_the prep_in_defined_WordNet amod_relatedness_semantic vmod_measure_defined prep_of_measure_relatedness det_measure_a dobj_using_measure dep_computed_package xcomp_computed_using auxpass_computed_is nsubjpass_computed_relatedness nn_senses_word num_senses_two prep_between_relatedness_senses det_relatedness_The dep_``_computed
W09-2403	N04-3012	o	-LRB- 2007 -RRB- observe that their predominant sense method is not performing as well for 3We use the Lesk -LRB- overlap -RRB- similarity as implemented by the WordNet similarity package -LRB- Pedersen et al. 2004 -RRB-	amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et appos_package_Pedersen nn_package_similarity det_WordNet_the prep_by_implemented_WordNet mark_implemented_as dep_similarity_implemented det_Lesk_the dep_use_overlap dobj_use_Lesk nsubj_use_3We dobj_performing_similarity conj_and_performing_use neg_performing_not aux_performing_is nsubj_performing_method mark_performing_that nn_method_sense amod_method_predominant poss_method_their dep_observe_package ccomp_observe_use ccomp_observe_performing nsubj_observe_2007
W09-2404	N04-3012	o	We measure semantic similarity using the shortest path length in WordNet -LRB- Fellbaum 1998 -RRB- as implemented in the WordNet Similarity package -LRB- Pedersen et al. 2004 -RRB-	amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et nn_package_Similarity nn_package_WordNet det_package_the prep_in_implemented_package mark_implemented_as amod_Fellbaum_1998 dep_WordNet_Fellbaum prep_in_length_WordNet nn_length_path amod_length_shortest det_length_the advcl_using_implemented dobj_using_length vmod_similarity_using amod_similarity_semantic dep_measure_Pedersen dobj_measure_similarity nsubj_measure_We
W09-2405	N04-3012	p	The WordNet Similarity package provides a flexible implementation of many of these measures -LRB- Pedersen et al. 2004 -RRB-	amod_Pedersen_2004 dep_Pedersen_al. nn_Pedersen_et det_measures_these prep_of_many_measures prep_of_implementation_many amod_implementation_flexible det_implementation_a dep_provides_Pedersen dobj_provides_implementation nsubj_provides_package nn_package_Similarity dep_WordNet_provides det_WordNet_The dep_``_WordNet
C08-1064	N06-1002	o	It is often argued that the ability to translate discontiguous phrases is important to modeling translation -LRB- Chiang 2007 Simard et al. 2005 Quirk and Menezes 2006 -RRB- and it may be that this explains the results	det_results_the dobj_explains_results nsubj_explains_this mark_explains_that ccomp_be_explains aux_be_may nsubj_be_it dep_Quirk_2006 conj_and_Quirk_Menezes num_Simard_2005 nn_Simard_al. nn_Simard_et dep_Chiang_Menezes dep_Chiang_Quirk conj_Chiang_Simard appos_Chiang_2007 appos_translation_Chiang nn_translation_modeling prep_to_important_translation cop_important_is nsubj_important_ability mark_important_that amod_phrases_discontiguous dobj_translate_phrases aux_translate_to vmod_ability_translate det_ability_the conj_and_argued_be ccomp_argued_important advmod_argued_often auxpass_argued_is nsubjpass_argued_It
E09-1082	N06-1002	o	This definition is similar to that of minimal translation units as described in Quirk and Menezes -LRB- 2006 -RRB- although they allow null words on either side	det_side_either amod_words_null prep_on_allow_side dobj_allow_words nsubj_allow_they mark_allow_although appos_Menezes_2006 conj_and_Quirk_Menezes prep_in_described_Menezes prep_in_described_Quirk mark_described_as nn_units_translation amod_units_minimal prep_of_that_units advcl_similar_allow advcl_similar_described prep_to_similar_that cop_similar_is nsubj_similar_definition det_definition_This
N07-1008	N06-1002	p	Also slightly restating the advantages of phrase-pairs identified in -LRB- Quirk and Menezes 2006 -RRB- these blocks are effective at capturing context including the encoding of non-compositional phrase pairs and capturing local reordering but they lack variables -LRB- e.g. embedding between ne ... pas in French -RRB- have sparsity problems and lack a strategy for global reordering	amod_reordering_global prep_for_strategy_reordering det_strategy_a dobj_lack_strategy nsubj_lack_blocks nn_problems_sparsity dobj_have_problems nsubj_have_blocks prep_in_pas_French prep_pas_e.g. prep_between_embedding_ne pobj_e.g._embedding dep_lack_pas dobj_lack_variables nsubj_lack_they amod_reordering_local dobj_capturing_reordering nsubj_capturing_blocks nn_pairs_phrase amod_pairs_non-compositional prep_of_encoding_pairs det_encoding_the prep_including_context_encoding dobj_capturing_context conj_and_effective_lack conj_but_effective_have conj_but_effective_lack conj_and_effective_capturing prepc_at_effective_capturing cop_effective_are nsubj_effective_blocks vmod_effective_restating advmod_effective_Also det_blocks_these dep_Quirk_2006 conj_and_Quirk_Menezes dep_in_Menezes dep_in_Quirk prep_identified_in amod_phrase-pairs_identified prep_of_advantages_phrase-pairs det_advantages_the dobj_restating_advantages advmod_restating_slightly
N07-1008	N06-1002	p	However in -LRB- Quirk and Menezes 2006 -RRB- the authors investigate minimum translation units -LRB- MTU -RRB- which is a refinement over a similar approach by -LRB- Banchs et al. 2005 -RRB- to eliminate the overlap issue	amod_issue_overlap det_issue_the dobj_eliminate_issue aux_eliminate_to num_al._2005 nn_al._et amod_al._Banchs xcomp_by_eliminate dep_by_al. prep_approach_by amod_approach_similar det_approach_a prep_over_refinement_approach det_refinement_a cop_refinement_is nsubj_refinement_which rcmod_units_refinement appos_units_MTU nn_units_translation amod_units_minimum dobj_investigate_units nsubj_investigate_authors prep_investigate_in advmod_investigate_However det_authors_the dep_Quirk_2006 conj_and_Quirk_Menezes dep_in_Menezes dep_in_Quirk
P08-1064	N06-1002	o	However it can not handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features -LRB- Quirk and Menezes 2006 -RRB-	dep_Quirk_2006 conj_and_Quirk_Menezes appos_features_Menezes appos_features_Quirk nn_features_structure amod_features_syntactic advmod_features_linguistically conj_and_phrases_features amod_phrases_discontinuous dobj_exploit_features dobj_exploit_phrases neg_exploit_not aux_exploit_does nsubj_exploit_it amod_reorderings_long-distance conj_and_handle_exploit advmod_handle_properly dobj_handle_reorderings neg_handle_not aux_handle_can nsubj_handle_it advmod_handle_However
W09-2306	N06-1002	o	One of the theoretical problems with phrase based SMT models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue -LRB- Simard et al. 2005 Quirk and Menezes 2006 Wellington et al. 2006 Bod 2007 Zhang et al. 2007 -RRB-	num_Zhang_2007 nn_Zhang_al. nn_Zhang_et num_Bod_2007 num_Wellington_2006 nn_Wellington_al. nn_Wellington_et dep_Quirk_Zhang conj_and_Quirk_Bod conj_and_Quirk_Wellington conj_and_Quirk_2006 conj_and_Quirk_Menezes dep_Simard_Bod dep_Simard_Wellington dep_Simard_2006 dep_Simard_Menezes dep_Simard_Quirk appos_Simard_2005 dep_Simard_al. nn_Simard_et det_issue_this prep_on_made_issue auxpass_made_been aux_made_have nsubjpass_made_model amod_attempts_numerous conj_and_translations_attempts amod_translations_discontiguous det_translations_the dobj_model_attempts dobj_model_translations advmod_model_effectively neg_model_not aux_model_can nsubj_model_they mark_model_that dep_is_Simard ccomp_is_made nsubj_is_One nn_models_SMT amod_models_based nn_models_phrase prep_with_problems_models amod_problems_theoretical det_problems_the prep_of_One_problems ccomp_``_is
W07-0211	N06-1005	o	Some authors have already designed similar matching techniques such as the ones described in -LRB- MacCartney et al. 2006 -RRB- and -LRB- Snow et al. 2006 -RRB-	amod_Snow_2006 dep_Snow_al. nn_Snow_et conj_and_MacCartney_Snow amod_MacCartney_2006 dep_MacCartney_al. nn_MacCartney_et prep_in_described_Snow prep_in_described_MacCartney vmod_ones_described det_ones_the prep_such_as_techniques_ones nn_techniques_matching amod_techniques_similar dobj_designed_techniques advmod_designed_already aux_designed_have nsubj_designed_authors det_authors_Some ccomp_``_designed
C08-1071	N06-1020	p	David McClosky Eugene Charniak and Mark Johnson Brown Laboratory for Linguistic Information Processing -LRB- BLLIP -RRB- Brown University Providence RI 02912 -LCB- dmcc | ec | mj -RCB- @cs brown.edu Abstract Self-training has been shown capable of improving on state-of-the-art parser performance -LRB- McClosky et al. 2006 -RRB- despite the conventional wisdom on the matter and several studies to the contrary -LRB- Charniak 1997 Steedman et al. 2003 -RRB-	num_Steedman_2003 nn_Steedman_al. nn_Steedman_et dep_Charniak_Steedman dep_Charniak_1997 dep_contrary_Charniak det_contrary_the prep_to_studies_contrary amod_studies_several det_matter_the conj_and_wisdom_studies prep_on_wisdom_matter amod_wisdom_conventional det_wisdom_the amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et nn_performance_parser amod_performance_state-of-the-art prep_on_improving_performance dep_capable_McClosky prepc_of_capable_improving prep_despite_shown_studies prep_despite_shown_wisdom acomp_shown_capable auxpass_shown_been aux_shown_has nsubjpass_shown_Self-training amod_Self-training_Abstract nn_Self-training_brown.edu parataxis_@cs_shown dep_@cs_mj dep_@cs_ec dep_@cs_| dep_@cs_dmcc dep_@cs_Providence dep_@cs_Processing dep_@cs_for dep_@cs_Laboratory dep_@cs_McClosky num_mj_| num_RI_02912 appos_Providence_RI nn_Providence_University nn_Providence_Brown appos_Processing_BLLIP nn_Processing_Information nn_Processing_Linguistic nn_Laboratory_Brown nn_Laboratory_Johnson nn_Laboratory_Mark nn_Charniak_Eugene conj_and_McClosky_Laboratory appos_McClosky_Charniak nn_McClosky_David
C08-1071	N06-1020	o	4 Testing the Four Hypotheses The question of why self-training helps in some cases -LRB- McClosky et al. 2006 Reichart and Rappoport 2007 -RRB- but not others -LRB- Charniak 1997 Steedman et al. 2003 -RRB- has inspired various theories	amod_theories_various dobj_inspired_theories aux_inspired_has nsubj_inspired_4 num_Steedman_2003 nn_Steedman_al. nn_Steedman_et dep_Charniak_Steedman dep_Charniak_1997 appos_others_Charniak dep_Reichart_2007 conj_and_Reichart_Rappoport dep_McClosky_Rappoport dep_McClosky_Reichart appos_McClosky_2006 dep_McClosky_al. nn_McClosky_et det_cases_some prep_in_helps_cases nsubj_helps_self-training advmod_helps_why prepc_of_question_helps det_question_The conj_negcc_Hypotheses_others appos_Hypotheses_McClosky dep_Hypotheses_question num_Hypotheses_Four det_Hypotheses_the dobj_Testing_others dobj_Testing_Hypotheses vmod_4_Testing
C08-1071	N06-1020	o	The self-training protocol is the same as in -LRB- Charniak 1997 McClosky et al. 2006 Reichart and Rappoport 2007 -RRB- we parse the entire unlabeled corpus in one iteration	num_iteration_one amod_corpus_unlabeled amod_corpus_entire det_corpus_the prep_in_parse_iteration dobj_parse_corpus nsubj_parse_we dep_Reichart_2007 conj_and_Reichart_Rappoport num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Charniak_Rappoport dep_Charniak_Reichart dep_Charniak_McClosky dep_Charniak_1997 dep_in_Charniak pcomp_as_in parataxis_same_parse prep_same_as det_same_the cop_same_is nsubj_same_protocol amod_protocol_self-training det_protocol_The
D08-1071	N06-1020	o	glish -LRB- previously used for self-training of parsers -LRB- McClosky et al. 2006 -RRB- -RRB-	dep_McClosky_2006 dep_McClosky_al. nn_McClosky_et prep_of_self-training_parsers dep_used_McClosky prep_for_used_self-training advmod_used_previously dep_glish_used
D08-1071	N06-1020	p	Its success stories range from parsing -LRB- McClosky et al. 2006 -RRB- to machine translation -LRB- Ueffing 2006 -RRB-	amod_Ueffing_2006 dep_translation_Ueffing nn_translation_machine amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et prep_to_range_translation dep_range_McClosky prep_from_range_parsing nsubj_range_stories nn_stories_success poss_stories_Its
D09-1087	N06-1020	p	For English self-training contributes 0.83 % absolute improvement to the PCFG-LA parser which is comparable to the improvement obtained from using semi-supervised training with the twostage parser in -LRB- McClosky et al. 2006 -RRB-	amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et dep_in_McClosky nn_parser_twostage det_parser_the prep_with_training_parser amod_training_semi-supervised prep_using_in dobj_using_training prepc_from_obtained_using vmod_improvement_obtained det_improvement_the prep_to_comparable_improvement cop_comparable_is nsubj_comparable_which rcmod_parser_comparable nn_parser_PCFG-LA det_parser_the prep_to_improvement_parser amod_improvement_absolute dep_%_improvement num_%_0.83 dobj_contributes_% nsubj_contributes_self-training prep_for_contributes_English
D09-1087	N06-1020	o	-LRB- 2006 -RRB- effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set but they used a two-stage parser comprised of Charniaks lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser -LRB- Charniak and Johnson 2005 -RRB- and thus it would be better categorized as co-training -LRB- McClosky et al. 2008 -RRB-	amod_McClosky_2008 dep_McClosky_al. nn_McClosky_et dep_co-training_McClosky prep_as_categorized_co-training advmod_categorized_better auxpass_categorized_be aux_categorized_would nsubjpass_categorized_it advmod_categorized_thus dep_Charniak_2005 conj_and_Charniak_Johnson appos_parser_Johnson appos_parser_Charniak nn_parser_reranking amod_parser_discriminative det_parser_a conj_and_parsing_parser amod_parsing_n-best amod_parser_probabilistic prep_with_lexicalized_parser prep_with_lexicalized_parsing dobj_lexicalized_parser prep_of_comprised_Charniaks vmod_parser_comprised amod_parser_two-stage det_parser_a vmod_used_lexicalized dobj_used_parser nsubj_used_they nn_set_training nn_set_WSJ amod_set_standard det_set_the prep_on_accuracy_set nn_accuracy_parsing dobj_improve_accuracy aux_improve_to amod_data_unlabeled conj_and_utilized_categorized conj_but_utilized_used vmod_utilized_improve dobj_utilized_data advmod_utilized_effectively nsubj_utilized_2006
D09-1160	N06-1020	o	1 Introduction Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications and it is inadequate to process Web-scale corpora for knowledge acquisition -LRB- Pantel 2007 Saeger et al. 2009 -RRB- or semi-supervised learning -LRB- McClosky et al. 2006 Spoustov et al. 2009 -RRB-	num_Spoustov_2009 nn_Spoustov_al. nn_Spoustov_et dep_McClosky_Spoustov appos_McClosky_2006 dep_McClosky_al. nn_McClosky_et amod_learning_semi-supervised num_Saeger_2009 nn_Saeger_al. nn_Saeger_et dep_Pantel_Saeger appos_Pantel_2007 conj_or_acquisition_learning appos_acquisition_Pantel nn_acquisition_knowledge amod_corpora_Web-scale prep_for_process_learning prep_for_process_acquisition dobj_process_corpora aux_process_to xcomp_inadequate_process cop_inadequate_is nsubj_inadequate_it amod_applications_real-time prep_of_component_applications det_component_a dep_efficient_McClosky conj_and_efficient_inadequate prep_as_efficient_component advmod_efficient_enough advmod_efficient_yet neg_efficient_not cop_efficient_is nsubj_efficient_analysis amod_models_discriminative prep_on_based_models vmod_analysis_based nn_analysis_text amod_analysis_accurate amod_analysis_Deep conj_and_Deep_accurate npadvmod_Deep_Introduction num_Introduction_1
D09-1160	N06-1020	o	The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks recent examples include dependency parsing -LRB- Koo et al. 2008 -RRB- parse re-ranking -LRB- McClosky et al. 2006 -RRB- pronoun resolution -LRB- Nguyen and Kim 2008 -RRB- and semantic role labeling -LRB- Liu and Sarkar 2007 -RRB-	dep_Liu_2007 conj_and_Liu_Sarkar appos_labeling_Sarkar appos_labeling_Liu nn_labeling_role amod_labeling_semantic dep_Nguyen_2008 conj_and_Nguyen_Kim appos_resolution_Kim appos_resolution_Nguyen nn_resolution_pronoun amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et conj_and_re-ranking_labeling conj_and_re-ranking_resolution dep_re-ranking_McClosky dobj_parse_labeling dobj_parse_resolution dobj_parse_re-ranking amod_Koo_2008 dep_Koo_al. nn_Koo_et dep_parsing_parse appos_parsing_Koo nn_parsing_dependency dobj_include_parsing nsubj_include_examples amod_examples_recent nn_tasks_NLP amod_tasks_several prep_for_accuracy_tasks amod_accuracy_state-of-the-art det_classifier_a prep_with_obtaining_accuracy dobj_obtaining_classifier amod_role_essential det_role_an parataxis_play_include prepc_in_play_obtaining dobj_play_role nsubj_play_combinations nn_combinations_feature det_combinations_The
D09-1161	N06-1020	o	The other is the self-training -LRB- McClosky et al. 2006 -RRB- which first parses and reranks the NANC corpus and then use them as additional training data to retrain the model	det_model_the dobj_retrain_model aux_retrain_to vmod_data_retrain nn_data_training amod_data_additional prep_as_use_data dobj_use_them advmod_use_then nn_corpus_NANC det_corpus_the nsubj_reranks_which dobj_parses_corpus conj_and_parses_reranks advmod_parses_first nsubj_parses_which dep_2006_al. nn_al._et num_McClosky_2006 conj_and_self-training_use ccomp_self-training_reranks ccomp_self-training_parses dep_self-training_McClosky dep_the_use dep_the_self-training nsubj_is_the ccomp_other_is det_other_The dep_``_other
E09-1033	N06-1020	o	Third we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training -LRB- McClosky et al. 2006 -RRB-	amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et prep_to_similar_self-training amod_process_similar det_process_a dobj_using_process amod_parsing_monolingual vmod_improving_using dobj_improving_parsing dep_data_McClosky prepc_for_data_improving nn_data_training nn_data_quality amod_data_higher prep_as_serve_data aux_serve_will nsubj_serve_parses mark_serve_that prep_of_parses_bitext amod_parses_improved det_parses_the ccomp_hope_serve nsubj_hope_we nsubj_hope_Third
E09-1090	N06-1020	o	A totally different approach to improving the accuracy of our parser is to use the idea of selftraining described in -LRB- McClosky et al. 2006 -RRB-	amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et dep_in_McClosky prep_described_in vmod_selftraining_described prepc_of_idea_selftraining det_idea_the dobj_use_idea aux_use_to xcomp_is_use nsubj_is_approach poss_parser_our prep_of_accuracy_parser det_accuracy_the dobj_improving_accuracy prepc_to_approach_improving amod_approach_different det_approach_A advmod_different_totally ccomp_``_is
E09-3005	N06-1020	o	The problem itself has started to get attention only recently -LRB- Roark and Bacchiani 2003 Hara et al. 2005 Daume III and Marcu 2006 Daume III 2007 Blitzer et al. 2006 McClosky et al. 2006 Dredze et al. 2007 -RRB-	num_Dredze_2007 nn_Dredze_al. nn_Dredze_et num_McClosky_2006 nn_McClosky_al. nn_McClosky_et num_Blitzer_2006 nn_Blitzer_al. nn_Blitzer_et appos_III_2007 nn_III_Daume dep_III_Dredze conj_and_III_McClosky conj_and_III_Blitzer conj_and_III_III conj_and_III_2006 conj_and_III_Marcu nn_III_Daume num_Hara_2005 nn_Hara_al. nn_Hara_et dep_Roark_McClosky dep_Roark_Blitzer dep_Roark_III dep_Roark_2006 dep_Roark_Marcu dep_Roark_III conj_and_Roark_Hara conj_and_Roark_2003 conj_and_Roark_Bacchiani advmod_recently_only dep_get_Hara dep_get_2003 dep_get_Bacchiani dep_get_Roark advmod_get_recently dobj_get_attention aux_get_to xcomp_started_get aux_started_has nsubj_started_problem npadvmod_problem_itself det_problem_The
E09-3005	N06-1020	o	In contrast semi-supervised domain adaptation -LRB- Blitzer et al. 2006 McClosky et al. 2006 Dredze et al. 2007 -RRB- is the scenario in which in addition to the labeled source data we only have unlabeled and no labeled target domain data	nn_data_domain nn_data_target amod_data_labeled dep_unlabeled_data conj_and_unlabeled_no dobj_have_no dobj_have_unlabeled advmod_have_only nsubj_have_we prep_in_addition_to_have_data prep_in_have_which nn_data_source amod_data_labeled det_data_the rcmod_scenario_have det_scenario_the cop_scenario_is nsubj_scenario_adaptation prep_in_scenario_contrast num_Dredze_2007 nn_Dredze_al. nn_Dredze_et dep_McClosky_Dredze num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Blitzer_McClosky appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et appos_adaptation_Blitzer nn_adaptation_domain amod_adaptation_semi-supervised
E09-3005	N06-1020	p	2 Motivation and Prior Work While several authors have looked at the supervised adaptation case there are less -LRB- and especially less successful -RRB- studies on semi-supervised domain adaptation -LRB- McClosky et al. 2006 Blitzer et al. 2006 Dredze et al. 2007 -RRB-	num_Dredze_2007 nn_Dredze_al. nn_Dredze_et dep_Blitzer_Dredze num_Blitzer_2006 nn_Blitzer_al. nn_Blitzer_et dep_McClosky_Blitzer appos_McClosky_2006 dep_McClosky_al. nn_McClosky_et nn_adaptation_domain amod_adaptation_semi-supervised dep_studies_McClosky prep_on_studies_adaptation amod_studies_less dep_less_successful advmod_less_especially cc_less_and dep_less_less nsubj_are_studies expl_are_there ccomp_are_Work ccomp_are_Motivation nn_case_adaptation amod_case_supervised det_case_the prep_at_looked_case aux_looked_have nsubj_looked_authors mark_looked_While amod_authors_several amod_Work_Prior dep_Motivation_looked conj_and_Motivation_Work num_Motivation_2 ccomp_``_are
E09-3005	N06-1020	o	So far most previous work on domain adaptation for parsing has focused on data-driven systems -LRB- Gildea 2001 Roark and Bacchiani 2003 McClosky et al. 2006 Shimizu and Nakagawa 2007 -RRB- i.e. systems employing -LRB- constituent or dependency based -RRB- treebank grammars -LRB- Charniak 1996 -RRB-	amod_Charniak_1996 dep_grammars_Charniak amod_grammars_treebank dep_grammars_dependency dep_grammars_constituent amod_grammars_employing dep_constituent_based conj_or_constituent_dependency dep_systems_grammars nn_systems_i.e. dep_Shimizu_2007 conj_and_Shimizu_Nakagawa num_McClosky_2006 nn_McClosky_al. nn_McClosky_et appos_Roark_systems dep_Roark_Nakagawa dep_Roark_Shimizu conj_and_Roark_McClosky conj_and_Roark_2003 conj_and_Roark_Bacchiani dep_Gildea_McClosky dep_Gildea_2003 dep_Gildea_Bacchiani dep_Gildea_Roark appos_Gildea_2001 dep_systems_Gildea amod_systems_data-driven prep_on_focused_systems aux_focused_has nsubj_focused_work advmod_focused_far prep_for_adaptation_parsing nn_adaptation_domain prep_on_work_adaptation amod_work_previous advmod_work_most advmod_far_So
E09-3005	N06-1020	o	Parse selection constitutes an important part of many parsing systems -LRB- Johnson et al. 1999 Hara et al. 2005 van Noord and Malouf 2005 McClosky et al. 2006 -RRB-	num_McClosky_2006 nn_McClosky_al. nn_McClosky_et num_Malouf_2005 conj_and_Noord_Malouf nn_Noord_van dep_Hara_Malouf dep_Hara_Noord num_Hara_2005 nn_Hara_al. nn_Hara_et dep_Johnson_McClosky dep_Johnson_Hara amod_Johnson_1999 dep_Johnson_al. nn_Johnson_et nn_systems_parsing amod_systems_many prep_of_part_systems amod_part_important det_part_an dep_constitutes_Johnson dobj_constitutes_part csubj_constitutes_Parse dobj_Parse_selection
I08-2097	N06-1020	p	There are only a few successful studies such as -LRB- Ando and Zhang 2005 -RRB- for chunking and -LRB- McClosky et al. 2006a McClosky et al. 2006b -RRB- on constituency parsing	nn_parsing_constituency prep_on_McClosky_parsing appos_McClosky_2006b dep_McClosky_al. nn_McClosky_et appos_McClosky_2006a dep_McClosky_al. nn_McClosky_et conj_and_chunking_McClosky prep_for_Ando_McClosky prep_for_Ando_chunking amod_Ando_2005 conj_and_Ando_Zhang prep_such_as_studies_Zhang prep_such_as_studies_Ando amod_studies_successful amod_studies_few det_studies_a advmod_studies_only dep_are_McClosky nsubj_are_studies expl_are_There ccomp_``_are
I08-2097	N06-1020	o	Theyalsoappliedself-training to domain adaptation of a constituency parser -LRB- McClosky et al. 2006b -RRB-	appos_McClosky_2006b dep_McClosky_al. nn_McClosky_et nn_parser_constituency det_parser_a prep_of_adaptation_parser nn_adaptation_domain dep_Theyalsoappliedself-training_McClosky prep_to_Theyalsoappliedself-training_adaptation
N07-1070	N06-1020	o	Recently there have been some improvements to the Charniak parser use n-best re-ranking as reported in -LRB- Charniak and Johnson 2005 -RRB- and selftraining and re-ranking using data from the North American News corpus -LRB- NANC -RRB- and adapts much better to the Brown corpus -LRB- McClosky et al. 2006a McClosky et al. 2006b -RRB-	appos_al._2006b nn_al._et nn_al._McClosky dep_McClosky_al. appos_McClosky_2006a dep_McClosky_al. nn_McClosky_et amod_corpus_Brown det_corpus_the prep_to_better_corpus advmod_better_much amod_adapts_better appos_corpus_NANC nn_corpus_News nn_corpus_American amod_corpus_North det_corpus_the conj_and_data_adapts prep_from_data_corpus dobj_using_adapts dobj_using_data dep_Charniak_McClosky vmod_Charniak_using conj_and_Charniak_re-ranking conj_and_Charniak_selftraining conj_and_Charniak_2005 conj_and_Charniak_Johnson prep_in_reported_re-ranking prep_in_reported_selftraining prep_in_reported_2005 prep_in_reported_Johnson prep_in_reported_Charniak mark_reported_as amod_re-ranking_n-best advcl_use_reported dobj_use_re-ranking nsubj_use_improvements nn_parser_Charniak det_parser_the prep_to_improvements_parser det_improvements_some cop_improvements_been aux_improvements_have expl_improvements_there advmod_improvements_Recently
N07-1070	N06-1020	o	The syntactic parser is the version that is selftrained using 2,500,000 sentences from NANC and where the starting version is trained only on WSJ data -LRB- McClosky et al. 2006b -RRB-	appos_McClosky_2006b dep_McClosky_al. nn_McClosky_et nn_data_WSJ dep_trained_McClosky prep_on_trained_data advmod_trained_only auxpass_trained_is nsubjpass_trained_version advmod_trained_where amod_version_starting det_version_the prep_from_sentences_NANC num_sentences_2,500,000 dobj_using_sentences xcomp_selftrained_using auxpass_selftrained_is nsubjpass_selftrained_that conj_and_version_trained rcmod_version_selftrained det_version_the cop_version_is nsubj_version_parser amod_parser_syntactic det_parser_The
N07-2045	N06-1020	o	As far as we know language modeling always improves with additional training data so we add data from the North American News Text Corpus -LRB- NANC -RRB- -LRB- Graff 1995 -RRB- automatically parsed with the Charniak parser -LRB- McClosky et al. 2006 -RRB- to train our language model on up to 20 million additional words	amod_words_additional num_words_million number_million_20 dep_million_to quantmod_million_up nn_model_language poss_model_our prep_on_train_words dobj_train_model aux_train_to amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et nn_parser_Charniak det_parser_the prep_with_parsed_parser advmod_parsed_automatically amod_Graff_1995 nn_Graff_Corpus appos_Corpus_NANC dep_Text_McClosky vmod_Text_parsed dobj_Text_Graff amod_News_Text nn_News_American amod_News_North det_News_the vmod_add_train prep_from_add_News dobj_add_data nsubj_add_we mark_add_so nn_data_training amod_data_additional advcl_improves_add prep_with_improves_data advmod_improves_always nsubj_improves_modeling advcl_improves_know prep_improves_As nn_modeling_language nsubj_know_we mark_know_as pobj_As_far
P06-1043	N06-1020	o	While -LRB- McClosky et al. 2006 -RRB- showed that this technique was effective when testing on WSJ the true distribution was closer to WSJ so it made sense to emphasize it	dobj_emphasize_it aux_emphasize_to vmod_made_emphasize dobj_made_sense nsubj_made_it mark_made_so advcl_closer_made prep_to_closer_WSJ cop_closer_was nsubj_closer_distribution amod_distribution_true det_distribution_the rcmod_testing_closer prep_on_testing_WSJ dep_when_testing dep_effective_when cop_effective_was nsubj_effective_technique mark_effective_that det_technique_this ccomp_showed_effective nsubj_showed_McClosky mark_showed_While amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et advcl_``_showed
P06-1043	N06-1020	o	Thus the WSJ+NANC model has better oracle rates than the WSJ model -LRB- McClosky et al. 2006 -RRB- for both the WSJ and BROWN domains	nn_domains_BROWN nn_domains_WSJ det_domains_the preconj_domains_both conj_and_WSJ_BROWN prep_for_McClosky_domains amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et nn_model_WSJ det_model_the prep_than_rates_model nn_rates_oracle amod_rates_better dep_has_McClosky dobj_has_rates nsubj_has_model advmod_has_Thus nn_model_WSJ+NANC det_model_the
P06-1043	N06-1020	p	Recent work -LRB- McClosky et al. 2006 -RRB- has shown that adding many millions of words of machine parsed and reranked LA Times articles does in fact improve performance of the parser on the closely related WSJ data	nn_data_WSJ amod_data_related det_data_the advmod_related_closely det_parser_the prep_of_performance_parser prep_on_improve_data dobj_improve_performance dep_does_improve prep_in_does_fact nsubj_does_articles nn_articles_Times nn_articles_LA ccomp_reranked_does csubj_reranked_adding conj_and_parsed_reranked csubj_parsed_adding mark_parsed_that prep_of_words_machine prep_of_millions_words amod_millions_many dobj_adding_millions ccomp_shown_reranked ccomp_shown_parsed aux_shown_has nsubj_shown_work amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et appos_work_McClosky amod_work_Recent
P06-1043	N06-1020	o	To use the data from NANC we use self-training -LRB- McClosky et al. 2006 -RRB-	amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et dep_self-training_McClosky dobj_use_self-training nsubj_use_we advcl_use_use det_data_the prep_from_use_NANC dobj_use_data aux_use_To
P06-1043	N06-1020	p	Furthermore use of the self-training techniques described in -LRB- McClosky et al. 2006 -RRB- raise this to 87.8 % -LRB- an error reduction of 28 % -RRB- again without any use of labeled Brown data	amod_data_Brown amod_data_labeled prep_of_use_data det_use_any num_%_28 prep_of_reduction_% nn_reduction_error det_reduction_an appos_%_reduction num_%_87.8 prep_without_raise_use advmod_raise_again prep_to_raise_% dobj_raise_this nsubj_raise_use advmod_raise_Furthermore amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et dep_in_McClosky prep_described_in vmod_techniques_described amod_techniques_self-training det_techniques_the prep_of_use_techniques
P06-1043	N06-1020	o	The trends are the same as in -LRB- McClosky et al. 2006 -RRB- Adding NANC data improves parsing performance on BROWN development considerably improving the f-score from 83.9 % to 86.4 %	num_%_86.4 num_%_83.9 det_f-score_the prep_to_improving_% prep_from_improving_% dobj_improving_f-score nn_development_BROWN prep_on_performance_development amod_performance_parsing xcomp_improves_improving advmod_improves_considerably dobj_improves_performance csubj_improves_Adding nn_data_NANC dobj_Adding_data amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et dep_in_McClosky pcomp_as_in parataxis_same_improves prep_same_as det_same_the cop_same_are nsubj_same_trends det_trends_The
P06-1109	N06-1020	o	McClosky et al. 2006 -RRB-	dep_al._2006 nn_al._et nn_al._McClosky
P07-1036	N06-1020	o	Another way to look the algorithm is from the self-training perspective -LRB- McClosky et al. 2006 -RRB-	amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et amod_perspective_self-training det_perspective_the dep_is_McClosky prep_from_is_perspective nsubj_is_way det_algorithm_the dobj_look_algorithm aux_look_to vmod_way_look det_way_Another ccomp_``_is
P07-1049	N06-1020	o	This can either be semi-supervised parsing using both annotated and unannotated data -LRB- McClosky et al. 2006 -RRB- or unsupervised parsing training entirely on unannotated text	amod_text_unannotated prep_on_training_text advmod_training_entirely dep_McClosky_2006 dep_McClosky_al. nn_McClosky_et dep_annotated_training dep_annotated_parsing conj_or_annotated_unsupervised dep_annotated_McClosky dep_annotated_data conj_and_annotated_unannotated preconj_annotated_both dobj_using_unsupervised dobj_using_unannotated dobj_using_annotated vmod_parsing_using amod_parsing_semi-supervised cop_parsing_be advmod_parsing_either aux_parsing_can nsubj_parsing_This ccomp_``_parsing
P07-1051	N06-1020	n	While most parsing methods are currently supervised or semi-supervised -LRB- McClosky et al. 2006 Henderson 2004 Steedman et al. 2003 -RRB- they depend on hand-annotated data which are difficult to come by and which exist only for a few languages	amod_languages_few det_languages_a prep_for_exist_languages advmod_exist_only nsubj_exist_which conj_and_by_exist prep_come_exist prep_come_by aux_come_to xcomp_difficult_come cop_difficult_are nsubj_difficult_which rcmod_data_difficult amod_data_hand-annotated prep_on_depend_data nsubj_depend_they advcl_depend_semi-supervised advcl_depend_supervised dep_al._2003 nn_al._et nn_al._Steedman dep_Henderson_al. num_Henderson_2004 num_al._2006 nn_al._et dep_McClosky_Henderson advmod_McClosky_al. nsubj_semi-supervised_methods dep_supervised_McClosky conj_or_supervised_semi-supervised advmod_supervised_currently auxpass_supervised_are nsubjpass_supervised_methods mark_supervised_While amod_methods_parsing amod_methods_most
P07-1078	N06-1020	o	Unknown words were not identified in -LRB- McClosky et al. 2006a -RRB- as a useful predictor for the benefit of self-training	prep_of_benefit_self-training det_benefit_the prep_for_predictor_benefit amod_predictor_useful det_predictor_a appos_McClosky_2006a dep_McClosky_al. nn_McClosky_et prep_as_identified_predictor prep_in_identified_McClosky neg_identified_not auxpass_identified_were nsubjpass_identified_words amod_words_Unknown
P07-1078	N06-1020	o	622 We also identified a length effect similar to that studied by -LRB- McClosky et al. 2006a -RRB- for self-training -LRB- using a reranker and large seed as detailed in Section 2 -RRB-	num_Section_2 prep_in_detailed_Section mark_detailed_as amod_seed_large conj_and_reranker_seed det_reranker_a dobj_using_seed dobj_using_reranker dep_McClosky_detailed dep_McClosky_using prep_for_McClosky_self-training appos_McClosky_2006a dep_McClosky_al. nn_McClosky_et dep_by_McClosky prep_studied_by vmod_that_studied prep_to_similar_that amod_effect_similar nn_effect_length det_effect_a dobj_identified_effect advmod_identified_also nsubj_identified_We rcmod_622_identified ccomp_``_622
P07-1078	N06-1020	o	Indeed in the II scenario -LRB- Steedman et al. 2003a McClosky et al. 2006a Charniak 1997 -RRB- reported no improvement of the base parser for small -LRB- 500 sentences in the first paper -RRB- and large -LRB- 40K sentences in the last two papers -RRB- seed datasets respectively	advmod_datasets_respectively nsubj_datasets_seed num_papers_two amod_papers_last det_papers_the rcmod_sentences_datasets prep_in_sentences_papers amod_sentences_40K amod_sentences_large amod_paper_first det_paper_the num_sentences_500 amod_sentences_small prep_for_parser_sentences nn_parser_base det_parser_the conj_and_improvement_sentences prep_in_improvement_paper prep_of_improvement_parser neg_improvement_no dobj_reported_sentences dobj_reported_improvement nsubj_reported_Steedman prep_in_reported_scenario advmod_reported_Indeed dep_Charniak_1997 appos_al._2006a nn_al._et nn_al._McClosky nn_al._et dep_Steedman_Charniak dep_Steedman_al. appos_Steedman_2003a dep_Steedman_al. num_scenario_II det_scenario_the
P07-1078	N06-1020	p	In the II OO and OI scenarios -LRB- McClosky et al 2006a 2006b -RRB- succeeded in improving the parser performance only when a reranker was used to reorder the 50-best list of the generative parser with a seed size of 40K sentences	num_sentences_40K prep_of_size_sentences nn_size_seed det_size_a amod_parser_generative det_parser_the prep_of_list_parser amod_list_50-best det_list_the dobj_reorder_list aux_reorder_to prep_with_used_size xcomp_used_reorder auxpass_used_was nsubjpass_used_reranker advmod_used_when det_reranker_a advmod_when_only nn_performance_parser det_performance_the advcl_improving_used dobj_improving_performance prepc_in_succeeded_improving nsubj_succeeded_scenarios nsubj_succeeded_OO prep_in_succeeded_II dep_McClosky_2006b appos_McClosky_2006a dep_McClosky_al nn_McClosky_et nn_scenarios_OI appos_OO_McClosky conj_and_OO_scenarios det_II_the
P07-1078	N06-1020	p	Recently -LRB- McClosky et al. 2006a McClosky et al. 2006b -RRB- have successfully applied self-training to various parser adaptation scenarios using the reranking parser of -LRB- Charniak and Johnson 2005 -RRB-	dep_Charniak_2005 conj_and_Charniak_Johnson dep_of_Johnson dep_of_Charniak prep_parser_of nn_parser_reranking det_parser_the dobj_using_parser vmod_scenarios_using nn_scenarios_adaptation nn_scenarios_parser amod_scenarios_various prep_to_applied_scenarios dobj_applied_self-training advmod_applied_successfully aux_applied_have nsubj_applied_McClosky advmod_applied_Recently appos_al._2006b nn_al._et nn_al._McClosky nn_al._et dep_McClosky_al. appos_McClosky_2006a dep_McClosky_al.
P07-1078	N06-1020	o	McClosky et al -LRB- 2006a -RRB- use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500 K unlabeled NANC corpus sentences as self-training data	amod_data_self-training nn_sentences_corpus nn_sentences_NANC amod_sentences_unlabeled nn_sentences_K num_sentences_2,500 dep_2,500_to number_2,500_50K prep_as_between_data pobj_between_sentences nn_data_seed nn_PennTreebank_WSJ det_PennTreebank_the prep_of_sections_PennTreebank num_sections_2-21 conj_and_use_between prep_as_use_data dobj_use_sections advmod_use_al nsubj_use_McClosky appos_al_2006a nn_al_et
P07-1078	N06-1020	p	As a result the good results of -LRB- McClosky et al 2006a 2006b -RRB- with large seed sets do not immediately imply success with small seed sets	nn_sets_seed amod_sets_small prep_with_success_sets dobj_imply_success advmod_imply_immediately neg_imply_not aux_imply_do nsubj_imply_results prep_as_imply_result nn_sets_seed amod_sets_large prep_with_McClosky_sets dep_McClosky_2006b appos_McClosky_2006a dep_McClosky_al nn_McClosky_et prep_of_results_McClosky amod_results_good det_results_the det_result_a
P07-1078	N06-1020	o	For the Brown corpus we based our division on -LRB- Bacchiani et al. 2006 McClosky et al. 2006b -RRB-	nn_al._et nn_al._McClosky appos_al._2006b dep_al._al. num_al._2006 nn_al._et amod_al._Bacchiani dep_on_al. poss_division_our advmod_based_on dobj_based_division nsubj_based_we prep_for_based_corpus amod_corpus_Brown det_corpus_the
P08-1037	N06-1020	p	Tighter integration of semantics into the parsing models possibly in the form of discriminative reranking models -LRB- Collins and Koo 2005 Charniak and Johnson 2005 McClosky et al. 2006 -RRB- is a promising way forward in this regard	det_regard_this prep_in_way_regard advmod_way_forward amod_way_promising det_way_a cop_way_is nsubj_way_integration num_McClosky_2006 nn_McClosky_al. nn_McClosky_et num_Charniak_2005 conj_and_Charniak_Johnson dep_Collins_McClosky dep_Collins_Johnson dep_Collins_Charniak amod_Collins_2005 conj_and_Collins_Koo appos_models_Koo appos_models_Collins nn_models_reranking amod_models_discriminative prep_of_form_models det_form_the nn_models_parsing det_models_the prep_in_integration_form advmod_integration_possibly prep_into_integration_models prep_of_integration_semantics amod_integration_Tighter
P08-1067	N06-1020	o	type system F1 % D Collins -LRB- 2000 -RRB- 89.7 Henderson -LRB- 2004 -RRB- 90.1 Charniak and Johnson -LRB- 2005 -RRB- 91.0 updated -LRB- Johnson 2006 -RRB- 91.4 this work 91.7 G Bod -LRB- 2003 -RRB- 90.7 Petrov and Klein -LRB- 2007 -RRB- 90.1 S McClosky et al.	nn_al._et dep_McClosky_al. nn_McClosky_S num_McClosky_90.1 num_McClosky_2007 dep_Klein_McClosky conj_and_Petrov_Klein num_Petrov_90.7 dep_Petrov_2003 dep_Bod_Klein dep_Bod_Petrov nn_Bod_G num_Bod_91.7 nn_Bod_work det_Bod_this num_Bod_91.4 nn_Bod_Johnson vmod_91.0_updated dep_2005_2006 dep_2005_Johnson dep_2005_91.0 dep_Johnson_2005 conj_and_Charniak_Bod num_Charniak_90.1 dep_Charniak_Henderson dep_Charniak_2000 dep_Charniak_Collins dep_Charniak_system appos_Henderson_2004 num_Henderson_89.7 nn_Collins_D nn_Collins_% num_Collins_F1 nn_system_type
P08-2026	N06-1020	o	However more recent results have shown that it can indeed improve parser performance -LRB- Bacchiani et al. 2006 McClosky et al. 2006a McClosky et al. 2006b -RRB-	nn_al._et nn_al._McClosky nn_al._et nn_al._McClosky dep_al._2006b dep_al._al. dep_al._2006a dep_al._al. dep_al._2006 nn_al._et amod_al._Bacchiani nn_performance_parser dep_improve_al. dobj_improve_performance advmod_improve_indeed aux_improve_can nsubj_improve_it mark_improve_that ccomp_shown_improve aux_shown_have nsubj_shown_results advmod_shown_However amod_results_recent advmod_recent_more
P08-2026	N06-1020	o	-LRB- 2006 -RRB- and McClosky et al.	nn_al._et nn_al._McClosky conj_and_2006_al.
P08-2026	N06-1020	o	This second point is emphasized by the second paper on self-training for adaptation -LRB- McClosky et al. 2006b -RRB-	nn_al._et appos_McClosky_2006b dep_McClosky_al. prep_on_paper_self-training amod_paper_second det_paper_the dep_emphasized_McClosky prep_for_emphasized_adaptation agent_emphasized_paper auxpass_emphasized_is nsubjpass_emphasized_point amod_point_second det_point_This
P09-1006	N06-1020	o	Recently there have been some works on using multiple treebanks for domain adaptation of parsers where these treebanks have the same grammar formalism -LRB- McClosky et al. 2006b Roark and Bacchiani 2003 -RRB-	dep_Roark_2003 conj_and_Roark_Bacchiani dep_McClosky_Bacchiani dep_McClosky_Roark appos_McClosky_2006b dep_McClosky_al. nn_McClosky_et nn_formalism_grammar amod_formalism_same det_formalism_the dobj_have_formalism nsubj_have_treebanks advmod_have_where det_treebanks_these dep_parsers_McClosky rcmod_parsers_have prep_of_adaptation_parsers nn_adaptation_domain amod_treebanks_multiple prep_for_using_adaptation dobj_using_treebanks prepc_on_works_using det_works_some cop_works_been aux_works_have expl_works_there advmod_works_Recently
P09-1006	N06-1020	o	4.3 Using Unlabeled Data for Parsing Recent studies on parsing indicate that the use of unlabeled data by self-training can help parsing on the WSJ data even when labeled data is relatively large -LRB- McClosky et al. 2006a Reichart and Rappoport 2007 -RRB-	dep_Reichart_2007 conj_and_Reichart_Rappoport dep_McClosky_Rappoport dep_McClosky_Reichart appos_McClosky_2006a dep_McClosky_al. nn_McClosky_et dep_large_McClosky advmod_large_relatively cop_large_is nsubj_large_data ccomp_labeled_large advmod_labeled_when advmod_labeled_even nn_data_WSJ det_data_the prep_on_parsing_data advcl_help_labeled xcomp_help_parsing aux_help_can nsubj_help_use mark_help_that amod_data_unlabeled prep_by_use_self-training prep_of_use_data det_use_the ccomp_indicate_help nsubj_indicate_4.3 prep_on_studies_parsing amod_studies_Recent nn_studies_Parsing prep_for_Data_studies nn_Data_Unlabeled dobj_Using_Data vmod_4.3_Using
P09-1006	N06-1020	o	Our results on Chinese data confirm previous findings on English data shown in -LRB- McClosky et al. 2006a Reichart and Rappoport 2007 -RRB-	dep_Reichart_2007 conj_and_Reichart_Rappoport dep_McClosky_Rappoport dep_McClosky_Reichart appos_McClosky_2006a dep_McClosky_al. nn_McClosky_et prep_in_shown_McClosky vmod_data_shown nn_data_English prep_on_findings_data amod_findings_previous dobj_confirm_findings nsubj_confirm_results amod_data_Chinese prep_on_results_data poss_results_Our
P09-1108	N06-1020	o	Uses for k-best lists include minimum Bayes risk decoding -LRB- Goodman 1998 Kumar and Byrne 2004 -RRB- discriminative reranking -LRB- Collins 2000 Charniak and Johnson 2005 -RRB- and discriminative training -LRB- Och 2003 McClosky et al. 2006 -RRB-	num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Och_McClosky appos_Och_2003 appos_training_Och amod_training_discriminative dep_Charniak_2005 conj_and_Charniak_Johnson dep_Collins_Johnson dep_Collins_Charniak amod_Collins_2000 appos_reranking_Collins amod_reranking_discriminative dep_Kumar_2004 conj_and_Kumar_Byrne dep_Goodman_Byrne dep_Goodman_Kumar amod_Goodman_1998 conj_and_decoding_training conj_and_decoding_reranking appos_decoding_Goodman nn_decoding_risk nn_decoding_Bayes amod_decoding_minimum dobj_include_training dobj_include_reranking dobj_include_decoding nsubj_include_Uses nn_lists_k-best prep_for_Uses_lists
W07-1033	N06-1020	p	We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser -LRB- McClosky et al. 2006 -RRB-	amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et nn_parser_syntactic nn_parser_English amod_parser_existing amod_parser_best det_parser_the prep_of_one_parser prep_of_performance_one det_performance_the dobj_boosted_performance advmod_boosted_successfully nsubj_boosted_which rcmod_tagger_boosted amod_tagger_n-best dep_self-training_McClosky prep_of_self-training_tagger dobj_apply_self-training aux_apply_to xcomp_plan_apply advmod_plan_also nsubj_plan_We
W08-1122	N06-1020	o	These parser output trees can by produced by a second parser in a co-training scenario -LRB- Steedman et al. 2003 -RRB- or by the same parser with a reranking component in a type of selftraining scenario -LRB- McCloskyetal. 2006 -RRB-	amod_McCloskyetal._2006 amod_scenario_selftraining prep_of_type_scenario det_type_a dep_component_McCloskyetal. prep_in_component_type nn_component_reranking det_component_a prep_with_parser_component amod_parser_same det_parser_the pobj_by_parser amod_Steedman_2003 dep_Steedman_al. nn_Steedman_et nn_scenario_co-training det_scenario_a prep_in_parser_scenario amod_parser_second det_parser_a conj_or_produced_by dep_produced_Steedman prep_by_produced_parser prepc_by_can_by prepc_by_can_produced rcmod_trees_can nn_trees_output nn_trees_parser det_trees_These dep_``_trees
W09-1008	N06-1020	o	-LRB- McClosky et al. 2006 -RRB- uses selftraining to perform this step -RRB- -LRB- 2 -RRB- smoothing usually this is performed using a markovization procedure -LRB- Collins 1999 Klein and Manning 2003a -RRB- and -LRB- 3 -RRB- make the data more coarse -LRB- i.e. clustering -RRB-	advmod_clustering_i.e. dep_coarse_clustering advmod_coarse_more nsubj_coarse_data det_data_the xcomp_make_coarse dep_make_3 conj_and_Klein_2003a conj_and_Klein_Manning conj_and_Collins_make dep_Collins_2003a dep_Collins_Manning dep_Collins_Klein amod_Collins_1999 dep_procedure_make dep_procedure_Collins nn_procedure_markovization det_procedure_a dobj_using_procedure xcomp_performed_using auxpass_performed_is nsubjpass_performed_this advmod_performed_usually nsubjpass_performed_smoothing dep_performed_2 dep_performed_uses det_step_this dobj_perform_step aux_perform_to xcomp_selftraining_perform vmod_uses_selftraining nsubj_uses_2006 dep_2006_McClosky dep_McClosky_al. nn_McClosky_et
W09-1104	N06-1020	o	Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training -LRB- Pereira and Schabes 1992 Klein and Manning 2004 Smith and Eisner 2004 Smith and Eisner 2005 -RRB- over bootstrapping approaches like selftraining -LRB- McClosky et al. 2006 -RRB- to feature-based enhancements of discriminative reranking models -LRB- Koo et al. 2008 -RRB- and the application of semisupervised SVMs -LRB- Wang et al. 2008 -RRB-	amod_Wang_2008 dep_Wang_al. nn_Wang_et amod_SVMs_semisupervised dep_application_Wang prep_of_application_SVMs det_application_the amod_Koo_2008 dep_Koo_al. nn_Koo_et nn_models_reranking amod_models_discriminative prep_of_enhancements_models amod_enhancements_feature-based dep_McClosky_2006 dep_McClosky_al. nn_McClosky_et prep_like_approaches_selftraining amod_approaches_bootstrapping amod_Smith_2005 conj_and_Smith_Eisner conj_and_Smith_Smith conj_and_Smith_2004 conj_and_Smith_Eisner dep_Pereira_Eisner dep_Pereira_Smith dep_Pereira_2004 dep_Pereira_Eisner dep_Pereira_Smith conj_and_Pereira_2004 conj_and_Pereira_Manning conj_and_Pereira_Klein conj_and_Pereira_1992 conj_and_Pereira_Schabes nn_training_EM appos_forms_2004 appos_forms_Manning appos_forms_Klein appos_forms_1992 appos_forms_Schabes appos_forms_Pereira prep_of_forms_training amod_forms_various prep_from_ranges_forms nn_ranges_parsing amod_ranges_supervised advmod_supervised_weakly prep_of_field_unsupervised det_field_the conj_and_Research_application dep_Research_Koo prep_to_Research_enhancements dep_Research_McClosky prep_over_Research_approaches conj_and_Research_ranges prep_in_Research_field
W09-2201	N06-1020	o	Such approaches have shown promise in applications such as web page classification -LRB- Blum and Mitchell 1998 -RRB- named entity classification -LRB- Collins and Singer 1999 -RRB- parsing -LRB- McClosky et al. 2006 -RRB- and machine translation -LRB- Ueffing 2006 -RRB-	amod_Ueffing_2006 dep_translation_Ueffing nn_translation_machine amod_McClosky_2006 dep_McClosky_al. nn_McClosky_et dep_parsing_McClosky amod_Collins_1999 conj_and_Collins_Singer dep_classification_Singer dep_classification_Collins nn_classification_entity dep_named_classification amod_Blum_1998 conj_and_Blum_Mitchell dep_classification_Mitchell dep_classification_Blum nn_classification_page nn_classification_web prep_such_as_applications_classification conj_and_promise_translation conj_and_promise_parsing vmod_promise_named prep_in_promise_applications dobj_shown_translation dobj_shown_parsing dobj_shown_promise aux_shown_have nsubj_shown_approaches amod_approaches_Such
W09-2205	N06-1020	o	5 Conclusions and Future Work The paper compares Structural Correspondence Learning -LRB- Blitzer et al. 2006 -RRB- with -LRB- various instances of -RRB- self-training -LRB- Abney 2007 McClosky et al. 2006 -RRB- for the adaptation of a parse selection model to Wikipedia domains	nn_domains_Wikipedia prep_to_model_domains nn_model_selection nn_model_parse det_model_a prep_of_adaptation_model det_adaptation_the num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Abney_McClosky appos_Abney_2007 prep_for_self-training_adaptation appos_self-training_Abney dep_instances_self-training dep_instances_of amod_instances_various amod_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et prep_with_Learning_instances dep_Learning_Blitzer nn_Learning_Correspondence amod_Learning_Structural dobj_compares_Learning nsubj_compares_paper dep_compares_Work dep_compares_Conclusions det_paper_The amod_Work_Future conj_and_Conclusions_Work num_Conclusions_5
W09-2205	N06-1020	o	We examine Structural Correspondence Learning -LRB- SCL -RRB- -LRB- Blitzer et al. 2006 -RRB- for this task and compare it to several variants of Self-training -LRB- Abney 2007 McClosky et al. 2006 -RRB-	num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Abney_McClosky appos_Abney_2007 appos_Self-training_Abney prep_of_variants_Self-training amod_variants_several prep_to_compare_variants dobj_compare_it nsubj_compare_We det_task_this amod_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et dep_Learning_Blitzer appos_Learning_SCL nn_Learning_Correspondence amod_Learning_Structural conj_and_examine_compare prep_for_examine_task dobj_examine_Learning nsubj_examine_We
W09-2205	N06-1020	o	Studies on self-training have focused mainly on generative constituent based parsing -LRB- Steedman et al. 2003 McClosky et al. 2006 Reichart and Rappoport 2007 -RRB-	dep_Reichart_2007 conj_and_Reichart_Rappoport dep_McClosky_Rappoport dep_McClosky_Reichart num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Steedman_McClosky appos_Steedman_2003 dep_Steedman_al. nn_Steedman_et dep_parsing_Steedman pobj_based_parsing prep_constituent_based appos_generative_constituent prep_on_focused_generative advmod_focused_mainly aux_focused_have nsubj_focused_Studies prep_on_Studies_self-training
W09-2205	N06-1020	o	Improvements are obtained -LRB- McClosky et al. 2006 McClosky and Charniak 2008 -RRB- showing that a reranker is necessary for successful self-training in such a high-resource scenario	amod_scenario_high-resource det_scenario_a amod_scenario_such prep_in_self-training_scenario amod_self-training_successful prep_for_necessary_self-training cop_necessary_is nsubj_necessary_reranker mark_necessary_that det_reranker_a ccomp_showing_necessary dep_McClosky_2008 conj_and_McClosky_Charniak dep_McClosky_Charniak dep_McClosky_McClosky appos_McClosky_2006 dep_McClosky_al. nn_McClosky_et xcomp_obtained_showing dep_obtained_McClosky auxpass_obtained_are nsubjpass_obtained_Improvements
W09-2205	N06-1020	o	The techniques examined are Structural Correspondence Learning -LRB- SCL -RRB- -LRB- Blitzer et al. 2006 -RRB- and Self-training -LRB- Abney 2007 McClosky et al. 2006 -RRB-	num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Abney_McClosky appos_Abney_2007 dep_Self-training_Abney amod_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et conj_and_Learning_Self-training dep_Learning_Blitzer appos_Learning_SCL nn_Learning_Correspondence amod_Learning_Structural cop_Learning_are nsubj_Learning_techniques vmod_techniques_examined det_techniques_The
W09-2205	N06-1020	o	1 Introduction and Motivation Parse selection constitutes an important part of many parsing systems -LRB- Hara et al. 2005 van Noord and Malouf 2005 McClosky et al. 2006 -RRB-	num_McClosky_2006 nn_McClosky_al. nn_McClosky_et num_Malouf_2005 conj_and_Noord_Malouf nn_Noord_van dep_Hara_McClosky dep_Hara_Malouf dep_Hara_Noord appos_Hara_2005 dep_Hara_al. nn_Hara_et nn_systems_parsing amod_systems_many appos_part_Hara prep_of_part_systems amod_part_important det_part_an dobj_constitutes_part nsubj_constitutes_selection nsubj_constitutes_Introduction nn_selection_Parse nn_selection_Motivation conj_and_Introduction_selection num_Introduction_1
D07-1068	N06-1025	o	Strube and Ponzetto explored the use of Wikipedia for measuring Semantic Relatedness between two concepts -LRB- 2006 -RRB- and for Coreference Resolution -LRB- 2006 -RRB-	appos_Resolution_2006 nn_Resolution_Coreference pobj_for_Resolution appos_concepts_2006 num_concepts_two prep_between_Relatedness_concepts nn_Relatedness_Semantic dobj_measuring_Relatedness prepc_for_use_measuring prep_of_use_Wikipedia det_use_the conj_and_explored_for dobj_explored_use nsubj_explored_Ponzetto nsubj_explored_Strube conj_and_Strube_Ponzetto
D07-1073	N06-1025	o	In fact many studies that try to exploit Wikipedia as a knowledge source have recently emerged -LRB- Bunescu and Pasca 2006 Toral and Munoz 2006 Ruiz-Casado et al. 2006 Ponzetto and Strube 2006 Strube and Ponzetto 2006 Zesch et al. 2007 -RRB-	num_Zesch_2007 nn_Zesch_al. nn_Zesch_et dep_Ponzetto_Zesch conj_and_Ponzetto_2006 conj_and_Ponzetto_Ponzetto conj_and_Ponzetto_Strube conj_and_Ponzetto_2006 conj_and_Ponzetto_Strube num_Ruiz-Casado_2006 nn_Ruiz-Casado_al. nn_Ruiz-Casado_et conj_and_Toral_2006 conj_and_Toral_Ponzetto conj_and_Toral_Strube conj_and_Toral_2006 conj_and_Toral_Strube conj_and_Toral_Ponzetto conj_and_Toral_Ruiz-Casado conj_and_Toral_2006 conj_and_Toral_Munoz dep_Bunescu_Ponzetto dep_Bunescu_Ruiz-Casado dep_Bunescu_2006 dep_Bunescu_Munoz dep_Bunescu_Toral conj_and_Bunescu_2006 conj_and_Bunescu_Pasca dep_emerged_2006 dep_emerged_Pasca dep_emerged_Bunescu advmod_emerged_recently aux_emerged_have nsubj_emerged_source mark_emerged_as nn_source_knowledge det_source_a advcl_exploit_emerged dobj_exploit_Wikipedia aux_exploit_to xcomp_try_exploit nsubj_try_that rcmod_studies_try amod_studies_many prep_in_studies_fact
D08-1067	N06-1025	o	-LRB- 2005 -RRB- Ponzetto and Strube -LRB- 2006 -RRB- -RRB- and the exploitation of advanced techniques that involve joint learning -LRB- e.g. Daume III and Marcu -LRB- 2005 -RRB- -RRB- and joint inference -LRB- e.g. Denis and Baldridge -LRB- 2007 -RRB- -RRB- for coreference resolution and a related extraction task	nn_task_extraction amod_task_related det_task_a nn_resolution_coreference appos_Baldridge_2007 conj_and_Denis_task prep_for_Denis_resolution conj_and_Denis_Baldridge dep_e.g._task dep_e.g._Baldridge dep_e.g._Denis dep_inference_e.g. amod_inference_joint appos_Marcu_2005 conj_and_III_inference conj_and_III_Marcu nn_III_Daume dep_,_inference dep_,_Marcu dep_,_III dep_-LRB-_e.g. amod_learning_joint dobj_involve_learning nsubj_involve_that rcmod_techniques_involve amod_techniques_advanced prep_of_exploitation_techniques det_exploitation_the dep_Strube_2006 conj_and_Ponzetto_exploitation conj_and_Ponzetto_Strube dep_Ponzetto_2005
D09-1081	N06-1025	o	Also on WS-353 our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures -LRB- Jarmasz and Szpakowicz 2003 -RRB- -LRB- .33 to .35 -RRB- and Wikipediabased methods -LRB- Ponzetto and Strube 2006 -RRB- -LRB- .19 to .48 -RRB- and very close to the results obtained by thesaurus-based -LRB- Jarmasz and Szpakowicz 2003 -RRB- -LRB- .55 -RRB- and LSA-based methods -LRB- Finkelstein et al. 2002 -RRB- -LRB- .56 -RRB-	amod_Finkelstein_2002 dep_Finkelstein_al. nn_Finkelstein_et amod_methods_LSA-based amod_methods_thesaurus-based dep_Jarmasz_2003 conj_and_Jarmasz_Szpakowicz conj_and_thesaurus-based_LSA-based dep_thesaurus-based_.55 dep_thesaurus-based_Szpakowicz dep_thesaurus-based_Jarmasz agent_obtained_methods vmod_results_obtained det_results_the prep_to_close_results advmod_close_very nsubj_close_variants dep_.48_to num_.48_.19 dep_Ponzetto_2006 conj_and_Ponzetto_Strube appos_methods_.48 dep_methods_Strube dep_methods_Ponzetto amod_methods_Wikipediabased dep_.35_to number_.35_.33 dep_Jarmasz_2003 conj_and_Jarmasz_Szpakowicz conj_and_measures_methods dep_measures_.35 dep_measures_Szpakowicz dep_measures_Jarmasz amod_measures_WordNet-based dobj_using_methods dobj_using_measures vmod_results_using amod_results_published prep_than_higher_results amod_score_higher nn_score_correlation det_score_a dep_obtained_.56 dep_obtained_Finkelstein conj_and_obtained_close dobj_obtained_score nsubj_obtained_word-cos-ll nsubj_obtained_variants prep_on_obtained_WS-353 advmod_obtained_Also conj_and_variants_word-cos-ll amod_variants_sense-filtered nn_variants_hybrid poss_variants_our
D09-1101	N06-1025	o	-LRB- 2004 -RRB- Ponzetto and Strube -LRB- 2006 -RRB- -RRB-	appos_Strube_2006 conj_and_Ponzetto_Strube dep_Ponzetto_2004
E09-1051	N06-1025	o	-LRB- Luo et al. 2004 Ponzetto and Strube 2006 -RRB- for other approaches with an evaluation based on true mentions only -RRB-	dep_mentions_only amod_mentions_true prepc_on_based_mentions vmod_evaluation_based det_evaluation_an prep_with_approaches_evaluation amod_approaches_other dep_Ponzetto_2006 conj_and_Ponzetto_Strube prep_for_Luo_approaches dep_Luo_Strube dep_Luo_Ponzetto appos_Luo_2004 dep_Luo_al. nn_Luo_et dep_''_Luo
N07-3003	N06-1025	o	and Semantic Knowledge Sources for Coreference Resolution Ponzetto & Strube -LRB- 2006 -RRB- and Strube & Ponzetto -LRB- 2006 -RRB- aimed at showing that the encyclopedia that anyone can edit can be indeed used as a semantic resource for research in NLP	prep_in_research_NLP prep_for_resource_research amod_resource_semantic det_resource_a prep_as_used_resource advmod_used_indeed auxpass_used_be aux_used_can nsubjpass_used_encyclopedia mark_used_that aux_edit_can nsubj_edit_anyone dobj_edit_that rcmod_encyclopedia_edit det_encyclopedia_the ccomp_showing_used prepc_at_aimed_showing vmod_Strube_aimed appos_Strube_2006 conj_and_Strube_Ponzetto appos_Strube_2006 conj_and_Ponzetto_Ponzetto conj_and_Ponzetto_Strube conj_and_Ponzetto_Strube nn_Ponzetto_Resolution nn_Ponzetto_Coreference prep_for_Sources_Strube prep_for_Sources_Strube prep_for_Sources_Ponzetto dep_Knowledge_Sources conj_and_Knowledge_Semantic
N07-3003	N06-1025	p	The novel idea presented in Strube & Ponzetto -LRB- 2006 -RRB- was to induce a semantic network from the Wikipedia categorization graph to compute measures of semantic relatedness	amod_relatedness_semantic prep_of_measures_relatedness dobj_compute_measures aux_compute_to nn_graph_categorization nn_graph_Wikipedia det_graph_the prep_from_network_graph amod_network_semantic det_network_a vmod_induce_compute dobj_induce_network aux_induce_to xcomp_was_induce nsubj_was_idea appos_Strube_2006 conj_and_Strube_Ponzetto prep_in_presented_Ponzetto prep_in_presented_Strube vmod_idea_presented amod_idea_novel det_idea_The
N07-3003	N06-1025	o	Accordingly in Ponzetto & Strube -LRB- 2006 -RRB- we used a machine learning based coreference resolution system to provide an extrinsic evaluation of the utility of WordNet and Wikipedia relatedness measures for NLP applications	nn_applications_NLP nn_measures_relatedness nn_measures_Wikipedia conj_and_WordNet_measures prep_of_utility_measures prep_of_utility_WordNet det_utility_the prep_for_evaluation_applications prep_of_evaluation_utility amod_evaluation_extrinsic det_evaluation_an dobj_provide_evaluation aux_provide_to vmod_system_provide nn_system_resolution nn_system_coreference amod_system_based dep_learning_system nn_learning_machine det_learning_a dobj_used_learning nsubj_used_we prep_in_used_Strube prep_in_used_Ponzetto advmod_used_Accordingly appos_Ponzetto_2006 conj_and_Ponzetto_Strube
N09-1065	N06-1025	o	2 Baseline Coreference Resolution System Our baseline coreference system implements the standard machine learning approach to coreference resolution -LRB- see Ng and Cardie -LRB- 2002b -RRB- Ponzetto and Strube -LRB- 2006 -RRB- Yang and Su -LRB- 2007 -RRB- for instance -RRB- which consists of probabilistic classification and clustering as described below	advmod_described_below mark_described_as conj_and_classification_clustering amod_classification_probabilistic advcl_consists_described prep_of_consists_clustering prep_of_consists_classification nsubj_consists_which appos_Su_2007 conj_and_Yang_Su appos_Strube_2006 conj_and_Ponzetto_Strube appos_Cardie_2002b conj_and_Ng_Cardie prep_for_see_instance dep_see_Su dep_see_Yang dep_see_Strube dep_see_Ponzetto dobj_see_Cardie dobj_see_Ng nn_resolution_coreference amod_approach_learning nn_approach_machine amod_machine_standard det_machine_the dep_implements_consists dep_implements_see prep_to_implements_resolution dobj_implements_approach nsubj_implements_system nn_system_coreference nn_system_baseline poss_system_Our rcmod_System_implements nn_System_Resolution nn_System_Coreference nn_System_Baseline num_System_2 dep_``_System
P07-1067	N06-1025	p	Recently Ponzetto and Strube -LRB- 2006 -RRB- suggest to mine semantic relatedness from Wikipedia which can deal with the data sparseness problem suffered by using WordNet	dobj_using_WordNet agent_suffered_using vmod_problem_suffered nn_problem_sparseness nn_problem_data det_problem_the prep_with_deal_problem aux_deal_can nsubj_deal_which rcmod_Wikipedia_deal amod_relatedness_semantic prep_from_mine_Wikipedia dobj_mine_relatedness aux_mine_to xcomp_suggest_mine nsubj_suggest_Strube nsubj_suggest_Ponzetto advmod_suggest_Recently appos_Strube_2006 conj_and_Ponzetto_Strube
P07-1068	N06-1025	o	-LRB- 2004 -RRB- -RRB- or Wikipedia -LRB- Ponzetto and Strube 2006 -RRB- and the contextual role played by an NP -LRB- see Bean and Riloff -LRB- 2004 -RRB- -RRB-	appos_Riloff_2004 conj_and_see_Riloff dobj_see_Bean det_NP_an agent_played_NP vmod_role_played amod_role_contextual det_role_the dep_Ponzetto_2006 conj_and_Ponzetto_Strube nn_Ponzetto_Wikipedia dep_2004_Riloff dep_2004_see conj_and_2004_role conj_or_2004_Strube conj_or_2004_Ponzetto dep_''_role dep_''_Ponzetto dep_''_2004
P07-1068	N06-1025	o	Following Ponzetto and Strube -LRB- 2006 -RRB- we consider an anaphoric reference NPi correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition	amod_partition_resulting det_partition_the nn_chain_coreference amod_chain_same det_chain_the prep_in_are_partition prep_in_are_chain nsubj_are_antecedent nsubj_are_NPi mark_are_if amod_antecedent_closest poss_antecedent_its conj_and_NPi_antecedent advcl_resolved_are advmod_resolved_correctly vmod_reference_resolved appos_reference_NPi amod_reference_anaphoric det_reference_an dobj_consider_reference nsubj_consider_we prep_following_consider_Strube prep_following_consider_Ponzetto appos_Strube_2006 conj_and_Ponzetto_Strube
P07-1068	N06-1025	o	-LRB- 2001 -RRB- and Ponzetto and Strube -LRB- 2006 -RRB- -RRB- we generate training instances as follows a positive instance is created for each anaphoric NP NPj and its closest antecedent NPi and a negative instance is created for NPj paired with each of the intervening NPs NPi +1 NPi +2 ... NPj1	num_NPi_+2 appos_NPi_NPj1 appos_NPi_NPi num_NPi_+1 amod_NPs_intervening det_NPs_the prep_of_each_NPs prep_with_paired_each vmod_NPj_paired dep_created_NPi prep_for_created_NPj auxpass_created_is nsubjpass_created_instance amod_instance_negative det_instance_a appos_antecedent_NPi amod_antecedent_closest poss_antecedent_its conj_and_NP_antecedent conj_and_NP_NPj amod_NP_anaphoric det_NP_each conj_and_created_created prep_for_created_antecedent prep_for_created_NPj prep_for_created_NP auxpass_created_is nsubjpass_created_instance amod_instance_positive det_instance_a mark_follows_as nn_instances_training parataxis_generate_created parataxis_generate_created advcl_generate_follows dobj_generate_instances nsubj_generate_we dep_generate_Strube dep_generate_Ponzetto dep_generate_2001 dep_Strube_2006 conj_and_2001_Strube conj_and_2001_Ponzetto
P08-4003	N06-1025	o	It is based on code and ideas from the system of Ponzetto and Strube -LRB- 2006 -RRB- but also includes some ideas from GUITAR -LRB- Steinberger et al. 2007 -RRB- and other coreference systems -LRB- Versley 2006 Yang et al. 2006 -RRB-	num_Yang_2006 nn_Yang_al. nn_Yang_et dep_Versley_Yang appos_Versley_2006 appos_systems_Versley nn_systems_coreference amod_systems_other amod_Steinberger_2007 dep_Steinberger_al. nn_Steinberger_et conj_and_GUITAR_systems dep_GUITAR_Steinberger prep_from_ideas_systems prep_from_ideas_GUITAR det_ideas_some dobj_includes_ideas advmod_includes_also nsubj_includes_It appos_Strube_2006 conj_and_Ponzetto_Strube prep_of_system_Strube prep_of_system_Ponzetto det_system_the prep_from_code_system conj_and_code_ideas conj_but_based_includes prep_on_based_ideas prep_on_based_code auxpass_based_is nsubjpass_based_It
P09-5006	N06-1025	o	We accordingly introduce approaches which attempt to include semantic information into the coreference models from a variety of knowledge sources e.g. WordNet -LRB- Harabagiu et al. 2001 -RRB- Wikipedia -LRB- Ponzetto & Strube 2006 -RRB- and automatically harvested patterns -LRB- Poesio et al. 2002 Markert & Nissim 2005 Yang & Su 2007 -RRB-	dep_Yang_2007 conj_and_Yang_Su dep_Markert_Su dep_Markert_Yang conj_and_Markert_2005 conj_and_Markert_Nissim dep_Poesio_2005 dep_Poesio_Nissim dep_Poesio_Markert appos_Poesio_2002 dep_Poesio_al. nn_Poesio_et dep_patterns_Poesio amod_patterns_harvested advmod_harvested_automatically dep_Ponzetto_2006 conj_and_Ponzetto_Strube dep_Wikipedia_Strube dep_Wikipedia_Ponzetto amod_Harabagiu_2001 dep_Harabagiu_al. nn_Harabagiu_et conj_and_WordNet_patterns conj_and_WordNet_Wikipedia dep_WordNet_Harabagiu pobj_e.g._patterns pobj_e.g._Wikipedia pobj_e.g._WordNet nn_sources_knowledge prep_of_variety_sources det_variety_a nn_models_coreference det_models_the prep_information_e.g. prep_from_information_variety prep_into_information_models amod_information_semantic dobj_include_information aux_include_to xcomp_attempt_include nsubj_attempt_which rcmod_approaches_attempt dobj_introduce_approaches advmod_introduce_accordingly nsubj_introduce_We ccomp_``_introduce
C08-1145	N06-1032	o	Riezler and Maxwell -LRB- 2006 -RRB- combine transfer-based and statistical MT they back off to the SMT translation when the grammar is inadequate analysing the grammar to determine this	dobj_determine_this aux_determine_to det_grammar_the vmod_analysing_determine dobj_analysing_grammar xcomp_inadequate_analysing cop_inadequate_is nsubj_inadequate_grammar advmod_inadequate_when det_grammar_the nn_translation_SMT det_translation_the prep_to_off_translation advmod_off_back rcmod_they_inadequate advmod_they_off amod_MT_statistical amod_MT_transfer-based conj_and_transfer-based_statistical parataxis_combine_they dobj_combine_MT nsubj_combine_Maxwell nsubj_combine_Riezler appos_Maxwell_2006 conj_and_Riezler_Maxwell
D07-1028	N06-1032	o	It is an important and growing field of natural language processing with applications in areas such as transferbased machine translation -LRB- Riezler and Maxwell 2006 -RRB- and sentence condensation -LRB- Riezler et al. 2003 -RRB-	amod_Riezler_2003 dep_Riezler_al. nn_Riezler_et dep_condensation_Riezler nn_condensation_sentence num_Riezler_2006 conj_and_Riezler_Maxwell conj_and_translation_condensation appos_translation_Maxwell appos_translation_Riezler nn_translation_machine amod_translation_transferbased prep_such_as_areas_condensation prep_such_as_areas_translation prep_in_applications_areas nn_processing_language amod_processing_natural prep_with_field_applications prep_of_field_processing amod_field_growing amod_field_important det_field_an cop_field_is nsubj_field_It conj_and_important_growing
P08-1114	N06-1032	o	Finally another soft-constraint approach that can also be viewed as coming from the data-driven side adding syntax is taken by Riezler and Maxwell -LRB- 2006 -RRB-	appos_Maxwell_2006 conj_and_Riezler_Maxwell agent_taken_Maxwell agent_taken_Riezler auxpass_taken_is nsubjpass_taken_approach advmod_taken_Finally dobj_adding_syntax amod_side_data-driven det_side_the prep_from_coming_side prepc_as_viewed_coming auxpass_viewed_be advmod_viewed_also aux_viewed_can nsubjpass_viewed_that vmod_approach_adding rcmod_approach_viewed amod_approach_soft-constraint det_approach_another
P08-1114	N06-1032	p	Riezler and Maxwell -LRB- 2006 -RRB- do not achieve higher BLEU scores but do score better according to human grammaticality judgments for in-coverage cases	amod_cases_in-coverage prep_for_judgments_cases nn_judgments_grammaticality amod_judgments_human pobj_score_judgments prepc_according_to_score_to advmod_score_better aux_score_do nsubj_score_Riezler nn_scores_BLEU amod_scores_higher conj_but_achieve_score dobj_achieve_scores neg_achieve_not aux_achieve_do nsubj_achieve_Maxwell nsubj_achieve_Riezler appos_Maxwell_2006 conj_and_Riezler_Maxwell
W06-1628	N06-1032	o	Riezler and Maxwell -LRB- 2006 -RRB- describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English	prep_in_parse_English dobj_parse_structures vmod_LFG_parse amod_structures_parse nn_structures_LFG prep_into_maps_LFG prep_in_maps_German dobj_maps_structures nsubj_maps_that rcmod_model_maps amod_model_probabilistic det_model_a dobj_learning_model prepc_for_method_learning det_method_a dobj_describe_method nsubj_describe_Maxwell nsubj_describe_Riezler appos_Maxwell_2006 conj_and_Riezler_Maxwell
W08-0319	N06-1032	p	Riezler and III -LRB- 2006 -RRB- report an improvement in MT grammaticality on a very restricted test set short sentences parsable by an LFG grammar without back-off rules	amod_rules_back-off prep_without_grammar_rules nn_grammar_LFG det_grammar_an prep_by_parsable_grammar amod_sentences_parsable amod_sentences_short nn_set_test amod_set_restricted det_set_a advmod_restricted_very nn_grammaticality_MT dep_improvement_sentences prep_on_improvement_set prep_in_improvement_grammaticality det_improvement_an dobj_report_improvement nsubj_report_III nsubj_report_Riezler appos_III_2006 conj_and_Riezler_III
D07-1078	N06-1033	o	We used a bottom-up CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et nn_procedure_binarization amod_procedure_synchronous det_procedure_a prep_via_obtained_procedure vmod_rules_obtained nn_rules_xRs amod_rules_binary prep_with_works_rules nsubj_works_that rcmod_decoder_works amod_decoder_CKY-style amod_decoder_bottom-up det_decoder_a dep_used_Zhang dobj_used_decoder nsubj_used_We
D07-1078	N06-1033	o	2 Related Research Several researchers -LRB- Melamed et al. 2004 Zhang et al. 2006 -RRB- have already proposed methods for binarizing synchronous grammars in the context of machine translation	nn_translation_machine prep_of_context_translation det_context_the amod_grammars_synchronous prep_in_binarizing_context dobj_binarizing_grammars prepc_for_methods_binarizing dobj_proposed_methods advmod_proposed_already aux_proposed_have nsubj_proposed_researchers num_Zhang_2006 nn_Zhang_al. nn_Zhang_et dep_Melamed_Zhang appos_Melamed_2004 dep_Melamed_al. nn_Melamed_et appos_researchers_Melamed amod_researchers_Several nn_researchers_Research nn_researchers_Related num_researchers_2 ccomp_``_proposed
D07-1079	N06-1033	o	Translation rules can look like phrase pairs with syntax decoration NPB -LRB- NNP -LRB- prime -RRB- NNP -LRB- minister -RRB- NNP -LRB- keizo -RRB- NNP -LRB- obuchi -RRB- -RRB- BUFDFKEUBWAZ carry extra contextual constraints VP -LRB- VBD -LRB- said -RRB- x0 SBAR-C -RRB- DKx0 -LRB- according to this rule DK can translate to said only if some Chinese sequence to the right ofDK is translated into an SBAR-C -RRB- be non-constituent phrases VP -LRB- VBD -LRB- said -RRB- SBAR-C -LRB- IN -LRB- that -RRB- x0 S-C -RRB- -RRB- DKx0 VP -LRB- VBD -LRB- pointed -RRB- PRT -LRB- RP -LRB- out -RRB- -RRB- x0 SBAR-C -RRB- DXGPx0 contain non-contiguous phrases effectively phrases with holes PP -LRB- IN -LRB- on -RRB- NP-C -LRB- NPB -LRB- DT -LRB- the -RRB- x0 NNP -RRB- -RRB- NN -LRB- issue -RRB- -RRB- -RRB- -RRB- GRx0 EVABG6 PP -LRB- IN -LRB- on -RRB- NP-C -LRB- NPB -LRB- DT -LRB- the -RRB- NN -LRB- issue -RRB- -RRB- x0 PP -RRB- -RRB- GRx0 EVEVABABG6 be purely structural -LRB- no words -RRB- S -LRB- x0 NP-C x1 VP -RRB- x0 x1 re-order their children NP-C -LRB- NPB -LRB- DT -LRB- the -RRB- x0 NN -RRB- PP -LRB- IN -LRB- of -RRB- x1 NP-C -RRB- -RRB- x1 DFx0 Decoding with this model produces a tree in the target language bottom-up by parsing the foreign string using a CYK parser and a binarized rule set -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et dep_set_Zhang dep_rule_set amod_rule_binarized det_rule_a conj_and_parser_rule nn_parser_CYK det_parser_a dobj_using_rule dobj_using_parser amod_string_foreign det_string_the vmod_parsing_using dobj_parsing_string nn_language_target det_language_the det_tree_a dobj_produces_tree nsubj_produces_DT det_model_this prep_with_Decoding_model vmod_DFx0_Decoding nn_DFx0_x1 nn_DFx0_PP dep_x1_NP-C prep_in_PP_x1 dep_PP_of dep_x0_NN det_x0_the dep_DT_DFx0 dep_DT_x0 prepc_by_NPB_parsing amod_NPB_bottom-up prep_in_NPB_language rcmod_NPB_produces dep_NP-C_NPB poss_children_their dep_re-order_NP-C dobj_re-order_children nsubj_re-order_x1 nn_x1_x0 nn_x1_VP dep_x1_re-order nn_x1_NP-C dep_x0_x1 nn_x0_S neg_words_no dep_structural_x0 dep_structural_words advmod_structural_purely cop_structural_be nsubj_structural_EVEVABABG6 nn_EVEVABABG6_GRx0 nn_issue_NN det_issue_the appos_DT_issue dep_NPB_PP dep_NPB_x0 dep_NPB_DT dep_NP-C_NPB dep_on_structural dep_on_NP-C pcomp_IN_on prep_PP_IN nn_PP_EVABG6 nn_PP_GRx0 nn_PP_VP appos_NN_issue dep_x0_NNP det_x0_the dep_DT_x0 dep_NPB_NN appos_NPB_DT dep_NP-C_NPB pobj_on_NP-C pcomp_IN_on prep_PP_IN prep_with_phrases_holes amod_phrases_non-contiguous dobj_contain_phrases nsubj_contain_DXGPx0 dep_contain_x0 mark_contain_IN nn_DXGPx0_VP nn_x0_PRT dep_RP_out appos_PRT_RP dep_VBD_SBAR-C dep_VBD_x0 dep_VBD_pointed appos_VP_VBD nn_VP_DKx0 dep_x0_S-C mark_x0_that dep_VBD_PP dep_VBD_phrases advmod_VBD_effectively dep_VBD_contain dep_VBD_SBAR-C dep_VBD_said dep_VP_VBD dep_phrases_PP amod_phrases_non-constituent cop_phrases_be dep_phrases_translated det_SBAR-C_an prep_into_translated_SBAR-C auxpass_translated_is nsubjpass_translated_sequence mark_translated_if advmod_translated_only amod_ofDK_right det_ofDK_the prep_to_sequence_ofDK amod_sequence_Chinese det_sequence_some ccomp_said_phrases xcomp_translate_said prep_translate_to aux_translate_can nsubj_translate_DK pobj_translate_rule prepc_according_to_translate_to dobj_translate_DKx0 det_rule_this nn_DKx0_VP dep_VBD_SBAR-C dep_VBD_x0 dep_VBD_said appos_VP_VBD dep_constraints_translate amod_constraints_contextual amod_constraints_extra dobj_carry_constraints nsubj_carry_BUFDFKEUBWAZ dep_BUFDFKEUBWAZ_NNP nn_BUFDFKEUBWAZ_NPB appos_NNP_obuchi nn_NNP_NNP appos_NNP_keizo nn_NNP_NNP appos_NNP_minister dep_NNP_NNP dep_NNP_prime nn_decoration_syntax prep_with_pairs_decoration nn_pairs_phrase parataxis_look_carry prep_like_look_pairs aux_look_can nsubj_look_rules nn_rules_Translation
D08-1060	N06-1033	o	A CYK-style decoder has to rely on binarization to preprocess the grammar as did in -LRB- Zhang et al. 2006 -RRB- to handle multi-nonterminal rules	amod_rules_multi-nonterminal dobj_handle_rules aux_handle_to num_Zhang_2006 dep_Zhang_al. nn_Zhang_et prepc_in_did_handle dep_did_Zhang det_grammar_the prepc_as_preprocess_did dobj_preprocess_grammar aux_preprocess_to xcomp_rely_preprocess prep_on_rely_binarization aux_rely_to xcomp_has_rely nsubj_has_decoder amod_decoder_CYK-style det_decoder_A
D08-1060	N06-1033	o	Work in -LRB- Al-Onaizan and Kishore 2006 Xiong et al. 2006 Zens et al. 2004 Kumar and Byrne 2005 Tillmann and Zhang 2005 -RRB- modeled the limited information available at phrase-boundaries	prep_at_available_phrase-boundaries amod_information_available amod_information_limited det_information_the dobj_modeled_information nsubj_modeled_Tillmann nsubj_modeled_2005 nsubj_modeled_Byrne nsubj_modeled_Kumar amod_Tillmann_2005 conj_and_Tillmann_Zhang conj_and_Kumar_Zhang conj_and_Kumar_Tillmann conj_and_Kumar_2005 conj_and_Kumar_Byrne num_Zens_2004 nn_Zens_al. nn_Zens_et num_Xiong_2006 nn_Xiong_al. nn_Xiong_et parataxis_Al-Onaizan_modeled dep_Al-Onaizan_Zens dep_Al-Onaizan_Xiong dep_Al-Onaizan_2006 conj_and_Al-Onaizan_Kishore prep_in_Work_Kishore prep_in_Work_Al-Onaizan
D08-1066	N06-1033	o	This is in line with earlier work on consistent estimation for similar models -LRB- Zollmann and Simaan 2006 -RRB- and agrees with the most up-to-date work that employs Bayesian priors over the estimates -LRB- Zhang et al. 2008 -RRB-	amod_Zhang_2008 dep_Zhang_al. nn_Zhang_et det_estimates_the amod_priors_Bayesian prep_over_employs_estimates dobj_employs_priors nsubj_employs_that rcmod_work_employs amod_work_up-to-date det_work_the advmod_up-to-date_most prep_with_agrees_work nsubj_agrees_This dep_Zollmann_2006 conj_and_Zollmann_Simaan appos_models_Simaan appos_models_Zollmann amod_models_similar prep_for_estimation_models amod_estimation_consistent amod_work_earlier prep_with_line_work dep_is_Zhang conj_and_is_agrees prep_on_is_estimation prep_in_is_line nsubj_is_This ccomp_``_agrees ccomp_``_is
D08-1066	N06-1033	o	Our work expands on the general approach taken by -LRB- DeNero et al. 2006 Moore and Quirk 2007 -RRB- but arrives at insights similar to those of the most recent work -LRB- Zhang et al. 2006 -RRB- albeit in a completely different manner	amod_manner_different det_manner_a advmod_different_completely pobj_in_manner pcomp_albeit_in num_Zhang_2006 dep_Zhang_al. nn_Zhang_et amod_work_recent det_work_the advmod_recent_most prep_of_those_work prep_to_similar_those amod_insights_similar prep_arrives_albeit dep_arrives_Zhang prep_at_arrives_insights nsubj_arrives_work num_Moore_2007 conj_and_Moore_Quirk dep_DeNero_Quirk dep_DeNero_Moore appos_DeNero_2006 dep_DeNero_al. nn_DeNero_et agent_taken_DeNero vmod_approach_taken amod_approach_general det_approach_the conj_but_expands_arrives prep_on_expands_approach nsubj_expands_work poss_work_Our
D08-1066	N06-1033	o	3.1 Binarizable segmentations -LRB- a -RRB- Following -LRB- Zhang et al. 2006 Huang et al. 2008 -RRB- every sequence of phrase alignments can be viewed 1For example if the cut-off on phrase pairs is ten words all sentence pairs smaller than ten words in the training data will be included as phrase pairs as well	advmod_well_as advmod_pairs_well nn_pairs_phrase prep_as_included_pairs auxpass_included_be aux_included_will nsubjpass_included_pairs advcl_included_words nn_data_training det_data_the prep_in_words_data num_words_ten prep_than_smaller_words amod_pairs_smaller nn_pairs_sentence det_pairs_all num_words_ten cop_words_is nsubj_words_cut-off mark_words_if nn_pairs_phrase prep_on_cut-off_pairs det_cut-off_the nn_example_1For dep_viewed_included dobj_viewed_example auxpass_viewed_be aux_viewed_can nsubjpass_viewed_segmentations nn_alignments_phrase prep_of_sequence_alignments det_sequence_every num_Huang_2008 nn_Huang_al. nn_Huang_et dep_Zhang_Huang num_Zhang_2006 dep_Zhang_al. nn_Zhang_et appos_segmentations_sequence appos_segmentations_Zhang vmod_segmentations_Following appos_segmentations_a amod_segmentations_Binarizable num_segmentations_3.1
D08-1066	N06-1033	o	-LRB- Zhang et al. 2006 Huang et al. 2008 -RRB- -RRB- a binarizable segmentation/permutation can be recognized by a binarized Synchronous Context-Free Grammar -LRB- SCFG -RRB- i.e. an SCFG in which the right hand sides of all non-lexical rules constitute binarizable permutations	amod_permutations_binarizable dobj_constitute_permutations nsubj_constitute_sides prep_in_constitute_which amod_rules_non-lexical det_rules_all prep_of_sides_rules nn_sides_hand amod_sides_right det_sides_the rcmod_SCFG_constitute det_SCFG_an advmod_SCFG_i.e. appos_Grammar_SCFG appos_Grammar_SCFG nn_Grammar_Context-Free amod_Grammar_Synchronous amod_Grammar_binarized det_Grammar_a agent_recognized_Grammar auxpass_recognized_be aux_recognized_can nsubjpass_recognized_segmentation/permutation amod_segmentation/permutation_binarizable det_segmentation/permutation_a num_Huang_2008 nn_Huang_al. nn_Huang_et rcmod_Zhang_recognized dep_Zhang_Huang appos_Zhang_2006 dep_Zhang_al. nn_Zhang_et dep_''_Zhang
D08-1066	N06-1033	o	While this heuristic estimator gives good empirical results it does not seem to optimize any intuitively reasonable objective function of the -LRB- wordaligned -RRB- parallel corpus -LRB- see e.g. -LRB- DeNero et al. 2006 -RRB- -RRB- The mounting number of efforts attacking this problem over the last few years -LRB- DeNero et al. 2006 Marcu and Wong 2002 Birch et al. 2006 Moore and Quirk 2007 Zhang et al. 2008 -RRB- exhibits its difficulty	poss_difficulty_its dobj_exhibits_difficulty nsubj_exhibits_2007 nsubj_exhibits_Quirk nsubj_exhibits_Moore nsubj_exhibits_Birch nsubj_exhibits_Wong nsubj_exhibits_Marcu num_Zhang_2008 nn_Zhang_al. nn_Zhang_et num_Birch_2006 nn_Birch_al. nn_Birch_et dep_Marcu_Zhang conj_and_Marcu_2007 conj_and_Marcu_Quirk conj_and_Marcu_Moore conj_and_Marcu_Birch num_Marcu_2002 conj_and_Marcu_Wong parataxis_DeNero_exhibits appos_DeNero_2006 dep_DeNero_al. nn_DeNero_et amod_years_few amod_years_last det_years_the det_problem_this prep_over_attacking_years dobj_attacking_problem vmod_efforts_attacking dep_number_DeNero prep_of_number_efforts amod_number_mounting det_number_The amod_DeNero_2006 dep_DeNero_al. nn_DeNero_et dep_see_number dep_see_DeNero dep_see_e.g. amod_corpus_parallel dep_corpus_wordaligned det_corpus_the prep_of_function_corpus amod_function_objective amod_function_reasonable advmod_function_intuitively det_function_any dobj_optimize_function aux_optimize_to parataxis_seem_see xcomp_seem_optimize neg_seem_not aux_seem_does nsubj_seem_it advcl_seem_gives amod_results_empirical amod_results_good dobj_gives_results nsubj_gives_estimator mark_gives_While nn_estimator_heuristic det_estimator_this
D09-1007	N06-1033	o	Binarizing the grammars -LRB- Zhang et al. 2006 -RRB- further increases the size of these sets due to the introduction of virtual nonterminals	amod_nonterminals_virtual prep_of_introduction_nonterminals det_introduction_the det_sets_these prep_due_to_size_introduction prep_of_size_sets det_size_the dep_increases_size amod_increases_further amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et det_grammars_the dep_Binarizing_increases dep_Binarizing_Zhang dobj_Binarizing_grammars ccomp_``_Binarizing
D09-1037	N06-1033	o	7Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding in particular rule binarisation -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et nn_binarisation_rule amod_binarisation_particular nn_decoding_grammar amod_decoding_synchronous prep_to_beneficial_decoding cop_beneficial_be aux_beneficial_to xcomp_shown_beneficial vmod_features_shown amod_features_certain dep_lacks_Zhang prep_in_lacks_binarisation dobj_lacks_features nsubj_lacks_decoder nn_decoder_7Our
D09-1038	N06-1033	o	The baseline system is based on the synchronous binarization -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et appos_binarization_Zhang amod_binarization_synchronous det_binarization_the prep_on_based_binarization auxpass_based_is nsubjpass_based_system nn_system_baseline det_system_The
D09-1038	N06-1033	o	4.2 Binarization Schemes Besides the baseline -LRB- Zhang et al. 2006 -RRB- and iterative cost reduction binarization methods we also perform right-heavy and random synchronous binarizations for comparison	amod_binarizations_synchronous amod_binarizations_random amod_binarizations_right-heavy conj_and_right-heavy_random prep_for_perform_comparison dobj_perform_binarizations advmod_perform_also nsubj_perform_we nsubj_perform_methods nsubj_perform_Schemes nn_methods_binarization nn_methods_reduction nn_methods_cost amod_methods_iterative amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et det_baseline_the conj_and_Schemes_methods dep_Schemes_Zhang prep_besides_Schemes_baseline nn_Schemes_Binarization num_Schemes_4.2
D09-1038	N06-1033	o	Given the following SCFG rule VP VB NP JJR VB NP will be JJR we can obtain a set of equivalent binary rules using the synchronous binarization method -LRB- Zhang et al. 2006 -RRB- as follows VP V1 JJR V1 JJR V1 VB V2 VB V2 V2 NP NP will be This binarization is shown with the solid lines as binarization -LRB- a -RRB- in Figure 1	num_Figure_1 prep_in_binarization_Figure dep_binarization_a prep_as_lines_binarization amod_lines_solid det_lines_the prep_with_shown_lines auxpass_shown_is nsubjpass_shown_binarization det_binarization_This cop_binarization_be aux_binarization_will nsubj_binarization_NP nn_NP_V2 nn_NP_V2 nn_NP_VB nn_V2_VB nn_V2_V1 nn_V2_JJR nn_V2_V1 nn_JJR_V1 nn_JJR_VP dep_JJR_follows dep_JJR_JJR mark_follows_as amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et nn_method_binarization amod_method_synchronous det_method_the dobj_using_method amod_rules_binary amod_rules_equivalent prep_of_set_rules det_set_a vmod_obtain_using dobj_obtain_set aux_obtain_can nsubj_obtain_we dep_JJR_Zhang ccomp_JJR_obtain cop_JJR_be aux_JJR_will nsubj_JJR_NP nn_NP_VB rcmod_JJR_shown conj_JJR_NP conj_JJR_V2 conj_JJR_JJR nn_JJR_NP nn_JJR_VB nn_JJR_VP prep_JJR_Given nn_rule_SCFG amod_rule_following det_rule_the pobj_Given_rule
D09-1038	N06-1033	o	Generally two edges can be re-combined if they satisfy the following two constraints 1 -RRB- the LHS -LRB- left-hand side -RRB- nonterminals are identical and the sub-alignments are the same -LRB- Zhang et al. 2006 -RRB- and 2 -RRB- the boundary words 1 on both sides of the partial translations are equal between the two edges -LRB- Chiang 2007 -RRB-	amod_Chiang_2007 appos_edges_Chiang num_edges_two det_edges_the prep_between_equal_edges cop_equal_are nsubj_equal_words dep_equal_2 amod_translations_partial det_translations_the prep_of_sides_translations det_sides_both prep_on_words_sides num_words_1 nn_words_boundary det_words_the amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et det_same_the cop_same_are nsubj_same_sub-alignments det_sub-alignments_the conj_and_identical_equal dep_identical_Zhang conj_and_identical_same cop_identical_are nsubj_identical_nonterminals dep_identical_1 nn_nonterminals_LHS det_nonterminals_the amod_side_left-hand appos_LHS_side num_constraints_two amod_constraints_following det_constraints_the dobj_satisfy_constraints nsubj_satisfy_they mark_satisfy_if parataxis_re-combined_equal parataxis_re-combined_same parataxis_re-combined_identical advcl_re-combined_satisfy auxpass_re-combined_be aux_re-combined_can nsubjpass_re-combined_edges advmod_re-combined_Generally num_edges_two ccomp_``_re-combined
D09-1038	N06-1033	n	V B N P J J R -LRB- a -RRB- -LRB- b -RRB- V 2 V 1 V 2 ' V 1 ' V P V B N P w ill b e J J R Figure 1 Two different binarizations -LRB- a -RRB- and -LRB- b -RRB- of the same SCFG rule distinguished by the solid lines and dashed lines -LRB- W e h o p e t h e s i t u a t i o n w i l l b e b e t t e r -RRB- N P J J R d e c o d i n g m a t c h 8 7 4 r u l e s m a t c h 6 2 r u l e s c o m p e t i n g e d g e s 8 0 1 c o m p e t i n g e d g e s 5 7 Figure 2 Edge competitions caused by different binarizations The edge competition problem for SMT decoding is not addressed in previous work -LRB- Zhang et al. 2006 Huang 2007 -RRB- in which each SCFG rule is binarized in a fixed way	amod_way_fixed det_way_a prep_in_binarized_way auxpass_binarized_is nsubjpass_binarized_rule prep_in_binarized_which dep_binarized_Zhang nn_rule_SCFG det_rule_each dep_Huang_2007 dep_Zhang_Huang appos_Zhang_2006 dep_Zhang_al. nn_Zhang_et amod_work_previous xcomp_addressed_binarized prep_in_addressed_work neg_addressed_not auxpass_addressed_is nsubjpass_addressed_s nn_decoding_SMT prep_for_problem_decoding nn_problem_competition nn_problem_edge det_problem_The dep_binarizations_problem amod_binarizations_different agent_caused_binarizations vmod_competitions_caused nn_competitions_Edge num_Figure_2 num_Figure_7 number_7_5 dep_s_competitions dep_s_Figure dep_s_e dep_s_g nn_g_d dep_g_e nn_g_g dep_g_n dep_g_i dep_g_t dep_g_p dep_g_m dep_g_o dep_g_c dep_g_0 dep_t_e number_c_1 number_0_8 nn_s_e nn_g_d dep_g_e nn_g_g dep_g_n dep_g_i dep_g_t dep_g_p dep_g_m dep_g_o dep_g_c dep_g_s dep_g_h dep_g_m dep_t_e nn_l_u nn_l_r number_2_6 dep_h_e dep_h_l num_h_2 nn_h_c nn_h_t det_h_a nn_m_s dep_m_e dep_m_l dep_m_u nn_m_r dep_4_7 number_7_8 dep_h_s dep_h_g num_h_4 nn_h_c nn_h_t det_h_a npadvmod_m_h nn_m_g nn_m_n nn_m_i nn_m_d nn_m_o nn_m_c dep_m_e num_d_m nn_d_R nn_d_J nn_d_J nn_d_P nn_d_N dep_d_r dep_d_b dep_r_e nn_r_t nn_r_t dep_r_e dep_r_b dep_r_e dep_r_b dep_r_u dep_r_t dep_r_e dep_r_h dep_r_t dep_r_e dep_r_p dep_r_o dep_r_W dep_r_lines nn_b_l nn_b_l nn_b_i nn_b_w nn_b_n nn_b_o nn_b_i nn_b_t det_b_a nn_t_i nn_t_s nn_o_h dep_o_e amod_lines_dashed amod_lines_solid det_lines_the agent_distinguished_lines vmod_rule_distinguished nn_rule_SCFG amod_rule_same det_rule_the conj_and_b_r prep_of_b_rule dep_binarizations_addressed conj_and_binarizations_d appos_binarizations_a amod_binarizations_different num_binarizations_Two dep_Figure_d dep_Figure_binarizations num_Figure_1 nn_Figure_R nn_Figure_J nn_Figure_J dep_Figure_e nn_Figure_b amod_Figure_ill nn_Figure_w nn_Figure_P nn_Figure_N nn_Figure_B nn_Figure_V nn_Figure_P nn_Figure_V dep_Figure_V num_V_1 num_V_2 dep_V_Figure num_V_1 nn_V_V num_V_2 nn_V_V nn_V_R appos_R_b appos_R_a nn_R_J nn_R_J nn_R_P nn_R_N nn_R_B nn_R_V ccomp_``_V
D09-1038	N06-1033	n	The experimental results show that our method outperforms the synchronous binarization method -LRB- Zhang et al. 2006 -RRB- with over 0.8 BLEU scores on both NIST 2005 and NIST 2008 Chinese-to-English evaluation data sets	nn_sets_data nn_sets_evaluation nn_sets_Chinese-to-English num_sets_2008 nn_sets_NIST conj_and_NIST_sets num_NIST_2005 dep_both_sets dep_both_NIST prep_on_scores_both nn_scores_BLEU num_scores_0.8 quantmod_0.8_over num_Zhang_2006 dep_Zhang_al. nn_Zhang_et appos_method_Zhang nn_method_binarization amod_method_synchronous det_method_the prep_with_outperforms_scores dobj_outperforms_method nsubj_outperforms_method mark_outperforms_that poss_method_our ccomp_show_outperforms nsubj_show_results amod_results_experimental det_results_The
D09-1038	N06-1033	o	A synchronous 363 binarization method is proposed in -LRB- Zhang et al. 2006 -RRB- whose basic idea is to build a left-heavy binary synchronous tree -LRB- Shapiro and Stephens 1991 -RRB- with a left-to-right shift-reduce algorithm	nn_algorithm_shift-reduce amod_algorithm_left-to-right det_algorithm_a num_Shapiro_1991 conj_and_Shapiro_Stephens appos_tree_Stephens appos_tree_Shapiro amod_tree_synchronous amod_tree_binary amod_tree_left-heavy det_tree_a prep_with_build_algorithm dobj_build_tree aux_build_to xcomp_is_build nsubj_is_idea amod_idea_basic poss_idea_whose rcmod_Zhang_is num_Zhang_2006 dep_Zhang_al. nn_Zhang_et prep_in_proposed_Zhang auxpass_proposed_is nsubjpass_proposed_method nn_method_binarization num_method_363 amod_method_synchronous det_method_A ccomp_``_proposed
D09-1038	N06-1033	n	Although this method is comparatively easy to be implemented it just achieves the same performance as the synchronous binarization method -LRB- Zhang et al. 2006 -RRB- for syntaxbased SMT systems	nn_systems_SMT amod_systems_syntaxbased num_Zhang_2006 dep_Zhang_al. nn_Zhang_et nn_method_binarization amod_method_synchronous det_method_the prep_as_performance_method amod_performance_same det_performance_the prep_for_achieves_systems dep_achieves_Zhang dobj_achieves_performance advmod_achieves_just nsubj_achieves_it advcl_achieves_easy auxpass_implemented_be aux_implemented_to xcomp_easy_implemented advmod_easy_comparatively cop_easy_is nsubj_easy_method mark_easy_Although det_method_this
D09-1038	N06-1033	o	3 Synchronous Binarization Optimization by Cost Reduction As discussed in Section 1 binarizing an SCFG in a fixed -LRB- left-heavy -RRB- way -LRB- Zhang et al. 2006 -RRB- may lead to a large number of competing edges and consequently high risk of making search errors	nn_errors_search dobj_making_errors prepc_of_risk_making amod_risk_high dep_consequently_risk amod_edges_competing prep_of_number_edges amod_number_large det_number_a conj_and_lead_consequently prep_to_lead_number aux_lead_may csubj_lead_binarizing nsubj_lead_Optimization amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et amod_way_left-heavy amod_way_fixed det_way_a prep_in_SCFG_way det_SCFG_an dep_binarizing_Zhang dobj_binarizing_SCFG num_Section_1 prep_in_discussed_Section mark_discussed_As nn_Reduction_Cost advcl_Optimization_discussed prep_by_Optimization_Reduction nn_Optimization_Binarization amod_Optimization_Synchronous num_Optimization_3
D09-1038	N06-1033	o	The time complexity of the CKY-based binarization algorithm is -LRB- n3 -RRB- which is higher than that of the linear binarization such as the synchronous binarization -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et dep_binarization_Zhang amod_binarization_synchronous det_binarization_the prep_such_as_binarization_binarization amod_binarization_linear det_binarization_the prep_of_that_binarization prep_than_higher_that cop_higher_is nsubj_higher_which rcmod_is_higher dep_is_n3 nn_algorithm_binarization amod_algorithm_CKY-based det_algorithm_the dep_complexity_is prep_of_complexity_algorithm nn_complexity_time det_complexity_The ccomp_``_complexity
D09-1038	N06-1033	o	Iterative cost reduction algorithm Input An SCFG Output An equivalent binary SCFG of 1 Function ITERATIVECOSTREDUCTION -LRB- -RRB- 2 0 3 for each 0do 4 -LRB- -RRB- = 0 5 while -LRB- -RRB- does not converge do 6 for each do 7 -LSB- -RSB- -LRB- -RRB- 8 for each -LRB- -RRB- do 9 for each do 10 1 11 -LRB- -RRB- CKYBINARIZATION -LRB- -RRB- 12 -LSB- -RSB- -LRB- -RRB- 13 for each -LRB- -RRB- do 14 for each do 15 + 1 16 return In the iterative cost reduction algorithm we first obtain an initial binary SCFG 0 using the synchronous binarization method proposed in -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et dep_in_Zhang prep_proposed_in vmod_method_proposed nn_method_binarization amod_method_synchronous det_method_the dobj_using_method num_SCFG_0 amod_SCFG_binary amod_SCFG_initial det_SCFG_an xcomp_obtain_using dobj_obtain_SCFG advmod_obtain_first nsubj_obtain_we nn_algorithm_reduction nn_algorithm_cost amod_algorithm_iterative det_algorithm_the rcmod_return_obtain prep_in_return_algorithm number_16_1 dobj_do_15 prep_for_do_each dobj_do_14 dep_CKYBINARIZATION_return conj_+_CKYBINARIZATION_16 dep_CKYBINARIZATION_do amod_CKYBINARIZATION_do prep_for_CKYBINARIZATION_each num_CKYBINARIZATION_13 num_CKYBINARIZATION_12 number_11_1 dobj_do_10 prep_for_9_each dep_9_do number_9_8 prep_for_8_each dep_do_16 dep_do_CKYBINARIZATION dep_do_11 dep_do_do dep_do_9 dobj_do_7 vmod_each_do prep_for_do_each dobj_do_6 dobj_converge_do neg_converge_not aux_converge_does mark_converge_while number_5_0 amod_4_0do det_4_each prep_for_3_4 number_3_0 dep_2_converge appos_2_5 amod_2_= dep_2_3 appos_ITERATIVECOSTREDUCTION_2 nn_ITERATIVECOSTREDUCTION_Function dep_SCFG_ITERATIVECOSTREDUCTION prep_of_SCFG_1 amod_SCFG_binary amod_SCFG_equivalent det_SCFG_An dep_Output_SCFG nn_Output_SCFG det_Output_An dep_Input_Output dep_algorithm_Input nn_algorithm_reduction nn_algorithm_cost amod_algorithm_Iterative
N06-3004	N06-1033	o	We develop this intuition into a technique called synchronous binarization -LRB- Zhang et al. 2006 -RRB- which binarizes a synchronous production or treetranduction rule on both source and target sides simultaneously	nn_sides_target conj_and_source_sides preconj_source_both nn_rule_treetranduction conj_or_production_rule amod_production_synchronous det_production_a advmod_binarizes_simultaneously prep_on_binarizes_sides prep_on_binarizes_source dobj_binarizes_rule dobj_binarizes_production nsubj_binarizes_which amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et rcmod_binarization_binarizes dep_binarization_Zhang amod_binarization_synchronous dep_called_binarization vmod_technique_called det_technique_a det_intuition_this prep_into_develop_technique dobj_develop_intuition nsubj_develop_We ccomp_``_develop
N07-1063	N06-1033	o	-LRB- Zhang et al. 2006 -RRB- binarize grammars into CNF normal form while -LRB- Watanabe et al. 2006 -RRB- allow only Griebach-Normal form grammars	nn_grammars_form amod_grammars_Griebach-Normal advmod_Griebach-Normal_only dobj_allow_grammars nsubj_allow_Watanabe mark_allow_while amod_Watanabe_2006 dep_Watanabe_al. nn_Watanabe_et amod_form_normal nn_form_CNF dep_grammars_allow prep_into_grammars_form amod_grammars_binarize dep_grammars_Zhang amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et
N09-1026	N06-1033	o	Rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly -LRB- Zhang et al. 2006 -RRB- or implicitly binarized using Early-style intermediate symbols -LRB- Zollmann et al. 2006 -RRB-	amod_Zollmann_2006 dep_Zollmann_al. nn_Zollmann_et dep_symbols_Zollmann amod_symbols_intermediate amod_symbols_Early-style dobj_using_symbols advmod_binarized_implicitly nsubj_binarized_Rulesize dep_Zhang_2006 dep_Zhang_al. nn_Zhang_et advmod_binarized_explicitly auxpass_binarized_is nsubjpass_binarized_grammar mark_binarized_whether det_grammar_the ccomp_complexity_binarized nn_complexity_parsing xcomp_affect_using conj_or_affect_binarized dep_affect_Zhang dobj_affect_complexity nsubj_affect_lexicalization nsubj_affect_Rulesize conj_and_Rulesize_lexicalization
N09-1049	N06-1033	o	Extensions to Hiero Several authors describe extensions to Hiero to incorporate additional syntactic information -LRB- Zollmann and Venugopal 2006 Zhang and Gildea 2006 Shen et al. 2008 Marton and Resnik 2008 -RRB- or to combine it with discriminative latent models -LRB- Blunsom et al. 2008 -RRB-	amod_Blunsom_2008 dep_Blunsom_al. nn_Blunsom_et amod_models_latent amod_models_discriminative dep_combine_Blunsom prep_with_combine_models dobj_combine_it aux_combine_to dep_Marton_2008 conj_and_Marton_Resnik num_Shen_2008 nn_Shen_al. nn_Shen_et num_Zhang_2006 conj_and_Zhang_Gildea dep_Zollmann_Resnik dep_Zollmann_Marton conj_and_Zollmann_Shen conj_and_Zollmann_Gildea conj_and_Zollmann_Zhang conj_and_Zollmann_2006 conj_and_Zollmann_Venugopal conj_or_information_combine appos_information_Shen appos_information_Zhang appos_information_2006 appos_information_Venugopal appos_information_Zollmann amod_information_syntactic amod_information_additional dobj_incorporate_combine dobj_incorporate_information aux_incorporate_to prep_to_extensions_Hiero vmod_describe_incorporate dobj_describe_extensions nsubj_describe_Extensions amod_authors_Several dobj_Hiero_authors aux_Hiero_to vmod_Extensions_Hiero
N09-1061	N06-1033	o	Not only is this beneficial in terms of parsing complexity but smaller rules can also improve a translation models ability to generalize to new data -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et amod_data_new prep_to_generalize_data aux_generalize_to vmod_ability_generalize dep_models_ability nn_models_translation det_models_a dobj_improve_models advmod_improve_also aux_improve_can nsubj_improve_rules amod_rules_smaller nn_complexity_parsing prep_of_terms_complexity dep_beneficial_Zhang conj_but_beneficial_improve prep_in_beneficial_terms det_beneficial_this cop_beneficial_is dep_beneficial_only neg_only_Not
P06-1121	N06-1033	o	Its rule binarization is described in -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et dep_in_Zhang prep_described_in auxpass_described_is nsubjpass_described_binarization nn_binarization_rule poss_binarization_Its ccomp_``_described
P06-1123	N06-1033	o	So unlike some other studies -LRB- Zens and Ney 2003 Zhang et al. 2006 -RRB- we used manually annotated alignments instead of automatically generated ones	amod_ones_generated advmod_generated_automatically amod_alignments_annotated advmod_annotated_manually prep_instead_of_used_ones dobj_used_alignments nsubj_used_we prep_unlike_used_studies advmod_used_So num_Zhang_2006 nn_Zhang_al. nn_Zhang_et dep_Zens_Zhang conj_and_Zens_2003 conj_and_Zens_Ney appos_studies_2003 appos_studies_Ney appos_studies_Zens amod_studies_other det_studies_some
P06-1123	N06-1033	o	Decomposing the translational equivalence relations in the training data into smaller units of knowledge can improve a models ability to generalize -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et aux_generalize_to vmod_ability_generalize nn_ability_models det_ability_a dep_improve_Zhang dobj_improve_ability aux_improve_can csubj_improve_Decomposing prep_of_units_knowledge amod_units_smaller nn_data_training det_data_the prep_in_relations_data amod_relations_equivalence amod_relations_translational det_relations_the prep_into_Decomposing_units dobj_Decomposing_relations
P08-1023	N06-1033	o	Compared with their string-based counterparts treebased systems offer some attractive features they are much faster in decoding -LRB- linear time vs. cubic time see -LRB- Huang et al. 2006 -RRB- -RRB- do not require a binary-branching grammar as in string-based models -LRB- Zhang et al. 2006 -RRB- and can have separate grammars for parsing and translation say a context-free grammar for the former and a tree substitution grammar for the latter -LRB- Huang et al. 2006 -RRB-	amod_Huang_2006 dep_Huang_al. nn_Huang_et dep_latter_Huang det_latter_the nn_grammar_substitution nn_grammar_tree det_grammar_a prep_for_former_latter conj_and_former_grammar amod_the_grammar amod_the_former prep_for_grammar_the amod_grammar_context-free det_grammar_a nsubj_say_they conj_and_parsing_translation amod_grammars_separate prep_for_have_translation prep_for_have_parsing dobj_have_grammars aux_have_can nsubj_have_they num_Zhang_2006 dep_Zhang_al. nn_Zhang_et amod_models_string-based pobj_in_models pcomp_as_in amod_grammar_binary-branching det_grammar_a prep_require_as dobj_require_grammar neg_require_not aux_require_do amod_Huang_2006 dep_Huang_al. nn_Huang_et dep_see_Huang amod_time_cubic dep_time_see prep_vs._time_time amod_time_linear appos_much_grammar conj_and_much_say conj_and_much_have dep_much_Zhang dep_much_require dep_much_time prep_in_much_decoding advmod_much_faster cop_much_are nsubj_much_they amod_features_attractive det_features_some parataxis_offer_say parataxis_offer_have parataxis_offer_much dobj_offer_features nsubj_offer_systems pobj_offer_counterparts prepc_compared_with_offer_with amod_systems_treebased amod_counterparts_string-based poss_counterparts_their
P08-1069	N06-1033	o	and Gildea 2007 Zhang et al. 2006 Gildea Satta and Zhang 2006 -RRB-	dep_Zhang_2006 conj_and_Gildea_Zhang conj_and_Gildea_Satta num_Zhang_2006 nn_Zhang_al. nn_Zhang_et conj_Gildea_Zhang conj_Gildea_Satta conj_Gildea_Gildea conj_Gildea_Zhang num_Gildea_2007 cc_``_and
P09-2036	N06-1033	o	Past work has synchronously binarized such rules for efficiency -LRB- Zhang et al. 2006 Huang et al. 2008 -RRB-	num_Huang_2008 nn_Huang_al. nn_Huang_et dep_Zhang_Huang num_Zhang_2006 dep_Zhang_al. nn_Zhang_et prep_for_rules_efficiency amod_rules_such dep_binarized_Zhang dobj_binarized_rules advmod_binarized_synchronously aux_binarized_has nsubj_binarized_work amod_work_Past ccomp_``_binarized
P09-2036	N06-1033	o	model reranking has also been established both for synchronous binarization -LRB- Zhang et al. 2006 -RRB- and for target-only binarization -LRB- Huang 2007 -RRB-	amod_Huang_2007 dep_binarization_Huang amod_binarization_target-only pobj_for_binarization dep_Zhang_2006 dep_Zhang_al. nn_Zhang_et amod_binarization_synchronous conj_and_both_for appos_both_Zhang prep_for_both_binarization dep_established_for dep_established_both auxpass_established_been advmod_established_also aux_established_has nsubjpass_established_reranking nn_reranking_model ccomp_``_established
W06-1606	N06-1033	o	The decoder uses a binarized representation of the rules which is obtained via a syncronous binarization procedure -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et nn_procedure_binarization amod_procedure_syncronous det_procedure_a prep_via_obtained_procedure auxpass_obtained_is nsubjpass_obtained_which rcmod_rules_obtained det_rules_the prep_of_representation_rules amod_representation_binarized det_representation_a dep_uses_Zhang dobj_uses_representation nsubj_uses_decoder det_decoder_The
W07-0403	N06-1033	o	We can use a linear-time algorithm -LRB- Zhang et al. 2006 -RRB- to detect non-ITG movement in our high-confidence links and remove the offending sentence pairs from our training corpus	nn_corpus_training poss_corpus_our nn_pairs_sentence amod_pairs_offending det_pairs_the prep_from_remove_corpus dobj_remove_pairs nn_links_high-confidence poss_links_our prep_in_movement_links amod_movement_non-ITG conj_and_detect_remove dobj_detect_movement aux_detect_to num_Zhang_2006 dep_Zhang_al. nn_Zhang_et amod_algorithm_linear-time det_algorithm_a dep_use_remove dep_use_detect dep_use_Zhang dobj_use_algorithm aux_use_can nsubj_use_We
W07-0405	N06-1033	p	Synchronous binarization -LRB- Zhang et al. 2006 -RRB- solves this problem by simultaneously binarizing both source and target-sides of a synchronous rule making sure of contiguous spans on both sides whenever possible	advmod_possible_whenever det_sides_both prep_on_spans_sides amod_spans_contiguous prep_of_sure_spans advcl_making_possible dobj_making_sure amod_rule_synchronous det_rule_a prep_of_source_rule conj_and_source_target-sides preconj_source_both dobj_binarizing_target-sides dobj_binarizing_source advmod_binarizing_simultaneously det_problem_this xcomp_solves_making prepc_by_solves_binarizing dobj_solves_problem nsubj_solves_binarization amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et appos_binarization_Zhang amod_binarization_Synchronous
W07-0405	N06-1033	o	Decoding with an SCFG -LRB- e.g. translating from Chinese to English using the above grammar -RRB- can be cast as a parsing problem -LRB- see Section 3 for details -RRB- in which case we need to binarize a synchronous rule with more than two nonterminals to achieve polynomial time algorithms -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et nn_algorithms_time amod_algorithms_polynomial dobj_achieve_algorithms aux_achieve_to num_nonterminals_two quantmod_two_than mwe_than_more amod_rule_synchronous det_rule_a vmod_binarize_achieve prep_with_binarize_nonterminals dobj_binarize_rule aux_binarize_to xcomp_need_binarize nsubj_need_we dep_case_Zhang rcmod_case_need num_Section_3 prep_for_see_details dobj_see_Section nn_problem_parsing det_problem_a dep_cast_case prep_in_cast_which dep_cast_see prep_as_cast_problem auxpass_cast_be aux_cast_can csubjpass_cast_Decoding amod_grammar_above det_grammar_the dobj_using_grammar vmod_translating_using prep_to_translating_English prep_from_translating_Chinese dep_e.g._translating dep_SCFG_e.g. det_SCFG_an prep_with_Decoding_SCFG
W07-0405	N06-1033	o	Intuitively speaking the gaps on the target-side will lead to exponential complexity in decoding with integrated language models -LRB- see Section 3 -RRB- as well as synchronous parsing -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et appos_parsing_Zhang amod_parsing_synchronous num_Section_3 dobj_see_Section nn_models_language amod_models_integrated prep_with_decoding_models prep_in_complexity_decoding amod_complexity_exponential conj_and_lead_parsing dep_lead_see prep_to_lead_complexity aux_lead_will nsubj_lead_gaps vmod_lead_speaking det_target-side_the prep_on_gaps_target-side det_gaps_the advmod_speaking_Intuitively
W07-0405	N06-1033	o	This representation being contiguous on both sides successfully reduces the decoding complexity to a low polynomial and significantly improved the search quality -LRB- Zhang et al. 2006 -RRB-	amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et nn_quality_search det_quality_the dobj_improved_quality advmod_improved_significantly nsubj_improved_representation amod_polynomial_low det_polynomial_a amod_complexity_decoding det_complexity_the dep_reduces_Zhang conj_and_reduces_improved prep_to_reduces_polynomial dobj_reduces_complexity advmod_reduces_successfully dep_reduces_contiguous nsubj_reduces_representation det_sides_both prep_on_contiguous_sides aux_contiguous_being det_representation_This
W07-0412	N06-1033	o	On the positive side recent work exploring the automaticbinarizationofsynchronousgrammars -LRB- Zhang et al. 2006 -RRB- has indicated that non-binarizable constructions seem to be relatively rare in practice	prep_in_rare_practice advmod_rare_relatively cop_rare_be aux_rare_to xcomp_seem_rare nsubj_seem_constructions mark_seem_that amod_constructions_non-binarizable ccomp_indicated_seem aux_indicated_has nsubj_indicated_work prep_on_indicated_side amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et det_automaticbinarizationofsynchronousgrammars_the dobj_exploring_automaticbinarizationofsynchronousgrammars dep_work_Zhang vmod_work_exploring amod_work_recent amod_side_positive det_side_the
W08-0403	N06-1033	p	Recent work by -LRB- Zhang et al. 2006 -RRB- shows a practically ef cient approach that binarizes linguistically SCFG rules when possible	advmod_possible_when amod_rules_SCFG advmod_rules_linguistically advcl_binarizes_possible dobj_binarizes_rules nsubj_binarizes_that rcmod_approach_binarizes amod_approach_cient nn_approach_ef det_approach_a advmod_ef_practically dobj_shows_approach nsubj_shows_work amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et prep_by_work_Zhang amod_work_Recent
C08-1008	N06-1041	o	This is the scenario considered by Haghighi and Klein -LRB- 2006 -RRB- for POS tagging how to construct an accurate tagger given a set of tags and a few example words for each of those tags	det_tags_those prep_of_each_tags prep_for_words_each dep_example_words amod_example_few det_example_a conj_and_set_example prep_of_set_tags det_set_a pobj_given_example pobj_given_set prep_tagger_given amod_tagger_accurate det_tagger_an dobj_construct_tagger aux_construct_to advmod_construct_how nn_tagging_POS appos_Klein_2006 conj_and_Haghighi_Klein prep_for_considered_tagging agent_considered_Klein agent_considered_Haghighi dep_scenario_construct vmod_scenario_considered det_scenario_the cop_scenario_is nsubj_scenario_This
C08-1042	N06-1041	o	The mapping typically is made to try to give the most favorable mapping in terms of accuracy typically using a greedy assignment -LRB- Haghighi and Klein 2006 -RRB-	amod_Haghighi_2006 conj_and_Haghighi_Klein dep_assignment_Klein dep_assignment_Haghighi amod_assignment_greedy det_assignment_a dobj_using_assignment advmod_using_typically prep_of_terms_accuracy amod_mapping_favorable det_mapping_the advmod_favorable_most prep_in_give_terms dobj_give_mapping aux_give_to xcomp_try_give aux_try_to xcomp_made_using xcomp_made_try auxpass_made_is advmod_made_typically nsubjpass_made_mapping det_mapping_The ccomp_``_made
D07-1023	N06-1041	o	Haghighi and Klein s -LRB- 2006 -RRB- prototype-driven approach requires just a few prototype examples for each POS tag exploiting these labeled words to constrain the labels of their distributionally similar words when training a generative log-linear model for POS tagging	nn_tagging_POS prep_for_model_tagging amod_model_log-linear amod_model_generative det_model_a dobj_training_model advmod_training_when amod_words_similar advmod_words_distributionally poss_words_their prep_of_labels_words det_labels_the advcl_constrain_training dobj_constrain_labels aux_constrain_to amod_words_labeled det_words_these xcomp_exploiting_constrain dobj_exploiting_words ccomp_,_exploiting nn_tag_POS det_tag_each prep_for_examples_tag nn_examples_prototype amod_examples_few det_examples_a advmod_examples_just dobj_requires_examples nsubj_requires_approach amod_approach_prototype-driven num_approach_2006 rcmod_s_requires nn_s_Klein conj_and_Haghighi_s dep_``_s dep_``_Haghighi
D07-1031	N06-1041	o	In fact we found that it doesnt do so badly at all the bitag HMM estimated by EM achieves a mean 1-to1 tagging accuracy of 40 % which is approximately the same as the 41.3 % reported by -LRB- Haghighi and Klein 2006 -RRB- for their sophisticated MRF model	nn_model_MRF amod_model_sophisticated poss_model_their prep_for_Haghighi_model dep_Haghighi_2006 conj_and_Haghighi_Klein agent_reported_Klein agent_reported_Haghighi vmod_%_reported num_%_41.3 det_%_the prep_as_same_% det_same_the advmod_same_approximately cop_same_is nsubj_same_which rcmod_%_same num_%_40 prep_of_accuracy_% amod_accuracy_tagging dep_1-to1_accuracy amod_1-to1_mean det_1-to1_a dobj_achieves_1-to1 nsubj_achieves_HMM agent_estimated_EM vmod_HMM_estimated nn_HMM_bitag det_HMM_the prep_at_badly_all advmod_badly_so dep_do_achieves advmod_do_badly dep_doesnt_do dep_it_doesnt prep_that_found_it nsubj_found_we prep_in_found_fact
D07-1031	N06-1041	o	Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi-supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word -LRB- Merialdo 1994 Smith and Eisner 2005 Goldwater and Griffiths 2007 -RRB- or a small number of prototypes for each POS -LRB- Haghighi and Klein 2006 -RRB-	amod_Haghighi_2006 conj_and_Haghighi_Klein det_POS_each prep_of_number_prototypes amod_number_small det_number_a dep_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Smith_Klein dep_Smith_Haghighi prep_for_Smith_POS conj_or_Smith_number conj_and_Smith_Griffiths conj_and_Smith_Goldwater conj_and_Smith_2005 conj_and_Smith_Eisner dep_Merialdo_number dep_Merialdo_Goldwater dep_Merialdo_2005 dep_Merialdo_Eisner dep_Merialdo_Smith appos_Merialdo_1994 dep_word_Merialdo det_word_each prep_for_tags_word amod_tags_possible det_tags_the dobj_specifying_tags vmod_lexicon_specifying det_lexicon_a prep_with_provided_lexicon auxpass_provided_is nsubjpass_provided_learner prep_in_provided_which mark_provided_the det_learner_the prepc_in_methods_provided amod_methods_semi-supervised prep_on_focused_methods aux_focused_has nsubj_focused_work amod_models_tagging nn_models_POS dobj_inferring_models nn_data_training amod_data_unsupervised prepc_for_exploiting_inferring dobj_exploiting_data vmod_work_exploiting amod_work_previous amod_work_Most
D07-1031	N06-1041	o	Haghighi and Klein -LRB- 2006 -RRB- propose constraining the mapping from hidden states to POS tags so that at most one hidden state maps to any POS tag	nn_tag_POS det_tag_any nn_maps_state amod_maps_hidden num_maps_one amod_maps_most prep_to_that_tag prep_at_that_maps pcomp_so_that nn_tags_POS amod_states_hidden prep_to_mapping_tags prep_from_mapping_states det_mapping_the prep_constraining_so dobj_constraining_mapping xcomp_propose_constraining nsubj_propose_Klein nsubj_propose_Haghighi appos_Klein_2006 conj_and_Haghighi_Klein
D07-1031	N06-1041	o	It is difficult to compare these with previous work but Haghighi and Klein -LRB- 2006 -RRB- report that in a completely unsupervised setting their MRF model which uses a large set of additional features and a more complex estimation procedure achieves an average 1-to-1 accuracy of 41.3 %	num_%_41.3 prep_of_accuracy_% amod_accuracy_1-to-1 amod_accuracy_average det_accuracy_an dobj_achieves_accuracy nsubj_achieves_model prep_in_achieves_setting mark_achieves_that nn_procedure_estimation amod_procedure_complex det_procedure_a advmod_complex_more amod_features_additional conj_and_set_procedure prep_of_set_features amod_set_large det_set_a dobj_uses_procedure dobj_uses_set nsubj_uses_which rcmod_model_uses nn_model_MRF poss_model_their amod_setting_unsupervised det_setting_a advmod_unsupervised_completely ccomp_report_achieves nn_report_Klein nn_report_Haghighi appos_Klein_2006 conj_and_Haghighi_Klein amod_work_previous prep_with_compare_work dobj_compare_these aux_compare_to conj_but_difficult_report xcomp_difficult_compare cop_difficult_is nsubj_difficult_It
D08-1004	N06-1041	o	Haghighi and Klein -LRB- 2006 -RRB- do the reverse for each class label y they ask the annotators to propose a few prototypical featuresf such thatp -LRB- y | f -RRB- is as high as possible	prep_as_high_possible advmod_high_as cop_high_is nsubj_high_thatp num_f_| dep_f_y dep_thatp_f amod_thatp_such rcmod_featuresf_high amod_featuresf_prototypical amod_featuresf_few det_featuresf_a dobj_propose_featuresf aux_propose_to det_annotators_the xcomp_ask_propose dobj_ask_annotators nsubj_ask_they ccomp_ask_do nn_y_label nn_y_class det_y_each det_reverse_the prep_for_do_y dobj_do_reverse nsubj_do_Klein nsubj_do_Haghighi appos_Klein_2006 conj_and_Haghighi_Klein
D08-1036	N06-1041	o	Finally following Haghighi and Klein -LRB- 2006 -RRB- and Johnson -LRB- 2007 -RRB- we can instead insist that at most one HMM state can be mapped to any part-of-speech tag	amod_tag_part-of-speech det_tag_any prep_to_mapped_tag auxpass_mapped_be aux_mapped_can nsubjpass_mapped_that nn_state_HMM num_state_one advmod_state_most prep_at_that_state ccomp_insist_mapped advmod_insist_instead aux_insist_can nsubj_insist_we prep_following_insist_Johnson prep_following_insist_Klein prep_following_insist_Haghighi advmod_insist_Finally appos_Johnson_2007 appos_Klein_2006 conj_and_Haghighi_Johnson conj_and_Haghighi_Klein
D08-1109	N06-1041	o	In addition a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging -LRB- Smith and Eisner 2005 Haghighi and Klein 2006 -RRB-	amod_Haghighi_2006 conj_and_Haghighi_Klein dep_Smith_Klein dep_Smith_Haghighi amod_Smith_2005 conj_and_Smith_Eisner appos_tagging_Eisner appos_tagging_Smith amod_tagging_semi-supervised amod_tagging_unsupervised conj_and_unsupervised_semi-supervised prep_for_approaches_tagging amod_approaches_discriminative amod_approaches_developing prep_on_focused_approaches aux_focused_have nsubj_focused_number prep_in_focused_addition prep_of_number_approaches det_number_a
D09-1009	N06-1041	o	We use the same feature processing as Haghighi and Klein -LRB- 2006 -RRB- with the addition of context features in a window of3	nn_of3_window det_of3_a prep_in_features_of3 nn_features_context prep_of_addition_features det_addition_the appos_Klein_2006 conj_and_Haghighi_Klein nn_processing_feature amod_processing_same det_processing_the prep_with_use_addition prep_as_use_Klein prep_as_use_Haghighi dobj_use_processing nsubj_use_We
D09-1009	N06-1041	n	The 74.6 % final accuracy on apartments is higher than any result obtained by Haghighi and Klein -LRB- 2006 -RRB- -LRB- the highest is 74.1 % -RRB- higher than the supervised HMM results reported by Grenager et al.	nn_al._et nn_al._Grenager agent_reported_al. vmod_results_reported dep_HMM_results amod_HMM_supervised det_HMM_the prep_than_higher_HMM num_%_74.1 cop_%_is nsubj_%_highest det_highest_the appos_Klein_2006 conj_and_Haghighi_Klein agent_obtained_Klein agent_obtained_Haghighi vmod_result_obtained det_result_any amod_higher_higher dep_higher_% prep_than_higher_result cop_higher_is nsubj_higher_accuracy prep_on_accuracy_apartments amod_accuracy_final amod_accuracy_% det_accuracy_The number_%_74.6
D09-1009	N06-1041	o	For example both Haghighi and Klein -LRB- 2006 -RRB- and Mann and McCallum -LRB- 2008 -RRB- have demonstrated results better than 66.1 % on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate	nsubj_indicate_they rcmod_labels_indicate det_labels_the amod_features_discriminative num_features_33 advmod_discriminative_highly prep_of_list_features det_list_a advmod_list_only dobj_using_list prepc_above_described_using vmod_task_described nn_task_apartments det_task_the prep_on_%_task num_%_66.1 prep_than_better_% conj_and_results_labels amod_results_better dobj_demonstrated_labels dobj_demonstrated_results aux_demonstrated_have nsubj_demonstrated_McCallum nsubj_demonstrated_Mann nsubj_demonstrated_Klein nsubj_demonstrated_Haghighi prep_for_demonstrated_example appos_McCallum_2008 appos_Klein_2006 conj_and_Haghighi_McCallum conj_and_Haghighi_Mann conj_and_Haghighi_Klein preconj_Haghighi_both
D09-1009	N06-1041	o	Consequently we abstract away from specifying a distribution by allowing the user to assign labels to features -LRB- c.f. Haghighi and Klein -LRB- 2006 -RRB- Druck et al.	nn_al._et nn_al._Druck appos_Klein_2006 conj_and_Haghighi_al. conj_and_Haghighi_Klein nn_Haghighi_c.f. dep_features_al. dep_features_Klein dep_features_Haghighi prep_to_assign_features dobj_assign_labels aux_assign_to det_user_the xcomp_allowing_assign dobj_allowing_user det_distribution_a prepc_by_specifying_allowing dobj_specifying_distribution prepc_away_from_abstract_specifying nsubj_abstract_we advmod_abstract_Consequently
D09-1009	N06-1041	o	Similarly prototype-driven learning -LRB- PDL -RRB- -LRB- Haghighi and Klein 2006 -RRB- optimizes the joint marginal likelihood of data labeled with prototype input features for each label	det_label_each prep_for_features_label nn_features_input nn_features_prototype prep_with_labeled_features vmod_data_labeled prep_of_likelihood_data amod_likelihood_marginal amod_likelihood_joint det_likelihood_the dobj_optimizes_likelihood nsubj_optimizes_learning advmod_optimizes_Similarly amod_Haghighi_2006 conj_and_Haghighi_Klein dep_learning_Klein dep_learning_Haghighi appos_learning_PDL amod_learning_prototype-driven
D09-1072	N06-1041	o	One other work that investigates the use of a limited lexicon is -LRB- Haghighi & Klein 2006 -RRB- which develops a prototype-drive approach to propagate the categorical property using distributional similarity features using only three exemplars of each tag they achieve a tagging accuracy of 80.5 % using a somewhat larger dataset but also the full Penn tagset which is much larger	advmod_larger_much cop_larger_is nsubj_larger_which rcmod_tagset_larger nn_tagset_Penn amod_tagset_full det_tagset_the advmod_tagset_also amod_dataset_larger det_dataset_a advmod_larger_somewhat conj_but_using_tagset dobj_using_dataset num_%_80.5 prep_of_accuracy_% amod_accuracy_tagging det_accuracy_a xcomp_achieve_tagset xcomp_achieve_using dobj_achieve_accuracy nsubj_achieve_they vmod_achieve_using det_tag_each prep_of_exemplars_tag num_exemplars_three quantmod_three_only dobj_using_exemplars nn_features_similarity amod_features_distributional dobj_using_features amod_property_categorical det_property_the xcomp_propagate_using dobj_propagate_property aux_propagate_to vmod_approach_propagate amod_approach_prototype-drive det_approach_a dobj_develops_approach nsubj_develops_which dep_Haghighi_2006 conj_and_Haghighi_Klein dep_is_develops dep_is_Klein dep_is_Haghighi amod_lexicon_limited det_lexicon_a prep_of_use_lexicon det_use_the dobj_investigates_use nsubj_investigates_that dep_work_achieve prep_work_is rcmod_work_investigates amod_work_other num_work_One dep_``_work
D09-1134	N06-1041	o	This sparse information however can be propagated across all data based on distributional similarity -LRB- Haghighi and Klein 2006 -RRB-	amod_Haghighi_2006 conj_and_Haghighi_Klein dep_similarity_Klein dep_similarity_Haghighi amod_similarity_distributional det_data_all prep_based_on_propagated_similarity prep_across_propagated_data auxpass_propagated_be aux_propagated_can advmod_propagated_however nsubjpass_propagated_information amod_information_sparse det_information_This
D09-1134	N06-1041	o	Prototype-drive learning -LRB- Haghighi and Klein 2006 -RRB- specifies prior knowledge by providing a few prototypes -LRB- i.e. canonical example words -RRB- for each label	det_label_each nn_words_example amod_words_canonical advmod_words_i.e. prep_for_prototypes_label appos_prototypes_words amod_prototypes_few det_prototypes_a dobj_providing_prototypes amod_knowledge_prior prepc_by_specifies_providing dobj_specifies_knowledge nsubj_specifies_learning dep_Haghighi_2006 conj_and_Haghighi_Klein appos_learning_Klein appos_learning_Haghighi amod_learning_Prototype-drive ccomp_``_specifies
D09-1134	N06-1041	o	In this work we use the prototype lists originally defined by Haghighi and Klein -LRB- 2006 -RRB- -LRB- HK06 -RRB- and subsequently used by Chang et al.	dep_Chang_al. nn_Chang_et prep_by_used_Chang advmod_used_subsequently nsubj_used_we nn_HK06_Klein appos_Klein_2006 conj_and_Haghighi_HK06 agent_defined_HK06 agent_defined_Haghighi advmod_defined_originally vmod_lists_defined nsubj_lists_prototype det_prototype_the conj_and_use_used ccomp_use_lists nsubj_use_we prep_in_use_work det_work_this
E09-1041	N06-1041	o	354 supervised induction techniques that have been successfully developed for English -LRB- e.g. Schutze -LRB- 1995 -RRB- Clark -LRB- 2003 -RRB- -RRB- including the recentlyproposed prototype-driven approach -LRB- Haghighi and Klein 2006 -RRB- and Bayesian approach -LRB- Goldwater and Griffiths 2007 -RRB-	amod_Goldwater_2007 conj_and_Goldwater_Griffiths dep_approach_Griffiths dep_approach_Goldwater amod_approach_Bayesian amod_Haghighi_2006 conj_and_Haghighi_Klein conj_and_approach_approach dep_approach_Klein dep_approach_Haghighi amod_approach_prototype-driven amod_approach_recentlyproposed det_approach_the appos_Clark_2003 dep_Schutze_Clark dep_Schutze_1995 advmod_Schutze_e.g. dep_English_Schutze prep_for_developed_English advmod_developed_successfully auxpass_developed_been aux_developed_have nsubjpass_developed_that prep_including_techniques_approach prep_including_techniques_approach rcmod_techniques_developed nn_techniques_induction amod_techniques_supervised num_techniques_354 dep_``_techniques
E09-1042	N06-1041	o	Haghighi and Klein -LRB- 2006 -RRB- develop a prototype-driven approach which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words	amod_words_similar poss_words_their advmod_similar_distributionally prep_of_labels_words det_labels_the dobj_constrain_labels aux_constrain_to vmod_words_constrain amod_words_labeled det_words_these conj_and_tag_exploits nn_tag_POS det_tag_each dep_examples_words prep_for_examples_exploits prep_for_examples_tag nn_examples_prototype amod_examples_few det_examples_a advmod_examples_just dobj_requires_examples nsubj_requires_which rcmod_approach_requires amod_approach_prototype-driven det_approach_a dobj_develop_approach nsubj_develop_Klein nsubj_develop_Haghighi appos_Klein_2006 conj_and_Haghighi_Klein
I08-1069	N06-1041	o	Still however such techniques often require seeds or prototypes -LRB- c.f. -LRB- Haghighi and Klein 2006 -RRB- -RRB- which are used to prune search spaces or direct learners	amod_learners_direct conj_or_spaces_learners nn_spaces_search dobj_prune_learners dobj_prune_spaces aux_prune_to xcomp_used_prune auxpass_used_are nsubjpass_used_which dep_Haghighi_2006 conj_and_Haghighi_Klein appos_c.f._Klein appos_c.f._Haghighi dep_prototypes_c.f. rcmod_seeds_used conj_or_seeds_prototypes dobj_require_prototypes dobj_require_seeds advmod_require_often nsubj_require_techniques advmod_require_however advmod_require_Still amod_techniques_such
I08-1069	N06-1041	p	Haghighi and Klein -LRB- 2006 -RRB- showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly	advmod_tagging_significantly dobj_tagging_accuracy xcomp_improve_tagging aux_improve_can csubj_improve_adding mark_improve_that amod_data_unlabeled det_data_the prep_to_set_data prep_of_set_prototypes amod_set_small det_set_a dobj_adding_set ccomp_showed_improve nsubj_showed_Klein nsubj_showed_Haghighi appos_Klein_2006 conj_and_Haghighi_Klein
I08-1069	N06-1041	o	For instance the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach -LRB- Haghighi and Klein 2006 -RRB-	amod_Haghighi_2006 conj_and_Haghighi_Klein dep_approach_Klein dep_approach_Haghighi amod_approach_prototype-driven det_approach_the prep_in_examples_approach nn_examples_prototype prep_as_serve_examples aux_serve_can nsubj_serve_words nn_lines_target det_lines_the prep_in_words_lines amod_words_unambiguous amod_words_frequent det_words_the conj_and_frequent_unambiguous nn_approach_clustering nn_approach_word det_approach_the agent_produced_approach vmod_clusters_produced amod_clusters_resulting det_clusters_the dobj_label_clusters aux_label_to xcomp_used_label auxpass_used_be aux_used_can nsubjpass_used_words prep_in_words_IGT amod_words_tagged det_words_the nn_model_HMM det_model_an nn_probabilities_emission prep_in_transition_model conj_and_transition_probabilities amod_transition_initial nn_transition_bias parataxis_used_serve parataxis_used_used prep_to_used_probabilities prep_to_used_transition auxpass_used_be aux_used_can nsubjpass_used_frequency prep_for_used_instance det_data_the prep_from_collected_data vmod_frequency_collected det_frequency_the
I08-2093	N06-1041	o	The recent work of -LRB- Haghighi and Klein 2006 -RRB- and -LRB- Quirk et al. 2005 -RRB- were also sources of inspiration	prep_of_sources_inspiration advmod_sources_also cop_sources_were nsubj_sources_work amod_Quirk_2005 dep_Quirk_al. nn_Quirk_et conj_and_Haghighi_Quirk dep_Haghighi_2006 conj_and_Haghighi_Klein prep_of_work_Quirk prep_of_work_Klein prep_of_work_Haghighi amod_work_recent det_work_The
I08-2093	N06-1041	o	In some recent grammar induction and MT work -LRB- Haghighi and Klein 2006 Quirk et al. 2005 -RRB- it has been shown that even a small amount of knowledge about a language in the form of grammar fragments treelets or prototypes can go a long way in helping with the induction of a grammar from raw text or with alignment of parallel corpora	amod_corpora_parallel prep_of_alignment_corpora amod_text_raw det_grammar_a prep_with_induction_alignment prep_from_induction_text prep_of_induction_grammar conj_or_induction_induction det_induction_the prep_with_helping_induction prep_with_helping_induction prepc_in_way_helping amod_way_long det_way_a dobj_go_way aux_go_can prep_in_go_form nsubj_go_amount mark_go_that conj_or_fragments_prototypes conj_or_fragments_treelets nn_fragments_grammar prep_of_form_prototypes prep_of_form_treelets prep_of_form_fragments det_form_the det_language_a prep_about_amount_language prep_of_amount_knowledge amod_amount_small det_amount_a advmod_amount_even ccomp_shown_go auxpass_shown_been aux_shown_has nsubjpass_shown_it dep_shown_2006 dep_shown_Klein dep_shown_Haghighi prep_in_shown_work prep_in_shown_induction num_Quirk_2005 nn_Quirk_al. nn_Quirk_et dep_Haghighi_Quirk conj_and_Haghighi_2006 conj_and_Haghighi_Klein nn_work_MT conj_and_induction_work nn_induction_grammar amod_induction_recent det_induction_some
N07-1057	N06-1041	o	The order of constituents for instance can be used to inform prototype-driven learning strategies -LRB- Haghighi and Klein 2006 -RRB- which can then be applied to raw corpora	amod_corpora_raw prep_to_applied_corpora auxpass_applied_be advmod_applied_then aux_applied_can nsubjpass_applied_which dep_Haghighi_2006 conj_and_Haghighi_Klein rcmod_strategies_applied appos_strategies_Klein appos_strategies_Haghighi nn_strategies_learning amod_strategies_prototype-driven dobj_inform_strategies aux_inform_to xcomp_used_inform auxpass_used_be aux_used_can prep_for_used_instance nsubjpass_used_order prep_of_order_constituents det_order_The
N07-1057	N06-1041	o	In particular knowing a little about the structure of a language can help in developing annotated corpora and tools since a little knowledge can go a long way in inducing accurate structure and annotations -LRB- Haghighi and Klein 2006 -RRB-	amod_Haghighi_2006 conj_and_Haghighi_Klein conj_and_structure_annotations amod_structure_accurate dobj_inducing_annotations dobj_inducing_structure prepc_in_way_inducing amod_way_long det_way_a dep_go_Klein dep_go_Haghighi dobj_go_way aux_go_can nsubj_go_knowledge mark_go_since amod_knowledge_little det_knowledge_a advcl_,_go conj_and_corpora_tools amod_corpora_annotated dobj_developing_tools dobj_developing_corpora prepc_in_help_developing aux_help_can nsubj_help_little det_language_a prep_of_structure_language det_structure_the prep_about_little_structure det_little_a ccomp_knowing_help advcl_,_knowing pobj_In_particular dep_``_In
N09-1034	N06-1041	o	This has been shown both in supervised settings -LRB- Roth and Yih 2004 Riedel and Clarke 2006 -RRB- and unsupervised settings -LRB- Haghighi and Klein 2006 Chang et al. 2007 -RRB- in which constraints are used to bootstrap the model	det_model_the dobj_bootstrap_model aux_bootstrap_to xcomp_used_bootstrap auxpass_used_are nsubjpass_used_constraints prep_in_used_which num_Chang_2007 nn_Chang_al. nn_Chang_et rcmod_Haghighi_used dep_Haghighi_Chang conj_and_Haghighi_2006 conj_and_Haghighi_Klein amod_settings_unsupervised dep_Riedel_2006 conj_and_Riedel_Clarke dep_Roth_2006 dep_Roth_Klein dep_Roth_Haghighi conj_and_Roth_settings conj_and_Roth_Clarke conj_and_Roth_Riedel conj_and_Roth_2004 conj_and_Roth_Yih amod_settings_supervised dep_shown_settings dep_shown_Riedel dep_shown_2004 dep_shown_Yih dep_shown_Roth prep_in_shown_settings preconj_shown_both auxpass_shown_been aux_shown_has nsubjpass_shown_This
N09-3010	N06-1041	o	Haghighi and Klein -LRB- 2006 -RRB- ask the user to suggest a few prototypes -LRB- examples -RRB- for each class and use those as features	prep_as_use_features dobj_use_those det_class_each prep_for_prototypes_class appos_prototypes_examples amod_prototypes_few det_prototypes_a conj_and_suggest_use dobj_suggest_prototypes aux_suggest_to det_user_the xcomp_ask_use xcomp_ask_suggest dobj_ask_user nsubj_ask_Klein nsubj_ask_Haghighi appos_Klein_2006 conj_and_Haghighi_Klein
N09-3010	N06-1041	o	Supervision for simple features has been explored in the literature -LRB- Raghavan et al. 2006 Druck et al. 2008 Haghighi and Klein 2006 -RRB-	amod_Haghighi_2006 conj_and_Haghighi_Klein num_Druck_2008 nn_Druck_al. nn_Druck_et dep_Raghavan_Klein dep_Raghavan_Haghighi conj_Raghavan_Druck amod_Raghavan_2006 dep_Raghavan_al. nn_Raghavan_et det_literature_the dep_explored_Raghavan prep_in_explored_literature auxpass_explored_been aux_explored_has nsubjpass_explored_Supervision amod_features_simple prep_for_Supervision_features
P07-1035	N06-1041	o	First we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap and then computing tagging accuracy -LRB- Smith and Eisner 2005 Haghighi and Klein 2006 -RRB- .8 Additionally we compute the mutual information of the learned clusters with the gold tags and we compute the cluster F-score -LRB- Ghosh 2003 -RRB-	amod_Ghosh_2003 appos_F-score_Ghosh nn_F-score_cluster det_F-score_the dobj_compute_F-score nsubj_compute_we nn_tags_gold det_tags_the prep_with_clusters_tags amod_clusters_learned det_clusters_the prep_of_information_clusters amod_information_mutual det_information_the dobj_compute_information nsubj_compute_we advmod_.8_Additionally num_.8_2006 appos_Haghighi_.8 conj_and_Haghighi_Klein conj_and_Smith_compute dep_Smith_compute dep_Smith_Klein dep_Smith_Haghighi amod_Smith_2005 conj_and_Smith_Eisner dep_accuracy_compute dep_accuracy_Eisner dep_accuracy_Smith amod_accuracy_tagging dobj_computing_accuracy advmod_computing_then nsubj_computing_greatest conj_and_overlap_computing nsubj_overlap_greatest det_greatest_the dep_has_computing dep_has_overlap nsubj_has_it prep_with_has_which rcmod_tag_has nn_tag_POS det_tag_the amod_classes_learned det_classes_the prep_of_each_classes prep_to_assigning_tag dobj_assigning_each advmod_assigning_greedily prepc_of_approach_assigning amod_approach_standard det_approach_the dobj_use_approach nsubj_use_we advmod_use_First
P07-1035	N06-1041	o	For comparison Haghighi and Klein -LRB- 2006 -RRB- report an unsupervised baseline of 41.3 % and a best result of 80.5 % from using hand-labeled prototypes and distributional similarity	amod_similarity_distributional conj_and_prototypes_similarity amod_prototypes_hand-labeled dobj_using_similarity dobj_using_prototypes num_%_80.5 prepc_from_result_using prep_of_result_% amod_result_best det_result_a num_%_41.3 prep_of_baseline_% amod_baseline_unsupervised det_baseline_an conj_and_report_result dobj_report_baseline nsubj_report_Klein nsubj_report_Haghighi prep_for_report_comparison appos_Klein_2006 conj_and_Haghighi_Klein
P07-1036	N06-1041	o	In many cases improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology -LRB- Cohen and Sarawagi 2004 Collins and Singer 1999 Haghighi and Klein 2006 Thelen and Riloff 2002 -RRB-	dep_Thelen_2002 conj_and_Thelen_Riloff dep_Haghighi_Riloff dep_Haghighi_Thelen conj_and_Haghighi_2006 conj_and_Haghighi_Klein num_Collins_1999 conj_and_Collins_Singer dep_Cohen_2006 dep_Cohen_Klein dep_Cohen_Haghighi conj_and_Cohen_Singer conj_and_Cohen_Collins conj_and_Cohen_2004 conj_and_Cohen_Sarawagi conj_or_dictionaries_ontology prep_from_taken_ontology prep_from_taken_dictionaries vmod_information_taken nn_information_domain det_models_these dep_seeding_Collins dep_seeding_2004 dep_seeding_Sarawagi dep_seeding_Cohen prep_with_seeding_information dobj_seeding_models agent_done_seeding auxpass_done_was nsubjpass_done_improving prep_in_done_cases amod_models_semi-supervised dobj_improving_models amod_cases_many
P07-1036	N06-1041	o	-LRB- Grenager et al. 2005 -RRB- and -LRB- Haghighi and Klein 2006 -RRB- also report results for semi-supervised learning for these domains	det_domains_these prep_for_learning_domains amod_learning_semi-supervised prep_for_results_learning dobj_report_results advmod_report_also nsubj_report_Haghighi nsubj_report_Grenager dep_Haghighi_2006 conj_and_Haghighi_Klein conj_and_Grenager_Klein conj_and_Grenager_Haghighi amod_Grenager_2005 dep_Grenager_al. nn_Grenager_et
P07-1036	N06-1041	o	-LRB- Haghighi and Klein 2006 -RRB- also worked on one of our data sets	nn_sets_data poss_sets_our prep_of_one_sets prep_on_worked_one advmod_worked_also nsubj_worked_Klein nsubj_worked_Haghighi amod_Haghighi_2006 conj_and_Haghighi_Klein
P07-1036	N06-1041	o	-LRB- Haghighi and Klein 2006 -RRB- extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity	nn_similarity_word amod_similarity_contextual prep_with_seeds_similarity det_seeds_the prep_in_given_seeds vmod_information_given det_information_the dobj_propagating_information nn_tasks_labeling amod_tasks_sequential amod_approach_dictionarybased det_approach_the prepc_by_extends_propagating prep_to_extends_tasks dobj_extends_approach nsubj_extends_Klein nsubj_extends_Haghighi amod_Haghighi_2006 conj_and_Haghighi_Klein
P07-1036	N06-1041	o	We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in -LRB- Haghighi and Klein 2006 -RRB-	amod_Haghighi_2006 conj_and_Haghighi_Klein dep_in_Klein dep_in_Haghighi prep_used_in vmod_words_used nn_words_seed prep_of_list_words det_list_the prep_from_imported_list advmod_imported_largely auxpass_imported_were nsubjpass_imported_which rcmod_constraints_imported amod_constraints_unary dobj_include_constraints nsubj_include_We amod_constraints_global det_constraints_some conj_and_implement_include dobj_implement_constraints nsubj_implement_We
P07-1094	N06-1041	o	Haghighi and Klein -LRB- 2006 -RRB- use a small list of labeled prototypes and no dictionary	neg_dictionary_no conj_and_prototypes_dictionary dobj_labeled_dictionary dobj_labeled_prototypes prepc_of_list_labeled amod_list_small det_list_a dobj_use_list nsubj_use_Klein nsubj_use_Haghighi appos_Klein_2006 conj_and_Haghighi_Klein
P08-1085	N06-1041	p	In particular -LRB- Haghighi and Klein 2006 -RRB- presents very strong results using a distributional-similarity module and achieve impressive tagging accuracy while starting with a mere 116 prototypical words	amod_words_prototypical num_words_116 amod_words_mere det_words_a prep_with_starting_words mark_starting_while amod_accuracy_tagging amod_accuracy_impressive dobj_achieve_accuracy nn_module_distributional-similarity det_module_a advcl_using_starting conj_and_using_achieve dobj_using_module vmod_results_achieve vmod_results_using amod_results_strong advmod_strong_very dobj_presents_results nsubj_presents_Klein nsubj_presents_Haghighi prep_in_presents_particular amod_Haghighi_2006 conj_and_Haghighi_Klein
P08-1085	N06-1041	o	We also report state-of-the-art results for Hebrew full mor1Another notable work though within a slightly different framework is the prototype-driven method proposed by -LRB- Haghighi and Klein 2006 -RRB- in which the dictionary is replaced with a very small seed of prototypical examples	amod_examples_prototypical prep_of_seed_examples amod_seed_small det_seed_a advmod_small_very prep_with_replaced_seed auxpass_replaced_is nsubjpass_replaced_dictionary prep_in_replaced_which det_dictionary_the rcmod_Haghighi_replaced dep_Haghighi_2006 conj_and_Haghighi_Klein agent_proposed_Klein agent_proposed_Haghighi vmod_method_proposed amod_method_prototype-driven det_method_the cop_method_is prep_within_method_framework mark_method_though amod_framework_different det_framework_a advmod_different_slightly amod_work_notable nn_work_mor1Another amod_work_full nn_work_Hebrew prep_for_results_work amod_results_state-of-the-art advcl_report_method dobj_report_results advmod_report_also nsubj_report_We
P08-1099	N06-1041	o	We achieve competitive performance in comparison to alternate model families in particular generative models such as MRFs trained with EM -LRB- Haghighi and Klein 2006 -RRB- and HMMs trained with soft constraints -LRB- Chang et al. 2007 -RRB-	amod_Chang_2007 dep_Chang_al. nn_Chang_et amod_constraints_soft prep_with_trained_constraints vmod_HMMs_trained amod_Haghighi_2006 conj_and_Haghighi_Klein conj_and_EM_HMMs dep_EM_Klein dep_EM_Haghighi prep_with_trained_HMMs prep_with_trained_EM vmod_MRFs_trained prep_such_as_models_MRFs amod_models_generative amod_models_particular nn_families_model amod_families_alternate prep_to_comparison_families amod_performance_competitive dep_achieve_Chang prep_in_achieve_models prep_in_achieve_comparison dobj_achieve_performance nsubj_achieve_We ccomp_``_achieve
P08-1099	N06-1041	p	This type of input information -LRB- features + majority label -RRB- is a powerful and flexible model for specifying alternative inputs to a classifier and has been additionally used by Haghighi and Klein -LRB- 2006 -RRB-	appos_Klein_2006 conj_and_Haghighi_Klein agent_used_Klein agent_used_Haghighi advmod_used_additionally auxpass_used_been aux_used_has nsubjpass_used_type det_classifier_a amod_inputs_alternative prep_to_specifying_classifier dobj_specifying_inputs conj_and_model_used prepc_for_model_specifying amod_model_flexible amod_model_powerful det_model_a cop_model_is nsubj_model_type conj_and_powerful_flexible nn_label_majority conj_+_features_label dep_information_label dep_information_features nn_information_input prep_of_type_information det_type_This
P08-1099	N06-1041	o	-LRB- 2005 -RRB- and compare with results reported by HK06 -LRB- Haghighi and Klein 2006 -RRB- and CRR07 -LRB- Chang et al. 2007 -RRB-	amod_Chang_2007 dep_Chang_al. nn_Chang_et amod_Haghighi_2006 conj_and_Haghighi_Klein conj_and_HK06_CRR07 dep_HK06_Klein dep_HK06_Haghighi agent_reported_CRR07 agent_reported_HK06 vmod_results_reported prep_with_compare_results dep_2005_Chang conj_and_2005_compare dep_''_compare dep_''_2005
D08-1016	N06-1054	o	1As do constraint relaxation -LRB- Tromble and Eisner 2006 -RRB- and forest reranking -LRB- Huang 2008 -RRB-	amod_Huang_2008 appos_reranking_Huang nn_reranking_forest dep_Tromble_2006 conj_and_Tromble_Eisner conj_and_relaxation_reranking dep_relaxation_Eisner dep_relaxation_Tromble nn_relaxation_constraint dobj_do_reranking dobj_do_relaxation nsubj_do_1As
P08-1035	N06-1054	o	Semantic features are used for classifying entities into semantic types such as name of person organization or place while syntactic features characterize the kinds of dependency 5It is worth noting that the present approach can be recast into one based on constraint relaxation -LRB- Tromble and Eisner 2006 -RRB-	dep_Tromble_2006 conj_and_Tromble_Eisner dep_relaxation_Eisner dep_relaxation_Tromble nn_relaxation_constraint pobj_recast_relaxation prepc_based_on_recast_on prep_into_recast_one auxpass_recast_be aux_recast_can nsubjpass_recast_approach mark_recast_that amod_approach_present det_approach_the ccomp_noting_recast xcomp_worth_noting cop_worth_is csubj_worth_characterize nn_5It_dependency prep_of_kinds_5It det_kinds_the dobj_characterize_kinds nsubj_characterize_features mark_characterize_while nn_features_syntactic conj_or_person_place conj_or_person_organization prep_of_name_place prep_of_name_organization prep_of_name_person prep_such_as_types_name amod_types_semantic amod_entities_classifying parataxis_used_worth prep_into_used_types prep_for_used_entities auxpass_used_are nsubjpass_used_features amod_features_Semantic
C08-1075	N06-1060	o	This approach will generally take advantage of language-specific -LRB- e.g. in -LRB- Freeman et al. 2006 -RRB- -RRB- and domain-specific knowledge of any external resources -LRB- e.g. database names dictionaries etc. -RRB- and of any information about the entities to process e.g. their type -LRB- person name organization etc. -RRB- or internal structure -LRB- e.g. in -LRB- Prager et al. 2007 -RRB- -RRB-	num_Prager_2007 dep_Prager_al. nn_Prager_et dep_in_Prager dep_in_e.g. dep_structure_in amod_structure_internal dep_organization_etc. amod_name_organization nn_name_person nn_name_type poss_name_their dep_e.g._name det_entities_the prep_about_information_entities det_information_any prep_to_of_process pobj_of_information nn_dictionaries_names dep_database_etc. conj_database_dictionaries nn_database_e.g. dep_resources_database amod_resources_external det_resources_any amod_knowledge_domain-specific amod_Freeman_2006 dep_Freeman_al. nn_Freeman_et dep_e.g._Freeman dep_e.g._in conj_and_language-specific_knowledge dep_language-specific_e.g. prep_of_advantage_knowledge prep_of_advantage_language-specific conj_or_take_structure conj_and_take_e.g. conj_and_take_of prep_of_take_resources dobj_take_advantage advmod_take_generally aux_take_will nsubj_take_approach det_approach_This
W08-1402	N06-1060	o	2 Basic Approaches 2.1 Cross-Lingual Approach Our cross-lingual approach -LRB- called MLEV -RRB- is based on -LRB- Freeman et al. 2006 -RRB- who used a modified Levenshtein string edit-distance algorithm to match Arabic script person names against their corresponding English versions	nn_versions_English amod_versions_corresponding poss_versions_their nn_names_person nn_names_script amod_names_Arabic prep_against_match_versions dobj_match_names aux_match_to amod_algorithm_edit-distance nn_algorithm_string nn_algorithm_Levenshtein amod_algorithm_modified det_algorithm_a vmod_used_match dobj_used_algorithm nsubj_used_who dep_2006_al. rcmod_Freeman_used num_Freeman_2006 nn_Freeman_et prep_on_based_Freeman auxpass_based_is nsubjpass_based_approach dep_called_MLEV dep_approach_called amod_approach_cross-lingual poss_approach_Our nn_approach_Approach nn_approach_Cross-Lingual num_approach_2.1 nn_approach_Approaches amod_approach_Basic num_approach_2 ccomp_``_based
W08-1402	N06-1060	o	For this study the Levenshtein edit-distance score -LRB- where a perfect match scores zero -RRB- is Roman Chinese -LRB- Pinyin -RRB- Alignment Score LEV ashburton ashenbodu | a s h b u r t o n | | a s h e n b o d u | 0.67 MLEV ashburton ashenbodu | a s h b u r t o n | | a s h e n b o d u | 0.72 MALINE asVburton aseCnpotu | a sV b < u r t o | n | a s eC n p o t u | 0.48 3 normalized to a similarity score as in -LRB- Freeman et al. 2006 -RRB- where the score ranges from 0 to 1 with 1 being a perfect match	amod_match_perfect det_match_a cop_match_being nsubj_match_1 prep_to_ranges_1 prep_from_ranges_0 nsubj_ranges_score advmod_ranges_where det_score_the dep_al._2006 nn_al._et prepc_with_Freeman_match rcmod_Freeman_ranges advmod_Freeman_al. pobj_in_Freeman pcomp_as_in nn_score_similarity det_score_a prep_normalized_as prep_to_normalized_score nsubj_normalized_3 advmod_normalized_| nsubj_normalized_t num_3_0.48 nn_|_u nn_t_o nn_t_p nn_t_n nn_t_eC nn_t_s det_t_a ccomp_|_normalized nsubj_|_n num_n_| nn_n_o amod_n_< nn_o_t nn_o_r num_o_u rcmod_b_| nn_b_sV det_b_a dobj_|_b nn_aseCnpotu_asVburton nn_aseCnpotu_MALINE num_aseCnpotu_0.72 nn_aseCnpotu_| nn_aseCnpotu_u nn_aseCnpotu_d dep_aseCnpotu_h num_aseCnpotu_| nn_d_o nn_d_b nn_d_n dep_d_e nn_h_s det_h_a dobj_|_aseCnpotu vmod_n_| nn_n_o nn_n_t nn_n_r nn_n_u nn_n_b nn_n_h nn_n_s det_n_a dobj_|_n nn_ashenbodu_ashburton nn_ashenbodu_MLEV num_ashenbodu_0.67 nn_ashenbodu_| nn_ashenbodu_u nn_ashenbodu_d dep_ashenbodu_h num_ashenbodu_| nn_d_o nn_d_b nn_d_n dep_d_e nn_h_s det_h_a dobj_|_ashenbodu vmod_n_| nn_n_o nn_n_t nn_n_r nn_n_u nn_n_b nn_n_h nn_n_s det_n_a dep_|_| dep_|_| dobj_|_n dep_ashenbodu_| nn_ashenbodu_ashburton nn_ashenbodu_LEV nn_ashenbodu_Score nn_ashenbodu_Alignment amod_ashenbodu_Chinese amod_ashenbodu_Roman cop_ashenbodu_is nsubj_ashenbodu_scores advmod_ashenbodu_where dep_Chinese_Pinyin num_scores_zero nn_scores_match amod_scores_perfect det_scores_a advcl_-LRB-_ashenbodu amod_score_edit-distance nn_score_Levenshtein det_score_the prep_for_score_study det_study_this
W08-1402	N06-1060	o	In Table 1 the MALINE row 3 shows that the English name has a palato-alveolar modification 2 As -LRB- Freeman et al. 2006 -RRB- point out these insights are not easy to come by These rules are based on first author Dr. Andrew Freemans experience with reading and translating Arabic language texts for more than 16 years -LRB- Freeman et al. 2006 p. 474 -RRB-	num_p._474 dep_Freeman_p. amod_Freeman_2006 dep_Freeman_al. nn_Freeman_et num_years_16 quantmod_16_than mwe_than_more nn_texts_language amod_texts_Arabic dobj_translating_texts conj_and_reading_translating prep_for_experience_years prep_with_experience_translating prep_with_experience_reading nn_experience_Freemans nn_experience_Andrew nn_experience_Dr. nn_experience_author amod_experience_first dep_based_Freeman prep_on_based_experience auxpass_based_are nsubjpass_based_rules det_rules_These parataxis_come_based prep_come_by aux_come_to xcomp_easy_come neg_easy_not cop_easy_are nsubj_easy_insights det_insights_these prt_point_out appos_point_Freeman mark_point_As amod_Freeman_2006 dep_Freeman_al. nn_Freeman_et num_modification_2 amod_modification_palato-alveolar det_modification_a ccomp_has_easy advcl_has_point dobj_has_modification nsubj_has_name mark_has_that nn_name_English det_name_the ccomp_shows_has nsubj_shows_row prep_in_shows_Table num_row_3 nn_row_MALINE det_row_the num_Table_1
I08-2119	N07-1015	o	Early work employed a diverse range of features in a linear classifier -LRB- commonly referred to as feature-based approaches -RRB- including lexical features syntactic parse features dependency features and semantic features -LRB- Jiang and Zhai 2007 Kambhatla 2004 Zhou et al. 2005 -RRB-	num_Zhou_2005 nn_Zhou_al. nn_Zhou_et num_Kambhatla_2004 dep_Jiang_Zhou conj_and_Jiang_Kambhatla conj_and_Jiang_2007 conj_and_Jiang_Zhai dep_features_Kambhatla dep_features_2007 dep_features_Zhai dep_features_Jiang amod_features_semantic nn_features_dependency nn_features_parse amod_features_syntactic conj_and_features_features conj_and_features_features conj_and_features_features amod_features_lexical amod_approaches_feature-based prep_as_referred_approaches prep_referred_to advmod_referred_commonly prep_including_classifier_features prep_including_classifier_features prep_including_classifier_features prep_including_classifier_features dep_classifier_referred amod_classifier_linear det_classifier_a prep_of_range_features amod_range_diverse det_range_a prep_in_employed_classifier dobj_employed_range nsubj_employed_work amod_work_Early ccomp_``_employed
I08-2119	N07-1015	o	Feature-based methods -LRB- Jiang and Zhai 2007 Kambhatla 2004 Zhou et al. 2005 -RRB- use pre-defined feature sets to extract features to train classification models	nn_models_classification dobj_train_models aux_train_to vmod_extract_train dobj_extract_features aux_extract_to xcomp_sets_extract nsubj_sets_feature amod_feature_pre-defined ccomp_use_sets nsubj_use_methods num_Zhou_2005 nn_Zhou_al. nn_Zhou_et num_Kambhatla_2004 dep_Jiang_Zhou conj_and_Jiang_Kambhatla conj_and_Jiang_2007 conj_and_Jiang_Zhai appos_methods_Kambhatla appos_methods_2007 appos_methods_Zhai appos_methods_Jiang amod_methods_Feature-based
I08-2119	N07-1015	o	Jiang & Zhai -LRB- 2007 -RRB- gave a systematic examination of the efficacy of unigram bigram and trigram features drawn from different representations surface text constituency parse tree and dependency parse tree	dobj_parse_tree nsubj_parse_dependency nsubj_parse_tree conj_and_tree_dependency rcmod_parse_parse vmod_constituency_parse nn_text_surface nn_text_representations amod_text_different prep_from_drawn_text vmod_features_drawn nn_features_trigram nn_features_bigram nn_features_unigram conj_and_unigram_trigram conj_and_unigram_bigram prep_of_efficacy_features det_efficacy_the prep_of_examination_efficacy amod_examination_systematic det_examination_a dobj_gave_constituency dobj_gave_examination nsubj_gave_Zhai nsubj_gave_Jiang appos_Jiang_2007 conj_and_Jiang_Zhai
P08-2023	N07-1015	o	Jiang and Zhai -LRB- 2007 -RRB- then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence syntactic parse tree and dependency parse tree	dobj_parse_tree nsubj_parse_dependency nsubj_parse_tree nsubj_parse_effectiveness nn_tree_parse amod_tree_syntactic prep_to_corresponding_sequence vmod_subspaces_corresponding nn_subspaces_feature amod_subspaces_different conj_and_effectiveness_dependency conj_and_effectiveness_tree prep_of_effectiveness_subspaces det_effectiveness_the ccomp_evaluated_parse nsubj_evaluated_Jiang prep_of_space_features amod_space_large det_space_a conj_and_explored_evaluated dobj_explored_space advmod_explored_systematically advmod_explored_then nsubj_explored_Zhai nsubj_explored_Jiang appos_Zhai_2007 conj_and_Jiang_Zhai
P09-1114	N07-1015	o	The model presented above is based on our previous work -LRB- Jiang and Zhai 2007c -RRB- which bears the same spirit of some other recent work on multitask learning -LRB- Ando and Zhang 2005 Evgeniou and Pontil 2004 Daume III 2007 -RRB-	amod_III_2007 nn_III_Daume dep_Evgeniou_III conj_and_Evgeniou_2004 conj_and_Evgeniou_Pontil conj_and_Ando_2004 conj_and_Ando_Pontil conj_and_Ando_Evgeniou conj_and_Ando_2005 conj_and_Ando_Zhang dep_learning_Evgeniou dep_learning_2005 dep_learning_Zhang dep_learning_Ando amod_learning_multitask amod_work_recent amod_work_other det_work_some prep_of_spirit_work amod_spirit_same det_spirit_the prep_on_bears_learning dobj_bears_spirit nsubj_bears_which dep_Jiang_2007c conj_and_Jiang_Zhai rcmod_work_bears dep_work_Zhai dep_work_Jiang amod_work_previous poss_work_our prep_on_based_work auxpass_based_is nsubjpass_based_model prep_presented_above vmod_model_presented det_model_The
P09-1114	N07-1015	o	While transfer learning was proposed more than a decade ago -LRB- Thrun 1996 Caruana 1997 -RRB- its application in natural language processing is still a relatively new territory -LRB- Blitzer et al. 2006 Daume III 2007 Jiang and Zhai 2007a Arnold et al. 2008 Dredze and Crammer 2008 -RRB- and its application in relation extraction is still unexplored	advmod_unexplored_still cop_unexplored_is nsubj_unexplored_application nsubj_unexplored_III nsubj_unexplored_Blitzer nn_extraction_relation prep_in_application_extraction poss_application_its num_Arnold_2008 nn_Arnold_al. nn_Arnold_et dep_Jiang_2008 conj_and_Jiang_Crammer conj_and_Jiang_Dredze conj_and_Jiang_Arnold conj_and_Jiang_2007a conj_and_Jiang_Zhai amod_III_2007 nn_III_Daume conj_and_Blitzer_application dep_Blitzer_Crammer dep_Blitzer_Dredze dep_Blitzer_Arnold dep_Blitzer_2007a dep_Blitzer_Zhai dep_Blitzer_Jiang conj_and_Blitzer_III appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et rcmod_territory_unexplored amod_territory_new det_territory_a advmod_territory_still cop_territory_is nsubj_territory_application advcl_territory_proposed advmod_new_relatively nn_processing_language amod_processing_natural prep_in_application_processing poss_application_its dep_Caruana_1997 dep_Thrun_Caruana appos_Thrun_1996 npadvmod_ago_decade det_decade_a dep_than_Thrun advmod_than_ago advmod_than_more prep_proposed_than auxpass_proposed_was nsubjpass_proposed_learning mark_proposed_While nn_learning_transfer
P09-1114	N07-1015	o	We systematically explored the feature space for relation extraction -LRB- Jiang and Zhai 2007b -RRB- Kernel methods allow a large set of features to be used without being explicitly extracted	advmod_extracted_explicitly auxpass_extracted_being prepc_without_used_extracted auxpass_used_be aux_used_to prep_of_set_features amod_set_large det_set_a xcomp_allow_used dobj_allow_set nsubj_allow_methods nn_methods_Kernel appos_Jiang_2007b conj_and_Jiang_Zhai dep_extraction_Zhai dep_extraction_Jiang nn_extraction_relation prep_for_space_extraction nn_space_feature det_space_the dep_explored_allow dobj_explored_space advmod_explored_systematically nsubj_explored_We ccomp_``_explored
P09-1114	N07-1015	o	Following our previous work -LRB- Jiang and Zhai 2007b -RRB- we extract features from a sequence representation and a parse tree representation of each relation instance	nn_instance_relation det_instance_each prep_of_representation_instance nn_representation_tree nn_representation_parse det_representation_a conj_and_representation_representation nn_representation_sequence det_representation_a prep_from_extract_representation prep_from_extract_representation dobj_extract_features nsubj_extract_we prep_following_extract_work appos_Jiang_2007b conj_and_Jiang_Zhai dep_work_Zhai dep_work_Jiang amod_work_previous poss_work_our
W08-0603	N07-1015	o	A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai -LRB- 2007 -RRB- who also used features derived from the Collins parser	nn_parser_Collins det_parser_the prep_from_derived_parser vmod_features_derived dobj_used_features advmod_used_also nsubj_used_who appos_Zhai_2007 rcmod_Jiang_used conj_and_Jiang_Zhai agent_provided_Zhai agent_provided_Jiang advmod_provided_recently auxpass_provided_was nsubjpass_provided_exploration nn_extraction_interaction nn_extraction_proteinprotein prep_for_features_extraction amod_features_such prep_of_set_features det_set_a prep_of_exploration_set amod_exploration_systematic det_exploration_A
N07-4013	N07-1016	o	For a full discussion of previous work please see -LRB- Banko et al. 2007 -RRB- or see -LRB- Yates and Etzioni 2007 -RRB- for work relating to synonym resolution	nn_resolution_synonym prep_to_relating_resolution vmod_work_relating dep_Yates_2007 conj_and_Yates_Etzioni prep_for_see_work dep_see_Etzioni dep_see_Yates amod_Banko_2007 dep_Banko_al. nn_Banko_et conj_or_see_see dep_see_Banko aux_see_please prep_for_see_discussion amod_work_previous prep_of_discussion_work amod_discussion_full det_discussion_a
N07-4013	N07-1016	o	-LRB- Yates and Etzioni 2007 -RRB- 4	num_Yates_4 amod_Yates_2007 conj_and_Yates_Etzioni dep_''_Etzioni dep_''_Yates
P08-1004	N07-1016	o	Following extraction O-CRF applies the RESOLVER algorithm -LRB- Yates and Etzioni 2007 -RRB- to find relation synonyms the various ways in which a relation is expressed in text	prep_in_expressed_text auxpass_expressed_is nsubjpass_expressed_relation prep_in_expressed_which det_relation_a rcmod_ways_expressed amod_ways_various det_ways_the appos_synonyms_ways nn_synonyms_relation dobj_find_synonyms aux_find_to amod_Yates_2007 conj_and_Yates_Etzioni appos_algorithm_Etzioni appos_algorithm_Yates nn_algorithm_RESOLVER det_algorithm_the vmod_applies_find dobj_applies_algorithm nsubj_applies_O-CRF prep_following_applies_extraction
D07-1023	N07-1020	p	In particular we have implemented an unsupervised morphological analyzer that outperforms Goldsmith s -LRB- 2001 -RRB- Linguistica and Creutz and Lagus s -LRB- 2005 -RRB- Morfessor for our English and Bengali datasets and compares favorably to the bestperforming morphological parsers in MorphoChallenge 20053 -LRB- see Dasgupta and Ng -LRB- 2007 -RRB- -RRB-	appos_Ng_2007 conj_and_Dasgupta_Ng dobj_see_Ng dobj_see_Dasgupta num_MorphoChallenge_20053 prep_in_parsers_MorphoChallenge amod_parsers_morphological amod_parsers_bestperforming det_parsers_the prep_to_compares_parsers advmod_compares_favorably nsubj_compares_that nn_datasets_Bengali conj_and_English_datasets poss_English_our nn_Morfessor_s appos_s_2005 nn_s_Lagus conj_and_Linguistica_Morfessor conj_and_Linguistica_Creutz nn_Linguistica_s appos_s_2001 nn_s_Goldsmith conj_and_outperforms_compares prep_for_outperforms_datasets prep_for_outperforms_English dobj_outperforms_Morfessor dobj_outperforms_Creutz dobj_outperforms_Linguistica nsubj_outperforms_that rcmod_analyzer_compares rcmod_analyzer_outperforms amod_analyzer_morphological amod_analyzer_unsupervised det_analyzer_an dep_implemented_see dobj_implemented_analyzer aux_implemented_have nsubj_implemented_we prep_in_implemented_particular
D09-1070	N07-1020	o	Letter successor variety -LRB- LSV -RRB- models -LRB- Hafer and Weiss 1974 Gaussier 1999 Bernhard 2005 Bordag 2005 669 Keshava and Pitler 2005 Hammarstrom 2006 Dasgupta and Ng 2007 Demberg 2007 -RRB- use the hypothesis that there is less certainty when predicting the next character at morpheme boundaries	nn_boundaries_morpheme prep_at_character_boundaries amod_character_next det_character_the dobj_predicting_character advmod_predicting_when amod_certainty_less advcl_is_predicting nsubj_is_certainty expl_is_there mark_is_that det_hypothesis_the ccomp_use_is dobj_use_hypothesis nsubj_use_2007 nsubj_use_Ng nsubj_use_Dasgupta dep_Demberg_2007 dep_Dasgupta_Demberg conj_and_Dasgupta_2007 conj_and_Dasgupta_Ng num_Hammarstrom_2006 parataxis_Keshava_use conj_and_Keshava_Hammarstrom conj_and_Keshava_2005 conj_and_Keshava_Pitler num_Keshava_669 num_Bordag_2005 appos_Bernhard_2005 dep_Gaussier_Hammarstrom dep_Gaussier_2005 dep_Gaussier_Pitler dep_Gaussier_Keshava conj_Gaussier_Bordag conj_Gaussier_Bernhard conj_Gaussier_1999 dep_Hafer_Gaussier conj_and_Hafer_1974 conj_and_Hafer_Weiss dep_models_1974 dep_models_Weiss dep_models_Hafer nn_models_variety appos_variety_LSV nn_variety_successor nn_variety_Letter
E09-1015	N07-1020	o	Relative frequencies of word-forms have been used in previous work to detect incorrect affix attachments in Bengali and English -LRB- Dasgupta and Ng 2007 -RRB-	amod_Dasgupta_2007 conj_and_Dasgupta_Ng dep_Bengali_Ng dep_Bengali_Dasgupta conj_and_Bengali_English prep_in_attachments_English prep_in_attachments_Bengali nn_attachments_affix amod_attachments_incorrect dobj_detect_attachments aux_detect_to amod_work_previous xcomp_used_detect prep_in_used_work auxpass_used_been aux_used_have nsubjpass_used_frequencies prep_of_frequencies_word-forms amod_frequencies_Relative
I08-1003	N07-1020	o	There has been recent work on discovering allomorphic phenomena automatically -LRB- Dasgupta and Ng 2007 Demberg 2007 -RRB-	amod_Demberg_2007 dep_Dasgupta_Demberg conj_and_Dasgupta_2007 conj_and_Dasgupta_Ng dep_automatically_2007 dep_automatically_Ng dep_automatically_Dasgupta amod_phenomena_allomorphic advmod_discovering_automatically dobj_discovering_phenomena prepc_on_work_discovering amod_work_recent cop_work_been aux_work_has expl_work_There
N09-1024	N07-1020	o	Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text and unsupervised morphological segmentation has been extensively studied for a number of languages -LRB- Brent et al. 1995 Goldsmith 2001 Dasgupta and Ng 2007 Creutz and Lagus 2007 -RRB-	appos_Creutz_2007 conj_and_Creutz_Lagus num_Dasgupta_2007 conj_and_Dasgupta_Ng appos_Goldsmith_2001 dep_Brent_Lagus dep_Brent_Creutz dep_Brent_Ng dep_Brent_Dasgupta dep_Brent_Goldsmith dep_Brent_1995 dep_Brent_al. nn_Brent_et prep_of_number_languages det_number_a dep_studied_Brent prep_for_studied_number advmod_studied_extensively auxpass_studied_been aux_studied_has nsubjpass_studied_segmentation amod_segmentation_morphological amod_segmentation_unsupervised amod_text_unlabeled prep_of_quantities_text amod_quantities_large prep_of_availability_quantities det_availability_the det_availability_the conj_and_attractive_studied prep_due_to_attractive_availability cop_attractive_are nsubj_attractive_approaches amod_approaches_Unsupervised
N09-1024	N07-1020	o	Some adopt a pipeline approach -LRB- Schone and Jurafsky 2001 Dasgupta and Ng 2007 Demberg 2007 -RRB- which works by first extracting candidate affixes and stems and then segmenting the words based on the candidates	det_candidates_the det_words_the pobj_segmenting_candidates prepc_based_on_segmenting_on dobj_segmenting_words advmod_segmenting_then nsubj_stems_which nn_affixes_candidate amod_affixes_extracting amod_affixes_first conj_and_works_stems prep_by_works_affixes nsubj_works_which dep_Demberg_2007 rcmod_Dasgupta_stems rcmod_Dasgupta_works conj_and_Dasgupta_Demberg conj_and_Dasgupta_2007 conj_and_Dasgupta_Ng conj_and_Schone_segmenting conj_and_Schone_Demberg conj_and_Schone_2007 conj_and_Schone_Ng conj_and_Schone_Dasgupta conj_and_Schone_2001 conj_and_Schone_Jurafsky dep_approach_segmenting dep_approach_Dasgupta dep_approach_2001 dep_approach_Jurafsky dep_approach_Schone nn_approach_pipeline det_approach_a dobj_adopt_approach nsubj_adopt_Some
W08-2117	N07-1020	n	Allomorphs -LRB- e.g. deni and deny -RRB- are also automatically identified in -LRB- Dasgupta 2007 -RRB- but the general problem of recognizing highly irregular forms is examined more extensively in -LRB- Yarowsky and Wicentowski 2000 -RRB-	dep_Yarowsky_2000 conj_and_Yarowsky_Wicentowski dep_in_Wicentowski dep_in_Yarowsky prep_more_in advmod_more_extensively dobj_examined_more auxpass_examined_is nsubjpass_examined_problem amod_forms_irregular advmod_irregular_highly dobj_recognizing_forms prepc_of_problem_recognizing amod_problem_general det_problem_the conj_but_Dasgupta_examined amod_Dasgupta_2007 prep_in_identified_examined prep_in_identified_Dasgupta advmod_identified_automatically advmod_identified_also auxpass_identified_are nsubjpass_identified_Allomorphs conj_and_deni_deny dep_deni_e.g. dep_Allomorphs_deny dep_Allomorphs_deni
W09-0106	N07-1020	o	A second example of subtle language dependence comes from Dasgupta and Ng -LRB- 2007 -RRB- who present an unsupervised morphological segmentation algorithm meant to be language-independent	cop_language-independent_be aux_language-independent_to xcomp_meant_language-independent nsubj_meant_Ng nsubj_meant_Dasgupta mark_meant_from nn_algorithm_segmentation amod_algorithm_morphological amod_algorithm_unsupervised det_algorithm_an dobj_present_algorithm nsubj_present_who appos_Ng_2007 rcmod_Dasgupta_present conj_and_Dasgupta_Ng advcl_comes_meant nsubj_comes_example nn_dependence_language amod_dependence_subtle prep_of_example_dependence amod_example_second det_example_A
W09-0106	N07-1020	n	However it seems unrealistic to expect a one-size-fits-all approach to be achieve uniformly high performance across varied languages and in fact it doesnt Though the system presented in -LRB- Dasgupta and Ng 2007 -RRB- outperforms the best systems in the 2006 PASCAL challenge for Turkish and Finnish it still does significantly worse on these languages than English -LRB- F-scores of 66.2 and 66.5 compared to 79.4 -RRB-	prep_to_compared_79.4 conj_and_66.2_66.5 vmod_F-scores_compared prep_of_F-scores_66.5 prep_of_F-scores_66.2 dep_English_F-scores det_languages_these prep_than_worse_English prep_on_worse_languages advmod_worse_significantly aux_worse_does advmod_worse_still nsubj_worse_it advcl_worse_outperforms dep_worse_doesnt conj_and_Turkish_Finnish prep_for_challenge_Finnish prep_for_challenge_Turkish nn_challenge_PASCAL num_challenge_2006 det_challenge_the amod_systems_best det_systems_the prep_in_outperforms_challenge dobj_outperforms_systems nsubj_outperforms_system mark_outperforms_Though amod_Dasgupta_2007 conj_and_Dasgupta_Ng prep_in_presented_Ng prep_in_presented_Dasgupta vmod_system_presented det_system_the nsubj_doesnt_it prep_in_doesnt_fact amod_languages_varied prep_across_performance_languages amod_performance_high advmod_high_uniformly dobj_achieve_performance aux_achieve_be aux_achieve_to vmod_approach_achieve amod_approach_one-size-fits-all det_approach_a dobj_expect_approach aux_expect_to conj_and_seems_worse xcomp_seems_expect acomp_seems_unrealistic nsubj_seems_it advmod_seems_However
W09-0805	N07-1020	p	Dasgupta and Ng -LRB- 2007 -RRB- improves over -LRB- Creutz 2003 -RRB- by suggesting a simpler approach	amod_approach_simpler det_approach_a dobj_suggesting_approach dep_Creutz_2003 prepc_by_improves_suggesting prep_over_improves_Creutz nsubj_improves_Ng nsubj_improves_Dasgupta appos_Ng_2007 conj_and_Dasgupta_Ng
E09-1045	N07-1025	o	Thus some research has been focused on deriving different word-sense groupings to overcome the finegrained distinctions of WN -LRB- Hearst and Schutze 1993 -RRB- -LRB- Peters et al. 1998 -RRB- -LRB- Mihalcea and Moldovan 2001 -RRB- -LRB- Agirre and LopezDeLaCalle 2003 -RRB- -LRB- Navigli 2006 -RRB- and -LRB- Snow et al. 2007 -RRB-	amod_Snow_2007 dep_Snow_al. nn_Snow_et dep_Navigli_2006 dep_Agirre_2003 conj_and_Agirre_LopezDeLaCalle dep_Mihalcea_2001 conj_and_Mihalcea_Moldovan dep_Peters_Snow cc_Peters_and appos_Peters_Navigli appos_Peters_LopezDeLaCalle appos_Peters_Agirre appos_Peters_Moldovan appos_Peters_Mihalcea amod_Peters_1998 dep_Peters_al. nn_Peters_et dep_,_Peters amod_Hearst_1993 conj_and_Hearst_Schutze appos_WN_Schutze appos_WN_Hearst prep_of_distinctions_WN amod_distinctions_finegrained det_distinctions_the dobj_overcome_distinctions aux_overcome_to amod_groupings_word-sense amod_groupings_different vmod_deriving_overcome dobj_deriving_groupings prepc_on_focused_deriving auxpass_focused_been aux_focused_has nsubjpass_focused_research advmod_focused_Thus det_research_some advcl_``_focused
E09-1045	N07-1025	o	In this way Wikipedia provides a new very large source of annotated data constantly expanded -LRB- Mihalcea 2007 -RRB-	amod_Mihalcea_2007 dep_expanded_Mihalcea advmod_expanded_constantly amod_data_annotated prep_of_source_data amod_source_large amod_source_new det_source_a advmod_large_very dep_provides_expanded dobj_provides_source nsubj_provides_Wikipedia prep_in_provides_way det_way_this
N09-1004	N07-1025	o	Generally WSD methods use the context of a word for its sense disambiguation and the context information can come from either annotated/unannotated text or other knowledge resources such as WordNet -LRB- Fellbaum 1998 -RRB- SemCor -LRB- SemCor 2008 -RRB- Open Mind Word Expert -LRB- Chklovski and Mihalcea 2002 -RRB- eXtended WordNet -LRB- Moldovan and Rus 2001 -RRB- Wikipedia -LRB- Mihalcea 2007 -RRB- parallel corpora -LRB- Ng Wang and Chan 2003 -RRB-	amod_Ng_2003 conj_and_Ng_Chan conj_and_Ng_Wang appos_corpora_Chan appos_corpora_Wang appos_corpora_Ng amod_corpora_parallel dep_Mihalcea_2007 appos_Wikipedia_Mihalcea dep_Moldovan_2001 conj_and_Moldovan_Rus appos_WordNet_corpora appos_WordNet_Wikipedia appos_WordNet_Rus appos_WordNet_Moldovan amod_WordNet_eXtended dep_Chklovski_2002 conj_and_Chklovski_Mihalcea appos_Expert_WordNet appos_Expert_Mihalcea appos_Expert_Chklovski nn_Expert_Word nn_Expert_Mind dobj_Open_Expert dep_SemCor_2008 appos_SemCor_SemCor amod_Fellbaum_1998 vmod_WordNet_Open conj_WordNet_SemCor dep_WordNet_Fellbaum nn_resources_knowledge amod_resources_other prep_such_as_text_WordNet conj_or_text_resources amod_text_annotated/unannotated preconj_text_either prep_from_come_resources prep_from_come_text aux_come_can nsubj_come_information nn_information_context det_information_the nn_disambiguation_sense poss_disambiguation_its prep_for_word_disambiguation det_word_a prep_of_context_word det_context_the conj_and_use_come dobj_use_context nsubj_use_methods advmod_use_Generally nn_methods_WSD
W08-2231	N07-1025	o	1Mihalcea -LRB- 2007 -RRB- shows that Wikipedia can indeed be used as a sense inventory for sense disambiguation	nn_disambiguation_sense prep_for_inventory_disambiguation nn_inventory_sense det_inventory_a prep_as_used_inventory auxpass_used_be advmod_used_indeed aux_used_can nsubjpass_used_Wikipedia mark_used_that ccomp_shows_used nsubj_shows_1Mihalcea appos_1Mihalcea_2007
W09-2403	N07-1025	o	Mihalcea -LRB- 2007 -RRB- demonstrates that manual mappings can be created for a small number of words with relative ease but for a very large number of words the e ort involved in mapping would approach presented involves no be considerable	cop_considerable_be neg_considerable_no xcomp_involves_considerable dep_presented_involves vmod_approach_presented aux_approach_would nsubj_approach_ort prep_for_approach_number prep_in_involved_mapping vmod_ort_involved dep_ort_e det_ort_the prep_of_number_words amod_number_large det_number_a advmod_large_very amod_ease_relative prep_with_number_ease prep_of_number_words amod_number_small det_number_a conj_but_created_approach prep_for_created_number auxpass_created_be aux_created_can nsubjpass_created_mappings mark_created_that amod_mappings_manual ccomp_demonstrates_approach ccomp_demonstrates_created nsubj_demonstrates_Mihalcea appos_Mihalcea_2007
N09-1014	N07-1026	o	In natural language processing label propagation has been used for document classification -LRB- Zhu 2005 -RRB- word sense disambiguation -LRB- Niu et al. 2005 Alexandrescu and Kirchhoff 2007 -RRB- and sentiment categorization -LRB- Goldberg and Zhu 2006 -RRB-	amod_Goldberg_2006 conj_and_Goldberg_Zhu dep_categorization_Zhu dep_categorization_Goldberg nn_categorization_sentiment dep_Alexandrescu_2007 conj_and_Alexandrescu_Kirchhoff dep_Niu_Kirchhoff dep_Niu_Alexandrescu appos_Niu_2005 dep_Niu_al. nn_Niu_et appos_disambiguation_Niu nn_disambiguation_sense nn_disambiguation_word dep_Zhu_2005 nn_classification_document conj_and_used_categorization conj_and_used_disambiguation dep_used_Zhu prep_for_used_classification auxpass_used_been aux_used_has nsubjpass_used_propagation prep_in_used_processing nn_propagation_label nn_processing_language amod_processing_natural
D09-1020	N07-1039	o	They may rely only on this information -LRB- e.g. -LRB- Turney 2002 Whitelaw et al. 2005 Riloff and Wiebe 2003 -RRB- -RRB- or they may combine it with additional information as well -LRB- e.g. -LRB- Yu and Hatzivassiloglou 2003 Kim and Hovy 2004 Bloom et al. 2007 Wilson et al. 2005a -RRB- -RRB-	appos_Wilson_2005a nn_Wilson_al. nn_Wilson_et num_Bloom_2007 nn_Bloom_al. nn_Bloom_et num_Kim_2004 conj_and_Kim_Hovy dep_Yu_Wilson conj_and_Yu_Bloom conj_and_Yu_Hovy conj_and_Yu_Kim conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_e.g._Bloom dep_e.g._Kim dep_e.g._2003 dep_e.g._Hatzivassiloglou dep_e.g._Yu dep_well_e.g. advmod_well_as amod_information_well amod_information_additional prep_with_combine_information dobj_combine_it aux_combine_may nsubj_combine_they appos_Riloff_2003 conj_and_Riloff_Wiebe conj_or_Whitelaw_combine dep_Whitelaw_Wiebe dep_Whitelaw_Riloff num_Whitelaw_2005 nn_Whitelaw_al. nn_Whitelaw_et dep_Turney_combine dep_Turney_Whitelaw dep_Turney_2002 dep_e.g._Turney dep_information_e.g. det_information_this prep_on_rely_information advmod_rely_only aux_rely_may nsubj_rely_They
N09-1055	N07-1039	p	With the in-depth study of opinion mining researchers committed their efforts for more accurate results the research of sentiment summarization -LRB- Philip et al. 2004 Hu et al. KDD 2004 -RRB- domain transfer problem of the sentiment analysis -LRB- Kanayama et al. 2006 Tan et al. 2007 Blitzer et al. 2007 Tan et al. 2008 Andreevskaia et al. 2008 Tan et al. 2009 -RRB- and finegrained opinion mining -LRB- Hatzivassiloglou et al. 2000 Takamura et al. 2007 Bloom et al. 2007 Wang et al. 2008 Titov et al. 2008 -RRB- are the main branches of the research of opinion mining	nn_mining_opinion prep_of_research_mining det_research_the prep_of_branches_research amod_branches_main det_branches_the cop_branches_are nsubj_branches_mining nsubj_branches_problem nsubj_branches_research num_Titov_2008 nn_Titov_al. nn_Titov_et num_Wang_2008 nn_Wang_al. nn_Wang_et num_Bloom_2007 nn_Bloom_al. nn_Bloom_et num_Takamura_2007 nn_Takamura_al. nn_Takamura_et dep_Hatzivassiloglou_Titov conj_Hatzivassiloglou_Wang conj_Hatzivassiloglou_Bloom conj_Hatzivassiloglou_Takamura appos_Hatzivassiloglou_2000 dep_Hatzivassiloglou_al. nn_Hatzivassiloglou_et appos_mining_Hatzivassiloglou nn_mining_opinion amod_mining_finegrained num_Tan_2009 nn_Tan_al. nn_Tan_et num_Andreevskaia_2008 nn_Andreevskaia_al. nn_Andreevskaia_et num_Tan_2008 nn_Tan_al. nn_Tan_et num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et num_Tan_2007 nn_Tan_al. nn_Tan_et dep_Kanayama_Tan dep_Kanayama_Andreevskaia dep_Kanayama_Tan dep_Kanayama_Blitzer dep_Kanayama_Tan appos_Kanayama_2006 dep_Kanayama_al. nn_Kanayama_et nn_analysis_sentiment det_analysis_the prep_of_problem_analysis nn_problem_transfer nn_problem_domain num_KDD_2004 nn_al._et nn_al._Hu appos_Philip_KDD dep_Philip_al. appos_Philip_2004 dep_Philip_al. nn_Philip_et nn_summarization_sentiment conj_and_research_mining dep_research_Kanayama conj_and_research_problem dep_research_Philip prep_of_research_summarization det_research_the amod_results_accurate amod_results_more prep_for_efforts_results poss_efforts_their parataxis_committed_branches dobj_committed_efforts nsubj_committed_researchers prep_with_committed_study nn_mining_opinion prep_of_study_mining amod_study_in-depth det_study_the
P09-1026	N07-1039	o	A number of works in product review mining -LRB- Hu and Liu 2004 Popescu et al. 2005 Kobayashi et al. 2005 Bloom et al. 2007 -RRB- automatically find features of the reviewed products	amod_products_reviewed det_products_the prep_of_features_products dobj_find_features advmod_find_automatically nsubj_find_number num_Bloom_2007 nn_Bloom_al. nn_Bloom_et num_Kobayashi_2005 nn_Kobayashi_al. nn_Kobayashi_et num_Popescu_2005 nn_Popescu_al. nn_Popescu_et dep_Hu_Bloom conj_and_Hu_Kobayashi conj_and_Hu_Popescu num_Hu_2004 conj_and_Hu_Liu appos_mining_Kobayashi appos_mining_Popescu appos_mining_Liu appos_mining_Hu nn_mining_review nn_mining_product prep_in_works_mining prep_of_number_works det_number_A
C08-1051	N07-1043	o	One of the most relevant work is -LRB- Bollegala et al. 2007 -RRB- which proposed to integrate various patterns in order to measure semantic similarity between words	prep_between_similarity_words amod_similarity_semantic dobj_measure_similarity aux_measure_to dep_measure_order mark_measure_in amod_patterns_various advcl_integrate_measure dobj_integrate_patterns aux_integrate_to xcomp_proposed_integrate nsubj_proposed_which amod_Bollegala_2007 dep_Bollegala_al. nn_Bollegala_et dep_is_proposed dep_is_Bollegala amod_work_relevant det_work_the advmod_relevant_most dep_One_is prep_of_One_work
C08-1051	N07-1043	p	In addition to the classical window-based technique some studies investigated the use of lexico-syntactic patterns -LRB- e.g. X or Y -RRB- to get more accurate co-occurrence statistics -LRB- Chilovski and Pantel 2004 Bollegala et al. 2007 -RRB-	num_Bollegala_2007 nn_Bollegala_al. nn_Bollegala_et dep_Chilovski_Bollegala conj_and_Chilovski_2004 conj_and_Chilovski_Pantel dep_statistics_2004 dep_statistics_Pantel dep_statistics_Chilovski nn_statistics_co-occurrence amod_statistics_accurate amod_statistics_more dobj_get_statistics aux_get_to conj_or_X_Y dep_X_e.g. dep_patterns_Y dep_patterns_X amod_patterns_lexico-syntactic vmod_use_get prep_of_use_patterns det_use_the dobj_investigated_use nsubj_investigated_studies prep_in_addition_to_investigated_technique det_studies_some amod_technique_window-based amod_technique_classical det_technique_the
W08-2113	N07-1043	p	However the most interesting work is certainly proposed by -LRB- Bollegala et al. 2007 -RRB- who extract patterns in two steps	num_steps_two prep_in_extract_steps dobj_extract_patterns nsubj_extract_who rcmod_Bollegala_extract amod_Bollegala_2007 dep_Bollegala_al. nn_Bollegala_et agent_proposed_Bollegala advmod_proposed_certainly auxpass_proposed_is nsubjpass_proposed_work advmod_proposed_However amod_work_interesting det_work_the advmod_interesting_most ccomp_``_proposed
W08-2113	N07-1043	o	On the other hand works done by -LRB- Snow et al. 2005 Snow et al. 2006 Sang and Hofmann 2007 Bollegala et al. 2007 -RRB- have proposed methodologies to automatically acquire these patterns mostly based on supervised learning to leverage manual work	nn_work_manual nn_work_leverage prep_to_learning_work xcomp_supervised_learning prepc_on_based_supervised advmod_based_mostly det_patterns_these vmod_acquire_based dobj_acquire_patterns advmod_acquire_automatically aux_acquire_to vmod_methodologies_acquire dobj_proposed_methodologies aux_proposed_have nsubj_proposed_Snow mark_proposed_by dep_al._2007 nn_al._et nn_al._Bollegala num_Sang_2007 conj_and_Sang_Hofmann num_Snow_2006 nn_Snow_al. nn_Snow_et dep_Snow_al. conj_Snow_Hofmann conj_Snow_Sang conj_Snow_Snow amod_Snow_2005 dep_Snow_al. nn_Snow_et advcl_done_proposed dep_works_done amod_hand_other det_hand_the dep_On_works pobj_On_hand
C04-1015	P02-1040	o	BLEU Automatic evaluation by BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU dep_evaluation_Papineni prep_by_evaluation_score nn_evaluation_Automatic dep_BLEU_evaluation
C04-1016	P02-1040	p	Automated metrics such as BLEU -LRB- Papineni et al. 2002 -RRB- RED -LRB- Akiba et al 2001 -RRB- Weighted N-gram model -LRB- WNM -RRB- -LRB- Babych 2004 -RRB- syntactic relation / semantic vector model -LRB- Rajman and Hartley 2001 -RRB- have been shown to correlate closely with scoring or ranking by different human evaluation parameters	nn_parameters_evaluation amod_parameters_human amod_parameters_different prep_by_scoring_parameters conj_or_scoring_ranking prep_with_correlate_ranking prep_with_correlate_scoring advmod_correlate_closely aux_correlate_to xcomp_shown_correlate auxpass_shown_been aux_shown_have nsubjpass_shown_BLEU mark_shown_as dep_Rajman_2001 conj_and_Rajman_Hartley appos_model_Hartley appos_model_Rajman nn_model_vector amod_model_semantic dep_relation_model amod_relation_syntactic amod_Babych_2004 dep_model_Babych appos_model_WNM nn_model_N-gram amod_model_Weighted amod_Akiba_2001 dep_Akiba_al nn_Akiba_et dep_RED_Akiba amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_BLEU_relation conj_BLEU_model conj_BLEU_RED dep_BLEU_Papineni mwe_as_such advcl_metrics_shown amod_metrics_Automated
C04-1016	P02-1040	p	It was found to produce automated scores which strongly correlate with human judgements about translation fluency -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_fluency_Papineni nn_fluency_translation amod_fluency_about dep_judgements_fluency amod_judgements_human prep_with_correlate_judgements advmod_correlate_strongly nsubj_correlate_which rcmod_scores_correlate amod_scores_automated dobj_produce_scores aux_produce_to xcomp_found_produce auxpass_found_was nsubjpass_found_It
C04-1030	P02-1040	o	This score measures the precision of unigrams bigrams trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_sentences_short advmod_short_too prep_for_penalty_sentences det_penalty_a prep_with_translation_penalty nn_translation_reference det_translation_a conj_and_unigrams_fourgrams conj_and_unigrams_trigrams conj_and_unigrams_bigrams prep_of_precision_fourgrams prep_of_precision_trigrams prep_of_precision_bigrams prep_of_precision_unigrams det_precision_the dep_measures_Papineni prep_with_respect_to_measures_translation dobj_measures_precision nsubj_measures_score det_score_This
C04-1064	P02-1040	o	As an example of it s application N-gram co-occurrence is used for evaluating machine translations -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_translations_machine dobj_evaluating_translations prepc_for_used_evaluating auxpass_used_is nsubjpass_used_application nn_co-occurrence_N-gram appos_application_co-occurrence dep_s_Papineni ccomp_s_used prep_as_s_example prep_of_example_it det_example_an
C04-1114	P02-1040	o	Both calculate the precision of a translation by comparing it to a reference translation and incorporating a length penalty -LRB- Doddington 2001 Papineni et al. 2002 -RRB- -RSB-	num_Papineni_2002 nn_Papineni_al. nn_Papineni_et dep_Doddington_Papineni dep_Doddington_2001 dep_penalty_Doddington nn_penalty_length det_penalty_a dobj_incorporating_penalty nn_translation_reference det_translation_a conj_and_comparing_incorporating prep_to_comparing_translation dobj_comparing_it det_translation_a prep_of_precision_translation det_precision_the prepc_by_calculate_incorporating prepc_by_calculate_comparing dobj_calculate_precision nsubj_calculate_Both ccomp_``_calculate
C04-1168	P02-1040	o	The following four metrics were used speci cally in this study BLEU -LRB- Papineni et al. 2002 -RRB- A weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences	nn_sentences_translation amod_sentences_short dobj_penalizes_sentences nsubj_penalizes_that rcmod_penalty_penalizes nn_penalty_brevity det_penalty_a agent_multiplied_penalty nn_sentences_reference vmod_test_multiplied conj_and_test_sentences prep_between_matches_sentences prep_between_matches_test nsubj_matches_mean det_n-gram_the prep_of_mean_n-gram amod_mean_geometric amod_mean_weighted det_mean_A amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_matches dep_BLEU_Papineni det_study_this dep_speci_BLEU prep_in_speci_study advmod_speci_cally dobj_used_speci auxpass_used_were nsubjpass_used_metrics num_metrics_four amod_metrics_following det_metrics_The
C08-1014	P02-1040	o	Our evaluation metrics are BLEU -LRB- Papineni et al. 2002 -RRB- and NIST which are to perform caseinsensitive matching of n-grams up to n = 4	dep_=_4 amod_n_= pobj_to_n pcomp_up_to prep_of_matching_n-grams amod_matching_caseinsensitive prep_perform_up dobj_perform_matching aux_perform_to xcomp_are_perform nsubj_are_which rcmod_NIST_are amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni cop_BLEU_are nsubj_BLEU_metrics nn_metrics_evaluation poss_metrics_Our
C08-1038	P02-1040	o	5.2 Experimental Results Following -LRB- Langkilde 2002 -RRB- and other work on general-purpose generators BLEU score -LRB- Papineni et al. 2002 -RRB- average NIST simple string accuracy -LRB- SSA -RRB- and percentage of exactly matched sentences are adopted as evaluation metrics	nn_metrics_evaluation prep_as_adopted_metrics auxpass_adopted_are nsubjpass_adopted_percentage nsubjpass_adopted_accuracy nsubjpass_adopted_score nsubjpass_adopted_work nsubjpass_adopted_Langkilde amod_sentences_matched advmod_matched_exactly prep_of_percentage_sentences appos_accuracy_SSA nn_accuracy_string amod_accuracy_simple nn_accuracy_NIST amod_accuracy_average amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_score_Papineni nn_score_BLEU amod_generators_general-purpose prep_on_work_generators amod_work_other conj_and_Langkilde_percentage conj_and_Langkilde_accuracy conj_and_Langkilde_score conj_and_Langkilde_work amod_Langkilde_2002 prepc_following_Results_adopted amod_Results_Experimental num_Results_5.2
C08-1041	P02-1040	o	The translation quality is evaluated by BLEU metric -LRB- Papineni et al. 2002 -RRB- as calculated by mteval-v11b pl with case-insensitive matching of n-grams where n = 4	dobj_=_4 nsubj_=_n advmod_=_where rcmod_n-grams_= prep_of_matching_n-grams amod_matching_case-insensitive prep_with_pl_matching dep_calculated_pl prep_by_calculated_mteval-v11b mark_calculated_as advcl_,_calculated dep_al._2002 nn_al._et advmod_Papineni_al. amod_BLEU_Papineni amod_BLEU_metric agent_evaluated_BLEU auxpass_evaluated_is nsubjpass_evaluated_quality nn_quality_translation det_quality_The dep_``_evaluated
C08-1064	P02-1040	o	Optimization and measurement were done with the NIST implementation of case-insensitive BLEU 4n4r -LRB- Papineni et al. 2002 -RRB- .4 4.1 Baseline We compared translation by pattern matching with a conventional exact model representation using external prefix trees -LRB- Zens and Ney 2007 -RRB-	dep_Zens_2007 conj_and_Zens_Ney appos_trees_Ney appos_trees_Zens nn_trees_prefix amod_trees_external dobj_using_trees vmod_representation_using nn_representation_model amod_representation_exact amod_representation_conventional det_representation_a prep_with_matching_representation nn_matching_pattern prep_by_compared_matching dobj_compared_translation nsubj_compared_We rcmod_Baseline_compared num_Baseline_4.1 number_4.1_.4 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_4n4r_Baseline dep_4n4r_Papineni nn_4n4r_BLEU amod_4n4r_case-insensitive prep_of_implementation_4n4r nn_implementation_NIST det_implementation_the prep_with_done_implementation auxpass_done_were nsubjpass_done_measurement nsubjpass_done_Optimization conj_and_Optimization_measurement
C08-1128	P02-1040	o	We evaluated performance by measuring WER -LRB- word error rate -RRB- PER -LRB- position-independent word error rate -RRB- BLEU -LRB- Papineni et al. 2002 -RRB- and TER -LRB- translation error rate -RRB- -LRB- Snover et al. 2006 -RRB- using multiple references	amod_references_multiple dobj_using_references amod_Snover_2006 dep_Snover_al. nn_Snover_et nn_rate_error nn_rate_translation appos_TER_rate amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni nn_rate_error nn_rate_word amod_rate_position-independent appos_PER_rate nn_rate_error nn_rate_word conj_and_WER_TER conj_and_WER_BLEU appos_WER_PER appos_WER_rate dobj_measuring_TER dobj_measuring_BLEU dobj_measuring_WER xcomp_evaluated_using dep_evaluated_Snover prepc_by_evaluated_measuring dobj_evaluated_performance nsubj_evaluated_We
C08-1138	P02-1040	o	The evaluation metric is casesensitive BLEU-4 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU-4_Papineni amod_BLEU-4_casesensitive cop_BLEU-4_is nsubj_BLEU-4_metric nn_metric_evaluation det_metric_The
C08-1141	P02-1040	p	The state-of-the-art methods for automatic MT evaluation are using an n-gram based metric represented by BLEU -LRB- Papineni et al. 2002 -RRB- and its variants	poss_variants_its amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_variants dep_BLEU_Papineni agent_represented_variants agent_represented_BLEU vmod_metric_represented pobj_based_metric prep_n-gram_based det_n-gram_an dobj_using_n-gram aux_using_are nsubj_using_methods nn_evaluation_MT amod_evaluation_automatic prep_for_methods_evaluation amod_methods_state-of-the-art det_methods_The ccomp_``_using
C08-1144	P02-1040	o	Translation quality is automatically evaluated by the IBM-BLEU metric -LRB- Papineni et al. 2002 -RRB- -LRB- case-sensitive using length of the closest reference translation -RRB- on the following publicly 1148 Ch.-En	num_Ch.-En_1148 advmod_Ch.-En_publicly prep_following_the_Ch.-En nn_translation_reference amod_translation_closest det_translation_the prep_of_length_translation dobj_using_length dep_case-sensitive_using dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_case-sensitive dep_metric_Papineni prep_on_IBM-BLEU_the amod_IBM-BLEU_metric dep_the_IBM-BLEU agent_evaluated_the advmod_evaluated_automatically auxpass_evaluated_is nsubjpass_evaluated_quality nn_quality_Translation
D07-1007	P02-1040	p	In addition to the widely used BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- scores we also evaluate translation quality with the recently proposed Meteor -LRB- Banerjee and Lavie 2005 -RRB- and four edit-distance style metrics Word Error Rate -LRB- WER -RRB- Positionindependent word Error Rate -LRB- PER -RRB- -LRB- Tillmann et al. 1997 -RRB- CDER which allows block reordering -LRB- Leusch et al. 2006 -RRB- and Translation Edit Rate -LRB- TER -RRB- -LRB- Snover et al. 2006 -RRB-	amod_Snover_2006 dep_Snover_al. nn_Snover_et appos_Rate_TER nn_Rate_Edit nn_Rate_Translation amod_Leusch_2006 dep_Leusch_al. nn_Leusch_et dep_reordering_Leusch nn_reordering_block dobj_allows_reordering nsubj_allows_which amod_Tillmann_1997 dep_Tillmann_al. nn_Tillmann_et conj_and_Rate_Rate rcmod_Rate_allows appos_Rate_CDER dep_Rate_Tillmann appos_Rate_PER nn_Rate_Error nn_Rate_word amod_Rate_Positionindependent dep_Rate_Snover appos_Rate_Rate appos_Rate_Rate appos_Rate_WER nn_Rate_Error nn_Rate_Word nn_metrics_style amod_metrics_edit-distance num_metrics_four dep_Banerjee_2005 conj_and_Banerjee_Lavie conj_and_Meteor_metrics dep_Meteor_Lavie dep_Meteor_Banerjee amod_Meteor_proposed det_Meteor_the advmod_proposed_recently nn_quality_translation dep_evaluate_Rate prep_with_evaluate_metrics prep_with_evaluate_Meteor dobj_evaluate_quality advmod_evaluate_also nsubj_evaluate_we prep_in_addition_to_evaluate_scores prep_in_addition_to_evaluate_BLEU dep_scores_Doddington nn_scores_NIST dep_Doddington_2002 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_scores dep_BLEU_Papineni amod_BLEU_used det_BLEU_the advmod_used_widely rcmod_``_evaluate
D07-1008	P02-1040	o	To counteract this we introduce two brevity penalty measures -LRB- BP -RRB- inspired by BLEU -LRB- Papineni et al. 2002 -RRB- which we incorporate into the loss function using a product loss = 1PrecBP BP1 = exp -LRB- 1max -LRB- 1 rc -RRB- -RRB- -LRB- 6 -RRB- BP2 = exp -LRB- 1max -LRB- cr rc -RRB- -RRB- where r is the reference length and c is the candidate length	nn_length_candidate det_length_the cop_length_is nsubj_length_1max dep_length_loss vmod_length_using conj_and_length_c nn_length_reference det_length_the cop_length_is nsubj_length_r advmod_length_where appos_cr_rc dep_1max_c dep_1max_length dep_1max_cr nn_1max_exp amod_1max_= nn_1max_BP2 num_1max_6 num_rc_1 appos_1max_rc nn_1max_exp amod_1max_= nn_1max_BP1 dep_=_1PrecBP dep_loss_1max amod_loss_= det_product_a dobj_using_product nn_function_loss det_function_the prep_into_incorporate_function nsubj_incorporate_we dobj_incorporate_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et rcmod_BLEU_incorporate dep_BLEU_Papineni agent_inspired_BLEU vmod_measures_inspired appos_measures_BP nn_measures_penalty amod_measures_brevity num_measures_two dep_introduce_length dobj_introduce_measures nsubj_introduce_we advcl_introduce_counteract dobj_counteract_this aux_counteract_To
D07-1030	P02-1040	o	In our experiments using BLEU -LRB- Papineni et al. 2002 -RRB- as the metric the interpolated synthetic model achieves a relative improvement of 11.7 % over the best RBMT system that is used to produce the synthetic bilingual corpora	amod_corpora_bilingual amod_corpora_synthetic det_corpora_the dobj_produce_corpora aux_produce_to xcomp_used_produce auxpass_used_is nsubjpass_used_that rcmod_system_used nn_system_RBMT amod_system_best det_system_the num_%_11.7 prep_over_improvement_system prep_of_improvement_% amod_improvement_relative det_improvement_a dobj_achieves_improvement nsubj_achieves_model prep_in_achieves_experiments amod_model_synthetic amod_model_interpolated det_model_the det_metric_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_as_using_metric dobj_using_BLEU vmod_experiments_using poss_experiments_our
D07-1030	P02-1040	p	The translation quality is evaluated using a well-established automatic measure BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni nn_score_BLEU amod_measure_automatic amod_measure_well-established det_measure_a dobj_using_measure dep_evaluated_score xcomp_evaluated_using auxpass_evaluated_is nsubjpass_evaluated_quality nn_quality_translation det_quality_The rcmod_``_evaluated
D07-1036	P02-1040	o	The translation quality is evaluated by BLEU metric -LRB- Papineni et al. 2002 -RRB- as calculated by mteval-v11b pl 6 with case-sensitive matching of n-grams	prep_of_matching_n-grams amod_matching_case-sensitive prep_with_pl_matching num_pl_6 dep_calculated_pl prep_by_calculated_mteval-v11b mark_calculated_as advcl_,_calculated dep_al._2002 nn_al._et advmod_Papineni_al. amod_BLEU_Papineni amod_BLEU_metric agent_evaluated_BLEU auxpass_evaluated_is nsubjpass_evaluated_quality nn_quality_translation det_quality_The dep_``_evaluated
D07-1049	P02-1040	o	All evaluation is in terms of the BLEU score on our test set -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_set_test poss_set_our prep_on_score_set nn_score_BLEU det_score_the prep_of_terms_score dep_is_Papineni prep_in_is_terms nsubj_is_evaluation det_evaluation_All
D07-1054	P02-1040	o	This approach gave an improvement of 2.7 in BLEU -LRB- Papineni et al. 2002 -RRB- score on the IWSLT05 Japanese to English evaluation corpus -LRB- improving the score from 52.4 to 55.1 -RRB-	det_score_the prep_to_improving_55.1 prep_from_improving_52.4 dobj_improving_score nn_corpus_evaluation amod_corpus_English nn_Japanese_IWSLT05 det_Japanese_the dep_score_improving prep_to_score_corpus prep_on_score_Japanese dep_score_Papineni dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_in_improvement_BLEU prep_of_improvement_2.7 det_improvement_an dep_gave_score dobj_gave_improvement nsubj_gave_approach det_approach_This
D07-1055	P02-1040	o	There exists a variety of different metrics e.g. word error rate position-independent word error rate BLEU score -LRB- Papineni et al. 2002 -RRB- NIST score -LRB- Doddington 2002 -RRB- METEOR -LRB- Banerjee and Lavie 2005 -RRB- GTM -LRB- Turian et al. 2003 -RRB-	amod_Turian_2003 dep_Turian_al. nn_Turian_et dep_GTM_Turian dep_Banerjee_2005 conj_and_Banerjee_Lavie appos_METEOR_Lavie appos_METEOR_Banerjee amod_Doddington_2002 dep_score_Doddington nn_score_NIST amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni nn_score_BLEU nn_rate_error nn_rate_word amod_rate_position-independent nn_rate_error nn_rate_word conj_metrics_GTM conj_metrics_METEOR conj_metrics_score conj_metrics_score conj_metrics_rate conj_metrics_rate conj_metrics_e.g. amod_metrics_different prep_of_variety_metrics det_variety_a nsubj_exists_variety expl_exists_There
D07-1055	P02-1040	p	A popular metric for evaluating machine translation quality is the Bleu score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni nn_score_Bleu det_score_the cop_score_is nsubj_score_metric nn_quality_translation nn_quality_machine dobj_evaluating_quality prepc_for_metric_evaluating amod_metric_popular det_metric_A
D07-1077	P02-1040	o	The reordered sentence is then re-tokenized to be consistent with the baseline system which uses a different tokenization scheme that is more friendly to the MT system .3 We use BLEU scores as the performance measure in our evaluation -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et poss_evaluation_our prep_in_measure_evaluation nn_measure_performance det_measure_the nn_scores_BLEU prep_as_use_measure dobj_use_scores nsubj_use_We rcmod_.3_use nn_.3_system nn_.3_MT det_.3_the prep_to_friendly_.3 advmod_friendly_more cop_friendly_is nsubj_friendly_that rcmod_scheme_friendly nn_scheme_tokenization amod_scheme_different det_scheme_a dobj_uses_scheme nsubj_uses_which rcmod_system_uses nn_system_baseline det_system_the prep_with_consistent_system cop_consistent_be aux_consistent_to dep_re-tokenized_Papineni xcomp_re-tokenized_consistent advmod_re-tokenized_then cop_re-tokenized_is nsubj_re-tokenized_sentence amod_sentence_reordered det_sentence_The
D07-1080	P02-1040	o	The algorithm is slightly different from other online training algorithms -LRB- Tillmann and Zhang 2006 Liang et al. 2006 -RRB- in that we keep and update oracle translations which is a set of good translations reachable by a decoder according to a metric i.e. BLEU -LRB- Papineni et al. 2002 -RRB-	num_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_Papineni advmod_BLEU_i.e. dep_metric_BLEU det_metric_a det_decoder_a prep_by_reachable_decoder amod_translations_reachable amod_translations_good pobj_set_metric prepc_according_to_set_to prep_of_set_translations det_set_a cop_set_is nsubj_set_which rcmod_translations_set nn_translations_oracle nsubj_update_we dobj_keep_translations conj_and_keep_update nsubj_keep_we mark_keep_that num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Liang num_Tillmann_2006 conj_and_Tillmann_Zhang appos_algorithms_Zhang appos_algorithms_Tillmann nn_algorithms_training amod_algorithms_online amod_algorithms_other prepc_in_different_update prepc_in_different_keep prep_from_different_algorithms advmod_different_slightly cop_different_is nsubj_different_algorithm det_algorithm_The
D07-1080	P02-1040	o	4.2 Approximated BLEU We used the BLEU score -LRB- Papineni et al. 2002 -RRB- as the loss function computed by BLEU -LRB- E E -RRB- = exp 1N Nsummationdisplay n = 1 log pn -LRB- E E -RRB- BP -LRB- E E -RRB- -LRB- 7 -RRB- where pn -LRB- -RRB- is the n-gram precision of hypothesized translations E = -LCB- et -RCB- Tt = 1 given reference translations E = -LCB- et -RCB- Tt = 1 and BP -LRB- -RRB- 1 is a brevity penalty	nn_penalty_brevity det_penalty_a cop_penalty_is nsubj_penalty_1 nn_penalty_E nn_penalty_translations nn_penalty_reference conj_and_=_BP dobj_=_1 amod_Tt_BP amod_Tt_= dep_=_et dep_E_Tt amod_E_= pobj_given_penalty dep_=_1 amod_Tt_= dep_=_et prep_E_given dep_E_Tt amod_E_= dep_translations_E dobj_hypothesized_translations prepc_of_precision_hypothesized amod_precision_n-gram det_precision_the cop_precision_is nn_precision_pn dep_where_precision appos_E_E dep_BP_where appos_BP_7 dep_BP_E nn_BP_pn appos_E_E dep_pn_E nn_pn_log num_pn_1 dep_=_BP amod_n_= nn_n_Nsummationdisplay nn_n_1N nn_n_exp dobj_=_n dep_E_E dep_BLEU_= dep_BLEU_E dep_by_BLEU dep_computed_by vmod_function_computed nn_function_loss det_function_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dobj_used_score nsubj_used_We prep_as_BLEU_function dep_BLEU_Papineni rcmod_BLEU_used amod_BLEU_Approximated num_BLEU_4.2 dep_``_BLEU
D07-1080	P02-1040	o	The translation quality is evaluated by case-sensitive NIST -LRB- Doddington 2002 -RRB- and BLEU -LRB- Papineni et al. 2002 -RRB- 2	dep_Papineni_2 dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dep_Doddington_2002 conj_and_NIST_BLEU dep_NIST_Doddington amod_NIST_case-sensitive agent_evaluated_BLEU agent_evaluated_NIST auxpass_evaluated_is nsubjpass_evaluated_quality nn_quality_translation det_quality_The ccomp_``_evaluated
D07-1090	P02-1040	o	For each training data size we report the size of the resulting language model the fraction of 5-grams from the test data that is present in the language model and the BLEU score -LRB- Papineni et al. 2002 -RRB- obtained by the machine translation system	nn_system_translation nn_system_machine det_system_the agent_obtained_system amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et vmod_score_obtained dep_score_Papineni nn_score_BLEU det_score_the nn_model_language det_model_the prep_in_present_model cop_present_is nsubj_present_that rcmod_data_present nn_data_test det_data_the prep_from_fraction_data prep_of_fraction_5-grams det_fraction_the nn_model_language amod_model_resulting det_model_the prep_of_size_model det_size_the conj_and_report_score conj_and_report_fraction dobj_report_size nsubj_report_we prep_for_report_size nn_size_data nn_size_training det_size_each
D07-1092	P02-1040	o	Results in terms of word-error-rate -LRB- WER -RRB- and BLEU score -LRB- Papineni et al. 2002 -RRB- are reported in Table 4 for those sentences that contain at least one unknown word	amod_word_unknown num_word_one quantmod_one_at mwe_at_least dobj_contain_word nsubj_contain_that rcmod_sentences_contain det_sentences_those num_Table_4 prep_for_reported_sentences prep_in_reported_Table auxpass_reported_are nsubjpass_reported_Results amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU conj_and_word-error-rate_score appos_word-error-rate_WER prep_of_terms_score prep_of_terms_word-error-rate dep_Results_Papineni prep_in_Results_terms
D08-1010	P02-1040	o	The translation quality is evaluated by BLEU metric -LRB- Papineni et al. 2002 -RRB- as calculated by mtevalv11b.pl with case-insensitive matching of n-grams where n = 4	dobj_=_4 nsubj_=_n advmod_=_where rcmod_n-grams_= prep_of_matching_n-grams amod_matching_case-insensitive prep_with_mtevalv11b.pl_matching prep_by_calculated_mtevalv11b.pl mark_calculated_as advcl_,_calculated dep_al._2002 nn_al._et advmod_Papineni_al. amod_BLEU_Papineni amod_BLEU_metric agent_evaluated_BLEU auxpass_evaluated_is nsubjpass_evaluated_quality nn_quality_translation det_quality_The dep_``_evaluated
D08-1011	P02-1040	o	In the following experiments the NIST BLEU score is used as the evaluation metric -LRB- Papineni et al. 2002 -RRB- which is reported as a percentage in the following sections	amod_sections_following det_sections_the prep_in_percentage_sections det_percentage_a prep_as_reported_percentage auxpass_reported_is nsubjpass_reported_which dep_al._2002 nn_al._et dep_Papineni_al. rcmod_metric_reported appos_metric_Papineni amod_evaluation_metric det_evaluation_the prep_as_used_evaluation auxpass_used_is nsubjpass_used_score prep_in_used_experiments nn_score_BLEU nn_score_NIST det_score_the amod_experiments_following det_experiments_the
D08-1023	P02-1040	o	Model performance is evaluated using the standard BLEU metric -LRB- Papineni et al. 2002 -RRB- which measures average n-gram precision n 4 and we use the NIST definition of the brevity penalty for multiple reference test sets	nn_sets_test nn_sets_reference amod_sets_multiple prep_for_penalty_sets nn_penalty_brevity det_penalty_the prep_of_definition_penalty nn_definition_NIST det_definition_the dobj_use_definition nsubj_use_we num_n_4 conj_and_precision_use conj_and_precision_n nn_precision_n-gram amod_precision_average dobj_measures_use dobj_measures_n dobj_measures_precision nsubj_measures_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et rcmod_metric_measures dep_metric_Papineni nn_metric_BLEU amod_metric_standard det_metric_the dobj_using_metric xcomp_evaluated_using auxpass_evaluated_is nsubjpass_evaluated_performance nn_performance_Model ccomp_``_evaluated
D08-1028	P02-1040	o	There are however other similarity metrics -LRB- e.g. BLEU -LRB- Papineni et al. 2002 -RRB- -RRB- which could be used equally well	advmod_well_equally advmod_used_well auxpass_used_be aux_used_could nsubjpass_used_which num_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dep_BLEU_e.g. rcmod_metrics_used dep_metrics_BLEU nn_metrics_similarity amod_metrics_other nsubj_are_metrics advmod_are_however expl_are_There ccomp_``_are
D08-1033	P02-1040	o	scored with lowercased tokenized NIST BLEU and exact match METEOR -LRB- Papineni et al. 2002 Lavie and Agarwal 2007 -RRB-	dep_Lavie_2007 conj_and_Lavie_Agarwal dep_Papineni_Agarwal dep_Papineni_Lavie appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_METEOR_match amod_METEOR_exact nn_BLEU_NIST amod_BLEU_tokenized dep_lowercased_Papineni conj_and_lowercased_METEOR conj_and_lowercased_BLEU prep_with_scored_METEOR prep_with_scored_BLEU prep_with_scored_lowercased ccomp_``_scored
D08-1051	P02-1040	o	EsEn 63.00.9 59.20.9 6.01.4 EnEs 63.80.9 60.51.0 5.21.6 DeEn 71.60.8 69.00.9 3.61.3 EnDe 75.90.8 73.50.9 3.21.2 FrEn 62.90.9 59.21.0 5.91.6 EnFr 63.40.9 60.00.9 5.41.4 bined in a log-linear fashion by adjusting a weight for each of them by means of the MERT -LRB- Och 2003 -RRB- procedure optimising the BLEU -LRB- Papineni et al. 2002 -RRB- score obtained on the development partition	nn_partition_development det_partition_the prep_on_obtained_partition vmod_score_obtained dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_score dep_BLEU_Papineni det_BLEU_the dobj_optimising_BLEU dep_procedure_Och nn_procedure_MERT dep_Och_2003 det_MERT_the prep_of_each_them prep_for_weight_each det_weight_a prep_by_means_of_adjusting_procedure dobj_adjusting_weight amod_fashion_log-linear det_fashion_a agent_bined_adjusting prep_in_bined_fashion vmod_5.41.4_bined num_5.41.4_60.00.9 number_60.00.9_63.40.9 vmod_EnFr_optimising dep_EnFr_5.41.4 dep_5.91.6_EnFr dep_59.21.0_5.91.6 number_59.21.0_62.90.9 dep_FrEn_59.21.0 dep_3.21.2_FrEn dep_73.50.9_3.21.2 number_73.50.9_75.90.8 dep_EnDe_73.50.9 dep_3.61.3_EnDe number_3.61.3_69.00.9 dep_71.60.8_3.61.3 dep_DeEn_71.60.8 dep_5.21.6_DeEn dep_60.51.0_5.21.6 number_60.51.0_63.80.9 dep_EnEs_60.51.0 dep_6.01.4_EnEs dep_59.20.9_6.01.4 number_59.20.9_63.00.9 dep_EsEn_59.20.9
D08-1051	P02-1040	o	To this purpose different authors -LRB- Papineni et al. 1998 Och and Ney 2002 -RRB- propose the use of the so-called log-linear models where the decision rule is given by the expression y = argmax y Msummationdisplay m = 1 mhm -LRB- x y -RRB- -LRB- 3 -RRB- where hm -LRB- x y -RRB- is a score function representing an important feature for the translation of x into y M is the number of models -LRB- or features -RRB- and m are the weights of the log-linear combination	amod_combination_log-linear det_combination_the prep_of_weights_combination det_weights_the cop_weights_are nsubj_weights_use cc_features_or appos_models_features prep_of_number_models det_number_the cop_number_is nsubj_number_m dep_number_Msummationdisplay nn_number_y nn_number_argmax prep_into_x_y prepc_of_translation_x det_translation_the prep_for_feature_translation amod_feature_important det_feature_an dobj_representing_feature vmod_function_representing nn_function_score det_function_a cop_function_is nsubj_function_hm advmod_function_where appos_x_y dep_hm_x appos_x_y dep_mhm_x num_mhm_1 dep_=_M dep_=_function dep_=_3 dep_=_mhm amod_m_= conj_and_=_m dobj_=_number amod_y_m amod_y_= nn_y_expression det_y_the agent_given_y auxpass_given_is nsubjpass_given_rule advmod_given_where nn_rule_decision det_rule_the rcmod_models_given amod_models_log-linear amod_models_so-called det_models_the prep_of_use_models det_use_the ccomp_propose_weights nsubj_propose_authors prep_to_propose_purpose dep_Och_2002 conj_and_Och_Ney dep_Papineni_Ney dep_Papineni_Och amod_Papineni_1998 dep_Papineni_al. nn_Papineni_et appos_authors_Papineni amod_authors_different det_purpose_this
D08-1060	P02-1040	o	We also report the result of our translation quality in terms of both BLEU -LRB- Papineni et al. 2002 -RRB- and TER -LRB- Snover et al. 2006 -RRB- against four human reference translations	nn_translations_reference amod_translations_human num_translations_four amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_TER_Snover amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_TER dep_BLEU_Papineni preconj_BLEU_both prep_of_terms_TER prep_of_terms_BLEU prep_in_quality_terms nn_quality_translation poss_quality_our prep_against_result_translations prep_of_result_quality det_result_the dobj_report_result advmod_report_also nsubj_report_We
D08-1063	P02-1040	o	7 Experiments To show the effectiveness of cross-language mention propagation information in improving mention detection system performance in Arabic Chinese and Spanish we use three SMT systems with very competitive performance in terms of BLEU11 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_of_terms_BLEU11 amod_performance_competitive advmod_competitive_very nn_systems_SMT num_systems_three dep_use_Papineni prep_in_use_terms prep_with_use_performance dobj_use_systems nsubj_use_we nsubj_use_Experiments conj_and_Arabic_Spanish conj_and_Arabic_Chinese prep_in_performance_Spanish prep_in_performance_Chinese prep_in_performance_Arabic nn_performance_system nn_performance_detection nn_performance_mention dobj_improving_performance nn_information_propagation nn_information_mention amod_information_cross-language prep_of_effectiveness_information det_effectiveness_the prepc_in_show_improving dobj_show_effectiveness aux_show_To vmod_Experiments_show num_Experiments_7
D08-1066	P02-1040	o	Performance is measured by computing the BLEU scores -LRB- Papineni et al. 2002 -RRB- of the systems translations when compared against a single reference translation per sentence	prep_per_translation_sentence nn_translation_reference amod_translation_single det_translation_a prep_against_compared_translation advmod_compared_when nn_translations_systems det_translations_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_of_scores_translations appos_scores_Papineni nn_scores_BLEU det_scores_the dobj_computing_scores advcl_measured_compared agent_measured_computing auxpass_measured_is nsubjpass_measured_Performance
D08-1078	P02-1040	o	6.1 Evaluation of Translation Performance We use the BLEU score -LRB- Papineni et al. 2002 -RRB- to evaluate our systems	poss_systems_our dobj_evaluate_systems aux_evaluate_to amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni nn_score_BLEU det_score_the vmod_use_evaluate dobj_use_score nsubj_use_We dep_use_Evaluation nn_Performance_Translation prep_of_Evaluation_Performance num_Evaluation_6.1
D08-1090	P02-1040	o	All conditions were optimized using BLEU -LRB- Papineni et al. 2002 -RRB- and evaluated using both BLEU and Translation Edit Rate -LRB- TER -RRB- -LRB- Snover et al. 2006 -RRB-	amod_Snover_2006 dep_Snover_al. nn_Snover_et nn_Rate_Edit nn_Rate_Translation dep_BLEU_Snover appos_BLEU_TER conj_and_BLEU_Rate preconj_BLEU_both dobj_using_Rate dobj_using_BLEU xcomp_evaluated_using amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni conj_and_using_evaluated dobj_using_BLEU dep_optimized_evaluated dep_optimized_using auxpass_optimized_were nsubjpass_optimized_conditions det_conditions_All
D09-1006	P02-1040	p	2.1 The BLEU Metric The metric most often used with MERT is BLEU -LRB- Papineni et al. 2002 -RRB- where the score of a candidate c against a reference translation r is BLEU = BP -LRB- len -LRB- c -RRB- len -LRB- r -RRB- -RRB- exp -LRB- 4summationdisplay n = 1 1 4 logpn -RRB- where pn is the n-gram precision2 and BP is a brevity penalty meant to penalize short outputs to discourage improving precision at the expense of recall	prep_of_expense_recall det_expense_the prep_at_improving_expense dobj_improving_precision xcomp_discourage_improving aux_discourage_to amod_outputs_short dobj_penalize_outputs aux_penalize_to xcomp_meant_penalize nsubj_meant_penalty nn_penalty_brevity det_penalty_a ccomp_is_meant nsubj_is_BLEU dep_is_is nsubj_is_score advmod_is_where conj_and_precision2_BP nn_precision2_n-gram det_precision2_the cop_precision2_is nsubj_precision2_pn advmod_precision2_where num_logpn_4 number_4_1 number_4_1 dep_=_logpn amod_n_= nn_n_4summationdisplay dep_exp_n nn_exp_BP appos_len_r appos_len_len appos_len_c dep_BP_len dep_=_exp rcmod_BLEU_BP rcmod_BLEU_precision2 amod_BLEU_= nn_r_translation nn_r_reference det_r_a nn_c_candidate det_c_a prep_against_score_r prep_of_score_c det_score_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et vmod_BLEU_discourage rcmod_BLEU_is dep_BLEU_Papineni cop_BLEU_is nsubj_BLEU_The prep_with_used_MERT advmod_used_often advmod_often_most vmod_metric_used amod_The_metric rcmod_Metric_BLEU dep_BLEU_Metric dep_The_BLEU dep_2.1_The ccomp_``_2.1
D09-1006	P02-1040	p	It is the most widely reported metric in MT research and has been shown to correlate well with human judgment -LRB- Papineni et al. 2002 Coughlin 2003 -RRB-	amod_Coughlin_2003 dep_Papineni_Coughlin appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_judgment_Papineni amod_judgment_human prep_with_correlate_judgment advmod_correlate_well aux_correlate_to xcomp_shown_correlate auxpass_shown_been aux_shown_has nsubjpass_shown_It nn_research_MT conj_and_metric_shown prep_in_metric_research amod_metric_reported det_metric_the cop_metric_is nsubj_metric_It advmod_reported_widely advmod_widely_most
D09-1007	P02-1040	o	We report BLEU scores -LRB- Papineni et al. 2002 -RRB- on untokenized recapitalized output	amod_output_recapitalized dep_untokenized_output amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_on_scores_untokenized appos_scores_Papineni nn_scores_BLEU dobj_report_scores nsubj_report_We
D09-1021	P02-1040	o	For efficiency reasons we report results on sentences of length 30 words or less .10 The syntax-based method gives a BLEU -LRB- Papineni et al. 2002 -RRB- score of 25.04 a 0.46 BLEU point gain over Pharoah	prep_over_gain_Pharoah nn_gain_point nn_gain_BLEU num_gain_0.46 det_gain_a prep_of_score_25.04 dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_score appos_BLEU_Papineni det_BLEU_a dobj_gives_gain dobj_gives_BLEU nsubj_gives_results amod_method_syntax-based det_method_The num_method_.10 quantmod_.10_less conj_or_words_method num_words_30 nn_words_length prep_of_sentences_method prep_of_sentences_words prep_on_results_sentences ccomp_report_gives nsubj_report_we prep_for_report_reasons nn_reasons_efficiency
D09-1024	P02-1040	o	For this we aligned 170,863 pairs of Arabic/English newswire sentences from LDC trained a state-of-the-art syntax-based statistical machine translation system -LRB- Galley et al. 2006 -RRB- on these sentences and alignments and measured BLEU scores -LRB- Papineni et al. 2002 -RRB- on a separate set of 1298 newswire test sentences	nn_sentences_test amod_sentences_newswire num_sentences_1298 prep_of_set_sentences amod_set_separate det_set_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_on_scores_set dep_scores_Papineni nn_scores_BLEU amod_scores_measured conj_and_sentences_alignments det_sentences_these amod_Galley_2006 dep_Galley_al. nn_Galley_et dep_system_Galley nn_system_translation nn_system_machine amod_system_statistical amod_system_syntax-based amod_system_state-of-the-art det_system_a conj_and_trained_scores prep_on_trained_alignments prep_on_trained_sentences dobj_trained_system ccomp_trained_aligned amod_sentences_newswire amod_sentences_Arabic/English prep_of_pairs_sentences num_pairs_170,863 prep_from_aligned_LDC dobj_aligned_pairs nsubj_aligned_we prep_for_aligned_this
D09-1030	P02-1040	o	Instead researchers routinely use automatic metrics like Bleu -LRB- Papineni et al. 2002 -RRB- as the sole evidence of improvement to translation quality	nn_quality_translation prep_to_improvement_quality prep_of_evidence_improvement amod_evidence_sole det_evidence_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni prep_like_metrics_Bleu amod_metrics_automatic prep_as_use_evidence dobj_use_metrics advmod_use_routinely nsubj_use_researchers advmod_use_Instead
D09-1037	P02-1040	o	5.2 Translation In order to test the translation performance of the grammars induced by our model and the GHKM method6 we report BLEU -LRB- Papineni et al. 2002 -RRB- scores on sentences of up to twenty words in length from the MT03 NIST evaluation	nn_evaluation_NIST nn_evaluation_MT03 det_evaluation_the prep_from_length_evaluation prep_in_words_length num_words_twenty dep_twenty_to quantmod_twenty_up prep_of_sentences_words prep_on_scores_sentences dep_scores_Papineni nn_scores_BLEU dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dobj_report_scores nsubj_report_we nn_method6_GHKM det_method6_the conj_and_model_method6 poss_model_our agent_induced_method6 agent_induced_model vmod_grammars_induced det_grammars_the prep_of_performance_grammars nn_performance_translation det_performance_the parataxis_test_report dobj_test_performance aux_test_to dep_test_order mark_test_In dep_Translation_test num_Translation_5.2 dep_``_Translation
D09-1039	P02-1040	o	Similarly to MERT Tillmann and Zhang estimate the parameters of a weight vector on a linear combination of -LRB- binary -RRB- features using a global objective function correlated with BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_correlated_Papineni prep_with_correlated_BLEU amod_function_objective amod_function_global det_function_a dobj_using_function vmod_features_using dep_features_binary prep_of_combination_features amod_combination_linear det_combination_a nn_vector_weight det_vector_a prep_of_parameters_vector det_parameters_the dep_estimate_correlated prep_on_estimate_combination dobj_estimate_parameters prep_to_estimate_Zhang prep_to_estimate_Tillmann prep_to_estimate_MERT advmod_estimate_Similarly conj_and_MERT_Zhang conj_and_MERT_Tillmann
D09-1042	P02-1040	o	Following the evaluation methodology of Wong and Mooney -LRB- 2007 -RRB- we performed 4 runs of the standard 10-fold cross validation and report the averaged performance in this section using the standard automatic evaluation metric BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- 2	dep_2_Doddington dep_Doddington_2002 num_NIST_2 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni amod_BLEU_metric nn_BLEU_evaluation amod_BLEU_automatic amod_BLEU_standard det_BLEU_the dobj_using_NIST dobj_using_BLEU det_section_this prep_in_performance_section amod_performance_averaged det_performance_the xcomp_report_using dobj_report_performance nsubj_report_we nn_validation_cross amod_validation_10-fold amod_validation_standard det_validation_the prep_of_runs_validation num_runs_4 conj_and_performed_report dobj_performed_runs nsubj_performed_we prep_following_performed_methodology appos_Mooney_2007 conj_and_Wong_Mooney prep_of_methodology_Mooney prep_of_methodology_Wong nn_methodology_evaluation det_methodology_the
D09-1043	P02-1040	p	The full model yields a stateof-the-art BLEU -LRB- Papineni et al. 2002 -RRB- score of 0.8506 on Section 23 of the CCGbank which is to our knowledge the best score reported to date 410 using a reversible corpus-engineered grammar	amod_grammar_corpus-engineered amod_grammar_reversible det_grammar_a dobj_using_grammar num_date_410 xcomp_reported_using prep_to_reported_date nsubj_reported_score amod_score_best det_score_the poss_knowledge_our ccomp_is_reported prep_to_is_knowledge nsubj_is_which rcmod_CCGbank_is det_CCGbank_the prep_of_Section_CCGbank num_Section_23 prep_on_score_Section prep_of_score_0.8506 dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_score appos_BLEU_Papineni amod_BLEU_stateof-the-art det_BLEU_a dobj_yields_BLEU nsubj_yields_model amod_model_full det_model_The ccomp_``_yields
D09-1050	P02-1040	o	using the BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric det_BLEU_the dobj_using_BLEU ccomp_``_using
D09-1073	P02-1040	o	Besides the the case-sensitive BLEU-4 -LRB- Papineni et al. 2002 -RRB- used in the two experiments we design another evaluation metrics Reordering Accuracy -LRB- RAcc -RRB- for forced decoding evaluation	amod_evaluation_decoding amod_evaluation_forced appos_Accuracy_RAcc prep_for_Reordering_evaluation dobj_Reordering_Accuracy vmod_metrics_Reordering nn_metrics_evaluation det_metrics_another dobj_design_metrics nsubj_design_we prep_besides_design_BLEU-4 num_experiments_two det_experiments_the prep_in_used_experiments amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et vmod_BLEU-4_used dep_BLEU-4_Papineni amod_BLEU-4_case-sensitive det_BLEU-4_the det_BLEU-4_the
D09-1073	P02-1040	o	Similar to BLEU score we also use the similar Brevity Penalty BP -LRB- Papineni et al. 2002 -RRB- to penalize the short translations in computing RAcc	amod_RAcc_computing prep_in_translations_RAcc amod_translations_short det_translations_the dobj_penalize_translations aux_penalize_to amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BP_Papineni nn_BP_Penalty nn_BP_Brevity amod_BP_similar det_BP_the vmod_use_penalize dobj_use_BP advmod_use_also nsubj_use_we advmod_use_Similar nn_score_BLEU prep_to_Similar_score
D09-1075	P02-1040	o	For practical reasons the maximum size of a token was set at three for Chinese andfour forKorean .2 Minimum error rate training -LRB- Och 2003 -RRB- was run on each system afterwardsand BLEU score -LRB- Papineni et al. 2002 -RRB- was calculated on the test sets	nn_sets_test det_sets_the prep_on_calculated_sets auxpass_calculated_was csubjpass_calculated_set amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU amod_score_afterwardsand nn_score_system det_score_each prep_on_run_score auxpass_run_was nsubjpass_run_Chinese mark_run_for dep_Och_2003 appos_training_Och nn_training_rate nn_training_error nn_training_Minimum nn_training_.2 nn_training_forKorean amod_training_andfour conj_Chinese_training appos_three_Papineni rcmod_three_run prep_at_set_three auxpass_set_was nsubjpass_set_size prep_for_set_reasons det_token_a prep_of_size_token nn_size_maximum det_size_the amod_reasons_practical
D09-1105	P02-1040	o	Demonstrating the inadequacy of such approaches Al-Onaizan and Papineni -LRB- 2006 -RRB- showed that even given the words in the reference translation and their alignment to the source words a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score -LRB- Papineni et al. 2002 -RRB- of at best 69 % and that only when restricted to keep most words very close to their source positions	nn_positions_source poss_positions_their prep_to_close_positions advmod_close_very nsubj_close_words advmod_words_most xcomp_keep_close aux_keep_to xcomp_restricted_keep advmod_restricted_when advmod_when_only pcomp_that_restricted num_%_69 amod_%_best conj_and_at_that pobj_at_% pcomp_of_that pcomp_of_at prep_Papineni_of dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_a dep_achieve_Papineni dobj_achieve_score aux_achieve_could nsubj_achieve_decoder nn_order_target-language amod_order_correct det_order_the prep_into_rearranging_order dobj_rearranging_them advmod_rearranging_merely prepc_with_charged_rearranging vmod_sort_charged det_sort_this prep_of_decoder_sort det_decoder_a nn_words_source det_words_the rcmod_alignment_achieve prep_to_alignment_words poss_alignment_their nn_translation_reference det_translation_the prep_in_words_translation det_words_the conj_and_given_alignment dobj_given_words advmod_given_even dep_that_alignment dep_that_given prep_showed_that nsubj_showed_Papineni nsubj_showed_Al-Onaizan vmod_showed_Demonstrating appos_Papineni_2006 conj_and_Al-Onaizan_Papineni amod_approaches_such prep_of_inadequacy_approaches det_inadequacy_the dobj_Demonstrating_inadequacy
D09-1106	P02-1040	o	We evaluated the translation quality using case-insensitive BLEU metric -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_metric_BLEU amod_metric_case-insensitive dobj_using_metric vmod_quality_using nn_quality_translation det_quality_the dep_evaluated_Papineni dobj_evaluated_quality nsubj_evaluated_We ccomp_``_evaluated
D09-1123	P02-1040	o	We show that our DDTM system provides significant improvements in BLEU -LRB- Papineni et al. 2002 -RRB- and TER -LRB- Snover et al. 2006 -RRB- scores over the already extremely competitive DTM2 system	nn_system_DTM2 amod_system_competitive det_system_the advmod_competitive_extremely advmod_competitive_already prep_over_scores_system dep_scores_Snover dep_Snover_2006 dep_Snover_al. nn_Snover_et amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_TER dep_BLEU_Papineni prep_in_improvements_TER prep_in_improvements_BLEU amod_improvements_significant dobj_provides_improvements nsubj_provides_system mark_provides_that nn_system_DDTM poss_system_our dep_show_scores ccomp_show_provides nsubj_show_We
D09-1136	P02-1040	o	The model weights of the transducer are tuned based on the development set using a grid-based line search and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dobj_using_BLEU-4 vmod_reference6_using amod_reference6_Chinese amod_reference6_single det_reference6_a dep_evaluated_Papineni prep_based_on_evaluated_reference6 auxpass_evaluated_are nsubjpass_evaluated_results nn_results_translation det_results_the nn_search_line amod_search_grid-based det_search_a dobj_using_search xcomp_set_using vmod_development_set det_development_the pobj_on_development pcomp_based_on conj_and_tuned_evaluated prep_tuned_based auxpass_tuned_are nsubjpass_tuned_weights det_transducer_the prep_of_weights_transducer nn_weights_model det_weights_The ccomp_``_evaluated ccomp_``_tuned
D09-1141	P02-1040	o	We set all weights by optimizing Bleu -LRB- Papineni et al. 2002 -RRB- using minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- on a separate development set of 2,000 sentences -LRB- Indonesian or Spanish -RRB- and we used them in a beam search decoder -LRB- Koehn et al. 2007 -RRB- to translate 2,000 test sentences -LRB- Indonesian or Spanish -RRB- into English	conj_or_Indonesian_Spanish dep_sentences_Spanish dep_sentences_Indonesian nn_sentences_test num_sentences_2,000 prep_into_translate_English dobj_translate_sentences aux_translate_to nn_al._et amod_Koehn_2007 dep_Koehn_al. nn_decoder_search nn_decoder_beam det_decoder_a prep_in_used_decoder dobj_used_them nsubj_used_we conj_or_Indonesian_Spanish dep_sentences_Spanish dep_sentences_Indonesian num_sentences_2,000 prep_of_set_sentences nn_set_development amod_set_separate det_set_a dep_Och_2003 appos_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum prep_on_using_set dobj_using_training amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni dobj_optimizing_Bleu det_weights_all vmod_set_translate dep_set_Koehn conj_and_set_used xcomp_set_using prepc_by_set_optimizing dobj_set_weights nsubj_set_We
E06-1005	P02-1040	p	3.2 Evaluation Criteria Well-established objective evaluation measures like the word error rate -LRB- WER -RRB- positionindependent word error rate -LRB- PER -RRB- and the BLEU score -LRB- Papineni et al. 2002 -RRB- were used to assess the translation quality	nn_quality_translation det_quality_the dobj_assess_quality aux_assess_to xcomp_used_assess auxpass_used_were nsubjpass_used_measures amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_score_Papineni nn_score_BLEU det_score_the conj_and_rate_score appos_rate_PER nn_rate_error nn_rate_word amod_rate_positionindependent appos_rate_WER nn_rate_error nn_rate_word det_rate_the dep_measures_score dep_measures_rate prep_like_measures_rate nn_measures_evaluation amod_measures_objective amod_measures_Well-established nn_measures_Criteria nn_measures_Evaluation num_measures_3.2
E06-1031	P02-1040	p	State-of-the-art measures such as BLEU -LRB- Papineni et al. 2002 -RRB- or NIST -LRB- Doddington 2002 -RRB- aim at measuring the translation quality rather on the document level1 than on the level of single sentences	amod_sentences_single prep_of_level_sentences det_level_the pobj_on_level pcomp_than_on prep_level1_than dep_document_level1 det_document_the nn_quality_translation det_quality_the prep_on_measuring_document advmod_measuring_rather dobj_measuring_quality dep_aim_Doddington nn_aim_NIST dep_Doddington_2002 dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_or_BLEU_aim dep_BLEU_Papineni prepc_at_measures_measuring prep_such_as_measures_aim prep_such_as_measures_BLEU amod_measures_State-of-the-art
E06-1032	P02-1040	o	1 Introduction Over the past five years progress in machine translation and to a lesser extent progress in natural language generation tasks such as summarization has been driven by optimizing against n-grambased evaluation metrics such as Bleu -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni prep_such_as_metrics_Bleu nn_metrics_evaluation amod_metrics_n-grambased prep_against_optimizing_metrics agent_driven_optimizing auxpass_driven_been aux_driven_has nsubjpass_driven_to nsubjpass_driven_Introduction prep_such_as_tasks_summarization nn_tasks_generation nn_tasks_language amod_tasks_natural prep_in_progress_tasks nn_progress_extent det_progress_a amod_extent_lesser pobj_to_progress nn_translation_machine prep_in_progress_translation nn_progress_years amod_progress_past det_progress_the num_years_five conj_and_Introduction_to prep_over_Introduction_progress num_Introduction_1
E06-1040	P02-1040	p	While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments -LRB- Papineni et al. 2002 Doddington 2002 -RRB- we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments correlation studies have been made of individual components -LRB- Bangalore et al. 2000 -RRB- but not of systems	pobj_of_systems neg_of_not amod_Bangalore_2000 dep_Bangalore_al. nn_Bangalore_et amod_components_individual prep_of_made_components auxpass_made_been aux_made_have nsubjpass_made_studies nn_studies_correlation amod_judgments_human prep_with_correlated_judgments auxpass_correlated_are nsubjpass_correlated_metrics mark_correlated_that nn_systems_NLG prep_of_metrics_systems nn_metrics_evaluation amod_metrics_corpus-based ccomp_shown_correlated aux_shown_have nsubj_shown_that rcmod_studies_shown det_studies_any conj_but_aware_of dep_aware_Bangalore parataxis_aware_made prep_of_aware_studies neg_aware_not cop_aware_are nsubj_aware_we advcl_aware_shown dep_Doddington_2002 dep_Papineni_Doddington appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_judgments_Papineni amod_judgments_human prep_with_correlate_judgments advmod_correlate_well nsubj_correlate_ratings mark_correlate_that amod_metrics_similar conj_and_BLEU_metrics nn_systems_MT prep_by_ratings_metrics prep_by_ratings_BLEU prep_of_ratings_systems ccomp_shown_correlate aux_shown_have nsubj_shown_studies mark_shown_While
E06-1040	P02-1040	p	The BLEU metric -LRB- Papineni et al. 2002 -RRB- in MT has been particularly successful for example MT05 the 2005 NIST MT evaluation exercise used BLEU-4 as the only method of evaluation	prep_of_method_evaluation amod_method_only det_method_the prep_as_BLEU-4_method amod_BLEU-4_used appos_exercise_BLEU-4 nn_exercise_evaluation nn_exercise_MT nn_exercise_NIST num_exercise_2005 det_exercise_the ccomp_,_exercise nn_MT05_example pobj_for_MT05 dep_successful_for advmod_successful_particularly cop_successful_been aux_successful_has nsubj_successful_BLEU dep_successful_The dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni prep_in_BLEU_MT amod_BLEU_metric
E06-1040	P02-1040	p	Properly calculated BLEU scores have been shown to correlate reliably with human judgments -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_judgments_Papineni amod_judgments_human prep_with_correlate_judgments advmod_correlate_reliably aux_correlate_to xcomp_shown_correlate auxpass_shown_been aux_shown_have nsubjpass_shown_scores nn_scores_BLEU amod_scores_calculated advmod_calculated_Properly
E06-1040	P02-1040	p	Some NLG researchers are impressed by the success of the BLEU evaluation metric -LRB- Papineni et al. 2002 -RRB- in Machine Translation -LRB- MT -RRB- which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas algorithms and data sets	nn_sets_data conj_and_ideas_sets conj_and_ideas_algorithms amod_ideas_new prep_of_impact_sets prep_of_impact_algorithms prep_of_impact_ideas det_impact_the dobj_evaluate_impact advmod_evaluate_cheaply advmod_evaluate_quickly aux_evaluate_to conj_and_quickly_cheaply xcomp_allowing_evaluate dobj_allowing_researchers nn_field_MT det_field_the prepc_by_transformed_allowing dobj_transformed_field aux_transformed_has nsubj_transformed_which appos_Translation_MT nn_Translation_Machine dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni rcmod_evaluation_transformed prep_in_evaluation_Translation amod_evaluation_metric nn_evaluation_BLEU dep_the_evaluation prep_of_success_the det_success_the agent_impressed_success auxpass_impressed_are nsubjpass_impressed_researchers nn_researchers_NLG det_researchers_Some ccomp_``_impressed
E09-1017	P02-1040	o	For example in machine translation evaluation approaches such as BLEU -LRB- Papineni et al. 2002 -RRB- use n-gram overlap comparisons with a model to judge overall goodness with higher n-grams meant to capture fluency considerations	nn_considerations_fluency dobj_capture_considerations aux_capture_to xcomp_meant_capture nsubj_meant_n-grams mark_meant_with amod_n-grams_higher amod_goodness_overall dobj_judge_goodness aux_judge_to vmod_model_judge det_model_a prep_with_comparisons_model advcl_overlap_meant dobj_overlap_comparisons nsubj_overlap_approaches prep_in_overlap_evaluation prep_for_overlap_example dep_use_n-gram dep_use_Papineni nn_use_BLEU dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_such_as_approaches_use nn_evaluation_translation nn_evaluation_machine
E09-1017	P02-1040	o	Evaluation metrics such as BLEU -LRB- Papineni et al. 2002 -RRB- have a built-in preference for shorter translations	amod_translations_shorter prep_for_preference_translations amod_preference_built-in det_preference_a dobj_have_preference nsubj_have_metrics amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_such_as_metrics_BLEU nn_metrics_Evaluation
E09-1063	P02-1040	o	The performance of PB-SMT system is measured with BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU dep_measured_Papineni prep_with_measured_score auxpass_measured_is nsubjpass_measured_performance nn_system_PB-SMT prep_of_performance_system det_performance_The ccomp_``_measured
E09-1097	P02-1040	o	Success is indicated by the proportion of the original sentence regenerated as measured by any string comparison method in our case using the BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric det_BLEU_the dobj_using_BLEU ccomp_,_using poss_case_our pobj_in_case nn_method_comparison nn_method_string det_method_any prep_by_measured_method mark_measured_as dep_regenerated_in advcl_regenerated_measured amod_sentence_original det_sentence_the prep_of_proportion_sentence det_proportion_the dep_indicated_regenerated agent_indicated_proportion auxpass_indicated_is nsubjpass_indicated_Success
E09-3008	P02-1040	o	5.1 Evaluation of Translation Translations are evaluated on two automatic metrics Bleu -LRB- Papineni et al. 2002 -RRB- and PER position independent error-rate -LRB- Tillmann et al. 1997 -RRB-	amod_Tillmann_1997 dep_Tillmann_al. nn_Tillmann_et dep_error-rate_Tillmann amod_error-rate_independent amod_position_error-rate amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_Bleu_position conj_and_Bleu_PER dep_Bleu_Papineni dep_metrics_PER dep_metrics_Bleu amod_metrics_automatic num_metrics_two prep_on_evaluated_metrics auxpass_evaluated_are nsubjpass_evaluated_Evaluation nn_Translations_Translation prep_of_Evaluation_Translations num_Evaluation_5.1
E09-3008	P02-1040	o	This system was worse than the baseline on Bleu -LRB- Papineni et al. 2002 -RRB- but an error analysis showed some improvements	det_improvements_some dobj_showed_improvements nsubj_showed_analysis nn_analysis_error det_analysis_an amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_on_baseline_Bleu det_baseline_the conj_but_worse_showed dep_worse_Papineni prep_than_worse_baseline cop_worse_was nsubj_worse_system det_system_This
H05-1005	P02-1040	p	The table also shows the popular BLEU -LRB- Papineni et al. 2002 -RRB- and NIST2 MT metrics	nn_metrics_MT nn_metrics_NIST2 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_metrics dep_BLEU_Papineni amod_BLEU_popular det_BLEU_the dobj_shows_metrics dobj_shows_BLEU advmod_shows_also nsubj_shows_table det_table_The ccomp_``_shows
H05-1019	P02-1040	n	They reported that their method is superior to BLEU -LRB- Papineni et al. 2002 -RRB- in terms of the correlation between human assessment and automatic evaluation	amod_evaluation_automatic conj_and_assessment_evaluation amod_assessment_human prep_between_correlation_evaluation prep_between_correlation_assessment det_correlation_the prep_of_terms_correlation amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_to_superior_BLEU cop_superior_is nsubj_superior_method mark_superior_that poss_method_their prep_in_reported_terms dep_reported_Papineni ccomp_reported_superior nsubj_reported_They
H05-1023	P02-1040	o	We report case sensitive Bleu -LRB- Papineni et al. 2002 -RRB- scoreBleuCforallexperiments	dep_scoreBleuCforallexperiments_Papineni dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_scoreBleuCforallexperiments amod_Bleu_sensitive nn_Bleu_case dobj_report_Bleu nsubj_report_We
H05-1049	P02-1040	o	3 Semantic Representation 3.1 The Need for Dependencies Perhaps the most common representation of text for assessing content is Bag-Of-Words or Bag-of-NGrams -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_or_Bag-Of-Words_Bag-of-NGrams cop_Bag-Of-Words_is nsubj_Bag-Of-Words_representation dobj_assessing_content prepc_for_representation_assessing prep_of_representation_text amod_representation_common det_representation_the advmod_representation_Perhaps advmod_common_most dep_Need_Papineni rcmod_Need_Bag-of-NGrams rcmod_Need_Bag-Of-Words prep_for_Need_Dependencies det_Need_The num_Need_3.1 dep_Representation_Need amod_Representation_Semantic num_Representation_3 dep_``_Representation
H05-1095	P02-1040	n	Unfortunately this is not the case for such widely used MT evaluation metrics as BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni prep_as_metrics_NIST prep_as_metrics_BLEU nn_metrics_evaluation nn_metrics_MT amod_metrics_used amod_metrics_such advmod_used_widely prep_for_case_metrics det_case_the neg_case_not cop_case_is nsubj_case_this advmod_case_Unfortunately
H05-1098	P02-1040	o	The feature weights are learned by maximizing the BLEU score -LRB- Papineni et al. 2002 -RRB- on held-out data,usingminimum-error-ratetraining -LRB- Och ,2003 -RRB- as implemented by Koehn	prep_by_implemented_Koehn mark_implemented_as num_Och_,2003 appos_data,usingminimum-error-ratetraining_Och amod_data,usingminimum-error-ratetraining_held-out dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dobj_maximizing_score advcl_learned_implemented prep_on_learned_data,usingminimum-error-ratetraining dep_learned_Papineni agent_learned_maximizing auxpass_learned_are nsubjpass_learned_weights nn_weights_feature det_weights_The ccomp_``_learned
H05-1098	P02-1040	o	5 Analysis Over the last few years several automatic metrics for machine translation evaluation have been introduced largely to reduce the human cost of iterative system evaluation during the development cycle -LRB- Lin and Och 2004 Melamed et al. 2003 Papineni et al. 2002 -RRB-	num_Papineni_2002 nn_Papineni_al. nn_Papineni_et num_Melamed_2003 nn_Melamed_al. nn_Melamed_et dep_Lin_Papineni dep_Lin_Melamed num_Lin_2004 conj_and_Lin_Och dep_cycle_Och dep_cycle_Lin nn_cycle_development det_cycle_the nn_evaluation_system amod_evaluation_iterative prep_of_cost_evaluation amod_cost_human det_cost_the prep_during_reduce_cycle dobj_reduce_cost aux_reduce_to advmod_reduce_largely xcomp_introduced_reduce auxpass_introduced_been aux_introduced_have nsubjpass_introduced_metrics dep_introduced_Analysis nn_evaluation_translation nn_evaluation_machine prep_for_metrics_evaluation amod_metrics_automatic amod_metrics_several amod_years_few amod_years_last det_years_the prep_over_Analysis_years num_Analysis_5
H05-1109	P02-1040	o	For extrinsic evaluation of machine translation we use the BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric det_BLEU_the dobj_use_BLEU nsubj_use_we prep_for_use_evaluation nn_translation_machine prep_of_evaluation_translation amod_evaluation_extrinsic
H05-1117	P02-1040	p	3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the BLEU metric for machine translation -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_translation_machine nn_metric_BLEU det_metric_the dep_implemented_Papineni prep_for_implemented_translation prep_in_implemented_metric advmod_implemented_successfully dep_implemented_first auxpass_implemented_was nsubjpass_implemented_idea nn_outputs_reference amod_outputs_desired num_outputs_more num_outputs_one conj_or_one_more prep_against_system_outputs nn_system_computer det_system_a prep_of_output_system det_output_the dobj_score_output aux_score_to nn_statistics_co-occurrence amod_statistics_n-gram vmod_employing_score dobj_employing_statistics prepc_of_idea_employing det_idea_The rcmod_Work_implemented amod_Work_Previous num_Work_3
H05-2007	P02-1040	o	We can incorporate each model into the system in turn and rank the results on a test corpus using BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dobj_using_BLEU nn_corpus_test det_corpus_a vmod_results_using prep_on_results_corpus det_results_the dobj_rank_results nsubj_rank_We det_system_the det_model_each conj_and_incorporate_rank prep_in_incorporate_turn prep_into_incorporate_system dobj_incorporate_model aux_incorporate_can nsubj_incorporate_We ccomp_``_rank ccomp_``_incorporate
H05-2007	P02-1040	o	1 Introduction Over the last few years several automatic metrics for machine translation -LRB- MT -RRB- evaluation have been introduced largely to reduce the human cost of iterative system evaluation during the development cycle -LRB- Papineni et al. 2002 Melamed et al. 2003 -RRB-	num_Melamed_2003 nn_Melamed_al. nn_Melamed_et dep_Papineni_Melamed appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_cycle_development det_cycle_the nn_evaluation_system amod_evaluation_iterative prep_of_cost_evaluation amod_cost_human det_cost_the prep_during_reduce_cycle dobj_reduce_cost aux_reduce_to advmod_reduce_largely dep_introduced_Papineni xcomp_introduced_reduce auxpass_introduced_been aux_introduced_have nsubjpass_introduced_metrics dep_introduced_Introduction nn_evaluation_translation appos_translation_MT nn_translation_machine prep_for_metrics_evaluation amod_metrics_automatic amod_metrics_several amod_years_few amod_years_last det_years_the prep_over_Introduction_years num_Introduction_1
I05-2021	P02-1040	o	However recent progress in machine translation and the continuous improvement on evaluation metrics such as BLEU -LRB- Papineni et al. 2002 -RRB- suggest that SMT systems are already very good at choosing correct word translations	nn_translations_word amod_translations_correct dobj_choosing_translations prepc_at_good_choosing advmod_good_very advmod_good_already cop_good_are nsubj_good_systems mark_good_that nn_systems_SMT ccomp_suggest_good nsubj_suggest_improvement nsubj_suggest_progress advmod_suggest_However amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_such_as_metrics_BLEU nn_metrics_evaluation prep_on_improvement_metrics amod_improvement_continuous det_improvement_the nn_translation_machine conj_and_progress_improvement prep_in_progress_translation amod_progress_recent
I05-2039	P02-1040	o	Because it is not feasible here to have humans judge the quality of many sets of translated data we rely on an array of well known automatic evaluation measures to estimate translation quality BLEU -LRB- Papineni et al. 2002 -RRB- is the geometric mean of the n-gram precisions in the output with respect to a set of reference translations	nn_translations_reference prep_of_set_translations det_set_a det_output_the nn_precisions_n-gram det_precisions_the prep_with_respect_to_mean_set prep_in_mean_output prep_of_mean_precisions amod_mean_geometric det_mean_the cop_mean_is nsubj_mean_BLEU dep_2002_al. nn_al._et num_Papineni_2002 appos_BLEU_Papineni dep_quality_mean nn_quality_translation dobj_estimate_quality aux_estimate_to vmod_measures_estimate nn_measures_evaluation amod_measures_automatic amod_measures_known advmod_known_well prep_of_array_measures det_array_an prep_on_rely_array nsubj_rely_we advcl_rely_feasible amod_data_translated prep_of_sets_data amod_sets_many prep_of_quality_sets det_quality_the dobj_judge_quality nsubj_judge_humans ccomp_have_judge aux_have_to xcomp_feasible_have advmod_feasible_here neg_feasible_not cop_feasible_is nsubj_feasible_it mark_feasible_Because
I05-2042	P02-1040	n	Although there are various manual/automatic evaluation methods for these systems e.g. BLEU -LRB- Papineni et al. 2002 -RRB- these methods are basically incapable of dealing with an MTsystem and a w/p-MT-system at the same time as they have different output forms	nn_forms_output amod_forms_different dobj_have_forms nsubj_have_they mark_have_as amod_time_same det_time_the det_w/p-MT-system_a conj_and_MTsystem_w/p-MT-system det_MTsystem_an prep_at_dealing_time prep_with_dealing_w/p-MT-system prep_with_dealing_MTsystem advcl_incapable_have prepc_of_incapable_dealing advmod_incapable_basically cop_incapable_are nsubj_incapable_methods dep_incapable_BLEU advmod_incapable_e.g. det_methods_these dep_al._2002 nn_al._et advmod_Papineni_al. appos_BLEU_Papineni det_systems_these dep_methods_incapable prep_for_methods_systems nn_methods_evaluation amod_methods_manual/automatic amod_methods_various nsubj_are_methods expl_are_there mark_are_Although advcl_``_are
I05-5008	P02-1040	o	Automatic measures like BLEU -LRB- PAPINENI et al. 2001 -RRB- or NIST -LRB- DODDINGTON 2002 -RRB- do so by counting sequences of words in such paraphrases	amod_paraphrases_such prep_in_words_paraphrases prep_of_sequences_words dobj_counting_sequences prepc_by_do_counting advmod_do_so nsubj_do_NIST nsubj_do_measures amod_DODDINGTON_2002 dep_NIST_DODDINGTON dep_PAPINENI_2001 dep_PAPINENI_al. nn_PAPINENI_et conj_or_measures_NIST dep_measures_PAPINENI prep_like_measures_BLEU nn_measures_Automatic
I08-1030	P02-1040	o	The translations are evaluated in terms of BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU prep_of_terms_score dep_evaluated_Papineni prep_in_evaluated_terms auxpass_evaluated_are nsubjpass_evaluated_translations det_translations_The ccomp_``_evaluated
I08-2088	P02-1040	o	The horizontal axis represents the weight for the outof-domain translation model and the vertical axis 15 % 16 % 17 % 18 % 19 % 20 % 21 % 22 % 23 % 24 % 25 % 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Weight for out-of-domain translation model BLEU sco re 400 K 800 K 1.2 M 1.6 M 2.5 M Figure 2 Results of data selection and linear interpolation -LRB- BLEU -RRB- represents the automatic metric of translation quality -LRB- BLEU score -LRB- Papineni et al. 2002 -RRB- in Fig	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_in_score_Fig dep_score_Papineni nn_score_BLEU nn_quality_translation dep_metric_score prep_of_metric_quality amod_metric_automatic det_metric_the dobj_represents_metric nsubj_represents_Results appos_interpolation_BLEU amod_interpolation_linear conj_and_selection_interpolation nn_selection_data prep_of_Results_interpolation prep_of_Results_selection num_Figure_2 nn_Figure_M num_Figure_2.5 dep_M_Figure num_M_1.6 dep_M_M num_M_1.2 dep_K_M num_K_800 nn_K_K num_K_400 dep_re_K dep_sco_re dep_BLEU_sco dep_model_represents dep_model_BLEU dep_translation_model dep_out-of-domain_translation prep_for_Weight_out-of-domain num_Weight_1.0 num_Weight_0.1 number_1.0_0.9 dep_1.0_0.8 dep_1.0_0.5 dep_0.8_0.7 number_0.7_0.6 number_0.5_0.4 dep_0.5_0.3 number_0.3_0.2 number_0.1_0.0 dep_%_Weight num_%_25 dep_%_% num_%_24 dep_%_% num_%_23 dep_%_% num_%_22 dep_%_% num_%_21 dep_%_% num_%_20 dep_%_% num_%_19 dep_%_% num_%_18 dep_%_% num_%_17 dep_%_% num_%_16 dep_%_% num_%_15 dep_axis_% amod_axis_vertical det_axis_the nn_model_translation amod_model_outof-domain det_model_the prep_for_weight_model det_weight_the conj_and_represents_axis dobj_represents_weight nsubj_represents_axis amod_axis_horizontal det_axis_The
J05-3002	P02-1040	n	This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures such as BLEU -LRB- Papineni et al. 2002 -RRB- and ROUGE -LRB- Lin and Hovy 2003 -RRB- show that these metrics do not correlate with human judgments on the fusion task when tested against two reference outputs	nn_outputs_reference num_outputs_two prep_against_tested_outputs advmod_tested_when nn_task_fusion det_task_the rcmod_judgments_tested prep_on_judgments_task amod_judgments_human prep_with_correlate_judgments neg_correlate_not aux_correlate_do nsubj_correlate_metrics mark_correlate_that det_metrics_these ccomp_show_correlate num_Hovy_2003 conj_and_Lin_Hovy appos_ROUGE_Hovy appos_ROUGE_Lin dep_2002_al. nn_al._et num_Papineni_2002 conj_and_BLEU_ROUGE dep_BLEU_Papineni prep_such_as_measures_ROUGE prep_such_as_measures_BLEU dobj_overlap_measures nsubj_overlap_problem mark_overlap_because amod_experiments_preliminary poss_experiments_Our num_experiments_5 dep_alignments_experiments amod_alignments_many-to-many prep_with_optimizing_n-gram-based dobj_optimizing_alignments prepc_of_problem_optimizing det_problem_the dep_necessary_show advcl_necessary_overlap cop_necessary_is nsubj_necessary_restriction det_restriction_This
J05-4003	P02-1040	o	5 Translation performance was measured using the automatic BLEU evaluation metric -LRB- Papineni et al. 2002 -RRB- on four reference translations	nn_translations_reference num_translations_four num_al._2002 nn_al._et amod_al._Papineni dep_evaluation_al. amod_evaluation_metric prep_on_BLEU_translations amod_BLEU_evaluation dobj_automatic_BLEU dep_the_automatic dobj_using_the xcomp_measured_using auxpass_measured_was nsubjpass_measured_performance nn_performance_Translation num_performance_5
J06-4002	P02-1040	o	For instance several studies have shown that BLEU correlates with human ratings on machine translation quality -LRB- Papineni et al. 2002 Doddington 2002 Coughlin 2003 -RRB-	num_Coughlin_2003 num_Doddington_2002 nn_al._et dep_Papineni_Coughlin dep_Papineni_Doddington dep_Papineni_2002 dep_Papineni_al. nn_quality_translation nn_quality_machine prep_on_ratings_quality amod_ratings_human prep_with_correlates_ratings nsubj_correlates_BLEU mark_correlates_that dep_shown_Papineni ccomp_shown_correlates aux_shown_have nsubj_shown_studies prep_for_shown_instance amod_studies_several
J06-4002	P02-1040	o	However they can be usefully employed during system development for example for quickly assessing modeling ideas or for comparing across different system configurations -LRB- Papineni et al. 2002 Bangalore Rambow and Whittaker 2000 -RRB-	num_Whittaker_2000 conj_and_Bangalore_Whittaker conj_and_Bangalore_Rambow num_al._2002 nn_al._et dep_Papineni_Whittaker dep_Papineni_Rambow dep_Papineni_Bangalore advmod_Papineni_al. nn_configurations_system amod_configurations_different prep_across_comparing_configurations dep_for_Papineni pcomp_for_comparing nn_ideas_modeling conj_or_assessing_for dobj_assessing_ideas advmod_assessing_quickly prepc_for_,_for prepc_for_,_assessing pobj_for_example dep_,_for nn_development_system prep_during_employed_development advmod_employed_usefully auxpass_employed_be aux_employed_can nsubjpass_employed_they advmod_employed_However advcl_``_employed
J06-4004	P02-1040	o	Translation accuracy is measured in terms of the BLEU score -LRB- Papineni et al. 2002 -RRB- which is computed here for translations generated by using the tuple n-gram model alone in the case of Table 2 and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2 in the case of Table 3	num_Table_3 prep_of_case_Table det_case_the pobj_in_case ccomp_,_in num_Section_3.2 prep_in_described_Section vmod_functions_described nn_functions_feature num_functions_four amod_functions_additional det_functions_the nn_model_n-gram nn_model_tuple det_model_the pobj_using_functions prepc_along_with_using_with dobj_using_model pcomp_by_using num_Table_2 prep_of_case_Table det_case_the advmod_model_alone nn_model_n-gram nn_model_tuple det_model_the dobj_using_model agent_generated_using vmod_translations_generated conj_and_computed_by prep_in_computed_case prep_for_computed_translations advmod_computed_here auxpass_computed_is nsubjpass_computed_which dep_2002_al. nn_al._et num_Papineni_2002 nn_score_BLEU det_score_the prep_of_terms_score dep_measured_by dep_measured_computed dep_measured_Papineni prep_in_measured_terms auxpass_measured_is nsubjpass_measured_accuracy nn_accuracy_Translation
J06-4004	P02-1040	o	In our SMT system implementation this optimization procedure is performed by using a tool developed in-house which is based on a simplex method -LRB- Press et al. 2002 -RRB- and the BLEU score -LRB- Papineni et al. 2002 -RRB- is used as a translation quality measurement	nn_measurement_quality nn_measurement_translation det_measurement_a prep_as_used_measurement auxpass_used_is nsubjpass_used_score dep_2002_al. nn_al._et num_Papineni_2002 appos_score_Papineni nn_score_BLEU det_score_the dep_al._2002 nn_al._et advmod_Press_al. appos_method_Press nn_method_simplex det_method_a prep_on_based_method auxpass_based_is nsubjpass_based_which conj_and_in-house_used conj_and_in-house_based dobj_developed_used dobj_developed_based dobj_developed_in-house nsubj_developed_tool det_tool_a ccomp_using_developed agent_performed_using auxpass_performed_is nsubjpass_performed_procedure prep_in_performed_implementation nn_procedure_optimization det_procedure_this nn_implementation_system nn_implementation_SMT poss_implementation_our
J07-1003	P02-1040	o	The translation quality on the TransType2 task in terms of WER PER BLEU score -LRB- Papineni et al. 2002 -RRB- and NIST score -LRB- NIST 2002 -RRB- is given in Table 4	num_Table_4 prep_in_given_Table auxpass_given_is nsubjpass_given_quality num_NIST_2002 appos_score_NIST nn_score_NIST dep_2002_al. nn_al._et num_Papineni_2002 appos_score_Papineni nn_score_BLEU conj_and_WER_score conj_and_WER_score conj_and_WER_PER prep_of_terms_score prep_of_terms_score prep_of_terms_PER prep_of_terms_WER nn_task_TransType2 det_task_the prep_in_quality_terms prep_on_quality_task nn_quality_translation det_quality_The ccomp_``_given
J07-2003	P02-1040	o	Finally the parameters i of the log-linear model -LRB- 18 -RRB- are learned by minimumerror-rate training -LRB- Och 2003 -RRB- which tries to set the parameters so as to maximize the BLEU score -LRB- Papineni et al. 2002 -RRB- of a development set	nn_set_development det_set_a dep_2002_al. nn_al._et num_Papineni_2002 nn_score_BLEU det_score_the prep_of_maximize_set dep_maximize_Papineni dobj_maximize_score aux_maximize_to det_parameters_the prepc_as_set_maximize advmod_set_so dobj_set_parameters aux_set_to xcomp_tries_set nsubj_tries_which num_Och_2003 rcmod_training_tries appos_training_Och amod_training_minimumerror-rate agent_learned_training auxpass_learned_are dep_learned_i nsubjpass_learned_parameters advmod_learned_Finally appos_model_18 amod_model_log-linear det_model_the prep_of_i_model det_parameters_the
J07-2003	P02-1040	o	Our evaluation metric is case-insensitive BLEU-4 -LRB- Papineni et al. 2002 -RRB- as defined by NIST that is using the shortest -LRB- as opposed to closest -RRB- reference sentence length for the brevity penalty	nn_penalty_brevity det_penalty_the nn_length_sentence nn_length_reference prep_for_opposed_penalty prep_to_opposed_length dep_opposed_closest prepc_as_shortest_opposed det_shortest_the dobj_using_shortest xcomp_is_using nsubj_is_that rcmod_NIST_is prep_by_defined_NIST mark_defined_as dep_2002_al. nn_al._et num_Papineni_2002 advcl_BLEU-4_defined appos_BLEU-4_Papineni amod_BLEU-4_case-insensitive cop_BLEU-4_is nsubj_BLEU-4_metric nn_metric_evaluation poss_metric_Our
J07-2003	P02-1040	o	9 The definition of BLEU used in this training was the original IBM definition -LRB- Papineni et al. 2002 -RRB- which defines the effective reference length as the reference length that is closest to the test sentence length	nn_length_sentence nn_length_test det_length_the prep_to_closest_length cop_closest_is nsubj_closest_that rcmod_length_closest nn_length_reference det_length_the nn_length_reference amod_length_effective det_length_the prep_as_defines_length dobj_defines_length nsubj_defines_which dep_2002_al. nn_al._et num_Papineni_2002 rcmod_definition_defines appos_definition_Papineni nn_definition_IBM amod_definition_original det_definition_the cop_definition_was nsubj_definition_definition det_training_this prep_in_used_training vmod_BLEU_used prep_of_definition_BLEU det_definition_The num_definition_9
N03-1003	P02-1040	o	This could for example aid machine-translation evaluation where it has become common to evaluate systems by comparing their output against a bank of several reference translations for the same sentences -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_sentences_same det_sentences_the prep_for_translations_sentences nn_translations_reference amod_translations_several prep_of_bank_translations det_bank_a poss_output_their dep_comparing_Papineni prep_against_comparing_bank dobj_comparing_output dobj_evaluate_systems aux_evaluate_to prepc_by_become_comparing xcomp_become_evaluate acomp_become_common aux_become_has nsubj_become_it advmod_become_where rcmod_evaluation_become nn_evaluation_machine-translation nn_evaluation_aid dep_,_evaluation prep_for_,_example dep_This_could
N03-1010	P02-1040	o	distance -LRB- MSD -RRB- and the maximum swap segment size -LRB- MSSS -RRB- ranging from 0 to 10 and evaluated the translations with the BLEU7 metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU7_metric det_BLEU7_the prep_with_translations_BLEU7 det_translations_the dobj_evaluated_translations prep_to_ranging_10 prep_from_ranging_0 vmod_size_ranging appos_size_MSSS nn_size_segment nn_size_swap nn_size_maximum det_size_the conj_and_distance_evaluated conj_and_distance_size appos_distance_MSD
N03-1013	P02-1040	o	7For details about the Bleu evaluation metric see -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_see_Papineni amod_evaluation_metric nn_evaluation_Bleu det_evaluation_the vmod_details_see prep_about_details_evaluation nn_details_7For
N03-2013	P02-1040	o	Expansion of the equivalent sentence set can be applied to automatic evaluation of machine translation quality -LRB- Papineni et al. 2002 Akiba et al. 2001 -RRB- for example	pobj_for_example ccomp_,_for num_Akiba_2001 nn_Akiba_al. nn_Akiba_et dep_Papineni_Akiba appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_quality_translation nn_quality_machine prep_of_evaluation_quality amod_evaluation_automatic prep_to_applied_evaluation auxpass_applied_be aux_applied_can nsubjpass_applied_set rcmod_sentence_applied amod_sentence_equivalent det_sentence_the appos_Expansion_Papineni prep_of_Expansion_sentence ccomp_``_Expansion
N03-2016	P02-1040	o	For the evaluation of translation quality we used the BLEU metric -LRB- Papineni et al. 2002 -RRB- which measures the n-gram overlap between the translated output and one or more reference translations	nn_translations_reference num_translations_more num_translations_one conj_or_one_more conj_and_output_translations amod_output_translated det_output_the prep_between_overlap_translations prep_between_overlap_output nsubj_overlap_n-gram det_n-gram_the ccomp_measures_overlap nsubj_measures_which dep_al._2002 nn_al._et dep_Papineni_al. appos_metric_Papineni rcmod_BLEU_measures amod_BLEU_metric dep_the_BLEU dobj_used_the nsubj_used_we prep_for_used_evaluation nn_quality_translation prep_of_evaluation_quality det_evaluation_the
N03-2036	P02-1040	o	The third column reports the BLEU score -LRB- Papineni et al. 2002 -RRB- along with 95 % confidence interval	nn_interval_confidence amod_interval_% number_%_95 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et pobj_score_interval prepc_along_with_score_with dep_score_Papineni nn_score_BLEU det_score_the dobj_reports_score nsubj_reports_column amod_column_third det_column_The
N04-1008	P02-1040	p	4.4.1 N-gram Co-Occurrence Statistics for Answer Extraction N-gram co-occurrence statistics have been successfully used in automatic evaluation -LRB- Papineni et al. 2002 Lin and Hovy 2003 -RRB- and more recently as training criteria in statistical machine translation -LRB- Och 2003 -RRB-	num_Och_2003 appos_translation_Och nn_translation_machine amod_translation_statistical prep_in_criteria_translation nn_criteria_training prep_as_recently_criteria advmod_recently_more num_Hovy_2003 conj_and_Papineni_Hovy conj_and_Papineni_Lin dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_evaluation_automatic advmod_used_recently cc_used_and dep_used_Hovy dep_used_Lin dep_used_Papineni prep_in_used_evaluation advmod_used_successfully auxpass_used_been aux_used_have nsubjpass_used_Statistics nn_statistics_co-occurrence nn_statistics_N-gram nn_statistics_Extraction nn_statistics_Answer prep_for_Statistics_statistics nn_Statistics_Co-Occurrence nn_Statistics_N-gram num_Statistics_4.4.1
N04-1019	P02-1040	p	In machine translation the rankings from the automatic BLEU method -LRB- Papineni et al. 2002 -RRB- have been shown to correlate well with human evaluation and it has been widely used since and has even been adapted for summarization -LRB- Lin and Hovy 2003 -RRB-	amod_Lin_2003 conj_and_Lin_Hovy dep_summarization_Hovy dep_summarization_Lin prep_for_adapted_summarization auxpass_adapted_been advmod_adapted_even aux_adapted_has nsubjpass_adapted_it conj_and_used_adapted advmod_used_since advmod_used_widely auxpass_used_been aux_used_has nsubjpass_used_it amod_evaluation_human prep_with_correlate_evaluation advmod_correlate_well aux_correlate_to conj_and_shown_adapted conj_and_shown_used xcomp_shown_correlate auxpass_shown_been aux_shown_have nsubjpass_shown_rankings prep_in_shown_translation amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_method_BLEU amod_method_automatic det_method_the dep_rankings_Papineni prep_from_rankings_method det_rankings_the nn_translation_machine
N04-4003	P02-1040	o	Word Error Rate -LRB- WER -RRB- which penalizes the edit distance against reference translations -LRB- Su et al. 1992 -RRB- BLEU the geometric mean of n-gram precision for the translation results found in reference translations -LRB- Papineni et al. 2002 -RRB- Translation Accuracy -LRB- ACC -RRB- subjective evaluation ranks ranging from A to D -LRB- A perfect B fair C acceptable and D nonsense -RRB- judged blindly by a native speaker -LRB- Sumita et al. 1999 -RRB- In contrast to WER higher BLEU and ACC scores indicate better translations	amod_translations_better dobj_indicate_translations nsubj_indicate_C nn_scores_ACC amod_BLEU_higher conj_and_WER_scores conj_and_WER_BLEU prep_to_contrast_scores prep_to_contrast_BLEU prep_to_contrast_WER amod_Sumita_1999 dep_Sumita_al. nn_Sumita_et dep_speaker_Sumita amod_speaker_native det_speaker_a prep_in_judged_contrast agent_judged_speaker advmod_judged_blindly dep_acceptable_nonsense conj_and_acceptable_D vmod_C_judged dep_C_D dep_C_acceptable amod_C_fair dep_B_indicate amod_B_perfect dep_A_B nn_A_D prep_to_ranging_A prep_from_ranging_A vmod_ranks_ranging nn_ranks_evaluation amod_ranks_subjective appos_Accuracy_ACC nn_Accuracy_Translation dep_Papineni_ranks dep_Papineni_Accuracy amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_translations_reference prep_in_found_translations vmod_results_found det_translation_the amod_precision_n-gram dep_mean_Papineni dep_mean_results prep_for_mean_translation prep_of_mean_precision amod_mean_geometric det_mean_the dep_al._1992 nn_al._et nn_al._Su dep_translations_BLEU dep_translations_al. nn_translations_reference nn_distance_edit det_distance_the prep_against_penalizes_translations dobj_penalizes_distance nsubj_penalizes_which dep_Rate_mean rcmod_Rate_penalizes appos_Rate_WER nn_Rate_Error nn_Rate_Word
N04-4015	P02-1040	o	Translation qualities are measured by uncased BLEU -LRB- Papineni et al. 2002 -RRB- with 4 reference translations sysids ahb ahc ahd ahe	conj_ahb_ahe conj_ahb_ahd conj_ahb_ahc dep_sysids_ahb nn_translations_reference num_translations_4 dep_2002_al. nn_al._et appos_Papineni_sysids prep_with_Papineni_translations num_Papineni_2002 amod_BLEU_uncased dep_measured_Papineni agent_measured_BLEU auxpass_measured_are nsubjpass_measured_qualities nn_qualities_Translation
N06-1003	P02-1040	o	To set the weights m we performed minimum error rate training -LRB- Och 2003 -RRB- on the development set using Bleu -LRB- Papineni et al. 2002 -RRB- as the objective function	amod_function_objective det_function_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni prep_as_using_function dobj_using_Bleu xcomp_set_using vmod_development_set det_development_the appos_Och_2003 dep_training_Och nn_training_rate nn_training_error amod_training_minimum prep_on_performed_development dobj_performed_training nsubj_performed_we tmod_performed_m advcl_performed_set det_weights_the dobj_set_weights aux_set_To
N06-1004	P02-1040	o	2 Disperp and Distortion Corpora 2.1 Defining Disperp The ultimate reason for choosing one SCM over another will be the performance of an MT system containing it as measured by a metric like BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_like_metric_BLEU det_metric_a dep_measured_Papineni prep_by_measured_metric mark_measured_as dobj_containing_it vmod_system_containing nn_system_MT det_system_an prep_of_performance_system det_performance_the cop_performance_be aux_performance_will nsubj_performance_reason prep_over_SCM_another num_SCM_one dobj_choosing_SCM prepc_for_reason_choosing amod_reason_ultimate det_reason_The rcmod_Disperp_performance dobj_Defining_Disperp vmod_Corpora_Defining num_Corpora_2.1 dep_Distortion_Corpora dep_Disperp_measured conj_and_Disperp_Distortion num_Disperp_2 dep_``_Distortion dep_``_Disperp
N06-1013	P02-1040	o	MT output is evaluated using the standard MT evaluation metric BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_BLEU_metric nn_BLEU_evaluation nn_BLEU_MT amod_BLEU_standard det_BLEU_the dobj_using_BLEU xcomp_evaluated_using auxpass_evaluated_is nsubjpass_evaluated_output amod_output_MT
N06-1031	P02-1040	o	In the nal step we score our translations with 4-gram BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_BLEU_4-gram poss_translations_our prep_with_score_BLEU dobj_score_translations nsubj_score_we prep_in_score_step amod_step_nal det_step_the
N06-1058	P02-1040	o	4.2 Impact of Paraphrases on Machine Translation Evaluation The standard way to analyze the performance of an evaluation metric in machine translation is to compute the Pearson correlation between the automatic metric and human scores -LRB- Papineni et al. 2002 Koehn 2004 Lin and Och 2004 Stent et al. 2005 -RRB-	num_Stent_2005 nn_Stent_al. nn_Stent_et num_Lin_2004 conj_and_Lin_Och dep_Koehn_Stent conj_Koehn_Och conj_Koehn_Lin conj_Koehn_2004 dep_Papineni_Koehn appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_scores_human conj_and_metric_scores amod_metric_automatic det_metric_the prep_between_correlation_scores prep_between_correlation_metric nn_correlation_Pearson det_correlation_the dep_compute_Papineni dobj_compute_correlation aux_compute_to xcomp_is_compute nsubj_is_evaluation nn_translation_machine prep_in_evaluation_translation amod_evaluation_metric rcmod_an_is prep_of_performance_an det_performance_the dobj_analyze_performance aux_analyze_to vmod_way_analyze amod_way_standard det_way_The dep_Evaluation_way nn_Evaluation_Translation nn_Evaluation_Machine prep_on_Impact_Evaluation prep_of_Impact_Paraphrases num_Impact_4.2 ccomp_``_Impact
N06-1058	P02-1040	n	This strategy is commonly used in MT evaluation because of BLEUs well-known problems with documents of small size -LRB- Papineni et al. 2002 Koehn 2004 -RRB-	amod_Koehn_2004 dep_Papineni_Koehn appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_size_small prep_of_documents_size appos_problems_Papineni prep_with_problems_documents amod_problems_well-known nn_problems_BLEUs nn_evaluation_MT prep_because_of_used_problems prep_in_used_evaluation advmod_used_commonly auxpass_used_is nsubjpass_used_strategy det_strategy_This
N06-1058	P02-1040	o	The Pearson correlation is calculated over these ten pairs -LRB- Papineni et al. 2002 Stent et al. 2005 -RRB-	num_Stent_2005 nn_Stent_al. nn_Stent_et dep_Papineni_Stent appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et num_pairs_ten det_pairs_these dep_calculated_Papineni prep_over_calculated_pairs auxpass_calculated_is nsubjpass_calculated_correlation nn_correlation_Pearson det_correlation_The ccomp_``_calculated
N06-1058	P02-1040	o	Our scores fall within the range of previous researchers -LRB- Papineni et al. 2002 Lin and Och 2004 -RRB-	num_Lin_2004 conj_and_Lin_Och dep_Papineni_Och dep_Papineni_Lin appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_researchers_previous prep_of_range_researchers det_range_the dep_fall_Papineni prep_within_fall_range nsubj_fall_scores poss_scores_Our ccomp_``_fall
N06-1058	P02-1040	o	Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community -LRB- NIST 2002 Melamed et al. 2003 Papineni et al. 2002 -RRB-	num_Papineni_2002 nn_Papineni_al. nn_Papineni_et dep_Melamed_Papineni num_Melamed_2003 nn_Melamed_al. nn_Melamed_et dep_NIST_Melamed appos_NIST_2002 dep_community_NIST nn_community_translation nn_community_machine det_community_the prep_in_proposed_community advmod_proposed_recently auxpass_proposed_been aux_proposed_have nsubjpass_proposed_variety nn_methods_evaluation amod_methods_automatic prep_of_variety_methods det_variety_A nn_variety_Measures nn_variety_Evaluation nn_variety_Automatic
N06-2029	P02-1040	o	For evaluation we used the BLEU metrics which calculates the geometric mean of n-gram precision for the MT outputs found in reference translations -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_translations_reference prep_in_found_translations vmod_outputs_found nn_outputs_MT det_outputs_the amod_precision_n-gram prep_of_mean_precision amod_mean_geometric det_mean_the prep_for_calculates_outputs dobj_calculates_mean nsubj_calculates_which rcmod_metrics_calculates nn_metrics_BLEU det_metrics_the dep_used_Papineni dobj_used_metrics nsubj_used_we prep_for_used_evaluation
N06-2051	P02-1040	o	We optimized separately for both the NIST -LRB- Doddington 2002 -RRB- and the BLEU metrics -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_metrics_Papineni nn_metrics_BLEU det_metrics_the amod_Doddington_2002 conj_and_NIST_metrics dep_NIST_Doddington det_NIST_the preconj_NIST_both prep_for_optimized_metrics prep_for_optimized_NIST advmod_optimized_separately nsubj_optimized_We
N07-1005	P02-1040	o	Many methods for calculating the similarity have been proposed -LRB- Niessen et al. 2000 Akiba et al. 2001 Papineni et al. 2002 NIST 2002 Leusch et al. 2003 Turian et al. 2003 Babych and Hartley 2004 Lin and Och 2004 Banerjee and Lavie 2005 Gimenez et al. 2005 -RRB-	num_Gimenez_2005 nn_Gimenez_al. nn_Gimenez_et num_Lin_2004 conj_and_Lin_Och dep_Babych_Gimenez conj_and_Babych_2005 conj_and_Babych_Lavie conj_and_Babych_Banerjee conj_and_Babych_Och conj_and_Babych_Lin conj_and_Babych_2004 conj_and_Babych_Hartley num_Turian_2003 nn_Turian_al. nn_Turian_et num_Leusch_2003 nn_Leusch_al. nn_Leusch_et num_NIST_2002 num_Papineni_2002 nn_Papineni_al. nn_Papineni_et conj_Akiba_2005 conj_Akiba_Lavie conj_Akiba_Banerjee conj_Akiba_Lin conj_Akiba_2004 conj_Akiba_Hartley conj_Akiba_Babych conj_Akiba_Turian conj_Akiba_Leusch conj_Akiba_NIST conj_Akiba_Papineni num_Akiba_2001 nn_Akiba_al. nn_Akiba_et dep_Niessen_Akiba appos_Niessen_2000 dep_Niessen_al. nn_Niessen_et dep_proposed_Niessen auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods det_similarity_the dobj_calculating_similarity prepc_for_methods_calculating amod_methods_Many ccomp_``_proposed
N07-1005	P02-1040	o	In our research 23 scores namely BLEU -LRB- Papineni et al. 2002 -RRB- with maximum n-gram lengths of 1 2 3 and 4 NIST -LRB- NIST 2002 -RRB- with maximum n-gram lengths of 1 2 3 4 and 5 GTM -LRB- Turian et al. 2003 -RRB- with exponents of 1.0 2.0 and 3.0 METEOR -LRB- exact -RRB- -LRB- Banerjee and Lavie 2005 -RRB- WER -LRB- Niessen et al. 2000 -RRB- PER -LRB- Leusch et al. 2003 -RRB- and ROUGE -LRB- Lin 2004 -RRB- with n-gram lengths of 1 2 3 and 4 and 4 variants -LRB- LCS S SU W-1 .2 -RRB- were used to calculate each similarity S i Therefore the value of m in Eq	prep_in_value_Eq prep_of_value_m det_value_the dep_Therefore_value dep_i_Therefore nn_i_S dep_similarity_i det_similarity_each dobj_calculate_similarity aux_calculate_to xcomp_used_calculate auxpass_used_were nsubjpass_used_scores prep_in_used_research nn_.2_W-1 appos_LCS_.2 conj_LCS_SU conj_LCS_S dep_variants_LCS num_variants_4 num_variants_4 num_variants_3 num_variants_2 num_variants_1 conj_and_1_4 conj_and_1_4 conj_and_1_3 conj_and_1_2 prep_of_lengths_variants nn_lengths_n-gram num_Lin_2004 prep_with_ROUGE_lengths appos_ROUGE_Lin amod_Leusch_2003 dep_Leusch_al. nn_Leusch_et appos_PER_Leusch amod_Niessen_2000 dep_Niessen_al. nn_Niessen_et dep_WER_Niessen dep_Banerjee_2005 conj_and_Banerjee_Lavie nn_Banerjee_METEOR appos_METEOR_exact conj_and_1.0_3.0 conj_and_1.0_2.0 prep_of_exponents_3.0 prep_of_exponents_2.0 prep_of_exponents_1.0 amod_Turian_2003 dep_Turian_al. nn_Turian_et prep_with_GTM_exponents dep_GTM_Turian conj_and_1_5 conj_and_1_4 conj_and_1_3 conj_and_1_2 prep_of_lengths_5 prep_of_lengths_4 prep_of_lengths_3 prep_of_lengths_2 prep_of_lengths_1 nn_lengths_n-gram nn_lengths_maximum dep_NIST_2002 prep_with_NIST_lengths dep_NIST_NIST conj_and_1_4 conj_and_1_3 conj_and_1_2 prep_of_lengths_4 prep_of_lengths_3 prep_of_lengths_2 prep_of_lengths_1 nn_lengths_n-gram nn_lengths_maximum dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_ROUGE conj_and_BLEU_PER conj_and_BLEU_WER dep_BLEU_Lavie dep_BLEU_Banerjee dep_BLEU_GTM dep_BLEU_NIST prep_with_BLEU_lengths appos_BLEU_Papineni advmod_BLEU_namely appos_scores_ROUGE appos_scores_PER appos_scores_WER appos_scores_BLEU num_scores_23 poss_research_our
N07-1005	P02-1040	o	In recent years many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations -LRB- Niessen et al. 2000 Akiba et al. 2001 Papineni et al. 2002 NIST 2002 Leusch et al. 2003 Turian et al. 2003 Babych and Hartley 2004 Lin and Och 2004 Banerjee and Lavie 2005 Gimenez et al. 2005 -RRB- because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently	nn_systems_MT advmod_use_efficiently dobj_use_systems conj_and_use_improve aux_use_to xcomp_enable_improve xcomp_enable_use dobj_enable_us aux_enable_to xcomp_expected_enable auxpass_expected_is nsubjpass_expected_improving mark_expected_because nn_evaluation_MT amod_evaluation_automatic prep_of_performance_evaluation det_performance_the dobj_improving_performance num_Gimenez_2005 nn_Gimenez_al. nn_Gimenez_et conj_and_Banerjee_Lavie num_Lin_2004 conj_and_Lin_Och conj_and_Babych_2004 conj_and_Babych_Hartley num_Turian_2003 nn_Turian_al. nn_Turian_et num_Leusch_2003 nn_Leusch_al. nn_Leusch_et num_NIST_2002 num_Papineni_2002 nn_Papineni_al. nn_Papineni_et num_Akiba_2001 nn_Akiba_al. nn_Akiba_et dep_Niessen_Gimenez dep_Niessen_2005 dep_Niessen_Lavie dep_Niessen_Banerjee dep_Niessen_Och dep_Niessen_Lin dep_Niessen_2004 dep_Niessen_Hartley dep_Niessen_Babych dep_Niessen_Turian dep_Niessen_Leusch dep_Niessen_NIST dep_Niessen_Papineni dep_Niessen_Akiba dep_Niessen_2000 dep_Niessen_al. nn_Niessen_et nn_evaluations_MT amod_evaluations_automatic prep_of_performance_evaluations det_performance_the dobj_improve_performance prep_of_quality_MT det_quality_the conj_and_evaluate_improve dobj_evaluate_quality advmod_evaluate_automatically aux_evaluate_to advcl_tried_expected dep_tried_Niessen ccomp_tried_improve ccomp_tried_evaluate aux_tried_have nsubj_tried_researchers prep_in_tried_years amod_researchers_many amod_years_recent
N07-1006	P02-1040	o	2 Three New Features for MT Evaluation Since our source-sentence constrained n-gram precision and discriminative unigram precision are both derived from the normal n-gram precision it is worth describing the original n-gram precision metric BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni appos_precision_BLEU amod_precision_metric dep_n-gram_precision dep_original_n-gram amod_the_original dobj_describing_the xcomp_worth_describing cop_worth_is nsubj_worth_it nn_precision_n-gram amod_precision_normal det_precision_the prep_from_derived_precision vmod_both_derived nsubj_are_both nn_precision_unigram amod_precision_discriminative conj_and_precision_precision amod_precision_n-gram dobj_constrained_precision dobj_constrained_precision nsubj_constrained_source-sentence mark_constrained_Since poss_source-sentence_our nn_Evaluation_MT dep_Features_worth dep_Features_are dep_Features_constrained prep_for_Features_Evaluation nn_Features_New num_Features_Three number_Three_2 dep_``_Features
N07-1006	P02-1040	n	The most commonly used metric BLEU correlates well over large test sets with human judgments -LRB- Papineni et al. 2002 -RRB- but does not perform as well on sentence-level evaluation -LRB- Blatz et al. 2003 -RRB-	amod_Blatz_2003 dep_Blatz_al. nn_Blatz_et amod_evaluation_sentence-level conj_and_perform_evaluation neg_perform_not aux_perform_does nsubj_perform_BLEU amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_judgments_human prep_with_sets_judgments nn_sets_test amod_sets_large dep_correlates_Blatz conj_but_correlates_evaluation conj_but_correlates_perform dep_correlates_Papineni prep_over_correlates_sets advmod_correlates_well nsubj_correlates_BLEU ccomp_correlates_metric amod_metric_used det_metric_The advmod_used_commonly advmod_used_most
N07-1007	P02-1040	o	We also show that integrating our case prediction model improves the quality of translation according to BLEU -LRB- Papineni et al. 2002 -RRB- g2 and human evaluation	amod_evaluation_human conj_and_g2_evaluation dep_g2_Papineni nn_g2_BLEU dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_of_quality_translation det_quality_the pobj_improves_evaluation pobj_improves_g2 prepc_according_to_improves_to dobj_improves_quality csubj_improves_integrating mark_improves_that nn_model_prediction nn_model_case poss_model_our dobj_integrating_model ccomp_show_improves advmod_show_also nsubj_show_We
N07-1021	P02-1040	o	BLEU -LRB- Papineni et al. 2002 -RRB- is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams -LRB- n 4 has become standard -RRB- that it shares with several reference translations	nn_translations_reference amod_translations_several prep_with_shares_translations nsubj_shares_it mark_shares_that acomp_become_standard aux_become_has nsubj_become_n num_n_4 dep_n-grams_become nn_n-grams_word poss_n-grams_its ccomp_proportion_shares prep_of_proportion_n-grams det_proportion_the prep_of_terms_proportion det_translation_a prep_of_quality_translation det_quality_the prep_in_assesses_terms dobj_assesses_quality nsubj_assesses_that ccomp_metric_assesses nn_metric_precision det_metric_a cop_metric_is nsubj_metric_BLEU amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni
N07-1022	P02-1040	o	#Reference If our player 2 3 7 or 5 has the ball and the ball is close to our goal line PHARAOH + + If player 3 has the ball is in 2 5 the ball is in the area near our goal line WASP1 + + If players 2 3 7 and 5 has the ball and the ball is near our goal line Figure 4 Sample partial system output in the ROBOCUP domain ROBOCUP GEOQUERY BLEU NIST BLEU NIST PHARAOH 0.3247 5.0263 0.2070 3.1478 WASP1 0.4357 5.4486 0.4582 5.9900 PHARAOH + + 0.4336 5.9185 0.5354 6.3637 WASP1 + + 0.6022 6.8976 0.5370 6.4808 Table 1 Results of automatic evaluation bold type indicates the best performing system -LRB- or systems -RRB- for a given domain-metric pair -LRB- p < 0.05 -RRB- 5.1 Automatic Evaluation Weperformed4runsof10-foldcrossvalidation and measured the performance of the learned generators using the BLEU score -LRB- Papineni et al. 2002 -RRB- and the NIST score -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_score_Doddington nn_score_NIST det_score_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_score_score dep_score_Papineni nn_score_BLEU det_score_the dobj_using_score dobj_using_score amod_generators_learned det_generators_the vmod_performance_using prep_of_performance_generators det_performance_the dobj_measured_performance nsubj_measured_type nn_Weperformed4runsof10-foldcrossvalidation_Evaluation nn_Weperformed4runsof10-foldcrossvalidation_Automatic num_Weperformed4runsof10-foldcrossvalidation_5.1 num_<_0.05 dep_<_p dep_pair_Weperformed4runsof10-foldcrossvalidation dep_pair_< amod_pair_domain-metric amod_pair_given det_pair_a cc_systems_or prep_for_system_pair appos_system_systems amod_system_performing amod_system_best det_system_the conj_and_indicates_measured dobj_indicates_system nsubj_indicates_type amod_type_bold amod_evaluation_automatic parataxis_Results_measured parataxis_Results_indicates prep_of_Results_evaluation dep_Table_Results num_Table_1 num_Table_6.4808 num_Table_0.5370 num_Table_6.8976 number_6.8976_0.6022 dep_+_Table num_WASP1_6.3637 number_6.3637_0.5354 dep_6.3637_5.9185 cc_6.3637_+ number_5.9185_0.4336 conj_+_PHARAOH_+ conj_+_PHARAOH_WASP1 num_PHARAOH_5.9900 num_PHARAOH_5.4486 dep_PHARAOH_WASP1 num_PHARAOH_5.0263 nn_PHARAOH_PHARAOH nn_PHARAOH_NIST nn_PHARAOH_BLEU nn_PHARAOH_NIST nn_PHARAOH_BLEU nn_PHARAOH_GEOQUERY nn_PHARAOH_ROBOCUP nn_PHARAOH_domain nn_PHARAOH_ROBOCUP det_PHARAOH_the number_5.9900_0.4582 number_5.4486_0.4357 dep_WASP1_3.1478 number_3.1478_0.2070 number_5.0263_0.3247 nn_output_system amod_output_partial prep_in_Sample_+ prep_in_Sample_WASP1 prep_in_Sample_PHARAOH dobj_Sample_output num_Figure_4 nn_Figure_line nn_Figure_goal poss_Figure_our prep_near_is_Figure nsubj_is_ball det_ball_the det_ball_the dep_has_Sample conj_and_has_is dobj_has_ball nsubj_has_5 nsubj_has_players mark_has_If conj_and_players_5 num_players_7 num_players_3 num_players_2 conj_+_WASP1_+ dep_line_+ dep_line_WASP1 nn_line_goal poss_line_our prep_near_area_line det_area_the advcl_is_is advcl_is_has prep_in_is_area det_ball_the num_ball_5 number_5_2 dep_is_is prep_in_is_ball advcl_is_has det_ball_the dobj_has_ball nsubj_has_player mark_has_If num_player_3 conj_+_PHARAOH_+ dep_line_+ dep_line_PHARAOH nn_line_goal poss_line_our prep_to_close_line cop_close_is nsubj_close_ball det_ball_the det_ball_the dep_has_is conj_and_has_close dobj_has_ball nsubj_has_player mark_has_If conj_or_7_5 appos_player_5 appos_player_7 amod_player_3 num_player_2 poss_player_our dep_#Reference_close dep_#Reference_has
N07-1029	P02-1040	o	The NIST BLEU-4 is a variant of BLEU -LRB- Papineni et al. 2002 -RRB- and is computed as a49a51a50 a2a16a52a53a6 a0a9a8a10a0a12a11a54a13a55a15 a26a57a56a33a58a60a59 a43 a61a63a62 a64 a65a67a66a69a68 a28a71a70a46a72a74a73 a65 a6 a0a9a8a10a0a3a11a54a13a19a75a77a76 a6 a0a9a8a10a0a3a11a54a13 -LRB- 2 -RRB- where a73 a65 a6 a0a78a8a10a0a3a11a54a13 is the precision of a79 grams in the hypothesis a0 given the reference a0 a11 and a76 a6 a0a78a8a10a0a3a11a54a13a81a80 a43 is a brevity penalty	nn_penalty_brevity det_penalty_a cop_penalty_is nsubj_penalty_a0 nn_a43_a0a78a8a10a0a3a11a54a13a81a80 nn_a43_a6 nn_a43_a76 conj_and_a11_a43 nn_a11_a0 nn_a11_reference det_a11_the pobj_given_a43 pobj_given_a11 prep_a0_given rcmod_hypothesis_penalty det_hypothesis_the dep_a79_grams prep_in_precision_hypothesis prep_of_precision_a79 det_precision_the cop_precision_is nsubj_precision_a0a78a8a10a0a3a11a54a13 advmod_precision_where dep_precision_2 nn_a0a78a8a10a0a3a11a54a13_a6 nn_a0a78a8a10a0a3a11a54a13_a65 nn_a0a78a8a10a0a3a11a54a13_a73 dep_a0a9a8a10a0a3a11a54a13_precision nn_a0a9a8a10a0a3a11a54a13_a6 nn_a0a9a8a10a0a3a11a54a13_a0a9a8a10a0a3a11a54a13a19a75a77a76 nn_a0a9a8a10a0a3a11a54a13_a6 nn_a0a9a8a10a0a3a11a54a13_a65 nn_a0a9a8a10a0a3a11a54a13_a28a71a70a46a72a74a73 nn_a0a9a8a10a0a3a11a54a13_a65a67a66a69a68 nn_a0a9a8a10a0a3a11a54a13_a64 nn_a0a9a8a10a0a3a11a54a13_a61a63a62 nn_a0a9a8a10a0a3a11a54a13_a43 nn_a0a9a8a10a0a3a11a54a13_a26a57a56a33a58a60a59 nn_a0a9a8a10a0a3a11a54a13_a0a9a8a10a0a12a11a54a13a55a15 nn_a0a9a8a10a0a3a11a54a13_a2a16a52a53a6 nn_a0a9a8a10a0a3a11a54a13_a49a51a50 prep_as_computed_a0a9a8a10a0a3a11a54a13 auxpass_computed_is nsubjpass_computed_BLEU-4 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_variant_computed dep_variant_Papineni prep_of_variant_BLEU det_variant_a cop_variant_is nsubj_variant_BLEU-4 nn_BLEU-4_NIST det_BLEU-4_The
N07-1046	P02-1040	o	Therefore having correct transliterations would give only small improvements in terms of BLEU -LRB- Papineni et al. 2002 -RRB- and NIST scores	nn_scores_NIST amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_scores dep_BLEU_Papineni prep_of_terms_scores prep_of_terms_BLEU amod_improvements_small advmod_improvements_only prep_in_give_terms dobj_give_improvements aux_give_would csubj_give_having advmod_give_Therefore amod_transliterations_correct dobj_having_transliterations
N07-1061	P02-1040	o	To set the weights m we carried out minimum error rate training -LRB- Och 2003 -RRB- using BLEU -LRB- Papineni et al. 2002 -RRB- as the objective function	amod_function_objective det_function_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_as_using_function dobj_using_BLEU dep_Och_2003 vmod_training_using appos_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_carried_training prt_carried_out nsubj_carried_we tmod_carried_m advcl_carried_set det_weights_the dobj_set_weights aux_set_To
N07-1063	P02-1040	o	We present results in the form of search error analysis and translation quality as measured by the BLEU score -LRB- Papineni et al. 2002 -RRB- on the IWSLT 06 text translation task -LRB- Eck and Hori 2005 -RRB- 1 comparing Cube Pruning with our two-pass approach	amod_approach_two-pass poss_approach_our nn_Pruning_Cube prep_with_comparing_approach dobj_comparing_Pruning dep_Eck_2005 conj_and_Eck_Hori num_task_1 dep_task_Hori dep_task_Eck nn_task_translation nn_task_text num_task_06 nn_task_IWSLT det_task_the vmod_Papineni_comparing prep_on_Papineni_task amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the prep_by_measured_score mark_measured_as nn_quality_translation conj_and_analysis_quality nn_analysis_error nn_analysis_search prep_of_form_quality prep_of_form_analysis det_form_the prep_in_results_form dep_present_Papineni advcl_present_measured dobj_present_results nsubj_present_We ccomp_``_present
N07-2006	P02-1040	o	The baseline score using all phrase pairs was 59.11 -LRB- BLEU Papineni et al. 2002 -RRB- with a 95 % confidence interval of -LSB- 57.13 61.09 -RSB-	dep_57.13_61.09 dep_of_57.13 prep_interval_of nn_interval_confidence amod_interval_% det_interval_a number_%_95 nn_Papineni_al. nn_Papineni_et num_BLEU_2002 appos_BLEU_Papineni prep_with_59.11_interval dep_59.11_BLEU cop_59.11_was nsubj_59.11_score nn_pairs_phrase det_pairs_all dobj_using_pairs vmod_score_using nn_score_baseline det_score_The
N07-2013	P02-1040	o	BLEU score In order to measure the extent to which whole chunks of text from the prompt are reproduced in the student essays we used the BLEU score known from studies of machine translation -LRB- Papineni et al. 2002 -RRB-	nn_al._et dep_Papineni_2002 advmod_Papineni_al. nn_translation_machine prep_of_studies_translation prep_from_known_studies vmod_score_known nn_score_BLEU det_score_the dep_used_Papineni dobj_used_score nsubj_used_we nn_essays_student det_essays_the prep_in_reproduced_essays auxpass_reproduced_are nsubjpass_reproduced_chunks prep_to_reproduced_which det_prompt_the prep_from_chunks_prompt prep_of_chunks_text amod_chunks_whole rcmod_extent_reproduced det_extent_the parataxis_measure_used dobj_measure_extent aux_measure_to dep_measure_order mark_measure_In dep_score_measure nn_score_BLEU
N07-2037	P02-1040	o	We measure translation performance by the BLEU score -LRB- Papineni et al 2002 -RRB- with one reference for each hypothesis	det_hypothesis_each prep_for_reference_hypothesis num_reference_one amod_Papineni_2002 dep_Papineni_al nn_Papineni_et nn_score_BLEU det_score_the nn_performance_translation prep_with_measure_reference dep_measure_Papineni prep_by_measure_score dobj_measure_performance nsubj_measure_We
N09-1014	P02-1040	o	Here we compare two similarity measures the familiar BLEU score -LRB- Papineni et al. 2002 -RRB- and a score based on string kernels	nn_kernels_string prep_on_based_kernels vmod_score_based det_score_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_score_score dep_score_Papineni nn_score_BLEU amod_score_familiar det_score_the dep_measures_score dep_measures_score nn_measures_similarity num_measures_two dobj_compare_measures nsubj_compare_we advmod_compare_Here
N09-1027	P02-1040	o	Feature weights vector are trained discriminatively in concert with the language model weight to maximize the BLEU -LRB- Papineni et al. 2002 -RRB- automatic evaluation metric via Minimum Error Rate Training -LRB- MERT -RRB- -LRB- Och 2003 -RRB-	amod_Och_2003 dep_Training_Och appos_Training_MERT nn_Training_Rate nn_Training_Error nn_Training_Minimum prep_via_metric_Training nn_metric_evaluation amod_metric_automatic dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_BLEU_metric dep_BLEU_Papineni det_BLEU_the dobj_maximize_BLEU aux_maximize_to nn_weight_model nn_weight_language det_weight_the prep_with_concert_weight xcomp_trained_maximize prep_in_trained_concert advmod_trained_discriminatively auxpass_trained_are nsubjpass_trained_vector nn_vector_weights nn_vector_Feature
N09-1029	P02-1040	o	Our evaluation metric is BLEU -LRB- Papineni et al. 2002 -RRB- with caseinsensitive matching from unigram to four-gram	prep_to_matching_four-gram prep_from_matching_unigram amod_matching_caseinsensitive amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_with_BLEU_matching dep_BLEU_Papineni cop_BLEU_is nsubj_BLEU_metric nn_metric_evaluation poss_metric_Our
N09-1046	P02-1040	o	The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER -LRB- Papineni et al. 2002 Snover et al. 2006 -RRB- using the minimum error training algorithm on a packed forest representation of the decoders hypothesis space -LRB- Macherey et al. 2008 -RRB-	amod_Macherey_2008 dep_Macherey_al. nn_Macherey_et nn_space_hypothesis nn_space_decoders det_space_the prep_of_representation_space nn_representation_forest amod_representation_packed det_representation_a nn_algorithm_training nn_algorithm_error amod_algorithm_minimum det_algorithm_the dep_using_Macherey prep_on_using_representation dobj_using_algorithm num_Snover_2006 nn_Snover_al. nn_Snover_et dep_Papineni_Snover amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_1-TER prep_of_combination_1-TER prep_of_combination_BLEU amod_combination_linear amod_combination_weighted det_combination_an advmod_weighted_equally vmod_maximize_using dep_maximize_Papineni dobj_maximize_combination aux_maximize_to prepc_as_set_maximize advmod_set_so vmod_development_set amod_development_heldout det_development_a prep_on_tuned_development auxpass_tuned_were nsubjpass_tuned_weights nn_weights_feature det_weights_The ccomp_``_tuned
N09-1048	P02-1040	o	The results were evaluated using the character/pinyin-based 4-gram BLEU score -LRB- Papineni et al. 2002 -RRB- word error rate -LRB- WER -RRB- position independent word error rate -LRB- PER -RRB- and exact match -LRB- EMatch -RRB-	appos_match_EMatch amod_match_exact appos_rate_PER nn_rate_error nn_rate_word amod_rate_independent nn_rate_position conj_and_rate_match conj_and_rate_rate appos_rate_WER nn_rate_error nn_rate_word amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU amod_score_4-gram amod_score_character/pinyin-based det_score_the dep_using_match dep_using_rate dep_using_rate dep_using_Papineni dobj_using_score xcomp_evaluated_using auxpass_evaluated_were nsubjpass_evaluated_results det_results_The
N09-1058	P02-1040	o	The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU -LRB- Papineni et al. 2002 -RRB- NIST -LRB- Doddington 2002 -RRB- and METEOR -LRB- Banerjee and Lavie 2005 -RRB- scores	dep_scores_Lavie dep_scores_Banerjee nn_scores_METEOR dep_Banerjee_2005 conj_and_Banerjee_Lavie dep_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_scores conj_and_BLEU_NIST dep_BLEU_Papineni det_BLEU_the dobj_using_scores dobj_using_NIST dobj_using_BLEU num_sentences_3071 vmod_set_using prep_of_set_sentences nn_set_test amod_set_uncased det_set_a prep_on_evaluated_set auxpass_evaluated_is nsubjpass_evaluated_performance nn_performance_system nn_performance_SMT amod_performance_final det_performance_The ccomp_``_evaluated
N09-2001	P02-1040	o	Results are reported using lowercase BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_BLEU_lowercase dobj_using_BLEU xcomp_reported_using auxpass_reported_are nsubjpass_reported_Results
N09-2003	P02-1040	o	In this case one is often required to find the translation -LRB- s -RRB- in the hypergraph that are most similar to the desired translations with similarity computed via some automatic metric such as BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_such_as_metric_BLEU amod_metric_automatic det_metric_some prep_via_computed_metric amod_translations_desired det_translations_the prep_to_similar_translations advmod_similar_most cop_similar_are nsubj_similar_that rcmod_hypergraph_similar det_hypergraph_the prep_in_translation_hypergraph appos_translation_s det_translation_the dobj_find_translation aux_find_to dep_required_computed prep_with_required_similarity xcomp_required_find advmod_required_often auxpass_required_is nsubjpass_required_one prep_in_required_case det_case_this
N09-2006	P02-1040	n	Due to limited variations in the N-Best list the nature of ranking and more importantly the non-differentiable objective functions used for MT -LRB- such as BLEU -LRB- Papineni et al. 2002 -RRB- -RRB- one often found only local optimal solutions to with no clue to walk out of the riddles	det_riddles_the pobj_walk_riddles prepc_out_of_walk_of aux_walk_to vmod_clue_walk neg_clue_no pobj_with_clue ccomp_,_with dep_solutions_to amod_solutions_optimal amod_solutions_local advmod_local_only dep_found_solutions advmod_found_often vmod_one_found amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_such_as_MT_BLEU prep_for_used_MT appos_functions_one vmod_functions_used amod_functions_objective amod_functions_non-differentiable det_functions_the dep_,_functions advmod_importantly_more advmod_nature_importantly cc_nature_and prep_of_nature_ranking det_nature_the dep_,_nature nn_list_N-Best det_list_the prep_in_variations_list amod_variations_limited prep_due_to_``_variations
N09-2024	P02-1040	o	Including about 1.4 million sentence pairs extracted from the Gigaword data we obtain a statistically significant improvement from 42.3 to 45.6 in BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_improvement_significant det_improvement_a advmod_significant_statistically dep_obtain_Papineni prep_in_obtain_BLEU prep_to_obtain_45.6 prep_from_obtain_42.3 dobj_obtain_improvement nsubj_obtain_we vmod_obtain_Including nn_data_Gigaword det_data_the prep_from_extracted_data vmod_pairs_extracted nn_pairs_sentence num_pairs_million number_million_1.4 quantmod_million_about dobj_Including_pairs
N09-2038	P02-1040	o	Day 1 Day 2 No ASR adaptation 29.39 27.41 Unsupervised ASR adaptation 31.55 27.66 Supervised ASR adaptation 32.19 27.65 Table 2 Impact of ASR adaptation to SMT Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni nn_system_translation det_system_the prep_in_performance_BLEU prep_of_performance_system det_performance_the nn_adaptation_ASR prep_on_impact_performance prep_of_impact_adaptation det_impact_the dobj_shows_impact nsubj_shows_Impact num_Table_2 nn_Table_SMT nn_adaptation_ASR prep_to_Impact_Table prep_of_Impact_adaptation num_Table_2 num_Table_27.65 num_Table_32.19 dep_adaptation_Table nn_adaptation_ASR amod_adaptation_Supervised number_27.66_31.55 dep_adaptation_adaptation num_adaptation_27.66 nn_adaptation_ASR amod_adaptation_Unsupervised num_adaptation_27.41 number_27.41_29.39 dep_adaptation_adaptation nn_adaptation_ASR neg_adaptation_No num_adaptation_2 dep_Day_adaptation num_Day_1 dep_Day_shows dep_Day_Day
N09-2055	P02-1040	o	The automatic assessment of the translation quality has been carried out using the BiLingual Evaluation Understudy -LRB- BLEU -RRB- -LRB- Papineni et al. 2002 -RRB- and the Translation Error Rate -LRB- TER -RRB- -LRB- Snover et al. 2006 -RRB-	amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_Rate_Snover appos_Rate_TER nn_Rate_Error nn_Rate_Translation det_Rate_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_Understudy_Papineni appos_Understudy_BLEU nn_Understudy_Evaluation nn_Understudy_BiLingual det_Understudy_the dobj_using_Understudy conj_and_carried_Rate xcomp_carried_using prt_carried_out auxpass_carried_been aux_carried_has nsubjpass_carried_assessment nn_quality_translation det_quality_the prep_of_assessment_quality amod_assessment_automatic det_assessment_The
N09-2056	P02-1040	o	For the evaluation of translation quality we applied standard automatic evaluation metrics i.e. BLEU -LRB- Papineni et al. 2002 -RRB- and METEOR -LRB- Banerjee and Lavie 2005 -RRB-	amod_Banerjee_2005 conj_and_Banerjee_Lavie dep_METEOR_Lavie dep_METEOR_Banerjee amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_METEOR dep_BLEU_Papineni dep_i.e._METEOR dep_i.e._BLEU nn_metrics_evaluation amod_metrics_automatic amod_metrics_standard prep_applied_i.e. dobj_applied_metrics nsubj_applied_we prep_for_applied_evaluation nn_quality_translation prep_of_evaluation_quality det_evaluation_the
P02-1039	P02-1040	o	As an overall decoding performance measure we used the BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric det_BLEU_the dobj_used_BLEU nsubj_used_we prep_as_used_measure nn_measure_performance nn_measure_decoding amod_measure_overall det_measure_an
P03-1039	P02-1040	o	BLEU BLEU score which computes the ratio of n-gram for the translation results found in reference translations -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_translations_reference prep_in_found_translations dep_results_Papineni vmod_results_found nsubj_results_score det_translation_the prep_for_ratio_translation prep_of_ratio_n-gram det_ratio_the dobj_computes_ratio nsubj_computes_which rcmod_score_computes nn_score_BLEU dep_BLEU_results
P03-1040	P02-1040	o	Performance is also measured by the BLEU score -LRB- Papineni et al. 2002 -RRB- which measures similarity to the reference translation taken from the English side of the parallel corpus	amod_corpus_parallel det_corpus_the prep_of_side_corpus amod_side_English det_side_the prep_from_taken_side vmod_translation_taken nn_translation_reference det_translation_the prep_to_similarity_translation dobj_measures_similarity nsubj_measures_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dep_measured_measures dep_measured_Papineni agent_measured_score advmod_measured_also auxpass_measured_is nsubjpass_measured_Performance
P03-1057	P02-1040	o	Another current topic of machine translation is automatic evaluation of MT quality -LRB- Papineni et al. 2002 Yasuda et al. 2001 Akiba et al. 2001 -RRB-	num_Akiba_2001 nn_Akiba_al. nn_Akiba_et dep_Yasuda_Akiba num_Yasuda_2001 nn_Yasuda_al. nn_Yasuda_et dep_Papineni_Yasuda appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_quality_MT dep_evaluation_Papineni prep_of_evaluation_quality amod_evaluation_automatic cop_evaluation_is nsubj_evaluation_topic nn_translation_machine prep_of_topic_translation amod_topic_current det_topic_Another
P03-1057	P02-1040	o	3 Automatic Evaluation of MT Quality We utilize BLEU -LRB- Papineni et al. 2002 -RRB- for the automatic evaluation of MT quality in this paper	det_paper_this nn_quality_MT prep_in_evaluation_paper prep_of_evaluation_quality amod_evaluation_automatic det_evaluation_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_for_utilize_evaluation dobj_utilize_BLEU nsubj_utilize_We nn_Quality_MT rcmod_Evaluation_utilize prep_of_Evaluation_Quality nn_Evaluation_Automatic num_Evaluation_3 dep_``_Evaluation
P04-1027	P02-1040	o	From this point of view some of the measures used in the evaluation of Machine Translation systems such as BLEU -LRB- Papineni et al. 2002 -RRB- have been imported into the summarization task	nn_task_summarization det_task_the prep_into_imported_task auxpass_imported_been aux_imported_have nsubjpass_imported_some prep_from_imported_point amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_such_as_systems_BLEU nn_systems_Translation nn_systems_Machine prep_of_evaluation_systems det_evaluation_the prep_in_used_evaluation vmod_measures_used det_measures_the prep_of_some_measures prep_of_point_view det_point_this
P04-1063	P02-1040	o	Regressive FLM -LRB- rFLM -RRB- h -LRB- FLM -LRB- e j -RRB- -RRB- = w1 FLM -LRB- e j -RRB- + b Regressive ALM -LRB- rALM -RRB- h -LRB- ALM -LRB- e j -RRB- -RRB- = w1 ALM -LRB- e j -RRB- + b Notice that h -LRB- -RRB- here is supposed to relate FLM or ALM to some independent evaluation metric such as BLEU -LRB- Papineni et al. 2002 -RRB- not the log likelihood of a translation	det_translation_a prep_of_likelihood_translation nn_likelihood_log det_likelihood_the neg_likelihood_not dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_likelihood appos_BLEU_Papineni prep_such_as_metric_BLEU amod_evaluation_metric dep_independent_evaluation amod_some_independent conj_or_FLM_ALM prep_to_relate_some dobj_relate_ALM dobj_relate_FLM aux_relate_to xcomp_supposed_relate auxpass_supposed_is nsubjpass_supposed_here rcmod_h_supposed nn_Notice_b dep_e_j prep_that_ALM_h conj_+_ALM_Notice dep_ALM_e nn_ALM_w1 dobj_=_Notice dobj_=_ALM dep_e_j dep_ALM_e appos_h_ALM nn_h_ALM appos_ALM_rALM amod_ALM_Regressive nn_ALM_b dep_e_j dep_FLM_= conj_+_FLM_h appos_FLM_e nn_FLM_w1 dep_=_h dep_=_FLM dep_e_j amod_FLM_= dep_FLM_e nn_FLM_h nn_FLM_FLM appos_FLM_rFLM amod_FLM_Regressive
P04-1078	P02-1040	p	1 Introduction With the introduction of the BLEU metric for machine translation evaluation -LRB- Papineni et al 2002 -RRB- the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated they allow for faster implement-evaluate cycles -LRB- by by-passing the human evaluation bottleneck -RRB- less variation in evaluation performance due to errors in human assessor judgment and not least the possibility of hill-climbing on such metrics in order to improve system performance -LRB- Och 2003 -RRB-	num_Och_2003 appos_performance_Och nn_performance_system dobj_improve_performance aux_improve_to dep_improve_order mark_improve_in dep_metrics_improve amod_metrics_such prep_on_possibility_metrics prep_of_possibility_hill-climbing det_possibility_the neg_least_not nn_judgment_assessor amod_judgment_human prep_in_errors_judgment nn_performance_evaluation appos_variation_possibility conj_and_variation_least prep_due_to_variation_errors prep_in_variation_performance amod_variation_less appos_bottleneck_least appos_bottleneck_variation nn_bottleneck_evaluation amod_bottleneck_human det_bottleneck_the dobj_by-passing_bottleneck amod_cycles_implement-evaluate advmod_cycles_faster prepc_by_allow_by-passing prep_for_allow_cycles nsubj_allow_they advmod_appreciated_increasingly parataxis_become_allow acomp_become_appreciated aux_become_have nsubj_become_Introduction nn_applications_NLP amod_applications_various amod_evaluation_automatic prep_for_doing_applications dobj_doing_evaluation prepc_of_advantages_doing det_advantages_the amod_Papineni_2002 dep_Papineni_al nn_Papineni_et nn_evaluation_translation nn_evaluation_machine nn_metric_BLEU det_metric_the prep_for_introduction_evaluation prep_of_introduction_metric det_introduction_the appos_Introduction_advantages dep_Introduction_Papineni prep_with_Introduction_introduction num_Introduction_1 ccomp_``_become
P04-1078	P02-1040	o	For comparison purposes we also computed the value of R 2 for fluency using the BLEU score formula given in -LRB- Papineni et al. 2002 -RRB- for the 7 systems using the same one reference and we obtained a similar value 78.52 % computing the value of R 2 for fluency using the BLEU scores computed with all 4 references available yielded a lower value for R 2 64.96 % although BLEU scores obtained with multiple references are usually considered more reliable	advmod_reliable_more acomp_considered_reliable advmod_considered_usually auxpass_considered_are nsubjpass_considered_scores mark_considered_although amod_references_multiple prep_with_obtained_references vmod_scores_obtained nn_scores_BLEU num_%_64.96 num_R_2 appos_value_% prep_for_value_R amod_value_lower det_value_a advcl_yielded_considered dobj_yielded_value csubj_yielded_computing amod_references_available num_references_4 det_references_all prep_with_computed_references vmod_scores_computed nn_scores_BLEU det_scores_the dobj_using_scores num_R_2 prep_for_value_fluency prep_of_value_R det_value_the vmod_computing_using dobj_computing_value num_%_78.52 amod_value_similar det_value_a dobj_obtained_value nsubj_obtained_we num_reference_one amod_reference_same det_reference_the dobj_using_reference vmod_systems_using num_systems_7 det_systems_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_in_Papineni prep_given_in vmod_formula_given nn_formula_score nn_formula_BLEU det_formula_the conj_and_using_yielded conj_and_using_% conj_and_using_obtained prep_for_using_systems dobj_using_formula num_R_2 prep_for_value_fluency prep_of_value_R det_value_the dep_computed_yielded dep_computed_% dep_computed_obtained dep_computed_using dobj_computed_value advmod_computed_also nsubj_computed_we prep_for_computed_purposes nn_purposes_comparison
P04-1078	P02-1040	o	For comparison purposes we also computed the value of R 2 for adequacy using the BLEU score formula given in -LRB- Papineni et al. 2002 -RRB- for the 7 systems using the same one reference and we obtain a similar value 83.91 % computing the value of R 2 for adequacy using the BLEU scores computed with all 4 references available also yielded a lower value for R 2 62.21 %	num_%_62.21 num_R_2 prep_for_value_R amod_value_lower det_value_a dobj_yielded_value advmod_yielded_also nsubj_yielded_available nsubj_yielded_references mark_yielded_with num_references_4 det_references_all advcl_computed_yielded vmod_scores_computed nn_scores_BLEU det_scores_the dobj_using_scores num_R_2 prep_for_value_adequacy prep_of_value_R det_value_the npadvmod_computing_% vmod_computing_using dobj_computing_value num_%_83.91 amod_value_similar det_value_a dobj_obtain_value nsubj_obtain_we num_reference_one amod_reference_same det_reference_the dobj_using_reference vmod_systems_using num_systems_7 det_systems_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_in_Papineni prep_given_in vmod_formula_given nn_formula_score nn_formula_BLEU det_formula_the conj_and_using_computing conj_and_using_% conj_and_using_obtain prep_for_using_systems dobj_using_formula num_R_2 prep_for_value_adequacy prep_of_value_R det_value_the dep_computed_computing dep_computed_% dep_computed_obtain dep_computed_using dobj_computed_value advmod_computed_also nsubj_computed_we prep_for_computed_purposes nn_purposes_comparison
P04-1079	P02-1040	o	On the one hand using 1 human reference with uniform results is essential for our methodology since it means that there is no more trouble with Recall -LRB- Papineni et al. 2002:314 -RRB- a systems ability to avoid under-generation of N-grams can now be reliably measured	advmod_measured_reliably auxpass_measured_be advmod_measured_now aux_measured_can nsubjpass_measured_ability prep_of_under-generation_N-grams dobj_avoid_under-generation aux_avoid_to vmod_ability_avoid nn_ability_systems det_ability_a amod_Papineni_2002:314 dep_Papineni_al. nn_Papineni_et ccomp_Recall_measured dep_Recall_Papineni prepc_with_trouble_Recall advmod_trouble_more neg_trouble_no nsubj_is_trouble expl_is_there mark_is_that ccomp_means_is nsubj_means_it mark_means_since poss_methodology_our advcl_essential_means prep_for_essential_methodology cop_essential_is csubj_essential_using amod_results_uniform prep_with_reference_results amod_reference_human num_reference_1 dobj_using_reference prep_on_using_hand num_hand_one det_hand_the
P04-1079	P02-1040	o	A similar observation was made in -LRB- Papineni et al. 2002 313 -RRB-	dep_Papineni_313 appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_in_Papineni prep_made_in auxpass_made_was nsubjpass_made_observation amod_observation_similar det_observation_A
P04-1079	P02-1040	n	Automatic evaluation methods such as BLEU -LRB- Papineni et al. 2002 -RRB- RED -LRB- Akiba et al. 2001 -RRB- or the weighted N-gram model proposed here may be more consistent in judging quality as compared to human evaluators but human judgments remain the only criteria for metaevaluating the automatic methods	amod_methods_automatic det_methods_the dobj_metaevaluating_methods prepc_for_criteria_metaevaluating amod_criteria_only det_criteria_the xcomp_remain_criteria nsubj_remain_judgments amod_judgments_human amod_evaluators_human prep_to_compared_evaluators mark_compared_as advcl_judging_compared dobj_judging_quality conj_but_consistent_remain prepc_in_consistent_judging advmod_consistent_more cop_consistent_be aux_consistent_may nsubj_consistent_methods advmod_proposed_here vmod_model_proposed nn_model_N-gram amod_model_weighted det_model_the amod_Akiba_2001 dep_Akiba_al. nn_Akiba_et dep_RED_Akiba amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_or_BLEU_model conj_or_BLEU_RED dep_BLEU_Papineni prep_such_as_methods_model prep_such_as_methods_RED prep_such_as_methods_BLEU nn_methods_evaluation nn_methods_Automatic
P04-1079	P02-1040	o	Besides saving cost the ability to dependably work with a single human translation has an additional advantage it is now possible to create Recall-based evaluation measures for MT which has been problematic for evaluation with multiple reference translations since only one of the choices from the reference set is used in translation -LRB- Papineni et al. 2002:314 -RRB-	nn_al._et dep_Papineni_2002:314 advmod_Papineni_al. dep_used_Papineni prep_in_used_translation auxpass_used_is nsubjpass_used_one mark_used_since nn_set_reference det_set_the prep_from_choices_set det_choices_the prep_of_one_choices advmod_one_only nn_translations_reference amod_translations_multiple prep_with_evaluation_translations prep_for_problematic_evaluation cop_problematic_been aux_problematic_has nsubj_problematic_which rcmod_MT_problematic prep_for_measures_MT nn_measures_evaluation amod_measures_Recall-based dobj_create_measures aux_create_to advcl_possible_used xcomp_possible_create advmod_possible_now cop_possible_is nsubj_possible_it amod_advantage_additional det_advantage_an parataxis_has_possible dobj_has_advantage nsubj_has_ability prepc_besides_has_saving amod_translation_human amod_translation_single det_translation_a prep_with_work_translation advmod_work_dependably aux_work_to vmod_ability_work det_ability_the dobj_saving_cost
P04-1079	P02-1040	o	Some of them use human reference translations e.g. the BLEU method -LRB- Papineni et al. 2002 -RRB- which is based on comparison of N-gram models in MT output and in a set of human reference translations	nn_translations_reference amod_translations_human prep_of_set_translations det_set_a amod_output_MT nn_models_N-gram prep_in_comparison_output prep_of_comparison_models prep_in_based_set prep_on_based_comparison conj_and_based_based auxpass_based_is nsubjpass_based_which nsubjpass_based_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et rcmod_method_based rcmod_method_based dep_method_Papineni nn_method_BLEU det_method_the dep_e.g._method prep_translations_e.g. nn_translations_reference amod_translations_human dobj_use_translations nsubj_use_Some prep_of_Some_them
P05-1009	P02-1040	o	We evaluate accuracy performance using two automatic metrics an identity metric ID which measures the percent of sentences recreated exactly and BLEU -LRB- Papineni et al. 2002 -RRB- which gives the geometric average of the number of uni bi tri and four-grams recreated exactly	advmod_recreated_exactly prep_of_number_uni det_number_the prep_of_average_number amod_average_geometric det_average_the dobj_gives_average nsubj_gives_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et rcmod_BLEU_gives dep_BLEU_Papineni advmod_recreated_exactly vmod_percent_recreated prep_of_percent_sentences det_percent_the conj_and_measures_BLEU dobj_measures_percent nsubj_measures_which dep_metric_bi rcmod_metric_BLEU rcmod_metric_measures appos_metric_ID dep_identity_recreated conj_and_identity_four-grams conj_and_identity_tri dep_identity_metric dep_an_four-grams dep_an_tri dep_an_identity amod_metrics_automatic num_metrics_two dobj_using_metrics nn_performance_accuracy dep_evaluate_an xcomp_evaluate_using dobj_evaluate_performance nsubj_evaluate_We
P05-1018	P02-1040	o	Existing automatic evaluation measures such as BLEU -LRB- Papineni et al. 2002 -RRB- and ROUGE -LRB- Lin 2The collections are available from http://www.csail	prep_from_available_http://www.csail cop_available_are nsubj_available_measures nn_collections_2The nn_collections_Lin nn_collections_ROUGE amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_collections dep_BLEU_Papineni prep_such_as_measures_collections prep_such_as_measures_BLEU nn_measures_evaluation amod_measures_automatic amod_measures_Existing
P05-1032	P02-1040	o	We calculated the translation quality using Bleus modified n-gram precision metric -LRB- Papineni et al. 2002 -RRB- for n-grams of up to length four	num_length_four prep_to_up_length pobj_of_up prep_n-grams_of num_al._2002 nn_al._et amod_al._Papineni dep_precision_al. amod_precision_metric prep_for_n-gram_n-grams amod_n-gram_precision dobj_modified_n-gram vmod_Bleus_modified dobj_using_Bleus vmod_quality_using nn_quality_translation det_quality_the dobj_calculated_quality nsubj_calculated_We
P05-1032	P02-1040	o	They used the Bleu evaluation metric -LRB- Papineni et al. 2002 -RRB- but capped the n-gram precision at 4-grams	amod_precision_n-gram det_precision_the prep_at_capped_4-grams dobj_capped_precision nsubj_capped_They dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_evaluation_metric nn_evaluation_Bleu det_evaluation_the conj_but_used_capped dobj_used_evaluation nsubj_used_They
P05-1033	P02-1040	o	Our evaluation metric was BLEU -LRB- Papineni et al. 2002 -RRB- as calculated by the NIST script -LRB- version 11a -RRB- with its default settings which is to perform case-insensitive matching of n-grams up to n = 4 and to use the shortest -LRB- as opposed to nearest -RRB- reference sentence for the brevity penalty	nn_penalty_brevity det_penalty_the prep_for_sentence_penalty nn_sentence_reference dep_sentence_nearest prep_to_opposed_sentence prepc_as_shortest_opposed det_shortest_the dobj_use_shortest aux_use_to dep_=_4 amod_n_= pobj_to_n pcomp_up_to prep_of_matching_n-grams amod_matching_case-insensitive conj_and_perform_use prep_perform_up dobj_perform_matching aux_perform_to xcomp_is_use xcomp_is_perform nsubj_is_which rcmod_settings_is nn_settings_default poss_settings_its nn_11a_version prep_with_script_settings appos_script_11a nn_script_NIST det_script_the prep_by_calculated_script mark_calculated_as amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et advcl_BLEU_calculated dep_BLEU_Papineni cop_BLEU_was nsubj_BLEU_metric nn_metric_evaluation poss_metric_Our
P05-1048	P02-1040	o	Using our WSD model to constrain the translation candidates given to the decoder hurts translation quality as measured by the automated BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_BLEU_Papineni amod_BLEU_metric dep_automated_BLEU vmod_the_automated prep_by_measured_the mark_measured_as nn_quality_translation advcl_hurts_measured dobj_hurts_quality csubj_hurts_Using det_decoder_the prep_to_given_decoder vmod_candidates_given nn_candidates_translation det_candidates_the dobj_constrain_candidates aux_constrain_to nn_model_WSD poss_model_our vmod_Using_constrain dobj_Using_model ccomp_``_hurts
P05-1066	P02-1040	o	We use BLEU scores -LRB- Papineni et al. 2002 -RRB- to measure translation accuracy	nn_accuracy_translation dobj_measure_accuracy aux_measure_to amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_scores_BLEU vmod_use_measure dep_use_Papineni dobj_use_scores nsubj_use_We
P05-1067	P02-1040	o	Our MT system was evaluated using the n-gram based Bleu -LRB- Papineni et al. 2002 -RRB- and NIST machine translation evaluation software	nn_software_evaluation nn_software_translation nn_software_machine nn_software_NIST amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_Bleu_software dep_Bleu_Papineni pobj_based_software pobj_based_Bleu prep_n-gram_based det_n-gram_the dobj_using_n-gram xcomp_evaluated_using auxpass_evaluated_was nsubjpass_evaluated_system nn_system_MT poss_system_Our
P05-1069	P02-1040	o	Experimental results are reported in Table 2 here cased BLEU results are reported on MT03 Arabic-English test set -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_set_test amod_set_Arabic-English nn_set_MT03 prep_on_reported_set auxpass_reported_are nsubjpass_reported_results nn_results_BLEU dep_cased_Papineni ccomp_cased_reported advmod_cased_here num_Table_2 parataxis_reported_cased prep_in_reported_Table auxpass_reported_are nsubjpass_reported_results amod_results_Experimental
P05-1074	P02-1040	o	Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English and data created for machine translation evaluation methods such as Bleu -LRB- Papineni et al. 2002 -RRB- which use multiple reference translations	nn_translations_reference amod_translations_multiple dobj_use_translations nsubj_use_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et rcmod_Bleu_use dep_Bleu_Papineni prep_such_as_methods_Bleu nn_methods_evaluation nn_methods_translation nn_methods_machine prep_for_created_methods vmod_data_created amod_novels_French amod_novels_classical conj_and_translations_data prep_into_translations_English prep_of_translations_novels amod_translations_multiple cop_translations_are nsubj_translations_Examples auxpass_used_been aux_used_have nsubjpass_used_that amod_corpora_parallel amod_corpora_monolingual rcmod_Examples_used prep_of_Examples_corpora
P05-3026	P02-1040	n	METEOR was chosen since unlike the more commonly used BLEU metric -LRB- Papineni et al. 2002 -RRB- it provides reasonably reliable scores for individual sentences	amod_sentences_individual prep_for_scores_sentences amod_scores_reliable advmod_reliable_reasonably dobj_provides_scores nsubj_provides_it prep_unlike_provides_the mark_provides_since dep_al._2002 nn_al._et advmod_Papineni_al. amod_BLEU_Papineni amod_BLEU_metric dep_used_BLEU advmod_used_commonly advmod_commonly_more vmod_the_used advcl_chosen_provides auxpass_chosen_was nsubjpass_chosen_METEOR
P06-1002	P02-1040	o	Other metrics assess the impact of alignments externally e.g. different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics -LRB- e.g. BLEU -LRB- Papineni et al. 2002 -RRB- or METEOR -LRB- Banerjee and Lavie 2005 -RRB- -RRB-	dep_Banerjee_2005 conj_and_Banerjee_Lavie dep_METEOR_Lavie dep_METEOR_Banerjee dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_or_BLEU_METEOR dep_BLEU_Papineni dep_e.g._METEOR dep_e.g._BLEU ccomp_-LRB-_e.g. nn_metrics_evaluation amod_metrics_automated dobj_using_metrics nn_outputs_MT amod_outputs_corresponding det_outputs_the vmod_comparing_using dobj_comparing_outputs agent_tested_comparing auxpass_tested_are nsubjpass_tested_alignments advmod_tested_e.g. advmod_tested_externally amod_alignments_different vmod_alignments_tested prep_of_impact_alignments det_impact_the dobj_assess_impact nsubj_assess_metrics amod_metrics_Other ccomp_``_assess
P06-1002	P02-1040	o	MT output was evaluated using the standard evaluation metric BLEU -LRB- Papineni et al. 2002 -RRB- .2 The parameters of the MT System were optimized for BLEU metric on NIST MTEval2002 test sets using minimum error rate training -LRB- Och 2003 -RRB- and the systems were tested on NIST MTEval2003 test sets for both languages	det_languages_both prep_for_sets_languages nn_sets_test nn_sets_MTEval2003 nn_sets_NIST prep_on_tested_sets auxpass_tested_were nsubjpass_tested_systems det_systems_the dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training nn_sets_test nn_sets_MTEval2002 nn_sets_NIST prep_on_BLEU_sets amod_BLEU_metric conj_and_optimized_tested xcomp_optimized_using prep_for_optimized_BLEU auxpass_optimized_were nn_System_MT det_System_the prep_of_parameters_System det_parameters_The nn_parameters_.2 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_parameters dep_BLEU_Papineni amod_BLEU_metric nn_BLEU_evaluation amod_BLEU_standard det_BLEU_the dobj_using_BLEU dep_evaluated_tested dep_evaluated_optimized xcomp_evaluated_using auxpass_evaluated_was nsubjpass_evaluated_output amod_output_MT
P06-1011	P02-1040	o	Translation performance is measured using the automatic BLEU -LRB- Papineni et al. 2002 -RRB- metric on one reference translation	nn_translation_reference num_translation_one dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_BLEU_metric appos_BLEU_Papineni amod_BLEU_automatic det_BLEU_the dobj_using_BLEU prep_on_measured_translation xcomp_measured_using auxpass_measured_is nsubjpass_measured_performance nn_performance_Translation
P06-1067	P02-1040	o	This new model leads to significant improvements in MT quality as measured by BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_by_measured_BLEU mark_measured_as nn_quality_MT prep_in_improvements_quality amod_improvements_significant advcl_leads_measured prep_to_leads_improvements nsubj_leads_model amod_model_new det_model_This
P06-1077	P02-1040	o	We evaluated the translation quality using the BLEU metric -LRB- Papineni et al. 2002 -RRB- as calculated by mteval-v11b pl with its default setting except that we used case-sensitive matching of n-grams	prep_of_matching_n-grams amod_matching_case-sensitive dobj_used_matching nsubj_used_we mark_used_that nsubj_setting_default poss_default_its prepc_with_pl_setting prep_by_calculated_mteval-v11b mark_calculated_as dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni prepc_except_BLEU_used dep_BLEU_pl dep_BLEU_calculated amod_BLEU_metric dep_the_BLEU dobj_using_the nn_quality_translation det_quality_the xcomp_evaluated_using dobj_evaluated_quality nsubj_evaluated_We ccomp_``_evaluated
P06-1090	P02-1040	p	We report results using the well-known automatic evaluation metrics Bleu -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni dep_metrics_Bleu nn_metrics_evaluation amod_metrics_automatic amod_metrics_well-known det_metrics_the dobj_using_metrics vmod_results_using dobj_report_results nsubj_report_We
P06-1091	P02-1040	o	We show translation results in terms of the automatic BLEU evaluation metric -LRB- Papineni et al. 2002 -RRB- on the MT03 Arabic-English DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations	nn_translations_reference nn_translations_a95 amod_words_Arabic nn_words_a98a89a212a161a213a89a214a89a215 prep_with_sentences_words amod_sentences_a212a89a212a89a87 prep_with_consisting_translations prep_of_consisting_sentences xcomp_set_consisting vmod_test_set nn_test_evaluation nn_test_DARPA amod_test_Arabic-English nn_test_MT03 det_test_the num_al._2002 nn_al._et amod_al._Papineni dep_evaluation_al. amod_evaluation_metric prep_on_BLEU_test amod_BLEU_evaluation dep_automatic_BLEU amod_the_automatic prep_of_terms_the prep_in_results_terms nn_results_translation dobj_show_results nsubj_show_We
P06-1119	P02-1040	p	First we compared our system output to human reference translations using Bleu -LRB- Papineni et al. 2002 -RRB- a widelyaccepted objective metric for evaluation of machine translations	nn_translations_machine prep_of_evaluation_translations prep_for_metric_evaluation amod_metric_objective amod_metric_widelyaccepted det_metric_a nn_al._et num_Papineni_2002 appos_Papineni_al. appos_Bleu_metric appos_Bleu_Papineni dobj_using_Bleu vmod_translations_using nn_translations_reference amod_translations_human nn_output_system poss_output_our prep_to_compared_translations dobj_compared_output nsubj_compared_we advmod_compared_First
P06-1130	P02-1040	o	4.2 String-Based Evaluation We evaluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU -LRB- Papineni et al. 2002 -RRB- evaluation metrics	nn_metrics_evaluation dep_metrics_Papineni nn_metrics_BLEU dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_Accuracy_metrics nn_Accuracy_String nn_Accuracy_Simple det_Accuracy_the dobj_using_metrics dobj_using_Accuracy num_Section_23 prep_of_strings_Section amod_strings_raw det_strings_the nn_system_generation poss_system_our prep_of_output_system det_output_the xcomp_evaluate_using prep_against_evaluate_strings dobj_evaluate_output nsubj_evaluate_We rcmod_Evaluation_evaluate amod_Evaluation_String-Based num_Evaluation_4.2 dep_``_Evaluation
P06-1139	P02-1040	o	When evaluated against the state-of-the-art phrase-based decoder Pharaoh -LRB- Koehn 2004 -RRB- using the same experimental conditions translation table trained on the FBIS corpus -LRB- 7.2 M Chinese words and 9.2 M English words of parallel text -RRB- trigram language model trained on 155M words of English newswire interpolation weights a65 -LRB- Equation 2 -RRB- trained using discriminative training -LRB- Och 2003 -RRB- -LRB- on the 2002 NIST MT evaluation set -RRB- probabilistic beam a90 set to 0.01 histogram beam a58 set to 10 and BLEU -LRB- Papineni et al. 2002 -RRB- as our metric the WIDL-NGLM-Aa86 a129 algorithm produces translations that have a BLEU score of 0.2570 while Pharaoh translations have a BLEU score of 0.2635	prep_of_score_0.2635 nn_score_BLEU det_score_a dobj_have_score nsubj_have_translations mark_have_while nn_translations_Pharaoh prep_of_score_0.2570 nn_score_BLEU det_score_a dobj_have_score nsubj_have_that rcmod_translations_have advcl_produces_have dobj_produces_translations nsubj_produces_algorithm nn_algorithm_a129 nn_algorithm_WIDL-NGLM-Aa86 det_algorithm_the poss_metric_our amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_10_BLEU prep_to_set_BLEU prep_to_set_10 prep_as_a58_metric appos_a58_Papineni vmod_a58_set nn_a58_beam nn_a58_histogram prep_to_set_0.01 vmod_a90_set nn_a90_beam amod_a90_probabilistic nn_set_evaluation nn_set_MT nn_set_NIST num_set_2002 det_set_the pobj_on_set dep_Och_2003 appos_training_Och amod_training_discriminative dep_using_produces dobj_using_a58 conj_using_a90 dep_using_on dobj_using_training prep_trained_using num_Equation_2 vmod_a65_trained appos_a65_Equation nn_a65_weights nn_a65_interpolation nn_newswire_English prep_of_words_newswire nn_words_155M prep_on_trained_words dep_model_a65 vmod_model_trained nn_model_language nn_model_trigram amod_text_parallel prep_of_words_text nn_words_English dep_words_M num_M_9.2 conj_and_words_words amod_words_Chinese nn_words_M num_words_7.2 dep_corpus_words dep_corpus_words nn_corpus_FBIS det_corpus_the prep_on_trained_corpus vmod_table_trained nn_table_translation nn_table_conditions amod_table_experimental amod_table_same det_table_the parataxis_using_model dobj_using_table ccomp_,_using dep_Koehn_2004 dep_Pharaoh_Koehn nn_Pharaoh_decoder amod_Pharaoh_phrase-based amod_Pharaoh_state-of-the-art det_Pharaoh_the prep_against_evaluated_Pharaoh advmod_evaluated_When advcl_``_evaluated
P06-2005	P02-1040	o	For evaluation we use IBMs BLEU score -LRB- Papineni et al. 2002 -RRB- to measure the performance of the SMS normalization	nn_normalization_SMS det_normalization_the prep_of_performance_normalization det_performance_the dobj_measure_performance aux_measure_to amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni amod_score_BLEU nn_score_IBMs vmod_use_measure dobj_use_score nsubj_use_we prep_for_use_evaluation
P06-2005	P02-1040	o	We use IBMs BLEU score -LRB- Papineni et al. 2002 -RRB- to measure the performance of SMS text normalization	nn_normalization_text nn_normalization_SMS prep_of_performance_normalization det_performance_the dobj_measure_performance aux_measure_to amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni amod_score_BLEU nn_score_IBMs vmod_use_measure dobj_use_score nsubj_use_We
P06-2070	P02-1040	o	2 Recap of BLEU ROUGE-W and METEOR The most commonly used automatic evaluation metrics BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- are based on the assumption that The closer a machine translation is to a promt1 Life is like one nice chocolate in box ref Life is just like a box of tasty chocolate ref Life is just like a box of tasty chocolate mt2 Life is of one nice chocolate in box Figure 1 Alignment Example for ROUGE-W fessional human translation the better it is -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_is_Papineni dep_it_is dep_better_it det_better_the amod_translation_human amod_translation_fessional nn_translation_ROUGE-W appos_Example_better prep_for_Example_translation nn_Example_Alignment num_Figure_1 nn_Figure_box amod_chocolate_nice num_chocolate_one dep_is_Example prep_in_is_Figure prep_of_is_chocolate nsubj_is_Life nn_mt2_chocolate amod_mt2_tasty prep_of_box_mt2 det_box_a parataxis_is_is prep_like_is_box advmod_is_just nsubj_is_Life nn_ref_chocolate amod_ref_tasty prep_of_box_ref det_box_a parataxis_is_is prep_like_is_box advmod_is_just nsubj_is_Life nn_ref_box prep_in_chocolate_ref amod_chocolate_nice num_chocolate_one parataxis_is_is prep_like_is_chocolate nsubj_is_Life det_promt1_a prep_to_is_promt1 nsubj_is_translation mark_is_that nn_translation_machine det_translation_a amod_translation_closer det_translation_The ccomp_assumption_is det_assumption_the prep_on_based_assumption auxpass_based_are nsubjpass_based_metrics dep_Doddington_2002 appos_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni appos_metrics_NIST appos_metrics_BLEU nn_metrics_evaluation amod_metrics_automatic amod_metrics_used det_metrics_The advmod_used_commonly advmod_used_most conj_and_BLEU_METEOR conj_and_BLEU_ROUGE-W dep_Recap_is rcmod_Recap_based prep_of_Recap_METEOR prep_of_Recap_ROUGE-W prep_of_Recap_BLEU num_Recap_2 dep_``_Recap
P06-2070	P02-1040	p	BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities -LRB- Papineni et al. 2002 Doddington 2002 -RRB-	amod_Doddington_2002 dep_Papineni_Doddington dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_qualities_different prep_with_systems_qualities nn_systems_MT amod_systems_ranking prep_in_judgments_systems amod_judgments_human prep_with_correlate_judgments advmod_correlate_closely aux_correlate_to dep_shown_Papineni xcomp_shown_correlate auxpass_shown_been aux_shown_have nsubjpass_shown_NIST nsubjpass_shown_BLEU conj_and_BLEU_NIST
P06-2101	P02-1040	n	The ongoing evaluationliteratureisperhapsmostobviousinthe machine translation communitys efforts to better BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_BLEU_better prep_to_efforts_BLEU nn_efforts_communitys nn_efforts_translation nn_efforts_machine nn_efforts_evaluationliteratureisperhapsmostobviousinthe amod_efforts_ongoing det_efforts_The dep_``_efforts
P06-2103	P02-1040	p	One of the most successful metrics for judging machine-generated text is BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni cop_BLEU_is nsubj_BLEU_One amod_text_machine-generated amod_text_judging prep_for_metrics_text amod_metrics_successful det_metrics_the advmod_successful_most prep_of_One_metrics ccomp_``_BLEU
P06-2109	P02-1040	o	4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F-measures bigram F-measures and BLEU scores -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_scores_Papineni nn_scores_BLEU nn_F-measures_bigram conj_and_F-measures_scores conj_and_F-measures_F-measures nn_F-measures_word dobj_using_scores dobj_using_F-measures dobj_using_F-measures nn_method_compression nn_method_sentence det_method_each xcomp_evaluated_using dobj_evaluated_method nsubj_evaluated_We rcmod_Method_evaluated nn_Method_Evaluation num_Method_4.1 nn_Method_Experiment num_Method_4 dep_``_Method
P06-2124	P02-1040	o	For word alignment accuracy F-measure is reported i.e. the harmonic mean of precision and recall against a gold-standard reference set for translation quality Bleu -LRB- Papineni et al. 2002 -RRB- and its variation of NIST scores are reported	auxpass_reported_are nsubjpass_reported_variation nsubjpass_reported_Bleu prep_for_reported_quality dep_reported_recall dep_reported_mean advmod_reported_i.e. nn_scores_NIST prep_of_variation_scores poss_variation_its amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_Bleu_variation dep_Bleu_Papineni nn_quality_translation nn_set_reference amod_set_gold-standard det_set_a prep_against_recall_set conj_and_mean_recall prep_of_mean_precision amod_mean_harmonic det_mean_the dep_reported_reported auxpass_reported_is nsubjpass_reported_F-measure prep_for_reported_accuracy nn_accuracy_alignment nn_accuracy_word
P07-1001	P02-1040	o	We measure translation performance by the BLEU score -LRB- Papineni et al. 2002 -RRB- and Translation Error Rate -LRB- TER -RRB- -LRB- Snover et al. 2006 -RRB- with one reference for each hypothesis	det_hypothesis_each prep_for_reference_hypothesis num_reference_one amod_Snover_2006 dep_Snover_al. nn_Snover_et appos_Rate_TER nn_Rate_Error nn_Rate_Translation amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the nn_performance_translation prep_with_measure_reference dep_measure_Snover conj_and_measure_Rate dep_measure_Papineni prep_by_measure_score dobj_measure_performance nsubj_measure_We
P07-1004	P02-1040	o	Evaluation Metrics We evaluated the generated translations using three different evaluation metrics BLEU score -LRB- Papineni et al. 2002 -RRB- mWER -LRB- multi-reference word error rate -RRB- and mPER -LRB- multi-reference positionindependent word error rate -RRB- -LRB- Nieen et al. 2000 -RRB-	amod_Nieen_2000 dep_Nieen_al. nn_Nieen_et nn_rate_error nn_rate_word amod_rate_positionindependent amod_rate_multi-reference appos_mPER_rate nn_rate_error nn_rate_word amod_rate_multi-reference appos_mWER_rate amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Nieen conj_and_score_mPER conj_and_score_mWER dep_score_Papineni nn_score_BLEU nn_metrics_evaluation amod_metrics_different num_metrics_three dobj_using_metrics vmod_translations_using amod_translations_generated det_translations_the dobj_evaluated_translations nsubj_evaluated_We dep_Metrics_mPER dep_Metrics_mWER dep_Metrics_score rcmod_Metrics_evaluated nn_Metrics_Evaluation
P07-1005	P02-1040	o	Following -LRB- Chiang 2005 -RRB- we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores -LRB- Papineni et al. 2002 -RRB- based on case-insensitive ngram matching where n is up to 4	prep_to_up_4 advmod_is_up nsubj_is_n advmod_is_where rcmod_matching_is nn_matching_ngram amod_matching_case-insensitive amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_scores_BLEU det_scores_the dobj_calculate_scores aux_calculate_to nn_settings_default poss_settings_its nn_script_BLEU nn_script_NIST nn_script_11a nn_script_version det_script_the prep_based_on_used_matching dep_used_Papineni vmod_used_calculate prep_with_used_settings dobj_used_script nsubj_used_we vmod_used_Following amod_Chiang_2005 dep_Following_Chiang
P07-1038	P02-1040	p	The well-known BLEU -LRB- Papineni et al. 2002 -RRB- is based on the number of common n-grams between the translation hypothesis and human reference translations of the same sentence	amod_sentence_same det_sentence_the nn_translations_reference amod_translations_human prep_of_hypothesis_sentence conj_and_hypothesis_translations nn_hypothesis_translation det_hypothesis_the amod_n-grams_common prep_between_number_translations prep_between_number_hypothesis prep_of_number_n-grams det_number_the prep_on_based_number auxpass_based_is nsubjpass_based_BLEU amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_BLEU_well-known det_BLEU_The
P07-1038	P02-1040	o	Reference-based metrics such as BLEU -LRB- Papineni et al. 2002 -RRB- have rephrased this subjective task as a somewhat more objective question how closely does the translation resemble sentences that are known to be good translations for the same source ?	amod_source_same det_source_the prep_for_translations_source amod_translations_good cop_translations_be aux_translations_to xcomp_known_translations auxpass_known_are nsubjpass_known_that rcmod_sentences_known dobj_resemble_sentences nsubj_resemble_translation aux_resemble_does advmod_resemble_closely det_translation_the advmod_closely_how amod_question_objective det_question_a advmod_objective_more advmod_more_somewhat amod_task_subjective det_task_this dep_rephrased_resemble prep_as_rephrased_question dobj_rephrased_task aux_rephrased_have nsubj_rephrased_metrics amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_such_as_metrics_BLEU amod_metrics_Reference-based
P07-1039	P02-1040	o	The quality of the translation output is evaluated using BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dobj_using_BLEU xcomp_evaluated_using auxpass_evaluated_is nsubjpass_evaluated_quality nn_output_translation det_output_the prep_of_quality_output det_quality_The ccomp_``_evaluated
P07-1040	P02-1040	p	2 Evaluation Metrics Currently the most widely used automatic MT evaluation metric is the NIST BLEU-4 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU-4_Papineni nn_BLEU-4_NIST det_BLEU-4_the cop_BLEU-4_is nsubj_BLEU-4_metric advmod_BLEU-4_Currently dep_BLEU-4_Metrics nn_metric_evaluation nn_metric_MT amod_metric_automatic amod_metric_used det_metric_the advmod_used_widely advmod_widely_most nn_Metrics_Evaluation num_Metrics_2
P07-1044	P02-1040	o	BLEU -LRB- Papineni et al. 2002 -RRB- is a canonical example in matching n-grams in a candidate translation text with those in a reference text the metric measures faithfulness by counting the matches and fluency by implicitly using the reference n-grams as a language model	nn_model_language det_model_a nn_n-grams_reference det_n-grams_the prep_as_using_model dobj_using_n-grams advmod_using_implicitly prepc_by_fluency_using det_matches_the dobj_counting_matches prepc_by_faithfulness_counting conj_and_measures_fluency dep_measures_faithfulness amod_measures_metric det_measures_the nn_text_reference det_text_a prep_in_those_text prep_with_text_those nn_text_translation nn_text_candidate det_text_a prep_in_matching_text dobj_matching_n-grams dep_example_fluency dep_example_measures prepc_in_example_matching amod_example_canonical det_example_a cop_example_is nsubj_example_BLEU amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni
P07-1066	P02-1040	o	During evaluation two performance metrics BLEU -LRB- Papineni et al. 2002 -RRB- and NIST were computed	auxpass_computed_were nsubjpass_computed_NIST nsubjpass_computed_BLEU prep_during_computed_metrics amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni nn_metrics_performance num_metrics_two nn_metrics_evaluation
P07-1089	P02-1040	o	Our evaluation metric is BLEU-4 -LRB- Papineni et al. 2002 -RRB- as calculated by the script mteval-v11b pl with its default setting except that we used case-sensitive matching of n-grams	prep_of_matching_n-grams amod_matching_case-sensitive dobj_used_matching nsubj_used_we mark_used_that nsubj_setting_default poss_default_its prepc_except_pl_used prepc_with_pl_setting nn_mteval-v11b_script det_mteval-v11b_the prep_by_calculated_mteval-v11b mark_calculated_as amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU-4_pl dep_BLEU-4_calculated dep_BLEU-4_Papineni cop_BLEU-4_is nsubj_BLEU-4_metric nn_metric_evaluation poss_metric_Our
P07-1091	P02-1040	o	-LRB- Case-sensitive -RRB- BLEU-4 -LRB- Papineni et al. 2002 -RRB- is used as the evaluation metric	nn_metric_evaluation det_metric_the prep_as_used_metric auxpass_used_is nsubjpass_used_BLEU-4 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU-4_Papineni dep_BLEU-4_Case-sensitive
P07-1092	P02-1040	o	The parameters j were trained using minimum error rate training -LRB- Och 2003 -RRB- to maximise the BLEU score -LRB- Papineni et al. 2002 -RRB- on a 150 sentence development set	nn_set_development nn_set_sentence num_set_150 det_set_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dobj_maximise_score aux_maximise_to dep_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_using_maximise dobj_using_training prep_on_trained_set dep_trained_Papineni xcomp_trained_using auxpass_trained_were nsubjpass_trained_parameters appos_parameters_j det_parameters_The
P07-1108	P02-1040	o	Using BLEU -LRB- Papineni et al. 2002 -RRB- as a metric our method achieves an absolute improvement of 0.06 -LRB- 22.13 % relative -RRB- as compared with the standard model trained with 5,000 L f L e sentence pairs for French-Spanish translation	amod_translation_French-Spanish prep_for_pairs_translation nn_pairs_sentence dep_pairs_e nn_pairs_L dep_L_pairs dep_L_f num_L_5,000 prep_with_trained_L vmod_model_trained amod_model_standard det_model_the prep_with_compared_model mark_compared_as amod_%_relative num_%_22.13 dep_0.06_% prep_of_improvement_0.06 amod_improvement_absolute det_improvement_an advcl_achieves_compared dobj_achieves_improvement nsubj_achieves_method advmod_achieves_metric nsubj_achieves_a mark_achieves_as poss_method_our amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni advcl_Using_achieves dobj_Using_BLEU ccomp_``_Using
P07-1108	P02-1040	p	The translation quality was evaluated using a well-established automatic measure BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni nn_score_BLEU amod_measure_automatic amod_measure_well-established det_measure_a dobj_using_measure dep_evaluated_score xcomp_evaluated_using auxpass_evaluated_was nsubjpass_evaluated_quality nn_quality_translation det_quality_The rcmod_``_evaluated
P07-1111	P02-1040	o	Since the introduction of BLEU -LRB- Papineni et al. 2002 -RRB- the basic n-gram precision idea has been augmented in a number of ways	prep_of_number_ways det_number_a prep_in_augmented_number auxpass_augmented_been aux_augmented_has nsubjpass_augmented_idea nsubjpass_augmented_introduction mark_augmented_Since nn_idea_precision nn_idea_n-gram amod_idea_basic det_idea_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_of_introduction_BLEU det_introduction_the advcl_``_augmented
P07-2026	P02-1040	o	3.3 BLEU Score The BLEU score -LRB- Papineni et al. 2002 -RRB- measures the agreement between a hypothesiseI1 generated by the MT system and a reference translation eI1	nn_eI1_translation nn_eI1_reference det_eI1_a conj_and_system_eI1 nn_system_MT det_system_the agent_generated_eI1 agent_generated_system vmod_hypothesiseI1_generated det_hypothesiseI1_a prep_between_agreement_hypothesiseI1 det_agreement_the dobj_measures_agreement nsubj_measures_score amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni nn_score_BLEU det_score_The rcmod_Score_measures nn_Score_BLEU num_Score_3.3
P07-2045	P02-1040	o	It also contains tools for tuning these models using minimum error rate training -LRB- Och 2003 -RRB- and evaluating the resulting translations using the BLEU score -LRB- Papineni et al. 2002 -RRB-	dep_2002_al. nn_al._et num_Papineni_2002 nn_score_BLEU det_score_the dobj_using_score amod_translations_resulting det_translations_the vmod_evaluating_using dobj_evaluating_translations nsubj_evaluating_It num_Och_2003 appos_training_Och nn_training_rate nn_training_error amod_training_minimum dobj_using_training vmod_models_using det_models_these prep_for_tools_tuning dep_contains_Papineni conj_and_contains_evaluating dobj_contains_models dobj_contains_tools advmod_contains_also nsubj_contains_It
P08-1007	P02-1040	o	2.1 BLEU BLEU -LRB- Papineni et al. 2002 -RRB- is essentially a precision-based metric and is currently the standard metric for automatic evaluation of MT performance	nn_performance_MT prep_of_evaluation_performance amod_evaluation_automatic prep_for_metric_evaluation amod_metric_standard det_metric_the advmod_metric_currently cop_metric_is nsubj_metric_BLEU conj_and_metric_metric amod_metric_precision-based det_metric_a advmod_metric_essentially cop_metric_is nsubj_metric_BLEU amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni nn_BLEU_BLEU num_BLEU_2.1 ccomp_``_metric ccomp_``_metric
P08-1007	P02-1040	p	Among all the automatic MT evaluation metrics BLEU -LRB- Papineni et al. 2002 -RRB- is the most widely used	advmod_used_widely det_used_the auxpass_used_is nsubjpass_used_BLEU prep_among_used_metrics advmod_widely_most amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni nn_metrics_evaluation nn_metrics_MT amod_metrics_automatic det_metrics_the predet_metrics_all
P08-1009	P02-1040	o	4.2 Automatic Evaluation We first present our soft cohesion constraints effect on BLEU score -LRB- Papineni et al. 2002 -RRB- for both our dev-test and test sets	nn_sets_test conj_and_dev-test_sets poss_dev-test_our preconj_dev-test_both amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU prep_on_effect_score nn_effect_constraints nn_effect_cohesion amod_effect_soft poss_effect_our amod_effect_present advmod_present_first prep_for_We_sets prep_for_We_dev-test appos_We_Papineni dep_We_effect npadvmod_Evaluation_We nn_Evaluation_Automatic num_Evaluation_4.2 dep_``_Evaluation
P08-1010	P02-1040	o	We measure translation performance by the BLEU -LRB- Papineni et al. 2002 -RRB- and METEOR -LRB- Banerjee and Lavie 2005 -RRB- scores with multiple translation references	nn_references_translation amod_references_multiple prep_with_scores_references dep_scores_Lavie dep_scores_Banerjee nn_scores_METEOR dep_Banerjee_2005 conj_and_Banerjee_Lavie amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_scores dep_BLEU_Papineni det_BLEU_the nn_performance_translation prep_by_measure_scores prep_by_measure_BLEU dobj_measure_performance nsubj_measure_We
P08-1010	P02-1040	o	Other possibilities for the weighting include assigning constant one or the exponential of the final score etc. One of the advantages of the proposed phrase training algorithm is that it is a parameterized procedure that can be optimized jointly with the trans82 lation engine to minimize the final translation errors measured by automatic metrics such as BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_such_as_metrics_BLEU amod_metrics_automatic agent_measured_metrics vmod_errors_measured nn_errors_translation amod_errors_final det_errors_the dobj_minimize_errors aux_minimize_to nn_engine_lation nn_engine_trans82 det_engine_the xcomp_optimized_minimize prep_with_optimized_engine advmod_optimized_jointly auxpass_optimized_be aux_optimized_can nsubjpass_optimized_that rcmod_procedure_optimized amod_procedure_parameterized det_procedure_a cop_procedure_is nsubj_procedure_it mark_procedure_that ccomp_is_procedure nsubj_is_One nn_algorithm_training nn_algorithm_phrase amod_algorithm_proposed det_algorithm_the prep_of_advantages_algorithm det_advantages_the prep_of_One_advantages nn_etc._score amod_etc._final det_etc._the prep_of_exponential_etc. det_exponential_the conj_or_one_exponential amod_one_constant dobj_assigning_exponential dobj_assigning_one parataxis_include_is xcomp_include_assigning nsubj_include_possibilities det_weighting_the prep_for_possibilities_weighting amod_possibilities_Other
P08-1011	P02-1040	o	In addition to precision and recall we also evaluate the Bleu score -LRB- Papineni et al. 2002 -RRB- changes before and after applying our measure word generation method to the SMT output	nn_output_SMT det_output_the nn_method_generation nn_method_word nn_method_measure poss_method_our prep_to_applying_output dobj_applying_method pcomp_after_applying conj_and_before_after dep_changes_Papineni nn_changes_score dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_Bleu det_score_the prep_evaluate_after prep_evaluate_before dobj_evaluate_changes advmod_evaluate_also nsubj_evaluate_we prep_in_addition_to_evaluate_recall prep_in_addition_to_evaluate_precision conj_and_precision_recall
P08-1022	P02-1040	o	Moreover the overall BLEU -LRB- Papineni et al. 2002 -RRB- and METEOR -LRB- Lavie and Agarwal 2007 -RRB- scores as well as numbers of exact string matches -LRB- as measured against to the original sentences in the CCGbank -RRB- are higher for the hypertagger-seeded realizer than for the preexisting realizer	amod_realizer_preexisting det_realizer_the pobj_for_realizer pcomp_than_for amod_realizer_hypertagger-seeded det_realizer_the prep_higher_than prep_for_higher_realizer cop_higher_are nsubj_higher_numbers nsubj_higher_scores nsubj_higher_BLEU advmod_higher_Moreover det_CCGbank_the prep_in_sentences_CCGbank amod_sentences_original det_sentences_the pobj_to_sentences pcomp_against_to prep_measured_against mark_measured_as nn_matches_string amod_matches_exact dep_numbers_measured prep_of_numbers_matches dep_scores_Agarwal dep_scores_Lavie nn_scores_METEOR dep_Lavie_2007 conj_and_Lavie_Agarwal amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_numbers conj_and_BLEU_scores dep_BLEU_Papineni amod_BLEU_overall det_BLEU_the
P08-1064	P02-1040	o	The evaluation metric is case-sensitive BLEU-4 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU-4_Papineni amod_BLEU-4_case-sensitive cop_BLEU-4_is nsubj_BLEU-4_metric nn_metric_evaluation det_metric_The
P08-1071	P02-1040	o	For example in machine translation BLEU score -LRB- Papineni et al. 2002 -RRB- is developed to assess the quality of machine translated sentences	amod_sentences_translated nn_sentences_machine prep_of_quality_sentences det_quality_the dobj_assess_quality aux_assess_to xcomp_developed_assess auxpass_developed_is nsubjpass_developed_score prep_in_developed_translation prep_for_developed_example amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_score_Papineni nn_score_BLEU nn_translation_machine
P08-1086	P02-1040	o	Instead we report BLEU scores -LRB- Papineni et al. 2002 -RRB- of the machine translation system using different combinations of wordand classbased models for translation tasks from English to Arabic and Arabic to English	conj_and_Arabic_Arabic prep_to_tasks_English prep_to_tasks_Arabic prep_to_tasks_Arabic prep_from_tasks_English nn_tasks_translation amod_models_classbased nn_models_wordand prep_of_combinations_models amod_combinations_different prep_for_using_tasks dobj_using_combinations nn_system_translation nn_system_machine det_system_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et vmod_scores_using prep_of_scores_system dep_scores_Papineni nn_scores_BLEU dobj_report_scores nsubj_report_we advmod_report_Instead
P08-1112	P02-1040	o	Unfortunately as was shown by Fraser and Marcu -LRB- 2007 -RRB- AER can have weak correlation with translation performance as measured by BLEU score -LRB- Papineni et al. 2002 -RRB- when the alignments are used to train a phrase-based translation system	nn_system_translation amod_system_phrase-based det_system_a dobj_train_system aux_train_to xcomp_used_train auxpass_used_are nsubjpass_used_alignments advmod_used_when det_alignments_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU prep_by_measured_score mark_measured_as nn_performance_translation prep_with_correlation_performance amod_correlation_weak advcl_have_used dep_have_Papineni advcl_have_measured dobj_have_correlation aux_have_can csubj_have_shown advmod_have_Unfortunately nn_AER_Marcu nn_AER_Fraser appos_Marcu_2007 conj_and_Fraser_Marcu agent_shown_AER auxpass_shown_was mark_shown_as
P08-2015	P02-1040	o	We use the standard NIST MTEval data sets for the years 2003 2004 and 2005 -LRB- henceforth MT03 MT04 and MT05 respectively -RRB- .6 We report results in terms of case-insensitive 4gram BLEU -LRB- Papineni et al. 2002 -RRB- scores	dep_scores_Papineni dep_scores_results dep_scores_report dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_BLEU_4gram amod_BLEU_case-insensitive prep_of_terms_BLEU prep_in_results_terms nsubj_report_We dep_report_.6 dep_report_2005 dep_report_2004 advmod_MT03_respectively conj_and_MT03_MT05 conj_and_MT03_MT04 nn_MT03_henceforth dep_2004_MT05 dep_2004_MT04 dep_2004_MT03 conj_and_2004_2005 num_years_2003 det_years_the nn_sets_data nn_sets_MTEval nn_sets_NIST amod_sets_standard det_sets_the dobj_use_scores prep_for_use_years dobj_use_sets nsubj_use_We
P08-2020	P02-1040	o	1.2 Evaluation In this paper we report results using the BLEU metric -LRB- Papineni et al. 2002 -RRB- however as the evaluation criterion in GALE is HTER -LRB- Snover et al. 2006 -RRB- we also report in TER -LRB- Snover et al. 2005 -RRB-	amod_Snover_2005 dep_Snover_al. nn_Snover_et prep_in_report_TER advmod_report_also nsubj_report_we amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_HTER_Snover rcmod_HTER_report dep_HTER_Snover cop_HTER_is nsubj_HTER_criterion mark_HTER_as advmod_HTER_however dep_HTER_BLEU dep_HTER_the prep_in_criterion_GALE nn_criterion_evaluation det_criterion_the dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric dobj_using_HTER vmod_results_using dobj_report_results nsubj_report_we dep_report_Evaluation det_paper_this prep_in_Evaluation_paper num_Evaluation_1.2
P08-2040	P02-1040	o	Our evaluation metric is BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni cop_BLEU_is nsubj_BLEU_metric nn_metric_evaluation poss_metric_Our
P09-1018	P02-1040	o	In this paper we modify the method in Albrecht and Hwa -LRB- 2007 -RRB- to only prepare human reference translations for the training examples and then evaluate the translations produced by the subject systems against the references using BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU dobj_using_score vmod_references_using det_references_the prep_against_systems_references amod_systems_subject det_systems_the agent_produced_systems vmod_translations_produced det_translations_the dobj_evaluate_translations advmod_evaluate_then nn_examples_training det_examples_the nn_translations_reference amod_translations_human dep_prepare_Papineni conj_and_prepare_evaluate prep_for_prepare_examples dobj_prepare_translations advmod_prepare_only aux_prepare_to appos_Hwa_2007 conj_and_Albrecht_Hwa prep_in_method_Hwa prep_in_method_Albrecht det_method_the vmod_modify_evaluate vmod_modify_prepare dobj_modify_method nsubj_modify_we prep_in_modify_paper det_paper_this
P09-1020	P02-1040	o	Our evaluation metrics is casesensitive BLEU-4 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU-4_Papineni amod_BLEU-4_casesensitive cop_BLEU-4_is nsubj_BLEU-4_metrics nn_metrics_evaluation poss_metrics_Our
P09-1021	P02-1040	o	Afterwards we select and remove a subset of highly informative sentences from U and add those sentences together with their human-provided translations to L This process is continued iteratively until a certain level of translation quality is met -LRB- we use the BLEU score WER and PER -RRB- -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_score_PER conj_and_score_WER nn_score_BLEU det_score_the dobj_use_PER dobj_use_WER dobj_use_score nsubj_use_we auxpass_met_is nsubjpass_met_level mark_met_until nn_quality_translation prep_of_level_quality amod_level_certain det_level_a dep_continued_Papineni dep_continued_use advcl_continued_met advmod_continued_iteratively auxpass_continued_is nsubjpass_continued_process det_process_This prep_to_translations_L amod_translations_human-provided poss_translations_their det_sentences_those prep_together_with_add_translations dobj_add_sentences nsubj_add_we prep_from_sentences_U amod_sentences_informative advmod_informative_highly prep_of_subset_sentences det_subset_a nsubj_remove_we parataxis_select_continued conj_and_select_add dobj_select_subset conj_and_select_remove nsubj_select_we advmod_select_Afterwards
P09-1034	P02-1040	p	Since human evaluation is costly and difficult to do reliably a major focus of research has been on automatic measures of MT quality pioneered by BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni prep_by_pioneered_NIST prep_by_pioneered_BLEU nn_quality_MT prep_of_measures_quality amod_measures_automatic dep_been_pioneered prep_on_been_measures aux_been_has nsubj_been_focus advcl_been_difficult advcl_been_costly prep_of_focus_research amod_focus_major det_focus_a advmod_do_reliably aux_do_to xcomp_difficult_do nsubj_difficult_evaluation conj_and_costly_difficult cop_costly_is nsubj_costly_evaluation mark_costly_Since amod_evaluation_human ccomp_``_been
P09-1036	P02-1040	o	Our experimental results display that our SDB model achieves a substantial improvement over the baseline and significantly outperforms XP + according to the BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric dep_the_BLEU pobj_to_the pcomp_according_to conj_+_XP_according dobj_outperforms_according dobj_outperforms_XP advmod_outperforms_significantly nsubj_outperforms_model det_baseline_the prep_over_improvement_baseline amod_improvement_substantial det_improvement_a conj_and_achieves_outperforms dobj_achieves_improvement nsubj_achieves_model mark_achieves_that nn_model_SDB poss_model_our ccomp_display_outperforms ccomp_display_achieves nsubj_display_results amod_results_experimental poss_results_Our
P09-1064	P02-1040	o	1 Introduction In statistical machine translation output translations are evaluated by their similarity to human reference translations where similarity is most often measured by BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni agent_measured_BLEU advmod_measured_often auxpass_measured_is nsubjpass_measured_similarity advmod_measured_where advmod_often_most rcmod_translations_measured nn_translations_reference amod_translations_human prep_to_similarity_translations poss_similarity_their agent_evaluated_similarity auxpass_evaluated_are nsubjpass_evaluated_translations dep_evaluated_Introduction nn_translations_output nn_translation_machine amod_translation_statistical prep_in_Introduction_translation num_Introduction_1
P09-1065	P02-1040	o	We evaluated the translation quality using case-insensitive BLEU metric -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_metric_BLEU amod_metric_case-insensitive dobj_using_metric vmod_quality_using nn_quality_translation det_quality_the dep_evaluated_Papineni dobj_evaluated_quality nsubj_evaluated_We ccomp_``_evaluated
P09-1089	P02-1040	o	For example -LRB- Kauchak and Barzilay 2006 -RRB- paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_like_metrics_BLEU nn_metrics_evaluation amod_metrics_automatic dobj_using_metrics advmod_using_when amod_results_reliable amod_results_more advcl_obtain_using dobj_obtain_results aux_obtain_to dep_obtain_order mark_obtain_in nn_translation_system det_translation_the ccomp_closer_obtain prep_to_closer_translation nsubj_closer_them xcomp_make_closer aux_make_to vmod_references_make nn_references_paraphrase dep_references_Barzilay dep_references_Kauchak dep_Kauchak_2006 conj_and_Kauchak_Barzilay dep_For_references pobj_For_example
P09-1092	P02-1040	o	We evaluate the string chosen by the log-linear model against the original treebank string in terms of exact match and BLEU score -LRB- Papineni et al. 821 Syntactic feature Type Definites Definite descriptions SIMPLE DEF simple definite descriptions POSS DEF simple definite descriptions with a possessive determiner -LRB- pronoun or possibly genitive name -RRB- DEF ATTR ADJ definite descriptions with adjectival modifier DEF GENARG definite descriptions with a genitive argument DEF PPADJ definite descriptions with a PP adjunct DEF RELARG definite descriptions including a relative clause DEF APP definite descriptions including a title or job description as well as a proper name -LRB- e.g. an apposition -RRB- Names PROPER combinations of position/title and proper name -LRB- without article -RRB- BARE PROPER bare proper names Demonstrative descriptions SIMPLE DEMON simple demonstrative descriptions MOD DEMON adjectivally modified demonstrative descriptions Pronouns PERS PRON personal pronouns EXPL PRON expletive pronoun REFL PRON reflexive pronoun DEMON PRON demonstrative pronouns -LRB- not determiners -RRB- GENERIC PRON generic pronoun -LRB- man one -RRB- DA PRON da-pronouns -LRB- darauf daruber dazu -RRB- LOC ADV location-referring pronouns TEMP ADV YEAR Dates and times Indefinites SIMPLE INDEF simple indefinites NEG INDEF negative indefinites INDEF ATTR indefinites with adjectival modifiers INDEF CONTRAST indefinites with contrastive modifiers -LRB- einige some andere other weitere further -RRB- INDEF PPADJ indefinites with PP adjuncts INDEF REL indefinites with relative clause adjunct INDEF GEN indefinites with genitive adjuncts INDEF NUM measure/number phrases INDEF QUANT quantified indefinites Table 5 An inventory of interesting syntactic characteristics in IS phrases Label 1 -LRB- + features -RRB- Label 2 -LRB- + features -RRB- B/A Total D-GIVEN-PRONOUN INDEF-REL 0 19 PERS PRON 39 INDEF ATTR 23 DA PRON 25 SIMPLE INDEF 17 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-PRONOUN D-GIVEN-CATAPHOR 0.1 11 PERS PRON 39 SIMPLE DEF 13 DA PRON 25 DA PRON 10 DEMON PRON 19 GENERIC PRON 11 D-GIVEN-REFLEXIVE NEW 0.11 31 REFL PRON 54 SIMPLE INDEF 113 INDEF ATTR 53 INDEF NUM 32 INDEF PPADJ 26 INDEF GEN 25 Table 6 IS asymmetric pairs augmented with syntactic characteristics 822 2002 -RRB-	number_2002_822 num_characteristics_2002 amod_characteristics_syntactic prep_with_augmented_characteristics dep_pairs_augmented amod_pairs_asymmetric cop_pairs_IS num_Table_6 num_Table_25 nn_Table_GEN nn_Table_INDEF num_Table_26 nn_Table_PPADJ nn_Table_INDEF num_Table_32 nn_Table_NUM nn_Table_INDEF num_Table_53 nn_Table_ATTR nn_Table_INDEF num_Table_113 nn_Table_INDEF amod_Table_SIMPLE num_Table_54 nn_Table_PRON nn_Table_REFL num_Table_31 nn_Table_NEW nn_Table_D-GIVEN-REFLEXIVE num_Table_11 nn_Table_PRON nn_Table_GENERIC num_Table_19 nn_Table_PRON nn_Table_DEMON num_Table_10 nn_Table_PRON nn_Table_DA num_Table_25 nn_Table_PRON nn_Table_DA num_Table_13 nn_Table_DEF amod_Table_SIMPLE num_Table_39 nn_Table_PRON nn_Table_PERS num_Table_11 nn_Table_D-GIVEN-CATAPHOR nn_Table_D-GIVEN-PRONOUN num_Table_11 nn_Table_PRON nn_Table_GENERIC num_Table_19 nn_Table_PRON nn_Table_DEMON num_Table_17 nn_Table_INDEF amod_Table_SIMPLE num_Table_25 nn_Table_PRON nn_Table_DA num_Table_23 nn_Table_ATTR nn_Table_INDEF num_Table_39 nn_Table_PRON nn_Table_PERS num_Table_19 num_Table_0 number_31_0.11 number_11_0.1 dep_INDEF-REL_Table nn_INDEF-REL_D-GIVEN-PRONOUN nn_INDEF-REL_Total nn_INDEF-REL_B/A dep_features_+ dep_Label_INDEF-REL appos_Label_features num_Label_2 nn_Label_Label cc_features_+ appos_Label_features num_Label_1 dep_phrases_Label nsubj_IS_phrases amod_characteristics_syntactic amod_characteristics_interesting prepc_in_inventory_IS prep_of_inventory_characteristics det_inventory_An dep_Table_inventory num_Table_5 dep_indefinites_Table dobj_quantified_indefinites dep_QUANT_quantified dep_INDEF_QUANT amod_phrases_INDEF nn_phrases_measure/number nn_phrases_NUM nn_phrases_INDEF nn_phrases_adjuncts amod_phrases_genitive prep_with_indefinites_phrases nn_indefinites_GEN nn_indefinites_INDEF nn_indefinites_adjunct nn_indefinites_clause amod_indefinites_relative nn_indefinites_REL parataxis_INDEF_pairs prep_with_INDEF_indefinites dobj_INDEF_indefinites nsubj_INDEF_indefinites nn_adjuncts_PP prep_with_indefinites_adjuncts nn_indefinites_PPADJ nn_indefinites_INDEF rcmod_weitere_INDEF advmod_weitere_further amod_weitere_other det_weitere_some amod_other_andere dobj_einige_weitere amod_modifiers_contrastive prep_with_indefinites_modifiers nn_indefinites_CONTRAST amod_indefinites_INDEF nn_indefinites_modifiers amod_indefinites_adjectival nn_indefinites_ATTR dep_INDEF_einige prep_with_INDEF_indefinites dobj_INDEF_indefinites nsubj_INDEF_da-pronouns amod_indefinites_negative nn_indefinites_INDEF nn_indefinites_NEG nn_indefinites_indefinites amod_indefinites_simple nn_indefinites_INDEF nn_indefinites_SIMPLE nn_indefinites_Indefinites nn_indefinites_times conj_and_ADV_indefinites dep_ADV_Dates conj_and_ADV_YEAR nn_ADV_TEMP nn_ADV_pronouns amod_ADV_location-referring nn_ADV_ADV nn_ADV_LOC dep_ADV_darauf conj_darauf_dazu conj_darauf_daruber dep_da-pronouns_indefinites dep_da-pronouns_YEAR dep_da-pronouns_ADV nn_da-pronouns_PRON nn_da-pronouns_DA num_man_one rcmod_pronoun_INDEF appos_pronoun_man amod_pronoun_generic nn_pronoun_PRON nn_pronoun_GENERIC dep_determiners_not dep_pronouns_pronoun dep_pronouns_determiners amod_pronouns_demonstrative nn_pronouns_PRON nn_pronouns_DEMON nn_pronouns_pronoun amod_pronouns_reflexive nn_pronouns_PRON nn_pronouns_REFL nn_pronouns_pronoun nn_pronouns_expletive nn_pronouns_PRON nn_pronouns_EXPL nn_pronouns_pronouns amod_pronouns_personal nn_pronouns_PRON nn_pronouns_PERS nn_pronouns_Pronouns nn_pronouns_descriptions amod_pronouns_demonstrative amod_pronouns_modified nn_pronouns_DEMON nn_pronouns_MOD nn_pronouns_descriptions amod_pronouns_demonstrative amod_pronouns_simple nn_pronouns_DEMON amod_pronouns_SIMPLE nn_pronouns_descriptions amod_pronouns_Demonstrative advmod_modified_adjectivally dep_names_pronouns amod_names_proper amod_names_bare amod_names_PROPER amod_names_BARE dep_names_article pobj_without_names amod_name_proper conj_and_position/title_name dep_combinations_without prep_of_combinations_name prep_of_combinations_position/title amod_combinations_PROPER nn_combinations_Names det_apposition_an pobj_e.g._apposition dep_name_combinations dep_name_e.g. amod_name_proper det_name_a nn_description_job conj_and_title_name conj_or_title_description det_title_a prep_including_descriptions_name prep_including_descriptions_description prep_including_descriptions_title amod_descriptions_definite nn_descriptions_APP nn_descriptions_DEF nn_descriptions_clause amod_descriptions_relative det_descriptions_a prep_including_descriptions_descriptions amod_descriptions_definite nn_descriptions_RELARG nn_descriptions_DEF nn_descriptions_adjunct nn_descriptions_PP det_descriptions_a prep_with_descriptions_descriptions amod_descriptions_definite nn_descriptions_PPADJ nn_descriptions_DEF nn_descriptions_argument amod_descriptions_genitive det_descriptions_a prep_with_descriptions_descriptions amod_descriptions_definite nn_descriptions_GENARG nn_descriptions_DEF nn_descriptions_modifier amod_descriptions_adjectival prep_with_descriptions_descriptions amod_descriptions_definite nn_descriptions_ADJ nn_descriptions_ATTR nn_descriptions_DEF amod_name_genitive advmod_name_possibly conj_or_pronoun_name dep_determiner_descriptions dep_determiner_name dep_determiner_pronoun amod_determiner_possessive det_determiner_a prep_with_descriptions_determiner amod_descriptions_definite amod_descriptions_simple nn_descriptions_DEF nn_descriptions_POSS nn_descriptions_descriptions amod_descriptions_definite amod_descriptions_simple nn_descriptions_DEF amod_descriptions_SIMPLE nn_descriptions_descriptions amod_descriptions_Definite nn_descriptions_Definites nn_descriptions_Type nn_descriptions_feature amod_descriptions_Syntactic num_descriptions_821 appos_Papineni_descriptions dep_Papineni_al. nn_Papineni_et nn_score_BLEU conj_and_match_score amod_match_exact prep_of_terms_score prep_of_terms_match prep_in_string_terms nn_string_treebank amod_string_original det_string_the prep_against_model_string amod_model_log-linear det_model_the agent_chosen_model vmod_string_chosen det_string_the dep_evaluate_Papineni dobj_evaluate_string nsubj_evaluate_We ccomp_``_evaluate
P09-1093	P02-1040	o	For MCE learning we selected the reference compression that maximize the BLEU score -LRB- Papineni et al. 2002 -RRB- -LRB- = argmax rR BLEU -LRB- r R \ r -RRB- -RRB- from the set of reference compressions and used it as correct data for training	prep_for_data_training amod_data_correct prep_as_used_data dobj_used_it nsubj_used_we nn_compressions_reference prep_of_set_compressions det_set_the num_r_\ nn_r_R dep_r_r dep_BLEU_r nn_BLEU_rR nn_BLEU_argmax dep_=_BLEU dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dobj_maximize_score nsubj_maximize_that rcmod_compression_maximize nn_compression_reference det_compression_the conj_and_selected_used prep_from_selected_set dep_selected_= dep_selected_Papineni dobj_selected_compression nsubj_selected_we prep_for_selected_learning nn_learning_MCE
P09-1093	P02-1040	o	For automatic evaluation we employed BLEU -LRB- Papineni et al. 2002 -RRB- by following -LRB- Unno et al. 2006 -RRB-	amod_Unno_2006 dep_Unno_al. nn_Unno_et dep_following_Unno appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prepc_by_employed_following dobj_employed_BLEU nsubj_employed_we prep_for_employed_evaluation amod_evaluation_automatic
P09-1094	P02-1040	o	Methods have been proposed for automatic evaluation in MT -LRB- e.g. BLEU -LRB- Papineni et al. 2002 -RRB- -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dep_e.g._BLEU ccomp_-LRB-_e.g. prep_in_evaluation_MT amod_evaluation_automatic prep_for_proposed_evaluation auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Methods
P09-1099	P02-1040	o	Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields widely used measures include BLEU -LRB- Papineni et al. 2002 -RRB- for machine translation and ROUGE -LRB- Lin 2004 -RRB- for summarisation for example	dep_Lin_2004 appos_ROUGE_Lin conj_and_translation_ROUGE nn_translation_machine amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_for_BLEU_example prep_for_BLEU_summarisation prep_for_BLEU_ROUGE prep_for_BLEU_translation dep_BLEU_Papineni dobj_include_BLEU nsubj_include_measures amod_measures_used advmod_used_widely amod_fields_other prep_of_number_fields det_number_a prep_in_developed_number auxpass_developed_been aux_developed_have nsubjpass_developed_properties advmod_developed_automatically amod_properties_computable prepc_on_based_developed dep_behaviour_include vmod_behaviour_based nn_behaviour_system nn_behaviour_rate dep_that_behaviour dep_metrics_that nn_metrics_evaluation amod_metrics_Automated
P09-1103	P02-1040	o	The evaluation metric is casesensitive BLEU-4 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU-4_Papineni amod_BLEU-4_casesensitive cop_BLEU-4_is nsubj_BLEU-4_metric nn_metric_evaluation det_metric_The
P09-1106	P02-1040	o	4.3 Experiments results Our evaluation metric is BLEU -LRB- Papineni et al. 2002 -RRB- which are to perform case-insensitive matching of n-grams up to n = 4	dep_=_4 amod_n_= pobj_to_n pcomp_up_to prep_of_matching_n-grams amod_matching_case-insensitive prep_perform_up dobj_perform_matching aux_perform_to xcomp_are_perform nsubj_are_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_are dep_BLEU_Papineni cop_BLEU_is nn_metric_evaluation poss_metric_Our dep_results_BLEU dobj_results_metric nsubj_results_Experiments num_Experiments_4.3 ccomp_``_results
P09-2034	P02-1040	p	It could be shown that such methods of which BLEU -LRB- Papineni et al. 2002 -RRB- is the most common can deliver evaluation results that show a high agreement with human judgments -LRB- Papineni et al. 2002 Coughlin 2003 Koehn & Monz 2006 -RRB-	dep_Koehn_2006 conj_and_Koehn_Monz num_Coughlin_2003 dep_Papineni_Monz dep_Papineni_Koehn dep_Papineni_Coughlin dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_judgments_human prep_with_agreement_judgments amod_agreement_high det_agreement_a dobj_show_agreement nsubj_show_that rcmod_results_show nn_results_evaluation dobj_deliver_results aux_deliver_can nsubj_deliver_methods mark_deliver_that advmod_common_most det_common_the cop_common_is nsubj_common_BLEU prep_of_common_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni rcmod_methods_common amod_methods_such dep_shown_Papineni ccomp_shown_deliver auxpass_shown_be aux_shown_could nsubjpass_shown_It
P09-2034	P02-1040	n	By doing so we must emphasize that as described in the previous section the BLEU score was not designed to deliver satisfactory results at the sentence level -LRB- Papineni et al. 2002 -RRB- and this also applies to the closely related NIST score	nn_score_NIST amod_score_related det_score_the advmod_related_closely prep_to_applies_score advmod_applies_also nsubj_applies_this amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_level_sentence det_level_the amod_results_satisfactory prep_at_deliver_level dobj_deliver_results aux_deliver_to xcomp_designed_deliver neg_designed_not auxpass_designed_was nsubjpass_designed_score advcl_designed_described mark_designed_that nn_score_BLEU det_score_the amod_section_previous det_section_the prep_in_described_section mark_described_as conj_and_emphasize_applies dep_emphasize_Papineni ccomp_emphasize_designed aux_emphasize_must nsubj_emphasize_we prepc_by_emphasize_doing advmod_doing_so
P09-2035	P02-1040	o	We evaluate our results with case-sensitive BLEU-4 metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et amod_al._Papineni dep_BLEU-4_al. amod_BLEU-4_metric dep_case-sensitive_BLEU-4 prep_with_results_case-sensitive poss_results_our dobj_evaluate_results nsubj_evaluate_We ccomp_``_evaluate
P09-2058	P02-1040	o	We tune all feature weights automatically -LRB- Och 2003 -RRB- to maximize the BLEU -LRB- Papineni et al. 2002 -RRB- score on the dev set	amod_set_dev det_set_the prep_on_score_set dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_score dep_BLEU_Papineni det_BLEU_the dobj_maximize_BLEU aux_maximize_to amod_Och_2003 nn_weights_feature det_weights_all xcomp_tune_maximize dep_tune_Och advmod_tune_automatically dobj_tune_weights nsubj_tune_We
P09-3004	P02-1040	o	The measures are word overlap length difference -LRB- in words -RRB- BLEU -LRB- Papineni et al. 2002 -RRB- dependency relation overlap -LRB- i.e. R1 and R2 but not FR1 R2 -RRB- and dependency tree edit distance	dobj_edit_distance vmod_tree_edit nn_tree_dependency conj_negcc_R1_R2 conj_negcc_R1_FR1 conj_and_R1_R2 dep_i.e._R2 dep_i.e._FR1 dep_i.e._R2 dep_i.e._R1 conj_and_overlap_tree prep_overlap_i.e. nsubj_overlap_difference nn_relation_dependency amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni conj_difference_relation conj_difference_BLEU prep_in_difference_words nn_difference_length parataxis_overlap_tree parataxis_overlap_overlap nsubj_overlap_word parataxis_are_overlap nsubj_are_measures det_measures_The ccomp_``_are
W02-1022	P02-1040	o	While recent proposals for evaluation of MT systems have involved multi-parallel corpora -LRB- Thompson and Brew 1996 Papineni et al. 2002 -RRB- statistical MT algorithms typically only use one-parallel data	amod_data_one-parallel dobj_use_data advmod_use_only advmod_use_typically nsubj_use_algorithms advcl_use_involved nn_algorithms_MT amod_algorithms_statistical num_Papineni_2002 nn_Papineni_al. nn_Papineni_et dep_Thompson_Papineni amod_Thompson_1996 conj_and_Thompson_Brew appos_corpora_Brew appos_corpora_Thompson amod_corpora_multi-parallel dobj_involved_corpora aux_involved_have nsubj_involved_proposals mark_involved_While nn_systems_MT prep_of_evaluation_systems prep_for_proposals_evaluation amod_proposals_recent
W03-0501	P02-1040	o	5.2 Bleu Automatic Evaluation BLEU -LRB- Papineni et al 2002 -RRB- is a system for automatic evaluation of machine translation	nn_translation_machine prep_of_evaluation_translation amod_evaluation_automatic prep_for_system_evaluation det_system_a cop_system_is nsubj_system_BLEU amod_Papineni_2002 dep_Papineni_al nn_Papineni_et appos_BLEU_Papineni nn_BLEU_Evaluation nn_BLEU_Automatic dep_Bleu_system num_Bleu_5.2 dep_``_Bleu
W03-1001	P02-1040	o	These blocks are used to compute the results in the fourth column the BLEU score -LRB- Papineni et al. 2002 -RRB- with a153 reference translation using a153 grams along with 95 % confidence interval is reported 4	dobj_reported_4 auxpass_reported_is nsubjpass_reported_score nn_interval_confidence amod_interval_% number_%_95 pobj_grams_interval prepc_along_with_grams_with dep_a153_grams dobj_using_a153 nn_translation_reference nn_translation_a153 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et vmod_score_using prep_with_score_translation dep_score_Papineni nn_score_BLEU det_score_the amod_column_fourth det_column_the prep_in_results_column det_results_the dobj_compute_results aux_compute_to parataxis_used_reported xcomp_used_compute auxpass_used_are nsubjpass_used_blocks det_blocks_These
W03-1612	P02-1040	o	BLEU -LRB- Papineni et al. 2002b -RRB- is one of the methods for automatic evaluation of translation quality	nn_quality_translation prep_of_evaluation_quality amod_evaluation_automatic prep_for_methods_evaluation det_methods_the prep_of_one_methods cop_one_is nsubj_one_BLEU appos_Papineni_2002b dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni
W03-1612	P02-1040	p	High correlation is reported between the BLEU score and human evaluations for translations from Arabic Chinese French and Spanish to English -LRB- Papineni et al. 2002a -RRB-	appos_Papineni_2002a dep_Papineni_al. nn_Papineni_et conj_and_Arabic_Spanish conj_and_Arabic_French conj_and_Arabic_Chinese prep_to_translations_English prep_from_translations_Spanish prep_from_translations_French prep_from_translations_Chinese prep_from_translations_Arabic amod_evaluations_human prep_for_score_translations conj_and_score_evaluations nn_score_BLEU det_score_the dep_reported_Papineni prep_between_reported_evaluations prep_between_reported_score auxpass_reported_is nsubjpass_reported_correlation amod_correlation_High
W03-1612	P02-1040	o	2 Background Overview of BLEU This section briefly describes the original BLEU -LRB- Papineni et al. 2002b -RRB- 1 which was designed for English translation evaluation so English sentences are used as examples in this section	det_section_this prep_in_examples_section prep_as_used_examples auxpass_used_are nsubjpass_used_BLEU amod_sentences_English num_sentences_1 advmod_English_so nn_evaluation_translation nn_evaluation_English prep_for_designed_evaluation auxpass_designed_was nsubjpass_designed_which rcmod_1_designed appos_Papineni_2002b dep_Papineni_al. nn_Papineni_et dep_BLEU_sentences dep_BLEU_Papineni amod_BLEU_original det_BLEU_the ccomp_describes_used nsubj_describes_briefly tmod_describes_section advmod_describes_BLEU mark_describes_of dep_describes_Overview det_section_This dep_Background_describes num_Background_2 dep_``_Background
W03-1612	P02-1040	p	Empirically the BLEU score has a high correlation with human evaluation when N = 4 for English translation evaluations -LRB- Papineni et al. 2002b -RRB-	appos_Papineni_2002b dep_Papineni_al. nn_Papineni_et nn_evaluations_translation nn_evaluations_English dep_=_Papineni prep_for_=_evaluations dobj_=_4 nsubj_=_N advmod_=_when amod_evaluation_human prep_with_correlation_evaluation amod_correlation_high det_correlation_a advcl_has_= dobj_has_correlation nsubj_has_score advmod_has_Empirically nn_score_BLEU det_score_the
W03-2804	P02-1040	o	BLEU Score BLEU is an automatic metric designed by IBM which uses several references -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_references_Papineni amod_references_several dobj_uses_references nsubj_uses_which rcmod_IBM_uses agent_designed_IBM vmod_metric_designed amod_metric_automatic det_metric_an cop_metric_is nsubj_metric_BLEU dep_Score_metric nn_Score_BLEU
W04-1014	P02-1040	o	To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans various metrics using n-gram precision and word accuracy have been proposed word string precision -LRB- Hori and Furui 2000b -RRB- for summarization through word extraction ROUGE -LRB- Lin and Hovy 2003 -RRB- for abstracts and BLEU -LRB- Papineni et al. 2002 -RRB- for machine translation	nn_translation_machine amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_for_BLEU_translation dep_BLEU_Papineni dep_Lin_2003 conj_and_Lin_Hovy prep_for_ROUGE_abstracts appos_ROUGE_Hovy appos_ROUGE_Lin nn_extraction_word dep_Hori_2000b conj_and_Hori_Furui prep_for_precision_summarization appos_precision_Furui appos_precision_Hori nn_precision_string nn_precision_word conj_and_proposed_BLEU conj_and_proposed_ROUGE prep_through_proposed_extraction dobj_proposed_precision auxpass_proposed_been aux_proposed_have nsubjpass_proposed_metrics nn_accuracy_word conj_and_precision_accuracy amod_precision_n-gram dobj_using_accuracy dobj_using_precision vmod_metrics_using amod_metrics_various parataxis_varied_BLEU parataxis_varied_ROUGE parataxis_varied_proposed prep_among_varied_humans csubj_varied_evaluate dobj_using_references pcomp_by_using pcomp_into_by nn_concatenation_word nn_concatenation_consideration prep_taking_into dobj_taking_concatenation prepc_with_generated_taking advmod_generated_automatically vmod_sentence_generated dobj_evaluate_sentence aux_evaluate_To
W04-1016	P02-1040	o	Work in this area includes that of Lin and Hovy -LRB- 2003 -RRB- and Pastra and Saggion -LRB- 2003 -RRB- both of whom inspect the use of Bleu-like metrics -LRB- Papineni et al. 2002 -RRB- in summarization	prep_in_Papineni_summarization amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_metrics_Bleu-like prep_of_use_metrics det_use_the dep_inspect_Papineni dobj_inspect_use prep_of_inspect_whom dep_inspect_both prep_of_inspect_Saggion prep_of_inspect_Pastra prep_of_inspect_Hovy prep_of_inspect_Lin mark_inspect_that appos_Saggion_2003 appos_Hovy_2003 conj_and_Lin_Saggion conj_and_Lin_Pastra conj_and_Lin_Hovy ccomp_includes_inspect nsubj_includes_Work det_area_this prep_in_Work_area
W04-1708	P02-1040	o	The core technology of the proposed method i.e. the automatic evaluation of translations was developed in research aiming at the efficient development of Machine Translation -LRB- MT -RRB- technology -LRB- Su et al. 1992 Papineni et al. 2002 NIST 2002 -RRB-	num_NIST_2002 nn_al._et nn_al._Papineni dep_al._NIST dep_al._2002 dep_al._al. dep_al._1992 nn_al._et nn_al._Su nn_technology_Translation appos_Translation_MT nn_Translation_Machine prep_of_development_technology amod_development_efficient det_development_the prep_at_aiming_development vmod_research_aiming dep_developed_al. prep_in_developed_research auxpass_developed_was nsubjpass_developed_evaluation advmod_developed_i.e. prep_of_evaluation_translations amod_evaluation_automatic det_evaluation_the rcmod_method_developed amod_method_proposed det_method_the prep_of_technology_method nn_technology_core det_technology_The dep_``_technology
W04-1708	P02-1040	o	The unit of utterance corresponds to the unit of segment in the original BLEU and NIST studies -LRB- Papineni et al. 2002 NIST 2002 -RRB-	amod_NIST_2002 dep_Papineni_NIST appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_studies_NIST dep_BLEU_Papineni conj_and_BLEU_studies amod_BLEU_original det_BLEU_the prep_of_unit_segment det_unit_the prep_in_corresponds_studies prep_in_corresponds_BLEU prep_to_corresponds_unit nsubj_corresponds_unit prep_of_unit_utterance det_unit_The
W04-2203	P02-1040	o	3.1 Golden-standard-based criteria In the domain of machine translation systems an increasingly accepted way to measure the quality of a system is to compare the outputs it produces with a set of reference translations considered as an approximation of a golden standard -LRB- Papineni et al. 2002 hovy et al. 2002 -RRB-	num_hovy_2002 nn_hovy_al. nn_hovy_et dep_Papineni_hovy appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_standard_golden det_standard_a prep_of_approximation_standard det_approximation_an prep_as_considered_approximation nn_translations_reference dep_set_Papineni vmod_set_considered prep_of_set_translations det_set_a prep_with_produces_set nsubj_produces_it rcmod_outputs_produces det_outputs_the dobj_compare_outputs aux_compare_to xcomp_is_compare nsubj_is_criteria det_system_a prep_of_quality_system det_quality_the dobj_measure_quality aux_measure_to vmod_way_measure amod_way_accepted det_way_an advmod_accepted_increasingly nn_systems_translation nn_systems_machine prep_of_domain_systems det_domain_the appos_criteria_way prep_in_criteria_domain amod_criteria_Golden-standard-based num_criteria_3.1 ccomp_``_is
W05-0712	P02-1040	o	of Words Person names 803 1749 Organization names 312 867 Location names 345 614 The BLEU score -LRB- Papineni et al. 2002 -RRB- with a single reference translation was deployed for evaluation	prep_for_deployed_evaluation auxpass_deployed_was nsubjpass_deployed_score nn_translation_reference amod_translation_single det_translation_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_with_score_translation dep_score_Papineni nn_score_BLEU det_score_The num_score_614 number_614_345 rcmod_names_deployed nn_names_Location num_names_867 num_names_312 dep_names_names nn_names_Organization num_names_1749 num_names_803 dep_names_names nn_names_Person nn_names_Words prep_of_``_names
W05-0806	P02-1040	o	4.2 Translation Results The evaluation metrics used in our experiments are WER -LRB- Word Error Rate -RRB- PER -LRB- Positionindependent word Error Rate -RRB- and BLEU -LRB- BiLingual Evaluation Understudy -RRB- -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_Understudy_Evaluation amod_Understudy_BiLingual appos_BLEU_Understudy nn_Rate_Error nn_Rate_word amod_Rate_Positionindependent dep_PER_Papineni conj_and_PER_BLEU appos_PER_Rate nn_Rate_Error nn_Rate_Word appos_WER_BLEU appos_WER_PER appos_WER_Rate cop_WER_are nsubj_WER_metrics poss_experiments_our prep_in_used_experiments vmod_metrics_used nn_metrics_evaluation det_metrics_The dep_Results_WER nn_Results_Translation num_Results_4.2 dep_``_Results
W05-0820	P02-1040	o	Translation performance was measured using the BLEU score -LRB- Papineni et al. 2002 -RRB- which measures n-gram overlap with a reference translation	nn_translation_reference det_translation_a prep_with_overlap_translation nsubj_overlap_n-gram ccomp_measures_overlap nsubj_measures_which amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dep_using_measures dep_using_Papineni dobj_using_score xcomp_measured_using auxpass_measured_was nsubjpass_measured_performance nn_performance_Translation
W05-0822	P02-1040	o	Once this is accomplished a variant of Powells algorithm is used to find weights that optimize BLEU score -LRB- Papineni et al 2002 -RRB- over these hypotheses compared to reference translations	dobj_reference_translations det_hypotheses_these amod_Papineni_2002 dep_Papineni_al nn_Papineni_et nn_score_BLEU dobj_optimize_score nsubj_optimize_that rcmod_weights_optimize dobj_find_weights aux_find_to pcomp_used_reference prepc_compared_to_used_to prep_over_used_hypotheses dep_used_Papineni xcomp_used_find auxpass_used_is nsubjpass_used_variant advcl_used_accomplished nn_algorithm_Powells prep_of_variant_algorithm det_variant_a auxpass_accomplished_is nsubjpass_accomplished_this advmod_accomplished_Once
W05-0823	P02-1040	o	This algorithm adjusts the log-linear weights so that BLEU -LRB- Papineni et al. 2002 -RRB- is maximized over a given development set	nn_set_development amod_set_given det_set_a prep_over_maximized_set auxpass_maximized_is nsubjpass_maximized_BLEU mark_maximized_that amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_Papineni ccomp_so_maximized amod_weights_log-linear det_weights_the advmod_adjusts_so dobj_adjusts_weights nsubj_adjusts_algorithm det_algorithm_This
W05-0828	P02-1040	o	3.2 Results and Discussion The BLEU scores -LRB- Papineni et al. 2002 -RRB- for 10 direct translations and 4 sets of heuristic selections 4Admittedly in typical instances of such chains English would appear earlier	acomp_appear_earlier aux_appear_would nsubj_appear_English advmod_appear_4Admittedly amod_chains_such prep_of_instances_chains amod_instances_typical prep_in_4Admittedly_instances rcmod_selections_appear nn_selections_heuristic prep_of_sets_selections num_sets_4 conj_and_translations_sets amod_translations_direct num_translations_10 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_scores_BLEU det_scores_The nn_scores_Discussion prep_for_Results_sets prep_for_Results_translations dep_Results_Papineni conj_and_Results_scores num_Results_3.2
W05-0831	P02-1040	o	5.2 Evaluation Criteria For the automatic evaluation we used the criteria from the IWSLT evaluation campaign -LRB- Akiba et al. 2004 -RRB- namely word error rate -LRB- WER -RRB- positionindependent word error rate -LRB- PER -RRB- and the BLEU and NIST scores -LRB- Papineni et al. 2002 Doddington 2002 -RRB-	amod_Doddington_2002 dep_Papineni_Doddington appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_scores_NIST dep_BLEU_Papineni conj_and_BLEU_scores det_BLEU_the appos_rate_PER nn_rate_error nn_rate_word amod_rate_positionindependent conj_and_rate_scores conj_and_rate_BLEU conj_and_rate_rate appos_rate_WER nn_rate_error nn_rate_word pobj_namely_BLEU pobj_namely_rate pobj_namely_rate amod_Akiba_2004 dep_Akiba_al. nn_Akiba_et nn_campaign_evaluation nn_campaign_IWSLT det_campaign_the prep_from_criteria_campaign det_criteria_the prep_used_namely dep_used_Akiba dobj_used_criteria nsubj_used_we ccomp_used_Criteria amod_evaluation_automatic det_evaluation_the prep_for_Criteria_evaluation nn_Criteria_Evaluation num_Criteria_5.2
W05-0833	P02-1040	o	We provide results using a range of automatic evaluation metrics BLEU -LRB- Papineni et al. 2002 -RRB- Precision and Recall -LRB- Turian et al. 2003 -RRB- and Wordand Sentence Error Rates	nn_Rates_Error nn_Rates_Sentence nn_Rates_Wordand dep_al._2003 nn_al._et amod_al._Turian amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_Rates dep_BLEU_al. conj_and_BLEU_Recall conj_and_BLEU_Precision dep_BLEU_Papineni nn_metrics_evaluation amod_metrics_automatic prep_of_range_metrics det_range_a dobj_using_range vmod_results_using dep_provide_Rates dep_provide_Recall dep_provide_Precision dep_provide_BLEU dobj_provide_results nsubj_provide_We rcmod_``_provide
W05-0833	P02-1040	o	In order to create the necessary SMT language and translation models they used Giza + + -LRB- Och & Ney 2003 -RRB- 2 the CMU-Cambridge statistical toolkit 3 the ISI ReWrite Decoder .4 Translation was performed from EnglishFrench and FrenchEnglish and the resulting translations were evaluated using a range of automatic metrics BLEU -LRB- Papineni et al. 2002 -RRB- Precision and Recall 2http / / www.isi.edu/och/Giza++.html 3http / / mi.eng.cam.ac.uk / prc14/toolkit html 4http / / www.isi.edu/licensed-sw/rewrite-decoder/ 185 -LRB- Turian et al. 2003 -RRB- and Wordand Sentence Error Rates	nn_Rates_Error nn_Rates_Sentence nn_Rates_Wordand dep_al._2003 nn_al._et amod_al._Turian appos_www.isi.edu/licensed-sw/rewrite-decoder/_al. num_www.isi.edu/licensed-sw/rewrite-decoder/_185 nn_4http_html dep_prc14/toolkit_4http amod_prc14/toolkit_mi.eng.cam.ac.uk nn_3http_www.isi.edu/och/Giza++.html dobj_Recall_2http amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_Rates conj_and_BLEU_www.isi.edu/licensed-sw/rewrite-decoder/ dep_BLEU_prc14/toolkit dep_BLEU_3http conj_and_BLEU_Recall conj_and_BLEU_Precision dep_BLEU_Papineni amod_metrics_automatic prep_of_range_metrics det_range_a dobj_using_range xcomp_evaluated_using auxpass_evaluated_were nsubjpass_evaluated_translations amod_translations_resulting det_translations_the conj_and_EnglishFrench_FrenchEnglish dep_performed_Rates dep_performed_www.isi.edu/licensed-sw/rewrite-decoder/ dep_performed_Recall dep_performed_Precision dep_performed_BLEU conj_and_performed_evaluated prep_from_performed_FrenchEnglish prep_from_performed_EnglishFrench auxpass_performed_was nsubjpass_performed_Translation dep_performed_3 num_Translation_.4 nn_Translation_Decoder nn_Translation_ReWrite nn_Translation_ISI det_Translation_the amod_toolkit_statistical amod_toolkit_CMU-Cambridge det_toolkit_the pobj_2_toolkit dep_Och_2003 conj_and_Och_Ney appos_Giza_Ney appos_Giza_Och conj_+_Giza_+ parataxis_used_evaluated parataxis_used_performed dep_used_2 dobj_used_+ dobj_used_Giza nsubj_used_they advcl_used_create nn_models_translation conj_and_language_models nn_language_SMT amod_language_necessary det_language_the dobj_create_models dobj_create_language aux_create_to dep_create_order mark_create_In
W05-0836	P02-1040	o	5.3 Evaluation Metric This paper focuses on the BLEU metric as presented in -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_in_Papineni prep_presented_in mark_presented_as nn_metric_BLEU det_metric_the advcl_focuses_presented prep_on_focuses_metric nsubj_focuses_paper det_paper_This rcmod_Metric_focuses nn_Metric_Evaluation num_Metric_5.3 dep_``_Metric
W05-0836	P02-1040	o	The piecewise linearity observation made in -LRB- Papineni et al. 2002 -RRB- is no longer applicable since we can not move the log operation into the expected value	amod_value_expected det_value_the nn_operation_log det_operation_the prep_into_move_value dobj_move_operation neg_move_not aux_move_can nsubj_move_we mark_move_since advcl_applicable_move advmod_applicable_longer cop_applicable_is nsubj_applicable_observation neg_longer_no amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_in_made_Papineni vmod_observation_made nn_observation_linearity amod_observation_piecewise det_observation_The
W05-0904	P02-1040	p	BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities -LRB- Papineni et al. 2002 Doddington 2002 -RRB-	amod_Doddington_2002 dep_Papineni_Doddington dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_qualities_different prep_with_systems_qualities nn_systems_MT amod_systems_ranking prep_in_judgments_systems amod_judgments_human prep_with_correlate_judgments advmod_correlate_closely aux_correlate_to dep_shown_Papineni xcomp_shown_correlate auxpass_shown_been aux_shown_have nsubjpass_shown_NIST nsubjpass_shown_BLEU conj_and_BLEU_NIST
W05-0904	P02-1040	o	The most commonly used automatic evaluation metrics BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- are based on the assumption that The closer a machine translation is to a professional human translation the better it is -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_is_Papineni dep_it_is dep_better_it det_better_the dep_,_better amod_translation_human amod_translation_professional det_translation_a prep_to_is_translation nsubj_is_translation mark_is_that nn_translation_machine det_translation_a amod_translation_closer det_translation_The ccomp_assumption_is det_assumption_the dep_Doddington_2002 appos_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni appos_metrics_NIST appos_metrics_BLEU nn_metrics_evaluation amod_metrics_automatic amod_metrics_used det_metrics_The advmod_used_commonly advmod_used_most prep_based_on_``_assumption auxpass_``_are nsubjpass_``_metrics
W05-0906	P02-1040	o	This idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the BLEU metric for machine translation -LRB- Papineni et al. 2002 -RRB- and the ROUGE -LRB- Lin and Hovy 2003 -RRB- metric for summarization	prep_for_Lin_summarization amod_Lin_metric num_Lin_2003 conj_and_Lin_Hovy dep_ROUGE_Hovy dep_ROUGE_Lin det_ROUGE_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_translation_machine nn_metric_BLEU det_metric_the prep_for_roots_translation prep_in_roots_metric poss_roots_its conj_and_has_ROUGE dep_has_Papineni dobj_has_roots nsubj_has_idea nn_outputs_reference amod_outputs_desired num_outputs_more num_outputs_one conj_or_one_more prep_against_system_outputs nn_system_computer det_system_a prep_of_output_system det_output_the dobj_score_output aux_score_to nn_statistics_co-occurrence amod_statistics_n-gram vmod_employing_score dobj_employing_statistics prepc_of_idea_employing det_idea_This
W05-0909	P02-1040	p	1 Introduction Automatic Metrics for machine translation -LRB- MT -RRB- evaluation have been receiving significant attention in the past two years since IBM 's BLEU metric was proposed and made available -LRB- Papineni et al 2002 -RRB-	advmod_2002_al nn_al_et num_Papineni_2002 dep_available_Papineni acomp_made_available nsubjpass_made_metric conj_and_proposed_made auxpass_proposed_was nsubjpass_proposed_metric mark_proposed_since nn_metric_BLEU poss_metric_IBM num_years_two amod_years_past det_years_the amod_attention_significant advcl_receiving_made advcl_receiving_proposed prep_in_receiving_years dobj_receiving_attention aux_receiving_been aux_receiving_have nsubj_receiving_Metrics nn_evaluation_translation appos_translation_MT nn_translation_machine prep_for_Metrics_evaluation nn_Metrics_Automatic nn_Metrics_Introduction num_Metrics_1 ccomp_``_receiving
W05-0909	P02-1040	o	2 The METEOR Metric 2.1 Weaknesses in BLEU Addressed in METEOR The main principle behind IBMs BLEU metric -LRB- Papineni et al 2002 -RRB- is the measurement of the 66 overlap in unigrams -LRB- single words -RRB- and higher order n-grams of words between a translation being evaluated and a set of one or more reference translations	nn_translations_reference num_translations_more num_translations_one conj_or_one_more prep_of_set_translations det_set_a conj_and_evaluated_set auxpass_evaluated_being nsubjpass_evaluated_translation det_translation_a prep_of_n-grams_words nn_n-grams_order amod_n-grams_higher amod_words_single conj_and_unigrams_n-grams appos_unigrams_words prepc_between_overlap_set prepc_between_overlap_evaluated prep_in_overlap_n-grams prep_in_overlap_unigrams nsubj_overlap_Weaknesses det_66_the prep_of_measurement_66 det_measurement_the cop_measurement_is nsubj_measurement_principle dep_al_2002 nn_al_et dep_Papineni_al amod_BLEU_Papineni amod_BLEU_metric dep_IBMs_BLEU prep_behind_principle_IBMs amod_principle_main det_principle_The prep_in_Addressed_METEOR nn_Addressed_BLEU rcmod_Weaknesses_measurement prep_in_Weaknesses_Addressed num_Weaknesses_2.1 nn_Weaknesses_Metric nn_Weaknesses_METEOR det_Weaknesses_The num_Weaknesses_2 ccomp_``_overlap
W05-1203	P02-1040	o	Text similarity has been also used for relevance feedback and text classification -LRB- Rocchio 1971 -RRB- word sense disambiguation -LRB- Lesk 1986 -RRB- and more recently for extractive summarization -LRB- Salton et al. 1997b -RRB- and methods for automatic evaluation of machine translation -LRB- Papineni et al. 2002 -RRB- or text summarization -LRB- Lin and Hovy 2003 -RRB-	amod_Lin_2003 conj_and_Lin_Hovy appos_summarization_Hovy appos_summarization_Lin nn_summarization_text dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_translation_machine prep_of_evaluation_translation amod_evaluation_automatic prep_for_methods_evaluation appos_Salton_1997b dep_Salton_al. nn_Salton_et amod_summarization_extractive dep_recently_Salton prep_for_recently_summarization advmod_recently_more dep_Lesk_1986 appos_disambiguation_Lesk nn_disambiguation_sense nn_disambiguation_word dep_Rocchio_1971 nn_classification_text conj_and_feedback_classification nn_feedback_relevance conj_or_used_summarization dep_used_Papineni conj_and_used_methods conj_and_used_recently conj_and_used_disambiguation dep_used_Rocchio prep_for_used_classification prep_for_used_feedback advmod_used_also auxpass_used_been aux_used_has nsubjpass_used_Text dobj_Text_similarity
W05-1204	P02-1040	o	Consequently here we employ multiple references to evaluate MT systems like BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_like_systems_BLEU nn_systems_MT dobj_evaluate_systems aux_evaluate_to amod_references_multiple conj_and_employ_NIST dep_employ_Papineni vmod_employ_evaluate dobj_employ_references nsubj_employ_we advmod_employ_here advmod_employ_Consequently
W05-1510	P02-1040	o	The accuracy of the generator outputs was evaluated by the BLEU score -LRB- Papineni et al. 2001 -RRB- which is commonly used for the evaluation of machine translation and recently used for the evaluation of generation -LRB- Langkilde-Geary 2002 Velldal and Oepen 2005 -RRB-	dep_Velldal_2005 conj_and_Velldal_Oepen dep_Langkilde-Geary_Oepen dep_Langkilde-Geary_Velldal appos_Langkilde-Geary_2002 dep_generation_Langkilde-Geary prep_of_evaluation_generation det_evaluation_the prep_for_used_evaluation advmod_used_recently nsubjpass_used_which nn_translation_machine prep_of_evaluation_translation det_evaluation_the conj_and_used_used prep_for_used_evaluation advmod_used_commonly auxpass_used_is nsubjpass_used_which rcmod_Papineni_used rcmod_Papineni_used amod_Papineni_2001 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the dep_evaluated_Papineni agent_evaluated_score auxpass_evaluated_was nsubjpass_evaluated_accuracy nn_outputs_generator det_outputs_the prep_of_accuracy_outputs det_accuracy_The ccomp_``_evaluated
W06-1112	P02-1040	n	They are a bit controversial in a proper machine translation where the popular BLEU score -LRB- Papineni et al. 2002 -RRB- although widely accepted as a measure of translation accuracy seems to favor stochastic approaches based on 91 an n-gram model over other MT methods -LRB- see the results in -LRB- Nist 2001 -RRB- -RRB-	dep_Nist_2001 dep_in_Nist prep_results_in det_results_the dobj_see_results nn_methods_MT amod_methods_other prep_over_model_methods nn_model_n-gram det_model_an num_model_91 prep_on_based_model vmod_approaches_based amod_approaches_stochastic dobj_favor_approaches aux_favor_to dep_seems_see xcomp_seems_favor nsubj_seems_score advmod_seems_where nn_accuracy_translation prep_of_measure_accuracy det_measure_a prep_as_accepted_measure advmod_accepted_widely mark_accepted_although amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_accepted dep_score_Papineni nn_score_BLEU amod_score_popular det_score_the rcmod_translation_seems nn_translation_machine amod_translation_proper det_translation_a prep_in_controversial_translation amod_bit_controversial det_bit_a cop_bit_are nsubj_bit_They
W06-1608	P02-1040	o	3.2 Translation quality Table 2 presents the impact of parse quality on a treelet translation system measured using BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dobj_using_BLEU xcomp_measured_using nn_system_translation amod_system_treelet det_system_a amod_quality_parse prep_on_impact_system prep_of_impact_quality det_impact_the vmod_presents_measured dobj_presents_impact nsubj_presents_Table num_Table_2 nn_Table_quality nn_Table_Translation num_Table_3.2
W06-3101	P02-1040	p	The most widely used are Word Error Rate -LRB- WER -RRB- Position Independent Word Error Rate -LRB- PER -RRB- the BLEU score -LRB- Papineni et al. 2002 -RRB- and the NIST score -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_score_Doddington nn_score_NIST det_score_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_score_score dep_score_Papineni nn_score_BLEU det_score_the appos_Rate_score appos_Rate_score appos_Rate_PER nn_Rate_Error nn_Rate_Word nn_Rate_Independent nn_Rate_Position dep_Rate_Rate appos_Rate_WER nn_Rate_Error nn_Rate_Word cop_Rate_are nsubj_Rate_used advmod_used_widely det_used_The advmod_widely_most
W06-3101	P02-1040	o	2 Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output some of them proposing new measures some proposing improvements and extensions of the existing ones -LRB- Doddington 2002 Papineni et al. 2002 Babych and Hartley 2004 Matusov et al. 2005 -RRB-	num_Matusov_2005 nn_Matusov_al. nn_Matusov_et num_Babych_2004 conj_and_Babych_Hartley num_Papineni_2002 nn_Papineni_al. nn_Papineni_et dep_Doddington_Matusov dep_Doddington_Hartley dep_Doddington_Babych dep_Doddington_Papineni dep_Doddington_2002 appos_ones_Doddington amod_ones_existing det_ones_the prep_of_extensions_ones conj_and_improvements_extensions amod_improvements_proposing det_improvements_some amod_measures_new dep_proposing_extensions dep_proposing_improvements dobj_proposing_measures nsubj_proposing_some prep_of_some_them nn_output_translation nn_output_machine prep_for_measures_output nn_measures_evaluation amod_measures_automatic amod_measures_various prep_with_dealing_measures vmod_publications_dealing prep_of_number_publications det_number_a nsubj_is_number expl_is_There parataxis_Work_proposing rcmod_Work_is amod_Work_Related num_Work_2
W06-3102	P02-1040	o	Although the BLEU -LRB- Papineni et al. 2002 -RRB- score from Finnish to English is 21.8 the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores -LRB- Koehn 2005 -RRB-	amod_Koehn_2005 dep_scores_Koehn nn_scores_languages amod_scores_European num_scores_11 prep_in_scores_scores amod_scores_lowest det_scores_the prep_of_one_scores cop_one_is nsubj_one_which rcmod_13.0_one prep_as_reported_13.0 auxpass_reported_is nsubjpass_reported_21.8 amod_direction_reverse det_direction_the prep_in_score_direction det_score_the appos_21.8_score cop_21.8_is dep_21.8_score prep_to_score_English prep_from_score_Finnish nsubj_score_BLEU mark_score_Although dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_Papineni det_BLEU_the
W06-3103	P02-1040	o	5.2 Evaluation Metrics The commonly used criteria to evaluate the translation results in the machine translation community are WER -LRB- word error rate -RRB- PER -LRB- positionindependent word error rate -RRB- BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_Papineni nn_rate_error nn_rate_word amod_rate_positionindependent appos_PER_rate nn_rate_error nn_rate_word conj_and_WER_NIST conj_and_WER_BLEU appos_WER_PER appos_WER_rate dep_are_NIST dep_are_BLEU dep_are_WER nn_community_translation nn_community_machine det_community_the prep_in_results_community nsubj_results_criteria det_translation_the dobj_evaluate_translation aux_evaluate_to vmod_criteria_evaluate amod_criteria_used det_criteria_The advmod_used_commonly dep_Metrics_are rcmod_Metrics_results nn_Metrics_Evaluation num_Metrics_5.2
W06-3106	P02-1040	o	Two error rates the sentence error rate -LRB- SER -RRB- and the word error rate -LRB- WER -RRB- that we seek to minimize and BLEU -LRB- Papineni et al. 2002 -RRB- that we seek to maximize	aux_maximize_to xcomp_seek_maximize nsubj_seek_we dobj_seek_that amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_Papineni aux_minimize_to xcomp_seek_minimize nsubj_seek_we dobj_seek_that rcmod_rate_seek appos_rate_WER nn_rate_error nn_rate_word det_rate_the rcmod_rate_seek conj_and_rate_BLEU conj_and_rate_rate appos_rate_SER nn_rate_error nn_rate_sentence det_rate_the dep_rates_BLEU dep_rates_rate dep_rates_rate nn_rates_error num_rates_Two dep_``_rates
W06-3108	P02-1040	o	5.3 Translation Results For the translation experiments on the BTEC task we report the two accuracy measures BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- as well as the two error rates word error rate -LRB- WER -RRB- and position-independent word error rate -LRB- PER -RRB-	appos_rate_PER nn_rate_error nn_rate_word amod_rate_position-independent conj_and_rate_rate appos_rate_WER nn_rate_error nn_rate_word nn_rates_error num_rates_two det_rates_the dep_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_rates conj_and_BLEU_NIST dep_BLEU_Papineni dep_measures_rate dep_measures_rate dep_measures_rates dep_measures_NIST dep_measures_BLEU dep_accuracy_measures dep_two_accuracy dep_the_two dobj_report_the nsubj_report_we nsubj_report_Results nn_task_BTEC det_task_the prep_on_experiments_task nn_experiments_translation det_experiments_the prep_for_Results_experiments nn_Results_Translation num_Results_5.3
W06-3110	P02-1040	o	To measure the translation quality we use the BLEU score -LRB- Papineni et al. 2002 -RRB- and the NIST score -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_score_Doddington nn_score_NIST det_score_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_score_score dep_score_Papineni nn_score_BLEU det_score_the dobj_use_score dobj_use_score nsubj_use_we advcl_use_measure nn_quality_translation det_quality_the dobj_measure_quality aux_measure_To
W06-3111	P02-1040	o	-LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_''_Papineni
W06-3112	P02-1040	n	Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level -LRB- Papineni et al. 2002 -RRB- a problem also noted by -LRB- Och et al. 2003 -RRB- and -LRB- Russo-Lassner et al. 2005 -RRB-	amod_Russo-Lassner_2005 dep_Russo-Lassner_al. nn_Russo-Lassner_et conj_and_Och_Russo-Lassner amod_Och_2003 dep_Och_al. nn_Och_et agent_noted_Russo-Lassner agent_noted_Och advmod_noted_also vmod_problem_noted det_problem_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_level_sentence det_level_the prep_at_judgment_level amod_judgment_human advmod_well_particularly prep_with_correlate_judgment advmod_correlate_well neg_correlate_not aux_correlate_may nsubj_correlate_it mark_correlate_that nn_point_BLEU prep_out_creators_problem dep_creators_Papineni ccomp_creators_correlate prep_of_creators_point det_creators_the advmod_creators_Even ccomp_``_creators
W06-3112	P02-1040	o	1 Introduction Since their appearance BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- have been the standard tools used for evaluating the quality of machine translation	nn_translation_machine prep_of_quality_translation det_quality_the dobj_evaluating_quality prepc_for_used_evaluating vmod_tools_used nn_tools_standard det_tools_the cop_tools_been aux_tools_have nsubj_tools_Introduction dep_Doddington_2002 appos_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni poss_appearance_their appos_Introduction_NIST appos_Introduction_BLEU prep_since_Introduction_appearance num_Introduction_1
W06-3121	P02-1040	o	The release has implementations for BLEU -LRB- Papineni et al. 2002 -RRB- WER and PER error criteria and it has decoding interfaces for Phramer and Pharaoh	conj_and_Phramer_Pharaoh prep_for_interfaces_Pharaoh prep_for_interfaces_Phramer dobj_decoding_interfaces aux_decoding_has nsubj_decoding_it nn_criteria_error nn_criteria_PER nn_criteria_WER conj_and_WER_PER conj_and_Papineni_decoding conj_and_Papineni_criteria amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_for_implementations_BLEU dep_has_decoding dep_has_criteria dep_has_Papineni dobj_has_implementations nsubj_has_release det_release_The
W06-3122	P02-1040	o	Although Phramer provides decoding functionality equivalent to Pharaohs we preferred to use Pharaoh for this task because it is much faster than Phramer between 2 and 15 times faster depending on the configuration and preliminary tests showed that there is no noticeable difference between the output of these two in terms of BLEU -LRB- Papineni et al. 2002 -RRB- score	dep_score_Papineni nn_score_BLEU dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_of_terms_score det_two_these prep_in_output_terms prep_of_output_two det_output_the prep_between_difference_output amod_difference_noticeable neg_difference_no nsubj_is_difference expl_is_there mark_is_that ccomp_showed_is nsubj_showed_much amod_tests_preliminary conj_and_configuration_tests det_configuration_the prep_depending_on_faster_tests prep_depending_on_faster_configuration npadvmod_faster_times num_times_15 num_times_2 conj_and_2_15 prepc_between_Phramer_faster prep_than_faster_Phramer advmod_much_faster cop_much_is nsubj_much_it mark_much_because det_task_this prep_for_use_task dobj_use_Pharaoh aux_use_to ccomp_preferred_showed xcomp_preferred_use nsubj_preferred_we advcl_preferred_provides prep_to_equivalent_Pharaohs nn_equivalent_functionality amod_equivalent_decoding dobj_provides_equivalent nsubj_provides_Phramer mark_provides_Although
W06-3508	P02-1040	o	What therefore has to be explored are various similarity metrics defining similarity in a concrete way and evaluate the results against human annotations -LRB- see Papineni et al. 2002 -RRB-	num_Papineni_2002 nn_Papineni_al. nn_Papineni_et dobj_see_Papineni amod_annotations_human prep_against_results_annotations det_results_the dobj_evaluate_results amod_way_concrete det_way_a prep_in_similarity_way dep_defining_see conj_and_defining_evaluate dobj_defining_similarity vmod_metrics_evaluate vmod_metrics_defining nn_metrics_similarity amod_metrics_various cop_metrics_are csubj_metrics_has auxpass_explored_be aux_explored_to xcomp_has_explored advmod_has_therefore nsubj_has_What
W07-0401	P02-1040	o	6.2 Translation Results For the translation experiments we report the two accuracy measures BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- as well as the two error rates word error rate -LRB- WER -RRB- and positionindependent word error rate -LRB- PER -RRB-	appos_rate_PER nn_rate_error nn_rate_word amod_rate_positionindependent conj_and_rate_rate appos_rate_WER nn_rate_error nn_rate_word nn_rate_rates nn_rate_error num_rate_two det_rate_the dep_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_rate conj_and_BLEU_rate conj_and_BLEU_NIST dep_BLEU_Papineni dep_measures_rate dep_measures_NIST dep_measures_BLEU dep_accuracy_measures dep_two_accuracy dep_the_two dobj_report_the nsubj_report_we nsubj_report_Results nn_experiments_translation det_experiments_the prep_for_Results_experiments nn_Results_Translation num_Results_6.2
W07-0403	P02-1040	o	Results on the provided 2000sentence development set are reported using the BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric det_BLEU_the dobj_using_BLEU xcomp_reported_using auxpass_reported_are nsubjpass_reported_Results nn_set_development nn_set_2000sentence amod_set_provided det_set_the prep_on_Results_set
W07-0409	P02-1040	o	2.2 Weight optimization A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score -LRB- Papineni et al. 2002 -RRB- on a development set -LRB- Och and Ney 2002 -RRB-	amod_Och_2002 conj_and_Och_Ney dep_set_Ney dep_set_Och nn_set_development det_set_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the prep_on_maximize_set dep_maximize_Papineni dobj_maximize_score aux_maximize_to xcomp_is_maximize nsubj_is_criterion nn_functions_feature prep_of_combination_functions amod_combination_log-linear det_combination_the prep_of_coefficients_combination det_coefficients_the dobj_optimize_coefficients aux_optimize_to vmod_criterion_optimize amod_criterion_common det_criterion_A nn_criterion_optimization nn_criterion_Weight num_criterion_2.2 ccomp_``_is
W07-0410	P02-1040	o	On the other hand both BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- scores are higher for the baseline system -LRB- mteval-v11b pl -RRB-	dep_mteval-v11b_pl dep_system_mteval-v11b nn_system_baseline det_system_the prep_for_higher_system cop_higher_are nsubj_higher_scores nsubj_higher_BLEU prep_on_higher_hand dep_scores_Doddington nn_scores_NIST num_Doddington_2002 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_scores dep_BLEU_Papineni preconj_BLEU_both amod_hand_other det_hand_the
W07-0411	P02-1040	n	Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_level_sentence det_level_the prep_at_judgment_level amod_judgment_human advmod_well_particularly prep_with_correlate_judgment advmod_correlate_well neg_correlate_not aux_correlate_may nsubj_correlate_it mark_correlate_that dep_out_Papineni ccomp_out_correlate nn_point_BLEU advmod_creators_out prep_of_creators_point det_creators_the advmod_creators_Even
W07-0411	P02-1040	o	1 Introduction Since their appearance string-based evaluation metrics such as BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- have been the standard tools used for evaluating MT quality	nn_quality_MT dobj_evaluating_quality prepc_for_used_evaluating vmod_tools_used nn_tools_standard det_tools_the cop_tools_been aux_tools_have nsubj_tools_appearance mark_tools_Since dep_Doddington_2002 appos_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni prep_such_as_metrics_NIST prep_such_as_metrics_BLEU nn_metrics_evaluation amod_metrics_string-based appos_appearance_metrics poss_appearance_their advcl_Introduction_tools num_Introduction_1
W07-0701	P02-1040	o	6 Experiments We evaluated the translation quality of the system using the BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_BLEU_metric det_BLEU_the dobj_using_BLEU det_system_the prep_of_quality_system nn_quality_translation det_quality_the xcomp_evaluated_using dobj_evaluated_quality nsubj_evaluated_We rcmod_Experiments_evaluated num_Experiments_6
W07-0703	P02-1040	o	BLEU -LRB- Papineni et al 2002 -RRB- was devised to provide automatic evaluation of MT output	amod_output_MT prep_of_evaluation_output amod_evaluation_automatic dobj_provide_evaluation aux_provide_to xcomp_devised_provide auxpass_devised_was nsubjpass_devised_BLEU amod_Papineni_2002 dep_Papineni_al nn_Papineni_et appos_BLEU_Papineni
W07-0704	P02-1040	o	We employ the phrase-based SMT framework -LRB- Koehn et al. 2003 -RRB- and use the Moses toolkit -LRB- Koehn et al. 2007 -RRB- and the SRILM language modelling toolkit -LRB- Stolcke 2002 -RRB- and evaluate our decoded translations using the BLEU measure -LRB- Papineni et al. 2002 -RRB- using a single reference translation	nn_translation_reference amod_translation_single det_translation_a dobj_using_translation nsubj_using_We amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_measure_Papineni nn_measure_BLEU det_measure_the dobj_using_measure amod_translations_decoded poss_translations_our xcomp_evaluate_using dobj_evaluate_translations nsubj_evaluate_We dep_Stolcke_2002 appos_toolkit_Stolcke amod_toolkit_modelling dep_language_toolkit nn_language_SRILM det_language_the amod_Koehn_2007 dep_Koehn_al. nn_Koehn_et dep_toolkit_Koehn nn_toolkit_Moses det_toolkit_the dobj_use_toolkit nsubj_use_We amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_framework_SMT amod_framework_phrase-based det_framework_the conj_and_employ_using conj_and_employ_evaluate conj_and_employ_language conj_and_employ_use dep_employ_Koehn dobj_employ_framework nsubj_employ_We
W07-0707	P02-1040	p	The most widely used are Word Error Rate -LRB- WER -RRB- Position independent word Error Rate -LRB- PER -RRB- the BLEU score -LRB- Papineni et al. 2002 -RRB- and the NIST score -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_score_Doddington nn_score_NIST det_score_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_score_score dep_score_Papineni nn_score_BLEU det_score_the appos_Rate_score appos_Rate_score appos_Rate_PER nn_Rate_Error nn_Rate_word amod_Rate_independent nn_Rate_Position dep_Rate_Rate appos_Rate_WER nn_Rate_Error nn_Rate_Word cop_Rate_are nsubj_Rate_used advmod_used_widely det_used_The advmod_widely_most
W07-0707	P02-1040	p	The BLEU metric -LRB- Papineni et al. 2002 -RRB- and the closely related NIST metric -LRB- Doddington 2002 -RRB- along with WER and PER 48 have been widely used by many machine translation researchers	nn_researchers_translation nn_researchers_machine amod_researchers_many agent_used_researchers advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_metric nsubjpass_used_Papineni advmod_used_metric num_PER_48 conj_and_WER_PER amod_Doddington_2002 dep_metric_Doddington nn_metric_NIST amod_metric_related det_metric_the advmod_related_closely conj_al._2002 nn_al._et pobj_Papineni_PER pobj_Papineni_WER prepc_along_with_Papineni_with conj_and_Papineni_metric dep_Papineni_al. dep_BLEU_used dep_The_BLEU dep_``_The
W07-0710	P02-1040	o	2.2.1 BLEU Evaluation The BLEU score -LRB- Papineni et al. 2002 -RRB- was defined to measure overlap between a hypothesized translation and a set of human references	amod_references_human prep_of_set_references det_set_a conj_and_translation_set amod_translation_hypothesized det_translation_a prep_between_overlap_set prep_between_overlap_translation nsubj_overlap_Evaluation aux_measure_to xcomp_defined_measure auxpass_defined_was nsubjpass_defined_score amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_score_Papineni nn_score_BLEU det_score_The rcmod_Evaluation_defined nn_Evaluation_BLEU num_Evaluation_2.2.1
W07-0710	P02-1040	p	1 Introduction In recent years statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation -LRB- Papineni et al. 2002 -RRB- and errorbased optimization -LRB- Och 2003 -RRB-	amod_Och_2003 dep_optimization_Och amod_optimization_errorbased amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_evaluation_automatic prep_to_thanks_evaluation nn_thanks_quality prep_in_leap_thanks nn_leap_quantum det_leap_a conj_and_experienced_optimization dep_experienced_Papineni dobj_experienced_leap aux_experienced_have nsubj_experienced_translation dep_experienced_Introduction nn_translation_machine amod_translation_statistical amod_years_recent prep_in_Introduction_years num_Introduction_1 ccomp_``_optimization ccomp_``_experienced
W07-0711	P02-1040	o	84 5.2 Machine translation on Europarl corpus We further tested our WDHMM on a phrase-based machine translation system to see whether our improvement on word alignment can also improve MT accuracy measured by BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU agent_measured_score vmod_accuracy_measured nn_accuracy_MT dobj_improve_accuracy advmod_improve_also aux_improve_can nsubj_improve_improvement mark_improve_whether nn_alignment_word prep_on_improvement_alignment poss_improvement_our ccomp_see_improve aux_see_to nn_system_translation nn_system_machine amod_system_phrase-based det_system_a poss_WDHMM_our vmod_tested_see prep_on_tested_system dobj_tested_WDHMM advmod_tested_further nsubj_tested_We nn_corpus_Europarl dep_translation_Papineni rcmod_translation_tested prep_on_translation_corpus nn_translation_Machine num_translation_5.2 num_translation_84 dep_``_translation
W07-0713	P02-1040	p	The most widely known are the Word Error Rate -LRB- WER -RRB- the Position independent word Error Rate -LRB- PER -RRB- the NIST score -LRB- Doddington 2002 -RRB- and especially in recent years the BLEU score -LRB- Papineni et al. 2002 -RRB- and the Translation Error Rate -LRB- TER -RRB- -LRB- Snover et al. 2005 -RRB-	amod_Snover_2005 dep_Snover_al. nn_Snover_et appos_Rate_TER nn_Rate_Error nn_Rate_Translation det_Rate_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni nn_score_BLEU det_score_the amod_years_recent dep_Doddington_2002 dep_score_Snover conj_and_score_Rate conj_and_score_score prep_in_score_years advmod_score_especially appos_score_Doddington nn_score_NIST det_score_the dep_Rate_Rate dep_Rate_score dep_Rate_score appos_Rate_PER nn_Rate_Error nn_Rate_word amod_Rate_independent nn_Rate_Position det_Rate_the dep_Rate_Rate dep_Rate_WER nn_Rate_Error nn_Rate_Word det_Rate_the cop_Rate_are nsubj_Rate_known advmod_known_widely det_known_The advmod_widely_most
W07-0714	P02-1040	n	Even the 3 A demo of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_level_sentence det_level_the prep_at_judgment_level amod_judgment_human advmod_well_particularly prep_with_correlate_judgment advmod_correlate_well neg_correlate_not aux_correlate_may nsubj_correlate_it mark_correlate_that dep_out_Papineni ccomp_out_correlate nn_point_BLEU prep_of_creators_point amod_creators_http://lfgdemo.computing.dcu.ie/lfgparser.html advmod_found_out prep_at_found_creators auxpass_found_be aux_found_can nsubjpass_found_demo det_parser_the prep_of_demo_parser nn_demo_A num_demo_3 det_demo_the advmod_demo_Even
W07-0714	P02-1040	o	1 Introduction Since the creation of BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- the subject of automatic evaluation metrics for MT has been given quite a lot of attention	prep_of_lot_attention det_lot_a advmod_lot_quite dobj_given_lot auxpass_given_been aux_given_has nsubjpass_given_subject dep_given_Introduction prep_for_metrics_MT nn_metrics_evaluation amod_metrics_automatic prep_of_subject_metrics det_subject_the dep_Doddington_2002 appos_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_NIST dep_BLEU_Papineni prep_of_creation_NIST prep_of_creation_BLEU det_creation_the prep_since_Introduction_creation num_Introduction_1 ccomp_``_given
W07-0715	P02-1040	o	Since this trade-off is also affected by the settings of various pruning parameters we compared decoding time and translation quality as measured by BLEU score -LRB- Papineni et al 2002 -RRB- for the two models on our first test set over a broad range of settings for the decoder pruning parameters	nn_parameters_pruning nn_parameters_decoder det_parameters_the prep_of_range_settings amod_range_broad det_range_a prep_for_set_parameters prep_over_set_range vmod_test_set amod_test_first poss_test_our prep_on_models_test num_models_two det_models_the amod_Papineni_2002 dep_Papineni_al nn_Papineni_et nn_score_BLEU prep_by_measured_score mark_measured_as nn_quality_translation nn_quality_time amod_quality_decoding conj_and_time_translation prep_for_compared_models dep_compared_Papineni advcl_compared_measured dobj_compared_quality nsubj_compared_we advcl_compared_affected nn_parameters_pruning amod_parameters_various prep_of_settings_parameters det_settings_the agent_affected_settings advmod_affected_also auxpass_affected_is nsubjpass_affected_trade-off mark_affected_Since det_trade-off_this
W07-0716	P02-1040	o	Och showed thatsystemperformanceisbestwhenparametersare optimizedusingthesameobjectivefunctionthatwill be used for evaluation BLEU -LRB- Papineni et al. 2002 -RRB- remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used e.g. -LRB- Banerjee and Lavie 2005 Snover et al. 2006 -RRB-	num_Snover_2006 nn_Snover_al. nn_Snover_et dep_Banerjee_Snover dep_Banerjee_2005 conj_and_Banerjee_Lavie dep_,_Lavie dep_,_Banerjee dep_,_e.g. auxpass_used_are nsubjpass_used_measures advmod_used_when nn_measures_evaluation amod_measures_alternative advmod_when_even nn_optimization_parameter advcl_retained_used prep_for_retained_optimization advmod_retained_often auxpass_retained_is nsubjpass_retained_BLEU det_purposes_both conj_and_remains_retained prep_for_remains_purposes acomp_remains_common nsubj_remains_BLEU amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_for_used_evaluation auxpass_used_be nsubjpass_used_optimizedusingthesameobjectivefunctionthatwill amod_optimizedusingthesameobjectivefunctionthatwill_thatsystemperformanceisbestwhenparametersare parataxis_showed_retained parataxis_showed_remains ccomp_showed_used nsubj_showed_Och
W07-0729	P02-1040	o	Translation scores are reported using caseinsensitive BLEU -LRB- Papineni et al. 2002 -RRB- with a single reference translation	nn_translation_reference amod_translation_single det_translation_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_BLEU_caseinsensitive prep_with_using_translation dobj_using_BLEU xcomp_reported_using auxpass_reported_are nsubjpass_reported_scores nn_scores_Translation
W07-0734	P02-1040	o	The most commonly used MT evaluation metric in recent years has been IBM?s Bleu metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_Bleu_Papineni amod_Bleu_metric dep_IBM?s_Bleu dep_been_IBM?s dep_has_been amod_years_recent prep_in_evaluation_years amod_evaluation_metric dobj_MT_evaluation dep_used_has dep_used_MT advmod_used_commonly advmod_used_most vmod_The_used dep_``_The
W07-0735	P02-1040	o	To further emphasize the importance of morphology in MT to Czech we compare the standard BLEU -LRB- Papineni et al. 2002 -RRB- of a baseline phrasebased translation with BLEU which disregards word forms -LRB- lemmatized MT output is compared to lemmatized reference translation -RRB-	nn_translation_reference amod_translation_lemmatized prep_to_compared_translation auxpass_compared_is nsubjpass_compared_output nn_output_MT amod_output_lemmatized rcmod_forms_compared nn_forms_word dobj_disregards_forms nsubj_disregards_which rcmod_BLEU_disregards prep_with_translation_BLEU amod_translation_phrasebased nn_translation_baseline det_translation_a amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_of_BLEU_translation dep_BLEU_Papineni amod_BLEU_standard det_BLEU_the dobj_compare_BLEU nsubj_compare_we advcl_compare_emphasize prep_to_importance_Czech prep_in_importance_MT prep_of_importance_morphology det_importance_the dobj_emphasize_importance advmod_emphasize_further aux_emphasize_To
W07-0737	P02-1040	o	We further assume that the degree of difficulty of a phrase is directly correlated with the quality of the translation produced by the MT system which can be approximated using an automatic evaluation metric such as BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni prep_such_as_metric_BLEU npadvmod_metric_evaluation amod_metric_automatic det_metric_an dobj_using_metric xcomp_approximated_using auxpass_approximated_be aux_approximated_can nsubjpass_approximated_which rcmod_system_approximated nn_system_MT det_system_the agent_produced_system vmod_translation_produced det_translation_the prep_of_quality_translation det_quality_the prep_with_correlated_quality advmod_correlated_directly auxpass_correlated_is nsubjpass_correlated_degree mark_correlated_that det_phrase_a prep_of_difficulty_phrase prep_of_degree_difficulty det_degree_the ccomp_assume_correlated advmod_assume_further nsubj_assume_We
W08-0301	P02-1040	o	-LRB- Case-insensitive -RRB- BLEU-4 -LRB- Papineni et al. 2002 -RRB- is used as the evaluation metric	nn_metric_evaluation det_metric_the prep_as_used_metric auxpass_used_is nsubjpass_used_BLEU-4 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU-4_Papineni dep_BLEU-4_Case-insensitive
W08-0302	P02-1040	o	Evaluation We evaluate translation output using three automatic evaluation measures BLEU -LRB- Papineni et al. 2002 -RRB- NIST -LRB- Doddington 2002 -RRB- and METEOR -LRB- Banerjee and Lavie 2005 version 0.6 -RRB- .5 All measures used were the case-sensitive corpuslevel versions	amod_versions_corpuslevel amod_versions_case-sensitive det_versions_the cop_versions_were nsubj_versions_measures nsubj_versions_NIST nsubj_versions_BLEU det_measures_All num_measures_.5 nn_measures_METEOR num_version_0.6 appos_Banerjee_version amod_Banerjee_2005 conj_and_Banerjee_Lavie dep_METEOR_Lavie dep_METEOR_Banerjee dep_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et vmod_BLEU_used conj_and_BLEU_measures conj_and_BLEU_NIST dep_BLEU_Papineni dep_measures_versions nn_measures_evaluation amod_measures_automatic num_measures_three dobj_using_measures nn_output_translation xcomp_evaluate_using dobj_evaluate_output nsubj_evaluate_We rcmod_Evaluation_evaluate
W08-0302	P02-1040	o	The weights 1 M are typically learned to directly minimize a standard evaluation criterion on development data -LRB- e.g. the BLEU score Papineni et al. -LRB- 2002 -RRB- -RRB- using numerical search -LRB- Och 2003 -RRB-	amod_Och_2003 dep_search_Och amod_search_numerical dobj_using_search nn_al._et nn_al._Papineni nn_score_BLEU det_score_the appos_e.g._2002 dep_e.g._al. appos_e.g._score dep_data_e.g. nn_data_development prep_on_criterion_data nn_criterion_evaluation amod_criterion_standard det_criterion_a dobj_minimize_criterion advmod_minimize_directly aux_minimize_to xcomp_learned_using xcomp_learned_minimize advmod_learned_typically auxpass_learned_are nsubjpass_learned_M rcmod_weights_learned num_weights_1 det_weights_The dep_``_weights
W08-0304	P02-1040	o	This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric such as the BLEU score -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_Papineni nn_score_BLEU det_score_the prep_such_as_evaluation_score amod_evaluation_metric nn_evaluation_translation amod_evaluation_automatic det_evaluation_an dobj_optimizing_evaluation nn_quality_translation prepc_by_improve_optimizing dobj_improve_quality aux_improve_to xcomp_attempts_improve nsubj_attempts_approach det_approach_This
W08-0306	P02-1040	o	BLEU For all translation tasks we report caseinsensitive NIST BLEU scores -LRB- Papineni et al. 2002 -RRB- using 4 references per sentence	prep_per_references_sentence num_references_4 dobj_using_references amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et vmod_scores_using appos_scores_Papineni nn_scores_BLEU nn_scores_NIST amod_scores_caseinsensitive dobj_report_scores nsubj_report_we nsubj_report_BLEU nn_tasks_translation det_tasks_all prep_for_BLEU_tasks
W08-0307	P02-1040	o	5http / / opennlp.sourceforge.net / We use the standard four-reference NIST MTEval data sets for the years 2003 2004 and 2005 -LRB- henceforth MT03 MT04 and MT05 respectively -RRB- for testing and the 2002 data set for tuning .6 BLEU4 -LRB- Papineni et al. 2002 -RRB- METEOR -LRB- Banerjee and Lavie 2005 -RRB- and multiple-reference Word Error Rate scores are reported	auxpass_reported_are nsubjpass_reported_5http nn_scores_Rate nn_scores_Error nn_scores_Word amod_scores_multiple-reference dep_Banerjee_2005 conj_and_Banerjee_Lavie dep_METEOR_Lavie dep_METEOR_Banerjee amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU4_scores conj_and_BLEU4_METEOR dep_BLEU4_Papineni num_BLEU4_.6 dep_tuning_scores dep_tuning_METEOR dep_tuning_BLEU4 prep_for_set_tuning vmod_data_set num_data_2002 det_data_the conj_and_testing_data conj_and_MT03_MT05 conj_and_MT03_MT04 nn_MT03_henceforth conj_and_2004_2005 num_years_2003 det_years_the nn_sets_data nn_sets_MTEval nn_sets_NIST amod_sets_four-reference amod_sets_standard det_sets_the prep_for_use_years dobj_use_sets nsubj_use_We advmod_opennlp.sourceforge.net_respectively dep_opennlp.sourceforge.net_MT05 dep_opennlp.sourceforge.net_MT04 dep_opennlp.sourceforge.net_MT03 dep_opennlp.sourceforge.net_2005 dep_opennlp.sourceforge.net_2004 rcmod_opennlp.sourceforge.net_use prep_for_5http_data prep_for_5http_testing dep_5http_opennlp.sourceforge.net
W08-0308	P02-1040	o	TheChinesesentencefromtheselected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_BLEU-4_word-based prep_with_system_BLEU-4 nn_system_MT det_system_the dobj_evaluate_system conj_and_tune_evaluate aux_tune_to vmod_reference_evaluate vmod_reference_tune amod_reference_single det_reference_the dep_used_Papineni prep_as_used_reference auxpass_used_is nsubjpass_used_pair nn_pair_TheChinesesentencefromtheselected
W08-0309	P02-1040	o	The automatic metrics that were evaluated in this years shared task were the following Bleu -LRB- Papineni et al. 2002 -RRB- Bleu remains the de facto standard in machine translation evaluation	nn_evaluation_translation nn_evaluation_machine prep_in_standard_evaluation nn_standard_facto nn_standard_de det_standard_the xcomp_remains_standard nsubj_remains_Bleu dep_Bleu_Papineni nn_Bleu_Bleu amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_following_remains det_following_the cop_following_were nsubj_following_task tmod_following_years mark_following_in amod_task_shared det_years_this advcl_evaluated_following auxpass_evaluated_were nsubjpass_evaluated_that rcmod_metrics_evaluated amod_metrics_automatic det_metrics_The dep_``_metrics
W08-0312	P02-1040	p	3 Extending Bleu and Ter with Flexible Matching Many widely used metrics like Bleu -LRB- Papineni et al. 2002 -RRB- and Ter -LRB- Snover et al. 2006 -RRB- are based on measuring string level similarity between the reference translation and translation hypothesis just like Meteor Most of them however depend on finding exact matches between the words in two strings	num_strings_two prep_in_words_strings det_words_the prep_between_matches_words amod_matches_exact dobj_finding_matches prep_on_depend_finding advmod_depend_however nsubj_depend_Most dep_depend_Meteor advmod_depend_like prep_of_Most_them advmod_like_just nn_hypothesis_translation conj_and_translation_hypothesis nn_translation_reference det_translation_the prep_between_similarity_hypothesis prep_between_similarity_translation nn_similarity_level nn_similarity_string dobj_measuring_similarity dep_based_depend prepc_on_based_measuring auxpass_based_are nsubjpass_based_Ter nsubjpass_based_Bleu amod_Snover_2006 dep_Snover_al. nn_Snover_et amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_Bleu_Ter dep_Bleu_Papineni appos_metrics_Snover prep_like_metrics_Ter prep_like_metrics_Bleu amod_metrics_used amod_metrics_Many advmod_used_widely dobj_Matching_metrics xcomp_Flexible_Matching prep_with_Ter_Flexible conj_and_Bleu_Ter amod_Bleu_Extending num_Bleu_3
W08-0312	P02-1040	o	The most commonly used MT evaluation metric in recent years has been IBMs Bleu metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. amod_Bleu_Papineni amod_Bleu_metric dep_IBMs_Bleu dep_been_IBMs dep_has_been amod_years_recent prep_in_evaluation_years amod_evaluation_metric dobj_MT_evaluation dep_used_has dep_used_MT advmod_used_commonly advmod_used_most vmod_The_used dep_``_The
W08-0317	P02-1040	o	De-En En-De Baseline 26.95 20.16 Factored baseline 27.43 20.27 Submitted system 27.63 20.46 Table 1 Bleu scores for Europarl -LRB- test2007 -RRB- De-En En-De Baseline 19.54 14.31 Factored baseline 20.16 14.37 Submitted system 20.61 14.77 Table 2 Bleu scores for News Commentary -LRB- nc-test2007 -RRB- 5 Results Case-sensitive Bleu scores4 -LRB- Papineni et al. 2002 -RRB- for the Europarl devtest set -LRB- test2007 -RRB- are shown in table 1	num_table_1 prep_in_shown_table auxpass_shown_are nsubjpass_shown_scores appos_set_test2007 amod_set_devtest nn_set_Europarl det_set_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_for_scores4_set appos_scores4_Papineni nn_scores4_Bleu amod_scores4_Case-sensitive dep_Results_scores4 num_Results_5 dep_Commentary_Results dep_Commentary_nc-test2007 nn_Commentary_News prep_for_scores_Commentary nn_scores_Bleu num_Table_2 num_Table_14.77 num_Table_20.61 dep_system_Table amod_system_Submitted num_system_14.37 number_14.37_20.16 prep_baseline_system dep_Factored_baseline dep_14.31_Factored number_14.31_19.54 dep_Baseline_14.31 nn_Baseline_En-De amod_Baseline_De-En nn_Baseline_Europarl appos_Europarl_test2007 dep_scores_shown prep_for_scores_Baseline nn_scores_Bleu num_Table_1 num_Table_20.46 num_Table_27.63 dep_system_Table amod_system_Submitted num_system_20.27 number_20.27_27.43 prep_baseline_system dep_Factored_baseline dep_20.16_Factored number_20.16_26.95 dep_Baseline_scores dep_Baseline_20.16 nn_Baseline_En-De amod_Baseline_De-En
W08-0320	P02-1040	o	We used these weights in a beam search decoder to produce translations for the test sentences which we compared to the WMT07 gold standard using Bleu -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni dobj_using_Bleu vmod_standard_using nn_standard_gold nn_standard_WMT07 det_standard_the prep_to_compared_standard nsubj_compared_we dobj_compared_which rcmod_sentences_compared nn_sentences_test det_sentences_the prep_for_translations_sentences dobj_produce_translations aux_produce_to nn_decoder_search nn_decoder_beam det_decoder_a det_weights_these vmod_used_produce prep_in_used_decoder dobj_used_weights nsubj_used_We
W08-0321	P02-1040	o	Table 2 shows results in lowercase BLEU -LRB- Papineni et al. 2002 -RRB- for both the baseline -LRB- B -RRB- and the improved baseline systems -LRB- B5 -RRB- on development and held151 out evaluation sets	nn_sets_evaluation conj_and_development_held151 prep_on_systems_held151 prep_on_systems_development appos_systems_B5 nn_systems_baseline amod_systems_improved det_systems_the prep_out_baseline_sets conj_and_baseline_systems appos_baseline_B det_baseline_the preconj_baseline_both amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_BLEU_lowercase prep_for_results_systems prep_for_results_baseline prep_in_results_BLEU nsubj_results_shows num_shows_2 nn_shows_Table
W08-0322	P02-1040	o	The results evaluated by BLEU score -LRB- Papineni et al. 2002 -RRB- is shown in Table 2	num_Table_2 prep_in_shown_Table auxpass_shown_is nsubjpass_shown_results amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU agent_evaluated_score dep_results_Papineni vmod_results_evaluated det_results_The
W08-0324	P02-1040	o	3 Evaluation We trained our model parameters on a subset of the provided dev2006 development set optimizing for case-insensitive IBM-style BLEU -LRB- Papineni et al. 2002 -RRB- with several iterations of minimum error rate training on n-best lists	amod_lists_n-best nn_training_rate nn_training_error amod_training_minimum prep_of_iterations_training amod_iterations_several amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_BLEU_IBM-style amod_BLEU_case-insensitive prep_on_optimizing_lists prep_with_optimizing_iterations dep_optimizing_Papineni prep_for_optimizing_BLEU nn_set_development nn_set_dev2006 amod_set_provided det_set_the prep_of_subset_set det_subset_a nn_parameters_model poss_parameters_our prep_on_trained_subset dobj_trained_parameters nsubj_trained_We vmod_Evaluation_optimizing rcmod_Evaluation_trained num_Evaluation_3 dep_``_Evaluation
W08-0324	P02-1040	o	We report case-insensitive scores for version 0.6 of METEOR -LRB- Lavie and Agarwal 2007 -RRB- with all modules enabled version 1.04 of IBM-style BLEU -LRB- Papineni et al. 2002 -RRB- and version 5 of TER -LRB- Snover et al. 2006 -RRB-	amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_version_Snover prep_of_version_TER num_version_5 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_BLEU_IBM-style dep_version_Papineni prep_of_version_BLEU num_version_1.04 conj_and_enabled_version dobj_enabled_version nsubj_enabled_scores det_modules_all dep_Lavie_2007 conj_and_Lavie_Agarwal appos_METEOR_Agarwal appos_METEOR_Lavie prep_with_version_modules prep_of_version_METEOR num_version_0.6 prep_for_scores_version amod_scores_case-insensitive ccomp_report_version ccomp_report_enabled nsubj_report_We
W08-0328	P02-1040	o	Table 1 shows the evaluation of all the systems in terms of BLEU score -LRB- Papineni et al. 2002 -RRB- with the best score highlighted	nsubj_highlighted_Papineni dep_highlighted_shows amod_score_best det_score_the prep_with_Papineni_score amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU prep_of_terms_score det_systems_the predet_systems_all prep_in_evaluation_terms prep_of_evaluation_systems det_evaluation_the dobj_shows_evaluation nsubj_shows_Table num_Table_1
W08-0329	P02-1040	o	The translation quality is measured by three MT evaluation metrics TER -LRB- Snover et al. 2006 -RRB- BLEU -LRB- Papineni et al. 2002 -RRB- and METEOR -LRB- Lavie and Agarwal 2007 -RRB-	amod_Lavie_2007 conj_and_Lavie_Agarwal dep_METEOR_Agarwal dep_METEOR_Lavie amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_Snover_2006 dep_Snover_al. nn_Snover_et conj_and_TER_METEOR conj_and_TER_BLEU dep_TER_Snover dep_metrics_METEOR dep_metrics_BLEU dep_metrics_TER nn_metrics_evaluation nn_metrics_MT num_metrics_three agent_measured_metrics auxpass_measured_is nsubjpass_measured_quality nn_quality_translation det_quality_The
W08-0401	P02-1040	o	4 5 Experiments 5.1 Evaluation Measures We evaluated the proposed method using four evaluation measures BLEU -LRB- Papineni et al. 2002 -RRB- NIST -LRB- Doddington 2002 -RRB- WER -LRB- word error rate -RRB- and PER -LRB- position independent word error rate -RRB-	nn_rate_error nn_rate_word amod_rate_independent nn_rate_position nn_rate_PER nn_rate_error nn_rate_word appos_WER_rate num_Doddington_2002 appos_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_rate conj_and_BLEU_WER conj_and_BLEU_NIST dep_BLEU_Papineni nn_measures_evaluation num_measures_four dobj_using_measures appos_method_rate appos_method_WER appos_method_NIST appos_method_BLEU vmod_method_using amod_method_proposed det_method_the dobj_evaluated_method nsubj_evaluated_We rcmod_Measures_evaluated nn_Measures_Evaluation num_Measures_5.1 nn_Measures_Experiments num_Measures_5 num_Measures_4 dep_``_Measures
W08-0402	P02-1040	o	As shown in Table 1 the JAVA decoder -LRB- without explicit parallelization -RRB- is 22 times faster than the PYTHON decoder while achieving slightly better translation quality as measured by BLEU-4 -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_by_measured_BLEU-4 mark_measured_as nn_quality_translation amod_quality_better advmod_better_slightly advcl_achieving_measured dobj_achieving_quality mark_achieving_while nn_decoder_PYTHON det_decoder_the dep_faster_Papineni advcl_faster_achieving prep_than_faster_decoder npadvmod_faster_times cop_faster_is nsubj_faster_decoder advcl_faster_shown num_times_22 amod_parallelization_explicit prep_without_decoder_parallelization nn_decoder_JAVA det_decoder_the num_Table_1 prep_in_shown_Table mark_shown_As
W08-0405	P02-1040	o	Translation results are given in terms of the automaticBLEUevaluation metric -LRB- Papineni et al. 2002 -RRB- as well as the TER metric -LRB- Snover et al. 2006 -RRB-	dep_al._2006 nn_al._et advmod_Snover_al. dep_metric_Snover amod_TER_metric det_TER_the dep_al._2002 nn_al._et advmod_Papineni_al. conj_and_metric_TER dep_metric_Papineni amod_automaticBLEUevaluation_TER amod_automaticBLEUevaluation_metric det_automaticBLEUevaluation_the prep_of_terms_automaticBLEUevaluation prep_in_given_terms auxpass_given_are nsubjpass_given_results nn_results_Translation
W08-0409	P02-1040	o	The translation output is measured using BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dobj_using_BLEU xcomp_measured_using auxpass_measured_is nsubjpass_measured_output nn_output_translation det_output_The
W08-0509	P02-1040	o	To compare the performance of system we recorded the total training time and the BLEU score which is a standard automatic measurement of the translation quality -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_quality_translation det_quality_the prep_of_measurement_quality amod_measurement_automatic amod_measurement_standard det_measurement_a cop_measurement_is nsubj_measurement_which dep_score_Papineni rcmod_score_measurement nn_score_BLEU det_score_the conj_and_time_score nn_time_training amod_time_total det_time_the dobj_recorded_score dobj_recorded_time nsubj_recorded_we advcl_recorded_compare prep_of_performance_system det_performance_the dobj_compare_performance aux_compare_To
W08-0903	P02-1040	o	A summary of the differences between our proposed approach and that of -LRB- Papineni et al. 2002 -RRB- would include The reliance of BLEU on the diversity of multiple reference translations in order to capture some of the acceptable alternatives in both word choice and word ordering that we have shown above	advmod_shown_above aux_shown_have nsubj_shown_we mark_shown_that ccomp_ordering_shown vmod_word_ordering conj_and_choice_word nn_choice_word preconj_choice_both prep_in_alternatives_word prep_in_alternatives_choice amod_alternatives_acceptable det_alternatives_the prep_of_some_alternatives dobj_capture_some aux_capture_to dep_capture_order mark_capture_in nn_translations_reference amod_translations_multiple prep_of_diversity_translations det_diversity_the advcl_reliance_capture prep_on_reliance_diversity prep_of_reliance_BLEU det_reliance_The dep_include_reliance aux_include_would nsubj_include_of dobj_include_that amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_of_Papineni conj_and_approach_include amod_approach_proposed poss_approach_our prep_between_differences_include prep_between_differences_approach det_differences_the prep_of_summary_differences det_summary_A
W08-0903	P02-1040	o	Techniques that analyze n-gram precision such as BLEU score -LRB- Papineni et al. 2002 -RRB- have been developed with the goal of comparing candidate translations against references provided by human experts in order to determine accuracy although in our application the candidate translator is a student and not a machine the principle is the same and we wish to adapt their technique to our context	poss_context_our poss_technique_their prep_to_adapt_context dobj_adapt_technique aux_adapt_to xcomp_wish_adapt nsubj_wish_we conj_and_same_wish det_same_the cop_same_is nsubj_same_principle prep_in_same_application mark_same_although det_principle_the det_machine_a neg_machine_not conj_and_student_machine det_student_a cop_student_is nsubj_student_translator nn_translator_candidate det_translator_the rcmod_application_machine rcmod_application_student poss_application_our dobj_determine_accuracy aux_determine_to dep_determine_order mark_determine_in amod_experts_human advcl_provided_determine agent_provided_experts vmod_references_provided nn_translations_candidate prep_against_comparing_references dobj_comparing_translations prepc_of_goal_comparing det_goal_the advcl_developed_wish advcl_developed_same prep_with_developed_goal auxpass_developed_been aux_developed_have nsubjpass_developed_Techniques amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_score_Papineni nn_score_BLEU prep_such_as_precision_score amod_precision_n-gram dobj_analyze_precision nsubj_analyze_that rcmod_Techniques_analyze
W08-1112	P02-1040	o	Following -LRB- Langkilde 2002 -RRB- and other work on general-purpose generators we adopt BLEU score -LRB- Papineni et al. 2002 -RRB- average simple string accuracy -LRB- SSA -RRB- and percentage of exactly matched sentences for accuracy evaluation .6 For coverage evaluation we measure the percentage of input fstructures that generate a sentence	det_sentence_a dobj_generate_sentence nsubj_generate_that rcmod_fstructures_generate nn_fstructures_input prep_of_percentage_fstructures det_percentage_the dobj_measure_percentage nsubj_measure_we ccomp_measure_adopt nn_evaluation_coverage nn_.6_evaluation nn_.6_accuracy prep_for_sentences_.6 amod_sentences_matched advmod_matched_exactly prep_of_percentage_sentences prep_for_accuracy_evaluation conj_and_accuracy_percentage appos_accuracy_SSA nn_accuracy_string amod_accuracy_simple amod_accuracy_average amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_score_BLEU dobj_adopt_percentage dobj_adopt_accuracy dep_adopt_Papineni dobj_adopt_score nsubj_adopt_we vmod_adopt_work vmod_adopt_Following amod_generators_general-purpose prep_on_work_generators amod_work_other amod_Langkilde_2002 conj_and_Following_work dep_Following_Langkilde
W08-1113	P02-1040	o	Such metrics have been introduced in other fields including PARADISE -LRB- Walker et al. 1997 -RRB- for spoken dialogue systems BLEU -LRB- Papineni et al. 2002 -RRB- for machine translation ,1 and ROUGE -LRB- Lin 2004 -RRB- for summarisation	dep_Lin_2004 appos_ROUGE_Lin conj_and_,1_ROUGE nn_,1_translation nn_,1_machine amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_for_BLEU_summarisation prep_for_BLEU_ROUGE prep_for_BLEU_,1 dep_BLEU_Papineni nn_systems_dialogue amod_systems_spoken amod_Walker_1997 dep_Walker_al. nn_Walker_et dep_PARADISE_Walker prep_including_fields_PARADISE amod_fields_other dep_introduced_BLEU prep_for_introduced_systems prep_in_introduced_fields auxpass_introduced_been aux_introduced_have nsubjpass_introduced_metrics amod_metrics_Such
W08-2118	P02-1040	o	To optimize the parameters of the decoder we performed minimum error rate training on IWSLT04 optimizing for the IBM-BLEU metric -LRB- Papineni et al. 2002 -RRB-	dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni amod_IBM-BLEU_metric det_IBM-BLEU_the prep_for_optimizing_IBM-BLEU vmod_IWSLT04_optimizing prep_on_training_IWSLT04 nn_training_rate nn_training_error amod_training_minimum dobj_performed_training nsubj_performed_we advcl_performed_optimize det_decoder_the prep_of_parameters_decoder det_parameters_the dobj_optimize_parameters aux_optimize_To ccomp_``_performed
W09-0401	P02-1040	o	In this years shared task we evaluated a number of different automatic metrics Bleu -LRB- Papineni et al. 2002 -RRB- Bleu remains the de facto standard in machine translation evaluation	nn_evaluation_translation nn_evaluation_machine prep_in_standard_evaluation nn_standard_facto nn_standard_de det_standard_the xcomp_remains_standard nsubj_remains_Bleu nn_Bleu_Bleu amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_Bleu_Papineni amod_metrics_automatic amod_metrics_different prep_of_number_metrics det_number_a dobj_evaluated_number nsubj_evaluated_we dep_task_remains rcmod_task_evaluated amod_task_shared dep_years_task det_years_this pobj_In_years dep_``_In
W09-0402	P02-1040	o	2 Syntactic-oriented evaluation metrics We investigated the following metrics oriented on the syntactic structure of a translation output POSBLEU The standard BLEU score -LRB- Papineni et al. 2002 -RRB- calculated on POS tags instead of words POSP POS n-gram precision percentage of POS ngrams in the hypothesis which have a counterpart in the reference POSR Recall measure based on POS n-grams percentage of POS n-grams in the reference which are also present in the hypothesis POSF POS n-gram based F-measure takes into account all POS n-grams which have a counter29 part both in the reference and in the hypothesis	det_hypothesis_the conj_and_reference_hypothesis det_reference_the nn_part_counter29 det_part_a dobj_have_part nsubj_have_which rcmod_n-grams_have nn_n-grams_POS det_n-grams_all prep_in_takes_hypothesis prep_in_takes_reference preconj_takes_both dobj_takes_n-grams prep_into_takes_account amod_F-measure_based nn_F-measure_n-gram nn_n-gram_POS nn_n-gram_POSF det_hypothesis_the prep_in_present_hypothesis advmod_present_also cop_present_are nsubj_present_which rcmod_reference_present det_reference_the nn_n-grams_POS dep_percentage_takes dep_percentage_F-measure prep_in_percentage_reference prep_of_percentage_n-grams nn_n-grams_POS prep_on_based_n-grams vmod_measure_based dobj_Recall_measure vmod_POSR_Recall det_reference_the prep_in_counterpart_reference det_counterpart_a dobj_have_counterpart nsubj_have_which rcmod_hypothesis_have det_hypothesis_the nn_ngrams_POS prep_in_percentage_hypothesis prep_of_percentage_ngrams dep_precision_percentage nn_precision_n-gram nn_precision_POS nn_precision_POSP prep_instead_of_tags_words nn_tags_POS prep_on_calculated_tags amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_score_percentage dep_score_POSR dep_score_precision vmod_score_calculated appos_score_Papineni nn_score_BLEU amod_score_standard nn_score_The nn_score_POSBLEU nn_output_translation det_output_a prep_of_structure_output amod_structure_syntactic det_structure_the prep_on_oriented_structure vmod_metrics_oriented prep_following_the_metrics dobj_investigated_the nsubj_investigated_We dep_metrics_score rcmod_metrics_investigated nn_metrics_evaluation amod_metrics_Syntactic-oriented num_metrics_2 dep_``_metrics
W09-0403	P02-1040	p	After a brief period following the introduction of generally accepted and widely used metrics BLEU -LRB- Papineni et al. 2002 -RRB- and NIST -LRB- Doddington 2002 -RRB- when it seemed that this persistent problem has finally been solved the researchers active in the field of machine translation -LRB- MT -RRB- started to express their worries that although these metrics are simple fast and able to provide consistent results for a particular system during its development they are not sufficiently reliable for the comparison of different systems or different language pairs	nn_pairs_language amod_pairs_different conj_or_systems_pairs amod_systems_different prep_of_comparison_pairs prep_of_comparison_systems det_comparison_the prep_for_reliable_comparison advmod_reliable_sufficiently neg_reliable_not cop_reliable_are nsubj_reliable_they poss_development_its prep_during_system_development amod_system_particular det_system_a prep_for_results_system amod_results_consistent dobj_provide_results aux_provide_to xcomp_fast_provide conj_and_fast_able advcl_fast_simple mark_fast_that cop_simple_are nsubj_simple_metrics mark_simple_although det_metrics_these ccomp_worries_able ccomp_worries_fast poss_worries_their dobj_express_worries aux_express_to ccomp_started_reliable xcomp_started_express nsubj_started_researchers advcl_started_seemed prep_after_started_period appos_translation_MT nn_translation_machine prep_of_field_translation det_field_the prep_in_active_field amod_researchers_active det_researchers_the auxpass_solved_been advmod_solved_finally aux_solved_has nsubjpass_solved_problem mark_solved_that amod_problem_persistent det_problem_this ccomp_seemed_solved nsubj_seemed_it advmod_seemed_when dep_Doddington_2002 appos_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni conj_and_metrics_NIST conj_and_metrics_BLEU advmod_used_widely dobj_accepted_NIST dobj_accepted_BLEU dobj_accepted_metrics conj_and_accepted_used advmod_accepted_generally prepc_of_introduction_used prepc_of_introduction_accepted det_introduction_the prep_following_period_introduction amod_period_brief det_period_a
W09-0404	P02-1040	o	We combine different parametrization of -LRB- smoothed -RRB- BLEU -LRB- Papineni et al. 2002 -RRB- NIST -LRB- Doddington 2002 -RRB- and TER -LRB- Snover et al. 2006 -RRB- to give a total of roughly 100 features	num_features_100 quantmod_100_roughly prep_of_total_features det_total_a dobj_give_total aux_give_to amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_Doddington_2002 vmod_NIST_give dep_NIST_Snover conj_and_NIST_TER dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_smoothed prep_of_parametrization_BLEU amod_parametrization_different dobj_combine_TER dobj_combine_NIST dep_combine_Papineni dobj_combine_parametrization nsubj_combine_We
W09-0404	P02-1040	o	BLEU -LRB- Papineni et al. 2002 -RRB- NIST -LRB- Doddington 2002 -RRB-	amod_Doddington_2002 dep_NIST_Doddington amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_NIST dep_BLEU_Papineni
W09-0407	P02-1040	o	For the WMT 2009 Workshop we selected a linear combination of BLEU -LRB- Papineni et al. 2002 -RRB- and TER -LRB- Snover et al. 2006 -RRB- as optimization criterion = argmax -LCB- -LRB- 2BLEU -RRB- TER -RCB- based on previous experience -LRB- Mauser et al. 2008 -RRB-	amod_Mauser_2008 dep_Mauser_al. nn_Mauser_et amod_experience_previous dep_TER_2BLEU appos_argmax_TER dep_=_Mauser prep_based_on_=_experience dep_=_argmax nn_criterion_optimization amod_Snover_2006 dep_Snover_al. nn_Snover_et prep_as_TER_criterion dep_TER_Snover amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et prep_of_combination_BLEU amod_combination_linear det_combination_a dep_selected_= conj_and_selected_TER dep_selected_Papineni dobj_selected_combination nsubj_selected_we prep_for_selected_Workshop num_Workshop_2009 nn_Workshop_WMT det_Workshop_the
W09-0408	P02-1040	o	Of these only feature weights can be trained for which we used minimum error rate training with version 1.04 of IBM-style BLEU -LRB- Papineni et al. 2002 -RRB- in case-insensitive mode	amod_mode_case-insensitive amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_BLEU_IBM-style prep_of_version_BLEU num_version_1.04 nn_training_rate nn_training_error amod_training_minimum prep_in_used_mode dep_used_Papineni prep_with_used_version dobj_used_training nsubj_used_we prep_for_used_which rcmod_trained_used auxpass_trained_be aux_trained_can nsubjpass_trained_weights prep_of_trained_these nn_weights_feature advmod_weights_only
W09-0408	P02-1040	o	We scored systems and our own output using case-insensitive IBM-style BLEU 1.04 -LRB- Papineni et al. 2002 -RRB- METEOR 0.6 -LRB- Lavie and Agarwal 2007 -RRB- with all modules and TER 5 -LRB- Snover et al. 2006 -RRB-	amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_TER_Snover num_TER_5 det_modules_all dep_Lavie_2007 conj_and_Lavie_Agarwal prep_with_METEOR_modules appos_METEOR_Agarwal appos_METEOR_Lavie num_METEOR_0.6 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et num_BLEU_1.04 amod_BLEU_IBM-style amod_BLEU_case-insensitive dobj_using_BLEU amod_output_own poss_output_our vmod_systems_using conj_and_systems_output conj_and_scored_TER dobj_scored_METEOR dep_scored_Papineni dobj_scored_output dobj_scored_systems nsubj_scored_We
W09-0409	P02-1040	o	The system combination weights one for each system LM weight and word and NULL insertion penalties were tuned to maximize the BLEU -LRB- Papineni et al. 2002 -RRB- score on the tuning set -LRB- newssyscomb2009 -RRB-	appos_set_newssyscomb2009 nn_set_tuning det_set_the prep_on_score_set dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_score dep_BLEU_Papineni det_BLEU_the dobj_maximize_BLEU aux_maximize_to xcomp_tuned_maximize auxpass_tuned_were nsubjpass_tuned_penalties nsubjpass_tuned_weight nsubjpass_tuned_weights nn_penalties_insertion nn_penalties_NULL nn_penalties_word conj_and_word_NULL nn_weight_LM det_system_each prep_for_one_system conj_and_weights_penalties conj_and_weights_weight dep_weights_one nn_weights_combination nn_weights_system det_weights_The ccomp_``_tuned
W09-0412	P02-1040	o	We set all feature weights by optimizing Bleu -LRB- Papineni et al. 2002 -RRB- directly using minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- on the tuning part of the development set -LRB- dev-test2009a -RRB-	appos_set_dev-test2009a nn_set_development det_set_the prep_of_part_set nn_part_tuning det_part_the dep_Och_2003 appos_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum prep_on_using_part dobj_using_training advmod_using_directly amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_Bleu_Papineni dobj_optimizing_Bleu nn_weights_feature det_weights_all dep_set_using prepc_by_set_optimizing dobj_set_weights nsubj_set_We
W09-0416	P02-1040	o	Instead of using a single system output as the skeleton we employ a minimum Bayes-risk decoder to select the best single system output from the merged N-best list by minimizing the BLEU -LRB- Papineni et al. 2002 -RRB- loss	dep_loss_Papineni nsubj_loss_BLEU dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et det_BLEU_the xcomp_minimizing_loss nn_list_N-best amod_list_merged det_list_the prepc_by_output_minimizing prep_from_output_list dep_system_output amod_system_single amod_system_best det_system_the dobj_select_system aux_select_to amod_decoder_Bayes-risk amod_decoder_minimum det_decoder_a vmod_employ_select dobj_employ_decoder nsubj_employ_we prepc_instead_of_employ_using det_skeleton_the nn_output_system amod_output_single det_output_a prep_as_using_skeleton dobj_using_output
W09-0418	P02-1040	o	In this paper translation quality is evaluated according to -LRB- 1 -RRB- the BLEU metrics which calculates the geometric mean of ngram precision by the system output with respect to reference translations -LRB- Papineni et al. 2002 -RRB- and -LRB- 2 -RRB- the METEOR metrics that calculates unigram overlaps between translations -LRB- Banerjee and Lavie 2005 -RRB-	amod_Banerjee_2005 conj_and_Banerjee_Lavie dep_translations_Lavie dep_translations_Banerjee prep_between_overlaps_translations nsubj_overlaps_metrics dep_overlaps_2 dobj_calculates_unigram nsubj_calculates_that rcmod_metrics_calculates nn_metrics_METEOR det_metrics_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_translations_reference nn_output_system det_output_the nn_precision_ngram prep_of_mean_precision amod_mean_geometric det_mean_the prep_with_respect_to_calculates_translations prep_by_calculates_output dobj_calculates_mean nsubj_calculates_which conj_and_metrics_overlaps dep_metrics_Papineni rcmod_metrics_calculates nn_metrics_BLEU det_metrics_the dobj_1_overlaps dobj_1_metrics dep_evaluated_1 prepc_according_to_evaluated_to auxpass_evaluated_is nsubjpass_evaluated_quality prep_in_evaluated_paper nn_quality_translation det_paper_this ccomp_``_evaluated
W09-0421	P02-1040	o	In this paper we report case-insensitive Bleu scores -LRB- Papineni et al. 2002 -RRB- unless otherwise stated calculated with the NIST tool and caseinsensitive Meteor-ranking scores without WordNet -LRB- Agarwal and Lavie 2008 -RRB-	dep_Agarwal_2008 conj_and_Agarwal_Lavie dep_WordNet_Lavie dep_WordNet_Agarwal prep_without_scores_WordNet amod_scores_Meteor-ranking amod_scores_caseinsensitive nn_tool_NIST det_tool_the prep_with_calculated_tool conj_and_stated_scores conj_and_stated_calculated advmod_stated_otherwise mark_stated_unless amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_scores_Papineni nn_scores_Bleu amod_scores_case-insensitive advcl_report_scores advcl_report_calculated advcl_report_stated dobj_report_scores nsubj_report_we prep_in_report_paper det_paper_this
W09-0425	P02-1040	o	For each we give case-insensitive scores on version 0.6 of METEOR -LRB- Lavie and Agarwal 2007 -RRB- with all modules enabled version 1.04 of IBMstyle BLEU -LRB- Papineni et al. 2002 -RRB- and version 5 of TER -LRB- Snover et al. 2006 -RRB-	amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_version_Snover prep_of_version_TER num_version_5 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni nn_BLEU_IBMstyle prep_of_version_BLEU num_version_1.04 conj_and_enabled_version dobj_enabled_version det_modules_all dep_Lavie_2007 conj_and_Lavie_Agarwal appos_METEOR_Agarwal appos_METEOR_Lavie prep_with_version_modules prep_of_version_METEOR num_version_0.6 amod_scores_case-insensitive dep_give_version dep_give_enabled prep_on_give_version dobj_give_scores nsubj_give_we prep_for_give_each
W09-0426	P02-1040	o	2.2 Automatic evaluation metric Since the official evaluation criterion for WMT09 is human sentence ranking we chose to minimize a linear combination of two common evaluation metrics BLEU and TER -LRB- Papineni et al. 2002 Snover et al. 2006 -RRB- during system development and tuning TERBLEU 2 Although we are not aware of any work demonstrating that this combination of metrics correlates better than either individually in sentence ranking Yaser Al-Onaizan -LRB- personal communication -RRB- reports that it correlates well with the human evaluation metric HTER	amod_HTER_metric nn_HTER_evaluation amod_HTER_human det_HTER_the prep_with_correlates_HTER advmod_correlates_well nsubj_correlates_it mark_correlates_that ccomp_reports_correlates nsubj_reports_either mark_reports_than amod_communication_personal appos_Al-Onaizan_communication nn_Al-Onaizan_Yaser nn_ranking_sentence appos_either_Al-Onaizan prep_in_either_ranking advmod_either_individually advcl_better_reports acomp_correlates_better nsubj_correlates_combination mark_correlates_that prep_of_combination_metrics det_combination_this ccomp_demonstrating_correlates det_work_any xcomp_aware_demonstrating prep_of_aware_work neg_aware_not cop_aware_are nsubj_aware_we mark_aware_Although advcl_TERBLEU_aware num_TERBLEU_2 conj_and_development_tuning nn_development_system num_Snover_2006 nn_Snover_al. nn_Snover_et dep_Papineni_Snover appos_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_Papineni conj_and_BLEU_TER appos_metrics_TER appos_metrics_BLEU nn_metrics_evaluation amod_metrics_common num_metrics_two prep_of_combination_metrics amod_combination_linear det_combination_a prep_during_minimize_tuning prep_during_minimize_development dobj_minimize_combination aux_minimize_to dep_chose_TERBLEU xcomp_chose_minimize nsubj_chose_we ccomp_chose_metric nn_ranking_sentence amod_ranking_human cop_ranking_is nsubj_ranking_criterion mark_ranking_Since prep_for_criterion_WMT09 nn_criterion_evaluation amod_criterion_official det_criterion_the advcl_metric_ranking nn_metric_evaluation nn_metric_Automatic num_metric_2.2
W09-0427	P02-1040	o	The loglinear model feature weights were learned using minimum error rate training -LRB- MERT -RRB- -LRB- Och 2003 -RRB- with BLEU score -LRB- Papineni et al. 2002 -RRB- as the objective function	amod_function_objective det_function_the amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU dep_Och_2003 dep_training_Och appos_training_MERT nn_training_rate nn_training_error amod_training_minimum prep_with_using_score dobj_using_training prep_as_learned_function dep_learned_Papineni xcomp_learned_using auxpass_learned_were nsubjpass_learned_weights nn_weights_feature nn_weights_model amod_weights_loglinear det_weights_The ccomp_``_learned
W09-0431	P02-1040	o	As expected as we double the size of the data the BLEU score -LRB- Papineni et al. 2002 -RRB- increases	dep_increases_Papineni dep_increases_score dep_increases_double dep_increases_expected dep_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU det_score_the det_data_the prep_of_size_data det_size_the dobj_double_size nsubj_double_we mark_double_as mark_expected_As
W09-0432	P02-1040	o	5http / / www.statmt.org/wmt08 185 the BLEU score -LRB- Papineni et al. 2002 -RRB- and tested on test2008	prep_on_tested_test2008 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_score_Papineni nn_score_BLEU det_score_the conj_and_www.statmt.org/wmt08_tested dep_www.statmt.org/wmt08_score num_www.statmt.org/wmt08_185 dep_5http_tested dep_5http_www.statmt.org/wmt08
W09-0437	P02-1040	o	1 Introduction Most empirical work in translation analyzes models and algorithms using BLEU -LRB- Papineni et al. 2002 -RRB- and related metrics	amod_metrics_related amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_metrics dep_BLEU_Papineni dobj_using_metrics dobj_using_BLEU vmod_algorithms_using conj_and_analyzes_algorithms dobj_analyzes_models nsubj_analyzes_work prep_in_work_translation amod_work_empirical amod_work_Most rcmod_Introduction_algorithms rcmod_Introduction_analyzes num_Introduction_1
W09-0441	P02-1040	o	1 Introduction Since the introduction of the BLEU metric -LRB- Papineni et al. 2002 -RRB- statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics	amod_metrics_automatic dobj_using_metrics vmod_evaluation_using amod_evaluation_rapid pobj_towards_evaluation poss_performance_their prep_of_evaluation_performance amod_evaluation_human conj_and_moved_towards prep_away_from_moved_evaluation aux_moved_have nsubj_moved_systems vmod_moved_metric dep_moved_BLEU det_moved_the nn_systems_MT amod_systems_statistical dep_al._2002 nn_al._et advmod_Papineni_al. dep_metric_Papineni prepc_of_introduction_towards prepc_of_introduction_moved det_introduction_the prep_since_Introduction_introduction num_Introduction_1
W09-0441	P02-1040	o	We compare TERp with BLEU -LRB- Papineni et al. 2002 -RRB- METEOR -LRB- Banerjee and Lavie 2005 -RRB- and TER -LRB- Snover et al. 2006 -RRB-	amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_Banerjee_2005 conj_and_Banerjee_Lavie appos_METEOR_Lavie appos_METEOR_Banerjee amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et conj_and_BLEU_TER conj_and_BLEU_METEOR dep_BLEU_Papineni dep_compare_Snover prep_with_compare_TER prep_with_compare_METEOR prep_with_compare_BLEU dobj_compare_TERp nsubj_compare_We
W09-1114	P02-1040	o	Translation quality is reported using case-insensitive BLEU -LRB- Papineni et al. 2002 -RRB-	amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni amod_BLEU_case-insensitive dobj_using_BLEU xcomp_reported_using auxpass_reported_is nsubjpass_reported_quality nn_quality_Translation
W09-2301	P02-1040	o	We report case-insensitive scores on version 0.6 of METEOR -LRB- Lavie and Agarwal 2007 -RRB- with all modules enabled version 1.04 of IBM-style BLEU -LRB- Papineni et al. 2002 -RRB- and version 5 of TER -LRB- Snover et al. 2006 -RRB-	amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_version_Snover prep_of_version_TER num_version_5 amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et amod_BLEU_IBM-style dep_version_Papineni prep_of_version_BLEU num_version_1.04 conj_and_enabled_version dobj_enabled_version nsubj_enabled_scores det_modules_all dep_Lavie_2007 conj_and_Lavie_Agarwal appos_METEOR_Agarwal appos_METEOR_Lavie prep_with_version_modules prep_of_version_METEOR num_version_0.6 prep_on_scores_version amod_scores_case-insensitive ccomp_report_version ccomp_report_enabled nsubj_report_We
W09-2309	P02-1040	o	To tune the decoder parameters we conducted minimum error rate training -LRB- Och 2003 -RRB- with respect to the word BLEU score -LRB- Papineni et al. 2002 -RRB- using 2.0 K development sentence pairs	nn_pairs_sentence nn_pairs_development nn_pairs_K num_pairs_2.0 dobj_using_pairs amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et nn_score_BLEU nn_score_word det_score_the dep_Och_2003 prep_with_respect_to_training_score appos_training_Och nn_training_rate nn_training_error amod_training_minimum vmod_conducted_using dep_conducted_Papineni dobj_conducted_training nsubj_conducted_we advcl_conducted_tune nn_parameters_decoder det_parameters_the dobj_tune_parameters aux_tune_To ccomp_``_conducted
W09-2310	P02-1040	o	The highest BLEU score -LRB- Papineni et al. 2002 -RRB- was chosen as the optimization criterion	nn_criterion_optimization det_criterion_the prep_as_chosen_criterion auxpass_chosen_was nsubjpass_chosen_score amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_score_Papineni nn_score_BLEU amod_score_highest det_score_The
W09-2404	P02-1040	o	5.2 Impact on translation quality As reported in Table 3 small increases in METEOR -LRB- Banerjee and Lavie 2005 -RRB- BLEU -LRB- Papineni et al. 2002 -RRB- and NIST scores -LRB- Doddington 2002 -RRB- suggest that SMT output matches the references better after postprocessing or decoding with the suggested lemma translations	nn_translations_lemma amod_translations_suggested det_translations_the prep_with_postprocessing_translations conj_or_postprocessing_decoding prepc_after_better_decoding prepc_after_better_postprocessing advmod_references_better det_references_the dobj_matches_references nsubj_matches_output mark_matches_that nn_output_SMT ccomp_suggest_matches nsubj_suggest_increases nsubj_suggest_Impact amod_Doddington_2002 nn_scores_NIST amod_Papineni_2002 dep_Papineni_al. nn_Papineni_et dep_BLEU_Papineni dep_Banerjee_2005 conj_and_Banerjee_Lavie dep_METEOR_Doddington conj_and_METEOR_scores conj_and_METEOR_BLEU appos_METEOR_Lavie appos_METEOR_Banerjee prep_in_increases_scores prep_in_increases_BLEU prep_in_increases_METEOR amod_increases_small num_Table_3 prep_in_reported_Table mark_reported_As nn_quality_translation advcl_Impact_reported prep_on_Impact_quality num_Impact_5.2
D07-1009	P02-1047	o	4 Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis -LRB- Marcu and Echihabi 2002 Lapata 2003 Elsner et al. 2007 -RRB-	num_Elsner_2007 nn_Elsner_al. nn_Elsner_et num_Lapata_2003 dep_Marcu_Elsner conj_and_Marcu_Lapata conj_and_Marcu_2002 conj_and_Marcu_Echihabi nn_analysis_discourse dep_approaches_Lapata dep_approaches_2002 dep_approaches_Echihabi dep_approaches_Marcu prep_for_approaches_analysis amod_approaches_corpus-based amod_work_previous prep_on_inspired_approaches agent_inspired_work auxpass_inspired_are nsubjpass_inspired_Features poss_experiments_our prep_in_used_experiments vmod_Features_used nn_Features_Features num_Features_4 ccomp_``_inspired
D08-1103	P02-1047	o	Antonyms often indicate the discourse relation of contrast -LRB- Marcu and Echihabi 2002 -RRB-	amod_Marcu_2002 conj_and_Marcu_Echihabi dep_relation_Echihabi dep_relation_Marcu prep_of_relation_contrast nn_relation_discourse det_relation_the dobj_indicate_relation advmod_indicate_often nsubj_indicate_Antonyms
D08-1103	P02-1047	o	As Marcu and Echihabi -LRB- 2002 -RRB- point out WordNet does not encode antonymy across part-of-speech -LRB- for example legallyembargo -RRB-	dep_for_legallyembargo pobj_for_example prep_across_antonymy_part-of-speech dep_encode_for dobj_encode_antonymy neg_encode_not aux_encode_does nsubj_encode_WordNet prep_as_encode_point prep_as_encode_Marcu advmod_point_out dep_point_2002 dep_point_Echihabi conj_and_Marcu_point
D09-1036	P02-1047	o	Marcu and Echihabi -LRB- 2002 -RRB- demonstrated that word pairs extracted from the respective text spans are a good signal of the discourse relation between arguments	prep_between_relation_arguments nn_relation_discourse det_relation_the prep_of_signal_relation amod_signal_good det_signal_a cop_signal_are nsubj_signal_pairs mark_signal_that nn_spans_text amod_spans_respective det_spans_the prep_from_extracted_spans vmod_pairs_extracted nn_pairs_word ccomp_demonstrated_signal nsubj_demonstrated_Echihabi nsubj_demonstrated_Marcu appos_Echihabi_2002 conj_and_Marcu_Echihabi
D09-1036	P02-1047	o	Note that Row 3 of Table 3 corresponds to Marcu and Echihabi -LRB- 2002 -RRB- s system which applies only word pair features	nn_features_pair nn_features_word advmod_features_only dobj_applies_features nsubj_applies_which rcmod_system_applies amod_system_s appos_Echihabi_2002 conj_and_Marcu_Echihabi dobj_corresponds_system prep_to_corresponds_Echihabi prep_to_corresponds_Marcu nsubj_corresponds_Row mark_corresponds_that num_Table_3 prep_of_Row_Table num_Row_3 ccomp_Note_corresponds
D09-1036	P02-1047	o	2 Related Work One of the first works that use statistical methods to detect implicit discourse relations is that of Marcu and Echihabi -LRB- 2002 -RRB-	appos_Echihabi_2002 conj_and_Marcu_Echihabi prep_of_that_Echihabi prep_of_that_Marcu cop_that_is nsubj_that_Work nn_relations_discourse amod_relations_implicit dobj_detect_relations aux_detect_to amod_methods_statistical vmod_use_detect dobj_use_methods nsubj_use_that rcmod_works_use amod_works_first det_works_the prep_of_One_works dep_Work_One amod_Work_Related num_Work_2
I05-6007	P02-1047	o	Syntactic criteria are relevant but clearly not decisive as can be observed in -LRB- Marcu and Echihabi 2002 -RRB-	dep_Marcu_2002 conj_and_Marcu_Echihabi dep_in_Echihabi dep_in_Marcu prep_observed_in auxpass_observed_be aux_observed_can mark_observed_as neg_decisive_not advmod_decisive_clearly nsubj_decisive_criteria advcl_relevant_observed conj_but_relevant_decisive cop_relevant_are nsubj_relevant_criteria amod_criteria_Syntactic
J03-4002	P02-1047	o	In showing how DLTAG and an interpretative process on its derivations operate we must of necessity gloss over how inference triggered by adjacency or associated with a structural connective provides the intended relation between adjacent discourse 578 Computational Linguistics Volume 29 Number 4 units It may be a matter simply of statistical inference as in Marcu and Echihabi -LRB- 2002 -RRB- or of more complex inference as in Hobbs et al.	nn_al._et nn_al._Hobbs pobj_in_al. pcomp_as_in nn_inference_complex amod_inference_more pobj_of_inference appos_Echihabi_2002 conj_and_Marcu_Echihabi pobj_in_Echihabi pobj_in_Marcu pcomp_as_in amod_inference_statistical prep_matter_as conj_or_matter_of prep_matter_as prep_of_matter_inference advmod_matter_simply det_matter_a cop_matter_be aux_matter_may nsubj_matter_It num_units_4 nn_units_Number appos_29_units dep_Volume_29 nn_Volume_Linguistics nn_Volume_Computational num_Volume_578 dep_discourse_Volume amod_discourse_adjacent prep_between_relation_discourse amod_relation_intended det_relation_the dobj_provides_relation nsubj_provides_inference advmod_provides_how amod_connective_structural det_connective_a prep_with_associated_connective conj_or_adjacency_associated agent_triggered_associated agent_triggered_adjacency vmod_inference_triggered parataxis_gloss_of parataxis_gloss_matter prepc_over_gloss_provides prep_of_gloss_necessity ccomp_gloss_must nsubj_must_we prepc_in_must_showing nsubj_operate_process nsubj_operate_DLTAG advmod_operate_how poss_derivations_its amod_process_interpretative det_process_an prep_on_DLTAG_derivations conj_and_DLTAG_process ccomp_showing_operate
N04-1020	P02-1047	o	A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi -LRB- 2002 -RRB-	appos_Echihabi_2002 conj_and_Marcu_Echihabi nn_relations_discourse prep_by_interpretation_Echihabi prep_by_interpretation_Marcu prep_of_interpretation_relations det_interpretation_the prep_for_advocated_interpretation auxpass_advocated_been aux_advocated_has nsubjpass_advocated_approach amod_approach_similar det_approach_A
N04-1020	P02-1047	o	Apart from the fact that we present an alternative model our work differs from Marcu and Echihabi -LRB- 2002 -RRB- in two important ways	amod_ways_important num_ways_two appos_Echihabi_2002 conj_and_Marcu_Echihabi prep_in_differs_ways prep_from_differs_Echihabi prep_from_differs_Marcu nsubj_differs_work prep_apart_from_differs_fact poss_work_our amod_model_alternative det_model_an dobj_present_model nsubj_present_we mark_present_that ccomp_fact_present det_fact_the
N06-2034	P02-1047	o	-LRB- Marcu and Echihabi 2002 -RRB- proposed a method to identify discourse relations between text segments using Nave Bayes classifiers trained on a huge corpus	amod_corpus_huge det_corpus_a prep_on_trained_corpus vmod_classifiers_trained nn_classifiers_Bayes nn_classifiers_Nave dobj_using_classifiers vmod_segments_using nn_segments_text prep_between_relations_segments nn_relations_discourse dobj_identify_relations aux_identify_to vmod_method_identify det_method_a dobj_proposed_method nsubj_proposed_Echihabi nsubj_proposed_Marcu num_Echihabi_2002 conj_and_Marcu_Echihabi
N06-2034	P02-1047	o	When we consider the frequency of discourse relations i.e. 43 % for ELABORATION 32 % for CONTRAST etc. the weighted accuracy was 53 % using only lexical information which is comparable to the similar experiment by -LRB- Marcu and Echihabi 2002 -RRB- of 49.7 %	num_%_49.7 num_Echihabi_2002 prep_of_Marcu_% conj_and_Marcu_Echihabi amod_experiment_similar det_experiment_the prep_by_comparable_Echihabi prep_by_comparable_Marcu prep_to_comparable_experiment cop_comparable_is nsubj_comparable_which rcmod_information_comparable amod_information_lexical advmod_information_only dobj_using_information vmod_%_using num_%_53 cop_%_was nsubj_%_accuracy amod_accuracy_weighted det_accuracy_the dep_CONTRAST_etc. prep_for_%_CONTRAST num_%_32 rcmod_%_% dep_%_% prep_for_%_ELABORATION num_%_43 dep_%_i.e. advcl_%_consider nn_relations_discourse prep_of_frequency_relations det_frequency_the dobj_consider_frequency nsubj_consider_we advmod_consider_When
N07-1038	P02-1047	o	The effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi -LRB- 2002 -RRB-	appos_Echihabi_2002 conj_and_Marcu_Echihabi agent_shown_Echihabi agent_shown_Marcu advmod_shown_previously auxpass_shown_been aux_shown_has nsubjpass_shown_effectiveness nn_relations_discourse prep_of_recognition_relations det_features_these prep_for_effectiveness_recognition prep_of_effectiveness_features det_effectiveness_The
N07-1038	P02-1047	o	Since this relation can often be determined automatically for a given text -LRB- Marcu and Echihabi 2002 -RRB- we can readily use it to improve rank prediction	nn_prediction_rank dobj_improve_prediction aux_improve_to vmod_use_improve dobj_use_it advmod_use_readily aux_use_can nsubj_use_we advcl_use_determined dep_Marcu_2002 conj_and_Marcu_Echihabi dep_text_Echihabi dep_text_Marcu amod_text_given det_text_a prep_for_determined_text advmod_determined_automatically auxpass_determined_be advmod_determined_often aux_determined_can nsubjpass_determined_relation mark_determined_Since det_relation_this
N07-1054	P02-1047	o	We draw on and extend the work of Marcu and Echihabi -LRB- 2002 -RRB-	appos_Echihabi_2002 conj_and_Marcu_Echihabi prep_of_work_Echihabi prep_of_work_Marcu det_work_the dobj_extend_work nsubj_extend_We conj_and_draw_extend prt_draw_on nsubj_draw_We
N07-1054	P02-1047	o	Marcu and Echihabi -LRB- 2002 -RRB- use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large unannotated corpora	amod_corpora_unannotated amod_corpora_large prep_from_Contrast_corpora conj_and_Contrast_Elaboration prep_such_as_RSRs_Elaboration prep_such_as_RSRs_Contrast prep_of_instances_RSRs nn_instances_mining prep_in_approach_instances amod_approach_pattern-based det_approach_a dobj_use_approach nsubj_use_Echihabi nsubj_use_Marcu appos_Echihabi_2002 conj_and_Marcu_Echihabi
N07-1054	P02-1047	o	Marcu and Echihabi -LRB- 2002 -RRB- lter training instances based on Part-of-Speech -LRB- POS -RRB- tags and Soricut and Marcu -LRB- 2003 -RRB- use syntactic features to identify sentence-internal RST structure	nn_structure_RST amod_structure_sentence-internal dobj_identify_structure aux_identify_to amod_features_syntactic vmod_use_identify dobj_use_features nsubj_use_Marcu nsubj_use_Soricut nsubj_use_instances nsubj_use_Marcu appos_Marcu_2003 nn_tags_Part-of-Speech appos_Part-of-Speech_POS nn_instances_training nn_instances_lter nn_instances_Echihabi appos_Echihabi_2002 conj_and_Marcu_Marcu conj_and_Marcu_Soricut pobj_Marcu_tags prepc_based_on_Marcu_on conj_and_Marcu_instances
N07-1054	P02-1047	o	We adopt the approach of Marcu and Echihabi -LRB- 2002 -RRB- using a small set of patterns to build relation models and extend their work by re ning the training and classi cation process using parameter optimization topic segmentation and syntactic parsing	nn_parsing_syntactic nn_segmentation_topic conj_and_optimization_parsing conj_and_optimization_segmentation nn_optimization_parameter dobj_using_parsing dobj_using_segmentation dobj_using_optimization nn_process_cation nn_process_classi conj_and_training_process det_training_the dobj_ning_process dobj_ning_training vmod_re_ning poss_work_their xcomp_extend_using prep_by_extend_re dobj_extend_work nsubj_extend_We nn_models_relation dobj_build_models aux_build_to prep_of_set_patterns amod_set_small det_set_a vmod_using_build dobj_using_set nsubj_using_We appos_Echihabi_2002 conj_and_Marcu_Echihabi prep_of_approach_Echihabi prep_of_approach_Marcu det_approach_the conj_and_adopt_extend conj_and_adopt_using dobj_adopt_approach nsubj_adopt_We
N07-1054	P02-1047	o	3 The M&E Framework We model two RSRs Cause and Contrast adopting the de nitions of Marcu and Echihabi -LRB- 2002 -RRB- -LRB- henceforth M&E -RRB- for their Cause-ExplanationEvidence and Contrast relations respectively	nn_relations_Contrast conj_and_Cause-ExplanationEvidence_relations poss_Cause-ExplanationEvidence_their nn_M&E_henceforth appos_Echihabi_2002 appos_Marcu_M&E conj_and_Marcu_Echihabi advmod_nitions_respectively prep_for_nitions_relations prep_for_nitions_Cause-ExplanationEvidence prep_of_nitions_Echihabi prep_of_nitions_Marcu nn_nitions_de det_nitions_the dobj_adopting_nitions conj_and_Cause_Contrast vmod_RSRs_adopting appos_RSRs_Contrast appos_RSRs_Cause num_RSRs_two dobj_model_RSRs nsubj_model_We rcmod_Framework_model nn_Framework_M&E det_Framework_The dep_3_Framework ccomp_``_3
P04-1087	P02-1047	o	4.1.1 Lexical co-occurrences Lexical co-occurrences have previously been shown to be useful for discourse level learning tasks -LRB- Lapata and Lascarides 2004 Marcu and Echihabi 2002 -RRB-	dep_Marcu_2002 conj_and_Marcu_Echihabi dep_Lapata_Echihabi dep_Lapata_Marcu conj_and_Lapata_2004 conj_and_Lapata_Lascarides dep_tasks_2004 dep_tasks_Lascarides dep_tasks_Lapata amod_tasks_learning dep_level_tasks nn_level_discourse prep_for_useful_level cop_useful_be aux_useful_to xcomp_shown_useful auxpass_shown_been advmod_shown_previously aux_shown_have nsubjpass_shown_co-occurrences amod_co-occurrences_Lexical nn_co-occurrences_co-occurrences amod_co-occurrences_Lexical num_co-occurrences_4.1.1
P04-1087	P02-1047	o	As such discourse markers play an important role in the parsing of natural language discourse -LRB- Forbes et al. 2001 Marcu 2000 -RRB- and their correspondence with discourse relations can be exploited for the unsupervised learning of discourse relations -LRB- Marcu and Echihabi 2002 -RRB-	dep_Marcu_2002 conj_and_Marcu_Echihabi nn_relations_discourse appos_learning_Echihabi appos_learning_Marcu prep_of_learning_relations amod_learning_unsupervised det_learning_the prep_for_exploited_learning auxpass_exploited_be aux_exploited_can nsubjpass_exploited_correspondence nn_relations_discourse prep_with_correspondence_relations poss_correspondence_their dep_Marcu_2000 dep_Forbes_Marcu amod_Forbes_2001 dep_Forbes_al. nn_Forbes_et nn_discourse_language amod_discourse_natural prep_of_parsing_discourse det_parsing_the amod_role_important det_role_an conj_and_play_exploited dep_play_Forbes prep_in_play_parsing dobj_play_role nsubj_play_markers prep_such_as_play_such nn_markers_discourse
P05-1019	P02-1047	o	For such cases unsupervised approaches have been developed for predicting relations by using sentences containing discourse connectives as training data -LRB- Marcu and Echihabi 2002 Lapata and Lascarides 2004 -RRB-	dep_Lapata_2004 conj_and_Lapata_Lascarides dep_Marcu_Lascarides dep_Marcu_Lapata conj_and_Marcu_2002 conj_and_Marcu_Echihabi dep_data_2002 dep_data_Echihabi dep_data_Marcu nn_data_training nn_connectives_discourse prep_as_containing_data dobj_containing_connectives vmod_sentences_containing dobj_using_sentences dobj_predicting_relations agent_developed_using prepc_for_developed_predicting auxpass_developed_been aux_developed_have nsubjpass_developed_approaches prep_for_developed_cases amod_approaches_unsupervised amod_cases_such
P08-1118	P02-1047	n	Presently there exist methods for learning oppositional terms -LRB- Marcu and Echihabi 2002 -RRB- and paraphrase learning has been thoroughly studied but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution	nn_distribution_data det_distribution_the prep_because_of_poses_distribution dobj_poses_difficulties amod_phrases_incompatible dobj_learn_phrases aux_learn_to det_techniques_these vmod_extending_learn dobj_extending_techniques advmod_extending_successfully nsubjpass_extending_methods dep_studied_poses conj_but_studied_extending advmod_studied_thoroughly auxpass_studied_been aux_studied_has nsubjpass_studied_methods nn_learning_paraphrase dep_Marcu_2002 conj_and_Marcu_Echihabi conj_and_terms_learning dep_terms_Echihabi dep_terms_Marcu amod_terms_oppositional dobj_learning_learning dobj_learning_terms prepc_for_methods_learning ccomp_exist_extending ccomp_exist_studied expl_exist_there advmod_exist_Presently
P09-1076	P02-1047	o	7 Automated Sense Labelling of Discourse Connectives The focus here is on automated sense labelling of discourse connectives -LRB- Elwell and Baldridge 2008 Marcu and Echihabi 2002 Pitler et al. 2009 Wellner and Pustejovsky 2007 Wellner 679 Total Density of Intra-Sentential Intra-Sentential Total Intra-Sentential Intra-Sentential Subordinating Coordinating Discourse Genre Sentences Connectives Connectives/Sentence Conjunctions Conjunctions Adverbials ESSAYS 4774 1397 0.293 808 -LRB- 57.8 % -RRB- 438 -LRB- 31.4 % -RRB- 151 -LRB- 10.8 % -RRB- SUMMARIES 2118 275 0.130 166 -LRB- 60.4 % -RRB- 99 -LRB- 36.0 % -RRB- 10 -LRB- 3.6 % -RRB- LETTERS 739 200 0.271 126 -LRB- 63.0 % -RRB- 56 -LRB- 28.0 % -RRB- 18 -LRB- 9.0 % -RRB- NEWS 40095 9336 0.233 5514 -LRB- 59.1 % -RRB- 3015 -LRB- 32.3 % -RRB- 807 -LRB- 8.6 % -RRB- Figure 4 Distribution of Explicit Intra-Sentential Connectives	amod_Connectives_Intra-Sentential amod_Connectives_Explicit prep_of_Distribution_Connectives dep_4_Figure dep_4_5514 dep_Figure_807 num_%_8.6 dep_807_% number_807_3015 num_%_32.3 appos_3015_% num_%_59.1 appos_5514_% num_5514_0.233 number_0.233_9336 number_0.233_40095 num_NEWS_4 dep_%_Distribution dep_%_NEWS num_%_9.0 num_%_28.0 num_%_63.0 number_126_0.271 dep_126_200 dep_126_LETTERS dep_126_166 number_200_739 dep_LETTERS_10 num_%_3.6 dep_10_% number_10_99 num_%_36.0 appos_99_% num_%_60.4 appos_166_% num_166_0.130 number_0.130_275 number_0.130_2118 dep_SUMMARIES_% num_SUMMARIES_18 appos_SUMMARIES_% num_SUMMARIES_56 appos_SUMMARIES_% num_SUMMARIES_126 num_%_10.8 num_%_31.4 num_%_57.8 num_808_0.293 number_0.293_1397 number_0.293_4774 dep_ESSAYS_SUMMARIES dep_ESSAYS_% num_ESSAYS_151 dep_ESSAYS_% num_ESSAYS_438 dep_ESSAYS_% num_ESSAYS_808 nn_ESSAYS_Adverbials nn_ESSAYS_Conjunctions nn_ESSAYS_Conjunctions nn_ESSAYS_Connectives/Sentence nn_ESSAYS_Connectives dobj_Sentences_ESSAYS dep_Genre_Sentences dep_Discourse_Genre dep_Coordinating_Discourse dep_Subordinating_Coordinating dep_Intra-Sentential_Subordinating dep_Intra-Sentential_Intra-Sentential dep_Total_Intra-Sentential dep_Intra-Sentential_Total dep_Intra-Sentential_Intra-Sentential prep_of_Density_Intra-Sentential amod_Density_Total num_Density_679 appos_Wellner_Density conj_and_Wellner_Wellner conj_and_Wellner_2007 conj_and_Wellner_Pustejovsky num_Pitler_2009 nn_Pitler_al. nn_Pitler_et dep_Marcu_Wellner dep_Marcu_2007 dep_Marcu_Pustejovsky dep_Marcu_Wellner conj_and_Marcu_Pitler conj_and_Marcu_2002 conj_and_Marcu_Echihabi dep_Elwell_Pitler dep_Elwell_2002 dep_Elwell_Echihabi dep_Elwell_Marcu conj_and_Elwell_2008 conj_and_Elwell_Baldridge dep_connectives_2008 dep_connectives_Baldridge dep_connectives_Elwell nn_connectives_discourse prep_of_labelling_connectives nn_labelling_sense amod_labelling_automated prep_on_is_labelling nsubj_is_focus advmod_focus_here det_focus_The nn_Connectives_Discourse dep_Labelling_is prep_of_Labelling_Connectives nn_Labelling_Sense amod_Labelling_Automated num_Labelling_7 dep_``_Labelling
P09-1076	P02-1047	o	Section 7 considers recent efforts to induce effective procedures for automated sense labelling of discourse relations that are not lexically marked -LRB- Elwell and Baldridge 2008 Marcu and Echihabi 2002 Pitler et al. 2009 Wellner and Pustejovsky 2007 Wellner 2008 -RRB-	amod_Wellner_2008 dep_Wellner_Wellner conj_and_Wellner_2007 conj_and_Wellner_Pustejovsky num_Pitler_2009 nn_Pitler_al. nn_Pitler_et conj_and_Marcu_2007 conj_and_Marcu_Pustejovsky conj_and_Marcu_Wellner conj_and_Marcu_Pitler conj_and_Marcu_2002 conj_and_Marcu_Echihabi dep_Elwell_Wellner dep_Elwell_Pitler dep_Elwell_2002 dep_Elwell_Echihabi dep_Elwell_Marcu conj_and_Elwell_2008 conj_and_Elwell_Baldridge dep_marked_2008 dep_marked_Baldridge dep_marked_Elwell advmod_marked_lexically neg_marked_not cop_marked_are nsubj_marked_that rcmod_relations_marked nn_relations_discourse prep_of_labelling_relations nn_labelling_sense amod_labelling_automated prep_for_procedures_labelling amod_procedures_effective dobj_induce_procedures aux_induce_to vmod_efforts_induce amod_efforts_recent dobj_considers_efforts nsubj_considers_Section num_Section_7
W06-3309	P02-1047	o	Although this study falls under the general topic of discourse modeling our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements -LRB- McKeown 1985 Marcu and Echihabi 2002 -RRB-	dep_Marcu_2002 conj_and_Marcu_Echihabi dep_McKeown_Echihabi dep_McKeown_Marcu appos_McKeown_1985 dep_elements_McKeown amod_elements_rhetorical amod_elements_domainindependent prep_of_terms_elements prep_in_characterize_terms dobj_characterize_text aux_characterize_to vmod_attempts_characterize amod_attempts_previous prep_from_differs_attempts nsubj_differs_work advcl_differs_falls poss_work_our nn_modeling_discourse prep_of_topic_modeling amod_topic_general det_topic_the prep_under_falls_topic nsubj_falls_study mark_falls_Although det_study_this
W06-3401	P02-1047	p	A novel approach was described in -LRB- Marcu and Echihabi 2002 -RRB- which used an unsupervised training technique extracting relations that were explicitly and unamibiguously signalled and automatically labelling those examples as the training set	nn_set_training det_set_the prep_as_examples_set det_examples_those dobj_labelling_examples advmod_labelling_automatically nsubjpass_labelling_that conj_and_signalled_labelling advmod_signalled_unamibiguously advmod_signalled_explicitly auxpass_signalled_were nsubjpass_signalled_that conj_and_explicitly_unamibiguously rcmod_relations_labelling rcmod_relations_signalled dobj_extracting_relations nn_technique_training amod_technique_unsupervised det_technique_an vmod_used_extracting dobj_used_technique nsubj_used_which rcmod_Marcu_used dep_Marcu_2002 conj_and_Marcu_Echihabi prep_in_described_Echihabi prep_in_described_Marcu auxpass_described_was nsubjpass_described_approach amod_approach_novel det_approach_A
C04-1071	P02-1053	n	2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the document-level classification of sentiment -LRB- Turney 2002 Pang et al. 2002 -RRB- where a document is assumed to have only a single sentiment thus these studies are not applicable to our goal	poss_goal_our prep_to_applicable_goal neg_applicable_not cop_applicable_are nsubj_applicable_studies advmod_applicable_thus det_studies_these amod_sentiment_single det_sentiment_a advmod_sentiment_only dobj_have_sentiment aux_have_to xcomp_assumed_have auxpass_assumed_is nsubjpass_assumed_document advmod_assumed_where det_document_a rcmod_Pang_assumed num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_applicable conj_Turney_Pang conj_Turney_2002 dep_sentiment_Turney prep_of_classification_sentiment amod_classification_document-level det_classification_the prep_on_focused_classification nsubj_focused_studies nn_analysis_sentiment prep_on_studies_analysis amod_studies_prior det_studies_Some rcmod_Analysis_focused nn_Analysis_Sentiment prep_on_work_Analysis amod_work_Previous num_work_2
C04-1121	P02-1053	o	Much research is also being directed at acquiring affect lexica automatically -LRB- Turney 2002 Turney and Littman 2002 -RRB-	num_Littman_2002 conj_and_Turney_Littman conj_and_Turney_Turney num_Turney_2002 dep_automatically_Littman dep_automatically_Turney dep_automatically_Turney advmod_lexica_automatically dobj_affect_lexica dep_acquiring_affect prepc_at_directed_acquiring auxpass_directed_being advmod_directed_also aux_directed_is nsubjpass_directed_research amod_research_Much
C04-1145	P02-1053	o	SO can be used to classify reviews -LRB- e.g. movie reviews -RRB- as positive or negative -LRB- Turney 2002 -RRB- and applied to subjectivity analysis such as recognizing hostile messages classifying emails mining reviews -LRB- Wiebe et al. 2001 -RRB-	amod_Wiebe_2001 dep_Wiebe_al. nn_Wiebe_et dep_reviews_Wiebe amod_reviews_mining conj_emails_reviews amod_emails_classifying amod_messages_hostile dep_recognizing_emails dobj_recognizing_messages prepc_such_as_analysis_recognizing nn_analysis_subjectivity prep_to_applied_analysis nsubjpass_applied_SO dep_Turney_2002 dep_positive_Turney conj_or_positive_negative nn_reviews_movie advmod_reviews_e.g. dep_reviews_reviews prep_as_classify_negative prep_as_classify_positive dobj_classify_reviews aux_classify_to conj_and_used_applied xcomp_used_classify auxpass_used_be aux_used_can nsubjpass_used_SO
C04-1200	P02-1053	o	Recent computational work either focuses on sentence subjectivity -LRB- Wiebe et al. 2002 Riloff et al. 2003 -RRB- concentrates just on explicit statements of evaluation such as of films -LRB- Turney 2002 Pang et al. 2002 -RRB- or focuses on just one aspect of opinion e.g. -LRB- Hatzivassiloglou and McKeown 1997 -RRB- on adjectives	num_McKeown_1997 conj_and_Hatzivassiloglou_McKeown prep_on_aspect_adjectives appos_aspect_McKeown appos_aspect_Hatzivassiloglou dep_aspect_e.g. prep_of_aspect_opinion num_aspect_one advmod_aspect_just prep_on_focuses_aspect dep_al._2002 nn_al._et nn_al._Pang conj_or_Turney_focuses dep_Turney_al. num_Turney_2002 dep_films_focuses dep_films_Turney pobj_statements_films prepc_as_of_statements_of mwe_statements_such prep_of_statements_evaluation amod_statements_explicit prep_on_concentrates_statements advmod_concentrates_just dep_al._2003 nn_al._et nn_al._Riloff dep_Wiebe_al. dep_Wiebe_2002 dep_Wiebe_al. nn_Wiebe_et appos_subjectivity_Wiebe nn_subjectivity_sentence dep_focuses_concentrates prep_on_focuses_subjectivity preconj_focuses_either nsubj_focuses_work amod_work_computational amod_work_Recent
C08-1024	P02-1053	o	Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis -LRB- Turney 2002 Takamura et al. 2006 Kaji and Kitsuregawa 2006 -RRB-	dep_Kaji_2006 conj_and_Kaji_Kitsuregawa dep_Takamura_Kitsuregawa dep_Takamura_Kaji num_Takamura_2006 nn_Takamura_al. nn_Takamura_et dep_Turney_Takamura appos_Turney_2002 nn_analysis_sentiment dep_orientation_Turney conj_and_orientation_analysis amod_orientation_semantic prep_of_recognition_analysis prep_of_recognition_orientation det_recognition_the cop_recognition_is nsubj_recognition_line poss_work_our prep_to_related_work advmod_related_closely amod_line_related prep_of_line_research det_line_Another
C08-1031	P02-1053	o	One major focus is sentiment classification and opinion mining -LRB- e.g. Pang et al 2002 Turney 2002 Hu and Liu 2004 Wilson et al 2004 Kim and Hovy 2004 Popescu and Etzioni 2005 -RRB- 2008	dep_2008_Pang dep_2008_e.g. num_Etzioni_2005 conj_and_Popescu_Etzioni num_Hovy_2004 conj_and_Kim_Hovy nn_2004_al num_Wilson_2004 nn_Wilson_et num_Liu_2004 conj_and_Hu_Liu num_Turney_2002 dep_al_2002 nn_al_et dep_Pang_Etzioni dep_Pang_Popescu dep_Pang_Hovy dep_Pang_Kim dep_Pang_Wilson dep_Pang_Liu dep_Pang_Hu dep_Pang_Turney advmod_Pang_al dep_mining_2008 nn_mining_opinion conj_and_classification_mining nn_classification_sentiment cop_classification_is nsubj_classification_focus amod_focus_major num_focus_One
C08-1031	P02-1053	n	Point-wise mutual information -LRB- PMI -RRB- is commonly used for computing the association of two terms -LRB- e.g. Turney 2002 -RRB- which is defined as nullnullnull null null null null nullnullnull nullnullnullnull nullnull nullnull null null null nullnullnullnullnull However we argue that PMI is not a suitable measure for our purpose	poss_purpose_our prep_for_measure_purpose amod_measure_suitable det_measure_a neg_measure_not cop_measure_is nsubj_measure_PMI mark_measure_that ccomp_argue_measure nsubj_argue_we amod_nullnullnullnullnull_null amod_nullnullnullnullnull_null amod_nullnullnullnullnull_null nn_nullnullnullnullnull_nullnull nn_nullnullnullnullnull_nullnull nn_nullnullnullnull_nullnullnull amod_nullnullnullnull_null amod_nullnullnullnull_null rcmod_null_argue advmod_null_However conj_null_nullnullnullnullnull conj_null_nullnullnullnull amod_null_null nn_null_nullnullnull prep_as_defined_null auxpass_defined_is nsubjpass_defined_which num_Turney_2002 dep_e.g._Turney rcmod_terms_defined dep_terms_e.g. num_terms_two prep_of_association_terms det_association_the dobj_computing_association prepc_for_used_computing advmod_used_commonly auxpass_used_is nsubjpass_used_information appos_information_PMI amod_information_mutual amod_information_Point-wise
C08-1031	P02-1053	o	Sentiment classification at the document level investigates ways to classify each evaluative document -LRB- e.g. product review -RRB- as positive or negative -LRB- Pang et al 2002 Turney 2002 -RRB-	num_Turney_2002 num_al_2002 dep_Pang_Turney dep_Pang_al nn_Pang_et conj_or_positive_negative nn_review_product dep_review_e.g. dep_document_review amod_document_evaluative det_document_each prep_as_classify_negative prep_as_classify_positive dobj_classify_document aux_classify_to vmod_ways_classify dep_investigates_Pang dobj_investigates_ways nsubj_investigates_classification nn_level_document det_level_the prep_at_classification_level nn_classification_Sentiment
C08-1052	P02-1053	o	The acquisition of clues is a key technology in these research efforts as seen in learning methods for document-level SA -LRB- Hatzivassiloglou and McKeown 1997 Turney 2002 -RRB- and for phraselevel SA -LRB- Wilson et al. 2005 Kanayama and Nasukawa 2006 -RRB-	appos_Kanayama_2006 conj_and_Kanayama_Nasukawa dep_Wilson_Nasukawa dep_Wilson_Kanayama amod_Wilson_2005 dep_Wilson_al. nn_Wilson_et dep_SA_Wilson amod_SA_phraselevel pobj_for_SA dep_Turney_2002 num_McKeown_1997 conj_and_Hatzivassiloglou_McKeown nn_Hatzivassiloglou_SA amod_Hatzivassiloglou_document-level prep_for_methods_McKeown prep_for_methods_Hatzivassiloglou amod_methods_learning prep_in_seen_methods mark_seen_as nn_efforts_research det_efforts_these conj_and_technology_for dep_technology_Turney advcl_technology_seen prep_in_technology_efforts amod_technology_key det_technology_a cop_technology_is nsubj_technology_acquisition prep_of_acquisition_clues det_acquisition_The
C08-1103	P02-1053	o	-LRB- 2002 -RRB- Turney -LRB- 2002 -RRB- -RRB- we are interested in fine-grained subjectivity analysis which is concerned with subjectivity at the phrase or clause level	nn_level_clause conj_or_phrase_level det_phrase_the prep_at_subjectivity_level prep_at_subjectivity_phrase prep_with_concerned_subjectivity auxpass_concerned_is nsubjpass_concerned_which rcmod_analysis_concerned nn_analysis_subjectivity amod_analysis_fine-grained prep_in_interested_analysis cop_interested_are nsubj_interested_we dep_interested_Turney dep_interested_2002 dep_Turney_2002
C08-1111	P02-1053	o	80 8.0 % Positive child education Positive cost Negative SUBJECT increase Figure 3 An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification ranging from the use of co-occurrence with typical positive and negative words -LRB- Turney 2002 -RRB- to bag of words -LRB- Pang et al. 2002 -RRB- and dependency structure -LRB- Kudo and Matsumoto 2004 -RRB-	dep_Kudo_2004 conj_and_Kudo_Matsumoto nn_structure_dependency amod_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_bag_words amod_Turney_2002 dep_words_Turney amod_words_negative amod_words_positive amod_words_typical conj_and_positive_negative prep_with_use_words prep_of_use_co-occurrence det_use_the prep_to_ranging_bag prep_from_ranging_use nn_classification_polarity nn_classification_sentiment prep_for_proposed_classification auxpass_proposed_been advmod_proposed_already aux_proposed_have nsubjpass_proposed_methods amod_methods_Various vmod_lattice_ranging rcmod_lattice_proposed amod_lattice_word-polarity det_lattice_a dep_example_Matsumoto dep_example_Kudo conj_and_example_structure dep_example_Pang prep_of_example_lattice det_example_An dep_Figure_structure dep_Figure_example num_Figure_3 nn_Figure_increase amod_Figure_SUBJECT amod_Figure_Negative nn_Figure_cost amod_Figure_Positive nn_Figure_education nn_Figure_child amod_Figure_Positive amod_Figure_% num_Figure_80 number_%_8.0
C08-1111	P02-1053	o	-LRB- 2007 -RRB- apply the theory of -LRB- Hatzivassiloglou and McKeown 1997 -RRB- and -LRB- Turney 2002 -RRB- to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words -LRB- e.g. joy fear -RRB-	appos_joy_fear pobj_e.g._joy dep_words_e.g. nn_words_emotion num_words_six amod_words_content conj_and_distribution_words prep_over_distribution_words amod_distribution_co-occurrence det_distribution_the prep_on_based_words prep_on_based_distribution vmod_method_based det_method_a dobj_propose_method nn_classification_emotion prep_to_Turney_classification amod_Turney_2002 conj_and_Hatzivassiloglou_propose conj_and_Hatzivassiloglou_Turney dep_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown prep_of_theory_propose prep_of_theory_Turney prep_of_theory_McKeown prep_of_theory_Hatzivassiloglou det_theory_the dobj_apply_theory nsubj_apply_2007
C08-1135	P02-1053	o	Another possible comparison could be with a version of Turney 's -LRB- 2002 -RRB- sentiment classification method applied to Chinese	prep_to_applied_Chinese vmod_method_applied nn_method_classification nn_method_sentiment num_method_2002 poss_method_Turney prep_of_version_method det_version_a prep_with_be_version aux_be_could nsubj_be_comparison amod_comparison_possible det_comparison_Another
C08-1135	P02-1053	o	Turney -LRB- 2002 -RRB- describes a method of sentiment classification using two human-selected seed words -LRB- the words poor and excellent -RRB- in conjunction with a very large text corpus the semantic orientation of phrases is computed as their association with the seed words -LRB- as measured by pointwise mutual information -RRB-	amod_information_mutual nn_information_pointwise prep_by_measured_information mark_measured_as dep_words_measured nn_words_seed det_words_the prep_with_association_words poss_association_their prep_as_computed_association auxpass_computed_is nsubjpass_computed_orientation prep_of_orientation_phrases amod_orientation_semantic det_orientation_the nn_corpus_text amod_corpus_large det_corpus_a advmod_large_very prep_with_conjunction_corpus conj_and_poor_excellent amod_words_excellent amod_words_poor det_words_the prep_in_words_conjunction dep_words_words nn_words_seed amod_words_human-selected num_words_two dobj_using_words nn_classification_sentiment prep_of_method_classification det_method_a parataxis_describes_computed xcomp_describes_using dobj_describes_method nsubj_describes_Turney appos_Turney_2002
D07-1113	P02-1053	o	-LRB- 2002 -RRB- and Turney -LRB- 2002 -RRB- classified sentiment polarity of reviews at the document level	nn_level_document det_level_the prep_of_polarity_reviews nn_polarity_sentiment amod_polarity_classified nn_polarity_Turney appos_Turney_2002 prep_at_2002_level conj_and_2002_polarity
D07-1114	P02-1053	o	This direction has been forming the mainstream of research on opinion-sensitive text processing -LRB- Pang et al. 2002 Turney 2002 etc. -RRB-	amod_Turney_etc. num_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_processing_text amod_processing_opinion-sensitive prep_of_mainstream_research det_mainstream_the dep_forming_Pang prep_on_forming_processing dobj_forming_mainstream aux_forming_been aux_forming_has nsubj_forming_direction det_direction_This ccomp_``_forming
D07-1115	P02-1053	o	This idea is the same as -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_as_Turney prep_same_as det_same_the cop_same_is nsubj_same_idea det_idea_This
D07-1115	P02-1053	p	-LRB- Turney 2002 -RRB- is one of the most famous work that discussed learning polarity from corpus	prep_from_learning_corpus dobj_learning_polarity xcomp_discussed_learning nsubj_discussed_that rcmod_work_discussed amod_work_famous det_work_the advmod_famous_most prep_of_one_work cop_one_is nsubj_one_Turney amod_Turney_2002
D07-1115	P02-1053	o	The polarity value proposed by -LRB- Turney 2002 -RRB- is as follows	mark_follows_as advcl_is_follows nsubj_is_value dep_Turney_2002 agent_proposed_Turney vmod_value_proposed nn_value_polarity det_value_The ccomp_``_is
D07-1115	P02-1053	o	Typically a small set of seed polar phrases are prepared and new polar phrases are detected based on the strength of co-occurrence with the seeds -LRB- Hatzivassiloglous and McKeown 1997 Turney 2002 Kanayama and Nasukawa 2006 -RRB-	amod_Turney_2006 conj_and_Turney_Nasukawa conj_and_Turney_Kanayama conj_and_Turney_2002 dep_Hatzivassiloglous_Nasukawa dep_Hatzivassiloglous_Kanayama dep_Hatzivassiloglous_2002 dep_Hatzivassiloglous_Turney dep_Hatzivassiloglous_1997 conj_and_Hatzivassiloglous_McKeown dep_seeds_McKeown dep_seeds_Hatzivassiloglous det_seeds_the prep_with_strength_seeds prep_of_strength_co-occurrence det_strength_the prep_based_on_detected_strength auxpass_detected_are nsubjpass_detected_phrases amod_phrases_polar amod_phrases_new conj_and_prepared_detected auxpass_prepared_are nsubjpass_prepared_set advmod_prepared_Typically amod_phrases_polar nn_phrases_seed prep_of_set_phrases amod_set_small det_set_a
D07-1115	P02-1053	o	In summary the strength of our approach is to exploit extremely precise structural clues and to use 5 Semantic Orientation in -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_in_Turney nn_Orientation_Semantic num_Orientation_5 prep_use_in dobj_use_Orientation aux_use_to nsubj_use_strength amod_clues_structural amod_clues_precise advmod_precise_extremely dobj_exploit_clues aux_exploit_to conj_and_is_use xcomp_is_exploit nsubj_is_strength prep_in_is_summary poss_approach_our prep_of_strength_approach det_strength_the
D07-1115	P02-1053	o	In Turneys work the co-occurrence is considered as the appearance in the same window -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_window_Turney amod_window_same det_window_the prep_in_appearance_window det_appearance_the prep_as_considered_appearance auxpass_considered_is nsubjpass_considered_co-occurrence prep_in_considered_work det_co-occurrence_the amod_work_Turneys
D07-1115	P02-1053	o	For example if the lexicon contains an adjective excellent it matches every adjective phrase that includes excellent such as view-excellent etc. As a baseline we built lexicon similarly by using polarity value of -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_of_Turney prep_value_of nn_value_polarity dobj_using_value prepc_by_built_using advmod_built_similarly dobj_built_lexicon nsubj_built_we det_baseline_a amod_etc._view-excellent prep_such_as_excellent_etc. acomp_includes_excellent nsubj_includes_that rcmod_phrase_includes amod_phrase_adjective det_phrase_every parataxis_matches_built prep_as_matches_baseline dobj_matches_phrase nsubj_matches_it advcl_matches_contains prep_for_matches_example amod_excellent_adjective det_excellent_an dobj_contains_excellent nsubj_contains_lexicon mark_contains_if det_lexicon_the
D07-1115	P02-1053	o	Its size is compatible to -LRB- Turney and Littman 2002 -RRB-	amod_Turney_2002 conj_and_Turney_Littman dep_to_Littman dep_to_Turney prep_compatible_to cop_compatible_is nsubj_compatible_size poss_size_Its
D07-1115	P02-1053	n	Turneys method did not work well although they reported 80 % accuracy in -LRB- Turney and Littman 2002 -RRB-	dep_Turney_2002 conj_and_Turney_Littman dep_in_Littman dep_in_Turney prep_accuracy_in amod_accuracy_% number_%_80 dobj_reported_accuracy nsubj_reported_they mark_reported_although advcl_work_reported advmod_work_well neg_work_not aux_work_did nsubj_work_method nn_method_Turneys
D08-1058	P02-1053	o	Turney -LRB- 2002 -RRB- predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs which is denoted as the semantic oriented method	amod_method_oriented amod_method_semantic det_method_the prep_as_denoted_method auxpass_denoted_is nsubjpass_denoted_which rcmod_adjectives_denoted conj_or_adjectives_adverbs dobj_contain_adverbs dobj_contain_adjectives nsubj_contain_that rcmod_review_contain det_review_the prep_in_phrases_review det_phrases_the prep_of_orientation_phrases amod_orientation_semantic amod_orientation_average det_orientation_the det_review_a prep_of_orientation_review nn_orientation_sentiment det_orientation_the prep_by_predicates_orientation dobj_predicates_orientation nsubj_predicates_Turney appos_Turney_2002
D09-1017	P02-1053	o	Sentiment summarization has been well studied in the past decade -LRB- Turney 2002 Pang et al. 2002 Dave et al. 2003 Hu and Liu 2004a 2004b Carenini et al. 2006 Liu et al. 2007 -RRB-	num_Liu_2007 nn_Liu_al. nn_Liu_et num_Carenini_2006 nn_Carenini_al. nn_Carenini_et conj_and_Hu_2004b conj_and_Hu_2004a conj_and_Hu_Liu num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_Liu conj_Pang_Carenini conj_Pang_2004b conj_Pang_2004a conj_Pang_Liu conj_Pang_Hu conj_Pang_Dave num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang appos_Turney_2002 amod_decade_past det_decade_the dep_studied_Turney prep_in_studied_decade advmod_studied_well auxpass_studied_been aux_studied_has nsubjpass_studied_summarization nn_summarization_Sentiment
D09-1019	P02-1053	o	There are many research directions e.g. sentiment classification -LRB- classifying an opinion document as positive or negative -RRB- -LRB- e.g. Pang Lee and Vaithyanathan 2002 Turney 2002 -RRB- subjectivity classification -LRB- determining whether a sentence is subjective or objective and its associated opinion -RRB- -LRB- Wiebe and Wilson 2002 Yu and Hatzivassiloglou 2003 Wilson et al 2004 Kim and Hovy 2004 Riloff and Wiebe 2005 -RRB- feature/topic-based sentiment analysis -LRB- assigning positive or negative sentiments to topics or product features -RRB- -LRB- Hu and Liu 2004 Popescu and Etzioni 2005 Carenini et al. 2005 Ku et al. 2006 Kobayashi Inui and Matsumoto 2007 Titov and McDonald	num_Ku_2006 nn_Ku_al. nn_Ku_et num_Carenini_2005 nn_Carenini_al. nn_Carenini_et conj_and_Popescu_McDonald conj_and_Popescu_Titov conj_and_Popescu_2007 conj_and_Popescu_Matsumoto conj_and_Popescu_Inui conj_and_Popescu_Kobayashi conj_and_Popescu_Ku conj_and_Popescu_Carenini conj_and_Popescu_2005 conj_and_Popescu_Etzioni num_Liu_2004 dep_Hu_McDonald dep_Hu_Titov dep_Hu_2007 dep_Hu_Matsumoto dep_Hu_Inui dep_Hu_Kobayashi dep_Hu_Ku dep_Hu_Carenini dep_Hu_2005 dep_Hu_Etzioni dep_Hu_Popescu conj_and_Hu_Liu nn_features_product conj_or_topics_features amod_sentiments_negative amod_sentiments_positive conj_or_positive_negative prep_to_assigning_features prep_to_assigning_topics dobj_assigning_sentiments dep_analysis_Liu dep_analysis_Hu dep_analysis_assigning nn_analysis_sentiment amod_analysis_feature/topic-based amod_al_2004 nn_al_et nn_al_Wilson dep_Yu_2005 conj_and_Yu_Wiebe conj_and_Yu_Riloff conj_and_Yu_2004 conj_and_Yu_Hovy conj_and_Yu_Kim conj_and_Yu_al conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Wiebe_Wiebe dep_Wiebe_Riloff dep_Wiebe_2004 dep_Wiebe_Hovy dep_Wiebe_Kim dep_Wiebe_al dep_Wiebe_2003 dep_Wiebe_Hatzivassiloglou dep_Wiebe_Yu conj_and_Wiebe_2002 conj_and_Wiebe_Wilson dep_opinion_2002 dep_opinion_Wilson dep_opinion_Wiebe amod_opinion_associated poss_opinion_its nsubj_objective_sentence conj_or_subjective_objective cop_subjective_is nsubj_subjective_sentence mark_subjective_whether det_sentence_a dobj_determining_analysis conj_and_determining_opinion ccomp_determining_objective ccomp_determining_subjective dep_classification_opinion dep_classification_determining nn_classification_subjectivity dep_classification_2002 dep_classification_Vaithyanathan dep_classification_Lee dep_classification_Pang dep_classification_e.g. dep_Turney_2002 dep_Pang_Turney conj_and_Pang_2002 conj_and_Pang_Vaithyanathan conj_and_Pang_Lee conj_or_positive_negative nn_document_opinion det_document_an parataxis_classifying_classification prep_as_classifying_negative prep_as_classifying_positive dobj_classifying_document dep_classification_classifying nn_classification_sentiment pobj_e.g._classification prep_directions_e.g. nn_directions_research amod_directions_many nsubj_are_directions expl_are_There ccomp_``_are
D09-1019	P02-1053	o	One of the main directions is sentiment classification which classifies the whole opinion document -LRB- e.g. a product review -RRB- as positive or negative -LRB- e.g. Pang et al 2002 Turney 2002 Dave et al 2003 Ng et al. 2006 McDonald et al 2007 -RRB-	appos_al_2007 nn_al_et nn_al_McDonald num_al._2006 nn_al._et nn_al._Ng amod_al_2003 nn_al_et nn_al_Dave conj_Turney_al conj_Turney_al. conj_Turney_al appos_Turney_2002 dep_Pang_Turney num_Pang_2002 nn_Pang_al nn_Pang_et dep_e.g._Pang dep_positive_e.g. conj_or_positive_negative nn_review_product det_review_a dep_review_e.g. dep_document_review nn_document_opinion amod_document_whole det_document_the prep_as_classifies_negative prep_as_classifies_positive dobj_classifies_document nsubj_classifies_which rcmod_classification_classifies nn_classification_sentiment cop_classification_is nsubj_classification_One amod_directions_main det_directions_the prep_of_One_directions
D09-1020	P02-1053	o	They may rely only on this information -LRB- e.g. -LRB- Turney 2002 Whitelaw et al. 2005 Riloff and Wiebe 2003 -RRB- -RRB- or they may combine it with additional information as well -LRB- e.g. -LRB- Yu and Hatzivassiloglou 2003 Kim and Hovy 2004 Bloom et al. 2007 Wilson et al. 2005a -RRB- -RRB-	appos_Wilson_2005a nn_Wilson_al. nn_Wilson_et num_Bloom_2007 nn_Bloom_al. nn_Bloom_et num_Kim_2004 conj_and_Kim_Hovy dep_Yu_Wilson conj_and_Yu_Bloom conj_and_Yu_Hovy conj_and_Yu_Kim conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_e.g._Bloom dep_e.g._Kim dep_e.g._2003 dep_e.g._Hatzivassiloglou dep_e.g._Yu dep_well_e.g. advmod_well_as amod_information_well amod_information_additional prep_with_combine_information dobj_combine_it aux_combine_may nsubj_combine_they appos_Riloff_2003 conj_and_Riloff_Wiebe conj_or_Whitelaw_combine dep_Whitelaw_Wiebe dep_Whitelaw_Riloff num_Whitelaw_2005 nn_Whitelaw_al. nn_Whitelaw_et dep_Turney_combine dep_Turney_Whitelaw dep_Turney_2002 dep_e.g._Turney dep_information_e.g. det_information_this prep_on_rely_information advmod_rely_only aux_rely_may nsubj_rely_They
D09-1061	P02-1053	p	Turneys -LRB- 2002 -RRB- work is perhaps one of the most notable examples of unsupervised polarity classification	nn_classification_polarity amod_classification_unsupervised prep_of_examples_classification amod_examples_notable det_examples_the advmod_notable_most prep_of_one_examples advmod_one_perhaps cop_one_is nsubj_one_work nn_work_Turneys appos_Turneys_2002
D09-1063	P02-1053	o	Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation -LRB- Turney 2002 Wilson et al. 2005 Pang and Lee 2008 -RRB-	amod_Pang_2008 conj_and_Pang_Lee num_Wilson_2005 nn_Wilson_al. nn_Wilson_et dep_Turney_Lee dep_Turney_Pang conj_Turney_Wilson dep_Turney_2002 dep_orientation_Turney amod_orientation_semantic amod_orientation_negative amod_orientation_positive conj_and_positive_negative prep_with_tagged_orientation vmod_words_tagged prep_of_lexicons_words prep_of_use_lexicons dobj_make_use advmod_make_often nsubj_make_this mark_make_for dep_methods_make nn_methods_Automatic
E06-1025	P02-1053	o	determining document orientation -LRB- or polarity -RRB- as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter -LRB- Pang and Lee 2004 Turney 2002 -RRB- 3	amod_Turney_2002 dep_Pang_Turney conj_and_Pang_2004 conj_and_Pang_Lee amod_matter_subject poss_matter_its amod_opinion_Negative det_opinion_a dep_Positive_2004 dep_Positive_Lee dep_Positive_Pang prep_on_Positive_matter conj_or_Positive_opinion amod_a_opinion amod_a_Positive dobj_expresses_a nsubj_expresses_text mark_expresses_if amod_text_Subjective amod_text_given det_text_a advcl_deciding_expresses pcomp_in_deciding pcomp_as_in cc_polarity_or appos_orientation_polarity nn_orientation_document conj_determining_3 prep_determining_as dobj_determining_orientation dep_``_determining
E06-1025	P02-1053	p	The conceptually simplest approach to this latter problem is probably Turneys -LRB- 2002 -RRB- who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to but more sophisticated approaches arealsopossible -LRB- Hatzivassiloglou and Wiebe 2000 Riloff et al. 2003 Wilson et al. 2004 -RRB-	num_Wilson_2004 nn_Wilson_al. nn_Wilson_et num_Riloff_2003 nn_Riloff_al. nn_Riloff_et dep_Hatzivassiloglou_Wilson conj_and_Hatzivassiloglou_Riloff conj_and_Hatzivassiloglou_2000 conj_and_Hatzivassiloglou_Wiebe dep_arealsopossible_Riloff dep_arealsopossible_2000 dep_arealsopossible_Wiebe dep_arealsopossible_Hatzivassiloglou nn_arealsopossible_approaches amod_arealsopossible_sophisticated advmod_sophisticated_more prep_belong_to nsubj_belong_they rcmod_document_belong det_document_the prep_of_orientation_document det_orientation_the prep_of_representative_orientation prep_of_orientations_terms det_orientations_the prep_of_sum_orientations amod_sum_algebraic det_sum_the prep_as_considering_representative dobj_considering_sum num_Task_2 amod_results_interesting prepc_by_obtained_considering prep_on_obtained_Task dobj_obtained_results aux_obtained_has nsubj_obtained_who conj_but_Turneys_arealsopossible rcmod_Turneys_obtained appos_Turneys_2002 advmod_Turneys_probably cop_Turneys_is nsubj_Turneys_approach amod_problem_latter det_problem_this prep_to_approach_problem amod_approach_simplest advmod_approach_conceptually det_approach_The
E06-1026	P02-1053	o	This problem will be solved by incorporating other resources such as thesaurus or a dictionary orcombiningourmethodwithothermethods using external wider contexts -LRB- Suzuki et al. 2006 Turney 2002 Baron and Hirst 2004 -RRB-	amod_Baron_2004 conj_and_Baron_Hirst num_Turney_2002 dep_Suzuki_Hirst dep_Suzuki_Baron dep_Suzuki_Turney amod_Suzuki_2006 dep_Suzuki_al. nn_Suzuki_et amod_contexts_wider amod_contexts_external dobj_using_contexts vmod_orcombiningourmethodwithothermethods_using det_dictionary_a appos_thesaurus_Suzuki conj_or_thesaurus_orcombiningourmethodwithothermethods conj_or_thesaurus_dictionary prep_such_as_resources_orcombiningourmethodwithothermethods prep_such_as_resources_dictionary prep_such_as_resources_thesaurus amod_resources_other dobj_incorporating_resources agent_solved_incorporating auxpass_solved_be aux_solved_will nsubjpass_solved_problem det_problem_This
E06-1026	P02-1053	o	Turney -LRB- 2002 -RRB- applied an internet-based technique to the semantic orientation classification of phrases whichhadoriginallybeendevelopedforwordsentiment classification	nn_classification_whichhadoriginallybeendevelopedforwordsentiment appos_phrases_classification prep_of_classification_phrases nn_classification_orientation amod_classification_semantic det_classification_the amod_technique_internet-based det_technique_an prep_to_applied_classification dobj_applied_technique nsubj_applied_Turney appos_Turney_2002
E09-1004	P02-1053	o	2 Literature Survey The task of sentiment analysis has evolved from document level analysis -LRB- e.g. -LRB- Turney. 2002 -RRB- -LRB- Pang and Lee 2004 -RRB- -RRB- to sentence level analysis -LRB- e.g. -LRB- Hu and Liu. 2004 -RRB- -LRB- Kim and Hovy. 2004 -RRB- -LRB- Yu and Hatzivassiloglou 2003 -RRB- -RRB-	dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Kim_2004 conj_and_Kim_Hovy. dep_Hu_2004 conj_and_Hu_Liu. dep_e.g._Hatzivassiloglou dep_e.g._Yu dep_e.g._Hovy. dep_e.g._Kim appos_e.g._Liu. appos_e.g._Hu nn_analysis_level nn_analysis_sentence dep_Pang_2004 conj_and_Pang_Lee dep_Turney._e.g. prep_to_Turney._analysis dep_Turney._Lee dep_Turney._Pang amod_Turney._2002 dep_,_Turney. dep_-LRB-_e.g. nn_analysis_level nn_analysis_document prep_from_evolved_analysis aux_evolved_has nsubj_evolved_task nn_analysis_sentiment prep_of_task_analysis det_task_The rcmod_Survey_evolved nn_Survey_Literature num_Survey_2
H05-1043	P02-1053	n	While other systems such as -LRB- Hu and Liu 2004 Turney 2002 -RRB- have addressed these tasks to some degree OPINE is the first to report results	dobj_report_results aux_report_to xcomp_first_report det_first_the cop_first_is nsubj_first_OPINE advcl_first_addressed det_degree_some det_tasks_these prep_to_addressed_degree dobj_addressed_tasks aux_addressed_have nsubj_addressed_systems mark_addressed_While dep_Turney_2002 dep_Hu_Turney conj_and_Hu_2004 conj_and_Hu_Liu dep_as_2004 dep_as_Liu dep_as_Hu mwe_as_such prep_systems_as amod_systems_other
H05-1043	P02-1053	o	PMI + + is an extended version of -LRB- Turney 2002 -RRB- s method for finding the SO label of a phrase -LRB- as an attempt to deal with context-sensitive words -RRB-	amod_words_context-sensitive prep_with_deal_words aux_deal_to vmod_attempt_deal det_attempt_an prep_as_phrase_attempt det_phrase_a prep_of_label_phrase nn_label_SO det_label_the dobj_finding_label prepc_for_method_finding amod_method_s dep_Turney_2002 dep_version_method prep_of_version_Turney amod_version_extended det_version_an cop_version_is nsubj_version_+ nsubj_version_PMI conj_+_PMI_+
H05-1043	P02-1053	o	Subjective phrases are used by -LRB- Turney 2002 Pang and Vaithyanathan 2002 Kushal et al. 2003 Kim and Hovy 2004 -RRB- and others in order to classify reviews or sentences as positive or negative	conj_or_positive_negative prep_as_reviews_negative prep_as_reviews_positive conj_or_reviews_sentences dobj_classify_sentences dobj_classify_reviews aux_classify_to num_Kim_2004 conj_and_Kim_Hovy num_Kushal_2003 nn_Kushal_al. nn_Kushal_et vmod_Pang_classify prep_in_Pang_order conj_and_Pang_others conj_and_Pang_Hovy conj_and_Pang_Kim conj_and_Pang_Kushal conj_and_Pang_2002 conj_and_Pang_Vaithyanathan dep_Turney_others dep_Turney_Kim dep_Turney_Kushal dep_Turney_2002 dep_Turney_Vaithyanathan dep_Turney_Pang appos_Turney_2002 agent_used_Turney auxpass_used_are nsubjpass_used_phrases amod_phrases_Subjective
H05-1043	P02-1053	o	As a result the problem of opinion mining has seen increasing attention over the last three years from -LRB- Turney 2002 Hu and Liu 2004 -RRB- and many others	amod_others_many num_Hu_2004 conj_and_Hu_Liu conj_and_Turney_others dep_Turney_Liu dep_Turney_Hu appos_Turney_2002 num_years_three amod_years_last det_years_the prep_from_increasing_others prep_from_increasing_Turney prep_over_increasing_years dobj_increasing_attention xcomp_seen_increasing aux_seen_has nsubj_seen_problem prep_as_seen_result nn_mining_opinion prep_of_problem_mining det_problem_the det_result_a
H05-1044	P02-1053	o	7 Related Work Much work on sentiment analysis classifies documents by their overall sentiment for example determining whether a review is positive or negative -LRB- e.g. -LRB- Turney 2002 Dave et al. 2003 Pang and Lee 2004 Beineke et al. 2004 -RRB- -RRB-	num_Beineke_2004 nn_Beineke_al. nn_Beineke_et num_Pang_2004 conj_and_Pang_Lee dep_Dave_Beineke dep_Dave_Lee dep_Dave_Pang num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Turney_Dave dep_Turney_2002 dep_e.g._Turney nsubj_negative_review dep_positive_e.g. conj_or_positive_negative cop_positive_is nsubj_positive_review mark_positive_whether det_review_a ccomp_determining_negative ccomp_determining_positive vmod_example_determining pobj_for_example ccomp_,_for amod_sentiment_overall poss_sentiment_their prep_by_classifies_sentiment dobj_classifies_documents nsubj_classifies_work nn_analysis_sentiment prep_on_work_analysis amod_work_Much rcmod_Work_classifies amod_Work_Related num_Work_7 dep_``_Work
H05-1044	P02-1053	o	A number of researchers have explored learning words and phrases with prior positive or negative polarity -LRB- another term is semantic orientation -RRB- -LRB- e.g. -LRB- Hatzivassiloglou and McKeown 1997 Kamps and Marx 2002 Turney 2002 -RRB- -RRB-	dep_Turney_2002 dep_Kamps_Turney conj_and_Kamps_2002 conj_and_Kamps_Marx dep_Hatzivassiloglou_2002 dep_Hatzivassiloglou_Marx dep_Hatzivassiloglou_Kamps conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown dep_e.g._1997 dep_e.g._McKeown dep_e.g._Hatzivassiloglou amod_orientation_semantic cop_orientation_is nsubj_orientation_term det_term_another dep_polarity_e.g. dep_polarity_orientation amod_polarity_negative amod_polarity_positive amod_polarity_prior conj_or_positive_negative conj_and_words_phrases prep_with_learning_polarity dobj_learning_phrases dobj_learning_words xcomp_explored_learning aux_explored_have nsubj_explored_number prep_of_number_researchers det_number_A
H05-1045	P02-1053	o	-LRB- 2002 -RRB- Turney -LRB- 2002 -RRB- Dave et al.	dep_Dave_al. nn_Dave_et appos_Turney_Dave appos_Turney_2002 dep_2002_Turney dep_''_2002
H05-1071	P02-1053	o	6 Conclusions and Future Directions In previous work statistical NLP computation over large corpora has been a slow of ine process as in KNOWITALL -LRB- Etzioni et al. 2005 -RRB- and also in PMI-IR applications such as sentiment classi cation -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_cation_Turney nn_cation_classi nn_cation_sentiment prep_such_as_applications_cation nn_applications_PMI-IR pobj_in_applications amod_Etzioni_2005 dep_Etzioni_al. nn_Etzioni_et pobj_in_KNOWITALL pcomp_as_in nn_process_ine conj_and_slow_in advmod_slow_also dep_slow_Etzioni prep_slow_as prep_of_slow_process amod_a_in amod_a_slow dep_been_a dep_has_been amod_corpora_large dep_computation_has prep_over_computation_corpora nn_computation_NLP amod_computation_statistical amod_work_previous amod_Directions_Future dep_Conclusions_computation prep_in_Conclusions_work conj_and_Conclusions_Directions num_Conclusions_6 dep_``_Directions dep_``_Conclusions
H05-1073	P02-1053	o	-LRB- Turney 2002 -RRB- -LRB- Bai Padman and Airoldi 2004 -RRB- -LRB- Beineke Hastie and Vaithyanathan 2003 -RRB- -LRB- Mullen and Collier 2003 -RRB- -LRB- Pang and Lee 2003 -RRB-	amod_Pang_2003 conj_and_Pang_Lee dep_Mullen_2003 conj_and_Mullen_Collier dep_Beineke_2003 conj_and_Beineke_Vaithyanathan conj_and_Beineke_Hastie dep_Bai_2004 conj_and_Bai_Airoldi conj_and_Bai_Padman appos_Turney_Lee appos_Turney_Pang appos_Turney_Collier appos_Turney_Mullen appos_Turney_Vaithyanathan appos_Turney_Hastie appos_Turney_Beineke appos_Turney_Airoldi appos_Turney_Padman appos_Turney_Bai amod_Turney_2002 dep_''_Turney
H05-1116	P02-1053	o	-LRB- 2002 -RRB- Turney -LRB- 2002 -RRB- Dave et al.	dep_Dave_al. nn_Dave_et appos_Turney_Dave appos_Turney_2002 dep_2002_Turney dep_''_2002
I05-2011	P02-1053	o	Turney -LRB- 2002 -RRB- and Wiebe -LRB- 2000 -RRB- focused on learning adjectives and adjectival phrases and Wiebe et al.	nn_al._et nn_al._Wiebe dep_adjectives_phrases conj_and_adjectives_adjectival conj_and_learning_al. dobj_learning_adjectival dobj_learning_adjectives prepc_on_focused_al. prepc_on_focused_learning nsubj_focused_Wiebe nsubj_focused_Turney appos_Wiebe_2000 conj_and_Turney_Wiebe appos_Turney_2002
I05-2030	P02-1053	o	-LRB- Dave et al. 2003 Pang and Lee 2004 Turney 2002 -RRB- -RRB-	dep_Turney_2002 dep_Pang_Turney num_Pang_2004 conj_and_Pang_Lee dep_Dave_Lee dep_Dave_Pang appos_Dave_2003 dep_Dave_al. nn_Dave_et dep_''_Dave
I08-1040	P02-1053	o	-LRB- Turney 2002 Pang et al. 2002 Dave at al. 2003 -RRB-	dep_al._2003 prep_at_Dave_al. dep_Pang_Dave num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang appos_Turney_2002 dep_''_Turney
I08-1040	P02-1053	o	2 RelatedWork 2.1 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment -LRB- Turney 2002 Pang et al. 2002 Dave at al. 2003 -RRB-	dep_al._2003 prep_at_Dave_al. num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Dave conj_Turney_Pang conj_Turney_2002 dep_sentiment_Turney amod_sentiment_negative amod_sentiment_positive conj_and_positive_negative prep_of_classification_sentiment amod_classification_binary det_classification_the prep_on_focused_classification aux_focused_has nsubj_focused_work amod_texts_opinionated amod_texts_categorizing prep_of_problem_texts det_problem_the prep_on_work_problem amod_work_previous amod_work_Most rcmod_Classification_focused nn_Classification_Sentiment num_Classification_2.1 nn_Classification_RelatedWork num_Classification_2
I08-1040	P02-1053	o	But it is close to the paradigm described by Yarowsky -LRB- 1995 -RRB- and Turney -LRB- 2002 -RRB- as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples	amod_samples_unlabelled prep_with_enlarged_samples advmod_enlarged_incrementally cop_enlarged_is nsubj_enlarged_which ccomp_set_enlarged vmod_data_set nn_data_seed amod_data_small det_data_a advmod_small_relatively pobj_self-training_data prepc_based_on_self-training_on dobj_employs_self-training advmod_employs_also nsubj_employs_it mark_employs_as appos_Turney_2002 conj_and_Yarowsky_Turney appos_Yarowsky_1995 advcl_described_employs agent_described_Turney agent_described_Yarowsky vmod_paradigm_described det_paradigm_the prep_to_close_paradigm cop_close_is nsubj_close_it cc_close_But
J06-3003	P02-1053	o	Measures of attributional similarity have been studied extensively due to their applications in problems such as recognizing synonyms -LRB- Landauer and Dumais 1997 -RRB- information retrieval -LRB- Deerwester et al. 1990 -RRB- determining semantic orientation -LRB- Turney 2002 -RRB- grading student essays -LRB- Rehder et al. 1998 -RRB- measuring textual cohesion -LRB- Morris and Hirst 1991 -RRB- and word sense disambiguation -LRB- Lesk 1986 -RRB-	num_Lesk_1986 appos_disambiguation_Lesk nn_disambiguation_sense nn_disambiguation_word num_Hirst_1991 conj_and_Morris_Hirst amod_cohesion_textual dep_measuring_Hirst dep_measuring_Morris dobj_measuring_cohesion advmod_1998_al. nn_al._et num_Rehder_1998 appos_essays_Rehder nn_essays_student amod_essays_grading num_Turney_2002 appos_orientation_Turney amod_orientation_semantic dobj_determining_orientation dep_al._1990 nn_al._et advmod_Deerwester_al. appos_retrieval_Deerwester nn_retrieval_information num_Dumais_1997 conj_and_Landauer_Dumais conj_and_synonyms_disambiguation conj_and_synonyms_measuring conj_and_synonyms_essays conj_and_synonyms_determining conj_and_synonyms_retrieval dep_synonyms_Dumais dep_synonyms_Landauer dobj_recognizing_disambiguation dobj_recognizing_measuring dobj_recognizing_essays dobj_recognizing_determining dobj_recognizing_retrieval dobj_recognizing_synonyms prepc_such_as_problems_recognizing prep_in_applications_problems poss_applications_their prep_due_to_studied_applications advmod_studied_extensively auxpass_studied_been aux_studied_have nsubjpass_studied_Measures amod_similarity_attributional prep_of_Measures_similarity
J06-3003	P02-1053	o	Many 412 Turney Similarity of Semantic Relations researchers have argued that metaphor is the heart of human thinking -LRB- Lakoff and Johnson 1980 Hofstadter and the Fluid Analogies Research Group 1995 Gentner et al. 2001 French 2002 -RRB-	amod_2002_French num_al._2001 nn_al._et nn_al._Gentner num_Group_1995 nn_Group_Research nn_Group_Analogies nn_Group_Fluid det_Group_the conj_and_Hofstadter_Group num_Johnson_1980 dep_Lakoff_2002 conj_and_Lakoff_al. conj_and_Lakoff_Group conj_and_Lakoff_Hofstadter conj_and_Lakoff_Johnson dep_thinking_al. dep_thinking_Hofstadter dep_thinking_Johnson dep_thinking_Lakoff amod_thinking_human prep_of_heart_thinking det_heart_the cop_heart_is nsubj_heart_metaphor mark_heart_that ccomp_argued_heart aux_argued_have nsubj_argued_Similarity nn_researchers_Relations nn_researchers_Semantic prep_of_Similarity_researchers nn_Similarity_Turney num_Similarity_412 amod_Similarity_Many
N03-1025	P02-1053	o	We are currently investigating more challenging problems like multiple category classification using the Reuters-21578 data set -LRB- Lewis 1992 -RRB- and subjective sentiment classification -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_classification_Turney nn_classification_sentiment amod_classification_subjective dep_Lewis_1992 conj_and_set_classification appos_set_Lewis nn_set_data nn_set_Reuters-21578 det_set_the dobj_using_classification dobj_using_set nn_classification_category amod_classification_multiple prep_like_problems_classification amod_problems_challenging amod_problems_more xcomp_investigating_using dobj_investigating_problems advmod_investigating_currently aux_investigating_are nsubj_investigating_We ccomp_``_investigating
N03-1025	P02-1053	o	Such techniques are currently being applied in many areas including language identification authorship attribution -LRB- Stamatatos et al. 2000 -RRB- text genre classification -LRB- Kesseler et al. 1997 Stamatatos et al. 2000 -RRB- topic identification -LRB- Dumais et al. 1998 Lewis 1992 McCallum 1998 Yang 1999 -RRB- and subjective sentiment classification -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_classification_Turney nn_classification_sentiment amod_classification_subjective dep_Yang_1999 conj_and_McCallum_classification conj_and_McCallum_Yang conj_and_McCallum_1998 dep_Lewis_classification dep_Lewis_Yang dep_Lewis_1998 dep_Lewis_McCallum num_Lewis_1992 dep_Dumais_Lewis appos_Dumais_1998 dep_Dumais_al. nn_Dumais_et dep_identification_Dumais nn_identification_topic num_Stamatatos_2000 nn_Stamatatos_al. nn_Stamatatos_et dep_Kesseler_Stamatatos appos_Kesseler_1997 dep_Kesseler_al. nn_Kesseler_et appos_classification_identification appos_classification_Kesseler nn_classification_genre nn_classification_text dep_,_classification amod_Stamatatos_2000 dep_Stamatatos_al. nn_Stamatatos_et nn_attribution_authorship dep_identification_Stamatatos conj_identification_attribution nn_identification_language prep_including_areas_identification amod_areas_many prep_in_applied_areas auxpass_applied_being advmod_applied_currently aux_applied_are nsubjpass_applied_techniques amod_techniques_Such dep_``_applied
N06-1026	P02-1053	o	-LRB- 2002 -RRB- and Turney -LRB- 2002 -RRB- classified sentiment polarity of reviews at the document level	nn_level_document det_level_the prep_of_polarity_reviews nn_polarity_sentiment amod_polarity_classified nn_polarity_Turney appos_Turney_2002 prep_at_2002_level conj_and_2002_polarity
N07-1037	P02-1053	o	Turney -LRB- 2002 -RRB- applied an internet-based technique to the semantic orientation classification of phrases which had originally been developed for word sentiment classification	nn_classification_sentiment nn_classification_word prep_for_developed_classification auxpass_developed_been advmod_developed_originally aux_developed_had nsubjpass_developed_which rcmod_phrases_developed prep_of_classification_phrases nn_classification_orientation amod_classification_semantic det_classification_the amod_technique_internet-based det_technique_an prep_to_applied_classification dobj_applied_technique nsubj_applied_Turney appos_Turney_2002
N07-1038	P02-1053	o	2 Related Work Sentiment Classi cation Traditionally categorization of opinion texts has been cast as a binary classication task -LRB- Pang et al. 2002 Turney 2002 Yu and Hatzivassiloglou 2003 Dave et al. 2003 -RRB-	num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Yu_Dave conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Turney_2003 dep_Turney_Hatzivassiloglou dep_Turney_Yu num_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_task_classication amod_task_binary det_task_a dep_cast_Pang prep_as_cast_task auxpass_cast_been aux_cast_has nsubjpass_cast_categorization nn_texts_opinion prep_of_categorization_texts rcmod_cation_cast advmod_cation_Traditionally nn_cation_Classi nn_cation_Sentiment nn_cation_Work amod_cation_Related num_cation_2
N07-1038	P02-1053	o	1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text -LRB- Pang et al. 2002 Turney 2002 Yu and Hatzivassiloglou 2003 -RRB-	dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Turney_Hatzivassiloglou dep_Turney_Yu num_Turney_2002 nn_al._et dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_text_opinion det_text_an prep_of_polarity_text det_polarity_the dobj_express_polarity aux_express_can nsubj_express_score mark_express_that amod_score_single det_score_a ccomp_assumption_express amod_assumption_implicit det_assumption_an dep_makes_Pang dobj_makes_assumption nsubj_makes_Introduction nn_categorization_sentiment prep_on_work_categorization amod_work_Previous dep_Introduction_work num_Introduction_1
N07-1039	P02-1053	o	Much of this work has utilized the fundamental concept of semantic orientation -LRB- Turney 2002 -RRB- however sentiment analysis still lacks a unified field theory	nn_theory_field amod_theory_unified det_theory_a dobj_lacks_theory advmod_lacks_still nsubj_lacks_analysis nn_analysis_sentiment amod_Turney_2002 amod_orientation_semantic prep_of_concept_orientation amod_concept_fundamental det_concept_the parataxis_utilized_lacks advmod_utilized_however dep_utilized_Turney dobj_utilized_concept aux_utilized_has nsubj_utilized_Much det_work_this prep_of_Much_work
N07-1039	P02-1053	o	Sentiment analysis includes a variety of different problems including sentiment classification techniques to classify reviews as positive or negative based on bag of words -LRB- Pang et al. 2002 -RRB- or positive and negative words -LRB- Turney 2002 Mullen and Collier 2004 -RRB- classifying sentences in a document as either subjective or objective -LRB- Riloff and Wiebe 2003 Pang and Lee 2004 -RRB- identifying or classifying appraisal targets -LRB- Nigam and Hurst 2004 -RRB- identifying the source of an opinion in a text -LRB- Choi et al. 2005 -RRB- whether the author is expressing the opinion or whether he is attributing the opinion to someone else and developing interactive and visual opinion mining methods -LRB- Gamon et al. 2005 Popescu and Etzioni 2005 -RRB-	dep_Popescu_2005 conj_and_Popescu_Etzioni dep_Gamon_Etzioni dep_Gamon_Popescu appos_Gamon_2005 dep_Gamon_al. nn_Gamon_et dep_methods_Gamon nn_methods_mining nn_methods_opinion amod_methods_visual amod_methods_interactive conj_and_interactive_visual dobj_developing_methods advmod_someone_else det_opinion_the prep_to_attributing_someone dobj_attributing_opinion aux_attributing_is nsubj_attributing_he mark_attributing_whether det_opinion_the conj_or_expressing_attributing dobj_expressing_opinion aux_expressing_is nsubj_expressing_author mark_expressing_whether det_author_the amod_Choi_2005 dep_Choi_al. nn_Choi_et det_text_a prep_in_opinion_text det_opinion_an prep_of_source_opinion det_source_the ccomp_identifying_attributing ccomp_identifying_expressing dep_identifying_Choi dobj_identifying_source dep_Nigam_2004 conj_and_Nigam_Hurst dep_targets_Hurst dep_targets_Nigam nn_targets_appraisal dobj_identifying_targets conj_or_identifying_classifying amod_Pang_2004 conj_and_Pang_Lee dep_Riloff_Lee dep_Riloff_Pang dep_Riloff_2003 conj_and_Riloff_Wiebe dep_subjective_Wiebe dep_subjective_Riloff conj_or_subjective_objective preconj_subjective_either det_document_a prep_as_sentences_objective prep_as_sentences_subjective prep_in_sentences_document amod_sentences_classifying dep_Mullen_2004 conj_and_Mullen_Collier dep_Turney_Collier dep_Turney_Mullen appos_Turney_2002 appos_words_Turney dep_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_bag_words conj_or_positive_negative prep_as_classify_negative prep_as_classify_positive dobj_classify_reviews aux_classify_to conj_and_techniques_developing conj_and_techniques_identifying conj_and_techniques_classifying conj_and_techniques_identifying conj_and_techniques_sentences dep_techniques_words conj_and_techniques_negative conj_or_techniques_positive dep_techniques_Pang prep_based_on_techniques_bag vmod_techniques_classify nn_techniques_classification nn_techniques_sentiment prep_including_problems_developing prep_including_problems_identifying prep_including_problems_identifying prep_including_problems_sentences prep_including_problems_negative prep_including_problems_positive prep_including_problems_techniques amod_problems_different prep_of_variety_problems det_variety_a dobj_includes_variety nsubj_includes_analysis nn_analysis_Sentiment
N07-2048	P02-1053	o	Turney -LRB- 2002 -RRB- has presented an unsupervised opinion classification algorithm called SO-PMI -LRB- Semantic Orientation Using Pointwise Mutual Information -RRB-	nn_Information_Mutual nn_Information_Pointwise dobj_Using_Information vmod_Orientation_Using nn_Orientation_Semantic dep_SO-PMI_Orientation dep_called_SO-PMI vmod_algorithm_called nn_algorithm_classification nn_algorithm_opinion amod_algorithm_unsupervised det_algorithm_an dobj_presented_algorithm aux_presented_has nsubj_presented_Turney appos_Turney_2002
N07-2048	P02-1053	o	2 Details of the SO-PMI Algorithm The SO-PMI algorithm -LRB- Turney 2002 -RRB- is used to estimate the semantic orientation -LRB- SO -RRB- of a phrase by 1http / / www.epinions.com 189 References Peter D. Turney	nn_Turney_D. nn_Turney_Peter nn_Turney_References num_Turney_189 nn_Turney_www.epinions.com det_phrase_a prep_of_orientation_phrase appos_orientation_SO amod_orientation_semantic det_orientation_the prep_by_estimate_1http dobj_estimate_orientation aux_estimate_to dep_used_Turney xcomp_used_estimate auxpass_used_is nsubjpass_used_Details amod_Turney_2002 dep_algorithm_Turney nn_algorithm_SO-PMI nn_algorithm_The nn_algorithm_Algorithm nn_algorithm_SO-PMI det_algorithm_the prep_of_Details_algorithm num_Details_2 ccomp_``_used
N07-4010	P02-1053	o	In related work -LRB- Chaovalit 2005 Turney 2002 -RRB- both supervised and unsupervised approaches have been shown to have their pros and cons	conj_and_pros_cons poss_pros_their dobj_have_cons dobj_have_pros aux_have_to xcomp_shown_have auxpass_shown_been aux_shown_have nsubjpass_shown_approaches prep_in_shown_work amod_approaches_unsupervised amod_approaches_supervised conj_and_supervised_unsupervised preconj_supervised_both dep_Turney_2002 dep_Chaovalit_Turney appos_Chaovalit_2005 appos_work_Chaovalit amod_work_related
N09-1001	P02-1053	o	2 Related Work There has been a large and diverse body of research in opinion mining with most research at the text -LRB- Pang et al. 2002 Pang and Lee 2004 Popescu and Etzioni 2005 Ounis et al. 2006 -RRB- sentence -LRB- Kim and Hovy 2005 Kudo and Matsumoto 2004 Riloff et al. 2003 Yu and Hatzivassiloglou 2003 -RRB- or word -LRB- Hatzivassiloglou and McKeown 1997 Turney and Littman 2003 Kim and Hovy 2004 Takamura et al. 2005 Andreevskaia and Bergler 2006 Kaji and Kitsuregawa 2007 -RRB- level	dep_level_Kitsuregawa dep_level_Kaji dep_level_2006 dep_level_Bergler dep_level_Andreevskaia dep_Andreevskaia_2007 conj_and_Andreevskaia_Kitsuregawa conj_and_Andreevskaia_Kaji conj_and_Andreevskaia_2006 conj_and_Andreevskaia_Bergler num_Takamura_2005 nn_Takamura_al. nn_Takamura_et num_Kim_2004 conj_and_Kim_Hovy dep_Turney_level conj_and_Turney_Takamura conj_and_Turney_Hovy conj_and_Turney_Kim conj_and_Turney_2003 conj_and_Turney_Littman dep_Hatzivassiloglou_Takamura dep_Hatzivassiloglou_Kim dep_Hatzivassiloglou_2003 dep_Hatzivassiloglou_Littman dep_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown dep_word_1997 dep_word_McKeown dep_word_Hatzivassiloglou dep_Yu_2003 conj_and_Yu_Hatzivassiloglou num_Riloff_2003 nn_Riloff_al. nn_Riloff_et num_Kudo_2004 conj_and_Kudo_Matsumoto dep_Kim_Hatzivassiloglou dep_Kim_Yu conj_and_Kim_Riloff conj_and_Kim_Matsumoto conj_and_Kim_Kudo conj_and_Kim_2005 conj_and_Kim_Hovy dep_sentence_Riloff dep_sentence_Kudo dep_sentence_2005 dep_sentence_Hovy dep_sentence_Kim num_Ounis_2006 nn_Ounis_al. nn_Ounis_et conj_or_Popescu_word conj_and_Popescu_sentence conj_and_Popescu_Ounis conj_and_Popescu_2005 conj_and_Popescu_Etzioni dep_Pang_word dep_Pang_sentence dep_Pang_Ounis dep_Pang_2005 dep_Pang_Etzioni dep_Pang_Popescu num_Pang_2004 conj_and_Pang_Lee dep_Pang_Lee dep_Pang_Pang appos_Pang_2002 dep_Pang_al. nn_Pang_et det_text_the amod_research_most nn_mining_opinion dep_body_Pang prep_at_body_text prep_with_body_research prep_in_body_mining prep_of_body_research amod_body_diverse amod_body_large det_body_a cop_body_been aux_body_has expl_body_There dep_body_Work conj_and_large_diverse amod_Work_Related num_Work_2
N09-1002	P02-1053	o	3 Related Work Many methods have been developed for automatically identifying subjective -LRB- opinion sentiment attitude affect-bearing etc. -RRB- words e.g. -LRB- Turney 2002 Riloff and Wiebe 2003 Kim and Hovy 2004 Taboada et al. 2006 Takamura et al. 2006 -RRB-	num_Takamura_2006 nn_Takamura_al. nn_Takamura_et num_Taboada_2006 nn_Taboada_al. nn_Taboada_et num_Kim_2004 conj_and_Kim_Hovy dep_Riloff_Takamura conj_and_Riloff_Taboada conj_and_Riloff_Hovy conj_and_Riloff_Kim conj_and_Riloff_2003 conj_and_Riloff_Wiebe dep_Turney_Taboada dep_Turney_Kim dep_Turney_2003 dep_Turney_Wiebe dep_Turney_Riloff appos_Turney_2002 amod_words_subjective dep_opinion_etc. conj_opinion_affect-bearing conj_opinion_attitude conj_opinion_sentiment dep_subjective_opinion appos_identifying_Turney dep_identifying_e.g. dobj_identifying_words advmod_identifying_automatically prepc_for_developed_identifying auxpass_developed_been aux_developed_have nsubjpass_developed_methods amod_methods_Many rcmod_Work_developed amod_Work_Related num_Work_3
N09-1055	P02-1053	o	The research of opinion mining began in 1997 the early research results mainly focused on the polarity of opinion words -LRB- Hatzivassiloglou et al. 1997 -RRB- and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text -LRB- Turney et al. 2003 Pang et al. 2002 Zagibalov et al. 2008 ;-RRB-	num_;-RRB-_2008 appos_Zagibalov_;-RRB- dep_Zagibalov_al. nn_Zagibalov_et num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Zagibalov dep_Turney_Pang amod_Turney_2003 dep_Turney_al. nn_Turney_et num_text_one nn_words_opinion amod_words_negative amod_words_positive conj_or_positive_negative prep_of_number_words det_number_the conj_or_positive_negative preconj_positive_either prep_in_classification_text prep_on_classification_number prep_of_classification_negative prep_of_classification_positive det_classification_a nn_mining_opinion amod_mining_text-level det_mining_the dobj_treated_mining nsubj_treated_research amod_Hatzivassiloglou_1997 dep_Hatzivassiloglou_al. nn_Hatzivassiloglou_et nn_words_opinion prep_of_polarity_words det_polarity_the prep_on_focused_polarity advmod_focused_mainly dep_results_Turney prep_as_results_classification conj_and_results_treated dep_results_Hatzivassiloglou vmod_results_focused nsubj_results_research ccomp_results_began amod_research_early det_research_the prep_in_began_1997 nsubj_began_research nn_mining_opinion prep_of_research_mining det_research_The
N09-1056	P02-1053	o	Examples of such early work include -LRB- Turney 2002 Pang et al. 2002 Dave et al. 2003 Hu and Liu 2004 Popescu and Etzioni 2005 -RRB-	dep_Popescu_2005 conj_and_Popescu_Etzioni num_Hu_2004 conj_and_Hu_Liu num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Pang_Etzioni dep_Pang_Popescu conj_Pang_Liu conj_Pang_Hu conj_Pang_Dave num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang dep_Turney_2002 dep_include_Turney nsubj_include_Examples amod_work_early amod_work_such prep_of_Examples_work
N09-2046	P02-1053	o	1 Introduction In the community of sentiment analysis -LRB- Turney 2002 Pang et al. 2002 Tang et al. 2009 -RRB- transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work because sentiment expression often behaves with strong domain-specific nature	amod_nature_domain-specific amod_nature_strong prep_with_behaves_nature advmod_behaves_often nsubj_behaves_expression mark_behaves_because nn_expression_sentiment amod_work_trivial det_work_a prep_from_far_work advmod_far_still cop_far_is nsubj_far_classifier nn_domain_target det_domain_another nn_domain_source num_domain_one prep_to_classifier_domain prep_from_classifier_domain rcmod_sentiment_far det_sentiment_a dobj_transferring_sentiment num_Tang_2009 nn_Tang_al. nn_Tang_et num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Tang conj_Turney_Pang num_Turney_2002 nn_analysis_sentiment appos_community_Turney prep_of_community_analysis det_community_the dep_Introduction_behaves vmod_Introduction_transferring prep_in_Introduction_community num_Introduction_1
N09-3013	P02-1053	o	In general previous work in opinion mining includes document level sentiment classification using supervised -LRB- Chaovalit and Zhou 2005 -RRB- and unsupervised methods -LRB- Turney 2002 -RRB- machine learning techniques and sentiment classification considering rating scales -LRB- Pang Lee and Vaithyanathan 2002 -RRB- and scoring of features -LRB- Dave Lawrence and Pennock 2003 -RRB-	dep_Dave_2003 conj_and_Dave_Pennock conj_and_Dave_Lawrence appos_features_Pennock appos_features_Lawrence appos_features_Dave prep_of_scoring_features dep_Pang_2002 conj_and_Pang_Vaithyanathan conj_and_Pang_Lee appos_scales_Vaithyanathan appos_scales_Lee appos_scales_Pang nn_scales_rating dobj_considering_scales nn_classification_sentiment conj_and_techniques_scoring vmod_techniques_considering conj_and_techniques_classification nn_techniques_learning nn_techniques_machine amod_Turney_2002 dep_methods_Turney amod_methods_unsupervised dep_Chaovalit_2005 conj_and_Chaovalit_Zhou dep_supervised_scoring dep_supervised_classification dep_supervised_techniques conj_and_supervised_methods dep_supervised_Zhou dep_supervised_Chaovalit dobj_using_methods dobj_using_supervised vmod_classification_using nn_classification_sentiment nn_classification_level nn_classification_document dobj_includes_classification nsubj_includes_work prep_in_includes_general nn_mining_opinion prep_in_work_mining amod_work_previous
P04-1034	P02-1053	o	A contrasting approach -LRB- Turney 2002 -RRB- relies only upon documents whose labels are unknown	cop_unknown_are nsubj_unknown_labels poss_labels_whose rcmod_documents_unknown prep_upon_relies_documents advmod_relies_only nsubj_relies_approach amod_Turney_2002 dep_approach_Turney amod_approach_contrasting det_approach_A
P04-1034	P02-1053	o	Accuracy on sentiment classification in other domains exceeds 80 % -LRB- Turney 2002 -RRB-	dep_Turney_2002 appos_%_Turney num_%_80 dobj_exceeds_% nsubj_exceeds_Accuracy amod_domains_other prep_in_classification_domains nn_classification_sentiment prep_on_Accuracy_classification
P04-1034	P02-1053	o	In Turney -LRB- 2002 -RRB- features are selected according to part-of-speech labels	amod_labels_part-of-speech pobj_selected_labels prepc_according_to_selected_to auxpass_selected_are nsubjpass_selected_features prep_in_selected_Turney appos_Turney_2002 rcmod_``_selected
P04-1035	P02-1053	o	Second movie reviews are apparently harder to classify than reviews of other products -LRB- Turney 2002 Dave Lawrence and Pennock 2003 -RRB-	amod_Pennock_2003 conj_and_Dave_Pennock conj_and_Dave_Lawrence dep_Turney_Pennock dep_Turney_Lawrence dep_Turney_Dave appos_Turney_2002 dep_products_Turney amod_products_other prep_of_reviews_products prep_than_classify_reviews aux_classify_to xcomp_harder_classify advmod_harder_apparently cop_harder_are nsubj_harder_reviews advmod_harder_Second nn_reviews_movie
P04-3025	P02-1053	o	2 Motivation In the past work has been done in the area of characterizing words and phrases according to their emotive tone -LRB- Turney and Littman 2003 Turney 2002 Kamps et al. 2002 Hatzivassiloglou and Wiebe 2000 Hatzivassiloglou and McKeown 2002 Wiebe 2000 -RRB- but in many domains of text the values of individual phrases may bear little relation to the overall sentiment expressed by the text	det_text_the agent_expressed_text vmod_sentiment_expressed amod_sentiment_overall det_sentiment_the prep_to_relation_sentiment amod_relation_little dobj_bear_relation aux_bear_may nsubj_bear_values amod_phrases_individual prep_of_values_phrases det_values_the prep_of_domains_text amod_domains_many pobj_in_domains dep_Wiebe_2000 conj_but_Hatzivassiloglou_in conj_and_Hatzivassiloglou_Wiebe conj_and_Hatzivassiloglou_2002 conj_and_Hatzivassiloglou_McKeown conj_and_Hatzivassiloglou_Hatzivassiloglou conj_and_Hatzivassiloglou_2000 conj_and_Hatzivassiloglou_Wiebe num_Kamps_2002 nn_Kamps_al. nn_Kamps_et num_Turney_2002 rcmod_Turney_bear dep_Turney_in dep_Turney_Wiebe dep_Turney_2002 dep_Turney_McKeown dep_Turney_Hatzivassiloglou dep_Turney_2000 dep_Turney_Wiebe dep_Turney_Hatzivassiloglou conj_and_Turney_Kamps conj_and_Turney_Turney conj_and_Turney_2003 conj_and_Turney_Littman dep_tone_Kamps dep_tone_Turney dep_tone_2003 dep_tone_Littman dep_tone_Turney amod_tone_emotive poss_tone_their conj_and_words_phrases dobj_characterizing_phrases dobj_characterizing_words prepc_of_area_characterizing det_area_the pobj_done_tone prepc_according_to_done_to prep_in_done_area auxpass_done_been aux_done_has nsubjpass_done_work dep_done_Motivation det_past_the prep_in_Motivation_past num_Motivation_2
P04-3025	P02-1053	o	In the present work the approach taken by Turney -LRB- 2002 -RRB- is used to derive such values for selected phrases in the text	det_text_the prep_in_phrases_text amod_phrases_selected prep_for_values_phrases amod_values_such dobj_derive_values aux_derive_to xcomp_used_derive auxpass_used_is nsubjpass_used_approach prep_in_used_work appos_Turney_2002 agent_taken_Turney vmod_approach_taken det_approach_the amod_work_present det_work_the
P05-1015	P02-1053	o	Also even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classi cation techniques -LRB- Pang Lee and Vaithyanathan 2002 Turney 2002 -RRB-	amod_Turney_2002 num_Vaithyanathan_2002 dep_Pang_Turney conj_and_Pang_Vaithyanathan conj_and_Pang_Lee dep_techniques_Vaithyanathan dep_techniques_Lee dep_techniques_Pang nn_techniques_cation nn_techniques_classi amod_techniques_automated amod_techniques_many prep_for_challenging_techniques advmod_challenging_quite xcomp_proven_challenging aux_proven_has nsubj_proven_version advmod_proven_Also nn_reviews_movie prep_for_problem_reviews nn_problem_rating-inference det_problem_the prep_of_version_problem amod_version_two-category det_version_the advmod_version_even
P05-1015	P02-1053	o	Most prior work on the speci c problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative -LRB- Turney 2002 Pang Lee and Vaithyanathan 2002 Dave Lawrence and Pennock 2003 Yu and Hatzivassiloglou 2003 -RRB-	amod_Yu_2003 conj_and_Yu_Hatzivassiloglou num_Pennock_2003 conj_and_Dave_Pennock conj_and_Dave_Lawrence appos_Vaithyanathan_2002 conj_and_Pang_Vaithyanathan conj_and_Pang_Lee dep_Turney_Hatzivassiloglou dep_Turney_Yu conj_Turney_Pennock conj_Turney_Lawrence conj_Turney_Dave conj_Turney_Vaithyanathan conj_Turney_Lee conj_Turney_Pang conj_Turney_2002 dep_positive_Turney conj_vs._positive_negative prep_of_distinction_negative prep_of_distinction_positive amod_distinction_binary det_distinction_the prep_on_focused_distinction aux_focused_has nsubj_focused_work amod_text_opinionated advmod_text_expressly amod_text_categorizing prep_of_problem_text nn_problem_c nn_problem_speci det_problem_the prep_on_work_problem amod_work_prior amod_work_Most
P05-1015	P02-1053	o	-LRB- Termbased versions of this premise have motivated much sentiment-analysis work for over a decade -LRB- Das and Chen 2001 Tong 2001 Turney 2002 -RRB- -RRB-	dep_Turney_2002 num_Tong_2001 dep_Das_Turney conj_and_Das_Tong conj_and_Das_2001 conj_and_Das_Chen det_decade_a pobj_over_decade pcomp_for_over nn_work_sentiment-analysis amod_work_much dep_motivated_Tong dep_motivated_2001 dep_motivated_Chen dep_motivated_Das prep_motivated_for dobj_motivated_work aux_motivated_have nsubj_motivated_versions det_premise_this prep_of_versions_premise amod_versions_Termbased
P05-2008	P02-1053	o	Turney -LRB- 2002 -RRB- noted that the unigram unpredictable might have a positive sentiment in a movie review -LRB- e.g. unpredictable plot -RRB- but could be negative in the review of an automobile -LRB- e.g. unpredictable steering -RRB-	dep_unpredictable_steering dep_e.g._unpredictable dep_automobile_e.g. det_automobile_an prep_of_review_automobile det_review_the prep_in_negative_review cop_negative_be aux_negative_could nsubj_negative_Turney amod_plot_unpredictable advmod_plot_e.g. appos_review_plot nn_review_movie det_review_a amod_sentiment_positive det_sentiment_a prep_in_have_review dobj_have_sentiment aux_have_might nsubj_have_unpredictable mark_have_that amod_unpredictable_unigram det_unpredictable_the conj_but_noted_negative ccomp_noted_have nsubj_noted_Turney appos_Turney_2002
P06-1034	P02-1053	o	-LRB- Turney 2002 -RRB- -RRB-	dep_Turney_2002 dep_''_Turney
P06-1134	P02-1053	o	The first is identifying words and phrases that are associated with subjectivity for example that think is associated with private states and that beautiful is associated with positive sentiments -LRB- e.g. -LRB- Hatzivassiloglou and McKeown 1997 Wiebe 2000 Kamps and Marx 2002 Turney 2002 Esuli and Sebastiani 2005 -RRB- -RRB-	appos_Esuli_2005 conj_and_Esuli_Sebastiani num_Turney_2002 dep_Kamps_Sebastiani dep_Kamps_Esuli conj_and_Kamps_Turney conj_and_Kamps_2002 conj_and_Kamps_Marx num_Wiebe_2000 dep_Hatzivassiloglou_Turney dep_Hatzivassiloglou_2002 dep_Hatzivassiloglou_Marx dep_Hatzivassiloglou_Kamps conj_and_Hatzivassiloglou_Wiebe conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown dep_e.g._Wiebe dep_e.g._1997 dep_e.g._McKeown dep_e.g._Hatzivassiloglou dep_sentiments_e.g. amod_sentiments_positive prep_with_associated_sentiments auxpass_associated_is nsubjpass_associated_beautiful mark_associated_that amod_states_private conj_and_associated_associated prep_with_associated_states auxpass_associated_is ccomp_think_associated ccomp_think_associated nsubj_think_that rcmod_,_think pobj_for_example dep_,_for prep_with_associated_subjectivity auxpass_associated_are nsubjpass_associated_that rcmod_words_associated conj_and_words_phrases dobj_identifying_phrases dobj_identifying_words aux_identifying_is nsubj_identifying_first det_first_The advcl_``_identifying
P06-1134	P02-1053	o	The third exploits automatic subjectivity analysis in applications such as review classification -LRB- e.g. -LRB- Turney 2002 Pang and Lee 2004 -RRB- -RRB- mining texts for product reviews -LRB- e.g. -LRB- Yi et al. 2003 Hu and Liu 2004 Popescu and Etzioni 2005 -RRB- -RRB- summarization -LRB- e.g. -LRB- Kim and Hovy 2004 -RRB- -RRB- information extraction -LRB- e.g. -LRB- Riloff et al. 2005 -RRB- -RRB- 1Note that sentiment the focus of much recent work in the area is a type of subjectivity specifically involving positive or negative opinion emotion or evaluation	conj_or_opinion_evaluation conj_or_opinion_emotion amod_opinion_negative amod_opinion_positive conj_or_positive_negative dobj_involving_evaluation dobj_involving_emotion dobj_involving_opinion advmod_involving_specifically vmod_type_involving prep_of_type_subjectivity det_type_a cop_type_is nsubj_type_sentiment dobj_type_that det_area_the prep_in_work_area amod_work_recent amod_work_much prep_of_focus_work det_focus_the appos_sentiment_focus rcmod_1Note_type amod_Riloff_2005 dep_Riloff_al. nn_Riloff_et dep_e.g._Riloff dep_extraction_e.g. nn_extraction_information dep_Kim_2004 conj_and_Kim_Hovy appos_e.g._Hovy appos_e.g._Kim dep_summarization_e.g. appos_Popescu_2005 conj_and_Popescu_Etzioni num_Hu_2004 conj_and_Hu_Liu nn_al._et appos_Yi_1Note conj_Yi_extraction amod_Yi_summarization dep_Yi_Etzioni dep_Yi_Popescu dep_Yi_Liu dep_Yi_Hu dep_Yi_2003 dep_Yi_al. dep_,_Yi dep_-LRB-_e.g. nn_reviews_product prep_for_texts_reviews nn_texts_mining appos_Pang_2004 conj_and_Pang_Lee dep_Turney_Lee dep_Turney_Pang amod_Turney_2002 dep_e.g._texts appos_e.g._Turney dep_classification_e.g. nn_classification_review prep_such_as_applications_classification prep_in_analysis_applications nn_analysis_subjectivity amod_analysis_automatic dep_exploits_analysis amod_exploits_third det_exploits_The ccomp_``_exploits
P06-2059	P02-1053	p	Turney also reported good result without domain customization -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_customization_Turney nn_customization_domain prep_without_result_customization amod_result_good dobj_reported_result advmod_reported_also nsubj_reported_Turney ccomp_``_reported
P06-2059	P02-1053	o	6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification -LRB- Turney 2002 -RRB- and similar method is utilized by many other researchers -LRB- Yu and Hatzivassiloglou 2003 -RRB-	amod_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_researchers_Hatzivassiloglou dep_researchers_Yu amod_researchers_other amod_researchers_many agent_utilized_researchers auxpass_utilized_is nsubjpass_utilized_method nsubjpass_utilized_classification amod_method_similar dep_Turney_2002 appos_classification_Turney nn_classification_sentiment prep_for_method_classification amod_method_unsupervised det_method_the dobj_proposed_method nsubj_proposed_Turney conj_and_classification_method rcmod_classification_proposed nn_classification_sentiment amod_classification_Unsupervised num_classification_6.3
P06-2063	P02-1053	o	Identifying subjectivity helps separate opinions from fact which may be useful in question answering summarization etc. Semantic orientation classification is a task of determining positive or negative sentiment of words -LRB- Hatzivassiloglou and McKeown 1997 Turney 2002 Esuli and Sebastiani 2005 -RRB-	amod_Esuli_2005 conj_and_Esuli_Sebastiani num_Turney_2002 dep_Hatzivassiloglou_Sebastiani dep_Hatzivassiloglou_Esuli conj_and_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown appos_words_Turney appos_words_1997 appos_words_McKeown appos_words_Hatzivassiloglou prep_of_sentiment_words amod_sentiment_negative amod_sentiment_positive conj_or_positive_negative dobj_determining_sentiment prepc_of_task_determining det_task_a cop_task_is nsubj_task_classification dep_task_summarization nn_classification_orientation amod_classification_Semantic dep_summarization_etc. conj_answering_task nn_answering_question prep_in_useful_answering cop_useful_be aux_useful_may nsubj_useful_which amod_opinions_separate dep_helps_useful prep_from_helps_fact dobj_helps_opinions nsubj_helps_subjectivity amod_subjectivity_Identifying
P06-2063	P02-1053	o	Document level sentiment classification is mostly applied to reviews where systems assign a positive or negative sentiment for a whole review document -LRB- Pang et al. 2002 Turney 2002 -RRB-	amod_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_document_review amod_document_whole det_document_a amod_sentiment_negative amod_sentiment_positive det_sentiment_a conj_or_positive_negative dep_assign_Pang prep_for_assign_document dobj_assign_sentiment nsubj_assign_systems advmod_assign_where rcmod_reviews_assign prep_to_applied_reviews advmod_applied_mostly auxpass_applied_is nsubjpass_applied_classification nn_classification_sentiment nn_classification_level nn_classification_Document
P06-2079	P02-1053	o	-LRB- 2002 -RRB- Turney -LRB- 2002 -RRB- -RRB- a sentence -LRB- e.g. Liu et al.	nn_al._et nn_al._Liu dep_e.g._al. dep_sentence_e.g. det_sentence_a appos_Turney_sentence dep_Turney_2002 dep_2002_Turney dep_''_2002
P06-2079	P02-1053	o	Much work has been performed on learning to identify and classify polarity terms -LRB- i.e. terms expressing a positive sentiment -LRB- e.g. happy -RRB- or a negative sentiment -LRB- e.g. terrible -RRB- -RRB- and exploiting them to do polarity classification -LRB- e.g. Hatzivassiloglou and McKeown -LRB- 1997 -RRB- Turney -LRB- 2002 -RRB- Kim and Hovy -LRB- 2004 -RRB- Whitelaw et al.	nn_al._et nn_al._Whitelaw appos_Hovy_2004 appos_Turney_2002 appos_McKeown_1997 dep_Hatzivassiloglou_al. conj_and_Hatzivassiloglou_Hovy conj_and_Hatzivassiloglou_Kim conj_and_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_McKeown dep_e.g._Hovy dep_e.g._Kim dep_e.g._Turney dep_e.g._McKeown dep_e.g._Hatzivassiloglou ccomp_-LRB-_e.g. nn_classification_polarity dobj_do_classification aux_do_to xcomp_exploiting_do dobj_exploiting_them dep_e.g._terrible dep_sentiment_e.g. amod_sentiment_negative det_sentiment_a dep_e.g._happy conj_or_sentiment_sentiment dep_sentiment_e.g. amod_sentiment_positive det_sentiment_a dobj_expressing_sentiment dobj_expressing_sentiment conj_and_terms_exploiting vmod_terms_expressing pobj_i.e._exploiting pobj_i.e._terms ccomp_-LRB-_i.e. nn_terms_polarity dobj_identify_terms conj_and_identify_classify aux_identify_to xcomp_learning_classify xcomp_learning_identify prepc_on_performed_learning auxpass_performed_been aux_performed_has nsubjpass_performed_work amod_work_Much
P06-2079	P02-1053	o	For instance instead of representing the polarity of a term using a binary value Mullen and Collier -LRB- 2004 -RRB- use Turneys -LRB- 2002 -RRB- method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration	det_document_the prep_under_selected_consideration prep_from_selected_document vmod_terms_selected prep_of_values_terms nn_values_polarity det_values_the prep_of_measures_values amod_measures_aggregate cop_measures_are nsubj_measures_that rcmod_features_measures amod_features_numerical prep_of_variety_features det_variety_a dobj_introduce_variety nn_polarity_term conj_and_represent_introduce dobj_represent_polarity aux_represent_to amod_value_real det_value_a vmod_assign_introduce vmod_assign_represent dobj_assign_value aux_assign_to nn_method_Turneys appos_Turneys_2002 vmod_use_assign dobj_use_method nsubj_use_Collier appos_Collier_2004 conj_and_value_use conj_and_value_Mullen amod_value_binary det_value_a dobj_using_use dobj_using_Mullen dobj_using_value det_term_a prep_of_polarity_term det_polarity_the vmod_representing_using dobj_representing_polarity pcomp_of_representing advmod_of_instead ccomp_,_of pobj_For_instance dep_``_For
P06-2081	P02-1053	o	For instance both Pang and Lee -LRB- 2002 -RRB- and Turney -LRB- 2002 -RRB- consider the thumbs up/thumbs down decision is a film review positive or negative ?	conj_or_positive_negative amod_review_negative amod_review_positive nn_review_film det_review_a nsubj_is_review prep_down_up/thumbs_decision nsubj_up/thumbs_thumbs det_thumbs_the dep_consider_is xcomp_consider_up/thumbs nsubj_consider_Turney nsubj_consider_Lee nsubj_consider_Pang prep_for_consider_instance appos_Turney_2002 appos_Lee_2002 conj_and_Pang_Turney conj_and_Pang_Lee preconj_Pang_both
P06-2081	P02-1053	o	Work focusses on analysing subjective features of text or speech such as sentiment opinion emotion or point of view -LRB- Pang et al. 2002 Turney 2002 Dave et al. 2003 Liu et al. 2003 Pang and Lee 2005 Shanahan et al. 2005 -RRB-	num_Shanahan_2005 nn_Shanahan_al. nn_Shanahan_et num_Liu_2003 nn_Liu_al. nn_Liu_et num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Turney_Shanahan num_Turney_2005 conj_and_Turney_Lee conj_and_Turney_Pang conj_and_Turney_Liu conj_and_Turney_Dave num_Turney_2002 dep_Pang_Lee dep_Pang_Pang dep_Pang_Liu dep_Pang_Dave dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et prep_of_point_view conj_or_sentiment_point conj_or_sentiment_emotion conj_or_sentiment_opinion conj_or_text_speech prep_such_as_features_point prep_such_as_features_emotion prep_such_as_features_opinion prep_such_as_features_sentiment prep_of_features_speech prep_of_features_text amod_features_subjective dobj_analysing_features dep_focusses_Pang prepc_on_focusses_analysing nn_focusses_Work ccomp_``_focusses
P07-1053	P02-1053	o	We follow the approach by Turney -LRB- 2002 -RRB- who note that the semantic orientation of an adjective depends on the noun that it modifies and suggest using adjective-noun or adverb-verb pairs to extract semantic orientation	amod_orientation_semantic dobj_extract_orientation aux_extract_to amod_pairs_adverb-verb amod_pairs_adjective-noun conj_or_adjective-noun_adverb-verb vmod_using_extract dobj_using_pairs xcomp_suggest_using nsubj_suggest_it conj_and_modifies_suggest nsubj_modifies_it mark_modifies_that dep_noun_suggest dep_noun_modifies det_noun_the prep_on_depends_noun nsubj_depends_orientation mark_depends_that det_adjective_an prep_of_orientation_adjective amod_orientation_semantic det_orientation_the ccomp_note_depends nsubj_note_who appos_Turney_2002 rcmod_approach_note prep_by_approach_Turney det_approach_the dobj_follow_approach nsubj_follow_We
P07-1053	P02-1053	o	However we do not rely on linguistic resources -LRB- Kamps and Marx 2002 -RRB- or on search engines -LRB- Turney and Littman 2003 -RRB- to determine the semantic orientation but rather rely on econometrics for this task	det_task_this prep_for_rely_task prep_on_rely_econometrics advmod_rely_rather nsubj_rely_we amod_orientation_semantic det_orientation_the dobj_determine_orientation aux_determine_to dep_Turney_2003 conj_and_Turney_Littman dep_engines_Littman dep_engines_Turney nn_engines_search dep_Kamps_2002 conj_and_Kamps_Marx conj_or_resources_engines appos_resources_Marx appos_resources_Kamps amod_resources_linguistic conj_but_rely_rely xcomp_rely_determine prep_on_rely_engines prep_on_rely_resources neg_rely_not aux_rely_do nsubj_rely_we advmod_rely_However
P07-1053	P02-1053	o	To evaluate the polarity and strength of opinions most of the existing approaches rely either on training from human-annotated data -LRB- Hatzivassiloglou and McKeown 1997 -RRB- or use linguistic resources -LRB- Hu and Liu 2004 Kim and Hovy 2004 -RRB- like WordNet or rely on co-occurrence statistics -LRB- Turney 2002 -RRB- between words that are unambiguously positive -LRB- e.g. excellent -RRB- and unambiguously negative -LRB- e.g. horrible -RRB-	dep_e.g._horrible dep_negative_e.g. advmod_negative_unambiguously dep_e.g._excellent dep_positive_e.g. advmod_positive_unambiguously cop_positive_are nsubj_positive_that conj_and_words_negative rcmod_words_positive dep_Turney_2002 dep_statistics_Turney nn_statistics_co-occurrence prep_between_rely_negative prep_between_rely_words prep_on_rely_statistics conj_or_WordNet_rely dep_Kim_2004 conj_and_Kim_Hovy prep_like_Hu_rely prep_like_Hu_WordNet dep_Hu_Hovy dep_Hu_Kim conj_and_Hu_2004 conj_and_Hu_Liu dep_resources_2004 dep_resources_Liu dep_resources_Hu amod_resources_linguistic dobj_use_resources nsubj_use_most dep_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown appos_data_McKeown appos_data_Hatzivassiloglou amod_data_human-annotated prep_from_training_data conj_or_rely_use prep_on_rely_training advmod_rely_either nsubj_rely_most advcl_rely_evaluate amod_approaches_existing det_approaches_the prep_of_most_approaches prep_of_polarity_opinions conj_and_polarity_strength det_polarity_the dobj_evaluate_strength dobj_evaluate_polarity aux_evaluate_To
P07-1055	P02-1053	o	Previous workonsentimentanalysishascoveredawiderange of tasks including polarity classification -LRB- Pang et al. 2002 Turney 2002 -RRB- opinion extraction -LRB- Pang and Lee 2004 -RRB- and opinion source assignment -LRB- Choi et al. 2005 Choi et al. 2006 -RRB-	num_Choi_2006 nn_Choi_al. nn_Choi_et dep_Choi_Choi appos_Choi_2005 dep_Choi_al. nn_Choi_et nn_assignment_source nn_assignment_opinion dep_Pang_2004 conj_and_Pang_Lee appos_extraction_Lee appos_extraction_Pang nn_extraction_opinion dep_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_classification_assignment conj_and_classification_extraction appos_classification_Pang nn_classification_polarity prep_including_tasks_assignment prep_including_tasks_extraction prep_including_tasks_classification dep_workonsentimentanalysishascoveredawiderange_Choi prep_of_workonsentimentanalysishascoveredawiderange_tasks amod_workonsentimentanalysishascoveredawiderange_Previous ccomp_``_workonsentimentanalysishascoveredawiderange
P07-1055	P02-1053	o	Furthermore these systems have tackled the problem at different levels of granularity from the document level -LRB- Pang et al. 2002 -RRB- sentence level -LRB- Pang and Lee 2004 Mao and Lebanon 2006 -RRB- phrase level -LRB- Turney 2002 Choi et al. 2005 -RRB- as well as the speaker level in debates -LRB- Thomas et al. 2006 -RRB-	amod_Thomas_2006 dep_Thomas_al. nn_Thomas_et prep_in_level_debates nn_level_speaker det_level_the num_Choi_2005 nn_Choi_al. nn_Choi_et dep_Turney_Choi appos_Turney_2002 appos_level_Turney nn_level_phrase num_Mao_2006 conj_and_Mao_Lebanon dep_Pang_Lebanon dep_Pang_Mao conj_and_Pang_2004 conj_and_Pang_Lee appos_level_2004 appos_level_Lee appos_level_Pang nn_level_sentence amod_Pang_2002 dep_Pang_al. nn_Pang_et nn_level_document det_level_the prep_of_levels_granularity amod_levels_different prep_at_problem_levels det_problem_the dep_tackled_Thomas conj_and_tackled_level conj_and_tackled_level dep_tackled_level dep_tackled_Pang prep_from_tackled_level dobj_tackled_problem aux_tackled_have nsubj_tackled_systems advmod_tackled_Furthermore det_systems_these ccomp_``_level ccomp_``_level ccomp_``_tackled
P07-1056	P02-1053	o	The work most similar in spirit to ours that of Turney -LRB- 2002 -RRB-	appos_Turney_2002 prep_of_that_Turney dep_similar_that prep_to_similar_ours prep_in_similar_spirit advmod_similar_most amod_work_similar det_work_The
P07-1056	P02-1053	n	While we do not have a direct comparison we note that Turney -LRB- 2002 -RRB- performs worse on movie reviews than on his other datasets the same type of data as the polarity dataset	nn_dataset_polarity det_dataset_the prep_as_type_dataset prep_of_type_data amod_type_same det_type_the amod_datasets_other poss_datasets_his pobj_on_datasets pcomp_than_on nn_reviews_movie dep_worse_type prep_worse_than prep_on_worse_reviews dobj_performs_worse nsubj_performs_Turney mark_performs_that appos_Turney_2002 ccomp_note_performs nsubj_note_we advcl_note_have amod_comparison_direct det_comparison_a dobj_have_comparison neg_have_not aux_have_do nsubj_have_we mark_have_While
P07-1056	P02-1053	o	1 Introduction Sentiment detection and classification has received considerable attention recently -LRB- Pang et al. 2002 Turney 2002 Goldberg and Zhu 2004 -RRB-	amod_Turney_2004 conj_and_Turney_Zhu conj_and_Turney_Goldberg conj_and_Turney_2002 dep_Pang_Zhu dep_Pang_Goldberg dep_Pang_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et amod_attention_considerable dep_received_Pang advmod_received_recently dobj_received_attention aux_received_has nsubj_received_classification nsubj_received_detection conj_and_detection_classification nn_detection_Sentiment nn_detection_Introduction num_detection_1 ccomp_``_received
P07-1123	P02-1053	o	2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications such as tracking sentiment timelines in online forums and news -LRB- Lloyd et al. 2005 Balog et al. 2006 -RRB- review classification -LRB- Turney 2002 Pang et al. 2002 -RRB- mining opinions from product reviews -LRB- Hu and Liu 2004 -RRB- automatic expressive text-to-speech synthesis -LRB- Alm et al. 2005 -RRB- text semantic analysis -LRB- Wiebe and Mihalcea 2006 Esuli and Sebastiani 2006 -RRB- and question answering -LRB- Yu and Hatzivassiloglou 2003 -RRB-	amod_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_answering_Hatzivassiloglou dep_answering_Yu dep_question_answering dep_Esuli_2006 conj_and_Esuli_Sebastiani dep_Wiebe_Sebastiani dep_Wiebe_Esuli conj_and_Wiebe_2006 conj_and_Wiebe_Mihalcea appos_analysis_2006 appos_analysis_Mihalcea appos_analysis_Wiebe amod_analysis_semantic nn_analysis_text amod_Alm_2005 dep_Alm_al. nn_Alm_et dep_synthesis_Alm amod_synthesis_text-to-speech amod_synthesis_expressive amod_synthesis_automatic dep_Hu_2004 conj_and_Hu_Liu dep_reviews_Liu dep_reviews_Hu nn_reviews_product prep_from_opinions_reviews nn_opinions_mining num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang appos_Turney_2002 dep_classification_Turney nn_classification_review num_Balog_2006 nn_Balog_al. nn_Balog_et dep_Lloyd_Balog amod_Lloyd_2005 dep_Lloyd_al. nn_Lloyd_et conj_and_forums_news amod_forums_online conj_and_timelines_question appos_timelines_analysis appos_timelines_synthesis appos_timelines_opinions appos_timelines_classification appos_timelines_Lloyd prep_in_timelines_news prep_in_timelines_forums nn_timelines_sentiment nn_timelines_tracking prep_such_as_applications_question prep_such_as_applications_timelines nn_applications_processing nn_applications_text prep_of_variety_applications amod_variety_wide det_variety_a prep_in_used_variety auxpass_used_been aux_used_have nsubjpass_used_methods nn_methods_analysis nn_methods_subjectivity nn_methods_Automatic nn_methods_Motivation num_methods_2
P07-1124	P02-1053	o	Others such as Turney -LRB- 2002 -RRB- Pang and Vaithyanathan -LRB- 2002 -RRB- have examined the positive or negative polarity rather than presence or absence of affective content in text	amod_content_affective prep_in_presence_text prep_of_presence_content conj_or_presence_absence conj_negcc_polarity_absence conj_negcc_polarity_presence amod_polarity_negative amod_polarity_positive det_polarity_the conj_or_positive_negative dobj_examined_presence dobj_examined_polarity aux_examined_have nsubj_examined_Others appos_Vaithyanathan_2002 conj_and_Turney_Vaithyanathan conj_and_Turney_Pang appos_Turney_2002 prep_such_as_Others_Vaithyanathan prep_such_as_Others_Pang prep_such_as_Others_Turney
P07-1124	P02-1053	o	Much of the work in sentiment analysis in the computational linguistics domain has focused either on short segments such as sentences -LRB- Wilson et al. 2005 -RRB- or on longer documents with an explicit polarity orientation like movie or product reviews -LRB- Turney 2002 -RRB-	dep_Turney_2002 dep_reviews_Turney nn_reviews_product nn_reviews_movie conj_or_movie_product prep_like_orientation_reviews nn_orientation_polarity amod_orientation_explicit det_orientation_an amod_documents_longer prep_with_on_orientation pobj_on_documents amod_Wilson_2005 dep_Wilson_al. nn_Wilson_et dep_segments_Wilson prep_such_as_segments_sentences amod_segments_short conj_or_focused_on prep_on_focused_segments advmod_focused_either aux_focused_has nsubj_focused_Much nn_domain_linguistics amod_domain_computational det_domain_the nn_analysis_sentiment det_work_the prep_in_Much_domain prep_in_Much_analysis prep_of_Much_work
P07-3007	P02-1053	o	-LRB- 2002 -RRB- Turney -LRB- 2002 -RRB- Kim and Hovy -LRB- 2004 -RRB- and others -RRB- however the research described in this paper uses the information retrieval -LRB- IR -RRB- paradigm which has also been used by some researchers	det_researchers_some agent_used_researchers auxpass_used_been advmod_used_also aux_used_has nsubjpass_used_which rcmod_paradigm_used dep_retrieval_paradigm appos_retrieval_IR nn_retrieval_information det_retrieval_the dobj_uses_retrieval nsubj_uses_research advmod_uses_however nsubj_uses_Turney dep_uses_2002 det_paper_this prep_in_described_paper vmod_research_described det_research_the appos_Hovy_2004 conj_and_Kim_others conj_and_Kim_Hovy appos_Turney_others appos_Turney_Hovy appos_Turney_Kim appos_Turney_2002
P08-1034	P02-1053	o	291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text -LRB- Aue and Gamon 2005 Pang et al. 2002 Pang and Lee 2004 Riloff et al. 2006 Turney 2002 Turney and Littman 2003 -RRB- or at the sentence levels -LRB- Gamon and Aue 2005 Hu and Liu 2004 Kim and Hovy 2005 Riloff et al. 2006 -RRB-	num_Riloff_2006 nn_Riloff_al. nn_Riloff_et num_Kim_2005 conj_and_Kim_Hovy num_Hu_2004 conj_and_Hu_Liu dep_Gamon_Riloff conj_and_Gamon_Hovy conj_and_Gamon_Kim conj_and_Gamon_Liu conj_and_Gamon_Hu conj_and_Gamon_2005 conj_and_Gamon_Aue nn_levels_sentence det_levels_the dep_Turney_2003 conj_and_Turney_Littman num_Turney_2002 num_Riloff_2006 nn_Riloff_al. nn_Riloff_et num_Pang_2004 conj_and_Pang_Lee num_Pang_2002 nn_Pang_al. nn_Pang_et conj_and_Aue_Littman conj_and_Aue_Turney conj_and_Aue_Turney conj_and_Aue_Riloff conj_and_Aue_Lee conj_and_Aue_Pang conj_and_Aue_Pang conj_and_Aue_2005 conj_and_Aue_Gamon conj_or_text_levels dep_text_Turney dep_text_Turney dep_text_Riloff dep_text_Pang dep_text_Pang dep_text_2005 dep_text_Gamon dep_text_Aue det_text_the dep_conducted_Kim dep_conducted_Hu dep_conducted_2005 dep_conducted_Aue dep_conducted_Gamon prep_at_conducted_levels prep_at_conducted_text advmod_conducted_usually auxpass_conducted_is nsubjpass_conducted_Level nn_annotation_sentiment nn_Research_Analysis prep_on_Level_annotation prep_of_Level_Research num_Level_3.1 num_Level_291
P08-1034	P02-1053	o	For example it has been observed that texts often contain multiple opinions on different topics -LRB- Turney 2002 Wiebe et al. 2001 -RRB- which makes assignment of the overall sentiment to the whole document problematic	amod_document_problematic amod_document_whole det_document_the amod_sentiment_overall det_sentiment_the prep_to_assignment_document prep_of_assignment_sentiment dobj_makes_assignment nsubj_makes_which num_Wiebe_2001 nn_Wiebe_al. nn_Wiebe_et dep_Turney_Wiebe appos_Turney_2002 rcmod_topics_makes appos_topics_Turney amod_topics_different prep_on_opinions_topics amod_opinions_multiple dobj_contain_opinions advmod_contain_often nsubj_contain_texts mark_contain_that ccomp_observed_contain auxpass_observed_been aux_observed_has nsubjpass_observed_it prep_for_observed_example
P08-1036	P02-1053	o	Sentiment classification is a well studied problem -LRB- Wiebe 2000 Pang et al. 2002 Turney 2002 -RRB- and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay -LRB- 2007 -RRB-	appos_Barzilay_2007 conj_and_Snyder_Barzilay pobj_in_Barzilay pobj_in_Snyder pcomp_as_in det_user_a prep_rated_as agent_rated_user auxpass_rated_be aux_rated_can nsubjpass_rated_that rcmod_object_rated det_object_an prep_of_properties_object dobj_denote_properties aux_denote_to nn_aspect_term det_aspect_the vmod_use_denote dobj_use_aspect ccomp_1We_use advmod_1We_explicitly nsubj_1We_in nsubj_1We_Pang nn_users_domains amod_users_many pobj_in_users dep_Turney_2002 conj_and_Pang_in dep_Pang_Turney num_Pang_2002 nn_Pang_al. nn_Pang_et parataxis_Wiebe_1We appos_Wiebe_2000 dep_problem_Wiebe amod_problem_studied det_problem_a cop_problem_is nsubj_problem_classification advmod_studied_well nn_classification_Sentiment
P09-1027	P02-1053	o	Turney -LRB- 2002 -RRB- predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs which is denoted as the semantic oriented method	amod_method_oriented amod_method_semantic det_method_the prep_as_denoted_method auxpass_denoted_is nsubjpass_denoted_which rcmod_adjectives_denoted conj_or_adjectives_adverbs dobj_contain_adverbs dobj_contain_adjectives nsubj_contain_that rcmod_review_contain det_review_the prep_in_phrases_review det_phrases_the prep_of_orientation_phrases amod_orientation_semantic amod_orientation_average det_orientation_the det_review_a prep_of_orientation_review nn_orientation_sentiment det_orientation_the prep_by_predicates_orientation dobj_predicates_orientation nsubj_predicates_Turney appos_Turney_2002
P09-1028	P02-1053	o	Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons -LRB- Das and Chen 2001 -RRB- to semi-automated approaches -LRB- Hu and Liu 2004 Zhuang et al. 2006 Kim and Hovy 2004 -RRB- and even an almost fully automated approach -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_approach_Turney amod_approach_automated det_approach_an advmod_approach_even advmod_automated_fully advmod_fully_almost dep_Kim_2004 conj_and_Kim_Hovy num_Zhuang_2006 nn_Zhuang_al. nn_Zhuang_et dep_Hu_Hovy dep_Hu_Kim conj_and_Hu_Zhuang num_Hu_2004 conj_and_Hu_Liu appos_approaches_Zhuang appos_approaches_Liu appos_approaches_Hu amod_approaches_semi-automated amod_Das_2001 conj_and_Das_Chen dep_lexicons_Chen dep_lexicons_Das amod_lexicons_domain-dependent prep_to_developing_approaches dobj_developing_lexicons prepc_of_approaches_developing amod_approaches_manual conj_and_ranged_approach prep_from_ranged_approaches aux_ranged_have nsubj_ranged_Methods prep_of_sentiment_words det_sentiment_the dobj_capturing_sentiment vmod_dictionaries_capturing prep_of_use_dictionaries conj_and_use_generation det_use_the prep_on_focussing_generation prep_on_focussing_use vmod_Methods_focussing ccomp_``_approach ccomp_``_ranged
P09-1079	P02-1053	p	Turneys -LRB- 2002 -RRB- work is perhaps one of the most notable examples of unsupervised polarity classification	nn_classification_polarity amod_classification_unsupervised prep_of_examples_classification amod_examples_notable det_examples_the advmod_notable_most prep_of_one_examples advmod_one_perhaps cop_one_is nsubj_one_work nn_work_Turneys appos_Turneys_2002
P09-2041	P02-1053	o	3 Method 3.1 Standard text classication approach We take our starting point from topic-based text classication -LRB- Dumais et al. 1998 Joachims 1998 -RRB- and sentiment classication -LRB- Turney 2002 Pang and Lee 2008 -RRB-	amod_Pang_2008 conj_and_Pang_Lee dep_Turney_Lee dep_Turney_Pang appos_Turney_2002 appos_classication_Turney nn_classication_sentiment dep_Joachims_1998 dep_Dumais_Joachims dep_Dumais_1998 dep_Dumais_al. nn_Dumais_et conj_and_classication_classication appos_classication_Dumais nn_classication_text amod_classication_topic-based amod_point_starting poss_point_our prep_from_take_classication prep_from_take_classication dobj_take_point nsubj_take_We rcmod_approach_take nn_approach_classication nn_approach_text nn_approach_Standard num_approach_3.1 nn_approach_Method num_approach_3
P09-2043	P02-1053	o	1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews product reviews news and blog reviews -LRB- Pang et al. 2002 Turney 2002 -RRB-	amod_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et nn_reviews_blog dep_reviews_Pang conj_and_reviews_reviews conj_and_reviews_news nn_reviews_product nn_reviews_movie prep_such_as_domains_reviews amod_domains_several dep_conducted_reviews dep_conducted_news dep_conducted_reviews prep_in_conducted_domains advmod_conducted_widely auxpass_conducted_been aux_conducted_have nsubjpass_conducted_analysis nn_analysis_Sentiment nn_analysis_Introduction num_analysis_1 ccomp_``_conducted
W02-1011	P02-1053	o	Since adjectives have been a focus of previous work in sentiment detection -LRB- Hatzivassiloglou and Wiebe 2000 Turney 2002 -RRB- 13 we looked at the performance of using adjectives alone	advmod_adjectives_alone dobj_using_adjectives prepc_of_performance_using det_performance_the prep_at_looked_performance nsubj_looked_we dep_Turney_2002 rcmod_Hatzivassiloglou_looked dep_Hatzivassiloglou_13 dep_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_2000 conj_and_Hatzivassiloglou_Wiebe dep_detection_2000 dep_detection_Wiebe dep_detection_Hatzivassiloglou nn_detection_sentiment amod_work_previous prep_in_focus_detection prep_of_focus_work det_focus_a cop_focus_been aux_focus_have nsubj_focus_adjectives mark_focus_Since advcl_``_focus
W02-1011	P02-1053	o	In terms of relative performance Naive Bayes tends to do the worst and SVMs tend to do the best although the 12http / / www.english.bham.ac.uk/stafi/oliver/software/tagger/index.htm 13Turneys -LRB- 2002 -RRB- unsupervised algorithm uses bigrams containing an adjective or an adverb	det_adverb_an conj_or_adjective_adverb det_adjective_an dobj_containing_adverb dobj_containing_adjective vmod_bigrams_containing dobj_uses_bigrams vmod_algorithm_uses amod_algorithm_unsupervised num_algorithm_2002 dep_13Turneys_algorithm nn_13Turneys_www.english.bham.ac.uk/stafi/oliver/software/tagger/index.htm dep_12http_13Turneys det_12http_the det_best_the dobj_do_best aux_do_to prep_although_tend_12http xcomp_tend_do conj_and_worst_SVMs det_worst_the dobj_do_SVMs dobj_do_worst aux_do_to dep_tends_tend xcomp_tends_do nsubj_tends_Bayes prep_in_tends_terms amod_Bayes_Naive amod_performance_relative prep_of_terms_performance
W02-1011	P02-1053	o	-LRB- Turney -LRB- 2002 -RRB- makes a similar point noting that for reviews \ the whole is not necessarily the sum of the parts -RRB-	det_parts_the prep_of_sum_parts det_sum_the advmod_sum_necessarily neg_sum_not cop_sum_is nsubj_sum_whole mark_sum_\ prep_for_sum_reviews mark_sum_that det_whole_the ccomp_noting_sum amod_point_similar det_point_a vmod_makes_noting dobj_makes_point nsubj_makes_Turney appos_Turney_2002
W02-1011	P02-1053	o	Some of this work focuses on classifying the semantic orientation of individual words or phrases using linguistic heuristics or a pre-selected set of seed words -LRB- Hatzivassiloglou and McKeown 1997 Turney and Littman 2002 -RRB-	amod_Turney_2002 conj_and_Turney_Littman dep_Hatzivassiloglou_Littman dep_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown dep_words_1997 dep_words_McKeown dep_words_Hatzivassiloglou nn_words_seed prep_of_set_words amod_set_pre-selected det_set_a conj_or_heuristics_set amod_heuristics_linguistic dobj_using_set dobj_using_heuristics conj_or_words_phrases amod_words_individual prep_of_orientation_phrases prep_of_orientation_words amod_orientation_semantic det_orientation_the dobj_classifying_orientation xcomp_focuses_using prepc_on_focuses_classifying nsubj_focuses_Some det_work_this prep_of_Some_work ccomp_``_focuses
W02-1011	P02-1053	o	Turneys -LRB- 2002 -RRB- work on classiflcation of reviews is perhaps the closest to ours .2 He applied a speciflc unsupervised learning technique based on the mutual information between document phrases and the words \ excellent and \ poor where the mutual information is computed using statistics gathered by a search engine	nn_engine_search det_engine_a agent_gathered_engine vmod_statistics_gathered dobj_using_statistics xcomp_computed_using auxpass_computed_is nsubjpass_computed_information advmod_computed_where amod_information_mutual det_information_the num_poor_\ conj_and_excellent_poor dep_\_poor dep_\_excellent det_words_the conj_and_phrases_words nn_phrases_document prep_between_information_words prep_between_information_phrases amod_information_mutual det_information_the prep_on_based_information vmod_technique_based nn_technique_learning amod_technique_unsupervised nn_technique_speciflc det_technique_a advcl_applied_computed dep_applied_\ dobj_applied_technique nsubj_applied_He amod_.2_ours dep_closest_applied prep_to_closest_.2 det_closest_the advmod_closest_perhaps cop_closest_is nsubj_closest_work prep_of_classiflcation_reviews prep_on_work_classiflcation nn_work_Turneys appos_Turneys_2002
W02-1011	P02-1053	o	We also note that Turney -LRB- 2002 -RRB- found movie reviews to be the most 2Indeed although our choice of title was completely independent of his our selections were eerily similar	advmod_similar_eerily cop_similar_were nsubj_similar_selections dep_similar_his poss_selections_our prep_of_independent_similar advmod_independent_completely cop_independent_was nsubj_independent_choice mark_independent_although prep_of_choice_title poss_choice_our advcl_2Indeed_independent advmod_2Indeed_most det_2Indeed_the cop_2Indeed_be aux_2Indeed_to vmod_reviews_2Indeed nn_reviews_movie dobj_found_reviews nsubj_found_Turney mark_found_that appos_Turney_2002 ccomp_note_found advmod_note_also nsubj_note_We ccomp_``_note
W03-0404	P02-1053	o	Some work identifies inflammatory texts -LRB- e.g. -LRB- Spertus 1997 -RRB- -RRB- or classifies reviews as positive or negative -LRB- -LRB- Turney 2002 Pang et al. 2002 -RRB- -RRB-	num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang conj_Turney_2002 dep_positive_Turney conj_or_positive_negative prep_as_classifies_negative prep_as_classifies_positive dobj_classifies_reviews dep_Spertus_1997 appos_e.g._Spertus conj_or_texts_classifies dep_texts_e.g. amod_texts_inflammatory dobj_identifies_classifies dobj_identifies_texts nsubj_identifies_work det_work_Some ccomp_``_identifies
W03-0404	P02-1053	o	Researchers have focused on learning adjectives or adjectival phrases -LRB- Turney 2002 Hatzivassiloglou and McKeown 1997 Wiebe 2000 -RRB- and verbs -LRB- Wiebe et al. 2001 -RRB- but no previous work has focused on learning nouns	dobj_learning_nouns prepc_on_focused_learning aux_focused_has nsubj_focused_work amod_work_previous neg_work_no amod_Wiebe_2001 dep_Wiebe_al. nn_Wiebe_et dep_Wiebe_2000 dep_Hatzivassiloglou_Wiebe conj_and_Hatzivassiloglou_verbs conj_and_Hatzivassiloglou_Wiebe conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown conj_but_Turney_focused conj_but_Turney_verbs conj_but_Turney_Wiebe conj_but_Turney_1997 conj_but_Turney_McKeown conj_but_Turney_Hatzivassiloglou appos_Turney_2002 dep_phrases_focused dep_phrases_Hatzivassiloglou dep_phrases_Turney nn_phrases_adjectival nn_phrases_adjectives amod_phrases_learning conj_or_adjectives_adjectival prep_on_focused_phrases aux_focused_have nsubj_focused_Researchers
W03-0404	P02-1053	o	-LRB- Turney 2002 -RRB- used patterns representing part-of-speech sequences -LRB- Hatzivassiloglou and McKeown 1997 -RRB- recognized adjectival phrases and -LRB- Wiebe et al. 2001 -RRB- learned N-grams	dobj_learned_N-grams nsubj_learned_Wiebe amod_Wiebe_2001 dep_Wiebe_al. nn_Wiebe_et amod_phrases_adjectival conj_and_recognized_learned dobj_recognized_phrases dep_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown amod_sequences_part-of-speech dobj_representing_sequences vmod_patterns_representing dep_used_learned dep_used_recognized dep_used_McKeown dep_used_Hatzivassiloglou dobj_used_patterns dep_used_Turney dep_Turney_2002
W03-1014	P02-1053	o	Some existing resources contain lists of subjective words -LRB- e.g. Levins desire verbs -LRB- 1993 -RRB- -RRB- and some empirical methods in NLP have automatically identified adjectives verbs and N-grams that are statistically associated with subjective language -LRB- e.g. -LRB- Turney 2002 Hatzivassiloglou and McKeown 1997 Wiebe 2000 Wiebe et al. 2001 -RRB- -RRB-	num_Wiebe_2001 nn_Wiebe_al. nn_Wiebe_et num_Wiebe_2000 dep_Hatzivassiloglou_Wiebe conj_and_Hatzivassiloglou_Wiebe conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown dep_Turney_Wiebe dep_Turney_1997 dep_Turney_McKeown dep_Turney_Hatzivassiloglou amod_Turney_2002 appos_e.g._Turney dep_language_e.g. amod_language_subjective prep_with_associated_language advmod_associated_statistically auxpass_associated_are nsubjpass_associated_that rcmod_adjectives_associated conj_and_adjectives_N-grams conj_and_adjectives_verbs dobj_identified_N-grams dobj_identified_verbs dobj_identified_adjectives advmod_identified_automatically aux_identified_have nsubj_identified_methods prep_in_methods_NLP amod_methods_empirical det_methods_some dep_verbs_1993 nn_verbs_desire nn_verbs_Levins appos_e.g._verbs dep_words_e.g. amod_words_subjective prep_of_lists_words conj_and_contain_identified dobj_contain_lists nsubj_contain_resources amod_resources_existing det_resources_Some ccomp_``_identified ccomp_``_contain
W03-1014	P02-1053	o	For example -LRB- Spertus 1997 -RRB- developed a system to identify inflammatory texts and -LRB- Turney 2002 Pang et al. 2002 -RRB- developed methods for classifying reviews as positive or negative	conj_or_positive_negative prep_as_reviews_negative prep_as_reviews_positive amod_reviews_classifying prep_for_methods_reviews dobj_developed_methods dep_developed_Turney nsubj_developed_Spertus num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Turney_Pang appos_Turney_2002 amod_texts_inflammatory dobj_identify_texts aux_identify_to vmod_system_identify det_system_a conj_and_developed_developed dobj_developed_system nsubj_developed_Spertus prep_for_developed_example amod_Spertus_1997
W03-1017	P02-1053	o	Turney -LRB- 2002 -RRB- showed that it is possible to use only a few of those semantically oriented words -LRB- namely excellent and poor -RRB- to label other phrases co-occuring with them as positive or negative	conj_or_positive_negative prep_as_co-occuring_negative prep_as_co-occuring_positive prep_with_co-occuring_them vmod_phrases_co-occuring amod_phrases_other dobj_label_phrases aux_label_to conj_and_excellent_poor dep_namely_poor dep_namely_excellent dep_words_namely amod_words_oriented det_words_those advmod_oriented_semantically prep_of_few_words det_few_a advmod_few_only vmod_use_label dobj_use_few aux_use_to xcomp_possible_use cop_possible_is nsubj_possible_it mark_possible_that ccomp_showed_possible nsubj_showed_Turney appos_Turney_2002
W03-1017	P02-1053	o	For determining whether an opinion sentence is positive or negative we have used seed words similar to those produced by -LRB- Hatzivassiloglou and McKeown 1997 -RRB- and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_by_Turney prep_proposed_by vmod_that_proposed prep_to_similar_that amod_method_similar det_method_a amod_words_oriented advmod_oriented_semantically prep_of_set_words amod_set_larger amod_set_much det_set_a prep_with_construct_method dobj_construct_set aux_construct_to xcomp_extended_construct dobj_extended_them nsubj_extended_we dep_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown agent_produced_McKeown agent_produced_Hatzivassiloglou vmod_those_produced prep_to_similar_those amod_words_similar nn_words_seed conj_and_used_extended dobj_used_words aux_used_have nsubj_used_we prepc_for_used_determining nsubj_negative_sentence conj_or_positive_negative cop_positive_is nsubj_positive_sentence mark_positive_whether nn_sentence_opinion det_sentence_an ccomp_determining_negative ccomp_determining_positive
W03-1017	P02-1053	n	Our focus is on the sentence level unlike -LRB- Pang et al. 2002 -RRB- and -LRB- Turney 2002 -RRB- we employ a significantly larger set of seed words and we explore as indicators of orientation words from syntactic classes other than adjectives -LRB- nouns verbs and adverbs -RRB-	conj_and_nouns_adverbs conj_and_nouns_verbs dep_adjectives_adverbs dep_adjectives_verbs dep_adjectives_nouns prep_than_other_adjectives amod_classes_other amod_classes_syntactic prep_from_words_classes nn_words_orientation prep_of_indicators_words prep_as_explore_indicators nsubj_explore_we nn_words_seed prep_of_set_words amod_set_larger det_set_a advmod_larger_significantly conj_and_employ_explore dobj_employ_set nsubj_employ_we amod_Turney_2002 amod_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_unlike_Turney dep_unlike_Pang nn_level_sentence det_level_the parataxis_is_explore parataxis_is_employ dep_is_Turney dep_is_unlike prep_on_is_level nsubj_is_focus poss_focus_Our ccomp_``_is
W03-1017	P02-1053	o	The approach is based on the hypothesis that positive words co-occur more than expected by chance and so do negative words this hypothesis was validated at least for strong positive/negative words in -LRB- Turney 2002 -RRB-	amod_Turney_2002 dep_in_Turney amod_words_positive/negative amod_words_strong pobj_at_least prep_validated_in prep_for_validated_words advmod_validated_at auxpass_validated_was nsubjpass_validated_hypothesis det_hypothesis_this amod_words_negative dobj_do_words advmod_do_so nsubj_do_words prep_by_expected_chance mark_expected_than ccomp_more_expected conj_and_co-occur_do dobj_co-occur_more nsubj_co-occur_words mark_co-occur_that amod_words_positive ccomp_hypothesis_do ccomp_hypothesis_co-occur det_hypothesis_the parataxis_based_validated prep_on_based_hypothesis auxpass_based_is nsubjpass_based_approach det_approach_The ccomp_``_based
W03-1017	P02-1053	o	In earlier work -LRB- Turney 2002 -RRB- only singletons were used as seed words varying their number allows us to test whether multiple seed words have a positive effect in detection performance	nn_performance_detection amod_effect_positive det_effect_a prep_in_have_performance dobj_have_effect nsubj_have_words mark_have_whether nn_words_seed amod_words_multiple ccomp_test_have aux_test_to xcomp_allows_test dobj_allows_us csubj_allows_varying poss_number_their dobj_varying_number nn_words_seed parataxis_used_allows prep_as_used_words auxpass_used_were nsubjpass_used_singletons prep_in_used_work advmod_singletons_only dep_Turney_2002 dep_work_Turney amod_work_earlier
W05-0408	P02-1053	o	1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years -LRB- Pang and Lee 2002 Pang et al. 2004 Turney 2002 Turney and Littman 2002 Wiebe et al. 2001 Bai et al. 2004 Yu and Hatzivassiloglou 2003 and many others -RRB-	amod_others_many num_Hatzivassiloglou_2003 num_al._2004 dep_Bai_al. nn_Bai_et num_al._2001 dep_Wiebe_al. nn_Wiebe_et num_Littman_2002 num_Turney_2002 num_al._2004 dep_Pang_al. nn_Pang_et num_Lee_2002 conj_and_Pang_others conj_and_Pang_Hatzivassiloglou conj_and_Pang_Yu conj_and_Pang_Bai conj_and_Pang_Wiebe conj_and_Pang_Littman conj_and_Pang_Turney appos_Pang_Turney appos_Pang_Pang conj_and_Pang_Lee dep_years_others dep_years_Hatzivassiloglou dep_years_Yu dep_years_Bai dep_years_Wiebe dep_years_Littman dep_years_Turney dep_years_Lee dep_years_Pang amod_years_recent amod_attention_considerable prep_in_received_years prep_from_received_researchers dobj_received_attention aux_received_has nsubj_received_field nn_classification_sentiment prep_of_field_classification det_field_The rcmod_Introduction_received num_Introduction_1
W05-0408	P02-1053	o	Movie and product reviews have been the main focus of many of the recent studies in this area -LRB- Pang and Lee 2002 Pang et al. 2004 Turney 2002 Turney and Littman 2002 -RRB-	num_Littman_2002 num_Turney_2002 num_al._2004 dep_Pang_al. nn_Pang_et num_Lee_2002 conj_and_Pang_Littman conj_and_Pang_Turney conj_and_Pang_Turney conj_and_Pang_Pang conj_and_Pang_Lee det_area_this dep_studies_Littman dep_studies_Turney dep_studies_Turney dep_studies_Pang dep_studies_Lee dep_studies_Pang prep_in_studies_area amod_studies_recent det_studies_the prep_of_many_studies prep_of_focus_many amod_focus_main det_focus_the cop_focus_been aux_focus_have nsubj_focus_reviews nn_reviews_product nn_reviews_Movie conj_and_Movie_product
W05-0408	P02-1053	o	accuracy Training data Turney -LRB- 2002 -RRB- 66 % unsupervised Pang & Lee -LRB- 2004 -RRB- 87.15 % supervised Aue & Gamon -LRB- 2005 -RRB- 91.4 % supervised SO 73.95 % unsupervised SM+SO to increase seed words then SO 74.85 % weakly supervised Table 7 Classification accuracy on the movie review domain Turney -LRB- 2002 -RRB- achieves 66 % accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web	det_web_the prep_from_scores_web nn_scores_association dobj_gather_scores aux_gather_to nn_algorithm_PMI-IR det_algorithm_the vmod_using_gather dobj_using_algorithm nn_domain_review nn_domain_movie det_domain_the prep_on_accuracy_domain amod_accuracy_% number_%_66 xcomp_achieves_using dobj_achieves_accuracy nsubj_achieves_accuracy appos_Turney_2002 nn_Turney_domain nn_Turney_review nn_Turney_movie det_Turney_the prep_on_accuracy_Turney nn_accuracy_Classification num_Table_7 dobj_supervised_Table advmod_supervised_weakly nsubj_supervised_% advmod_supervised_then num_%_74.85 advmod_%_SO nn_words_seed dobj_increase_words aux_increase_to amod_SM+SO_unsupervised amod_SM+SO_% nn_SM+SO_SO number_%_73.95 vmod_supervised_increase dobj_supervised_SM+SO vmod_%_supervised num_%_91.4 dep_Aue_% dep_Aue_2005 conj_and_Aue_Gamon amod_Aue_supervised npadvmod_%_Gamon npadvmod_%_Aue num_%_87.15 dep_Pang_2004 conj_and_Pang_Lee amod_Pang_unsupervised parataxis_%_achieves rcmod_%_supervised dep_%_% dep_%_Lee dep_%_Pang num_%_66 num_%_2002 dep_Turney_% nn_Turney_data nn_Turney_Training nn_Turney_accuracy
W05-0408	P02-1053	o	c2005 Association for Computational Linguistics Automatic identification of sentiment vocabulary exploiting low association with known sentiment terms Michael Gamon Anthony Aue Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research mgamon@microsoft.com anthaue@microsoft.com Abstract We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney -LRB- 2002 -RRB- and Turney and Littman -LRB- 2002 -RRB-	appos_Littman_2002 conj_and_Turney_Littman conj_and_Turney_Littman conj_and_Turney_Turney appos_Turney_2002 prep_in_described_Turney prep_in_described_Turney vmod_terms_described nn_terms_sentiment prep_of_labeling_terms conj_and_identification_labeling amod_identification_automatic det_identification_the prep_for_technique_labeling prep_for_technique_identification det_technique_the prep_to_extension_technique det_extension_an dobj_describe_extension nsubj_describe_We rcmod_Abstract_describe nn_Abstract_anthaue@microsoft.com nn_Abstract_mgamon@microsoft.com nn_Abstract_Research nn_Abstract_Microsoft nn_Abstract_Research nn_Abstract_Microsoft nn_Abstract_Group nn_Abstract_Processing nn_Abstract_Language nn_Abstract_Natural nn_Abstract_Group nn_Abstract_Processing nn_Abstract_Language nn_Abstract_Natural nn_Abstract_Aue nn_Abstract_Anthony nn_Abstract_Gamon nn_Abstract_Michael dep_terms_Abstract dep_sentiment_terms dep_known_sentiment prep_with_association_known amod_association_low dobj_exploiting_association nn_vocabulary_sentiment prep_of_identification_vocabulary nn_identification_Automatic nn_identification_Linguistics nn_identification_Computational dep_Association_exploiting prep_for_Association_identification nn_Association_c2005
W05-0408	P02-1053	o	Turney -LRB- 2002 -RRB- and Turney and Littman -LRB- 2002 -RRB- exploit the first two generalizations for unsupervised sentiment classification of movie reviews	nn_reviews_movie prep_of_classification_reviews nn_classification_sentiment amod_classification_unsupervised prep_for_generalizations_classification num_generalizations_two amod_generalizations_first det_generalizations_the dobj_exploit_generalizations nsubj_exploit_Turney nsubj_exploit_Turney appos_Littman_2002 conj_and_Turney_Littman conj_and_Turney_Littman conj_and_Turney_Turney appos_Turney_2002
W05-0408	P02-1053	o	Turney -LRB- 2002 -RRB- starts from a small -LRB- 2 word -RRB- set of terms with known orientation -LRB- excellent and poor -RRB-	conj_and_excellent_poor dep_orientation_poor dep_orientation_excellent amod_orientation_known prep_with_set_orientation prep_of_set_terms amod_set_small det_set_a num_word_2 dep_small_word prep_from_starts_set nsubj_starts_Turney appos_Turney_2002
W05-0408	P02-1053	o	Given a set of terms with unknown sentiment orientation Turney -LRB- 2002 -RRB- then uses the PMI-IR algorithm -LRB- Turney 2001 -RRB- to issue queries to the web and determine for each of these terms its pointwise mutual information -LRB- PMI -RRB- with the two seed words across a large set of documents	prep_of_set_documents amod_set_large det_set_a prep_across_words_set nn_words_seed num_words_two det_words_the prep_with_information_words appos_information_PMI amod_information_mutual amod_information_pointwise poss_information_its det_terms_these prep_of_each_terms prep_for_determine_each det_web_the conj_and_issue_determine prep_to_issue_web dobj_issue_queries aux_issue_to num_Turney_2001 appos_algorithm_Turney nn_algorithm_PMI-IR det_algorithm_the dobj_uses_information vmod_uses_determine vmod_uses_issue dobj_uses_algorithm advmod_uses_then nsubj_uses_Turney prep_uses_Given appos_Turney_2002 nn_orientation_sentiment amod_orientation_unknown prep_with_set_orientation prep_of_set_terms det_set_a pobj_Given_set
W05-0408	P02-1053	o	We can then use this newly identified set to -LRB- 1 -RRB- use Turneys method to find the orientation for the terms and employ the terms and their scores in a classifier and -LRB- 2 -RRB- use Turneys method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to Turney -LRB- 2002 -RRB- we do not use the web as a resource to find associations rather we apply the method directly to in-domain data	amod_data_in-domain det_method_the prep_to_apply_data advmod_apply_directly dobj_apply_method nsubj_apply_we advmod_apply_rather dobj_find_associations aux_find_to vmod_resource_find det_resource_a det_web_the parataxis_use_apply prep_as_use_resource dobj_use_web neg_use_not aux_use_do nsubj_use_we appos_Turney_2002 prep_to_opposed_Turney mark_opposed_As amod_iteration_second det_iteration_a prep_for_terms_iteration nn_terms_seed amod_terms_additional amod_terms_new det_terms_the advcl_add_opposed prep_as_add_terms dobj_add_terms det_terms_the prep_for_orientation_terms det_orientation_the conj_and_find_add dobj_find_orientation aux_find_to nn_method_Turneys vmod_use_add vmod_use_find dobj_use_method dep_use_2 det_classifier_a prep_in_scores_classifier poss_scores_their det_terms_the dobj_employ_terms det_terms_the prep_for_orientation_terms det_orientation_the conj_and_find_scores conj_and_find_employ dobj_find_orientation aux_find_to nn_method_Turneys conj_and_use_use conj_and_use_use vmod_use_scores vmod_use_employ vmod_use_find dobj_use_method dep_use_1 dep_set_to prep_identified_set advmod_identified_newly vmod_this_identified dep_use_use dep_use_use dep_use_use dobj_use_this advmod_use_then aux_use_can nsubj_use_We
W05-0408	P02-1053	o	It is worth noting however that even in Turney -LRB- 2002 -RRB- the choice of seed words is explicitly motivated by domain properties of movie reviews	nn_reviews_movie prep_of_properties_reviews nn_properties_domain agent_motivated_properties advmod_motivated_explicitly auxpass_motivated_is nsubjpass_motivated_choice prep_in_motivated_Turney advmod_motivated_even mark_motivated_that nn_words_seed prep_of_choice_words det_choice_the appos_Turney_2002 ccomp_worth_motivated advmod_worth_however dep_worth_noting cop_worth_is nsubj_worth_It ccomp_``_worth
W06-0301	P02-1053	o	Identifying subjectivity helps separate opinions from fact which may be useful in question answering summarization etc. Sentiment detection is the task of determining positive or negative sentiment of words -LRB- Hatzivassiloglou and McKeown 1997 Turney 2002 Esuli and Sebastiani 2005 -RRB- phrases and sentences -LRB- Kim and Hovy 2004 Wilson et al. 2005 -RRB- or documents -LRB- Pang et al. 2002 Turney 2002 -RRB-	amod_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et appos_documents_Pang num_Wilson_2005 nn_Wilson_al. nn_Wilson_et dep_Kim_Wilson num_Kim_2004 conj_and_Kim_Hovy dep_Esuli_2005 conj_and_Esuli_Sebastiani num_Turney_2002 dep_Hatzivassiloglou_Sebastiani dep_Hatzivassiloglou_Esuli conj_and_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown conj_and_words_sentences conj_and_words_phrases appos_words_Turney appos_words_1997 appos_words_McKeown appos_words_Hatzivassiloglou prep_of_sentiment_sentences prep_of_sentiment_phrases prep_of_sentiment_words amod_sentiment_negative amod_sentiment_positive conj_or_positive_negative dobj_determining_sentiment conj_or_task_documents dep_task_Hovy dep_task_Kim prepc_of_task_determining det_task_the cop_task_is nsubj_task_detection nn_detection_Sentiment nn_etc._summarization dep_answering_etc. nn_answering_question prep_in_useful_answering cop_useful_be aux_useful_may nsubj_useful_which amod_opinions_separate parataxis_helps_documents parataxis_helps_task dep_helps_useful prep_from_helps_fact dobj_helps_opinions nsubj_helps_subjectivity amod_subjectivity_Identifying
W06-0302	P02-1053	o	-LRB- 2002 -RRB- Turney -LRB- 2002 -RRB- Dave et al.	dep_Dave_al. nn_Dave_et appos_Turney_Dave appos_Turney_2002 dep_2002_Turney dep_''_2002
W06-0305	P02-1053	o	Most of the annotation approaches tackling these issues however are aimed at performing classifications at either the document level -LRB- Pang et al. 2002 Turney 2002 -RRB- or the sentence or word level -LRB- Wiebe et al. 2004 Yu and Hatzivassiloglou 2003 -RRB-	dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Wiebe_Hatzivassiloglou dep_Wiebe_Yu amod_Wiebe_2004 dep_Wiebe_al. nn_Wiebe_et nn_level_word det_sentence_the dep_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et conj_or_level_level conj_or_level_sentence appos_level_Pang nn_level_document det_level_the preconj_level_either prep_at_performing_level prep_at_performing_sentence prep_at_performing_level dobj_performing_classifications dep_aimed_Wiebe prepc_at_aimed_performing auxpass_aimed_are advmod_aimed_however advcl_aimed_approaches det_issues_these dobj_tackling_issues xcomp_approaches_tackling nsubj_approaches_Most det_annotation_the prep_of_Most_annotation
W06-0306	P02-1053	o	In analyzing opinions -LRB- Cardie et al. 2003 Wilson et al. 2004 -RRB- judging document-level subjectivity -LRB- Pang et al. 2002 Turney 2002 -RRB- and answering opinion questions -LRB- Cardie et al. 2003 Yu and Hatzivassiloglou 2003 -RRB- the output of a sentence-level subjectivity classification can be used without modification	prep_without_used_modification auxpass_used_be aux_used_can nsubjpass_used_output vmod_used_answering vmod_used_judging dep_used_Cardie prepc_in_used_analyzing nn_classification_subjectivity amod_classification_sentence-level det_classification_a prep_of_output_classification det_output_the dep_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Cardie_Hatzivassiloglou dep_Cardie_Yu appos_Cardie_2003 dep_Cardie_al. nn_Cardie_et appos_questions_Cardie nn_questions_opinion dobj_answering_questions dep_Turney_2002 dep_Pang_Turney appos_Pang_2002 dep_Pang_al. nn_Pang_et appos_subjectivity_Pang amod_subjectivity_document-level conj_and_judging_answering dobj_judging_subjectivity num_Wilson_2004 nn_Wilson_al. nn_Wilson_et nn_al._et dep_Cardie_Wilson dep_Cardie_2003 dep_Cardie_al. dobj_analyzing_opinions
W06-0308	P02-1053	o	The ve part-ofspeech -LRB- POS -RRB- patterns from -LRB- Turney 2002 -RRB- were used for the extraction of indicators all involving at least one adjective or adverb	conj_or_adjective_adverb num_adjective_one quantmod_one_at mwe_at_least prep_involving_all_adverb prep_involving_all_adjective prep_of_extraction_indicators det_extraction_the dep_used_all prep_for_used_extraction auxpass_used_were nsubjpass_used_patterns dep_Turney_2002 dep_from_Turney prep_patterns_from nn_patterns_part-ofspeech appos_part-ofspeech_POS nn_part-ofspeech_ve det_part-ofspeech_The
W06-1613	P02-1053	o	-LRB- 2002 -RRB- and Turney -LRB- 2002 -RRB-	appos_Turney_2002 conj_and_2002_Turney
W06-1639	P02-1053	o	In particular since we treat each individual speech within a debate as a single document we are considering a version of document-level sentiment-polarity classification namely automatically distinguishing between positive and negative documents -LRB- Das and Chen 2001 Pang et al. 2002 Turney 2002 Dave et al. 2003 -RRB-	num_Dave_2003 nn_Dave_al. nn_Dave_et num_Turney_2002 num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Das_Dave conj_and_Das_Turney conj_and_Das_Pang conj_and_Das_2001 conj_and_Das_Chen dep_documents_Turney dep_documents_Pang dep_documents_2001 dep_documents_Chen dep_documents_Das amod_documents_negative amod_documents_positive conj_and_positive_negative prep_between_distinguishing_documents advmod_distinguishing_automatically nn_classification_sentiment-polarity amod_classification_document-level prep_of_version_classification det_version_a vmod_considering_distinguishing advmod_considering_namely dobj_considering_version aux_considering_are nsubj_considering_we advcl_considering_treat prep_in_considering_particular amod_document_single det_document_a det_debate_a amod_speech_individual det_speech_each prep_as_treat_document prep_within_treat_debate dobj_treat_speech nsubj_treat_we mark_treat_since ccomp_``_considering
W06-1640	P02-1053	o	-LRB- 2002 -RRB- Turney -LRB- 2002 -RRB- Dave et al.	dep_Dave_al. nn_Dave_et appos_Turney_Dave appos_Turney_2002 dep_2002_Turney dep_''_2002
W06-1641	P02-1053	o	For example the adjective unpredictable may have a negative orientation in an automotive review in a phrase such as unpredictable steering but it could have a positive orientation in a movie review in a phrase such as unpredictable plot as mentioned in -LRB- Turney 2002 -RRB- in the context of his sentiment word detection	nn_detection_word nn_detection_sentiment poss_detection_his prep_of_context_detection det_context_the prep_in_Turney_context dep_Turney_2002 prep_in_mentioned_Turney mark_mentioned_as amod_plot_unpredictable prep_such_as_phrase_plot det_phrase_a nn_review_movie det_review_a amod_orientation_positive det_orientation_a prep_in_have_review dobj_have_orientation aux_have_could nsubj_have_it amod_steering_unpredictable prep_such_as_phrase_steering det_phrase_a amod_review_automotive det_review_an amod_orientation_negative det_orientation_a advcl_have_mentioned prep_in_have_phrase conj_but_have_have prep_in_have_phrase prep_in_have_review dobj_have_orientation aux_have_may nsubj_have_unpredictable prep_for_have_example amod_unpredictable_adjective det_unpredictable_the
W06-1641	P02-1053	o	C3BTC5 and CCCDCA were used in -LRB- Kamps and Marx 2002 -RRB- and -LRB- Turney and Littman 2003 -RRB- respectively	dep_Turney_2003 conj_and_Turney_Littman advmod_Kamps_respectively conj_and_Kamps_Littman conj_and_Kamps_Turney dep_Kamps_2002 conj_and_Kamps_Marx prep_in_used_Turney prep_in_used_Marx prep_in_used_Kamps auxpass_used_were nsubjpass_used_CCCDCA nsubjpass_used_C3BTC5 conj_and_C3BTC5_CCCDCA
W06-1642	P02-1053	o	The token precision is higher than 90 % in all of the corpora including the movie domain which is considered to be difficult for SA -LRB- Turney 2002 -RRB-	amod_Turney_2002 nn_Turney_SA prep_for_difficult_Turney cop_difficult_be aux_difficult_to xcomp_considered_difficult auxpass_considered_is nsubjpass_considered_which rcmod_domain_considered nn_domain_movie det_domain_the det_corpora_the prep_of_all_corpora prep_including_%_domain prep_in_%_all num_%_90 prep_than_higher_% cop_higher_is nsubj_higher_precision amod_precision_token det_precision_The
W06-1642	P02-1053	o	Turney -LRB- 2002 -RRB- used collocation with excellent or poor to obtain positive and negative clues for document classification	nn_classification_document prep_for_clues_classification amod_clues_negative amod_clues_positive conj_and_positive_negative dobj_obtain_clues aux_obtain_to xcomp_excellent_obtain conj_or_excellent_poor prep_with_collocation_poor prep_with_collocation_excellent amod_collocation_used nn_collocation_Turney appos_Turney_2002
W06-1650	P02-1053	n	In the thriving area of research on automatic analysis and processing of product reviews -LRB- Hu and Liu 2004 Turney 2002 Pang and Lee 2005 -RRB- little attention has been paid to the important task studied here assessing review helpfulness	nn_helpfulness_review dobj_assessing_helpfulness advmod_studied_here vmod_task_assessing vmod_task_studied amod_task_important det_task_the prep_to_paid_task auxpass_paid_been aux_paid_has nsubjpass_paid_attention prep_on_paid_processing prep_on_paid_analysis prep_in_paid_area amod_attention_little num_Lee_2005 conj_and_Pang_Lee num_Turney_2002 num_Liu_2004 dep_Hu_Lee dep_Hu_Pang conj_and_Hu_Turney conj_and_Hu_Liu appos_reviews_Turney appos_reviews_Liu appos_reviews_Hu nn_reviews_product prep_of_processing_reviews conj_and_analysis_processing amod_analysis_automatic prep_of_area_research amod_area_thriving det_area_the
W06-1650	P02-1053	o	-LRB- 2002 -RRB- and Turney -LRB- 2002 -RRB- classified sentiment polarity of reviews at the document level	nn_level_document det_level_the prep_of_polarity_reviews nn_polarity_sentiment amod_polarity_classified nn_polarity_Turney appos_Turney_2002 prep_at_2002_level conj_and_2002_polarity
W06-1652	P02-1053	o	Lexical cues of differing complexities have been used including single words and Ngrams -LRB- e.g. -LRB- Mullen and Collier 2004 Pang et al. 2002 Turney 2002 Yu and Hatzivassiloglou 2003 Wiebe et al. 2004 -RRB- -RRB- as well as phrases and lexico-syntactic patterns -LRB- e.g -LRB- Kim and Hovy 2004 Hu and Liu 2004 Popescu and Etzioni 2005 Riloff and Wiebe 2003 Whitelaw et al. 2005 -RRB- -RRB-	num_Whitelaw_2005 nn_Whitelaw_al. nn_Whitelaw_et dep_Popescu_Whitelaw conj_and_Popescu_2003 conj_and_Popescu_Wiebe conj_and_Popescu_Riloff conj_and_Popescu_2005 conj_and_Popescu_Etzioni num_Hu_2004 conj_and_Hu_Liu dep_Kim_2003 dep_Kim_Wiebe dep_Kim_Riloff dep_Kim_2005 dep_Kim_Etzioni dep_Kim_Popescu conj_and_Kim_Liu conj_and_Kim_Hu conj_and_Kim_2004 conj_and_Kim_Hovy appos_e.g_Hu appos_e.g_2004 appos_e.g_Hovy appos_e.g_Kim dep_patterns_e.g amod_patterns_lexico-syntactic conj_and_phrases_patterns num_Wiebe_2004 nn_Wiebe_al. nn_Wiebe_et dep_Yu_Wiebe conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou num_Turney_2002 num_Pang_2002 nn_Pang_al. nn_Pang_et dep_Mullen_2003 dep_Mullen_Hatzivassiloglou dep_Mullen_Yu conj_and_Mullen_Turney conj_and_Mullen_Pang conj_and_Mullen_2004 conj_and_Mullen_Collier conj_and_e.g._patterns conj_and_e.g._phrases appos_e.g._Turney appos_e.g._Pang appos_e.g._2004 appos_e.g._Collier appos_e.g._Mullen dep_Ngrams_phrases dep_Ngrams_e.g. conj_and_words_Ngrams amod_words_single prep_including_used_Ngrams prep_including_used_words auxpass_used_been aux_used_have nsubjpass_used_cues amod_complexities_differing prep_of_cues_complexities amod_cues_Lexical
W06-1664	P02-1053	o	Also PMI-IR is useful for calculating semantic orientation and rating reviews -LRB- Turney 2002 -RRB-	amod_Turney_2002 nn_reviews_rating conj_and_orientation_reviews amod_orientation_semantic dobj_calculating_reviews dobj_calculating_orientation dep_useful_Turney prepc_for_useful_calculating cop_useful_is nsubj_useful_PMI-IR advmod_useful_Also
W06-3301	P02-1053	o	For example researchers -LRB- Turney 2002 Yu and Hatzivassiloglou 2003 -RRB- have identified semantic correlation between words and views positive words tend to appear more frequently in positive movie and product reviews and newswire article sentences that have a positive semantic orientation and vice versa for negative reviews or sentences with a negative semantic orientation	amod_orientation_semantic amod_orientation_negative det_orientation_a prep_with_reviews_orientation conj_or_reviews_sentences amod_reviews_negative prep_for_vice_sentences prep_for_vice_reviews advmod_vice_versa conj_and_orientation_vice amod_orientation_semantic amod_orientation_positive det_orientation_a dobj_have_vice dobj_have_orientation nsubj_have_that nn_sentences_article nn_sentences_newswire nn_reviews_product rcmod_movie_have conj_and_movie_sentences conj_and_movie_reviews amod_movie_positive advmod_frequently_more prep_in_appear_sentences prep_in_appear_reviews prep_in_appear_movie advmod_appear_frequently aux_appear_to xcomp_tend_appear nsubj_tend_words amod_words_positive conj_and_words_views prep_between_correlation_views prep_between_correlation_words amod_correlation_semantic parataxis_identified_tend dobj_identified_correlation aux_identified_have nsubj_identified_researchers prep_for_identified_example num_Hatzivassiloglou_2003 conj_and_Yu_Hatzivassiloglou dep_Turney_Hatzivassiloglou dep_Turney_Yu num_Turney_2002 appos_researchers_Turney
W06-3808	P02-1053	o	1 Introduction Sentiment analysis of text documents has received considerable attention recently -LRB- Shanahan et al. 2005 Turney 2002 Dave et al. 2003 Hu and Liu 2004 Chaovalit and Zhou 2005 -RRB-	dep_Chaovalit_2005 conj_and_Chaovalit_Zhou num_Hu_2004 conj_and_Hu_Liu num_Dave_2003 nn_Dave_al. nn_Dave_et dep_Turney_Zhou dep_Turney_Chaovalit conj_Turney_Liu conj_Turney_Hu conj_Turney_Dave num_Turney_2002 dep_Shanahan_Turney appos_Shanahan_2005 dep_Shanahan_al. nn_Shanahan_et amod_attention_considerable dep_received_Shanahan advmod_received_recently dobj_received_attention aux_received_has nsubj_received_analysis nn_documents_text prep_of_analysis_documents nn_analysis_Sentiment nn_analysis_Introduction num_analysis_1 ccomp_``_received
W07-1515	P02-1053	o	TheauthorsapplySO-PMI-IR -LRB- Turney 2002 -RRB- to extract and determine the polarity of adjectives	prep_of_polarity_adjectives det_polarity_the dobj_extract_polarity conj_and_extract_determine aux_extract_to amod_Turney_2002 vmod_TheauthorsapplySO-PMI-IR_determine vmod_TheauthorsapplySO-PMI-IR_extract dep_TheauthorsapplySO-PMI-IR_Turney
W07-2064	P02-1053	o	As comparison Turney and Littman -LRB- 2003 -RRB- used seed sets consisting of 7 words in their word valence annotation experiments while Turney -LRB- 2002 -RRB- used minimal seed sets consisting of only one positive and one negative word -LRB- excellent and poor -RRB- in his experiments on review classification	nn_classification_review prep_on_experiments_classification poss_experiments_his conj_and_excellent_poor dep_word_poor dep_word_excellent amod_word_negative num_word_one conj_and_positive_word num_positive_one quantmod_one_only prep_in_consisting_experiments prep_of_consisting_word prep_of_consisting_positive xcomp_sets_consisting nsubj_sets_seed mark_sets_while amod_seed_minimal amod_seed_used nn_seed_Turney appos_Turney_2002 nn_experiments_annotation nn_experiments_valence nn_experiments_word poss_experiments_their prep_in_words_experiments num_words_7 prep_of_consisting_words vmod_sets_consisting nn_sets_seed advcl_used_sets dobj_used_sets nsubj_used_Littman nsubj_used_Turney prep_as_used_comparison appos_Littman_2003 conj_and_Turney_Littman
W07-2072	P02-1053	o	2 Related work Our approach for emotion classification is based on the idea of -LRB- Hatzivassiloglou and McKeown 1997 -RRB- and is similar to those of -LRB- Turney 2002 -RRB- and -LRB- Turney and Littman 2003 -RRB-	dep_Turney_2003 conj_and_Turney_Littman conj_and_Turney_Littman conj_and_Turney_Turney amod_Turney_2002 prep_of_those_Turney prep_of_those_Turney prep_to_similar_those cop_similar_is conj_and_Hatzivassiloglou_similar dep_Hatzivassiloglou_1997 conj_and_Hatzivassiloglou_McKeown prep_of_idea_similar prep_of_idea_McKeown prep_of_idea_Hatzivassiloglou det_idea_the prep_on_based_idea auxpass_based_is nsubjpass_based_work nn_classification_emotion prep_for_approach_classification poss_approach_Our dep_work_approach amod_work_Related num_work_2 ccomp_``_based
W07-2072	P02-1053	o	The idea of tracing polarity through adjective cooccurrence is adopted by Turney -LRB- 2002 -RRB- for the binary -LRB- positive and negative -RRB- classification of text reviews	nn_reviews_text prep_of_classification_reviews amod_classification_binary det_classification_the conj_and_positive_negative dep_binary_negative dep_binary_positive prep_for_Turney_classification appos_Turney_2002 agent_adopted_Turney auxpass_adopted_is nsubjpass_adopted_idea amod_cooccurrence_adjective prep_through_tracing_cooccurrence dobj_tracing_polarity prepc_of_idea_tracing det_idea_The
W07-2072	P02-1053	o	Following Hatzivassiloglou and McKeown -LRB- 1997 -RRB- and Turney -LRB- 2002 -RRB- we decided to observe how often the words from the headline co-occur with each one of the six emotions	num_emotions_six det_emotions_the prep_of_one_emotions det_one_each prep_with_co-occur_one nn_co-occur_headline det_co-occur_the prep_from_words_co-occur det_words_the dep_often_words advmod_often_how ccomp_observe_often aux_observe_to xcomp_decided_observe nsubj_decided_we prep_following_decided_Turney prep_following_decided_McKeown prep_following_decided_Hatzivassiloglou appos_Turney_2002 appos_McKeown_1997 conj_and_Hatzivassiloglou_Turney conj_and_Hatzivassiloglou_McKeown
W07-2072	P02-1053	o	Some of the differences between our approach and those of Turney -LRB- 2002 -RRB- are mentioned below ?? objectives Turney -LRB- 2002 -RRB- aims at binary text classification while our objective is six class classification of one-liner headlines	nn_headlines_one-liner prep_of_classification_headlines nn_classification_class num_classification_six cop_classification_is nsubj_classification_objective mark_classification_while poss_objective_our nn_classification_text amod_classification_binary advcl_aims_classification prep_at_aims_classification nsubj_aims_Turney appos_Turney_2002 dep_objectives_aims num_objectives_?? dep_below_objectives prep_mentioned_below auxpass_mentioned_are nsubjpass_mentioned_Some appos_Turney_2002 prep_of_those_Turney conj_and_approach_those poss_approach_our prep_between_differences_those prep_between_differences_approach det_differences_the prep_of_Some_differences ccomp_``_mentioned
W07-2072	P02-1053	n	?? word class Turney -LRB- 2002 -RRB- measures polarity using only adjectives however in our approach we consider the noun the verb the adverb and the adjective content words	nn_words_content amod_words_adjective det_words_the conj_and_adverb_words det_adverb_the dobj_verb_words dobj_verb_adverb det_verb_the det_noun_the dep_consider_verb dobj_consider_noun nsubj_consider_we prep_in_consider_approach advmod_consider_however poss_approach_our advmod_adjectives_only dobj_using_adjectives parataxis_measures_consider xcomp_measures_using dobj_measures_polarity nsubj_measures_Turney appos_Turney_2002 dep_class_measures nn_class_word num_class_?? ccomp_''_class
W07-2072	P02-1053	o	?? search engines Turney -LRB- 2002 -RRB- uses the Altavista web browser while we consider and combine the frequency information acquired from three web search engines	nn_engines_search nn_engines_web num_engines_three prep_from_acquired_engines vmod_information_acquired nn_information_frequency det_information_the dobj_combine_information nsubj_combine_we conj_and_consider_combine nsubj_consider_we mark_consider_while nn_browser_web nn_browser_Altavista det_browser_the advcl_uses_combine advcl_uses_consider dobj_uses_browser nsubj_uses_Turney appos_Turney_2002 dep_engines_uses nn_engines_search nn_engines_?? ccomp_''_engines
W07-2072	P02-1053	o	?? word proximity For the web searches Turney -LRB- 2002 -RRB- uses the NEAR operator and considers only those documents that contain the adjectives within a specific proximity	amod_proximity_specific det_proximity_a prep_within_adjectives_proximity det_adjectives_the dobj_contain_adjectives nsubj_contain_that rcmod_documents_contain det_documents_those advmod_documents_only dobj_considers_documents nsubj_considers_Turney nn_operator_NEAR det_operator_the conj_and_uses_considers dobj_uses_operator nsubj_uses_Turney dep_uses_proximity appos_Turney_2002 nn_searches_web det_searches_the prep_for_proximity_searches nn_proximity_word num_proximity_??