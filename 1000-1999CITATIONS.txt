P02-1024	J92-4003	o	2 Related Work A large amount of previous research on clustering has been focused on how to find the best clusters -LSB- Brown et al. 1992 Kneser and Ney 1993 Yamamoto and Sagisaka 1999 Ueberla 1996 Pereira et al. 1993 Bellegarda et al. 1996 Bai et al. 1998 -RSB-	num_Bai_1998 nn_Bai_al. nn_Bai_et num_Bellegarda_1996 nn_Bellegarda_al. nn_Bellegarda_et num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Ueberla_1996 dep_Kneser_Bai conj_and_Kneser_Bellegarda conj_and_Kneser_Pereira conj_and_Kneser_Ueberla conj_and_Kneser_1999 conj_and_Kneser_Sagisaka conj_and_Kneser_Yamamoto conj_and_Kneser_1993 conj_and_Kneser_Ney dep_Brown_Bellegarda dep_Brown_Pereira dep_Brown_Ueberla dep_Brown_1999 dep_Brown_Sagisaka dep_Brown_Yamamoto dep_Brown_1993 dep_Brown_Ney dep_Brown_Kneser amod_Brown_1992 dep_Brown_al. nn_Brown_et amod_clusters_best det_clusters_the dobj_find_clusters aux_find_to advmod_find_how dep_focused_Brown prepc_on_focused_find auxpass_focused_been aux_focused_has nsubjpass_focused_amount nsubjpass_focused_Work amod_research_previous prep_on_amount_clustering prep_of_amount_research amod_amount_large det_amount_A amod_Work_Related num_Work_2 ccomp_``_focused
P02-1024	J92-4003	o	Many traditional clustering techniques -LSB- Brown et al. 1992 -RSB- attempt to maximize the average mutual information of adjacent clusters = 21 2 12 2121 -RRB- -LRB- -RRB- | -LRB- log -RRB- -LRB- -RRB- -LRB- WW WP WWP WWPWWI -LRB- 2 -RRB- where the same clusters are used for both predicted and conditional words	amod_words_conditional conj_and_predicted_words preconj_predicted_both prep_for_used_words prep_for_used_predicted auxpass_used_are nsubjpass_used_clusters advmod_used_where amod_clusters_same det_clusters_the rcmod_WWPWWI_used appos_WWPWWI_2 nn_WWPWWI_WWP nn_WWPWWI_WP nn_WWPWWI_WW appos_|_WWPWWI appos_|_log num_2121_12 number_12_2 dep_21_2121 dep_=_21 amod_clusters_= amod_clusters_adjacent prep_of_information_clusters amod_information_mutual amod_information_average det_information_the dobj_maximize_information aux_maximize_to vmod_attempt_maximize dep_attempt_Brown amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_techniques_| dep_techniques_attempt nn_techniques_clustering amod_techniques_traditional amod_techniques_Many
P02-1030	J92-4003	o	Proceedings of the 40th Annual Meeting of the Association for cently semantic resources have also been used in collocation discovery -LRB- Pearce 2001 -RRB- smoothing and model estimation -LRB- Brown et al. 1992 Clark and Weir 2001 -RRB- and text classi cation -LRB- Baker and McCallum 1998 -RRB-	amod_Baker_1998 conj_and_Baker_McCallum dep_cation_McCallum dep_cation_Baker nn_cation_classi nn_cation_text num_Clark_2001 conj_and_Clark_Weir dep_al._Weir dep_al._Clark num_al._1992 nn_al._et amod_al._Brown dep_estimation_al. nn_estimation_model amod_Pearce_2001 conj_and_discovery_cation conj_and_discovery_estimation conj_and_discovery_smoothing dep_discovery_Pearce nn_discovery_collocation prep_in_used_cation prep_in_used_estimation prep_in_used_smoothing prep_in_used_discovery auxpass_used_been advmod_used_also aux_used_have nsubjpass_used_resources dep_used_Proceedings amod_resources_semantic pobj_for_cently det_Association_the prep_Meeting_for prep_of_Meeting_Association amod_Meeting_Annual amod_Meeting_40th det_Meeting_the prep_of_Proceedings_Meeting
P02-1030	J92-4003	o	Most systems extract co-occurrence and syntactic information from the words surrounding the target term which is then converted into a vector-space representation of the contexts that each target term appears in -LRB- Brown et al. 1992 Pereira et al. 1993 Ruge 1997 Lin 1998b -RRB-	appos_Lin_1998b appos_Ruge_1997 num_al._1993 nn_al._et nn_al._Pereira dep_al._Lin conj_al._Ruge conj_al._al. num_al._1992 nn_al._et amod_al._Brown dep_in_al. prep_appears_in nsubj_appears_term dobj_appears_that nn_term_target det_term_each rcmod_contexts_appears det_contexts_the prep_of_representation_contexts amod_representation_vector-space det_representation_a prep_into_converted_representation advmod_converted_then auxpass_converted_is nsubjpass_converted_which rcmod_term_converted nn_term_target det_term_the dobj_surrounding_term vmod_words_surrounding det_words_the nn_information_syntactic nn_information_co-occurrence conj_and_co-occurrence_syntactic prep_from_extract_words dobj_extract_information nsubj_extract_systems amod_systems_Most
P03-1006	J92-4003	o	In many applications it is natural and convenient to construct class-based language models that is models based on classes of words -LRB- Brown et al. 1992 -RRB-	amod_Brown_1992 dep_Brown_al. nn_Brown_et prep_of_classes_words prep_on_based_classes vmod_models_based cop_models_is nsubj_models_that nn_models_language amod_models_class-based dobj_construct_models aux_construct_to xcomp_convenient_construct nsubj_convenient_it dep_natural_Brown ccomp_natural_models conj_and_natural_convenient cop_natural_is nsubj_natural_it prep_in_natural_applications amod_applications_many
P06-1038	J92-4003	o	Agglomerative clustering -LRB- e.g. -LRB- Brown et al 1992 Li 1996 -RRB- -RRB- can produce hierarchical word categories from an unannotated corpus	amod_corpus_unannotated det_corpus_an nn_categories_word amod_categories_hierarchical prep_from_produce_corpus dobj_produce_categories aux_produce_can nsubj_produce_clustering dep_Li_1996 dep_Brown_Li amod_Brown_1992 dep_Brown_al nn_Brown_et dep_e.g._Brown dep_clustering_e.g. amod_clustering_Agglomerative
P06-1096	J92-4003	n	For example we would like to know that if a -LRB- JJ JJ -RRB- 7We also tried using word clusters -LRB- Brown et al. 1992 -RRB- instead of POS but found that POS was more helpful	advmod_helpful_more cop_helpful_was nsubj_helpful_POS mark_helpful_that ccomp_found_helpful nsubj_found_7We amod_Brown_1992 dep_Brown_al. nn_Brown_et nn_clusters_word dobj_using_clusters conj_but_tried_found prep_instead_of_tried_POS dep_tried_Brown xcomp_tried_using advmod_tried_also nsubj_tried_7We dep_tried_JJ dep_tried_a mark_tried_if appos_JJ_JJ dep_that_found dep_that_tried prep_know_that aux_know_to xcomp_like_know aux_like_would nsubj_like_we prep_for_like_example
P06-2069	J92-4003	o	In class-based n-gram modeling -LRB- Brown et al. 1992 -RRB- for example classbased n-grams are used to determine the probability of occurrence of a POS class given its preceding classes and the probability of a particular word given its own POS class	nn_class_POS amod_class_own poss_class_its pobj_given_class amod_word_particular det_word_a prep_probability_given prep_of_probability_word det_probability_the amod_classes_preceding poss_classes_its dobj_given_classes nsubjpass_given_n-grams nn_class_POS det_class_a prep_of_occurrence_class prep_of_probability_occurrence det_probability_the dobj_determine_probability aux_determine_to conj_and_used_probability conj_and_used_given xcomp_used_determine auxpass_used_are nsubjpass_used_n-grams prep_for_used_example dep_used_al. prep_in_used_modeling amod_n-grams_classbased num_al._1992 nn_al._et amod_al._Brown nn_modeling_n-gram amod_modeling_class-based
P07-1094	J92-4003	o	Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired -LRB- Schutze 1995 Clark 2000 Finch et al. 1995 -RRB- probabilistic models have been used to find classes that can improve smoothing and reduce perplexity -LRB- Brown et al. 1992 Saul and Pereira 1997 -RRB-	num_Saul_1997 conj_and_Saul_Pereira dep_al._Pereira dep_al._Saul num_al._1992 nn_al._et amod_al._Brown dep_perplexity_al. dobj_reduce_perplexity nsubj_reduce_that conj_and_improve_reduce dobj_improve_smoothing aux_improve_can nsubj_improve_that rcmod_classes_reduce rcmod_classes_improve dobj_find_classes aux_find_to xcomp_used_find auxpass_used_been aux_used_have nsubjpass_used_models amod_models_probabilistic num_Finch_1995 nn_Finch_al. nn_Finch_et num_Clark_2000 dep_Schutze_Finch dep_Schutze_Clark dep_Schutze_1995 dep_desired_Schutze auxpass_desired_are nsubjpass_desired_classes advmod_desired_when amod_classes_meaningful advmod_classes_linguistically parataxis_applied_used advcl_applied_desired advmod_applied_typically auxpass_applied_are nsubjpass_applied_techniques nsubjpass_applied_clustering nn_techniques_reduction nn_techniques_dimensionality conj_and_clustering_techniques amod_clustering_Distributional
P08-1047	J92-4003	o	For example we can use automatically extracted hyponymy relations -LRB- Hearst 1992 Shinzato and Torisawa 2004 -RRB- or automatically induced MN clusters -LRB- Rooth et al. 1999 Torisawa 2001 -RRB-	amod_Torisawa_2001 dep_Rooth_Torisawa appos_Rooth_1999 dep_Rooth_al. nn_Rooth_et nn_clusters_MN dobj_induced_clusters advmod_induced_automatically dep_Shinzato_Rooth conj_or_Shinzato_induced appos_Shinzato_2004 conj_and_Shinzato_Torisawa dep_Hearst_induced dep_Hearst_Torisawa dep_Hearst_Shinzato appos_Hearst_1992 dep_relations_Hearst amod_relations_hyponymy amod_relations_extracted advmod_extracted_automatically dobj_use_relations aux_use_can nsubj_use_we prep_for_use_example
P08-1047	J92-4003	n	In addition the clustering methods used such as HMMs and Browns algorithm -LRB- Brown et al. 1992 -RRB- seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words	amod_words_adjacent prep_of_information_words det_information_the prep_on_based_information advmod_based_only auxpass_based_are nsubjpass_based_they mark_based_since prep_of_semantics_MNs det_semantics_the dobj_capture_semantics advmod_capture_adequately aux_capture_to xcomp_unable_capture advcl_seem_based acomp_seem_unable nsubj_seem_methods prep_in_seem_addition dep_al._1992 nn_al._et amod_al._Brown nn_algorithm_Browns dep_HMMs_al. conj_and_HMMs_algorithm prep_such_as_methods_algorithm prep_such_as_methods_HMMs vmod_methods_used nn_methods_clustering det_methods_the
P08-1047	J92-4003	o	They constructed word clusters by using HMMs or Browns clustering algorithm -LRB- Brown et al. 1992 -RRB- which utilize only information from neighboring words	amod_words_neighboring prep_from_information_words advmod_information_only dobj_utilize_information nsubj_utilize_which dep_al._1992 nn_al._et amod_al._Brown rcmod_algorithm_utilize dep_algorithm_al. nn_algorithm_clustering nn_algorithm_Browns nn_algorithm_HMMs conj_or_HMMs_Browns dobj_using_algorithm nn_clusters_word prepc_by_constructed_using dobj_constructed_clusters nsubj_constructed_They
P08-1058	J92-4003	o	To scale LMs to larger corpora with higher-order dependencies researchers Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models -LRB- Brown et al. 1992 -RRB- model reduction techniques such as entropy-based pruning -LRB- Stolcke 1998 -RRB- novel represention schemes such as suffix arrays -LRB- Emami et al. 2007 -RRB- Golomb Coding -LRB- Church et al. 2007 -RRB- and distributed language models that scale more readily -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_readily_more dep_scale_Brants advmod_scale_readily nsubj_scale_that nn_models_language amod_models_distributed amod_Church_2007 dep_Church_al. nn_Church_et dep_Coding_Church nn_Coding_Golomb amod_Emami_2007 dep_Emami_al. nn_Emami_et conj_and_arrays_models conj_and_arrays_Coding dep_arrays_Emami nn_arrays_suffix prep_such_as_schemes_models prep_such_as_schemes_Coding prep_such_as_schemes_arrays nn_schemes_represention amod_schemes_novel amod_Stolcke_1998 rcmod_pruning_scale conj_pruning_schemes dep_pruning_Stolcke amod_pruning_entropy-based prep_such_as_techniques_pruning nn_techniques_reduction amod_techniques_model amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_models_Brown amod_models_class-based appos_parameterizations_techniques prep_as_parameterizations_models mwe_parameterizations_such amod_parameterizations_alternative dobj_considered_parameterizations aux_considered_have nn_Inc._Google prep_at_was_Inc. nsubj_was_author mark_was_while det_author_this dep_completed_considered advcl_completed_was prep_Work_completed nsubj_Work_researchers advcl_Work_scale amod_dependencies_higher-order prep_with_corpora_dependencies amod_corpora_larger prep_to_scale_corpora dobj_scale_LMs aux_scale_To
P08-1068	J92-4003	o	2.2 Brown clustering algorithm In order to provide word clusters for our experiments we used the Brown clustering algorithm -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown dep_algorithm_al. nn_algorithm_clustering amod_algorithm_Brown det_algorithm_the dobj_used_algorithm nsubj_used_we ccomp_used_algorithm poss_experiments_our nn_clusters_word prep_for_provide_experiments dobj_provide_clusters aux_provide_to dep_provide_order mark_provide_In advcl_algorithm_provide nn_algorithm_clustering amod_algorithm_Brown num_algorithm_2.2
P09-1015	J92-4003	o	To group the letters into classes we employ a hierarchical clustering algorithm -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown dep_algorithm_al. nn_algorithm_clustering amod_algorithm_hierarchical det_algorithm_a dobj_employ_algorithm nsubj_employ_we advcl_employ_group det_letters_the prep_into_group_classes dobj_group_letters aux_group_To
P09-1015	J92-4003	o	129 5 Active learning Whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random an active learner has control over the labelled data that it obtains -LRB- Cohn et al. 1992 -RRB-	amod_Cohn_1992 dep_Cohn_al. nn_Cohn_et dep_obtains_Cohn nsubj_obtains_it mark_obtains_that ccomp_data_obtains amod_data_labelled det_data_the prep_over_control_data dobj_has_control nsubj_has_learner ccomp_has_5 amod_learner_active det_learner_an prep_at_drawn_random advmod_drawn_typically auxpass_drawn_are nsubjpass_drawn_that rcmod_examples_drawn nn_examples_training prep_of_collection_examples det_collection_a prep_with_provided_collection auxpass_provided_is nsubjpass_provided_algorithm mark_provided_Whereas nn_algorithm_learning amod_algorithm_supervised amod_algorithm_passive det_algorithm_a advcl_learning_provided dep_Active_learning amod_5_Active number_5_129
P09-1031	J92-4003	o	We have -LRB- 11 -RRB- Hypernym Patterns based on patterns proposed by -LRB- Hearst 1992 -RRB- and -LRB- Snow et al. 2005 -RRB- -LRB- 12 -RRB- Sibling Patterns which are basically conjunctions and -LRB- 13 -RRB- Part-of Patterns based on patterns proposed by -LRB- Girju et al. 2003 -RRB- and -LRB- Cimiano and Wenderoth 2007 -RRB-	dep_Cimiano_2007 conj_and_Cimiano_Wenderoth conj_and_Girju_Wenderoth conj_and_Girju_Cimiano amod_Girju_2003 dep_Girju_al. nn_Girju_et agent_proposed_Cimiano agent_proposed_Girju vmod_patterns_proposed prep_on_based_patterns vmod_Patterns_based amod_Patterns_Part-of dep_Patterns_13 advmod_conjunctions_basically cop_conjunctions_are nsubj_conjunctions_which nn_Patterns_Sibling amod_Snow_2005 dep_Snow_al. nn_Snow_et conj_and_Hearst_Patterns rcmod_Hearst_conjunctions appos_Hearst_12 conj_and_Hearst_Snow amod_Hearst_1992 agent_proposed_Patterns agent_proposed_Snow agent_proposed_Hearst vmod_patterns_proposed prep_on_based_patterns vmod_Patterns_based nn_Patterns_Hypernym num_Patterns_11 dobj_have_Patterns nsubj_have_We
P09-1031	J92-4003	o	5.3 Performance of Taxonomy Induction In this section we compare the following automatic taxonomy induction systems HE the system by Hearst -LRB- 1992 -RRB- with 6 hypernym patterns GI the system by Girju et al.	nn_al._et nn_al._Girju prep_by_system_al. det_system_the appos_GI_system nn_patterns_hypernym num_patterns_6 appos_Hearst_1992 dep_system_GI prep_with_system_patterns prep_by_system_Hearst det_system_the dep_HE_system dep_systems_HE nn_systems_induction nn_systems_taxonomy amod_systems_automatic amod_systems_following det_systems_the dobj_compare_systems nsubj_compare_we nsubj_compare_Performance det_section_this nn_Induction_Taxonomy prep_in_Performance_section prep_of_Performance_Induction num_Performance_5.3
P09-1031	J92-4003	o	Pattern-based approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen either manually -LRB- Berland and Charniak 1999 Kozareva et al. 2008 -RRB- or via automatic bootstrapping -LRB- Hearst 1992 Widdows and Dorow 2002 Girju et al. 2003 -RRB-	num_Girju_2003 nn_Girju_al. nn_Girju_et conj_and_Widdows_Girju conj_and_Widdows_2002 conj_and_Widdows_Dorow dep_Hearst_Girju dep_Hearst_2002 dep_Hearst_Dorow dep_Hearst_Widdows amod_Hearst_1992 dep_bootstrapping_Hearst amod_bootstrapping_automatic pobj_via_bootstrapping num_Kozareva_2008 nn_Kozareva_al. nn_Kozareva_et conj_or_Berland_via dep_Berland_Kozareva conj_and_Berland_1999 conj_and_Berland_Charniak dep_manually_via dep_manually_1999 dep_manually_Charniak dep_manually_Berland preconj_manually_either advmod_chosen_manually advmod_chosen_carefully auxpass_chosen_are nsubjpass_chosen_patterns mark_chosen_if det_patterns_the prep_of_instances_relations dobj_recognizing_instances prepc_in_accuracy_recognizing amod_accuracy_high poss_accuracy_their advcl_known_chosen prep_for_known_accuracy auxpass_known_are nsubjpass_known_approaches amod_approaches_Pattern-based
P09-1031	J92-4003	o	Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors -LRB- Brown et al. 1992 Lin 1998 -RRB-	dep_Lin_1998 dep_Brown_Lin appos_Brown_1992 dep_Brown_al. nn_Brown_et det_vectors_the dep_similarities_Brown prep_of_similarities_vectors prep_on_based_similarities vmod_vectors_based dep_vectors_words conj_and_vectors_cluster nn_contexts_word prep_as_represent_cluster prep_as_represent_vectors dobj_represent_contexts advmod_represent_usually nsubj_represent_approaches amod_approaches_Clustering-based
P09-1031	J92-4003	o	Agglomerative clustering -LRB- Brown et al. 1992 Caraballo 1999 Rosenfeld and Feldman 2007 Yang and Callan 2008 -RRB- iteratively merges the most similar clusters into bigger clusters which need to be labeled	auxpass_labeled_be aux_labeled_to xcomp_need_labeled nsubj_need_which rcmod_clusters_need amod_clusters_bigger amod_clusters_similar det_clusters_the advmod_similar_most prep_into_merges_clusters dobj_merges_clusters advmod_merges_iteratively dep_Yang_2008 conj_and_Yang_Callan num_Rosenfeld_2007 conj_and_Rosenfeld_Feldman dep_Caraballo_merges dep_Caraballo_Callan dep_Caraballo_Yang conj_Caraballo_Feldman conj_Caraballo_Rosenfeld num_Caraballo_1999 dep_al._Caraballo appos_al._1992 nn_al._et amod_al._Brown dep_clustering_al. amod_clustering_Agglomerative
P09-1116	J92-4003	o	Previous approaches e.g. -LRB- Miller et al. 2004 -RRB- and -LRB- Koo et al. 2008 -RRB- have all used the Brown algorithm for clustering -LRB- Brown et al. 1992 -RRB-	num_al._1992 nn_al._et amod_al._Brown amod_algorithm_Brown det_algorithm_the dep_used_al. prep_for_used_clustering dobj_used_algorithm dep_used_all aux_used_have dep_2008_al. nn_al._et num_Koo_2008 dep_2004_al. nn_al._et num_Miller_2004 dep_approaches_used conj_and_approaches_Koo appos_approaches_Miller dep_approaches_e.g. amod_approaches_Previous
P93-1022	J92-4003	o	5.2 A data recovery task In the second evaluation the estimation method had to distinguish between members of two sets of 8It should be emphasized that the TWS method uses only a monolingual target corpus and not a bilingual corpus as in other methods -LRB- -LRB- Brown et al. 1991 Gale et al. 1992 -RRB- -RRB-	num_Gale_1992 nn_Gale_al. nn_Gale_et dep_Brown_Gale amod_Brown_1991 dep_Brown_al. nn_Brown_et amod_methods_other pobj_in_methods pcomp_as_in appos_corpus_Brown prep_corpus_as amod_corpus_bilingual det_corpus_a neg_corpus_not nn_corpus_target amod_corpus_monolingual det_corpus_a advmod_corpus_only conj_and_uses_corpus dobj_uses_corpus nsubj_uses_method mark_uses_that nn_method_TWS det_method_the ccomp_emphasized_corpus ccomp_emphasized_uses auxpass_emphasized_be aux_emphasized_should nsubjpass_emphasized_had prep_of_sets_8It num_sets_two prep_of_members_sets prep_between_distinguish_members aux_distinguish_to xcomp_had_distinguish nsubj_had_method dep_had_task nn_method_estimation det_method_the amod_evaluation_second det_evaluation_the prep_in_task_evaluation nn_task_recovery nn_task_data det_task_A num_task_5.2
P93-1022	J92-4003	o	Class based models -LRB- Brown et al. Pereira et al. 1993 Hirschman 1986 Resnik 1992 -RRB- distinguish between unobserved cooccurrences using classes of similar words	amod_words_similar prep_of_classes_words dobj_using_classes amod_cooccurrences_unobserved xcomp_distinguish_using prep_between_distinguish_cooccurrences nsubj_distinguish_models dep_Resnik_1992 num_Hirschman_1986 nn_al._et nn_al._Pereira dep_Brown_Resnik dep_Brown_Hirschman amod_Brown_1993 dep_Brown_al. dep_Brown_al. nn_Brown_et appos_models_Brown amod_models_based nn_models_Class
P93-1023	J92-4003	o	However only recently has work been done on the automatic computation of such relationships from text quantifying similarity between words and clustering them -LRB- -LRB- Brown et aL 1992 -RRB- -LRB- Pereira et al. 1993 -RRB- -RRB-	dep_Pereira_1993 dep_Pereira_al. nn_Pereira_et dep_aL_1992 nn_aL_et dep_Brown_Pereira dep_Brown_aL dep_clustering_Brown dobj_clustering_them conj_and_words_clustering prep_between_similarity_clustering prep_between_similarity_words dobj_quantifying_similarity prep_from_relationships_text amod_relationships_such prep_of_computation_relationships amod_computation_automatic det_computation_the xcomp_done_quantifying prep_on_done_computation auxpass_done_been nsubjpass_done_work aux_done_has advmod_done_However advmod_has_recently advmod_recently_only
P93-1023	J92-4003	o	One other published model for grouping semantically related words -LRB- Brown et al. 1992 -RRB- is based on a statistical model of bigrams and trigrams and produces word groups using no linguistic knowledge but no evaluation of the results is reported	auxpass_reported_is nsubjpass_reported_evaluation det_results_the prep_of_evaluation_results neg_evaluation_no amod_knowledge_linguistic neg_knowledge_no dobj_using_knowledge vmod_groups_using nn_groups_word dobj_produces_groups nsubj_produces_model conj_and_bigrams_trigrams prep_of_model_trigrams prep_of_model_bigrams amod_model_statistical det_model_a conj_but_based_reported conj_and_based_produces prep_on_based_model auxpass_based_is nsubjpass_based_model amod_Brown_1992 dep_Brown_al. nn_Brown_et amod_words_related advmod_related_semantically dobj_grouping_words appos_model_Brown prepc_for_model_grouping amod_model_published amod_model_other num_model_One ccomp_``_reported ccomp_``_produces ccomp_``_based
P93-1034	J92-4003	o	-LRB- Brown et al. 1992 -RRB- where the same idea of improving generalization and accuracy by looking at word classes instead of individual words is used	auxpass_used_is nsubjpass_used_idea advmod_used_where amod_words_individual nn_classes_word prep_instead_of_looking_words prep_at_looking_classes conj_and_generalization_accuracy prepc_by_improving_looking dobj_improving_accuracy dobj_improving_generalization prepc_of_idea_improving amod_idea_same det_idea_the nn_al._et rcmod_Brown_used dep_Brown_1992 advmod_Brown_al.
P93-1043	J92-4003	p	The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in -LRB- Brown et al. 1992 -RRB-	advmod_1992_al. nn_al._et dep_Brown_1992 dep_in_Brown prep_explored_in prep_in_explored_detail auxpass_explored_is nsubjpass_explored_notion conj_and_satisfying_explored advmod_satisfying_intuitively cop_satisfying_is nsubj_satisfying_notion amod_items_lexical prep_of_classes_items dobj_merging_classes advmod_merging_incrementally prepc_of_notion_merging det_notion_The
P94-1038	J92-4003	o	272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss -LRB- 1992 -RRB- derived from work on acoustic model smoothing by Sugawara et al.	nn_al._et nn_al._Sugawara nn_smoothing_model amod_smoothing_acoustic agent_derived_al. prep_on_derived_smoothing prep_from_derived_work appos_Steinbiss_1992 conj_and_Essen_Steinbiss vmod_method_derived prep_of_method_Steinbiss prep_of_method_Essen nn_method_smoothing nn_method_cooccurrence det_method_the nn_modeling_language prep_in_used_method prep_for_used_modeling advmod_used_first auxpass_used_was nsubjpass_used_estimation amod_estimation_Similarity-based num_estimation_272
P95-1025	J92-4003	o	The class-based approaches -LRB- Brown et al. 1992 Resnik 1992 Pereira et al. 1993 -RRB- calculate co-occurrence data of words belonging to different classes ~ rather than individual words to enhance the co-occurrence data collected and to cover words which have low occurrence frequencies	nn_frequencies_occurrence amod_frequencies_low dobj_have_frequencies nsubj_have_which rcmod_words_have dobj_cover_words aux_cover_to conj_and_collected_cover vmod_data_cover vmod_data_collected nn_data_co-occurrence det_data_the dobj_enhance_data aux_enhance_to amod_words_individual vmod_~_enhance conj_negcc_~_words amod_classes_different prep_to_belonging_classes vmod_words_belonging appos_data_words appos_data_~ prep_of_data_words nn_data_co-occurrence dobj_calculate_data nsubj_calculate_approaches dep_al._1993 nn_al._et nn_al._Pereira num_Resnik_1992 dep_Brown_al. conj_Brown_Resnik appos_Brown_1992 dep_Brown_al. nn_Brown_et appos_approaches_Brown amod_approaches_class-based det_approaches_The
P95-1025	J92-4003	o	On the other hand the thesaurus-based method of Yarowsky -LRB- 1992 -RRB- may suffer from loss of information -LRB- since it is semi-class-based -RRB- as well as data sparseness -LRB- since H Classes used in Resnik -LRB- 1992 -RRB- are based on the WordNet taxonomy while classes of Brown et al.	nn_al._et nn_al._Brown prep_of_classes_al. nn_taxonomy_WordNet det_taxonomy_the prep_while_based_classes prep_on_based_taxonomy auxpass_based_are nsubjpass_based_Classes mark_based_since appos_Resnik_1992 prep_in_used_Resnik vmod_Classes_used nn_Classes_H advcl_-LRB-_based nn_sparseness_data cop_semi-class-based_is nsubj_semi-class-based_it mark_semi-class-based_since prep_of_loss_information conj_and_suffer_sparseness parataxis_suffer_semi-class-based prep_from_suffer_loss aux_suffer_may nsubj_suffer_method prep_on_suffer_hand appos_Yarowsky_1992 prep_of_method_Yarowsky amod_method_thesaurus-based det_method_the amod_hand_other det_hand_the
P95-1025	J92-4003	o	1 Introduction Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data -LRB- Kelly and Stone 1975 Black 1988 and Hearst 1991 -RRB- or aligned bilingual corpora -LRB- Brown et al. 1991 Dagan 1991 and Gale et al. 1992 -RRB-	dep_al._1992 nn_al._et nn_al._Gale conj_and_Dagan_al. conj_and_Dagan_1991 conj_al._1991 nn_al._et advmod_Brown_al. amod_corpora_bilingual amod_corpora_aligned dep_Black_1991 conj_and_Black_Hearst conj_and_Black_1988 dep_Kelly_al. dep_Kelly_1991 dep_Kelly_Dagan dep_Kelly_Brown conj_or_Kelly_corpora conj_and_Kelly_Hearst conj_and_Kelly_1988 conj_and_Kelly_Black conj_and_Kelly_1975 conj_and_Kelly_Stone dep_data_corpora dep_data_Black dep_data_1975 dep_data_Stone dep_data_Kelly nn_data_training amod_data_sense-tagged prep_of_amounts_data amod_amounts_substantial dobj_require_amounts nsubj_require_methods nn_methods_disambiguation nn_methods_sense amod_methods_corpus-based amod_methods_Previous nn_methods_Introduction num_methods_1 ccomp_``_require
P95-1037	J92-4003	o	These 30 questions are determined by growing a classification tree on the word vocabulary as described in -LRB- Brown et al. 1992 -RRB-	num_al._1992 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_as nn_vocabulary_word det_vocabulary_the nn_tree_classification det_tree_a advcl_growing_described prep_on_growing_vocabulary dobj_growing_tree agent_determined_growing auxpass_determined_are nsubjpass_determined_questions num_questions_30 det_questions_These ccomp_``_determined
P96-1004	J92-4003	o	Finally inducing lexical semantics from distributional data -LRB- e.g. -LRB- Brown et al. 1992 Church et al. 1989 -RRB- -RRB- is also a form of surface cueing	nn_cueing_surface prep_of_form_cueing det_form_a advmod_form_also cop_form_is csubj_form_inducing advmod_form_Finally num_Church_1989 nn_Church_al. nn_Church_et dep_Brown_Church amod_Brown_1992 dep_Brown_al. nn_Brown_et appos_e.g._Brown dep_data_e.g. amod_data_distributional amod_semantics_lexical prep_from_inducing_data dobj_inducing_semantics
P97-1008	J92-4003	o	Class-based methods -LRB- Brown et al. 1992 Pereira Tishby and Lee 1993 Resnik 1992 -RRB- cluster words into classes of similar words so that one can base the estimate of a word pair 's probability on the averaged cooccurrence probability of the classes to which the two words belong	nsubj_belong_words prep_to_belong_which num_words_two det_words_the rcmod_classes_belong det_classes_the prep_of_probability_classes nn_probability_cooccurrence amod_probability_averaged det_probability_the poss_probability_pair nn_pair_word det_pair_a prep_on_estimate_probability prep_of_estimate_probability det_estimate_the dobj_base_estimate aux_base_can nsubj_base_one mark_base_that advmod_base_so amod_words_similar prep_of_classes_words prep_into_cluster_classes dep_cluster_words dep_cluster_Brown dep_Resnik_1992 num_Pereira_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby dep_Brown_Resnik conj_Brown_Lee conj_Brown_Tishby conj_Brown_Pereira amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_methods_base dep_methods_cluster amod_methods_Class-based
P97-1023	J92-4003	o	Given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues -LRB- Brown et al. 1992 Pereira et al. 1993 Hatzivassiloglou and McKeown 1993 -RRB- identifying the semantic orientation of words would allow a system to further refine the retrieved semantic similarity relationships extracting antonyms	dobj_extracting_antonyms nn_relationships_similarity amod_relationships_semantic amod_relationships_retrieved det_relationships_the dobj_refine_relationships advmod_refine_further aux_refine_to det_system_a xcomp_allow_extracting xcomp_allow_refine dobj_allow_system aux_allow_would nsubj_allow_Given prep_of_orientation_words amod_orientation_semantic det_orientation_the dobj_identifying_orientation dep_Hatzivassiloglou_1993 conj_and_Hatzivassiloglou_McKeown nn_al._et nn_al._Pereira dep_Brown_McKeown dep_Brown_Hatzivassiloglou amod_Brown_1993 dep_Brown_al. amod_Brown_1992 dep_Brown_al. nn_Brown_et amod_cues_linguistic conj_and_properties_cues amod_properties_distributional prep_of_basis_cues prep_of_basis_properties det_basis_the prep_on_identified_basis advmod_identified_automatically auxpass_identified_be aux_identified_can nsubjpass_identified_words mark_identified_that amod_words_similar advmod_words_semantically vmod_Given_identifying dep_Given_Brown ccomp_Given_identified
P97-1033	J92-4003	o	-LRB- Wang and Hirschberg 1992 Wightman and Ostendorf 1994 Stolcke and Shriberg 1996a Kompe et al. 1994 Mast et al. 1996 -RRB- -RRB- and on speech repair detection and correction -LRB- e.g.	nn_e.g._correction conj_and_detection_e.g. nn_detection_repair dep_speech_e.g. dep_speech_detection pobj_on_speech num_Mast_1996 nn_Mast_al. nn_Mast_et num_Kompe_1994 nn_Kompe_al. nn_Kompe_et dep_Stolcke_Mast conj_and_Stolcke_Kompe conj_and_Stolcke_1996a conj_and_Stolcke_Shriberg conj_and_Wightman_on dep_Wightman_Kompe dep_Wightman_1996a dep_Wightman_Shriberg dep_Wightman_Stolcke num_Wightman_1994 conj_and_Wightman_Ostendorf dep_Wang_on dep_Wang_Ostendorf dep_Wang_Wightman appos_Wang_1992 conj_and_Wang_Hirschberg dep_''_Hirschberg dep_''_Wang
P97-1033	J92-4003	o	Since there is no well-agreed to definition of what an utterance is we instead focus on intonational phrases -LRB- Silverman et al. 1992 -RRB- which end with an acoustically signaled boundary lone	amod_boundary_lone dobj_signaled_boundary advmod_signaled_acoustically vmod_an_signaled prep_with_end_an nsubj_end_which dep_al._1992 nn_al._et nn_al._Silverman amod_phrases_intonational dep_focus_end dep_focus_al. prep_on_focus_phrases advmod_focus_instead nsubj_focus_we advcl_focus_is nsubj_is_utterance dobj_is_what det_utterance_an prepc_of_definition_is neg_well-agreed_no prep_to_is_definition nsubj_is_well-agreed expl_is_there mark_is_Since
P97-1033	J92-4003	o	-LRB- Black et al. 1992 Materman 1995 -RRB- -RRB- we treat the word identities as a further refinement of the POS tags thus we build a word classification tree for each POS tag	nn_tag_POS det_tag_each prep_for_tree_tag nn_tree_classification nn_tree_word det_tree_a dobj_build_tree nsubj_build_we advmod_build_thus nn_tags_POS det_tags_the prep_of_refinement_tags amod_refinement_further det_refinement_a nn_identities_word det_identities_the parataxis_treat_build prep_as_treat_refinement dobj_treat_identities nsubj_treat_we dep_treat_Black dep_Materman_1995 dep_Black_Materman appos_Black_1992 dep_Black_al. nn_Black_et
P97-1056	J92-4003	o	An exception is the use of similarity for alleviating the sparse data problem in language modeling -LRB- Essen & Steinbiss 1992 Brown et al. 1992 Dagan et al. 1994 -RRB-	num_Dagan_1994 nn_Dagan_al. nn_Dagan_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Essen_Dagan dep_Essen_Brown dep_Essen_1992 conj_and_Essen_Steinbiss appos_modeling_Steinbiss appos_modeling_Essen nn_modeling_language nn_problem_data amod_problem_sparse det_problem_the prep_in_alleviating_modeling dobj_alleviating_problem prepc_for_similarity_alleviating prep_of_use_similarity det_use_the cop_use_is nsubj_use_exception det_exception_An
P98-1016	J92-4003	o	Clustering can be done statistically by analyzing text corpora -LRB- Wilks et al. 1989 Brown et al. 1992 Pereira et al. 1995 -RRB- and usually results in a set of words or word senses	nn_senses_word conj_or_words_senses prep_of_set_senses prep_of_set_words det_set_a prep_in_results_set advmod_results_usually num_Pereira_1995 nn_Pereira_al. nn_Pereira_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Wilks_Pereira conj_Wilks_Brown appos_Wilks_1989 dep_Wilks_al. nn_Wilks_et appos_corpora_Wilks nn_corpora_text conj_and_analyzing_results dobj_analyzing_corpora agent_done_results agent_done_analyzing advmod_done_statistically auxpass_done_be aux_done_can nsubjpass_done_Clustering
P98-1047	J92-4003	o	In our experiments the class assignment is performed by maximizing the mutual information between adjacent phrases following the line described in -LRB- Brown 301 et al. 1992 -RRB- with only the modification that candidates to clustering are phrases instead of words	prep_instead_of_phrases_words cop_phrases_are nsubj_phrases_candidates dobj_phrases_that prep_to_candidates_clustering rcmod_modification_phrases det_modification_the advmod_modification_only nn_al._et num_al._301 dep_Brown_1992 dep_Brown_al. dep_in_Brown prep_described_in vmod_line_described det_line_the amod_phrases_adjacent prep_between_information_phrases amod_information_mutual det_information_the dobj_maximizing_information prep_with_performed_modification prep_following_performed_line agent_performed_maximizing auxpass_performed_is nsubjpass_performed_assignment prep_in_performed_experiments nn_assignment_class det_assignment_the poss_experiments_our
P98-1119	J92-4003	o	In some cases class -LRB- or part of speech -RRB- n-grams are used instead of word n-grams -LRB- Brown et al. 1992 Chang and Chen 1996 -RRB-	num_Chang_1996 conj_and_Chang_Chen dep_al._Chen dep_al._Chang num_al._1992 nn_al._et amod_al._Brown dep_n-grams_al. nn_n-grams_word prep_instead_of_used_n-grams auxpass_used_are nsubjpass_used_n-grams prep_in_used_cases nn_n-grams_class prep_of_part_speech cc_part_or dep_class_part det_cases_some
P98-2124	J92-4003	o	There have been a number of methods proposed in the literature to address the word clustering problem -LRB- e.g. -LRB- Brown et al. 1992 Pereira et al. 1993 Li and Abe 1996 -RRB- -RRB-	num_Li_1996 conj_and_Li_Abe nn_al._et nn_al._Pereira dep_Brown_Abe dep_Brown_Li amod_Brown_1993 dep_Brown_al. amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_e.g._Brown ccomp_-LRB-_e.g. nn_problem_clustering nn_problem_word det_problem_the dobj_address_problem aux_address_to det_literature_the xcomp_proposed_address prep_in_proposed_literature vmod_methods_proposed prep_of_number_methods det_number_a cop_number_been aux_number_have expl_number_There
P98-2124	J92-4003	n	Our method is a natural extension of those proposed in -LRB- Brown et al. 1992 -RRB- and -LRB- Li and Abe 1996 -RRB- and overcomes their drawbacks while retaining their advantages	poss_advantages_their dobj_retaining_advantages mark_retaining_while poss_drawbacks_their advcl_overcomes_retaining dobj_overcomes_drawbacks nsubj_overcomes_method num_Li_1996 conj_and_Li_Abe num_al._1992 nn_al._et amod_al._Brown dep_in_al. prep_proposed_in vmod_those_proposed conj_and_extension_overcomes conj_and_extension_Abe conj_and_extension_Li prep_of_extension_those amod_extension_natural det_extension_a cop_extension_is nsubj_extension_method poss_method_Our
P98-2148	J92-4003	o	To cope with this problem we 898 use the concept of class proposed for a word n-gram model -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown dep_model_al. nn_model_n-gram nn_model_word det_model_a prep_for_proposed_model vmod_class_proposed prep_of_concept_class det_concept_the dobj_use_concept nsubj_use_898 nsubj_use_we det_problem_this ccomp_cope_use prep_with_cope_problem aux_cope_To
P98-2148	J92-4003	o	To avoid this problem we use the concept of class proposed for a word n-gram model -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown dep_model_al. nn_model_n-gram nn_model_word det_model_a prep_for_proposed_model vmod_class_proposed prep_of_concept_class det_concept_the dobj_use_concept nsubj_use_we det_problem_this parataxis_avoid_use dobj_avoid_problem aux_avoid_To ccomp_``_avoid
P98-2180	J92-4003	o	Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts -LRB- e.g. Brown et al. 1992 Yarowsky 1992 -RRB- as well as in similar predicateargument structure contexts -LRB- e.g. Grishman and Sterling 1994 -RRB-	num_Sterling_1994 conj_and_Grishman_Sterling dep_e.g._Sterling dep_e.g._Grishman ccomp_-LRB-_e.g. nn_contexts_structure nn_contexts_predicateargument amod_contexts_similar pobj_in_contexts num_Yarowsky_1992 conj_and_al._in appos_al._Yarowsky dep_al._1992 nn_al._et nn_al._Brown dep_,_in dep_,_al. dep_-LRB-_e.g. nn_contexts_trigram conj_and_bigram_contexts amod_bigram_similar prep_in_occurring_contexts prep_in_occurring_bigram vmod_words_occurring prep_of_clusters_words dobj_yield_clusters nsubj_yield_that rcmod_corpora_yield amod_corpora_large prep_of_analyses_corpora amod_analyses_statistical prep_on_based_analyses auxpass_based_been advmod_based_often aux_based_have nsubjpass_based_strategies dobj_determining_similarity prepc_for_strategies_determining amod_strategies_Syntagmatic
P98-2221	J92-4003	o	The mutual information clustering algorithm -LRB- Brown et al. 1992 -RRB- were used for this	prep_for_used_this auxpass_used_were nsubjpass_used_algorithm num_al._1992 nn_al._et amod_al._Brown appos_algorithm_al. nn_algorithm_clustering nn_algorithm_information amod_algorithm_mutual det_algorithm_The
P99-1005	J92-4003	o	Furthermore early work on class-based language models was inconclusive -LRB- Brown et al. 1992 -RRB-	num_al._1992 nn_al._et amod_al._Brown dep_inconclusive_al. cop_inconclusive_was nsubj_inconclusive_work advmod_inconclusive_Furthermore nn_models_language amod_models_class-based prep_on_work_models amod_work_early
W00-0725	J92-4003	o	In this spirit we introduce a generalization of the classic k-gram models widely used for string processing -LRB- Brown et al. 1992 Ney et al. 1995 -RRB- to the case of trees	prep_of_case_trees det_case_the dep_al._1995 nn_al._et nn_al._Ney dep_al._al. num_al._1992 nn_al._et amod_al._Brown nn_processing_string dep_used_al. prep_for_used_processing advmod_used_widely nn_models_k-gram amod_models_classic det_models_the prep_of_generalization_models det_generalization_a prep_to_introduce_case vmod_introduce_used dobj_introduce_generalization nsubj_introduce_we prep_in_introduce_spirit det_spirit_this
W00-1305	J92-4003	o	-LRB- Brown et al. 1992 -RRB- -RRB-	amod_Brown_1992 dep_Brown_al. nn_Brown_et
W00-1311	J92-4003	o	For better probability estimation the model was extended to work with -LRB- hidden -RRB- word classes -LRB- Brown et al. 1992 Ward and Issar 1996 -RRB-	amod_Brown_1996 conj_and_Brown_Issar conj_and_Brown_Ward amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_classes_Issar dep_classes_Ward dep_classes_Brown nn_classes_word dep_classes_hidden mark_classes_with advcl_work_classes aux_work_to xcomp_extended_work auxpass_extended_was nsubjpass_extended_model prep_for_extended_estimation det_model_the nn_estimation_probability amod_estimation_better
W01-1004	J92-4003	o	In the literature approaches to construction of taxonomies of concepts have been proposed -LRB- Brown et al. 1992 McMahon and Smith 1996 Sanderson and Croft 1999 -RRB-	num_Croft_1999 num_Smith_1996 conj_and_Brown_Croft conj_and_Brown_Sanderson conj_and_Brown_Smith conj_and_Brown_McMahon dep_Brown_1992 dep_Brown_al. nn_Brown_et dep_proposed_Croft dep_proposed_Sanderson dep_proposed_Smith dep_proposed_McMahon dep_proposed_Brown auxpass_proposed_been aux_proposed_have prep_of_taxonomies_concepts prep_of_construction_taxonomies advcl_approaches_proposed prep_to_approaches_construction prep_in_approaches_literature det_literature_the
W02-0908	J92-4003	o	These tasks include collocation discovery -LRB- Pearce 2001 -RRB- smoothing and model estimation -LRB- Brown et al. 1992 Clark and Weir 2001 -RRB- and text classi cation -LRB- Baker and McCallum 1998 -RRB-	amod_Baker_1998 conj_and_Baker_McCallum dep_cation_McCallum dep_cation_Baker nn_cation_classi nn_cation_text num_Clark_2001 conj_and_Clark_Weir dep_al._Weir dep_al._Clark num_al._1992 nn_al._et amod_al._Brown dep_estimation_al. nn_estimation_model amod_Pearce_2001 conj_and_discovery_cation conj_and_discovery_estimation conj_and_discovery_smoothing dep_discovery_Pearce nn_discovery_collocation dobj_include_cation dobj_include_estimation dobj_include_smoothing dobj_include_discovery nsubj_include_tasks det_tasks_These
W02-1029	J92-4003	o	Proceedings of the Conference on Empirical Methods in Natural 2 Automatic Thesaurus Extraction The development of large thesauri and semantic resources such as WordNet -LRB- Fellbaum 1998 -RRB- has allowed lexical semantic information to be leveraged to solve NLP tasks including collocation discovery -LRB- Pearce 2001 -RRB- model estimation -LRB- Brown et al. 1992 Clark and Weir 2001 -RRB- and text classi cation -LRB- Baker and McCallum 1998 -RRB-	amod_Baker_1998 conj_and_Baker_McCallum dep_cation_McCallum dep_cation_Baker nn_cation_classi nn_cation_text num_Clark_2001 conj_and_Clark_Weir dep_al._Weir dep_al._Clark num_al._1992 nn_al._et amod_al._Brown conj_and_estimation_cation dep_estimation_al. nn_estimation_model amod_Pearce_2001 dep_discovery_Pearce nn_discovery_collocation appos_tasks_cation appos_tasks_estimation prep_including_tasks_discovery nn_tasks_NLP dobj_solve_tasks aux_solve_to xcomp_leveraged_solve auxpass_leveraged_be aux_leveraged_to amod_information_semantic amod_information_lexical xcomp_allowed_leveraged dobj_allowed_information aux_allowed_has nsubj_allowed_development amod_Fellbaum_1998 dep_WordNet_Fellbaum amod_resources_semantic prep_such_as_thesauri_WordNet conj_and_thesauri_resources amod_thesauri_large prep_of_development_resources prep_of_development_thesauri det_development_The rcmod_Extraction_allowed nn_Extraction_Thesaurus nn_Extraction_Automatic num_Extraction_2 amod_Extraction_Natural prep_in_Methods_Extraction amod_Methods_Empirical det_Conference_the prep_on_Proceedings_Methods prep_of_Proceedings_Conference
W02-1032	J92-4003	o	The clusters were found automatically by attempting to minimize perplexity -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown dep_perplexity_al. dobj_minimize_perplexity aux_minimize_to xcomp_attempting_minimize agent_found_attempting advmod_found_automatically auxpass_found_were nsubjpass_found_clusters det_clusters_The
W03-0416	J92-4003	o	The second type has clear interpretation as a probability model but no criteria to determine the number of clusters -LRB- Brown et al. 1992 Kneser and Ney 1993 -RRB-	amod_Kneser_1993 conj_and_Kneser_Ney dep_Brown_Ney dep_Brown_Kneser amod_Brown_1992 dep_Brown_al. nn_Brown_et prep_of_number_clusters det_number_the dobj_determine_number aux_determine_to vmod_criteria_determine neg_criteria_no nn_model_probability det_model_a amod_interpretation_clear dep_has_Brown conj_but_has_criteria prep_as_has_model dobj_has_interpretation nsubj_has_type amod_type_second det_type_The
W03-0416	J92-4003	o	The idea of word class -LRB- Brown et al. 1992 -RRB- gives a general solution to this problem	det_problem_this amod_solution_general det_solution_a prep_to_gives_problem dobj_gives_solution nsubj_gives_idea dep_al._1992 nn_al._et dep_Brown_al. appos_class_Brown nn_class_word prep_of_idea_class det_idea_The
W03-0416	J92-4003	o	Examples have been class-based D2-gram models -LRB- Brown et al. 1992 Kneser and Ney 1993 -RRB- smoothing techniques for structural disambiguation -LRB- Li and Abe 1998 -RRB- and word sense disambiguation -LRB- Shutze 1998 -RRB-	amod_Shutze_1998 dep_disambiguation_Shutze nn_disambiguation_sense nn_disambiguation_word num_Li_1998 conj_and_Li_Abe conj_and_disambiguation_disambiguation appos_disambiguation_Abe appos_disambiguation_Li amod_disambiguation_structural prep_for_techniques_disambiguation prep_for_techniques_disambiguation amod_techniques_smoothing dep_Kneser_1993 conj_and_Kneser_Ney dep_Brown_Ney dep_Brown_Kneser amod_Brown_1992 dep_Brown_al. nn_Brown_et appos_models_techniques appos_models_Brown nn_models_D2-gram amod_models_class-based cop_models_been aux_models_have nsubj_models_Examples
W03-1710	J92-4003	o	Among various language modeling approaches ngram modeling has been widely used in many applications such as speech recognition machine translation -LRB- Katz 1987 Jelinek 1989 Gale and Church 1990 Brown et al. 1992 Yang et al. 1996 Bai et al 1998 Zhou et al 1999 Rosenfeld 2000 Gao et al 2002 -RRB-	nn_2002_al dep_Gao_2002 nn_Gao_et num_Rosenfeld_2000 nn_1999_al dep_Zhou_1999 nn_Zhou_et num_al_1998 dep_Bai_al nn_Bai_et dep_Yang_1996 dep_Yang_al. nn_Yang_et num_al._1992 dep_Brown_al. nn_Brown_et num_Church_1990 conj_and_Gale_Church num_Jelinek_1989 dep_Katz_Gao dep_Katz_Rosenfeld dep_Katz_Zhou dep_Katz_Bai dep_Katz_Yang dep_Katz_Brown dep_Katz_Church dep_Katz_Gale dep_Katz_Jelinek num_Katz_1987 appos_translation_Katz nn_translation_machine conj_recognition_translation nn_recognition_speech prep_such_as_applications_recognition amod_applications_many prep_in_used_applications advmod_used_widely auxpass_used_been aux_used_has nsubjpass_used_modeling prep_among_used_approaches nn_modeling_ngram nn_approaches_modeling nn_approaches_language amod_approaches_various
W03-2907	J92-4003	o	-LRB- Brown et al. 1992 -RRB- is one of the first works to use statistical methods of distributional analysis to induce clusters of words	prep_of_clusters_words dobj_induce_clusters aux_induce_to vmod_analysis_induce amod_analysis_distributional prep_of_methods_analysis amod_methods_statistical dobj_use_methods aux_use_to vmod_works_use amod_works_first det_works_the prep_of_one_works nsubj_is_one nsubj_is_Brown amod_Brown_1992 dep_Brown_al. nn_Brown_et
W03-2907	J92-4003	n	While we have shown an increase in performance over a purely syntactic baseline model -LRB- the algorithm of -LRB- Brown et al. 1992 -RRB- -RRB- there are a number of avenues to pursue in extending this work	det_work_this dobj_extending_work prepc_in_pursue_extending aux_pursue_to vmod_number_pursue prep_of_number_avenues det_number_a nsubj_are_number expl_are_there advcl_are_shown num_al._1992 nn_al._et amod_al._Brown dep_of_al. amod_algorithm_of det_algorithm_the nn_model_baseline amod_model_syntactic det_model_a advmod_syntactic_purely prep_over_increase_model prep_in_increase_performance det_increase_an dep_shown_algorithm dobj_shown_increase aux_shown_have nsubj_shown_we mark_shown_While
W03-2907	J92-4003	o	The corpus used for training our models was on the order of 100,000 words whereas that used by -LRB- Brown et al. 1992 -RRB- was around 1,000 times this size	det_size_this num_size_times number_times_1,000 quantmod_times_around dobj_was_size nsubj_was_that mark_was_whereas num_al._1992 nn_al._et amod_al._Brown dep_by_al. prep_used_by vmod_that_used num_words_100,000 prep_of_order_words det_order_the advcl_was_was prep_on_was_order nsubj_was_corpus poss_models_our dobj_training_models prep_for_used_training vmod_corpus_used det_corpus_The ccomp_``_was
W03-2907	J92-4003	o	In this article we used the algorithm of -LRB- Brown et al. 1992 -RRB- to initialize the model	det_model_the dobj_initialize_model aux_initialize_to num_al._1992 nn_al._et amod_al._Brown xcomp_of_initialize dep_of_al. prep_algorithm_of det_algorithm_the dobj_used_algorithm nsubj_used_we prep_in_used_article det_article_this
W03-2907	J92-4003	o	Since there is no practical way of determining the classification a0 which maximizes this quantity for a given corpus -LRB- Brown et al. 1992 -RRB- use a greedy algorithm which proceeds from the initial classification performing the merge which results in the least loss in mutual information at each stage	det_stage_each amod_information_mutual prep_at_loss_stage prep_in_loss_information amod_loss_least det_loss_the prep_in_results_loss nsubj_results_which ccomp_merge_results nsubj_merge_the ccomp_performing_merge advcl_performing_is amod_classification_initial det_classification_the prep_from_proceeds_classification nsubj_proceeds_which rcmod_algorithm_proceeds amod_algorithm_greedy det_algorithm_a dobj_use_algorithm nsubj_use_a0 amod_Brown_1992 dep_Brown_al. nn_Brown_et amod_corpus_given det_corpus_a det_quantity_this prep_for_maximizes_corpus dobj_maximizes_quantity nsubj_maximizes_which appos_a0_Brown rcmod_a0_maximizes rcmod_classification_use det_classification_the dobj_determining_classification prepc_of_way_determining amod_way_practical neg_way_no nsubj_is_way expl_is_there mark_is_Since
W03-2907	J92-4003	o	The observation probabilities for a given state representing a certain word class are determined by the relative frequencies of words belonging to that class -LRB- as determined by the algorithm of -LRB- Brown et al. 1992 -RRB- -RRB- the probabilities of other words are set to a small initial value	amod_value_initial amod_value_small det_value_a prep_to_set_value auxpass_set_are nsubjpass_set_probabilities amod_words_other prep_of_probabilities_words det_probabilities_the num_al._1992 nn_al._et amod_al._Brown dep_of_al. amod_algorithm_of det_algorithm_the prep_by_determined_algorithm mark_determined_as det_class_that prep_to_belonging_class vmod_words_belonging prep_of_frequencies_words amod_frequencies_relative det_frequencies_the parataxis_determined_set parataxis_determined_determined agent_determined_frequencies auxpass_determined_are vmod_determined_representing nsubjpass_determined_probabilities nn_class_word amod_class_certain det_class_a dobj_representing_class amod_state_given det_state_a prep_for_probabilities_state nn_probabilities_observation det_probabilities_The
W03-2907	J92-4003	o	Since these morphological generalizations are based on the initial categorization provided by the algorithm of -LRB- Brown et al. 1992 -RRB- we hope that they will foster speedy convergence of HNN training	nn_training_HNN prep_of_convergence_training amod_convergence_speedy dobj_foster_convergence aux_foster_will nsubj_foster_they mark_foster_that ccomp_hope_foster nsubj_hope_we advcl_hope_based num_al._1992 nn_al._et amod_al._Brown dep_of_al. prep_algorithm_of det_algorithm_the agent_provided_algorithm vmod_categorization_provided amod_categorization_initial det_categorization_the prep_on_based_categorization auxpass_based_are nsubjpass_based_generalizations mark_based_Since amod_generalizations_morphological det_generalizations_these
W04-2410	J92-4003	p	For example the class-based language model of -LRB- Brown et al. 1992 -RRB- is defined as p -LRB- w2 | w1 -RRB- = p -LRB- w2 | c2 -RRB- p -LRB- c2 | c1 -RRB- -LRB- 1 -RRB- This helps solve the sparse data problem since the number of classes is usually much smaller than the number of words	prep_of_number_words det_number_the prep_than_smaller_number advmod_much_smaller advmod_much_usually cop_much_is nsubj_much_number mark_much_since prep_of_number_classes det_number_the nn_problem_data amod_problem_sparse det_problem_the advcl_solve_much dobj_solve_problem xcomp_helps_solve nsubj_helps_This dep_helps_1 num_c1_| nn_c1_c2 rcmod_p_helps appos_p_c1 nn_p_w2 nn_p_p num_c2_| appos_w2_c2 dep_=_p num_w1_| nn_w1_w2 amod_p_= appos_p_w1 dep_as_p prep_defined_as auxpass_defined_is nsubjpass_defined_model prep_for_defined_example num_al._1992 nn_al._et amod_al._Brown dep_of_al. amod_model_of nn_model_language amod_model_class-based det_model_the
W04-2410	J92-4003	o	-LRB- 1994 -RRB- uses the mutual information clustering algorithm described in -LRB- Brown et al. 1992 -RRB-	num_al._1992 nn_al._et amod_al._Brown dep_in_al. prep_described_in vmod_algorithm_described nn_algorithm_clustering nn_algorithm_information amod_algorithm_mutual det_algorithm_the dobj_uses_algorithm dep_uses_1994
W04-2602	J92-4003	o	It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model -LRB- Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_model_Cutting nn_model_Markov amod_model_hidden det_model_a conj_and_tagging_model amod_tagging_partial prep_with_realized_model prep_with_realized_tagging auxpass_realized_be aux_realized_can nsubjpass_realized_performance amod_performance_good det_performance_that rcmod_years_realized det_years_some prep_for_known_years auxpass_known_been aux_known_has nsubjpass_known_It
W04-2602	J92-4003	o	Fortunately using distributional characteristics of term contexts it is feasible to induce part-of-speech categories directly from a corpus of suf cient size as several papers have made clear -LRB- Brown et al. 1992 Schcurrency1utze 1993 Clark 2000 -RRB-	amod_Clark_2000 conj_Schcurrency1utze_Clark conj_Schcurrency1utze_1993 conj_al._1992 nn_al._et dep_Brown_Schcurrency1utze advmod_Brown_al. dep_clear_Brown acomp_made_clear aux_made_have nsubj_made_papers mark_made_as amod_papers_several amod_size_cient nn_size_suf prep_of_corpus_size det_corpus_a amod_categories_part-of-speech advcl_induce_made prep_from_induce_corpus advmod_induce_directly dobj_induce_categories aux_induce_to xcomp_feasible_induce cop_feasible_is nsubj_feasible_it vmod_feasible_using advmod_feasible_Fortunately nn_contexts_term prep_of_characteristics_contexts amod_characteristics_distributional dobj_using_characteristics
W04-2602	J92-4003	o	Our approach to inducing syntactic clusters is closely related to that described in Brown et al -LRB- 1992 -RRB- which is one of the earliest papers on the subject	det_subject_the prep_on_papers_subject amod_papers_earliest det_papers_the prep_of_one_papers cop_one_is nsubj_one_which nn_al_et rcmod_Brown_one appos_Brown_1992 appos_Brown_al prep_in_described_Brown vmod_that_described prep_to_related_that advmod_related_closely cop_related_is nsubj_related_approach amod_clusters_syntactic dobj_inducing_clusters prepc_to_approach_inducing poss_approach_Our ccomp_``_related
W04-2602	J92-4003	o	Clark -LRB- 2000 -RRB- reports results on a corpus containing 12 million terms Schcurrency1utze -LRB- 1993 -RRB- on one containing 25 million terms and Brown et al -LRB- 1992 -RRB- on one containing 365 million terms	num_terms_million number_million_365 dobj_containing_terms vmod_one_containing nn_al_et prep_on_Brown_one appos_Brown_1992 appos_Brown_al num_terms_million number_million_25 dobj_containing_terms vmod_one_containing prep_on_Schcurrency1utze_one appos_Schcurrency1utze_1993 conj_and_terms_Brown conj_and_terms_Schcurrency1utze num_terms_million number_million_12 dobj_containing_Brown dobj_containing_Schcurrency1utze dobj_containing_terms vmod_corpus_containing det_corpus_a prep_on_results_corpus nsubj_results_reports nn_reports_Clark appos_Clark_2000
W05-0503	J92-4003	p	This paper is heavily indebted to prior work on unsupervised learning of position categories such as Brown et al 1992 Schtze 1997 Higgins 2002 and others cited there	advmod_cited_there num_Higgins_2002 dep_1997_Schtze num_al_1992 dep_et_al conj_and_Brown_others conj_and_Brown_Higgins advmod_Brown_1997 amod_Brown_et prep_such_as_categories_others prep_such_as_categories_Higgins prep_such_as_categories_Brown nn_categories_position prep_of_learning_categories amod_learning_unsupervised prep_on_work_learning amod_work_prior dep_indebted_cited prep_to_indebted_work advmod_indebted_heavily cop_indebted_is nsubj_indebted_paper det_paper_This
W05-0609	J92-4003	o	A key example is that of class-based language models -LRB- Brown et al. 1992 Dagan et al. 1999 -RRB- where clustering approaches are used in order to partition words determined to be similar into sets	prep_into_similar_sets cop_similar_be aux_similar_to xcomp_determined_similar nn_words_partition prep_to_order_words prep_in_used_order auxpass_used_are nsubjpass_used_approaches advmod_used_where nn_approaches_clustering dep_al._1999 nn_al._et nn_al._Dagan vmod_Brown_determined rcmod_Brown_used dep_Brown_al. amod_Brown_1992 dep_Brown_al. nn_Brown_et nn_models_language amod_models_class-based dep_that_Brown prep_of_that_models prep_is_that nsubj_is_example amod_example_key det_example_A
W05-0617	J92-4003	o	This approach to term clustering is closely related to others from the literature -LRB- Brown et al. 1992 Clark 2000 -RRB- .2 Recall that the mutual information between random variables a0 and a1 can be written a2a4a3a6a5a8a7a10a9a11a13a12a15a14a17a16a19a18a21a20a23a22a25a24a27a26a29a28 a14a17a16a19a18a21a20a23a22a25a24 a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24 -LRB- 1 -RRB- Here a0 and a1 correspond to term and context clusters respectively each event a18 and a22 the observation of some term and contextual term in the corpus	det_corpus_the amod_term_contextual conj_and_term_term det_term_some prep_of_observation_term prep_of_observation_term det_observation_the dep_a18_observation conj_and_a18_a22 prep_in_event_corpus dep_event_a22 dep_event_a18 det_event_each nn_clusters_context dobj_term_event advmod_term_respectively conj_and_term_clusters aux_term_to xcomp_correspond_clusters xcomp_correspond_term nsubj_correspond_a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24 conj_and_a0_a1 appos_a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24_a1 appos_a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24_a0 advmod_a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24_Here appos_a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24_1 nn_a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24_a14a17a16a19a18a21a20a23a22a25a24 num_a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24_a2a4a3a6a5a8a7a10a9a11a13a12a15a14a17a16a19a18a21a20a23a22a25a24a27a26a29a28 auxpass_written_be aux_written_can nsubjpass_written_information mark_written_that conj_and_a0_a1 dep_variables_a1 dep_variables_a0 amod_variables_random prep_between_information_variables amod_information_mutual det_information_the ccomp_Recall_written nsubj_Recall_.2 dep_Recall_others dep_Recall_to num_Clark_2000 dep_al._Clark dep_al._1992 nn_al._et amod_al._Brown det_literature_the dep_others_al. prep_from_others_literature parataxis_related_correspond xcomp_related_Recall advmod_related_closely cop_related_is nsubj_related_approach dobj_term_clustering aux_term_to vmod_approach_term det_approach_This
W05-0708	J92-4003	o	Many approaches for POS tagging have been developed in the past including rule-based tagging -LRB- Brill 1995 -RRB- HMM taggers -LRB- Brants 2000 Cutting and others 1992 -RRB- maximum-entropy models -LRB- Rathnaparki 1996 -RRB- cyclic dependency networks -LRB- Toutanova et al. 2003 -RRB- memory-based learning -LRB- Daelemans et al. 1996 -RRB- etc. All of these approaches require either a large amount of annotated training data -LRB- for supervised tagging -RRB- or a lexicon listing all possible tags for each word -LRB- for unsupervised tagging -RRB-	amod_tagging_unsupervised det_word_each prep_for_tags_tagging prep_for_tags_word amod_tags_possible det_tags_all dobj_listing_tags vmod_lexicon_listing det_lexicon_a conj_or_tagging_lexicon amod_tagging_supervised pobj_for_lexicon pobj_for_tagging ccomp_-LRB-_for nn_data_training amod_data_annotated prep_of_amount_data amod_amount_large det_amount_a preconj_amount_either dobj_require_amount nsubj_require_All det_approaches_these prep_of_All_approaches amod_Daelemans_1996 dep_Daelemans_al. nn_Daelemans_et dep_learning_Daelemans amod_learning_memory-based amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et nn_networks_dependency amod_networks_cyclic amod_Rathnaparki_1996 dep_models_Rathnaparki amod_models_maximum-entropy dep_others_1992 conj_and_Cutting_others dep_Brants_others dep_Brants_Cutting appos_Brants_2000 appos_taggers_Brants nn_taggers_HMM amod_Brill_1995 conj_tagging_etc. conj_tagging_learning dep_tagging_Toutanova conj_tagging_networks conj_tagging_models conj_tagging_taggers dep_tagging_Brill amod_tagging_rule-based det_past_the parataxis_developed_require prep_including_developed_tagging prep_in_developed_past auxpass_developed_been aux_developed_have nsubjpass_developed_approaches nn_tagging_POS prep_for_approaches_tagging amod_approaches_Many
W05-1011	J92-4003	o	These problems include collocation discovery -LRB- Pearce 2001 -RRB- smoothing and estimation -LRB- Brown et al. 1992 Clark and Weir 2001 -RRB- and question answering -LRB- Pasca and Harabagiu 2001 -RRB-	dep_Pasca_2001 conj_and_Pasca_Harabagiu dep_answering_Harabagiu dep_answering_Pasca nn_answering_question num_Clark_2001 conj_and_Clark_Weir dep_al._Weir dep_al._Clark num_al._1992 nn_al._et amod_al._Brown dep_estimation_al. amod_Pearce_2001 conj_and_discovery_answering conj_and_discovery_estimation conj_and_discovery_smoothing dep_discovery_Pearce nn_discovery_collocation dobj_include_answering dobj_include_estimation dobj_include_smoothing dobj_include_discovery nsubj_include_problems det_problems_These
W06-1615	J92-4003	o	There are many choices for modeling co-occurrence data -LRB- Brown et al. 1992 Pereira et al. 1993 Blei et al. 2003 -RRB-	appos_al._2003 nn_al._et nn_al._Blei nn_al._et nn_al._Pereira dep_Brown_al. amod_Brown_1993 dep_Brown_al. amod_Brown_1992 dep_Brown_al. nn_Brown_et nn_data_co-occurrence nn_data_modeling prep_for_choices_data amod_choices_many dep_are_Brown nsubj_are_choices expl_are_There
W07-0735	J92-4003	o	8 Related Research Class-based LMs -LRB- Brown et al. 1992 -RRB- or factored LMs -LRB- Bilmes and Kirchhoff 2003 -RRB- are very similar to our T+C scenario	nn_scenario_T+C poss_scenario_our prep_to_similar_scenario advmod_similar_very cop_similar_are nsubj_similar_LMs nsubj_similar_LMs dep_Bilmes_2003 conj_and_Bilmes_Kirchhoff appos_LMs_Kirchhoff appos_LMs_Bilmes amod_LMs_factored amod_Brown_1992 dep_Brown_al. nn_Brown_et conj_or_LMs_LMs dep_LMs_Brown amod_LMs_Class-based nn_LMs_Research nn_LMs_Related num_LMs_8
W09-0905	J92-4003	o	This merging of contexts is different than clustering words -LRB- e.g. Clark 2000 Brown et al. 1992 -RRB- but is applicable as word clustering relies on knowing which contexts identify the same category	amod_category_same det_category_the dobj_identify_category nsubj_identify_contexts det_contexts_which ccomp_knowing_identify prepc_on_relies_knowing nsubj_relies_clustering mark_relies_as nn_clustering_word advcl_applicable_relies cop_applicable_is nsubj_applicable_Clark dep_applicable_e.g. num_Brown_1992 nn_Brown_al. nn_Brown_et cc_Clark_but dep_Clark_Brown num_Clark_2000 dep_words_applicable nn_words_clustering prep_than_different_words cop_different_is nsubj_different_This prep_of_merging_contexts vmod_This_merging ccomp_``_different
W09-1119	J92-4003	o	The technique is based on word class models pioneered by -LRB- Brown et al. 1992 -RRB- which hierarchically 151 CoNLL03 CoNLL03 MUC7 MUC7 Web Component Test data Dev data Dev Test pages 1 -RRB- Baseline 83.65 89.25 74.72 71.28 71.41 2 -RRB- -LRB- 1 -RRB- + Gazetteer Match 87.22 91.61 85.83 80.43 74.46 3 -RRB- -LRB- 1 -RRB- + Word Class Model 86.82 90.85 80.25 79.88 72.26 4 -RRB- All External Knowledge 88.55 92.49 84.50 83.23 74.44 Table 4 Utility of external knowledge	amod_knowledge_external prep_of_Utility_knowledge num_Table_4 num_Table_74.44 num_Table_83.23 dep_84.50_Table dep_92.49_84.50 number_92.49_88.55 dep_Knowledge_Utility dep_Knowledge_92.49 amod_Knowledge_External dep_All_Knowledge dep_72.26_All dep_72.26_4 dep_79.88_72.26 dep_79.88_80.25 number_80.25_90.85 dep_86.82_79.88 dep_Model_86.82 nn_Model_Class nn_Model_Word conj_+_1_Model parataxis_3_Model parataxis_3_1 dep_3_74.46 dep_3_91.61 dep_3_Match dep_3_+ dep_3_1 dep_3_2 dep_3_al. num_74.46_80.43 number_80.43_85.83 number_91.61_87.22 nn_Match_Gazetteer dep_2_71.41 dep_2_89.25 dep_2_Baseline dep_2_1 dep_2_pages num_71.41_71.28 number_71.28_74.72 number_89.25_83.65 nn_pages_Test nn_pages_Dev nn_pages_data amod_pages_Dev nn_pages_data nn_pages_Test nn_pages_Component nn_pages_Web nn_pages_MUC7 nn_pages_MUC7 nn_pages_CoNLL03 nn_pages_CoNLL03 num_pages_151 advmod_pages_hierarchically det_pages_which num_al._1992 nn_al._et amod_al._Brown agent_pioneered_3 vmod_models_pioneered nn_models_class nn_models_word prep_on_based_models auxpass_based_is nsubjpass_based_technique det_technique_The
W09-1119	J92-4003	o	The approach is related but not identical to distributional similarity -LRB- for details see -LRB- Brown et al. 1992 -RRB- and -LRB- Liang 2005 -RRB- -RRB-	dep_Liang_2005 amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_see_Brown conj_and_for_Liang conj_and_for_see pobj_for_details dep_similarity_Liang dep_similarity_see dep_similarity_for amod_similarity_distributional neg_identical_not nsubj_identical_approach prep_to_related_similarity conj_but_related_identical cop_related_is nsubj_related_approach det_approach_The ccomp_``_identical ccomp_``_related
W93-0113	J92-4003	o	\ -LSB- Brown et al. 1992 \ -RSB- Peter F. Brown Vincent J. Della Pietra Petere V. deSouza Jenifer C. Lai and Robert L. Mercer	nn_Mercer_L. nn_Mercer_Robert nn_Lai_C. nn_Lai_Jenifer nn_deSouza_V. nn_deSouza_Petere nn_Pietra_Della nn_Pietra_J. nn_Pietra_Vincent conj_and_Brown_Mercer appos_Brown_Lai appos_Brown_deSouza appos_Brown_Pietra nn_Brown_F. nn_Brown_Peter dep_Brown_Brown nn_Brown_\ num_\_1992 dep_al._\ nn_al._et dep_Brown_al.
W93-0113	J92-4003	o	A number of knowledge-rich \ -LSB- Jacobs and Rau 1990 Calzolari and Bindi 1990 Mauldin 1991 \ -RSB- and knowledge-poor \ -LSB- Brown et al. 1992 Hindle 1990 Ruge 1991 Grefenstette 1992 \ -RSB- methods have been proposed for recognizing when words are similar	cop_similar_are nsubj_similar_words advmod_similar_when advcl_recognizing_similar prepc_for_proposed_recognizing auxpass_proposed_been aux_proposed_have nsubjpass_proposed_al. dep_proposed_Brown nn_methods_Hindle num_\_1992 dep_Ruge_\ conj_Ruge_Grefenstette conj_Ruge_1991 dep_Hindle_Ruge num_Hindle_1990 conj_al._methods conj_al._1992 nn_al._et amod_\_knowledge-poor num_\_1991 dep_Bindi_\ dep_Bindi_Mauldin dep_Bindi_1990 conj_and_Jacobs_Bindi conj_and_Jacobs_Calzolari amod_Jacobs_1990 conj_and_Jacobs_Rau amod_\_knowledge-rich rcmod_number_proposed conj_and_number_\ dep_number_Bindi dep_number_Calzolari dep_number_Rau dep_number_Jacobs prep_of_number_\ det_number_A
W94-0106	J92-4003	o	Other researchers have also reported similar problems of excessive resource demands with the collect all neighbors model \ -LSB- Gale et al. 1992 \ -RSB-	num_\_1992 appos_Gale_\ dep_Gale_al. nn_Gale_et num_model_\ det_neighbors_all dep_collect_model dobj_collect_neighbors dep_collect_the dep_with_collect prep_demands_with nn_demands_resource amod_demands_excessive prep_of_problems_demands amod_problems_similar dep_reported_Gale dobj_reported_problems advmod_reported_also aux_reported_have nsubj_reported_researchers amod_researchers_Other ccomp_``_reported
W94-0106	J92-4003	n	Other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word class \ -LSB- Brown et al. 1992 \ -RSB- or do not exploit as much linguistic knowledge as we do \ -LSB- Pereira et al. 1993 \ -RSB-	num_\_1993 appos_Pereira_\ dep_Pereira_al. nn_Pereira_et dobj_do_\ nsubj_do_we mark_do_as amod_knowledge_linguistic amod_knowledge_much advmod_knowledge_as dobj_exploit_knowledge neg_exploit_not aux_exploit_do num_\_1992 dep_Brown_Pereira advcl_Brown_do conj_or_Brown_exploit appos_Brown_\ dep_Brown_al. nn_Brown_et num_class_\ nn_class_word amod_class_specific det_class_a prep_with_deal_class neg_deal_not aux_deal_do nsubj_deal_systems amod_knowledge_linguistic prep_of_use_knowledge det_use_the dep_emphasize_exploit dep_emphasize_Brown conj_and_emphasize_deal dobj_emphasize_use neg_emphasize_not aux_emphasize_do nsubj_emphasize_systems nn_probleans_classification nn_probleans_word dobj_address_probleans nsubj_address_that rcmod_systems_address amod_systems_statistical amod_systems_Other
W94-0106	J92-4003	o	Similarly the sense disambiguation problem is typically attacked by comparing the distribution of the neighbors of a word 's occurrence to prototypical distributions associated with each of the word 's senses \ -LSB- Gale et al. 1992 Schtltze 1992 \ -RSB-	num_\_1992 appos_Gale_\ appos_Gale_Schtltze num_Gale_1992 dep_Gale_al. nn_Gale_et dep_\_Gale poss_senses_word det_word_the prep_of_each_senses prep_with_associated_each vmod_distributions_associated amod_distributions_prototypical prep_to_occurrence_distributions poss_occurrence_word det_word_a prep_of_neighbors_occurrence det_neighbors_the prep_of_distribution_neighbors det_distribution_the dobj_comparing_distribution dep_attacked_\ agent_attacked_comparing advmod_attacked_typically auxpass_attacked_is nsubjpass_attacked_problem advmod_attacked_Similarly nn_problem_disambiguation nn_problem_sense det_problem_the
W94-0106	J92-4003	o	It has been used for diverse problems such as machine translation and sense disambiguation \ -LSB- Gale et al. 1992 Schiltze 1992 \ -RSB-	num_\_1992 dep_Gale_\ conj_Gale_Schiltze num_Gale_1992 dep_Gale_al. nn_Gale_et nn_\_disambiguation nn_\_sense conj_and_translation_\ nn_translation_machine prep_such_as_problems_\ prep_such_as_problems_translation amod_problems_diverse dep_used_Gale prep_for_used_problems auxpass_used_been aux_used_has nsubjpass_used_It
W95-0105	J92-4003	o	3.1 Distributionally derived groupings Distributional cluster -LRB- Brown et al. 1992 -RRB- head body hands eye voice arm seat hair mouth Word ` head ' -LRB- 17 alternatives -RRB- 0.0000 crown peak summit head top subconceptofupperbound 0.0000 principaL school principal head teacher head educator who has executive authority 0.0000 head chief top dog subeoncept of leader 0.0000 head a user of -LRB- usually soft -RRB- drugs 0.1983 head the head of the page the head of the fist 0.1983 beginning head origin root source the point or place where something begins 0.0000 pass head straits a difficult juncture a pretty pass 0.0000 headway head subconcept of progress progression advance 0.0903 point hod a V-shaped mark at one end of an arrow pointer 0.0000 heading head a line of text serving to indicate what the passage below it is about 0.0000 mind head intellect psyche that which is responsible for your thoughts and feelings 0.5428 head the upper or front part of the body that contains the faee and brains 0.0000 toilet lavatory can head facility john privy bathroom 0.0000 head the striking part of a tool hammerhead 0.1685 head a part that projects out from the rest the head of the nail pinhead 0.0000 drumhead head stretched taut 0.0000 oral sex head oral-genital stimulation Word ` body ' -LRB- 8 alternatives -RRB- 0.0000 body an individual 3-dimensional object that has mass 0.0000 gathering assemblage assembly body confluence group of people together in one place 0.0000 body people associated by some common tie or occupation 0.0000 body the centralmessage of a communication 0.9178 torso trunk body subconcept of body part member 0.0000 body organic structure the entire physical structure of an animal or human being 60 0.0822 consistency consistence body subeoncept of property 0.0000 fuselage body the central portion of an airplane Word ` hands ' 0.0000 0.0653 0.0653 0.0000 0.0000 0.0000 0.2151 0.7196 0.0000 0.0000 -LRB- 10 alternatives -RRB- hand subconeept of linear unit hired hand hand hired man a hired laborer on a farm or ranch bridge player hand we need a 4th hand for bridge hand deal the cards held in a card game by a given player at any given time hand a round of applause to signify approval give the little lady a great big hand handwriting cursive hand script something written by hand hand ability he wanted to try his hand at singing hand manus hook mauler mitt paw the distal extremity of the superior limb hand subconcept of pointer hand physical assistance give me a hand with the chores Word ` eye ' -LRB- 4 alternatives -RRB- 0.1479 center centre middle heart eye approximately central within some region 0.1547 eye good judgment she has an eye for fresh talent 0.6432 eye eyeball oculus optic peeper organ of sight 0.0542 eye a sanall hole or loop -LRB- as in a needle -RRB- Word ` voice ' -LRB- 7 alternatives -RRB- 0.0000 0.1414 0.1122 0.2029 0.3895 0.0000 0.1539 voice the relation of the subject of a verb to the action that the verb denotes spokesperson spokesman interpreter representative mouthpiece voice voice vocalization the sound made by the vibration of vocal folds articulation voice expressing in coherent verbal form I gave voice to my feelings part voice the melody carried by a particular voice or instrument in polyphonic music voice the ability to speak he lost his voice voice the distinctive sound of a person 's speech I recognized her voice Word ` arm ' -LRB- 6 alternatives -RRB- 0.0000 branch subdivision arm an administrative division a branch of Congress 0.6131 arm eornrnonly used to refer to the whole superior limb 0.0346 weapon arm weapon system used in fighting or hunting 0.2265 sleeve arm attached at armhole 0.1950 arm any proj ~ tion that is thought to resemble an arm the arm of the record player 0.0346 arm the part of an armchair that supports the elbow and forearm of a seated person Word ` seat ' -LRB- 6 alternatives -RRB- 0.0000 seat a city from which authority is exercised 0.0000 seat place a space reserved for sitting 0.7369 buttocks arse butt backside burn buns can 0.2631 seat covers the buttocks 0.0402 seat designed for sitting on 0.0402 seat where one sits Word ` hair ' -LRB- 5 0.0323 0.2313 1.0000 1.0000 1.0000 alternatives -RRB- hair pilus threadlike keratinous filaments growing from the skin of mammals hair tomentum filamentous hairlike growth on a plant hair follicular growth subeoncept of externalbody part hair mane head of hair hair on the head hair hairy covering of an animal or body part Word ` mouth ' -LRB- 5 alternatives -RRB- 0.0000 mouth the point where a stream issues into a larger body of water 0.0000 mouth an opening that resembles a mouth -LRB- as of a cave or a gorge -RRB- 0.0613 sass sassing baektalk lip mouth an impudent or insolent rejoinder 0.9387 mouth oral cavity subconcept of cavity body cavity bodily cavity 0.9387 mouth trap hole maw yap muzzle suout list includes informal terms for mouth This group was among classes hand-selected by Brown et al. as particularly interesting	advmod_interesting_particularly dep_interesting_as dep_Brown_al. nn_Brown_et agent_hand-selected_Brown vmod_classes_hand-selected dep_was_interesting prep_among_was_classes nsubj_was_group det_group_This rcmod_mouth_was amod_terms_informal prep_for_includes_mouth dobj_includes_terms nsubj_includes_list appos_mouth_suout conj_mouth_muzzle conj_mouth_yap conj_mouth_maw conj_mouth_hole conj_mouth_trap num_mouth_0.9387 dep_cavity_mouth amod_cavity_bodily nn_cavity_body appos_cavity_cavity conj_cavity_cavity prep_of_subconcept_cavity amod_cavity_oral dep_mouth_includes dep_mouth_subconcept appos_mouth_cavity num_mouth_0.9387 nn_mouth_rejoinder amod_mouth_insolent amod_mouth_impudent det_mouth_an conj_or_impudent_insolent dep_sassing_mouth conj_sassing_mouth conj_sassing_lip conj_sassing_baektalk appos_sass_sassing num_sass_0.0613 pobj_0.0613_gorge pobj_0.0613_cave prepc_as_of_0.0613_of det_gorge_a conj_or_cave_gorge det_cave_a dep_mouth_sass det_mouth_a dobj_resembles_mouth nsubj_resembles_that rcmod_opening_resembles det_opening_an num_mouth_0.0000 nn_mouth_water prep_of_body_mouth amod_body_larger det_body_a dep_issues_opening prep_into_issues_body nn_issues_stream det_issues_a dep_where_issues prep_point_where det_point_the num_mouth_0.0000 num_alternatives_5 dep_Word_point dobj_Word_mouth dep_Word_alternatives dobj_Word_mouth nsubj_Word_covering nn_part_body nn_part_animal det_part_an conj_or_animal_body prep_of_covering_part amod_covering_hairy nn_hair_head det_hair_the dep_hair_Word prep_on_hair_hair prep_of_head_hair appos_hair_head appos_hair_mane nn_hair_part amod_hair_externalbody dep_subeoncept_hair prep_of_subeoncept_hair dep_growth_subeoncept amod_growth_follicular appos_hair_growth nn_hair_plant det_hair_a prep_on_growth_hair amod_growth_hairlike amod_growth_filamentous dep_tomentum_growth nn_hair_mammals prep_of_skin_hair det_skin_the prep_from_growing_skin appos_filaments_tomentum vmod_filaments_growing amod_filaments_keratinous amod_filaments_threadlike dep_hair_filaments appos_hair_pilus nn_hair_hair num_alternatives_1.0000 dep_1.0000_1.0000 dep_1.0000_0.2313 number_1.0000_1.0000 dep_0.2313_0.0323 number_0.0323_5 dep_hair_alternatives nn_hair_Word dobj_sits_hair nsubj_sits_one advmod_sits_where dep_seat_sits num_seat_0.0402 prep_on_sitting_seat prepc_for_designed_sitting num_seat_0.0402 nn_seat_buttocks det_seat_the dep_covers_designed dobj_covers_seat nsubj_covers_city advmod_seat_0.2631 aux_seat_can dep_seat_arse dep_seat_buttocks dep_seat_sitting conj_arse_buns conj_arse_burn conj_arse_backside conj_arse_butt num_buttocks_0.7369 prep_for_reserved_seat vmod_space_reserved det_space_a dep_seat_space appos_seat_place num_seat_0.0000 dobj_exercised_seat auxpass_exercised_is nsubjpass_exercised_authority prep_from_exercised_which rcmod_city_exercised det_city_a num_seat_0.0000 num_alternatives_6 dep_seat_seat dep_seat_alternatives nn_seat_Word nn_seat_person amod_seat_seated det_seat_a prep_of_elbow_seat conj_and_elbow_forearm det_elbow_the dobj_supports_forearm dobj_supports_elbow nsubj_supports_that rcmod_armchair_supports det_armchair_an dep_part_covers prep_of_part_armchair det_part_the num_arm_0.0346 dep_arm_arm dep_arm_arm dep_arm_sleeve dep_arm_fighting nn_player_record det_player_the prep_of_arm_player det_arm_the det_arm_an dobj_resemble_arm aux_resemble_to xcomp_thought_resemble auxpass_thought_is nsubjpass_thought_that rcmod_tion_thought nn_tion_~ nn_tion_proj det_tion_any num_arm_0.1950 nn_arm_armhole dep_attached_tion prep_at_attached_arm dep_arm_attached num_sleeve_0.2265 nn_sleeve_hunting conj_or_fighting_arm conj_or_fighting_sleeve dep_used_part prep_in_used_arm dep_system_used nn_system_weapon nn_system_arm appos_weapon_system num_weapon_0.0346 nn_weapon_limb amod_weapon_superior amod_weapon_whole det_weapon_the prep_to_refer_weapon aux_refer_to xcomp_used_refer advmod_used_eornrnonly num_arm_0.6131 dep_branch_used dep_branch_arm prep_of_branch_Congress det_branch_a dep_division_branch amod_division_administrative det_division_an dep_branch_division appos_branch_arm appos_branch_subdivision num_branch_0.0000 nn_branch_arm num_alternatives_6 dep_arm_alternatives nn_arm_Word poss_voice_her dobj_recognized_voice nsubj_recognized_I poss_speech_person det_person_a prep_of_sound_speech amod_sound_distinctive det_sound_the dep_voice_sound poss_voice_his dep_lost_voice dobj_lost_voice nsubj_lost_he aux_speak_to vmod_ability_speak det_ability_the nn_voice_music amod_voice_polyphonic prep_in_instrument_voice conj_or_voice_instrument amod_voice_particular det_voice_a agent_carried_instrument agent_carried_voice dep_melody_ability vmod_melody_carried det_melody_the dep_feelings_melody dep_feelings_voice dep_feelings_part poss_feelings_my prep_to_gave_feelings dobj_gave_voice nsubj_gave_I dep_gave_verb nsubj_gave_that amod_form_verbal amod_form_coherent prep_in_expressing_form appos_articulation_voice dobj_folds_articulation nsubj_folds_vocal prepc_of_vibration_folds det_vibration_the agent_made_vibration dep_sound_expressing vmod_sound_made det_sound_the nn_voice_voice dep_spokesperson_sound appos_spokesperson_vocalization conj_spokesperson_voice conj_spokesperson_mouthpiece conj_spokesperson_representative conj_spokesperson_interpreter conj_spokesperson_spokesman dobj_denotes_spokesperson dep_verb_denotes det_verb_the rcmod_action_gave det_action_the prep_to_verb_action det_verb_a prep_of_subject_verb det_subject_the prep_of_relation_subject det_relation_the dep_voice_relation num_voice_0.1539 num_voice_0.1414 dep_0.1539_0.0000 number_0.0000_0.3895 dep_0.0000_0.2029 number_0.2029_0.1122 number_0.1414_0.0000 num_alternatives_7 dep_voice_voice dep_voice_alternatives nn_voice_Word det_needle_a pobj_in_needle pcomp_as_in dep_hole_voice prep_hole_as conj_or_hole_loop nn_hole_sanall det_hole_a num_eye_0.0542 nn_eye_sight prep_of_organ_eye dep_eye_loop dep_eye_hole appos_eye_organ conj_eye_peeper conj_eye_optic conj_eye_oculus conj_eye_eyeball num_eye_0.6432 amod_talent_fresh prep_for_eye_talent det_eye_an dobj_has_eye dobj_has_eye nsubj_has_she amod_judgment_good num_eye_0.1547 nn_eye_region det_region_some dep_central_judgment prep_within_central_eye advmod_central_approximately dep_center_central appos_center_eye conj_center_heart conj_center_middle conj_center_centre num_center_0.1479 num_alternatives_4 dep_eye_center appos_eye_alternatives nn_eye_Word dep_eye_chores det_chores_the det_hand_a prep_with_give_eye dobj_give_hand iobj_give_me amod_assistance_physical nn_hand_pointer dep_subconcept_assistance prep_of_subconcept_hand nn_hand_limb amod_hand_superior det_hand_the dep_extremity_subconcept prep_of_extremity_hand amod_extremity_distal det_extremity_the dep_hand_extremity conj_hand_paw conj_hand_mitt conj_hand_mauler conj_hand_hook conj_hand_manus poss_hand_his dobj_try_hand prep_at_try_singing dobj_try_hand aux_try_to dep_wanted_branch ccomp_wanted_recognized dep_wanted_lost dep_wanted_has dep_wanted_give xcomp_wanted_try nsubj_wanted_he dep_wanted_drumhead nn_hand_hand agent_written_hand dep_something_ability vmod_something_written dep_handwriting_something conj_handwriting_script conj_handwriting_hand conj_handwriting_cursive dep_handwriting_hired amod_hand_big amod_hand_great det_hand_a amod_lady_little det_lady_the dobj_give_hand iobj_give_lady dobj_signify_approval aux_signify_to vmod_round_signify prep_of_round_applause det_round_a nn_hand_time amod_hand_given det_hand_any amod_player_given det_player_a nn_game_card det_game_a agent_held_player prep_in_held_game dep_cards_round prep_at_cards_hand vmod_cards_held det_cards_the dep_hand_cards conj_hand_deal amod_hand_4th det_hand_a dobj_need_hand prep_for_need_bridge dobj_need_hand nsubj_need_we nn_player_bridge nn_player_ranch conj_or_farm_player det_farm_a dep_laborer_need appos_laborer_hand prep_on_laborer_player prep_on_laborer_farm amod_laborer_hired det_laborer_a dep_man_laborer dobj_hired_man appos_hand_hand dep_hired_give conj_hired_hired dobj_hired_hand nsubj_hired_subconeept amod_unit_linear prep_of_subconeept_unit num_hand_0.0000 poss_hand_hands dep_hand_Word num_alternatives_10 dep_0.0000_alternatives dep_0.0000_0.0000 dep_0.0000_0.0000 dep_0.0000_0.0653 dep_0.0000_0.7196 number_0.7196_0.2151 number_0.0000_0.0000 dep_0.0000_0.0000 number_0.0000_0.0653 number_0.0653_0.0000 nn_Word_airplane det_Word_an dep_portion_handwriting prep_of_portion_hand amod_portion_central det_portion_the appos_fuselage_body num_fuselage_0.0000 nn_fuselage_property dep_subeoncept_portion prep_of_subeoncept_fuselage num_consistency_0.0822 cop_consistency_being amod_consistency_human number_0.0822_60 dep_animal_subeoncept conj_or_animal_body conj_or_animal_consistence conj_or_animal_consistency det_animal_an prep_of_structure_body prep_of_structure_consistence prep_of_structure_consistency prep_of_structure_animal amod_structure_physical amod_structure_entire det_structure_the amod_structure_organic num_body_0.0000 nn_body_member nn_part_body dep_subconcept_structure conj_subconcept_structure conj_subconcept_body prep_of_subconcept_part appos_torso_body appos_torso_trunk num_torso_0.9178 nn_torso_communication det_torso_a dep_centralmessage_subconcept prep_of_centralmessage_torso det_centralmessage_the num_body_0.0000 nn_body_occupation conj_or_tie_body amod_tie_common det_tie_some agent_associated_body agent_associated_tie dep_people_centralmessage vmod_people_associated num_body_0.0000 nn_body_place num_body_one prep_in_together_body dep_group_people advmod_group_together prep_of_group_people dep_gathering_group conj_gathering_confluence conj_gathering_body conj_gathering_assembly conj_gathering_assemblage num_gathering_0.0000 nn_gathering_mass dobj_has_gathering nsubj_has_that rcmod_object_has amod_object_3-dimensional amod_object_individual det_object_an num_body_0.0000 nn_body_body num_alternatives_8 appos_body_alternatives nn_body_Word nn_body_stimulation amod_body_oral-genital dep_sex_object dep_sex_body appos_sex_head amod_sex_oral num_sex_0.0000 amod_sex_taut amod_sex_stretched dep_drumhead_sex appos_drumhead_head num_drumhead_0.0000 dep_drumhead_pinhead dep_pinhead_headway det_nail_the prep_of_head_nail det_head_the det_rest_the prep_from_projects_rest prt_projects_out nsubj_projects_that rcmod_part_projects det_part_a num_head_0.1685 dep_hammerhead_part dep_hammerhead_head prepc_that_hammerhead_responsible det_tool_a prep_of_part_tool amod_part_striking det_part_the num_head_0.0000 nn_head_bathroom dep_john_part conj_john_head conj_john_privy appos_facility_john nn_facility_head dep_can_facility appos_toilet_lavatory num_toilet_0.0000 nn_toilet_brains nn_toilet_faee det_toilet_the conj_and_faee_brains dobj_contains_toilet nsubj_contains_that rcmod_body_contains det_body_the prep_part_can prep_of_part_body amod_part_front amod_part_upper det_part_the conj_or_upper_front dep_head_part num_head_0.5428 dep_feelings_head conj_and_thoughts_feelings poss_thoughts_your prep_for_responsible_feelings prep_for_responsible_thoughts cop_responsible_is nsubj_responsible_which appos_mind_psyche conj_mind_intellect conj_mind_head num_mind_0.0000 quantmod_0.0000_about dep_is_mind nsubj_is_passage dobj_is_what prep_below_passage_it det_passage_the ccomp_indicate_is aux_indicate_to xcomp_serving_indicate dep_line_hammerhead vmod_line_serving prep_of_line_text det_line_a dobj_heading_head nsubj_heading_pointer num_pointer_0.0000 nn_pointer_arrow det_pointer_an prepc_of_end_heading num_end_one dep_mark_line prep_at_mark_end amod_mark_V-shaped det_mark_a num_point_0.0903 nn_point_advance dep_subconcept_mark conj_subconcept_hod conj_subconcept_point conj_subconcept_progression prep_of_subconcept_progress dep_headway_head dep_headway_subconcept appos_headway_head num_headway_0.0000 dep_headway_beginning advmod_pass_pretty nsubj_pass_a amod_juncture_difficult det_juncture_a dep_pass_juncture appos_pass_straits appos_pass_head num_pass_0.0000 dobj_begins_pass nsubj_begins_something advmod_begins_where rcmod_point_begins conj_or_point_place det_point_the dep_beginning_pass dep_beginning_place dep_beginning_point appos_beginning_source appos_beginning_root appos_beginning_origin appos_beginning_head num_beginning_0.1983 dep_beginning_cluster det_fist_the prep_of_head_fist det_head_the det_page_the prep_of_head_page det_head_the num_head_0.1983 nn_head_drugs dep_head_soft advmod_soft_usually dep_user_head prep_of_user_head det_user_a num_head_0.0000 nn_head_leader dep_subeoncept_user prep_of_subeoncept_head amod_dog_top dep_chief_subeoncept conj_chief_dog appos_head_chief num_head_0.0000 dep_authority_head nn_authority_executive dobj_has_authority nsubj_has_who rcmod_educator_has nn_teacher_head dep_principal_educator appos_principal_head appos_principal_teacher nn_principal_school nn_principal_principaL num_principal_0.0000 amod_principal_subconceptofupperbound conj_crown_top conj_crown_head conj_crown_summit conj_crown_peak num_crown_0.0000 num_alternatives_17 dep_head_crown dep_head_alternatives dep_Word_head vmod_head_Word appos_head_mouth conj_head_hair conj_head_seat conj_head_arm conj_head_voice conj_head_eye conj_head_hands conj_head_body num_al._1992 nn_al._et amod_al._Brown dep_cluster_head dep_cluster_principal dep_cluster_head dep_cluster_al. amod_cluster_Distributional nn_cluster_groupings amod_cluster_derived num_cluster_3.1 advmod_derived_Distributionally
W95-0105	J92-4003	o	61 Distributional cluster -LRB- Brown et al. 1992 -RRB- tie jacket suit Word ` tie ' -LRB- 7 alternatives -RRB- 0.0000 0.0000 0.0000 1.0000 0.0000 0.0000 0.0000 draw standoff tie stalemate affiliation association tie tie-up a social or business relationship tie crosstie sleeper subconcept of brace bracing necktie tie link linkup tie tie-in something that serves to join or link drawstring string tie cord used as a fastener tie tie beam used to prevent two rafters e.g. from spreading apart Word ` jacket ' -LRB- 4 alternatives -RRB- 0.0000 book jacket dust cover subeoncept of promotional material 0.0000 jacket crown jacket artificial crown fitted over a broken or decayed tooth 0.0000 jacket subconceptofwrapping wrap wrapper 1.0000 jacket a short coat Word ` suit ' -LRB- 4 alternatives -RRB- 0.0000 suit suing subconcept of entreaty prayer appeal 1.0000 suit suit of clothes subconcept of garment 0.0000 suit any of four sets of13 cards in a paek 0.0000 legal action action case lawsuit suit a judicial proceeding This cluster was derived by Brown et al. using a modification of their algorithm designed to uncover semantically sticky clusters	amod_clusters_sticky advmod_sticky_semantically dobj_uncover_clusters aux_uncover_to xcomp_designed_uncover poss_algorithm_their vmod_modification_designed prep_of_modification_algorithm det_modification_a dobj_using_modification nn_al._et nn_al._Brown xcomp_derived_using agent_derived_al. auxpass_derived_was nsubjpass_derived_cluster det_cluster_This rcmod_proceeding_derived amod_proceeding_judicial det_proceeding_a dep_action_proceeding conj_action_suit conj_action_lawsuit conj_action_case amod_action_legal num_action_0.0000 nn_action_paek det_action_a appos_cards_action prep_in_cards_action det_cards_any nn_of13_sets num_of13_four prep_of_any_of13 num_suit_0.0000 nn_suit_garment dep_subconcept_cards prep_of_subconcept_suit prep_of_suit_clothes num_suit_1.0000 nn_suit_appeal appos_entreaty_suit conj_entreaty_suit conj_entreaty_prayer dep_subconcept_subconcept prep_of_subconcept_entreaty dep_suing_subconcept vmod_suit_suing num_suit_0.0000 nn_suit_suit num_alternatives_4 appos_suit_alternatives nn_suit_Word nn_suit_coat amod_suit_short det_suit_a num_jacket_1.0000 nn_jacket_wrapper dep_subconceptofwrapping_suit conj_subconceptofwrapping_jacket conj_subconceptofwrapping_wrap dep_jacket_subconceptofwrapping num_jacket_0.0000 nn_jacket_tooth amod_jacket_decayed amod_jacket_broken det_jacket_a conj_or_broken_decayed prep_over_fitted_jacket nsubj_fitted_crown amod_crown_artificial appos_crown_jacket nn_crown_jacket num_crown_0.0000 nn_crown_material amod_crown_promotional dep_subeoncept_fitted prep_of_subeoncept_crown nn_cover_dust nn_jacket_book num_jacket_0.0000 dep_jacket_alternatives dep_jacket_jacket dep_jacket_from dep_jacket_used dep_jacket_cord dep_jacket_tie num_alternatives_4 amod_jacket_Word amod_jacket_spreading advmod_Word_apart num_rafters_two dobj_prevent_rafters aux_prevent_to dep_used_e.g. xcomp_used_prevent nn_beam_tie nn_tie_fastener det_tie_a prep_as_used_tie appos_cord_beam vmod_cord_used dep_drawstring_subeoncept conj_drawstring_cover conj_drawstring_jacket conj_drawstring_string dobj_join_drawstring conj_or_join_link aux_join_to xcomp_serves_link xcomp_serves_join nsubj_serves_that rcmod_something_serves nn_link_tie dep_necktie_something conj_necktie_tie-in conj_necktie_tie conj_necktie_linkup conj_necktie_link dobj_bracing_necktie vmod_brace_bracing prep_of_subconcept_brace dep_tie_subconcept conj_tie_sleeper conj_tie_crosstie nn_tie_relationship amod_tie_business amod_tie_social det_tie_a conj_or_social_business nn_affiliation_stalemate dep_draw_tie conj_draw_tie-up conj_draw_tie conj_draw_association conj_draw_affiliation conj_draw_tie conj_draw_standoff num_draw_0.0000 dep_draw_0.0000 dep_draw_0.0000 dep_draw_tie number_0.0000_0.0000 dep_0.0000_1.0000 number_1.0000_0.0000 number_0.0000_0.0000 num_alternatives_7 nn_Word_suit appos_tie_alternatives dep_tie_tie conj_tie_Word conj_tie_jacket num_al._1992 nn_al._et amod_al._Brown dep_cluster_draw appos_cluster_al. amod_cluster_Distributional num_cluster_61
W95-0105	J92-4003	o	Distributional cluster -LRB- Brown et al. 1992 -RRB- cost expense risk profitability deferral earmarks capstone cardinality mintage reseller Word ` cost ' -LRB- 2 alternatives -RRB- 0.5426 cost price terms damage the amount of money paid for something 0.4574 monetary value price cost the amount of money it would bring if sold Word ` expense ' -LRB- 2 alternatives -RRB- 1.0000 expense expenditure outlay outgo spending disbursal disbursement 0.0000 expense a detriment or sacrifice at the expense of Word ` risk ' -LRB- 2 alternatives -RRB- 0.6267 hazard jeopardy peril risk subconeept of danger 0.3733 risk peril danger subeonceptofventure Word ` profitability ' -LRB- 1 alternatives -RRB- 1.0000 profitableness profitability subconcept of advantage benefit usefulness Word ` deferral ' -LRB- 3 alternatives -RRB- 0.6267 abeyance deferral recess subconcept of inaction inactivity inactiveness 0.3733 postponement deferment deferral moratorium an agreed suspension of activity 0.3733 deferral subconeeptofpause wait Word ` earmarks ' -LRB- 2 alternatives -RRB- 0.2898 earmark identification mark on the ear of a domestic animal 0.7102 hallma.k trademark earmark a distinguishing characteristic or attribute Word ` capstone ' -LRB- 1 alternatives -RRB- 1.0000 capstone coping stone stretcher used at top of wall Word ` eardinality ' Not in WordNet Word ` mintage ' -LRB- 1 alternatives -RRB- 62 1.0000 coinage mintage specie metal money subconcept of cash Word ` reseller ' Not in WordNet This cluster was one presented by Brown et al. as a randomly-selected class rather than one hand-picked for its coherence	poss_coherence_its prep_for_hand-picked_coherence num_hand-picked_one conj_negcc_class_hand-picked amod_class_randomly-selected det_class_a dep_Brown_al. nn_Brown_et prep_as_presented_hand-picked prep_as_presented_class agent_presented_Brown vmod_one_presented cop_one_was nsubj_one_subconcept det_cluster_This nn_cluster_WordNet nn_reseller_Word nn_reseller_cash prep_in_subconcept_cluster neg_subconcept_Not prep_of_subconcept_reseller nn_money_metal dep_coinage_one appos_coinage_money appos_coinage_specie appos_coinage_mintage num_coinage_1.0000 number_1.0000_62 num_alternatives_1 dep_mintage_coinage dep_mintage_alternatives nn_mintage_Word nn_mintage_WordNet nn_eardinality_Word nn_eardinality_wall prep_of_top_eardinality prep_in_used_mintage neg_used_Not prep_at_used_top amod_stone_coping num_capstone_1.0000 dep_alternatives_used appos_alternatives_stretcher dep_alternatives_stone dep_alternatives_capstone num_alternatives_1 nn_capstone_Word nn_capstone_attribute dep_characteristic_alternatives conj_or_characteristic_capstone acomp_distinguishing_capstone acomp_distinguishing_characteristic vmod_a_distinguishing dobj_earmark_a nsubj_earmark_subconeeptofpause appos_hallma.k_trademark num_hallma.k_0.7102 dep_animal_hallma.k amod_animal_domestic det_animal_a prep_of_ear_animal det_ear_the prep_on_mark_ear nn_mark_identification nsubj_earmark_0.2898 num_alternatives_2 rcmod_earmarks_earmark dep_earmarks_alternatives nn_earmarks_Word nsubj_wait_earmarks dep_subconeeptofpause_mark dep_subconeeptofpause_wait num_deferral_0.3733 nn_deferral_activity dep_suspension_earmark prep_of_suspension_deferral amod_suspension_agreed det_suspension_an num_postponement_0.3733 nn_postponement_inactiveness dep_subconcept_suspension conj_subconcept_moratorium conj_subconcept_deferral conj_subconcept_deferment conj_subconcept_postponement conj_subconcept_inactivity prep_of_subconcept_inaction num_abeyance_0.6267 nn_abeyance_deferral num_alternatives_3 appos_deferral_alternatives nn_deferral_Word nn_deferral_usefulness dep_subconcept_subconcept conj_subconcept_recess conj_subconcept_deferral conj_subconcept_abeyance conj_subconcept_benefit prep_of_subconcept_advantage dep_profitableness_subconcept appos_profitableness_profitability num_profitableness_1.0000 nn_profitableness_profitability num_alternatives_1 appos_profitability_alternatives nn_profitability_Word nn_profitability_subeonceptofventure dep_danger_profitableness nn_danger_peril num_risk_0.3733 nn_risk_danger appos_subconeept_danger prep_of_subconeept_risk nn_risk_peril appos_hazard_risk appos_hazard_jeopardy num_hazard_0.6267 num_alternatives_2 dep_risk_subconeept dep_risk_hazard dep_risk_alternatives nn_risk_Word prep_at_risk_expense dep_risk_cluster prep_expense_of det_expense_the conj_or_detriment_sacrifice det_detriment_a num_expense_0.0000 nn_expense_disbursement num_expense_1.0000 nn_expense_expense num_alternatives_2 dep_expense_alternatives nn_expense_Word dobj_sold_expense mark_sold_if advcl_bring_sold aux_bring_would nsubj_bring_it dep_amount_sacrifice dep_amount_detriment appos_amount_expense appos_amount_disbursal appos_amount_spending appos_amount_outgo appos_amount_outlay appos_amount_expenditure rcmod_amount_bring prep_of_amount_money det_amount_the amod_value_monetary num_value_0.4574 nn_value_something prep_for_paid_value dep_amount_amount appos_amount_cost appos_amount_price vmod_amount_paid prep_of_amount_money det_amount_the dep_cost_amount appos_cost_damage appos_cost_terms appos_cost_price num_cost_0.5426 nn_cost_cost num_alternatives_2 nn_cost_Word amod_cost_reseller dep_cost_alternatives conj_cost_cost conj_cost_mintage conj_cost_cardinality conj_cost_capstone conj_cost_earmarks conj_cost_deferral conj_cost_profitability conj_cost_risk conj_cost_expense num_al._1992 nn_al._et amod_al._Brown dep_cluster_cost dep_cluster_al. amod_cluster_Distributional
W95-0105	J92-4003	o	5 Conclusions and Future Work The results of the evaluation are exlremely encouraging especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs -LRB- Hearst 1991 Cowie et al. 1992 -RRB-	num_Cowie_1992 nn_Cowie_al. nn_Cowie_et dep_Hearst_Cowie amod_Hearst_1991 appos_homographs_Hearst prep_of_level_homographs det_level_the prep_to_disambiguation_level prep_than_difficult_disambiguation advmod_difficult_more cop_difficult_is csubj_difficult_disambiguating mark_difficult_that npadvmod_more_bit det_bit_a advmod_bit_quite prep_in_found_WordNet vmod_fine-grainedness_found prep_of_level_fine-grainedness det_level_the nn_senses_word prep_to_disambiguating_level dobj_disambiguating_senses ccomp_considering_difficult advmod_considering_especially xcomp_encouraging_considering advmod_encouraging_exlremely cop_encouraging_are nsubj_encouraging_results dep_encouraging_Work dep_encouraging_Conclusions det_evaluation_the prep_of_results_evaluation det_results_The amod_Work_Future conj_and_Conclusions_Work num_Conclusions_5 ccomp_``_encouraging
W95-0105	J92-4003	o	-LRB- Bensch and Savitch 1992 Brill 1991 Brown et al. 1992 Grefenstette 1994 McKcown and Hatzivassiloglou 1993 Pereira et al. 1993 Schtltze 1993 -RRB- -RRB-	dep_Schtltze_1993 num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_McKcown_1993 conj_and_McKcown_Hatzivassiloglou num_Grefenstette_1994 num_Brown_1992 nn_Brown_al. nn_Brown_et num_Brill_1991 dep_Bensch_Schtltze dep_Bensch_Pereira dep_Bensch_Hatzivassiloglou dep_Bensch_McKcown dep_Bensch_Grefenstette dep_Bensch_Brown dep_Bensch_Brill dep_Bensch_1992 conj_and_Bensch_Savitch dep_''_Savitch dep_''_Bensch
W96-0103	J92-4003	o	2 Hierarchical Clustering of Words Several algorithms have been proposed for automatically clustering words based on a large corpus -LRB- Jardino and Adda 91 Brown et al. 1992 Kneser and Ney 1993 Martin et al. 1995 Ueberla 1995 -RRB-	num_Ueberla_1995 num_al._1995 dep_Martin_al. nn_Martin_et num_Ney_1993 nn_1992_al. num_Brown_1992 nn_Brown_et num_Adda_91 appos_Jardino_Ueberla appos_Jardino_Martin conj_and_Jardino_Ney conj_and_Jardino_Kneser conj_and_Jardino_Brown conj_and_Jardino_Adda dep_corpus_Ney dep_corpus_Kneser dep_corpus_Brown dep_corpus_Adda dep_corpus_Jardino amod_corpus_large det_corpus_a pobj_words_corpus prepc_based_on_words_on amod_words_clustering advmod_clustering_automatically prep_for_proposed_words auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Clustering amod_algorithms_Several nn_algorithms_Words prep_of_Clustering_algorithms amod_Clustering_Hierarchical num_Clustering_2 ccomp_``_proposed
W96-0103	J92-4003	o	The reader is referred to -LRB- Ushioda 1996 -RRB- and -LRB- Brown et al. 1992 -RRB- for details of MI clustering but we will first briefly summarize the MI clustering and then describe our hierarchical clustering algorithm	nn_algorithm_clustering amod_algorithm_hierarchical poss_algorithm_our dobj_describe_algorithm advmod_describe_then nsubj_describe_we nn_clustering_MI det_clustering_the conj_and_summarize_describe dobj_summarize_clustering advmod_summarize_briefly advmod_summarize_first aux_summarize_will nsubj_summarize_we nn_clustering_MI prep_of_details_clustering dep_1992_al. nn_al._et num_Brown_1992 conj_and_Ushioda_Brown num_Ushioda_1996 conj_but_referred_describe conj_but_referred_summarize prep_for_referred_details prep_to_referred_Brown prep_to_referred_Ushioda auxpass_referred_is nsubjpass_referred_reader det_reader_The
W96-0213	J92-4003	o	However the aforementioned SDT techniques require word classes -LRB- Brown et al. 1992 -RRB- to help prevent data fragmentation and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs	nsubj_occurs_that rcmod_fragmentation_occurs det_fragmentation_any prep_of_effects_fragmentation det_effects_the dobj_mitigate_effects aux_mitigate_to vmod_algorithm_mitigate amod_algorithm_smoothing amod_algorithm_sophisticated det_algorithm_a nn_fragmentation_data dobj_prevent_fragmentation ccomp_help_prevent aux_help_to amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_classes_Brown nn_classes_word conj_and_require_algorithm vmod_require_help dobj_require_classes nsubj_require_techniques advmod_require_However nn_techniques_SDT amod_techniques_aforementioned det_techniques_the
W97-0105	J92-4003	o	In all other respects our work departs from previous research on broad -- coverage 16 I t I I I I I i I i I I I I I I I I I I I i I 1 I. I I I I i I 1 I I I I probabilistic parsing which either attempts to learn to predict gr ~ rarn ~ tical structure of test data directly from a training treebank -LRB- Brill 1993 Collins 1996 Eisner 1996 Jelinek et al. 1994 Magerman 1995 S ~ kine and Orishman 1995 Sharman et al. 1990 -RRB- or employs a grammar and sometimes a dictionary to capture linguistic expertise directly -LRB- Black et al. 1993a GrinBerg et al. 1995 Schabes 1992 -RRB- but arguably at a less detailed and informative level than in the research reported here	advmod_reported_here vmod_research_reported det_research_the pobj_in_research pcomp_than_in prep_level_than amod_level_informative amod_level_detailed det_level_a conj_and_detailed_informative advmod_detailed_less pobj_at_level advmod_at_arguably dep_Schabes_1992 num_al._1995 nn_al._et nn_al._GrinBerg dep_al._Schabes dep_al._al. appos_al._1993a nn_al._et amod_al._Black amod_expertise_linguistic dep_capture_al. advmod_capture_directly dobj_capture_expertise aux_capture_to vmod_dictionary_capture det_dictionary_a advmod_dictionary_sometimes conj_and_grammar_dictionary det_grammar_a dobj_employs_dictionary dobj_employs_grammar nsubj_employs_work num_Sharman_1990 nn_Sharman_al. nn_Sharman_et conj_and_kine_Orishman nn_kine_~ nn_kine_S dep_Magerman_1995 num_Jelinek_1994 nn_Jelinek_al. nn_Jelinek_et num_Eisner_1996 num_Collins_1996 dep_Brill_Sharman dep_Brill_1995 dep_Brill_Orishman dep_Brill_kine dep_Brill_Magerman dep_Brill_Jelinek dep_Brill_Eisner dep_Brill_Collins dep_Brill_1993 appos_treebank_Brill nn_treebank_training det_treebank_a nn_data_test prep_of_structure_data amod_structure_tical nn_structure_~ nn_structure_rarn nn_structure_~ nn_structure_gr prep_from_predict_treebank advmod_predict_directly dobj_predict_structure aux_predict_to xcomp_learn_predict aux_learn_to xcomp_attempts_learn preconj_attempts_either nsubj_attempts_which rcmod_parsing_attempts amod_parsing_probabilistic dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_1 dep_parsing_I dep_parsing_i dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I nn_parsing_I. dep_parsing_I nn_parsing_i dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I nn_parsing_i dep_parsing_I dep_parsing_coverage amod_parsing_broad num_I_1 dep_i_I dep_i_I dep_i_I dep_i_I dep_i_I dep_i_t dep_i_I dep_i_16 dep_coverage_i prep_on_research_parsing amod_research_previous conj_but_departs_at conj_or_departs_employs prep_from_departs_research nsubj_departs_work prep_in_departs_respects poss_work_our amod_respects_other det_respects_all
W97-0105	J92-4003	o	For example the sets of tags and rule labels have been clustered by our team gr ~ mm ~ trian while a vocabulary of about 60,000 words has been clustered by machine -LRB- Brown et al. 1992 Ushioda ~ 1996a Ushioda 1996b -RRB-	appos_Ushioda_1996b num_1996a_~ nn_1996a_Ushioda dep_al._Ushioda conj_al._1996a dep_al._1992 nn_al._et amod_al._Brown dep_machine_al. agent_clustered_machine auxpass_clustered_been aux_clustered_has nsubjpass_clustered_vocabulary mark_clustered_while num_words_60,000 quantmod_60,000_about prep_of_vocabulary_words det_vocabulary_a dep_trian_clustered nn_trian_~ nn_trian_mm nn_~_gr nn_~_team poss_~_our dep_clustered_trian agent_clustered_~ auxpass_clustered_been aux_clustered_have nsubjpass_clustered_sets prep_for_clustered_example nn_labels_rule conj_and_tags_labels prep_of_sets_labels prep_of_sets_tags det_sets_the
W97-0127	J92-4003	o	The concept of mutual information taken from information theory was proposed as a measure of word association -LRB- Church 1990 Jelinek et al. 1990,1992 Dagan 1995 ;-RRB-	num_;-RRB-_1995 appos_Dagan_;-RRB- dep_al._Dagan dep_al._1990,1992 nn_al._et nn_al._Jelinek num_Church_1990 dep_association_Church nn_association_word prep_of_measure_association det_measure_a dep_proposed_al. prep_as_proposed_measure auxpass_proposed_was nsubjpass_proposed_concept nn_theory_information prep_from_taken_theory amod_information_mutual vmod_concept_taken prep_of_concept_information det_concept_The ccomp_``_proposed
W97-0210	J92-4003	o	Semantic classification programs -LRB- Brown et al. 1992 Hatzivassiloglou and McKeown 1993 Pereira et al. 1993 -RRB- use statistical information based on cooccurrence with appropriate marker words to partition a set of words into semantic groups or classes	conj_or_groups_classes amod_groups_semantic prep_into_set_classes prep_into_set_groups prep_of_set_words det_set_a dep_partition_set nn_words_marker amod_words_appropriate prep_with_cooccurrence_words prep_on_based_cooccurrence vmod_information_based amod_information_statistical prep_to_use_partition dobj_use_information nsubj_use_programs nn_al._et nn_al._Pereira num_Hatzivassiloglou_1993 conj_and_Hatzivassiloglou_McKeown amod_Brown_1993 dep_Brown_al. dep_Brown_McKeown dep_Brown_Hatzivassiloglou appos_Brown_1992 dep_Brown_al. nn_Brown_et dep_programs_Brown nn_programs_classification amod_programs_Semantic
W97-0211	J92-4003	o	Many authors claim that class-based methods are more robust against data sparseness problems -LRB- Dagan ,1994 -RRB- -LRB- Pereira 1993 -RRB- -LRB- Brown et al. ,1992 -RRB-	advmod_,1992_al. nn_al._et num_Brown_,1992 amod_Pereira_1993 num_Dagan_,1994 appos_problems_Dagan nn_problems_sparseness nn_problems_data prep_against_robust_problems advmod_robust_more cop_robust_are nsubj_robust_methods mark_robust_that amod_methods_class-based dep_claim_Brown dep_claim_Pereira ccomp_claim_robust nsubj_claim_authors amod_authors_Many ccomp_``_claim
W97-0213	J92-4003	o	-LRB- Brown et al. 1992 -RRB- -RRB-	amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_''_Brown
W97-0213	J92-4003	o	In contrast approaches to WSD attempt to take advantage of many different sources of information -LRB- e.g. see -LRB- McRoy 1992 Ng and Lee 1996 Bruce and Wiebe 1994 -RRB- -RRB- it seems possible to obtain benefit from sources ranging from local collocational clues -LRB- Yarowsky 1993 -RRB- to membership in semantically or topically related word classes -LRB- arowsky 1992 Resnik 1993 -RRB- to consistency of word usages within a discourse -LRB- Gale et al. 1992 -RRB- and disambignation seems highly lexically sensitive in effect requiring specialized disamhignators for each polysemous word	amod_word_polysemous det_word_each amod_disamhignators_specialized prep_for_requiring_word dobj_requiring_disamhignators vmod_effect_requiring advmod_sensitive_lexically advmod_sensitive_highly prep_in_seems_effect acomp_seems_sensitive nsubj_seems_disambignation amod_Gale_1992 dep_Gale_al. nn_Gale_et det_discourse_a nn_usages_word prep_of_consistency_usages dep_Resnik_1993 dep_arowsky_Resnik appos_arowsky_1992 appos_classes_arowsky nn_classes_word amod_classes_related advmod_related_topically advmod_related_semantically conj_or_semantically_topically prep_in_membership_classes amod_Yarowsky_1993 dep_clues_Yarowsky amod_clues_collocational amod_clues_local prep_within_ranging_discourse prep_to_ranging_consistency prep_to_ranging_membership prep_from_ranging_clues vmod_sources_ranging prep_from_benefit_sources dobj_obtain_benefit aux_obtain_to xcomp_seems_obtain acomp_seems_possible nsubj_seems_it dep_Ng_1994 conj_and_Ng_Wiebe conj_and_Ng_Bruce conj_and_Ng_1996 conj_and_Ng_Lee dep_McRoy_Wiebe dep_McRoy_Bruce dep_McRoy_1996 dep_McRoy_Lee dep_McRoy_Ng appos_McRoy_1992 dep_see_McRoy advmod_see_e.g. rcmod_sources_see prep_of_sources_information amod_sources_different amod_sources_many prep_of_take_sources dobj_take_advantage aux_take_to vmod_attempt_take nn_attempt_WSD conj_and_approaches_seems dep_approaches_Gale parataxis_approaches_seems prep_to_approaches_attempt prep_in_approaches_contrast
W97-0307	J92-4003	o	Their weights are calculated by deleted interpolation -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown dep_interpolation_al. amod_interpolation_deleted agent_calculated_interpolation auxpass_calculated_are nsubjpass_calculated_weights poss_weights_Their ccomp_``_calculated
W97-0307	J92-4003	o	-LRB- Cutting et al. 1992 Feldweg 1995 -RRB- -RRB- the tagger for grammatical functions works with lexical and contextual probability measures Pq -LRB- -RRB-	dep_Pq_-LRB- nn_Pq_measures nn_Pq_probability amod_Pq_contextual amod_Pq_lexical conj_and_lexical_contextual prep_with_works_Pq nsubj_works_tagger parataxis_works_Cutting amod_functions_grammatical prep_for_tagger_functions det_tagger_the dep_Feldweg_1995 conj_al._1992 nn_al._et dep_Cutting_Feldweg dobj_Cutting_al.
W97-0309	J92-4003	o	Aggregate models based on higher-order n-grams -LRB- Brown et al. 1992 -RRB- might be able to capture multi-word structures such as noun phrases	nn_phrases_noun prep_such_as_structures_phrases amod_structures_multi-word dobj_capture_structures aux_capture_to xcomp_able_capture cop_able_be aux_able_might nsubj_able_models amod_Brown_1992 dep_Brown_al. nn_Brown_et appos_n-grams_Brown amod_n-grams_higher-order prep_on_based_n-grams vmod_models_based nn_models_Aggregate
W97-0309	J92-4003	o	In Section 2 we examine aggregate Markov models or class-based bigram models -LRB- Brown et al. 1992 -RRB- in which the mapping from words to classes 81 is probabilistic	cop_probabilistic_is nsubj_probabilistic_mapping prep_in_probabilistic_which num_classes_81 prep_to_mapping_classes prep_from_mapping_words det_mapping_the rcmod_Brown_probabilistic dep_Brown_1992 dep_Brown_al. nn_Brown_et nn_models_bigram amod_models_class-based conj_or_models_models nn_models_Markov amod_models_aggregate dep_examine_Brown dobj_examine_models dobj_examine_models nsubj_examine_we prep_in_examine_Section num_Section_2
W97-0309	J92-4003	o	82 2 Aggregate Markov models In this section we consider how to construct classbased bigram models -LRB- Brown et al. 1992 -RRB-	amod_Brown_1992 dep_Brown_al. nn_Brown_et nn_models_bigram amod_models_classbased dobj_construct_models aux_construct_to advmod_construct_how ccomp_consider_construct nsubj_consider_we det_section_this dep_models_Brown rcmod_models_consider prep_in_models_section nn_models_Markov nn_models_Aggregate num_models_2 num_models_82 dep_``_models
W97-0309	J92-4003	o	Though several algorithms -LRB- Brown et al. 1992 Pereira Tishby and Lee 1993 -RRB- have been proposed 100 -LRB- 9o -LRB- 80 -LRB- 4O -LRB- 20 -LRB- 1000 goo 80 ~ 41111 2 @ 5 10 15 20 25 30 5 10 15 20 25 30 iteration of EM iteration of EM -LRB- a -RRB- -LRB- b -RRB- Figure 1 Plots of -LRB- a -RRB- training and -LRB- b -RRB- test perplexity versus number of iterations of the EM algorithm for the aggregate Markov model with C = 32 classes	num_classes_32 dep_=_classes amod_C_= prep_with_model_C nn_model_Markov nn_model_aggregate det_model_the pobj_for_model ccomp_,_for nn_algorithm_EM det_algorithm_the prep_of_iterations_algorithm prep_of_number_iterations conj_versus_perplexity_number nn_perplexity_test dep_b_number dep_b_perplexity conj_and_training_b det_training_a prep_of_Plots_b prep_of_Plots_training dep_Figure_Plots num_Figure_1 appos_EM_b appos_EM_a dep_iteration_Figure prep_of_iteration_EM nn_iteration_EM prep_of_iteration_iteration num_iteration_30 num_iteration_5 num_iteration_~ dep_30_25 dep_25_20 dep_20_15 number_15_10 number_5_30 number_5_25 dep_5_20 dep_5_5 dep_20_15 number_15_10 quantmod_5_@ dep_5_2 number_2_41111 number_~_80 dep_goo_iteration num_goo_1000 appos_20_goo dep_4O_20 appos_80_4O dep_9o_80 num_9o_100 dobj_proposed_9o auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Lee nsubjpass_proposed_Tishby nsubjpass_proposed_Pereira dep_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby parataxis_Brown_proposed amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_algorithms_Brown amod_algorithms_several pobj_Though_algorithms dep_``_Though
W97-0309	J92-4003	o	Our approach differs in important ways from the use of hidden Markov models -LRB- HMMs -RRB- for classbased language modeling -LRB- Jelinek et al. 1992 -RRB-	amod_Jelinek_1992 dep_Jelinek_al. nn_Jelinek_et nn_modeling_language amod_modeling_classbased appos_models_HMMs nn_models_Markov amod_models_hidden prep_for_use_modeling prep_of_use_models det_use_the prep_from_ways_use amod_ways_important dep_differs_Jelinek prep_in_differs_ways nsubj_differs_approach poss_approach_Our ccomp_``_differs
W97-0311	J92-4003	o	Several authors have used mutual information and similar statistics as an objective function for word clustering -LRB- Dagan et al. 1993 Brown et al. 1992 Pereira et al. 1993 Wang et al. 1996 -RRB- for automatic determination of phonemic baseforms -LRB- Lucassen & Mercer 1984 -RRB- and for language modeling for speech recognition -LRB- Ries ct al. 1996 -RRB-	dep_ct_1996 dep_ct_al. nn_ct_Ries nn_recognition_speech dep_modeling_ct prep_for_modeling_recognition nn_modeling_language pobj_for_modeling dep_Lucassen_1984 conj_and_Lucassen_Mercer appos_baseforms_Mercer appos_baseforms_Lucassen amod_baseforms_phonemic prep_of_determination_baseforms amod_determination_automatic num_Wang_1996 nn_Wang_al. nn_Wang_et num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Dagan_Wang dep_Dagan_Pereira dep_Dagan_Brown dep_Dagan_1993 dep_Dagan_al. nn_Dagan_et nn_clustering_word prep_for_function_clustering amod_function_objective det_function_an amod_statistics_similar conj_and_information_statistics amod_information_mutual conj_and_used_for prep_for_used_determination dep_used_Dagan prep_as_used_function dobj_used_statistics dobj_used_information aux_used_have nsubj_used_authors amod_authors_Several ccomp_``_for ccomp_``_used
W97-0311	J92-4003	o	In practice texts contain an enormous number of word sequences -LRB- Brown et al. 1992 -RRB- only a tiny fraction of which are NCCs and it takes considerable computational effort to induce each translation model	nn_model_translation det_model_each dobj_induce_model aux_induce_to amod_effort_computational amod_effort_considerable vmod_takes_induce dobj_takes_effort nsubj_takes_it conj_and_NCCs_takes cop_NCCs_are nsubj_NCCs_fraction prep_of_fraction_which amod_fraction_tiny det_fraction_a advmod_fraction_only amod_Brown_1992 dep_Brown_al. nn_Brown_et nn_sequences_word prep_of_number_sequences amod_number_enormous det_number_an dep_contain_takes dep_contain_NCCs dep_contain_Brown dobj_contain_number nsubj_contain_texts prep_in_contain_practice
W97-1003	J92-4003	o	Various methods are based on Mutual Information between classes see -LRB- Brown et al. 1992 McMahon and Smith 1996 Kneser and Ney 1993 Jardino and Adda 1993 Martin Liermann and Ney 1995 Ueberla 1995 -RRB-	conj_and_Smith_Liermann conj_and_Smith_Martin conj_and_Smith_1993 conj_and_Smith_Adda conj_and_Smith_Jardino amod_Smith_1993 conj_and_Smith_Ney conj_and_Smith_Kneser amod_Smith_1996 amod_Brown_1995 appos_Brown_Ueberla amod_Brown_1995 conj_and_Brown_Ney conj_and_Brown_Liermann conj_and_Brown_Martin conj_and_Brown_1993 conj_and_Brown_Adda conj_and_Brown_Jardino conj_and_Brown_Ney conj_and_Brown_Kneser conj_and_Brown_Smith conj_and_Brown_McMahon amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_see_Ney dep_see_Smith dep_see_McMahon dep_see_Brown prep_between_Information_classes amod_Information_Mutual dep_based_see prep_on_based_Information auxpass_based_are nsubjpass_based_methods amod_methods_Various ccomp_``_based
W97-1003	J92-4003	o	Another application of hard clustering methods -LRB- in particular bottom-up variants -RRB- is that they can also produce a binary tree which can be used for decision-tree based systems such as the SPATTER parser -LRB- Magerman 1995 -RRB- or the ATR Decision-Tree Part-OfSpeech Tagger -LRB- Black et al. 1992 Ushioda 1996 -RRB-	dep_Black_1996 appos_Black_Ushioda num_Black_1992 dep_Black_al. nn_Black_et appos_Tagger_Black nn_Tagger_Part-OfSpeech nn_Tagger_Decision-Tree nn_Tagger_ATR det_Tagger_the appos_Magerman_1995 conj_or_parser_Tagger dep_parser_Magerman nn_parser_SPATTER det_parser_the prep_such_as_systems_Tagger prep_such_as_systems_parser amod_systems_based amod_systems_decision-tree prep_for_used_systems auxpass_used_be aux_used_can nsubjpass_used_which rcmod_tree_used amod_tree_binary det_tree_a dobj_produce_tree advmod_produce_also aux_produce_can nsubj_produce_they mark_produce_that ccomp_is_produce prep_in_is_variants amod_variants_bottom-up amod_variants_particular dep_methods_is nn_methods_clustering amod_methods_hard prep_of_application_methods det_application_Another
W97-1006	J92-4003	o	Brown -LRB- Brown et al. 1992 -RRB- uses the same bigrams and by means of a greedy algorithm forms the hierarchical clusters of words	prep_of_clusters_words amod_clusters_hierarchical det_clusters_the dobj_forms_clusters prep_by_means_of_forms_algorithm nsubj_forms_Brown amod_algorithm_greedy det_algorithm_a amod_bigrams_same det_bigrams_the conj_and_uses_forms dobj_uses_bigrams nsubj_uses_Brown amod_Brown_1992 dep_Brown_al. nn_Brown_et appos_Brown_Brown
W98-1109	J92-4003	n	As with similar work -LRB- e.g. Brown et al 1992 -RRB- the size of the corpus makes preprocessing such as lemmatization POS tagging or partial parsing too costly	advmod_costly_too amod_parsing_partial nn_tagging_POS conj_or_lemmatization_parsing conj_or_lemmatization_tagging prep_such_as_preprocessing_parsing prep_such_as_preprocessing_tagging prep_such_as_preprocessing_lemmatization dep_makes_costly xcomp_makes_preprocessing nsubj_makes_size dep_makes_e.g. det_corpus_the prep_of_size_corpus det_size_the dep_al_1992 nn_al_et advmod_Brown_al pobj_e.g._Brown dep_work_makes amod_work_similar pobj_with_work pcomp_As_with dep_``_As
W98-1109	J92-4003	o	While Schiitze and Pedersen -LRB- 1993 -RRB- Brown et al -LRB- 1992 -RRB- and Futrelle and Gauch -LRB- 1993 -RRB- all demonstrate the ability of their systems to identify word similarity using clustering on the most frequently occurring words in their corpus only Grefenstette -LRB- 1992 -RRB- demonstrates his system by generating word similarities with respect to a set of target words	nn_words_target prep_of_set_words det_set_a prep_with_respect_to_similarities_set nn_similarities_word dobj_generating_similarities poss_system_his prepc_by_demonstrates_generating dobj_demonstrates_system nsubj_demonstrates_Grefenstette advcl_demonstrates_demonstrate appos_Grefenstette_1992 advmod_Grefenstette_only poss_corpus_their prep_in_words_corpus amod_words_occurring det_words_the advmod_occurring_frequently advmod_frequently_most prep_on_clustering_words dobj_using_clustering vmod_similarity_using nn_similarity_word dobj_identify_similarity aux_identify_to poss_systems_their vmod_ability_identify prep_of_ability_systems det_ability_the dobj_demonstrate_ability nsubj_demonstrate_Futrelle nsubj_demonstrate_al dep_demonstrate_Pedersen dep_demonstrate_Schiitze mark_demonstrate_While appos_Gauch_1993 conj_and_Futrelle_Gauch dep_al_all conj_and_al_Gauch conj_and_al_Futrelle appos_al_1992 nn_al_et nn_al_Brown dep_Schiitze_1993 conj_and_Schiitze_Pedersen
W98-1109	J92-4003	o	This is in contrast to work by researchers such as Schiitze and Pedersen -LRB- 1992 -RRB- Brown et al -LRB- 1992 -RRB- and Futrelle and Gauch -LRB- 1995 -RRB- where it is often the most frequent words in the lexicon which are clustered predominantly with the purpose of determining their grammatical classes	amod_classes_grammatical poss_classes_their dobj_determining_classes prepc_of_purpose_determining det_purpose_the auxpass_clustered_are nsubjpass_clustered_which rcmod_lexicon_clustered det_lexicon_the prep_in_words_lexicon amod_words_frequent det_words_the advmod_words_often cop_words_is nsubj_words_it advmod_words_where advmod_frequent_most appos_Gauch_1995 appos_al_1992 nn_al_et nn_al_Brown appos_Pedersen_1992 rcmod_Schiitze_words conj_and_Schiitze_Gauch conj_and_Schiitze_Futrelle conj_and_Schiitze_al conj_and_Schiitze_Pedersen prep_such_as_researchers_Gauch prep_such_as_researchers_Futrelle prep_such_as_researchers_al prep_such_as_researchers_Pedersen prep_such_as_researchers_Schiitze prep_with_work_purpose advmod_work_predominantly prep_by_work_researchers aux_work_to vmod_contrast_work prep_in_is_contrast nsubj_is_This
W98-1109	J92-4003	o	While previous researchers have used agglomerative nesting clustering -LRB- e.g. Brown et al -LRB- 1992 -RRB- Futrelle and Gauch -LRB- 1993 -RRB- -RRB- comparisons with our work are difficult to draw due to their use of the 1,000 commonest words from their respective corpora	amod_corpora_respective poss_corpora_their prep_from_words_corpora amod_words_commonest num_words_1,000 det_words_the prep_of_use_words poss_use_their prep_due_to_draw_use aux_draw_to xcomp_difficult_draw cop_difficult_are nsubj_difficult_comparisons dep_difficult_Gauch dep_difficult_Futrelle dep_difficult_e.g. poss_work_our prep_with_comparisons_work appos_Gauch_1993 conj_and_Futrelle_Gauch dep_al_1992 nn_al_et advmod_Brown_al amod_e.g._Brown dep_clustering_difficult amod_clustering_nesting amod_clustering_agglomerative dobj_used_clustering aux_used_have nsubj_used_researchers mark_used_While amod_researchers_previous advcl_``_used
W98-1109	J92-4003	o	In Brown et al -LRB- 1992 -RRB- the authors provide some sample subtrees resulting from such a 1,000-word clustering	amod_clustering_1,000-word det_clustering_a amod_clustering_such prep_from_resulting_clustering vmod_subtrees_resulting nn_subtrees_sample det_subtrees_some dobj_provide_subtrees nsubj_provide_authors prep_in_provide_Brown det_authors_the dep_al_1992 nn_al_et dep_Brown_al
W98-1113	J92-4003	o	Precursors to this work include -LRB- Pereira et al 1993 -RRB- -LRB- Brown et al. 1992 -RRB- -LRB- Brill & Kapur 1993 -RRB- -LRB- Jelinek 1990 -RRB- and -LRB- Brill et al 1990 -RRB- and as applied to child language acquisition -LRB- Finch & Chater 1992 -RRB-	dep_Finch_1992 conj_and_Finch_Chater nn_acquisition_language nn_acquisition_child dep_applied_Chater dep_applied_Finch prep_to_applied_acquisition mark_applied_as amod_Brill_1990 dep_Brill_al nn_Brill_et dep_Jelinek_1990 dep_Brill_1993 conj_and_Brill_Kapur dep_al._1992 nn_al._et advmod_Brown_al. conj_and_Pereira_applied conj_and_Pereira_Brill appos_Pereira_Jelinek appos_Pereira_Kapur appos_Pereira_Brill appos_Pereira_Brown amod_Pereira_1993 dep_Pereira_al nn_Pereira_et dep_include_applied dep_include_Brill dep_include_Pereira nsubj_include_Precursors det_work_this prep_to_Precursors_work
W98-1113	J92-4003	n	Clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes -LRB- Brown et al. 1992 -RRB- but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem particularly without prior knowledge of the item classification	nn_classification_item det_classification_the prep_of_knowledge_classification amod_knowledge_prior prep_without_problem_knowledge advmod_problem_particularly amod_problem_difficult amod_problem_ongoing det_problem_an cop_problem_is csubj_problem_determining nn_tree_cluster amod_tree_hierarchical det_tree_a prep_for_number_tree prep_of_number_classes amod_number_optimum det_number_the dobj_determining_number dep_al._1992 nn_al._et advmod_Brown_al. dep_syntactic_classes conj_and_syntactic_semantic prep_into_classification_semantic prep_into_classification_syntactic prep_of_classification_words det_classification_the advmod_well_fairly dep_work_Brown prep_for_work_classification advmod_work_well aux_work_to conj_but_shown_problem xcomp_shown_work advmod_shown_previously auxpass_shown_been aux_shown_have nsubjpass_shown_algorithms nn_algorithms_Clustering
W98-1113	J92-4003	o	The fact that information consisting of nothing more than bigrams can capture syntactic information about English has already been noted by -LRB- Brown et al. 1992 -RRB-	num_al._1992 nn_al._et amod_al._Brown dep_by_al. prep_noted_by auxpass_noted_been advmod_noted_already aux_noted_has nsubjpass_noted_information mark_noted_that prep_about_information_English amod_information_syntactic dobj_capture_information aux_capture_can nsubj_capture_bigrams mark_capture_than ccomp_more_capture amod_nothing_more prep_of_consisting_nothing vmod_information_consisting ccomp_fact_noted det_fact_The
W98-1117	J92-4003	o	In the Link Grammar framework -LRB- Lagerty et al. 1992 Della Pietra et al. 1994 -RRB- strictly local contexts are naturally combined with long-distance information coming from long-range trigrams	amod_trigrams_long-range prep_from_coming_trigrams vmod_information_coming amod_information_long-distance prep_with_combined_information advmod_combined_naturally auxpass_combined_are nsubjpass_combined_contexts dep_combined_Lagerty prep_in_combined_framework amod_contexts_local advmod_local_strictly dep_al._1994 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della nn_al._et dep_Lagerty_Pietra appos_Lagerty_1992 dep_Lagerty_al. nn_framework_Grammar nn_framework_Link det_framework_the rcmod_``_combined
W98-1122	J92-4003	o	Most clustering schemes -LRB- et.al 1992 Kneser and Ney 1993 Pereira et al. 1993 McCandless and Glass 1993 Bellegarda et al. 1996 Saul and Pereira 1997 -RRB- use the average entropy reduction to decide when two words fall into the same cluster	amod_cluster_same det_cluster_the prep_into_fall_cluster nsubj_fall_words advmod_fall_when num_words_two advcl_decide_fall aux_decide_to amod_reduction_entropy amod_reduction_average det_reduction_the vmod_use_decide dobj_use_reduction nsubj_use_schemes num_Bellegarda_1996 nn_Bellegarda_al. nn_Bellegarda_et num_McCandless_1993 conj_and_McCandless_Glass num_Pereira_1993 nn_Pereira_al. nn_Pereira_et dep_Kneser_1997 conj_and_Kneser_Pereira conj_and_Kneser_Saul conj_and_Kneser_Bellegarda conj_and_Kneser_Glass conj_and_Kneser_McCandless conj_and_Kneser_Pereira conj_and_Kneser_1993 conj_and_Kneser_Ney dep_et.al_Pereira dep_et.al_Saul dep_et.al_Bellegarda dep_et.al_McCandless dep_et.al_Pereira dep_et.al_1993 dep_et.al_Ney dep_et.al_Kneser conj_et.al_1992 appos_schemes_et.al nn_schemes_clustering amod_schemes_Most
W98-1207	J92-4003	o	-LRB- Cutting et al. 1992 Feldweg 1995 -RRB- -RRB- the tagger for grammatical functions works with lexical -LRB- 1 -RRB- Selbst besucht ADV VVPP himself visited hat Peter Sabine VAFIN NE NE has Peter Sabine ` Peter never visited Sabine himself ' l hie ADV never Figure 2 Example sentence and contextual probability measures PO -LRB- ' -RRB- depending on the category of a mother node -LRB- Q -RRB-	appos_node_Q nn_node_mother det_node_a prep_of_category_node det_category_the nn_PO_measures nn_PO_probability amod_PO_contextual conj_and_sentence_PO nn_sentence_Example pobj_Figure_category prepc_depending_on_Figure_on dep_Figure_PO dep_Figure_sentence num_Figure_2 neg_Figure_never dep_ADV_Figure nn_ADV_hie nn_ADV_l nn_ADV_Sabine npadvmod_Sabine_himself dobj_visited_ADV neg_visited_never nsubj_visited_Peter aux_visited_has parataxis_visited_works nn_Peter_Sabine nn_Peter_Peter nn_NE_NE nn_NE_VAFIN nn_NE_Sabine nn_NE_Peter nn_NE_hat dobj_visited_NE nsubj_visited_himself rcmod_VVPP_visited nn_VVPP_ADV nn_VVPP_besucht nn_VVPP_Selbst dep_VVPP_1 amod_VVPP_lexical prep_with_works_VVPP nsubj_works_tagger parataxis_works_Cutting amod_functions_grammatical prep_for_tagger_functions det_tagger_the dep_Feldweg_1995 conj_al._1992 nn_al._et dep_Cutting_Feldweg dobj_Cutting_al.
W98-1207	J92-4003	o	Their weights are calculated by deleted interpolation -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown dep_interpolation_al. amod_interpolation_deleted agent_calculated_interpolation auxpass_calculated_are nsubjpass_calculated_weights poss_weights_Their ccomp_``_calculated
W99-0617	J92-4003	o	-LRB- Black et al. 1992 Magerman 1994 -RRB- -RRB- and view the POS tags and word identities as two separate sources of information	prep_of_sources_information amod_sources_separate num_sources_two nn_identities_word nn_tags_POS det_tags_the dobj_view_tags dep_Magerman_1994 prep_as_Black_sources conj_and_Black_identities conj_and_Black_view dep_Black_Magerman appos_Black_1992 dep_Black_al. nn_Black_et dep_''_identities dep_''_view dep_''_Black
A97-1053	J93-1003	p	7Another related measure is Dunning -LRB- 1993 -RRB- 's likelihood ratio tests for binomial and multinomial distributions which are claimed to be effective even with very much smaller volumes of text than is necessary for other tests based on assumed normal distributions	amod_distributions_normal amod_distributions_assumed prep_on_based_distributions vmod_tests_based amod_tests_other prep_for_necessary_tests cop_necessary_is mark_necessary_than prep_of_volumes_text amod_volumes_smaller advmod_smaller_much advmod_much_very prep_with_effective_volumes advmod_effective_even cop_effective_be aux_effective_to advcl_claimed_necessary xcomp_claimed_effective auxpass_claimed_are nsubjpass_claimed_which rcmod_distributions_claimed amod_distributions_multinomial amod_distributions_binomial conj_and_binomial_multinomial prep_for_tests_distributions nn_tests_ratio nn_tests_likelihood poss_tests_Dunning appos_Dunning_1993 cop_Dunning_is nsubj_Dunning_measure amod_measure_related nn_measure_7Another
A97-1053	J93-1003	o	For example in the context of syntactic disambiguation Black -LRB- 1993 -RRB- and Magerman -LRB- 1995 -RRB- proposed statistical parsing models based-on decisiontree learning techniques which incorporated not only syntactic but also lexical/semantic information in the decision-trees	det_decision-trees_the prep_in_information_decision-trees amod_information_lexical/semantic amod_information_syntactic conj_and_syntactic_lexical/semantic preconj_syntactic_only neg_only_not dobj_incorporated_information nsubj_incorporated_which rcmod_techniques_incorporated nn_techniques_learning nn_techniques_decisiontree amod_techniques_based-on dep_models_techniques nn_models_parsing amod_models_statistical amod_models_proposed nn_models_Magerman appos_Magerman_1995 appos_Black_1993 conj_and_disambiguation_models conj_and_disambiguation_Black amod_disambiguation_syntactic prep_of_context_models prep_of_context_Black prep_of_context_disambiguation det_context_the pobj_in_context ccomp_,_in pobj_For_example dep_``_For
A97-1053	J93-1003	o	-LRB- ~ -LRB- e -RRB- = max -LRB- -LRB- fl f ~ -RRB- ~ e -RRB- -LRB- 23 -RRB- -LRB- 11 Y ~ -RRB- Now the problem of learning probabilistic subcategorization preference is stated as for every verb-noun collocation e in C estimating the probability distribution P -LRB- -LRB- fl 6Resnik -LRB- 1993 -RRB- applys the idea of the KL distance to measuring the association of a verb v and its object noun class c Our definition of ekt corresponds to an extension of Resnik 's association score which considers dependencies of more than one case-markers in a subcategorization frame	nn_frame_subcategorization det_frame_a num_case-markers_one quantmod_one_than mwe_than_more prep_in_dependencies_frame prep_of_dependencies_case-markers dobj_considers_dependencies nsubj_considers_which rcmod_score_considers nn_score_association poss_score_Resnik prep_of_extension_score det_extension_an prep_to_corresponds_extension nsubj_corresponds_definition dep_corresponds_c dep_corresponds_v dep_corresponds_verb nsubj_corresponds_a mark_corresponds_of prep_of_definition_ekt poss_definition_Our nn_c_class nn_c_noun nn_c_object poss_c_its conj_and_v_c rcmod_association_corresponds det_association_the dobj_measuring_association nn_distance_KL det_distance_the prep_of_idea_distance det_idea_the prepc_to_applys_measuring dobj_applys_idea nsubj_applys_6Resnik appos_6Resnik_1993 rcmod_fl_applys nn_P_distribution nn_P_probability det_P_the dep_estimating_fl dobj_estimating_P prep_in_collocation_C dep_collocation_e amod_collocation_verb-noun det_collocation_every conj_for_estimating pobj_for_collocation dep_as_for prep_stated_as auxpass_stated_is nsubjpass_stated_problem nn_preference_subcategorization amod_preference_probabilistic dobj_learning_preference prepc_of_problem_learning det_problem_the num_Y_~ num_Y_11 parataxis_23_stated advmod_23_Now dep_23_Y dep_e_23 amod_~_e nn_~_f parataxis_=_~ advmod_=_~ dep_=_fl dep_=_max dep_=_e dep_=_~
A97-2010	J93-1003	o	toilet/bathroom Since the word facility is the subject of employ and is modified by new in -LRB- 3 -RRB- we retrieve other words that appeared in the same contexts and obtain the following two groups of selectors -LRB- the log A column shows the likelihood ratios -LRB- Dunning 1993 -RRB- of these words in the local contexts -RRB- Subjects of employ with top-20 highest likelihood ratios word freq Iog k word freq ORG 64 50.4 plant 14 31.0 company 27 28.6 operation 8 23.0 industry 9 14.6 firm 8 13.5 pirate 2 12.1 unit 9 9.32 shift 3 8.48 postal service 2 7.73 machine 3 6.56 corporation 3 6.47 manufacturer 3 6.21 insurance company 2 6.06 aerospace 2 5.81 memory device 1 5.79 department 3 5.55 foreign office 1 5.41 enterprise 2 5.39 pilot 2 537 * ORG includes all proper names recognized as organizations 18 Modifiees of new with top-20 highest likelihood ratios word freq log k post 432 952.9 issue 805 902.8 product 675 888.6 rule 459 875.8 law 356 541.5 technology 237 382.7 generation 150 323.2 model 207 319.3 job 260 269.2 system 318 251.8 word freq log -RRB- ~ bonds 223 245.4 capital 178 241.8 order 228 236.5 version 158 223.7 position 236 207.3 high 152 201.2 contract 279 198.1 bill 208 194.9 venture 123 193.7 program 283 183.8 Since the similarity between Sense 1 of facility and the selectors is greater than that of other senses the word facility in -LRB- 3 -RRB- is tagged Sense The key innovation of our algorithm is that a polysemous word is disambiguated with past usages of other words	amod_words_other prep_of_usages_words amod_usages_past prep_with_disambiguated_usages auxpass_disambiguated_is nsubjpass_disambiguated_word mark_disambiguated_that amod_word_polysemous det_word_a ccomp_is_disambiguated nsubj_is_innovation poss_algorithm_our prep_of_innovation_algorithm amod_innovation_key det_innovation_The rcmod_Sense_is dep_tagged_Sense auxpass_tagged_is dep_tagged_3 advmod_tagged_in dep_facility_word det_word_the amod_senses_other prep_of_that_senses prep_than_greater_that cop_greater_is nsubj_greater_selectors det_selectors_the vmod_facility_tagged dep_facility_facility conj_and_facility_greater dep_facility_Sense prep_Sense_of num_Sense_1 prep_between_similarity_greater prep_between_similarity_facility det_similarity_the number_183.8_283 num_program_183.8 dep_193.7_program dep_123_193.7 num_venture_123 num_venture_194.9 dep_208_venture num_bill_208 num_bill_198.1 num_bill_279 nn_bill_contract num_bill_201.2 num_bill_152 amod_bill_high num_bill_207.3 nn_bill_position num_bill_223.7 num_bill_158 nn_bill_version num_bill_236.5 num_bill_228 nn_bill_order num_bill_241.8 num_bill_178 nn_bill_capital num_bill_245.4 num_bill_223 number_207.3_236 prep_since_bonds_similarity dep_bonds_bill nn_bonds_~ dep_bonds_log nn_bonds_system dep_bonds_269.2 nn_log_freq nn_log_word num_log_251.8 num_log_318 dep_260_bonds dep_job_260 dep_319.3_job dep_207_319.3 dep_model_207 dep_323.2_model dep_150_323.2 num_generation_150 dep_382.7_generation dep_237_382.7 dep_technology_237 dep_541.5_technology dep_356_541.5 dep_law_356 dep_875.8_law dep_459_875.8 dep_rule_459 dep_888.6_rule dep_675_888.6 dep_product_675 dep_902.8_product dep_805_902.8 dep_issue_805 dep_952.9_issue dep_432_952.9 num_post_432 nn_post_k appos_log_post nn_log_freq nn_log_word nn_ratios_likelihood amod_ratios_highest amod_ratios_top-20 dep_with_log pobj_with_ratios ccomp_''_with dep_``_new dep_Modifiees_of num_Modifiees_18 nn_Modifiees_organizations prep_as_recognized_Modifiees nsubj_recognized_names amod_names_proper det_names_all ccomp_includes_recognized nsubj_includes_shift dep_includes_9.32 nsubj_includes_9 dep_includes_unit dep_includes_12.1 nsubj_includes_2 dep_includes_pirate dep_includes_13.5 nsubj_includes_8 dep_includes_firm dep_includes_14.6 nsubj_includes_9 dep_includes_industry dep_includes_23.0 nsubj_includes_8 dep_includes_operation dep_includes_28.6 nsubj_includes_27 dep_includes_company dep_includes_31.0 nsubj_includes_14 dep_includes_plant dep_includes_50.4 nsubj_includes_64 dep_includes_freq dep_ORG_* num_ORG_537 nn_ORG_pilot num_ORG_5.39 num_ORG_2 nn_ORG_enterprise num_ORG_5.41 num_ORG_1 number_537_2 dep_office_ORG amod_office_foreign num_office_5.55 nn_office_department num_office_5.79 num_office_1 number_5.55_3 dep_device_office nn_device_memory amod_device_5.81 nn_device_aerospace number_5.81_2 dep_6.06_device dep_2_6.06 dep_company_2 nn_company_insurance num_company_6.21 dep_3_company dep_manufacturer_3 num_manufacturer_6.47 num_manufacturer_3 nn_manufacturer_corporation num_manufacturer_6.56 num_manufacturer_3 nn_manufacturer_machine num_manufacturer_7.73 num_manufacturer_2 dep_service_manufacturer amod_service_postal num_service_8.48 number_8.48_3 dep_shift_service nn_ORG_freq nn_ORG_word nn_ORG_k conj_freq_ORG conj_freq_Iog nn_freq_word parataxis_ratios_includes nn_ratios_likelihood amod_ratios_highest amod_ratios_top-20 prep_with_employ_ratios dep_employ_Subjects prep_Subjects_of amod_contexts_local det_contexts_the det_words_these dep_Dunning_1993 prep_in_ratios_contexts prep_of_ratios_words dep_ratios_Dunning nn_ratios_likelihood det_ratios_the dobj_shows_ratios nsubj_shows_column nn_column_A nn_column_log det_column_the rcmod_groups_shows prep_of_groups_selectors num_groups_two dep_the_employ prep_following_the_groups dobj_obtain_the nsubj_obtain_that amod_contexts_same det_contexts_the conj_and_appeared_obtain prep_in_appeared_contexts nsubj_appeared_that rcmod_words_obtain rcmod_words_appeared amod_words_other dobj_retrieve_words nsubj_retrieve_we dep_retrieve_3 mark_retrieve_in dep_retrieve_modified dep_retrieve_subject prep_modified_by auxpass_modified_is dep_subject_new conj_and_subject_modified dep_subject_employ prep_subject_of det_subject_the cop_subject_is nn_subject_facility dep_facility_word mark_facility_Since dep_facility_toilet/bathroom det_word_the
C00-1029	J93-1003	o	The problem of choosing an appropria.te level in the h.ierarchy at which to represent a particular noun sense -LRB- given a predicate and argument position -RRB- has been investigated by Resnik -LRB- 1993 -RRB- Li and Abe -LRB- 1998 -RRB- and ll iba s -LRB- 1995 -RRB-	appos_s_1995 appos_Abe_1998 conj_and_Resnik_s conj_and_Resnik_iba conj_and_Resnik_ll conj_and_Resnik_Abe conj_and_Resnik_Li appos_Resnik_1993 agent_investigated_s agent_investigated_iba agent_investigated_ll agent_investigated_Abe agent_investigated_Li agent_investigated_Resnik auxpass_investigated_been aux_investigated_has nsubjpass_investigated_problem nn_position_argument nn_position_predicate det_position_a conj_and_predicate_argument pobj_given_position prep_sense_given nn_sense_noun amod_sense_particular det_sense_a dobj_represent_sense aux_represent_to prep_at_represent_which rcmod_h.ierarchy_represent det_h.ierarchy_the prep_in_level_h.ierarchy nn_level_appropria.te det_level_an dobj_choosing_level prepc_of_problem_choosing det_problem_The
C00-1047	J93-1003	p	For instance mutual information -LRB- Church ct al. 1990 -RRB- and the log-likelihood -LRB- Dunning 1993 -RRB- methods for extracting word bigrams have been widely used	advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_methods nsubjpass_used_information prep_for_used_instance nn_bigrams_word dobj_extracting_bigrams prepc_for_methods_extracting amod_methods_log-likelihood det_methods_the dep_Dunning_1993 dep_log-likelihood_Dunning nn_al._ct tmod_Church_1990 advmod_Church_al. conj_and_information_methods dep_information_Church amod_information_mutual
C00-1058	J93-1003	o	We adopted log-likelihood ratio -LRB- Danning 1993 -RRB- which gave the best pertbrmance among crude non-iterative methods in our test experiments 6	num_experiments_6 nn_experiments_test poss_experiments_our amod_methods_non-iterative amod_methods_crude amod_pertbrmance_best det_pertbrmance_the prep_in_gave_experiments prep_among_gave_methods dobj_gave_pertbrmance nsubj_gave_which dobj_Danning_1993 rcmod_ratio_gave dep_ratio_Danning amod_ratio_log-likelihood dobj_adopted_ratio nsubj_adopted_We ccomp_``_adopted
C00-1059	J93-1003	o	Mutual infornaation involves a problem in that it is overestimated for low-frequency terms -LRB- I -RRB- unning 1993 -RRB-	amod_1993_unning dep_1993_I num_terms_1993 nn_terms_low-frequency prep_for_overestimated_terms auxpass_overestimated_is nsubjpass_overestimated_it mark_overestimated_that ccomp_in_overestimated prep_problem_in det_problem_a dobj_involves_problem nsubj_involves_infornaation amod_infornaation_Mutual
C00-2100	J93-1003	o	Several techniques and results have been reported on learning subcategorization frames -LRB- SFs -RRB- from text corpora -LRB- Webster and Marcus 1989 Brent 1991 Brent 1993 Brent 1994 Ushioda et al. 1993 Manning 1993 Ersan and Charniak 1996 Briscoe and Carroll 1997 Carroll and Minnen 1998 Carroll and Rooth 1998 -RRB-	dep_1998_Rooth dep_1998_Carroll conj_and_Carroll_Rooth conj_and_Briscoe_Minnen conj_and_Briscoe_Carroll conj_and_Briscoe_1997 conj_and_Briscoe_Carroll conj_and_Ersan_Charniak num_Manning_1993 num_Ushioda_1993 nn_Ushioda_al. nn_Ushioda_et num_Brent_1994 num_Brent_1991 dep_Webster_1998 num_Webster_1998 dep_Webster_Minnen dep_Webster_Carroll dep_Webster_1997 dep_Webster_Carroll dep_Webster_Briscoe amod_Webster_1996 dep_Webster_Charniak dep_Webster_Ersan dep_Webster_Manning dep_Webster_Ushioda dep_Webster_Brent dep_Webster_1993 dep_Webster_Brent dep_Webster_Brent num_Webster_1989 conj_and_Webster_Marcus appos_corpora_Marcus appos_corpora_Webster nn_corpora_text appos_frames_SFs nn_frames_subcategorization prep_from_learning_corpora dobj_learning_frames prepc_on_reported_learning auxpass_reported_been aux_reported_have nsubjpass_reported_results nsubjpass_reported_techniques conj_and_techniques_results amod_techniques_Several
C00-2100	J93-1003	o	Using the values computed above Pl -7 tl k2 P2 -- = -77 2 kl + k2 p -7 z 1 \ -RSB- ` It 2 Taking these probabilities to be binomially distributed the log likelihood statistic -LRB- Dunning 1993 -RRB- is given by 2 log A = 2 \ -LSB- log L -LRB- pt k l rtl -RRB- @ log L -LRB- p2 k2 rl ,2 -RRB- log L -LRB- p kl n2 -RRB- log L -LRB- p k2 n2 -RRB- \ -RSB- where log L -LRB- p n k -RRB- = k logp + -LRB- z k -RRB- log -LRB- 1 p -RRB- According to this statistic tile greater the value of -2 log A for a particular pair of observed frame and verb the more likely that frame is to be valid SF of the verb	det_verb_the prep_of_SF_verb amod_SF_valid cop_SF_be aux_SF_to xcomp_is_SF nsubj_is_frame mark_is_that ccomp_likely_is advmod_likely_more det_likely_the amod_frame_observed prep_of_pair_frame amod_pair_particular det_pair_a nn_A_log num_A_-2 prep_for_value_pair prep_of_value_A det_value_the amod_value_greater nn_value_tile det_statistic_this num_p_1 appos_log_p dep_log_z dep_z_k nn_logp_k amod_logp_= nn_logp_L appos_p_k appos_p_n dep_L_p nn_L_log nn_\_L appos_p_n2 appos_p_k2 dep_L_p nn_L_log nn_L_L appos_p_n2 appos_p_kl dep_L_p nn_L_log nn_,2_rl appos_p2_,2 appos_p2_k2 dep_L_p2 nn_L_log appos_l_rtl dep_pt_l appos_pt_k dep_L_\ prep_@_L_L dep_L_pt nn_L_log number_\_2 num_=_\ nsubj_=_A dep_=_2 nsubj_=_It dep_=_\ dep_=_1 dep_=_2 dep_=_-77 nn_A_log prep_given_by auxpass_given_is nsubjpass_given_2 amod_Dunning_1993 dep_statistic_Dunning nn_statistic_likelihood nn_statistic_log det_statistic_the advmod_distributed_binomially auxpass_distributed_be aux_distributed_to det_probabilities_these vmod_Taking_distributed dobj_Taking_probabilities appos_2_statistic vmod_2_Taking rcmod_It_given dep_1_z dep_1_-7 dep_1_p dep_1_k2 conj_+_-77_1 dep_-77_kl conj_+_-77_2 dep_=_L dep_=_= appos_P2_likely conj_and_P2_verb conj_+_P2_value pobj_P2_statistic prepc_according_to_P2_to conj_+_P2_log conj_+_P2_logp dep_P2_where dep_P2_= nn_P2_k2 nn_P2_tl num_P2_-7 nn_P2_Pl advmod_computed_above nsubj_computed_values det_values_the dep_Using_verb dep_Using_value dep_Using_log dep_Using_logp dep_Using_P2 ccomp_Using_computed ccomp_``_Using
C00-2100	J93-1003	o	5 Comparison with related work Preliminary work on SF extraction from coq ~ ora was done by -LRB- Brent 1991 Brunt 1993 Brent 1994 -RRB- and -LRB- Webster and Marcus 1989 Ushioda et al. 1993 -RRB-	num_Ushioda_1993 nn_Ushioda_al. nn_Ushioda_et dep_Webster_Ushioda num_Webster_1989 conj_and_Webster_Marcus num_Brent_1994 num_Brunt_1993 conj_1991_Brent conj_1991_Brunt dep_1991_Brent dep_by_1991 conj_and_done_Marcus conj_and_done_Webster prep_done_by auxpass_done_was nsubjpass_done_Comparison num_ora_~ nn_ora_coq nn_extraction_SF prep_from_work_ora prep_on_work_extraction amod_work_Preliminary nn_work_work amod_work_related prep_with_Comparison_work num_Comparison_5
C02-1007	J93-1003	o	However Dunning -LRB- 1993 -RRB- pointed out that for the purpose of corpus statistics where the sparseness of data is an important issue it is better to use the log-likelihood ratio	amod_ratio_log-likelihood det_ratio_the dobj_use_ratio aux_use_to xcomp_better_use cop_better_is nsubj_better_it prep_for_better_purpose mark_better_that amod_issue_important det_issue_an cop_issue_is nsubj_issue_sparseness advmod_issue_where prep_of_sparseness_data det_sparseness_the rcmod_statistics_issue nn_statistics_corpus prep_of_purpose_statistics det_purpose_the ccomp_pointed_better prt_pointed_out nsubj_pointed_Dunning advmod_pointed_However appos_Dunning_1993
C02-1016	J93-1003	o	In the first step the scores are initialized according to the G 2 statistic -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_statistic_Dunning num_statistic_2 nn_statistic_G det_statistic_the pobj_initialized_statistic prepc_according_to_initialized_to auxpass_initialized_are nsubjpass_initialized_scores prep_in_initialized_step det_scores_the amod_step_first det_step_the ccomp_``_initialized
C02-1040	J93-1003	o	We describe the experiment in greater detail 2The particular verbs selected were looked up in -LRB- Levin 1993 -RRB- and the class for each verb in the classification system defined in -LRB- Stevenson and Merlo 1997 -RRB- was selected with some discussion with linguists	prep_with_discussion_linguists det_discussion_some prep_with_selected_discussion auxpass_selected_was csubjpass_selected_looked amod_Stevenson_1997 conj_and_Stevenson_Merlo prep_in_defined_Merlo prep_in_defined_Stevenson vmod_system_defined nn_system_classification det_system_the prep_in_verb_system dep_verb_each prep_for_class_verb det_class_the conj_and_Levin_class amod_Levin_1993 prep_in_looked_class prep_in_looked_Levin prt_looked_up auxpass_looked_were vmod_verbs_selected amod_verbs_particular nn_verbs_2The nn_verbs_detail amod_verbs_greater det_experiment_the vmod_describe_selected prep_in_describe_verbs dobj_describe_experiment nsubj_describe_We
C02-1040	J93-1003	o	For example in John saw Mary yesterday at the station only John and Mary are required arguments while the other constituents are optional -LRB- adjuncts -RRB- .3 The problem of SF identification using statistical methods has had a rich discussion in the literature -LRB- Ushioda et al. 1993 Manning 1993 Briscoe and Carroll 1997 Brent 1994 -RRB- -LRB- also see the refences cited in -LRB- Sarkar and Zeman 2000 -RRB- -RRB-	dep_Sarkar_2000 conj_and_Sarkar_Zeman dep_in_Zeman dep_in_Sarkar prep_cited_in nsubj_cited_refences det_refences_the ccomp_see_cited advmod_see_also dep_Brent_1994 dep_Briscoe_see dep_Briscoe_Brent conj_and_Briscoe_1997 conj_and_Briscoe_Carroll dep_Manning_1997 dep_Manning_Carroll dep_Manning_Briscoe num_Manning_1993 dep_Ushioda_Manning appos_Ushioda_1993 dep_Ushioda_al. nn_Ushioda_et det_literature_the prep_in_discussion_literature amod_discussion_rich det_discussion_a dobj_had_discussion aux_had_has nsubj_had_problem amod_methods_statistical dobj_using_methods nn_identification_SF vmod_problem_using prep_of_problem_identification det_problem_The dep_.3_Ushioda rcmod_.3_had dep_.3_adjuncts ccomp_optional_.3 cop_optional_are nsubj_optional_constituents mark_optional_while amod_constituents_other det_constituents_the advcl_required_optional dobj_required_arguments auxpass_required_are nsubjpass_required_Mary nsubjpass_required_John conj_and_John_Mary advmod_John_only det_station_the parataxis_saw_required prep_at_saw_station tmod_saw_yesterday dobj_saw_Mary prep_in_saw_John prep_for_saw_example
C02-1040	J93-1003	o	ther background on this method of hypothesis testing the reader is referred to -LRB- Bickel and Doksum 1977 Dunning 1993 -RRB-	amod_Dunning_1993 dep_Bickel_Dunning conj_and_Bickel_1977 conj_and_Bickel_Doksum prep_to_referred_1977 prep_to_referred_Doksum prep_to_referred_Bickel auxpass_referred_is nsubjpass_referred_reader advmod_referred_ther det_reader_the nn_reader_background nn_testing_hypothesis prep_of_method_testing det_method_this prep_on_background_method ccomp_``_referred
C02-1040	J93-1003	o	Using the values computed above p1 = k1n 1 p2 = k2n 2 p = k1 + k2n 1 + n2 Taking these probabilities to be binomially distributed the log likelihood statistic -LRB- Dunning 1993 -RRB- is given by 2 log = 2 -LSB- log L -LRB- p1 k1 n1 -RRB- + log L -LRB- p2 k2 n2 -RRB- log L -LRB- p k1 n2 -RRB- log L -LRB- p k2 n2 -RRB- -RSB- where log L -LRB- p n k -RRB- = k log p + -LRB- n k -RRB- log -LRB- 1 p -RRB- According to this statistic the greater the value of 2 log for a particular pair of observed frame and verb the more likely that frame is to be valid SF of the verb	det_verb_the prep_of_SF_verb amod_SF_valid cop_SF_be aux_SF_to xcomp_is_SF nsubj_is_frame mark_is_that ccomp_likely_is advmod_likely_more det_likely_the amod_frame_observed prep_of_pair_frame amod_pair_particular det_pair_a num_log_2 conj_and_value_verb prep_for_value_pair prep_of_value_log det_value_the dep_greater_verb dep_greater_value det_greater_the det_statistic_this num_p_1 pobj_log_statistic prepc_according_to_log_to appos_log_p nn_log_p advmod_log_where dep_log_L nn_k_n dep_p_k cc_p_+ nn_p_log nn_p_k amod_p_= dep_p_L dep_p_k dep_p_n dep_L_p nn_L_log dep_p_n2 dep_p_k2 dep_L_p nn_L_log nn_L_n2 appos_p_likely appos_p_greater parataxis_p_log conj_p_k1 nn_p_L dep_p_log dep_p_2 nn_log_log nn_L_log dep_p1_n1 dep_p1_k1 dep_L_n2 dep_L_k2 dep_L_p2 conj_+_L_L dep_L_p1 nn_L_log dep_=_2 dep_log_L dep_log_L amod_log_= dep_given_p prep_given_by auxpass_given_is nsubjpass_given_p1 amod_Dunning_1993 dep_statistic_Dunning nn_statistic_likelihood nn_statistic_log det_statistic_the advmod_distributed_binomially auxpass_distributed_be aux_distributed_to det_probabilities_these vmod_Taking_distributed dobj_Taking_probabilities vmod_1_Taking conj_+_1_n2 dep_k1_n2 dep_k1_1 conj_+_k1_k2n dep_=_k2n dep_=_k1 appos_p_statistic amod_p_= num_p_2 nn_p_k2n dep_=_p amod_p2_= num_p2_1 nn_p2_k1n dep_=_p2 amod_p1_= advmod_computed_above nsubj_computed_values det_values_the parataxis_Using_given ccomp_Using_computed rcmod_``_Using
C02-1065	J93-1003	o	As the strength of relevance between a target compound noun t and its co-occurring word r the feature value of r w -LRB- t r -RRB- is deflned by the log likelihood ratio -LRB- Dunning 1993 -RRB- 1 as follows	mark_follows_as dep_Dunning_follows num_Dunning_1 dep_Dunning_1993 rcmod_ratio_Dunning nn_ratio_likelihood nn_ratio_log det_ratio_the agent_deflned_ratio auxpass_deflned_is nsubjpass_deflned_value prep_as_deflned_strength dep_t_r appos_w_t appos_r_w prep_of_value_r nn_value_feature det_value_the nn_r_word amod_r_co-occurring poss_r_its conj_and_t_r nn_t_noun nn_t_compound nn_t_target det_t_a prep_between_relevance_r prep_between_relevance_t prep_of_strength_relevance det_strength_the advcl_``_deflned
C02-1125	J93-1003	p	For instance the mutual information -LRB- Church et al. 1990 -RRB- and log-likelihood ratio -LRB- Dunning 1993 Cohen 1995 -RRB- have been widely used for extracting word bigrams	nn_bigrams_word dobj_extracting_bigrams prepc_for_used_extracting advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_ratio nsubjpass_used_information prep_for_used_instance num_Cohen_1995 dep_Dunning_Cohen dobj_Dunning_1993 dep_ratio_Dunning amod_ratio_log-likelihood dep_1990_al. nn_al._et num_Church_1990 conj_and_information_ratio dep_information_Church amod_information_mutual det_information_the
C02-1125	J93-1003	o	The value of Dist -LRB- D -LRB- T -RRB- -RRB- can be defined in various ways and they found that using log-likelihood ratio -LRB- see Dunning 1993 -RRB- worked best which is represented as follows 0 # log -RRB- -LRB- # log D K k TD k k i M ii i i M ii i == where k i and K i are the frequency of a word w i in D -LRB- W -RRB- and D 0 respectively and -LCB- w 1 w M -RCB- is the set of all words in D 0 As stated in introduction Dist -LRB- D -LRB- T -RRB- -RRB- is normalized by the baseline function which is referred as B Dist -LRB- -RRB- here	advmod_Dist_here nn_Dist_B prep_as_referred_Dist auxpass_referred_is nsubjpass_referred_which rcmod_function_referred nn_function_baseline det_function_the agent_normalized_function auxpass_normalized_is nsubjpass_normalized_Dist advcl_normalized_stated appos_D_T dep_Dist_D prep_in_stated_introduction mark_stated_As num_D_0 prep_in_words_D det_words_all prep_of_set_words det_set_the cop_set_is nsubj_set_M dep_set_ii dep_set_M dep_set_log nn_M_w appos_w_M num_w_1 num_D_0 conj_and_D_D appos_D_W prep_in_i_D prep_in_i_D dep_w_i nn_w_word det_w_a advmod_frequency_respectively prep_of_frequency_w det_frequency_the cop_frequency_are nsubj_frequency_k advmod_frequency_where nn_i_K conj_and_i_i amod_k_i amod_k_i conj_and_==_w rcmod_==_frequency nn_==_i dep_ii_w dep_ii_== amod_M_ii nn_M_i nn_M_i nn_M_i dep_M_k nn_k_k nn_k_TD nn_k_k nn_k_K nn_k_D nn_k_log dep_k_# dep_log_# num_log_0 mark_follows_as advcl_represented_follows auxpass_represented_is nsubjpass_represented_which rcmod_best_represented dobj_worked_best csubj_worked_using mark_worked_that num_Dunning_1993 dobj_see_Dunning amod_ratio_log-likelihood dep_using_see dobj_using_ratio ccomp_found_worked nsubj_found_they amod_ways_various parataxis_defined_normalized parataxis_defined_set conj_and_defined_found prep_in_defined_ways auxpass_defined_be aux_defined_can nsubjpass_defined_value appos_D_T dep_Dist_D prep_of_value_Dist det_value_The
C02-1130	J93-1003	o	In order to avoid this problem we implemented a simple bootstrapping procedure in which a seed data set of 100 instances of each of the eight categories was hand tagged and used to generate a decision list classifier using the C4 .5 algorithm -LRB- Quinlan 1993 -RRB- with the word frequency and topic signature features described below	advmod_described_below dep_features_described nn_signature_topic conj_and_frequency_signature nn_frequency_word det_frequency_the amod_Quinlan_1993 dep_algorithm_Quinlan num_algorithm_.5 nn_algorithm_C4 det_algorithm_the prep_with_using_signature prep_with_using_frequency dobj_using_algorithm dep_classifier_features vmod_classifier_using dep_list_classifier nn_list_decision det_list_a dobj_generate_list aux_generate_to xcomp_used_generate conj_and_tagged_used cop_hand_was nsubj_hand_set prep_in_hand_which num_categories_eight det_categories_the prep_of_each_categories prep_of_instances_each num_instances_100 prep_of_set_instances nn_set_data nn_set_seed det_set_a rcmod_procedure_hand nn_procedure_bootstrapping amod_procedure_simple det_procedure_a dep_implemented_used dep_implemented_tagged dobj_implemented_procedure nsubj_implemented_we advcl_implemented_avoid det_problem_this dobj_avoid_problem aux_avoid_to dep_avoid_order mark_avoid_In
C02-1130	J93-1003	o	The topic signatures are automatically generated for each specific term by computing the likelihood ratio -LRB- score -RRB- between two hypotheses -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_hypotheses_Dunning num_hypotheses_two prep_between_ratio_hypotheses appos_ratio_score nn_ratio_likelihood det_ratio_the dobj_computing_ratio amod_term_specific det_term_each agent_generated_computing prep_for_generated_term advmod_generated_automatically auxpass_generated_are nsubjpass_generated_signatures nn_signatures_topic det_signatures_The ccomp_``_generated
C02-1130	J93-1003	o	The scores were then weighted by the inverse of their height in the tree and then summed together similarly to the procedure in -LRB- Resnik 1993 -RRB-	amod_Resnik_1993 dep_in_Resnik prep_procedure_in det_procedure_the prep_to_summed_procedure advmod_summed_similarly advmod_summed_together advmod_summed_then nsubjpass_summed_scores det_tree_the prep_in_height_tree poss_height_their prep_of_inverse_height det_inverse_the conj_and_weighted_summed agent_weighted_inverse advmod_weighted_then auxpass_weighted_were nsubjpass_weighted_scores det_scores_The ccomp_``_summed ccomp_``_weighted
C02-1130	J93-1003	o	Methods 4.1 Experiment 1 Held out data To examine the generalizability of classifiers trained on the automatically generated data a C4 .5 decision tree classifier -LRB- Quinlan 1993 -RRB- was trained and tested on the held out test set described above	advmod_described_above vmod_set_described nn_set_test amod_set_held det_set_the prt_held_out nsubjpass_tested_classifier prep_on_trained_set conj_and_trained_tested auxpass_trained_was nsubjpass_trained_classifier amod_Quinlan_1993 appos_classifier_Quinlan nn_classifier_tree nn_classifier_decision num_classifier_.5 nn_classifier_C4 det_classifier_a rcmod_data_tested rcmod_data_trained amod_data_generated det_data_the advmod_generated_automatically prep_on_trained_data vmod_classifiers_trained prep_of_generalizability_classifiers det_generalizability_the dobj_examine_generalizability aux_examine_To vmod_data_examine amod_data_Held prt_Held_out dep_Experiment_data num_Experiment_1 num_Experiment_4.1 dep_Methods_Experiment
C02-1166	J93-1003	o	Each word i in the context vector of w is then weighted with a measure of its association with w We chose the loglikelihood ratio test -LRB- Dunning 1993 -RRB- to measure this association the context vectors of the target words are then translated with our general bilingual dictionary leaving the weights unchanged -LRB- when several translations are proposed by the dictionary we consider all of them with the same weight -RRB- the similarity of each source word s for each target word t is computed on the basis of the cosine measure the similarities are then normalized to yield a probabilistic translation lexicon P -LRB- t | s -RRB-	num_s_| nn_s_t appos_P_s nn_lexicon_translation amod_lexicon_probabilistic det_lexicon_a dobj_yield_lexicon aux_yield_to xcomp_normalized_yield advmod_normalized_then auxpass_normalized_are nsubjpass_normalized_chose det_similarities_the dobj_measure_similarities det_cosine_the prep_of_basis_cosine det_basis_the dep_computed_measure prep_on_computed_basis auxpass_computed_is nsubjpass_computed_similarity dep_computed_proposed nn_t_word nn_t_target det_t_each nn_s_word nn_s_source det_s_each prep_for_similarity_t prep_of_similarity_s det_similarity_the amod_weight_same det_weight_the prep_of_all_them prep_with_consider_weight dobj_consider_all nsubj_consider_we det_dictionary_the parataxis_proposed_consider agent_proposed_dictionary auxpass_proposed_are nsubjpass_proposed_translations advmod_proposed_when amod_translations_several ccomp_unchanged_computed nsubj_unchanged_weights det_weights_the xcomp_leaving_unchanged amod_dictionary_bilingual amod_dictionary_general poss_dictionary_our xcomp_translated_leaving prep_with_translated_dictionary advmod_translated_then auxpass_translated_are nsubjpass_translated_vectors nn_words_target det_words_the prep_of_vectors_words nn_vectors_context det_vectors_the det_association_this ccomp_measure_translated dobj_measure_association aux_measure_to dep_Dunning_1993 appos_test_Dunning nn_test_ratio nn_test_loglikelihood det_test_the xcomp_chose_measure dobj_chose_test nsubj_chose_We prep_with_association_w poss_association_its prep_of_measure_association det_measure_a parataxis_weighted_P ccomp_weighted_normalized prep_with_weighted_measure advmod_weighted_then auxpass_weighted_is nsubjpass_weighted_word prep_of_vector_w nn_vector_context det_vector_the prep_in_i_vector dep_word_i det_word_Each ccomp_``_weighted
C02-2003	J93-1003	o	3.1 The Likelihood Ratio We adopted a method for collocation discovery based on the likelihood ratio -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_ratio_Dunning nn_ratio_likelihood det_ratio_the pobj_discovery_ratio prepc_based_on_discovery_on nn_discovery_collocation prep_for_method_discovery det_method_a dobj_adopted_method nsubj_adopted_We rcmod_Ratio_adopted nn_Ratio_Likelihood det_Ratio_The dep_3.1_Ratio ccomp_``_3.1
C02-2005	J93-1003	o	The starting point is the log likelihood ratio -LRB- log Dunning 1993 -RRB-	num_Dunning_1993 appos_log_Dunning dep_ratio_log nn_ratio_likelihood nn_ratio_log det_ratio_the cop_ratio_is nsubj_ratio_point amod_point_starting det_point_The
C02-2005	J93-1003	o	Its distribution is asymptotic to a 2 distribution and can hence be used as a test statistic -LRB- Dunning 1993 -RRB-	dobj_Dunning_1993 dep_statistic_Dunning nn_statistic_test det_statistic_a prep_as_used_statistic auxpass_used_be advmod_used_hence aux_used_can nsubjpass_used_distribution num_distribution_2 det_distribution_a conj_and_asymptotic_used prep_to_asymptotic_distribution cop_asymptotic_is nsubj_asymptotic_distribution poss_distribution_Its
C02-2005	J93-1003	p	-LRB- 3 -RRB- -LRB- -RRB- -LRB- -RRB- 0 log 2 log A LH LH = 1 Problems for an unscaled log approach Although log identifies collocations much better than competing approaches -LRB- Dunning 1993 -RRB- in terms of its recall it suffers from its relatively poor precision rates	nn_rates_precision amod_rates_poor poss_rates_its advmod_poor_relatively prep_from_suffers_rates nsubj_suffers_it nsubj_suffers_Problems poss_recall_its prep_of_terms_recall dobj_Dunning_1993 prep_in_approaches_terms dep_approaches_Dunning amod_approaches_competing prep_than_better_approaches advmod_better_much amod_collocations_better dobj_identifies_collocations nsubj_identifies_log mark_identifies_Although nn_approach_log amod_approach_unscaled det_approach_an advcl_Problems_identifies prep_for_Problems_approach num_Problems_1 amod_Problems_= nn_Problems_LH nn_LH_LH det_LH_A nn_LH_log num_LH_2 nn_LH_log num_LH_0 dep_LH_3
C04-1088	J93-1003	o	We have begun experimenting with log likelihood ratio -LRB- Dunning 1993 -RRB- as a thresholding technique	nn_technique_thresholding det_technique_a dobj_Dunning_1993 dep_ratio_Dunning nn_ratio_likelihood nn_ratio_log prep_as_experimenting_technique prep_with_experimenting_ratio xcomp_begun_experimenting aux_begun_have nsubj_begun_We
C04-1094	J93-1003	o	In the iNeast system -LRB- Leuski et al. 2003 -RRB- the identification of relevant terms is oriented towards multi-document summarization and they use a likelihood ratio -LRB- Dunning 1993 -RRB- which favours terms which are representative of the set of documents as opposed to the full collection	amod_collection_full det_collection_the prep_to_opposed_collection mark_opposed_as prep_of_set_documents det_set_the advcl_representative_opposed prep_of_representative_set cop_representative_are nsubj_representative_which rcmod_terms_representative dobj_favours_terms nsubj_favours_which amod_Dunning_1993 rcmod_ratio_favours dep_ratio_Dunning nn_ratio_likelihood det_ratio_a dobj_use_ratio nsubj_use_they amod_summarization_multi-document conj_and_oriented_use prep_towards_oriented_summarization auxpass_oriented_is nsubjpass_oriented_identification dep_oriented_Leuski prep_in_oriented_system amod_terms_relevant prep_of_identification_terms det_identification_the nn_al._et amod_Leuski_2003 dep_Leuski_al. nn_system_iNeast det_system_the
C04-1111	J93-1003	o	We apply the log likelihood principle -LRB- Dunning 1993 -RRB- to compute this score	det_score_this dobj_compute_score aux_compute_to dobj_Dunning_1993 dep_principle_Dunning nn_principle_likelihood nn_principle_log det_principle_the vmod_apply_compute dobj_apply_principle nsubj_apply_We
C04-1121	J93-1003	o	The measure of predictiveness we employed is log likelihood ratio with respect to the target variable -LRB- Dunning 1993 -RRB-	dobj_Dunning_1993 dep_variable_Dunning nn_variable_target det_variable_the prep_with_respect_to_ratio_variable nn_ratio_likelihood nn_ratio_log cop_ratio_is nsubj_ratio_measure nsubj_employed_we rcmod_measure_employed prep_of_measure_predictiveness det_measure_The
C04-1136	J93-1003	o	The candidates were then ranked according to the scores assigned by four association measures the log-likelihood ratio G2 -LRB- Dunning 1993 -RRB- Pearsons chi-squared statistic X2 -LRB- Manning and Schutze 1999 169172 -RRB- the t-score statistic t -LRB- Church et al. 1991 -RRB- and mere cooccurrence frequency f. 4 TPs were identified according to the definition of Krenn -LRB- 2000 -RRB-	appos_Krenn_2000 prep_of_definition_Krenn det_definition_the pobj_identified_definition prepc_according_to_identified_to auxpass_identified_were nsubjpass_identified_TPs nsubjpass_identified_t nsubjpass_identified_X2 nsubjpass_identified_G2 num_TPs_4 nn_TPs_f. nn_TPs_frequency nn_TPs_cooccurrence amod_TPs_mere amod_Church_1991 dep_Church_al. nn_Church_et appos_t_Church nn_t_statistic nn_t_t-score det_t_the amod_Manning_169172 conj_and_Manning_1999 conj_and_Manning_Schutze appos_X2_1999 appos_X2_Schutze appos_X2_Manning nn_X2_statistic amod_X2_chi-squared nn_X2_Pearsons amod_Dunning_1993 conj_and_G2_TPs conj_and_G2_t conj_and_G2_X2 dep_G2_Dunning nn_G2_ratio amod_G2_log-likelihood det_G2_the nn_measures_association num_measures_four agent_assigned_measures vmod_scores_assigned det_scores_the pobj_to_scores pcomp_according_to parataxis_ranked_identified prep_ranked_according advmod_ranked_then auxpass_ranked_were nsubjpass_ranked_candidates det_candidates_The
C04-1136	J93-1003	p	The evaluation results also confirm the argument of Dunning -LRB- 1993 -RRB- who suggested G2 as a more robust alternative to X2	prep_to_alternative_X2 amod_alternative_robust det_alternative_a advmod_robust_more prep_as_suggested_alternative dobj_suggested_G2 nsubj_suggested_who rcmod_Dunning_suggested appos_Dunning_1993 prep_of_argument_Dunning det_argument_the dobj_confirm_argument advmod_confirm_also ccomp_results_confirm nsubj_results_evaluation det_evaluation_The
C04-1141	J93-1003	o	Smadja -LRB- 1993 -RRB- which is the classic work on collocation extraction uses a two-stage filtering model in which in the first step n-gram statistics determine possible collocations and in the second step these candidates are submitted to a syntactic valida7Of course lexical material is always at least partially dependent on the domain in question	det_domain_the prep_in_dependent_question prep_on_dependent_domain advmod_dependent_at cop_dependent_is nsubj_dependent_material advmod_at_partially pobj_at_least advmod_at_always amod_material_lexical nn_course_valida7Of amod_course_syntactic det_course_a ccomp_submitted_dependent prep_to_submitted_course auxpass_submitted_are nsubjpass_submitted_candidates prep_in_submitted_step det_candidates_these amod_step_second det_step_the amod_collocations_possible conj_and_determine_submitted dobj_determine_collocations nsubj_determine_statistics prep_in_determine_step prep_in_determine_which nn_statistics_n-gram amod_step_first det_step_the rcmod_model_submitted rcmod_model_determine amod_model_filtering amod_model_two-stage det_model_a dobj_uses_model nsubj_uses_Smadja nn_extraction_collocation prep_on_work_extraction amod_work_classic det_work_the cop_work_is nsubj_work_which rcmod_Smadja_work appos_Smadja_1993
C04-1141	J93-1003	o	Almost all of these measures can be grouped into one of the following three categories a0 frequency-based measures -LRB- e.g. based on absolute and relative co-occurrence frequencies -RRB- a0 information-theoretic measures -LRB- e.g. mutual information entropy -RRB- a0 statistical measures -LRB- e.g. chi-square t-test log-likelihood Dices coefficient -RRB- The corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties -LRB- Dunning 1993 Manning and Schutze 1999 -RRB- and their suitability for the task of collocation extraction -LRB- see Evert and Krenn -LRB- 2001 -RRB- and Krenn and Evert -LRB- 2001 -RRB- for recent evaluations -RRB-	amod_evaluations_recent appos_Evert_2001 appos_Krenn_2001 conj_and_Evert_Evert conj_and_Evert_Krenn conj_and_Evert_Krenn prep_for_see_evaluations dobj_see_Evert dobj_see_Krenn dobj_see_Krenn dobj_see_Evert nn_extraction_collocation prep_of_task_extraction det_task_the dep_suitability_see prep_for_suitability_task poss_suitability_their dep_Manning_1999 conj_and_Manning_Schutze dep_Dunning_Schutze dep_Dunning_Manning appos_Dunning_1993 dep_properties_Dunning amod_properties_mathematical poss_properties_their prep_of_terms_properties det_literature_the conj_and_discussed_suitability prep_in_discussed_terms preconj_discussed_both prep_in_discussed_literature advmod_discussed_extensively auxpass_discussed_been aux_discussed_have nsubjpass_discussed_metrics amod_metrics_corresponding det_metrics_The dobj_Dices_coefficient nsubj_Dices_chi-square advmod_Dices_e.g. conj_chi-square_log-likelihood conj_chi-square_t-test dep_measures_suitability dep_measures_discussed dep_measures_Dices amod_measures_statistical nn_measures_a0 dep_information_measures dep_information_entropy amod_information_mutual dep_information_e.g. dep_measures_information amod_measures_information-theoretic advmod_measures_a0 nn_frequencies_co-occurrence amod_frequencies_relative amod_frequencies_absolute conj_and_absolute_relative pobj_e.g._frequencies prepc_based_on_e.g._on dep_measures_measures dep_measures_e.g. amod_measures_frequency-based nn_measures_a0 dep_categories_measures num_categories_three prep_following_the_categories prep_of_one_the prep_into_grouped_one auxpass_grouped_be aux_grouped_can nsubjpass_grouped_all det_measures_these prep_of_all_measures advmod_all_Almost
C96-2098	J93-1003	o	2.2 The Choice of Co-occurrence ~ qeasure and Matrix Distance There ~ c many alternatives to measure cooccurrence between two words x and y -LRB- Church 1990 Dunning 1993 -RRB-	amod_Dunning_1993 dep_Church_Dunning appos_Church_1990 conj_x_y conj_x_words_Church num_words_two prep_between_measure_Church prep_between_measure_words dobj_measure_cooccurrence aux_measure_to vmod_alternatives_measure amod_alternatives_many nn_alternatives_c dep_~_alternatives dep_:_~ nn_Distance_Matrix conj_and_qeasure_Distance num_qeasure_~ nn_qeasure_Co-occurrence dep_Choice_There prep_of_Choice_Distance prep_of_Choice_qeasure det_Choice_The num_Choice_2.2 ccomp_``_Choice
D07-1052	J93-1003	o	However while similarity measures -LRB- such as WordNet distance or Lins similarity metric -RRB- only detect cases of semantic similarity association measures -LRB- such as the ones used by Poesio et al. or by Garera and Yarowsky -RRB- also find cases of associative bridg497 Lin98 RFF TheY TheY G2 PL03 Land -LRB- country/state/land -RRB- Staat Staat Kemalismus Regierung Kontinent state state Kemalism government continent Stadt Stadt Bauernfamilie Prasident Region city city agricultural family president region Region Landesregierung Bankgesellschaft Dollar Stadt region country government banking corporation dollar city Bundesrepublik Bundesregierung Baht Albanien Staat federal republic federal government Baht Albania state Republik Gewerkschaft Gasag Hauptstadt Bundesland republic trade union -LRB- a gas company -RRB- capital state Medikament -LRB- medical drug -RRB- Arzneimittel Pille RU Patient Arzneimittel pharmaceutical pill -LRB- a drug -RRB- patient pharmaceutical Praparat Droge Abtreibungspille Arzt Lebensmittel preparation drug -LRB- non-medical -RRB- abortion pill doctor foodstuff Pille Praparat Viagra Pille Praparat pill preparation Viagra pill preparation Hormon Pestizid Pharmakonzern Behandlung Behandlung hormone pesticide pharmaceutical company treatment treatment Lebensmittel Lebensmittel Praparat Abtreibungspille Arznei foodstuff foodstuff preparation abortion pill drug highest ranked words with very rare words removed RU 486 an abortifacient drug Lin98 Lins distributional similarity measure -LRB- Lin 1998 -RRB- RFF Geffet and Dagans Relative Feature Focus measure -LRB- Geffet and Dagan 2004 -RRB- TheY association measure introduced by Garera and Yarowsky -LRB- 2006 -RRB- TheY G2 similar method using a log-likelihood-based statistic -LRB- see Dunning 1993 -RRB- this statistic has a preference for higher-frequency terms PL03 semantic space association measure proposed by Pado and Lapata -LRB- 2003 -RRB- Table 1 Similarity and association measures most similar items ing like 1a b the result of this can be seen in table -LRB- 2 -RRB- while the similarity measures -LRB- Lin98 RFF -RRB- list substitutable terms -LRB- which behave like synonyms in many contexts -RRB- the association measures -LRB- Garera and Yarowskys TheY measure Pado and Lapatas association measure -RRB- also find non-compatible associations such as countrycapital or drugtreatment which is why they are commonly called relationfree	dep_called_relationfree advmod_called_commonly auxpass_called_are nsubjpass_called_they advmod_called_why advcl_is_called nsubj_is_which rcmod_countrycapital_is conj_or_countrycapital_drugtreatment prep_such_as_associations_drugtreatment prep_such_as_associations_countrycapital amod_associations_non-compatible dobj_find_associations advmod_find_also nsubj_find_measures nn_measure_association nn_measure_Lapatas nn_measure_TheY nn_measure_Yarowskys conj_and_Garera_measure conj_and_Garera_Pado conj_and_Garera_measure dep_measures_measure dep_measures_Pado dep_measures_measure dep_measures_Garera nn_measures_association det_measures_the amod_contexts_many prep_in_synonyms_contexts prep_like_behave_synonyms nsubj_behave_which dep_terms_behave amod_terms_substitutable nn_terms_list appos_Lin98_RFF rcmod_measures_find dep_measures_terms dep_measures_Lin98 nn_measures_similarity det_measures_the appos_table_2 prep_while_seen_measures prep_in_seen_table auxpass_seen_be aux_seen_can nsubjpass_seen_result prep_of_result_this det_result_the appos_1a_b prep_like_ing_1a vmod_items_ing amod_items_similar advmod_similar_most dep_measures_items nn_measures_association nn_measures_Similarity conj_and_Similarity_association num_Table_1 nn_Table_Lapata appos_Lapata_2003 conj_and_Pado_Table agent_proposed_Table agent_proposed_Pado dep_measure_seen dep_measure_measures vmod_measure_proposed nn_measure_association nn_measure_space amod_measure_semantic nn_PL03_terms nn_PL03_higher-frequency dep_preference_measure prep_for_preference_PL03 det_preference_a dobj_has_preference nsubj_has_method det_statistic_this num_Dunning_1993 dobj_see_Dunning amod_statistic_log-likelihood-based det_statistic_a dobj_using_statistic dep_using_see dobj_using_statistic vmod_method_using amod_method_similar dep_G2_has appos_Yarowsky_2006 dep_Garera_TheY conj_and_Garera_Yarowsky agent_introduced_Yarowsky agent_introduced_Garera vmod_measure_introduced nn_measure_association dep_TheY_measure nn_TheY_measure nn_TheY_Geffet dep_Geffet_2004 conj_and_Geffet_Dagan nn_measure_Focus nn_measure_Feature amod_measure_Relative nn_measure_Dagans dep_Geffet_Dagan dep_Geffet_Geffet conj_and_Geffet_measure amod_Lin_1998 dep_measure_G2 dep_measure_TheY dep_measure_RFF dep_measure_Lin nn_measure_similarity amod_measure_distributional nn_measure_Lins nn_Lin98_drug nn_Lin98_abortifacient det_Lin98_an dep_RU_measure appos_RU_Lin98 num_RU_486 dep_removed_RU amod_words_rare advmod_rare_very dep_ranked_removed prep_with_ranked_words dobj_ranked_words nsubj_ranked_drug amod_drug_highest nn_drug_pill nn_drug_abortion nn_drug_preparation nn_drug_foodstuff nn_drug_foodstuff nn_drug_Arznei nn_drug_Abtreibungspille nn_drug_Praparat nn_drug_Lebensmittel nn_drug_Lebensmittel nn_drug_treatment nn_drug_treatment nn_drug_company amod_drug_pharmaceutical nn_drug_pesticide nn_drug_hormone nn_drug_Behandlung nn_drug_Behandlung nn_drug_Pharmakonzern nn_drug_Pestizid nn_drug_Hormon nn_drug_preparation nn_drug_pill nn_drug_Viagra nn_drug_preparation nn_drug_pill nn_drug_Praparat nn_drug_Pille nn_drug_Viagra nn_drug_Praparat nn_drug_Pille nn_drug_foodstuff nn_drug_doctor nn_drug_pill nn_drug_abortion nn_drug_drug dep_drug_non-medical nn_drug_preparation nn_drug_Lebensmittel nn_drug_Arzt nn_drug_Abtreibungspille nn_drug_Droge nn_drug_Praparat amod_drug_pharmaceutical nn_drug_patient dep_drug_drug nn_drug_pill det_drug_a amod_pill_pharmaceutical nn_pill_Arzneimittel nn_pill_Patient nn_pill_RU nn_pill_Pille nn_pill_Arzneimittel nn_pill_Medikament amod_drug_medical appos_Medikament_drug nn_Medikament_state nn_Medikament_capital nn_Medikament_union nn_company_gas det_company_a appos_union_company nn_union_trade nn_union_republic nn_union_Bundesland nn_union_Hauptstadt nn_union_Gasag nn_union_Gewerkschaft nn_union_Republik nn_union_state nn_union_Albania nn_union_Baht nn_union_government amod_union_federal nn_union_republic amod_union_federal nn_union_Staat nn_union_Albanien nn_union_Baht nn_union_Bundesregierung nn_union_Bundesrepublik nn_union_city nn_union_dollar nn_union_corporation nn_union_banking nn_union_government nn_union_country nn_union_region nn_union_Stadt nn_union_Dollar nn_union_Bankgesellschaft nn_union_Landesregierung nn_union_Region nn_union_region nn_union_president nn_union_family amod_union_agricultural nn_union_city nn_union_city nn_union_Region nn_union_Prasident nn_union_Bauernfamilie nn_union_Stadt nn_union_Stadt nn_union_continent nn_union_government nn_union_Kemalism nn_union_state nn_union_state amod_union_Kontinent nn_union_Regierung nn_union_Kemalismus nn_union_Staat nn_union_Staat nn_union_Land appos_Land_country/state/land nn_Land_PL03 nn_Land_G2 nn_TheY_TheY nn_TheY_RFF nn_TheY_Lin98 nn_TheY_bridg497 amod_TheY_associative prep_of_cases_TheY dobj_find_cases advmod_find_also nsubj_find_ones conj_and_Garera_Yarowsky pobj_by_Yarowsky pobj_by_Garera conj_or_Poesio_by dep_Poesio_al. nn_Poesio_et agent_used_by agent_used_Poesio vmod_ones_used det_ones_the dep_as_ranked pcomp_as_find mwe_as_such dep_measures_as nn_measures_association prep_while_measures_measures amod_similarity_semantic prep_of_cases_similarity dobj_detect_cases advmod_detect_only amod_similarity_metric nn_similarity_Lins conj_or_distance_similarity amod_distance_WordNet dep_measures_detect prep_as_measures_similarity prep_as_measures_distance mwe_measures_such nn_measures_similarity dep_However_measures dep_``_However
D08-1048	J93-1003	o	As association measure we apply log-likelihood ratio -LRB- Dunning 1993 -RRB- to normalized frequency	amod_frequency_normalized dep_Dunning_1993 dep_ratio_Dunning amod_ratio_log-likelihood prep_to_apply_frequency dobj_apply_ratio nsubj_apply_we rcmod_measure_apply nn_measure_association pobj_As_measure dep_``_As
D09-1040	J93-1003	o	Beside simple cooccurrence counts within sliding windows other SoA measures include functions based on TF/IDF -LRB- Fung and Yee 1998 -RRB- mutual information -LRB- PMI -RRB- -LRB- Lin 1998 -RRB- conditional probabilities -LRB- Schuetze and Pedersen 1997 -RRB- chi-square test and the loglikelihood ratio -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_ratio_Dunning nn_ratio_loglikelihood det_ratio_the amod_test_chi-square dep_Schuetze_1997 conj_and_Schuetze_Pedersen appos_probabilities_Pedersen appos_probabilities_Schuetze amod_probabilities_conditional dep_Lin_1998 dep_information_Lin appos_information_PMI amod_information_mutual dep_Fung_1998 conj_and_Fung_Yee conj_and_TF/IDF_ratio conj_and_TF/IDF_test conj_and_TF/IDF_probabilities conj_and_TF/IDF_information appos_TF/IDF_Yee appos_TF/IDF_Fung prep_on_based_ratio prep_on_based_test prep_on_based_probabilities prep_on_based_information prep_on_based_TF/IDF vmod_functions_based dobj_include_functions nsubj_include_measures nn_measures_SoA amod_measures_other amod_windows_sliding rcmod_counts_include prep_within_counts_windows nn_counts_cooccurrence amod_counts_simple amod_counts_Beside
D09-1040	J93-1003	o	1 Introduction Phrase-based systems flat and hierarchical alike -LRB- Koehn et al. 2003 Koehn 2004b Koehn et al. 2007 Chiang 2005 Chiang 2007 -RRB- have achieved a much better translation coverage than wordbased ones -LRB- Brown et al. 1993 -RRB- but untranslated words remain a major problem in SMT	prep_in_problem_SMT amod_problem_major det_problem_a xcomp_remain_problem nsubj_remain_words amod_words_untranslated amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_ones_wordbased prep_than_coverage_ones nn_coverage_translation amod_coverage_better det_coverage_a advmod_better_much conj_but_achieved_remain dep_achieved_Brown dobj_achieved_coverage aux_achieved_have nsubj_achieved_systems dep_Chiang_2007 num_Chiang_2005 num_Koehn_2007 nn_Koehn_al. nn_Koehn_et appos_Koehn_2004b dep_Koehn_Chiang dep_Koehn_Chiang dep_Koehn_Koehn dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et advmod_flat_alike conj_and_flat_hierarchical dep_systems_Koehn amod_systems_hierarchical amod_systems_flat amod_systems_Phrase-based nn_systems_Introduction num_systems_1 ccomp_``_remain ccomp_``_achieved
D09-1051	J93-1003	o	Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts -LRB- Choueka et al. 1983 Church and Hanks 1990 Smadja 1993 Dunning 1993 Pearce 2002 Evert 2004 -RRB-	amod_Evert_2004 num_Pearce_2002 num_Dunning_1993 num_Smadja_1993 dep_Church_Evert conj_and_Church_Pearce conj_and_Church_Dunning conj_and_Church_Smadja conj_and_Church_1990 conj_and_Church_Hanks dep_Choueka_Pearce dep_Choueka_Dunning dep_Choueka_Smadja dep_Choueka_1990 dep_Choueka_Hanks dep_Choueka_Church dep_Choueka_1983 dep_Choueka_al. nn_Choueka_et prep_in_pairs_texts nn_pairs_word det_pairs_the prep_of_frequencies_pairs amod_frequencies_co-occurring dep_carried_Choueka prep_based_on_carried_frequencies prt_carried_out auxpass_carried_are nsubjpass_carried_studies nn_extraction_collocation prep_on_studies_extraction amod_studies_Many ccomp_``_carried
D09-1051	J93-1003	o	Thus the alignment set is denoted as -RCB- & -RSB- ,1 -LSB- | -RRB- -LCB- -LRB- ialiaiA ii = We adapt the bilingual word alignment model IBM Model 3 -LRB- Brown et al. 1993 -RRB- to monolingual word alignment	nn_alignment_word amod_alignment_monolingual amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_Model_Brown num_Model_3 nn_Model_IBM nn_model_alignment nn_model_word amod_model_bilingual det_model_the dobj_adapt_model nsubj_adapt_We dep_adapt_,1 dep_adapt_as dep_=_ii dep_=_ialiaiA dep_,1_= appos_,1_| cc_as_& dep_denoted_adapt auxpass_denoted_is nsubjpass_denoted_set prep_to_alignment_alignment conj_alignment_Model rcmod_alignment_denoted det_alignment_the dep_Thus_alignment dep_``_Thus
D09-1066	J93-1003	p	One popular and statistically appealing such measure is Log-Likelihood -LRB- LL -RRB- -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_Log-Likelihood_Dunning appos_Log-Likelihood_LL cop_Log-Likelihood_is nsubj_Log-Likelihood_measure amod_measure_such amod_measure_appealing amod_measure_popular num_measure_One advmod_appealing_statistically conj_and_popular_appealing
D09-1081	J93-1003	o	For example in this work we use loglikelihood ratio -LRB- Dunning 1993 -RRB- to determine the SoA between a word sense and co-occurring words and cosine to determine the distance between two DPWSs log likelihood vectors -LRB- McDonald 2000 -RRB-	amod_McDonald_2000 dep_vectors_McDonald nn_vectors_likelihood dobj_log_vectors nsubj_log_cosine num_DPWSs_two prep_between_distance_DPWSs det_distance_the dobj_determine_distance aux_determine_to vmod_cosine_determine nn_words_co-occurring conj_and_sense_words nn_sense_word det_sense_a prep_between_SoA_words prep_between_SoA_sense det_SoA_the dobj_determine_SoA aux_determine_to amod_Dunning_1993 dep_ratio_Dunning amod_ratio_loglikelihood conj_and_use_log vmod_use_determine dobj_use_ratio nsubj_use_we prep_in_use_work prep_for_use_example det_work_this
D09-1081	J93-1003	p	This further supports the claim by Dunning -LRB- 1993 -RRB- that loglikelihood ratio is much less sensitive than pmi to low counts	amod_counts_low prep_to_pmi_counts prep_than_sensitive_pmi advmod_sensitive_less advmod_sensitive_much cop_sensitive_is csubj_sensitive_supports amod_ratio_loglikelihood det_ratio_that nn_ratio_Dunning appos_Dunning_1993 prep_by_claim_ratio det_claim_the dobj_supports_claim advmod_supports_further nsubj_supports_This
D09-1154	J93-1003	o	We then scored each query pair -LRB- q1 q2 -RRB- in this subset using the log-likelihood ratio -LRB- LLR Dunning 1993 -RRB- between q1 and q2 which measures the mutual dependence within the context of web search queries -LRB- Jones et al. 2006a -RRB-	appos_Jones_2006a dep_Jones_al. nn_Jones_et nn_queries_search nn_queries_web prep_of_context_queries det_context_the prep_within_dependence_context amod_dependence_mutual det_dependence_the dobj_measures_dependence nsubj_measures_which dep_q1_Jones rcmod_q1_measures conj_and_q1_q2 dep_LLR_1993 appos_LLR_Dunning dep_ratio_LLR amod_ratio_log-likelihood det_ratio_the prep_between_using_q2 prep_between_using_q1 dobj_using_ratio det_subset_this appos_q1_q2 dep_pair_q1 nn_pair_query det_pair_each xcomp_scored_using prep_in_scored_subset dobj_scored_pair advmod_scored_then nsubj_scored_We
E06-1018	J93-1003	o	The significance values are obtained using the loglikelihood measure assuming a binomial distribution for the unrelatedness hypothesis -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_hypothesis_Dunning nn_hypothesis_unrelatedness det_hypothesis_the prep_for_distribution_hypothesis amod_distribution_binomial det_distribution_a dobj_assuming_distribution nn_measure_loglikelihood det_measure_the vmod_using_assuming dobj_using_measure xcomp_obtained_using auxpass_obtained_are nsubjpass_obtained_values nn_values_significance det_values_The
E09-1074	J93-1003	o	As a result we can use collocation measures like point-wise mutual information -LRB- Church and Hanks 1989 -RRB- or the log-likelihood ratio -LRB- Dunning 1993 -RRB- to predict the strong association for a given cue	amod_cue_given det_cue_a prep_for_association_cue amod_association_strong det_association_the dobj_predict_association aux_predict_to amod_Dunning_1993 dep_ratio_Dunning amod_ratio_log-likelihood det_ratio_the conj_and_Church_1989 conj_and_Church_Hanks conj_or_information_ratio dep_information_1989 dep_information_Hanks dep_information_Church amod_information_mutual amod_information_point-wise prep_like_measures_ratio prep_like_measures_information nn_measures_collocation vmod_use_predict dobj_use_measures aux_use_can nsubj_use_we prep_as_use_result det_result_a
E09-2012	J93-1003	p	By default the log-likelihood ratio measure -LRB- LLR -RRB- is proposed since it was shown to be particularly suited to language data -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_data_Dunning nn_data_language prep_to_suited_data advmod_suited_particularly auxpass_suited_be aux_suited_to xcomp_shown_suited auxpass_shown_was nsubjpass_shown_it mark_shown_since advcl_proposed_shown auxpass_proposed_is nsubjpass_proposed_measure agent_proposed_default appos_measure_LLR nn_measure_ratio amod_measure_log-likelihood det_measure_the rcmod_``_proposed
E95-1008	J93-1003	o	Collocation map that is first suggested in -LRB- Itan 1993 -RRB- is a sigmoid belief network with words as probabilistic variables	amod_variables_probabilistic prep_as_words_variables prep_with_network_words nn_network_belief amod_network_sigmoid det_network_a cop_network_is nsubj_network_Itan mark_network_in num_Itan_1993 ccomp_suggested_network vmod_first_suggested nsubj_is_first dep_that_is dep_map_that nn_map_Collocation
E95-1008	J93-1003	p	This results also agree with Dunning 's argument about overestimation on the infrequent occurrences in which many infrequent pairs tend to get higher estimation -LRB- Dunning 1993 -RRB-	dobj_Dunning_1993 amod_estimation_higher dobj_get_estimation aux_get_to xcomp_tend_get nsubj_tend_pairs prep_in_tend_which amod_pairs_infrequent amod_pairs_many dep_occurrences_Dunning rcmod_occurrences_tend amod_occurrences_infrequent det_occurrences_the prep_on_argument_occurrences prep_about_argument_overestimation poss_argument_Dunning prep_with_agree_argument advmod_agree_also dep_results_agree nsubj_results_This
E95-1008	J93-1003	o	The problem is due to the assumption of normality in naive frequency based statistics according to Dunning -LRB- 1993 -RRB-	appos_Dunning_1993 pobj_statistics_Dunning prepc_according_to_statistics_to amod_statistics_based amod_frequency_naive prep_in_assumption_frequency prep_of_assumption_normality det_assumption_the dobj_due_statistics prep_to_due_assumption cop_due_is nsubj_due_problem det_problem_The
E99-1005	J93-1003	o	Proceedings of EACL '99 Determinants of Adjective-Noun Plausibility Maria Lapata and Scott McDonald and Frank Keller School of Cognitive Science Division of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW UK -LCB- mlap scottm keller -RCB- @cogsci ed.ac.uk Abstract This paper explores the determinants of adjective-noun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpus-based variables co-occurrence frequency of the adjective-noun pair noun frequency conditional probability of the noun given the adjective the log-likelihood ratio and Resnik 's -LRB- 1993 -RRB- selectional association measure	nn_measure_association amod_measure_selectional dep_Resnik_1993 amod_ratio_log-likelihood det_ratio_the dep_adjective_measure conj_and_adjective_Resnik conj_and_adjective_ratio det_adjective_the pobj_given_Resnik pobj_given_ratio pobj_given_adjective prep_noun_given det_noun_the prep_of_probability_noun amod_probability_conditional nn_frequency_noun amod_pair_adjective-noun det_pair_the appos_frequency_probability appos_frequency_frequency prep_of_frequency_pair nn_frequency_co-occurrence amod_variables_corpus-based num_variables_five prep_with_subjects_variables amod_subjects_human prep_from_elicited_subjects vmod_judgements_elicited dobj_compare_judgements aux_compare_to nn_analysis_correlation vmod_using_compare dobj_using_analysis amod_plausibility_adjective-noun prep_of_determinants_plausibility det_determinants_the dep_explores_frequency prepc_by_explores_using dobj_explores_determinants nsubj_explores_paper det_paper_This amod_paper_Abstract nn_paper_ed.ac.uk nn_@cogsci_keller rcmod_mlap_explores appos_mlap_@cogsci appos_mlap_scottm nn_mlap_UK nn_9LW_EH8 nn_9LW_Edinburgh nn_Place_Buccleuch num_Place_2 nn_Place_Edinburgh prep_of_University_Place appos_Division_mlap conj_Division_9LW conj_Division_University prep_of_Division_Informatics nn_Division_Science nn_Division_Cognitive prep_of_School_Division nn_School_Keller nn_School_Frank nn_McDonald_Scott conj_and_Lapata_School conj_and_Lapata_McDonald nn_Lapata_Maria nn_Lapata_Plausibility nn_Lapata_Adjective-Noun prep_of_Determinants_School prep_of_Determinants_McDonald prep_of_Determinants_Lapata num_Determinants_'99 nn_Determinants_EACL prep_of_Proceedings_Determinants
E99-1005	J93-1003	o	Conditional probability the log-likelihood ratio and Resnik 's -LRB- 1993 -RRB- selectional association measure were also significantly correlated with plausibility ratings	nn_ratings_plausibility prep_with_correlated_ratings advmod_correlated_significantly advmod_correlated_also auxpass_correlated_were nsubjpass_correlated_measure nsubjpass_correlated_ratio nsubjpass_correlated_probability nn_measure_association amod_measure_selectional nn_measure_Resnik dep_Resnik_1993 possessive_Resnik_'s amod_ratio_log-likelihood det_ratio_the conj_and_probability_measure conj_and_probability_ratio amod_probability_Conditional
E99-1005	J93-1003	o	The research presented in this paper is similar in motivation to Resnik 's -LRB- 1993 -RRB- work on selectional restrictions	amod_restrictions_selectional nn_work_Resnik dep_Resnik_1993 possessive_Resnik_'s prep_on_motivation_restrictions prep_to_motivation_work prep_in_similar_motivation cop_similar_is nsubj_similar_research det_paper_this prep_in_presented_paper vmod_research_presented det_research_The
E99-1005	J93-1003	o	We employ the loglikelihood ratio as a measure of the collocational status of the adjective-noun pair -LRB- Dunning 1993 Daille 1996 -RRB-	amod_Daille_1996 dep_Dunning_Daille appos_Dunning_1993 appos_pair_Dunning amod_pair_adjective-noun det_pair_the prep_of_status_pair amod_status_collocational det_status_the prep_of_measure_status det_measure_a nn_ratio_loglikelihood det_ratio_the prep_as_employ_measure dobj_employ_ratio nsubj_employ_We
E99-1005	J93-1003	o	We estimated the probabilities P -LRB- c I Pi -RRB- and P -LRB- c -RRB- similarly to Resnik -LRB- 1993 -RRB- by using relative frequencies from the BNC together with WordNet -LRB- Miller et al. 1990 -RRB- as a source of taxonomic semantic class information	nn_information_class amod_information_semantic amod_information_taxonomic prep_of_source_information det_source_a amod_Miller_1990 dep_Miller_al. nn_Miller_et dep_WordNet_Miller prep_as_,_source prep_together_with_,_WordNet det_BNC_the amod_frequencies_relative prep_from_using_BNC dobj_using_frequencies pcomp_by_using appos_Resnik_1993 appos_P_c num_Pi_I nn_Pi_c dep_P_by prep_to_P_Resnik advmod_P_similarly conj_and_P_P appos_P_Pi dep_probabilities_P dep_probabilities_P det_probabilities_the dobj_estimated_probabilities nsubj_estimated_We
H05-1013	J93-1003	o	The second attempts to instill knowledge of collocations in the data we use the technique described by -LRB- Dunning 1993 -RRB- to compute multi-word expressions and then mark words that are commonly used as such with a feature that expresses this fact	det_fact_this dobj_expresses_fact nsubj_expresses_that rcmod_feature_expresses det_feature_a pobj_with_feature mwe_with_such pcomp_as_with prep_used_as advmod_used_commonly auxpass_used_are nsubjpass_used_that rcmod_words_used nn_words_mark advmod_words_then amod_expressions_multi-word conj_and_compute_words dobj_compute_expressions aux_compute_to dep_Dunning_1993 xcomp_described_words xcomp_described_compute agent_described_Dunning vmod_technique_described det_technique_the dobj_use_technique nsubj_use_we det_data_the prep_of_knowledge_collocations prep_in_instill_data dobj_instill_knowledge aux_instill_to parataxis_attempts_use vmod_attempts_instill amod_attempts_second det_attempts_The
H05-1089	J93-1003	o	1 Introduction Word associations -LRB- co-occurrences -RRB- have a wide range of applications including Speech Recognition Optical Character Recognition and Information Retrieval -LRB- IR -RRB- -LRB- Church and Hanks 1991 Dunning 1993 Manning and Schutze 1999 -RRB-	dep_Manning_1999 conj_and_Manning_Schutze num_Dunning_1993 dep_Church_Schutze dep_Church_Manning conj_and_Church_Dunning appos_Church_1991 conj_and_Church_Hanks dep_Retrieval_Dunning dep_Retrieval_Hanks dep_Retrieval_Church appos_Retrieval_IR nn_Retrieval_Information nn_Recognition_Character nn_Recognition_Optical conj_and_Recognition_Retrieval conj_and_Recognition_Recognition nn_Recognition_Speech prep_including_applications_Retrieval prep_including_applications_Recognition prep_including_applications_Recognition prep_of_range_applications amod_range_wide det_range_a dobj_have_range nsubj_have_associations appos_associations_co-occurrences nn_associations_Word nn_associations_Introduction num_associations_1 ccomp_``_have
H05-1089	J93-1003	o	Many studies focus on rare words -LRB- Dunning 1993 Moore 2004 -RRB- butterflies are more interesting than moths	prep_than_interesting_moths advmod_interesting_more cop_interesting_are nsubj_interesting_butterflies amod_Moore_2004 dep_Dunning_Moore appos_Dunning_1993 appos_words_Dunning amod_words_rare parataxis_focus_interesting prep_on_focus_words nsubj_focus_studies amod_studies_Many ccomp_``_focus
H05-1113	J93-1003	o	Several other measures like Log-Likelihood -LRB- Dunning 1993 -RRB- Pearsons a2a4a3 -LRB- Church et al. 1991 -RRB- Z-Score -LRB- Church et al. 1991 -RRB- Cubic Association Ratio -LRB- MI3 -RRB- etc. have been also proposed	advmod_proposed_also auxpass_proposed_been aux_proposed_have nsubjpass_proposed_measures appos_Ratio_MI3 nn_Ratio_Association nn_Ratio_Cubic amod_Church_1991 dep_Church_al. nn_Church_et dep_Z-Score_Church amod_Church_1991 dep_Church_al. nn_Church_et dep_a2a4a3_Church nn_a2a4a3_Pearsons amod_Dunning_1993 dep_Log-Likelihood_etc. conj_Log-Likelihood_Ratio conj_Log-Likelihood_Z-Score conj_Log-Likelihood_a2a4a3 dep_Log-Likelihood_Dunning prep_like_measures_Log-Likelihood amod_measures_other amod_measures_Several
I08-1013	J93-1003	o	5http / / cl.cs.okayama-u ac.jp / rsc / jacabit / a4a6a5 which gathers the set of co-occurrence units a7 associated with the number of times that a7 and a2 occur together a8a6a9a10a9 a5 a11 In order to identify speci c words in the lexical context and to reduce word-frequency effects we normalize context vectors using an association score such as Mutual Information -LRB- Fano 1961 -RRB- or Log-likelihood -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_Log-likelihood_Dunning dep_Fano_1961 conj_or_Information_Log-likelihood dep_Information_Fano amod_Information_Mutual prep_such_as_score_Log-likelihood prep_such_as_score_Information nn_score_association det_score_an dobj_using_score nn_vectors_context xcomp_normalize_using dobj_normalize_vectors nsubj_normalize_we ccomp_normalize_5http nn_effects_word-frequency dobj_reduce_effects aux_reduce_to amod_context_lexical det_context_the nn_words_c nn_words_speci conj_and_identify_reduce prep_in_identify_context dobj_identify_words aux_identify_to dep_identify_order mark_identify_In nn_a11_a5 amod_a11_a8a6a9a10a9 dep_occur_a11 advmod_occur_together nsubj_occur_a2 nsubj_occur_a7 dobj_occur_that conj_and_a7_a2 rcmod_times_occur prep_of_number_times det_number_the prep_with_associated_number vmod_a7_associated nn_a7_units nn_a7_co-occurrence prep_of_set_a7 det_set_the dobj_gathers_set nsubj_gathers_which rcmod_cl.cs.okayama-u_gathers dep_cl.cs.okayama-u_a4a6a5 dep_cl.cs.okayama-u_jacabit dep_cl.cs.okayama-u_rsc dep_cl.cs.okayama-u_ac.jp dep_5http_reduce dep_5http_identify dep_5http_cl.cs.okayama-u
I08-1013	J93-1003	o	4 Pattern switching The compositional translation presents problems which have been reported by -LRB- Baldwin and Tanaka 2004 Brown et al. 1993 -RRB- Fertility SWTs and MWTs are not translated by a term of a same length	amod_length_same det_length_a prep_of_term_length det_term_a agent_translated_term neg_translated_not auxpass_translated_are nsubjpass_translated_MWTs nsubjpass_translated_SWTs conj_and_SWTs_MWTs nn_SWTs_Fertility num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Baldwin_translated dep_Baldwin_Brown num_Baldwin_2004 conj_and_Baldwin_Tanaka agent_reported_Tanaka agent_reported_Baldwin auxpass_reported_been aux_reported_have nsubjpass_reported_which rcmod_problems_reported dobj_presents_problems nsubj_presents_translation amod_translation_compositional det_translation_The rcmod_switching_presents nn_switching_Pattern num_switching_4
I08-1038	J93-1003	o	Many methods have been proposed to measure the co-occurrence relation between two words such as 2 -LRB- Church and Mercer ,1993 -RRB- mutual information -LRB- Church and Hanks 1989 Pantel and Lin 2002 -RRB- t-test -LRB- Church and Hanks 1989 -RRB- and loglikelihood -LRB- Dunning ,1993 -RRB-	num_Dunning_,1993 appos_loglikelihood_Dunning dep_Church_1989 conj_and_Church_Hanks appos_t-test_Hanks appos_t-test_Church amod_Pantel_2002 conj_and_Pantel_Lin conj_and_Church_Lin conj_and_Church_Pantel conj_and_Church_1989 conj_and_Church_Hanks conj_and_information_loglikelihood conj_and_information_t-test dep_information_Pantel dep_information_1989 dep_information_Hanks dep_information_Church amod_information_mutual num_Mercer_,1993 conj_and_Church_Mercer num_Church_2 prep_such_as_words_Mercer prep_such_as_words_Church num_words_two prep_between_relation_words nn_relation_co-occurrence det_relation_the dep_measure_loglikelihood dep_measure_t-test dep_measure_information dobj_measure_relation aux_measure_to xcomp_proposed_measure auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_Many ccomp_``_proposed
I08-1059	J93-1003	o	Before training the classifiers we perform feature ablation by imposing a count cutoff of 10 and by limiting the number of features to the top 75K features in terms of log likelihood ratio -LRB- Dunning 1993 -RRB-	dobj_Dunning_1993 nn_ratio_likelihood nn_ratio_log prep_of_terms_ratio prep_in_features_terms num_features_75K amod_features_top det_features_the prep_of_number_features det_number_the dep_limiting_Dunning prep_to_limiting_features dobj_limiting_number pcomp_by_limiting prep_of_cutoff_10 nn_cutoff_count det_cutoff_a dobj_imposing_cutoff nn_ablation_feature conj_and_perform_by prepc_by_perform_imposing dobj_perform_ablation nsubj_perform_we prepc_before_perform_training det_classifiers_the dobj_training_classifiers
I08-2134	J93-1003	p	All the enumerated segment pairs are listed in the following table Feature x y Feature x y AM1 +1 c1 c0 AM2 +1 c2c1 c0 AM1 +2 c1 c0c1 AM2 +2 c2c1 c0c1 AM1 +3 c1 c0c1c2 AM3 +1 c3c2c1 c0 We use Dunnings method -LRB- Dunning 1993 -RRB- because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon	dep_rare_phenomenon conj_and_rare_common preconj_rare_both prep_of_occurrences_common prep_of_occurrences_rare det_occurrences_the prep_of_signiflcance_occurrences det_signiflcance_the prep_between_made_signiflcance auxpass_made_be aux_made_to vmod_comparisons_made dobj_allows_comparisons nsubj_allows_it prep_of_assumption_normality det_assumption_the conj_and_depend_allows prep_on_depend_assumption neg_depend_not aux_depend_does nsubj_depend_it mark_depend_because appos_Dunning_1993 dep_method_Dunning nn_method_Dunnings dobj_use_method nsubj_use_We rcmod_c0_use num_c3c2c1_+1 nn_c3c2c1_AM3 nn_c3c2c1_c0c1c2 num_c1_+3 nn_c1_AM1 nn_c1_c0c1 num_c2c1_+2 nn_c2c1_AM2 nn_c2c1_c0c1 num_c1_+2 nn_c1_AM1 nn_c1_c0 num_c2c1_+1 nn_c2c1_AM2 nn_c2c1_c0 num_c1_+1 nn_c1_AM1 nn_c1_y nn_x_Feature nn_x_y dep_x_allows dep_x_depend appos_x_c0 conj_x_c3c2c1 conj_x_c1 conj_x_c2c1 conj_x_c1 conj_x_c2c1 conj_x_c1 conj_x_x nn_x_Feature amod_table_following det_table_the dep_listed_x prep_in_listed_table auxpass_listed_are nsubjpass_listed_pairs nn_pairs_segment amod_pairs_enumerated det_pairs_the predet_pairs_All ccomp_``_listed
I08-2134	J93-1003	o	-LRB- Choueka 1988 -RRB- regarded MWE as connected collocations a sequence of neighboring words whose exact meaning can not be derived from the meaning or connotation of its components which means that MWEs also have low ST. As some pioneers provide MWE identiflcation methods which are based on association metrics -LRB- AM -RRB- such as likelihood ratio -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_ratio_Dunning nn_ratio_likelihood prep_such_as_metrics_ratio appos_metrics_AM nn_metrics_association prep_on_based_metrics auxpass_based_are nsubjpass_based_which rcmod_methods_based nn_methods_identiflcation nn_methods_MWE dobj_provide_methods nsubj_provide_pioneers mark_provide_As det_pioneers_some amod_ST._low advcl_have_provide dobj_have_ST. advmod_have_also nsubj_have_MWEs mark_have_that ccomp_means_have nsubj_means_which rcmod_components_means poss_components_its prep_of_meaning_components conj_or_meaning_connotation det_meaning_the prep_from_derived_connotation prep_from_derived_meaning auxpass_derived_be neg_derived_not aux_derived_can nsubjpass_derived_meaning amod_meaning_exact poss_meaning_whose rcmod_words_derived amod_words_neighboring prep_of_sequence_words det_sequence_a amod_collocations_connected dep_MWE_sequence prep_as_MWE_collocations dobj_regarded_MWE dep_regarded_Choueka dep_Choueka_1988
J00-2004	J93-1003	o	A boundary-based model of co-occurrence assumes that both halves of the bitext have been segmented into s segments so that segment Ui in one half of the bitext and segment Vi in the other half are mutual translations 1 < i < s. Under the boundary-based model of co-occurrence there are several ways to compute co-occurrence counts cooc -LRB- u v -RRB- between word types u and v In the models of Brown Della Pietra Della Pietra and Mercer -LRB- 1993 -RRB- reviewed in Section 4.3 s COOC -LRB- R V -RRB- = ~ ei -LRB- u -RRB- j ~ -LRB- V -RRB- -LRB- 12 -RRB- i = 1 where ei and j5 are the unigram frequencies of u and v respectively in each aligned text segment i. For most translation models this method produces suboptimal results however when ei -LRB- u -RRB- > 1 and -RRB- ~ -LRB- v -RRB- > 1	num_>_1 dep_>_v dep_>_~ dep_>_1 advmod_>_however conj_and_1_~ quantmod_1_> dep_1_ei advmod_1_when appos_ei_u amod_results_suboptimal dobj_produces_results nsubj_produces_method det_method_this nn_models_translation amod_models_most nn_i._segment nn_i._text amod_i._aligned det_i._each prep_in_u_i. advmod_u_respectively conj_and_u_v prep_for_frequencies_models prep_of_frequencies_v prep_of_frequencies_u amod_frequencies_unigram det_frequencies_the cop_frequencies_are nsubj_frequencies_j5 nsubj_frequencies_ei advmod_frequencies_where conj_and_ei_j5 rcmod_1_frequencies dobj_=_1 dep_i_= dep_~_> rcmod_~_produces dep_~_i appos_~_12 appos_~_V nn_~_j appos_ei_u nn_ei_~ dep_=_ei appos_R_V amod_COOC_= dep_COOC_R dep_s_COOC num_Section_4.3 prep_in_reviewed_Section appos_Mercer_1993 nn_Pietra_Della nn_Pietra_Della conj_and_Brown_Mercer conj_and_Brown_Pietra conj_and_Brown_Pietra prep_of_models_Mercer prep_of_models_Pietra prep_of_models_Pietra prep_of_models_Brown det_models_the conj_and_u_v dep_types_v dep_types_u nn_types_word dep_u_v prep_between_cooc_types appos_cooc_u dep_counts_cooc nn_counts_co-occurrence dobj_compute_counts aux_compute_to vmod_ways_compute amod_ways_several nsubj_are_ways expl_are_there prep_of_model_co-occurrence amod_model_boundary-based det_model_the dep_s._< nn_s._i prep_under_<_model num_<_s. dep_1_< dep_translations_are amod_translations_1 amod_translations_mutual cop_translations_are nsubj_translations_Ui mark_translations_that advmod_translations_so amod_half_other det_half_the nn_Vi_segment conj_and_bitext_Vi det_bitext_the prep_of_half_Vi prep_of_half_bitext num_half_one prep_in_Ui_half prep_in_Ui_half nn_Ui_segment nn_segments_s dep_segmented_s dep_segmented_reviewed prep_in_segmented_models dep_segmented_translations prep_into_segmented_segments cop_segmented_been aux_segmented_have nsubj_segmented_halves mark_segmented_that det_bitext_the prep_of_halves_bitext preconj_halves_both dobj_assumes_~ ccomp_assumes_segmented nsubj_assumes_model prep_of_model_co-occurrence amod_model_boundary-based det_model_A
J00-2004	J93-1003	o	Due to the parameter interdependencies introduced by the one-to-one assumption we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. \ -LSB- 1993b Equation 26 \ -RSB- -RRB-	num_\_26 nn_\_Equation appos_1993b_\ dep_\_1993b nn_\_al. nn_\_Brown nn_al._et pobj_in_\ pcomp_as_in det_other_each prep_of_independently_other prep_estimated_as advmod_estimated_independently auxpass_estimated_be aux_estimated_can nsubjpass_estimated_that rcmod_parameters_estimated det_assignments_the prep_into_decomposing_parameters dobj_decomposing_assignments prepc_for_method_decomposing det_method_a dobj_find_method aux_find_to xcomp_unlikely_find cop_unlikely_are nsubj_unlikely_we prep_due_to_unlikely_interdependencies amod_assumption_one-to-one det_assumption_the agent_introduced_assumption vmod_interdependencies_introduced nn_interdependencies_parameter det_interdependencies_the
J00-2004	J93-1003	p	In informal experiments described elsewhere -LRB- Melamed 1995 -RRB- I found that the G 2 statistic suggested by Dunning -LRB- 1993 -RRB- slightly outperforms 2	dobj_outperforms_2 advmod_outperforms_slightly nsubj_outperforms_statistic mark_outperforms_that appos_Dunning_1993 agent_suggested_Dunning vmod_statistic_suggested num_statistic_2 nn_statistic_G det_statistic_the ccomp_found_outperforms nsubj_found_I dep_found_Melamed prep_in_found_experiments num_Melamed_1995 advmod_described_elsewhere vmod_experiments_described amod_experiments_informal
J00-2004	J93-1003	o	Until now translation models have been evaluated either subjectively -LRB- e.g. White and O'Connell 1993 -RRB- or using relative metrics such as perplexity with respect to other models -LRB- Brown et al. 1993b -RRB-	nn_1993b_al. nn_1993b_et advmod_Brown_1993b amod_models_other dep_perplexity_Brown prep_with_respect_to_perplexity_models prep_such_as_metrics_perplexity amod_metrics_relative dobj_using_metrics num_O'Connell_1993 conj_and_White_O'Connell conj_or_e.g._using dep_e.g._O'Connell dep_e.g._White dep_subjectively_using dep_subjectively_e.g. preconj_subjectively_either advmod_evaluated_subjectively auxpass_evaluated_been aux_evaluated_have nsubjpass_evaluated_models prep_evaluated_Until nn_models_translation pcomp_Until_now
J00-2004	J93-1003	o	Bilingual lexicographers can work with bilingual concordancing software that can point them to instances of any link type induced from a bitext and display these instances sorted by their contexts -LRB- e.g. Simard Foster and Perrault 1993 -RRB-	num_Perrault_1993 conj_and_Simard_Perrault conj_and_Simard_Foster nn_Simard_e.g. poss_contexts_their agent_sorted_contexts vmod_instances_sorted det_instances_these dep_display_Perrault dep_display_Foster dep_display_Simard dobj_display_instances nsubj_display_that det_bitext_a prep_from_induced_bitext vmod_type_induced nn_type_link det_type_any prep_of_instances_type conj_and_point_display prep_to_point_instances dobj_point_them aux_point_can nsubj_point_that rcmod_software_display rcmod_software_point nn_software_concordancing amod_software_bilingual prep_with_work_software aux_work_can nsubj_work_lexicographers amod_lexicographers_Bilingual
J00-2004	J93-1003	o	The performance of cross-language information retrieval with a uniform T is likely to be limited in the same way as the performance of conventional information retrieval without term-frequency information i.e. where the system knows which terms occur in which documents but not how often -LRB- Buckley 1993 -RRB-	num_Buckley_1993 dep_often_Buckley advmod_often_how conj_negcc_documents_often dep_occur_often dep_occur_documents prep_in_occur_which nsubj_occur_terms det_terms_which ccomp_knows_occur nsubj_knows_system advmod_knows_where det_system_the rcmod_i.e._knows nn_information_term-frequency dep_retrieval_i.e. prep_without_retrieval_information nn_retrieval_information amod_retrieval_conventional prep_of_performance_retrieval det_performance_the prep_as_way_performance amod_way_same det_way_the prep_in_limited_way auxpass_limited_be aux_limited_to xcomp_likely_limited cop_likely_is nsubj_likely_performance amod_T_uniform det_T_a prep_with_retrieval_T nn_retrieval_information amod_retrieval_cross-language prep_of_performance_retrieval det_performance_The
J00-2004	J93-1003	o	-LRB- 1993b -RRB- this model is symmetric because both word bags are generated together from a joint probability distribution	nn_distribution_probability amod_distribution_joint det_distribution_a prep_from_generated_distribution advmod_generated_together auxpass_generated_are nsubjpass_generated_bags mark_generated_because nn_bags_word det_bags_both advcl_symmetric_generated cop_symmetric_is nsubj_symmetric_model dep_symmetric_1993b det_model_this
J00-3001	J93-1003	o	Dunning -LRB- 1993 -RRB- has called attention to the log-likelihood ratio G 2 as appropriate for the analysis of such contingency tables especially when such contingency tables concern very low frequency words	nn_words_frequency amod_words_low nsubj_words_concern advmod_words_when advmod_low_very nn_concern_tables nn_concern_contingency amod_concern_such advmod_when_especially nn_tables_contingency amod_tables_such prep_of_analysis_tables det_analysis_the num_G_2 prep_for_ratio_analysis prep_as_ratio_appropriate appos_ratio_G amod_ratio_log-likelihood det_ratio_the prep_to_attention_ratio advcl_called_words dep_called_attention aux_called_has nsubj_called_Dunning appos_Dunning_1993
J02-2003	J93-1003	o	Dunning -LRB- 1993 -RRB- argues for the use of G 2 rather than X 2 based on an analysis of the sampling distributions of G 2 and X 2 and results obtained when using the statistics to acquire highly associated bigrams	amod_bigrams_associated advmod_associated_highly dobj_acquire_bigrams aux_acquire_to det_statistics_the vmod_using_acquire dobj_using_statistics advmod_using_when advcl_obtained_using dep_results_obtained nsubj_results_Dunning num_X_2 conj_and_G_X num_G_2 prep_of_distributions_X prep_of_distributions_G nn_distributions_sampling det_distributions_the prep_of_analysis_distributions det_analysis_an num_X_2 conj_negcc_G_X num_G_2 prep_of_use_X prep_of_use_G det_use_the conj_and_argues_results pobj_argues_analysis prepc_based_on_argues_on prep_for_argues_use nsubj_argues_Dunning appos_Dunning_1993
J02-2003	J93-1003	o	8 An alternative formula for G 2 is given in Dunning -LRB- 1993 -RRB- but the two are equivalent	cop_equivalent_are nsubj_equivalent_two det_two_the appos_Dunning_1993 conj_but_given_equivalent prep_in_given_Dunning auxpass_given_is nsubjpass_given_formula dep_given_8 num_G_2 prep_for_formula_G amod_formula_alternative det_formula_An ccomp_``_equivalent ccomp_``_given
J02-2003	J93-1003	o	Dunning -LRB- 1993 -RRB- argues for the use of G 2 rather than X 2 based on the claim that the sampling distribution of G 2 approaches the true chi-square distribution quicker than the sampling distribution of X 2 However Agresti -LRB- 1996 page 34 -RRB- makes the opposite claim The sampling distributions of X 2 and G 2 get closer to chi-squared as the sample size n increasesThe convergence is quicker for X 2 than G 2 In addition Pedersen -LRB- 2001 -RRB- questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read -LRB- 1984 -RRB- who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic	amod_statistic_log-likelihood det_statistic_the prep_than_reliable_statistic advmod_reliable_more cop_reliable_is nsubj_reliable_statistic advmod_reliable_where nn_statistic_Pearson det_statistic_the rcmod_cases_reliable det_cases_some nsubj_are_cases expl_are_there mark_are_that ccomp_argue_are nsubj_argue_who dep_Read_1984 rcmod_Cressie_argue conj_and_Cressie_Read dobj_cites_Read dobj_cites_Cressie nsubj_cites_statistic nn_task_acquisition nn_task_bigram det_task_the prep_for_other_task det_other_the conj_and_preferred_cites prep_over_preferred_other auxpass_preferred_be aux_preferred_should nsubjpass_preferred_statistic mark_preferred_whether num_statistic_one ccomp_questions_cites ccomp_questions_preferred nn_questions_Pedersen appos_Pedersen_2001 num_G_2 num_X_2 prep_than_quicker_G prep_for_quicker_X cop_quicker_is nsubj_quicker_convergence mark_quicker_as nn_convergence_increasesThe nn_convergence_n nn_convergence_size nn_convergence_sample det_convergence_the prep_to_closer_chi-squared dep_get_questions prep_in_get_addition advcl_get_quicker acomp_get_closer nsubj_get_distributions num_G_2 conj_and_X_G num_X_2 prep_of_distributions_G prep_of_distributions_X nn_distributions_sampling det_distributions_The dep_claim_get amod_claim_opposite det_claim_the dobj_makes_claim nsubj_makes_Agresti advmod_makes_However prep_based_on_makes_claim num_page_34 num_page_1996 appos_Agresti_page num_X_2 prep_of_distribution_X nn_distribution_sampling det_distribution_the prep_than_quicker_distribution nsubj_quicker_distribution amod_distribution_chi-square amod_distribution_true det_distribution_the xcomp_approaches_quicker nsubj_approaches_distribution mark_approaches_that num_G_2 prep_of_distribution_G nn_distribution_sampling det_distribution_the ccomp_claim_approaches det_claim_the num_X_2 conj_negcc_G_X num_G_2 prep_of_use_X prep_of_use_G det_use_the parataxis_argues_makes prep_for_argues_use nsubj_argues_Dunning appos_Dunning_1993
J02-2003	J93-1003	o	Alternative Class-Based Estimation Methods The approaches used for comparison are that of Resnik -LRB- 1993 1998 -RRB- subsequently developed by Ribas -LRB- 1995 -RRB- and that of Li and Abe -LRB- 1998 -RRB- which has been adopted by McCarthy -LRB- 2000 -RRB-	appos_McCarthy_2000 agent_adopted_McCarthy auxpass_adopted_been aux_adopted_has nsubjpass_adopted_which rcmod_Li_adopted appos_Li_1998 conj_and_Li_Abe prep_of_that_Abe prep_of_that_Li appos_Ribas_1995 conj_and_developed_that prep_by_developed_Ribas advmod_developed_subsequently prep_of_developed_Resnik mark_developed_that dep_1993_1998 dep_Resnik_1993 ccomp_are_that ccomp_are_developed nsubj_are_approaches prep_for_used_comparison vmod_approaches_used det_approaches_The rcmod_Methods_are nn_Methods_Estimation amod_Methods_Class-Based amod_Methods_Alternative
J02-2003	J93-1003	o	The X 2 statistic is performing at least as well as G 2 and the results show that the average level of generalization is slightly higher for G 2 than X 2 This suggests a possible explanation for the results presented here and those in Dunning -LRB- 1993 -RRB- that the X 2 statistic provides a less conservative test when counts in the contingency table are low	cop_low_are nsubj_low_counts advmod_low_when nn_table_contingency det_table_the prep_in_counts_table amod_test_conservative det_test_a advmod_conservative_less advcl_provides_low dobj_provides_test nsubj_provides_statistic mark_provides_that num_statistic_2 nn_statistic_X det_statistic_the appos_Dunning_1993 prep_in_those_Dunning conj_and_presented_those advmod_presented_here vmod_results_those vmod_results_presented det_results_the ccomp_explanation_provides prep_for_explanation_results amod_explanation_possible det_explanation_a dobj_suggests_explanation nsubj_suggests_This num_X_2 num_G_2 prep_than_higher_X prep_for_higher_G advmod_higher_slightly cop_higher_is nsubj_higher_level mark_higher_that prep_of_level_generalization amod_level_average det_level_the ccomp_show_higher nsubj_show_results det_results_the num_G_2 advmod_well_as conj_and_at_show prep_as_at_G advmod_at_well pobj_at_least parataxis_performing_suggests prep_performing_show prep_performing_at aux_performing_is nsubj_performing_statistic num_statistic_2 nn_statistic_X det_statistic_The ccomp_``_performing
J02-4002	J93-1003	o	-LRB- The example paper we use throughout the article is F. Pereira N. Tishby and L. Lees Distributional Clustering of English Words -LSB- ACL-1993 cmp lg/9408011 -RSB- it was chosen because it is the paper most often cited within our collection -RRB-	poss_collection_our prep_within_cited_collection advmod_cited_often advmod_often_most vmod_paper_cited det_paper_the cop_paper_is nsubj_paper_it mark_paper_because advcl_chosen_paper auxpass_chosen_was nsubjpass_chosen_it nn_lg/9408011_cmp appos_ACL-1993_lg/9408011 amod_Words_English dep_Clustering_ACL-1993 prep_of_Clustering_Words nn_Clustering_Distributional nn_Clustering_Lees nn_Clustering_L. nn_Tishby_N. parataxis_Pereira_chosen conj_and_Pereira_Clustering conj_and_Pereira_Tishby nn_Pereira_F. cop_Pereira_is nsubj_Pereira_paper det_article_the prep_throughout_use_article nsubj_use_we rcmod_paper_use nn_paper_example det_paper_The
J02-4002	J93-1003	o	We measured associations using the log-likelihood measure -LRB- Dunning 1993 -RRB- for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table	nn_table_contingency num_table_22 det_table_a det_contingency_the prep_of_cell_contingency det_cell_each prep_into_converting_table dobj_converting_cell amod_class_semantic conj_and_category_class nn_category_target prep_of_combination_class prep_of_combination_category det_combination_each dobj_Dunning_1993 nn_measure_log-likelihood det_measure_the dobj_using_measure vmod_associations_using prepc_by_measured_converting prep_for_measured_combination dep_measured_Dunning dobj_measured_associations nsubj_measured_We
J02-4002	J93-1003	o	There are good reasons for using such a hand-crafted genre-specific verb lexicon instead of a general resource such as WordNet or Levins -LRB- 1993 -RRB- classes Many verbs used in the domain of scientific argumentation have assumed a specialized meaning which our lexicon readily encodes	advmod_encodes_readily nsubj_encodes_lexicon dobj_encodes_which poss_lexicon_our rcmod_meaning_encodes amod_meaning_specialized det_meaning_a dobj_assumed_meaning aux_assumed_have nsubj_assumed_verbs amod_argumentation_scientific prep_of_domain_argumentation det_domain_the prep_in_used_domain vmod_verbs_used amod_verbs_Many appos_Levins_1993 dep_WordNet_classes conj_or_WordNet_Levins parataxis_resource_assumed prep_such_as_resource_Levins prep_such_as_resource_WordNet amod_resource_general det_resource_a amod_lexicon_verb amod_lexicon_genre-specific amod_lexicon_hand-crafted det_lexicon_a predet_lexicon_such conj_negcc_using_resource dobj_using_lexicon prepc_for_reasons_resource prepc_for_reasons_using amod_reasons_good nsubj_are_reasons expl_are_There ccomp_``_are
J06-1005	J93-1003	o	Their systems output was an ordered list of possible parts according to some statistical metrics -LRB- e.g. the log-likelihood metric -LRB- Dunning 1993 -RRB- -RRB-	dep_Dunning_1993 dep_metric_Dunning amod_metric_log-likelihood det_metric_the dep_,_metric dep_-LRB-_e.g. amod_metrics_statistical det_metrics_some amod_parts_possible pobj_list_metrics prepc_according_to_list_to prep_of_list_parts amod_list_ordered det_list_an cop_list_was nsubj_list_output nn_output_systems poss_output_Their
J06-1005	J93-1003	o	5 The SemCor collection -LRB- Miller et al. 1993 -RRB- is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns verbs adverbs and adjectives have been manually tagged with their corresponding WordNet senses and part-of-speech tags using Brills tagger -LRB- 1995 -RRB-	appos_tagger_1995 nn_tagger_Brills dobj_using_tagger amod_tags_part-of-speech conj_and_senses_tags nn_senses_WordNet amod_senses_corresponding poss_senses_their xcomp_tagged_using prep_with_tagged_tags prep_with_tagged_senses advmod_tagged_manually auxpass_tagged_been aux_tagged_have nsubjpass_tagged_adjectives nsubjpass_tagged_adverbs nsubjpass_tagged_verbs nsubjpass_tagged_nouns prep_in_tagged_which conj_and_nouns_adjectives conj_and_nouns_adverbs conj_and_nouns_verbs det_nouns_the rcmod_sets_tagged num_sets_three prep_into_distributed_sets vmod_articles_distributed nn_articles_news num_articles_352 prep_of_consists_articles nsubj_consists_collection amod_Corpus_Brown det_Corpus_the conj_and_subset_consists prep_of_subset_Corpus det_subset_a cop_subset_is nsubj_subset_collection amod_Miller_1993 dep_Miller_al. nn_Miller_et dep_collection_Miller nn_collection_SemCor det_collection_The num_collection_5
J06-3001	J93-1003	o	Introduction The automated analysis of large corpora has many useful applications -LRB- Church and Mercer 1993 -RRB-	num_Mercer_1993 conj_and_Church_Mercer dep_applications_Mercer dep_applications_Church amod_applications_useful amod_applications_many dobj_has_applications nsubj_has_analysis amod_corpora_large prep_of_analysis_corpora amod_analysis_automated det_analysis_The rcmod_Introduction_has
J06-3001	J93-1003	o	Other corpus-based methods determine associations between words -LRB- Grefenstette 1992 Dunning 1993 Lin et al. 1998 -RRB- which yields a basis for computing thesauri or dictionaries of terminological expressions and multiword lexemes -LRB- Gaizauskas Demetriou and Humphreys 2000 Grefenstette 2001 -RRB-	num_Grefenstette_2001 num_Humphreys_2000 dep_Gaizauskas_Grefenstette conj_and_Gaizauskas_Humphreys conj_and_Gaizauskas_Demetriou dep_lexemes_Humphreys dep_lexemes_Demetriou dep_lexemes_Gaizauskas amod_lexemes_multiword conj_and_expressions_lexemes amod_expressions_terminological prep_of_dictionaries_lexemes prep_of_dictionaries_expressions conj_or_thesauri_dictionaries dobj_computing_dictionaries dobj_computing_thesauri prepc_for_basis_computing det_basis_a dobj_yields_basis nsubj_yields_which dep_al._1998 nn_al._et nn_al._Lin num_Dunning_1993 dep_Grefenstette_al. dep_Grefenstette_Dunning num_Grefenstette_1992 rcmod_words_yields appos_words_Grefenstette prep_between_associations_words dobj_determine_associations nsubj_determine_methods amod_methods_corpus-based amod_methods_Other ccomp_``_determine
J06-3001	J93-1003	o	From multilingual texts translation lexica can be generated -LRB- Gale and Church 1991 Kupiec 1993 Kumano and Hirakawa 1994 Boutsis Piperidis and Demiros 1999 Grefenstette 1999 -RRB-	num_Grefenstette_1999 num_Demiros_1999 conj_and_Boutsis_Demiros conj_and_Boutsis_Piperidis num_Hirakawa_1994 conj_and_Kumano_Hirakawa num_Kupiec_1993 num_Church_1991 dep_Gale_Grefenstette dep_Gale_Demiros dep_Gale_Piperidis dep_Gale_Boutsis conj_and_Gale_Hirakawa conj_and_Gale_Kumano conj_and_Gale_Kupiec conj_and_Gale_Church dep_generated_Kumano dep_generated_Kupiec dep_generated_Church dep_generated_Gale auxpass_generated_be aux_generated_can nsubjpass_generated_lexica prep_from_generated_texts nn_lexica_translation amod_texts_multilingual
J06-4003	J93-1003	o	In the usual case considered by Dunning -LRB- 1993 -RRB- and discussed by Manning and Sch utze -LRB- 1999 -RRB- the right-hand side of the equation is larger than the left-hand side	amod_side_left-hand det_side_the prep_than_larger_side cop_larger_is nsubj_larger_side prep_larger_discussed prep_larger_In det_equation_the prep_of_side_equation amod_side_right-hand det_side_the appos_utze_1999 nn_utze_Sch conj_and_Manning_utze prep_by_discussed_utze prep_by_discussed_Manning appos_Dunning_1993 agent_considered_Dunning vmod_case_considered amod_case_usual det_case_the conj_and_In_discussed pobj_In_case
J06-4003	J93-1003	o	As has been pointed out by Dunning -LRB- 1993 -RRB- the calculation of log assumes a binomial distribution	amod_distribution_binomial det_distribution_a dobj_assumes_distribution nsubj_assumes_calculation advcl_assumes_pointed prep_of_calculation_log det_calculation_the appos_Dunning_1993 agent_pointed_Dunning prt_pointed_out auxpass_pointed_been aux_pointed_has mark_pointed_As
J06-4003	J93-1003	o	A period should therefore be interpreted as an abbreviation marker and not as a sentence boundary marker if the two tokens surrounding it can indeed be considered as a collocation according to Dunnings -LRB- 1993 -RRB- original log-likelihood ratio amended with the one-sidedness constraint introduced in Section 2.2	num_Section_2.2 prep_in_introduced_Section vmod_constraint_introduced nn_constraint_one-sidedness det_constraint_the prep_with_amended_constraint vmod_ratio_amended amod_ratio_log-likelihood amod_ratio_original nn_ratio_Dunnings appos_Dunnings_1993 det_collocation_a pobj_considered_ratio prepc_according_to_considered_to prep_as_considered_collocation auxpass_considered_be advmod_considered_indeed aux_considered_can nsubjpass_considered_tokens mark_considered_if dobj_surrounding_it vmod_tokens_surrounding num_tokens_two det_tokens_the nn_marker_boundary nn_marker_sentence det_marker_a pobj_as_marker neg_as_not conj_and_marker_as nn_marker_abbreviation det_marker_an advcl_interpreted_considered prep_as_interpreted_as prep_as_interpreted_marker auxpass_interpreted_be advmod_interpreted_therefore aux_interpreted_should nsubjpass_interpreted_period det_period_A
J06-4003	J93-1003	o	For English we have used sections 03-06 of the WSJ portion of the Penn Treebank -LRB- Marcus Santorini and Marcinkiewicz 1993 -RRB- distributed by the Linguistic Data Consortium -LRB- LDC -RRB- which have frequently been used to evaluate sentence boundary detection systems before compare Section 7	num_Section_7 dobj_compare_Section nn_systems_detection nn_systems_boundary nn_systems_sentence advmod_evaluate_before dobj_evaluate_systems aux_evaluate_to xcomp_used_evaluate auxpass_used_been advmod_used_frequently aux_used_have nsubjpass_used_which rcmod_Consortium_used appos_Consortium_LDC nn_Consortium_Data nn_Consortium_Linguistic det_Consortium_the agent_distributed_Consortium num_Marcinkiewicz_1993 conj_and_Marcus_Marcinkiewicz conj_and_Marcus_Santorini dep_Treebank_Marcinkiewicz dep_Treebank_Santorini dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_WSJ det_portion_the vmod_sections_distributed prep_of_sections_portion num_sections_03-06 parataxis_used_compare dobj_used_sections aux_used_have nsubj_used_we prep_for_used_English
J06-4003	J93-1003	o	The usefulness of likelihood ratios for collocation detection has been made explicit by Dunning -LRB- 1993 -RRB- and has been confirmed by an evaluation of various collocation detection methods carried out by Evert and Krenn -LRB- 2001 -RRB-	appos_Krenn_2001 conj_and_Evert_Krenn prep_by_carried_Krenn prep_by_carried_Evert prt_carried_out nn_methods_detection nn_methods_collocation amod_methods_various prep_of_evaluation_methods det_evaluation_an dep_confirmed_carried agent_confirmed_evaluation auxpass_confirmed_been aux_confirmed_has nsubjpass_confirmed_usefulness appos_Dunning_1993 prep_by_explicit_Dunning conj_and_made_confirmed xcomp_made_explicit auxpass_made_been aux_made_has nsubjpass_made_usefulness nn_detection_collocation prep_for_ratios_detection nn_ratios_likelihood prep_of_usefulness_ratios det_usefulness_The
J06-4003	J93-1003	o	2.1 Likelihood Ratios in the Type-based Stage The log-likelihood ratio by Dunning -LRB- 1993 -RRB- tests whether the probability of a word is dependent on the occurrence of the preceding word type	nn_type_word amod_type_preceding det_type_the prep_of_occurrence_type det_occurrence_the prep_on_dependent_occurrence cop_dependent_is nsubj_dependent_probability mark_dependent_whether det_word_a prep_of_probability_word det_probability_the nn_tests_Dunning appos_Dunning_1993 dep_ratio_dependent prep_by_ratio_tests amod_ratio_log-likelihood det_ratio_The amod_Stage_Type-based det_Stage_the dep_Ratios_ratio prep_in_Ratios_Stage nn_Ratios_Likelihood num_Ratios_2.1
J07-2002	J93-1003	p	In our experiments we follow Lowe and McDonald -LRB- 2000 -RRB- in using the well-known log-likelihood ratio G 2 -LRB- Dunning 1993 -RRB-	dobj_Dunning_1993 dep_G_Dunning num_G_2 nn_G_ratio amod_G_log-likelihood amod_G_well-known det_G_the dobj_using_G appos_McDonald_2000 conj_and_Lowe_McDonald prepc_in_follow_using dobj_follow_McDonald dobj_follow_Lowe nsubj_follow_we prep_in_follow_experiments poss_experiments_our
J07-3003	J93-1003	o	One could use the estimated co-occurrences from a small sample to compute the test statistics most commonly Pearsons chi-squared test the likelihood ratio test Fishers exact test cosine similarity or resemblance -LRB- Jaccard coefficient -RRB- -LRB- Dunning 1993 Manning and Schutze 1999 Agresti 2002 Moore 2004 -RRB-	num_Moore_2004 num_Agresti_2002 num_Schutze_1999 dep_Manning_Moore conj_and_Manning_Agresti conj_and_Manning_Schutze dep_Dunning_Agresti dep_Dunning_Schutze dep_Dunning_Manning dobj_Dunning_1993 nn_coefficient_Jaccard vmod_resemblance_Dunning appos_resemblance_coefficient nn_similarity_cosine amod_test_exact nn_test_Fishers nn_test_ratio nn_test_likelihood det_test_the conj_or_test_resemblance conj_or_test_similarity conj_or_test_test conj_or_test_test amod_test_chi-squared nn_test_Pearsons advmod_test_commonly amod_test_most nn_statistics_test det_statistics_the dobj_compute_statistics aux_compute_to amod_sample_small det_sample_a amod_co-occurrences_estimated det_co-occurrences_the parataxis_use_resemblance parataxis_use_similarity parataxis_use_test parataxis_use_test parataxis_use_test vmod_use_compute prep_from_use_sample dobj_use_co-occurrences aux_use_could nsubj_use_One ccomp_``_use
J07-3003	J93-1003	o	1 Word associations -LRB- co-occurrences or joint frequencies -RRB- have a wide range of applications including speech recognition optical character recognition and information retrieval -LRB- IR -RRB- -LRB- Salton 1989 Church and Hanks 1991 Dunning 1993 Baeza-Yates and Ribeiro-Neto 1999 Manning and Schutze 1999 -RRB-	num_Schutze_1999 conj_and_Manning_Schutze num_Ribeiro-Neto_1999 conj_and_Baeza-Yates_Ribeiro-Neto num_Dunning_1993 dep_Church_1991 conj_and_Church_Hanks dep_Salton_Schutze dep_Salton_Manning conj_Salton_Ribeiro-Neto conj_Salton_Baeza-Yates conj_Salton_Dunning conj_Salton_Hanks conj_Salton_Church num_Salton_1989 dep_retrieval_Salton appos_retrieval_IR nn_retrieval_information nn_recognition_character amod_recognition_optical conj_and_recognition_retrieval conj_and_recognition_recognition nn_recognition_speech vmod_applications_including prep_of_range_applications amod_range_wide det_range_a dep_have_retrieval dep_have_recognition dep_have_recognition dobj_have_range nsubj_have_associations amod_frequencies_joint conj_or_co-occurrences_frequencies dep_associations_frequencies dep_associations_co-occurrences nn_associations_Word num_associations_1 rcmod_``_have
J94-4005	J93-1003	o	The third function is an original variant of the second the fourth is original and the fifth is prompted by the arguments of Dunning -LRB- 1993 -RRB-	appos_Dunning_1993 prep_of_arguments_Dunning det_arguments_the agent_prompted_arguments auxpass_prompted_is nsubjpass_prompted_fifth det_fifth_the conj_and_original_prompted cop_original_is nsubj_original_fourth det_fourth_the det_second_the parataxis_variant_prompted parataxis_variant_original prep_of_variant_second amod_variant_original det_variant_an cop_variant_is nsubj_variant_function amod_function_third det_function_The
J94-4005	J93-1003	o	Lexical collocation functions especially those determined statistically have recently attracted considerable attention in computational linguistics -LRB- Calzolari and Bindi 1990 Church and Hanks 1990 Sekine et al. 1992 Hindle and Rooth 1993 -RRB- mainly though not exclusively for use in disambiguation	prep_in_use_disambiguation pobj_for_use ccomp_,_for neg_exclusively_not mark_exclusively_though advcl_,_exclusively num_Rooth_1993 conj_and_Hindle_Rooth dep_al._1992 nn_al._et nn_al._Sekine num_Church_1990 conj_and_Church_Hanks num_Bindi_1990 dep_Calzolari_mainly dep_Calzolari_Rooth dep_Calzolari_Hindle conj_and_Calzolari_al. conj_and_Calzolari_Hanks conj_and_Calzolari_Church conj_and_Calzolari_Bindi dep_linguistics_al. dep_linguistics_Church dep_linguistics_Bindi dep_linguistics_Calzolari amod_linguistics_computational amod_attention_considerable prep_in_attracted_linguistics dobj_attracted_attention advmod_attracted_recently aux_attracted_have nsubj_attracted_functions advmod_determined_statistically vmod_those_determined advmod_those_especially appos_functions_those nn_functions_collocation amod_functions_Lexical
N03-1018	J93-1003	o	translation lexicon entries were scored according to the log likelihood ratio -LRB- Dunning 1993 -RRB- -LRB- cf.	dep_Dunning_1993 appos_ratio_Dunning nn_ratio_likelihood nn_ratio_log det_ratio_the dep_scored_cf. pobj_scored_ratio prepc_according_to_scored_to auxpass_scored_were nsubjpass_scored_entries nn_entries_lexicon nn_entries_translation
N03-1032	J93-1003	o	Dunning -LRB- 1993 -RRB- also used windows of size 2 which corresponds to word bigrams	nn_bigrams_word prep_to_corresponds_bigrams nsubj_corresponds_which num_size_2 rcmod_windows_corresponds prep_of_windows_size dobj_used_windows advmod_used_also nsubj_used_Dunning appos_Dunning_1993
N03-1032	J93-1003	o	3.3 Syntax based approach An alternative to the Window and Document-oriented approach is to use syntactical information -LRB- Grefenstette 1993 -RRB-	amod_Grefenstette_1993 dep_information_Grefenstette amod_information_syntactical dobj_use_information aux_use_to xcomp_is_use nsubj_is_alternative amod_approach_Document-oriented amod_approach_Window det_approach_the conj_and_Window_Document-oriented prep_to_alternative_approach det_alternative_An rcmod_approach_is amod_approach_based dep_approach_Syntax num_Syntax_3.3
N03-1032	J93-1003	o	Dunning -LRB- 1993 -RRB- used a likelihood ratio to test word similarity under the assumption that the words in text have a binomial distribution	amod_distribution_binomial det_distribution_a dobj_have_distribution nsubj_have_words mark_have_that prep_in_words_text det_words_the ccomp_assumption_have det_assumption_the prep_under_similarity_assumption nn_similarity_word dobj_test_similarity aux_test_to vmod_ratio_test nn_ratio_likelihood det_ratio_a dobj_used_ratio nsubj_used_Dunning appos_Dunning_1993
N03-1032	J93-1003	o	1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts -LRB- Dunning 1993 Church and Hanks 1990 Dagan et al. 1999 -RRB-	appos_al._1999 nn_al._et nn_al._Dagan conj_and_Church_al. conj_and_Church_1990 conj_and_Church_Hanks dep_Dunning_al. dep_Dunning_1990 dep_Dunning_Hanks dep_Dunning_Church appos_Dunning_1993 dep_texts_Dunning nn_texts_language amod_texts_natural nn_association_word prep_in_similarity_texts conj_or_similarity_association nn_similarity_word prep_of_strength_association prep_of_strength_similarity det_strength_the dobj_measure_strength aux_measure_to xcomp_proposed_measure auxpass_proposed_been aux_proposed_have nsubjpass_proposed_tests amod_tests_statistical amod_tests_different amod_tests_Many nn_tests_Introduction num_tests_1 ccomp_``_proposed
N04-1008	J93-1003	o	The chunker is trained on the answer side of the Training corpus in order to learn 2 and 3word collocations defined using the likelihood ratio of Dunning -LRB- 1993 -RRB-	appos_Dunning_1993 prep_of_ratio_Dunning nn_ratio_likelihood det_ratio_the dobj_using_ratio xcomp_defined_using num_collocations_3word num_collocations_2 conj_and_2_3word dobj_learn_collocations aux_learn_to dep_learn_order mark_learn_in nn_corpus_Training det_corpus_the prep_of_side_corpus nn_side_answer det_side_the dep_trained_defined advcl_trained_learn prep_on_trained_side auxpass_trained_is nsubjpass_trained_chunker det_chunker_The
N04-1038	J93-1003	o	BABAR uses the log-likelihood statistic -LRB- Dunning 1993 -RRB- to evaluate the strength of a co-occurrence relationship	nn_relationship_co-occurrence det_relationship_a prep_of_strength_relationship det_strength_the dobj_evaluate_strength aux_evaluate_to amod_Dunning_1993 dep_statistic_Dunning amod_statistic_log-likelihood det_statistic_the vmod_uses_evaluate dobj_uses_statistic nsubj_uses_BABAR
N07-1014	J93-1003	o	For comparing the sentence generator sample to the English sample we compute log-likelihood statistics -LRB- Dunning 1993 -RRB- on neighboring words that at least co-occur twice	advmod_co-occur_twice advmod_co-occur_at nsubj_co-occur_that pobj_at_least rcmod_words_co-occur amod_words_neighboring dep_Dunning_1993 appos_statistics_Dunning amod_statistics_log-likelihood prep_on_compute_words dobj_compute_statistics nsubj_compute_we prepc_for_compute_comparing nn_sample_English det_sample_the nn_sample_generator nn_sample_sentence det_sample_the prep_to_comparing_sample dobj_comparing_sample
N07-1014	J93-1003	o	Significant neighbor-based co-occurrence As discussed in -LRB- Dunning 1993 -RRB- it is possible to measure the amount of surprise to see two neighboring words in a corpus at a certain frequency under the assumption of independence	prep_of_assumption_independence det_assumption_the prep_under_frequency_assumption amod_frequency_certain det_frequency_a det_corpus_a prep_in_words_corpus amod_words_neighboring num_words_two prep_at_see_frequency dobj_see_words aux_see_to vmod_amount_see prep_of_amount_surprise det_amount_the dobj_measure_amount aux_measure_to xcomp_possible_measure cop_possible_is nsubj_possible_it advcl_possible_discussed dobj_Dunning_1993 dep_in_Dunning prep_discussed_in mark_discussed_As dep_co-occurrence_possible amod_co-occurrence_neighbor-based amod_co-occurrence_Significant
N07-1014	J93-1003	o	We use the log-likelihood ratio for determining significance as in -LRB- Dunning 1993 -RRB- but other measures are possible as well	advmod_well_as advmod_possible_well cop_possible_are nsubj_possible_measures amod_measures_other appos_Dunning_1993 pobj_in_Dunning pcomp_as_in prep_determining_as dobj_determining_significance amod_ratio_log-likelihood det_ratio_the conj_but_use_possible prepc_for_use_determining dobj_use_ratio nsubj_use_We
N07-3010	J93-1003	o	Schtze 1993 -RRB- is not suited to highly skewed distributions omni-present in natural language	amod_language_natural prep_in_omni-present_language amod_distributions_omni-present amod_distributions_skewed advmod_skewed_highly prep_to_suited_distributions neg_suited_not auxpass_suited_is nsubjpass_suited_1993 dep_1993_Schtze
N07-3010	J93-1003	p	Throughout the likelihood ratio -LRB- Dunning 1993 -RRB- is used as significance measure because of its stable performance in various evaluations yet many more measures are possible	cop_possible_are nsubj_possible_measures amod_measures_more amod_measures_many advmod_many_yet amod_evaluations_various prep_in_performance_evaluations amod_performance_stable poss_performance_its nn_measure_significance ccomp_used_possible prep_because_of_used_performance prep_as_used_measure auxpass_used_is nsubjpass_used_ratio advmod_used_Throughout amod_Dunning_1993 dep_ratio_Dunning nn_ratio_likelihood det_ratio_the
N09-1022	J93-1003	o	We then ranked the collected query pairs using loglikelihoodratio -LRB- LLR -RRB- -LRB- Dunning 1993 -RRB- whichmeasures the dependence between q1 and q2 within the context of web queries -LRB- Jones et al. 2006b -RRB-	appos_Jones_2006b dep_Jones_al. nn_Jones_et nn_queries_web prep_of_context_queries det_context_the conj_and_q1_q2 dep_dependence_Jones prep_within_dependence_context prep_between_dependence_q2 prep_between_dependence_q1 det_dependence_the dep_whichmeasures_dependence amod_Dunning_1993 dep_loglikelihoodratio_Dunning appos_loglikelihoodratio_LLR dobj_using_loglikelihoodratio nn_pairs_query amod_pairs_collected det_pairs_the dep_ranked_whichmeasures xcomp_ranked_using dobj_ranked_pairs advmod_ranked_then nsubj_ranked_We ccomp_``_ranked
P01-1025	J93-1003	o	The measures2 Mutual Information -LRB- a0a2a1 -RRB- -LRB- Church and Hanks 1989 -RRB- the log-likelihood ratio test -LRB- Dunning 1993 -RRB- two statistical tests t-test and a3a5a4 test and co-occurrence frequency are applied to two sets of data adjective-noun -LRB- AdjN -RRB- pairs and preposition-noun-verb -LRB- PNV -RRB- triples where the AMs are applied to -LRB- PN V -RRB- pairs	dep_pairs_PN appos_PN_V prep_to_applied_pairs auxpass_applied_are nsubjpass_applied_AMs advmod_applied_where det_AMs_the nn_triples_preposition-noun-verb appos_preposition-noun-verb_PNV rcmod_pairs_applied conj_and_pairs_triples amod_pairs_adjective-noun dep_adjective-noun_AdjN prep_of_sets_data num_sets_two dep_applied_triples dep_applied_pairs prep_to_applied_sets auxpass_applied_are nsubjpass_applied_frequency nsubjpass_applied_test nsubjpass_applied_a3a5a4 nsubjpass_applied_t-test nn_frequency_co-occurrence conj_and_t-test_frequency conj_and_t-test_test conj_and_t-test_a3a5a4 amod_tests_statistical num_tests_two dep_Dunning_1993 dep_test_Dunning nn_test_ratio amod_test_log-likelihood det_test_the amod_Church_1989 conj_and_Church_Hanks dep_Information_applied appos_Information_tests appos_Information_test dep_Information_Hanks dep_Information_Church appos_Information_a0a2a1 amod_Information_Mutual nn_Information_measures2 det_Information_The ccomp_``_Information
P01-1025	J93-1003	o	the remarks on the a3 a4 measure in -LRB- Dunning 1993 -RRB- -RRB-	dep_Dunning_1993 prep_in_measure_Dunning nn_measure_a4 nn_measure_a3 det_measure_the prep_on_remarks_measure det_remarks_the
P02-1054	J93-1003	o	Substituting the probabilities in the PMI formula with the previously introduced Web statistics we obtain a15a17a16a25a18a26a11a22a21 Qspa49a6a50a22a51a6a52 Aspa24 a15a17a16a25a18a26a11a22a21 Qspa24a56a55a57a15a33a16a19a18a26a11a6a21 Aspa24 a55 a38 a1a6a39a17a34a40a1a8a41a45a43a46a11 Maximal Likelihood Ratio -LRB- MLHR -RRB- is also used for word co-occurrence mining -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_mining_Dunning nn_mining_co-occurrence nn_mining_word prep_for_used_mining advmod_used_also auxpass_used_is nsubjpass_used_Ratio appos_Ratio_MLHR nn_Ratio_Likelihood amod_Ratio_Maximal nn_Ratio_a1a6a39a17a34a40a1a8a41a45a43a46a11 nn_Ratio_a38 nn_Ratio_a55 nn_Ratio_Aspa24 nn_Ratio_Qspa24a56a55a57a15a33a16a19a18a26a11a6a21 nn_Ratio_a15a17a16a25a18a26a11a22a21 nn_Ratio_Aspa24 nn_Ratio_Qspa49a6a50a22a51a6a52 nn_Ratio_a15a17a16a25a18a26a11a22a21 parataxis_obtain_used nsubj_obtain_we vmod_obtain_Substituting nn_statistics_Web amod_statistics_introduced det_statistics_the advmod_introduced_previously prep_with_formula_statistics nn_formula_PMI det_formula_the prep_in_probabilities_formula det_probabilities_the dobj_Substituting_probabilities
P02-1058	J93-1003	o	Proceedings of the 40th Annual Meeting of the Association for In a key step for locating important sentences NeATS computes the likelihood ratio -LRB- Dunning 1993 -RRB- to identify key concepts in unigrams bigrams and trigrams1 using the ontopic document collection as the relevant set and the off-topic document collection as the irrelevant set	amod_set_irrelevant det_set_the nn_collection_document amod_collection_off-topic det_collection_the prep_as_set_set conj_and_set_collection amod_set_relevant det_set_the nn_collection_document amod_collection_ontopic det_collection_the prep_as_using_collection prep_as_using_set dobj_using_collection conj_and_unigrams_trigrams1 conj_and_unigrams_bigrams amod_concepts_key prep_in_identify_trigrams1 prep_in_identify_bigrams prep_in_identify_unigrams dobj_identify_concepts aux_identify_to amod_Dunning_1993 vmod_ratio_identify dep_ratio_Dunning nn_ratio_likelihood det_ratio_the vmod_computes_using dobj_computes_ratio nsubj_computes_NeATS ccomp_computes_Proceedings amod_sentences_important dobj_locating_sentences prepc_for_step_locating amod_step_key det_step_a pobj_In_step pcomp_for_In det_Association_the prep_of_Meeting_Association amod_Meeting_Annual amod_Meeting_40th det_Meeting_the prep_Proceedings_for prep_of_Proceedings_Meeting
P03-1012	J93-1003	o	These methods often involve using a statistic such as 2 -LRB- Gale and Church 1991 -RRB- or the log likelihood ratio -LRB- Dunning 1993 -RRB- to create a score to measure the strength of correlation between source and target words	nn_words_target conj_and_source_words prep_between_correlation_words prep_between_correlation_source prep_of_strength_correlation det_strength_the dobj_measure_strength aux_measure_to vmod_score_measure det_score_a dobj_create_score aux_create_to amod_Dunning_1993 nn_ratio_likelihood nn_ratio_log det_ratio_the dep_Gale_1991 conj_and_Gale_Church dep_2_Dunning conj_or_2_ratio dep_2_Church dep_2_Gale prep_such_as_statistic_ratio prep_such_as_statistic_2 det_statistic_a vmod_using_create dobj_using_statistic xcomp_involve_using advmod_involve_often nsubj_involve_methods det_methods_These ccomp_``_involve
P03-1012	J93-1003	o	These constraints tie words in such a way that the space of alignments can not be enumerated as in IBM models 1 and 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_1_2 dep_models_2 dep_models_1 nn_models_IBM dep_in_Brown pobj_in_models pobj_as_in prep_enumerated_as auxpass_enumerated_be neg_enumerated_not aux_enumerated_can nsubjpass_enumerated_space mark_enumerated_that prep_of_space_alignments det_space_the ccomp_way_enumerated det_way_a amod_way_such prep_in_tie_way dobj_tie_words nsubj_tie_constraints det_constraints_These
P03-1017	J93-1003	o	Each element of the resulting vector was replaced with its log-likelihood value -LRB- see Definition 10 in Section 2.3 -RRB- which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_is_Dunning dep_pair_is nn_pair_co-occurrence det_pair_a dep_surprising_pair conj_or_surprising_distinctive advmod_surprising_how prep_of_estimate_distinctive prep_of_estimate_surprising det_estimate_an prep_as_considered_estimate auxpass_considered_be aux_considered_can nsubjpass_considered_which num_Section_2.3 num_Definition_10 dep_see_considered prep_in_see_Section dobj_see_Definition amod_value_log-likelihood poss_value_its dep_replaced_see prep_with_replaced_value auxpass_replaced_was nsubjpass_replaced_element amod_vector_resulting det_vector_the prep_of_element_vector det_element_Each
P03-1030	J93-1003	o	Our approach was to identify a parallel corpus of manually and automatically transcribed documents the TDT2 corpus and then use a statistical approach -LRB- Dunning 1993 -RRB- to identify tokens with significantly Table 5 Impact of recall and precision enhancing devices	nn_devices_enhancing nn_devices_precision nn_devices_recall conj_and_recall_precision prep_of_Impact_devices num_Table_5 advmod_Table_significantly prep_with_identify_Table dobj_identify_tokens aux_identify_to amod_Dunning_1993 dep_approach_Dunning amod_approach_statistical det_approach_a dobj_use_approach advmod_use_then dep_corpus_Impact vmod_corpus_identify conj_and_corpus_use nn_corpus_TDT2 det_corpus_the dep_corpus_was amod_documents_transcribed advmod_documents_automatically advmod_documents_manually conj_and_manually_automatically prep_of_corpus_documents amod_corpus_parallel det_corpus_a dobj_identify_corpus aux_identify_to xcomp_was_identify nsubj_was_approach poss_approach_Our
P03-1030	J93-1003	o	Second the significance of the K-S distance in case of the null hypothesis -LRB- data sets are drawn from same distribution -RRB- can be calculated -LRB- Press et al. 1993 -RRB-	amod_Press_1993 dep_Press_al. nn_Press_et dep_calculated_Press auxpass_calculated_be aux_calculated_can nsubjpass_calculated_significance advmod_calculated_Second amod_distribution_same prep_from_drawn_distribution auxpass_drawn_are nsubjpass_drawn_sets nn_sets_data amod_hypothesis_null det_hypothesis_the nn_distance_K-S det_distance_the rcmod_significance_drawn prep_in_case_of_significance_hypothesis prep_of_significance_distance det_significance_the ccomp_``_calculated
P03-2021	J93-1003	o	NeATS computes the likelihood ratio -LRB- Dunning 1993 -RRB- to identify key concepts in unigrams bigrams and trigrams and clusters these concepts in order to identify major subtopics within the main topic	amod_topic_main det_topic_the amod_subtopics_major prep_within_identify_topic dobj_identify_subtopics aux_identify_to dep_identify_order mark_identify_in dep_concepts_identify det_concepts_these conj_and_trigrams_clusters conj_and_unigrams_clusters conj_and_unigrams_trigrams conj_and_unigrams_bigrams prep_in_concepts_trigrams prep_in_concepts_bigrams prep_in_concepts_unigrams amod_concepts_key dobj_identify_concepts dobj_identify_concepts aux_identify_to amod_Dunning_1993 dep_ratio_Dunning nn_ratio_likelihood det_ratio_the vmod_computes_identify dobj_computes_ratio nsubj_computes_NeATS
P04-1022	J93-1003	o	In addition to collocation translation there is also some related work in acquiring phrase or term translations from parallel corpus -LRB- Kupiec 1993 Yamamoto and Matsumoto 2000 -RRB-	num_Matsumoto_2000 conj_and_Yamamoto_Matsumoto dep_Kupiec_Matsumoto dep_Kupiec_Yamamoto appos_Kupiec_1993 appos_corpus_Kupiec amod_corpus_parallel dep_phrase_translations conj_or_phrase_term prep_from_acquiring_corpus dobj_acquiring_term dobj_acquiring_phrase prepc_in_work_acquiring amod_work_related det_work_some nsubj_is_work advmod_is_also expl_is_there prep_in_addition_to_is_translation nn_translation_collocation
P04-1022	J93-1003	o	We have -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- 21 21 trictrictric trictritri erpercpercp ecrcpecp = = -LRB- 6 -RRB- Assumption 2 For an English triple tri e assume that i c only depends on -LCB- 1,2 -RCB- -RRB- -LRB- i i e and c r only depends on e r Equation -LRB- 6 -RRB- is rewritten as -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- 2211 21 ec trietrictrictritri rrpecpecp erpercpercpecp = = -LRB- 7 -RRB- Notice that -RRB- | -LRB- 11 ecp and -RRB- | -LRB- 22 ecp are translation probabilities within triples they are different from the unrestricted probabilities such as the ones in IBM models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM prep_in_ones_models det_ones_the prep_such_as_probabilities_ones amod_probabilities_unrestricted det_probabilities_the prep_from_different_probabilities cop_different_are nsubj_different_they prep_within_probabilities_triples nn_probabilities_translation cop_probabilities_are nsubj_probabilities_ecp dep_probabilities_| num_ecp_22 nn_|_| cc_ecp_and num_ecp_11 appos_|_ecp det_|_that dep_Notice_Brown dep_Notice_different dep_Notice_probabilities dep_Notice_7 dep_Notice_= dep_Notice_= dep_Notice_erpercpercpecp dep_Notice_rrpecpecp dep_Notice_trietrictrictritri dep_Notice_ec dep_Notice_21 dep_Notice_2211 dep_|_Notice nn_|_| dep_|_| appos_|_| dep_|_| nn_|_| nn_|_| prep_as_rewritten_| auxpass_rewritten_is appos_Equation_6 nn_Equation_r dep_Equation_e dep_depends_rewritten prep_on_depends_Equation advmod_depends_only nsubj_depends_r nn_r_c conj_and_e_depends nn_e_i dep_e_i num_e_1,2 prep_on_depends_depends prep_on_depends_e advmod_depends_only nsubj_depends_c nn_c_i det_c_that ccomp_assume_depends dep_assume_e nsubj_assume_tri mark_assume_For amod_tri_triple amod_tri_English det_tri_an num_Assumption_2 dep_=_Assumption dep_=_6 dep_=_= amod_ecrcpecp_= advcl_erpercpercp_assume dep_erpercpercp_ecrcpecp dep_trictritri_erpercpercp dep_trictrictric_trictritri dep_trictrictric_21 number_21_21 dep_|_trictrictric advmod_|_| nn_|_| appos_|_| appos_|_| dobj_have_| nsubj_have_We
P04-1066	J93-1003	o	To test whether a better set of initial parameter estimates can improve Model 1 alignment accuracy we use a heuristic model based on the loglikelihood-ratio -LRB- LLR -RRB- statistic recommended by Dunning -LRB- 1993 -RRB-	appos_Dunning_1993 agent_recommended_Dunning vmod_statistic_recommended amod_statistic_loglikelihood-ratio det_statistic_the dep_loglikelihood-ratio_LLR prep_on_based_statistic vmod_model_based nn_model_heuristic det_model_a dobj_use_model nsubj_use_we advcl_use_test nn_accuracy_alignment num_accuracy_1 nn_accuracy_Model dobj_improve_accuracy aux_improve_can nsubj_improve_set mark_improve_whether nn_estimates_parameter amod_estimates_initial prep_of_set_estimates amod_set_better det_set_a ccomp_test_improve aux_test_To
P04-3002	J93-1003	p	In order to filter some noise caused by the error alignment links we only retain those translation pairs whose translation probabilities are above a threshold 1 D 1 or co-occurring frequencies are above a threshold 2 When we train the IBM statistical word alignment model with a limited bilingual corpus in the specific domain we build another translation dictionary with the same method as for the dictionary But we adopt a different filtering strategy for the translation dictionary We use log-likelihood ratio to estimate the association strength of each translation pair because Dunning -LRB- 1993 -RRB- proved that log-likelihood ratio performed very well on small-scale data	amod_data_small-scale advmod_well_very prep_on_performed_data advmod_performed_well nsubj_performed_ratio mark_performed_that amod_ratio_log-likelihood ccomp_proved_performed nsubj_proved_Dunning mark_proved_because appos_Dunning_1993 nn_pair_translation det_pair_each prep_of_strength_pair nn_strength_association det_strength_the advcl_estimate_proved dobj_estimate_strength aux_estimate_to amod_ratio_log-likelihood xcomp_use_estimate dobj_use_ratio nsubj_use_We nn_dictionary_translation det_dictionary_the prep_for_strategy_dictionary amod_strategy_filtering amod_strategy_different det_strategy_a dobj_adopt_strategy nsubj_adopt_we det_dictionary_the pobj_method_dictionary prepc_as_for_method_for amod_method_same det_method_the nn_dictionary_translation det_dictionary_another prep_with_build_method dobj_build_dictionary nsubj_build_we advcl_build_train amod_domain_specific det_domain_the prep_in_corpus_domain amod_corpus_bilingual amod_corpus_limited det_corpus_a nn_model_alignment nn_model_word amod_model_statistical nn_model_IBM det_model_the prep_with_train_corpus dobj_train_model nsubj_train_we advmod_train_When num_threshold_2 det_threshold_a conj_but_are_adopt ccomp_are_build prep_above_are_threshold amod_frequencies_co-occurring conj_or_D_frequencies num_D_1 num_D_1 nn_D_threshold det_D_a prep_above_are_frequencies prep_above_are_D nsubj_are_probabilities nn_probabilities_translation poss_probabilities_whose rcmod_pairs_are nn_pairs_translation det_pairs_those parataxis_retain_use dep_retain_adopt dep_retain_are dobj_retain_pairs advmod_retain_only nsubj_retain_we prep_in_retain_order nn_links_alignment nn_links_error det_links_the agent_caused_links vmod_noise_caused det_noise_some nn_noise_filter prep_to_order_noise
P04-3019	J93-1003	o	Smadja -LRB- 1993 -RRB- also detailed techniques for collocation extraction and developed a program called XTRACT which is capable of computing flexible collocations based on elaborated statistical calculation	amod_calculation_statistical amod_calculation_elaborated prep_on_based_calculation vmod_collocations_based amod_collocations_flexible amod_collocations_computing prep_of_capable_collocations cop_capable_is nsubj_capable_which dep_called_XTRACT rcmod_program_capable vmod_program_called det_program_a dobj_developed_program nn_extraction_collocation conj_and_techniques_developed prep_for_techniques_extraction amod_techniques_detailed nn_techniques_Smadja advmod_detailed_also appos_Smadja_1993
P04-3019	J93-1003	p	Moreover log likelihood ratios are regarded as a more effective method to identify collocations especially when the occurrence count is very low -LRB- Dunning 1993 -RRB-	dep_Dunning_1993 dep_low_Dunning advmod_low_very cop_low_is nsubj_low_count advmod_low_when nn_count_occurrence det_count_the advmod_when_especially advcl_identify_low dobj_identify_collocations aux_identify_to vmod_method_identify amod_method_effective det_method_a advmod_effective_more prep_as_regarded_method auxpass_regarded_are nsubjpass_regarded_ratios advmod_regarded_Moreover nn_ratios_likelihood nn_ratios_log
P05-1058	J93-1003	o	2 Statistical Word Alignment According to the IBM models -LRB- Brown et al. 1993 -RRB- the statistical word alignment model can be generally represented as in Equation -LRB- 1 -RRB-	appos_Equation_1 pobj_in_Equation pcomp_as_in prep_represented_as advmod_represented_generally auxpass_represented_be aux_represented_can nsubjpass_represented_model nn_model_alignment nn_model_word amod_model_statistical det_model_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the rcmod_Alignment_represented dep_Alignment_Brown pobj_Alignment_models prepc_according_to_Alignment_to nn_Alignment_Word amod_Alignment_Statistical num_Alignment_2
P05-1058	J93-1003	o	= == = = m aj j m j aj l i i l i ii m j j mlajdeft en pp m ap 0:1 11 1 2 0 0 0 -RRB- -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | Pr -LRB- -RRB- | -LRB- 00 eef -LRB- 3 -RRB- 1 A cept is defined as the set of target words connected to a source word -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. appos_word_Brown nn_word_source det_word_a prep_to_connected_word vmod_words_connected nn_words_target prep_of_set_words det_set_the prep_as_defined_set auxpass_defined_is nsubjpass_defined_eef advmod_defined_| det_cept_A num_cept_1 dep_eef_cept dep_eef_3 num_eef_00 ccomp_Pr_defined appos_|_Pr advmod_|_| dep_|_| nn_|_| num_0_0 num_0_11 number_0_0 dep_0_2 number_2_1 number_11_0:1 dep_ap_0 nn_ap_m nn_ap_pp prep_en_mlajdeft_ap nn_mlajdeft_j nn_mlajdeft_j nn_mlajdeft_m appos_ii_| dep_ii_mlajdeft dep_i_ii dep_l_i nn_l_i nn_l_i amod_l_l dep_aj_l dep_j_aj dep_m_j dep_j_m dep_aj_j dep_m_aj dep_=_m dep_=_= amod_==_= dep_=_== ccomp_''_=
P05-1058	J93-1003	o	In order to filter the noise caused by the error alignment links we only retain those translation pairs whose log-likelihood ratio scores -LRB- Dunning 1993 -RRB- are above a threshold	det_threshold_a prep_above_are_threshold nsubj_are_scores amod_Dunning_1993 dep_scores_Dunning nn_scores_ratio amod_scores_log-likelihood poss_scores_whose rcmod_pairs_are nn_pairs_translation det_pairs_those dobj_retain_pairs advmod_retain_only nsubj_retain_we prep_in_retain_order nn_links_alignment nn_links_error det_links_the agent_caused_links vmod_noise_caused det_noise_the nn_noise_filter prep_to_order_noise
P05-1075	J93-1003	o	A variety of methods have been applied ranging from simple frequency -LRB- Justeson & Katz 1995 -RRB- modified frequency measures such as c-values -LRB- Frantzi Anadiou & Mima 2000 Maynard & Anadiou 2000 -RRB- and standard statistical significance tests such as the t-test the chi-squared test and loglikelihood -LRB- Church and Hanks 1990 Dunning 1993 -RRB- and information-based methods e.g. pointwise mutual information -LRB- Church & Hanks 1990 -RRB-	dep_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual nn_information_pointwise pobj_e.g._information amod_methods_information-based num_Dunning_1993 num_Hanks_1990 appos_Church_Dunning conj_and_Church_Hanks dep_loglikelihood_Hanks dep_loglikelihood_Church amod_test_chi-squared det_test_the det_t-test_the prep_such_as_tests_t-test nn_tests_significance amod_tests_statistical amod_tests_standard num_Frantzi_2000 conj_and_Frantzi_Anadiou conj_and_Frantzi_Maynard dep_Frantzi_2000 conj_and_Frantzi_Mima conj_and_Frantzi_Anadiou conj_and_c-values_tests appos_c-values_Anadiou appos_c-values_Maynard appos_c-values_Mima appos_c-values_Anadiou appos_c-values_Frantzi prep_measures_e.g. conj_and_measures_methods conj_and_measures_loglikelihood conj_and_measures_test prep_such_as_measures_tests prep_such_as_measures_c-values nn_measures_frequency amod_measures_modified dep_Justeson_1995 conj_and_Justeson_Katz appos_frequency_Katz appos_frequency_Justeson amod_frequency_simple prep_from_ranging_frequency dobj_applied_methods dobj_applied_loglikelihood dobj_applied_test dobj_applied_measures xcomp_applied_ranging auxpass_applied_been aux_applied_have nsubjpass_applied_variety prep_of_variety_methods det_variety_A
P05-1075	J93-1003	o	Dunning 1993 -RRB- or else -LRB- as with mutual information -RRB- eschew significance testing in favor of a generic information-theoretic approach	amod_approach_information-theoretic amod_approach_generic det_approach_a prep_of_favor_approach prep_in_testing_favor nn_testing_significance dep_testing_eschew num_testing_1993 amod_information_mutual pobj_with_information pcomp_as_with dep_1993_as advmod_1993_else cc_1993_or dobj_Dunning_testing
P06-1011	J93-1003	o	2.2 Using Log-Likelihood-Ratios to Estimate Word Translation Probabilities Our method for computing the probabilistic translation lexicon LLR-Lex is based on the the Log2http / / www.fjoch.com/GIZA++.html Likelihood-Ratio -LRB- LLR -RRB- statistic -LRB- Dunning 1993 -RRB- which has also been used by Moore -LRB- 2004a 2004b -RRB- and Melamed -LRB- 2000 -RRB- as a measure of word association	nn_association_word prep_of_measure_association det_measure_a appos_Melamed_2000 dep_2004a_2004b conj_and_Moore_Melamed dep_Moore_2004a prep_as_used_measure agent_used_Melamed agent_used_Moore auxpass_used_been advmod_used_also aux_used_has nsubjpass_used_which dep_Dunning_1993 rcmod_statistic_used appos_statistic_Dunning nn_statistic_LLR nn_statistic_Likelihood-Ratio nn_Likelihood-Ratio_www.fjoch.com/GIZA++.html det_Log2http_the det_Log2http_the prep_on_based_Log2http auxpass_based_is nsubjpass_based_method dep_lexicon_LLR-Lex nn_lexicon_translation amod_lexicon_probabilistic det_lexicon_the dobj_computing_lexicon prepc_for_method_computing poss_method_Our rcmod_Probabilities_based nn_Probabilities_Translation nn_Probabilities_Word nn_Probabilities_Estimate prep_to_Using_Probabilities dobj_Using_Log-Likelihood-Ratios dep_2.2_statistic vmod_2.2_Using ccomp_``_2.2
P06-1120	J93-1003	o	Finally the loglikelihood ratios test -LRB- henceforth LLR -RRB- -LRB- Dunning 1993 -RRB- is applied on each set of pairs	prep_of_set_pairs det_set_each prep_on_applied_set auxpass_applied_is nsubjpass_applied_test advmod_applied_Finally amod_Dunning_1993 nn_LLR_henceforth dep_test_Dunning appos_test_LLR nn_test_ratios amod_test_loglikelihood det_test_the
P06-2007	J93-1003	o	This metric tests the hypothesis that the probability of phrase is the same whether phrase has been seen or not by calculating the likelihood of the observed data under a binomial distribution using probabilities derived using each hypothesis -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_hypothesis_Dunning det_hypothesis_each dobj_using_hypothesis xcomp_derived_using vmod_probabilities_derived dobj_using_probabilities amod_distribution_binomial det_distribution_a prep_under_data_distribution amod_data_observed det_data_the prep_of_likelihood_data det_likelihood_the dobj_calculating_likelihood pcomp_by_calculating neg_by_not xcomp_seen_using conj_or_seen_by auxpass_seen_been aux_seen_has nsubjpass_seen_phrase mark_seen_whether ccomp_same_by ccomp_same_seen det_same_the cop_same_is nsubj_same_probability mark_same_that prep_of_probability_phrase det_probability_the ccomp_hypothesis_same det_hypothesis_the dep_tests_hypothesis amod_tests_metric det_tests_This dep_``_tests
P06-2016	J93-1003	o	This method uses mutual information and loglikelihood which Dunning -LRB- 1993 -RRB- used to calculate the dependency value between words	nn_value_dependency det_value_the prep_between_calculate_words dobj_calculate_value aux_calculate_to xcomp_used_calculate vmod_Dunning_used appos_Dunning_1993 dep_which_Dunning dep_information_which conj_and_information_loglikelihood amod_information_mutual dobj_uses_loglikelihood dobj_uses_information nsubj_uses_method det_method_This
P06-2020	J93-1003	o	To identify these terms,weusethelog-likelihoodstatisticsuggested by Dunning -LRB- Dunning 1993 -RRB- and first used in summarization by Lin and Hovy -LRB- Hovy and Lin 2000 -RRB-	num_Lin_2000 conj_and_Hovy_Lin dep_Lin_Lin dep_Lin_Hovy conj_and_Lin_Hovy prep_by_used_Hovy prep_by_used_Lin prep_in_used_summarization advmod_used_first num_Dunning_1993 appos_Dunning_Dunning det_terms,weusethelog-likelihoodstatisticsuggested_these conj_and_identify_used prep_by_identify_Dunning dobj_identify_terms,weusethelog-likelihoodstatisticsuggested aux_identify_To
P06-3002	J93-1003	o	Partitioning 2 Medium and low frequency words As noted in -LRB- Dunning 1993 -RRB- log-likelihood statistics are able to capture word bi-gram regularities	nn_regularities_bi-gram nn_regularities_word dobj_capture_regularities aux_capture_to xcomp_able_capture cop_able_are nsubj_able_statistics amod_statistics_log-likelihood rcmod_Dunning_able dep_Dunning_1993 prep_in_noted_Dunning mark_noted_As advcl_words_noted nn_words_frequency nn_words_low nn_words_Medium conj_and_Medium_low dep_Partitioning_words num_Partitioning_2
P06-3014	J93-1003	o	-LRB- ii -RRB- Apply some statistical tests such as the Binomial Hypothesis Test -LRB- Brent 1993 -RRB- and loglikelihood ratio score -LRB- Dunning 1993 -RRB- to SCCs to filter out false SCCs on the basis of their reliability and likelihood	conj_and_reliability_likelihood poss_reliability_their prep_of_basis_likelihood prep_of_basis_reliability det_basis_the amod_SCCs_false dep_Dunning_1993 appos_score_Dunning nn_score_ratio nn_score_loglikelihood num_Brent_1993 conj_and_Test_score appos_Test_Brent nn_Test_Hypothesis nn_Test_Binomial det_Test_the prep_such_as_tests_score prep_such_as_tests_Test amod_tests_statistical det_tests_some prep_on_Apply_basis prep_out_Apply_SCCs prep_to_Apply_filter prep_to_Apply_SCCs dobj_Apply_tests dep_Apply_ii
P06-3014	J93-1003	o	1 Introduction Robust statistical syntactic parsers made possible by new statistical techniques -LRB- Collins 1999 Charniak 2000 Bikel 2004 -RRB- and by the availability of large hand-annotated training corpora such as WSJ -LRB- Marcus et al. 1993 -RRB- and Switchboard -LRB- Godefrey et al. 1992 -RRB- have had a major impact on the field of natural language processing	nn_processing_language amod_processing_natural prep_of_field_processing det_field_the prep_on_impact_field amod_impact_major det_impact_a dobj_had_impact aux_had_have amod_Godefrey_1992 dep_Godefrey_al. nn_Godefrey_et amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_WSJ_Godefrey conj_and_WSJ_Switchboard dep_WSJ_Marcus prep_such_as_corpora_Switchboard prep_such_as_corpora_WSJ nn_corpora_training amod_corpora_hand-annotated amod_corpora_large prep_of_availability_corpora det_availability_the dep_Bikel_2004 appos_Charniak_2000 dep_Collins_Bikel dep_Collins_Charniak amod_Collins_1999 conj_and_techniques_availability appos_techniques_Collins amod_techniques_statistical amod_techniques_new dep_made_had prep_by_made_availability prep_by_made_techniques acomp_made_possible nsubj_made_parsers nn_parsers_syntactic amod_parsers_statistical amod_parsers_Robust nn_parsers_Introduction num_parsers_1
P07-1070	J93-1003	o	Such measures as mutual information -LRB- Turney 2001 -RRB- latent semantic analysis -LRB- Landauer et al. 1998 -RRB- log-likelihood ratio -LRB- Dunning 1993 -RRB- have been proposed to evaluate word semantic similarity based on the co-occurrence information on a large corpus	amod_corpus_large det_corpus_a prep_on_information_corpus nn_information_co-occurrence det_information_the prep_on_based_information vmod_similarity_based amod_similarity_semantic nn_similarity_word dobj_evaluate_similarity aux_evaluate_to xcomp_proposed_evaluate auxpass_proposed_been aux_proposed_have nsubjpass_proposed_analysis amod_Dunning_1993 appos_ratio_Dunning amod_ratio_log-likelihood amod_Landauer_1998 dep_Landauer_al. nn_Landauer_et appos_analysis_ratio dep_analysis_Landauer amod_analysis_semantic amod_analysis_latent num_Turney_2001 appos_information_Turney amod_information_mutual rcmod_measures_proposed prep_as_measures_information amod_measures_Such
P09-2017	J93-1003	o	Log-likelihood ratio -LRB- G2 -RRB- -LRB- Dunning 1993 -RRB- with respect to a large reference corpus Web 1T 5-gram Corpus -LRB- Brants and Franz 2006 -RRB- is used to capture the contextually relevant nouns	amod_nouns_relevant det_nouns_the advmod_relevant_contextually dobj_capture_nouns aux_capture_to xcomp_used_capture auxpass_used_is nsubjpass_used_Corpus dep_used_ratio dep_Brants_2006 conj_and_Brants_Franz appos_Corpus_Franz appos_Corpus_Brants nn_Corpus_5-gram nn_Corpus_1T nn_Corpus_Web nn_corpus_reference amod_corpus_large det_corpus_a amod_Dunning_1993 prep_with_respect_to_ratio_corpus dep_ratio_Dunning appos_ratio_G2 amod_ratio_Log-likelihood
P09-2062	J93-1003	o	We compute log-likelihood significance between features and target nouns -LRB- as in -LRB- Dunning 1993 -RRB- -RRB- and keep only the most significant 200 features per target word	nn_word_target prep_per_features_word num_features_200 amod_features_significant det_features_the advmod_features_only advmod_significant_most dobj_keep_features nsubj_keep_We dep_Dunning_1993 pobj_in_Dunning pcomp_as_in prep_nouns_as nn_nouns_target conj_and_features_nouns prep_between_significance_nouns prep_between_significance_features amod_significance_log-likelihood conj_and_compute_keep dobj_compute_significance nsubj_compute_We ccomp_``_keep ccomp_``_compute
P95-1054	J93-1003	o	Many researchers -LRB- -LRB- Smadja 1991 -RRB- -LRB- Srihari & Baltus 1993 -RRB- -RRB- have suggested that the informationtheoretic notion of mutual information score -LRB- MIS -RRB- directly captures the idea of context	prep_of_idea_context det_idea_the dobj_captures_idea advmod_captures_directly nsubj_captures_notion mark_captures_that appos_score_MIS nn_score_information amod_score_mutual prep_of_notion_score amod_notion_informationtheoretic det_notion_the ccomp_suggested_captures aux_suggested_have nsubj_suggested_Smadja dep_Srihari_1993 conj_and_Srihari_Baltus dep_Smadja_Baltus dep_Smadja_Srihari amod_Smadja_1991 vmod_researchers_suggested amod_researchers_Many
P95-1054	J93-1003	o	It forms a baseline for performance evaluations but is prone to sparse data problems -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_problems_Dunning nn_problems_data amod_problems_sparse prep_to_prone_problems cop_prone_is nsubj_prone_It nn_evaluations_performance prep_for_baseline_evaluations det_baseline_a conj_but_forms_prone dobj_forms_baseline nsubj_forms_It
P97-1009	J93-1003	o	64 Table 1 Subjects of employ with highest likelihood ratio word freq logA word freq logA bRG 64 50.4 plant 14 31.0 company 27 28.6 operation 8 23.0 industry 9 14.6 firm 8 13.5 pirate 2 12.1 unit 9 9.32 shift 3 8.48 postal service 2 7.73 machine 3 6.56 corporation 3 6.47 manufacturer 3 6.21 insurance company 2 6.06 aerospace 2 5.81 memory device 1 5.79 department 3 5.55 foreign office 1 5.41 enterprise 2 5.39 pilot 2 5.37 * ORG includes all proper names recognized as organizations The logA column are their likelihood ratios -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_ratios_Dunning nn_ratios_likelihood poss_ratios_their cop_ratios_are nsubj_ratios_bRG dep_ratios_ratio nn_ratios_likelihood amod_ratios_highest nn_column_logA det_column_The dobj_recognized_column prep_as_recognized_organizations nsubj_recognized_names amod_names_proper det_names_all ccomp_includes_recognized nsubj_includes_shift dep_includes_9.32 nsubj_includes_9 dep_includes_unit dep_includes_12.1 nsubj_includes_2 dep_includes_pirate dep_includes_13.5 nsubj_includes_8 dep_includes_firm dep_includes_14.6 nsubj_includes_9 dep_includes_industry dep_includes_23.0 nsubj_includes_8 dep_includes_operation dep_includes_28.6 nsubj_includes_27 dep_includes_company dep_includes_31.0 nsubj_includes_14 dep_includes_plant dep_includes_50.4 nsubj_includes_64 dep_ORG_* num_ORG_5.37 nn_ORG_pilot num_ORG_5.39 num_ORG_2 nn_ORG_enterprise num_ORG_5.41 num_ORG_1 number_5.37_2 dep_office_ORG amod_office_foreign num_office_5.55 nn_office_department num_office_5.79 num_office_1 number_5.55_3 dep_device_office nn_device_memory amod_device_5.81 nn_device_aerospace number_5.81_2 dep_6.06_device dep_2_6.06 dep_company_2 nn_company_insurance num_company_6.21 dep_3_company dep_manufacturer_3 num_manufacturer_6.47 num_manufacturer_3 nn_manufacturer_corporation num_manufacturer_6.56 num_manufacturer_3 nn_manufacturer_machine num_manufacturer_7.73 num_manufacturer_2 dep_service_manufacturer amod_service_postal num_service_8.48 number_8.48_3 dep_shift_service rcmod_bRG_includes nn_bRG_logA nn_bRG_freq nn_bRG_word nn_bRG_logA nn_bRG_freq nn_bRG_word prep_with_employ_ratios dep_employ_of nsubj_employ_Subjects dep_employ_Table num_Table_1 num_Table_64
P97-1009	J93-1003	o	The likelihood ratio is obtained by treating word and Ic as a bigram and computed with the formula in -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_in_Dunning prep_formula_in det_formula_the prep_with_computed_formula det_bigram_a conj_and_word_Ic conj_and_treating_computed prep_as_treating_bigram dobj_treating_Ic dobj_treating_word agent_obtained_computed agent_obtained_treating auxpass_obtained_is nsubjpass_obtained_ratio nn_ratio_likelihood det_ratio_The
P97-1046	J93-1003	o	-LRB- 1990 1993 -RRB- these models have non-uniform linguistically motivated structure at present coded by hand	prep_by_coded_hand dep_present_coded amod_structure_motivated amod_structure_non-uniform advmod_motivated_linguistically prep_at_have_present dobj_have_structure nsubj_have_models dep_have_1993 det_models_these num_1993_1990
P97-1046	J93-1003	o	Dunning 1993 -RRB- make use of both positive and negative instances of performing a task	det_task_a dobj_performing_task prepc_of_instances_performing amod_instances_negative amod_instances_positive conj_and_positive_negative preconj_positive_both prep_of_use_instances dobj_make_use nsubj_make_Dunning num_Dunning_1993
P97-1046	J93-1003	o	5 Effectiveness Comparison 5.1 English-Chinese ATIS Models Both the transfer and transducer systems were trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus -LRB- Hirschman et al. 1993 -RRB-	dep_1993_al. nn_al._et num_Hirschman_1993 nn_corpus_ATIS det_corpus_the prep_from_utterances_corpus amod_utterances_transcribed prep_of_translation_utterances nn_translation_Chinese nn_translation_English-to-Mandarin prep_on_evaluated_translation nsubjpass_evaluated_transfer dep_trained_Hirschman conj_and_trained_evaluated auxpass_trained_were nsubjpass_trained_systems nsubjpass_trained_transfer nn_systems_transducer conj_and_transfer_systems dep_transfer_the preconj_transfer_Both rcmod_Models_evaluated rcmod_Models_trained nn_Models_ATIS amod_Models_English-Chinese num_Models_5.1 dep_Comparison_Models nn_Comparison_Effectiveness num_Comparison_5
P97-1063	J93-1003	o	-LRB- Macklovitch 1994 Melamed 1996b -RRB- -RRB- concordancing for bilingual lexicography -LRB- Catizone et al. 1993 Gale & Church 1991 -RRB- computerassisted language learning corpus linguistics -LRB- Melby	dep_linguistics_Melby nn_linguistics_corpus nn_learning_language amod_learning_computerassisted dep_Gale_1991 conj_and_Gale_Church dep_Catizone_Church dep_Catizone_Gale appos_Catizone_1993 dep_Catizone_al. nn_Catizone_et amod_lexicography_bilingual dep_concordancing_Catizone prep_for_concordancing_lexicography appos_Melamed_1996b appos_Macklovitch_linguistics appos_Macklovitch_learning vmod_Macklovitch_concordancing dep_Macklovitch_Melamed dep_Macklovitch_1994 dep_''_Macklovitch
P97-1063	J93-1003	o	The co-occurrence relation can also be based on distance in a bitext space which is a more general representations of bitext correspondence -LRB- Dagan et al. 1993 Resnik & Melamed 1997 -RRB- or it can be restricted to words pairs that satisfy some matching predicate which can be extrinsic to the model -LRB- Melamed 1995 Melamed 1997 -RRB-	amod_Melamed_1997 dep_Melamed_Melamed appos_Melamed_1995 dep_model_Melamed det_model_the prep_to_extrinsic_model cop_extrinsic_be aux_extrinsic_can nsubj_extrinsic_which rcmod_predicate_extrinsic amod_predicate_matching det_predicate_some dobj_satisfy_predicate nsubj_satisfy_that rcmod_pairs_satisfy nn_pairs_words prep_to_restricted_pairs cop_restricted_be aux_restricted_can nsubj_restricted_it dep_Resnik_1997 conj_and_Resnik_Melamed conj_or_Dagan_restricted dep_Dagan_Melamed dep_Dagan_Resnik appos_Dagan_1993 dep_Dagan_al. nn_Dagan_et nn_correspondence_bitext prep_of_representations_correspondence amod_representations_general det_representations_a cop_representations_is nsubj_representations_which advmod_general_more rcmod_space_representations nn_space_bitext det_space_a parataxis_based_restricted parataxis_based_Dagan prep_in_based_space prep_on_based_distance auxpass_based_be advmod_based_also aux_based_can nsubjpass_based_relation nn_relation_co-occurrence det_relation_The advcl_``_based
P97-1063	J93-1003	o	For each co-occurring pair of word types u and v these likelihoods are initially set proportional to their co-occurrence frequency n -LRB- u v -RRB- and inversely proportional to their marginal frequencies n -LRB- u -RRB- and n -LRB- v -RRB- z following -LRB- Dunning 1993 -RRB- 2	dep_Dunning_2 dep_Dunning_1993 dep_following_Dunning ccomp_,_following dep_z_v nn_z_n conj_and_n_z appos_n_u dep_frequencies_z dep_frequencies_n amod_frequencies_marginal poss_frequencies_their prep_to_proportional_frequencies advmod_proportional_inversely dep_u_v appos_n_u nn_n_frequency nn_n_co-occurrence poss_n_their conj_and_proportional_proportional prep_to_proportional_n acomp_set_proportional acomp_set_proportional advmod_set_initially auxpass_set_are nsubjpass_set_likelihoods dep_set_v det_likelihoods_these dep_types_u nn_types_word prep_of_pair_types nn_pair_co-occurring det_pair_each conj_and_For_set pobj_For_pair dep_``_set dep_``_For
P97-1063	J93-1003	o	1 Introduction Over the past decade researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation -LRB- Brown et al. 1988 Brown et al. 1990 Brown et al. 1993a -RRB-	nn_al._et nn_al._Brown num_Brown_1990 nn_Brown_al. nn_Brown_et appos_al._1993a dep_al._al. dep_al._Brown num_al._1988 nn_al._et amod_al._Brown nn_translation_machine prep_for_models_translation amod_models_statistical amod_models_sophisticated advmod_sophisticated_increasingly dep_series_al. prep_of_series_models det_series_a dobj_developed_series aux_developed_have nsubj_developed_researchers dep_developed_Introduction prep_at_researchers_IBM amod_decade_past det_decade_the prep_over_Introduction_decade num_Introduction_1
P97-1063	J93-1003	o	Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications including crummy MT on the World Wide Web -LRB- Church & I-Iovy 1993 -RRB- certain machine-assisted translation tools -LRB- e.g.	dep_tools_e.g. nn_tools_translation amod_tools_machine-assisted amod_tools_certain dep_Church_1993 conj_and_Church_I-Iovy appos_Web_I-Iovy appos_Web_Church nn_Web_Wide nn_Web_World det_Web_the prep_on_MT_Web amod_MT_crummy appos_applications_tools prep_including_applications_MT nn_applications_NLP amod_applications_multilingual amod_applications_many prep_for_preferable_applications nsubj_preferable_look-up conj_and_sufficient_preferable cop_sufficient_is nsubj_sufficient_look-up nn_lexicon_translation amod_lexicon_explicit det_lexicon_an dobj_using_lexicon vmod_look-up_using nn_look-up_Table
P98-1074	J93-1003	o	1 Introduction Early works -LRB- Gale and Church 1993 Brown et al. 1993 -RRB- and to a certain extent -LRB- Kay and R6scheisen 1993 -RRB- presented methods to ex ~ ' ~ ct bi ' _ ` i ~ gua	nn_gua_~ nn_gua_i dep___gua nn___bi dep___ct dep___~ dep___methods dep___to dep___works nn_~_ex prep_to_methods_~ amod_methods_presented num_Kay_1993 conj_and_Kay_R6scheisen amod_extent_certain det_extent_a dep_to_R6scheisen dep_to_Kay pobj_to_extent num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Gale_Brown conj_and_Gale_1993 conj_and_Gale_Church conj_and_works_methods conj_and_works_to appos_works_1993 appos_works_Church appos_works_Gale amod_works_Early nn_works_Introduction num_works_1 ccomp_``__
P98-1074	J93-1003	o	Probabilities based on relative frequencies or derived fl ` om the measure defined in -LRB- Dunning 1993 -RRB- for example allow to take this fact into account	det_fact_this prep_into_take_account dobj_take_fact aux_take_to xcomp_allow_take nsubj_allow_measure dep_allow_om dep_allow_fl dep_allow_derived appos_Dunning_1993 prep_in_defined_Dunning prep_for_measure_example vmod_measure_defined det_measure_the conj_or_frequencies_allow amod_frequencies_relative pobj_Probabilities_allow pobj_Probabilities_frequencies prepc_based_on_Probabilities_on
P98-2182	J93-1003	o	For the final ranking we chose the log likelihood statistic outlined in Dunning -LRB- 1993 -RRB- which is based upon the co-occurrence counts of all nouns -LRB- see Dunning for details -RRB-	prep_for_see_details dobj_see_Dunning det_nouns_all prep_of_counts_nouns nn_counts_co-occurrence det_counts_the dep_based_see prep_upon_based_counts auxpass_based_is nsubjpass_based_which rcmod_Dunning_based appos_Dunning_1993 prep_in_outlined_Dunning vmod_statistic_outlined nn_statistic_likelihood nn_statistic_log det_statistic_the dobj_chose_statistic nsubj_chose_we prep_for_chose_ranking amod_ranking_final det_ranking_the
P99-1032	J93-1003	o	In this work model fit is reported in terms of the likelihood ratio statistic G 2 and its significance -LRB- Read and Cressie 1988 Dunning 1993 -RRB-	amod_Dunning_1993 dep_Cressie_1988 dep_Read_Dunning conj_and_Read_Cressie dep_significance_Cressie dep_significance_Read poss_significance_its num_G_2 nn_statistic_ratio nn_statistic_likelihood det_statistic_the prep_of_terms_statistic conj_and_reported_significance conj_and_reported_G prep_in_reported_terms auxpass_reported_is nsubjpass_reported_fit prep_in_reported_work nn_fit_model det_work_this
P99-1041	J93-1003	o	It is clear that Appendix B contains far fewer true non-compositional phrases than Appendix A. 7 Related Work There have been numerous previous research on extracting collocations from corpus e.g. -LRB- Choueka 1988 -RRB- and -LRB- Smadja 1993 -RRB-	amod_Smadja_1993 dep_Choueka_1988 prep_from_extracting_corpus dobj_extracting_collocations conj_and_research_Smadja appos_research_Choueka dep_research_e.g. prepc_on_research_extracting amod_research_previous amod_research_numerous cop_research_been aux_research_have expl_research_There amod_Work_Related num_Work_7 nn_Work_A. nn_Work_Appendix prep_than_phrases_Work amod_phrases_non-compositional amod_phrases_true amod_phrases_fewer advmod_fewer_far dep_contains_Smadja dep_contains_research dobj_contains_phrases nsubj_contains_B mark_contains_that nn_B_Appendix ccomp_clear_contains cop_clear_is nsubj_clear_It
P99-1041	J93-1003	o	We parsed a 125-million word newspaper corpus with Minipar 1 a descendent of Principar -LRB- Lin 1993 Lin 1994 -RRB- and extracted dependency relationships from the parsed corpus	amod_corpus_parsed det_corpus_the prep_from_relationships_corpus nn_relationships_dependency amod_relationships_extracted num_Lin_1994 conj_and_Lin_relationships dep_Lin_Lin appos_Lin_1993 dep_Principar_relationships dep_Principar_Lin prep_of_descendent_Principar det_descendent_a num_descendent_1 appos_Minipar_descendent nn_corpus_newspaper nn_corpus_word amod_corpus_125-million det_corpus_a prep_with_parsed_Minipar dobj_parsed_corpus nsubj_parsed_We ccomp_``_parsed
P99-1041	J93-1003	o	The frequency counts of dependency relationships are filtered with the loglikelihood ratio -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_ratio_Dunning nn_ratio_loglikelihood det_ratio_the prep_with_filtered_ratio auxpass_filtered_are nsubjpass_filtered_counts nn_relationships_dependency prep_of_counts_relationships nn_counts_frequency det_counts_The
P99-1041	J93-1003	o	Not only many combinations are found in the corpus many of them have very similar mutual information values to that of 318 Table 2 economic impact verb economic financial political social budgetary ecological economic economic economic economic economic economic economic economic economic object impact impact impact impact impact impact effect implication consequence significance fallout repercussion potential ramification risk mutual freq info 171 1.85 127 1.72 46 0.50 15 0.94 8 3.20 4 2.59 84 0.70 17 0.80 59 1.88 10 0.84 7 1.66 7 1.84 27 1.24 8 2.19 17 -0.33 nomial distribution can be accurately approximated by a normal distribution -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_distribution_Dunning amod_distribution_normal det_distribution_a agent_approximated_distribution advmod_approximated_accurately auxpass_approximated_be aux_approximated_can nsubjpass_approximated_repercussion amod_distribution_nomial num_distribution_-0.33 dep_distribution_17 dep_distribution_1.24 dep_distribution_1.88 num_distribution_1.85 dep_17_2.19 number_2.19_8 dep_1.24_27 number_27_1.84 dep_27_7 number_7_1.66 dep_7_7 dep_7_0.84 number_0.84_10 number_1.88_59 dep_1.88_0.80 dep_1.88_2.59 dep_1.88_0.94 number_0.80_17 dep_0.80_0.70 number_0.70_84 number_2.59_4 dep_2.59_3.20 number_3.20_8 number_0.94_15 dep_0.94_0.50 number_0.50_46 dep_0.50_1.72 number_1.72_127 number_1.85_171 dep_info_distribution nn_info_freq amod_info_mutual prep_risk_info dobj_ramification_risk dep_potential_ramification amod_repercussion_potential rcmod_fallout_approximated dep_significance_fallout dep_consequence_significance vmod_implication_consequence dobj_effect_implication dep_impact_effect dep_impact_impact dep_impact_impact dep_impact_impact dep_impact_impact dep_impact_impact dep_object_impact dep_economic_object amod_economic_economic amod_economic_economic amod_economic_economic amod_economic_economic amod_economic_economic amod_economic_economic amod_economic_economic amod_economic_economic amod_ecological_economic amod_budgetary_ecological amod_social_budgetary dep_political_social amod_financial_political amod_economic_financial dobj_verb_economic nsubj_verb_impact amod_impact_economic num_Table_2 num_Table_318 prep_of_that_Table nn_values_information amod_values_mutual amod_values_similar advmod_similar_very parataxis_have_verb prep_to_have_that dobj_have_values nsubj_have_many prep_of_many_them det_corpus_the parataxis_found_have prep_in_found_corpus auxpass_found_are nsubjpass_found_combinations amod_combinations_many advmod_many_only neg_many_Not
P99-1041	J93-1003	o	A total of 216 collocations were extracted shown in Appendix A We compared the collocations in Appendix A with the entries for the above 10 words in the NTC 's English Idioms Dictionary -LRB- henceforth NTC-EID -RRB- -LRB- Spears and Kirkpatrick 1993 -RRB- which contains approximately 6000 definitions of idioms	prep_of_definitions_idioms amod_definitions_6000 advmod_6000_approximately dobj_contains_definitions nsubj_contains_which dep_Spears_1993 conj_and_Spears_Kirkpatrick dobj_henceforth_NTC-EID nn_Dictionary_Idioms nn_Dictionary_English poss_Dictionary_NTC det_NTC_the prep_in_words_Dictionary num_words_10 amod_words_above det_words_the prep_for_entries_words det_entries_the nn_A_Appendix prep_in_collocations_A det_collocations_the dep_compared_contains dep_compared_Kirkpatrick dep_compared_Spears dep_compared_henceforth prep_with_compared_entries dobj_compared_collocations nsubj_compared_We nn_A_Appendix prep_in_shown_A parataxis_extracted_compared xcomp_extracted_shown auxpass_extracted_were nsubjpass_extracted_total num_collocations_216 prep_of_total_collocations det_total_A
P99-1051	J93-1003	o	Levin -LRB- 1993 -RRB- assumes that the syntactic realization of a verb 's arguments is directly correlated with its meaning -LRB- cf.	poss_meaning_its dep_correlated_cf. prep_with_correlated_meaning advmod_correlated_directly auxpass_correlated_is nsubjpass_correlated_realization mark_correlated_that possessive_arguments_'s poss_arguments_verb det_arguments_a prep_of_realization_arguments amod_realization_syntactic det_realization_the ccomp_assumes_correlated nsubj_assumes_Levin appos_Levin_1993
P99-1051	J93-1003	o	We also experimented with a method suggested by Brent -LRB- 1993 -RRB- which applies the binomial test on frame frequency data	nn_data_frequency nn_data_frame prep_on_test_data amod_test_binomial det_test_the dobj_applies_test nsubj_applies_which rcmod_Brent_applies appos_Brent_1993 agent_suggested_Brent vmod_method_suggested det_method_a prep_with_experimented_method advmod_experimented_also nsubj_experimented_We
P99-1051	J93-1003	o	For instance the to-PP frame is poorly ' represented in the syntactically annotated version of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_version_Treebank amod_version_annotated advmod_version_syntactically det_version_the dep_represented_Marcus prep_in_represented_version advmod_represented_poorly auxpass_represented_is nsubjpass_represented_frame prep_for_represented_instance nn_frame_to-PP det_frame_the
P99-1051	J93-1003	p	We preferred the log-likelihood ratio to other statistical scores such as the association ratio -LRB- Church and Hanks 1990 -RRB- or -LRB- 2 since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpussize -LRB- Dunning 1993 Daille 1996 -RRB-	amod_Daille_1996 dep_Dunning_Daille appos_Dunning_1993 appos_corpussize_Dunning conj_and_events_corpussize amod_events_rare prep_to_sensitive_corpussize prep_to_sensitive_events advmod_sensitive_less cop_sensitive_is nsubj_sensitive_it amod_words_co-occurring det_words_the prep_of_frequency_words det_frequency_the conj_and_takes_sensitive dobj_takes_frequency prep_into_takes_account advmod_takes_adequately nsubj_takes_it mark_takes_since rcmod_2_sensitive rcmod_2_takes dep_Church_1990 conj_and_Church_Hanks conj_or_ratio_2 appos_ratio_Hanks appos_ratio_Church nn_ratio_association det_ratio_the prep_such_as_scores_2 prep_such_as_scores_ratio amod_scores_statistical amod_scores_other prep_to_ratio_scores amod_ratio_log-likelihood det_ratio_the dobj_preferred_ratio nsubj_preferred_We ccomp_``_preferred
P99-1067	J93-1003	n	It is faster and more mnemonic than the one in Dunning -LRB- 1993 -RRB-	appos_Dunning_1993 prep_in_one_Dunning det_one_the prep_than_mnemonic_one advmod_mnemonic_more nsubj_mnemonic_It conj_and_faster_mnemonic cop_faster_is nsubj_faster_It
P99-1067	J93-1003	o	However in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case -LRB- see also Grefenstette 1993 -RRB- so we did not try to include these methods in our system	poss_system_our det_methods_these prep_in_include_system dobj_include_methods aux_include_to xcomp_try_include neg_try_not aux_try_did nsubj_try_we mark_try_so dep_Grefenstette_1993 dep_also_Grefenstette advcl_see_try advmod_see_also amod_case_monolingual det_case_the prep_to_applied_case advmod_applied_when advcl_described_applied advmod_described_here vmod_approach_described det_approach_the prep_than_results_approach amod_results_better advmod_better_significantly nn_lead_decomposition nn_lead_value amod_lead_singular dep_analysis_see prep_to_analysis_results conj_nor_analysis_lead amod_analysis_syntactical preconj_analysis_neither amod_words_related conj_and_synonyms_words prep_of_computation_words prep_of_computation_synonyms det_computation_the pobj_at_least dep_that_lead dep_that_analysis prep_for_that_computation advmod_that_at dep_found_that nsubj_found_we prep_found_unpublished prep_found_in advmod_found_However pobj_in_work conj_yet_in_unpublished
P99-1067	J93-1003	p	They were based on mutual information -LRB- Church & Hanks 1989 -RRB- conditional probabilities -LRB- Rapp 1996 -RRB- or on some standard statistical tests such as the chi-square test or the loglikelihood ratio -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_ratio_Dunning nn_ratio_loglikelihood det_ratio_the conj_or_test_ratio amod_test_chi-square det_test_the prep_such_as_tests_ratio prep_such_as_tests_test amod_tests_statistical amod_tests_standard det_tests_some pobj_on_tests appos_Rapp_1996 conj_or_probabilities_on dep_probabilities_Rapp amod_probabilities_conditional dep_Church_1989 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual dep_based_on dep_based_probabilities prep_on_based_information auxpass_based_were nsubjpass_based_They ccomp_``_based
W00-0901	J93-1003	o	Dunning -LRB- 1993 -RRB- reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts	amod_texts_smaller prep_for_alternative_texts advmod_alternative_better det_alternative_a cop_alternative_is nsubj_alternative_analysis mark_alternative_that amod_distributions_multinomial amod_distributions_binomial det_distributions_the conj_or_binomial_multinomial prep_on_based_distributions vmod_analysis_based amod_analysis_parametric ccomp_suggests_alternative nn_analysis_text amod_analysis_statistical dobj_performing_analysis advmod_performing_when amod_distribution_normal det_distribution_a prep_of_assumption_distribution det_assumption_the advcl_rely_performing prep_on_rely_assumption neg_rely_not aux_rely_should nsubj_rely_we mark_rely_that conj_and_reports_suggests ccomp_reports_rely nn_reports_Dunning appos_Dunning_1993
W00-1325	J93-1003	o	The different approaches -LRB- e.g. Brent 991 1993 Ushioda et al. 1993 Briscoe and Carroll 1997 Manning 1993 Carroll and Rooth 1998 Gahl 1998 Lapata 1999 Sarkar and Zeman 2000 -RRB- vary largely according to the methods used and the number of SCFS being extracted	auxpass_extracted_being vmod_SCFS_extracted prep_of_number_SCFS det_number_the conj_and_used_number vmod_methods_number vmod_methods_used det_methods_the pobj_vary_methods prepc_according_to_vary_to advmod_vary_largely dep_vary_e.g. num_Lapata_1999 num_Gahl_1998 num_Manning_1993 num_Ushioda_1993 nn_Ushioda_al. nn_Ushioda_et dep_991_2000 conj_and_991_Zeman conj_and_991_Sarkar conj_and_991_Lapata conj_and_991_Gahl conj_and_991_1998 conj_and_991_Rooth conj_and_991_Carroll conj_and_991_Manning conj_and_991_1997 conj_and_991_Carroll conj_and_991_Briscoe conj_and_991_Ushioda conj_and_991_1993 dep_Brent_Zeman dep_Brent_Sarkar dep_Brent_Lapata dep_Brent_Gahl dep_Brent_1998 dep_Brent_Rooth dep_Brent_Carroll dep_Brent_Manning dep_Brent_1997 dep_Brent_Carroll dep_Brent_Briscoe dep_Brent_Ushioda dep_Brent_1993 dep_Brent_991 dep_e.g._Brent dep_approaches_vary amod_approaches_different det_approaches_The
W00-1325	J93-1003	o	According to one account -LRB- Briscoe and Carroll 1997 -RRB- the majority of errors arise because of the statistical filtering process which is reported to be particularly unreliable for low frequency SCFs -LRB- Brent 1993 Briscoe and Carroll 1997 Manning 1993 Manning and Schiitze 1999 -RRB-	num_Manning_1993 appos_Briscoe_1999 conj_and_Briscoe_Schiitze conj_and_Briscoe_Manning conj_and_Briscoe_Manning conj_and_Briscoe_1997 conj_and_Briscoe_Carroll dep_Brent_Schiitze dep_Brent_Manning dep_Brent_Manning dep_Brent_1997 dep_Brent_Carroll dep_Brent_Briscoe num_Brent_1993 dep_SCFs_Brent nn_SCFs_frequency amod_SCFs_low prep_for_unreliable_SCFs advmod_unreliable_particularly cop_unreliable_be aux_unreliable_to xcomp_reported_unreliable auxpass_reported_is nsubjpass_reported_which rcmod_process_reported amod_process_filtering amod_process_statistical det_process_the prep_because_of_arise_process nsubj_arise_majority pobj_arise_account prepc_according_to_arise_to prep_of_majority_errors det_majority_the num_majority_1997 dep_majority_Carroll dep_majority_Briscoe conj_and_Briscoe_Carroll num_account_one
W00-1325	J93-1003	o	Adopting the SCF acquisition system of Briscoe and Carroll we have experimented with an alternative hypothesis test the binomial log-likelihood ratio -LRB- LLR -RRB- test -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_test_Dunning nn_test_ratio appos_ratio_LLR nn_ratio_log-likelihood amod_ratio_binomial det_ratio_the nn_test_hypothesis amod_test_alternative det_test_an conj_experimented_test prep_with_experimented_test aux_experimented_have nsubj_experimented_we vmod_experimented_Adopting conj_and_Briscoe_Carroll prep_of_system_Carroll prep_of_system_Briscoe nn_system_acquisition nn_system_SCF det_system_the dobj_Adopting_system
W00-1325	J93-1003	o	Brent -LRB- 1993 -RRB- estimated the error probabilities for each SCF experimentally from the behaviour of his SCF extractor which detected simple morpho-syntactic cues in the corpus data	nn_data_corpus det_data_the amod_cues_morpho-syntactic amod_cues_simple prep_in_detected_data dobj_detected_cues nsubj_detected_which rcmod_extractor_detected nn_extractor_SCF poss_extractor_his prep_of_behaviour_extractor det_behaviour_the det_SCF_each prep_from_probabilities_behaviour advmod_probabilities_experimentally prep_for_probabilities_SCF nn_probabilities_error det_probabilities_the dobj_estimated_probabilities nsubj_estimated_Brent appos_Brent_1993 ccomp_``_estimated
W00-1325	J93-1003	p	2.2.2 The Binomial Log Likelihood Ratio as a Statistical Filter Dunning -LRB- 1993 -RRB- demonstrates the benefits of the LLR statistic compared to Pearson 's chisquared on the task of ranking bigram data	nn_data_bigram amod_data_ranking prep_of_task_data det_task_the poss_chisquared_Pearson nn_statistic_LLR det_statistic_the prep_of_benefits_statistic det_benefits_the prep_on_demonstrates_task pobj_demonstrates_chisquared prepc_compared_to_demonstrates_to dobj_demonstrates_benefits nsubj_demonstrates_Ratio appos_Dunning_1993 nn_Dunning_Filter amod_Dunning_Statistical det_Dunning_a prep_as_Ratio_Dunning nn_Ratio_Likelihood nn_Ratio_Log amod_Ratio_Binomial det_Ratio_The num_Ratio_2.2.2
W01-0513	J93-1003	o	We then rank-order the P X | Y MI XY M Z Pr Z | Y MI ZY G092log -LSB- P X P Y P X P Y -RSB- f Y -LSB- P XY P XY -RSB- f XY -LSB- P XY P XY -RSB- f XY M iG13X X -RCB- jG13Y Y -RCB- -LRB- f ij G09 ij -RRB- 2 ij f XY G09 XY XY -LRB- 1G09 -LRB- XY / N -RRB- -RRB- f XY G09 XY f XY -LRB- 1G09 -LRB- f XY / N -RRB- -RRB- Table 1 Probabilistic Approaches METHOD FORMULA Frequency -LRB- Guiliano 1964 -RRB- f XY Pointwise Mutual Information -LRB- MI -RRB- -LRB- Fano 1961 Church and Hanks 1990 -RRB- log -LRB- P / PP -RRB- 2XY XY Selectional Association -LRB- Resnik 1996 -RRB- Symmetric Conditional Probability -LRB- Ferreira and Pereira 1999 -RRB- P / PP XY X Y 2 Dice Formula -LRB- Dice 1945 -RRB- 2 f / -LRB- f + f -RRB- XY X Y Log-likelihood -LRB- Dunning 1993 -LRB- Daille 1996 -RRB-	amod_Daille_1996 dep_Dunning_Daille appos_Dunning_1993 dep_Log-likelihood_Dunning nn_Log-likelihood_Y nn_Log-likelihood_X nn_Log-likelihood_XY dep_Log-likelihood_f dep_f_+ dep_f_f dep_f_P nsubj_f_Probability dep_f_Fano dep_f_Information dep_f_f dep_f_Frequency num_f_2 dep_Dice_1945 dep_Formula_f dep_Formula_Dice nn_Formula_Dice num_Formula_2 dep_Y_Formula nn_Y_X nn_Y_XY nn_Y_PP dep_P_Y dep_Ferreira_1999 conj_and_Ferreira_Pereira appos_Probability_Pereira appos_Probability_Ferreira amod_Probability_Conditional amod_Probability_Symmetric nn_Probability_Association dep_Resnik_1996 dep_Association_Resnik nn_Association_Selectional nn_Association_XY nn_Association_2XY dep_Association_P nn_Association_log dep_P_PP dep_Church_1990 conj_and_Church_Hanks dep_Fano_Hanks dep_Fano_Church appos_Fano_1961 appos_Information_MI nn_Information_Mutual nn_Information_Pointwise nn_Information_XY dep_Guiliano_1964 appos_Frequency_Guiliano nn_Frequency_FORMULA nn_Frequency_METHOD nn_Frequency_Approaches nn_Frequency_Probabilistic dep_Table_Log-likelihood num_Table_1 nn_Table_XY dep_XY_N nn_XY_f dep_1G09_XY appos_XY_1G09 nn_XY_f nn_XY_XY nn_XY_G09 nn_XY_XY dep_f_Table dep_XY_N dep_1G09_XY amod_XY_f appos_XY_1G09 nn_XY_XY nn_XY_G09 nn_XY_XY nn_XY_f nn_XY_ij num_XY_2 dep_ij_XY nn_ij_G09 nn_ij_ij nn_ij_f nn_jG13Y_X nn_iG13X_M nn_iG13X_XY nn_iG13X_f appos_XY_Y conj_XY_jG13Y dep_XY_iG13X nn_XY_P nn_XY_XY nn_XY_P nn_XY_f nn_XY_Y nn_XY_Z det_XY_the nn_XY_P nn_XY_XY nn_XY_P appos_Y_XY nn_Y_f nn_Y_G092log nn_Y_P nn_Y_X nn_Y_P nn_Y_Y nn_Y_P nn_Y_X nn_Y_P appos_G092log_Y nn_G092log_ZY nn_G092log_MI nn_G092log_Y num_G092log_| nn_Z_Pr nn_Z_Z nn_Z_M nn_Z_XY nn_Z_MI nn_Z_Y num_Z_| nn_Z_X nn_Z_P dep_rank-order_ij dep_rank-order_XY dobj_rank-order_XY advmod_rank-order_then nsubj_rank-order_We ccomp_``_rank-order
W01-0513	J93-1003	o	Since we need knowledge-poor Daille 1996 -RRB- induction we can not use human-suggested filtering Chi-squared -LRB- G24 -RRB- 2 -LRB- Church and Gale 1991 -RRB- Z-Score -LRB- Smadja 1993 Fontenelle et al. 1994 -RRB- Students t-Score -LRB- Church and Hanks 1990 -RRB- n-gram list in accordance to each probabilistic algorithm	amod_algorithm_probabilistic det_algorithm_each prep_to_list_algorithm prep_in_list_accordance nn_list_n-gram dep_list_Hanks dep_list_Church nn_list_t-Score nn_list_Students dep_Church_1990 conj_and_Church_Hanks dep_al._1994 nn_al._et dep_Fontenelle_al. dep_Smadja_1993 nn_Smadja_Z-Score dep_Smadja_Gale dep_Gale_1991 conj_and_Church_Smadja num_Church_2 amod_Church_Chi-squared amod_Church_filtering amod_Church_human-suggested appos_Chi-squared_G24 dobj_use_Smadja dobj_use_Church neg_use_not aux_use_can nsubj_use_we dep_induction_list dep_induction_Fontenelle rcmod_induction_use dep_induction_need dep_Daille_1996 amod_Daille_knowledge-poor dobj_need_Daille nsubj_need_we mark_need_Since
W01-0513	J93-1003	o	In particular we use a randomly-selected corpus the first five columns as information-like consisting of a 6.7 million word subset of the TREC Similarly since the last four columns share databases -LRB- DARPA 1993-1997 -RRB-	amod_DARPA_1993-1997 dep_share_DARPA dobj_share_databases nsubj_share_columns mark_share_since num_columns_four amod_columns_last det_columns_the det_TREC_the prep_of_subset_TREC nn_subset_word num_subset_million det_subset_a number_million_6.7 advcl_consisting_share advmod_consisting_Similarly prep_of_consisting_subset prep_as_columns_information-like num_columns_five amod_columns_first det_columns_the amod_corpus_randomly-selected det_corpus_a dep_use_consisting dobj_use_columns dobj_use_corpus nsubj_use_we prep_in_use_particular
W01-1411	J93-1003	o	As a measure of association we use the loglikelihood-ratio statistic recommended by Dunning -LRB- 1993 -RRB- which is the same statistic used by Melamed to initialize his models	poss_models_his dobj_initialize_models aux_initialize_to xcomp_used_initialize agent_used_Melamed vmod_statistic_used amod_statistic_same det_statistic_the cop_statistic_is nsubj_statistic_which rcmod_Dunning_statistic appos_Dunning_1993 agent_recommended_Dunning vmod_statistic_recommended amod_statistic_loglikelihood-ratio det_statistic_the dobj_use_statistic nsubj_use_we prep_as_use_measure prep_of_measure_association det_measure_a
W02-0906	J93-1003	o	Method Number of frames Number of verbs Linguistic resources F-Score -LRB- evaluation based on a gold standard -RRB- Coverage on a corpus C. Manning -LRB- 1993 -RRB- 19 200 POS tagger + simple finite state parser 58 T. Briscoe & J. Carroll -LRB- 1997 -RRB- 161 14 Full parser 55 A. Sarkar & D. Zeman -LRB- 2000 -RRB- 137 914 Annotated treebank 88 D. Kawahara et al.	nn_al._et dep_Kawahara_al. nn_Kawahara_D. num_Kawahara_88 nn_Kawahara_treebank amod_Kawahara_Annotated num_Kawahara_914 number_914_137 nn_Zeman_D. dep_Sarkar_Kawahara dep_Sarkar_2000 conj_and_Sarkar_Zeman nn_Sarkar_A. num_Sarkar_55 nn_Sarkar_parser amod_Sarkar_Full num_Sarkar_14 num_Sarkar_161 dep_1997_Zeman dep_1997_Sarkar nn_Carroll_J. dep_Briscoe_1997 conj_and_Briscoe_Carroll nn_Briscoe_T. num_Briscoe_58 dep_parser_Carroll dep_parser_Briscoe nn_parser_state amod_parser_finite amod_parser_simple conj_+_tagger_parser nn_tagger_POS num_tagger_200 number_200_19 dep_Manning_parser dep_Manning_tagger dep_Manning_1993 nn_Manning_C. nn_Manning_corpus det_Manning_a nn_Coverage_F-Score amod_standard_gold det_standard_a pobj_evaluation_standard prepc_based_on_evaluation_on dep_F-Score_evaluation nn_F-Score_resources amod_F-Score_Linguistic nn_F-Score_verbs prep_on_Number_Manning prep_of_Number_Coverage dep_frames_Number prep_of_Number_frames nn_Number_Method
W02-0909	J93-1003	o	For each cell in the contingency table the expected counts are mi j = ni + n + jn + + The measures are calculated as -LRB- Pedersen 1996 -RRB- 2 = i j -LRB- ni j mi j -RRB- 2 mi j LL = 2 i j log2 n 2i j mi j Log-likelihood ratios -LRB- Dunning 1993 -RRB- are more appropriate for sparse data than chi-square	amod_data_sparse prep_than_appropriate_chi-square prep_for_appropriate_data advmod_appropriate_more cop_appropriate_are nsubj_appropriate_ratios amod_Dunning_1993 dep_ratios_Dunning amod_ratios_Log-likelihood nn_ratios_j nn_ratios_mi nn_ratios_j nn_ratios_2i nn_ratios_n nn_ratios_log2 nn_ratios_j num_i_2 dep_=_i parataxis_LL_appropriate amod_LL_= dep_j_LL dep_mi_j amod_2_mi nn_j_mi dep_j_2 dep_j_j dep_ni_j dep_j_ni dep_=_i dep_2_j amod_2_= amod_Pedersen_1996 dep_as_Pedersen prep_calculated_as auxpass_calculated_are nsubjpass_calculated_measures det_measures_The conj_+_ni_+ conj_+_ni_jn conj_+_ni_n dep_=_2 dep_=_calculated dep_=_+ dep_=_jn dep_=_n dep_=_ni advmod_=_j dep_=_mi dep_=_are nsubj_=_counts prep_for_=_cell amod_counts_expected det_counts_the nn_table_contingency det_table_the prep_in_cell_table det_cell_each
W02-2001	J93-1003	o	In the latter case we use an unsupervised attachment disambiguation method based on the log-likelihood ratio -LRB- \ LLR Dunning -LRB- 1993 -RRB- -RRB-	appos_Dunning_1993 appos_LLR_Dunning num_LLR_\ dep_ratio_LLR amod_ratio_log-likelihood det_ratio_the nn_method_disambiguation nn_method_attachment amod_method_unsupervised det_method_an pobj_use_ratio prepc_based_on_use_on dobj_use_method nsubj_use_we prep_in_use_case amod_case_latter det_case_the
W02-2001	J93-1003	o	One aspect of VPCs that makes them dicult to extract -LRB- cited in e.g. Smadja -LRB- 1993 -RRB- -RRB- is that the verb and particle can be non-contiguous e.g. hand the paper in and battle right on	prep_right_on pobj_in_right conj_and_in_battle prep_paper_battle prep_paper_in det_paper_the dep_hand_paper nn_hand_e.g. appos_non-contiguous_hand cop_non-contiguous_be aux_non-contiguous_can nsubj_non-contiguous_particle nsubj_non-contiguous_verb det_non-contiguous_the conj_and_verb_particle prep_that_is_non-contiguous nsubj_is_aspect dep_Smadja_1993 dep_e.g._Smadja dep_in_e.g. prep_cited_in dep_dicult_cited prep_to_dicult_extract nsubj_dicult_them xcomp_makes_dicult nsubj_makes_that rcmod_VPCs_makes prep_of_aspect_VPCs num_aspect_One ccomp_``_is
W02-2001	J93-1003	o	One of the earliest attempts at extracting \ interrupted collocations -LRB- i.e. non-contiguous collocations including VPCs -RRB- was that of Smadja -LRB- 1993 -RRB-	appos_Smadja_1993 prep_of_that_Smadja dep_was_that dep_collocations_was prep_including_collocations_VPCs amod_collocations_non-contiguous advmod_collocations_i.e. amod_collocations_interrupted npadvmod_interrupted_\ dobj_extracting_collocations prepc_at_attempts_extracting amod_attempts_earliest det_attempts_the appos_One_collocations prep_of_One_attempts
W02-2001	J93-1003	o	2.2 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types -LRB- % -RRB- Corpus frequency Figure 1 Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl = 1 Brill 135135 1.000 0.177 0.301 Penn 667800 0.834 0.565 0.673 Table 1 POS-based extraction results the WSJ section of the Penn Treebank we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar -LRB- Grover et al. 1993 -RRB- and did a manual corpus search for each	prep_for_search_each nn_search_corpus amod_search_manual det_search_a dobj_did_search nsubj_did_we amod_Grover_1993 dep_Grover_al. nn_Grover_et nn_grammar_Tools nn_grammar_Language nn_grammar_Natural nn_grammar_Alvey det_grammar_the num_VPCs_200 prep_of_sample_VPCs amod_sample_random det_sample_a conj_and_took_did dep_took_Grover prep_from_took_grammar dobj_took_sample nsubj_took_we nn_Treebank_Penn det_Treebank_the prep_of_section_Treebank nn_section_WSJ det_section_the dobj_results_section nsubj_results_extraction amod_extraction_POS-based num_Table_1 num_Table_0.673 num_Table_0.565 dep_0.834_Table number_0.834_667800 dep_Penn_0.834 dep_0.301_Penn dep_0.177_0.301 dep_1.000_0.177 number_1.000_135135 amod_Brill_1.000 num_Brill_1 dep_=_Brill amod_Ffl_= nn_Ffl_Rec nn_Ffl_Prec parataxis_correctextracted_did parataxis_correctextracted_took parataxis_correctextracted_results dobj_correctextracted_Ffl nsubj_correctextracted_distribution nn_Tagger_WSJ det_Tagger_the prep_in_distribution_Tagger prep_of_distribution_VPCs nn_distribution_Frequency num_Figure_1 nn_Figure_frequency nn_Figure_Corpus nn_Figure_types appos_types_% nn_types_VPC num_types_70 num_types_0 num_types_30 advmod_types_namely dep_70_60 dep_60_50 number_50_40 dep_50_30 dep_30_20 number_20_10 num_0_40 number_40_35 number_30_25 dep_30_20 dep_30_5 dep_20_15 number_15_10 number_5_0 dep_extraction_correctextracted appos_extraction_Figure prep_for_targeted_extraction vmod_corpus_targeted det_corpus_the prep_of_frequency_VPCs amod_frequency_relative det_frequency_the prep_in_feel_corpus prep_for_feel_frequency det_feel_a dobj_get_feel aux_get_to dep_get_order mark_get_In advcl_occurrence_get nn_occurrence_Corpus num_occurrence_2.2
W02-2001	J93-1003	o	4 Method-2 Simple Chunk-based Extraction To overcome the shortcomings of the Brill tagger in identifying particles we next look to full chunk 2Note this is the same as the maximum span length of 5 used by Smadja -LRB- 1993 -RRB- and above the maximum attested NP length of 3 from our corpus study -LRB- see Section 2.2 -RRB-	num_Section_2.2 dobj_see_Section nn_study_corpus poss_study_our prep_of_length_3 nn_length_NP parataxis_attested_see prep_from_attested_study dobj_attested_length nsubj_attested_maximum mark_attested_above det_maximum_the appos_Smadja_1993 agent_used_Smadja vmod_5_used prep_of_length_5 nn_length_span nn_length_maximum det_length_the prep_as_same_length det_same_the cop_same_is nsubj_same_this nn_2Note_chunk amod_2Note_full prep_to_look_2Note advmod_look_next nsubj_look_we dobj_identifying_particles nn_tagger_Brill det_tagger_the prep_of_shortcomings_tagger det_shortcomings_the conj_and_overcome_attested ccomp_overcome_same ccomp_overcome_look prepc_in_overcome_identifying dobj_overcome_shortcomings aux_overcome_To nsubj_overcome_Extraction amod_Extraction_Chunk-based amod_Extraction_Simple dep_Method-2_attested dep_Method-2_overcome num_Method-2_4 dep_``_Method-2
W03-0201	J93-1003	o	Likelihood ratios are particularly useful when comparing common and rare events -LRB- Dunning 1993 Plaunt and Norgard 1998 -RRB- making them natural here given the rareness of most question categories and the frequency of contributions	prep_of_frequency_contributions det_frequency_the nn_categories_question amod_categories_most conj_and_rareness_frequency prep_of_rareness_categories det_rareness_the pobj_given_frequency pobj_given_rareness advmod_given_here amod_here_natural vmod_them_given dobj_making_them num_Norgard_1998 conj_and_Plaunt_Norgard dep_Dunning_Norgard dep_Dunning_Plaunt dobj_Dunning_1993 dep_events_Dunning amod_events_rare amod_events_common conj_and_common_rare vmod_comparing_making dobj_comparing_events advmod_comparing_when advcl_useful_comparing advmod_useful_particularly cop_useful_are nsubj_useful_ratios nn_ratios_Likelihood
W03-0314	J93-1003	o	For this present work we use Dunnings log-likelihood ratio statistics -LRB- Dunning 1993 -RRB- defined as follows sim = aloga + blogb + clogc + dlogd -LRB- a + b -RRB- log -LRB- a + b -RRB- -LRB- a + c -RRB- log -LRB- a + c -RRB- -LRB- b + d -RRB- log -LRB- b + d -RRB- -LRB- c + d -RRB- log -LRB- c + d -RRB- + -LRB- a + b + c + d -RRB- log -LRB- a + b + c + d -RRB- For each bilingual pattern EiJj we compute its similarity score and qualify it as a bilingual sequence-to-sequence correspondence if no equally strong or stronger association for monolingual constituent is found	auxpass_found_is nsubjpass_found_association mark_found_if amod_constituent_monolingual prep_for_association_constituent amod_association_stronger amod_association_strong neg_association_no conj_or_strong_stronger advmod_strong_equally nn_correspondence_sequence-to-sequence amod_correspondence_bilingual det_correspondence_a advcl_qualify_found prep_as_qualify_correspondence dobj_qualify_it nsubj_qualify_a nn_score_similarity poss_score_its conj_and_compute_qualify dobj_compute_score nsubj_compute_we prep_for_compute_EiJj nsubj_compute_a nn_EiJj_pattern amod_EiJj_bilingual det_EiJj_each conj_+_c_d pobj_+_d pobj_+_c prep_b_+ pobj_+_b prep_a_+ rcmod_log_qualify rcmod_log_compute dep_log_+ det_log_a conj_+_c_d dep_+_d dep_+_c amod_b_+ dep_+_b dep_-LRB-_log conj_+_c_d conj_+_log_-LRB- dep_log_d dep_log_c nn_log_log dep_log_d dep_log_b dep_log_a conj_+_c_d conj_+_b_d dep_log_d dep_log_c dep_log_d dep_log_b conj_+_b_d cc_a_+ dep_log_-LRB- dep_log_log dep_+_c conj_a_log dep_a_+ advcl_-LRB-_a dep_+_b dep_a_+ poss_-LRB-_a nn_log_b dep_log_+ det_log_a dep_aloga_log conj_+_aloga_dlogd conj_+_aloga_clogc conj_+_aloga_blogb dep_=_dlogd dep_=_clogc dep_=_blogb dep_=_aloga amod_sim_= mark_follows_as advcl_defined_follows dep_Dunning_1993 vmod_statistics_defined appos_statistics_Dunning nn_statistics_ratio amod_statistics_log-likelihood nn_statistics_Dunnings dep_use_sim dobj_use_statistics nsubj_use_we prep_for_use_work amod_work_present det_work_this
W03-1101	J93-1003	o	NeATS computes the likelihood ratio -LRB- Dunning 1993 -RRB- to identify key concepts in unigrams bigrams and trigrams and clusters these concepts in order to identify major subtopics within the main topic	amod_topic_main det_topic_the amod_subtopics_major prep_within_identify_topic dobj_identify_subtopics aux_identify_to dep_identify_order mark_identify_in dep_concepts_identify det_concepts_these dep_clusters_concepts conj_and_unigrams_trigrams conj_and_unigrams_bigrams prep_in_concepts_trigrams prep_in_concepts_bigrams prep_in_concepts_unigrams amod_concepts_key dobj_identify_concepts aux_identify_to amod_Dunning_1993 dep_ratio_Dunning nn_ratio_likelihood det_ratio_the conj_and_computes_clusters vmod_computes_identify dobj_computes_ratio nsubj_computes_NeATS
W03-1108	J93-1003	o	First word frequencies context word frequencies in surrounding positions -LRB- here three-words window -RRB- are computed following a statistics-based metrics the log-likelihood ratio -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_ratio_Dunning amod_ratio_log-likelihood det_ratio_the amod_metrics_statistics-based det_metrics_a dep_computed_ratio prep_following_computed_metrics auxpass_computed_are nsubjpass_computed_frequencies advmod_computed_First advmod_window_three-words advmod_window_here amod_positions_surrounding appos_frequencies_window prep_in_frequencies_positions nn_frequencies_word nn_frequencies_context appos_frequencies_frequencies nn_frequencies_word ccomp_``_computed
W03-1702	J93-1003	o	To make sense tagging more precise it is advisable to place constraint on the translation counterpart c of w. SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm -LRB- Melamed 1997 -RRB- and logarithmic likelihood ratio -LRB- Dunning 1993 -RRB-	dobj_Dunning_1993 dep_ratio_Dunning nn_ratio_likelihood amod_ratio_logarithmic num_Melamed_1997 conj_and_Algorithm_ratio appos_Algorithm_Melamed amod_Algorithm_Linking amod_Algorithm_Competitive det_Algorithm_the pobj_based_ratio pobj_based_Algorithm prep_w_based prep_with_linked_w auxpass_linked_been aux_linked_has nsubjpass_linked_that rcmod_c_linked nn_c_translations det_c_those advmod_c_only dobj_considers_c nn_SWAT_w. prep_of_c_SWAT nn_c_counterpart nn_c_translation det_c_the prep_on_constraint_c dobj_place_constraint aux_place_to dep_advisable_considers xcomp_advisable_place cop_advisable_is nsubj_advisable_it advcl_advisable_make advmod_precise_more acomp_tagging_precise vmod_sense_tagging dobj_make_sense aux_make_To
W03-1702	J93-1003	o	There is potential of developing Sense Definition Model to identify and represent semantic and stylistic differentiation reflected in the MRD glosses pointed out in DiMarco Hirst and Stede -LRB- 1993 -RRB-	appos_Stede_1993 conj_and_DiMarco_Stede conj_and_DiMarco_Hirst prep_in_pointed_Stede prep_in_pointed_Hirst prep_in_pointed_DiMarco prt_pointed_out dep_glosses_pointed det_MRD_the prep_in_reflected_MRD vmod_differentiation_reflected amod_differentiation_stylistic amod_differentiation_semantic conj_and_semantic_stylistic dobj_identify_differentiation conj_and_identify_represent aux_identify_to vmod_Model_represent vmod_Model_identify nn_Model_Definition nn_Model_Sense dobj_developing_Model prepc_of_potential_developing dep_is_glosses nsubj_is_potential expl_is_There ccomp_``_is
W03-1717	J93-1003	o	The approach is in the spirit of Smadja -LRB- 1993 -RRB- on retrieving collocations from text corpora but is more integrated with parsing	prep_with_integrated_parsing advmod_integrated_more auxpass_integrated_is nsubjpass_integrated_approach nn_corpora_text prep_from_collocations_corpora amod_collocations_retrieving appos_Smadja_1993 prep_on_spirit_collocations prep_of_spirit_Smadja det_spirit_the conj_but_is_integrated prep_in_is_spirit nsubj_is_approach det_approach_The ccomp_``_integrated ccomp_``_is
W03-1717	J93-1003	o	To prune away those pairs we used the log-likelihood-ratio algorithm -LRB- Dunning 1993 -RRB- to compute the degree of association between the verb and the noun in each pair	det_pair_each prep_in_noun_pair det_noun_the conj_and_verb_noun det_verb_the prep_between_association_noun prep_between_association_verb prep_of_degree_association det_degree_the dobj_compute_degree aux_compute_to amod_Dunning_1993 dep_algorithm_Dunning amod_algorithm_log-likelihood-ratio det_algorithm_the vmod_used_compute dobj_used_algorithm nsubj_used_we advcl_used_prune det_pairs_those dobj_prune_pairs advmod_prune_away aux_prune_To
W03-1802	J93-1003	p	a list of pilot terms ranked from the most representative of the corpus to the least thanks to the Loglikelihood coefficient introduced by -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_by_Dunning prep_introduced_by vmod_coefficient_introduced nn_coefficient_Loglikelihood det_coefficient_the prep_to_thanks_coefficient amod_thanks_least det_thanks_the det_corpus_the prep_of_representative_corpus advmod_representative_most det_representative_the prep_to_ranked_thanks prep_from_ranked_representative nsubj_ranked_list nn_terms_pilot prep_of_list_terms det_list_a
W03-1805	J93-1003	o	1 minority report 2 box office 3 scooby doo 4 sixth sense 5 national guard 6 bourne identity 7 air national guard 8 united states 9 phantom menace 10 special effects 11 hotel room 12 comic book 13 blair witch project 14 short story 15 real life 16 jude law 17 iron giant 18 bin laden 19 black people 20 opening weekend 21 bad guy 22 country bears 23 mans man 24 long time 25 spoiler space 26 empire strikes back 27 top ten 28 politically correct 29 white people 30 tv show 31 bad guys 32 freddie prinze jr 33 monsters ball 34 good thing 35 evil minions 36 big screen 37 political correctness 38 martial arts 39 supreme court 40 beautiful mind Figure 7 Result of re-ranking output from the phrase extension module 6.4 Revisiting unigram informativeness An alternative approach to calculate informativeness from the foreground LM and the background LM is just to take the ratio of likelihood scores a11 fga9a54a86 a15 a23 a11 bga9a54a86 a15 This is a smoothed version of relative frequency ratio which is commonly used to find subject-specific terms -LRB- Damerau 1993 -RRB-	amod_Damerau_1993 dep_terms_Damerau amod_terms_subject-specific dobj_find_terms aux_find_to xcomp_used_find advmod_used_commonly auxpass_used_is nsubjpass_used_which rcmod_ratio_used nn_ratio_frequency amod_ratio_relative prep_of_version_ratio amod_version_smoothed det_version_a cop_version_is nsubj_version_This nn_a15_bga9a54a86 nn_a15_a11 nn_a15_a23 nn_a15_a15 nn_a15_fga9a54a86 nn_a15_a11 nn_scores_likelihood prep_of_ratio_scores det_ratio_the dobj_take_ratio aux_take_to advmod_take_just aux_take_is nsubj_take_LM nsubj_take_background det_background_the conj_and_LM_take nn_LM_foreground det_LM_the prep_from_calculate_take prep_from_calculate_LM dobj_calculate_informativeness aux_calculate_to vmod_approach_calculate amod_approach_alternative det_approach_An dep_informativeness_approach amod_informativeness_unigram dobj_Revisiting_informativeness vmod_module_Revisiting num_module_6.4 nn_module_extension nn_module_phrase det_module_the prep_from_output_module amod_output_re-ranking appos_Result_a15 prep_of_Result_output num_Figure_7 nn_Figure_mind amod_Figure_beautiful num_Figure_40 nn_Figure_court nn_Figure_supreme num_Figure_39 nn_Figure_arts amod_Figure_martial num_Figure_38 nn_Figure_correctness amod_Figure_political num_Figure_37 nn_Figure_screen amod_Figure_big num_Figure_36 dep_minions_Figure amod_minions_evil num_minions_35 dep_thing_minions amod_thing_good num_thing_34 nn_thing_ball nn_thing_monsters num_thing_33 nn_thing_jr nn_thing_prinze nn_thing_freddie num_thing_32 dep_guys_thing amod_guys_bad num_guys_31 dep_show_guys nn_show_tv num_show_30 dep_people_show amod_people_white num_people_29 dep_correct_people advmod_correct_politically amod_28_correct num_28_ten quantmod_ten_top number_ten_27 pobj_back_28 advmod_strikes_back nsubj_strikes_empire tmod_strikes_time num_empire_26 nn_empire_space nn_empire_spoiler num_empire_25 amod_time_long num_time_24 rcmod_man_strikes nn_man_mans num_man_23 dobj_bears_man nsubj_bears_country num_country_22 nn_country_guy amod_country_bad num_country_21 nn_country_weekend amod_country_opening num_country_20 rcmod_people_bears amod_people_black num_people_19 amod_people_laden nn_people_bin num_people_18 amod_people_giant nn_people_iron num_people_17 nn_people_law nn_people_jude num_people_16 dep_life_people amod_life_real num_life_15 dep_story_life amod_story_short num_story_14 nn_project_witch nn_project_blair dep_book_story dep_book_project num_book_13 amod_book_comic num_book_12 dep_room_Result dep_room_book nn_room_hotel num_room_11 rcmod_effects_version dep_effects_room dep_special_effects amod_10_special tmod_menace_10 amod_menace_phantom num_menace_9 dep_states_menace amod_states_united num_states_8 dep_guard_states amod_guard_national nn_guard_air num_guard_7 nn_guard_identity nn_guard_bourne dep_6_guard dep_guard_6 amod_guard_national num_guard_5 nn_guard_sense amod_guard_sixth num_guard_4 nn_guard_doo nn_guard_scooby num_guard_3 nn_guard_office nn_guard_box num_guard_2 nn_guard_report nn_guard_minority num_guard_1
W03-1805	J93-1003	o	3 Related work Word collocation Various collocation metrics have been proposed including mean and variance -LRB- Smadja 1994 -RRB- the t-test -LRB- Church et al. 1991 -RRB- the chi-square test pointwise mutual information -LRB- MI -RRB- -LRB- Church and Hanks 1990 -RRB- and binomial loglikelihood ratio test -LRB- BLRT -RRB- -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_test_Dunning appos_test_BLRT nn_test_ratio nn_test_loglikelihood amod_test_binomial dep_Church_1990 conj_and_Church_Hanks appos_information_Hanks appos_information_Church appos_information_MI amod_information_mutual amod_information_pointwise amod_test_chi-square det_test_the amod_Church_1991 dep_Church_al. nn_Church_et conj_and_t-test_test conj_and_t-test_information appos_t-test_test dep_t-test_Church det_t-test_the dep_,_test dep_,_information dep_,_t-test amod_Smadja_1994 dep_mean_Smadja conj_and_mean_variance prep_including_proposed_variance prep_including_proposed_mean auxpass_proposed_been aux_proposed_have nsubjpass_proposed_metrics nn_metrics_collocation amod_metrics_Various nn_metrics_collocation nn_metrics_Word nn_metrics_work amod_metrics_Related num_metrics_3 ccomp_``_proposed
W03-1805	J93-1003	o	For our baseline we have selected the method based on binomial loglikelihood ratio test -LRB- BLRT -RRB- described in -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_in_Dunning prep_described_in vmod_test_described appos_test_BLRT nn_test_ratio nn_test_loglikelihood amod_test_binomial prep_on_based_test vmod_method_based det_method_the dobj_selected_method aux_selected_have nsubj_selected_we prep_for_selected_baseline poss_baseline_our rcmod_``_selected
W03-1806	J93-1003	o	For that purpose syntactical -LRB- Didier Bourigault 1993 -RRB- statistical -LRB- Frank Smadja 1993 Ted Dunning 1993 Gal Dias 2002 -RRB- and hybrid syntaxicostatistical methodologies -LRB- Batrice Daille 1996 JeanPhilippe Goldman et al. 2001 -RRB- have been proposed	auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methodologies nsubjpass_proposed_statistical advmod_proposed_syntactical prep_for_proposed_purpose dep_al._2001 nn_al._et nn_al._Goldman nn_al._JeanPhilippe dep_Daille_al. dep_Daille_1996 nn_Daille_Batrice amod_methodologies_syntaxicostatistical nn_methodologies_hybrid dep_Dias_2002 nn_Dias_Gal amod_Dunning_1993 nn_Dunning_Ted dep_Smadja_Dias conj_Smadja_Dunning dep_Smadja_1993 nn_Smadja_Frank dep_statistical_Daille conj_and_statistical_methodologies dep_statistical_Smadja dep_Bourigault_1993 nn_Bourigault_Didier dep_syntactical_Bourigault det_purpose_that
W03-1806	J93-1003	o	alpha 0 0.1 0.2 0.3 0.4 0.5 Freq = 2 13555 13093 12235 11061 10803 10458 Freq = 3 4203 3953 3616 3118 2753 2384 Freq = 4 1952 1839 1649 1350 1166 960 Freq = 5 1091 1019 917 743 608 511 Freq > 2 2869 2699 2488 2070 1666 1307 TOTAL 23670 22603 20905 18342 16996 15620 alpha 0.6 0.7 0.8 0.9 1.0 Freq = 2 10011 9631 9596 9554 9031 Freq = 3 2088 1858 1730 1685 1678 Freq = 4 766 617 524 485 468 Freq = 5 392 276 232 202 189 Freq > 2 1000 796 627 517 439 TOTAL 14257 13178 12709 12443 11805 Table 7 Number of extracted MWUs by frequency 6.2 Qualitative Analysis As many authors assess -LRB- Frank Smadja 1993 John Justeson and Slava Katz 1995 -RRB- deciding whether a sequence of words is a multiword unit or not is a tricky problem	amod_problem_tricky det_problem_a cop_problem_is conj_or_unit_problem conj_or_unit_not amod_unit_multiword det_unit_a cop_unit_is nsubj_unit_sequence mark_unit_whether prep_of_sequence_words det_sequence_a ccomp_deciding_problem ccomp_deciding_not ccomp_deciding_unit dep_Katz_1995 nn_Katz_Slava conj_and_Justeson_Katz nn_Justeson_John vmod_Smadja_deciding dep_Smadja_Katz dep_Smadja_Justeson amod_Smadja_1993 nn_Smadja_Frank nsubj_assess_authors mark_assess_As dep_assess_Number dep_assess_12709 dep_assess_13178 dep_assess_TOTAL nsubj_assess_439 dep_assess_1000 796 627 517 dep_assess_Freq dep_assess_= dep_assess_Freq dep_assess_468 dep_assess_= dep_assess_Freq dep_assess_1678 dep_assess_= dep_assess_Freq dep_assess_9631 9596 9554 9031 dep_assess_= dep_assess_Freq dep_assess_1.0 dep_assess_0.7 dep_assess_alpha dep_assess_15620 dep_assess_16996 dep_assess_18342 dep_assess_22603 nsubj_assess_TOTAL dep_assess_1307 dep_assess_1666 nsubj_assess_2869 2699 2488 2070 dep_assess_608 dep_assess_= dep_assess_Freq dep_assess_960 dep_assess_1166 amod_authors_many amod_Analysis_Qualitative num_Analysis_6.2 nn_Analysis_frequency amod_MWUs_extracted prep_by_Number_Analysis prep_of_Number_MWUs num_Table_7 num_Table_11805 num_Table_12443 dep_12709_Table number_13178_14257 number_1000 796 627 517_2 quantmod_1000 796 627 517_> num_Freq_189 num_Freq_392 276 232 202 number_392 276 232 202_5 dep_468_766 617 524 485 number_766 617 524 485_4 dep_1678_2088 1858 1730 1685 number_2088 1858 1730 1685_3 dep_9631 9596 9554 9031_10011 number_10011_2 dep_1.0_0.9 number_0.9_0.8 number_0.7_0.6 number_18342_20905 number_22603_23670 number_2869 2699 2488 2070_2 quantmod_2869 2699 2488 2070_> dep_2869 2699 2488 2070_Freq num_Freq_511 dep_608_1091 1019 917 743 number_1091 1019 917 743_5 dep_1166_1952 1839 1649 1350 number_1952 1839 1649 1350_4 dep_=_assess dep_Freq_Smadja amod_Freq_= dep_2384_Freq number_2384_2753 dep_2384_4203 3953 3616 3118 number_4203 3953 3616 3118_3 dep_=_2384 amod_Freq_= dep_10458_Freq dep_10458_10803 number_10803_11061 dep_10803_12235 number_12235_13093 dep_13555_10458 number_13555_2 dep_=_13555 amod_Freq_= dep_0.5_Freq dep_0.5_0.4 dep_0.4_0.3 dep_0.3_0.2 number_0.2_0.1 dep_0_0.5 dep_alpha_0
W03-1806	J93-1003	o	On the other hand purely statistical systems -LRB- Frank Smadja 1993 Ted Dunning 1993 Gal Dias 2002 -RRB- extract discriminating MWUs from text corpora by means of association measure regularities	nn_regularities_measure nn_regularities_association nn_corpora_text prep_from_MWUs_corpora amod_MWUs_discriminating nn_MWUs_extract dep_Dias_2002 nn_Dias_Gal amod_Dunning_1993 nn_Dunning_Ted dep_Smadja_Dias conj_Smadja_Dunning amod_Smadja_1993 nn_Smadja_Frank prep_by_means_of_systems_regularities dep_systems_MWUs dep_systems_Smadja amod_systems_statistical advmod_systems_purely amod_hand_other det_hand_the dep_``_systems prep_on_``_hand
W04-1122	J93-1003	o	Presently many systems -LRB- Tan et al 1999 -RRB- -LRB- Liu 2000 -RRB- -LRB- Song 1993 -RRB- -LRB- Luo et al 2001 -RRB- focus on online recognition of proper nouns and have achieved inspiring results in newscorpus but will be deteriorated in special text such as spoken corpus novels	conj_corpus_novels amod_corpus_spoken prep_such_as_text_corpus amod_text_special prep_in_deteriorated_text auxpass_deteriorated_be aux_deteriorated_will nsubjpass_deteriorated_achieved nsubjpass_deteriorated_systems advmod_deteriorated_Presently prep_in_results_newscorpus amod_results_inspiring dobj_achieved_results aux_achieved_have amod_nouns_proper prep_of_recognition_nouns amod_recognition_online prep_on_focus_recognition amod_Luo_2001 dep_Luo_al nn_Luo_et appos_Song_1993 dep_Liu_2000 amod_Tan_1999 dep_Tan_al nn_Tan_et conj_and_systems_achieved dep_systems_focus appos_systems_Luo appos_systems_Song appos_systems_Liu appos_systems_Tan amod_systems_many
W04-1122	J93-1003	o	Many statistical metrics have been proposed including pointwise mutual information -LRB- MI -RRB- -LRB- Church et al 1990 -RRB- mean and variance hypothesis testing -LRB- t-test chisquare test etc. -RRB- log-likelihood ratio -LRB- LR -RRB- -LRB- Dunning 1993 -RRB- statistic language model -LRB- Tomokiyo et al 2003 -RRB- and so on	advmod_on_so cc_so_and nn_al_et num_Tomokiyo_2003 appos_Tomokiyo_al appos_model_Tomokiyo nn_model_language nn_model_statistic amod_Dunning_1993 dep_ratio_Dunning appos_ratio_LR amod_ratio_log-likelihood amod_test_chisquare dep_t-test_etc. appos_t-test_test advmod_testing_on conj_testing_model conj_testing_ratio appos_testing_t-test nn_testing_hypothesis conj_and_mean_variance amod_Church_1990 dep_Church_al nn_Church_et dep_information_Church appos_information_MI amod_information_mutual amod_information_pointwise dep_proposed_testing dep_proposed_variance dep_proposed_mean prep_including_proposed_information auxpass_proposed_been aux_proposed_have nsubjpass_proposed_metrics amod_metrics_statistical amod_metrics_Many ccomp_``_proposed
W04-1122	J93-1003	o	Relative frequency ratio -LRB- RFR -RRB- of terms between two different corpora can also be used to discover domain-oriented multi-word terms that are characteristic of a corpus when compared with another -LRB- Damerau 1993 -RRB-	amod_Damerau_1993 dep_another_Damerau prep_with_compared_another advmod_compared_when det_corpus_a advcl_characteristic_compared prep_of_characteristic_corpus cop_characteristic_are nsubj_characteristic_that rcmod_terms_characteristic amod_terms_multi-word amod_terms_domain-oriented dobj_discover_terms aux_discover_to xcomp_used_discover auxpass_used_be advmod_used_also aux_used_can nsubjpass_used_ratio amod_corpora_different num_corpora_two prep_between_ratio_corpora prep_of_ratio_terms appos_ratio_RFR nn_ratio_frequency amod_ratio_Relative
W04-1122	J93-1003	o	3 Candidates extraction on Suffix array Suffix array -LRB- also known as String PATarray -RRB- -LRB- Manber et al 1993 -RRB- is a compact data structure to handle arbitrary-length strings and performs much powerful on-line string search operations such as the ones supported by PAT-tree but has less space overhead	nn_overhead_space amod_overhead_less dobj_has_overhead nsubj_has_extraction agent_supported_PAT-tree vmod_ones_supported det_ones_the prep_such_as_operations_ones nn_operations_search nn_operations_string amod_operations_on-line amod_operations_powerful amod_operations_much dobj_performs_operations nsubj_performs_extraction amod_strings_arbitrary-length dobj_handle_strings aux_handle_to conj_but_structure_has conj_and_structure_performs vmod_structure_handle nn_structure_data amod_structure_compact det_structure_a cop_structure_is nsubj_structure_extraction amod_Manber_1993 dep_Manber_al nn_Manber_et nn_PATarray_String prep_as_known_PATarray advmod_known_also nn_array_Suffix nn_array_array nn_array_Suffix dep_extraction_Manber dep_extraction_known prep_on_extraction_array nn_extraction_Candidates num_extraction_3
W04-1122	J93-1003	o	Candidate term Segment result of GPWS for one sentence in which term appears / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / Table 2 Examples of candidates eliminated by GPWS 5 Relative frequency ratio against background corpus Relative frequency ratio -LRB- RFR -RRB- is a useful method to be used to discover characteristic linguistic phenomena of a corpus when compared with another -LRB- Damerau 1993 -RRB-	amod_Damerau_1993 dep_another_Damerau prep_with_compared_another advmod_compared_when det_corpus_a prep_of_phenomena_corpus amod_phenomena_linguistic amod_phenomena_characteristic advcl_discover_compared dobj_discover_phenomena aux_discover_to xcomp_used_discover auxpass_used_be aux_used_to vmod_method_used amod_method_useful det_method_a cop_method_is nsubj_method_Examples appos_ratio_RFR nn_ratio_frequency amod_ratio_Relative nn_ratio_corpus nn_ratio_background prep_against_ratio_ratio nn_ratio_frequency amod_ratio_Relative num_ratio_5 nn_ratio_GPWS agent_eliminated_ratio vmod_candidates_eliminated prep_of_Examples_candidates dep_Table_method num_Table_2 dep_appears_Table nsubj_appears_term prep_in_appears_which rcmod_sentence_appears num_sentence_one prep_for_result_sentence prep_of_result_GPWS nn_result_Segment nn_result_term nn_result_Candidate
W04-1806	J93-1003	o	Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge e.g. -LRB- Craven and Kumlien 1999 -RRB- -LRB- Hull and Gomez 1993 -RRB- or require human-annotated training data with relation information for each domain -LRB- Craven et al. 1998 -RRB-	advmod_1998_al. nn_al._et num_Craven_1998 det_domain_each prep_for_information_domain nn_information_relation nn_data_training amod_data_human-annotated prep_with_require_information dobj_require_data num_Gomez_1993 dep_Hull_Craven conj_or_Hull_require conj_and_Hull_Gomez num_Kumlien_1999 conj_and_Craven_Kumlien amod_knowledge_domain amod_knowledge_linguistic amod_knowledge_handcreated amod_knowledge_substantial conj_or_linguistic_domain dep_require_e.g. dobj_require_knowledge preconj_require_either nsubj_require_relations amod_relations_labeled dep_infer_require dep_infer_Gomez dep_infer_Hull dep_infer_Kumlien dep_infer_Craven ccomp_infer_require dep_that_infer prep_approaches_that nsubj_approaches_extraction nn_extraction_Information
W04-1806	J93-1003	o	We use the log likelihood ratio -LRB- LLR -RRB- -LRB- Dunning 1993 -RRB- given by -2 log 2 -LRB- H o -LRB- p k 1 n 1 k 2 n 2 -RRB- / H a -LRB- p 1 p 2 n 1 k 1 n 2 k 2 -RRB- -RRB- LLR measures the extent to which a hypothesized model of the distribution of cell counts H a differs from the null hypothesis H o -LRB- namely that the percentage of documents containing this term is the same in both corpora -RRB-	preconj_corpora_both prep_in_same_corpora det_same_the cop_same_is nsubj_same_percentage mark_same_that det_term_this dobj_containing_term vmod_documents_containing prep_of_percentage_documents det_percentage_the advcl_namely_same dep_o_namely nn_o_H appos_hypothesis_o amod_hypothesis_null det_hypothesis_the prep_from_differs_hypothesis nsubj_differs_model prep_to_differs_which dep_H_a nn_counts_cell prep_of_distribution_counts det_distribution_the appos_model_H prep_of_model_distribution amod_model_hypothesized det_model_a rcmod_extent_differs det_extent_the dobj_measures_extent dep_LLR_o num_k_2 num_n_2 num_k_1 appos_n_k appos_n_n appos_n_k num_n_1 num_p_2 dep_p_n appos_p_p num_p_1 dep_a_p num_n_2 num_k_2 num_n_1 appos_k_n appos_k_k appos_k_n num_k_1 dep_p_k dep_o_a dep_o_H dep_o_p nn_o_H num_o_2 dep_2_log number_2_-2 dep_given_measures agent_given_LLR dobj_Dunning_1993 dep_ratio_Dunning appos_ratio_LLR nn_ratio_likelihood nn_ratio_log det_ratio_the vmod_use_given dobj_use_ratio nsubj_use_We
W04-2105	J93-1003	o	The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score -LRB- Church et al. 1991 -RRB- the X2 the log-likelihood -LRB- Dunning 1993 -RRB- and Fishers exact test -LRB- Pedersen 1996 -RRB-	amod_Pedersen_1996 dep_test_Pedersen amod_test_exact nn_test_Fishers nn_test_log-likelihood nn_test_t-score amod_Dunning_1993 dep_log-likelihood_Dunning det_log-likelihood_the det_X2_the amod_Church_1991 dep_Church_al. nn_Church_et conj_and_t-score_Fishers conj_and_t-score_log-likelihood appos_t-score_X2 dep_t-score_Church prep_such_as_tests_test nn_tests_hypothesis dobj_using_tests xcomp_independant_using cop_independant_are nsubj_independant_words mark_independant_whether num_words_two ccomp_evaluate_independant advmod_evaluate_often nsubj_evaluate_significance amod_significance_statistical det_significance_The
W04-2105	J93-1003	o	The various extraction measures have been discussed in great detail in the literature -LRB- Manning and Schutze 1999 McKeown and Radev 2000 -RRB- their performance has been compared -LRB- Dunning 1993 Pedersen 1996 Evert and Krenn 2001 -RRB- and the methods have been combined to improve overall performance -LRB- Inkpen and Hirst 2002 -RRB-	dep_Inkpen_2002 conj_and_Inkpen_Hirst dep_performance_Hirst dep_performance_Inkpen amod_performance_overall dobj_improve_performance aux_improve_to xcomp_combined_improve auxpass_combined_been aux_combined_have nsubjpass_combined_methods det_methods_the dep_Evert_2001 conj_and_Evert_Krenn num_Pedersen_1996 dep_Dunning_Krenn dep_Dunning_Evert conj_Dunning_Pedersen conj_Dunning_1993 conj_and_compared_combined dep_compared_Dunning auxpass_compared_been aux_compared_has nsubjpass_compared_performance advcl_compared_discussed poss_performance_their dep_McKeown_2000 conj_and_McKeown_Radev dep_Manning_Radev dep_Manning_McKeown conj_and_Manning_1999 conj_and_Manning_Schutze appos_literature_1999 appos_literature_Schutze appos_literature_Manning det_literature_the amod_detail_great prep_in_discussed_literature prep_in_discussed_detail auxpass_discussed_been aux_discussed_have nsubjpass_discussed_measures nn_measures_extraction amod_measures_various det_measures_The
W05-0801	J93-1003	o	3 The Log-Likelihood-Ratio Association Measure We base all our association-based word-alignment methods on the log-likelihood-ratio -LRB- LLR -RRB- statistic introduced to the NLP community by Dunning -LRB- 1993 -RRB-	appos_Dunning_1993 nn_community_NLP det_community_the agent_introduced_Dunning prep_to_introduced_community vmod_statistic_introduced amod_statistic_log-likelihood-ratio det_statistic_the dep_log-likelihood-ratio_LLR nn_methods_word-alignment amod_methods_association-based poss_methods_our predet_methods_all prep_on_base_statistic dobj_base_methods nsubj_base_We rcmod_Measure_base nn_Measure_Association nn_Measure_Log-Likelihood-Ratio det_Measure_The num_Measure_3
W05-0801	J93-1003	o	-LRB- 1993 -RRB- sometimes augmented by an HMM-based model or Och and Neys Model 6 -LRB- Och and Ney 2003 -RRB-	dep_Och_2003 conj_and_Och_Ney appos_Model_Ney appos_Model_Och num_Model_6 nn_Model_Neys conj_and_model_Model conj_or_model_Och amod_model_HMM-based det_model_an agent_augmented_Model agent_augmented_Och agent_augmented_model advmod_augmented_sometimes vmod_1993_augmented dep_''_1993
W05-0810	J93-1003	o	A second pass aligns the sentences in a way similar1 to the algorithm described by Gale and Church -LRB- 1993 -RRB- but where the search space is constrained to be close to the one delimited by the word alignment	nn_alignment_word det_alignment_the agent_delimited_alignment vmod_one_delimited det_one_the prep_to_close_one cop_close_be aux_close_to xcomp_constrained_close auxpass_constrained_is nsubjpass_constrained_space advmod_constrained_where nn_space_search det_space_the appos_Church_1993 conj_and_Gale_Church agent_described_Church agent_described_Gale vmod_algorithm_described det_algorithm_the prep_to_similar1_algorithm amod_way_similar1 det_way_a det_sentences_the conj_but_aligns_constrained prep_in_aligns_way dobj_aligns_sentences nsubj_aligns_pass amod_pass_second det_pass_A
W05-0810	J93-1003	o	First we considered single sentences as documents and tokens as sentences -LRB- we define a token as a sequence of characters delimited by 1In our case the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in -LRB- Gale and Church 1993 -RRB- but also a cognate-based one similar to -LRB- Simard et al. 1992 -RRB-	amod_Simard_1992 dep_Simard_al. nn_Simard_et prep_similar_to dep_one_Simard amod_one_similar amod_one_cognate-based det_one_a dep_also_one amod_Gale_1993 conj_and_Gale_Church prep_in_described_Church prep_in_described_Gale vmod_criteria_described nn_criteria_length det_criteria_the dobj_taking_criteria prep_into_taking_account preconj_taking_only aux_taking_is nsubj_taking_case neg_only_not amod_programming_dynamic prep_by_maximize_programming advmod_maximize_globally aux_maximize_to xcomp_seek_maximize nsubj_seek_we rcmod_score_seek det_score_the appos_case_score poss_case_our rcmod_1In_taking agent_delimited_1In vmod_characters_delimited prep_of_sequence_characters det_sequence_a prep_as_token_sequence det_token_a conj_but_define_also dobj_define_token nsubj_define_we dep_tokens_also dep_tokens_define prep_as_tokens_sentences prep_as_sentences_documents amod_sentences_single conj_and_considered_tokens dobj_considered_sentences nsubj_considered_we advmod_considered_First
W05-0810	J93-1003	o	In our case we computed a likelihood ratio score -LRB- Dunning 1993 -RRB- for all pairs of English tokens and Inuktitut substrings of length ranging from 3 to 10 characters	num_characters_10 dep_10_to number_10_3 prep_from_ranging_characters prep_of_substrings_length amod_substrings_Inuktitut vmod_tokens_ranging conj_and_tokens_substrings nn_tokens_English prep_of_pairs_substrings prep_of_pairs_tokens det_pairs_all dep_Dunning_1993 prep_for_score_pairs appos_score_Dunning nn_score_ratio nn_score_likelihood det_score_a dobj_computed_score nsubj_computed_we prep_in_computed_case poss_case_our
W05-0810	J93-1003	o	When efficient techniques have been proposed -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- they have been mostly evaluated on safe pairs of languages where the notion of word is rather clear	advmod_clear_rather cop_clear_is nsubj_clear_notion advmod_clear_where prep_of_notion_word det_notion_the rcmod_languages_clear prep_of_pairs_languages amod_pairs_safe prep_on_evaluated_pairs advmod_evaluated_mostly auxpass_evaluated_been aux_evaluated_have nsubjpass_evaluated_they advcl_evaluated_proposed dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_proposed_Brown auxpass_proposed_been aux_proposed_have nsubjpass_proposed_techniques advmod_proposed_When amod_techniques_efficient ccomp_``_evaluated
W05-1005	J93-1003	o	Since in these LVCs the complement is a predicative noun in stem form identical to a verb we form development and test expressions by combining give or take with verbs from selected semantic classes of Levin -LRB- 1993 -RRB- taken from Stevenson et al.	dep_Stevenson_al. nn_Stevenson_et prep_from_taken_Stevenson vmod_Levin_taken appos_Levin_1993 prep_of_classes_Levin amod_classes_semantic amod_classes_selected prep_from_verbs_classes prep_with_give_verbs conj_or_give_take dobj_combining_take dobj_combining_give nn_expressions_test nn_expressions_development conj_and_development_test prepc_by_form_combining dobj_form_expressions nsubj_form_we advcl_form_noun det_verb_a prep_to_identical_verb nn_form_stem amod_noun_identical prep_in_noun_form amod_noun_predicative det_noun_a cop_noun_is nsubj_noun_complement prep_in_noun_LVCs mark_noun_Since det_complement_the det_LVCs_these
W05-1005	J93-1003	o	1PMI is subject to overestimation for low frequency items -LRB- Dunning 1993 -RRB- thus we require a minimum frequency of occurrence for the expressions under study	prep_under_expressions_study det_expressions_the prep_for_frequency_expressions prep_of_frequency_occurrence amod_frequency_minimum det_frequency_a dobj_require_frequency nsubj_require_we advmod_require_thus amod_Dunning_1993 appos_items_Dunning nn_items_frequency amod_items_low prep_for_overestimation_items parataxis_subject_require prep_to_subject_overestimation cop_subject_is nsubj_subject_1PMI
W06-1006	J93-1003	o	The sets obtainedare then ranked usingthe loglikelihoodratiostest -LRB- Dunning ,1993 -RRB-	num_Dunning_,1993 appos_loglikelihoodratiostest_Dunning amod_loglikelihoodratiostest_usingthe dobj_ranked_loglikelihoodratiostest advmod_ranked_then nsubj_ranked_obtainedare rcmod_sets_ranked det_sets_The ccomp_``_sets
W06-1006	J93-1003	o	3.2 German Germanis thesecondmostinvestigatedlanguage thanks to the early work of Breidt -LRB- 1993 -RRB- and morerecently to thatof KrennandEvert such as -LRB- Krennand Evert 2001 Evert and Krenn ,2001 Evert ,2004 -RRB- centeredonevaluation	dep_centeredonevaluation_Evert num_Evert_,2004 num_Krenn_,2001 conj_and_Evert_Krenn dep_Evert_Evert dep_Evert_Krenn dep_Evert_Evert appos_Evert_2001 nn_Evert_Krennand prep_such_as_KrennandEvert_centeredonevaluation amod_KrennandEvert_thatof appos_Breidt_1993 prep_of_work_Breidt amod_work_early det_work_the prep_to_thanks_KrennandEvert conj_and_thanks_morerecently prep_to_thanks_work dep_thanks_thesecondmostinvestigatedlanguage nn_thesecondmostinvestigatedlanguage_Germanis amod_thesecondmostinvestigatedlanguage_German num_thesecondmostinvestigatedlanguage_3.2
W06-1006	J93-1003	o	Breidt -LRB- 1993 -RRB- alsopointedouta coupleof problemsthatmakes extractionfor Germanmoredifficultthanfor English the stronginflectionfor verbs the variable word-order andthepositionalambiguityofthearguments.Sheshowsthatevendistinguishingsubjectsfromobjectsisverydifficultwithoutparsing	appos_word-order_andthepositionalambiguityofthearguments.Sheshowsthatevendistinguishingsubjectsfromobjectsisverydifficultwithoutparsing amod_word-order_variable det_word-order_the appos_verbs_word-order nn_verbs_stronginflectionfor det_verbs_the dep_English_verbs nn_English_Germanmoredifficultthanfor nn_English_extractionfor nn_English_problemsthatmakes nn_English_coupleof nn_English_alsopointedouta nn_English_Breidt appos_Breidt_1993 dep_``_English
W06-1101	J93-1003	o	Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist J.R. Firth You shall know a word by the company it keeps -LRB- Firth 1957 p. 11 -RRB- Context similarity has been used as a means of extracting collocations from corpora e.g. by Church & Hanks -LRB- 1990 -RRB- and by Dunning -LRB- 1993 -RRB- of identifying word senses e.g. by Yarowski -LRB- 1995 -RRB- and by Schutze -LRB- 1998 -RRB- of clustering verb classes e.g. by Schulte im Walde -LRB- 2003 -RRB- and of inducing selectional restrictions of verbs e.g. by Resnik -LRB- 1993 -RRB- by Abe & Li -LRB- 1996 -RRB- by Rooth et al.	nn_al._et nn_al._Rooth appos_Abe_1996 conj_and_Abe_Li prep_by_by_al. pobj_by_Li pobj_by_Abe appos_Resnik_1993 dep_e.g._by prep_by_e.g._Resnik dep_,_e.g. prep_of_restrictions_verbs amod_restrictions_selectional dobj_inducing_restrictions pcomp_of_inducing conj_and_Walde_of appos_Walde_2003 nn_Walde_im nn_Walde_Schulte prep_by_e.g._of prep_by_e.g._Walde dep_verb_e.g. nsubj_verb_classes prep_of_verb_clustering appos_Schutze_1998 conj_and_Yarowski_Schutze appos_Yarowski_1995 prep_by_e.g._Schutze prep_by_e.g._Yarowski nn_senses_word dobj_identifying_senses parataxis_of_verb dep_of_e.g. pcomp_of_identifying appos_Dunning_1993 pobj_by_Dunning dep_Church_1990 conj_and_Church_Hanks conj_and_by_by pobj_by_Hanks pobj_by_Church dep_e.g._of dep_e.g._by dep_e.g._by prep_from_extracting_corpora dobj_extracting_collocations prepc_of_means_extracting det_means_a prep_used_e.g. prep_as_used_means auxpass_used_been aux_used_has nsubjpass_used_similarity nn_similarity_Context num_p._11 dep_Firth_p. dep_Firth_1957 nsubj_keeps_it rcmod_company_keeps det_company_the prep_by_word_company det_word_a dobj_know_word aux_know_shall nsubj_know_You nn_Firth_J.R. nn_Firth_linguist num_Firth_3 amod_Firth_British det_Firth_the prep_of_dictum_Firth amod_dictum_famous det_dictum_the prep_in_best_dictum parataxis_summarized_know dobj_summarized_best nn_meaning_word prep_to_approach_meaning nn_approach_empiricist det_approach_the parataxis_follow_used dep_follow_Firth dep_follow_summarized dobj_follow_approach nsubj_follow_studies amod_studies_Such
W06-1653	J93-1003	o	Typicality was measured using the log-likelihood ratio test -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_test_Dunning nn_test_ratio amod_test_log-likelihood det_test_the dobj_using_test xcomp_measured_using auxpass_measured_was nsubjpass_measured_Typicality
W06-2403	J93-1003	o	2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing -LRB- NLP -RRB- community including Smadja 1993 Dagan and Church 1994 Daille 1995 1995 McEnery et al. 1997 Wu 1997 Michiels and Dufour 1998 Maynard and Ananiadou 2000 Merkel and Andersson 2000 Piao and McEnery 2001 Sag et al. 2001 Tanaka and Baldwin 2003 Dias 2003 Baldwin et al. 2003 Nivre and Nilsson 2004 Pereira et al	dep_Pereira_al nn_Pereira_et num_Pereira_2004 appos_Nivre_Pereira conj_and_Nivre_Nilsson num_Baldwin_2003 nn_Baldwin_al. nn_Baldwin_et num_Dias_2003 num_Baldwin_2003 num_Sag_2001 nn_Sag_al. nn_Sag_et dep_Piao_Nilsson dep_Piao_Nivre conj_and_Piao_Baldwin conj_and_Piao_Dias conj_and_Piao_Baldwin conj_and_Piao_Tanaka conj_and_Piao_Sag conj_and_Piao_2001 conj_and_Piao_McEnery appos_Andersson_2000 num_Wu_1997 num_McEnery_1997 nn_McEnery_al. nn_McEnery_et appos_Daille_1995 conj_and_Smadja_Andersson conj_and_Smadja_Merkel conj_and_Smadja_2000 conj_and_Smadja_Ananiadou conj_and_Smadja_Maynard conj_and_Smadja_1998 conj_and_Smadja_Dufour conj_and_Smadja_Michiels conj_and_Smadja_Wu conj_and_Smadja_McEnery conj_and_Smadja_1995 conj_and_Smadja_Daille conj_and_Smadja_1994 conj_and_Smadja_Church conj_and_Smadja_Dagan conj_and_Smadja_1993 prep_including_community_Andersson prep_including_community_Merkel prep_including_community_2000 prep_including_community_Ananiadou prep_including_community_Maynard prep_including_community_1998 prep_including_community_Dufour prep_including_community_Michiels prep_including_community_Wu prep_including_community_McEnery prep_including_community_1995 prep_including_community_Daille prep_including_community_1994 prep_including_community_Church prep_including_community_Dagan prep_including_community_1993 prep_including_community_Smadja nn_community_Processing appos_Processing_NLP nn_Processing_Language amod_Processing_Natural det_Processing_the amod_attention_much prep_from_attracted_community dobj_attracted_attention aux_attracted_has nsubj_attracted_issue nn_processing_MWE prep_of_issue_processing det_issue_The dep_Work_Baldwin dep_Work_Dias dep_Work_Baldwin dep_Work_Tanaka dep_Work_Sag dep_Work_2001 dep_Work_McEnery dep_Work_Piao rcmod_Work_attracted amod_Work_Related num_Work_2
W06-2405	J93-1003	o	For each candidate triple the log-likelihood -LRB- Dunning 1993 -RRB- and salience -LRB- Kilgarriff and Tugwell 2001 -RRB- scores were calculated	auxpass_calculated_were nsubjpass_calculated_salience nsubjpass_calculated_log-likelihood advmod_calculated_triple dep_Kilgarriff_2001 conj_and_Kilgarriff_Tugwell appos_salience_Tugwell appos_salience_Kilgarriff amod_Dunning_1993 dep_log-likelihood_scores conj_and_log-likelihood_salience dep_log-likelihood_Dunning det_log-likelihood_the vmod_candidate_calculated det_candidate_each pobj_For_candidate dep_``_For
W06-2405	J93-1003	o	The other 5 have been suggested for Dutch by -LRB- Hollebrandse 1993 -RRB-	amod_Hollebrandse_1993 dep_by_Hollebrandse prep_suggested_by prep_for_suggested_Dutch auxpass_suggested_been aux_suggested_have nsubjpass_suggested_5 amod_5_other det_5_The
W06-3307	J93-1003	o	It is known that PMI gives undue importance to low frequency events -LRB- Dunning 1993 -RRB- therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus	amod_corpus_whole det_corpus_the prep_in_times_corpus num_times_5 quantmod_5_at mwe_at_least dobj_occur_times nsubj_occur_that rcmod_genes_occur prep_of_pairs_genes advmod_pairs_only dobj_considers_pairs nsubj_considers_evaluation advmod_considers_therefore det_evaluation_the dep_Dunning_1993 appos_events_Dunning nn_events_frequency amod_events_low amod_importance_undue parataxis_gives_considers prep_to_gives_events dobj_gives_importance nsubj_gives_PMI mark_gives_that ccomp_known_gives auxpass_known_is nsubjpass_known_It ccomp_``_known
W06-3804	J93-1003	o	We use the likelihood ratio for a binomial distribution -LRB- Dunning 1993 -RRB- which tests the hypothesis whether the term occurs independently in texts of biographical nature given a large corpus of biographical and non-biographical texts	amod_texts_non-biographical amod_texts_biographical conj_and_biographical_non-biographical prep_of_corpus_texts amod_corpus_large det_corpus_a pobj_given_corpus amod_nature_biographical prep_texts_given prep_of_texts_nature prep_in_occurs_texts advmod_occurs_independently nsubj_occurs_term mark_occurs_whether det_term_the ccomp_hypothesis_occurs det_hypothesis_the dobj_tests_hypothesis nsubj_tests_which dobj_Dunning_1993 rcmod_distribution_tests dep_distribution_Dunning amod_distribution_binomial det_distribution_a nn_ratio_likelihood det_ratio_the prep_for_use_distribution dobj_use_ratio nsubj_use_We
W06-3812	J93-1003	o	In this case we use the log-likelihood measure as described in -LRB- Dunning 1993 -RRB-	dobj_Dunning_1993 dep_in_Dunning prep_described_in mark_described_as nn_measure_log-likelihood det_measure_the advcl_use_described dobj_use_measure nsubj_use_we prep_in_use_case det_case_this
W06-3812	J93-1003	o	The outcomes of CW resemble those of MinCut -LRB- Wu & Leahy 1993 -RRB- Dense regions in the graph are grouped into one cluster while sparsely connected regions are separated	auxpass_separated_are ccomp_separated_grouped nsubjpass_separated_those dep_connected_regions advmod_connected_sparsely mark_connected_while num_cluster_one advcl_grouped_connected prep_into_grouped_cluster auxpass_grouped_are nsubjpass_grouped_regions det_graph_the prep_in_regions_graph amod_regions_Dense num_Wu_1993 conj_and_Wu_Leahy nn_Wu_MinCut prep_of_those_Leahy prep_of_those_Wu ccomp_resemble_separated nsubj_resemble_outcomes prep_of_outcomes_CW det_outcomes_The
W07-1002	J93-1003	o	It was later applied by -LRB- Dunning 1993 -RRB- as a way to determine if a sequence of N words -LRB- Ngram -RRB- came from an independently distributed sample	amod_sample_distributed det_sample_an advmod_distributed_independently prep_from_came_sample nsubj_came_sequence mark_came_if appos_words_Ngram nn_words_N prep_of_sequence_words det_sequence_a advcl_determine_came aux_determine_to vmod_way_determine det_way_a appos_Dunning_1993 prep_as_applied_way agent_applied_Dunning advmod_applied_later auxpass_applied_was nsubjpass_applied_It
W07-1108	J93-1003	o	To model aspects of co-occurrence association that might be obscured by raw frequency the log-likelihood ratio G2 -LRB- Dunning 1993 -RRB- was also used to transform the feature space	nn_space_feature det_space_the dobj_transform_space aux_transform_to xcomp_used_transform advmod_used_also auxpass_used_was nsubjpass_used_G2 advcl_used_model amod_Dunning_1993 dep_G2_Dunning nn_G2_ratio amod_G2_log-likelihood det_G2_the amod_frequency_raw agent_obscured_frequency auxpass_obscured_be aux_obscured_might nsubjpass_obscured_that nn_association_co-occurrence rcmod_aspects_obscured prep_of_aspects_association dobj_model_aspects aux_model_To
W07-1708	J93-1003	p	For the current work the Log-likelihood coefficient has been employed -LRB- Dunning 1993 -RRB- as it is reported to perform well among other scoring methods -LRB- Daille 1995 -RRB-	amod_Daille_1995 dep_methods_Daille amod_methods_scoring amod_methods_other prep_among_perform_methods advmod_perform_well aux_perform_to xcomp_reported_perform auxpass_reported_is nsubjpass_reported_it mark_reported_as dep_Dunning_1993 advcl_employed_reported dep_employed_Dunning auxpass_employed_been aux_employed_has nsubjpass_employed_coefficient prep_for_employed_work amod_coefficient_Log-likelihood det_coefficient_the amod_work_current det_work_the
W07-1708	J93-1003	o	In this study we have concentrated on the NPs ?? term extraction which comprises the focus of interest in several studies -LRB- Jacquemin 2001 Justeson & Katz 1995 Voutanen 1993 -RRB-	amod_Voutanen_1993 dep_Justeson_Voutanen conj_and_Justeson_1995 conj_and_Justeson_Katz dep_Jacquemin_1995 dep_Jacquemin_Katz dep_Jacquemin_Justeson appos_Jacquemin_2001 dep_studies_Jacquemin amod_studies_several prep_of_focus_interest det_focus_the prep_in_comprises_studies dobj_comprises_focus nsubj_comprises_which rcmod_extraction_comprises nn_extraction_term dobj_??_extraction det_NPs_the dep_concentrated_?? prep_on_concentrated_NPs aux_concentrated_have nsubj_concentrated_we prep_in_concentrated_study det_study_this
W08-0409	J93-1003	o	Generative word alignment models initially developed at IBM -LRB- Brown et al. 1993 -RRB- and then augmented by an HMM-based model -LRB- Vogel et al. 1996 -RRB- have provided powerful modeling capability for word alignment	nn_alignment_word nn_capability_modeling amod_capability_powerful prep_for_provided_alignment dobj_provided_capability aux_provided_have amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et amod_model_HMM-based det_model_an prep_by_augmented_model advmod_augmented_then nsubj_augmented_models amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_IBM_Brown dep_developed_provided dep_developed_Vogel conj_and_developed_augmented prep_at_developed_IBM advmod_developed_initially nsubj_developed_models nn_models_alignment nn_models_word amod_models_Generative
W08-0409	J93-1003	p	Pr -LRB- cJ1 aJ1 | eI1 -RRB- = p -LRB- J | I -RRB- -LRB- I + 1 -RRB- J Jproductdisplay j = 1 p -LRB- cj | eaj -RRB- -LRB- 8 -RRB- 3.1.2 Log-likelihood ratio The log-likelihood ratio statistic has been found to be accurate for modeling the associations between rare events -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_events_Dunning amod_events_rare prep_between_associations_events det_associations_the dobj_modeling_associations prep_for_accurate_modeling cop_accurate_be aux_accurate_to xcomp_found_accurate auxpass_found_been aux_found_has nsubjpass_found_statistic nn_statistic_ratio amod_statistic_log-likelihood det_statistic_The rcmod_ratio_found amod_ratio_Log-likelihood num_ratio_3.1.2 num_ratio_8 num_eaj_| nn_eaj_cj dep_p_ratio appos_p_eaj num_p_1 dep_=_p amod_j_= nn_j_Jproductdisplay nn_j_J conj_1_j conj_+_I_1 dep_|_I dep_|_J dep_p_1 dep_p_I dep_p_| dobj_=_p num_eI1_| nn_eI1_aJ1 appos_cJ1_eI1 dep_Pr_= dep_Pr_cJ1 dep_``_Pr
W08-1914	J93-1003	p	Many previous studies have shown that the log-likelihood ratio is well suited for this purpose -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_purpose_Dunning det_purpose_this prep_for_suited_purpose advmod_suited_well auxpass_suited_is nsubjpass_suited_ratio mark_suited_that amod_ratio_log-likelihood det_ratio_the ccomp_shown_suited aux_shown_have nsubj_shown_studies amod_studies_previous amod_studies_Many ccomp_``_shown
W08-1914	J93-1003	p	It can be expected that the log-likelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment -LRB- Dunning 1993 -RRB- although there are other measures which come close in performance -LRB- e.g. Rapp 1998 -RRB-	amod_Rapp_1998 pobj_e.g._Rapp dep_performance_e.g. prep_in_come_performance advmod_come_close nsubj_come_which rcmod_measures_come amod_measures_other nsubj_are_measures expl_are_there mark_are_although appos_Dunning_1993 dep_judgment_Dunning amod_judgment_human prep_with_correlates_judgment advmod_correlates_highly nsubj_correlates_that rcmod_pairs_correlates nn_pairs_word prep_of_ranking_pairs amod_ranking_accurate det_ranking_an dobj_produces_ranking nsubj_produces_ratio mark_produces_that amod_ratio_log-likelihood det_ratio_the advcl_expected_are ccomp_expected_produces auxpass_expected_be aux_expected_can nsubjpass_expected_It ccomp_``_expected
W09-0202	J93-1003	o	corpus -LRB- Dunning 1993 Scott 1997 Rayson et al. 2004 -RRB-	num_Rayson_2004 nn_Rayson_al. nn_Rayson_et num_Scott_1997 dep_Dunning_Rayson conj_Dunning_Scott conj_Dunning_1993 nn_Dunning_corpus
W09-0202	J93-1003	o	2.1 Keywords As our starting point we calculated the keywords of the Belgian corpus with respect to the Netherlandic corpus both on the basis of a chi-square test -LRB- with Yates continuity correction -RRB- -LRB- Scott 1997 -RRB- and the log-likelihood ratio -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_ratio_Dunning amod_ratio_log-likelihood det_ratio_the conj_and_Scott_ratio num_Scott_1997 nn_correction_continuity nn_correction_Yates dep_test_ratio dep_test_Scott prep_with_test_correction amod_test_chi-square det_test_a prep_of_basis_test det_basis_the prep_on_both_basis amod_corpus_Netherlandic det_corpus_the amod_corpus_Belgian det_corpus_the dep_keywords_both prep_with_respect_to_keywords_corpus prep_of_keywords_corpus det_keywords_the dobj_calculated_keywords nsubj_calculated_we dep_calculated_Keywords amod_point_starting poss_point_our prep_as_Keywords_point num_Keywords_2.1
W09-0202	J93-1003	o	The most obvious comparison takes on the form of a keyword analysis which looks for the words that are significantly more frequent in the one corpus as compared to the other -LRB- Dunning 1993 Scott 1997 Rayson et al. 2004 -RRB-	num_Rayson_2004 nn_Rayson_al. nn_Rayson_et num_Scott_1997 dep_Dunning_Rayson dep_Dunning_Scott dep_Dunning_1993 dep_other_Dunning amod_the_other prep_to_compared_the mark_compared_as num_corpus_one det_corpus_the advcl_frequent_compared prep_in_frequent_corpus advmod_frequent_more cop_frequent_are nsubj_frequent_that advmod_more_significantly rcmod_words_frequent det_words_the prep_for_looks_words nsubj_looks_which rcmod_analysis_looks amod_analysis_keyword det_analysis_a prep_of_form_analysis det_form_the prep_on_takes_form nsubj_takes_comparison amod_comparison_obvious det_comparison_The advmod_obvious_most
W09-0203	J93-1003	o	We worked with an implementation of the log likelihood ratio -LRB- g-Score -RRB- as proposed by Dunning -LRB- 1993 -RRB- and two variants of the t-score one considering all values -LRB- t-score -RRB- and one where only positive values -LRB- t-score + -RRB- are kept following the results of Curran and Moens -LRB- 2002 -RRB-	appos_Moens_2002 conj_and_Curran_Moens prep_of_results_Moens prep_of_results_Curran det_results_the prep_following_kept_results auxpass_kept_are nsubjpass_kept_values advmod_kept_where cc_t-score_+ appos_values_t-score amod_values_positive advmod_positive_only rcmod_one_kept conj_and_values_one appos_values_t-score det_values_all dobj_considering_one dobj_considering_values vmod_one_considering det_t-score_the prep_of_variants_t-score num_variants_two conj_and_Dunning_variants appos_Dunning_1993 prep_by_proposed_variants prep_by_proposed_Dunning mark_proposed_as appos_ratio_g-Score nn_ratio_likelihood nn_ratio_log det_ratio_the prep_of_implementation_ratio det_implementation_an dobj_worked_one advcl_worked_proposed prep_with_worked_implementation nsubj_worked_We
W09-0426	J93-1003	o	Rapp -LRB- 1999 -RRB- Dunning -LRB- 1993 -RRB- -RRB- but using cosine rather than cityblock distance to measure profile similarity	nn_similarity_profile dobj_measure_similarity aux_measure_to amod_distance_cityblock conj_negcc_cosine_distance vmod_using_measure dobj_using_distance dobj_using_cosine appos_Dunning_1993 conj_but_Rapp_using appos_Rapp_Dunning appos_Rapp_1999
W09-1705	J93-1003	o	Given a contextual word cw that occurs in the paragraphs of bc a log-likelihood ratio -LRB- G2 -RRB- test is employed -LRB- Dunning 1993 -RRB- which checks if the distribution of cw in bc is similar to the distribution of cw in rc p -LRB- cw | bc -RRB- = p -LRB- cw | rc -RRB- -LRB- null hypothesis -RRB-	amod_hypothesis_null num_rc_| nn_rc_cw appos_p_rc dep_=_p nn_bc_| nn_bc_cw appos_p_hypothesis amod_p_= appos_p_bc prep_in_distribution_rc prep_of_distribution_cw det_distribution_the prep_to_similar_distribution cop_similar_is nsubj_similar_distribution mark_similar_if prep_in_distribution_bc prep_of_distribution_cw det_distribution_the dep_checks_p advcl_checks_similar det_checks_which dep_Dunning_1993 dep_employed_checks dep_employed_Dunning auxpass_employed_is dep_employed_Given nn_test_ratio appos_ratio_G2 amod_ratio_log-likelihood det_ratio_a appos_bc_test prep_of_paragraphs_bc det_paragraphs_the prep_in_occurs_paragraphs nsubj_occurs_that rcmod_cw_occurs nn_cw_word amod_cw_contextual det_cw_a dobj_Given_cw
W94-0103	J93-1003	o	The algorithm is based on the Machine Learning method for word categorisation inspired by the well known study on basic-level categories \ -LSB- Rosch 1978 \ -RSB- presented in \ -LSB- Basili et al 1993a \ -RSB-	amod_\_1993a appos_Basili_\ dep_Basili_al nn_Basili_et prep_in_presented_\ num_\_1978 appos_Rosch_\ dep_\_Basili dep_\_presented dep_\_Rosch amod_categories_basic-level prep_on_study_categories amod_study_known det_study_the advmod_known_well dep_inspired_\ prep_by_inspired_study nn_categorisation_word prep_for_method_categorisation nn_method_Learning nn_method_Machine det_method_the dep_based_inspired prep_on_based_method auxpass_based_is nsubjpass_based_algorithm det_algorithm_The
W94-0103	J93-1003	o	The algorithm to acquire the lexicon implemented in the ARIOSTQLEX system has been extensively described in \ -LSB- Basili et al 1993c \ -RSB-	amod_\_1993c appos_Basili_\ dep_Basili_al nn_Basili_et dep_described_Basili prep_in_described_\ advmod_described_extensively auxpass_described_been aux_described_has nsubjpass_described_algorithm nn_system_ARIOSTQLEX det_system_the prep_in_implemented_system det_lexicon_the dobj_acquire_lexicon aux_acquire_to vmod_algorithm_implemented vmod_algorithm_acquire det_algorithm_The
W94-0103	J93-1003	o	Pustejovsky confronted with the problem of automatic acquisition more extensively in \ -LSB- Pustejovsky et al. 1993 \ -RSB-	num_\_1993 dep_\_al. nn_al._et dep_Pustejovsky_\ advmod_extensively_more amod_acquisition_automatic prep_of_problem_acquisition det_problem_the dep_confronted_Pustejovsky prep_in_confronted_\ advmod_confronted_extensively prep_with_confronted_problem nsubj_confronted_Pustejovsky
W94-0103	J93-1003	o	The interest reader is referred to \ -LSB- Basili et al 1993 b and c \ -RSB- for a summary of ARIOSTO an integrated tool for extensive acquisition of lexieal knowledge from corpora that we used to demonstrate and validate our approach	poss_approach_our dobj_validate_approach conj_and_demonstrate_validate aux_demonstrate_to xcomp_used_validate xcomp_used_demonstrate nsubj_used_we mark_used_that nn_knowledge_lexieal prep_from_acquisition_corpora prep_of_acquisition_knowledge amod_acquisition_extensive prep_for_tool_acquisition amod_tool_integrated det_tool_an appos_ARIOSTO_tool prep_of_summary_ARIOSTO det_summary_a nn_\_c conj_and_b_\ num_b_1993 dep_Basili_used prep_for_Basili_summary appos_Basili_\ appos_Basili_b dep_Basili_al nn_Basili_et dobj_\_Basili aux_\_to xcomp_referred_\ auxpass_referred_is nsubjpass_referred_reader nn_reader_interest det_reader_The
W94-0103	J93-1003	o	The statistical methods are based on distributional analysis -LRB- we defined a measure called mutual conditioned plausibility a derivation of the well known mutual information -RRB- and cluster analysis -LRB- a COBWEB-like algorithm for word classification is presented in \ -LSB- Basili et al 1993 a \ -RSB- -RRB-	det_\_a appos_Basili_\ amod_Basili_1993 dep_Basili_al nn_Basili_et prep_in_presented_\ auxpass_presented_is nsubjpass_presented_algorithm nn_classification_word prep_for_algorithm_classification amod_algorithm_COBWEB-like det_algorithm_a rcmod_analysis_presented nn_analysis_cluster amod_information_mutual amod_information_known det_information_the advmod_known_well prep_of_derivation_information det_derivation_a amod_plausibility_conditioned amod_plausibility_mutual dep_called_plausibility vmod_measure_called det_measure_a dobj_defined_derivation dobj_defined_measure nsubj_defined_we amod_analysis_distributional dep_based_Basili conj_and_based_analysis parataxis_based_defined prep_on_based_analysis auxpass_based_are nsubjpass_based_methods amod_methods_statistical det_methods_The ccomp_``_analysis ccomp_``_based
W97-0116	J93-1003	o	4 Related Work The automatic extraction of English subcategorization frames has been considered in -LRB- Brent 1991 Brent 1993 -RRB- where a procedure is presented that takes untamed text as input and generates a list of verbal subcategorization frames	nn_frames_subcategorization amod_frames_verbal prep_of_list_frames det_list_a dobj_generates_list nsubj_generates_that amod_text_untamed conj_and_takes_generates prep_as_takes_input dobj_takes_text nsubj_takes_that ccomp_presented_generates ccomp_presented_takes auxpass_presented_is nsubjpass_presented_procedure advmod_presented_where det_procedure_a num_Brent_1993 dep_Brent_Brent dep_Brent_1991 dep_in_Brent advcl_considered_presented prep_considered_in auxpass_considered_been aux_considered_has nsubjpass_considered_extraction nn_frames_subcategorization nn_frames_English prep_of_extraction_frames amod_extraction_automatic det_extraction_The dep_Work_considered amod_Work_Related num_Work_4
W97-0116	J93-1003	o	This statistic is given by -2 log A = 2 -LRB- log L -LRB- p1 kl hi -RRB- log L -LRB- p2 k2 n2 -RRB- log L -LRB- p kl R1 -RRB- -- log L -LRB- p k2 n2 -RRB- -RRB- where log LCo k n -RRB- = k logp + -LRB- n k -RRB- log -LRB- 1 p -RRB- and Pl = ~ P2 = ~ P = ~ ' ~ -LRB- For a detailed description of the statistic used see -LRB- Dunning 1993 -RRB- -RRB-	dep_Dunning_1993 dep_see_Dunning prep_for_see_description vmod_statistic_used det_statistic_the prep_of_description_statistic amod_description_detailed det_description_a nsubj_=_P dep_=_~ dep_=_= dep_=_P2 dep_=_~ dep_=_= dep_=_Pl num_p_1 appos_log_k nn_k_n appos_logp_p conj_+_logp_log nn_logp_k amod_logp_= parataxis_LCo_see conj_and_LCo_~ conj_and_LCo_~ conj_and_LCo_= dep_LCo_log dep_LCo_logp appos_LCo_n conj_and_LCo_k nn_LCo_log dep_where_~ dep_where_~ dep_where_= dep_where_k dep_where_LCo appos_p_n2 appos_p_k2 dep_L_p nn_L_log appos_p_R1 appos_p_kl dep_L_L dep_L_p nn_L_log appos_p2_n2 appos_p2_k2 dep_L_p2 nn_L_log nn_L_L dep_p1_hi appos_p1_kl appos_L_p1 nn_L_log dep_=_2 dep_A_L amod_A_= nn_A_log num_A_-2 dep_given_where dep_given_L agent_given_A auxpass_given_is nsubjpass_given_statistic det_statistic_This
W97-0118	J93-1003	o	The Logllkelihood Ratio G 2 is a mathematically well-grounded and accurate method for calculating how surprising an event is -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_is_Dunning dep_event_is det_event_an amod_event_surprising advmod_surprising_how dep_calculating_event prepc_for_method_calculating amod_method_accurate amod_method_well-grounded det_method_a cop_method_is nsubj_method_Ratio conj_and_well-grounded_accurate advmod_well-grounded_mathematically num_G_2 appos_Ratio_G nn_Ratio_Logllkelihood det_Ratio_The
W97-0122	J93-1003	p	-LRB- Dunning 1993 -RRB- and -LRB- Pedersen 1996 -RRB- shows how some of the methods which have been used in the past -LRB- particularly mutual information scores -RRB- are invalid for rare events and introduce accurate measures of how ` surprising ' rare events are	amod_events_rare amod_events_surprising advmod_events_how prep_of_measures_events amod_measures_accurate dobj_introduce_measures nsubj_introduce_Dunning amod_events_rare prep_for_invalid_events cop_invalid_are nsubj_invalid_some advmod_invalid_how nn_scores_information amod_scores_mutual advmod_mutual_particularly det_past_the prep_in_used_past auxpass_used_been aux_used_have nsubjpass_used_which rcmod_methods_used det_methods_the appos_some_scores prep_of_some_methods dep_shows_are conj_and_shows_introduce advcl_shows_invalid nsubj_shows_Pedersen nsubj_shows_Dunning amod_Pedersen_1996 conj_and_Dunning_Pedersen amod_Dunning_1993
W97-0203	J93-1003	o	Its roots are the same as computational linguistics -LRB- CL -RRB- but it has been largely ignored in CL until recently -LRB- Dunning 1993 Carletta 1996 Kilgarriff 1996 -RRB-	amod_Kilgarriff_1996 dep_Carletta_Kilgarriff num_Carletta_1996 dep_Dunning_Carletta dep_Dunning_1993 dep_recently_Dunning advmod_until_recently prep_ignored_until prep_in_ignored_CL advmod_ignored_largely auxpass_ignored_been aux_ignored_has nsubjpass_ignored_it appos_linguistics_CL amod_linguistics_computational conj_but_same_ignored prep_as_same_linguistics det_same_the cop_same_are nsubj_same_roots poss_roots_Its
W97-0203	J93-1003	o	This set of words -LRB- rooted primarily in the verbs of the set -RRB- corresponds to the -LRB- Levin 1993 -RRB- Characterize -LRB- class 29.2 -RRB- Declare -LRB- 29.4 -RRB- Admire -LRB- 31.2 -RRB- and Judgment verbs -LRB- 33 -RRB- and hence may have particular syntactic and semantic patterning	amod_patterning_semantic conj_and_syntactic_patterning amod_syntactic_particular dobj_have_patterning dobj_have_syntactic aux_have_may appos_verbs_33 nn_verbs_Judgment appos_Admire_31.2 dep_Declare_29.4 num_class_29.2 dep_Characterize_have conj_and_Characterize_hence conj_and_Characterize_verbs conj_and_Characterize_Admire conj_and_Characterize_Declare dep_Characterize_class dep_Characterize_Levin dep_Characterize_the dep_Levin_1993 prepc_to_corresponds_hence prepc_to_corresponds_verbs prepc_to_corresponds_Admire prepc_to_corresponds_Declare prepc_to_corresponds_Characterize nsubj_corresponds_set det_set_the prep_of_verbs_set det_verbs_the prep_in_rooted_verbs advmod_rooted_primarily dep_words_rooted prep_of_set_words det_set_This
W98-1119	J93-1003	o	3.1 The gender/animaticity statistics After we have identified the correct antecedents it is a simple counting procedure to compute P -LRB- p \ -LSB- wa -RRB- where wa is in the correct antecedent for the pronoun p -LRB- Note the pronouns are grouped by their gender -RRB- \ -LSB- wain the antecedent for p \ -LSB- P -LRB- pl o -RRB- = When there are multiple relevant words in the antecedent we apply the likelihood test designed by Dunning -LRB- 1993 -RRB- on all the words in the candidate NP	nn_NP_candidate det_NP_the prep_in_words_NP det_words_the predet_words_all appos_Dunning_1993 prep_on_designed_words agent_designed_Dunning vmod_test_designed nn_test_likelihood det_test_the dobj_apply_test nsubj_apply_we nsubj_apply_P dep_apply_wain dep_apply_\ det_antecedent_the amod_words_relevant amod_words_multiple prep_in_are_antecedent nsubj_are_words expl_are_there advmod_are_When nn_o_pl rcmod_P_are amod_P_= appos_P_o nn_\_p prep_for_antecedent_\ det_antecedent_the dobj_wain_antecedent poss_gender_their agent_grouped_gender auxpass_grouped_are nsubjpass_grouped_pronouns det_pronouns_the ccomp_Note_grouped nn_p_pronoun det_p_the prep_for_antecedent_p amod_antecedent_correct det_antecedent_the prep_in_is_antecedent nsubj_is_wa advmod_is_where parataxis_\_apply dep_\_Note rcmod_\_is appos_\_wa nn_\_p dep_P_\ dobj_compute_P aux_compute_to vmod_procedure_compute nn_procedure_counting amod_procedure_simple det_procedure_a cop_procedure_is nsubj_procedure_it rcmod_antecedents_procedure amod_antecedents_correct det_antecedents_the dobj_identified_antecedents aux_identified_have nsubj_identified_we mark_identified_After dep_statistics_identified nn_statistics_gender/animaticity det_statistics_The num_statistics_3.1
W99-0631	J93-1003	o	C c C p -LRB- C \ -RSB- v r -RRB- is just the probability of the disjunction of the concepts in C that is = Zp -LRB- clv r -RRB- cEC In order to see how p -LRB- clv r -RRB- relates to the input data note that given a concept c verb v and argument position r a noun can be generated according to the distribution p -LRB- n \ -LSB- c v r -RRB- where p -LRB- nlc v r -RRB- = 1 nEsyn -LRB- c -RRB- Now we have a model for the input data p -LRB- n v r -RRB- = p -LRB- v r -RRB- p -LRB- niv r -RRB- = p -LRB- v r -RRB- p -LRB- clv rlp -LRB- ntc v r -RRB- cecn -LRB- n -RRB- Note that for c cn -LRB- n -RRB- p -LRB- nlc v r -RRB- = O The association norm -LRB- and similar measures such as the mutual information score -RRB- have been criticised -LRB- Dunning 1993 -RRB- because these scores can be greatly over-estimated when frequency counts are low	cop_low_are nsubj_low_counts advmod_low_when nn_counts_frequency advcl_over-estimated_low advmod_over-estimated_greatly auxpass_over-estimated_be aux_over-estimated_can nsubjpass_over-estimated_scores mark_over-estimated_because det_scores_these amod_Dunning_1993 advcl_criticised_over-estimated dep_criticised_Dunning auxpass_criticised_been aux_criticised_have nsubjpass_criticised_norm nn_score_information amod_score_mutual det_score_the prep_such_as_measures_score amod_measures_similar cc_measures_and appos_norm_measures nn_norm_association det_norm_The dobj_=_O npadvmod_=_r dep_=_v dep_nlc_criticised dep_nlc_= dep_p_nlc appos_cn_n nn_cn_c dep_that_p prep_for_that_cn dep_Note_that nsubj_Note_ntc dep_Note_rlp dep_Note_clv dep_Note_p dep_Note_v dep_Note_p dep_Note_= nsubj_Note_p dep_Note_v dep_Note_p dep_Note_= nsubj_Note_n dep_Note_nlc dep_Note_p advmod_Note_where appos_cecn_n nn_cecn_r dep_cecn_v dep_ntc_cecn dep_v_r appos_niv_r dep_p_niv dep_v_r npadvmod_=_r dep_=_v nn_n_p nn_data_input det_data_the prep_for_model_data det_model_a dobj_have_model nsubj_have_we advmod_have_Now appos_nEsyn_c num_nEsyn_1 parataxis_=_have dobj_=_nEsyn dep_=_r dep_=_v dep_nlc_= dep_r_v dep_c_r dep_\_Note appos_\_c nn_\_n dep_p_\ nn_p_distribution det_p_the pobj_generated_p prepc_according_to_generated_to auxpass_generated_be aux_generated_can nsubjpass_generated_r det_noun_a appos_r_noun nn_r_position nn_r_argument conj_and_v_generated dep_verb_generated dep_verb_v nn_c_concept det_c_a parataxis_given_verb pobj_given_c vmod_that_given dep_note_that nn_data_input det_data_the prep_to_relates_data nsubj_relates_p advmod_relates_how appos_clv_r dep_p_clv ccomp_see_relates aux_see_to dep_see_order mark_see_In dep_cEC_note advcl_cEC_see cop_cEC_is nsubj_cEC_that dep_=_r dep_=_clv dep_=_Zp dep_is_= prep_in_concepts_C det_concepts_the prep_of_disjunction_concepts det_disjunction_the prep_of_probability_disjunction det_probability_the advmod_probability_just cop_probability_is nsubj_probability_r dep_probability_v nn_v_p nn_\_C appos_p_\ parataxis_C_cEC appos_C_probability nn_C_c nn_C_C
W99-0631	J93-1003	o	Although this approach can give inaccurate estimates the counts given to the incorrect senses will disperse randomly throughout the hierarchy as noise and by accumulating counts up the hierarchy we will tend to gather counts from the correct senses of related words -LRB- Yarowsky 1992 Resnik 1993 -RRB-	amod_Resnik_1993 dep_Yarowsky_Resnik appos_Yarowsky_1992 appos_words_Yarowsky amod_words_related prep_of_senses_words amod_senses_correct det_senses_the prep_from_gather_senses dobj_gather_counts aux_gather_to xcomp_tend_gather aux_tend_will nsubj_tend_we rcmod_hierarchy_tend det_hierarchy_the dobj_counts_hierarchy advmod_counts_up dep_accumulating_counts det_hierarchy_the prepc_by_disperse_accumulating prep_as_disperse_noise prep_throughout_disperse_hierarchy advmod_disperse_randomly conj_and_disperse_disperse aux_disperse_will nsubj_disperse_counts nsubj_disperse_counts advcl_disperse_give amod_senses_incorrect det_senses_the prep_to_given_senses vmod_counts_given det_counts_the amod_estimates_inaccurate dobj_give_estimates aux_give_can nsubj_give_approach mark_give_Although det_approach_this
W99-0631	J93-1003	o	-LRB- This example is adapted from Resnik -LRB- 1993 -RRB- -RRB-	appos_Resnik_1993 prep_from_adapted_Resnik auxpass_adapted_is nsubjpass_adapted_example det_example_This
W99-0631	J93-1003	o	We use the log-likelihood X ~ statistic rather than the Pearson 's X 2 statistic as this is thought to be more appropriate when the counts in the contingency table are low -LRB- Dunning 1993 -RRB-	dep_Dunning_1993 cop_low_are nsubj_low_counts advmod_low_when nn_table_contingency det_table_the prep_in_counts_table det_counts_the dep_appropriate_Dunning advcl_appropriate_low advmod_appropriate_more cop_appropriate_be aux_appropriate_to xcomp_thought_appropriate auxpass_thought_is nsubjpass_thought_this mark_thought_as num_statistic_2 nn_statistic_X poss_statistic_Pearson det_Pearson_the conj_negcc_statistic_statistic num_statistic_~ nn_statistic_X nn_statistic_log-likelihood det_statistic_the advcl_use_thought dobj_use_statistic dobj_use_statistic nsubj_use_We
A00-1042	J93-1007	o	Smadja Frank -LRB- 1993 -RRB- Retrieving collocations from text Computational Linguistics 19 -LRB- 1 -RRB- :143 -177	num_-177_:143 num_-177_1 num_-177_19 num_Linguistics_-177 nn_Linguistics_Computational dep_collocations_Linguistics prep_from_collocations_text amod_collocations_Retrieving dep_Frank_collocations dep_Frank_1993 dep_Smadja_Frank
A94-1006	J93-1007	o	In particular mutual information -LRB- Church and Hanks 1990 Wu and Su 1993 -RRB- and other statistical methods such as -LRB- Smadja 1993 -RRB- and frequency-based methods such as -LRB- Justeson and Katz 1993 -RRB- exclude infrequent phrases because they tend to introduce too much noise	amod_noise_much advmod_much_too dobj_introduce_noise aux_introduce_to xcomp_tend_introduce nsubj_tend_they mark_tend_because amod_phrases_infrequent advcl_exclude_tend dobj_exclude_phrases nsubj_exclude_methods nsubj_exclude_information prep_in_exclude_particular dep_Justeson_1993 conj_and_Justeson_Katz prep_such_as_methods_Katz prep_such_as_methods_Justeson amod_methods_frequency-based conj_and_Smadja_methods amod_Smadja_1993 prep_such_as_methods_methods prep_such_as_methods_Smadja amod_methods_statistical amod_methods_other dep_Wu_1993 conj_and_Wu_Su conj_and_Church_Su conj_and_Church_Wu conj_and_Church_1990 conj_and_Church_Hanks conj_and_information_methods dep_information_Wu dep_information_1990 dep_information_Hanks dep_information_Church amod_information_mutual
A94-1006	J93-1007	o	have been used in statistical machine translation -LRB- Brown et al. 1990 -RRB- terminology research and translation aids -LRB- Isabelle 1992 Ogden and Gonzales 1993 van der Eijk 1993 -RRB- bilingual lexicography -LRB- Klavans and Tzoukermann 1990 Smadja 1992 -RRB- word-sense disambiguation -LRB- Brown et al. 1991b Gale et al. 1992 -RRB- and information retrieval in a multilingual environment -LRB- Landauer and Littman 1990 -RRB-	dep_Landauer_1990 conj_and_Landauer_Littman dep_environment_Littman dep_environment_Landauer amod_environment_multilingual det_environment_a nn_retrieval_information dep_al._1992 nn_al._et nn_al._Gale dep_al._al. appos_al._1991b nn_al._et amod_al._Brown dep_disambiguation_al. amod_disambiguation_word-sense dep_Smadja_1992 dep_Klavans_Smadja conj_and_Klavans_1990 conj_and_Klavans_Tzoukermann appos_lexicography_1990 appos_lexicography_Tzoukermann appos_lexicography_Klavans amod_lexicography_bilingual dep_Eijk_1993 nn_Eijk_der nn_Eijk_van num_Ogden_1993 conj_and_Ogden_Gonzales dep_Isabelle_Eijk dep_Isabelle_Gonzales dep_Isabelle_Ogden appos_Isabelle_1992 nn_aids_translation prep_in_research_environment conj_and_research_retrieval conj_and_research_disambiguation appos_research_lexicography appos_research_Isabelle conj_and_research_aids nn_research_terminology dep_al._1990 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical dep_used_retrieval dep_used_disambiguation dep_used_aids dep_used_research dep_used_al. prep_in_used_translation auxpass_used_been aux_used_have
A94-1006	J93-1007	o	Some methods use sentence alignment and additional statistics to find candidate translations of terms -LRB- Smadja 1992 van der Eijk 1993 -RRB-	dep_Eijk_1993 nn_Eijk_der nn_Eijk_van dep_Smadja_Eijk dep_Smadja_1992 appos_terms_Smadja prep_of_translations_terms nn_translations_candidate dobj_find_translations aux_find_to amod_statistics_additional conj_and_alignment_statistics nn_alignment_sentence vmod_use_find dobj_use_statistics dobj_use_alignment nsubj_use_methods det_methods_Some ccomp_``_use
A94-1006	J93-1007	o	3.4 Related work and issues for future research Smadja -LRB- 1992 -RRB- and van der Eijk -LRB- 1993 -RRB- describe term translation methods that use bilingual texts that were aligned at the sentence level	nn_level_sentence det_level_the prep_at_aligned_level auxpass_aligned_were nsubjpass_aligned_that rcmod_texts_aligned amod_texts_bilingual dobj_use_texts nsubj_use_that rcmod_methods_use nn_methods_translation nn_methods_term dobj_describe_methods nsubj_describe_issues nsubj_describe_work appos_Eijk_1993 nn_Eijk_der nn_der_van conj_and_Smadja_Eijk appos_Smadja_1992 nn_Smadja_research amod_Smadja_future prep_for_work_Eijk prep_for_work_Smadja conj_and_work_issues amod_work_Related num_work_3.4
A97-1026	J93-1007	o	Manual processes such as lexicon development could be automated in the future using standard contextbased word distribution methods -LRB- Smadja 1993 -RRB- or other corpus-based techniques	amod_techniques_corpus-based amod_techniques_other dep_Smadja_1993 appos_methods_Smadja nn_methods_distribution nn_methods_word conj_or_contextbased_techniques conj_or_contextbased_methods amod_contextbased_standard dobj_using_techniques dobj_using_methods dobj_using_contextbased det_future_the dep_automated_using prep_in_automated_future auxpass_automated_be aux_automated_could nsubjpass_automated_processes nn_development_lexicon prep_such_as_processes_development amod_processes_Manual
A97-1026	J93-1007	o	Smadja Frank -LRB- 1993 -RRB-	appos_Frank_1993 nn_Frank_Smadja
A97-1045	J93-1007	p	Tools like Xtract -LRB- Smadja 1993 -RRB- were based on the work of Church and others but made a step forward by incorporating various statistical measurements like z-score and variance of distribution as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output	amod_output_raw prep_of_parsing_output amod_parsing_partial conj_and_data_parsing nn_data_input prep_of_tagging_parsing prep_of_tagging_data conj_and_tagging_lemmatization amod_tagging_part-of-speech prep_like_techniques_lemmatization prep_like_techniques_tagging amod_techniques_linguistic amod_techniques_shallow prep_of_z-score_distribution conj_and_z-score_variance prep_like_measurements_variance prep_like_measurements_z-score amod_measurements_statistical amod_measurements_various dobj_incorporating_measurements advmod_step_forward det_step_a conj_and_made_techniques prepc_by_made_incorporating dobj_made_step nsubjpass_made_Tools conj_and_Church_others prep_of_work_others prep_of_work_Church det_work_the conj_but_based_techniques conj_but_based_made prep_on_based_work auxpass_based_were nsubjpass_based_Tools num_Smadja_1993 appos_Xtract_Smadja prep_like_Tools_Xtract
A97-1050	J93-1007	o	-LRB- Daille 1996 Smadja 1993 -RRB- -RRB- less prior work exists for bilingual acquisition of domain-specific translations	amod_translations_domain-specific prep_of_acquisition_translations amod_acquisition_bilingual prep_for_exists_acquisition nsubj_exists_work dep_exists_Daille amod_work_prior amod_work_less dep_Smadja_1993 dep_Daille_Smadja dep_Daille_1996
A97-1054	J93-1007	o	For instance one might be interested in frequencies of co-occurences of a word with other words and phrases -LRB- collocations -RRB- -LRB- Smadja 1993 -RRB- or one might be interested in inducing wordclasses from the text by collecting frequencies of the left and right context words for a word in focus -LRB- Finch & Chater 1993 -RRB-	amod_Finch_1993 conj_and_Finch_Chater dep_word_Chater dep_word_Finch prep_in_word_focus det_word_a prep_for_words_word nn_words_context amod_words_right amod_words_left det_words_the conj_and_left_right prep_of_frequencies_words dobj_collecting_frequencies det_text_the prepc_by_inducing_collecting prep_from_inducing_text dobj_inducing_wordclasses prepc_in_interested_inducing cop_interested_be aux_interested_might nsubj_interested_one dep_Smadja_1993 appos_phrases_collocations dep_words_Smadja conj_and_words_phrases amod_words_other conj_or_word_interested prep_with_word_phrases prep_with_word_words det_word_a prep_of_co-occurences_interested prep_of_co-occurences_word prep_of_frequencies_co-occurences prep_in_interested_frequencies cop_interested_be aux_interested_might nsubj_interested_one prep_for_interested_instance
C00-2113	J93-1007	o	For colnparison ~ we refer here to Smadja 's method -LRB- 1993 -RRB- because this method and the proposed method have much in connnon	prep_in_much_connnon dobj_have_much nsubj_have_method nsubj_have_method mark_have_because amod_method_proposed det_method_the conj_and_method_method det_method_this appos_method_1993 poss_method_Smadja advcl_refer_have prep_to_refer_method advmod_refer_here nsubj_refer_we prep_for_refer_~ nn_~_colnparison
C00-2121	J93-1007	o	Smadja 1993 -RRB- 1	dep_Smadja_1 dep_Smadja_1993
C02-1007	J93-1007	o	Algorithms for the computation of first-order associations have been used in lexicography for the extraction of collocations -LRB- Smadja 1993 -RRB- and in cognitive psychology for the simulation of associative learning -LRB- Wettler & Rapp 1993 -RRB-	amod_Wettler_1993 conj_and_Wettler_Rapp dep_learning_Rapp dep_learning_Wettler amod_learning_associative prep_of_simulation_learning det_simulation_the amod_psychology_cognitive dep_Smadja_1993 appos_collocations_Smadja prep_of_extraction_collocations det_extraction_the prep_for_used_simulation prep_in_used_psychology prep_for_used_extraction prep_in_used_lexicography conj_and_used_used auxpass_used_been aux_used_have nsubjpass_used_Algorithms nsubjpass_used_Algorithms amod_associations_first-order prep_of_computation_associations det_computation_the prep_for_Algorithms_computation
C02-2003	J93-1007	o	Sometimes the notion of collocation is defined in terms of syntax -LRB- by possible part-of-speech patterns -RRB- or in terms of semantics -LRB- requiring collocations to exhibit non-compositional meaning -RRB- -LRB- Smadja 1993 -RRB-	amod_Smadja_1993 amod_meaning_non-compositional dobj_exhibit_meaning aux_exhibit_to vmod_requiring_exhibit dobj_requiring_collocations prep_of_terms_semantics pobj_in_terms amod_patterns_part-of-speech amod_patterns_possible dep_by_Smadja dep_by_requiring conj_or_by_in pobj_by_patterns prep_of_terms_syntax dep_defined_in dep_defined_by prep_in_defined_terms auxpass_defined_is nsubjpass_defined_notion advmod_defined_Sometimes prep_of_notion_collocation det_notion_the
C04-1141	J93-1007	p	Smadja -LRB- 1993 -RRB- which is the classic work on collocation extraction uses a two-stage filtering model in which in the first step n-gram statistics determine possible collocations and in the second step these candidates are submitted to a syntactic valida7Of course lexical material is always at least partially dependent on the domain in question	det_domain_the prep_in_dependent_question prep_on_dependent_domain advmod_dependent_at cop_dependent_is nsubj_dependent_material advmod_at_partially pobj_at_least advmod_at_always amod_material_lexical nn_course_valida7Of amod_course_syntactic det_course_a ccomp_submitted_dependent prep_to_submitted_course auxpass_submitted_are nsubjpass_submitted_candidates prep_in_submitted_step det_candidates_these amod_step_second det_step_the amod_collocations_possible conj_and_determine_submitted dobj_determine_collocations nsubj_determine_statistics prep_in_determine_step prep_in_determine_which nn_statistics_n-gram amod_step_first det_step_the rcmod_model_submitted rcmod_model_determine amod_model_filtering amod_model_two-stage det_model_a dobj_uses_model nsubj_uses_Smadja nn_extraction_collocation prep_on_work_extraction amod_work_classic det_work_the cop_work_is nsubj_work_which rcmod_Smadja_work appos_Smadja_1993
C08-1030	J93-1007	o	Future work will include -LRB- i -RRB- applying the method to retrieve other types of collocations -LRB- Smadja 1993 -RRB- and -LRB- ii -RRB- evaluating the method using Internet directories	nn_directories_Internet dobj_using_directories vmod_method_using det_method_the dobj_evaluating_method dep_ii_evaluating dep_Smadja_1993 appos_collocations_Smadja prep_of_types_collocations amod_types_other dobj_retrieve_types aux_retrieve_to vmod_method_retrieve det_method_the conj_and_applying_ii dobj_applying_method dep_i_ii dep_i_applying dep_include_i aux_include_will nsubj_include_work amod_work_Future
C94-1074	J93-1007	o	One example of the 450 latter problem is the following in -LRB- Smadja 1993 -RRB- the nature of a syntactic link between two associated words is detected a posteriori	det_posteriori_a dobj_detected_posteriori auxpass_detected_is nsubjpass_detected_nature dep_detected_Smadja mark_detected_in amod_words_associated num_words_two prep_between_link_words amod_link_syntactic det_link_a prep_of_nature_link det_nature_the num_Smadja_1993 dep_following_detected det_following_the cop_following_is nsubj_following_example amod_problem_latter num_problem_450 det_problem_the prep_of_example_problem num_example_One
C94-1074	J93-1007	o	of ACL 1990 -LRB- Smadja 1993 -RRB- F. Smadja Retrieving collocations fi ` cma text XTRACT -LRB- 1993 -RRB-	appos_XTRACT_1993 dep_text_XTRACT nn_text_cma dobj_fi_text nsubj_fi_Smadja prep_of_fi_ACL amod_collocations_Retrieving appos_Smadja_collocations nn_Smadja_F. amod_Smadja_1993 dep_ACL_Smadja num_ACL_1990
C94-1091	J93-1007	o	We propose a corpus-based method -LRB- Biber ,1993 Nagao ,1993 Smadja ,1993 -RRB- which generates Noun Classifier Associations -LRB- NCA -RRB- to overcome the problems in classifier assignment and semantic construction of noun phrase	nn_phrase_noun prep_of_construction_phrase amod_construction_semantic conj_and_assignment_construction nn_assignment_classifier prep_in_problems_construction prep_in_problems_assignment det_problems_the dobj_overcome_problems aux_overcome_to appos_Associations_NCA nn_Associations_Classifier nn_Associations_Noun xcomp_generates_overcome dobj_generates_Associations nsubj_generates_which num_Smadja_,1993 num_Nagao_,1993 dep_Biber_Smadja dep_Biber_Nagao num_Biber_,1993 rcmod_method_generates dep_method_Biber amod_method_corpus-based det_method_a dobj_propose_method nsubj_propose_We ccomp_``_propose
C94-1096	J93-1007	o	Unlike Smadja -LRB- 1993 -RRB- the ke ~ vord rnay be part of a Chinese word	amod_word_Chinese det_word_a prep_of_part_word cop_part_be nsubj_part_rnay prep_unlike_part_Smadja nn_rnay_vord nn_rnay_~ nn_rnay_ke det_rnay_the appos_Smadja_1993
C94-1096	J93-1007	o	The user can select characters by their frequencies -LRB- i.e. f and g options -RRB- the top or bottom N % -LRB- i.e. m and n options -RRB- their ranks -LRB- i.e. r and s options -RRB- and by their frequencies above two standard deviations phlS the mean -LRB- Smadja 1993 -RRB- -LRB- i.e. z option -RRB-	dep_option_z dep_i.e._option dep_Smadja_1993 dep_mean_i.e. dep_mean_Smadja det_mean_the dobj_phlS_mean amod_deviations_standard num_deviations_two prep_above_frequencies_deviations poss_frequencies_their pobj_by_frequencies nn_options_s conj_and_r_options dep_i.e._options dep_i.e._r dep_ranks_i.e. poss_ranks_their nn_options_n dep_i.e._options cc_i.e._and dep_i.e._m dep_%_phlS conj_and_%_by conj_and_%_ranks dep_%_i.e. nn_%_N amod_%_bottom amod_%_top det_%_the conj_or_top_bottom nn_options_g dep_options_f cc_f_and advmod_f_i.e. poss_frequencies_their dep_select_by dep_select_ranks dep_select_% parataxis_select_options prep_by_select_frequencies dobj_select_characters aux_select_can nsubj_select_user det_user_The
C94-1096	J93-1007	o	Further enhancement of these utilities include compiling collocation statistics -LRB- Smadja 1993 -RRB- and semi-automatic gloassary construction -LRB- Tong 1993 -RRB-	dep_Tong_1993 appos_construction_Tong amod_construction_gloassary amod_construction_semi-automatic dep_Smadja_1993 conj_and_statistics_construction dep_statistics_Smadja nn_statistics_collocation dobj_compiling_construction dobj_compiling_statistics xcomp_include_compiling nsubj_include_enhancement det_utilities_these prep_of_enhancement_utilities amod_enhancement_Further
C94-2202	J93-1007	o	For instance there is a substantial body of papers on the extraction of frequently co-occurring words from corpora using statistical methods -LRB- e.g. -LRB- Choueka et al. 1983 -RRB- -LRB- Church and Hanks 1989 -RRB- -LRB- Smadja 1993 -RRB- to list only a few -RRB-	det_few_a advmod_few_only dobj_list_few aux_list_to dep_Smadja_1993 appos_Church_1989 conj_and_Church_Hanks vmod_Choueka_list dep_Choueka_Smadja dep_Choueka_Hanks dep_Choueka_Church amod_Choueka_1983 dep_Choueka_al. nn_Choueka_et dep_,_Choueka dep_-LRB-_e.g. amod_methods_statistical dobj_using_methods prep_from_words_corpora amod_words_co-occurring advmod_co-occurring_frequently prep_of_extraction_words det_extraction_the prep_on_papers_extraction vmod_body_using prep_of_body_papers amod_body_substantial det_body_a nsubj_is_body expl_is_there prep_for_is_instance ccomp_``_is
C96-1009	J93-1007	o	Iegar -LRB- ling l his l ypu of -LRB- olloeation the approaches till ilOW could be divi -LRB- led inl o t wo groups those thai do uo -LRB- refer to s ` ttbstrings of colloco l ions as a l -RRB- arti -LRB- ular problem -LRB- Church and lla.nks t99 -LRB- -RRB- Kim and Cho 1993 Nagao and Mori 1994 -RRB- and those t.hat do -LRB- Kita et al. t994 Smadja 1993 lkchara et al. 1995 Kjelhner 11994 -RRB-	amod_Kjelhner_11994 dep_lkchara_al. nn_lkchara_et num_Smadja_1993 dep_Kita_Kjelhner dep_Kita_1995 dep_Kita_lkchara dep_Kita_Smadja appos_Kita_t994 dep_Kita_al. nn_Kita_et dep_do_Kita vmod_those_t.hat dep_Kim_1994 conj_and_Kim_Mori conj_and_Kim_Nagao conj_and_Kim_1993 conj_and_Kim_Cho nn_Kim_t99 conj_and_Church_lla.nks conj_and_problem_those appos_problem_Mori appos_problem_Nagao appos_problem_1993 appos_problem_Cho appos_problem_Kim appos_problem_lla.nks appos_problem_Church amod_problem_ular nn_problem_arti dep_l_those dep_l_problem det_l_a nn_ions_colloco conj_colloco_l prep_of_ttbstrings_ions nn_ttbstrings_s prep_as_refer_l prep_to_refer_ttbstrings aux_refer_do dobj_do_uo conj_thai_do conj_thai_refer det_thai_those dep_groups_thai nsubj_wo_groups nn_t_o dep_led_wo dep_led_t dobj_led_inl dep_divi_led cop_divi_be aux_divi_could nsubj_divi_ilOW mark_divi_till advcl_approaches_divi det_approaches_the appos_olloeation_approaches prep_of_ypu_olloeation poss_l_his conj_l_ypu conj_l_l nn_l_ling nn_l_Iegar
C96-1009	J93-1007	o	From the extracted n-grams those with a flequc ` ncy of 3 or more were kept -LRB- other approaches get rid of n-grams of such low frequencies -LRB- Smadja 1993 -RRB- -RRB-	dep_Smadja_1993 amod_frequencies_low amod_frequencies_such prep_of_n-grams_frequencies dep_rid_Smadja prep_of_rid_n-grams dep_get_rid dep_approaches_get amod_approaches_other dep_kept_approaches auxpass_kept_were nsubjpass_kept_those prep_from_kept_n-grams conj_or_3_more prep_of_ncy_more prep_of_ncy_3 nn_ncy_flequc det_ncy_a prep_with_those_ncy amod_n-grams_extracted det_n-grams_the
C96-1009	J93-1007	o	-LRB- Smadja 1993 -RRB- extracts uninterrupted as well as interrupted collocations -LRB- predicative relations rigid noun phrases and phrasal templates -RRB-	amod_templates_phrasal nn_phrases_noun amod_phrases_rigid conj_and_relations_templates conj_and_relations_phrases amod_relations_predicative dep_collocations_templates dep_collocations_phrases dep_collocations_relations amod_collocations_interrupted amod_collocations_uninterrupted conj_and_uninterrupted_interrupted dep_extracts_collocations appos_extracts_Smadja amod_Smadja_1993
C96-1009	J93-1007	o	The COlllillOil poini s regarding collocations appear to be as -LRB- Smadja 1993 -RRB- suggestsl they are m ` bil rary -LRB- it is nol clear why to Bill through means to fail -RRB- th -LRB- ' y are domain-dependent -LRB- interest rate stock market -RRB- t hey are recurrenl and cohesive lo ~ xical clusters the presence of one of the	prep_of_one_the prep_of_presence_one det_presence_the dep_clusters_presence amod_clusters_xical nn_clusters_~ nn_clusters_lo amod_clusters_cohesive cop_recurrenl_are nsubj_recurrenl_hey dep_t_suggestsl cop_t_be aux_t_to nn_market_stock dep_market_rate dep_market_bil nn_rate_interest cop_domain-dependent_are nsubj_domain-dependent_y cop_domain-dependent_fail det_y_th prep_means_to nsubj_means_nol dep_means_rary advmod_through_why prep_to_why_Bill advcl_clear_through dep_nol_clear cop_nol_is nsubj_nol_it ccomp_bil_domain-dependent parataxis_bil_means nn_bil_m cop_bil_are nsubj_bil_they dep_suggestsl_market dep_suggestsl_Smadja mark_suggestsl_as amod_Smadja_1993 conj_and_appear_clusters parataxis_appear_recurrenl xcomp_appear_t nsubj_appear_s prep_regarding_s_collocations parataxis_poini_clusters parataxis_poini_appear nn_poini_COlllillOil det_poini_The dep_``_poini
C96-1009	J93-1007	o	-LRB- Smadja 1993 Kits et al. 1994 Ikehara et al. 1995 -RRB- mention about substrings of collocations	prep_of_substrings_collocations prep_about_mention_substrings num_Ikehara_1995 nn_Ikehara_al. nn_Ikehara_et appos_Kits_mention dep_Kits_Ikehara appos_Kits_1994 dep_Kits_al. nn_Kits_et dep_Smadja_Kits appos_Smadja_1993 dep_''_Smadja
C96-1039	J93-1007	o	Some papers -LRB- Fung & Wu 1994 Wang et al. 1994 -RRB- based on Smadja 's paradigm -LRB- 1993 -RRB- learned an aided dictionary from a corpus to reduce the possibility of unknown words	amod_words_unknown prep_of_possibility_words det_possibility_the dobj_reduce_possibility aux_reduce_to det_corpus_a vmod_dictionary_reduce prep_from_dictionary_corpus amod_dictionary_aided det_dictionary_an dobj_learned_dictionary nsubj_learned_papers appos_paradigm_1993 poss_paradigm_Smadja prep_on_based_paradigm num_Wang_1994 nn_Wang_al. nn_Wang_et dep_Fung_Wang num_Fung_1994 conj_and_Fung_Wu vmod_papers_based appos_papers_Wu appos_papers_Fung det_papers_Some ccomp_``_learned
C96-1083	J93-1007	p	In the past five years important research on the automatic acquisition of word classes based on lexical distribution has been published -LRB- Church and Hanks 1990 Hindle 1990 Smadja 1993 Grei ~ nstette 1994 Grishman and Sterling 1994 -RRB-	num_Grishman_1994 conj_and_Grishman_Sterling appos_nstette_1994 num_nstette_~ nn_nstette_Grei num_Smadja_1993 num_Hindle_1990 dep_Church_Sterling dep_Church_Grishman conj_and_Church_nstette conj_and_Church_Smadja conj_and_Church_Hindle conj_and_Church_1990 conj_and_Church_Hanks dep_published_nstette dep_published_Smadja dep_published_Hindle dep_published_1990 dep_published_Hanks dep_published_Church auxpass_published_been aux_published_has nsubjpass_published_research prep_in_published_years amod_distribution_lexical prep_on_based_distribution nn_classes_word prep_of_acquisition_classes amod_acquisition_automatic det_acquisition_the vmod_research_based prep_on_research_acquisition amod_research_important num_years_five amod_years_past det_years_the
C96-1089	J93-1007	o	They first extract English collocations using the Xtract systetn -LRB- Smadja 1993 -RRB- and theu look for French coutlterparts	amod_coutlterparts_French prep_for_look_coutlterparts nn_look_theu dep_Smadja_1993 appos_systetn_Smadja amod_systetn_Xtract det_systetn_the dobj_using_systetn conj_and_collocations_look vmod_collocations_using amod_collocations_English nn_collocations_extract nsubj_collocations_They advmod_extract_first
C96-1097	J93-1007	o	There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words -LRB- Church and Hanks 1990 -RRB- the distance between words -LRB- Smadja and Makeown 1990 -RRB- and the number of combined words and frequency of appearance -LRB- Kita 1993 1994 -RRB-	amod_Kita_1994 num_Kita_1993 prep_of_words_appearance conj_and_words_frequency amod_words_combined dep_number_Kita prep_of_number_frequency prep_of_number_words det_number_the amod_1990_Makeown conj_and_Smadja_1990 dep_words_1990 dep_words_Smadja conj_and_distance_number prep_between_distance_words det_distance_the num_Hanks_1990 conj_and_Church_Hanks appos_words_Hanks appos_words_Church num_words_two prep_of_strength_words nn_strength_binding det_strength_the prep_on_focusing_strength prepc_of_method_focusing det_method_a prep_such_as_corpora_method prep_from_expressions_corpora amod_expressions_rigid dobj_extract_expressions aux_extract_to xcomp_proposed_extract vmod_method_proposed amod_method_many parataxis_are_number parataxis_are_distance nsubj_are_method expl_are_There ccomp_``_are
C96-1097	J93-1007	o	Thus conventional methods had to introduce some kinds of restrictions such as the limitation of the kind of chains or the length of chains to be extracted -LRB- Smadja 1993 Shinnou and Isahara 1995 -RRB-	num_Isahara_1995 conj_and_Smadja_Isahara conj_and_Smadja_Shinnou num_Smadja_1993 dep_extracted_Isahara dep_extracted_Shinnou dep_extracted_Smadja auxpass_extracted_be aux_extracted_to prep_of_length_chains det_length_the conj_or_kind_length prep_of_kind_chains det_kind_the vmod_limitation_extracted prep_of_limitation_length prep_of_limitation_kind det_limitation_the prep_such_as_restrictions_limitation prep_of_kinds_restrictions det_kinds_some dobj_introduce_kinds aux_introduce_to xcomp_had_introduce nsubj_had_methods advmod_had_Thus amod_methods_conventional
C96-2100	J93-1007	o	Smadja -LRB- 1993 -RRB- finds significant bigrams using an estimate of z-score -LRB- deviation from an expected mean -RRB-	amod_mean_expected det_mean_an prep_from_deviation_mean prep_of_estimate_z-score det_estimate_an dep_using_deviation dobj_using_estimate amod_bigrams_significant xcomp_finds_using dobj_finds_bigrams nsubj_finds_Smadja appos_Smadja_1993
C96-2100	J93-1007	o	-LRB- Smadja 1993 :p .168 -RRB- Kita & al.	conj_and_Kita_al. appos_Kita_Smadja num_:p_.168 num_:p_1993 appos_Smadja_:p
E06-1026	J93-1007	o	Baron and Hirst -LRB- 2004 -RRB- extracted collocations with Xtract -LRB- Smadja 1993 -RRB- and classified the collocations using the orientations of the words in the neighboring sentences	amod_sentences_neighboring det_sentences_the prep_in_words_sentences det_words_the prep_of_orientations_words det_orientations_the dobj_using_orientations vmod_collocations_using det_collocations_the dobj_classified_collocations amod_Smadja_1993 dep_Xtract_Smadja amod_collocations_extracted dep_extracted_2004 dep_extracted_Hirst conj_and_Baron_classified prep_with_Baron_Xtract conj_and_Baron_collocations
E06-1043	J93-1007	o	Most previous work on compositionality of MWEs either treat them as collocations -LRB- Smadja 1993 -RRB- or examine the distributional similarity between the expression and its constituents -LRB- McCarthy et al. 2003 Baldwin et al. 2003 Bannard et al. 2003 -RRB-	num_Bannard_2003 nn_Bannard_al. nn_Bannard_et dep_Baldwin_Bannard num_Baldwin_2003 nn_Baldwin_al. nn_Baldwin_et dep_McCarthy_Baldwin amod_McCarthy_2003 dep_McCarthy_al. nn_McCarthy_et poss_constituents_its conj_and_expression_constituents det_expression_the prep_between_similarity_constituents prep_between_similarity_expression amod_similarity_distributional det_similarity_the dobj_examine_similarity nsubj_examine_work dep_Smadja_1993 appos_collocations_Smadja dep_treat_McCarthy conj_or_treat_examine prep_as_treat_collocations dobj_treat_them preconj_treat_either nsubj_treat_work prep_of_compositionality_MWEs prep_on_work_compositionality amod_work_previous amod_work_Most
E95-1003	J93-1007	o	These measures have in fact been used previously in measuring term recognition -LRB- Smadja 1993 Bourigault 1994 Lauriston 1994 -RRB-	amod_Lauriston_1994 dep_Bourigault_Lauriston appos_Bourigault_1994 dep_Smadja_Bourigault appos_Smadja_1993 dep_recognition_Smadja nn_recognition_term dobj_measuring_recognition prepc_in_used_measuring advmod_used_previously auxpass_used_been prep_in_used_fact advcl_used_have nsubj_have_measures det_measures_These
E95-1003	J93-1007	p	One of the best efforts to quantify the performance of a term-recognition system -LRB- Smadja 1993 -RRB- does so only for one processing stage leaving unassessed the text-to-output performance of the system	det_system_the prep_of_performance_system amod_performance_text-to-output det_performance_the amod_performance_unassessed dobj_leaving_performance nn_stage_processing num_stage_one advmod_only_so dep_does_leaving prep_for_does_stage advmod_does_only nsubj_does_One appos_Smadja_1993 dep_system_Smadja nn_system_term-recognition det_system_a prep_of_performance_system det_performance_the dobj_quantify_performance aux_quantify_to vmod_efforts_quantify amod_efforts_best det_efforts_the prep_of_One_efforts ccomp_``_does
I08-1014	J93-1007	o	Since Odds = P / -LRB- 1 P -RRB- we multiply both sides of Definition 3 by -LRB- 1P -LRB- U | E -RRB- -RRB- 1 to obtain P -LRB- U | E -RRB- 1P -LRB- U | E -RRB- = P -LRB- E | U -RRB- P -LRB- U -RRB- P -LRB- E -RRB- -LRB- 1P -LRB- U | E -RRB- -RRB- -LRB- 7 -RRB- By substituting Equation 6 in Equation 7 and later applying the multiplication rule P -LRB- U | E -RRB- P -LRB- E -RRB- = P -LRB- E | U -RRB- P -LRB- U -RRB- to it we will obtain P -LRB- U | E -RRB- P -LRB- U | E -RRB- = P -LRB- E | U -RRB- P -LRB- U -RRB- P -LRB- E | U -RRB- P -LRB- U -RRB- -LRB- 8 -RRB- We proceed to take the log of the odds in Equation 8 -LRB- i.e. logit -RRB- to get log P -LRB- E | U -RRB- P -LRB- E | U -RRB- = log P -LRB- U | E -RRB- P -LRB- U | E -RRB- log P -LRB- U -RRB- P -LRB- U -RRB- -LRB- 9 -RRB- While it is obvious that certain words tend to cooccur more frequently than others -LRB- i.e. idioms and collocations -RRB- such phenomena are largely arbitrary -LRB- Smadja 1993 -RRB-	amod_Smadja_1993 dep_arbitrary_Smadja advmod_arbitrary_largely cop_arbitrary_are nsubj_arbitrary_phenomena dep_arbitrary_others mark_arbitrary_than amod_phenomena_such conj_and_idioms_collocations dep_idioms_i.e. dep_others_collocations dep_others_idioms advcl_frequently_arbitrary advmod_frequently_more advmod_cooccur_frequently aux_cooccur_to xcomp_tend_cooccur nsubj_tend_words mark_tend_that amod_words_certain ccomp_obvious_tend cop_obvious_is nsubj_obvious_it mark_obvious_While appos_P_9 appos_P_U nn_P_P appos_P_U nn_P_log nn_P_P num_E_| nn_E_U appos_P_E nn_P_P num_E_| nn_E_U appos_P_E nn_P_log dep_=_P num_U_| nn_U_E advcl_P_obvious amod_P_= appos_P_U nn_P_P num_U_| nn_U_E appos_P_U nn_P_log dep_get_P aux_get_to dep_logit_i.e. num_Equation_8 prep_in_odds_Equation det_odds_the prep_of_log_odds det_log_the vmod_take_get dep_take_logit dobj_take_log aux_take_to xcomp_proceed_take nsubj_proceed_We appos_P_8 appos_P_U nn_P_P num_U_| nn_U_E appos_P_U nn_P_P appos_P_U nn_P_P num_U_| nn_U_E appos_P_U dep_=_P num_E_| nn_E_U rcmod_P_proceed amod_P_= appos_P_E nn_P_P num_E_| nn_E_U appos_P_E aux_obtain_will nsubj_obtain_we vmod_obtain_applying advmod_obtain_later appos_P_U nn_P_P num_U_| nn_U_E appos_P_U prep_to_=_it dep_=_P amod_P_= appos_P_E nn_P_P num_E_| nn_E_U appos_P_E dep_rule_P nn_rule_multiplication det_rule_the dobj_applying_rule num_Equation_7 conj_and_Equation_obtain prep_in_Equation_Equation num_Equation_6 dobj_substituting_obtain dobj_substituting_Equation num_E_| nn_E_U appos_1P_E appos_P_7 dep_P_1P appos_P_E nn_P_P appos_P_U nn_P_P num_U_| nn_U_E appos_P_U dep_=_P num_E_| nn_E_U prepc_by_1P_substituting amod_1P_= appos_1P_E nn_1P_P num_E_| nn_E_U appos_P_E aux_obtain_to num_E_| nn_E_U dep_1P_P dep_1P_1P vmod_1P_obtain num_1P_1 appos_1P_E num_Definition_3 prep_of_sides_Definition det_sides_both prep_by_multiply_1P dobj_multiply_sides nsubj_multiply_we rcmod_P_multiply dep_P_1 prep_since_P_P amod_P_= nn_P_Odds
J00-3001	J93-1007	o	In Smadja 's collocation algorithm Xtract the lowest-frequency words are effectively discarded as well -LRB- Smadja 1993 -RRB-	num_Smadja_1993 dep_well_Smadja advmod_well_as advmod_discarded_well advmod_discarded_effectively auxpass_discarded_are nsubjpass_discarded_Xtract nn_words_lowest-frequency det_words_the appos_Xtract_words rcmod_algorithm_discarded nn_algorithm_collocation poss_algorithm_Smadja pobj_In_algorithm dep_``_In
J94-4003	J93-1007	o	The use of such relations -LRB- mainly relations between verbs or nouns and their arguments and modifiers -RRB- for various purposes has received growing attention in recent research -LRB- Church and Hanks 1990 Zernik and Jacobs 1990 Hindle 1990 Smadja 1993 -RRB-	num_Smadja_1993 num_Hindle_1990 num_Jacobs_1990 conj_and_Zernik_Jacobs num_Hanks_1990 dep_Church_Smadja conj_and_Church_Hindle conj_and_Church_Jacobs conj_and_Church_Zernik conj_and_Church_Hanks dep_research_Hindle dep_research_Zernik dep_research_Hanks dep_research_Church amod_research_recent amod_attention_growing prep_in_received_research dobj_received_attention aux_received_has nsubj_received_use amod_purposes_various poss_arguments_their conj_and_verbs_modifiers conj_and_verbs_arguments conj_or_verbs_nouns prep_between_relations_modifiers prep_between_relations_arguments prep_between_relations_nouns prep_between_relations_verbs advmod_relations_mainly amod_relations_such prep_for_use_purposes dep_use_relations prep_of_use_relations det_use_The
J94-4003	J93-1007	o	Statistics on co-occurrence of words in a local context were used recently for monolingual word sense disambiguation -LRB- Gale Church and Yarowsky 1992b 1993 Sch6tze 1992 1993 -RRB- -LRB- see Section 7 for more details and Church and Hanks 1990 Smadja 1993 for other applications of these statistics -RRB-	det_statistics_these prep_of_applications_statistics amod_applications_other prep_for_Smadja_applications num_Smadja_1993 num_Hanks_1990 conj_and_details_Hanks conj_and_details_Church amod_details_more num_Section_7 parataxis_see_Smadja prep_for_see_Hanks prep_for_see_Church prep_for_see_details dobj_see_Section num_Sch6tze_1993 num_Sch6tze_1992 appos_1992b_1993 nn_1992b_Yarowsky dep_Gale_see dep_Gale_Sch6tze conj_and_Gale_1992b conj_and_Gale_Church dep_disambiguation_1992b dep_disambiguation_Church dep_disambiguation_Gale nn_disambiguation_sense nn_disambiguation_word amod_disambiguation_monolingual prep_for_used_disambiguation advmod_used_recently auxpass_used_were nsubjpass_used_Statistics amod_context_local det_context_a prep_of_co-occurrence_words prep_in_Statistics_context prep_on_Statistics_co-occurrence
J98-2002	J93-1007	p	For the extraction problem there have been various methods proposed to date which are quite adequate -LRB- Hindle and Rooth 1991 Grishman and Sterling 1992 Manning 1992 Utsuro Matsumoto and Nagao 1992 Brent 1993 Smadja 1993 Grefenstette 1994 Briscoe and Carroll 1997 -RRB-	num_Carroll_1997 conj_and_Briscoe_Carroll num_Grefenstette_1994 num_Smadja_1993 num_Brent_1993 num_Nagao_1992 conj_and_Utsuro_Nagao conj_and_Utsuro_Matsumoto dobj_Manning_1992 num_Sterling_1992 conj_and_Grishman_Sterling num_Rooth_1991 dep_Hindle_Carroll dep_Hindle_Briscoe dep_Hindle_Grefenstette dep_Hindle_Smadja dep_Hindle_Brent dep_Hindle_Nagao dep_Hindle_Matsumoto dep_Hindle_Utsuro dep_Hindle_Manning dep_Hindle_Sterling dep_Hindle_Grishman conj_and_Hindle_Rooth dep_adequate_Rooth dep_adequate_Hindle advmod_adequate_quite cop_adequate_are nsubj_adequate_which prep_to_proposed_date rcmod_methods_adequate vmod_methods_proposed amod_methods_various cop_methods_been aux_methods_have expl_methods_there prep_for_methods_problem nn_problem_extraction det_problem_the
J98-2002	J93-1007	o	As we remarked earlier however the input data required by our method -LRB- triples -RRB- could be generated automatically from unparsed corpora making use of existing heuristic rules -LRB- Brent 1993 Smadja 1993 -RRB- although for the experiments we report here we used a parsed corpus	amod_corpus_parsed det_corpus_a dobj_used_corpus nsubj_used_we ccomp_report_used advmod_report_here nsubj_report_we prep_for_report_experiments mark_report_although det_experiments_the num_Smadja_1993 dep_Smadja_Brent num_Brent_1993 appos_rules_Smadja nn_rules_heuristic amod_rules_existing prep_of_use_rules dobj_making_use vmod_corpora_making amod_corpora_unparsed advcl_generated_report prep_from_generated_corpora advmod_generated_automatically auxpass_generated_be aux_generated_could nsubjpass_generated_data advmod_generated_however advcl_generated_remarked appos_method_triples poss_method_our agent_required_method vmod_data_required nn_data_input det_data_the advmod_remarked_earlier nsubj_remarked_we mark_remarked_As
N07-1037	J93-1007	o	Baron and Hirst -LRB- 2004 -RRB- extracted collocations with Xtract -LRB- Smadja 1993 -RRB- and classified the collocations using the orientations of the words in the neighboring sentences	amod_sentences_neighboring det_sentences_the prep_in_words_sentences det_words_the prep_of_orientations_words det_orientations_the dobj_using_orientations vmod_collocations_using det_collocations_the dobj_classified_collocations amod_Smadja_1993 dep_Xtract_Smadja amod_collocations_extracted dep_extracted_2004 dep_extracted_Hirst conj_and_Baron_classified prep_with_Baron_Xtract conj_and_Baron_collocations
P04-1022	J93-1007	o	Some studies have been done for acquiring collocation translations using parallel corpora -LRB- Smadja et al 1996 Kupiec 1993 Echizen-ya et al. 2003 -RRB-	num_Echizen-ya_2003 nn_Echizen-ya_al. nn_Echizen-ya_et num_Kupiec_1993 dep_Smadja_Echizen-ya dep_Smadja_Kupiec amod_Smadja_1996 dep_Smadja_al nn_Smadja_et appos_corpora_Smadja amod_corpora_parallel dobj_using_corpora nn_translations_collocation vmod_acquiring_using dobj_acquiring_translations prepc_for_done_acquiring auxpass_done_been aux_done_have nsubjpass_done_studies det_studies_Some ccomp_``_done
P04-1022	J93-1007	o	The former extracts collocations within a fixed window -LRB- Church and Hanks 1990 Smadja 1993 -RRB-	amod_Smadja_1993 num_Hanks_1990 dep_Church_Smadja conj_and_Church_Hanks appos_window_Hanks appos_window_Church amod_window_fixed det_window_a prep_within_collocations_window nn_collocations_extracts amod_collocations_former det_collocations_The dep_``_collocations
P04-1022	J93-1007	o	These range from twoword to multi-word with or without syntactic structure -LRB- Smadja 1993 Lin 1998 Pearce 2001 Seretan et al. 2003 -RRB-	dep_al._2003 nn_al._et nn_al._Seretan num_Pearce_2001 dep_Lin_al. conj_Lin_Pearce num_Lin_1998 dep_Smadja_Lin num_Smadja_1993 dep_structure_Smadja amod_structure_syntactic prep_without_from_structure prep_with_from_structure conj_or_from_from prep_to_range_multi-word prep_from_range_twoword det_range_These
P04-3019	J93-1007	o	Smadja -LRB- 1993 -RRB- also detailed techniques for collocation extraction and developed a program called XTRACT which is capable of computing flexible collocations based on elaborated statistical calculation	amod_calculation_statistical amod_calculation_elaborated prep_on_based_calculation vmod_collocations_based amod_collocations_flexible amod_collocations_computing prep_of_capable_collocations cop_capable_is nsubj_capable_which dep_called_XTRACT rcmod_program_capable vmod_program_called det_program_a dobj_developed_program nn_extraction_collocation conj_and_techniques_developed prep_for_techniques_extraction amod_techniques_detailed nn_techniques_Smadja advmod_detailed_also appos_Smadja_1993
P05-1075	J93-1007	o	3 Schone & Jurafsky 's results indicate similar results for log-likelihood & T-score and strong parallelism among information-theoretic measures such as ChiSquared Selectional Association -LRB- Resnik 1996 -RRB- Symmetric Conditional Probability -LRB- Ferreira and Pereira Lopes 1999 -RRB- and the Z-Score -LRB- Smadja 1993 -RRB-	num_Smadja_1993 appos_Z-Score_Smadja det_Z-Score_the nn_Lopes_Pereira amod_Ferreira_1999 conj_and_Ferreira_Lopes dep_Probability_Lopes dep_Probability_Ferreira amod_Probability_Conditional amod_Probability_Symmetric num_Resnik_1996 appos_Association_Resnik nn_Association_Selectional conj_and_ChiSquared_Z-Score conj_and_ChiSquared_Probability conj_and_ChiSquared_Association prep_such_as_measures_Z-Score prep_such_as_measures_Probability prep_such_as_measures_Association prep_such_as_measures_ChiSquared amod_measures_information-theoretic prep_among_parallelism_measures amod_parallelism_strong conj_and_log-likelihood_T-score conj_and_results_parallelism prep_for_results_T-score prep_for_results_log-likelihood amod_results_similar dobj_indicate_parallelism dobj_indicate_results nsubj_indicate_results poss_results_Jurafsky poss_results_Schone conj_and_Schone_Jurafsky num_Schone_3
P05-1075	J93-1007	o	It is true that various term extraction systems have been developed such as Xtract -LRB- Smadja 1993 -RRB- Termight -LRB- Dagan & Church 1994 -RRB- and TERMS -LRB- Justeson & Katz 1995 -RRB- among others -LRB- cf.	dep_Justeson_1995 conj_and_Justeson_Katz appos_TERMS_Katz appos_TERMS_Justeson num_Dagan_1994 conj_and_Dagan_Church appos_Termight_Church appos_Termight_Dagan num_Smadja_1993 conj_and_Xtract_TERMS conj_and_Xtract_Termight appos_Xtract_Smadja dep_developed_cf. prep_among_developed_others prep_such_as_developed_TERMS prep_such_as_developed_Termight prep_such_as_developed_Xtract auxpass_developed_been aux_developed_have nsubjpass_developed_systems mark_developed_that nn_systems_extraction nn_systems_term amod_systems_various ccomp_true_developed cop_true_is nsubj_true_It ccomp_``_true
P06-1120	J93-1007	o	Parsing has been also used after extraction -LRB- Smadja 1993 -RRB- for filtering out invalid results	amod_results_invalid dobj_filtering_results prt_filtering_out appos_Smadja_1993 dep_extraction_Smadja prepc_for_used_filtering prep_after_used_extraction advmod_used_also auxpass_used_been aux_used_has nsubjpass_used_Parsing
P06-1120	J93-1007	o	This fact is being seriously challenged by current research -LRB- -RRB- and might not be true in the near future -LRB- Smadja 1993 151 -RRB-	amod_Smadja_151 num_Smadja_1993 dep_future_Smadja amod_future_near det_future_the prep_in_true_future cop_true_be neg_true_not aux_true_might conj_and_research_true amod_research_current agent_challenged_true agent_challenged_research advmod_challenged_seriously auxpass_challenged_being aux_challenged_is nsubjpass_challenged_fact det_fact_This
P95-1007	J93-1007	o	Similarly Smadja -LRB- 1993 -RRB- uses a six content word window to extract significant collocations	amod_collocations_significant dobj_extract_collocations aux_extract_to vmod_window_extract nn_window_word nn_window_content num_window_six det_window_a dobj_uses_window nsubj_uses_Smadja advmod_uses_Similarly appos_Smadja_1993
P95-1027	J93-1007	o	Finally knowledge of polarity can be combined with corpus-based collocation extraction methods -LRB- Smadja 1993 -RRB- to automatically produce entries for the lexical functions used in MeaningText Theory -LRB- Mel ' ~ uk and Pertsov 1987 -RRB- for text generation	nn_generation_text dep_uk_1987 conj_and_uk_Pertsov nn_uk_~ poss_uk_Mel nn_Theory_MeaningText prep_in_used_Theory vmod_functions_used amod_functions_lexical det_functions_the prep_for_entries_functions dobj_produce_entries advmod_produce_automatically aux_produce_to dep_Smadja_1993 dep_methods_Smadja nn_methods_extraction nn_methods_collocation amod_methods_corpus-based prep_for_combined_generation dep_combined_Pertsov dep_combined_uk xcomp_combined_produce prep_with_combined_methods auxpass_combined_be aux_combined_can nsubjpass_combined_knowledge advmod_combined_Finally prep_of_knowledge_polarity ccomp_``_combined
P97-1061	J93-1007	o	4 Related work Algorithms for retrieving collocations has been described -LRB- Smadja 1993 -RRB- -LRB- Haruno et al. 1996 -RRB-	amod_Haruno_1996 dep_Haruno_al. nn_Haruno_et dep_Smadja_1993 dep_described_Haruno dep_described_Smadja auxpass_described_been aux_described_has nsubjpass_described_Algorithms amod_collocations_retrieving prep_for_Algorithms_collocations nn_Algorithms_work amod_Algorithms_Related num_Algorithms_4
P97-1061	J93-1007	o	-LRB- Smadja 1993 -RRB- proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold 3	num_threshold_3 pobj_given_threshold prep_a_given prep_than_greater_a cop_greater_are nsubj_greater_cooccurrences poss_cooccurrences_whose rcmod_bigrams_greater dobj_combining_bigrams prepc_by_retrieve_combining dobj_retrieve_collocations aux_retrieve_to vmod_method_retrieve det_method_a dobj_proposed_method nsubj_proposed_Smadja amod_Smadja_1993
P97-1061	J93-1007	o	There has been a growing interest in corpus-based approaches which retrieve collocations from large corpora -LRB- Nagao and Mori 1994 -RRB- -LRB- Ikehara et al. 1996 -RRB- -LRB- Kupiec 1993 -RRB- -LRB- Fung 1995 -RRB- -LRB- Kitamura and Matsumoto 1996 -RRB- -LRB- Smadja 1993 -RRB- -LRB- Smadja et al. 1996 -RRB- -LRB- Haruno et al. 1996 -RRB-	amod_Haruno_1996 dep_Haruno_al. nn_Haruno_et amod_Smadja_1996 dep_Smadja_al. nn_Smadja_et dep_Smadja_1993 dep_Kitamura_1996 conj_and_Kitamura_Matsumoto dep_Fung_1995 dep_Kupiec_1993 dep_Ikehara_1996 dep_Ikehara_al. nn_Ikehara_et dep_Nagao_1994 conj_and_Nagao_Mori appos_corpora_Mori appos_corpora_Nagao amod_corpora_large prep_from_retrieve_corpora dobj_retrieve_collocations nsubj_retrieve_which rcmod_approaches_retrieve amod_approaches_corpus-based appos_interest_Haruno appos_interest_Smadja appos_interest_Smadja appos_interest_Matsumoto appos_interest_Kitamura appos_interest_Fung dep_interest_Kupiec dep_interest_Ikehara prep_in_interest_approaches amod_interest_growing det_interest_a cop_interest_been aux_interest_has expl_interest_There
P98-1092	J93-1007	o	However morphosyntactic features alone can not verify the terminological status of the units extracted since they can also select non terms -LRB- see Smadja 1993 -RRB-	num_Smadja_1993 dobj_see_Smadja dep_terms_see amod_terms_non dobj_select_terms advmod_select_also aux_select_can nsubj_select_they mark_select_since advcl_extracted_select vmod_units_extracted det_units_the prep_of_status_units amod_status_terminological det_status_the dobj_verify_status neg_verify_not aux_verify_can nsubj_verify_features advmod_verify_However advmod_features_alone amod_features_morphosyntactic
P98-2125	J93-1007	o	We then replaced fi with its associated z-score k $ e. k $ e is the strength of code frequency f at Lt and represents the standard deviation above the average of frequency fave t. Referring to Smadja 's definition -LRB- Smadja 1993 -RRB- the standard deviation at at Lt and strength kf t of the code frequencies are defined as shown in formulas 1 and 2	conj_and_1_2 dep_formulas_2 dep_formulas_1 prep_in_shown_formulas mark_shown_as advcl_defined_shown auxpass_defined_are nsubjpass_defined_deviation nn_frequencies_code det_frequencies_the prep_of_t_frequencies appos_kf_t nn_kf_strength nn_kf_Lt conj_and_Lt_strength pobj_at_kf pcomp_at_at prep_deviation_at amod_deviation_standard det_deviation_the amod_Smadja_1993 dep_definition_Smadja poss_definition_Smadja prep_to_Referring_definition vmod_t._Referring nn_fave_frequency prep_of_average_fave det_average_the prep_above_deviation_average amod_deviation_standard det_deviation_the dobj_represents_deviation nsubj_represents_strength prep_at_f_Lt dep_frequency_f nn_frequency_code prep_of_strength_frequency det_strength_the conj_and_is_defined conj_and_is_t. conj_and_is_represents nsubj_is_strength dep_is_e nn_$_k nn_k_e. appos_k_$ num_k_$ nn_k_z-score amod_k_associated poss_k_its dep_replaced_defined dep_replaced_t. dep_replaced_represents dep_replaced_is prep_with_replaced_k dobj_replaced_fi advmod_replaced_then nsubj_replaced_We
P98-2127	J93-1007	o	In -LRB- Smadja 1993 -RRB- automatically extracted collocations are judged by a lexicographer	det_lexicographer_a agent_judged_lexicographer auxpass_judged_are nsubjpass_judged_collocations prep_judged_In amod_collocations_extracted advmod_extracted_automatically dep_Smadja_1993 dep_In_Smadja
P98-2176	J93-1007	o	Some examples of language reuse include collocation analysis -LRB- Smadja 1993 -RRB- the use of entire factual sentences extracted from corpora -LRB- e.g. ` Toy Story ' is the Academy Award winning animated film developed by Pixar ~ ' -RRB- and summarization using sentence extraction -LRB- Paice 1990 Kupiec et al. 1995 -RRB-	num_Kupiec_1995 nn_Kupiec_al. nn_Kupiec_et dep_Paice_Kupiec appos_Paice_1990 appos_extraction_Paice nn_extraction_sentence dobj_using_extraction vmod_summarization_using nn_~_Pixar agent_developed_~ vmod_film_developed amod_film_animated dobj_winning_film vmod_Award_winning nn_Award_Academy det_Award_the nsubj_is_Award conj_and_Story_summarization dep_Story_is possessive_Story_' nn_Story_Toy dep_e.g._summarization dep_e.g._Story dep_corpora_e.g. prep_from_extracted_corpora vmod_sentences_extracted amod_sentences_factual amod_sentences_entire prep_of_use_sentences det_use_the appos_Smadja_1993 conj_analysis_use dep_analysis_Smadja nn_analysis_collocation dobj_include_analysis nsubj_include_examples nn_reuse_language prep_of_examples_reuse det_examples_Some ccomp_``_include
P98-2216	J93-1007	o	Other classes such as the ones below can be extracted using lexico-statistical tools such as in -LRB- Smadja 1993 -RRB- and then checked by a human	det_human_a prep_by_checked_human advmod_checked_then conj_and_Smadja_checked dep_Smadja_1993 pobj_in_checked pobj_in_Smadja prepc_such_as_tools_in amod_tools_lexico-statistical dobj_using_tools xcomp_extracted_using auxpass_extracted_be aux_extracted_can nsubjpass_extracted_below rcmod_ones_extracted det_ones_the prep_such_as_classes_ones amod_classes_Other
P98-2216	J93-1007	o	It seems nevertheless that all 2Church and Hanks -LRB- 1989 -RRB- Smadja -LRB- 1993 -RRB- use statistics in their algorithms to extract collocations from texts	prep_from_extract_texts dobj_extract_collocations aux_extract_to poss_algorithms_their prep_in_statistics_algorithms nn_statistics_use nn_statistics_Smadja appos_Smadja_1993 appos_Hanks_1989 vmod_2Church_extract appos_2Church_statistics conj_and_2Church_Hanks det_2Church_all prep_that_seems_Hanks prep_that_seems_2Church advmod_seems_nevertheless nsubj_seems_It
P99-1029	J93-1007	o	For the correct identification of phrases in a Korean query it would help to identify the lexical relations and produce statistical information on pairs of words in a text corpus as in Smadja -LRB- 1993 -RRB-	appos_Smadja_1993 pobj_in_Smadja pcomp_as_in nn_corpus_text det_corpus_a prep_of_pairs_words prep_on_information_pairs amod_information_statistical prep_produce_as prep_in_produce_corpus dobj_produce_information nsubj_produce_it amod_relations_lexical det_relations_the dobj_identify_relations aux_identify_to conj_and_help_produce xcomp_help_identify aux_help_would nsubj_help_it prep_for_help_identification amod_query_Korean det_query_a prep_in_phrases_query prep_of_identification_phrases amod_identification_correct det_identification_the
P99-1041	J93-1007	o	It is clear that Appendix B contains far fewer true non-compositional phrases than Appendix A. 7 Related Work There have been numerous previous research on extracting collocations from corpus e.g. -LRB- Choueka 1988 -RRB- and -LRB- Smadja 1993 -RRB-	amod_Smadja_1993 dep_Choueka_1988 prep_from_extracting_corpus dobj_extracting_collocations conj_and_research_Smadja appos_research_Choueka dep_research_e.g. prepc_on_research_extracting amod_research_previous amod_research_numerous cop_research_been aux_research_have expl_research_There amod_Work_Related num_Work_7 nn_Work_A. nn_Work_Appendix prep_than_phrases_Work amod_phrases_non-compositional amod_phrases_true amod_phrases_fewer advmod_fewer_far dep_contains_Smadja dep_contains_research dobj_contains_phrases nsubj_contains_B mark_contains_that nn_B_Appendix ccomp_clear_contains cop_clear_is nsubj_clear_It
P99-1043	J93-1007	o	Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction -LRB- Smadja 1993 Fung and Wu 1994 -RRB- phrasal translation -LRB- Smadja et al. 1996 Kupiec 1993 Wu 1995 Dagan and Church 1994 -RRB- target word selection -LRB- Liu and Li 1997 Tanaka and Iwasaki 1996 -RRB- domain word translation -LRB- Fung and Lo 1998 Fung 1998 -RRB- sense disambiguation -LRB- Brown et al. 1991 Dagan et al. 1991 Dagan and Itai 1994 Gale et al. 1992a Gale et al. 1992b Gale et al. 1992c Shiitze 1992 Gale et al. 1993 Yarowsky 1995 -RRB- and even recently for query translation in cross-language IR as well -LRB- Ballesteros and Croft 1998 -RRB-	amod_Ballesteros_1998 conj_and_Ballesteros_Croft dep_well_Croft dep_well_Ballesteros advmod_well_as amod_IR_cross-language advmod_translation_well prep_in_translation_IR nn_translation_query pobj_for_translation advmod_for_recently advmod_recently_even dep_Yarowsky_1995 num_Gale_1993 nn_Gale_al. nn_Gale_et num_Shiitze_1992 conj_and_1992c_for conj_and_1992c_Yarowsky conj_and_1992c_Gale conj_and_1992c_Shiitze dep_al._for dep_al._Yarowsky dep_al._Gale dep_al._Shiitze dep_al._1992c nn_al._et nn_al._Gale appos_al._1992b nn_al._et nn_al._Gale nn_al._et nn_al._Gale num_Dagan_1994 conj_and_Dagan_Itai nn_al._et nn_al._Dagan dep_al._al. conj_al._al. appos_al._1992a dep_al._al. dep_al._Itai dep_al._Dagan num_al._1991 dep_al._al. num_al._1991 nn_al._et amod_al._Brown dep_disambiguation_al. nn_disambiguation_sense num_Fung_1998 appos_Fung_disambiguation dep_Fung_Fung conj_and_Fung_1998 conj_and_Fung_Lo dep_translation_1998 dep_translation_Lo dep_translation_Fung nn_translation_word nn_translation_domain dep_Tanaka_1996 conj_and_Tanaka_Iwasaki dep_Liu_Iwasaki dep_Liu_Tanaka appos_Liu_1997 conj_and_Liu_Li appos_selection_Li appos_selection_Liu nn_selection_word nn_selection_target num_Wu_1995 dep_Kupiec_1994 conj_and_Kupiec_Church conj_and_Kupiec_Dagan conj_and_Kupiec_Wu num_Kupiec_1993 dep_Smadja_Church dep_Smadja_Dagan dep_Smadja_Wu dep_Smadja_Kupiec appos_Smadja_1996 dep_Smadja_al. nn_Smadja_et appos_translation_translation conj_translation_selection appos_translation_Smadja amod_translation_phrasal num_Fung_1994 conj_and_Fung_Wu conj_Smadja_Wu conj_Smadja_Fung conj_Smadja_1993 dep_extraction_Smadja nn_extraction_phrase dobj_used_translation prep_in_used_extraction auxpass_used_been aux_used_has nsubjpass_used_information amod_sentence_same det_sentence_the prep_in_words_sentence conj_and_words_words amod_words_neighboring prep_between_information_words prep_between_information_words nn_information_Co-occurrence
P99-1043	J93-1007	o	Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora -LRB- Smadja et al. 1996 Kupiec 1993 Wu 1995 Tanaka and Iwasaki 1996 Fung and Lo 1998 -RRB- or monolingual corpora -LRB- Smadja 1993 Fung and Wu 1994 Liu and Li 1997 Shiitze 1992 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 appos_Shiitze_1992 num_Liu_1997 conj_and_Liu_Li num_Fung_1994 conj_and_Fung_Wu dep_Smadja_Yarowsky conj_Smadja_Shiitze conj_Smadja_Li conj_Smadja_Liu conj_Smadja_Wu conj_Smadja_Fung conj_Smadja_1993 dep_corpora_Smadja amod_corpora_monolingual num_Wu_1995 dep_Kupiec_1998 conj_and_Kupiec_Lo conj_and_Kupiec_Fung conj_and_Kupiec_1996 conj_and_Kupiec_Iwasaki conj_and_Kupiec_Tanaka conj_and_Kupiec_Wu num_Kupiec_1993 conj_or_Smadja_corpora dep_Smadja_Lo dep_Smadja_Fung dep_Smadja_1996 dep_Smadja_Iwasaki dep_Smadja_Tanaka dep_Smadja_Wu dep_Smadja_Kupiec appos_Smadja_1996 dep_Smadja_al. nn_Smadja_et amod_corpora_non-parallel num_corpora_334 conj_and_parallel_corpora amod_parallel_bilingual preconj_parallel_either dep_collected_corpora dep_collected_Smadja prep_from_collected_corpora prep_from_collected_parallel auxpass_collected_is nsubjpass_collected_statistics nn_statistics_Co-occurrence
W00-1203	J93-1007	o	The recurrence property had been utilized to extract keywords or key-phrases from text -LRB- Chien 1999 Fung 1998 Smadja 1993 -RRB-	num_Smadja_1993 num_Fung_1998 appos_Chien_Smadja appos_Chien_Fung num_Chien_1999 dep_text_Chien conj_or_keywords_key-phrases prep_from_extract_text dobj_extract_key-phrases dobj_extract_keywords aux_extract_to xcomp_utilized_extract auxpass_utilized_been aux_utilized_had nsubjpass_utilized_property nn_property_recurrence det_property_The
W01-0513	J93-1007	o	Since we need knowledge-poor Daille 1996 -RRB- induction we can not use human-suggested filtering Chi-squared -LRB- G24 -RRB- 2 -LRB- Church and Gale 1991 -RRB- Z-Score -LRB- Smadja 1993 Fontenelle et al. 1994 -RRB- Students t-Score -LRB- Church and Hanks 1990 -RRB- n-gram list in accordance to each probabilistic algorithm	amod_algorithm_probabilistic det_algorithm_each prep_to_list_algorithm prep_in_list_accordance nn_list_n-gram dep_list_Hanks dep_list_Church nn_list_t-Score nn_list_Students dep_Church_1990 conj_and_Church_Hanks dep_al._1994 nn_al._et dep_Fontenelle_al. dep_Smadja_1993 nn_Smadja_Z-Score dep_Smadja_Gale dep_Gale_1991 conj_and_Church_Smadja num_Church_2 amod_Church_Chi-squared amod_Church_filtering amod_Church_human-suggested appos_Chi-squared_G24 dobj_use_Smadja dobj_use_Church neg_use_not aux_use_can nsubj_use_we dep_induction_list dep_induction_Fontenelle rcmod_induction_use dep_induction_need dep_Daille_1996 amod_Daille_knowledge-poor dobj_need_Daille nsubj_need_we mark_need_Since
W02-0909	J93-1007	o	The second method considers the means and variance of the distance between two words and can compute flexible collocations -LRB- Smadja 1993 -RRB-	amod_Smadja_1993 dep_collocations_Smadja amod_collocations_flexible dobj_compute_collocations aux_compute_can nsubj_compute_method num_words_two prep_between_distance_words det_distance_the prep_of_means_distance conj_and_means_variance det_means_the conj_and_considers_compute dobj_considers_variance dobj_considers_means nsubj_considers_method amod_method_second det_method_The ccomp_``_compute ccomp_``_considers
W02-1606	J93-1007	o	To perform code generalization Li adopted to Smadjas work -LRB- Smadja 1993 -RRB- and defined the code strength using a code frequency and a standard deviation in each level of the concept hierarchy	nn_hierarchy_concept det_hierarchy_the prep_of_level_hierarchy det_level_each prep_in_deviation_level amod_deviation_standard det_deviation_a conj_and_frequency_deviation nn_frequency_code det_frequency_a dobj_using_deviation dobj_using_frequency vmod_strength_using nn_strength_code det_strength_the dobj_defined_strength nsubj_defined_Li dep_Smadja_1993 dep_work_Smadja nn_work_Smadjas conj_and_adopted_defined prep_to_adopted_work nsubj_adopted_Li advcl_adopted_perform nn_generalization_code dobj_perform_generalization aux_perform_To
W02-2001	J93-1007	o	One aspect of VPCs that makes them dicult to extract -LRB- cited in e.g. Smadja -LRB- 1993 -RRB- -RRB- is that the verb and particle can be non-contiguous e.g. hand the paper in and battle right on	prep_right_on pobj_in_right conj_and_in_battle prep_paper_battle prep_paper_in det_paper_the dep_hand_paper nn_hand_e.g. appos_non-contiguous_hand cop_non-contiguous_be aux_non-contiguous_can nsubj_non-contiguous_particle nsubj_non-contiguous_verb det_non-contiguous_the conj_and_verb_particle prep_that_is_non-contiguous nsubj_is_aspect dep_Smadja_1993 dep_e.g._Smadja dep_in_e.g. prep_cited_in dep_dicult_cited prep_to_dicult_extract nsubj_dicult_them xcomp_makes_dicult nsubj_makes_that rcmod_VPCs_makes prep_of_aspect_VPCs num_aspect_One ccomp_``_is
W02-2001	J93-1007	o	One of the earliest attempts at extracting \ interrupted collocations -LRB- i.e. non-contiguous collocations including VPCs -RRB- was that of Smadja -LRB- 1993 -RRB-	appos_Smadja_1993 prep_of_that_Smadja dep_was_that dep_collocations_was prep_including_collocations_VPCs amod_collocations_non-contiguous advmod_collocations_i.e. amod_collocations_interrupted npadvmod_interrupted_\ dobj_extracting_collocations prepc_at_attempts_extracting amod_attempts_earliest det_attempts_the appos_One_collocations prep_of_One_attempts
W02-2001	J93-1007	o	4 Method-2 Simple Chunk-based Extraction To overcome the shortcomings of the Brill tagger in identifying particles we next look to full chunk 2Note this is the same as the maximum span length of 5 used by Smadja -LRB- 1993 -RRB- and above the maximum attested NP length of 3 from our corpus study -LRB- see Section 2.2 -RRB-	num_Section_2.2 dobj_see_Section nn_study_corpus poss_study_our prep_of_length_3 nn_length_NP parataxis_attested_see prep_from_attested_study dobj_attested_length nsubj_attested_maximum mark_attested_above det_maximum_the appos_Smadja_1993 agent_used_Smadja vmod_5_used prep_of_length_5 nn_length_span nn_length_maximum det_length_the prep_as_same_length det_same_the cop_same_is nsubj_same_this nn_2Note_chunk amod_2Note_full prep_to_look_2Note advmod_look_next nsubj_look_we dobj_identifying_particles nn_tagger_Brill det_tagger_the prep_of_shortcomings_tagger det_shortcomings_the conj_and_overcome_attested ccomp_overcome_same ccomp_overcome_look prepc_in_overcome_identifying dobj_overcome_shortcomings aux_overcome_To nsubj_overcome_Extraction amod_Extraction_Chunk-based amod_Extraction_Simple dep_Method-2_attested dep_Method-2_overcome num_Method-2_4 dep_``_Method-2
W03-1705	J93-1007	o	There have been many statistical measures which estimate co-occurrence and the degree of association in previous researches such as mutual information -LRB- Church 1990 Sporat 1990 -RRB- t-score -LRB- Church 1991 -RRB- dice matrix -LRB- Smadja 1993 1996 -RRB-	amod_Smadja_1996 num_Smadja_1993 dep_matrix_Smadja nn_matrix_dice num_Church_1991 appos_t-score_Church num_Sporat_1990 appos_Church_Sporat num_Church_1990 conj_information_matrix conj_information_t-score dep_information_Church amod_information_mutual prep_such_as_researches_information prep_of_degree_association det_degree_the conj_and_co-occurrence_degree dep_estimate_researches prep_in_estimate_previous dobj_estimate_degree dobj_estimate_co-occurrence nsubj_estimate_which rcmod_measures_estimate amod_measures_statistical amod_measures_many cop_measures_been aux_measures_have expl_measures_There
W03-1717	J93-1007	o	The approach is in the spirit of Smadja -LRB- 1993 -RRB- on retrieving collocations from text corpora but is more integrated with parsing	prep_with_integrated_parsing advmod_integrated_more auxpass_integrated_is nsubjpass_integrated_approach nn_corpora_text prep_from_collocations_corpora amod_collocations_retrieving appos_Smadja_1993 prep_on_spirit_collocations prep_of_spirit_Smadja det_spirit_the conj_but_is_integrated prep_in_is_spirit nsubj_is_approach det_approach_The ccomp_``_integrated ccomp_``_is
W03-1805	J93-1007	o	3 Related work Word collocation Various collocation metrics have been proposed including mean and variance -LRB- Smadja 1994 -RRB- the t-test -LRB- Church et al. 1991 -RRB- the chi-square test pointwise mutual information -LRB- MI -RRB- -LRB- Church and Hanks 1990 -RRB- and binomial loglikelihood ratio test -LRB- BLRT -RRB- -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_test_Dunning appos_test_BLRT nn_test_ratio nn_test_loglikelihood amod_test_binomial dep_Church_1990 conj_and_Church_Hanks appos_information_Hanks appos_information_Church appos_information_MI amod_information_mutual amod_information_pointwise amod_test_chi-square det_test_the amod_Church_1991 dep_Church_al. nn_Church_et conj_and_t-test_test conj_and_t-test_information appos_t-test_test dep_t-test_Church det_t-test_the dep_,_test dep_,_information dep_,_t-test amod_Smadja_1994 dep_mean_Smadja conj_and_mean_variance prep_including_proposed_variance prep_including_proposed_mean auxpass_proposed_been aux_proposed_have nsubjpass_proposed_metrics nn_metrics_collocation amod_metrics_Various nn_metrics_collocation nn_metrics_Word nn_metrics_work amod_metrics_Related num_metrics_3 ccomp_``_proposed
W03-1806	J93-1007	o	For that purpose syntactical -LRB- Didier Bourigault 1993 -RRB- statistical -LRB- Frank Smadja 1993 Ted Dunning 1993 Gal Dias 2002 -RRB- and hybrid syntaxicostatistical methodologies -LRB- Batrice Daille 1996 JeanPhilippe Goldman et al. 2001 -RRB- have been proposed	auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methodologies nsubjpass_proposed_statistical advmod_proposed_syntactical prep_for_proposed_purpose dep_al._2001 nn_al._et nn_al._Goldman nn_al._JeanPhilippe dep_Daille_al. dep_Daille_1996 nn_Daille_Batrice amod_methodologies_syntaxicostatistical nn_methodologies_hybrid dep_Dias_2002 nn_Dias_Gal amod_Dunning_1993 nn_Dunning_Ted dep_Smadja_Dias conj_Smadja_Dunning dep_Smadja_1993 nn_Smadja_Frank dep_statistical_Daille conj_and_statistical_methodologies dep_statistical_Smadja dep_Bourigault_1993 nn_Bourigault_Didier dep_syntactical_Bourigault det_purpose_that
W03-1806	J93-1007	o	alpha 0 0.1 0.2 0.3 0.4 0.5 Freq = 2 13555 13093 12235 11061 10803 10458 Freq = 3 4203 3953 3616 3118 2753 2384 Freq = 4 1952 1839 1649 1350 1166 960 Freq = 5 1091 1019 917 743 608 511 Freq > 2 2869 2699 2488 2070 1666 1307 TOTAL 23670 22603 20905 18342 16996 15620 alpha 0.6 0.7 0.8 0.9 1.0 Freq = 2 10011 9631 9596 9554 9031 Freq = 3 2088 1858 1730 1685 1678 Freq = 4 766 617 524 485 468 Freq = 5 392 276 232 202 189 Freq > 2 1000 796 627 517 439 TOTAL 14257 13178 12709 12443 11805 Table 7 Number of extracted MWUs by frequency 6.2 Qualitative Analysis As many authors assess -LRB- Frank Smadja 1993 John Justeson and Slava Katz 1995 -RRB- deciding whether a sequence of words is a multiword unit or not is a tricky problem	amod_problem_tricky det_problem_a cop_problem_is conj_or_unit_problem conj_or_unit_not amod_unit_multiword det_unit_a cop_unit_is nsubj_unit_sequence mark_unit_whether prep_of_sequence_words det_sequence_a ccomp_deciding_problem ccomp_deciding_not ccomp_deciding_unit dep_Katz_1995 nn_Katz_Slava conj_and_Justeson_Katz nn_Justeson_John vmod_Smadja_deciding dep_Smadja_Katz dep_Smadja_Justeson amod_Smadja_1993 nn_Smadja_Frank nsubj_assess_authors mark_assess_As dep_assess_Number dep_assess_12709 dep_assess_13178 dep_assess_TOTAL nsubj_assess_439 dep_assess_1000 796 627 517 dep_assess_Freq dep_assess_= dep_assess_Freq dep_assess_468 dep_assess_= dep_assess_Freq dep_assess_1678 dep_assess_= dep_assess_Freq dep_assess_9631 9596 9554 9031 dep_assess_= dep_assess_Freq dep_assess_1.0 dep_assess_0.7 dep_assess_alpha dep_assess_15620 dep_assess_16996 dep_assess_18342 dep_assess_22603 nsubj_assess_TOTAL dep_assess_1307 dep_assess_1666 nsubj_assess_2869 2699 2488 2070 dep_assess_608 dep_assess_= dep_assess_Freq dep_assess_960 dep_assess_1166 amod_authors_many amod_Analysis_Qualitative num_Analysis_6.2 nn_Analysis_frequency amod_MWUs_extracted prep_by_Number_Analysis prep_of_Number_MWUs num_Table_7 num_Table_11805 num_Table_12443 dep_12709_Table number_13178_14257 number_1000 796 627 517_2 quantmod_1000 796 627 517_> num_Freq_189 num_Freq_392 276 232 202 number_392 276 232 202_5 dep_468_766 617 524 485 number_766 617 524 485_4 dep_1678_2088 1858 1730 1685 number_2088 1858 1730 1685_3 dep_9631 9596 9554 9031_10011 number_10011_2 dep_1.0_0.9 number_0.9_0.8 number_0.7_0.6 number_18342_20905 number_22603_23670 number_2869 2699 2488 2070_2 quantmod_2869 2699 2488 2070_> dep_2869 2699 2488 2070_Freq num_Freq_511 dep_608_1091 1019 917 743 number_1091 1019 917 743_5 dep_1166_1952 1839 1649 1350 number_1952 1839 1649 1350_4 dep_=_assess dep_Freq_Smadja amod_Freq_= dep_2384_Freq number_2384_2753 dep_2384_4203 3953 3616 3118 number_4203 3953 3616 3118_3 dep_=_2384 amod_Freq_= dep_10458_Freq dep_10458_10803 number_10803_11061 dep_10803_12235 number_12235_13093 dep_13555_10458 number_13555_2 dep_=_13555 amod_Freq_= dep_0.5_Freq dep_0.5_0.4 dep_0.4_0.3 dep_0.3_0.2 number_0.2_0.1 dep_0_0.5 dep_alpha_0
W03-1806	J93-1007	o	On the other hand purely statistical systems -LRB- Frank Smadja 1993 Ted Dunning 1993 Gal Dias 2002 -RRB- extract discriminating MWUs from text corpora by means of association measure regularities	nn_regularities_measure nn_regularities_association nn_corpora_text prep_from_MWUs_corpora amod_MWUs_discriminating nn_MWUs_extract dep_Dias_2002 nn_Dias_Gal amod_Dunning_1993 nn_Dunning_Ted dep_Smadja_Dias conj_Smadja_Dunning amod_Smadja_1993 nn_Smadja_Frank prep_by_means_of_systems_regularities dep_systems_MWUs dep_systems_Smadja amod_systems_statistical advmod_systems_purely amod_hand_other det_hand_the dep_``_systems prep_on_``_hand
W03-1807	J93-1007	o	For example Smadja -LRB- 1993 -RRB- suggests a basic characteristic of collocations and multiword units is recurrent domain-dependent and cohesive lexical clusters	amod_clusters_lexical amod_clusters_cohesive nsubj_domain-dependent_characteristic conj_and_recurrent_clusters conj_and_recurrent_domain-dependent cop_recurrent_is nsubj_recurrent_characteristic dep_collocations_units conj_and_collocations_multiword prep_of_characteristic_multiword prep_of_characteristic_collocations amod_characteristic_basic det_characteristic_a ccomp_suggests_clusters ccomp_suggests_domain-dependent ccomp_suggests_recurrent nsubj_suggests_Smadja prep_for_suggests_example appos_Smadja_1993
W03-1807	J93-1007	o	Related Works Generally speaking approaches to MWE extraction proposed so far can be divided into three categories a -RRB- statistical approaches based on frequency and co-occurrence affinity b -RRB- knowledgebased or symbolic approaches using parsers lexicons and language filters and c -RRB- hybrid approaches combining different methods -LRB- Smadja 1993 Dagan and Church 1994 Daille 1995 McEnery et al. 1997 Wu 1997 Wermter et al. 1997 Michiels and Dufour 1998 Merkel and Andersson 2000 Piao and McEnery 2001 Sag et al. 2001a 2001b Biber et al. 2003 -RRB-	dep_al._2003 nn_al._et nn_al._Biber advmod_2001a_al. nn_2001a_Sag nn_al._et num_McEnery_2001 conj_and_Piao_McEnery num_Merkel_2000 conj_and_Merkel_Andersson num_Dufour_1998 conj_and_Michiels_Dufour dep_al._1997 nn_al._et nn_al._Wermter num_Wu_1997 dep_al._1997 nn_al._et nn_al._McEnery num_Daille_1995 num_Church_1994 conj_and_Dagan_Church dep_Smadja_al. appos_Smadja_2001b dep_Smadja_2001a dep_Smadja_McEnery dep_Smadja_Piao dep_Smadja_Andersson dep_Smadja_Merkel dep_Smadja_Dufour dep_Smadja_Michiels dep_Smadja_al. dep_Smadja_Wu dep_Smadja_al. dep_Smadja_Daille dep_Smadja_Church dep_Smadja_Dagan num_Smadja_1993 dep_methods_Smadja amod_methods_different dobj_combining_methods xcomp_approaches_combining nsubj_approaches_hybrid nsubj_approaches_approaches nn_hybrid_c nn_filters_language conj_and_parsers_filters conj_and_parsers_lexicons dobj_using_filters dobj_using_lexicons dobj_using_parsers vmod_approaches_using amod_approaches_symbolic amod_approaches_knowledgebased nn_approaches_affinity conj_or_knowledgebased_symbolic appos_affinity_b nn_affinity_co-occurrence nn_affinity_frequency conj_and_frequency_co-occurrence prep_on_based_approaches conj_and_approaches_hybrid vmod_approaches_based amod_approaches_statistical det_approaches_a dep_categories_approaches num_categories_three prep_into_divided_categories auxpass_divided_be aux_divided_can csubjpass_divided_speaking nsubjpass_divided_Works advmod_far_so advmod_proposed_far vmod_extraction_proposed nn_extraction_MWE prep_to_approaches_extraction conj_speaking_approaches advmod_speaking_Generally amod_Works_Related
W03-1807	J93-1007	o	In his Xtract system Smadja -LRB- 1993 -RRB- first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance strength and spread and then examined concordances of the bi-grams to find longer frequent multiword units	nn_units_multiword amod_units_frequent amod_units_longer dobj_find_units aux_find_to det_bi-grams_the prep_of_concordances_bi-grams vmod_examined_find dobj_examined_concordances advmod_examined_then conj_and_strength_spread dep_called_distance conj_and_scores_examined conj_and_scores_spread conj_and_scores_strength vmod_scores_called amod_scores_statistical dobj_using_examined dobj_using_strength dobj_using_scores nn_structure_syntactic amod_structure_single det_structure_a prep_within_co-occur_structure advmod_co-occur_consistently nsubj_co-occur_that rcmod_words_co-occur vmod_pairs_using prep_of_pairs_words amod_pairs_significant amod_pairs_extracted amod_pairs_first nn_pairs_Smadja dep_Smadja_1993 dep_,_pairs nn_system_Xtract poss_system_his pobj_In_system dep_``_In
W04-0407	J93-1007	o	The group of collocations and compounds should be delimited using statistical approaches such as Xtract -LRB- Smadja 1993 -RRB- or LocalMax -LRB- Silva et al. 1999 -RRB- so that only the most relevantthose of higher frequency are included in the database	det_database_the prep_in_included_database auxpass_included_are nsubjpass_included_relevantthose mark_included_that advmod_included_so amod_frequency_higher prep_of_relevantthose_frequency advmod_relevantthose_most det_relevantthose_the advmod_relevantthose_only amod_Silva_1999 dep_Silva_al. nn_Silva_et dep_Smadja_1993 conj_or_Xtract_LocalMax dep_Xtract_Smadja dep_approaches_Silva prep_such_as_approaches_LocalMax prep_such_as_approaches_Xtract amod_approaches_statistical dobj_using_approaches advcl_delimited_included xcomp_delimited_using auxpass_delimited_be aux_delimited_should nsubjpass_delimited_group conj_and_collocations_compounds prep_of_group_compounds prep_of_group_collocations det_group_The
W04-0412	J93-1007	p	Many efficient techniques exist to extract multiword expressions collocations lexical units and idioms -LRB- Church and Hanks 1989 Smadja 1993 Dias et al. 2000 Dias 2003 -RRB-	amod_Dias_2003 num_Dias_2000 nn_Dias_al. nn_Dias_et dep_Smadja_Dias conj_Smadja_Dias num_Smadja_1993 dep_Church_Smadja appos_Church_1989 conj_and_Church_Hanks dep_idioms_Hanks dep_idioms_Church amod_units_lexical conj_and_expressions_idioms conj_and_expressions_units conj_and_expressions_collocations amod_expressions_multiword dobj_extract_idioms dobj_extract_units dobj_extract_collocations dobj_extract_expressions aux_extract_to xcomp_exist_extract nsubj_exist_techniques amod_techniques_efficient amod_techniques_Many
W04-1113	J93-1007	p	Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction -LRB- Church and Hanks 1990 Smadja 1993 Choueka 1993 Lin 1998 -RRB-	num_Lin_1998 num_Choueka_1993 num_Smadja_1993 num_Hanks_1990 appos_Church_Lin appos_Church_Choueka appos_Church_Smadja conj_and_Church_Hanks dep_extraction_Hanks dep_extraction_Church nn_extraction_collocation prep_in_faced_extraction vmod_issues_faced det_issues_the prep_to_insights_issues det_insights_some dobj_gained_insights aux_gained_has nsubj_gained_Study amod_statistics_lexical dobj_using_statistics nn_extraction_collocation vmod_Study_using prep_in_Study_extraction
W04-1113	J93-1007	o	The precision rate using the lexical statistics approach can reach around 60 % if both word bi-gram extraction and n-gram extractions are taking into account -LRB- Smadja 1993 Lin 1997 and Lu et al. 2003 -RRB-	nn_al._et nn_al._Lu num_Lin_1997 dep_Smadja_2003 conj_and_Smadja_al. conj_and_Smadja_Lin num_Smadja_1993 prep_into_taking_account aux_taking_are nsubj_taking_extractions nsubj_taking_extraction mark_taking_if nn_extractions_n-gram conj_and_extraction_extractions nn_extraction_bi-gram nn_extraction_word preconj_extraction_both num_%_60 dep_reach_al. dep_reach_Lin dep_reach_Smadja advcl_reach_taking dobj_reach_% prt_reach_around aux_reach_can nsubj_reach_rate nn_approach_statistics amod_approach_lexical det_approach_the dobj_using_approach vmod_rate_using nn_rate_precision det_rate_The
W04-1113	J93-1007	o	Smadja -LRB- Smadja 1993 -RRB- proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength	amod_strength_higher prep_with_words_strength prep_of_pairs_words amod_pairs_cooccurring prep_of_distribution_pairs det_distribution_the prep_of_spread_distribution det_spread_the dobj_measuring_spread amod_model_statistical det_model_a prepc_by_proposed_measuring dobj_proposed_model nsubj_proposed_Smadja num_Smadja_1993 appos_Smadja_Smadja
W04-2105	J93-1007	o	There are several basic methods for evaluating associations between words based on frequency counts -LRB- Choueka 1988 Wettler and Rapp 1993 -RRB- information theoretic -LRB- Church and Hanks 1990 -RRB- and statistical significance -LRB- Smadja 1993 -RRB-	amod_Smadja_1993 amod_significance_statistical dep_Church_1990 conj_and_Church_Hanks dep_theoretic_Hanks dep_theoretic_Church dep_information_Smadja conj_and_information_significance amod_information_theoretic dep_Wettler_1993 conj_and_Wettler_Rapp dep_Choueka_Rapp dep_Choueka_Wettler appos_Choueka_1988 appos_counts_significance appos_counts_information appos_counts_Choueka nn_counts_frequency prep_on_based_counts prep_between_associations_words dobj_evaluating_associations vmod_methods_based prepc_for_methods_evaluating amod_methods_basic amod_methods_several nsubj_are_methods expl_are_There ccomp_``_are
W05-1006	J93-1007	o	In the work of Smadja -LRB- 1993 -RRB- on extracting collocations preference was given to constructions whose constituents appear in a fixed order a similar -LRB- and more generally implemented -RRB- version of our assumption here that asymmetric constructions are more idiomatic than symmetric ones	amod_ones_symmetric prep_than_idiomatic_ones advmod_idiomatic_more cop_idiomatic_are nsubj_idiomatic_constructions mark_idiomatic_that amod_constructions_asymmetric ccomp_assumption_idiomatic advmod_assumption_here poss_assumption_our prep_of_version_assumption amod_version_similar det_version_a advmod_implemented_generally cc_implemented_and advmod_generally_more dep_similar_implemented amod_order_fixed det_order_a prep_in_appear_order nsubj_appear_constituents poss_constituents_whose rcmod_constructions_appear dobj_given_version prep_to_given_constructions auxpass_given_was nsubjpass_given_preference prep_in_given_work dobj_extracting_collocations appos_Smadja_1993 prepc_on_work_extracting prep_of_work_Smadja det_work_the rcmod_``_given
W06-1006	J93-1007	o	We can mentionhere only part of this work -LRB- Berry-Rogghe 1973 Church et al. 1989 Smadja ,1993 Lin ,1998 KrennandEvert ,2001 -RRB- for monolingualextraction and -LRB- Kupiec 1993 Wu ,1994 Smadjaetal	dep_Wu_Smadjaetal num_Wu_,1994 dep_Kupiec_Wu appos_Kupiec_1993 conj_and_for_Kupiec pobj_for_monolingualextraction num_KrennandEvert_,2001 num_Lin_,1998 num_Smadja_,1993 num_Church_1989 nn_Church_al. nn_Church_et dep_Berry-Rogghe_KrennandEvert dep_Berry-Rogghe_Lin dep_Berry-Rogghe_Smadja dep_Berry-Rogghe_Church dep_Berry-Rogghe_1973 det_work_this prep_of_part_work advmod_part_only prep_mentionhere_Kupiec prep_mentionhere_for dep_mentionhere_Berry-Rogghe dobj_mentionhere_part aux_mentionhere_can nsubj_mentionhere_We ccomp_``_mentionhere
W06-1006	J93-1007	o	Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults -LRB- Breidt 1993 Smadja 1993 Zajac et al. 2003 -RRB-	num_Zajac_2003 nn_Zajac_al. nn_Zajac_et num_Smadja_1993 dep_Breidt_Zajac dep_Breidt_Smadja dep_Breidt_1993 appos_extractionresults_Breidt det_extractionresults_the dobj_significantlyimprove_extractionresults aux_significantlyimprove_to xcomp_shown_significantlyimprove auxpass_shown_been prep_in_shown_fact nsubjpass_shown_Morphosyntacticinformationhas
W06-1006	J93-1007	o	Morphologicaltoolssuch as lemmatizers andPOStaggersarebeingcommonlyusedin extractionsystems they areemployedbothfordealingwithtext variationandfor validatingthe candidatepairs combinationsof functionwordsare typicallyruledout -LRB- Justesonand Katz 1995 -RRB- as are the ungrammaticalcombinationsin the systemsthatmake useofparsers -LRB- ChurchandHanks 1990 Smadja ,1993 Basilietal	num_Smadja_,1993 dep_ChurchandHanks_Basilietal dep_ChurchandHanks_Smadja dep_ChurchandHanks_1990 dep_useofparsers_ChurchandHanks nn_useofparsers_systemsthatmake det_useofparsers_the dep_ungrammaticalcombinationsin_useofparsers det_ungrammaticalcombinationsin_the nsubj_are_ungrammaticalcombinationsin advmod_are_as amod_Katz_1995 nn_Katz_Justesonand dep_typicallyruledout_are appos_typicallyruledout_Katz nn_typicallyruledout_functionwordsare amod_typicallyruledout_combinationsof nn_candidatepairs_validatingthe nn_candidatepairs_variationandfor dep_areemployedbothfordealingwithtext_typicallyruledout dobj_areemployedbothfordealingwithtext_candidatepairs nsubj_areemployedbothfordealingwithtext_they nn_extractionsystems_andPOStaggersarebeingcommonlyusedin nn_extractionsystems_lemmatizers dep_Morphologicaltoolssuch_areemployedbothfordealingwithtext prep_as_Morphologicaltoolssuch_extractionsystems
W06-1006	J93-1007	o	Given the motivations for performing a linguistically-informedextraction whichwere also put forth among others by Church and Hanks -LRB- 1990,25 -RRB- Smadja -LRB- 1993,151 -RRB- and Heid -LRB- 1994 -RRB- and given the recent developmentof linguisticanalysistools itseemsplausiblethatthe linguisticstructurewill be more and more taken intoaccountbycollocationextractionsystems	xcomp_taken_intoaccountbycollocationextractionsystems advmod_taken_more advmod_taken_more auxpass_taken_be nsubjpass_taken_linguisticstructurewill conj_and_more_more nn_linguisticstructurewill_itseemsplausiblethatthe rcmod_linguisticanalysistools_taken amod_linguisticanalysistools_developmentof amod_linguisticanalysistools_recent det_linguisticanalysistools_the pobj_given_linguisticanalysistools nsubj_given_motivations appos_Heid_1994 appos_Smadja_1993,151 appos_Hanks_1990,25 conj_and_Church_Heid conj_and_Church_Smadja conj_and_Church_Hanks conj_and_put_given prep_by_put_Heid prep_by_put_Smadja prep_by_put_Hanks prep_by_put_Church prep_among_put_others advmod_put_forth advmod_put_also nsubj_put_motivations amod_whichwere_linguistically-informedextraction det_whichwere_a dobj_performing_whichwere prepc_for_motivations_performing det_motivations_the dep_Given_given dep_Given_put ccomp_``_Given
W06-1006	J93-1007	o	3 OverviewofExtractionWork 3.1 English As one mightexpect the bulk of the collocation extractionwork concernsthe English language -LRB- Choueka ,1988 Churchet al. ,1989 Churchand Hanks ,1990 Smadja ,1993 Justesonand Katz 1995 Kjellmer 1994 Sinclair 1995 Lin ,1998 -RRB- amongmany others1	nn_others1_amongmany num_Lin_,1998 num_Sinclair_1995 num_Kjellmer_1994 num_Katz_1995 nn_Katz_Justesonand num_Smadja_,1993 num_Hanks_,1990 nn_Hanks_Churchand num_al._,1989 nn_al._Churchet dep_Choueka_Lin dep_Choueka_Sinclair dep_Choueka_Kjellmer dep_Choueka_Katz dep_Choueka_Smadja conj_Choueka_Hanks dep_Choueka_al. num_Choueka_,1988 amod_language_English nn_language_concernsthe nn_language_extractionwork nn_language_collocation det_language_the appos_bulk_others1 dep_bulk_Choueka prep_of_bulk_language det_bulk_the num_mightexpect_one dep_English_bulk prep_as_English_mightexpect num_English_3.1 nn_English_OverviewofExtractionWork num_English_3 dep_``_English
W06-1006	J93-1007	o	Smadja -LRB- 1993 -RRB- employsthez-scoreinconjunction with several heuristics -LRB- e.g. the systematic occurrenceof two lexical items at the same distanceintext -RRB- andextractspredicativecollocations 1E g. -LRB- Frantziet al. ,2000 Pearce ,2001 Goldmanet al. 2001 ZaiuInkpenandHirst ,2002 Dias ,2003 Seretanetal	num_Dias_,2003 num_ZaiuInkpenandHirst_,2002 appos_al._2001 nn_al._Goldmanet num_Pearce_,2001 num_al._,2000 nn_al._Frantziet nn_g._1E conj_andextractspredicativecollocations_Seretanetal conj_andextractspredicativecollocations_Dias conj_andextractspredicativecollocations_ZaiuInkpenandHirst conj_andextractspredicativecollocations_al. conj_andextractspredicativecollocations_Pearce conj_andextractspredicativecollocations_al. conj_andextractspredicativecollocations_g. dep_distanceintext_andextractspredicativecollocations amod_distanceintext_same det_distanceintext_the prep_at_items_distanceintext amod_items_lexical num_items_two nn_items_occurrenceof det_items_the amod_occurrenceof_systematic pobj_e.g._items prep_heuristics_e.g. amod_heuristics_several prep_with_employsthez-scoreinconjunction_heuristics nn_employsthez-scoreinconjunction_Smadja appos_Smadja_1993
W06-1006	J93-1007	o	,2004 -RRB- appliedextractiontechniquessimilarto Xtractsystem -LRB- Smadja ,1993 -RRB- Japanese -LRB- Ikeharaetal	num_Smadja_,1993 dep_Xtractsystem_Ikeharaetal dep_Xtractsystem_Japanese dep_Xtractsystem_Smadja nn_Xtractsystem_appliedextractiontechniquessimilarto num_Xtractsystem_,2004
W06-2403	J93-1007	o	2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing -LRB- NLP -RRB- community including Smadja 1993 Dagan and Church 1994 Daille 1995 1995 McEnery et al. 1997 Wu 1997 Michiels and Dufour 1998 Maynard and Ananiadou 2000 Merkel and Andersson 2000 Piao and McEnery 2001 Sag et al. 2001 Tanaka and Baldwin 2003 Dias 2003 Baldwin et al. 2003 Nivre and Nilsson 2004 Pereira et al	dep_Pereira_al nn_Pereira_et num_Pereira_2004 appos_Nivre_Pereira conj_and_Nivre_Nilsson num_Baldwin_2003 nn_Baldwin_al. nn_Baldwin_et num_Dias_2003 num_Baldwin_2003 num_Sag_2001 nn_Sag_al. nn_Sag_et dep_Piao_Nilsson dep_Piao_Nivre conj_and_Piao_Baldwin conj_and_Piao_Dias conj_and_Piao_Baldwin conj_and_Piao_Tanaka conj_and_Piao_Sag conj_and_Piao_2001 conj_and_Piao_McEnery appos_Andersson_2000 num_Wu_1997 num_McEnery_1997 nn_McEnery_al. nn_McEnery_et appos_Daille_1995 conj_and_Smadja_Andersson conj_and_Smadja_Merkel conj_and_Smadja_2000 conj_and_Smadja_Ananiadou conj_and_Smadja_Maynard conj_and_Smadja_1998 conj_and_Smadja_Dufour conj_and_Smadja_Michiels conj_and_Smadja_Wu conj_and_Smadja_McEnery conj_and_Smadja_1995 conj_and_Smadja_Daille conj_and_Smadja_1994 conj_and_Smadja_Church conj_and_Smadja_Dagan conj_and_Smadja_1993 prep_including_community_Andersson prep_including_community_Merkel prep_including_community_2000 prep_including_community_Ananiadou prep_including_community_Maynard prep_including_community_1998 prep_including_community_Dufour prep_including_community_Michiels prep_including_community_Wu prep_including_community_McEnery prep_including_community_1995 prep_including_community_Daille prep_including_community_1994 prep_including_community_Church prep_including_community_Dagan prep_including_community_1993 prep_including_community_Smadja nn_community_Processing appos_Processing_NLP nn_Processing_Language amod_Processing_Natural det_Processing_the amod_attention_much prep_from_attracted_community dobj_attracted_attention aux_attracted_has nsubj_attracted_issue nn_processing_MWE prep_of_issue_processing det_issue_The dep_Work_Baldwin dep_Work_Dias dep_Work_Baldwin dep_Work_Tanaka dep_Work_Sag dep_Work_2001 dep_Work_McEnery dep_Work_Piao rcmod_Work_attracted amod_Work_Related num_Work_2
W06-2406	J93-1007	o	We argue that linguistic knowledge could not only improve results -LRB- Krenn 2000b Smadja 1993 -RRB- but is essential when extracting collocations from certain languages this knowledge provides other applications -LRB- or a lexicon user respectively -RRB- with a ne-grained description of how the extracted collocations are to be used in context	prep_in_used_context auxpass_used_be aux_used_to xcomp_are_used nsubj_are_collocations advmod_are_how amod_collocations_extracted det_collocations_the prepc_of_description_are amod_description_ne-grained det_description_a advmod_user_respectively nn_user_lexicon det_user_a cc_user_or prep_with_applications_description dep_applications_user amod_applications_other dobj_provides_applications nsubj_provides_knowledge det_knowledge_this amod_languages_certain prep_from_extracting_languages dobj_extracting_collocations advmod_extracting_when advcl_essential_extracting cop_essential_is nsubj_essential_knowledge dep_Smadja_1993 dep_Krenn_Smadja appos_Krenn_2000b appos_results_Krenn conj_but_improve_essential dobj_improve_results advmod_improve_only neg_improve_not aux_improve_could nsubj_improve_knowledge mark_improve_that amod_knowledge_linguistic parataxis_argue_provides ccomp_argue_essential ccomp_argue_improve nsubj_argue_We
W06-2406	J93-1007	o	-LRB- Krenn 2000b Smadja 1993 -RRB- -RRB-	dep_Smadja_1993 dep_Krenn_Smadja appos_Krenn_2000b dep_''_Krenn
W07-1511	J93-1007	o	There are some existing corpus linguistic researches on automatic extraction of collocations from electronic text -LRB- Smadja 1993 Lin 1998 Xu and Lu 2006 -RRB-	num_Lu_2006 conj_and_Xu_Lu num_Lin_1998 dep_Smadja_Lu dep_Smadja_Xu dep_Smadja_Lin num_Smadja_1993 appos_text_Smadja amod_text_electronic prep_from_extraction_text prep_of_extraction_collocations amod_extraction_automatic prep_on_researches_extraction nsubj_researches_corpus amod_corpus_linguistic amod_corpus_existing det_corpus_some ccomp_are_researches expl_are_There
W07-1511	J93-1007	o	Lastly collocations are domain-dependent -LRB- Smadja 1993 -RRB- and language-dependent	nsubj_language-dependent_collocations dep_Smadja_1993 conj_and_domain-dependent_language-dependent dep_domain-dependent_Smadja cop_domain-dependent_are nsubj_domain-dependent_collocations advmod_domain-dependent_Lastly
W94-0311	J93-1007	o	The problem is that with such a definition of collocations even when improved one identifies not only collocations but freecombining pairs frequently appearing together such as lawyer-client doctor-hospital as pointed out by Smadja -LRB- 1993 -RRB-	appos_Smadja_1993 prep_by_pointed_Smadja prt_pointed_out mark_pointed_as dep_doctor-hospital_pointed dep_lawyer-client_doctor-hospital prep_such_as_appearing_lawyer-client advmod_appearing_together advmod_appearing_frequently amod_pairs_freecombining vmod_collocations_appearing conj_but_collocations_pairs preconj_collocations_only neg_only_not dobj_identifies_pairs dobj_identifies_collocations nsubj_identifies_one advcl_identifies_improved prep_with_identifies_definition mark_identifies_that advmod_improved_when advmod_improved_even prep_of_definition_collocations det_definition_a predet_definition_such ccomp_is_identifies nsubj_is_problem det_problem_The ccomp_``_is
W95-0111	J93-1007	o	Other representative collocation research can be found in Church and Hanks -LRB- 1990 -RRB- and Smadja -LRB- 1993 -RRB-	appos_Smadja_1993 appos_Hanks_1990 conj_and_Church_Smadja conj_and_Church_Hanks prep_in_found_Smadja prep_in_found_Hanks prep_in_found_Church auxpass_found_be aux_found_can nsubjpass_found_research nn_research_collocation amod_research_representative amod_research_Other
W95-0111	J93-1007	o	Unlike Church and Hanks -LRB- 1990 -RRB- Smadja -LRB- 1993 -RRB- goes beyond the two-word limitation and deals with collocations of arbitrary length	amod_length_arbitrary prep_of_collocations_length conj_and_limitation_deals amod_limitation_two-word det_limitation_the prep_with_goes_collocations prep_beyond_goes_deals prep_beyond_goes_limitation nsubj_goes_Smadja prep_unlike_goes_Hanks prep_unlike_goes_Church appos_Smadja_1993 appos_Hanks_1990 conj_and_Church_Hanks
W96-0103	J93-1007	n	While several methods have been proposed to automatically extract compounds -LRB- Smadja 1993 Suet al. 1994 -RRB- we know of no successful attempt to automatically make classes of compounds	prep_of_classes_compounds dobj_make_classes advmod_make_automatically aux_make_to vmod_attempt_make amod_attempt_successful neg_attempt_no prep_of_know_attempt nsubj_know_we advcl_know_proposed dep_al._1994 nn_al._Suet dep_Smadja_al. num_Smadja_1993 appos_compounds_Smadja dobj_extract_compounds advmod_extract_automatically aux_extract_to xcomp_proposed_extract auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods mark_proposed_While amod_methods_several
W96-0304	J93-1007	n	Therefore sublanguage techniques such as Sager -LRB- 1981 -RRB- and Smadja -LRB- 1993 -RRB- do not work	neg_work_not aux_work_do nsubj_work_techniques advmod_work_Therefore appos_Smadja_1993 conj_and_Sager_Smadja appos_Sager_1981 prep_such_as_techniques_Smadja prep_such_as_techniques_Sager nn_techniques_sublanguage
W97-0205	J93-1007	o	MI is defined in general as follows y -RRB- I ix y -RRB- = log2 P -LRB- x -RRB- P -LRB- y -RRB- We can use this definition to derive an estimate of the connectedness between words in terms of collocations -LRB- Smadja 1993 -RRB- but also in terms of phrases and grammatical relations -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 amod_relations_grammatical conj_and_phrases_relations dep_terms_Hindle prep_of_terms_relations prep_of_terms_phrases pobj_in_terms advmod_in_also amod_Smadja_1993 appos_collocations_Smadja prep_of_terms_collocations det_connectedness_the prep_between_estimate_words prep_of_estimate_connectedness det_estimate_an dobj_derive_estimate aux_derive_to det_definition_this vmod_use_derive dobj_use_definition aux_use_can nsubj_use_We rcmod_P_use appos_P_y nn_P_P appos_P_x nn_P_log2 conj_but_=_in prep_in_=_terms dobj_=_P dep_y_in dep_y_= dobj_ix_y nsubj_ix_I dep_ix_y mark_follows_as parataxis_defined_ix advcl_defined_follows prep_in_defined_general auxpass_defined_is nsubjpass_defined_MI
W97-1004	J93-1007	o	While bound compositions are not predictable i.e. their reasonableness can not be derived from the syntactic and semantic properties of the words in them -LRB- Smadja 1993 -RRB-	num_Smadja_1993 prep_in_words_them det_words_the amod_properties_semantic appos_syntactic_Smadja prep_of_syntactic_words conj_and_syntactic_properties det_syntactic_the prep_from_derived_properties prep_from_derived_syntactic auxpass_derived_be neg_derived_not aux_derived_can nsubjpass_derived_reasonableness advmod_derived_i.e. advcl_derived_predictable poss_reasonableness_their neg_predictable_not cop_predictable_are nsubj_predictable_compositions mark_predictable_While amod_compositions_bound ccomp_``_derived
W97-1004	J93-1007	o	Now with the availability of large-scale corpus automatic acquisition of word compositions especially word collocations from them have been extensively studied -LRB- e.g. Choueka et al. 1988 Church and Hanks 1989 Smadja 1993 -RRB-	num_Smadja_1993 num_Hanks_1989 conj_and_Church_Smadja conj_and_Church_Hanks num_al._1988 nn_al._et nn_al._Choueka dep_e.g._Smadja dep_e.g._Hanks dep_e.g._Church conj_e.g._al. dep_studied_e.g. advmod_studied_extensively auxpass_studied_been aux_studied_have nsubjpass_studied_acquisition prep_with_studied_availability advmod_studied_Now prep_from_collocations_them nn_collocations_word advmod_word_especially nn_compositions_word appos_acquisition_collocations prep_of_acquisition_compositions amod_acquisition_automatic amod_corpus_large-scale prep_of_availability_corpus det_availability_the
W99-0610	J93-1007	o	Based on this assumption -LRB- Smadja 1993 -RRB- stored all bigrams of words along with their relative position p -LRB- -5 < p _ ~ 5 -RRB-	num_~_5 num_~__ nn_~_p dep_<_~ amod_-5_< dep_p_-5 amod_position_relative poss_position_their prep_of_bigrams_words det_bigrams_all dep_stored_p pobj_stored_position prepc_along_with_stored_with dobj_stored_bigrams nsubj_stored_Smadja vmod_stored_Based amod_Smadja_1993 det_assumption_this prep_on_Based_assumption
W99-0610	J93-1007	o	~ F ~ c ~ R ~ cR -LRB- 2 -RRB- ~ \ -RSB- ~ -RRB- continue explanations we begin by mentioning the ` Xtrgct ' tool by Smadja -LRB- Smadja 1993 -RRB-	amod_Smadja_1993 dep_Smadja_Smadja prep_by_tool_Smadja nn_tool_Xtrgct det_tool_the dobj_mentioning_tool prepc_by_begin_mentioning nsubj_begin_we parataxis_continue_begin dobj_continue_explanations nsubj_continue_~ nn_~_cR num_\_~ num_\_2 appos_cR_\ num_cR_~ nn_cR_R nn_cR_~ nn_cR_c nn_cR_~ nn_cR_F num_cR_~
A00-1004	J93-2003	o	A number of alignment techniques have been proposed varying from statistical methods -LRB- Brown et al. 1991 Gale and Church 1991 -RRB- to lexical methods -LRB- Kay and RSscheisen 1993 Chen 1993 -RRB-	amod_Chen_1993 dep_Kay_Chen num_Kay_1993 conj_and_Kay_RSscheisen amod_methods_lexical dep_Gale_1991 conj_and_Gale_Church dep_Brown_RSscheisen dep_Brown_Kay prep_to_Brown_methods dep_Brown_Church dep_Brown_Gale amod_Brown_1991 dep_Brown_al. nn_Brown_et amod_methods_statistical prep_from_varying_methods dep_proposed_Brown ccomp_proposed_varying auxpass_proposed_been aux_proposed_have nsubjpass_proposed_number nn_techniques_alignment prep_of_number_techniques det_number_A
A00-1019	J93-2003	o	2.1 The Evaluator The evaluator is a function p -LRB- t \ -LSB- t ' s -RRB- which assigns to each target-text unit t an estimate of its probability given a source text s and the tokens t ' which precede t in the current translation of s. 1 Our approach to modeling this distribution is based to a large extent on that of the IBM group -LRB- Brown et al. 1993 -RRB- but it differs in one significant aspect whereas the IBM model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model p -LRB- tlt ~ -RRB- and a translation model p -LRB- tls -RRB-	appos_p_tls dep_model_p nn_model_translation det_model_a nn_~_tlt conj_and_p_model appos_p_~ nn_p_model nn_p_language det_p_a prep_from_predictions_model prep_from_predictions_p amod_predictions_separate prep_of_combination_predictions amod_combination_linear det_combination_a dobj_use_combination nsubj_use_we advcl_use_involves nn_decomposition_channel amod_channel_noisy det_channel_a dobj_involves_decomposition nsubj_involves_model mark_involves_whereas nn_model_IBM det_model_the amod_aspect_significant num_aspect_one prep_in_differs_aspect nsubj_differs_it num_al._1993 nn_al._et amod_al._Brown nn_group_IBM det_group_the prep_of_that_group prep_on_extent_that amod_extent_large det_extent_a prep_to_based_extent auxpass_based_is nsubjpass_based_distribution det_distribution_this nn_distribution_t prep_to_approach_modeling poss_approach_Our num_approach_1 nn_approach_s. prep_of_translation_approach amod_translation_current det_translation_the dobj_precede_t nsubj_precede_which prep_in_t_translation rcmod_t_precede rcmod_tokens_based det_tokens_the appos_s_al. conj_and_s_tokens nn_s_text nn_s_source det_s_a pobj_given_tokens pobj_given_s poss_probability_its prep_estimate_given prep_of_estimate_probability det_estimate_an nn_t_unit nn_t_target-text det_t_each dobj_assigns_estimate prep_to_assigns_t nsubj_assigns_which dep_t_s dep_\_use conj_but_\_differs rcmod_\_assigns dep_\_t nn_\_t dep_p_differs dep_p_\ nn_p_function det_p_a cop_p_is nsubj_p_Evaluator det_evaluator_The dep_Evaluator_evaluator det_Evaluator_The rcmod_2.1_p ccomp_``_2.1
A00-1019	J93-2003	o	Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work -LRB- Brown et al. 1993 Berger et al. 1996 Och and Weber 98 Wang and Waibel 98 Wu and Wong 98 -RRB-	amod_Wu_98 conj_and_Wu_Wong dep_Wang_Wong dep_Wang_Wu conj_and_Wang_98 conj_and_Wang_Waibel conj_and_Och_98 conj_and_Och_Waibel conj_and_Och_Wang conj_and_Och_98 conj_and_Och_Weber dep_Berger_Wang dep_Berger_98 dep_Berger_Weber dep_Berger_Och num_Berger_1996 nn_Berger_al. nn_Berger_et dep_al._Berger dep_al._1993 nn_al._et amod_al._Brown amod_work_recent dep_proposed_al. prep_in_proposed_work auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Techniques conj_and_1_2 dep_models_2 dep_models_1 nn_models_IBM det_models_the agent_made_models vmod_assumptions_made nn_assumptions_independence det_assumptions_the dobj_weakening_assumptions prepc_for_Techniques_weakening
A00-1019	J93-2003	o	Furthermore the underlying decoding strategies are too time consuming for our application We therefore use a translation model based on the simple linear interpolation given in equation 2 which combines predictions of two translation models Ms and M ~ both based on IBM-like model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 amod_model_IBM-like dep_Ms_~ conj_and_Ms_M dep_models_Brown prep_based_on_models_model preconj_models_both dep_models_M dep_models_Ms nn_models_translation num_models_two prep_of_predictions_models dobj_combines_predictions nsubj_combines_which rcmod_equation_combines num_equation_2 prep_in_given_equation vmod_interpolation_given amod_interpolation_linear amod_interpolation_simple det_interpolation_the pobj_on_interpolation pcomp_based_on nn_model_translation det_model_a prep_use_based dobj_use_model advmod_use_therefore nsubj_use_We rcmod_application_use poss_application_our prep_for_consuming_application vmod_time_consuming advmod_time_too cop_time_are nsubj_time_strategies advmod_time_Furthermore nn_strategies_decoding amod_strategies_underlying det_strategies_the
A00-1019	J93-2003	o	3.2 Mapping Mapping the identified units -LRB- tokens or sequences -RRB- to their equivalents in the other language was achieved by training a new translation model -LRB- IBM 2 -RRB- using the EM algorithm as described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_as nn_algorithm_EM det_algorithm_the advcl_using_described dobj_using_algorithm dep_using_2 dep_using_IBM nn_model_translation amod_model_new det_model_a dobj_training_model agent_achieved_training auxpass_achieved_was nsubjpass_achieved_units amod_language_other det_language_the prep_in_equivalents_language poss_equivalents_their conj_or_tokens_sequences prep_to_units_equivalents dep_units_sequences dep_units_tokens amod_units_identified det_units_the dep_Mapping_using rcmod_Mapping_achieved nn_Mapping_Mapping num_Mapping_3.2 dep_``_Mapping
A94-1006	J93-2003	o	3 Bilingual Task An Application for Word Alignment 3.1 Sentence and word alignment Bilingual alignment methods -LRB- Warwick et al. 1990 Brown et al. 1991a Brown et al. 1993 Gale and Church 1991b Gale and Church 1991a Kay and Roscheisen 1993 Simard et al. 1992 Church 1993 Kupiec 1993a Matsumoto et al. 1993 Dagan et al. 1993 -RRB-	tmod_al._1993 nn_al._et nn_al._Dagan dep_al._1993 nn_al._et nn_al._Matsumoto appos_Kupiec_1993a num_Church_1993 dep_al._1992 nn_al._et nn_al._Simard conj_and_Gale_1991b conj_and_Gale_Church num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Brown_al. conj_and_Brown_al. conj_and_Brown_Kupiec conj_and_Brown_Church conj_and_Brown_al. conj_and_Brown_1993 conj_and_Brown_Roscheisen conj_and_Brown_Kay conj_and_Brown_1991a conj_and_Brown_Church conj_and_Brown_Gale conj_and_Brown_1991b conj_and_Brown_Church conj_and_Brown_Gale conj_and_Brown_Brown conj_and_Brown_1991a dep_Brown_al. nn_Brown_et dep_Warwick_al. dep_Warwick_Kupiec dep_Warwick_Church dep_Warwick_al. dep_Warwick_1993 dep_Warwick_Roscheisen dep_Warwick_Kay dep_Warwick_1991a dep_Warwick_Church dep_Warwick_Gale dep_Warwick_Gale dep_Warwick_Brown dep_Warwick_1991a dep_Warwick_Brown appos_Warwick_1990 dep_Warwick_al. nn_Warwick_et nn_methods_alignment amod_methods_Bilingual nn_methods_alignment nn_methods_word conj_and_Sentence_methods num_Sentence_3.1 dep_Alignment_methods dep_Alignment_Sentence nn_Alignment_Word dep_Application_Warwick prep_for_Application_Alignment det_Application_An dep_Task_Application amod_Task_Bilingual num_Task_3 dep_``_Task
A94-1006	J93-2003	o	have been used in statistical machine translation -LRB- Brown et al. 1990 -RRB- terminology research and translation aids -LRB- Isabelle 1992 Ogden and Gonzales 1993 van der Eijk 1993 -RRB- bilingual lexicography -LRB- Klavans and Tzoukermann 1990 Smadja 1992 -RRB- word-sense disambiguation -LRB- Brown et al. 1991b Gale et al. 1992 -RRB- and information retrieval in a multilingual environment -LRB- Landauer and Littman 1990 -RRB-	dep_Landauer_1990 conj_and_Landauer_Littman dep_environment_Littman dep_environment_Landauer amod_environment_multilingual det_environment_a nn_retrieval_information dep_al._1992 nn_al._et nn_al._Gale dep_al._al. appos_al._1991b nn_al._et amod_al._Brown dep_disambiguation_al. amod_disambiguation_word-sense dep_Smadja_1992 dep_Klavans_Smadja conj_and_Klavans_1990 conj_and_Klavans_Tzoukermann appos_lexicography_1990 appos_lexicography_Tzoukermann appos_lexicography_Klavans amod_lexicography_bilingual dep_Eijk_1993 nn_Eijk_der nn_Eijk_van num_Ogden_1993 conj_and_Ogden_Gonzales dep_Isabelle_Eijk dep_Isabelle_Gonzales dep_Isabelle_Ogden appos_Isabelle_1992 nn_aids_translation prep_in_research_environment conj_and_research_retrieval conj_and_research_disambiguation appos_research_lexicography appos_research_Isabelle conj_and_research_aids nn_research_terminology dep_al._1990 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical dep_used_retrieval dep_used_disambiguation dep_used_aids dep_used_research dep_used_al. prep_in_used_translation auxpass_used_been aux_used_have
A94-1006	J93-2003	o	Algorithms for the more difficult task of word alignment were proposed in -LRB- Gale and Church 1991a Brown et al. 1993 Dagan et al. 1993 -RRB- and were applied for parameter estimation in the IBM statistical machine translation system -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown nn_system_translation nn_system_machine amod_system_statistical nn_system_IBM det_system_the prep_in_estimation_system nn_estimation_parameter prep_for_applied_estimation auxpass_applied_were nsubjpass_applied_Algorithms num_Dagan_1993 nn_Dagan_al. nn_Dagan_et num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Gale_Dagan conj_and_Gale_Brown conj_and_Gale_1991a conj_and_Gale_Church dep_proposed_al. conj_and_proposed_applied prep_in_proposed_Brown prep_in_proposed_1991a prep_in_proposed_Church prep_in_proposed_Gale auxpass_proposed_were nsubjpass_proposed_Algorithms nn_alignment_word prep_of_task_alignment amod_task_difficult det_task_the advmod_difficult_more prep_for_Algorithms_task
A94-1006	J93-2003	o	We have been using the output of word_align a robust alignment program that proved useful for bilingual concordancing of noisy texts -LRB- Dagan et al. 1993 -RRB-	amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et amod_texts_noisy prep_of_concordancing_texts amod_concordancing_bilingual prep_for_useful_concordancing acomp_proved_useful nsubj_proved_that rcmod_program_proved nn_program_alignment amod_program_robust det_program_a appos_word_align_program prep_of_output_word_align det_output_the dep_using_Dagan dobj_using_output aux_using_been aux_using_have nsubj_using_We ccomp_``_using
A94-1006	J93-2003	o	Part-ofspeech taggers are used in a few applications such as speech synthesis -LRB- Sproat et al. 1992 -RRB- and question answering -LRB- Kupiec 1993b -RRB-	appos_Kupiec_1993b dep_answering_Kupiec nn_answering_question amod_Sproat_1992 dep_Sproat_al. nn_Sproat_et conj_and_synthesis_answering dep_synthesis_Sproat nn_synthesis_speech prep_such_as_applications_answering prep_such_as_applications_synthesis amod_applications_few det_applications_a prep_in_used_applications auxpass_used_are nsubjpass_used_taggers amod_taggers_Part-ofspeech
A94-1006	J93-2003	o	Word alignment is newer found only in a few places -LRB- Gale and Church 1991a Brown et al. 1993 Dagan et al. 1993 -RRB-	num_Dagan_1993 nn_Dagan_al. nn_Dagan_et num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Gale_Dagan conj_and_Gale_Brown conj_and_Gale_1991a conj_and_Gale_Church dep_places_Brown dep_places_1991a dep_places_Church dep_places_Gale amod_places_few det_places_a prep_in_found_places advmod_found_only ccomp_found_newer cop_newer_is nsubj_newer_alignment nn_alignment_Word
A94-1012	J93-2003	o	Unlike probabilistic parsing proposed by -LRB- Fujisaki et al. 1989 Briscoe and Carroll 1993 -RRB- * also a staff member of Matsushita Electric Industrial Co. Ltd. Shinagawa Tokyo JAPAN	appos_Ltd._JAPAN conj_Ltd._Tokyo conj_Ltd._Shinagawa appos_Co._Ltd. nn_Co._Industrial nn_Co._Electric nn_Co._Matsushita prep_of_member_Co. nn_member_staff det_member_a dep_also_member dep_also_* advmod_Briscoe_also appos_Briscoe_1993 conj_and_Briscoe_Carroll dep_Fujisaki_Carroll dep_Fujisaki_Briscoe appos_Fujisaki_1989 dep_Fujisaki_al. nn_Fujisaki_et prep_by_proposed_Fujisaki ccomp_,_proposed amod_parsing_probabilistic pobj_Unlike_parsing dep_``_Unlike
A94-1012	J93-2003	n	It also differs from previous proposals on lexical acquisition using statistical measures such as -LRB- Church et al. 1991 Brent 1991 Brown et al. 1993 -RRB- which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways	nn_ways_hoc nn_ways_ad amod_knowledge_linguistic nn_knowledge_use conj_or_knowledge_knowledge amod_knowledge_linguistic prep_of_existence_knowledge prep_of_existence_knowledge amod_existence_prior det_existence_the prep_in_deny_ways dobj_deny_existence preconj_deny_either nsubj_deny_which dep_al._deny appos_al._1993 nn_al._et nn_al._Brown num_Brent_1991 conj_Church_al. conj_Church_Brent appos_Church_1991 dep_Church_al. nn_Church_et prep_such_as_measures_Church amod_measures_statistical dobj_using_measures amod_acquisition_lexical prep_on_proposals_acquisition amod_proposals_previous xcomp_differs_using prep_from_differs_proposals advmod_differs_also nsubj_differs_It
A94-1012	J93-2003	o	The computation mechanism of GP and LP bears a resemblance to the EM algorithm -LRB- Dempster et al. 1977 Brown et al. 1993 -RRB- which iteratively computes maximum likelihood estimates from incomplete data	amod_data_incomplete prep_from_estimates_data nn_estimates_likelihood amod_estimates_maximum dobj_computes_estimates advmod_computes_iteratively nsubj_computes_which num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Dempster_Brown num_Dempster_1977 dep_Dempster_al. nn_Dempster_et nn_algorithm_EM det_algorithm_the prep_to_resemblance_algorithm det_resemblance_a dep_bears_computes dep_bears_Dempster dobj_bears_resemblance nsubj_bears_mechanism conj_and_GP_LP prep_of_mechanism_LP prep_of_mechanism_GP nn_mechanism_computation det_mechanism_The
A97-1050	J93-2003	o	On the other end of the spectrum character-based bitext mapping algorithms -LRB- Church 1993 Davis et al. 1995 -RRB- are limited to language pairs where cognates are common in addition they may easily be misled by superficial differences in formatting and page layout and must sacrifice precision to be computationally tractable	advmod_tractable_computationally cop_tractable_be aux_tractable_to vmod_sacrifice_tractable dobj_sacrifice_precision aux_sacrifice_must nsubj_sacrifice_they nn_layout_page nn_layout_formatting conj_and_formatting_page prep_in_differences_layout amod_differences_superficial conj_and_misled_sacrifice agent_misled_differences auxpass_misled_be advmod_misled_easily aux_misled_may nsubjpass_misled_they prep_in_misled_addition cop_common_are nsubj_common_cognates advmod_common_where rcmod_pairs_common nn_pairs_language parataxis_limited_sacrifice parataxis_limited_misled prep_to_limited_pairs auxpass_limited_are nsubjpass_limited_algorithms prep_on_limited_end num_Davis_1995 nn_Davis_al. nn_Davis_et dep_Church_Davis dep_Church_1993 appos_algorithms_Church nn_algorithms_mapping nn_algorithms_bitext amod_algorithms_character-based det_spectrum_the prep_of_end_spectrum amod_end_other det_end_the
C00-1064	J93-2003	p	In statistical machine translation IBM 1 ~ 5 models -LRB- Brown et al. 1993 -RRB- based on the source-chmmel model have been widely used and revised for many language donmins and applications	conj_and_donmins_applications nn_donmins_language amod_donmins_many prep_for_revised_applications prep_for_revised_donmins nsubjpass_revised_IBM conj_and_used_revised advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_IBM prep_in_used_translation amod_model_source-chmmel det_model_the prep_on_based_model amod_Brown_1993 dep_Brown_al. nn_Brown_et vmod_models_based dep_models_Brown num_models_5 num_models_~ number_~_1 dep_IBM_models nn_translation_machine amod_translation_statistical
C00-1064	J93-2003	o	Thus a lot of alignment techniques have been suggested at the sentence -LRB- Gale et al. 1993 -RRB- phrase -LRB- Shin et al. 1996 -RRB- nomt t -RRB- hrase -LRB- Kupiec 1993 -RRB- word -LRB- Brown et al. 1993 Berger et al. 1996 Melamed 1997 -RRB- collocation -LRB- Smadja et al. 1996 -RRB- and terminology level	nn_level_terminology amod_Smadja_1996 dep_Smadja_al. nn_Smadja_et dep_collocation_Smadja dep_Melamed_1997 conj_and_Berger_level conj_and_Berger_collocation dep_Berger_Melamed num_Berger_1996 nn_Berger_al. nn_Berger_et num_al._1993 nn_al._et dep_Brown_al. amod_word_Brown amod_Kupiec_1993 appos_hrase_word dep_hrase_Kupiec nn_t_nomt num_Shin_1996 dep_Shin_al. nn_Shin_et appos_phrase_Shin amod_Gale_1993 dep_Gale_al. nn_Gale_et dep_sentence_level dep_sentence_collocation dep_sentence_Berger dep_sentence_hrase appos_sentence_t appos_sentence_phrase dep_sentence_Gale det_sentence_the dep_suggested_sentence advmod_suggested_at auxpass_suggested_been aux_suggested_have nsubjpass_suggested_lot advmod_suggested_Thus nn_techniques_alignment prep_of_lot_techniques det_lot_a
C00-1078	J93-2003	o	In our Machine % ` anslation system transfer rules are generated automatically from parsed parallel text along the lines of -LRB- Matsulnoto el al 1993 Meyers et al. 1996 Meyers et al. 1998b -RRB-	appos_Meyers_1998b nn_Meyers_al. nn_Meyers_et num_Meyers_1996 nn_Meyers_al. nn_Meyers_et num_al_1993 dep_el_Meyers conj_el_Meyers conj_el_al amod_el_Matsulnoto dep_of_el prep_lines_of det_lines_the nn_text_parallel amod_text_parsed prep_along_generated_lines prep_from_generated_text advmod_generated_automatically auxpass_generated_are nsubjpass_generated_rules prep_in_generated_system nn_rules_transfer nn_system_anslation nn_system_% nn_system_Machine poss_system_our
C00-1078	J93-2003	o	These transtbr rules are pairs of corresponding rooted substructures where a substructure -LRB- Matsumoto et al. 1993 -RRB- is a connected set of arcs and nodes	conj_and_arcs_nodes prep_of_set_nodes prep_of_set_arcs amod_set_connected det_set_a cop_set_is nsubj_set_substructure advmod_set_where amod_Matsumoto_1993 dep_Matsumoto_al. nn_Matsumoto_et dep_substructure_Matsumoto det_substructure_a rcmod_substructures_set amod_substructures_rooted amod_substructures_corresponding prep_of_pairs_substructures cop_pairs_are nsubj_pairs_rules amod_rules_transtbr det_rules_These
C00-2123	J93-2003	o	The model is often further restricted so that each source word is assigned to exactly one target word -LRB- Brown et al. 1993 Ney et al. 2000 -RRB-	num_Ney_2000 nn_Ney_al. nn_Ney_et num_al._1993 nn_al._et dep_Brown_al. amod_word_Brown nn_word_target num_word_one quantmod_one_exactly prep_to_assigned_word auxpass_assigned_is nsubjpass_assigned_word mark_assigned_that mark_assigned_so nn_word_source det_word_each parataxis_restricted_Ney advcl_restricted_assigned amod_restricted_further advmod_restricted_often cop_restricted_is nsubj_restricted_model det_model_The
C00-2162	J93-2003	o	Many existing systems tbr SMT -LRB- Wang and Waibel 1997 Niefien et al. 1 -LRB- / 98 Och and Weber 1998 -RRB- make use of a special way of structuring the string translation model -LRB- Brown et al. 1993 -RRB- ` l?he correspondence between the words in the source and the target string is described by aligmuents that assign one target word position to each source word position	nn_position_word nn_position_source det_position_each nn_position_word nn_position_target num_position_one prep_to_assign_position dobj_assign_position nsubj_assign_that rcmod_aligmuents_assign agent_described_aligmuents auxpass_described_is nsubjpass_described_correspondence nn_string_target det_string_the det_source_the conj_and_words_string prep_in_words_source det_words_the prep_between_correspondence_string prep_between_correspondence_words amod_correspondence_l?he num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation nn_model_string det_model_the dobj_structuring_model prepc_of_way_structuring amod_way_special det_way_a prep_of_use_way ccomp_make_described nsubj_make_use nsubj_make_Waibel nsubj_make_Wang dep_make_SMT dep_Och_1998 conj_and_Och_Weber dep_98_Weber dep_98_Och num_Niefien_1 nn_Niefien_al. nn_Niefien_et dep_Wang_98 dep_Wang_Niefien amod_Wang_1997 conj_and_Wang_Waibel nn_SMT_tbr nn_SMT_systems amod_SMT_existing amod_SMT_Many
C00-2163	J93-2003	o	In this paper we will describe extensions to tile Hidden-Markov alignment model froln -LRB- Vogel et al. 1.996 -RRB- and compare tlmse to Models 1 4 of -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_of_al. number_4_1 amod_Models_of num_Models_4 prep_to_compare_Models dobj_compare_tlmse nsubj_compare_we amod_Vogel_1.996 dep_Vogel_al. nn_Vogel_et nn_froln_model nn_froln_alignment amod_froln_Hidden-Markov nn_froln_tile prep_to_extensions_froln conj_and_describe_compare dep_describe_Vogel dobj_describe_extensions aux_describe_will nsubj_describe_we prep_in_describe_paper det_paper_this
C00-2163	J93-2003	o	3 Model 1 and Model 2 l ~ cl -RRB- lacing the -LRB- l -LRB- ~ t -RRB- endence on aj-l in the HMM alignment mo -LRB- M I -RRB- y a del -RRB- endence on j we olltain a model wlfich -LRB- an lie seen as a zero-order Hid -LRB- l -LRB- mMarkov Model which is similar to Model 2 1 -RRB- rot -RRB- ose -LRB- t t/y -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_t/y_al. nn_t/y_t nn_t/y_ose dep_rot_t/y dep_rot_1 num_rot_2 dep_Model_rot prep_to_similar_Model cop_similar_is nsubj_similar_which rcmod_Model_similar nn_Model_mMarkov nn_Model_l dep_Hid_Model nn_Hid_zero-order det_Hid_a prep_as_seen_Hid vmod_lie_seen det_lie_an dep_wlfich_lie nn_wlfich_model det_wlfich_a dobj_olltain_wlfich nsubj_olltain_we prep_on_endence_j nn_endence_del det_endence_a dobj_y_endence nn_I_M appos_mo_I dep_alignment_mo nn_alignment_HMM det_alignment_the appos_~_t rcmod_l_olltain vmod_l_y prep_in_l_alignment prep_on_l_aj-l dep_l_endence dep_l_~ dep_the_l dep_lacing_the nn_cl_~ nn_cl_l num_cl_2 nn_cl_Model conj_and_Model_cl num_Model_1 vmod_3_lacing dep_3_cl dep_3_Model
C00-2163	J93-2003	o	1087 Model 3 of -LRB- Brown et al. 1993 -RRB- is a zero-order alignment model like Model 2 including in addition fertility paranmters	nn_paranmters_fertility nn_paranmters_addition pobj_in_paranmters pcomp_including_in prep_Model_including num_Model_2 prep_like_model_Model nn_model_alignment nn_model_zero-order det_model_a cop_model_is nsubj_model_3 nsubj_model_Model num_al._1993 nn_al._et amod_al._Brown dep_of_al. amod_3_of num_Model_1087 ccomp_``_model
C00-2163	J93-2003	o	Model 4 of -LRB- Brown et al. 1993 -RRB- is also a first-order alignment model -LRB- along the source positions -RRB- like the HMM trot includes also fertilities	advmod_fertilities_also dep_fertilities_includes dep_fertilities_trot det_HMM_the nn_positions_source det_positions_the dep_model_fertilities prep_like_model_HMM prep_along_model_positions nn_model_alignment amod_model_first-order det_model_a advmod_model_also cop_model_is nsubj_model_Model num_al._1993 nn_al._et amod_al._Brown prep_of_Model_al. num_Model_4
C00-2163	J93-2003	o	Tile full description of Model 4 -LRB- Brown et al. 1993 -RRB- is rather complica.ted as there have to be considered tile cases that English words have fertility larger than one and that English words have fertility zero	num_fertility_zero dobj_have_fertility nsubj_have_words mark_have_that nn_words_English prep_than_larger_one amod_fertility_larger dobj_have_fertility nsubj_have_words dobj_have_that nn_words_English rcmod_cases_have nn_cases_tile dobj_considered_cases auxpass_considered_be aux_considered_to conj_and_have_have xcomp_have_considered expl_have_there mark_have_as advcl_complica.ted_have advcl_complica.ted_have advmod_complica.ted_rather auxpass_complica.ted_is nsubjpass_complica.ted_description amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 dep_description_Brown prep_of_description_Model amod_description_full nn_description_Tile
C00-2163	J93-2003	o	Therefore the Viterbi alignment is comlmted only approximately using the method described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in vmod_method_described det_method_the dobj_using_method advmod_using_approximately advmod_approximately_only xcomp_comlmted_using auxpass_comlmted_is nsubjpass_comlmted_alignment advmod_comlmted_Therefore nn_alignment_Viterbi det_alignment_the ccomp_``_comlmted
C00-2163	J93-2003	o	As in tile HMM we easily can extend the dependencies in the alignment model of Model 4 easily using the word class of the previous English word E = G -LRB- ci -RRB- or the word class of the French word F = G -LRB- Ij -RRB- -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_G_Ij dep_=_G npadvmod_=_F npadvmod_=_word amod_=_French det_=_the prep_of_class_= nn_class_word det_class_the conj_or_G_class appos_G_ci dep_=_class dep_=_G npadvmod_=_E dep_word_Brown amod_word_= dep_English_word dep_previous_English amod_the_previous prep_of_class_the nn_class_word det_class_the dobj_using_class advmod_using_easily num_Model_4 prep_of_model_Model nn_model_alignment det_model_the det_dependencies_the xcomp_extend_using prep_in_extend_model dobj_extend_dependencies aux_extend_can advmod_extend_easily nsubj_extend_we prep_extend_As nn_HMM_tile pobj_in_HMM pcomp_As_in rcmod_``_extend
C00-2163	J93-2003	o	Most SMT models -LRB- Brown et al. 1993 Vogel et al. 1996 -RRB- try to model word-to-word corresl -RRB- ondences between source and target words using an alignment nmpl -RRB- ing from source l -RRB- osition j to target position i = aj	dep_=_aj dep_i_= dep_position_i dobj_target_position aux_target_to nn_j_osition dep_j_l nn_l_source nn_ing_nmpl dep_ing_alignment det_alignment_an xcomp_using_target prep_from_using_j dobj_using_ing nn_words_target conj_and_source_words vmod_ondences_using prep_between_ondences_words prep_between_ondences_source dep_corresl_ondences amod_corresl_word-to-word dobj_model_corresl aux_model_to xcomp_try_model nsubj_try_models num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_Brown_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_models_Brown nn_models_SMT amod_models_Most
C02-1002	J93-2003	o	The assumptions we made were the following a lexical token in one half of the translation unit -LRB- TU -RRB- corresponds to at most one non-empty lexical unit in the other half of the TU this is the 1:1 mapping assumption which underlines the work of many other researchers -LRB- Ahrenberg et al -LRB- 2000 -RRB- Brew and McKelvie -LRB- 1996 -RRB- Hiemstra -LRB- 1996 -RRB- Kay and Rscheisen -LRB- 1993 -RRB- Tiedmann -LRB- 1998 -RRB- Melamed -LRB- 2001 -RRB- etc -RRB- a polysemous lexical token if used several times in the same TU is used with the same meaning this assumption is explicitly used by Gale and Church -LRB- 1991 -RRB- Melamed -LRB- 2001 -RRB- and implicitly by all the previously mentioned authors a lexical token in one part of a TU can be aligned to a lexical token in the other part of the TU only if the two tokens have compatible types -LRB- part-of-speech -RRB- in most cases compatibility reduces to the same POS but it is also possible to define other compatibility mappings -LRB- e.g. participles or gerunds in English are quite often translated as adjectives or nouns in Romanian and vice-versa -RRB- although the word order is not an invariant of translation it is not random either -LRB- Ahrenberg et al -LRB- 2000 -RRB- -RRB- when two or more candidate translation pairs are equally scored the one containing tokens which are closer in relative position are preferred	auxpass_preferred_are nsubjpass_preferred_one advcl_preferred_scored amod_position_relative prep_in_closer_position cop_closer_are nsubj_closer_which rcmod_tokens_closer dobj_containing_tokens vmod_one_containing det_one_the advmod_scored_equally auxpass_scored_are nsubjpass_scored_pairs advmod_scored_when nn_pairs_translation nn_pairs_candidate num_pairs_more num_pairs_two conj_or_two_more appos_Ahrenberg_2000 dep_Ahrenberg_al nn_Ahrenberg_et dep_either_Ahrenberg dep_random_preferred dep_random_either neg_random_not cop_random_is nsubj_random_it amod_random_invariant det_random_an neg_random_not cop_random_is nsubj_random_order mark_random_although prep_of_invariant_translation nn_order_word det_order_the conj_and_Romanian_vice-versa prep_in_adjectives_vice-versa prep_in_adjectives_Romanian conj_or_adjectives_nouns prep_as_translated_nouns prep_as_translated_adjectives advmod_translated_often auxpass_translated_are nsubjpass_translated_gerunds nsubjpass_translated_participles dep_translated_e.g. advmod_often_quite prep_in_participles_English conj_or_participles_gerunds dep_mappings_translated nn_mappings_compatibility amod_mappings_other dobj_define_mappings aux_define_to xcomp_possible_define advmod_possible_also cop_possible_is nsubj_possible_it amod_POS_same det_POS_the conj_but_reduces_possible prep_to_reduces_POS nsubj_reduces_compatibility prep_in_reduces_cases amod_cases_most appos_types_part-of-speech amod_types_compatible advcl_have_random parataxis_have_possible parataxis_have_reduces dobj_have_types nsubj_have_tokens mark_have_if advmod_have_only num_tokens_two det_tokens_the det_TU_the prep_of_part_TU amod_part_other det_part_the advcl_token_have prep_in_token_part amod_token_lexical det_token_a prep_to_aligned_token auxpass_aligned_be aux_aligned_can nsubjpass_aligned_token det_TU_a prep_of_part_TU num_part_one prep_in_token_part amod_token_lexical det_token_a amod_authors_mentioned det_authors_the predet_authors_all advmod_mentioned_previously pobj_by_authors advmod_by_implicitly conj_and_Melamed_by appos_Melamed_2001 appos_Church_1991 conj_and_Gale_Church parataxis_used_aligned dep_used_by dep_used_Melamed agent_used_Church agent_used_Gale advmod_used_explicitly auxpass_used_is nsubjpass_used_assumption det_assumption_this amod_meaning_same det_meaning_the prep_with_used_meaning auxpass_used_is nsubjpass_used_token amod_TU_same det_TU_the amod_times_several prep_in_used_TU tmod_used_times mark_used_if advcl_token_used amod_token_lexical amod_token_polysemous det_token_a dep_2001_etc appos_Melamed_2001 appos_Tiedmann_1998 appos_Rscheisen_1993 appos_Hiemstra_1996 appos_McKelvie_1996 parataxis_Ahrenberg_used parataxis_Ahrenberg_used appos_Ahrenberg_Melamed conj_and_Ahrenberg_Tiedmann conj_and_Ahrenberg_Rscheisen conj_and_Ahrenberg_Kay conj_and_Ahrenberg_Hiemstra conj_and_Ahrenberg_McKelvie conj_and_Ahrenberg_Brew dep_Ahrenberg_2000 dep_Ahrenberg_al nn_Ahrenberg_et dep_researchers_Tiedmann dep_researchers_Rscheisen dep_researchers_Kay dep_researchers_Hiemstra dep_researchers_McKelvie dep_researchers_Brew dep_researchers_Ahrenberg amod_researchers_other amod_researchers_many prep_of_work_researchers det_work_the dobj_underlines_work nsubj_underlines_which rcmod_assumption_underlines nn_assumption_mapping num_assumption_1:1 det_assumption_the cop_assumption_is nsubj_assumption_this det_TU_the prep_of_half_TU amod_half_other det_half_the prep_in_unit_half amod_unit_lexical amod_unit_non-empty num_unit_one amod_unit_most pobj_at_unit pcomp_to_at parataxis_corresponds_assumption prep_corresponds_to nsubj_corresponds_token appos_unit_TU nn_unit_translation det_unit_the prep_of_half_unit num_half_one prep_in_token_half amod_token_lexical det_token_a dep_following_corresponds det_following_the cop_following_were nsubj_following_assumptions nsubj_made_we rcmod_assumptions_made det_assumptions_The
C02-1008	J93-2003	p	Another kind of popular approaches to dealing with query translation based on corpus-based techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other -LRB- Brown et al. 1993 Dagan et al. 1993 Smadja et al. 1996 -RRB-	appos_al._1996 nn_al._et nn_al._Smadja num_al._1993 nn_al._et nn_al._Dagan dep_al._al. conj_al._al. num_al._1993 nn_al._et amod_al._Brown dep_other_al. amod_each_other prep_to_corresponding_each aux_corresponding_are nsubj_corresponding_pairs nn_pairs_translation poss_pairs_whose rcmod_sentences_corresponding amod_sentences_aligned dobj_containing_sentences vmod_corpus_containing amod_corpus_parallel det_corpus_a dobj_uses_corpus nsubj_uses_kind amod_techniques_corpus-based nn_translation_query prep_based_on_dealing_techniques prep_with_dealing_translation prep_to_approaches_dealing amod_approaches_popular prep_of_kind_approaches det_kind_Another
C02-1011	J93-2003	o	Related Work 2.1 Translation with Non-parallel Corpora A straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora -LRB- e.g. Brown et al 1993 -RRB-	dep_al_1993 nn_al_et nn_al_Brown nn_al_e.g. dep_corpora_al amod_corpora_bilingual amod_corpora_parallel dobj_using_corpora det_task_the prepc_by_perform_using dobj_perform_task aux_perform_to xcomp_is_perform nsubj_is_Work nn_translation_phrase conj_or_word_translation prep_to_approach_translation prep_to_approach_word amod_approach_straightforward nn_approach_A nn_approach_Corpora amod_approach_Non-parallel prep_with_Translation_approach num_Translation_2.1 dep_Work_Translation amod_Work_Related
C02-1050	J93-2003	o	According to the Bayes Rule the problem is transformed into the noisy channel model paradigm where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_text_source nn_text_channel det_text_the amod_distribution_prior det_distribution_a prep_for_text_text conj_and_text_distribution nn_text_source nn_text_channel det_text_a dobj_given_distribution dobj_given_text vmod_text_given nn_text_target nn_text_channel det_text_a prep_for_distribution_text det_distribution_a appos_solution_al. prep_of_solution_distribution amod_solution_posteriori det_solution_a dobj_maximum_solution det_maximum_the cop_maximum_is nsubj_maximum_translation advmod_maximum_where det_translation_the rcmod_paradigm_maximum nn_paradigm_model nn_paradigm_channel amod_paradigm_noisy det_paradigm_the prep_into_transformed_paradigm auxpass_transformed_is nsubjpass_transformed_problem pobj_transformed_Rule prepc_according_to_transformed_to det_problem_the nn_Rule_Bayes det_Rule_the
C04-1005	J93-2003	n	For example the statistical word alignment in IBM translation models -LRB- Brown et al. 1993 -RRB- can only handle word to word and multi-word to word alignments	nn_alignments_word prep_to_word_alignments conj_and_word_multi-word prep_to_word_multi-word prep_to_word_word dobj_handle_word advmod_handle_only aux_handle_can nsubj_handle_alignment prep_for_handle_example dep_al._1993 nn_al._et dep_Brown_al. nn_models_translation nn_models_IBM appos_alignment_Brown prep_in_alignment_models nn_alignment_word amod_alignment_statistical det_alignment_the
C04-1005	J93-2003	n	2 Statistical Word Alignment Statistical translation models -LRB- Brown et al. 1993 -RRB- only allow word to word and multi-word to word alignments	nn_alignments_word prep_to_word_alignments conj_and_word_multi-word prep_to_allow_multi-word prep_to_allow_word dobj_allow_word advmod_allow_only nsubj_allow_models dep_al._1993 nn_al._et advmod_Brown_al. appos_models_Brown nn_models_translation amod_models_Statistical nn_models_Alignment nn_models_Word amod_models_Statistical num_models_2
C04-1005	J93-2003	o	1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation -LRB- SMT -RRB- -LRB- Brown et al. 1993 -RRB-	nn_al._et dep_Brown_1993 advmod_Brown_al. appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_result_translation amod_result_intermediate det_result_an dep_introduced_Brown prep_as_introduced_result advmod_introduced_first auxpass_introduced_is nsubjpass_introduced_alignment nn_alignment_word amod_alignment_Bilingual nn_alignment_Introduction num_alignment_1
C04-1005	J93-2003	o	Besides being used in SMT it is also used in translation lexicon building -LRB- Melamed 1996 -RRB- transfer rule learning -LRB- Menezes and Richardson 2001 -RRB- example-based machine translation -LRB- Somers 1999 -RRB- etc. In previous alignment methods some researches modeled the alignments as hidden parameters in a statistical translation model -LRB- Brown et al. 1993 Och and Ney 2000 -RRB- or directly modeled them given the sentence pairs -LRB- Cherry and Lin 2003 -RRB-	num_Lin_2003 conj_and_Cherry_Lin dep_pairs_Lin dep_pairs_Cherry nn_pairs_sentence det_pairs_the dobj_given_pairs vmod_modeled_given dobj_modeled_them advmod_modeled_directly num_Ney_2000 conj_and_Och_Ney dep_al._Ney dep_al._Och num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation amod_model_statistical det_model_a amod_parameters_hidden det_alignments_the conj_or_modeled_modeled prep_in_modeled_model prep_as_modeled_parameters dobj_modeled_alignments dep_researches_modeled dep_researches_modeled nsubj_researches_some nn_methods_alignment amod_methods_previous num_Somers_1999 appos_translation_etc. appos_translation_Somers nn_translation_machine amod_translation_example-based num_Richardson_2001 conj_and_Menezes_Richardson prep_in_learning_methods appos_learning_translation appos_learning_Richardson appos_learning_Menezes nn_learning_rule nn_learning_transfer num_Melamed_1996 appos_building_Melamed nn_building_lexicon nn_building_translation parataxis_used_researches dobj_used_learning prep_in_used_building advmod_used_also auxpass_used_is nsubjpass_used_it prepc_besides_used_used prep_in_used_SMT auxpass_used_being
C04-1006	J93-2003	o	Word alignment models were first introduced in statistical machine translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical dep_introduced_al. prep_in_introduced_translation advmod_introduced_first auxpass_introduced_were nsubjpass_introduced_models nn_models_alignment nn_models_Word
C04-1006	J93-2003	p	Using the IBM translation models IBM-1 to IBM-5 -LRB- Brown et al. 1993 -RRB- as well as the Hidden-Markov alignment model -LRB- Vogel et al. 1996 -RRB- we can produce alignments of good quality	amod_quality_good prep_of_alignments_quality dobj_produce_alignments aux_produce_can nsubj_produce_we vmod_produce_Using amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_model_Vogel nn_model_alignment nn_model_Hidden-Markov det_model_the dep_al._1993 nn_al._et amod_al._Brown conj_and_IBM-1_model dep_IBM-1_al. prep_to_IBM-1_IBM-5 dep_models_model dep_models_IBM-1 nn_models_translation nn_models_IBM det_models_the dobj_Using_models
C04-1006	J93-2003	p	6 Related Work The popular IBM models for statistical machine translation are described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in auxpass_described_are nsubjpass_described_models nn_translation_machine amod_translation_statistical prep_for_models_translation nn_models_IBM amod_models_popular det_models_The rcmod_Work_described amod_Work_Related num_Work_6
C04-1006	J93-2003	o	These alignment models stem from the source-channel approach to statistical machine translation -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_translation_al. nn_translation_machine amod_translation_statistical prep_to_approach_translation amod_approach_source-channel det_approach_the prep_from_stem_approach nsubj_stem_models nn_models_alignment det_models_These
C04-1006	J93-2003	p	A detailed description of the popular translation models IBM-1 to IBM-5 -LRB- Brown et al. 1993 -RRB- aswellastheHidden-Markovalignmentmodel -LRB- HMM -RRB- -LRB- Vogel et al. 1996 -RRB- can be found in -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och prep_found_in auxpass_found_be aux_found_can nsubjpass_found_aswellastheHidden-Markovalignmentmodel dep_found_description amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_aswellastheHidden-Markovalignmentmodel_Vogel appos_aswellastheHidden-Markovalignmentmodel_HMM dep_al._1993 nn_al._et amod_al._Brown prep_to_IBM-1_IBM-5 dep_models_IBM-1 nn_models_translation amod_models_popular det_models_the dep_description_al. prep_of_description_models amod_description_detailed det_description_A
C04-1015	J93-2003	o	On the other hand statistical MT employing IBM models -LRB- Brown et al. 1993 -RRB- translates an input sentence by the combination of word transfer and word re-ordering	nn_re-ordering_word conj_and_transfer_re-ordering nn_transfer_word prep_of_combination_re-ordering prep_of_combination_transfer det_combination_the nn_sentence_input det_sentence_an prep_by_translates_combination dobj_translates_sentence nsubj_translates_MT prep_on_translates_hand amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM dobj_employing_models dep_MT_Brown vmod_MT_employing amod_MT_statistical amod_hand_other det_hand_the
C04-1031	J93-2003	o	Estimated clues are derived from the parallel data using for example measures of co-occurrence -LRB- e.g. the Dice coefficient -LRB- Smadja et al. 1996 -RRB- -RRB- statistical alignment models -LRB- e.g. IBM models from statistical machine translation -LRB- Brown et al. 1993 -RRB- -RRB- or string similarity measures -LRB- e.g. the longest common sub-sequence ratio -LRB- Melamed 1995 -RRB- -RRB-	dep_Melamed_1995 dep_ratio_Melamed nn_ratio_sub-sequence amod_ratio_common amod_ratio_longest det_ratio_the advmod_ratio_e.g. dep_measures_ratio nn_measures_similarity nn_measures_string dep_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical dep_models_al. prep_from_models_translation nn_models_IBM dep_models_e.g. dep_models_models nn_models_alignment amod_models_statistical amod_Smadja_1996 dep_Smadja_al. nn_Smadja_et dep_coefficient_Smadja nn_coefficient_Dice det_coefficient_the pobj_e.g._coefficient conj_or_measures_measures conj_or_measures_models prep_measures_e.g. prep_of_measures_co-occurrence dobj_using_measures dobj_using_models dobj_using_measures prep_for_using_example amod_data_parallel det_data_the xcomp_derived_using prep_from_derived_data auxpass_derived_are nsubjpass_derived_clues amod_clues_Estimated
C04-1031	J93-2003	o	-LRB- Brown et al. 1993 Vogel et al. 1996 Garca-Varea et al. 2002 Ahrenberg et al. 1998 Tiedemann 1999 Tufis and Barbu 2002 Melamed 2000 -RRB-	amod_Melamed_2000 dep_Tufis_Melamed conj_and_Tufis_2002 conj_and_Tufis_Barbu num_Tiedemann_1999 nn_al._et nn_al._Ahrenberg nn_al._et nn_al._Garca-Varea num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_Brown_2002 dep_Brown_Barbu dep_Brown_Tufis dep_Brown_Tiedemann amod_Brown_1998 dep_Brown_al. amod_Brown_2002 dep_Brown_al. dep_Brown_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_''_Brown
C04-1032	J93-2003	o	Word alignment models were first introduced in statistical machine translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical dep_introduced_al. prep_in_introduced_translation advmod_introduced_first auxpass_introduced_were nsubjpass_introduced_models nn_models_alignment nn_models_Word
C04-1032	J93-2003	p	Using the IBM translation models IBM-1 to IBM-5 -LRB- Brown et al. 1993 -RRB- as well as the Hidden-Markov alignment model -LRB- Vogel et al. 1996 -RRB- we can produce alignments of good quality	amod_quality_good prep_of_alignments_quality dobj_produce_alignments aux_produce_can nsubj_produce_we vmod_produce_Using amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_model_Vogel nn_model_alignment nn_model_Hidden-Markov det_model_the dep_al._1993 nn_al._et amod_al._Brown conj_and_IBM-1_model dep_IBM-1_al. prep_to_IBM-1_IBM-5 dep_models_model dep_models_IBM-1 nn_models_translation nn_models_IBM det_models_the dobj_Using_models
C04-1032	J93-2003	o	6 Related Work A description of the IBM models for statistical machine translation can be found in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_found_in auxpass_found_be aux_found_can nsubjpass_found_description nn_translation_machine amod_translation_statistical nn_models_IBM det_models_the prep_for_description_translation prep_of_description_models det_description_A rcmod_Work_found amod_Work_Related num_Work_6
C04-1032	J93-2003	o	They are based on the sourcechannel approach to statistical machine translation -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_translation_al. nn_translation_machine amod_translation_statistical prep_to_approach_translation nn_approach_sourcechannel det_approach_the prep_on_based_approach auxpass_based_are nsubjpass_based_They
C04-1032	J93-2003	p	A detailed description of the popular translation/alignment models IBM-1 to IBM-5 -LRB- Brown et al. 1993 -RRB- as well as the Hidden-Markov alignment model -LRB- HMM -RRB- -LRB- Vogel et al. 1996 -RRB- can be found in -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och prep_found_in auxpass_found_be aux_found_can advmod_found_well nsubjpass_found_description amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et appos_model_HMM nn_model_alignment nn_model_Hidden-Markov det_model_the dep_well_Vogel dep_well_model mwe_well_as advmod_well_as dep_al._1993 nn_al._et amod_al._Brown prep_to_IBM-1_IBM-5 dep_models_IBM-1 nn_models_translation/alignment amod_models_popular det_models_the dep_description_al. prep_of_description_models amod_description_detailed det_description_A
C04-1045	J93-2003	p	2 Related Work The popular IBM models for statistical machine translation are described in -LRB- Brown et al. 1993 -RRB- and the HMM-based alignment model was introduced in -LRB- Vogel et al. 1996 -RRB-	amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_in_Vogel prep_introduced_in auxpass_introduced_was nsubjpass_introduced_model nn_model_alignment amod_model_HMM-based det_model_the conj_and_al._introduced num_al._1993 nn_al._et amod_al._Brown prep_in_described_introduced prep_in_described_al. auxpass_described_are nsubjpass_described_models nn_translation_machine amod_translation_statistical prep_for_models_translation nn_models_IBM amod_models_popular det_models_The rcmod_Work_described amod_Work_Related num_Work_2
C04-1045	J93-2003	o	Detailed description of those models can be found in -LRB- Brown et al. 1993 -RRB- -LRB- Vogel et al. 1996 -RRB- and -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et num_al._1993 nn_al._et amod_al._Brown dep_in_al. conj_and_found_Ney conj_and_found_Och dep_found_Vogel prep_found_in auxpass_found_be aux_found_can nsubjpass_found_description det_models_those prep_of_description_models amod_description_Detailed
C04-1045	J93-2003	o	So far most of the statistical machine translation systems are based on the single-word alignment models as described in -LRB- Brown et al. 1993 -RRB- as well as the Hidden Markov alignment model -LRB- Vogel et al. 1996 -RRB-	amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_model_Vogel nn_model_alignment nn_model_Markov nn_model_Hidden det_model_the conj_and_al._model num_al._1993 nn_al._et amod_al._Brown prep_in_described_model prep_in_described_al. mark_described_as nn_models_alignment amod_models_single-word det_models_the advcl_based_described prep_on_based_models auxpass_based_are nsubjpass_based_most advmod_based_far nn_systems_translation nn_systems_machine amod_systems_statistical det_systems_the prep_of_most_systems advmod_far_So
C04-1047	J93-2003	o	For scoring MT outputs the proposed RSCM uses a score based on a translation model called IBM4 -LRB- Brown et al. 1993 -RRB- -LRB- TM-score -RRB- and a score based on a language model for the translation target language -LRB- LM-score -RRB-	appos_language_LM-score nn_language_target nn_language_translation det_language_the prep_for_model_language nn_model_language det_model_a prep_on_based_model vmod_score_based det_score_a num_al._1993 nn_al._et amod_al._Brown appos_IBM4_TM-score dep_IBM4_al. conj_and_called_score dep_called_IBM4 vmod_model_score vmod_model_called nn_model_translation det_model_a prep_on_based_model vmod_score_based det_score_a dobj_uses_score nsubj_uses_RSCM prepc_for_uses_scoring amod_RSCM_proposed det_RSCM_the nn_outputs_MT dobj_scoring_outputs
C04-1051	J93-2003	o	Giza + + is a freely available implementation of IBM Models 1-5 -LRB- Brown et al. 1993 -RRB- and the HMM alignment -LRB- Vogel et al. 1996 -RRB- along with various improvements and modifications motivated by experimentation by Och & Ney -LRB- 2000 -RRB-	appos_Och_2000 conj_and_Och_Ney prep_by_experimentation_Ney prep_by_experimentation_Och agent_motivated_experimentation vmod_modifications_motivated conj_and_improvements_modifications amod_improvements_various dep_al._1996 nn_al._et advmod_Vogel_al. appos_alignment_Vogel nn_alignment_HMM det_alignment_the num_al._1993 dep_Brown_al. nn_Brown_et num_Models_1-5 nn_Models_IBM pobj_implementation_modifications pobj_implementation_improvements prepc_along_with_implementation_with conj_and_implementation_alignment appos_implementation_Brown prep_of_implementation_Models amod_implementation_available det_implementation_a cop_implementation_is nsubj_implementation_+ nsubj_implementation_Giza advmod_available_freely conj_+_Giza_+
C04-1059	J93-2003	o	Statistical machine translation is based on the noisy channel model where the translation hypothesis is searched over the space defined by a translation model and a target language -LRB- Brown et al 1993 -RRB-	dep_al_1993 nn_al_et advmod_Brown_al appos_language_Brown nn_language_target det_language_a conj_and_model_language nn_model_translation det_model_a agent_defined_language agent_defined_model vmod_space_defined det_space_the prep_over_searched_space auxpass_searched_is nsubjpass_searched_hypothesis advmod_searched_where nn_hypothesis_translation det_hypothesis_the rcmod_model_searched nn_model_channel amod_model_noisy det_model_the prep_on_based_model auxpass_based_is nsubjpass_based_translation nn_translation_machine amod_translation_Statistical
C04-1090	J93-2003	o	In word-based models such as IBM Model 1-5 -LRB- Brown et al 1993 -RRB- the probability P -LRB- T | S -RRB- is decomposed into statistical parameters involving words	dobj_involving_words vmod_parameters_involving amod_parameters_statistical prep_into_decomposed_parameters auxpass_decomposed_is nsubjpass_decomposed_P prep_such_as_decomposed_Model prep_in_decomposed_models num_S_| nn_S_T appos_P_S nn_P_probability det_P_the dep_al_1993 nn_al_et dep_Brown_al appos_Model_Brown num_Model_1-5 nn_Model_IBM amod_models_word-based ccomp_``_decomposed
C04-1091	J93-2003	o	1 Introduction Decoding is one of the three fundamental problems in classical SMT -LRB- translation model and language model being the other two -RRB- as proposed by IBM in the early 1990s -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_1990s_early det_1990s_the prep_in_proposed_1990s prep_by_proposed_IBM mark_proposed_as amod_two_other det_two_the cop_two_being nn_model_language dep_model_two conj_and_model_model nn_model_translation dep_SMT_model dep_SMT_model amod_SMT_classical dep_problems_Brown dep_problems_proposed prep_in_problems_SMT amod_problems_fundamental num_problems_three det_problems_the prep_of_one_problems cop_one_is nsubj_one_Decoding nn_Decoding_Introduction num_Decoding_1 ccomp_``_one
C04-1091	J93-2003	o	2 Decoding The decoding problem in SMT is one of finding the most probable translation e in the target language of a given source language sentence f in accordance with the Fundamental Equation of SMT -LRB- Brown et al. 1993 -RRB- e = argmaxe Pr -LRB- f | e -RRB- Pr -LRB- e -RRB-	appos_Pr_e dep_e_Pr dep_|_e nn_|_f dep_Pr_| amod_Pr_argmaxe xcomp_=_Pr dep_=_e dep_al._1993 nn_al._et amod_al._Brown dep_Equation_al. prep_of_Equation_SMT amod_Equation_Fundamental det_Equation_the prep_in_accordance_with_f_Equation dep_sentence_f nn_sentence_language nn_sentence_source amod_sentence_given det_sentence_a prep_of_language_sentence nn_language_target det_language_the prep_in_translation_language dep_translation_e amod_translation_probable det_translation_the advmod_probable_most dobj_finding_translation dep_one_= prepc_of_one_finding cop_one_is nsubj_one_2 prep_in_problem_SMT amod_problem_decoding det_problem_The dobj_Decoding_problem vmod_2_Decoding rcmod_``_one
C04-1091	J93-2003	o	In each iteration of local search we look in the neighborhood of the current best alignment for a better alignment -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_alignment_al. amod_alignment_better det_alignment_a amod_alignment_best amod_alignment_current det_alignment_the prep_of_neighborhood_alignment det_neighborhood_the prep_for_look_alignment prep_in_look_neighborhood nsubj_look_we prep_in_look_iteration amod_search_local prep_of_iteration_search det_iteration_each
C04-1168	J93-2003	o	According to the statistical machine translation formalism -LRB- Brown et al. 1993 -RRB- the translation process is to search for the best sentence bE such that bE = arg max E P -LRB- EjJ -RRB- = arg maxE P -LRB- JjE -RRB- P -LRB- E -RRB- where P -LRB- JjE -RRB- is a translation model characterizing the correspondence between E and J P -LRB- E -RRB- the English language model probability	nn_probability_model nn_probability_language nn_probability_English det_probability_the appos_P_probability appos_P_E conj_and_E_J prep_between_correspondence_J prep_between_correspondence_E det_correspondence_the dobj_characterizing_correspondence vmod_model_characterizing nn_model_translation det_model_a cop_model_is nsubj_model_P advmod_model_where appos_P_JjE rcmod_P_model appos_P_E nn_P_P appos_P_JjE nn_P_maxE nn_P_arg dep_=_P amod_P_= appos_P_EjJ nn_P_E nn_P_max nn_P_arg dep_=_P dep_bE_P amod_bE_= dep_that_bE dep_such_that amod_bE_such nn_bE_sentence amod_bE_best det_bE_the prep_for_search_bE aux_search_to xcomp_is_search nsubj_is_process pobj_is_formalism prepc_according_to_is_to nn_process_translation det_process_the num_al._1993 nn_al._et amod_al._Brown dep_formalism_al. nn_formalism_translation nn_formalism_machine amod_formalism_statistical det_formalism_the
C08-1014	J93-2003	o	By introducing the hidden word alignment variable a -LRB- Brown et al. 1993 -RRB- the optimal translation can be searched for based on the following criterion * 1 arg max -LRB- -LRB- -RRB- -RRB- M mm m ea eh = = efa -LRB- 1 -RRB- where is a string of phrases in the target language e f fa is the source language string of phrases he are feature functions weights -LRB- -RRB- m m are typically optimized to maximize the scoring function -LRB- Och 2003 -RRB-	amod_Och_2003 dep_function_Och amod_function_scoring det_function_the dobj_maximize_function aux_maximize_to xcomp_optimized_maximize advmod_optimized_typically auxpass_optimized_are nsubjpass_optimized_m nn_m_m rcmod_weights_optimized nn_functions_feature cop_functions_are nsubj_functions_he appos_string_weights rcmod_string_functions prep_of_string_phrases nn_string_language nn_string_source det_string_the cop_string_is nsubj_string_1 dep_is_fa nn_fa_f dep_fa_e nn_language_target det_language_the prep_in_string_language prep_of_string_phrases det_string_a cop_string_is advmod_string_where num_string_1 nn_string_efa amod_string_= amod_string_= discourse_string_eh dep_string_ea dep_string_max nn_ea_m nn_ea_mm nn_ea_M nn_max_arg conj_1_string dep_1_* prepc_by_1_introducing amod_criterion_following det_criterion_the prep_on_based_criterion prepc_for_searched_based auxpass_searched_be aux_searched_can nsubjpass_searched_translation dep_searched_a amod_translation_optimal det_translation_the amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_a_Brown rcmod_variable_searched nn_variable_alignment nn_variable_word amod_variable_hidden det_variable_the dobj_introducing_variable
C08-1128	J93-2003	o	Given a manually compiled lexicon containing words and their relative frequencies Ps -LRB- fprimej -RRB- the best segmentationfJ1 is the one that maximizes the joint probability of all words in the sentence with the assumption that words are independent of each other1 fJ1 = argmax fprimeJprime1 Pr -LRB- fprimeJprime1 | cK1 -RRB- argmax fprimeJprime1 Jprimeproductdisplay j = 1 Ps -LRB- fprimej -RRB- where the maximization is taken over Chinese word sequences whose character sequence is cK1 2.2 Translation system Once we have segmented the Chinese sentences into words we train standard alignment models in both directions with GIZA + + -LRB- Och and Ney 2002 -RRB- using models of IBM-1 -LRB- Brown et al. 1993 -RRB- HMM -LRB- Vogel et al. 1996 -RRB- and IBM-4 -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_HMM_Vogel num_al._1993 nn_al._et amod_al._Brown conj_and_IBM-1_IBM-4 conj_and_IBM-1_HMM dep_IBM-1_al. prep_of_models_IBM-4 prep_of_models_HMM prep_of_models_IBM-1 dep_using_al. dobj_using_models num_Och_2002 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ prep_with_directions_+ prep_with_directions_GIZA det_directions_both nn_models_alignment amod_models_standard dep_train_using prep_in_train_directions dobj_train_models nsubj_train_we nsubj_train_fprimeJprime1 amod_sentences_Chinese det_sentences_the amod_sentences_segmented prep_into_have_words dobj_have_sentences nsubj_have_we mark_have_Once advcl_system_have nn_system_Translation num_system_2.2 cop_cK1_is nsubj_cK1_sequence nn_sequence_character poss_sequence_whose rcmod_sequences_cK1 nn_sequences_word amod_sequences_Chinese dobj_taken_sequences prt_taken_over auxpass_taken_is nsubjpass_taken_maximization advmod_taken_where det_maximization_the appos_Ps_fprimej num_Ps_1 dep_=_Ps dep_j_system rcmod_j_taken amod_j_= nn_j_Jprimeproductdisplay nn_j_fprimeJprime1 nn_j_argmax dep_cK1_j num_cK1_| dep_fprimeJprime1_cK1 rcmod_Pr_train nn_Pr_fprimeJprime1 nn_Pr_argmax dobj_=_Pr dep_fJ1_= det_other1_each prep_of_independent_other1 cop_independent_are nsubj_independent_words mark_independent_that ccomp_assumption_independent det_assumption_the det_sentence_the prep_in_words_sentence det_words_all prep_of_probability_words amod_probability_joint det_probability_the dobj_maximizes_probability nsubj_maximizes_that det_one_the cop_one_is nsubj_one_Ps amod_segmentationfJ1_best det_segmentationfJ1_the appos_Ps_segmentationfJ1 appos_Ps_fprimej rcmod_frequencies_one amod_frequencies_relative poss_frequencies_their rcmod_words_maximizes conj_and_words_frequencies dobj_containing_words vmod_lexicon_containing amod_lexicon_compiled det_lexicon_a advmod_compiled_manually dep_Given_fJ1 prep_with_Given_assumption dobj_Given_lexicon ccomp_``_Given
C08-1136	J93-2003	o	In the context of statistical machine translation -LRB- Brown et al. 1993 -RRB- we may interpretE as an English sentence F its translation in French and A a representation of how the words correspond to each other in the two sentences	num_sentences_two det_sentences_the det_other_each prep_to_correspond_other nsubj_correspond_words advmod_correspond_how det_words_the prep_in_representation_sentences prepc_of_representation_correspond det_representation_a det_representation_A conj_and_translation_representation prep_in_translation_French poss_translation_its dep_translation_F dep_translation_interpretE amod_sentence_English det_sentence_an prep_as_interpretE_sentence aux_interpretE_may nsubj_interpretE_we dep_interpretE_al. prep_in_interpretE_context num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical prep_of_context_translation det_context_the
C08-1138	J93-2003	o	Based on these grammars a great number of SMT models have been recently proposed including string-to-string model -LRB- Synchronous FSG -RRB- -LRB- Brown et al. 1993 Koehn et al. 2003 -RRB- tree-to-string model -LRB- TSG-string -RRB- -LRB- Huang et al. 2006 Liu et al. 2006 Liu et al. 2007 -RRB- string-totree model -LRB- string-CFG/TSG -RRB- -LRB- Yamada and Knight 2001 Galley et al. 2006 Marcu et al. 2006 -RRB- tree-to-tree model -LRB- Synchronous CFG/TSG Data-Oriented Translation -RRB- -LRB- Chiang 2005 Cowan et al. 2006 Eisner 2003 Ding and Palmer 2005 Zhang et al. 2007 Bod 2007 Quirk wt al. 2005 Poutsma 2000 Hearne and Way 2003 -RRB- and so on	advmod_on_so dep_Hearne_2003 conj_and_Hearne_Way num_Poutsma_2000 amod_al._wt nn_al._Quirk num_Bod_2007 num_Zhang_2007 nn_Zhang_al. nn_Zhang_et conj_and_Ding_2005 conj_and_Ding_Palmer num_Eisner_2003 num_Cowan_2006 nn_Cowan_al. nn_Cowan_et dep_Chiang_Way dep_Chiang_Hearne conj_Chiang_Poutsma appos_Chiang_2005 dep_Chiang_al. dep_Chiang_Bod conj_Chiang_Zhang conj_Chiang_2005 conj_Chiang_Palmer conj_Chiang_Ding conj_Chiang_Eisner conj_Chiang_Cowan appos_Chiang_2005 amod_Translation_Data-Oriented appos_CFG/TSG_Translation amod_CFG/TSG_Synchronous dep_model_Chiang dep_model_CFG/TSG amod_model_tree-to-tree num_Marcu_2006 nn_Marcu_al. nn_Marcu_et conj_and_Galley_on conj_and_Galley_model dep_Galley_Marcu num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Yamada_on dep_Yamada_model dep_Yamada_Galley appos_Yamada_2001 conj_and_Yamada_Knight dep_model_Knight dep_model_Yamada appos_model_string-CFG/TSG amod_model_string-totree num_Liu_2007 nn_Liu_al. nn_Liu_et dep_Liu_Liu num_Liu_2006 nn_Liu_al. nn_Liu_et dep_Huang_Liu appos_Huang_2006 dep_Huang_al. nn_Huang_et dep_model_model appos_model_Huang appos_model_TSG-string amod_model_tree-to-string nn_al._et nn_al._Koehn amod_Brown_2003 dep_Brown_al. amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_FSG_Synchronous dep_model_Brown appos_model_FSG amod_model_string-to-string dep_proposed_model prep_including_proposed_model advmod_proposed_recently auxpass_proposed_been aux_proposed_have nsubjpass_proposed_number pobj_proposed_grammars prepc_based_on_proposed_on nn_models_SMT prep_of_number_models amod_number_great det_number_a det_grammars_these
C94-2175	J93-2003	o	For example sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters -LRB- Brown et al. 1991 Gale and Church 1993 -RRB- or by statistically estimating word level correspondences -LRB- Chen 1993 Kay and RSscheisen 1993 -RRB-	amod_Kay_1993 conj_and_Kay_RSscheisen dep_Chen_RSscheisen dep_Chen_Kay amod_Chen_1993 appos_correspondences_Chen nn_correspondences_level nn_correspondences_word dobj_estimating_correspondences advmod_estimating_statistically pcomp_by_estimating dep_Gale_1993 conj_and_Gale_Church dep_Brown_Church dep_Brown_Gale amod_Brown_1991 dep_Brown_al. nn_Brown_et conj_or_words_characters prep_in_lengths_characters prep_in_lengths_words nn_lengths_sentence dobj_measuring_lengths conj_or_performed_by dep_performed_Brown agent_performed_measuring advmod_performed_just auxpass_performed_are nsubjpass_performed_alignment prep_for_performed_example amod_texts_bilingual prep_of_alignment_texts nn_alignment_sentence
C94-2175	J93-2003	o	Then those structurally matched parallel sentences are used as a source for acquiring lexical knowledge snch as verbal case frames -LRB- Utsuro et al. 1992 Utsuro et al. 1993 -RRB-	num_Utsuro_1993 nn_Utsuro_al. nn_Utsuro_et dep_Utsuro_Utsuro appos_Utsuro_1992 dep_Utsuro_al. nn_Utsuro_et dep_frames_Utsuro nn_frames_case amod_frames_verbal nn_snch_knowledge amod_snch_lexical prep_as_acquiring_frames dobj_acquiring_snch prepc_for_source_acquiring det_source_a prep_as_used_source auxpass_used_are nsubjpass_used_sentences advmod_used_Then amod_sentences_parallel amod_sentences_matched det_sentences_those advmod_matched_structurally
C94-2175	J93-2003	o	So fitr we have implemented the following sentence ~ dignment btLsed-on word correspondence information word correspondence estimation by cooccnl ` rence-ffequency-based methods in GMe mid Church -LRB- 19 ~ H -RRB- and Kay and R6scheisen -LRB- 1993 -RRB- structured Imttehlng of parallel sentences -LRB- Matsumoto et a l. 1993 -RRB- and case Dame acquisition of Japanese verbs -LRB- Utsuro et al. 1993 -RRB-	amod_Utsuro_1993 dep_Utsuro_al. nn_Utsuro_et amod_verbs_Japanese prep_of_acquisition_verbs nn_acquisition_Dame nn_acquisition_case dep_l._1993 det_l._a dep_et_l. dep_Matsumoto_Utsuro conj_and_Matsumoto_acquisition dep_Matsumoto_et amod_sentences_parallel dep_Imttehlng_acquisition dep_Imttehlng_Matsumoto prep_of_Imttehlng_sentences amod_Imttehlng_structured appos_R6scheisen_1993 conj_and_Kay_R6scheisen nn_H_~ dep_19_H conj_and_Church_R6scheisen conj_and_Church_Kay dep_Church_19 amod_Church_mid nn_Church_GMe prep_in_methods_Kay prep_in_methods_Church amod_methods_rence-ffequency-based nn_methods_cooccnl prep_by_estimation_methods nn_estimation_correspondence nn_estimation_word appos_information_Imttehlng appos_information_estimation nn_information_correspondence nn_information_word nn_information_btLsed-on nn_information_dignment nn_information_~ nn_information_sentence dep_information_following det_information_the dobj_implemented_information aux_implemented_have nsubj_implemented_we dep_implemented_fitr advmod_fitr_So
C94-2175	J93-2003	o	Dynamic programming is applied to bilingual sentence alignment in most of previous works -LRB- Brown et al. 1991 Gate and Church 1993 Chen 1993 -RRB-	num_Chen_1993 dep_Gate_Chen conj_and_Gate_1993 conj_and_Gate_Church dep_Brown_1993 dep_Brown_Church dep_Brown_Gate amod_Brown_1991 dep_Brown_al. nn_Brown_et amod_works_previous prep_of_most_works nn_alignment_sentence amod_alignment_bilingual dep_applied_Brown prep_in_applied_most prep_to_applied_alignment auxpass_applied_is nsubjpass_applied_programming nn_programming_Dynamic
C94-2175	J93-2003	o	The statistical approach involves the following alignment of bilingual texts at the sentence level nsing statistical techniques -LRB- e.g. Brown Lai and Mercer -LRB- 1991 -RRB- Gale and Church -LRB- 1993 -RRB- Chen -LRB- 1993 -RRB- and Kay and RSscheisen -LRB- 1993 -RRB- -RRB- statistical machine translation models -LRB- e.g. Brown Cooke Pietra Pietra et al.	nn_al._et nn_al._Pietra dep_Brown_al. conj_Brown_Pietra conj_Brown_Cooke dep_Brown_e.g. dep_models_Brown nn_models_translation nn_models_machine amod_models_statistical appos_RSscheisen_1993 conj_and_Kay_RSscheisen appos_Chen_1993 appos_Church_1993 appos_Mercer_1991 conj_and_Brown_RSscheisen conj_and_Brown_Kay conj_and_Brown_Chen conj_and_Brown_Church conj_and_Brown_Gale conj_and_Brown_Mercer conj_and_Brown_Lai dep_Brown_e.g. dep_techniques_Kay dep_techniques_Chen dep_techniques_Church dep_techniques_Gale dep_techniques_Mercer dep_techniques_Lai dep_techniques_Brown amod_techniques_statistical nn_techniques_nsing nn_techniques_level nn_techniques_sentence det_techniques_the amod_texts_bilingual appos_alignment_models prep_at_alignment_techniques prep_of_alignment_texts dep_following_alignment det_following_the dobj_involves_following nsubj_involves_approach amod_approach_statistical det_approach_The
C94-2178	J93-2003	o	In previous work -LRB- Church et al 1993 -RRB- we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual -LRB- Aho Kernighan Weinberger -LRB- 1980 -RRB- -RRB- using charalign -LRB- Church 1993 -RRB- a method that looks for character sequences that are the same in both the source and target	conj_and_source_target det_source_the preconj_source_both prep_in_same_target prep_in_same_source det_same_the cop_same_are nsubj_same_that rcmod_sequences_same nn_sequences_character prep_for_looks_sequences nsubj_looks_that rcmod_method_looks det_method_a dep_Church_1993 nn_Church_charalign dobj_using_method dobj_using_Church dep_Weinberger_1980 appos_Aho_Weinberger conj_Aho_Kernighan dep_manual_Aho nn_manual_AWK det_manual_the nn_versions_Japanese prep_of_English_manual conj_and_English_versions det_English_the dobj_aligning_versions dobj_aligning_English prepc_in_success_aligning amod_success_preliminary det_success_some dep_reported_using dobj_reported_success aux_reported_have nsubj_reported_we dep_reported_Church prep_in_reported_work amod_Church_1993 dep_Church_al nn_Church_et amod_work_previous
C94-2178	J93-2003	o	This estimate could be used as a starting point for a more detailed alignment algorithm such as word_align -LRB- Dagan et al 1993 -RRB-	amod_Dagan_1993 dep_Dagan_al nn_Dagan_et dep_word_align_Dagan prep_such_as_algorithm_word_align nn_algorithm_alignment amod_algorithm_detailed det_algorithm_a advmod_detailed_more prep_for_point_algorithm amod_point_starting det_point_a prep_as_used_point auxpass_used_be aux_used_could nsubjpass_used_estimate det_estimate_This
C94-2178	J93-2003	o	These tables were computed from a small fragment of the Canadian Hansards that has been used in a number of other studies Church -LRB- 1993 -RRB- and Simard et al -LRB- 1992 -RRB-	dep_al_1992 nn_al_et nn_al_Simard conj_and_Church_al appos_Church_1993 amod_studies_other prep_of_number_studies det_number_a prep_in_used_number auxpass_used_been aux_used_has nsubjpass_used_that rcmod_Hansards_used amod_Hansards_Canadian det_Hansards_the prep_of_fragment_Hansards amod_fragment_small det_fragment_a dep_computed_al dep_computed_Church prep_from_computed_fragment auxpass_computed_were nsubjpass_computed_tables det_tables_These
C94-2178	J93-2003	o	Motivation There have been quite a number of recent papers on parallel text Brown et al -LRB- 1990 1991 1993 -RRB- Chen -LRB- 1993 -RRB- Church -LRB- 1993 -RRB- Church et al -LRB- 1993 -RRB- Dagan et al -LRB- 1993 -RRB- Gale and Church -LRB- 1991 1993 -RRB- Isabelle -LRB- 1992 -RRB- Kay and Rgsenschein -LRB- 1993 -RRB- Klavans and Tzoukermann -LRB- 1990 -RRB- Kupiec -LRB- 1993 -RRB- Matsumoto -LRB- 1991 -RRB- Ogden and Gonzales -LRB- 1993 -RRB- Shemtov -LRB- 1993 -RRB- Simard et al -LRB- 1992 -RRB- WarwickArmstrong and Russell -LRB- 1990 -RRB- Wu -LRB- to appear -RRB-	aux_appear_to appos_Russell_1990 appos_al_1992 dep_Simard_al nn_Simard_et appos_Shemtov_1993 appos_Matsumoto_1991 appos_Kupiec_1993 appos_Tzoukermann_1990 appos_Rgsenschein_1993 dep_Kay_appear appos_Kay_Wu conj_and_Kay_Russell conj_and_Kay_WarwickArmstrong amod_Kay_Simard appos_Kay_Shemtov appos_Kay_1993 conj_and_Kay_Gonzales conj_and_Kay_Ogden conj_and_Kay_Matsumoto conj_and_Kay_Kupiec conj_and_Kay_Tzoukermann conj_and_Kay_Klavans conj_and_Kay_Rgsenschein appos_Isabelle_1992 dep_1991_1993 conj_and_Gale_Church appos_al_1993 dep_Dagan_al nn_Dagan_et appos_al_1993 dep_Church_al nn_Church_et appos_Church_1993 nn_Church_Chen appos_Chen_1993 appos_1990_1993 appos_1990_1991 dep_al_Russell dep_al_WarwickArmstrong dep_al_Gonzales dep_al_Ogden dep_al_Matsumoto dep_al_Kupiec dep_al_Tzoukermann dep_al_Klavans dep_al_Rgsenschein dep_al_Kay conj_al_Isabelle dep_al_1991 appos_al_Church appos_al_Gale appos_al_Dagan appos_al_Church conj_al_Church dep_al_1990 nn_al_et nn_al_Brown amod_text_parallel amod_papers_recent dep_number_al prep_on_number_text prep_of_number_papers det_number_a advmod_number_quite cop_number_been aux_number_have expl_number_There dep_number_Motivation
C94-2178	J93-2003	o	Results This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies Church -LRB- 1993 -RRB- and Simard et al -LRB- 1992 -RRB-	dep_al_1992 nn_al_et nn_al_Simard conj_and_Church_al appos_Church_1993 amod_studies_other prep_of_number_studies det_number_a prep_in_used_number auxpass_used_been aux_used_has nsubjpass_used_that rcmod_Hansards_used amod_Hansards_Canadian det_Hansards_the prep_of_fragment_Hansards det_fragment_a dep_applied_al dep_applied_Church prep_to_applied_fragment auxpass_applied_was nsubjpass_applied_algorithm det_algorithm_This dep_Results_applied
C96-1037	J93-2003	o	The resolution of alignment can vat3 from low to high section paragraph sentence phrase and word -LRB- Gale and Church 1993 Matsumoto et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Matsumoto num_Church_1993 dep_Gale_al. conj_and_Gale_Church dep_word_Church dep_word_Gale conj_and_sentence_word conj_and_sentence_phrase dep_paragraph_word dep_paragraph_phrase dep_paragraph_sentence dep_section_paragraph dep_high_section prep_to_low_high prep_from_vat3_low aux_vat3_can nsubj_vat3_resolution prep_of_resolution_alignment det_resolution_The
C96-1037	J93-2003	o	-LRB- McArthur 1992 Mei et al. 1993 -RRB- Classification allows a word to align with a target word using the collective translation tendency of words in the same class	amod_class_same det_class_the prep_in_tendency_class prep_of_tendency_words nn_tendency_translation amod_tendency_collective det_tendency_the dobj_using_tendency vmod_word_using nn_word_target det_word_a prep_with_align_word aux_align_to vmod_word_align det_word_a dobj_allows_word nsubj_allows_Classification amod_Classification_McArthur dep_al._1993 nn_al._et nn_al._Mei dep_McArthur_al. num_McArthur_1992
C96-1040	J93-2003	o	machine translation -LRB- Brown et al. 1993 -RRB- but also in other applications such as word sense disanabiguation -LRB- Brown et al. 1991 -RRB- and bilingnal lexicography -LRB- Klavans and Tzoukermann 1990 -RRB-	num_Tzoukermann_1990 conj_and_Klavans_Tzoukermann dep_lexicography_Tzoukermann dep_lexicography_Klavans amod_lexicography_bilingnal dep_1991_al. dep_1991_Brown nn_al._et conj_and_disanabiguation_lexicography dep_disanabiguation_1991 nn_disanabiguation_sense nn_disanabiguation_word prep_such_as_applications_lexicography prep_such_as_applications_disanabiguation amod_applications_other prep_in_also_applications cc_also_but num_al._1993 nn_al._et amod_al._Brown advmod_translation_also appos_translation_al. nn_translation_machine
C96-1040	J93-2003	p	of the position infer marion of words at ltlat -LRB- hillg pairs of sellte/lCeS which turned out useful -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_useful_al. acomp_turned_useful prt_turned_out nsubj_turned_which rcmod_pairs_turned prep_of_pairs_sellte/lCeS nn_pairs_hillg conj_ltlat_pairs prep_of_marion_words prep_at_infer_ltlat dobj_infer_marion prep_of_infer_position det_position_the
C96-1040	J93-2003	o	-LRB- > \ -LSB- t he EM algorit hnt -LRB- Brown et al. 1993 -RRB- -LRB- I -RRB- etrtt > stcr et al. 1977 -RRB-	num_al._1977 nn_al._et nn_al._stcr dep_al._> dep_al._etrtt dep_al._I num_al._1993 nn_al._et amod_al._Brown dep_hnt_al. dep_hnt_al. amod_algorit_hnt dobj_EM_algorit nsubj_EM_he rcmod_t_EM appos_\_t amod_\_> dep_''_\
C96-1067	J93-2003	o	This conclusion is supported by the fact that true IMT is not to our knowledge used in most modern translator 's support environments eg -LRB- Eurolang 1995 I ` rederking et al. 1993 IBM 1995 Kugler et al. 1991 Nirenburg 1992 ~ li'ados 1995 -RRB-	amod_li'ados_1995 num_li'ados_~ dep_Nirenburg_li'ados num_Nirenburg_1992 num_al._1991 nn_al._et nn_al._Kugler num_IBM_1995 conj_al._Nirenburg conj_al._al. conj_al._IBM conj_al._1993 nn_al._et dobj_rederking_al. dep_I_rederking dep_Eurolang_I dep_Eurolang_1995 dep_eg_Eurolang nn_environments_support poss_environments_translator amod_translator_modern amod_translator_most prep_used_eg prep_in_used_environments prep_to_used_knowledge neg_used_not auxpass_used_is nsubjpass_used_IMT mark_used_that poss_knowledge_our amod_IMT_true ccomp_fact_used det_fact_the agent_supported_fact auxpass_supported_is nsubjpass_supported_conclusion det_conclusion_This
C96-1067	J93-2003	o	IIowever -LRB- Dagan et al. 1993 -RRB- have shown that knowledge of target-text length is not crucial to the model 's i -RRB- ertbrmanee	dep_ertbrmanee_i poss_ertbrmanee_model det_model_the prep_to_crucial_ertbrmanee neg_crucial_not cop_crucial_is nsubj_crucial_knowledge mark_crucial_that amod_length_target-text prep_of_knowledge_length ccomp_shown_crucial aux_shown_have nsubj_shown_IIowever amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et appos_IIowever_Dagan
C96-1078	J93-2003	o	Some o1 ' l his research has treated the sentenees as unstructured word sequences to be aligned this work has primarily involved the acquisition of bilingual lexical correspondences -LRB- Chen 1993 -RRB- although there has also been a n attempt to create a full MT system based on such trcat ment -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_ment_al. appos_trcat_ment amod_trcat_such prep_on_based_trcat vmod_system_based nn_system_MT amod_system_full det_system_a dobj_create_system aux_create_to vmod_attempt_create nn_attempt_n det_attempt_a cop_attempt_been advmod_attempt_also aux_attempt_has expl_attempt_there mark_attempt_although amod_Chen_1993 appos_correspondences_Chen amod_correspondences_lexical amod_correspondences_bilingual prep_of_acquisition_correspondences det_acquisition_the advcl_involved_attempt dobj_involved_acquisition advmod_involved_primarily aux_involved_has nsubj_involved_work det_work_this auxpass_aligned_be aux_aligned_to vmod_sequences_aligned nn_sequences_word amod_sequences_unstructured det_sentenees_the parataxis_treated_involved prep_as_treated_sequences dobj_treated_sentenees aux_treated_has nsubj_treated_research poss_research_his parataxis_l_treated nn_l_o1 det_l_Some ccomp_``_l
C96-2141	J93-2003	o	A sinfilar approach has been chosen by -LRB- Da.gan et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Da.gan dep_by_al. prep_chosen_by auxpass_chosen_been aux_chosen_has nsubjpass_chosen_approach amod_approach_sinfilar det_approach_A
C96-2141	J93-2003	o	In the recent years there have been a number of papers considering this or similar problems -LRB- Brown et al. 1990 -RRB- -LRB- Dagan et al. 1993 -RRB- -LRB- Kay et al. 1993 -RRB- -LRB- Fung et al. 1993 -RRB-	amod_Fung_1993 dep_Fung_al. nn_Fung_et amod_Kay_1993 dep_Kay_al. nn_Kay_et amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et amod_Brown_1990 dep_Brown_al. nn_Brown_et amod_problems_similar conj_or_this_problems dobj_considering_problems dobj_considering_this appos_number_Fung appos_number_Kay dep_number_Dagan dep_number_Brown vmod_number_considering prep_of_number_papers det_number_a cop_number_been aux_number_have expl_number_there prep_in_number_years amod_years_recent det_years_the
C96-2211	J93-2003	o	1993 Graham et al. 1980 -RRB- where K is the number of distinct nonternfinal symbols in the gramma.r G We ca.n expect a. very etfide.nt pa.rser tbr our pa.tterns r The input string ca.n a.lso be scanned to reduce the number of relewmt gramma.r rules before pa.rsing e The combined process is a.lso known as offlineparsing in LTAC	prep_in_offlineparsing_LTAC prep_as_known_offlineparsing dep_a.lso_known cop_a.lso_is nsubj_a.lso_tbr mark_a.lso_a. amod_process_combined det_process_The dep_process_e nn_rules_gramma.r nn_rules_relewmt prep_of_number_rules det_number_the prepc_before_reduce_pa.rsing dobj_reduce_number aux_reduce_to xcomp_scanned_reduce auxpass_scanned_be nsubjpass_scanned_pa.tterns nn_a.lso_ca.n nn_a.lso_string nn_a.lso_input det_a.lso_The nn_a.lso_r appos_pa.tterns_a.lso poss_pa.tterns_our appos_tbr_process rcmod_tbr_scanned nn_tbr_pa.rser amod_tbr_etfide.nt advmod_etfide.nt_very ccomp_expect_a.lso vmod_ca.n_expect nsubj_ca.n_We nn_G_gramma.r det_G_the prep_in_symbols_G amod_symbols_nonternfinal amod_symbols_distinct dep_number_ca.n prep_of_number_symbols det_number_the cop_number_is nsubj_number_K advmod_number_where dep_where_1980 dep_1980_Graham dep_1980_1993 nn_Graham_al. nn_Graham_et ccomp_''_number
D07-1003	J93-2003	o	This sort of problem can be solved in principle by conditional variants of the Expectation-Maximization algorithm -LRB- Baum et al. 1970 Dempster et al. 1977 Meng and Rubin 1993 Jebara and Pentland 1999 -RRB-	amod_Jebara_1999 conj_and_Jebara_Pentland num_Meng_1993 conj_and_Meng_Rubin dep_Dempster_Pentland dep_Dempster_Jebara conj_Dempster_Rubin conj_Dempster_Meng num_Dempster_1977 nn_Dempster_al. nn_Dempster_et dep_Baum_Dempster appos_Baum_1970 dep_Baum_al. nn_Baum_et nn_algorithm_Expectation-Maximization det_algorithm_the prep_of_variants_algorithm amod_variants_conditional dep_solved_Baum agent_solved_variants prep_in_solved_principle auxpass_solved_be aux_solved_can nsubjpass_solved_sort prep_of_sort_problem det_sort_This ccomp_``_solved
D07-1003	J93-2003	o	Similarly Murdock and Croft -LRB- 2005 -RRB- adopted a simple translation model from IBM model 1 -LRB- Brown et al. 1990 Brown et al. 1993 -RRB- and applied it to QA	prep_to_applied_QA dobj_applied_it nsubj_applied_Murdock num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Brown_Brown amod_Brown_1990 dep_Brown_al. nn_Brown_et num_model_1 nn_model_IBM dep_model_Brown prep_from_model_model nn_model_translation amod_model_simple det_model_a conj_and_adopted_applied dobj_adopted_model nsubj_adopted_Croft nsubj_adopted_Murdock advmod_adopted_Similarly appos_Croft_2005 conj_and_Murdock_Croft
D07-1003	J93-2003	o	The tree is produced by a state-of-the-art dependency parser -LRB- McDonald et al. 2005 -RRB- trained on the Wall Street Journal Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn nn_Treebank_Journal nn_Treebank_Street nn_Treebank_Wall det_Treebank_the prep_on_trained_Treebank amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et vmod_parser_trained appos_parser_McDonald nn_parser_dependency amod_parser_state-of-the-art det_parser_a dep_produced_Marcus agent_produced_parser auxpass_produced_is nsubjpass_produced_tree det_tree_The
D07-1005	J93-2003	o	2 Word Alignment Framework A statistical translation model -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- describes the relationship between a pair of sentences in the source and target languages -LRB- f = fJ1 e = eI1 -RRB- using a translation probability P -LRB- f | e -RRB-	dep_|_e nn_|_f dep_P_| nn_P_probability nn_P_translation det_P_a dobj_using_P dobj_=_eI1 dep_=_e dep_=_= dobj_=_fJ1 dep_=_f dep_languages_= nn_languages_target conj_and_source_languages det_source_the prep_of_pair_sentences det_pair_a prep_in_relationship_languages prep_in_relationship_source prep_between_relationship_pair det_relationship_the xcomp_describes_using dobj_describes_relationship nsubj_describes_model dep_Och_2003 conj_and_Och_Ney dep_al._Ney dep_al._Och num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation amod_model_statistical nn_model_A nn_model_Framework nn_model_Alignment nn_model_Word num_model_2
D07-1005	J93-2003	o	Given a sentence-pair -LRB- f e -RRB- the most likely -LRB- Viterbi -RRB- word alignment is found as -LRB- Brown et al. 1993 -RRB- a = argmaxa P -LRB- f a | e -RRB-	dep_|_e det_|_a appos_f_| dep_P_f nn_P_argmaxa dep_=_P amod_a_= num_al._1993 nn_al._et amod_al._Brown dep_as_a dep_as_al. dep_found_as auxpass_found_is nsubjpass_found_alignment prep_found_Given nn_alignment_word amod_alignment_likely det_alignment_the dep_likely_Viterbi advmod_likely_most dep_f_e appos_sentence-pair_f det_sentence-pair_a pobj_Given_sentence-pair dep_``_found
D07-1005	J93-2003	o	Given any word alignment model posterior probabilities can be computed as -LRB- Brown et al. 1993 -RRB- P -LRB- aj = i | e f -RRB- = summationdisplay a P -LRB- a | f e -RRB- -LRB- i aj -RRB- -LRB- 1 -RRB- where i -LCB- 0,1 I -RCB-	dep_0,1_i advmod_0,1_where appos_i_aj dep_|_e dep_|_f det_|_a dep_P_i appos_P_| det_P_a nn_P_summationdisplay dep_=_P dep_=_f dep_=_e advmod_=_| nn_|_i dep_=_= dep_aj_I advcl_aj_0,1 appos_aj_1 amod_aj_= appos_P_aj dep_P_al. mark_P_as dep_al._1993 nn_al._et amod_al._Brown advcl_computed_P auxpass_computed_be aux_computed_can nsubjpass_computed_probabilities amod_probabilities_posterior rcmod_model_computed nn_model_alignment nn_model_word det_model_any pobj_Given_model ccomp_``_Given
D07-1005	J93-2003	p	-LRB- 2 -RRB- We note that these posterior probabilities can be computed efficiently for some alignment models such as the HMM -LRB- Vogel et al. 1996 Och and Ney 2003 -RRB- Models 1 and 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_1_2 dep_Models_2 dep_Models_1 dep_Och_2003 conj_and_Och_Ney dep_Vogel_Ney dep_Vogel_Och amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_HMM_Brown appos_HMM_Models appos_HMM_Vogel det_HMM_the prep_such_as_models_HMM nn_models_alignment det_models_some prep_for_computed_models advmod_computed_efficiently auxpass_computed_be aux_computed_can nsubjpass_computed_probabilities mark_computed_that amod_probabilities_posterior det_probabilities_these ccomp_note_computed nsubj_note_We dep_note_2
D07-1006	J93-2003	o	5 Previous Work The LEAF model is inspired by the literature on generative modeling for statistical word alignment and particularly by Model 4 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_4 nn_alignment_word amod_alignment_statistical amod_modeling_generative conj_and_literature_Model prep_for_literature_alignment prep_on_literature_modeling det_literature_the agent_inspired_Model advmod_inspired_particularly agent_inspired_literature auxpass_inspired_is nsubjpass_inspired_model nn_model_LEAF det_model_The rcmod_Work_inspired amod_Work_Previous num_Work_5
D07-1006	J93-2003	o	2.2 Unsupervised Parameter Estimation We can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of Model 4 -LRB- Brown et al. 1993 -RRB- described thoroughly in -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och prep_described_in advmod_described_thoroughly amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 prep_of_that_Model amod_fashion_similar det_fashion_a det_model_this prep_of_parameters_model det_parameters_the prep_of_estimation_parameters nn_estimation_likelihood amod_estimation_maximum prep_to_perform_that prep_in_perform_fashion dobj_perform_estimation aux_perform_can nsubj_perform_We vmod_Estimation_described dep_Estimation_Brown rcmod_Estimation_perform nn_Estimation_Parameter amod_Estimation_Unsupervised num_Estimation_2.2 dep_``_Estimation
D07-1006	J93-2003	o	We use Viterbi training -LRB- Brown et al. 1993 -RRB- but neighborhood estimation -LRB- Al-Onaizan et al. 1999 Och and Ney 2003 -RRB- or pegging -LRB- Brown et al. 1993 -RRB- could also be used	auxpass_used_be advmod_used_also aux_used_could nsubjpass_used_al. num_Brown_1993 dep_Brown_al. nn_Brown_et conj_or_Och_pegging appos_Och_2003 conj_and_Och_Ney appos_al._Brown dep_al._pegging dep_al._Ney dep_al._Och num_al._1999 nn_al._et amod_al._Al-Onaizan rcmod_estimation_used nn_estimation_neighborhood num_al._1993 nn_al._et amod_al._Brown nn_training_Viterbi conj_but_use_estimation dep_use_al. dobj_use_training nsubj_use_We
D07-1006	J93-2003	o	-LRB- Brown et al. 1993 -RRB- defined two local search operations for their 1-to-N alignment models 3 4 and 5	conj_and_3_5 conj_and_3_4 dep_models_5 dep_models_4 dep_models_3 nn_models_alignment amod_models_1-to-N poss_models_their nn_operations_search amod_operations_local num_operations_two prep_for_defined_models dobj_defined_operations nsubj_defined_Brown amod_Brown_1993 dep_Brown_al. nn_Brown_et
D07-1025	J93-2003	o	Therefore to make the phrase-based SMT system robust against data sparseness for the ranking task we also make use of the IBM Model 4 -LRB- Brown et al. 1993 -RRB- in both directions	preconj_directions_both prep_in_Brown_directions amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM det_Model_the prep_of_use_Model dobj_make_use advmod_make_also nsubj_make_we amod_task_ranking det_task_the nn_sparseness_data prep_for_robust_task prep_against_robust_sparseness nsubj_robust_system nn_system_SMT amod_system_phrase-based det_system_the dep_make_Brown parataxis_make_make xcomp_make_robust aux_make_to advmod_make_Therefore
D07-1030	J93-2003	o	SMT has evolved from the original word-based approach -LRB- Brown et al. 1993 -RRB- into phrase-based approaches -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and syntax-based approaches -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knignt 2001 Chiang 2005 -RRB-	amod_Chiang_2005 dep_Yamada_Chiang conj_and_Yamada_2001 conj_and_Yamada_Knignt nn_al._et nn_al._Alshawi dep_Wu_2001 dep_Wu_Knignt dep_Wu_Yamada num_Wu_2000 dep_Wu_al. num_Wu_1997 dep_approaches_Wu amod_approaches_syntax-based dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_approaches_phrase-based dep_al._1993 nn_al._et amod_al._Brown dep_approach_al. amod_approach_word-based amod_approach_original det_approach_the conj_and_evolved_approaches dep_evolved_Koehn prep_into_evolved_approaches prep_from_evolved_approach aux_evolved_has nsubj_evolved_SMT ccomp_``_approaches ccomp_``_evolved
D07-1038	J93-2003	o	3.1 The traditional IBM alignment model IBM Model 4 -LRB- Brown et al. 1993 -RRB- learns a set of 4 probability tables to compute p -LRB- f | e -RRB- given a foreign sentence f and its target translation e via the following -LRB- greatly simplified -RRB- generative story 361 NP-C NPB NPB NNP taiwan POS s NN surplus PP IN in NP-C NPB NN trade PP IN between NP-C NPB DT the CD two NNS shores FTD0 GR G4E7 DYBG EL DIDV TAIWAN IN TWO-SHORES TRADE MIDDLE SURPLUS R1 NP-C NPB x0 NPB x1 NN x2 PP x0 x2EL x1 R10 NP-C NPB x0 NPB x1 NN x2 PP x0 x2 x1 R10 NP-C NPB x0 NPB x1 NN x2 PP x0 x2 x1 R2 NPB NNP taiwan POS s FTD0 R11 NPB x0 NNP POS s x0 R17 NPB NNP taiwan x0 POS x0 R12 NNP taiwan FTD0 R18 POS s FTD0 R3 PP x0 IN x1 NP-C x0 x1 R13 PP IN in x0 NP-C GR x0EL R19 PP IN in x0 NP-C x0 R4 IN in GR R5 NP-C x0 NPB x1 PP x1 x0 R5 NP-C x0 NPB x1 PP x1 x0 R20 NP-C x0 NPB PP x1 IN x2 NP-C x2 x0 x1 R6 PP IN between NP-C NPB DT the CD two NNS shores G4E7 R14 PP IN between x0 NP-C x0 R21 IN between EL R15 NP-C x0 NPB x0 R15 NP-C x0 NPB x0 R16 NPB DT the CD two NNS shores G4E7 R22 NPB x0 DT CD two x1 NNS x0 x1 R23 NNS shores G4E7 R24 DT the GR R7 NPB x0 NN x0 R7 NPB x0 NN x0 R7 NPB x0 NN x0 R8 NN trade DYBG R9 NN surplus DIDV R8 NN trade DYBG R9 NN surplus DIDV R8 NN trade DYBG R9 NN surplus DIDV Figure 2 A -LRB- English tree Chinese string -RRB- pair and three different sets of multilevel tree-to-string rules that can explain it the first set is obtained from bootstrap alignments the second from this papers re-alignment procedure and the third is a viable if poor quality alternative that is not learned	neg_learned_not auxpass_learned_is nsubjpass_learned_that rcmod_alternative_learned appos_quality_alternative amod_quality_poor pobj_if_quality ccomp_,_if dep_a_viable dep_is_a dep_third_is det_third_the nn_procedure_re-alignment nn_procedure_papers det_procedure_this conj_and_second_third prep_from_second_procedure det_second_the nn_alignments_bootstrap prep_from_obtained_alignments auxpass_obtained_is nsubjpass_obtained_set amod_set_first det_set_the dobj_explain_it aux_explain_can nsubj_explain_that rcmod_rules_explain amod_rules_tree-to-string amod_rules_multilevel prep_of_sets_rules amod_sets_different num_sets_three conj_and_pair_sets nn_pair_tree amod_string_Chinese conj_tree_string amod_tree_English nn_tree_A dep_Figure_sets dep_Figure_pair num_Figure_2 nn_Figure_DIDV nn_Figure_surplus nn_Figure_NN dep_R9_Figure nn_R9_DYBG nn_R9_trade nn_R9_NN dep_R8_R9 nn_R8_DIDV nn_R8_surplus nn_R8_NN dep_R9_R8 nn_R9_DYBG nn_R9_trade nn_R9_NN dep_R8_R9 nn_R8_DIDV nn_R8_surplus nn_R8_NN dep_R9_R8 nn_R9_DYBG nn_R9_trade nn_R9_NN dep_R8_R9 nn_R8_x0 nn_R8_NN dep_x0_R8 nn_x0_NPB dep_R7_x0 nn_R7_x0 nn_R7_NN dep_x0_R7 nn_x0_NPB dep_R7_x0 nn_R7_x0 nn_R7_NN dep_x0_R7 nn_x0_NPB dep_R7_x0 nn_R7_GR det_R7_the nn_R7_DT dep_R24_R7 nn_R24_G4E7 nn_R24_shores nn_R24_NNS dep_R23_R24 nn_R23_x1 nn_R23_x0 nn_R23_NNS dep_x1_R23 num_x1_two dep_CD_x1 nn_CD_DT dep_x0_CD nn_x0_NPB dep_R22_x0 nn_R22_G4E7 nn_R22_shores nn_R22_NNS num_R22_two nn_R22_CD det_R22_the dep_DT_R22 nn_DT_NPB dep_R16_DT nn_R16_x0 nn_R16_NPB dep_x0_R16 nn_x0_NP-C dep_R15_x0 nn_R15_x0 nn_R15_NPB dep_x0_R15 nn_x0_NP-C dep_R15_x0 nn_R15_EL pobj_between_R15 pcomp_IN_between prep_R21_IN nn_R21_x0 nn_R21_NP-C pobj_between_x0 pcomp_IN_between dep_PP_R21 prep_PP_IN dep_R14_PP nn_R14_G4E7 nn_R14_shores nn_R14_NNS num_R14_two nn_R14_CD det_R14_the dep_DT_R14 nn_DT_NPB nn_DT_NP-C pobj_between_DT pcomp_IN_between prep_PP_IN dep_R6_PP nn_R6_x1 nn_R6_x0 nn_R6_x2 nn_R6_NP-C dep_x1_R6 prep_in_x1_x2 nn_x1_PP nn_x1_NPB dep_x0_x1 nn_x0_NP-C dep_R20_x0 nn_R20_x0 nn_R20_x1 nn_R20_PP dep_x1_R20 nn_x1_NPB dep_x0_x1 nn_x0_NP-C dep_R5_x0 nn_R5_x0 nn_R5_x1 nn_R5_PP dep_x1_R5 nn_x1_NPB dep_x0_x1 nn_x0_NP-C nn_R5_GR pobj_in_R5 pcomp_IN_in prep_R4_IN nn_R4_x0 nn_R4_NP-C pobj_in_x0 pcomp_IN_in dep_PP_x0 dep_PP_R4 prep_PP_IN dep_R19_PP nn_R19_x0EL nn_R19_GR nn_R19_NP-C pobj_in_x0 pcomp_IN_in dep_PP_R19 prep_PP_IN dep_R13_PP nn_R13_x1 nn_R13_x0 nn_R13_NP-C dep_x0_R13 prep_in_x0_x1 nn_x0_PP dep_R3_x0 nn_R3_FTD0 nn_R3_s nn_R3_POS dep_R18_R3 nn_R18_FTD0 nn_R18_taiwan nn_R18_NNP dep_R12_R18 nn_R12_x0 nn_R12_POS dep_x0_R12 nn_x0_taiwan nn_x0_NNP nn_x0_NPB dep_x0_third dep_x0_second parataxis_x0_obtained dep_x0_x0 dobj_x0_R17 nsubj_x0_s nn_s_POS nn_s_NNP parataxis_x0_x0 nn_x0_NPB nn_R11_FTD0 nn_R11_s nn_R11_POS dep_taiwan_x0 dobj_taiwan_R11 nsubj_taiwan_NNP nn_NNP_NPB dep_R2_taiwan nn_R2_x1 nn_R2_x2 nn_R2_x0 nn_R2_PP dep_x2_R2 nn_x2_NN dep_x1_x2 nn_x1_NPB dep_x0_x1 nn_x0_NPB nn_x0_NP-C dep_R10_x0 nn_R10_x1 nn_R10_x2 nn_R10_x0 nn_R10_PP dep_x2_R10 nn_x2_NN dep_x1_x2 nn_x1_NPB dep_x0_x1 nn_x0_NPB nn_x0_NP-C dep_R10_x0 nn_R10_x1 nn_R10_x2EL nn_R10_x0 nn_R10_PP dep_x2_R10 nn_x2_NN dep_x1_x2 nn_x1_NPB dep_x0_x1 nn_x0_NPB nn_x0_NP-C nn_R1_SURPLUS amod_R1_MIDDLE nn_R1_TRADE nn_R1_TWO-SHORES prep_in_TAIWAN_R1 nn_TAIWAN_DIDV nn_TAIWAN_EL nn_TAIWAN_DYBG nn_TAIWAN_G4E7 nn_TAIWAN_GR nn_TAIWAN_FTD0 nn_TAIWAN_shores nn_TAIWAN_NNS num_TAIWAN_two nn_TAIWAN_CD det_TAIWAN_the dep_DT_TAIWAN nn_DT_NPB nn_DT_NP-C pobj_between_DT pcomp_IN_between nn_PP_trade nn_PP_NN nn_PP_NPB nn_PP_NP-C pobj_in_PP pcomp_IN_in nn_PP_surplus nn_PP_NN dep_s_x0 prep_s_IN prep_s_IN dobj_s_PP nsubj_s_POS nn_POS_taiwan nn_POS_NNP nn_POS_NPB nn_POS_NPB nn_POS_NP-C num_POS_361 dep_story_s amod_story_generative nn_story_following advmod_simplified_greatly dep_following_simplified det_following_the dep_translation_e nn_translation_target poss_translation_its conj_and_f_translation dep_sentence_translation dep_sentence_f amod_sentence_foreign det_sentence_a pobj_given_sentence prep_via_|_story prep_|_given dep_|_e nn_|_f dep_p_| dobj_compute_p aux_compute_to nn_tables_probability num_tables_4 vmod_set_compute prep_of_set_tables det_set_a dobj_learns_set nsubj_learns_Model amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_4 nn_Model_IBM nn_Model_model nn_Model_alignment nn_Model_IBM amod_Model_traditional det_Model_The rcmod_3.1_learns
D07-1038	J93-2003	o	However searching the space of all possible alignments is intractable for EM so in practice the procedure is bootstrapped by models with narrower search space such as IBM Model 1 -LRB- Brown et al. 1993 -RRB- or Aachen HMM -LRB- Vogel et al. 1996 -RRB-	amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_HMM_Vogel nn_HMM_Aachen amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_or_Model_HMM dep_Model_Brown num_Model_1 nn_Model_IBM prep_such_as_space_HMM prep_such_as_space_Model nn_space_search amod_space_narrower prep_with_models_space agent_bootstrapped_models auxpass_bootstrapped_is nsubjpass_bootstrapped_procedure det_procedure_the dep_intractable_bootstrapped prep_in_intractable_practice advmod_intractable_so prep_for_intractable_EM cop_intractable_is csubj_intractable_searching advmod_intractable_However amod_alignments_possible det_alignments_all prep_of_space_alignments det_space_the dobj_searching_space
D07-1045	J93-2003	o	This approach is usually referred to as the noisy source-channel approach in statistical machine translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical prep_in_approach_translation amod_approach_source-channel amod_approach_noisy det_approach_the dep_referred_al. prep_as_to_referred_approach advmod_referred_usually auxpass_referred_is nsubjpass_referred_approach det_approach_This
D07-1079	J93-2003	o	Approaches include word substitution systems -LRB- Brown et al. 1993 -RRB- phrase substitution systems -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and synchronous context-free grammar systems -LRB- Wu and Wong 1998 Chiang 2005 -RRB- all of which train on string pairs and seek to establish connections between source and target strings	nn_strings_target conj_and_source_strings prep_between_connections_strings prep_between_connections_source dobj_establish_connections aux_establish_to xcomp_seek_establish nn_pairs_string conj_and_train_seek prep_on_train_pairs dep_all_seek dep_all_train prep_of_all_which num_Chiang_2005 dep_Wu_Chiang num_Wu_1998 conj_and_Wu_Wong appos_systems_Wong appos_systems_Wu nn_systems_grammar amod_systems_context-free amod_systems_synchronous dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et appos_systems_Koehn nn_systems_substitution nn_systems_phrase amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_systems_all conj_and_systems_systems conj_and_systems_systems dep_systems_Brown nn_systems_substitution nn_systems_word dobj_include_all dobj_include_systems dobj_include_systems dobj_include_systems nsubj_include_Approaches
D07-1090	J93-2003	o	1 Introduction Given a source-language -LRB- e.g. French -RRB- sentence f the problem of machine translation is to automatically produce a target-language -LRB- e.g. English -RRB- translation e The mathematics of the problem were formalized by -LRB- Brown et al. 1993 -RRB- and re-formulated by -LRB- Och and Ney 2004 -RRB- in terms of the optimization e = arg maxe Msummationdisplay m = 1 mhm -LRB- e f -RRB- -LRB- 1 -RRB- where fhm -LRB- e f -RRB- g is a set of M feature functions and fmg a set of weights	prep_of_set_weights det_set_a dep_fmg_set conj_and_functions_fmg nn_functions_feature nn_functions_M prep_of_set_fmg prep_of_set_functions det_set_a cop_set_is nsubj_set_g advmod_set_where nn_g_fhm dep_e_f dep_fhm_e dep_e_f num_mhm_1 dep_=_mhm appos_m_e amod_m_= nn_m_Msummationdisplay nn_m_maxe nn_m_arg dep_=_set dep_=_1 dep_=_m dep_=_e dep_=_optimization dep_=_the prep_of_terms_= prep_in_Och_terms num_Och_2004 conj_and_Och_Ney prep_by_re-formulated_Ney prep_by_re-formulated_Och conj_and_al._re-formulated num_al._1993 nn_al._et amod_al._Brown agent_formalized_re-formulated agent_formalized_al. auxpass_formalized_were nsubjpass_formalized_mathematics dep_formalized_e det_problem_the prep_of_mathematics_problem det_mathematics_The dep_translation_formalized nn_translation_English advmod_translation_e.g. dep_target-language_translation det_target-language_a dobj_produce_target-language advmod_produce_automatically aux_produce_to aux_produce_is nsubj_produce_French nn_translation_machine prep_of_problem_translation det_problem_the nn_f_sentence appos_French_problem dep_French_f dep_French_e.g. rcmod_source-language_produce det_source-language_a pobj_Given_source-language prep_Introduction_Given num_Introduction_1
D07-1103	J93-2003	o	These joint counts are estimated using the phrase induction algorithm described in -LRB- Koehn et al. 2003 -RRB- with symmetrized word alignments generated using IBM model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_IBM dobj_using_model vmod_generated_using vmod_alignments_generated nn_alignments_word amod_alignments_symmetrized dep_Koehn_Brown prep_with_Koehn_alignments amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn vmod_algorithm_described nn_algorithm_induction nn_algorithm_phrase det_algorithm_the dobj_using_algorithm xcomp_estimated_using auxpass_estimated_are nsubjpass_estimated_counts amod_counts_joint det_counts_These
D08-1026	J93-2003	o	The words with the highest association probabilities are chosen as acquired words for entity e. 4.1 Base Model I Using the translation model I -LRB- Brown et al. 1993 -RRB- where each word is equally likely to be aligned with each entity we have p -LRB- w | e -RRB- = 1 -LRB- l + 1 -RRB- m mproductdisplay j = 1 lsummationdisplay i = 0 p -LRB- wj | ei -RRB- -LRB- 1 -RRB- where l and m are the lengths of entity and word sequences respectively	nn_sequences_word conj_and_entity_sequences advmod_lengths_respectively prep_of_lengths_sequences prep_of_lengths_entity det_lengths_the cop_lengths_are nsubj_lengths_m nsubj_lengths_l advmod_lengths_where conj_and_l_m dep_1_lengths num_ei_| nn_ei_wj dep_p_1 appos_p_ei num_p_0 tmod_=_p amod_i_= dep_lsummationdisplay_i nn_j_mproductdisplay nn_j_m conj_+_l_1 dep_=_lsummationdisplay dep_=_1 dep_=_= dep_=_j dep_=_1 dep_=_l num_=_1 dep_=_e dep_=_| dep_=_w dep_p_= dobj_have_p nsubj_have_we dep_have_Model dep_have_Base dep_have_4.1 nsubj_have_e. dep_have_entity dep_have_for det_entity_each prep_with_aligned_entity auxpass_aligned_be aux_aligned_to xcomp_likely_aligned advmod_likely_equally cop_likely_is nsubj_likely_word advmod_likely_where det_word_each num_al._1993 nn_al._et amod_al._Brown rcmod_I_likely appos_I_al. nn_I_model nn_I_translation det_I_the dobj_Using_I vmod_I_Using dep_Model_I vmod_words_have amod_words_acquired prep_as_chosen_words auxpass_chosen_are nsubjpass_chosen_words nn_probabilities_association amod_probabilities_highest det_probabilities_the prep_with_words_probabilities det_words_The ccomp_``_chosen
D08-1026	J93-2003	o	4.2 Base Model II Using the translation model II -LRB- Brown et al. 1993 -RRB- where alignments are dependent on word/entity positions and word/entity sequence lengths we have p -LRB- w | e -RRB- = mproductdisplay j = 1 lsummationdisplay i = 0 p -LRB- aj = i | j m l -RRB- p -LRB- wj | ei -RRB- -LRB- 2 -RRB- where aj = i means that wj is aligned with ei	prep_with_aligned_ei auxpass_aligned_is nsubjpass_aligned_wj mark_aligned_that ccomp_means_aligned nsubj_means_aj advmod_means_where dep_=_i amod_aj_= dep_2_means num_ei_| nn_ei_wj dep_p_2 appos_p_ei nn_p_l nn_p_m conj_j_p nn_j_| nn_|_i dobj_=_j amod_aj_= appos_p_aj num_p_0 dobj_=_p dep_i_= dep_lsummationdisplay_i num_lsummationdisplay_1 dep_=_lsummationdisplay amod_j_= nn_j_mproductdisplay dobj_=_j dep_=_e dep_=_| dep_=_w dep_p_= dobj_have_p nsubj_have_we nn_lengths_sequence nn_lengths_word/entity conj_and_positions_lengths nn_positions_word/entity prep_on_dependent_lengths prep_on_dependent_positions cop_dependent_are nsubj_dependent_alignments advmod_dependent_where amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_II nn_model_translation det_model_the dobj_Using_model rcmod_II_have rcmod_II_dependent dep_II_Brown vmod_II_Using nn_II_Model nn_II_Base num_II_4.2 dep_``_II
D08-1039	J93-2003	o	One of the simplest models that can be seen in the context of lexical triggers is the IBM model 1 -LRB- Brown et al. 1993 -RRB- which captures lexical dependencies between source and target words	nn_words_target conj_and_source_words prep_between_dependencies_words prep_between_dependencies_source amod_dependencies_lexical dobj_captures_dependencies nsubj_captures_which amod_Brown_1993 dep_Brown_al. nn_Brown_et rcmod_1_captures dep_1_Brown dobj_model_1 nn_model_IBM det_model_the cop_model_is nsubj_model_One amod_triggers_lexical prep_of_context_triggers det_context_the prep_in_seen_context auxpass_seen_be aux_seen_can nsubjpass_seen_that rcmod_models_seen amod_models_simplest det_models_the prep_of_One_models
D08-1039	J93-2003	o	3 Model As an extension to commonly used lexical word pair probabilities p -LRB- f | e -RRB- as introduced in -LRB- Brown et al. 1993 -RRB- we define our model to operate on word triplets	nn_triplets_word prep_on_operate_triplets aux_operate_to vmod_model_operate poss_model_our dobj_define_model nsubj_define_we num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_introduced_in mark_introduced_as rcmod_|_define advcl_|_introduced dep_|_e nn_|_f dep_p_| amod_probabilities_p nn_probabilities_pair nn_probabilities_word amod_probabilities_lexical dobj_used_probabilities advmod_used_commonly dep_to_used prep_extension_to det_extension_an prep_as_Model_extension num_Model_3 dep_``_Model
D08-1039	J93-2003	o	The resulting training procedure is analogous to the one presented in -LRB- Brown et al. 1993 -RRB- and -LRB- Tillmann and Ney 1997 -RRB-	amod_Tillmann_1997 conj_and_Tillmann_Ney num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_presented_in vmod_one_presented det_one_the conj_and_analogous_Ney conj_and_analogous_Tillmann prep_to_analogous_one cop_analogous_is nsubj_analogous_procedure nn_procedure_training amod_procedure_resulting det_procedure_The
D08-1043	J93-2003	o	Usually the IBM Model 1 developed in the statistical machine translation field -LRB- Brown et al. 1993 -RRB- is used to construct translation models for retrieval purposes in practice	prep_in_purposes_practice nn_purposes_retrieval nn_models_translation prep_for_construct_purposes dobj_construct_models aux_construct_to xcomp_used_construct auxpass_used_is nsubjpass_used_Model advmod_used_Usually num_al._1993 nn_al._et amod_al._Brown nn_field_translation nn_field_machine amod_field_statistical det_field_the dep_developed_al. prep_in_developed_field vmod_Model_developed num_Model_1 nn_Model_IBM det_Model_the
D08-1053	J93-2003	p	Compared with clean parallel corpora such as Hansard -LRB- Brown et al. 1993 -RRB- which consists of 505 French-English translations of political debates in the Canadian parliament texts from the web are far more diverse and noisy	nsubj_noisy_Compared conj_and_diverse_noisy advmod_diverse_more cop_diverse_are nsubj_diverse_Compared advmod_more_far det_web_the prep_from_texts_web amod_parliament_Canadian det_parliament_the prep_in_debates_parliament amod_debates_political prep_of_translations_debates amod_translations_French-English num_translations_505 prep_of_consists_translations nsubj_consists_which dep_al._1993 nn_al._et advmod_Brown_al. appos_Hansard_texts rcmod_Hansard_consists appos_Hansard_Brown prep_such_as_corpora_Hansard amod_corpora_parallel amod_corpora_clean prep_with_Compared_corpora
D08-1053	J93-2003	o	1 Introduction Sentence-aligned parallel bilingual corpora have been essential resources for statistical machine translation -LRB- Brown et al. 1993 -RRB- and many other multi-lingual natural language processing applications	nn_applications_processing nn_applications_language amod_applications_natural amod_applications_multi-lingual amod_applications_other amod_applications_many num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical conj_and_resources_applications dep_resources_al. prep_for_resources_translation amod_resources_essential cop_resources_been aux_resources_have nsubj_resources_corpora amod_corpora_bilingual nn_corpora_parallel amod_corpora_Sentence-aligned nn_corpora_Introduction num_corpora_1 ccomp_``_applications ccomp_``_resources
D08-1082	J93-2003	o	9.1 Training Methodology Given a training set we first run a variant of IBM alignment model 1 -LRB- Brown et al. 1993 -RRB- for 100 iterations and then initialize Model I with the learned parameter values	nn_values_parameter amod_values_learned det_values_the prep_with_I_values dep_Model_I dobj_initialize_Model advmod_initialize_then nsubj_initialize_Methodology num_iterations_100 amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_alignment nn_model_IBM prep_of_variant_model det_variant_a conj_and_run_initialize prep_for_run_iterations dep_run_Brown dobj_run_variant advmod_run_first nsubj_run_we nsubj_run_Methodology nn_set_training det_set_a pobj_Given_set prep_Methodology_Given amod_Methodology_Training num_Methodology_9.1
D08-1082	J93-2003	o	It acquires a set of synchronous lexical entries by running the IBM alignment model -LRB- Brown et al. 1993 -RRB- and learns a log-linear model to weight parses	nsubj_parses_model prep_to_model_weight amod_model_log-linear det_model_a ccomp_learns_parses nsubj_learns_It num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_alignment nn_model_IBM det_model_the dobj_running_model amod_entries_lexical amod_entries_synchronous prep_of_set_entries det_set_a conj_and_acquires_learns prepc_by_acquires_running dobj_acquires_set nsubj_acquires_It ccomp_``_learns ccomp_``_acquires
D08-1084	J93-2003	o	Although we have argued -LRB- section 2 -RRB- that this is unlikely to succeed to our knowledge we are the first to investigate the matter empirically .11 The best-known MT aligner is undoubtedly GIZA + + -LRB- Och and Ney 2003 -RRB- which contains implementations of various IBM models -LRB- Brown et al. 1993 -RRB- as well as the HMM model of Vogel et al.	nn_al._et nn_al._Vogel prep_of_model_al. nn_model_HMM det_model_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM amod_models_various conj_and_implementations_model appos_implementations_Brown prep_of_implementations_models dobj_contains_model dobj_contains_implementations nsubj_contains_which num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och rcmod_GIZA_contains conj_+_GIZA_+ advmod_GIZA_undoubtedly cop_GIZA_is nsubj_GIZA_aligner nn_aligner_MT amod_aligner_best-known det_aligner_The num_aligner_.11 quantmod_.11_empirically rcmod_matter_+ rcmod_matter_GIZA det_matter_the dobj_investigate_matter aux_investigate_to xcomp_first_investigate det_first_the cop_first_are nsubj_first_we prep_to_first_knowledge advcl_first_argued poss_knowledge_our aux_succeed_to xcomp_unlikely_succeed cop_unlikely_is nsubj_unlikely_this mark_unlikely_that num_section_2 ccomp_argued_unlikely dep_argued_section aux_argued_have nsubj_argued_we mark_argued_Although
D08-1084	J93-2003	o	The MT community has developed not only an extensive literature on alignment -LRB- Brown et al. 1993 Vogel et al. 1996 Marcu and Wong 2002 DeNero et al. 2006 -RRB- but also standard proven alignment tools such as GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ prep_such_as_tools_+ prep_such_as_tools_GIZA nn_tools_alignment amod_tools_proven amod_tools_standard advmod_standard_also num_DeNero_2006 nn_DeNero_al. nn_DeNero_et conj_but_Marcu_tools dep_Marcu_DeNero num_Marcu_2002 conj_and_Marcu_Wong dep_Vogel_tools dep_Vogel_Wong dep_Vogel_Marcu num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_al._Vogel num_al._1993 nn_al._et amod_al._Brown dep_literature_al. prep_on_literature_alignment amod_literature_extensive det_literature_an preconj_literature_only neg_only_not dobj_developed_literature aux_developed_has nsubj_developed_community nn_community_MT det_community_The ccomp_``_developed
D09-1014	J93-2003	o	We use GIZA + + -LRB- Och and Ney 2003 -RRB- to train generative directed alignment models HMM and IBM Model4 -LRB- Brown et al. 1993 -RRB- from training record-text pairs	nn_pairs_record-text nn_pairs_training num_al._1993 nn_al._et amod_al._Brown nn_Model4_IBM prep_from_HMM_pairs dep_HMM_al. conj_and_HMM_Model4 dep_models_Model4 dep_models_HMM nn_models_alignment amod_models_directed amod_models_generative dobj_train_models aux_train_to num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ xcomp_use_train dobj_use_+ dobj_use_GIZA nsubj_use_We
D09-1014	J93-2003	o	Traditionally generative word alignment models have been trained on massive parallel corpora -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_corpora_al. amod_corpora_parallel amod_corpora_massive prep_on_trained_corpora auxpass_trained_been aux_trained_have nsubjpass_trained_models advmod_trained_Traditionally nn_models_alignment nn_models_word amod_models_generative
D09-1014	J93-2003	o	The structure of the graphical model resembles IBM Model 1 -LRB- Brown et al. 1993 -RRB- in which each target -LRB- record -RRB- word is assigned one or more source -LRB- text -RRB- words	nn_words_source appos_source_text num_source_more num_source_one conj_or_one_more dobj_assigned_words auxpass_assigned_is nsubjpass_assigned_word prep_in_assigned_which nn_word_target appos_target_record det_target_each amod_Brown_1993 dep_Brown_al. nn_Brown_et rcmod_Model_assigned dep_Model_Brown num_Model_1 nn_Model_IBM dobj_resembles_Model nsubj_resembles_structure amod_model_graphical det_model_the prep_of_structure_model det_structure_The
D09-1014	J93-2003	n	Furthermore we provide a 63.8 % error reduction compared to IBM Model 4 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM pobj_reduction_Model prepc_compared_to_reduction_to nn_reduction_error amod_reduction_% det_reduction_a number_%_63.8 dep_provide_Brown dobj_provide_reduction nsubj_provide_we advmod_provide_Furthermore
D09-1014	J93-2003	o	3.1 Conditional Random Field for Alignment Our conditional random field -LRB- CRF -RRB- for alignment has a graphical model structure that resembles that of IBM Model 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_1 nn_Model_IBM dep_that_Brown prep_of_that_Model dobj_resembles_that nsubj_resembles_that rcmod_structure_resembles nn_structure_model amod_structure_graphical det_structure_a dobj_has_structure nsubj_has_Field appos_field_CRF amod_field_random amod_field_conditional poss_field_Our prep_for_Field_alignment dep_Field_field prep_for_Field_Alignment nn_Field_Random amod_Field_Conditional num_Field_3.1
D09-1022	J93-2003	p	In this work we propose two models that can be categorized as extensions of standard word lexicons A discriminative word lexicon that uses global i.e. sentence-level source information to predict the target words using a statistical classifier and a trigger-based lexicon model that extends the well-known IBM model 1 -LRB- Brown et al. 1993 -RRB- with a second trigger allowing for a more finegrained lexical choice of target words	nn_words_target prep_of_choice_words amod_choice_lexical amod_choice_finegrained det_choice_a advmod_finegrained_more prep_for_allowing_choice amod_trigger_second det_trigger_a amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_model_Brown num_model_1 nn_model_IBM amod_model_well-known det_model_the xcomp_extends_allowing prep_with_extends_trigger dobj_extends_model nsubj_extends_that rcmod_model_extends nn_model_lexicon amod_model_trigger-based det_model_a conj_and_classifier_model amod_classifier_statistical det_classifier_a dobj_using_model dobj_using_classifier vmod_words_using nn_words_target det_words_the dobj_predict_words aux_predict_to nn_information_source amod_information_sentence-level amod_information_global dep_global_i.e. vmod_uses_predict dobj_uses_information nsubj_uses_that rcmod_lexicon_uses nn_lexicon_word amod_lexicon_discriminative det_lexicon_A nn_lexicons_word amod_lexicons_standard prep_of_extensions_lexicons prep_as_categorized_extensions auxpass_categorized_be aux_categorized_can nsubjpass_categorized_that dep_models_lexicon rcmod_models_categorized num_models_two dobj_propose_models nsubj_propose_we prep_in_propose_work det_work_this
D09-1022	J93-2003	o	There are three major types of models Heuristic models as in -LRB- Melamed 2000 -RRB- generative models as the IBM models -LRB- Brown et al. 1993 -RRB- and discriminative models -LRB- Varea et al. 2001 Bangalore et al. 2006 -RRB-	num_Bangalore_2006 nn_Bangalore_al. nn_Bangalore_et dep_Varea_Bangalore appos_Varea_2001 dep_Varea_al. nn_Varea_et amod_models_discriminative amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the dep_models_Brown prep_as_models_models amod_models_generative dep_Melamed_Varea conj_and_Melamed_models conj_and_Melamed_models dep_Melamed_2000 pobj_in_models pobj_in_models pobj_in_Melamed pcomp_as_in prep_models_as amod_models_Heuristic dep_types_models prep_of_types_models amod_types_major num_types_three nsubj_are_types expl_are_There ccomp_``_are
D09-1022	J93-2003	o	One of the simplest models in the context of lexical triggers is the IBM model 1 -LRB- Brown et al. 1993 -RRB- which captures lexical dependencies between source and target words	nn_words_target conj_and_source_words prep_between_dependencies_words prep_between_dependencies_source amod_dependencies_lexical dobj_captures_dependencies nsubj_captures_which amod_Brown_1993 dep_Brown_al. nn_Brown_et rcmod_1_captures dep_1_Brown dobj_model_1 nn_model_IBM det_model_the cop_model_is nsubj_model_One amod_triggers_lexical prep_of_context_triggers det_context_the prep_in_models_context amod_models_simplest det_models_the prep_of_One_models
D09-1023	J93-2003	n	This is a problem with other direct translation models such as IBM model 1 used as a direct model rather than a channel model -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_channel det_model_a amod_model_direct det_model_a prep_as_used_model conj_negcc_1_model vmod_1_used dep_model_model dep_model_1 nn_model_IBM prep_such_as_models_model nn_models_translation amod_models_direct amod_models_other prep_with_problem_models det_problem_a cop_problem_is nsubj_problem_This
D09-1024	J93-2003	o	The GIZA + + aligner is based on IBM Model 4 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM dep_based_Brown prep_on_based_Model auxpass_based_is nsubjpass_based_+ nsubjpass_based_GIZA pobj_+_aligner conj_+_GIZA_+ det_GIZA_The
D09-1024	J93-2003	o	1 Introduction Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research for example -LRB- Brown et al. 1993 Ittycheriah and Roukos 2005 Fraser and Marcu 2007 -RRB- including work leveraging syntactic parse trees e.g. -LRB- Cherry and Lin 2006 DeNero and Klein 2007 Fossum et al. 2008 -RRB-	nn_al._et nn_al._Fossum num_DeNero_2007 conj_and_DeNero_Klein amod_Cherry_2008 dep_Cherry_al. dep_Cherry_Klein dep_Cherry_DeNero num_Cherry_2006 conj_and_Cherry_Lin appos_e.g._Lin appos_e.g._Cherry nn_trees_parse amod_trees_syntactic dobj_leveraging_trees vmod_work_leveraging dep_Ittycheriah_2007 conj_and_Ittycheriah_Marcu conj_and_Ittycheriah_Fraser conj_and_Ittycheriah_2005 conj_and_Ittycheriah_Roukos dep_Brown_Marcu dep_Brown_Fraser dep_Brown_2005 dep_Brown_Roukos dep_Brown_Ittycheriah amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_of_amount_research amod_amount_significant det_amount_a prep_received_e.g. prep_including_received_work appos_received_Brown prep_for_received_example dobj_received_amount aux_received_has nsubj_received_alignment nn_systems_translation nn_systems_machine amod_systems_statistical nn_systems_training conj_and_component_received prep_in_component_systems amod_component_critical det_component_a cop_component_is nsubj_component_alignment nn_alignment_Word nn_alignment_Introduction num_alignment_1
D09-1039	J93-2003	o	-LRB- Yamada and Knight 2001 -RRB- follow -LRB- Brown et al. 1993 -RRB- in using the noisy channel model by decomposing the translation decisions modeled by the translation model into different types and inducing probability distributions via maximum likelihood estimation over each decision type	nn_type_decision det_type_each prep_over_estimation_type nn_estimation_likelihood nn_estimation_maximum nn_distributions_probability prep_via_inducing_estimation dobj_inducing_distributions nsubj_inducing_Yamada amod_types_different nn_model_translation det_model_the prep_into_modeled_types agent_modeled_model vmod_decisions_modeled nn_decisions_translation det_decisions_the dobj_decomposing_decisions nn_model_channel amod_model_noisy det_model_the dobj_using_model amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_follow_inducing prepc_by_follow_decomposing prepc_in_follow_using dep_follow_Brown nsubj_follow_Knight nsubj_follow_Yamada amod_Yamada_2001 conj_and_Yamada_Knight
D09-1050	J93-2003	o	Previous SMT systems -LRB- e.g. Brown et al. 1993 -RRB- used a word-based translation model which assumes that a sentence can be translated into other languages by translating each word into one or more words in the target language	nn_language_target det_language_the prep_in_words_language num_words_more num_words_one conj_or_one_more det_word_each prep_into_translating_words dobj_translating_word amod_languages_other agent_translated_translating prep_into_translated_languages auxpass_translated_be aux_translated_can nsubjpass_translated_sentence mark_translated_that det_sentence_a ccomp_assumes_translated nsubj_assumes_which rcmod_model_assumes nn_model_translation amod_model_word-based det_model_a dobj_used_model nsubj_used_systems dep_al._1993 nn_al._et nn_al._Brown dep_al._e.g. dep_systems_al. nn_systems_SMT amod_systems_Previous
D09-1050	J93-2003	o	However since we are interested in the word counts that correlate to w we adopt the concept of the translation model proposed by Brown et al -LRB- 1993 -RRB-	appos_al_1993 nn_al_et nn_al_Brown agent_proposed_al vmod_model_proposed nn_model_translation det_model_the prep_of_concept_model det_concept_the dobj_adopt_concept nsubj_adopt_we aux_w_to xcomp_correlate_w nsubj_correlate_that parataxis_counts_adopt ccomp_counts_correlate csubj_counts_interested advmod_counts_However det_word_the prep_in_interested_word cop_interested_are nsubj_interested_we mark_interested_since
D09-1051	J93-2003	o	Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts -LRB- Choueka et al. 1983 Church and Hanks 1990 Smadja 1993 Dunning 1993 Pearce 2002 Evert 2004 -RRB-	amod_Evert_2004 num_Pearce_2002 num_Dunning_1993 num_Smadja_1993 dep_Church_Evert conj_and_Church_Pearce conj_and_Church_Dunning conj_and_Church_Smadja conj_and_Church_1990 conj_and_Church_Hanks dep_Choueka_Pearce dep_Choueka_Dunning dep_Choueka_Smadja dep_Choueka_1990 dep_Choueka_Hanks dep_Choueka_Church dep_Choueka_1983 dep_Choueka_al. nn_Choueka_et prep_in_pairs_texts nn_pairs_word det_pairs_the prep_of_frequencies_pairs amod_frequencies_co-occurring dep_carried_Choueka prep_based_on_carried_frequencies prt_carried_out auxpass_carried_are nsubjpass_carried_studies nn_extraction_collocation prep_on_studies_extraction amod_studies_Many ccomp_``_carried
D09-1051	J93-2003	o	The difference between MWA and bilingual word alignment -LRB- Brown et al. 1993 -RRB- is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment	nn_alignment_word amod_alignment_bilingual agent_used_alignment vmod_corpus_used amod_corpus_bilingual amod_corpus_parallel amod_corpus_monolingual prep_instead_of_works_corpus prep_on_works_corpus nsubj_works_method mark_works_that nn_method_MWA det_method_the ccomp_is_works nsubj_is_difference dep_1993_al. nn_al._et num_Brown_1993 nn_alignment_word amod_alignment_bilingual dep_MWA_Brown conj_and_MWA_alignment prep_between_difference_alignment prep_between_difference_MWA det_difference_The ccomp_``_is
D09-1051	J93-2003	o	Thus the alignment set is denoted as -RCB- & -RSB- ,1 -LSB- | -RRB- -LCB- -LRB- ialiaiA ii = We adapt the bilingual word alignment model IBM Model 3 -LRB- Brown et al. 1993 -RRB- to monolingual word alignment	nn_alignment_word amod_alignment_monolingual amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_Model_Brown num_Model_3 nn_Model_IBM nn_model_alignment nn_model_word amod_model_bilingual det_model_the dobj_adapt_model nsubj_adapt_We dep_adapt_,1 dep_adapt_as dep_=_ii dep_=_ialiaiA dep_,1_= appos_,1_| cc_as_& dep_denoted_adapt auxpass_denoted_is nsubjpass_denoted_set prep_to_alignment_alignment conj_alignment_Model rcmod_alignment_denoted det_alignment_the dep_Thus_alignment dep_``_Thus
D09-1092	J93-2003	o	In the early statistical translation model work at IBM these representations were called cepts short for concepts -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_for_short_concepts dep_cepts_Brown amod_cepts_short dobj_called_cepts auxpass_called_were nsubjpass_called_representations prep_in_called_work det_representations_these prep_at_work_IBM nn_work_model nn_work_translation amod_work_statistical amod_work_early det_work_the
D09-1106	J93-2003	o	Generative methods -LRB- Brown et al. 1993 Vogel and Ney 1996 -RRB- treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization -LRB- EM -RRB- algorithm	nn_algorithm_maximization nn_algorithm_expectation det_algorithm_the appos_maximization_EM dobj_using_algorithm nn_corpus_training amod_corpus_bilingual prep_of_likelihood_corpus det_likelihood_the xcomp_maximize_using dobj_maximize_likelihood amod_process_hidden det_process_a conj_and_alignment_maximize prep_as_alignment_process nn_alignment_word nn_alignment_treat dep_alignment_Brown nn_alignment_methods conj_and_Vogel_Ney dep_Brown_1996 dep_Brown_Ney dep_Brown_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_methods_Generative
D09-1117	J93-2003	n	Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true -LRB- for example figure in the first row and first column means that for 98.9 percent of the language pairs the BLEU score for the bidirectional decoder was better than that of the forward decoder -RRB- proach -LRB- Brown et al. 1993 -RRB- -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_proach_al. amod_decoder_proach amod_decoder_forward det_decoder_the prep_of_that_decoder prep_than_better_that cop_better_was nsubj_better_score prep_for_better_percent mark_better_that amod_decoder_bidirectional det_decoder_the prep_for_score_decoder nn_score_BLEU det_score_the nn_pairs_language det_pairs_the prep_of_percent_pairs num_percent_98.9 ccomp_means_better csubj_means_correspond amod_column_first conj_and_row_column amod_row_first det_row_the prep_in_figure_column prep_in_figure_row nn_figure_example prep_for_true_figure cop_true_was nsubj_true_condition prep_in_true_which det_column_the prep_of_head_column det_head_the prep_at_condition_head det_condition_the rcmod_experiments_true prep_of_percentage_experiments det_percentage_the prep_to_correspond_percentage nsubj_correspond_Numbers det_table_the prep_in_Numbers_table
D09-1117	J93-2003	o	Their experiments were performed using a decoder based on IBM Model 4 using the translation techniques developed at IBM -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_IBM_Brown prep_at_developed_IBM dep_developed_using nn_techniques_translation det_techniques_the dobj_using_techniques vmod_Model_using num_Model_4 nn_Model_IBM prep_on_based_Model vmod_decoder_based det_decoder_a dobj_using_decoder xcomp_performed_developed auxpass_performed_were nsubjpass_performed_experiments poss_experiments_Their
D09-1136	J93-2003	o	Becausesuchapproachesdirectly learn a generative model over phrase pairs they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models -LRB- Brown et al. 1993 -RRB- or the Hidden Markov Model -LRB- HMM -RRB- -LRB- Vogel et al. 1996 -RRB-	amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et appos_Model_HMM nn_Model_Markov nn_Model_Hidden det_Model_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_series nn_models_IBM det_models_the agent_produced_models vmod_alignments_produced amod_alignments_word-level nn_alignments_many-to-one det_alignments_the nn_pairs_phrase det_pairs_the prep_from_extracting_alignments dobj_extracting_pairs amod_heuristics_standard det_heuristics_the conj_or_preferable_Model dep_preferable_Brown prepc_for_preferable_extracting prep_to_preferable_heuristics advmod_preferable_theoretically cop_preferable_are nsubj_preferable_they nn_pairs_phrase prep_over_model_pairs amod_model_generative det_model_a dep_learn_Vogel parataxis_learn_Model parataxis_learn_preferable dobj_learn_model advmod_learn_Becausesuchapproachesdirectly
D09-1141	J93-2003	o	We then built separate directed word alignments for EnglishX andXEnglish -LRB- X -LCB- Indonesian Spanish -RCB- -RRB- using IBM model 4 -LRB- Brown et al. 1993 -RRB- combined them using the intersect + grow heuristic -LRB- Och and Ney 2003 -RRB- and extracted phrase-level translation pairs of maximum length seven using the alignment template approach -LRB- Och and Ney 2004 -RRB-	amod_Och_2004 conj_and_Och_Ney dep_approach_Ney dep_approach_Och nn_approach_template nn_approach_alignment det_approach_the dobj_using_approach num_length_seven nn_length_maximum vmod_pairs_using prep_of_pairs_length nn_pairs_translation amod_pairs_phrase-level amod_pairs_extracted conj_and_Och_2003 conj_and_Och_Ney nn_Och_heuristic dep_intersect_2003 dep_intersect_Ney dep_intersect_Och conj_+_intersect_grow det_intersect_the dobj_using_grow dobj_using_intersect conj_and_combined_pairs xcomp_combined_using dobj_combined_them csubj_combined_using dep_combined_X amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_IBM dep_using_Brown dobj_using_model dep_Indonesian_Spanish dep_X_Indonesian dep_andXEnglish_pairs dep_andXEnglish_combined nn_andXEnglish_EnglishX prep_for_alignments_andXEnglish nn_alignments_word amod_alignments_directed amod_alignments_separate dobj_built_alignments advmod_built_then nsubj_built_We ccomp_``_built
E06-1004	J93-2003	o	Increasingly parallel corpora are becoming available for many language pairs and SMT systems have been built for French-English German-English Arabic-English Chinese-English Hindi-English and other language pairs -LRB- Brown et al. 1993 -RRB- -LRB- AlOnaizan et al. 1999 -RRB- -LRB- Udupa 2004 -RRB-	amod_Udupa_2004 amod_AlOnaizan_1999 dep_AlOnaizan_al. nn_AlOnaizan_et amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_pairs_language amod_pairs_other amod_pairs_Hindi-English amod_pairs_Chinese-English amod_pairs_Arabic-English amod_pairs_German-English amod_pairs_French-English conj_and_French-English_other conj_and_French-English_Hindi-English conj_and_French-English_Chinese-English conj_and_French-English_Arabic-English conj_and_French-English_German-English dep_built_Udupa dep_built_AlOnaizan dep_built_Brown prep_for_built_pairs auxpass_built_been aux_built_have nn_systems_SMT conj_and_pairs_systems nn_pairs_language amod_pairs_many prep_for_available_systems prep_for_available_pairs dep_becoming_built acomp_becoming_available aux_becoming_are nsubj_becoming_corpora advmod_becoming_Increasingly amod_corpora_parallel
E06-1004	J93-2003	p	In the classic work on SMT Brownandhiscolleagues atIBMintroduced the notion of alignment between a sentence f and its translation e and used it in the development of translation models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation prep_of_development_models det_development_the dep_used_Brown prep_in_used_development dobj_used_it nsubj_used_Brownandhiscolleagues nn_e_translation nn_e_f poss_translation_its conj_and_f_translation advmod_sentence_e det_sentence_a prep_between_alignment_sentence prep_of_notion_alignment det_notion_the conj_and_atIBMintroduced_used dobj_atIBMintroduced_notion nsubj_atIBMintroduced_Brownandhiscolleagues prep_in_atIBMintroduced_work prep_on_work_SMT amod_work_classic det_work_the
E06-1004	J93-2003	o	An open question in SMT is whether there existsclosed formexpressions -LRB- whoserepresentation is polynomial in the size of the input -RRB- for P -LRB- f | e -RRB- and the counts in the EM iterations for models 3-5 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_models_3-5 prep_for_iterations_models nn_iterations_EM det_iterations_the det_counts_the dep_|_Brown prep_in_|_iterations conj_and_|_counts dep_|_e nn_|_f dep_P_counts dep_P_| det_input_the prep_of_size_input det_size_the prep_for_polynomial_P prep_in_polynomial_size cop_polynomial_is nsubj_polynomial_whoserepresentation dep_polynomial_formexpressions advcl_polynomial_existsclosed expl_existsclosed_there mark_existsclosed_whether xcomp_is_polynomial nsubj_is_question prep_in_question_SMT amod_question_open det_question_An
E06-1004	J93-2003	o	For a detailed introduction to IBM translation models please see -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_see_Brown aux_see_please prep_for_see_introduction nn_models_translation nn_models_IBM prep_to_introduction_models amod_introduction_detailed det_introduction_a
E06-1004	J93-2003	o	Expectation Evaluation is the soul of parameter estimation -LRB- Brown et al. 1993 -RRB- -LRB- Al-Onaizan et al. 1999 -RRB-	amod_Al-Onaizan_1999 dep_Al-Onaizan_al. nn_Al-Onaizan_et nn_al._et dep_Brown_1993 advmod_Brown_al. nn_estimation_parameter appos_soul_Al-Onaizan dep_soul_Brown prep_of_soul_estimation det_soul_the cop_soul_is nsubj_soul_Evaluation nn_Evaluation_Expectation
E06-1004	J93-2003	o	Exact Decoding is the original decoding problem as defined in -LRB- Brown et al. 1993 -RRB- and Relaxed Decoding is the relaxation of the decoding problem typically used in practice	prep_in_used_practice advmod_used_typically vmod_problem_used nn_problem_decoding det_problem_the prep_of_relaxation_problem det_relaxation_the cop_relaxation_is nsubj_relaxation_Decoding nsubj_relaxation_al. mark_relaxation_in amod_Decoding_Relaxed conj_and_al._Decoding num_al._1993 nn_al._et amod_al._Brown dep_defined_relaxation mark_defined_as advcl_problem_defined nn_problem_decoding amod_problem_original det_problem_the cop_problem_is nsubj_problem_Decoding amod_Decoding_Exact
E06-1004	J93-2003	p	In their seminal paper on SMT Brownand his colleagues highlighted the problems weface aswe go from IBM Models 1-2 to 3-5 -LRB- Brown et al. 1993 -RRB- 3 Asweprogress from Model1toModel5 evaluating the expectations that gives us counts becomes increasingly difficult	advmod_difficult_increasingly acomp_becomes_difficult dep_counts_becomes dobj_gives_us nsubj_gives_that det_expectations_the dep_evaluating_counts rcmod_evaluating_gives dobj_evaluating_expectations vmod_Asweprogress_evaluating prep_from_Asweprogress_Model1toModel5 dep_Brown_Asweprogress num_Brown_3 amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_3-5_to number_3-5_1-2 num_Models_3-5 nn_Models_IBM prep_from_go_Models nsubj_go_aswe aux_go_weface dep_problems_go det_problems_the dep_highlighted_Brown dobj_highlighted_problems nsubj_highlighted_colleagues prep_in_highlighted_paper poss_colleagues_his nn_colleagues_Brownand prep_on_paper_SMT amod_paper_seminal poss_paper_their
E06-1004	J93-2003	o	1 Introduction Statistical Machine Translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic translation -LRB- Brown et al. 1993 -RRB- -LRB- Al-Onaizan et al. 1999 -RRB-	amod_Al-Onaizan_1999 dep_Al-Onaizan_al. nn_Al-Onaizan_et dep_al._1993 nn_al._et amod_al._Brown amod_translation_automatic amod_language_natural prep_of_models_language amod_models_probabilistic dep_uses_al. prep_for_uses_translation dobj_uses_models nsubj_uses_which appos_technique_Al-Onaizan rcmod_technique_uses nn_technique_translation nn_technique_machine amod_technique_driven nn_technique_data det_technique_a cop_technique_is nsubj_technique_Translation nn_Translation_Machine amod_Translation_Statistical nn_Translation_Introduction num_Translation_1
E06-1004	J93-2003	o	The parameters of the models are estimated by iterative maximum-likelihood training on a large parallel corpus of natural language texts using the EM algorithm -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_algorithm_al. nn_algorithm_EM det_algorithm_the dobj_using_algorithm nn_texts_language amod_texts_natural prep_of_corpus_texts nn_corpus_parallel amod_corpus_large det_corpus_a amod_training_maximum-likelihood amod_training_iterative xcomp_estimated_using prep_on_estimated_corpus agent_estimated_training auxpass_estimated_are nsubjpass_estimated_parameters det_models_the prep_of_parameters_models det_parameters_The ccomp_``_estimated
E06-1005	J93-2003	o	We use the IBM Model 1 -LRB- Brown et al. 1993 -RRB- -LRB- uniform distribution -RRB- and the Hidden Markov Model -LRB- HMM first-order dependency -LRB- Vogel et al. 1996 -RRB- -RRB- to estimate the alignment model	nn_model_alignment det_model_the dobj_estimate_model aux_estimate_to amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et amod_dependency_first-order appos_HMM_Vogel appos_HMM_dependency vmod_Model_estimate dep_Model_HMM nn_Model_Markov nn_Model_Hidden det_Model_the amod_distribution_uniform dep_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_distribution dep_Model_Brown num_Model_1 nn_Model_IBM det_Model_the conj_and_use_Model dobj_use_Model nsubj_use_We
E06-1019	J93-2003	o	Alignment spaces can emerge from generative stories -LRB- Brown et al. 1993 -RRB- from syntactic notions -LRB- Wu 1997 -RRB- or they can be imposed to create competition between links -LRB- Melamed 2000 -RRB-	amod_Melamed_2000 dep_links_Melamed prep_between_competition_links dobj_create_competition aux_create_to xcomp_imposed_create auxpass_imposed_be aux_imposed_can nsubjpass_imposed_they dep_Wu_1997 appos_notions_Wu amod_notions_syntactic conj_or_from_imposed pobj_from_notions amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_stories_generative dep_emerge_imposed dep_emerge_from dep_emerge_Brown prep_from_emerge_stories aux_emerge_can nsubj_emerge_spaces nn_spaces_Alignment
E06-1019	J93-2003	o	The IBM models -LRB- Brown et al. 1993 -RRB- search a version of permutation space with a one-to-many constraint	amod_constraint_one-to-many det_constraint_a nn_space_permutation prep_with_version_constraint prep_of_version_space det_version_a dobj_search_version dep_search_Brown nsubj_search_models amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_The
E06-1019	J93-2003	o	The task originally emerged as an intermediate result of training the IBM translation models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation nn_models_IBM det_models_the dobj_training_models prepc_of_result_training amod_result_intermediate det_result_an dep_emerged_Brown prep_as_emerged_result advmod_emerged_originally nsubj_emerged_task det_task_The ccomp_``_emerged
E06-1020	J93-2003	p	The implementation of MEBA was strongly influenced by the notorious five IBM models described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in vmod_models_described nn_models_IBM num_models_five amod_models_notorious det_models_the agent_influenced_models advmod_influenced_strongly auxpass_influenced_was nsubjpass_influenced_implementation prep_of_implementation_MEBA det_implementation_The
E06-1046	J93-2003	o	First a parsing-based approach attempts to recover partial parses from the parse chart when the input can not be parsed in its entirety due to noise in order to construct a -LRB- partial -RRB- semantic representation -LRB- Dowding et al. 1993 Allen et al. 2001 Ward 1991 -RRB-	num_Ward_1991 dep_Allen_Ward num_Allen_2001 nn_Allen_al. nn_Allen_et conj_al._Allen conj_al._1993 nn_al._et dobj_Dowding_al. amod_representation_semantic dep_representation_partial det_representation_a dep_construct_Dowding dobj_construct_representation aux_construct_to dep_construct_order mark_construct_in prep_due_to_entirety_noise poss_entirety_its prep_in_parsed_entirety auxpass_parsed_be neg_parsed_not aux_parsed_can nsubjpass_parsed_input advmod_parsed_when det_input_the nn_chart_parse det_chart_the amod_parses_partial advcl_recover_parsed prep_from_recover_chart dobj_recover_parses aux_recover_to dep_attempts_construct vmod_attempts_recover nn_attempts_approach amod_attempts_parsing-based det_attempts_a dep_First_attempts ccomp_``_First
E06-1046	J93-2003	o	To this end we adopt techniques from statistical machine translation -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- and use statistical alignment to learn the edit patterns	nn_patterns_edit det_patterns_the dobj_learn_patterns aux_learn_to amod_alignment_statistical vmod_use_learn dobj_use_alignment nsubj_use_we num_Och_2003 conj_and_Och_Ney dep_al._Ney dep_al._Och num_al._1993 nn_al._et amod_al._Brown dep_translation_al. nn_translation_machine amod_translation_statistical conj_and_adopt_use prep_from_adopt_translation dobj_adopt_techniques nsubj_adopt_we prep_to_adopt_end det_end_this
E06-1046	J93-2003	o	We adopt an approach similar to -LRB- Ciaramella 1993 Boros et al. 1996 -RRB- in which the meaning representation in our case XML is transformed into a sorted flat list of attribute-value pairs indicating the core contentful concepts of each command	det_command_each prep_of_concepts_command amod_concepts_contentful nn_concepts_core det_concepts_the dobj_indicating_concepts amod_pairs_attribute-value prep_of_list_pairs amod_list_flat amod_list_sorted det_list_a xcomp_transformed_indicating prep_into_transformed_list auxpass_transformed_is nsubjpass_transformed_representation prep_in_transformed_which dep_case_XML poss_case_our prep_in_representation_case nn_representation_meaning det_representation_the num_Boros_1996 nn_Boros_al. nn_Boros_et dep_Ciaramella_Boros dep_Ciaramella_1993 rcmod_to_transformed dep_to_Ciaramella prep_similar_to amod_approach_similar det_approach_an dobj_adopt_approach nsubj_adopt_We
E06-2002	J93-2003	o	By introducing the hidden word alignment variable a the following approximate optimization criterion can be applied for that purpose e = argmaxe Pr -LRB- e | f -RRB- = argmaxe summationdisplay a Pr -LRB- e a | f -RRB- argmaxe a Pr -LRB- e a | f -RRB- Exploiting the maximum entropy -LRB- Berger et al. 1996 -RRB- framework the conditional distribution Pr -LRB- e a | f -RRB- can be determined through suitable real valued functions -LRB- called features -RRB- hr -LRB- e f a -RRB- r = 1R and takes the parametric form p -LRB- e a | f -RRB- exp Rsummationdisplay r = 1 rhr -LRB- e f a -RRB- -RCB- The ITC-irst system -LRB- Chen et al. 2005 -RRB- is based on a log-linear model which extends the original IBM Model 4 -LRB- Brown et al. 1993 -RRB- to phrases -LRB- Koehn et al. 2003 Federico and Bertoldi 2005 -RRB-	amod_Federico_2005 conj_and_Federico_Bertoldi dep_Koehn_Bertoldi dep_Koehn_Federico appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Koehn prep_to_Model_phrases dep_Model_Brown num_Model_4 dep_IBM_Model dep_original_IBM amod_the_original dobj_extends_the nsubj_extends_which rcmod_model_extends amod_model_log-linear det_model_a amod_Chen_2005 dep_Chen_al. nn_Chen_et dep_system_Chen amod_system_ITC-irst det_system_The dep_system_e nn_system_p dep_e_a dep_e_f num_rhr_1 dep_=_rhr dep_r_e amod_r_= nn_r_Rsummationdisplay dep_exp_r dep_exp_f dep_|_exp det_|_a appos_e_| amod_form_parametric det_form_the nsubj_takes_system dobj_takes_form dobj_=_1R npadvmod_=_r appos_e_a appos_e_f appos_hr_e prep_based_on_called_model auxpass_called_is nsubjpass_called_system conj_and_called_takes dep_called_= dep_called_hr dep_called_features dep_functions_takes dep_functions_called amod_functions_valued amod_functions_real amod_functions_suitable prep_through_determined_functions auxpass_determined_be aux_determined_can nsubjpass_determined_| dep_determined_e dep_|_f det_|_a vmod_Pr_determined nn_Pr_distribution amod_Pr_conditional det_Pr_the appos_framework_Pr dep_framework_Berger dep_framework_entropy dep_framework_Exploiting dep_framework_| dep_framework_e dep_Berger_1996 dep_Berger_al. nn_Berger_et nn_entropy_maximum det_entropy_the dep_Exploiting_f det_|_a dep_Pr_framework det_Pr_a appos_argmaxe_Pr dep_argmaxe_f dep_|_argmaxe det_|_a dep_e_| appos_Pr_e det_Pr_a nn_summationdisplay_argmaxe dobj_=_Pr iobj_=_summationdisplay dep_=_f dep_f_| dep_|_e dep_Pr_= amod_Pr_argmaxe dep_=_Pr dep_=_e prepc_by_=_introducing det_purpose_that prep_for_applied_purpose auxpass_applied_be aux_applied_can nsubjpass_applied_criterion dep_applied_a nn_criterion_optimization amod_criterion_approximate amod_criterion_following det_criterion_the dep_variable_applied nn_variable_alignment nn_variable_word amod_variable_hidden det_variable_the dobj_introducing_variable
E09-1003	J93-2003	o	This approach is usually referred to as the noisy sourcechannel approach in SMT -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_SMT_Brown prep_in_approach_SMT nn_approach_sourcechannel amod_approach_noisy det_approach_the pobj_referred_approach prepc_as_to_referred_as advmod_referred_usually auxpass_referred_is nsubjpass_referred_approach det_approach_This
E09-1018	J93-2003	p	While EM has worked quite well for a few tasks notably machine translations -LRB- starting with the IBM models 1-5 -LRB- Brown et al. 1993 -RRB- it has not had success in most others such as part-of-speech tagging -LRB- Merialdo 1991 -RRB- named-entity recognition -LRB- Collins and Singer 1999 -RRB- and context-free-grammar induction -LRB- numerous attempts too many to mention -RRB-	aux_mention_to xcomp_many_mention advmod_many_too amod_attempts_many amod_attempts_numerous amod_induction_context-free-grammar amod_Collins_1999 conj_and_Collins_Singer dep_recognition_Singer dep_recognition_Collins nn_recognition_named-entity amod_Merialdo_1991 dep_tagging_attempts conj_and_tagging_induction conj_and_tagging_recognition dep_tagging_Merialdo amod_tagging_part-of-speech amod_others_most prep_such_as_had_induction prep_such_as_had_recognition prep_such_as_had_tagging prep_in_had_others dobj_had_success neg_had_not aux_had_has nsubj_had_it amod_Brown_1993 dep_Brown_al. nn_Brown_et num_models_1-5 nn_models_IBM det_models_the parataxis_starting_had dep_starting_Brown prep_with_starting_models dep_translations_starting nn_translations_machine advmod_translations_notably appos_tasks_translations amod_tasks_few det_tasks_a advmod_well_quite prep_for_worked_tasks advmod_worked_well aux_worked_has nsubj_worked_EM mark_worked_While advcl_``_worked
E09-1033	J93-2003	o	287 System Train + base Test + base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 -LRB- 5 trials/fold -RRB- 3 Contrastive 88.82 0.93 88.55 0.66 -LRB- greedy selection -RRB- Table 1 Average F1 of 7-way cross-validation To generate the alignments we used Model 4 -LRB- Brown et al. 1993 -RRB- as implemented in GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ prep_in_implemented_+ prep_in_implemented_GIZA mark_implemented_as amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_4 advcl_used_implemented dobj_used_Model nsubj_used_we nsubj_used_base nsubj_used_Test nsubj_used_Train det_alignments_the dobj_generate_alignments aux_generate_To amod_cross-validation_7-way vmod_F1_generate prep_of_F1_cross-validation amod_F1_Average num_Table_1 dep_Table_0.66 amod_selection_greedy appos_0.66_selection dep_0.66_88.55 dep_88.55_0.93 number_0.93_88.82 dep_Contrastive_F1 dep_Contrastive_Table dep_3_Contrastive num_trials/fold_5 dep_0.56_3 dep_0.56_trials/fold number_0.56_88.45 dep_0.56_0.82 number_0.82_88.70 dep_Contrastive_0.56 amod_2_Contrastive dep_87.89_2 number_87.89_87.89 amod_Baseline_87.89 num_Baseline_1 dep_base_Baseline nn_Test_base conj_+_Train_base conj_+_Train_Test nn_Train_System num_Train_287
E09-1033	J93-2003	o	Our test set is 3718 sentences from the English Penn treebank -LRB- Marcus et al. 1993 -RRB- which were translated into German	prep_into_translated_German auxpass_translated_were nsubjpass_translated_which rcmod_Marcus_translated amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_treebank_Penn nn_treebank_English det_treebank_the dep_sentences_Marcus prep_from_sentences_treebank num_sentences_3718 cop_sentences_is nsubj_sentences_set nn_set_test poss_set_Our
E09-1050	J93-2003	o	According to this model when translating a stringf in the source language into the target language a string e is chosen out of all target language strings e if it has the maximal probability given f -LRB- Brown et al. 1993 -RRB- e = arg maxe -LCB- Pr -LRB- e | f -RRB- -RCB- = arg maxe -LCB- Pr -LRB- f | e -RRB- Pr -LRB- e -RRB- -RCB- where Pr -LRB- f | e -RRB- is the translation model and Pr -LRB- e -RRB- is the target language model	nn_model_language nn_model_target det_model_the cop_model_is nsubj_model_Pr nsubj_model_model dep_model_e nn_model_Pr dep_model_e dep_model_| dep_model_Pr nn_model_maxe appos_Pr_e conj_and_model_Pr nn_model_translation det_model_the cop_model_is nsubj_model_Pr advmod_model_where advmod_e_| nn_|_f dep_Pr_e nn_|_f nn_maxe_arg dep_=_model dep_f_| dep_|_e dep_Pr_f amod_maxe_= appos_maxe_Pr nn_maxe_arg dobj_=_maxe dep_=_e dep_=_e amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_given_f vmod_probability_given amod_probability_maximal det_probability_the dep_has_Brown dobj_has_probability nsubj_has_it mark_has_if advcl_e_has nn_strings_language nn_strings_target det_strings_all ccomp_chosen_= pobj_chosen_strings prepc_out_of_chosen_of auxpass_chosen_is nsubjpass_chosen_e rcmod_string_chosen det_string_a nn_language_target det_language_the nn_language_source det_language_the prep_in_stringf_language det_stringf_a dep_translating_string prep_into_translating_language dobj_translating_stringf advmod_translating_when advcl_,_translating det_model_this pobj_``_model prepc_according_to_``_to
E09-1050	J93-2003	o	The method uses a translation model based on IBM Model 1 -LRB- Brown et al. 1993 -RRB- in which translation candidates of a phrase are generated by combining translations and transliterations of the phrase components and matching the result against a large corpus	amod_corpus_large det_corpus_a det_result_the prep_against_matching_corpus dobj_matching_result nn_components_phrase det_components_the prep_of_translations_components conj_and_translations_transliterations dobj_combining_transliterations dobj_combining_translations agent_generated_combining auxpass_generated_are nsubjpass_generated_candidates prep_in_generated_which det_phrase_a prep_of_candidates_phrase nn_candidates_translation amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_Model_matching rcmod_Model_generated appos_Model_Brown num_Model_1 nn_Model_IBM prep_on_based_matching prep_on_based_Model vmod_model_based nn_model_translation det_model_a dobj_uses_model nsubj_uses_method det_method_The
E09-1056	J93-2003	o	One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques -LRB- Brown et al. 1993 -RRB- to mine new translations	amod_translations_new dobj_mine_translations aux_mine_to amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_techniques_Brown nn_techniques_alignment amod_techniques_standard nn_corpus_parallel amod_corpus_domain-specific det_corpus_a vmod_using_mine prep_with_using_techniques dobj_using_corpus prepc_in_consists_using nsubj_consists_approach dobj_translate_terms aux_translate_to vmod_approach_translate num_approach_One
E09-1061	J93-2003	o	Alignment is often used in training both generative and discriminative models -LRB- Brown et al. 1993 Blunsom et al. 2008 Liang et al. 2006 -RRB-	nn_al._et nn_al._Liang nn_al._et nn_al._Blunsom amod_Brown_2006 dep_Brown_al. amod_Brown_2008 dep_Brown_al. amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_models_discriminative amod_models_generative nn_models_training conj_and_generative_discriminative preconj_generative_both dep_used_Brown prep_in_used_models advmod_used_often auxpass_used_is nsubjpass_used_Alignment
E09-1061	J93-2003	o	Each item is associated with a stack whose signa12Specifically a B-hypergraph equivalent to an and-or graph -LRB- Gallo et al. 1993 -RRB- or context-free grammar -LRB- Nederhof 2003 -RRB-	amod_Nederhof_2003 amod_grammar_context-free amod_Gallo_1993 dep_Gallo_al. nn_Gallo_et amod_graph_and-or det_graph_an dep_equivalent_Nederhof conj_or_equivalent_grammar dep_equivalent_Gallo prep_to_equivalent_graph amod_equivalent_B-hypergraph det_equivalent_a advmod_equivalent_signa12Specifically amod_whose_grammar amod_whose_equivalent dep_stack_whose det_stack_a prep_with_associated_stack auxpass_associated_is nsubjpass_associated_item det_item_Each ccomp_``_associated
E09-1061	J93-2003	o	Logics for the IBM Models -LRB- Brown et al. 1993 -RRB- would be similar to our logics for phrase-based models	amod_models_phrase-based poss_logics_our prep_for_similar_models prep_to_similar_logics cop_similar_be aux_similar_would nsubj_similar_Logics amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_Models_IBM det_Models_the dep_Logics_Brown prep_for_Logics_Models
E99-1010	J93-2003	o	To model p -LRB- fJle ~ 8 T -RRB- we assume the existence of an alignment a J We assume that every word fj is produced by the word e ~ j at position aj in the training corpus with the probability P -LRB- f ~ le ~ i -RRB- J p -LRB- f lc ' -RRB- = 1 \ -RSB- p -LRB- L Icon -RRB- j = l -LRB- 7 -RRB- The word alignment a J is trained automatically using statistical translation models as described in -LRB- Brown et al. 1993 Vogel et al. 1996 -RRB-	num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_al._Vogel num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_as nn_models_translation amod_models_statistical advcl_using_described dobj_using_models xcomp_trained_using advmod_trained_automatically auxpass_trained_is nsubjpass_trained_J det_J_a rcmod_alignment_trained nn_alignment_word det_alignment_The nn_alignment_l appos_l_7 dep_=_alignment amod_j_= nn_j_p nn_Icon_L appos_p_Icon num_\_1 dep_=_\ nn_lc_f dep_p_j dep_p_= dep_p_lc nn_p_J nn_i_~ nn_le_~ dep_f_i dep_f_le appos_P_f nn_P_probability det_P_the nn_corpus_training det_corpus_the prep_in_aj_corpus nn_aj_position prep_with_j_P prep_at_j_aj nn_j_~ dep_j_e dep_word_j det_word_the agent_produced_word auxpass_produced_is nsubjpass_produced_fj mark_produced_that nn_fj_word det_fj_every ccomp_assume_produced nsubj_assume_We dep_assume_J det_J_a dep_alignment_assume det_alignment_an dep_existence_p prep_of_existence_alignment det_existence_the dobj_assume_existence nsubj_assume_we dep_assume_8 dep_8_T nn_~_fJle nn_~_p parataxis_model_assume dobj_model_~ aux_model_To
E99-1010	J93-2003	o	Various clustering techniques have been proposed -LRB- Brown et al. 1992 Jardino and Adda 1993 Martin et al. 1998 -RRB- which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms	nn_algorithms_clustering amod_algorithms_iterative amod_criterion_maximum-likelihood det_criterion_a prep_with_optimizing_algorithms dobj_optimizing_criterion vmod_clustering_optimizing nn_clustering_word amod_clustering_automatic dobj_perform_clustering nsubj_perform_which num_Martin_1998 nn_Martin_al. nn_Martin_et num_Jardino_1993 conj_and_Jardino_Adda dep_Brown_Martin dep_Brown_Adda dep_Brown_Jardino amod_Brown_1992 dep_Brown_al. nn_Brown_et ccomp_proposed_perform dep_proposed_Brown auxpass_proposed_been aux_proposed_have nsubjpass_proposed_techniques nn_techniques_clustering amod_techniques_Various
H05-1012	J93-2003	p	The IBM models 1-5 -LRB- Brown et al. 1993 -RRB- produce word alignments with increasing algorithmic complexity and performance	conj_and_complexity_performance amod_complexity_algorithmic amod_complexity_increasing nn_alignments_word prep_with_produce_performance prep_with_produce_complexity dobj_produce_alignments nsubj_produce_1-5 amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_1-5_Brown rcmod_models_produce nn_models_IBM det_models_The dep_``_models
H05-1021	J93-2003	o	The 1000-best lists are augmented with IBM Model-1 -LRB- Brown et al. 1993 -RRB- scores and then rescored with a second set of MET parameters	nn_parameters_MET prep_of_set_parameters amod_set_second det_set_a prep_with_rescored_set advmod_rescored_then nsubj_rescored_lists amod_scores_Model-1 dep_al._1993 nn_al._et amod_al._Brown dep_Model-1_al. nn_Model-1_IBM conj_and_augmented_rescored prep_with_augmented_scores cop_augmented_are nsubj_augmented_lists amod_lists_1000-best det_lists_The
H05-1021	J93-2003	o	The IBM translation models -LRB- Brown et al. 1993 -RRB- describe word reordering via a distortion model defined over word positions within sentence pairs	nn_pairs_sentence prep_within_positions_pairs nn_positions_word prep_over_defined_positions vmod_model_defined nn_model_distortion det_model_a nn_reordering_word prep_via_describe_model dobj_describe_reordering nsubj_describe_models amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_translation nn_models_IBM det_models_The
H05-1021	J93-2003	o	2 The WFST Reordering Model The Translation Template Model -LRB- TTM -RRB- is a generative model of phrase-based translation -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown amod_translation_phrase-based prep_of_model_translation amod_model_generative det_model_a cop_model_is nsubj_model_Model appos_Model_TTM nn_Model_Template nn_Model_Translation det_Model_The dep_Model_al. rcmod_Model_model nn_Model_Reordering nn_Model_WFST det_Model_The pobj_2_Model ccomp_``_2
H05-1023	J93-2003	o	Word alignments traditionally are based on IBM Models 1-5 -LRB- Brown et al. 1993 -RRB- or on HMMs -LRB- Vogel et al. 1996 -RRB-	amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et pobj_on_HMMs amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Models_1-5 nn_Models_IBM dep_based_Vogel conj_or_based_on dep_based_Brown prep_on_based_Models auxpass_based_are advmod_based_traditionally nsubjpass_based_alignments nn_alignments_Word
H05-1024	J93-2003	n	2 Related Work One of the major problems with the IBM models -LRB- Brown et al. 1993 -RRB- and the HMM models -LRB- Vogel et al. 1996 -RRB- is that they are restricted to the alignment of each source-language word to at most one targetlanguage word	nn_word_targetlanguage num_word_one amod_word_most pobj_at_word pcomp_to_at amod_word_source-language det_word_each prep_of_alignment_word det_alignment_the prep_restricted_to prep_to_restricted_alignment cop_restricted_are nsubj_restricted_they mark_restricted_that ccomp_is_restricted nsubj_is_Work amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et nn_models_HMM det_models_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the prep_with_problems_models amod_problems_major det_problems_the dep_One_Vogel conj_and_One_models dep_One_Brown prep_of_One_problems dep_Work_models dep_Work_One amod_Work_Related num_Work_2 ccomp_``_is
H05-1057	J93-2003	o	There are five different IBM translation models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_translation nn_models_IBM amod_models_different num_models_five nsubj_are_models expl_are_There ccomp_``_are
H05-1057	J93-2003	o	Further details are in the original paper -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. amod_paper_original det_paper_the dep_are_Brown prep_in_are_paper nsubj_are_details amod_details_Further
H05-1057	J93-2003	p	The IBM models have shown good performance in machine translation and especially so within certain families of languages for example in translating between French and English or between Sinhalese and Tamil -LRB- Brown et al. 1993 Weerasinghe 2004 -RRB-	amod_Weerasinghe_2004 dep_Brown_Weerasinghe amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_Sinhalese_Tamil conj_or_French_Tamil conj_or_French_Sinhalese conj_and_French_English prep_between_translating_Sinhalese prep_between_translating_English prep_between_translating_French dep_for_Brown prepc_in_for_translating pobj_for_example ccomp_,_for prep_of_families_languages amod_families_certain prep_within_so_families advmod_so_especially nn_translation_machine prep_in_performance_translation amod_performance_good conj_and_shown_so dobj_shown_performance aux_shown_have nsubj_shown_models nn_models_IBM det_models_The
H05-1057	J93-2003	p	This is a common technique in machine translation for which the IBM translation models are popular methods -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_methods_popular cop_methods_are nsubj_methods_models prep_for_methods_which nn_models_translation nn_models_IBM det_models_the rcmod_translation_methods nn_translation_machine dep_technique_Brown prep_in_technique_translation amod_technique_common det_technique_a cop_technique_is nsubj_technique_This
H05-1057	J93-2003	o	In the first of our methods we align manual transcripts and ASR sentences using the IBM translation model -LRB- Brown et al. 1993 -RRB- to obtain a probabilistic dictionary	amod_dictionary_probabilistic det_dictionary_a dobj_obtain_dictionary aux_obtain_to num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation nn_model_IBM det_model_the vmod_using_obtain dobj_using_model nn_sentences_ASR vmod_transcripts_using conj_and_transcripts_sentences amod_transcripts_manual dobj_align_sentences dobj_align_transcripts nsubj_align_we prep_in_align_first poss_methods_our prep_of_first_methods det_first_the ccomp_``_align
H05-1057	J93-2003	o	3.2 Details To learn alignments translation probabilities etc in the first method we used work that has been done in statistical machine translation -LRB- Brown et al. 1993 -RRB- where the translation process is considered to be equivalent to a corruption of the source language text to the target language text due to a noisy channel	amod_channel_noisy det_channel_a nn_text_language nn_text_target det_text_the nn_text_language nn_text_source det_text_the prep_due_to_corruption_channel prep_to_corruption_text prep_of_corruption_text det_corruption_a prep_to_equivalent_corruption cop_equivalent_be aux_equivalent_to xcomp_considered_equivalent auxpass_considered_is nsubjpass_considered_process advmod_considered_where nn_process_translation det_process_the dep_al._1993 nn_al._et amod_al._Brown rcmod_translation_considered dep_translation_al. nn_translation_machine amod_translation_statistical prep_in_done_translation auxpass_done_been aux_done_has nsubjpass_done_that rcmod_work_done dobj_used_work nsubj_used_we dep_used_etc amod_method_first det_method_the prep_in_etc_method conj_probabilities_used nn_probabilities_translation dobj_learn_alignments aux_learn_To dep_Details_probabilities vmod_Details_learn num_Details_3.2 dep_``_Details
H05-1061	J93-2003	p	One widely used model is the IBM model -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_IBM det_model_the cop_model_is nsubj_model_model amod_model_used num_model_One advmod_used_widely
H05-1095	J93-2003	o	757 hbps strong tendency to overestimate the probability of rare bi-phrases it is computed as in equation -LRB- 2 -RRB- except that bi-phrase probabilities are computed based on individual word translation probabilities somewhat as in IBM model 1 -LRB- Brown et al. 1993 -RRB- Pr -LRB- t | s -RRB- = 1 | s | | t | productdisplay tt summationdisplay ss Pr -LRB- t | s -RRB- The target language feature function htl this is based on a N-gram language model of the target language	nn_language_target det_language_the prep_of_model_language nn_model_language nn_model_N-gram det_model_a prep_based_on_htl_model auxpass_htl_is nsubjpass_htl_this nn_htl_function nn_htl_feature nn_htl_language nn_htl_target det_htl_The appos_s_htl num_s_| nn_s_t nn_s_Pr dobj_ss_s nsubj_ss_summationdisplay nn_summationdisplay_tt nn_summationdisplay_productdisplay nn_summationdisplay_| nn_summationdisplay_t num_summationdisplay_| ccomp_|_ss nsubj_|_s num_s_| num_s_1 amod_s_= nn_s_s num_s_| nn_s_t nn_s_Pr amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_IBM pobj_in_model pcomp_as_in advmod_as_somewhat nn_probabilities_translation nn_probabilities_word amod_probabilities_individual pobj_on_probabilities pcomp_based_on prep_computed_based auxpass_computed_are nsubjpass_computed_probabilities mark_computed_except nn_probabilities_bi-phrase det_probabilities_that appos_equation_2 pobj_in_equation pcomp_as_in parataxis_computed_| dep_computed_Brown prep_computed_as advcl_computed_computed prep_computed_as auxpass_computed_is nsubjpass_computed_it amod_bi-phrases_rare prep_of_probability_bi-phrases det_probability_the dobj_overestimate_probability aux_overestimate_to parataxis_tendency_computed vmod_tendency_overestimate amod_tendency_strong nn_tendency_hbps num_tendency_757
H05-1095	J93-2003	n	While in traditional word-based statistical models -LRB- Brown et al. 1993 -RRB- the atomic unit that translation operates on is the word phrase-based methods acknowledge the significant role played in language by multiword expressions thus incorporating in a statistical framework the insight behind Example-Based Machine Translation -LRB- Somers 1999 -RRB-	amod_Somers_1999 dep_Translation_Somers nn_Translation_Machine amod_Translation_Example-Based prep_behind_insight_Translation det_insight_the dep_framework_insight amod_framework_statistical det_framework_a prep_in_incorporating_framework advmod_incorporating_thus amod_expressions_multiword agent_played_expressions prep_in_played_language vmod_role_played amod_role_significant det_role_the vmod_acknowledge_incorporating dobj_acknowledge_role nsubj_acknowledge_methods advcl_acknowledge_word amod_methods_phrase-based det_word_the cop_word_is nsubj_word_unit dep_word_Brown prep_in_word_models mark_word_While prep_operates_on nsubj_operates_translation dobj_operates_that rcmod_unit_operates amod_unit_atomic det_unit_the amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_models_statistical amod_models_word-based amod_models_traditional
H05-1096	J93-2003	o	Therefore we determine the maximal translation probability of the target word e over the source sentence words pIBM1 -LRB- e | fJ1 -RRB- = maxj = 0 J p -LRB- e | fj -RRB- -LRB- 9 -RRB- where f0 is the empty source word -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. dep_word_Brown nn_word_source amod_word_empty det_word_the cop_word_is nsubj_word_f0 advmod_word_where nn_fj_| dep_e_fj appos_p_e nn_p_J dobj_=_0 nsubj_=_maxj dep_=_= dep_|_fJ1 dep_|_e rcmod_pIBM1_word appos_pIBM1_9 appos_pIBM1_p dep_pIBM1_= dep_pIBM1_| nn_words_sentence nn_words_source det_words_the prep_over_word_words dep_word_e nn_word_target det_word_the dep_probability_pIBM1 prep_of_probability_word nn_probability_translation amod_probability_maximal det_probability_the dobj_determine_probability nsubj_determine_we advmod_determine_Therefore
H05-1097	J93-2003	o	For example in the IBM Models -LRB- Brown et al. 1993 -RRB- each word ti independently generates 0 1 or more 2Note that we refer to t as the target sentence even though in the source-channel model t is the source sentence which goes through the channel model P -LRB- s | t -RRB- to produce the observed sentence s. words in the source language	nn_language_source det_language_the nn_words_s. nn_words_sentence amod_words_observed det_words_the prep_in_produce_language dobj_produce_words aux_produce_to dobj_|_t nsubj_|_s dep_P_| nn_P_model nn_P_channel det_P_the xcomp_goes_produce prep_through_goes_P nsubj_goes_which rcmod_sentence_goes nn_sentence_source det_sentence_the cop_sentence_is nsubj_sentence_t prep_in_sentence_model mark_sentence_though advmod_sentence_even amod_model_source-channel det_model_the nn_sentence_target det_sentence_the advcl_refer_sentence prep_as_refer_sentence prep_to_refer_t nsubj_refer_we mark_refer_that ccomp_2Note_refer num_2Note_more num_2Note_1 num_2Note_0 conj_or_0_more conj_or_0_1 dobj_generates_2Note advmod_generates_independently nsubj_generates_ti dep_generates_Brown prep_in_generates_Models prep_for_generates_example nn_ti_word det_ti_each amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_Models_IBM det_Models_the
I05-2012	J93-2003	p	It was initially proposed by -LRB- Brown et al. 1993 -RRB- and more recently have been intensively studied by several research groups -LRB- Germann et al. 2001 Och et al. 2003 -RRB-	num_Och_2003 nn_Och_al. nn_Och_et dep_Germann_Och appos_Germann_2001 dep_Germann_al. nn_Germann_et dep_groups_Germann nn_groups_research amod_groups_several agent_studied_groups advmod_studied_intensively auxpass_studied_been aux_studied_have nsubjpass_studied_al. mark_studied_by advmod_recently_more advmod_al._recently cc_al._and num_al._1993 nn_al._et amod_al._Brown advcl_proposed_studied advmod_proposed_initially auxpass_proposed_was nsubjpass_proposed_It ccomp_``_proposed
I05-2012	J93-2003	o	For the give source text S it finds the most probable alignment set A and target text T. = Aa SaTpSTp -RRB- | -LRB- -RRB- | -LRB- -LRB- 1 -RRB- Brown -LRB- Brown et al. 1993 -RRB- proposed five alignment models called IBM Model for an English-French alignment task based on equa68 tion -LRB- 1 -RRB-	appos_tion_1 nn_tion_equa68 prep_on_based_tion vmod_task_based nn_task_alignment amod_task_English-French det_task_an nn_Model_IBM dep_called_Model nn_models_alignment num_models_five prep_for_proposed_task vmod_proposed_called dobj_proposed_models nsubj_proposed_Brown dep_proposed_1 nn_al._et dep_Brown_1993 advmod_Brown_al. dep_Brown_Brown dep_|_proposed amod_|_= nn_|_T. nn_SaTpSTp_Aa dep_=_SaTpSTp nn_text_target appos_set_| conj_and_set_| conj_and_set_text appos_set_A nn_set_alignment amod_set_probable det_set_the advmod_probable_most dobj_finds_| dobj_finds_text dobj_finds_set nsubj_finds_it nn_text_source parataxis_give_finds dep_give_S dobj_give_text prep_for_give_the
I05-2012	J93-2003	o	-LRB- Chen et al. 1993 Gale et al. 1993 -RRB- proposed sentence alignment techniques based on dynamic programming using sentence length and lexical mapping information	nn_information_mapping amod_information_lexical conj_and_length_information nn_length_sentence dobj_using_information dobj_using_length amod_programming_dynamic prep_on_based_programming vmod_techniques_using vmod_techniques_based nn_techniques_alignment nn_techniques_sentence amod_techniques_proposed dep_techniques_Chen num_Gale_1993 nn_Gale_al. nn_Gale_et dep_Chen_Gale appos_Chen_1993 dep_Chen_al. nn_Chen_et
I05-2012	J93-2003	o	-LRB- Haruno et al. 1996 Kay et al. 1993 -RRB- applied iterative refinement algorithms to sentence level alignment tasks	nn_tasks_alignment nn_tasks_level nn_tasks_sentence nn_algorithms_refinement amod_algorithms_iterative prep_to_applied_tasks dobj_applied_algorithms nsubj_applied_Haruno num_Kay_1993 nn_Kay_al. nn_Kay_et dep_Haruno_Kay appos_Haruno_1996 dep_Haruno_al. nn_Haruno_et
I05-2012	J93-2003	o	In this paper we propose an alignment algorithm between English and Korean conceptual units -LRB- or between English and Korean term constituents -RRB- in English-Korean technical term pairs based on IBM Model -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown nn_Model_IBM prep_on_based_Model nn_pairs_term amod_pairs_technical amod_pairs_English-Korean nn_constituents_term amod_constituents_Korean amod_constituents_English amod_constituents_between cc_constituents_or conj_and_English_Korean prep_in_units_pairs dep_units_constituents amod_units_conceptual nn_units_Korean nn_units_English conj_and_English_Korean vmod_algorithm_based prep_between_algorithm_units nn_algorithm_alignment det_algorithm_an dobj_propose_algorithm nsubj_propose_we prep_in_propose_paper det_paper_this
I05-2014	J93-2003	o	2 Overview 2.1 The word segmentation problem As statistical machine translation systems basically rely on the notion of words through their lexicon models -LRB- BROWN et al. 1993 -RRB- they are usually capable of outputting sentences already segmented into words when they translate into languages like Chinese or Japanese	conj_or_Chinese_Japanese prep_like_languages_Japanese prep_like_languages_Chinese prep_into_translate_languages nsubj_translate_they advmod_translate_when advcl_segmented_translate prep_into_segmented_words advmod_segmented_already amod_sentences_segmented amod_sentences_outputting prep_of_capable_sentences advmod_capable_usually cop_capable_are nsubj_capable_they advcl_capable_rely amod_BROWN_1993 dep_BROWN_al. nn_BROWN_et nn_models_lexicon poss_models_their prep_of_notion_words det_notion_the dep_rely_BROWN prep_through_rely_models prep_on_rely_notion advmod_rely_basically nsubj_rely_systems mark_rely_As nn_systems_translation nn_systems_machine amod_systems_statistical rcmod_problem_capable nn_problem_segmentation nn_problem_word det_problem_The num_problem_2.1 nn_problem_Overview num_problem_2
I05-4010	J93-2003	o	Large volumes of training data of this kind are indispensable for constructing statistical translation models -LRB- Brown et al. 1993 Melamed 2000 -RRB- acquiring bilingual lexicon -LRB- Gale and Church 1991 Melamed 1997 -RRB- and building example-based machine translation -LRB- EBMT -RRB- systems -LRB- Nagao 1984 Carl and Way 2003 Way and Gough 2003 -RRB-	num_Way_2003 conj_and_Way_Gough num_Carl_2003 conj_and_Carl_Way dep_Nagao_Gough dep_Nagao_Way dep_Nagao_Way dep_Nagao_Carl appos_Nagao_1984 appos_systems_Nagao nn_systems_EBMT dep_translation_systems nn_translation_machine amod_translation_example-based nn_translation_building dep_Melamed_1997 conj_and_Gale_translation conj_and_Gale_Melamed conj_and_Gale_1991 conj_and_Gale_Church dep_lexicon_translation dep_lexicon_Melamed dep_lexicon_1991 dep_lexicon_Church dep_lexicon_Gale amod_lexicon_bilingual dobj_acquiring_lexicon dep_Melamed_2000 dep_Brown_Melamed amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_models_Brown nn_models_translation amod_models_statistical vmod_constructing_acquiring dobj_constructing_models prepc_for_indispensable_constructing cop_indispensable_are nsubj_indispensable_volumes det_kind_this prep_of_data_kind nn_data_training prep_of_volumes_data amod_volumes_Large
I05-5001	J93-2003	o	One promising approach extends standard Statistical Machine Translation -LRB- SMT -RRB- techniques -LRB- e.g. Brown et al. 1993 Och & Ney 2000 2003 -RRB- to the problems of monolingual paraphrase identification and generation	conj_and_identification_generation nn_identification_paraphrase amod_identification_monolingual prep_of_problems_generation prep_of_problems_identification det_problems_the num_Och_2003 num_Och_2000 conj_and_Och_Ney dep_al._Ney dep_al._Och dep_al._1993 nn_al._et nn_al._Brown nn_al._e.g. dep_techniques_al. nn_techniques_Translation appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical amod_Translation_standard prep_to_extends_problems dobj_extends_techniques nsubj_extends_approach amod_approach_promising num_approach_One ccomp_``_extends
I08-1013	J93-2003	o	4 Pattern switching The compositional translation presents problems which have been reported by -LRB- Baldwin and Tanaka 2004 Brown et al. 1993 -RRB- Fertility SWTs and MWTs are not translated by a term of a same length	amod_length_same det_length_a prep_of_term_length det_term_a agent_translated_term neg_translated_not auxpass_translated_are nsubjpass_translated_MWTs nsubjpass_translated_SWTs conj_and_SWTs_MWTs nn_SWTs_Fertility num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Baldwin_translated dep_Baldwin_Brown num_Baldwin_2004 conj_and_Baldwin_Tanaka agent_reported_Tanaka agent_reported_Baldwin auxpass_reported_been aux_reported_have nsubjpass_reported_which rcmod_problems_reported dobj_presents_problems nsubj_presents_translation amod_translation_compositional det_translation_The rcmod_switching_presents nn_switching_Pattern num_switching_4
I08-1024	J93-2003	o	These probabilities are estimated with IBM model 1 -LRB- Brown et al. 1993 -RRB- on parallel corpora	nn_corpora_parallel amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_IBM prep_on_estimated_corpora dep_estimated_Brown prep_with_estimated_model auxpass_estimated_are nsubjpass_estimated_probabilities det_probabilities_These ccomp_``_estimated
I08-1033	J93-2003	o	Most existing methods treat word tokens as basic alignment units -LRB- Brown et al. 1993 Vogel et al. 1996 Deng and Byrne 2005 -RRB- however many languages have no explicit word boundary markers such as Chinese and Japanese	conj_and_Chinese_Japanese prep_such_as_markers_Japanese prep_such_as_markers_Chinese nn_markers_boundary nn_markers_word amod_markers_explicit neg_markers_no dobj_have_markers nsubj_have_languages amod_languages_many num_Deng_2005 conj_and_Deng_Byrne num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_Brown_Byrne dep_Brown_Deng dep_Brown_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_units_alignment amod_units_basic nn_tokens_word parataxis_treat_have advmod_treat_however dep_treat_Brown prep_as_treat_units dobj_treat_tokens nsubj_treat_methods amod_methods_existing amod_methods_Most
I08-1068	J93-2003	o	5.1.2 Learning Translation Model According to the standard statistical translation model -LRB- Brown et al. 1993 -RRB- we can find the optimal model M by maximizing the probability of generating queries from documents or M = argmax M NY i = 1 P -LRB- QijDi M -RRB- 524 qw dw P -LRB- qwjdw u -RRB- journal kdd 0.0176 journal conference 0.0123 journal journal 0.0176 journal sigkdd 0.0088 journal discovery 0.0211 journal mining 0.0017 journal acm 0.0088 music music 0.0375 music purchase 0.0090 music mp3 0.0090 music listen 0.0180 music mp3.com 0.0450 music free 0.0008 Table 1 Sample user profile To find the optimal word translation probabilities P -LRB- qwjdw M -RRB- we can use the EM algorithm	nn_algorithm_EM det_algorithm_the dobj_use_algorithm aux_use_can nsubj_use_we nsubj_use_profile dep_qwjdw_M dep_P_qwjdw dep_probabilities_P nn_probabilities_translation nn_probabilities_word amod_probabilities_optimal det_probabilities_the dobj_find_probabilities aux_find_To vmod_profile_find nn_profile_user nn_profile_Sample num_Table_1 num_Table_0.0008 amod_Table_free dep_music_Table num_music_0.0450 nn_music_mp3.com nn_music_music num_music_0.0180 nsubj_listen_= dep_listen_i tmod_listen_M nsubj_listen_argmax dep_listen_= tmod_listen_M num_music_0.0090 nn_music_mp3 nn_music_music num_music_0.0090 nn_music_purchase nn_music_music num_music_0.0375 nn_music_music nn_music_music num_music_0.0088 nn_music_acm nn_music_journal num_music_0.0017 nn_music_mining nn_music_journal num_music_0.0211 nn_music_discovery nn_music_journal num_music_0.0088 nn_music_sigkdd nn_music_journal num_music_0.0176 nn_music_journal nn_music_journal num_music_0.0123 nn_music_conference nn_music_journal num_music_0.0176 nn_music_kdd nn_music_journal nn_music_P nn_music_P appos_qwjdw_u dep_P_qwjdw nn_P_dw nn_P_qw num_P_524 dep_QijDi_M dep_P_QijDi num_P_1 dobj_=_music nn_i_NY dep_documents_music conj_or_documents_listen prep_from_queries_listen prep_from_queries_documents dobj_generating_queries prepc_of_probability_generating det_probability_the dobj_maximizing_probability nn_M_model amod_M_optimal det_M_the parataxis_find_use prepc_by_find_maximizing dobj_find_M aux_find_can nsubj_find_we nsubj_find_Model num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation amod_model_statistical amod_model_standard det_model_the pobj_Model_model prepc_according_to_Model_to nn_Model_Translation nn_Model_Learning num_Model_5.1.2
I08-1068	J93-2003	o	The details of the algorithm can be found in the literature for statistical translation models such as -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_as_al. mwe_as_such amod_models_as nn_models_translation amod_models_statistical prep_for_literature_models det_literature_the prep_in_found_literature auxpass_found_be aux_found_can nsubjpass_found_details det_algorithm_the prep_of_details_algorithm det_details_The
I08-1068	J93-2003	n	IBM Model1 -LRB- Brown et al. 1993 -RRB- is a simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages	prep_across_differ_languages aux_differ_to xcomp_tends_differ nsubj_tends_order nn_order_word rcmod_way_tends det_way_the nn_translation_language prep_including_aspects_way prep_of_aspects_translation amod_aspects_subtler det_aspects_the prep_of_account_aspects neg_account_no dobj_takes_account nsubj_takes_which rcmod_model_takes amod_model_simplistic det_model_a cop_model_is nsubj_model_Model1 num_al._1993 nn_al._et amod_al._Brown dep_Model1_al. nn_Model1_IBM
I08-2104	J93-2003	o	In the proposed method the statistical machine translation -LRB- SMT -RRB- -LRB- Brown et al. 1993 -RRB- is deeply incorporated into the question answering process instead of using the SMT as the preprocessing before the mono-lingual QA process as in the previous work	amod_work_previous det_work_the pobj_in_work pcomp_as_in nn_process_QA amod_process_mono-lingual det_process_the prep_preprocessing_as prep_before_preprocessing_process det_preprocessing_the det_SMT_the prep_as_using_preprocessing dobj_using_SMT nn_process_answering nn_process_question det_process_the prepc_instead_of_incorporated_using prep_into_incorporated_process advmod_incorporated_deeply auxpass_incorporated_is nsubjpass_incorporated_translation prep_in_incorporated_method amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_translation_Brown appos_translation_SMT nn_translation_machine amod_translation_statistical det_translation_the amod_method_proposed det_method_the
I08-2104	J93-2003	o	In this paper we use IBM model 1 -LRB- Brown et al. 1993 -RRB- in order to get the probability P -LRB- Q | DA -RRB- as follows	mark_follows_as num_DA_| nn_DA_Q appos_P_DA dep_probability_P det_probability_the advcl_get_follows dobj_get_probability aux_get_to dep_get_order mark_get_in amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_IBM advcl_use_get dep_use_Brown dobj_use_model nsubj_use_we prep_in_use_paper det_paper_this
I08-6006	J93-2003	o	Most current transliteration systems use a generative model for transliteration such as freely available GIZA + +1 -LRB- Och and Ney 2000 -RRB- an implementation of the IBM alignment models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_alignment nn_models_IBM det_models_the dep_implementation_Brown prep_of_implementation_models det_implementation_an dep_Och_2000 conj_and_Och_Ney appos_GIZA_implementation appos_GIZA_Ney appos_GIZA_Och conj_+_GIZA_+1 dep_available_+1 dep_available_GIZA advmod_available_freely prep_such_as_transliteration_available amod_model_generative det_model_a prep_for_use_transliteration dobj_use_model nsubj_use_systems nn_systems_transliteration amod_systems_current amod_systems_Most
J00-1004	J93-2003	o	Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Brown
J00-1004	J93-2003	n	At the same time we believe our method has advantages over the approach developed initially at IBM -LRB- Brown et al. 1990 Brown et al. 1993 -RRB- for training translation systems automatically	nn_systems_translation nn_systems_training dep_al._1993 nn_al._et nn_al._Brown dep_Brown_al. dep_Brown_1990 dep_Brown_al. nn_Brown_et appos_IBM_Brown advmod_developed_automatically prep_for_developed_systems prep_at_developed_IBM advmod_developed_initially nsubj_developed_advantages det_approach_the prep_over_advantages_approach ccomp_has_developed nsubj_has_method poss_method_our ccomp_believe_has nsubj_believe_we prep_at_believe_time amod_time_same det_time_the
J00-2004	J93-2003	o	Dagan Church and Gale -LRB- 1993 -RRB- expanded on this idea by replacing Brown et al. 's -LRB- 1988 -RRB- word alignment parameters which were based on absolute word positions in aligned segments with a much smaller set of relative offset parameters	amod_parameters_offset amod_parameters_relative prep_of_set_parameters amod_set_smaller det_set_a advmod_smaller_much pobj_with_set amod_segments_aligned prep_in_positions_segments nn_positions_word amod_positions_absolute prep_on_based_positions auxpass_based_were nsubjpass_based_which rcmod_parameters_based nn_parameters_alignment nn_parameters_word nn_parameters_al. dep_al._1988 possessive_al._'s nn_al._et nn_al._Brown dobj_replacing_parameters det_idea_this agent_expanded_replacing prep_on_expanded_idea vmod_Gale_expanded appos_Gale_1993 dep_Dagan_with conj_and_Dagan_Gale conj_and_Dagan_Church
J00-2004	J93-2003	n	A word order correlation bias as well as the phrase structure biases in Brown et al. 's -LRB- 1993b -RRB- Models 4 and 5 would be less beneficial with noisier training bitexts or for language pairs with less similar word order	nn_order_word amod_order_similar amod_order_less prep_with_pairs_order nn_pairs_language pobj_for_pairs nn_bitexts_training amod_bitexts_noisier conj_or_beneficial_for prep_with_beneficial_bitexts advmod_beneficial_less cop_beneficial_be aux_beneficial_would nsubj_beneficial_1993b conj_and_4_5 dep_Models_5 dep_Models_4 dep_1993b_Models rcmod_al._for rcmod_al._beneficial possessive_al._'s nn_al._et nn_al._Brown prep_in_biases_al. nn_biases_structure nn_biases_phrase det_biases_the conj_and_bias_biases nn_bias_correlation nn_bias_order nn_bias_word det_bias_A ccomp_``_biases ccomp_``_bias
J00-2004	J93-2003	o	Choosing the most advantageous Hiemstra has published parts of the translational distributions of certain words induced using both his method and Brown et al. 's -LRB- 1993b -RRB- Model 1 from the same training bitext	nn_bitext_training amod_bitext_same det_bitext_the prep_from_Model_bitext num_Model_1 dep_1993b_Model dep_al._1993b possessive_al._'s nn_al._et advmod_Brown_al. conj_and_method_Brown poss_method_his preconj_method_both dobj_using_Brown dobj_using_method xcomp_induced_using amod_words_certain prep_of_distributions_words amod_distributions_translational det_distributions_the prep_of_parts_distributions vmod_published_induced dobj_published_parts aux_published_has nsubj_published_Hiemstra vmod_published_Choosing advmod_advantageous_most det_advantageous_the dobj_Choosing_advantageous ccomp_``_published
J00-2004	J93-2003	o	Due to the parameter interdependencies introduced by the one-to-one assumption we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. \ -LSB- 1993b Equation 26 \ -RSB- -RRB-	num_\_26 nn_\_Equation appos_1993b_\ dep_\_1993b nn_\_al. nn_\_Brown nn_al._et pobj_in_\ pcomp_as_in det_other_each prep_of_independently_other prep_estimated_as advmod_estimated_independently auxpass_estimated_be aux_estimated_can nsubjpass_estimated_that rcmod_parameters_estimated det_assignments_the prep_into_decomposing_parameters dobj_decomposing_assignments prepc_for_method_decomposing det_method_a dobj_find_method aux_find_to xcomp_unlikely_find cop_unlikely_are nsubj_unlikely_we prep_due_to_unlikely_interdependencies amod_assumption_one-to-one det_assumption_the agent_introduced_assumption vmod_interdependencies_introduced nn_interdependencies_parameter det_interdependencies_the
J00-2004	J93-2003	o	Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Brown
J00-2004	J93-2003	o	Evaluation 6.1 Evaluation at the Token Level This section compares translation model estimation methods A B and C to each other and to Brown et al. 's -LRB- 1993b -RRB- Model 1	num_Model_1 dep_1993b_Model dep_Brown_1993b possessive_Brown_'s nn_Brown_al. nn_Brown_et pobj_to_Brown det_other_each conj_and_A_to prep_to_A_other conj_and_A_C conj_and_A_B dep_methods_to dep_methods_C dep_methods_B dep_methods_A nn_methods_estimation nn_methods_model nn_methods_translation dobj_compares_methods nsubj_compares_section det_section_This amod_Level_Token det_Level_the rcmod_Evaluation_compares prep_at_Evaluation_Level num_Evaluation_6.1 nn_Evaluation_Evaluation
J00-2004	J93-2003	o	Until now translation models have been evaluated either subjectively -LRB- e.g. White and O'Connell 1993 -RRB- or using relative metrics such as perplexity with respect to other models -LRB- Brown et al. 1993b -RRB-	nn_1993b_al. nn_1993b_et advmod_Brown_1993b amod_models_other dep_perplexity_Brown prep_with_respect_to_perplexity_models prep_such_as_metrics_perplexity amod_metrics_relative dobj_using_metrics num_O'Connell_1993 conj_and_White_O'Connell conj_or_e.g._using dep_e.g._O'Connell dep_e.g._White dep_subjectively_using dep_subjectively_e.g. preconj_subjectively_either advmod_evaluated_subjectively auxpass_evaluated_been aux_evaluated_have nsubjpass_evaluated_models prep_evaluated_Until nn_models_translation pcomp_Until_now
J03-1002	J93-2003	o	An analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language -LRB- Brown Della Pietra Della Pietra Goldsmith et al. 1993 -RRB-	advmod_1993_al. nn_al._et appos_Pietra_1993 appos_Pietra_Goldsmith nn_Pietra_Della nn_Pietra_Della dep_Brown_Pietra appos_Brown_Pietra dep_language_Brown amod_language_other det_language_the prep_in_words_language amod_words_many advmod_many_too prep_with_align_words aux_align_to xcomp_tend_align nsubj_tend_they mark_tend_that nn_collectors_garbage prepc_in_forming_tend dobj_forming_collectors vmod_words_forming amod_words_rare prep_of_problem_words amod_problem_occurring det_problem_the advmod_occurring_frequently dobj_reduces_problem advmod_reduces_significantly csubj_reduces_smoothing mark_reduces_that nn_probabilities_fertility det_probabilities_the dobj_smoothing_probabilities ccomp_shows_reduces nsubj_shows_analysis det_alignments_the prep_of_analysis_alignments det_analysis_An
J03-1005	J93-2003	o	The fertility for the null word is treated specially -LRB- for details see Brown et al. -LSB- 1993 -RSB- -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_see_al. prep_for_see_details parataxis_treated_see advmod_treated_specially auxpass_treated_is nsubjpass_treated_fertility amod_word_null det_word_the prep_for_fertility_word det_fertility_The
J03-1005	J93-2003	o	For placing the head the center function center -LRB- i -RRB- -LRB- Brown et al. -LSB- 1993 -RSB- uses the notation circledot i -RRB- is used the average position of the source words with which the target word e i1 is aligned	auxpass_aligned_is nsubjpass_aligned_i1 prep_with_aligned_which dep_i1_e nn_i1_word nn_i1_target det_i1_the rcmod_words_aligned dep_source_words det_source_the prep_of_position_source amod_position_average det_position_the xcomp_:_position auxpass_used_is nsubjpass_used_center dep_circledot_i nn_circledot_notation det_circledot_the dobj_uses_circledot nsubj_uses_Brown dep_Brown_1993 dep_Brown_al. nn_Brown_et rcmod_center_uses dep_center_i nn_center_function nn_center_center det_center_the rcmod_head_used det_head_the dobj_placing_head pcomp_For_placing
J04-2003	J93-2003	o	Many existing systems for statistical machine translation -LRB- Garca-Varea and Casacuberta 2001 Germann et al. 2001 Nieen et al. 1998 Och Tillmann and Ney 1999 -RRB- implement models presented by Brown Della Pietra Della Pietra and Mercer -LRB- 1993 -RRB- The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position	nn_position_word nn_position_source det_position_each nn_positions_word nn_positions_target prep_to_assign_position dobj_assign_positions nsubj_assign_that rcmod_alignments_assign agent_described_alignments auxpass_described_is nsubjpass_described_correspondence nn_strings_target det_strings_the det_source_the conj_and_words_strings prep_in_words_source det_words_the prep_between_correspondence_strings prep_between_correspondence_words det_correspondence_The appos_Mercer_1993 nn_Pietra_Della nn_Pietra_Della conj_and_Brown_Mercer conj_and_Brown_Pietra conj_and_Brown_Pietra agent_presented_Mercer agent_presented_Pietra agent_presented_Pietra agent_presented_Brown vmod_models_presented parataxis_implement_described dobj_implement_models nsubj_implement_systems num_Ney_1999 conj_and_Och_Ney conj_and_Och_Tillmann dep_al._1998 nn_al._et nn_al._Nieen num_al._2001 nn_al._et nn_al._Germann num_Casacuberta_2001 dep_Garca-Varea_Ney dep_Garca-Varea_Tillmann dep_Garca-Varea_Och conj_and_Garca-Varea_al. conj_and_Garca-Varea_al. conj_and_Garca-Varea_Casacuberta appos_translation_al. appos_translation_al. appos_translation_Casacuberta appos_translation_Garca-Varea nn_translation_machine amod_translation_statistical prep_for_systems_translation amod_systems_existing amod_systems_Many ccomp_``_implement
J04-2003	J93-2003	o	The translation models they presented in various papers between 1988 and 1993 -LRB- Brown et al. 1988 Brown et al. 1990 Brown Della Pietra Della Pietra and Mercer 1993 -RRB- are commonly referred to as IBM models 15 based on the numbering in Brown Della Pietra Della Pietra and Mercer -LRB- 1993 -RRB-	appos_Mercer_1993 nn_Pietra_Della nn_Pietra_Della conj_and_Brown_Mercer conj_and_Brown_Pietra conj_and_Brown_Pietra prep_in_numbering_Mercer prep_in_numbering_Pietra prep_in_numbering_Pietra prep_in_numbering_Brown det_numbering_the num_models_15 nn_models_IBM pobj_referred_numbering prepc_based_on_referred_on pobj_referred_models prepc_as_to_referred_as advmod_referred_commonly auxpass_referred_are nsubjpass_referred_models num_Mercer_1993 nn_Pietra_Della nn_Pietra_Della appos_Brown_Pietra appos_Brown_Pietra dep_al._1990 nn_al._et nn_al._Brown conj_and_Brown_Mercer dep_Brown_Brown dep_Brown_al. dep_Brown_1988 dep_Brown_al. nn_Brown_et conj_and_1988_1993 prep_between_papers_1993 prep_between_papers_1988 amod_papers_various prep_in_presented_papers nsubj_presented_they appos_models_Mercer appos_models_Brown rcmod_models_presented nn_models_translation det_models_The
J04-2004	J93-2003	o	These results were achieved using the statistical alignments provided by model 5 -LRB- Brown et al. 1993 Och and Ney 2000 -RRB- and smoothed 11-grams and 6-grams respectively	conj_and_11-grams_6-grams dobj_smoothed_6-grams dobj_smoothed_11-grams nsubjpass_smoothed_results num_Ney_2000 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och dep_Brown_1993 dep_Brown_al. nn_Brown_et num_model_5 agent_provided_model vmod_alignments_provided amod_alignments_statistical det_alignments_the dobj_using_alignments advmod_achieved_respectively conj_and_achieved_smoothed dep_achieved_Brown xcomp_achieved_using auxpass_achieved_were nsubjpass_achieved_results det_results_These
J04-2004	J93-2003	o	In the following section we show how this drawback can be overcome using statistical alignments -LRB- Brown et al. 1993 -RRB-	dep_1993_al. num_Brown_1993 nn_Brown_et appos_alignments_Brown amod_alignments_statistical dobj_using_alignments xcomp_overcome_using auxpass_overcome_be aux_overcome_can nsubjpass_overcome_drawback advmod_overcome_how det_drawback_this ccomp_show_overcome nsubj_show_we prep_in_show_section amod_section_following det_section_the
J04-4002	J93-2003	p	Yet the modeling training and search methods have also improved since the field of statistical machine translation was pioneered by IBM in the late 1980s and early 1990s -LRB- Brown et al. 1990 Brown et al. 1993 Berger et al. 1994 -RRB-	dep_al._1994 nn_al._et nn_al._Berger dep_al._1993 nn_al._et nn_al._Brown dep_Brown_al. dep_Brown_al. dep_Brown_1990 dep_Brown_al. nn_Brown_et amod_1990s_early conj_and_1980s_1990s amod_1980s_late det_1980s_the prep_in_pioneered_1990s prep_in_pioneered_1980s agent_pioneered_IBM auxpass_pioneered_was nsubjpass_pioneered_field mark_pioneered_since nn_translation_machine amod_translation_statistical prep_of_field_translation det_field_the dobj_improved_Brown advcl_improved_pioneered advmod_improved_also aux_improved_have nsubj_improved_methods nsubj_improved_training nsubj_improved_modeling cc_improved_Yet nn_methods_search conj_and_modeling_methods conj_and_modeling_training det_modeling_the
J04-4002	J93-2003	o	As an alternative to the often used sourcechannel approach -LRB- Brown et al. 1993 -RRB- we directly model the posterior probability Pr -LRB- e I 1 | f J 1 -RRB- -LRB- Och and Ney 2002 -RRB-	num_Ney_2002 conj_and_Och_Ney dep_J_Ney dep_J_Och num_J_1 nn_J_f nn_J_| pobj_1_J dep_I_1 dep_e_I dep_-LRB-_e nn_Pr_probability amod_Pr_posterior det_Pr_the dobj_model_Pr advmod_model_directly nsubj_model_we prep_as_model_alternative num_al._1993 nn_al._et amod_al._Brown dep_approach_al. nn_approach_sourcechannel amod_approach_used det_approach_the advmod_used_often prep_to_alternative_approach det_alternative_an
J05-3002	J93-2003	o	Knight and Marcu -LRB- 2000 -RRB- treat reduction as a translation process using a noisychannel model -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_noisychannel det_model_a dobj_using_model nn_process_translation det_process_a vmod_reduction_using prep_as_reduction_process nn_reduction_treat nn_reduction_Marcu nn_reduction_Knight appos_Marcu_2000 conj_and_Knight_Marcu
J05-4003	J93-2003	o	One such model is the IBM Model 1 -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. appos_Model_Brown num_Model_1 nn_Model_IBM det_Model_the cop_Model_is nsubj_Model_model amod_model_such num_model_One
J05-4004	J93-2003	o	We compare against several competing systems the first of which is based on the original IBM Model 4 for machine translation -LRB- Brown et al. 1993 -RRB- and the HMM machine translation alignment model -LRB- Vogel Ney and Tillmann 1996 -RRB- as implemented in the GIZA + + package -LRB- Och and Ney 2003 -RRB-	num_Ney_2003 nn_Och_package preconj_Och_+ conj_and_GIZA_Ney conj_+_GIZA_Och det_GIZA_the prep_in_implemented_Ney prep_in_implemented_Och prep_in_implemented_GIZA mark_implemented_as num_Tillmann_1996 conj_and_Vogel_Tillmann conj_and_Vogel_Ney dep_model_implemented dep_model_Tillmann dep_model_Ney dep_model_Vogel nn_model_alignment nn_model_translation nn_model_machine nn_model_HMM det_model_the num_al._1993 nn_al._et amod_al._Brown nn_translation_machine conj_and_Model_model dep_Model_al. prep_for_Model_translation num_Model_4 nn_Model_IBM amod_Model_original det_Model_the prep_on_based_model prep_on_based_Model auxpass_based_is nsubjpass_based_first prep_of_first_which det_first_the rcmod_systems_based amod_systems_competing amod_systems_several prep_against_compare_systems nsubj_compare_We
J05-4004	J93-2003	n	Our system outperforms competing approaches including the standard machine translation alignment models -LRB- Brown et al. 1993 Vogel Ney and Tillmann 1996 -RRB- and the state-of-the-art Cut and Paste summary alignment technique -LRB- Jing 2002 -RRB-	dobj_Jing_2002 nn_technique_alignment nn_technique_summary nn_technique_Paste dep_Cut_Jing conj_and_Cut_technique amod_Cut_state-of-the-art det_Cut_the num_Tillmann_1996 conj_and_Vogel_Tillmann conj_and_Vogel_Ney dep_Brown_Tillmann dep_Brown_Ney dep_Brown_Vogel dep_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_models_technique conj_and_models_Cut appos_models_Brown nn_models_alignment nn_models_translation nn_models_machine amod_models_standard det_models_the prep_including_approaches_Cut prep_including_approaches_models amod_approaches_competing dobj_outperforms_approaches nsubj_outperforms_system poss_system_Our
J05-4004	J93-2003	o	One obvious first approach would be to run a simpler model for the first iteration -LRB- for example Model 1 from machine translation -LRB- Brown et al. 1993 -RRB- which tends to be very recall oriented -RRB- and use this to see subsequent iterations of the more complex model	amod_model_complex det_model_the advmod_complex_more prep_of_iterations_model amod_iterations_subsequent dobj_see_iterations aux_see_to vmod_use_see dobj_use_this dep_recall_oriented advmod_recall_very conj_and_be_use dep_be_recall dep_to_use dep_to_be prep_tends_to nsubj_tends_which num_al._1993 nn_al._et amod_al._Brown nn_translation_machine rcmod_Model_tends dep_Model_al. prep_from_Model_translation num_Model_1 amod_iteration_first det_iteration_the prep_for_model_iteration amod_model_simpler det_model_a dep_run_Model prep_for_run_example dobj_run_model aux_run_to xcomp_be_run aux_be_would nsubj_be_approach amod_approach_first amod_approach_obvious num_approach_One
J05-4004	J93-2003	o	In the context of headline generation simple statistical models are used for aligning documents and headlines -LRB- Banko Mittal and Witbrock 2000 Berger and Mittal 2000 Schwartz Zajic and Dorr 2002 -RRB- based on IBM Model 1 -LRB- Brown et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_Brown_1993 appos_Model_Brown num_Model_1 nn_Model_IBM prep_on_based_Model num_Dorr_2002 conj_and_Schwartz_Dorr conj_and_Schwartz_Zajic num_Mittal_2000 conj_and_Berger_Mittal num_Witbrock_2000 dep_Banko_Dorr dep_Banko_Zajic dep_Banko_Schwartz conj_and_Banko_Mittal conj_and_Banko_Berger conj_and_Banko_Witbrock conj_and_Banko_Mittal dep_documents_Berger dep_documents_Witbrock dep_documents_Mittal dep_documents_Banko conj_and_documents_headlines amod_documents_aligning vmod_used_based prep_for_used_headlines prep_for_used_documents auxpass_used_are nsubjpass_used_models prep_in_used_context amod_models_statistical amod_models_simple nn_generation_headline prep_of_context_generation det_context_the
J06-4004	J93-2003	o	For these first SMT systems translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_corpora_al. amod_corpora_bilingual dobj_using_corpora agent_trained_using auxpass_trained_were nsubjpass_trained_that rcmod_models_trained nn_models_translation amod_models_word-based prep_from_approximated_models auxpass_approximated_were nsubjpass_approximated_probabilities prep_for_approximated_systems nn_level_sentence det_level_the prep_at_probabilities_level amod_probabilities_translation-model nn_systems_SMT amod_systems_first det_systems_these rcmod_``_approximated
J06-4004	J93-2003	o	This feature is implemented by using the IBM-1 lexical parameters -LRB- Brown et al. 1993 Och et al. 2004 -RRB-	dep_al._2004 nn_al._et nn_al._Och dep_Brown_al. dep_Brown_1993 dep_Brown_al. nn_Brown_et amod_parameters_lexical nn_parameters_IBM-1 det_parameters_the dobj_using_parameters dep_implemented_Brown agent_implemented_using auxpass_implemented_is nsubjpass_implemented_feature det_feature_This
J06-4004	J93-2003	o	More specifically the latter system uses the IBM-1 lexical parameters -LRB- Brown et al. 1993 -RRB- for computing the translation probabilities of two possible new tuples the one resulting when the null-aligned-word is attached to Table 6 Evaluation results for experiments on n-gram size incidence	nn_incidence_size nn_incidence_n-gram prep_on_experiments_incidence prep_for_results_experiments nsubj_results_one num_Evaluation_6 nn_Evaluation_Table prep_to_attached_Evaluation auxpass_attached_is nsubjpass_attached_null-aligned-word advmod_attached_when det_null-aligned-word_the advcl_resulting_attached vmod_one_resulting det_one_the amod_tuples_new amod_tuples_possible num_tuples_two prep_of_probabilities_tuples nn_probabilities_translation det_probabilities_the dobj_computing_probabilities dep_1993_al. nn_al._et num_Brown_1993 amod_parameters_lexical nn_parameters_IBM-1 det_parameters_the parataxis_uses_results prepc_for_uses_computing dep_uses_Brown dobj_uses_parameters nsubj_uses_system advmod_uses_specifically amod_system_latter det_system_the dep_specifically_More
J06-4004	J93-2003	p	According to our experience the best performance is achieved when the union of the source-to-target and target-to-source alignment sets -LRB- IBM models Brown et al. -LSB- 1993 -RSB- -RRB- is used for tuple extraction -LRB- some experimental results regarding this issue are presented in Section 4.2.2 -RRB-	num_Section_4.2.2 prep_in_presented_Section auxpass_presented_are nsubjpass_presented_results advcl_presented_used det_issue_this prep_regarding_results_issue amod_results_experimental det_results_some nn_extraction_tuple prep_for_used_extraction auxpass_used_is nsubjpass_used_union advmod_used_when dep_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_IBM nn_sets_alignment nn_sets_target-to-source conj_and_source-to-target_sets det_source-to-target_the dep_union_models prep_of_union_sets prep_of_union_source-to-target det_union_the ccomp_achieved_presented auxpass_achieved_is nsubjpass_achieved_performance pobj_achieved_experience prepc_according_to_achieved_to amod_performance_best det_performance_the poss_experience_our
J06-4004	J93-2003	o	The first SMT systems were developed in the early nineties -LRB- Brown et al. 1990 1993 -RRB-	amod_Brown_1993 dep_Brown_1990 dep_Brown_al. nn_Brown_et dep_nineties_Brown amod_nineties_early det_nineties_the prep_in_developed_nineties auxpass_developed_were nsubjpass_developed_systems nn_systems_SMT amod_systems_first det_systems_The
J07-1003	J93-2003	o	Therefore we determine the maximal translation probability of the target word e over the source sentence words p ibm1 -LRB- e | f J 1 -RRB- = max j = 0 J p -LRB- e | f j -RRB- -LRB- 18 -RRB- where f 0 is the empty source word -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_word_al. nn_word_source amod_word_empty det_word_the cop_word_is nsubj_word_f advmod_word_where num_f_0 nn_j_f nn_j_| dep_e_j rcmod_p_word appos_p_18 appos_p_e nn_p_J dobj_=_0 nsubj_=_j nn_j_max dep_=_= num_J_1 nn_J_f nn_J_| dep_e_J conj_ibm1_p dep_ibm1_= dep_ibm1_e nn_ibm1_p nn_words_sentence nn_words_source det_words_the prep_over_word_words dep_word_e nn_word_target det_word_the prep_of_probability_word nn_probability_translation amod_probability_maximal det_probability_the dep_determine_ibm1 dobj_determine_probability nsubj_determine_we advmod_determine_Therefore
J07-2003	J93-2003	o	The basic phrase-based model is an instance of the noisy-channel approach -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown amod_approach_noisy-channel det_approach_the dep_instance_al. prep_of_instance_approach det_instance_an cop_instance_is nsubj_instance_model amod_model_phrase-based amod_model_basic det_model_The
J07-3002	J93-2003	p	Introduction Automatic word alignment -LRB- Brown et al. 1993 -RRB- is a vital component of all statistical machine translation -LRB- SMT -RRB- approaches	nn_approaches_translation appos_translation_SMT nn_translation_machine amod_translation_statistical det_translation_all prep_of_component_approaches amod_component_vital det_component_a cop_component_is nsubj_component_alignment num_al._1993 nn_al._et amod_al._Brown dep_alignment_al. nn_alignment_word nn_alignment_Automatic nn_alignment_Introduction
J09-1002	J93-2003	o	Different approaches have been proposed for modeling Pr -LRB- s a | t -RRB- in Equation -LRB- 8 -RRB- Zero-order models such as model 1 model 2 andmodel 3 -LRB- Brown et al. 1993 -RRB- and the rstorder models such as model 4 model 5 -LRB- Brown et al. 1993 -RRB- hidden Markov model -LRB- Ney et al. 2000 -RRB- and model 6 -LRB- Och and Ney 2003 -RRB-	num_Ney_2003 conj_and_Och_Ney dep_model_Ney dep_model_Och num_model_6 dep_al._2000 nn_al._et advmod_Ney_al. appos_model_Ney nn_model_Markov amod_model_hidden dep_al._1993 nn_al._et advmod_Brown_al. appos_model_Brown num_model_5 num_model_4 prep_such_as_models_model nn_models_rstorder det_models_the num_al._1993 dep_Brown_al. nn_Brown_et appos_andmodel_Brown num_andmodel_3 num_model_2 conj_and_model_models conj_and_model_andmodel appos_model_model num_model_1 conj_and_models_model conj_and_models_model conj_and_models_model prep_such_as_models_models prep_such_as_models_andmodel prep_such_as_models_model nn_models_Zero-order appos_Equation_8 nn_t_| det_t_a appos_s_t prep_in_Pr_Equation dep_Pr_s nn_Pr_modeling dep_proposed_model dep_proposed_model dep_proposed_model dep_proposed_models prep_for_proposed_Pr auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches amod_approaches_Different
J09-1002	J93-2003	o	Note that the translation direction is inverted from what would be normally expected correspondingly the models built around this equation are often called invertedtranslationmodels -LRB- Brown et al. 1990 1993 -RRB-	amod_Brown_1993 dep_Brown_1990 dep_Brown_al. nn_Brown_et dep_invertedtranslationmodels_Brown dep_called_invertedtranslationmodels advmod_called_often auxpass_called_are nsubjpass_called_models advmod_called_correspondingly det_equation_this prep_around_built_equation vmod_models_built det_models_the advmod_expected_normally auxpass_expected_be aux_expected_would nsubjpass_expected_what prepc_from_inverted_expected auxpass_inverted_is nsubjpass_inverted_direction mark_inverted_that nn_direction_translation det_direction_the parataxis_Note_called ccomp_Note_inverted
J93-1001	J93-2003	o	Four alternatives are proposed in these special issues -LRB- 1 -RRB- Brent -LRB- 1993 -RRB- -LRB- 2 -RRB- Briscoe and Carroll -LRB- this issue -RRB- -LRB- 3 -RRB- Hindle and Rooth -LRB- this issue -RRB- and -LRB- 4 -RRB- Weischedel et al.	nn_al._et nn_al._Weischedel dep_al._4 det_issue_this appos_Rooth_issue conj_and_Hindle_al. conj_and_Hindle_Rooth dep_Hindle_3 nn_Hindle_Brent det_issue_this dep_Briscoe_issue conj_and_Briscoe_Carroll dep_Briscoe_2 appos_Brent_Carroll appos_Brent_Briscoe appos_Brent_1993 dep_1_al. dep_1_Rooth dep_1_Hindle amod_issues_special det_issues_these dep_proposed_1 prep_in_proposed_issues auxpass_proposed_are nsubjpass_proposed_alternatives num_alternatives_Four
J93-1001	J93-2003	o	This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues Biber -LRB- 1993 -RRB- Brent -LRB- 1993 -RRB- Hindle and Rooth -LRB- this issue -RRB- Pustejovsky et al.	nn_al._et nn_al._Pustejovsky det_issue_this appos_Brent_1993 dep_Biber_al. appos_Biber_issue conj_and_Biber_Rooth conj_and_Biber_Hindle conj_and_Biber_Brent appos_Biber_1993 amod_issues_special det_issues_these prep_in_contributions_issues prep_of_number_contributions amod_number_large det_number_the prep_by_evidenced_number mark_evidenced_as amod_linguistics_computational dep_area_Rooth dep_area_Hindle dep_area_Brent dep_area_Biber advcl_area_evidenced prep_in_area_linguistics amod_area_exciting det_area_a cop_area_is nsubj_area_This advmod_exciting_particularly
J96-1001	J93-2003	o	Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas including sentence alignment -LRB- Gale and Church 1991b Brown Lai and Mercer 1991 Simard Foster and Isabelle 1992 Gale and Church 1993 Chen 1993 -RRB- word alignment -LRB- Gale and Church 1991a Brown et al. 1993 Dagan Church and Gale 1993 Fung and McKeown 1994 Fung 1995b -RRB- alignment of groups of words -LRB- Smadja 1992 Kupiec 1993 van der Eijk 1993 -RRB- and statistical translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_translation_al. amod_translation_statistical dep_Eijk_1993 nn_Eijk_der nn_Eijk_van num_Kupiec_1993 dep_Smadja_Eijk dep_Smadja_Kupiec num_Smadja_1992 appos_words_Smadja prep_of_groups_words prep_of_alignment_groups nn_1995b_Fung num_Fung_1994 conj_and_Fung_McKeown num_Gale_1993 conj_and_Dagan_Gale conj_and_Dagan_Church dep_al._1993 nn_al._et nn_al._Brown nn_1991a_Church dep_Gale_1995b conj_and_Gale_McKeown conj_and_Gale_Fung conj_and_Gale_Gale conj_and_Gale_Church conj_and_Gale_Dagan conj_and_Gale_al. conj_and_Gale_1991a appos_alignment_Fung appos_alignment_Dagan appos_alignment_al. appos_alignment_1991a appos_alignment_Gale nn_alignment_word num_Chen_1993 num_Church_1993 conj_and_Gale_Church num_Isabelle_1992 conj_and_Simard_Isabelle conj_and_Simard_Foster num_Mercer_1991 conj_and_Brown_Mercer conj_and_Brown_Lai nn_1991b_Church dep_Gale_Chen dep_Gale_Church dep_Gale_Gale conj_and_Gale_Isabelle conj_and_Gale_Foster conj_and_Gale_Simard conj_and_Gale_Mercer conj_and_Gale_Lai conj_and_Gale_Brown conj_and_Gale_1991b conj_and_alignment_translation appos_alignment_alignment appos_alignment_alignment appos_alignment_Simard appos_alignment_Brown appos_alignment_1991b appos_alignment_Gale nn_alignment_sentence prep_including_areas_translation prep_including_areas_alignment amod_areas_several prep_in_attracted_areas dobj_attracted_interest aux_attracted_has nsubj_attracted_availability amod_data_bilingual prep_of_amounts_data amod_amounts_large prep_of_availability_amounts amod_availability_recent det_availability_The rcmod_Work_attracted amod_Work_Related
J97-2004	J93-2003	o	Notice that most in-context and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the LLOCE -LRB- McArthur 1992 -RRB- and CILIN -LRB- Mei et al. 1993 -RRB-	nn_al._et dep_Mei_1993 advmod_Mei_al. dep_CILIN_Mei num_McArthur_1992 conj_and_LLOCE_CILIN dep_LLOCE_McArthur det_LLOCE_the prep_such_as_thesaurus_CILIN prep_such_as_thesaurus_LLOCE amod_thesaurus_typical det_thesaurus_a prep_in_category_thesaurus amod_category_same det_category_the prep_within_bounded_category auxpass_bounded_are nsubjpass_bounded_translations mark_bounded_that nn_words_source prep_of_translations_words amod_translations_dictionary amod_translations_in-context amod_translations_most conj_and_in-context_dictionary ccomp_Notice_bounded
J97-2004	J93-2003	o	The above observations can be stated formally from the perspective of Brown et al. 's -LRB- 1993 -RRB- Model 2	num_Model_2 dep_al._Model dep_al._1993 possessive_al._'s nn_al._et nn_al._Brown prep_of_perspective_al. det_perspective_the prep_from_stated_perspective advmod_stated_formally auxpass_stated_be aux_stated_can nsubjpass_stated_observations amod_observations_above det_observations_The
J97-2004	J93-2003	n	In terms of alignment this wordnumber difference means that multiword connections must be considered a task which 334 Sue J. Ker and Jason S. Chang Word Alignment is beyond the reach of methods proposed in recent alignment works based on Brown et al. 's -LRB- 1993 -RRB- Model 1 and 2	conj_and_1_2 dep_Model_2 dep_Model_1 dep_al._Model dep_al._1993 possessive_al._'s nn_al._et nn_al._Brown pobj_works_al. prepc_based_on_works_on amod_alignment_recent prep_in_proposed_alignment vmod_methods_proposed prep_of_reach_methods det_reach_the prep_beyond_is_reach nsubj_is_Word nsubj_is_Ker dobj_is_which nn_Word_Chang nn_Word_S. nn_Word_Jason dep_Ker_Alignment conj_and_Ker_Word nn_Ker_J. nn_Ker_Sue num_Ker_334 dep_task_works rcmod_task_is det_task_a dobj_considered_task auxpass_considered_be aux_considered_must nsubjpass_considered_connections mark_considered_that amod_connections_multiword ccomp_means_considered nsubj_means_difference prep_in_means_terms nn_difference_wordnumber det_difference_this prep_of_terms_alignment
J97-3002	J93-2003	o	Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis -LRB- Brown et al. 1990 Gale and Church 1991 Gale Church and Yarowsky 1992 Church 1993 Brown et al. 1993 Dagan Church and Gale 1993 Department of Computer Science University of Science and Technology Clear Water Bay Hong Kong	nn_Kong_Hong nn_Bay_Water nn_Bay_Clear conj_and_Science_Technology appos_University_Kong conj_University_Bay prep_of_University_Technology prep_of_University_Science nn_Science_Computer appos_Department_University prep_of_Department_Science num_Gale_1993 conj_and_Dagan_Gale conj_and_Dagan_Church num_al._1993 nn_al._et nn_al._Brown num_Church_1993 num_Yarowsky_1992 conj_and_Gale_Yarowsky conj_and_Gale_Church num_Church_1991 conj_and_Gale_Church dep_al._Department dep_al._Gale dep_al._Church dep_al._Dagan dep_al._al. dep_al._Church dep_al._Yarowsky dep_al._Church dep_al._Gale dep_al._Church dep_al._Gale num_al._1990 nn_al._et amod_al._Brown amod_analysis_statistical prep_for_constraints_analysis prep_of_source_constraints amod_source_rich det_source_a dobj_provide_source aux_provide_to dep_shown_al. xcomp_shown_provide auxpass_shown_been aux_shown_have nsubjpass_shown_corpora amod_corpora_bilingual amod_corpora_Parallel
J97-3002	J93-2003	o	The usual Chinese NLP architecture first preprocesses input text through a word segmentation module -LRB- Chiang et al. 1992 Lin Chiang and Su 1992 1993 Chang and Chen 1993 Wu and Tseng 1993 Sproat et al. 1994 Wu and Fung 1994 -RRB- but clearly bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually it may not agree with the words present in the English sentence	amod_sentence_English det_sentence_the prep_in_present_sentence amod_words_present det_words_the prep_with_agree_words neg_agree_not aux_agree_may nsubj_agree_it advcl_agree_acceptable mark_agree_because advmod_acceptable_monolingually cop_acceptable_is nsubj_acceptable_segmentation mark_acceptable_if advmod_acceptable_even amod_segmentation_Chinese det_segmentation_the amod_context_monolingual amod_context_isolated det_context_the advcl_resolved_agree prep_in_resolved_context auxpass_resolved_be neg_resolved_not aux_resolved_could nsubjpass_resolved_that rcmod_ambiguities_resolved nn_ambiguities_segmentation prep_from_arising_ambiguities vmod_errors_arising det_errors_any agent_hampered_errors auxpass_hampered_be aux_hampered_will nsubjpass_hampered_parsing amod_parsing_bilingual num_Wu_1994 conj_and_Wu_Fung nn_al._et nn_al._Sproat num_Wu_1993 conj_and_Wu_Tseng num_Chen_1993 dep_1992_Su appos_Lin_Chiang dep_Chiang_Fung dep_Chiang_Wu dep_Chiang_1994 dep_Chiang_al. dep_Chiang_Tseng dep_Chiang_Wu conj_and_Chiang_Chen conj_and_Chiang_Chang conj_and_Chiang_1993 conj_and_Chiang_1992 dep_Chiang_Lin dep_Chiang_1992 dep_Chiang_al. nn_Chiang_et nn_module_segmentation nn_module_word det_module_a nn_text_input parataxis_preprocesses_hampered advmod_preprocesses_clearly cc_preprocesses_but dep_preprocesses_Chen dep_preprocesses_Chang dep_preprocesses_1993 dep_preprocesses_1992 dep_preprocesses_Chiang prep_through_preprocesses_module dobj_preprocesses_text advmod_preprocesses_first nsubj_preprocesses_architecture nn_architecture_NLP amod_architecture_Chinese amod_architecture_usual det_architecture_The ccomp_``_preprocesses
J97-3002	J93-2003	o	The later IBM models are formulated to prefer collocations -LRB- Brown et al. 1993 -RRB-	dep_1993_al. num_Brown_1993 nn_Brown_et appos_collocations_Brown dobj_prefer_collocations aux_prefer_to xcomp_formulated_prefer auxpass_formulated_are nsubjpass_formulated_models nn_models_IBM amod_models_later det_models_The
J98-4003	J93-2003	o	-LRB- p. 18 -RRB- Whether this is a useful perspective for machine translation is debatable -LRB- Brown et al. 1993 Knoblock 1996 -RRB- -- however it is a dead-on description of transliteration	prep_of_description_transliteration amod_description_dead-on det_description_a cop_description_is nsubj_description_it num_Knoblock_1996 dep_al._Knoblock num_al._1993 nn_al._et amod_al._Brown parataxis_debatable_description advmod_debatable_however dep_debatable_al. cop_debatable_is nsubj_debatable_p. nn_translation_machine prep_for_perspective_translation amod_perspective_useful det_perspective_a cop_perspective_is nsubj_perspective_this mark_perspective_Whether dep_p._perspective num_p._18
J99-1003	J93-2003	o	Then they adapted Brown et al. 's -LRB- 1993 -RRB- statistical translation Model 2 to work with this model of cooccurrence	prep_of_model_cooccurrence det_model_this prep_with_work_model aux_work_to vmod_Model_work num_Model_2 nn_Model_translation amod_Model_statistical dep_al._Model dep_al._1993 possessive_al._'s nn_al._et nn_al._Brown dobj_adapted_al. nsubj_adapted_they advmod_adapted_Then
J99-1003	J93-2003	o	A limitation of Church 's method and therefore also of Dagan Church and Gale 's method is that orthographic cognates exist only among languages with similar alphabets -LRB- Church et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_Church_1993 appos_alphabets_Church amod_alphabets_similar prep_with_languages_alphabets prep_among_exist_languages advmod_exist_only nsubj_exist_cognates mark_exist_that amod_cognates_orthographic ccomp_is_exist nsubj_is_of nsubj_is_limitation poss_method_Gale conj_and_Dagan_method conj_and_Dagan_Church pobj_of_method pobj_of_Church pobj_of_Dagan advmod_of_also advmod_of_therefore poss_method_Church conj_and_limitation_of prep_of_limitation_method det_limitation_A ccomp_``_is
J99-1003	J93-2003	o	Although the above statement was made about translation problems faced by human translators recent research -LRB- Brown et al. 1993 Melamed 1996b -RRB- suggests that it also applies to problems in machine translation	nn_translation_machine prep_in_problems_translation prep_to_applies_problems advmod_applies_also nsubj_applies_it mark_applies_that ccomp_suggests_applies nsubj_suggests_research advcl_suggests_made nn_1996b_Melamed dep_al._1996b num_al._1993 nn_al._et amod_al._Brown dep_research_al. amod_research_recent amod_translators_human agent_faced_translators vmod_problems_faced nn_problems_translation prep_about_made_problems auxpass_made_was nsubjpass_made_statement mark_made_Although amod_statement_above det_statement_the
J99-1003	J93-2003	o	For example bilingual lexicographers can use bitexts to discover new cross-language lexicalization patterns -LRB- Catizone Russell and Warwick 1993 Gale and Church 1991b -RRB- students of foreign languages can use one half of a bitext to practice their reading skills referring to the other half for translation when they get stuck -LRB- Nerbonne et al. 1997 -RRB-	nn_al._et dep_Nerbonne_1997 advmod_Nerbonne_al. dep_stuck_Nerbonne auxpass_stuck_get nsubjpass_stuck_they advmod_stuck_when prep_for_half_translation amod_half_other det_half_the advcl_referring_stuck prep_to_referring_half nn_skills_reading poss_skills_their dobj_practice_skills aux_practice_to det_bitext_a prep_of_half_bitext num_half_one xcomp_use_referring vmod_use_practice dobj_use_half aux_use_can nsubj_use_students amod_languages_foreign prep_of_students_languages nn_1991b_Church conj_and_Gale_1991b num_Warwick_1993 dep_Catizone_1991b dep_Catizone_Gale conj_and_Catizone_Warwick conj_and_Catizone_Russell dep_patterns_Warwick dep_patterns_Russell dep_patterns_Catizone nn_patterns_lexicalization amod_patterns_cross-language amod_patterns_new dobj_discover_patterns aux_discover_to parataxis_use_use vmod_use_discover dobj_use_bitexts aux_use_can nsubj_use_lexicographers prep_for_use_example amod_lexicographers_bilingual
N03-1010	J93-2003	o	1 Introduction Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s -LRB- Brown et al. 1990 1993 Berger et al. 1994 1996 -RRB-	amod_Berger_1996 num_Berger_1994 nn_Berger_al. nn_Berger_et dep_Brown_Berger num_Brown_1993 num_Brown_1990 dep_Brown_al. nn_Brown_et amod_1990s_early det_1990s_the prep_in_developed_1990s prep_at_developed_IBM nn_models_replacement nn_models_word dep_builds_Brown dep_builds_developed prep_on_builds_models nsubj_builds_Introduction nn_translation_machine amod_translation_statistical prep_in_work_translation amod_work_current det_work_the prep_of_Most_work amod_Introduction_Most num_Introduction_1 ccomp_``_builds
N03-1017	J93-2003	o	For more information on these models please refer to Brown et al. -LSB- 1993 -RSB-	dep_Brown_1993 dep_Brown_al. nn_Brown_et prep_to_refer_Brown aux_refer_please prep_for_refer_information det_models_these prep_on_information_models amod_information_more
N03-1017	J93-2003	o	As the first method we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model the Giza + + -LSB- Och and Ney 2000 -RSB- toolkit for the IBM models -LSB- Brown et al. 1993 -RSB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the dep_toolkit_Ney dep_toolkit_Och dep_Och_2000 conj_and_Och_Ney dep_Giza_Brown prep_for_Giza_models dep_Giza_toolkit conj_+_Giza_+ det_Giza_the dep_model_+ dep_model_Giza nn_model_translation amod_model_word-based det_model_a prep_for_toolkit_model nn_toolkit_training det_toolkit_a agent_word-aligned_toolkit auxpass_word-aligned_been aux_word-aligned_has nsubjpass_word-aligned_that rcmod_corpus_word-aligned det_corpus_a nn_alignments_phrase prep_from_learn_corpus dobj_learn_alignments nsubj_learn_we prep_as_learn_method amod_method_first det_method_the
N03-1019	J93-2003	n	The ATTM attempts to overcome the deficiencies of word-to-word translation models -LRB- Brown et al. 1993 -RRB- through the use of phrasal translations	amod_translations_phrasal prep_of_use_translations det_use_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation amod_models_word-to-word prep_of_deficiencies_models det_deficiencies_the dobj_overcome_deficiencies aux_overcome_to prep_through_attempts_use dep_attempts_Brown xcomp_attempts_overcome nsubj_attempts_ATTM det_ATTM_The
N03-2036	J93-2003	n	1 Phrase-based Unigram Model Various papers use phrase-based translation systems -LRB- Och et al. 1999 Marcu and Wong 2002 Yamada and Knight 2002 -RRB- that have shown to improve translation quality over single-word based translation systems introduced in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_introduced_in vmod_systems_introduced nn_systems_translation amod_systems_based amod_systems_single-word nn_quality_translation prep_over_improve_systems dobj_improve_quality aux_improve_to xcomp_shown_improve aux_shown_have nsubj_shown_that amod_Yamada_2002 conj_and_Yamada_Knight rcmod_Marcu_shown dep_Marcu_Knight dep_Marcu_Yamada num_Marcu_2002 conj_and_Marcu_Wong dep_Och_Wong dep_Och_Marcu appos_Och_1999 dep_Och_al. nn_Och_et nn_systems_translation amod_systems_phrase-based dep_use_Och dobj_use_systems nsubj_use_papers amod_papers_Various nn_papers_Model nn_papers_Unigram amod_papers_Phrase-based num_papers_1 ccomp_``_use
N03-4001	J93-2003	n	By segmenting words into morphemes we can improve the performance of natural language systems including machine translation -LRB- Brown et al. 1993 -RRB- and information retrieval -LRB- Franz M. and McCarley S. 2002 -RRB-	num_S._2002 appos_Franz_S. conj_and_Franz_McCarley conj_and_Franz_M. dep_retrieval_McCarley dep_retrieval_M. dep_retrieval_Franz nn_retrieval_information num_al._1993 nn_al._et amod_al._Brown conj_and_translation_retrieval dep_translation_al. nn_translation_machine prep_including_systems_retrieval prep_including_systems_translation nn_systems_language amod_systems_natural prep_of_performance_systems det_performance_the dobj_improve_performance aux_improve_can nsubj_improve_we prep_by_improve_words prep_into_words_morphemes amod_words_segmenting
N04-1008	J93-2003	o	For comparison purposes we consider two different algorithms for our AnswerExtraction module one that does not bridge the lexical chasm based on N-gram cooccurrences between the question terms and the answer terms and one that attempts to bridge the lexical chasm using Statistical Machine Translation inspired techniques -LRB- Brown et al. 1993 -RRB- in order to find the best answer for a given question	amod_question_given det_question_a prep_for_answer_question amod_answer_best det_answer_the dobj_find_answer aux_find_to dep_find_order mark_find_in amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_inspired_Brown dobj_inspired_techniques nsubj_inspired_one nn_Translation_Machine amod_Translation_Statistical dobj_using_Translation amod_chasm_lexical det_chasm_the vmod_bridge_using dobj_bridge_chasm aux_bridge_to xcomp_attempts_bridge nsubj_attempts_that rcmod_one_attempts nn_terms_answer det_terms_the conj_and_terms_terms nn_terms_question det_terms_the prep_between_cooccurrences_terms prep_between_cooccurrences_terms nn_cooccurrences_N-gram prep_on_based_cooccurrences amod_chasm_lexical det_chasm_the dobj_bridge_chasm neg_bridge_not aux_bridge_does nsubj_bridge_that advcl_one_find conj_and_one_inspired vmod_one_based rcmod_one_bridge nn_module_AnswerExtraction poss_module_our dep_algorithms_inspired dep_algorithms_one prep_for_algorithms_module amod_algorithms_different num_algorithms_two dobj_consider_algorithms nsubj_consider_we prep_for_consider_purposes nn_purposes_comparison
N04-1008	J93-2003	o	The mapping of answer terms to question terms is modeled using Black et al. s -LRB- 1993 -RRB- simplest model called IBM Model 1	num_Model_1 nn_Model_IBM dep_called_Model vmod_model_called amod_model_simplest num_model_1993 dep_s_model nn_s_al. nn_s_et amod_s_Black dobj_using_s xcomp_modeled_using auxpass_modeled_is nsubjpass_modeled_mapping dobj_question_terms aux_question_to nn_terms_answer vmod_mapping_question prep_of_mapping_terms det_mapping_The
N04-1021	J93-2003	o	We are given a source -LRB- Chinese -RRB- sentence f = fJ1 = f1 fj fJ which is to be translated into a target -LRB- English -RRB- sentence e = eI1 = e1 ei eI Among all possible target sentences we will choose the sentence with the highest probability eI1 = argmax eI1 -LCB- Pr -LRB- eI1 | fJ1 -RRB- -RCB- -LRB- 1 -RRB- As an alternative to the often used source-channel approach -LRB- Brown et al. 1993 -RRB- we directly model the posterior probability Pr -LRB- eI1 | fJ1 -RRB- -LRB- Och and Ney 2002 -RRB- using a log-linear combination of feature functions	nn_functions_feature prep_of_combination_functions amod_combination_log-linear det_combination_a dobj_using_combination num_Och_2002 conj_and_Och_Ney num_fJ1_| nn_fJ1_eI1 appos_Pr_Ney appos_Pr_Och appos_Pr_fJ1 nn_Pr_probability amod_Pr_posterior det_Pr_the xcomp_model_using dobj_model_Pr advmod_model_directly nsubj_model_we num_al._1993 nn_al._et amod_al._Brown dep_approach_al. amod_approach_source-channel amod_approach_used det_approach_the advmod_used_often prep_to_alternative_approach det_alternative_an parataxis_1_model prep_as_1_alternative num_fJ1_| nn_fJ1_eI1 appos_Pr_fJ1 dep_eI1_Pr nn_eI1_argmax dep_=_eI1 dep_eI1_1 amod_eI1_= amod_probability_highest det_probability_the dep_sentence_eI1 prep_with_sentence_probability det_sentence_the dobj_choose_sentence aux_choose_will nsubj_choose_we nn_sentences_target amod_sentences_possible det_sentences_all prep_among_eI_sentences rcmod_e1_choose appos_e1_eI appos_e1_ei amod_e1_= nn_e1_eI1 dobj_=_e1 dep_=_e dep_sentence_= dep_target_sentence appos_target_English det_target_a prep_into_translated_target auxpass_translated_be aux_translated_to xcomp_is_translated nsubj_is_which rcmod_fJ_is dep_f1_fJ conj_f1_fj amod_f1_= nn_f1_fJ1 amod_f1_= nn_f1_f nn_f1_sentence dep_f1_Chinese dep_source_f1 det_source_a dobj_given_source auxpass_given_are nsubjpass_given_We ccomp_``_given
N04-1021	J93-2003	o	4.1 Model 1 Score We used IBM Model 1 -LRB- Brown et al. 1993 -RRB- as one of the feature functions	nn_functions_feature det_functions_the prep_of_one_functions amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_1 nn_Model_IBM prep_as_used_one dobj_used_Model nsubj_used_We rcmod_Score_used num_Score_1 nn_Score_Model num_Score_4.1
N04-4003	J93-2003	o	1 Introduction The statistical machine translation framework -LRB- SMT -RRB- formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability TM LM = argmaxT p -LRB- SjT -RRB- p -LRB- T -RRB- -LRB- 1 -RRB- where p -LRB- SjT -RRB- is called a translation model -LRB- TM -RRB- representing the generation probability from T into S p -LRB- T -RRB- is called a language model -LRB- LM -RRB- and represents the likelihood of the target language -LRB- Brown et al. 1993 -RRB-	nn_al._et dep_Brown_1993 advmod_Brown_al. nn_language_target det_language_the dep_likelihood_Brown prep_of_likelihood_language det_likelihood_the dobj_represents_likelihood nsubj_represents_p appos_model_LM nn_model_language det_model_a conj_and_called_represents xcomp_called_model auxpass_called_is nsubjpass_called_p appos_p_T prep_from_probability_T nn_probability_generation det_probability_the parataxis_representing_represents parataxis_representing_called prep_into_representing_S dobj_representing_probability vmod_model_representing appos_model_TM nn_model_translation det_model_a dobj_called_model auxpass_called_is nsubjpass_called_p advmod_called_where appos_p_SjT appos_p_T nn_p_p appos_p_SjT nn_p_argmaxT dep_=_p rcmod_LM_called appos_LM_1 amod_LM_= nn_LM_TM amod_probability_conditional det_probability_the prep_of_problem_probability nn_problem_maximization det_problem_the nn_T_language nn_T_target det_T_a nn_S_language nn_S_source det_S_a prep_into_sentence_T prep_from_sentence_S det_sentence_a prep_as_translating_problem dobj_translating_sentence prepc_of_problem_translating det_problem_the dobj_formulates_problem nsubj_formulates_framework appos_framework_SMT nn_framework_translation nn_framework_machine amod_framework_statistical det_framework_The dep_Introduction_LM rcmod_Introduction_formulates num_Introduction_1
N04-4015	J93-2003	o	Introduction Translation of two languages with highly different morphological structures as exemplified by Arabic and English poses a challenge to successful implementation of statistical machine translation models -LRB- Brown et al. 1993 -RRB-	nn_al._et dep_Brown_1993 advmod_Brown_al. nn_models_translation nn_models_machine amod_models_statistical prep_of_implementation_models amod_implementation_successful prep_to_challenge_implementation det_challenge_a dep_poses_Brown dobj_poses_challenge nsubj_poses_Translation conj_and_Arabic_English prep_by_exemplified_English prep_by_exemplified_Arabic mark_exemplified_as dep_structures_exemplified amod_structures_morphological amod_structures_different advmod_different_highly num_languages_two prep_with_Translation_structures prep_of_Translation_languages nn_Translation_Introduction
N04-4026	J93-2003	o	The orientation model is related to the distortion model in -LRB- Brown et al. 1993 -RRB- but we do not compute a block alignment during training	prep_during_alignment_training nn_alignment_block det_alignment_a dobj_compute_alignment neg_compute_not aux_compute_do nsubj_compute_we num_al._1993 nn_al._et amod_al._Brown dep_in_al. amod_model_in nn_model_distortion det_model_the conj_but_related_compute prep_to_related_model auxpass_related_is nsubjpass_related_model nn_model_orientation det_model_The
N06-1002	J93-2003	o	= = = = -RRB- -LRB- -RRB- -LRB- InverseM1 -RRB- -LRB- -RRB- -LRB- DirectM1 -RRB- -LRB- -RRB- -LRB- InverseMLE -RRB- -LRB- -RRB- -LRB- DirectMLE -RRB- | -LRB- -RRB- -LRB- -RRB- | -LRB- -RRB- -LRB- -RRB- -LRB- * -RRB- -LRB- -RRB- -LRB- * -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- Atreelets s t Atreelets t s Atreelets Atreelets tspATSf stpATSf c cATSf c cATSf We use word probability tables p -LRB- t | s -RRB- and p -LRB- s | t -RRB- estimated by IBM Model 1 -LRB- Brown et al. 1993 -RRB-	dep_1993_al. num_Brown_1993 nn_Brown_et num_Model_1 nn_Model_IBM dep_estimated_| dep_estimated_p dobj_|_t nsubj_|_s dep_s_Brown prep_by_s_Model conj_and_s_estimated num_s_| nn_s_t nn_s_p dep_tables_estimated dep_tables_s nn_tables_probability nn_tables_word dobj_use_tables nsubj_use_We rcmod_cATSf_use nn_cATSf_c nn_cATSf_cATSf nn_cATSf_c nn_cATSf_stpATSf nn_cATSf_tspATSf nn_cATSf_Atreelets nn_cATSf_Atreelets dobj_s_cATSf nn_t_Atreelets nn_t_t dep_s_s dobj_s_t nsubj_s_Atreelets dep_|_s dep_|_* appos_|_* appos_|_| nn_|_DirectMLE dep_InverseMLE_| dep_DirectM1_InverseMLE dep_InverseM1_DirectM1 dep_=_= dep_=_= dep_=_InverseM1 dep_=_= dep_''_=
N06-1003	J93-2003	n	By 17 0 10 20 30 40 50 60 70 80 90 100 10000 100000 1e +06 1e +07 Test Set Items with Translations -LRB- % -RRB- Training Corpus Size -LRB- num words -RRB- unigrams bigrams trigrams 4-grams Figure 1 Percent of unique unigrams bigrams trigrams and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation phrase-based machine translation does away with many of the problems associated with the original word-based formulation of statistical machine translation -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. nn_translation_machine amod_translation_statistical prep_of_formulation_translation amod_formulation_word-based amod_formulation_original det_formulation_the prep_with_associated_formulation vmod_problems_associated det_problems_the dep_many_Brown prep_of_many_problems prep_with_does_many prt_does_away nsubj_does_translation nn_translation_machine amod_translation_phrase-based prep_of_unit_translation amod_unit_basic det_unit_the prep_of_size_unit det_size_the dobj_increasing_size vmod_corpora_increasing nn_corpora_training amod_corpora_large advmod_large_increasingly prep_in_learned_corpora auxpass_learned_were nsubjpass_learned_translations prep_for_learned_which rcmod_sentences_learned nn_sentences_test nn_sentences_Spanish nn_sentences_Europarl det_sentences_the prep_from_4-grams_sentences conj_and_bigrams_4-grams conj_and_bigrams_trigrams amod_unigrams_unique rcmod_Percent_does appos_Percent_4-grams appos_Percent_trigrams appos_Percent_bigrams prep_of_Percent_unigrams prep_by_Percent_Test num_Figure_1 nn_Figure_4-grams nn_Figure_trigrams nn_Figure_bigrams nn_Figure_unigrams nn_words_num dep_Size_Figure appos_Size_words nn_Size_Corpus amod_Size_Training nn_Size_Translations appos_Translations_% prep_with_Set_Size dobj_Set_Items dep_Test_Set num_Test_+07 nn_Test_1e num_Test_+06 nn_Test_1e num_Test_100000 dep_Test_80 90 100 10000 num_Test_0 dep_80 90 100 10000_70 number_70_60 dep_70_50 number_50_40 dep_50_30 dep_30_20 number_20_10 number_0_17
N06-1003	J93-2003	p	1 Introduction As with many other statistical natural language processing tasks statistical machine translation -LRB- Brown et al. 1993 -RRB- produces high quality results when ample training data is available	cop_available_is nsubj_available_data advmod_available_when nn_data_training amod_data_ample advcl_results_available nsubj_results_quality amod_quality_high ccomp_produces_results nsubj_produces_translation ccomp_produces_Introduction num_al._1993 nn_al._et amod_al._Brown appos_translation_al. nn_translation_machine amod_translation_statistical nn_tasks_processing nn_tasks_language amod_tasks_natural amod_tasks_statistical amod_tasks_other amod_tasks_many pobj_with_tasks pcomp_As_with prep_Introduction_As num_Introduction_1
N06-1013	J93-2003	n	stance the IBM models -LRB- Brown et al. 1993 -RRB- can be improved by adding more context dependencies into the translation model using a ME framework rather than using only p -LRB- f j | e i -RRB- -LRB- Garcia-Varea et al. 2002 -RRB-	amod_Garcia-Varea_2002 dep_Garcia-Varea_al. nn_Garcia-Varea_et dep_i_e dep_i_| dep_i_p dep_i_using nn_|_j nn_|_f advmod_p_only dep_framework_Garcia-Varea conj_negcc_framework_i nn_framework_ME det_framework_a dobj_using_i dobj_using_framework nn_model_translation det_model_the nn_dependencies_context amod_dependencies_more vmod_adding_using prep_into_adding_model dobj_adding_dependencies agent_improved_adding auxpass_improved_be aux_improved_can nsubjpass_improved_models dep_improved_stance amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_IBM det_models_the
N06-1013	J93-2003	o	1 Introduction Word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation -LRB- MT -RRB- -LRB- Brown et al. 1993 Och and Ney 2003 Koehn et al. 2003 -RRB- but also has been shown useful for other applications such as construction of bilingual lexicons word-sense disambiguation projection of resources and crosslanguage information retrieval	nn_retrieval_information nn_retrieval_crosslanguage prep_of_projection_resources amod_disambiguation_word-sense conj_and_lexicons_retrieval conj_and_lexicons_projection conj_and_lexicons_disambiguation amod_lexicons_bilingual prep_of_construction_retrieval prep_of_construction_projection prep_of_construction_disambiguation prep_of_construction_lexicons prep_such_as_applications_construction amod_applications_other prep_for_useful_applications acomp_shown_useful auxpass_shown_been aux_shown_has advmod_shown_also nn_al._et nn_al._Koehn conj_and_Och_2003 conj_and_Och_Ney amod_Brown_2003 dep_Brown_al. dep_Brown_2003 dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_translation_MT nn_translation_machine amod_translation_statistical dep_step_Brown prep_of_step_translation amod_step_intermediate det_step_an advmod_step_usually nsubj_step_otheris det_otheris_each prep_of_translations_step cop_translations_are nsubj_translations_that rcmod_sentences_translations num_sentences_two prep_between_words_sentences amod_words_corresponding conj_but_alignmentdetection_shown prep_of_alignmentdetection_words nn_alignmentdetection_Word nn_alignmentdetection_Introduction num_alignmentdetection_1 dep_``_shown dep_``_alignmentdetection
N06-1056	J93-2003	o	More specifically a statistical word alignment model -LRB- Brown et al. 1993 -RRB- is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL	nn_MRL_target det_MRL_the prep_in_translations_MRL poss_translations_their prep_with_coupled_translations vmod_substrings_coupled nn_substrings_NL prep_of_consisting_substrings vmod_lexicon_consisting amod_lexicon_bilingual det_lexicon_a dobj_acquire_lexicon aux_acquire_to xcomp_used_acquire auxpass_used_is nsubjpass_used_model advmod_used_specifically num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_alignment nn_model_word amod_model_statistical det_model_a dep_specifically_More ccomp_``_used
N06-1056	J93-2003	o	In this work we use the GIZA + + implementation -LRB- Och and Ney 2003 -RRB- of IBM Model 5 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_5 nn_Model_IBM dep_Ney_2003 nn_Och_implementation preconj_Och_+ dep_GIZA_Brown prep_of_GIZA_Model conj_and_GIZA_Ney conj_+_GIZA_Och det_GIZA_the dobj_use_Ney dobj_use_Och dobj_use_GIZA nsubj_use_we prep_in_use_work det_work_this
N06-2051	J93-2003	n	Lexical relationships under the standard IBM models -LRB- Brown et al. 1993 -RRB- do not account for many-to-many mappings and phrase extraction relies heavily on the accuracy of the IBM word-toword alignment	nn_alignment_word-toword nn_alignment_IBM det_alignment_the prep_of_accuracy_alignment det_accuracy_the prep_on_relies_accuracy advmod_relies_heavily nsubj_relies_extraction nn_extraction_phrase amod_mappings_many-to-many conj_and_account_relies prep_for_account_mappings neg_account_not aux_account_do nsubj_account_relationships amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM amod_models_standard det_models_the dep_relationships_Brown prep_under_relationships_models amod_relationships_Lexical
N06-4004	J93-2003	o	Alignment quality can be further improved when the chunking procedure is based on translation lexicons from IBM Model-1 alignment model -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_model_alignment nn_model_Model-1 nn_model_IBM dep_lexicons_al. prep_from_lexicons_model nn_lexicons_translation prep_on_based_lexicons auxpass_based_is nsubjpass_based_procedure advmod_based_when amod_procedure_chunking det_procedure_the advcl_improved_based advmod_improved_further auxpass_improved_be aux_improved_can nsubjpass_improved_quality nn_quality_Alignment
N06-4004	J93-2003	o	MTTK provides implementations of various alignment models including IBM Model-1 Model-2 -LRB- Brown et al. 1993 -RRB- HMM-based word-to-word alignment model -LRB- Vogel et al. 1996 Och and Ney 2003 -RRB- and HMM-based word-to-phrase alignment model -LRB- Deng and Byrne 2005 -RRB-	num_Deng_2005 conj_and_Deng_Byrne appos_model_Byrne appos_model_Deng nn_model_alignment nn_model_word-to-phrase amod_model_HMM-based dep_Och_2003 conj_and_Och_Ney dep_Vogel_Ney dep_Vogel_Och amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et nn_model_alignment nn_model_word-to-word amod_model_HMM-based num_al._1993 nn_al._et amod_al._Brown dep_Model-2_al. conj_and_Model-1_model dep_Model-1_Vogel conj_and_Model-1_model conj_and_Model-1_Model-2 nn_Model-1_IBM prep_including_models_model prep_including_models_model prep_including_models_Model-2 prep_including_models_Model-1 amod_alignment_various appos_implementations_models prep_of_implementations_alignment dobj_provides_implementations nsubj_provides_MTTK
N06-4004	J93-2003	o	Atthefinestlevel thisinvolvesthealignment of words and phrases within two sentences that are known to be translations -LRB- Brown et al. 1993 Och and Ney 2003 Vogel et al. 1996 Deng and Byrne 2005 -RRB-	num_Vogel_1996 nn_Vogel_al. nn_Vogel_et appos_Och_2005 conj_and_Och_Byrne conj_and_Och_Deng conj_and_Och_Vogel conj_and_Och_2003 conj_and_Och_Ney dep_Brown_Byrne dep_Brown_Deng dep_Brown_Vogel dep_Brown_2003 dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_translations_Brown cop_translations_be aux_translations_to xcomp_known_translations auxpass_known_are nsubjpass_known_that rcmod_sentences_known num_sentences_two conj_and_words_phrases prep_within_thisinvolvesthealignment_sentences prep_of_thisinvolvesthealignment_phrases prep_of_thisinvolvesthealignment_words dep_Atthefinestlevel_thisinvolvesthealignment
N07-1008	J93-2003	o	1.2 Statistical modeling for translation Earlier work in statistical machine translation -LRB- Brown et al. 1993 -RRB- is based on the noisy-channel formulation where T = arg max T p -LRB- TjS -RRB- = argmax T p -LRB- T -RRB- p -LRB- SjT -RRB- -LRB- 1 -RRB- where the target language model p -LRB- T -RRB- is further decomposed as p -LRB- T -RRB- / productdisplay i p -LRB- tijti1 ... tik +1 -RRB- where k is the order of the language model and the translation model p -LRB- SjT -RRB- has been modeled by a sequence of five models with increasing complexity -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_complexity_al. amod_complexity_increasing num_models_five prep_of_sequence_models det_sequence_a prep_with_modeled_complexity agent_modeled_sequence auxpass_modeled_been aux_modeled_has nsubjpass_modeled_model appos_p_SjT dep_model_p nn_model_translation det_model_the nn_model_language det_model_the conj_and_order_modeled prep_of_order_model det_order_the cop_order_is nsubj_order_k advmod_order_where nn_+1_tik dep_tijti1_+1 rcmod_p_modeled rcmod_p_order appos_p_tijti1 nn_p_i nn_p_productdisplay appos_p_T prep_as_decomposed_p dep_further_p dep_further_decomposed nsubj_is_further dep_p_is appos_p_T nn_p_model nn_p_language nn_p_target det_p_the dep_where_p dep_1_where dep_p_1 appos_p_SjT nn_p_p appos_p_T nn_p_T nn_p_argmax tmod_=_p amod_p_= appos_p_TjS nn_p_T nn_p_max nn_p_arg dep_=_p amod_T_= dep_where_T dep_formulation_where amod_formulation_noisy-channel det_formulation_the prep_on_based_formulation auxpass_based_is nsubjpass_based_work num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical dep_work_al. prep_in_work_translation advmod_work_Earlier nsubj_work_modeling prep_for_modeling_translation amod_modeling_Statistical num_modeling_1.2
N07-1008	J93-2003	o	The translation model is estimated via the EM algorithm or approximations that are bootstrapped from the previous model in the sequence as introduced in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_introduced_in mark_introduced_as det_sequence_the prep_in_model_sequence amod_model_previous det_model_the advcl_bootstrapped_introduced prep_from_bootstrapped_model auxpass_bootstrapped_are nsubjpass_bootstrapped_that rcmod_algorithm_bootstrapped conj_or_algorithm_approximations nn_algorithm_EM det_algorithm_the prep_via_estimated_approximations prep_via_estimated_algorithm auxpass_estimated_is nsubjpass_estimated_model nn_model_translation det_model_The ccomp_``_estimated
N07-1008	J93-2003	o	3 A Categorization of Block Styles In -LRB- Brown et al. 1993 -RRB- multi-word cepts -LRB- which are realized in our block concept -RRB- are discussed and the authors state that when a target sequence is sufficiently different from a word by word translation only then should the target sequence should be promoted to a cept	det_cept_a prep_to_promoted_cept auxpass_promoted_be aux_promoted_should nsubjpass_promoted_sequence nn_sequence_target det_sequence_the ccomp_should_promoted advmod_should_then advmod_then_only nn_translation_word prep_by_word_translation det_word_a parataxis_different_should prep_from_different_word advmod_different_sufficiently cop_different_is nsubj_different_sequence advmod_different_when nn_sequence_target det_sequence_a advcl_that_different dep_state_that nsubj_state_authors det_authors_the conj_and_discussed_state auxpass_discussed_are nsubjpass_discussed_cepts prep_discussed_In nsubjpass_discussed_Categorization nn_concept_block poss_concept_our prep_in_realized_concept auxpass_realized_are nsubjpass_realized_which dep_cepts_realized amod_cepts_multi-word num_al._1993 nn_al._et amod_al._Brown dep_In_al. nn_Styles_Block prep_of_Categorization_Styles det_Categorization_A num_Categorization_3
N07-1008	J93-2003	o	Following the perspective of -LRB- Brown et al. 1993 -RRB- a minimal set of phrase blocks with lengths -LRB- m n -RRB- where either m or n must be greater than zero results in the following types of blocks 1	prep_of_types_blocks amod_types_following det_types_the prep_in_results_types num_results_zero prep_than_greater_results cop_greater_be aux_greater_must nsubj_greater_n nsubj_greater_m advmod_greater_where conj_or_m_n preconj_m_either appos_m_n rcmod_lengths_greater dep_lengths_m prep_with_blocks_lengths nsubj_blocks_set prep_of_set_phrase amod_set_minimal det_set_a dep_al._1 rcmod_al._blocks num_al._1993 nn_al._et amod_al._Brown prep_of_perspective_al. det_perspective_the pobj_Following_perspective ccomp_``_Following
N07-1022	J93-2003	n	Compared to earlier word-based methods such as IBM Models -LRB- Brown et al. 1993 -RRB- phrasebased methods such as PHARAOH are much more effective in producing idiomatic translations and are currently the best performing methods in SMT -LRB- Koehn and Monz 2006 -RRB-	dep_Koehn_2006 conj_and_Koehn_Monz dep_SMT_Monz dep_SMT_Koehn prep_in_methods_SMT amod_methods_performing dep_best_methods det_best_the advmod_best_currently cop_best_are nsubj_best_methods amod_translations_idiomatic dobj_producing_translations conj_and_effective_best prepc_in_effective_producing advmod_effective_more cop_effective_are nsubj_effective_methods vmod_effective_Compared advmod_more_much prep_such_as_methods_PHARAOH amod_methods_phrasebased amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Models_Brown nn_Models_IBM prep_such_as_methods_Models amod_methods_word-based amod_methods_earlier prep_to_Compared_methods
N07-1022	J93-2003	o	These rules are learned using a word alignment model which finds an optimal mapping from words to MR predicates given a set of training sentences and their correct MRs. Word alignment models have been widely used for lexical acquisition in SMT -LRB- Brown et al. 1993 Koehn et al. 2003 -RRB-	nn_al._et nn_al._Koehn amod_Brown_2003 dep_Brown_al. amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_SMT_Brown prep_in_acquisition_SMT amod_acquisition_lexical prep_for_used_acquisition advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_model nn_models_alignment nn_models_Word nn_models_MRs. amod_models_correct poss_models_their nn_sentences_training conj_and_set_models prep_of_set_sentences det_set_a pobj_given_models pobj_given_set prep_predicates_given prep_to_mapping_MR prep_from_mapping_words amod_mapping_optimal det_mapping_an dep_finds_predicates dobj_finds_mapping nsubj_finds_which rcmod_model_finds nn_model_alignment nn_model_word det_model_a ccomp_using_used xcomp_learned_using auxpass_learned_are nsubjpass_learned_rules det_rules_These ccomp_``_learned
N07-1046	J93-2003	o	4.1.3 Letter Lexical Transliteration Similar to IBM Model-1 -LRB- Brown et al. 1993 -RRB- we use a bag-of-letter generative model within a block to approximate the lexical transliteration equivalence P -LRB- fj + lj | ei + ki -RRB- = j + lproductdisplay jprime = j i + ksummationdisplay iprime = i P -LRB- fjprime | eiprime -RRB- P -LRB- eiprime | ei + ki -RRB- -LRB- 10 -RRB- where P -LRB- eiprime | ei + ki -RRB- similarequal 1 / -LRB- k +1 -RRB- is approximated by a bagof-word unigram	amod_unigram_bagof-word det_unigram_a agent_approximated_unigram auxpass_approximated_is nsubjpass_approximated_P advmod_approximated_where num_k_+1 appos_similarequal_k num_similarequal_1 conj_+_ei_ki num_ei_| nn_ei_eiprime dep_P_similarequal dep_P_ki dep_P_ei conj_+_ei_ki num_ei_| nn_ei_eiprime rcmod_P_approximated appos_P_10 dep_P_ki dep_P_ei nn_P_fjprime num_eiprime_| appos_fjprime_eiprime dep_P_P nn_P_i dobj_=_P dep_iprime_= dep_ksummationdisplay_iprime conj_+_i_ksummationdisplay amod_j_ksummationdisplay amod_j_i dep_=_j amod_jprime_= nn_jprime_lproductdisplay nn_jprime_j conj_+_j_lproductdisplay dep_=_jprime num_ei_| nn_ei_lj conj_+_fj_ki conj_+_fj_ei amod_P_= dep_P_ki dep_P_ei dep_P_fj dep_transliteration_P amod_transliteration_equivalence amod_transliteration_lexical det_transliteration_the dobj_approximate_transliteration prep_to_block_approximate det_block_a amod_model_generative nn_model_bag-of-letter det_model_a prep_within_use_block dobj_use_model nsubj_use_we ccomp_use_Transliteration num_al._1993 nn_al._et amod_al._Brown appos_Model-1_al. nn_Model-1_IBM prep_to_Similar_Model-1 amod_Transliteration_Similar amod_Transliteration_Lexical nn_Transliteration_Letter num_Transliteration_4.1.3
N07-1046	J93-2003	o	Standard SMT alignment models -LRB- Brown et al. 1993 -RRB- are used to align letter-pairs within named entity pairs for transliteration	prep_for_pairs_transliteration nn_pairs_entity dobj_named_pairs prepc_within_letter-pairs_named dobj_align_letter-pairs aux_align_to xcomp_used_align auxpass_used_are nsubjpass_used_models amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_alignment nn_models_SMT nn_models_Standard
N07-1046	J93-2003	o	3 Bi-Stream HMMs for Transliteration Standard IBM translation models -LRB- Brown et al. 1993 -RRB- can be used to obtain letter-to-letter translations	amod_translations_letter-to-letter dobj_obtain_translations aux_obtain_to xcomp_used_obtain auxpass_used_be aux_used_can nsubjpass_used_HMMs amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation nn_models_IBM nn_models_Standard nn_models_Transliteration appos_HMMs_Brown prep_for_HMMs_models amod_HMMs_Bi-Stream num_HMMs_3
N07-1057	J93-2003	o	We then train IBM models -LRB- Brown et al. 1993 -RRB- using the GIZA + + package -LRB- Och and Ney 2000 -RRB-	nn_Och_package preconj_Och_+ appos_GIZA_2000 conj_and_GIZA_Ney conj_+_GIZA_Och det_GIZA_the dobj_using_Ney dobj_using_Och dobj_using_GIZA amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM xcomp_train_using dep_train_Brown dobj_train_models advmod_train_then nsubj_train_We ccomp_``_train
N07-1061	J93-2003	o	1 Introduction The rapid and steady progress in corpus-based machine translation -LRB- Nagao 1981 Brown et al. 1993 -RRB- has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus -LRB- Koehn 2005 -RRB- which consists of 11 European languages	amod_languages_European num_languages_11 prep_of_consists_languages nsubj_consists_which dep_Koehn_2005 appos_corpus_Koehn nn_corpus_Europarl det_corpus_the rcmod_Consortium_consists conj_and_Consortium_corpus nn_Consortium_Data nn_Consortium_Linguistic det_Consortium_the agent_distributed_corpus agent_distributed_Consortium vmod_corpora_distributed nn_corpora_parallel amod_corpora_Chinese-English amod_corpora_Arabic-English det_corpora_the conj_and_Arabic-English_Chinese-English prep_such_as_corpora_corpora amod_corpora_parallel amod_corpora_large agent_supported_corpora auxpass_supported_been aux_supported_has nsubjpass_supported_progress num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Nagao_Brown appos_Nagao_1981 appos_translation_Nagao nn_translation_machine amod_translation_corpus-based prep_in_progress_translation amod_progress_steady amod_progress_rapid det_progress_The conj_and_rapid_steady rcmod_Introduction_supported num_Introduction_1
N07-1064	J93-2003	o	To improve raw output from decoding Portage relies on a rescoring strategy given a list of n-best translations from the decoder the system reorders this list this time using a more elaborate loglinear model incorporating more feature functions in addition to those of the decoding model these typically include IBM-1 and IBM-2 model probabilities -LRB- Brown et al. 1993 -RRB- and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language all of these feature functions can be used in both language directions i.e. source-to-target and target-to-source	conj_and_source-to-target_target-to-source pobj_i.e._target-to-source pobj_i.e._source-to-target prep_directions_i.e. nn_directions_language preconj_directions_both prep_in_used_directions auxpass_used_be aux_used_can nsubjpass_used_all nn_functions_feature det_functions_these prep_of_all_functions amod_language_other det_language_the prep_in_translation_language amod_translation_satisfactory prep_without_left_translation auxpass_left_been aux_left_have aux_left_to xcomp_appears_left nsubj_appears_word mark_appears_whether num_language_one prep_in_word_language det_word_any ccomp_detect_appears aux_detect_to xcomp_designed_detect vmod_function_designed nn_function_feature amod_function_IBM-1-based det_function_an amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_probabilities_function dep_probabilities_Brown nn_probabilities_model nn_probabilities_IBM-2 nn_probabilities_IBM-1 conj_and_IBM-1_IBM-2 parataxis_include_used dobj_include_function dobj_include_probabilities advmod_include_typically nsubj_include_these amod_model_decoding det_model_the prep_of_those_model nn_functions_feature amod_functions_more parataxis_incorporating_include prep_in_addition_to_incorporating_those dobj_incorporating_functions amod_model_loglinear amod_model_elaborate det_model_a advmod_elaborate_more dobj_using_model dep_time_incorporating vmod_time_using det_time_this det_list_this prep_reorders_time dobj_reorders_list nsubj_reorders_system vmod_reorders_given det_system_the det_decoder_the prep_from_translations_decoder amod_translations_n-best prep_of_list_translations det_list_a dobj_given_list nn_strategy_rescoring det_strategy_a parataxis_relies_reorders prep_on_relies_strategy nsubj_relies_Portage advcl_relies_improve prepc_from_output_decoding amod_output_raw dobj_improve_output aux_improve_To
N07-2009	J93-2003	o	3 GM Representation of IBM MT Models In this section we present a GM representation for IBM model 3 -LRB- Brown et al. 1993 -RRB- in fig	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_3 nn_model_IBM prep_in_representation_fig dep_representation_Brown prep_for_representation_model nn_representation_GM det_representation_a dobj_present_representation nsubj_present_we det_section_this nn_Models_MT nn_Models_IBM rcmod_Representation_present prep_in_Representation_section prep_of_Representation_Models nn_Representation_GM num_Representation_3 dep_``_Representation
N07-2009	J93-2003	o	We attribute the difference in M3/4 scores to the fact we use a Viterbi-like training procedure -LRB- i.e. we consider a single configuration of the hidden variables in EM training -RRB- while GIZA uses pegging -LRB- Brown et al. 1993 -RRB- to sum over a set of likely hidden variable configurations in EM	prep_in_configurations_EM amod_configurations_variable amod_configurations_hidden amod_configurations_likely prep_of_set_configurations det_set_a prep_over_sum_set aux_sum_to amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_pegging_Brown vmod_uses_sum dobj_uses_pegging nsubj_uses_GIZA mark_uses_while nn_training_EM prep_in_variables_training amod_variables_hidden det_variables_the prep_of_configuration_variables amod_configuration_single det_configuration_a dobj_consider_configuration nsubj_consider_we advmod_consider_i.e. dep_procedure_consider nn_procedure_training amod_procedure_Viterbi-like det_procedure_a advcl_use_uses dobj_use_procedure nsubj_use_we rcmod_fact_use det_fact_the prep_to_scores_fact nn_scores_M3/4 prep_in_difference_scores det_difference_the dobj_attribute_difference nsubj_attribute_We
N07-2010	J93-2003	o	Similar to work in image retrieval -LRB- Barnard et al. 2003 -RRB- we cast the problem in terms of Machine Translation given a paired corpus of words and a set of video event representations to which they refer we make the IBM Model 1 assumption and use the expectation-maximization method to estimate the parameters -LRB- Brown et al. 1993 -RRB- = + = m j ajm jvideowordpl Cvideowordp 1 -RRB- | -LRB- -RRB- 1 -LRB- -RRB- | -LRB- -LRB- 1 -RRB- This paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above	prep_described_above vmod_streams_described nn_streams_feature det_streams_the det_video_each prep_into_abstracting_streams dobj_abstracting_video vmod_first_abstracting amod_video_raw prep_of_corpus_video det_corpus_a agent_created_first prep_from_created_corpus auxpass_created_is nsubjpass_created_corpus dep_created_1 amod_corpus_paired det_corpus_This ccomp_|_created num_|_1 dep_|_| dep_|_= dep_|_= nn_Cvideowordp_jvideowordpl nn_Cvideowordp_ajm nn_Cvideowordp_j nn_Cvideowordp_m dep_=_1 dep_=_Cvideowordp conj_+_=_= amod_Brown_1993 dep_Brown_al. nn_Brown_et det_parameters_the dobj_estimate_parameters aux_estimate_to nn_method_expectation-maximization det_method_the vmod_use_estimate dobj_use_method nsubj_use_we num_assumption_1 nn_assumption_Model nn_assumption_IBM det_assumption_the dep_make_| dep_make_Brown conj_and_make_use dobj_make_assumption nsubj_make_we prep_make_given nsubj_refer_they prep_to_refer_which nn_representations_event nn_representations_video rcmod_set_refer prep_of_set_representations det_set_a conj_and_corpus_set prep_of_corpus_words amod_corpus_paired det_corpus_a pobj_given_set pobj_given_corpus nn_Translation_Machine prep_of_terms_Translation det_problem_the parataxis_cast_use parataxis_cast_make prep_in_cast_terms dobj_cast_problem nsubj_cast_we advmod_cast_Similar amod_Barnard_2003 dep_Barnard_al. nn_Barnard_et nn_retrieval_image prep_in_work_retrieval aux_work_to dep_Similar_Barnard xcomp_Similar_work
N07-2022	J93-2003	o	1 Introduction In the first SMT systems -LRB- Brown et al. 1993 -RRB- word alignment was introduced as a hidden variable of the translation model	nn_model_translation det_model_the prep_of_variable_model amod_variable_hidden det_variable_a prep_as_introduced_variable auxpass_introduced_was nsubjpass_introduced_Introduction nn_alignment_word amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_systems_SMT amod_systems_first det_systems_the appos_Introduction_alignment dep_Introduction_Brown prep_in_Introduction_systems num_Introduction_1
N07-2034	J93-2003	o	A monotonous segmentation copes with monotonous alignments that is j < k aj < ak following the notation of -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_of_al. prep_notation_of det_notation_the prep_following_ak_notation quantmod_ak_< dep_aj_ak nn_aj_k dep_<_aj amod_j_< nsubj_is_j nsubj_is_that ccomp_is_copes amod_alignments_monotonous prep_with_copes_alignments nsubj_copes_segmentation amod_segmentation_monotonous det_segmentation_A
N09-1013	J93-2003	o	Standard CI Model 1 training initialised with a uniform translation table so that t -LRB- ejf -RRB- is constant for all source/target word pairs -LRB- f e -RRB- was run on untagged data for 10 iterations in each direction -LRB- Brown et al. 1993 Deng and Byrne 2005b -RRB-	appos_Deng_2005b conj_and_Deng_Byrne conj_al._1993 nn_al._et dep_Brown_Byrne dep_Brown_Deng advmod_Brown_al. det_direction_each num_iterations_10 prep_for_data_iterations amod_data_untagged dep_run_Brown prep_in_run_direction prep_on_run_data auxpass_run_was nsubjpass_run_training dep_f_e appos_pairs_f nn_pairs_word amod_pairs_source/target det_pairs_all prep_for_constant_pairs cop_constant_is nsubj_constant_t mark_constant_that mark_constant_so appos_t_ejf nn_table_translation amod_table_uniform det_table_a advcl_initialised_constant prep_with_initialised_table vmod_training_initialised num_training_1 nn_training_Model nn_training_CI nn_training_Standard
N09-1013	J93-2003	o	-LRB- 1993 -RRB- introduce IBM Models 1-5 for alignment modelling Vogel et al.	dep_Vogel_al. nn_Vogel_et nn_modelling_alignment prep_for_1-5_modelling nn_Models_IBM dep_introduce_1-5 dobj_introduce_Models dep_1993_Vogel dep_1993_introduce
N09-1013	J93-2003	o	2.1 EM parameter estimation We train using Expectation Maximisation -LRB- EM -RRB- optimising the log probability of the training setfe -LRB- s -RRB- f -LRB- s -RRB- gSs = 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_=_1 dep_gSs_Brown amod_gSs_= dep_s_gSs nn_s_f appos_s_s nn_s_setfe nn_s_training det_s_the prep_of_probability_s nn_probability_log det_probability_the dobj_optimising_probability vmod_Maximisation_optimising appos_Maximisation_EM nn_Maximisation_Expectation dobj_using_Maximisation xcomp_train_using nsubj_train_We rcmod_estimation_train nn_estimation_parameter nn_estimation_EM num_estimation_2.1 dep_``_estimation
N09-1013	J93-2003	o	Then P -LRB- eI1jfj1 -RRB- = summationtextaI 1 P -LRB- eI1 aI1jfj1 -RRB- -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_eI1_aI1jfj1 appos_P_Brown dep_P_eI1 num_P_1 nn_P_summationtextaI dobj_=_P nsubj_=_P advmod_=_Then appos_P_eI1jfj1
N09-1025	J93-2003	o	Following previous work in statistical MT -LRB- Brown et al. 1993 -RRB- we envision a noisy-channel model in which a language model generates English and then a translation model transforms English trees into Chinese	nn_trees_English prep_into_transforms_Chinese dobj_transforms_trees nsubj_transforms_model nn_model_translation det_model_a advmod_model_then dobj_generates_English nsubj_generates_model prep_in_generates_which nn_model_language det_model_a rcmod_model_generates amod_model_noisy-channel det_model_a conj_and_envision_transforms nsubj_envision_model nsubj_envision_we ccomp_envision_Following num_al._1993 nn_al._et amod_al._Brown amod_MT_statistical prep_in_work_MT amod_work_previous dep_Following_al. pobj_Following_work
N09-2002	J93-2003	o	2 IBM Model 4 In this paper we focus on the translation model defined by IBM Model 4 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM agent_defined_Model vmod_model_defined nn_model_translation det_model_the dep_focus_Brown prep_on_focus_model nsubj_focus_we dep_focus_Model det_paper_this prep_in_4_paper dep_Model_4 nn_Model_IBM num_Model_2 ccomp_``_focus
N09-2005	J93-2003	o	The triplet lexicon model presented in this work can also be interpreted as an extension of the standard IBM model 1 -LRB- Brown et al. 1993 -RRB- with an additional trigger	amod_trigger_additional det_trigger_an amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_IBM amod_model_standard det_model_the prep_of_extension_model det_extension_an prep_with_interpreted_trigger dep_interpreted_Brown prep_as_interpreted_extension auxpass_interpreted_be advmod_interpreted_also aux_interpreted_can nsubjpass_interpreted_model det_work_this prep_in_presented_work vmod_model_presented nn_model_lexicon nn_model_triplet det_model_The
N09-2024	J93-2003	o	In this paper sentence pairs are extracted by a simple model that is based on the so-called IBM Model1 -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_Model1_al. nn_Model1_IBM amod_Model1_so-called det_Model1_the prep_on_based_Model1 auxpass_based_is nsubjpass_based_that rcmod_model_based amod_model_simple det_model_a agent_extracted_model auxpass_extracted_are nsubjpass_extracted_pairs prep_in_extracted_paper nn_pairs_sentence det_paper_this
N09-2055	J93-2003	o	The translation problem can be statistically formulated as in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. pcomp_as_in prep_formulated_as advmod_formulated_statistically auxpass_formulated_be aux_formulated_can nsubjpass_formulated_problem nn_problem_translation det_problem_The
P00-1041	J93-2003	o	The work reported in this paper is most closely related to work on statistical machine translation particularly the IBM-style work on CANDIDE -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_CANDIDE_Brown prep_on_work_CANDIDE amod_work_IBM-style det_work_the advmod_work_particularly nn_translation_machine amod_translation_statistical prep_on_work_translation aux_work_to appos_related_work xcomp_related_work advmod_related_closely cop_related_is nsubj_related_work advmod_closely_most det_paper_this prep_in_reported_paper vmod_work_reported det_work_The
P01-1008	J93-2003	o	Examples of such contexts are verb-object relations and noun-modifier relations which were traditionally used in word similarity tasks from non-parallel corpora -LRB- Pereira et al. 1993 Hatzivassiloglou and McKeown 1993 -RRB-	amod_Hatzivassiloglou_1993 conj_and_Hatzivassiloglou_McKeown dep_Pereira_McKeown dep_Pereira_Hatzivassiloglou appos_Pereira_1993 dep_Pereira_al. nn_Pereira_et amod_corpora_non-parallel prep_from_tasks_corpora nn_tasks_similarity nn_tasks_word dep_used_Pereira prep_in_used_tasks advmod_used_traditionally auxpass_used_were nsubjpass_used_which amod_relations_noun-modifier rcmod_relations_used conj_and_relations_relations amod_relations_verb-object cop_relations_are nsubj_relations_Examples amod_contexts_such prep_of_Examples_contexts
P01-1008	J93-2003	o	This characteristic of our corpus is similar to problems with noisy and comparable corpora -LRB- Veronis 2000 -RRB- and it prevents us from using methods developed in the MT community based on clean parallel corpora such as -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_as_al. mwe_as_such amod_corpora_as amod_corpora_parallel amod_corpora_clean nn_community_MT det_community_the prep_based_on_developed_corpora prep_in_developed_community vmod_methods_developed dobj_using_methods prepc_from_prevents_using dobj_prevents_us nsubj_prevents_it dep_Veronis_2000 appos_corpora_Veronis dep_noisy_corpora conj_and_noisy_comparable prep_with_problems_comparable prep_with_problems_noisy conj_and_similar_prevents prep_to_similar_problems cop_similar_is nsubj_similar_characteristic poss_corpus_our prep_of_characteristic_corpus det_characteristic_This ccomp_``_prevents ccomp_``_similar
P01-1008	J93-2003	o	We also record for each token its derivational root using the CELEX -LRB- Baayen et al. 1993 -RRB- database	dep_database_Baayen nn_database_CELEX dep_Baayen_1993 dep_Baayen_al. nn_Baayen_et det_CELEX_the dobj_using_database amod_root_derivational poss_root_its amod_root_token det_root_each vmod_record_using prep_for_record_root advmod_record_also nsubj_record_We ccomp_``_record
P01-1026	J93-2003	o	P -LRB- d -RRB- P L -LRB- d -RRB- -LRB- 4 -RRB- Statistical approaches to language modeling have been used in much NLP research such as machine translation -LRB- Brown et al. 1993 -RRB- and speech recognition -LRB- Bahl et al. 1983 -RRB-	amod_Bahl_1983 dep_Bahl_al. nn_Bahl_et dep_recognition_Bahl nn_recognition_speech num_al._1993 nn_al._et amod_al._Brown conj_and_translation_recognition dep_translation_al. nn_translation_machine prep_such_as_research_recognition prep_such_as_research_translation nn_research_NLP amod_research_much prep_in_used_research auxpass_used_been aux_used_have nsubjpass_used_approaches nn_modeling_language prep_to_approaches_modeling amod_approaches_Statistical dep_approaches_4 nn_approaches_L appos_L_d nn_L_P nn_L_P appos_P_d
P01-1027	J93-2003	o	Similar techniques are used in -LRB- Papineni et al. 1996 Papineni et al. 1998 -RRB- for socalled direct translation models instead of those proposed in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_proposed_in vmod_those_proposed conj_negcc_models_those nn_models_translation amod_models_direct amod_models_socalled num_Papineni_1998 nn_Papineni_al. nn_Papineni_et dep_Papineni_Papineni appos_Papineni_1996 dep_Papineni_al. nn_Papineni_et prep_for_used_those prep_for_used_models prep_in_used_Papineni auxpass_used_are nsubjpass_used_techniques amod_techniques_Similar
P01-1027	J93-2003	o	If we assign a probability a15a17a16a19a18 a12 a13a7a21a20a4a6a5a7a23a22 to each pair of strings a18 a12a14a13a7a25a24 a4 a5a7 a22 then according to Bayes decision rule we have to choose the target string that maximizes the product of the target language model a15a17a16a19a18 a12a14a13a7 a22 and the string translation model a15a17a16a19a18a26a4a6a5 a7 a20 a12 a13 a7 a22 Many existing systems for statistical machine translation -LRB- Berger et al. 1994 Wang and Waibel 1997 Tillmann et al. 1997 Nieen et al. 1998 -RRB- make use of a special way of structuring the string translation model like proposed by -LRB- Brown et al. 1993 -RRB- The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position	nn_position_word nn_position_source det_position_each nn_position_word nn_position_target num_position_one prep_to_assign_position dobj_assign_position nsubj_assign_that rcmod_alignments_assign agent_described_alignments auxpass_described_is nsubjpass_described_correspondence nn_string_target det_string_the det_source_the conj_and_words_string prep_in_words_source det_words_the prep_between_correspondence_string prep_between_correspondence_words det_correspondence_The num_al._1993 nn_al._et amod_al._Brown dep_by_al. prep_proposed_by nn_model_translation nn_model_string det_model_the prep_like_structuring_proposed dobj_structuring_model prepc_of_way_structuring amod_way_special det_way_a prep_of_use_way parataxis_make_described dobj_make_use nsubj_make_Tillmann nsubj_make_1997 nsubj_make_Waibel nsubj_make_Wang num_Nieen_1998 nn_Nieen_al. nn_Nieen_et num_Tillmann_1997 nn_Tillmann_al. nn_Tillmann_et dep_Wang_Nieen conj_and_Wang_Tillmann conj_and_Wang_1997 conj_and_Wang_Waibel dep_Berger_make appos_Berger_1994 dep_Berger_al. nn_Berger_et nn_translation_machine amod_translation_statistical dep_systems_Berger prep_for_systems_translation amod_systems_existing amod_systems_Many nn_a22_a7 nn_a22_a13 nn_a22_a12 nn_a22_a20 nn_a22_a7 nn_a22_a15a17a16a19a18a26a4a6a5 nn_a22_model nn_a22_translation nn_a22_string det_a22_the conj_and_a22_a22 nn_a22_a12a14a13a7 nn_a22_a15a17a16a19a18 nn_a22_model nn_a22_language nn_a22_target det_a22_the prep_of_product_a22 prep_of_product_a22 det_product_the dobj_maximizes_product nsubj_maximizes_that rcmod_string_maximizes nn_string_target det_string_the dobj_choose_string aux_choose_to dobj_have_systems xcomp_have_choose nsubj_have_we pobj_have_rule prepc_according_to_have_to advmod_have_then advcl_have_assign nn_rule_decision nn_rule_Bayes nn_a22_a5a7 nn_a22_a4 nn_a22_a12a14a13a7a25a24 nn_a22_a18 nn_a22_strings prep_of_pair_a22 det_pair_each nn_a13a7a21a20a4a6a5a7a23a22_a12 nn_a13a7a21a20a4a6a5a7a23a22_a15a17a16a19a18 nn_a13a7a21a20a4a6a5a7a23a22_probability det_a13a7a21a20a4a6a5a7a23a22_a prep_to_assign_pair dobj_assign_a13a7a21a20a4a6a5a7a23a22 nsubj_assign_we mark_assign_If
P01-1027	J93-2003	o	That is obtained using the Viterbi alignment provided by a translation model as described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_as nn_model_translation det_model_a advcl_provided_described agent_provided_model vmod_alignment_provided nn_alignment_Viterbi det_alignment_the dobj_using_alignment xcomp_obtained_using auxpass_obtained_is nsubjpass_obtained_That
P01-1027	J93-2003	o	This is exactly the standard lexicon probability a27a28a18a26a4 a20a12 a22 employed in the translation model described in -LRB- Brown et al. 1993 -RRB- and in Section 2	num_Section_2 conj_and_al._Section num_al._1993 nn_al._et amod_al._Brown prep_in_described_Section prep_in_described_al. vmod_model_described nn_model_translation det_model_the prep_in_employed_model vmod_a22_employed nn_a22_a20a12 nn_a22_a27a28a18a26a4 nn_a22_probability nn_a22_lexicon amod_a22_standard det_a22_the pobj_exactly_a22 prep_is_exactly amod_This_is dep_``_This
P01-1050	J93-2003	o	In this framework the source language let-s say English is assumed to be generated by a noisy probabilistic source .1 Most of the current statistical MT systems treat this source as a sequence of words -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_of_sequence_words det_sequence_a det_source_this prep_as_treat_sequence dobj_treat_source nsubj_treat_Most nn_systems_MT amod_systems_statistical amod_systems_current det_systems_the prep_of_Most_systems rcmod_source_treat num_source_.1 amod_source_probabilistic amod_source_noisy det_source_a agent_generated_source auxpass_generated_be aux_generated_to dep_assumed_Brown xcomp_assumed_generated auxpass_assumed_is nsubjpass_assumed_English parataxis_assumed_say nsubjpass_assumed_language prep_in_assumed_framework nsubj_say_let-s nn_language_source det_language_the det_framework_this
P01-1050	J93-2003	o	First we show how one can use an existing statistical translation model -LRB- Brown et al. 1993 -RRB- in order to automatically derive a statistical TMEM	amod_TMEM_statistical det_TMEM_a dobj_derive_TMEM advmod_derive_automatically aux_derive_to dep_derive_order mark_derive_in num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation amod_model_statistical amod_model_existing det_model_an advcl_use_derive dobj_use_model aux_use_can nsubj_use_one advmod_use_how ccomp_show_use nsubj_show_we advmod_show_First
P01-1050	J93-2003	o	2 The IBM Model 4 For the work described in this paper we used a modified version of the statistical machine translation tool developed in the context of the 1999 Johns HopkinsSummer Workshop -LRB- Al-Onaizan et al. 1999 -RRB- which implements IBM translation model 4 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_translation nn_model_IBM dobj_implements_model nsubj_implements_which amod_Al-Onaizan_1999 dep_Al-Onaizan_al. nn_Al-Onaizan_et dep_Workshop_Brown rcmod_Workshop_implements dep_Workshop_Al-Onaizan nn_Workshop_HopkinsSummer nn_Workshop_Johns num_Workshop_1999 det_Workshop_the prep_of_context_Workshop det_context_the prep_in_developed_context nsubj_developed_Model nn_tool_translation nn_tool_machine amod_tool_statistical det_tool_the prep_of_version_tool amod_version_modified det_version_a dobj_used_version nsubj_used_we det_paper_this prep_in_described_paper vmod_work_described det_work_the rcmod_Model_used prep_for_Model_work num_Model_4 nn_Model_IBM det_Model_The num_Model_2 ccomp_``_developed
P01-1050	J93-2003	o	a65 The rest of the factors denote distorsion probabilities -LRB- d -RRB- which capture the probability that words change their position when translated from one language into another the probability of some French words being generated from an invisible English NULL element -LRB- pa6 -RRB- etc. See -LRB- Brown et al. 1993 -RRB- or -LRB- Germann et al. 2001 -RRB- for a detailed discussion of this translation model and a description of its parameters	poss_parameters_its prep_of_description_parameters det_description_a nn_model_translation det_model_this conj_and_discussion_description prep_of_discussion_model amod_discussion_detailed det_discussion_a amod_Germann_2001 dep_Germann_al. nn_Germann_et amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_for_See_description prep_for_See_discussion conj_or_See_Germann dep_See_Brown advmod_element_etc. appos_element_pa6 nn_element_NULL nn_element_English amod_element_invisible det_element_an prep_from_generated_element auxpass_generated_being vmod_words_generated amod_words_French det_words_some dep_probability_Germann dep_probability_See prep_of_probability_words det_probability_the num_language_one prep_into_translated_another prep_from_translated_language advmod_translated_when poss_position_their advcl_change_translated dobj_change_position nsubj_change_words dep_change_that rcmod_probability_change det_probability_the dobj_capture_probability nsubj_capture_which rcmod_probabilities_capture appos_probabilities_d nn_probabilities_distorsion parataxis_denote_probability dobj_denote_probabilities nsubj_denote_rest mark_denote_a65 det_factors_the prep_of_rest_factors det_rest_The ccomp_``_denote
P01-1067	J93-2003	o	Mathematical details are fully described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in advmod_described_fully auxpass_described_are nsubjpass_described_details amod_details_Mathematical
P01-1067	J93-2003	o	Let a183a49a48a50 a69 a188 a50 a51a181a51a181a51a212a188 a50a7a51a24a52 a48a54a53 a185a56a55 be a substring of a183 from the word a188 a50 with length a57 Note this notation is different from -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_from_al. prep_different_from cop_different_is nsubj_different_notation det_notation_this ccomp_Note_different nn_a57_length prep_with_a50_a57 nn_a50_a188 nn_a50_word det_a50_the prep_from_substring_a50 prep_of_substring_a183 det_substring_a cop_substring_be nsubj_substring_a185a56a55 nn_a185a56a55_a48a54a53 nn_a185a56a55_a50a7a51a24a52 nn_a185a56a55_a51a181a51a181a51a212a188 nn_a185a56a55_a50 num_a185a56a55_a188 nn_a185a56a55_a69 nn_a185a56a55_a183a49a48a50 dep_Let_Note ccomp_Let_substring
P01-1067	J93-2003	o	Following -LRB- Brown et al. 1993 -RRB- and the other literature in TM this paper only focuses the details of TM	prep_of_details_TM det_details_the dobj_focuses_details advmod_focuses_only nsubj_focuses_paper vmod_focuses_literature vmod_focuses_Following det_paper_this prep_in_literature_TM amod_literature_other det_literature_the amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_Following_literature dep_Following_Brown
P01-1067	J93-2003	o	To make this paper comparable to -LRB- Brown et al. 1993 -RRB- we use English-French notation in this section	det_section_this amod_notation_English-French prep_in_use_section dobj_use_notation nsubj_use_we advcl_use_make amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_to_Brown prep_comparable_to nsubj_comparable_paper det_paper_this xcomp_make_comparable aux_make_To
P02-1038	J93-2003	o	1 perform the following maximization eI1 = argmax eI1 fPr -LRB- eI1 -RRB- Pr -LRB- fJ1 jeI1 -RRB- g -LRB- 2 -RRB- This approach is referred to as source-channel approach to statistical MT. Sometimes it is also referred to as the fundamental equation of statistical MT -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_MT_al. amod_MT_statistical prep_of_equation_MT amod_equation_fundamental det_equation_the prep_as_referred_equation prep_referred_to advmod_referred_also auxpass_referred_is nsubjpass_referred_it advcl_referred_referred amod_MT._statistical advmod_approach_Sometimes prep_to_approach_MT. nn_approach_source-channel prep_as_referred_approach prep_referred_to auxpass_referred_is nsubjpass_referred_g dep_referred_Pr dep_referred_= dep_referred_eI1 det_approach_This dep_g_approach appos_g_2 nn_g_jeI1 nn_g_fJ1 nn_Pr_fPr appos_fPr_eI1 nn_fPr_eI1 nn_fPr_argmax amod_maximization_following det_maximization_the parataxis_perform_referred dobj_perform_maximization nsubj_perform_1
P02-1038	J93-2003	o	If the language model Pr -LRB- eI1 -RRB- = p -LRB- eI1 -RRB- depends on parameters and the translation model Pr -LRB- fJ1 jeI1 -RRB- = p -LRB- fJ1 jeI1 -RRB- depends on parameters then the optimal parameter values are obtained by maximizing the likelihood on a parallel training corpus fS1 eS1 -LRB- Brown et al. 1993 -RRB- = argmax SY s = 1 p -LRB- fsjes -RRB- -LRB- 3 -RRB- = argmax SY s = 1 p -LRB- es -RRB- -LRB- 4 -RRB- Computational Linguistics -LRB- ACL -RRB- Philadelphia July 2002 pp	appos_July_pp num_July_2002 appos_Linguistics_July appos_Linguistics_Philadelphia appos_Linguistics_ACL amod_Linguistics_Computational dep_Linguistics_4 nn_Linguistics_p appos_p_es num_p_1 dep_=_Linguistics acomp_s_= nsubj_s_SY nn_SY_argmax dep_=_s dep_3_= dep_p_3 appos_p_fsjes num_p_1 tmod_=_p acomp_s_= nsubj_s_SY nn_SY_argmax dep_=_s num_al._1993 nn_al._et amod_al._Brown dep_eS1_= appos_eS1_al. nn_fS1_corpus nn_fS1_training amod_fS1_parallel det_fS1_a det_likelihood_the prep_on_maximizing_fS1 dobj_maximizing_likelihood dep_obtained_eS1 agent_obtained_maximizing auxpass_obtained_are nsubjpass_obtained_values advmod_obtained_then advcl_obtained_depends nn_values_parameter amod_values_optimal det_values_the prep_on_depends_parameters nsubj_depends_= nn_jeI1_fJ1 nn_jeI1_p dep_=_jeI1 nn_jeI1_fJ1 appos_Pr_jeI1 nn_Pr_model nn_Pr_translation det_Pr_the rcmod_parameters_depends conj_and_parameters_Pr prep_on_depends_Pr prep_on_depends_parameters nsubj_depends_Pr mark_depends_If appos_p_eI1 dep_=_p amod_Pr_= appos_Pr_eI1 nn_Pr_model nn_Pr_language det_Pr_the
P02-1039	J93-2003	p	For the IBM models defined by a pioneering paper -LRB- Brown et al. 1993 -RRB- a decoding algorithm based on a left-to-right search was described in -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_in_Berger prep_described_in auxpass_described_was nsubjpass_described_algorithm prep_for_described_models amod_search_left-to-right det_search_a prep_on_based_search vmod_algorithm_based amod_algorithm_decoding det_algorithm_a dep_al._1993 nn_al._et amod_al._Brown dep_paper_al. amod_paper_pioneering det_paper_a agent_defined_paper vmod_models_defined nn_models_IBM det_models_the rcmod_``_described
P02-1051	J93-2003	o	The score for a given candidate a9 is given by a modified IBM Model 1 probability -LRB- Brown et al. 1993 -RRB- as follows a2a4a3a6a9a21a10a13a12a15a7a14a2 a15 a24a26a17a16 a2a4a3a6a9a19a18 a14a15a10a12 a7 -LRB- 4 -RRB- a2 a15 a20 a24a16a22a21a24a23a26a25a1a27a28a27a28a27 a20 a24a16a30a29a1a23a26a25 a31 a32 a33 a23a35a34a37a36 a3a38a12 a33 a10a12a9 a16a8a39 a7 -LRB- 5 -RRB- where a40 is the length of a9 a41 is the length of a12 a15 is a scaling factor based on the number of matches of a9 found and a14 a33 is the index of the English word aligned with a12 a33 according to alignment a14 The probability a36 a3a6a9 a16a8a39 a10a12 a33 a7 is a linear combination of the transliteration and translation score where the translation score is a uniform probability over all dictionary entries for a12 a33 The scored matches form the list of translation candidates	nn_candidates_translation prep_of_list_candidates det_list_the dobj_form_list nsubj_form_matches amod_matches_scored det_matches_The nn_a33_a12 prep_for_entries_a33 nn_entries_dictionary det_entries_all prep_over_probability_entries amod_probability_uniform det_probability_a cop_probability_is nsubj_probability_score advmod_probability_where nn_score_translation det_score_the rcmod_score_probability nn_score_translation nn_score_transliteration det_score_the conj_and_transliteration_translation parataxis_combination_form prep_of_combination_score amod_combination_linear det_combination_a cop_combination_is nsubj_combination_a7 nn_a7_a33 nn_a7_a10a12 nn_a7_a16a8a39 nn_a7_a3a6a9 nn_a7_a36 nn_a7_probability det_a7_The nn_a14_alignment nn_a33_a12 prep_with_aligned_a33 vmod_word_aligned nn_word_English det_word_the pobj_index_a14 prepc_according_to_index_to prep_of_index_word det_index_the cop_index_is nsubj_index_a33 nn_a33_a14 vmod_a9_found prep_of_matches_a9 prep_of_number_matches det_number_the prep_on_based_number dep_factor_combination conj_and_factor_index vmod_factor_based nn_factor_scaling det_factor_a cop_factor_is nsubj_factor_a7 prep_of_length_a12 det_length_the cop_length_is nsubj_length_a7 prep_of_length_a9 det_length_the cop_length_is nsubj_length_a40 advmod_length_where appos_a7_a41 rcmod_a7_length appos_a7_5 nn_a7_a16a8a39 nn_a7_a10a12a9 nn_a7_a33 nn_a7_a3a38a12 nn_a7_a23a35a34a37a36 nn_a7_a33 nn_a7_a32 nn_a7_a31 num_a7_a24a16a30a29a1a23a26a25 nn_a7_a20 nn_a7_a24a16a22a21a24a23a26a25a1a27a28a27a28a27 nn_a7_a20 nn_a7_a15 nn_a7_a2 num_a7_4 appos_a7_a15 rcmod_a7_length nn_a7_a14a15a10a12 nn_a7_a2a4a3a6a9a19a18 nn_a7_a24a26a17a16 nn_a7_a15 nn_a7_a2a4a3a6a9a21a10a13a12a15a7a14a2 ccomp_follows_index ccomp_follows_factor mark_follows_as num_al._1993 nn_al._et amod_al._Brown dep_probability_al. num_probability_1 nn_probability_Model nn_probability_IBM amod_probability_modified det_probability_a advcl_given_follows agent_given_probability auxpass_given_is nsubjpass_given_score nn_a9_candidate amod_a9_given det_a9_a prep_for_score_a9 det_score_The
P02-1052	J93-2003	o	Proceedings of the 40th Annual Meeting of the Association for -LRB- Brown et al. 1990 Brown et al. 1993 -RRB- a number of other algorithms have been developed	auxpass_developed_been aux_developed_have nsubjpass_developed_number prep_developed_for amod_algorithms_other prep_of_number_algorithms det_number_a num_Brown_1993 nn_Brown_al. nn_Brown_et dep_al._Brown num_al._1990 nn_al._et amod_al._Brown dep_for_al. rcmod_Association_developed det_Association_the prep_of_Meeting_Association amod_Meeting_Annual amod_Meeting_40th det_Meeting_the prep_of_Proceedings_Meeting
P03-1003	J93-2003	p	Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition -LRB- Jelinek 1997 -RRB- part of speech tagging -LRB- Church 1988 -RRB- machine translation -LRB- Brown et al. 1993 -RRB- information retrieval -LRB- Berger and Lafferty 1999 -RRB- and text summarization -LRB- Knight and Marcu 2002 -RRB- we develop a noisy channel model for QA	prep_for_model_QA nn_model_channel amod_model_noisy det_model_a dobj_develop_model nsubj_develop_we dep_develop_summarization dep_develop_retrieval nsubj_develop_translation dep_develop_part vmod_develop_inspired dep_Knight_2002 conj_and_Knight_Marcu appos_summarization_Marcu appos_summarization_Knight nn_summarization_text dep_Berger_1999 conj_and_Berger_Lafferty conj_and_retrieval_summarization appos_retrieval_Lafferty appos_retrieval_Berger nn_retrieval_information dep_al._1993 nn_al._et amod_al._Brown dep_translation_al. nn_translation_machine dep_Church_1988 nn_Church_tagging nn_Church_speech prep_of_part_Church dep_Jelinek_1997 dep_recognition_Jelinek nn_recognition_speech prep_as_diverse_recognition prep_in_approaches_applications amod_approaches_noisy-channel-based prep_as_success_diverse prep_of_success_approaches det_success_the agent_inspired_success auxpass_inspired_Being
P03-1003	J93-2003	o	-LRB- see Brown et al. 1993 for a detailed mathematical description of the model and the formula for computing the probability of an alignment and target string given a source string -RRB-	nn_string_source det_string_a dobj_given_string nn_string_target conj_and_alignment_string det_alignment_an vmod_probability_given prep_of_probability_string prep_of_probability_alignment det_probability_the dobj_computing_probability prepc_for_formula_computing det_formula_the det_model_the prep_of_description_model amod_description_mathematical amod_description_detailed det_description_a prep_for_Brown_description num_Brown_1993 nn_Brown_al. nn_Brown_et conj_and_see_formula dobj_see_Brown dep_''_formula dep_''_see
P03-1003	J93-2003	o	To help our model learn that it is desirable to copy answer words into the question we add to each corpus a list of identical dictionary word pairs w iw i For each corpus we use GIZA -LRB- Al-Onaizan et al. 1999 -RRB- a publicly available SMT package that implements the IBM models -LRB- Brown et al. 1993 -RRB- to train a QA noisy-channel model that maps flattened answer parse trees obtained using the cut procedure described in Section 3.1 into questions	num_Section_3.1 prep_in_described_Section vmod_procedure_described nn_procedure_cut det_procedure_the prep_into_using_questions dobj_using_procedure xcomp_obtained_using nn_trees_parse nn_trees_answer amod_trees_flattened dobj_maps_trees nsubj_maps_that rcmod_model_maps nn_model_noisy-channel nn_model_QA det_model_a vmod_train_obtained dobj_train_model aux_train_to amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the dobj_implements_models nsubj_implements_that dep_package_Brown rcmod_package_implements nn_package_SMT amod_package_available det_package_a advmod_available_publicly amod_Al-Onaizan_1999 dep_Al-Onaizan_al. nn_Al-Onaizan_et appos_GIZA_package dep_GIZA_Al-Onaizan xcomp_use_train dobj_use_GIZA nsubj_use_we det_corpus_each nn_i_iw prep_for_w_corpus dobj_w_i nn_pairs_word nn_pairs_dictionary amod_pairs_identical vmod_list_w prep_of_list_pairs det_list_a det_corpus_each parataxis_add_use dobj_add_list prep_to_add_corpus nsubj_add_we advcl_add_help det_question_the nn_words_answer prep_into_copy_question dobj_copy_words aux_copy_to xcomp_desirable_copy cop_desirable_is nsubj_desirable_it mark_desirable_that ccomp_learn_desirable poss_model_our dep_help_learn dobj_help_model aux_help_To
P03-1012	J93-2003	o	These constraints tie words in such a way that the space of alignments can not be enumerated as in IBM models 1 and 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_1_2 dep_models_2 dep_models_1 nn_models_IBM dep_in_Brown pobj_in_models pobj_as_in prep_enumerated_as auxpass_enumerated_be neg_enumerated_not aux_enumerated_can nsubjpass_enumerated_space mark_enumerated_that prep_of_space_alignments det_space_the ccomp_way_enumerated det_way_a amod_way_such prep_in_tie_way dobj_tie_words nsubj_tie_constraints det_constraints_These
P03-1012	J93-2003	o	1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_systems_translation nn_systems_machine amod_systems_statistical prep_of_result_systems amod_result_intermediate det_result_an dep_introduced_Brown prep_as_introduced_result advmod_introduced_first auxpass_introduced_were nsubjpass_introduced_alignments nn_alignments_Word nn_alignments_Introduction num_alignments_1 ccomp_``_introduced
P03-1016	J93-2003	o	Equation -LRB- 2 -RRB- is rewritten as -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- 2211 21 ce colecolcolcolcol rrpcepcep crpcepcepcep = = -LRB- 3 -RRB- It is equal to a word translation model if we take the relation type in the collocations as an element like a word which is similar to Model 1 in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. amod_Model_in num_Model_1 prep_to_similar_Model cop_similar_is nsubj_similar_which rcmod_word_similar det_word_a prep_like_element_word det_element_an det_collocations_the nn_type_relation det_type_the prep_in_take_collocations dobj_take_type nsubj_take_we mark_take_if nn_model_translation nn_model_word det_model_a prep_to_equal_model cop_equal_is nsubj_equal_It dep_equal_3 ccomp_=_equal advcl_=_take amod_=_= prep_as_crpcepcepcep_element amod_crpcepcepcep_= dep_rrpcepcep_crpcepcepcep dep_colecolcolcolcol_rrpcepcep dep_ce_colecolcolcolcol dep_21_ce dep_2211_21 dep_|_2211 nn_|_| nn_|_| nn_|_| nn_|_| nn_|_| nn_|_| prep_as_rewritten_| auxpass_rewritten_is nsubjpass_rewritten_Equation appos_Equation_2
P03-1016	J93-2003	o	2.3.4 Word Translation Probability Estimation Many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora -LRB- Koehn and Knight 2000 Brown et al. 1993 -RRB-	num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Koehn_Brown conj_and_Koehn_2000 conj_and_Koehn_Knight dep_corpora_2000 dep_corpora_Knight dep_corpora_Koehn amod_corpora_bilingual amod_corpora_parallel amod_corpora_unparallel conj_or_unparallel_parallel prep_from_probabilities_corpora nn_probabilities_translation nn_probabilities_word dobj_estimate_probabilities aux_estimate_to xcomp_used_estimate auxpass_used_are nsubjpass_used_methods amod_methods_Many nn_methods_Estimation nn_methods_Probability nn_methods_Translation nn_methods_Word num_methods_2.3.4 ccomp_``_used
P03-1039	J93-2003	o	The next section briefly reviews the word alignment based statistical machine translation -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_translation_al. nn_translation_machine amod_translation_statistical pobj_based_translation vmod_alignment_based nn_alignment_word det_alignment_the dobj_reviews_alignment nsubj_reviews_briefly dep_reviews_section amod_section_next det_section_The ccomp_``_reviews
P03-1039	J93-2003	p	The former term P -LRB- E -RRB- is called a language model representing the likelihood of E The latter term P -LRB- J | E -RRB- is called a translation model representing the generation probability from E into J As an implementation of P -LRB- J | E -RRB- the word alignment based statistical translation -LRB- Brown et al. 1993 -RRB- has been successfully applied to similar language pairs such as FrenchEnglish and German English but not to drastically dierent ones such as JapaneseEnglish	prep_such_as_ones_JapaneseEnglish dobj_dierent_ones advmod_dierent_drastically aux_dierent_to neg_dierent_not nsubj_dierent_alignment amod_English_German conj_and_FrenchEnglish_English prep_such_as_pairs_English prep_such_as_pairs_FrenchEnglish nn_pairs_language amod_pairs_similar conj_but_applied_dierent prep_to_applied_pairs advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_alignment num_al._1993 nn_al._et amod_al._Brown appos_translation_al. amod_translation_statistical pobj_based_translation vmod_alignment_based nn_alignment_word det_alignment_the num_E_| nn_E_J appos_P_E prep_of_implementation_P det_implementation_an prep_into_probability_J prep_from_probability_E nn_probability_generation det_probability_the dobj_representing_probability nn_model_translation det_model_a xcomp_called_model auxpass_called_is nsubjpass_called_P num_E_| nn_E_J appos_P_E nn_P_term amod_P_latter det_P_The prep_of_likelihood_E det_likelihood_the dobj_representing_likelihood vmod_model_representing nn_model_language det_model_a parataxis_called_dierent parataxis_called_applied prep_as_called_implementation xcomp_called_representing parataxis_called_called dobj_called_model auxpass_called_is nsubjpass_called_P appos_P_E nn_P_term amod_P_former det_P_The
P03-1040	J93-2003	o	As a baseline we use an IBM Model 4 -LRB- Brown et al. 1993 -RRB- system3 with a greedy decoder4 -LRB- Germann et al. 2001 -RRB-	amod_Germann_2001 dep_Germann_al. nn_Germann_et amod_decoder4_greedy det_decoder4_a dep_system3_Germann prep_with_system3_decoder4 amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_system3 appos_Model_Brown num_Model_4 nn_Model_IBM det_Model_an dobj_use_Model nsubj_use_we prep_as_use_baseline det_baseline_a
P03-1041	J93-2003	o	Re-ordering effects across languages have been modeled in several ways including word-based -LRB- Brown et al. 1993 -RRB- template-based -LRB- Och et al. 1999 -RRB- and syntax-based -LRB- Yamada Knight 2001 -RRB-	amod_Yamada_2001 appos_Yamada_Knight dep_syntax-based_Yamada conj_al._1999 nn_al._et conj_and_Och_syntax-based dep_Och_al. dep_al._1993 nn_al._et advmod_Brown_al. dep_word-based_syntax-based dep_word-based_Och dep_word-based_template-based dep_word-based_Brown prep_including_ways_word-based amod_ways_several prep_in_modeled_ways auxpass_modeled_been aux_modeled_have nsubjpass_modeled_effects prep_across_effects_languages amod_effects_Re-ordering
P03-1041	J93-2003	o	The traditional framework presented in -LRB- Brown et al. 1993 -RRB- assumes a generative process where the source sentence is passed through a noisy stochastic process to produce the target sentence	nn_sentence_target det_sentence_the dobj_produce_sentence aux_produce_to vmod_process_produce amod_process_stochastic amod_process_noisy det_process_a prep_through_passed_process auxpass_passed_is nsubjpass_passed_sentence advmod_passed_where nn_sentence_source det_sentence_the rcmod_process_passed amod_process_generative det_process_a dobj_assumes_process nsubj_assumes_al. mark_assumes_in num_al._1993 nn_al._et amod_al._Brown advcl_presented_assumes vmod_framework_presented amod_framework_traditional det_framework_The ccomp_``_framework
P03-1041	J93-2003	p	Within the generative model the Bayes reformulation is used to estimate a31 a0a15a14a35a33a1a26a13a37a36 a31 a0a15a14a19a13 a31 a0a2a1a38a33a14a39a13 where a31 a0a15a14a39a13 is considered the language model and a31 a0a2a1a38a33a14a19a13 is the translation model the IBM -LRB- Brown et al. 1993 -RRB- models being the de facto standard	nn_standard_facto nn_standard_de det_standard_the cop_standard_being nsubj_standard_models nn_models_IBM amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_IBM_Brown det_IBM_the nn_model_translation det_model_the cop_model_is nsubj_model_a0a2a1a38a33a14a19a13 nn_a0a2a1a38a33a14a19a13_a31 nn_model_language det_model_the xcomp_considered_model auxpass_considered_is nsubjpass_considered_a0a15a14a39a13 advmod_considered_where nn_a0a15a14a39a13_a31 rcmod_a0a2a1a38a33a14a39a13_considered nn_a0a2a1a38a33a14a39a13_a31 nn_a0a2a1a38a33a14a39a13_a0a15a14a19a13 nn_a0a2a1a38a33a14a39a13_a31 nn_a0a2a1a38a33a14a39a13_a0a15a14a35a33a1a26a13a37a36 nn_a0a2a1a38a33a14a39a13_a31 dobj_estimate_a0a2a1a38a33a14a39a13 aux_estimate_to parataxis_used_standard conj_and_used_model xcomp_used_estimate auxpass_used_is nsubjpass_used_reformulation prep_within_used_model nn_reformulation_Bayes det_reformulation_the amod_model_generative det_model_the
P03-1050	J93-2003	o	2.2 The Translation Model We adapted Model 1 -LRB- Brown et al. 1993 -RRB- to our purposes	poss_purposes_our amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_1 prep_to_adapted_purposes dobj_adapted_Model nsubj_adapted_We rcmod_Model_adapted nn_Model_Translation det_Model_The dep_2.2_Model ccomp_``_2.2
P03-1051	J93-2003	o	-LRB- Darwish 2002 -RRB- is not very useful for applications like statistical machine translation -LRB- Brown et al. 1993 -RRB- for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations	nn_translations_quality amod_translations_high prep_for_critical_translations cop_critical_is nsubj_critical_alignment prep_for_critical_which nn_languages_target det_languages_the conj_and_source_languages det_source_the prep_between_alignment_languages prep_between_alignment_source amod_alignment_word-to-word amod_alignment_accurate det_alignment_an num_al._1993 dep_Brown_al. nn_Brown_et nn_translation_machine amod_translation_statistical prep_like_applications_translation rcmod_useful_critical appos_useful_Brown prep_for_useful_applications advmod_useful_very neg_useful_not cop_useful_is nsubj_useful_Darwish num_Darwish_2002
P03-1051	J93-2003	n	By segmenting words into morphemes we can improve the performance of natural language systems including machine translation -LRB- Brown et al. 1993 -RRB- and information retrieval -LRB- Franz M. and McCarley S. 2002 -RRB-	num_S._2002 appos_Franz_S. conj_and_Franz_McCarley conj_and_Franz_M. dep_retrieval_McCarley dep_retrieval_M. dep_retrieval_Franz nn_retrieval_information num_al._1993 nn_al._et amod_al._Brown conj_and_translation_retrieval dep_translation_al. nn_translation_machine prep_including_systems_retrieval prep_including_systems_translation nn_systems_language amod_systems_natural prep_of_performance_systems det_performance_the dobj_improve_performance aux_improve_can nsubj_improve_we prep_by_improve_words prep_into_words_morphemes amod_words_segmenting
P03-2017	J93-2003	o	4 we see strong parallels between TransType and ITU language model enumerating word sequences vs 4 Initially statistical MT used a noisy-channel approach -LSB- Brown et al. 1993 -RSB- but recently -LSB- Och and Ney 2002 -RSB- have introduced a more general framework based on the maximum-entropy principle which shows nice prospects in terms of flexibility and learnability	conj_and_flexibility_learnability prep_of_terms_learnability prep_of_terms_flexibility amod_prospects_nice prep_in_shows_terms dobj_shows_prospects nsubj_shows_which rcmod_principle_shows amod_principle_maximum-entropy det_principle_the pobj_framework_principle prepc_based_on_framework_on amod_framework_general advmod_framework_more det_framework_a dobj_introduced_framework aux_introduced_have nsubj_introduced_Ney nsubj_introduced_Och advmod_introduced_recently num_Ney_2002 conj_and_Och_Ney num_al._1993 nn_al._et amod_al._Brown amod_approach_noisy-channel det_approach_a dobj_used_approach nsubj_used_MT advmod_used_Initially dep_used_4 amod_MT_statistical conj_vs_sequences_used nn_sequences_word dobj_enumerating_used dobj_enumerating_sequences appos_model_al. vmod_model_enumerating nn_model_language conj_and_TransType_ITU prep_between_parallels_ITU prep_between_parallels_TransType amod_parallels_strong conj_but_see_introduced dep_see_model dobj_see_parallels nsubj_see_we dep_see_4
P03-2017	J93-2003	o	He then goes on to adapt the conventional noisy channel MT model of -LSB- Brown et al 1993 -RSB- to NLU where extracting a semantic representation from an input text corresponds to finding argmax -LRB- Sem -RRB- -LCB- p -LRB- Input | Sem -RRB- p -LRB- Sem -RRB- -RCB- where p -LRB- Sem -RRB- is a model for generating semantic representations and p -LRB- Input | Sem -RRB- is a model for the relation between semantic representations and corresponding texts	amod_texts_corresponding conj_and_representations_texts amod_representations_semantic prep_between_relation_texts prep_between_relation_representations det_relation_the prep_for_model_relation det_model_a cop_model_is num_Sem_| dep_Input_Sem appos_p_Input amod_representations_semantic dobj_generating_representations prepc_for_model_generating det_model_a cop_model_is nsubj_model_p advmod_model_where appos_p_Sem appos_p_Sem nn_p_p num_Sem_| dep_Input_Sem appos_p_Input nn_p_argmax appos_argmax_Sem prep_to_corresponds_finding nn_text_input det_text_an amod_representation_semantic det_representation_a dep_extracting_corresponds prep_from_extracting_text dobj_extracting_representation advmod_extracting_where rcmod_NLU_extracting advmod_1993_al nn_al_et conj_and_Brown_p rcmod_Brown_model dep_Brown_p prep_to_Brown_NLU num_Brown_1993 prep_of_model_p prep_of_model_Brown nn_model_MT nn_model_channel amod_model_noisy amod_model_conventional det_model_the dobj_adapt_model aux_adapt_to dep_goes_model prepc_on_goes_adapt advmod_goes_then nsubj_goes_He
P04-1022	J93-2003	o	Some studies have been done for acquiring collocation translations using parallel corpora -LRB- Smadja et al 1996 Kupiec 1993 Echizen-ya et al. 2003 -RRB-	num_Echizen-ya_2003 nn_Echizen-ya_al. nn_Echizen-ya_et num_Kupiec_1993 dep_Smadja_Echizen-ya dep_Smadja_Kupiec amod_Smadja_1996 dep_Smadja_al nn_Smadja_et appos_corpora_Smadja amod_corpora_parallel dobj_using_corpora nn_translations_collocation vmod_acquiring_using dobj_acquiring_translations prepc_for_done_acquiring auxpass_done_been aux_done_have nsubjpass_done_studies det_studies_Some ccomp_``_done
P04-1022	J93-2003	o	Most previous research in translation knowledge acquisition is based on parallel corpora -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_corpora_al. amod_corpora_parallel prep_on_based_corpora auxpass_based_is nsubjpass_based_research nn_acquisition_knowledge nn_acquisition_translation prep_in_research_acquisition amod_research_previous amod_research_Most
P04-1022	J93-2003	o	We have -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- 21 21 trictrictric trictritri erpercpercp ecrcpecp = = -LRB- 6 -RRB- Assumption 2 For an English triple tri e assume that i c only depends on -LCB- 1,2 -RCB- -RRB- -LRB- i i e and c r only depends on e r Equation -LRB- 6 -RRB- is rewritten as -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- 2211 21 ec trietrictrictritri rrpecpecp erpercpercpecp = = -LRB- 7 -RRB- Notice that -RRB- | -LRB- 11 ecp and -RRB- | -LRB- 22 ecp are translation probabilities within triples they are different from the unrestricted probabilities such as the ones in IBM models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM prep_in_ones_models det_ones_the prep_such_as_probabilities_ones amod_probabilities_unrestricted det_probabilities_the prep_from_different_probabilities cop_different_are nsubj_different_they prep_within_probabilities_triples nn_probabilities_translation cop_probabilities_are nsubj_probabilities_ecp dep_probabilities_| num_ecp_22 nn_|_| cc_ecp_and num_ecp_11 appos_|_ecp det_|_that dep_Notice_Brown dep_Notice_different dep_Notice_probabilities dep_Notice_7 dep_Notice_= dep_Notice_= dep_Notice_erpercpercpecp dep_Notice_rrpecpecp dep_Notice_trietrictrictritri dep_Notice_ec dep_Notice_21 dep_Notice_2211 dep_|_Notice nn_|_| dep_|_| appos_|_| dep_|_| nn_|_| nn_|_| prep_as_rewritten_| auxpass_rewritten_is appos_Equation_6 nn_Equation_r dep_Equation_e dep_depends_rewritten prep_on_depends_Equation advmod_depends_only nsubj_depends_r nn_r_c conj_and_e_depends nn_e_i dep_e_i num_e_1,2 prep_on_depends_depends prep_on_depends_e advmod_depends_only nsubj_depends_c nn_c_i det_c_that ccomp_assume_depends dep_assume_e nsubj_assume_tri mark_assume_For amod_tri_triple amod_tri_English det_tri_an num_Assumption_2 dep_=_Assumption dep_=_6 dep_=_= amod_ecrcpecp_= advcl_erpercpercp_assume dep_erpercpercp_ecrcpecp dep_trictritri_erpercpercp dep_trictrictric_trictritri dep_trictrictric_21 number_21_21 dep_|_trictrictric advmod_|_| nn_|_| appos_|_| appos_|_| dobj_have_| nsubj_have_We
P04-1022	J93-2003	o	These range from twoword to multi-word with or without syntactic structure -LRB- Smadja 1993 Lin 1998 Pearce 2001 Seretan et al. 2003 -RRB-	dep_al._2003 nn_al._et nn_al._Seretan num_Pearce_2001 dep_Lin_al. conj_Lin_Pearce num_Lin_1998 dep_Smadja_Lin num_Smadja_1993 dep_structure_Smadja amod_structure_syntactic prep_without_from_structure prep_with_from_structure conj_or_from_from prep_to_range_multi-word prep_from_range_twoword det_range_These
P04-1023	J93-2003	o	1 Introduction Machine translation systems based on probabilistic translation models -LRB- Brown et al. 1993 -RRB- are generally trained using sentence-aligned parallel corpora	nn_corpora_parallel amod_corpora_sentence-aligned dobj_using_corpora xcomp_trained_using advmod_trained_generally auxpass_trained_are nsubjpass_trained_systems amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_models_Brown nn_models_translation amod_models_probabilistic prep_on_based_models vmod_systems_based nn_systems_translation nn_systems_Machine nn_systems_Introduction num_systems_1 ccomp_``_trained
P04-1063	J93-2003	o	ALM does this by using alignment models from the statistical machine translation literature -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_literature_al. nn_literature_translation nn_literature_machine amod_literature_statistical det_literature_the nn_models_alignment prep_from_using_literature dobj_using_models prepc_by_does_using dobj_does_this nsubj_does_ALM
P04-1064	J93-2003	n	Although the first three are particular cases where N = 1 and/or M = 1 the distinction is relevant because most word-based translation models -LRB- eg IBM models -LRB- Brown et al. 1993 -RRB- -RRB- can typically not accommodate general M-N alignments	nn_alignments_M-N amod_alignments_general dobj_accommodate_alignments neg_accommodate_not advmod_accommodate_typically aux_accommodate_can nsubj_accommodate_models mark_accommodate_because amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_IBM dep_models_eg dep_models_models nn_models_translation amod_models_word-based amod_models_most advcl_relevant_accommodate cop_relevant_is nsubj_relevant_distinction advcl_relevant_cases det_distinction_the dep_=_1 amod_1_= conj_and/or_1_M dobj_=_M dobj_=_1 nsubj_=_N advmod_=_where rcmod_cases_= amod_cases_particular cop_cases_are nsubj_cases_three mark_cases_Although amod_three_first det_three_the
P04-1064	J93-2003	o	Note that our use of cepts differs slightly from that of -LRB- Brown et al. 1993 sec .3 -RRB- inasmuch cepts may not overlap according to our definition	poss_definition_our neg_overlap_not aux_overlap_may nsubj_overlap_cepts nn_cepts_inasmuch nn_.3_sec appos_al._.3 num_al._1993 nn_al._et amod_al._Brown dep_of_al. amod_that_of prep_from_differs_that advmod_differs_slightly nsubj_differs_use mark_differs_that prep_of_use_cepts poss_use_our pobj_Note_definition prepc_according_to_Note_to parataxis_Note_overlap ccomp_Note_differs
P04-1064	J93-2003	o	Obtaining a word-aligned corpus usually involves training a word-based translation models -LRB- Brown et al. 1993 -RRB- in each directions and combining the resulting alignments	amod_alignments_resulting det_alignments_the dobj_combining_alignments det_directions_each amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_in_models_directions dep_models_Brown nn_models_translation amod_models_word-based det_models_a conj_and_training_combining dobj_training_models dobj_involves_combining dobj_involves_training advmod_involves_usually csubj_involves_Obtaining amod_corpus_word-aligned det_corpus_a dobj_Obtaining_corpus
P04-1066	J93-2003	p	1 Introduction IBM Model 1 -LRB- Brown et al. 1993a -RRB- is a wordalignment model that is widely used in working with parallel bilingual corpora	amod_corpora_bilingual amod_corpora_parallel prep_with_working_corpora prepc_in_used_working advmod_used_widely auxpass_used_is nsubjpass_used_that rcmod_model_used nn_model_wordalignment det_model_a cop_model_is nsubj_model_Model appos_Brown_1993a dep_Brown_al. nn_Brown_et dep_1_Brown dep_Model_1 nn_Model_IBM nn_Model_Introduction num_Model_1
P04-1066	J93-2003	o	The first of these nonstructural problems with Model 1 as standardly trained is that rare words in the source language tend to act as garbage collectors -LRB- Brown et al. 1993b Och and Ney 2004 -RRB- aligning to too many words in the target language	nn_language_target det_language_the prep_in_words_language amod_words_many advmod_many_too prep_to_aligning_words dep_Och_2004 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och appos_Brown_1993b dep_Brown_al. nn_Brown_et appos_collectors_Brown nn_collectors_garbage prep_as_act_collectors aux_act_to xcomp_tend_aligning xcomp_tend_act nsubj_tend_words mark_tend_that nn_language_source det_language_the prep_in_words_language amod_words_rare ccomp_is_tend dep_is_trained nsubj_is_first advmod_trained_standardly mark_trained_as num_Model_1 amod_problems_nonstructural det_problems_these prep_with_first_Model prep_of_first_problems det_first_The rcmod_``_is
P04-1083	J93-2003	p	Bootstrapping a PMTG from a lower-dimensional PMTG and a word-to-word translation model is similar in spirit to the way that regular grammars can help to estimate CFGs -LRB- Lari & Young 1990 -RRB- and the way that simple translation models can help to bootstrap more sophisticated ones -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_ones_Brown amod_ones_sophisticated advmod_sophisticated_more dobj_bootstrap_ones aux_bootstrap_to xcomp_help_bootstrap aux_help_can nsubj_help_models mark_help_that nn_models_translation amod_models_simple ccomp_way_help det_way_the dep_Lari_1990 conj_and_Lari_Young appos_CFGs_Young appos_CFGs_Lari dobj_estimate_CFGs aux_estimate_to xcomp_help_estimate aux_help_can nsubj_help_grammars mark_help_that amod_grammars_regular ccomp_way_help det_way_the conj_and_similar_way prep_to_similar_way prep_in_similar_spirit cop_similar_is csubj_similar_Bootstrapping nn_model_translation amod_model_word-to-word det_model_a conj_and_PMTG_model amod_PMTG_lower-dimensional det_PMTG_a det_PMTG_a prep_from_Bootstrapping_model prep_from_Bootstrapping_PMTG dobj_Bootstrapping_PMTG
P04-1083	J93-2003	o	This kind of synchronizer stands in contrast to more ad-hoc approaches -LRB- e.g. Matsumoto 1993 Meyers 1996 Wu 1998 Hwa et al. 2002 -RRB-	num_Hwa_2002 nn_Hwa_al. nn_Hwa_et num_Wu_1998 num_Meyers_1996 dep_Matsumoto_Hwa conj_Matsumoto_Wu conj_Matsumoto_Meyers conj_Matsumoto_1993 dep_e.g._Matsumoto dep_approaches_e.g. amod_approaches_ad-hoc amod_approaches_more prep_to_contrast_approaches prep_in_stands_contrast nsubj_stands_kind prep_of_kind_synchronizer det_kind_This
P04-3002	J93-2003	n	2 2.1 Word Alignment Adaptation Bi-directional Word Alignment In statistical translation models -LRB- Brown et al. 1993 -RRB- only one-to-one and more-to-one word alignment links can be found	auxpass_found_be aux_found_can nsubjpass_found_links dep_found_Alignment nn_links_alignment nn_links_word amod_links_more-to-one amod_links_one-to-one conj_and_one-to-one_more-to-one advmod_one-to-one_only amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation amod_models_statistical dep_Alignment_Brown prep_in_Alignment_models nn_Alignment_Word amod_Alignment_Bi-directional nn_Alignment_Adaptation nn_Alignment_Alignment nn_Alignment_Word num_Alignment_2.1 number_2.1_2
P04-3002	J93-2003	o	1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation -LRB- SMT -RRB- -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_result_translation amod_result_intermediate det_result_an dep_introduced_Brown prep_as_introduced_result advmod_introduced_first auxpass_introduced_is nsubjpass_introduced_alignment nn_alignment_word amod_alignment_Bilingual nn_alignment_Introduction num_alignment_1 ccomp_``_introduced
P04-3005	J93-2003	n	For the results in this paper we have used Pointwise Mutual Information -LRB- PMI -RRB- instead of IBM Model 1 -LRB- Brown et al. 1993 -RRB- since -LRB- Rogati and Yang 2004 -RRB- found it to be as effective on Springer but faster to compute	aux_compute_to xcomp_faster_compute nsubj_faster_Rogati prep_on_effective_Springer advmod_effective_as cop_effective_be aux_effective_to conj_but_found_faster xcomp_found_effective dobj_found_it nsubj_found_Yang nsubj_found_Rogati mark_found_since amod_Rogati_2004 conj_and_Rogati_Yang amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_1 nn_Model_IBM prep_instead_of_Information_Model appos_Information_PMI nn_Information_Mutual nn_Information_Pointwise advcl_used_faster advcl_used_found dep_used_Brown dobj_used_Information aux_used_have nsubj_used_we prep_for_used_results det_paper_this prep_in_results_paper det_results_the
P04-3014	J93-2003	p	Syntax-light alignment models such as the five IBM models -LRB- Brown et al. 1993 -RRB- and their relatives have proved to be very successful and robust at producing word-level alignments especially for closely related languages with similar word order and mostly local reorderings which can be captured via simple models of relative word distortion	nn_distortion_word amod_distortion_relative prep_of_models_distortion amod_models_simple prep_via_captured_models auxpass_captured_be aux_captured_can nsubjpass_captured_which amod_reorderings_local advmod_local_mostly conj_and_order_reorderings nn_order_word amod_order_similar prep_with_languages_reorderings prep_with_languages_order amod_languages_related advmod_related_closely amod_alignments_word-level dobj_producing_alignments prepc_at_successful_producing conj_and_successful_robust advmod_successful_very cop_successful_be aux_successful_to ccomp_proved_captured prep_for_proved_languages advmod_proved_especially xcomp_proved_robust xcomp_proved_successful aux_proved_have nsubj_proved_models poss_relatives_their amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_models_relatives dep_models_Brown nn_models_IBM num_models_five det_models_the prep_such_as_models_relatives prep_such_as_models_models nn_models_alignment amod_models_Syntax-light
P05-1009	J93-2003	o	In Machine Translation for example sentences are produced using application-specific decoders inspired by work on speech recognition -LRB- Brown et al. 1993 -RRB- whereas in Summarization summaries are produced as either extracts or using task-specific strategies -LRB- Barzilay 2003 -RRB-	amod_Barzilay_2003 dep_strategies_Barzilay amod_strategies_task-specific dobj_using_strategies nsubjpass_using_summaries det_extracts_either conj_or_produced_using prep_as_produced_extracts auxpass_produced_are nsubjpass_produced_summaries prep_in_produced_Summarization mark_produced_whereas num_al._1993 nn_al._et amod_al._Brown nn_recognition_speech prep_on_work_recognition agent_inspired_work dep_decoders_al. vmod_decoders_inspired amod_decoders_application-specific dobj_using_decoders advcl_produced_using advcl_produced_produced xcomp_produced_using auxpass_produced_are nsubjpass_produced_sentences prep_for_produced_example prep_in_produced_Translation nn_Translation_Machine
P05-1032	J93-2003	o	translation including the joint probability phrasebased model -LRB- Marcu and Wong 2002 -RRB- and a variant on the alignment template approach -LRB- Och and Ney 2004 -RRB- and contrast them to the performance of the word-based IBM Model 4 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM amod_Model_word-based det_Model_the prep_of_performance_Model det_performance_the dep_them_Brown prep_to_them_performance dep_contrast_them dep_Och_2004 conj_and_Och_Ney appos_approach_Ney appos_approach_Och nn_approach_template nn_approach_alignment det_approach_the prep_on_variant_approach det_variant_a amod_Marcu_2002 conj_and_Marcu_Wong conj_and_model_variant dep_model_Wong dep_model_Marcu amod_model_phrasebased nn_model_probability amod_model_joint det_model_the conj_and_translation_contrast prep_including_translation_variant prep_including_translation_model
P05-1032	J93-2003	n	By increasing the size of the basic unit of translation phrase-based machine translation does away with many of the problems associated with the original word-based formulation of statistical machine translation -LRB- Brown et al. 1993 -RRB- in particular The Brown et al.	nn_al._et nsubj_al._Brown det_Brown_The dep_al._1993 nn_al._et advmod_Brown_al. nn_translation_machine amod_translation_statistical prep_of_formulation_translation amod_formulation_word-based amod_formulation_original det_formulation_the prep_with_associated_formulation vmod_problems_associated det_problems_the dep_many_Brown prep_of_many_problems parataxis_does_al. prep_in_does_particular prep_with_does_many prt_does_away nsubj_does_translation prepc_by_does_increasing nn_translation_machine amod_translation_phrase-based prep_of_unit_translation amod_unit_basic det_unit_the prep_of_size_unit det_size_the dobj_increasing_size
P05-1033	J93-2003	o	The basic phrase-based model is an instance of the noisy-channel approach -LRB- Brown et al. 1993 -RRB- ,1 in which the translation of a French sentence f into an 1Throughout this paper we follow the convention of Brown et al. of designating the source and target languages as French and English respectively	conj_and_French_English nn_languages_target conj_and_source_languages det_source_the prep_as_designating_English prep_as_designating_French dobj_designating_languages dobj_designating_source prepc_of_Brown_designating dep_Brown_al. nn_Brown_et advmod_convention_respectively prep_of_convention_Brown det_convention_the dobj_follow_convention nsubj_follow_we det_paper_this dep_1Throughout_paper det_1Throughout_an prep_into_f_1Throughout dep_sentence_f amod_sentence_French det_sentence_a rcmod_translation_follow prep_of_translation_sentence det_translation_the dep_,1_translation prep_in_,1_which dep_,1_Brown nn_al._et dep_Brown_1993 advmod_Brown_al. amod_approach_noisy-channel det_approach_the dep_instance_,1 prep_of_instance_approach det_instance_an cop_instance_is nsubj_instance_model amod_model_phrase-based amod_model_basic det_model_The
P05-1057	J93-2003	o	Statistical approaches which depend on a set of unknown parameters that are learned from training data try to describe the relationship between a bilingual sentence pair -LRB- Brown et al. 1993 Vogel and Ney 1996 -RRB-	num_Vogel_1996 conj_and_Vogel_Ney dep_al._Ney dep_al._Vogel num_al._1993 nn_al._et amod_al._Brown dep_pair_al. nn_pair_sentence amod_pair_bilingual det_pair_a prep_between_relationship_pair det_relationship_the dobj_describe_relationship aux_describe_to xcomp_try_describe nsubj_try_approaches nn_data_training prep_from_learned_data auxpass_learned_are nsubjpass_learned_that rcmod_parameters_learned amod_parameters_unknown prep_of_set_parameters det_set_a prep_on_depend_set nsubj_depend_which rcmod_approaches_depend amod_approaches_Statistical
P05-1057	J93-2003	o	1 Introduction Word alignment which can be defined as an object for indicating the corresponding words in a parallel text was first introduced as an intermediate result of statistical translation models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation amod_models_statistical prep_of_result_models amod_result_intermediate det_result_an dep_introduced_Brown prep_as_introduced_result advmod_introduced_first auxpass_introduced_was nsubjpass_introduced_alignment amod_text_parallel det_text_a prep_in_words_text amod_words_corresponding det_words_the dobj_indicating_words prepc_for_object_indicating det_object_an prep_as_defined_object auxpass_defined_be aux_defined_can nsubjpass_defined_which rcmod_alignment_defined nn_alignment_Word nn_alignment_Introduction num_alignment_1 ccomp_``_introduced
P05-1057	J93-2003	o	If e has length l and f has length m there are possible 2lm alignments between e and f -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_e_Brown conj_and_e_f prep_between_alignments_f prep_between_alignments_e amod_alignments_2lm amod_alignments_possible nsubj_are_alignments expl_are_there advcl_are_f advcl_are_has nn_m_length dobj_has_m nn_l_length dep_has_has conj_and_has_f dobj_has_l nsubj_has_e mark_has_If
P05-1058	J93-2003	o	2 Statistical Word Alignment According to the IBM models -LRB- Brown et al. 1993 -RRB- the statistical word alignment model can be generally represented as in Equation -LRB- 1 -RRB-	appos_Equation_1 pobj_in_Equation pcomp_as_in prep_represented_as advmod_represented_generally auxpass_represented_be aux_represented_can nsubjpass_represented_model nn_model_alignment nn_model_word amod_model_statistical det_model_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the rcmod_Alignment_represented dep_Alignment_Brown pobj_Alignment_models prepc_according_to_Alignment_to nn_Alignment_Word amod_Alignment_Statistical num_Alignment_2
P05-1058	J93-2003	o	This simplified version does not take word classes into account as described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_as nn_classes_word advcl_take_described prep_into_take_account dobj_take_classes neg_take_not aux_take_does nsubj_take_version amod_version_simplified det_version_This
P05-1058	J93-2003	o	= == = = m aj j m j aj l i i l i ii m j j mlajdeft en pp m ap 0:1 11 1 2 0 0 0 -RRB- -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | Pr -LRB- -RRB- | -LRB- 00 eef -LRB- 3 -RRB- 1 A cept is defined as the set of target words connected to a source word -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. appos_word_Brown nn_word_source det_word_a prep_to_connected_word vmod_words_connected nn_words_target prep_of_set_words det_set_the prep_as_defined_set auxpass_defined_is nsubjpass_defined_eef advmod_defined_| det_cept_A num_cept_1 dep_eef_cept dep_eef_3 num_eef_00 ccomp_Pr_defined appos_|_Pr advmod_|_| dep_|_| nn_|_| num_0_0 num_0_11 number_0_0 dep_0_2 number_2_1 number_11_0:1 dep_ap_0 nn_ap_m nn_ap_pp prep_en_mlajdeft_ap nn_mlajdeft_j nn_mlajdeft_j nn_mlajdeft_m appos_ii_| dep_ii_mlajdeft dep_i_ii dep_l_i nn_l_i nn_l_i amod_l_l dep_aj_l dep_j_aj dep_m_j dep_j_m dep_aj_j dep_m_aj dep_=_m dep_=_= amod_==_= dep_=_== ccomp_''_=
P05-1058	J93-2003	o	1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical prep_of_result_translation amod_result_intermediate det_result_an prep_as_proposed_result dep_first_al. dep_first_proposed nsubj_was_first dep_alignment_was nn_alignment_Word nn_alignment_Introduction num_alignment_1
P05-1066	J93-2003	o	2 Background 2.1 Previous Work 2.1.1 Research on Phrase-Based SMT The original work on statistical machine translation was carried out by researchers at IBM -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_at_carried_IBM agent_carried_researchers prt_carried_out auxpass_carried_was nsubjpass_carried_work nn_translation_machine amod_translation_statistical prep_on_work_translation amod_work_original det_work_The rcmod_SMT_carried amod_SMT_Phrase-Based prep_on_Research_SMT num_Research_2.1.1 dep_Work_Research amod_Work_Previous num_Work_2.1 dep_Background_Brown dep_Background_Work num_Background_2 dep_``_Background
P05-1066	J93-2003	n	These methods go beyond the original IBM machine translation models -LRB- Brown et al. 1993 -RRB- by allowing multi-word units -LRB- phrases -RRB- in one language to be translated directly into phrases in another language	det_language_another prep_in_phrases_language prep_into_translated_phrases advmod_translated_directly auxpass_translated_be aux_translated_to vmod_language_translated num_language_one appos_units_phrases amod_units_multi-word prep_in_allowing_language dobj_allowing_units amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation nn_models_machine nn_models_IBM amod_models_original det_models_the prepc_by_go_allowing dep_go_Brown prep_beyond_go_models nsubj_go_methods det_methods_These
P05-1067	J93-2003	p	1 Introduction Statistical approaches to machine translation pioneered by -LRB- Brown et al. 1993 -RRB- achieved impressive performance by leveraging large amounts of parallel corpora	amod_corpora_parallel prep_of_amounts_corpora amod_amounts_large dobj_leveraging_amounts amod_performance_impressive prepc_by_achieved_leveraging dobj_achieved_performance nsubj_achieved_approaches num_al._1993 nn_al._et amod_al._Brown dep_by_al. prep_pioneered_by nn_translation_machine vmod_approaches_pioneered prep_to_approaches_translation amod_approaches_Statistical nn_approaches_Introduction num_approaches_1
P05-1067	J93-2003	o	As a unified approach we augment the SDIG by adding all the possible word pairs -LRB- -RRB- ji fe as a parallel ET pair and using the IBM Model 1 -LRB- Brown et al. 1993 -RRB- word to word translation probability as the ET translation probability	nn_probability_translation nn_probability_ET det_probability_the nn_probability_translation nn_probability_word prep_as_word_probability prep_to_word_probability dep_word_Brown nsubj_word_Model amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_1 nn_Model_IBM det_Model_the ccomp_using_word nn_pair_ET amod_pair_parallel det_pair_a conj_and_fe_using prep_as_fe_pair nn_fe_ji dep_pairs_using dep_pairs_fe nn_pairs_word amod_pairs_possible det_pairs_the predet_pairs_all dobj_adding_pairs det_SDIG_the prepc_by_augment_adding dobj_augment_SDIG nsubj_augment_we prep_as_augment_approach amod_approach_unified det_approach_a
P05-1067	J93-2003	o	In comparison we deployed the GIZA + + MT modeling tool kit which is an implementation of the IBM Models 1 to 4 -LRB- Brown et al. 1993 AlOnaizan et al. 1999 Och and Ney 2003 -RRB-	dep_Och_2003 conj_and_Och_Ney nn_al._et nn_al._AlOnaizan dep_Brown_Ney dep_Brown_Och num_Brown_1999 dep_Brown_al. num_Brown_1993 dep_Brown_al. nn_Brown_et appos_4_Brown dep_4_to number_4_1 dep_Models_4 nn_Models_IBM det_Models_the prep_of_implementation_Models det_implementation_an cop_implementation_is nsubj_implementation_which nn_kit_tool nn_kit_modeling nn_kit_MT dep_kit_+ rcmod_GIZA_implementation conj_+_GIZA_kit det_GIZA_the dobj_deployed_kit dobj_deployed_GIZA nsubj_deployed_we prep_in_deployed_comparison
P05-1067	J93-2003	o	In our implementation the IBM Model 1 -LRB- Brown et al. 1993 -RRB- is used	auxpass_used_is nsubjpass_used_1 nsubjpass_used_Model prep_in_used_implementation amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_1_Brown nn_Model_IBM det_Model_the poss_implementation_our
P05-1068	J93-2003	o	Most of the phrase-based translation models have adopted the noisy-channel based IBM style models -LRB- Brown et al. 1993 -RRB- CMCT C1 BD BP CPD6CVD1CPDC CT C1 BD C8D6B4CU C2 BD CYCT C1 BD B5C8D6B4CT C1 BD B5 -LRB- 1 -RRB- In these model we have two types of knowledge translation model C8D6B4CU C2 BD CYCT C1 BD B5 and language model C8D6B4CT C1 BD B5	nn_B5_BD nn_B5_C1 nn_B5_C8D6B4CT nn_model_language nn_B5_BD nn_B5_C1 nn_B5_CYCT nn_B5_BD nn_B5_C2 nn_B5_C8D6B4CU conj_and_model_B5 conj_and_model_model conj_and_model_B5 nn_model_translation prep_of_types_knowledge num_types_two dobj_have_types nsubj_have_we det_model_these dep_B5_B5 dep_B5_model dep_B5_B5 dep_B5_model rcmod_B5_have prep_in_B5_model appos_B5_1 nn_B5_BD nn_B5_C1 nn_B5_B5C8D6B4CT nn_B5_BD nn_B5_C1 nn_B5_CYCT nn_B5_BD nn_B5_C2 nn_B5_C8D6B4CU nn_B5_BD nn_B5_C1 nn_B5_CT nn_B5_CPD6CVD1CPDC nn_B5_BP nn_B5_BD nn_B5_C1 nn_B5_CMCT amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_style nn_models_IBM amod_models_based amod_models_noisy-channel det_models_the dep_adopted_B5 dep_adopted_Brown dobj_adopted_models aux_adopted_have nsubj_adopted_Most nn_models_translation amod_models_phrase-based det_models_the prep_of_Most_models ccomp_``_adopted
P05-1068	J93-2003	o	3.1 Learning Chunk-based Translation We learn chunk alignments from a corpus that has been word-aligned by a training toolkit for wordbased translation models the Giza + + -LRB- Och and Ney 2000 -RRB- toolkit for the IBM models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the appos_toolkit_Ney appos_toolkit_Och num_Och_2000 conj_and_Och_Ney pobj_+_toolkit appos_Giza_Brown prep_for_Giza_models conj_+_Giza_+ det_Giza_the nn_models_translation amod_models_wordbased prep_for_toolkit_models nn_toolkit_training det_toolkit_a agent_word-aligned_toolkit auxpass_word-aligned_been aux_word-aligned_has nsubjpass_word-aligned_that rcmod_corpus_word-aligned det_corpus_a nn_alignments_chunk prep_from_learn_corpus dobj_learn_alignments nsubj_learn_We dep_Translation_+ dep_Translation_Giza rcmod_Translation_learn amod_Translation_Chunk-based nn_Translation_Learning num_Translation_3.1 dep_``_Translation
P05-1074	J93-2003	o	The original formulation of statistical machine translation -LRB- Brown et al. 1993 -RRB- was defined as a word-based operation	amod_operation_word-based det_operation_a prep_as_defined_operation auxpass_defined_was nsubjpass_defined_formulation num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical appos_formulation_al. prep_of_formulation_translation amod_formulation_original det_formulation_The
P05-2016	J93-2003	o	The first work on SMT done at IBM -LRB- Brown et al. 1990 Brown et al. 1992 Brown et al. 1993 Berger et al. 1994 -RRB- used a noisy-channel model resulting in what Brown et al.	nn_al._et nn_al._Brown dep_al._what dep_in_al. prep_resulting_in amod_model_noisy-channel det_model_a vmod_used_resulting dobj_used_model ccomp_used_done num_Berger_1994 nn_Berger_al. nn_Berger_et num_Brown_1993 nn_Brown_al. nn_Brown_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Brown_Berger dep_Brown_Brown dep_Brown_Brown amod_Brown_1990 dep_Brown_al. nn_Brown_et appos_IBM_Brown prep_at_done_IBM nsubj_done_work prep_on_work_SMT amod_work_first det_work_The
P05-2022	J93-2003	p	There are basically two kinds of systems working at these segmentation levels the most widespread rely on statistical models in particular the IBM ones -LRB- Brown et al. 1993 -RRB- others combine simpler association measures with different kinds of linguistic information -LRB- Arhenberg et al. 2000 Barbu 2004 -RRB-	amod_Barbu_2004 dep_Arhenberg_Barbu appos_Arhenberg_2000 dep_Arhenberg_al. nn_Arhenberg_et amod_information_linguistic prep_of_kinds_information amod_kinds_different prep_with_measures_kinds nn_measures_association amod_measures_simpler dep_combine_Arhenberg dobj_combine_measures nsubj_combine_others amod_Brown_1993 dep_Brown_al. nn_Brown_et parataxis_ones_combine dep_ones_Brown nn_ones_IBM det_ones_the amod_models_statistical dep_rely_ones prep_in_rely_particular prep_on_rely_models nsubj_rely_widespread advmod_widespread_most det_widespread_the nn_levels_segmentation det_levels_these prep_at_working_levels vmod_systems_working dep_kinds_rely prep_of_kinds_systems num_kinds_two nsubj_are_kinds advmod_are_basically expl_are_There
P06-1002	J93-2003	o	2 Related Work Starting with the IBM models -LRB- Brown et al. 1993 -RRB- researchers have developed various statistical word alignment systems based on different models such as hidden Markov models -LRB- HMM -RRB- -LRB- Vogel et al. 1996 -RRB- log-linear models -LRB- Och and Ney 2003 -RRB- and similarity-based heuristic methods -LRB- Melamed 2000 -RRB-	amod_Melamed_2000 dep_methods_Melamed nn_methods_heuristic amod_methods_similarity-based dep_Och_2003 conj_and_Och_Ney appos_models_Ney appos_models_Och amod_models_log-linear amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et appos_models_HMM nn_models_Markov amod_models_hidden prep_such_as_models_models amod_models_different prep_on_based_models vmod_systems_based nn_systems_alignment nn_systems_word amod_systems_statistical amod_systems_various conj_and_developed_methods conj_and_developed_models dep_developed_Vogel dobj_developed_systems aux_developed_have nsubj_developed_researchers dep_developed_Work amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the prep_with_Starting_models dep_Work_Brown vmod_Work_Starting amod_Work_Related num_Work_2 ccomp_``_methods ccomp_``_models ccomp_``_developed
P06-1009	J93-2003	o	Most current SMT systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- use a generative model for word alignment such as the freely available GIZA + + -LRB- Och and Ney 2003 -RRB- an implementation of the IBM alignment models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_alignment nn_models_IBM det_models_the dep_implementation_Brown prep_of_implementation_models det_implementation_an num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_implementation conj_+_GIZA_+ amod_GIZA_available det_GIZA_the advmod_available_freely prep_such_as_alignment_implementation prep_such_as_alignment_+ prep_such_as_alignment_GIZA nn_alignment_word amod_model_generative det_model_a prep_for_use_alignment dobj_use_model nsubj_use_systems num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney appos_systems_2004 appos_systems_Ney appos_systems_Och nn_systems_SMT amod_systems_current amod_systems_Most
P06-1011	J93-2003	o	The first one GIZA-Lex is obtained by running the GIZA + +2 implementation of the IBM word alignment models -LRB- Brown et al. 1993 -RRB- on the initial parallel corpus	nn_corpus_parallel amod_corpus_initial det_corpus_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_alignment nn_models_word nn_models_IBM det_models_the num_implementation_+2 prep_of_GIZA_models conj_+_GIZA_implementation det_GIZA_the dobj_running_implementation dobj_running_GIZA prep_on_obtained_corpus dep_obtained_Brown agent_obtained_running auxpass_obtained_is nsubjpass_obtained_GIZA-Lex dobj_obtained_one amod_one_first det_one_The ccomp_``_obtained
P06-1032	J93-2003	p	In this paper we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation -LRB- SMT -RRB- -LRB- Brown et al. 1993 -RRB- can successfully provide editorial assistance for non-native writers	amod_writers_non-native prep_for_assistance_writers amod_assistance_editorial dobj_provide_assistance advmod_provide_successfully aux_provide_can nsubj_provide_Brown amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical prep_of_paradigm_Translation det_paradigm_the prep_within_instantiated_paradigm nsubj_instantiated_model mark_instantiated_that nn_model_channel amod_model_noisy det_model_a ccomp_show_provide ccomp_show_instantiated nsubj_show_we prep_in_show_paper det_paper_this
P06-1032	J93-2003	o	Rather than learning how strings in one language map to strings in another however translation now involves learning how systematic patterns of errors in ESL learners English map to corresponding patterns in native English 2.2 A Noisy Channel Model of ESL Errors If ESL error correction is seen as a translation task the task can be treated as an SMT problem using the noisy channel model of -LRB- Brown et al. 1993 -RRB- here the L2 sentence produced by the learner can be regarded as having been corrupted by noise in the form of interference from his or her L1 model and incomplete language models internalized during language learning	nn_learning_language prep_during_internalized_learning nn_models_language amod_models_incomplete vmod_model_internalized conj_and_model_models nn_model_L1 nn_model_her nn_model_his conj_or_his_her prep_from_interference_models prep_from_interference_model prep_of_form_interference det_form_the prep_in_noise_form agent_corrupted_noise auxpass_corrupted_been aux_corrupted_having prepc_as_regarded_corrupted auxpass_regarded_be aux_regarded_can nsubjpass_regarded_sentence advmod_regarded_here det_learner_the agent_produced_learner vmod_sentence_produced nn_sentence_L2 det_sentence_the num_al._1993 nn_al._et amod_al._Brown dep_of_al. prep_model_of nn_model_channel amod_model_noisy det_model_the dobj_using_model nn_problem_SMT det_problem_an prep_as_treated_problem auxpass_treated_be aux_treated_can nsubjpass_treated_task advcl_treated_seen dep_treated_patterns auxpass_treated_learning det_task_the nn_task_translation det_task_a prep_as_seen_task auxpass_seen_is nsubjpass_seen_correction mark_seen_If nn_correction_error nn_correction_ESL nn_Errors_ESL prep_of_Model_Errors nn_Model_Channel nn_Model_Noisy det_Model_A num_Model_2.2 amod_Model_English amod_Model_native prep_in_patterns_Model amod_patterns_corresponding amod_map_English nn_map_learners nn_map_ESL prep_to_errors_patterns prep_in_errors_map prep_of_patterns_errors amod_patterns_systematic advmod_systematic_how xcomp_involves_using xcomp_involves_treated advmod_involves_now nsubj_involves_translation nn_map_language num_map_one dep_strings_regarded rcmod_strings_involves advmod_strings_however prep_in_strings_another prep_to_strings_strings prep_in_strings_map dep_how_strings dobj_learning_how mark_learning_than advmod_learning_Rather advcl_``_learning
P06-1062	J93-2003	o	is combined with -LSB- -RSB- E jiT ,1 + to be aligned with -LSB- -RSB- F nmT then -LSB- -RSB- -LRB- -RRB- -LSB- -RSB- -LRB- -RRB- ATTCNTATTr E K E i FEF jinmjinm Pr P ,1 -RSB- -LSB- -RSB- -LSB- -RSB- ,1 -LSB- + = where K is the degree of EiN Finally the node translation probability is modeled as -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- tNtNlNlNNN EiFlEiFlEjFl PrPrPr And the text translation probability -LRB- -RRB- EF ttPr is model using IBM model I -LRB- Brown et al 1993 -RRB-	dep_al_1993 nn_al_et amod_al_Brown dep_I_al nn_I_model nn_I_IBM dobj_using_I vmod_model_using cop_model_is nsubj_model_ttPr nn_ttPr_EF rcmod_probability_model nn_probability_translation nn_probability_text det_probability_the nn_PrPrPr_EiFlEiFlEjFl nn_PrPrPr_tNtNlNlNNN dep_PrPrPr_as advcl_modeled_PrPrPr auxpass_modeled_is nsubjpass_modeled_probability nn_probability_translation nn_probability_node det_probability_the advmod_EiN_Finally prep_degree_of det_degree_the cop_degree_is nsubj_degree_K advmod_degree_where conj_and_=_probability dep_=_modeled dep_=_EiN advcl_=_degree dep_,1_probability dep_,1_= dep_,1_P nn_,1_Pr nn_jinmjinm_FEF appos_E_,1 appos_E_,1 npadvmod_E_jinmjinm nn_E_i nn_E_K nn_E_E nn_E_ATTCNTATTr dep_then_E advmod_nmT_then nn_nmT_F prep_with_aligned_nmT auxpass_aligned_be aux_aligned_to conj_+_jiT_aligned num_jiT_,1 nn_jiT_E prep_with_combined_aligned prep_with_combined_jiT auxpass_combined_is
P06-1062	J93-2003	o	However current sentence alignment models -LRB- Brown et al 1991 Gale & Church 1991 Wu 1994 Chen 489 1993 Zhao and Vogel 2002 etc -RRB-	conj_and_Zhao_Vogel number_1993_489 num_Chen_1993 num_Wu_1994 num_Gale_1991 conj_and_Gale_Church dep_Brown_etc dep_Brown_2002 dep_Brown_Vogel dep_Brown_Zhao dep_Brown_Chen dep_Brown_Wu dep_Brown_Church dep_Brown_Gale dep_Brown_1991 dep_Brown_al nn_Brown_et appos_models_Brown nn_models_alignment nn_models_sentence amod_models_current dep_However_models dep_``_However
P06-1067	J93-2003	o	N-gram language models have also been used in Statistical Machine Translation -LRB- SMT -RRB- as proposed by -LRB- Brown et al. 1990 Brown et al. 1993 -RRB-	num_Brown_1993 nn_Brown_al. nn_Brown_et dep_al._Brown num_al._1990 nn_al._et amod_al._Brown dep_by_al. prep_proposed_by mark_proposed_as appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical advcl_used_proposed prep_in_used_Translation auxpass_used_been advmod_used_also aux_used_have nsubjpass_used_models nn_models_language nn_models_N-gram
P06-1067	J93-2003	o	Distortion models were first proposed by -LRB- Brown et al. 1993 -RRB- in the so-called IBM Models	nn_Models_IBM amod_Models_so-called det_Models_the num_al._1993 nn_al._et amod_al._Brown dep_by_al. prep_in_proposed_Models prep_proposed_by dep_first_proposed nsubj_were_first dep_models_were nn_models_Distortion
P06-1077	J93-2003	n	1 Introduction Phrase-based translation models -LRB- Marcu and Wong 2002 Koehn et al. 2003 Och and Ney 2004 -RRB- which go beyond the original IBM translation models -LRB- Brown et al. 1993 -RRB- 1 by modeling translations of phrases rather than individual words have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations	amod_evaluations_empirical nn_translation_machine amod_translation_statistical prep_by_state-of-theart_evaluations prep_in_state-of-theart_translation det_state-of-theart_the cop_state-of-theart_be aux_state-of-theart_to xcomp_suggested_state-of-theart auxpass_suggested_been aux_suggested_have nsubjpass_suggested_models amod_words_individual conj_negcc_translations_words prep_of_translations_phrases nn_translations_modeling prep_by_1_words prep_by_1_translations amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation nn_models_IBM amod_models_original det_models_the prep_beyond_go_models nsubj_go_which dep_Och_2004 conj_and_Och_Ney num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Marcu_Ney dep_Marcu_Och conj_and_Marcu_Koehn num_Marcu_2002 conj_and_Marcu_Wong dep_models_1 dep_models_Brown rcmod_models_go appos_models_Koehn appos_models_Wong appos_models_Marcu nn_models_translation amod_models_Phrase-based nn_models_Introduction num_models_1 ccomp_``_suggested
P06-1082	J93-2003	o	Use of sententially aligned corpora for word alignment has already been recommended in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_recommended_in auxpass_recommended_been advmod_recommended_already aux_recommended_has nsubjpass_recommended_Use nn_alignment_word amod_corpora_aligned advmod_aligned_sententially prep_for_Use_alignment prep_of_Use_corpora
P06-1082	J93-2003	o	1 Introduction Several approaches including statistical techniques -LRB- Gale and Church 1991 Brown et al. 1993 -RRB- lexical techniques -LRB- Huang and Choi 2000 Tiedemann 2003 -RRB- and hybrid techniques -LRB- Ahrenberg et al. 2000 -RRB- have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus	amod_corpus_parallel det_corpus_a nn_language_target det_language_a conj_and_language_language nn_language_source det_language_a prep_of_words_language prep_of_words_language prep_between_links_words prep_in_establishing_corpus dobj_establishing_links prepc_at_aims_establishing nsubj_aims_which rcmod_alignment_aims nn_alignment_word prep_for_design_alignment dobj_design_schemes aux_design_to xcomp_pursued_design auxpass_pursued_been aux_pursued_have nsubjpass_pursued_approaches dep_pursued_Introduction amod_Ahrenberg_2000 dep_Ahrenberg_al. nn_Ahrenberg_et nn_techniques_hybrid num_Tiedemann_2003 conj_and_Huang_Tiedemann conj_and_Huang_2000 conj_and_Huang_Choi dep_techniques_Tiedemann dep_techniques_2000 dep_techniques_Choi dep_techniques_Huang amod_techniques_lexical num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Gale_Brown conj_and_Gale_1991 conj_and_Gale_Church conj_and_techniques_techniques conj_and_techniques_techniques dep_techniques_1991 dep_techniques_Church dep_techniques_Gale amod_techniques_statistical appos_approaches_Ahrenberg prep_including_approaches_techniques prep_including_approaches_techniques prep_including_approaches_techniques amod_approaches_Several num_Introduction_1
P06-1091	J93-2003	o	5 Discussion and Future Work The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_presented_in vmod_approach_presented nn_approach_channel amod_approach_noisy det_approach_the prep_on_based_approach prep_in_work_SMT amod_work_previous vmod_differs_based prep_from_differs_work advmod_differs_substantially nsubj_differs_work dep_differs_Work dep_differs_Discussion det_paper_this prep_in_work_paper det_work_The amod_Work_Future conj_and_Discussion_Work num_Discussion_5
P06-1097	J93-2003	p	1 Introduction The most widely applied training procedure for statistical machine translation IBM model 4 -LRB- Brown et al. 1993 -RRB- unsupervised training followed by post-processing with symmetrization heuristics -LRB- Och and Ney 2003 -RRB- yields low quality word alignments	nn_alignments_word nn_alignments_quality amod_alignments_low dep_yields_alignments dep_yields_Ney dep_yields_Och dep_Och_2003 conj_and_Och_Ney dep_heuristics_yields nn_heuristics_symmetrization prep_with_post-processing_heuristics agent_followed_post-processing vmod_training_followed amod_training_unsupervised dep_training_Introduction amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_IBM nn_model_translation nn_model_machine amod_model_statistical prep_for_procedure_model nn_procedure_training amod_procedure_applied det_procedure_The advmod_applied_widely advmod_widely_most dep_Introduction_Brown dep_Introduction_procedure num_Introduction_1
P06-1097	J93-2003	o	We rst recast the problem of estimating the IBM models -LRB- Brown et al. 1993 -RRB- in a discriminative framework which leads to an initial increase in word-alignment accuracy	amod_accuracy_word-alignment prep_in_increase_accuracy amod_increase_initial det_increase_an prep_to_leads_increase nsubj_leads_which rcmod_framework_leads amod_framework_discriminative det_framework_a amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the dobj_estimating_models prep_in_problem_framework dep_problem_Brown prepc_of_problem_estimating det_problem_the prep_recast_rst_problem nsubj_rst_We ccomp_``_rst
P06-1097	J93-2003	o	4 Semi-Supervised Training for Word Alignments Intuitively in approximate EM training for Model 4 -LRB- Brown et al. 1993 -RRB- the E-step corresponds to calculating the probability of all alignments according to the current model estimate while the M-step is the creation of a new model estimate given a probability distribution over alignments -LRB- calculated in the E-step -RRB-	det_E-step_the prep_in_calculated_E-step dep_distribution_calculated prep_over_distribution_alignments nn_distribution_probability det_distribution_a pobj_given_distribution prep_estimate_given nn_estimate_model amod_estimate_new det_estimate_a prep_of_creation_estimate det_creation_the cop_creation_is nsubj_creation_M-step mark_creation_while det_M-step_the nn_estimate_model amod_estimate_current det_estimate_the det_alignments_all prep_of_probability_alignments det_probability_the pobj_calculating_estimate prepc_according_to_calculating_to dobj_calculating_probability prepc_to_corresponds_calculating nsubj_corresponds_E-step det_E-step_the advcl_Brown_creation rcmod_Brown_corresponds amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 prep_for_training_Model nn_training_EM amod_training_approximate dep_,_Brown prep_in_,_training nn_Intuitively_Alignments nn_Intuitively_Word prep_for_Training_Intuitively amod_Training_Semi-Supervised num_Training_4 dep_``_Training
P06-1098	J93-2003	o	1 Introduction In a classical statistical machine translation a foreign language sentence f J1 = f1 f2 fJ is translated into another language i.e. English eI1 = e1 e2 eI by seeking a maximum likely solution of eI1 = argmax eI1 Pr -LRB- eI1 | f J1 -RRB- -LRB- 1 -RRB- = argmax eI1 Pr -LRB- f J1 | eI1 -RRB- Pr -LRB- eI1 -RRB- -LRB- 2 -RRB- The source channel approach in Equation 2 independently decomposes translation knowledge into a translation model and a language model respectively -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_respectively_Brown nn_model_language det_model_a conj_and_model_model nn_model_translation det_model_a nn_knowledge_translation advmod_decomposes_respectively prep_into_decomposes_model prep_into_decomposes_model dobj_decomposes_knowledge advmod_decomposes_independently num_Equation_2 dep_approach_decomposes prep_in_approach_Equation nn_approach_channel nn_approach_source det_approach_The dep_approach_2 nn_approach_Pr appos_Pr_eI1 nn_Pr_Pr nn_eI1_| nn_eI1_J1 nn_eI1_f appos_Pr_eI1 nn_Pr_eI1 nn_Pr_argmax dep_=_approach dep_=_1 dep_J1_f num_J1_| nn_J1_eI1 amod_Pr_= appos_Pr_J1 nn_Pr_eI1 nn_Pr_argmax dep_=_Pr amod_eI1_= dep_of_eI1 prep_solution_of amod_solution_likely nn_solution_maximum det_solution_a dobj_seeking_solution prepc_by_eI_seeking appos_e1_eI conj_e1_e2 dep_=_e1 amod_eI1_= dep_i.e._English det_language_another dep_translated_eI1 dep_translated_i.e. prep_into_translated_language auxpass_translated_is nsubjpass_translated_Introduction appos_f1_fJ conj_f1_f2 dep_=_f1 amod_J1_= nn_J1_f nn_J1_sentence nn_J1_language amod_J1_foreign det_J1_a nn_translation_machine amod_translation_statistical amod_translation_classical det_translation_a appos_Introduction_J1 prep_in_Introduction_translation num_Introduction_1 ccomp_``_translated
P06-1122	J93-2003	p	Aligning tokens in parallel sentences using the IBM Models -LRB- Brown et al. 1993 -RRB- -LRB- Och and Ney 2003 -RRB- may require less information than full-blown translation since the task is constrained by the source and target tokens present in each sentence pair	nn_pair_sentence det_pair_each prep_in_present_pair nn_tokens_target amod_source_present conj_and_source_tokens det_source_the agent_constrained_tokens agent_constrained_source auxpass_constrained_is nsubjpass_constrained_task mark_constrained_since det_task_the amod_translation_full-blown prep_than_information_translation amod_information_less advcl_require_constrained dobj_require_information aux_require_may nsubj_require_tokens amod_Och_2003 conj_and_Och_Ney amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_Models_IBM det_Models_the dobj_using_Models amod_sentences_parallel appos_tokens_Ney appos_tokens_Och dep_tokens_Brown vmod_tokens_using prep_in_tokens_sentences amod_tokens_Aligning
P06-2005	J93-2003	o	We thus propose to adapt the statistical machine translation model -LRB- Brown et al. 1993 Zens and Ney 2004 -RRB- for SMS text normalization	nn_normalization_text nn_normalization_SMS num_Zens_2004 conj_and_Zens_Ney dep_al._Ney dep_al._Zens num_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation nn_model_machine amod_model_statistical det_model_the prep_for_adapt_normalization dobj_adapt_model aux_adapt_to xcomp_propose_adapt advmod_propose_thus nsubj_propose_We ccomp_``_propose
P06-2005	J93-2003	o	A null Assuming that one SMS word is mapped exactly to one English word in the channel model under an alignment we need to consider only two types of probabilities the alignment probabilities denoted by Pm and the lexicon mapping probabilities denoted by -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_by_al. prep_denoted_by vmod_probabilities_denoted nn_probabilities_mapping nn_probabilities_lexicon det_probabilities_the conj_and_Pm_probabilities agent_denoted_probabilities agent_denoted_Pm vmod_probabilities_denoted nn_probabilities_alignment det_probabilities_the prep_of_types_probabilities num_types_two quantmod_two_only dobj_consider_types aux_consider_to dobj_need_probabilities xcomp_need_consider nsubj_need_we ccomp_need_null det_alignment_an nn_model_channel det_model_the prep_in_word_model nn_word_English num_word_one prep_under_mapped_alignment prep_to_mapped_word advmod_mapped_exactly auxpass_mapped_is nsubjpass_mapped_word mark_mapped_that nn_word_SMS num_word_one ccomp_Assuming_mapped amod_null_Assuming det_null_A
P06-2014	J93-2003	o	The IBM models -LRB- Brown et al. 1993 -RRB- benefit from a one-tomany constraint where each target word has ex105 the tax causes unrest l' impt cause le malaise Figure 1 A cohesion constraint violation	nn_violation_constraint nn_violation_cohesion det_violation_A num_Figure_1 nn_Figure_malaise det_Figure_le nn_Figure_cause amod_Figure_impt nn_Figure_l' nn_Figure_unrest dobj_causes_Figure nsubj_causes_tax det_tax_the ccomp_ex105_causes aux_ex105_has nsubj_ex105_word advmod_ex105_where nn_word_target det_word_each dep_constraint_violation rcmod_constraint_ex105 amod_constraint_one-tomany det_constraint_a prep_from_benefit_constraint appos_benefit_Brown nsubj_benefit_models amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_The
P06-2014	J93-2003	o	Originally introduced as a byproduct of training statistical translation models in -LRB- Brown et al. 1993 -RRB- word alignment has become the first step in training most statistical translation systems and alignments are useful to a host of other tasks	amod_tasks_other prep_of_host_tasks det_host_a prep_to_useful_host cop_useful_are nsubj_useful_alignments nn_systems_translation amod_systems_statistical nn_systems_training advmod_statistical_most prep_in_step_systems amod_step_first det_step_the xcomp_become_step aux_become_has nsubj_become_alignment nn_alignment_word num_al._1993 nn_al._et amod_al._Brown dep_in_al. nn_models_translation amod_models_statistical nn_models_training amod_byproduct_in prep_of_byproduct_models det_byproduct_a conj_and_introduced_useful parataxis_introduced_become prep_as_introduced_byproduct advmod_introduced_Originally
P06-2061	J93-2003	o	4.7 Fertility-Based Transducer In -LRB- Brown et al. 1993 -RRB- three alignment models are described that include fertility models these are IBM Models 3 4 and 5	conj_and_Models_5 num_Models_4 num_Models_3 nn_Models_IBM cop_Models_are nsubj_Models_these ccomp_Models_described nn_models_fertility dobj_include_models nsubj_include_that ccomp_described_include auxpass_described_are nsubjpass_described_Transducer nn_models_alignment num_models_three num_al._1993 nn_al._et amod_al._Brown dep_In_al. appos_Transducer_models amod_Transducer_In amod_Transducer_Fertility-Based num_Transducer_4.7
P06-2061	J93-2003	o	In -LRB- Brown et al. 1994 -RRB- the authors proposed a method to integrate the IBM translation model 2 -LRB- Brown et al. 1993 -RRB- with an ASR system	nn_system_ASR det_system_an amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_translation nn_model_IBM det_model_the dobj_integrate_model aux_integrate_to vmod_method_integrate det_method_a prep_with_proposed_system dep_proposed_Brown dobj_proposed_method nsubj_proposed_authors prep_proposed_In det_authors_the num_al._1994 nn_al._et amod_al._Brown dep_In_al.
P06-2061	J93-2003	o	We rescore the ASR N-best lists with the standard HMM -LRB- Vogel et al. 1996 -RRB- and IBM -LRB- Brown et al. 1993 -RRB- MT models	nn_models_MT nn_models_IBM amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_IBM_Brown amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et amod_HMM_standard det_HMM_the nn_lists_N-best nn_lists_ASR det_lists_the conj_and_rescore_models dep_rescore_Vogel prep_with_rescore_HMM dobj_rescore_lists nsubj_rescore_We ccomp_``_models ccomp_``_rescore
P06-2065	J93-2003	o	This is similar to Model 3 of -LRB- Brown et al. 1993 -RRB- but without null-generated elements or re-ordering	conj_or_elements_re-ordering amod_elements_null-generated pobj_without_re-ordering pobj_without_elements num_al._1993 nn_al._et amod_al._Brown dep_of_al. amod_Model_of num_Model_3 conj_but_similar_without prep_to_similar_Model cop_similar_is nsubj_similar_This ccomp_``_without ccomp_``_similar
P06-2065	J93-2003	o	Learned vowels include -LRB- in order of generation probability -RRB- e a o u i y. Learned sonorous consonants include n s r l m. Learned non-sonorous consonants include d c t l b m p q The model bootstrapping is good for dealing with too many parameters we see a similar approach in Brown et als -LRB- 1993 -RRB- march from Model 1 to Model 5	num_Model_5 num_Model_1 prep_to_march_Model prep_from_march_Model nsubj_march_approach appos_als_1993 nn_als_et nn_als_Brown prep_in_approach_als amod_approach_similar det_approach_a ccomp_see_march nsubj_see_we amod_parameters_many advmod_many_too prep_with_dealing_parameters prepc_for_good_dealing cop_good_is nsubj_good_bootstrapping nn_bootstrapping_model det_bootstrapping_The parataxis_d_see conj_d_good conj_d_q conj_d_p conj_d_m conj_d_b conj_d_l conj_d_t conj_d_c dep_include_d nsubj_include_n amod_consonants_non-sonorous amod_consonants_Learned nn_consonants_m. appos_n_consonants appos_n_l appos_n_r appos_n_s nsubj_include_consonants amod_consonants_sonorous amod_consonants_Learned dep_o_include rcmod_o_include appos_o_y. appos_o_i appos_o_u det_o_a appos_e_o dep_e_in nn_probability_generation prep_of_order_probability pobj_in_order dep_include_e nsubj_include_vowels amod_vowels_Learned
P06-2065	J93-2003	o	Machine translation has code-like characteristics and indeed the initial models of -LRB- Brown et al. 1993 -RRB- took a word-substitution/transposition approach trained on a parallel text	amod_text_parallel det_text_a prep_on_trained_text vmod_approach_trained amod_approach_word-substitution/transposition det_approach_a dobj_took_approach nsubj_took_models advmod_took_indeed num_al._1993 nn_al._et amod_al._Brown dep_of_al. amod_models_of amod_models_initial det_models_the amod_characteristics_code-like conj_and_has_took dobj_has_characteristics nsubj_has_translation nn_translation_Machine
P06-2065	J93-2003	p	Such methods have also been a key driver of progress in statistical machine translation which depends heavily on unsupervised word alignments -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_alignments_Brown nn_alignments_word amod_alignments_unsupervised prep_on_depends_alignments advmod_depends_heavily nsubj_depends_which nn_translation_machine amod_translation_statistical rcmod_driver_depends prep_in_driver_translation prep_of_driver_progress amod_driver_key det_driver_a cop_driver_been advmod_driver_also aux_driver_have nsubj_driver_methods amod_methods_Such
P06-2070	J93-2003	o	The corpus is aligned in the word level using IBM Model4 -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_Model4_al. nn_Model4_IBM dobj_using_Model4 nn_level_word det_level_the xcomp_aligned_using prep_in_aligned_level auxpass_aligned_is nsubjpass_aligned_corpus det_corpus_The
P06-2092	J93-2003	o	The alignment of sentences can be done sufficiently well using cues such as sentence length -LRB- Gale and Church 1993 -RRB- or cognates -LRB- Simard et al. 1992 -RRB-	amod_Simard_1992 dep_Simard_al. nn_Simard_et dep_Gale_1993 conj_and_Gale_Church conj_or_length_cognates dep_length_Church dep_length_Gale nn_length_sentence prep_such_as_cues_cognates prep_such_as_cues_length dobj_using_cues advmod_using_well advmod_well_sufficiently dep_done_Simard xcomp_done_using auxpass_done_be aux_done_can nsubjpass_done_alignment prep_of_alignment_sentences det_alignment_The
P06-2092	J93-2003	o	Wordalignment however isalmost exclusively done using statistics -LRB- Brown et al. 1993 Hiemstra 1996 Vogel et al. 1999 Toutanova et al. 2002 -RRB-	nn_al._et nn_al._Toutanova nn_Vogel_al. nn_Vogel_et num_Hiemstra_1996 amod_Brown_2002 dep_Brown_al. dep_Brown_1999 dep_Brown_Vogel dep_Brown_Hiemstra amod_Brown_1993 dep_Brown_al. nn_Brown_et dobj_using_statistics dep_done_Brown xcomp_done_using advmod_done_exclusively nsubj_done_isalmost advmod_done_however nsubj_done_Wordalignment
P06-2092	J93-2003	o	2.2 Word Alignment Aligning below the sentence level is usually done using statistical models for machine translation -LRB- Brown et al. 1991 Brown et al. 1993 Hiemstra 1996 Vogel et al. 1999 -RRB- where any word of the targetlanguageistakentobeapossibletranslation for each source language word	nn_word_language nn_word_source det_word_each det_targetlanguageistakentobeapossibletranslation_the prep_for_word_word prep_of_word_targetlanguageistakentobeapossibletranslation det_word_any dep_where_word num_Vogel_1999 nn_Vogel_al. nn_Vogel_et num_Hiemstra_1996 dep_Brown_where dep_Brown_Vogel conj_Brown_Hiemstra num_Brown_1993 nn_Brown_al. nn_Brown_et dep_al._Brown dep_al._1991 nn_al._et amod_al._Brown nn_translation_machine amod_models_statistical prep_for_using_translation dobj_using_models dep_done_al. xcomp_done_using advmod_done_usually auxpass_done_is nsubjpass_done_Alignment nn_level_sentence det_level_the prep_below_Aligning_level vmod_Alignment_Aligning nn_Alignment_Word num_Alignment_2.2
P06-2092	J93-2003	o	3.2 Word Order Differences Another problem that has been noticed as early as 1993 with the first research on word alignment -LRB- Brown et al. 1993 -RRB- concerns the differences in word order between source and target language	nn_language_target conj_and_source_language prep_between_order_language prep_between_order_source nn_order_word prep_in_differences_order det_differences_the dobj_concerns_differences nsubj_concerns_problem num_al._1993 nn_al._et amod_al._Brown nn_alignment_word prep_on_research_alignment amod_research_first det_research_the prep_as_early_1993 advmod_early_as prep_with_noticed_research advmod_noticed_early auxpass_noticed_been aux_noticed_has nsubjpass_noticed_that dep_problem_al. rcmod_problem_noticed det_problem_Another rcmod_Differences_concerns nn_Differences_Order nn_Differences_Word num_Differences_3.2
P06-2092	J93-2003	o	While simple statistical alignment models like IBM-1 -LRB- Brown et al. 1993 -RRB- and the symmetric alignment approach by Hiemstra -LRB- 1996 -RRB- treat sentences as unstructured bags of words the more sophisticated IBM-models by Brown et al.	dep_Brown_al. nn_Brown_et prep_by_IBM-models_Brown amod_IBM-models_sophisticated det_IBM-models_the advmod_sophisticated_more prep_of_bags_words amod_bags_unstructured nn_sentences_treat nn_sentences_Hiemstra appos_Hiemstra_1996 prep_as_approach_bags prep_by_approach_sentences nn_approach_alignment amod_approach_symmetric det_approach_the num_al._1993 nn_al._et amod_al._Brown conj_and_IBM-1_approach dep_IBM-1_al. appos_models_IBM-models prep_like_models_approach prep_like_models_IBM-1 nn_models_alignment amod_models_statistical amod_models_simple pobj_While_models dep_``_While
P06-2092	J93-2003	o	1 Introduction Aligning parallel text i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation is a very useful preprocessing step for a range of applications including but not limited to machine translation -LRB- Brown et al. 1993 -RRB- cross-language information retrieval -LRB- Hiemstra 1996 -RRB- dictionary creation -LRB- Smadja et al. 1996 -RRB- and induction of NLP-tools -LRB- Kuhn 2004 -RRB-	amod_Kuhn_2004 dep_NLP-tools_Kuhn prep_of_induction_NLP-tools amod_Smadja_1996 dep_Smadja_al. nn_Smadja_et dep_creation_Smadja nn_creation_dictionary dep_Hiemstra_1996 conj_and_retrieval_induction conj_and_retrieval_creation dep_retrieval_Hiemstra nn_retrieval_information amod_retrieval_cross-language num_al._1993 nn_al._et amod_al._Brown nn_translation_machine prep_to_limited_translation dep_including_al. conj_negcc_including_limited rcmod_applications_limited rcmod_applications_including prep_of_range_applications det_range_a dep_step_induction dep_step_creation dep_step_retrieval prep_for_step_range amod_step_preprocessing amod_step_useful det_step_a cop_step_is nsubj_step_Introduction advmod_useful_very det_translation_a prep_in_equivalents_translation poss_equivalents_their prep_with_correspondence_equivalents num_text_one conj_or_sentences_words det_sentences_the prep_into_setting_correspondence prep_in_setting_text dobj_setting_words dobj_setting_sentences advmod_setting_automatically advmod_setting_i.e. amod_text_parallel dobj_Aligning_text vmod_Introduction_setting vmod_Introduction_Aligning num_Introduction_1
P06-2093	J93-2003	o	The classical Bayes relation is used to introduce a target language model -LRB- Brown et al. 1993 -RRB- e = argmaxe Pr -LRB- e | f -RRB- = argmaxe Pr -LRB- f | e -RRB- Pr -LRB- e -RRB- where Pr -LRB- f | e -RRB- is the translation model and Pr -LRB- e -RRB- is the target language model	nn_model_language nn_model_target det_model_the cop_model_is nsubj_model_Pr nsubj_model_model dep_model_e nn_model_Pr dep_model_e nn_model_| appos_Pr_e conj_and_model_Pr nn_model_translation det_model_the cop_model_is nsubj_model_Pr advmod_model_where advmod_e_| nn_|_f dep_Pr_e nn_|_f amod_Pr_argmaxe dep_=_model dep_=_Pr dep_=_f dep_=_Pr dep_f_| dep_|_e amod_Pr_argmaxe dep_=_= dep_=_e dep_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_language nn_model_target det_model_a dobj_introduce_model aux_introduce_to parataxis_used_= xcomp_used_introduce auxpass_used_is nsubjpass_used_relation nn_relation_Bayes amod_relation_classical det_relation_The
P06-2093	J93-2003	o	2 Statistical Translation Engine A word-based translation engine is used based on the so-called IBM-4 model -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_IBM-4 amod_model_so-called det_model_the prep_based_on_used_model auxpass_used_is nsubjpass_used_engine nn_engine_translation amod_engine_word-based nn_engine_A nn_engine_Engine nn_engine_Translation amod_engine_Statistical num_engine_2 ccomp_``_used
P06-2103	J93-2003	o	-LRB- A similar intuition holds for the Machine Translation models generically known as the IBM models -LRB- Brown et al. 1993 -RRB- which assume that certain words in a source language sentence tend to trigger the usage of certain words in a target language translation of that sentence -RRB-	det_sentence_that prep_of_translation_sentence nn_translation_language nn_translation_target det_translation_a prep_in_words_translation amod_words_certain prep_of_usage_words det_usage_the dobj_trigger_usage aux_trigger_to xcomp_tend_trigger nsubj_tend_words mark_tend_that nn_sentence_language nn_sentence_source det_sentence_a prep_in_words_sentence amod_words_certain ccomp_assume_tend nsubj_assume_which amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the prep_as_known_models advmod_known_generically vmod_models_known nn_models_Translation nn_models_Machine det_models_the dep_holds_assume dep_holds_Brown prep_for_holds_models nsubj_holds_intuition amod_intuition_similar det_intuition_A
P06-2107	J93-2003	o	The methodology used -LRB- Brown et al. 1993 -RRB- is based on the definition of a function Pr -LRB- tI1 | sJ1 -RRB- that returns the probability that tI1 is a 835 source Transferir documentos explorados a otro directorio interaction-0 Move documents scanned to other directory interaction-1 Move s canned documents to other directory interaction-2 Move scanned documents to a nother directory interaction-3 Move scanned documents to another f older acceptance Move scanned documents to another folder Figure 1 Example of CAT system interactions to translate the Spanish source sentence into English	nn_sentence_source amod_sentence_Spanish det_sentence_the prep_into_translate_English dobj_translate_sentence aux_translate_to nn_interactions_system nn_interactions_CAT vmod_Example_translate prep_of_Example_interactions num_Figure_1 nn_Figure_folder det_Figure_another amod_documents_scanned dobj_Move_Example prep_to_Move_Figure dobj_Move_documents amod_acceptance_older nn_acceptance_f det_acceptance_another amod_documents_scanned prep_to_Move_acceptance dobj_Move_documents nn_interaction-3_directory amod_interaction-3_nother det_interaction-3_a amod_documents_scanned dep_Move_Move dep_Move_Move prep_to_Move_interaction-3 dobj_Move_documents nn_interaction-2_directory amod_interaction-2_other amod_documents_canned nn_documents_s prep_to_Move_interaction-2 dobj_Move_documents nn_interaction-1_directory amod_interaction-1_other prep_to_scanned_interaction-1 vmod_documents_scanned dep_Move_Move dobj_Move_documents nn_interaction-0_directorio nn_interaction-0_otro det_interaction-0_a dobj_explorados_interaction-0 dep_documentos_explorados nn_documentos_Transferir nn_documentos_source num_documentos_835 det_documentos_a cop_documentos_is nsubj_documentos_tI1 dobj_documentos_that rcmod_probability_documentos det_probability_the dobj_returns_probability nsubj_returns_that num_sJ1_| nn_sJ1_tI1 rcmod_Pr_returns appos_Pr_sJ1 nn_Pr_function det_Pr_a prep_of_definition_Pr det_definition_the dep_based_Move dep_based_Move prep_on_based_definition auxpass_based_is nsubjpass_based_methodology amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_methodology_Brown vmod_methodology_used det_methodology_The
P06-2107	J93-2003	o	Models of this kind assume that an input word is generated by only one output word -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. appos_word_Brown nn_word_output num_word_one quantmod_one_only agent_generated_word auxpass_generated_is nsubjpass_generated_word mark_generated_that nn_word_input det_word_an ccomp_assume_generated nsubj_assume_Models det_kind_this prep_of_Models_kind
P06-2107	J93-2003	o	These alignments can be obtained from single-word models -LRB- Brown et al. 1993 -RRB- using the available public software GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ nn_GIZA_software amod_GIZA_public amod_GIZA_available det_GIZA_the dobj_using_+ dobj_using_GIZA amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_models_single-word xcomp_obtained_using dep_obtained_Brown prep_from_obtained_models auxpass_obtained_be aux_obtained_can nsubjpass_obtained_alignments det_alignments_These
P06-2111	J93-2003	o	For this we used two resources CELEX a linguistically annotated dictionary of English Dutch and German -LRB- Baayen et al. 1993 -RRB- and the Dutch snowball stemmer implementing a suf x stripping algorithm based on the Porter stemmer	nn_stemmer_Porter det_stemmer_the prep_on_based_stemmer vmod_algorithm_based amod_algorithm_stripping nn_algorithm_x nn_algorithm_suf det_algorithm_a dobj_implementing_algorithm vmod_stemmer_implementing nn_stemmer_snowball amod_stemmer_Dutch det_stemmer_the amod_Baayen_1993 dep_Baayen_al. nn_Baayen_et conj_and_English_German conj_and_English_Dutch conj_and_dictionary_stemmer dep_dictionary_Baayen prep_of_dictionary_German prep_of_dictionary_Dutch prep_of_dictionary_English amod_dictionary_annotated det_dictionary_a nn_dictionary_CELEX advmod_annotated_linguistically dep_resources_stemmer dep_resources_dictionary num_resources_two dobj_used_resources nsubj_used_we prep_for_used_this
P06-2111	J93-2003	p	For the word alignment we apply standard techniques derived from statistical machine translation using the well-known IBM alignment models -LRB- Brown et al. 1993 -RRB- implemented in the opensource tool GIZA + + -LRB- Och 2003 -RRB-	amod_Och_2003 dep_+_Och conj_+_GIZA_+ nn_GIZA_tool nn_GIZA_opensource det_GIZA_the prep_in_implemented_+ prep_in_implemented_GIZA amod_Brown_1993 dep_Brown_al. nn_Brown_et vmod_models_implemented dep_models_Brown nn_models_alignment nn_models_IBM amod_models_well-known det_models_the dobj_using_models nn_translation_machine amod_translation_statistical prep_from_derived_translation vmod_techniques_derived amod_techniques_standard xcomp_apply_using dobj_apply_techniques nsubj_apply_we prep_for_apply_alignment nn_alignment_word det_alignment_the
P06-2112	J93-2003	o	1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical prep_of_result_translation amod_result_intermediate det_result_an prep_as_proposed_result dep_first_al. dep_first_proposed nsubj_was_first dep_alignment_was nn_alignment_Word nn_alignment_Introduction num_alignment_1
P06-2112	J93-2003	o	3 Statistical Word Alignment According to the IBM models -LRB- Brown et al. 1993 -RRB- the statistical word alignment model can be generally represented as in equation -LRB- 1 -RRB-	appos_equation_1 pobj_in_equation pcomp_as_in prep_represented_as advmod_represented_generally auxpass_represented_be aux_represented_can nsubjpass_represented_model nn_model_alignment nn_model_word amod_model_statistical det_model_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the rcmod_Alignment_represented dep_Alignment_Brown pobj_Alignment_models prepc_according_to_Alignment_to nn_Alignment_Word amod_Alignment_Statistical num_Alignment_3
P06-2117	J93-2003	o	2 Statistical Word Alignment Model According to the IBM models -LRB- Brown et al. 1993 -RRB- the statistical word alignment model can be generally represented as in equation -LRB- 1 -RRB-	appos_equation_1 pobj_in_equation pcomp_as_in prep_represented_as advmod_represented_generally auxpass_represented_be aux_represented_can nsubjpass_represented_model dep_represented_Model nn_model_alignment nn_model_word amod_model_statistical det_model_the amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the dep_Model_Brown pobj_Model_models prepc_according_to_Model_to nn_Model_Alignment nn_Model_Word amod_Model_Statistical num_Model_2
P06-2117	J93-2003	o	1 A cept is defined as the set of target words connected to a source word -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. appos_word_Brown nn_word_source det_word_a prep_to_connected_word vmod_words_connected nn_words_target prep_of_set_words det_set_the prep_as_defined_set auxpass_defined_is nsubjpass_defined_cept det_cept_A num_cept_1 ccomp_``_defined
P06-2117	J93-2003	o	1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_translation_machine amod_translation_statistical prep_of_result_translation amod_result_intermediate det_result_an prep_as_proposed_result dep_first_al. dep_first_proposed nsubj_was_first dep_alignment_was nn_alignment_Word nn_alignment_Introduction num_alignment_1
P06-2124	J93-2003	o	Most current approaches emphasize within-sentence dependencies such as the distortion in -LRB- Brown et al. 1993 -RRB- the dependency of alignment in HMM -LRB- Vogel et al. 1996 -RRB- and syntax mappings in -LRB- Yamada and Knight 2001 -RRB-	amod_Yamada_2001 conj_and_Yamada_Knight dep_in_Knight dep_in_Yamada prep_mappings_in nn_mappings_syntax num_Vogel_1996 nn_Vogel_al. nn_Vogel_et prep_in_alignment_HMM conj_and_dependency_mappings appos_dependency_Vogel prep_of_dependency_alignment det_dependency_the num_al._1993 nn_al._et amod_al._Brown dep_in_al. amod_distortion_in det_distortion_the prep_such_as_dependencies_distortion nn_dependencies_within-sentence dobj_emphasize_mappings dobj_emphasize_dependency dobj_emphasize_dependencies nsubj_emphasize_approaches amod_approaches_current amod_approaches_Most
P06-2124	J93-2003	o	2.1 Baseline IBM Model-1 The translation process can be viewed as operations of word substitutions permutations and insertions/deletions -LRB- Brown et al. 1993 -RRB- in noisychannel modeling scheme at parallel sentence-pair level	nn_level_sentence-pair amod_level_parallel nn_scheme_modeling nn_scheme_noisychannel amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_insertions/deletions_Brown conj_and_substitutions_insertions/deletions conj_and_substitutions_permutations nn_substitutions_word prep_in_operations_scheme prep_of_operations_insertions/deletions prep_of_operations_permutations prep_of_operations_substitutions prep_at_viewed_level prep_as_viewed_operations auxpass_viewed_be aux_viewed_can nsubjpass_viewed_process nn_process_translation nn_process_The nn_process_Model-1 nn_process_IBM dep_Baseline_viewed num_Baseline_2.1 dep_``_Baseline
P06-2124	J93-2003	o	-LRB- 1 -RRB- 1We follow the notations in -LRB- Brown et al. 1993 -RRB- for English-French i.e. e f although our models are tested in this paper for English-Chinese	pobj_for_English-Chinese ccomp_,_for det_paper_this pobj_in_paper dep_,_in auxpass_tested_are nsubjpass_tested_models mark_tested_although poss_models_our advcl_f_tested dep_f_e nsubj_f_English-French dep_English-French_i.e. prepc_for_al._f num_al._1993 nn_al._et amod_al._Brown det_notations_the prep_in_follow_al. dobj_follow_notations nsubj_follow_1We dep_follow_1
P07-1001	J93-2003	o	For instance the most relaxed IBM Model-1 which assumes that any source word can be generated by any target word equally regardless of distance can be improved by demanding a Markov process of alignments as in HMM-based models -LRB- Vogel et al. 1996 -RRB- or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_models_fertility-based nn_models_IBM dep_in_Brown pobj_in_models pobj_as_in prep_word_as nn_word_source det_word_a prep_to_linked_word vmod_words_linked nn_words_target prep_of_number_words prep_of_distribution_number det_distribution_a dobj_implementing_distribution nsubjpass_implementing_Model-1 amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et amod_models_HMM-based pobj_in_models pcomp_as_in prep_of_process_alignments nn_process_Markov det_process_a prep_demanding_as dobj_demanding_process conj_or_improved_implementing dep_improved_Vogel agent_improved_demanding auxpass_improved_be aux_improved_can nsubjpass_improved_Model-1 prep_for_improved_instance nn_word_target det_word_any prep_regardless_of_generated_distance advmod_generated_equally agent_generated_word auxpass_generated_be aux_generated_can nsubjpass_generated_word mark_generated_that nn_word_source det_word_any ccomp_assumes_generated nsubj_assumes_which rcmod_Model-1_assumes nn_Model-1_IBM amod_Model-1_relaxed det_Model-1_the advmod_relaxed_most rcmod_``_implementing rcmod_``_improved
P07-1001	J93-2003	o	For the simple bag-of-word bilingual LSA as describedinSection2 .2.1 afterSVDonthesparsematrix using the toolkit SVDPACK -LRB- Berry et al. 1993 -RRB- all source and target words are projected into a lowdimensional -LRB- R = 88 -RRB- LSA-space	amod_LSA-space_lowdimensional det_LSA-space_a dobj_=_88 nsubj_=_R dep_lowdimensional_= prep_into_projected_LSA-space auxpass_projected_are nsubjpass_projected_words nsubjpass_projected_source dep_projected_afterSVDonthesparsematrix prep_for_projected_LSA nn_words_target conj_and_source_words det_source_all amod_Berry_1993 dep_Berry_al. nn_Berry_et appos_SVDPACK_Berry nn_SVDPACK_toolkit det_SVDPACK_the dobj_using_SVDPACK vmod_afterSVDonthesparsematrix_using num_describedinSection2_.2.1 prep_as_LSA_describedinSection2 amod_LSA_bilingual amod_LSA_bag-of-word amod_LSA_simple det_LSA_the rcmod_``_projected
P07-1001	J93-2003	o	It can be applied to complicated models such IBM Model-4 -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_Model-4_al. nn_Model-4_IBM amod_Model-4_such dep_models_Model-4 amod_models_complicated prep_to_applied_models auxpass_applied_be aux_applied_can nsubjpass_applied_It
P07-1001	J93-2003	o	We shall take HMM-based word alignment model -LRB- Vogel et al. 1996 -RRB- as an example and follow the notation of -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_of_al. amod_notation_of det_notation_the dobj_follow_notation nsubj_follow_We det_example_an amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et nn_model_alignment nn_model_word amod_model_HMM-based conj_and_take_follow prep_as_take_example dep_take_Vogel dobj_take_model aux_take_shall nsubj_take_We
P07-1001	J93-2003	o	Berry et al -LRB- 1993 -RRB- -RRB- to yield W W = U S V T as Figure 3 shows where for some order R lessmuch min -LRB- M N -RRB- of the decomposition U is a MR left singular matrix with rows ui i = 1 M S is a RR diagonal matrix of singular values s1 s2 sR greatermuch 0 and V is NR a right singular matrix with rows vj j = 1 N. For each i the scaled R-vector uiS may be viewed as representing wi thei-th word in the vocabulary and similarly the scaled R-vector vjS as representing dj j-th document in the corpus	det_corpus_the prep_in_document_corpus nn_document_j-th appos_dj_document dobj_representing_dj prepc_as_vjS_representing nn_vjS_R-vector amod_vjS_scaled det_vjS_the advmod_vjS_similarly det_vocabulary_the prep_in_word_vocabulary amod_word_thei-th conj_and_wi_vjS conj_and_wi_word dobj_representing_vjS dobj_representing_word dobj_representing_wi prepc_as_viewed_representing auxpass_viewed_be aux_viewed_may nsubjpass_viewed_matrix nn_uiS_R-vector amod_uiS_scaled det_uiS_the det_i_each prep_for_N._i dep_=_1 amod_j_= nn_vj_rows appos_matrix_uiS appos_matrix_N. appos_matrix_j prep_with_matrix_vj amod_matrix_singular amod_matrix_right det_matrix_a rcmod_NR_viewed cop_NR_is nsubj_NR_V num_greatermuch_0 nn_greatermuch_sR nn_greatermuch_s2 dobj_s1_greatermuch nsubj_s1_matrix amod_values_singular prep_of_matrix_values amod_matrix_diagonal nn_matrix_RR det_matrix_a cop_matrix_is nsubj_matrix_S nn_matrix_M conj_and_=_NR conj_and_=_s1 dobj_=_1 advmod_=_i advcl_=_left nn_ui_rows amod_matrix_singular prep_with_left_ui dobj_left_matrix nsubj_left_MR advmod_left_where det_MR_a cop_MR_is nsubj_MR_U prep_for_MR_min det_decomposition_the appos_M_N prep_of_min_decomposition dep_min_M nn_min_lessmuch nn_min_R nn_min_order det_min_some rcmod_shows_NR rcmod_shows_s1 rcmod_shows_= num_shows_3 nn_shows_Figure nn_T_V nn_T_S nn_T_U amod_T_= dep_W_T nn_W_W prep_as_yield_shows dobj_yield_W aux_yield_to nsubj_yield_al appos_al_1993 nn_al_et nn_al_Berry
P07-1004	J93-2003	o	These lists are rescored with the following models -LRB- a -RRB- the different models used in the decoder which are described above -LRB- b -RRB- two different features based on IBM Model 1 -LRB- Brown et al. 1993 -RRB- -LRB- c -RRB- posterior probabilities for words phrases n-grams and sentence length -LRB- Zens and Ney 2006 Ueffing and Ney 2007 -RRB- all calculated over the Nbest list and using the sentence probabilities which the baseline system assigns to the translation hypotheses	nn_hypotheses_translation det_hypotheses_the prep_to_assigns_hypotheses nsubj_assigns_system dobj_assigns_which nn_system_baseline det_system_the rcmod_probabilities_assigns nn_probabilities_sentence det_probabilities_the dobj_using_probabilities nn_list_Nbest det_list_the prep_over_calculated_list conj_and_all_using vmod_all_calculated dep_Ueffing_2007 conj_and_Ueffing_Ney dep_Zens_Ney dep_Zens_Ueffing conj_and_Zens_2006 conj_and_Zens_Ney appos_length_2006 appos_length_Ney appos_length_Zens nn_length_sentence conj_and_words_length conj_and_words_n-grams conj_and_words_phrases prep_for_probabilities_length prep_for_probabilities_n-grams prep_for_probabilities_phrases prep_for_probabilities_words amod_probabilities_posterior nn_probabilities_Model amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_Model_c appos_Model_Brown num_Model_1 nn_Model_IBM dep_features_using dep_features_all pobj_features_probabilities prepc_based_on_features_on amod_features_different num_features_two dep_b_features advmod_described_above auxpass_described_are nsubjpass_described_which rcmod_decoder_described det_decoder_the prep_in_used_decoder appos_models_b vmod_models_used amod_models_different det_models_the dep_a_models amod_models_following det_models_the dep_rescored_a prep_with_rescored_models auxpass_rescored_are nsubjpass_rescored_lists det_lists_These
P07-1011	J93-2003	o	Second it can be applied to control the quality of parallel bilingual sentences mined from the Web which are critical sources for a wide range of applications such as statistical machine translation -LRB- Brown et al. 1993 -RRB- and cross-lingual information retrieval -LRB- Nie et al. 1999 -RRB-	amod_Nie_1999 dep_Nie_al. nn_Nie_et dep_retrieval_Nie nn_retrieval_information amod_retrieval_cross-lingual num_al._1993 nn_al._et amod_al._Brown conj_and_translation_retrieval dep_translation_al. nn_translation_machine amod_translation_statistical prep_such_as_applications_retrieval prep_such_as_applications_translation prep_of_range_applications amod_range_wide det_range_a prep_for_sources_range amod_sources_critical cop_sources_are nsubj_sources_which rcmod_Web_sources det_Web_the prep_from_mined_Web vmod_sentences_mined amod_sentences_bilingual amod_sentences_parallel prep_of_quality_sentences det_quality_the dobj_control_quality aux_control_to xcomp_applied_control auxpass_applied_be aux_applied_can nsubjpass_applied_it advmod_applied_Second ccomp_``_applied
P07-1016	J93-2003	o	By treating a letter/character as a word and a group of letters/characters as a phrase or token unit in SMT one can easily apply the traditional SMT models such as the IBM generative model -LRB- Brown et al. 1993 -RRB- or the phrase-based translation model -LRB- Crego et al. 2005 -RRB- to transliteration	amod_Crego_2005 dep_Crego_al. nn_Crego_et prep_to_model_transliteration dep_model_Crego nn_model_translation amod_model_phrase-based det_model_the num_al._1993 nn_al._et amod_al._Brown conj_or_model_model dep_model_al. amod_model_generative nn_model_IBM det_model_the prep_such_as_models_model prep_such_as_models_model nn_models_SMT amod_models_traditional det_models_the dobj_apply_models advmod_apply_easily aux_apply_can nsubj_apply_one prepc_by_apply_treating amod_unit_token prep_in_phrase_SMT conj_or_phrase_unit det_phrase_a prep_of_group_letters/characters det_group_a prep_as_word_unit prep_as_word_phrase conj_and_word_group det_word_a det_letter/character_a prep_as_treating_group prep_as_treating_word dobj_treating_letter/character
P07-1020	J93-2003	o	Most of the previous work on statistical machine translation as exemplified in -LRB- Brown et al. 1993 -RRB- employs word-alignment algorithm -LRB- such as GIZA + + -LRB- Och and Ney 2003 -RRB- -RRB- that provides local associations between source and target words	nn_words_target conj_and_source_words amod_associations_local prep_between_provides_words prep_between_provides_source dobj_provides_associations nsubj_provides_that num_Och_2003 conj_and_Och_Ney dep_+_Ney dep_+_Och conj_+_GIZA_+ prepc_such_as_algorithm_provides dep_algorithm_+ dep_algorithm_GIZA amod_algorithm_word-alignment dobj_employs_algorithm dep_employs_exemplified nsubj_employs_Most num_al._1993 nn_al._et amod_al._Brown dep_in_al. advmod_exemplified_in mark_exemplified_as nn_translation_machine amod_translation_statistical amod_work_previous det_work_the prep_on_Most_translation prep_of_Most_work
P07-1039	J93-2003	o	4.3 Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline GIZA + + implementation of IBM word alignment model 4 -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- ,8 the refinement and phrase-extraction heuristics described in -LRB- Koehn et al. 2003 -RRB- minimum-error-rate training 7More specifically we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs	nn_pairs_sentence amod_pairs_new dobj_construct_pairs aux_construct_to amod_sentence_Chinese det_sentence_the vmod_references_construct conj_and_references_sentence num_references_7 det_references_the prep_from_reference_sentence prep_from_reference_references nn_reference_English amod_reference_first det_reference_the dobj_choose_reference nsubj_choose_we advmod_7More_specifically nn_7More_training amod_7More_minimum-error-rate rcmod_Koehn_choose appos_Koehn_7More amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn amod_heuristics_phrase-extraction vmod_refinement_described conj_and_refinement_heuristics det_refinement_the dep_,8_heuristics dep_,8_refinement dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_model_,8 dep_model_Brown num_model_4 nn_model_alignment nn_model_word dep_GIZA_model prep_of_GIZA_IBM conj_+_GIZA_implementation det_baseline_a nn_system_translation nn_system_machine amod_system_statistical amod_system_phrase-based amod_system_log-linear amod_system_standard det_system_a prep_as_use_baseline dobj_use_system nsubj_use_We dep_Baseline_implementation dep_Baseline_GIZA rcmod_Baseline_use num_Baseline_4.3 dep_``_Baseline
P07-1039	J93-2003	o	To quickly -LRB- and approximately -RRB- evaluate this phenomenon we trained the statistical IBM wordalignment model 4 -LRB- Brown et al. 1993 -RRB- ,1 using the GIZA + + software -LRB- Och and Ney 2003 -RRB- for the following language pairs ChineseEnglish Italian English and DutchEnglish using the IWSLT-2006 corpus -LRB- Takezawa et al. 2002 Paul 2006 -RRB- for the first two language pairs and the Europarl corpus -LRB- Koehn 2005 -RRB- for the last one	amod_one_last det_one_the dep_Koehn_2005 prep_for_corpus_one appos_corpus_Koehn nn_corpus_Europarl det_corpus_the nn_pairs_language num_pairs_two amod_pairs_first det_pairs_the dep_Paul_2006 conj_and_Takezawa_corpus prep_for_Takezawa_pairs dep_Takezawa_Paul appos_Takezawa_2002 dep_Takezawa_al. nn_Takezawa_et nn_corpus_IWSLT-2006 det_corpus_the dobj_using_corpus amod_English_Italian dep_ChineseEnglish_corpus dep_ChineseEnglish_Takezawa vmod_ChineseEnglish_using conj_and_ChineseEnglish_DutchEnglish appos_ChineseEnglish_English nn_pairs_language amod_pairs_following det_pairs_the dep_Och_2003 conj_and_Och_Ney prep_for_software_pairs appos_software_Ney appos_software_Och pobj_+_software dep_GIZA_DutchEnglish dep_GIZA_ChineseEnglish conj_+_GIZA_+ det_GIZA_the dobj_using_+ dobj_using_GIZA advmod_,1_quickly num_Brown_1993 nn_Brown_al. nn_Brown_et num_model_4 nn_model_wordalignment nn_model_IBM amod_model_statistical det_model_the dobj_trained_model nsubj_trained_we det_phenomenon_this dobj_evaluate_Brown parataxis_evaluate_trained dobj_evaluate_phenomenon advmod_evaluate_approximately cc_evaluate_and dep_quickly_evaluate dep_To_using pobj_To_,1 dep_``_To
P07-1039	J93-2003	o	They can be seen as extensions of the simpler IBM models 1 and 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_1_2 dep_models_2 dep_models_1 nn_models_IBM amod_models_simpler det_models_the prep_of_extensions_models dep_seen_Brown prep_as_seen_extensions auxpass_seen_be aux_seen_can nsubjpass_seen_They
P07-1039	J93-2003	o	Most current statistical models -LRB- Brown et al. 1993 Vogel et al. 1996 Deng and Byrne 2005 -RRB- treat the aligned sentences in the corpus as sequences of tokens that are meant to be words the goal of the alignment process is to find links between source and target words	nn_words_target conj_and_source_words prep_between_links_words prep_between_links_source dobj_find_links aux_find_to xcomp_is_find nsubj_is_goal nn_process_alignment det_process_the prep_of_goal_process det_goal_the cop_words_be aux_words_to xcomp_meant_words auxpass_meant_are nsubjpass_meant_that rcmod_tokens_meant prep_of_sequences_tokens det_corpus_the amod_sentences_aligned det_sentences_the prep_as_treat_sequences prep_in_treat_corpus dobj_treat_sentences dep_Deng_2005 conj_and_Deng_Byrne num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_Brown_Byrne dep_Brown_Deng dep_Brown_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et rcmod_models_is vmod_models_treat appos_models_Brown amod_models_statistical amod_models_current amod_models_Most
P07-1047	J93-2003	o	This situation is very similar to the training process of translation models in statistical machine translation -LRB- Brown et al. 1993 -RRB- where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns	nn_patterns_co-occurrence poss_patterns_their dobj_exploiting_patterns amod_languages_different prep_from_words_languages prep_between_mappings_words det_mappings_the prepc_by_find_exploiting dobj_find_mappings aux_find_to xcomp_used_find auxpass_used_is nsubjpass_used_corpus advmod_used_where amod_corpus_parallel dep_al._1993 nn_al._et advmod_Brown_al. nn_translation_machine amod_translation_statistical nn_models_translation prep_in_process_translation prep_of_process_models nn_process_training det_process_the advcl_similar_used dep_similar_Brown prep_to_similar_process advmod_similar_very cop_similar_is nsubj_similar_situation det_situation_This
P07-1047	J93-2003	p	Finally the translation model can be formalized as the following optimization problem argmax logPr -LRB- D ;-RRB- s.t. mwsummationdisplay j = 1 Pr -LRB- wj | ok -RRB- = 1 k This optimization problem can be solved by the EM algorithm -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_algorithm_EM det_algorithm_the agent_solved_algorithm auxpass_solved_be aux_solved_can nsubjpass_solved_problem nn_problem_optimization det_problem_This nn_problem_k dep_=_solved dobj_=_1 nsubj_=_Pr num_ok_| nn_ok_wj appos_Pr_ok num_Pr_1 dep_=_= dep_j_al. amod_j_= nn_j_mwsummationdisplay nn_j_s.t. nn_j_;-RRB- nn_j_D dep_logPr_j nn_logPr_argmax nn_logPr_problem nn_logPr_optimization prep_following_the_logPr prep_as_formalized_the auxpass_formalized_be aux_formalized_can nsubjpass_formalized_model advmod_formalized_Finally nn_model_translation det_model_the
P07-1082	J93-2003	o	-LRB- 2004 -RRB- argue that precise alignment can improve transliteration effectiveness experimenting on English-Chinese data and comparing IBM models -LRB- Brown et al. 1993 -RRB- with phonemebased alignments using direct probabilities	amod_probabilities_direct dobj_using_probabilities amod_alignments_phonemebased vmod_Brown_using prep_with_Brown_alignments amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM dobj_comparing_models nsubj_comparing_alignment amod_data_English-Chinese prep_on_experimenting_data nsubj_experimenting_alignment nn_effectiveness_transliteration conj_and_improve_comparing conj_and_improve_experimenting dobj_improve_effectiveness aux_improve_can nsubj_improve_alignment mark_improve_that amod_alignment_precise dobj_argue_Brown ccomp_argue_comparing ccomp_argue_experimenting ccomp_argue_improve dep_argue_2004
P07-1090	J93-2003	n	In pursuit of better translation phrase-based models -LRB- OchandNey ,2004 -RRB- havesignificantlyimprovedthe quality over classical word-based models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_models_word-based amod_models_classical appos_quality_Brown prep_over_quality_models amod_quality_havesignificantlyimprovedthe num_OchandNey_,2004 dep_models_quality appos_models_OchandNey amod_models_phrase-based amod_translation_better prep_of_pursuit_translation dep_``_models prep_in_``_pursuit
P07-1092	J93-2003	o	1 Introduction Statistical machine translation -LRB- Brown et al. 1993 -RRB- has seen many improvements in recent years most notably the transition from wordto phrase-based models -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_models_phrase-based amod_models_wordto dep_transition_Koehn prep_from_transition_models det_transition_the advmod_transition_notably advmod_notably_most amod_years_recent prep_in_improvements_years amod_improvements_many dep_seen_transition dobj_seen_improvements aux_seen_has nsubj_seen_translation num_al._1993 nn_al._et amod_al._Brown appos_translation_al. nn_translation_machine amod_translation_Statistical nn_translation_Introduction num_translation_1 ccomp_``_seen
P07-1108	J93-2003	n	1 Introduction For statistical machine translation -LRB- SMT -RRB- phrasebased methods -LRB- Koehn et al. 2003 Och and Ney 2004 -RRB- and syntax-based methods -LRB- Wu 1997 Alshawi et al. 2000 Yamada and Knignt 2001 Melamed 2004 Chiang 2005 Quick et al. 2005 Mellebeek et al. 2006 -RRB- outperform word-based methods -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_methods_Brown amod_methods_word-based dobj_outperform_methods nsubj_outperform_methods nsubj_outperform_methods ccomp_outperform_Introduction nn_al._et nn_al._Mellebeek dep_al._2005 nn_al._et nn_al._Quick num_Chiang_2005 num_Melamed_2004 conj_and_Yamada_al. conj_and_Yamada_Chiang conj_and_Yamada_Melamed conj_and_Yamada_2001 conj_and_Yamada_Knignt num_al._2000 nn_al._et nn_al._Alshawi amod_Wu_2006 dep_Wu_al. dep_Wu_al. dep_Wu_Chiang dep_Wu_Melamed dep_Wu_2001 dep_Wu_Knignt dep_Wu_Yamada dep_Wu_al. amod_Wu_1997 dep_methods_Wu amod_methods_syntax-based dep_Och_2004 conj_and_Och_Ney dep_Koehn_Ney dep_Koehn_Och appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et conj_and_methods_methods dep_methods_Koehn amod_methods_phrasebased appos_translation_SMT nn_translation_machine amod_translation_statistical prep_for_Introduction_translation num_Introduction_1
P07-3010	J93-2003	o	Similarity measures can be based on any level of linguistic analysis semantic similarity relies on context vectors -LRB- Rapp 1999 -RRB- whilesyntacticsimilarityisbased on the alignment of parallel corpora -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_corpora_al. amod_corpora_parallel prep_of_alignment_corpora det_alignment_the prep_on_whilesyntacticsimilarityisbased_alignment amod_Rapp_1999 dep_vectors_Rapp nn_vectors_context dep_relies_whilesyntacticsimilarityisbased prep_on_relies_vectors nsubj_relies_similarity amod_similarity_semantic amod_analysis_linguistic prep_of_level_analysis det_level_any parataxis_based_relies prep_on_based_level auxpass_based_be aux_based_can nsubjpass_based_measures nn_measures_Similarity
P08-1010	J93-2003	o	3.1 Model-based Phrase Pair Posterior In a statistical generative word alignment model -LRB- Brown et al. 1993 -RRB- it is assumed that -LRB- i -RRB- a random variable a specifies how each target word fj is generated by -LRB- therefore aligned to -RRB- a source 1 word eaj and -LRB- ii -RRB- the likelihood function f -LRB- f a | e -RRB- specifies a generativeprocedurefromthesourcesentencetothe target sentence	nn_sentence_target nn_sentence_generativeprocedurefromthesourcesentencetothe det_sentence_a dobj_specifies_sentence nsubj_specifies_| dep_|_e det_|_a rcmod_f_specifies nn_f_f advmod_function_f nn_function_likelihood det_function_the dep_ii_function nn_eaj_word num_eaj_1 nn_eaj_source det_eaj_a conj_and_aligned_ii dobj_aligned_eaj prep_aligned_to advmod_aligned_therefore agent_generated_ii agent_generated_aligned auxpass_generated_is nsubjpass_generated_fj advmod_generated_how nn_fj_word nn_fj_target det_fj_each ccomp_specifies_generated nsubj_specifies_a rcmod_variable_specifies amod_variable_random det_variable_a dep_i_variable dep_that_i prep_assumed_that auxpass_assumed_is nsubjpass_assumed_it ccomp_assumed_Posterior num_al._1993 nn_al._et amod_al._Brown nn_model_alignment nn_model_word amod_model_generative amod_model_statistical det_model_a appos_Posterior_al. prep_in_Posterior_model nn_Posterior_Pair nn_Posterior_Phrase amod_Posterior_Model-based num_Posterior_3.1
P08-1012	J93-2003	o	The traditional estimation method for word 98 alignment models is the EM algorithm -LRB- Brown et al. 1993 -RRB- which iteratively updates parameters to maximize the likelihood of the data	det_data_the prep_of_likelihood_data det_likelihood_the dobj_maximize_likelihood aux_maximize_to vmod_parameters_maximize nn_parameters_updates advmod_parameters_iteratively det_parameters_which num_al._1993 nn_al._et amod_al._Brown dep_algorithm_parameters dep_algorithm_al. nn_algorithm_EM det_algorithm_the cop_algorithm_is nsubj_algorithm_method nn_models_alignment num_models_98 nn_models_word prep_for_method_models nn_method_estimation amod_method_traditional det_method_The
P08-1019	J93-2003	o	More specifically by using translation probabilities we can rewrite equation -LRB- 11 -RRB- and -LRB- 12 -RRB- as follow nullnullnullnullnull null nullnull null nullnullnull null null nullnullnullnull null nullnull null null nullnull null nullnull null null | null null null null nullnull null nullnull null nullnull null null null null nullnull null nullnull null null null 1nullnull null nullnull null null null nullnull | nullnull -LRB- 13 -RRB- nullnullnullnullnull null nullnull null nullnullnull null null nullnullnullnull null nullnull null null nullnull null nullnull null null | null null null null nullnull null nullnull null nullnull null null null null nullnull null nullnull null null null 1nullnull null nullnull null null null nullnull | nullnull -LRB- 14 -RRB- where nullnullnullnull | null null null denotes the probability that topic term null is the translation of null null In our experiments to estimate the probability nullnullnullnull | null null null we used the collections of question titles and question descriptions as the parallel corpus and the IBM model 1 -LRB- Brown et al. 1993 -RRB- as the alignment model	nn_model_alignment det_model_the amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_as_model_model dep_model_Brown num_model_1 nn_model_IBM det_model_the amod_corpus_parallel det_corpus_the nn_descriptions_question nn_titles_question conj_and_collections_descriptions prep_of_collections_titles det_collections_the conj_and_used_model prep_as_used_corpus dobj_used_descriptions dobj_used_collections nsubj_used_we amod_null_null amod_null_null num_null_| nn_null_nullnullnullnull nn_null_probability det_null_the dobj_estimate_null aux_estimate_to prep_in_estimate_experiments nsubj_estimate_nullnull poss_experiments_our amod_null_null prep_of_translation_null det_translation_the cop_translation_is nsubj_translation_null nn_null_term nn_null_topic det_null_that rcmod_probability_translation det_probability_the dobj_denotes_probability nsubj_denotes_nullnullnullnull advmod_denotes_where amod_null_null amod_null_null amod_|_null dep_nullnullnullnull_| rcmod_nullnull_denotes appos_nullnull_14 nn_nullnull_| nn_nullnull_nullnull amod_nullnull_null amod_nullnull_null nn_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_1nullnull amod_nullnull_null amod_nullnull_null nn_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null amod_nullnull_null amod_nullnull_null nn_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null amod_nullnull_null amod_nullnull_null amod_nullnull_null nn_nullnull_| amod_nullnull_null amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnullnullnull amod_nullnull_null amod_nullnull_null nn_nullnull_nullnullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnullnullnullnull num_nullnull_13 rcmod_nullnull_model rcmod_nullnull_used rcmod_nullnull_estimate nn_nullnull_| nn_nullnull_nullnull amod_nullnull_null amod_nullnull_null nn_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_1nullnull amod_nullnull_null amod_nullnull_null nn_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null amod_nullnull_null amod_nullnull_null nn_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null amod_nullnull_null amod_nullnull_null amod_nullnull_null nn_nullnull_| amod_nullnull_null amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnullnullnull amod_nullnull_null amod_nullnull_null nn_nullnull_nullnullnull amod_nullnull_null nn_nullnull_nullnull amod_nullnull_null nn_nullnull_nullnullnullnullnull dobj_follow_nullnull mark_follow_as dep_follow_12 nsubj_follow_we appos_equation_11 conj_and_rewrite_follow dobj_rewrite_equation aux_rewrite_can nsubj_rewrite_we prepc_by_rewrite_using advmod_rewrite_specifically nn_probabilities_translation dobj_using_probabilities dep_specifically_More ccomp_``_follow ccomp_``_rewrite
P08-1082	J93-2003	o	The text was split at the sentence level tokenized and PoS tagged in the style of the Wall Street Journal Penn TreeBank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn nn_TreeBank_Journal nn_TreeBank_Street nn_TreeBank_Wall det_TreeBank_the prep_of_style_TreeBank det_style_the nsubj_tagged_PoS nsubjpass_tokenized_text nn_level_sentence det_level_the dep_split_Marcus prep_in_split_style conj_and_split_tagged conj_and_split_tokenized prep_at_split_level auxpass_split_was nsubjpass_split_text det_text_The
P08-1082	J93-2003	o	This probability is computed using IBMs Model 1 -LRB- Brown et al. 1993 -RRB- P -LRB- Q | A -RRB- = productdisplay qQ P -LRB- q | A -RRB- -LRB- 3 -RRB- P -LRB- q | A -RRB- = -LRB- 1 -RRB- Pml -LRB- q | A -RRB- + Pml -LRB- q | C -RRB- -LRB- 4 -RRB- Pml -LRB- q | A -RRB- = summationdisplay aA -LRB- T -LRB- q | a -RRB- Pml -LRB- a | A -RRB- -RRB- -LRB- 5 -RRB- where the probability that the question term q is generated from answer A P -LRB- q | A -RRB- is smoothed using the prior probability that the term q is generated from the entire collection of answers C Pml -LRB- q | C -RRB-	nn_C_| amod_C_q appos_Pml_C appos_C_Pml dep_answers_C prep_of_collection_answers amod_collection_entire det_collection_the prep_from_generated_collection auxpass_generated_is nsubjpass_generated_q mark_generated_that nn_q_term det_q_the ccomp_probability_generated amod_probability_prior det_probability_the dobj_using_probability xcomp_smoothed_using auxpass_smoothed_is nsubjpass_smoothed_question mark_smoothed_that nn_A_| amod_A_q appos_P_A appos_A_P nn_A_answer prep_from_generated_A auxpass_generated_is nsubjpass_generated_q nn_q_term rcmod_question_generated det_question_the ccomp_probability_smoothed det_probability_the dep_where_probability nn_A_| det_A_a appos_Pml_A det_Pml_a dep_|_Pml amod_|_q nn_|_T dep_aA_where appos_aA_5 appos_aA_| nn_aA_summationdisplay dobj_=_aA nsubj_=_Pml dep_=_4 nn_A_| amod_A_q appos_Pml_A nn_C_| amod_C_q rcmod_Pml_= appos_Pml_C nn_A_| amod_A_q conj_+_Pml_Pml appos_Pml_A dobj_1_Pml dobj_1_Pml dep_=_1 nsubj_=_P dep_=_3 dep_=_P dep_=_= dep_=_P dep_=_Model dep_=_IBMs dep_=_using nn_A_| amod_A_q appos_P_A nn_A_| amod_A_q appos_P_A nn_P_qQ nn_P_productdisplay num_A_| nn_A_Q appos_P_A amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_1 ccomp_computed_= auxpass_computed_is nsubjpass_computed_probability det_probability_This ccomp_``_computed
P09-1011	J93-2003	o	This model is similar in spirit to IBM model 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_IBM dep_similar_Brown prep_to_similar_model prep_in_similar_spirit cop_similar_is nsubj_similar_model det_model_This
P09-1088	J93-2003	o	We use the GIZA + + implementation of IBM Model 4 -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- coupled with the phrase extraction heuristics of Koehn et al.	nn_al._et nn_al._Koehn prep_of_heuristics_al. nn_heuristics_extraction nn_heuristics_phrase det_heuristics_the prep_with_coupled_heuristics dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM prep_of_implementation_Model pobj_+_implementation dep_GIZA_Brown conj_+_GIZA_+ det_GIZA_the dep_use_coupled dobj_use_+ dobj_use_GIZA nsubj_use_We
P09-1088	J93-2003	n	1 Introduction The field of machine translation has seen many advances in recent years most notably the shift from word-based -LRB- Brown et al. 1993 -RRB- to phrasebased models which use token n-grams as translation units -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_units_Koehn nn_units_translation amod_n-grams_token prep_as_use_units dobj_use_n-grams nsubj_use_which rcmod_models_use amod_models_phrasebased dep_al._1993 nn_al._et advmod_Brown_al. dep_word-based_Brown prep_to_shift_models prep_from_shift_word-based det_shift_the advmod_shift_notably advmod_notably_most amod_years_recent amod_advances_many prep_in_seen_years dobj_seen_advances aux_seen_has nsubj_seen_field nn_translation_machine prep_of_field_translation det_field_The dep_Introduction_shift rcmod_Introduction_seen num_Introduction_1
P09-1098	J93-2003	o	1 Introduction Bilingual data -LRB- including bilingual sentences and bilingual terms -RRB- are critical resources for building many applications such as machine translation -LRB- Brown 1993 -RRB- and cross language information retrieval -LRB- Nie et al. 1999 -RRB-	amod_Nie_1999 dep_Nie_al. nn_Nie_et dep_retrieval_Nie nn_retrieval_information nn_retrieval_language dobj_cross_retrieval dep_Brown_1993 conj_and_translation_cross dep_translation_Brown nn_translation_machine prep_such_as_applications_cross prep_such_as_applications_translation amod_applications_many dobj_building_applications prepc_for_resources_building amod_resources_critical cop_resources_are nsubj_resources_data amod_terms_bilingual conj_and_sentences_terms amod_sentences_bilingual prep_including_data_terms prep_including_data_sentences amod_data_Bilingual nn_data_Introduction num_data_1 ccomp_``_resources
P09-2037	J93-2003	p	This is an important feature from the MT viewpoint since the decomposition into translation model and language model proved to be extremely useful in statistical MT since -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_since_al. amod_MT_statistical prep_in_useful_MT advmod_useful_extremely cop_useful_be aux_useful_to advmod_proved_since xcomp_proved_useful nsubj_proved_decomposition mark_proved_since nn_model_language conj_and_model_model nn_model_translation prep_into_decomposition_model prep_into_decomposition_model det_decomposition_the nn_viewpoint_MT det_viewpoint_the advcl_feature_proved prep_from_feature_viewpoint amod_feature_important det_feature_an cop_feature_is nsubj_feature_This
P09-2057	J93-2003	o	1 is based on several realvalued feature functions fi Their computation is based on the so-called IBM Model-1 -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_Model-1_al. nn_Model-1_IBM amod_Model-1_so-called det_Model-1_the prep_on_based_Model-1 auxpass_based_is nsubjpass_based_computation poss_computation_Their parataxis_fi_based nsubj_fi_functions mark_fi_on nn_functions_feature amod_functions_realvalued amod_functions_several advcl_based_fi auxpass_based_is nsubjpass_based_1 ccomp_``_based
P09-2058	J93-2003	p	Widely used alignment models such as IBM Model serial -LRB- Brown et al. 1993 -RRB- and HMM all assume one-to-many alignments	amod_alignments_one-to-many dobj_assume_alignments dep_assume_all dep_HMM_assume num_al._1993 nn_al._et amod_al._Brown conj_and_serial_HMM dep_serial_al. nn_serial_Model nn_serial_IBM prep_such_as_models_HMM prep_such_as_models_serial nn_models_alignment amod_models_used advmod_used_Widely
P93-1002	J93-2003	o	11 However modeling word order under translation is notoriously difficult -LRB- Brown et al. 1993 -RRB- and it is unclear how much improvement in accuracy a good model of word order would provide	aux_provide_would nsubj_provide_model dobj_provide_improvement nn_order_word prep_of_model_order amod_model_good det_model_a prep_in_improvement_accuracy amod_improvement_much advmod_much_how ccomp_unclear_provide cop_unclear_is nsubj_unclear_it dep_al._1993 nn_al._et amod_al._Brown conj_and_difficult_unclear dep_difficult_al. advmod_difficult_notoriously cop_difficult_is nsubj_difficult_order prep_under_order_translation nn_order_word nn_order_modeling num_order_11 advmod_11_However ccomp_``_unclear ccomp_``_difficult
P93-1002	J93-2003	o	The natural next step in sentence alignment is to account for word ordering in the translation model e.g. the models described in -LRB- Brown et al. 1993 -RRB- could be used	auxpass_used_be aux_used_could nsubjpass_used_al. mark_used_in num_al._1993 nn_al._et amod_al._Brown advcl_described_used vmod_models_described det_models_the nn_model_translation det_model_the prep_in_ordering_model appos_word_models dep_word_e.g. vmod_word_ordering prep_for_account_word aux_account_to xcomp_is_account nsubj_is_step nn_alignment_sentence prep_in_step_alignment amod_step_next amod_step_natural det_step_The ccomp_``_is
P95-1033	J93-2003	o	Since Chinese text is not orthographically separated into words the standard methodology is to first preproce ~ input texts through a segmentation module -LRB- Chiang et al. 1992 Linet al. 1992 Chang & Chert 1993 Linet al. 1993 Wu & Tseng 1993 Sproat et al. 1994 -RRB-	nn_al._et nn_al._Sproat num_Wu_1993 conj_and_Wu_Tseng num_al._1993 nn_al._Linet num_Chang_1993 conj_and_Chang_Chert num_al._1992 nn_al._Linet dep_Chiang_1994 dep_Chiang_al. dep_Chiang_Tseng dep_Chiang_Wu dep_Chiang_al. dep_Chiang_Chert dep_Chiang_Chang dep_Chiang_al. dep_Chiang_1992 dep_Chiang_al. nn_Chiang_et nn_module_segmentation det_module_a nn_texts_input nn_texts_~ nn_texts_preproce amod_texts_first dep_is_Chiang prep_through_is_module prep_to_is_texts nsubj_is_methodology advcl_is_separated amod_methodology_standard det_methodology_the prep_into_separated_words preconj_separated_orthographically aux_separated_is nsubj_separated_text mark_separated_Since neg_orthographically_not amod_text_Chinese
P95-1033	J93-2003	o	1 Introduction Parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis -LRB- e.g. Brown et al. 1990 Gale & Church 1991 Gale et al. 1992 Church 1993 Brown et al. 1993 Dagan et al. 1993 Dagan & Church 1994 Fung & Church 1994 Wu & Xia 1994 Fung & McKeown 1994 -RRB-	num_Fung_1994 conj_and_Fung_McKeown num_Wu_1994 conj_and_Wu_Xia num_Fung_1994 conj_and_Fung_Church dep_Dagan_McKeown dep_Dagan_Fung conj_and_Dagan_Xia conj_and_Dagan_Wu conj_and_Dagan_Church conj_and_Dagan_Fung num_Dagan_1994 conj_and_Dagan_Church num_al._1993 nn_al._et nn_al._Dagan num_al._1993 nn_al._et nn_al._Brown num_Church_1993 dep_al._1992 nn_al._et advmod_Gale_al. num_Gale_1991 conj_and_Gale_Church dep_al._Wu dep_al._Fung dep_al._Church dep_al._Dagan dep_al._al. dep_al._al. dep_al._Church dep_al._Gale dep_al._Church dep_al._Gale num_al._1990 nn_al._et nn_al._Brown dep_al._e.g. dep_analysis_al. amod_analysis_statistical prep_for_constraints_analysis prep_of_source_constraints amod_source_rich det_source_an advmod_rich_extremely dobj_provide_source aux_provide_to xcomp_shown_provide auxpass_shown_been aux_shown_have nsubjpass_shown_corpora amod_corpora_Parallel nn_corpora_Introduction num_corpora_1
P95-1033	J93-2003	o	Aside from purely linguistic interest bracket structure has been empirically shown to be highly effective at constraining subsequent training of for example stochastic context-free grammars -LRB- Pereira & ~ 1992 Black et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Black dep_Pereira_al. num_Pereira_1992 conj_and_Pereira_~ dep_grammars_~ dep_grammars_Pereira amod_grammars_context-free amod_grammars_stochastic prep_training_of amod_training_subsequent dobj_constraining_training prepc_at_effective_constraining advmod_effective_highly cop_effective_be aux_effective_to parataxis_shown_grammars prep_for_shown_example xcomp_shown_effective advmod_shown_empirically auxpass_shown_been aux_shown_has nsubjpass_shown_structure prep_aside_from_shown_interest nn_structure_bracket amod_interest_linguistic advmod_linguistic_purely
P95-1033	J93-2003	o	A simpler related idea of penalizing distortion from some ideal matching pattern can be found in the statistical translation -LRB- Brown et al. 1990 Brown et al. 1993 -RRB- and word alignment -LRB- Dagan et al. 1993 Dagan & Church 1994 -RRB- models	dep_models_Dagan nn_models_alignment num_Dagan_1994 conj_and_Dagan_Church num_al._1993 nn_al._et dep_Dagan_Church dep_Dagan_Dagan advmod_Dagan_al. nn_alignment_word dep_al._1993 nn_al._et nn_al._Brown nn_al._et dep_Brown_al. dep_Brown_1990 dep_Brown_al. amod_translation_statistical det_translation_the conj_and_found_models dep_found_Brown prep_in_found_translation auxpass_found_be aux_found_can nsubjpass_found_idea nn_pattern_matching amod_pattern_ideal det_pattern_some prep_from_penalizing_pattern dobj_penalizing_distortion prepc_of_idea_penalizing amod_idea_related amod_idea_simpler det_idea_A
P95-1034	J93-2003	n	This approach addresses the problematic aspects of both pure knowledge-based generation -LRB- where incomplete knowledge is inevitable -RRB- and pure statistical bag generation -LRB- Brown et al. 1993 -RRB- -LRB- where the statistical system has no linguistic guidance -RRB-	amod_guidance_linguistic neg_guidance_no dobj_has_guidance nsubj_has_system advmod_has_where amod_system_statistical det_system_the num_al._1993 nn_al._et amod_al._Brown dep_generation_al. nn_generation_bag amod_generation_statistical amod_generation_pure cop_inevitable_is nsubj_inevitable_knowledge advmod_inevitable_where amod_knowledge_incomplete parataxis_generation_has conj_and_generation_generation dep_generation_inevitable amod_generation_knowledge-based amod_generation_pure det_generation_both prep_of_aspects_generation prep_of_aspects_generation amod_aspects_problematic det_aspects_the dobj_addresses_aspects nsubj_addresses_approach det_approach_This
P95-1034	J93-2003	o	However compositional approaches to lexical choice have been successful whenever detailed representations of lexical constraints can be collected and entered into the lexicon -LRB- e.g. -LRB- Elhadad 1993 Kukich et al. 1994 -RRB- -RRB-	num_Kukich_1994 nn_Kukich_al. nn_Kukich_et dep_Elhadad_Kukich dep_Elhadad_1993 dep_,_Elhadad dep_-LRB-_e.g. det_lexicon_the prep_into_entered_lexicon nsubjpass_entered_representations conj_and_collected_entered auxpass_collected_be aux_collected_can nsubjpass_collected_representations dep_collected_detailed amod_constraints_lexical prep_of_representations_constraints advmod_detailed_whenever dep_successful_entered dep_successful_collected cop_successful_been aux_successful_have nsubj_successful_approaches advmod_successful_However amod_choice_lexical prep_to_approaches_choice amod_approaches_compositional
P96-1020	J93-2003	p	Corpus-based or example-based MT -LRB- Sato and Nagao 1990 Sumita and Iida 1991 -RRB- and statistical MT -LRB- Brown et al. 1993 -RRB- systems provide the easiest customizability since users have only to supply a collection of source and target sentence pairs -LRB- a bilingual corpus -RRB-	amod_corpus_bilingual det_corpus_a nn_pairs_sentence nn_pairs_target conj_and_source_pairs appos_collection_corpus prep_of_collection_pairs prep_of_collection_source det_collection_a dobj_supply_collection aux_supply_to advmod_supply_only xcomp_have_supply nsubj_have_users mark_have_since amod_customizability_easiest det_customizability_the advcl_provide_have dobj_provide_customizability nsubj_provide_systems nn_systems_MT nn_systems_MT dep_al._1993 nn_al._et amod_al._Brown dep_MT_al. amod_MT_statistical dep_Sumita_1991 conj_and_Sumita_Iida dep_Sato_Iida dep_Sato_Sumita conj_and_Sato_1990 conj_and_Sato_Nagao conj_and_MT_MT appos_MT_1990 appos_MT_Nagao appos_MT_Sato amod_MT_example-based amod_MT_Corpus-based conj_or_Corpus-based_example-based
P96-1021	J93-2003	o	Estimation of the parameters has been described elsewhere -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_described_Brown advmod_described_elsewhere auxpass_described_been aux_described_has nsubjpass_described_Estimation det_parameters_the prep_of_Estimation_parameters
P96-1021	J93-2003	o	Such linguistic-preprocessing techniques could 1Various models have been constructed by the IBM team -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_team_al. nn_team_IBM det_team_the agent_constructed_team auxpass_constructed_been aux_constructed_have nsubjpass_constructed_models aux_constructed_could nn_models_1Various rcmod_techniques_constructed nn_techniques_linguistic-preprocessing amod_techniques_Such ccomp_``_techniques
P96-1023	J93-2003	o	The node mapping function f for the entire tree thus has a different role from the alignment function in the IBM statistical translation model -LRB- Brown et al. 1990 1993 -RRB- the role of the latter includes the linear ordering of words in the target string	nn_string_target det_string_the prep_in_words_string prep_of_ordering_words amod_ordering_linear det_ordering_the xcomp_includes_ordering nsubj_includes_role det_latter_the prep_of_role_latter det_role_the num_al._1993 num_al._1990 nn_al._et amod_al._Brown nn_model_translation amod_model_statistical nn_model_IBM det_model_the prep_in_function_model nn_function_alignment det_function_the prep_from_role_function amod_role_different det_role_a parataxis_has_includes dep_has_al. dobj_has_role advmod_has_thus nsubj_has_function amod_tree_entire det_tree_the prep_for_function_tree dep_function_f nn_function_mapping nn_function_node det_function_The
P97-1022	J93-2003	o	Model 1 is the word-pair translation model used in simple machine translation and understanding models -LRB- Brown et al. 1993 Epstein et al. 1996 -RRB-	num_Epstein_1996 nn_Epstein_al. nn_Epstein_et dep_Brown_Epstein amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_understanding conj_and_translation_models nn_translation_machine amod_translation_simple prep_in_used_models prep_in_used_translation dep_model_Brown vmod_model_used nn_model_translation amod_model_word-pair det_model_the cop_model_is nsubj_model_Model num_Model_1
P97-1022	J93-2003	o	In earlier IBM translation systems -LRB- Brown et al. 1993 -RRB- each English word would be generated by or aligned to exactly one formal language word	nn_word_language amod_word_formal num_word_one quantmod_one_exactly dep_aligned_word prep_aligned_to dep_by_aligned cc_by_or prep_generated_by auxpass_generated_be aux_generated_would nsubjpass_generated_word dep_generated_Brown prep_in_generated_systems nn_word_English det_word_each amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_systems_translation nn_systems_IBM amod_systems_earlier rcmod_``_generated
P97-1022	J93-2003	o	This paper extends the IBM Machine Translation Group 's concept of fertility -LRB- Brown et al. 1993 -RRB- to the generation of clumps for natural language understanding	nn_understanding_language amod_understanding_natural prep_for_generation_understanding prep_of_generation_clumps det_generation_the num_al._1993 nn_al._et amod_al._Brown dep_concept_al. prep_of_concept_fertility poss_concept_Group nn_Group_Translation nn_Group_Machine nn_Group_IBM det_Group_the prep_to_extends_generation dobj_extends_concept nsubj_extends_paper det_paper_This
P97-1037	J93-2003	o	Among all possible target strings we will choose the one with the highest probability which is given by Bayes ' decision rule -LRB- Brown et al 1993 -RRB- ~ = argmaxP ' -LRB- e \ -RSB- ~ lfg ~ -RRB- -RCB- = argmax -LCB- P ' -LRB- ef -RRB-	appos_P_ef nn_P_argmax dobj_=_P nsubj_=_one nn_~_lfg num_~_~ dep_\_e dep_=_argmaxP amod_~_= dep_al_1993 nn_al_et dep_Brown_al appos_rule_Brown nn_rule_decision poss_rule_Bayes agent_given_rule auxpass_given_is nsubjpass_given_which rcmod_probability_given amod_probability_highest det_probability_the dep_one_~ dep_one_\ appos_one_~ prep_with_one_probability det_one_the xcomp_choose_= aux_choose_will nsubj_choose_we prep_among_choose_strings nn_strings_target amod_strings_possible det_strings_all
P97-1037	J93-2003	o	Models describing these types of dependencies are referred to as alignrnen.t models -LRB- Brown et al. 1993 -RRB- -LRB- Dagan eta \ -RSB- 1993 -RRB-	num_\_1993 nn_\_eta nn_\_Dagan amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_alignrnen.t dep_referred_\ dep_referred_Brown prep_as_to_referred_models auxpass_referred_are nsubjpass_referred_Models prep_of_types_dependencies det_types_these dobj_describing_types vmod_Models_describing
P97-1037	J93-2003	o	The concept of these alignments is similar to the ones introduced by -LRB- Brown et al. 1993 -RRB- but we will use another type of dependence in the probability distributions	nn_distributions_probability det_distributions_the prep_of_type_dependence det_type_another prep_in_use_distributions dobj_use_type aux_use_will nsubj_use_we num_al._1993 nn_al._et amod_al._Brown dep_by_al. prep_introduced_by vmod_ones_introduced det_ones_the conj_but_similar_use prep_to_similar_ones cop_similar_is nsubj_similar_concept det_alignments_these prep_of_concept_alignments det_concept_The
P97-1037	J93-2003	o	Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position O j_l P -LRB- -LRB- / j \ -LSB- -LRB- / j-1 -RRB- A similar approach has been chosen by -LRB- Dagan et al. 1993 -RRB- and -LRB- Vogel et al 1996 -RRB-	nn_1996_al dep_Vogel_1996 nn_Vogel_et conj_and_Dagan_Vogel amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et agent_chosen_Vogel agent_chosen_Dagan auxpass_chosen_been aux_chosen_has nsubjpass_chosen_approach amod_approach_similar det_approach_A dep_approach_j-1 rcmod_\_chosen nn_\_j dep_P_\ nn_j_l_O nn_j_l_position nn_j_l_alignment amod_j_l_previous det_j_l_the prep_on_dependence_j_l det_dependence_a dep_have_P dobj_have_dependence aux_have_should nsubj_have_probability advmod_have_Therefore nn_j_position nn_aj_alignment prep_for_probability_j prep_of_probability_aj det_probability_the
P97-1037	J93-2003	o	The IBM model 1 -LRB- Brown et al. 1993 -RRB- is used to find an initial estimate of the translation probabilities	nn_probabilities_translation det_probabilities_the prep_of_estimate_probabilities amod_estimate_initial det_estimate_an dobj_find_estimate aux_find_to xcomp_used_find auxpass_used_is nsubjpass_used_model amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_1_Brown dep_model_1 nn_model_IBM det_model_The
P97-1039	J93-2003	o	correspondence points associated with frequent token types -LRB- Church 1993 -RRB- or by deleting frequent token types from the bitext altogether -LRB- Dagan et al. 1993 -RRB-	amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et advmod_bitext_altogether det_bitext_the amod_types_token amod_types_frequent prep_from_deleting_bitext dobj_deleting_types pcomp_by_deleting dep_Church_1993 dep_types_Church amod_types_token amod_types_frequent prep_with_associated_types dep_points_Dagan conj_or_points_by vmod_points_associated nn_points_correspondence
P97-1039	J93-2003	o	One important application of bitext maps is the construction of translation lexicons -LRB- Dagan et al. 1993 -RRB- and as discussed translation lexicons are an important information source for bitext mapping	nn_mapping_bitext prep_for_source_mapping nn_source_information amod_source_important det_source_an cop_source_are nsubj_source_lexicons advcl_source_discussed nn_lexicons_translation mark_discussed_as amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et nn_lexicons_translation conj_and_construction_source dep_construction_Dagan prep_of_construction_lexicons det_construction_the cop_construction_is nsubj_construction_application nn_maps_bitext prep_of_application_maps amod_application_important num_application_One
P97-1039	J93-2003	o	In addition to their use in machine translation -LRB- Sato & Nagao 1990 Brown et al. 1993 Melamed 1997 -RRB- translation models can be applied to machineassisted translation -LRB- Sato 1992 Foster et al. 1996 -RRB- cross-lingual information retrieval -LRB- SIGIR 1996 -RRB- and gisting of World Wide Web pages -LRB- Resnik 1997 -RRB-	amod_Resnik_1997 dep_pages_Resnik nn_pages_Web nn_pages_Wide nn_pages_World prep_of_gisting_pages appos_SIGIR_1996 conj_and_retrieval_gisting dep_retrieval_SIGIR nn_retrieval_information amod_retrieval_cross-lingual num_Foster_1996 nn_Foster_al. nn_Foster_et dep_Sato_Foster appos_Sato_1992 appos_translation_Sato amod_translation_machineassisted xcomp_applied_gisting xcomp_applied_retrieval prep_to_applied_translation auxpass_applied_be aux_applied_can nsubjpass_applied_models prep_in_addition_to_applied_use nn_models_translation dep_Melamed_1997 dep_Brown_Melamed num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Sato_Brown dep_Sato_1990 conj_and_Sato_Nagao appos_translation_Nagao appos_translation_Sato nn_translation_machine prep_in_use_translation poss_use_their ccomp_``_applied
P97-1039	J93-2003	o	Bitexts also play a role in less automated applications such as concordancing for bilingual lexicography -LRB- Catizone et al. 1993 Gale & Church 1991b -RRB- computer-assisted language learning and tools for translators -LRB- e.g.	dep_translators_e.g. prep_for_tools_translators nn_learning_language amod_learning_computer-assisted appos_Gale_1991b conj_and_Gale_Church dep_Catizone_Church dep_Catizone_Gale appos_Catizone_1993 dep_Catizone_al. nn_Catizone_et amod_lexicography_bilingual conj_and_concordancing_tools conj_and_concordancing_learning appos_concordancing_Catizone prep_for_concordancing_lexicography prep_such_as_applications_tools prep_such_as_applications_learning prep_such_as_applications_concordancing amod_applications_automated amod_applications_less det_role_a prep_in_play_applications dobj_play_role advmod_play_also nsubj_play_Bitexts
P97-1046	J93-2003	o	5 Effectiveness Comparison 5.1 English-Chinese ATIS Models Both the transfer and transducer systems were trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus -LRB- Hirschman et al. 1993 -RRB-	dep_1993_al. nn_al._et num_Hirschman_1993 nn_corpus_ATIS det_corpus_the prep_from_utterances_corpus amod_utterances_transcribed prep_of_translation_utterances nn_translation_Chinese nn_translation_English-to-Mandarin prep_on_evaluated_translation nsubjpass_evaluated_transfer dep_trained_Hirschman conj_and_trained_evaluated auxpass_trained_were nsubjpass_trained_systems nsubjpass_trained_transfer nn_systems_transducer conj_and_transfer_systems dep_transfer_the preconj_transfer_Both rcmod_Models_evaluated rcmod_Models_trained nn_Models_ATIS amod_Models_English-Chinese num_Models_5.1 dep_Comparison_Models nn_Comparison_Effectiveness num_Comparison_5