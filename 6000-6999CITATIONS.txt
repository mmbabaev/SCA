D07-1025	P06-1091	o	Both Liang et al -LRB- 2006 -RRB- and Tillmann and Zhang -LRB- 2006 -RRB- report on effective machine translation -LRB- MT -RRB- models involving large numbers of features with discriminatively trained weights	amod_weights_trained advmod_trained_discriminatively prep_with_features_weights prep_of_numbers_features amod_numbers_large dobj_involving_numbers vmod_models_involving nn_models_translation appos_translation_MT nn_translation_machine amod_translation_effective prep_on_report_models nn_report_Zhang nn_report_Tillmann appos_Zhang_2006 conj_and_Tillmann_Zhang dep_al_2006 nn_al_et conj_and_Liang_report conj_and_Liang_al det_Liang_Both dep_``_report dep_``_al dep_``_Liang
D07-1055	P06-1091	o	Tillmann and Zhang -LRB- 2006 -RRB- describe a perceptron style algorithm for training millions of features	prep_of_millions_features nn_millions_training prep_for_algorithm_millions nn_algorithm_style nn_algorithm_perceptron det_algorithm_a dobj_describe_algorithm nsubj_describe_Zhang nsubj_describe_Tillmann appos_Zhang_2006 conj_and_Tillmann_Zhang
D07-1080	P06-1091	o	The algorithm is slightly different from other online training algorithms -LRB- Tillmann and Zhang 2006 Liang et al. 2006 -RRB- in that we keep and update oracle translations which is a set of good translations reachable by a decoder according to a metric i.e. BLEU -LRB- Papineni et al. 2002 -RRB-	num_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_Papineni advmod_BLEU_i.e. dep_metric_BLEU det_metric_a det_decoder_a prep_by_reachable_decoder amod_translations_reachable amod_translations_good pobj_set_metric prepc_according_to_set_to prep_of_set_translations det_set_a cop_set_is nsubj_set_which rcmod_translations_set nn_translations_oracle nsubj_update_we dobj_keep_translations conj_and_keep_update nsubj_keep_we mark_keep_that num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Liang num_Tillmann_2006 conj_and_Tillmann_Zhang appos_algorithms_Zhang appos_algorithms_Tillmann nn_algorithms_training amod_algorithms_online amod_algorithms_other prepc_in_different_update prepc_in_different_keep prep_from_different_algorithms advmod_different_slightly cop_different_is nsubj_different_algorithm det_algorithm_The
D07-1080	P06-1091	o	Tillmann and Zhang -LRB- 2006 -RRB- avoided the problem by precomputing the oracle translations in advance	nn_translations_oracle det_translations_the prep_in_precomputing_advance dobj_precomputing_translations det_problem_the prepc_by_avoided_precomputing dobj_avoided_problem nsubj_avoided_Zhang nsubj_avoided_Tillmann appos_Zhang_2006 conj_and_Tillmann_Zhang
D07-1080	P06-1091	o	Tillmann and Zhang -LRB- 2006 -RRB- used a different update style based on a convex loss function = L -LRB- e e et -RRB- max parenleftBig 0 1 parenleftBig si -LRB- f t e -RRB- si -LRB- f t e -RRB- parenrightBigparenrightBig 768 Table 1 Experimental results obtained by varying normalized tokens used with surface form	nn_form_surface prep_with_used_form vmod_tokens_used dobj_normalized_tokens nsubj_normalized_results agent_obtained_varying vmod_results_obtained amod_results_Experimental num_Table_1 num_Table_768 dep_parenrightBigparenrightBig_Table dep_parenrightBigparenrightBig_e nn_t_f nn_t_si parataxis_t_normalized appos_t_parenrightBigparenrightBig dep_t_t appos_t_e nn_t_f dep_si_t nn_si_parenleftBig num_si_1 appos_parenleftBig_si num_parenleftBig_0 nn_parenleftBig_max nn_max_L dep_e_et appos_e_e appos_L_e dep_=_parenleftBig nn_function_loss nn_function_convex det_function_a prep_on_based_function dep_style_= vmod_style_based dobj_update_style dep_different_update amod_a_different dobj_used_a nsubj_used_Zhang nsubj_used_Tillmann appos_Zhang_2006 conj_and_Tillmann_Zhang
D07-1080	P06-1091	o	Tillmann and Zhang -LRB- 2006 -RRB- and Liang et al.	nn_al._et nn_al._Liang appos_Zhang_2006 conj_and_Tillmann_al. conj_and_Tillmann_Zhang
D07-1080	P06-1091	o	Tillmann and Zhang -LRB- 2006 -RRB- trained their feature set using an online discriminative algorithm	amod_algorithm_discriminative amod_algorithm_online det_algorithm_an dobj_using_algorithm xcomp_set_using vmod_feature_set poss_feature_their dobj_trained_feature nsubj_trained_Zhang appos_Zhang_2006 conj_and_Tillmann_trained
D07-1080	P06-1091	o	Online discriminative training has already been studied by Tillmann and Zhang -LRB- 2006 -RRB- and Liang et al.	nn_al._et nn_al._Liang appos_Zhang_2006 conj_and_Tillmann_al. conj_and_Tillmann_Zhang agent_studied_al. agent_studied_Zhang agent_studied_Tillmann auxpass_studied_been advmod_studied_already aux_studied_has nsubjpass_studied_training amod_training_discriminative amod_training_Online
D07-1080	P06-1091	o	Tillmann and Zhang -LRB- 2006 -RRB- Liang et al.	nn_al._et nn_al._Liang appos_Zhang_2006 dep_Tillmann_al. conj_and_Tillmann_Zhang
D08-1024	P06-1091	o	This paper continues a line of research on online discriminative training -LRB- Tillmann and Zhang 2006 Liang et al. 2006 Arun and Koehn 2007 -RRB- extending that of Watanabe et al.	nn_al._et nn_al._Watanabe prep_of_that_al. dep_extending_that dep_Arun_2007 conj_and_Arun_Koehn num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Koehn dep_Tillmann_Arun conj_and_Tillmann_Liang num_Tillmann_2006 conj_and_Tillmann_Zhang appos_training_Liang appos_training_Zhang appos_training_Tillmann amod_training_discriminative nn_training_online prep_on_line_training prep_of_line_research det_line_a dep_continues_extending dobj_continues_line nsubj_continues_paper det_paper_This
D08-1024	P06-1091	o	The second uses the decoder to search for the highest-B translation -LRB- Tillmann and Zhang 2006 -RRB- which Arun and Koehn -LRB- 2007 -RRB- call max-B updating	nsubj_updating_max-B rcmod_call_updating num_call_2007 dep_Koehn_call conj_and_Arun_Koehn dep_which_Koehn dep_which_Arun dep_Tillmann_2006 conj_and_Tillmann_Zhang dep_translation_which appos_translation_Zhang appos_translation_Tillmann amod_translation_highest-B det_translation_the prep_for_search_translation aux_search_to det_decoder_the vmod_uses_search dobj_uses_decoder nsubj_uses_second det_second_The
D09-1008	P06-1091	o	One is to use a stochastic gradient descent -LRB- SGD -RRB- or Perceptron like online learning algorithm to optimize the weights of these features directly for MT -LRB- Shen et al. 2004 Liang et al. 2006 Tillmann and Zhang 2006 -RRB-	amod_Tillmann_2006 conj_and_Tillmann_Zhang dep_Liang_Zhang dep_Liang_Tillmann num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Shen_Liang appos_Shen_2004 dep_Shen_al. nn_Shen_et dep_MT_Shen det_features_these prep_of_weights_features det_weights_the prep_for_optimize_MT advmod_optimize_directly dobj_optimize_weights aux_optimize_to nn_algorithm_learning amod_algorithm_online prep_like_Perceptron_algorithm conj_or_descent_Perceptron appos_descent_SGD nn_descent_gradient amod_descent_stochastic det_descent_a vmod_use_optimize dobj_use_Perceptron dobj_use_descent aux_use_to xcomp_is_use nsubj_is_One ccomp_``_is
D09-1039	P06-1091	o	Tillmann and Zhang -LRB- 2006 -RRB- present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations	det_translations_the prep_of_accuracy_translations det_accuracy_the amod_decoder_phrasebased det_decoder_a prep_on_used_accuracy agent_used_decoder vmod_function_used amod_function_scoring amod_function_global det_function_the dobj_optimize_function advmod_optimize_directly aux_optimize_to vmod_procedure_optimize det_procedure_a dobj_present_procedure nsubj_present_Zhang nsubj_present_Tillmann appos_Zhang_2006 conj_and_Tillmann_Zhang
I08-2087	P06-1091	o	2 Related Work This method is similar to block-orientation modeling -LRB- Tillmann and Zhang 2005 -RRB- and maximum entropy based phrase reordering model -LRB- Xiong et al. 2006 -RRB- in which local orientations -LRB- left/right -RRB- of phrase pairs -LRB- blocks -RRB- are learned via MaxEnt classifiers	nn_classifiers_MaxEnt prep_via_learned_classifiers auxpass_learned_are nsubjpass_learned_orientations prep_in_learned_which appos_pairs_blocks nn_pairs_phrase prep_of_orientations_pairs appos_orientations_left/right amod_orientations_local dep_2006_al. nn_al._et num_Xiong_2006 nn_model_reordering nn_model_phrase amod_model_based nn_model_entropy nn_entropy_maximum num_Zhang_2005 conj_and_Tillmann_Zhang conj_and_modeling_model dep_modeling_Zhang dep_modeling_Tillmann amod_modeling_block-orientation parataxis_similar_learned dep_similar_Xiong prep_to_similar_model prep_to_similar_modeling cop_similar_is nsubj_similar_method det_method_This dep_Work_similar amod_Work_Related num_Work_2
I08-2087	P06-1091	o	The use of structured prediction to SMT is also investigated by -LRB- Liang et al. 2006 Tillmann and Zhang 2006 Watanabe et al. 2007 -RRB-	dep_al._2007 nn_al._et nn_al._Watanabe dep_Tillmann_2006 conj_and_Tillmann_Zhang dep_Liang_al. dep_Liang_Zhang dep_Liang_Tillmann dep_Liang_2006 dep_Liang_al. nn_Liang_et agent_investigated_Liang advmod_investigated_also auxpass_investigated_is nsubjpass_investigated_use prep_to_prediction_SMT amod_prediction_structured prep_of_use_prediction det_use_The
N07-1008	P06-1091	o	Recently there have been several discriminative approaches at training large parameter sets including -LRB- Tillmann and Zhang 2006 -RRB- and -LRB- Liang et al. 2006 -RRB-	amod_Liang_2006 dep_Liang_al. nn_Liang_et conj_and_Tillmann_Liang amod_Tillmann_2006 conj_and_Tillmann_Zhang prep_including_sets_Liang prep_including_sets_Zhang prep_including_sets_Tillmann nn_sets_parameter amod_sets_large nn_sets_training prep_at_approaches_sets amod_approaches_discriminative amod_approaches_several cop_approaches_been aux_approaches_have expl_approaches_there advmod_approaches_Recently
N07-1008	P06-1091	o	In -LRB- Tillmann and Zhang 2006 -RRB- the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU	nn_BLEU_level nn_BLEU_sentence det_BLEU_a dobj_computing_BLEU prepc_for_used_computing advmod_used_only auxpass_used_is csubjpass_used_optimized nn_sentence_target det_sentence_the conj_and_orientation_sentence nn_orientation_block det_orientation_a dobj_produce_sentence dobj_produce_orientation aux_produce_to xcomp_optimized_produce auxpass_optimized_is nsubjpass_optimized_model prep_optimized_In det_model_the amod_Tillmann_2006 conj_and_Tillmann_Zhang dep_In_Zhang dep_In_Tillmann
N09-1025	P06-1091	o	Others have introduced alternative discriminative training methods -LRB- Tillmann and Zhang 2006 Liang et al. 2006 Turian et al. 2007 Blunsom et al. 2008 Macherey et al. 2008 -RRB- in which a recurring challenge is scalability to train many features we need many train218 ing examples and to train discriminatively we need to search through all possible translations of each training example	nn_example_training det_example_each prep_of_translations_example amod_translations_possible det_translations_all prep_through_search_translations aux_search_to xcomp_need_search nsubj_need_we advmod_train_discriminatively aux_train_to nn_examples_ing nn_examples_train218 amod_examples_many dobj_need_examples nsubj_need_we amod_features_many parataxis_train_need conj_and_train_train parataxis_train_need dobj_train_features aux_train_to dep_scalability_train dep_scalability_train cop_scalability_is nsubj_scalability_challenge prep_in_scalability_which amod_challenge_recurring det_challenge_a num_Macherey_2008 nn_Macherey_al. nn_Macherey_et num_Blunsom_2008 nn_Blunsom_al. nn_Blunsom_et num_Turian_2007 nn_Turian_al. nn_Turian_et num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Macherey dep_Tillmann_Blunsom dep_Tillmann_Turian dep_Tillmann_Liang amod_Tillmann_2006 conj_and_Tillmann_Zhang appos_methods_scalability appos_methods_Zhang appos_methods_Tillmann nn_methods_training amod_methods_discriminative amod_methods_alternative dobj_introduced_methods aux_introduced_have nsubj_introduced_Others ccomp_``_introduced
P07-1020	P06-1091	o	Discriminative training has been used mainly for translation model combination -LRB- Och and Ney 2002 -RRB- and with the exception of -LRB- Wellington et al. 2006 Tillmann and Zhang 2006 -RRB- has not been used to directly train parameters of a translation model	nn_model_translation det_model_a prep_of_parameters_model dobj_train_parameters advmod_train_directly aux_train_to xcomp_used_train auxpass_used_been neg_used_not aux_used_has amod_Tillmann_2006 conj_and_Tillmann_Zhang dep_Wellington_Zhang dep_Wellington_Tillmann appos_Wellington_2006 dep_Wellington_al. nn_Wellington_et prep_of_exception_Wellington det_exception_the dep_Och_2002 conj_and_Och_Ney appos_combination_Ney appos_combination_Och nn_combination_model nn_combination_translation dep_used_used prep_with_used_exception prep_for_used_combination advmod_used_mainly conj_and_used_used auxpass_used_been aux_used_has nsubjpass_used_training nsubjpass_used_training amod_training_Discriminative
P08-1010	P06-1091	o	The translation probability can also be discriminatively trained such as in Tillmann and Zhang -LRB- 2006 -RRB-	appos_Zhang_2006 conj_and_Tillmann_Zhang pobj_in_Zhang pobj_in_Tillmann prepc_such_as_trained_in advmod_trained_discriminatively auxpass_trained_be advmod_trained_also aux_trained_can nsubjpass_trained_probability nn_probability_translation det_probability_The
P09-1054	P06-1091	o	SGD was recently used for NLP tasks including machine translation -LRB- Tillmann and Zhang 2006 -RRB- and syntactic parsing -LRB- Smith and Eisner 2008 Finkel et al. 2008 -RRB-	num_Finkel_2008 nn_Finkel_al. nn_Finkel_et dep_Smith_Finkel amod_Smith_2008 conj_and_Smith_Eisner appos_parsing_Eisner appos_parsing_Smith nn_parsing_syntactic amod_Tillmann_2006 conj_and_Tillmann_Zhang conj_and_translation_parsing dep_translation_Zhang dep_translation_Tillmann nn_translation_machine prep_including_tasks_parsing prep_including_tasks_translation nn_tasks_NLP prep_for_used_tasks advmod_used_recently auxpass_used_was nsubjpass_used_SGD
W07-0414	P06-1091	o	If the input consists of sevWe also adopt the approximation that treats every sentence with its reference as a separate corpus -LRB- Tillmann and Zhang 2006 -RRB- so that ngram counts are not accumulated and parallel processing of sentences becomes possible	acomp_becomes_possible nsubj_becomes_processing prep_of_processing_sentences nn_processing_parallel conj_and_accumulated_becomes neg_accumulated_not auxpass_accumulated_are nsubjpass_accumulated_counts mark_accumulated_that advmod_accumulated_so nn_counts_ngram amod_Tillmann_2006 conj_and_Tillmann_Zhang appos_corpus_Zhang appos_corpus_Tillmann amod_corpus_separate det_corpus_a prep_as_reference_corpus poss_reference_its det_sentence_every advcl_treats_becomes advcl_treats_accumulated prep_with_treats_reference dobj_treats_sentence nsubj_treats_that rcmod_approximation_treats det_approximation_the dobj_adopt_approximation advmod_adopt_also advcl_adopt_consists prep_of_consists_sevWe nsubj_consists_input mark_consists_If det_input_the
W07-0414	P06-1091	o	Tillmann and Zhang -LRB- 2006 -RRB- use a BLEU oracle decoder for discriminative training of a local reordering model	nn_model_reordering amod_model_local det_model_a prep_of_training_model amod_training_discriminative nn_decoder_oracle nn_decoder_BLEU det_decoder_a prep_for_use_training dobj_use_decoder nsubj_use_Zhang nsubj_use_Tillmann appos_Zhang_2006 conj_and_Tillmann_Zhang
W07-0414	P06-1091	o	They can be used for discriminative training of reordering models -LRB- Tillmann and Zhang 2006 -RRB-	amod_Tillmann_2006 conj_and_Tillmann_Zhang dep_models_Zhang dep_models_Tillmann nn_models_reordering prep_of_training_models amod_training_discriminative prep_for_used_training auxpass_used_be aux_used_can nsubjpass_used_They
W07-0716	P06-1091	o	where they are expected to be maximally discriminative -LRB- Tillmann and Zhang 2006 -RRB-	amod_Tillmann_2006 conj_and_Tillmann_Zhang dep_discriminative_Zhang dep_discriminative_Tillmann advmod_discriminative_maximally cop_discriminative_be aux_discriminative_to xcomp_expected_discriminative auxpass_expected_are nsubjpass_expected_they advmod_expected_where advcl_``_expected
W07-0716	P06-1091	o	This might prove beneficial for various discriminative training methods -LRB- Tillmann and Zhang 2006 -RRB-	amod_Tillmann_2006 conj_and_Tillmann_Zhang dep_methods_Zhang dep_methods_Tillmann nn_methods_training amod_methods_discriminative amod_methods_various prep_for_beneficial_methods acomp_prove_beneficial aux_prove_might nsubj_prove_This ccomp_``_prove
W07-0717	P06-1091	o	This makes it suitable for discriminative SMT training which is still a challenge for large parameter sets -LRB- Tillmann and Zhang 2006 Liang et al. 2006 -RRB-	num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Liang num_Tillmann_2006 conj_and_Tillmann_Zhang appos_sets_Zhang appos_sets_Tillmann nn_sets_parameter amod_sets_large prep_for_challenge_sets det_challenge_a advmod_challenge_still cop_challenge_is nsubj_challenge_which rcmod_training_challenge nn_training_SMT amod_training_discriminative prep_for_suitable_training amod_it_suitable dobj_makes_it nsubj_makes_This
W07-0719	P06-1091	o	However at the short term the incorporation of these type of features will force us to either build a new decoder or extend an existing one or to move to a new MT architecture for instance in the fashion of the architectures suggested by Tillmann and Zhang -LRB- 2006 -RRB- or Liang et al.	nn_al._et nn_al._Liang appos_Zhang_2006 conj_or_Tillmann_al. conj_and_Tillmann_Zhang agent_suggested_al. agent_suggested_Zhang agent_suggested_Tillmann vmod_architectures_suggested det_architectures_the prep_of_fashion_architectures det_fashion_the nn_architecture_MT amod_architecture_new det_architecture_a prep_to_move_architecture aux_move_to conj_or_one_move amod_one_existing det_one_an dobj_extend_move dobj_extend_one amod_decoder_new det_decoder_a conj_or_build_extend dobj_build_decoder preconj_build_either aux_build_to prep_in_force_fashion prep_for_force_instance xcomp_force_extend xcomp_force_build dobj_force_us aux_force_will nsubj_force_incorporation prep_at_force_term advmod_force_However prep_of_type_features det_type_these prep_of_incorporation_type det_incorporation_the amod_term_short det_term_the
W08-0404	P06-1091	o	Several studies have shown that large-margin methods can be adapted to the special complexities of the task -LRB- Liang et al. 2006 Tillmann and Zhang 2006 Cowan et al. 2006 -RRB- However the capacity of these algorithms to improve over state-of-the-art baselines is currently limited by their lack of robust dimensionality reduction	nn_reduction_dimensionality amod_reduction_robust prep_of_lack_reduction poss_lack_their agent_limited_lack advmod_limited_currently auxpass_limited_is nsubjpass_limited_capacity amod_baselines_state-of-the-art prep_over_improve_baselines aux_improve_to det_algorithms_these vmod_capacity_improve prep_of_capacity_algorithms det_capacity_the num_Cowan_2006 nn_Cowan_al. nn_Cowan_et num_Tillmann_2006 conj_and_Tillmann_Zhang dep_Liang_Cowan dep_Liang_Zhang dep_Liang_Tillmann amod_Liang_2006 dep_Liang_al. nn_Liang_et det_task_the prep_of_complexities_task amod_complexities_special det_complexities_the prep_to_adapted_complexities auxpass_adapted_be aux_adapted_can nsubjpass_adapted_methods mark_adapted_that amod_methods_large-margin parataxis_shown_limited advmod_shown_However dep_shown_Liang ccomp_shown_adapted aux_shown_have nsubj_shown_studies amod_studies_Several
D07-1025	P06-1096	p	Both Liang et al -LRB- 2006 -RRB- and Tillmann and Zhang -LRB- 2006 -RRB- report on effective machine translation -LRB- MT -RRB- models involving large numbers of features with discriminatively trained weights	amod_weights_trained advmod_trained_discriminatively prep_with_features_weights prep_of_numbers_features amod_numbers_large dobj_involving_numbers vmod_models_involving nn_models_translation appos_translation_MT nn_translation_machine amod_translation_effective prep_on_report_models nn_report_Zhang nn_report_Tillmann appos_Zhang_2006 conj_and_Tillmann_Zhang dep_al_2006 nn_al_et conj_and_Liang_report conj_and_Liang_al det_Liang_Both dep_``_report dep_``_al dep_``_Liang
D07-1080	P06-1096	o	The algorithm is slightly different from other online training algorithms -LRB- Tillmann and Zhang 2006 Liang et al. 2006 -RRB- in that we keep and update oracle translations which is a set of good translations reachable by a decoder according to a metric i.e. BLEU -LRB- Papineni et al. 2002 -RRB-	num_Papineni_2002 dep_Papineni_al. nn_Papineni_et appos_BLEU_Papineni advmod_BLEU_i.e. dep_metric_BLEU det_metric_a det_decoder_a prep_by_reachable_decoder amod_translations_reachable amod_translations_good pobj_set_metric prepc_according_to_set_to prep_of_set_translations det_set_a cop_set_is nsubj_set_which rcmod_translations_set nn_translations_oracle nsubj_update_we dobj_keep_translations conj_and_keep_update nsubj_keep_we mark_keep_that num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Liang num_Tillmann_2006 conj_and_Tillmann_Zhang appos_algorithms_Zhang appos_algorithms_Tillmann nn_algorithms_training amod_algorithms_online amod_algorithms_other prepc_in_different_update prepc_in_different_keep prep_from_different_algorithms advmod_different_slightly cop_different_is nsubj_different_algorithm det_algorithm_The
D07-1080	P06-1096	o	Tillmann and Zhang -LRB- 2006 -RRB- and Liang et al.	nn_al._et nn_al._Liang appos_Zhang_2006 conj_and_Tillmann_al. conj_and_Tillmann_Zhang
D07-1080	P06-1096	o	Online discriminative training has already been studied by Tillmann and Zhang -LRB- 2006 -RRB- and Liang et al.	nn_al._et nn_al._Liang appos_Zhang_2006 conj_and_Tillmann_al. conj_and_Tillmann_Zhang agent_studied_al. agent_studied_Zhang agent_studied_Tillmann auxpass_studied_been advmod_studied_already aux_studied_has nsubjpass_studied_training amod_training_discriminative amod_training_Online
D07-1080	P06-1096	o	In this method each training sentence is decoded and weights are updated at every iteration -LRB- Liang et al. 2006 -RRB-	amod_Liang_2006 dep_Liang_al. nn_Liang_et det_iteration_every dep_updated_Liang prep_at_updated_iteration auxpass_updated_are nsubjpass_updated_weights nsubjpass_updated_decoded conj_and_decoded_weights auxpass_decoded_is nsubjpass_decoded_sentence prep_in_decoded_method nn_sentence_training det_sentence_each det_method_this
D07-1080	P06-1096	o	When updating model parameters we employ a memorizationvariant of a local updating strategy -LRB- Liang et al. 2006 -RRB- in which parameters are optimized toward a set of good translations found in the k-best list across iterations	prep_across_list_iterations amod_list_k-best det_list_the prep_in_found_list vmod_translations_found amod_translations_good prep_of_set_translations det_set_a prep_toward_optimized_set auxpass_optimized_are nsubjpass_optimized_parameters prep_in_optimized_which rcmod_Liang_optimized dep_Liang_2006 dep_Liang_al. nn_Liang_et amod_strategy_updating amod_strategy_local det_strategy_a prep_of_memorizationvariant_strategy det_memorizationvariant_a dep_employ_Liang dobj_employ_memorizationvariant nsubj_employ_we advcl_employ_updating nn_parameters_model dobj_updating_parameters advmod_updating_When
D07-1080	P06-1096	o	Tillmann and Zhang -LRB- 2006 -RRB- Liang et al.	nn_al._et nn_al._Liang appos_Zhang_2006 dep_Tillmann_al. conj_and_Tillmann_Zhang
D08-1023	P06-1096	o	Most work on discriminative training for SMT has focussed on linear models often with margin based algorithms -LRB- Liang et al. 2006 Watanabe et al. 2006 -RRB- or rescaling a product of sub-models -LRB- Och 2003 Ittycheriah and Roukos 2007 -RRB-	amod_Ittycheriah_2007 conj_and_Ittycheriah_Roukos dep_Och_Roukos dep_Och_Ittycheriah conj_Och_2003 dep_sub-models_Och prep_of_product_sub-models det_product_a dobj_rescaling_product num_Watanabe_2006 nn_Watanabe_al. nn_Watanabe_et dep_Liang_Watanabe appos_Liang_2006 dep_Liang_al. nn_Liang_et conj_or_algorithms_rescaling appos_algorithms_Liang dobj_based_rescaling dobj_based_algorithms vmod_margin_based amod_models_linear prep_with_focussed_margin advmod_focussed_often prep_on_focussed_models aux_focussed_has nsubj_focussed_work prep_for_training_SMT amod_training_discriminative prep_on_work_training amod_work_Most ccomp_``_focussed
D08-1024	P06-1096	o	This paper continues a line of research on online discriminative training -LRB- Tillmann and Zhang 2006 Liang et al. 2006 Arun and Koehn 2007 -RRB- extending that of Watanabe et al.	nn_al._et nn_al._Watanabe prep_of_that_al. dep_extending_that dep_Arun_2007 conj_and_Arun_Koehn num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Koehn dep_Tillmann_Arun conj_and_Tillmann_Liang num_Tillmann_2006 conj_and_Tillmann_Zhang appos_training_Liang appos_training_Zhang appos_training_Tillmann amod_training_discriminative nn_training_online prep_on_line_training prep_of_line_research det_line_a dep_continues_extending dobj_continues_line nsubj_continues_paper det_paper_This
D08-1024	P06-1096	n	Sentence-level approximations to B exist -LRB- Lin and Och 2004 Liang et al. 2006 -RRB- but we found it most effective to perform B computations in the context of a setOof previously-translated sentences following Watanabe et al.	nn_al._et nn_al._Watanabe dep_following_al. amod_sentences_previously-translated amod_sentences_setOof det_sentences_a prep_of_context_sentences det_context_the nn_computations_B vmod_perform_following prep_in_perform_context dobj_perform_computations aux_perform_to dep_perform_effective nsubj_perform_it advmod_effective_most xcomp_found_perform nsubj_found_we num_Liang_2006 nn_Liang_al. nn_Liang_et dep_2004_Liang dep_2004_Och dep_2004_Lin conj_and_Lin_Och conj_but_exist_found dep_exist_2004 nsubj_exist_approximations prep_to_approximations_B amod_approximations_Sentence-level
D08-1064	P06-1096	o	Moreover this evaluation concern dovetails with a frequent engineering concern that sentence-level scores are useful at various points in the MT pipeline for example minimum Bayes risk decoding -LRB- Kumar and Byrne 2004 -RRB- selecting oracle translations for discriminative reranking -LRB- Liang 614 et al. 2006 Watanabe et al. 2007 -RRB- and sentenceby-sentence comparisons of outputs during error analysis	nn_analysis_error prep_of_comparisons_outputs nn_comparisons_sentenceby-sentence num_Watanabe_2007 nn_Watanabe_al. nn_Watanabe_et nn_al._et num_al._614 dep_Liang_Watanabe appos_Liang_2006 dep_Liang_al. appos_reranking_Liang amod_reranking_discriminative prep_for_translations_reranking nn_translations_oracle dobj_selecting_translations dep_Kumar_2004 conj_and_Kumar_Byrne prep_during_decoding_analysis conj_and_decoding_comparisons vmod_decoding_selecting dep_decoding_Byrne dep_decoding_Kumar nn_decoding_risk nn_decoding_Bayes amod_decoding_minimum nn_pipeline_MT det_pipeline_the prep_in_points_pipeline amod_points_various prep_at_useful_points cop_useful_are nsubj_useful_scores dobj_useful_that amod_scores_sentence-level rcmod_concern_useful nn_concern_engineering amod_concern_frequent det_concern_a dep_dovetails_comparisons dep_dovetails_decoding prep_for_dovetails_example prep_with_dovetails_concern nsubj_dovetails_concern advmod_dovetails_Moreover nn_concern_evaluation det_concern_this
D09-1008	P06-1096	o	One is to use a stochastic gradient descent -LRB- SGD -RRB- or Perceptron like online learning algorithm to optimize the weights of these features directly for MT -LRB- Shen et al. 2004 Liang et al. 2006 Tillmann and Zhang 2006 -RRB-	amod_Tillmann_2006 conj_and_Tillmann_Zhang dep_Liang_Zhang dep_Liang_Tillmann num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Shen_Liang appos_Shen_2004 dep_Shen_al. nn_Shen_et dep_MT_Shen det_features_these prep_of_weights_features det_weights_the prep_for_optimize_MT advmod_optimize_directly dobj_optimize_weights aux_optimize_to nn_algorithm_learning amod_algorithm_online prep_like_Perceptron_algorithm conj_or_descent_Perceptron appos_descent_SGD nn_descent_gradient amod_descent_stochastic det_descent_a vmod_use_optimize dobj_use_Perceptron dobj_use_descent aux_use_to xcomp_is_use nsubj_is_One ccomp_``_is
D09-1107	P06-1096	o	In -LRB- Liang et al. 2006 -RRB- a standard phrase-based model is augmented with more than a million features whose weights are trained discriminatively by a variant of the perceptron algorithm	nn_algorithm_perceptron det_algorithm_the prep_of_variant_algorithm det_variant_a agent_trained_variant advmod_trained_discriminatively auxpass_trained_are nsubjpass_trained_weights poss_weights_whose rcmod_features_trained num_features_million quantmod_million_a quantmod_million_than mwe_than_more prep_with_augmented_features cop_augmented_is nsubj_augmented_model dep_augmented_Liang mark_augmented_In amod_model_phrase-based amod_model_standard det_model_a nn_al._et amod_Liang_2006 dep_Liang_al. advcl_``_augmented
D09-1111	P06-1096	o	By building the entire system on the derivation level we side-step issues that can occur when perceptron training with hidden derivations -LRB- Liang et al. 2006 -RRB- but we also introduce the need to transform our training source-target pairs into training derivations	nn_derivations_training prep_into_pairs_derivations amod_pairs_source-target dep_training_pairs poss_training_our dobj_transform_training aux_transform_to vmod_need_transform det_need_the dobj_introduce_need advmod_introduce_also nsubj_introduce_we amod_Liang_2006 dep_Liang_al. nn_Liang_et amod_derivations_hidden conj_but_training_introduce dep_training_Liang prep_with_training_derivations nn_training_perceptron dep_when_introduce dep_when_training dep_occur_when aux_occur_can nsubj_occur_that rcmod_issues_occur dobj_side-step_issues nsubj_side-step_we prepc_by_side-step_building nn_level_derivation det_level_the prep_on_system_level amod_system_entire det_system_the dobj_building_system
D09-1127	P06-1096	o	So we will engineer more such features especially with lexicalization and soft alignments -LRB- Liang et al. 2006 -RRB- and study the impact of alignment quality on parsing improvement	amod_improvement_parsing nn_quality_alignment prep_on_impact_improvement prep_of_impact_quality det_impact_the dobj_study_impact nsubj_study_we amod_Liang_2006 dep_Liang_al. nn_Liang_et amod_alignments_soft conj_and_lexicalization_alignments amod_features_such advmod_features_more conj_and_engineer_study dep_engineer_Liang prep_with_engineer_alignments prep_with_engineer_lexicalization advmod_engineer_especially dobj_engineer_features aux_engineer_will nsubj_engineer_we dep_engineer_So
E09-1056	P06-1096	p	Online votedperceptrons have been reported to work well in a number of NLP tasks -LRB- Collins 2002 Liang et al. 2006 -RRB-	num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Collins_Liang amod_Collins_2002 nn_tasks_NLP prep_of_number_tasks det_number_a dep_work_Collins prep_in_work_number advmod_work_well aux_work_to xcomp_reported_work auxpass_reported_been aux_reported_have nsubjpass_reported_votedperceptrons nn_votedperceptrons_Online
E09-1061	P06-1096	o	Alignment is often used in training both generative and discriminative models -LRB- Brown et al. 1993 Blunsom et al. 2008 Liang et al. 2006 -RRB-	nn_al._et nn_al._Liang nn_al._et nn_al._Blunsom amod_Brown_2006 dep_Brown_al. amod_Brown_2008 dep_Brown_al. amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_models_discriminative amod_models_generative nn_models_training conj_and_generative_discriminative preconj_generative_both dep_used_Brown prep_in_used_models advmod_used_often auxpass_used_is nsubjpass_used_Alignment
E09-1061	P06-1096	o	item form -LSB- i j ueve -RSB- goal -LSB- I j ue -RSB- rules -LSB- i j ue -RSB- R -LRB- fifiprime/ejejprime -RRB- -LSB- iprime j ejejprime -RSB- -LSB- i j ueejve -RSB- -LSB- i j + 1 ueejve -RSB- ej +1 = rj +1 -LRB- Logic MONOTONE-ALIGN -RRB- Under the boolean semiring this -LRB- minimal -RRB- logic decides if a training example is reachable by the model which is required by some discriminative training regimens -LRB- Liang et al. 2006 Blunsom et al. 2008 -RRB-	num_Blunsom_2008 nn_Blunsom_al. nn_Blunsom_et dep_Liang_Blunsom dep_Liang_2006 dep_Liang_al. nn_Liang_et nn_regimens_training amod_regimens_discriminative det_regimens_some agent_required_regimens auxpass_required_is nsubjpass_required_which rcmod_model_required det_model_the prep_by_reachable_model cop_reachable_is nsubj_reachable_example mark_reachable_if nn_example_training det_example_a advcl_decides_reachable nsubj_decides_logic nsubj_decides_j amod_logic_minimal det_logic_this amod_semiring_boolean det_semiring_the nn_MONOTONE-ALIGN_Logic appos_+1_MONOTONE-ALIGN nn_+1_rj dep_=_+1 prep_under_+1_semiring amod_+1_= nn_+1_ej conj_+_j_1 dep_i_ueejve appos_i_1 appos_i_j appos_i_ueejve appos_i_j appos_iprime_ejejprime appos_iprime_j appos_R_fifiprime/ejejprime nn_R_ue dep_j_+1 dep_j_i dep_j_i dep_j_iprime appos_j_R dep_i_Liang ccomp_i_decides nn_rules_ue dep_rules_I appos_I_j nn_goal_ueve dep_j_i dep_j_rules appos_j_goal conj_i_j dep_form_i nn_form_item
I08-2087	P06-1096	o	The use of structured prediction to SMT is also investigated by -LRB- Liang et al. 2006 Tillmann and Zhang 2006 Watanabe et al. 2007 -RRB-	dep_al._2007 nn_al._et nn_al._Watanabe dep_Tillmann_2006 conj_and_Tillmann_Zhang dep_Liang_al. dep_Liang_Zhang dep_Liang_Tillmann dep_Liang_2006 dep_Liang_al. nn_Liang_et agent_investigated_Liang advmod_investigated_also auxpass_investigated_is nsubjpass_investigated_use prep_to_prediction_SMT amod_prediction_structured prep_of_use_prediction det_use_The
N07-1008	P06-1096	o	Recently there have been several discriminative approaches at training large parameter sets including -LRB- Tillmann and Zhang 2006 -RRB- and -LRB- Liang et al. 2006 -RRB-	amod_Liang_2006 dep_Liang_al. nn_Liang_et conj_and_Tillmann_Liang amod_Tillmann_2006 conj_and_Tillmann_Zhang prep_including_sets_Liang prep_including_sets_Zhang prep_including_sets_Tillmann nn_sets_parameter amod_sets_large nn_sets_training prep_at_approaches_sets amod_approaches_discriminative amod_approaches_several cop_approaches_been aux_approaches_have expl_approaches_there advmod_approaches_Recently
N07-1008	P06-1096	o	-LRB- Liang et al. 2006 -RRB- demonstrates a discriminatively trained system for machine translation that has the following characteristics 1 -RRB- requires a varying update strategy -LRB- local vs. bold -RRB- depending on whether the reference sentence is reachable or not 2 -RRB- uses sentence level BLEU as a criterion for selecting which output to update towards and 3 -RRB- only trains on limited length -LRB- 5-15 words -RRB- sentences	nn_sentences_trains dep_sentences_3 num_words_5-15 amod_length_limited appos_trains_words prep_on_trains_length advmod_trains_only prep_update_towards aux_update_to det_output_which vmod_selecting_update dobj_selecting_output prepc_for_criterion_selecting det_criterion_a nn_BLEU_level nn_BLEU_sentence prep_as_uses_criterion dobj_uses_BLEU dep_uses_2 dep_2_not dep_2_reachable conj_or_reachable_not cop_reachable_is nsubj_reachable_sentence mark_reachable_whether nn_sentence_reference det_sentence_the conj_vs._local_bold conj_and_strategy_sentences prepc_depending_on_strategy_uses dep_strategy_bold dep_strategy_local dobj_update_sentences dobj_update_strategy pcomp_varying_update dep_a_varying dep_requires_a dep_requires_1 amod_characteristics_following det_characteristics_the dobj_has_characteristics nsubj_has_that nn_translation_machine rcmod_system_has prep_for_system_translation amod_system_trained advmod_system_discriminatively det_system_a parataxis_demonstrates_requires dobj_demonstrates_system nsubj_demonstrates_Liang dep_Liang_2006 dep_Liang_al. nn_Liang_et
N07-1008	P06-1096	n	This latter point is a critical difference that contrasts to the major weakness of the work of -LRB- Liang et al. 2006 -RRB- which uses a top-N list of translations to select the maximum BLEU sentence as a target for training -LRB- so called local update -RRB-	dep_called_update dep_called_local advmod_called_so prep_for_target_training det_target_a prep_as_sentence_target nn_sentence_BLEU nn_sentence_maximum det_sentence_the dep_select_called dobj_select_sentence aux_select_to prep_of_list_translations amod_list_top-N det_list_a vmod_uses_select dobj_uses_list nsubj_uses_which rcmod_Liang_uses amod_Liang_2006 dep_Liang_al. nn_Liang_et prep_of_work_Liang det_work_the prep_of_weakness_work amod_weakness_major det_weakness_the prep_to_contrasts_weakness nsubj_contrasts_that rcmod_difference_contrasts amod_difference_critical det_difference_a cop_difference_is nsubj_difference_point amod_point_latter det_point_This
N09-1025	P06-1096	o	Others have introduced alternative discriminative training methods -LRB- Tillmann and Zhang 2006 Liang et al. 2006 Turian et al. 2007 Blunsom et al. 2008 Macherey et al. 2008 -RRB- in which a recurring challenge is scalability to train many features we need many train218 ing examples and to train discriminatively we need to search through all possible translations of each training example	nn_example_training det_example_each prep_of_translations_example amod_translations_possible det_translations_all prep_through_search_translations aux_search_to xcomp_need_search nsubj_need_we advmod_train_discriminatively aux_train_to nn_examples_ing nn_examples_train218 amod_examples_many dobj_need_examples nsubj_need_we amod_features_many parataxis_train_need conj_and_train_train parataxis_train_need dobj_train_features aux_train_to dep_scalability_train dep_scalability_train cop_scalability_is nsubj_scalability_challenge prep_in_scalability_which amod_challenge_recurring det_challenge_a num_Macherey_2008 nn_Macherey_al. nn_Macherey_et num_Blunsom_2008 nn_Blunsom_al. nn_Blunsom_et num_Turian_2007 nn_Turian_al. nn_Turian_et num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Macherey dep_Tillmann_Blunsom dep_Tillmann_Turian dep_Tillmann_Liang amod_Tillmann_2006 conj_and_Tillmann_Zhang appos_methods_scalability appos_methods_Zhang appos_methods_Tillmann nn_methods_training amod_methods_discriminative amod_methods_alternative dobj_introduced_methods aux_introduced_have nsubj_introduced_Others ccomp_``_introduced
P07-1055	P06-1096	o	Work on learning with hidden variables can be used for both CRFs -LRB- Quattoni et al. 2004 -RRB- and for inference based learning algorithms like those used in this work -LRB- Liang et al. 2006 -RRB-	amod_Liang_2006 dep_Liang_al. nn_Liang_et det_work_this prep_in_used_work vmod_those_used prep_like_algorithms_those dobj_learning_algorithms pcomp_based_learning prep_inference_based pobj_for_inference appos_Quattoni_2004 dep_Quattoni_al. nn_Quattoni_et dep_CRFs_Liang conj_and_CRFs_for dep_CRFs_Quattoni preconj_CRFs_both prep_for_used_for prep_for_used_CRFs auxpass_used_be aux_used_can nsubjpass_used_Work amod_variables_hidden prep_with_learning_variables prepc_on_Work_learning
P07-1055	P06-1096	o	These algorithms are usually applied to sequential labeling or chunking but have also been applied to parsing -LRB- Taskar et al. 2004 McDonald et al. 2005 -RRB- machine translation -LRB- Liang et al. 2006 -RRB- and summarization -LRB- Daume III et al. 2006 -RRB-	amod_III_2006 dep_III_al. nn_III_et nn_III_Daume dep_summarization_III amod_Liang_2006 dep_Liang_al. nn_Liang_et conj_and_translation_summarization dep_translation_Liang nn_translation_machine num_McDonald_2005 nn_McDonald_al. nn_McDonald_et dep_Taskar_McDonald appos_Taskar_2004 dep_Taskar_al. nn_Taskar_et prep_to_applied_parsing auxpass_applied_been advmod_applied_also aux_applied_have nsubjpass_applied_algorithms conj_or_labeling_chunking amod_labeling_sequential dep_applied_summarization dep_applied_translation dep_applied_Taskar conj_but_applied_applied prep_to_applied_chunking prep_to_applied_labeling advmod_applied_usually auxpass_applied_are nsubjpass_applied_algorithms det_algorithms_These
P08-1024	P06-1096	o	For this reason to our knowledge all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures such that spurious ambiguity is lessened or removed entirely -LRB- Ittycheriah and Roukos 2007 Watanabe et al. 2007 -RRB- or else ignore the problem and treat derivations as translations -LRB- Liang et al. 2006 Tillmann and Zhang 2007 -RRB-	amod_Tillmann_2007 conj_and_Tillmann_Zhang dep_Liang_Zhang dep_Liang_Tillmann appos_Liang_2006 dep_Liang_al. nn_Liang_et prep_as_treat_translations dobj_treat_derivations det_problem_the dobj_ignore_problem advmod_ignore_else num_Watanabe_2007 nn_Watanabe_al. nn_Watanabe_et dep_Ittycheriah_Liang conj_and_Ittycheriah_treat conj_or_Ittycheriah_ignore conj_and_Ittycheriah_Watanabe conj_and_Ittycheriah_2007 conj_and_Ittycheriah_Roukos nsubjpass_removed_ambiguity dep_lessened_treat dep_lessened_ignore dep_lessened_Watanabe dep_lessened_2007 dep_lessened_Roukos dep_lessened_Ittycheriah advmod_lessened_entirely conj_or_lessened_removed auxpass_lessened_is nsubjpass_lessened_ambiguity mark_lessened_that amod_ambiguity_spurious ccomp_such_removed ccomp_such_lessened nn_structures_feature conj_and_model_structures amod_model_simple dobj_choosing_structures dobj_choosing_model det_problem_the conj_side-step_such prepc_by_side-step_choosing dobj_side-step_problem preconj_side-step_either nsubj_side-step_models prep_to_side-step_knowledge prep_for_side-step_reason prep_to_proposed_date vmod_models_proposed amod_models_discriminative det_models_all poss_knowledge_our det_reason_this
P08-1024	P06-1096	n	To our knowledge no systems directly address Problem 1 instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list -LRB- Liang et al. 2006 Watanabe et al. 2007 -RRB- or else making local independence assumptions which side-step the issue -LRB- Ittycheriah and Roukos 2007 Tillmann and Zhang 2007 Wellington et al. 2006 -RRB-	num_Wellington_2006 nn_Wellington_al. nn_Wellington_et num_Tillmann_2007 conj_and_Tillmann_Zhang dep_Ittycheriah_Wellington conj_and_Ittycheriah_Zhang conj_and_Ittycheriah_Tillmann conj_and_Ittycheriah_2007 conj_and_Ittycheriah_Roukos dep_issue_Tillmann dep_issue_2007 dep_issue_Roukos dep_issue_Ittycheriah det_issue_the dobj_side-step_issue nsubj_side-step_which rcmod_assumptions_side-step nn_assumptions_independence amod_assumptions_local dobj_making_assumptions advmod_making_else num_Watanabe_2007 nn_Watanabe_al. nn_Watanabe_et conj_or_Liang_making dep_Liang_Watanabe appos_Liang_2006 dep_Liang_al. nn_Liang_et amod_list_n-best det_list_an nn_derivations_reference prep_of_handful_derivations amod_handful_small det_handful_a conj_or_one_handful prep_in_using_list dobj_using_handful dobj_using_one det_problem_the prepc_by_ignore_using dobj_ignore_problem aux_ignore_to dobj_choosing_making dobj_choosing_Liang xcomp_choosing_ignore advmod_choosing_instead num_Problem_1 vmod_address_choosing dobj_address_Problem advmod_address_directly nsubj_address_systems prep_to_address_knowledge neg_systems_no poss_knowledge_our
P08-1024	P06-1096	n	Both the global models -LRB- Liang et al. 2006 Watanabe et al. 2007 -RRB- use fairly small training sets and there is no evidence that their techniques will scale to larger data sets	nn_sets_data amod_sets_larger prep_to_scale_sets aux_scale_will nsubj_scale_techniques mark_scale_that poss_techniques_their ccomp_evidence_scale neg_evidence_no nsubj_is_evidence expl_is_there nn_sets_training amod_sets_small advmod_sets_fairly conj_and_use_is dobj_use_sets nsubj_use_models num_Watanabe_2007 nn_Watanabe_al. nn_Watanabe_et dep_Liang_Watanabe appos_Liang_2006 dep_Liang_al. nn_Liang_et appos_models_Liang amod_models_global det_models_the preconj_models_Both
P08-2007	P06-1096	o	Forced decoding arises in online discriminative training where model updates are made toward the most likely derivation of a gold translation -LRB- Liang et al. 2006 -RRB-	amod_Liang_2006 dep_Liang_al. nn_Liang_et nn_translation_gold det_translation_a prep_of_derivation_translation amod_derivation_likely det_derivation_the advmod_likely_most prep_toward_made_derivation auxpass_made_are nsubjpass_made_updates advmod_made_where nn_updates_model rcmod_training_made amod_training_discriminative nn_training_online dep_arises_Liang prep_in_arises_training nsubj_arises_decoding amod_decoding_Forced
W07-0717	P06-1096	o	This makes it suitable for discriminative SMT training which is still a challenge for large parameter sets -LRB- Tillmann and Zhang 2006 Liang et al. 2006 -RRB-	num_Liang_2006 nn_Liang_al. nn_Liang_et dep_Tillmann_Liang num_Tillmann_2006 conj_and_Tillmann_Zhang appos_sets_Zhang appos_sets_Tillmann nn_sets_parameter amod_sets_large prep_for_challenge_sets det_challenge_a advmod_challenge_still cop_challenge_is nsubj_challenge_which rcmod_training_challenge nn_training_SMT amod_training_discriminative prep_for_suitable_training amod_it_suitable dobj_makes_it nsubj_makes_This
W07-0719	P06-1096	o	However at the short term the incorporation of these type of features will force us to either build a new decoder or extend an existing one or to move to a new MT architecture for instance in the fashion of the architectures suggested by Tillmann and Zhang -LRB- 2006 -RRB- or Liang et al.	nn_al._et nn_al._Liang appos_Zhang_2006 conj_or_Tillmann_al. conj_and_Tillmann_Zhang agent_suggested_al. agent_suggested_Zhang agent_suggested_Tillmann vmod_architectures_suggested det_architectures_the prep_of_fashion_architectures det_fashion_the nn_architecture_MT amod_architecture_new det_architecture_a prep_to_move_architecture aux_move_to conj_or_one_move amod_one_existing det_one_an dobj_extend_move dobj_extend_one amod_decoder_new det_decoder_a conj_or_build_extend dobj_build_decoder preconj_build_either aux_build_to prep_in_force_fashion prep_for_force_instance xcomp_force_extend xcomp_force_build dobj_force_us aux_force_will nsubj_force_incorporation prep_at_force_term advmod_force_However prep_of_type_features det_type_these prep_of_incorporation_type det_incorporation_the amod_term_short det_term_the
W08-0306	P06-1096	o	In general Agold / Acandidates following -LRB- Collins 2000 -RRB- and -LRB- Charniak and Johnson 2005 -RRB- for parse reranking and -LRB- Liang et al. 2006 -RRB- for translation reranking we define Aoracle as alignment in Acandidates that is most similar to Agold .8 We update each feature weight i as follows i = i + hAoraclei hA1-besti .9 Following -LRB- Moore 2005 -RRB- after each training pass we average all the feature weight vectors seen during the pass and decode the discriminative training set using the vector of averaged feature weights	nn_weights_feature dobj_averaged_weights prepc_of_vector_averaged det_vector_the dobj_using_vector xcomp_set_using vmod_training_set amod_training_discriminative det_training_the dobj_decode_training det_pass_the prep_during_seen_pass vmod_vectors_seen nn_vectors_weight nn_vectors_feature det_vectors_the predet_vectors_all dobj_average_vectors nsubj_average_we dep_average_after dep_average_hA1-besti dep_average_i nn_pass_training det_pass_each pobj_after_pass amod_Moore_2005 dep_Following_Moore vmod_hA1-besti_Following num_hA1-besti_.9 nn_hA1-besti_hAoraclei conj_+_i_after conj_+_i_hA1-besti ccomp_=_average conj_and_i_decode amod_i_= mark_follows_as dep_weight_i nn_weight_feature det_weight_each dep_update_decode dep_update_i advcl_update_follows dobj_update_weight nsubj_update_We num_Agold_.8 dep_similar_update prep_to_similar_Agold advmod_similar_most cop_similar_is nsubj_similar_that rcmod_alignment_similar prep_in_alignment_Acandidates prep_as_define_alignment dobj_define_Aoracle nsubj_define_we nn_reranking_translation amod_Liang_2006 dep_Liang_al. nn_Liang_et nn_reranking_parse prep_for_Charniak_reranking dep_Charniak_2005 conj_and_Charniak_Johnson rcmod_Collins_define prep_for_Collins_reranking conj_and_Collins_Liang conj_and_Collins_Johnson conj_and_Collins_Charniak amod_Collins_2000 prep_following_Acandidates_Liang prep_following_Acandidates_Charniak prep_following_Acandidates_Collins dep_Acandidates_Agold prep_in_Acandidates_general
W08-0306	P06-1096	o	9 -LRB- Liang et al. 2006 -RRB- report that for translation reranking such local updates -LRB- towards the oracle -RRB- outperform bold updates -LRB- towards the gold standard -RRB-	amod_standard_gold det_standard_the prep_towards_updates_standard amod_updates_bold dobj_outperform_updates nsubj_outperform_updates prep_for_outperform_reranking mark_outperform_that dep_outperform_report det_oracle_the prep_towards_updates_oracle amod_updates_local amod_updates_such nn_reranking_translation dep_report_Liang num_report_9 amod_Liang_2006 dep_Liang_al. nn_Liang_et ccomp_``_outperform
W08-0404	P06-1096	n	Several studies have shown that large-margin methods can be adapted to the special complexities of the task -LRB- Liang et al. 2006 Tillmann and Zhang 2006 Cowan et al. 2006 -RRB- However the capacity of these algorithms to improve over state-of-the-art baselines is currently limited by their lack of robust dimensionality reduction	nn_reduction_dimensionality amod_reduction_robust prep_of_lack_reduction poss_lack_their agent_limited_lack advmod_limited_currently auxpass_limited_is nsubjpass_limited_capacity amod_baselines_state-of-the-art prep_over_improve_baselines aux_improve_to det_algorithms_these vmod_capacity_improve prep_of_capacity_algorithms det_capacity_the num_Cowan_2006 nn_Cowan_al. nn_Cowan_et num_Tillmann_2006 conj_and_Tillmann_Zhang dep_Liang_Cowan dep_Liang_Zhang dep_Liang_Tillmann amod_Liang_2006 dep_Liang_al. nn_Liang_et det_task_the prep_of_complexities_task amod_complexities_special det_complexities_the prep_to_adapted_complexities auxpass_adapted_be aux_adapted_can nsubjpass_adapted_methods mark_adapted_that amod_methods_large-margin parataxis_shown_limited advmod_shown_However dep_shown_Liang ccomp_shown_adapted aux_shown_have nsubj_shown_studies amod_studies_Several
W08-0410	P06-1096	o	As modern systems move toward integrating many features -LRB- Liang et al. 2006 -RRB- resources such as this will become increasingly important in improving translation quality	nn_quality_translation dobj_improving_quality advmod_important_increasingly prepc_in_become_improving acomp_become_important aux_become_will nsubj_become_resources prep_such_as_resources_this amod_Liang_2006 dep_Liang_al. nn_Liang_et rcmod_features_become dep_features_Liang amod_features_many dobj_integrating_features prepc_toward_move_integrating nsubj_move_systems mark_move_As amod_systems_modern advcl_``_move
W08-0510	P06-1096	o	Research have also been made into alternatives to the current log-linear scoring model such as discriminative models with millions of features -LRB- Liang et al. 2006 -RRB- or kernel based models -LRB- Wang et al. 2007 -RRB-	nn_al._et dep_Wang_2007 advmod_Wang_al. amod_models_based amod_models_kernel amod_models_current det_models_the dep_2006_al. nn_al._et num_Liang_2006 prep_of_millions_features prep_with_models_millions amod_models_discriminative prep_such_as_model_models dobj_scoring_model dep_log-linear_scoring conj_or_current_kernel dep_current_Liang dep_current_log-linear prep_to_alternatives_models dep_made_Wang prep_into_made_alternatives auxpass_made_been advmod_made_also aux_made_have nsubjpass_made_Research
W09-2211	P06-1096	p	Perhaps more importantly discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models such as letter-to-phoneme conversion -LRB- Jiampojamarn et al. 2008 -RRB- semantic role labeling -LRB- Toutanova et al. 2005 -RRB- syntactic parsing -LRB- Taskar et al. 2004 -RRB- language modeling -LRB- Roark et al. 2004 -RRB- and machine translation -LRB- Liang et al. 2006 -RRB-	amod_Liang_2006 dep_Liang_al. nn_Liang_et dep_translation_Liang nn_translation_machine amod_Roark_2004 dep_Roark_al. nn_Roark_et nn_modeling_language amod_Taskar_2004 dep_Taskar_al. nn_Taskar_et appos_parsing_Taskar amod_parsing_syntactic amod_Toutanova_2005 dep_Toutanova_al. nn_Toutanova_et dep_labeling_Toutanova nn_labeling_role amod_labeling_semantic amod_Jiampojamarn_2008 dep_Jiampojamarn_al. nn_Jiampojamarn_et dep_conversion_Roark conj_conversion_modeling conj_conversion_parsing conj_conversion_labeling dep_conversion_Jiampojamarn amod_conversion_letter-to-phoneme prep_such_as_models_conversion amod_models_generative conj_and_tackled_translation prep_via_tackled_models advmod_tackled_traditionally auxpass_tackled_are nsubjpass_tackled_that rcmod_tasks_translation rcmod_tasks_tackled prep_in_tasks_NLP nn_tasks_learning amod_tasks_structured amod_tasks_sequential conj_and_sequential_structured prep_of_variety_tasks det_variety_a amod_performance_competitive prep_on_offer_variety dobj_offer_performance aux_offer_to xcomp_shown_offer auxpass_shown_been aux_shown_have nsubjpass_shown_models advmod_shown_importantly amod_models_discriminative advmod_importantly_more advmod_importantly_Perhaps
D07-1005	P06-1097	o	We observe that AER is loosely correlated to BLEU -LRB- = 0.81 -RRB- though the relation is weak as observed earlier by Fraser and Marcu -LRB- 2006a -RRB-	dep_Marcu_2006a conj_and_Fraser_Marcu prep_by_earlier_Marcu prep_by_earlier_Fraser advmod_observed_earlier prepc_as_weak_observed cop_weak_is nsubj_weak_relation mark_weak_though det_relation_the dobj_=_0.81 dep_BLEU_= advcl_correlated_weak prep_to_correlated_BLEU advmod_correlated_loosely auxpass_correlated_is nsubjpass_correlated_AER mark_correlated_that ccomp_observe_correlated nsubj_observe_We ccomp_``_observe
D07-1005	P06-1097	o	High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system -LRB- Och and Ney 2003 Fraser and Marcu 2006b -RRB-	appos_Och_2006b conj_and_Och_Marcu conj_and_Och_Fraser conj_and_Och_2003 conj_and_Och_Ney dep_system_Marcu dep_system_Fraser dep_system_2003 dep_system_Ney dep_system_Och nn_system_SMT amod_system_phrase-based det_system_a prep_of_quality_system dobj_improve_quality nsubj_improve_which rcmod_phrase-pairs_improve amod_phrase-pairs_accurate advmod_accurate_more dobj_yield_phrase-pairs aux_yield_can nsubj_yield_alignments nn_alignments_word nn_alignments_quality amod_alignments_High ccomp_``_yield
D07-1005	P06-1097	o	Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling -LRB- Och and Ney 2003 Deng and Byrne 2005 Martin et al. 2005 -RRB- or alternative approaches to training -LRB- Fraser and Marcu 2006b Moore 2005 Ittycheriah and Roukos 2005 -RRB-	appos_Ittycheriah_2005 conj_and_Ittycheriah_Roukos num_Moore_2005 dep_Fraser_Roukos dep_Fraser_Ittycheriah conj_and_Fraser_Moore conj_and_Fraser_2006b conj_and_Fraser_Marcu dep_training_Moore dep_training_2006b dep_training_Marcu dep_training_Fraser prep_to_approaches_training amod_approaches_alternative num_Martin_2005 nn_Martin_al. nn_Martin_et conj_and_Och_Martin conj_and_Och_2005 conj_and_Och_Byrne conj_and_Och_Deng conj_and_Och_2003 conj_and_Och_Ney conj_or_modeling_approaches dep_modeling_Martin dep_modeling_2005 dep_modeling_Byrne dep_modeling_Deng dep_modeling_2003 dep_modeling_Ney dep_modeling_Och amod_modeling_better nn_quality_alignment nn_quality_word det_quality_the prep_through_improving_approaches prep_through_improving_modeling dobj_improving_quality prepc_on_focussed_improving aux_focussed_has nsubj_focussed_Much nn_alignment_word prep_in_work_alignment amod_work_recent det_work_the prep_of_Much_work ccomp_``_focussed
D07-1006	P06-1097	p	We compare semisupervised LEAF with a previous state of the art semi-supervised system -LRB- Fraser and Marcu 2006b -RRB-	dep_Fraser_2006b conj_and_Fraser_Marcu amod_system_semi-supervised nn_system_art det_system_the dep_state_Marcu dep_state_Fraser prep_of_state_system amod_state_previous det_state_a amod_LEAF_semisupervised prep_with_compare_state dobj_compare_LEAF nsubj_compare_We
D07-1006	P06-1097	o	We ran the baseline semisupervised system for two iterations -LRB- line 2 -RRB- and in contrast with -LRB- Fraser and Marcu 2006b -RRB- we found that the best symmetrization heuristic for this system was union which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3	num_Section_3 prep_of_end_Section det_end_the prep_at_discussed_end auxpass_discussed_was nsubjpass_discussed_which rcmod_alignments_discussed amod_alignments_linked advmod_linked_fully prep_of_use_alignments poss_use_our prep_due_to_likely_use advmod_likely_most cop_likely_is nsubj_likely_which rcmod_union_likely cop_union_was nsubj_union_heuristic mark_union_that det_system_this prep_for_heuristic_system nn_heuristic_symmetrization amod_heuristic_best det_heuristic_the ccomp_found_union nsubj_found_we dep_found_Marcu dep_found_Fraser mark_found_with dep_Fraser_2006b conj_and_Fraser_Marcu vmod_contrast_found pobj_in_contrast num_line_2 conj_and_iterations_in appos_iterations_line num_iterations_two amod_system_semisupervised nn_system_baseline det_system_the prep_for_ran_in prep_for_ran_iterations dobj_ran_system nsubj_ran_We
D07-1006	P06-1097	o	-LRB- Och and Ney 2003 -RRB- invented heuristic symmetriza57 FRENCH/ENGLISH ARABIC/ENGLISH SYSTEM F-MEASURE -LRB- = 0.4 -RRB- BLEU F-MEASURE -LRB- = 0.1 -RRB- BLEU GIZA + + 73.5 30.63 75.8 51.55 -LRB- FRASER AND MARCU 2006B -RRB- 74.1 31.40 79.1 52.89 LEAF UNSUPERVISED 74.5 72.3 LEAF SEMI-SUPERVISED 76.3 31.86 84.5 54.34 Table 3 Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment this was extended in -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_in_Koehn prep_extended_in auxpass_extended_was nsubjpass_extended_this amod_alignment_M-to-N det_alignment_a prep_in_resulting_alignment vmod_model_resulting nn_model_M-to-1 det_model_a conj_and_model_model amod_model_1-to-N det_model_a prep_of_output_model prep_of_output_model det_output_the prep_of_tion_output dep_Results_extended dep_Results_tion amod_Results_Experimental num_Table_3 num_Table_54.34 num_Table_84.5 appos_31.86_Table dep_76.3_31.86 dep_SEMI-SUPERVISED_76.3 dep_LEAF_SEMI-SUPERVISED dep_72.3_LEAF number_72.3_74.5 dep_UNSUPERVISED_72.3 amod_LEAF_UNSUPERVISED num_LEAF_52.89 dep_79.1_LEAF dep_79.1_31.40 number_31.40_74.1 appos_FRASER_2006B conj_and_FRASER_MARCU dep_51.55_79.1 dep_51.55_MARCU dep_51.55_FRASER dep_51.55_75.8 cc_51.55_+ dep_75.8_30.63 number_30.63_73.5 dep_GIZA_Results conj_+_GIZA_51.55 nn_GIZA_BLEU dep_=_51.55 dep_=_GIZA dep_=_0.1 nn_F-MEASURE_BLEU dep_=_0.4 dep_F-MEASURE_F-MEASURE dep_F-MEASURE_= nn_F-MEASURE_SYSTEM nn_F-MEASURE_ARABIC/ENGLISH nn_F-MEASURE_FRENCH/ENGLISH nn_F-MEASURE_symmetriza57 nn_F-MEASURE_heuristic dep_invented_= dobj_invented_F-MEASURE nsubj_invented_Ney nsubj_invented_Och amod_Och_2003 conj_and_Och_Ney
D07-1006	P06-1097	o	-LRB- Fraser and Marcu 2006b -RRB- described symmetrized training of a 1-toN log-linear model and a M-to-1 log-linear model	amod_model_log-linear nn_model_M-to-1 det_model_a conj_and_model_model amod_model_log-linear amod_model_1-toN det_model_a prep_of_training_model prep_of_training_model amod_training_symmetrized dobj_described_training nsubj_described_Marcu nsubj_described_Fraser appos_Fraser_2006b conj_and_Fraser_Marcu
D07-1006	P06-1097	o	We use the semi-supervised EMD algorithm -LRB- Fraser and Marcu 2006b -RRB- to train the model	det_model_the dobj_train_model aux_train_to dep_Fraser_2006b conj_and_Fraser_Marcu appos_algorithm_Marcu appos_algorithm_Fraser nn_algorithm_EMD amod_algorithm_semi-supervised det_algorithm_the vmod_use_train dobj_use_algorithm nsubj_use_We
D07-1006	P06-1097	o	We then perform the D-step following -LRB- Fraser and A B C D d110d110d110d110d110 d110d110d110d110d110 d110d110d110d110 E d64d64d64 d64d64d64 d64 d126d126d126 d126d126d126 d126 A B C D d110d110d110d110d110 d110d110d110d110d110 d110d110d110d110 E d64d64d64 d64d64d64 d64 d126d126d126 d126d126d126 d126 Figure 2 Two alignments with the same translational correspondence Marcu 2006b -RRB-	appos_Marcu_2006b dep_correspondence_Marcu amod_correspondence_translational amod_correspondence_same det_correspondence_the prep_with_alignments_correspondence num_alignments_Two dep_:_alignments num_Figure_d126 nn_Figure_d126d126d126 nn_Figure_d126d126d126 nn_Figure_d64 nn_Figure_d64d64d64 nn_Figure_d64d64d64 nn_Figure_E nn_Figure_d110d110d110d110 nn_Figure_d110d110d110d110d110 nn_Figure_d110d110d110d110d110 nn_Figure_D nn_Figure_C nn_Figure_B nn_Figure_A nn_Figure_d126 nn_Figure_d126d126d126 nn_Figure_d126d126d126 nn_Figure_d64 nn_Figure_d64d64d64 nn_Figure_d64d64d64 nn_Figure_E nn_Figure_d110d110d110d110 nn_Figure_d110d110d110d110d110 nn_Figure_d110d110d110d110d110 nn_Figure_D nn_Figure_C nn_Figure_B nn_Figure_A num_Fraser_2 conj_and_Fraser_Figure dep_following_Figure dep_following_Fraser amod_following_D-step det_following_the dobj_perform_following advmod_perform_then nsubj_perform_We ccomp_``_perform
D07-1006	P06-1097	o	-LRB- Fraser and Marcu 2006a -RRB- established that it is important to tune -LRB- the trade-off between Precision and Recall -RRB- to maximize performance	dobj_maximize_performance aux_maximize_to conj_and_Precision_Recall prep_between_trade-off_Recall prep_between_trade-off_Precision det_trade-off_the xcomp_important_maximize dep_important_trade-off prep_to_important_tune cop_important_is nsubj_important_it mark_important_that ccomp_established_important nsubj_established_Marcu nsubj_established_Fraser appos_Fraser_2006a conj_and_Fraser_Marcu
D07-1038	P06-1097	o	For an alignment model most of these use the Aachen HMM approach -LRB- Vogel et al. 1996 -RRB- the implementation of IBM Model 4 in GIZA + + -LRB- Och and Ney 2000 -RRB- or more recently the semi-supervised EMD algorithm -LRB- Fraser and Marcu 2006 -RRB-	amod_Fraser_2006 conj_and_Fraser_Marcu dep_algorithm_Marcu dep_algorithm_Fraser nn_algorithm_EMD amod_algorithm_semi-supervised det_algorithm_the advmod_recently_more num_Och_2000 conj_and_Och_Ney pobj_+_Ney pobj_+_Och appos_GIZA_algorithm advmod_GIZA_recently conj_+_GIZA_+ num_Model_4 nn_Model_IBM prep_in_implementation_+ prep_in_implementation_GIZA prep_of_implementation_Model det_implementation_the amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et conj_approach_implementation dep_approach_Vogel nn_approach_HMM nn_approach_Aachen det_approach_the dep_approach_most prep_for_approach_model det_use_these prep_of_most_use nn_model_alignment det_model_an
D07-1038	P06-1097	o	If human-aligned data is available the EMD algorithm provides higher baseline alignments than GIZA + + that have led to better MT performance -LRB- Fraser and Marcu 2006 -RRB-	dep_Fraser_2006 conj_and_Fraser_Marcu appos_performance_Marcu appos_performance_Fraser nn_performance_MT amod_performance_better aux_led_have nsubj_led_that cc_led_+ conj_+_GIZA_led prep_than_alignments_led prep_than_alignments_GIZA nn_alignments_baseline amod_alignments_higher prep_to_provides_performance dobj_provides_alignments nsubj_provides_algorithm advcl_provides_available nn_algorithm_EMD det_algorithm_the cop_available_is nsubj_available_data mark_available_If amod_data_human-aligned
D07-1038	P06-1097	o	We follow the approach of bootstrapping from a model with a narrower parameter space as is done in e.g. Och and Ney -LRB- 2000 -RRB- and Fraser and Marcu -LRB- 2006 -RRB-	appos_Marcu_2006 dep_Ney_2000 conj_and_Och_Marcu conj_and_Och_Fraser conj_and_Och_Ney pobj_e.g._Marcu pobj_e.g._Fraser pobj_e.g._Ney pobj_e.g._Och conj_in_e.g. prep_done_in auxpass_done_is mark_done_as nn_space_parameter amod_space_narrower det_space_a prep_with_model_space det_model_a advcl_bootstrapping_done prep_from_bootstrapping_model prepc_of_approach_bootstrapping det_approach_the dobj_follow_approach nsubj_follow_We
D07-1079	P06-1097	o	A superset of the parallel data was word aligned by GIZA union -LRB- Och and Ney 2003 -RRB- and EMD -LRB- Fraser and Marcu 2006 -RRB-	amod_Fraser_2006 conj_and_Fraser_Marcu dep_EMD_Marcu dep_EMD_Fraser num_Och_2003 conj_and_Och_Ney conj_and_union_EMD appos_union_Ney appos_union_Och nn_union_GIZA agent_aligned_EMD agent_aligned_union vmod_word_aligned cop_word_was nsubj_word_superset amod_data_parallel det_data_the prep_of_superset_data det_superset_A
J07-3002	P06-1097	o	F-Measure with an appropriate setting of will be useful during the development process of new alignment models or as a maximization criterion for discriminative training of alignment models -LRB- Cherry and Lin 2003 Ayan Dorr and Monz 2005 Ittycheriah and Roukos 2005 Liu Liu and Lin 2005 Fraser and Marcu 2006 Lacoste-Julien et al. 2006 Moore Yih and Bode 2006 -RRB-	num_Bode_2006 conj_Moore_Yih dep_al._2006 nn_al._et nn_al._Lacoste-Julien num_Marcu_2006 conj_and_Fraser_Marcu num_Lin_2005 conj_and_Liu_Lin conj_and_Liu_Liu num_Roukos_2005 conj_and_Ittycheriah_Roukos num_Monz_2005 conj_and_Ayan_Monz conj_and_Ayan_Dorr num_Lin_2003 conj_and_Cherry_Bode dep_Cherry_Moore dep_Cherry_al. dep_Cherry_Marcu dep_Cherry_Fraser conj_and_Cherry_Lin conj_and_Cherry_Liu conj_and_Cherry_Liu conj_and_Cherry_Roukos conj_and_Cherry_Ittycheriah conj_and_Cherry_Monz conj_and_Cherry_Dorr conj_and_Cherry_Ayan conj_and_Cherry_Lin appos_models_Bode appos_models_Liu appos_models_Ittycheriah appos_models_Ayan appos_models_Lin appos_models_Cherry nn_models_alignment prep_of_training_models amod_training_discriminative prep_for_criterion_training nn_criterion_maximization det_criterion_a pobj_as_criterion conj_or_models_as nn_models_alignment amod_models_new prep_of_process_as prep_of_process_models nn_process_development det_process_the prep_during_useful_process cop_useful_be aux_useful_will mark_useful_of dep_setting_useful amod_setting_appropriate det_setting_an prep_with_F-Measure_setting
N07-2007	P06-1097	p	2 Related Work Recently several successful attempts have been made at using supervised machine learning for word alignment -LRB- Liu et al. 2005 Taskar et al. 2005 Ittycheriah and Roukos 2005 Fraser and Marcu 2006 -RRB-	amod_Fraser_2006 conj_and_Fraser_Marcu conj_and_Ittycheriah_Roukos num_Taskar_2005 nn_Taskar_al. nn_Taskar_et dep_Liu_Marcu dep_Liu_Fraser dep_Liu_2005 dep_Liu_Roukos dep_Liu_Ittycheriah dep_Liu_Taskar appos_Liu_2005 dep_Liu_al. nn_Liu_et nn_alignment_word prep_for_learning_alignment vmod_machine_learning amod_machine_supervised dobj_using_machine dep_made_Liu prepc_at_made_using auxpass_made_been aux_made_have nsubjpass_made_attempts ccomp_made_Work amod_attempts_successful amod_attempts_several advmod_Work_Recently amod_Work_Related num_Work_2
N07-2007	P06-1097	o	With the exception of Fraser and Marcu -LRB- 2006 -RRB- these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features	nn_predictions_model nn_predictions_IBM prep_as_integrate_features dobj_integrate_predictions nsubj_integrate_they mark_integrate_that amod_models_generative det_models_the prepc_in_discard_integrate dobj_discard_models advmod_discard_entirely neg_discard_not aux_discard_do nsubj_discard_publications prep_with_discard_exception amod_publications_previous det_publications_these appos_Marcu_2006 conj_and_Fraser_Marcu prep_of_exception_Marcu prep_of_exception_Fraser det_exception_the
N07-2022	P06-1097	o	85 Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units -LRB- Fraser and Marcu 2006 Ayan and Dorr 2006 -RRB-	appos_Ayan_2006 conj_and_Ayan_Dorr dep_Fraser_Dorr dep_Fraser_Ayan conj_and_Fraser_2006 conj_and_Fraser_Marcu dep_units_2006 dep_units_Marcu dep_units_Fraser nn_units_translation dobj_extract_units aux_extract_to xcomp_used_extract auxpass_used_are nsubjpass_used_alignments advmod_used_when det_alignments_the advcl_informative_used advmod_informative_more cop_informative_are nsubj_informative_which ccomp_proposed_informative auxpass_proposed_been aux_proposed_have nsubjpass_proposed_metrics nn_metrics_evaluation nn_metrics_alignment det_metrics_some advmod_metrics_Recently num_metrics_85
P07-1001	P06-1097	o	It has been shown that human knowledge in the form of a small amount of manually annotated parallel data to be used to seed or guide model training can significantly improve word alignment F-measure and translation performance -LRB- Ittycheriah and Roukos 2005 Fraser and Marcu 2006 -RRB-	dep_Ittycheriah_2006 conj_and_Ittycheriah_Marcu conj_and_Ittycheriah_Fraser conj_and_Ittycheriah_2005 conj_and_Ittycheriah_Roukos nn_performance_translation appos_F-measure_Marcu appos_F-measure_Fraser appos_F-measure_2005 appos_F-measure_Roukos appos_F-measure_Ittycheriah conj_and_F-measure_performance nn_F-measure_alignment nn_F-measure_word dobj_improve_performance dobj_improve_F-measure advmod_improve_significantly aux_improve_can nsubj_improve_knowledge mark_improve_that nn_training_model nn_training_guide conj_or_seed_training prep_to_used_training prep_to_used_seed auxpass_used_be aux_used_to vmod_data_used nn_data_parallel amod_data_annotated advmod_annotated_manually prep_of_amount_data amod_amount_small det_amount_a prep_of_form_amount det_form_the prep_in_knowledge_form amod_knowledge_human ccomp_shown_improve auxpass_shown_been aux_shown_has nsubjpass_shown_It
P07-1004	P06-1097	o	Along similar lines -LRB- Fraser and Marcu 2006 -RRB- combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences	amod_sentences_aligned nn_sentences_hand prep_of_set_sentences amod_set_small det_set_a prep_on_trained_set vmod_model_trained amod_model_discriminative amod_model_log-linear det_model_a nn_alignment_word prep_of_model_alignment amod_model_generative det_model_a prep_with_combine_model dobj_combine_model nsubj_combine_Marcu nsubj_combine_Fraser prep_along_combine_lines amod_Fraser_2006 conj_and_Fraser_Marcu amod_lines_similar
P08-4006	P06-1097	o	Consequently considerable effort has gone into devising and improving automatic word alignment algorithms and into evaluating their performance -LRB- e.g. Och and Ney 2003 Taskar et al. 2005 Moore et al. 2006 Fraser and Marcu 2006 among many others -RRB-	amod_others_many prep_among_Fraser_others amod_Fraser_2006 conj_and_Fraser_Marcu num_Moore_2006 nn_Moore_al. nn_Moore_et num_Taskar_2005 nn_Taskar_al. nn_Taskar_et dep_Och_Marcu dep_Och_Fraser conj_and_Och_Moore conj_and_Och_Taskar conj_and_Och_2003 conj_and_Och_Ney dep_e.g._Moore dep_e.g._Taskar dep_e.g._2003 dep_e.g._Ney dep_e.g._Och ccomp_-LRB-_e.g. poss_performance_their dobj_evaluating_performance pcomp_into_evaluating nn_algorithms_alignment nn_algorithms_word amod_algorithms_automatic dobj_devising_algorithms conj_and_devising_improving conj_and_gone_into prepc_into_gone_improving prepc_into_gone_devising aux_gone_has nsubj_gone_effort advmod_gone_Consequently amod_effort_considerable
W07-0403	P06-1097	o	Method Prec Rec F-measure GIZA + + Intersect 96.7 53.0 68.5 GIZA + + Union 82.5 69.0 75.1 GIZA + + GDF 84.0 68.2 75.2 Phrasal ITG 50.7 80.3 62.2 Phrasal ITG + NCC 75.4 78.0 76.7 Following the lead of -LRB- Fraser and Marcu 2006 -RRB- we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines -LRB- Melamed 1998 -RRB-	amod_Melamed_1998 dep_guidelines_Melamed nn_guidelines_annotation nn_guidelines_Blinker det_guidelines_the pobj_set_guidelines prepc_according_to_set_to poss_training_our prep_of_pairs_training nn_pairs_sentence num_pairs_100 amod_pairs_first det_pairs_the vmod_hand-aligned_set dobj_hand-aligned_pairs nsubj_hand-aligned_we rcmod_Fraser_hand-aligned amod_Fraser_2006 conj_and_Fraser_Marcu prep_of_lead_Marcu prep_of_lead_Fraser det_lead_the prep_following_76.7_lead num_76.7_78.0 number_78.0_75.4 dep_NCC_76.7 conj_+_ITG_NCC nn_ITG_Phrasal num_ITG_62.2 num_ITG_80.3 number_80.3_50.7 dep_ITG_NCC dep_ITG_ITG dep_Phrasal_ITG dep_75.2_Phrasal dep_68.2_75.2 number_68.2_84.0 amod_GDF_68.2 conj_+_+_GDF num_GIZA_75.1 number_75.1_69.0 number_75.1_82.5 dep_75.1_Union cc_75.1_+ conj_+_GIZA_GIZA num_GIZA_68.5 nn_GIZA_Intersect number_68.5_53.0 number_68.5_96.7 pobj_+_GIZA pobj_+_GIZA dep_GIZA_GDF dep_GIZA_+ conj_+_GIZA_+ nn_GIZA_F-measure nn_GIZA_Rec nn_GIZA_Prec nn_GIZA_Method
W07-0407	P06-1097	o	-LRB- Fraser and Marcu 2006 -RRB- have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models	nn_models_IBM prep_in_used_models vmod_algorithm_used nn_algorithm_Expectation-Maximization amod_algorithm_traditional det_algorithm_the prep_of_iteration_algorithm det_iteration_every prep_at_step_iteration amod_step_discriminative det_step_a dobj_applies_step nsubj_applies_which rcmod_alignment_applies nn_alignment_word dobj_doing_alignment prepc_for_algorithm_doing det_algorithm_an dobj_proposed_algorithm aux_proposed_have nsubj_proposed_Marcu nsubj_proposed_Fraser amod_Fraser_2006 conj_and_Fraser_Marcu
W07-1520	P06-1097	o	Because of its central role in building machine translation systems and because of the complexity of the task sub-sentential alignment of parallel corpora continues to be an active area of research -LRB- e.g. Moore et al. 2006 Fraser and Marcu 2006 -RRB- and this implies a continuing demand for manually created or human-verified gold standard alignments for development and evaluation purposes	nn_purposes_evaluation nn_purposes_development conj_and_development_evaluation amod_alignments_standard amod_alignments_gold amod_alignments_human-verified prep_for_created_purposes conj_or_created_alignments advmod_created_manually prepc_for_demand_alignments prepc_for_demand_created amod_demand_continuing det_demand_a dobj_implies_demand nsubj_implies_this num_Fraser_2006 conj_and_Fraser_Marcu num_Moore_2006 nn_Moore_al. nn_Moore_et dep_e.g._Marcu dep_e.g._Fraser conj_e.g._Moore dep_area_e.g. prep_of_area_research amod_area_active det_area_an cop_area_be aux_area_to conj_and_continues_implies xcomp_continues_area nsubj_continues_alignment prep_of_continues_complexity mwe_continues_because prep_of_continues_role mwe_continues_Because amod_corpora_parallel prep_of_alignment_corpora amod_alignment_sub-sentential det_task_the prep_of_complexity_task det_complexity_the nn_systems_translation nn_systems_machine nn_systems_building conj_and_role_complexity prep_in_role_systems amod_role_central poss_role_its
W09-0421	P06-1097	o	5 Augmenting the corpus with an extracted dictionary Previous research -LRB- Callison-Burch et al. 2004 Fraser and Marcu 2006 -RRB- has shown that including word aligned data during training can improve translation results	nn_results_translation dobj_improve_results aux_improve_can prep_including_improve_data nsubj_improve_that prep_during_data_training amod_data_aligned nn_data_word ccomp_shown_improve aux_shown_has nsubj_shown_5 num_Fraser_2006 conj_and_Fraser_Marcu dep_al._Marcu dep_al._Fraser num_al._2004 nn_al._et amod_al._Callison-Burch amod_research_Previous nn_research_dictionary amod_research_extracted det_research_an det_corpus_the dep_Augmenting_al. prep_with_Augmenting_research dobj_Augmenting_corpus vmod_5_Augmenting ccomp_``_shown
W09-1804	P06-1097	o	EMD training -LRB- Fraser and Marcu 2006 -RRB- combines generative and discriminative elements	amod_elements_discriminative amod_elements_generative conj_and_generative_discriminative dobj_combines_elements nsubj_combines_training dep_Fraser_2006 conj_and_Fraser_Marcu appos_training_Marcu appos_training_Fraser nn_training_EMD
D08-1056	P06-1101	o	-LRB- Snow et al. 2006 Nakov & Hearst 2008 -RRB-	appos_Nakov_2008 conj_and_Nakov_Hearst dep_Snow_Hearst dep_Snow_Nakov appos_Snow_2006 dep_Snow_al. nn_Snow_et dep_''_Snow
D09-1089	P06-1101	o	Due to the importance of WN for NLP tasks substantial research was done on direct or indirect automated extension of the English WN -LRB- e.g. -LRB- Snow et al. 2006 -RRB- -RRB- or WN in other languages -LRB- e.g. -LRB- Vintar and Fiser 2008 -RRB- -RRB-	dep_Vintar_2008 conj_and_Vintar_Fiser appos_e.g._Fiser appos_e.g._Vintar dep_languages_e.g. amod_languages_other prep_in_WN_languages amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_e.g._Snow conj_or_WN_WN dep_WN_e.g. nn_WN_English det_WN_the prep_of_extension_WN prep_of_extension_WN amod_extension_automated amod_extension_indirect amod_extension_direct conj_or_direct_indirect prep_on_done_extension auxpass_done_was nsubjpass_done_research prep_due_to_done_importance amod_research_substantial nn_tasks_NLP prep_for_importance_tasks prep_of_importance_WN det_importance_the
D09-1089	P06-1101	o	The majority of this research was done on extending the tree structure -LRB- finding new synsets -LRB- Snow et al. 2006 -RRB- or enriching WN with new relationships -LRB- Cuadros and Rigau 2008 -RRB- -RRB- rather than improving the quality of existing concept/synset nodes	nn_nodes_concept/synset amod_nodes_existing prep_of_quality_nodes det_quality_the dobj_improving_quality dep_Cuadros_2008 conj_and_Cuadros_Rigau dep_relationships_Rigau dep_relationships_Cuadros amod_relationships_new amod_Snow_2006 dep_Snow_al. nn_Snow_et amod_synsets_new prep_with_finding_relationships dobj_finding_WN conj_or_finding_enriching dep_finding_Snow dobj_finding_synsets nn_structure_tree det_structure_the conj_negcc_extending_improving dep_extending_enriching dep_extending_finding dobj_extending_structure prepc_on_done_improving prepc_on_done_extending auxpass_done_was nsubjpass_done_majority det_research_this prep_of_majority_research det_majority_The
D09-1156	P06-1101	o	Although some early systems for web-page analysis induce rules at character-level -LRB- e.g. such as WIEN -LRB- Kushmerick et al. 1997 -RRB- and DIPRE -LRB- Brin 1998 -RRB- -RRB- most recent approaches for set expansion have used either tokenized and/or parsed free-text -LRB- Carlson et al. 2009 Talukdar et al. 2006 Snow et al. 2006 Pantel and Pennacchiotti 2006 -RRB- or have incorporated heuristics for exploiting HTML structures that are likely to encode lists and tables -LRB- Nadeau et al. 2006 Etzioni et al. 2005 -RRB-	num_Etzioni_2005 nn_Etzioni_al. nn_Etzioni_et dep_Nadeau_Etzioni amod_Nadeau_2006 dep_Nadeau_al. nn_Nadeau_et conj_and_lists_tables dobj_encode_tables dobj_encode_lists aux_encode_to xcomp_likely_encode cop_likely_are nsubj_likely_that rcmod_structures_likely nn_structures_HTML dobj_exploiting_structures prepc_for_incorporated_exploiting dobj_incorporated_heuristics aux_incorporated_have dep_Pantel_2006 conj_and_Pantel_Pennacchiotti num_Snow_2006 nn_Snow_al. nn_Snow_et num_Talukdar_2006 nn_Talukdar_al. nn_Talukdar_et dep_Carlson_Pennacchiotti dep_Carlson_Pantel dep_Carlson_Snow dep_Carlson_Talukdar amod_Carlson_2009 dep_Carlson_al. nn_Carlson_et dep_free-text_Nadeau conj_or_free-text_incorporated appos_free-text_Carlson amod_free-text_parsed amod_free-text_tokenized conj_and/or_tokenized_parsed preconj_tokenized_either dobj_used_incorporated dobj_used_free-text aux_used_have nsubj_used_approaches advcl_used_induce amod_expansion_set prep_for_approaches_expansion amod_approaches_recent advmod_recent_most dep_Brin_1998 dep_DIPRE_Brin amod_Kushmerick_1997 dep_Kushmerick_al. nn_Kushmerick_et conj_and_WIEN_DIPRE dep_WIEN_Kushmerick prep_such_as_e.g._DIPRE prep_such_as_e.g._WIEN dep_character-level_e.g. prep_at_induce_character-level dobj_induce_rules nsubj_induce_systems mark_induce_Although amod_analysis_web-page prep_for_systems_analysis amod_systems_early det_systems_some
D09-1156	P06-1101	o	1510 5 Related Work In recent years many research has been done on extracting relations from free text -LRB- e.g. -LRB- Pantel and Pennacchiotti 2006 Agichtein and Gravano 2000 Snow et al. 2006 -RRB- -RRB- however almost all of them require some language-dependent parsers or taggers for English which restrict the language of their extractions to English only -LRB- or languages that have these parsers -RRB-	det_parsers_these dobj_have_parsers nsubj_have_that rcmod_languages_have cc_languages_or dep_only_languages poss_extractions_their prep_of_language_extractions det_language_the advmod_restrict_only prep_to_restrict_English dobj_restrict_language nsubj_restrict_which rcmod_English_restrict prep_for_parsers_English conj_or_parsers_taggers amod_parsers_language-dependent det_parsers_some dobj_require_taggers dobj_require_parsers nsubj_require_all advmod_require_however prep_of_all_them advmod_all_almost num_Snow_2006 nn_Snow_al. nn_Snow_et dep_Agichtein_Snow conj_and_Agichtein_2000 conj_and_Agichtein_Gravano dep_Pantel_2000 dep_Pantel_Gravano dep_Pantel_Agichtein conj_and_Pantel_2006 conj_and_Pantel_Pennacchiotti dep_e.g._2006 dep_e.g._Pennacchiotti dep_e.g._Pantel dep_text_e.g. amod_text_free prep_from_relations_text dobj_extracting_relations parataxis_done_require prepc_on_done_extracting auxpass_done_been aux_done_has nsubjpass_done_research dep_done_Work amod_research_many amod_years_recent prep_in_Work_years amod_Work_Related num_Work_5 number_5_1510
E09-1064	P06-1101	o	Beyond WordNet -LRB- Fellbaum 1998 -RRB- a wide range of resources has been developed and utilized including extensions to WordNet -LRB- Moldovan and Rus 2001 Snow et al. 2006 -RRB- and resources based on automatic distributional similarity methods -LRB- Lin 1998 Pantel and Lin 2002 -RRB-	amod_Pantel_2002 conj_and_Pantel_Lin dep_Lin_Lin dep_Lin_Pantel num_Lin_1998 appos_methods_Lin nn_methods_similarity amod_methods_distributional amod_methods_automatic prep_on_based_methods num_Snow_2006 nn_Snow_al. nn_Snow_et conj_and_Moldovan_Snow conj_and_Moldovan_2001 conj_and_Moldovan_Rus conj_and_WordNet_resources dep_WordNet_Snow dep_WordNet_2001 dep_WordNet_Rus dep_WordNet_Moldovan vmod_extensions_based prep_to_extensions_resources prep_to_extensions_WordNet nsubjpass_utilized_WordNet prep_including_developed_extensions conj_and_developed_utilized auxpass_developed_been aux_developed_has nsubjpass_developed_WordNet dep_developed_Beyond prep_of_range_resources amod_range_wide det_range_a amod_Fellbaum_1998 appos_WordNet_range dep_WordNet_Fellbaum
E09-1068	P06-1101	o	Finally methods in the literature more focused on a specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the WordNet taxonomy -LRB- Snow et al. 2006 -RRB- structural approaches based on semantic clusters and distance metrics -LRB- Pennacchiotti and Pantel 2006 -RRB- supervised machine learning methods for the disambiguation of meronymy relations -LRB- Girju et al. 2003 -RRB- etc. 6 Conclusions In this paper we presented a novel approach to disambiguate the glosses of computational lexicons and machine-readable dictionaries with the aim of alleviating the knowledge acquisition bottleneck	nn_bottleneck_acquisition nn_bottleneck_knowledge det_bottleneck_the dobj_alleviating_bottleneck prepc_of_aim_alleviating det_aim_the amod_dictionaries_machine-readable conj_and_lexicons_dictionaries amod_lexicons_computational prep_of_glosses_dictionaries prep_of_glosses_lexicons det_glosses_the dobj_disambiguate_glosses aux_disambiguate_to vmod_approach_disambiguate amod_approach_novel det_approach_a dobj_presented_approach nsubj_presented_we det_paper_this rcmod_Conclusions_presented prep_in_Conclusions_paper num_Conclusions_6 nn_Conclusions_etc. amod_Girju_2003 dep_Girju_al. nn_Girju_et amod_relations_meronymy prep_of_disambiguation_relations det_disambiguation_the dep_methods_Girju prep_for_methods_disambiguation amod_methods_learning nn_methods_machine amod_methods_supervised dep_Pennacchiotti_2006 conj_and_Pennacchiotti_Pantel appos_metrics_Pantel appos_metrics_Pennacchiotti nn_metrics_distance conj_and_clusters_metrics amod_clusters_semantic pobj_approaches_metrics pobj_approaches_clusters prepc_based_on_approaches_on amod_approaches_structural amod_Snow_2006 dep_Snow_al. nn_Snow_et nn_taxonomy_WordNet det_taxonomy_the prep_in_hypernym_taxonomy amod_hypernym_likely det_hypernym_the advmod_likely_most prep_under_attachment_hypernym prep_of_attachment_hyponyms det_attachment_the prep_with_methods_aim conj_methods_Conclusions conj_methods_methods conj_methods_approaches dep_methods_Snow prep_for_methods_attachment amod_methods_statistical dobj_include_methods nsubj_include_methods advmod_include_Finally nn_task_disambiguation amod_task_specific det_task_a prep_on_focused_task advmod_focused_more det_literature_the vmod_methods_focused prep_in_methods_literature
N07-1016	P06-1101	o	Second we follow Snow et al. s work -LRB- 2006 -RRB- on taxonomy induction in incorporating transitive closure constraints in our probability calculations as explained below	advmod_explained_below mark_explained_as nn_calculations_probability poss_calculations_our prep_in_constraints_calculations nn_constraints_closure amod_constraints_transitive dobj_incorporating_constraints nn_induction_taxonomy prep_on_work_induction appos_work_2006 nn_work_s nn_work_al. nn_work_et nn_work_Snow advcl_follow_explained prepc_in_follow_incorporating dobj_follow_work nsubj_follow_we advmod_follow_Second
N07-1017	P06-1101	p	The state of the art technology for relation extraction primarily relies on pattern-based approaches -LRB- Snow et al. 2006 -RRB-	amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_approaches_Snow amod_approaches_pattern-based prep_on_relies_approaches advmod_relies_primarily nsubj_relies_state nn_extraction_relation nn_technology_art det_technology_the prep_for_state_extraction prep_of_state_technology det_state_The
P07-1072	P06-1101	o	Other researchers -LRB- Pantel and Pennacchiotti 2006 -RRB- -LRB- Snow et al. 2006 -RRB- use clustering techniques coupled with syntactic dependency features to identify IS-A relations in large text collections	nn_collections_text amod_collections_large prep_in_relations_collections amod_relations_IS-A dobj_identify_relations aux_identify_to nn_features_dependency amod_features_syntactic xcomp_coupled_identify prep_with_coupled_features vmod_techniques_coupled nn_techniques_clustering dobj_use_techniques nsubj_use_researchers amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_Pantel_2006 conj_and_Pantel_Pennacchiotti appos_researchers_Snow appos_researchers_Pennacchiotti appos_researchers_Pantel amod_researchers_Other
P07-2042	P06-1101	o	Recently Snow Jurafsky and Ng -LRB- 2005 -RRB- generated tens of thousands of hypernym patterns and combined these with noun clusters to generate high-precision suggestions for unknown noun insertion into WordNet -LRB- Snow et al. 2006 -RRB-	amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_WordNet_Snow prep_into_insertion_WordNet nn_insertion_noun amod_insertion_unknown prep_for_suggestions_insertion nn_suggestions_high-precision dobj_generate_suggestions aux_generate_to nn_clusters_noun prep_with_these_clusters vmod_combined_generate dobj_combined_these nn_patterns_hypernym prep_of_thousands_patterns conj_and_tens_combined prep_of_tens_thousands dobj_generated_combined dobj_generated_tens nsubj_generated_Ng nsubj_generated_Jurafsky nsubj_generated_Snow advmod_generated_Recently appos_Ng_2005 conj_and_Snow_Ng conj_and_Snow_Jurafsky
P08-1003	P06-1101	o	4 Related Work 4.1 Acquisition of Classes of Instances Although some researchers focus on re-organizing or extending classes of instances already available explicitly within manually-built resources such as Wikipedia -LRB- Ponzetto and Strube 2007 -RRB- or WordNet -LRB- Snow et al. 2006 -RRB- or both -LRB- Suchanek et al. 2007 -RRB- a large body of previous work focuses on compiling sets of instances not necessarily labeled from unstructured text	amod_text_unstructured prep_from_labeled_text advmod_labeled_necessarily neg_labeled_not prep_of_sets_instances dobj_compiling_sets dep_focuses_labeled prepc_on_focuses_compiling amod_work_previous prep_of_body_work amod_body_large det_body_a amod_Suchanek_2007 dep_Suchanek_al. nn_Suchanek_et dep_both_Suchanek amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_Ponzetto_2007 conj_and_Ponzetto_Strube conj_or_Wikipedia_WordNet dep_Wikipedia_Strube dep_Wikipedia_Ponzetto prep_such_as_resources_WordNet prep_such_as_resources_Wikipedia amod_resources_manually-built prep_within_available_resources advmod_available_explicitly advmod_available_already amod_classes_available prep_of_classes_instances dep_re-organizing_classes conj_or_re-organizing_extending dep_focus_focuses conj_or_focus_body conj_or_focus_both dep_focus_Snow prepc_on_focus_extending prepc_on_focus_re-organizing nsubj_focus_researchers mark_focus_Although det_researchers_some prep_of_Classes_Instances advcl_Acquisition_body advcl_Acquisition_both advcl_Acquisition_focus prep_of_Acquisition_Classes num_Acquisition_4.1 dep_Work_Acquisition amod_Work_Related num_Work_4
P08-1003	P06-1101	o	1 Introduction Current methods for large-scale information extraction take advantage of unstructured text available from either Web documents -LRB- Banko et al. 2007 Snow et al. 2006 -RRB- or more recently logs of Web search queries -LRB- Pasca 2007 -RRB- to acquire useful knowledge with minimal supervision	amod_supervision_minimal amod_knowledge_useful prep_with_acquire_supervision dobj_acquire_knowledge aux_acquire_to amod_Pasca_2007 nn_queries_search nn_queries_Web vmod_logs_acquire dep_logs_Pasca prep_of_logs_queries advmod_recently_more num_Snow_2006 nn_Snow_al. nn_Snow_et dep_Banko_logs advmod_Banko_recently cc_Banko_or dep_Banko_Snow appos_Banko_2007 dep_Banko_al. nn_Banko_et dep_documents_Banko nn_documents_Web preconj_documents_either prep_from_available_documents nsubj_available_advantage amod_text_unstructured prep_of_advantage_text xcomp_take_available nsubj_take_methods nn_extraction_information amod_extraction_large-scale prep_for_methods_extraction amod_methods_Current nn_methods_Introduction num_methods_1 ccomp_``_take
P08-1003	P06-1101	o	-LRB- Ponzetto and Strube 2007 Snow et al. 2006 -RRB- -RRB- can be summarized as -LSB- -RSB- C -LSB- such as | including -RSB- I -LSB- and | | -RSB- where I is a potential instance -LRB- e.g. Venezuelan equine encephalitis -RRB- and C is a potential class label for the instance -LRB- e.g. zoonotic diseases -RRB- for example in the sentence The expansion of the farms increased the spread of zoonotic diseases such as Venezuelan equine encephalitis -LSB- -RSB-	amod_encephalitis_equine amod_encephalitis_Venezuelan prep_such_as_diseases_encephalitis amod_diseases_zoonotic prep_of_spread_diseases det_spread_the dobj_increased_spread nsubj_increased_expansion det_farms_the prep_of_expansion_farms det_expansion_The det_sentence_the dep_in_increased pobj_in_sentence amod_diseases_zoonotic dep_diseases_e.g. dep_instance_diseases det_instance_the prep_for_label_instance nn_label_class amod_label_potential det_label_a cop_label_is nsubj_label_I nn_encephalitis_equine amod_encephalitis_Venezuelan advmod_encephalitis_e.g. conj_and_instance_C dep_instance_encephalitis amod_instance_potential det_instance_a cop_instance_is nsubj_instance_I advmod_instance_where dep_|_| cc_|_and rcmod_I_C rcmod_I_instance dep_I_| rcmod_|_label prep_|_including dep_C_in prep_for_C_example prep_such_as_C_| prep_as_summarized_C auxpass_summarized_be aux_summarized_can dep_summarized_Strube dep_summarized_Ponzetto num_Snow_2006 nn_Snow_al. nn_Snow_et dep_Ponzetto_Snow dep_Ponzetto_2007 conj_and_Ponzetto_Strube
P08-1027	P06-1101	o	Since -LRB- Hearst 1992 -RRB- numerous works have used patterns for discovery and identification of instances of semantic relationships -LRB- e.g. -LRB- Girju et al. 2006 Snow et al. 2006 Banko et al 2007 -RRB- -RRB-	appos_al_2007 nn_al_et nn_al_Banko dep_Snow_al num_Snow_2006 nn_Snow_al. nn_Snow_et dep_Girju_Snow dep_Girju_2006 dep_Girju_al. nn_Girju_et dep_,_Girju dep_-LRB-_e.g. amod_relationships_semantic prep_of_instances_relationships prep_of_discovery_instances conj_and_discovery_identification prep_for_patterns_identification prep_for_patterns_discovery dobj_used_patterns aux_used_have nsubj_used_works prep_used_Since amod_works_numerous amod_Hearst_1992 dep_Since_Hearst
P08-1048	P06-1101	o	Some work has been done on adding new terms and relations to WordNet -LRB- Snow et al. 2006 -RRB- and FACTOTUM -LRB- OHara and Wiebe 2003 -RRB-	dep_OHara_2003 conj_and_OHara_Wiebe dep_FACTOTUM_Wiebe dep_FACTOTUM_OHara amod_Snow_2006 dep_Snow_al. nn_Snow_et conj_and_WordNet_FACTOTUM dep_WordNet_Snow prep_to_relations_FACTOTUM prep_to_relations_WordNet conj_and_terms_relations amod_terms_new dobj_adding_relations dobj_adding_terms prepc_on_done_adding auxpass_done_been aux_done_has nsubjpass_done_work det_work_Some ccomp_``_done
P08-1079	P06-1101	o	2.1 Relationship Types There is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as WordNet including hypernymy -LRB- Hearst 1992 Pantel et al. 2004 Snow et al. 2006 -RRB- synonymy -LRB- Davidov and Rappoport 2006 Widdows and Dorow 2002 -RRB- and meronymy -LRB- Berland and Charniak 1999 Girju et al. 2006 -RRB-	num_Girju_2006 nn_Girju_al. nn_Girju_et dep_Berland_Girju conj_and_Berland_1999 conj_and_Berland_Charniak dep_meronymy_1999 dep_meronymy_Charniak dep_meronymy_Berland dep_Widdows_2002 conj_and_Widdows_Dorow dep_Davidov_Dorow dep_Davidov_Widdows conj_and_Davidov_2006 conj_and_Davidov_Rappoport appos_synonymy_2006 appos_synonymy_Rappoport appos_synonymy_Davidov num_Snow_2006 nn_Snow_al. nn_Snow_et num_Pantel_2004 nn_Pantel_al. nn_Pantel_et dep_Hearst_Snow dep_Hearst_Pantel amod_Hearst_1992 conj_and_hypernymy_meronymy conj_and_hypernymy_synonymy appos_hypernymy_Hearst prep_including_WordNet_meronymy prep_including_WordNet_synonymy prep_including_WordNet_hypernymy prep_such_as_resources_WordNet amod_resources_useful prep_in_represented_resources vmod_types_represented nn_types_relationship amod_types_basic prep_of_discovery_types prep_with_deals_discovery nsubj_deals_that rcmod_work_deals amod_work_related prep_of_body_work amod_body_large det_body_a nsubj_is_body expl_is_There ccomp_Types_is nsubj_Types_Relationship num_Relationship_2.1 ccomp_``_Types
P09-1031	P06-1101	o	To have a fair comparison for PR we estimate the conditional probability of a relation given the evidence P -LRB- Rij | Eij -RRB- as in -LRB- Snow et al. 2006 -RRB- by using the same set of features as in ME Table 3 shows precision recall and F1measure of each system for WordNet hypernyms -LRB- is-a -RRB- WordNet meronyms -LRB- part-of -RRB- and ODP hypernyms -LRB- is-a -RRB-	appos_hypernyms_is-a nn_hypernyms_ODP appos_meronyms_part-of amod_meronyms_WordNet conj_and_hypernyms_hypernyms conj_and_hypernyms_meronyms appos_hypernyms_is-a nn_hypernyms_WordNet prep_for_system_hypernyms prep_for_system_meronyms prep_for_system_hypernyms det_system_each conj_and_precision_F1measure conj_and_precision_recall prep_of_shows_system dep_shows_F1measure dep_shows_recall dep_shows_precision num_shows_3 dep_Table_shows pobj_in_ME pcomp_as_in prep_set_as prep_of_set_features amod_set_same det_set_the dobj_using_set nn_al._et dep_Snow_Table prepc_by_Snow_using dep_Snow_2006 advmod_Snow_al. pobj_in_Snow pcomp_as_in num_Eij_| nn_Eij_Rij appos_P_Eij prep_evidence_as dep_evidence_P det_evidence_the pobj_given_evidence prep_relation_given det_relation_a prep_of_probability_relation amod_probability_conditional det_probability_the dobj_estimate_probability nsubj_estimate_we prep_for_estimate_PR advcl_estimate_have amod_comparison_fair det_comparison_a dobj_have_comparison aux_have_To
P09-1031	P06-1101	o	We compare system performance between -LRB- Snow et al. 2006 -RRB- and our framework in Section 5	num_Section_5 prep_in_framework_Section poss_framework_our conj_and_Snow_framework amod_Snow_2006 dep_Snow_al. nn_Snow_et prep_between_performance_framework prep_between_performance_Snow nn_performance_system dobj_compare_performance nsubj_compare_We
P09-1050	P06-1101	o	5.3 -LRB- Snow et al. 2006 -RRB- Snow -LRB- Snow et al. 2006 -RRB- has extended the WordNet 2.1 by adding thousands of entries -LRB- synsets -RRB- at a relatively high precision	amod_precision_high det_precision_a advmod_high_relatively appos_entries_synsets prep_of_thousands_entries prep_at_adding_precision dobj_adding_thousands num_WordNet_2.1 det_WordNet_the prepc_by_extended_adding dobj_extended_WordNet aux_extended_has nsubj_extended_Snow amod_Snow_2006 dep_Snow_al. nn_Snow_et appos_Snow_Snow appos_Snow_Snow num_Snow_5.3 amod_Snow_2006 dep_Snow_al. nn_Snow_et ccomp_``_extended
P09-1050	P06-1101	n	We have also illustrated that ASIA outperforms three other English systems -LRB- Kozareva et al. 2008 Pasca 2007b Snow et al. 2006 -RRB- even though many of these use more input than just a semantic class name	nn_name_class amod_name_semantic det_name_a advmod_name_just prep_than_input_name advmod_input_more amod_input_many det_use_these prep_of_many_use prep_though_even_input num_Snow_2006 nn_Snow_al. nn_Snow_et advmod_Pasca_even conj_Pasca_Snow conj_Pasca_2007b dep_Kozareva_Pasca appos_Kozareva_2008 dep_Kozareva_al. nn_Kozareva_et dep_systems_Kozareva amod_systems_English amod_systems_other num_systems_three dobj_outperforms_systems nsubj_outperforms_ASIA mark_outperforms_that ccomp_illustrated_outperforms advmod_illustrated_also aux_illustrated_have nsubj_illustrated_We
P09-1050	P06-1101	n	We also compare ASIA on twelve additional benchmarks to the extended Wordnet 2.1 produced by Snow et al -LRB- Snow et al. 2006 -RRB- and show that for these twelve sets ASIA produces more than five times as many set instances with much higher precision -LRB- 98 % versus 70 % -RRB-	num_%_70 conj_versus_%_% num_%_98 dep_precision_% dep_precision_% amod_precision_higher advmod_higher_much prep_with_set_precision dobj_set_instances dep_times_set quantmod_times_many quantmod_times_as number_times_five quantmod_times_than mwe_than_more dobj_produces_times nsubj_produces_ASIA prep_for_produces_sets mark_produces_that num_sets_twelve det_sets_these ccomp_show_produces amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_Snow_Snow dep_Snow_al nn_Snow_et agent_produced_Snow vmod_Wordnet_produced num_Wordnet_2.1 conj_and_extended_show dep_extended_Wordnet amod_the_show amod_the_extended amod_benchmarks_additional num_benchmarks_twelve prep_to_compare_the prep_on_compare_benchmarks dobj_compare_ASIA advmod_compare_also nsubj_compare_We ccomp_``_compare
P09-1050	P06-1101	o	Snow etal -LRB- Snow et al. 2006 -RRB- use known hypernym/hyponym pairs to generate training data for a machine-learning system which then learns many lexico-syntactic patterns	amod_patterns_lexico-syntactic amod_patterns_many dobj_learns_patterns advmod_learns_then nsubj_learns_which rcmod_system_learns nn_system_machine-learning det_system_a prep_for_data_system nn_data_training dobj_generate_data aux_generate_to nn_pairs_hypernym/hyponym amod_pairs_known vmod_use_generate dobj_use_pairs nsubj_use_etal amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_etal_Snow rcmod_Snow_use
P09-1051	P06-1101	o	4.3 Scoring All-N Rules We observed that the likelihood of nouns mentioned in a definition to be referred by the concept title depends greatly on the syntactic path connecting them -LRB- which was exploited also in -LRB- Snow et al. 2006 -RRB- -RRB-	amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_in_Snow prep_exploited_in advmod_exploited_also auxpass_exploited_was nsubjpass_exploited_which rcmod_them_exploited dobj_connecting_them vmod_path_connecting amod_path_syntactic det_path_the prep_on_depends_path advmod_depends_greatly nsubj_depends_likelihood mark_depends_that nn_title_concept det_title_the agent_referred_title auxpass_referred_be aux_referred_to vmod_definition_referred det_definition_a prep_in_mentioned_definition vmod_nouns_mentioned prep_of_likelihood_nouns det_likelihood_the ccomp_observed_depends nsubj_observed_We rcmod_Rules_observed nn_Rules_All-N nn_Rules_Scoring num_Rules_4.3 dep_``_Rules
P09-1051	P06-1101	o	An extension to WordNet was presented by -LRB- Snow et al. 2006 -RRB-	amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_by_Snow prep_presented_by auxpass_presented_was nsubjpass_presented_extension prep_to_extension_WordNet det_extension_An
P09-1051	P06-1101	o	4.1 Judging Rule Correctness Following the spirit of the fine-grained human evaluation in -LRB- Snow et al. 2006 -RRB- we randomly sampled 800 rules from our rule-base and presented them to an annotator who judged them for correctness according to the lexical reference notion specified above	advmod_specified_above vmod_notion_specified nn_notion_reference amod_notion_lexical det_notion_the prep_for_judged_correctness dobj_judged_them nsubj_judged_who rcmod_annotator_judged det_annotator_an prep_to_presented_annotator dobj_presented_them nsubj_presented_we poss_rule-base_our num_rules_800 conj_and_sampled_presented prep_from_sampled_rule-base dobj_sampled_rules advmod_sampled_randomly nsubj_sampled_we rcmod_Snow_presented rcmod_Snow_sampled amod_Snow_2006 dep_Snow_al. nn_Snow_et prep_in_evaluation_Snow amod_evaluation_human amod_evaluation_fine-grained det_evaluation_the prep_of_spirit_evaluation det_spirit_the pobj_Following_notion prepc_according_to_Following_to dobj_Following_spirit vmod_Correctness_Following nn_Correctness_Rule amod_Correctness_Judging num_Correctness_4.1 dep_``_Correctness
P09-1070	P06-1101	o	6 Related Work A large body of previous work exists on extending WORDNET with additional concepts and instances -LRB- Snow et al. 2006 Suchanek et al. 2007 -RRB- these methods do not address attributes directly	advmod_address_directly dobj_address_attributes neg_address_not aux_address_do nsubj_address_methods det_methods_these dep_al._2007 nn_al._et nn_al._Suchanek dep_Snow_al. amod_Snow_2006 dep_Snow_al. nn_Snow_et conj_and_concepts_instances amod_concepts_additional prep_with_extending_instances prep_with_extending_concepts dobj_extending_WORDNET prepc_on_exists_extending nsubj_exists_body amod_work_previous prep_of_body_work amod_body_large det_body_A parataxis_Work_address appos_Work_Snow rcmod_Work_exists amod_Work_Related num_Work_6
W07-1527	P06-1101	p	Currently the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances -LRB- noun compounds -RRB- and are either -LRB- weakly -RRB- supervised knowledge-intensive -LRB- Rosario and Hearst 2001 -RRB- -LRB- Rosario et al. 2002 -RRB- -LRB- Moldovan et al. 2004 -RRB- -LRB- Pantel and Pennacchiotti 2006 -RRB- -LRB- Pennacchiotti and Pantel 2006 -RRB- -LRB- Kim and Baldwin 2006 -RRB- -LRB- Snow et al. 2006 -RRB- -LRB- Girju et al. 2005 Girju et al. 2006 -RRB- or use statistical models on large collections of unlabeled data -LRB- Berland and Charniak 1999 -RRB- -LRB- Lapata and Keller 2004 -RRB- -LRB- Nakov and Hearst 2005 -RRB- -LRB- Turney 2006 -RRB-	amod_Turney_2006 dep_Nakov_2005 conj_and_Nakov_Hearst dep_Lapata_2004 conj_and_Lapata_Keller dep_Berland_1999 conj_and_Berland_Charniak appos_data_Charniak appos_data_Berland amod_data_unlabeled prep_of_collections_data amod_collections_large amod_models_statistical prep_on_use_collections dobj_use_models num_Girju_2006 nn_Girju_al. nn_Girju_et dep_Girju_Girju appos_Girju_2005 dep_Girju_al. nn_Girju_et amod_Snow_2006 dep_Snow_al. nn_Snow_et num_Kim_2006 conj_and_Kim_Baldwin dep_Pennacchiotti_2006 conj_and_Pennacchiotti_Pantel dep_Pantel_2006 conj_and_Pantel_Pennacchiotti amod_Moldovan_2004 dep_Moldovan_al. nn_Moldovan_et amod_Rosario_2002 dep_Rosario_al. nn_Rosario_et dep_Rosario_2001 conj_and_Rosario_Hearst appos_knowledge-intensive_Turney appos_knowledge-intensive_Hearst appos_knowledge-intensive_Nakov appos_knowledge-intensive_Keller appos_knowledge-intensive_Lapata conj_or_knowledge-intensive_use appos_knowledge-intensive_Girju appos_knowledge-intensive_Snow appos_knowledge-intensive_Baldwin appos_knowledge-intensive_Kim dep_knowledge-intensive_Pantel dep_knowledge-intensive_Pennacchiotti dep_knowledge-intensive_Pennacchiotti dep_knowledge-intensive_Pantel dep_knowledge-intensive_Moldovan dep_knowledge-intensive_Rosario dep_knowledge-intensive_Hearst dep_knowledge-intensive_Rosario dep_supervised_use dep_supervised_knowledge-intensive advmod_supervised_either auxpass_supervised_are nsubjpass_supervised_methods dep_either_weakly nn_compounds_noun appos_instances_compounds nn_instances_noun amod_instances_consecutive num_instances_two conj_and_focus_supervised prep_on_focus_instances advmod_focus_mostly nsubj_focus_methods advmod_focus_Currently amod_linguistics_computational prep_in_methods_linguistics nn_methods_interpretation nn_methods_NP amod_methods_English amod_methods_best-performing det_methods_the
W08-2207	P06-1101	o	Obviously all these semantic resources have been acquiredusing a very differentset of processes -LRB- Snow et al. 2006 -RRB- tools and corpora	conj_and_tools_corpora amod_tools_differentset det_tools_a amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_differentset_Snow prep_of_differentset_processes advmod_differentset_very dobj_acquiredusing_corpora dobj_acquiredusing_tools aux_acquiredusing_been aux_acquiredusing_have nsubj_acquiredusing_resources advmod_acquiredusing_Obviously amod_resources_semantic det_resources_these predet_resources_all
W09-0209	P06-1101	o	Given the probabilistic taxonomy learning model introduced by -LRB- Snow et al. 2006 -RRB- we leverage on the computation of logistic regression to exploit singular value decomposition -LRB- SVD -RRB- as unsupervised feature selection	nn_selection_feature amod_selection_unsupervised appos_decomposition_SVD nn_decomposition_value amod_decomposition_singular prep_as_exploit_selection dobj_exploit_decomposition aux_exploit_to amod_regression_logistic prep_of_computation_regression det_computation_the vmod_leverage_exploit prep_on_leverage_computation nsubj_leverage_we rcmod_Snow_leverage amod_Snow_2006 dep_Snow_al. nn_Snow_et agent_introduced_Snow vmod_model_introduced amod_model_learning nn_model_taxonomy amod_model_probabilistic det_model_the pobj_Given_model ccomp_``_Given
W09-0209	P06-1101	o	First we need to determine whether or not the positive effect of SVD feature selection is preserved in more complex feature spaces such as syntactic feature spaces as those used in -LRB- Snow et al. 2006 -RRB-	amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_in_Snow prep_used_in vmod_those_used nn_spaces_feature amod_spaces_syntactic prep_such_as_spaces_spaces nn_spaces_feature amod_spaces_complex advmod_complex_more prep_as_preserved_those prep_in_preserved_spaces auxpass_preserved_is nsubjpass_preserved_effect dep_preserved_not nn_selection_feature nn_selection_SVD prep_of_effect_selection amod_effect_positive det_effect_the cc_not_or mark_not_whether ccomp_determine_preserved aux_determine_to xcomp_need_determine nsubj_need_we advmod_need_First
W09-0209	P06-1101	o	In Section 3 we then describe the probabilistic taxonomy learning model introduced by -LRB- Snow et al. 2006 -RRB-	amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_by_Snow prep_introduced_by vmod_model_introduced amod_model_learning nn_model_taxonomy amod_model_probabilistic det_model_the dobj_describe_model advmod_describe_then nsubj_describe_we prep_in_describe_Section num_Section_3
W09-0209	P06-1101	o	3.4 -RRB- 3.1 Probabilistic model In the probabilistic formulation -LRB- Snow et al. 2006 -RRB- the task of learning taxonomies from a corpus is seen as a probability maximization problem	nn_problem_maximization nn_problem_probability det_problem_a prep_as_seen_problem auxpass_seen_is nsubjpass_seen_model det_corpus_a prep_from_learning_corpus dobj_learning_taxonomies prepc_of_task_learning det_task_the amod_Snow_2006 dep_Snow_al. nn_Snow_et amod_formulation_probabilistic det_formulation_the appos_model_task dep_model_Snow prep_in_model_formulation nn_model_Probabilistic num_model_3.1 num_model_3.4
W09-0209	P06-1101	o	Given a set of evidences E over all the relevant word pairs in -LRB- Snow et al. 2006 -RRB- the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy hatwideT that maximizes the 67 probability of having the evidences E i.e. hatwideT = arg max T P -LRB- E | T -RRB- In -LRB- Snow et al. 2006 -RRB- this maximization problem is solved with a local search	amod_search_local det_search_a prep_with_solved_search auxpass_solved_is nsubjpass_solved_problem nn_problem_maximization det_problem_this amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_In_Snow num_T_| nn_T_E appos_P_T nn_P_T nn_P_max nn_P_arg dep_=_P prep_hatwideT_In amod_hatwideT_= dep_i.e._hatwideT nn_E_evidences det_E_the dep_having_i.e. dobj_having_E prepc_of_probability_having num_probability_67 det_probability_the dobj_maximizes_probability nsubj_maximizes_that rcmod_hatwideT_maximizes nn_hatwideT_taxonomy det_hatwideT_the dobj_finding_hatwideT prepc_of_problem_finding det_problem_the prep_as_defined_problem auxpass_defined_is nsubjpass_defined_task amod_task_learning nn_task_taxonomy amod_task_probabilistic det_task_the dep_Snow_solved rcmod_Snow_defined amod_Snow_2006 dep_Snow_al. nn_Snow_et pobj_in_Snow ccomp_,_in nn_pairs_word amod_pairs_relevant det_pairs_the predet_pairs_all nn_E_evidences prep_over_set_pairs prep_of_set_E det_set_a pobj_Given_set dep_``_Given
W09-0209	P06-1101	p	This increase of probabilities is defined as multiplicative change -LRB- N -RRB- as follows -LRB- N -RRB- = P -LRB- E | Tprime -RRB- / P -LRB- E | T -RRB- -LRB- 2 -RRB- The main innovation of the model in -LRB- Snow et al. 2006 -RRB- is the possibility of adding at each step the best relation N = -LCB- Ri j -RCB- as well as N = I -LRB- Ri j -RRB- that is Ri j with all the relations by the existing taxonomy	amod_taxonomy_existing det_taxonomy_the prep_by_relations_taxonomy det_relations_the predet_relations_all prep_with_j_relations appos_Ri_j cop_Ri_is nsubj_Ri_that appos_Ri_j num_Ri_I dep_=_Ri rcmod_N_Ri amod_N_= appos_Ri_j dep_=_Ri conj_and_N_N amod_N_= nn_N_relation amod_N_best det_N_the dep_step_N dep_step_N det_step_each prep_at_adding_step prepc_of_possibility_adding det_possibility_the cop_possibility_is nsubj_possibility_innovation dep_possibility_2 amod_Snow_2006 dep_Snow_al. nn_Snow_et prep_in_model_Snow det_model_the prep_of_innovation_model amod_innovation_main det_innovation_The num_T_| nn_T_E appos_P_T num_Tprime_| nn_Tprime_E appos_P_Tprime dep_=_P dep_N_possibility dep_N_P dep_N_= mark_follows_as dep_change_follows appos_change_N amod_change_multiplicative dep_defined_N prep_as_defined_change auxpass_defined_is nsubjpass_defined_increase prep_of_increase_probabilities det_increase_This
W09-0209	P06-1101	o	The last important fact is that it is possible to demonstrate that -LRB- Ei j -RRB- = k P -LRB- Ri jT | ei j -RRB- 1P -LRB- Ri jT | ei j -RRB- = = kodds -LRB- Ri j -RRB- where k is a constant -LRB- see -LRB- Snow et al. 2006 -RRB- -RRB- that will be neglected in the maximization process	nn_process_maximization det_process_the prep_in_neglected_process auxpass_neglected_be aux_neglected_will nsubjpass_neglected_that amod_Snow_2006 dep_Snow_al. nn_Snow_et dep_see_Snow dep_constant_see det_constant_a cop_constant_is nsubj_constant_k advmod_constant_where appos_Ri_j rcmod_kodds_neglected rcmod_kodds_constant dep_kodds_Ri dobj_=_kodds dep_=_= num_ei_| nn_ei_jT appos_Ri_j appos_Ri_ei amod_1P_= dep_1P_Ri num_ei_| nn_ei_jT appos_Ri_j appos_Ri_ei dep_P_1P dep_P_Ri nn_P_k amod_P_= dep_Ei_P appos_Ei_j dep_that_Ei dobj_demonstrate_that aux_demonstrate_to xcomp_possible_demonstrate cop_possible_is nsubj_possible_it mark_possible_that ccomp_is_possible nsubj_is_fact amod_fact_important amod_fact_last det_fact_The ccomp_``_is
W09-0209	P06-1101	p	Automatically creating or extending taxonomies for specific domains is then a very interesting area of research -LRB- OSullivan et al. 1995 Magnini and Speranza 2001 Snow et al. 2006 -RRB-	num_Snow_2006 nn_Snow_al. nn_Snow_et dep_Magnini_Snow conj_and_Magnini_2001 conj_and_Magnini_Speranza dep_OSullivan_2001 dep_OSullivan_Speranza dep_OSullivan_Magnini appos_OSullivan_1995 dep_OSullivan_al. nn_OSullivan_et dep_area_OSullivan prep_of_area_research amod_area_interesting det_area_a advmod_area_then cop_area_is csubj_area_extending csubj_area_creating advmod_interesting_very amod_domains_specific prep_for_taxonomies_domains dobj_creating_taxonomies conj_or_creating_extending advmod_creating_Automatically
W09-1109	P06-1101	p	Because of this property vector space models have been used successfully both in computational linguistics -LRB- Manning et al. 2008 Snow et al. 2006 Gorman and Curran 2006 Schutze 1998 -RRB- and in cognitive science -LRB- Landauer and Dumais 1997 Lowe and McDonald 2000 McDonald and Ramscar 2001 -RRB-	dep_Landauer_2001 conj_and_Landauer_Ramscar conj_and_Landauer_McDonald conj_and_Landauer_2000 conj_and_Landauer_McDonald conj_and_Landauer_Lowe conj_and_Landauer_1997 conj_and_Landauer_Dumais amod_science_cognitive dep_in_Ramscar dep_in_McDonald dep_in_2000 dep_in_McDonald dep_in_Lowe dep_in_1997 dep_in_Dumais dep_in_Landauer pobj_in_science dep_Schutze_1998 num_Gorman_2006 conj_and_Gorman_Curran conj_and_Snow_in dep_Snow_Schutze conj_and_Snow_Curran conj_and_Snow_Gorman num_Snow_2006 nn_Snow_al. nn_Snow_et conj_al._in conj_al._Gorman conj_al._Snow conj_al._2008 nn_al._et dobj_Manning_al. amod_linguistics_computational dep_successfully_Manning prep_in_successfully_linguistics preconj_successfully_both advmod_used_successfully auxpass_used_been aux_used_have nsubjpass_used_models prep_because_of_used_property nn_models_space nn_models_vector det_property_this
W09-1109	P06-1101	o	In NLP vector space models have featured most prominently in information retrieval -LRB- Manning et al. 2008 -RRB- but have also been used for ontology learning -LRB- Lin 1998 Snow et al. 2006 Gorman and Curran 2006 -RRB- and word sense-related tasks -LRB- McCarthy et al. 2004 Schutze 1998 -RRB-	amod_Schutze_1998 dep_McCarthy_Schutze amod_McCarthy_2004 dep_McCarthy_al. nn_McCarthy_et amod_tasks_sense-related nn_tasks_word num_Snow_2006 nn_Snow_al. nn_Snow_et num_Lin_2006 conj_and_Lin_Curran conj_and_Lin_Gorman conj_and_Lin_Snow num_Lin_1998 conj_and_learning_tasks appos_learning_Curran appos_learning_Gorman appos_learning_Snow appos_learning_Lin amod_learning_ontology dep_used_McCarthy prep_for_used_tasks prep_for_used_learning auxpass_used_been advmod_used_also aux_used_have nsubjpass_used_models dep_al._2008 nn_al._et advmod_Manning_al. nn_retrieval_information advmod_prominently_most conj_but_featured_used dep_featured_Manning prep_in_featured_retrieval advmod_featured_prominently aux_featured_have nsubj_featured_models prep_in_featured_NLP nn_models_space nn_models_vector
W09-1122	P06-1101	o	We have adopted the evaluation method of Snow et al -LRB- 2006 -RRB- compare the generated hypernyms with hypernyms present in a lexical resource in our case the Dutch part of EuroWordNet -LRB- 1998 -RRB-	appos_EuroWordNet_1998 prep_of_part_EuroWordNet amod_part_Dutch det_part_the poss_case_our amod_resource_lexical det_resource_a prep_in_present_resource amod_hypernyms_present amod_hypernyms_generated det_hypernyms_the dep_compare_part prep_in_compare_case prep_with_compare_hypernyms dobj_compare_hypernyms appos_al_2006 dep_Snow_al nn_Snow_et prep_of_method_Snow nn_method_evaluation det_method_the dep_adopted_compare dobj_adopted_method aux_adopted_have nsubj_adopted_We ccomp_``_adopted
W09-2504	P06-1101	o	6 Related Work Several works attempt to extend WordNet with additional lexical semantic information -LRB- Moldovan and Rus 2001 Snow et al. 2006 Suchanek et al. 2007 Clark et al. 2008 -RRB-	num_Clark_2008 nn_Clark_al. nn_Clark_et num_Suchanek_2007 nn_Suchanek_al. nn_Suchanek_et num_Snow_2006 nn_Snow_al. nn_Snow_et dep_Moldovan_Clark conj_and_Moldovan_Suchanek conj_and_Moldovan_Snow conj_and_Moldovan_2001 conj_and_Moldovan_Rus dep_information_Suchanek dep_information_Snow dep_information_2001 dep_information_Rus dep_information_Moldovan amod_information_semantic amod_information_lexical amod_information_additional prep_with_extend_information dobj_extend_WordNet aux_extend_to xcomp_attempt_extend nsubj_attempt_works amod_works_Several rcmod_Work_attempt amod_Work_Related num_Work_6
W09-2508	P06-1101	o	As our basic data source we use 500 000 sentences from the Wikipedia XML corpus -LRB- Denoyer and Gallinari 2006 -RRB- this is the corpus used by Akhmatova and Dras -LRB- 2007 -RRB- and related to one used in one set of experiments by Snow et al.	dep_Snow_al. nn_Snow_et prep_by_set_Snow prep_of_set_experiments num_set_one prep_in_used_set vmod_one_used prep_to_related_one nsubj_related_this appos_Dras_2007 conj_and_Akhmatova_Dras agent_used_Dras agent_used_Akhmatova conj_and_corpus_related vmod_corpus_used det_corpus_the cop_corpus_is nsubj_corpus_this dep_Denoyer_2006 conj_and_Denoyer_Gallinari appos_corpus_Gallinari appos_corpus_Denoyer nn_corpus_XML nn_corpus_Wikipedia det_corpus_the num_sentences_000 number_000_500 parataxis_use_related parataxis_use_corpus prep_from_use_corpus dobj_use_sentences nsubj_use_we prep_as_use_source nn_source_data amod_source_basic poss_source_our
C08-1138	P06-1123	o	Thus it may not suffer from the issues of non-isomorphic structure alignment and non-syntactic phrase usage heavily -LRB- Wellington et al. 2006 -RRB-	amod_Wellington_2006 dep_Wellington_al. nn_Wellington_et nn_usage_phrase amod_usage_non-syntactic conj_and_alignment_usage nn_alignment_structure amod_alignment_non-isomorphic advmod_issues_heavily prep_of_issues_usage prep_of_issues_alignment det_issues_the dep_suffer_Wellington prep_from_suffer_issues neg_suffer_not aux_suffer_may nsubj_suffer_it advmod_suffer_Thus
C08-1138	P06-1123	o	1 Introduction Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning -LRB- Wellington et al. 2006 -RRB-	amod_Wellington_2006 dep_Wellington_al. nn_Wellington_et dep_meaning_Wellington amod_meaning_same det_meaning_the prep_with_expressions_meaning amod_expressions_linguistic prep_between_holds_expressions nsubj_holds_that rcmod_relation_holds amod_relation_mathematical det_relation_a cop_relation_is nsubj_relation_equivalence amod_equivalence_Translational dep_equivalence_Introduction num_equivalence_1
N07-1057	P06-1123	n	However to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data -LRB- Fox 2002 Hwa et al. 2002 Wellington et al. 2006 -RRB-	num_Wellington_2006 nn_Wellington_al. nn_Wellington_et num_Hwa_2002 nn_Hwa_al. nn_Hwa_et dep_Fox_Wellington dep_Fox_Hwa dep_Fox_2002 appos_data_Fox amod_data_aligned nn_data_hand dobj_using_data nn_pairs_language prep_of_number_pairs amod_number_small det_number_a xcomp_tested_using prep_on_tested_number advmod_tested_only auxpass_tested_is ccomp_holds_tested nsubj_holds_assumption mark_holds_that prep_to_holds_extent det_extent_what ccomp_,_holds dep_``_However
N07-1063	P06-1123	o	-LRB- Wellington et al. 2006 -RRB- argue that these restrictions reduce our ability to model translation equivalence effectively	advmod_equivalence_effectively amod_translation_equivalence dep_model_translation prep_to_ability_model poss_ability_our dobj_reduce_ability nsubj_reduce_restrictions mark_reduce_that det_restrictions_these ccomp_argue_reduce nsubj_argue_2006 dep_2006_Wellington dep_Wellington_al. nn_Wellington_et
P07-1002	P06-1123	o	Figure 1 -LRB- b -RRB- shows several orders of the sentence which violate this constraint .1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency the alignment between subtrees in the two languages is very complex -LRB- Wellington et al. 2006 -RRB-	amod_Wellington_2006 dep_Wellington_al. nn_Wellington_et advmod_complex_very cop_complex_is nsubj_complex_alignment advcl_complex_represent mark_complex_that num_languages_two det_languages_the prep_in_subtrees_languages prep_between_alignment_subtrees det_alignment_the amod_constituency_linguistic dobj_represent_constituency nsubj_represent_trees nsubj_represent_source mark_represent_if nn_trees_dependency nn_trees_target conj_and_source_trees det_source_the preconj_source_both ccomp_shown_complex aux_shown_have nsubj_shown_studies amod_studies_Previous num_studies_.1 rcmod_constraint_shown det_constraint_this dobj_violate_constraint nsubj_violate_which rcmod_sentence_violate det_sentence_the prep_of_orders_sentence amod_orders_several dep_shows_Wellington dobj_shows_orders nsubj_shows_b num_b_1 nn_b_Figure
P08-1064	P06-1123	o	565 es -LRB- Wellington et al. 2006 -RRB-	amod_Wellington_2006 dep_Wellington_al. nn_Wellington_et appos_es_Wellington num_es_565 dep_``_es
W07-0404	P06-1123	o	We use the same alignment data for the five language pairs Chinese/English Romanian/English Hindi/English Spanish/English and French/English -LRB- Wellington et al. 2006 -RRB-	amod_Wellington_2006 dep_Wellington_al. nn_Wellington_et conj_and_Romanian/English_French/English conj_and_Romanian/English_Spanish/English conj_and_Romanian/English_Hindi/English dep_Chinese/English_Wellington appos_Chinese/English_French/English appos_Chinese/English_Spanish/English appos_Chinese/English_Hindi/English appos_Chinese/English_Romanian/English nn_Chinese/English_pairs nn_Chinese/English_language num_Chinese/English_five det_Chinese/English_the nn_data_alignment amod_data_same det_data_the prep_for_use_Chinese/English dobj_use_data nsubj_use_We
W07-0405	P06-1123	o	However this method is more sophisticated to implement than the previous method and binarizability ratio decreases on freer word-order languages -LRB- Wellington et al. 2006 -RRB-	amod_Wellington_2006 dep_Wellington_al. nn_Wellington_et dep_languages_Wellington nn_languages_word-order amod_languages_freer prep_on_decreases_languages nsubj_decreases_ratio nsubj_decreases_method mark_decreases_than nn_ratio_binarizability conj_and_method_ratio amod_method_previous det_method_the advcl_implement_decreases aux_implement_to xcomp_sophisticated_implement advmod_sophisticated_more cop_sophisticated_is nsubj_sophisticated_method advmod_sophisticated_However det_method_this
W07-0405	P06-1123	o	More importantly the ratio of binarizability as expected decreases on freer word-order languages -LRB- Wellington et al. 2006 -RRB-	amod_Wellington_2006 dep_Wellington_al. nn_Wellington_et dep_languages_Wellington nn_languages_word-order amod_languages_freer prep_on_decreases_languages nsubj_decreases_ratio advmod_decreases_importantly mark_expected_as dep_ratio_expected prep_of_ratio_binarizability det_ratio_the advmod_importantly_More
W09-2303	P06-1123	o	It is for all three reasons i.e. translation induction from alignment structures and induction of alignment structures important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora -LRB- Wellington et al. 2006 -RRB-	amod_Wellington_2006 dep_Wellington_al. nn_Wellington_et appos_corpora_Wellington nn_corpora_parallel amod_corpora_standard amod_corpora_gold amod_corpora_hand-aligned prep_in_found_corpora vmod_structures_found nn_structures_alignment det_structures_the predet_structures_all dobj_induce_structures aux_induce_to xcomp_enough_induce acomp_expressive_enough cop_expressive_are nsubj_expressive_grammars mark_expressive_that amod_grammars_synchronous det_grammars_the ccomp_important_expressive nn_structures_alignment prep_of_induction_structures nn_structures_alignment prep_from_induction_structures amod_translation_important conj_and_translation_induction conj_and_translation_induction advmod_translation_i.e. num_reasons_three det_reasons_all dep_is_induction dep_is_induction dep_is_translation prep_for_is_reasons nsubj_is_It ccomp_``_is
W09-2303	P06-1123	o	-LRB- 2006 -RRB- and Chiang -LRB- 2007 -RRB- in terms of what alignments they induce has been discussed in Wu -LRB- 1997 -RRB- and Wellington et al.	nn_al._et nn_al._Wellington conj_and_Wu_al. appos_Wu_1997 prep_in_discussed_al. prep_in_discussed_Wu auxpass_discussed_been aux_discussed_has nsubjpass_discussed_Chiang nsubjpass_discussed_2006 nsubj_induce_they rcmod_alignments_induce det_alignments_what prep_of_terms_alignments appos_Chiang_2007 prep_in_2006_terms conj_and_2006_Chiang
W09-2303	P06-1123	o	2 Inside-out alignments Wu -LRB- 1997 -RRB- identified so-called inside-out alignments two alignment configurations that can not be induced by binary synchronous context-free grammars these alignment configurations while infrequent in language pairs such as EnglishFrench -LRB- Cherry and Lin 2006 Wellington et al. 2006 -RRB- have been argued to be frequent in other language pairs incl	appos_pairs_incl nn_pairs_language amod_pairs_other prep_in_frequent_pairs cop_frequent_be aux_frequent_to xcomp_argued_frequent auxpass_argued_been aux_argued_have nsubjpass_argued_configurations num_Wellington_2006 nn_Wellington_al. nn_Wellington_et dep_Cherry_Wellington amod_Cherry_2006 conj_and_Cherry_Lin appos_EnglishFrench_Lin appos_EnglishFrench_Cherry prep_such_as_pairs_EnglishFrench nn_pairs_language prep_in_infrequent_pairs mark_infrequent_while advcl_configurations_infrequent nn_configurations_alignment det_configurations_these amod_grammars_context-free amod_grammars_synchronous amod_grammars_binary agent_induced_grammars auxpass_induced_be neg_induced_not aux_induced_can nsubjpass_induced_that rcmod_configurations_induced nn_configurations_alignment num_configurations_two appos_alignments_configurations amod_alignments_inside-out amod_alignments_so-called parataxis_identified_argued dobj_identified_alignments nsubj_identified_Wu appos_Wu_1997 nn_Wu_alignments amod_Wu_Inside-out num_Wu_2
W09-2303	P06-1123	o	EnglishChinese -LRB- Wellington et al. 2006 -RRB- and EnglishSpanish -LRB- Lepage and Denoual 2005 -RRB-	dep_Lepage_2005 conj_and_Lepage_Denoual dep_EnglishSpanish_Denoual dep_EnglishSpanish_Lepage num_Wellington_2006 dep_Wellington_al. nn_Wellington_et conj_and_EnglishChinese_EnglishSpanish appos_EnglishChinese_Wellington
W09-2306	P06-1123	o	One of the theoretical problems with phrase based SMT models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue -LRB- Simard et al. 2005 Quirk and Menezes 2006 Wellington et al. 2006 Bod 2007 Zhang et al. 2007 -RRB-	num_Zhang_2007 nn_Zhang_al. nn_Zhang_et num_Bod_2007 num_Wellington_2006 nn_Wellington_al. nn_Wellington_et dep_Quirk_Zhang conj_and_Quirk_Bod conj_and_Quirk_Wellington conj_and_Quirk_2006 conj_and_Quirk_Menezes dep_Simard_Bod dep_Simard_Wellington dep_Simard_2006 dep_Simard_Menezes dep_Simard_Quirk appos_Simard_2005 dep_Simard_al. nn_Simard_et det_issue_this prep_on_made_issue auxpass_made_been aux_made_have nsubjpass_made_model amod_attempts_numerous conj_and_translations_attempts amod_translations_discontiguous det_translations_the dobj_model_attempts dobj_model_translations advmod_model_effectively neg_model_not aux_model_can nsubj_model_they mark_model_that dep_is_Simard ccomp_is_made nsubj_is_One nn_models_SMT amod_models_based nn_models_phrase prep_with_problems_models amod_problems_theoretical det_problems_the prep_of_One_problems ccomp_``_is
D08-1033	P06-1124	o	Gibbs sampling is not new to the natural language processing community -LRB- Teh 2006 Johnson et al. 2007 -RRB-	num_Johnson_2007 nn_Johnson_al. nn_Johnson_et dep_Teh_Johnson dep_Teh_2006 appos_community_Teh nn_community_processing nn_community_language amod_community_natural det_community_the prep_to_new_community neg_new_not cop_new_is nsubj_new_sampling nn_sampling_Gibbs
N09-1009	P06-1124	p	Nonparametricmodels -LRB- Teh 2006 -RRB- may be appropriate	cop_appropriate_be aux_appropriate_may nsubj_appropriate_Nonparametricmodels amod_Teh_2006 dep_Nonparametricmodels_Teh
P08-2036	P06-1124	o	The relationship between Kneser-Ney smoothing to the Bayesian approach have been explored in -LRB- Goldwater et al. 2006 Teh 2006 -RRB- using Pitman-Yor processes	nn_processes_Pitman-Yor dobj_using_processes dep_Teh_2006 vmod_Goldwater_using dep_Goldwater_Teh amod_Goldwater_2006 dep_Goldwater_al. nn_Goldwater_et prep_in_explored_Goldwater auxpass_explored_been aux_explored_have nsubjpass_explored_relationship amod_approach_Bayesian det_approach_the nn_smoothing_Kneser-Ney prep_to_relationship_approach prep_between_relationship_smoothing det_relationship_The
W09-0210	P06-1124	o	Recent work has applied Bayesian non-parametric models to anaphora resolution -LRB- Haghighi and Klein 2007 -RRB- lexical acquisition -LRB- Goldwater 2007 -RRB- and language modeling -LRB- Teh 2006 -RRB- with good results	amod_results_good appos_Teh_2006 dep_modeling_Teh nn_modeling_language dep_Goldwater_2007 prep_with_acquisition_results conj_and_acquisition_modeling appos_acquisition_Goldwater amod_acquisition_lexical amod_Haghighi_2007 conj_and_Haghighi_Klein dep_resolution_Klein dep_resolution_Haghighi nn_resolution_anaphora amod_models_non-parametric amod_models_Bayesian dobj_applied_modeling dobj_applied_acquisition prep_to_applied_resolution dobj_applied_models aux_applied_has nsubj_applied_work amod_work_Recent ccomp_``_applied
C08-1038	P06-1130	o	The most direct comparison is between our system and those presented in Cahill and van Genabith -LRB- 2006 -RRB- and Hogan et al.	dep_Hogan_al. nn_Hogan_et appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Hogan conj_and_Cahill_Genabith prep_in_presented_Hogan prep_in_presented_Genabith prep_in_presented_Cahill vmod_those_presented conj_and_system_those poss_system_our prep_between_is_those prep_between_is_system nsubj_is_comparison amod_comparison_direct det_comparison_The advmod_direct_most
C08-1038	P06-1130	o	298 within LFG includes the XLE ,3 Cahill and van Genabith -LRB- 2006 -RRB- Hogan et al.	nn_al._et nn_al._Hogan appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_al. conj_and_Cahill_Genabith num_Cahill_,3 nn_Cahill_XLE det_Cahill_the dobj_includes_al. dobj_includes_Genabith dobj_includes_Cahill nsubj_includes_298 prep_within_298_LFG
C08-1038	P06-1130	o	Cahill and van Genabith -LRB- 2006 -RRB- and Hogan et al.	dep_Hogan_al. nn_Hogan_et appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Hogan conj_and_Cahill_Genabith
C08-1038	P06-1130	o	1999 -RRB- OpenCCG -LRB- White 2004 -RRB- and XLE -LRB- Crouch et al. 2007 -RRB- or created semi-automatically -LRB- Belz 2007 -RRB- or fully automatically extracted from annotated corpora like the HPSG -LRB- Nakanishi et al. 2005 -RRB- LFG -LRB- Cahill and van Genabith 2006 Hogan et al. 2007 -RRB- and CCG -LRB- White et al. 2007 -RRB- resources derived from the Penn-II Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn-II det_Treebank_the prep_from_derived_Treebank dep_resources_Marcus vmod_resources_derived nn_resources_CCG nn_resources_LFG nn_resources_created nn_resources_XLE nn_resources_White dep_White_2007 dep_White_al. nn_White_et num_Hogan_2007 nn_Hogan_al. nn_Hogan_et nn_Genabith_van dep_Cahill_Hogan dep_Cahill_2006 conj_and_Cahill_Genabith dep_LFG_Genabith dep_LFG_Cahill amod_Nakanishi_2005 dep_Nakanishi_al. nn_Nakanishi_et det_HPSG_the amod_corpora_annotated prep_from_extracted_corpora advmod_extracted_automatically advmod_automatically_fully amod_Belz_2007 dep_semi-automatically_Nakanishi prep_like_semi-automatically_HPSG conj_or_semi-automatically_extracted dep_semi-automatically_Belz advmod_created_extracted advmod_created_semi-automatically amod_Crouch_2007 dep_Crouch_al. nn_Crouch_et appos_XLE_Crouch dep_White_White conj_and_White_CCG conj_or_White_LFG conj_or_White_created conj_and_White_XLE num_White_2004 nn_White_OpenCCG conj_1999_resources
D07-1028	P06-1130	o	aoife.cahill@ims.uni-stuttgart.de and van Genabith -LRB- 2006 -RRB- which do not rely on handcrafted grammars and thus can easily be ported to new languages	amod_languages_new prep_to_ported_languages auxpass_ported_be advmod_ported_easily aux_ported_can advmod_ported_thus nsubjpass_ported_which amod_grammars_handcrafted conj_and_rely_ported prep_on_rely_grammars neg_rely_not aux_rely_do nsubj_rely_which appos_Genabith_2006 nn_Genabith_van rcmod_aoife.cahill@ims.uni-stuttgart.de_ported rcmod_aoife.cahill@ims.uni-stuttgart.de_rely conj_and_aoife.cahill@ims.uni-stuttgart.de_Genabith
D07-1028	P06-1130	o	As in -LRB- Cahill and van Genabith 2006 -RRB- fstructures are generated from the -LRB- now altered -RRB- treebank and from this data along with the treebank trees the PCFG-based grammar which is used for training the generation model is extracted	auxpass_extracted_is nsubjpass_extracted_grammar nn_model_generation det_model_the dobj_training_model prepc_for_used_training auxpass_used_is nsubjpass_used_which rcmod_grammar_used amod_grammar_PCFG-based det_grammar_the nn_trees_treebank det_trees_the det_data_this dep_treebank_altered det_treebank_the advmod_altered_now conj_generated_extracted prep_with_generated_trees prep_from_generated_data prep_along_generated_treebank prep_from_generated_treebank conj_and_generated_generated conj_and_generated_generated auxpass_generated_are prep_generated_As dep_fstructures_Genabith dep_fstructures_Cahill nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith pobj_in_fstructures pcomp_As_in
D07-1028	P06-1130	o	In Table 10 Baseline gives the results of the generation algorithm of -LRB- Cahill and van Genabith 2006 -RRB-	nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith dep_of_Genabith dep_of_Cahill prep_algorithm_of nn_algorithm_generation det_algorithm_the prep_of_results_algorithm det_results_the dobj_gives_results nsubj_gives_Baseline prep_in_gives_Table num_Table_10
D07-1028	P06-1130	o	In the LFG-based generation algorithm presented by Cahill and van Genabith -LRB- 2006 -RRB- complex named entities -LRB- i.e. those consisting of more than one word token -RRB- and other multi-word units can be fragmented in the surface realization	nn_realization_surface det_realization_the prep_in_fragmented_realization cop_fragmented_be aux_fragmented_can nsubj_fragmented_those dep_fragmented_i.e. amod_units_multi-word amod_units_other conj_and_word_units amod_word_token num_word_one quantmod_one_than mwe_than_more prep_of_consisting_units prep_of_consisting_word vmod_those_consisting dep_entities_fragmented dep_named_entities vmod_complex_named num_complex_2006 dep_Genabith_complex nn_Genabith_van conj_and_Cahill_Genabith agent_presented_Genabith agent_presented_Cahill vmod_algorithm_presented nn_algorithm_generation amod_algorithm_LFG-based det_algorithm_the pobj_In_algorithm
D07-1028	P06-1130	o	We take the generator of -LRB- Cahill and van Genabith 2006 -RRB- as our baseline generator	nn_generator_baseline poss_generator_our nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith prep_of_generator_Genabith prep_of_generator_Cahill det_generator_the prep_as_take_generator dobj_take_generator nsubj_take_We
D07-1028	P06-1130	o	These rules can be handcrafted grammar rules such as those of -LRB- LangkildeGeary 2002 Carroll and Oepen 2005 -RRB- created semi-automatically -LRB- Belz 2007 -RRB- or alternatively extracted fully automatically from treebanks -LRB- Bangalore and Rambow 2000 Nakanishi et al. 2005 Cahill and van Genabith 2006 -RRB-	nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith num_Nakanishi_2005 nn_Nakanishi_al. nn_Nakanishi_et dep_Bangalore_Genabith dep_Bangalore_Cahill conj_and_Bangalore_Nakanishi conj_and_Bangalore_2000 conj_and_Bangalore_Rambow dep_treebanks_Nakanishi dep_treebanks_2000 dep_treebanks_Rambow dep_treebanks_Bangalore prep_from_extracted_treebanks advmod_extracted_automatically advmod_extracted_fully advmod_extracted_alternatively num_Belz_2007 conj_or_semi-automatically_extracted dep_semi-automatically_Belz dep_created_extracted dep_created_semi-automatically dep_Carroll_2005 conj_and_Carroll_Oepen vmod_LangkildeGeary_created dep_LangkildeGeary_Oepen dep_LangkildeGeary_Carroll appos_LangkildeGeary_2002 prep_of_those_LangkildeGeary prep_such_as_rules_those nn_rules_grammar amod_rules_handcrafted cop_rules_be aux_rules_can nsubj_rules_rules det_rules_These
D07-1028	P06-1130	o	3 Surface Realisation from f-Structures Cahill and van Genabith -LRB- 2006 -RRB- present a probabilistic surface generation model for LFG -LRB- Kaplan 1995 -RRB-	amod_Kaplan_1995 dep_LFG_Kaplan prep_for_model_LFG nn_model_generation nn_model_surface amod_model_probabilistic det_model_a dobj_present_model nsubj_present_Realisation appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Genabith nn_Cahill_f-Structures prep_from_Realisation_Genabith prep_from_Realisation_Cahill nn_Realisation_Surface num_Realisation_3
D07-1028	P06-1130	o	The up-arrows and down-arrows are shorthand for -LRB- M -LRB- ni -RRB- -RRB- = -LRB- ni -RRB- where ni is the c-structure node annotated with the equation .2 Treebest = argmaxTreeP -LRB- Tree | F-Str -RRB- -LRB- 1 -RRB- P -LRB- Tree | F-Str -RRB- = productdisplay X Y in Tree Feats = -LCB- ai | vj -LRB- -LRB- X -RRB- -RRB- ai = vj -RCB- P -LRB- X Y | X Feats -RRB- -LRB- 2 -RRB- The generation model of -LRB- Cahill and van Genabith 2006 -RRB- maximises the probability of a tree given an f-structure -LRB- Eqn	dep_f-structure_Eqn det_f-structure_an pobj_given_f-structure prep_tree_given det_tree_a prep_of_probability_tree det_probability_the dobj_maximises_probability nsubj_maximises_model dep_maximises_2 nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith prep_of_model_Genabith prep_of_model_Cahill nn_model_generation det_model_The num_X_| dep_Y_Feats dep_Y_X nn_Y_X rcmod_P_maximises appos_P_Y nn_P_vj amod_P_= dobj_ai_P dep_X_ai dep_vj_X num_vj_| dobj_ai_vj dep_=_ai nn_Feats_Tree amod_Y_= prep_in_Y_Feats nn_Y_X nn_Y_productdisplay dobj_=_Y num_F-Str_| nn_F-Str_Tree dep_P_= appos_P_F-Str dep_1_P num_F-Str_| nn_F-Str_Tree nn_F-Str_argmaxTreeP amod_F-Str_= nn_Treebest_.2 nn_Treebest_equation det_Treebest_the prep_with_annotated_Treebest amod_node_annotated nn_node_c-structure det_node_the cop_node_is nsubj_node_ni advmod_node_where dep_=_node dep_=_ni dep_M_1 dep_M_F-Str amod_M_= appos_M_ni prep_for_shorthand_M cop_shorthand_are nsubj_shorthand_down-arrows nsubj_shorthand_up-arrows conj_and_up-arrows_down-arrows det_up-arrows_The
D07-1028	P06-1130	o	Cahill and van Genabith -LRB- 2006 -RRB- note that conditioning f-structure annotated generation rules on local features -LRB- Eqn	dep_features_Eqn amod_features_local prep_on_rules_features nn_rules_generation amod_rules_annotated nn_rules_f-structure nn_rules_conditioning dep_that_rules prep_note_that nsubj_note_Genabith nsubj_note_Cahill appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Genabith
D07-1028	P06-1130	o	To solve the problem Cahill and van Genabith -LRB- 2006 -RRB- apply an automatic generation grammar transformation to their training data they automatically label CFG nodes with additional case information and the model now learns the new improved generation rules of Tables 4 and 5	conj_and_Tables_5 num_Tables_4 prep_of_rules_5 prep_of_rules_Tables nn_rules_generation amod_rules_improved amod_rules_new det_rules_the dobj_learns_rules advmod_learns_now det_model_the conj_and_information_model nn_information_case amod_information_additional nn_nodes_CFG dep_label_learns prep_with_label_model prep_with_label_information dobj_label_nodes advmod_label_automatically nsubj_label_they nn_data_training poss_data_their nn_transformation_grammar nn_transformation_generation amod_transformation_automatic det_transformation_an parataxis_apply_label prep_to_apply_data dobj_apply_transformation nsubj_apply_Genabith nsubj_apply_Cahill advcl_apply_solve appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Genabith det_problem_the dobj_solve_problem aux_solve_To
D07-1028	P06-1130	o	F-Struct Feats Grammar Rules -LCB- PRED = PRO NUM = SG PER = 3 GEN = FEM -RCB- PRP-nom -LRB- =-RRB- she -LCB- PRED = PRO NUM = SG PER = 3 GEN = FEM -RCB- PRP-acc -LRB- =-RRB- her Table 5 Lexical item rules with case markings 4 A History-Based Generation Model The automatic generation grammar transform presented in -LRB- Cahill and van Genabith 2006 -RRB- provides a solution to coarse-grained and -LRB- in fact -RRB- inappropriate independence assumptions in the basic generation model	nn_model_generation amod_model_basic det_model_the prep_in_assumptions_model nn_assumptions_independence amod_assumptions_inappropriate dep_in_assumptions pobj_in_fact prep_to_solution_coarse-grained det_solution_a conj_and_provides_in dobj_provides_solution nsubj_provides_grammar nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith prep_in_presented_Genabith prep_in_presented_Cahill dep_transform_presented vmod_grammar_transform nn_grammar_generation amod_grammar_automatic nn_grammar_The rcmod_Model_in rcmod_Model_provides nn_Model_Generation dep_History-Based_Model amod_A_History-Based dep_4_A dep_markings_4 dep_case_markings prep_with_rules_case nn_rules_item amod_rules_Lexical dep_Table_rules num_Table_5 poss_Table_her dep_=-RRB-_Table dep_PRP-acc_=-RRB- nn_PRP-acc_FEM amod_PRP-acc_= nn_PRP-acc_GEN conj_=_PRP-acc dobj_=_3 nsubj_=_PER nn_PER_SG dep_=_= amod_NUM_= nn_NUM_PRO dobj_=_NUM nsubj_=_PRED dep_she_= dep_=-RRB-_she dep_PRP-nom_=-RRB- nn_PRP-nom_FEM amod_PRP-nom_= nn_PRP-nom_GEN dep_=_PRP-nom dobj_=_3 nsubj_=_PER nn_PER_SG dep_=_= amod_NUM_= nn_NUM_PRO dobj_=_NUM dep_PRED_= dep_Rules_PRED nn_Rules_Grammar nn_Rules_Feats nn_Rules_F-Struct
D07-1028	P06-1130	n	This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process and we argue provides a simpler more uniform general intuitive and natural probabilistic generation model obviating the need for CFG-grammar transforms in the original proposal of -LRB- Cahill and van Genabith 2006 -RRB-	nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith dep_of_Genabith dep_of_Cahill prep_proposal_of amod_proposal_original det_proposal_the prep_in_transforms_proposal prep_for_need_CFG-grammar det_need_the dobj_obviating_need vmod_model_obviating nn_model_generation amod_model_probabilistic amod_model_natural amod_model_intuitive amod_model_uniform det_model_a conj_and_intuitive_natural amod_intuitive_general conj_and_intuitive_uniform dep_intuitive_simpler advmod_uniform_more dobj_provides_model parataxis_provides_argue nsubj_provides_conditioning nsubj_argue_we nn_process_generation det_process_the prep_of_history_process det_history_the prep_to_sensitive_history amod_rules_sensitive nn_rules_generation prep_of_choice_rules det_choice_the dobj_making_choice prepc_of_effect_making det_effect_the dep_has_transforms conj_and_has_provides dobj_has_effect nsubj_has_conditioning amod_conditioning_additional det_conditioning_This
D07-1028	P06-1130	o	Note that for our example the effect of the uniform additional conditioning on mother grammatical function has the same effect as the generation grammar transform of -LRB- Cahill and van Genabith 2006 -RRB- but without the need for the gramF-Struct Feats Grammar Rules -LCB- PRED = PRO NUM = SG PER = 3 GEN = FEM SUBJ -RCB- PRP -LRB- =-RRB- she -LCB- PRED = PRO NUM = SG PER = 3 GEN = FEM OBJ -RCB- PRP -LRB- =-RRB- her Table 7 Lexical item rules	nn_rules_item amod_rules_Lexical dep_Table_rules num_Table_7 poss_Table_her dep_=-RRB-_Table dep_PRP_=-RRB- dep_FEM_OBJ dep_=_PRP dep_=_FEM amod_GEN_= dep_=_GEN dobj_=_3 nsubj_=_PER nn_PER_SG dep_=_= amod_NUM_= nn_NUM_PRO dobj_=_NUM nsubj_=_PRED dep_she_= dep_=-RRB-_she nn_=-RRB-_PRP amod_=-RRB-_= nn_=-RRB-_GEN dep_FEM_SUBJ dep_=_FEM dep_=_=-RRB- dobj_=_3 nsubj_=_PER nn_PER_SG xcomp_=_= amod_NUM_= nn_NUM_PRO dobj_=_NUM dep_PRED_= dep_Rules_PRED nn_Rules_Grammar nn_Rules_Feats amod_Rules_gramF-Struct det_Rules_the prep_for_need_Rules det_need_the pobj_without_need nn_Genabith_van conj_but_Cahill_without dep_Cahill_2006 conj_and_Cahill_Genabith prep_of_transform_without prep_of_transform_Genabith prep_of_transform_Cahill nsubj_transform_grammar mark_transform_as nn_grammar_generation det_grammar_the dep_effect_transform amod_effect_same det_effect_the dobj_has_effect nsubj_has_effect prep_for_has_example mark_has_that amod_function_grammatical nn_function_mother amod_conditioning_additional amod_conditioning_uniform det_conditioning_the prep_on_effect_function prep_of_effect_conditioning det_effect_the poss_example_our ccomp_Note_has
D07-1028	P06-1130	n	In addition uniform conditioning on mother grammatical function is more general than the case-phenomena specific generation grammar transform of -LRB- Cahill and van Genabith 2006 -RRB- in that it applies to each and every sub-part of a recursive input f-structure driving generation making available relevant generation history -LRB- context -RRB- to guide local generation decisions	nn_decisions_generation amod_decisions_local dobj_guide_decisions aux_guide_to appos_history_context nn_history_generation amod_history_relevant dep_available_history xcomp_making_guide acomp_making_available dobj_driving_generation vmod_f-structure_driving nn_f-structure_input amod_f-structure_recursive det_f-structure_a prep_of_sub-part_f-structure dep_each_sub-part conj_and_each_every xcomp_applies_making prep_to_applies_every prep_to_applies_each nsubj_applies_it mark_applies_that nn_Genabith_van prepc_in_Cahill_applies dep_Cahill_2006 conj_and_Cahill_Genabith prep_of_transform_Genabith prep_of_transform_Cahill nsubj_transform_grammar mark_transform_than nn_grammar_generation amod_grammar_specific nn_grammar_case-phenomena det_grammar_the ccomp_general_transform advmod_general_more cop_general_is nsubj_general_conditioning prep_in_general_addition amod_function_grammatical nn_function_mother prep_on_conditioning_function amod_conditioning_uniform
N07-1021	P06-1130	o	Existing statistical NLG -LRB- i -RRB- uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation -LRB- Varges and Mellish 2001 White 2004 Paiva and Evans 2005 -RRB- -LRB- ii -RRB- applies n-gram models to select the overall most likely realisation after generation -LRB- HALOGEN family -RRB- or -LRB- iii -RRB- reuses an existing parsing grammar or treebank for surface realisation -LRB- Velldal et al. 2004 Cahill and van Genabith 2006 -RRB-	nn_Genabith_van num_Cahill_2006 conj_and_Cahill_Genabith dep_al._Genabith dep_al._Cahill num_al._2004 nn_al._et amod_al._Velldal dep_realisation_al. nn_realisation_surface conj_or_grammar_treebank nn_grammar_parsing amod_grammar_existing det_grammar_an prep_for_reuses_realisation dobj_reuses_treebank dobj_reuses_grammar dep_reuses_iii nn_family_HALOGEN conj_or_generation_reuses appos_generation_family amod_realisation_likely amod_realisation_overall det_realisation_the advmod_likely_most prep_after_select_reuses prep_after_select_generation dobj_select_realisation aux_select_to amod_models_n-gram xcomp_applies_select dobj_applies_models dep_applies_ii amod_Paiva_2005 conj_and_Paiva_Evans num_White_2004 dep_Varges_Evans dep_Varges_Paiva conj_and_Varges_White conj_and_Varges_2001 conj_and_Varges_Mellish appos_generation_White appos_generation_2001 appos_generation_Mellish appos_generation_Varges amod_generation_symbolic cop_generation_is nsubj_generation_what advmod_symbolic_otherwise nn_decisions_heuristic prepc_in_inform_generation dobj_inform_decisions aux_inform_to nn_statistics_corpus conj_uses_applies vmod_uses_inform dobj_uses_statistics dep_uses_i nsubj_uses_NLG amod_NLG_statistical amod_NLG_Existing
N09-3004	P06-1130	o	There are other approaches in which the generation grammars are extracted semiautomatically -LRB- Belz 2007 -RRB- or automatically -LRB- such as HPSG -LRB- Nakanishi and Miyao 2005 -RRB- LFG -LRB- Cahill and van Genabith 2006 Hogan et al. 2007 -RRB- and CCG -LRB- White et al. 2007 -RRB- -RRB-	amod_White_2007 dep_White_al. nn_White_et num_Hogan_2007 nn_Hogan_al. nn_Hogan_et nn_Genabith_van dep_Cahill_Hogan dep_Cahill_2006 conj_and_Cahill_Genabith appos_LFG_Genabith appos_LFG_Cahill dep_Nakanishi_2005 conj_and_Nakanishi_Miyao dep_HPSG_White conj_and_HPSG_CCG conj_and_HPSG_LFG appos_HPSG_Miyao appos_HPSG_Nakanishi num_Belz_2007 conj_or_semiautomatically_automatically dep_semiautomatically_Belz advmod_extracted_automatically advmod_extracted_semiautomatically auxpass_extracted_are nsubjpass_extracted_grammars prep_in_extracted_which nn_grammars_generation det_grammars_the prep_such_as_approaches_CCG prep_such_as_approaches_LFG prep_such_as_approaches_HPSG rcmod_approaches_extracted amod_approaches_other nsubj_are_approaches expl_are_There ccomp_``_are
P08-1022	P06-1130	n	Even with the current incomplete set of semantic templates the hypertagger brings realizer performance roughly up to state-of-the-art levels as our overall test set BLEU score -LRB- 0.6701 -RRB- slightly exceeds that of Cahill and van Genabith -LRB- 2006 -RRB- though at a coverage of 96 % insteadof98 %	num_%_insteadof98 dep_%_% num_%_96 prep_of_coverage_% det_coverage_a pobj_at_coverage advmod_at_though prep_of_at_Genabith prep_of_at_Cahill mark_at_that appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Genabith ccomp_exceeds_at advmod_exceeds_slightly appos_score_0.6701 nn_score_BLEU amod_score_set dep_test_score amod_test_overall poss_test_our amod_levels_state-of-the-art prep_to_up_levels advmod_up_roughly nn_performance_realizer dep_brings_exceeds prep_as_brings_test advmod_brings_up dobj_brings_performance nsubj_brings_hypertagger prep_with_brings_set advmod_brings_Even det_hypertagger_the amod_templates_semantic prep_of_set_templates amod_set_incomplete amod_set_current det_set_the
P08-1022	P06-1130	o	-LRB- 2005 -RRB- and Cahill and van Genabith -LRB- 2006 -RRB- with HPSG and LFG grammars	nn_grammars_LFG nn_grammars_HPSG conj_and_HPSG_LFG pobj_with_grammars dep_with_Genabith dep_with_Cahill nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith conj_and_2005_with
W08-1111	P06-1130	o	One possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains -LRB- White et al. 2007 Cahill and van Genabith 2006 Zhong and Stent 2005 Langkilde-Geary 2002 Langkilde and Knight 1998 Elhadad 1991 -RRB-	amod_Elhadad_1991 num_Langkilde-Geary_2002 nn_Genabith_van num_Cahill_1998 conj_and_Cahill_Knight conj_and_Cahill_Langkilde conj_and_Cahill_Langkilde-Geary conj_and_Cahill_2005 conj_and_Cahill_Stent conj_and_Cahill_Zhong conj_and_Cahill_2006 conj_and_Cahill_Genabith dep_White_Elhadad conj_White_Knight conj_White_Langkilde conj_White_Langkilde-Geary conj_White_2005 conj_White_Stent conj_White_Zhong conj_White_2006 conj_White_Genabith conj_White_Cahill amod_White_2007 dep_White_al. nn_White_et nn_domains_application amod_domains_multiple prep_in_applicability_domains prep_for_aims_applicability nsubj_aims_that rcmod_realizer_aims nn_realizer_widecoverage det_realizer_a dep_exploit_White dobj_exploit_realizer aux_exploit_to xcomp_is_exploit nsubj_is_strategy amod_strategy_possible num_strategy_One ccomp_``_is
W08-1112	P06-1130	o	From the same treebank Cahill and van Genabith -LRB- 2006 -RRB- automatically extracted wide-coverage LFG approximations for a PCFG-based generation model	nn_model_generation amod_model_PCFG-based det_model_a nn_approximations_LFG amod_approximations_wide-coverage prep_for_extracted_model dobj_extracted_approximations advmod_extracted_automatically nsubj_extracted_Genabith nsubj_extracted_Cahill prep_from_extracted_treebank appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Genabith amod_treebank_same det_treebank_the
W08-1112	P06-1130	n	Our model improves the baseline provided by -LRB- Cahill and van Genabith 2006 -RRB- -LRB- i -RRB- accuracy is increased by creating a lexicalised PCFG grammar and enriching conditioning context with parent f-structure features and -LRB- ii -RRB- coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing	nn_smoothing_rule nn_techniques_matching amod_techniques_fuzzy conj_and_smoothing_techniques amod_smoothing_lexical prep_for_providing_smoothing dobj_providing_techniques dobj_providing_smoothing agent_increased_providing auxpass_increased_is nsubjpass_increased_coverage dep_increased_ii nn_features_f-structure nn_features_parent prep_with_context_features dep_conditioning_context dobj_enriching_conditioning nn_grammar_PCFG amod_grammar_lexicalised det_grammar_a conj_and_creating_enriching dobj_creating_grammar conj_and_increased_increased agent_increased_enriching agent_increased_creating auxpass_increased_is nsubjpass_increased_accuracy dep_increased_i nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith dep_by_Genabith dep_by_Cahill prep_provided_by vmod_baseline_provided det_baseline_the dep_improves_increased dep_improves_increased dobj_improves_baseline nsubj_improves_model poss_model_Our ccomp_``_improves
W08-1112	P06-1130	o	Based on this theoretical cornerstone Cahill and van Genabith -LRB- 2006 -RRB- presented a PCFG-based chart generator using wide-coverage LFG approximations automatically extracted from the Penn-II treebank	nn_treebank_Penn-II det_treebank_the prep_from_extracted_treebank advmod_extracted_automatically vmod_approximations_extracted nn_approximations_LFG amod_approximations_wide-coverage dobj_using_approximations nn_generator_chart amod_generator_PCFG-based det_generator_a vmod_presented_using dobj_presented_generator nsubj_presented_Genabith nsubj_presented_Cahill vmod_presented_Based appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Genabith amod_cornerstone_theoretical det_cornerstone_this prep_on_Based_cornerstone
W08-1112	P06-1130	o	Tbest = argmax T P -LRB- T | F -RRB- -LRB- 1 -RRB- P -LRB- T | F -RRB- = productdisplay X Y in T Feats = -LCB- ai | ai -LRB- X -RRB- -RCB- P -LRB- X Y | X Feats -RRB- -LRB- 2 -RRB- 3 Disambiguation Models The basic generation model presented in -LRB- Cahill and van Genabith 2006 -RRB- used simple probabilistic context-free grammars	amod_grammars_context-free amod_grammars_probabilistic amod_grammars_simple dobj_used_grammars dep_used_Genabith dep_used_Cahill mark_used_in nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith advcl_presented_used vmod_model_presented nn_model_generation amod_model_basic det_model_The dep_Models_model nn_Models_Disambiguation num_Models_3 dep_2_Models num_X_| dep_Y_Feats dep_Y_X nn_Y_X appos_P_Y dep_ai_X dep_|_P dep_|_ai dep_ai_2 dobj_ai_| dep_=_ai nn_Feats_T amod_Y_= prep_in_Y_Feats nn_Y_X nn_Y_productdisplay dobj_=_Y num_F_| nn_F_T amod_P_= appos_P_F xcomp_1_P num_F_| nn_F_T dep_P_1 appos_P_F nn_P_T nn_P_argmax dobj_=_P amod_Tbest_=
W08-1112	P06-1130	n	-LRB- 2007 -RRB- presented a history-based generation model to overcome some of the inappropriate independence assumptions in the basic generation model of -LRB- Cahill and van Genabith 2006 -RRB-	nn_Genabith_van dep_Cahill_2006 conj_and_Cahill_Genabith dep_of_Genabith dep_of_Cahill prep_model_of nn_model_generation amod_model_basic det_model_the prep_in_assumptions_model nn_assumptions_independence amod_assumptions_inappropriate det_assumptions_the prep_of_some_assumptions dobj_overcome_some aux_overcome_to vmod_model_overcome nn_model_generation amod_model_history-based det_model_a dobj_presented_model dep_presented_2007
W08-1122	P06-1130	o	-LRB- Cahill and van Genabith 2006 -RRB- and the third type is a mixture of the first and second type employing n-gram and grammarbased features e.g.	amod_features_grammarbased amod_features_n-gram conj_and_n-gram_grammarbased dobj_employing_features amod_type_second amod_type_first det_type_the conj_and_first_second dep_mixture_e.g. vmod_mixture_employing prep_of_mixture_type det_mixture_a cop_mixture_is nsubj_mixture_type amod_type_third det_type_the nn_Genabith_van conj_and_Cahill_mixture amod_Cahill_2006 conj_and_Cahill_Genabith dep_''_mixture dep_''_Genabith dep_''_Cahill
W08-1122	P06-1130	o	The generator used in our experiments is an instance of the second type using a probability model defined over Lexical Functional Grammar c-structure and f-structure annotations -LRB- Cahill and van Genabith 2006 Hogan et al. 2007 -RRB-	num_Hogan_2007 nn_Hogan_al. nn_Hogan_et nn_Genabith_van dep_Cahill_Hogan dep_Cahill_2006 conj_and_Cahill_Genabith nn_annotations_f-structure appos_c-structure_Genabith appos_c-structure_Cahill conj_and_c-structure_annotations nn_c-structure_Grammar amod_c-structure_Functional amod_c-structure_Lexical prep_over_defined_annotations prep_over_defined_c-structure vmod_model_defined nn_model_probability det_model_a dobj_using_model amod_type_second det_type_the vmod_instance_using prep_of_instance_type det_instance_an cop_instance_is nsubj_instance_generator poss_experiments_our prep_in_used_experiments vmod_generator_used det_generator_The
W08-1122	P06-1130	o	2 Background The natural language generator used in our experiments is the WSJ-trained system described in Cahill and van Genabith -LRB- 2006 -RRB- and Hogan et al.	dep_Hogan_al. nn_Hogan_et appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Hogan conj_and_Cahill_Genabith prep_in_described_Hogan prep_in_described_Genabith prep_in_described_Cahill vmod_system_described amod_system_WSJ-trained det_system_the cop_system_is nsubj_system_Background poss_experiments_our prep_in_used_experiments vmod_generator_used nn_generator_language amod_generator_natural det_generator_The dep_Background_generator num_Background_2
W08-1122	P06-1130	o	Cahill and van Genabith -LRB- 2006 -RRB- attain 98.2 % coverage and a BLEU score of 0.6652 on the standard WSJ test set -LRB- Section 23 -RRB-	num_Section_23 appos_set_Section nn_set_test nn_set_WSJ amod_set_standard det_set_the prep_of_score_0.6652 nn_score_BLEU det_score_a conj_and_coverage_score amod_coverage_% number_%_98.2 prep_on_attain_set dobj_attain_score dobj_attain_coverage nsubj_attain_Genabith nsubj_attain_Cahill appos_Genabith_2006 nn_Genabith_van conj_and_Cahill_Genabith
W09-0806	P06-1130	o	From this LFG annotated treebank large-scale unification grammar resources were automatically extracted and used in parsing -LRB- Cahill and al. 2008 -RRB- and generation -LRB- Cahill and van Genabith 2006 -RRB-	nn_Genabith_van amod_Cahill_2006 conj_and_Cahill_Genabith dep_generation_Genabith dep_generation_Cahill num_Cahill_2008 conj_and_Cahill_al. conj_and_parsing_generation appos_parsing_al. appos_parsing_Cahill prep_in_used_generation prep_in_used_parsing nsubjpass_used_resources conj_and_extracted_used advmod_extracted_automatically auxpass_extracted_were nsubjpass_extracted_resources prep_from_extracted_treebank nn_resources_grammar nn_resources_unification amod_resources_large-scale amod_treebank_annotated nn_treebank_LFG det_treebank_this
W09-0806	P06-1130	o	This approach was subsequently extended to other languages including German -LRB- Cahill and al. 2003 -RRB- Chinese -LRB- Burke 2004 -RRB- -LRB- Guo and al. 2007 -RRB- Spanish -LRB- ODonovan 2004 -RRB- -LRB- Chrupala and van Genabith 2006 -RRB- and French -LRB- Schluter and van Genabith 2008 -RRB-	nn_Genabith_van dep_Schluter_2008 conj_and_Schluter_Genabith dep_French_Genabith dep_French_Schluter nn_Genabith_van dep_Chrupala_2006 conj_and_Chrupala_Genabith amod_ODonovan_2004 appos_Spanish_ODonovan amod_Guo_2007 conj_and_Guo_al. amod_Burke_2004 dep_Chinese_Burke num_Cahill_2003 conj_and_Cahill_al. conj_and_German_French dep_German_Genabith dep_German_Chrupala conj_and_German_Spanish appos_German_al. appos_German_Guo conj_and_German_Chinese dep_German_al. dep_German_Cahill prep_including_languages_French prep_including_languages_Spanish prep_including_languages_Chinese prep_including_languages_German amod_languages_other prep_to_extended_languages advmod_extended_subsequently auxpass_extended_was nsubjpass_extended_approach det_approach_This
W09-0806	P06-1130	o	c2009 Association for Computational Linguistics Automatic Treebank-Based Acquisition of Arabic LFG Dependency Structures Lamia Tounsi Mohammed Attia NCLT School of Computing Dublin City University Ireland -LCB- lamia.tounsi mattia josef -RCB- @computing dcu.ie Josef van Genabith Abstract A number of papers have reported on methods for the automatic acquisition of large-scale probabilistic LFG-based grammatical resources from treebanks for English -LRB- Cahill and al. 2002 -RRB- -LRB- Cahill and al. 2004 -RRB- German -LRB- Cahill and al. 2003 -RRB- Chinese -LRB- Burke 2004 -RRB- -LRB- Guo and al. 2007 -RRB- Spanish -LRB- ODonovan 2004 -RRB- -LRB- Chrupala and van Genabith 2006 -RRB- and French -LRB- Schluter and van Genabith 2008 -RRB-	nn_Genabith_van dep_Schluter_2008 conj_and_Schluter_Genabith dep_French_Genabith dep_French_Schluter nn_Genabith_van dep_Chrupala_2006 conj_and_Chrupala_Genabith amod_ODonovan_2004 appos_Spanish_ODonovan amod_Guo_2007 conj_and_Guo_al. amod_Burke_2004 dep_Chinese_Burke num_Cahill_2003 conj_and_Cahill_al. conj_and_German_French appos_German_Genabith appos_German_Chrupala conj_and_German_Spanish appos_German_al. appos_German_Guo conj_and_German_Chinese dep_German_al. dep_German_Cahill num_Cahill_2004 conj_and_Cahill_al. amod_Cahill_2002 conj_and_Cahill_al. dep_English_al. dep_English_Cahill prep_for_treebanks_English amod_resources_grammatical amod_resources_LFG-based amod_resources_probabilistic amod_resources_large-scale prep_from_acquisition_treebanks prep_of_acquisition_resources amod_acquisition_automatic det_acquisition_the prep_for_methods_acquisition dep_reported_French dep_reported_Spanish dep_reported_Chinese dep_reported_German dep_reported_al. dep_reported_Cahill prep_on_reported_methods aux_reported_have nsubj_reported_number prep_of_number_papers nn_number_A nn_number_Abstract nn_number_Genabith nn_number_van nn_number_Josef nn_number_dcu.ie dep_number_Association dep_@computing_josef dep_@computing_lamia.tounsi appos_lamia.tounsi_mattia nn_lamia.tounsi_Ireland nn_University_City nn_University_Dublin prep_of_School_Computing dep_NCLT_@computing appos_NCLT_University appos_NCLT_School nn_NCLT_Attia nn_NCLT_Mohammed nn_NCLT_Tounsi nn_NCLT_Lamia nn_NCLT_Structures nn_NCLT_Dependency nn_NCLT_LFG nn_NCLT_Arabic prep_of_Acquisition_NCLT nn_Acquisition_Treebank-Based nn_Acquisition_Automatic nn_Acquisition_Linguistics nn_Acquisition_Computational prep_for_Association_Acquisition nn_Association_c2009
D09-1024	P07-1003	o	1 Introduction Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research for example -LRB- Brown et al. 1993 Ittycheriah and Roukos 2005 Fraser and Marcu 2007 -RRB- including work leveraging syntactic parse trees e.g. -LRB- Cherry and Lin 2006 DeNero and Klein 2007 Fossum et al. 2008 -RRB-	nn_al._et nn_al._Fossum num_DeNero_2007 conj_and_DeNero_Klein amod_Cherry_2008 dep_Cherry_al. dep_Cherry_Klein dep_Cherry_DeNero num_Cherry_2006 conj_and_Cherry_Lin appos_e.g._Lin appos_e.g._Cherry nn_trees_parse amod_trees_syntactic dobj_leveraging_trees vmod_work_leveraging dep_Ittycheriah_2007 conj_and_Ittycheriah_Marcu conj_and_Ittycheriah_Fraser conj_and_Ittycheriah_2005 conj_and_Ittycheriah_Roukos dep_Brown_Marcu dep_Brown_Fraser dep_Brown_2005 dep_Brown_Roukos dep_Brown_Ittycheriah amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_of_amount_research amod_amount_significant det_amount_a prep_received_e.g. prep_including_received_work appos_received_Brown prep_for_received_example dobj_received_amount aux_received_has nsubj_received_alignment nn_systems_translation nn_systems_machine amod_systems_statistical nn_systems_training conj_and_component_received prep_in_component_systems amod_component_critical det_component_a cop_component_is nsubj_component_alignment nn_alignment_Word nn_alignment_Introduction num_alignment_1
D09-1105	P07-1003	o	The second alternative used BerkeleyAligner -LRB- Liang et al. 2006 DeNero and Klein 2007 -RRB- which shares information between the two alignment directions to improve alignment quality	nn_quality_alignment dobj_improve_quality aux_improve_to nn_directions_alignment num_directions_two det_directions_the prep_between_information_directions vmod_shares_improve dobj_shares_information nsubj_shares_which dep_DeNero_2007 conj_and_DeNero_Klein dep_Liang_Klein dep_Liang_DeNero appos_Liang_2006 dep_Liang_al. nn_Liang_et rcmod_BerkeleyAligner_shares appos_BerkeleyAligner_Liang dobj_used_BerkeleyAligner vmod_alternative_used amod_alternative_second det_alternative_The ccomp_``_alternative
D09-1136	P07-1003	o	DeNero and Klein -LRB- 2007 -RRB- use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments	amod_alignments_syntax-friendly dobj_favor_alignments aux_favor_to vmod_model_favor nn_model_alignment nn_model_word nn_model_HMM det_model_an amod_distance_syntaxbased det_distance_a prep_in_use_model dobj_use_distance nsubj_use_Klein nsubj_use_DeNero appos_Klein_2007 conj_and_DeNero_Klein
D09-1136	P07-1003	o	The word alignment used in GHKM is usually computed independent ofthesyntacticstructure andasDeNeroandKlein -LRB- 2007 -RRB- and May and Knight -LRB- 2007 -RRB- have noted Ch-En En-Ch Union Heuristic 28.6 % 33.0 % 45.9 % 20.1 % Table 1 Percentage of corpus used to generate big templates based on different word alignments 9-12 13-20 21 Ch-En 18.2 % 17.4 % 64.4 % En-Ch 15.9 % 20.7 % 63.4 % Union 9.8 % 15.1 % 75.1 % Heuristic 24.6 % 27.9 % 47.5 % Table 2 In the selected big templates the distribution of words in the templates of different sizes which are measured based on the number of symbols in their RHSs is not the best for SSMT systems	nn_systems_SSMT prep_for_best_systems det_best_the neg_best_not cop_best_is nsubj_best_distribution prep_in_best_templates poss_RHSs_their prep_in_symbols_RHSs prep_of_number_symbols det_number_the pobj_measured_number prepc_based_on_measured_on auxpass_measured_are nsubjpass_measured_which rcmod_sizes_measured amod_sizes_different prep_of_templates_sizes det_templates_the prep_in_words_templates prep_of_distribution_words det_distribution_the amod_templates_big amod_templates_selected det_templates_the num_Table_2 dep_%_Table num_%_47.5 dep_%_% num_%_27.9 dep_%_% num_%_24.6 dep_Heuristic_% amod_%_Heuristic num_%_75.1 dep_%_% num_%_15.1 dep_%_% num_%_9.8 dep_Union_best dep_Union_% amod_Union_% number_%_63.4 npadvmod_%_Union num_%_20.7 dep_%_% num_%_15.9 dep_En-Ch_% amod_%_En-Ch num_%_64.4 dep_%_% num_%_17.4 dep_%_% num_%_18.2 dep_Ch-En_% dep_21_Ch-En dep_13-20_21 number_13-20_9-12 dep_alignments_13-20 nn_alignments_word amod_alignments_different prep_on_based_alignments amod_templates_big dobj_generate_templates aux_generate_to xcomp_used_generate vmod_Percentage_based vmod_Percentage_used prep_of_Percentage_corpus num_Table_1 dep_%_Table num_%_20.1 dep_%_% num_%_45.9 dep_%_% num_%_33.0 dep_%_% num_%_28.6 dep_Heuristic_Percentage dep_Heuristic_% nn_Heuristic_Union nn_Heuristic_En-Ch amod_Heuristic_Ch-En dep_noted_Heuristic aux_noted_have dep_noted_computed appos_Knight_2007 appos_andasDeNeroandKlein_2007 conj_and_ofthesyntacticstructure_Knight conj_and_ofthesyntacticstructure_May conj_and_ofthesyntacticstructure_andasDeNeroandKlein amod_ofthesyntacticstructure_independent dobj_computed_Knight dobj_computed_May dobj_computed_andasDeNeroandKlein dobj_computed_ofthesyntacticstructure advmod_computed_usually auxpass_computed_is nsubjpass_computed_alignment prep_in_used_GHKM vmod_alignment_used nn_alignment_word det_alignment_The
P09-1104	P07-1003	o	When we trained external Chinese models we used the same unlabeled data set as DeNero and Klein -LRB- 2007 -RRB- including the bilingual dictionary	amod_dictionary_bilingual det_dictionary_the appos_Klein_2007 conj_and_DeNero_Klein prep_as_set_Klein prep_as_set_DeNero vmod_data_set amod_data_unlabeled amod_data_same det_data_the prep_including_used_dictionary dobj_used_data nsubj_used_we advcl_used_trained amod_models_Chinese amod_models_external dobj_trained_models nsubj_trained_we advmod_trained_When
P09-1104	P07-1003	o	For example the HMM aligner achieves an AER of 20.7 when using the competitive thresholding heuristic of DeNero and Klein -LRB- 2007 -RRB-	appos_Klein_2007 conj_and_DeNero_Klein prep_of_heuristic_Klein prep_of_heuristic_DeNero amod_heuristic_thresholding amod_heuristic_competitive det_heuristic_the dobj_using_heuristic advmod_using_when prep_of_AER_20.7 det_AER_an advcl_achieves_using dobj_achieves_AER nsubj_achieves_aligner prep_for_achieves_example nn_aligner_HMM det_aligner_the
P09-1104	P07-1003	o	We also trained an HMM aligner as described in DeNero and Klein -LRB- 2007 -RRB- and used the posteriors of this model as features	det_model_this prep_of_posteriors_model det_posteriors_the prep_as_used_features dobj_used_posteriors appos_Klein_2007 conj_and_DeNero_Klein conj_and_described_used prep_in_described_Klein prep_in_described_DeNero mark_described_as nn_aligner_HMM det_aligner_an advcl_trained_used advcl_trained_described dobj_trained_aligner advmod_trained_also nsubj_trained_We
P09-1104	P07-1003	o	thresholding -LRB- DeNero and Klein 2007 -RRB-	amod_DeNero_2007 conj_and_DeNero_Klein dep_thresholding_Klein dep_thresholding_DeNero
W08-0306	P07-1003	o	-LRB- Lopez and Resnik 2005 -RRB- and -LRB- Denero and Klein 2007 -RRB- modify the distortion model of the HMM alignment model -LRB- Vogel et al. 1996 -RRB- to reflect tree distance rather than string distance -LRB- Cherry and Lin 2006 -RRB- modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints	nn_constraints_bracketing amod_constraints_syntactic dobj_violate_constraints nsubj_violate_that rcmod_parses_violate amod_parses_induced prep_for_penalty_parses det_penalty_a dobj_introducing_penalty nn_aligner_ITG det_aligner_an prepc_by_modify_introducing dobj_modify_aligner amod_Cherry_2006 conj_and_Cherry_Lin nn_distance_string conj_negcc_distance_distance nn_distance_tree dobj_reflect_distance dobj_reflect_distance aux_reflect_to amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et nn_model_alignment nn_model_HMM det_model_the prep_of_model_model nn_model_distortion det_model_the dep_modify_modify parataxis_modify_Lin parataxis_modify_Cherry vmod_modify_reflect dep_modify_Vogel dobj_modify_model nsubj_modify_Denero nsubj_modify_Resnik nsubj_modify_Lopez dep_Denero_2007 conj_and_Denero_Klein conj_and_Lopez_Klein conj_and_Lopez_Denero amod_Lopez_2005 conj_and_Lopez_Resnik
W08-0308	P07-1003	o	For example the word alignment computed by GIZA + + and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT -LRB- DeNero and Klein 2007 May and Knight 2007 -RRB- due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and could produce many syntax-violating word alignments	nn_alignments_word amod_alignments_syntax-violating amod_alignments_many dobj_produce_alignments aux_produce_could nsubj_produce_models det_sentences_the prep_of_structure_sentences amod_structure_syntactic det_structure_the conj_and_aware_produce prep_of_aware_structure neg_aware_not cop_aware_are nsubj_aware_models mark_aware_that nn_models_alignment amod_models_word-based det_models_the ccomp_fact_produce ccomp_fact_aware det_fact_the num_May_2007 conj_and_May_Knight prep_due_to_DeNero_fact dep_DeNero_Knight dep_DeNero_May conj_and_DeNero_2007 conj_and_DeNero_Klein dep_SSMT_2007 dep_SSMT_Klein dep_SSMT_DeNero prep_for_problem_SSMT det_problem_a cop_problem_be aux_problem_to xcomp_observed_problem auxpass_observed_been aux_observed_has nsubjpass_observed_alignment prep_for_observed_example amod_systems_SSMT amod_systems_most nn_templates_TTS det_templates_the prep_in_extract_systems dobj_extract_templates aux_extract_to vmod_basis_extract det_basis_a conj_and_GIZA_used conj_+_GIZA_+ prep_as_computed_basis agent_computed_used agent_computed_+ agent_computed_GIZA vmod_alignment_computed nn_alignment_word det_alignment_the rcmod_``_observed
W08-0308	P07-1003	o	Approaches have been proposed recently towards getting better word alignment and thus better TTS templates such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein -LRB- 2007 -RRB- and build62 ing a syntax-based word alignment model May and Knight -LRB- 2007 -RRB- with TTS templates	nn_templates_TTS appos_Knight_2007 conj_and_May_Knight nn_May_model nn_May_alignment nn_May_word amod_May_syntax-based det_May_a prep_with_ing_templates tmod_ing_Knight tmod_ing_May nn_ing_build62 appos_Klein_2007 conj_and_DeNero_Klein nn_DeNero_model nn_DeNero_alignment nn_DeNero_word amod_DeNero_HMM-based det_DeNero_the nn_information_structure amod_information_syntactic prep_into_encoding_Klein prep_into_encoding_DeNero dobj_encoding_information prepc_such_as_templates_encoding nn_templates_TTS amod_templates_better advmod_better_thus conj_and_alignment_templates nn_alignment_word amod_alignment_better dobj_getting_templates dobj_getting_alignment conj_and_proposed_ing prepc_towards_proposed_getting advmod_proposed_recently auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Approaches
W08-0308	P07-1003	n	DeNero and Klein -LRB- 2007 -RRB- focus on alignment and do not present MT results while May and Knight -LRB- 2007 -RRB- takesthesyntacticre-alignmentasaninputtoanEM algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplates -LRB- Galleyetal. ,2006 -RRB-	num_Galleyetal._,2006 dep_combinedintobiggertemplates_Galleyetal. amod_combinedintobiggertemplates_insertedintothetemplatesandminimumtemplatesare cop_combinedintobiggertemplates_are nsubj_combinedintobiggertemplates_words advmod_combinedintobiggertemplates_where nn_words_target amod_words_unaligned det_words_the amod_algorithm_takesthesyntacticre-alignmentasaninputtoanEM num_algorithm_2007 nn_algorithm_Knight rcmod_May_combinedintobiggertemplates conj_and_May_algorithm nn_results_MT amod_results_present aux_results_do neg_present_not prep_while_focus_algorithm prep_while_focus_May conj_and_focus_results prep_on_focus_alignment nsubj_focus_Klein nsubj_focus_DeNero appos_Klein_2007 conj_and_DeNero_Klein
N09-1067	P07-1009	o	We use the version extracted and preprocessed by Daume III and Campbell -LRB- 2007 -RRB-	appos_Campbell_2007 conj_and_III_Campbell nn_III_Daume agent_extracted_Campbell agent_extracted_III conj_and_extracted_preprocessed vmod_version_preprocessed vmod_version_extracted det_version_the dobj_use_version nsubj_use_We
C08-1029	P07-1010	o	One way of obtaining a suitable granularity of nodes is to introduce latent classes such as the Semi-Markov class model -LRB- Okanohara and Tsujii 2007 -RRB-	amod_Okanohara_2007 conj_and_Okanohara_Tsujii dep_model_Tsujii dep_model_Okanohara nn_model_class nn_model_Semi-Markov det_model_the prep_such_as_classes_model amod_classes_latent dobj_introduce_classes aux_introduce_to xcomp_is_introduce nsubj_is_way prep_of_granularity_nodes amod_granularity_suitable det_granularity_a dobj_obtaining_granularity prepc_of_way_obtaining num_way_One ccomp_``_is
D08-1006	P07-1010	o	More recently however Okanohara and Tsujii -LRB- 2007 -RRB- showed that a 1 Conditional maximum entropy models -LRB- Rosenfeld 1996 -RRB- provide somewhat of a counter-example but there too many kinds of global and non-local features are difficult to use -LRB- Rosenfeld 1997 -RRB-	amod_Rosenfeld_1997 dep_use_Rosenfeld aux_use_to xcomp_difficult_use cop_difficult_are nsubj_difficult_kinds advmod_difficult_too advmod_difficult_there amod_features_non-local amod_features_global conj_and_global_non-local prep_of_kinds_features amod_kinds_many det_counter-example_a prep_of_provide_counter-example advmod_provide_somewhat nsubj_provide_models mark_provide_that amod_Rosenfeld_1996 appos_models_Rosenfeld nn_models_entropy nn_models_maximum amod_models_Conditional num_models_1 det_models_a conj_but_showed_difficult ccomp_showed_provide nsubj_showed_Tsujii nsubj_showed_Okanohara advmod_showed_however advmod_showed_recently appos_Tsujii_2007 conj_and_Okanohara_Tsujii advmod_recently_More
D08-1006	P07-1010	o	Unfortunately as shown in -LRB- Okanohara and Tsujii 2007 -RRB- with the represetation of sentences that we use linear classifiers can not discriminate real sentences from sentences sampled from a trigram which is the model we use as a baseline so here we resort to a non-linear large-margin classifier -LRB- see section 3 for details -RRB-	prep_for_section_details num_section_3 dobj_see_section amod_classifier_large-margin amod_classifier_non-linear det_classifier_a dep_resort_see prep_to_resort_classifier nsubj_resort_we advmod_resort_here advmod_here_so det_baseline_a prep_as_use_baseline nsubj_use_we rcmod_model_use det_model_the cop_model_is nsubj_model_which rcmod_trigram_resort rcmod_trigram_model det_trigram_a prep_from_sampled_trigram vmod_sentences_sampled amod_sentences_real prep_from_discriminate_sentences dobj_discriminate_sentences neg_discriminate_not aux_discriminate_can nsubj_discriminate_classifiers amod_classifiers_linear ccomp_use_discriminate nsubj_use_we dobj_use_that prep_of_represetation_sentences det_represetation_the dep_Okanohara_use prep_with_Okanohara_represetation num_Okanohara_2007 conj_and_Okanohara_Tsujii prep_in_shown_Tsujii prep_in_shown_Okanohara mark_shown_as advcl_,_shown advcl_``_Unfortunately
D08-1006	P07-1010	o	As shown in -LRB- Okanohara and Tsujii 2007 -RRB- using this representation a linear classifier can not distinguish sentences sampled from a trigram and real sentences	amod_sentences_real conj_and_trigram_sentences det_trigram_a prep_from_sampled_sentences prep_from_sampled_trigram vmod_sentences_sampled dobj_distinguish_sentences neg_distinguish_not aux_distinguish_can nsubj_distinguish_classifier vmod_distinguish_using advcl_distinguish_shown amod_classifier_linear det_classifier_a det_representation_this dobj_using_representation dep_Okanohara_2007 conj_and_Okanohara_Tsujii prep_in_shown_Tsujii prep_in_shown_Okanohara mark_shown_As
D08-1007	P07-1010	o	Our technique of generating negative examples is similar to the approach of Okanohara and Tsujii -LRB- 2007 -RRB-	appos_Tsujii_2007 conj_and_Okanohara_Tsujii prep_of_approach_Tsujii prep_of_approach_Okanohara det_approach_the prep_to_similar_approach cop_similar_is nsubj_similar_technique amod_examples_negative dobj_generating_examples prepc_of_technique_generating poss_technique_Our
P08-2056	P07-1010	o	Artificial ungrammaticalities have been used in various NLP tasks -LRB- Smith and Eisner 2005 Okanohara and Tsujii 2007 -RRB- The idea of an automatically generated ungrammatical treebank was proposed by Foster -LRB- 2007 -RRB-	appos_Foster_2007 agent_proposed_Foster auxpass_proposed_was nsubjpass_proposed_idea amod_treebank_ungrammatical amod_treebank_generated det_treebank_an advmod_generated_automatically prep_of_idea_treebank det_idea_The dep_Okanohara_2007 conj_and_Okanohara_Tsujii rcmod_Smith_proposed dep_Smith_Tsujii dep_Smith_Okanohara conj_and_Smith_2005 conj_and_Smith_Eisner dep_tasks_2005 dep_tasks_Eisner dep_tasks_Smith nn_tasks_NLP amod_tasks_various prep_in_used_tasks auxpass_used_been aux_used_have nsubjpass_used_ungrammaticalities amod_ungrammaticalities_Artificial
W09-2112	P07-1010	o	Examples are Andersen -LRB- 2006 2007 -RRB- Okanohara and Tsujii -LRB- 2007 -RRB- Sun et al.	nn_al._et nn_al._Sun appos_Tsujii_2007 dep_2006_2007 conj_and_Andersen_al. conj_and_Andersen_Tsujii conj_and_Andersen_Okanohara dep_Andersen_2006 nsubj_are_al. nsubj_are_Tsujii nsubj_are_Okanohara nsubj_are_Andersen dep_Examples_are
W09-2112	P07-1010	o	Both Okanohara and Tsujii -LRB- 2007 -RRB- and Wagner et al.	dep_Wagner_al. nn_Wagner_et appos_Tsujii_2007 conj_and_Okanohara_Wagner conj_and_Okanohara_Tsujii preconj_Okanohara_Both
W09-2112	P07-1010	o	Okanohara and Tsujii -LRB- 2007 -RRB- generate ill-formed sentences by sampling a probabilistic language model and end up with pseudo-negative examples which resemble machine translation output more than they do learner texts	amod_texts_learner dobj_do_texts nsubj_do_they mark_do_than advcl_more_do nn_output_translation nn_output_machine advmod_resemble_more dobj_resemble_output nsubj_resemble_which rcmod_examples_resemble amod_examples_pseudo-negative prep_with_end_examples prt_end_up nn_model_language amod_model_probabilistic det_model_a conj_and_sampling_end dobj_sampling_model amod_sentences_ill-formed prep_by_generate_end prep_by_generate_sampling dobj_generate_sentences nsubj_generate_Tsujii nsubj_generate_Okanohara appos_Tsujii_2007 conj_and_Okanohara_Tsujii
C08-1144	P07-1019	o	Adaptations to the algorithms in the presence of ngram LMs are discussed in -LRB- Chiang 2007 Venugopal et al. 2007 Huang and Chiang 2007 -RRB-	amod_Huang_2007 conj_and_Huang_Chiang num_Venugopal_2007 nn_Venugopal_al. nn_Venugopal_et dep_Chiang_Chiang dep_Chiang_Huang conj_Chiang_Venugopal appos_Chiang_2007 dep_in_Chiang prep_discussed_in auxpass_discussed_are nsubjpass_discussed_Adaptations nn_LMs_ngram prep_of_presence_LMs det_presence_the det_algorithms_the prep_in_Adaptations_presence prep_to_Adaptations_algorithms
D08-1012	P07-1019	o	Huang and Chiang -LRB- 2007 -RRB- searches with the full model but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration	dobj_limit_exploration aux_limit_to dep_limit_order mark_limit_in advcl_trigger_limit aux_trigger_can nsubj_trigger_model dep_trigger_assumptions nn_model_language det_model_the prep_of_amount_reordering det_amount_the det_amount_the prep_about_assumptions_amount ccomp_makes_trigger amod_model_full det_model_the nn_searches_Chiang appos_Chiang_2007 conj_but_Huang_makes prep_with_Huang_model conj_and_Huang_searches
D08-1022	P07-1019	o	The forest concept is also used in machine translation decoding for example to characterize the search space of decoding with integrated language models -LRB- Huang and Chiang 2007 -RRB-	amod_Huang_2007 conj_and_Huang_Chiang dep_models_Chiang dep_models_Huang nn_models_language amod_models_integrated prep_with_decoding_models prepc_of_space_decoding nn_space_search det_space_the dobj_characterize_space aux_characterize_to nn_decoding_translation nn_decoding_machine xcomp_used_characterize prep_for_used_example prep_in_used_decoding advmod_used_also auxpass_used_is nsubjpass_used_concept nn_concept_forest det_concept_The
D09-1007	P07-1019	o	These include cube pruning -LRB- Chiang 2007 -RRB- cube growing -LRB- Huang and Chiang 2007 -RRB- early pruning -LRB- Moore and Quirk 2007 -RRB- closing spans -LRB- Roark and Hollingshead 2008 Roark and Hollingshead 2009 -RRB- coarse-to-fine methods -LRB- Petrov et al. 2008 -RRB- pervasive laziness -LRB- Pust and Knight 2009 -RRB- and many more	amod_more_many dep_Pust_2009 conj_and_Pust_Knight appos_laziness_Knight appos_laziness_Pust amod_laziness_pervasive amod_Petrov_2008 dep_Petrov_al. nn_Petrov_et dep_methods_Petrov amod_methods_coarse-to-fine dep_Roark_2009 conj_and_Roark_Hollingshead dep_Roark_Hollingshead dep_Roark_Roark conj_and_Roark_2008 conj_and_Roark_Hollingshead appos_spans_2008 appos_spans_Hollingshead appos_spans_Roark amod_spans_closing dep_Moore_2007 conj_and_Moore_Quirk appos_pruning_Quirk appos_pruning_Moore amod_pruning_early dep_Huang_2007 conj_and_Huang_Chiang dep_growing_Chiang dep_growing_Huang vmod_cube_growing dep_Chiang_2007 conj_and_pruning_more conj_and_pruning_laziness conj_and_pruning_methods conj_and_pruning_spans conj_and_pruning_pruning appos_pruning_cube appos_pruning_Chiang nn_pruning_cube dobj_include_more dobj_include_laziness dobj_include_methods dobj_include_spans dobj_include_pruning dobj_include_pruning nsubj_include_These ccomp_``_include
D09-1108	P07-1019	p	In the SMT research community the second step has been well studied and many methods have been proposed to speed up the decoding process such as node-based or span-based beam search with different pruning strategies -LRB- Liu et al. 2006 Zhang et al. 2008a 2008b -RRB- and cube pruning -LRB- Huang and Chiang 2007 Mi et al. 2008 -RRB-	num_Mi_2008 nn_Mi_al. nn_Mi_et dep_Huang_Mi conj_and_Huang_2007 conj_and_Huang_Chiang dep_pruning_2007 dep_pruning_Chiang dep_pruning_Huang nn_pruning_cube appos_Zhang_2008b appos_Zhang_2008a dep_Zhang_al. nn_Zhang_et dep_Liu_Zhang appos_Liu_2006 dep_Liu_al. nn_Liu_et nn_strategies_pruning amod_strategies_different prep_with_search_strategies nn_search_beam amod_search_span-based amod_search_node-based conj_or_node-based_span-based prep_such_as_process_search nn_process_decoding det_process_the conj_and_speed_pruning dep_speed_Liu dobj_speed_process prt_speed_up aux_speed_to xcomp_proposed_pruning xcomp_proposed_speed auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_many conj_and_studied_proposed advmod_studied_well auxpass_studied_been aux_studied_has nsubjpass_studied_step prep_in_studied_community amod_step_second det_step_the nn_community_research nn_community_SMT det_community_the
D09-1123	P07-1019	o	To circumvent these computational limitations various pruning techniques are usually needed e.g. -LRB- Huang and Chiang 2007 -RRB-	amod_Chiang_2007 conj_and_Huang_Chiang appos_e.g._Chiang appos_e.g._Huang prep_needed_e.g. advmod_needed_usually auxpass_needed_are nsubjpass_needed_techniques advcl_needed_circumvent nn_techniques_pruning amod_techniques_various amod_limitations_computational det_limitations_these dobj_circumvent_limitations aux_circumvent_To ccomp_``_needed
D09-1147	P07-1019	p	To speed our computations we use the cube pruning method of Huang and Chiang -LRB- 2007 -RRB- with a fixed beam size	nn_size_beam amod_size_fixed det_size_a appos_Chiang_2007 conj_and_Huang_Chiang prep_of_method_Chiang prep_of_method_Huang nn_method_pruning nn_method_cube det_method_the prep_with_use_size dobj_use_method nsubj_use_we advcl_use_speed poss_computations_our dobj_speed_computations aux_speed_To
D09-1147	P07-1019	o	3.1 Translation Model Form We first assume the general hypergraph setting of Huang and Chiang -LRB- 2007 -RRB- namely that derivations under our translation model form a hypergraph	det_hypergraph_a dobj_form_hypergraph nsubj_form_derivations mark_form_that nn_model_translation poss_model_our prep_under_derivations_model appos_Chiang_2007 conj_and_Huang_Chiang ccomp_setting_form advmod_setting_namely prep_of_setting_Chiang prep_of_setting_Huang nn_setting_hypergraph amod_setting_general det_setting_the dobj_assume_setting advmod_assume_first nsubj_assume_We rcmod_Form_assume nn_Form_Model nn_Form_Translation num_Form_3.1 dep_``_Form
E09-1044	P07-1019	p	Hiero Search Refinements Huang and Chiang -LRB- 2007 -RRB- offer several refinements to cube pruning to improve translation speed	nn_speed_translation dobj_improve_speed aux_improve_to nn_pruning_cube amod_refinements_several vmod_offer_improve prep_to_offer_pruning dobj_offer_refinements appos_Chiang_2007 conj_and_Huang_Chiang nn_Huang_Refinements dep_Search_offer dobj_Search_Chiang dobj_Search_Huang nsubj_Search_Hiero
E09-1061	P07-1019	n	13Huang and Chiang -LRB- 2007 -RRB- give an informal example but do not elaborate on it	prep_on_elaborate_it neg_elaborate_not aux_elaborate_do nsubj_elaborate_13Huang amod_example_informal det_example_an conj_but_give_elaborate dobj_give_example nsubj_give_Chiang nsubj_give_13Huang appos_Chiang_2007 conj_and_13Huang_Chiang
N09-1026	P07-1019	p	Recent innovations have greatly improved the efficiency of language model integration through multipass techniques such as forest reranking -LRB- Huang and Chiang 2007 -RRB- local search -LRB- Venugopal et al. 2007 -RRB- and coarse-to-fine pruning -LRB- Petrov et al. 2008 Zhang and Gildea 2008 -RRB-	dep_Zhang_2008 conj_and_Zhang_Gildea dep_Petrov_Gildea dep_Petrov_Zhang appos_Petrov_2008 dep_Petrov_al. nn_Petrov_et nn_pruning_coarse-to-fine amod_Venugopal_2007 dep_Venugopal_al. nn_Venugopal_et dep_search_Venugopal amod_search_local dep_Huang_2007 conj_and_Huang_Chiang conj_and_reranking_pruning conj_and_reranking_search appos_reranking_Chiang appos_reranking_Huang nn_reranking_forest prep_such_as_techniques_pruning prep_such_as_techniques_search prep_such_as_techniques_reranking nn_techniques_multipass nn_integration_model nn_integration_language prep_through_efficiency_techniques prep_of_efficiency_integration det_efficiency_the dep_improved_Petrov dobj_improved_efficiency advmod_improved_greatly aux_improved_have nsubj_improved_innovations amod_innovations_Recent
N09-1026	P07-1019	o	As an alternative Huang and Chiang -LRB- 2007 -RRB- describes a forest-based reranking algorithm called cube growing which also employs beam search but focuses computation only where necessary in a top-down pass through a parse forest	amod_forest_parse det_forest_a prep_through_pass_forest amod_pass_top-down det_pass_a prep_in_necessary_pass advmod_necessary_where advmod_where_only rcmod_computation_necessary dobj_focuses_computation nsubj_focuses_Huang nn_search_beam dobj_employs_search advmod_employs_also nsubj_employs_which ccomp_growing_employs nsubj_growing_cube dep_growing_called vmod_algorithm_growing nn_algorithm_reranking amod_algorithm_forest-based det_algorithm_a conj_but_describes_focuses dobj_describes_algorithm nsubj_describes_Chiang nsubj_describes_Huang prep_as_describes_alternative appos_Chiang_2007 conj_and_Huang_Chiang det_alternative_an
N09-1026	P07-1019	o	3Huang and Chiang -LRB- 2007 -RRB- describes the cube growing algorithm in further detail including the precise form of the successor function for derivations	prep_for_function_derivations nn_function_successor det_function_the prep_of_form_function amod_form_precise det_form_the amod_detail_further amod_algorithm_growing nn_algorithm_cube det_cube_the prep_including_describes_form prep_in_describes_detail dobj_describes_algorithm nsubj_describes_Chiang nsubj_describes_3Huang appos_Chiang_2007 conj_and_3Huang_Chiang
N09-1027	P07-1019	o	In a second top-down pass similar to Huang and Chiang -LRB- 2007 -RRB- we can recalculate psyn -LRB- d -RRB- for alternative derivations in the hypergraph potentially correcting search errors made in the first pass	amod_pass_first det_pass_the prep_in_made_pass vmod_errors_made nn_errors_search dobj_correcting_errors advmod_correcting_potentially det_hypergraph_the prep_in_derivations_hypergraph amod_derivations_alternative appos_psyn_d parataxis_recalculate_correcting prep_for_recalculate_derivations dobj_recalculate_psyn aux_recalculate_can nsubj_recalculate_we prep_in_recalculate_pass appos_Chiang_2007 conj_and_Huang_Chiang prep_to_similar_Chiang prep_to_similar_Huang amod_pass_similar amod_pass_top-down amod_pass_second det_pass_a
N09-1049	P07-1019	p	433 Hiero Search Refinements Huang and Chiang -LRB- 2007 -RRB- offer several refinements to cube pruning to improve translation speed	nn_speed_translation dobj_improve_speed aux_improve_to nn_pruning_cube amod_refinements_several vmod_offer_improve prep_to_offer_pruning dobj_offer_refinements appos_Chiang_2007 conj_and_Huang_Chiang nn_Huang_Refinements dep_Search_offer dobj_Search_Chiang dobj_Search_Huang nsubj_Search_Hiero num_Hiero_433
N09-2003	P07-1019	p	1 Introduction A hypergraph as demonstrated by Huang and Chiang -LRB- 2007 -RRB- is a compact data-structure that can encode an exponential number of hypotheses generated by a regular phrase-based machine translation -LRB- MT -RRB- system -LRB- e.g. Koehn et al.	nn_al._et nn_al._Koehn dep_e.g._al. dep_system_e.g. nn_system_MT amod_translation_system nn_translation_machine amod_translation_phrase-based amod_translation_regular det_translation_a agent_generated_translation vmod_hypotheses_generated prep_of_number_hypotheses amod_number_exponential det_number_an dobj_encode_number aux_encode_can nsubj_encode_that rcmod_data-structure_encode amod_data-structure_compact det_data-structure_a cop_data-structure_is dep_data-structure_demonstrated dep_data-structure_hypergraph appos_Chiang_2007 conj_and_Huang_Chiang prep_by_demonstrated_Chiang prep_by_demonstrated_Huang mark_demonstrated_as det_hypergraph_A nn_hypergraph_Introduction num_hypergraph_1
N09-2036	P07-1019	o	Taken together with cube pruning -LRB- Chiang 2007 -RRB- k-best tree extraction -LRB- Huang and Chiang 2005 -RRB- and cube growing -LRB- Huang and Chiang 2007 -RRB- these results provide evidence that lazy techniques may penetrate deeper yet into MT decoding and other NLP search problems	nn_problems_search nn_problems_NLP amod_problems_other conj_and_decoding_problems nn_decoding_MT prep_into_deeper_problems prep_into_deeper_decoding advmod_deeper_yet acomp_penetrate_deeper aux_penetrate_may nsubj_penetrate_techniques mark_penetrate_that amod_techniques_lazy ccomp_evidence_penetrate dobj_provide_evidence nsubj_provide_results vmod_provide_Taken det_results_these amod_Huang_2007 conj_and_Huang_Chiang dep_growing_Chiang dep_growing_Huang vmod_cube_growing dep_Huang_2005 conj_and_Huang_Chiang appos_extraction_Chiang appos_extraction_Huang nn_extraction_tree amod_extraction_k-best dep_Chiang_2007 conj_and_pruning_cube conj_and_pruning_extraction appos_pruning_Chiang nn_pruning_cube prep_together_with_Taken_cube prep_together_with_Taken_extraction prep_together_with_Taken_pruning
N09-2036	P07-1019	o	Huang and Chiang -LRB- 2007 -RRB- de143 5x108 1x109 1.5 x109 2x109 2.5 x109 3x109 edges created 42000 43000 44000 45000 model cost lazy cube generation exhaustive cube generation Figure 3 Number of edges produced by the decoder versus model cost of 1-best decodings	amod_decodings_1-best prep_of_cost_decodings nn_cost_model det_decoder_the agent_produced_decoder vmod_edges_produced conj_versus_Number_cost prep_of_Number_edges dep_Figure_cost dep_Figure_Number num_Figure_3 nn_Figure_generation nn_Figure_cube amod_Figure_exhaustive nn_Figure_generation nn_Figure_cube amod_Figure_lazy nn_Figure_cost nn_Figure_model dep_Figure_45000 num_Figure_42000 dep_45000_44000 number_44000_43000 dep_created_Figure auxpass_created_edges nsubjpass_created_3x109 nsubjpass_created_Huang nn_3x109_x109 num_3x109_2.5 nn_3x109_x109 num_3x109_1.5 nn_3x109_5x108 nn_3x109_de143 num_3x109_2007 nn_3x109_Chiang number_2.5_2x109 number_1.5_1x109 conj_and_Huang_3x109
P08-1023	P07-1019	o	For 1-best search we use the cube pruning technique -LRB- Chiang 2007 Huang and Chiang 2007 -RRB- which approximately intersects the translation forest with the LM	det_LM_the nn_forest_translation det_forest_the prep_with_intersects_LM dobj_intersects_forest advmod_intersects_approximately nsubj_intersects_which amod_Huang_2007 conj_and_Huang_Chiang dep_Chiang_Chiang dep_Chiang_Huang appos_Chiang_2007 rcmod_technique_intersects appos_technique_Chiang nn_technique_pruning nn_technique_cube det_technique_the dobj_use_technique nsubj_use_we prep_for_use_search amod_search_1-best
P08-1025	P07-1019	o	But we did not use any LM estimate to achieve early stopping as suggested by Huang and Chiang -LRB- 2007 -RRB-	appos_Chiang_2007 conj_and_Huang_Chiang prep_by_suggested_Chiang prep_by_suggested_Huang mark_suggested_as advcl_stopping_suggested amod_stopping_early xcomp_achieve_stopping aux_achieve_to nn_estimate_LM det_estimate_any vmod_use_achieve dobj_use_estimate neg_use_not aux_use_did nsubj_use_we cc_use_But
P08-1025	P07-1019	o	The cube-pruning by Chiang -LRB- 2007 -RRB- and the lazy cube-pruning of Huang and Chiang -LRB- 2007 -RRB- turn the computation of beam pruning of CYK decoders into a top-k selection problem given two columns of translation hypotheses that need to be combined	auxpass_combined_be aux_combined_to xcomp_need_combined nsubj_need_that rcmod_hypotheses_need nn_hypotheses_translation prep_of_columns_hypotheses num_columns_two dobj_given_columns vmod_problem_given nn_problem_selection amod_problem_top-k det_problem_a nn_decoders_CYK prep_of_pruning_decoders nn_pruning_beam prep_of_computation_pruning det_computation_the prep_into_turn_problem dobj_turn_computation nsubj_turn_cube-pruning appos_Chiang_2007 conj_and_Huang_Chiang prep_of_cube-pruning_Chiang prep_of_cube-pruning_Huang amod_cube-pruning_lazy det_cube-pruning_the conj_and_Chiang_cube-pruning appos_Chiang_2007 prep_by_cube-pruning_cube-pruning prep_by_cube-pruning_Chiang det_cube-pruning_The
P08-1067	P07-1019	o	So we propose forest reranking a technique inspired by forest rescoring -LRB- Huang and Chiang 2007 -RRB- that approximately reranks the packed forest of exponentially many parses	amod_parses_many advmod_parses_exponentially prep_of_forest_parses amod_forest_packed det_forest_the dobj_reranks_forest advmod_reranks_approximately nsubj_reranks_that amod_Huang_2007 conj_and_Huang_Chiang rcmod_rescoring_reranks appos_rescoring_Chiang appos_rescoring_Huang nn_rescoring_forest agent_inspired_rescoring vmod_technique_inspired det_technique_a appos_reranking_technique nn_reranking_forest dobj_propose_reranking nsubj_propose_we dep_propose_So
P08-1067	P07-1019	o	For non-local features we adapt cube pruning from forest rescoring -LRB- Chiang 2007 Huang and Chiang 2007 -RRB- since the situation here is analogous to machine translation decoding with integrated language models we can view the scores of unit nonlocal features as the language model cost computed on-the-fly when combining sub-constituents	dobj_combining_sub-constituents advmod_combining_when advcl_on-the-fly_combining amod_on-the-fly_computed amod_cost_on-the-fly nn_cost_model nn_cost_language det_cost_the amod_features_nonlocal nn_features_unit prep_of_scores_features det_scores_the prep_as_view_cost dobj_view_scores aux_view_can nsubj_view_we nn_models_language amod_models_integrated prep_with_decoding_models vmod_translation_decoding nn_translation_machine prep_to_analogous_translation cop_analogous_is nsubj_analogous_situation mark_analogous_since advmod_situation_here det_situation_the amod_Huang_2007 conj_and_Huang_Chiang dep_Chiang_Chiang dep_Chiang_Huang appos_Chiang_2007 appos_rescoring_Chiang nn_rescoring_forest nn_pruning_cube parataxis_adapt_view advcl_adapt_analogous prep_from_adapt_rescoring dobj_adapt_pruning nsubj_adapt_we prep_for_adapt_features amod_features_non-local
P09-1020	P07-1019	p	We also use Cube Pruning algorithm -LRB- Huang and Chiang 2007 -RRB- to speed up the translation process	nn_process_translation det_process_the dobj_speed_process prt_speed_up aux_speed_to num_Chiang_2007 conj_and_Huang_Chiang appos_algorithm_Chiang appos_algorithm_Huang nn_algorithm_Pruning nn_algorithm_Cube vmod_use_speed dobj_use_algorithm advmod_use_also nsubj_use_We
P09-1063	P07-1019	o	6 Related Work In machine translation the concept of packed forest is first used by Huang and Chiang -LRB- 2007 -RRB- to characterize the search space of decoding with language models	nn_models_language prep_with_decoding_models prepc_of_space_decoding nn_space_search det_space_the dobj_characterize_space aux_characterize_to appos_Chiang_2007 conj_and_Huang_Chiang xcomp_used_characterize agent_used_Chiang agent_used_Huang advmod_used_first auxpass_used_is nsubjpass_used_concept dep_used_Work amod_forest_packed prep_of_concept_forest det_concept_the nn_translation_machine prep_in_Work_translation amod_Work_Related num_Work_6 ccomp_``_used
P09-1065	P07-1019	p	Hypergraphs have been successfully used in parsing -LRB- Klein and Manning. 2001 Huang and Chiang 2005 Huang 2008 -RRB- and machine translation -LRB- Huang and Chiang 2007 Mi et al. 2008 Mi and Huang 2008 -RRB-	amod_Mi_2008 conj_and_Mi_Huang num_Mi_2008 nn_Mi_al. nn_Mi_et dep_Huang_Huang dep_Huang_Mi conj_and_Huang_Mi conj_and_Huang_2007 conj_and_Huang_Chiang dep_translation_Mi dep_translation_2007 dep_translation_Chiang dep_translation_Huang nn_translation_machine dep_Huang_2008 num_Huang_2005 conj_and_Huang_Chiang dep_Klein_Huang conj_and_Klein_Chiang conj_and_Klein_Huang conj_and_Klein_2001 conj_and_Klein_Manning. conj_and_parsing_translation appos_parsing_Huang appos_parsing_2001 appos_parsing_Manning. appos_parsing_Klein prep_in_used_translation prep_in_used_parsing advmod_used_successfully auxpass_used_been aux_used_have nsubjpass_used_Hypergraphs
P09-1067	P07-1019	o	3A hypergraph is analogous to a parse forest -LRB- Huang and Chiang 2007 -RRB-	amod_Huang_2007 conj_and_Huang_Chiang appos_forest_Chiang appos_forest_Huang amod_forest_parse det_forest_a prep_to_analogous_forest cop_analogous_is nsubj_analogous_hypergraph nn_hypergraph_3A
P09-2035	P07-1019	o	4 Sub Translation Combining For sub translation combining we mainly use the best-first expansion idea from cube pruning -LRB- Huang and Chiang 2007 -RRB- to combine subtranslations and generate the whole k-best translations	amod_translations_k-best amod_translations_whole det_translations_the dobj_generate_translations conj_and_combine_generate dobj_combine_subtranslations aux_combine_to dep_Huang_2007 conj_and_Huang_Chiang appos_pruning_Chiang appos_pruning_Huang nn_pruning_cube nn_idea_expansion amod_idea_best-first det_idea_the dep_use_generate dep_use_combine prep_from_use_pruning dobj_use_idea advmod_use_mainly nsubj_use_we nsubj_use_Combining vmod_translation_combining amod_translation_sub prep_for_Combining_translation nn_Combining_Translation nn_Combining_Sub num_Combining_4
P09-2035	P07-1019	o	Decoding time of our experiments -LRB- h means hours -RRB- language model for rescoring -LRB- Huang and Chiang 2007 -RRB-	amod_Huang_2007 conj_and_Huang_Chiang dep_rescoring_Chiang dep_rescoring_Huang prep_for_model_rescoring nn_model_language dobj_means_hours nsubj_means_h poss_experiments_our dep_time_model dep_time_means prep_of_time_experiments nn_time_Decoding
P09-2036	P07-1019	o	Recent work has explored two-stage decoding which explicitly decouples decoding into a source parsing stage and a target language model integration stage -LRB- Huang and Chiang 2007 -RRB-	amod_Huang_2007 conj_and_Huang_Chiang appos_stage_Chiang appos_stage_Huang nn_stage_integration nn_stage_model nn_stage_language nn_stage_target det_stage_a conj_and_stage_stage nn_stage_parsing nn_stage_source det_stage_a prep_into_decoding_stage prep_into_decoding_stage xcomp_decouples_decoding advmod_decouples_explicitly nsubj_decouples_which rcmod_decoding_decouples amod_decoding_two-stage dobj_explored_decoding aux_explored_has nsubj_explored_work amod_work_Recent ccomp_``_explored
P09-2036	P07-1019	o	We rerank derivations with cube growing a lazy beam search algorithm -LRB- Huang and Chiang 2007 -RRB-	amod_Huang_2007 conj_and_Huang_Chiang dep_algorithm_Chiang dep_algorithm_Huang nn_algorithm_search nn_algorithm_beam amod_algorithm_lazy det_algorithm_a dobj_growing_algorithm nsubj_growing_cube prepc_with_derivations_growing dobj_rerank_derivations nsubj_rerank_We ccomp_``_rerank
P09-2036	P07-1019	o	Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang -LRB- 2007 -RRB-	appos_Chiang_2007 conj_and_Huang_Chiang prep_of_algorithm_Chiang prep_of_algorithm_Huang amod_algorithm_growing nn_algorithm_cube det_cube_the dobj_using_algorithm amod_forest_n-ary det_forest_this xcomp_performed_using prep_over_performed_forest auxpass_performed_be aux_performed_can csubjpass_performed_reranking nn_model_language det_model_a prep_with_reranking_model nsubj_reranking_Forest
W07-0701	P07-1019	o	Since we approach decoding as xR transduction the process is identical to that of constituencybased algorithms -LRB- e.g. Huang and Chiang 2007 -RRB-	num_Chiang_2007 conj_and_Huang_Chiang nn_Huang_e.g. dep_algorithms_Chiang dep_algorithms_Huang amod_algorithms_constituencybased prep_of_that_algorithms prep_to_identical_that cop_identical_is nsubj_identical_process advcl_identical_approach det_process_the nn_transduction_xR prep_as_decoding_transduction dobj_approach_decoding nsubj_approach_we mark_approach_Since
W08-0402	P07-1019	o	However with the algorithms proposed in -LRB- Huang and Chiang 2005 Chiang 2007 Huang and Chiang 2007 -RRB- it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems	amod_systems_parsing-based det_systems_the predet_systems_all agent_used_systems auxpass_used_be aux_used_can nsubjpass_used_that rcmod_decoder_used amod_decoder_general-purpose det_decoder_a dobj_develop_decoder aux_develop_to xcomp_possible_develop cop_possible_is nsubj_possible_it prep_with_possible_algorithms advmod_possible_However amod_Huang_2007 conj_and_Huang_Chiang num_Chiang_2007 dep_Huang_Chiang dep_Huang_Huang conj_and_Huang_Chiang conj_and_Huang_2005 conj_and_Huang_Chiang dep_in_Chiang dep_in_2005 dep_in_Chiang dep_in_Huang prep_proposed_in vmod_algorithms_proposed det_algorithms_the
W08-0402	P07-1019	o	In our decoder we incorporate two pruning techniques described by -LRB- Chiang 2007 Huang and Chiang 2007 -RRB-	amod_Huang_2007 conj_and_Huang_Chiang dep_Chiang_Chiang dep_Chiang_Huang appos_Chiang_2007 dep_by_Chiang prep_described_by vmod_techniques_described nn_techniques_pruning num_techniques_two dobj_incorporate_techniques nsubj_incorporate_we prep_in_incorporate_decoder poss_decoder_our
W09-0429	P07-1019	o	Note that this early discarding is related to ideas behind cube pruning -LRB- Huang and Chiang 2007 -RRB- which generates the top n most promising hypotheses but in our method the decision not to generate hypotheses is guided by the quality of hypotheses on the result stack	det_result_the prep_on_quality_result prep_of_quality_hypotheses det_quality_the dep_guided_stack agent_guided_quality auxpass_guided_is nsubjpass_guided_decision prep_in_guided_method dobj_generate_hypotheses aux_generate_to neg_generate_not vmod_decision_generate det_decision_the poss_method_our amod_hypotheses_promising advmod_promising_most dep_n_hypotheses amod_n_top det_n_the dobj_generates_n nsubj_generates_which dep_Huang_2007 conj_and_Huang_Chiang rcmod_pruning_generates appos_pruning_Chiang appos_pruning_Huang nn_pruning_cube prep_behind_ideas_pruning conj_but_related_guided prep_to_related_ideas auxpass_related_is nsubjpass_related_discarding mark_related_that amod_discarding_early det_discarding_this ccomp_Note_guided ccomp_Note_related
W09-0439	P07-1019	o	Decoding used beam search with the cube pruning algorithm -LRB- Huang and Chiang 2007 -RRB-	amod_Huang_2007 conj_and_Huang_Chiang dep_algorithm_Chiang dep_algorithm_Huang nn_algorithm_pruning nn_algorithm_cube det_algorithm_the nn_search_beam prep_with_used_algorithm dobj_used_search nsubj_used_Decoding
E09-1034	P07-1021	o	In order to prove this induction step we use the concept of order annotations -LRB- Kuhlmann 2007 Kuhlmann and Mohl 2007 -RRB- which are strings that lexicalise the precedence relation between the nodes of a dependency tree	nn_tree_dependency det_tree_a prep_of_nodes_tree det_nodes_the prep_between_relation_nodes nn_relation_precedence det_relation_the dobj_lexicalise_relation nsubj_lexicalise_that rcmod_strings_lexicalise cop_strings_are nsubj_strings_which dep_Kuhlmann_2007 conj_and_Kuhlmann_Mohl dep_Kuhlmann_Mohl dep_Kuhlmann_Kuhlmann appos_Kuhlmann_2007 rcmod_annotations_strings appos_annotations_Kuhlmann nn_annotations_order prep_of_concept_annotations det_concept_the dobj_use_concept nsubj_use_we advcl_use_prove nn_step_induction det_step_this dobj_prove_step aux_prove_to dep_prove_order mark_prove_In
N09-1038	P07-1021	o	The dependency trees induced when each rewrite rule in an i-th order LCFRS distinguish a unique head can similarly be characterized by being of gap-degree i so that i is the maximum number of gaps that may appear between contiguous substrings of any subtree in the dependency tree -LRB- Kuhlmann and Mohl 2007 -RRB-	amod_Kuhlmann_2007 conj_and_Kuhlmann_Mohl dep_tree_Mohl dep_tree_Kuhlmann nn_tree_dependency det_tree_the prep_in_subtree_tree det_subtree_any prep_of_substrings_subtree amod_substrings_contiguous prep_between_appear_substrings aux_appear_may nsubj_appear_that rcmod_number_appear prep_of_number_gaps amod_number_maximum det_number_the cop_number_is nsubj_number_i mark_number_that advmod_number_so nn_i_gap-degree prep_of_being_i agent_characterized_being auxpass_characterized_be advmod_characterized_similarly aux_characterized_can amod_head_unique det_head_a dobj_distinguish_number ccomp_distinguish_characterized dobj_distinguish_head nsubj_distinguish_trees nn_LCFRS_order amod_LCFRS_i-th det_LCFRS_an prep_in_rewrite_LCFRS dobj_rewrite_rule nsubj_rewrite_each advmod_rewrite_when advcl_induced_rewrite vmod_trees_induced nn_trees_dependency det_trees_The
P09-3002	P07-1021	o	-LRB- Kuhlmann and Mohl 2007 McDonald and Nivre 2007 Nivre et al. 2007 -RRB- Hindi is a verb final flexible word order language and therefore has frequent occurrences of non-projectivity in its dependency structures	nn_structures_dependency poss_structures_its prep_in_occurrences_structures prep_of_occurrences_non-projectivity amod_occurrences_frequent dobj_has_occurrences advmod_language_therefore cc_language_and nn_language_order nn_language_word amod_language_flexible amod_language_final amod_language_verb det_language_a cop_language_is nsubj_language_Hindi nn_Hindi_Nivre num_Nivre_2007 nn_Nivre_al. nn_Nivre_et dep_McDonald_has parataxis_McDonald_language conj_and_McDonald_2007 conj_and_McDonald_Nivre dep_Kuhlmann_2007 dep_Kuhlmann_Nivre dep_Kuhlmann_McDonald dep_Kuhlmann_2007 conj_and_Kuhlmann_Mohl dep_''_Mohl dep_''_Kuhlmann
C08-1003	P07-1033	p	In the supervised setting a recent paper by Daume III -LRB- 2007 -RRB- shows that using a very simple feature augmentation method coupled with Support Vector Machines he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks	nn_tasks_NLP prep_of_number_tasks det_number_a prep_in_results_number amod_results_best det_results_the dobj_provide_results aux_provide_to nn_data_source conj_and_target_data amod_target_labeled preconj_target_both vmod_use_provide dobj_use_data dobj_use_target advmod_use_effectively aux_use_to xcomp_able_use cop_able_is nsubj_able_he vmod_able_using mark_able_that nn_Machines_Vector nn_Machines_Support prep_with_coupled_Machines vmod_method_coupled nn_method_augmentation nn_method_feature amod_method_simple det_method_a advmod_simple_very dobj_using_method ccomp_shows_able nsubj_shows_paper prep_in_shows_setting appos_III_2007 nn_III_Daume prep_by_paper_III amod_paper_recent det_paper_a amod_setting_supervised det_setting_the
C08-1003	P07-1033	o	In order to build models that perform well in new -LRB- target -RRB- domains we usually find two settings -LRB- Daume III 2007 -RRB- In the semi-supervised setting the goal is to improve the system trained on the source domain using unlabeled data from the target domain and the baseline is that of the system c2008	nn_c2008_system det_c2008_the prep_of_that_c2008 prep_is_that nsubj_is_baseline det_baseline_the nn_domain_target det_domain_the amod_data_unlabeled prep_from_using_domain dobj_using_data nn_domain_source det_domain_the prep_on_trained_domain vmod_system_trained det_system_the vmod_improve_using dobj_improve_system aux_improve_to conj_and_is_is xcomp_is_improve prep_in_is_the det_goal_the dobj_setting_goal xcomp_semi-supervised_setting amod_the_semi-supervised amod_III_2007 nn_III_Daume appos_settings_III num_settings_two dobj_find_settings advmod_find_usually nsubj_find_we nn_domains_target amod_domains_new parataxis_perform_find prep_in_perform_domains advmod_perform_well nsubj_perform_that rcmod_models_perform parataxis_build_is parataxis_build_is dobj_build_models aux_build_to dep_build_order mark_build_In advcl_``_build
C08-1015	P07-1033	o	There are two tasks -LRB- Daume III 2007 -RRB- for the domain adaptation problem	nn_problem_adaptation nn_problem_domain det_problem_the amod_III_2007 nn_III_Daume prep_for_tasks_problem dep_tasks_III num_tasks_two nsubj_are_tasks expl_are_There ccomp_``_are
C08-1059	P07-1033	o	This is comparable to the accuracy of 96.29 % reported by -LRB- Daume III 2007 -RRB- on the newswire domain	nn_domain_newswire det_domain_the dep_III_2007 nn_III_Daume prep_on_reported_domain agent_reported_III vmod_%_reported num_%_96.29 prep_of_accuracy_% det_accuracy_the prep_to_comparable_accuracy cop_comparable_is nsubj_comparable_This ccomp_``_comparable
D08-1105	P07-1033	o	In particular we use a feature augmentation technique recently introduced by Daume III -LRB- 2007 -RRB- and active learning -LRB- Lewis and Gale 1994 -RRB- to perform domain adaptation of WSD systems	nn_systems_WSD prep_of_adaptation_systems nn_adaptation_domain dobj_perform_adaptation aux_perform_to dep_Lewis_1994 conj_and_Lewis_Gale vmod_learning_perform appos_learning_Gale appos_learning_Lewis amod_learning_active appos_III_2007 nn_III_Daume agent_introduced_III advmod_introduced_recently vmod_technique_introduced nn_technique_augmentation nn_technique_feature det_technique_a conj_and_use_learning dobj_use_technique nsubj_use_we prep_in_use_particular
D08-1105	P07-1033	o	5 Combining In-Domain and Out-of-Domain Data for Training In this section we will first introduce the AUGMENT technique of Daume III -LRB- 2007 -RRB- before showing the performance of our WSD system with and without using this technique	det_technique_this dobj_using_technique nn_system_WSD poss_system_our prep_of_performance_system det_performance_the prep_without_showing_using prep_with_showing_using dobj_showing_performance conj_and_showing_showing appos_III_2007 nn_III_Daume prep_of_technique_III nn_technique_AUGMENT det_technique_the prepc_before_introduce_showing prepc_before_introduce_showing dobj_introduce_technique advmod_introduce_first aux_introduce_will nsubj_introduce_we ccomp_introduce_Data ccomp_introduce_In-Domain det_section_this prep_in_Training_section nn_Data_Out-of-Domain prepc_for_In-Domain_Training conj_and_In-Domain_Data amod_In-Domain_Combining num_In-Domain_5
D08-1105	P07-1033	p	5.1 The AUGMENT technique for Domain Adaptation The AUGMENT technique introduced by Daume III -LRB- 2007 -RRB- is a simple yet very effective approach to performing domain adaptation	nn_adaptation_domain amod_adaptation_performing prep_to_approach_adaptation amod_approach_effective amod_approach_simple det_approach_a cop_approach_is nsubj_approach_technique dep_approach_technique advmod_effective_very advmod_effective_yet appos_III_2007 nn_III_Daume agent_introduced_III vmod_technique_introduced amod_technique_AUGMENT det_technique_The nn_Adaptation_Domain prep_for_technique_Adaptation nn_technique_AUGMENT det_technique_The num_technique_5.1
D08-1105	P07-1033	o	In the English all-words task of the previous SENSEVAL evaluations -LRB- SENSEVAL-2 SENSEVAL3 SemEval-2007 -RRB- the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR -LRB- Mihalcea and Moldovan 2001 Decadt et al. 2004 Chan et al. 2007b -RRB-	appos_Chan_2007b dep_Chan_al. nn_Chan_et num_Decadt_2004 nn_Decadt_al. nn_Decadt_et dep_Mihalcea_Chan dep_Mihalcea_Decadt dep_Mihalcea_2001 conj_and_Mihalcea_Moldovan dep_SEMCOR_Moldovan dep_SEMCOR_Mihalcea prep_on_trained_SEMCOR auxpass_trained_were nsubjpass_trained_systems prep_in_trained_task nn_accuracy_WSD amod_accuracy_highest det_accuracy_the prep_with_systems_accuracy nn_systems_task nn_systems_all-words amod_systems_English amod_systems_performing amod_systems_best det_systems_the appos_SENSEVAL-2_SemEval-2007 appos_SENSEVAL-2_SENSEVAL3 dep_evaluations_SENSEVAL-2 nn_evaluations_SENSEVAL amod_evaluations_previous det_evaluations_the prep_of_task_evaluations nn_task_all-words nn_task_English det_task_the
D09-1086	P07-1033	o	Many adaptation methods operate by simple augmentations of the target feature space as we have donehere -LRB- DaumeIII ,2007 -RRB-	num_DaumeIII_,2007 appos_donehere_DaumeIII dobj_have_donehere nsubj_have_we mark_have_as nn_space_feature nn_space_target det_space_the prep_of_augmentations_space amod_augmentations_simple advcl_operate_have prep_by_operate_augmentations nsubj_operate_methods nn_methods_adaptation amod_methods_Many ccomp_``_operate
D09-1158	P07-1033	o	-LRB- Blitzer et al. 2006 Jiang and Zhai 2007 Daume III 2007 Finkel and Manning 2009 -RRB- or -LSB- S+T -RSB- where no labeled target domain data is available e.g.	cop_available_is nsubj_available_data advmod_available_where nn_data_domain nn_data_target amod_data_labeled neg_data_no amod_III_2007 nn_III_Daume dep_Jiang_2009 conj_and_Jiang_Manning conj_and_Jiang_Finkel conj_and_Jiang_III num_Jiang_2007 conj_and_Jiang_Zhai dep_Blitzer_e.g. rcmod_Blitzer_available dep_Blitzer_S+T cc_Blitzer_or dep_Blitzer_Manning dep_Blitzer_Finkel dep_Blitzer_III dep_Blitzer_Zhai dep_Blitzer_Jiang appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et dep_''_Blitzer
D09-1158	P07-1033	o	Daume III -LRB- Daume III 2007 -RRB- divided features into three classes domainindependent features source-domain features and target-domain features	amod_features_target-domain amod_features_source-domain conj_and_features_features conj_and_features_features amod_features_domainindependent num_classes_three prep_into_features_classes amod_features_divided dep_III_2007 nn_III_Daume dep_III_features dep_III_features dep_III_features dep_III_features dep_III_III nn_III_Daume dep_``_III
E09-1006	P07-1033	o	The last row shows the results for the feature augmentation algorithm -LRB- Daume III 2007 -RRB-	amod_Daume_2007 num_Daume_III appos_algorithm_Daume nn_algorithm_augmentation nn_algorithm_feature det_algorithm_the prep_for_results_algorithm det_results_the dobj_shows_results nsubj_shows_row amod_row_last det_row_The
E09-1006	P07-1033	p	In the supervised setting a recent paper by Daume III -LRB- 2007 -RRB- shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks	nn_tasks_NLP prep_of_number_tasks det_number_a prep_in_results_number amod_domainadaptation_best det_domainadaptation_the dobj_provide_domainadaptation aux_provide_to nn_data_source conj_and_target_data amod_target_labeled preconj_target_both vmod_use_provide dobj_use_data dobj_use_target advmod_use_effectively aux_use_to xcomp_able_use cop_able_is nsubj_able_method mark_able_that prep_for_method_SVM nn_method_augmentation nn_method_feature amod_method_simple det_method_a dep_shows_results ccomp_shows_able nsubj_shows_paper prep_in_shows_setting appos_III_2007 nn_III_Daume prep_by_paper_III amod_paper_recent det_paper_a amod_setting_supervised det_setting_the
E09-1006	P07-1033	o	In order to build models that perform well in new -LRB- target -RRB- domains we usually find two settings -LRB- Daume III 2007 -RRB-	amod_III_2007 nn_III_Daume dep_settings_III num_settings_two dobj_find_settings advmod_find_usually nsubj_find_we nn_domains_target amod_domains_new parataxis_perform_find prep_in_perform_domains advmod_perform_well nsubj_perform_that rcmod_models_perform dobj_build_models aux_build_to dep_build_order mark_build_In advcl_``_build
E09-3005	P07-1033	o	The problem itself has started to get attention only recently -LRB- Roark and Bacchiani 2003 Hara et al. 2005 Daume III and Marcu 2006 Daume III 2007 Blitzer et al. 2006 McClosky et al. 2006 Dredze et al. 2007 -RRB-	num_Dredze_2007 nn_Dredze_al. nn_Dredze_et num_McClosky_2006 nn_McClosky_al. nn_McClosky_et num_Blitzer_2006 nn_Blitzer_al. nn_Blitzer_et appos_III_2007 nn_III_Daume dep_III_Dredze conj_and_III_McClosky conj_and_III_Blitzer conj_and_III_III conj_and_III_2006 conj_and_III_Marcu nn_III_Daume num_Hara_2005 nn_Hara_al. nn_Hara_et dep_Roark_McClosky dep_Roark_Blitzer dep_Roark_III dep_Roark_2006 dep_Roark_Marcu dep_Roark_III conj_and_Roark_Hara conj_and_Roark_2003 conj_and_Roark_Bacchiani advmod_recently_only dep_get_Hara dep_get_2003 dep_get_Bacchiani dep_get_Roark advmod_get_recently dobj_get_attention aux_get_to xcomp_started_get aux_started_has nsubj_started_problem npadvmod_problem_itself det_problem_The
E09-3005	P07-1033	o	We distinguish two main approaches to domain adaptation that have been addressed in the literature -LRB- Daume III 2007 -RRB- supervised and semi-supervised	conj_and_supervised_semi-supervised amod_Daume_2007 num_Daume_III dep_literature_Daume det_literature_the prep_in_addressed_literature auxpass_addressed_been aux_addressed_have nsubjpass_addressed_that rcmod_adaptation_addressed nn_adaptation_domain prep_to_approaches_adaptation amod_approaches_main num_approaches_two dep_distinguish_semi-supervised dep_distinguish_supervised dobj_distinguish_approaches nsubj_distinguish_We
E09-3005	P07-1033	o	In supervised domain adaptation -LRB- Gildea 2001 Roark and Bacchiani 2003 Hara et al. 2005 Daume III 2007 -RRB- besides the labeled source data we have access to a comparably small but labeled amount of target data	nn_data_target prep_of_amount_data amod_amount_labeled amod_amount_small det_amount_a conj_but_small_labeled advmod_small_comparably prep_to_access_amount dobj_have_access nsubj_have_we nn_data_source amod_data_labeled det_data_the amod_III_2007 nn_III_Daume num_Hara_2005 nn_Hara_al. nn_Hara_et rcmod_Roark_have prep_besides_Roark_data dep_Roark_III conj_and_Roark_Hara conj_and_Roark_2003 conj_and_Roark_Bacchiani dep_Gildea_Hara dep_Gildea_2003 dep_Gildea_Bacchiani dep_Gildea_Roark appos_Gildea_2001 dep_adaptation_Gildea nn_adaptation_domain amod_adaptation_supervised prep_in_``_adaptation
E09-3005	P07-1033	p	Studies on the supervised task have shown that straightforward baselines -LRB- e.g. models based on source only target only or the union of the data -RRB- achieve a relatively high performance level and are surprisingly difficult to beat -LRB- Daume III 2007 -RRB-	amod_III_2007 nn_III_Daume dep_beat_III aux_beat_to xcomp_difficult_beat advmod_difficult_surprisingly cop_difficult_are nsubj_difficult_baselines nn_level_performance amod_level_high det_level_a advmod_high_relatively conj_and_achieve_difficult dobj_achieve_level nsubj_achieve_baselines mark_achieve_that det_data_the prep_of_union_data det_union_the conj_or_only_union dep_target_union dep_target_only prep_target_e.g. advmod_source_only prep_on_based_source vmod_models_based pobj_e.g._models dep_baselines_target amod_baselines_straightforward ccomp_shown_difficult ccomp_shown_achieve aux_shown_have nsubj_shown_Studies amod_task_supervised det_task_the prep_on_Studies_task
E09-3005	P07-1033	o	Thus one conclusion from that line of work is that as soon as there is a reasonable -LRB- often even small -RRB- amount of labeled target data it is often more fruitful to either just use that or to apply simple adaptation techniques -LRB- Daume III 2007 Plank and van Noord 2008 -RRB-	nn_Noord_van amod_Plank_2008 conj_and_Plank_Noord dep_III_Noord dep_III_Plank appos_III_2007 nn_III_Daume dep_techniques_III nn_techniques_adaptation amod_techniques_simple dobj_apply_techniques aux_apply_to cc_apply_or dep_that_apply prep_use_that advmod_use_just preconj_use_either aux_use_to xcomp_fruitful_use advmod_fruitful_more advmod_fruitful_often cop_fruitful_is nsubj_fruitful_it advmod_fruitful_soon mark_fruitful_that nn_data_target amod_data_labeled prep_of_amount_data amod_amount_reasonable det_amount_a advmod_small_even advmod_small_often dep_reasonable_small nsubj_is_amount expl_is_there mark_is_as advcl_soon_is advmod_soon_as ccomp_is_fruitful nsubj_is_conclusion advmod_is_Thus prep_of_line_work det_line_that prep_from_conclusion_line num_conclusion_one
E09-3005	P07-1033	o	Therefore whenever we have access to a large amount of labeled data from some source -LRB- out-of-domain -RRB- but we would like a model that performs well on some new target domain -LRB- Gildea 2001 Daume III 2007 -RRB- we face the problem of domain adaptation	nn_adaptation_domain prep_of_problem_adaptation det_problem_the dobj_face_problem nsubj_face_we advcl_face_like advcl_face_have advmod_face_Therefore amod_III_2007 nn_III_Daume dep_Gildea_III dep_Gildea_2001 appos_domain_Gildea nn_domain_target amod_domain_new det_domain_some prep_on_performs_domain advmod_performs_well nsubj_performs_that rcmod_model_performs det_model_a dobj_like_model aux_like_would nsubj_like_we appos_source_out-of-domain det_source_some prep_from_data_source amod_data_labeled prep_of_amount_data amod_amount_large det_amount_a prep_to_access_amount conj_but_have_like dobj_have_access nsubj_have_we advmod_have_whenever
I08-2097	P07-1033	o	There are many possible methods for combining unlabeled and labeled data -LRB- Daume III 2007 -RRB- but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses	amod_parses_reliable amod_parses_selected det_parses_the prep_of_effectiveness_parses det_effectiveness_the dobj_see_effectiveness aux_see_to vmod_data_see amod_data_labeled prep_with_data_data amod_data_unlabeled dobj_concatenate_data advmod_concatenate_simply nsubj_concatenate_we amod_III_2007 nn_III_Daume appos_data_III amod_data_labeled amod_data_unlabeled conj_and_unlabeled_labeled dobj_combining_data prepc_for_methods_combining amod_methods_possible amod_methods_many conj_but_are_concatenate nsubj_are_methods expl_are_There ccomp_``_concatenate ccomp_``_are
I08-2097	P07-1033	o	This was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data -LRB- Dredze et al. 2007 -RRB-	amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et amod_data_unlabeled prep_from_gains_data amod_gains_meaningful det_gains_any dobj_obtain_gains aux_obtain_to xcomp_failed_obtain nsubj_failed_participants mark_failed_as det_task_the prep_in_participants_task amod_participants_many dep_challenge_Dredze advcl_challenge_failed nn_challenge_difcult det_challenge_a cop_challenge_was nsubj_challenge_This
I08-2097	P07-1033	o	For the multilingual dependency parsing track which was the other track of the shared task Nilsson et al. achieved the best performance using an ensemble method -LRB- Hall et al. 2007 -RRB-	amod_Hall_2007 dep_Hall_al. nn_Hall_et dep_method_Hall nn_method_ensemble det_method_an dobj_using_method amod_performance_best det_performance_the xcomp_achieved_using dobj_achieved_performance nsubj_achieved_al. prep_for_achieved_track nn_al._et nn_al._Nilsson amod_task_shared det_task_the prep_of_track_task amod_track_other det_track_the cop_track_was nsubj_track_which rcmod_track_track nn_track_parsing nn_track_dependency amod_track_multilingual det_track_the
N09-1032	P07-1033	o	Daume III -LRB- 2007 -RRB- further augments the feature space on the instances of both domains	det_domains_both prep_of_instances_domains det_instances_the prep_on_space_instances nn_space_feature det_space_the dobj_augments_space advmod_augments_further nsubj_augments_III appos_III_2007 nn_III_Daume
N09-1068	P07-1033	o	Because Daume III -LRB- 2007 -RRB- views the adaptation as merely augmenting the feature space each of his features has the same prior mean and variance regardless of whether it is domain specific or independent	nsubj_independent_it conj_or_specific_independent npadvmod_specific_domain cop_specific_is nsubj_specific_it mark_specific_whether conj_and_mean_variance amod_mean_prior amod_mean_same det_mean_the dobj_has_variance dobj_has_mean nsubj_has_each poss_features_his prep_of_each_features prepc_regardless_of_space_independent prepc_regardless_of_space_specific rcmod_space_has nn_space_feature det_space_the dobj_augmenting_space advmod_augmenting_merely det_adaptation_the prepc_as_views_augmenting dobj_views_adaptation nsubj_views_III mark_views_Because appos_III_2007 nn_III_Daume advcl_``_views
N09-1068	P07-1033	o	Trained and tested using the same technique as -LRB- Daume III 2007 -RRB-	amod_III_2007 nn_III_Daume dep_as_III amod_technique_same det_technique_the prep_using_as dobj_using_technique xcomp_Trained_using conj_and_Trained_tested
N09-1068	P07-1033	o	5 Related Work We already discussed the relation of our work to -LRB- Daume III 2007 -RRB- in Section 2.4	num_Section_2.4 amod_III_2007 nn_III_Daume poss_work_our prep_to_relation_III prep_of_relation_work det_relation_the prep_in_discussed_Section dobj_discussed_relation advmod_discussed_already nsubj_discussed_We rcmod_Work_discussed amod_Work_Related num_Work_5
N09-1068	P07-1033	o	We also show that the domain adaptation work of -LRB- Daume III 2007 -RRB- which is presented as an ad-hoc preprocessing step is actually equivalent to our formal model	amod_model_formal poss_model_our prep_to_equivalent_model advmod_equivalent_actually cop_equivalent_is nsubj_equivalent_work mark_equivalent_that amod_step_preprocessing amod_step_ad-hoc det_step_an prep_as_presented_step auxpass_presented_is nsubjpass_presented_which rcmod_III_presented dep_III_2007 nn_III_Daume prep_of_work_III nn_work_adaptation nn_work_domain det_work_the ccomp_show_equivalent advmod_show_also nsubj_show_We ccomp_``_show
N09-1068	P07-1033	o	However our representation of the model conceptually separates some of the hyperparameters which are not separated in -LRB- Daume III 2007 -RRB- and we found that setting these hyperparameters with different values from one another was critical for improving performance	dobj_improving_performance prepc_for_critical_improving cop_critical_was csubj_critical_setting mark_critical_that num_another_one prep_from_values_another amod_values_different det_hyperparameters_these prep_with_setting_values dobj_setting_hyperparameters ccomp_found_critical nsubj_found_we conj_and_III_found amod_III_2007 nn_III_Daume prep_in_separated_found prep_in_separated_III neg_separated_not auxpass_separated_are nsubjpass_separated_which rcmod_hyperparameters_separated det_hyperparameters_the prep_of_some_hyperparameters dobj_separates_some advmod_separates_conceptually nsubj_separates_model det_model_the prepc_of_representation_separates poss_representation_our dep_,_representation dep_``_However
N09-1068	P07-1033	o	We show that the method of -LRB- Daume III 2007 -RRB- which was presented as a simple preprocessing step is actually equivalent except our representation explicitly separates hyperparameters which were tied in his work	poss_work_his prep_in_tied_work auxpass_tied_were nsubjpass_tied_which rcmod_hyperparameters_tied dobj_separates_hyperparameters advmod_separates_explicitly nsubj_separates_representation mark_separates_except poss_representation_our advcl_equivalent_separates advmod_equivalent_actually cop_equivalent_is nsubj_equivalent_method mark_equivalent_that amod_step_preprocessing amod_step_simple det_step_a prep_as_presented_step auxpass_presented_was nsubjpass_presented_which rcmod_III_presented dep_III_2007 nn_III_Daume prep_of_method_III det_method_the ccomp_show_equivalent nsubj_show_We
N09-1068	P07-1033	o	We demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and -LRB- Daume III 2007 -RRB- within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser	nn_parser_dependency amod_parser_trained det_parser_a advmod_trained_discriminatively conj_and_recognition_parser nn_recognition_entity dep_named_parser dep_named_recognition prepc_for_model_named nn_model_sequence nn_model_field amod_model_random amod_model_conditional det_model_a dep_model_both dep_III_2007 nn_III_Daume conj_and_baseline_III amod_baseline_strong det_baseline_a preconj_baseline_both prep_within_performance_model prep_over_performance_III prep_over_performance_baseline dobj_improves_performance advmod_improves_significantly csubj_improves_allowing mark_improves_that det_hyperparameters_these prep_for_values_hyperparameters amod_values_different dobj_allowing_values ccomp_demonstrate_improves nsubj_demonstrate_We
N09-1068	P07-1033	o	2.4 Formalization of -LRB- Daume III 2007 -RRB- As mentioned earlier our model is equivalent to that presented in -LRB- Daume III 2007 -RRB- and can be viewed as a formal version of his model .2 In his presentation the adapation is done through feature augmentation	nn_augmentation_feature prep_through_done_augmentation auxpass_done_is nsubjpass_done_adapation det_adapation_the poss_presentation_his nn_.2_model poss_.2_his prep_of_version_.2 amod_version_formal det_version_a prep_in_viewed_presentation prep_as_viewed_version auxpass_viewed_be aux_viewed_can nsubjpass_viewed_model amod_III_2007 nn_III_Daume prep_in_presented_III vmod_that_presented conj_and_equivalent_done conj_and_equivalent_viewed prep_to_equivalent_that cop_equivalent_is nsubj_equivalent_model ccomp_equivalent_Formalization poss_model_our advmod_mentioned_earlier mark_mentioned_As amod_III_2007 nn_III_Daume advcl_Formalization_mentioned prep_of_Formalization_III num_Formalization_2.4
N09-1068	P07-1033	o	Recall that the log likelihood of our model is d parenleftBigg Lorig -LRB- Dd ;d -RRB- i -LRB- d i i -RRB- 2 2 2d parenrightBigg i -LRB- i -RRB- 2 2 2 We now introduce a new variable d = d and plug it into the equation for log likelihood d parenleftBigg Lorig -LRB- Dd ;d + -RRB- i -LRB- d i -RRB- 2 2 2d parenrightBigg i -LRB- i -RRB- 2 2 2 The result is the model of -LRB- Daume III 2007 -RRB- where the d are the domain-specific feature weights and d are the domain-independent feature weights	nn_weights_feature amod_weights_domain-independent det_weights_the cop_weights_are nsubj_weights_d dep_weights_plug dep_weights_i nn_weights_i dep_weights_parenrightBigg conj_and_weights_d nn_weights_feature amod_weights_domain-specific det_weights_the cop_weights_are nsubj_weights_d advmod_weights_where det_d_the rcmod_III_d rcmod_III_weights amod_III_2007 nn_III_Daume prep_of_model_III det_model_the cop_model_is nsubj_model_result dep_model_i nn_model_i nn_model_parenrightBigg num_model_2 num_model_2 dep_model_i det_result_The num_result_2 num_2_2 number_2_2 amod_parenrightBigg_2d appos_d_model nn_d_i nn_d_Lorig cc_;d_+ nn_;d_Dd appos_Lorig_;d nn_Lorig_parenleftBigg nn_Lorig_d nn_likelihood_log det_equation_the prep_for_plug_likelihood prep_into_plug_equation dobj_plug_it amod_d_= nn_d_d amod_d_variable amod_d_new det_d_a dobj_introduce_d advmod_introduce_now nsubj_introduce_We rcmod_2_introduce num_2_2 number_2_2 conj_and_i_plug dep_i_2 amod_parenrightBigg_2d num_parenrightBigg_2 number_2_2 dep_d_weights appos_d_i appos_d_i dep_i_d nn_i_Dd appos_Dd_;d dep_Lorig_i nn_Lorig_parenleftBigg nn_Lorig_d dep_is_Lorig nsubj_is_likelihood mark_is_that poss_model_our prep_of_likelihood_model nn_likelihood_log det_likelihood_the ccomp_Recall_is
P08-1029	P07-1033	o	Other techniques have tried to quantify the generalizability of certain features across domains -LRB- Daume III and Marcu 2006 Jiang and Zhai 2006 -RRB- or tried to exploit the common structure of related problems -LRB- Ben-David et al. 2007 Scholkopf et al. 2005 -RRB-	num_Scholkopf_2005 nn_Scholkopf_al. nn_Scholkopf_et dep_Ben-David_Scholkopf appos_Ben-David_2007 dep_Ben-David_al. nn_Ben-David_et amod_problems_related prep_of_structure_problems amod_structure_common det_structure_the dobj_exploit_structure aux_exploit_to xcomp_tried_exploit dep_Jiang_2006 conj_and_Jiang_Zhai dep_III_Ben-David conj_or_III_tried conj_and_III_Zhai conj_and_III_Jiang conj_and_III_2006 conj_and_III_Marcu nn_III_Daume prep_across_features_domains amod_features_certain dep_generalizability_tried dep_generalizability_Jiang dep_generalizability_2006 dep_generalizability_Marcu dep_generalizability_III prep_of_generalizability_features det_generalizability_the dobj_quantify_generalizability aux_quantify_to xcomp_tried_quantify aux_tried_have nsubj_tried_techniques amod_techniques_Other ccomp_``_tried
P08-1029	P07-1033	o	Daume allows an extra degree of freedom among the features of his domains implicitly creating a two-level feature hierarchy with one branch for general features and another for domain specific ones but does not extend his hierarchy further -LRB- Daume III 2007 -RRB- -RRB-	dep_III_2007 nn_III_Daume dep_further_III poss_hierarchy_his advmod_extend_further dobj_extend_hierarchy neg_extend_not aux_extend_does nsubj_extend_Daume amod_ones_specific nn_ones_domain prep_for_another_ones amod_features_general prep_for_branch_features num_branch_one nn_hierarchy_feature amod_hierarchy_two-level det_hierarchy_a conj_and_creating_another prep_with_creating_branch dobj_creating_hierarchy advmod_creating_implicitly poss_domains_his prep_of_features_domains det_features_the prep_among_degree_features prep_of_degree_freedom amod_degree_extra det_degree_an conj_but_allows_extend dep_allows_another dep_allows_creating dobj_allows_degree nsubj_allows_Daume
P09-1056	P07-1033	o	Unlike our technique in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain -LRB- e.g. -LRB- Daume III 2007 Chelba and Acero 2004 Daume III and Marcu 2006 -RRB- -RRB-	nn_III_Daume dep_Chelba_2006 conj_and_Chelba_Marcu conj_and_Chelba_III conj_and_Chelba_2004 conj_and_Chelba_Acero dep_III_Marcu dep_III_III dep_III_2004 dep_III_Acero dep_III_Chelba dep_III_2007 nn_III_Daume appos_e.g._III dep_domain_e.g. nn_domain_target det_domain_the conj_and_source_domain det_source_the preconj_source_both prep_in_available_domain prep_in_available_source cop_available_is nsubj_available_data advmod_available_where nn_data_training amod_data_labeled rcmod_scenario_available det_scenario_the prep_on_focused_scenario aux_focused_have nsubj_focused_researchers prep_in_focused_cases prep_unlike_focused_technique amod_cases_most poss_technique_our
P09-1059	P07-1033	o	-LRB- 2006 -RRB- and Daume III -LRB- 2007 -RRB- -LRB- and see below for discussions -RRB- so in this paper we focus on the less studied but equally important problem of annotationstyle adaptation	nn_adaptation_annotationstyle prep_of_problem_adaptation amod_problem_important advmod_important_equally vmod_less_studied det_less_the prep_on_focus_less nsubj_focus_we det_paper_this pobj_for_discussions pcomp_below_for prep_see_below appos_III_2007 nn_III_Daume conj_but_2006_problem rcmod_2006_focus prep_in_2006_paper advmod_2006_so conj_and_2006_see conj_and_2006_III dep_''_problem dep_''_see dep_''_III dep_''_2006
P09-1059	P07-1033	o	This method is very similar to some ideas in domain adaptation -LRB- Daume III and Marcu 2006 Daume III 2007 -RRB- but we argue that the underlying problems are quite different	advmod_different_quite cop_different_are nsubj_different_problems mark_different_that amod_problems_underlying det_problems_the ccomp_argue_different nsubj_argue_we amod_III_2007 nn_III_Daume conj_and_Daume_III conj_and_Daume_2006 conj_and_Daume_Marcu num_Daume_III dep_adaptation_III dep_adaptation_2006 dep_adaptation_Marcu dep_adaptation_Daume nn_adaptation_domain prep_in_ideas_adaptation det_ideas_some conj_but_similar_argue prep_to_similar_ideas advmod_similar_very cop_similar_is nsubj_similar_method det_method_This
P09-1059	P07-1033	o	ald 2008 -RRB- and is also similar to the Pred baseline for domain adaptation in -LRB- Daume III and Marcu 2006 Daume III 2007 -RRB-	amod_III_2007 nn_III_Daume dep_III_III dep_III_2006 conj_and_III_Marcu nn_III_Daume prep_in_adaptation_Marcu prep_in_adaptation_III nn_adaptation_domain prep_for_baseline_adaptation nn_baseline_Pred det_baseline_the prep_to_similar_baseline advmod_similar_also cop_similar_is conj_and_2008_similar advmod_2008_ald
P09-1087	P07-1033	p	For example -LRB- Daume III 2007 -RRB- shows that training a learning algorithm on the weighted union of different data sets -LRB- which is basically what we did -RRB- performs almost as well as more involved domain adaptation approaches	nn_approaches_adaptation nn_approaches_domain amod_approaches_involved amod_approaches_more advmod_well_as advmod_well_almost prep_as_performs_approaches advmod_performs_well nsubj_did_we dobj_did_what advmod_did_basically dep_is_performs ccomp_is_did nsubj_is_which ccomp_-LRB-_is nn_sets_data amod_sets_different prep_of_union_sets amod_union_weighted det_union_the amod_algorithm_learning det_algorithm_a prep_on_training_union dobj_training_algorithm dep_that_training prep_shows_that nsubj_shows_III prep_for_shows_example amod_III_2007 nn_III_Daume
P09-1114	P07-1033	o	The model presented above is based on our previous work -LRB- Jiang and Zhai 2007c -RRB- which bears the same spirit of some other recent work on multitask learning -LRB- Ando and Zhang 2005 Evgeniou and Pontil 2004 Daume III 2007 -RRB-	amod_III_2007 nn_III_Daume dep_Evgeniou_III conj_and_Evgeniou_2004 conj_and_Evgeniou_Pontil conj_and_Ando_2004 conj_and_Ando_Pontil conj_and_Ando_Evgeniou conj_and_Ando_2005 conj_and_Ando_Zhang dep_learning_Evgeniou dep_learning_2005 dep_learning_Zhang dep_learning_Ando amod_learning_multitask amod_work_recent amod_work_other det_work_some prep_of_spirit_work amod_spirit_same det_spirit_the prep_on_bears_learning dobj_bears_spirit nsubj_bears_which dep_Jiang_2007c conj_and_Jiang_Zhai rcmod_work_bears dep_work_Zhai dep_work_Jiang amod_work_previous poss_work_our prep_on_based_work auxpass_based_is nsubjpass_based_model prep_presented_above vmod_model_presented det_model_The
P09-1114	P07-1033	o	While transfer learning was proposed more than a decade ago -LRB- Thrun 1996 Caruana 1997 -RRB- its application in natural language processing is still a relatively new territory -LRB- Blitzer et al. 2006 Daume III 2007 Jiang and Zhai 2007a Arnold et al. 2008 Dredze and Crammer 2008 -RRB- and its application in relation extraction is still unexplored	advmod_unexplored_still cop_unexplored_is nsubj_unexplored_application nsubj_unexplored_III nsubj_unexplored_Blitzer nn_extraction_relation prep_in_application_extraction poss_application_its num_Arnold_2008 nn_Arnold_al. nn_Arnold_et dep_Jiang_2008 conj_and_Jiang_Crammer conj_and_Jiang_Dredze conj_and_Jiang_Arnold conj_and_Jiang_2007a conj_and_Jiang_Zhai amod_III_2007 nn_III_Daume conj_and_Blitzer_application dep_Blitzer_Crammer dep_Blitzer_Dredze dep_Blitzer_Arnold dep_Blitzer_2007a dep_Blitzer_Zhai dep_Blitzer_Jiang conj_and_Blitzer_III appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et rcmod_territory_unexplored amod_territory_new det_territory_a advmod_territory_still cop_territory_is nsubj_territory_application advcl_territory_proposed advmod_new_relatively nn_processing_language amod_processing_natural prep_in_application_processing poss_application_its dep_Caruana_1997 dep_Thrun_Caruana appos_Thrun_1996 npadvmod_ago_decade det_decade_a dep_than_Thrun advmod_than_ago advmod_than_more prep_proposed_than auxpass_proposed_was nsubjpass_proposed_learning mark_proposed_While nn_learning_transfer
P09-1114	P07-1033	o	Daume III -LRB- 2007 -RRB- proposed a simple feature augmentation method to achieve domain adaptation	nn_adaptation_domain dobj_achieve_adaptation aux_achieve_to vmod_method_achieve nn_method_augmentation nn_method_feature amod_method_simple det_method_a dobj_proposed_method nsubj_proposed_III appos_III_2007 nn_III_Daume
P09-2079	P07-1033	o	Also the aspect of generalizing features across different products is closely related to fully supervised domain adaptation -LRB- Daume III 2007 -RRB- and we plan to combine our approach with the idea from Daume III -LRB- 2007 -RRB- to gain insights into whether the composite back-off features exhibit different behavior in domain-general versus domain-specific feature sub-spaces	nn_sub-spaces_feature amod_sub-spaces_domain-specific amod_sub-spaces_domain-general conj_versus_domain-general_domain-specific amod_behavior_different prep_in_exhibit_sub-spaces dobj_exhibit_behavior nsubj_exhibit_features mark_exhibit_whether amod_features_back-off amod_features_composite det_features_the prepc_into_insights_exhibit dobj_gain_insights aux_gain_to appos_III_2007 nn_III_Daume prep_from_idea_III det_idea_the poss_approach_our vmod_combine_gain prep_with_combine_idea dobj_combine_approach aux_combine_to xcomp_plan_combine nsubj_plan_we amod_Daume_2007 num_Daume_III appos_adaptation_Daume nn_adaptation_domain amod_adaptation_supervised advmod_supervised_fully conj_and_related_plan prep_to_related_adaptation advmod_related_closely cop_related_is nsubj_related_aspect advmod_related_Also amod_products_different prep_across_features_products amod_features_generalizing prep_of_aspect_features det_aspect_the
W09-2420	P07-1033	o	Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions -LRB- Kilgarriff 2001 Mihalcea et al. 2004 Pradhan et al. 2007 -RRB- supervised systems attain results on the high 80s and beat the most frequent baseline by a large margin for lexical-sample datasets but results on the all-words datasets were much more modest on the low 70s and a few points above the most frequent baseline	amod_baseline_frequent det_baseline_the advmod_frequent_most prep_above_points_baseline amod_points_few det_points_a amod_70s_low det_70s_the prep_on_modest_70s advmod_modest_more cop_modest_were nsubj_modest_results advmod_more_much amod_datasets_all-words det_datasets_the prep_on_results_datasets amod_datasets_lexical-sample prep_for_margin_datasets amod_margin_large det_margin_a amod_baseline_frequent det_baseline_the advmod_frequent_most prep_by_beat_margin dobj_beat_baseline nsubj_beat_systems amod_80s_high det_80s_the prep_on_results_80s conj_and_attain_points conj_but_attain_modest conj_and_attain_beat dobj_attain_results nsubj_attain_systems amod_systems_supervised num_Pradhan_2007 nn_Pradhan_al. nn_Pradhan_et num_Mihalcea_2004 nn_Mihalcea_al. nn_Mihalcea_et dep_Kilgarriff_Pradhan dep_Kilgarriff_Mihalcea dep_Kilgarriff_2001 appos_competitions_Kilgarriff amod_competitions_Semeval amod_competitions_Senseval amod_competitions_previous conj_and_Senseval_Semeval parataxis_observed_points parataxis_observed_modest parataxis_observed_beat parataxis_observed_attain prep_on_observed_competitions auxpass_observed_been aux_observed_have nsubjpass_observed_Differences amod_datasets_all-words amod_datasets_lexical-sample conj_and_lexical-sample_all-words prep_to_applied_datasets advmod_applied_when nn_systems_WSD prep_of_behavior_systems rcmod_Differences_applied prep_in_Differences_behavior
W09-2420	P07-1033	o	1 Introduction Word Sense Disambiguation -LRB- WSD -RRB- competitions have focused on general domain texts as attested in the last Senseval and Semeval competitions -LRB- Kilgarriff 2001 Mihalcea et al. 2004 Pradhan et al. 2007 -RRB-	num_Pradhan_2007 nn_Pradhan_al. nn_Pradhan_et num_Mihalcea_2004 nn_Mihalcea_al. nn_Mihalcea_et dep_Kilgarriff_Pradhan dep_Kilgarriff_Mihalcea dep_Kilgarriff_2001 appos_competitions_Kilgarriff amod_competitions_Semeval amod_competitions_Senseval det_competitions_the conj_and_Senseval_Semeval amod_Senseval_last nn_texts_domain amod_texts_general prep_in_focused_competitions prep_as_focused_attested prep_on_focused_texts aux_focused_have nsubj_focused_competitions nn_competitions_Disambiguation appos_Disambiguation_WSD nn_Disambiguation_Sense nn_Disambiguation_Word nn_Disambiguation_Introduction num_Disambiguation_1
W09-2420	P07-1033	p	For instance -LRB- Daume III 2007 -RRB- shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks	nn_tasks_NLP prep_of_number_tasks det_number_a prep_in_results_number amod_domainadaptation_best det_domainadaptation_the dobj_provide_domainadaptation aux_provide_to nn_data_source conj_and_target_data amod_target_labeled preconj_target_both vmod_use_provide dobj_use_data dobj_use_target advmod_use_effectively aux_use_to xcomp_able_use cop_able_is nsubj_able_method mark_able_that prep_for_method_SVM nn_method_augmentation nn_method_feature amod_method_simple det_method_a dep_shows_results ccomp_shows_able nsubj_shows_III prep_for_shows_instance amod_III_2007 nn_III_Daume
C08-1014	P07-1040	o	This technique is called system combination -LRB- Bangalore et al. 2001 Matusov et al. 2006 Sim et al. 2007 Rosti et al. 2007a Rosti et al. 2007b -RRB-	appos_al._2007b nn_al._et nn_al._Rosti appos_al._2007a nn_al._et nn_al._Rosti num_Sim_2007 nn_Sim_al. nn_Sim_et conj_Matusov_al. conj_Matusov_al. conj_Matusov_Sim num_Matusov_2006 nn_Matusov_al. nn_Matusov_et dep_Bangalore_Matusov appos_Bangalore_2001 dep_Bangalore_al. nn_Bangalore_et dep_combination_Bangalore nn_combination_system dep_called_combination auxpass_called_is nsubjpass_called_technique det_technique_This advcl_``_called
C08-1014	P07-1040	o	Re-decoding -LRB- Rosti et al. 2007a -RRB- based regeneration re-decodes the source sentence using original LM as well as new trans105 lation and reordering models that are trained on the source-to-target N-best translations generated in the first pass	amod_pass_first det_pass_the prep_in_generated_pass vmod_translations_generated nn_translations_N-best amod_translations_source-to-target det_translations_the prep_on_trained_translations auxpass_trained_are nsubjpass_trained_that rcmod_models_trained nn_models_reordering nn_lation_trans105 amod_lation_new conj_and_LM_lation amod_LM_original conj_and_using_models dobj_using_lation dobj_using_LM dep_sentence_models dep_sentence_using nn_sentence_source det_sentence_the dep_re-decodes_sentence nn_re-decodes_regeneration amod_re-decodes_based appos_Rosti_2007a dep_Rosti_al. nn_Rosti_et dep_Re-decoding_re-decodes dep_Re-decoding_Rosti
C08-1014	P07-1040	o	Confusion network and re-decoding have been well studied in the combination of different MT systems -LRB- Bangalore et al. 2001 Matusov et al. 2006 Sim et al. 2007 Rosti et al. 2007a Rosti et al. 2007b -RRB-	appos_al._2007b nn_al._et nn_al._Rosti appos_al._2007a nn_al._et nn_al._Rosti num_Sim_2007 nn_Sim_al. nn_Sim_et conj_Matusov_al. conj_Matusov_al. conj_Matusov_Sim num_Matusov_2006 nn_Matusov_al. nn_Matusov_et dep_Bangalore_Matusov appos_Bangalore_2001 dep_Bangalore_al. nn_Bangalore_et nn_systems_MT amod_systems_different prep_of_combination_systems det_combination_the dep_studied_Bangalore prep_in_studied_combination advmod_studied_well auxpass_studied_been aux_studied_have nsubjpass_studied_re-decoding nsubjpass_studied_network conj_and_network_re-decoding nn_network_Confusion
C08-1014	P07-1040	o	-LRB- Rosti et al. 2007a -RRB- also used re-decoding to do system combination by extracting sentence-specific phrase translation tables from the outputs of different MT systems and running a phrase-based decoding with this new translation table	nn_table_translation amod_table_new det_table_this amod_decoding_phrase-based det_decoding_a prep_with_running_table dobj_running_decoding nn_systems_MT amod_systems_different prep_of_outputs_systems det_outputs_the nn_tables_translation nn_tables_phrase amod_tables_sentence-specific conj_and_extracting_running prep_from_extracting_outputs dobj_extracting_tables nn_combination_system prepc_by_do_running prepc_by_do_extracting dobj_do_combination aux_do_to xcomp_used_do dobj_used_re-decoding advmod_used_also nsubj_used_Rosti nn_al._et appos_Rosti_2007a dep_Rosti_al.
C08-1014	P07-1040	o	3.1 Regeneration with Re-decoding One way of regeneration is by running the decoding again to obtain new hypotheses through a re-decoding process -LRB- Rosti et al. 2007a -RRB-	appos_Rosti_2007a dep_Rosti_al. nn_Rosti_et amod_process_re-decoding det_process_a amod_hypotheses_new prep_through_obtain_process dobj_obtain_hypotheses aux_obtain_to vmod_decoding_obtain advmod_decoding_again det_decoding_the xcomp_running_decoding dep_is_Rosti prepc_by_is_running nsubj_is_Regeneration prep_of_way_regeneration num_way_One amod_way_Re-decoding prep_with_Regeneration_way num_Regeneration_3.1 ccomp_``_is
C08-1014	P07-1040	o	-LRB- 2007 -RRB- Rosti et al.	nn_al._et dep_Rosti_al. dep_Rosti_2007
C08-1014	P07-1040	o	-LRB- 2007a -RRB- and Rosti et al.	nn_al._et nn_al._Rosti conj_and_2007a_al. dep_''_al. dep_''_2007a
C08-1014	P07-1040	o	-LRB- 2007 -RRB- Rosti et al.	nn_al._et dep_Rosti_al. dep_Rosti_2007
C08-1014	P07-1040	o	-LRB- 2007a -RRB- and Rosti et al.	nn_al._et nn_al._Rosti conj_and_2007a_al. dep_''_al. dep_''_2007a
D08-1011	P07-1040	o	-LRB- 2007 -RRB- and Rosti et al.	nn_al._et nn_al._Rosti conj_and_2007_al.
D08-1011	P07-1040	o	Similar to -LRB- Rosti et al. 2007 -RRB- each word in the confusion network is associated with a word posterior probability	nn_probability_posterior nn_probability_word det_probability_a prep_with_associated_probability auxpass_associated_is nsubjpass_associated_word nn_network_confusion det_network_the prep_in_word_network det_word_each rcmod_Rosti_associated amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et prep_to_Similar_Rosti dep_``_Similar
D08-1011	P07-1040	p	2 Confusion-network-based MT system combination The current state-of-the-art is confusion-networkbased MT system combination as described by 98 Rosti and colleagues -LRB- Rosti et al. 2007a Rosti et al. 2007b -RRB-	nn_al._et nn_al._Rosti appos_Rosti_2007b dep_Rosti_al. appos_Rosti_2007a dep_Rosti_al. nn_Rosti_et conj_and_Rosti_colleagues num_Rosti_98 prep_by_described_colleagues prep_by_described_Rosti mark_described_as dep_combination_Rosti dep_combination_described nn_combination_system nn_combination_MT amod_combination_confusion-networkbased cop_combination_is nsubj_combination_state-of-the-art dep_combination_combination amod_state-of-the-art_current det_state-of-the-art_The nn_combination_system nn_combination_MT amod_combination_Confusion-network-based num_combination_2 ccomp_``_combination
D08-1011	P07-1040	o	Recently confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation -LRB- MT -RRB- systems to form a consensus output -LRB- Bangalore et al. 2001 Matusov et al. 2006 Rosti et al. 2007 Sim et al. 2007 -RRB-	nn_Sim_al. nn_Sim_et nn_Rosti_al. nn_Rosti_et nn_Matusov_al. nn_Matusov_et advmod_2001_al. nn_al._et amod_Bangalore_2007 appos_Bangalore_Sim amod_Bangalore_2007 conj_Bangalore_Rosti amod_Bangalore_2006 appos_Bangalore_Matusov dep_Bangalore_2001 appos_output_Bangalore nn_output_consensus det_output_a dobj_form_output aux_form_to nn_systems_translation appos_translation_MT nn_translation_machine amod_translation_multiple prep_of_outputs_systems vmod_combine_form dobj_combine_outputs aux_combine_to xcomp_developed_combine auxpass_developed_been aux_developed_have nsubjpass_developed_algorithms advmod_developed_Recently nn_algorithms_combination nn_algorithms_system amod_algorithms_confusion-network-based
D09-1114	P07-1040	n	Although various approaches to SMT system combination have been explored including enhanced combination model structure -LRB- Rosti et al. 2007 -RRB- better word alignment between translations -LRB- Ayan et al. 2008 He et al. 2008 -RRB- and improved confusion network construction -LRB- Rosti et al. 2008 -RRB- most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way	amod_way_principled det_way_a det_ensemble_the prep_in_obtain_way dobj_obtain_ensemble aux_obtain_to advmod_obtain_how prepc_of_issue_obtain det_issue_the dobj_tackle_issue neg_tackle_not aux_tackle_did nsubj_tackle_work prep_at_models_hand conj_and_models_paradigms amod_models_different prep_on_based_paradigms prep_on_based_models nn_systems_SMT prep_of_ensemble_systems det_ensemble_the conj_and_used_tackle vmod_used_based dobj_used_ensemble advmod_used_simply nsubj_used_work dep_used_construction dep_used_He amod_work_previous amod_work_most amod_Rosti_2008 dep_Rosti_al. nn_Rosti_et dep_construction_Rosti nn_construction_network nn_construction_confusion amod_construction_improved dep_al._2008 nn_al._et conj_and_He_construction dep_He_al. parataxis_Ayan_tackle parataxis_Ayan_used appos_Ayan_2008 dep_Ayan_al. nn_Ayan_et dep_translations_Ayan prep_between_alignment_translations nn_alignment_word amod_alignment_better amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_structure_Rosti nn_structure_model nn_structure_combination amod_structure_enhanced dep_,_alignment prep_including_,_structure auxpass_explored_been aux_explored_have nsubjpass_explored_approaches mark_explored_Although nn_combination_system nn_combination_SMT prep_to_approaches_combination amod_approaches_various advcl_``_explored
D09-1114	P07-1040	o	3.2 System Combination Scheme In our work we use a sentence-level system combination model to select best translation hypothesis from the candidate pool -LRB- -RRB- This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method -LRB- Rosti et al. 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et nn_method_combination amod_method_word-level det_method_the prep_in_done_method mark_done_as nn_network_confusion det_network_a advcl_performing_done prep_over_performing_network dobj_performing_decoding amod_translations_existing det_translations_the prepc_instead_of_use_performing dobj_use_translations advmod_use_only nsubj_use_we mark_use_since dep_model_Rosti ccomp_model_use amod_model_reranking dep_hypotheses_model det_hypotheses_a cop_hypotheses_be aux_hypotheses_to xcomp_viewed_hypotheses auxpass_viewed_be advmod_viewed_also aux_viewed_can nsubjpass_viewed_method det_method_This nn_pool_candidate det_pool_the prep_from_hypothesis_pool nn_hypothesis_translation amod_hypothesis_best dobj_select_hypothesis aux_select_to nn_model_combination nn_model_system amod_model_sentence-level det_model_a dep_use_viewed vmod_use_select dobj_use_model nsubj_use_we nsubj_use_Scheme poss_work_our prep_in_Scheme_work nn_Scheme_Combination nn_Scheme_System num_Scheme_3.2
D09-1115	P07-1040	o	ps -LRB- arc -RRB- is increased by 1110 1 / -LRB- k +1 -RRB- if the hypothesis ranking k in the system s contains the arc -LRB- Rosti et al. 2007a He et al. 2008 -RRB-	dep_al._2008 nn_al._et dep_He_al. dep_Rosti_He appos_Rosti_2007a dep_Rosti_al. nn_Rosti_et dep_arc_Rosti det_arc_the dobj_contains_arc dep_s_contains nsubj_s_k mark_s_if det_system_the prep_in_k_system amod_k_ranking nn_k_hypothesis det_k_the num_k_+1 dep_1_k number_1_1110 advcl_increased_s agent_increased_1 auxpass_increased_is nsubjpass_increased_ps appos_ps_arc
D09-1115	P07-1040	p	In recent several years the system combination methods based on confusion networks developed rapidly -LRB- Bangalore et al. 2001 Matusov et al. 2006 Sim et al. 2007 Rosti et al. 2007a Rosti et al. 2007b Rosti et al. 2008 He et al. 2008 -RRB- which show state-of-the-art performance in benchmarks	prep_in_performance_benchmarks amod_performance_state-of-the-art dobj_show_performance nsubj_show_which dep_al._2008 nn_al._et rcmod_He_show dep_He_al. num_Rosti_2008 nn_Rosti_al. nn_Rosti_et appos_al._2007b nn_al._et nn_al._Rosti appos_al._2007a nn_al._et nn_al._Rosti num_Sim_2007 nn_Sim_al. nn_Sim_et conj_Matusov_He conj_Matusov_Rosti conj_Matusov_al. conj_Matusov_al. conj_Matusov_Sim num_Matusov_2006 nn_Matusov_al. nn_Matusov_et dep_Bangalore_Matusov appos_Bangalore_2001 dep_Bangalore_al. nn_Bangalore_et dep_developed_Bangalore advmod_developed_rapidly nsubj_developed_methods prep_in_developed_years nn_networks_confusion prep_on_based_networks vmod_methods_based nn_methods_combination nn_methods_system det_methods_the amod_years_several amod_years_recent
D09-1125	P07-1040	o	al 2006 Rosti et al. 2007a -RRB-	nn_2007a_al. nn_2007a_et dep_2007a_Rosti dep_2007a_2006 nn_2006_al
N09-2019	P07-1040	p	It is very likely that even greater gains can be achieved by more complicated combination schemes -LRB- Rosti et al. 2007 -RRB- although significantly more effort in tuning would be required	auxpass_required_be aux_required_would nsubjpass_required_effort mark_required_although prep_in_effort_tuning amod_effort_more advmod_more_significantly amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_schemes_Rosti nn_schemes_combination amod_schemes_complicated advmod_complicated_more advcl_achieved_required agent_achieved_schemes auxpass_achieved_be aux_achieved_can nsubjpass_achieved_gains mark_achieved_that amod_gains_greater advmod_greater_even ccomp_likely_achieved advmod_likely_very cop_likely_is nsubj_likely_It
P09-1065	P07-1040	o	5.3 Comparison with System Combination We re-implemented a state-of-the-art system combination method -LRB- Rosti et al. 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_method_Rosti nn_method_combination nn_method_system amod_method_state-of-the-art det_method_a dobj_re-implemented_method nsubj_re-implemented_We nn_Combination_System rcmod_Comparison_re-implemented prep_with_Comparison_Combination num_Comparison_5.3 dep_``_Comparison
P09-1065	P07-1040	p	In machine translation confusion-network based combination techniques -LRB- e.g. -LRB- Rosti et al. 2007 He et al. 2008 -RRB- -RRB- have achieved the state-of-theart performance in MT evaluations	nn_evaluations_MT prep_in_performance_evaluations amod_performance_state-of-theart det_performance_the dobj_achieved_performance aux_achieved_have nsubj_achieved_techniques prep_in_achieved_translation dep_2008_al. dep_2008_He nn_al._et dep_Rosti_2008 num_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_e.g._Rosti dep_techniques_e.g. nn_techniques_combination amod_techniques_based nn_techniques_confusion-network nn_translation_machine
P09-1065	P07-1040	p	Recent several years have witnessed the rapid development of system combination methods based on confusion networks -LRB- e.g. -LRB- Rosti et al. 2007 He et al. 2008 -RRB- -RRB- which show state-of-theart performance in MT benchmarks	nn_benchmarks_MT prep_in_performance_benchmarks amod_performance_state-of-theart dobj_show_performance nsubj_show_which dep_2008_al. dep_2008_He nn_al._et dep_Rosti_2008 num_Rosti_2007 dep_Rosti_al. nn_Rosti_et appos_e.g._Rosti dep_networks_e.g. nn_networks_confusion prep_on_based_networks nn_methods_combination nn_methods_system rcmod_development_show vmod_development_based prep_of_development_methods amod_development_rapid det_development_the dobj_witnessed_development aux_witnessed_have nsubj_witnessed_years amod_years_several amod_years_Recent
P09-1106	P07-1040	o	Among the four steps the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems -LRB- Rosti et al 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al nn_Rosti_et nn_systems_MT amod_systems_different prep_from_outputs_systems prep_between_orders_outputs nn_orders_word amod_orders_varying det_orders_the prep_due_to_method_orders det_method_the prep_to_challenge_method amod_challenge_biggest det_challenge_the dep_presents_Rosti dobj_presents_challenge nsubj_presents_alignment prep_among_presents_steps nn_alignment_hypothesis det_alignment_the num_steps_four det_steps_the
P09-1106	P07-1040	o	Similar to -LRB- Rosti et al. 2007a -RRB- each word in the hypothesis is assigned with a rank-based score of 1 / -LRB- 1 -RRB- r + where r is the rank of the hypothesis	det_hypothesis_the prep_of_rank_hypothesis det_rank_the cop_rank_is nsubj_rank_r advmod_rank_where rcmod_r_rank cc_r_+ dep_1_r prep_of_score_1 amod_score_rank-based det_score_a dep_assigned_1 prep_with_assigned_score auxpass_assigned_is nsubjpass_assigned_word ccomp_assigned_Similar det_hypothesis_the prep_in_word_hypothesis det_word_each appos_Rosti_2007a dep_Rosti_al. nn_Rosti_et dep_to_Rosti prep_Similar_to
P09-1106	P07-1040	o	-LRB- 2007 -RRB- Rosti et al.	nn_al._et dep_Rosti_al. dep_Rosti_2007
P09-1106	P07-1040	o	-LRB- 2007a -RRB- and Rosti et al.	nn_al._et nn_al._Rosti conj_and_2007a_al. dep_''_al. dep_''_2007a
P09-1106	P07-1040	o	We follow the work of -LRB- Sim et al. 2007 Rosti et al. 2007a Rosti et al. 2007b He et al. 2008 -RRB- and choose the hypothesis that best agrees with other hypotheses on average as the backbone by applying Minimum Bayes Risk -LRB- MBR -RRB- decoding -LRB- Kumar and Byrne 2004 -RRB-	num_Kumar_2004 conj_and_Kumar_Byrne appos_decoding_Byrne appos_decoding_Kumar nn_decoding_Risk appos_Risk_MBR nn_Risk_Bayes nn_Risk_Minimum dobj_applying_decoding det_backbone_the prep_on_hypotheses_average amod_hypotheses_other prepc_by_agrees_applying prep_as_agrees_backbone prep_with_agrees_hypotheses nsubj_agrees_best mark_agrees_that ccomp_hypothesis_agrees det_hypothesis_the dobj_choose_hypothesis conj_al._2008 nn_al._et conj_and_He_choose dep_He_al. appos_al._2007b nn_al._et nn_al._Rosti appos_al._2007a nn_al._et nn_al._Rosti dep_Sim_choose dep_Sim_He conj_Sim_al. conj_Sim_al. appos_Sim_2007 dep_Sim_al. nn_Sim_et prep_of_work_Sim det_work_the dobj_follow_work nsubj_follow_We
P09-1106	P07-1040	p	Confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination such as sentence level hypothesis selection by voting and source sentence re-decoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs -LRB- Rosti et al. 2007a Huang and Papineni 2007 -RRB-	dep_Huang_2007 conj_and_Huang_Papineni dep_Rosti_Papineni dep_Rosti_Huang appos_Rosti_2007a dep_Rosti_al. nn_Rosti_et nn_pairs_hypotheses nn_pairs_target conj_and_sentences_pairs nn_sentences_source det_sentences_the prep_from_learned_pairs prep_from_learned_sentences auxpass_learned_are nsubjpass_learned_that nn_models_translation rcmod_phrases_learned conj_or_phrases_models det_phrases_the dep_using_Rosti dobj_using_models dobj_using_phrases nn_re-decoding_sentence nn_re-decoding_source conj_and_voting_re-decoding prep_by_selection_re-decoding prep_by_selection_voting nn_selection_hypothesis nn_selection_level nn_selection_sentence dep_combination_using prep_such_as_combination_selection nn_combination_system amod_combination_based dep_techniques_combination amod_techniques_other pobj_advantage_techniques prepc_compared_with_advantage_with amod_advantage_promising dobj_shown_advantage aux_shown_has nsubj_shown_combination nn_translation_machine prep_for_combination_translation nn_combination_system amod_combination_based nn_combination_network nn_combination_Confusion
P09-1106	P07-1040	o	TER-based TER-based word alignment method -LRB- Sim et al. 2007 Rosti et al. 2007a Rosti et al. 2007b -RRB- is an extension of multiple string matching algorithm based on Levenshtein edit distance -LRB- Bangalore et al. 2001 -RRB-	amod_Bangalore_2001 dep_Bangalore_al. nn_Bangalore_et dobj_edit_distance nsubj_edit_Levenshtein prepc_on_based_edit vmod_algorithm_based amod_algorithm_matching nn_algorithm_string amod_string_multiple dep_extension_Bangalore prep_of_extension_algorithm det_extension_an cop_extension_is nsubj_extension_method appos_al._2007b nn_al._et nn_al._Rosti appos_al._2007a nn_al._et nn_al._Rosti dep_Sim_al. conj_Sim_al. appos_Sim_2007 dep_Sim_al. nn_Sim_et dep_method_Sim nn_method_alignment nn_method_word amod_method_TER-based dep_method_TER-based
W08-0309	P07-1040	o	73 ID Participant BBN-COMBO BBN system combination -LRB- Rosti et al. 2008 -RRB- CMU-COMBO Carnegie Mellon University system combination -LRB- Jayaraman and Lavie 2005 -RRB- CMU-GIMPEL Carnegie Mellon University Gimpel -LRB- Gimpel and Smith 2008 -RRB- CMU-SMT Carnegie Mellon University SMT -LRB- Bach et al. 2008 -RRB- CMU-STATXFER Carnegie Mellon University Stat-XFER -LRB- Hanneman et al. 2008 -RRB- CU-TECTOMT Charles University TectoMT -LRB- Zabokrtsky et al. 2008 -RRB- CU-BOJAR Charles University Bojar -LRB- Bojar and Hajic 2008 -RRB- CUED Cambridge University -LRB- Blackwood et al. 2008 -RRB- DCU Dublin City University -LRB- Tinsley et al. 2008 -RRB- LIMSI LIMSI -LRB- Dechelotte et al. 2008 -RRB- LIU Linkoping University -LRB- Stymne et al. 2008 -RRB- LIUM-SYSTRAN LIUM / Systran -LRB- Schwenk et al. 2008 -RRB- MLOGIC Morphologic -LRB- Novak et al. 2008 -RRB- PCT a commercial MT provider from the Czech Republic RBMT16 Babelfish Lingenio Lucy OpenLogos ProMT SDL -LRB- ordering anonymized -RRB- SAAR University of Saarbruecken -LRB- Eisele et al. 2008 -RRB- SYSTRAN Systran -LRB- Dugast et al. 2008 -RRB- UCB University of California at Berkeley -LRB- Nakov 2008 -RRB- UCL University College London -LRB- Wang and Shawe-Taylor 2008 -RRB- UEDIN University of Edinburgh -LRB- Koehn et al. 2008 -RRB- UEDIN-COMBO University of Edinburgh system combination -LRB- Josh Schroeder -RRB- UMD University of Maryland -LRB- Dyer 2007 -RRB- UPC Universitat Politecnica de Catalunya Barcelona -LRB- Khalilov et al. 2008 -RRB- UW University of Washington -LRB- Axelrod et al. 2008 -RRB- XEROX Xerox Research Centre Europe -LRB- Nikoulina and Dymetman 2008 -RRB- Table 2 Participants in the shared translation task	nn_task_translation amod_task_shared det_task_the prep_in_Participants_task num_Table_2 dep_Table_Dymetman dep_Table_Nikoulina dep_Table_Europe dep_Nikoulina_2008 conj_and_Nikoulina_Dymetman nn_Europe_Centre nn_Europe_Research nn_Europe_Xerox nn_Europe_XEROX dep_Axelrod_2008 dep_Axelrod_al. nn_Axelrod_et dep_University_Table dep_University_Axelrod prep_of_University_Washington nn_University_UW nn_University_Catalunya amod_University_de nn_University_Politecnica nn_University_Universitat nn_University_UPC amod_Khalilov_2008 dep_Khalilov_al. nn_Khalilov_et dep_Catalunya_Khalilov appos_Catalunya_Barcelona dep_Dyer_2007 dep_University_University dep_University_Dyer prep_of_University_Maryland nn_University_UMD nn_University_University nn_Schroeder_Josh nn_combination_system nn_combination_Edinburgh appos_University_Schroeder prep_of_University_combination nn_University_UEDIN-COMBO amod_Koehn_2008 dep_Koehn_al. nn_Koehn_et prep_of_University_Edinburgh nn_University_UEDIN dep_University_Shawe-Taylor dep_University_Wang dep_University_London dep_Wang_2008 conj_and_Wang_Shawe-Taylor nn_London_College nn_London_University nn_London_UCL dep_London_Nakov dep_London_University dep_London_Systran dep_London_University dep_London_ordering nn_London_SDL dep_Nakov_2008 prep_at_University_Berkeley prep_of_University_California nn_University_UCB amod_Dugast_2008 dep_Dugast_al. nn_Dugast_et dep_Systran_Dugast nn_Systran_SYSTRAN dep_Eisele_2008 dep_Eisele_al. nn_Eisele_et dep_University_Eisele prep_of_University_Saarbruecken nn_University_SAAR dep_ordering_anonymized dep_Lingenio_University dep_Lingenio_Koehn conj_Lingenio_University conj_Lingenio_ProMT conj_Lingenio_OpenLogos conj_Lingenio_Lucy appos_Babelfish_Lingenio nn_Babelfish_RBMT16 nn_Babelfish_Republic nn_Babelfish_Czech det_Babelfish_the prep_from_provider_Babelfish nn_provider_MT amod_provider_commercial det_provider_a dep_PCT_provider amod_PCT_Morphologic nn_PCT_MLOGIC dep_PCT_Schwenk nn_PCT_Systran amod_Novak_2008 dep_Novak_al. nn_Novak_et dep_Morphologic_Novak amod_Schwenk_2008 dep_Schwenk_al. nn_Schwenk_et dep_LIUM_PCT nn_LIUM_LIUM-SYSTRAN dep_LIUM_Stymne dep_Stymne_2008 dep_Stymne_al. nn_Stymne_et dep_University_Participants dep_University_LIUM nn_University_Linkoping nn_University_LIU amod_Dechelotte_2008 dep_Dechelotte_al. nn_Dechelotte_et dep_LIMSI_University dep_LIMSI_Dechelotte nn_LIMSI_LIMSI dep_Tinsley_2008 dep_Tinsley_al. nn_Tinsley_et dep_University_LIMSI dep_University_Tinsley nn_University_City nn_University_Dublin nn_University_DCU amod_Blackwood_2008 dep_Blackwood_al. nn_Blackwood_et nn_University_Cambridge nn_University_CUED dep_Bojar_2008 conj_and_Bojar_Hajic dep_Bojar_University dep_Bojar_Hajic dep_Bojar_Bojar nn_Bojar_University nn_Bojar_Charles nn_Bojar_CU-BOJAR dep_Bojar_Zabokrtsky amod_Zabokrtsky_2008 dep_Zabokrtsky_al. nn_Zabokrtsky_et dep_TectoMT_University dep_TectoMT_Blackwood dep_TectoMT_Bojar nn_TectoMT_University nn_TectoMT_Charles nn_TectoMT_CU-TECTOMT amod_Hanneman_2008 dep_Hanneman_al. nn_Hanneman_et dep_Stat-XFER_TectoMT dep_Stat-XFER_Hanneman nn_Stat-XFER_University nn_Stat-XFER_Mellon nn_Stat-XFER_Carnegie nn_Stat-XFER_CMU-STATXFER amod_Bach_2008 dep_Bach_al. nn_Bach_et dep_SMT_Stat-XFER dep_SMT_Bach nn_SMT_University nn_SMT_Mellon nn_SMT_Carnegie nn_SMT_CMU-SMT dep_SMT_Smith dep_SMT_Gimpel dep_SMT_Gimpel dep_Gimpel_2008 conj_and_Gimpel_Smith nn_Gimpel_University nn_Gimpel_Mellon nn_Gimpel_Carnegie nn_Gimpel_CMU-GIMPEL dep_Gimpel_Lavie dep_Gimpel_Jayaraman dep_Gimpel_combination dep_Jayaraman_2005 conj_and_Jayaraman_Lavie nn_combination_system nn_combination_University nn_combination_Mellon nn_combination_Carnegie nn_combination_CMU-COMBO dep_combination_Rosti dep_combination_combination amod_Rosti_2008 dep_Rosti_al. nn_Rosti_et nn_combination_system nn_combination_BBN nn_combination_BBN-COMBO nn_combination_Participant nn_combination_ID num_combination_73
W08-0329	P07-1040	o	As in -LRB- Rosti et al. 2007 -RRB- confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models	nn_models_language prep_with_rescored_models nsubjpass_rescored_which conj_and_expanded_rescored auxpass_expanded_is nsubjpass_expanded_which rcmod_lattice_rescored rcmod_lattice_expanded det_lattice_a prep_into_joined_lattice auxpass_joined_are nsubjpass_joined_built det_skeletons_all prep_around_built_skeletons nsubj_built_networks prep_built_As nn_networks_confusion amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_in_Rosti pcomp_As_in
W08-0329	P07-1040	o	Other scores for the word arc are set as in -LRB- Rosti et al. 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_in_Rosti pcomp_as_in prep_set_as auxpass_set_are nsubjpass_set_scores nn_arc_word det_arc_the prep_for_scores_arc amod_scores_Other
W08-0329	P07-1040	o	The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model -LRB- Matusov et al. 2006 -RRB- or edit distance alignments allowing shifts -LRB- Rosti et al. 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_shifts_Rosti dobj_allowing_shifts vmod_alignments_allowing nn_alignments_distance dobj_edit_alignments dep_Matusov_2006 dep_Matusov_al. nn_Matusov_et nn_model_alignment nn_model_HMM det_model_a dep_alignments_Matusov prep_from_alignments_model amod_alignments_symmetric prep_on_based_alignments conj_or_algorithms_edit vmod_algorithms_based nn_algorithms_alignment amod_algorithms_pair-wise amod_algorithms_used dep_approaches_edit dep_approaches_algorithms amod_approaches_recent det_approaches_The ccomp_``_approaches
W08-0329	P07-1040	o	to the pair-wise TER alignment described in -LRB- Rosti et al. 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_in_Rosti prep_described_in vmod_alignment_described nn_alignment_TER amod_alignment_pair-wise det_alignment_the pobj_to_alignment dep_``_to
W09-0405	P07-1040	o	Previous work on building hybrid systems includes among others approaches using reranking regeneration with an SMT decoder -LRB- Eisele et al. 2008 Chen et al. 2007 -RRB- and confusion networks -LRB- Matusov et al. 2006 Rosti et al. 2007 He et al. 2008 -RRB-	dep_al._2008 nn_al._et dep_He_al. num_Rosti_2007 nn_Rosti_al. nn_Rosti_et dep_Matusov_He dep_Matusov_Rosti appos_Matusov_2006 dep_Matusov_al. nn_Matusov_et nn_networks_confusion num_Chen_2007 nn_Chen_al. nn_Chen_et dep_Eisele_Chen appos_Eisele_2008 dep_Eisele_al. nn_Eisele_et appos_decoder_Eisele nn_decoder_SMT det_decoder_an prep_with_regeneration_decoder conj_and_reranking_networks conj_and_reranking_regeneration dobj_using_networks dobj_using_regeneration dobj_using_reranking dep_approaches_Matusov vmod_approaches_using dobj_includes_approaches prep_among_includes_others nsubj_includes_work nn_systems_hybrid nn_systems_building prep_on_work_systems amod_work_Previous
W09-0407	P07-1040	n	In contrast to existing approaches -LRB- Jayaraman and Lavie 2005 Rosti et al. 2007 -RRB- the context of the whole corpus rather than a single sentence is considered in this iterative unsupervised procedure yielding a more reliable alignment	amod_alignment_reliable det_alignment_a advmod_reliable_more dobj_yielding_alignment amod_procedure_unsupervised amod_procedure_iterative det_procedure_this xcomp_considered_yielding prep_in_considered_procedure auxpass_considered_is nsubjpass_considered_sentence nsubjpass_considered_context prep_in_considered_contrast amod_sentence_single det_sentence_a amod_corpus_whole det_corpus_the conj_negcc_context_sentence prep_of_context_corpus det_context_the num_Rosti_2007 nn_Rosti_al. nn_Rosti_et dep_Jayaraman_Rosti conj_and_Jayaraman_2005 conj_and_Jayaraman_Lavie appos_approaches_2005 appos_approaches_Lavie appos_approaches_Jayaraman amod_approaches_existing prep_to_contrast_approaches
W09-0407	P07-1040	p	In our experience this approach is advantageous in terms of translation quality e.g. by 0.7 % in BLEU compared to a minimum Bayes risk primary -LRB- Rosti et al. 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_primary_Rosti amod_risk_primary nn_risk_Bayes amod_risk_minimum det_risk_a prep_in_%_BLEU num_%_0.7 prep_by_e.g._risk prepc_compared_to_e.g._to prep_by_e.g._% nn_quality_translation prep_of_terms_quality dep_advantageous_e.g. prep_in_advantageous_terms cop_advantageous_is nsubj_advantageous_approach prep_in_advantageous_experience det_approach_this poss_experience_our
W09-0409	P07-1040	p	The availability of the TER software has made it easy to build a high performance system combination baseline -LRB- Rosti et al. 2007 -RRB-	nn_al._et amod_Rosti_2007 dep_Rosti_al. nn_baseline_combination nn_baseline_system nn_baseline_performance amod_baseline_high det_baseline_a dobj_build_baseline aux_build_to dep_build_easy nsubj_build_it dep_made_Rosti xcomp_made_build aux_made_has nsubj_made_availability nn_software_TER det_software_the prep_of_availability_software det_availability_The
W09-0409	P07-1040	o	The hypothesis scores and tuning are identical to the setup used in -LRB- Rosti et al. 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_in_Rosti prep_used_in vmod_setup_used det_setup_the prep_to_identical_setup cop_identical_are nsubj_identical_tuning nsubj_identical_scores conj_and_scores_tuning nn_scores_hypothesis det_scores_The
W09-0411	P07-1040	o	Besides continued research on improving MT techniques one line of research is dedicated to better exploitation of existing methods for the combination of their respective advantages -LRB- Macherey and Och 2007 Rosti et al. 2007a -RRB-	appos_Rosti_2007a dep_Rosti_al. nn_Rosti_et dep_Macherey_Rosti conj_and_Macherey_2007 conj_and_Macherey_Och appos_advantages_2007 appos_advantages_Och appos_advantages_Macherey amod_advantages_respective poss_advantages_their prep_of_combination_advantages det_combination_the prep_for_methods_combination amod_methods_existing prep_of_exploitation_methods amod_exploitation_better prep_to_dedicated_exploitation auxpass_dedicated_is nsubjpass_dedicated_line prep_besides_dedicated_research prep_of_line_research num_line_one nn_techniques_MT dobj_improving_techniques prepc_on_research_improving amod_research_continued
W09-0411	P07-1040	o	This can be seen as a simplified version of -LRB- Rosti et al. 2007b -RRB-	appos_Rosti_2007b dep_Rosti_al. nn_Rosti_et prep_of_version_Rosti amod_version_simplified det_version_a prep_as_seen_version auxpass_seen_be aux_seen_can nsubjpass_seen_This
W09-0441	P07-1040	o	Such a technique has been used with TER to combine the output of multiple translation systems -LRB- Rosti et al. 2007 -RRB-	amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et nn_systems_translation amod_systems_multiple prep_of_output_systems det_output_the dep_combine_Rosti dobj_combine_output aux_combine_to xcomp_used_combine prep_with_used_TER auxpass_used_been aux_used_has nsubjpass_used_technique det_technique_a amod_technique_Such
D07-1102	P07-1050	o	Recent work shows that k-best maximum spanning tree -LRB- MST -RRB- parsing and reranking is also viable -LRB- Hall 2007 -RRB-	amod_Hall_2007 dep_viable_Hall advmod_viable_also cop_viable_is nsubj_viable_k-best mark_viable_that conj_and_parsing_reranking nn_parsing_tree appos_tree_MST dobj_spanning_reranking dobj_spanning_parsing vmod_maximum_spanning ccomp_k-best_maximum ccomp_shows_viable nsubj_shows_work amod_work_Recent
D07-1102	P07-1050	o	2.1.4 Model Features Our MST models are based on the features described in -LRB- Hall 2007 -RRB- specifically we use features based on a dependency nodes form lemma coarse and fine part-of-speech tag and morphologicalstring attributes	amod_attributes_morphologicalstring amod_tag_part-of-speech amod_tag_fine conj_and_form_tag conj_and_form_coarse conj_and_form_lemma nn_nodes_dependency det_nodes_a prep_on_based_nodes conj_and_use_attributes dep_use_tag dep_use_coarse dep_use_lemma dep_use_form vmod_use_based dobj_use_features nsubj_use_we advmod_use_specifically dep_Hall_2007 prep_in_described_Hall vmod_features_described det_features_the prep_on_based_features auxpass_based_are nsubjpass_based_models nn_models_MST poss_models_Our parataxis_Features_attributes parataxis_Features_use rcmod_Features_based nn_Features_Model num_Features_2.1.4 dep_``_Features
D07-1102	P07-1050	o	The tree-based reranker includes the features described in -LRB- Hall 2007 -RRB- as well as features based on non-projective edge attributes explored in -LRB- Havelka 2007a Havelka 2007b -RRB-	appos_Havelka_2007b dep_Havelka_Havelka appos_Havelka_2007a dep_in_Havelka prep_explored_in vmod_attributes_explored amod_edge_non-projective prep_on_based_edge dep_features_attributes vmod_features_based appos_Hall_2007 prep_in_described_Hall conj_and_features_features vmod_features_described det_features_the dobj_includes_features dobj_includes_features nsubj_includes_reranker amod_reranker_tree-based det_reranker_The
D07-1102	P07-1050	o	3 Results and Analysis Hall -LRB- 2007 -RRB- shows that the oracle parsing accuracy of a k-best edge-factored MST parser is considerably higher than the one-best score of the same parser even when k is small	cop_small_is nsubj_small_k advmod_small_when advmod_small_even amod_parser_same det_parser_the prep_of_score_parser amod_score_one-best det_score_the advcl_higher_small prep_than_higher_score advmod_higher_considerably cop_higher_is nsubj_higher_accuracy mark_higher_that nn_parser_MST amod_parser_edge-factored amod_parser_k-best det_parser_a prep_of_accuracy_parser nn_accuracy_parsing nn_accuracy_oracle det_accuracy_the ccomp_shows_higher nsubj_shows_Hall nsubj_shows_Results appos_Hall_2007 nn_Hall_Analysis conj_and_Results_Hall num_Results_3
D08-1059	P07-1050	o	Nakagawa -LRB- 2007 -RRB- and Hall -LRB- 2007 -RRB- also showed the effectiveness of global features in improving the accuracy of graph-based parsing using the approximate Gibbs sampling method and a reranking approach respectively	nn_approach_reranking det_approach_a advmod_method_respectively conj_and_method_approach nn_method_sampling nn_method_Gibbs amod_method_approximate det_method_the dobj_using_approach dobj_using_method amod_parsing_graph-based prep_of_accuracy_parsing det_accuracy_the dobj_improving_accuracy amod_features_global prep_of_effectiveness_features det_effectiveness_the vmod_showed_using prepc_in_showed_improving dobj_showed_effectiveness advmod_showed_also nsubj_showed_Hall nsubj_showed_Nakagawa appos_Hall_2007 conj_and_Nakagawa_Hall appos_Nakagawa_2007
D08-1059	P07-1050	o	An existing method to combine multiple parsing algorithms is the ensemble approach -LRB- Sagae and Lavie 2006a -RRB- which was reported to be useful in improving dependency parsing -LRB- Hall et al. 2007 -RRB-	amod_Hall_2007 dep_Hall_al. nn_Hall_et nn_parsing_dependency dobj_improving_parsing dep_useful_Hall prepc_in_useful_improving cop_useful_be aux_useful_to xcomp_reported_useful auxpass_reported_was nsubjpass_reported_which dep_Sagae_2006a conj_and_Sagae_Lavie rcmod_approach_reported dep_approach_Lavie dep_approach_Sagae nn_approach_ensemble det_approach_the cop_approach_is nsubj_approach_method nn_algorithms_parsing amod_algorithms_multiple dobj_combine_algorithms aux_combine_to vmod_method_combine amod_method_existing det_method_An
P08-1108	P07-1050	o	Thus Nakagawa -LRB- 2007 -RRB- and Hall -LRB- 2007 -RRB- both try to overcome the limited feature scope of graph-based models by adding global features in the former case using Gibbs sampling to deal with the intractable inference problem in the latter case using a re-ranking scheme	amod_scheme_re-ranking det_scheme_a dobj_using_scheme amod_case_latter det_case_the nn_problem_inference amod_problem_intractable det_problem_the prep_with_deal_problem aux_deal_to nn_sampling_Gibbs xcomp_using_using prep_in_using_case vmod_using_deal dobj_using_sampling amod_case_former det_case_the amod_features_global dobj_adding_features amod_models_graph-based prep_of_scope_models nn_scope_feature amod_scope_limited det_scope_the prepc_by_overcome_adding dobj_overcome_scope aux_overcome_to dep_try_using prep_in_try_case xcomp_try_overcome dep_try_both nsubj_try_Hall nsubj_try_Nakagawa advmod_try_Thus appos_Hall_2007 conj_and_Nakagawa_Hall dep_Nakagawa_2007
D08-1013	P07-1055	o	-LRB- McDonald et al 2007 Ivan et al 2008 -RRB- proposed a structured model based on CRFs for jointly classifying the sentiment of text at varying levels of granularity	prep_of_levels_granularity amod_levels_varying prep_of_sentiment_text det_sentiment_the prep_at_classifying_levels dobj_classifying_sentiment advmod_classifying_jointly prepc_for_CRFs_classifying prep_on_based_CRFs vmod_model_based amod_model_structured det_model_a dobj_proposed_model nsubj_proposed_McDonald num_al_2008 nn_al_et nn_al_Ivan dep_McDonald_al dep_McDonald_2007 dep_McDonald_al nn_McDonald_et
D09-1019	P07-1055	o	There are many research directions e.g. sentiment classification -LRB- classifying an opinion document as positive or negative -RRB- -LRB- e.g. Pang Lee and Vaithyanathan 2002 Turney 2002 -RRB- subjectivity classification -LRB- determining whether a sentence is subjective or objective and its associated opinion -RRB- -LRB- Wiebe and Wilson 2002 Yu and Hatzivassiloglou 2003 Wilson et al 2004 Kim and Hovy 2004 Riloff and Wiebe 2005 -RRB- feature/topic-based sentiment analysis -LRB- assigning positive or negative sentiments to topics or product features -RRB- -LRB- Hu and Liu 2004 Popescu and Etzioni 2005 Carenini et al. 2005 Ku et al. 2006 Kobayashi Inui and Matsumoto 2007 Titov and McDonald	num_Ku_2006 nn_Ku_al. nn_Ku_et num_Carenini_2005 nn_Carenini_al. nn_Carenini_et conj_and_Popescu_McDonald conj_and_Popescu_Titov conj_and_Popescu_2007 conj_and_Popescu_Matsumoto conj_and_Popescu_Inui conj_and_Popescu_Kobayashi conj_and_Popescu_Ku conj_and_Popescu_Carenini conj_and_Popescu_2005 conj_and_Popescu_Etzioni num_Liu_2004 dep_Hu_McDonald dep_Hu_Titov dep_Hu_2007 dep_Hu_Matsumoto dep_Hu_Inui dep_Hu_Kobayashi dep_Hu_Ku dep_Hu_Carenini dep_Hu_2005 dep_Hu_Etzioni dep_Hu_Popescu conj_and_Hu_Liu nn_features_product conj_or_topics_features amod_sentiments_negative amod_sentiments_positive conj_or_positive_negative prep_to_assigning_features prep_to_assigning_topics dobj_assigning_sentiments dep_analysis_Liu dep_analysis_Hu dep_analysis_assigning nn_analysis_sentiment amod_analysis_feature/topic-based amod_al_2004 nn_al_et nn_al_Wilson dep_Yu_2005 conj_and_Yu_Wiebe conj_and_Yu_Riloff conj_and_Yu_2004 conj_and_Yu_Hovy conj_and_Yu_Kim conj_and_Yu_al conj_and_Yu_2003 conj_and_Yu_Hatzivassiloglou dep_Wiebe_Wiebe dep_Wiebe_Riloff dep_Wiebe_2004 dep_Wiebe_Hovy dep_Wiebe_Kim dep_Wiebe_al dep_Wiebe_2003 dep_Wiebe_Hatzivassiloglou dep_Wiebe_Yu conj_and_Wiebe_2002 conj_and_Wiebe_Wilson dep_opinion_2002 dep_opinion_Wilson dep_opinion_Wiebe amod_opinion_associated poss_opinion_its nsubj_objective_sentence conj_or_subjective_objective cop_subjective_is nsubj_subjective_sentence mark_subjective_whether det_sentence_a dobj_determining_analysis conj_and_determining_opinion ccomp_determining_objective ccomp_determining_subjective dep_classification_opinion dep_classification_determining nn_classification_subjectivity dep_classification_2002 dep_classification_Vaithyanathan dep_classification_Lee dep_classification_Pang dep_classification_e.g. dep_Turney_2002 dep_Pang_Turney conj_and_Pang_2002 conj_and_Pang_Vaithyanathan conj_and_Pang_Lee conj_or_positive_negative nn_document_opinion det_document_an parataxis_classifying_classification prep_as_classifying_negative prep_as_classifying_positive dobj_classifying_document dep_classification_classifying nn_classification_sentiment pobj_e.g._classification prep_directions_e.g. nn_directions_research amod_directions_many nsubj_are_directions expl_are_There ccomp_``_are
D09-1019	P07-1055	o	One of the main directions is sentiment classification which classifies the whole opinion document -LRB- e.g. a product review -RRB- as positive or negative -LRB- e.g. Pang et al 2002 Turney 2002 Dave et al 2003 Ng et al. 2006 McDonald et al 2007 -RRB-	appos_al_2007 nn_al_et nn_al_McDonald num_al._2006 nn_al._et nn_al._Ng amod_al_2003 nn_al_et nn_al_Dave conj_Turney_al conj_Turney_al. conj_Turney_al appos_Turney_2002 dep_Pang_Turney num_Pang_2002 nn_Pang_al nn_Pang_et dep_e.g._Pang dep_positive_e.g. conj_or_positive_negative nn_review_product det_review_a dep_review_e.g. dep_document_review nn_document_opinion amod_document_whole det_document_the prep_as_classifies_negative prep_as_classifies_positive dobj_classifies_document nsubj_classifies_which rcmod_classification_classifies nn_classification_sentiment cop_classification_is nsubj_classification_One amod_directions_main det_directions_the prep_of_One_directions
D09-1019	P07-1055	o	Another important direction is classifying sentences as subjective or objective and classifying subjective sentences or clauses as positive or negative -LRB- Wiebe et al 1999 Wiebe and Wilson 2002 Yu and Hatzivassiloglou 2003 Wilson et al 2004 Kim and Hovy 2004 Riloff and Wiebe 2005 Gamon et al 2005 McDonald et al 2007 -RRB-	appos_al_2007 nn_al_et nn_al_McDonald num_al_2005 dep_Gamon_al nn_Gamon_et amod_al_2004 nn_al_et nn_al_Wilson conj_and_Wiebe_al conj_and_Wiebe_Gamon conj_and_Wiebe_2005 conj_and_Wiebe_Wiebe conj_and_Wiebe_Riloff conj_and_Wiebe_2004 conj_and_Wiebe_Hovy conj_and_Wiebe_Kim conj_and_Wiebe_al conj_and_Wiebe_2003 conj_and_Wiebe_Hatzivassiloglou conj_and_Wiebe_Yu conj_and_Wiebe_2002 conj_and_Wiebe_Wilson num_al_1999 nn_al_et dep_Wiebe_al dep_Wiebe_Gamon dep_Wiebe_2005 dep_Wiebe_Wiebe dep_Wiebe_Riloff dep_Wiebe_2004 dep_Wiebe_Hovy dep_Wiebe_Kim dep_Wiebe_al dep_Wiebe_2003 dep_Wiebe_Hatzivassiloglou dep_Wiebe_Yu dep_Wiebe_2002 dep_Wiebe_Wilson dep_Wiebe_Wiebe dep_Wiebe_al conj_or_positive_negative conj_or_sentences_clauses amod_sentences_subjective dep_classifying_Wiebe prep_as_classifying_negative prep_as_classifying_positive dobj_classifying_clauses dobj_classifying_sentences nsubj_classifying_direction conj_or_subjective_objective conj_and_classifying_classifying prep_as_classifying_objective prep_as_classifying_subjective dobj_classifying_sentences aux_classifying_is nsubj_classifying_direction amod_direction_important det_direction_Another
D09-1019	P07-1055	o	Several researchers also studied feature/topicbased sentiment analysis -LRB- e.g. Hu and Liu 2004 Popescu and Etzioni 2005 Ku et al 2006 Carenini et al 2006 Mei et al 2007 Ding Liu and Yu 2008 Titov and R. McDonald 2008 Stoyanov and Cardie 2008 Lu and Zhai 2008 -RRB-	nn_McDonald_R. appos_al_2007 nn_al_et nn_al_Mei appos_al_2006 nn_al_et nn_al_Carenini amod_al_2006 nn_al_et nn_al_Ku dep_Popescu_2008 conj_and_Popescu_Zhai conj_and_Popescu_Lu conj_and_Popescu_2008 conj_and_Popescu_Cardie conj_and_Popescu_Stoyanov conj_and_Popescu_2008 conj_and_Popescu_McDonald conj_and_Popescu_Titov conj_and_Popescu_2008 conj_and_Popescu_Yu conj_and_Popescu_Liu conj_and_Popescu_Ding conj_and_Popescu_al conj_and_Popescu_al conj_and_Popescu_al conj_and_Popescu_2005 conj_and_Popescu_Etzioni dep_Hu_Zhai dep_Hu_Lu dep_Hu_2008 dep_Hu_Cardie dep_Hu_Stoyanov dep_Hu_2008 dep_Hu_McDonald dep_Hu_Titov dep_Hu_2008 dep_Hu_Yu dep_Hu_Liu dep_Hu_Ding dep_Hu_al dep_Hu_al dep_Hu_al dep_Hu_2005 dep_Hu_Etzioni dep_Hu_Popescu num_Hu_2004 conj_and_Hu_Liu dep_e.g._Liu dep_e.g._Hu ccomp_-LRB-_e.g. nn_analysis_sentiment amod_analysis_feature/topicbased dobj_studied_analysis advmod_studied_also nsubj_studied_researchers amod_researchers_Several ccomp_``_studied
D08-1072	P07-1056	o	Domain adaptation deals with these feature distribution changes -LRB- Blitzer et al. 2007 Jiang and Zhai 2007 -RRB-	dep_Jiang_2007 conj_and_Jiang_Zhai dep_Blitzer_Zhai dep_Blitzer_Jiang appos_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et dep_changes_Blitzer nn_changes_distribution nn_changes_feature det_changes_these prep_with_deals_changes nn_deals_adaptation nn_deals_Domain
D08-1072	P07-1056	o	5 Datasets For evaluation we selected two domain adaptation datasets spam -LRB- Jiang and Zhai 2007 -RRB- and sentiment -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et dep_Jiang_2007 conj_and_Jiang_Zhai dep_spam_Blitzer conj_and_spam_sentiment appos_spam_Zhai appos_spam_Jiang dep_datasets_sentiment dep_datasets_spam nn_datasets_adaptation nn_datasets_domain num_datasets_two dobj_selected_datasets nsubj_selected_we rcmod_Datasets_selected prep_for_Datasets_evaluation num_Datasets_5 dep_``_Datasets
D09-1061	P07-1056	o	We use five sentiment classification datasets including the widely-used movie review dataset -LSB- MOV -RSB- -LRB- Pang et al. 2002 -RRB- as well as four datasets containing reviews of four different types of products from Amazon -LSB- books -LRB- BOO -RRB- DVDs -LRB- DVD -RRB- electronics -LRB- ELE -RRB- and kitchen appliances -LRB- KIT -RRB- -RSB- -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et appos_appliances_KIT nn_appliances_kitchen appos_electronics_ELE appos_DVDs_DVD conj_and_books_appliances conj_and_books_electronics appos_books_DVDs appos_books_BOO prep_from_products_Amazon dep_types_appliances dep_types_electronics dep_types_books prep_of_types_products amod_types_different num_types_four prep_of_reviews_types dobj_containing_reviews vmod_datasets_containing num_datasets_four amod_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_dataset_datasets dep_dataset_Pang appos_dataset_MOV nn_dataset_review nn_dataset_movie amod_dataset_widely-used det_dataset_the dep_datasets_Blitzer prep_including_datasets_datasets prep_including_datasets_dataset nn_datasets_classification nn_datasets_sentiment num_datasets_five dobj_use_datasets nsubj_use_We
D09-1061	P07-1056	o	However such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language -LRB- Banea et al. 2008 Wan 2008 -RRB- or a domain that is similar enough to the target domain -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et nn_domain_target det_domain_the dep_similar_Blitzer prep_to_similar_domain advmod_similar_enough cop_similar_is nsubj_similar_that rcmod_domain_similar det_domain_a dep_Wan_2008 dep_Banea_Wan appos_Banea_2008 dep_Banea_al. nn_Banea_et nn_language_target det_language_the amod_language_resource-rich det_language_a amod_annotations/lexica_projecting/translating conj_or_engine_domain dep_engine_Banea prep_to_engine_language prep_from_engine_language prep_for_engine_annotations/lexica nn_engine_translation nn_engine_corpus/machine amod_engine_parallel det_engine_a preconj_engine_either prep_of_existence_domain prep_of_existence_engine det_existence_the dobj_require_existence nsubj_require_methods advmod_require_However amod_methods_such
E09-3005	P07-1056	o	The problem itself has started to get attention only recently -LRB- Roark and Bacchiani 2003 Hara et al. 2005 Daume III and Marcu 2006 Daume III 2007 Blitzer et al. 2006 McClosky et al. 2006 Dredze et al. 2007 -RRB-	num_Dredze_2007 nn_Dredze_al. nn_Dredze_et num_McClosky_2006 nn_McClosky_al. nn_McClosky_et num_Blitzer_2006 nn_Blitzer_al. nn_Blitzer_et appos_III_2007 nn_III_Daume dep_III_Dredze conj_and_III_McClosky conj_and_III_Blitzer conj_and_III_III conj_and_III_2006 conj_and_III_Marcu nn_III_Daume num_Hara_2005 nn_Hara_al. nn_Hara_et dep_Roark_McClosky dep_Roark_Blitzer dep_Roark_III dep_Roark_2006 dep_Roark_Marcu dep_Roark_III conj_and_Roark_Hara conj_and_Roark_2003 conj_and_Roark_Bacchiani advmod_recently_only dep_get_Hara dep_get_2003 dep_get_Bacchiani dep_get_Roark advmod_get_recently dobj_get_attention aux_get_to xcomp_started_get aux_started_has nsubj_started_problem npadvmod_problem_itself det_problem_The
E09-3005	P07-1056	o	In contrast semi-supervised domain adaptation -LRB- Blitzer et al. 2006 McClosky et al. 2006 Dredze et al. 2007 -RRB- is the scenario in which in addition to the labeled source data we only have unlabeled and no labeled target domain data	nn_data_domain nn_data_target amod_data_labeled dep_unlabeled_data conj_and_unlabeled_no dobj_have_no dobj_have_unlabeled advmod_have_only nsubj_have_we prep_in_addition_to_have_data prep_in_have_which nn_data_source amod_data_labeled det_data_the rcmod_scenario_have det_scenario_the cop_scenario_is nsubj_scenario_adaptation prep_in_scenario_contrast num_Dredze_2007 nn_Dredze_al. nn_Dredze_et dep_McClosky_Dredze num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Blitzer_McClosky appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et appos_adaptation_Blitzer nn_adaptation_domain amod_adaptation_semi-supervised
E09-3005	P07-1056	n	2 Motivation and Prior Work While several authors have looked at the supervised adaptation case there are less -LRB- and especially less successful -RRB- studies on semi-supervised domain adaptation -LRB- McClosky et al. 2006 Blitzer et al. 2006 Dredze et al. 2007 -RRB-	num_Dredze_2007 nn_Dredze_al. nn_Dredze_et dep_Blitzer_Dredze num_Blitzer_2006 nn_Blitzer_al. nn_Blitzer_et dep_McClosky_Blitzer appos_McClosky_2006 dep_McClosky_al. nn_McClosky_et nn_adaptation_domain amod_adaptation_semi-supervised dep_studies_McClosky prep_on_studies_adaptation amod_studies_less dep_less_successful advmod_less_especially cc_less_and dep_less_less nsubj_are_studies expl_are_there ccomp_are_Work ccomp_are_Motivation nn_case_adaptation amod_case_supervised det_case_the prep_at_looked_case aux_looked_have nsubj_looked_authors mark_looked_While amod_authors_several amod_Work_Prior dep_Motivation_looked conj_and_Motivation_Work num_Motivation_2 ccomp_``_are
E09-3005	P07-1056	p	While SCL has been successfully applied to PoS tagging and Sentiment Analysis -LRB- Blitzer et al. 2006 Blitzer et al. 2007 -RRB- its effectiveness for parsing was rather unexplored	advmod_unexplored_rather cop_unexplored_was nsubj_unexplored_effectiveness advcl_unexplored_applied prep_for_effectiveness_parsing poss_effectiveness_its num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et dep_Blitzer_Blitzer appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et nn_Analysis_Sentiment conj_and_tagging_Analysis nn_tagging_PoS dep_applied_Blitzer prep_to_applied_Analysis prep_to_applied_tagging advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_SCL mark_applied_While
E09-3005	P07-1056	p	Similarly Structural Correspondence Learning -LRB- Blitzer et al. 2006 Blitzer et al. 2007 Blitzer 2008 -RRB- has proven to be successful for the two tasks examined PoS tagging and Sentiment Classification	nn_Classification_Sentiment conj_and_tagging_Classification nn_tagging_PoS vmod_tasks_examined num_tasks_two det_tasks_the prep_for_successful_tasks cop_successful_be aux_successful_to dep_proven_Classification dep_proven_tagging xcomp_proven_successful aux_proven_has nsubj_proven_Learning advmod_proven_Similarly dep_Blitzer_2008 dep_Blitzer_Blitzer num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et dep_Blitzer_Blitzer appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et appos_Learning_Blitzer nn_Learning_Correspondence amod_Learning_Structural
E09-3005	P07-1056	o	So far SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis -LRB- Blitzer et al. 2006 Blitzer et al. 2007 -RRB-	num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et dep_Blitzer_Blitzer appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et nn_Analysis_Sentiment conj_and_tagging_Analysis nn_tagging_Part-of-Speech dep_applied_Blitzer prep_for_applied_Analysis prep_for_applied_tagging prep_in_applied_NLP advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_SCL advmod_applied_far advmod_far_So
E09-3005	P07-1056	o	4 Structural Correspondence Learning SCL -LRB- Structural Correspondence Learning -RRB- -LRB- Blitzer et al. 2006 Blitzer et al. 2007 Blitzer 2008 -RRB- is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains	amod_domains_different prep_from_features_domains prep_between_correspondences_features dobj_learn_correspondences aux_learn_to nn_domain_target vmod_source_learn conj_and_source_domain preconj_source_both amod_data_unlabeled prep_from_uses_domain prep_from_uses_source dobj_uses_data nsubj_uses_which rcmod_technique_uses nn_technique_adaptation nn_technique_domain amod_technique_proposed det_technique_a cop_technique_is nsubj_technique_SCL advmod_proposed_recently dep_Blitzer_2008 dep_Blitzer_Blitzer num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et dep_Blitzer_Blitzer appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et nn_Learning_Correspondence amod_Learning_Structural appos_SCL_Blitzer appos_SCL_Learning nn_SCL_Learning nn_SCL_Correspondence amod_SCL_Structural num_SCL_4
E09-3005	P07-1056	o	So far pivot features on the word level were used -LRB- Blitzer et al. 2006 Blitzer et al. 2007 Blitzer 2008 -RRB- e.g. Does the bigram not buy occur in this document ? -LRB- Blitzer 2008 -RRB-	amod_Blitzer_2008 det_document_this prep_in_buy_document dobj_buy_occur neg_buy_not nsubj_buy_bigram aux_buy_Does det_bigram_the dep_Blitzer_2008 dep_Blitzer_Blitzer dep_Blitzer_buy dep_Blitzer_e.g. dep_Blitzer_Blitzer num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et dep_Blitzer_Blitzer appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et dep_used_Blitzer auxpass_used_were nsubjpass_used_features advmod_used_far nn_level_word det_level_the prep_on_features_level amod_features_pivot advmod_far_So
N09-1055	P07-1056	p	With the in-depth study of opinion mining researchers committed their efforts for more accurate results the research of sentiment summarization -LRB- Philip et al. 2004 Hu et al. KDD 2004 -RRB- domain transfer problem of the sentiment analysis -LRB- Kanayama et al. 2006 Tan et al. 2007 Blitzer et al. 2007 Tan et al. 2008 Andreevskaia et al. 2008 Tan et al. 2009 -RRB- and finegrained opinion mining -LRB- Hatzivassiloglou et al. 2000 Takamura et al. 2007 Bloom et al. 2007 Wang et al. 2008 Titov et al. 2008 -RRB- are the main branches of the research of opinion mining	nn_mining_opinion prep_of_research_mining det_research_the prep_of_branches_research amod_branches_main det_branches_the cop_branches_are nsubj_branches_mining nsubj_branches_problem nsubj_branches_research num_Titov_2008 nn_Titov_al. nn_Titov_et num_Wang_2008 nn_Wang_al. nn_Wang_et num_Bloom_2007 nn_Bloom_al. nn_Bloom_et num_Takamura_2007 nn_Takamura_al. nn_Takamura_et dep_Hatzivassiloglou_Titov conj_Hatzivassiloglou_Wang conj_Hatzivassiloglou_Bloom conj_Hatzivassiloglou_Takamura appos_Hatzivassiloglou_2000 dep_Hatzivassiloglou_al. nn_Hatzivassiloglou_et appos_mining_Hatzivassiloglou nn_mining_opinion amod_mining_finegrained num_Tan_2009 nn_Tan_al. nn_Tan_et num_Andreevskaia_2008 nn_Andreevskaia_al. nn_Andreevskaia_et num_Tan_2008 nn_Tan_al. nn_Tan_et num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et num_Tan_2007 nn_Tan_al. nn_Tan_et dep_Kanayama_Tan dep_Kanayama_Andreevskaia dep_Kanayama_Tan dep_Kanayama_Blitzer dep_Kanayama_Tan appos_Kanayama_2006 dep_Kanayama_al. nn_Kanayama_et nn_analysis_sentiment det_analysis_the prep_of_problem_analysis nn_problem_transfer nn_problem_domain num_KDD_2004 nn_al._et nn_al._Hu appos_Philip_KDD dep_Philip_al. appos_Philip_2004 dep_Philip_al. nn_Philip_et nn_summarization_sentiment conj_and_research_mining dep_research_Kanayama conj_and_research_problem dep_research_Philip prep_of_research_summarization det_research_the amod_results_accurate amod_results_more prep_for_efforts_results poss_efforts_their parataxis_committed_branches dobj_committed_efforts nsubj_committed_researchers prep_with_committed_study nn_mining_opinion prep_of_study_mining amod_study_in-depth det_study_the
N09-1056	P07-1056	o	On a separate note previous research has explicitly studied sentiment analysis as an application of transfer learning -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et nn_learning_transfer prep_of_application_learning det_application_an nn_analysis_sentiment dep_studied_Blitzer prep_as_studied_application dobj_studied_analysis advmod_studied_explicitly aux_studied_has nsubj_studied_research prep_on_studied_note amod_research_previous amod_note_separate det_note_a
P08-2059	P07-1056	o	We selected four binary NLP datasets for evaluation 20 Newsgroups1 and Reuters -LRB- Lewis et al. 2004 -RRB- -LRB- used by Tong and Koller -RRB- and sentiment classification -LRB- Blitzer et al. 2007 -RRB- and spam -LRB- Bickel 2006 -RRB-	amod_Bickel_2006 dep_spam_Bickel amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et nn_classification_sentiment conj_and_Tong_Koller prep_by_used_Koller prep_by_used_Tong amod_Lewis_2004 dep_Lewis_al. nn_Lewis_et conj_and_Newsgroups1_spam dep_Newsgroups1_Blitzer conj_and_Newsgroups1_classification dep_Newsgroups1_used dep_Newsgroups1_Lewis conj_and_Newsgroups1_Reuters num_Newsgroups1_20 dep_datasets_spam dep_datasets_classification dep_datasets_Reuters dep_datasets_Newsgroups1 prep_for_datasets_evaluation nn_datasets_NLP amod_datasets_binary num_datasets_four dobj_selected_datasets nsubj_selected_We
P08-2065	P07-1056	o	As the training data from DVDs is much more similar to books than that from kitchen -LRB- Blitzer et al. 2007 -RRB- we should give the data from DVDs a higher weight	amod_weight_higher det_weight_a dep_DVDs_weight det_data_the prep_from_give_DVDs dobj_give_data aux_give_should nsubj_give_we advcl_give_similar amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et dep_that_Blitzer prep_from_that_kitchen prep_than_similar_that prep_to_similar_books advmod_similar_more cop_similar_is nsubj_similar_data mark_similar_As advmod_more_much prep_from_data_DVDs nn_data_training det_data_the
P09-1001	P07-1056	o	Various machine learning strategies have been proposed to address this problem including semi-supervised learning -LRB- Zhu 2007 -RRB- domain adaptation -LRB- Wu and Dietterich 2004 Blitzer et al. 2006 Blitzer et al. 2007 Arnold et al. 2007 Chan and Ng 2007 Daume 2007 Jiang and Zhai 2007 Reichart and Rappoport 2007 Andreevskaia and Bergler 2008 -RRB- multi-task learning -LRB- Caruana 1997 Reichart et al. 2008 Arnold et al. 2008 -RRB- self-taught learning -LRB- Raina et al. 2007 -RRB- etc. A commonality among these methods is that they all require the training data and test data to be in the same feature space	nn_space_feature amod_space_same det_space_the prep_in_be_space aux_be_to nn_data_test conj_and_data_data nn_data_training det_data_the xcomp_require_be dobj_require_data dobj_require_data nsubj_require_they mark_require_that det_they_all ccomp_is_require nsubj_is_commonality det_methods_these prep_among_commonality_methods det_commonality_A amod_Raina_2007 dep_Raina_al. nn_Raina_et amod_learning_self-taught num_Arnold_2008 nn_Arnold_al. nn_Arnold_et dep_Reichart_Arnold num_Reichart_2008 nn_Reichart_al. nn_Reichart_et dep_Caruana_Reichart appos_Caruana_1997 appos_learning_Caruana amod_learning_multi-task dep_Andreevskaia_2008 conj_and_Andreevskaia_Bergler num_Jiang_2007 conj_and_Jiang_Zhai num_Daume_2007 num_Chan_2007 conj_and_Chan_Ng num_Arnold_2007 nn_Arnold_al. nn_Arnold_et nn_al._et nn_al._Blitzer nn_al._et nn_al._Blitzer dep_Wu_Bergler dep_Wu_Andreevskaia num_Wu_2007 conj_and_Wu_Rappoport conj_and_Wu_Reichart conj_and_Wu_Zhai conj_and_Wu_Jiang conj_and_Wu_Daume conj_and_Wu_Ng conj_and_Wu_Chan conj_and_Wu_Arnold num_Wu_2007 dep_Wu_al. num_Wu_2006 dep_Wu_al. num_Wu_2004 conj_and_Wu_Dietterich appos_adaptation_Rappoport appos_adaptation_Reichart appos_adaptation_Jiang appos_adaptation_Daume appos_adaptation_Chan appos_adaptation_Arnold appos_adaptation_Dietterich appos_adaptation_Wu nn_adaptation_domain dep_Zhu_2007 dep_learning_etc. dep_learning_Raina conj_learning_learning conj_learning_learning conj_learning_adaptation dep_learning_Zhu amod_learning_semi-supervised det_problem_this parataxis_address_is prep_including_address_learning dobj_address_problem aux_address_to xcomp_proposed_address auxpass_proposed_been aux_proposed_have nsubjpass_proposed_strategies nn_strategies_learning nn_strategies_machine amod_strategies_Various
P09-1027	P07-1056	o	Training Set -LRB- Labeled English Reviews -RRB- There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification -LRB- Blitzer et al. 2007 -RRB- 9 because the corpus was large-scale and it was within similar domains as the test set	nn_set_test det_set_the amod_domains_similar prep_as_was_set prep_within_was_domains nsubj_was_it conj_and_large-scale_was cop_large-scale_was nsubj_large-scale_corpus mark_large-scale_because det_corpus_the advcl_Blitzer_was advcl_Blitzer_large-scale num_Blitzer_9 dep_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et nn_classification_sentiment amod_classification_multi-domain prep_for_constructed_classification vmod_corpus_constructed det_corpus_the dobj_used_corpus nsubj_used_we det_Web_the prep_on_available_Web amod_corpora_available amod_corpora_English amod_corpora_labeled amod_corpora_many dep_are_Blitzer conj_and_are_used nsubj_are_corpora expl_are_There amod_Reviews_English dobj_Labeled_Reviews dep_Set_Labeled parataxis_Training_used parataxis_Training_are dep_Training_Set dep_``_Training
P09-1027	P07-1056	o	We will employ the structural correspondence learning -LRB- SCL -RRB- domain adaption algorithm used in -LRB- Blitzer et al. 2007 -RRB- for linking the translated text and the natural text	amod_text_natural det_text_the conj_and_text_text amod_text_translated det_text_the dobj_linking_text dobj_linking_text amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et prepc_for_used_linking prep_in_used_Blitzer vmod_algorithm_used nn_algorithm_adaption nn_algorithm_domain nn_algorithm_SCL nn_algorithm_learning dep_correspondence_algorithm amod_correspondence_structural det_correspondence_the dobj_employ_correspondence aux_employ_will nsubj_employ_We
P09-1028	P07-1056	o	Amazon Reviews The dataset contains product reviews taken from Amazon.com from 4 product types Kitchen Books DVDs and Electronics -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et dep_Kitchen_Blitzer conj_and_Kitchen_Electronics conj_and_Kitchen_DVDs conj_and_Kitchen_Books dep_types_Electronics dep_types_DVDs dep_types_Books dep_types_Kitchen nn_types_product num_types_4 prep_from_taken_types prep_from_taken_Amazon.com vmod_reviews_taken nn_reviews_product dobj_contains_reviews nsubj_contains_dataset det_dataset_The dep_Reviews_contains nn_Reviews_Amazon
P09-1028	P07-1056	o	Finally recent efforts have also looked at transfer learning mechanisms for sentiment analysis e.g. see -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et dep_see_Blitzer advmod_see_e.g. nn_analysis_sentiment dep_mechanisms_see prep_for_mechanisms_analysis nn_mechanisms_learning nn_mechanisms_transfer prep_at_looked_mechanisms advmod_looked_also aux_looked_have nsubj_looked_efforts advmod_looked_Finally amod_efforts_recent
P09-1078	P07-1056	o	And 20NG is a collection of approximately 20,000 20-category documents 1 In sentiment text classification we also use two data sets one is the widely used Cornell movie-review dataset2 -LRB- Pang and Lee 2004 -RRB- and one dataset from product reviews of domain DVD3 -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et nn_DVD3_domain prep_of_reviews_DVD3 nn_reviews_product prep_from_dataset_reviews num_dataset_one dep_Pang_2004 conj_and_Pang_Lee dep_dataset2_Blitzer conj_and_dataset2_dataset dep_dataset2_Lee dep_dataset2_Pang nn_dataset2_movie-review nn_dataset2_Cornell amod_dataset2_used det_dataset2_the cop_dataset2_is nsubj_dataset2_one advmod_used_widely nn_sets_data num_sets_two dobj_use_sets advmod_use_also nsubj_use_we nn_classification_text nn_classification_sentiment num_documents_1 amod_documents_20-category num_documents_20,000 quantmod_20,000_approximately dep_collection_dataset dep_collection_dataset2 parataxis_collection_use prep_in_collection_classification prep_of_collection_documents det_collection_a cop_collection_is nsubj_collection_20NG cc_collection_And
P09-1079	P07-1056	o	4 Evaluation 4.1 Experimental Setup For evaluation we use five sentiment classification datasets including the widely-used movie review dataset -LSB- MOV -RSB- -LRB- Pang et al. 2002 -RRB- as well as four datasets that contain reviews of four different types of product from Amazon -LSB- books -LRB- BOO -RRB- DVDs -LRB- DVD -RRB- electronics -LRB- ELE -RRB- and kitchen appliances -LRB- KIT -RRB- -RSB- -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et appos_appliances_KIT nn_appliances_kitchen appos_electronics_ELE appos_DVDs_DVD conj_and_books_appliances conj_and_books_electronics conj_and_books_DVDs appos_books_BOO nn_books_Amazon prep_from_product_appliances prep_from_product_electronics prep_from_product_DVDs prep_from_product_books prep_of_types_product amod_types_different num_types_four prep_of_reviews_types dobj_contain_reviews nsubj_contain_that rcmod_datasets_contain num_datasets_four amod_Pang_2002 dep_Pang_al. nn_Pang_et conj_and_dataset_datasets dep_dataset_Pang appos_dataset_MOV nn_dataset_review nn_dataset_movie amod_dataset_widely-used det_dataset_the dep_datasets_Blitzer prep_including_datasets_datasets prep_including_datasets_dataset nn_datasets_classification nn_datasets_sentiment num_datasets_five dobj_use_datasets nsubj_use_we nsubj_use_Setup prep_for_Setup_evaluation amod_Setup_Experimental num_Setup_4.1 nn_Setup_Evaluation num_Setup_4
P09-2080	P07-1056	o	The second one needs no labeled data for the new domain -LRB- Blitzer et al. 2007 Tan et al. 2007 Andreevskaia and Bergler 2008 Tan et al. 2008 Tan et al. 2009 -RRB-	num_Tan_2009 nn_Tan_al. nn_Tan_et num_Tan_2008 nn_Tan_al. nn_Tan_et dep_Andreevskaia_Tan conj_and_Andreevskaia_Tan conj_and_Andreevskaia_2008 conj_and_Andreevskaia_Bergler num_Tan_2007 nn_Tan_al. nn_Tan_et dep_Blitzer_Tan dep_Blitzer_2008 dep_Blitzer_Bergler dep_Blitzer_Andreevskaia conj_Blitzer_Tan appos_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et amod_domain_new det_domain_the prep_for_data_domain amod_data_labeled neg_data_no dep_needs_Blitzer dobj_needs_data nsubj_needs_one amod_one_second det_one_The ccomp_``_needs
P09-2080	P07-1056	o	We also compare our algorithm to Structural Correspondence Learning -LRB- SCL -RRB- -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et appos_Learning_SCL nn_Learning_Correspondence amod_Learning_Structural poss_algorithm_our dep_compare_Blitzer prep_to_compare_Learning dobj_compare_algorithm advmod_compare_also nsubj_compare_We ccomp_``_compare
P09-2080	P07-1056	o	Seen from Table 2 our result about SCL is in accord with that in -LRB- Blitzer et al. 2007 -RRB- on the whole	det_whole_the prep_on_Blitzer_whole amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et prep_with_accord_that prep_in_is_Blitzer prep_in_is_accord nsubj_is_result vmod_is_Seen prep_about_result_SCL poss_result_our num_Table_2 prep_from_Seen_Table
W08-0804	P07-1056	p	3 Experiments We evaluated the effect of random feature mixing on four popular learning methods Perceptron MIRA -LRB- Crammer et al. 2006 -RRB- SVM and Maximum entropy with 4 NLP datasets 20 Newsgroups1 Reuters -LRB- Lewis et al. 2004 -RRB- Sentiment -LRB- Blitzer et al. 2007 -RRB- and Spam -LRB- Bickel 2006 -RRB-	amod_Bickel_2006 dep_Spam_Bickel amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et dep_Sentiment_Blitzer amod_Lewis_2004 dep_Lewis_al. nn_Lewis_et dep_Reuters_Lewis conj_and_Newsgroups1_Spam conj_and_Newsgroups1_Sentiment appos_Newsgroups1_Reuters num_Newsgroups1_20 nn_datasets_NLP num_datasets_4 dep_with_Spam dep_with_Sentiment dep_with_Newsgroups1 pobj_with_datasets nn_entropy_Maximum amod_Crammer_2006 dep_Crammer_al. nn_Crammer_et dep_MIRA_Crammer conj_and_Perceptron_entropy conj_and_Perceptron_SVM conj_and_Perceptron_MIRA dep_methods_entropy dep_methods_SVM dep_methods_MIRA dep_methods_Perceptron amod_methods_learning amod_methods_popular num_methods_four prep_on_mixing_methods vmod_feature_mixing amod_feature_random prep_of_effect_feature det_effect_the dobj_evaluated_effect nsubj_evaluated_We dep_Experiments_with rcmod_Experiments_evaluated num_Experiments_3
W09-2205	P07-1056	o	5 Conclusions and Future Work The paper compares Structural Correspondence Learning -LRB- Blitzer et al. 2006 -RRB- with -LRB- various instances of -RRB- self-training -LRB- Abney 2007 McClosky et al. 2006 -RRB- for the adaptation of a parse selection model to Wikipedia domains	nn_domains_Wikipedia prep_to_model_domains nn_model_selection nn_model_parse det_model_a prep_of_adaptation_model det_adaptation_the num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Abney_McClosky appos_Abney_2007 prep_for_self-training_adaptation appos_self-training_Abney dep_instances_self-training dep_instances_of amod_instances_various amod_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et prep_with_Learning_instances dep_Learning_Blitzer nn_Learning_Correspondence amod_Learning_Structural dobj_compares_Learning nsubj_compares_paper dep_compares_Work dep_compares_Conclusions det_paper_The amod_Work_Future conj_and_Conclusions_Work num_Conclusions_5
W09-2205	P07-1056	o	We examine Structural Correspondence Learning -LRB- SCL -RRB- -LRB- Blitzer et al. 2006 -RRB- for this task and compare it to several variants of Self-training -LRB- Abney 2007 McClosky et al. 2006 -RRB-	num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Abney_McClosky appos_Abney_2007 appos_Self-training_Abney prep_of_variants_Self-training amod_variants_several prep_to_compare_variants dobj_compare_it nsubj_compare_We det_task_this amod_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et dep_Learning_Blitzer appos_Learning_SCL nn_Learning_Correspondence amod_Learning_Structural conj_and_examine_compare prep_for_examine_task dobj_examine_Learning nsubj_examine_We
W09-2205	P07-1056	p	2 Previous Work So far Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis -LRB- Blitzer et al. 2006 Blitzer et al. 2007 -RRB-	num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et dep_Blitzer_Blitzer appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et nn_Analysis_Sentiment conj_and_tagging_Analysis nn_tagging_PoS dep_applied_Blitzer prep_to_applied_Analysis prep_to_applied_tagging advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_Learning advmod_applied_Work nn_Learning_Correspondence amod_Learning_Structural advmod_far_So advmod_Work_far amod_Work_Previous num_Work_2 ccomp_``_applied
W09-2205	P07-1056	o	The techniques examined are Structural Correspondence Learning -LRB- SCL -RRB- -LRB- Blitzer et al. 2006 -RRB- and Self-training -LRB- Abney 2007 McClosky et al. 2006 -RRB-	num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Abney_McClosky appos_Abney_2007 dep_Self-training_Abney amod_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et conj_and_Learning_Self-training dep_Learning_Blitzer appos_Learning_SCL nn_Learning_Correspondence amod_Learning_Structural cop_Learning_are nsubj_Learning_techniques vmod_techniques_examined det_techniques_The
W09-2205	P07-1056	o	SCL for Discriminative Parse Selection So far pivot features on the word level were used -LRB- Blitzer et al. 2006 Blitzer et al. 2007 -RRB-	num_Blitzer_2007 nn_Blitzer_al. nn_Blitzer_et dep_Blitzer_Blitzer appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et dep_used_Blitzer auxpass_used_were nsubjpass_used_features advmod_used_far nsubjpass_used_SCL nn_level_word det_level_the prep_on_features_level amod_features_pivot advmod_far_So nn_Selection_Parse amod_Selection_Discriminative prep_for_SCL_Selection
W09-2211	P07-1056	o	Labeled data for one domain might be used to train a initial classifier for another -LRB- possibly related -RRB- domain and then bootstrapping can be employed to learn new knowledge from the new domain -LRB- Blitzer et al. 2007 -RRB-	amod_Blitzer_2007 dep_Blitzer_al. nn_Blitzer_et amod_domain_new det_domain_the amod_knowledge_new prep_from_learn_domain dobj_learn_knowledge aux_learn_to xcomp_employed_learn auxpass_employed_be aux_employed_can csubjpass_employed_bootstrapping advmod_bootstrapping_then dep_domain_related det_domain_another advmod_related_possibly amod_classifier_initial det_classifier_a prep_for_train_domain dobj_train_classifier aux_train_to dep_used_Blitzer conj_and_used_employed xcomp_used_train auxpass_used_be aux_used_might nsubjpass_used_data num_domain_one prep_for_data_domain amod_data_Labeled
D07-1049	P07-1065	o	5.3 Analysis of BF-LM framework We refer to -LRB- Talbot and Osborne 2007 -RRB- for empirical results establishing the performance of the logfrequency BF-LM overestimation errors occur with 474 0.01 0.025 0.05 0.1 0.25 0.5 0.03 0.02 0.01 0.005 0.0025 0.001 Mean squared error of log probabilites Memory in GB MSE between WB 3-gram SRILM and BF-LMs Base 3 Base 1.5 Base 1.1 Figure 5 MSE between SRILM and BF-LMs 22 23 24 25 26 27 28 29 30 0.01 0.1 1 BLEU Score Mean squared error WB-smoothed BF-LM 3-gram model BF-LM base 1.1 BF-LM base 1.5 BF-LM base 3 Figure 6 MSE vs. BLEU for WB 3-gram BF-LMs a probability that decays exponentially in the size of the overestimation error	nn_error_overestimation det_error_the prep_of_size_error det_size_the prep_in_decays_size advmod_decays_exponentially nsubj_decays_that rcmod_probability_decays det_probability_a amod_BF-LMs_3-gram nn_BF-LMs_WB dep_BLEU_probability prep_for_BLEU_BF-LMs nn_BLEU_vs. nn_BLEU_MSE num_Figure_6 num_Figure_3 nn_Figure_base nn_Figure_BF-LM num_Figure_1.5 nn_Figure_base nn_Figure_BF-LM num_Figure_1.1 nn_Figure_base nn_Figure_BF-LM nn_Figure_model nn_Figure_3-gram nn_Figure_BF-LM amod_Figure_WB-smoothed nn_Figure_error dobj_squared_Figure nsubj_squared_MSE nn_Mean_Score nn_Mean_BLEU num_Mean_1 dep_Mean_0.1 dep_0.1_0.01 number_0.01_30 dep_29_Mean number_29_28 dep_29_27 dep_29_23 number_27_26 dep_27_25 number_25_24 number_23_22 amod_BF-LMs_29 conj_and_SRILM_BF-LMs prep_between_MSE_BF-LMs prep_between_MSE_SRILM num_Figure_5 num_Figure_1.1 dep_Base_Figure num_Base_1.5 dep_Base_squared dep_Base_Base dep_3_BLEU dep_3_Base dep_Base_3 dep_BF-LMs_Base conj_and_SRILM_BF-LMs amod_SRILM_3-gram nn_SRILM_WB prep_between_MSE_BF-LMs prep_between_MSE_SRILM nn_MSE_GB nn_Memory_probabilites nn_Memory_log prep_of_error_Memory amod_error_squared prep_in_Mean_MSE dep_Mean_error dep_Mean_0.001 num_Mean_0.01 number_0.001_0.0025 number_0.001_0.005 dep_0.001_0.01 number_0.01_0.02 dep_0.01_0.03 dep_0.01_0.05 number_0.03_0.5 dep_0.03_0.25 number_0.25_0.1 number_0.05_0.025 number_0.01_474 prep_with_occur_Mean nsubj_occur_errors nn_errors_overestimation nn_BF-LM_logfrequency det_BF-LM_the prep_of_performance_BF-LM det_performance_the dobj_establishing_performance vmod_results_establishing amod_results_empirical num_Talbot_2007 conj_and_Talbot_Osborne prep_for_refer_results prep_to_refer_Osborne prep_to_refer_Talbot nsubj_refer_We nn_framework_BF-LM dep_Analysis_occur rcmod_Analysis_refer prep_of_Analysis_framework num_Analysis_5.3
D07-1049	P07-1065	o	Wehope the present work will together with Talbot and Osborne -LRB- 2007 -RRB- establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics	amod_linguistics_computational prep_in_used_linguistics vmod_structures_used nn_structures_data amod_structures_associative amod_structures_conventional prep_to_alternative_structures amod_alternative_practical det_alternative_a nn_filter_Bloom det_filter_the prep_as_establish_alternative dobj_establish_filter prep_together_with_establish_Osborne prep_together_with_establish_Talbot aux_establish_will nsubj_establish_work appos_Osborne_2007 conj_and_Talbot_Osborne amod_work_present det_work_the ccomp_Wehope_establish ccomp_``_Wehope
D07-1049	P07-1065	o	In this paper we build on recent work -LRB- Talbot and Osborne 2007 -RRB- that demonstrated how the Bloom filter -LRB- Bloom -LRB- 1970 -RRB- BF -RRB- a space-efficient randomised data structure for representing sets could be used to store corpus statistics efficiently	nn_statistics_corpus advmod_store_efficiently dobj_store_statistics aux_store_to xcomp_used_store auxpass_used_be aux_used_could nsubjpass_used_filter advmod_used_how dobj_representing_sets prepc_for_structure_representing nn_structure_data amod_structure_randomised amod_structure_space-efficient det_structure_a dep_Bloom_BF appos_Bloom_1970 appos_filter_structure appos_filter_Bloom nn_filter_Bloom det_filter_the ccomp_demonstrated_used nsubj_demonstrated_that appos_Talbot_2007 conj_and_Talbot_Osborne dep_work_Osborne dep_work_Talbot amod_work_recent dep_build_demonstrated prep_on_build_work nsubj_build_we prep_in_build_paper det_paper_this
D07-1049	P07-1065	o	Our framework makes use of the log-frequency Bloom filter presented in -LRB- Talbot and Osborne 2007 -RRB- and described briefly below to compute smoothed conditional n-gram probabilities on the fly	det_fly_the prep_on_probabilities_fly nn_probabilities_n-gram amod_probabilities_conditional amod_probabilities_smoothed dobj_compute_probabilities aux_compute_to advmod_below_briefly vmod_described_compute advmod_described_below conj_and_Talbot_described num_Talbot_2007 conj_and_Talbot_Osborne prep_in_presented_described prep_in_presented_Osborne prep_in_presented_Talbot vmod_filter_presented nn_filter_Bloom nn_filter_log-frequency det_filter_the prep_of_use_filter dobj_makes_use nsubj_makes_framework poss_framework_Our
D07-1049	P07-1065	p	3 Language modelling with Bloom filters Recentwork -LRB- TalbotandOsborne 2007 -RRB- presenteda scheme for associating static frequency information with a set of n-grams in a BF efficiently .1 3.1 Log-frequency Bloom filter The efficiency of the scheme for storing n-gram statistics within a BF presented in Talbot and Osborne -LRB- 2007 -RRB- relies on the Zipf-like distribution of n-gramfrequencies mosteventsoccuranextremely small number of times while a small number are very frequent	advmod_frequent_very cop_frequent_are nsubj_frequent_number mark_frequent_while amod_number_small det_number_a advcl_number_frequent prep_of_number_times amod_number_small advmod_number_mosteventsoccuranextremely prep_of_distribution_n-gramfrequencies amod_distribution_Zipf-like det_distribution_the prep_on_relies_distribution nsubj_relies_efficiency appos_Osborne_2007 conj_and_Talbot_Osborne prep_in_presented_Osborne prep_in_presented_Talbot vmod_BF_presented det_BF_a amod_statistics_n-gram prep_within_storing_BF dobj_storing_statistics det_scheme_the prepc_for_efficiency_storing prep_of_efficiency_scheme det_efficiency_The dep_filter_number rcmod_filter_relies nn_filter_Bloom nn_filter_Log-frequency num_filter_3.1 number_3.1_.1 dep_efficiently_filter advmod_BF_efficiently det_BF_a prep_in_set_BF prep_of_set_n-grams det_set_a prep_with_information_set nn_information_frequency amod_information_static dobj_associating_information prepc_for_scheme_associating nn_scheme_presenteda nn_scheme_Recentwork dep_TalbotandOsborne_2007 dep_Recentwork_TalbotandOsborne dep_filters_scheme nn_filters_Bloom prep_with_modelling_filters vmod_Language_modelling num_Language_3 dep_``_Language
D07-1049	P07-1065	o	As noted in Talbot and Osborne -LRB- 2007 -RRB- errors for this log-frequency BF scheme are one-sided frequencies will never be underestimated	auxpass_underestimated_be neg_underestimated_never aux_underestimated_will nsubjpass_underestimated_frequencies parataxis_one-sided_underestimated cop_one-sided_are nsubj_one-sided_errors advcl_one-sided_noted nn_scheme_BF nn_scheme_log-frequency det_scheme_this prep_for_errors_scheme appos_Osborne_2007 conj_and_Talbot_Osborne prep_in_noted_Osborne prep_in_noted_Talbot mark_noted_As
D07-1049	P07-1065	o	3.2.1 Proxy items There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne -LRB- 2007 -RRB-	appos_Osborne_2007 conj_and_Talbot_Osborne prep_in_presented_Osborne prep_in_presented_Talbot vmod_scheme_presented nn_scheme_BF nn_scheme_log-frequency det_scheme_the dobj_using_scheme vmod_statistics_using amod_statistics_related dobj_represent_statistics nsubj_represent_we mark_represent_if prep_of_risk_redundancy amod_risk_potential det_risk_a advcl_is_represent nsubj_is_risk expl_is_There rcmod_items_is nn_items_Proxy num_items_3.2.1
I08-2089	P07-1065	o	Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs -LRB- Talbot and Osborne 2007a Talbot and Osborne 2007b -RRB-	appos_Talbot_2007b conj_and_Talbot_Osborne dep_Talbot_Osborne dep_Talbot_Talbot conj_and_Talbot_2007a conj_and_Talbot_Osborne dep_LMs_2007a dep_LMs_Osborne dep_LMs_Talbot prep_for_effective_LMs cop_effective_be aux_effective_to xcomp_demonstrated_effective auxpass_demonstrated_been aux_demonstrated_has nsubjpass_demonstrated_use nn_filters_Bloom prep_on_based_filters nn_structures_data amod_structures_lossy vmod_use_based prep_of_use_structures det_use_the advmod_use_Also
N09-1058	P07-1065	p	There also have been prior work on maintaining approximate counts for higher-order language models -LRB- LMs -RRB- -LRB- -LRB- Talbot and Osborne 2007a Talbot and Osborne 2007b Talbot and Brants 2008 -RRB- -RRB- operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately	nn_queries_count advmod_answer_approximately dobj_answer_queries aux_answer_to amod_representation_compressed det_representation_this vmod_use_answer dobj_use_representation prep_of_table_counts amod_table_disk-resident det_table_a prep_of_representation_table amod_representation_compressed det_representation_a conj_and_store_use dobj_store_representation aux_store_to xcomp_is_use xcomp_is_store nsubj_is_goal mark_is_that det_goal_the ccomp_model_is det_model_the prep_under_operates_model nsubj_operates_2007a nsubj_operates_Osborne nsubj_operates_Talbot dep_Talbot_2008 conj_and_Talbot_Brants conj_and_Talbot_Talbot conj_and_Talbot_2007b conj_and_Talbot_Osborne dep_Talbot_Brants dep_Talbot_Talbot dep_Talbot_2007b dep_Talbot_Osborne dep_Talbot_Talbot conj_and_Talbot_2007a conj_and_Talbot_Osborne appos_models_LMs nn_models_language amod_models_higher-order amod_counts_approximate prep_for_maintaining_models dobj_maintaining_counts dep_work_operates prepc_on_work_maintaining amod_work_prior cop_work_been aux_work_have advmod_work_also expl_work_There
N09-1058	P07-1065	p	Since the use of cluster of machines is not always practical -LRB- Talbot and Osborne 2007b Talbot and Osborne 2007a -RRB- showed a randomized data structure called Bloom filter that can be used to construct space efficient language models 513 for SMT	prep_for_513_SMT dep_models_513 nn_models_language amod_models_efficient nn_models_space dobj_construct_models aux_construct_to xcomp_used_construct auxpass_used_be aux_used_can nsubjpass_used_that nn_filter_Bloom dep_called_filter rcmod_structure_used vmod_structure_called nn_structure_data amod_structure_randomized det_structure_a dobj_showed_structure nsubj_showed_2007b nsubj_showed_Osborne nsubj_showed_Talbot advcl_showed_practical appos_Talbot_2007a conj_and_Talbot_Osborne dep_Talbot_Osborne dep_Talbot_Talbot conj_and_Talbot_2007b conj_and_Talbot_Osborne advmod_practical_always neg_practical_not cop_practical_is nsubj_practical_use mark_practical_Since prep_of_cluster_machines prep_of_use_cluster det_use_the
N09-1058	P07-1065	p	3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a no-false-negative guarantee ensuring that counts for n-grams in the model are returned exactly while working to make sure the false-positive rate remains small -LRB- Talbot and Osborne 2007a -RRB-	appos_Talbot_2007a conj_and_Talbot_Osborne dep_small_Osborne dep_small_Talbot acomp_remains_small nsubj_remains_rate dep_remains_sure amod_rate_false-positive det_rate_the ccomp_make_remains aux_make_to xcomp_working_make mark_working_while advcl_returned_working advmod_returned_exactly auxpass_returned_are nsubjpass_returned_counts mark_returned_that det_model_the prep_in_n-grams_model prep_for_counts_n-grams ccomp_ensuring_returned amod_guarantee_no-false-negative det_guarantee_a vmod_provide_ensuring dobj_provide_guarantee nsubj_provide_work nn_models_language prep_for_estimation_models nn_estimation_frequency amod_estimation_approximate prep_on_work_estimation amod_work_Prior nn_work_Estimation nn_Estimation_Frequency nn_Estimation_Approximate amod_Estimation_Space-Efficient num_Estimation_3
P08-1058	P07-1065	p	Following -LRB- Talbot and Osborne 2007a -RRB- we can avoid unnecessary false positives by not querying for the longer n-gram in such cases	amod_cases_such prep_in_n-gram_cases amod_n-gram_longer det_n-gram_the prep_for_querying_n-gram neg_querying_not prepc_by_positives_querying amod_positives_false amod_positives_unnecessary dobj_avoid_positives aux_avoid_can nsubj_avoid_we prep_avoid_Following appos_Talbot_2007a conj_and_Talbot_Osborne dep_Following_Osborne dep_Following_Talbot ccomp_``_avoid
P08-1058	P07-1065	p	Recent work -LRB- Talbot and Osborne 2007b -RRB- has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability	amod_probability_small det_probability_some prep_with_errors_probability dobj_allowing_errors nn_structures_data amod_structures_lossless prep_on_constraints_structures amod_constraints_information-theoretic prepc_by_circumventing_allowing dobj_circumventing_constraints amod_space-savings_signficant prep_with_LMs_space-savings nn_counts_n-gram prep_for_represent_LMs dobj_represent_counts aux_represent_to xcomp_used_circumventing xcomp_used_represent auxpass_used_be aux_used_can nsubjpass_used_encodings mark_used_that amod_encodings_randomized ccomp_demonstrated_used aux_demonstrated_has nsubj_demonstrated_work appos_Talbot_2007b conj_and_Talbot_Osborne dep_work_Osborne dep_work_Talbot amod_work_Recent
P08-1058	P07-1065	o	However if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams then it is possible to store each parameter in constant space independent of both n and the vocabulary size -LRB- Carter et al. 1978 -RRB- -LRB- Talbot and Osborne 2007a -RRB-	appos_Talbot_2007a conj_and_Talbot_Osborne amod_Carter_1978 dep_Carter_al. nn_Carter_et nn_size_vocabulary det_size_the conj_and_n_size preconj_n_both prep_of_independent_size prep_of_independent_n amod_space_independent amod_space_constant det_parameter_each prep_in_store_space dobj_store_parameter aux_store_to dep_possible_Osborne dep_possible_Talbot dep_possible_Carter xcomp_possible_store cop_possible_is nsubj_possible_it advcl_possible_willing advmod_possible_However advmod_it_then amod_n-grams_distinct prep_between_distinguish_n-grams aux_distinguish_to xcomp_unable_distinguish cop_unable_be aux_unable_will nsubj_unable_model advmod_unable_occasionally mark_unable_that poss_model_our ccomp_accept_unable aux_accept_to xcomp_willing_accept cop_willing_are nsubj_willing_we mark_willing_if
P08-1058	P07-1065	o	2.3 Previous Randomized LMs Recent work -LRB- Talbot and Osborne 2007b -RRB- has used lossy encodings based on Bloom filters -LRB- Bloom 1970 -RRB- to represent logarithmically quantized corpus statistics for language modeling	nn_modeling_language prep_for_statistics_modeling nn_statistics_corpus amod_statistics_quantized advmod_statistics_logarithmically dobj_represent_statistics aux_represent_to dep_Bloom_1970 appos_filters_Bloom nn_filters_Bloom xcomp_based_represent prep_on_based_filters vmod_encodings_based amod_encodings_lossy dobj_used_encodings aux_used_has nsubj_used_work appos_Talbot_2007b conj_and_Talbot_Osborne dep_work_Osborne dep_work_Talbot amod_work_Recent nn_work_LMs nn_work_Randomized amod_work_Previous num_work_2.3 ccomp_``_used
P08-1058	P07-1065	o	Note that unlike the constructions in -LRB- Talbot and Osborne 2007b -RRB- and -LRB- Church et al. 2007 -RRB- no errors are possible for ngrams stored in the model	det_model_the prep_in_stored_model vmod_ngrams_stored prep_for_possible_ngrams cop_possible_are nsubj_possible_that neg_errors_no amod_Church_2007 dep_Church_al. nn_Church_et dep_Talbot_errors conj_and_Talbot_Church appos_Talbot_2007b conj_and_Talbot_Osborne prep_in_constructions_Church prep_in_constructions_Osborne prep_in_constructions_Talbot det_constructions_the prep_unlike_that_constructions ccomp_Note_possible
W09-0420	P07-1065	p	RANDLM -LRB- Talbot and Osborne 2007 -RRB- performs well and scaled to the full data with improvement -LRB- resulting in our best overall system -RRB-	amod_system_overall amod_system_best poss_system_our prep_in_resulting_system dep_improvement_resulting prep_with_data_improvement amod_data_full det_data_the prep_to_scaled_data nsubj_scaled_RANDLM conj_and_performs_scaled advmod_performs_well nsubj_performs_RANDLM num_Talbot_2007 conj_and_Talbot_Osborne appos_RANDLM_Osborne appos_RANDLM_Talbot
W09-0424	P07-1065	o	We have also implemented a Bloom Filter LM in Joshua following Talbot and Osborne -LRB- 2007 -RRB-	appos_Osborne_2007 conj_and_Talbot_Osborne prep_in_LM_Joshua nn_LM_Filter nn_LM_Bloom det_LM_a prep_following_implemented_Osborne prep_following_implemented_Talbot dobj_implemented_LM advmod_implemented_also aux_implemented_have nsubj_implemented_We ccomp_``_implemented
C08-1071	P07-1080	o	1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure -LRB- Bod 2003 Charniak and Johnson 2005 Collins and Koo 2005 Petrov et al. 2006 Titov and Henderson 2007 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et appos_Charniak_2007 conj_and_Charniak_Henderson conj_and_Charniak_Titov conj_and_Charniak_Petrov conj_and_Charniak_2005 conj_and_Charniak_Koo conj_and_Charniak_Collins conj_and_Charniak_2005 conj_and_Charniak_Johnson dep_Bod_Henderson dep_Bod_Titov dep_Bod_Petrov dep_Bod_2005 dep_Bod_Koo dep_Bod_Collins dep_Bod_2005 dep_Bod_Johnson dep_Bod_Charniak appos_Bod_2003 dep_structure_Bod poss_structure_their amod_sentences_new prep_with_annotating_structure dobj_annotating_sentences prepc_of_purpose_annotating det_purpose_the prep_for_examples_purpose prep_of_set_examples amod_set_labeled det_set_a amod_structure_syntactic prep_of_patterns_structure prep_from_capture_set dobj_capture_patterns aux_capture_to xcomp_attempt_capture nsubj_attempt_parsers amod_parsers_statistical amod_parsers_Supervised rcmod_Introduction_attempt num_Introduction_1
D07-1099	P07-1080	p	We use a recently proposed dependency parser -LRB- Titov and Henderson 2007b -RRB- 1 which has demonstrated state-of-theart performance on a selection of languages from the 1The ISBN parser will be soon made downloadable from the authors web-page	dep_authors_web-page det_authors_the prep_from_made_authors dobj_made_downloadable advmod_made_soon auxpass_made_be aux_made_will nsubjpass_made_Henderson nsubjpass_made_Titov nn_parser_ISBN nn_parser_1The det_parser_the prep_from_selection_parser prep_of_selection_languages det_selection_a prep_on_performance_selection amod_performance_state-of-theart dobj_demonstrated_performance aux_demonstrated_has nsubj_demonstrated_which rcmod_Titov_demonstrated num_Titov_1 dep_Titov_2007b conj_and_Titov_Henderson rcmod_parser_made nn_parser_dependency amod_parser_proposed advmod_parser_recently det_parser_a dobj_use_parser nsubj_use_We
D07-1099	P07-1080	p	When conditioning on words we treated each word feature individually as this proved to be useful in -LRB- Titov and Henderson 2007b -RRB-	appos_Titov_2007b conj_and_Titov_Henderson prep_in_useful_Henderson prep_in_useful_Titov cop_useful_be aux_useful_to xcomp_proved_useful nsubj_proved_this mark_proved_as advmod_feature_individually nn_feature_word det_feature_each advcl_treated_proved dobj_treated_feature nsubj_treated_we nsubj_treated_conditioning advmod_treated_When prep_on_conditioning_words advcl_``_treated
D07-1099	P07-1080	o	ISBNs originally proposed for constituent parsing in -LRB- Titov and Henderson 2007a -RRB- use vectors of binary latent variables to encode information about the parse history	amod_history_parse det_history_the prep_about_information_history dobj_encode_information aux_encode_to amod_variables_latent amod_variables_binary vmod_vectors_encode prep_of_vectors_variables nn_vectors_use appos_Titov_2007a conj_and_Titov_Henderson dep_in_Henderson dep_in_Titov prep_parsing_in nn_parsing_constituent dobj_proposed_vectors prep_for_proposed_parsing advmod_proposed_originally nsubj_proposed_ISBNs
D07-1099	P07-1080	o	In fact in -LRB- Titov and Henderson 2007a -RRB- it was shown that this neural network can be viewed as a coarse approximation to the corresponding ISBN model	nn_model_ISBN amod_model_corresponding det_model_the prep_to_approximation_model amod_approximation_coarse det_approximation_a prep_as_viewed_approximation auxpass_viewed_be aux_viewed_can nsubjpass_viewed_network mark_viewed_that amod_network_neural det_network_this ccomp_shown_viewed auxpass_shown_was nsubjpass_shown_it prep_in_shown_Henderson prep_in_shown_Titov prep_in_shown_fact appos_Titov_2007a conj_and_Titov_Henderson
D07-1099	P07-1080	o	In our experiments we use the same definition of structural locality as was proposed for the ISBN dependency parser in -LRB- Titov and Henderson 2007b -RRB-	appos_Titov_2007b conj_and_Titov_Henderson prep_in_parser_Henderson prep_in_parser_Titov nn_parser_dependency nn_parser_ISBN det_parser_the prep_for_proposed_parser auxpass_proposed_was mark_proposed_as amod_locality_structural prep_of_definition_locality amod_definition_same det_definition_the advcl_use_proposed dobj_use_definition nsubj_use_we prep_in_use_experiments poss_experiments_our
D07-1099	P07-1080	p	3 Parsing Exact inference in ISBN models is not tractable but effective approximations were proposed in -LRB- Titov and Henderson 2007a -RRB-	appos_Titov_2007a conj_and_Titov_Henderson prep_in_proposed_Henderson prep_in_proposed_Titov auxpass_proposed_were nsubjpass_proposed_approximations amod_approximations_effective conj_but_tractable_proposed neg_tractable_not cop_tractable_is nsubj_tractable_inference nn_models_ISBN prep_in_inference_models amod_inference_Exact nn_inference_Parsing num_inference_3 ccomp_``_proposed ccomp_``_tractable
D07-1099	P07-1080	o	Unlike -LRB- Titov and Henderson 2007b -RRB- in the shared task we used only the simplest feed-forward approximation which replicates the computation of a neural network of the type proposed in -LRB- Henderson 2003 -RRB-	amod_Henderson_2003 dep_in_Henderson prep_proposed_in vmod_type_proposed det_type_the prep_of_network_type amod_network_neural det_network_a prep_of_computation_network det_computation_the dobj_replicates_computation nsubj_replicates_which rcmod_approximation_replicates amod_approximation_feed-forward amod_approximation_simplest det_approximation_the advmod_approximation_only dobj_used_approximation nsubj_used_we prep_in_used_task prep_used_Unlike amod_task_shared det_task_the appos_Titov_2007b conj_and_Titov_Henderson dep_Unlike_Henderson dep_Unlike_Titov
D07-1099	P07-1080	o	We would expect better performance with the more accurate approximation based on variational inference proposed and evaluated in -LRB- Titov and Henderson 2007a -RRB-	appos_Titov_2007a conj_and_Titov_Henderson prep_in_proposed_Henderson prep_in_proposed_Titov conj_and_proposed_evaluated vmod_inference_evaluated vmod_inference_proposed amod_inference_variational prep_on_based_inference vmod_approximation_based amod_approximation_accurate det_approximation_the advmod_accurate_more amod_performance_better prep_with_expect_approximation dobj_expect_performance aux_expect_would nsubj_expect_We
D07-1099	P07-1080	o	To search for the most probable parse we use the heuristic search algorithm described in -LRB- Titov and Henderson 2007b -RRB- which is a form of beam search	nn_search_beam prep_of_form_search det_form_a cop_form_is nsubj_form_which rcmod_Titov_form appos_Titov_2007b conj_and_Titov_Henderson prep_in_described_Henderson prep_in_described_Titov vmod_algorithm_described nn_algorithm_search nn_algorithm_heuristic det_algorithm_the dobj_use_algorithm nsubj_use_we advcl_use_search amod_parse_probable det_parse_the advmod_probable_most prep_for_search_parse aux_search_To
D07-1099	P07-1080	o	As was demonstrated in -LRB- Titov and Henderson 2007b -RRB- even a minimal set of local explicit features achieves results which are non-significantly different from a carefully chosen set of explicit features given the language independent definition of locality described in section 2	num_section_2 prep_in_described_section vmod_locality_described prep_of_definition_locality amod_definition_independent nn_definition_language det_definition_the pobj_given_definition amod_features_explicit prep_of_set_features amod_set_chosen det_set_a advmod_chosen_carefully prep_from_different_set advmod_different_non-significantly cop_different_are nsubj_different_which rcmod_results_different prep_achieves_given dobj_achieves_results nsubj_achieves_set advcl_achieves_demonstrated amod_features_explicit amod_features_local prep_of_set_features amod_set_minimal det_set_a advmod_set_even appos_Titov_2007b conj_and_Titov_Henderson dep_in_Henderson dep_in_Titov prep_demonstrated_in auxpass_demonstrated_was mark_demonstrated_As
D07-1099	P07-1080	o	This curve plots the average labeled attachment score over Basque Chinese English and Turkish as a function of parsing time per token .4 Accuracy of only 1 % below the maximum can be achieved with average processing time of 17 ms per token or 60 tokens per second .5 We also refer the reader to -LRB- Titov and Henderson 2007b -RRB- for more detailed analysis of the ISBN dependency parser results where among other things it was shown that the ISBN model is especially accurate at modeling long dependencies	amod_dependencies_long nn_dependencies_modeling prep_at_accurate_dependencies advmod_accurate_especially cop_accurate_is nsubj_accurate_model mark_accurate_that nn_model_ISBN det_model_the ccomp_shown_accurate auxpass_shown_was nsubjpass_shown_it prep_among_shown_things advmod_shown_where amod_things_other advcl_results_shown nsubj_results_plots nn_parser_dependency nn_parser_ISBN det_parser_the prep_of_analysis_parser amod_analysis_detailed amod_analysis_more appos_Titov_2007b conj_and_Titov_Henderson det_reader_the prep_for_refer_analysis prep_to_refer_Henderson prep_to_refer_Titov dobj_refer_reader advmod_refer_also nsubj_refer_We amod_.5_second prep_per_tokens_.5 num_tokens_60 conj_or_token_tokens prep_per_ms_tokens prep_per_ms_token num_ms_17 prep_of_time_ms nn_time_processing amod_time_average prep_with_achieved_time auxpass_achieved_be aux_achieved_can nsubjpass_achieved_score det_maximum_the prep_below_%_maximum num_%_1 quantmod_1_only prep_of_Accuracy_% nn_Accuracy_.4 amod_Accuracy_token prep_per_time_Accuracy nn_time_parsing prep_of_function_time det_function_a conj_and_Basque_Turkish conj_and_Basque_English conj_and_Basque_Chinese prep_as_score_function prep_over_score_Turkish prep_over_score_English prep_over_score_Chinese prep_over_score_Basque nn_score_attachment amod_score_labeled amod_score_average det_score_the rcmod_plots_refer rcmod_plots_achieved nn_plots_curve det_plots_This
N09-2032	P07-1080	o	5.1 The statistical parser The parsing model is the one proposed in Merlo and Musillo -LRB- 2008 -RRB- which extends the syntactic parser of Henderson -LRB- 2003 -RRB- and Titov and Henderson -LRB- 2007 -RRB- with annotations which identify semantic role labels and has competitive performance	amod_performance_competitive dobj_has_performance nn_labels_role amod_labels_semantic dobj_identify_labels nsubj_identify_which rcmod_annotations_identify appos_Henderson_2007 conj_and_Titov_Henderson conj_and_Henderson_Henderson conj_and_Henderson_Titov appos_Henderson_2003 prep_with_parser_annotations prep_of_parser_Titov prep_of_parser_Henderson amod_parser_syntactic det_parser_the dobj_extends_parser nsubj_extends_which appos_Musillo_2008 conj_and_Merlo_Musillo prep_in_proposed_Musillo prep_in_proposed_Merlo conj_and_one_has rcmod_one_extends vmod_one_proposed det_one_the nsubj_is_has nsubj_is_one nn_model_parsing det_model_The dep_parser_is dep_parser_model amod_parser_statistical det_parser_The dep_5.1_parser ccomp_``_5.1
N09-2032	P07-1080	o	The probabilities of derivation decisions are modelled using the neural network approximation -LRB- Henderson 2003 -RRB- to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network -LRB- ISBN -RRB- -LRB- Titov and Henderson 2007 -RRB-	dep_Titov_2007 conj_and_Titov_Henderson dep_Network_Henderson dep_Network_Titov appos_Network_ISBN nn_Network_Belief nn_Network_Sigmoid amod_Network_Incremental det_Network_an dobj_called_Network nsubj_called_type dep_called_to dep_called_using nn_Network_Bayesian amod_Network_dynamic prep_of_type_Network det_type_a amod_Henderson_2003 appos_approximation_Henderson nn_approximation_network amod_approximation_neural det_approximation_the dobj_using_approximation xcomp_modelled_called auxpass_modelled_are nsubjpass_modelled_probabilities nn_decisions_derivation prep_of_probabilities_decisions det_probabilities_The ccomp_``_modelled
P08-1068	P07-1080	o	Previous research in this area includes several models which incorporate hidden variables -LRB- Matsuzaki et al. 2005 Koo and Collins 2005 Petrov et al. 2006 Titov and Henderson 2007 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et appos_Koo_2007 conj_and_Koo_Henderson conj_and_Koo_Titov conj_and_Koo_Petrov num_Koo_2005 conj_and_Koo_Collins dep_Matsuzaki_Henderson dep_Matsuzaki_Titov dep_Matsuzaki_Petrov dep_Matsuzaki_Collins dep_Matsuzaki_Koo appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et amod_variables_hidden dobj_incorporate_variables nsubj_incorporate_which rcmod_models_incorporate amod_models_several dep_includes_Matsuzaki dobj_includes_models nsubj_includes_research det_area_this prep_in_research_area amod_research_Previous
W07-2218	P07-1080	o	It is based on Incremental Sigmoid Belief Networks -LRB- ISBNs -RRB- a class of directed graphical model for structure prediction problems recently proposed in -LRB- Titov and Henderson 2007 -RRB- where they were demonstrated to achieve competitive results on the constituent parsing task	nn_task_parsing nn_task_constituent det_task_the prep_on_results_task amod_results_competitive dobj_achieve_results aux_achieve_to xcomp_demonstrated_achieve auxpass_demonstrated_were nsubjpass_demonstrated_they advmod_demonstrated_where rcmod_Titov_demonstrated dep_Titov_2007 conj_and_Titov_Henderson prep_in_proposed_Henderson prep_in_proposed_Titov advmod_proposed_recently nn_problems_prediction nn_problems_structure amod_model_graphical amod_model_directed vmod_class_proposed prep_for_class_problems prep_of_class_model det_class_a appos_Networks_class appos_Networks_ISBNs nn_Networks_Belief nn_Networks_Sigmoid amod_Networks_Incremental prep_on_based_Networks auxpass_based_is nsubjpass_based_It
W07-2218	P07-1080	p	As discussed in -LRB- Titov and Henderson 2007 -RRB- computing the conditional probabilities which we need for parsing is in general intractable with ISBNs but they can be approximated efficiently in several ways	amod_ways_several prep_in_approximated_ways advmod_approximated_efficiently auxpass_approximated_be aux_approximated_can nsubjpass_approximated_they prep_with_intractable_ISBNs dep_general_intractable prep_in_is_general prep_for_need_parsing nsubj_need_we dobj_need_which rcmod_probabilities_need amod_probabilities_conditional det_probabilities_the dobj_computing_probabilities conj_but_Titov_approximated dep_Titov_is vmod_Titov_computing dep_Titov_2007 conj_and_Titov_Henderson prep_in_discussed_approximated prep_in_discussed_Henderson prep_in_discussed_Titov mark_discussed_As advcl_``_discussed
W07-2218	P07-1080	o	We expect that the mean field approximation should demonstrate better results than feed-forward approximation on this task as it is theoretically expected and confirmed on the constituent parsing task -LRB- Titov and Henderson 2007 -RRB-	dep_Titov_2007 conj_and_Titov_Henderson appos_task_Henderson appos_task_Titov nn_task_parsing nn_task_constituent det_task_the prep_on_confirmed_task nsubjpass_confirmed_it conj_and_expected_confirmed advmod_expected_theoretically auxpass_expected_is nsubjpass_expected_it mark_expected_as det_task_this prep_on_approximation_task amod_approximation_feed-forward prep_than_results_approximation amod_results_better advcl_demonstrate_confirmed advcl_demonstrate_expected dobj_demonstrate_results aux_demonstrate_should nsubj_demonstrate_approximation mark_demonstrate_that nn_approximation_field amod_approximation_mean det_approximation_the ccomp_expect_demonstrate nsubj_expect_We
W07-2218	P07-1080	o	As discussed in -LRB- Titov and Henderson 2007 -RRB- undirected graphical models do not seem to be suitable for history-based parsing models	nn_models_parsing amod_models_history-based prep_for_suitable_models cop_suitable_be aux_suitable_to xcomp_seem_suitable neg_seem_not aux_seem_do nsubj_seem_models parataxis_seem_Henderson parataxis_seem_Titov mark_seem_in amod_models_graphical amod_models_undirected dep_Titov_2007 conj_and_Titov_Henderson ccomp_discussed_seem mark_discussed_As advcl_``_discussed
W07-2218	P07-1080	o	The extension of dynamic SBNs with incrementally specified model structure -LRB- i.e. Incremental Sigmoid Belief Networks used in this paper -RRB- was proposed and applied to constituent parsing in -LRB- Titov and Henderson 2007 -RRB-	dep_Titov_2007 conj_and_Titov_Henderson dep_in_Henderson dep_in_Titov amod_parsing_constituent prep_applied_in prep_to_applied_parsing nsubjpass_applied_extension conj_and_proposed_applied auxpass_proposed_was nsubjpass_proposed_extension det_paper_this prep_in_used_paper vmod_Networks_used nn_Networks_Belief nn_Networks_Sigmoid amod_Networks_Incremental nn_Networks_i.e. dep_structure_Networks nn_structure_model amod_structure_specified advmod_specified_incrementally amod_SBNs_dynamic prep_with_extension_structure prep_of_extension_SBNs det_extension_The ccomp_``_applied ccomp_``_proposed
W07-2218	P07-1080	o	145 2 The Latent Variable Architecture In this section we will begin by briefly introducing the class of graphical models we will be using Incremental Sigmoid Belief Networks -LRB- Titov and Henderson 2007 -RRB-	dep_Titov_2007 conj_and_Titov_Henderson appos_Networks_Henderson appos_Networks_Titov nn_Networks_Belief nn_Networks_Sigmoid amod_Networks_Incremental dobj_using_Networks aux_using_be aux_using_will nsubj_using_we amod_models_graphical prep_of_class_models det_class_the parataxis_introducing_using dobj_introducing_class dep_briefly_introducing prep_by_begin_briefly aux_begin_will nsubj_begin_we prep_in_begin_section det_section_this rcmod_Architecture_begin amod_Architecture_Variable amod_Architecture_Latent det_Architecture_The num_Architecture_2 number_2_145 dep_``_Architecture
W07-2218	P07-1080	p	They are latent variable models which are not tractable to compute exactly but two approximations exist which have been shown to be effective for constituent parsing -LRB- Titov and Henderson 2007 -RRB-	dep_Titov_2007 conj_and_Titov_Henderson appos_parsing_Henderson appos_parsing_Titov amod_parsing_constituent prep_for_effective_parsing cop_effective_be aux_effective_to xcomp_shown_effective auxpass_shown_been aux_shown_have nsubjpass_shown_which ccomp_exist_shown nsubj_exist_approximations num_approximations_two advmod_compute_exactly aux_compute_to xcomp_tractable_compute neg_tractable_not cop_tractable_are nsubj_tractable_which conj_but_models_exist rcmod_models_tractable amod_models_variable amod_models_latent cop_models_are nsubj_models_They
W07-2218	P07-1080	o	Incremental Sigmoid Belief Networks -LRB- Titov and Henderson 2007 -RRB- differ from simple dynamic SBNs in that they allow the model structure to depend on the output variable values	amod_values_variable nn_values_output det_values_the prep_on_depend_values aux_depend_to nn_structure_model det_structure_the xcomp_allow_depend dobj_allow_structure nsubj_allow_they mark_allow_that amod_SBNs_dynamic amod_SBNs_simple prepc_in_differ_allow prep_from_differ_SBNs nsubj_differ_Networks dep_Titov_2007 conj_and_Titov_Henderson appos_Networks_Henderson appos_Networks_Titov nn_Networks_Belief nn_Networks_Sigmoid amod_Networks_Incremental
W07-2218	P07-1080	o	146 2.3 Approximating ISBNs -LRB- Titov and Henderson 2007 -RRB- proposes two approximations for inference in ISBNs both based on variational methods	amod_methods_variational prep_on_based_methods vmod_both_based prep_in_inference_ISBNs appos_approximations_both prep_for_approximations_inference num_approximations_two dobj_proposes_approximations nsubj_proposes_ISBNs dep_Titov_2007 conj_and_Titov_Henderson appos_ISBNs_Henderson appos_ISBNs_Titov amod_ISBNs_Approximating num_ISBNs_2.3 number_2.3_146
W07-2218	P07-1080	o	-LRB- Titov and Henderson 2007 -RRB- proposes two approximate models based on the variational approach	amod_approach_variational det_approach_the prep_on_based_approach vmod_models_based amod_models_approximate num_models_two dobj_proposes_models nsubj_proposes_Henderson nsubj_proposes_Titov dep_Titov_2007 conj_and_Titov_Henderson
W07-2218	P07-1080	o	The second approximation proposed in -LRB- Titov and Henderson 2007 -RRB- takes into consideration the fact that after each decision is made all the preceding latent variables should have their means i updated	nsubj_updated_i dep_updated_means poss_means_their dep_have_updated aux_have_should nsubj_have_variables advcl_have_made mark_have_that amod_variables_latent amod_variables_preceding det_variables_the predet_variables_all auxpass_made_is nsubjpass_made_decision mark_made_after det_decision_each ccomp_fact_have det_fact_the dobj_takes_fact prep_into_takes_consideration nsubj_takes_approximation dep_Titov_2007 conj_and_Titov_Henderson prep_in_proposed_Henderson prep_in_proposed_Titov vmod_approximation_proposed amod_approximation_second det_approximation_The
W07-2218	P07-1080	p	For the mean field approximation propagating the error all the way back through the structure of the graphical model requires a more complicated calculation but it can still be done efficiently -LRB- see -LRB- Titov and Henderson 2007 -RRB- for details -RRB-	dep_Titov_2007 conj_and_Titov_Henderson prep_for_see_details dep_see_Henderson dep_see_Titov dep_done_see advmod_done_efficiently auxpass_done_be advmod_done_still aux_done_can nsubjpass_done_it amod_calculation_complicated det_calculation_a advmod_complicated_more dobj_requires_calculation nsubj_requires_way amod_model_graphical det_model_the prep_of_structure_model det_structure_the prep_through_back_structure advmod_way_back det_way_the predet_way_all rcmod_error_requires det_error_the conj_but_propagating_done dobj_propagating_error ccomp_,_done ccomp_,_propagating nn_approximation_field amod_approximation_mean det_approximation_the pobj_For_approximation dep_``_For
W08-2101	P07-1080	p	3 The Syntactic and Semantic Parser Architecture To achieve the complex task of joint syntactic and semantic parsing we extend a current state-of-theart statistical parser -LRB- Titov and Henderson 2007 -RRB- to learn semantic role annotation as well as syntactic structure	amod_structure_syntactic conj_and_annotation_structure nn_annotation_role amod_annotation_semantic dobj_learn_structure dobj_learn_annotation aux_learn_to dep_Titov_2007 conj_and_Titov_Henderson appos_parser_Henderson appos_parser_Titov amod_parser_statistical amod_parser_state-of-theart amod_parser_current det_parser_a vmod_extend_learn dobj_extend_parser nsubj_extend_we nsubj_extend_Architecture nsubj_extend_Syntactic amod_parsing_semantic conj_and_syntactic_parsing amod_syntactic_joint prep_of_task_parsing prep_of_task_syntactic nn_task_complex det_task_the dobj_achieve_task aux_achieve_To nn_Architecture_Parser nn_Architecture_Semantic vmod_Syntactic_achieve conj_and_Syntactic_Architecture det_Syntactic_The num_Syntactic_3
W08-2101	P07-1080	o	Following -LRB- Titov and Henderson 2007 -RRB- we describe the original parsing architecture and our modifications to it as a Dynamic Bayesian network	nn_network_Bayesian nn_network_Dynamic det_network_a poss_modifications_our conj_and_architecture_modifications nn_architecture_parsing amod_architecture_original det_architecture_the prep_as_describe_network prep_to_describe_it dobj_describe_modifications dobj_describe_architecture nsubj_describe_we vmod_describe_Following dep_Titov_2007 conj_and_Titov_Henderson dep_Following_Henderson dep_Following_Titov
W08-2101	P07-1080	o	For more detail explanations and experiments see -LRB- Titov and Henderson 2007 -RRB-	dep_Titov_2007 conj_and_Titov_Henderson dep_see_Henderson dep_see_Titov nsubj_see_experiments nsubj_see_explanations prep_for_see_detail conj_and_explanations_experiments amod_detail_more
W08-2101	P07-1080	o	parsing -LRB- Titov and Henderson 2007 -RRB-	dep_Titov_2007 conj_and_Titov_Henderson dep_parsing_Henderson dep_parsing_Titov
W08-2122	P07-1080	p	Our probabilistic model is based on Incremental Sigmoid Belief Networks -LRB- ISBNs -RRB- a recently proposed latent variable model for syntactic structure prediction which has shown very good behaviour for both constituency -LRB- Titov and Henderson 2007a -RRB- and dependency parsing -LRB- Titov and Henderson 2007b -RRB-	appos_Titov_2007b conj_and_Titov_Henderson dep_parsing_Henderson dep_parsing_Titov nn_parsing_dependency conj_and_Titov_2007a conj_and_Titov_Henderson det_constituency_both conj_and_behaviour_parsing dep_behaviour_2007a dep_behaviour_Henderson dep_behaviour_Titov prep_for_behaviour_constituency amod_behaviour_good advmod_good_very dobj_shown_parsing dobj_shown_behaviour aux_shown_has nsubj_shown_which rcmod_prediction_shown nn_prediction_structure amod_prediction_syntactic prep_for_model_prediction amod_model_variable amod_model_latent amod_model_proposed det_model_a advmod_proposed_recently appos_Networks_model appos_Networks_ISBNs nn_Networks_Belief nn_Networks_Sigmoid amod_Networks_Incremental prep_on_based_Networks auxpass_based_is nsubjpass_based_model amod_model_probabilistic poss_model_Our ccomp_``_based
W08-2122	P07-1080	o	2.1 Synchronous derivations The derivations for syntactic dependency trees are the same as specified in -LRB- Titov and Henderson 2007b -RRB- which are based on the shift-reduce style parser of -LRB- Nivre et al. 2006 -RRB-	amod_Nivre_2006 dep_Nivre_al. nn_Nivre_et dep_of_Nivre prep_parser_of nn_parser_style amod_parser_shift-reduce det_parser_the prep_on_based_parser auxpass_based_are nsubjpass_based_which rcmod_Titov_based appos_Titov_2007b conj_and_Titov_Henderson prep_in_specified_Henderson prep_in_specified_Titov mark_specified_as advcl_same_specified det_same_the cop_same_are nsubj_same_derivations nn_trees_dependency amod_trees_syntactic prep_for_derivations_trees det_derivations_The rcmod_derivations_same amod_derivations_Synchronous num_derivations_2.1 dep_``_derivations
W08-2122	P07-1080	o	P -LRB- ctd | C1 Ct1 -RRB- = producttextiP -LRB- Did | Dbtdd Di1d C1 Ct1 -RRB- -LRB- 3 -RRB- The actions are also sometimes split into a sequence of elementary decisions Di = di1 din as discussed in -LRB- Titov and Henderson 2007a -RRB-	appos_Titov_2007a conj_and_Titov_Henderson dep_in_Henderson dep_in_Titov prep_discussed_in mark_discussed_as advcl_di1_discussed appos_di1_din dep_=_di1 npadvmod_=_Di amod_decisions_= amod_decisions_elementary prep_of_sequence_decisions det_sequence_a prep_into_split_sequence advmod_split_sometimes advmod_split_also auxpass_split_are nsubjpass_split_actions dep_split_3 dep_split_P det_actions_The appos_Di1d_Ct1 conj_Di1d_C1 num_Dbtdd_| dep_Did_Di1d dobj_Did_Dbtdd dep_=_producttextiP dep_C1_Ct1 num_C1_| nn_C1_ctd dep_P_Did amod_P_= dep_P_C1
W08-2122	P07-1080	o	As with many dependency parsers -LRB- Nivre et al. 2006 Titov and Henderson 2007b -RRB- we handle non-projective -LRB- i.e. crossing -RRB- arcs by transforming them into noncrossing arcs with augmented labels .1 Because our syntactic derivations are equivalent to those of -LRB- Nivre et al. 2006 -RRB- we use their HEAD methods to projectivise the syntactic dependencies	amod_dependencies_syntactic det_dependencies_the dobj_projectivise_dependencies aux_projectivise_to nn_methods_HEAD poss_methods_their vmod_use_projectivise dobj_use_methods nsubj_use_we amod_Nivre_2006 dep_Nivre_al. nn_Nivre_et dep_of_Nivre prep_those_of prep_to_equivalent_those cop_equivalent_are nsubj_equivalent_derivations mark_equivalent_Because amod_derivations_syntactic poss_derivations_our num_labels_.1 amod_labels_augmented amod_arcs_noncrossing prep_with_transforming_labels prep_into_transforming_arcs dobj_transforming_them amod_arcs_non-projective pcomp_i.e._crossing dep_non-projective_i.e. parataxis_handle_use advcl_handle_equivalent prepc_by_handle_transforming dobj_handle_arcs nsubj_handle_we prep_handle_As appos_Titov_2007b conj_and_Titov_Henderson dep_Nivre_Henderson dep_Nivre_Titov appos_Nivre_2006 dep_Nivre_al. nn_Nivre_et appos_parsers_Nivre nn_parsers_dependency amod_parsers_many pobj_with_parsers pcomp_As_with
W08-2122	P07-1080	o	3 The Learning Architecture The synchronous derivations described above are modelled with an Incremental Sigmoid Belief Network -LRB- ISBN -RRB- -LRB- Titov and Henderson 2007a -RRB-	appos_Titov_2007a conj_and_Titov_Henderson dep_Network_Henderson dep_Network_Titov appos_Network_ISBN nn_Network_Belief nn_Network_Sigmoid amod_Network_Incremental det_Network_an prep_with_modelled_Network auxpass_modelled_are nsubjpass_modelled_derivations advmod_described_above vmod_derivations_described amod_derivations_synchronous nn_derivations_The nn_derivations_Architecture nn_derivations_Learning det_derivations_The rcmod_3_modelled ccomp_``_3
W08-2122	P07-1080	o	We use the neural network approximation -LRB- Titov and Henderson 2007a -RRB- to perform inference in our model	poss_model_our prep_in_perform_model dobj_perform_inference aux_perform_to appos_Titov_2007a conj_and_Titov_Henderson dep_approximation_Henderson dep_approximation_Titov nn_approximation_network amod_approximation_neural det_approximation_the vmod_use_perform dobj_use_approximation nsubj_use_We
W09-0438	P07-1080	o	Neural networks have been used in NLP in the past e.g. for machine translation -LRB- Asuncion Castano et al. 1997 -RRB- and constituent parsing -LRB- Titov and Henderson 2007 -RRB-	dep_Titov_2007 conj_and_Titov_Henderson appos_parsing_Henderson appos_parsing_Titov amod_parsing_constituent amod_Castano_1997 dep_Castano_al. nn_Castano_et nn_Castano_Asuncion conj_and_translation_parsing dep_translation_Castano nn_translation_machine pobj_for_parsing pobj_for_translation pcomp_e.g._for det_past_the prep_used_e.g. prep_in_used_past prep_in_used_NLP auxpass_used_been aux_used_have nsubjpass_used_networks amod_networks_Neural
P08-1078	P07-1088	o	Recently some generic methods were proposed to handle context-sensitive inference -LRB- Dagan et al. 2006 Pantel et al. 2007 Downey et al. 2007 Connor and Roth 2007 -RRB- but these usually treat only a single aspect of context matching -LRB- see Section 6 -RRB-	num_Section_6 dobj_see_Section nn_matching_context prep_of_aspect_matching amod_aspect_single det_aspect_a advmod_aspect_only parataxis_treat_see dobj_treat_aspect advmod_treat_usually nsubj_treat_these num_Downey_2007 nn_Downey_al. nn_Downey_et dep_Pantel_2007 conj_and_Pantel_Roth conj_and_Pantel_Connor conj_and_Pantel_Downey num_Pantel_2007 nn_Pantel_al. nn_Pantel_et dep_Dagan_Roth dep_Dagan_Connor dep_Dagan_Downey dep_Dagan_Pantel appos_Dagan_2006 dep_Dagan_al. nn_Dagan_et appos_inference_Dagan amod_inference_context-sensitive dobj_handle_inference aux_handle_to conj_but_proposed_treat xcomp_proposed_handle auxpass_proposed_were nsubjpass_proposed_methods advmod_proposed_Recently amod_methods_generic det_methods_some
P08-1078	P07-1088	o	-LRB- Downey et al. 2007 -RRB- use HMM-based similarity for the same purpose	amod_purpose_same det_purpose_the amod_similarity_HMM-based prep_for_use_purpose dobj_use_similarity nsubj_use_Downey amod_Downey_2007 dep_Downey_al. nn_Downey_et
P09-1056	P07-1088	o	REALM uses an HMM trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type -LRB- Downey et al. 2007 -RRB-	amod_Downey_2007 dep_Downey_al. nn_Downey_et amod_type_appropriate det_type_the prep_of_are_type nsubj_are_arguments mark_are_whether nn_relation_candidate det_relation_a prep_of_arguments_relation det_arguments_the ccomp_determine_are ccomp_help_determine aux_help_to amod_corpus_large det_corpus_a xcomp_trained_help prep_on_trained_corpus vmod_HMM_trained det_HMM_an dep_uses_Downey dobj_uses_HMM nsubj_uses_REALM
D08-1052	P07-1096	o	Similar to bidirectional labelling in -LRB- Shen et al. 2007 -RRB- there are two learning tasking in this model	det_model_this prep_in_tasking_model amod_tasking_learning num_tasking_two nsubj_are_tasking expl_are_there advmod_are_Similar amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_in_Shen prep_labelling_in amod_labelling_bidirectional prep_to_Similar_labelling
D08-1052	P07-1096	o	The idea of bidirectional parsing is related to the bidirectional sequential classification method described in -LRB- Shen et al. 2007 -RRB-	amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_in_Shen prep_described_in vmod_method_described nn_method_classification amod_method_sequential amod_method_bidirectional det_method_the prep_to_related_method auxpass_related_is nsubjpass_related_idea amod_parsing_bidirectional prep_of_idea_parsing det_idea_The
D08-1052	P07-1096	o	The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described in -LRB- Shen et al. 2007 -RRB-	amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_in_Shen prep_described_in mark_described_as prep_for_algorithm_labelling nn_algorithm_learning amod_algorithm_guided det_algorithm_the advcl_similar_described prep_to_similar_algorithm cop_similar_is nsubj_similar_algorithm nn_dependency_level-0 prep_for_algorithm_dependency nn_algorithm_learning det_algorithm_The
E09-1087	P07-1096	p	The state-of-the art taggers are using feature sets discribed in the corresponding articles -LRB- -LRB- Collins 2002 -RRB- -LRB- Gimenez and M`arquez 2004 -RRB- -LRB- Toutanova et al. 2003 -RRB- and -LRB- Shen et al. 2007 -RRB- -RRB- Morce supervised and Morce semi-supervised are using feature set desribed in section 4	num_section_4 prep_in_desribed_section dep_set_desribed dobj_using_feature aux_using_are amod_Morce_semi-supervised dep_supervised_set dep_supervised_using conj_and_supervised_Morce nsubj_supervised_Morce dep_supervised_Collins dep_Shen_2007 dep_Shen_al. nn_Shen_et amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_Gimenez_2004 conj_and_Gimenez_M`arquez dep_Collins_Shen cc_Collins_and appos_Collins_Toutanova appos_Collins_M`arquez appos_Collins_Gimenez amod_Collins_2002 dep_articles_Morce dep_articles_supervised amod_articles_corresponding det_articles_the prep_in_discribed_articles vmod_sets_discribed nn_sets_feature dobj_using_sets aux_using_are nsubj_using_taggers nn_taggers_art amod_taggers_state-of-the det_taggers_The ccomp_``_using
E09-1087	P07-1096	n	The combination is significantly better than -LRB- Shen et al. 2007 -RRB- at a very high level but more importantly Shens results -LRB- currently representing the replicable state-of-the-art in POS tagging -RRB- have been significantly surpassed also by the semisupervised Morce -LRB- at the 99 % confidence level -RRB-	nn_level_confidence amod_level_% det_level_the number_%_99 prep_at_Morce_level amod_Morce_semisupervised det_Morce_the agent_surpassed_Morce advmod_surpassed_also advmod_surpassed_significantly auxpass_surpassed_been aux_surpassed_have nsubjpass_surpassed_results ccomp_surpassed_importantly ccomp_surpassed_better nn_tagging_POS prep_in_state-of-the-art_tagging amod_state-of-the-art_replicable det_state-of-the-art_the dobj_representing_state-of-the-art advmod_representing_currently dep_results_representing nn_results_Shens advmod_importantly_more amod_level_high det_level_a advmod_high_very amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_than_Shen conj_but_better_importantly prep_at_better_level prep_better_than advmod_better_significantly cop_better_is nsubj_better_combination det_combination_The
E09-1087	P07-1096	n	In addition the semi-supervised Morce performs -LRB- on single CPU and development data set -RRB- 77 times faster than the combination and 23 times faster than -LRB- Shen et al. 2007 -RRB-	amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_than_Shen prep_faster_than npadvmod_faster_times num_times_23 conj_and_combination_faster det_combination_the prep_than_faster_faster prep_than_faster_combination npadvmod_faster_times num_times_77 amod_data_faster dep_data_set dep_development_data conj_and_CPU_development amod_CPU_single prep_on_performs_development prep_on_performs_CPU nsubj_performs_Morce prep_in_performs_addition amod_Morce_semi-supervised det_Morce_the
E09-1087	P07-1096	o	Finally it would be nice to merge some of the approaches by -LRB- Toutanova et al. 2003 -RRB- and -LRB- Shen et al. 2007 -RRB- with the ideas of semi-supervised learning introduced here since they seem orthogonal in at least some aspects -LRB- e.g. to replace the rudimentary lookahead features with full bidirectionality -RRB-	amod_bidirectionality_full prep_with_features_bidirectionality nn_features_lookahead amod_features_rudimentary det_features_the dobj_replace_features aux_replace_to dep_e.g._replace dep_aspects_e.g. det_aspects_some advmod_aspects_at pobj_at_least prep_in_seem_aspects acomp_seem_orthogonal nsubj_seem_they mark_seem_since advmod_introduced_here vmod_learning_introduced amod_learning_semi-supervised prep_of_ideas_learning det_ideas_the prep_with_Shen_ideas amod_Shen_2007 dep_Shen_al. nn_Shen_et conj_and_Toutanova_Shen amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et prep_by_approaches_Shen prep_by_approaches_Toutanova det_approaches_the prep_of_some_approaches dobj_merge_some aux_merge_to advcl_nice_seem xcomp_nice_merge cop_nice_be aux_nice_would nsubj_nice_it advmod_nice_Finally
E09-1087	P07-1096	p	For English after a relatively big jump achieved by -LRB- Collins 2002 -RRB- we have seen two significant improvements -LRB- Toutanova et al. 2003 -RRB- and -LRB- Shen et al. 2007 -RRB- pushed the results by a significant amount each time .1 1In our final comparison we have also included the results of -LRB- Gimenez and M`arquez 2004 -RRB- because it has surpassed -LRB- Collins 2002 -RRB- as well and we have used this tagger in the data preparation phase	nn_phase_preparation nn_phase_data det_phase_the det_tagger_this prep_in_used_phase dobj_used_tagger aux_used_have nsubj_used_we advmod_well_as amod_Collins_2002 conj_and_surpassed_used advmod_surpassed_well dep_surpassed_Collins aux_surpassed_has nsubj_surpassed_it mark_surpassed_because advcl_Gimenez_used advcl_Gimenez_surpassed dep_Gimenez_2004 conj_and_Gimenez_M`arquez prep_of_results_M`arquez prep_of_results_Gimenez det_results_the dobj_included_results advmod_included_also aux_included_have nsubj_included_we amod_comparison_final poss_comparison_our dep_1In_comparison num_1In_.1 dep_time_1In det_time_each tmod_amount_time amod_amount_significant det_amount_a det_results_the parataxis_pushed_included prep_by_pushed_amount dobj_pushed_results nsubj_pushed_Shen nsubj_pushed_improvements amod_Shen_2007 dep_Shen_al. nn_Shen_et amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et conj_and_improvements_Shen dep_improvements_Toutanova amod_improvements_significant num_improvements_two dep_seen_pushed aux_seen_have nsubj_seen_we prep_after_seen_jump prep_for_seen_English amod_Collins_2002 agent_achieved_Collins vmod_jump_achieved amod_jump_big det_jump_a advmod_big_relatively
E09-1087	P07-1096	p	As a result of this tuning our -LRB- fully supervised -RRB- version of the Morce tagger gives the best accuracy among all single taggers for Czech and also very good results for English being beaten only by the tagger -LRB- Shen et al. 2007 -RRB- -LRB- by 0.10 % absolute -RRB- and -LRB- not significantly -RRB- by -LRB- Toutanova et al. 2003 -RRB-	amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_by_Toutanova prep_significantly_by dep_significantly_not amod_%_absolute num_%_0.10 dep_Shen_2007 dep_Shen_al. nn_Shen_et prep_by_tagger_% appos_tagger_Shen det_tagger_the conj_and_beaten_significantly agent_beaten_tagger advmod_beaten_only auxpass_beaten_being dep_results_significantly dep_results_beaten prep_for_results_English amod_results_good advmod_results_also advmod_good_very prep_for_taggers_Czech amod_taggers_single det_taggers_all prep_among_accuracy_taggers amod_accuracy_best det_accuracy_the conj_and_gives_results dobj_gives_accuracy nsubj_gives_version prep_as_gives_result nn_tagger_Morce det_tagger_the prep_of_version_tagger dep_version_supervised poss_version_our advmod_supervised_fully det_tuning_this prep_of_result_tuning det_result_a
E09-1087	P07-1096	o	3 The data 3.1 The supervised data For English we use the same data division of Penn Treebank -LRB- PTB -RRB- parsed section -LRB- Marcus et al. 1994 -RRB- as all of -LRB- Collins 2002 -RRB- -LRB- Toutanova et al. 2003 -RRB- -LRB- Gimenez and M`arquez 2004 -RRB- and -LRB- Shen et al. 2007 -RRB- do for details see Table 1	num_Table_1 dobj_see_Table dep_do_see prep_for_do_details nsubj_do_division amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_Gimenez_2004 conj_and_Gimenez_M`arquez amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et conj_and_Collins_Shen appos_Collins_M`arquez appos_Collins_Gimenez appos_Collins_Toutanova amod_Collins_2002 prep_of_all_Shen prep_of_all_Collins dep_Marcus_1994 dep_Marcus_al. nn_Marcus_et prep_as_section_all appos_section_Marcus amod_section_parsed appos_Treebank_PTB nn_Treebank_Penn dep_division_section prep_of_division_Treebank nn_division_data amod_division_same det_division_the parataxis_use_do nsubj_use_we dep_use_data prep_for_data_English amod_data_supervised det_data_The num_data_3.1 nn_data_data det_data_The num_data_3
E09-1087	P07-1096	o	In the following sections we present the best performing set of feature templates as determined on the development data set using only the supervised training setting our feature templates have thus not been influenced nor extended by the unsupervised data .13 11The full list of tags as used by -LRB- Shen et al. 2007 -RRB- also makes the underlying Viterbi algorithm unbearably slow	advmod_slow_unbearably nsubj_slow_algorithm nn_algorithm_Viterbi amod_algorithm_underlying det_algorithm_the xcomp_makes_slow advmod_makes_also amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_by_Shen prep_used_by mark_used_as prep_of_list_tags amod_list_full nn_list_11The num_list_.13 dep_data_list amod_data_unsupervised det_data_the nsubjpass_extended_templates dep_influenced_makes advcl_influenced_used agent_influenced_data conj_nor_influenced_extended auxpass_influenced_been neg_influenced_not advmod_influenced_thus aux_influenced_have nsubjpass_influenced_templates nn_templates_feature poss_templates_our nn_setting_training amod_setting_supervised det_setting_the advmod_setting_only dobj_using_setting xcomp_set_using vmod_data_set nn_data_development det_data_the prep_on_determined_data mark_determined_as nn_templates_feature prep_of_set_templates amod_set_performing amod_set_best det_set_the parataxis_present_extended parataxis_present_influenced advcl_present_determined dobj_present_set nsubj_present_we prep_in_present_sections amod_sections_following det_sections_the
E09-1087	P07-1096	n	Most recently -LRB- Suzuki and Isozaki 2008 -RRB- published their Semi-supervised sequential labelling method whose results on POS tagging seem to be optically better than -LRB- Shen et al. 2007 -RRB- but no significance tests were given and the tool is not available for download i.e. for repeating the results and significance testing	nn_testing_significance conj_and_results_testing det_results_the dobj_repeating_testing dobj_repeating_results pcomp_for_repeating pcomp_i.e._for prep_for_available_download neg_available_not cop_available_is nsubj_available_tool det_tool_the prep_given_i.e. conj_and_given_available auxpass_given_were nsubjpass_given_tests nn_tests_significance neg_tests_no amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_than_Shen prep_better_than advmod_better_optically cop_better_be aux_better_to xcomp_seem_better nsubj_seem_results vmod_POS_tagging prep_on_results_POS poss_results_whose conj_but_method_available conj_but_method_given rcmod_method_seem nn_method_labelling amod_method_sequential amod_method_Semi-supervised poss_method_their dobj_published_given dobj_published_method dep_published_Isozaki dep_published_Suzuki advmod_published_recently dep_Suzuki_2008 conj_and_Suzuki_Isozaki advmod_recently_Most
E09-1087	P07-1096	p	For English we use three state-of-the-art taggers the taggers of -LRB- Toutanova et al. 2003 -RRB- and -LRB- Shen et al. 2007 -RRB- in Step 1 and the SVM tagger -LRB- Gimenez and M`arquez 2004 -RRB- in Step 3	num_Step_3 dep_Gimenez_2004 conj_and_Gimenez_M`arquez prep_in_tagger_Step dep_tagger_M`arquez dep_tagger_Gimenez nn_tagger_SVM det_tagger_the num_Step_1 amod_Shen_2007 dep_Shen_al. nn_Shen_et conj_and_Toutanova_tagger prep_in_Toutanova_Step conj_and_Toutanova_Shen amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et prep_of_taggers_tagger prep_of_taggers_Shen prep_of_taggers_Toutanova det_taggers_the dep_taggers_taggers amod_taggers_state-of-the-art num_taggers_three dobj_use_taggers nsubj_use_we prep_for_use_English
P08-1076	P07-1096	o	test additional resources JESS-CM -LRB- CRF/HMM -RRB- 97.35 97.40 1G-word unlabeled data -LRB- Shen et al. 2007 -RRB- 97.28 97.33 -LRB- Toutanova et al. 2003 -RRB- 97.15 97.24 crude company name detector -LSB- sup	dep_detector_sup nn_detector_name nn_detector_company amod_detector_crude num_detector_97.24 num_detector_97.15 num_detector_97.33 dep_detector_Shen dep_detector_data dep_detector_CRF/HMM dep_detector_JESS-CM amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et dep_97.33_Toutanova number_97.33_97.28 dep_Shen_2007 dep_Shen_al. nn_Shen_et amod_data_unlabeled nn_data_1G-word num_data_97.40 number_97.40_97.35 nn_JESS-CM_resources amod_JESS-CM_additional nn_JESS-CM_test
P08-1076	P07-1096	p	5 Comparison with Previous Top Systems and Related Work In POS tagging the previous best performance was reported by -LRB- Shen et al. 2007 -RRB- as summarized in Table 7	num_Table_7 prep_in_summarized_Table mark_summarized_as amod_Shen_2007 dep_Shen_al. nn_Shen_et advcl_reported_summarized agent_reported_Shen auxpass_reported_was nsubjpass_reported_performance advcl_reported_Comparison amod_performance_best amod_performance_previous det_performance_the nn_tagging_POS amod_Work_Related conj_and_Systems_Work nn_Systems_Top amod_Systems_Previous prep_in_Comparison_tagging prep_with_Comparison_Work prep_with_Comparison_Systems num_Comparison_5
P08-1076	P07-1096	o	For our POS tagging experiments we used the Wall Street Journal in PTB III -LRB- Marcus et al. 1994 -RRB- with the same data split as used in -LRB- Shen et al. 2007 -RRB-	amod_Shen_2007 dep_Shen_al. nn_Shen_et dep_in_Shen prep_used_in advmod_used_as dep_split_used amod_data_same det_data_the amod_Marcus_1994 dep_Marcus_al. nn_Marcus_et nn_III_PTB nn_Journal_Street nn_Journal_Wall det_Journal_the dep_used_split prep_with_used_data dep_used_Marcus prep_in_used_III dobj_used_Journal nsubj_used_we prep_for_used_POS amod_experiments_tagging dep_POS_experiments poss_POS_our
P08-2009	P07-1096	o	5 Bidirectional Sequence Classification Bidirectional POS tagging -LRB- Shen et al. 2007 -RRB- the current state of the art for English has some properties that make it appropriate for Icelandic	prep_for_appropriate_Icelandic amod_it_appropriate dobj_make_it nsubj_make_that rcmod_properties_make det_properties_some dobj_has_properties nsubj_has_tagging prep_for_art_English det_art_the prep_of_state_art amod_state_current det_state_the amod_Shen_2007 dep_Shen_al. nn_Shen_et appos_tagging_state appos_tagging_Shen nn_tagging_POS amod_tagging_Bidirectional nn_tagging_Classification nn_tagging_Sequence amod_tagging_Bidirectional num_tagging_5
P09-1054	P07-1096	o	Shen et al. -LRB- 2007 -RRB- report an accuracy of 97.33 % on the same data set using a perceptron-based bidirectional tagging model	nn_model_tagging amod_model_bidirectional amod_model_perceptron-based det_model_a dobj_using_model xcomp_set_using vmod_data_set amod_data_same det_data_the num_%_97.33 prep_on_accuracy_data prep_of_accuracy_% det_accuracy_an dobj_report_accuracy nsubj_report_al. appos_al._2007 nn_al._et nn_al._Shen
W08-2103	P07-1096	o	Networks -LRB- Toutanova et al. 2003 -RRB- 97.24 SVM -LRB- Gimenez and M`arquez 2003 -RRB- 97.05 ME based a bidirectional inference -LRB- Tsuruoka and Tsujii 2005 -RRB- 97.15 Guided learning for bidirectional sequence classification -LRB- Shen et al. 2007 -RRB- 97.33 AdaBoost.SDF with candidate features -LRB- = 2 = 1 = 100 W-dist -RRB- 97.32 AdaBoost.SDF with candidate features -LRB- = 2 = 10 = 10 F-dist -RRB- 97.32 SVM with candidate features -LRB- C = 0.1 d = 2 -RRB- 97.32 Text Chunking F = 1 Regularized Winnow + full parser output -LRB- Zhang et al. 2001 -RRB- 94.17 SVM-voting -LRB- Kudo and Matsumoto 2001 -RRB- 93.91 ASO + unlabeled data -LRB- Ando and Zhang 2005 -RRB- 94.39 CRF+R eranking -LRB- Kudo et al. 2005 -RRB- 94.12 ME based a bidirectional inference -LRB- Tsuruoka and Tsujii 2005 -RRB- 93.70 LaSo -LRB- Approximate Large Margin Update -RRB- -LRB- Daume III and Marcu 2005 -RRB- 94.4 HySOL -LRB- Suzuki et al. 2007 -RRB- 94.36 AdaBoost.SDF with candidate featuers -LRB- = 2 = 1 = W-dist -RRB- 94.32 AdaBoost.SDF with candidate featuers -LRB- = 2 = 10 = 10,W-dist -RRB- 94.30 SVM with candidate features -LRB- C = 1 d = 2 -RRB- 94.31 One of the reasons that boosting-based classifiers realize faster classification speed is sparseness of rules	prep_of_sparseness_rules cop_sparseness_is nsubj_sparseness_speed nn_speed_classification advmod_speed_faster ccomp_realize_sparseness nsubj_realize_classifiers dobj_realize_that amod_classifiers_boosting-based rcmod_reasons_realize det_reasons_the prep_of_One_reasons num_One_94.31 dobj_=_2 nsubj_=_d dep_=_1 rcmod_C_= amod_C_= dep_features_One dep_features_C nn_features_candidate prep_with_SVM_features num_SVM_94.30 dep_=_SVM dep_=_10,W-dist dep_=_10 dep_=_= dep_=_= dep_=_2 dep_featuers_= nn_featuers_candidate prep_with_AdaBoost.SDF_featuers num_AdaBoost.SDF_94.32 dep_=_1 dep_=_W-dist dep_=_= dep_=_= dep_=_2 dep_featuers_AdaBoost.SDF dep_featuers_= nn_featuers_candidate prep_with_AdaBoost.SDF_featuers num_AdaBoost.SDF_94.36 amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et dep_HySOL_AdaBoost.SDF dep_HySOL_Suzuki num_HySOL_94.4 dep_III_2005 conj_and_III_Marcu nn_III_Daume nn_Update_Margin nn_Update_Approximate amod_Margin_Large dep_LaSo_HySOL dep_LaSo_Marcu dep_LaSo_III appos_LaSo_Update num_LaSo_93.70 dep_Tsuruoka_LaSo dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii dep_inference_Tsujii dep_inference_Tsuruoka amod_inference_bidirectional det_inference_a pobj_based_inference num_ME_94.12 amod_Kudo_2005 dep_Kudo_al. nn_Kudo_et dep_eranking_ME dep_eranking_Kudo nn_eranking_CRF+R num_eranking_94.39 amod_Ando_2005 conj_and_Ando_Zhang amod_data_unlabeled dep_ASO_eranking dep_ASO_Zhang dep_ASO_Ando conj_+_ASO_data num_ASO_93.91 conj_and_Kudo_Matsumoto num_SVM-voting_94.17 amod_Zhang_2001 dep_Zhang_al. nn_Zhang_et nn_output_parser amod_output_full conj_+_Winnow_output amod_Winnow_Regularized num_Winnow_1 dep_=_output dep_=_Winnow amod_F_= dobj_Chunking_F vmod_Text_Chunking vmod_97.32_Text dobj_=_2 nsubj_=_d dep_=_0.1 dep_C_= amod_C_= dep_features_97.32 dep_features_C nn_features_candidate prep_with_SVM_features num_SVM_97.32 dep_=_10 dep_=_10 dep_=_F-dist dep_=_= dep_=_= dep_=_2 dep_features_SVM dep_features_= nn_features_candidate prep_with_AdaBoost.SDF_features nn_AdaBoost.SDF_W-dist num_W-dist_97.32 num_W-dist_100 dep_=_AdaBoost.SDF dep_=_1 dep_=_2001 dep_=_Matsumoto dep_=_Kudo dep_=_SVM-voting dep_=_Zhang dep_=_= dep_=_= dep_=_2 dep_features_data dep_features_ASO dep_features_= nn_features_candidate prep_AdaBoost.SDF_based prep_with_AdaBoost.SDF_features num_AdaBoost.SDF_97.33 dep_AdaBoost.SDF_Shen amod_Shen_2007 dep_Shen_al. nn_Shen_et nn_classification_sequence amod_classification_bidirectional prep_for_learning_classification xcomp_Guided_learning dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii dep_inference_AdaBoost.SDF vmod_inference_Guided num_inference_97.15 appos_inference_Tsujii appos_inference_Tsuruoka amod_inference_bidirectional det_inference_a pobj_based_inference prep_ME_based num_ME_97.05 dep_Gimenez_2003 conj_and_Gimenez_M`arquez dep_SVM_ME dep_SVM_M`arquez dep_SVM_Gimenez num_SVM_97.24 dep_SVM_Toutanova nn_SVM_Networks dep_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et
D09-1160	P07-1104	o	lscript1-regularized log-linear models -LRB- lscript1-LLMs -RRB- on the other hand provide sparse solutions in which weights of irrelevant features are exactly zero by assumingaLaplacianpriorontheweights -LRB- Tibshirani 1996 Kazama and Tsujii 2003 Goodman 2004 Gao et al. 2007 -RRB-	num_Gao_2007 nn_Gao_al. nn_Gao_et num_Goodman_2004 dep_Kazama_Gao conj_and_Kazama_Goodman conj_and_Kazama_2003 conj_and_Kazama_Tsujii dep_Tibshirani_Goodman dep_Tibshirani_2003 dep_Tibshirani_Tsujii dep_Tibshirani_Kazama appos_Tibshirani_1996 dep_assumingaLaplacianpriorontheweights_Tibshirani advmod_zero_exactly cop_zero_are nsubj_zero_weights prep_in_zero_which amod_features_irrelevant prep_of_weights_features rcmod_solutions_zero amod_solutions_sparse prep_by_provide_assumingaLaplacianpriorontheweights dobj_provide_solutions nsubj_provide_models amod_hand_other det_hand_the prep_on_models_hand appos_models_lscript1-LLMs amod_models_log-linear amod_models_lscript1-regularized ccomp_``_provide
E09-1090	P07-1104	o	The L1 or L2 norm is commonly used in statistical natural language processing -LRB- Gao et al. 2007 -RRB-	amod_Gao_2007 dep_Gao_al. nn_Gao_et nn_processing_language amod_processing_natural amod_processing_statistical dep_used_Gao prep_in_used_processing advmod_used_commonly auxpass_used_is nsubjpass_used_norm nn_norm_L2 nn_norm_L1 det_norm_The conj_or_L1_L2
N09-2025	P07-1104	o	In other words learning with L1 regularization naturally has an intrinsic effect of feature selection which results in an 97 efficient and interpretable inference with almost the same performance as L2 regularization -LRB- Gao et al. 2007 -RRB-	amod_Gao_2007 dep_Gao_al. nn_Gao_et dep_regularization_Gao nn_regularization_L2 prep_as_performance_regularization amod_performance_same det_performance_the advmod_performance_almost prep_with_inference_performance amod_inference_interpretable amod_inference_efficient num_inference_97 det_inference_an conj_and_efficient_interpretable prep_in_results_inference nsubj_results_which rcmod_selection_results nn_selection_feature prep_of_effect_selection amod_effect_intrinsic det_effect_an dobj_has_effect csubj_has_learning prep_in_has_words nn_regularization_L1 advmod_learning_naturally prep_with_learning_regularization amod_words_other
P09-1054	P07-1104	p	There is usually not a considerable difference between the two methods in terms of the accuracy of the resulting model -LRB- Gao et al. 2007 -RRB- but L1 regularization has a significant advantage in practice	amod_advantage_significant det_advantage_a prep_in_has_practice dobj_has_advantage nsubj_has_regularization nn_regularization_L1 amod_Gao_2007 dep_Gao_al. nn_Gao_et amod_model_resulting det_model_the prep_of_accuracy_model det_accuracy_the prep_of_terms_accuracy dep_methods_Gao prep_in_methods_terms num_methods_two det_methods_the prep_between_difference_methods amod_difference_considerable det_difference_a neg_difference_not advmod_difference_usually conj_but_is_has nsubj_is_difference expl_is_There ccomp_``_has ccomp_``_is
W08-0404	P07-1104	o	3.5 Regularization We apply lscript1 regularization -LRB- Ng 2004 Gao et al. 2007 -RRB- to make learning more robust to noise and control the effective dimensionality of the feature spacebysubtractingaweightedsumofabsolutevalues of parameter weights from the log-likelihood of the training data w = argmaxw LL -LRB- w -RRB- summationdisplay i Ci | wi | -LRB- 6 -RRB- We optimize the objective using a variant of the orthant-wise limited-memory quasi-Newton algorithm proposed by Andrew & Gao -LRB- 2007 -RRB- .3 All values Ci are set to 1 in most of the experiments below although we apply stronger regularization -LRB- Ci = 3 -RRB- to reordering features	nn_features_reordering dep_=_3 amod_Ci_= dep_regularization_Ci amod_regularization_stronger prep_to_apply_features dobj_apply_regularization nsubj_apply_we mark_apply_although advmod_experiments_below det_experiments_the prep_of_most_experiments prep_to_set_1 auxpass_set_are nsubjpass_set_Ci prep_in_values_most rcmod_values_set det_values_All nn_.3_Gao nn_.3_Andrew appos_Andrew_2007 conj_and_Andrew_Gao dobj_proposed_values agent_proposed_.3 vmod_algorithm_proposed amod_algorithm_quasi-Newton amod_algorithm_limited-memory amod_algorithm_orthant-wise det_algorithm_the prep_of_variant_algorithm det_variant_a dobj_using_variant det_objective_the xcomp_optimize_using dobj_optimize_objective nsubj_optimize_We appos_|_6 nn_|_wi nn_|_| nn_|_Ci nn_|_i nn_|_summationdisplay nn_|_LL appos_LL_w nn_LL_argmaxw dep_=_| rcmod_w_optimize amod_w_= dobj_data_w advcl_training_apply dep_training_data dep_the_training prep_of_log-likelihood_the det_log-likelihood_the nn_weights_parameter prep_from_spacebysubtractingaweightedsumofabsolutevalues_log-likelihood prep_of_spacebysubtractingaweightedsumofabsolutevalues_weights nn_spacebysubtractingaweightedsumofabsolutevalues_feature det_spacebysubtractingaweightedsumofabsolutevalues_the prep_of_dimensionality_spacebysubtractingaweightedsumofabsolutevalues amod_dimensionality_effective det_dimensionality_the dobj_control_dimensionality prep_to_robust_noise advmod_robust_more acomp_learning_robust conj_and_make_control xcomp_make_learning aux_make_to num_Gao_2007 nn_Gao_al. nn_Gao_et dep_Ng_control dep_Ng_make dep_Ng_Gao appos_Ng_2004 dep_regularization_Ng nn_regularization_lscript1 dobj_apply_regularization nsubj_apply_We rcmod_Regularization_apply num_Regularization_3.5
C08-1079	P07-1107	o	Since Soon -LRB- Soon et al. 2001 -RRB- started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem many machine learning-based systems have been built using both supervised and unsupervised learning methods -LRB- Haghighi and Klein 2007 -RRB-	amod_Haghighi_2007 conj_and_Haghighi_Klein dep_methods_Klein dep_methods_Haghighi nn_methods_learning amod_methods_unsupervised conj_and_supervised_methods preconj_supervised_both dobj_using_methods dobj_using_supervised xcomp_built_using auxpass_built_been aux_built_have nsubjpass_built_systems advcl_built_started amod_systems_learning-based nn_systems_machine amod_systems_many nn_problem_resolution nn_problem_co-reference dobj_solving_problem prepc_for_manner_solving amod_manner_pairwise det_manner_a amod_classifier_binary det_classifier_a prep_in_using_manner dobj_using_classifier amod_approach_learning nn_approach_machine det_machine_the prepc_by_using_using dobj_using_approach prepc_of_trend_using det_trend_the dobj_started_trend advmod_started_Soon dep_started_Since num_al._2001 nn_al._et advmod_al._Soon dep_Soon_al.
D08-1033	P07-1107	o	CRP-based samplers have served the communitywellinrelatedlanguagetasks suchaswordsegmentation and coreference resolution -LRB- Goldwater et al. 2006 Haghighi and Klein 2007 -RRB-	appos_Haghighi_2007 conj_and_Haghighi_Klein dep_Goldwater_Klein dep_Goldwater_Haghighi amod_Goldwater_2006 dep_Goldwater_al. nn_Goldwater_et nn_resolution_coreference dep_communitywellinrelatedlanguagetasks_Goldwater conj_and_communitywellinrelatedlanguagetasks_resolution conj_and_communitywellinrelatedlanguagetasks_suchaswordsegmentation det_communitywellinrelatedlanguagetasks_the dobj_served_resolution dobj_served_suchaswordsegmentation dobj_served_communitywellinrelatedlanguagetasks aux_served_have nsubj_served_samplers amod_samplers_CRP-based
D08-1067	P07-1107	o	Salience Feature Pronoun Name Nominal TOP 0.75 0.17 0.08 HIGH 0.55 0.28 0.17 MID 0.39 0.40 0.21 LOW 0.20 0.45 0.35 NONE 0.00 0.88 0.12 Table 2 Posterior distribution of mention type given salience -LRB- taken from Haghighi and Klein -LRB- 2007 -RRB- -RRB- 3.3 Modifications to the H&K Model Next we discuss the potential weaknesses of H&K s model and propose three modifications to it	prep_to_modifications_it num_modifications_three dobj_propose_modifications nsubj_propose_Model nn_model_s nn_model_H&K prep_of_weaknesses_model amod_weaknesses_potential det_weaknesses_the conj_and_discuss_propose dobj_discuss_weaknesses nsubj_discuss_we advmod_discuss_Next nsubj_discuss_Model dep_discuss_to nn_Model_H&K det_Model_the rcmod_Modifications_propose rcmod_Modifications_discuss num_Modifications_3.3 nn_Modifications_salience appos_Klein_2007 conj_and_Haghighi_Klein prep_from_taken_Klein prep_from_taken_Haghighi dep_salience_taken pobj_given_Modifications prep_type_given nn_type_mention prep_of_distribution_type amod_distribution_Posterior num_Table_2 num_Table_0.12 num_Table_0.88 num_Table_0.00 nn_Table_NONE num_Table_0.35 num_Table_0.45 advmod_Table_LOW num_Table_0.21 number_0.45_0.20 dep_0.40_Table number_0.40_0.39 dep_MID_0.40 appos_0.17_MID dep_0.28_0.17 number_0.28_0.55 dep_HIGH_0.28 appos_0.08_HIGH dep_0.17_0.08 number_0.17_0.75 dep_TOP_0.17 amod_TOP_Nominal dep_Name_distribution dobj_Name_TOP nsubj_Name_Pronoun nn_Pronoun_Feature nn_Pronoun_Salience
D08-1067	P07-1107	n	For comparison purposes we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by Haghighi and Klein -LRB- 2007 -RRB- discuss its potential weaknesses and consequently propose three modifications to their model -LRB- Section 3 -RRB-	num_Section_3 appos_model_Section poss_model_their prep_to_modifications_model num_modifications_three dobj_propose_modifications advmod_propose_consequently nsubj_propose_we amod_weaknesses_potential poss_weaknesses_its dobj_discuss_weaknesses nsubj_discuss_we appos_Klein_2007 conj_and_Haghighi_Klein agent_introduced_Klein agent_introduced_Haghighi advmod_introduced_recently nn_resolution_coreference amod_resolution_unsupervised vmod_model_introduced prep_for_model_resolution amod_model_Bayesian amod_model_fullygenerative det_model_a conj_and_revisit_propose conj_and_revisit_discuss dobj_revisit_model nsubj_revisit_we prep_for_revisit_purposes nn_purposes_comparison
D08-1067	P07-1107	o	First the addition of each modification improves the F-score for both true and system mentions 9The H&K results shown here are not directly comparable with those reported in Haghighi and Klein -LRB- 2007 -RRB- since H&K evaluated their system on the ACE 2004 coreference corpus	nn_corpus_coreference num_corpus_2004 nn_corpus_ACE det_corpus_the poss_system_their prep_on_evaluated_corpus dobj_evaluated_system nsubj_evaluated_H&K mark_evaluated_since appos_Klein_2007 conj_and_Haghighi_Klein prep_in_reported_Klein prep_in_reported_Haghighi vmod_those_reported advcl_comparable_evaluated prep_with_comparable_those advmod_comparable_directly neg_comparable_not cop_comparable_are nsubj_comparable_H&K advmod_shown_here vmod_results_shown dep_H&K_results nn_H&K_9The ccomp_mentions_comparable nsubj_mentions_F-score conj_and_true_system preconj_true_both prep_for_F-score_system prep_for_F-score_true det_F-score_the ccomp_improves_mentions nsubj_improves_addition advmod_improves_First det_modification_each prep_of_addition_modification det_addition_the
D08-1067	P07-1107	n	Experimental results indicate that our model outperforms Haghighi and Kleins -LRB- 2007 -RRB- coreference model by a large margin on the ACE data sets and compares favorably to a modified version of their model	poss_model_their prep_of_version_model amod_version_modified det_version_a prep_to_compares_version advmod_compares_favorably nsubj_compares_model nn_sets_data nn_sets_ACE det_sets_the prep_on_margin_sets amod_margin_large det_margin_a nn_model_coreference num_model_2007 nn_model_Kleins conj_and_Haghighi_model conj_and_outperforms_compares prep_by_outperforms_margin dobj_outperforms_model dobj_outperforms_Haghighi nsubj_outperforms_model mark_outperforms_that poss_model_our ccomp_indicate_compares ccomp_indicate_outperforms nsubj_indicate_results amod_results_Experimental
D08-1067	P07-1107	n	For comparison purposes we revisit Haghighi and Kleins -LRB- 2007 -RRB- fully-generative Bayesian model for unsupervised coreference resolution discuss its potential weaknesses and consequently propose three modifications to their model	poss_model_their prep_to_modifications_model num_modifications_three dobj_propose_modifications advmod_propose_consequently nsubj_propose_we amod_weaknesses_potential poss_weaknesses_its dobj_discuss_weaknesses nsubj_discuss_we nn_resolution_coreference amod_resolution_unsupervised amod_model_Bayesian amod_model_fully-generative nn_model_Kleins appos_Kleins_2007 prep_for_Haghighi_resolution conj_and_Haghighi_model conj_and_revisit_propose conj_and_revisit_discuss dobj_revisit_model dobj_revisit_Haghighi nsubj_revisit_we prep_for_revisit_purposes nn_purposes_comparison
D08-1067	P07-1107	o	3 Haghighi and Kleins Coreference Model To gauge the performance of our model we compare it with a Bayesian model for unsupervised coreference resolution that was recently proposed by Haghighi and Klein -LRB- 2007 -RRB-	appos_Klein_2007 conj_and_Haghighi_Klein agent_proposed_Klein agent_proposed_Haghighi advmod_proposed_recently auxpass_proposed_was nsubjpass_proposed_that rcmod_resolution_proposed nn_resolution_coreference amod_resolution_unsupervised prep_for_model_resolution amod_model_Bayesian det_model_a prep_with_compare_model dobj_compare_it nsubj_compare_we nsubj_compare_Model nsubj_compare_Haghighi poss_model_our prep_of_performance_model det_performance_the dobj_gauge_performance aux_gauge_To nn_Model_Coreference nn_Model_Kleins vmod_Haghighi_gauge conj_and_Haghighi_Model num_Haghighi_3
D08-1069	P07-1107	o	More recently Haghighi and Klein -LRB- 2007 -RRB- use the distinction between pronouns nominals and proper nouns 660 in their unsupervised generative model for coreference resolution for their model this is absolutely critical for achieving better accuracy	amod_accuracy_better dobj_achieving_accuracy prepc_for_critical_achieving advmod_critical_absolutely cop_critical_is nsubj_critical_this prep_for_critical_model poss_model_their nn_resolution_coreference amod_model_generative amod_model_unsupervised poss_model_their num_nouns_660 amod_nouns_proper conj_and_pronouns_nouns conj_and_pronouns_nominals prep_between_distinction_nouns prep_between_distinction_nominals prep_between_distinction_pronouns det_distinction_the parataxis_use_critical prep_for_use_resolution prep_in_use_model dobj_use_distinction nsubj_use_Klein nsubj_use_Haghighi advmod_use_recently appos_Klein_2007 conj_and_Haghighi_Klein advmod_recently_More
D08-1069	P07-1107	o	This therefore suggests that better parameters are likely to be learned in the 2Haghighi and Kleins -LRB- 2007 -RRB- generative coreference model mirrors this in the posterior distribution which it assigns to mention types given their salience -LRB- see their Table 1 -RRB-	nsubj_1_Table poss_Table_their ccomp_see_1 poss_salience_their pobj_given_salience prep_types_given dobj_mention_types aux_mention_to xcomp_assigns_mention nsubj_assigns_it dobj_assigns_which rcmod_distribution_assigns amod_distribution_posterior det_distribution_the prep_in_mirrors_distribution dobj_mirrors_this nn_model_coreference amod_model_generative nn_model_Kleins appos_Kleins_2007 conj_and_2Haghighi_model det_2Haghighi_the prep_in_learned_model prep_in_learned_2Haghighi auxpass_learned_be aux_learned_to dep_likely_see dep_likely_mirrors xcomp_likely_learned cop_likely_are nsubj_likely_parameters mark_likely_that amod_parameters_better ccomp_suggests_likely advmod_suggests_therefore nsubj_suggests_This
D09-1120	P07-1107	n	12Poon and Domingos -LRB- 2008 -RRB- outperformed Haghighi and Klein -LRB- 2007 -RRB-	appos_Klein_2007 conj_and_Haghighi_Klein dobj_outperformed_Klein dobj_outperformed_Haghighi nsubj_outperformed_Domingos nsubj_outperformed_12Poon appos_Domingos_2008 conj_and_12Poon_Domingos
D09-1120	P07-1107	o	1153 While much research -LRB- Ng and Cardie 2002 Culotta et al. 2007 Haghighi and Klein 2007 Poon and Domingos 2008 Finkel and Manning 2008 -RRB- has explored how to reconcile pairwise decisions to form coherent clusters we simply take the transitive closure of our pairwise decision -LRB- as in Ng and Cardie -LRB- 2002 -RRB- and Bengston and Roth -LRB- 2008 -RRB- -RRB- which can and does cause system errors	nn_errors_system dobj_cause_errors aux_cause_does conj_and_can_cause dep_which_cause dep_which_can appos_Roth_2008 appos_Cardie_2002 conj_and_Ng_Roth conj_and_Ng_Bengston conj_and_Ng_Cardie pobj_in_Roth pobj_in_Bengston pobj_in_Cardie pobj_in_Ng pcomp_as_in dep_decision_which prep_decision_as amod_decision_pairwise poss_decision_our prep_of_closure_decision amod_closure_transitive det_closure_the dobj_take_closure advmod_take_simply nsubj_take_we prep_take_1153 amod_clusters_coherent dobj_form_clusters aux_form_to amod_decisions_pairwise vmod_reconcile_form dobj_reconcile_decisions aux_reconcile_to advmod_reconcile_how ccomp_explored_reconcile aux_explored_has nsubj_explored_Poon nsubj_explored_2007 nsubj_explored_Klein nsubj_explored_Haghighi dep_Finkel_2008 conj_and_Finkel_Manning num_Poon_2008 conj_and_Poon_Domingos dep_Haghighi_Manning dep_Haghighi_Finkel conj_and_Haghighi_Domingos conj_and_Haghighi_Poon conj_and_Haghighi_2007 conj_and_Haghighi_Klein num_Culotta_2007 nn_Culotta_al. nn_Culotta_et parataxis_Ng_explored conj_and_Ng_Culotta conj_and_Ng_2002 conj_and_Ng_Cardie dep_research_Culotta dep_research_2002 dep_research_Cardie dep_research_Ng amod_research_much mark_research_While advcl_1153_research
E09-1018	P07-1107	o	The probabilities are ordered according to at least my intuition with pronoun being the most likely -LRB- 0.094 -RRB- followed by proper nouns -LRB- 0.057 -RRB- followed by common nouns -LRB- 0.032 -RRB- a fact also noted by -LRB- Haghighi and Klein 2007 -RRB-	amod_Haghighi_2007 conj_and_Haghighi_Klein dep_by_Klein dep_by_Haghighi prep_noted_by advmod_noted_also vmod_fact_noted det_fact_a appos_nouns_0.032 amod_nouns_common agent_followed_nouns appos_nouns_fact vmod_nouns_followed appos_nouns_0.057 amod_nouns_proper agent_followed_nouns dep_likely_0.094 advmod_likely_most det_likely_the cop_likely_being nsubj_likely_pronoun vmod_intuition_followed prepc_with_intuition_likely poss_intuition_my advmod_intuition_at pobj_at_least pobj_ordered_intuition prepc_according_to_ordered_to auxpass_ordered_are nsubjpass_ordered_probabilities det_probabilities_The
E09-1018	P07-1107	p	In addition their system does not classify non-anaphoric pronouns A third paper that has significantly influenced our work is that of -LRB- Haghighi and Klein 2007 -RRB-	amod_Haghighi_2007 conj_and_Haghighi_Klein dep_of_Klein dep_of_Haghighi pcomp_that_of prep_is_that poss_work_our dobj_influenced_work advmod_influenced_significantly aux_influenced_has nsubj_influenced_that rcmod_paper_influenced amod_paper_third det_paper_A appos_pronouns_paper amod_pronouns_non-anaphoric dep_classify_is dobj_classify_pronouns neg_classify_not aux_classify_does nsubj_classify_system prep_in_classify_addition poss_system_their
N09-1019	P07-1107	o	The model of Haghighi and Klein -LRB- 2007 -RRB- incorporated a latent variable for named entity class	nn_class_entity amod_class_named prep_for_variable_class amod_variable_latent det_variable_a dobj_incorporated_variable dep_incorporated_Klein appos_Klein_2007 conj_and_Haghighi_incorporated prep_of_model_incorporated prep_of_model_Haghighi det_model_The
N09-1019	P07-1107	o	5 Discussion As stated above we aim to build an unsupervised generative model for named entity clustering since such a model could be integrated with unsupervised coreference models like Haghighi and Klein -LRB- 2007 -RRB- for joint inference	amod_inference_joint appos_Klein_2007 conj_and_Haghighi_Klein prep_like_models_Klein prep_like_models_Haghighi nn_models_coreference amod_models_unsupervised prep_for_integrated_inference prep_with_integrated_models auxpass_integrated_be aux_integrated_could nsubjpass_integrated_model mark_integrated_since det_model_a predet_model_such nn_clustering_entity advcl_named_integrated dep_named_clustering prepc_for_model_named amod_model_generative amod_model_unsupervised det_model_an dobj_build_model aux_build_to xcomp_aim_build nsubj_aim_we nsubj_aim_Discussion advmod_stated_above mark_stated_As dep_Discussion_stated num_Discussion_5
N09-1019	P07-1107	o	Named entities also pose another problem with the Haghighi and Klein -LRB- 2007 -RRB- coreference model since it models only the heads of NPs it will fail to resolve some references to named entities -LRB- Ford Motor Co. Ford -RRB- while erroneously merging others -LRB- Ford Motor Co. Lockheed Martin Co. -RRB-	nn_Co._Martin nn_Co._Lockheed appos_Co._Co. nn_Co._Motor nn_Co._Ford dobj_merging_others advmod_merging_erroneously dep_merging_while dep_Co._Co. rcmod_Co._merging appos_Co._Ford nn_Co._Motor nn_Co._Ford amod_entities_named prep_to_references_entities det_references_some dobj_resolve_references aux_resolve_to dep_fail_Co. xcomp_fail_resolve aux_fail_will nsubj_fail_it prep_since_fail_it prep_of_heads_NPs det_heads_the advmod_heads_only dep_models_heads dep_it_models nn_model_coreference num_model_2007 nn_model_Klein conj_and_Haghighi_model dep_the_model dep_the_Haghighi prep_with_problem_the det_problem_another parataxis_pose_fail dobj_pose_problem advmod_pose_also nsubj_pose_entities amod_entities_Named
N09-1019	P07-1107	n	Our system improves over the latent named-entity tagging in Haghighi and Klein -LRB- 2007 -RRB- from 61 % to 87 %	num_%_87 num_%_61 appos_Klein_2007 conj_and_Haghighi_Klein prep_in_tagging_Klein prep_in_tagging_Haghighi nn_tagging_named-entity amod_tagging_latent det_tagging_the prep_to_improves_% prep_from_improves_% prep_over_improves_tagging nsubj_improves_system poss_system_Our ccomp_``_improves
N09-1019	P07-1107	o	Like Haghighi and Klein -LRB- 2007 -RRB- we give our model information about the basic types of pronouns in English	prep_in_pronouns_English prep_of_types_pronouns amod_types_basic det_types_the nn_information_model poss_information_our prep_about_give_types dobj_give_information nsubj_give_we prep_like_give_Klein prep_like_give_Haghighi appos_Klein_2007 conj_and_Haghighi_Klein
P08-1002	P07-1107	n	Secondly while most pronoun resolution evaluations simply exclude non-referential pronouns recent unsupervised approaches -LRB- Cherry and Bergsma 2005 Haghighi and Klein 2007 -RRB- must deal with all pronouns in unrestricted text and therefore need robust modules to automatically handle non-referential instances	amod_instances_non-referential dobj_handle_instances advmod_handle_automatically aux_handle_to amod_modules_robust vmod_need_handle dobj_need_modules advmod_need_therefore nsubj_need_approaches amod_text_unrestricted prep_in_pronouns_text det_pronouns_all conj_and_deal_need prep_with_deal_pronouns aux_deal_must nsubj_deal_approaches advcl_deal_exclude advmod_deal_Secondly amod_Haghighi_2007 conj_and_Haghighi_Klein dep_Cherry_Klein dep_Cherry_Haghighi conj_and_Cherry_2005 conj_and_Cherry_Bergsma appos_approaches_2005 appos_approaches_Bergsma appos_approaches_Cherry amod_approaches_unsupervised amod_approaches_recent amod_pronouns_non-referential dobj_exclude_pronouns advmod_exclude_simply nsubj_exclude_evaluations mark_exclude_while nn_evaluations_resolution nn_evaluations_pronoun amod_evaluations_most
W09-0210	P07-1107	p	In terms of applying non-parametric Bayesian approaches to NLP Haghighi and Klein -LRB- 2007 -RRB- evaluated the clustering properties of DPMMs by performing anaphora resolution with good results	amod_results_good nn_resolution_anaphora prep_with_performing_results dobj_performing_resolution prep_of_properties_DPMMs nn_properties_clustering det_properties_the prepc_by_evaluated_performing dobj_evaluated_properties prep_in_evaluated_terms appos_Klein_2007 conj_and_NLP_Klein conj_and_NLP_Haghighi amod_approaches_Bayesian amod_approaches_non-parametric prep_to_applying_Klein prep_to_applying_Haghighi prep_to_applying_NLP dobj_applying_approaches prepc_of_terms_applying
W09-0210	P07-1107	p	Recent work has applied Bayesian non-parametric models to anaphora resolution -LRB- Haghighi and Klein 2007 -RRB- lexical acquisition -LRB- Goldwater 2007 -RRB- and language modeling -LRB- Teh 2006 -RRB- with good results	amod_results_good appos_Teh_2006 dep_modeling_Teh nn_modeling_language dep_Goldwater_2007 prep_with_acquisition_results conj_and_acquisition_modeling appos_acquisition_Goldwater amod_acquisition_lexical amod_Haghighi_2007 conj_and_Haghighi_Klein dep_resolution_Klein dep_resolution_Haghighi nn_resolution_anaphora amod_models_non-parametric amod_models_Bayesian dobj_applied_modeling dobj_applied_acquisition prep_to_applied_resolution dobj_applied_models aux_applied_has nsubj_applied_work amod_work_Recent ccomp_``_applied
E09-1070	P08-1001	o	Richman and Schone -LRB- 2008 -RRB- used a method similar to Nothman et al.	dep_Nothman_al. nn_Nothman_et prep_to_similar_Nothman amod_method_similar det_method_a dobj_used_method nsubj_used_Schone nsubj_used_Richman appos_Schone_2008 conj_and_Richman_Schone
N09-1032	P08-1001	o	5.2.1 Generate English Annotated Corpus from Wikipedia Wikipedia provides a variety of data resources for NER and other NLP research -LRB- Richman and Schone 2008 -RRB-	amod_Richman_2008 conj_and_Richman_Schone nn_research_NLP amod_research_other dep_NER_Schone dep_NER_Richman conj_and_NER_research nn_resources_data prep_of_variety_resources det_variety_a prep_for_provides_research prep_for_provides_NER dobj_provides_variety nsubj_provides_Corpus nn_Wikipedia_Wikipedia prep_from_Corpus_Wikipedia nn_Corpus_Annotated nn_Corpus_English nn_Corpus_Generate num_Corpus_5.2.1
D09-1101	P08-1002	o	-LRB- 2008 -RRB- -RRB- and distributional methods -LRB- e.g. Bergsma et al.	nn_al._et nn_al._Bergsma dep_al._e.g. dep_methods_al. amod_methods_distributional conj_and_2008_methods dep_''_methods dep_''_2008
D09-1102	P08-1002	o	More recently the problem has been tackled using statistics-based -LRB- e.g. Bean and Riloff 1999 Bergsma et al 2008 -RRB- and learning-based -LRB- e.g. Evans 2001 Ng and Cardie 2002a Ng 2004 Yang et al 2005 Denis and Balbridge 2007 -RRB- methods	nn_methods_e.g. amod_methods_learning-based num_Balbridge_2007 dep_Yang_2005 dep_Yang_al nn_Yang_et num_Ng_2004 nn_2002a_Cardie nn_2002a_Ng conj_and_Ng_Cardie conj_and_Evans_Balbridge conj_and_Evans_Denis conj_and_Evans_Yang conj_and_Evans_Ng conj_and_Evans_2002a num_Evans_2001 dep_e.g._Balbridge dep_e.g._Denis dep_e.g._Yang dep_e.g._Ng dep_e.g._2002a dep_e.g._Evans num_al_2008 nn_al_et nn_al_Bergsma num_Riloff_1999 conj_and_Bean_Riloff dep_e.g._al dep_e.g._Riloff dep_e.g._Bean conj_and_statistics-based_methods dep_statistics-based_e.g. dobj_using_methods dobj_using_statistics-based xcomp_tackled_using auxpass_tackled_been aux_tackled_has nsubjpass_tackled_problem advmod_tackled_recently det_problem_the advmod_recently_More
D09-1102	P08-1002	o	2 Related Work Given its potential usefulness in coreference resolution anaphoricity determination has been studied fairly extensively in the literature and can be classified into three categories heuristic rule-based -LRB- e.g. Paice and Husk 1987 Lappin and Leass 1994 Kennedy and Boguraev 1996 Denber 1998 Vieira and Poesio 2000 -RRB- statistics-based -LRB- e.g. Bean and Riloff 1999 Cherry and Bergsma 2005 Bergsma et al 2008 -RRB- and learning-based -LRB- e.g. Evans 2001 Ng and Cardie 2002a Ng 2004 Yang et al 2005 Denis and Balbridge 2007 -RRB-	num_Balbridge_2007 dep_Yang_2005 dep_Yang_al nn_Yang_et num_Ng_2004 nn_2002a_Cardie nn_2002a_Ng conj_and_Ng_Cardie conj_and_Evans_Balbridge conj_and_Evans_Denis conj_and_Evans_Yang conj_and_Evans_Ng conj_and_Evans_2002a num_Evans_2001 pobj_e.g._Balbridge pobj_e.g._Denis pobj_e.g._Yang pobj_e.g._Ng pobj_e.g._2002a pobj_e.g._Evans prep_-LRB-_e.g. num_al_2008 nn_al_et nn_al_Bergsma num_Bergsma_2005 conj_and_Cherry_Bergsma num_Riloff_1999 conj_and_Bean_Riloff dep_e.g._al dep_e.g._Bergsma dep_e.g._Cherry dep_e.g._Riloff dep_e.g._Bean dep_statistics-based_e.g. num_Poesio_2000 conj_and_Vieira_Poesio num_Denber_1998 num_Boguraev_1996 conj_and_Kennedy_Boguraev num_Leass_1994 conj_and_Lappin_Leass num_Husk_1987 dep_Paice_Poesio dep_Paice_Vieira dep_Paice_Denber dep_Paice_Boguraev dep_Paice_Kennedy dep_Paice_Leass dep_Paice_Lappin conj_and_Paice_Husk nn_Paice_e.g. dep_rule-based_Husk dep_rule-based_Paice conj_and_heuristic_learning-based conj_and_heuristic_statistics-based amod_heuristic_rule-based num_categories_three prep_into_classified_categories auxpass_classified_be aux_classified_can nsubjpass_classified_determination det_literature_the advmod_extensively_fairly dep_studied_learning-based dep_studied_statistics-based dep_studied_heuristic conj_and_studied_classified prep_in_studied_literature advmod_studied_extensively auxpass_studied_been aux_studied_has nsubjpass_studied_determination dep_studied_Work nn_determination_anaphoricity nn_resolution_coreference prep_in_usefulness_resolution amod_usefulness_potential poss_usefulness_its pobj_Given_usefulness prep_Work_Given amod_Work_Related num_Work_2 advcl_``_classified advcl_``_studied
D09-1102	P08-1002	o	Bergsma et al -LRB- 2008 -RRB- proposed a distributional method in detecting non-anaphoric pronouns by first extracting the surrounding textual context of the pronoun then gathering the distribution of words that occurred within that context from a large corpus and finally learning to classify these distributions as representing either anaphoric and non-anaphoric pronoun instances	nn_instances_pronoun amod_instances_non-anaphoric amod_instances_anaphoric conj_and_anaphoric_non-anaphoric preconj_anaphoric_either dobj_representing_instances det_distributions_these prepc_as_classify_representing dobj_classify_distributions aux_classify_to xcomp_learning_classify advmod_learning_finally amod_corpus_large det_corpus_a conj_and_from_learning pobj_from_corpus det_context_that prep_occurred_learning prep_occurred_from prep_within_occurred_context nsubj_occurred_that rcmod_words_occurred prep_of_distribution_words det_distribution_the dobj_gathering_distribution advmod_gathering_then det_pronoun_the prep_of_context_pronoun amod_context_textual amod_context_surrounding det_context_the dobj_extracting_context dep_first_extracting amod_pronouns_non-anaphoric prep_by_detecting_first dobj_detecting_pronouns prepc_in_method_detecting amod_method_distributional det_method_a vmod_proposed_gathering dobj_proposed_method nsubj_proposed_al appos_al_2008 nn_al_et nn_al_Bergsma
N09-1065	P08-1002	o	-LRB- 2008 -RRB- -RSB- and distributional methods -LSB- e.g. Bergsma et al.	nn_al._et nn_al._Bergsma dep_e.g._al. dep_methods_e.g. amod_methods_distributional conj_and_2008_methods dep_''_methods dep_''_2008
D08-1002	P08-1004	o	Instead of analyzing sentences directly AUCONTRAIRE relies on the TEXTRUNNER Open Information Extraction system -LRB- Banko et al. 2007 Banko and Etzioni 2008 -RRB- to map each sentence to one or more tuples that represent the entities in the sentences and the relationships between them -LRB- e.g. was born in -LRB- Mozart Salzburg -RRB- -RRB-	appos_Mozart_Salzburg prep_in_born_Mozart auxpass_born_was nsubjpass_born_e.g. dobj_born_them prepc_between_relationships_born det_relationships_the det_sentences_the det_entities_the conj_and_represent_relationships prep_in_represent_sentences dobj_represent_entities nsubj_represent_that rcmod_tuples_relationships rcmod_tuples_represent num_tuples_more num_tuples_one conj_or_one_more det_sentence_each prep_to_map_tuples dobj_map_sentence aux_map_to dep_Banko_2008 conj_and_Banko_Etzioni dep_Banko_Etzioni dep_Banko_Banko appos_Banko_2007 dep_Banko_al. nn_Banko_et nn_system_Extraction nn_system_Information nn_system_Open nn_system_TEXTRUNNER det_system_the xcomp_relies_map dep_relies_Banko prep_on_relies_system nsubj_relies_AUCONTRAIRE prepc_instead_of_relies_analyzing advmod_analyzing_directly dobj_analyzing_sentences
D09-1152	P08-1004	o	Recent research in open information extraction -LRB- Banko and Etzioni 2008 Davidov and Rappaport 2008 -RRB- has shown that we can extract large amounts of relational data from open-domain text with high accuracy	amod_accuracy_high amod_text_open-domain prep_from_data_text amod_data_relational prep_of_amounts_data amod_amounts_large prep_with_extract_accuracy dobj_extract_amounts aux_extract_can nsubj_extract_we mark_extract_that ccomp_shown_extract aux_shown_has nsubj_shown_research dep_Davidov_2008 conj_and_Davidov_Rappaport dep_Banko_Rappaport dep_Banko_Davidov conj_and_Banko_2008 conj_and_Banko_Etzioni appos_extraction_2008 appos_extraction_Etzioni appos_extraction_Banko nn_extraction_information amod_extraction_open prep_in_research_extraction amod_research_Recent
E09-1073	P08-1004	o	1 Introduction Motivation Sharing basic intuitions and longterm goals with other tasks within the area of Webbased information extraction -LRB- Banko and Etzioni 2008 Davidov and Rappoport 2008 -RRB- the task of acquiring class attributes relies on unstructured text available on the Web as a data source for extracting generally-useful knowledge	amod_knowledge_generally-useful dobj_extracting_knowledge prepc_for_source_extracting nn_source_data det_source_a det_Web_the prep_on_available_Web amod_text_available amod_text_unstructured prep_as_relies_source prep_on_relies_text nsubj_relies_task vmod_relies_Sharing nn_attributes_class dobj_acquiring_attributes prepc_of_task_acquiring det_task_the dep_Davidov_2008 conj_and_Davidov_Rappoport dep_Banko_Rappoport dep_Banko_Davidov conj_and_Banko_2008 conj_and_Banko_Etzioni appos_extraction_2008 appos_extraction_Etzioni appos_extraction_Banko nn_extraction_information amod_extraction_Webbased prep_of_area_extraction det_area_the prep_within_tasks_area amod_tasks_other amod_goals_longterm conj_and_intuitions_goals amod_intuitions_basic prep_with_Sharing_tasks dobj_Sharing_goals dobj_Sharing_intuitions dep_Motivation_relies nn_Motivation_Introduction num_Motivation_1 dep_``_Motivation
P09-1114	P08-1004	o	Banko and Etzioni -LRB- 2008 -RRB- studied open domain relation extraction for which they manually identified several common relation patterns	nn_patterns_relation amod_patterns_common amod_patterns_several dobj_identified_patterns advmod_identified_manually nsubj_identified_they prep_for_identified_which nn_extraction_relation nn_extraction_domain amod_extraction_open amod_extraction_studied nn_extraction_Etzioni appos_Etzioni_2008 rcmod_Banko_identified conj_and_Banko_extraction
P09-1093	P08-1035	p	For Japanese dependency trees are trimmed instead of full parse trees -LRB- Takeuchi and Matsumoto 2001 Oguro et al. 2002 Nomoto 2008 -RRB- 1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter we refer these compression processes as tree trimming input is grammatical but it offers only moderate compression rates	nn_rates_compression amod_rates_moderate advmod_rates_only dobj_offers_rates nsubj_offers_it cop_grammatical_is nsubj_grammatical_input amod_tree_trimming prep_as_processes_tree nsubj_processes_compression det_compression_these ccomp_refer_processes nsubj_refer_we nsubj_refer_Hereafter mark_refer_if num_Hereafter_1 det_Hereafter_the advcl_grammatical_refer cop_grammatical_is nsubj_grammatical_output mark_grammatical_because amod_output_compressed det_output_the conj_but_reasonable_offers parataxis_reasonable_grammatical advcl_reasonable_grammatical cop_reasonable_is csubj_reasonable_trimmed nn_approach_parsing det_approach_This num_approach_1 dep_Nomoto_2008 num_Oguro_2002 nn_Oguro_al. nn_Oguro_et dep_Takeuchi_Nomoto conj_and_Takeuchi_Oguro conj_and_Takeuchi_2001 conj_and_Takeuchi_Matsumoto dep_trees_approach dep_trees_Oguro dep_trees_2001 dep_trees_Matsumoto dep_trees_Takeuchi nn_trees_parse amod_trees_full prep_instead_of_trimmed_trees auxpass_trimmed_are nsubjpass_trimmed_trees advmod_trimmed_Japanese mark_trimmed_For nn_trees_dependency
P09-1093	P08-1035	o	For Japanese sentences instead of using full parse trees existing sentence compression methods trim dependency trees by the discriminative model -LRB- Takeuchi and Matsumoto 2001 Nomoto 2008 -RRB- through the use of simple linear combined features -LRB- Oguro et al. 2002 -RRB-	amod_Oguro_2002 dep_Oguro_al. nn_Oguro_et dep_features_Oguro amod_features_combined amod_features_linear amod_features_simple prep_of_use_features det_use_the dep_Nomoto_2008 dep_Takeuchi_Nomoto dep_Takeuchi_2001 conj_and_Takeuchi_Matsumoto appos_model_Matsumoto appos_model_Takeuchi amod_model_discriminative det_model_the prep_through_trees_use prep_by_trees_model nn_trees_dependency amod_trees_trim nn_trees_methods nn_trees_compression nn_trees_sentence amod_trees_existing ccomp_,_trees nn_trees_parse amod_trees_full dobj_using_trees pcomp_of_using advmod_of_instead ccomp_,_of amod_sentences_Japanese pobj_For_sentences dep_``_For
D08-1058	P08-1036	o	2005 Choi et al. 2006 Ku et al. 2006 Titov and McDonald 2008 -RRB-	num_Ku_2006 nn_Ku_al. nn_Ku_et num_Choi_2006 nn_Choi_al. nn_Choi_et appos_2005_2008 conj_and_2005_McDonald conj_and_2005_Titov conj_and_2005_Ku conj_and_2005_Choi ccomp_``_McDonald ccomp_``_Titov ccomp_``_Ku ccomp_``_Choi ccomp_``_2005
D09-1017	P08-1036	o	Specifically aspect rating as an interesting topic has also been widely studied -LRB- Titov and McDonald 2008a Snyder and Barzilay 2007 Goldberg and Zhu 2006 -RRB-	amod_Snyder_2006 conj_and_Snyder_Zhu conj_and_Snyder_Goldberg conj_and_Snyder_2007 conj_and_Snyder_Barzilay dep_Titov_Zhu dep_Titov_Goldberg dep_Titov_2007 dep_Titov_Barzilay dep_Titov_Snyder conj_and_Titov_2008a conj_and_Titov_McDonald dep_studied_2008a dep_studied_McDonald dep_studied_Titov advmod_studied_widely auxpass_studied_been advmod_studied_also aux_studied_has nsubjpass_studied_rating advmod_studied_Specifically amod_topic_interesting det_topic_an prep_as_rating_topic nn_rating_aspect
D09-1017	P08-1036	o	Titov and McDonald -LRB- 2008b -RRB- proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects and builds a set of sentiment predictors	nn_predictors_sentiment prep_of_set_predictors det_set_a dobj_builds_set nsubj_builds_Titov amod_aspects_ratable prep_of_representative_aspects cop_representative_are nsubj_representative_that rcmod_topics_representative dobj_build_topics aux_build_to vmod_model_build nn_model_topic nn_model_LDA amod_model_modified det_model_a dobj_utilizes_model nsubj_utilizes_which nn_ratings_aspect nn_ratings_text conj_and_text_aspect rcmod_model_utilizes prep_of_model_ratings amod_model_joint det_model_a conj_and_proposed_builds dobj_proposed_model nsubj_proposed_McDonald nsubj_proposed_Titov appos_McDonald_2008b conj_and_Titov_McDonald
D09-1019	P08-1036	o	Several researchers also studied feature/topicbased sentiment analysis -LRB- e.g. Hu and Liu 2004 Popescu and Etzioni 2005 Ku et al 2006 Carenini et al 2006 Mei et al 2007 Ding Liu and Yu 2008 Titov and R. McDonald 2008 Stoyanov and Cardie 2008 Lu and Zhai 2008 -RRB-	nn_McDonald_R. appos_al_2007 nn_al_et nn_al_Mei appos_al_2006 nn_al_et nn_al_Carenini amod_al_2006 nn_al_et nn_al_Ku dep_Popescu_2008 conj_and_Popescu_Zhai conj_and_Popescu_Lu conj_and_Popescu_2008 conj_and_Popescu_Cardie conj_and_Popescu_Stoyanov conj_and_Popescu_2008 conj_and_Popescu_McDonald conj_and_Popescu_Titov conj_and_Popescu_2008 conj_and_Popescu_Yu conj_and_Popescu_Liu conj_and_Popescu_Ding conj_and_Popescu_al conj_and_Popescu_al conj_and_Popescu_al conj_and_Popescu_2005 conj_and_Popescu_Etzioni dep_Hu_Zhai dep_Hu_Lu dep_Hu_2008 dep_Hu_Cardie dep_Hu_Stoyanov dep_Hu_2008 dep_Hu_McDonald dep_Hu_Titov dep_Hu_2008 dep_Hu_Yu dep_Hu_Liu dep_Hu_Ding dep_Hu_al dep_Hu_al dep_Hu_al dep_Hu_2005 dep_Hu_Etzioni dep_Hu_Popescu num_Hu_2004 conj_and_Hu_Liu dep_e.g._Liu dep_e.g._Hu ccomp_-LRB-_e.g. nn_analysis_sentiment amod_analysis_feature/topicbased dobj_studied_analysis advmod_studied_also nsubj_studied_researchers amod_researchers_Several ccomp_``_studied
E09-1059	P08-1036	o	For example aspects of a digital camera could include picture quality battery life size color value etc. Finding such aspects is a challenging research problem that has been addressed in a number of ways -LRB- Hu and Liu 2004b Gamon et al. 2005 Carenini et al. 2005 Zhuang et al. 2006 Branavan et al. 2008 Blair-Goldensohn et al. 2008 Titov and McDonald 2008b Titov and McDonald 2008a -RRB-	appos_Titov_2008a conj_and_Titov_McDonald conj_and_Titov_2008b conj_and_Titov_McDonald num_Blair-Goldensohn_2008 nn_Blair-Goldensohn_al. nn_Blair-Goldensohn_et num_Branavan_2008 nn_Branavan_al. nn_Branavan_et num_Zhuang_2006 nn_Zhuang_al. nn_Zhuang_et num_Carenini_2005 nn_Carenini_al. nn_Carenini_et num_Gamon_2005 nn_Gamon_al. nn_Gamon_et dep_Hu_McDonald dep_Hu_Titov conj_and_Hu_2008b conj_and_Hu_McDonald conj_and_Hu_Titov conj_and_Hu_Blair-Goldensohn conj_and_Hu_Branavan conj_and_Hu_Zhuang conj_and_Hu_Carenini conj_and_Hu_Gamon conj_and_Hu_2004b conj_and_Hu_Liu dep_ways_Titov dep_ways_Blair-Goldensohn dep_ways_Branavan dep_ways_Zhuang dep_ways_Carenini dep_ways_Gamon dep_ways_2004b dep_ways_Liu dep_ways_Hu prep_of_number_ways det_number_a prep_in_addressed_number auxpass_addressed_been aux_addressed_has nsubjpass_addressed_that rcmod_problem_addressed nn_problem_research amod_problem_challenging det_problem_a cop_problem_is csubj_problem_Finding amod_aspects_such dobj_Finding_aspects nn_life_battery conj_quality_etc. conj_quality_value conj_quality_color conj_quality_size conj_quality_life nn_quality_picture dep_include_problem dobj_include_quality aux_include_could nsubj_include_aspects prep_for_include_example amod_camera_digital det_camera_a prep_of_aspects_camera
P09-1027	P08-1036	o	In recent years sentiment classification has drawn much attention in the NLP field and it has many useful applications such as opinion mining and summarization -LRB- Liu et al. 2005 Ku et al. 2006 Titov and McDonald 2008 -RRB-	appos_Titov_2008 conj_and_Titov_McDonald nn_al._et nn_al._Ku dep_Liu_McDonald dep_Liu_Titov appos_Liu_2006 dep_Liu_al. appos_Liu_2005 dep_Liu_al. nn_Liu_et dep_mining_Liu conj_and_mining_summarization nn_mining_opinion prep_such_as_applications_summarization prep_such_as_applications_mining amod_applications_useful amod_applications_many dobj_has_applications nsubj_has_it nn_field_NLP det_field_the amod_attention_much conj_and_drawn_has prep_in_drawn_field dobj_drawn_attention aux_drawn_has nsubj_drawn_classification prep_in_drawn_years nn_classification_sentiment amod_years_recent
P09-2043	P08-1036	o	Aspect-based sentiment analysis summarizes sentiments with diverse attributes so that customers may have to look more closely into analyzed sentiments -LRB- Titov and McDonald 2008 -RRB-	amod_Titov_2008 conj_and_Titov_McDonald dep_sentiments_McDonald dep_sentiments_Titov amod_sentiments_analyzed advmod_closely_more prep_into_look_sentiments advmod_look_closely aux_look_to xcomp_have_look aux_have_may nsubj_have_customers mark_have_that advmod_have_so amod_attributes_diverse prep_with_sentiments_attributes advcl_summarizes_have dobj_summarizes_sentiments nsubj_summarizes_analysis nn_analysis_sentiment amod_analysis_Aspect-based
D08-1037	P08-1045	o	Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words such as machine translation and multilingual information retrieval -LRB- Klementiev and Roth 2006b Hermjakob et al. 2008 -RRB-	num_Hermjakob_2008 nn_Hermjakob_al. nn_Hermjakob_et dep_Klementiev_Hermjakob appos_Klementiev_2006b conj_and_Klementiev_Roth nn_retrieval_information amod_retrieval_multilingual dep_translation_Roth dep_translation_Klementiev conj_and_translation_retrieval nn_translation_machine prep_such_as_words_retrieval prep_such_as_words_translation amod_words_out-of-vocabulary dobj_identifying_words xcomp_require_identifying nsubj_require_which rcmod_applications_require amod_applications_linguistic amod_applications_many prep_in_component_applications amod_component_important det_component_an cop_component_is nsubj_component_pairs nn_pairs_transliteration amod_pairs_Identifying
D09-1024	P08-1045	o	We finally also include as alignment candidates those word pairs that are transliterations of each other to cover rare proper names -LRB- Hermjakob et al. 2008 -RRB- which is important for language pairs that dont share the same alphabet such as Arabic and English	conj_and_Arabic_English prep_such_as_alphabet_English prep_such_as_alphabet_Arabic amod_alphabet_same det_alphabet_the dobj_dont_alphabet dobj_dont_share nsubj_dont_that rcmod_pairs_dont nn_pairs_language prep_for_important_pairs cop_important_is nsubj_important_which amod_Hermjakob_2008 dep_Hermjakob_al. nn_Hermjakob_et amod_names_proper amod_names_rare dobj_cover_names aux_cover_to det_other_each vmod_transliterations_cover prep_of_transliterations_other cop_transliterations_are nsubj_transliterations_that rcmod_pairs_important appos_pairs_Hermjakob rcmod_pairs_transliterations nn_pairs_word det_pairs_those dep_candidates_pairs nn_candidates_alignment prep_as_include_candidates advmod_include_also advmod_include_finally nsubj_include_We
E09-1050	P08-1045	o	Identification of Terms To-be Transliterated -LRB- TTT -RRB- must not be confused with recognition of Named Entities -LRB- NE -RRB- -LRB- Hermjakob et al. 2008 -RRB-	amod_Hermjakob_2008 dep_Hermjakob_al. nn_Hermjakob_et appos_Entities_NE amod_Entities_Named prep_of_recognition_Entities dep_confused_Hermjakob prep_with_confused_recognition auxpass_confused_be neg_confused_not aux_confused_must nsubjpass_confused_Transliterated appos_Transliterated_TTT nn_Transliterated_To-be rcmod_Terms_confused prep_of_Identification_Terms
N09-1005	P08-1045	o	There are many techniques for transliteration and back-transliteration and they vary along a number of dimensions phoneme substitution vs. character substitution heuristic vs. generative vs. discriminative models manual vs. automatic knowledge acquisition We explore the third dimension where we see several techniques in use Manually-constructed transliteration models e.g. -LRB- Hermjakob et al. 2008 -RRB-	amod_Hermjakob_2008 dep_Hermjakob_al. nn_Hermjakob_et appos_e.g._Hermjakob prep_models_e.g. nn_models_transliteration amod_models_Manually-constructed dep_techniques_models prep_in_techniques_use amod_techniques_several dobj_see_techniques nsubj_see_we advmod_see_where rcmod_dimension_see amod_dimension_third det_dimension_the dobj_explore_dimension nsubj_explore_We rcmod_acquisition_explore nn_acquisition_knowledge amod_acquisition_automatic amod_acquisition_manual conj_vs._manual_automatic amod_models_discriminative amod_models_generative conj_vs._generative_discriminative nn_heuristic_substitution nn_heuristic_character dep_substitution_acquisition conj_vs._substitution_models conj_vs._substitution_heuristic nn_substitution_phoneme prep_of_number_dimensions det_number_a prep_along_vary_number nsubj_vary_they conj_and_transliteration_back-transliteration prep_for_techniques_back-transliteration prep_for_techniques_transliteration amod_techniques_many dep_are_models dep_are_heuristic dep_are_substitution conj_and_are_vary nsubj_are_techniques expl_are_There ccomp_``_vary ccomp_``_are
N09-1034	P08-1045	o	Automatic NE transliteration is an important component in many cross-language applications such as Cross-Lingual Information Retrieval -LRB- CLIR -RRB- and Machine Translation -LRB- MT -RRB- -LRB- Hermjakob et al. 2008 Klementiev and Roth 2006a Meng et al. 2001 Knight and Graehl 1998 -RRB-	num_Meng_2001 nn_Meng_al. nn_Meng_et amod_Klementiev_1998 conj_and_Klementiev_Graehl conj_and_Klementiev_Knight conj_and_Klementiev_Meng conj_and_Klementiev_2006a conj_and_Klementiev_Roth dep_Hermjakob_Graehl dep_Hermjakob_Knight dep_Hermjakob_Meng dep_Hermjakob_2006a dep_Hermjakob_Roth dep_Hermjakob_Klementiev appos_Hermjakob_2008 dep_Hermjakob_al. nn_Hermjakob_et dep_Translation_Hermjakob appos_Translation_MT nn_Translation_Machine conj_and_Retrieval_Translation appos_Retrieval_CLIR nn_Retrieval_Information nn_Retrieval_Cross-Lingual prep_such_as_applications_Translation prep_such_as_applications_Retrieval amod_applications_cross-language amod_applications_many prep_in_component_applications amod_component_important det_component_an cop_component_is nsubj_component_transliteration nn_transliteration_NE nn_transliteration_Automatic
C08-1051	P08-1047	o	Furthermore recent studies revealed that word clustering is useful for semi-supervised learning in NLP -LRB- Miller et al. 2004 Li and McCallum 2005 Kazama and Torisawa 2008 Koo et al. 2008 -RRB-	num_Koo_2008 nn_Koo_al. nn_Koo_et dep_Li_Koo num_Li_2008 conj_and_Li_Torisawa conj_and_Li_Kazama conj_and_Li_2005 conj_and_Li_McCallum dep_Miller_Torisawa dep_Miller_Kazama dep_Miller_2005 dep_Miller_McCallum dep_Miller_Li appos_Miller_2004 dep_Miller_al. nn_Miller_et prep_in_learning_NLP amod_learning_semi-supervised dep_useful_Miller prep_for_useful_learning cop_useful_is nsubj_useful_clustering mark_useful_that nn_clustering_word ccomp_revealed_useful nsubj_revealed_studies advmod_revealed_Furthermore amod_studies_recent
D08-1056	P08-1052	o	-LRB- 2006 -RRB- and Nakov and Hearst -LRB- 2008 -RRB- among others look at using a large amount of unlabeled data to classify relations between words	prep_between_relations_words dobj_classify_relations aux_classify_to amod_data_unlabeled prep_of_amount_data amod_amount_large det_amount_a vmod_using_classify dobj_using_amount prepc_at_look_using appos_Hearst_2008 conj_and_Nakov_Hearst vmod_2006_look prep_among_2006_others conj_and_2006_Hearst conj_and_2006_Nakov dep_''_Nakov dep_''_2006
D08-1056	P08-1052	o	-LRB- Snow et al. 2006 Nakov & Hearst 2008 -RRB-	appos_Nakov_2008 conj_and_Nakov_Hearst dep_Snow_Hearst dep_Snow_Nakov appos_Snow_2006 dep_Snow_al. nn_Snow_et dep_''_Snow
N09-1059	P08-1052	p	Nakov and Hearst -LRB- 2008 -RRB- solved relational similarity problems using the Web as a corpus	det_corpus_a det_Web_the prep_as_using_corpus dobj_using_Web nn_problems_similarity amod_problems_relational xcomp_solved_using dobj_solved_problems nsubj_solved_Hearst nsubj_solved_Nakov appos_Hearst_2008 conj_and_Nakov_Hearst
N09-1059	P08-1052	o	-LRB- Nakov and Hearst 2005 Gledson and Keane 2008 -RRB- -RRB-	appos_Gledson_2008 conj_and_Gledson_Keane dep_Nakov_Keane dep_Nakov_Gledson conj_and_Nakov_2005 conj_and_Nakov_Hearst dep_''_2005 dep_''_Hearst dep_''_Nakov
W09-2415	P08-1052	o	The patterns will be manually constructed following the approach of Hearst -LRB- 1992 -RRB- and Nakov and Hearst -LRB- 2008 -RRB- .6 The example collection for each relation R will be passed to two independent annotators	amod_annotators_independent num_annotators_two prep_to_passed_annotators auxpass_passed_be aux_passed_will csubjpass_passed_constructed nn_R_relation det_R_each nn_collection_example det_collection_The num_collection_.6 nn_collection_Hearst appos_Hearst_2008 conj_and_Nakov_collection conj_and_Hearst_collection conj_and_Hearst_Nakov appos_Hearst_1992 prep_for_approach_R prep_of_approach_Nakov prep_of_approach_Hearst det_approach_the prep_following_constructed_approach advmod_constructed_manually auxpass_constructed_be aux_constructed_will nsubjpass_constructed_patterns det_patterns_The
W09-2415	P08-1052	o	They propose a two-level hierarchy with 5 classes at the first level and 30 classes at the second one other researchers -LRB- Kim and Baldwin 2005 Nakov and Hearst 2008 Nastase et al. 2006 Turney 2005 Turney and Littman 2005 -RRB- have used their class scheme and data set	nn_set_data conj_and_scheme_set nn_scheme_class poss_scheme_their dobj_used_set dobj_used_scheme aux_used_have nsubj_used_researchers dep_Turney_2005 conj_and_Turney_Littman num_Turney_2005 num_Nastase_2006 nn_Nastase_al. nn_Nastase_et conj_and_Nakov_2008 conj_and_Nakov_Hearst dep_Kim_Littman dep_Kim_Turney conj_and_Kim_Turney conj_and_Kim_Nastase conj_and_Kim_2008 conj_and_Kim_Hearst conj_and_Kim_Nakov conj_and_Kim_2005 conj_and_Kim_Baldwin appos_researchers_Turney appos_researchers_Nastase appos_researchers_Nakov appos_researchers_2005 appos_researchers_Baldwin appos_researchers_Kim amod_researchers_other amod_one_second det_one_the num_classes_30 prep_at_level_one conj_and_level_classes amod_level_first det_level_the num_classes_5 amod_hierarchy_two-level det_hierarchy_a parataxis_propose_used prep_at_propose_classes prep_at_propose_level prep_with_propose_classes dobj_propose_hierarchy nsubj_propose_They
W09-2415	P08-1052	o	As a first step SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification it has also motivated followup research -LRB- Davidov and Rappoport 2008 KatrenkoandAdriaans 2008 NakovandHearst 2008 O Seaghdha and Copestake 2008 -RRB-	dep_Seaghdha_2008 conj_and_Seaghdha_Copestake nn_Seaghdha_O dep_NakovandHearst_Copestake dep_NakovandHearst_Seaghdha num_NakovandHearst_2008 num_KatrenkoandAdriaans_2008 dep_Davidov_NakovandHearst conj_and_Davidov_KatrenkoandAdriaans conj_and_Davidov_2008 conj_and_Davidov_Rappoport dep_research_KatrenkoandAdriaans dep_research_2008 dep_research_Rappoport dep_research_Davidov amod_research_followup dobj_motivated_research advmod_motivated_also aux_motivated_has nsubj_motivated_it nn_classification_relation amod_classification_semantic prep_to_approaches_classification amod_approaches_different prep_of_performance_approaches det_performance_the amod_insights_useful amod_insights_many prep_into_offered_performance dobj_offered_insights nsubj_offered_Task num_Task_4 parataxis_SemEval2007_motivated rcmod_SemEval2007_offered dep_,_SemEval2007 amod_step_first det_step_a pobj_As_step dep_``_As
W09-2416	P08-1052	o	Pearsons correlation coefficient is a standard measure of the correlation strength between two distributions it can be calculated as follows = E -LRB- XY -RRB- E -LRB- X -RRB- E -LRB- Y -RRB- radicalbigE -LRB- X2 -RRB- -LSB- E -LRB- X -RRB- -RSB- 2radicalbigE -LRB- Y 2 -RRB- -LSB- E -LRB- Y -RRB- -RSB- 2 -LRB- 1 -RRB- where X = -LRB- x1 xn -RRB- and Y = -LRB- y1 yn -RRB- are vectors of numerical scores for each paraphrase provided by the humans and the competing systems respectively n is the number of paraphrases to score and E -LRB- X -RRB- is the expectation of X. Cosine correlation coefficient is another popular alternative and was used by Nakov and Hearst -LRB- 2008 -RRB- it can be seen as an uncentered version of Pearsons correlation coefficient = X.YbardblXbardblbardblYbardbl -LRB- 2 -RRB- Spearmans rank correlation coefficient is suitable for comparing rankings of sets of items it is a special case of Pearsons correlation derived by considering rank indices -LRB- 1,2 -RRB- as item scores It is defined as follows = n summationtextx iyi -LRB- summationtextx i -RRB- -LRB- summationtexty i -RRB- radicalBig nsummationtextx2i -LRB- summationtextxi -RRB- 2 radicalBig nsummationtexty2i -LRB- summationtextyi -RRB- 2 -LRB- 3 -RRB- One problem with using Spearmans rank coefficient for the current task is the assumption that swapping any two ranks has the same effect	amod_effect_same det_effect_the dobj_has_effect csubj_has_swapping mark_has_that num_ranks_two det_ranks_any dobj_swapping_ranks ccomp_assumption_has det_assumption_the cop_assumption_is nsubj_assumption_nsummationtextx2i tmod_assumption_iyi amod_task_current det_task_the nn_coefficient_rank amod_coefficient_Spearmans prep_for_using_task dobj_using_coefficient num_problem_One dep_problem_3 npadvmod_nsummationtexty2i_problem num_nsummationtexty2i_2 appos_nsummationtexty2i_summationtextyi nn_nsummationtexty2i_radicalBig num_nsummationtexty2i_2 prepc_with_nsummationtextx2i_using dep_nsummationtextx2i_nsummationtexty2i appos_nsummationtextx2i_summationtextxi nn_nsummationtextx2i_radicalBig dep_nsummationtextx2i_i amod_nsummationtextx2i_summationtexty nn_nsummationtextx2i_summationtextx dep_summationtextx_i nn_iyi_summationtextx nn_iyi_n dep_=_assumption mark_follows_as dep_defined_= advcl_defined_follows auxpass_defined_is nsubjpass_defined_It nn_scores_item dep_indices_1,2 nn_indices_rank prep_as_considering_scores dobj_considering_indices prepc_by_derived_considering nn_correlation_Pearsons prep_of_case_correlation amod_case_special det_case_a cop_case_is nsubj_case_it prep_of_sets_items prep_of_rankings_sets dobj_comparing_rankings prepc_for_suitable_comparing cop_suitable_is nsubj_suitable_coefficient nn_coefficient_correlation amod_coefficient_rank nn_coefficient_Spearmans num_coefficient_2 nn_coefficient_X.YbardblXbardblbardblYbardbl amod_coefficient_= nn_coefficient_correlation nn_coefficient_Pearsons prep_of_version_coefficient amod_version_uncentered det_version_an dep_seen_derived parataxis_seen_case dep_seen_suitable prep_as_seen_version auxpass_seen_be aux_seen_can nsubjpass_seen_it appos_Hearst_2008 conj_and_Nakov_Hearst agent_used_Hearst agent_used_Nakov auxpass_used_was conj_and_alternative_used amod_alternative_popular det_alternative_another cop_alternative_is nn_coefficient_correlation nn_coefficient_Cosine nn_coefficient_X. dep_expectation_used dep_expectation_alternative prep_of_expectation_coefficient det_expectation_the cop_expectation_is nsubj_expectation_X nsubj_expectation_= nn_X_E prep_to_number_score prep_of_number_paraphrases det_number_the cop_number_is nsubj_number_n amod_systems_competing det_systems_the conj_and_humans_systems det_humans_the agent_provided_systems agent_provided_humans vmod_paraphrase_provided det_paraphrase_each prep_for_scores_paraphrase amod_scores_numerical prep_of_vectors_scores cop_vectors_are nsubj_vectors_= nsubj_vectors_X advmod_vectors_where dep_vectors_1 num_vectors_2 dep_vectors_E nn_vectors_radicalbigE appos_y1_yn dep_=_y1 npadvmod_=_Y appos_x1_xn dep_=_x1 conj_and_X_= amod_X_= appos_E_Y num_Y_2 appos_2radicalbigE_Y dep_E_E dep_E_2radicalbigE appos_E_X appos_radicalbigE_X2 nn_radicalbigE_Y nn_radicalbigE_E nn_radicalbigE_X nn_radicalbigE_E nn_radicalbigE_XY nn_radicalbigE_E conj_and_=_X ccomp_=_number advmod_=_respectively dobj_=_vectors parataxis_follows_expectation mark_follows_as parataxis_calculated_defined parataxis_calculated_seen advcl_calculated_follows auxpass_calculated_be aux_calculated_can nsubjpass_calculated_it num_distributions_two prep_between_strength_distributions nn_strength_correlation det_strength_the parataxis_measure_calculated prep_of_measure_strength amod_measure_standard det_measure_a cop_measure_is nsubj_measure_coefficient nn_coefficient_correlation nn_coefficient_Pearsons
W09-2416	P08-1052	o	The SemEval-2010 task we present here builds on thework ofNakov -LRB- Nakovand Hearst 2006 Nakov 2007 Nakov 2008b -RRB- where NCs are paraphrased by combinations of verbs and prepositions	conj_and_verbs_prepositions prep_of_combinations_prepositions prep_of_combinations_verbs agent_paraphrased_combinations auxpass_paraphrased_are nsubjpass_paraphrased_NCs advmod_paraphrased_where appos_Nakov_2008b num_Nakov_2007 dep_Hearst_Nakov dep_Hearst_Nakov amod_Hearst_2006 nn_Hearst_Nakovand rcmod_ofNakov_paraphrased appos_ofNakov_Hearst nn_ofNakov_thework prep_on_builds_ofNakov nsubj_builds_task advmod_present_here nsubj_present_we rcmod_task_present nn_task_SemEval-2010 det_task_The ccomp_``_builds
W09-2416	P08-1052	o	Paraphrasesofthiskind have been shown to be useful in applications such as machine translation -LRB- Nakov 2008a -RRB- and as an intermediate step in inventory-based classification of abstract relations -LRB- Kim and Baldwin 2006 Nakov and Hearst 2008 -RRB-	appos_Nakov_2008 conj_and_Nakov_Hearst conj_and_Kim_Hearst conj_and_Kim_Nakov conj_and_Kim_2006 conj_and_Kim_Baldwin dep_relations_Nakov dep_relations_2006 dep_relations_Baldwin dep_relations_Kim amod_relations_abstract prep_of_classification_relations amod_classification_inventory-based prep_in_step_classification amod_step_intermediate det_step_an appos_Nakov_2008a dep_translation_Nakov nn_translation_machine prep_such_as_applications_translation prep_as_useful_step prep_in_useful_applications conj_and_useful_useful cop_useful_be aux_useful_to xcomp_shown_useful xcomp_shown_useful auxpass_shown_been aux_shown_have nsubjpass_shown_Paraphrasesofthiskind
P09-1062	P08-1054	o	In practice we used MMR in our experiments since the original MEAD considers also sentence positions 3 which can always been added later as in -LRB- Penn and Zhu 2008 -RRB-	amod_Penn_2008 conj_and_Penn_Zhu dep_in_Zhu dep_in_Penn advmod_in_as prep_added_in advmod_added_later auxpass_added_been advmod_added_always aux_added_can nsubjpass_added_which rcmod_positions_added num_positions_3 nn_positions_sentence advmod_positions_also dobj_considers_positions nsubj_considers_MEAD mark_considers_since amod_MEAD_original det_MEAD_the poss_experiments_our advcl_used_considers prep_in_used_experiments dobj_used_MMR nsubj_used_we prep_in_used_practice
P09-1062	P08-1054	o	3The usefulness of position varies significantly in different genres -LRB- Penn and Zhu 2008 -RRB-	amod_Penn_2008 conj_and_Penn_Zhu dep_genres_Zhu dep_genres_Penn amod_genres_different prep_in_varies_genres advmod_varies_significantly nsubj_varies_usefulness prep_of_usefulness_position nn_usefulness_3The
P09-1062	P08-1054	o	This obviously does not preclude using the audio-based system together with other features such as utterance position length speakers roles and most others used in the literature -LRB- Penn and Zhu 2008 -RRB-	amod_Penn_2008 conj_and_Penn_Zhu dep_literature_Zhu dep_literature_Penn det_literature_the prep_in_used_literature vmod_others_used amod_others_most nn_roles_speakers conj_and_position_others conj_and_position_roles conj_and_position_length nn_position_utterance prep_such_as_features_others prep_such_as_features_roles prep_such_as_features_length prep_such_as_features_position amod_features_other amod_system_audio-based det_system_the prep_together_with_using_features dobj_using_system xcomp_preclude_using neg_preclude_not aux_preclude_does advmod_preclude_obviously nsubj_preclude_This ccomp_``_preclude
P09-1062	P08-1054	p	These models have achieved state-of-the-art performance in transcript-based speech summarization -LRB- Zechner 2001 Penn and Zhu 2008 -RRB-	num_Penn_2008 conj_and_Penn_Zhu dep_Zechner_Zhu dep_Zechner_Penn conj_Zechner_2001 dep_summarization_Zechner nn_summarization_speech amod_summarization_transcript-based amod_performance_state-of-the-art prep_in_achieved_summarization dobj_achieved_performance aux_achieved_have nsubj_achieved_models det_models_These ccomp_``_achieved
P09-1062	P08-1054	o	Audio data amenable to summarization include meeting recordings -LRB- Murray et al. 2005 -RRB- telephone conversations -LRB- Zhu and Penn 2006 Zechner 2001 -RRB- news broadcasts -LRB- Maskey and Hirschberg 2005 Christensen et al. 2004 -RRB- presentations -LRB- He et al. 2000 Zhang et al. 2007 Penn and Zhu 2008 -RRB- etc. Although extractive summarization is not as ideal as abstractive summarization it outperforms several comparable alternatives	amod_alternatives_comparable amod_alternatives_several dobj_outperforms_alternatives nsubj_outperforms_it dep_outperforms_summarization mark_outperforms_Although dep_outperforms_al. amod_summarization_abstractive prep_as_ideal_summarization prep_as_is_ideal neg_is_not vmod_summarization_is amod_summarization_extractive num_Penn_2008 conj_and_Penn_Zhu dep_Zhang_etc. dep_Zhang_Zhu dep_Zhang_Penn num_Zhang_2007 nn_Zhang_al. nn_Zhang_et dep_al._Zhang dep_al._2000 nn_al._et nsubj_al._He advcl_presentations_outperforms num_Christensen_2004 nn_Christensen_al. nn_Christensen_et conj_and_Maskey_Christensen conj_and_Maskey_2005 conj_and_Maskey_Hirschberg appos_broadcasts_Christensen appos_broadcasts_2005 appos_broadcasts_Hirschberg appos_broadcasts_Maskey nn_broadcasts_news dep_Zechner_2001 dep_Zhu_Zechner conj_and_Zhu_2006 conj_and_Zhu_Penn appos_conversations_2006 appos_conversations_Penn appos_conversations_Zhu nn_conversations_telephone amod_Murray_2005 dep_Murray_al. nn_Murray_et conj_recordings_presentations conj_recordings_broadcasts conj_recordings_conversations dep_recordings_Murray nn_recordings_meeting dobj_include_recordings nsubj_include_data prep_to_amenable_summarization amod_data_amenable nn_data_Audio
P09-1062	P08-1054	o	The usefulness of prosody was found to be very limited by itself if the effect of utterance length is not considered -LRB- Penn and Zhu 2008 -RRB-	amod_Penn_2008 conj_and_Penn_Zhu dep_considered_Zhu dep_considered_Penn neg_considered_not auxpass_considered_is nsubjpass_considered_effect mark_considered_if nn_length_utterance prep_of_effect_length det_effect_the agent_limited_itself advmod_limited_very auxpass_limited_be aux_limited_to advcl_found_considered xcomp_found_limited auxpass_found_was nsubjpass_found_usefulness prep_of_usefulness_prosody det_usefulness_The
P09-1062	P08-1054	o	1 Introduction Summarizing spoken documents has been extensively studied over the past several years -LRB- Penn and Zhu 2008 Maskey and Hirschberg 2005 Murray et al. 2005 Christensen et al. 2004 Zechner 2001 -RRB-	amod_Zechner_2001 num_Christensen_2004 nn_Christensen_al. nn_Christensen_et num_Murray_2005 nn_Murray_al. nn_Murray_et dep_Maskey_Zechner conj_and_Maskey_Christensen conj_and_Maskey_Murray conj_and_Maskey_2005 conj_and_Maskey_Hirschberg dep_Penn_Christensen dep_Penn_Murray dep_Penn_2005 dep_Penn_Hirschberg dep_Penn_Maskey num_Penn_2008 conj_and_Penn_Zhu amod_years_several amod_years_past det_years_the dep_studied_Zhu dep_studied_Penn prep_over_studied_years advmod_studied_extensively auxpass_studied_been aux_studied_has nsubjpass_studied_Introduction amod_documents_spoken dobj_Summarizing_documents vmod_Introduction_Summarizing num_Introduction_1
D08-1024	P08-1058	o	Both were 5gram models with modified Kneser-Ney smoothing lossily compressed using a perfect-hashing scheme similar to that of Talbot and Brants -LRB- 2008 -RRB- but using minimal perfect hashing -LRB- Botelho et al. 2005 -RRB-	amod_Botelho_2005 dep_Botelho_al. nn_Botelho_et amod_hashing_perfect dep_minimal_Botelho dep_minimal_hashing dobj_using_minimal appos_Brants_2008 conj_and_Talbot_Brants prep_of_that_Brants prep_of_that_Talbot prep_to_similar_that amod_scheme_similar amod_scheme_perfect-hashing det_scheme_a dobj_using_scheme conj_but_compressed_using xcomp_compressed_using advmod_compressed_lossily nn_smoothing_Kneser-Ney amod_smoothing_modified dep_models_using dep_models_compressed prep_with_models_smoothing amod_models_5gram cop_models_were nsubj_models_Both
D09-1079	P08-1058	o	This fact along with the observation that machine translation quality improves as the amount of monolingual training material increases has lead to the introduction of randomised techniques for representing large LMs in small space -LRB- Talbot and Osborne 2007 Talbot and Brants 2008 -RRB-	amod_Talbot_2008 conj_and_Talbot_Brants dep_Talbot_Brants dep_Talbot_Talbot conj_and_Talbot_2007 conj_and_Talbot_Osborne dep_space_2007 dep_space_Osborne dep_space_Talbot amod_space_small prep_in_LMs_space amod_LMs_large dobj_representing_LMs prepc_for_techniques_representing amod_techniques_randomised prep_of_introduction_techniques det_introduction_the prep_to_lead_introduction aux_lead_has pobj_lead_observation prepc_along_with_lead_with nsubj_lead_fact nn_increases_material nn_increases_training amod_increases_monolingual prep_of_amount_increases det_amount_the prep_as_improves_amount nsubj_improves_quality mark_improves_that nn_quality_translation nn_quality_machine ccomp_observation_improves det_observation_the det_fact_This
D09-1079	P08-1058	o	We set our space usage to match the 3.08 bytes per n-gram reported in Talbot and Brants -LRB- 2008 -RRB- and held out just over 1M unseen n-grams to test the error rates of our models	poss_models_our prep_of_rates_models nn_rates_error det_rates_the dobj_test_rates aux_test_to amod_n-grams_unseen nn_n-grams_1M prep_over_held_n-grams advmod_held_just prt_held_out appos_Brants_2008 conj_and_Talbot_Brants prep_in_reported_Brants prep_in_reported_Talbot vmod_bytes_reported prep_per_bytes_n-gram num_bytes_3.08 det_bytes_the xcomp_match_test conj_and_match_held dobj_match_bytes aux_match_to nn_usage_space poss_usage_our xcomp_set_held xcomp_set_match dobj_set_usage nsubj_set_We
D09-1079	P08-1058	o	It is a variant of the batch-based Bloomier filter LM of Talbot and Brants -LRB- 2008 -RRB- which we refer to as the TB-LM henceforth	nn_henceforth_TB-LM det_henceforth_the pobj_refer_henceforth prepc_as_to_refer_as nsubj_refer_we dobj_refer_which appos_Brants_2008 conj_and_Talbot_Brants prep_of_LM_Brants prep_of_LM_Talbot nn_LM_filter nn_LM_Bloomier amod_LM_batch-based det_LM_the rcmod_variant_refer prep_of_variant_LM det_variant_a cop_variant_is nsubj_variant_It
D09-1079	P08-1058	o	Any encoding scheme such as the packed representation of Talbot and Brants -LRB- 2008 -RRB- is viable here	advmod_viable_here cop_viable_is nsubj_viable_scheme appos_Brants_2008 conj_and_Talbot_Brants prep_of_representation_Brants prep_of_representation_Talbot amod_representation_packed det_representation_the prep_such_as_scheme_representation amod_scheme_encoding det_scheme_Any ccomp_``_viable
D09-1079	P08-1058	o	As with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently -LRB- Talbot and Brants 2008 -RRB-	amod_Talbot_2008 conj_and_Talbot_Brants dep_rate_Brants dep_rate_Talbot advmod_rate_efficiently nn_rate_error det_rate_the dobj_lower_rate aux_lower_to vmod_checks_lower nn_checks_sanity amod_checks_appropriate det_checks_the prep_with_queries_checks dobj_construct_queries nsubj_construct_we prep_construct_As amod_models_randomised amod_models_other pobj_with_models pcomp_As_with rcmod_``_construct
D09-1079	P08-1058	o	Talbot and Brants -LRB- 2008 -RRB- used a Bloomier filter to encode a LM	det_LM_a dobj_encode_LM aux_encode_to nn_filter_Bloomier det_filter_a vmod_used_encode dobj_used_filter nsubj_used_Brants nsubj_used_Talbot appos_Brants_2008 conj_and_Talbot_Brants
D09-1079	P08-1058	o	The Bloomier filter LM -LRB- Talbot and Brants 2008 -RRB- has a precomputed matching of keys shared between a constant number of cells in the filter array	nn_array_filter det_array_the prep_in_number_array prep_of_number_cells amod_number_constant det_number_a prep_between_shared_number vmod_keys_shared prep_of_matching_keys amod_matching_precomputed det_matching_a dobj_has_matching nsubj_has_LM dep_Talbot_2008 conj_and_Talbot_Brants appos_LM_Brants appos_LM_Talbot nn_LM_filter nn_LM_Bloomier det_LM_The
N09-1058	P08-1058	o	There also have been prior work on maintaining approximate counts for higher-order language models -LRB- LMs -RRB- -LRB- -LRB- Talbot and Osborne 2007a Talbot and Osborne 2007b Talbot and Brants 2008 -RRB- -RRB- operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately	nn_queries_count advmod_answer_approximately dobj_answer_queries aux_answer_to amod_representation_compressed det_representation_this vmod_use_answer dobj_use_representation prep_of_table_counts amod_table_disk-resident det_table_a prep_of_representation_table amod_representation_compressed det_representation_a conj_and_store_use dobj_store_representation aux_store_to xcomp_is_use xcomp_is_store nsubj_is_goal mark_is_that det_goal_the ccomp_model_is det_model_the prep_under_operates_model nsubj_operates_2007a nsubj_operates_Osborne nsubj_operates_Talbot dep_Talbot_2008 conj_and_Talbot_Brants conj_and_Talbot_Talbot conj_and_Talbot_2007b conj_and_Talbot_Osborne dep_Talbot_Brants dep_Talbot_Talbot dep_Talbot_2007b dep_Talbot_Osborne dep_Talbot_Talbot conj_and_Talbot_2007a conj_and_Talbot_Osborne appos_models_LMs nn_models_language amod_models_higher-order amod_counts_approximate prep_for_maintaining_models dobj_maintaining_counts dep_work_operates prepc_on_work_maintaining amod_work_prior cop_work_been aux_work_have advmod_work_also expl_work_There
N09-1058	P08-1058	o	-LRB- Talbot and Brants 2008 -RRB- presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions	nn_reductions_memory amod_reductions_further dobj_achieve_reductions aux_achieve_to amod_pruning_entropy xcomp_combined_achieve prep_with_combined_pruning vmod_hashing_combined xcomp_perfect_hashing prepc_on_based_perfect vmod_model_based nn_model_language amod_model_randomized amod_model_presented dep_model_Brants dep_model_Talbot dep_Talbot_2008 conj_and_Talbot_Brants
N09-1058	P08-1058	o	A problem mentioned in -LRB- Talbot and Brants 2008 -RRB- is that the algorithm that computes the compressed representation might need to retain the entire database in memory in their paper they design strategies to work around this problem	det_problem_this dobj_work_problem advmod_work_around aux_work_to vmod_strategies_work dobj_design_strategies nsubj_design_they prep_in_design_paper poss_paper_their nn_database_entire det_database_the prep_in_retain_memory dobj_retain_database aux_retain_to xcomp_need_retain aux_need_might nsubj_need_representation amod_representation_compressed det_representation_the ccomp_computes_need nsubj_computes_that dep_algorithm_design rcmod_algorithm_computes det_algorithm_the prep_that_is_algorithm nsubj_is_problem amod_Talbot_2008 conj_and_Talbot_Brants prep_in_mentioned_Brants prep_in_mentioned_Talbot vmod_problem_mentioned det_problem_A
P09-2086	P08-1058	o	Either pruning -LRB- Stolcke 1998 Church et al. 2007 -RRB- or lossy randomizing approaches -LRB- Talbot and Brants 2008 -RRB- may result in a compact representation for the application run-time	nn_run-time_application det_run-time_the prep_for_representation_run-time amod_representation_compact det_representation_a prep_in_result_representation aux_result_may nsubj_result_approaches nsubj_result_pruning amod_Talbot_2008 conj_and_Talbot_Brants dep_approaches_Brants dep_approaches_Talbot amod_approaches_randomizing amod_approaches_lossy num_Church_2007 nn_Church_al. nn_Church_et dep_Stolcke_Church dep_Stolcke_1998 conj_or_pruning_approaches appos_pruning_Stolcke preconj_pruning_Either
P09-2086	P08-1058	o	By using 8-bit floating point quantization 1 N-gram language models are compressed into 10 GB which is comparable to a lossy representation -LRB- Talbot and Brants 2008 -RRB-	dep_Talbot_2008 conj_and_Talbot_Brants dep_representation_Brants dep_representation_Talbot amod_representation_lossy det_representation_a prep_to_comparable_representation cop_comparable_is nsubj_comparable_which rcmod_GB_comparable num_GB_10 prep_into_compressed_GB auxpass_compressed_are agent_compressed_using nn_models_language nn_models_N-gram appos_quantization_models num_quantization_1 nn_quantization_point amod_quantization_floating amod_quantization_8-bit dobj_using_quantization rcmod_``_compressed
W09-1505	P08-1058	o	Talbot and Brants -LRB- 2008 -RRB- show that Bloomier filters -LRB- Chazelle et al. 2004 -RRB- can be used to create perfect hash functions for language models	nn_models_language prep_for_functions_models amod_functions_hash amod_functions_perfect dobj_create_functions aux_create_to xcomp_used_create auxpass_used_be aux_used_can nsubjpass_used_filters mark_used_that amod_Chazelle_2004 dep_Chazelle_al. nn_Chazelle_et dep_filters_Chazelle nn_filters_Bloomier ccomp_show_used nsubj_show_Brants nsubj_show_Talbot appos_Brants_2008 conj_and_Talbot_Brants
N09-1035	P08-1065	o	Although some work has been done on syllabifying orthographic forms -LRB- Muller et al. 2000 Bouma 2002 Marchand and Damper 2007 Bartlett et al. 2008 -RRB- syllables are technically speaking phonological entities that can only be composed of strings of phonemes	prep_of_strings_phonemes prep_of_composed_strings auxpass_composed_be advmod_composed_only aux_composed_can nsubjpass_composed_that rcmod_entities_composed amod_entities_phonological dobj_speaking_entities advmod_speaking_technically ccomp_,_speaking num_Bartlett_2008 nn_Bartlett_al. nn_Bartlett_et dep_Marchand_are appos_Marchand_syllables conj_and_Marchand_Bartlett conj_and_Marchand_2007 conj_and_Marchand_Damper dep_Bouma_Bartlett dep_Bouma_2007 dep_Bouma_Damper dep_Bouma_Marchand num_Bouma_2002 dep_Muller_Bouma appos_Muller_2000 dep_Muller_al. nn_Muller_et dep_forms_Muller amod_forms_orthographic amod_forms_syllabifying prep_on_done_forms auxpass_done_been aux_done_has nsubjpass_done_work mark_done_Although det_work_some advcl_``_done
P09-1014	P08-1065	o	Stress is an attribute of syllables but syllabification is a non-trivial task in itself -LRB- Bartlett et al. 2008 -RRB-	amod_Bartlett_2008 dep_Bartlett_al. nn_Bartlett_et dep_task_Bartlett prep_in_task_itself amod_task_non-trivial det_task_a cop_task_is nsubj_task_syllabification conj_but_attribute_task prep_of_attribute_syllables det_attribute_an cop_attribute_is nsubj_attribute_Stress
W09-0106	P08-1065	o	-LRB- Jiampojamarn et al. 2008 -RRB- and -LRB- Bartlett et al. 2008 -RRB- do worse on the English test data than they do on German Dutch or French	conj_or_German_French conj_or_German_Dutch prep_on_do_French prep_on_do_Dutch prep_on_do_German nsubj_do_they mark_do_than nn_data_test nn_data_English det_data_the advcl_worse_do prep_on_worse_data aux_worse_do nsubj_worse_Bartlett nsubj_worse_Jiampojamarn amod_Bartlett_2008 dep_Bartlett_al. nn_Bartlett_et conj_and_Jiampojamarn_Bartlett amod_Jiampojamarn_2008 dep_Jiampojamarn_al. nn_Jiampojamarn_et
D09-1008	P08-1066	o	Thus we can compute the source dependency LM score in the same way we compute the target side score using a procedure described in -LRB- Shen et al. 2008 -RRB-	amod_Shen_2008 dep_Shen_al. nn_Shen_et dep_in_Shen prep_described_in vmod_procedure_described det_procedure_a dobj_using_procedure nn_score_side nn_score_target det_score_the vmod_compute_using dobj_compute_score nsubj_compute_we amod_way_same det_way_the nn_score_LM nn_score_dependency nn_score_source det_score_the dep_compute_compute prep_in_compute_way dobj_compute_score aux_compute_can nsubj_compute_we advmod_compute_Thus
D09-1008	P08-1066	o	Due to the lack of a good Arabic parser compatible with the Sakhr tokenization that we used on the source side we did not test the source dependency LM for Arabic-to-English MT. When extracting rules with source dependency structures we applied the same well-formedness constraint on the source side as we did on the target side using a procedure described by -LRB- Shen et al. 2008 -RRB-	amod_Shen_2008 dep_Shen_al. nn_Shen_et dep_by_Shen prep_described_by vmod_procedure_described det_procedure_a dobj_using_procedure nn_side_target det_side_the prep_on_did_side nsubj_did_we mark_did_as nn_side_source det_side_the prep_on_constraint_side nn_constraint_well-formedness amod_constraint_same det_constraint_the advcl_applied_did dobj_applied_constraint nsubj_applied_we nn_structures_dependency nn_structures_source prep_with_rules_structures dobj_extracting_rules advmod_extracting_When nn_MT._Arabic-to-English prep_for_LM_MT. nn_LM_dependency nn_LM_source det_LM_the xcomp_test_using parataxis_test_applied advcl_test_extracting dobj_test_LM neg_test_not aux_test_did nsubj_test_we prep_due_to_test_lack nn_side_source det_side_the prep_on_used_side nsubj_used_we mark_used_that nn_tokenization_Sakhr det_tokenization_the ccomp_compatible_used prep_with_compatible_tokenization amod_parser_compatible amod_parser_Arabic amod_parser_good det_parser_a prep_of_lack_parser det_lack_the
D09-1008	P08-1066	o	In -LRB- Post and Gildea 2008 Shen et al. 2008 -RRB- target trees were employed to improve the scoring of translation theories	nn_theories_translation prep_of_scoring_theories vmod_the_scoring dobj_improve_the aux_improve_to xcomp_employed_improve auxpass_employed_were nsubjpass_employed_trees prep_employed_In nn_trees_target num_Shen_2008 nn_Shen_al. nn_Shen_et dep_Post_Shen num_Post_2008 conj_and_Post_Gildea dep_In_Gildea dep_In_Post
D09-1008	P08-1066	o	73 1.2.2 Baseline System and Experimental Setup We take BBNs HierDec a string-to-dependency decoder as described in -LRB- Shen et al. 2008 -RRB- as our baseline for the following two reasons It provides a strong baseline which ensures the validity of the improvement we would obtain	aux_obtain_would nsubj_obtain_we det_improvement_the prep_of_validity_improvement det_validity_the parataxis_ensures_obtain dobj_ensures_validity nsubj_ensures_which rcmod_baseline_ensures amod_baseline_strong det_baseline_a dobj_provides_baseline nsubj_provides_It num_reasons_two amod_reasons_following det_reasons_the prep_for_baseline_reasons poss_baseline_our dep_Shen_provides prep_as_Shen_baseline amod_Shen_2008 dep_Shen_al. nn_Shen_et prep_in_described_Shen mark_described_as dep_decoder_described nn_decoder_string-to-dependency det_decoder_a appos_HierDec_decoder nn_HierDec_BBNs dobj_take_HierDec nsubj_take_We amod_Setup_Experimental rcmod_System_take conj_and_System_Setup nn_System_Baseline num_System_1.2.2 num_System_73 dep_``_Setup dep_``_System
D09-1008	P08-1066	o	2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model -LRB- Shen et al. 2008 -RRB- a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side	nn_side_target det_side_the nn_structure_dependency amod_structure_well-formed det_structure_a conj_and_side_structure nn_side_source det_side_the conj_and_words_non-terminals prep_of_string_non-terminals prep_of_string_words det_string_a prep_on_composed_side prep_on_composed_structure prep_on_composed_side prep_of_composed_string auxpass_composed_is nsubjpass_composed_rule nn_rule_translation det_rule_a amod_Shen_2008 dep_Shen_al. nn_Shen_et nn_model_string-to-dependency amod_model_original det_model_the amod_Labels_Non-terminal num_Labels_2.1 ccomp_Features_composed dep_Features_Shen prep_in_Features_model dobj_Features_Labels nsubj_Features_Context nsubj_Features_Linguistic conj_and_Linguistic_Context num_Linguistic_2 ccomp_``_Features
D09-1021	P08-1066	o	Early examples of this work include -LRB- Alshawi 1996 Wu 1997 -RRB- more recent models include -LRB- Yamada and Knight 2001 Eisner 2003 Melamed 2004 Zhang and Gildea 2005 Chiang 2005 Quirk et al. 2005 Marcu et al. 2006 Zollmann and Venugopal 2006 Nesson et al. 2006 Cherry 2008 Mi et al. 2008 Shen et al. 2008 -RRB-	num_Shen_2008 nn_Shen_al. nn_Shen_et num_Mi_2008 nn_Mi_al. nn_Mi_et num_Cherry_2008 num_Nesson_2006 nn_Nesson_al. nn_Nesson_et num_Marcu_2006 nn_Marcu_al. nn_Marcu_et num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Chiang_2005 dep_Zhang_Shen conj_and_Zhang_Mi conj_and_Zhang_Cherry conj_and_Zhang_Nesson num_Zhang_2006 conj_and_Zhang_Venugopal conj_and_Zhang_Zollmann conj_and_Zhang_Marcu conj_and_Zhang_Quirk conj_and_Zhang_Chiang conj_and_Zhang_2005 conj_and_Zhang_Gildea num_Melamed_2004 num_Eisner_2003 dep_Yamada_Mi dep_Yamada_Cherry dep_Yamada_Nesson dep_Yamada_Venugopal dep_Yamada_Zollmann dep_Yamada_Marcu dep_Yamada_Quirk dep_Yamada_Chiang dep_Yamada_2005 dep_Yamada_Gildea dep_Yamada_Zhang conj_and_Yamada_Melamed conj_and_Yamada_Eisner conj_and_Yamada_2001 conj_and_Yamada_Knight dep_include_Melamed dep_include_Eisner dep_include_2001 dep_include_Knight dep_include_Yamada nsubj_include_models amod_models_recent advmod_recent_more dep_Wu_1997 parataxis_Alshawi_include dep_Alshawi_Wu conj_Alshawi_1996 dep_include_Alshawi nsubj_include_examples det_work_this prep_of_examples_work advmod_examples_Early ccomp_``_include
D09-1021	P08-1066	o	Other factors that distinguish us from previous work are the use of all phrases proposed by a phrase-based system and the use of a dependency language model that also incorporates constituent information -LRB- although see -LRB- Charniak et al. 2003 Shen et al. 2008 -RRB- for related approaches -RRB-	amod_approaches_related num_Shen_2008 nn_Shen_al. nn_Shen_et dep_Charniak_Shen appos_Charniak_2003 dep_Charniak_al. nn_Charniak_et prep_for_see_approaches dep_see_Charniak mark_see_although amod_information_constituent dobj_incorporates_information advmod_incorporates_also nsubj_incorporates_that rcmod_model_incorporates nn_model_language nn_model_dependency det_model_a dep_use_see prep_of_use_model det_use_the amod_system_phrase-based det_system_a agent_proposed_system vmod_phrases_proposed det_phrases_all conj_and_use_use prep_of_use_phrases det_use_the cop_use_are nsubj_use_factors amod_work_previous prep_from_distinguish_work dobj_distinguish_us nsubj_distinguish_that rcmod_factors_distinguish amod_factors_Other
D09-1023	P08-1066	p	There is also substantial work in the use of target-side syntax -LRB- Galley et al. 2006 Marcu et al. 2006 Shen et al. 2008 -RRB-	num_Shen_2008 nn_Shen_al. nn_Shen_et dep_Marcu_Shen num_Marcu_2006 nn_Marcu_al. nn_Marcu_et dep_Galley_Marcu appos_Galley_2006 dep_Galley_al. nn_Galley_et amod_syntax_target-side prep_of_use_syntax det_use_the prep_in_work_use amod_work_substantial dep_is_Galley nsubj_is_work advmod_is_also expl_is_There ccomp_``_is
D09-1023	P08-1066	o	Features that consider only target-side syntax and words without considering s can be seen as syntactic language model features -LRB- Shen et al. 2008 -RRB-	amod_Shen_2008 dep_Shen_al. nn_Shen_et dep_features_Shen nn_features_model nn_features_language amod_features_syntactic prep_as_seen_features auxpass_seen_be aux_seen_can nsubjpass_seen_Features dobj_considering_s conj_and_syntax_words amod_syntax_target-side advmod_target-side_only prepc_without_consider_considering dobj_consider_words dobj_consider_syntax nsubj_consider_that rcmod_Features_consider
D09-1073	P08-1066	p	1 Introduction Phrase-based method -LRB- Koehn et al. 2003 Och and Ney 2004 Koehn et al. 2007 -RRB- and syntaxbased method -LRB- Wu 1997 Yamada and Knight 2001 Eisner 2003 Chiang 2005 Cowan et al. 2006 Marcu et al. 2006 Liu et al. 2007 Zhang et al. 2007c 2008a 2008b Shen et al. 2008 Mi and Huang 2008 -RRB- represent the state-of-the-art technologies in statistical machine translation -LRB- SMT -RRB-	appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_technologies_translation amod_technologies_state-of-the-art det_technologies_the dobj_represent_technologies nsubj_represent_method nsubj_represent_Koehn nsubj_represent_2004 nsubj_represent_Ney nsubj_represent_Och dep_Mi_2008 conj_and_Mi_Huang nn_al._et nn_al._Shen appos_Zhang_2008b appos_Zhang_2008a appos_Zhang_2007c dep_Zhang_al. nn_Zhang_et num_Liu_2007 nn_Liu_al. nn_Liu_et dep_al._2006 nn_al._et nn_al._Marcu num_Cowan_2006 nn_Cowan_al. nn_Cowan_et num_Chiang_2005 num_Eisner_2003 num_Yamada_2001 conj_and_Yamada_Knight dep_Wu_Huang dep_Wu_Mi num_Wu_2008 dep_Wu_al. conj_Wu_Zhang conj_Wu_Liu conj_Wu_al. conj_Wu_Cowan conj_Wu_Chiang conj_Wu_Eisner conj_Wu_Knight conj_Wu_Yamada appos_Wu_1997 appos_method_Wu amod_method_syntaxbased num_Koehn_2007 nn_Koehn_al. nn_Koehn_et conj_and_Och_method conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney parataxis_Koehn_represent appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_method_Koehn amod_method_Phrase-based nn_method_Introduction num_method_1
D09-1106	P08-1066	o	Word-aligned corpora have been found to be an excellent source for translation-related knowledge not only for phrase-based models -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- but also for syntax-based models -LRB- e.g. -LRB- Chiang 2007 Galley et al. 2006 Shen et al. 2008 Liu et al. 2006 -RRB- -RRB-	num_Liu_2006 nn_Liu_al. nn_Liu_et num_Shen_2008 nn_Shen_al. nn_Shen_et num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Chiang_Liu dep_Chiang_Shen dep_Chiang_Galley num_Chiang_2007 dep_e.g._Chiang dep_models_e.g. amod_models_syntax-based num_Koehn_2003 nn_Koehn_al. nn_Koehn_et conj_and_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney conj_and_models_models dep_models_Koehn dep_models_2004 dep_models_Ney dep_models_Och amod_models_phrase-based neg_only_not amod_knowledge_translation-related prep_for_source_models prep_for_source_models preconj_source_only prep_for_source_knowledge amod_source_excellent det_source_an cop_source_be aux_source_to xcomp_found_source auxpass_found_been aux_found_have nsubjpass_found_corpora amod_corpora_Word-aligned
D09-1123	P08-1066	o	Recently -LRB- Shen et al. 2008 -RRB- introduced an approach for incorporating a dependency-based language model into SMT	nn_model_language amod_model_dependency-based det_model_a prep_into_incorporating_SMT dobj_incorporating_model prepc_for_approach_incorporating det_approach_an dobj_introduced_approach nsubj_introduced_Shen advmod_introduced_Recently nn_al._et amod_Shen_2008 dep_Shen_al.
D09-1123	P08-1066	o	Firstly -LRB- Shen et al. 2008 -RRB- resorted to heuristics to extract the Stringto-Dependency trees whereas our approach employs the well formalized CCG grammatical theory	amod_theory_grammatical nn_theory_CCG amod_theory_formalized det_theory_the advmod_formalized_well dobj_employs_theory nsubj_employs_approach poss_approach_our nn_trees_Stringto-Dependency det_trees_the dobj_extract_trees aux_extract_to parataxis_resorted_employs dep_resorted_whereas xcomp_resorted_extract prep_to_resorted_heuristics nsubj_resorted_Shen advmod_resorted_Firstly nn_al._et amod_Shen_2008 dep_Shen_al.
D09-1123	P08-1066	n	Thirdly -LRB- Shen et al. 2008 -RRB- deploys the dependency language model to augment the lexical language model probability be1183 tween two head words but never seek a full dependency graph	nn_graph_dependency amod_graph_full det_graph_a dobj_seek_graph neg_seek_never nn_words_head num_words_two nn_words_tween nn_words_be1183 nn_words_probability nn_words_model nn_words_language amod_words_lexical det_words_the conj_but_augment_seek dobj_augment_words aux_augment_to nn_model_language nn_model_dependency det_model_the dep_deploys_seek dep_deploys_augment dobj_deploys_model nsubj_deploys_Shen advmod_deploys_Thirdly nn_al._et amod_Shen_2008 dep_Shen_al.
E09-1044	P08-1066	n	This is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance -LRB- Shen et al. 2008 Zollmann et al. 2008 -RRB-	num_Zollmann_2008 nn_Zollmann_al. nn_Zollmann_et dep_Shen_Zollmann appos_Shen_2008 dep_Shen_al. nn_Shen_et amod_performance_degraded prep_to_lead_performance nsubj_lead_strategies prep_in_lead_which amod_strategies_filtering amod_strategies_other rcmod_results_lead amod_results_reported amod_results_recent prep_to_contrast_results amod_contrast_direct dep_is_Shen prep_in_is_contrast nsubj_is_This
N09-1049	P08-1066	o	Extensions to Hiero Several authors describe extensions to Hiero to incorporate additional syntactic information -LRB- Zollmann and Venugopal 2006 Zhang and Gildea 2006 Shen et al. 2008 Marton and Resnik 2008 -RRB- or to combine it with discriminative latent models -LRB- Blunsom et al. 2008 -RRB-	amod_Blunsom_2008 dep_Blunsom_al. nn_Blunsom_et amod_models_latent amod_models_discriminative dep_combine_Blunsom prep_with_combine_models dobj_combine_it aux_combine_to dep_Marton_2008 conj_and_Marton_Resnik num_Shen_2008 nn_Shen_al. nn_Shen_et num_Zhang_2006 conj_and_Zhang_Gildea dep_Zollmann_Resnik dep_Zollmann_Marton conj_and_Zollmann_Shen conj_and_Zollmann_Gildea conj_and_Zollmann_Zhang conj_and_Zollmann_2006 conj_and_Zollmann_Venugopal conj_or_information_combine appos_information_Shen appos_information_Zhang appos_information_2006 appos_information_Venugopal appos_information_Zollmann amod_information_syntactic amod_information_additional dobj_incorporate_combine dobj_incorporate_information aux_incorporate_to prep_to_extensions_Hiero vmod_describe_incorporate dobj_describe_extensions nsubj_describe_Extensions amod_authors_Several dobj_Hiero_authors aux_Hiero_to vmod_Extensions_Hiero
P09-1042	P08-1066	o	Dependency representation has been used for language modeling textual entailment and machine translation -LRB- Haghighi et al. 2005 Chelba et al. 1997 Quirk et al. 2005 Shen et al. 2008 -RRB- to name a few tasks	amod_tasks_few det_tasks_a dobj_name_tasks aux_name_to num_Shen_2008 nn_Shen_al. nn_Shen_et num_Quirk_2005 nn_Quirk_al. nn_Quirk_et num_Chelba_1997 nn_Chelba_al. nn_Chelba_et dep_Haghighi_Shen dep_Haghighi_Quirk dep_Haghighi_Chelba appos_Haghighi_2005 dep_Haghighi_al. nn_Haghighi_et nn_translation_machine amod_entailment_textual conj_and_modeling_translation conj_and_modeling_entailment nn_modeling_language xcomp_used_name dep_used_Haghighi prep_for_used_translation prep_for_used_entailment prep_for_used_modeling auxpass_used_been aux_used_has nsubjpass_used_representation nn_representation_Dependency
P09-1063	P08-1066	o	They can be roughly divided into three categories string-to-tree models -LRB- e.g. -LRB- Galley et al. 2006 Marcu et al. 2006 Shen et al. 2008 -RRB- -RRB- tree-to-string models -LRB- e.g. -LRB- Liu et al. 2006 Huang et al. 2006 -RRB- -RRB- and tree-totree models -LRB- e.g. -LRB- Eisner 2003 Ding and Palmer 2005 Cowan et al. 2006 Zhang et al. 2008 -RRB- -RRB-	num_Zhang_2008 nn_Zhang_al. nn_Zhang_et num_Cowan_2006 nn_Cowan_al. nn_Cowan_et dep_Ding_Zhang conj_and_Ding_Cowan conj_and_Ding_2005 conj_and_Ding_Palmer dep_Eisner_Cowan dep_Eisner_2005 dep_Eisner_Palmer dep_Eisner_Ding amod_Eisner_2003 appos_e.g._Eisner dep_models_e.g. amod_models_tree-totree num_Huang_2006 nn_Huang_al. nn_Huang_et dep_Liu_Huang num_Liu_2006 dep_Liu_al. nn_Liu_et conj_and_e.g._models appos_e.g._Liu dep_models_models dep_models_e.g. amod_models_tree-to-string num_Shen_2008 nn_Shen_al. nn_Shen_et dep_Marcu_Shen num_Marcu_2006 nn_Marcu_al. nn_Marcu_et dep_Galley_Marcu amod_Galley_2006 dep_Galley_al. nn_Galley_et dep_e.g._Galley appos_models_models dep_models_e.g. amod_models_string-to-tree dep_categories_models num_categories_three prep_into_divided_categories advmod_divided_roughly auxpass_divided_be aux_divided_can nsubjpass_divided_They
P09-1065	P08-1066	o	On the contrary a string-to-tree decoder -LRB- e.g. -LRB- Galley et al. 2006 Shen et al. 2008 -RRB- -RRB- is a parser that applies string-to-tree rules to obtain a target parse for the source string	nn_string_source det_string_the prep_for_parse_string vmod_target_parse det_target_a dobj_obtain_target aux_obtain_to amod_rules_string-to-tree vmod_applies_obtain dobj_applies_rules nsubj_applies_that rcmod_parser_applies det_parser_a cop_parser_is nsubj_parser_decoder prep_on_parser_contrary num_Shen_2008 nn_Shen_al. nn_Shen_et dep_Galley_Shen amod_Galley_2006 dep_Galley_al. nn_Galley_et appos_e.g._Galley dep_decoder_e.g. amod_decoder_string-to-tree det_decoder_a det_contrary_the
P09-1087	P08-1066	n	This provides a compelling advantage over previous dependency language models for MT -LRB- Shen et al. 2008 -RRB- whichusea5-gramLMonlyduringreranking	appos_Shen_whichusea5-gramLMonlyduringreranking amod_Shen_2008 dep_Shen_al. nn_Shen_et prep_for_models_MT nn_models_language nn_models_dependency amod_models_previous prep_over_advantage_models amod_advantage_compelling det_advantage_a dep_provides_Shen dobj_provides_advantage nsubj_provides_This
P09-1087	P08-1066	o	Dependency models have recently gained considerable interest in many NLP applications including machine translation -LRB- Ding and Palmer 2005 Quirk et al. 2005 Shen et al. 2008 -RRB-	num_Shen_2008 nn_Shen_al. nn_Shen_et num_Quirk_2005 nn_Quirk_al. nn_Quirk_et dep_Ding_Shen conj_and_Ding_Quirk conj_and_Ding_2005 conj_and_Ding_Palmer dep_translation_Quirk dep_translation_2005 dep_translation_Palmer dep_translation_Ding nn_translation_machine prep_including_applications_translation nn_applications_NLP amod_applications_many amod_interest_considerable prep_in_gained_applications dobj_gained_interest advmod_gained_recently aux_gained_have nsubj_gained_models nn_models_Dependency
P09-1087	P08-1066	p	1 Introduction Hierarchical approaches to machine translation have proven increasingly successful in recent years -LRB- Chiang 2005 Marcu et al. 2006 Shen et al. 2008 -RRB- and often outperform phrase-based systems -LRB- Och and Ney 2004 Koehn et al. 2003 -RRB- on target-language fluency and adequacy	conj_and_fluency_adequacy nn_fluency_target-language num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2004 conj_and_Och_Ney prep_on_systems_adequacy prep_on_systems_fluency appos_systems_2004 appos_systems_Ney appos_systems_Och amod_systems_phrase-based amod_systems_outperform advmod_outperform_often num_Shen_2008 nn_Shen_al. nn_Shen_et num_Marcu_2006 nn_Marcu_al. nn_Marcu_et dep_Chiang_Shen dep_Chiang_Marcu appos_Chiang_2005 appos_years_Chiang amod_years_recent advmod_successful_increasingly conj_and_proven_systems prep_in_proven_years acomp_proven_successful aux_proven_have nsubj_proven_approaches nn_translation_machine prep_to_approaches_translation amod_approaches_Hierarchical nn_approaches_Introduction num_approaches_1 ccomp_``_systems ccomp_``_proven
W09-2423	P08-1066	o	Finally we are investigating several avenues for using this system output for Machine Translation -LRB- MT -RRB- including -LRB- 1 -RRB- aiding word alignment for other MT system -LRB- Wang et al. 2007 -RRB- and -LRB- 2 -RRB- aiding the creation various MT models involving analyzed text e.g. -LRB- Gildea 2004 Shen et al. 2008 -RRB-	num_Shen_2008 nn_Shen_al. nn_Shen_et dep_Gildea_Shen dep_Gildea_2004 dep_,_Gildea amod_text_analyzed dobj_involving_text dep_models_e.g. vmod_models_involving nn_models_MT amod_models_various amod_creation_models det_creation_the dobj_aiding_creation dep_2_aiding amod_Wang_2007 dep_Wang_al. nn_Wang_et nn_system_MT amod_system_other dep_alignment_Wang prep_for_alignment_system nn_alignment_word conj_and_aiding_2 dobj_aiding_alignment dep_1_2 dep_1_aiding vmod_Translation_including appos_Translation_MT nn_Translation_Machine nn_output_system det_output_this prep_for_using_Translation dobj_using_output amod_avenues_several parataxis_investigating_1 prepc_for_investigating_using dobj_investigating_avenues aux_investigating_are nsubj_investigating_we advmod_investigating_Finally
E09-1021	P08-1079	o	In the concept extension part of our algorithm we adapt our concept acquisition framework -LRB- Davidov and Rappoport 2006 Davidov et al. 2007 Davidov and Rappoport 2008a Davidov and Rappoport 2008b -RRB- to suit diverse languages including ones without explicit word segmentation	nn_segmentation_word amod_segmentation_explicit prep_including_languages_ones amod_languages_diverse nn_languages_suit appos_Davidov_2008b conj_and_Davidov_Rappoport conj_and_Davidov_Davidov conj_and_Davidov_2008a conj_and_Davidov_Rappoport num_Davidov_2007 nn_Davidov_al. nn_Davidov_et dep_Davidov_Rappoport dep_Davidov_Davidov dep_Davidov_2008a dep_Davidov_Rappoport dep_Davidov_Davidov conj_and_Davidov_Davidov conj_and_Davidov_2006 conj_and_Davidov_Rappoport dep_framework_Davidov dep_framework_2006 dep_framework_Rappoport dep_framework_Davidov nn_framework_acquisition nn_framework_concept poss_framework_our prep_without_adapt_segmentation prep_to_adapt_languages dobj_adapt_framework nsubj_adapt_we prep_in_adapt_part poss_algorithm_our prep_of_part_algorithm nn_part_extension nn_part_concept det_part_the
W09-0805	P08-1079	o	While in this paper we evaluated our framework on the discovery of concepts we have recently proposed fully unsupervised frameworks for the discovery of different relationship types -LRB- Davidov et al. 2007 Davidov and Rappoport 2008a Davidov and Rappoport 2008b -RRB-	appos_Davidov_2008b conj_and_Davidov_Rappoport conj_and_Davidov_Davidov conj_and_Davidov_2008a conj_and_Davidov_Rappoport dep_Davidov_Rappoport dep_Davidov_Davidov dep_Davidov_2008a dep_Davidov_Rappoport dep_Davidov_Davidov appos_Davidov_2007 dep_Davidov_al. nn_Davidov_et nn_types_relationship amod_types_different prep_of_discovery_types det_discovery_the appos_frameworks_Davidov prep_for_frameworks_discovery amod_frameworks_unsupervised advmod_unsupervised_fully dobj_proposed_frameworks advmod_proposed_recently aux_proposed_have nsubj_proposed_we advcl_proposed_evaluated prep_of_discovery_concepts det_discovery_the prep_on_framework_discovery poss_framework_our dobj_evaluated_framework nsubj_evaluated_we prep_in_evaluated_paper mark_evaluated_While det_paper_this
W09-1111	P08-1079	o	In computational linguistics our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs -LRB- -LRB- Hearst 1998 Chklovski and Pantel 2004 Etzioni et al. 2004 Turney 2006 Davidov and Rappoport 2008 -RRB- inter alia -RRB-	nn_alia_inter dep_alia_Rappoport dep_alia_Davidov dep_Davidov_2008 conj_and_Davidov_Rappoport num_Turney_2006 num_Etzioni_2004 nn_Etzioni_al. nn_Etzioni_et dep_Chklovski_alia conj_and_Chklovski_Turney conj_and_Chklovski_Etzioni conj_and_Chklovski_2004 conj_and_Chklovski_Pantel dep_Hearst_Turney dep_Hearst_Etzioni dep_Hearst_2004 dep_Hearst_Pantel dep_Hearst_Chklovski amod_Hearst_1998 conj_or_nouns_verbs prep_between_relations_verbs prep_between_relations_nouns amod_relations_semantic dep_indicators_Hearst prep_of_indicators_relations nn_patterns_surface prep_as_use_indicators dobj_use_patterns nsubj_use_that rcmod_approaches_use amod_approaches_previous prep_over_extends_approaches nsubj_extends_procedure prep_in_extends_linguistics nn_procedure_discovery nn_procedure_pattern poss_procedure_our amod_linguistics_computational
W09-1111	P08-1079	o	This approach is similar to that of seed words -LRB- e.g. -LRB- Hearst 1998 -RRB- -RRB- or hook words -LRB- e.g. -LRB- Davidov and Rappoport 2008 -RRB- -RRB- in previous work	amod_work_previous dep_Davidov_2008 conj_and_Davidov_Rappoport appos_e.g._Rappoport appos_e.g._Davidov dep_words_e.g. nn_words_hook amod_Hearst_1998 dep_e.g._Hearst conj_or_words_words dep_words_e.g. nn_words_seed prep_of_that_words prep_of_that_words prep_in_similar_work prep_to_similar_that cop_similar_is nsubj_similar_approach det_approach_This ccomp_``_similar
D09-1054	P08-1081	o	Note that apart from previous work -LRB- Ding et al. 2008 -RRB- we use complete skip-chain -LRB- contextanswer -RRB- edges in hc -LRB- x y -RRB-	appos_x_y dep_hc_x prep_in_edges_hc appos_skip-chain_contextanswer amod_skip-chain_complete dep_use_edges dobj_use_skip-chain nsubj_use_we dep_use_Ding prep_apart_from_use_work amod_Ding_2008 dep_Ding_al. nn_Ding_et amod_work_previous rcmod_that_use dobj_Note_that
D09-1054	P08-1081	o	We made use of the same data set as introduced in -LRB- Cong et al. 2008 Ding et al. 2008 -RRB-	num_Ding_2008 nn_Ding_al. nn_Ding_et dep_Cong_Ding appos_Cong_2008 dep_Cong_al. nn_Cong_et prep_in_introduced_Cong mark_introduced_as advcl_set_introduced vmod_data_set amod_data_same det_data_the prep_of_use_data dobj_made_use nsubj_made_We
D09-1054	P08-1081	o	The suffixes C * and V * denote the models using incomplete skip-chain edges and vertical sequential edges proposed in -LRB- Ding et al. 2008 -RRB- as shown in Figures 2 -LRB- a -RRB- and 2 -LRB- c -RRB-	appos_2_c conj_and_Figures_2 dep_Figures_a num_Figures_2 prep_in_shown_2 prep_in_shown_Figures mark_shown_as amod_Ding_2008 dep_Ding_al. nn_Ding_et dep_in_Ding prep_proposed_in vmod_edges_proposed amod_edges_sequential amod_edges_vertical conj_and_edges_edges nn_edges_skip-chain amod_edges_incomplete advcl_using_shown dobj_using_edges dobj_using_edges det_models_the xcomp_denote_using dobj_denote_models nsubj_denote_V nsubj_denote_* dep_*_* conj_and_*_V nn_*_C nn_*_suffixes det_*_The
D09-1054	P08-1081	n	In comparison the 2D model in Figure 2 -LRB- c -RRB- used in previous work -LRB- Ding et al. 2008 -RRB- can only model the interaction between adjacent questions	amod_questions_adjacent prep_between_interaction_questions det_interaction_the dobj_model_interaction advmod_model_only aux_model_can nsubj_model_model prep_in_model_comparison amod_Ding_2008 dep_Ding_al. nn_Ding_et amod_work_previous prep_in_used_work vmod_Figure_used appos_Figure_c num_Figure_2 dep_model_Ding prep_in_model_Figure nn_model_2D det_model_the
D09-1054	P08-1081	n	Our graphical representation has two advantages over previous work -LRB- Ding et al. 2008 -RRB- unifying sentence relations and incorporating question interactions	nn_interactions_question dobj_incorporating_interactions conj_and_relations_incorporating nn_relations_sentence amod_relations_unifying amod_Ding_2008 dep_Ding_al. nn_Ding_et amod_work_previous prep_over_advantages_work num_advantages_two dep_has_incorporating dep_has_relations dep_has_Ding dobj_has_advantages nsubj_has_representation amod_representation_graphical poss_representation_Our
D09-1054	P08-1081	n	Previous work -LRB- Ding et al. 2008 -RRB- performs the extraction of contexts and answers in multiple passes of the thread -LRB- with each pass corresponding to one question -RRB- which can not address the interactions well	det_interactions_the advmod_address_well dobj_address_interactions neg_address_not aux_address_can nsubj_address_which num_question_one prep_to_corresponding_question vmod_pass_corresponding det_pass_each det_thread_the rcmod_passes_address prep_with_passes_pass prep_of_passes_thread amod_passes_multiple prep_in_answers_passes conj_and_extraction_answers prep_of_extraction_contexts det_extraction_the dobj_performs_answers dobj_performs_extraction nsubj_performs_work amod_Ding_2008 dep_Ding_al. nn_Ding_et dep_work_Ding amod_work_Previous
D09-1054	P08-1081	o	We design special inference algorithms instead of general-purpose inference algorithms used in previous works -LRB- Cong et al. 2008 Ding et al. 2008 -RRB- by taking advantage of special properties of our task	poss_task_our prep_of_properties_task amod_properties_special prep_of_advantage_properties dobj_taking_advantage num_Ding_2008 nn_Ding_al. nn_Ding_et dep_Cong_Ding appos_Cong_2008 dep_Cong_al. nn_Cong_et amod_works_previous prep_in_used_works vmod_algorithms_used nn_algorithms_inference amod_algorithms_general-purpose nn_algorithms_inference amod_algorithms_special prepc_by_design_taking dep_design_Cong prep_instead_of_design_algorithms dobj_design_algorithms nsubj_design_We ccomp_``_design
D09-1054	P08-1081	o	1 Introduction Recently extracting questions contexts and answers from post discussions of online forums incurs increasing academic attention -LRB- Cong et al. 2008 Ding et al. 2008 -RRB-	num_Ding_2008 nn_Ding_al. nn_Ding_et dep_Cong_Ding appos_Cong_2008 dep_Cong_al. nn_Cong_et amod_attention_academic amod_attention_increasing dep_incurs_Cong dobj_incurs_attention nsubj_incurs_answers nsubj_incurs_contexts nsubj_incurs_questions dep_incurs_Introduction amod_forums_online prep_of_discussions_forums nn_discussions_post prep_from_answers_discussions conj_and_questions_answers conj_and_questions_contexts amod_questions_extracting advmod_Introduction_Recently num_Introduction_1
C08-1026	P08-1085	o	Even for many unsupervised situations this is available from a lexicon -LRB- e.g. Banko and Moore 2004 Goldberg et al. 2008 -RRB-	num_Goldberg_2008 nn_Goldberg_al. nn_Goldberg_et dep_Banko_Goldberg conj_and_Banko_2004 conj_and_Banko_Moore dep_e.g._2004 dep_e.g._Moore dep_e.g._Banko dep_lexicon_e.g. det_lexicon_a prep_from_available_lexicon cop_available_is nsubj_available_this prep_for_available_situations advmod_available_Even amod_situations_unsupervised amod_situations_many
C08-1026	P08-1085	o	Thus an orthogonal line of research can involve inducing classes for words which are more general than single categories i.e. something akin to ambiguity classes -LRB- see e.g. the discussion of ambiguity class guessers in Goldberg et al. 2008 -RRB-	num_Goldberg_2008 nn_Goldberg_al. nn_Goldberg_et nn_guessers_class nn_guessers_ambiguity prep_in_discussion_Goldberg prep_of_discussion_guessers det_discussion_the dobj_see_discussion dep_see_e.g. nn_classes_ambiguity prep_to_akin_classes dep_something_see amod_something_akin dep_i.e._something prep_categories_i.e. amod_categories_single prep_than_general_categories advmod_general_more cop_general_are nsubj_general_which rcmod_words_general prep_for_inducing_words dobj_inducing_classes xcomp_involve_inducing aux_involve_can nsubj_involve_line advmod_involve_Thus prep_of_line_research amod_line_orthogonal det_line_an
C08-1026	P08-1085	p	4.1 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in words with the same category possibilities are grouped together .4 And ambiguity classes have been shown to be successfully employed in a variety of ways to improve POS tagging -LRB- e.g. Cutting et al. 1992 Daelemans et al. 1996 Dickinson 2007 Goldberg et al. 2008 Tseng et al. 2005 -RRB-	num_Tseng_2005 nn_Tseng_al. nn_Tseng_et nn_al._et nn_al._Goldberg num_Dickinson_2007 nn_al._et nn_al._Daelemans num_al._1992 nn_al._et dobj_Cutting_al. dep_e.g._Tseng dep_e.g._2008 dep_e.g._al. dep_e.g._Dickinson dep_e.g._1996 dep_e.g._al. vmod_e.g._Cutting dep_tagging_e.g. nn_tagging_POS dobj_improve_tagging aux_improve_to prep_of_variety_ways det_variety_a advmod_employed_successfully auxpass_employed_be aux_employed_to xcomp_shown_improve prep_in_shown_variety xcomp_shown_employed auxpass_shown_been aux_shown_have num_classes_ambiguity num_classes_.4 conj_and_.4_ambiguity advmod_.4_together advcl_grouped_shown xcomp_grouped_classes auxpass_grouped_are nsubjpass_grouped_words nn_possibilities_category amod_possibilities_same det_possibilities_the prep_with_words_possibilities dep_in_grouped prep_interested_in cop_interested_are nsubj_interested_we amod_property_relevant det_property_the dep_capture_interested dobj_capture_property nsubj_capture_classes nn_classes_Ambiguity nn_classes_classes nn_classes_ambiguity amod_classes_Complete num_classes_4.1
E09-1038	P08-1085	o	Traditionally such unsupervised EM-trained HMM taggers are thought to be inaccurate but -LRB- Goldberg et al. 2008 -RRB- showed that by feeding the EM process with sufficiently good initial probabilities accurate taggers -LRB- > 91 % accuracy -RRB- can be learned for both English and Hebrew based on a -LRB- possibly incomplete -RRB- lexicon and large amount of raw text	amod_text_raw prep_of_amount_text amod_amount_large conj_and_lexicon_amount dep_lexicon_incomplete det_lexicon_a advmod_incomplete_possibly conj_and_English_Hebrew preconj_English_both pobj_learned_amount pobj_learned_lexicon prepc_based_on_learned_on prep_for_learned_Hebrew prep_for_learned_English auxpass_learned_be aux_learned_can nsubjpass_learned_taggers agent_learned_feeding mark_learned_that dep_accuracy_% num_%_91 quantmod_91_> appos_taggers_accuracy amod_taggers_accurate amod_probabilities_initial amod_probabilities_good advmod_good_sufficiently nn_process_EM det_process_the prep_with_feeding_probabilities dobj_feeding_process ccomp_showed_learned nsubj_showed_Goldberg amod_Goldberg_2008 dep_Goldberg_al. nn_Goldberg_et cop_inaccurate_be aux_inaccurate_to conj_but_thought_showed xcomp_thought_inaccurate auxpass_thought_are nsubjpass_thought_taggers advmod_thought_Traditionally nn_taggers_HMM amod_taggers_EM-trained amod_taggers_unsupervised amod_taggers_such
P09-1057	P08-1085	o	6 Smaller Tagset and Incomplete Dictionaries Previously researchers working on this task have also reported results for unsupervised tagging with a smaller tagset -LRB- Smith and Eisner 2005 Goldwater and Griffiths 2007 Toutanova and Johnson 2008 Goldberg et al. 2008 -RRB-	num_Goldberg_2008 nn_Goldberg_al. nn_Goldberg_et dep_Goldwater_Goldberg conj_and_Goldwater_2008 conj_and_Goldwater_Johnson conj_and_Goldwater_Toutanova conj_and_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Smith_2008 dep_Smith_Johnson dep_Smith_Toutanova dep_Smith_2007 dep_Smith_Griffiths dep_Smith_Goldwater conj_and_Smith_2005 conj_and_Smith_Eisner dep_tagset_2005 dep_tagset_Eisner dep_tagset_Smith amod_tagset_smaller det_tagset_a prep_with_tagging_tagset amod_unsupervised_tagging prep_for_results_unsupervised amod_results_reported advmod_reported_also aux_reported_have nsubj_reported_researchers ccomp_reported_Dictionaries ccomp_reported_Tagset dep_reported_6 det_task_this prep_on_working_task vmod_researchers_working nn_Dictionaries_Incomplete advmod_Tagset_Previously conj_and_Tagset_Dictionaries amod_Tagset_Smaller
P09-1057	P08-1085	o	The table in Figure 9 shows a comparison of different systems for which tagging accuracies have been reported previously for the 17-tagset case -LRB- Goldberg et al. 2008 -RRB-	amod_Goldberg_2008 dep_Goldberg_al. nn_Goldberg_et amod_case_17-tagset det_case_the prep_for_reported_case advmod_reported_previously auxpass_reported_been aux_reported_have nsubjpass_reported_accuracies prep_for_reported_which amod_accuracies_tagging rcmod_systems_reported amod_systems_different prep_of_comparison_systems det_comparison_a dep_shows_Goldberg dobj_shows_comparison nsubj_shows_table num_Figure_9 prep_in_table_Figure det_table_The
P09-1057	P08-1085	o	Some previous approaches -LRB- Toutanova and Johnson 2008 Goldberg et al. 2008 -RRB- handle unknown words explicitly using ambiguity class components conditioned on various morphological features and this has shown to produce good tagging results especially when dealing with incomplete dictionaries	amod_dictionaries_incomplete prep_with_dealing_dictionaries dep_when_dealing advmod_when_especially nn_results_tagging amod_results_good dobj_produce_results aux_produce_to xcomp_shown_produce aux_shown_has nsubj_shown_this amod_features_morphological amod_features_various prep_on_conditioned_features vmod_components_conditioned nn_components_class nn_components_ambiguity dobj_using_components advmod_using_explicitly vmod_words_using amod_words_unknown dep_handle_when conj_and_handle_shown dobj_handle_words nsubj_handle_approaches num_Goldberg_2008 nn_Goldberg_al. nn_Goldberg_et dep_Toutanova_Goldberg conj_and_Toutanova_2008 conj_and_Toutanova_Johnson dep_approaches_2008 dep_approaches_Johnson dep_approaches_Toutanova amod_approaches_previous det_approaches_Some ccomp_``_shown ccomp_``_handle
P09-1057	P08-1085	o	EM-HMM tagger provided with good initial conditions -LRB- Goldberg et al. 2008 -RRB- 91.4 * -LRB- * uses linguistic constraints and manual adjustments to the dictionary -RRB- Figure 1 Previous results on unsupervised POS tagging using a dictionary -LRB- Merialdo 1994 -RRB- on the full 45-tag set	amod_set_45-tag amod_set_full det_set_the dep_Merialdo_1994 dep_dictionary_Merialdo det_dictionary_a prep_on_using_set dobj_using_dictionary xcomp_tagging_using vmod_POS_tagging amod_POS_unsupervised prep_on_results_POS amod_results_Previous dep_Figure_results num_Figure_1 det_dictionary_the amod_adjustments_manual conj_and_constraints_adjustments amod_constraints_linguistic dobj_uses_Figure prep_to_uses_dictionary dobj_uses_adjustments dobj_uses_constraints dep_uses_* dep_uses_* nsubj_uses_tagger num_*_91.4 amod_Goldberg_2008 dep_Goldberg_al. nn_Goldberg_et amod_conditions_initial amod_conditions_good prep_with_provided_conditions dep_tagger_Goldberg vmod_tagger_provided nn_tagger_EM-HMM
W09-0905	P08-1085	o	Due to its popularity for unsupervised POS induction research -LRB- e.g. Goldberg et al. 2008 Goldwater and Griffiths 2007 Toutanova and Johnson 2008 -RRB- and its often-used tagset for our initial research we use the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- with 36 tags -LRB- plus 9 punctuation tags -RRB- and we use sections 00-18 leaving held-out data for future experiments .4 Defining frequent frames as those occurring at 4Even if we wanted child-directed speech the CHILDES database -LRB- MacWhinney 2000 -RRB- uses coarse POS tags	nn_tags_POS amod_tags_coarse dobj_uses_tags nsubj_uses_database advcl_uses_wanted amod_MacWhinney_2000 dep_database_MacWhinney nn_database_CHILDES det_database_the amod_speech_child-directed dobj_wanted_speech nsubj_wanted_we mark_wanted_if dep_wanted_use dep_wanted_tagset dep_wanted_Griffiths dep_wanted_Goldwater prep_at_occurring_4Even vmod_those_occurring prep_as_frames_those amod_frames_frequent amod_frames_Defining num_frames_.4 dep_experiments_frames amod_experiments_future amod_data_held-out prep_for_leaving_experiments dobj_leaving_data num_sections_00-18 dobj_use_sections nsubj_use_we nn_tags_punctuation num_tags_9 cc_tags_plus appos_tags_tags num_tags_36 dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the dobj_use_portion nsubj_use_we amod_research_initial poss_research_our amod_tagset_often-used poss_tagset_its dep_Toutanova_2008 conj_and_Toutanova_Johnson vmod_Goldwater_leaving conj_and_Goldwater_use prep_with_Goldwater_tags dep_Goldwater_Marcus rcmod_Goldwater_use prep_for_Goldwater_research conj_and_Goldwater_tagset dep_Goldwater_Johnson dep_Goldwater_Toutanova num_Goldwater_2007 conj_and_Goldwater_Griffiths parataxis_al._uses dep_al._2008 nn_al._et nn_al._Goldberg dep_al._e.g. dep_research_al. nn_research_induction nn_research_POS amod_research_unsupervised prep_for_popularity_research poss_popularity_its prep_due_to_``_popularity
N09-1031	P08-1092	o	Regression has also been used to order sentences in extractive summarization -LRB- Biadsy et al. 2008 -RRB-	amod_Biadsy_2008 dep_Biadsy_al. nn_Biadsy_et amod_summarization_extractive dep_order_Biadsy prep_in_order_summarization dobj_order_sentences aux_order_to xcomp_used_order auxpass_used_been advmod_used_also aux_used_has nsubjpass_used_Regression
P09-1024	P08-1092	o	These domains have been commonly used in prior work on summarization -LRB- Weischedel et al. 2004 Zhou et al. 2004 Filatova and Prager 2005 DemnerFushman and Lin 2007 Biadsy et al. 2008 -RRB-	num_Biadsy_2008 nn_Biadsy_al. nn_Biadsy_et dep_DemnerFushman_Biadsy conj_and_DemnerFushman_2007 conj_and_DemnerFushman_Lin conj_and_Filatova_2007 conj_and_Filatova_Lin conj_and_Filatova_DemnerFushman conj_and_Filatova_2005 conj_and_Filatova_Prager dep_Zhou_DemnerFushman dep_Zhou_2005 dep_Zhou_Prager dep_Zhou_Filatova num_Zhou_2004 nn_Zhou_al. nn_Zhou_et dep_Weischedel_Zhou appos_Weischedel_2004 dep_Weischedel_al. nn_Weischedel_et prep_on_work_summarization amod_work_prior dep_used_Weischedel prep_in_used_work advmod_used_commonly auxpass_used_been aux_used_have nsubjpass_used_domains det_domains_These
P09-1024	P08-1092	o	Instead we follow a simplified form of previous work on biography creation where a classifier is trained to distinguish biographical text -LRB- Zhou et al. 2004 Biadsy et al. 2008 -RRB-	num_Biadsy_2008 nn_Biadsy_al. nn_Biadsy_et dep_Zhou_Biadsy amod_Zhou_2004 dep_Zhou_al. nn_Zhou_et amod_text_biographical dobj_distinguish_text aux_distinguish_to xcomp_trained_distinguish auxpass_trained_is nsubjpass_trained_classifier advmod_trained_where det_classifier_a rcmod_creation_trained nn_creation_biography amod_work_previous prep_on_form_creation prep_of_form_work amod_form_simplified det_form_a dep_follow_Zhou dobj_follow_form nsubj_follow_we advmod_follow_Instead
P09-1024	P08-1092	o	For instance some approaches coarsely discriminate between biographical and non-biographical information -LRB- Zhou et al. 2004 Biadsyetal. ,2008 -RRB- whileothersgobeyondbinary distinction by identifying atomic events e.g. occupation and marital status that are typically included in a biography -LRB- Weischedel et al. 2004 Filatova and Prager 2005 Filatova et al. 2006 -RRB-	num_Filatova_2006 nn_Filatova_al. nn_Filatova_et dep_Filatova_Filatova conj_and_Filatova_2005 conj_and_Filatova_Prager dep_Weischedel_2005 dep_Weischedel_Prager dep_Weischedel_Filatova appos_Weischedel_2004 dep_Weischedel_al. nn_Weischedel_et det_biography_a prep_in_included_biography advmod_included_typically auxpass_included_are nsubjpass_included_that amod_status_marital dep_occupation_Weischedel rcmod_occupation_included conj_and_occupation_status prep_occupation_e.g. amod_events_atomic advmod_identifying_status advmod_identifying_occupation dobj_identifying_events prepc_by_distinction_identifying amod_distinction_whileothersgobeyondbinary num_Biadsyetal._,2008 dep_Zhou_Biadsyetal. appos_Zhou_2004 dep_Zhou_al. nn_Zhou_et appos_information_distinction appos_information_Zhou amod_information_non-biographical amod_information_biographical conj_and_biographical_non-biographical prep_between_discriminate_information advmod_discriminate_coarsely nsubj_discriminate_approaches prep_for_discriminate_instance det_approaches_some
P09-1024	P08-1092	o	Instances of this work include information extraction ontology induction and resource acquisition -LRB- Wu and Weld 2007 Biadsy et al. 2008 Nastase 2008 Nastase and Strube 2008 -RRB-	amod_Nastase_2008 conj_and_Nastase_Strube num_Nastase_2008 nn_al._et nn_al._Biadsy dep_Wu_Strube dep_Wu_Nastase conj_and_Wu_Nastase num_Wu_2008 dep_Wu_al. num_Wu_2007 conj_and_Wu_Weld dep_acquisition_Nastase dep_acquisition_Weld dep_acquisition_Wu nn_acquisition_resource nn_induction_ontology conj_and_extraction_acquisition conj_and_extraction_induction nn_extraction_information dobj_include_acquisition dobj_include_induction dobj_include_extraction nsubj_include_Instances det_work_this prep_of_Instances_work
N09-1066	P08-1093	o	Citation texts have also been used to create summaries of single scientific articles in Qazvinian and Radev -LRB- 2008 -RRB- and Mei and Zhai -LRB- 2008 -RRB-	appos_Zhai_2008 appos_Radev_2008 conj_and_Qazvinian_Zhai conj_and_Qazvinian_Mei conj_and_Qazvinian_Radev prep_in_articles_Zhai prep_in_articles_Mei prep_in_articles_Radev prep_in_articles_Qazvinian amod_articles_scientific amod_articles_single prep_of_summaries_articles dobj_create_summaries aux_create_to xcomp_used_create auxpass_used_been advmod_used_also aux_used_have nsubjpass_used_texts nn_texts_Citation
P09-1023	P08-1093	o	By analyzing rhetorical discourse structure of aim background solution etc. or citation context we can obtain appropriate abstracts and the most influential contents from scientific articles -LRB- Teufel and Moens 2002 Mei and Zhai 2008 -RRB-	dep_Mei_2008 conj_and_Mei_Zhai dep_Teufel_Zhai dep_Teufel_Mei conj_and_Teufel_2002 conj_and_Teufel_Moens dep_articles_2002 dep_articles_Moens dep_articles_Teufel amod_articles_scientific prep_from_contents_articles amod_contents_influential det_contents_the advmod_influential_most conj_and_abstracts_contents amod_abstracts_appropriate dobj_obtain_contents dobj_obtain_abstracts aux_obtain_can nsubj_obtain_we prepc_by_obtain_analyzing nn_context_citation conj_or_aim_context conj_or_aim_etc. conj_or_aim_solution conj_or_aim_background prep_of_structure_context prep_of_structure_etc. prep_of_structure_solution prep_of_structure_background prep_of_structure_aim nn_structure_discourse amod_structure_rhetorical dobj_analyzing_structure
D08-1079	P08-1094	o	Nenkova and Louis -LRB- 2008 -RRB- investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization	amod_summarization_multi-document prep_in_quality_summarization nn_quality_summary det_quality_the dobj_influence_quality nsubj_influence_characteristics nsubj_influence_length advmod_influence_how det_input_the prep_of_characteristics_input det_characteristics_the conj_and_length_characteristics nn_length_summary ccomp_investigate_influence nsubj_investigate_Louis nsubj_investigate_Nenkova appos_Louis_2008 conj_and_Nenkova_Louis
E09-1062	P08-1094	o	For the first set of experiments we divide all inputs based on the mean value of the average system scores as in -LRB- Nenkova and Louis 2008 -RRB-	dep_Nenkova_2008 conj_and_Nenkova_Louis dep_in_Louis dep_in_Nenkova pcomp_as_in prep_scores_as nn_scores_system amod_scores_average det_scores_the prep_of_value_scores amod_value_mean det_value_the prep_on_based_value vmod_inputs_based det_inputs_all dobj_divide_inputs nsubj_divide_we prep_for_divide_set prep_of_set_experiments amod_set_first det_set_the
E09-1062	P08-1094	o	Only recently the issue has drawn attention -LRB- Nenkova and Louis 2008 -RRB- present an initial analysis of the factors that influence system performance in content selection	amod_selection_content nn_performance_system prep_in_influence_selection dobj_influence_performance nsubj_influence_that rcmod_factors_influence det_factors_the prep_of_analysis_factors amod_analysis_initial det_analysis_an dobj_present_analysis dep_Nenkova_2008 conj_and_Nenkova_Louis dep_drawn_present dep_drawn_Louis dep_drawn_Nenkova dobj_drawn_attention aux_drawn_has nsubj_drawn_issue advmod_drawn_recently det_issue_the advmod_recently_Only ccomp_``_drawn
E09-1062	P08-1094	o	4 Features For our experiments we use the features proposed motivated and described in detail by -LRB- Nenkova and Louis 2008 -RRB-	dep_Nenkova_2008 conj_and_Nenkova_Louis dep_by_Louis dep_by_Nenkova prep_described_by prep_in_described_detail conj_and_proposed_described conj_and_proposed_motivated dep_features_described dep_features_motivated dep_features_proposed det_features_the dobj_use_features nsubj_use_we poss_experiments_our rcmod_Features_use prep_for_Features_experiments num_Features_4 dep_``_Features
P09-1023	P08-1094	o	204 4.2.2 Correlation between TREC nuggets and non-text features Analyzing the features used could let us understand summarization better -LRB- Nenkova and Louis 2008 -RRB-	dep_Nenkova_2008 conj_and_Nenkova_Louis dep_better_Louis dep_better_Nenkova amod_summarization_better dobj_understand_summarization nsubj_understand_us ccomp_let_understand aux_let_could nsubj_let_Correlation vmod_features_used det_features_the dobj_Analyzing_features vmod_features_Analyzing amod_features_non-text conj_and_nuggets_features nn_nuggets_TREC prep_between_Correlation_features prep_between_Correlation_nuggets num_Correlation_4.2.2 num_Correlation_204
E09-1031	P08-1101	o	Finally Zhang and Clark -LRB- 2008 -RRB- achieve an SF of 95.90 % and a TF of 91.34 % by 10-fold cross validation using CTB data	nn_data_CTB dobj_using_data nn_validation_cross amod_validation_10-fold num_%_91.34 vmod_TF_using prep_by_TF_validation prep_of_TF_% det_TF_a num_%_95.90 conj_and_SF_TF prep_of_SF_% det_SF_an dobj_achieve_TF dobj_achieve_SF nsubj_achieve_Clark nsubj_achieve_Zhang advmod_achieve_Finally appos_Clark_2008 conj_and_Zhang_Clark
P09-1058	P08-1101	o	0.9595 0.9590 0.9611 0.9085 0.9134 0.9152 Table 8 Comparison of F1 results of our baseline model with Nakagawa and Uchimoto -LRB- 2007 -RRB- and Zhang and Clark -LRB- 2008 -RRB- on CTB 3.0	num_CTB_3.0 appos_Clark_2008 appos_Uchimoto_2007 conj_and_Nakagawa_Clark conj_and_Nakagawa_Zhang conj_and_Nakagawa_Uchimoto prep_with_model_Clark prep_with_model_Zhang prep_with_model_Uchimoto prep_with_model_Nakagawa nn_model_baseline poss_model_our prep_of_results_model nn_results_F1 prep_on_Comparison_CTB prep_of_Comparison_results dep_8_Comparison dep_8_Table dep_8_0.9590 num_Table_0.9152 num_Table_0.9134 dep_Table_0.9085 number_0.9085_0.9611 number_0.9590_0.9595
P09-1058	P08-1101	o	Zhang and Clark -LRB- 2008 -RRB- -LRB- Z&C 08 -RRB- generated CTB 3.0 from CTB 4.0	num_CTB_4.0 prep_from_CTB_CTB num_CTB_3.0 dobj_generated_CTB nsubj_generated_Clark nsubj_generated_Zhang num_Z&C_08 appos_Clark_2008 dep_Zhang_Z&C conj_and_Zhang_Clark
P09-1058	P08-1101	o	Zhang and Clark -LRB- 2008 -RRB- indicated that their results can not directly compare to the results of Shi and Wang -LRB- 2007 -RRB- due to different experimental settings	amod_settings_experimental amod_settings_different appos_Wang_2007 conj_and_Shi_Wang prep_due_to_results_settings prep_of_results_Wang prep_of_results_Shi det_results_the prep_to_compare_results advmod_compare_directly neg_compare_not aux_compare_can nsubj_compare_results mark_compare_that poss_results_their ccomp_indicated_compare nsubj_indicated_Clark nsubj_indicated_Zhang appos_Clark_2008 conj_and_Zhang_Clark
P09-1058	P08-1101	o	-LRB- 2008a 2008b -RRB- on CTB 5.0 and Zhang and Clark -LRB- 2008 -RRB- on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora	det_corpora_the prep_from_derived_corpora advmod_derived_only vmod_materials_derived nn_materials_training det_materials_the dobj_using_materials nn_tagging_POS conj_and_segmentation_tagging nn_segmentation_word amod_segmentation_joint prep_on_performances_tagging prep_on_performances_segmentation amod_performances_best det_performances_the xcomp_reported_using dobj_reported_performances nsubj_reported_they mark_reported_since num_CTB_4.0 appos_Clark_2008 conj_and_CTB_Clark conj_and_CTB_Zhang num_CTB_5.0 advcl_2008a_reported prep_on_2008a_CTB prep_on_2008a_Clark prep_on_2008a_Zhang prep_on_2008a_CTB dep_2008a_2008b
P09-1058	P08-1101	o	Following Zhang and Clark -LRB- 2008 -RRB- we first generated CTB 3.0 from CTB 4.0 using sentence IDs 110364	num_IDs_110364 dep_sentence_IDs dobj_using_sentence num_CTB_4.0 prep_from_CTB_CTB num_CTB_3.0 xcomp_generated_using dobj_generated_CTB advmod_generated_first nsubj_generated_we prep_following_generated_Clark prep_following_generated_Zhang appos_Clark_2008 conj_and_Zhang_Clark
P09-1058	P08-1101	o	Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto -LRB- 2007 -RRB- and Zhang and Clark -LRB- 2008 -RRB- on CTB 3.0	num_CTB_3.0 appos_Clark_2008 appos_Uchimoto_2007 conj_and_Nakagawa_Clark conj_and_Nakagawa_Zhang conj_and_Nakagawa_Uchimoto prep_with_model_Clark prep_with_model_Zhang prep_with_model_Uchimoto prep_with_model_Nakagawa nn_model_baseline poss_model_our prep_of_results_model nn_results_F1 det_results_the prep_on_compares_CTB dobj_compares_results nsubj_compares_Table num_Table_8
P09-1058	P08-1101	o	For example a perceptron algorithm is used for joint Chinese word segmentation and POS tagging -LRB- Zhang and Clark 2008 Jiang et al. 2008a Jiang et al. 2008b -RRB-	amod_2008b_Jiang dep_Jiang_al. nn_Jiang_et amod_2008a_Jiang dep_Jiang_al. nn_Jiang_et conj_and_Zhang_2008b conj_and_Zhang_2008a num_Zhang_2008 conj_and_Zhang_Clark dep_tagging_2008b dep_tagging_2008a dep_tagging_Clark dep_tagging_Zhang nn_tagging_POS conj_and_segmentation_tagging nn_segmentation_word amod_segmentation_Chinese amod_segmentation_joint prep_for_used_tagging prep_for_used_segmentation auxpass_used_is nsubjpass_used_algorithm prep_for_used_example nn_algorithm_perceptron det_algorithm_a rcmod_``_used
P09-1058	P08-1101	p	Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion -LRB- Ng and Low 2004 Nakagawa and Uchimoto 2007 Zhang and Clark 2008 Jiang et al. 2008a Jiang et al. 2008b -RRB-	appos_Jiang_2008b dep_Jiang_al. nn_Jiang_et appos_Jiang_2008a dep_Jiang_al. nn_Jiang_et num_Nakagawa_2008 conj_and_Nakagawa_Clark conj_and_Nakagawa_Zhang conj_and_Nakagawa_2007 conj_and_Nakagawa_Uchimoto dep_Ng_Jiang conj_and_Ng_Jiang conj_and_Ng_Clark conj_and_Ng_Zhang conj_and_Ng_2007 conj_and_Ng_Uchimoto conj_and_Ng_Nakagawa conj_and_Ng_2004 conj_and_Ng_Low appos_fashion_Jiang appos_fashion_Nakagawa appos_fashion_2004 appos_fashion_Low appos_fashion_Ng amod_fashion_pipelined det_fashion_a prep_over_improvements_fashion dobj_shown_improvements aux_shown_have nsubj_shown_segmentation amod_research_recent amod_attention_much conj_and_received_shown prep_in_received_research dobj_received_attention aux_received_have nsubj_received_tagging nsubj_received_segmentation amod_process_joint det_process_a nn_tagging_POS prep_in_segmentation_process conj_and_segmentation_tagging nn_segmentation_Word
P09-1058	P08-1102	o	In this paper we used CTB 5.0 -LRB- LDC2005T01 -RRB- as our main corpus defined the training development and test sets according to -LRB- Jiang et al. 2008a Jiang et al. 2008b -RRB- and designed our experiments to explore the impact of the training corpus size on our approach	poss_approach_our nn_size_corpus nn_size_training det_size_the prep_on_impact_approach prep_of_impact_size det_impact_the dobj_explore_impact aux_explore_to poss_experiments_our xcomp_designed_explore dobj_designed_experiments amod_2008b_Jiang dep_Jiang_al. nn_Jiang_et conj_and_Jiang_designed dep_Jiang_2008b appos_Jiang_2008a dep_Jiang_al. nn_Jiang_et nn_sets_test conj_and_training_sets conj_and_training_development det_training_the pobj_defined_designed pobj_defined_Jiang prepc_according_to_defined_to dobj_defined_sets dobj_defined_development dobj_defined_training amod_corpus_main poss_corpus_our appos_CTB_LDC2005T01 num_CTB_5.0 dep_used_defined prep_as_used_corpus dobj_used_CTB nsubj_used_we prep_in_used_paper det_paper_this
P09-1058	P08-1102	o	-LRB- Jiang et al. 2008a Jiang et al. 2008b -RRB-	appos_Jiang_2008b dep_Jiang_al. nn_Jiang_et dep_2008a_Jiang dep_2008a_Jiang dep_Jiang_al. nn_Jiang_et
P09-1058	P08-1102	o	For example a perceptron algorithm is used for joint Chinese word segmentation and POS tagging -LRB- Zhang and Clark 2008 Jiang et al. 2008a Jiang et al. 2008b -RRB-	amod_2008b_Jiang dep_Jiang_al. nn_Jiang_et amod_2008a_Jiang dep_Jiang_al. nn_Jiang_et conj_and_Zhang_2008b conj_and_Zhang_2008a num_Zhang_2008 conj_and_Zhang_Clark dep_tagging_2008b dep_tagging_2008a dep_tagging_Clark dep_tagging_Zhang nn_tagging_POS conj_and_segmentation_tagging nn_segmentation_word amod_segmentation_Chinese amod_segmentation_joint prep_for_used_tagging prep_for_used_segmentation auxpass_used_is nsubjpass_used_algorithm prep_for_used_example nn_algorithm_perceptron det_algorithm_a rcmod_``_used
P09-1058	P08-1102	p	Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion -LRB- Ng and Low 2004 Nakagawa and Uchimoto 2007 Zhang and Clark 2008 Jiang et al. 2008a Jiang et al. 2008b -RRB-	appos_Jiang_2008b dep_Jiang_al. nn_Jiang_et appos_Jiang_2008a dep_Jiang_al. nn_Jiang_et num_Nakagawa_2008 conj_and_Nakagawa_Clark conj_and_Nakagawa_Zhang conj_and_Nakagawa_2007 conj_and_Nakagawa_Uchimoto dep_Ng_Jiang conj_and_Ng_Jiang conj_and_Ng_Clark conj_and_Ng_Zhang conj_and_Ng_2007 conj_and_Ng_Uchimoto conj_and_Ng_Nakagawa conj_and_Ng_2004 conj_and_Ng_Low appos_fashion_Jiang appos_fashion_Nakagawa appos_fashion_2004 appos_fashion_Low appos_fashion_Ng amod_fashion_pipelined det_fashion_a prep_over_improvements_fashion dobj_shown_improvements aux_shown_have nsubj_shown_segmentation amod_research_recent amod_attention_much conj_and_received_shown prep_in_received_research dobj_received_attention aux_received_have nsubj_received_tagging nsubj_received_segmentation amod_process_joint det_process_a nn_tagging_POS prep_in_segmentation_process conj_and_segmentation_tagging nn_segmentation_Word
P09-1059	P08-1102	n	In addition the performance of the adapted model for Joint S&T obviously surpass that of -LRB- Jiang et al. 2008 -RRB- which achieves an F1 of 93.41 % for Joint S&T although with more complicated models and features	conj_and_models_features amod_models_complicated advmod_complicated_more pobj_with_features pobj_with_models pcomp_although_with nn_S&T_Joint num_%_93.41 prep_for_F1_S&T prep_of_F1_% det_F1_an dobj_achieves_F1 nsubj_achieves_which rcmod_Jiang_achieves amod_Jiang_2008 dep_Jiang_al. nn_Jiang_et prep_of_that_Jiang prep_surpass_although dobj_surpass_that advmod_surpass_obviously nsubj_surpass_performance prep_in_surpass_addition nn_S&T_Joint prep_for_model_S&T amod_model_adapted det_model_the prep_of_performance_model det_performance_the
P09-1059	P08-1102	o	following our previous work -LRB- Jiang et al. 2008 -RRB-	dep_al._2008 nn_al._et nn_al._Jiang amod_work_previous poss_work_our dep_following_al. pobj_following_work ccomp_``_following
P09-1059	P08-1102	p	It is an online training algorithm and has been successfully used in many NLP tasks such as POS tagging -LRB- Collins 2002 -RRB- parsing -LRB- Collins and Roark 2004 -RRB- Chinese word segmentation -LRB- Zhang and Clark 2007 Jiang et al. 2008 -RRB- and so on	advmod_on_so num_Jiang_2008 nn_Jiang_al. nn_Jiang_et dep_Zhang_Jiang num_Zhang_2007 conj_and_Zhang_Clark appos_segmentation_Clark appos_segmentation_Zhang nn_segmentation_word amod_segmentation_Chinese amod_Collins_2004 conj_and_Collins_Roark appos_parsing_Roark appos_parsing_Collins amod_Collins_2002 conj_and_tagging_on conj_and_tagging_segmentation conj_and_tagging_parsing dep_tagging_Collins nn_tagging_POS prep_such_as_tasks_on prep_such_as_tasks_segmentation prep_such_as_tasks_parsing prep_such_as_tasks_tagging nn_tasks_NLP amod_tasks_many prep_in_used_tasks advmod_used_successfully auxpass_used_been aux_used_has nsubjpass_used_It conj_and_algorithm_used nn_algorithm_training amod_algorithm_online det_algorithm_an cop_algorithm_is nsubj_algorithm_It
W09-1120	P08-1117	o	Many NLP systems use the output of supervised parsers -LRB- e.g. -LRB- Kwok et al. 2001 -RRB- for QA -LRB- Moldovan et al. 2003 -RRB- for IE -LRB- Punyakanok et al. 2008 -RRB- for SRL -LRB- Srikumar et al. 2008 -RRB- for Textual Inference and -LRB- Avramidis and Koehn 2008 -RRB- for MT -RRB-	prep_for_Avramidis_MT dep_Avramidis_2008 conj_and_Avramidis_Koehn nn_Inference_Textual amod_Srikumar_2008 dep_Srikumar_al. nn_Srikumar_et conj_and_SRL_Koehn conj_and_SRL_Avramidis prep_for_SRL_Inference appos_SRL_Srikumar num_Punyakanok_2008 dep_Punyakanok_al. nn_Punyakanok_et amod_Moldovan_2003 dep_Moldovan_al. nn_Moldovan_et prep_for_QA_IE appos_QA_Moldovan num_Kwok_2001 dep_Kwok_al. nn_Kwok_et prep_for_e.g._Avramidis prep_for_e.g._SRL appos_e.g._Punyakanok prep_for_e.g._QA appos_e.g._Kwok dep_parsers_e.g. amod_parsers_supervised prep_of_output_parsers det_output_the dobj_use_output nsubj_use_systems nn_systems_NLP amod_systems_Many
D08-1002	P08-1118	o	1 Introduction and Motivation Detecting contradictory statements is an important and challenging NLP task with a wide range of potential applications including analysis of political discourse of scientific literature and more -LRB- de Marneffe et al. 2008 Condoravdi et al. 2003 Harabagiu et al. 2006 -RRB-	num_Harabagiu_2006 nn_Harabagiu_al. nn_Harabagiu_et dep_Condoravdi_Harabagiu amod_Condoravdi_2003 dep_Condoravdi_al. nn_Condoravdi_et dep_al._2008 nn_al._et nn_al._Marneffe nn_al._de advmod_al._more amod_literature_scientific amod_discourse_political prep_of_analysis_discourse prep_including_applications_analysis amod_applications_potential prep_of_range_applications amod_range_wide det_range_a conj_and_task_Condoravdi conj_and_task_al. prep_of_task_literature prep_with_task_range nn_task_NLP amod_task_challenging amod_task_important det_task_an cop_task_is nsubj_task_statements nsubj_task_Introduction conj_and_important_challenging amod_statements_contradictory amod_statements_Detecting nn_statements_Motivation conj_and_Introduction_statements num_Introduction_1
D08-1103	P08-1118	o	Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases -LRB- The dementors caught Sirius Black / Black could not escape the dementors -RRB- and detecting contradictions -LRB- Marneffe et al. 2008 Voorhees 2008 -RRB- -LRB- Kyoto has a predominantly wet climate / It is mostly dry in Kyoto -RRB-	prep_in_dry_Kyoto advmod_dry_mostly cop_dry_is nsubj_dry_It amod_climate_wet det_climate_a advmod_wet_predominantly parataxis_has_dry dobj_has_climate nsubj_has_Kyoto dep_Voorhees_2008 dep_Marneffe_Voorhees dep_Marneffe_2008 dep_Marneffe_al. nn_Marneffe_et dobj_detecting_contradictions det_dementors_the dobj_escape_dementors neg_escape_not aux_escape_could nsubj_escape_Black dep_Black_escape nn_Black_Sirius dobj_caught_Black nsubj_caught_dementors det_dementors_The dep_paraphrases_caught dobj_generating_paraphrases conj_and_detecting_detecting conj_and_detecting_generating prepc_including_uses_detecting prepc_including_uses_generating prepc_including_uses_detecting amod_uses_many parataxis_has_has dep_has_Marneffe dobj_has_uses prep_between_degree_words prep_of_degree_antonymy det_degree_the dep_determining_has dobj_determining_degree advmod_determining_Automatically ccomp_``_determining
D09-1082	P08-1118	o	Some other researchers also work on detecting negative cases i.e. contradiction instead of entailment -LRB- de Marneffe et al. 2008 -RRB-	dep_al._2008 nn_al._et dep_Marneffe_al. nn_Marneffe_de dep_entailment_Marneffe prep_instead_of_contradiction_entailment nn_contradiction_i.e. amod_cases_negative dobj_detecting_cases dep_work_contradiction prepc_on_work_detecting advmod_work_also nsubj_work_researchers amod_researchers_other det_researchers_Some ccomp_``_work
E09-1025	P08-1118	o	This can be the base of a principled method for detecting structural contradictions -LRB- de Marneffe et al. 2008 -RRB-	dep_al._2008 nn_al._et dep_Marneffe_al. nn_Marneffe_de dep_contradictions_Marneffe amod_contradictions_structural dobj_detecting_contradictions prepc_for_method_detecting amod_method_principled det_method_a prep_of_base_method det_base_the cop_base_be aux_base_can nsubj_base_This
A88-1026	P85-1008	o	By associating natural language with concepts as they are entered into a knowledge A Model Of Semantic Analysis All of the following discussion is based on a model of semantic analysis similar to that proposed in -LRB- Hobbs 1985 -RRB-	amod_Hobbs_1985 dep_in_Hobbs prep_proposed_in vmod_that_proposed prep_to_similar_that amod_analysis_semantic amod_model_similar prep_of_model_analysis det_model_a prep_on_based_model auxpass_based_is agent_based_associating amod_discussion_following det_discussion_the prep_of_All_discussion nn_All_Analysis nn_All_Semantic prep_of_Model_All nn_Model_A nn_Model_knowledge det_Model_a prep_into_entered_Model auxpass_entered_are nsubjpass_entered_they mark_entered_as amod_language_natural advcl_associating_entered prep_with_associating_concepts dobj_associating_language
A88-1032	P85-1008	o	Hobbs Jerry -LRB- 1985 -RRB- Ontological Promiscuity Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics Chicago Illinois pp	appos_Chicago_pp appos_Chicago_Illinois appos_Linguistics_Chicago nn_Linguistics_Computational prep_for_Association_Linguistics det_Association_the prep_of_Meeting_Association amod_Meeting_Annual amod_Meeting_23rd det_Meeting_the prep_of_Proceedings_Meeting appos_Promiscuity_Proceedings amod_Promiscuity_Ontological nn_Promiscuity_Jerry nn_Promiscuity_Hobbs dep_Jerry_1985
A88-1034	P85-1008	o	Stage 2 processing is then free to assign to the compound any bracketing for which it 3The design of this level of Lucy is influenced by Hobbs -LRB- 1985 -RRB- which advocates a level of surfaey logical form with predicates close to actual English words and a structure similar to the syntactic structure of the sentence	det_sentence_the prep_of_structure_sentence amod_structure_syntactic det_structure_the prep_to_similar_structure amod_structure_similar det_structure_a amod_words_English amod_words_actual prep_close_to_predicates_words amod_form_logical nn_form_surfaey conj_and_level_structure prep_with_level_predicates prep_of_level_form det_level_a dobj_advocates_structure dobj_advocates_level nsubj_advocates_which rcmod_Hobbs_advocates appos_Hobbs_1985 agent_influenced_Hobbs auxpass_influenced_is nsubjpass_influenced_design dobj_influenced_it prep_for_influenced_which prep_of_level_Lucy det_level_this prep_of_design_level nn_design_3The rcmod_bracketing_influenced det_bracketing_any det_compound_the dobj_assign_bracketing prep_to_assign_compound aux_assign_to xcomp_free_assign advmod_free_then cop_free_is nsubj_free_processing num_processing_2 nn_processing_Stage
C90-2008	P85-1008	o	-LRB- 1 -RRB- a -RRB- ~ x e ' ~ y ? read -LRB- e ' x y -RRB- & book -LRB- y -RRB- b -RRB- ~ x 3 e e ' y past -LRB- e -RRB- & enjoy -LRB- e x e ' -RRB- & ? read -LRB- e ' x y -RRB- & book -LRB- y -RRB- c -RRB- 3 e e ' y past -LRB- e -RRB- & enjoy -LRB- e j e ' -RRB- & ? read -LRB- e ' j y -RRB- & book -LRB- y -RRB- We follow Hobbs -LRB- 1985 -RRB- Alshawi et al.	nn_al._et nn_al._Alshawi dep_Hobbs_al. appos_Hobbs_1985 dobj_follow_Hobbs nsubj_follow_We appos_book_y rcmod_y_follow conj_and_y_book nn_y_j dep_y_e dep_read_book dep_read_y dep_read_j dep_read_enjoy dep_read_e cc_j_& dep_j_e dep_j_e conj_and_e_enjoy dep_past_read amod_past_y dep_past_e dep_past_e num_past_3 dep_past_c dep_c_y cc_c_x dep_c_e conj_x_book dobj_read_past cc_read_& dep_read_e dep_read_e conj_x_e_e dep_enjoy_read conj_and_e_enjoy dep_past_enjoy dep_past_e prep_y_past dep_y_3 dep_y_b dep_b_e dep_b_e conj_x_b_3 num_b_~ dep_b_y dep_b_book dep_b_x dep_b_e conj_and_x_book dep_x_y acomp_read_y dep_read_y dep_y_~ dep_y_e dep_y_~ conj_x_~_e det_~_a dep_~_1
C90-2008	P85-1008	o	Thus we are lead to an ` ontologically promiscuous ' semantics -LRB- Hobbs 1985 -RRB-	amod_Hobbs_1985 dep_semantics_Hobbs amod_semantics_promiscuous advmod_semantics_ontologically det_semantics_an prep_to_lead_semantics cop_lead_are nsubj_lead_we advmod_lead_Thus
H86-1013	P85-1008	o	Independently in AI an effort arose to encode large amounts of commonsense knowledge -LRB- Hayes 1979 Hobbs and Moore 1985 Hobbs et al. 1985 -RRB-	dep_al._1985 nn_al._et nn_al._Hobbs dep_Hobbs_al. num_Hobbs_1985 conj_and_Hobbs_Moore dep_Hayes_Moore dep_Hayes_Hobbs amod_Hayes_1979 dep_knowledge_Hayes amod_knowledge_commonsense prep_of_amounts_knowledge amod_amounts_large dobj_encode_amounts aux_encode_to xcomp_arose_encode nsubj_arose_effort prep_in_arose_AI advmod_arose_Independently det_effort_an
H86-1013	P85-1008	o	We can stipulate the time line to be linearly ordered -LRB- although it is not in approaches that build ignorance of relative times into the representation of time -LRB- e.g. Hobbs 1974 -RRB- nor in approaches using branching futures -LRB- e.g. McDermott 1985 -RRB- -RRB- and we can stipulate it to be dense -LRB- although it is not in the situation calculus -RRB-	dep_situation_calculus det_situation_the prep_in_is_situation neg_is_not nsubj_is_it mark_is_although advcl_-LRB-_is cop_dense_be aux_dense_to xcomp_stipulate_dense dobj_stipulate_it aux_stipulate_can nsubj_stipulate_we conj_and_McDermott_stipulate appos_McDermott_1985 dep_,_stipulate dep_,_McDermott dep_-LRB-_e.g. amod_futures_branching dobj_using_futures pobj_in_approaches dep_Hobbs_using conj_nor_Hobbs_in amod_Hobbs_1974 ccomp_,_in ccomp_,_Hobbs dep_-LRB-_e.g. prep_of_representation_time det_representation_the amod_times_relative prep_of_ignorance_times prep_into_build_representation dobj_build_ignorance nsubj_build_that rcmod_approaches_build prep_in_is_approaches neg_is_not nsubj_is_it mark_is_although advcl_-LRB-_is advmod_ordered_linearly auxpass_ordered_be aux_ordered_to nn_line_time det_line_the xcomp_stipulate_ordered dobj_stipulate_line aux_stipulate_can nsubj_stipulate_We
H86-1013	P85-1008	o	We are encoding the knowledge as axioms in what is for the most part first-order logic described in Hobbs -LRB- 1985a -RRB- although quantification over predicates is sometimes convenient	advmod_convenient_sometimes cop_convenient_is nsubj_convenient_quantification mark_convenient_although prep_over_quantification_predicates appos_Hobbs_1985a prep_in_described_Hobbs amod_logic_first-order nn_logic_part amod_logic_most det_logic_the prep_for_is_logic nsubj_is_what prepc_in_axioms_is prep_as_knowledge_axioms det_knowledge_the advcl_encoding_convenient dep_encoding_described dobj_encoding_knowledge aux_encoding_are nsubj_encoding_We
H86-1013	P85-1008	o	Since so many concepts used in discourse are $ q ` aindependent a theory of granularity is also fundamental -LRB- see Hobbs 1985b -RRB-	nn_1985b_Hobbs dobj_see_1985b dep_fundamental_see advmod_fundamental_also cop_fundamental_is nsubj_fundamental_$ prep_of_theory_granularity det_theory_a amod_aindependent_q appos_$_theory dep_$_aindependent ccomp_are_fundamental prep_in_used_discourse vmod_concepts_are vmod_concepts_used amod_concepts_many advmod_many_so prep_since_``_concepts
J03-4002	P85-1008	o	Essentially we follow Hobbs -LRB- 1985 -RRB- in using a rich ontology and a representation scheme that makes explicit all the individuals and abstract objects -LRB- i.e. propositions facts/beliefs and eventualities -RRB- -LRB- Asher 1993 -RRB- involved in the LF interpretation of an utterance	det_utterance_an prep_of_interpretation_utterance nn_interpretation_LF det_interpretation_the prep_in_involved_interpretation nsubj_involved_Asher num_Asher_1993 conj_and_propositions_eventualities conj_and_propositions_facts/beliefs dep_propositions_i.e. dep_objects_eventualities dep_objects_facts/beliefs dep_objects_propositions amod_objects_abstract conj_and_individuals_objects det_individuals_the predet_individuals_all amod_individuals_explicit dobj_makes_objects dobj_makes_individuals nsubj_makes_that rcmod_scheme_makes nn_scheme_representation det_scheme_a conj_and_ontology_scheme amod_ontology_rich det_ontology_a dobj_using_scheme dobj_using_ontology appos_Hobbs_1985 dep_follow_involved prepc_in_follow_using dobj_follow_Hobbs nsubj_follow_we advmod_follow_Essentially
J87-3004	P85-1008	o	We can stipulate the time line to be linearly ordered -LRB- although it is not in approaches that build ignorance of relative times into the representation of time -LRB- e.g. Hobbs 1974 -RRB- nor in approaches employing branching futures -LRB- e.g. McDermott 1985 -RRB- -RRB- and we can stipulate it to be dense -LRB- although it is not in the situation calculus -RRB-	dep_situation_calculus det_situation_the prep_in_is_situation neg_is_not nsubj_is_it mark_is_although advcl_-LRB-_is cop_dense_be aux_dense_to xcomp_stipulate_dense dobj_stipulate_it aux_stipulate_can nsubj_stipulate_we appos_McDermott_1985 dep_e.g._McDermott amod_futures_branching dobj_employing_futures vmod_approaches_employing pobj_in_approaches conj_and_Hobbs_stipulate dep_Hobbs_e.g. conj_nor_Hobbs_in amod_Hobbs_1974 dep_,_stipulate dep_,_in dep_,_Hobbs dep_-LRB-_e.g. prep_of_representation_time det_representation_the amod_times_relative prep_of_ignorance_times prep_into_build_representation dobj_build_ignorance nsubj_build_that rcmod_approaches_build prep_in_is_approaches neg_is_not nsubj_is_it mark_is_although advcl_-LRB-_is advmod_ordered_linearly auxpass_ordered_be aux_ordered_to nn_line_time det_line_the xcomp_stipulate_ordered dobj_stipulate_line aux_stipulate_can nsubj_stipulate_We
J87-3004	P85-1008	o	Independently in artificial intelligence an effort arose to encode large amounts of commonsense knowledge -LRB- Hayes 1979 Hobbs and Moore 1985 Hobbs et al. 1985 -RRB-	dep_al._1985 nn_al._et nn_al._Hobbs dep_Hobbs_al. num_Hobbs_1985 conj_and_Hobbs_Moore dep_Hayes_Moore dep_Hayes_Hobbs amod_Hayes_1979 dep_knowledge_Hayes amod_knowledge_commonsense prep_of_amounts_knowledge amod_amounts_large dobj_encode_amounts aux_encode_to xcomp_arose_encode nsubj_arose_effort prep_in_arose_intelligence advmod_arose_Independently det_effort_an amod_intelligence_artificial
J87-3004	P85-1008	o	We are encoding the knowledge as axioms in what is for the most part a first-order logic described by Hobbs -LRB- 1985a -RRB- although quantification over predicates is sometimes convenient	advmod_convenient_sometimes cop_convenient_is nsubj_convenient_quantification mark_convenient_although prep_over_quantification_predicates appos_Hobbs_1985a agent_described_Hobbs advcl_logic_convenient vmod_logic_described amod_logic_first-order det_logic_a amod_part_most det_part_the dep_is_logic prep_for_is_part nsubj_is_what prepc_in_axioms_is prep_as_knowledge_axioms det_knowledge_the dobj_encoding_knowledge aux_encoding_are nsubj_encoding_We
J87-3004	P85-1008	o	Since so many concepts used in discourse are graindependent a theory of granularity is also fundamental -LRB- see Hobbs 1985b -RRB-	nn_1985b_Hobbs dobj_see_1985b dep_fundamental_see advmod_fundamental_also cop_fundamental_is nsubj_fundamental_theory advcl_fundamental_graindependent prep_of_theory_granularity det_theory_a cop_graindependent_are nsubj_graindependent_concepts mark_graindependent_Since prep_in_used_discourse vmod_concepts_used amod_concepts_many advmod_many_so
J98-4001	P85-1008	o	The separation of these two requirements 7 A more precise account of what it means to be able to identify an object is beyond the scope of this paper for further details see the discussions by Hobbs -LRB- 1985 -RRB- Appelt -LRB- 1985 -RRB- Kronfeld -LRB- 1986 1990 -RRB- and Morgenstern -LRB- 1988 -RRB-	appos_Morgenstern_1988 dep_1986_1990 dep_Kronfeld_1986 appos_Appelt_1985 conj_and_Hobbs_Morgenstern conj_and_Hobbs_Kronfeld conj_and_Hobbs_Appelt appos_Hobbs_1985 prep_by_discussions_Morgenstern prep_by_discussions_Kronfeld prep_by_discussions_Appelt prep_by_discussions_Hobbs det_discussions_the dobj_see_discussions amod_details_further det_paper_this prep_of_scope_paper det_scope_the dep_is_see prep_for_is_details prep_beyond_is_scope nsubj_is_separation det_object_an dobj_identify_object aux_identify_to xcomp_able_identify cop_able_be aux_able_to xcomp_means_able nsubj_means_it dobj_means_what prepc_of_account_means amod_account_precise det_account_A num_account_7 advmod_precise_more dep_requirements_account num_requirements_two det_requirements_these prep_of_separation_requirements det_separation_The ccomp_``_is
M93-1013	P85-1008	o	The proxy slot denotes a semantic individual which serves the role of an event instance in a partially Davidsonian scheme as in -LRB- Hobbs 1985 -RRB- or -LRB- Bayer d-Vilai n 1991 -RRB-	num_n_1991 nn_n_d-Vilai nn_n_Bayer conj_or_Hobbs_n num_Hobbs_1985 pobj_in_n pobj_in_Hobbs pcomp_as_in amod_scheme_Davidsonian det_scheme_a advmod_Davidsonian_partially dep_event_instance det_event_an prep_of_role_event det_role_the prep_in_serves_scheme dobj_serves_role nsubj_serves_which rcmod_individual_serves amod_individual_semantic det_individual_a prep_denotes_as dobj_denotes_individual nsubj_denotes_slot nn_slot_proxy det_slot_The ccomp_``_denotes
P86-1035	P85-1008	o	Independently in AI an effort arose to encode large amounts of commonsense knowledge -LRB- Hayes 1979 Hobbs and Moore 1985 Hobbs et al. 1985 -RRB-	dep_al._1985 nn_al._et nn_al._Hobbs dep_Hobbs_al. num_Hobbs_1985 conj_and_Hobbs_Moore dep_Hayes_Moore dep_Hayes_Hobbs amod_Hayes_1979 dep_knowledge_Hayes amod_knowledge_commonsense prep_of_amounts_knowledge amod_amounts_large dobj_encode_amounts aux_encode_to xcomp_arose_encode nsubj_arose_effort prep_in_arose_AI advmod_arose_Independently det_effort_an
P86-1035	P85-1008	o	We can stipulate the time line to be linearly ordered -LRB- although it is not in approaches that build ignorance of relative times into the representation of time -LRB- e.g. Hobbs 1974 -RRB- nor in approaches using branching futures -LRB- e.g. McDermott 1985 -RRB- -RRB- and we can stipulate it to be dense -LRB- although it is not in the situation calculus -RRB-	dep_situation_calculus det_situation_the prep_in_is_situation neg_is_not nsubj_is_it mark_is_although advcl_-LRB-_is cop_dense_be aux_dense_to xcomp_stipulate_dense dobj_stipulate_it aux_stipulate_can nsubj_stipulate_we conj_and_McDermott_stipulate appos_McDermott_1985 dep_,_stipulate dep_,_McDermott dep_-LRB-_e.g. amod_futures_branching dobj_using_futures pobj_in_approaches dep_Hobbs_using conj_nor_Hobbs_in amod_Hobbs_1974 ccomp_,_in ccomp_,_Hobbs dep_-LRB-_e.g. prep_of_representation_time det_representation_the amod_times_relative prep_of_ignorance_times prep_into_build_representation dobj_build_ignorance nsubj_build_that rcmod_approaches_build prep_in_is_approaches neg_is_not nsubj_is_it mark_is_although advcl_-LRB-_is advmod_ordered_linearly auxpass_ordered_be aux_ordered_to nn_line_time det_line_the xcomp_stipulate_ordered dobj_stipulate_line aux_stipulate_can nsubj_stipulate_We
P86-1035	P85-1008	o	Since so many concepts used in discourse are graindependent a theory of granularity is also fundamental -LRB- see Hobbs 1985b -RRB-	nn_1985b_Hobbs dobj_see_1985b dep_fundamental_see advmod_fundamental_also cop_fundamental_is nsubj_fundamental_theory advcl_fundamental_graindependent prep_of_theory_granularity det_theory_a cop_graindependent_are nsubj_graindependent_concepts mark_graindependent_Since prep_in_used_discourse vmod_concepts_used amod_concepts_many advmod_many_so
P88-1012	P85-1008	o	A subst -LRB- req cons -LRB- c argo -RRB- -RRB- st ^ rel -LRB- c z -RRB- s2 ~ -LRB- i k = ~ z \ -LSB- p ~ -LRB- :-RRB- ^ ~ -LRB- ~ -RRB- \ -RSB- -RRB- -LRB- Vi j w -RRB- n -LRB- i j w -RRB- D -LRB- 3z -RRB- cn -LRB- i j z w -RRB- -LRB- Vi j k w z c rel -RRB- prep -LRB- i j w -RRB- ^ np -LRB- j k x -RRB- A rel -LRB- c z -RRB- In 3 ptXi k ~ z \ -LSB- w -LRB- c z -RRB- \ -RSB- <c> Req -LRB- w -RRB- -RRB- For example the first axiom says that there is a sentence from point i to point k asserting eventuality e if there is a noun phrase from i to j referring to z and a verb phrase from j to k denoting predicate p with arguments arg8 and having an associated requirement req and there is -LRB- or for $ 3 can be assumed to be -RRB- an eventuality e of p 's being true of where c is related to or coercible from x -LRB- with an assumability cost of $ 20 -RRB- and the requirement req associated with p can be proved or for $ 10 assumed to hold of the arguments of p The symbol c & el denotes the conjunction of eventualities e and el -LRB- See Hobbs -LRB- 1985b -RRB- p. 35 -RRB-	num_p._35 appos_Hobbs_p. appos_Hobbs_1985b dobj_See_Hobbs dep_el_See conj_and_e_el dep_eventualities_el dep_eventualities_e prep_of_conjunction_eventualities det_conjunction_the dobj_denotes_conjunction nsubj_denotes_el nsubj_denotes_c conj_and_c_el nn_c_symbol det_c_The prep_of_arguments_p det_arguments_the prep_of_hold_arguments aux_hold_to xcomp_assumed_hold num_$_10 prep_for_proved_$ cc_proved_or auxpass_proved_be aux_proved_can nsubjpass_proved_requirement nsubjpass_proved_is nsubjpass_proved_req prep_with_associated_p vmod_req_associated dep_requirement_req det_requirement_the num_$_20 prep_of_cost_$ nn_cost_assumability det_cost_an pobj_with_cost dep_coercible_with prep_from_coercible_x conj_or_to_coercible prep_related_coercible prep_related_to auxpass_related_is nsubjpass_related_c advmod_related_where prepc_of_true_related cop_true_being nsubj_true_p possessive_p_'s prepc_of_eventuality_true dep_eventuality_e det_eventuality_an cop_eventuality_be aux_eventuality_to xcomp_assumed_eventuality auxpass_assumed_be aux_assumed_can prep_for_assumed_$ num_$_3 conj_or_is_assumed expl_is_there conj_and_req_requirement conj_and_req_assumed conj_and_req_is rcmod_requirement_proved amod_requirement_associated det_requirement_an dobj_having_requirement nn_arg8_arguments nn_p_predicate prep_with_denoting_arg8 dobj_denoting_p vmod_k_denoting amod_phrase_verb det_phrase_a conj_and_z_phrase conj_and_referring_having prep_to_referring_k prep_from_referring_j prep_to_referring_phrase prep_to_referring_z dep_j_having dep_j_referring aux_j_to vmod_i_j nn_phrase_noun det_phrase_a prep_from_is_i nsubj_is_phrase expl_is_there mark_is_if dep_eventuality_e dobj_asserting_eventuality nsubj_asserting_k xcomp_point_asserting aux_point_to nn_i_point vmod_sentence_point prep_from_sentence_i det_sentence_a advcl_is_is nsubj_is_sentence expl_is_there mark_is_that parataxis_says_denotes dep_says_assumed ccomp_says_is nsubj_says_axiom prep_for_says_example nsubj_says_<c> amod_axiom_first det_axiom_the appos_Req_w appos_<c>_Req dep_<c>_\ dep_<c>_z dep_<c>_~ dep_<c>_cn dep_c_z dep_w_\ appos_w_c dep_\_w appos_ptXi_k num_ptXi_3 dep_c_z appos_rel_c det_rel_A appos_j_x appos_j_k dep_np_rel dep_np_j num_np_^ nn_np_prep appos_j_w nn_j_i dep_prep_j appos_Vi_rel appos_Vi_c conj_Vi_z conj_Vi_w conj_Vi_k conj_Vi_j appos_j_w conj_j_z nn_j_i prep_in_cn_ptXi dep_cn_np dep_cn_Vi dep_cn_j nn_cn_D dep_cn_j nn_cn_n dep_cn_k nn_cn_~ appos_D_3z dep_j_w nn_j_i appos_Vi_w appos_Vi_j dep_~_\ appos_~_~ num_~_^ nn_~_:-RRB- dep_~_~ nn_~_p dep_z_\ nn_z_~ dep_k_Vi dep_k_~ dep_k_z amod_k_= nn_k_i nn_~_s2 nn_~_rel dep_c_z appos_rel_c num_rel_^ nn_rel_st nn_rel_subst dep_c_argo dep_cons_c appos_req_cons dep_subst_req det_subst_A
P88-1012	P85-1008	o	SSee Hobbs -LRB- 1985a -RRB- for explanation of this notation for events	det_notation_this prep_for_explanation_events prep_of_explanation_notation prep_for_Hobbs_explanation appos_Hobbs_1985a nn_Hobbs_SSee
P88-1012	P85-1008	o	4For justification for this kind of logical form for sentences with quantifiers and inteusional operators see Hobbs -LRB- 1983 -RRB- and Hobbs -LRB- 1985a -RRB-	appos_Hobbs_1985a conj_and_Hobbs_Hobbs appos_Hobbs_1983 dobj_see_Hobbs dobj_see_Hobbs nsubj_see_justification amod_operators_inteusional conj_and_quantifiers_operators prep_with_sentences_operators prep_with_sentences_quantifiers amod_form_logical prep_of_kind_form det_kind_this prep_for_justification_sentences prep_for_justification_kind nn_justification_4For
P96-1027	P85-1008	o	They have made semantic formalisms like those now usually associated with Davison -LRB- Davidson 1980 Parsons 1990 -RRB- attractive in artificial intelligence for many years -LRB- Hobbs 1985 Kay 1970 -RRB-	appos_Hobbs_1970 appos_Hobbs_Kay num_Hobbs_1985 amod_years_many amod_intelligence_artificial dep_attractive_Hobbs prep_for_attractive_years prep_in_attractive_intelligence nsubj_attractive_Davidson num_Davidson_1990 conj_Davidson_Parsons conj_Davidson_1980 nn_Davidson_Davison prepc_with_associated_attractive advmod_associated_usually advmod_associated_now vmod_those_associated prep_like_formalisms_those amod_formalisms_semantic dobj_made_formalisms aux_made_have nsubj_made_They
P97-1026	P85-1008	o	First we adopt an ONTOLOGICALLY PROMISCUOUS representation -LRB- Hobbs 1985 -RRB- that includes a wide variety of types of entities	prep_of_types_entities prep_of_variety_types amod_variety_wide det_variety_a dobj_includes_variety nsubj_includes_that appos_Hobbs_1985 rcmod_representation_includes dep_representation_Hobbs nn_representation_PROMISCUOUS nn_representation_ONTOLOGICALLY det_representation_an dobj_adopt_representation nsubj_adopt_we advmod_adopt_First
W03-0906	P85-1008	p	We do not completely rule out the possibility that some more sophisticated ontologically promiscuous first-order analysis -LRB- perhaps along the lines of -LRB- Hobbs 1985 -RRB- -RRB- might account for these kinds of monotonicity inferences	nn_inferences_monotonicity prep_of_kinds_inferences det_kinds_these prep_for_account_kinds aux_account_might nsubj_account_analysis mark_account_that dep_Hobbs_1985 prep_of_lines_Hobbs det_lines_the prep_along_analysis_lines advmod_analysis_perhaps amod_analysis_first-order amod_analysis_promiscuous amod_analysis_sophisticated advmod_promiscuous_ontologically advmod_sophisticated_more det_sophisticated_some ccomp_possibility_account det_possibility_the dobj_rule_possibility prt_rule_out advmod_rule_completely neg_rule_not aux_rule_do nsubj_rule_We
W03-0906	P85-1008	p	More sophisticated first-order accounts -LRB- Hirst 1991 Hobbs 1985 -RRB- may be extendable to bear this load	det_load_this dobj_bear_load aux_bear_to xcomp_extendable_bear cop_extendable_be aux_extendable_may nsubj_extendable_accounts dep_Hobbs_1985 dep_Hirst_Hobbs appos_Hirst_1991 appos_accounts_Hirst amod_accounts_first-order amod_accounts_sophisticated advmod_sophisticated_More
W03-2806	P85-1008	o	The MLFs use reification to achieve flat expressions very much in the line of Davidson -LRB- 1967 -RRB- Hobbs -LRB- 1985 -RRB- and Copestake et al.	nn_al._et nn_al._Copestake appos_Hobbs_1985 conj_and_Davidson_al. conj_and_Davidson_Hobbs appos_Davidson_1967 prep_of_line_al. prep_of_line_Hobbs prep_of_line_Davidson det_line_the prep_in_much_line advmod_much_very amod_expressions_flat dobj_achieve_expressions aux_achieve_to advmod_use_much vmod_use_achieve dobj_use_reification nsubj_use_MLFs det_MLFs_The
W04-2803	P85-1008	o	Note that the predicate language representation utilized by Carmel-Tools is in the style of Davidsonian event based semantics -LRB- Hobbs 1985 -RRB-	amod_Hobbs_1985 dep_semantics_Hobbs amod_semantics_based dep_event_semantics amod_event_Davidsonian prep_of_style_event det_style_the prep_in_is_style nsubj_is_representation mark_is_that agent_utilized_Carmel-Tools vmod_representation_utilized nn_representation_language nn_representation_predicate det_representation_the ccomp_Note_is
W04-2803	P85-1008	o	After the parser produces a semantic feature structure representation of the sentence predicate mapping rules then match against that representation in order to produce a predicate language representation in the style of Davidsonian event based semantics -LRB- Davidson 1967 Hobbs 1985 -RRB- as mentioned above	advmod_mentioned_above mark_mentioned_as dep_Hobbs_1985 advcl_Davidson_mentioned dep_Davidson_Hobbs appos_Davidson_1967 dep_semantics_Davidson amod_semantics_based dep_event_semantics amod_event_Davidsonian prep_of_style_event det_style_the prep_in_representation_style nn_representation_language nn_representation_predicate det_representation_a dobj_produce_representation aux_produce_to dep_produce_order mark_produce_in det_representation_that advcl_match_produce prep_against_match_representation advmod_match_then nsubj_match_sentence mark_match_of nn_rules_mapping nn_rules_predicate appos_sentence_rules det_sentence_the rcmod_representation_match nn_representation_structure nn_representation_feature amod_representation_semantic det_representation_a dobj_produces_representation nsubj_produces_parser mark_produces_After det_parser_the advcl_``_produces
W05-1609	P85-1008	o	We adopt their idea of an utterance as a description generated from a communicative goal and also use an ontologically promiscuous formalism for representing meaning -LSB- Hobbs 1985 -RSB-	amod_Hobbs_1985 dep_representing_Hobbs dobj_representing_meaning amod_formalism_promiscuous advmod_formalism_ontologically det_formalism_an prepc_for_use_representing dobj_use_formalism advmod_use_also nsubj_use_We amod_goal_communicative det_goal_a prep_from_generated_goal vmod_description_generated det_description_a det_utterance_an prep_of_idea_utterance poss_idea_their conj_and_adopt_use prep_as_adopt_description dobj_adopt_idea nsubj_adopt_We
W05-1609	P85-1008	o	2 Background 2.1 Hybrid Logic Dependency Semantics Hybrid Logic Dependency Semantics -LRB- HLDS -LSB- Kruijff 2001 Baldridge and Kruijff 2002 -RSB- -RRB- is an ontologically promiscuous -LSB- Hobbs 1985 -RSB- framework for representing the propositional content -LRB- or meaning -RRB- of an expression as an ontologically richly sorted relational structure	amod_structure_relational dobj_sorted_structure advmod_sorted_richly advmod_sorted_ontologically nsubj_sorted_an mark_sorted_as det_expression_an cc_meaning_or prep_of_content_expression dep_content_meaning amod_content_propositional det_content_the advcl_representing_sorted dobj_representing_content prepc_for_framework_representing amod_framework_promiscuous det_framework_an cop_framework_is nsubj_framework_HLDS dep_framework_Semantics dep_Hobbs_1985 dep_promiscuous_Hobbs advmod_promiscuous_ontologically appos_Baldridge_2002 conj_and_Baldridge_Kruijff dep_Kruijff_Kruijff dep_Kruijff_Baldridge dep_Kruijff_2001 dep_HLDS_Kruijff nn_Semantics_Dependency nn_Semantics_Logic nn_Semantics_Hybrid nn_Semantics_Semantics nn_Semantics_Dependency nn_Semantics_Logic nn_Semantics_Hybrid num_Semantics_2.1 nn_Semantics_Background num_Semantics_2
W07-1430	P85-1008	o	Nevertheless as -LRB- Hobbs 1985 -RRB- and others have argued semantic representations for natural language need not be higher-order in that ontological promiscuity can solve the problem	det_problem_the dobj_solve_problem aux_solve_can nsubj_solve_promiscuity mark_solve_that amod_promiscuity_ontological prepc_in_higher-order_solve cop_higher-order_be neg_higher-order_not aux_higher-order_need nsubj_higher-order_representations ccomp_higher-order_argued amod_language_natural prep_for_representations_language amod_representations_semantic aux_argued_have dep_argued_others dep_argued_as advmod_argued_Nevertheless amod_Hobbs_1985 conj_and_as_others dep_as_Hobbs
W07-1430	P85-1008	o	Moreover as stated in -LRB- Hobbs 1985 -RRB- we assume that the alleged predicate is existentially opaque in its second argument	amod_argument_second poss_argument_its prep_in_opaque_argument advmod_opaque_existentially cop_opaque_is nsubj_opaque_predicate mark_opaque_that amod_predicate_alleged det_predicate_the ccomp_assume_opaque nsubj_assume_we advcl_assume_stated advmod_assume_Moreover amod_Hobbs_1985 dep_in_Hobbs prep_stated_in mark_stated_as
W07-1431	P85-1008	o	Doing inference with representations close to natural language has also been advocated by Jerry Hobbs as in -LRB- Hobbs 1985 -RRB-	amod_Hobbs_1985 dep_in_Hobbs pcomp_as_in nn_Hobbs_Jerry prep_advocated_as agent_advocated_Hobbs auxpass_advocated_been advmod_advocated_also aux_advocated_has csubjpass_advocated_Doing amod_language_natural prep_close_to_representations_language prep_with_Doing_representations dobj_Doing_inference
W96-0410	P85-1008	o	Second in keeping with ontological promiscuity -LRB- Hobbs 1985 -RRB- we represent the importance of attributes by the salience of events and states in the discourse model -- these states and events now have the same status in the discourse model as any other entities	amod_entities_other det_entities_any nn_model_discourse det_model_the prep_in_status_model amod_status_same det_status_the prep_as_have_entities dobj_have_status advmod_have_now nsubj_have_events nsubj_have_states conj_and_states_events det_states_these nn_model_discourse det_model_the conj_and_events_states prep_of_salience_states prep_of_salience_events det_salience_the prep_in_attributes_model prep_by_attributes_salience prep_of_importance_attributes det_importance_the parataxis_represent_have dobj_represent_importance nsubj_represent_we prepc_in_represent_keeping advmod_represent_Second amod_Hobbs_1985 dep_promiscuity_Hobbs amod_promiscuity_ontological prep_with_keeping_promiscuity
W96-0410	P85-1008	o	First as originally advocated by Hobbs -LRB- 1985 -RRB- we adopt an ONTOLOGICALLY PROMISCUOUS representation that includes a wide variety of types of entities	prep_of_types_entities prep_of_variety_types amod_variety_wide det_variety_a dobj_includes_variety nsubj_includes_that rcmod_representation_includes nn_representation_PROMISCUOUS nn_representation_ONTOLOGICALLY det_representation_an dobj_adopt_representation nsubj_adopt_we advcl_adopt_advocated advmod_adopt_First appos_Hobbs_1985 prep_by_advocated_Hobbs advmod_advocated_originally mark_advocated_as
C88-1026	P86-1010	n	Previous literature on GB parsing / Wehrli 1984 Sharp 1985 Kashket 1986 Kuhns 1986 Abney 1986/has not addressed the issue of implementation of the Binding theory -RRB- The present paper intends in part to fill this gap	det_gap_this dobj_fill_gap aux_fill_to xcomp_intends_fill prep_in_intends_part nsubj_intends_paper amod_paper_present det_paper_The nn_theory_Binding det_theory_the prep_of_implementation_theory prep_of_issue_implementation det_issue_the ccomp_addressed_intends dobj_addressed_issue neg_addressed_not auxpass_addressed_1986/has nsubjpass_addressed_Abney num_Kuhns_1986 num_Kashket_1986 appos_Sharp_1985 parataxis_Wehrli_addressed conj_Wehrli_Kuhns conj_Wehrli_Kashket conj_Wehrli_Sharp conj_Wehrli_1984 dep_parsing_Wehrli nn_parsing_GB prep_on_literature_parsing amod_literature_Previous
J90-4003	P86-1010	n	Formal complexity analysis has not been carried out but my algorithm is simpler at least conceptually than the variable-word-order parsers of Johnson -LRB- 1985 -RRB- Kashket -LRB- 1986 -RRB- and Abramson and Dahl -LRB- 1989 -RRB-	appos_Dahl_1989 appos_Kashket_1986 conj_and_Johnson_Dahl conj_and_Johnson_Abramson conj_and_Johnson_Kashket appos_Johnson_1985 prep_of_parsers_Dahl prep_of_parsers_Abramson prep_of_parsers_Kashket prep_of_parsers_Johnson nn_parsers_variable-word-order det_parsers_the prep_than_at_parsers advmod_at_conceptually pobj_at_least dep_simpler_at cop_simpler_is nsubj_simpler_algorithm poss_algorithm_my conj_but_carried_simpler prt_carried_out auxpass_carried_been neg_carried_not aux_carried_has nsubjpass_carried_analysis nn_analysis_complexity amod_analysis_Formal
P87-1007	P86-1010	n	Although the parser is not yet complete we expect that its breath of coverage of the language will be substantially larger than that of other Government-binding parsers recently reported in the literature -LRB- Kashket -LRB- 1986 -RRB- Kuhns -LRB- 1986 -RRB- Sharp -LRB- 1985 -RRB- and Wehrli -LRB- 1984 -RRB- -RRB-	appos_Wehrli_1984 dep_Sharp_1985 appos_Kuhns_1986 conj_and_Kashket_Wehrli conj_and_Kashket_Sharp appos_Kashket_Kuhns appos_Kashket_1986 dep_literature_Wehrli dep_literature_Sharp dep_literature_Kashket det_literature_the prep_in_reported_literature advmod_reported_recently nsubj_reported_that mark_reported_than amod_parsers_Government-binding amod_parsers_other prep_of_that_parsers ccomp_larger_reported advmod_larger_substantially cop_larger_be aux_larger_will nsubj_larger_breath mark_larger_that det_language_the prep_of_coverage_language prep_of_breath_coverage poss_breath_its ccomp_expect_larger nsubj_expect_we advcl_expect_complete advmod_complete_yet neg_complete_not cop_complete_is nsubj_complete_parser mark_complete_Although det_parser_the
P93-1015	P86-1010	o	There are similarities with dependency grammars here because such constraint graphs are also produced by dependency grammars -LRB- Covington 1990 -RRB- -LRB- Kashket 1986 -RRB-	amod_Kashket_1986 dep_Covington_1990 dep_grammars_Kashket appos_grammars_Covington nn_grammars_dependency agent_produced_grammars advmod_produced_also auxpass_produced_are nsubjpass_produced_graphs mark_produced_because nn_graphs_constraint amod_graphs_such nn_grammars_dependency advcl_similarities_produced advmod_similarities_here prep_with_similarities_grammars nsubj_are_similarities expl_are_There ccomp_``_are
C90-2071	P88-1012	o	The construction is defined in Fillmore 's -LRB- 1988 -RRB- Construction Grammar as a pairing of a syntactic pattern with a meaning structure they are similar to signs in HPSG -LRB- Pollard & Sag 1987 -RRB- and pattern-concept pairs -LRB- Wilensky & Arens 1980 Wilensky et al. 1988 -RRB-	dep_al._1988 nn_al._et nn_al._Wilensky num_Wilensky_1980 conj_and_Wilensky_Arens dep_pairs_Arens dep_pairs_Wilensky amod_pairs_pattern-concept num_Pollard_1987 conj_and_Pollard_Sag conj_and_HPSG_pairs appos_HPSG_Sag appos_HPSG_Pollard prep_in_signs_pairs prep_in_signs_HPSG dep_similar_al. prep_to_similar_signs cop_similar_are nsubj_similar_they nn_structure_meaning det_structure_a prep_with_pattern_structure amod_pattern_syntactic det_pattern_a dep_pairing_similar prep_of_pairing_pattern det_pairing_a dep_``_pairing nn_Grammar_Construction dep_Grammar_1988 poss_Grammar_Fillmore dep_defined_as prep_in_defined_Grammar auxpass_defined_is nsubjpass_defined_construction det_construction_The
C90-3028	P88-1012	o	-LRB- See also Kaplan et al. 1988 on the latter point -RRB-	amod_point_latter det_point_the prep_on_Kaplan_point num_Kaplan_1988 nn_Kaplan_al. nn_Kaplan_et dep_also_Kaplan advmod_See_also dep_''_See
C90-3028	P88-1012	o	It has been implemented in the TACITUS System -LRB- Itobbs et al. 1988 1990 Stickel 1989 -RRB- and has been applied to several varieties of text	prep_of_varieties_text amod_varieties_several prep_to_applied_varieties auxpass_applied_been aux_applied_has nsubjpass_applied_It dep_Stickel_1989 dep_Itobbs_Stickel appos_Itobbs_1990 appos_Itobbs_1988 dep_Itobbs_al. nn_Itobbs_et nn_System_TACITUS det_System_the conj_and_implemented_applied dep_implemented_Itobbs prep_in_implemented_System auxpass_implemented_been aux_implemented_has nsubjpass_implemented_It
C90-3028	P88-1012	p	Recently an elegant approach to inference in discourse interpretation has been developed at a number of sites -LRB- e.g. ltobbs et al. 1988 Charniak and Goldman 1988 Norvig 1987 -RRB- all based on tim notion of abduction and we have begun to explore its potential application to machine translation	nn_translation_machine prep_to_application_translation amod_application_potential poss_application_its dobj_explore_application aux_explore_to xcomp_begun_explore aux_begun_have nsubj_begun_we prep_of_notion_abduction nn_notion_tim pobj_all_notion prepc_based_on_all_on dep_Norvig_1987 num_Charniak_1988 conj_and_Charniak_Goldman dep_al._all dep_al._Norvig conj_al._Goldman conj_al._Charniak conj_al._1988 nn_al._et conj_and_ltobbs_begun advmod_ltobbs_al. dep_e.g._begun dep_e.g._ltobbs dep_number_e.g. prep_of_number_sites det_number_a prep_at_developed_number auxpass_developed_been aux_developed_has nsubjpass_developed_approach advmod_developed_Recently nn_interpretation_discourse prep_in_approach_interpretation prep_to_approach_inference amod_approach_elegant det_approach_an
C90-3040	P88-1012	o	Probability Based Commensurability Charniak and Goldman -LRB- 1988 -RRB- started out with a model very similar to Hobbs et al. but became concerned with 227 the lack of theoretical grounding for Ihe number in rules much as we we.re	nsubj_we.re_we mark_we.re_as advmod_we.re_much nn_number_Ihe prep_for_grounding_number amod_grounding_theoretical prep_of_lack_grounding det_lack_the num_lack_227 prep_with_concerned_lack advcl_became_we.re prep_in_became_rules acomp_became_concerned csubj_became_Based dep_Hobbs_al. nn_Hobbs_et prep_to_similar_Hobbs advmod_similar_very amod_model_similar det_model_a conj_but_started_became prep_with_started_model prt_started_out csubj_started_Based appos_Goldman_1988 conj_and_Charniak_Goldman nn_Charniak_Commensurability dobj_Based_Goldman dobj_Based_Charniak nsubj_Based_Probability
C92-2108	P88-1012	o	We suggest two ways to do it a version of \ -LSB- \ -LSB- obbs et al 's \ -LSB- 1988 1990 \ -RSB- Generation as Abduction and the Interactive Defaults strategy introduced by aoshi et al \ -LSB- 1984a 1984b 1986 \ -RSB-	num_\_1986 appos_1984a_\ appos_1984a_1984b nn_\_al dep_aoshi_1984a dep_aoshi_\ nn_aoshi_et agent_introduced_aoshi vmod_strategy_introduced nn_strategy_Defaults nn_strategy_Interactive det_strategy_the prep_as_Generation_Abduction dep_Generation_\ nn_Generation_\ num_Generation_\ num_\_1990 num_\_1988 poss_\_obbs dep_obbs_al nn_obbs_et conj_and_version_strategy dep_version_Generation prep_of_version_\ det_version_a dobj_do_it aux_do_to vmod_ways_do num_ways_two dep_suggest_strategy dep_suggest_version dobj_suggest_ways nsubj_suggest_We
C92-2108	P88-1012	o	Modulo more minor differences these notions are close to the ideas of interpretation as abduction -LRB- Hobbs et al \ -LSB- 1988 \ -RSB- -RRB- and generation as abduction -LRB- ltobbs et al \ -LSB- 1990:26 -28 \ -RSB- -RRB- where we take abduction in the former case for instance to be a process returning a temporal-causal structure which can explain the utterance in context	det_utterance_the prep_in_explain_context dobj_explain_utterance aux_explain_can nsubj_explain_which rcmod_structure_explain amod_structure_temporal-causal det_structure_a dobj_returning_structure vmod_process_returning det_process_a cop_process_be aux_process_to amod_case_former det_case_the dobj_take_abduction nsubj_take_we advmod_take_where num_\_-28 number_-28_1990:26 nn_\_al dep_ltobbs_\ dep_ltobbs_\ nn_ltobbs_et num_\_1988 nn_\_al dep_Hobbs_\ dep_Hobbs_\ nn_Hobbs_et prep_of_ideas_interpretation det_ideas_the vmod_close_process prep_for_close_instance prep_in_close_case advcl_close_take dep_close_ltobbs prep_as_close_abduction conj_and_close_generation dep_close_Hobbs prep_as_close_abduction prep_to_close_ideas cop_close_are nsubj_close_notions advcl_close_differences det_notions_these amod_differences_minor nn_differences_Modulo advmod_minor_more
H90-1012	P88-1012	o	Ordinary Prologstyle backchaining deduction is augmented with the capability of making assumptions and of factoring two goal literals that are unifiable -LRB- see Hobbs et al. 1988 -RRB-	num_Hobbs_1988 nn_Hobbs_al. nn_Hobbs_et dobj_see_Hobbs dep_unifiable_see cop_unifiable_are nsubj_unifiable_that rcmod_literals_unifiable nn_literals_goal num_literals_two dobj_factoring_literals pcomp_of_factoring dobj_making_assumptions prepc_of_capability_making det_capability_the conj_and_augmented_of prep_with_augmented_capability cop_augmented_is nsubj_augmented_deduction dep_augmented_Prologstyle amod_deduction_backchaining nn_Prologstyle_Ordinary
J90-2003	P88-1012	o	We borrow this useful term from the Core Language Engine project -LRB- Alshawi et al. 1988 1989 -RRB-	num_al._1988 nn_al._et dep_Alshawi_1989 advmod_Alshawi_al. appos_project_Alshawi nn_project_Engine nn_project_Language nn_project_Core det_project_the amod_term_useful det_term_this prep_from_borrow_project dobj_borrow_term nsubj_borrow_We
J90-2003	P88-1012	o	-LRB- 1972 -RRB- later elaborations and refinements have been implemented in a number of systems notably CHAT-80 -LRB- Pereira 1983 -RRB- TEAM -LRB- Grosz et al. 1986 -RRB- and CLE -LRB- Moran 1988 Alshawi et al. 1989 -RRB-	dep_al._1989 nn_al._et nn_al._Alshawi dep_Moran_al. num_Moran_1988 dep_CLE_Moran dep_al._1986 nn_al._et advmod_Grosz_al. appos_TEAM_Grosz num_Pereira_1983 conj_and_CHAT-80_CLE conj_and_CHAT-80_TEAM appos_CHAT-80_Pereira advmod_CHAT-80_notably appos_number_CLE appos_number_TEAM appos_number_CHAT-80 prep_of_number_systems det_number_a prep_in_implemented_number auxpass_implemented_been aux_implemented_have nsubjpass_implemented_refinements nsubjpass_implemented_elaborations conj_and_elaborations_refinements advmod_elaborations_later parataxis_1972_implemented dep_''_1972
J90-2003	P88-1012	o	1.2.2 SPECIFIC SYNTACTIC AND SEMANTIC ASSUMPTIONS The basic scheme or some not too distant relative is the one used in many large-scale implemented systems as examples we can quote TEAM -LRB- Grosz et al. 1987 -RRB- PUNDIT -LRB- Dahl et al. 1987 -RRB- TACITUS -LRB- Hobbs et al. 1988 -RRB- MODL -LRB- McCord 1987 -RRB- CLE -LRB- Alshawi et al. 1989 -RRB- and SNACK-85 -LRB- Rayner and Banks 1986 -RRB-	num_Banks_1986 conj_and_Rayner_Banks dep_SNACK-85_Banks dep_SNACK-85_Rayner dep_al._1989 nn_al._et advmod_Alshawi_al. num_McCord_1987 appos_MODL_McCord dep_al._1988 nn_al._et advmod_Hobbs_al. dep_al._1987 nn_al._et advmod_Dahl_al. appos_PUNDIT_Dahl dep_al._1987 nn_al._et advmod_Grosz_al. conj_and_TEAM_SNACK-85 appos_TEAM_Alshawi conj_and_TEAM_CLE conj_and_TEAM_MODL appos_TEAM_Hobbs conj_and_TEAM_TACITUS conj_and_TEAM_PUNDIT appos_TEAM_Grosz dobj_quote_SNACK-85 dobj_quote_CLE dobj_quote_MODL dobj_quote_TACITUS dobj_quote_PUNDIT dobj_quote_TEAM aux_quote_can nsubj_quote_we prep_as_quote_examples amod_systems_implemented amod_systems_large-scale amod_systems_many prep_in_used_systems parataxis_one_quote vmod_one_used det_one_the cop_one_is nsubj_one_relative nsubj_one_ASSUMPTIONS nsubj_one_SYNTACTIC amod_relative_distant det_relative_some advmod_distant_too neg_distant_not amod_scheme_basic det_scheme_The nn_ASSUMPTIONS_SEMANTIC conj_or_SYNTACTIC_relative dep_SYNTACTIC_scheme conj_and_SYNTACTIC_ASSUMPTIONS nn_SYNTACTIC_SPECIFIC num_SYNTACTIC_1.2.2 ccomp_``_one
J90-2003	P88-1012	o	It also has close links with theoretical work in situation semantics -LRB- Pollard and Sag 1988 Fenstad et al. 1987 -RRB-	dep_al._1987 nn_al._et nn_al._Fenstad num_Sag_1988 dep_Pollard_al. conj_and_Pollard_Sag nn_semantics_situation amod_work_theoretical prep_in_links_semantics prep_with_links_work amod_links_close dep_has_Sag dep_has_Pollard dobj_has_links advmod_has_also nsubj_has_It
J91-4003	P88-1012	o	Walker et al. \ -LSB- forthcoming \ -RSB- and Boguraev and Briscoe \ -LSB- 1988 \ -RSB- -RRB-	num_\_1988 num_Briscoe_\ dep_Boguraev_\ conj_and_Boguraev_Briscoe amod_\_forthcoming conj_and_\_Briscoe conj_and_\_Boguraev appos_\_\ nn_\_al. nn_\_Walker nn_al._et
J91-4003	P88-1012	o	Additional evidence for this distinction is given in Pustejovsky and Anick -LRB- 1988 -RRB- and Briscoe et al.	dep_Briscoe_al. nn_Briscoe_et dep_Anick_1988 conj_and_Pustejovsky_Briscoe conj_and_Pustejovsky_Anick prep_in_given_Briscoe prep_in_given_Anick prep_in_given_Pustejovsky auxpass_given_is nsubjpass_given_evidence det_distinction_this prep_for_evidence_distinction amod_evidence_Additional
J91-4003	P88-1012	o	Hobbs et al. 1988 Charniak and Goldman 1988 -RRB-	num_Goldman_1988 conj_and_Charniak_Goldman dep_al._Goldman dep_al._Charniak num_al._1988 nn_al._et nn_al._Hobbs
J95-3001	P88-1012	o	-LRB- 1980 -RRB- Walker -LRB- 1978 -RRB- Fink and Biermann -LRB- 1986 -RRB- Mudler and Paulus -LRB- 1988 -RRB- Carbonell and Pierrel -LRB- 1988 -RRB- Young -LRB- 1990 -RRB- and Young et al.	nn_al._et nn_al._Young appos_Young_1990 appos_Pierrel_1988 conj_and_Carbonell_Pierrel appos_Paulus_1988 conj_and_Mudler_Paulus appos_Biermann_1986 conj_and_Fink_Biermann conj_and_Walker_al. conj_and_Walker_Young appos_Walker_Pierrel appos_Walker_Carbonell appos_Walker_Paulus appos_Walker_Mudler appos_Walker_Biermann appos_Walker_Fink appos_Walker_1978 dep_1980_al. dep_1980_Young dep_1980_Walker dep_''_1980
J95-4001	P88-1012	o	Abduction has been applied to the solution of local pragmatics problems -LRB- Hobbs et al. 1988 1993 -RRB- and to story understanding -LRB- Charniak and Goldman 1988 -RRB-	num_Goldman_1988 conj_and_Charniak_Goldman dep_understanding_Goldman dep_understanding_Charniak nn_understanding_story pobj_to_understanding num_1988_1993 tmod_al._1988 nn_al._et advmod_Hobbs_al. nn_problems_pragmatics amod_problems_local prep_of_solution_problems det_solution_the conj_and_applied_to dep_applied_Hobbs prep_to_applied_solution auxpass_applied_been aux_applied_has nsubjpass_applied_Abduction
N06-1006	P88-1012	o	-LRB- 2005 -RRB- is to translate dependency parses into neo-Davidsonian-style quasilogical forms and to perform weighted abductive theorem proving in the tradition of -LRB- Hobbs et al. 1988 -RRB-	amod_Hobbs_1988 dep_Hobbs_al. nn_Hobbs_et dep_of_Hobbs prep_tradition_of det_tradition_the prep_in_proving_tradition vmod_theorem_proving amod_theorem_abductive amod_theorem_weighted dobj_perform_theorem aux_perform_to nsubj_perform_dependency amod_forms_quasilogical amod_forms_neo-Davidsonian-style conj_and_parses_perform prep_into_parses_forms nsubj_parses_dependency ccomp_translate_perform ccomp_translate_parses aux_translate_to xcomp_is_translate dep_is_2005
P93-1012	P88-1012	o	Volume 17 Number 1 March 1991 References Lakoff George and Johnson Mark Metaphors We Live 8y University of Chicago Press 1980 MADCOW Committee -LRB- Hirschman Lynette et al -RRB- Multi-Site Data Collection for a Spoken Language Corpus in Proceedings Speech and Natural Language Workshop February 1992 Grice H. P. Logic and Conversation in P. Cole and J. L. Morgan Speech Acts New York Academic Press 1975 Pustejovsky James The Generative Lexicon Computational Linguistics Volume 17 Number 4 December 1991 Hobbs Jerry R. and Stickel Mark Interpretation as Abduction in Proceedings of the 26th ACL June 1988 Bobrow R. Ingria R. and Stallard D The Mapping Unit Approach to Subcategorization in Proceedings Speech and Natural Language Workshop February 1991 Hobbs Jerry R. and Martin Paul Local Pragmatics in Proceedings 10th International Joint Conference on Artificial Intelligence -LRB- IJCAI-87 -RRB-	appos_Intelligence_IJCAI-87 nn_Intelligence_Artificial prep_on_Conference_Intelligence nn_Conference_Joint nn_Conference_International amod_Conference_10th prep_in_Pragmatics_Proceedings nn_Pragmatics_Local nn_Pragmatics_Paul nn_R._Jerry nn_Hobbs_February nn_Hobbs_Workshop nn_Hobbs_Language nn_Hobbs_Natural num_February_1991 conj_and_Speech_Hobbs nn_Speech_Proceedings prep_to_Approach_Subcategorization prep_in_Unit_Hobbs prep_in_Unit_Speech dep_Unit_Approach nn_Unit_Mapping det_Unit_The appos_R._D conj_and_R._Stallard conj_and_R._R. conj_and_R._Ingria appos_Bobrow_Stallard appos_Bobrow_R. appos_Bobrow_Ingria appos_Bobrow_R. nn_Bobrow_June nn_Bobrow_ACL amod_Bobrow_26th det_Bobrow_the num_June_1988 prep_of_Proceedings_Bobrow prep_in_Interpretation_Proceedings prep_as_Interpretation_Abduction nn_Interpretation_Mark nn_R._Jerry nn_Hobbs_December num_Hobbs_4 nn_Hobbs_Number num_December_1991 conj_and_17_Stickel conj_and_17_R. conj_and_17_Hobbs dep_Volume_Stickel dep_Volume_R. dep_Volume_Hobbs dep_Volume_17 nn_Volume_Linguistics nn_Volume_Computational nn_Volume_Lexicon nn_Volume_Generative nn_Volume_The nn_Volume_James num_Pustejovsky_1975 nn_Press_Academic nn_York_New nn_Acts_Speech nn_Morgan_L. nn_Morgan_J. conj_and_Cole_Morgan nn_Cole_P. nn_Logic_P. nn_Logic_H. num_Grice_1992 nn_Grice_February nn_Grice_Workshop nn_Grice_Language nn_Grice_Natural conj_and_Speech_Grice nn_Speech_Proceedings prep_in_Corpus_Grice prep_in_Corpus_Speech nn_Corpus_Language nn_Corpus_Spoken det_Corpus_a prep_for_Collection_Corpus nn_Collection_Data nn_Collection_Multi-Site nn_Collection_Lynette nn_al_et dep_Lynette_al appos_Hirschman_Conference appos_Hirschman_Pragmatics conj_and_Hirschman_Martin conj_and_Hirschman_R. dep_Hirschman_Unit dep_Hirschman_Interpretation dep_Hirschman_Volume dep_Hirschman_Pustejovsky dep_Hirschman_Press appos_Hirschman_York conj_and_Hirschman_Acts prep_in_Hirschman_Morgan prep_in_Hirschman_Cole conj_and_Hirschman_Conversation conj_and_Hirschman_Logic appos_Hirschman_Collection nn_Committee_MADCOW num_Committee_1980 nn_Committee_Press nn_Committee_Chicago dep_University_Martin dep_University_R. dep_University_Acts dep_University_Conversation dep_University_Logic dep_University_Hirschman prep_of_University_Committee amod_University_8y dobj_Live_University nsubj_Live_We rcmod_Metaphors_Live nn_Metaphors_Mark nn_Lakoff_References nn_Lakoff_March num_Lakoff_1 nn_Lakoff_Number num_March_1991 appos_17_Metaphors conj_and_17_Johnson conj_and_17_George conj_and_17_Lakoff dep_Volume_Johnson dep_Volume_George dep_Volume_Lakoff dep_Volume_17
P94-1030	P88-1012	o	This is known as cost-based abduction -LRB- Hobbs et al. 1988 -RRB-	amod_Hobbs_1988 dep_Hobbs_al. nn_Hobbs_et amod_abduction_cost-based dep_known_Hobbs prep_as_known_abduction auxpass_known_is nsubjpass_known_This ccomp_``_known
P94-1030	P88-1012	p	The abduction-based approach -LRB- Hobbs et al. 1988 -RRB- has provided a simple and elegant way to realize such a task	det_task_a predet_task_such dobj_realize_task aux_realize_to vmod_way_realize amod_way_elegant amod_way_simple det_way_a conj_and_simple_elegant dobj_provided_way aux_provided_has nsubj_provided_approach amod_Hobbs_1988 dep_Hobbs_al. nn_Hobbs_et dep_approach_Hobbs amod_approach_abduction-based det_approach_The ccomp_``_provided
W02-0211	P88-1012	o	Two main extensions from that work that we are making use of are 1 -RRB- proofs falling below a user defined cost threshold halt the search 2 -RRB- a simple variable typing system reduces the number of axioms written and the size of the search space -LRB- Hobbs et al. 1988 pg 102 -RRB-	num_pg_102 dep_Hobbs_pg amod_Hobbs_1988 dep_Hobbs_al. nn_Hobbs_et nn_space_search det_space_the prep_of_size_space det_size_the conj_and_written_size vmod_axioms_size vmod_axioms_written prep_of_number_axioms det_number_the dobj_reduces_number nsubj_reduces_system nsubj_reduces_halt dep_reduces_user mark_reduces_below nn_system_typing amod_system_variable amod_system_simple det_system_a num_search_2 det_search_the dep_halt_search nn_halt_threshold nn_halt_cost amod_halt_defined det_user_a parataxis_falling_Hobbs advcl_falling_reduces nsubj_falling_proofs dep_falling_1 prepc_of_use_are dobj_making_use aux_making_are nsubj_making_we mark_making_that det_work_that dep_extensions_falling dep_extensions_making prep_from_extensions_work amod_extensions_main num_extensions_Two
W02-0211	P88-1012	o	The domain axioms will bind the body variables to their most likely referents during unification with facts and previously assumed and proven propositions similarly to -LRB- Hobbs et al. 1988 -RRB-	amod_Hobbs_1988 dep_Hobbs_al. nn_Hobbs_et dep_to_Hobbs prep_propositions_to advmod_propositions_similarly amod_propositions_proven amod_propositions_assumed conj_and_assumed_proven advmod_assumed_previously prep_with_unification_facts amod_referents_likely poss_referents_their advmod_likely_most nn_variables_body det_variables_the conj_and_bind_propositions prep_during_bind_unification prep_to_bind_referents dobj_bind_variables aux_bind_will nsubj_bind_axioms nn_axioms_domain det_axioms_The
W94-0101	P88-1012	o	.1 is a set of assumptions sufficient to support the inI ` rl -RRB- n'lation given S and R In other words this is h ~ crl -RRB- rctal ion as abduction ' -LRB- Itobbs et al. 1988 -RRB- since ~ -RRB- -LRB- i -LRB- ` lion not deduction is needed to arrive at the ~ > 'd H II I ~ tiOIIS ,4	num_tiOIIS_,4 dobj_~_tiOIIS nsubj_~_I rcmod_H_~ num_H_II dobj_'d_H dep_~_'d amod_~_> dep_the_~ prep_at_arrive_the aux_arrive_to xcomp_needed_arrive auxpass_needed_is nsubjpass_needed_i dep_needed_set neg_deduction_not appos_lion_deduction dep_i_lion dep_1988_al. nn_al._et num_Itobbs_1988 prep_as_ion_abduction appos_rctal_Itobbs appos_rctal_ion nn_crl_~ dep_h_rctal appos_h_crl cop_h_is nsubj_h_this amod_words_other conj_and_S_R pobj_given_R pobj_given_S prep_n'lation_given nn_n'lation_rl dep_inI_n'lation det_inI_the dobj_support_inI aux_support_to xcomp_sufficient_support amod_assumptions_sufficient prep_since_set_~ parataxis_set_h prep_in_set_words prep_of_set_assumptions det_set_a cop_set_is nsubj_set_.1
A92-1013	P90-1034	o	D. Hindle Noun classification from predicate argument structures in -LRB- ACL ,1990 -RRB-	num_ACL_,1990 nn_structures_argument nn_structures_predicate prep_from_classification_structures nn_classification_Noun prep_in_Hindle_ACL appos_Hindle_classification nn_Hindle_D.
A92-1013	P90-1034	o	In -LRB- Hindle ,1990 Zernik 1989 Webster el Marcus 1989 -RRB- cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification	nn_classification_word prep_of_purpose_classification det_purpose_the prep_for_used_purpose auxpass_used_is amod_parsing_syntactic dep_augmented_used prep_with_augmented_parsing nsubj_augmented_analyses nn_analyses_cooccurrence nn_analyses_Hindle dep_Marcus_1989 nn_Marcus_el nn_Marcus_Webster dep_Zernik_1989 dep_Hindle_Marcus dep_Hindle_Zernik num_Hindle_,1990 pcomp_In_augmented dep_``_In
A92-1013	P90-1034	o	-LRB- Hindle 1990 Hindle and Rooths ,1991 -RRB- and -LRB- Smadja 1991 -RRB- use syntactic markers to increase the significance of the data	det_data_the prep_of_significance_data det_significance_the dobj_increase_significance aux_increase_to amod_markers_syntactic vmod_use_increase dobj_use_markers nsubj_use_Smadja nsubj_use_Hindle amod_Smadja_1991 dep_Hindle_,1991 conj_and_Hindle_Rooths conj_and_Hindle_Smadja dep_Hindle_Rooths dep_Hindle_Hindle appos_Hindle_1990
A92-1013	P90-1034	o	Combining statistical and parsing methods has been done by -LRB- Hindle 1990 Hindle and Rooths ,1991 -RRB- and -LRB- Smadja and McKewon 1990 Smadja ,1991 -RRB-	num_Smadja_,1991 dep_Smadja_Smadja dep_Smadja_1990 conj_and_Smadja_McKewon dep_Hindle_,1991 conj_and_Hindle_Rooths conj_and_Hindle_McKewon conj_and_Hindle_Smadja dep_Hindle_Rooths dep_Hindle_Hindle appos_Hindle_1990 agent_done_Smadja agent_done_Hindle auxpass_done_been aux_done_has nsubjpass_done_methods amod_methods_parsing amod_methods_statistical amod_methods_Combining conj_and_statistical_parsing
A94-1011	P90-1034	p	Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category -LRB- although this has not been found to be effective for 1R -RRB- lemma of the word -LRB- e.g. corpus for corpora -RRB- phrasal information -LRB- e.g. identifying noun groups and phrases -LRB- Lewis 1992c Church 1988 -RRB- -RRB- and subject-predicate identification -LRB- e.g. Hindle 1990 -RRB-	dep_1990_Hindle dep_1990_e.g. dep_identification_1990 amod_identification_subject-predicate num_Church_1988 appos_1992c_Church nn_1992c_Lewis conj_and_groups_phrases nn_groups_noun dep_identifying_1992c dobj_identifying_phrases dobj_identifying_groups advmod_identifying_e.g. dep_information_identifying amod_information_phrasal dep_corpora_for dep_corpora_lemma dep_corpora_found dep_corpus_e.g. nn_corpus_word det_word_the prep_of_lemma_corpus prep_for_effective_1R cop_effective_be aux_effective_to xcomp_found_effective auxpass_found_been neg_found_not aux_found_has nsubjpass_found_this mark_found_although amod_category_syntactic poss_category_their prep_with_words_category amod_words_tagging conj_and_include_identification conj_and_include_information dep_include_corpora dobj_include_words nsubj_include_examples amod_annotation_sophisticated advmod_annotation_linguistically prep_of_examples_annotation amod_examples_Typical
C00-2104	P90-1034	o	Hindle -LRB- 1990 -RRB- classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs	dep_subjectverb_pairs conj_and_subjectverb_verb-object prep_of_patterns_verb-object prep_of_patterns_subjectverb amod_patterns_co-occurring prep_of_basis_patterns det_basis_the prep_on_nouns_basis amod_nouns_classified nn_nouns_Hindle appos_Hindle_1990
C04-1036	P90-1034	p	Probably the most widely used association weight function is -LRB- point-wise -RRB- Mutual Information -LRB- MI -RRB- -LRB- Church et al. 1990 -RRB- -LRB- Hindle 1990 -RRB- -LRB- Lin 1998 -RRB- -LRB- Dagan 2000 -RRB- defined by -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- log -RRB- -LRB- 2 fPwP fwPfwMI = A known weakness of MI is its tendency to assign high weights for rare features	amod_features_rare amod_weights_high prep_for_assign_features dobj_assign_weights aux_assign_to vmod_tendency_assign poss_tendency_its cop_tendency_is nsubj_tendency_point-wise aux_tendency_is dep_tendency_function advmod_tendency_Probably prep_of_weakness_MI amod_weakness_known det_weakness_A dobj_=_weakness dep_fwPfwMI_= nn_fwPfwMI_fPwP num_fwPfwMI_2 dep_defined_by amod_Dagan_2000 amod_Lin_1998 dep_Hindle_1990 amod_Church_1990 dep_Church_al. nn_Church_et appos_Information_MI amod_Information_Mutual appos_point-wise_fwPfwMI appos_point-wise_log dep_point-wise_defined appos_point-wise_Dagan appos_point-wise_Lin appos_point-wise_Hindle dep_point-wise_Church dep_point-wise_Information nn_function_weight nn_function_association amod_function_used det_function_the advmod_used_widely advmod_used_most
C04-1036	P90-1034	o	1 Introduction Distributional Similarity has been an active research area for more than a decade -LRB- Hindle 1990 -RRB- -LRB- Ruge 1992 -RRB- -LRB- Grefenstette 1994 -RRB- -LRB- Lee 1997 -RRB- -LRB- Lin 1998 -RRB- -LRB- Dagan et al. 1999 -RRB- -LRB- Weeds and Weir 2003 -RRB-	amod_Weeds_2003 conj_and_Weeds_Weir amod_Dagan_1999 dep_Dagan_al. nn_Dagan_et amod_Lin_1998 amod_Lee_1997 amod_Grefenstette_1994 dep_Ruge_1992 dep_Hindle_1990 det_decade_a prep_than_more_decade appos_area_Weir appos_area_Weeds appos_area_Dagan appos_area_Lin appos_area_Lee appos_area_Grefenstette appos_area_Ruge dep_area_Hindle prep_for_area_more nn_area_research amod_area_active det_area_an cop_area_been aux_area_has nsubj_area_Similarity nn_Similarity_Distributional nn_Similarity_Introduction num_Similarity_1
C04-1111	P90-1034	o	2.2 Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics -LRB- Hindle 1990 Lin 1998 -RRB-	num_Lin_1998 appos_1990_Lin amod_1990_Hindle nn_statistics_cooccurrence dep_uses_1990 dobj_uses_statistics nsubj_uses_class prep_of_class_algorithms amod_class_second det_class_The rcmod_approaches_uses amod_approaches_Co-occurrence-based num_approaches_2.2 dep_``_approaches
C04-1116	P90-1034	o	Our method is similar to -LRB- Hindle 1990 -RRB- -LRB- Lin 1998 -RRB- and -LRB- Gasperin 2001 -RRB- in the use of dependency relationships as the word features	nn_features_word det_features_the nn_relationships_dependency prep_of_use_relationships det_use_the prep_as_Gasperin_features prep_in_Gasperin_use dep_Gasperin_2001 dep_Lin_1998 conj_and_Hindle_Gasperin appos_Hindle_Lin amod_Hindle_1990 prep_to_similar_Gasperin prep_to_similar_Hindle cop_similar_is nsubj_similar_method poss_method_Our ccomp_``_similar
C04-1116	P90-1034	o	The words we want to aggregate for text analysis are not rigorous synonyms but the role is the same so we have to consider the syntactic relation based on the assumptions that words with the same role tend to modify or be modified by similar words -LRB- Hindle 1990 Strzalkowski 1992 -RRB-	amod_Strzalkowski_1992 dep_Hindle_Strzalkowski appos_Hindle_1990 appos_words_Hindle amod_words_similar agent_modified_words auxpass_modified_be conj_or_modify_modified aux_modify_to xcomp_tend_modified xcomp_tend_modify amod_role_same det_role_the dep_words_tend prep_with_words_role nsubj_words_that rcmod_assumptions_words det_assumptions_the prep_on_based_assumptions vmod_relation_based nn_relation_syntactic det_relation_the dobj_consider_relation aux_consider_to xcomp_have_consider nsubj_have_we parataxis_same_have dep_same_so det_same_the cop_same_is nsubj_same_role det_role_the conj_but_synonyms_same amod_synonyms_rigorous neg_synonyms_not cop_synonyms_are nsubj_synonyms_words nn_analysis_text prep_for_want_analysis prep_to_want_aggregate nsubj_want_we rcmod_words_want det_words_The
C04-1165	P90-1034	o	Hindle -LRB- 1990 -RRB- used noun-verb syntactic relations and Hatzivassiloglou and McKeown -LRB- 1993 -RRB- used coordinated adjective-adjective modifier pairs	nn_pairs_modifier amod_pairs_adjective-adjective amod_pairs_coordinated amod_pairs_used appos_McKeown_1993 dep_Hatzivassiloglou_pairs conj_and_Hatzivassiloglou_McKeown conj_and_relations_McKeown conj_and_relations_Hatzivassiloglou amod_relations_syntactic amod_relations_noun-verb amod_relations_used nn_relations_Hindle appos_Hindle_1990
C08-1051	P90-1034	o	Others proposed distributional similarity measures between words -LRB- Hindle 1990 Lin 1998 Lee 1999 Weeds et al. 2004 -RRB-	amod_Weeds_2004 dep_Weeds_al. nn_Weeds_et num_Lee_1999 dep_Lin_Weeds conj_Lin_Lee num_Lin_1998 dep_Hindle_Lin appos_Hindle_1990 dep_words_Hindle prep_between_measures_words nn_measures_similarity amod_measures_distributional dobj_proposed_measures nsubj_proposed_Others ccomp_``_proposed
C92-2082	P90-1034	o	There bas recently been work in the detection of semantically related nouns via for example shared argument structures -LRB- Hindle 1990 -RRB- and shared dictionary definition context -LRB- Wilks e al. 1990 -RRB-	num_al._1990 dep_al._e dep_Wilks_al. appos_context_Wilks dep_definition_context nn_definition_dictionary dobj_shared_definition dep_Hindle_1990 conj_and_structures_shared dep_structures_Hindle nn_structures_argument amod_structures_shared dep_,_shared dep_,_structures prep_for_,_example amod_nouns_related advmod_related_semantically prep_of_detection_nouns det_detection_the dep_work_via prep_in_work_detection cop_work_been advmod_work_recently nn_work_bas expl_work_There
C94-1074	P90-1034	n	Among the applications of collocational analysis for lexical acquisition are the derivation of syntactic disambiguation cues -LRB- Basili et al. 1991 1993a Hindle and Rooths 1991,1993 Sekine 1992 -RRB- -LRB- Bogges et al. 1992 -RRB- sense preference -LRB- Yarowski 1992 -RRB- acquisition of selectional restrictions -LRB- Basili et al. 1992b 1993b Utsuro et al. 1993 -RRB- lexical preference in generation -LRB- Smadjia 1991 -RRB- word clustering -LRB- Pereira 1993 Hindle 1990 Basili et al. 1993c -RRB- etc. In the majority of these papers even though the -LRB- precedent or subsequent -RRB- statistical processing reduces the number of accidental associations very large corpora -LRB- 10,000,000 words -RRB- are necessary to obtain reliable data on a large enough number of words	prep_of_number_words amod_enough_large det_enough_a amod_data_reliable prep_on_obtain_enough dobj_obtain_data aux_obtain_to dep_necessary_number xcomp_necessary_obtain cop_necessary_are nsubj_necessary_corpora num_words_10,000,000 appos_corpora_words amod_corpora_large advmod_large_very amod_associations_accidental prep_of_number_associations det_number_the parataxis_reduces_necessary dobj_reduces_number nsubj_reduces_processing nsubj_reduces_precedent mark_reduces_though advmod_reduces_even amod_processing_statistical amod_processing_subsequent conj_or_precedent_processing det_precedent_the det_papers_these prep_of_majority_papers det_majority_the nn_1993c_al. nn_1993c_et nn_1993c_Basili num_Hindle_1990 dep_Pereira_1993c dep_Pereira_Hindle num_Pereira_1993 dep_clustering_Pereira nn_clustering_word dep_Smadjia_1991 dep_generation_Smadjia prep_in_preference_generation amod_preference_lexical dep_al._1993 nn_al._et nn_al._Utsuro nn_1992b_al. dep_Basili_al. appos_Basili_1993b dep_Basili_1992b nn_Basili_et amod_restrictions_selectional prep_of_acquisition_restrictions dep_Yarowski_1992 dep_preference_Yarowski nn_preference_sense dep_al._1992 nn_al._et advmod_Bogges_al. num_Sekine_1992 num_Rooths_1991,1993 conj_and_Hindle_Rooths num_al._1991 dep_Basili_Sekine dep_Basili_Rooths dep_Basili_Hindle appos_Basili_1993a dep_Basili_al. nn_Basili_et nn_cues_disambiguation amod_cues_syntactic dep_derivation_reduces prep_in_derivation_majority appos_derivation_etc. appos_derivation_clustering appos_derivation_preference appos_derivation_Basili appos_derivation_acquisition appos_derivation_preference appos_derivation_Bogges appos_derivation_Basili prep_of_derivation_cues det_derivation_the dep_are_derivation prep_among_are_applications amod_acquisition_lexical amod_analysis_collocational prep_for_applications_acquisition prep_of_applications_analysis det_applications_the
C96-1003	P90-1034	o	have been proposed -LRB- Hindle 1990 Brown et al. 1992 Pereira et al. 1993 Tokunaga et al. 1995 -RRB-	num_Tokunaga_1995 nn_Tokunaga_al. nn_Tokunaga_et num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Hindle_Tokunaga dep_Hindle_Pereira dep_Hindle_Brown dep_Hindle_1990 dep_proposed_Hindle auxpass_proposed_been aux_proposed_have
C96-1083	P90-1034	o	4 Towards an adequate similarity esfimatation for the building of ontologies The comparison with the similarity score of -LRB- Hindle 1990 -RRB- shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning	conj_and_bootstrap_tuning amod_bootstrap_ontology prep_for_relevant_tuning prep_for_relevant_bootstrap advmod_relevant_specifically cop_relevant_is nsubj_relevant_indicator mark_relevant_that nn_indicator_similarity nn_indicator_SYCLADE ccomp_shows_relevant nsubj_shows_comparison prep_towards_shows_esfimatation amod_Hindle_1990 prep_of_score_Hindle nn_score_similarity det_score_the prep_with_comparison_score det_comparison_The prep_of_building_ontologies det_building_the prep_for_esfimatation_building nn_esfimatation_similarity amod_esfimatation_adequate det_esfimatation_an rcmod_4_shows ccomp_``_4
C96-1083	P90-1034	o	Hindle uses the observed frequencies within a specific syntactic pattern -LRB- subject/verb and verb/object -RRB- to derive a cooccu > rence score which is an estimate of mutual information -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual prep_of_estimate_information det_estimate_an cop_estimate_is nsubj_estimate_which rcmod_score_estimate nn_score_rence amod_score_> appos_cooccu_score det_cooccu_a dobj_derive_cooccu aux_derive_to conj_and_subject/verb_verb/object dep_pattern_verb/object dep_pattern_subject/verb nn_pattern_syntactic amod_pattern_specific det_pattern_a amod_frequencies_observed det_frequencies_the vmod_uses_derive prep_within_uses_pattern dobj_uses_frequencies nsubj_uses_Hindle
C96-1083	P90-1034	o	In the past five years important research on the automatic acquisition of word classes based on lexical distribution has been published -LRB- Church and Hanks 1990 Hindle 1990 Smadja 1993 Grei ~ nstette 1994 Grishman and Sterling 1994 -RRB-	num_Grishman_1994 conj_and_Grishman_Sterling appos_nstette_1994 num_nstette_~ nn_nstette_Grei num_Smadja_1993 num_Hindle_1990 dep_Church_Sterling dep_Church_Grishman conj_and_Church_nstette conj_and_Church_Smadja conj_and_Church_Hindle conj_and_Church_1990 conj_and_Church_Hanks dep_published_nstette dep_published_Smadja dep_published_Hindle dep_published_1990 dep_published_Hanks dep_published_Church auxpass_published_been aux_published_has nsubjpass_published_research prep_in_published_years amod_distribution_lexical prep_on_based_distribution nn_classes_word prep_of_acquisition_classes amod_acquisition_automatic det_acquisition_the vmod_research_based prep_on_research_acquisition amod_research_important num_years_five amod_years_past det_years_the
C96-1083	P90-1034	o	Section 4 compares our results to Itindle 's ones -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 dep_ones_Hindle poss_ones_Itindle poss_results_our prep_to_compares_ones dobj_compares_results nsubj_compares_Section num_Section_4
C96-1083	P90-1034	o	Itowever Harris ' methodology implies also to simplify and transform each parse tree 2 so as to obtain so-called elementary sentences exhibiting the main conceptual classes for the domain -LRB- Sager lIa'or instance Hindle -LRB- Hindle 1990 -RRB- needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures	amod_structures_predicate-argunlent prep_from_similarities_structures nn_similarities_noun nn_similarities_extract prep_to_order_similarities nn_corpus_word num_corpus_million det_corpus_a number_million_six prep_in_needs_order dobj_needs_corpus nsubj_needs_Hindle dep_Hindle_1990 appos_Hindle_Hindle rcmod_instance_needs nn_instance_lIa'or nn_instance_Sager dep_domain_instance det_domain_the amod_classes_conceptual amod_classes_main det_classes_the prep_for_exhibiting_domain dobj_exhibiting_classes amod_sentences_elementary amod_sentences_so-called xcomp_obtain_exhibiting dobj_obtain_sentences aux_obtain_to num_tree_2 nn_tree_parse det_tree_each dobj_simplify_tree conj_and_simplify_transform aux_simplify_to prepc_as_implies_obtain advmod_implies_so xcomp_implies_transform xcomp_implies_simplify advmod_implies_also nsubj_implies_methodology dep_implies_Itowever poss_methodology_Harris
C96-2205	P90-1034	o	2.3 Measuring the similarity between classes -LRB- step 3 -RRB- In step 3 we measure the similarity between two primitive classes by using the method given by Hindle -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 dep_Hindle_Hindle agent_given_Hindle vmod_method_given det_method_the dobj_using_method amod_classes_primitive num_classes_two prep_between_similarity_classes det_similarity_the prepc_by_measure_using dobj_measure_similarity nsubj_measure_we nsubj_measure_2.3 num_step_3 num_step_3 appos_classes_step prep_in_similarity_step prep_between_similarity_classes det_similarity_the dobj_Measuring_similarity vmod_2.3_Measuring
C96-2205	P90-1034	o	Since a handmade thesaurus is not slfitahle for machine use and expensive to compile automatical construction of ~ a thesaurus has been attempted using corpora -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 dep_corpora_Hindle dobj_using_corpora xcomp_attempted_using auxpass_attempted_been aux_attempted_has csubjpass_attempted_expensive nn_thesaurus_a nn_thesaurus_~ prep_of_construction_thesaurus amod_construction_automatical dobj_compile_construction aux_compile_to xcomp_expensive_compile cc_expensive_and advcl_expensive_slfitahle nn_use_machine prep_for_slfitahle_use neg_slfitahle_not cop_slfitahle_is nsubj_slfitahle_thesaurus mark_slfitahle_Since amod_thesaurus_handmade det_thesaurus_a
E09-1086	P90-1034	o	Distributional approaches on the other hand rely on text corpora and model relatedness by comparing the contexts in which two words occur assuming that related words occur in similar context -LRB- e.g. Hindle -LRB- 1990 -RRB- Lin -LRB- 1998 -RRB- Mohammad and Hirst -LRB- 2006 -RRB- -RRB-	appos_Hirst_2006 conj_and_Lin_Hirst conj_and_Lin_Mohammad appos_Lin_1998 appos_Hindle_Hirst appos_Hindle_Mohammad appos_Hindle_Lin appos_Hindle_1990 dep_e.g._Hindle ccomp_-LRB-_e.g. amod_context_similar prep_in_occur_context nsubj_occur_words mark_occur_that amod_words_related ccomp_assuming_occur nsubj_occur_words prep_in_occur_which num_words_two rcmod_contexts_occur det_contexts_the dobj_comparing_contexts nn_relatedness_model nn_corpora_text prep_on_rely_corpora amod_hand_other det_hand_the vmod_approaches_assuming prepc_by_approaches_comparing conj_and_approaches_relatedness conj_and_approaches_rely prep_on_approaches_hand amod_approaches_Distributional
E91-1038	P90-1034	p	Semantic collocations are harder to extract than cooccurrence patterns -- the state of the art does not enable us to find semantic collocations automatically t This paper however argues that if we take advantage of lexicai paradigmatic behavior underlying the lexicon we can at least achieve semi-automatic extraction of semantic collocations -LRB- see also Calzolari and Bindi -LRB- 1990 -RRB- I But note the important work by Hindle \ -LSB- HindlegO \ -RSB- on extracting semantically similar nouns based on their substitutability in certain verb contexts	amod_contexts_verb amod_contexts_certain prep_in_substitutability_contexts poss_substitutability_their prep_on_based_substitutability vmod_nouns_based amod_nouns_similar advmod_nouns_semantically dobj_extracting_nouns nn_\_HindlegO num_Hindle_\ prep_by_work_Hindle amod_work_important det_work_the dep_work_note dep_work_I conj_but_I_note appos_Bindi_1990 prepc_on_Calzolari_extracting appos_Calzolari_\ dep_Calzolari_work conj_and_Calzolari_Bindi ccomp_also_Bindi ccomp_also_Calzolari advmod_see_also amod_collocations_semantic prep_of_extraction_collocations amod_extraction_semi-automatic dep_achieve_see dobj_achieve_extraction advmod_achieve_at aux_achieve_can nsubj_achieve_we advcl_achieve_take mark_achieve_that pobj_at_least det_lexicon_the dobj_underlying_lexicon vmod_behavior_underlying amod_behavior_paradigmatic nn_behavior_lexicai prep_of_advantage_behavior dobj_take_advantage nsubj_take_we mark_take_if ccomp_argues_achieve advmod_argues_however nsubj_argues_paper det_paper_This advmod_t_automatically nsubj_t_collocations amod_collocations_semantic xcomp_find_t aux_find_to xcomp_enable_find dobj_enable_us neg_enable_not aux_enable_does nsubj_enable_state det_art_the prep_of_state_art det_state_the nn_patterns_cooccurrence prep_than_extract_patterns aux_extract_to parataxis_harder_argues parataxis_harder_enable xcomp_harder_extract cop_harder_are nsubj_harder_collocations amod_collocations_Semantic
E99-1013	P90-1034	n	Our syntactic-relation-based thesaurus is based on the method proposed by Hindle -LRB- 1990 -RRB- although Hindle did not apply it to information retrieval	nn_retrieval_information prep_to_apply_retrieval dobj_apply_it neg_apply_not aux_apply_did nsubj_apply_Hindle mark_apply_although appos_Hindle_1990 agent_proposed_Hindle vmod_method_proposed det_method_the advcl_based_apply prep_on_based_method auxpass_based_is nsubjpass_based_thesaurus amod_thesaurus_syntactic-relation-based poss_thesaurus_Our ccomp_``_based
E99-1013	P90-1034	o	Words appearing in similax grammatical contexts are assumed to be similar and therefore classified into the same class -LRB- Lin 1998 Grefenstette 1994 Grefenstette 1992 Ruge 1992 Hindle 1990 -RRB-	amod_Hindle_1990 num_Ruge_1992 num_Grefenstette_1992 num_Grefenstette_1994 dep_Lin_Hindle dep_Lin_Ruge dep_Lin_Grefenstette dep_Lin_Grefenstette num_Lin_1998 appos_class_Lin amod_class_same det_class_the prep_into_classified_class advmod_classified_therefore conj_and_similar_classified cop_similar_be aux_similar_to xcomp_assumed_classified xcomp_assumed_similar auxpass_assumed_are nsubjpass_assumed_Words amod_contexts_grammatical nn_contexts_similax prep_in_appearing_contexts vmod_Words_appearing
H93-1049	P90-1034	o	Hindle D. -LRB- 1990 -RRB- Noun Classification from Predicate-Argument Structures Proceedings of the 28th Annual Meeting of the ACL pp	appos_ACL_pp det_ACL_the prep_of_Meeting_ACL amod_Meeting_Annual amod_Meeting_28th det_Meeting_the prep_of_Proceedings_Meeting nn_Structures_Predicate-Argument prep_from_Classification_Structures nn_Classification_Noun dep_D._Proceedings dep_D._Classification appos_D._1990 nn_D._Hindle
I08-1060	P90-1034	o	Some researchers -LRB- Hindle 1990 Grefenstette 1994 Lin 1998 -RRB- classify terms by similarities based on their distributional syntactic patterns	nn_patterns_syntactic amod_patterns_distributional poss_patterns_their prep_on_based_patterns vmod_similarities_based prep_by_classify_similarities dobj_classify_terms nsubj_classify_Grefenstette dep_Lin_1998 dep_Grefenstette_Lin appos_Grefenstette_1994 parataxis_Hindle_classify appos_Hindle_1990 dep_researchers_Hindle det_researchers_Some ccomp_``_researchers
I08-1072	P90-1034	p	A wide range of contextual information such as surrounding words -LRB- Lowe and McDonald 2000 Curran and Moens 2002a -RRB- dependency or case structure -LRB- Hindle 1990 Ruge 1997 Lin 1998 -RRB- and dependency path -LRB- Lin and Pantel 2001 Pado and Lapata 2007 -RRB- has been utilized for similarity calculation and achieved considerable success	amod_success_considerable dobj_achieved_success nn_calculation_similarity prep_for_utilized_calculation auxpass_utilized_been aux_utilized_has nsubjpass_utilized_Hindle amod_Pado_2007 conj_and_Pado_Lapata dep_Lin_Lapata dep_Lin_Pado num_Lin_2001 conj_and_Lin_Pantel appos_path_Pantel appos_path_Lin nn_path_dependency num_Lin_1998 conj_and_Ruge_path conj_and_Ruge_Lin appos_Ruge_1997 dep_Hindle_path dep_Hindle_Lin dep_Hindle_Ruge appos_Hindle_1990 rcmod_structure_utilized nn_structure_case nn_structure_dependency conj_or_dependency_case appos_Lowe_2002a conj_and_Lowe_Moens conj_and_Lowe_Curran conj_and_Lowe_2000 conj_and_Lowe_McDonald appos_words_structure dep_words_Moens dep_words_Curran dep_words_2000 dep_words_McDonald dep_words_Lowe amod_words_surrounding prep_such_as_information_words amod_information_contextual conj_and_range_achieved prep_of_range_information amod_range_wide det_range_A
J04-3002	P90-1034	o	Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation -LRB- Hindle 1990 Dagan Pereira and Lee 1994 -RRB- and to develop lexical resources from corpora -LRB- Lin 1998 Riloff and Jones 1999 -RRB-	num_Jones_1999 conj_and_Riloff_Jones dep_Lin_Jones dep_Lin_Riloff num_Lin_1998 dep_corpora_Lin amod_resources_lexical prep_from_develop_corpora dobj_develop_resources aux_develop_to nsubj_develop_Features num_Lee_1994 conj_and_Dagan_Lee conj_and_Dagan_Pereira dep_Hindle_Lee dep_Hindle_Pereira dep_Hindle_Dagan dep_Hindle_1990 amod_disambiguation_semantic dep_syntactic_Hindle conj_and_syntactic_disambiguation conj_and_used_develop prep_for_used_disambiguation prep_for_used_syntactic auxpass_used_been advmod_used_previously aux_used_have nsubjpass_used_Features amod_similarity_distributional dobj_using_similarity xcomp_identified_using vmod_Features_identified ccomp_``_develop ccomp_``_used
J05-4002	P90-1034	o	Similarity-based smoothing -LRB- Hindle 1990 Brown et al. 1992 Dagan Marcus and Markovitch 1993 Pereira Tishby and Lee 1993 Dagan Lee and Pereira 1999 -RRB- provides an intuitively appealing approach to language modeling	nn_modeling_language prep_to_approach_modeling amod_approach_appealing det_approach_an advmod_appealing_intuitively dobj_provides_approach nsubj_provides_smoothing num_Pereira_1999 conj_and_Dagan_Pereira conj_and_Dagan_Lee num_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby num_Markovitch_1993 conj_and_Dagan_Markovitch conj_and_Dagan_Marcus nn_1992_al. num_Brown_1992 nn_Brown_et dep_1990_Pereira dep_1990_Lee dep_1990_Dagan conj_1990_Lee conj_1990_Tishby conj_1990_Pereira conj_1990_Markovitch conj_1990_Marcus conj_1990_Dagan conj_1990_Brown amod_1990_Hindle dep_smoothing_1990 amod_smoothing_Similarity-based
J05-4002	P90-1034	o	4.5 Hindles Measure Hindle -LRB- 1990 -RRB- proposed an MI-based measure which he used to show that nouns could be reliably clustered based on their verb co-occurrences	amod_co-occurrences_verb poss_co-occurrences_their prep_based_on_clustered_co-occurrences advmod_clustered_reliably auxpass_clustered_be aux_clustered_could nsubjpass_clustered_nouns mark_clustered_that ccomp_show_clustered aux_show_to xcomp_used_show nsubj_used_he dobj_used_which rcmod_measure_used amod_measure_MI-based det_measure_an dobj_proposed_measure nsubj_proposed_Hindle appos_Hindle_1990 nn_Hindle_Measure nn_Hindle_Hindles num_Hindle_4.5 ccomp_``_proposed
J05-4002	P90-1034	o	This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation -LRB- Hindle 1990 Grefenstette 1994 Lin 1998a Curran and Moens 2002 Kilgarriff 2003 -RRB-	num_Kilgarriff_2003 num_Moens_2002 conj_and_Curran_Moens nn_1998a_Lin num_Grefenstette_1994 dep_Hindle_Kilgarriff dep_Hindle_Moens dep_Hindle_Curran dep_Hindle_1998a dep_Hindle_Grefenstette dep_Hindle_1990 dep_generation_Hindle nn_generation_thesaurus amod_generation_automatic prep_of_body_work amod_body_large det_body_a prep_to_rise_body prep_on_given_generation dobj_given_rise aux_given_has nsubj_given_relationship amod_similarity_semantic conj_and_similarity_similarity amod_similarity_distributional prep_between_relationship_similarity prep_between_relationship_similarity ccomp_hypothesized_given nsubj_hypothesized_This
J93-2002	P90-1034	o	Most work on corpora of naturally occurring language 244 Michael R. Brent From Grammar to Lexicon either uses no a priori grammatical knowledge -LRB- Brill and Marcus 1992 Ellison 1991 Finch and Chater 1992 Pereira and Schabes 1992 -RRB- or else it relies on a large and complex grammar -LRB- Hindle 1990 1991 -RRB-	dep_1990_1991 amod_1990_Hindle dep_grammar_1990 amod_grammar_complex amod_grammar_large det_grammar_a conj_and_large_complex prep_on_relies_grammar nsubj_relies_it num_Schabes_1992 conj_and_Pereira_Schabes num_Chater_1992 conj_and_Finch_Chater num_Ellison_1991 num_Marcus_1992 dep_Brill_Schabes dep_Brill_Pereira conj_and_Brill_Chater conj_and_Brill_Finch conj_and_Brill_Ellison conj_and_Brill_Marcus conj_or_knowledge_else appos_knowledge_Finch appos_knowledge_Ellison appos_knowledge_Marcus appos_knowledge_Brill amod_knowledge_grammatical nn_knowledge_priori det_knowledge_a neg_knowledge_no dep_uses_relies dobj_uses_else dobj_uses_knowledge preconj_uses_either nsubj_uses_work prep_from_Brent_Grammar nn_Brent_R. nn_Brent_Michael num_Brent_244 nn_Brent_language prep_to_occurring_Lexicon dobj_occurring_Brent advmod_occurring_naturally prepc_of_corpora_occurring prep_on_work_corpora amod_work_Most
J93-2002	P90-1034	o	Many other projects have used statistics in a way that summarizes facts about the text but does not draw any explicit conclusions from them -LRB- Finch and Chater 1992 Hindle 1990 -RRB-	num_Hindle_1990 num_Chater_1992 dep_Finch_Hindle conj_and_Finch_Chater amod_conclusions_explicit det_conclusions_any dep_draw_Chater dep_draw_Finch prep_from_draw_them dobj_draw_conclusions neg_draw_not aux_draw_does nsubj_draw_that det_text_the prep_about_facts_text conj_but_summarizes_draw dobj_summarizes_facts nsubj_summarizes_that det_way_a dep_used_draw dep_used_summarizes prep_in_used_way dobj_used_statistics aux_used_have nsubj_used_projects amod_projects_other amod_projects_Many
J93-2005	P90-1034	o	We have found however that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships using a strategy similar to that employed by Hindle -LRB- 1990 -RRB- for detecting synonyms	dobj_detecting_synonyms appos_Hindle_1990 prepc_for_employed_detecting agent_employed_Hindle vmod_that_employed prep_to_similar_that amod_strategy_similar det_strategy_a dobj_using_strategy amod_relationships_taxonomic vmod_reflect_using dobj_reflect_relationships nsubj_reflect_compounds nn_compounds_noun det_compounds_which ccomp_suggest_reflect aux_suggest_to xcomp_employed_suggest auxpass_employed_be aux_employed_can nsubjpass_employed_evidence mark_employed_that amod_evidence_collocational ccomp_found_employed advmod_found_however aux_found_have nsubj_found_We ccomp_``_found
J93-2005	P90-1034	o	Hindle 1990 -RRB-	num_Hindle_1990
J93-2005	P90-1034	o	Using techniques described in Church and Hindle -LRB- 1990 -RRB- Church and Hanks -LRB- 1990 -RRB- and Hindle and Rooth -LRB- 1991 -RRB- Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus	nn_corpus_AP det_corpus_the prep_from_pairs_corpus nn_pairs_V-O amod_pairs_frequent det_pairs_the advmod_frequent_most prep_of_examples_pairs det_examples_some dobj_shows_examples nsubj_shows_Figure nsubj_shows_Rooth nsubj_shows_Hindle nsubj_shows_Hanks nsubj_shows_Church vmod_shows_Using num_Figure_4 appos_Rooth_1991 appos_Hanks_1990 conj_and_Church_Figure conj_and_Church_Rooth conj_and_Church_Hindle conj_and_Church_Hanks appos_Hindle_1990 conj_and_Church_Hindle prep_in_described_Hindle prep_in_described_Church vmod_techniques_described dobj_Using_techniques
J93-2005	P90-1034	o	Hindle -LRB- 1990 -RRB- reports interesting results of this kind based on literal collocations where he parses the corpus -LRB- Hindle 1983 -RRB- into predicate-argument structures and applies a mutual information measure -LRB- Fano 1961 Magerman and Marcus 1990 -RRB- to weigh the association between the predicate and each of its arguments	poss_arguments_its prep_of_each_arguments conj_and_predicate_each det_predicate_the prep_between_association_each prep_between_association_predicate det_association_the dobj_weigh_association aux_weigh_to dep_Magerman_1990 conj_and_Magerman_Marcus dep_Fano_Marcus dep_Fano_Magerman num_Fano_1961 dep_measure_Fano nn_measure_information amod_measure_mutual det_measure_a vmod_applies_weigh dobj_applies_measure nsubj_applies_he amod_structures_predicate-argument dep_Hindle_1983 dep_corpus_Hindle det_corpus_the conj_and_parses_applies prep_into_parses_structures dobj_parses_corpus nsubj_parses_he advmod_parses_where rcmod_collocations_applies rcmod_collocations_parses amod_collocations_literal det_kind_this pobj_results_collocations prepc_based_on_results_on prep_of_results_kind amod_results_interesting dep_reports_results nn_reports_Hindle appos_Hindle_1990
J94-4003	P90-1034	o	The use of such relations -LRB- mainly relations between verbs or nouns and their arguments and modifiers -RRB- for various purposes has received growing attention in recent research -LRB- Church and Hanks 1990 Zernik and Jacobs 1990 Hindle 1990 Smadja 1993 -RRB-	num_Smadja_1993 num_Hindle_1990 num_Jacobs_1990 conj_and_Zernik_Jacobs num_Hanks_1990 dep_Church_Smadja conj_and_Church_Hindle conj_and_Church_Jacobs conj_and_Church_Zernik conj_and_Church_Hanks dep_research_Hindle dep_research_Zernik dep_research_Hanks dep_research_Church amod_research_recent amod_attention_growing prep_in_received_research dobj_received_attention aux_received_has nsubj_received_use amod_purposes_various poss_arguments_their conj_and_verbs_modifiers conj_and_verbs_arguments conj_or_verbs_nouns prep_between_relations_modifiers prep_between_relations_arguments prep_between_relations_nouns prep_between_relations_verbs advmod_relations_mainly amod_relations_such prep_for_use_purposes dep_use_relations prep_of_use_relations det_use_The
J94-4003	P90-1034	o	More specifically two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment -LRB- Hindle and Rooth 1991 -RRB- and pronoun references -LRB- Dagan and Itai 1990 1991 -RRB-	num_Itai_1990 amod_Dagan_1991 conj_and_Dagan_Itai dep_references_Itai dep_references_Dagan nn_references_pronoun num_Rooth_1991 conj_and_Hindle_Rooth conj_and_attachment_references dep_attachment_Rooth dep_attachment_Hindle nn_attachment_phrase amod_attachment_prepositional prep_of_ambiguity_references prep_of_ambiguity_attachment dobj_resolving_ambiguity amod_relations_lexical amod_data_statistical prepc_for_using_resolving prep_on_using_relations dobj_using_data xcomp_suggested_using aux_suggested_have nsubj_suggested_works advmod_suggested_specifically amod_works_recent num_works_two dep_specifically_More ccomp_``_suggested
J94-4003	P90-1034	p	His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words -LRB- such as in Hindle 1990 -RRB-	num_Hindle_1990 pobj_in_Hindle prepc_such_as_words_in prep_between_similarity_words dobj_establish_similarity aux_establish_to xcomp_used_establish auxpass_used_are nsubjpass_used_corpora nsubjpass_used_methods mark_used_if amod_corpora_larger conj_and_methods_corpora amod_methods_sophisticated advmod_sophisticated_more advcl_improved_used auxpass_improved_be aux_improved_may nsubjpass_improved_results poss_results_His
J98-4002	P90-1034	o	Predicate argument structures which consist of complements -LRB- case filler nouns and case markers -RRB- and verbs have also been used in the task of noun classification -LRB- Hindle 1990 -RRB-	dep_Hindle_1990 nn_classification_noun dep_task_Hindle prep_of_task_classification det_task_the prep_in_used_task auxpass_used_been advmod_used_also aux_used_have dep_used_Predicate nn_markers_case conj_and_nouns_markers nn_nouns_filler nn_nouns_case conj_and_complements_verbs dep_complements_markers dep_complements_nouns prep_of_consist_verbs prep_of_consist_complements nsubj_consist_which rcmod_structures_consist nn_structures_argument dobj_Predicate_structures
N03-1015	P90-1034	o	32-39 Proceedings of HLT-NAACL 2003 similar distribution patterns -LRB- Hindle 1990 Peraira et al. 1993 Grefenstette 1994 -RRB-	amod_Grefenstette_1994 nn_al._et dep_Peraira_Grefenstette num_Peraira_1993 appos_Peraira_al. dep_Hindle_Peraira appos_Hindle_1990 dep_patterns_Hindle nn_patterns_distribution amod_patterns_similar num_patterns_2003 nn_patterns_HLT-NAACL prep_of_Proceedings_patterns num_Proceedings_32-39
N03-4011	P90-1034	o	There have been many approaches to compute the similarity between words based on their distribution in a corpus -LRB- Hindle 1990 Landauer and Dumais 1997 Lin 1998 -RRB-	num_Lin_1998 num_Dumais_1997 conj_and_Landauer_Dumais dep_Hindle_Lin dep_Hindle_Dumais dep_Hindle_Landauer dep_Hindle_1990 dep_corpus_Hindle det_corpus_a prep_in_distribution_corpus poss_distribution_their pobj_words_distribution prepc_based_on_words_on prep_between_similarity_words det_similarity_the dobj_compute_similarity aux_compute_to vmod_approaches_compute amod_approaches_many cop_approaches_been aux_approaches_have expl_approaches_There
N04-1041	P90-1034	o	One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus -LRB- Hindle 1990 Lin 1998 -RRB-	num_Lin_1998 dep_Hindle_Lin dep_Hindle_1990 dep_corpus_Hindle det_corpus_a prep_in_distribution_corpus poss_distribution_their pobj_words_distribution prepc_based_on_words_on prep_between_similarity_words det_similarity_the dobj_computing_similarity prepc_by_thesauri_computing amod_thesauri_automatic dep_constructs_thesauri nn_constructs_approach num_constructs_One
N07-1016	P90-1034	o	This second source of evidence is sometimes referred to as distributional similarity -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 dep_similarity_Hindle amod_similarity_distributional pobj_referred_similarity prepc_as_to_referred_as advmod_referred_sometimes auxpass_referred_is nsubjpass_referred_source prep_of_source_evidence amod_source_second det_source_This
N09-3007	P90-1034	o	glish nouns first appeared in Hindle -LRB- 1990 -RRB-	appos_Hindle_1990 prep_in_appeared_Hindle advmod_appeared_first nsubj_appeared_nouns nn_nouns_glish
P05-1016	P90-1034	o	Researchers have mostly looked at representing words by their surrounding words -LRB- Lund and Burgess 1996 -RRB- and by their syntactical contexts -LRB- Hindle 1990 Lin 1998 -RRB-	num_Lin_1998 dep_1990_Lin amod_1990_Hindle dep_contexts_1990 amod_contexts_syntactical poss_contexts_their pobj_by_contexts num_Burgess_1996 conj_and_Lund_Burgess appos_words_Burgess appos_words_Lund amod_words_surrounding poss_words_their prep_by_representing_words dobj_representing_words conj_and_looked_by prepc_at_looked_representing advmod_looked_mostly aux_looked_have nsubj_looked_Researchers
P05-1077	P90-1034	o	4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text -LRB- Hindle 1990 Lin 1998 -RRB-	dep_Lin_1998 dep_Hindle_Lin appos_Hindle_1990 dep_text_Hindle prep_in_meaning_text poss_meaning_their nn_words_clustering nn_community_NLP det_community_the pobj_done_meaning prepc_according_to_done_to prep_on_done_words prep_in_done_community auxpass_done_been aux_done_has csubjpass_done_Lists prep_of_lot_work det_lot_A dobj_Lists_lot nsubj_Lists_Similarity nn_Similarity_Noun nn_Similarity_Building num_Similarity_4
P06-1015	P90-1034	o	To date researchers have harvested with varying success several resources including concept lists -LRB- Lin and Pantel 2002 -RRB- topic signatures -LRB- Lin and Hovy 2000 -RRB- facts -LRB- Etzioni et al. 2005 -RRB- and word similarity lists -LRB- Hindle 1990 -RRB-	dep_Hindle_1990 dep_lists_Hindle nn_lists_similarity nn_lists_word dep_2005_al. nn_al._et num_Etzioni_2005 num_Hovy_2000 conj_and_Lin_Hovy dep_signatures_Hovy dep_signatures_Lin nn_signatures_topic num_Pantel_2002 conj_and_Lin_Pantel dep_lists_Etzioni conj_lists_facts conj_lists_signatures dep_lists_Pantel dep_lists_Lin nn_lists_concept conj_and_resources_lists prep_including_resources_lists amod_resources_several amod_success_varying dobj_harvested_lists dobj_harvested_resources prep_with_harvested_success aux_harvested_have nsubj_harvested_researchers prep_to_harvested_date
P06-1045	P90-1034	p	For example Hindle -LRB- 1990 -RRB- used cooccurrences between verbs and their subjects and objects and proposed a similarity metric based on mutual information but no exploration concerning the effectiveness of other kinds of word relationship is provided although it is extendable to any kinds of contextual information	amod_information_contextual prep_of_kinds_information det_kinds_any prep_to_extendable_kinds cop_extendable_is nsubj_extendable_it mark_extendable_although advcl_provided_extendable auxpass_provided_is nsubjpass_provided_exploration nn_relationship_word prep_of_kinds_relationship amod_kinds_other prep_of_effectiveness_kinds det_effectiveness_the dobj_concerning_effectiveness vmod_exploration_concerning neg_exploration_no amod_information_mutual prep_on_based_information vmod_similarity_based amod_similarity_metric det_similarity_a dobj_proposed_similarity nsubj_proposed_Hindle poss_subjects_their conj_and_verbs_objects conj_and_verbs_subjects prep_between_cooccurrences_objects prep_between_cooccurrences_subjects prep_between_cooccurrences_verbs conj_but_used_provided conj_and_used_proposed dobj_used_cooccurrences nsubj_used_Hindle prep_for_used_example appos_Hindle_1990
P06-1045	P90-1034	o	Various methods -LRB- Hindle 1990 Lin 1998 Hagiwara et al. 2005 -RRB- have been proposed for synonym acquisition	nn_acquisition_synonym prep_for_proposed_acquisition auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods num_Hagiwara_2005 nn_Hagiwara_al. nn_Hagiwara_et dep_Lin_Hagiwara num_Lin_1998 dep_Hindle_Lin appos_Hindle_1990 appos_methods_Hindle amod_methods_Various ccomp_``_proposed
P06-1072	P90-1034	o	The only difference is that we 5See also work on partial parsing as a task in its own right Hindle -LRB- 1990 -RRB- inter alia	nn_alia_inter nn_alia_Hindle appos_Hindle_1990 amod_right_own poss_right_its prep_in_task_right det_task_a amod_parsing_partial dep_work_alia prep_as_work_task prep_on_work_parsing advmod_work_also nsubj_work_5See nsubj_work_we mark_work_that ccomp_is_work nsubj_is_difference amod_difference_only det_difference_The
P06-1100	P90-1034	o	1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web including facts -LRB- Etzioni et al. 2005 -RRB- semantic lexicons -LRB- Riloff and Shepherd 1997 -RRB- concept lists -LRB- Lin and Pantel 2002 -RRB- and word similarity lists -LRB- Hindle 1990 -RRB-	dep_Hindle_1990 dep_lists_Hindle nn_lists_similarity nn_lists_word num_Pantel_2002 conj_and_Lin_Pantel dep_lists_Pantel dep_lists_Lin nn_lists_concept num_Shepherd_1997 conj_and_Riloff_Shepherd dep_lexicons_Shepherd dep_lexicons_Riloff amod_lexicons_semantic dep_al._2005 nn_al._et advmod_Etzioni_al. conj_and_facts_lists conj_and_facts_lists appos_facts_lexicons appos_facts_Etzioni det_Web_the conj_and_text_Web prep_including_knowledge_lists prep_including_knowledge_lists prep_including_knowledge_facts prep_from_knowledge_Web prep_from_knowledge_text nn_knowledge_mining prep_for_algorithms_knowledge amod_algorithms_many dobj_developed_algorithms aux_developed_have nsubj_developed_researchers nn_researchers_NLP nn_researchers_Introduction num_researchers_1 ccomp_``_developed
P06-1101	P90-1034	o	3.2 -LRB- m n -RRB- cousin Classification The classifier for learning coordinate terms relies on the notion of distributional similarity i.e. the idea that two words with similar meanings will be used in similar contexts -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 dep_contexts_Hindle amod_contexts_similar prep_in_used_contexts auxpass_used_be aux_used_will nsubjpass_used_words mark_used_that amod_meanings_similar prep_with_words_meanings num_words_two ccomp_idea_used det_idea_the advmod_idea_i.e. appos_similarity_idea amod_similarity_distributional prep_of_notion_similarity det_notion_the prep_on_relies_notion nsubj_relies_classifier amod_terms_coordinate dobj_learning_terms prepc_for_classifier_learning det_classifier_The rcmod_Classification_relies nn_Classification_cousin dep_Classification_3.2 appos_m_n dep_3.2_m
P06-1102	P90-1034	o	Many methods have been proposed to compute distributional similarity between words e.g. -LRB- Hindle 1990 -RRB- -LRB- Pereira et al. 1993 -RRB- -LRB- Grefenstette 1994 -RRB- and -LRB- Lin 1998 -RRB-	amod_Lin_1998 amod_Grefenstette_1994 amod_Pereira_1993 dep_Pereira_al. nn_Pereira_et dep_Hindle_1990 conj_and_words_Lin appos_words_Grefenstette appos_words_Pereira appos_words_Hindle dep_words_e.g. prep_between_similarity_Lin prep_between_similarity_words amod_similarity_distributional dobj_compute_similarity aux_compute_to xcomp_proposed_compute auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_Many ccomp_``_proposed
P06-1116	P90-1034	o	We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space Hindles -LRB- 1990 -RRB- measure the weighted Lin measure -LRB- Wu and Zhou 2003 -RRB- the Skew divergence measure -LRB- Lee 1999 -RRB- the Jensen-Shannon -LRB- JS -RRB- divergence measure -LRB- Lin 1991 -RRB- Jaccards coef cient -LRB- van Rijsbergen 1979 -RRB- and the Confusion probability -LRB- Essen and Steinbiss 1992 -RRB-	amod_Essen_1992 conj_and_Essen_Steinbiss dep_probability_Steinbiss dep_probability_Essen nn_probability_Confusion det_probability_the amod_Rijsbergen_1979 nn_Rijsbergen_van dep_cient_Rijsbergen conj_and_coef_probability dobj_coef_cient nsubj_coef_Jaccards amod_Lin_1991 dep_measure_Lin nn_measure_divergence nn_measure_Jensen-Shannon det_measure_the appos_Jensen-Shannon_JS amod_Lee_1999 dep_measure_Lee nn_measure_divergence dep_measure_Skew det_measure_the dep_Wu_2003 conj_and_Wu_Zhou appos_measure_Zhou appos_measure_Wu nn_measure_Lin amod_measure_weighted det_measure_the rcmod_measure_probability rcmod_measure_coef appos_measure_measure appos_measure_measure appos_measure_measure nn_measure_Hindles appos_Hindles_1990 nn_space_vector nn_space_syntactic det_space_the prep_for_measures_space nn_measures_similarity amod_measures_used advmod_used_commonly dep_the_measure prep_following_the_measures amod_contexts_windowbased nn_measure_similarity nn_measure_cosine det_measure_the conj_and_use_the prep_for_use_contexts dobj_use_measure nsubj_use_We
P07-1028	P90-1034	o	We will be using the similarity metrics shown in Table 1 Cosine the Dice and Jaccard coefficients and Hindles -LRB- 1990 -RRB- and Lins -LRB- 1998 -RRB- mutual information-based metrics	amod_metrics_information-based amod_metrics_mutual nn_metrics_Lins appos_Lins_1998 appos_Hindles_1990 nn_coefficients_Jaccard det_Dice_the conj_and_Cosine_metrics conj_and_Cosine_Hindles conj_and_Cosine_coefficients conj_and_Cosine_Dice num_Table_1 prep_in_shown_Table vmod_metrics_shown nn_metrics_similarity det_metrics_the dep_using_metrics dep_using_Hindles dep_using_coefficients dep_using_Dice dep_using_Cosine dobj_using_metrics aux_using_be aux_using_will nsubj_using_We ccomp_``_using
P07-1057	P90-1034	o	Hindle -LRB- 1990 -RRB- uses a mutual-information based metric derived from the distribution of subject verb and object in a large corpus to classify nouns	dobj_classify_nouns aux_classify_to amod_corpus_large det_corpus_a nsubj_object_metric xcomp_verb_classify prep_in_verb_corpus conj_and_verb_object nsubj_verb_metric dep_verb_based prep_of_distribution_subject det_distribution_the prep_from_derived_distribution vmod_metric_derived vmod_mutual-information_object vmod_mutual-information_verb det_mutual-information_a dobj_uses_mutual-information nsubj_uses_Hindle appos_Hindle_1990
P08-1002	P90-1034	o	Our method is thus related to previous work based on Harris -LRB- 1985 -RRB- s distributional hypothesis .2 It has been used to determine both word and syntactic path similarity -LRB- Hindle 1990 Lin 1998a Lin and Pantel 2001 -RRB-	appos_Lin_1998a dep_Hindle_2001 conj_and_Hindle_Pantel conj_and_Hindle_Lin conj_and_Hindle_Lin conj_and_Hindle_1990 appos_similarity_Pantel appos_similarity_Lin appos_similarity_Lin appos_similarity_1990 appos_similarity_Hindle nn_similarity_path nn_similarity_syntactic conj_and_word_similarity preconj_word_both dobj_determine_similarity dobj_determine_word aux_determine_to xcomp_used_determine auxpass_used_been aux_used_has nsubjpass_used_It rcmod_.2_used dep_hypothesis_.2 amod_hypothesis_distributional dobj_s_hypothesis nsubj_s_Harris appos_Harris_1985 prepc_on_based_s amod_work_previous vmod_related_based prep_to_related_work advmod_related_thus auxpass_related_is nsubjpass_related_method poss_method_Our ccomp_``_related
P08-2008	P90-1034	o	Hindle -LRB- 1990 -RRB- grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts	amod_contexts_syntactic poss_contexts_their prep_of_similarity_contexts det_similarity_the prep_on_based_similarity vmod_lists_based amod_lists_thesaurus-like prep_into_nouns_lists amod_nouns_grouped nn_nouns_Hindle appos_Hindle_1990
P08-3001	P90-1034	o	A number of researches which utilized distributional similarity have been conducted including -LRB- Hindle 1990 Lin 1998 Geffet and Dagan 2004 -RRB- and many others	amod_others_many num_Lin_1998 conj_and_Hindle_others dep_Hindle_2004 conj_and_Hindle_Dagan conj_and_Hindle_Geffet conj_and_Hindle_Lin conj_and_Hindle_1990 prep_including_conducted_others prep_including_conducted_Dagan prep_including_conducted_Geffet prep_including_conducted_Lin prep_including_conducted_1990 prep_including_conducted_Hindle auxpass_conducted_been aux_conducted_have amod_similarity_distributional dep_utilized_conducted dobj_utilized_similarity nsubj_utilized_which ccomp_researches_utilized prepc_of_number_researches det_number_A
P09-1052	P90-1034	o	Syntactic context information is used -LRB- Hindle 1990 Ruge 1992 Lin 1998 -RRB- to compute term similarities based on which similar words to a particular word can directly be returned	auxpass_returned_be advmod_returned_directly aux_returned_can nsubjpass_returned_words prep_on_returned_which amod_word_particular det_word_a prep_to_words_word amod_words_similar pcomp_based_returned prep_similarities_based nn_similarities_term dobj_compute_similarities aux_compute_to num_Lin_1998 appos_Ruge_1992 dep_Hindle_Lin dep_Hindle_Ruge dep_Hindle_1990 xcomp_used_compute dep_used_Hindle auxpass_used_is nsubjpass_used_information nn_information_context amod_information_Syntactic
P09-2018	P90-1034	o	This has been now an active research area for a couple of decades -LRB- Hindle 1990 Lin 1998 Weeds and Weir 2003 -RRB-	amod_Weeds_2003 conj_and_Weeds_Weir dep_Lin_Weir dep_Lin_Weeds num_Lin_1998 dep_Hindle_Lin appos_Hindle_1990 prep_of_couple_decades det_couple_a dep_area_Hindle prep_for_area_couple nn_area_research amod_area_active det_area_an advmod_area_now cop_area_been aux_area_has nsubj_area_This ccomp_``_area
P91-1017	P90-1034	o	The use of such relations -LRB- mainly relations between verbs or nouns and their arguments and modifiers -RRB- for various purposes has received growing attention in recent research -LRB- Church and Hanks 1990 Zernik and Jacobs 1990 Hindle 1990 -RRB-	amod_Hindle_1990 dep_Zernik_Hindle conj_and_Zernik_1990 conj_and_Zernik_Jacobs conj_and_Church_1990 conj_and_Church_Jacobs conj_and_Church_Zernik conj_and_Church_1990 conj_and_Church_Hanks dep_research_Zernik dep_research_1990 dep_research_Hanks dep_research_Church amod_research_recent amod_attention_growing prep_in_received_research dobj_received_attention aux_received_has nsubj_received_use amod_purposes_various poss_arguments_their conj_and_verbs_modifiers conj_and_verbs_arguments conj_or_verbs_nouns prep_between_relations_modifiers prep_between_relations_arguments prep_between_relations_nouns prep_between_relations_verbs advmod_relations_mainly amod_relations_such prep_for_use_purposes dep_use_relations prep_of_use_relations det_use_The ccomp_``_received
P91-1017	P90-1034	o	More specifically two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment -LRB- Hindle and Rooth 1990 -RRB- and pronoun references -LRB- Dagan and Itai 1990a Dagan and Itai 1990b -RRB-	appos_Dagan_1990b conj_and_Dagan_Itai dep_Dagan_Itai dep_Dagan_Dagan conj_and_Dagan_1990a conj_and_Dagan_Itai dep_references_1990a dep_references_Itai dep_references_Dagan nn_references_pronoun dep_Hindle_1990 conj_and_Hindle_Rooth conj_and_PP-attachment_references dep_PP-attachment_Rooth dep_PP-attachment_Hindle prep_of_cases_references prep_of_cases_PP-attachment nn_cases_ambiguity dobj_resolving_cases amod_relations_lexical amod_data_statistical prepc_for_use_resolving prep_on_use_relations dobj_use_data aux_use_to xcomp_suggested_use aux_suggested_have nsubj_suggested_works advmod_suggested_specifically amod_works_recent num_works_two dep_specifically_More ccomp_``_suggested
P91-1017	P90-1034	o	His results may be improved if more sophisticated techniques and larger corpora are used to establish similarity between words -LRB- such as in -LRB- Hindle 1990 -RRB- -RRB-	dep_Hindle_1990 pobj_in_Hindle prepc_such_as_words_in prep_between_similarity_words dobj_establish_similarity aux_establish_to xcomp_used_establish auxpass_used_are nsubjpass_used_corpora nsubjpass_used_techniques mark_used_if amod_corpora_larger conj_and_techniques_corpora amod_techniques_sophisticated advmod_sophisticated_more advcl_improved_used auxpass_improved_be aux_improved_may nsubjpass_improved_results poss_results_His
P91-1027	P90-1034	o	Three recent papers in this area are Church and Hanks -LRB- 1990 -RRB- Hindle -LRB- 1990 -RRB- and Smadja and McKeown -LRB- 1990 -RRB-	appos_McKeown_1990 conj_and_Smadja_McKeown appos_Hindle_1990 appos_Hanks_1990 conj_and_Church_McKeown conj_and_Church_Smadja conj_and_Church_Hindle conj_and_Church_Hanks cop_Church_are nsubj_Church_papers det_area_this prep_in_papers_area amod_papers_recent num_papers_Three
P91-1027	P90-1034	o	-LRB- 1 -RRB- a. I expected \ -LSB- nv the man who smoked NP \ -RSB- to eat ice-cream h. I doubted \ -LSB- NP the man who liked to eat ice-cream NP \ -RSB- Current high-coverage parsers tend to use either custom hand-generated lists of subcategorization frames -LRB- e.g. Hindle 1983 -RRB- or published handgenerated lists like the Ozford Advanced Learner 's Dictionary of Contemporary English Hornby and Covey -LRB- 1973 -RRB- -LRB- e.g. DeMarcken 1990 -RRB-	amod_DeMarcken_1990 dep_e.g._DeMarcken dep_Covey_e.g. appos_Covey_1973 conj_and_English_Covey conj_and_English_Hornby nn_English_Contemporary prep_of_Dictionary_Covey prep_of_Dictionary_Hornby prep_of_Dictionary_English poss_Dictionary_Learner nn_Learner_Advanced nn_Learner_Ozford det_Learner_the prep_like_lists_Dictionary amod_lists_handgenerated appos_Hindle_lists conj_or_Hindle_published amod_Hindle_1983 advcl_,_published advcl_,_Hindle dep_-LRB-_e.g. nn_frames_subcategorization prep_of_lists_frames amod_lists_hand-generated appos_custom_lists preconj_custom_either dobj_use_custom aux_use_to xcomp_tend_use nsubj_tend_NP amod_parsers_high-coverage amod_parsers_Current dep_parsers_\ nn_\_NP amod_\_ice-cream dobj_eat_parsers aux_eat_to xcomp_liked_eat nsubj_liked_who rcmod_man_liked det_man_the dep_NP_man rcmod_\_tend dobj_doubted_\ nsubj_doubted_I rcmod_h._doubted amod_h._ice-cream dobj_eat_h. aux_eat_to num_NP_\ dobj_smoked_NP nsubj_smoked_who rcmod_man_smoked det_man_the dep_nv_man dep_\_nv xcomp_expected_eat dobj_expected_\ nsubj_expected_I mark_expected_a. dep_expected_1
P92-1028	P90-1034	p	8Interestingly in work on the automated classification of nouns -LRB- Hindle 1990 -RRB- also noted problems with empty words that depend on their complements for meaning	prep_for_complements_meaning nsubj_complements_their mark_complements_on advcl_depend_complements nsubj_depend_that rcmod_words_depend amod_words_empty prep_with_problems_words dobj_noted_problems advmod_noted_also nsubj_noted_Hindle prep_in_noted_work advmod_noted_8Interestingly amod_Hindle_1990 prep_of_classification_nouns amod_classification_automated det_classification_the prep_on_work_classification
P92-1028	P90-1034	o	In comparison most corpus-based algorithms employ substantially larger corpora -LRB- e.g. 1 million words -LRB- de Marcken 1990 -RRB- 2.5 million words -LRB- Brent 1991 -RRB- 6 million words -LRB- Hindle 1990 -RRB- 13 million words -LRB- Hindle & Rooth 1991 -RRB- -RRB-	dep_Hindle_1991 conj_and_Hindle_Rooth dep_words_Rooth dep_words_Hindle num_words_million number_million_13 dep_Hindle_1990 appos_words_words appos_words_Hindle num_words_million number_million_6 dep_Brent_1991 appos_words_Brent num_words_million number_million_2.5 dep_Marcken_1990 nn_Marcken_de dep_words_words appos_words_words dep_words_Marcken num_words_million number_million_1 dep_e.g._words dep_corpora_e.g. amod_corpora_larger advmod_larger_substantially dobj_employ_corpora nsubj_employ_algorithms prep_in_employ_comparison amod_algorithms_corpus-based amod_algorithms_most
P93-1022	P90-1034	o	Statistical data about these various cooccurrence relations is employed for a variety of applications such as speech recognition -LRB- Jelinek 1990 -RRB- language generation -LRB- Smadja and McKeown 1990 -RRB- lexicography -LRB- Church and Hanks 1990 -RRB- machine translation -LRB- Brown et al. Sadler 1989 -RRB- information retrieval -LRB- Maarek and Smadja 1989 -RRB- and various disambiguation tasks -LRB- Dagan et al. 1991 Hindle and Rooth 1991 Grishman et al. 1986 Dagan and Itai 1990 -RRB-	dep_Dagan_1990 conj_and_Dagan_Itai num_Grishman_1986 nn_Grishman_al. nn_Grishman_et dep_Hindle_Itai dep_Hindle_Dagan conj_and_Hindle_Grishman conj_and_Hindle_1991 conj_and_Hindle_Rooth dep_Dagan_Grishman dep_Dagan_1991 dep_Dagan_Rooth dep_Dagan_Hindle appos_Dagan_1991 dep_Dagan_al. nn_Dagan_et dep_tasks_Dagan nn_tasks_disambiguation amod_tasks_various conj_and_Maarek_1989 conj_and_Maarek_Smadja dep_retrieval_1989 dep_retrieval_Smadja dep_retrieval_Maarek nn_retrieval_information dep_Sadler_1989 dep_al._Sadler nn_al._et amod_al._Brown dep_translation_al. nn_translation_machine amod_Church_1990 conj_and_Church_Hanks appos_lexicography_Hanks appos_lexicography_Church dep_Smadja_1990 conj_and_Smadja_McKeown appos_generation_McKeown appos_generation_Smadja nn_generation_language dep_Jelinek_1990 conj_and_recognition_tasks conj_and_recognition_retrieval conj_and_recognition_translation conj_and_recognition_lexicography conj_and_recognition_generation dep_recognition_Jelinek nn_recognition_speech prep_such_as_applications_tasks prep_such_as_applications_retrieval prep_such_as_applications_translation prep_such_as_applications_lexicography prep_such_as_applications_generation prep_such_as_applications_recognition prep_of_variety_applications det_variety_a prep_for_employed_variety auxpass_employed_is nsubjpass_employed_data nn_relations_cooccurrence amod_relations_various det_relations_these prep_about_data_relations amod_data_Statistical ccomp_``_employed
P93-1022	P90-1034	o	The search is based on the property that when computing sim -LRB- wl w2 -RRB- words that have high mutual information values 5The nominator in our metric resembles the similarity metric in -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 dep_in_Hindle prep_metric_in nn_metric_similarity det_metric_the xcomp_resembles_metric nsubj_resembles_sim dep_resembles_computing advmod_resembles_when poss_metric_our nn_nominator_5The nn_nominator_values nn_nominator_information amod_nominator_mutual amod_nominator_high prep_in_have_metric dobj_have_nominator nsubj_have_that rcmod_words_have appos_wl_w2 appos_sim_words dep_sim_wl advcl_that_resembles dep_property_that det_property_the prep_on_based_property auxpass_based_is nsubjpass_based_search det_search_The ccomp_``_based
P93-1024	P90-1034	o	Hindle -LRB- 1990 -RRB- proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of similar events that have been seen	auxpass_seen_been aux_seen_have nsubjpass_seen_that rcmod_events_seen amod_events_similar prep_of_that_events prep_from_events_that amod_events_unseen prep_of_likelihood_events det_likelihood_the dobj_estimating_likelihood nn_problem_sparseness det_problem_the prepc_by_dealing_estimating prep_with_dealing_problem xcomp_proposed_dealing vmod_Hindle_proposed appos_Hindle_1990
P94-1032	P90-1034	o	Some researchers apply shallow or partial parsers -LRB- Smadja 1991 Hindle 1990 -RRB- to acquiring specific patterns from texts	amod_patterns_specific prep_from_acquiring_texts dobj_acquiring_patterns dep_Hindle_1990 dep_Smadja_Hindle dep_Smadja_1991 appos_parsers_Smadja amod_parsers_partial amod_parsers_shallow conj_or_shallow_partial prepc_to_apply_acquiring dobj_apply_parsers nsubj_apply_researchers det_researchers_Some ccomp_``_apply
P97-1066	P90-1034	o	In fact we are considering word usage rather than word meanin \ -RSB- ' -LRB- Zernik 1990 -RRB- following in this the distributional point of view see -LRB- Harris 1968 -RRB- -LRB- Hindle 1990 -RRB-	dep_Hindle_1990 amod_Harris_1968 dep_see_Harris prep_of_point_view amod_point_distributional det_point_the det_point_this appos_following_Hindle conj_following_see prep_in_following_point dep_following_Zernik dep_following_\ dep_following_usage dep_Zernik_1990 nn_\_meanin nn_\_word conj_negcc_usage_\ nn_usage_word xcomp_considering_following aux_considering_are nsubj_considering_we prep_in_considering_fact rcmod_``_considering
P97-1066	P90-1034	o	Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools -LRB- Hindle 1990 -RRB- -LRB- Zernik 1990 -RRB- -LRB- Resnik 1993 -RRB- or for automatic thesaurus generation -LRB- Grefenstette 1994 -RRB-	amod_Grefenstette_1994 dep_generation_Grefenstette nn_generation_thesaurus amod_generation_automatic pobj_for_generation dep_Resnik_1993 dep_Zernik_1990 dep_Hindle_1990 appos_tools_Hindle nn_tools_ANLP amod_resources_lexical prep_for_build_tools dobj_build_resources aux_build_to dep_build_order mark_build_in amod_clusters_semantic advcl_extract_build prep_from_extract_corpora dobj_extract_clusters aux_extract_to conj_or_used_for dep_used_Resnik dep_used_Zernik xcomp_used_extract advmod_used_often auxpass_used_are nsubjpass_used_methods amod_methods_probabilistic amod_methods_Statistical conj_or_Statistical_probabilistic
P98-1082	P90-1034	o	Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora -LRB- Hindle 1990 Agirre and Rigau 1996 -RRB-	amod_Agirre_1996 conj_and_Agirre_Rigau dep_Hindle_Rigau dep_Hindle_Agirre appos_Hindle_1990 appos_corpora_Hindle amod_corpora_large amod_corpora_large advmod_large_very advmod_large_even conj_or_large_large prep_for_designed_corpora vmod_methods_designed amod_methods_statistical prep_on_based_methods advmod_based_generally auxpass_based_are nsubjpass_based_Works nn_disambiguation_sense nn_disambiguation_word conj_and_similarity_disambiguation nn_similarity_word prep_on_Works_disambiguation prep_on_Works_similarity
P98-2127	P90-1034	o	In -LRB- Hindle 1990 -RRB- a small set of sample results are presented	auxpass_presented_are nsubjpass_presented_set nn_results_sample prep_of_set_results amod_set_small det_set_a rcmod_Hindle_presented amod_Hindle_1990 pobj_In_Hindle dep_``_In
P98-2127	P90-1034	o	When the value of Ilw r w 'll is unknown we assume that A and C are conditionally independent given B The probability of A B and C cooccurring is estimated by PMLE -LRB- B -RRB- PMLE -LRB- A \ -LSB- B -RRB- PMLE -LRB- C \ -LSB- B -RRB- where PMLE is the maximum likelihood estimation of a probability distribution and P.LE -LRB- B -RRB- = II * * * ll ' P. ~ E -LRB- AIB -RRB- = II * ~ * ll ' P LE -LRB- CIB -RRB- = When the value of Hw r w ~ H is known we can obtain PMLE -LRB- A B C -RRB- directly PMLE -LRB- A B C -RRB- = \ -LSB- \ -LSB- w r wll / \ -LSB- \ -LSB- * * * H Let I -LRB- w r w ~ -RRB- denote the amount information contained in Hw r w ~ \ -RSB- \ -RSB- = c. Its value can be corn769 simgindZe -LRB- Wl W2 -RRB- = ~ ' ~ -LRB- r w -RRB- eTCwl -RRB- NTCw2 -RRB- Aresubj.of.obj-of -RCB- min -LRB- I -LRB- Wl r w -RRB- I -LRB- w2 r w -RRB- -RRB- simHindte -LRB- Wl W2 -RRB- = ~ -LRB- r w -RRB- eT -LRB- w -RRB- nT -LRB- w2 -RRB- min -LRB- I -LRB- wl r w -RRB- I -LRB- w2 r w -RRB- -RRB- \ -RSB- T -LRB- Wl -RRB- NT -LRB- w2 -RRB- I simcosine -LRB- Wl W2 -RRB- = x/IZ -LRB- w ~ -RRB- llZ -LRB- w2 -RRB- l 2x IT -LRB- wl -RRB- nZ -LRB- w2 -RRB- l simDice -LRB- Wl W2 -RRB- = iT -LRB- wl -RRB- l + lT -LRB- w2 -RRB- I simJacard -LRB- Wl W2 -RRB- = T -LRB- wl -RRB- OT -LRB- w2 -RRB- l T -LRB- wl -RRB- + T -LRB- w2 -RRB- l-IT -LRB- Wl -RRB- rlT -LRB- w2 -RRB- l Figure 1 Other Similarity Measures puted as follows I -LRB- w r w ' -RRB- = _ Iog -LRB- PMLE -LRB- B -RRB- PMLE -LRB- A \ -RSB- B -RRB- PMLE -LRB- CIB -RRB- -RRB- -- -LRB- log PMLE -LRB- A B C -RRB- -RRB- log IIw r wflll * r * ll IIw r * ll xll * r w 'll It is worth noting that I -LRB- w r w ' -RRB- is equal to the mutual information between w and w ' -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 dep_w_Hindle conj_and_w_w prep_between_information_w prep_between_information_w amod_information_mutual det_information_the prep_to_equal_information cop_equal_is nsubj_equal_I mark_equal_that appos_w_w appos_w_r dep_I_w ccomp_noting_equal xcomp_worth_noting cop_worth_is nsubj_worth_It aux_worth_'ll dep_*_worth appos_*_w appos_*_r nn_*_xll nn_*_ll dep_*_* dep_IIw_* appos_IIw_r dep_ll_* appos_*_r nn_*_wflll dep_IIw_* dep_IIw_r nn_IIw_log dep_IIw_= dep_IIw_I appos_A_C appos_A_B dep_PMLE_A nn_PMLE_log appos_PMLE_CIB nn_B_\ det_B_A dep_PMLE_PMLE appos_PMLE_B nn_PMLE_B appos_PMLE_PMLE dep_Iog_PMLE num_Iog__ dep_=_PMLE dep_=_Iog appos_w_w dep_w_r dep_I_w mark_follows_as advcl_puted_follows dep_Measures_IIw dep_Measures_ll parataxis_Measures_IIw vmod_Measures_puted nn_Measures_Similarity amod_Measures_Other num_Figure_1 nn_Figure_l nn_Figure_rlT appos_rlT_w2 nn_rlT_l-IT appos_l-IT_Wl nn_l-IT_T appos_T_w2 conj_+_T_Figure appos_T_wl nn_T_l nn_T_OT appos_OT_w2 nn_OT_T appos_T_wl dobj_=_Figure dobj_=_T appos_Wl_W2 dep_simJacard_= dep_simJacard_Wl dep_I_simJacard dep_l_I appos_l_w2 conj_+_l_lT nn_l_iT appos_iT_wl dep_=_lT dep_=_l appos_Wl_W2 dep_simDice_Measures amod_simDice_= dep_simDice_Wl nn_simDice_l nn_simDice_nZ nn_simDice_2x appos_nZ_w2 nn_nZ_IT appos_IT_wl nn_2x_l nn_2x_llZ appos_llZ_w2 nn_llZ_~ nn_llZ_w nn_llZ_x/IZ dobj_=_simDice appos_Wl_W2 dep_simcosine_= dep_simcosine_Wl nsubj_simcosine_I rcmod_NT_simcosine appos_NT_w2 nn_NT_T appos_T_Wl dep_\_min dep_\_nT dep_\_= dep_\_A dep_\_PMLE appos_w2_w appos_w2_r dep_I_w2 appos_wl_w appos_wl_r num_wl_I dep_min_I dep_min_wl nn_min_w2 nn_nT_eT appos_eT_w nn_eT_\ amod_~_= appos_Wl_W2 appos_w2_w appos_w2_r dep_I_w2 appos_Wl_w appos_Wl_r num_Wl_I dep_min_Wl amod_min_Aresubj.of.obj-of dep_min_NTCw2 nn_NTCw2_eTCwl nn_NTCw2_~ appos_r_w dep_~_r nn_~_~ dep_=_min appos_Wl_W2 dep_simgindZe_~ dep_simgindZe_Wl dep_simgindZe_simHindte dep_simgindZe_I amod_simgindZe_= dep_simgindZe_Wl amod_simgindZe_corn769 cop_simgindZe_be aux_simgindZe_can nsubj_simgindZe_= dep_simgindZe_\ poss_value_Its nn_value_c. dep_=_value dep_\_\ dep_\_w dep_\_Hw num_\_~ conj_Hw_r prep_in_contained_simgindZe vmod_information_contained nn_information_amount det_information_the dobj_denote_information nsubj_denote_I dep_denote_w nn_~_w appos_w_~ appos_w_r dep_I_w dep_H_* dep_H_* num_H_\ dep_*_* num_\_\ dep_w_Let dep_w_H dep_w_wll dep_w_r dep_\_w dep_\_r dep_\_denote dep_\_\ appos_A_C appos_A_B appos_A_C appos_A_B dep_PMLE_A advmod_obtain_directly dobj_obtain_PMLE aux_obtain_can nsubj_obtain_we dep_obtain_\ auxpass_known_is nsubjpass_known_value advmod_known_When nn_H_~ nn_H_w conj_Hw_H conj_Hw_r prep_of_value_Hw det_value_the advcl_=_known nsubj_=_LE appos_LE_CIB dep_ll_P dep_ll_* appos_*_~ num_*_II dep_=_* npadvmod_=_E appos_E_AIB num_E_~ rcmod_ll_= appos_ll_ll amod_ll_= dep_ll_P. dep_ll_* num_*_II dep_=_* amod_P.LE_= appos_P.LE_B conj_and_distribution_P.LE nn_distribution_probability det_distribution_a prep_of_estimation_P.LE prep_of_estimation_distribution nn_estimation_likelihood nn_estimation_maximum det_estimation_the cop_estimation_is nsubj_estimation_PMLE advmod_estimation_where appos_\_ll dep_\_* rcmod_\_estimation appos_\_B nn_\_C dep_\_\ dep_\_PMLE appos_\_B det_\_A dep_PMLE_NT dep_PMLE_\ dep_PMLE_obtain nn_PMLE_B dep_PMLE_PMLE agent_estimated_PMLE auxpass_estimated_is nsubjpass_estimated_probability nn_cooccurring_C conj_and_A_cooccurring conj_and_A_B prep_of_probability_cooccurring prep_of_probability_B prep_of_probability_A det_probability_The dep_B_estimated amod_B_given dep_independent_B advmod_independent_conditionally cop_independent_are nsubj_independent_C nsubj_independent_A mark_independent_that conj_and_A_C ccomp_assume_independent nsubj_assume_we nsubj_assume_value advmod_assume_When cop_unknown_is aux_unknown_'ll nsubj_unknown_Ilw conj_Ilw_w conj_Ilw_r prepc_of_value_unknown det_value_the advcl_``_assume
P98-2127	P90-1034	o	The measure simHinate is the same as the similarity measure proposed in -LRB- Hindle 1990 -RRB- except that it does not use dependency triples with negative mutual information	amod_information_mutual amod_information_negative nn_triples_dependency prep_with_use_information dobj_use_triples neg_use_not aux_use_does nsubj_use_it mark_use_that mark_use_except dep_Hindle_1990 dep_in_Hindle prep_proposed_in vmod_measure_proposed nn_measure_similarity det_measure_the advcl_same_use prep_as_same_measure det_same_the cop_same_is nsubj_same_simHinate nn_simHinate_measure det_simHinate_The
P98-2127	P90-1034	o	Ours is 772 similar to -LRB- Grefenstette 1994 Hindle 1990 Ruge 1992 -RRB- in the use of dependency relationship as the word features based on which word similarities are computed	auxpass_computed_are nsubjpass_computed_similarities prep_on_computed_which nn_similarities_word pcomp_based_computed prep_features_based nn_features_word det_features_the nn_relationship_dependency prep_of_use_relationship det_use_the dep_Ruge_1992 appos_Hindle_1990 prep_in_Grefenstette_use dep_Grefenstette_Ruge dep_Grefenstette_Hindle dep_Grefenstette_1994 prep_as_similar_features prep_to_similar_Grefenstette amod_772_similar nsubj_is_772 dep_Ours_is
P99-1004	P90-1034	p	Arguably the most widely used is the mutual information -LRB- Hindle 1990 Church and Hanks 1990 Dagan et al. 1995 Luk 1995 D. Lin 1998a -RRB-	appos_Lin_1998a nn_Lin_D. dep_Luk_1995 dep_al._1995 nn_al._et nn_al._Dagan dep_Church_Lin conj_and_Church_Luk conj_and_Church_al. conj_and_Church_1990 conj_and_Church_Hanks dep_Hindle_Luk dep_Hindle_al. dep_Hindle_1990 dep_Hindle_Hanks dep_Hindle_Church appos_Hindle_1990 dep_information_Hindle amod_information_mutual det_information_the cop_information_is nsubj_information_used advmod_used_widely det_used_the advmod_used_Arguably advmod_widely_most
W02-1107	P90-1034	o	To extract semantic information of words such as synonyms and antonyms from corpora previous research used syntactic structures -LRB- Hindle 1990 Hatzivassiloglou 1993 and Tokunaga 1995 -RRB- response time to associate synonyms and antonyms in psychological experiments -LRB- Gross 1989 -RRB- or extracting related words automatically from corpora -LRB- Grefensette 1994 -RRB-	num_Grefensette_1994 appos_corpora_Grefensette amod_words_related prep_from_extracting_corpora advmod_extracting_automatically dobj_extracting_words num_Gross_1989 conj_or_experiments_extracting appos_experiments_Gross amod_experiments_psychological conj_and_synonyms_antonyms prep_in_associate_extracting prep_in_associate_experiments dobj_associate_antonyms dobj_associate_synonyms aux_associate_to vmod_time_associate nn_time_response num_Tokunaga_1995 num_Hatzivassiloglou_1993 conj_and_1990_Tokunaga conj_and_1990_Hatzivassiloglou amod_1990_Hindle dep_structures_Tokunaga dep_structures_Hatzivassiloglou dep_structures_1990 amod_structures_syntactic amod_structures_used nn_structures_research amod_research_previous appos_synonyms_time conj_and_synonyms_structures prep_from_synonyms_corpora conj_and_synonyms_antonyms prep_such_as_words_structures prep_such_as_words_antonyms prep_such_as_words_synonyms prep_of_information_words amod_information_semantic dobj_extract_information aux_extract_To
W03-1610	P90-1034	o	The most frequently used resource for synonym extraction is large monolingual corpora -LRB- Hindle 1990 Crouch and Yang 1992 Grefenstatte 1994 Park and Choi 1997 Gasperin et al. 2001 and Lin 1998 -RRB-	conj_and_2001_Lin amod_Gasperin_1998 dep_Gasperin_Lin dep_Gasperin_2001 dep_Gasperin_al. nn_Gasperin_et dep_Park_Gasperin conj_and_Park_1997 conj_and_Park_Choi num_Grefenstatte_1994 num_Crouch_1992 conj_and_Crouch_Yang dep_Hindle_1997 dep_Hindle_Choi dep_Hindle_Park conj_Hindle_Grefenstatte conj_Hindle_Yang conj_Hindle_Crouch appos_Hindle_1990 dep_corpora_Hindle amod_corpora_monolingual amod_corpora_large cop_corpora_is nsubj_corpora_resource nn_extraction_synonym prep_for_resource_extraction amod_resource_used det_resource_The advmod_used_frequently advmod_frequently_most
W04-1216	P90-1034	o	For example the words corruption and abuse are similar because both of them can be subjects of verbs like arouse become betray cause continue cost exist force go on grow have increase lead to and persist etc and both of them can modify nouns like accusation act allegation appearance and case etc. Many methods have been proposed to compute distributional similarity between words e.g. -LRB- Hindle 1990 -RRB- -LRB- Pereira et al. 1993 -RRB- -LRB- Grefenstette 1994 -RRB- and -LRB- Lin 1998 -RRB-	num_Lin_1998 num_Grefenstette_1994 dep_al._1993 nn_al._et advmod_Pereira_al. dep_Hindle_1990 conj_and_words_Lin appos_words_Grefenstette appos_words_Pereira appos_words_Hindle dep_words_e.g. prep_between_similarity_Lin prep_between_similarity_words amod_similarity_distributional dobj_compute_similarity aux_compute_to xcomp_proposed_compute auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_Many conj_and_accusation_case conj_and_accusation_appearance conj_and_accusation_allegation conj_and_accusation_act prep_like_nouns_case prep_like_nouns_appearance prep_like_nouns_allegation prep_like_nouns_act prep_like_nouns_accusation advmod_modify_etc. dobj_modify_nouns aux_modify_can nsubj_modify_both prep_of_both_them conj_persist_etc prep_lead_to prep_go_on conj_and_arouse_persist conj_and_arouse_lead conj_and_arouse_increase conj_and_arouse_have conj_and_arouse_grow conj_and_arouse_go conj_and_arouse_force conj_and_arouse_exist conj_and_arouse_cost conj_and_arouse_continue conj_and_arouse_cause conj_and_arouse_betray conj_and_arouse_become dep_like_persist dep_like_lead dep_like_increase dep_like_have dep_like_grow dep_like_go dep_like_force dep_like_exist dep_like_cost dep_like_continue dep_like_cause dep_like_betray dep_like_become dep_like_arouse conj_and_subjects_modify prep_subjects_like prep_of_subjects_verbs cop_subjects_be aux_subjects_can nsubj_subjects_both mark_subjects_because prep_of_both_them parataxis_similar_proposed advcl_similar_modify advcl_similar_subjects cop_similar_are nsubj_similar_abuse nsubj_similar_corruption prep_for_similar_example conj_and_corruption_abuse nn_corruption_words det_corruption_the
W05-1504	P90-1034	o	If the bound is too tight to allow the correct parse of some sentence we would still like to allow an accurate partial parse a sequence of accurate parse fragments -LRB- Hindle 1990 Abney 1991 Appelt et al. 1993 Chen 1995 Grefenstette 1996 -RRB-	amod_Grefenstette_1996 num_Chen_1995 num_Appelt_1993 nn_Appelt_al. nn_Appelt_et dep_Abney_Grefenstette conj_Abney_Chen conj_Abney_Appelt num_Abney_1991 dep_Hindle_Abney appos_Hindle_1990 dep_fragments_Hindle nn_fragments_parse amod_fragments_accurate prep_of_sequence_fragments det_sequence_a dep_parse_sequence amod_parse_partial amod_parse_accurate det_parse_an dobj_allow_parse aux_allow_to xcomp_like_allow advmod_like_still aux_like_would nsubj_like_we advcl_like_tight det_sentence_some prep_of_parse_sentence amod_parse_correct det_parse_the dobj_allow_parse aux_allow_to xcomp_tight_allow advmod_tight_too cop_tight_is dep_tight_bound det_bound_the mark_bound_If
W05-1516	P90-1034	o	For example the words test and exam are similar because both of them follow verbs such as administer cancel cheat on conduct and both of them can be preceded by adjectives such as academic comprehensive diagnostic difficult Many methods have been proposed to compute distributional similarity between words -LRB- Hindle 1990 Pereira et al. 1993 Grefenstette 1994 Lin 1998 -RRB-	num_Lin_1998 appos_Grefenstette_1994 dep_Pereira_Lin conj_Pereira_Grefenstette num_Pereira_1993 nn_Pereira_al. nn_Pereira_et dep_Hindle_Pereira appos_Hindle_1990 dep_words_Hindle prep_between_similarity_words amod_similarity_distributional dobj_compute_similarity aux_compute_to xcomp_proposed_compute auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_Many conj_academic_proposed conj_academic_difficult conj_academic_diagnostic conj_academic_comprehensive prep_such_as_adjectives_academic agent_preceded_adjectives auxpass_preceded_be aux_preceded_can nsubjpass_preceded_both prep_of_both_them prep_cheat_on conj_and_administer_preceded conj_and_administer_conduct conj_and_administer_cheat conj_and_administer_cancel prep_such_as_verbs_preceded prep_such_as_verbs_conduct prep_such_as_verbs_cheat prep_such_as_verbs_cancel prep_such_as_verbs_administer dobj_follow_verbs nsubj_follow_both mark_follow_because prep_of_both_them advcl_similar_follow cop_similar_are nsubj_similar_exam nsubj_similar_test prep_for_similar_example conj_and_test_exam nn_test_words det_test_the
W06-2904	P90-1034	o	For example the words test and exam are similar because both of them can follow verbs such as administer cancel cheat on conduct etc. Many methods have been proposed to compute distributional similarity between words e.g. -LRB- Hindle 1990 Pereira et al. 1993 Grefenstette 1994 Lin 1998 -RRB-	num_Lin_1998 appos_Grefenstette_1994 dep_Pereira_Lin conj_Pereira_Grefenstette num_Pereira_1993 nn_Pereira_al. nn_Pereira_et dep_Hindle_Pereira appos_Hindle_1990 appos_words_Hindle dep_words_e.g. prep_between_similarity_words amod_similarity_distributional dobj_compute_similarity aux_compute_to xcomp_proposed_compute auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_Many prep_cheat_on parataxis_administer_proposed conj_administer_etc. conj_administer_conduct conj_administer_cheat conj_administer_cancel prepc_such_as_verbs_administer dobj_follow_verbs aux_follow_can nsubj_follow_both mark_follow_because prep_of_both_them advcl_similar_follow cop_similar_are nsubj_similar_exam nsubj_similar_test prep_for_similar_example conj_and_test_exam nn_test_words det_test_the
W93-0107	P90-1034	o	More recent papers Hindle -LRB- 1990 -RRB- Pereira and Tishby -LRB- 1992 -RRB- proposed to cluster nouns on the basis of a metric derived from the distribution of subject verb and object in the texts	det_texts_the prep_in_object_texts prep_of_distribution_subject det_distribution_the prep_from_derived_distribution vmod_metric_derived det_metric_a prep_of_basis_metric det_basis_the prep_on_cluster_basis dobj_cluster_nouns aux_cluster_to xcomp_proposed_cluster vmod_Tishby_proposed appos_Tishby_1992 conj_and_Hindle_object conj_and_Hindle_verb conj_and_Hindle_Tishby conj_and_Hindle_Pereira appos_Hindle_1990 dep_papers_object dep_papers_verb dep_papers_Tishby dep_papers_Pereira dep_papers_Hindle amod_papers_recent advmod_papers_More
W93-0113	P90-1034	o	A number of knowledge-rich \ -LSB- Jacobs and Rau 1990 Calzolari and Bindi 1990 Mauldin 1991 \ -RSB- and knowledge-poor \ -LSB- Brown et al. 1992 Hindle 1990 Ruge 1991 Grefenstette 1992 \ -RSB- methods have been proposed for recognizing when words are similar	cop_similar_are nsubj_similar_words advmod_similar_when advcl_recognizing_similar prepc_for_proposed_recognizing auxpass_proposed_been aux_proposed_have nsubjpass_proposed_al. dep_proposed_Brown nn_methods_Hindle num_\_1992 dep_Ruge_\ conj_Ruge_Grefenstette conj_Ruge_1991 dep_Hindle_Ruge num_Hindle_1990 conj_al._methods conj_al._1992 nn_al._et amod_\_knowledge-poor num_\_1991 dep_Bindi_\ dep_Bindi_Mauldin dep_Bindi_1990 conj_and_Jacobs_Bindi conj_and_Jacobs_Calzolari amod_Jacobs_1990 conj_and_Jacobs_Rau amod_\_knowledge-rich rcmod_number_proposed conj_and_number_\ dep_number_Bindi dep_number_Calzolari dep_number_Rau dep_number_Jacobs prep_of_number_\ det_number_A
W97-0205	P90-1034	o	MI is defined in general as follows y -RRB- I ix y -RRB- = log2 P -LRB- x -RRB- P -LRB- y -RRB- We can use this definition to derive an estimate of the connectedness between words in terms of collocations -LRB- Smadja 1993 -RRB- but also in terms of phrases and grammatical relations -LRB- Hindle 1990 -RRB-	amod_Hindle_1990 amod_relations_grammatical conj_and_phrases_relations dep_terms_Hindle prep_of_terms_relations prep_of_terms_phrases pobj_in_terms advmod_in_also amod_Smadja_1993 appos_collocations_Smadja prep_of_terms_collocations det_connectedness_the prep_between_estimate_words prep_of_estimate_connectedness det_estimate_an dobj_derive_estimate aux_derive_to det_definition_this vmod_use_derive dobj_use_definition aux_use_can nsubj_use_We rcmod_P_use appos_P_y nn_P_P appos_P_x nn_P_log2 conj_but_=_in prep_in_=_terms dobj_=_P dep_y_in dep_y_= dobj_ix_y nsubj_ix_I dep_ix_y mark_follows_as parataxis_defined_ix advcl_defined_follows prep_in_defined_general auxpass_defined_is nsubjpass_defined_MI
W97-0803	P90-1034	o	This criticism leads us to automatic approaches for building thesauri from large corpora \ -LSB- Hirschman et al. 1975 Hindle 1990 Hatzivassiloglou and McKeown 1993 Pereira et al. 1993 Tokunaga et aL 1995 Ushioda 1996 \ -RSB-	num_\_1996 appos_Ushioda_\ appos_aL_1995 nn_aL_et nn_aL_Tokunaga num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Hatzivassiloglou_1993 conj_and_Hatzivassiloglou_McKeown dep_Hindle_Ushioda conj_Hindle_aL conj_Hindle_Pereira conj_Hindle_McKeown conj_Hindle_Hatzivassiloglou num_Hindle_1990 dep_Hirschman_Hindle appos_Hirschman_1975 dep_Hirschman_al. nn_Hirschman_et nn_\_corpora amod_\_large prep_from_thesauri_\ dobj_building_thesauri prepc_for_approaches_building amod_approaches_automatic dep_leads_Hirschman prep_to_leads_approaches dobj_leads_us nsubj_leads_criticism det_criticism_This
W98-0704	P90-1034	n	Our predicate-argument structure-based thesatmis is based on the method proposed by Hindie -LRB- Hindle 1990 -RRB- although Hindle did not apply it to information retrieval	nn_retrieval_information prep_to_apply_retrieval dobj_apply_it neg_apply_not aux_apply_did nsubj_apply_Hindle mark_apply_although dep_Hindle_1990 dep_Hindie_Hindle agent_proposed_Hindie vmod_method_proposed det_method_the advcl_based_apply prep_on_based_method auxpass_based_is nsubjpass_based_thesatmis amod_thesatmis_structure-based amod_thesatmis_predicate-argument poss_thesatmis_Our ccomp_``_based
A00-1039	P93-1022	o	This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis -LRB- Riloff and Jones 1999 Pereira et al. 1993 Dagan et al. 1993 Hirschman et al. 1975 -RRB- although in our case the contexts are limited to selected patterns relevant to the scenario	det_scenario_the prep_to_relevant_scenario amod_patterns_selected prep_to_limited_patterns auxpass_limited_are nsubjpass_limited_contexts det_contexts_the amod_case_relevant rcmod_case_limited poss_case_our pobj_in_case mark_in_although num_Hirschman_1975 nn_Hirschman_al. nn_Hirschman_et num_Dagan_1993 nn_Dagan_al. nn_Dagan_et num_Pereira_1993 nn_Pereira_al. nn_Pereira_et dep_Riloff_Hirschman conj_and_Riloff_Dagan conj_and_Riloff_Pereira conj_and_Riloff_1999 conj_and_Riloff_Jones appos_analysis_Dagan appos_analysis_Pereira appos_analysis_1999 appos_analysis_Jones appos_analysis_Riloff nn_analysis_co-occurrence amod_analysis_syntactic amod_classes_semantic prep_through_induce_analysis dobj_induce_classes aux_induce_to xcomp_aims_induce nsubj_aims_which rcmod_groups_aims amod_groups_other amod_groups_several prep_by_work_groups aux_work_to advcl_similar_in xcomp_similar_work cop_similar_is nsubj_similar_This ccomp_``_similar
C96-2205	P93-1022	o	I -RRB- agan eL al. proposed a similarity-based model in which each word is generalized not to its own specific class but to a set of words which are most similar to it -LRB- Dagan et al. 1993 -RRB-	amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et prep_to_similar_it advmod_similar_most cop_similar_are nsubj_similar_which rcmod_words_similar prep_of_set_words det_set_a pobj_to_set amod_class_specific amod_class_own poss_class_its conj_but_generalized_to prep_to_generalized_class neg_generalized_not auxpass_generalized_is nsubjpass_generalized_word prep_in_generalized_which det_word_each dep_model_Dagan rcmod_model_to rcmod_model_generalized amod_model_similarity-based det_model_a amod_model_proposed dep_al._model nn_al._eL amod_al._agan dep_al._I
E99-1028	P93-1022	o	We say that wv and nq are semantically related if w ~ i and nq are semantically related and -LRB- wp nq -RRB- and -LRB- w ~ i nq -RRB- are semantically similar -LRB- Dagan et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Dagan_al. dep_similar_Dagan advmod_similar_semantically cop_similar_are csubj_similar_~ csubj_similar_wp csubj_similar_related nn_nq_i dep_~_nq nn_~_w appos_wp_nq conj_and_related_~ conj_and_related_wp advmod_related_semantically cop_related_are nsubj_related_nq nsubj_related_i mark_related_if conj_and_i_nq nn_i_~ nn_i_w ccomp_related_similar advmod_related_semantically cop_related_are nsubj_related_nq nsubj_related_wv mark_related_that conj_and_wv_nq ccomp_say_related nsubj_say_We ccomp_``_say
J02-2001	P93-1022	o	There are many different similarity measures which variously use taxonomic lexical hierarchies or lexical-semantic networks large text corpora word definitions in machine-readable dictionaries or other semantic formalisms or a combination of these -LRB- Dagan Marcus and Markovitch 1993 Kozima and Furugori 1993 Pereira Tishby and Lee 1993 Church et al. 1994 Grefenstette 1994 Resnik 1995 McMahon and Smith 1996 Jiang and Conrath 1997 Sch utze 1998 Lin 1998 Resnik and Diab 2000 Budanitsky 1999 Budanitsky and Hirst 2001 2002 -RRB-	num_Hirst_2001 dep_Budanitsky_2002 conj_and_Budanitsky_Hirst num_Budanitsky_1999 num_Diab_2000 conj_and_Resnik_Diab num_Lin_1998 amod_1998_utze num_Sch_1998 num_Jiang_1997 conj_and_Jiang_Conrath num_Smith_1996 conj_and_McMahon_Smith num_Resnik_1995 num_Grefenstette_1994 dep_al._1994 nn_al._et nn_al._Church num_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby num_Furugori_1993 conj_and_Kozima_Furugori num_Markovitch_1993 dep_Dagan_Hirst dep_Dagan_Budanitsky dep_Dagan_Budanitsky dep_Dagan_Diab dep_Dagan_Resnik dep_Dagan_Lin dep_Dagan_Sch dep_Dagan_Conrath dep_Dagan_Jiang dep_Dagan_Smith dep_Dagan_McMahon dep_Dagan_Resnik dep_Dagan_Grefenstette dep_Dagan_al. dep_Dagan_Lee dep_Dagan_Tishby dep_Dagan_Pereira dep_Dagan_Furugori dep_Dagan_Kozima conj_and_Dagan_Markovitch appos_Dagan_Marcus appos_these_Markovitch appos_these_Dagan prep_of_combination_these det_combination_a amod_formalisms_semantic amod_formalisms_other conj_or_dictionaries_formalisms amod_dictionaries_machine-readable prep_in_definitions_formalisms prep_in_definitions_dictionaries nn_definitions_word conj_or_corpora_combination conj_or_corpora_definitions nn_corpora_text amod_corpora_large amod_networks_lexical-semantic conj_or_hierarchies_networks amod_hierarchies_lexical amod_hierarchies_taxonomic dobj_use_networks dobj_use_hierarchies advmod_use_variously nsubj_use_which appos_measures_combination appos_measures_definitions appos_measures_corpora rcmod_measures_use nn_measures_similarity amod_measures_different amod_measures_many nsubj_are_measures expl_are_There ccomp_``_are
J05-4002	P93-1022	p	Similarity-based smoothing -LRB- Hindle 1990 Brown et al. 1992 Dagan Marcus and Markovitch 1993 Pereira Tishby and Lee 1993 Dagan Lee and Pereira 1999 -RRB- provides an intuitively appealing approach to language modeling	nn_modeling_language prep_to_approach_modeling amod_approach_appealing det_approach_an advmod_appealing_intuitively dobj_provides_approach nsubj_provides_smoothing num_Pereira_1999 conj_and_Dagan_Pereira conj_and_Dagan_Lee num_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby num_Markovitch_1993 conj_and_Dagan_Markovitch conj_and_Dagan_Marcus nn_1992_al. num_Brown_1992 nn_Brown_et dep_1990_Pereira dep_1990_Lee dep_1990_Dagan conj_1990_Lee conj_1990_Tishby conj_1990_Pereira conj_1990_Markovitch conj_1990_Marcus conj_1990_Dagan conj_1990_Brown amod_1990_Hindle dep_smoothing_1990 amod_smoothing_Similarity-based
J05-4002	P93-1022	o	5.2 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique -LRB- Gale Church and Yarowsky 1992 Sch utze 1992 Pereira Tishby and Lee 1993 Sch utze 1998 Lee 1999 Dagan Lee and Pereira 1999 Golding and Roth 1999 Rooth et al. 1999 EvenZohar and Roth 2000 Lee 2001 Clark and Weir 2002 -RRB- and in the current setting we may use a nouns neighbors to decide which of two co-occurrences is the most likely	advmod_likely_most det_likely_the cop_likely_is nsubj_likely_which num_co-occurrences_two prep_of_which_co-occurrences ccomp_decide_likely aux_decide_to nn_neighbors_nouns det_neighbors_a xcomp_use_decide dobj_use_neighbors aux_use_may nsubj_use_we ccomp_use_in ccomp_use_technique amod_setting_current det_setting_the pobj_in_setting num_Clark_2002 conj_and_Clark_Weir num_Lee_2001 num_EvenZohar_2000 conj_and_EvenZohar_Roth dep_al._1999 nn_al._et nn_al._Rooth dep_Golding_1999 conj_and_Golding_Roth num_Pereira_1999 conj_and_Dagan_Pereira conj_and_Dagan_Lee num_Lee_1999 amod_1998_utze num_Sch_1998 num_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby amod_1992_utze num_Sch_1992 num_Yarowsky_1992 conj_and_Gale_Weir conj_and_Gale_Clark conj_and_Gale_Lee conj_and_Gale_Roth conj_and_Gale_EvenZohar conj_and_Gale_al. conj_and_Gale_Roth conj_and_Gale_Golding conj_and_Gale_Pereira conj_and_Gale_Lee conj_and_Gale_Dagan conj_and_Gale_Lee conj_and_Gale_Sch conj_and_Gale_Lee conj_and_Gale_Tishby conj_and_Gale_Pereira conj_and_Gale_Sch conj_and_Gale_Yarowsky conj_and_Gale_Church conj_and_technique_in appos_technique_Clark appos_technique_Lee appos_technique_EvenZohar appos_technique_al. appos_technique_Golding appos_technique_Dagan appos_technique_Lee appos_technique_Sch appos_technique_Pereira appos_technique_Sch appos_technique_Yarowsky appos_technique_Church appos_technique_Gale nn_technique_evaluation amod_technique_standard det_technique_a xcomp_become_use aux_become_have nsubj_become_tasks nn_tasks_Pseudo-disambiguation nn_tasks_Task nn_tasks_Pseudo-Disambiguation num_tasks_5.2
J94-4003	P93-1022	p	A promising approach may be to use aligned bilingual corpora especially for augmenting existing lexicons with domain-specific terminology -LRB- Brown et al. 1993 Dagan Church and Gale 1993 -RRB-	num_Gale_1993 conj_and_Dagan_Gale conj_and_Dagan_Church dep_al._Gale dep_al._Church dep_al._Dagan num_al._1993 nn_al._et amod_al._Brown dep_terminology_al. amod_terminology_domain-specific prep_with_lexicons_terminology amod_lexicons_existing dobj_augmenting_lexicons prepc_for_corpora_augmenting advmod_corpora_especially amod_corpora_bilingual amod_corpora_aligned dobj_use_corpora aux_use_to xcomp_be_use aux_be_may nsubj_be_approach amod_approach_promising det_approach_A ccomp_``_be
J96-1001	P93-1022	o	Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas including sentence alignment -LRB- Gale and Church 1991b Brown Lai and Mercer 1991 Simard Foster and Isabelle 1992 Gale and Church 1993 Chen 1993 -RRB- word alignment -LRB- Gale and Church 1991a Brown et al. 1993 Dagan Church and Gale 1993 Fung and McKeown 1994 Fung 1995b -RRB- alignment of groups of words -LRB- Smadja 1992 Kupiec 1993 van der Eijk 1993 -RRB- and statistical translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_translation_al. amod_translation_statistical dep_Eijk_1993 nn_Eijk_der nn_Eijk_van num_Kupiec_1993 dep_Smadja_Eijk dep_Smadja_Kupiec num_Smadja_1992 appos_words_Smadja prep_of_groups_words prep_of_alignment_groups nn_1995b_Fung num_Fung_1994 conj_and_Fung_McKeown num_Gale_1993 conj_and_Dagan_Gale conj_and_Dagan_Church dep_al._1993 nn_al._et nn_al._Brown nn_1991a_Church dep_Gale_1995b conj_and_Gale_McKeown conj_and_Gale_Fung conj_and_Gale_Gale conj_and_Gale_Church conj_and_Gale_Dagan conj_and_Gale_al. conj_and_Gale_1991a appos_alignment_Fung appos_alignment_Dagan appos_alignment_al. appos_alignment_1991a appos_alignment_Gale nn_alignment_word num_Chen_1993 num_Church_1993 conj_and_Gale_Church num_Isabelle_1992 conj_and_Simard_Isabelle conj_and_Simard_Foster num_Mercer_1991 conj_and_Brown_Mercer conj_and_Brown_Lai nn_1991b_Church dep_Gale_Chen dep_Gale_Church dep_Gale_Gale conj_and_Gale_Isabelle conj_and_Gale_Foster conj_and_Gale_Simard conj_and_Gale_Mercer conj_and_Gale_Lai conj_and_Gale_Brown conj_and_Gale_1991b conj_and_alignment_translation appos_alignment_alignment appos_alignment_alignment appos_alignment_Simard appos_alignment_Brown appos_alignment_1991b appos_alignment_Gale nn_alignment_sentence prep_including_areas_translation prep_including_areas_alignment amod_areas_several prep_in_attracted_areas dobj_attracted_interest aux_attracted_has nsubj_attracted_availability amod_data_bilingual prep_of_amounts_data amod_amounts_large prep_of_availability_amounts amod_availability_recent det_availability_The rcmod_Work_attracted amod_Work_Related
J96-2003	P93-1022	p	Successful approaches aimed at trying to overcome the sparse data limitation include backoff -LRB- Katz 1987 -RRB- Turing-Good variants -LRB- Good 1953 Church and Gale 1991 -RRB- interpolation -LRB- Jelinek 1985 -RRB- deleted estimation -LRB- Jelinek 1985 Church and Gale 1991 -RRB- similarity-based models -LRB- Dagan Pereira and Lee 1994 Essen and Steinbiss 1992 -RRB- Pos-language models -LRB- Derouault and Merialdo 1986 -RRB- and decision tree models -LRB- Bahl et al. 1989 Black Garside and Leech 1993 Magerman 1994 -RRB-	num_Magerman_1994 num_Leech_1993 conj_and_Black_Leech conj_and_Black_Garside dep_Bahl_Magerman dep_Bahl_Leech dep_Bahl_Garside dep_Bahl_Black dep_Bahl_1989 dep_Bahl_al. nn_Bahl_et nn_models_tree nn_models_decision num_Merialdo_1986 conj_and_Derouault_Merialdo dep_models_Merialdo dep_models_Derouault amod_models_Pos-language num_Steinbiss_1992 conj_and_Essen_Steinbiss num_Lee_1994 dep_Dagan_Steinbiss dep_Dagan_Essen conj_and_Dagan_Lee conj_and_Dagan_Pereira appos_models_Lee appos_models_Pereira appos_models_Dagan amod_models_similarity-based num_Church_1991 conj_and_Church_Gale dep_Jelinek_Gale dep_Jelinek_Church num_Jelinek_1985 appos_estimation_Jelinek amod_estimation_deleted num_Jelinek_1985 appos_interpolation_Jelinek dep_Church_1991 conj_and_Church_Gale dep_1953_Gale dep_1953_Church amod_1953_Good dep_variants_1953 amod_variants_Turing-Good num_Katz_1987 appos_backoff_Bahl conj_and_backoff_models conj_and_backoff_models conj_and_backoff_models conj_and_backoff_estimation conj_and_backoff_interpolation conj_and_backoff_variants appos_backoff_Katz dobj_include_models dobj_include_models dobj_include_models dobj_include_estimation dobj_include_interpolation dobj_include_variants dobj_include_backoff nsubj_include_approaches nn_limitation_data amod_limitation_sparse det_limitation_the dobj_overcome_limitation aux_overcome_to xcomp_trying_overcome prepc_at_aimed_trying vmod_approaches_aimed amod_approaches_Successful
J98-1002	P93-1022	o	This can be done by smoothing the observed frequencies 7 -LRB- Church and Mercer 1993 -RRB- or by class-based methods -LRB- Brown et al. 1991 Pereira and Tishby 1992 Pereira Tishby and Lee 1993 Hirschman 1986 Resnik 1992 Brill et al. 1990 Dagan Marcus and Markovitch 1993 -RRB-	num_Markovitch_1993 conj_and_Dagan_Markovitch conj_and_Dagan_Marcus dep_al._1990 nn_al._et nn_al._Brill num_Resnik_1992 num_Hirschman_1986 num_Lee_1993 conj_Pereira_Tishby num_Tishby_1992 conj_and_Pereira_Tishby dep_Brown_Markovitch dep_Brown_Marcus dep_Brown_Dagan conj_and_Brown_al. conj_and_Brown_Resnik conj_and_Brown_Hirschman conj_and_Brown_Lee dep_Brown_Pereira dep_Brown_Tishby dep_Brown_Pereira dep_Brown_1991 dep_Brown_al. nn_Brown_et amod_methods_class-based num_Mercer_1993 conj_and_Church_Mercer appos_frequencies_Mercer appos_frequencies_Church num_frequencies_7 amod_frequencies_observed det_frequencies_the conj_or_smoothing_methods dobj_smoothing_frequencies dep_done_al. dep_done_Resnik dep_done_Hirschman dep_done_Lee dep_done_Brown agent_done_methods agent_done_smoothing auxpass_done_be aux_done_can nsubjpass_done_This ccomp_``_done
J98-1004	P93-1022	n	Regardless of whether it takes the form of dictionaries -LRB- Lesk 1986 Guthrie et al. 1991 Dagan Itai and Schwall 1991 Karov and Edelman 1996 -RRB- thesauri -LRB- Yarowsky 1992 Walker and Amsler 1986 -RRB- bilingual corpora -LRB- Brown et al. 1991 Church and Gale 1991 -RRB- or hand-labeled training sets -LRB- Hearst 1991 Leacock Towell and Voorhees 1993 Niwa and Nitta 1994 Bruce and Wiebe 1994 -RRB- providing information for sense definitions can be a considerable burden	amod_burden_considerable det_burden_a cop_burden_be aux_burden_can nsubj_burden_sets nsubj_burden_Brown nn_definitions_sense prep_for_providing_definitions dobj_providing_information num_Wiebe_1994 conj_and_Bruce_Wiebe num_Nitta_1994 conj_and_Niwa_Nitta num_Voorhees_1993 conj_and_Leacock_Voorhees conj_and_Leacock_Towell dep_Hearst_Wiebe dep_Hearst_Bruce dep_Hearst_Nitta dep_Hearst_Niwa dep_Hearst_Voorhees dep_Hearst_Towell dep_Hearst_Leacock num_Hearst_1991 appos_sets_Hearst nn_sets_training amod_sets_hand-labeled num_Church_1991 conj_and_Church_Gale num_al._1991 nn_al._et vmod_Brown_providing conj_or_Brown_sets dep_Brown_Gale dep_Brown_Church tmod_Brown_al. dep_corpora_burden amod_corpora_bilingual num_Amsler_1986 conj_and_Walker_Amsler dep_Yarowsky_Amsler dep_Yarowsky_Walker num_Yarowsky_1992 appos_thesauri_Yarowsky dep_Karov_1996 conj_and_Karov_Edelman num_Schwall_1991 conj_and_Dagan_Schwall conj_and_Dagan_Itai dep_al._1991 nn_al._et nn_al._Guthrie dep_Lesk_Edelman dep_Lesk_Karov dep_Lesk_Schwall dep_Lesk_Itai dep_Lesk_Dagan dep_Lesk_al. num_Lesk_1986 appos_dictionaries_corpora appos_dictionaries_thesauri appos_dictionaries_Lesk prep_of_form_dictionaries det_form_the dobj_takes_form nsubj_takes_it mark_takes_whether prepc_of_Regardless_takes ccomp_``_Regardless
P95-1025	P93-1022	o	In the similaritybased approaches -LRB- Dagan et al. 1993 & 1994 Grishman et al. 1993 -RRB- rather than a class each word is modelled by its own set of similar words derived from statistical data collected from corpora	prep_from_collected_corpora vmod_data_collected amod_data_statistical prep_from_derived_data vmod_words_derived amod_words_similar prep_of_set_words amod_set_own poss_set_its agent_modelled_set auxpass_modelled_is nsubjpass_modelled_word advmod_modelled_rather dep_modelled_Dagan prep_in_modelled_approaches det_word_each det_class_a pobj_rather_class mwe_rather_than num_Grishman_1993 nn_Grishman_al. nn_Grishman_et conj_and_1993_1994 nn_al._et dep_Dagan_Grishman appos_Dagan_1994 appos_Dagan_1993 advmod_Dagan_al. amod_approaches_similaritybased det_approaches_the ccomp_``_modelled
P98-2127	P93-1022	o	In -LRB- Dagan et al. 1993 -RRB- and -LRB- Pereira et al. 993 -RRB- clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time	det_time_a num_corpus_one nn_corpus_input det_corpus_the prep_at_removed_time prep_from_removed_corpus auxpass_removed_are nsubjpass_removed_that rcmod_items_removed nn_items_data dobj_recover_items aux_recover_to xcomp_able_recover cop_able_are nsubj_able_they advmod_able_well advmod_well_how agent_evaluated_able auxpass_evaluated_are nsubjpass_evaluated_Pereira nsubjpass_evaluated_Dagan mark_evaluated_In amod_words_similar prep_of_clusters_words dep_Pereira_993 dep_Pereira_al. nn_Pereira_et nn_al._et appos_Dagan_clusters conj_and_Dagan_Pereira amod_Dagan_1993 dep_Dagan_al. advcl_``_evaluated
W96-0104	P93-1022	o	This can be done by smoothing the observed frequencies -LRB- Church and Mercer 1993 -RRB- or by class-based methods -LRB- Brown et al. 1991 Pereira and Tishby 1992 Pereira et ah 1993 Hirschman 1986 Resnik 1992 Brill et ah 1990 Dagan et al. 1993 -RRB-	nn_al._et nn_al._Dagan nn_ah_et nn_ah_Brill num_Resnik_1992 num_Hirschman_1986 nn_ah_et nn_ah_Pereira conj_and_Pereira_1992 conj_and_Pereira_Tishby amod_Brown_1993 dep_Brown_al. amod_Brown_1990 dep_Brown_ah dep_Brown_Resnik dep_Brown_Hirschman amod_Brown_1993 dep_Brown_ah dep_Brown_1992 dep_Brown_Tishby dep_Brown_Pereira amod_Brown_1991 dep_Brown_al. nn_Brown_et amod_methods_class-based dep_Church_1993 conj_and_Church_Mercer appos_frequencies_Mercer appos_frequencies_Church amod_frequencies_observed det_frequencies_the conj_or_smoothing_methods dobj_smoothing_frequencies dep_done_Brown agent_done_methods agent_done_smoothing auxpass_done_be aux_done_can nsubjpass_done_This
W97-0311	P93-1022	o	Several authors have used mutual information and similar statistics as an objective function for word clustering -LRB- Dagan et al. 1993 Brown et al. 1992 Pereira et al. 1993 Wang et al. 1996 -RRB- for automatic determination of phonemic baseforms -LRB- Lucassen & Mercer 1984 -RRB- and for language modeling for speech recognition -LRB- Ries ct al. 1996 -RRB-	dep_ct_1996 dep_ct_al. nn_ct_Ries nn_recognition_speech dep_modeling_ct prep_for_modeling_recognition nn_modeling_language pobj_for_modeling dep_Lucassen_1984 conj_and_Lucassen_Mercer appos_baseforms_Mercer appos_baseforms_Lucassen amod_baseforms_phonemic prep_of_determination_baseforms amod_determination_automatic num_Wang_1996 nn_Wang_al. nn_Wang_et num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Dagan_Wang dep_Dagan_Pereira dep_Dagan_Brown dep_Dagan_1993 dep_Dagan_al. nn_Dagan_et nn_clustering_word prep_for_function_clustering amod_function_objective det_function_an amod_statistics_similar conj_and_information_statistics amod_information_mutual conj_and_used_for prep_for_used_determination dep_used_Dagan prep_as_used_function dobj_used_statistics dobj_used_information aux_used_have nsubj_used_authors amod_authors_Several ccomp_``_for ccomp_``_used
A00-3006	P95-1026	o	-LRB- 1991 -RRB- Yarowsky -LRB- 1995 -RRB- -RRB-	dep_Yarowsky_1995 dep_1991_Yarowsky dep_''_1991
A97-2010	P95-1026	o	A Broad-Coverage Word Sense Tagger Dekang Lin Department of Computer Science University of Manitoba Winnipeg Manitoba Canada R3T 2N2 lindek@cs.umanitoba.ca Previous corpus-based Word Sense Disambiguation -LRB- WSD -RRB- algorithms -LRB- Hearst 1991 Bruce and Wiebe 1994 Leacock et al. 1996 Ng and Lee 1996 Yarowsky 1992 Yarowsky 1995 -RRB- determine the meanings of polysemous words by exploiting their local contexts	amod_contexts_local poss_contexts_their dobj_exploiting_contexts amod_words_polysemous prep_of_meanings_words det_meanings_the prepc_by_determine_exploiting dobj_determine_meanings nsubj_determine_Yarowsky nsubj_determine_1996 nsubj_determine_Lee nsubj_determine_Ng amod_Yarowsky_1995 appos_Yarowsky_1992 dep_Ng_Yarowsky conj_and_Ng_Yarowsky conj_and_Ng_1996 conj_and_Ng_Lee num_Leacock_1996 nn_Leacock_al. nn_Leacock_et num_Bruce_1994 conj_and_Bruce_Wiebe parataxis_Hearst_determine dep_Hearst_Leacock dep_Hearst_Wiebe dep_Hearst_Bruce amod_Hearst_1991 dep_algorithms_Hearst dep_Disambiguation_algorithms appos_Disambiguation_WSD nn_Disambiguation_Sense nn_Disambiguation_Word amod_Disambiguation_corpus-based amod_Disambiguation_Previous nn_Disambiguation_lindek@cs.umanitoba.ca nn_Disambiguation_2N2 nn_Disambiguation_R3T nn_Disambiguation_Canada conj_Winnipeg_Disambiguation conj_Winnipeg_Manitoba prep_of_University_Manitoba nn_University_Science nn_University_Computer dep_Department_Winnipeg prep_of_Department_University nn_Department_Lin nn_Department_Dekang nn_Department_Tagger nn_Department_Sense nn_Department_Word nn_Department_Broad-Coverage det_Department_A
C00-1023	P95-1026	o	Statistical techniques both supervised learning from tagged corpora -LRB- Yarowsky 1992 -RRB- -LRB- Ng and Lee 1.996 -RRB- and unsupervised learning -LRB- Yarowsky 1995 -RRB- -LRB- Resnik 1997 -RRB- have been investigated	auxpass_investigated_been aux_investigated_have nsubjpass_investigated_supervised dep_investigated_techniques dep_Resnik_1997 dep_Yarowsky_1995 appos_learning_Yarowsky amod_learning_unsupervised dep_Ng_1.996 conj_and_Ng_Lee amod_Yarowsky_1992 dep_corpora_Yarowsky amod_corpora_tagged conj_and_learning_learning appos_learning_Lee appos_learning_Ng prep_from_learning_corpora dep_supervised_Resnik dobj_supervised_learning dobj_supervised_learning dep_supervised_both amod_techniques_Statistical
C00-1023	P95-1026	o	The model can be seen as a bootstrapping learning process tbr disambiguation where the information gained from one part -LRB- selectional preference -RRB- is used to improve tile other -LRB- disambiguation -RRB- and vice versa reminiscent of the work by Riloff and Jones -LRB- 1.999 -RRB- and Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 appos_Jones_1.999 conj_and_Riloff_Yarowsky conj_and_Riloff_Jones prep_by_work_Yarowsky prep_by_work_Jones prep_by_work_Riloff det_work_the prep_of_reminiscent_work npadvmod_versa_vice conj_and_other_reminiscent conj_and_other_versa appos_other_disambiguation amod_tile_reminiscent amod_tile_versa amod_tile_other dobj_improve_tile aux_improve_to xcomp_used_improve auxpass_used_is amod_preference_selectional appos_part_preference num_part_one prep_from_gained_part nsubj_gained_information advmod_gained_where det_information_the rcmod_disambiguation_gained nn_disambiguation_tbr nn_disambiguation_process nn_disambiguation_learning amod_disambiguation_bootstrapping det_disambiguation_a ccomp_seen_used prep_as_seen_disambiguation auxpass_seen_be aux_seen_can nsubjpass_seen_model det_model_The
C00-2094	P95-1026	o	However the best performing statistical approaches to lexical ambiguity resolution l lmmselves rely on complex infornmtion sources such as lemmas inflected forms parts of speech and arbitrary word classes If \ -RSB- local and distant collocations trigram sequences a.nd predicate m ` gument association -LRB- Yarowsky -LRB- 1995 -RRB- p. 190 -RRB- or large context-windows up to 1000 neighboring words -LRB- Sch/itze 1992 -RRB-	amod_Sch/itze_1992 appos_words_Sch/itze amod_words_neighboring num_words_1000 pobj_to_words pcomp_up_to prep_context-windows_up amod_context-windows_large num_p._190 appos_Yarowsky_p. appos_Yarowsky_1995 nn_association_gument nn_association_m nn_association_predicate amod_association_a.nd nn_sequences_trigram dep_local_collocations conj_and_local_distant npadvmod_local_\ mark_local_If nn_classes_word amod_classes_arbitrary prep_of_parts_speech conj_or_forms_context-windows dep_forms_Yarowsky conj_and_forms_association conj_and_forms_sequences advcl_forms_distant advcl_forms_local conj_and_forms_classes conj_and_forms_parts amod_forms_inflected nn_forms_lemmas prep_such_as_sources_context-windows prep_such_as_sources_association prep_such_as_sources_sequences prep_such_as_sources_classes prep_such_as_sources_parts prep_such_as_sources_forms nn_sources_infornmtion amod_sources_complex prep_on_rely_sources nsubj_rely_lmmselves nn_l_resolution nn_l_ambiguity amod_l_lexical dep_approaches_rely prep_to_approaches_l amod_approaches_statistical amod_approaches_performing amod_approaches_best det_approaches_the ccomp_,_approaches dep_``_However
C00-2094	P95-1026	o	The SENSEVAL ' ~ tan -LRB- lard is clearly beaten by the earlier results of Yarowsky -LRB- 1995 -RRB- -LRB- 96.5 % precision -RRB- and Schiitze -LRB- 1992 -RRB- -LRB- 92 % precision -RRB-	amod_precision_% nn_precision_Schiitze number_%_92 appos_Schiitze_1992 amod_precision_% number_%_96.5 conj_and_Yarowsky_precision appos_Yarowsky_precision appos_Yarowsky_1995 prep_of_results_precision prep_of_results_Yarowsky amod_results_earlier det_results_the agent_beaten_results advmod_beaten_clearly auxpass_beaten_is nsubjpass_beaten_lard rcmod_tan_beaten nn_tan_~ poss_tan_SENSEVAL det_SENSEVAL_The
C02-1058	P95-1026	o	A variety of unsupervised WSD methods which use a machinereadable dictionary or thesaurus in addition to a corpus have also been proposed -LRB- Yarowsky 1992 Yarowsky 1995 Karov and Edelman 1998 -RRB-	dep_Karov_1998 conj_and_Karov_Edelman num_Yarowsky_1995 dep_Yarowsky_Edelman dep_Yarowsky_Karov dep_Yarowsky_Yarowsky num_Yarowsky_1992 dep_proposed_Yarowsky auxpass_proposed_been advmod_proposed_also aux_proposed_have nsubjpass_proposed_variety det_corpus_a conj_or_dictionary_thesaurus amod_dictionary_machinereadable det_dictionary_a prep_in_addition_to_use_corpus dobj_use_thesaurus dobj_use_dictionary nsubj_use_which rcmod_methods_use nn_methods_WSD amod_methods_unsupervised prep_of_variety_methods det_variety_A ccomp_``_proposed
C02-1088	P95-1026	o	David Yarowsky -LRB- 1995 -RRB- showed it was accurate in the word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word det_disambiguation_the prep_in_accurate_disambiguation cop_accurate_was nsubj_accurate_it ccomp_showed_accurate nsubj_showed_Yarowsky appos_Yarowsky_1995 nn_Yarowsky_David
C02-1097	P95-1026	o	Distance from a target word is used for this purpose and it is calculated by the assumption that the target words in the context window have the same sense -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_sense_Yarowsky amod_sense_same det_sense_the dobj_have_sense nsubj_have_words mark_have_that nn_window_context det_window_the prep_in_words_window nn_words_target det_words_the ccomp_assumption_have det_assumption_the agent_calculated_assumption auxpass_calculated_is nsubjpass_calculated_it det_purpose_this conj_and_used_calculated prep_for_used_purpose auxpass_used_is nsubjpass_used_Distance nn_word_target det_word_a prep_from_Distance_word
C02-1127	P95-1026	p	Recent work emphasizes corpus-based unsupervised approach -LRB- Dagon and Itai 1994 Yarowsky 1992 Yarowsky 1995 -RRB- that avoids the need for costly truthed training data	nn_data_training amod_data_truthed amod_data_costly prep_for_need_data det_need_the dobj_avoids_need nsubj_avoids_that dep_Yarowsky_1995 appos_Yarowsky_1992 dep_Dagon_avoids dep_Dagon_Yarowsky conj_and_Dagon_Yarowsky conj_and_Dagon_1994 conj_and_Dagon_Itai dep_approach_Yarowsky dep_approach_1994 dep_approach_Itai dep_approach_Dagon amod_approach_unsupervised amod_approach_corpus-based dobj_emphasizes_approach nsubj_emphasizes_work amod_work_Recent ccomp_``_emphasizes
C04-1071	P95-1026	o	Many techniques which have been studied for the purpose of machine translation such as word sense disambiguation -LRB- Dagan and Itai 1994 Yarowsky 1995 -RRB- anaphora resolution -LRB- Mitamura et al. 2002 -RRB- and automatic pattern extraction from corpora -LRB- Watanabe et al. 2003 -RRB- can accelerate the further enhancement of sentiment analysis or other NLP tasks	nn_tasks_NLP amod_tasks_other conj_or_analysis_tasks nn_analysis_sentiment prep_of_enhancement_tasks prep_of_enhancement_analysis amod_enhancement_further det_enhancement_the dobj_accelerate_enhancement aux_accelerate_can nsubj_accelerate_techniques amod_Watanabe_2003 dep_Watanabe_al. nn_Watanabe_et prep_from_extraction_corpora nn_extraction_pattern amod_extraction_automatic amod_Mitamura_2002 dep_Mitamura_al. nn_Mitamura_et dep_resolution_Mitamura nn_resolution_anaphora dep_Yarowsky_1995 dep_Dagan_Yarowsky conj_and_Dagan_1994 conj_and_Dagan_Itai conj_and_disambiguation_extraction conj_and_disambiguation_resolution dep_disambiguation_1994 dep_disambiguation_Itai dep_disambiguation_Dagan nn_disambiguation_sense nn_disambiguation_word nn_translation_machine prep_such_as_purpose_extraction prep_such_as_purpose_resolution prep_such_as_purpose_disambiguation prep_of_purpose_translation det_purpose_the prep_for_studied_purpose auxpass_studied_been aux_studied_have nsubjpass_studied_which dep_techniques_Watanabe rcmod_techniques_studied amod_techniques_Many ccomp_``_accelerate
C04-1133	P95-1026	o	However following the work of Yarowsky -LRB- 1992 -RRB- Yarowsky -LRB- 1995 -RRB- many supervised WSD systems use minimal information about syntactic structures for the most part restricting the notion of context to topical and local features	amod_features_local amod_features_topical conj_and_topical_local prep_of_notion_context det_notion_the prep_to_restricting_features dobj_restricting_notion amod_part_most det_part_the amod_structures_syntactic amod_information_minimal xcomp_use_restricting prep_for_use_part prep_about_use_structures dobj_use_information nsubj_use_systems nn_systems_WSD amod_systems_supervised amod_systems_many appos_Yarowsky_1995 rcmod_Yarowsky_use appos_Yarowsky_Yarowsky appos_Yarowsky_1992 prep_of_work_Yarowsky det_work_the pobj_following_work ccomp_,_following advcl_``_However
C08-1135	P95-1026	o	Yarowsky -LRB- 1995 -RRB- describes a ` semi-unsupervised ' approach to the problem of sense disambiguation of words also using a set of initial seeds in this case a few high quality sense annotations	nn_annotations_sense nn_annotations_quality amod_annotations_high amod_annotations_few det_annotations_a det_case_this amod_seeds_initial prep_of_set_seeds det_set_a dobj_using_annotations prep_in_using_case dobj_using_set advmod_using_also prep_of_disambiguation_words nn_disambiguation_sense prep_of_problem_disambiguation det_problem_the prep_to_approach_problem amod_approach_semi-unsupervised det_approach_a vmod_describes_using dobj_describes_approach nsubj_describes_Yarowsky appos_Yarowsky_1995
C96-2157	P95-1026	o	An alternative method we considered was to estimate certain conditional probabilities similarly to the formula used in -LRB- Yarowsky 1995 -RRB- SW -LRB- t -RRB- log P -LRB- p C A/t -RRB- f -LRB- t A -RRB- f -LRB- A -RRB- = ~ log -LRB- 2 -RRB- P -LRB- p C R/t -RRB- f -LRB- t R -RRB- f -LRB- l ~ -RRB- Here f -LRB- A -RRB- is -LRB- an estimate of -RRB- the probability that any given candidate phrase will be accepted by the spotter and f -LRB- R -RRB- is the probability that this phrase is rejected i.e. f -LRB- R -RRB- = l-f -LRB- A -RRB-	appos_l-f_A dep_=_l-f amod_f_= appos_f_R dep_rejected_f dep_rejected_i.e. auxpass_rejected_is nsubjpass_rejected_phrase mark_rejected_that det_phrase_this ccomp_probability_rejected det_probability_the cop_probability_is nsubj_probability_R nsubj_probability_probability nn_R_f det_spotter_the agent_accepted_spotter auxpass_accepted_be aux_accepted_will nsubjpass_accepted_phrase mark_accepted_that nn_phrase_candidate amod_phrase_given det_phrase_any conj_and_probability_R ccomp_probability_accepted det_probability_the rcmod_estimate_probability prep_estimate_of det_estimate_an cop_estimate_is nsubj_estimate_t appos_f_A advmod_f_Here nn_~_l dep_t_R nn_R/t_C nn_R/t_p dep_P_~ dep_P_f appos_P_t dep_P_f appos_P_R/t num_P_2 dep_log_P nn_log_~ dep_=_f dep_=_log nn_A_f nn_A_A amod_t_= dep_t_A nn_t_f nn_t_P nn_A/t_C nn_A/t_p appos_P_A/t nn_P_log nn_P_SW appos_SW_t amod_Yarowsky_1995 dep_in_Yarowsky prep_used_in vmod_formula_used det_formula_the amod_probabilities_conditional amod_probabilities_certain ccomp_estimate_estimate prep_to_estimate_formula advmod_estimate_similarly dobj_estimate_probabilities aux_estimate_to xcomp_was_estimate nsubj_was_method nsubj_considered_we rcmod_method_considered nn_method_alternative det_method_An
C96-2157	P95-1026	o	In additioil -LRB- Yarowsky 1995 -RRB- -LRB- Gale Church & Yarowsky 1992 -RRB- point ou that there is a st rent tenden -LRB- y for words 1 ;O occur in -LRB- -RCB- Ile sense within any given dis ourse -LRB- one sense pe r dis ourse -RRB-	nn_dis_r dep_pe_ourse appos_pe_dis nn_pe_sense num_pe_one dep_ourse_pe amod_dis_given det_dis_any dep_sense_ourse prep_within_sense_dis nn_sense_Ile prep_in_occur_sense num_;O_1 nn_;O_words dep_y_occur prep_for_y_;O dep_tenden_y nn_tenden_rent appos_st_tenden det_st_a nsubj_is_st expl_is_there mark_is_that nn_ou_point dep_ou_Yarowsky dep_Yarowsky_1992 dep_Gale_is conj_and_Gale_ou conj_and_Gale_Church appos_Yarowsky_ou appos_Yarowsky_Church appos_Yarowsky_Gale amod_Yarowsky_1995 dep_``_Yarowsky prep_in_``_additioil
D07-1070	P95-1026	o	In his analysis of Yarowsky -LRB- 1995 -RRB- Abney -LRB- 2004 -RRB- formulates several variants of bootstrapping	prep_of_variants_bootstrapping amod_variants_several dobj_formulates_variants nsubj_formulates_Abney appos_Abney_2004 rcmod_Yarowsky_formulates appos_Yarowsky_1995 prep_of_analysis_Yarowsky poss_analysis_his pobj_In_analysis dep_``_In
D07-1070	P95-1026	p	Although we see statistically significant improvements -LRB- at the .05 level on a paired permutation test -RRB- the quality of the parsers is still quite poor in contrast to other applications of bootstrapping which rival supervised methods -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_methods_Yarowsky amod_methods_supervised amod_methods_rival det_methods_which dep_bootstrapping_methods prep_of_applications_bootstrapping amod_applications_other prep_to_contrast_applications prep_in_poor_contrast advmod_poor_quite advmod_poor_still cop_poor_is nsubj_poor_quality advcl_poor_see det_parsers_the prep_of_quality_parsers det_quality_the nn_test_permutation amod_test_paired det_test_a prep_on_level_test num_level_.05 det_level_the prep_at_improvements_level amod_improvements_significant advmod_improvements_statistically dobj_see_improvements nsubj_see_we mark_see_Although
D07-1070	P95-1026	o	Our observation is that this situation is ideal for so-called bootstrapping co-training or minimally supervised learning methods -LRB- Yarowsky 1995 Blum and Mitchell 1998 Yarowsky and Wicentowski 2000 -RRB-	appos_Yarowsky_2000 conj_and_Yarowsky_Wicentowski conj_and_Blum_Wicentowski conj_and_Blum_Yarowsky conj_and_Blum_1998 conj_and_Blum_Mitchell dep_Yarowsky_Yarowsky dep_Yarowsky_1998 dep_Yarowsky_Mitchell dep_Yarowsky_Blum appos_Yarowsky_1995 dep_methods_Yarowsky nn_methods_learning amod_methods_supervised advmod_supervised_minimally conj_or_bootstrapping_methods conj_or_bootstrapping_co-training amod_bootstrapping_so-called prep_for_ideal_methods prep_for_ideal_co-training prep_for_ideal_bootstrapping cop_ideal_is nsubj_ideal_situation mark_ideal_that det_situation_this ccomp_is_ideal nsubj_is_observation poss_observation_Our ccomp_``_is
D08-1108	P95-1026	o	An alternative approach to extracting the informal phrases is to use a bootstrapping algorithm -LRB- e.g. Yarowsky -LRB- 1995 -RRB- -RRB-	dep_Yarowsky_1995 dep_e.g._Yarowsky ccomp_-LRB-_e.g. nn_algorithm_bootstrapping det_algorithm_a dobj_use_algorithm aux_use_to xcomp_is_use nsubj_is_approach amod_phrases_informal det_phrases_the dobj_extracting_phrases prepc_to_approach_extracting amod_approach_alternative det_approach_An
D09-1070	P95-1026	p	Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word amod_disambiguation_unsupervised advmod_effectively_quite prep_in_used_disambiguation advmod_used_effectively auxpass_used_been aux_used_has nsubjpass_used_Constraining nn_boundaries_document dobj_using_boundaries prepc_by_Constraining_using dobj_Constraining_learning
D09-1070	P95-1026	o	Our intuition comes from an observation by Yarowsky -LRB- 1995 -RRB- regarding multiple tokens of words in documents	prep_in_tokens_documents prep_of_tokens_words amod_tokens_multiple prep_regarding_Yarowsky_tokens appos_Yarowsky_1995 prep_by_observation_Yarowsky det_observation_an prep_from_comes_observation nsubj_comes_intuition poss_intuition_Our
D09-1095	P95-1026	o	The benefits of using grammatical information for automatic WSD were first explored by Yarowsky -LRB- 1995 -RRB- and Resnik -LRB- 1996 -RRB- in unsupervised approaches to disambiguating single words in context	amod_words_single prep_in_disambiguating_context dobj_disambiguating_words prepc_to_approaches_disambiguating amod_approaches_unsupervised appos_Resnik_1996 conj_and_Yarowsky_Resnik appos_Yarowsky_1995 prep_in_explored_approaches agent_explored_Resnik agent_explored_Yarowsky advmod_explored_first auxpass_explored_were nsubjpass_explored_benefits amod_WSD_automatic prep_for_information_WSD amod_information_grammatical dobj_using_information prepc_of_benefits_using det_benefits_The
D09-1134	P95-1026	o	One heuristic approach is to adapt the self-training algorithm -LRB- Yarowsky 1995 -RRB- to our model	poss_model_our amod_Yarowsky_1995 dep_algorithm_Yarowsky amod_algorithm_self-training det_algorithm_the prep_to_adapt_model dobj_adapt_algorithm aux_adapt_to xcomp_is_adapt nsubj_is_approach nn_approach_heuristic num_approach_One ccomp_``_is
D09-1134	P95-1026	o	This process is repeated for a number of iterations in a self-training fashion -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_fashion_Yarowsky amod_fashion_self-training det_fashion_a prep_in_number_fashion prep_of_number_iterations det_number_a prep_for_repeated_number auxpass_repeated_is nsubjpass_repeated_process det_process_This
D09-1148	P95-1026	o	We propose a method similar to Yarowsky -LRB- 1995 -RRB- to generalize beyond the training set	nn_set_training det_set_the prep_beyond_generalize_set aux_generalize_to appos_Yarowsky_1995 xcomp_similar_generalize prep_to_similar_Yarowsky amod_method_similar det_method_a dobj_propose_method nsubj_propose_We
D09-1149	P95-1026	n	Although previous work -LRB- Yarowsky 1995 Blum and Mitchell 1998 Abney 2000 Zhang 2004 -RRB- has tackled the bootstrapping approach from both the theoretical and practical point of view many key problems still remain unresolved such as the selection of initial seed set	nn_set_seed amod_set_initial prep_of_selection_set det_selection_the prep_such_as_remain_selection acomp_remain_unresolved advmod_remain_still nsubj_remain_problems amod_problems_key amod_problems_many prep_of_point_view amod_point_practical amod_point_theoretical det_point_the preconj_point_both conj_and_theoretical_practical nn_approach_bootstrapping det_approach_the dep_tackled_remain prep_from_tackled_point dobj_tackled_approach aux_tackled_has nsubj_tackled_Abney nsubj_tackled_1998 nsubj_tackled_Mitchell nsubj_tackled_Blum dep_Zhang_2004 appos_Abney_2000 dep_Blum_Zhang conj_and_Blum_Abney conj_and_Blum_1998 conj_and_Blum_Mitchell parataxis_Yarowsky_tackled appos_Yarowsky_1995 dep_work_Yarowsky amod_work_previous pobj_Although_work dep_``_Although
E06-1018	P95-1026	p	However as also pointed out by Yarowsky -LRB- 1995 -RRB- this observation does not hold uniformly over all possible co-occurrences of two words	num_words_two prep_of_co-occurrences_words amod_co-occurrences_possible det_co-occurrences_all prep_over_hold_co-occurrences advmod_hold_uniformly neg_hold_not aux_hold_does nsubj_hold_observation advcl_hold_pointed advmod_hold_However det_observation_this appos_Yarowsky_1995 prep_by_pointed_Yarowsky prt_pointed_out advmod_pointed_also mark_pointed_as
E06-2018	P95-1026	o	This can be done in a supervised -LRB- Yarowsky 1994 -RRB- a semi-supervised -LRB- Yarowsky 1995 -RRB- or a fully unsupervised way -LRB- Pantel & Lin 2002 -RRB-	amod_Pantel_2002 conj_and_Pantel_Lin dep_way_Lin dep_way_Pantel amod_way_unsupervised det_way_a advmod_unsupervised_fully dep_Yarowsky_1995 dep_semi-supervised_Yarowsky det_semi-supervised_a dep_Yarowsky_1994 conj_or_supervised_way dep_supervised_semi-supervised dep_supervised_Yarowsky amod_a_way amod_a_supervised prep_in_done_a auxpass_done_be aux_done_can nsubjpass_done_This
E06-2018	P95-1026	p	Although the relative success of previous disambiguation systems -LRB- e.g. Yarowsky 1995 -RRB- suggests that this should be the case the effect has usually not been quantified as the emphasis was on a task-based evaluation	amod_evaluation_task-based det_evaluation_a prep_on_was_evaluation nsubj_was_emphasis mark_was_as det_emphasis_the advcl_quantified_was auxpass_quantified_been neg_quantified_not advmod_quantified_usually aux_quantified_has nsubjpass_quantified_effect advcl_quantified_suggests det_effect_the det_case_the cop_case_be aux_case_should nsubj_case_this mark_case_that ccomp_suggests_case nsubj_suggests_success mark_suggests_Although dep_Yarowsky_1995 nn_Yarowsky_e.g. dep_systems_Yarowsky nn_systems_disambiguation amod_systems_previous prep_of_success_systems amod_success_relative det_success_the
E06-3004	P95-1026	o	-LRB- Yarowsky 1995 -RRB- and -LRB- Mihalcea and Moldovan 2001 -RRB- utilized bootstrapping for word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word prep_for_bootstrapping_disambiguation xcomp_utilized_bootstrapping nsubj_utilized_Mihalcea nsubj_utilized_Yarowsky dep_Mihalcea_2001 conj_and_Mihalcea_Moldovan conj_and_Yarowsky_Moldovan conj_and_Yarowsky_Mihalcea amod_Yarowsky_1995
E99-1024	P95-1026	o	Our method is based on a decision list proposed by Yarowsky -LRB- Yarowsky 1994 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Yarowsky_Yarowsky appos_Yarowsky_1994 dep_Yarowsky_Yarowsky agent_proposed_Yarowsky vmod_list_proposed nn_list_decision det_list_a prep_on_based_list auxpass_based_is nsubjpass_based_method poss_method_Our ccomp_``_based
E99-1028	P95-1026	o	213 Proceedings of EACL '99 Table 2 The result of disambiguation experiment -LRB- two senses -RRB- -LRB- 6 -RRB- \ -LSB- __ 122 ~ cause ~ e ~ ` ect ~ require a ~ Telose open ~ rrect -LRB- ~ ' -LRB- fall decline win -RCB- \ -RSB- 278 ~ feel think sense T T 280 -LCB- hit attack strike -RCB- I 250 -LCB- leave remain go -RCB- \ -LSB- 183 gcty t ~ Ol accomplish operate ' -RCB- -216 -- -LCB- occur happen ~ -- -LCB- order request arrange ' ~ ~ 240 ~ ass adopt ~ 274 ' ~ roduce create gro ' ~ ~ -- 2 ~ -- ~ ush attack pull ~ ~ s ~ ve 223 -LCB- ship put send -RCB- -LCB- stop end move -RCB- -LCB- add append total -RCB- -LCB- keep maintain protect -RCB- Total 215 -LRB- 77.3 181 -LRB- 72.4 160 -LRB- 87.4 349 -LRB- 92.3 -RRB- ~ ~ Correct -LRB- % -RRB- \ -RSB- 83 -LRB- 77.0 -RRB- 113 -LRB- 86.2 -RRB- I 169 -LRB- 87.5 -RRB- J Yarowsky used an unsupervised learning procedure to perform noun WSD -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_WSD_Yarowsky nn_WSD_noun dobj_perform_WSD aux_perform_to vmod_procedure_perform nn_procedure_learning amod_procedure_unsupervised det_procedure_an dobj_used_procedure nn_Yarowsky_J nn_Yarowsky_169 dep_Yarowsky_I appos_169_87.5 dep_113_used dep_113_Yarowsky dep_113_86.2 num_113_77.0 dep_83_113 nn_Correct_~ dep_349_~ dep_349_92.3 num_349_87.4 dep_160_349 number_160_72.4 dep_181_\ dep_181_% dep_181_Correct dep_181_160 number_181_77.3 dep_215_83 dep_215_181 amod_215_Total dobj_protect_215 conj_keep_protect conj_keep_maintain dep_keep_total parataxis_add_keep conj_add_append dep_add_move parataxis_stop_add conj_stop_end dep_stop_send dep_ve_223 parataxis_~_stop conj_~_put dobj_~_ship dobj_~_ve nsubj_~_s dep_~_~ nn_s_~ dep_pull_~ nsubj_pull_ush dep_pull_~ dep_pull_Proceedings appos_ush_attack num_ush_~ number_~_2 nn_~_~ nn_~_gro dep_create_~ nsubj_create_ass nn_roduce_~ num_~_274 advmod_ass_roduce conj_ass_~ conj_ass_adopt nn_ass_~ num_~_240 parataxis_arrange_create dobj_arrange_~ dobj_arrange_~ nsubj_arrange_order appos_order_request parataxis_occur_arrange dep_occur_~ dep_occur_happen dep_-216_accomplish dep_-216_\ dep_-216_go dep_accomplish_operate nsubj_accomplish_Ol nn_Ol_~ nn_Ol_t nn_Ol_gcty num_Ol_183 nsubj_go_result nsubj_remain_~ dep_remain_require dep_remain_122 nsubj_remain_\ num_leave_250 dep_leave_I dep_hit_strike dep_hit_attack nn_hit_T nn_hit_T dep_hit_sense dep_hit_~ dep_hit_rrect dep_hit_Telose num_T_280 dep_\_win dep_fall_\ appos_fall_decline dep_~_think dep_~_feel dep_~_~ dep_~_278 dep_~_fall nn_rrect_~ amod_Telose_open dep_~_leave dep_~_hit dep_~_a nn_~_ect dep_~_~ nn_~_~ dep_~_e nn_~_cause nn_~_~ dep_122_~ num_122___ number_\_6 num_senses_two nn_experiment_disambiguation rcmod_result_remain appos_result_senses prep_of_result_experiment det_result_The num_Table_2 num_Table_'99 nn_Table_EACL dep_Proceedings_occur dep_Proceedings_-216 prep_of_Proceedings_Table num_Proceedings_213
E99-1028	P95-1026	p	1 Introduction One of the major approaches to disambiguate word senses is supervised learning -LRB- Gale et al. 1992 -RRB- -LRB- Yarowsky 1992 -RRB- -LRB- Bruce and Janyce 1994 -RRB- -LRB- Miller et al. 1994 -RRB- -LRB- Niwa and Nitta 1994 -RRB- -LRB- Luk 1995 -RRB- -LRB- Ng and Lee 1996 -RRB- -LRB- Wilks and Stevenson 1998 -RRB-	amod_Wilks_1998 conj_and_Wilks_Stevenson dep_Ng_1996 conj_and_Ng_Lee dep_Luk_1995 dep_Niwa_1994 conj_and_Niwa_Nitta amod_Miller_1994 dep_Miller_al. nn_Miller_et dep_Bruce_1994 conj_and_Bruce_Janyce dep_Yarowsky_1992 amod_Gale_1992 dep_Gale_al. nn_Gale_et appos_learning_Stevenson appos_learning_Wilks appos_learning_Lee appos_learning_Ng appos_learning_Luk appos_learning_Nitta appos_learning_Niwa appos_learning_Miller appos_learning_Janyce appos_learning_Bruce appos_learning_Yarowsky appos_learning_Gale dobj_supervised_learning auxpass_supervised_is nsubjpass_supervised_One nn_senses_word dobj_disambiguate_senses aux_disambiguate_to vmod_approaches_disambiguate amod_approaches_major det_approaches_the prep_of_One_approaches rcmod_Introduction_supervised num_Introduction_1
H05-1017	P95-1026	o	Within the machine learning paradigm IL has been incorporated as a technique for bootstrapping an extensional learning algorithm as in -LRB- Yarowsky 1995 Collins and Singer 1999 Liu et al. 2004 -RRB-	num_Liu_2004 nn_Liu_al. nn_Liu_et dep_Collins_Liu num_Collins_1999 conj_and_Collins_Singer dep_Yarowsky_Singer dep_Yarowsky_Collins appos_Yarowsky_1995 pobj_in_Yarowsky pcomp_as_in nn_algorithm_learning amod_algorithm_extensional det_algorithm_an dobj_bootstrapping_algorithm prepc_for_technique_bootstrapping det_technique_a prep_incorporated_as prep_as_incorporated_technique auxpass_incorporated_been aux_incorporated_has prep_within_incorporated_machine appos_paradigm_IL amod_paradigm_learning dep_machine_paradigm det_machine_the
H05-1017	P95-1026	o	It is possible to recognize a common structure of these works based on a typical bootstrap schema -LRB- Yarowsky 1995 Collins and Singer 1999 -RRB- Step 1 Initial unsupervised categorization	amod_categorization_unsupervised amod_categorization_Initial dep_Step_categorization num_Step_1 amod_Collins_1999 conj_and_Collins_Singer dep_Yarowsky_Singer dep_Yarowsky_Collins dep_Yarowsky_1995 dep_schema_Yarowsky nn_schema_bootstrap amod_schema_typical det_schema_a det_works_these prep_of_structure_works amod_structure_common det_structure_a dobj_recognize_structure aux_recognize_to dep_possible_Step prep_based_on_possible_schema xcomp_possible_recognize cop_possible_is nsubj_possible_It
H05-1046	P95-1026	o	There has of course been a large amount of work on the more general problem of word-sense disambiguation e.g. -LRB- Yarowsky 1995 -RRB- -LRB- Kilgarriff and Edmonds 2002 -RRB-	num_Edmonds_2002 conj_and_Kilgarriff_Edmonds num_Yarowsky_1995 amod_disambiguation_word-sense prep_of_problem_disambiguation amod_problem_general det_problem_the advmod_general_more dep_amount_Edmonds dep_amount_Kilgarriff appos_amount_Yarowsky dep_amount_e.g. prep_on_amount_problem prep_of_amount_work amod_amount_large det_amount_a cop_amount_been ccomp_has_amount prep_of_has_course expl_has_There
H05-1050	P95-1026	o	We extracted all examples of each word from the 14-million-word English portion of the Hansards .8 Note that this is considerably smaller than Yarowskys -LRB- 1995 -RRB- corpus of 460 million words so bootstrapping will not perform as well and may be more sensitive to the choice of seed	prep_of_choice_seed det_choice_the prep_to_sensitive_choice advmod_sensitive_more cop_sensitive_be aux_sensitive_may nsubj_sensitive_this advmod_well_as advmod_perform_well neg_perform_not aux_perform_will ccomp_bootstrapping_perform advmod_bootstrapping_so num_words_million number_million_460 prep_of_corpus_words nn_corpus_Yarowskys appos_Yarowskys_1995 conj_and_smaller_sensitive dep_smaller_bootstrapping prep_than_smaller_corpus advmod_smaller_considerably cop_smaller_is nsubj_smaller_this mark_smaller_that ccomp_Note_sensitive ccomp_Note_smaller num_Note_.8 nn_Note_Hansards det_Note_the prep_of_portion_Note amod_portion_English amod_portion_14-million-word det_portion_the prep_from_word_portion det_word_each prep_of_examples_word det_examples_all dobj_extracted_examples nsubj_extracted_We ccomp_``_extracted
H05-1050	P95-1026	o	In the supervised condition we used just 2 additional task instances plant and tank each with 4000 handannotated instances drawn from a large balanced corpus -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 appos_corpus_Yarowsky amod_corpus_balanced amod_corpus_large det_corpus_a prep_from_drawn_corpus vmod_instances_drawn amod_instances_handannotated num_instances_4000 prep_with_each_instances conj_and_plant_tank appos_instances_each appos_instances_tank appos_instances_plant nn_instances_task amod_instances_additional num_instances_2 quantmod_2_just dobj_used_instances nsubj_used_we prep_in_used_condition amod_condition_supervised det_condition_the
H05-1050	P95-1026	n	6 Conclusions In this paper we showed that it is sometimes possible indeed preferableto eliminate the initial bit of supervision in bootstrapping algorithms such as the Yarowsky -LRB- 1995 -RRB- algorithm for word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word prep_for_algorithm_disambiguation nn_algorithm_Yarowsky appos_Yarowsky_1995 det_Yarowsky_the prep_such_as_algorithms_algorithm dobj_bootstrapping_algorithms prepc_in_bit_bootstrapping prep_of_bit_supervision amod_bit_initial det_bit_the dobj_eliminate_bit nsubj_eliminate_preferableto ccomp_possible_eliminate advmod_possible_indeed advmod_possible_sometimes cop_possible_is nsubj_possible_it mark_possible_that ccomp_showed_possible nsubj_showed_we nsubj_showed_Conclusions det_paper_this prep_in_Conclusions_paper num_Conclusions_6
H05-1050	P95-1026	p	2.1 The Yarowsky algorithm Yarowsky -LRB- 1995 -RRB- sparked considerable interest in bootstrapping with his successful method for word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word prep_for_method_disambiguation amod_method_successful poss_method_his prep_with_bootstrapping_method amod_interest_considerable prepc_in_sparked_bootstrapping dobj_sparked_interest nsubj_sparked_algorithm appos_Yarowsky_1995 dep_algorithm_Yarowsky nn_algorithm_Yarowsky det_algorithm_The rcmod_2.1_sparked
H05-1050	P95-1026	n	Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand -LRB- Yarowsky 1995 -RRB- which in turn is known to rival supervised methods	amod_methods_supervised amod_methods_rival prep_to_known_methods auxpass_known_is prep_in_known_turn nsubjpass_known_which dep_Yarowsky_1995 rcmod_hand_known appos_hand_Yarowsky prep_by_picking_hand dobj_picking_seeds prepc_than_effective_picking advmod_effective_more advmod_effective_significantly cop_effective_is nsubj_effective_technique mark_effective_that amod_technique_unsupervised poss_technique_our ccomp_show_effective nsubj_show_experiments amod_Hansards_Canadian det_Hansards_the prep_on_experiments_Hansards poss_experiments_Our
H05-1107	P95-1026	o	Yarowsky -LRB- 1995 -RRB- used this method for word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word prep_for_method_disambiguation det_method_this dobj_used_method vmod_Yarowsky_used appos_Yarowsky_1995
H05-1114	P95-1026	o	Many corpus based statistical methods have been proposed to solve this problem including supervised learning algorithms -LRB- Leacock et al. 1998 Towel and Voorheest 1998 -RRB- weakly supervised learning algorithms -LRB- Dagan and Itai 1994 Li and Li 2004 Mihalcea 2004 Niu et al. 2005 Park et al. 2000 Yarowsky 1995 -RRB- unsupervised learning algorithms -LRB- or word sense discrimination -RRB- -LRB- Pedersen and Bruce 1997 Schutze 1998 -RRB- and knowledge based algorithms -LRB- Lesk 1986 McCarthy et al. 2004 -RRB-	num_McCarthy_2004 nn_McCarthy_al. nn_McCarthy_et dep_Lesk_McCarthy dep_Lesk_1986 appos_algorithms_Lesk pobj_based_algorithms prep_knowledge_based dep_Schutze_1998 dep_Pedersen_Schutze num_Pedersen_1997 conj_and_Pedersen_Bruce dep_sense_discrimination conj_or_sense_word appos_algorithms_Bruce appos_algorithms_Pedersen dep_algorithms_sense dep_algorithms_word nn_algorithms_learning amod_algorithms_unsupervised dep_Yarowsky_1995 num_Park_2000 nn_Park_al. nn_Park_et conj_and_Niu_knowledge conj_and_Niu_algorithms dep_Niu_Yarowsky conj_and_Niu_Park num_Niu_2005 nn_Niu_al. nn_Niu_et num_Mihalcea_2004 num_Li_2004 conj_and_Li_Li dep_Dagan_knowledge dep_Dagan_algorithms dep_Dagan_Park dep_Dagan_Niu conj_and_Dagan_Mihalcea conj_and_Dagan_Li conj_and_Dagan_Li conj_and_Dagan_1994 conj_and_Dagan_Itai dep_algorithms_Mihalcea dep_algorithms_Li dep_algorithms_1994 dep_algorithms_Itai dep_algorithms_Dagan dobj_learning_algorithms xcomp_supervised_learning advmod_supervised_weakly dep_Towel_1998 conj_and_Towel_Voorheest dep_Leacock_Voorheest dep_Leacock_Towel appos_Leacock_1998 dep_Leacock_al. nn_Leacock_et vmod_algorithms_supervised appos_algorithms_Leacock nn_algorithms_learning dep_supervised_algorithms det_problem_this prep_including_solve_supervised dobj_solve_problem aux_solve_to xcomp_proposed_solve auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_statistical amod_methods_based nn_methods_corpus amod_corpus_Many
I05-3009	P95-1026	o	It is appreciated that multi-sense words appearing in the same document tend to be tagged with the same word sense if they belong to the same common domain in the semantic hierarchy -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 amod_hierarchy_semantic det_hierarchy_the prep_in_domain_hierarchy amod_domain_common amod_domain_same det_domain_the dep_belong_Yarowsky prep_to_belong_domain nsubj_belong_they mark_belong_if nn_sense_word amod_sense_same det_sense_the prep_with_tagged_sense auxpass_tagged_be aux_tagged_to advcl_tend_belong xcomp_tend_tagged nsubj_tend_words mark_tend_that amod_document_same det_document_the prep_in_appearing_document vmod_words_appearing amod_words_multi-sense ccomp_appreciated_tend auxpass_appreciated_is nsubjpass_appreciated_It
I08-1040	P95-1026	o	But it is close to the paradigm described by Yarowsky -LRB- 1995 -RRB- and Turney -LRB- 2002 -RRB- as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples	amod_samples_unlabelled prep_with_enlarged_samples advmod_enlarged_incrementally cop_enlarged_is nsubj_enlarged_which ccomp_set_enlarged vmod_data_set nn_data_seed amod_data_small det_data_a advmod_small_relatively pobj_self-training_data prepc_based_on_self-training_on dobj_employs_self-training advmod_employs_also nsubj_employs_it mark_employs_as appos_Turney_2002 conj_and_Yarowsky_Turney appos_Yarowsky_1995 advcl_described_employs agent_described_Turney agent_described_Yarowsky vmod_paradigm_described det_paradigm_the prep_to_close_paradigm cop_close_is nsubj_close_it cc_close_But
J01-3001	P95-1026	o	Yarowsky -LRB- 1995 -RRB- dealt with this problem largely by producing an unsupervised learning algorithm that generates probabilistic decision list models of word senses from seed collocates	nn_collocates_seed nn_senses_word prep_of_models_senses nn_models_list nn_models_decision amod_models_probabilistic prep_from_generates_collocates dobj_generates_models nsubj_generates_that rcmod_algorithm_generates nn_algorithm_learning amod_algorithm_unsupervised det_algorithm_an dobj_producing_algorithm det_problem_this prep_with_dealt_problem prepc_by_Yarowsky_producing advmod_Yarowsky_largely vmod_Yarowsky_dealt appos_Yarowsky_1995
J01-3001	P95-1026	o	Currently machine learning methods -LRB- Yarowsky 1995 Rigau Atserias and Agirre 1997 -RRB- and combinations of classifiers -LRB- McRoy 1992 -RRB- have been popular	cop_popular_been aux_popular_have nsubj_popular_combinations nsubj_popular_methods advmod_popular_Currently num_McRoy_1992 appos_classifiers_McRoy prep_of_combinations_classifiers num_Agirre_1997 conj_and_Rigau_Agirre conj_and_Rigau_Atserias dep_Yarowsky_Agirre dep_Yarowsky_Atserias dep_Yarowsky_Rigau num_Yarowsky_1995 conj_and_methods_combinations dep_methods_Yarowsky nn_methods_learning nn_methods_machine
J01-3001	P95-1026	o	Some researchers have concentrated on producing WSD systems that base results on a limited number of words for example Yarowsky -LRB- 1995 -RRB- and Schtitze -LRB- 1992 -RRB- who quoted results for 12 words and a second group including Leacock Towell and Voorhees -LRB- 1993 -RRB- and Bruce and Wiebe -LRB- 1994 -RRB- who gave results for just one namely interest	advmod_interest_namely appos_one_interest advmod_one_just prep_for_gave_one dobj_gave_results nsubj_gave_who appos_Wiebe_1994 appos_Voorhees_1993 rcmod_Leacock_gave conj_and_Leacock_Wiebe conj_and_Leacock_Bruce conj_and_Leacock_Voorhees conj_and_Leacock_Towell prep_including_group_Wiebe prep_including_group_Bruce prep_including_group_Voorhees prep_including_group_Towell prep_including_group_Leacock amod_group_second det_group_a num_words_12 prep_for_results_words conj_and_quoted_group dobj_quoted_results nsubj_quoted_who appos_Schtitze_1992 conj_and_Yarowsky_Schtitze appos_Yarowsky_1995 nn_Yarowsky_example dep_for_group dep_for_quoted pobj_for_Schtitze pobj_for_Yarowsky ccomp_,_for prep_of_number_words amod_number_limited det_number_a prep_on_base_number dobj_base_results nsubj_base_that rcmod_systems_base nn_systems_WSD dobj_producing_systems prepc_on_concentrated_producing aux_concentrated_have nsubj_concentrated_researchers det_researchers_Some ccomp_``_concentrated
J01-3001	P95-1026	o	-LRB- 1991 -RRB- Yarowsky -LRB- 1995 -RRB- and others	conj_and_Yarowsky_others dep_Yarowsky_1995 dep_1991_others dep_1991_Yarowsky dep_''_1991
J02-3002	P95-1026	o	Since then this idea has been applied to several tasks including word sense disambiguation -LRB- Yarowsky 1995 -RRB- and named-entity recognition -LRB- Cucerzan and Yarowsky 1999 -RRB-	num_Yarowsky_1999 conj_and_Cucerzan_Yarowsky dep_recognition_Yarowsky dep_recognition_Cucerzan nn_recognition_named-entity num_Yarowsky_1995 conj_and_disambiguation_recognition dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word prep_including_tasks_recognition prep_including_tasks_disambiguation amod_tasks_several prep_to_applied_tasks auxpass_applied_been aux_applied_has nsubjpass_applied_idea mark_applied_Since det_idea_this advmod_idea_then advcl_``_applied
J03-3002	P95-1026	o	This approach to minimally supervised classifier construction has been widely studied -LRB- Yarowsky 1995 -RRB- especially in cases in which the features of interest are orthogonal in some sense -LRB- e.g. Blum and Mitchell 1998 Abney 2002 -RRB-	num_Abney_2002 dep_1998_Abney nsubj_1998_Mitchell nsubj_1998_Blum dep_1998_e.g. conj_and_Blum_Mitchell det_sense_some prep_in_orthogonal_sense cop_orthogonal_are nsubj_orthogonal_features prep_in_orthogonal_which prep_of_features_interest det_features_the dep_cases_1998 rcmod_cases_orthogonal num_Yarowsky_1995 prep_in_studied_cases advmod_studied_especially dep_studied_Yarowsky advmod_studied_widely auxpass_studied_been aux_studied_has nsubjpass_studied_approach nn_construction_classifier amod_construction_supervised advmod_supervised_minimally prep_to_approach_construction det_approach_This
J04-1001	P95-1026	o	Yarowsky -LRB- 1995 -RRB- has proposed a bootstrapping method for word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word prep_for_method_disambiguation nn_method_bootstrapping det_method_a dobj_proposed_method aux_proposed_has nsubj_proposed_Yarowsky appos_Yarowsky_1995
J04-1001	P95-1026	o	This implementation is exactly the one proposed in Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 prep_in_proposed_Yarowsky vmod_one_proposed det_one_the advmod_one_exactly cop_one_is nsubj_one_implementation det_implementation_This
J04-1001	P95-1026	o	We viewed the seed word as a classified sentence following a similar proposal in Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 prep_in_proposal_Yarowsky amod_proposal_similar det_proposal_a amod_sentence_classified det_sentence_a nn_word_seed det_word_the prep_following_viewed_proposal prep_as_viewed_sentence dobj_viewed_word nsubj_viewed_We
J04-1001	P95-1026	o	4.4 Experiment 2 Yarowskys Words We also conducted translation on seven of the twelve English words studied in Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 prep_in_studied_Yarowsky vmod_words_studied nn_words_English num_words_twelve det_words_the prep_of_seven_words prep_on_conducted_seven dobj_conducted_translation advmod_conducted_also nsubj_conducted_We rcmod_Words_conducted nn_Words_Yarowskys dep_Experiment_Words num_Experiment_2 num_Experiment_4.4
J04-1001	P95-1026	o	Note that the results of MB-D here can not be directly compared with those in Yarowsky -LRB- 1995 -RRB- because the data used are different	cop_different_are nsubj_different_data mark_different_because vmod_data_used det_data_the appos_Yarowsky_1995 prep_in_those_Yarowsky advcl_compared_different prep_with_compared_those advmod_compared_directly auxpass_compared_be neg_compared_not aux_compared_can nsubjpass_compared_results mark_compared_that advmod_MB-D_here prep_of_results_MB-D det_results_the ccomp_Note_compared
J04-1001	P95-1026	o	Yarowsky -LRB- 1995 -RRB- proposed such a method for word sense disambiguation which we refer to as monolingual bootstrapping	amod_bootstrapping_monolingual pobj_refer_bootstrapping prepc_as_to_refer_as nsubj_refer_we dobj_refer_which rcmod_disambiguation_refer nn_disambiguation_sense nn_disambiguation_word prep_for_method_disambiguation det_method_a predet_method_such dobj_proposed_method nsubj_proposed_Yarowsky appos_Yarowsky_1995
J04-1001	P95-1026	o	After line 17 we can employ the one-sense-per-discourse heuristic to further classify unclassified data as proposed in Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 prep_in_proposed_Yarowsky mark_proposed_as amod_data_unclassified dobj_classify_data dep_further_classify amod_heuristic_one-sense-per-discourse det_heuristic_the advcl_employ_proposed prep_to_employ_further dobj_employ_heuristic aux_employ_can nsubj_employ_we prep_after_employ_line num_line_17
J04-1003	P95-1026	p	A variety of classifiers have been employed for this task -LRB- see Mooney -LSB- 1996 -RSB- and Ide and Veronis -LSB- 1998 -RSB- for overviews -RRB- the most popular being decision lists -LRB- Yarowsky 1994 1995 -RRB- and naive Bayesian classifiers -LRB- Pedersen 2000 Ng 1997 Pedersen and Bruce 1998 Mooney 1996 Cucerzan and Yarowsky 2002 -RRB-	num_Yarowsky_2002 conj_and_Cucerzan_Yarowsky num_Mooney_1996 num_Bruce_1998 conj_and_Pedersen_Bruce num_Ng_1997 dep_Pedersen_Yarowsky dep_Pedersen_Cucerzan dep_Pedersen_Mooney dep_Pedersen_Bruce dep_Pedersen_Pedersen dep_Pedersen_Ng num_Pedersen_2000 appos_classifiers_Pedersen amod_classifiers_Bayesian amod_classifiers_naive conj_and_Yarowsky_classifiers amod_Yarowsky_1995 num_Yarowsky_1994 dep_lists_classifiers dep_lists_Yarowsky nn_lists_decision cop_lists_being nsubj_lists_popular advmod_popular_most det_popular_the appos_Veronis_1998 conj_and_Mooney_Veronis conj_and_Mooney_Ide dep_Mooney_1996 prep_for_see_overviews dobj_see_Veronis dobj_see_Ide dobj_see_Mooney det_task_this dep_employed_lists dep_employed_see prep_for_employed_task auxpass_employed_been aux_employed_have nsubjpass_employed_variety prep_of_variety_classifiers det_variety_A ccomp_``_employed
J04-3004	P95-1026	p	The Yarowsky -LRB- 1995 -RRB- algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics	amod_linguistics_computational prep_in_known_linguistics advmod_known_widely acomp_become_known aux_become_to vmod_algorithms_become nn_algorithms_bootstrapping amod_algorithms_first det_algorithms_the prep_of_one_algorithms cop_one_was nsubj_one_algorithm nn_algorithm_Yarowsky det_algorithm_The appos_Yarowsky_1995
J06-2003	P95-1026	o	The algorithm we implemented is inspired by the work of Yarowsky -LRB- 1995 -RRB- on word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word appos_Yarowsky_1995 prep_of_work_Yarowsky det_work_the prep_on_inspired_disambiguation agent_inspired_work auxpass_inspired_is nsubjpass_inspired_algorithm nsubj_implemented_we rcmod_algorithm_implemented det_algorithm_The
J98-1001	P95-1026	o	-LRB- 1992 -RRB- Pereira and Tishby -LRB- 1992 -RRB- and Pereira Tishby and Lee -LRB- 1993 -RRB- propose methods that derive classes from the distributional properties of the corpus itself while other authors use external information sources to define classes Resnik -LRB- 1992 -RRB- uses the taxonomy of WordNet Yarowsky -LRB- 1992 -RRB- uses the categories of Roget 's Thesaurus Slator -LRB- 1992 -RRB- and Liddy and Paik -LRB- 1993 -RRB- use the subject codes in the LDOCE Luk -LRB- 1995 -RRB- uses conceptual sets built from the LDOCE definitions	amod_definitions_LDOCE det_definitions_the prep_from_built_definitions vmod_sets_built amod_sets_conceptual dobj_uses_sets nsubj_uses_Luk appos_Luk_1995 det_LDOCE_the amod_codes_subject det_codes_the parataxis_use_uses prep_in_use_LDOCE dobj_use_codes appos_Paik_1993 conj_and_Liddy_Paik appos_Slator_1992 conj_and_Thesaurus_Paik conj_and_Thesaurus_Liddy conj_and_Thesaurus_Slator poss_Thesaurus_Roget prep_of_categories_Liddy prep_of_categories_Slator prep_of_categories_Thesaurus det_categories_the dep_uses_use dobj_uses_categories nsubj_uses_Yarowsky appos_Yarowsky_1992 prep_of_taxonomy_WordNet det_taxonomy_the parataxis_uses_uses dobj_uses_taxonomy nsubj_uses_Resnik appos_Resnik_1992 dobj_define_classes aux_define_to nn_sources_information amod_sources_external parataxis_use_uses xcomp_use_define dobj_use_sources nsubj_use_authors mark_use_while amod_authors_other npadvmod_corpus_itself det_corpus_the prep_of_properties_corpus amod_properties_distributional det_properties_the prep_from_derive_properties dobj_derive_classes nsubj_derive_that rcmod_methods_derive dobj_propose_methods nsubj_propose_Lee nsubj_propose_Tishby nsubj_propose_Pereira appos_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby appos_Tishby_1992 dep_Pereira_use conj_and_Pereira_propose conj_and_Pereira_Tishby dep_Pereira_1992
J98-1001	P95-1026	o	Aware of this problem Resnik and Yarowsky suggest creating the sense distance matrix based on results in experimental psychology such as Miller and Charles -LRB- 1991 -RRB- or Resnik -LRB- 1995b -RRB-	appos_Resnik_1995b appos_Charles_1991 conj_or_Miller_Resnik conj_and_Miller_Charles prep_such_as_psychology_Resnik prep_such_as_psychology_Charles prep_such_as_psychology_Miller amod_psychology_experimental prep_in_results_psychology pobj_matrix_results prepc_based_on_matrix_on nn_matrix_distance nn_matrix_sense det_matrix_the dobj_creating_matrix xcomp_suggest_creating nsubj_suggest_Yarowsky nsubj_suggest_Resnik nsubj_suggest_Aware det_problem_this conj_and_Aware_Yarowsky conj_and_Aware_Resnik prep_of_Aware_problem
J98-1002	P95-1026	o	Word Senses Sample Size Feedback Size % Correct % Correct per Sense Total drug narcotic 65 100 92.3 90.5 medicine 83 65 89.1 sentence judgment 23327100 .0 92.5 grammar 4 42 50.0 suit court 212 1,461 98.6 94.8 garment 21 81 55.0 player performer 48 230 87.5 92.3 participant 44 1,552 97.7 the feedback sets -RRB- consisted of a few dozen examples in comparison to thousands of examples needed in other corpus-based methods -LRB- Sch ~ itze 1992 Yarowsky 1995 -RRB-	num_Yarowsky_1995 amod_1992_itze number_1992_~ dep_Sch_Yarowsky num_Sch_1992 dep_methods_Sch amod_methods_corpus-based amod_methods_other prep_in_needed_methods vmod_examples_needed prep_of_thousands_examples prep_to_comparison_thousands nn_examples_dozen amod_examples_few det_examples_a prep_in_consisted_comparison prep_of_consisted_examples dep_consisted_Senses nn_sets_feedback det_sets_the num_sets_97.7 nn_sets_participant num_sets_92.3 num_sets_87.5 num_97.7_1,552 number_1,552_44 number_87.5_230 number_87.5_48 dep_performer_sets nn_performer_player num_performer_55.0 num_performer_81 number_81_21 dep_garment_performer num_garment_94.8 dep_98.6_garment dep_1,461_98.6 dep_212_1,461 amod_court_212 nn_court_suit dep_50.0_court dep_42_50.0 dep_4_42 dep_grammar_4 dep_92.5_grammar dep_.0_92.5 number_.0_23327100 dep_judgment_.0 nn_judgment_sentence num_judgment_89.1 number_89.1_65 number_89.1_83 dep_medicine_judgment dep_90.5_medicine dep_92.3_90.5 dep_100_92.3 number_100_65 dep_narcotic_100 amod_drug_narcotic dep_Total_drug amod_Sense_Total prep_per_Correct_Sense nn_Correct_% amod_Correct_Correct nn_Correct_% nn_Correct_Size nn_Correct_Feedback nn_Correct_Size nn_Correct_Sample dobj_Senses_Correct nsubj_Senses_Word
J98-1002	P95-1026	o	Recently Yarowsky -LRB- 1995 -RRB- combined an MRD and a corpus in a bootstrapping process	nn_process_bootstrapping det_process_a det_corpus_a prep_in_MRD_process conj_and_MRD_corpus det_MRD_an dobj_combined_corpus dobj_combined_MRD vmod_Yarowsky_combined appos_Yarowsky_1995 dep_Recently_Yarowsky advmod_``_Recently
J98-1003	P95-1026	o	Roget 's has been used as the sense division in two recent WSD works -LRB- Yarowsky 1992 Luk 1995 -RRB- more or less as is except for a small number of senses added to fill gaps	dobj_fill_gaps aux_fill_to xcomp_added_fill prep_of_number_senses amod_number_small det_number_a dep_is_added pobj_is_number prepc_except_for_is_for advmod_is_as advmod_is_less advmod_is_more conj_or_more_less num_Luk_1995 dep_Yarowsky_Luk num_Yarowsky_1992 dep_works_is dep_works_Yarowsky nn_works_WSD amod_works_recent num_works_two nn_division_sense det_division_the prep_in_used_works prep_as_used_division auxpass_used_been aux_used_has nsubjpass_used_Roget possessive_Roget_'s
J98-1003	P95-1026	o	WSD has received increasing attention in recent literature on computational linguistics -LRB- Lesk 1986 Schi.itze 1992 Gale Church and Yarowsky 1992 Yarowsky 1992 1995 Bruce and Wiebe 1995 Luk 1995 Ng and Lee 1996 Chang et al. 1996 -RRB-	dep_al._1996 nn_al._et nn_al._Chang dep_Ng_1996 conj_and_Ng_Lee num_Luk_1995 num_Bruce_1995 conj_and_Bruce_Wiebe conj_Yarowsky_al. conj_Yarowsky_Lee conj_Yarowsky_Ng conj_Yarowsky_Luk conj_Yarowsky_Wiebe conj_Yarowsky_Bruce num_Yarowsky_1995 num_Yarowsky_1992 num_Yarowsky_1992 conj_and_Gale_Yarowsky conj_and_Gale_Church num_Schi.itze_1992 dep_Lesk_Yarowsky conj_Lesk_Yarowsky conj_Lesk_Church conj_Lesk_Gale conj_Lesk_Schi.itze num_Lesk_1986 dep_linguistics_Lesk amod_linguistics_computational amod_literature_recent amod_attention_increasing prep_on_received_linguistics prep_in_received_literature dobj_received_attention aux_received_has nsubj_received_WSD
J98-1003	P95-1026	o	-LRB- ~ 1998 Association for Computational Linguistics Computational Linguistics Volume 24 Number 1 1995 -RRB- -LRB- 3 -RRB- thesaurus categories -LRB- Yarowsky 1992 Chen and Chang 1994 -RRB- -LRB- 4 -RRB- translation in another language -LRB- Gale Church and Yarowsky 1992 Dagan Itai and Schwall 1991 Dagan and Itai 1994 -RRB- -LRB- 5 -RRB- automatically induced clusters with sublexical representation -LRB- Schiitze 1992 -RRB- and -LRB- 6 -RRB- hand-crafted lexicons -LRB- McRoy 1992 -RRB-	num_McRoy_1992 appos_lexicons_McRoy amod_lexicons_hand-crafted dep_lexicons_6 dep_Schiitze_1992 dep_representation_Schiitze amod_representation_sublexical dep_induced_lexicons cc_induced_and prep_with_induced_representation dobj_induced_clusters advmod_induced_automatically nsubj_induced_Association num_Itai_1994 conj_and_Dagan_Itai num_Schwall_1991 conj_and_Dagan_Schwall conj_and_Dagan_Itai num_Yarowsky_1992 dep_Gale_Itai dep_Gale_Dagan conj_and_Gale_Schwall conj_and_Gale_Itai conj_and_Gale_Dagan conj_and_Gale_Yarowsky conj_and_Gale_Church det_language_another dep_translation_Dagan dep_translation_Yarowsky dep_translation_Church dep_translation_Gale prep_in_translation_language dep_translation_4 dep_Chen_1994 conj_and_Chen_Chang dep_Yarowsky_Chang dep_Yarowsky_Chen num_Yarowsky_1992 dep_categories_Yarowsky nn_categories_thesaurus dep_categories_3 number_1995_1 num_Number_1995 dep_24_Number dep_Volume_24 nn_Volume_Linguistics nn_Volume_Computational nn_Volume_Linguistics nn_Volume_Computational appos_Association_5 appos_Association_translation appos_Association_categories prep_for_Association_Volume num_Association_1998 number_1998_~
J98-1003	P95-1026	o	Lacking an automatic method recent WSD works -LRB- Bruce and Wiebe 1995 Luk 1995 Yarowsky 1995 -RRB- still resort to human intervention to identify and group closely related senses in an MRD	det_MRD_an prep_in_senses_MRD amod_senses_related nn_senses_group advmod_related_closely conj_and_identify_senses aux_identify_to vmod_intervention_senses vmod_intervention_identify amod_intervention_human prep_to_resort_intervention advmod_resort_still num_Yarowsky_1995 num_Luk_1995 num_Wiebe_1995 dep_Bruce_Yarowsky conj_and_Bruce_Luk conj_and_Bruce_Wiebe dep_works_resort dep_works_Luk dep_works_Wiebe dep_works_Bruce nn_works_WSD amod_works_recent dep_works_Lacking amod_method_automatic det_method_an dobj_Lacking_method
J98-1003	P95-1026	o	Using thesaurus categories directly as a coarse sense division may seem to be a viable alternative -LRB- Yarowsky 1995 -RRB-	num_Yarowsky_1995 appos_alternative_Yarowsky amod_alternative_viable det_alternative_a cop_alternative_be aux_alternative_to xcomp_seem_alternative aux_seem_may csubj_seem_Using nn_division_sense amod_division_coarse det_division_a nn_categories_thesaurus prep_as_Using_division advmod_Using_directly dobj_Using_categories
J98-1003	P95-1026	o	TopSense is tested on 20 words extensively investigated in recent WSD literature -LRB- Schi ~ tze 1992 Yarowsky 1992 Luk 1995 -RRB-	num_Luk_1995 num_Yarowsky_1992 num_tze_1992 num_tze_~ dep_Schi_Luk conj_Schi_Yarowsky dep_Schi_tze dep_literature_Schi nn_literature_WSD amod_literature_recent prep_in_investigated_literature advmod_investigated_extensively vmod_words_investigated num_words_20 prep_on_tested_words auxpass_tested_is nsubjpass_tested_TopSense
J98-1004	P95-1026	o	The fact that the error rate more than doubles when the seeds in Yarowsky 's -LRB- 1995 -RRB- experiments are reduced from a sense 's best collocations to just one word per sense suggests that the error rate would increase further if no seeds were provided	auxpass_provided_were nsubjpass_provided_seeds mark_provided_if neg_seeds_no advcl_increase_provided advmod_increase_further aux_increase_would nsubj_increase_rate mark_increase_that nn_rate_error det_rate_the ccomp_suggests_increase nsubj_suggests_rate mark_suggests_that prep_per_word_sense num_word_one quantmod_one_just prep_to_collocations_word amod_collocations_best poss_collocations_sense det_sense_a prep_from_reduced_collocations auxpass_reduced_are nsubjpass_reduced_seeds advmod_reduced_when nn_experiments_Yarowsky dep_Yarowsky_1995 possessive_Yarowsky_'s prep_in_seeds_experiments det_seeds_the mark_doubles_than dep_more_doubles rcmod_rate_reduced amod_rate_more nn_rate_error det_rate_the ccomp_fact_suggests det_fact_The
J98-1004	P95-1026	p	Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process -LRB- Yarowsky 1995 -RRB-	num_Yarowsky_1995 appos_process_Yarowsky nn_process_training det_process_the dobj_start_process aux_start_to prep_per_word_sense nn_word_seed num_word_one vmod_input_start prep_as_input_word nn_input_user amod_input_little pobj_as_input prep_requires_as nsubj_requires_that rcmod_algorithm_requires det_algorithm_an dobj_proposed_algorithm aux_proposed_has nsubj_proposed_Yarowsky
J98-1005	P95-1026	o	At each training-set size a new copy of the network is trained under each of the following conditions -LRB- 1 -RRB- using SULU -LRB- 2 -RRB- using SULU but supplying only the labeled training examples to synthesize -LRB- 3 -RRB- standard network training -LRB- 4 -RRB- using a re-implementation of an algorithm proposed by Yarowsky -LRB- 1995 -RRB- and -LRB- 5 -RRB- using standard network training but with all training examples labeled to establish an upper bound	amod_bound_upper det_bound_an dep_establish_bound aux_establish_to xcomp_labeled_establish vmod_examples_labeled nn_examples_training det_examples_all pobj_with_examples nn_training_network amod_training_standard conj_but_using_with dobj_using_training dep_5_with dep_5_using appos_Yarowsky_1995 agent_proposed_Yarowsky vmod_algorithm_proposed det_algorithm_an prep_of_re-implementation_algorithm det_re-implementation_a conj_and_using_5 dobj_using_re-implementation dep_4_5 dep_4_using nn_training_network amod_training_standard dep_training_3 aux_synthesize_to vmod_examples_synthesize nn_examples_training amod_examples_labeled det_examples_the advmod_examples_only dobj_supplying_examples conj_but_using_4 conj_but_using_training conj_but_using_supplying dobj_using_SULU dep_2_4 dep_2_training dep_2_supplying dep_2_using dep_using_2 dobj_using_SULU dep_1_using amod_conditions_following det_conditions_the prep_of_each_conditions dep_trained_1 prep_under_trained_each auxpass_trained_is nsubjpass_trained_copy prep_at_trained_size det_network_the prep_of_copy_network amod_copy_new det_copy_a nn_size_training-set det_size_each
J98-1006	P95-1026	o	Several artificial techniques have been used so that classifiers can be developed and tested without having to invest in manually tagging the data Yarowsky -LRB- 1993 -RRB- and Sch/itze -LRB- 1995 -RRB- have acquired training and testing materials by creating pseudowords from existing nonhomographic forms	amod_forms_nonhomographic amod_forms_existing prep_from_creating_forms dobj_creating_pseudowords nn_materials_testing nn_materials_training conj_and_training_testing prepc_by_acquired_creating dobj_acquired_materials aux_acquired_have nsubj_acquired_Sch/itze nsubj_acquired_Yarowsky appos_Sch/itze_1995 conj_and_Yarowsky_Sch/itze appos_Yarowsky_1993 det_data_the dobj_tagging_data advmod_tagging_manually prepc_in_invest_tagging aux_invest_to xcomp_having_invest nsubjpass_tested_classifiers prepc_without_developed_having conj_and_developed_tested auxpass_developed_be aux_developed_can nsubjpass_developed_classifiers mark_developed_that mark_developed_so parataxis_used_acquired advcl_used_tested advcl_used_developed auxpass_used_been aux_used_have nsubjpass_used_techniques amod_techniques_artificial amod_techniques_Several
J98-1006	P95-1026	o	Yarowsky -LRB- 1995 -RRB- has proposed automatically augmenting a small set of experimenter-supplied seed collocations -LRB- e.g. manufacturing plant and plant life for two different senses of the noun plant -RRB- into a much larger set of training materials	nn_materials_training prep_of_set_materials amod_set_larger amod_set_much det_set_a nn_plant_noun det_plant_the prep_of_senses_plant amod_senses_different num_senses_two nn_life_plant prep_for_plant_senses conj_and_plant_life amod_plant_manufacturing dep_plant_e.g. dep_collocations_life dep_collocations_plant nn_collocations_seed amod_collocations_experimenter-supplied prep_into_set_set prep_of_set_collocations amod_set_small det_set_a dobj_augmenting_set advmod_augmenting_automatically xcomp_proposed_augmenting aux_proposed_has nsubj_proposed_Yarowsky appos_Yarowsky_1995 ccomp_``_proposed
J98-4002	P95-1026	o	Various corpus-based approaches to word sense disambiguation have been proposed -LRB- Bruce and Wiebe 1994 Charniak 1993 Dagan and Itai 1994 Fujii et al. 1996 Hearst 1991 Karov and Edelman 1996 Kurohashi and Nagao 1994 Li Szpakowicz and Matwin 1995 Ng and Lee 1996 Niwa and Nitta 1994 Sch ~ itze 1992 Uramoto 1994b Yarowsky 1995 -RRB-	num_Yarowsky_1995 nn_1994b_Uramoto dobj_itze_1992 nsubj_itze_~ nn_~_Sch num_Nitta_1994 conj_and_Niwa_Nitta dep_Ng_1996 conj_and_Ng_Lee num_Matwin_1995 nn_Szpakowicz_Li num_Nagao_1994 conj_and_Kurohashi_Nagao num_Karov_1996 conj_and_Karov_Edelman num_Hearst_1991 num_al._1996 nn_al._et nn_al._Fujii num_Itai_1994 conj_and_Dagan_Itai num_Charniak_1993 num_Wiebe_1994 dep_Bruce_Yarowsky conj_and_Bruce_1994b parataxis_Bruce_itze conj_and_Bruce_Nitta conj_and_Bruce_Niwa conj_and_Bruce_Lee conj_and_Bruce_Ng conj_and_Bruce_Matwin conj_and_Bruce_Szpakowicz dep_Bruce_Nagao dep_Bruce_Kurohashi dep_Bruce_Edelman dep_Bruce_Karov dep_Bruce_Hearst dep_Bruce_al. dep_Bruce_Itai dep_Bruce_Dagan conj_and_Bruce_Charniak conj_and_Bruce_Wiebe dep_proposed_1994b dep_proposed_Niwa dep_proposed_Ng dep_proposed_Matwin dep_proposed_Szpakowicz dep_proposed_Charniak dep_proposed_Wiebe dep_proposed_Bruce auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches nn_disambiguation_sense nn_disambiguation_word prep_to_approaches_disambiguation amod_approaches_corpus-based amod_approaches_Various
J98-4002	P95-1026	o	External information such as the discourse or domain dependency of each word sense -LRB- Guthrie et al. 1991 Nasukawa 1993 Yarowsky 1995 -RRB- is expected to lead to system improvement	nn_improvement_system prep_to_lead_improvement aux_lead_to xcomp_expected_lead auxpass_expected_is nsubjpass_expected_information num_Yarowsky_1995 dep_Nasukawa_Yarowsky num_Nasukawa_1993 num_al._1991 nn_al._et dep_Guthrie_Nasukawa advmod_Guthrie_al. nn_sense_word det_sense_each nn_dependency_domain prep_of_discourse_sense conj_or_discourse_dependency det_discourse_the appos_information_Guthrie prep_such_as_information_dependency prep_such_as_information_discourse amod_information_External
J98-4002	P95-1026	o	Iterating between these two 1 Note that these problems are associated with corpus-based approaches in general and have been identified by a number of researchers -LRB- Engelson and Dagan 1996 Lewis and Gale 1994 Uramoto 1994a Yarowsky 1995 -RRB-	num_Yarowsky_1995 nn_1994a_Uramoto num_Lewis_1994 conj_and_Lewis_Gale num_Dagan_1996 dep_Engelson_Yarowsky conj_and_Engelson_1994a conj_and_Engelson_Gale conj_and_Engelson_Lewis conj_and_Engelson_Dagan dep_researchers_1994a dep_researchers_Lewis dep_researchers_Dagan dep_researchers_Engelson prep_of_number_researchers det_number_a agent_identified_number auxpass_identified_been aux_identified_have prep_in_approaches_general amod_approaches_corpus-based prep_with_associated_approaches auxpass_associated_are nsubjpass_associated_problems mark_associated_that det_problems_these num_Note_1 num_Note_two det_Note_these conj_and_Iterating_identified ccomp_Iterating_associated prep_between_Iterating_Note ccomp_``_identified ccomp_``_Iterating
N01-1023	P95-1026	o	1998 Goldman and Zhou 2000 -RRB- that has been used previously to train classifiers in applications like word-sense disambiguation -LRB- Yarowsky 1995 -RRB- document classification -LRB- Blum and Mitchell 1998 -RRB- and named-entity recognition -LRB- Collins and Singer 1999 -RRB- and apply this method to the more complex domain of statistical parsing	amod_parsing_statistical prep_of_domain_parsing amod_domain_complex det_domain_the advmod_complex_more det_method_this prep_to_apply_domain dobj_apply_method amod_Collins_1999 conj_and_Collins_Singer dep_recognition_Singer dep_recognition_Collins nn_recognition_named-entity amod_Blum_1998 conj_and_Blum_Mitchell conj_and_classification_apply conj_and_classification_recognition dep_classification_Mitchell dep_classification_Blum nn_classification_document ccomp_classification_Zhou ccomp_classification_Goldman ccomp_classification_1998 amod_Yarowsky_1995 dep_disambiguation_Yarowsky amod_disambiguation_word-sense prep_like_applications_disambiguation prep_in_train_applications dobj_train_classifiers aux_train_to xcomp_used_train advmod_used_previously auxpass_used_been aux_used_has nsubjpass_used_that rcmod_1998_used dep_1998_2000 conj_and_1998_Zhou conj_and_1998_Goldman
N01-1023	P95-1026	o	Our approach is closely related to previous CoTraining methods -LRB- Yarowsky 1995 Blum and Mitchell 1998 Goldman and Zhou 2000 Collins and Singer 1999 -RRB-	conj_and_Goldman_2000 conj_and_Goldman_Zhou amod_Blum_1999 conj_and_Blum_Singer conj_and_Blum_Collins conj_and_Blum_2000 conj_and_Blum_Zhou conj_and_Blum_Goldman conj_and_Blum_1998 conj_and_Blum_Mitchell dep_Yarowsky_Singer dep_Yarowsky_Collins dep_Yarowsky_Goldman dep_Yarowsky_1998 dep_Yarowsky_Mitchell dep_Yarowsky_Blum appos_Yarowsky_1995 appos_methods_Yarowsky nn_methods_CoTraining amod_methods_previous prep_to_related_methods advmod_related_closely cop_related_is nsubj_related_approach poss_approach_Our ccomp_``_related
N01-1023	P95-1026	o	-LRB- Yarowsky 1995 -RRB- first introduced an iterative method for increasing a small set of seed data used to disambiguate dual word senses by exploiting the constraint that in a segment of discourse only one sense of a word is used	auxpass_used_is nsubjpass_used_sense prep_in_used_segment mark_used_that det_word_a prep_of_sense_word num_sense_one quantmod_one_only prep_of_segment_discourse det_segment_a ccomp_constraint_used det_constraint_the dobj_exploiting_constraint nn_senses_word amod_senses_dual prepc_by_disambiguate_exploiting dobj_disambiguate_senses aux_disambiguate_to xcomp_used_disambiguate vmod_data_used nn_data_seed prep_of_set_data amod_set_small det_set_a dobj_increasing_set prepc_for_method_increasing amod_method_iterative det_method_an dobj_introduced_method advmod_introduced_first nsubj_introduced_Yarowsky amod_Yarowsky_1995
N01-1023	P95-1026	o	Co-Training has been used before in applications like word-sense disambiguation -LRB- Yarowsky 1995 -RRB- web-page classification -LRB- Blum and Mitchell 1998 -RRB- and namedentity identification -LRB- Collins and Singer 1999 -RRB-	amod_Collins_1999 conj_and_Collins_Singer dep_identification_Singer dep_identification_Collins nn_identification_namedentity amod_Blum_1998 conj_and_Blum_Mitchell dep_classification_Mitchell dep_classification_Blum amod_classification_web-page amod_Yarowsky_1995 conj_and_disambiguation_identification conj_and_disambiguation_classification dep_disambiguation_Yarowsky amod_disambiguation_word-sense prep_like_applications_identification prep_like_applications_classification prep_like_applications_disambiguation pobj_in_applications pcomp_before_in prep_used_before auxpass_used_been aux_used_has nsubjpass_used_Co-Training
N01-1023	P95-1026	o	Co-training -LRB- Blum and Mitchell 1998 Yarowsky 1995 -RRB- can be informally described in the following manner # 0F Pick two -LRB- or more -RRB- views of a classification problem	nn_problem_classification det_problem_a prep_of_views_problem num_views_two cc_more_or dep_two_more dobj_Pick_views nsubj_Pick_0F dep_Pick_# amod_manner_following det_manner_the parataxis_described_Pick prep_in_described_manner advmod_described_informally auxpass_described_be aux_described_can nsubjpass_described_Yarowsky nsubjpass_described_1998 nsubjpass_described_Mitchell nsubjpass_described_Blum dep_Yarowsky_1995 conj_and_Blum_Yarowsky conj_and_Blum_1998 conj_and_Blum_Mitchell nn_Blum_Co-training
N03-2025	P95-1026	o	The tag propagation/elimination scheme is adopted from -LSB- Yarowsky 1995 -RSB-	num_Yarowsky_1995 prep_from_adopted_Yarowsky auxpass_adopted_is nsubjpass_adopted_scheme nn_scheme_propagation/elimination nn_scheme_tag det_scheme_The
N03-3004	P95-1026	p	The best example of such an approach is -LRB- Yarowsky 1995 -RRB- who proposes a method that automatically identifies collocations that are indicative of the sense of a word and uses those to iteratively label more examples	amod_examples_more dobj_label_examples advmod_label_iteratively aux_label_to vmod_those_label dobj_uses_those nsubj_uses_who det_word_a prep_of_sense_word det_sense_the prep_of_indicative_sense cop_indicative_are nsubj_indicative_that rcmod_collocations_indicative dobj_identifies_collocations advmod_identifies_automatically nsubj_identifies_that rcmod_method_identifies det_method_a conj_and_proposes_uses dobj_proposes_method nsubj_proposes_who amod_Yarowsky_1995 dep_is_uses dep_is_proposes dep_is_Yarowsky det_approach_an predet_approach_such dep_example_is prep_of_example_approach amod_example_best det_example_The dep_``_example
N03-4012	P95-1026	o	This iterative optimiser derived from a word disambiguation technique -LRB- Yarowsky 1995 -RRB- finds the nearest local maximum in the lexical cooccurrence network from each concept seed	nn_seed_concept det_seed_each nn_network_cooccurrence amod_network_lexical det_network_the prep_in_maximum_network amod_maximum_local amod_maximum_nearest det_maximum_the prep_from_finds_seed dobj_finds_maximum nsubj_finds_optimiser amod_Yarowsky_1995 appos_technique_Yarowsky nn_technique_disambiguation nn_technique_word det_technique_a prep_from_derived_technique vmod_optimiser_derived amod_optimiser_iterative det_optimiser_This
N04-2003	P95-1026	o	Two major research topics in this field are Named Entity Recognition -LRB- NER -RRB- -LRB- N. Wacholder and Choi 1997 Cucerzan and Yarowsky 1999 -RRB- and Word Sense Disambiguation -LRB- WSD -RRB- -LRB- Yarowsky 1995 Wilks and Stevenson 1999 -RRB-	amod_Wilks_1999 conj_and_Wilks_Stevenson dep_Yarowsky_Stevenson dep_Yarowsky_Wilks appos_Yarowsky_1995 dep_Disambiguation_Yarowsky appos_Disambiguation_WSD nn_Disambiguation_Sense nn_Disambiguation_Word dep_Cucerzan_1999 conj_and_Cucerzan_Yarowsky dep_Wacholder_Yarowsky dep_Wacholder_Cucerzan conj_and_Wacholder_1997 conj_and_Wacholder_Choi nn_Wacholder_N. conj_and_Recognition_Disambiguation dep_Recognition_1997 dep_Recognition_Choi dep_Recognition_Wacholder appos_Recognition_NER nn_Recognition_Entity dobj_Named_Disambiguation dobj_Named_Recognition auxpass_Named_are nsubjpass_Named_topics det_field_this prep_in_topics_field nn_topics_research amod_topics_major num_topics_Two ccomp_``_Named
N06-2014	P95-1026	p	To alleviate this effort various semi-supervised learning algorithms such as self-training -LRB- Yarowsky 1995 -RRB- cotraining -LRB- Blum and Mitchell 1998 Goldman and Zhou 2000 -RRB- transductive SVM -LRB- Joachims 1999 -RRB- and many others have been proposed and successfully applied under different assumptions and settings	conj_and_assumptions_settings amod_assumptions_different prep_under_applied_settings prep_under_applied_assumptions advmod_applied_successfully nsubjpass_applied_algorithms conj_and_proposed_applied auxpass_proposed_been aux_proposed_have nsubjpass_proposed_others nsubjpass_proposed_SVM nsubjpass_proposed_algorithms advcl_proposed_alleviate amod_others_many amod_Joachims_1999 dep_SVM_Joachims amod_SVM_transductive nn_Blum_cotraining amod_Yarowsky_1995 dep_self-training_2000 conj_and_self-training_Zhou conj_and_self-training_Goldman conj_and_self-training_1998 conj_and_self-training_Mitchell conj_and_self-training_Blum dep_self-training_Yarowsky conj_and_algorithms_others conj_and_algorithms_SVM prep_such_as_algorithms_Zhou prep_such_as_algorithms_Goldman prep_such_as_algorithms_1998 prep_such_as_algorithms_Mitchell prep_such_as_algorithms_Blum prep_such_as_algorithms_self-training nn_algorithms_learning amod_algorithms_semi-supervised amod_algorithms_various det_effort_this dobj_alleviate_effort aux_alleviate_To ccomp_``_applied ccomp_``_proposed
N07-1025	P95-1026	o	This includes the automatic generation of sense-tagged data using monosemous relatives -LRB- Leacock et al. 1998 Mihalcea and Moldovan 1999 Agirre and Martinez 2004 -RRB- automatically bootstrapped disambiguation patterns -LRB- Yarowsky 1995 Mihalcea 2002 -RRB- parallel texts as a way to point out word senses bearing different translations in a second language -LRB- Diab and Resnik 2002 Ng et al. 2003 Diab 2004 -RRB- and the use of volunteer contributions over the Web -LRB- Chklovski and Mihalcea 2002 -RRB-	dep_Chklovski_2002 conj_and_Chklovski_Mihalcea appos_Web_Mihalcea appos_Web_Chklovski det_Web_the prep_over_contributions_Web nn_contributions_volunteer prep_of_use_contributions det_use_the dep_Diab_2004 num_Ng_2003 nn_Ng_al. nn_Ng_et dep_Diab_Diab conj_and_Diab_Ng conj_and_Diab_2002 conj_and_Diab_Resnik appos_language_Ng appos_language_2002 appos_language_Resnik appos_language_Diab amod_language_second det_language_a amod_translations_different dobj_bearing_translations vmod_senses_bearing nn_senses_word prep_in_point_language dobj_point_senses prt_point_out aux_point_to vmod_way_point det_way_a prep_as_texts_way amod_texts_parallel dep_Mihalcea_2002 dep_Yarowsky_Mihalcea appos_Yarowsky_1995 appos_patterns_texts appos_patterns_Yarowsky nn_patterns_disambiguation conj_and_bootstrapped_use dobj_bootstrapped_patterns advmod_bootstrapped_automatically nsubj_bootstrapped_1999 nsubj_bootstrapped_Moldovan nsubj_bootstrapped_Mihalcea dep_Agirre_2004 conj_and_Agirre_Martinez dep_Mihalcea_Martinez dep_Mihalcea_Agirre conj_and_Mihalcea_1999 conj_and_Mihalcea_Moldovan parataxis_Leacock_use parataxis_Leacock_bootstrapped appos_Leacock_1998 dep_Leacock_al. nn_Leacock_et amod_relatives_monosemous dobj_using_relatives amod_data_sense-tagged dep_generation_Leacock vmod_generation_using prep_of_generation_data amod_generation_automatic det_generation_the dobj_includes_generation nsubj_includes_This
N07-1025	P95-1026	p	This method initially proposed by -LRB- Yarowsky 1995 -RRB- was successfully evaluated in the context of the SENSEVAL framework -LRB- Mihalcea 2002 -RRB-	amod_Mihalcea_2002 dep_framework_Mihalcea nn_framework_SENSEVAL det_framework_the prep_of_context_framework det_context_the prep_in_evaluated_context advmod_evaluated_successfully auxpass_evaluated_was vmod_evaluated_proposed nsubjpass_evaluated_method amod_Yarowsky_1995 dep_by_Yarowsky prep_proposed_by advmod_proposed_initially det_method_This
N07-1025	P95-1026	p	Among the various knowledge-based -LRB- Lesk 1986 Galley and McKeown 2003 Navigli and Velardi 2005 -RRB- and data-driven -LRB- Yarowsky 1995 Ng and Lee 1996 Pedersen 2001 -RRB- word sense disambiguation methods that have been proposed to date supervised systems have been constantly observed as leading to the highest performance	amod_performance_highest det_performance_the prep_to_leading_performance prepc_as_observed_leading advmod_observed_constantly auxpass_observed_been aux_observed_have nsubjpass_observed_systems nsubjpass_observed_methods amod_systems_supervised prep_to_proposed_date auxpass_proposed_been aux_proposed_have nsubjpass_proposed_that rcmod_methods_proposed nn_methods_disambiguation nn_methods_sense nn_methods_word nn_methods_Pedersen nn_methods_1996 nn_methods_Lee nn_methods_Ng dep_Pedersen_2001 conj_and_Ng_Pedersen conj_and_Ng_1996 conj_and_Ng_Lee dep_Yarowsky_observed dep_Yarowsky_1995 dep_data-driven_Yarowsky dep_Galley_2005 conj_and_Galley_Velardi conj_and_Galley_Navigli conj_and_Galley_2003 conj_and_Galley_McKeown dep_Lesk_Velardi dep_Lesk_Navigli dep_Lesk_2003 dep_Lesk_McKeown dep_Lesk_Galley dep_Lesk_1986 conj_and_knowledge-based_data-driven dep_knowledge-based_Lesk amod_knowledge-based_various amod_the_data-driven amod_the_knowledge-based prep_among_``_the
N07-1032	P95-1026	o	A variety of algorithms -LRB- e.g. bootstrapping -LRB- Yarowsky 1995 -RRB- co-training -LRB- Blum and Mitchell 1998 -RRB- alternating structure optimization -LRB- Ando and Zhang 2005 -RRB- etc -RRB-	amod_Ando_2005 conj_and_Ando_Zhang appos_optimization_etc appos_optimization_Zhang appos_optimization_Ando nn_optimization_structure amod_optimization_alternating dep_Blum_1998 conj_and_Blum_Mitchell nn_Blum_co-training dep_Blum_bootstrapping dep_Blum_e.g. amod_Yarowsky_1995 dep_bootstrapping_Yarowsky dep_algorithms_Mitchell dep_algorithms_Blum dep_variety_optimization prep_of_variety_algorithms det_variety_A
N09-1004	P95-1026	o	To overcome the knowledge acquisition bottleneck problem suffered by supervised methods these methods make use of a small annotated corpus as seed data in a bootstrapping process -LRB- Hearst 1991 -RRB- -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Hearst_1991 nn_process_bootstrapping det_process_a dep_data_Yarowsky dep_data_Hearst prep_in_data_process nn_data_seed amod_corpus_annotated amod_corpus_small det_corpus_a prep_as_use_data prep_of_use_corpus dobj_make_use nsubj_make_methods advcl_make_overcome det_methods_these amod_methods_supervised agent_suffered_methods vmod_problem_suffered nn_problem_bottleneck nn_problem_acquisition nn_problem_knowledge det_problem_the dobj_overcome_problem aux_overcome_To
N09-1004	P95-1026	p	Disambiguation of a limited number of words is not hard and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_in_Yarowsky prep_shown_in mark_shown_as nn_accuracy_disambiguation amod_accuracy_high advcl_achieve_shown dobj_achieve_accuracy aux_achieve_to xcomp_hand-crafted_achieve nsubj_hand-crafted_information conj_and_collected_hand-crafted advmod_collected_carefully auxpass_collected_be aux_collected_can nsubjpass_collected_information nn_information_context amod_information_necessary conj_and_hard_hand-crafted conj_and_hard_collected neg_hard_not cop_hard_is nsubj_hard_Disambiguation prep_of_number_words amod_number_limited det_number_a prep_of_Disambiguation_number
P01-1005	P95-1026	o	Numerous approaches have been explored for exploiting situations where some amount of annotated data is available and a much larger amount of data exists unannotated e.g. Marialdo 's HMM part-of-speech tagger training -LRB- 1994 -RRB- Charniak 's parser retraining experiment -LRB- 1996 -RRB- Yarowsky 's seeds for word sense disambiguation -LRB- 1995 -RRB- and Nigam et al 's -LRB- 1998 -RRB- topic classifier learned in part from unlabelled documents	amod_documents_unlabelled prep_from_learned_documents prep_in_learned_part nsubj_learned_Charniak nn_classifier_topic nn_classifier_al dep_al_1998 possessive_al_'s nn_al_et nn_al_Nigam conj_and_disambiguation_classifier appos_disambiguation_1995 nn_disambiguation_sense nn_disambiguation_word prep_for_seeds_classifier prep_for_seeds_disambiguation poss_seeds_Yarowsky appos_experiment_1996 dobj_retraining_experiment vmod_parser_retraining appos_Charniak_seeds dep_Charniak_parser possessive_Charniak_'s appos_training_1994 nn_training_tagger nn_training_part-of-speech nn_training_HMM poss_training_Marialdo pobj_e.g._training rcmod_unannotated_learned prep_unannotated_e.g. dobj_exists_unannotated nsubj_exists_amount prep_of_amount_data amod_amount_larger amod_amount_much det_amount_a conj_and_available_exists cop_available_is nsubj_available_amount advmod_available_where amod_data_annotated prep_of_amount_data det_amount_some rcmod_situations_exists rcmod_situations_available dobj_exploiting_situations prepc_for_explored_exploiting auxpass_explored_been aux_explored_have nsubjpass_explored_approaches amod_approaches_Numerous
P01-1005	P95-1026	o	The more recent set of techniques includes mult iplicative weightupdate algorithms -LRB- Golding and Roth 1998 -RRB- latent semantic analysis -LRB- Jones and Martin 1997 -RRB- transformation-based learning -LRB- Mangu and Brill 1997 -RRB- differential grammars -LRB- Powers 1997 -RRB- decision lists -LRB- Yarowsky 1994 -RRB- and a variety of Bayesian classifiers -LRB- Gale et al. 1993 Golding 1995 Golding and Schabes 1996 -RRB-	appos_Golding_1996 conj_and_Golding_Schabes conj_and_Golding_Golding conj_and_Golding_1995 appos_Gale_Schabes appos_Gale_Golding appos_Gale_1995 appos_Gale_Golding amod_Gale_1993 dep_Gale_al. nn_Gale_et amod_classifiers_Bayesian prep_of_variety_classifiers det_variety_a dep_Yarowsky_1994 appos_lists_Yarowsky nn_lists_decision dep_Powers_1997 appos_grammars_Powers amod_grammars_differential dep_Mangu_1997 conj_and_Mangu_Brill appos_learning_Brill appos_learning_Mangu amod_learning_transformation-based amod_Jones_1997 conj_and_Jones_Martin dep_analysis_Martin dep_analysis_Jones amod_analysis_semantic amod_analysis_latent dep_Golding_1998 conj_and_Golding_Roth conj_and_algorithms_variety conj_and_algorithms_lists conj_and_algorithms_grammars conj_and_algorithms_learning conj_and_algorithms_analysis appos_algorithms_Roth appos_algorithms_Golding nn_algorithms_weightupdate amod_algorithms_iplicative amod_algorithms_mult dep_includes_Gale dobj_includes_variety dobj_includes_lists dobj_includes_grammars dobj_includes_learning dobj_includes_analysis dobj_includes_algorithms nsubj_includes_set prep_of_set_techniques amod_set_recent det_set_The advmod_recent_more
P01-1008	P95-1026	o	This method of co-training has been previously applied to a variety of natural language tasks such as word sense disambiguation -LRB- Yarowsky 1995 -RRB- lexicon construction for information extraction -LRB- Riloff and Jones 1999 -RRB- and named entity classification -LRB- Collins and Singer 1999 -RRB-	amod_Collins_1999 conj_and_Collins_Singer dep_classification_Singer dep_classification_Collins nn_classification_entity amod_classification_named dep_Riloff_1999 conj_and_Riloff_Jones dep_extraction_Jones dep_extraction_Riloff nn_extraction_information prep_for_construction_extraction nn_construction_lexicon amod_Yarowsky_1995 conj_and_disambiguation_classification conj_and_disambiguation_construction dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word prep_such_as_tasks_classification prep_such_as_tasks_construction prep_such_as_tasks_disambiguation nn_tasks_language amod_tasks_natural prep_of_variety_tasks det_variety_a prep_to_applied_variety advmod_applied_previously auxpass_applied_been aux_applied_has nsubjpass_applied_method prep_of_method_co-training det_method_This
P01-1026	P95-1026	o	In addition since word senses are often associated with domains -LRB- Yarowsky 1995 -RRB- word senses can be consequently distinguished by way of determining the domain of each description	det_description_each prep_of_domain_description det_domain_the dobj_determining_domain prepc_of_way_determining agent_distinguished_way advmod_distinguished_consequently auxpass_distinguished_be aux_distinguished_can nsubjpass_distinguished_senses advcl_distinguished_associated prep_in_distinguished_addition nn_senses_word dep_Yarowsky_1995 dep_domains_Yarowsky prep_with_associated_domains advmod_associated_often auxpass_associated_are nsubjpass_associated_senses mark_associated_since nn_senses_word
P02-1044	P95-1026	o	6.2 Experiment 2 Yarowskys Words We also conducted translation on seven of the twelve English words studied in -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_in_Yarowsky prep_studied_in vmod_words_studied nn_words_English num_words_twelve det_words_the prep_of_seven_words prep_on_conducted_seven dobj_conducted_translation advmod_conducted_also nsubj_conducted_We rcmod_Words_conducted nn_Words_Yarowskys dep_Experiment_Words num_Experiment_2 num_Experiment_6.2 dep_``_Experiment
P02-1044	P95-1026	o	Note that the results of MB-D here can not be directly compared with those in -LRB- Yarowsky 1995 -RRB- mainly because the data used are different	cop_different_are nsubj_different_data mark_different_because advmod_different_mainly vmod_data_used det_data_the amod_Yarowsky_1995 dep_in_Yarowsky prep_those_in advcl_compared_different prep_with_compared_those advmod_compared_directly auxpass_compared_be neg_compared_not aux_compared_can nsubjpass_compared_results mark_compared_that advmod_MB-D_here prep_of_results_MB-D det_results_the ccomp_Note_compared
P02-1044	P95-1026	o	Yarowsky -LRB- 1995 -RRB- proposes a method for word sense disambiguation which is based on Monolingual Bootstrapping	nn_Bootstrapping_Monolingual prep_on_based_Bootstrapping auxpass_based_is nsubjpass_based_which rcmod_disambiguation_based nn_disambiguation_sense nn_disambiguation_word prep_for_method_disambiguation det_method_a dobj_proposes_method nsubj_proposes_Yarowsky appos_Yarowsky_1995
P02-1044	P95-1026	o	Yarowsky 1995 -RRB- after using an ensemble of NBCs	prep_of_ensemble_NBCs det_ensemble_an dobj_using_ensemble prepc_after_Yarowsky_using num_Yarowsky_1995
P02-1044	P95-1026	o	This implementation is exactly the one proposed in -LRB- Yarowsky 1995 -RRB- and we will denote it as MB-D hereafter	advmod_MB-D_hereafter prep_as_denote_MB-D dobj_denote_it aux_denote_will nsubj_denote_we conj_and_Yarowsky_denote num_Yarowsky_1995 prep_in_proposed_denote prep_in_proposed_Yarowsky vmod_one_proposed det_one_the pobj_exactly_one prep_is_exactly amod_implementation_is det_implementation_This dep_``_implementation
P02-1044	P95-1026	o	1 Yarowsky -LRB- 1995 -RRB- proposes a method for word sense -LRB- translation -RRB- disambiguation that is based on a bootstrapping technique which we refer to here as Monolingual Bootstrapping -LRB- MB -RRB-	appos_Bootstrapping_MB amod_Bootstrapping_Monolingual pobj_to_here prep_as_refer_Bootstrapping prep_refer_to nsubj_refer_we dobj_refer_which rcmod_technique_refer nn_technique_bootstrapping det_technique_a prep_on_based_technique auxpass_based_is nsubjpass_based_that rcmod_disambiguation_based nn_disambiguation_sense appos_sense_translation nn_sense_word prep_for_method_disambiguation det_method_a dobj_proposes_method nsubj_proposes_Yarowsky appos_Yarowsky_1995 num_Yarowsky_1 ccomp_``_proposes
P02-1044	P95-1026	o	This way of creating classified data is similar to that in -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 prep_in_that_Yarowsky prep_to_similar_that cop_similar_is nsubj_similar_way amod_data_classified dobj_creating_data prepc_of_way_creating det_way_This
P02-1046	P95-1026	o	Then the initial precision is 1 -LRB- Yarowsky 1995 -RRB- citing -LRB- Yarowsky 1994 -RRB- actually uses a superficially different score that is however a monotone transform of precision hence equivalent to precision since it is used only for sorting	prepc_for_used_sorting advmod_used_only auxpass_used_is nsubjpass_used_it mark_used_since prep_to_equivalent_precision advmod_equivalent_hence prep_of_transform_precision nsubj_transform_monotone advmod_transform_however aux_transform_is nsubj_transform_that det_monotone_a rcmod_score_transform amod_score_different advmod_score_superficially det_score_a dobj_uses_score advmod_uses_actually advcl_Yarowsky_used amod_Yarowsky_equivalent dep_Yarowsky_uses dep_Yarowsky_1994 dep_citing_Yarowsky amod_Yarowsky_1995 appos_1_Yarowsky dep_is_citing nsubj_is_1 dep_precision_is amod_precision_initial det_precision_the dep_Then_precision
P02-1046	P95-1026	o	Current work has been spurred by two papers -LRB- Yarowsky 1995 -RRB- and -LRB- Blum and Mitchell 1998 -RRB-	amod_Blum_1998 conj_and_Blum_Mitchell dep_Yarowsky_1995 conj_and_papers_Mitchell conj_and_papers_Blum appos_papers_Yarowsky num_papers_two agent_spurred_Blum agent_spurred_papers auxpass_spurred_been aux_spurred_has nsubjpass_spurred_work amod_work_Current
P02-1064	P95-1026	p	In order to overcome this some unsupervised learning methods and minimally-supervised methods e.g. -LRB- Yarowsky 1995 Yarowsky and Wicentowski 2000 -RRB- have been proposed	auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Yarowsky advmod_proposed_e.g. dep_Yarowsky_2000 conj_and_Yarowsky_Wicentowski dep_Yarowsky_Wicentowski dep_Yarowsky_Yarowsky dep_Yarowsky_1995 amod_methods_minimally-supervised dep_methods_proposed conj_and_methods_methods nn_methods_learning amod_methods_unsupervised det_methods_some dep_,_methods dep_,_methods dobj_overcome_this aux_overcome_to dep_overcome_order mark_overcome_In advcl_``_overcome
P02-1064	P95-1026	o	However few papers in the field of computational linguistics have focused on this approach -LRB- Dagan and Engelson 1995 Thompson et al. 1999 Ngai and Yarowsky 2000 Hwa 2000 Banko and Brill 2001 -RRB-	dep_Banko_2001 conj_and_Banko_Brill num_Hwa_2000 dep_Ngai_Brill dep_Ngai_Banko conj_and_Ngai_Hwa conj_and_Ngai_2000 conj_and_Ngai_Yarowsky num_Thompson_1999 nn_Thompson_al. nn_Thompson_et conj_and_Dagan_Hwa conj_and_Dagan_2000 conj_and_Dagan_Yarowsky conj_and_Dagan_Ngai conj_and_Dagan_Thompson conj_and_Dagan_1995 conj_and_Dagan_Engelson dep_approach_Ngai dep_approach_Thompson dep_approach_1995 dep_approach_Engelson dep_approach_Dagan det_approach_this prep_on_focused_approach aux_focused_have nsubj_focused_papers advmod_focused_However amod_linguistics_computational prep_of_field_linguistics det_field_the prep_in_papers_field amod_papers_few ccomp_``_focused
P03-1008	P95-1026	o	All features encountered in the training data are ranked in the DL -LRB- best evidence first -RRB- according to the following loglikelihood ratio -LRB- Yarowsky 1995 -RRB- Log Pr -LRB- reading i jfeature k -RRB- P j6 = i Pr -LRB- reading j jfeature k -RRB- We estimated probabilities via maximum likelihood adopting a simple smoothing method -LRB- Martinez and Agirre 2000 -RRB- 0.1 is added to both the denominator and numerator	conj_and_denominator_numerator dep_denominator_the preconj_denominator_both prep_to_added_numerator prep_to_added_denominator auxpass_added_is nsubjpass_added_0.1 amod_Martinez_2000 conj_and_Martinez_Agirre dep_method_Agirre dep_method_Martinez nn_method_smoothing amod_method_simple det_method_a dobj_adopting_method nn_likelihood_maximum dobj_estimated_probabilities nsubj_estimated_We nn_k_jfeature nn_k_j nn_k_reading appos_Pr_k nn_Pr_i dep_=_Pr dep_j6_estimated amod_j6_= prep_via_P_likelihood dobj_P_j6 nn_k_jfeature nn_k_i dep_reading_added vmod_reading_adopting dep_reading_P advmod_reading_k dep_Pr_reading dobj_Log_Pr amod_Yarowsky_1995 dep_ratio_Yarowsky nn_ratio_loglikelihood amod_ratio_following det_ratio_the advmod_evidence_first amod_evidence_best appos_DL_evidence det_DL_the parataxis_ranked_Log pobj_ranked_ratio prepc_according_to_ranked_to prep_in_ranked_DL auxpass_ranked_are nsubjpass_ranked_features nn_data_training det_data_the prep_in_encountered_data vmod_features_encountered det_features_All
P03-1042	P95-1026	o	The word sense disambiguation method proposed in Yarowsky -LRB- 1995 -RRB- can also be viewed as a kind of co-training	prep_of_kind_co-training det_kind_a prep_as_viewed_kind auxpass_viewed_be advmod_viewed_also aux_viewed_can nsubjpass_viewed_method appos_Yarowsky_1995 prep_in_proposed_Yarowsky vmod_method_proposed nn_method_disambiguation nn_method_sense nn_method_word det_method_The
P03-1043	P95-1026	o	The tag propagation/elimination scheme is adopted from -LRB- Yarowsky 1995 -RRB-	num_Yarowsky_1995 dep_from_Yarowsky prep_adopted_from auxpass_adopted_is nsubjpass_adopted_scheme nn_scheme_propagation/elimination nn_scheme_tag det_scheme_The
P03-1044	P95-1026	o	One example is the algorithm for word sense disambiguation in -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_in_Yarowsky nn_disambiguation_sense nn_disambiguation_word prep_algorithm_in prep_for_algorithm_disambiguation det_algorithm_the cop_algorithm_is nsubj_algorithm_example num_example_One
P04-1037	P95-1026	n	Supervised approaches which make use of a small hand-labeled training set -LRB- Bruce and Wiebe 1994 Yarowsky 1993 -RRB- typically outperform unsupervised approaches -LRB- Agirre et al. 2000 Litkowski 2000 Lin 2000 Resnik 1997 Yarowsky 1992 Yarowsky 1995 -RRB- but tend to be tuned to a speci c corpus and are constrained by scarcity of labeled data	amod_data_labeled prep_of_scarcity_data agent_constrained_scarcity auxpass_constrained_are nn_corpus_c nn_corpus_speci det_corpus_a prep_to_tuned_corpus auxpass_tuned_be aux_tuned_to xcomp_tend_tuned dep_Yarowsky_1995 num_Yarowsky_1992 num_Resnik_1997 conj_but_Lin_tend conj_but_Lin_Yarowsky conj_but_Lin_Yarowsky conj_but_Lin_Resnik num_Lin_2000 conj_and_Litkowski_constrained dep_Litkowski_tend dep_Litkowski_Yarowsky dep_Litkowski_Yarowsky dep_Litkowski_Resnik dep_Litkowski_Lin appos_Litkowski_2000 dep_Agirre_constrained dep_Agirre_Litkowski appos_Agirre_2000 dep_Agirre_al. nn_Agirre_et dep_approaches_Agirre amod_approaches_unsupervised dobj_outperform_approaches advmod_outperform_typically nsubj_outperform_approaches num_Yarowsky_1993 dep_Bruce_Yarowsky conj_and_Bruce_1994 conj_and_Bruce_Wiebe appos_set_1994 appos_set_Wiebe appos_set_Bruce nn_set_training amod_set_hand-labeled amod_set_small det_set_a prep_of_use_set dobj_make_use nsubj_make_which rcmod_approaches_make amod_approaches_Supervised
P04-1039	P95-1026	o	Two more recent investigations are by Yarowsky -LRB- Yarowsky 1995 -RRB- and later Mihalcea -LRB- Mihalcea 2002 -RRB-	amod_Mihalcea_2002 appos_Mihalcea_Mihalcea dep_later_Mihalcea amod_Yarowsky_1995 advmod_are_later cc_are_and dep_are_Yarowsky prep_by_are_Yarowsky nsubj_are_investigations amod_investigations_recent amod_investigations_more num_investigations_Two ccomp_``_are
P04-1062	P95-1026	p	Some tasks can thrive on a nearly pure diet of unlabeled data -LRB- Yarowsky 1995 Collins and Singer 1999 Cucerzan and Yarowsky 2003 -RRB-	dep_Cucerzan_2003 conj_and_Cucerzan_Yarowsky num_Collins_1999 conj_and_Collins_Singer dep_Yarowsky_Yarowsky dep_Yarowsky_Cucerzan conj_Yarowsky_Singer conj_Yarowsky_Collins conj_Yarowsky_1995 dep_data_Yarowsky amod_data_unlabeled prep_of_diet_data amod_diet_pure det_diet_a advmod_pure_nearly prep_on_thrive_diet aux_thrive_can nsubj_thrive_tasks det_tasks_Some ccomp_``_thrive
P04-3026	P95-1026	o	Starting from the list of 12 ambiguous words provided by Yarowsky -LRB- 1995 -RRB- which is shown in table 2 we created a concordance for each word with the lines in the concordances each relating to a context window of 20 words	num_words_20 prep_of_window_words nn_window_context det_window_a prep_to_relating_window vmod_each_relating det_concordances_the prep_in_lines_concordances det_lines_the det_word_each prep_for_concordance_word det_concordance_a tmod_created_each prep_with_created_lines dobj_created_concordance nsubj_created_we vmod_created_Starting num_table_2 prep_in_shown_table auxpass_shown_is nsubjpass_shown_which rcmod_Yarowsky_shown appos_Yarowsky_1995 agent_provided_Yarowsky vmod_words_provided amod_words_ambiguous num_words_12 prep_of_list_words det_list_the prep_from_Starting_list
P04-3026	P95-1026	o	In an attempt to provide a quantitative evaluation of our results for each of the 12 ambiguous words shown in table 1 we manually assigned the top 30 first-order associations to one of the two senses provided by Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 agent_provided_Yarowsky vmod_senses_provided num_senses_two det_senses_the prep_of_one_senses nn_associations_first-order num_associations_30 amod_associations_top det_associations_the prep_to_assigned_one dobj_assigned_associations advmod_assigned_manually nsubj_assigned_we prep_for_assigned_each prep_in_assigned_attempt num_table_1 prep_in_shown_table vmod_words_shown amod_words_ambiguous num_words_12 det_words_the prep_of_each_words poss_results_our prep_of_evaluation_results amod_evaluation_quantitative det_evaluation_a dobj_provide_evaluation aux_provide_to vmod_attempt_provide det_attempt_an
P05-1001	P95-1026	o	A number of bootstrapping methods have been proposed for NLP tasks -LRB- e.g. Yarowsky -LRB- 1995 -RRB- Collins and Singer -LRB- 1999 -RRB- Riloff and Jones -LRB- 1999 -RRB- -RRB-	appos_Jones_1999 appos_Singer_1999 conj_and_Collins_Jones conj_and_Collins_Riloff conj_and_Collins_Singer dep_Yarowsky_Jones dep_Yarowsky_Riloff dep_Yarowsky_Singer dep_Yarowsky_Collins dep_Yarowsky_1995 nn_Yarowsky_e.g. nn_tasks_NLP dep_proposed_Yarowsky prep_for_proposed_tasks auxpass_proposed_been aux_proposed_have nsubjpass_proposed_number amod_methods_bootstrapping prep_of_number_methods det_number_A
P05-1044	P95-1026	n	Unlike well-known bootstrapping approaches -LRB- Yarowsky 1995 -RRB- EM and CE have the possible advantage of maintaining posteriors over hidden labels -LRB- or structure -RRB- throughout learning bootstrapping either chooses for each example a single label or remains completely agnostic	advmod_agnostic_completely acomp_remains_agnostic amod_label_single det_label_a det_example_each conj_or_chooses_remains dobj_chooses_label prep_for_chooses_example preconj_chooses_either dep_bootstrapping_remains dep_bootstrapping_chooses prep_unlike_bootstrapping_approaches cc_structure_or dep_labels_structure amod_labels_hidden prep_throughout_maintaining_learning prep_over_maintaining_labels dobj_maintaining_posteriors prepc_of_advantage_maintaining amod_advantage_possible det_advantage_the dobj_have_advantage conj_and_EM_CE amod_Yarowsky_1995 dep_approaches_have dep_approaches_CE dep_approaches_EM dep_approaches_Yarowsky nn_approaches_bootstrapping amod_approaches_well-known
P05-1049	P95-1026	o	They roughly fall into three categories according to what is used for supervision in learning process -LRB- 1 -RRB- using external resources e.g. thesaurus or lexicons to disambiguate word senses or automatically generate sense-tagged corpus -LRB- Lesk 1986 Lin 1997 McCarthy et al. 2004 Seo et al. 2004 Yarowsky 1992 -RRB- -LRB- 2 -RRB- exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora -LRB- e.g. parallel corpora or untagged monolingual corpora in two languages -RRB- -LRB- Brown et al. 1991 Dagan and Itai 1994 Diab and Resnik 2002 Li and Li 2004 Ng et al. 2003 -RRB- -LRB- 3 -RRB- bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data -LRB- Hearst 1991 Karov and Edelman 1998 Mihalcea 2004 Park et al. 2000 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 num_Park_2000 nn_Park_al. nn_Park_et dep_Mihalcea_Yarowsky conj_Mihalcea_Park num_Mihalcea_2004 conj_and_Karov_Mihalcea conj_and_Karov_1998 conj_and_Karov_Edelman dep_Hearst_Mihalcea dep_Hearst_1998 dep_Hearst_Edelman dep_Hearst_Karov amod_Hearst_1991 dep_data_Hearst amod_data_sense-tagged amod_data_large prep_of_acquisition_data prep_of_bottleneck_acquisition det_bottleneck_the dobj_overcome_bottleneck aux_overcome_to vmod_examples_overcome nn_examples_seed amod_examples_sensetagged amod_examples_bootstrapping dep_examples_3 dep_examples_Lin num_Ng_2003 nn_Ng_al. nn_Ng_et num_Li_2004 conj_and_Li_Li conj_and_Diab_Resnik conj_and_Dagan_1994 conj_and_Dagan_Itai dep_Brown_Ng dep_Brown_Li dep_Brown_Li amod_Brown_2002 dep_Brown_Resnik dep_Brown_Diab dep_Brown_1994 dep_Brown_Itai dep_Brown_Dagan amod_Brown_1991 dep_Brown_al. nn_Brown_et num_languages_two amod_corpora_monolingual amod_corpora_untagged amod_corpora_corpora conj_or_corpora_untagged prep_in_parallel_languages dobj_parallel_corpora dep_parallel_e.g. appos_corpora_Brown dep_corpora_parallel amod_corpora_bilingual prep_of_use_corpora det_use_the amod_languages_different prep_in_mapping_languages prep_to_mapping_senses prep_of_mapping_words prep_between_differences_mapping det_differences_the prep_by_exploiting_use dobj_exploiting_differences dep_Yarowsky_1992 num_Seo_2004 nn_Seo_al. nn_Seo_et num_McCarthy_2004 nn_McCarthy_al. nn_McCarthy_et dep_Lin_exploiting dep_Lin_2 dep_Lin_Yarowsky conj_Lin_Seo conj_Lin_McCarthy num_Lin_1997 dep_Lesk_examples appos_Lesk_1986 amod_corpus_sense-tagged dobj_generate_corpus nn_senses_word dobj_disambiguate_senses aux_disambiguate_to dep_thesaurus_Lesk conj_or_thesaurus_generate conj_or_thesaurus_automatically conj_or_thesaurus_disambiguate conj_or_thesaurus_lexicons dep_thesaurus_e.g. dep_resources_generate dep_resources_automatically dep_resources_disambiguate dep_resources_lexicons dep_resources_thesaurus amod_resources_external dobj_using_resources dep_1_using amod_process_learning prep_in_used_process prep_for_used_supervision auxpass_used_is nsubjpass_used_what num_categories_three parataxis_fall_1 pcomp_fall_used prepc_according_to_fall_to prep_into_fall_categories advmod_fall_roughly nsubj_fall_They
P05-1049	P95-1026	o	It has been shown that one sense per discourse property can improve the performance of bootstrapping algorithm -LRB- Li and Li 2004 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Li_Yarowsky amod_Li_2004 conj_and_Li_Li appos_algorithm_Li appos_algorithm_Li amod_algorithm_bootstrapping prep_of_performance_algorithm det_performance_the dobj_improve_performance aux_improve_can nsubj_improve_sense mark_improve_that nn_property_discourse prep_per_sense_property num_sense_one ccomp_shown_improve auxpass_shown_been aux_shown_has nsubjpass_shown_It
P05-1049	P95-1026	p	3.2 Comparison between SVM Bootstrapping and LP For WSD SVM is one of the state of the art supervised learning algorithms -LRB- Mihalcea et al. 2004 -RRB- while bootstrapping is one of the state of the art semi-supervised learning algorithms -LRB- Li and Li 2004 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Li_Yarowsky num_Li_2004 conj_and_Li_Li appos_algorithms_Li appos_algorithms_Li nn_algorithms_learning amod_algorithms_semi-supervised nn_algorithms_art det_algorithms_the prep_of_state_algorithms det_state_the prep_of_one_state cop_one_is nsubj_one_bootstrapping mark_one_while amod_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et dep_learning_Mihalcea dobj_learning_algorithms advcl_supervised_one xcomp_supervised_learning nsubj_supervised_one det_art_the prep_of_state_art det_state_the prep_of_one_state cop_one_is nsubj_one_SVM dep_one_Comparison prep_for_SVM_WSD conj_and_SVM_LP conj_and_SVM_Bootstrapping prep_between_Comparison_LP prep_between_Comparison_Bootstrapping prep_between_Comparison_SVM num_Comparison_3.2
P05-1049	P95-1026	o	Many methods have been proposed to deal with this problem including supervised learning algorithms -LRB- Leacock et al. 1998 -RRB- semi-supervised learning algorithms -LRB- Yarowsky 1995 -RRB- and unsupervised learning algorithms -LRB- Schutze 1998 -RRB-	amod_Schutze_1998 dep_algorithms_Schutze nn_algorithms_learning amod_algorithms_unsupervised dep_Yarowsky_1995 appos_algorithms_Yarowsky nn_algorithms_learning amod_algorithms_semi-supervised amod_Leacock_1998 dep_Leacock_al. nn_Leacock_et conj_and_algorithms_algorithms conj_and_algorithms_algorithms dep_algorithms_Leacock nn_algorithms_learning dep_supervised_algorithms dep_supervised_algorithms dep_supervised_algorithms det_problem_this prepc_including_deal_supervised prep_with_deal_problem aux_deal_to xcomp_proposed_deal auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_Many ccomp_``_proposed
P06-1027	P95-1026	o	To compare the performance of different taggers learned by different mechanisms one can measure the precision recall and F-measure given by precision = # correct predictions # predicted gene mentions recall = # correct predictions # true gene mentions F-measure = a96a15a14 precision a14 recallprecision a44 recall In our evaluation we compared the proposed semi-supervised learning approach to the state of the art supervised CRF of McDonald and Pereira -LRB- 2005 -RRB- and also to self-training -LRB- Celeux and Govaert 1992 Yarowsky 1995 -RRB- using the same feature set as -LRB- McDonald and Pereira 2005 -RRB-	num_Pereira_2005 conj_and_McDonald_Pereira dep_as_Pereira dep_as_McDonald prep_set_as vmod_feature_set amod_feature_same det_feature_the dobj_using_feature num_Yarowsky_1995 num_Govaert_1992 dep_Celeux_Yarowsky conj_and_Celeux_Govaert appos_self-training_Govaert appos_self-training_Celeux pobj_to_self-training advmod_to_also appos_Pereira_2005 conj_and_McDonald_Pereira prep_of_CRF_Pereira prep_of_CRF_McDonald dobj_supervised_CRF nsubj_supervised_approach det_art_the prep_of_state_art det_state_the prep_to_approach_state amod_approach_learning amod_approach_semi-supervised amod_approach_proposed det_approach_the ccomp_compared_supervised nsubj_compared_we dep_compared_correct dep_compared_# dep_compared_= dep_compared_recall poss_evaluation_our nn_recall_a44 nn_recall_recallprecision nn_recall_a14 nn_recall_precision nn_recall_a96a15a14 amod_recall_= nn_recall_F-measure prep_in_mentions_evaluation dobj_mentions_recall nsubj_mentions_gene amod_gene_true dep_gene_# rcmod_predictions_mentions dep_correct_predictions xcomp_mentions_compared nsubj_mentions_gene amod_gene_predicted dep_gene_# conj_and_predictions_to rcmod_predictions_mentions dep_correct_to dep_correct_predictions amod_#_correct dobj_=_# dep_given_= prep_by_given_precision conj_and_precision_F-measure conj_and_precision_recall det_precision_the xcomp_measure_using dep_measure_given dobj_measure_F-measure dobj_measure_recall dobj_measure_precision aux_measure_can nsubj_measure_one advcl_measure_compare amod_mechanisms_different agent_learned_mechanisms vmod_taggers_learned amod_taggers_different prep_of_performance_taggers det_performance_the dobj_compare_performance aux_compare_To
P06-1027	P95-1026	o	Many approaches have been proposed for semisupervised learning in the past including generative models -LRB- Castelli and Cover 1996 Cohen and Cozman 2006 Nigam et al. 2000 -RRB- self-learning -LRB- Celeux and Govaert 1992 Yarowsky 1995 -RRB- cotraining -LRB- Blum and Mitchell 1998 -RRB- informationtheoretic regularization -LRB- Corduneanu and Jaakkola 2006 Grandvalet and Bengio 2004 -RRB- and graphbased transductive methods -LRB- Zhou et al. 2004 Zhou et al. 2005 Zhu et al. 2003 -RRB-	dep_al._2003 nn_al._et nn_al._Zhu dep_al._2005 nn_al._et nn_al._Zhou dep_Zhou_al. dep_Zhou_al. dep_Zhou_2004 dep_Zhou_al. nn_Zhou_et amod_methods_transductive amod_methods_graphbased num_Bengio_2004 conj_and_Grandvalet_Bengio amod_2006_Jaakkola dep_Corduneanu_Bengio dep_Corduneanu_Grandvalet conj_and_Corduneanu_2006 appos_regularization_2006 appos_regularization_Corduneanu amod_regularization_informationtheoretic num_Mitchell_1998 conj_and_Blum_Mitchell dep_cotraining_Mitchell dep_cotraining_Blum num_Yarowsky_1995 num_Govaert_1992 dep_Celeux_Yarowsky conj_and_Celeux_Govaert appos_self-learning_Govaert appos_self-learning_Celeux dep_al._2000 nn_al._et nn_al._Nigam num_Cozman_2006 num_Cover_1996 dep_Castelli_al. conj_and_Castelli_Cozman conj_and_Castelli_Cohen conj_and_Castelli_Cover dep_models_Zhou conj_and_models_methods conj_and_models_regularization conj_and_models_cotraining appos_models_self-learning appos_models_Cozman appos_models_Cohen appos_models_Cover appos_models_Castelli amod_models_generative det_past_the amod_learning_semisupervised prep_including_proposed_methods prep_including_proposed_regularization prep_including_proposed_cotraining prep_including_proposed_models prep_in_proposed_past prep_for_proposed_learning auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches amod_approaches_Many
P06-1027	P95-1026	p	5.1 Comparison to self-training For completeness we also compared our results to the self-learning algorithm which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation -LRB- Abney 2004 Yarowsky 1995 -RRB-	num_Yarowsky_1995 dep_Abney_Yarowsky num_Abney_2004 appos_disambiguation_Abney nn_disambiguation_sense nn_disambiguation_word prep_of_work_Yarowsky det_work_the prep_in_popularized_disambiguation prep_by_popularized_work advmod_popularized_originally nsubjpass_popularized_which nn_processing_language amod_processing_natural prep_in_bootstrapping_processing conj_and_referred_popularized pobj_referred_bootstrapping prepc_as_to_referred_as auxpass_referred_been advmod_referred_commonly aux_referred_has nsubjpass_referred_which rcmod_algorithm_popularized rcmod_algorithm_referred amod_algorithm_self-learning det_algorithm_the prep_to_results_algorithm poss_results_our dobj_compared_results advmod_compared_also nsubj_compared_we tmod_compared_Comparison prep_for_Comparison_completeness prep_to_Comparison_self-training num_Comparison_5.1
P06-1031	P95-1026	o	Equation -LRB- 3 -RRB- reads If the target noun appears then it is distinguished by the majority The log-likelihood ratio -LRB- Yarowsky 1995 -RRB- decides in which order rules are applied to the target noun in novel context	amod_context_novel nn_noun_target det_noun_the prep_in_applied_context prep_to_applied_noun auxpass_applied_are nsubjpass_applied_rules prep_in_applied_which nn_rules_order ccomp_decides_applied nsubj_decides_ratio amod_Yarowsky_1995 dep_ratio_Yarowsky amod_ratio_log-likelihood det_ratio_The det_majority_the agent_distinguished_majority auxpass_distinguished_is nsubjpass_distinguished_it advmod_distinguished_then advcl_distinguished_appears nsubj_appears_noun mark_appears_If nn_noun_target det_noun_the parataxis_reads_decides ccomp_reads_distinguished nsubj_reads_Equation appos_Equation_3
P06-1056	P95-1026	o	Determining the sense of an ambiguous word using bootstrapping and texts from a different language was done by Yarowsky -LRB- 1995 -RRB- Hearst -LRB- 1991 -RRB- Diab -LRB- 2002 -RRB- and Li and Li -LRB- 2004 -RRB-	dep_Li_2004 conj_and_Li_Li appos_Diab_2002 appos_Hearst_1991 conj_and_Yarowsky_Li conj_and_Yarowsky_Li conj_and_Yarowsky_Diab conj_and_Yarowsky_Hearst appos_Yarowsky_1995 agent_done_Li agent_done_Diab agent_done_Hearst agent_done_Yarowsky auxpass_done_was nsubjpass_done_using amod_language_different det_language_a conj_and_bootstrapping_texts prep_from_using_language dobj_using_texts dobj_using_bootstrapping amod_word_ambiguous det_word_an prep_of_sense_word det_sense_the parataxis_Determining_done dobj_Determining_sense ccomp_``_Determining
P06-1056	P95-1026	o	Yarowsky -LRB- 1995 -RRB- has used a few seeds and untagged sentences in a bootstrapping algorithm based on decision lists	nn_lists_decision pobj_algorithm_lists prepc_based_on_algorithm_on amod_algorithm_bootstrapping det_algorithm_a amod_sentences_untagged conj_and_seeds_sentences amod_seeds_few det_seeds_a prep_in_used_algorithm dobj_used_sentences dobj_used_seeds aux_used_has nsubj_used_Yarowsky appos_Yarowsky_1995
P06-1056	P95-1026	o	Unlike Yarowsky -LRB- 1995 -RRB- we use automatic collection of seeds	prep_of_collection_seeds amod_collection_automatic dobj_use_collection nsubj_use_we prep_unlike_use_Yarowsky appos_Yarowsky_1995
P06-1058	P95-1026	o	Yarowsky -LRB- 1994 and 1995 -RRB- Mihalcea and Moldovan -LRB- 2000 -RRB- and Mihalcea -LRB- 2002 -RRB- have made further research to obtain large corpus of higher quality from an initial seed corpus	nn_corpus_seed amod_corpus_initial det_corpus_an amod_quality_higher prep_of_corpus_quality amod_corpus_large prep_from_obtain_corpus dobj_obtain_corpus aux_obtain_to vmod_research_obtain amod_research_further dobj_made_research aux_made_have nsubj_made_Mihalcea nsubj_made_Moldovan nsubj_made_Mihalcea nsubj_made_Yarowsky appos_Mihalcea_2002 appos_Moldovan_2000 conj_and_1994_1995 conj_and_Yarowsky_Mihalcea conj_and_Yarowsky_Moldovan conj_and_Yarowsky_Mihalcea dep_Yarowsky_1995 dep_Yarowsky_1994
P06-1072	P95-1026	p	Annealing resembles the popular bootstrapping technique -LRB- Yarowsky 1995 -RRB- which starts out aiming for high precision and gradually improves coverage over time	prep_over_improves_time dobj_improves_coverage advmod_improves_gradually nsubj_improves_Annealing amod_precision_high prep_for_aiming_precision xcomp_starts_aiming prt_starts_out nsubj_starts_which dep_Yarowsky_1995 rcmod_technique_starts appos_technique_Yarowsky nn_technique_bootstrapping amod_technique_popular det_technique_the conj_and_resembles_improves dobj_resembles_technique nsubj_resembles_Annealing
P06-1089	P95-1026	o	Yarowsky -LRB- 1995 -RRB- studied a method for word sense disambiguation using unlabeled data	amod_data_unlabeled dobj_using_data nn_disambiguation_sense nn_disambiguation_word vmod_method_using prep_for_method_disambiguation det_method_a dobj_studied_method nsubj_studied_Yarowsky appos_Yarowsky_1995
P06-2022	P95-1026	o	Yarowsky -LRB- 1995 -RRB- uses a conceptually similar technique for WSD that learns from a small set of seed examples and then increases recall by bootstrapping evaluated on 12 idiosyncratically polysemous words	amod_words_polysemous advmod_words_idiosyncratically num_words_12 prep_on_evaluated_words nsubj_evaluated_that prep_by_recall_bootstrapping dobj_increases_recall advmod_increases_then nsubj_increases_that nn_examples_seed prep_of_set_examples amod_set_small det_set_a conj_and_learns_evaluated conj_and_learns_increases prep_from_learns_set nsubj_learns_that rcmod_technique_evaluated rcmod_technique_increases rcmod_technique_learns prep_for_technique_WSD amod_technique_similar advmod_technique_conceptually det_technique_a dobj_uses_technique nsubj_uses_Yarowsky appos_Yarowsky_1995
P06-2065	P95-1026	o	In cases like -LRB- Yarowsky 1995 -RRB- unsupervised methods offer accuracy results than rival supervised methods -LRB- Yarowsky 1994 -RRB- while requiring only a fraction of the data preparation effort	nn_effort_preparation nn_effort_data det_effort_the prep_of_fraction_effort det_fraction_a advmod_fraction_only dobj_requiring_fraction mark_requiring_while dep_Yarowsky_1994 appos_methods_Yarowsky amod_methods_supervised amod_methods_rival prep_than_results_methods nn_results_accuracy advcl_offer_requiring dobj_offer_results nsubj_offer_methods prep_in_offer_cases amod_methods_unsupervised amod_Yarowsky_1995 dep_like_Yarowsky prep_cases_like
P06-2071	P95-1026	p	-LRB- Yarowsky 1995 -RRB- demonstrated that semi-supervised WSD could be successful	cop_successful_be aux_successful_could nsubj_successful_WSD mark_successful_that amod_WSD_semi-supervised ccomp_demonstrated_successful nsubj_demonstrated_Yarowsky amod_Yarowsky_1995
P06-2071	P95-1026	o	Most importantly whereas the one-sense-per-discourse assumption -LRB- Yarowsky 1995 -RRB- also applies to discriminating images there is no guarantee of a local collocational or co-occurrence context around the target image	nn_image_target det_image_the nn_context_co-occurrence prep_around_collocational_image conj_or_collocational_context amod_collocational_local det_collocational_a prep_of_guarantee_context prep_of_guarantee_collocational neg_guarantee_no nsubj_is_guarantee expl_is_there advcl_is_applies advmod_is_importantly amod_images_discriminating prep_to_applies_images advmod_applies_also nsubj_applies_assumption mark_applies_whereas amod_Yarowsky_1995 dep_assumption_Yarowsky amod_assumption_one-sense-per-discourse det_assumption_the advmod_importantly_Most ccomp_``_is
P06-2071	P95-1026	o	2 Data and annotation Yahoo!s image query API was used to obtain a corpus of pairs of semantically ambiguous images in thumbnail and true size and their corresponding web sites for three ambiguous keywords inspired by -LRB- Yarowsky 1995 -RRB- BASS CRANE and SQUASH	conj_and_BASS_SQUASH conj_and_BASS_CRANE dep_Yarowsky_SQUASH dep_Yarowsky_CRANE dep_Yarowsky_BASS dep_Yarowsky_1995 agent_inspired_Yarowsky vmod_keywords_inspired amod_keywords_ambiguous num_keywords_three prep_for_sites_keywords nn_sites_web amod_sites_corresponding poss_sites_their amod_size_true amod_size_thumbnail conj_and_thumbnail_true amod_images_ambiguous advmod_images_semantically prep_of_pairs_images prep_of_corpus_pairs det_corpus_a dobj_obtain_corpus aux_obtain_to conj_and_used_sites prep_in_used_size xcomp_used_obtain auxpass_used_was nsubjpass_used_API nsubjpass_used_Data nn_API_query nn_API_image nn_API_Yahoo!s nn_API_annotation conj_and_Data_API num_Data_2 ccomp_``_sites ccomp_``_used
P06-2077	P95-1026	o	In this method the decision list -LRB- DL -RRB- learning algorithm -LRB- Yarowsky 1995 -RRB- is used	auxpass_used_is nsubjpass_used_algorithm prep_in_used_method amod_Yarowsky_1995 dep_algorithm_Yarowsky nn_algorithm_learning nn_algorithm_list appos_list_DL nn_list_decision det_list_the det_method_this
P06-2077	P95-1026	o	This improvement is close to that of one sense per discourse -LRB- Yarowsky 1995 -RRB- -LRB- improvement ranging from 1.3 % to 1.7 % -RRB- which seems to be a sensible upper bound of the proposed method	amod_method_proposed det_method_the prep_of_bound_method dep_upper_bound amod_upper_sensible det_upper_a cop_upper_be aux_upper_to xcomp_seems_upper nsubj_seems_which rcmod_%_seems num_%_1.7 num_%_1.3 prep_to_ranging_% prep_from_ranging_% vmod_improvement_ranging dep_Yarowsky_1995 prep_per_sense_discourse num_sense_one dep_that_Yarowsky prep_of_that_sense dep_close_improvement prep_to_close_that cop_close_is nsubj_close_improvement det_improvement_This ccomp_``_close
P06-2077	P95-1026	o	These instances can be retagged with their countability by using the proposed method and some kind of bootstrapping -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_bootstrapping_Yarowsky prep_of_kind_bootstrapping det_kind_some conj_and_method_kind amod_method_proposed det_method_the dobj_using_kind dobj_using_method poss_countability_their agent_retagged_using prep_with_retagged_countability auxpass_retagged_be aux_retagged_can nsubjpass_retagged_instances det_instances_These ccomp_``_retagged
P06-2077	P95-1026	o	Yarowsky -LRB- 1995 -RRB- tested the claim on about 37,000 examples and found that when a polysemous word appeared more than once in a discourse they took on the majority sense for the discourse 99.8 % of the time on average	prep_on_time_average det_time_the prep_of_%_time num_%_99.8 dep_discourse_% det_discourse_the prep_for_sense_discourse nn_sense_majority det_sense_the dobj_took_sense prt_took_on nsubj_took_they advcl_took_appeared mark_took_that det_discourse_a pobj_in_discourse advmod_in_once pcomp_than_in prep_more_than advmod_appeared_more nsubj_appeared_word advmod_appeared_when amod_word_polysemous det_word_a ccomp_found_took num_examples_37,000 quantmod_37,000_about prep_on_claim_examples det_claim_the dobj_tested_claim conj_and_Yarowsky_found vmod_Yarowsky_tested dep_Yarowsky_1995
P06-2077	P95-1026	o	Note that although the source of the data is the same as in Section 5 as Yarowsky -LRB- 1995 -RRB- did	nsubj_did_Yarowsky mark_did_as ccomp_did_same mark_did_that appos_Yarowsky_1995 num_Section_5 pobj_in_Section pcomp_as_in prep_same_as det_same_the cop_same_is nsubj_same_source mark_same_although det_data_the prep_of_source_data det_source_the ccomp_Note_did
P06-2117	P95-1026	o	Consequently semi-supervised learning which combines both labeled and unlabeled data has been applied to some NLP tasks such as word sense disambiguation -LRB- Yarowsky 1995 Pham et al. 2005 -RRB- classification -LRB- Blum and Mitchell 1998 Thorsten 1999 -RRB- clustering -LRB- Basu et al. 2004 -RRB- named entity classification -LRB- Collins and Singer 1999 -RRB- and parsing -LRB- Sarkar 2001 -RRB-	amod_Sarkar_2001 dep_parsing_Sarkar amod_Collins_1999 conj_and_Collins_Singer dep_classification_Singer dep_classification_Collins nn_classification_entity dep_named_classification amod_Basu_2004 dep_Basu_al. nn_Basu_et vmod_clustering_named dep_clustering_Basu dep_Thorsten_1999 conj_and_Blum_parsing conj_and_Blum_clustering dep_Blum_Thorsten num_Blum_1998 conj_and_Blum_Mitchell nn_Blum_classification num_Pham_2005 nn_Pham_al. nn_Pham_et dep_Yarowsky_Pham dep_Yarowsky_1995 appos_disambiguation_parsing appos_disambiguation_clustering appos_disambiguation_Mitchell appos_disambiguation_Blum appos_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word prep_such_as_tasks_disambiguation nn_tasks_NLP det_tasks_some prep_to_applied_tasks auxpass_applied_been aux_applied_has nsubjpass_applied_data nsubjpass_applied_labeled amod_data_unlabeled conj_and_labeled_data preconj_labeled_both ccomp_combines_applied nsubj_combines_which rcmod_learning_combines amod_learning_semi-supervised dep_Consequently_learning dep_``_Consequently
P07-1004	P95-1026	o	3 The Framework 3.1 The Algorithm Our transductive learning algorithm Algorithm 1 is inspired by the Yarowsky algorithm -LRB- Yarowsky 1995 Abney 2004 -RRB-	amod_Abney_2004 dep_Yarowsky_Abney appos_Yarowsky_1995 dep_algorithm_Yarowsky nn_algorithm_Yarowsky det_algorithm_the agent_inspired_algorithm auxpass_inspired_is nsubjpass_inspired_algorithm num_Algorithm_1 appos_algorithm_Algorithm nn_algorithm_learning amod_algorithm_transductive poss_algorithm_Our nn_algorithm_Algorithm det_algorithm_The num_algorithm_3.1 dep_algorithm_Framework num_algorithm_3 det_Framework_The
P07-1006	P95-1026	o	2 Related Work WSD approaches can be classified as -LRB- a -RRB- knowledge-based approaches which make use of linguistic knowledge manually coded or extracted from lexical resources -LRB- Agirre and Rigau 1996 Lesk 1986 -RRB- -LRB- b -RRB- corpus-based approaches which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models -LRB- Yarowsky 1995 Schtze 1998 -RRB- and -LRB- c -RRB- hybrid approaches which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge -LRB- Ng and Lee 1996 Stevenson and Wilks 2001 -RRB-	num_Lee_1996 amod_Ng_2001 conj_and_Ng_Wilks conj_and_Ng_Stevenson conj_and_Ng_Lee dep_knowledge_Wilks dep_knowledge_Stevenson dep_knowledge_Lee dep_knowledge_Ng amod_knowledge_linguistic agent_supported_knowledge vmod_corpus_supported nn_models_disambiguation prep_from_acquire_corpus dobj_acquire_models advmod_acquire_automatically aux_acquire_to vmod_approaches_acquire amod_approaches_other num_approaches_two det_approaches_the prep_from_mix_approaches dobj_mix_characteristics nsubj_mix_which rcmod_approaches_mix nn_approaches_hybrid dep_approaches_c dep_1998_Schtze dep_Yarowsky_1998 appos_Yarowsky_1995 appos_models_Yarowsky nn_models_disambiguation dobj_induce_models aux_induce_to nn_algorithms_learning nn_algorithms_machine nn_algorithms_statistical nn_algorithms_corpus conj_or_corpus_machine conj_and_corpus_statistical xcomp_acquired_induce prep_from_acquired_algorithms advmod_acquired_automatically amod_knowledge_shallow vmod_use_acquired prep_of_use_knowledge dobj_make_use nsubj_make_which rcmod_approaches_make amod_approaches_corpus-based dep_approaches_b num_Lesk_1986 dep_Agirre_Lesk conj_and_Agirre_1996 conj_and_Agirre_Rigau appos_resources_1996 appos_resources_Rigau appos_resources_Agirre amod_resources_lexical prep_from_coded_resources conj_or_coded_extracted advmod_coded_manually amod_knowledge_linguistic prep_of_use_knowledge dobj_make_use nsubj_make_which conj_and_approaches_approaches conj_and_approaches_approaches vmod_approaches_extracted vmod_approaches_coded rcmod_approaches_make amod_approaches_knowledge-based det_approaches_a prep_as_classified_approaches prep_as_classified_approaches prep_as_classified_approaches auxpass_classified_be aux_classified_can nsubjpass_classified_approaches nn_approaches_WSD nn_approaches_Work amod_approaches_Related num_approaches_2
P07-1109	P95-1026	o	Thus we propose a bootstrapping approach -LRB- Yarowsky 1995 -RRB- to train the stochastic transducer iteratively as it extracts transliterations from a bitext	det_bitext_a prep_from_extracts_bitext dobj_extracts_transliterations nsubj_extracts_it mark_extracts_as advmod_transducer_iteratively amod_transducer_stochastic det_transducer_the advcl_train_extracts dobj_train_transducer aux_train_to amod_Yarowsky_1995 vmod_approach_train dep_approach_Yarowsky amod_approach_bootstrapping det_approach_a dobj_propose_approach nsubj_propose_we advmod_propose_Thus
P07-1109	P95-1026	p	In order to overcome this problem we look to the bootstrapping method outlined in -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_in_Yarowsky prep_outlined_in vmod_method_outlined nn_method_bootstrapping det_method_the prep_to_look_method nsubj_look_we advcl_look_overcome det_problem_this dobj_overcome_problem aux_overcome_to dep_overcome_order mark_overcome_In
P07-1125	P95-1026	o	Early work by Yarowsky -LRB- 1995 -RRB- falls within this framework	det_framework_this prep_within_falls_framework nsubj_falls_work appos_Yarowsky_1995 prep_by_work_Yarowsky amod_work_Early
P08-1030	P95-1026	o	7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation -LRB- WSD -RRB- so we have used the idea of sense consistency introduced in -LRB- Yarowsky 1995 -RRB- extending it to operate across related documents	amod_documents_related prep_across_operate_documents aux_operate_to xcomp_extending_operate dobj_extending_it vmod_Yarowsky_extending amod_Yarowsky_1995 prep_in_introduced_Yarowsky vmod_consistency_introduced nn_consistency_sense prep_of_idea_consistency det_idea_the dobj_used_idea aux_used_have nsubj_used_we mark_used_so appos_disambiguation_WSD nn_disambiguation_sense nn_disambiguation_word advcl_task_used prep_of_task_disambiguation det_task_a prep_in_task_part cop_task_is nsubj_task_Work nsubj_is_task det_paper_this prep_in_described_paper vmod_task_described nn_task_labeling nn_task_trigger det_task_The amod_Work_Related num_Work_7
P08-1030	P95-1026	o	c2008 Association for Computational Linguistics Refining Event Extraction through Cross-document Inference Heng Ji Ralph Grishman Computer Science Department New York University New York NY 10003 USA -LRB- hengji grishman -RRB- @cs nyu.edu Abstract We apply the hypothesis of One Sense Per Discourse -LRB- Yarowsky 1995 -RRB- to information extraction -LRB- IE -RRB- and extend the scope of discourse from one single document to a cluster of topically-related documents	amod_documents_topically-related prep_of_cluster_documents det_cluster_a amod_document_single num_document_one prep_of_scope_discourse det_scope_the prep_to_extend_cluster prep_from_extend_document dobj_extend_scope nsubj_extend_Association appos_extraction_IE nn_extraction_information amod_Yarowsky_1995 prep_to_Discourse_extraction dep_Discourse_Yarowsky prep_per_Sense_Discourse num_Sense_One prep_of_hypothesis_Sense det_hypothesis_the conj_and_apply_extend dobj_apply_hypothesis nsubj_apply_We dep_apply_Abstract dep_apply_nyu.edu nsubj_apply_Association dep_@cs_hengji nn_@cs_USA appos_hengji_grishman appos_NY_@cs num_NY_10003 appos_York_NY nn_York_New nn_York_University nn_York_York nn_York_New nn_York_Department nn_York_Science nn_York_Computer nn_York_Grishman nn_York_Ralph nn_York_Ji nn_York_Heng nn_York_Inference nn_York_Cross-document nn_Extraction_Event nn_Extraction_Refining nn_Extraction_Linguistics nn_Extraction_Computational prep_through_Association_York prep_for_Association_Extraction nn_Association_c2008
P08-1061	P95-1026	o	Self-training is a commonly used technique for semi-supervised learning that has been ap532 plied to several natural language processing tasks -LRB- Yarowsky 1995 Charniak 1997 Steedman et al. 2003 -RRB-	num_Steedman_2003 nn_Steedman_al. nn_Steedman_et dep_Charniak_Steedman num_Charniak_1997 dep_Yarowsky_Charniak appos_Yarowsky_1995 dep_tasks_Yarowsky nn_tasks_processing nn_tasks_language amod_tasks_natural amod_tasks_several prep_to_plied_tasks advmod_plied_ap532 auxpass_plied_been aux_plied_has nsubjpass_plied_that rcmod_learning_plied amod_learning_semi-supervised prep_for_technique_learning amod_technique_used det_technique_a cop_technique_is nsubj_technique_Self-training advmod_used_commonly
P08-1088	P95-1026	o	In our context bootstrapping has a similar motivation to the annealing approach of Smith and Eisner -LRB- 2006 -RRB- which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step though of course the use of bootstrapping in general is quite widespread -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_widespread_Yarowsky advmod_widespread_quite cop_widespread_is nsubj_widespread_use prep_of_widespread_course mark_widespread_though prep_in_use_general prep_of_use_bootstrapping det_use_the det_M-step_the prep_in_learning_M-step dobj_facilitate_learning aux_facilitate_to det_E-step_the amod_outputs_hidden prep_of_space_outputs det_space_the xcomp_alter_facilitate prep_over_alter_time prep_in_alter_E-step dobj_alter_space aux_alter_to xcomp_tries_alter advmod_tries_also nsubj_tries_which appos_Eisner_2006 conj_and_Smith_Eisner rcmod_approach_tries prep_of_approach_Eisner prep_of_approach_Smith nn_approach_annealing det_approach_the prep_to_motivation_approach amod_motivation_similar det_motivation_a advcl_has_widespread dobj_has_motivation nsubj_has_bootstrapping prep_in_has_context poss_context_our
P09-1095	P95-1026	p	One of the most notable examples is Yarowskys -LRB- 1995 -RRB- bootstrapping algorithm for word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word prep_for_algorithm_disambiguation amod_algorithm_bootstrapping dep_Yarowskys_algorithm appos_Yarowskys_1995 cop_Yarowskys_is nsubj_Yarowskys_One amod_examples_notable det_examples_the advmod_notable_most prep_of_One_examples
P09-1117	P95-1026	o	Self-training -LRB- Yarowsky 1995 -RRB- is a form of semi-supervised learning	amod_learning_semi-supervised prep_of_form_learning det_form_a cop_form_is nsubj_form_Self-training amod_Yarowsky_1995 dep_Self-training_Yarowsky
P09-2017	P95-1026	o	To reduce it we exploit the one sense per collocation property -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 nn_property_collocation dep_sense_Yarowsky prep_per_sense_property num_sense_one det_sense_the dobj_exploit_sense nsubj_exploit_we ccomp_reduce_exploit dobj_reduce_it aux_reduce_To
P96-1006	P95-1026	o	Most recently Yarowsky used an unsupervised learning procedure to perform WSD -LRB- Yarowsky 1995 -RRB- although this is only tested on disambiguating words into binary coarse sense distinction	nn_distinction_sense amod_distinction_coarse conj_binary_distinction amod_words_disambiguating prep_into_tested_binary prep_on_tested_words advmod_tested_only auxpass_tested_is nsubjpass_tested_this mark_tested_although amod_Yarowsky_1995 dep_WSD_Yarowsky dobj_perform_WSD aux_perform_to vmod_procedure_perform nn_procedure_learning amod_procedure_unsupervised det_procedure_an advcl_used_tested dobj_used_procedure nsubj_used_Yarowsky advmod_used_recently advmod_recently_Most
P97-1007	P95-1026	o	Some of them have been fully tested in real size texts -LRB- e.g. statistical methods -LRB- Yarowsky 1992 -RRB- -LRB- Yarowsky 1994 -RRB- -LRB- Miller and Teibel 1991 -RRB- knowledge based methods -LRB- Sussna 1993 -RRB- -LRB- Agirre and Rigau 1996 -RRB- or mixed methods -LRB- Richardson et al. 1994 -RRB- -LRB- Resnik 1995 -RRB- -RRB-	dep_Resnik_1995 amod_Richardson_1994 dep_Richardson_al. nn_Richardson_et amod_methods_mixed dep_Agirre_1996 conj_and_Agirre_Rigau dep_Sussna_1993 conj_or_methods_methods appos_methods_Rigau appos_methods_Agirre appos_methods_Sussna pobj_based_methods pobj_based_methods dep_knowledge_Richardson prep_knowledge_based dep_Miller_1991 conj_and_Miller_Teibel dep_Yarowsky_1994 dep_Yarowsky_1992 appos_methods_Resnik conj_methods_knowledge appos_methods_Teibel appos_methods_Miller appos_methods_Yarowsky dep_methods_Yarowsky amod_methods_statistical pobj_e.g._methods dep_texts_e.g. nn_texts_size amod_texts_real prep_in_tested_texts advmod_tested_fully auxpass_tested_been aux_tested_have nsubjpass_tested_Some prep_of_Some_them
P97-1007	P95-1026	o	-LRB- Yarowsky 1995 -RRB- reports a success rate of 96 % disambiguating twelve words with two clear sense distinctions each one -RRB-	det_one_each dep_distinctions_one nn_distinctions_sense amod_distinctions_clear num_distinctions_two num_words_twelve prep_with_disambiguating_distinctions dobj_disambiguating_words vmod_%_disambiguating num_%_96 prep_of_rate_% nn_rate_success det_rate_a dobj_reports_rate nsubj_reports_Yarowsky amod_Yarowsky_1995
P97-1007	P95-1026	o	Furthermore it is not possible to apply the powerful one sense per discourse property -LRB- Yarowsky 1995 -RRB- because there is no discourse in dictionaries	neg_discourse_no prep_in_is_dictionaries nsubj_is_discourse expl_is_there mark_is_because amod_Yarowsky_1995 appos_property_Yarowsky dep_property_sense amod_property_powerful det_property_the prep_per_sense_discourse num_sense_one advcl_apply_is dobj_apply_property aux_apply_to xcomp_possible_apply neg_possible_not cop_possible_is nsubj_possible_it advmod_possible_Furthermore
P97-1009	P95-1026	o	In Yarowsky 's experiment -LRB- Yarowsky 1995 -RRB- an average of 3936 examples were used to disambiguate between two senses	num_senses_two prep_between_disambiguate_senses aux_disambiguate_to xcomp_used_disambiguate auxpass_used_were nsubjpass_used_average prep_in_used_experiment num_examples_3936 prep_of_average_examples det_average_an amod_Yarowsky_1995 dep_experiment_Yarowsky poss_experiment_Yarowsky
P97-1009	P95-1026	o	Yarowsky -LRB- Yarowsky 1995 -RRB- proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus thus avoided the need to hand-annotate any examples	det_examples_any dobj_hand-annotate_examples aux_hand-annotate_to vmod_need_hand-annotate det_need_the dobj_avoided_need advmod_avoided_thus det_corpus_the prep_of_parts_corpus amod_parts_other det_parts_the det_results_the prep_to_expanded_parts dobj_expanded_results nsubj_expanded_that nn_classifications_seed dobj_obtain_classifications aux_obtain_to conj_and_used_expanded vmod_used_obtain dobj_used_heuristics nsubj_used_that rcmod_method_expanded rcmod_method_used amod_method_unsupervised det_method_an dep_proposed_avoided dobj_proposed_method nsubj_proposed_Yarowsky dep_Yarowsky_1995 dep_Yarowsky_Yarowsky
P98-1037	P95-1026	o	Evidence have shown that by exploiting the constraint of so-called one sense per discourse -LRB- Gale Church and Yarowsky 1992b -RRB- and the strategy of bootstrapping -LRB- Yarowsky 1995 -RRB- it is possible to boost coverage while maintaining about the same level of precision	prep_of_level_precision amod_level_same det_level_the prep_about_maintaining_level mark_maintaining_while advcl_,_maintaining dobj_boost_coverage aux_boost_to cop_possible_is nsubj_possible_it ccomp_possible_strategy ccomp_possible_Gale dep_possible_sense amod_possible_so-called num_Yarowsky_1995 appos_bootstrapping_Yarowsky prep_of_strategy_bootstrapping det_strategy_the nn_1992b_Yarowsky nn_1992b_Church conj_and_Church_Yarowsky conj_and_Gale_strategy appos_Gale_1992b prep_per_sense_discourse num_sense_one prep_of_constraint_possible det_constraint_the vmod_exploiting_boost dobj_exploiting_constraint pcomp_by_exploiting dep_that_by prep_shown_that aux_shown_have nsubj_shown_Evidence
P98-1037	P95-1026	o	The adaptive approach is somehow similar to their idea of incremental learning and to the bootstrap approach proposed by Yarowsky -LRB- 1995 -RRB-	appos_Yarowsky_1995 agent_proposed_Yarowsky vmod_approach_proposed nn_approach_bootstrap det_approach_the pobj_to_approach amod_learning_incremental prep_of_idea_learning poss_idea_their conj_and_similar_to prep_to_similar_idea advmod_similar_somehow cop_similar_is nsubj_similar_approach amod_approach_adaptive det_approach_The
P98-1069	P95-1026	o	This approach has also been used by -LRB- Dagan and Itai 1994 Gale et al. 1992 Shiitze 1992 Gale et al. 1993 Yarowsky 1995 Gale and Church 1Lunar is not an unknown word in English Yeltsin finds its translation in the 4-th candidate	amod_candidate_4-th det_candidate_the poss_translation_its prep_in_finds_candidate dobj_finds_translation nsubj_finds_Yeltsin parataxis_word_finds prep_in_word_English amod_word_unknown det_word_an neg_word_not cop_word_is nsubj_word_Church nsubj_word_Gale appos_Gale_1Lunar conj_and_Gale_Church num_Yarowsky_1995 num_Gale_1993 nn_Gale_al. nn_Gale_et num_Shiitze_1992 num_Gale_1992 nn_Gale_al. nn_Gale_et dep_Dagan_word conj_and_Dagan_Yarowsky conj_and_Dagan_Gale conj_and_Dagan_Shiitze conj_and_Dagan_Gale conj_and_Dagan_1994 conj_and_Dagan_Itai agent_used_Yarowsky agent_used_Gale agent_used_Shiitze agent_used_Gale agent_used_1994 agent_used_Itai agent_used_Dagan auxpass_used_been advmod_used_also aux_used_has nsubjpass_used_approach det_approach_This
P98-2182	P95-1026	p	Extracting semantic information from word co-occurrence statistics has been effective particularly for sense disambiguation -LRB- Schiitze 1992 Gale et al. 1992 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 num_Gale_1992 nn_Gale_al. nn_Gale_et dep_Schiitze_Yarowsky conj_Schiitze_Gale conj_Schiitze_1992 appos_disambiguation_Schiitze nn_disambiguation_sense prep_for_effective_disambiguation advmod_effective_particularly cop_effective_been aux_effective_has nsubj_effective_information nn_statistics_co-occurrence nn_statistics_word prep_from_information_statistics amod_information_semantic amod_information_Extracting
P98-2228	P95-1026	p	Decision lists have already been successfully applied to lexical ambiguity resolution by -LRB- Yarowsky 1995 -RRB- where they perfromed well	advmod_perfromed_well nsubj_perfromed_they advmod_perfromed_where dep_Yarowsky_perfromed dep_Yarowsky_1995 nn_resolution_ambiguity amod_resolution_lexical agent_applied_Yarowsky prep_to_applied_resolution advmod_applied_successfully auxpass_applied_been advmod_applied_already aux_applied_have nsubjpass_applied_lists nn_lists_Decision
P98-2228	P95-1026	o	First researchers are divided between a general method -LRB- that attempts to apply WSD to all the content words of texts the option taken in this paper -RRB- and one that is applied only to a small trial selection of texts words -LRB- for example -LRB- Schiitze 1992 -RRB- -LRB- Yarowsky 1995 -RRB- -RRB-	dep_Yarowsky_1995 dep_Schiitze_1992 dep_example_Schiitze dep_for_Yarowsky pobj_for_example nn_words_texts dep_selection_for prep_of_selection_words nn_selection_trial amod_selection_small det_selection_a prep_to_applied_selection advmod_applied_only auxpass_applied_is nsubjpass_applied_that rcmod_one_applied det_paper_this prep_in_taken_paper vmod_option_taken det_option_the prep_of_words_texts appos_content_option dep_content_words det_content_the conj_and_all_one dep_all_content prep_to_apply_one prep_to_apply_all dobj_apply_WSD aux_apply_to xcomp_attempts_apply nsubj_attempts_that rcmod_method_attempts amod_method_general det_method_a prep_between_divided_method auxpass_divided_are nsubjpass_divided_researchers advmod_divided_First
P99-1020	P95-1026	o	WSD that use information gathered from raw corpora -LRB- unsupervised training methods -RRB- -LRB- Yarowsky 1995 -RRB- -LRB- Resnik 1997 -RRB-	amod_Resnik_1997 dep_Yarowsky_1995 nn_methods_training amod_methods_unsupervised dep_corpora_Resnik dep_corpora_Yarowsky appos_corpora_methods amod_corpora_raw prep_from_gathered_corpora vmod_information_gathered dobj_use_information nsubj_use_that rcmod_WSD_use
P99-1020	P95-1026	p	Some of the best results were reported in -LRB- Yarowsky 1995 -RRB- who uses a large training corpus	nn_corpus_training amod_corpus_large det_corpus_a dobj_uses_corpus nsubj_uses_who rcmod_Yarowsky_uses amod_Yarowsky_1995 prep_in_reported_Yarowsky auxpass_reported_were nsubjpass_reported_Some amod_results_best det_results_the prep_of_Some_results ccomp_``_reported
P99-1043	P95-1026	o	The results are consistent with the idea in -LRB- Gale and Church 1994 Shfitze 1992 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Shfitze_Yarowsky conj_Shfitze_1992 dep_Gale_Shfitze conj_and_Gale_1994 conj_and_Gale_Church prep_in_idea_1994 prep_in_idea_Church prep_in_idea_Gale det_idea_the prep_with_consistent_idea cop_consistent_are nsubj_consistent_results det_results_The
P99-1043	P95-1026	o	Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction -LRB- Smadja 1993 Fung and Wu 1994 -RRB- phrasal translation -LRB- Smadja et al. 1996 Kupiec 1993 Wu 1995 Dagan and Church 1994 -RRB- target word selection -LRB- Liu and Li 1997 Tanaka and Iwasaki 1996 -RRB- domain word translation -LRB- Fung and Lo 1998 Fung 1998 -RRB- sense disambiguation -LRB- Brown et al. 1991 Dagan et al. 1991 Dagan and Itai 1994 Gale et al. 1992a Gale et al. 1992b Gale et al. 1992c Shiitze 1992 Gale et al. 1993 Yarowsky 1995 -RRB- and even recently for query translation in cross-language IR as well -LRB- Ballesteros and Croft 1998 -RRB-	amod_Ballesteros_1998 conj_and_Ballesteros_Croft dep_well_Croft dep_well_Ballesteros advmod_well_as amod_IR_cross-language advmod_translation_well prep_in_translation_IR nn_translation_query pobj_for_translation advmod_for_recently advmod_recently_even dep_Yarowsky_1995 num_Gale_1993 nn_Gale_al. nn_Gale_et num_Shiitze_1992 conj_and_1992c_for conj_and_1992c_Yarowsky conj_and_1992c_Gale conj_and_1992c_Shiitze dep_al._for dep_al._Yarowsky dep_al._Gale dep_al._Shiitze dep_al._1992c nn_al._et nn_al._Gale appos_al._1992b nn_al._et nn_al._Gale nn_al._et nn_al._Gale num_Dagan_1994 conj_and_Dagan_Itai nn_al._et nn_al._Dagan dep_al._al. conj_al._al. appos_al._1992a dep_al._al. dep_al._Itai dep_al._Dagan num_al._1991 dep_al._al. num_al._1991 nn_al._et amod_al._Brown dep_disambiguation_al. nn_disambiguation_sense num_Fung_1998 appos_Fung_disambiguation dep_Fung_Fung conj_and_Fung_1998 conj_and_Fung_Lo dep_translation_1998 dep_translation_Lo dep_translation_Fung nn_translation_word nn_translation_domain dep_Tanaka_1996 conj_and_Tanaka_Iwasaki dep_Liu_Iwasaki dep_Liu_Tanaka appos_Liu_1997 conj_and_Liu_Li appos_selection_Li appos_selection_Liu nn_selection_word nn_selection_target num_Wu_1995 dep_Kupiec_1994 conj_and_Kupiec_Church conj_and_Kupiec_Dagan conj_and_Kupiec_Wu num_Kupiec_1993 dep_Smadja_Church dep_Smadja_Dagan dep_Smadja_Wu dep_Smadja_Kupiec appos_Smadja_1996 dep_Smadja_al. nn_Smadja_et appos_translation_translation conj_translation_selection appos_translation_Smadja amod_translation_phrasal num_Fung_1994 conj_and_Fung_Wu conj_Smadja_Wu conj_Smadja_Fung conj_Smadja_1993 dep_extraction_Smadja nn_extraction_phrase dobj_used_translation prep_in_used_extraction auxpass_used_been aux_used_has nsubjpass_used_information amod_sentence_same det_sentence_the prep_in_words_sentence conj_and_words_words amod_words_neighboring prep_between_information_words prep_between_information_words nn_information_Co-occurrence
P99-1043	P95-1026	o	Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora -LRB- Smadja et al. 1996 Kupiec 1993 Wu 1995 Tanaka and Iwasaki 1996 Fung and Lo 1998 -RRB- or monolingual corpora -LRB- Smadja 1993 Fung and Wu 1994 Liu and Li 1997 Shiitze 1992 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 appos_Shiitze_1992 num_Liu_1997 conj_and_Liu_Li num_Fung_1994 conj_and_Fung_Wu dep_Smadja_Yarowsky conj_Smadja_Shiitze conj_Smadja_Li conj_Smadja_Liu conj_Smadja_Wu conj_Smadja_Fung conj_Smadja_1993 dep_corpora_Smadja amod_corpora_monolingual num_Wu_1995 dep_Kupiec_1998 conj_and_Kupiec_Lo conj_and_Kupiec_Fung conj_and_Kupiec_1996 conj_and_Kupiec_Iwasaki conj_and_Kupiec_Tanaka conj_and_Kupiec_Wu num_Kupiec_1993 conj_or_Smadja_corpora dep_Smadja_Lo dep_Smadja_Fung dep_Smadja_1996 dep_Smadja_Iwasaki dep_Smadja_Tanaka dep_Smadja_Wu dep_Smadja_Kupiec appos_Smadja_1996 dep_Smadja_al. nn_Smadja_et amod_corpora_non-parallel num_corpora_334 conj_and_parallel_corpora amod_parallel_bilingual preconj_parallel_either dep_collected_corpora dep_collected_Smadja prep_from_collected_corpora prep_from_collected_parallel auxpass_collected_is nsubjpass_collected_statistics nn_statistics_Co-occurrence
P99-1043	P95-1026	o	-LRB- Shfitze 1992 Yarowsky 1995 -RRB- all use multiple context words as discriminating features	dobj_discriminating_features nn_words_context amod_words_multiple prepc_as_use_discriminating dobj_use_words dep_use_all nsubj_use_Shfitze dep_Yarowsky_1995 dep_Shfitze_Yarowsky dep_Shfitze_1992
W00-1320	P95-1026	o	Finally we would like to investigate the incorporation of unsupervised methods for WSD such as the heuristically-based methods of -LRB- Stetina and Nagao 1997 -RRB- and -LRB- Stetina et al. 1998 -RRB- and the theoretically purer bootstrapping method of -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_of_Yarowsky prep_method_of nn_method_bootstrapping amod_method_purer advmod_method_theoretically det_method_the amod_Stetina_1998 dep_Stetina_al. nn_Stetina_et conj_and_Stetina_method conj_and_Stetina_Stetina dep_Stetina_1997 conj_and_Stetina_Nagao prep_of_methods_method prep_of_methods_Stetina prep_of_methods_Nagao prep_of_methods_Stetina amod_methods_heuristically-based det_methods_the prep_such_as_WSD_methods prep_for_methods_WSD amod_methods_unsupervised prep_of_incorporation_methods det_incorporation_the dobj_investigate_incorporation aux_investigate_to xcomp_like_investigate aux_like_would nsubj_like_we advmod_like_Finally
W00-1320	P95-1026	o	-LRB- Yarowsky 1995 -RRB- also uses wide context but incorporates the one-senseper-discourse and one-sense-per-collocation constraints using an unsupervised learning technique	nn_technique_learning amod_technique_unsupervised det_technique_an dobj_using_technique amod_constraints_one-sense-per-collocation amod_constraints_one-senseper-discourse det_constraints_the conj_and_one-senseper-discourse_one-sense-per-collocation vmod_incorporates_using dobj_incorporates_constraints nsubj_incorporates_Yarowsky amod_context_wide conj_but_uses_incorporates dobj_uses_context advmod_uses_also nsubj_uses_Yarowsky amod_Yarowsky_1995
W00-1326	P95-1026	p	They have been successfully applied to accent restoration word sense disambiguation 209 and homograph disambiguation -LRB- Yarowsky 1994 1995 1996 -RRB-	dep_Yarowsky_1996 dep_Yarowsky_1995 appos_Yarowsky_1994 appos_disambiguation_Yarowsky nn_disambiguation_homograph num_disambiguation_209 nn_disambiguation_sense nn_disambiguation_word nn_restoration_accent conj_and_applied_disambiguation conj_and_applied_disambiguation prep_to_applied_restoration advmod_applied_successfully auxpass_applied_been aux_applied_have nsubjpass_applied_They
W01-1208	P95-1026	o	Since word senses are often associated with domains -LRB- Yarowsky 1995 -RRB- word senses can be consequently distinguished by way of determining the domain of each description	det_description_each prep_of_domain_description det_domain_the dobj_determining_domain prepc_of_way_determining agent_distinguished_way advmod_distinguished_consequently auxpass_distinguished_be aux_distinguished_can nsubjpass_distinguished_senses advcl_distinguished_associated nn_senses_word dep_Yarowsky_1995 dep_domains_Yarowsky prep_with_associated_domains advmod_associated_often auxpass_associated_are nsubjpass_associated_senses mark_associated_Since nn_senses_word ccomp_``_distinguished
W02-0903	P95-1026	o	In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation -LRB- Yarowsky 1995 Stevenson and Wilks 2001 -RRB- named entity recognition -LRB- Collins and Singer 1999 -RRB- and vocabulary construction for information extraction -LRB- Riloff 1996 -RRB-	amod_Riloff_1996 dep_extraction_Riloff nn_extraction_information nn_construction_vocabulary amod_Collins_1999 conj_and_Collins_Singer dep_recognition_Singer dep_recognition_Collins nn_recognition_entity amod_recognition_named num_Stevenson_2001 conj_and_Stevenson_Wilks dep_Yarowsky_Wilks dep_Yarowsky_Stevenson dep_Yarowsky_1995 conj_and_disambiguation_construction conj_and_disambiguation_recognition appos_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word prep_for_sub-problems_extraction prep_like_sub-problems_construction prep_like_sub-problems_recognition prep_like_sub-problems_disambiguation prep_on_focused_sub-problems advmod_focused_more aux_focused_has nsubj_focused_research amod_semantics_lexical prep_on_research_semantics amod_decade_last det_decade_the dep_In_focused conj_or_In_so pobj_In_decade dep_``_so dep_``_In
W02-0903	P95-1026	o	Collocations have been widely used for tasks such as word sense disambiguation -LRB- WSD -RRB- -LRB- Yarowsky 1995 -RRB- information extraction -LRB- IE -RRB- -LRB- Riloff 1996 -RRB- and named-entity recognition -LRB- Collins and Singer 1999 -RRB-	amod_Collins_1999 conj_and_Collins_Singer dep_recognition_Singer dep_recognition_Collins nn_recognition_named-entity dep_Riloff_1996 appos_extraction_Riloff appos_extraction_IE nn_extraction_information amod_Yarowsky_1995 conj_and_disambiguation_recognition conj_and_disambiguation_extraction dep_disambiguation_Yarowsky appos_disambiguation_WSD nn_disambiguation_sense nn_disambiguation_word prep_such_as_tasks_recognition prep_such_as_tasks_extraction prep_such_as_tasks_disambiguation prep_for_used_tasks advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_Collocations
W02-1304	P95-1026	o	In another line of research -LRB- Yarowsky 1995 -RRB- and -LRB- Blum and Mitchell 1998 -RRB- have shown that it is possible to reduce the need for supervision with the help of large amounts of unannotated data	amod_data_unannotated prep_of_amounts_data amod_amounts_large prep_of_help_amounts det_help_the prep_with_supervision_help prep_for_need_supervision det_need_the dobj_reduce_need aux_reduce_to xcomp_possible_reduce cop_possible_is nsubj_possible_it mark_possible_that ccomp_shown_possible aux_shown_have nsubj_shown_Blum nsubj_shown_Yarowsky prep_in_shown_line amod_Blum_1998 conj_and_Blum_Mitchell conj_and_Yarowsky_Mitchell conj_and_Yarowsky_Blum amod_Yarowsky_1995 prep_of_line_research det_line_another
W03-0106	P95-1026	o	Recent work emphasizes a corpus-based unsupervised approach -LSB- Dagon and Itai 1994 Yarowsky 1992 Yarowsky 1995 -RSB- that avoids the need for costly truthed training data	nn_data_training amod_data_truthed amod_data_costly prep_for_need_data det_need_the dobj_avoids_need nsubj_avoids_that num_Yarowsky_1995 num_Yarowsky_1992 num_Itai_1994 dep_Dagon_Yarowsky conj_and_Dagon_Yarowsky conj_and_Dagon_Itai rcmod_approach_avoids dep_approach_Yarowsky dep_approach_Itai dep_approach_Dagon amod_approach_unsupervised amod_approach_corpus-based det_approach_a dobj_emphasizes_approach nsubj_emphasizes_work amod_work_Recent
W03-0107	P95-1026	o	Bootstrapping methods similar to ours have been shown to be competitive in word sense disambiguation -LRB- Yarowsky and Florian 2003 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Yarowsky_Yarowsky conj_and_Yarowsky_2003 conj_and_Yarowsky_Florian dep_disambiguation_2003 dep_disambiguation_Florian dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word prep_in_competitive_disambiguation cop_competitive_be aux_competitive_to xcomp_shown_competitive auxpass_shown_been aux_shown_have nsubjpass_shown_methods prep_to_similar_ours amod_methods_similar nn_methods_Bootstrapping
W03-0406	P95-1026	p	To overcome this problem unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently -LRB- Blum and Mitchell 1998 -RRB- -LRB- Yarowsky 1995 -RRB- -LRB- Park et al. 2000 -RRB- -LRB- Li and Li 2002 -RRB-	amod_Li_2002 conj_and_Li_Li appos_Park_2000 dep_Park_al. nn_Park_et dep_Yarowsky_1995 amod_Blum_1998 conj_and_Blum_Mitchell dep_recently_Park dep_recently_Yarowsky dep_recently_Mitchell dep_recently_Blum dep_proposed_Li dep_proposed_Li advmod_proposed_recently auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods advcl_proposed_overcome amod_data_labeled amod_data_small agent_learned_data vmod_rules_learned prep_of_performance_rules det_performance_the dobj_boost_performance aux_boost_to amod_data_unlabeled amod_data_huge vmod_using_boost dobj_using_data vmod_methods_using nn_methods_learning amod_methods_unsupervised det_problem_this dobj_overcome_problem aux_overcome_To ccomp_``_proposed
W03-0406	P95-1026	o	Yarowsky proposed the unsupervised learning method for WSD -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_WSD_Yarowsky prep_for_method_WSD nn_method_learning amod_method_unsupervised det_method_the dobj_proposed_method nsubj_proposed_Yarowsky
W03-0407	P95-1026	o	1 Introduction Co-training -LRB- Blum and Mitchell 1998 -RRB- and several variants of co-training have been applied to a number of NLP problems including word sense disambiguation -LRB- Yarowsky 1995 -RRB- named entity recognition -LRB- Collins and Singer 1999 -RRB- noun phrase bracketing -LRB- Pierce and Cardie 2001 -RRB- and statistical parsing -LRB- Sarkar 2001 Steedman et al. 2003 -RRB-	num_Steedman_2003 nn_Steedman_al. nn_Steedman_et dep_Sarkar_Steedman dep_Sarkar_2001 appos_parsing_Sarkar amod_parsing_statistical dep_Pierce_2001 conj_and_Pierce_Cardie nn_bracketing_phrase nn_bracketing_noun amod_Collins_1999 conj_and_Collins_Singer dep_recognition_Singer dep_recognition_Collins nn_recognition_entity amod_recognition_named amod_Yarowsky_1995 conj_and_disambiguation_parsing dep_disambiguation_Cardie dep_disambiguation_Pierce conj_and_disambiguation_bracketing conj_and_disambiguation_recognition dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word nn_problems_NLP prep_including_number_parsing prep_including_number_bracketing prep_including_number_recognition prep_including_number_disambiguation prep_of_number_problems det_number_a prep_to_applied_number auxpass_applied_been aux_applied_have nsubjpass_applied_variants nsubjpass_applied_Co-training prep_of_variants_co-training amod_variants_several amod_Blum_1998 conj_and_Blum_Mitchell conj_and_Co-training_variants dep_Co-training_Mitchell dep_Co-training_Blum nn_Co-training_Introduction num_Co-training_1
W03-0417	P95-1026	o	Yarowsky -LRB- 1995 -RRB- presented an approach that significantly reduces the amount of labeled data needed for word sense disambiguation	nn_disambiguation_sense nn_disambiguation_word prep_for_needed_disambiguation nsubj_needed_approach amod_data_labeled prep_of_amount_data det_amount_the dobj_reduces_amount advmod_reduces_significantly nsubj_reduces_that rcmod_approach_reduces det_approach_an ccomp_presented_needed nsubj_presented_Yarowsky appos_Yarowsky_1995
W03-0427	P95-1026	o	Not unlike -LRB- Yarowsky 1995 -RRB- we use confidence of our classifier on unannotated data to enrich itself that is by adding confidently-classified instances to the memory	det_memory_the amod_instances_confidently-classified prep_to_adding_memory dobj_adding_instances prepc_by_is_adding nsubj_is_that prepc_unlike_is_use dep_is_Yarowsky neg_is_Not dobj_enrich_itself aux_enrich_to amod_data_unannotated poss_classifier_our prep_of_confidence_classifier vmod_use_enrich prep_on_use_data dobj_use_confidence nsubj_use_we amod_Yarowsky_1995
W03-0601	P95-1026	o	1998 Traupman and Wilensky 2003 Yarowsky 1995 -RRB-	amod_Yarowsky_1995 conj_and_1998_Yarowsky conj_and_1998_2003 conj_and_1998_Wilensky conj_and_1998_Traupman
W03-1015	P95-1026	o	See Yarowsky -LRB- 1995 -RRB- for details	prep_for_Yarowsky_details appos_Yarowsky_1995 dobj_See_Yarowsky
W03-1027	P95-1026	o	In order to overcome this several methods are proposed including minimally-supervised learning methods -LRB- e.g. -LRB- Yarowsky 1995 Blum and Mitchell 1998 -RRB- -RRB- and active learning methods -LRB- e.g. -LRB- Thompson et al. 1999 Sassano 2002 -RRB- -RRB-	dep_Sassano_2002 dep_Thompson_Sassano num_Thompson_1999 dep_Thompson_al. nn_Thompson_et appos_e.g._Thompson dep_methods_e.g. nn_methods_learning amod_methods_active amod_Blum_1998 conj_and_Blum_Mitchell conj_and_Yarowsky_methods dep_Yarowsky_Mitchell dep_Yarowsky_Blum dep_Yarowsky_1995 dep_,_methods dep_,_Yarowsky dep_-LRB-_e.g. nn_methods_learning amod_methods_minimally-supervised prep_including_proposed_methods auxpass_proposed_are nsubjpass_proposed_methods advcl_proposed_overcome amod_methods_several dobj_overcome_this aux_overcome_to dep_overcome_order mark_overcome_In ccomp_``_proposed
W03-1302	P95-1026	o	Yarowsky -LRB- 1995 -RRB- used the one sense per collocation property as an essential ingredient for an unsupervised Word-SenseDisambiguationalgorithm	amod_Word-SenseDisambiguationalgorithm_unsupervised det_Word-SenseDisambiguationalgorithm_an prep_for_ingredient_Word-SenseDisambiguationalgorithm amod_ingredient_essential det_ingredient_an nn_property_collocation prep_per_sense_property num_sense_one det_sense_the prep_as_used_ingredient dobj_used_sense nsubj_used_Yarowsky appos_Yarowsky_1995
W03-1315	P95-1026	o	For the former we made use Decision Lists similar to Yarowskys method for Word Sense Disambiguation -LRB- WSD -RRB- -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_Disambiguation_Yarowsky appos_Disambiguation_WSD nn_Disambiguation_Sense nn_Disambiguation_Word prep_for_method_Disambiguation nn_method_Yarowskys prep_to_similar_method acomp_Lists_similar nsubj_Lists_Decision nn_Decision_use ccomp_made_Lists nsubj_made_we prep_for_made_former det_former_the rcmod_``_made
W03-1315	P95-1026	o	For this reason name classification has been studied in solving the named entity extraction task in the NLP and information extraction communities -LRB- see for example -LRB- Collins and Singer 1999 Cucerzan and Yarowsky 1999 -RRB- and various approaches reported in the MUC conferences -LRB- MUC-6 1995 -RRB- -RRB-	dep_MUC-6_1995 dep_conferences_MUC-6 nn_conferences_MUC det_conferences_the prep_in_reported_conferences vmod_approaches_reported amod_approaches_various dep_Cucerzan_1999 conj_and_Cucerzan_Yarowsky dep_Collins_Yarowsky dep_Collins_Cucerzan amod_Collins_1999 conj_and_Collins_Singer conj_and_see_approaches dep_see_Singer dep_see_Collins prep_for_see_example nn_communities_extraction nn_communities_information nn_communities_NLP det_communities_the conj_and_NLP_information dep_task_approaches dep_task_see prep_in_task_communities nn_task_extraction nn_task_entity amod_task_named det_task_the dobj_solving_task prepc_in_studied_solving auxpass_studied_been aux_studied_has nsubjpass_studied_classification prep_for_studied_reason nn_classification_name det_reason_this
W03-1315	P95-1026	o	In the WSD work involving the use of context we can find two approaches one that uses few strong contextual evidences for disambiguation purposes as exemplified by -LRB- Yarowsky 1995 -RRB- and the other that uses weaker evidences but considers a combination of a number of them as exemplified by -LRB- Gale et al. 1992 -RRB-	amod_Gale_1992 dep_Gale_al. nn_Gale_et prep_by_exemplified_Gale mark_exemplified_as prep_of_number_them det_number_a dep_combination_exemplified prep_of_combination_number det_combination_a dobj_considers_combination nsubj_considers_that amod_evidences_weaker conj_but_uses_considers dobj_uses_evidences nsubj_uses_that rcmod_other_considers rcmod_other_uses det_other_the dep_Yarowsky_1995 dep_by_Yarowsky prep_exemplified_by mark_exemplified_as nn_purposes_disambiguation prep_for_evidences_purposes amod_evidences_contextual amod_evidences_strong amod_evidences_few dobj_uses_evidences nsubj_uses_that conj_and_one_other advcl_one_exemplified rcmod_one_uses dep_approaches_other dep_approaches_one num_approaches_two dobj_find_approaches aux_find_can nsubj_find_we prep_in_find_work prep_of_use_context det_use_the prep_involving_work_use nn_work_WSD det_work_the
W03-1611	P95-1026	p	To solve this problem we adopt an idea one sense per collocation which was introduced in word sense disambiguation research -LRB- Yarowsky 1995 -RRB-	amod_Yarowsky_1995 dep_research_Yarowsky nn_research_disambiguation nn_research_sense nn_research_word prep_in_introduced_research auxpass_introduced_was nsubjpass_introduced_which rcmod_sense_introduced prep_per_sense_collocation num_sense_one dep_idea_sense det_idea_an dobj_adopt_idea nsubj_adopt_we advcl_adopt_solve det_problem_this dobj_solve_problem aux_solve_To
W03-1702	P95-1026	p	The approach is very general and modular and can work in conjunction with a number of learning strategies for word sense disambiguation -LRB- Yarowsky 1995 Li and Li 2002 -RRB-	amod_Li_2002 conj_and_Li_Li dep_Yarowsky_Li dep_Yarowsky_Li appos_Yarowsky_1995 dep_disambiguation_Yarowsky nn_disambiguation_sense nn_disambiguation_word amod_strategies_learning prep_for_number_disambiguation prep_of_number_strategies det_number_a prep_with_conjunction_number prep_in_work_conjunction aux_work_can nsubj_work_approach nsubj_modular_approach conj_and_general_work conj_and_general_modular advmod_general_very cop_general_is nsubj_general_approach det_approach_The ccomp_``_work ccomp_``_modular ccomp_``_general