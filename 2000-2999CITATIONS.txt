P97-1047	J93-2003	o	1.2 Decoding in Statistical Machine Translation -LRB- Brown et al. 1993 -RRB- and -LRB- Vogel Ney and Tillman 1996 -RRB- have discussed the first two of the three problems in statistical machine translation	nn_translation_machine amod_translation_statistical prep_in_problems_translation num_problems_three det_problems_the prep_of_two_problems amod_two_first det_two_the dobj_discussed_two aux_discussed_have nsubj_discussed_Tillman nsubj_discussed_Vogel nsubj_discussed_Decoding dep_Tillman_1996 conj_Vogel_Ney num_al._1993 nn_al._et amod_al._Brown nn_Translation_Machine nn_Translation_Statistical conj_and_Decoding_Tillman conj_and_Decoding_Vogel dep_Decoding_al. prep_in_Decoding_Translation num_Decoding_1.2
P97-1047	J93-2003	n	Although the authors of -LRB- Brown et al. 1993 -RRB- stated that they would discuss the search problem in a follow-up arti cle so far there have no publications devoted to the decoding issue for statistical machine translation	nn_translation_machine amod_translation_statistical prep_for_issue_translation amod_issue_decoding det_issue_the prep_to_devoted_issue vmod_publications_devoted neg_publications_no dobj_have_publications expl_have_there advmod_far_so parataxis_cle_have advmod_cle_far amod_arti_follow-up det_arti_a prep_in_problem_arti nn_problem_search det_problem_the dep_discuss_cle dobj_discuss_problem aux_discuss_would nsubj_discuss_they mark_discuss_that ccomp_stated_discuss nsubj_stated_authors mark_stated_Although num_al._1993 nn_al._et amod_al._Brown prep_of_authors_al. det_authors_the advcl_``_stated
P97-1047	J93-2003	o	We dealt with this by either limiting the translation probability from the null word -LRB- Brown 367 et al. 1993 -RRB- at the hypothetical 0-position -LRB- Brown et al. 1993 -RRB- over a threshold during the EM training or setting SHo -LRB- j -RRB- to a small probability 7r instead of 0 for the initial null hypothesis H0	nn_H0_hypothesis amod_H0_null amod_H0_initial det_H0_the conj_negcc_7r_0 nn_7r_probability amod_7r_small det_7r_a appos_SHo_j prep_for_setting_H0 prep_to_setting_0 prep_to_setting_7r dobj_setting_SHo nn_training_EM det_training_the prep_during_threshold_training det_threshold_a num_al._1993 nn_al._et amod_al._Brown dep_0-position_al. amod_0-position_hypothetical det_0-position_the nn_al._et num_al._367 dep_Brown_1993 dep_Brown_al. prep_at_word_0-position dep_word_Brown amod_word_null det_word_the nn_probability_translation det_probability_the conj_or_limiting_setting prep_over_limiting_threshold prep_from_limiting_word dobj_limiting_probability preconj_limiting_either prepc_by_dealt_setting prepc_by_dealt_limiting prep_with_dealt_this nsubj_dealt_We
P97-1063	J93-2003	o	-LRB- Macklovitch 1994 Melamed 1996b -RRB- -RRB- concordancing for bilingual lexicography -LRB- Catizone et al. 1993 Gale & Church 1991 -RRB- computerassisted language learning corpus linguistics -LRB- Melby	dep_linguistics_Melby nn_linguistics_corpus nn_learning_language amod_learning_computerassisted dep_Gale_1991 conj_and_Gale_Church dep_Catizone_Church dep_Catizone_Gale appos_Catizone_1993 dep_Catizone_al. nn_Catizone_et amod_lexicography_bilingual dep_concordancing_Catizone prep_for_concordancing_lexicography appos_Melamed_1996b appos_Macklovitch_linguistics appos_Macklovitch_learning vmod_Macklovitch_concordancing dep_Macklovitch_Melamed dep_Macklovitch_1994 dep_''_Macklovitch
P97-1063	J93-2003	o	for their models -LRB- Brown et al. 1993b -RRB-	appos_Brown_1993b dep_Brown_al. nn_Brown_et poss_models_their dep_for_Brown pobj_for_models dep_``_for
P97-1063	J93-2003	o	The co-occurrence relation can also be based on distance in a bitext space which is a more general representations of bitext correspondence -LRB- Dagan et al. 1993 Resnik & Melamed 1997 -RRB- or it can be restricted to words pairs that satisfy some matching predicate which can be extrinsic to the model -LRB- Melamed 1995 Melamed 1997 -RRB-	amod_Melamed_1997 dep_Melamed_Melamed appos_Melamed_1995 dep_model_Melamed det_model_the prep_to_extrinsic_model cop_extrinsic_be aux_extrinsic_can nsubj_extrinsic_which rcmod_predicate_extrinsic amod_predicate_matching det_predicate_some dobj_satisfy_predicate nsubj_satisfy_that rcmod_pairs_satisfy nn_pairs_words prep_to_restricted_pairs cop_restricted_be aux_restricted_can nsubj_restricted_it dep_Resnik_1997 conj_and_Resnik_Melamed conj_or_Dagan_restricted dep_Dagan_Melamed dep_Dagan_Resnik appos_Dagan_1993 dep_Dagan_al. nn_Dagan_et nn_correspondence_bitext prep_of_representations_correspondence amod_representations_general det_representations_a cop_representations_is nsubj_representations_which advmod_general_more rcmod_space_representations nn_space_bitext det_space_a parataxis_based_restricted parataxis_based_Dagan prep_in_based_space prep_on_based_distance auxpass_based_be advmod_based_also aux_based_can nsubjpass_based_relation nn_relation_co-occurrence det_relation_The advcl_``_based
P97-1063	J93-2003	o	Models of translational equivalence that are ignorant of indirect associations have a tendency to be confused by collocates -LRB- Dagan et al. 1993 -RRB-	amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et agent_confused_collocates auxpass_confused_be aux_confused_to vmod_tendency_confused det_tendency_a dep_have_Dagan dobj_have_tendency nsubj_have_Models amod_associations_indirect prep_of_ignorant_associations cop_ignorant_are nsubj_ignorant_that rcmod_equivalence_ignorant amod_equivalence_translational prep_of_Models_equivalence
P97-1063	J93-2003	o	It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values -LRB- Brown et al. 1990 Dagan et al. 1993 Chen 1996 -RRB-	amod_Chen_1996 nn_al._et nn_al._Dagan dep_Brown_Chen appos_Brown_1993 dep_Brown_al. amod_Brown_1990 dep_Brown_al. nn_Brown_et amod_values_negligible prep_to_threshold_values amod_threshold_certain det_threshold_a prep_below_probabilities_threshold det_probabilities_all dobj_sets_probabilities nsubj_sets_that rcmod_algorithms_sets nn_algorithms_induction nn_algorithms_model nn_algorithms_translation amod_algorithms_other prep_in_step_algorithms det_step_the dep_analogous_Brown prep_to_analogous_step cop_analogous_is nsubj_analogous_It
P97-1063	J93-2003	p	1 Introduction Over the past decade researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation -LRB- Brown et al. 1988 Brown et al. 1990 Brown et al. 1993a -RRB-	nn_al._et nn_al._Brown num_Brown_1990 nn_Brown_al. nn_Brown_et appos_al._1993a dep_al._al. dep_al._Brown num_al._1988 nn_al._et amod_al._Brown nn_translation_machine prep_for_models_translation amod_models_statistical amod_models_sophisticated advmod_sophisticated_increasingly dep_series_al. prep_of_series_models det_series_a dobj_developed_series aux_developed_have nsubj_developed_researchers dep_developed_Introduction prep_at_researchers_IBM amod_decade_past det_decade_the prep_over_Introduction_decade num_Introduction_1
P98-1069	J93-2003	o	This approach has also been used by -LRB- Dagan and Itai 1994 Gale et al. 1992 Shiitze 1992 Gale et al. 1993 Yarowsky 1995 Gale and Church 1Lunar is not an unknown word in English Yeltsin finds its translation in the 4-th candidate	amod_candidate_4-th det_candidate_the poss_translation_its prep_in_finds_candidate dobj_finds_translation nsubj_finds_Yeltsin parataxis_word_finds prep_in_word_English amod_word_unknown det_word_an neg_word_not cop_word_is nsubj_word_Church nsubj_word_Gale appos_Gale_1Lunar conj_and_Gale_Church num_Yarowsky_1995 num_Gale_1993 nn_Gale_al. nn_Gale_et num_Shiitze_1992 num_Gale_1992 nn_Gale_al. nn_Gale_et dep_Dagan_word conj_and_Dagan_Yarowsky conj_and_Dagan_Gale conj_and_Dagan_Shiitze conj_and_Dagan_Gale conj_and_Dagan_1994 conj_and_Dagan_Itai agent_used_Yarowsky agent_used_Gale agent_used_Shiitze agent_used_Gale agent_used_1994 agent_used_Itai agent_used_Dagan auxpass_used_been advmod_used_also aux_used_has nsubjpass_used_approach det_approach_This
P98-1069	J93-2003	o	Some of the early statistical terminology translation methods are -LRB- Brown et al. 1993 Wu and Xia 1994 Dagan and Church 1994 Gale and Church 1991 Kupiec 1993 Smadja et al. 1996 Kay and RSscheisen 1993 Fung and Church 1994 Fung 1995b -RRB-	appos_Fung_1995b num_Kay_1993 conj_and_Kay_RSscheisen num_Smadja_1996 nn_Smadja_al. nn_Smadja_et num_Kupiec_1993 dep_Gale_Fung conj_and_Gale_1994 conj_and_Gale_Church conj_and_Gale_Fung conj_and_Gale_RSscheisen conj_and_Gale_Kay conj_and_Gale_Smadja conj_and_Gale_Kupiec conj_and_Gale_1991 conj_and_Gale_Church conj_and_Dagan_Church num_Wu_1994 conj_and_Wu_Xia dep_Brown_1994 dep_Brown_Church dep_Brown_Fung dep_Brown_Kay dep_Brown_Smadja dep_Brown_Kupiec dep_Brown_1991 dep_Brown_Church dep_Brown_Gale amod_Brown_1994 dep_Brown_Church dep_Brown_Dagan dep_Brown_Xia dep_Brown_Wu amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_are_Brown nn_methods_translation nn_methods_terminology amod_methods_statistical amod_methods_early det_methods_the vmod_Some_are prep_of_Some_methods dep_``_Some
P98-1069	J93-2003	o	In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation -LRB- Brown et al. 1993 Brown et al. 1991 Gale and Church 1993 Church 1993 Simard et al. 1992 -RRB- large amount of human effort and time has been invested in collecting parallel corpora of translated texts	amod_texts_translated prep_of_corpora_texts amod_corpora_parallel dobj_collecting_corpora prepc_in_invested_collecting auxpass_invested_been aux_invested_has nsubjpass_invested_time nsubjpass_invested_Church amod_effort_human prep_of_amount_effort amod_amount_large dep_al._1992 nn_al._et nn_al._Simard conj_and_Church_time appos_Church_amount dep_Church_al. num_Church_1993 parataxis_Gale_invested tmod_Gale_1993 conj_and_Gale_Church num_Brown_1991 nn_Brown_al. nn_Brown_et dep_al._Church dep_al._Gale conj_al._Brown num_al._1993 nn_al._et amod_al._Brown nn_translation_machine conj_and_compilation_translation nn_compilation_lexicon amod_compilation_bilingual amod_models_statistical prep_for_using_translation prep_for_using_compilation dobj_using_models amod_papers_first det_papers_the dep_appearance_al. prepc_on_appearance_using prep_of_appearance_papers det_appearance_the det_years_the prep_since_In_appearance pobj_In_years dep_``_In
P98-1074	J93-2003	o	1 Introduction Early works -LRB- Gale and Church 1993 Brown et al. 1993 -RRB- and to a certain extent -LRB- Kay and R6scheisen 1993 -RRB- presented methods to ex ~ ' ~ ct bi ' _ ` i ~ gua	nn_gua_~ nn_gua_i dep___gua nn___bi dep___ct dep___~ dep___methods dep___to dep___works nn_~_ex prep_to_methods_~ amod_methods_presented num_Kay_1993 conj_and_Kay_R6scheisen amod_extent_certain det_extent_a dep_to_R6scheisen dep_to_Kay pobj_to_extent num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Gale_Brown conj_and_Gale_1993 conj_and_Gale_Church conj_and_works_methods conj_and_works_to appos_works_1993 appos_works_Church appos_works_Gale amod_works_Early nn_works_Introduction num_works_1 ccomp_``__
P98-1074	J93-2003	p	-LRB- Brown et al. 1993 -RRB- then extended their method and established a sound probabilistic model series relying on different parameters describing how words within parallel sentences are aligned to each other	det_other_each prep_to_aligned_other auxpass_aligned_are nsubjpass_aligned_words advmod_aligned_how amod_sentences_parallel prep_within_words_sentences ccomp_describing_aligned vmod_parameters_describing amod_parameters_different prep_on_relying_parameters nn_series_model amod_series_probabilistic amod_series_sound det_series_a vmod_established_relying dobj_established_series nsubj_established_Brown poss_method_their conj_and_extended_established dobj_extended_method advmod_extended_then nsubj_extended_Brown amod_Brown_1993 dep_Brown_al. nn_Brown_et
P98-1074	J93-2003	o	On the other hand -LRB- Dagan et al. 1993 -RRB- proposed an algorithm borrowed to the field of dynamic programming and based on the output of their previous work to find the best alignment subject to certain constraints between words in parallel sentences	amod_sentences_parallel prep_in_words_sentences amod_constraints_certain prep_to_subject_constraints prep_between_alignment_words amod_alignment_subject amod_alignment_best det_alignment_the dobj_find_alignment aux_find_to amod_work_previous poss_work_their prep_of_output_work det_output_the pobj_on_output pcomp_based_on amod_programming_dynamic prep_of_field_programming det_field_the conj_and_borrowed_based prep_to_borrowed_field vmod_algorithm_find dep_algorithm_based dep_algorithm_borrowed det_algorithm_an dobj_proposed_algorithm nsubj_proposed_Dagan prep_on_proposed_hand amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et amod_hand_other det_hand_the
P98-2158	J93-2003	n	-LRB- Vogel et al. 1996 -RRB- report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_of_al. amod_Model_of num_Model_2 prep_to_comparison_Model nn_model_alignment amod_model_HMMbased poss_model_their nn_Corpus_Verbmobil det_Corpus_the prep_with_results_model prep_on_results_Corpus nsubj_results_perplexity amod_perplexity_better prep_in_report_comparison dep_report_results dep_report_Vogel amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et
P98-2158	J93-2003	o	960 1.2 Alignment with Mixture Distribution Several papers have discussed the first issue especially the problem of word alignments for bilingual corpora -LRB- Brown et al. 1993 -RRB- -LRB- Dagan et al. 1993 -RRB- -LRB- Kay and RSscheisen 1993 -RRB- -LRB- Fung and Church 1994 -RRB- -LRB- Vogel et al. 1996 -RRB-	amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_Fung_1994 conj_and_Fung_Church amod_Kay_1993 conj_and_Kay_RSscheisen amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et dep_al._1993 nn_al._et amod_al._Brown amod_corpora_bilingual prep_for_alignments_corpora nn_alignments_word dep_problem_al. prep_of_problem_alignments det_problem_the advmod_problem_especially amod_issue_first det_issue_the dobj_discussed_issue aux_discussed_have nsubj_discussed_papers amod_papers_Several rcmod_Distribution_discussed nn_Distribution_Mixture appos_Alignment_Vogel appos_Alignment_Church appos_Alignment_Fung appos_Alignment_RSscheisen appos_Alignment_Kay appos_Alignment_Dagan appos_Alignment_problem prep_with_Alignment_Distribution num_Alignment_1.2 num_Alignment_960 dep_``_Alignment
P98-2158	J93-2003	o	In our search procedure we use a mixture-based alignment model that slightly differs from the model introduced as Model 2 in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. amod_Model_in num_Model_2 prep_as_introduced_Model vmod_model_introduced det_model_the prep_from_differs_model advmod_differs_slightly nsubj_differs_that rcmod_model_differs nn_model_alignment amod_model_mixture-based det_model_a dobj_use_model nsubj_use_we prep_in_use_procedure nn_procedure_search poss_procedure_our
P98-2158	J93-2003	o	It assumes that the distance of the positions relative to the diagonal of the -LRB- j i -RRB- plane is the dominating factor r -LRB- i _ j I -RRB- p -LRB- ilj J I -RRB- = -LRB- 7 -RRB- Ei = l r -LRB- i ' j -RRB- As described in -LRB- Brown et al. 1993 -RRB- the EM algorithm can be used to estimate the parameters of the model	det_model_the prep_of_parameters_model det_parameters_the dobj_estimate_parameters aux_estimate_to xcomp_used_estimate auxpass_used_be aux_used_can nsubjpass_used_algorithm nn_algorithm_EM det_algorithm_the num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_As nn_j_i appos_r_j nn_r_l dep_=_r dep_=_7 appos_ilj_I appos_ilj_J rcmod_p_used dep_p_described amod_p_= appos_p_Ei amod_p_= appos_p_ilj nn_p_r nn_I_j nn_I__ nn_I_i appos_r_I dep_factor_p amod_factor_dominating det_factor_the cop_factor_is nsubj_factor_distance mark_factor_that det_j_the prep_of_diagonal_j det_diagonal_the prep_to_relative_diagonal det_positions_the dep_distance_plane appos_distance_i amod_distance_relative prep_of_distance_positions det_distance_the ccomp_assumes_factor nsubj_assumes_It
P98-2158	J93-2003	o	The underlying translation model is Model 2 from -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_from_al. prep_2_from dep_Model_2 cop_Model_is nsubj_Model_model nn_model_translation amod_model_underlying det_model_The
P98-2162	J93-2003	o	The simple model 1 -LRB- Brown et al. 1993 -RRB- for the translation of a SL sentence d = dldt in a TL sentence e = el em assumes that every TL word is generated independently as a mixture of the SL words m l P -LRB- e \ -LSB- d -RRB- ~ H ~ t -LRB- ej \ -LSB- di -RRB- -LRB- 2 -RRB- j = l i =O In the equation above t -LRB- ej \ -LSB- di -RRB- stands for the probability that ej is generated by di	agent_generated_di auxpass_generated_is nsubjpass_generated_ej dobj_generated_that rcmod_probability_generated det_probability_the prep_for_stands_probability nsubj_stands_\ dep_stands_t dep_stands_above nsubj_stands_equation mark_stands_In appos_\_di nn_\_ej det_equation_the nn_=O_i nn_=O_l dep_=_=O dep_j_stands amod_j_= dep_2_j dep_\_2 appos_\_di nn_\_ej rcmod_t_\ nn_t_~ nn_t_H nn_t_~ dep_\_t dep_\_d dep_\_e dep_P_\ nn_P_l nn_P_m nn_words_SL det_words_the prep_of_mixture_words det_mixture_a parataxis_generated_P prep_as_generated_mixture advmod_generated_independently auxpass_generated_is nsubjpass_generated_word mark_generated_that nn_word_TL det_word_every ccomp_assumes_generated nsubj_assumes_em nn_em_el ccomp_=_assumes dep_=_e prep_in_=_sentence nsubj_=_model nn_sentence_TL det_sentence_a amod_dldt_= dep_d_dldt nn_d_sentence nn_d_SL det_d_a prep_of_translation_d det_translation_the amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_for_model_translation dep_model_Brown num_model_1 amod_model_simple det_model_The ccomp_``_=
P98-2162	J93-2003	o	In the refined model 2 -LRB- Brown et al. 1993 -RRB- alignment probabilities a -LRB- ilj l m -RRB- are included to model the effect that the position of a word influences the position of its translation	poss_translation_its prep_of_position_translation det_position_the dobj_influences_position nsubj_influences_position mark_influences_that det_word_a prep_of_position_word det_position_the ccomp_effect_influences det_effect_the dobj_model_effect aux_model_to xcomp_included_model auxpass_included_are nsubjpass_included_a appos_ilj_m appos_ilj_l dep_a_ilj rcmod_probabilities_included nn_probabilities_alignment amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 amod_model_refined det_model_the dep_In_probabilities dep_In_Brown pobj_In_model dep_``_In
P98-2162	J93-2003	o	The application of this algorithm to the basic problem using a parallel bilingual corpus aligned on the sentence level is described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in auxpass_described_is nsubjpass_described_application nn_level_sentence det_level_the prep_on_aligned_level vmod_corpus_aligned amod_corpus_bilingual amod_corpus_parallel det_corpus_a dobj_using_corpus amod_problem_basic det_problem_the det_algorithm_this vmod_application_using prep_to_application_problem prep_of_application_algorithm det_application_The ccomp_``_described
P98-2221	J93-2003	o	1 Introduction Most -LRB- if not all -RRB- statistical machine translation systems employ a word-based alignment model -LRB- Brown et al. 1993 Vogel Ney and Tillman 1996 Wang and Waibel 1997 -RRB- which treats words in a sentence as independent entities and ignores the structural relationship among them	amod_relationship_structural det_relationship_the prep_among_ignores_them dobj_ignores_relationship amod_entities_independent det_sentence_a prep_as_treats_entities prep_in_treats_sentence dobj_treats_words nsubj_treats_which dep_Wang_1997 conj_and_Wang_Waibel appos_Tillman_1996 conj_and_Vogel_Tillman conj_and_Vogel_Ney num_al._1993 nn_al._et amod_al._Brown nn_model_alignment amod_model_word-based det_model_a dep_employ_Waibel dep_employ_Wang dep_employ_Tillman dep_employ_Ney dep_employ_Vogel dep_employ_al. dobj_employ_model nsubj_employ_systems mark_employ_if nn_systems_translation nn_systems_machine amod_systems_statistical det_systems_all neg_systems_not conj_and_Most_ignores rcmod_Most_treats dep_Most_employ amod_Introduction_ignores amod_Introduction_Most num_Introduction_1
P98-2221	J93-2003	o	The subset was the neighboring alignments -LRB- Brown et al. 1993 -RRB- of the Viterbi alignments discovered by Model 1 and Model 2	num_Model_2 conj_and_Model_Model num_Model_1 agent_discovered_Model agent_discovered_Model vmod_alignments_discovered nn_alignments_Viterbi det_alignments_the amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_of_alignments_alignments dep_alignments_Brown amod_alignments_neighboring det_alignments_the cop_alignments_was nsubj_alignments_subset det_subset_The
P98-2230	J93-2003	o	Estimation of the parameters has been described elsewhere -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_described_Brown advmod_described_elsewhere auxpass_described_been aux_described_has nsubjpass_described_Estimation det_parameters_the prep_of_Estimation_parameters
P98-2230	J93-2003	o	I Various models have been constructed by the IBM team -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_team_al. nn_team_IBM det_team_the agent_constructed_team auxpass_constructed_been aux_constructed_have nsubjpass_constructed_models amod_models_Various dep_models_I
P99-1027	J93-2003	o	2 Translation Model The algorithm for fast translation which has been described previously in some detail -LRB- McCarley and Roukos 1998 -RRB- and used with considerable success in TREC -LRB- Franz et al. 1999 -RRB- is a descendent of IBM Model 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_1 nn_Model_IBM dep_descendent_Brown prep_of_descendent_Model det_descendent_a cop_descendent_is nsubj_descendent_Model amod_Franz_1999 dep_Franz_al. nn_Franz_et prep_in_success_TREC amod_success_considerable prep_with_used_success nsubjpass_used_which conj_and_McCarley_1998 conj_and_McCarley_Roukos det_detail_some conj_and_described_used dep_described_1998 dep_described_Roukos dep_described_McCarley prep_in_described_detail advmod_described_previously auxpass_described_been aux_described_has nsubjpass_described_which amod_translation_fast dep_algorithm_Franz rcmod_algorithm_used rcmod_algorithm_described prep_for_algorithm_translation det_algorithm_The dep_Model_algorithm nn_Model_Translation num_Model_2
P99-1027	J93-2003	o	This model is trained on approximately 5 million sentence pairs of Hansard -LRB- Canadian parliamentary -RRB- and UN proceedings which have been aligned on a sentence-by-sentence basis by the methods of -LRB- Brown et al. 1991 -RRB- and then further aligned on a word-by-word basis by methods similar to -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_similar_to amod_methods_similar amod_basis_word-by-word det_basis_a prep_by_aligned_methods prep_on_aligned_basis advmod_aligned_further advmod_aligned_then nsubjpass_aligned_model num_al._1991 nn_al._et amod_al._Brown dep_of_al. amod_methods_of det_methods_the nn_basis_sentence-by-sentence det_basis_a agent_aligned_methods prep_on_aligned_basis auxpass_aligned_been aux_aligned_have nsubjpass_aligned_which nn_proceedings_UN dep_parliamentary_Canadian conj_and_Hansard_proceedings dep_Hansard_parliamentary rcmod_pairs_aligned prep_of_pairs_proceedings prep_of_pairs_Hansard nn_pairs_sentence num_pairs_million number_million_5 quantmod_million_approximately dep_trained_Brown conj_and_trained_aligned prep_on_trained_pairs auxpass_trained_is nsubjpass_trained_model det_model_This
W00-0507	J93-2003	o	2.2.1 The evaluator The evaluator is a function p -LRB- t \ -LSB- t ' s -RRB- which assigns to each target-text unit t an estimate of its probability given a source text s and the tokens t ' which precede t in the current translation of s Our approach to modeling this distribution is based to a large extent on that of the IBM group -LRB- Brown et al. 1993 -RRB- but it diflhrs in one significant aspect whereas the IBM model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model p -LRB- t \ -LSB- t ' -RRB- and a translation model p -LRB- t \ -LSB- s -RRB-	appos_\_s nn_\_t nn_\_p dep_model_\ nn_model_translation det_model_a conj_and_\_model appos_\_t nn_\_t dep_p_model dep_p_\ nn_p_model nn_p_language det_p_a prep_from_predictions_p amod_predictions_separate prep_of_combination_predictions amod_combination_linear det_combination_a dobj_use_combination nsubj_use_we ccomp_use_p nn_decomposition_channel amod_channel_noisy det_channel_a dobj_involves_decomposition nsubj_involves_model mark_involves_whereas nn_model_IBM det_model_the amod_aspect_significant num_aspect_one prep_in_diflhrs_aspect nsubj_diflhrs_it num_al._1993 nn_al._et amod_al._Brown nn_group_IBM det_group_the prep_of_that_group prep_on_extent_that amod_extent_large det_extent_a conj_but_based_diflhrs dep_based_al. prep_to_based_extent auxpass_based_is nsubjpass_based_distribution det_distribution_this rcmod_approach_diflhrs rcmod_approach_based prep_to_approach_modeling poss_approach_Our prep_of_translation_s amod_translation_current det_translation_the dobj_precede_t nsubj_precede_which prep_in_t_translation rcmod_t_precede dep_tokens_t det_tokens_the conj_and_s_tokens nn_s_text nn_s_source det_s_a pobj_given_tokens pobj_given_s poss_probability_its prep_estimate_given prep_of_estimate_probability det_estimate_an nn_t_unit nn_t_target-text det_t_each dobj_assigns_estimate prep_to_assigns_t nsubj_assigns_which dep_t_s dep_\_approach rcmod_\_assigns dep_\_t nn_\_t advcl_p_involves dep_p_\ nn_p_function det_p_a cop_p_is nsubj_p_2.2.1 det_evaluator_The dep_evaluator_evaluator det_evaluator_The dep_2.2.1_evaluator
W00-0507	J93-2003	o	Both models are based on IBM translation model 2 -LRB- Brown et al. 1993 -RRB- which has the 49 property that it generates tokens independently	advmod_generates_independently dobj_generates_tokens nsubj_generates_it mark_generates_that ccomp_property_generates num_property_49 det_property_the dobj_has_property nsubj_has_which rcmod_Brown_has amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_translation nn_model_IBM dep_based_Brown prep_on_based_model auxpass_based_are nsubjpass_based_models det_models_Both
W00-0507	J93-2003	o	This formula follows the convention of -LRB- Brown et al. 1993 -RRB- in letting so designate the null state	amod_state_null det_state_the dobj_designate_state advmod_designate_so ccomp_letting_designate num_al._1993 nn_al._et amod_al._Brown prep_of_convention_al. det_convention_the prepc_in_follows_letting dobj_follows_convention nsubj_follows_formula det_formula_This
W00-0508	J93-2003	o	In -LRB- Knight and A1-Onaizan 1998 -RRB- finite-state machine translation is based on -LRB- Brown et al. 1993 -RRB- and is used for decoding the target language string	nn_string_language nn_string_target det_string_the dobj_decoding_string prepc_for_used_decoding auxpass_used_is nsubjpass_used_translation num_al._1993 nn_al._et amod_al._Brown dep_on_al. conj_and_based_used advmod_based_on auxpass_based_is nsubjpass_based_translation prep_based_In nn_translation_machine amod_translation_finite-state dep_Knight_1998 conj_and_Knight_A1-Onaizan dep_In_A1-Onaizan dep_In_Knight
W00-0508	J93-2003	o	The statistical machine translation approach is based on the noisy channel paradigm and the Maximum-A-Posteriori decoding algorithm -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_algorithm_al. nn_algorithm_decoding nn_algorithm_Maximum-A-Posteriori det_algorithm_the conj_and_paradigm_algorithm nn_paradigm_channel amod_paradigm_noisy det_paradigm_the prep_on_based_algorithm prep_on_based_paradigm auxpass_based_is nsubjpass_based_approach nn_approach_translation nn_approach_machine amod_approach_statistical det_approach_The ccomp_``_based
W00-0508	J93-2003	o	The sequence Ws is thought as a noisy version of WT and the best guess I -RRB- d ~ is then computed as ^ W ~ = argmax P -LRB- WTWs -RRB- wT = argmax P -LRB- WslWT -RRB- P -LRB- WT -RRB- -LRB- 1 -RRB- wT In -LRB- Brown et al. 1993 -RRB- they propose a method for maximizing P -LRB- WTIWs -RRB- by estimating P -LRB- WT -RRB- and P -LRB- WsIWT -RRB- and solving the problem in equation 1	num_equation_1 det_problem_the prep_in_solving_equation dobj_solving_problem appos_P_WsIWT conj_and_P_P appos_P_WT conj_and_estimating_solving dobj_estimating_P dobj_estimating_P appos_P_WTIWs prepc_by_maximizing_solving prepc_by_maximizing_estimating dobj_maximizing_P prepc_for_method_maximizing det_method_a dobj_propose_method nsubj_propose_they mark_propose_In num_al._1993 nn_al._et amod_al._Brown dep_In_al. dep_wT_propose dep_wT_1 nn_wT_P appos_P_WT nn_P_P appos_P_WslWT nn_P_argmax dep_=_wT amod_wT_= nn_wT_P appos_P_WTWs nn_P_argmax dep_=_wT amod_~_= nn_~_W nn_~_^ prep_as_computed_~ advmod_computed_then auxpass_computed_is nn_~_d nn_~_I dep_~_guess amod_guess_best det_guess_the conj_and_WT_~ prep_of_version_~ prep_of_version_WT amod_version_noisy det_version_a ccomp_thought_computed prep_as_thought_version auxpass_thought_is nsubjpass_thought_Ws nn_Ws_sequence det_Ws_The dep_``_thought
W00-0508	J93-2003	o	Our approach to statistical machine translation differs from the model proposed in -LRB- Brown et al. 1993 -RRB- in that We compute the joint model P -LRB- Ws WT -RRB- from the bilanguage corpus to account for the direct mapping of the source sentence Ws into the target sentence I?VT that is ordered according to the source language word order	nn_order_word nn_order_language nn_order_source det_order_the pobj_ordered_order prepc_according_to_ordered_to auxpass_ordered_is nsubjpass_ordered_that rcmod_I?VT_ordered nn_I?VT_sentence nn_I?VT_target det_I?VT_the nn_Ws_sentence nn_Ws_source det_Ws_the prep_into_mapping_I?VT prep_of_mapping_Ws amod_mapping_direct det_mapping_the prep_for_account_mapping aux_account_to nn_corpus_bilanguage det_corpus_the appos_Ws_WT dep_P_Ws nn_P_model amod_P_joint det_P_the vmod_compute_account prep_from_compute_corpus dobj_compute_P nsubj_compute_We num_al._1993 nn_al._et amod_al._Brown prep_in_in_that dep_in_al. prep_proposed_in vmod_model_proposed det_model_the parataxis_differs_compute prep_from_differs_model nsubj_differs_approach nn_translation_machine amod_translation_statistical prep_to_approach_translation poss_approach_Our ccomp_``_differs
W00-0707	J93-2003	p	In previous work -LRB- Foster 2000 -RRB- I described a Maximum Entropy/Minimum Divergence -LRB- MEMD -RRB- model -LRB- Berger et al. 1996 -RRB- for p -LRB- w \ -LSB- hi s -RRB- which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_translation nn_model_IBM amod_model_well-known det_model_the prep_of_analog_model det_analog_an cop_analog_is nsubj_analog_which rcmod_component_analog nn_component_translation det_component_a dep_model_Brown conj_and_model_component nn_model_language nn_model_trigram det_model_a dobj_incorporates_component dobj_incorporates_model nsubj_incorporates_which dep_hi_s nn_\_w nn_\_p amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_for_model_\ dep_model_Berger nn_model_Divergence appos_Divergence_MEMD nn_Divergence_Entropy/Minimum nn_Divergence_Maximum det_Divergence_a dep_described_incorporates dep_described_hi dobj_described_model nsubj_described_I dep_described_Foster prep_in_described_work dep_Foster_2000 amod_work_previous
W00-0707	J93-2003	o	The model consists of a set of word-pair parameters p -LRB- t \ -LSB- s -RRB- and position parameters p -LRB- j \ -LSB- i / -RRB- in model 1 -LRB- IBM1 -RRB- the latter are fixed at 1 / -LRB- 1 + 1 -RRB- as each position including the empty position 0 is considered equally likely to contain a translation for w. Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus as described in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_as nn_corpus_training amod_corpus_bilingual det_corpus_a nn_algorithm_EM det_algorithm_the advcl_obtained_described prep_over_obtained_corpus prep_with_obtained_algorithm auxpass_obtained_be aux_obtained_can det_parameters_these dep_estimates_obtained prep_for_estimates_parameters dep_estimates_likely nn_likelihood_Maximum nn_likelihood_w. prep_for_translation_likelihood det_translation_a dobj_contain_translation aux_contain_to xcomp_likely_contain advmod_likely_equally dep_considered_estimates auxpass_considered_is nsubjpass_considered_position mark_considered_as num_position_0 amod_position_empty det_position_the prep_including_position_position det_position_each cc_1_+ dep_1_1 prep_at_fixed_1 auxpass_fixed_are nsubjpass_fixed_latter nsubjpass_fixed_1 det_latter_the appos_1_IBM1 rcmod_model_fixed advcl_in_considered dep_in_1 pobj_in_model appos_\_i nn_\_j nn_\_p dep_parameters_\ nn_parameters_position conj_and_\_parameters appos_\_s nn_\_t nn_\_p dep_parameters_parameters dep_parameters_\ amod_parameters_word-pair prep_of_set_parameters det_set_a parataxis_consists_in prep_of_consists_set nsubj_consists_model det_model_The
W00-0801	J93-2003	o	It is an implementation of Models 1-4 of Brown et al. \ -LSB- 1993 \ -RSB- where each of these models produces a Viterbi alignment	nn_alignment_Viterbi det_alignment_a dobj_produces_alignment nsubj_produces_each advmod_produces_where det_models_these prep_of_each_models num_\_1993 rcmod_\_produces appos_\_\ nn_\_al. nn_\_Brown nn_al._et prep_of_1-4_\ dep_Models_1-4 prep_of_implementation_Models det_implementation_an cop_implementation_is nsubj_implementation_It
W01-1208	J93-2003	o	P -LRB- d -RRB- P L -LRB- d -RRB- -LRB- 4 -RRB- Statistical approaches to language modeling have been used in much NLP research such as machine translation -LRB- Brown et al. 1993 -RRB- and speech recognition -LRB- Bahl et al. 1983 -RRB-	amod_Bahl_1983 dep_Bahl_al. nn_Bahl_et dep_recognition_Bahl nn_recognition_speech num_al._1993 nn_al._et amod_al._Brown conj_and_translation_recognition dep_translation_al. nn_translation_machine prep_such_as_research_recognition prep_such_as_research_translation nn_research_NLP amod_research_much prep_in_used_research auxpass_used_been aux_used_have nsubjpass_used_approaches nn_modeling_language prep_to_approaches_modeling amod_approaches_Statistical dep_approaches_4 nn_approaches_L appos_L_d nn_L_P nn_L_P appos_P_d
W01-1405	J93-2003	o	In order to minimize the number of decision errors at the sentence level we have to choose the sequence of target words eI1 according to the equation -LRB- Brown et al. 1993 -RRB- eI1 = argmax eI1 n Pr -LRB- eI1jfJ1 -RRB- o = argmax eI1 n Pr -LRB- eI1 -RRB- Pr -LRB- fJ1 jeI1 -RRB- o Here the posterior probability Pr -LRB- eI1jfJ1 -RRB- is decomposed into the language model probability Pr -LRB- eJ1 -RRB- and the string translation probability Pr -LRB- fJ1 jeI1 -RRB-	nn_jeI1_fJ1 appos_Pr_jeI1 nn_Pr_probability nn_Pr_translation nn_Pr_string det_Pr_the conj_and_Pr_Pr appos_Pr_eJ1 dep_probability_Pr dep_probability_Pr dep_model_probability dep_language_model dep_the_language prep_into_decomposed_the auxpass_decomposed_is nsubjpass_decomposed_o appos_Pr_eI1jfJ1 nn_Pr_probability amod_Pr_posterior det_Pr_the appos_o_Pr advmod_o_Here nn_o_Pr amod_o_= nn_o_o amod_o_= nn_o_eI1 nn_jeI1_fJ1 appos_Pr_jeI1 nn_Pr_Pr appos_Pr_eI1 nn_Pr_n nn_Pr_eI1 nn_Pr_argmax nn_o_Pr appos_Pr_eI1jfJ1 nn_Pr_n nn_Pr_eI1 nn_Pr_argmax num_al._1993 nn_al._et amod_al._Brown det_equation_the nn_eI1_words nn_eI1_target prep_of_sequence_eI1 det_sequence_the pobj_choose_equation prepc_according_to_choose_to dobj_choose_sequence aux_choose_to parataxis_have_decomposed dep_have_al. xcomp_have_choose nsubj_have_we advcl_have_minimize nn_level_sentence det_level_the prep_at_errors_level nn_errors_decision prep_of_number_errors det_number_the dobj_minimize_number aux_minimize_to dep_minimize_order mark_minimize_In
W01-1405	J93-2003	o	Models describing these types of dependencies are referred to as alignment mappings -LRB- Brown et al. 1993 -RRB- alignment mapping j i = aj which assigns a source word fj in position j to a target word ei in position i = aj	dep_=_aj advmod_=_i npadvmod_=_position prep_in_ei_= nn_ei_word nn_ei_target det_ei_a nn_j_position nn_fj_word nn_fj_source det_fj_a prep_to_assigns_ei prep_in_assigns_j dobj_assigns_fj nsubj_assigns_which rcmod_aj_assigns amod_aj_= dep_=_i dep_=_j dep_mapping_aj nn_mapping_alignment dep_1993_al. num_Brown_1993 nn_Brown_et appos_mappings_Brown nn_mappings_alignment dep_referred_mapping prep_as_to_referred_mappings auxpass_referred_are nsubjpass_referred_Models prep_of_types_dependencies det_types_these dobj_describing_types vmod_Models_describing
W01-1405	J93-2003	o	As a result the string translation probability can be decomposed into a lexicon probability and an alignment probability -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown appos_probability_al. nn_probability_alignment det_probability_an nn_probability_lexicon det_probability_a conj_and_decomposed_probability prep_into_decomposed_probability auxpass_decomposed_be aux_decomposed_can nsubjpass_decomposed_probability prep_as_decomposed_result nn_probability_translation nn_probability_string det_probability_the det_result_a advcl_``_probability advcl_``_decomposed
W01-1405	J93-2003	p	3 Experimental Results Whereas stochastic modelling is widely used in speech recognition there are so far only a few research groups that apply stochastic modelling to language translation -LRB- Berger et al. 1994 Brown et al. 1993 Knight 1999 -RRB-	num_Knight_1999 num_al._1993 dep_Brown_al. nn_Brown_et num_al._1994 dep_Berger_Knight dep_Berger_Brown dep_Berger_al. nn_Berger_et nn_translation_language amod_modelling_stochastic prep_to_apply_translation dobj_apply_modelling nsubj_apply_that appos_groups_Berger rcmod_groups_apply nn_groups_research amod_groups_few det_groups_a advmod_groups_only advmod_far_so dep_are_groups advmod_are_far expl_are_there nsubj_are_Results nn_recognition_speech prep_in_used_recognition advmod_used_widely auxpass_used_is nsubjpass_used_modelling mark_used_Whereas amod_modelling_stochastic advcl_Results_used amod_Results_Experimental num_Results_3
W01-1407	J93-2003	o	If we assign a probability a13a15a14a17a16 a10a12a11a5a19a18a2 a3a5a21a20 to each pair of strings a16 a10 a11a5a12a22 a2a4a3a5 a20 then according to Bayes decision rule we have to choose the English string that maximizes the product of the English language model a13a23a14a24a16 a10 a11a5 a20 and the string translation model a13a15a14a17a16a25a2 a3a5a26a18a10a27a11a5a28a20 Many existing systems for statistical machine translation -LRB- Wang and Waibel 1997 Nieen et al. 1998 Och and Weber 1998 -RRB- make use of a special way of structuring the string translation model like proposed by -LRB- Brown et al. 1993 -RRB- The correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position	nn_position_word nn_position_source det_position_each nn_position_word nn_position_target num_position_one prep_to_assign_position dobj_assign_position nsubj_assign_which rcmod_alignments_assign agent_described_alignments auxpass_described_is nsubjpass_described_correspondence nn_string_target det_string_the det_source_the conj_and_words_string prep_in_words_source det_words_the prep_between_correspondence_string prep_between_correspondence_words det_correspondence_The num_al._1993 nn_al._et amod_al._Brown dep_by_al. prep_proposed_by nn_model_translation nn_model_string det_model_the prep_like_structuring_proposed dobj_structuring_model prepc_of_way_structuring amod_way_special det_way_a prep_of_use_way dobj_make_use nsubj_make_systems amod_Och_1998 conj_and_Och_Weber num_Nieen_1998 nn_Nieen_al. nn_Nieen_et dep_Wang_Weber dep_Wang_Och conj_and_Wang_Nieen dep_Wang_1997 conj_and_Wang_Waibel nn_translation_machine amod_translation_statistical appos_systems_Nieen appos_systems_Waibel appos_systems_Wang prep_for_systems_translation amod_systems_existing amod_systems_Many nn_a3a5a26a18a10a27a11a5a28a20_a13a15a14a17a16a25a2 nn_a3a5a26a18a10a27a11a5a28a20_model nn_a3a5a26a18a10a27a11a5a28a20_translation nn_a3a5a26a18a10a27a11a5a28a20_string det_a3a5a26a18a10a27a11a5a28a20_the conj_and_a20_a3a5a26a18a10a27a11a5a28a20 nn_a20_a11a5 nn_a20_a10 nn_a20_a13a23a14a24a16 nn_a20_model nn_a20_language nn_a20_English det_a20_the prep_of_product_a3a5a26a18a10a27a11a5a28a20 prep_of_product_a20 det_product_the dobj_maximizes_product nsubj_maximizes_that rcmod_string_maximizes nn_string_English det_string_the dobj_choose_string aux_choose_to parataxis_have_described parataxis_have_make xcomp_have_choose nsubj_have_we pobj_have_rule prepc_according_to_have_to advmod_have_then advcl_have_assign nn_rule_decision nn_rule_Bayes nn_a20_a2a4a3a5 nn_a20_a11a5a12a22 nn_a20_a10 amod_a20_a16 nn_a20_strings prep_of_pair_a20 det_pair_each nn_a3a5a21a20_a10a12a11a5a19a18a2 nn_a3a5a21a20_a13a15a14a17a16 nn_a3a5a21a20_probability det_a3a5a21a20_a prep_to_assign_pair dobj_assign_a3a5a21a20 nsubj_assign_we mark_assign_If
W01-1407	J93-2003	o	Table 2 summarizes the characteristics of the training corpus used for training the parameters of Model 4 proposed in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_proposed_in vmod_4_proposed dep_Model_4 prep_of_parameters_Model det_parameters_the dobj_training_parameters prepc_for_used_training vmod_corpus_used nn_corpus_training det_corpus_the prep_of_characteristics_corpus det_characteristics_the dobj_summarizes_characteristics nsubj_summarizes_Table num_Table_2
W01-1408	J93-2003	o	2 IBM Model 4 Various statistical alignment models of the form Pr -LRB- fJ1 aJ1jeI1 -RRB- have been introduced in -LRB- Brown et al. 1993 Vogel et al. 1996 Och and Ney 2000a -RRB-	appos_Och_2000a conj_and_Och_Ney dep_Vogel_Ney dep_Vogel_Och num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_al._Vogel num_al._1993 nn_al._et amod_al._Brown prep_in_introduced_al. auxpass_introduced_been aux_introduced_have nsubjpass_introduced_models dep_fJ1_aJ1jeI1 appos_Pr_fJ1 nn_Pr_form det_Pr_the prep_of_models_Pr nn_models_alignment amod_models_statistical amod_models_Various num_models_4 nn_models_Model nn_models_IBM num_models_2
W01-1408	J93-2003	o	In this paper we use the so-called Model 4 from -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_from_al. num_Model_4 amod_Model_so-called det_Model_the prep_use_from dobj_use_Model nsubj_use_we prep_in_use_paper det_paper_this
W01-1408	J93-2003	o	For a detailed description for Model 4 the reader is referred to -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_to_Brown prep_referred_to auxpass_referred_is prep_for_referred_description det_reader_the num_reader_4 nn_reader_Model prep_for_description_reader amod_description_detailed det_description_a
W01-1408	J93-2003	o	They developed a simple heuristic function for Model 2 from -LRB- Brown et al. 1993 -RRB- which was non admissible	amod_admissible_non cop_admissible_was csubj_admissible_developed num_al._1993 nn_al._et amod_al._Brown prep_from_Model_which dep_Model_al. num_Model_2 prep_for_function_Model nn_function_heuristic amod_function_simple det_function_a dobj_developed_function nsubj_developed_They
W01-1408	J93-2003	o	Many statistical translation models -LRB- Brown et al. 1993 Vogel et al. 1996 Och and Ney 2000b -RRB- try to model word-to-word correspondences between source and target words	nn_words_target conj_and_source_words prep_between_correspondences_words prep_between_correspondences_source amod_correspondences_word-to-word dobj_model_correspondences aux_model_to xcomp_try_model nsubj_try_models appos_Och_2000b conj_and_Och_Ney num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_Brown_Ney dep_Brown_Och dep_Brown_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_models_Brown nn_models_translation amod_models_statistical amod_models_Many
W01-1409	J93-2003	o	-LRB- 1993 -RRB- Brown et al.	dep_Brown_al. nn_Brown_et parataxis_1993_Brown
W01-1409	J93-2003	o	input pegging a ? transfer correct partially correct b incorrect 1 raw no M4 decoding c 7 4 4 2 stemmed yes M4 decoding 8 3 4 3 stemmed no M4 decoding 13 2 0 4 raw no gloss 13 1 1 5a stemmed yes gloss 8 3 4 5b stemmed yes gloss 12 2 1 6 stemmed no gloss 11 2 2 a pegging causes the training algorithm to consider a larger search space b correct top level category but incorrect sub-category c translation by maximizing the IBM Model 4 probability of the source/translation pair -LRB- Brown et al. 1993 Brown et al. 1995 -RRB- classification might be performed by automatic procedures rather than humans	prep_than_procedures_humans advmod_procedures_rather amod_procedures_automatic agent_performed_procedures auxpass_performed_be aux_performed_might nsubjpass_performed_translation num_Brown_1995 nn_Brown_al. nn_Brown_et dep_al._Brown dep_al._1993 nn_al._et amod_al._Brown dep_pair_classification dep_pair_al. nn_pair_source/translation det_pair_the prep_of_probability_pair num_probability_4 nn_probability_Model nn_probability_IBM det_probability_the dobj_maximizing_probability prepc_by_translation_maximizing nn_translation_c amod_translation_sub-category amod_translation_incorrect nn_category_level amod_category_top amod_category_correct nn_category_b nn_category_space nn_category_search amod_category_larger det_category_a dobj_consider_category aux_consider_to nn_algorithm_training det_algorithm_the conj_but_causes_performed vmod_causes_consider dobj_causes_algorithm nsubj_causes_a amod_a_pegging dep_2_performed dep_2_causes num_2_2 number_2_11 dobj_gloss_2 neg_gloss_no dep_stemmed_gloss nsubj_stemmed_6 rcmod_1_stemmed num_1_2 number_2_12 dobj_gloss_1 advmod_gloss_yes ccomp_stemmed_gloss nsubj_stemmed_M4 dep_stemmed_no dep_stemmed_raw nsubj_stemmed_1 dep_stemmed_incorrect dep_stemmed_b dep_stemmed_correct dep_stemmed_correct num_5b_4 number_4_3 number_4_8 dobj_gloss_5b advmod_gloss_yes ccomp_stemmed_gloss vmod_5a_stemmed num_5a_1 num_5a_13 number_1_1 dobj_gloss_5a neg_gloss_no dep_gloss_raw nsubj_gloss_4 dep_gloss_0 number_0_2 number_0_13 ccomp_decoding_gloss vmod_M4_decoding neg_M4_no dobj_stemmed_M4 nsubj_stemmed_M4 advmod_stemmed_yes num_3_4 number_4_3 number_4_8 dobj_decoding_3 vmod_M4_decoding ccomp_stemmed_stemmed nsubj_stemmed_2 rcmod_4_stemmed num_4_4 number_4_7 num_c_4 dep_decoding_c dep_M4_decoding advmod_correct_partially dep_transfer_stemmed nsubj_transfer_input dobj_pegging_a vmod_input_pegging
W01-1409	J93-2003	o	We trained IBM Translation Model 4 -LRB- Brown et al. 1993 -RRB- both on our corpus alone and on the augmented corpus using the EGYPT toolkit -LRB- Knight et al. 1999 Al-Onaizan et al. 1999 -RRB- and then translated a number of texts using different translation models and different transfer methods namely glossing -LRB- replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit -RRB- and Model 4 decoding -LRB- Brown et al. 1995 Germann et al. 2001 -RRB-	nn_al._et nn_al._Germann num_al._2001 dep_al._al. num_al._1995 nn_al._et amod_al._Brown dep_decoding_al. num_decoding_4 dep_Model_decoding nn_toolkit_EGYPT det_toolkit_the prep_with_created_toolkit vmod_tables_created nn_tables_translation det_tables_the prep_from_candidate_tables amod_candidate_likely det_candidate_the advmod_likely_most nn_word_Tamil det_word_each prep_by_replacing_candidate dobj_replacing_word conj_and_glossing_Model dep_glossing_replacing advmod_glossing_namely preconj_glossing_translated preconj_glossing_on preconj_glossing_both nn_methods_transfer amod_methods_different conj_and_models_methods nn_models_translation amod_models_different dobj_using_methods dobj_using_models prep_of_number_texts det_number_a dobj_translated_number advmod_translated_then nn_al._et nn_al._Al-Onaizan amod_Knight_1999 dep_Knight_al. dep_Knight_1999 dep_Knight_al. nn_Knight_et nn_toolkit_EGYPT det_toolkit_the dobj_using_toolkit amod_corpus_augmented det_corpus_the pobj_on_corpus advmod_corpus_alone poss_corpus_our vmod_both_using conj_and_both_translated dep_both_Knight vmod_both_using conj_and_both_on prep_on_both_corpus amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_Model_Brown num_Model_4 nn_Model_Translation nn_Model_IBM xcomp_trained_Model xcomp_trained_glossing dobj_trained_Model nsubj_trained_We ccomp_``_trained
W01-1410	J93-2003	o	6 Concluding remarks Our work presents a set of improvements on previous state of the art of Grammar Association first by providing better language models to the original system described in -LRB- Vidal et al. 1993 -RRB- second by setting the technique into a rigorous statistical framework clarifying which kind of probabilities have to be estimated by association models third by developing a novel and especially adequate association model Loco C. On the other hand though experimental results are quite good we find them particularly relevant for pointing out directions to follow for further improvement of the Grammar Association technique	nn_technique_Association nn_technique_Grammar det_technique_the prep_of_improvement_technique amod_improvement_further prep_for_follow_improvement aux_follow_to vmod_directions_follow dobj_pointing_directions prt_pointing_out prepc_for_relevant_pointing advmod_relevant_particularly dep_find_relevant dobj_find_them nsubj_find_we advcl_find_good nsubj_find_C. advmod_good_quite cop_good_are nsubj_good_results mark_good_though amod_results_experimental amod_hand_other det_hand_the prep_on_C._hand nn_C._Loco nn_model_association amod_model_adequate advmod_adequate_especially conj_and_novel_model det_novel_a dobj_developing_model dobj_developing_novel nn_models_association agent_estimated_models auxpass_estimated_be aux_estimated_to xcomp_have_estimated dep_have_second prep_of_kind_probabilities det_kind_which dobj_clarifying_kind vmod_framework_clarifying amod_framework_statistical amod_framework_rigorous det_framework_a det_technique_the prep_into_setting_framework dobj_setting_technique prepc_by_second_setting amod_Vidal_1993 dep_Vidal_al. nn_Vidal_et dep_in_Vidal prep_described_in vmod_system_described amod_system_original det_system_the nn_models_language amod_models_better prep_to_providing_system dobj_providing_models dep_first_find prepc_by_first_developing conj_first_third conj_first_have prepc_by_first_providing nn_Association_Grammar prep_of_art_Association det_art_the prep_of_state_art amod_state_previous prep_on_improvements_state dep_set_first prep_of_set_improvements det_set_a dobj_presents_set nsubj_presents_work poss_work_Our rcmod_remarks_presents amod_remarks_Concluding num_remarks_6 dep_``_remarks
W01-1410	J93-2003	o	However in the Grammar Association context when developing -LRB- using Bayes decomposition -RRB- the basic equations of the system presented in -LRB- Vidal et al. 1993 -RRB- it is said that the reverse model for a28 a13a37a3a38a5a39a32a21a0a35a7 does not seem to admit a simple factorization which is also correct and convenient so crude heuristics were adopted in the mathematical development of the expression to be maximized	auxpass_maximized_be aux_maximized_to det_expression_the prep_of_development_expression amod_development_mathematical det_development_the xcomp_adopted_maximized prep_in_adopted_development auxpass_adopted_were nsubjpass_adopted_equations amod_heuristics_crude advmod_crude_so nsubj_convenient_which conj_and_correct_convenient advmod_correct_also cop_correct_is nsubj_correct_which rcmod_factorization_convenient rcmod_factorization_correct amod_factorization_simple det_factorization_a dobj_admit_factorization aux_admit_to xcomp_seem_admit neg_seem_not aux_seem_does nsubj_seem_model mark_seem_that nn_a13a37a3a38a5a39a32a21a0a35a7_a28 prep_for_model_a13a37a3a38a5a39a32a21a0a35a7 amod_model_reverse det_model_the ccomp_said_seem auxpass_said_is nsubjpass_said_it dep_said_Vidal mark_said_in dep_said_presented amod_Vidal_1993 dep_Vidal_al. nn_Vidal_et vmod_system_said det_system_the appos_equations_heuristics prep_of_equations_system amod_equations_basic det_equations_the nn_decomposition_Bayes parataxis_using_adopted dobj_using_decomposition dep_developing_using advmod_developing_when advcl_,_developing nn_context_Association nn_context_Grammar det_context_the pobj_in_context dep_,_in dep_``_However
W01-1410	J93-2003	o	Moreover it was -LRB- without imposing determinism -RRB- the inference technique employed in -LRB- Vidal et al. 1993 -RRB-	amod_Vidal_1993 dep_Vidal_al. nn_Vidal_et dep_in_Vidal prep_employed_in vmod_technique_employed nn_technique_inference det_technique_the dobj_imposing_determinism dep_without_technique pcomp_without_imposing dep_was_without rcmod_it_was dep_,_it dep_``_Moreover
W01-1410	J93-2003	o	We based our design on the IBM models 1 and 2 -LRB- Brown et al. 1993 -RRB- but taking into account that our model must generate correct derivations in a given grammar not any seBEGIN some END <animals> eat <animals> -LRB- a -RRB- some a88 animalsa89 eat a88 animalsa89 BEGIN some END <animals> eat are <animals> dangerous -LRB- b -RRB- some a88 animalsa89 are dangerous BEGIN <animals> some END eat are <animals> dangerous -LRB- c -RRB- a88 animalsa89 are dangerous BEGIN snakes rats people some END eat are snakes rats people dangerous -LRB- d -RRB- Expansion of a88 animalsa89 Figure 3 Using a category a86 animalsa87 for snakes rats and people in the example of Figure 1	num_Figure_1 prep_of_example_Figure det_example_the prep_in_people_example nn_animalsa87_a86 nn_animalsa87_category det_animalsa87_a dep_Using_people cc_Using_and dobj_Using_rats prep_for_Using_snakes dobj_Using_animalsa87 num_Figure_3 nn_Figure_animalsa89 nn_Figure_a88 prep_of_Expansion_Figure dep_Expansion_d amod_Expansion_dangerous dep_people_Expansion nn_people_rats nn_people_snakes cop_people_are nsubj_people_people nsubj_eat_END det_END_some rcmod_people_eat nn_people_rats ccomp_snakes_people nsubj_snakes_BEGIN dep_snakes_dangerous cop_dangerous_are nsubj_dangerous_animalsa89 cop_dangerous_are nsubj_dangerous_END advmod_dangerous_<animals> nn_animalsa89_a88 appos_dangerous_c dep_<animals>_dangerous nsubj_are_<animals> dep_END_eat det_END_some ccomp_BEGIN_snakes xcomp_dangerous_BEGIN cop_dangerous_are nsubj_dangerous_animalsa89 cop_dangerous_are csubj_dangerous_BEGIN dep_dangerous_eat dep_dangerous_eat nsubj_dangerous_seBEGIN neg_dangerous_not nn_animalsa89_a88 det_animalsa89_some appos_dangerous_b dep_<animals>_dangerous nsubj_are_<animals> nn_<animals>_END det_<animals>_some dep_BEGIN_eat dobj_BEGIN_<animals> nn_animalsa89_a88 dobj_eat_animalsa89 nsubj_eat_animalsa89 nn_animalsa89_a88 det_animalsa89_some appos_<animals>_a dobj_eat_<animals> nsubj_eat_<animals> nn_<animals>_END det_<animals>_some det_seBEGIN_any amod_grammar_given det_grammar_a amod_derivations_correct xcomp_generate_dangerous prep_in_generate_grammar dobj_generate_derivations aux_generate_must nsubj_generate_model dobj_generate_that poss_model_our ccomp_taking_generate prep_into_taking_account nsubj_taking_We amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_1_2 dep_models_2 dep_models_1 nn_models_IBM det_models_the prep_on_design_models poss_design_our dep_based_Using conj_but_based_taking dep_based_Brown dobj_based_design nsubj_based_We ccomp_``_taking ccomp_``_based
W01-1410	J93-2003	o	We carefully implemented the original Grammar Association system described in -LRB- Vidal et al. 1993 -RRB- tuned empirically a couple of smoothing parameters trained the models and finally obtained an a119a21a120 a100 a104a122a121 of correct translations .9 Then we studied the impact of -LRB- 1 -RRB- sorting as proposed in Section 3 the set of sentences presented to ECGI -LRB- 2 -RRB- making language models deterministic and minimum -LRB- 3 -RRB- constraining the best translation search to those sentences whose lengths have been seen in the training set related to the length of the input sentence	nn_sentence_input det_sentence_the prep_of_length_sentence det_length_the prep_to_related_length amod_set_related nn_set_training det_set_the prep_in_seen_set auxpass_seen_been aux_seen_have nsubjpass_seen_lengths poss_lengths_whose rcmod_sentences_seen det_sentences_those prep_to_search_sentences nn_search_translation amod_search_best det_search_the dobj_constraining_search nsubj_deterministic_models nn_models_language conj_and_making_minimum xcomp_making_deterministic dep_2_minimum dep_2_making prep_to_presented_ECGI vmod_sentences_presented prep_of_set_sentences det_set_the num_Section_3 prep_in_proposed_Section mark_proposed_as appos_sorting_set advcl_sorting_proposed dep_sorting_1 prep_impact_of det_impact_the dobj_studied_impact nsubj_studied_we num_translations_.9 amod_translations_correct prep_of_a104a122a121_translations nn_a104a122a121_a100 nn_a104a122a121_a119a21a120 det_a104a122a121_an advmod_obtained_Then dobj_obtained_a104a122a121 advmod_obtained_finally det_models_the conj_and_trained_obtained dobj_trained_models amod_parameters_smoothing prep_of_couple_parameters det_couple_a advmod_couple_empirically dobj_tuned_couple vmod_Vidal_constraining dep_Vidal_3 dep_Vidal_2 dep_Vidal_sorting rcmod_Vidal_studied dep_Vidal_obtained dep_Vidal_trained vmod_Vidal_tuned amod_Vidal_1993 dep_Vidal_al. nn_Vidal_et prep_in_described_Vidal vmod_system_described nn_system_Association nn_system_Grammar amod_system_original det_system_the dobj_implemented_system advmod_implemented_carefully nsubj_implemented_We ccomp_``_implemented
W01-1413	J93-2003	o	One interesting approach to extending the current system is to introduce a statistical translation model -LRB- Brown et al. 1993 -RRB- to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long English sequence as the translation by locally aligning the Japanese and English sequences	amod_sequences_English amod_sequences_Japanese det_sequences_the conj_and_Japanese_English dobj_aligning_sequences advmod_aligning_locally det_translation_the amod_sequence_English amod_sequence_long det_sequence_a amod_subpart_appropriate det_subpart_the advmod_appropriate_most prep_from_extract_sequence dobj_extract_subpart aux_extract_to nn_candidates_translation amod_candidates_irrelevant prepc_by_out_aligning prep_as_out_translation conj_and_out_extract pobj_out_candidates dep_al._1993 nn_al._et amod_al._Brown nn_model_translation amod_model_statistical det_model_a advmod_introduce_extract advmod_introduce_out prep_to_introduce_filter dep_introduce_al. dobj_introduce_model aux_introduce_to xcomp_is_introduce nsubj_is_approach amod_system_current det_system_the dobj_extending_system prepc_to_approach_extending amod_approach_interesting num_approach_One ccomp_``_is
W02-1012	J93-2003	o	-LRB- 1993 -RRB- and the HMM alignment model of -LRB- Vogel et al. 1996 -RRB-	amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_of_Vogel prep_model_of nn_model_alignment nn_model_HMM det_model_the conj_and_1993_model dep_''_model dep_''_1993
W02-1012	J93-2003	o	We refer to a3a16a5a7 as the source language string and a10 a11a7 as the target language string in accordance with the noisy channel terminology used in the IBM models of -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_of_al. amod_models_of nn_models_IBM det_models_the prep_in_used_models vmod_terminology_used nn_terminology_channel amod_terminology_noisy det_terminology_the nn_string_language nn_string_target det_string_the nn_a11a7_a10 prep_as_string_string conj_and_string_a11a7 nn_string_language nn_string_source det_string_the prep_in_accordance_with_a3a16a5a7_terminology prep_as_a3a16a5a7_a11a7 prep_as_a3a16a5a7_string aux_a3a16a5a7_to xcomp_refer_a3a16a5a7 nsubj_refer_We
W02-1018	J93-2003	o	Intuitively if we allow any Source words to be aligned to any Target words the best alignment that we can come up with is the one in Figure 1.c Sentence pair -LRB- S2 T2 -RRB- offers strong evidence that b c in language S means the same thing as x in language T. On the basis of this evidence we expect the system to also learn from sentence pair -LRB- S1 T1 -RRB- that a in language S means the same thing as y in language T. Unfortunately if one works with translation models that do not allow Target words to be aligned to more than one Source word as it is the case in the IBM models -LRB- Brown et al. 1993 -RRB- it is impossible to learn that the phrase b c in language S means the same thing as word x in language T The IBM Model 4 -LRB- Brown et al. 1993 -RRB- for example converges to the word alignments shown in Figure 1 b and learns the translation probabilities shown in Figure 1 a. 2 Since in the IBM model one can not link a Target word to more than a Source word the training procedure 2To train the IBM-4 model we used Giza -LRB- Al-Onaizan et al. 1999 -RRB-	amod_Al-Onaizan_1999 dep_Al-Onaizan_al. nn_Al-Onaizan_et dep_Giza_Al-Onaizan dobj_used_Giza nsubj_used_we dep_used_train nsubj_used_a. nn_model_IBM-4 det_model_the dep_train_model nn_train_2To nn_train_procedure nn_train_training det_train_the nn_word_Source det_word_a prep_than_more_word nn_word_Target det_word_a prep_to_link_more dobj_link_word neg_link_not aux_link_can nsubj_link_one rcmod_model_link nn_model_IBM det_model_the pobj_in_model pcomp_Since_in prep_a._Since num_a._2 num_Figure_1 prep_in_shown_Figure vmod_probabilities_shown nn_probabilities_translation det_probabilities_the dobj_learns_probabilities nsubj_learns_Model num_Figure_1 prep_in_shown_Figure vmod_alignments_shown nn_alignments_word det_alignments_the conj_and_converges_learns dobj_converges_b prep_to_converges_alignments prep_for_converges_example nsubj_converges_Model amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_4 nn_Model_IBM det_Model_The nn_T_language nn_x_word prep_in_thing_T prep_as_thing_x amod_thing_same det_thing_the dobj_means_thing nsubj_means_c mark_means_that nn_S_language prep_in_c_S nn_c_b nn_c_phrase det_c_the ccomp_learn_means aux_learn_to xcomp_impossible_learn cop_impossible_is nsubj_impossible_it amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the prep_in_case_models det_case_the cop_case_is nsubj_case_it mark_case_as nn_word_Source num_word_one quantmod_one_than mwe_than_more prep_to_aligned_word auxpass_aligned_be aux_aligned_to nn_words_Target advcl_allow_case xcomp_allow_aligned dobj_allow_words neg_allow_not aux_allow_do nsubj_allow_that rcmod_models_allow nn_models_translation ccomp_works_impossible dep_works_Brown prep_with_works_models nsubj_works_one mark_works_if advmod_T._Unfortunately nn_T._language prep_in_y_T. prep_as_thing_y amod_thing_same det_thing_the advcl_means_works dobj_means_thing nsubj_means_a mark_means_that nn_S_language prep_in_a_S appos_S1_T1 dep_pair_S1 nn_pair_sentence ccomp_learn_means prep_from_learn_pair advmod_learn_also aux_learn_to det_system_the xcomp_expect_learn dobj_expect_system nsubj_expect_we det_evidence_this prep_of_basis_evidence det_basis_the nn_T._language prep_in_thing_T. prep_as_thing_x amod_thing_same det_thing_the prep_on_means_basis dobj_means_thing nsubj_means_c mark_means_that nn_S_language prep_in_c_S nn_c_b ccomp_evidence_means amod_evidence_strong dobj_offers_evidence nsubj_offers_pair appos_S2_T2 dep_pair_S2 nn_pair_Sentence nn_1.c_Figure parataxis_one_used parataxis_one_learns parataxis_one_converges rcmod_one_expect rcmod_one_offers prep_in_one_1.c det_one_the cop_one_is nsubj_one_alignment advcl_one_allow advmod_one_Intuitively prep_come_with prt_come_up aux_come_can nsubj_come_we mark_come_that ccomp_alignment_come amod_alignment_best det_alignment_the nn_words_Target det_words_any prep_to_aligned_words auxpass_aligned_be aux_aligned_to vmod_words_aligned nn_words_Source det_words_any dobj_allow_words nsubj_allow_we mark_allow_if
W02-1018	J93-2003	o	For example in our previous work -LRB- Marcu 2001 -RRB- we have used a statistical translation memory of phrases in conjunction with a statistical translation model -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation amod_model_statistical det_model_a prep_with_conjunction_model prep_of_memory_phrases nn_memory_translation amod_memory_statistical det_memory_a prep_in_used_conjunction dobj_used_memory aux_used_have nsubj_used_we prep_in_used_work prep_for_used_example amod_Marcu_2001 dep_work_Marcu amod_work_previous poss_work_our
W02-1018	J93-2003	o	In constrast with many previous approaches -LRB- Brown et al. 1993 Och et al. 1999 Yamada and Knight 2001 -RRB- our model does not try to capture how Source sentences can be mapped into Target sentences but rather how Source and Target sentences can be generated simultaneously	advmod_generated_simultaneously auxpass_generated_be aux_generated_can nsubjpass_generated_sentences nsubjpass_generated_Source nn_sentences_Target conj_and_Source_sentences advmod_Source_how advmod_Source_rather nn_sentences_Target conj_but_mapped_generated prep_into_mapped_sentences auxpass_mapped_be aux_mapped_can nsubjpass_mapped_sentences advmod_mapped_how nn_sentences_Source ccomp_capture_generated ccomp_capture_mapped aux_capture_to xcomp_try_capture neg_try_not aux_try_does nsubj_try_model prep_in_try_constrast poss_model_our dep_Yamada_2001 conj_and_Yamada_Knight nn_al._et nn_al._Och dep_Brown_Knight dep_Brown_Yamada amod_Brown_1999 dep_Brown_al. amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_approaches_Brown amod_approaches_previous amod_approaches_many prep_with_constrast_approaches
W02-1018	J93-2003	o	1 Motivation Most of the noisy-channel-based models used in statistical machine translation -LRB- MT -RRB- -LRB- Brown et al. 1993 -RRB- are conditional probability models	nn_models_probability amod_models_conditional cop_models_are nsubj_models_Motivation amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_translation_MT nn_translation_machine amod_translation_statistical prep_in_used_translation vmod_models_used amod_models_noisy-channel-based det_models_the prep_of_Most_models dep_Motivation_Brown amod_Motivation_Most num_Motivation_1
W02-1018	J93-2003	o	A variety of methods are used to account for the re-ordering stage word-based -LRB- Brown et al. 1993 -RRB- templatebased -LRB- Och et al. 1999 -RRB- and syntax-based -LRB- Yamada and Knight 2001 -RRB- to name just a few	det_few_a advmod_few_just dobj_name_few aux_name_to amod_Yamada_2001 conj_and_Yamada_Knight dep_al._1999 nn_al._et advmod_Och_al. dep_al._1993 nn_al._et advmod_Brown_al. vmod_word-based_name dep_word-based_Knight dep_word-based_Yamada conj_and_word-based_syntax-based dep_word-based_Och dep_word-based_templatebased dep_word-based_Brown amod_stage_re-ordering det_stage_the prep_for_account_stage aux_account_to dep_used_syntax-based dep_used_word-based xcomp_used_account auxpass_used_are nsubjpass_used_variety prep_of_variety_methods det_variety_A
W02-1019	J93-2003	o	2 Word-to-Word Bitext Alignment We will study the problem of aligning an English sentence to a French sentence and we will use the word alignment of the IBM statistical translation models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation amod_models_statistical nn_models_IBM det_models_the dep_alignment_Brown prep_of_alignment_models nn_alignment_word det_alignment_the dobj_use_alignment aux_use_will nsubj_use_we amod_sentence_French det_sentence_a prep_to_sentence_sentence amod_sentence_English det_sentence_an dobj_aligning_sentence prepc_of_problem_aligning det_problem_the dobj_study_problem aux_study_will nsubj_study_We conj_and_Alignment_use rcmod_Alignment_study nn_Alignment_Bitext amod_Alignment_Word-to-Word num_Alignment_2
W02-1019	J93-2003	o	5.4 IBM-3 Word Alignment Models Since the true distribution over alignments is not known we used the IBM-3 statistical translation model -LRB- Brown et al. 1993 -RRB- to approximate This model is specified through four components Fertility probabilities for words Fertility probabilities for NULL Word Translation probabilities and Distortion probabilities	nn_probabilities_Distortion nn_probabilities_Translation nn_probabilities_Word prep_for_probabilities_NULL nn_probabilities_Fertility conj_and_probabilities_probabilities conj_and_probabilities_probabilities conj_and_probabilities_probabilities prep_for_probabilities_words nn_probabilities_Fertility num_components_four dep_specified_probabilities dep_specified_probabilities dep_specified_probabilities dep_specified_probabilities prep_through_specified_components auxpass_specified_is nsubjpass_specified_model det_model_This dep_al._1993 nn_al._et amod_al._Brown dep_model_al. nn_model_translation amod_model_statistical nn_model_IBM-3 det_model_the parataxis_used_specified prep_to_used_approximate dobj_used_model nsubj_used_we advcl_used_known nsubj_used_Models neg_known_not auxpass_known_is nsubjpass_known_distribution mark_known_Since prep_over_distribution_alignments amod_distribution_true det_distribution_the nn_Models_Alignment nn_Models_Word nn_Models_IBM-3 num_Models_5.4
W02-1020	J93-2003	o	The translation component is an analog of the IBM model 2 -LRB- Brown et al. 1993 -RRB- with parameters that are optimized for use with the trigram	det_trigram_the prep_with_use_trigram prep_for_optimized_use auxpass_optimized_are nsubjpass_optimized_that rcmod_parameters_optimized amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_IBM det_model_the prep_with_analog_parameters dep_analog_Brown prep_of_analog_model det_analog_an cop_analog_is nsubj_analog_component nn_component_translation det_component_The
W02-1022	J93-2003	o	Using alignment for grammar and lexicon induction has been an active area of research both in monolingual settings -LRB- van Zaanen 2000 -RRB- and in machine translation -LRB- MT -RRB- -LRB- Brown et al. 1993 Melamed 2000 Och and Ney 2000 -RRB- | interestingly statistical MT techniques have been used to derive lexico-semantic mappings in the \ reverse direction of language understanding rather than generation -LRB- Papineni et al. 1997 Macherey et al. 2001 -RRB-	num_Macherey_2001 nn_Macherey_al. nn_Macherey_et dep_Papineni_Macherey appos_Papineni_1997 dep_Papineni_al. nn_Papineni_et nn_understanding_language dep_direction_Papineni conj_negcc_direction_generation prep_of_direction_understanding amod_reverse_\ det_reverse_the prep_in_mappings_reverse amod_mappings_lexico-semantic dobj_derive_mappings aux_derive_to xcomp_used_derive auxpass_used_been aux_used_have nsubjpass_used_Brown prep_in_used_translation prep_in_used_settings preconj_used_both nn_techniques_MT amod_techniques_statistical advmod_|_interestingly number_|_2000 dep_|_Ney dep_|_Och conj_and_Och_Ney num_Melamed_2000 appos_Brown_techniques dep_Brown_| dep_Brown_Melamed amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_translation_MT nn_translation_machine dep_Zaanen_2000 nn_Zaanen_van conj_and_settings_translation appos_settings_Zaanen amod_settings_monolingual dep_area_generation dep_area_direction rcmod_area_used prep_of_area_research amod_area_active det_area_an cop_area_been aux_area_has csubj_area_Using nn_induction_lexicon nn_induction_grammar conj_and_grammar_lexicon prep_for_alignment_induction dobj_Using_alignment
W02-1039	J93-2003	o	When an S alignment exists there will always also exist a P alignment such that P a65 S The English sentences were parsed using a state-of-the-art statistical parser -LRB- Charniak 2000 -RRB- trained on the University of Pennsylvania Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus dep_Treebank_University prep_of_University_Pennsylvania det_University_the prep_on_trained_Treebank dep_Charniak_2000 vmod_parser_trained appos_parser_Charniak amod_parser_statistical amod_parser_state-of-the-art det_parser_a dobj_using_parser xcomp_parsed_using auxpass_parsed_were nsubjpass_parsed_sentences amod_sentences_English det_sentences_The nn_S_a65 dep_S_P dep_S_that dep_S_such dep_alignment_parsed dep_alignment_S nn_alignment_P det_alignment_a dobj_exist_alignment advmod_exist_also advmod_exist_always aux_exist_will expl_exist_there advcl_exist_exists nsubj_exists_alignment advmod_exists_When nn_alignment_S det_alignment_an
W02-1039	J93-2003	o	The first work in SMT done at IBM -LRB- Brown et al. 1993 -RRB- developed a noisy-channel model factoring the translation process into two portions the translation model and the language model	nn_model_language det_model_the conj_and_model_model nn_model_translation det_model_the num_portions_two nn_process_translation det_process_the dep_factoring_model dep_factoring_model prep_into_factoring_portions dobj_factoring_process amod_model_noisy-channel det_model_a dobj_developed_model amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_IBM_Brown dep_done_factoring dep_done_developed prep_at_done_IBM nsubj_done_work prep_in_work_SMT amod_work_first det_work_The
W02-1405	J93-2003	o	2 Our statistical engine 2.1 The statistical models In this study we built an SMT engine designed to translate from French to English following the noisy-channel paradigm flrst described by -LRB- Brown et al. 1993b -RRB-	dep_al._1993b nn_al._et amod_al._Brown dep_by_al. prep_described_by vmod_flrst_described nn_flrst_paradigm amod_flrst_noisy-channel det_flrst_the prep_to_French_English prep_from_translate_French aux_translate_to xcomp_designed_translate vmod_engine_designed nn_engine_SMT det_engine_an prep_following_built_flrst dobj_built_engine nsubj_built_we nsubj_built_engine det_study_this prep_in_models_study amod_models_statistical det_models_The num_models_2.1 dep_engine_models amod_engine_statistical poss_engine_Our num_engine_2
W02-1405	J93-2003	o	Among them -LRB- Brown et al. 1993a -RRB- have proposed a way to exploit bilingual dictionnaries at training time	nn_time_training amod_dictionnaries_bilingual prep_at_exploit_time dobj_exploit_dictionnaries aux_exploit_to vmod_way_exploit det_way_a dobj_proposed_way aux_proposed_have nsubj_proposed_Brown prep_among_proposed_them amod_Brown_1993a dep_Brown_al. nn_Brown_et rcmod_``_proposed
W03-0301	J93-2003	o	Four teams had approaches that relied -LRB- to varying degrees -RRB- on an IBM model of statistical machine translation -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. nn_translation_machine amod_translation_statistical prep_of_model_translation nn_model_IBM det_model_an amod_degrees_varying prep_on_relied_model prep_to_relied_degrees nsubj_relied_that rcmod_approaches_relied dep_had_Brown dobj_had_approaches nsubj_had_teams num_teams_Four
W03-0302	J93-2003	o	ProAlign models P -LRB- A | E F -RRB- directly using a different decomposition of terms than the model used by IBM -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_IBM_Brown agent_used_IBM vmod_model_used det_model_the prep_of_decomposition_terms amod_decomposition_different det_decomposition_a prep_than_using_model dobj_using_decomposition appos_E_F num_E_| nn_E_A vmod_P_using advmod_P_directly dep_P_E dep_models_P nn_models_ProAlign
W03-0302	J93-2003	o	To avoid this problem we sample from a space of probable alignments as is done in IBM models 3 and above -LRB- Brown et al. 1993 -RRB- and weight counts based on the likelihood of each alignment sampled under the current probability model	nn_model_probability amod_model_current det_model_the prep_under_sampled_model vmod_alignment_sampled det_alignment_each prep_of_likelihood_alignment det_likelihood_the pobj_counts_likelihood prepc_based_on_counts_on dep_weight_counts dep_al._1993 nn_al._et amod_al._Brown conj_and_3_above dep_models_above dep_models_3 nn_models_IBM dep_done_al. prep_in_done_models auxpass_done_is advmod_done_as nsubjpass_done_we amod_alignments_probable prep_of_space_alignments det_space_a conj_and_sample_weight conj_and_sample_done prep_from_sample_space nsubj_sample_we advcl_sample_avoid det_problem_this dobj_avoid_problem aux_avoid_To
W03-0303	J93-2003	o	However instead of estimating the probabilities for the production rules via EM as described in -LSB- Wu 1997 -RSB- we assign the probabilities to the rules using the Model-1 statistical translation lexicon -LSB- Brown et al. 1993 -RSB-	num_al._1993 nn_al._et amod_al._Brown nn_lexicon_translation amod_lexicon_statistical nn_lexicon_Model-1 det_lexicon_the dep_using_al. dobj_using_lexicon det_rules_the det_probabilities_the dep_assign_using prep_to_assign_rules dobj_assign_probabilities nsubj_assign_we rcmod_Wu_assign num_Wu_1997 prep_in_described_Wu mark_described_as nn_rules_production det_rules_the prep_for_probabilities_rules det_probabilities_the advcl_estimating_described prep_via_estimating_EM dobj_estimating_probabilities pcomp_of_estimating advmod_of_instead ccomp_,_of dep_``_However
W03-0304	J93-2003	o	Yet the very nature of these alignments as defined in the IBM modeling approach -LRB- Brown et al. 1993 -RRB- lead to descriptions of the correspondences between sourcelanguage -LRB- SL -RRB- and target-language -LRB- TL -RRB- words of a translation that are often unsatisfactory at least from a human perspective	amod_perspective_human det_perspective_a pobj_at_least prep_from_unsatisfactory_perspective advmod_unsatisfactory_at advmod_unsatisfactory_often cop_unsatisfactory_are nsubj_unsatisfactory_that rcmod_translation_unsatisfactory det_translation_a prep_of_words_translation nn_words_target-language nn_words_sourcelanguage appos_target-language_TL conj_and_sourcelanguage_target-language appos_sourcelanguage_SL prep_between_correspondences_words det_correspondences_the prep_of_descriptions_correspondences prep_to_lead_descriptions dep_al._1993 nn_al._et amod_al._Brown nn_approach_modeling nn_approach_IBM det_approach_the dep_defined_lead dep_defined_al. prep_in_defined_approach mark_defined_as det_alignments_these dep_nature_defined prep_of_nature_alignments advmod_nature_very det_nature_the dep_Yet_nature dep_``_Yet
W03-0305	J93-2003	o	2 Word Alignment algorithm We use IBM Model 4 -LRB- Brown et al. 1993 -RRB- as a basis for our word alignment system	nn_system_alignment nn_system_word poss_system_our prep_for_basis_system det_basis_a amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM dobj_use_Model nsubj_use_We prep_as_algorithm_basis dep_algorithm_Brown rcmod_algorithm_use nn_algorithm_Alignment nn_algorithm_Word num_algorithm_2
W03-0309	J93-2003	o	-LRB- Brown et al. 1993 -RRB- introduced five statistical translation models -LRB- IBM Models 1 5 -RRB-	number_5_1 num_Models_5 nn_Models_IBM appos_models_Models nn_models_translation amod_models_statistical num_models_five dobj_introduced_models nsubj_introduced_Brown amod_Brown_1993 dep_Brown_al. nn_Brown_et
W03-0309	J93-2003	o	The Duluth Word Alignment System is a Perl implementation of IBM Model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_2 nn_Model_IBM dep_implementation_Brown prep_of_implementation_Model nn_implementation_Perl det_implementation_a cop_implementation_is nsubj_implementation_System nn_System_Alignment nn_System_Word nn_System_Duluth det_System_The
W03-0310	J93-2003	o	This cost can often be substantial as with the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the pobj_with_Treebank pcomp_as_with dep_substantial_Marcus prep_substantial_as cop_substantial_be advmod_substantial_often aux_substantial_can nsubj_substantial_cost det_cost_This
W03-0313	J93-2003	o	These methods are based on IBM statistical translation Model 2 -LRB- Brown et al. 1993 -RRB- but take advantage of certain characteristics of the segments of text that can typically be extracted from translation memories	nn_memories_translation prep_from_extracted_memories auxpass_extracted_be advmod_extracted_typically aux_extracted_can nsubjpass_extracted_that rcmod_segments_extracted prep_of_segments_text det_segments_the prep_of_characteristics_segments amod_characteristics_certain prep_of_advantage_characteristics dobj_take_advantage nsubj_take_methods amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_2 nn_Model_translation amod_Model_statistical nn_Model_IBM conj_but_based_take prep_on_based_Model auxpass_based_are nsubjpass_based_methods det_methods_These ccomp_``_take ccomp_``_based
W03-0315	J93-2003	p	2.2 Statistical Translation Lexicon We use a statistical translation lexicon known as IBM Model-1 in -LRB- Brown et al. 1993 -RRB- for both efficiency and simplicity	conj_and_efficiency_simplicity preconj_efficiency_both pobj_for_simplicity pobj_for_efficiency num_al._1993 nn_al._et amod_al._Brown pcomp_in_for dep_in_al. prep_Model-1_in nn_Model-1_IBM prep_as_known_Model-1 vmod_lexicon_known nn_lexicon_translation amod_lexicon_statistical det_lexicon_a dobj_use_lexicon nsubj_use_We rcmod_Lexicon_use nn_Lexicon_Translation amod_Lexicon_Statistical num_Lexicon_2.2 dep_``_Lexicon
W03-0315	J93-2003	o	Given training data consisting of parallel sentences -RCB- 1 -RRB- -LCB- -LRB- -RRB- -LRB- -RRB- -LRB- Sief ii = our Model-1 training for t -LRB- f | e -RRB- is as follows = = S s ss e efefceft 1 -RRB- -LRB- -RRB- -LRB- 1 -RRB- ;| -LRB- -RRB- | -LRB- Where 1 e is a normalization factor such that 0.1 -RRB- | -LRB- = j j eft -RRB- ;| -LRB- -RRB- -LRB- -RRB- -LRB- ss efefc denotes the expected number of times that word e connects to word f. == = = l i i m j jl k k ss eeff eft eft efefc 11 1 -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- ;| -LRB- With the conditional probability t -LRB- f | e -RRB- the probability for an alignment of foreign string F given English string E is in -LRB- 1 -RRB- = = + = m j n i ijm eft l EFP 1 0 -RRB- | -LRB- -RRB- 1 -LRB- 1 -RRB- | -LRB- -LRB- 1 -RRB- The probability of alignment F given E -RRB- | -LRB- EFP is shown to achieve the global maximum under this EM framework as stated in -LRB- Brown et al. ,1993 -RRB-	num_al._,1993 nn_al._et amod_al._Brown dep_in_al. prep_stated_in mark_stated_as nn_framework_EM det_framework_this prep_under_maximum_framework amod_maximum_global det_maximum_the advcl_achieve_stated dobj_achieve_maximum aux_achieve_to xcomp_shown_achieve auxpass_shown_is nsubjpass_shown_EFP dep_shown_| pobj_given_E nn_F_alignment dep_probability_shown prep_probability_given prep_of_probability_F det_probability_The dep_1_probability dep_|_1 num_|_1 num_|_1 dep_|_= dep_|_1 mark_|_in nn_|_eft nn_|_ijm nn_|_i nn_|_n nn_|_j nn_|_m number_0_1 num_EFP_0 nn_EFP_l dep_eft_EFP dep_=_| conj_+_=_= dep_=_= dep_=_= advcl_is_| nsubj_is_probability dep_is_With dep_is_;| nn_E_string nn_E_English pobj_given_E nn_F_string amod_F_foreign prep_alignment_given prep_of_alignment_F det_alignment_an prep_for_probability_alignment det_probability_the advmod_e_| nn_|_f dep_t_e nn_t_probability amod_t_conditional det_t_the pobj_With_t rcmod_|_is nn_|_| num_|_11 dep_11_1 dep_efefc_| nn_efefc_eft nn_efefc_eft nn_efefc_eeff dobj_ss_efefc nsubj_ss_k nn_k_k nn_k_jl nn_k_j nn_k_m nn_k_i nn_k_i nn_k_l amod_k_= amod_k_= dep_==_ss nn_==_f. nn_==_word prep_to_connects_== nsubj_connects_word dep_word_e det_word_that rcmod_times_connects prep_of_number_times amod_number_expected det_number_the dobj_denotes_number nsubj_denotes_efefc nn_efefc_ss nn_efefc_;| nn_eft_j nn_eft_j dobj_=_eft rcmod_|_denotes dep_|_= num_|_0.1 dep_that_| prep_such_that amod_factor_such nn_factor_normalization det_factor_a cop_factor_is dep_is_e advmod_is_Where num_e_1 dep_|_factor nn_|_;| parataxis_1_| dep_1_efefceft dep_1_e dep_efefceft_1 dep_ss_1 nsubj_ss_s nn_s_S amod_s_= amod_s_= mark_follows_as parataxis_is_ss advcl_is_follows dep_is_e nsubj_is_training advmod_e_| nn_|_f prep_for_training_t nn_training_Model-1 poss_training_our parataxis_=_is dep_ii_= amod_Sief_ii dep_Sief_1 dep_1_Given amod_sentences_parallel prep_of_consisting_sentences vmod_data_consisting nn_data_training pobj_Given_data
W03-0315	J93-2003	o	In our approach equation -LRB- 1 -RRB- is further normalized so that the probability for different lengths of F is comparable at the word level m m j n i ijm eft l EFP / 1 10 -RRB- | -LRB- -RRB- 1 -LRB- 1 -RRB- | -LRB- + = == -LRB- 2 -RRB- The alignment models described in -LRB- Brown et al. 1993 -RRB- are all based on the notion that an alignment aligns each source word to exactly one target word	nn_word_target num_word_one quantmod_one_exactly prep_to_word_word dep_source_word det_source_each dobj_aligns_source nsubj_aligns_alignment mark_aligns_that det_alignment_an ccomp_notion_aligns det_notion_the pobj_all_notion prepc_based_on_all_on nsubj_are_all dep_al._are num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in vmod_models_described nn_models_alignment det_models_The nn_models_== appos_==_2 dobj_=_models cc_=_+ dep_|_= num_|_1 num_|_1 nn_|_| number_10_1 dep_EFP_| dep_EFP_10 nn_EFP_l nn_EFP_eft nn_EFP_ijm nn_EFP_i nn_EFP_n nn_EFP_j nn_EFP_m nn_EFP_m nn_level_word det_level_the prep_at_comparable_level cop_comparable_is nsubj_comparable_probability mark_comparable_that prep_of_lengths_F amod_lengths_different prep_for_probability_lengths det_probability_the ccomp_normalized_comparable advmod_normalized_so dep_further_EFP dep_further_normalized cop_further_is nsubj_further_equation prep_in_further_approach appos_equation_1 poss_approach_our
W03-0413	J93-2003	o	The first model referred to as Maxent1 below is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the IBM translation model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_translation nn_model_IBM det_model_the prep_of_analog_model det_analog_an cop_analog_is nsubj_analog_that rcmod_component_analog nn_component_translation nn_component_entropy nn_component_maximum det_component_a prep_with_model_component nn_model_language nn_model_trigram det_model_a dep_combination_Brown prep_of_combination_model amod_combination_loglinear det_combination_a cop_combination_is nsubj_combination_model advmod_Maxent1_below pobj_referred_Maxent1 prepc_as_to_referred_as vmod_model_referred amod_model_first det_model_The
W03-0414	J93-2003	p	-LRB- Brown et al. 1990 Brown et al. 1993 -RRB- -RRB- are best known and studied	nsubjpass_studied_Brown conj_and_known_studied advmod_known_best auxpass_known_are nsubjpass_known_Brown num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Brown_Brown appos_Brown_1990 dep_Brown_al. nn_Brown_et
W03-0604	J93-2003	o	We have developed a set of extensions to a probabilistic translation model -LRB- Brown et al. 1993 -RRB- that enable us to successfully merge oversegmented regions into coherent objects	amod_objects_coherent amod_regions_oversegmented prep_into_merge_objects dobj_merge_regions advmod_merge_successfully aux_merge_to xcomp_enable_merge dobj_enable_us nsubj_enable_that num_al._1993 nn_al._et amod_al._Brown nn_model_translation amod_model_probabilistic det_model_a prep_to_extensions_model prep_of_set_extensions det_set_a dep_developed_enable dep_developed_al. dobj_developed_set aux_developed_have nsubj_developed_We ccomp_``_developed
W03-0604	J93-2003	o	-LRB- 1993 -RRB- -LRB- as in Duygulu et al. 2002 -RRB- and extend it to structured shape descriptions of visual data	amod_data_visual prep_of_descriptions_data nn_descriptions_shape amod_descriptions_structured prep_to_extend_descriptions dobj_extend_it num_Duygulu_2002 nn_Duygulu_al. nn_Duygulu_et pobj_in_Duygulu pcomp_as_in conj_and_1993_extend prep_1993_as dep_''_extend dep_''_1993
W03-0604	J93-2003	o	Probabilistic translation models generally seek to find the translation string e that maximizes the probability Pra5 ea6fa7 given the source string f -LRB- where f referred to French and e to English in the original work Brown et al. 1993 -RRB-	num_Brown_1993 nn_Brown_al. nn_Brown_et nn_Brown_to nn_Brown_f amod_work_original det_work_the pobj_to_English dep_to_e prep_to_referred_French prep_in_f_work conj_and_f_to vmod_f_referred dep_where_Brown dep_-LRB-_where dep_string_f nn_string_source det_string_the pobj_given_string nn_ea6fa7_Pra5 nn_ea6fa7_probability det_ea6fa7_the dobj_maximizes_ea6fa7 nsubj_maximizes_that prep_string_given rcmod_string_maximizes dep_string_e nn_string_translation det_string_the dobj_find_string aux_find_to xcomp_seek_find advmod_seek_generally nsubj_seek_models nn_models_translation nn_models_Probabilistic
W03-0608	J93-2003	o	Fortunately there is a straightforward parallel between our object recognition formulation and the statistical machine translation problem of building a lexicon from an aligned bitext -LRB- Brown et al. 1993 Al-Onaizan et al. 1999 -RRB-	nn_al._et nn_al._Al-Onaizan num_al._1999 dep_al._al. num_al._1993 nn_al._et amod_al._Brown dep_bitext_al. amod_bitext_aligned det_bitext_an prep_from_lexicon_bitext det_lexicon_a dobj_building_lexicon prepc_of_problem_building nn_problem_translation nn_problem_machine amod_problem_statistical det_problem_the conj_and_formulation_problem nn_formulation_recognition nn_formulation_object poss_formulation_our prep_between_parallel_problem prep_between_parallel_formulation amod_parallel_straightforward det_parallel_a nsubj_is_parallel expl_is_there advmod_is_Fortunately
W03-1001	J93-2003	o	1 Introduction Various papers use phrase-based translation systems -LRB- Och et al. 1999 Marcu and Wong 2002 Yamada and Knight 2002 -RRB- that have shown to improve translation quality over single-word based translation systems introduced in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_introduced_in vmod_systems_introduced nn_systems_translation amod_systems_based amod_systems_single-word nn_quality_translation prep_over_improve_systems dobj_improve_quality aux_improve_to xcomp_shown_improve aux_shown_have nsubj_shown_that amod_Yamada_2002 conj_and_Yamada_Knight rcmod_Marcu_shown dep_Marcu_Knight dep_Marcu_Yamada num_Marcu_2002 conj_and_Marcu_Wong dep_Och_Wong dep_Och_Marcu appos_Och_1999 dep_Och_al. nn_Och_et nn_systems_translation amod_systems_phrase-based dep_use_Och dobj_use_systems nsubj_use_papers amod_papers_Various rcmod_Introduction_use num_Introduction_1
W03-1002	J93-2003	p	2 Prior Work Statistical machine translation as pioneered by IBM -LRB- e.g. Brown et al. 1993 -RRB- is grounded in the noisy channel model	nn_model_channel amod_model_noisy det_model_the prep_in_grounded_model auxpass_grounded_is nsubjpass_grounded_e.g. num_Brown_1993 nn_Brown_al. nn_Brown_et dep_e.g._Brown dep_IBM_grounded prep_by_pioneered_IBM mark_pioneered_as advcl_,_pioneered nn_translation_machine amod_translation_Statistical nn_translation_Work amod_translation_Prior num_translation_2 dep_``_translation
W03-1002	J93-2003	o	POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit -LRB- Ngai and Florian 2001 -RRB- both were trained from the annotated Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn amod_corpus_annotated det_corpus_the dep_trained_Marcus prep_from_trained_corpus auxpass_trained_were dep_trained_both dep_Ngai_2001 conj_and_Ngai_Florian appos_Toolkit_Florian appos_Toolkit_Ngai nn_Toolkit_fnTBL det_Toolkit_the prep_with_provided_Toolkit vmod_systems_provided amod_systems_trained det_systems_the dobj_using_systems parataxis_done_trained xcomp_done_using auxpass_done_were nsubjpass_done_chunking nsubjpass_done_tagging nn_chunking_phrase prep_in_tagging_English conj_and_tagging_chunking nn_tagging_POS
W03-1003	J93-2003	o	Specifically stochastic translation lexicons estimated using the IBM method -LRB- Brown et al. 1993 -RRB- from a fairly large sentence-aligned Chinese-English parallel corpus are used in their approach a considerable demand for a resourcedeficient language	amod_language_resourcedeficient det_language_a prep_for_demand_language amod_demand_considerable det_demand_a poss_approach_their dobj_used_demand prep_in_used_approach auxpass_used_are nsubjpass_used_lexicons advmod_used_Specifically nn_corpus_parallel amod_corpus_Chinese-English amod_corpus_sentence-aligned amod_corpus_large det_corpus_a advmod_large_fairly dep_al._1993 nn_al._et amod_al._Brown dep_method_al. nn_method_IBM det_method_the prep_from_using_corpus dobj_using_method xcomp_estimated_using vmod_lexicons_estimated nn_lexicons_translation amod_lexicons_stochastic
W03-1508	J93-2003	p	The IBM source-channel model for statistical machine translation -LRB- P. Brown et al. 1993 -RRB- plays a central role in our system	poss_system_our amod_role_central det_role_a prep_in_plays_system dobj_plays_role nsubj_plays_model amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_Brown_P. nn_translation_machine amod_translation_statistical dep_model_Brown prep_for_model_translation nn_model_source-channel nn_model_IBM det_model_The
W04-0857	J93-2003	o	To solve this problem we will adapt the idea of null generated words from machine translation -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown nn_translation_machine prep_from_words_translation amod_words_generated amod_words_null prep_of_idea_words det_idea_the dep_adapt_al. dobj_adapt_idea aux_adapt_will nsubj_adapt_we advcl_adapt_solve det_problem_this dobj_solve_problem aux_solve_To ccomp_``_adapt
W04-1118	J93-2003	o	The relationship between the translation model and the alignment model is given by Pr -LRB- fJ1 jeI1 -RRB- = X aJ1 Pr -LRB- fJ1 aJ1jeI1 -RRB- -LRB- 3 -RRB- In this paper we use the models IBM-1 IBM4 from -LRB- Brown et al. 1993 -RRB- and the HiddenMarkovalignmentmodel -LRB- HMM -RRB- from -LRB- Vogelet al. 1996 -RRB-	num_al._1996 amod_al._Vogelet dep_from_al. amod_HiddenMarkovalignmentmodel_from appos_HiddenMarkovalignmentmodel_HMM det_HiddenMarkovalignmentmodel_the num_al._1993 nn_al._et amod_al._Brown conj_and_from_HiddenMarkovalignmentmodel dep_from_al. prep_IBM4_HiddenMarkovalignmentmodel prep_IBM4_from appos_IBM-1_IBM4 dep_models_IBM-1 det_models_the dobj_use_models nsubj_use_we prep_in_use_paper dep_use_3 nsubj_use_Pr det_paper_this dep_fJ1_aJ1jeI1 appos_Pr_fJ1 nn_Pr_aJ1 nn_Pr_X rcmod_=_use nn_jeI1_fJ1 amod_Pr_= appos_Pr_jeI1 dep_by_Pr prep_given_by auxpass_given_is nsubjpass_given_relationship nn_model_alignment det_model_the conj_and_model_model nn_model_translation det_model_the prep_between_relationship_model prep_between_relationship_model det_relationship_The ccomp_``_given
W05-0612	J93-2003	o	Our methods are most influenced by IBMs Model 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_1 dep_IBMs_Model agent_influenced_IBMs advmod_influenced_most auxpass_influenced_are nsubjpass_influenced_methods poss_methods_Our ccomp_``_influenced
W05-0614	J93-2003	o	Further we can learn the channel probabilities in an unsupervised manner using a variant of the EM algorithm similar to machine translation -LRB- Brown et al. 1993 -RRB- and statistical language understanding -LRB- Epstein 1996 -RRB-	amod_Epstein_1996 dep_understanding_Epstein nn_understanding_language amod_understanding_statistical dep_al._1993 nn_al._et amod_al._Brown nn_translation_machine prep_to_similar_translation amod_algorithm_similar nn_algorithm_EM det_algorithm_the dep_variant_al. prep_of_variant_algorithm det_variant_a conj_and_using_understanding dobj_using_variant amod_manner_unsupervised det_manner_an nn_probabilities_channel det_probabilities_the dep_learn_understanding dep_learn_using prep_in_learn_manner dobj_learn_probabilities aux_learn_can nsubj_learn_we advmod_learn_Further
W05-0614	J93-2003	o	We follow IBM Model 1 -LRB- Brown et al. 1993 -RRB- and assume that each word in an utterance is generated by exactly one role in the parallel frame Using standard EM to learn the role to word mapping is only sufficient if one knows to which level in the tree the utterance should be mapped	auxpass_mapped_be aux_mapped_should nsubjpass_mapped_utterance det_utterance_the det_tree_the rcmod_level_mapped prep_in_level_tree dep_knows_level prep_to_knows_which nsubj_knows_one mark_knows_if ccomp_sufficient_knows advmod_sufficient_only cop_sufficient_is csubj_sufficient_assume csubj_sufficient_follow nn_mapping_word det_role_the prep_to_learn_mapping dobj_learn_role aux_learn_to amod_EM_standard vmod_Using_learn dobj_Using_EM nn_frame_parallel det_frame_the vmod_role_Using prep_in_role_frame num_role_one quantmod_one_exactly agent_generated_role auxpass_generated_is nsubjpass_generated_word mark_generated_that det_utterance_an prep_in_word_utterance det_word_each ccomp_assume_generated nsubj_assume_We amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_Model_Brown num_Model_1 nn_Model_IBM conj_and_follow_assume dobj_follow_Model nsubj_follow_We
W05-0712	J93-2003	n	A word based approach depends upon traditional statistical machine translation techniques such as IBM Model1 -LRB- Brown et al. 1993 -RRB- and may not always yield satisfactory results due to its inability to handle difficult many-to-many phrase translations	nn_translations_phrase nn_translations_many-to-many amod_translations_difficult dobj_handle_translations aux_handle_to vmod_inability_handle poss_inability_its prep_due_to_results_inability amod_results_satisfactory dobj_yield_results advmod_yield_always neg_yield_not aux_yield_may nsubj_yield_approach num_al._1993 nn_al._et amod_al._Brown dep_Model1_al. nn_Model1_IBM prep_such_as_techniques_Model1 nn_techniques_translation nn_techniques_machine amod_techniques_statistical amod_techniques_traditional conj_and_depends_yield prep_upon_depends_techniques nsubj_depends_approach amod_approach_based nn_approach_word det_word_A
W05-0804	J93-2003	o	Previous work from -LRB- Wang et al. 1996 -RRB- showed improvements in perplexity-oriented measures using mixture-based translation lexicon -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_lexicon_al. nn_lexicon_translation amod_lexicon_mixture-based dobj_using_lexicon amod_measures_perplexity-oriented prep_in_improvements_measures vmod_showed_using dobj_showed_improvements amod_Wang_1996 dep_Wang_al. nn_Wang_et prepc_from_work_showed dep_work_Wang amod_work_Previous
W05-0806	J93-2003	o	For detailed descriptions of SMT models see for example -LRB- Brown et al. 1993 Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney conj_al._1993 nn_al._et dep_Brown_Ney dep_Brown_Och advmod_Brown_al. dep_see_Brown prep_for_see_example prep_for_see_descriptions nn_models_SMT prep_of_descriptions_models amod_descriptions_detailed
W05-0809	J93-2003	n	Several teams had approaches that relied -LRB- to varying degrees -RRB- on an IBM model of statistical machine translation -LRB- Brown et al. 1993 -RRB- with different improvements brought by different teams consisting of new submodels improvements in the HMM model model combination for optimal alignment etc. Se-veral teams used symmetrization metrics as introduced in -LRB- Och and Ney 2003 -RRB- -LRB- union intersection refined -RRB- most of the times applied on the alignments produced for the two directions sourcetarget and targetsource but also as a way to combine different word alignment systems	nn_systems_alignment nn_systems_word amod_systems_different dobj_combine_systems aux_combine_to vmod_way_combine det_way_a prep_as_also_way conj_and_sourcetarget_targetsource dep_directions_targetsource dep_directions_sourcetarget num_directions_two det_directions_the prep_for_produced_directions nsubj_produced_most det_alignments_the prep_on_applied_alignments vmod_times_applied det_times_the prep_of_most_times amod_union_refined appos_union_intersection advmod_Och_also rcmod_Och_produced dep_Och_union dep_Och_2003 conj_and_Och_Ney prep_in_introduced_Ney prep_in_introduced_Och mark_introduced_as dep_metrics_introduced nn_metrics_symmetrization amod_metrics_used dep_teams_metrics amod_teams_Se-veral dep_alignment_etc. amod_alignment_optimal appos_combination_teams prep_for_combination_alignment nn_combination_model nn_model_HMM det_model_the appos_improvements_combination prep_in_improvements_model amod_submodels_new prep_of_consisting_submodels amod_teams_different agent_brought_teams vmod_improvements_brought amod_improvements_different dep_al._1993 nn_al._et advmod_Brown_al. nn_translation_machine amod_translation_statistical prep_of_model_translation nn_model_IBM det_model_an amod_degrees_varying prep_on_relied_model prep_to_relied_degrees nsubj_relied_that rcmod_approaches_relied dobj_had_improvements xcomp_had_consisting prep_with_had_improvements dep_had_Brown dobj_had_approaches nsubj_had_teams amod_teams_Several
W05-0810	J93-2003	o	First we considered single sentences as documents and tokens as sentences -LRB- we define a token as a sequence of characters delimited by 1In our case the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in -LRB- Gale and Church 1993 -RRB- but also a cognate-based one similar to -LRB- Simard et al. 1992 -RRB-	amod_Simard_1992 dep_Simard_al. nn_Simard_et prep_similar_to dep_one_Simard amod_one_similar amod_one_cognate-based det_one_a dep_also_one amod_Gale_1993 conj_and_Gale_Church prep_in_described_Church prep_in_described_Gale vmod_criteria_described nn_criteria_length det_criteria_the dobj_taking_criteria prep_into_taking_account preconj_taking_only aux_taking_is nsubj_taking_case neg_only_not amod_programming_dynamic prep_by_maximize_programming advmod_maximize_globally aux_maximize_to xcomp_seek_maximize nsubj_seek_we rcmod_score_seek det_score_the appos_case_score poss_case_our rcmod_1In_taking agent_delimited_1In vmod_characters_delimited prep_of_sequence_characters det_sequence_a prep_as_token_sequence det_token_a conj_but_define_also dobj_define_token nsubj_define_we dep_tokens_also dep_tokens_define prep_as_tokens_sentences prep_as_sentences_documents amod_sentences_single conj_and_considered_tokens dobj_considered_sentences nsubj_considered_we advmod_considered_First
W05-0810	J93-2003	p	When efficient techniques have been proposed -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- they have been mostly evaluated on safe pairs of languages where the notion of word is rather clear	advmod_clear_rather cop_clear_is nsubj_clear_notion advmod_clear_where prep_of_notion_word det_notion_the rcmod_languages_clear prep_of_pairs_languages amod_pairs_safe prep_on_evaluated_pairs advmod_evaluated_mostly auxpass_evaluated_been aux_evaluated_have nsubjpass_evaluated_they advcl_evaluated_proposed dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_proposed_Brown auxpass_proposed_been aux_proposed_have nsubjpass_proposed_techniques advmod_proposed_When amod_techniques_efficient ccomp_``_evaluated
W05-0812	J93-2003	o	IBM Model 4 parameters are then estimated over this partial search space as an approximation to EM -LRB- Brown et al. 1993 Och and Ney 2003 -RRB-	dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_EM_Brown prep_to_approximation_EM det_approximation_an nn_space_search amod_space_partial det_space_this prep_as_estimated_approximation prep_over_estimated_space advmod_estimated_then auxpass_estimated_are nsubjpass_estimated_parameters num_parameters_4 nn_parameters_Model nn_parameters_IBM
W05-0812	J93-2003	p	1 Introduction The most widely used alignment model is IBM Model 4 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM cop_Model_is nsubj_Model_model nn_model_alignment amod_model_used det_model_The advmod_used_widely advmod_widely_most dep_Introduction_Brown rcmod_Introduction_Model num_Introduction_1
W05-0814	J93-2003	o	For these experiments we have implemented an alignment package for IBM Model 4 using a hillclimbing search and Viterbi training as described in -LRB- Brown et al. 1993 -RRB- and extended this to use new submodels	amod_submodels_new dobj_use_submodels aux_use_to vmod_extended_use dobj_extended_this nsubj_extended_we num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_as nn_training_Viterbi conj_and_search_training amod_search_hillclimbing det_search_a advcl_using_described dobj_using_training dobj_using_search num_Model_4 nn_Model_IBM prep_for_package_Model nn_package_alignment det_package_an conj_and_implemented_extended xcomp_implemented_using dobj_implemented_package aux_implemented_have nsubj_implemented_we prep_for_implemented_experiments det_experiments_these
W05-0814	J93-2003	p	Turning off the extensions to GIZA + + and training p0 as in -LRB- Brown et al. 1993 -RRB- produces a substantial increase in AER	prep_in_increase_AER amod_increase_substantial det_increase_a dobj_produces_increase nsubj_produces_al. mark_produces_in mark_produces_as num_al._1993 nn_al._et amod_al._Brown nn_p0_training nn_p0_+ nn_p0_GIZA conj_and_GIZA_training conj_+_GIZA_+ prep_to_extensions_p0 det_extensions_the advcl_Turning_produces dobj_Turning_extensions prt_Turning_off ccomp_``_Turning
W05-0814	J93-2003	o	We solve this using the local search defined in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_defined_in vmod_search_defined amod_search_local det_search_the dobj_using_search vmod_this_using dobj_solve_this nsubj_solve_We ccomp_``_solve
W05-0814	J93-2003	o	The system used for baseline experiments is two runs of IBM Model 4 -LRB- Brown et al. 1993 -RRB- in the GIZA + + -LRB- Och and Ney 2003 -RRB- implementation which includes smoothing extensions to Model 4	num_Model_4 prep_to_extensions_Model dobj_smoothing_extensions xcomp_includes_smoothing nsubj_includes_which appos_implementation_Ney appos_implementation_Och num_Och_2003 conj_and_Och_Ney pobj_+_implementation rcmod_GIZA_includes conj_+_GIZA_+ det_GIZA_the amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_4 nn_Model_IBM prep_in_runs_+ prep_in_runs_GIZA dep_runs_Brown prep_of_runs_Model num_runs_two cop_runs_is nsubj_runs_system nn_experiments_baseline prep_for_used_experiments vmod_system_used det_system_The
W05-0815	J93-2003	o	The idea is that the translation of a sentence x into a sentence y can be performed in the following steps1 -LRB- a -RRB- If x is small enough IBMs model 1 -LRB- Brown et al. 1993 -RRB- is employed for the translation	det_translation_the prep_for_employed_translation auxpass_employed_is nsubjpass_employed_IBMs advcl_employed_small dep_employed_a amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_model_Brown num_model_1 dep_IBMs_model advmod_small_enough cop_small_is nsubj_small_x mark_small_If amod_steps1_following det_steps1_the parataxis_performed_employed prep_in_performed_steps1 auxpass_performed_be aux_performed_can nsubjpass_performed_translation mark_performed_that nn_y_sentence det_y_a nn_x_sentence det_x_a prep_into_translation_y prep_of_translation_x det_translation_the ccomp_is_performed nsubj_is_idea det_idea_The ccomp_``_is
W05-0816	J93-2003	o	Word correspondence was further developed in IBM Model-1 -LRB- Brown et al. 1993 -RRB- for statistical machine translation	nn_translation_machine amod_translation_statistical num_al._1993 nn_al._et amod_al._Brown dep_Model-1_al. nn_Model-1_IBM prep_for_developed_translation prep_in_developed_Model-1 advmod_developed_further auxpass_developed_was nsubjpass_developed_correspondence nn_correspondence_Word
W05-0817	J93-2003	o	The first one is a hypotheses testing approach -LRB- Gale and Church 1991 Melamed 2001 Tufi 2002 -RRB- while the second one is closer to a model estimating approach -LRB- Brown et al. 1993 Och and Ney 2000 -RRB-	amod_Och_2000 conj_and_Och_Ney nn_al._et dep_Brown_Ney dep_Brown_Och dep_Brown_1993 dep_Brown_al. dobj_estimating_approach vmod_model_estimating det_model_a dep_closer_Brown prep_to_closer_model cop_closer_is nsubj_closer_one mark_closer_while amod_one_second det_one_the num_Tufi_2002 dep_Melamed_2001 dep_Gale_Tufi conj_and_Gale_Melamed conj_and_Gale_1991 conj_and_Gale_Church advcl_approach_closer dep_approach_Melamed dep_approach_1991 dep_approach_Church dep_approach_Gale amod_approach_testing nn_approach_hypotheses det_approach_a cop_approach_is nsubj_approach_one amod_one_first det_one_The
W05-0817	J93-2003	p	A quite different approach from our hypotheses testing implemented in the TREQ-AL aligner is taken by the model-estimating aligners most of them relying on the IBM models -LRB- 1 to 5 -RRB- described in the -LRB- Brown et al. 1993 -RRB- seminal paper	amod_paper_seminal dep_paper_Brown det_paper_the dep_1993_al. dep_Brown_1993 nn_Brown_et prep_in_described_paper dep_5_to number_5_1 vmod_models_described dep_models_5 nn_models_IBM det_models_the prep_on_relying_models nsubj_relying_most ccomp_relying_taken prep_of_most_them amod_aligners_model-estimating det_aligners_the agent_taken_aligners auxpass_taken_is nsubjpass_taken_approach nn_aligner_TREQ-AL det_aligner_the prep_in_implemented_aligner vmod_testing_implemented vmod_hypotheses_testing poss_hypotheses_our prep_from_approach_hypotheses amod_approach_different det_approach_A advmod_different_quite
W05-0823	J93-2003	o	1 Introduction During the last decade statistical machine translation -LRB- SMT -RRB- systems have evolved from the original word-based approach -LRB- Brown et al. 1993 -RRB- into phrase-based translation systems -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_systems_translation amod_systems_phrase-based dep_al._1993 nn_al._et amod_al._Brown dep_approach_al. amod_approach_word-based amod_approach_original det_approach_the dep_evolved_Koehn prep_into_evolved_systems prep_from_evolved_approach aux_evolved_have nsubj_evolved_systems ccomp_evolved_Introduction nn_systems_translation appos_translation_SMT nn_translation_machine amod_translation_statistical amod_decade_last det_decade_the prep_during_Introduction_decade num_Introduction_1
W05-0823	J93-2003	o	Finally the fourth and fifth feature functions corresponded to two lexicon models based on IBM Model 1 lexical parameters p -LRB- t | s -RRB- -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_s_Brown num_s_| nn_s_t nn_s_p dep_parameters_s amod_parameters_lexical num_parameters_1 nn_parameters_Model nn_parameters_IBM pobj_models_parameters prepc_based_on_models_on nn_models_lexicon num_models_two prep_to_corresponded_models nsubj_corresponded_functions advmod_corresponded_Finally nn_functions_feature amod_functions_fifth amod_functions_fourth det_functions_the conj_and_fourth_fifth
W05-0825	J93-2003	o	3 Length Model Dynamic Programming Given the word fertility de nitions in IBM Models -LRB- Brown et al. 1993 -RRB- we can compute a probability to predict phrase length given the candidate target phrase -LRB- English -RRB- eI1 and a source phrase -LRB- French -RRB- of length J the model gives the estimation of P -LRB- J | eI1 -RRB- via a dynamic programming algorithm using the source word fertilities	nn_fertilities_word nn_fertilities_source det_fertilities_the dobj_using_fertilities vmod_algorithm_using nn_algorithm_programming amod_algorithm_dynamic det_algorithm_a num_eI1_| nn_eI1_J appos_P_eI1 prep_of_estimation_P det_estimation_the prep_via_gives_algorithm dobj_gives_estimation nsubj_gives_model det_model_the nn_J_length prep_of_phrase_J appos_phrase_French nn_phrase_source det_phrase_a nn_eI1_phrase appos_phrase_English nn_phrase_target nn_phrase_candidate det_phrase_the dep_given_gives conj_and_given_phrase pobj_given_eI1 dep_length_phrase dep_length_given nn_length_phrase dobj_predict_length aux_predict_to det_probability_a vmod_compute_predict dobj_compute_probability aux_compute_can nsubj_compute_we nsubj_compute_Programming amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_Models_IBM appos_nitions_Brown prep_in_nitions_Models amod_nitions_de nn_nitions_fertility nn_nitions_word det_nitions_the pobj_Given_nitions prep_Programming_Given nn_Programming_Dynamic parataxis_Model_compute nn_Model_Length num_Model_3 dep_``_Model
W05-0826	J93-2003	o	Far from full syntactic complexity we suggest to go back to the simpler alignment methods first described by -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_by_al. prep_described_by vmod_first_described amod_methods_first nn_methods_alignment amod_methods_simpler det_methods_the prep_to_back_methods advmod_go_back aux_go_to xcomp_suggest_go nsubj_suggest_we prep_far_from_suggest_complexity amod_complexity_syntactic amod_complexity_full
W05-0829	J93-2003	n	1 Introduction In recent years various phrase translation approaches -LRB- Marcu and Wong 2002 Och et al. 1999 Koehn et al. 2003 -RRB- have been shown to outperform word-to-word translation models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_translation amod_models_word-to-word dobj_outperform_models aux_outperform_to xcomp_shown_outperform auxpass_shown_been aux_shown_have nsubjpass_shown_approaches dep_shown_Introduction num_Koehn_2003 nn_Koehn_al. nn_Koehn_et num_Och_1999 nn_Och_al. nn_Och_et dep_Marcu_Koehn dep_Marcu_Och num_Marcu_2002 conj_and_Marcu_Wong appos_approaches_Wong appos_approaches_Marcu nn_approaches_translation nn_approaches_phrase amod_approaches_various amod_years_recent prep_in_Introduction_years num_Introduction_1
W05-0835	J93-2003	o	Different models have been presented in the literature see for instance -LRB- Brown et al. 1993 Och and Ney 2004 Vidal et al. 1993 Vogel et al. 1996 -RRB-	num_Vogel_1996 nn_Vogel_al. nn_Vogel_et num_Vidal_1993 nn_Vidal_al. nn_Vidal_et num_Och_2004 conj_and_Och_Ney conj_al._1993 nn_al._et dep_Brown_Vogel dep_Brown_Vidal dep_Brown_Ney dep_Brown_Och advmod_Brown_al. dep_see_Brown prep_for_see_instance det_literature_the dep_presented_see prep_in_presented_literature auxpass_presented_been aux_presented_have nsubjpass_presented_models amod_models_Different
W05-0835	J93-2003	o	Most of them rely on the concept of alignment a mapping from words or groups of words in a sentence into words or groups in the other -LRB- in the case of -LRB- Vidal et al. 1993 -RRB- the mapping goes from rules in a grammar for a language into rules of a grammar for the other language -RRB-	amod_language_other det_language_the prep_for_grammar_language det_grammar_a prep_of_rules_grammar prep_into_language_rules det_language_a prep_for_grammar_language det_grammar_a prep_in_goes_grammar prep_from_goes_rules nsubj_goes_mapping dep_goes_Vidal mark_goes_of det_mapping_the amod_Vidal_1993 dep_Vidal_al. nn_Vidal_et dep_case_goes det_case_the prep_in_other_case det_other_the prep_in_words_other conj_or_words_groups prep_into_sentence_groups prep_into_sentence_words det_sentence_a prep_of_words_words conj_or_words_groups prep_in_mapping_sentence prep_from_mapping_groups prep_from_mapping_words det_mapping_a prep_of_concept_alignment det_concept_the dep_rely_mapping prep_on_rely_concept nsubj_rely_Most prep_of_Most_them
W05-0835	J93-2003	o	This concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment -LRB- Dagan et al. 1993 -RRB-	amod_Dagan_1993 dep_Dagan_al. nn_Dagan_et nn_alignment_corpus conj_and_derivation_alignment nn_derivation_vocabulary amod_derivation_authomatic prep_like_tasks_alignment prep_like_tasks_derivation dep_used_Dagan prep_for_used_tasks advmod_used_also auxpass_used_been aux_used_has nsubjpass_used_concept prep_of_concept_alignment det_concept_This
W05-0835	J93-2003	o	The elements of this set are pairs -LRB- x y -RRB- where y is a possible translation for x. 4 IBMs model 1 IBMs model 1 is the simplest of a hierarchy of five statistical models introduced in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_introduced_in vmod_models_introduced amod_models_statistical num_models_five prep_of_hierarchy_models det_hierarchy_a prep_of_simplest_hierarchy det_simplest_the cop_simplest_is nsubj_simplest_pairs num_model_1 nn_model_IBMs num_model_1 nn_model_model nn_model_IBMs num_model_4 nn_model_x. prep_for_translation_model amod_translation_possible det_translation_a cop_translation_is nsubj_translation_y advmod_translation_where appos_x_y rcmod_pairs_translation dep_pairs_x cop_pairs_are nsubj_pairs_elements det_set_this prep_of_elements_set det_elements_The
W05-0836	J93-2003	o	Using the log-linear form to model p -LRB- e | f -RRB- gives us the flexibility to introduce overlapping features that can represent global context while decoding -LRB- searching the space of candidate translations -RRB- and rescoring -LRB- ranking a set of candidate translations before performing the argmax operation -RRB- albeit at the cost of the traditional source-channel generative model of translation proposed in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_proposed_in vmod_translation_proposed prep_of_model_translation amod_model_generative amod_model_source-channel amod_model_traditional det_model_the prep_of_cost_model det_cost_the pobj_at_cost pcomp_albeit_at nn_operation_argmax det_operation_the dobj_performing_operation nn_translations_candidate prep_of_set_translations det_set_a prepc_before_ranking_performing dobj_ranking_set dep_rescoring_ranking nn_translations_candidate prep_of_space_translations det_space_the dobj_searching_space prep_decoding_albeit conj_and_decoding_rescoring dep_decoding_searching amod_context_global prep_while_represent_rescoring prep_while_represent_decoding dobj_represent_context aux_represent_can nsubj_represent_that rcmod_features_represent amod_features_overlapping dobj_introduce_features aux_introduce_to vmod_flexibility_introduce det_flexibility_the dobj_gives_flexibility iobj_gives_us dep_gives_f nsubj_gives_p mark_gives_to dep_f_| dep_|_e nn_p_model amod_form_log-linear det_form_the advcl_Using_gives dobj_Using_form ccomp_``_Using
W05-1208	J93-2003	o	This differs from typical generative settings for IR and MT -LRB- Ponte and croft 1998 Brown et al. 1993 -RRB- where all conditioned events are disjoint by construction	prep_by_disjoint_construction cop_disjoint_are nsubj_disjoint_events advmod_disjoint_where amod_events_conditioned det_events_all num_Brown_1993 nn_Brown_al. nn_Brown_et rcmod_Ponte_disjoint dep_Ponte_Brown conj_and_Ponte_1998 conj_and_Ponte_croft dep_IR_1998 dep_IR_croft dep_IR_Ponte conj_and_IR_MT prep_for_settings_MT prep_for_settings_IR amod_settings_generative amod_settings_typical prep_from_differs_settings nsubj_differs_This
W05-1208	J93-2003	o	Alternatively one can view -LRB- 2 -RRB- as inducing an alignment between terms in the h to the terms in the t somewhat similar to alignment models in statistical MT -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et advmod_Brown_al. amod_MT_statistical prep_in_models_MT nn_models_alignment dep_similar_Brown prep_to_similar_models advmod_similar_somewhat det_t_the prep_in_terms_t det_terms_the det_h_the prep_in_terms_h prep_between_alignment_terms det_alignment_an prep_to_inducing_terms dobj_inducing_alignment acomp_view_similar prepc_as_view_inducing dep_view_2 aux_view_can nsubj_view_one advmod_view_Alternatively
W06-1204	J93-2003	p	State-of-art systems for doing word alignment use generative models like GIZA + + -LRB- Och and Ney 2003 Brown et al. 1993 -RRB-	num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Och_Brown conj_and_Och_2003 conj_and_Och_Ney pobj_+_2003 pobj_+_Ney pobj_+_Och conj_+_GIZA_+ prep_like_models_+ prep_like_models_GIZA amod_models_generative dep_use_models nn_use_alignment nn_use_word dobj_doing_use prepc_for_systems_doing amod_systems_State-of-art
W06-1606	J93-2003	o	Pr -LRB- pi F A -RRB- = summationdisplay i c -LRB- -RRB- = -LRB- pi F A -RRB- productdisplay rji p -LRB- rj -RRB- -LRB- 4 -RRB- In order to acquire the rules specific to our model and to induce their probabilities we parse the English side of our corpus with an in-house implementation -LRB- Soricut 2005 -RRB- of Collins parsing models -LRB- Collins 2003 -RRB- and we word-align the parallel corpus with the Giza + +2 implementation of the IBM models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM det_models_the num_implementation_+2 prep_of_Giza_models conj_+_Giza_implementation det_Giza_the amod_corpus_parallel det_corpus_the prep_with_word-align_implementation prep_with_word-align_Giza dobj_word-align_corpus nsubj_word-align_we amod_Collins_2003 appos_models_Collins amod_models_parsing nn_models_Collins amod_Soricut_2005 prep_of_implementation_models dep_implementation_Soricut amod_implementation_in-house det_implementation_an poss_corpus_our prep_of_side_corpus amod_side_English det_side_the dep_parse_Brown conj_and_parse_word-align prep_with_parse_implementation dobj_parse_side nsubj_parse_we poss_probabilities_their dobj_induce_probabilities aux_induce_to poss_model_our prep_to_specific_model amod_rules_specific det_rules_the conj_and_acquire_induce dobj_acquire_rules aux_acquire_to dep_acquire_order mark_acquire_In appos_p_rj nn_p_rji nn_p_productdisplay appos_pi_A appos_pi_F parataxis_=_word-align parataxis_=_parse dep_=_induce dep_=_acquire dep_=_4 dep_=_p dep_=_pi dep_=_c dep_=_i dep_=_= dep_=_Pr nn_i_summationdisplay appos_pi_A appos_pi_F dep_Pr_pi
W06-1607	J93-2003	o	To derive the joint counts c -LRB- s t -RRB- from which p -LRB- s | t -RRB- and p -LRB- t | s -RRB- are estimated we use the phrase induction algorithm described in -LRB- Koehn et al. 2003 -RRB- with symmetrized word alignments generated using IBM model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_IBM dobj_using_model vmod_generated_using vmod_alignments_generated nn_alignments_word amod_alignments_symmetrized dep_Koehn_Brown prep_with_Koehn_alignments amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn vmod_algorithm_described nn_algorithm_induction nn_algorithm_phrase det_algorithm_the dobj_use_algorithm nsubj_use_we advcl_use_derive auxpass_estimated_are nsubjpass_estimated_s nsubjpass_estimated_p prep_from_estimated_which num_s_| nn_s_t nn_s_p dobj_|_t nsubj_|_s conj_and_p_s dep_p_| appos_s_t nn_s_c nn_s_counts amod_s_joint det_s_the dep_derive_estimated dobj_derive_s aux_derive_To
W06-1609	J93-2003	o	This feature which is based on the lexical parameters of the IBM Model 1 -LRB- Brown et al. 1993 -RRB- provides a complementary probability for each tuple in the translation table	nn_table_translation det_table_the prep_in_tuple_table det_tuple_each prep_for_probability_tuple amod_probability_complementary det_probability_a dobj_provides_probability nsubj_provides_feature amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_1 nn_Model_IBM det_Model_the prep_of_parameters_Model amod_parameters_lexical det_parameters_the prep_on_based_parameters auxpass_based_is nsubjpass_based_which dep_feature_Brown rcmod_feature_based det_feature_This
W06-1609	J93-2003	o	1 Introduction During the last few years SMT systems have evolved from the original word-based approach -LRB- Brown et al. 1993 -RRB- to phrase-based translation systems -LRB- Koehn et al. 2003 -RRB-	amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_systems_translation amod_systems_phrase-based dep_al._1993 nn_al._et amod_al._Brown prep_to_approach_systems dep_approach_al. amod_approach_word-based amod_approach_original det_approach_the dep_evolved_Koehn prep_from_evolved_approach aux_evolved_have nsubj_evolved_systems ccomp_evolved_Introduction nn_systems_SMT amod_years_few amod_years_last det_years_the prep_during_Introduction_years num_Introduction_1
W06-1626	J93-2003	o	1 Introduction Statistical language modeling has been widely used in natural language processing applications such as Automatic Speech Recognition -LRB- ASR -RRB- Statistical Machine Translation -LRB- SMT -RRB- -LRB- Brown et al. 1993 -RRB- and Information Retrieval -LRB- IR -RRB- -LRB- Ponte and Croft 1998 -RRB-	amod_Ponte_1998 conj_and_Ponte_Croft dep_Retrieval_Croft dep_Retrieval_Ponte appos_Retrieval_IR nn_Retrieval_Information amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical conj_and_Recognition_Retrieval dep_Recognition_Brown conj_and_Recognition_Translation appos_Recognition_ASR nn_Recognition_Speech nn_Recognition_Automatic prep_such_as_applications_Retrieval prep_such_as_applications_Translation prep_such_as_applications_Recognition nn_applications_processing nn_applications_language amod_applications_natural prep_in_used_applications advmod_used_widely auxpass_used_been aux_used_has nsubjpass_used_modeling nn_modeling_language amod_modeling_Statistical nn_modeling_Introduction num_modeling_1 ccomp_``_used
W06-1628	J93-2003	n	1 Introduction Phrase-based approaches -LRB- Och and Ney 2004 -RRB- to statistical machine translation -LRB- SMT -RRB- have recently achieved impressive results leading to significant improvements in accuracy over the original IBM models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM amod_models_original det_models_the prep_over_improvements_models prep_in_improvements_accuracy amod_improvements_significant prep_to_leading_improvements amod_results_impressive dep_achieved_Brown vmod_achieved_leading dobj_achieved_results advmod_achieved_recently aux_achieved_have nsubj_achieved_approaches appos_translation_SMT nn_translation_machine amod_translation_statistical dep_Och_2004 conj_and_Och_Ney prep_to_approaches_translation dep_approaches_Ney dep_approaches_Och amod_approaches_Phrase-based nn_approaches_Introduction num_approaches_1 ccomp_``_achieved
W06-2008	J93-2003	o	GIZA + + consists of a set of statistical translation models of different complexity namely the IBM ones -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_ones_Brown nn_ones_IBM det_ones_the pobj_namely_ones amod_complexity_different prep_of_models_complexity nn_models_translation amod_models_statistical prep_of_set_models det_set_a prep_consists_namely prep_of_consists_set nsubj_consists_+ nsubj_consists_GIZA conj_+_GIZA_+
W06-2402	J93-2003	o	1 Introduction Statistical machine translation -LRB- SMT -RRB- was originally focused on word to word translation and was based on the noisy channel approach -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_approach_al. nn_approach_channel amod_approach_noisy det_approach_the prep_on_based_approach auxpass_based_was nsubjpass_based_translation nn_translation_word prep_to_word_translation conj_and_focused_based prep_on_focused_word advmod_focused_originally auxpass_focused_was nsubjpass_focused_translation appos_translation_SMT nn_translation_machine amod_translation_Statistical nn_translation_Introduction num_translation_1
W06-3102	J93-2003	p	1 Introduction The availability of large amounts of so-called parallel texts has motivated the application of statistical techniques to the problem of machine translation starting with the seminal work at IBM in the early 90s -LRB- Brown et al. 1992 Brown et al. 1993 -RRB-	num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Brown_Brown amod_Brown_1992 dep_Brown_al. nn_Brown_et amod_90s_early det_90s_the prep_at_work_IBM amod_work_seminal det_work_the dep_starting_Brown prep_in_starting_90s prep_with_starting_work nn_translation_machine prep_of_problem_translation det_problem_the amod_techniques_statistical prep_of_application_techniques det_application_the prep_motivated_starting prep_to_motivated_problem dobj_motivated_application aux_motivated_has nsubj_motivated_availability amod_texts_parallel amod_texts_so-called prep_of_amounts_texts amod_amounts_large prep_of_availability_amounts det_availability_The rcmod_Introduction_motivated num_Introduction_1
W06-3102	J93-2003	o	Statistical machine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input signal e from the observed output signal f. 1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et det_languages_the prep_of_properties_languages amod_properties_syntactic amod_properties_morphological det_properties_the conj_or_morphological_syntactic appos_any_Brown prep_of_any_properties dobj_taking_any prep_into_taking_account amod_approach_word-based det_approach_a advmod_word-based_purely prepc_without_used_taking dobj_used_approach nn_systems_translation nn_systems_machine amod_systems_statistical amod_systems_Early num_systems_1 nn_systems_f. nn_systems_signal nn_systems_output amod_systems_observed det_systems_the dep_signal_e nn_signal_input det_signal_the dobj_recover_signal aux_recover_to xcomp_tries_recover nsubj_tries_one prep_in_tries_which rcmod_process_tries nn_process_recovery nn_process_signal amod_process_noisy-channel det_process_a nn_process_translation det_process_the dep_views_used prep_from_views_systems prep_as_views_process dobj_views_process nsubj_views_translation nn_translation_machine amod_translation_Statistical
W06-3104	J93-2003	o	Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables produced by GIZA + + -LRB- Brown et al. 1993 Och and Ney 2003 -RRB-	dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_GIZA_Brown conj_+_GIZA_+ agent_produced_+ agent_produced_GIZA vmod_tables_produced nn_tables_translation num_tables_4 nn_tables_Model nn_tables_IBM det_tables_the prep_from_came_tables nsubj_came_estimates nn_probabilities_translation amod_probabilities_lexical prep_of_estimates_probabilities amod_estimates_Initial
W06-3104	J93-2003	o	1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything it is reminiscent of IBM Models 15 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Models_15 nn_Models_IBM dep_reminiscent_Brown prep_of_reminiscent_Models cop_reminiscent_is nsubj_reminiscent_it dep_reminiscent_1.2 prep_to_align_anything nsubj_align_anything xcomp_let_align aux_let_will nsubj_let_approach mark_let_Because poss_approach_our amod_Grammars_Quasi-Synchronous prep_to_Synchronous_Grammars advcl_1.2_let prep_from_1.2_Synchronous
W06-3105	J93-2003	p	Specifically in the task of word alignment heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts such as the IBM word alignment models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_alignment nn_models_word nn_models_IBM det_models_the prep_such_as_counterparts_models amod_counterparts_re-estimated poss_counterparts_their dobj_underperform_counterparts advmod_underperform_consistently nsubj_underperform_approaches prep_in_underperform_task advmod_underperform_Specifically nn_coefficient_Dice det_coefficient_the prep_such_as_approaches_coefficient nn_approaches_heuristic nn_alignment_word prep_of_task_alignment det_task_the
W06-3107	J93-2003	p	Statistical models for machine translation heavily depend on the concept of alignment specifically the well known IBM word based models -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_models_Brown amod_models_based dep_word_models nn_word_IBM amod_word_known det_word_the advmod_known_well prep_of_concept_alignment det_concept_the dobj_depend_word advmod_depend_specifically prep_on_depend_concept advmod_depend_heavily nsubj_depend_models nn_translation_machine prep_for_models_translation amod_models_Statistical
W06-3107	J93-2003	o	Alignment models to structure the translation model are introduced in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_introduced_in auxpass_introduced_are nsubjpass_introduced_models nn_model_translation det_model_the dobj_structure_model aux_structure_to vmod_models_structure nn_models_Alignment
W06-3107	J93-2003	o	Nevertheless in the problem described in this article the source and the target sentences are given and we are focusing on the optimization of the aligment a The translation probability Pr -LRB- f a | e -RRB- can be rewritten as follows Pr -LRB- f a | e -RRB- = Jproductdisplay j = 1 Pr -LRB- fj aj | fj11 aj11 eI1 -RRB- = Jproductdisplay j = 1 Pr -LRB- aj | fj11 aj11 eI1 -RRB- Pr -LRB- fj | fj11 aj1 eI1 -RRB- -LRB- 2 -RRB- The probability Pr -LRB- f a | e -RRB- can be estimated by using the word-based IBM statistical alignment models -LRB- Brown et al. 1993 -RRB-	num_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_alignment amod_models_statistical nn_models_IBM amod_models_word-based det_models_the dobj_using_models agent_estimated_using auxpass_estimated_be aux_estimated_can nsubjpass_estimated_| dep_|_e det_|_a appos_f_Brown rcmod_f_estimated dep_Pr_f nn_Pr_probability det_Pr_The dep_2_Pr appos_fj11_eI1 appos_fj11_aj1 num_fj11_| nn_fj11_fj dep_Pr_2 dep_Pr_fj11 nn_Pr_Pr appos_fj11_eI1 appos_fj11_aj11 num_fj11_| nn_fj11_aj dep_Pr_fj11 num_Pr_1 dep_=_Pr amod_j_= nn_j_Jproductdisplay dep_=_j nn_fj11_| nn_fj11_aj appos_fj_eI1 conj_fj_aj11 conj_fj_fj11 amod_Pr_= dep_Pr_fj num_Pr_1 dep_=_Pr amod_j_= nn_j_Jproductdisplay dobj_=_j dep_=_e dep_|_= det_|_a appos_f_| dep_Pr_f mark_follows_as advcl_rewritten_follows auxpass_rewritten_be aux_rewritten_can nsubjpass_rewritten_| dep_|_e det_|_a dep_f_Pr rcmod_f_rewritten dep_Pr_f nn_Pr_probability nn_Pr_translation det_Pr_The dep_a_Pr dep_aligment_a det_aligment_the prep_of_optimization_aligment det_optimization_the prep_on_focusing_optimization aux_focusing_are nsubj_focusing_we conj_and_given_focusing auxpass_given_are nsubjpass_given_sentences nsubjpass_given_source prep_in_given_problem advmod_given_Nevertheless nn_sentences_target det_sentences_the conj_and_source_sentences det_source_the det_article_this prep_in_described_article vmod_problem_described det_problem_the ccomp_``_focusing ccomp_``_given
W06-3109	J93-2003	p	The most widely used single-word-based statistical alignment models -LRB- SAMs -RRB- have been proposed in -LRB- Brown et al. 1993 Ney et al. 2000 -RRB-	appos_al._2000 nn_al._et nn_al._Ney dep_al._al. num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_proposed_in auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models appos_models_SAMs nn_models_alignment amod_models_statistical amod_models_single-word-based amod_models_used det_models_The advmod_used_widely advmod_widely_most
W06-3111	J93-2003	o	Monotone Nonmonotone Target B A Positions C D Source Positions Figure 1 Two Types of Alignment The IBM model 1 -LRB- IBM-1 -RRB- -LRB- Brown et al. 1993 -RRB- assumes that all alignments have the same probability by using a uniform distribution p -LRB- fJ1 | eI1 -RRB- = 1IJ Jproductdisplay j = 1 Isummationdisplay i = 1 p -LRB- fj | ei -RRB- -LRB- 2 -RRB- We use the IBM-1 to train the lexicon parameters p -LRB- f | e -RRB- the training software is GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ cop_GIZA_is nsubj_GIZA_| dep_GIZA_p nn_software_training det_software_the dep_|_software dep_|_e nn_|_f rcmod_parameters_+ rcmod_parameters_GIZA nn_parameters_lexicon det_parameters_the dobj_train_parameters aux_train_to det_IBM-1_the xcomp_use_train dobj_use_IBM-1 nsubj_use_We dep_use_2 dep_use_p dep_use_= nsubj_use_i dep_use_Isummationdisplay dep_use_1 dep_use_= nsubj_use_j num_ei_| nn_ei_fj appos_p_ei num_p_1 nn_j_Jproductdisplay nn_j_1IJ dep_=_use npadvmod_=_p num_eI1_| nn_eI1_fJ1 appos_p_eI1 amod_distribution_uniform det_distribution_a dobj_using_distribution amod_probability_same det_probability_the parataxis_have_= prepc_by_have_using dobj_have_probability nsubj_have_alignments mark_have_that det_alignments_all ccomp_assumes_have nsubj_assumes_Types amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_1_IBM-1 dep_model_1 nn_model_IBM det_model_The appos_Types_Brown dep_Types_model prep_of_Types_Alignment num_Types_Two dep_Figure_assumes num_Figure_1 nn_Figure_Positions nn_Figure_Source nn_Figure_D nn_Figure_C nn_Figure_Positions nn_Figure_A nn_Figure_B nn_Figure_Target nn_Figure_Nonmonotone nn_Figure_Monotone
W06-3116	J93-2003	o	A Viterbi alignment computed from an IBM model 4 -LRB- Brown et al. 1993 -RRB- was computed for each translation direction	nn_direction_translation det_direction_each prep_for_computed_direction auxpass_computed_was amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_IBM det_model_an dep_computed_computed dep_computed_Brown prep_from_computed_model nsubj_computed_alignment nn_alignment_Viterbi det_alignment_A ccomp_``_computed
W06-3119	J93-2003	o	1 Introduction Recent work in machine translation has evolved from the traditional word -LRB- Brown et al. 1993 -RRB- and phrase based -LRB- Koehn et al. 2003a -RRB- models to include hierarchical phrase models -LRB- Chiang 2005 -RRB- and bilingual synchronous grammars -LRB- Melamed 2004 -RRB-	amod_Melamed_2004 dep_grammars_Melamed amod_grammars_synchronous amod_grammars_bilingual amod_Chiang_2005 conj_and_models_grammars dep_models_Chiang nn_models_phrase amod_models_hierarchical dobj_include_grammars dobj_include_models aux_include_to dep_models_Koehn amod_models_based nn_models_phrase nn_al._et dep_Koehn_2003a dep_Koehn_al. conj_al._1993 nn_al._et vmod_Brown_include conj_and_Brown_models dep_Brown_al. dep_word_models dep_word_Brown amod_word_traditional det_word_the prep_from_evolved_word aux_evolved_has nsubj_evolved_work nn_translation_machine prep_in_work_translation amod_work_Recent nn_work_Introduction num_work_1 ccomp_``_evolved
W06-3123	J93-2003	o	The original IBM Models -LRB- Brown et al. 1993 -RRB- learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data	nn_data_training prep_of_amounts_data amod_amounts_large prep_from_parameters_amounts nn_parameters_model dobj_estimate_parameters aux_estimate_to xcomp_feasible_estimate advmod_feasible_computationally amod_it_feasible dobj_makes_it nsubj_makes_which rcmod_probabilities_makes nn_probabilities_alignment amod_probabilities_word-to-word dobj_learn_probabilities nsubj_learn_Models amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Models_Brown nn_Models_IBM amod_Models_original det_Models_The
W06-3125	J93-2003	o	Based on IBM Model 1 lexical parameters -LRB- Brown et al. 1993 -RRB- providing a complementary probability for each tuple in the translation table	nn_table_translation det_table_the prep_in_tuple_table det_tuple_each prep_for_probability_tuple amod_probability_complementary det_probability_a dobj_providing_probability amod_Brown_1993 dep_Brown_al. nn_Brown_et amod_parameters_lexical num_parameters_1 nn_parameters_Model nn_parameters_IBM dep_Based_providing dep_Based_Brown prep_on_Based_parameters ccomp_``_Based
W06-3602	J93-2003	o	4 -RRB- it constitutes a bijection between source and target sentence positions since the intersecting alignments are functions according to their definition in -LRB- Brown et al. 1993 -RRB- 3	num_al._3 num_al._1993 nn_al._et amod_al._Brown prep_in_definition_al. poss_definition_their pobj_functions_definition prepc_according_to_functions_to cop_functions_are nsubj_functions_alignments mark_functions_since amod_alignments_intersecting det_alignments_the nn_positions_sentence nn_positions_target conj_and_source_positions prep_between_bijection_positions prep_between_bijection_source det_bijection_a advcl_constitutes_functions dobj_constitutes_bijection nsubj_constitutes_it dep_constitutes_4
W07-0212	J93-2003	o	Appendix A Derivation of the Probability of RWE We take a noisy channel approach which is a common technique in NLP -LRB- for example -LRB- Brown et al. 1993 -RRB- -RRB- including spellchecking -LRB- Kernighan et al. 1990 -RRB-	amod_Kernighan_1990 dep_Kernighan_al. nn_Kernighan_et dep_spellchecking_Kernighan num_al._1993 nn_al._et amod_al._Brown dep_for_al. pobj_for_example dep_technique_for prep_in_technique_NLP amod_technique_common det_technique_a cop_technique_is nsubj_technique_which prep_including_approach_spellchecking rcmod_approach_technique nn_approach_channel amod_approach_noisy det_approach_a dobj_take_approach nsubj_take_We prep_of_Probability_RWE det_Probability_the rcmod_Derivation_take prep_of_Derivation_Probability dep_A_Derivation nn_A_Appendix
W07-0403	J93-2003	o	This alignment system is powered by the IBM translation models -LRB- Brown et al. 1993 -RRB- in which one sentence generates the other	det_other_the dobj_generates_other nsubj_generates_sentence prep_in_generates_which num_sentence_one amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_translation nn_models_IBM det_models_the parataxis_powered_generates dep_powered_Brown agent_powered_models auxpass_powered_is nsubjpass_powered_system nn_system_alignment det_system_This
W07-0409	J93-2003	n	1 Introduction Recent works in statistical machine translation -LRB- SMT -RRB- shows how phrase-based modeling -LRB- Och and Ney 2000a Koehn et al. 2003 -RRB- significantly outperform the historical word-based modeling -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_modeling_al. amod_modeling_word-based amod_modeling_historical det_modeling_the dobj_outperform_modeling advmod_outperform_significantly nsubj_outperform_modeling num_Koehn_2003 nn_Koehn_al. nn_Koehn_et dep_Och_Koehn conj_and_Och_2000a conj_and_Och_Ney dep_modeling_2000a dep_modeling_Ney dep_modeling_Och amod_modeling_phrase-based advmod_modeling_how ccomp_shows_outperform nsubj_shows_works appos_translation_SMT nn_translation_machine amod_translation_statistical prep_in_works_translation amod_works_Recent nn_works_Introduction num_works_1
W07-0412	J93-2003	o	Systems based on word-to-word lexicons such as the IBM systems -LRB- Brown et al. 1990 Brown et al. 1993 -RRB- incorporate further devices that allow reordering of words -LRB- a distortion model -RRB- and ranking of alternatives -LRB- a monolingual language model -RRB-	nn_model_language amod_model_monolingual det_model_a appos_alternatives_model prep_of_ranking_alternatives nn_model_distortion det_model_a conj_and_words_ranking appos_words_model prep_of_reordering_ranking prep_of_reordering_words dobj_allow_reordering nsubj_allow_that rcmod_devices_allow amod_devices_further dobj_incorporate_devices nsubj_incorporate_Systems num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Brown_Brown appos_Brown_1990 dep_Brown_al. nn_Brown_et appos_systems_Brown nn_systems_IBM det_systems_the prep_such_as_lexicons_systems amod_lexicons_word-to-word prep_on_based_lexicons vmod_Systems_based
W07-0703	J93-2003	o	To generate phrase pairs from a parallel corpus we use the diag-and phrase induction algorithm described in -LRB- Koehn et al 2003 -RRB- with symmetrized word alignments generated using IBM model 2 -LRB- Brown et al 1993 -RRB-	dep_al_1993 nn_al_et amod_al_Brown dep_model_al num_model_2 nn_model_IBM dobj_using_model xcomp_generated_using vmod_alignments_generated nn_alignments_word amod_alignments_symmetrized prep_with_Koehn_alignments amod_Koehn_2003 dep_Koehn_al nn_Koehn_et prep_in_described_Koehn vmod_algorithm_described nn_algorithm_induction nn_algorithm_phrase amod_algorithm_diag-and det_algorithm_the dobj_use_algorithm nsubj_use_we advcl_use_generate amod_corpus_parallel det_corpus_a nn_pairs_phrase prep_from_generate_corpus dobj_generate_pairs aux_generate_To
W07-0705	J93-2003	o	In the original work -LRB- Brown et al. 1993 -RRB- the posterior probability p -LRB- eI1 | fJ1 -RRB- is decomposed following a noisy-channel approach but current stateof-the-art systems model the translation probability directly using a log-linear model -LRB- Och and Ney 2002 -RRB- p -LRB- eI1 | fJ1 -RRB- = exp parenleftBigsummationtextM m = 1 mhm -LRB- e I1 fJ1 -RRB- parenrightBig summationdisplay ? eI1 exp parenleftBigsummationtextM m = 1 mhm -LRB- ? eI1 fJ1 -RRB- parenrightBig -LRB- 2 -RRB- with hm different models m scaling factors and the denominator a normalization factor that can be ignored in the maximization process	nn_process_maximization det_process_the prep_in_ignored_process auxpass_ignored_be aux_ignored_can nsubjpass_ignored_that rcmod_factor_ignored nn_factor_normalization det_factor_a dep_denominator_factor det_denominator_the nn_factors_scaling nn_factors_m conj_and_models_denominator conj_and_models_factors amod_models_different nn_models_hm prep_with_2_denominator prep_with_2_factors prep_with_2_models nn_parenrightBig_m appos_eI1_fJ1 num_mhm_1 dep_=_mhm dep_m_eI1 amod_m_= nn_m_parenleftBigsummationtextM nn_m_exp nn_m_eI1 dep_summationdisplay_2 dep_summationdisplay_parenrightBig nn_summationdisplay_parenrightBig nn_summationdisplay_m amod_summationdisplay_= nn_summationdisplay_p dep_I1_fJ1 dep_I1_e num_mhm_1 dep_=_mhm dep_m_I1 amod_m_= nn_m_parenleftBigsummationtextM nn_m_exp num_fJ1_| nn_fJ1_eI1 appos_p_fJ1 dep_Och_2002 conj_and_Och_Ney appos_model_Ney appos_model_Och amod_model_log-linear det_model_a dobj_using_model advmod_using_directly nn_probability_translation det_probability_the dep_model_summationdisplay vmod_model_using dobj_model_probability nsubj_model_systems amod_systems_stateof-the-art amod_systems_current amod_approach_noisy-channel det_approach_a conj_but_decomposed_model prep_following_decomposed_approach auxpass_decomposed_is nsubjpass_decomposed_p dep_decomposed_al. prep_in_decomposed_work num_fJ1_| nn_fJ1_eI1 appos_p_fJ1 nn_p_probability amod_p_posterior det_p_the num_al._1993 nn_al._et amod_al._Brown amod_work_original det_work_the ccomp_``_model ccomp_``_decomposed
W07-0708	J93-2003	o	A monotonic segmentation copes with monotonic alignments that is j < k ?? aj < ak following the notation of -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_of_al. prep_notation_of det_notation_the quantmod_ak_< num_aj_ak num_aj_?? nn_aj_k dep_<_aj amod_j_< prep_following_is_notation nsubj_is_j nsubj_is_that ccomp_is_copes amod_alignments_monotonic prep_with_copes_alignments nsubj_copes_segmentation amod_segmentation_monotonic det_segmentation_A
W07-0715	J93-2003	n	-LRB- 2006 -RRB- tried a different generative phrase translation model analogous to IBM word-translation Model 3 -LRB- Brown et al. 1993 -RRB- and again found that the standard model outperformed their generative model	amod_model_generative poss_model_their dobj_outperformed_model nsubj_outperformed_model mark_outperformed_that amod_model_standard det_model_the ccomp_found_outperformed amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_3 nn_Model_word-translation nn_Model_IBM prep_to_analogous_Model amod_model_analogous nn_model_translation nn_model_phrase amod_model_generative amod_model_different det_model_a dep_tried_found advmod_tried_again cc_tried_and dep_tried_Brown dobj_tried_model dep_tried_2006
W07-0715	J93-2003	o	The lexical scores are computed as the -LRB- unnormalized -RRB- log probability of the Viterbi alignment for a phrase pair under IBM word-translation Model 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_1 nn_Model_word-translation nn_Model_IBM prep_under_pair_Model nn_pair_phrase det_pair_a prep_for_alignment_pair nn_alignment_Viterbi det_alignment_the dep_probability_Brown prep_of_probability_alignment nn_probability_log amod_probability_unnormalized det_probability_the prep_as_computed_probability auxpass_computed_are nsubjpass_computed_scores amod_scores_lexical det_scores_The ccomp_``_computed
W07-0717	J93-2003	o	To derive the joint counts c -LRB- ? s ? t -RRB- from which p -LRB- ? s | ? t -RRB- and p -LRB- ? t | ? s -RRB- are estimated we use the phrase induction algorithm described in -LRB- Koehn et al. 2003 -RRB- with symmetrized word alignments generated using IBM model 2 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_2 nn_model_IBM dobj_using_model vmod_generated_using vmod_alignments_generated nn_alignments_word amod_alignments_symmetrized dep_Koehn_Brown prep_with_Koehn_alignments amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et prep_in_described_Koehn vmod_algorithm_described nn_algorithm_induction nn_algorithm_phrase det_algorithm_the dobj_use_algorithm nsubj_use_we advcl_use_derive auxpass_estimated_are nsubjpass_estimated_s nsubjpass_estimated_s prep_from_estimated_which nn_s_| nn_s_t nn_s_p conj_and_s_s dep_s_t num_s_| nn_s_p dep_s_t rcmod_c_estimated dep_c_s nn_c_counts amod_c_joint det_c_the dobj_derive_c aux_derive_To
W07-0721	J93-2003	o	This feature which is based on the lexical parameters of the IBM Model 1 -LRB- Brown et al. 1993 -RRB- provides a complementary probability for each tuple in the translation table	nn_table_translation det_table_the prep_in_tuple_table det_tuple_each prep_for_probability_tuple amod_probability_complementary det_probability_a dobj_provides_probability nsubj_provides_feature amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_1 nn_Model_IBM det_Model_the prep_of_parameters_Model amod_parameters_lexical det_parameters_the prep_on_based_parameters auxpass_based_is nsubjpass_based_which dep_feature_Brown rcmod_feature_based det_feature_This
W07-0724	J93-2003	o	These lists are rescored with the different models described above a character penalty and three different features based on IBM Models 1 and 2 -LRB- Brown et al. 1993 -RRB- calculated in both translation directions	nn_directions_translation preconj_directions_both prep_in_calculated_directions amod_Brown_1993 dep_Brown_al. nn_Brown_et conj_and_1_2 vmod_Models_calculated appos_Models_Brown dep_Models_2 dep_Models_1 nn_Models_IBM prep_on_based_Models vmod_features_based amod_features_different num_features_three conj_and_penalty_features nn_penalty_character det_penalty_a advmod_described_above vmod_models_described amod_models_different det_models_the dobj_rescored_features dobj_rescored_penalty prep_with_rescored_models auxpass_rescored_are nsubjpass_rescored_lists det_lists_These
W07-1205	J93-2003	o	Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text -LRB- Brown et al. 1993 -RRB- for identifying corresponding word blocks assuming no further linguistic analysis of the source or target language	nn_language_target conj_or_source_language det_source_the prep_of_analysis_language prep_of_analysis_source amod_analysis_linguistic amod_analysis_further neg_analysis_no dobj_assuming_analysis nn_blocks_word amod_blocks_corresponding dobj_identifying_blocks num_al._1993 nn_al._et amod_al._Brown amod_text_parallel prep_of_corpus_text nn_corpus_training det_corpus_the prep_over_alignment_corpus nn_alignment_word amod_alignment_statistical amod_alignment_standard det_alignment_a xcomp_builds_assuming prepc_for_builds_identifying dep_builds_al. prep_on_top_of_builds_alignment nsubj_builds_Training nn_model_translation nn_model_phrase det_model_the prep_of_Training_model
W08-0306	J93-2003	o	GIZA + + -LRB- Och and Ney 2003 -RRB- an implementation of the IBM -LRB- Brown et al. 1993 -RRB- and HMM -LRB- ? -RRB-	dep_HMM_-LRB- amod_Brown_1993 dep_Brown_al. nn_Brown_et det_IBM_the dep_implementation_Brown prep_of_implementation_IBM det_implementation_an num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_and_GIZA_HMM conj_+_GIZA_implementation conj_+_GIZA_+
W08-0307	J93-2003	o	The simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link words from 2We use IBM-1 to IBM-5 models -LRB- Brown et al. 1993 -RRB- implemented with GIZA + + -LRB- Och and Ney 2003 -RRB-	amod_Och_2003 conj_and_Och_Ney dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ prep_with_implemented_+ prep_with_implemented_GIZA amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_IBM-5 nn_IBM-1_use nn_IBM-1_2We prep_to_link_models prep_from_link_IBM-1 dobj_link_words nsubj_link_which rcmod_alignments_link dobj_discard_alignments aux_discard_to dep_used_implemented dep_used_Brown xcomp_used_discard auxpass_used_is nsubjpass_used_idea nn_chunk_target amod_chunk_possible amod_chunk_single det_chunk_a prep_in_aligned_chunk prep_to_aligned_words advmod_aligned_typically auxpass_aligned_are nsubjpass_aligned_words mark_aligned_that nn_chunk_source det_chunk_a prep_in_words_chunk ccomp_idea_aligned amod_idea_simple det_idea_The
W08-0321	J93-2003	p	In the well-known so-called IBM word alignment models -LRB- Brown et al. 1993 -RRB- re-estimating the model parameters depends on the empirical probability P -LRB- ek fk -RRB- for each sentence pair -LRB- ek fk -RRB-	appos_ek_fk dep_pair_ek nn_pair_sentence det_pair_each appos_ek_fk dep_P_ek nn_P_probability amod_P_empirical det_P_the prep_for_depends_pair prep_on_depends_P nsubj_depends_re-estimating dep_depends_Brown prep_in_depends_models nn_parameters_model det_parameters_the dobj_re-estimating_parameters amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_alignment nn_models_word nn_models_IBM amod_models_so-called amod_models_well-known det_models_the
W08-0321	J93-2003	o	The empirical probability for each sentence pair is estimated by maximum likelihood estimation over the training data -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_data_training det_data_the prep_over_estimation_data nn_estimation_likelihood nn_estimation_maximum dep_estimated_Brown agent_estimated_estimation auxpass_estimated_is nsubjpass_estimated_probability nn_pair_sentence det_pair_each prep_for_probability_pair amod_probability_empirical det_probability_The ccomp_``_estimated
W08-0326	J93-2003	o	Assuming that the parameters P -LRB- etk | fsk -RRB- are known the most likely alignment is computed by a simple dynamic-programming algorithm .1 Instead of using an Expectation-Maximization algorithm to estimate these parameters as commonly done when performing word alignment -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- we directly compute these parameters by relying on the information contained within the chunks	det_chunks_the prep_within_contained_chunks nsubj_contained_parameters mark_contained_that det_information_the prep_on_relying_information det_parameters_these prepc_by_compute_relying dobj_compute_parameters advmod_compute_directly nsubj_compute_we num_Och_2003 conj_and_Och_Ney dep_al._Ney dep_al._Och num_al._1993 nn_al._et amod_al._Brown dep_alignment_al. nn_alignment_word dobj_performing_alignment advmod_performing_when advcl_done_performing advmod_done_commonly mark_done_as det_parameters_these dobj_estimate_parameters aux_estimate_to nn_algorithm_Expectation-Maximization det_algorithm_an vmod_using_estimate dobj_using_algorithm num_algorithm_.1 amod_algorithm_dynamic-programming amod_algorithm_simple det_algorithm_a advcl_computed_done prepc_instead_of_computed_using agent_computed_algorithm auxpass_computed_is nsubjpass_computed_alignment amod_alignment_likely det_alignment_the advmod_likely_most auxpass_known_are nsubjpass_known_P num_fsk_| nn_fsk_etk appos_P_fsk rcmod_parameters_compute rcmod_parameters_computed rcmod_parameters_known det_parameters_the ccomp_Assuming_contained ccomp_``_Assuming
W08-0333	J93-2003	o	The IBM models together with a Hidden Markov Model -LRB- HMM -RRB- form a class of generative models that are based on a lexical translation model P -LRB- fj | ei -RRB- where each word fj in the foreign sentence fm1 is generated by precisely one word ei in the sentence el1 independently of the other translation decisions -LRB- Brown et al. 1993 Vogel et al. 1996 Och and Ney 2000 -RRB-	dep_Och_2000 conj_and_Och_Ney num_Vogel_1996 nn_Vogel_al. nn_Vogel_et dep_Brown_Ney dep_Brown_Och dep_Brown_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_decisions_translation amod_decisions_other det_decisions_the dep_independently_Brown prep_of_independently_decisions nn_el1_sentence det_el1_the prep_in_ei_el1 nn_ei_word num_ei_one advmod_ei_precisely agent_generated_ei auxpass_generated_is nsubjpass_generated_fj advmod_generated_where nn_fm1_sentence amod_fm1_foreign det_fm1_the prep_in_fj_fm1 nn_fj_word det_fj_each num_ei_| nn_ei_fj rcmod_P_generated appos_P_ei nn_P_model nn_P_translation amod_P_lexical det_P_a prep_on_based_P auxpass_based_are nsubjpass_based_that rcmod_models_based amod_models_generative prep_of_class_models det_class_a advmod_form_independently dobj_form_class prep_together_with_form_Model nsubj_form_models appos_Model_HMM nn_Model_Markov nn_Model_Hidden det_Model_a nn_models_IBM det_models_The
W08-0333	J93-2003	o	203 Estimating the parameters for these models is more difficult -LRB- and more computationally expensive -RRB- than with the models considered in the previous section rather than simply being able to count the word pairs and alignment relationships and estimate the models directly we must use an existing model to compute the expected counts for all possible alignments and then use these counts to update the new model .7 This training strategy is referred to as expectationmaximization -LRB- EM -RRB- and is guaranteed to always improve the quality of the prior model at each iteration -LRB- Brown et al. 1993 Dempster et al. 1977 -RRB-	nn_al._et nn_al._Dempster dep_al._1977 dep_al._al. num_al._1993 nn_al._et amod_al._Brown det_iteration_each amod_model_prior det_model_the prep_of_quality_model det_quality_the prep_at_improve_iteration dobj_improve_quality advmod_improve_always aux_improve_to xcomp_guaranteed_improve auxpass_guaranteed_is nsubjpass_guaranteed_strategy appos_expectationmaximization_EM conj_and_referred_guaranteed prep_as_to_referred_expectationmaximization auxpass_referred_is nsubjpass_referred_strategy nn_strategy_training det_strategy_This num_strategy_.7 dep_model_al. dep_model_guaranteed dep_model_referred amod_model_new det_model_the dobj_update_model aux_update_to det_counts_these vmod_use_update dobj_use_counts advmod_use_then nsubj_use_we amod_alignments_possible det_alignments_all prep_for_counts_alignments amod_counts_expected det_counts_the dobj_compute_counts aux_compute_to amod_model_existing det_model_an conj_and_use_use vmod_use_compute dobj_use_model aux_use_must nsubj_use_we ccomp_use_estimate ccomp_use_able ccomp_use_difficult det_models_the advmod_estimate_directly dobj_estimate_models nsubj_estimate_203 nn_relationships_alignment conj_and_pairs_relationships nn_pairs_word det_pairs_the dobj_count_relationships dobj_count_pairs aux_count_to xcomp_able_count cop_able_being advmod_able_simply nsubj_able_203 amod_section_previous det_section_the prep_in_considered_section vmod_models_considered det_models_the pobj_with_models pcomp_than_with conj_and_expensive_computationally advmod_computationally_more conj_and_difficult_estimate conj_negcc_difficult_able prep_difficult_than dep_difficult_expensive dep_difficult_computationally advmod_difficult_more cop_difficult_is nsubj_difficult_203 det_models_these prep_for_parameters_models det_parameters_the dobj_Estimating_parameters vmod_203_Estimating
W08-0409	J93-2003	o	4.3 Baselines 4.3.1 Word Alignment We used the GIZA + + implementation of IBM word alignment model 4 -LRB- Brown et al. 1993 Och and Ney 2003 -RRB- for word alignment and the heuristics described in -LRB- Och and Ney 2003 -RRB- to derive the intersection and refined alignment	amod_alignment_refined conj_and_intersection_alignment det_intersection_the dobj_derive_alignment dobj_derive_intersection aux_derive_to num_Och_2003 conj_and_Och_Ney dep_in_Ney dep_in_Och xcomp_described_derive prep_described_in vmod_heuristics_described det_heuristics_the nn_alignment_word dep_Och_2003 conj_and_Och_Ney dep_Brown_Ney dep_Brown_Och amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_alignment nn_model_word nn_model_IBM prep_of_implementation_model pobj_+_implementation appos_GIZA_Brown conj_+_GIZA_+ det_GIZA_the prep_for_used_alignment dobj_used_+ dobj_used_GIZA nsubj_used_We conj_and_Alignment_heuristics rcmod_Alignment_used nn_Alignment_Word num_Alignment_4.3.1 nn_Alignment_Baselines num_Alignment_4.3
W08-0409	J93-2003	p	Generative word alignment models initially developed at IBM -LRB- Brown et al. 1993 -RRB- and then augmented by an HMM-based model -LRB- Vogel et al. 1996 -RRB- have provided powerful modeling capability for word alignment	nn_alignment_word nn_capability_modeling amod_capability_powerful prep_for_provided_alignment dobj_provided_capability aux_provided_have amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et amod_model_HMM-based det_model_an prep_by_augmented_model advmod_augmented_then nsubj_augmented_models amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_IBM_Brown dep_developed_provided dep_developed_Vogel conj_and_developed_augmented prep_at_developed_IBM advmod_developed_initially nsubj_developed_models nn_models_alignment nn_models_word amod_models_Generative
W08-0409	J93-2003	o	The notation will assume ChineseEnglish word alignment and ChineseEnglish MT. Here we adopt a notation similar to -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_to_Brown prep_similar_to amod_notation_similar det_notation_a dobj_adopt_notation nsubj_adopt_we advmod_adopt_Here nn_MT._ChineseEnglish dep_alignment_adopt conj_and_alignment_MT. nn_alignment_word nn_alignment_ChineseEnglish dobj_assume_MT. dobj_assume_alignment aux_assume_will nsubj_assume_notation det_notation_The
W08-0509	J93-2003	o	The word alignment models implemented in GIZA + + the so-called IBM -LRB- Brown et al. 1993 -RRB- and HMM alignment models -LRB- Vogel et al. 1996 -RRB- are typical implementation of the EM algorithm -LRB- Dempster et al. 1977 -RRB-	amod_Dempster_1977 dep_Dempster_al. nn_Dempster_et nn_algorithm_EM det_algorithm_the dep_implementation_Dempster prep_of_implementation_algorithm amod_implementation_typical cop_implementation_are amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et nn_models_alignment nn_models_HMM nn_models_IBM nn_models_+ nn_models_GIZA amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_IBM_Brown amod_IBM_so-called det_IBM_the conj_and_GIZA_HMM conj_+_GIZA_IBM conj_+_GIZA_+ dep_implemented_implementation dep_implemented_Vogel prep_in_implemented_models nsubj_implemented_models nn_models_alignment nn_models_word det_models_The ccomp_``_implemented
W08-0509	J93-2003	o	2.2 Implementation of GIZA + + GIZA + + is an implementation of ML estimators for several statistical alignment models including IBM Model 1 through 5 -LRB- Brown et al. 1993 -RRB- HMM -LRB- Vogel et al. 1996 -RRB- and Model 6 -LRB- Och and Ney 2003 -RRB-	dep_Och_2003 conj_and_Och_Ney dep_Model_Ney dep_Model_Och num_Model_6 amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_HMM_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et quantmod_5_through number_5_1 conj_and_Model_Model conj_and_Model_HMM dep_Model_Brown num_Model_5 nn_Model_IBM prep_including_models_Model prep_including_models_HMM prep_including_models_Model nn_models_alignment amod_models_statistical amod_models_several nn_estimators_ML prep_for_implementation_models prep_of_implementation_estimators det_implementation_an cop_implementation_is nsubj_implementation_+ nsubj_implementation_+ nsubj_implementation_GIZA pobj_+_GIZA conj_+_GIZA_+ conj_+_GIZA_+ prepc_of_Implementation_implementation num_Implementation_2.2 dep_``_Implementation
W08-0509	J93-2003	o	For example -LRB- Brown et al. 1993 -RRB- suggested two different methods using only the alignment with the maximum probability the so-called Viterbi alignment or generating a set of alignments by starting from the Viterbi alignment and making changes which keep the alignment probability high	amod_probability_high dep_alignment_probability det_alignment_the dobj_keep_alignment nsubj_keep_which rcmod_changes_keep dobj_making_changes nn_alignment_Viterbi det_alignment_the conj_and_starting_making prep_from_starting_alignment prep_of_set_alignments det_set_a prepc_by_generating_making prepc_by_generating_starting dobj_generating_set conj_or_alignment_generating nn_alignment_Viterbi amod_alignment_so-called det_alignment_the nn_probability_maximum det_probability_the det_alignment_the advmod_alignment_only dobj_using_generating dobj_using_alignment prep_with_using_probability dobj_using_alignment amod_methods_different num_methods_two dep_suggested_using dobj_suggested_methods nsubj_suggested_Brown prep_for_suggested_example amod_Brown_1993 dep_Brown_al. nn_Brown_et
W09-0407	J93-2003	o	We use the IBM Model 1 -LRB- Brown et al. 1993 -RRB- and the Hidden Markov Model -LRB- HMM -LRB- Vogel et al. 1996 -RRB- -RRB- to estimate the alignment model	nn_model_alignment det_model_the dobj_estimate_model aux_estimate_to amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et appos_HMM_Vogel vmod_Model_estimate dep_Model_HMM nn_Model_Markov nn_Model_Hidden det_Model_the amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_1 nn_Model_IBM det_Model_the conj_and_use_Model dobj_use_Model nsubj_use_We
W09-0412	J93-2003	o	We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 -LRB- Brown et al. 1993 -RRB- combined them using the intersect + grow heuristic -LRB- Och and Ney 2003 -RRB- and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach -LRB- Och and Ney 2004 -RRB-	amod_Och_2004 conj_and_Och_Ney dep_approach_Ney dep_approach_Och nn_approach_template nn_approach_alignment det_approach_the dobj_using_approach num_length_7 nn_length_maximum vmod_pairs_using prep_of_pairs_length nn_pairs_translation amod_pairs_phrase-level amod_pairs_extracted conj_and_Och_2003 conj_and_Och_Ney nn_Och_heuristic dep_intersect_2003 dep_intersect_Ney dep_intersect_Och conj_+_intersect_grow det_intersect_the dobj_using_grow dobj_using_intersect xcomp_combined_using dobj_combined_them amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_4 nn_model_IBM dobj_using_model nn_alignments_word amod_alignments_directed amod_alignments_Spanish-to-English conj_and_English-to-Spanish_alignments amod_English-to-Spanish_separate conj_and_built_pairs dep_built_combined dep_built_Brown vmod_built_using dobj_built_alignments dobj_built_English-to-Spanish advmod_built_then nsubj_built_We ccomp_``_pairs ccomp_``_built
W09-0420	J93-2003	o	Word alignments were generated using Model 4 -LRB- Brown et al. 1993 -RRB- using the multi-threaded implementation of GIZA + + -LRB- Och and Ney 2003 Gao and Vogel 2008 -RRB-	amod_Gao_2008 conj_and_Gao_Vogel dep_Och_Vogel dep_Och_Gao conj_and_Och_2003 conj_and_Och_Ney pobj_+_2003 pobj_+_Ney pobj_+_Och conj_+_GIZA_+ prep_of_implementation_+ prep_of_implementation_GIZA amod_implementation_multi-threaded det_implementation_the dobj_using_implementation amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_4 xcomp_using_using dobj_using_Model xcomp_generated_using auxpass_generated_were nsubjpass_generated_alignments nn_alignments_Word
W09-0430	J93-2003	o	Then the two models and a search module are used to decode the best translation -LRB- Brown et al. 1993 Koehn et al. 2003 -RRB-	appos_al._2003 nn_al._et nn_al._Koehn dep_al._al. num_al._1993 nn_al._et amod_al._Brown dep_translation_al. amod_translation_best det_translation_the dobj_decode_translation aux_decode_to xcomp_used_decode auxpass_used_are nsubjpass_used_module nsubjpass_used_models advmod_used_Then nn_module_search det_module_a conj_and_models_module num_models_two det_models_the
W09-0430	J93-2003	o	Several automatic sentence alignment approaches have been proposed based on sentence length -LRB- Brown et al. 1991 -RRB- and lexical information -LRB- Kay and Roscheisen 1993 -RRB-	amod_Kay_1993 conj_and_Kay_Roscheisen dep_information_Roscheisen dep_information_Kay amod_information_lexical num_al._1991 nn_al._et amod_al._Brown nn_length_sentence conj_and_proposed_information dep_proposed_al. prep_based_on_proposed_length auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches nn_approaches_alignment nn_approaches_sentence amod_approaches_automatic amod_approaches_Several
W09-0432	J93-2003	o	To address this drawback we proposed a new method3 to compute a more reliable and smoothed score in the undefined case based on the IBM model 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et num_model_1 nn_model_IBM det_model_the amod_case_undefined det_case_the prep_in_score_case amod_score_smoothed amod_score_reliable det_score_a conj_and_reliable_smoothed advmod_reliable_more dobj_compute_score aux_compute_to vmod_method3_compute amod_method3_new det_method3_a dep_proposed_Brown prep_based_on_proposed_model dobj_proposed_method3 nsubj_proposed_we advcl_proposed_address det_drawback_this dobj_address_drawback aux_address_To
W09-0603	J93-2003	o	The lexical acquisition phase uses the GIZA + + word-alignment tool an implementation -LRB- Och and Ney 2003 -RRB- of IBM Model 5 -LRB- Brown et al. 1993 -RRB- to construct an alignment of MRs with NL strings	nn_strings_NL prep_with_MRs_strings prep_of_alignment_MRs det_alignment_an dobj_construct_alignment aux_construct_to amod_Brown_1993 dep_Brown_al. nn_Brown_et num_Model_5 nn_Model_IBM dep_Och_2003 conj_and_Och_Ney vmod_implementation_construct dep_implementation_Brown prep_of_implementation_Model dep_implementation_Ney dep_implementation_Och det_implementation_an nn_tool_word-alignment dep_tool_+ appos_GIZA_implementation conj_+_GIZA_tool det_GIZA_the dobj_uses_tool dobj_uses_GIZA nsubj_uses_phase nn_phase_acquisition amod_phase_lexical det_phase_The
W09-1804	J93-2003	o	Probabilistic generative models like IBM 1-5 -LRB- Brown et al. 1993 -RRB- HMM -LRB- Vogel et al. 1996 -RRB- ITG -LRB- Wu 1997 -RRB- and LEAF -LRB- Fraser and Marcu 2007 -RRB- define formulas for P -LRB- f | e -RRB- or P -LRB- e f -RRB- with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1 Word alignment exercise -LRB- Knight 1997 -RRB-	dep_Knight_1997 appos_exercise_Knight nn_exercise_alignment nn_exercise_Word num_Figure_1 nn_Figure_gat nn_Figure_vat nn_Figure_arrat nn_Figure_forat nn_Figure_nnat dobj_wat_Figure nsubj_wat_mok nn_mok_hihok nn_mok_izok nn_mok_nok nn_mok_rarok nn_mok_lalok nn_mok_zanzanat nn_mok_mat nn_mok_arrat nsubj_wat_zanzanok nn_zanzanok_yorok nn_zanzanok_hihok nn_zanzanok_crrrok nn_zanzanok_nok nn_zanzanok_lalok nn_zanzanok_hilat nn_zanzanok_bat nn_zanzanok_mat nn_zanzanok_gat nn_zanzanok_nnat amod_zanzanok_wat nn_clok_ghirok nn_clok_yorok nn_clok_nok nn_clok_mok nn_clok_lalok nn_clok_at-yurp nn_clok_oloat nn_clok_quat nn_clok_nnat nn_clok_totat nn_clok_ok-yurp nn_clok_kantok nn_clok_izok nn_clok_nok nn_clok_wiwok nn_clok_nnat nn_clok_rrat nn_clok_pippat nn_clok_lat nn_clok_iat nn_clok_nok nn_clok_plok nn_clok_anok nn_clok_brok nn_clok_lalok nn_clok_eneat nn_clok_vat nn_clok_dat dobj_wat_clok nsubj_wat_bichat nn_bichat_jjat amod_bichat_wat nn_enemok_izok nn_enemok_sprok nn_enemok_lalok nn_enemok_ororok nn_enemok_farok nn_enemok_lalok nn_enemok_cat nn_enemok_quat nn_enemok_krat nn_enemok_dat dep_wat_exercise parataxis_wat_wat dep_wat_nnat dep_wat_wat dep_wat_wat dobj_wat_enemok nsubj_wat_stok nn_stok_jok nn_stok_izok nn_stok_sprok nn_stok_lalok nn_stok_cat nn_stok_quat nn_stok_jjat nn_stok_totat nn_stok_stok nn_stok_izok nn_stok_farok nn_stok_wiwok nn_stok_lat dep_sat_wat nn_pippat_krat nn_pippat_at-voon nn_pippat_jok nn_pippat_brok nn_pippat_drok nn_pippat_anok nn_pippat_ok-voon nn_pippat_dat nn_pippat_rrat nn_pippat_pippat nn_pippat_at-voon nn_pippat_at-drubel nn_pippat_sprok nn_pippat_plok nn_pippat_anok nn_pippat_ok-voon amod_pippat_ok-drubel nn_pippat_hilat nn_pippat_vat nn_pippat_arrat nn_pippat_dat nn_pippat_totat nn_pippat_ghirok nn_pippat_hihok nn_pippat_izok nn_pippat_sprok nn_pippat_erok nn_pippat_dat nn_pippat_bichat nn_pippat_at-voon nn_pippat_sprok nn_pippat_ororok amod_pippat_ok-voon dep_|_f dep_|_e conj_or_|_P dep_|_e nn_|_f dep_P_P dep_P_| prep_for_formulas_P dep_define_sat prep_with_define_pippat dobj_define_formulas nsubj_define_models dep_Fraser_2007 conj_and_Fraser_Marcu appos_LEAF_Marcu appos_LEAF_Fraser dep_Wu_1997 appos_ITG_Wu amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et conj_and_HMM_LEAF conj_and_HMM_ITG dep_HMM_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et num_IBM_1-5 appos_models_LEAF appos_models_ITG appos_models_HMM dep_models_Brown prep_like_models_IBM amod_models_generative nn_models_Probabilistic
W09-1804	J93-2003	o	Practical Model 4 systems therefore make substantial search approximations -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_approximations_Brown nn_approximations_search amod_approximations_substantial dobj_make_approximations advmod_make_therefore nsubj_make_systems num_systems_4 nn_systems_Model nn_systems_Practical
W09-1804	J93-2003	o	For now we consider it to be one where Every foreign word is aligned exactly once -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et advmod_once_exactly dep_aligned_Brown advmod_aligned_once auxpass_aligned_is nsubjpass_aligned_word advmod_aligned_where amod_word_foreign det_word_Every rcmod_one_aligned cop_one_be aux_one_to xcomp_consider_one dobj_consider_it nsubj_consider_we prep_consider_For pobj_For_now
W09-1804	J93-2003	o	We use these tuples to calculate a balanced f-score against the gold alignment tuples .4 Method Dict size f-score Gold 28 100.0 Monotone 39 68.9 IBM-1 -LRB- Brown et al. 1993 -RRB- 30 80.3 IBM-4 -LRB- Brown et al. 1993 -RRB- 29 86.9 IP 28 95.9 The last line shows an average f-score over the 8 tied IP solutions	nn_solutions_IP amod_solutions_tied num_solutions_8 det_solutions_the prep_over_f-score_solutions amod_f-score_average det_f-score_an dobj_shows_f-score nsubj_shows_line nsubj_shows_f-score amod_line_last det_line_The nn_line_IP num_line_86.9 num_line_29 dep_line_IBM-4 dep_line_IBM-1 nn_line_Monotone num_line_100.0 nn_line_Gold amod_line_tuples number_95.9_28 num_IP_95.9 dep_al._1993 nn_al._et amod_al._Brown dep_IBM-4_al. num_IBM-4_80.3 number_80.3_30 dep_al._1993 nn_al._et amod_al._Brown dep_IBM-1_al. num_IBM-1_68.9 number_68.9_39 number_100.0_28 nn_Gold_f-score nn_Gold_size nn_Gold_Dict nn_Gold_Method num_Gold_.4 nn_alignment_gold det_alignment_the prep_against_f-score_alignment amod_f-score_balanced det_f-score_a ccomp_calculate_shows aux_calculate_to det_tuples_these vmod_use_calculate dobj_use_tuples nsubj_use_We
W09-2501	J93-2003	n	One prominent constraint of the IBM word alignment models -LRB- Brown et al. 1993 -RRB- is functional alignment that is each target word is mapped onto at most one source word	nn_word_source num_word_one amod_word_most pobj_at_word pcomp_onto_at prep_mapped_onto auxpass_mapped_is nsubjpass_mapped_word nn_word_target det_word_each cop_word_is nsubj_word_that rcmod_alignment_mapped amod_alignment_functional cop_alignment_is nsubj_alignment_constraint amod_Brown_1993 dep_Brown_al. nn_Brown_et nn_models_alignment nn_models_word nn_models_IBM det_models_the dep_constraint_Brown prep_of_constraint_models amod_constraint_prominent num_constraint_One
W93-0301	J93-2003	n	4 Conclusions Compared with other word alignment algorithms -LRB- Brown et al. 1993 Gale and Church 1991a -RRB- word_align does not require sentence alignment as input and was shown to produce useful alignments for small and noisy corpora	amod_corpora_noisy amod_corpora_small conj_and_small_noisy prep_for_alignments_corpora amod_alignments_useful dobj_produce_alignments aux_produce_to xcomp_shown_produce auxpass_shown_was nsubjpass_shown_word_align prep_as_alignment_input nn_alignment_sentence conj_and_require_shown dobj_require_alignment neg_require_not aux_require_does nsubj_require_word_align ccomp_require_Conclusions appos_Gale_1991a conj_and_Gale_Church dep_Brown_Church dep_Brown_Gale amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_algorithms_Brown nn_algorithms_alignment nn_algorithms_word amod_algorithms_other prep_with_Compared_algorithms vmod_Conclusions_Compared num_Conclusions_4
W93-0301	J93-2003	n	The program takes the output of char_align -LRB- Church 1993 -RRB- a robust alternative to sentence-based alignment programs and applies word-level constraints using a version of Brown el al. 's Model 2 -LRB- Brown et al. 1993 -RRB- modified and extended to deal with robustness issues	nn_issues_robustness prep_with_deal_issues aux_deal_to xcomp_modified_deal conj_and_modified_extended amod_Brown_1993 dep_Brown_al. nn_Brown_et vmod_Model_extended vmod_Model_modified dep_Model_Brown num_Model_2 dep_al._Model possessive_al._'s nn_al._el prep_Brown_al. prep_of_version_Brown det_version_a dobj_using_version amod_constraints_word-level vmod_applies_using dobj_applies_constraints nsubj_applies_program nn_programs_alignment amod_programs_sentence-based prep_to_alternative_programs amod_alternative_robust det_alternative_a appos_Church_1993 dep_char_align_Church prep_of_output_char_align det_output_the conj_and_takes_applies conj_and_takes_alternative dobj_takes_output nsubj_takes_program det_program_The
W93-0301	J93-2003	n	The method was intended as a replacement for sentence-based methods -LRB- e.g. -LRB- Brown et al. 1991a Gale and Church 1991b Kay and Rosenschein 1993 -RRB- -RRB- which are very sensitive to noise	prep_to_sensitive_noise advmod_sensitive_very cop_sensitive_are nsubj_sensitive_which appos_Gale_1993 conj_and_Gale_Rosenschein conj_and_Gale_Kay conj_and_Gale_1991b conj_and_Gale_Church rcmod_Brown_sensitive dep_Brown_Rosenschein dep_Brown_Kay dep_Brown_1991b dep_Brown_Church dep_Brown_Gale appos_Brown_1991a dep_Brown_al. nn_Brown_et dep_e.g._Brown ccomp_-LRB-_e.g. amod_methods_sentence-based prep_for_replacement_methods det_replacement_a prep_as_intended_replacement auxpass_intended_was nsubjpass_intended_method det_method_The
W93-0301	J93-2003	o	2 The alignment Algorithm 2.1 Estimation of translation probabilities The translation probabilities are estimated using a method based on Brown et al. 's Model 2 -LRB- 1993 -RRB- which is summarized in the following subsection 2.1.1	appos_subsection_2.1.1 amod_subsection_following det_subsection_the prep_in_summarized_subsection auxpass_summarized_is nsubjpass_summarized_which appos_2_1993 rcmod_Model_summarized dep_Model_2 dep_al._Model possessive_al._'s nn_al._et nn_al._Brown pobj_method_al. prepc_based_on_method_on det_method_a dobj_using_method xcomp_estimated_using auxpass_estimated_are nsubjpass_estimated_probabilities nn_probabilities_translation det_probabilities_The nn_probabilities_translation rcmod_Estimation_estimated prep_of_Estimation_probabilities num_Estimation_2.1 nn_Estimation_Algorithm nn_Estimation_alignment det_Estimation_The num_Estimation_2 dep_``_Estimation
W93-0301	J93-2003	o	1 Introduction Aligning parallel texts has recently received considerable attention -LRB- Warwick et al. 1990 Brown et al. 1991a Gale and Church 1991b Gale and Church 1991a Kay and Rosenschein 1993 Simard et al. 1992 Church 1993 Kupiec 1993 Matsumoto et al. 1993 -RRB-	num_Matsumoto_1993 nn_Matsumoto_al. nn_Matsumoto_et num_Kupiec_1993 num_Church_1993 num_Simard_1992 nn_Simard_al. nn_Simard_et num_Gale_1993 conj_and_Gale_Rosenschein conj_and_Gale_Kay conj_and_Gale_1991a conj_and_Gale_Church conj_and_Gale_Gale conj_and_Gale_1991b conj_and_Gale_Church appos_Brown_1991a dep_Brown_al. nn_Brown_et dep_Warwick_Matsumoto dep_Warwick_Kupiec dep_Warwick_Church dep_Warwick_Simard dep_Warwick_Rosenschein dep_Warwick_Kay dep_Warwick_1991a dep_Warwick_Church dep_Warwick_Gale dep_Warwick_1991b dep_Warwick_Church dep_Warwick_Gale dep_Warwick_Brown dep_Warwick_1990 dep_Warwick_al. nn_Warwick_et amod_attention_considerable dep_received_Warwick dobj_received_attention advmod_received_recently aux_received_has nsubj_received_texts amod_texts_parallel amod_texts_Aligning dep_texts_Introduction num_Introduction_1
W93-0301	J93-2003	o	These methods have been used in machine translation -LRB- Brown et al. 1990 Sadler 1989 -RRB- terminology research and translation aids -LRB- Isabelle 1992 Ogden and Gonzales 1993 -RRB- bilingual lexicography -LRB- Klavans and Tzoukermann 1990 -RRB- collocation studies -LRB- Smadja 1992 -RRB- word-sense disambiguation -LRB- Brown et al. 1991b Gale et al. 1992 -RRB- and information retrieval in a multilingual environment -LRB- Landauer and Littman 1990 -RRB-	dep_Landauer_1990 conj_and_Landauer_Littman dep_environment_Littman dep_environment_Landauer amod_environment_multilingual det_environment_a nn_retrieval_information dep_al._1992 nn_al._et nn_al._Gale dep_al._al. appos_al._1991b nn_al._et amod_al._Brown dep_disambiguation_al. amod_disambiguation_word-sense dep_Smadja_1992 appos_studies_Smadja nn_studies_collocation dep_Klavans_1990 conj_and_Klavans_Tzoukermann appos_lexicography_Tzoukermann appos_lexicography_Klavans amod_lexicography_bilingual dep_Ogden_1993 conj_and_Ogden_Gonzales dep_Isabelle_Gonzales dep_Isabelle_Ogden appos_Isabelle_1992 nn_aids_translation prep_in_research_environment conj_and_research_retrieval conj_and_research_disambiguation conj_and_research_studies appos_research_lexicography appos_research_Isabelle conj_and_research_aids nn_research_terminology dep_Sadler_1989 dep_al._Sadler num_al._1990 nn_al._et amod_al._Brown nn_translation_machine dep_used_retrieval dep_used_disambiguation dep_used_studies dep_used_aids dep_used_research dep_used_al. prep_in_used_translation auxpass_used_been aux_used_have nsubjpass_used_methods det_methods_These
W94-0115	J93-2003	p	Brute-force methods -LRB- ie those that exploit the massive raw computing power currently available cheaply -RRB- may well produce some useful results -LRB- eg Brown et al 1993 -RRB-	dep_al_1993 nn_al_et advmod_Brown_al amod_eg_Brown dep_results_eg amod_results_useful det_results_some dobj_produce_results advmod_produce_well aux_produce_may nsubj_produce_methods advmod_available_cheaply advmod_available_currently amod_power_available nn_power_computing amod_power_raw amod_power_massive det_power_the dobj_exploit_power nsubj_exploit_that rcmod_those_exploit advmod_those_ie dep_methods_those amod_methods_Brute-force
W95-0106	J93-2003	o	Numerous experiments have shown parallel bilingual corpora to provide a rich source of constraints for statistical analysis -LRB- e.g. Brown et al. 1990 Gale & Church 1991 Gale et al. 1992 Church 1993 Brown et al. 1993 Dagan et al. 1993 Fung & Church 1994 Wu & Xia 1994 Fung & McKeown 1994 -RRB-	num_Fung_1994 conj_and_Fung_McKeown dep_Wu_McKeown dep_Wu_Fung num_Wu_1994 conj_and_Wu_Xia nn_1994_Church nn_1994_Fung conj_and_Fung_Church num_al._1993 nn_al._et nn_al._Dagan num_al._1993 nn_al._et nn_al._Brown num_Church_1993 dep_al._1992 nn_al._et advmod_Gale_al. num_Gale_1991 conj_and_Gale_Church dep_al._Xia dep_al._Wu dep_al._1994 dep_al._al. dep_al._al. dep_al._Church dep_al._Gale dep_al._Church dep_al._Gale num_al._1990 nn_al._et nn_al._Brown dep_al._e.g. dep_analysis_al. amod_analysis_statistical prep_for_constraints_analysis prep_of_source_constraints amod_source_rich det_source_a dobj_provide_source aux_provide_to amod_corpora_bilingual amod_corpora_parallel vmod_shown_provide dobj_shown_corpora aux_shown_have nsubj_shown_experiments amod_experiments_Numerous
W95-0106	J93-2003	o	1 Introduction A number of empirical studies have found bracketing to be a useful type of corpus annotation -LRB- e.g. Pereira & Schabes 1992 Black et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Black dep_Pereira_al. num_Pereira_1992 conj_and_Pereira_Schabes dep_e.g._Schabes dep_e.g._Pereira dep_-LRB-_e.g. nn_annotation_corpus prep_of_type_annotation amod_type_useful det_type_a cop_type_be aux_type_to xcomp_bracketing_type xcomp_found_bracketing aux_found_have nsubj_found_number amod_studies_empirical prep_of_number_studies det_number_A nn_number_Introduction num_number_1
W95-0106	J93-2003	o	It is interesting to constrast this method with the parse-parse-match approaches that have been reported recently for producing parallel bracketed corpora -LRB- Sadler & Vendelmans 1990 Kaji et al. 1992 Matsumoto et al. 1993 Cranias et al. 1994 Gfishman 1994 -RRB-	num_Gfishman_1994 num_al._1994 nn_al._et nn_al._Cranias num_al._1993 nn_al._et nn_al._Matsumoto dep_al._1992 nn_al._et nn_al._Kaji dep_Sadler_Gfishman dep_Sadler_al. dep_Sadler_al. dep_Sadler_al. num_Sadler_1990 conj_and_Sadler_Vendelmans appos_corpora_Vendelmans appos_corpora_Sadler amod_corpora_bracketed amod_corpora_parallel dobj_producing_corpora prepc_for_reported_producing advmod_reported_recently auxpass_reported_been aux_reported_have nsubjpass_reported_that ccomp_approaches_reported ccomp_approaches_interesting det_parse-parse-match_the det_method_this prep_with_constrast_parse-parse-match dobj_constrast_method aux_constrast_to xcomp_interesting_constrast cop_interesting_is nsubj_interesting_It
W97-0119	J93-2003	n	1 Introduction Despite a surge in research using parallel corpora for various machine translation tasks -LRB- Brown et al. 1993 -RRB- -LRB- Brown et al. 1991 Gale & Church 1993 Church 1993 Dagan & Church 1994 Simard et al. 1992 Chen 1993 Melamed 1995 Wu & Xia 1994 Wu 1994 Smadja et aI	nn_aI_et nn_aI_Smadja num_Wu_1994 num_Wu_1994 conj_and_Wu_Xia num_Melamed_1995 num_Chen_1993 num_al._1992 nn_al._et nn_al._Simard num_Dagan_1994 conj_and_Dagan_Church num_Church_1993 num_Gale_1993 conj_and_Gale_Church dep_Brown_1991 dep_Brown_al. nn_Brown_et dep_al._1993 nn_al._et advmod_Brown_al. nn_tasks_translation nn_tasks_machine amod_tasks_various amod_corpora_parallel prep_for_using_tasks dobj_using_corpora vmod_surge_using prep_in_surge_research det_surge_a conj_Introduction_aI conj_Introduction_Wu conj_Introduction_Xia conj_Introduction_Wu conj_Introduction_Melamed conj_Introduction_Chen dep_Introduction_al. dep_Introduction_Church dep_Introduction_Dagan dep_Introduction_Church dep_Introduction_Church dep_Introduction_Gale appos_Introduction_Brown appos_Introduction_Brown prep_despite_Introduction_surge num_Introduction_1
W97-0311	J93-2003	o	-LRB- Brown et aL 1993 -RRB- The heuristics in Section 6 are designed specifically to find the interesting features in that featureless desert	amod_desert_featureless det_desert_that prep_in_features_desert amod_features_interesting det_features_the dobj_find_features aux_find_to advmod_find_specifically xcomp_designed_find auxpass_designed_are nsubjpass_designed_heuristics dep_designed_Brown num_Section_6 prep_in_heuristics_Section det_heuristics_The dep_aL_1993 nn_aL_et dep_Brown_aL
W97-0311	J93-2003	o	Several authors have used mutual information and similar statistics as an objective function for word clustering -LRB- Dagan et al. 1993 Brown et al. 1992 Pereira et al. 1993 Wang et al. 1996 -RRB- for automatic determination of phonemic baseforms -LRB- Lucassen & Mercer 1984 -RRB- and for language modeling for speech recognition -LRB- Ries ct al. 1996 -RRB-	dep_ct_1996 dep_ct_al. nn_ct_Ries nn_recognition_speech dep_modeling_ct prep_for_modeling_recognition nn_modeling_language pobj_for_modeling dep_Lucassen_1984 conj_and_Lucassen_Mercer appos_baseforms_Mercer appos_baseforms_Lucassen amod_baseforms_phonemic prep_of_determination_baseforms amod_determination_automatic num_Wang_1996 nn_Wang_al. nn_Wang_et num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Dagan_Wang dep_Dagan_Pereira dep_Dagan_Brown dep_Dagan_1993 dep_Dagan_al. nn_Dagan_et nn_clustering_word prep_for_function_clustering amod_function_objective det_function_an amod_statistics_similar conj_and_information_statistics amod_information_mutual conj_and_used_for prep_for_used_determination dep_used_Dagan prep_as_used_function dobj_used_statistics dobj_used_information aux_used_have nsubj_used_authors amod_authors_Several ccomp_``_for ccomp_``_used
W97-0311	J93-2003	o	2 Translation Models A translation model can be constructed automatically from texts that exist in two languages -LRB- bitexts -RRB- -LRB- Brown et al. 1993 Melamed 1997 -RRB-	amod_Melamed_1997 dep_Brown_Melamed appos_Brown_1993 dep_Brown_al. nn_Brown_et appos_languages_Brown appos_languages_bitexts num_languages_two prep_in_exist_languages nsubj_exist_that rcmod_texts_exist prep_from_constructed_texts advmod_constructed_automatically auxpass_constructed_be aux_constructed_can nsubjpass_constructed_model nn_model_translation det_model_A rcmod_Models_constructed nn_Models_Translation num_Models_2 dep_``_Models
W97-0405	J93-2003	o	Pure statistical machine translation -LRB- Brown et al. 1993 -RRB- mltst in principle recover the most probable alignment out of all possible alignments between the input and a translation	det_translation_a conj_and_input_translation det_input_the amod_alignments_possible det_alignments_all amod_alignment_probable det_alignment_the advmod_probable_most prep_between_recover_translation prep_between_recover_input pobj_recover_alignments prepc_out_of_recover_of dobj_recover_alignment nsubj_recover_translation num_al._1993 nn_al._et amod_al._Brown prep_in_translation_principle dep_translation_mltst dep_translation_al. nn_translation_machine amod_translation_statistical amod_translation_Pure
W97-0408	J93-2003	o	3.3 Model Construction The head transducer model was trained and evaluated on English-to-Mandarin Chinese translation of transcribed utterances from the ATIS corpus -LRB- Hirschman et al. 1993 -RRB-	dep_1993_al. nn_al._et num_Hirschman_1993 nn_corpus_ATIS det_corpus_the prep_from_utterances_corpus amod_utterances_transcribed prep_of_translation_utterances nn_translation_Chinese nn_translation_English-to-Mandarin prep_on_evaluated_translation nsubjpass_evaluated_model dep_trained_Hirschman conj_and_trained_evaluated auxpass_trained_was nsubjpass_trained_model nn_model_transducer nn_model_head nn_model_The nn_model_Construction nn_model_Model num_model_3.3
W97-1014	J93-2003	o	In several papers -LRB- Bahl et al. 1984 Lau and Rosenfeld 1993 Tillmann and Ney 1996 -RRB- selection criteria for single word trigger pairs were studied	auxpass_studied_were nsubjpass_studied_criteria dep_studied_Ney dep_studied_Tillmann dep_studied_1993 dep_studied_Rosenfeld dep_studied_Lau dep_studied_Bahl prep_in_studied_papers nn_pairs_trigger nn_pairs_word amod_pairs_single prep_for_criteria_pairs nn_criteria_selection nn_al._et dep_Bahl_1996 conj_and_Bahl_Ney conj_and_Bahl_Tillmann conj_and_Bahl_1993 conj_and_Bahl_Rosenfeld conj_and_Bahl_Lau amod_Bahl_1984 dep_Bahl_al. amod_papers_several
W97-1014	J93-2003	o	1 Introduction In this paper we study the use of so-called word trigger pairs -LRB- for short word triggers -RRB- -LRB- Bahl et al. 1984 Lau and Rosenfeld 1993 Tillmann and Ney 1996 -RRB- to improve an existing language model which is typically a trigram model in combination with a cache component -LRB- Ney and Essen 1994 -RRB-	dep_Ney_1994 conj_and_Ney_Essen dep_component_Essen dep_component_Ney nn_component_cache det_component_a prep_with_combination_component prep_in_model_combination nn_model_trigram det_model_a advmod_model_typically cop_model_is nsubj_model_which rcmod_model_model nn_model_language amod_model_existing det_model_an dobj_improve_model aux_improve_to vmod_Bahl_improve dep_Bahl_1996 conj_and_Bahl_Ney conj_and_Bahl_Tillmann conj_and_Bahl_1993 conj_and_Bahl_Rosenfeld conj_and_Bahl_Lau amod_Bahl_1984 dep_Bahl_al. nn_Bahl_et nsubj_triggers_word prep_for_pairs_short nn_pairs_trigger nn_pairs_word amod_pairs_so-called prep_of_use_pairs det_use_the dep_study_Ney dep_study_Tillmann dep_study_1993 dep_study_Rosenfeld dep_study_Lau dep_study_Bahl parataxis_study_triggers dobj_study_use nsubj_study_we nsubj_study_Introduction det_paper_this prep_in_Introduction_paper num_Introduction_1
W99-0602	J93-2003	o	Bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications such as computer assisted translation -LRB- Isabelle et al. 1993 Brown et al. 1990 -RRB- terminology -LRB- Dagan and Church 1994 -RRB- lexicography -LRB- Langlois 1996 Klavans and Tzoukermann 1995 Melamed 1996 -RRB- and cross-language information retrieval -LRB- Nie et al. * This research was funded by the Canadian Department of Foreign Affairs and International Trade -LRB- http://~.dfait-maeci.gc.ca/ -RRB- via the Agence de la francophonie -LRB- http / / ~	dep_http_~ dep_francophonie_http nn_francophonie_la nn_francophonie_de nn_francophonie_Agence det_francophonie_the appos_Trade_http://~.dfait-maeci.gc.ca/ nn_Trade_International conj_and_Affairs_Trade nn_Affairs_Foreign prep_of_Department_Trade prep_of_Department_Affairs nn_Department_Canadian det_Department_the agent_funded_Department auxpass_funded_was nsubjpass_funded_research det_research_This dep_research_* nn_al._et prep_via_Nie_francophonie rcmod_Nie_funded dep_Nie_al. nn_retrieval_information amod_retrieval_cross-language dep_Melamed_1996 dep_Klavans_Nie conj_and_Klavans_retrieval conj_and_Klavans_Melamed conj_and_Klavans_1995 conj_and_Klavans_Tzoukermann dep_Langlois_retrieval dep_Langlois_Melamed dep_Langlois_1995 dep_Langlois_Tzoukermann dep_Langlois_Klavans appos_Langlois_1996 dep_lexicography_Langlois dep_lexicography_Church dep_lexicography_Dagan nn_lexicography_terminology dep_Dagan_1994 conj_and_Dagan_Church num_Brown_1990 nn_Brown_al. nn_Brown_et dep_Isabelle_Brown appos_Isabelle_1993 dep_Isabelle_al. nn_Isabelle_et appos_translation_lexicography appos_translation_Isabelle amod_translation_assisted nn_translation_computer prep_such_as_applications_translation amod_applications_linguistic prep_of_range_applications amod_range_wide det_range_a amod_roles_multiple prep_in_play_range dobj_play_roles aux_play_can nsubj_play_they mark_play_that ccomp_shown_play advmod_shown_far aux_shown_have nsubj_shown_alignments advmod_far_so amod_alignments_Bilingual
W99-0602	J93-2003	o	However in the experiments described here we focus on alignment at the level of sentences this for a number of reasons First sentence alignments have so far proven their usefulness in a number of applications e.g. bilingual lexicography -LRB- Langlois 1996 Klavans and Tzoukermann 1995 Dagan and Church 1994 -RRB- automatic translation verification -LRB- Macklovitch 1995 Macklovitch 1996 -RRB- and the automatic acquisition of knowledge about translation -LRB- Brown et al. 1993 -RRB-	dep_al._1993 nn_al._et amod_al._Brown dep_translation_al. quantmod_translation_about amod_acquisition_translation prep_of_acquisition_knowledge amod_acquisition_automatic det_acquisition_the dep_Macklovitch_1996 dep_Macklovitch_Macklovitch appos_Macklovitch_1995 appos_verification_Macklovitch nn_verification_translation amod_verification_automatic dep_Dagan_1994 conj_and_Dagan_Church conj_and_Klavans_acquisition conj_and_Klavans_verification dep_Klavans_Church dep_Klavans_Dagan conj_and_Klavans_1995 conj_and_Klavans_Tzoukermann dep_Langlois_acquisition dep_Langlois_verification dep_Langlois_1995 dep_Langlois_Tzoukermann dep_Langlois_Klavans appos_Langlois_1996 dep_lexicography_Langlois amod_lexicography_bilingual dep_e.g._lexicography prep_number_e.g. prep_of_number_applications det_number_a prep_in_usefulness_number poss_usefulness_their dobj_proven_usefulness advmod_proven_far aux_proven_have nsubj_proven_alignments advmod_proven_First advmod_far_so nn_alignments_sentence prep_of_number_reasons det_number_a dep_this_proven prep_for_this_number prep_of_level_sentences det_level_the prep_at_alignment_level dep_focus_this prep_on_focus_alignment nsubj_focus_we prep_in_focus_experiments advmod_focus_However advmod_described_here vmod_experiments_described det_experiments_the
W99-0604	J93-2003	o	Many statistical translation models -LRB- Vogel et al. 1996 Tillmann et al. 1997 Niessen et al. 1998 Brown et al. 1993 -RRB- try to model word-toword correspondences between source and target words	nn_words_target conj_and_source_words prep_between_correspondences_words prep_between_correspondences_source amod_correspondences_word-toword dobj_model_correspondences aux_model_to xcomp_try_model nsubj_try_models num_Brown_1993 nn_Brown_al. nn_Brown_et nn_al._et nn_al._Niessen num_Tillmann_1997 nn_Tillmann_al. nn_Tillmann_et dep_Vogel_Brown amod_Vogel_1998 dep_Vogel_al. dep_Vogel_Tillmann amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et appos_models_Vogel nn_models_translation amod_models_statistical amod_models_Many
W99-0604	J93-2003	o	This alignment representation is a generalization of the baseline alignments described in -LRB- Brown et al. 1993 -RRB- and allows for many-to-many alignments	amod_alignments_many-to-many prep_for_allows_alignments nsubj_allows_representation num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_described_in vmod_alignments_described nn_alignments_baseline det_alignments_the conj_and_generalization_allows prep_of_generalization_alignments det_generalization_a cop_generalization_is nsubj_generalization_representation nn_representation_alignment det_representation_This
A00-1025	J93-2004	o	The approach is able to achieve 94 % precision and recall for base NPs derived from the Penn Treebank Wall Street Journal -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Journal_Street nn_Journal_Wall nn_Journal_Treebank nn_Journal_Penn det_Journal_the prep_from_derived_Journal vmod_NPs_derived nn_NPs_base prep_for_precision_NPs conj_and_precision_recall amod_precision_% number_%_94 dobj_achieve_recall dobj_achieve_precision aux_achieve_to dep_able_Marcus xcomp_able_achieve cop_able_is nsubj_able_approach det_approach_The
A00-1031	J93-2004	o	Recent comparisons of approaches that can be trained on corpora -LRB- van Halteren et al. 1998 Volk and Schneider 1998 -RRB- have shown that in most cases statistical aproaches -LRB- Cutting et al. 1992 Schmid 1995 Ratnaparkhi 1996 -RRB- yield better results than finite-state rule-based or memory-based taggers -LRB- Brill 1993 Daelemans et al. 1996 -RRB-	num_Daelemans_1996 nn_Daelemans_al. nn_Daelemans_et dep_Brill_Daelemans dep_Brill_1993 appos_taggers_Brill amod_taggers_memory-based amod_taggers_rule-based amod_taggers_finite-state conj_or_finite-state_memory-based conj_or_finite-state_rule-based prep_than_results_taggers amod_results_better dobj_yield_results num_Ratnaparkhi_1996 dep_Schmid_yield dep_Schmid_Ratnaparkhi num_Schmid_1995 conj_al._1992 nn_al._et dep_Cutting_Schmid dobj_Cutting_al. amod_aproaches_statistical nn_aproaches_cases amod_aproaches_most dep_that_Cutting prep_in_that_aproaches dep_shown_that aux_shown_have nsubj_shown_comparisons amod_Volk_1998 conj_and_Volk_Schneider dep_Halteren_Schneider dep_Halteren_Volk amod_Halteren_1998 dep_Halteren_al. nn_Halteren_et nn_Halteren_van appos_corpora_Halteren prep_on_trained_corpora auxpass_trained_be aux_trained_can nsubjpass_trained_that rcmod_approaches_trained prep_of_comparisons_approaches amod_comparisons_Recent
A00-1031	J93-2004	o	The annotation consists of four parts 1 -RRB- a context-free structure augmented with traces to mark movement and discontinuous constituents 2 -RRB- phrasal categories that are annotated as node labels 3 -RRB- a small set of grammatical functions that are annotated as extensions to the node labels and 4 -RRB- part-of-speech tags -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_tags_Marcus nn_tags_part-of-speech dep_tags_4 nn_labels_node det_labels_the prep_to_extensions_labels prep_as_annotated_extensions cop_annotated_are nsubj_annotated_that rcmod_functions_annotated amod_functions_grammatical prep_of_set_functions amod_set_small det_set_a dep_set_3 nn_labels_node prep_as_annotated_labels cop_annotated_are nsubj_annotated_that rcmod_categories_annotated amod_categories_phrasal dep_categories_2 nn_constituents_discontinuous nn_constituents_movement conj_and_movement_discontinuous dobj_mark_constituents aux_mark_to vmod_traces_mark conj_and_augmented_tags conj_and_augmented_set dobj_augmented_categories prep_with_augmented_traces nsubj_augmented_structure dep_augmented_1 amod_structure_context-free det_structure_a num_parts_four parataxis_consists_tags parataxis_consists_set parataxis_consists_augmented prep_of_consists_parts nsubj_consists_annotation det_annotation_The
A00-1031	J93-2004	o	As two examples -LRB- Rabiner 1989 -RRB- and -LRB- Charniak et al. 1993 -RRB- give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging but they are not very explicit in the details that are needed for their application	poss_application_their prep_for_needed_application auxpass_needed_are nsubjpass_needed_that rcmod_details_needed det_details_the prep_in_explicit_details advmod_explicit_very neg_explicit_not cop_explicit_are nsubj_explicit_they amod_tagging_part-ofspeech conj_and_models_tagging nn_models_Markov prep_for_used_tagging prep_for_used_models vmod_techniques_used conj_and_techniques_equations det_techniques_the prep_of_overviews_equations prep_of_overviews_techniques amod_overviews_good conj_but_give_explicit dobj_give_overviews nsubj_give_Charniak nsubj_give_examples mark_give_As amod_Charniak_1993 dep_Charniak_al. nn_Charniak_et appos_Rabiner_1989 conj_and_examples_Charniak appos_examples_Rabiner num_examples_two advcl_``_explicit advcl_``_give
A00-1031	J93-2004	o	Additionally we present results of the tagger on the NEGRA corpus -LRB- Brants et al. 1999 -RRB- and the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the amod_Brants_1999 dep_Brants_al. nn_Brants_et nn_corpus_NEGRA det_corpus_the prep_on_tagger_corpus det_tagger_the conj_and_results_Treebank dep_results_Brants prep_of_results_tagger dobj_present_Treebank dobj_present_results nsubj_present_we advmod_present_Additionally
A00-2005	J93-2004	o	2.3 Experiment The training set for these experiments was sections 01-21 of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_01-21_Marcus prep_of_01-21_Treebank dep_sections_01-21 cop_sections_was nsubj_sections_Experiment det_experiments_these prep_for_set_experiments vmod_training_set det_training_The dep_Experiment_training num_Experiment_2.3
A00-2007	J93-2004	o	The main data set consist of four sections -LRB- 15-18 -RRB- of the Wall Street Journal -LRB- WSJ -RRB- part of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- as training material and one section -LRB- 20 -RRB- as test material 1	num_material_1 nn_material_test appos_section_20 num_section_one conj_and_material_section nn_material_training amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_part_Treebank nn_part_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the prep_of_sections_part appos_sections_15-18 num_sections_four prep_as_consist_material prep_as_consist_section prep_as_consist_material dep_consist_Marcus prep_of_consist_sections nsubj_consist_set nn_set_data amod_set_main det_set_The
A00-2020	J93-2004	o	We evaluate this method over the part of speech tagged portion of the Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_of_portion_corpus dobj_tagged_portion prep_over_tagged_part nsubj_tagged_method prep_of_part_speech det_part_the det_method_this dep_evaluate_Marcus ccomp_evaluate_tagged nsubj_evaluate_We ccomp_``_evaluate
A00-2023	J93-2004	o	so they conform to the Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB- annotation style and then do experiments using models built with Treebank data	nn_data_Treebank prep_with_built_data vmod_models_built dobj_using_models xcomp_do_using dobj_do_experiments advmod_do_then nsubj_do_they nn_style_annotation appos_style_Marcus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn det_corpus_the conj_and_conform_do dobj_conform_style prep_to_conform_corpus nsubj_conform_they mark_conform_so advcl_``_do advcl_``_conform
A00-2030	J93-2004	o	However because these estimates are too sparse to be relied upon we use interpolated estimates consisting of mixtures of successively lowerorder estimates -LRB- as in Placeway et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Placeway pobj_in_al. pcomp_as_in prep_estimates_as nn_estimates_lowerorder advmod_estimates_successively prep_of_mixtures_estimates prep_of_consisting_mixtures vmod_estimates_consisting amod_estimates_interpolated dobj_use_estimates nsubj_use_we advcl_use_sparse advmod_use_However prep_relied_upon auxpass_relied_be aux_relied_to xcomp_sparse_relied advmod_sparse_too cop_sparse_are nsubj_sparse_estimates mark_sparse_because det_estimates_these
A00-2030	J93-2004	o	We were already using a generative statistical model for part-of-speech tagging -LRB- Weischedel et al. 1993 -RRB- and more recently had begun using a generative statistical model for name finding -LRB- Bikel et al. 1997 -RRB-	advmod_1997_al. nn_al._et num_Bikel_1997 nn_finding_name prep_for_model_finding amod_model_statistical amod_model_generative det_model_a dobj_using_model xcomp_begun_using aux_begun_had advmod_begun_recently nsubj_begun_We advmod_recently_more dep_1993_al. nn_al._et num_Weischedel_1993 amod_tagging_part-of-speech prep_for_model_tagging amod_model_statistical amod_model_generative det_model_a dep_using_Bikel conj_and_using_begun dep_using_Weischedel dobj_using_model advmod_using_already aux_using_were nsubj_using_We ccomp_``_begun ccomp_``_using
A00-2030	J93-2004	o	Word features are introduced primarily to help with unknown words as in -LRB- Weischedel et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_Weischedel_1993 dep_in_Weischedel pcomp_as_in amod_words_unknown prep_with_help_words aux_help_to prep_introduced_as xcomp_introduced_help advmod_introduced_primarily auxpass_introduced_are nsubjpass_introduced_features nn_features_Word
A00-2033	J93-2004	o	The PT grammar 2 was extracted from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_extracted_Marcus prep_from_extracted_Treebank auxpass_extracted_was nsubjpass_extracted_grammar num_grammar_2 nn_grammar_PT det_grammar_The
A94-1009	J93-2004	p	Preparing tagged corpora either by hand is labour-intensive and potentially error-prone and although a semi-automatic approach can be used -LRB- Marcus et al. 1993 -RRB- it is a good thing to reduce the human involvement as much as possible	prep_as_much_possible advmod_much_as amod_involvement_human det_involvement_the advmod_reduce_much dobj_reduce_involvement aux_reduce_to vmod_thing_reduce amod_thing_good det_thing_a cop_thing_is nsubj_thing_it advcl_thing_used amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_used_Marcus auxpass_used_be aux_used_can nsubjpass_used_approach mark_used_although amod_approach_semi-automatic det_approach_a advmod_error-prone_potentially nsubj_error-prone_corpora conj_and_labour-intensive_thing conj_and_labour-intensive_error-prone cop_labour-intensive_is preconj_labour-intensive_either nsubj_labour-intensive_corpora prep_by_either_hand amod_corpora_tagged ccomp_Preparing_thing ccomp_Preparing_error-prone ccomp_Preparing_labour-intensive ccomp_``_Preparing
A97-1017	J93-2004	o	2.2.2 ENGLISH TRAINING DATA For training in the English experiments we used WSJ -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_WSJ_Marcus dobj_used_WSJ nsubj_used_we nsubj_used_DATA amod_experiments_English det_experiments_the prep_in_training_experiments prep_for_DATA_training nn_DATA_TRAINING amod_DATA_ENGLISH num_DATA_2.2.2
C00-1009	J93-2004	o	The part of the 1Release 2 of this data set can be obtained t ` rmn the Linguistic Data Consortium with Catalogue number LDC94T4B -LRB- http://www.ldc.upenn.edu/ldc/nofranm.html -RRB- 2There are 48 labels defined in -LRB- Marcus et al. 1993 -RRB- however three of ttmm do not appear in the corpus	det_corpus_the prep_in_appear_corpus neg_appear_not aux_appear_do nsubj_appear_three prep_of_three_ttmm rcmod_Marcus_appear advmod_Marcus_however amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et prep_in_defined_Marcus vmod_labels_defined num_labels_48 nsubj_are_labels nn_2There_LDC94T4B appos_LDC94T4B_http://www.ldc.upenn.edu/ldc/nofranm.html nn_LDC94T4B_number nn_LDC94T4B_Catalogue nn_Consortium_Data nn_Consortium_Linguistic det_Consortium_the prep_with_rmn_2There dobj_rmn_Consortium vmod_t_rmn dep_obtained_are dobj_obtained_t auxpass_obtained_be aux_obtained_can nsubjpass_obtained_part nn_set_data det_set_this prep_of_1Release_set num_1Release_2 det_1Release_the prep_of_part_1Release det_part_The ccomp_``_obtained
C00-1011	J93-2004	o	We compared this nonprobabilistic DOP model against tile probabilistic DOP model -LRB- which estimales the most probable parse for each sentence -RRB- on three different domains tbe Penn ATIS treebank -LRB- Marcus et al. 1993 -RRB- the Dutch OVIS treebank -LRB- Bonnema el al. 1997 -RRB- and tile Penn Wall Street Journal -LRB- WSJ -RRB- treebank -LRB- Marcus el al. 1993 -RRB-	advmod_1993_al. nn_al._el num_Marcus_1993 nn_treebank_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall nn_Journal_Penn nn_Journal_tile nn_al._el dep_Bonnema_1997 advmod_Bonnema_al. conj_and_treebank_treebank dep_treebank_Bonnema nn_treebank_OVIS amod_treebank_Dutch det_treebank_the dep_al._1993 nn_al._et advmod_Marcus_al. appos_treebank_Marcus appos_treebank_treebank appos_treebank_treebank appos_treebank_Marcus nn_treebank_ATIS nn_treebank_Penn nn_treebank_tbe amod_domains_different num_domains_three det_sentence_each amod_parse_probable det_parse_the advmod_probable_most prep_for_estimales_sentence dobj_estimales_parse nsubj_estimales_which rcmod_model_estimales nn_model_DOP amod_model_probabilistic nn_model_tile prep_on_model_domains prep_against_model_model nn_model_DOP amod_model_nonprobabilistic det_model_this dep_compared_treebank dobj_compared_model nsubj_compared_We ccomp_``_compared
C00-1011	J93-2004	o	While this technique has been sttccessfully applied to parsing lhe ATIS portion in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- it is extremely time consuming	dep_time_consuming advmod_time_extremely cop_time_is nsubj_time_it advcl_time_applied dep_1993_al. nn_al._et num_Marcus_1993 nn_Treebank_Penn det_Treebank_the prep_in_portion_Treebank nn_portion_ATIS nn_portion_lhe amod_portion_parsing dep_applied_Marcus prep_to_applied_portion advmod_applied_sttccessfully auxpass_applied_been aux_applied_has nsubjpass_applied_technique mark_applied_While det_technique_this
C00-1011	J93-2004	o	Experimental Comparison 4.1 Experiments on the ATIS corpus For our first comparison we used I0 splits from the Penn ATIS corpus -LRB- Marcus et al. 1993 -RRB- into training sets of 675 sentences and test sets of 75 sentences	num_sentences_75 nn_sets_test prep_of_sentences_sentences conj_and_sentences_sets num_sentences_675 prep_of_sets_sets prep_of_sets_sentences nn_sets_training dep_1993_al. nn_al._et num_Marcus_1993 nn_corpus_ATIS nn_corpus_Penn det_corpus_the prep_into_splits_sets dep_splits_Marcus prep_from_splits_corpus nsubj_splits_I0 dep_used_splits nsubj_used_we ccomp_used_Comparison amod_comparison_first poss_comparison_our nn_corpus_ATIS det_corpus_the prep_for_Experiments_comparison prep_on_Experiments_corpus num_Experiments_4.1 dep_Comparison_Experiments amod_Comparison_Experimental
C00-1034	J93-2004	o	-LRB- levelopment of cor1 -RRB- ora with morl -RRB- ho-synta -LRB- ti -LRB- and syntacti -LRB- mmotation -LRB- Marcus et al. 1993 -RRB- -LRB- Sampson 1995 -RRB-	amod_Sampson_1995 amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_mmotation_Sampson appos_mmotation_Marcus nn_mmotation_syntacti conj_and_ti_mmotation dep_ho-synta_mmotation dep_ho-synta_ti nn_ho-synta_morl prep_with_ora_ho-synta appos_ora_levelopment prep_of_levelopment_cor1
C00-1034	J93-2004	o	2 ' \ -RSB- ` he WSJ corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_corpus_Marcus nn_corpus_WSJ dep_he_corpus dep_`_he num_\_2
C00-1041	J93-2004	o	5.1 The Prague Dependency Tree Bank -LRB- PDT in the sequel -RRB- which has been inspired by the build-up of the Penn Treebank -LRB- Marcus Santorini & Marcinkiewicz 1993 Marcus Kim Marcinkiewicz et al. 1994 -RRB- is aimed at a complex annotation of -LRB- a part of -RRB- the Czech National Corpus -LRB- CNC in the sequel -RRB- the creation of which is under progress at the Department of Czech National Corpus at the Faculty of Philosophy Charles University -LRB- the corpus currently comprises about 100 million tokens of word forms -RRB-	nn_forms_word prep_of_tokens_forms num_tokens_million number_million_100 quantmod_million_about dobj_comprises_tokens advmod_comprises_currently nsubj_comprises_corpus tmod_comprises_University det_corpus_the nn_University_Charles prep_of_Faculty_Philosophy det_Faculty_the nn_Corpus_National nn_Corpus_Czech prep_of_Department_Corpus det_Department_the prep_at_is_Faculty prep_at_is_Department prep_under_is_progress nsubj_is_creation prep_of_creation_which det_creation_the det_sequel_the prep_in_CNC_sequel rcmod_Corpus_comprises rcmod_Corpus_is appos_Corpus_CNC nn_Corpus_National nn_Corpus_Czech det_Corpus_the dep_part_Corpus prep_part_of det_part_a prep_of_annotation_part amod_annotation_complex det_annotation_a prep_at_aimed_annotation auxpass_aimed_is nsubjpass_aimed_Bank dep_al._1994 nn_al._et nn_al._Marcinkiewicz dep_Marcus_al. conj_Marcus_Kim dep_Marcus_Marcus num_Marcus_1993 conj_and_Marcus_Marcinkiewicz conj_and_Marcus_Santorini appos_Treebank_Marcinkiewicz appos_Treebank_Santorini appos_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_of_build-up_Treebank det_build-up_the agent_inspired_build-up auxpass_inspired_been aux_inspired_has nsubjpass_inspired_which det_sequel_the prep_in_PDT_sequel rcmod_Bank_inspired appos_Bank_PDT nn_Bank_Tree nn_Bank_Dependency nn_Bank_Prague det_Bank_The num_Bank_5.1 ccomp_``_aimed
C00-1044	J93-2004	o	302 -LRB- Marcus et al. 1993 -RRB- was nlanually annotated with subjeciivity chlssifications	nn_chlssifications_subjeciivity prep_with_annotated_chlssifications advmod_annotated_nlanually cop_annotated_was nsubj_annotated_302 amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_302_Marcus
C00-2157	J93-2004	o	-LRB- Carpenter 1992 -RRB- -LRB- Copestake 1999 -RRB- -LRB- DSrre and Dorna 1993 -RRB- -LRB- D6I 're et al. 1996 -RRB- -LRB- Emele and Zajac 1990 -RRB- -LRB- H6ht ~ ld and Smolka 1988 -RRB- -RRB- and to pick those ingredients which are known to be con ~ i -RRB- utationally ` tractable ' in some sense	det_sense_some prep_in_tractable_sense dep_utationally_tractable dep_utationally_i advmod_~_utationally amod_~_con nsubj_be_~ prepc_to_known_be auxpass_known_are nsubjpass_known_which rcmod_ingredients_known det_ingredients_those dobj_pick_ingredients aux_pick_to dep_ld_1988 conj_and_ld_Smolka num_ld_~ nn_ld_H6ht dep_Emele_1990 conj_and_Emele_Zajac dep_al._1996 nn_al._et cop_al._'re nsubj_al._D6I dep_DSrre_1993 conj_and_DSrre_Dorna dep_Copestake_1999 conj_and_Carpenter_pick appos_Carpenter_Smolka appos_Carpenter_ld appos_Carpenter_Zajac appos_Carpenter_Emele dep_Carpenter_al. appos_Carpenter_Dorna appos_Carpenter_DSrre appos_Carpenter_Copestake amod_Carpenter_1992 dep_''_pick dep_''_Carpenter
C00-2157	J93-2004	p	1 Introduction Syntactically annotated corpora like the Penn Treebank -LRB- Marcus et al. 1993 -RRB- the NeGra corpus -LRB- Skut et al. 1998 -RRB- or the statistically dismnbiguated parses in -LRB- Bell et al. 1999 -RRB- provide a wealth of intbrmation which can only be exploited with an adequate query language	nn_language_query amod_language_adequate det_language_an prep_with_exploited_language auxpass_exploited_be advmod_exploited_only aux_exploited_can nsubjpass_exploited_which rcmod_wealth_exploited prep_of_wealth_intbrmation det_wealth_a dobj_provide_wealth nsubj_provide_parses nsubj_provide_corpora amod_Bell_1999 dep_Bell_al. nn_Bell_et prep_in_parses_Bell nsubj_parses_dismnbiguated advmod_dismnbiguated_statistically det_dismnbiguated_the appos_Skut_1998 dep_Skut_al. nn_Skut_et dep_corpus_Skut nn_corpus_NeGra det_corpus_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the conj_or_corpora_parses appos_corpora_corpus appos_corpora_Marcus prep_like_corpora_Treebank amod_corpora_annotated nn_corpora_Introduction num_corpora_1 advmod_annotated_Syntactically
C02-1100	J93-2004	o	Without removing them extracted rules can not be triggered until when completely the same strings appear in a text .4 6 Performance Evaluation We measured the performance of our robust parsing algorithm by measuring coverage and degree of overgeneration for the Wall Street Journal in the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_in_Journal_Treebank nn_Journal_Street nn_Journal_Wall det_Journal_the prep_of_coverage_overgeneration conj_and_coverage_degree prep_for_measuring_Journal dobj_measuring_degree dobj_measuring_coverage nn_algorithm_parsing amod_algorithm_robust poss_algorithm_our prep_of_performance_algorithm det_performance_the prepc_by_measured_measuring dobj_measured_performance nsubj_measured_We rcmod_Evaluation_measured nn_Evaluation_Performance num_Evaluation_6 dep_.4_Evaluation nn_.4_text det_.4_a prep_in_appear_.4 nsubj_appear_strings advmod_appear_when amod_strings_same det_strings_the advmod_strings_completely dep_triggered_Marcus prepc_until_triggered_appear auxpass_triggered_be neg_triggered_not aux_triggered_can nsubjpass_triggered_rules prepc_without_triggered_removing amod_rules_extracted dobj_removing_them
C02-1100	J93-2004	o	2 Background Default unification has been investigated by many researchers -LRB- Bouma 1990 Russell et al. 1991 Copestake 1993 Carpenter 1993 Lascarides and Copestake 1999 -RRB- in the context of developing lexical semantics	amod_semantics_lexical dobj_developing_semantics prepc_of_context_developing det_context_the dep_Lascarides_1999 conj_and_Lascarides_Copestake num_Carpenter_1993 num_Copestake_1993 num_Russell_1991 nn_Russell_al. nn_Russell_et prep_in_Bouma_context dep_Bouma_Copestake dep_Bouma_Lascarides conj_Bouma_Carpenter conj_Bouma_Copestake conj_Bouma_Russell appos_Bouma_1990 dep_researchers_Bouma amod_researchers_many agent_investigated_researchers auxpass_investigated_been aux_investigated_has nsubjpass_investigated_unification nn_unification_Default nn_unification_Background num_unification_2
C02-1100	J93-2004	o	As other researchers pursued efficient default unification -LRB- Bouma 1990 Russell et al. 1991 Copestake 1993 -RRB- we also propose another definition of default unification which we call lenient default unification	nn_unification_default amod_unification_lenient dep_call_unification nsubj_call_we dobj_call_which nn_unification_default rcmod_definition_call prep_of_definition_unification det_definition_another dobj_propose_definition advmod_propose_also nsubj_propose_we advcl_propose_pursued dep_Copestake_1993 dep_Russell_Copestake num_Russell_1991 nn_Russell_al. nn_Russell_et dep_Bouma_Russell appos_Bouma_1990 nn_unification_default amod_unification_efficient dep_pursued_Bouma dobj_pursued_unification nsubj_pursued_researchers mark_pursued_As amod_researchers_other
C02-1126	J93-2004	o	For Penn Treebank II style annotation -LRB- Marcus et al. 1993 -RRB- in which a nonterminal symbol is a category together with zero or more functional tags we adopt the following scheme the atomic pattern a matches any label with category a or functional tag a moreover we define Boolean operators ^ _ and	nsubj___operators conj_and_^__ nsubj_^_operators amod_operators_Boolean ccomp_define__ ccomp_define_^ nsubj_define_we advmod_define_moreover conj_a_define amod_tag_functional dep_a_a conj_or_a_tag dep_category_tag dep_category_a prep_with_label_category det_label_any dep_matches_label det_matches_a dep_pattern_matches amod_pattern_atomic det_pattern_the amod_scheme_following det_scheme_the dobj_adopt_scheme nsubj_adopt_we amod_tags_functional num_tags_more num_tags_zero conj_or_zero_more dep_category_pattern rcmod_category_adopt prep_together_with_category_tags det_category_a cop_category_is nsubj_category_symbol prep_in_category_which dep_category_Marcus prep_for_category_annotation amod_symbol_nonterminal det_symbol_a nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_annotation_style nn_annotation_II nn_annotation_Treebank nn_annotation_Penn
C02-1138	J93-2004	o	One kind is the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the cop_Treebank_is nsubj_Treebank_kind num_kind_One
C02-2024	J93-2004	o	The data set consisting of 249,994 TFSs was generated by parsing the Figure 3 The size of Dpi for the size of the data set 800 bracketed sentences in the Wall Street Journal corpus -LRB- the first 800 sentences in Wall Street Journal 00 -RRB- in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- with the XHPSG grammar -LRB- Tateisi et al. 1998 -RRB-	amod_Tateisi_1998 dep_Tateisi_al. nn_Tateisi_et nn_grammar_XHPSG det_grammar_the nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the num_Journal_00 nn_Journal_Street nn_Journal_Wall prep_in_sentences_Journal num_sentences_800 amod_sentences_first det_sentences_the nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the amod_sentences_bracketed num_sentences_800 dep_set_Tateisi prep_with_set_grammar dep_set_Marcus prep_in_set_Treebank dep_set_sentences prep_in_set_corpus dobj_set_sentences prep_for_set_size det_data_the prep_of_size_data det_size_the prep_of_size_Dpi det_size_The dep_Figure_size num_Figure_3 det_Figure_the dobj_parsing_Figure parataxis_generated_set agent_generated_parsing auxpass_generated_was nsubjpass_generated_set num_TFSs_249,994 prep_of_consisting_TFSs vmod_set_consisting nn_set_data det_set_The
C04-1004	J93-2004	o	In recent years HMMs have enjoyed great success in many tagging applications most notably part-of-speech -LRB- POS -RRB- tagging -LRB- Church 1988 Weischedel et al 1993 Merialdo 1994 -RRB- and named entity recognition -LRB- Bikel et al 1999 Zhou et al 2002 -RRB-	dep_al_2002 nn_al_et nn_al_Zhou num_al_1999 dep_Bikel_al dep_Bikel_al nn_Bikel_et nn_recognition_entity amod_recognition_named num_Merialdo_1994 num_al_1993 dep_Weischedel_al nn_Weischedel_et dep_Church_Merialdo dep_Church_Weischedel num_Church_1988 conj_and_tagging_recognition dep_tagging_Church dep_part-of-speech_Bikel dep_part-of-speech_recognition dep_part-of-speech_tagging dep_part-of-speech_POS advmod_part-of-speech_notably advmod_notably_most amod_applications_tagging amod_applications_many amod_success_great dep_enjoyed_part-of-speech prep_in_enjoyed_applications dobj_enjoyed_success aux_enjoyed_have nsubj_enjoyed_HMMs prep_in_enjoyed_years amod_years_recent
C04-1004	J93-2004	o	Experimentation The corpus used in shallow parsing is extracted from the PENN TreeBank -LRB- Marcus et al. 1993 -RRB- of 1 million words -LRB- 25 sections -RRB- by a program provided by Sabine Buchholz from Tilburg University	nn_University_Tilburg prep_from_Buchholz_University nn_Buchholz_Sabine agent_provided_Buchholz vmod_program_provided det_program_a num_sections_25 appos_words_sections num_words_million number_million_1 dep_1993_al. nn_al._et num_Marcus_1993 prep_of_TreeBank_words appos_TreeBank_Marcus nn_TreeBank_PENN det_TreeBank_the agent_extracted_program prep_from_extracted_TreeBank auxpass_extracted_is nsubjpass_extracted_corpus amod_parsing_shallow prep_in_used_parsing vmod_corpus_used det_corpus_The rcmod_Experimentation_extracted
C04-1010	J93-2004	p	To some extent this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics but this trend has been reinforced by the fact that the major treebank of American English the Penn Treebank -LRB- Marcus et al. 1993 -RRB- is annotated primarily with constituent analysis	nn_analysis_constituent prep_with_annotated_analysis advmod_annotated_primarily cop_annotated_is nsubj_annotated_treebank mark_annotated_that amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the amod_English_American appos_treebank_Treebank prep_of_treebank_English amod_treebank_major det_treebank_the ccomp_fact_annotated det_fact_the agent_reinforced_fact auxpass_reinforced_been aux_reinforced_has nsubjpass_reinforced_trend det_trend_this nn_linguistics_Anglo-American prep_in_analysis_linguistics nn_analysis_constituent prep_of_tradition_analysis amod_tradition_strong det_tradition_the conj_but_explained_reinforced agent_explained_tradition auxpass_explained_be advmod_explained_probably aux_explained_can nsubjpass_explained_this prep_to_explained_extent det_extent_some
C04-1010	J93-2004	o	The learning algorithm used is the IB1 algorithm -LRB- Aha et al. 1991 -RRB- with k = 5 i.e. classification based on 5 nearest neighbors .4 Distances are measured using the modified value difference metric -LRB- MVDM -RRB- -LRB- Stanfill and Waltz 1986 Cost and Salzberg 1993 -RRB- for instances with a frequency of at least 3 -LRB- and the simple overlap metric otherwise -RRB- and classification is based on distance weighted class voting with inverse distance weighting -LRB- Dudani 1976 -RRB-	amod_Dudani_1976 dep_weighting_Dudani nn_weighting_distance amod_weighting_inverse prep_with_voting_weighting nn_voting_class amod_voting_weighted nn_voting_distance prep_on_based_voting auxpass_based_is nsubjpass_based_classification advmod_metric_otherwise conj_and_overlap_based dep_overlap_metric det_simple_the quantmod_3_at mwe_at_least prep_of_frequency_3 det_frequency_a prep_with_instances_frequency tmod_Cost_1993 conj_and_Cost_Salzberg dep_Stanfill_Salzberg dep_Stanfill_Cost dep_Stanfill_1986 conj_and_Stanfill_Waltz dep_metric_Waltz dep_metric_Stanfill appos_metric_MVDM prep_for_difference_instances amod_difference_metric conj_and_value_simple dobj_value_difference dep_modified_simple dep_modified_value vmod_the_modified dobj_using_the dep_measured_based dep_measured_overlap xcomp_measured_using auxpass_measured_are nsubjpass_measured_algorithm num_Distances_.4 dep_neighbors_Distances amod_neighbors_nearest num_neighbors_5 prep_on_based_neighbors vmod_classification_based pobj_i.e._classification dep_=_5 amod_k_= appos_Aha_1991 dep_Aha_al. nn_Aha_et prep_algorithm_i.e. prep_with_algorithm_k dep_algorithm_Aha nn_algorithm_IB1 det_algorithm_the cop_algorithm_is nsubj_algorithm_algorithm vmod_algorithm_used nn_algorithm_learning det_algorithm_The
C04-1040	J93-2004	o	Dependency Analyzer PP-Attachment Resolver Root-Node Finder Base NP Chunker -LRB- POS Tagger -RRB- = SVM = Preference Learning Figure 2 Module layers in the system That is we use Penn Treebanks Wall Street Journal data -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_data_Journal nn_data_Street nn_data_Wall nn_data_Treebanks nn_data_Penn dobj_use_data nsubj_use_we nsubj_is_That rcmod_system_is det_system_the prep_in_layers_system nn_layers_Module dep_Figure_layers num_Figure_2 nn_Figure_Learning nn_Figure_Preference dep_=_Figure dep_=_SVM nn_Tagger_POS dep_Chunker_Marcus rcmod_Chunker_use amod_Chunker_= amod_Chunker_= appos_Chunker_Tagger nn_Chunker_NP nn_Chunker_Base nn_Chunker_Finder nn_Chunker_Root-Node nn_Chunker_Resolver nn_Chunker_PP-Attachment nn_Chunker_Analyzer nn_Chunker_Dependency
C04-1055	J93-2004	o	The TRIPS structure generally has more levels of structure -LRB- roughly corresponding to levels in X-bar theory -RRB- than the Penn Treebank analyses -LRB- Marcus et al. 1993 -RRB- in particular for base noun phrases	nn_phrases_noun nn_phrases_base prep_for_Marcus_phrases prep_in_Marcus_particular dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_analyses_Treebank nn_analyses_Penn det_analyses_the nn_theory_X-bar prep_in_levels_theory prep_to_corresponding_levels advmod_corresponding_roughly prep_than_levels_analyses dep_levels_corresponding prep_of_levels_structure amod_levels_more dep_has_Marcus dobj_has_levels advmod_has_generally nsubj_has_structure nn_structure_TRIPS det_structure_The
C04-1082	J93-2004	o	The tagger described in this paper is based on the standard Hidden Markov Model architecture -LRB- Charniak et al. 1993 Brants 2000 -RRB-	dep_Brants_2000 dep_Charniak_Brants appos_Charniak_1993 dep_Charniak_al. nn_Charniak_et appos_architecture_Charniak nn_architecture_Model nn_architecture_Markov nn_architecture_Hidden amod_architecture_standard det_architecture_the prep_on_based_architecture auxpass_based_is nsubjpass_based_tagger det_paper_this prep_in_described_paper vmod_tagger_described det_tagger_The
C04-1082	J93-2004	o	3.1 Experiments The model described in section 2 has been tested on the Brown corpus -LRB- Francis and Kucera 1982 -RRB- tagged with the 45 tags of the Penn treebank tagset -LRB- Marcus et al. 1993 -RRB- which constitute the initial tagset T0	nn_T0_tagset amod_T0_initial det_T0_the dobj_constitute_T0 nsubj_constitute_which dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_tagset_treebank nn_tagset_Penn det_tagset_the prep_of_tags_tagset num_tags_45 det_tags_the prep_with_tagged_tags dep_Francis_1982 conj_and_Francis_Kucera appos_corpus_Kucera appos_corpus_Francis amod_corpus_Brown det_corpus_the prep_on_tested_corpus auxpass_tested_been aux_tested_has nsubjpass_tested_model num_section_2 prep_in_described_section vmod_model_described det_model_The rcmod_Experiments_constitute dep_Experiments_Marcus vmod_Experiments_tagged rcmod_Experiments_tested num_Experiments_3.1 dep_``_Experiments
C04-1140	J93-2004	o	The Brill tagger comes with an English default version also trained on general-purpose language corpora like the PENN TREEBANK -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TREEBANK_PENN det_TREEBANK_the nn_corpora_language amod_corpora_general-purpose prep_like_trained_TREEBANK prep_on_trained_corpora advmod_trained_also vmod_version_trained nn_version_default amod_version_English det_version_an dep_comes_Marcus prep_with_comes_version nsubj_comes_tagger nn_tagger_Brill det_tagger_The
C04-1140	J93-2004	o	This is most prominently evidenced by the PENN TREEBANK -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_TREEBANK_Marcus nn_TREEBANK_PENN det_TREEBANK_the agent_evidenced_TREEBANK advmod_evidenced_prominently auxpass_evidenced_is nsubjpass_evidenced_This advmod_prominently_most ccomp_``_evidenced
C04-1197	J93-2004	o	The training set is extracted from TreeBank -LRB- Marcus et al. 1993 -RRB- section 1518 the development set used in tuning parameters of the system from section 20 and the test set from section 21	num_section_21 prep_from_set_section vmod_test_set det_test_the num_section_20 det_system_the prep_of_parameters_system nn_parameters_tuning prep_in_used_parameters prep_from_set_section vmod_set_used nn_set_development det_set_the num_section_1518 conj_and_Marcus_test conj_and_Marcus_set dep_Marcus_section dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_extracted_test dep_extracted_set dep_extracted_Marcus prep_from_extracted_TreeBank auxpass_extracted_is nsubjpass_extracted_set nn_set_training det_set_The
C08-1012	J93-2004	o	2 Data Sets for the Experiments 2.1 Coordination Annotation in the PENN TREEBANK For our experiments we used the WSJ part of the PENN TREEBANK -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TREEBANK_PENN det_TREEBANK_the prep_of_part_TREEBANK nn_part_WSJ det_part_the dep_used_Marcus dobj_used_part nsubj_used_we poss_experiments_our nn_TREEBANK_PENN det_TREEBANK_the prep_for_Annotation_experiments prep_in_Annotation_TREEBANK nn_Annotation_Coordination num_Annotation_2.1 dep_Experiments_Annotation det_Experiments_the parataxis_Sets_used prep_for_Sets_Experiments nsubj_Sets_Data num_Data_2 ccomp_``_Sets
C08-1025	J93-2004	n	For instance about 38 % of verbs in the training sections of the Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB- occur only once the lexical properties of these verbs -LRB- such as their most common subcategorization frames -RRB- can not be represented accurately in a model trained exclusively on the Penn Treebank	nn_Treebank_Penn det_Treebank_the prep_on_trained_Treebank advmod_trained_exclusively vmod_model_trained det_model_a prep_in_represented_model advmod_represented_accurately auxpass_represented_be neg_represented_not aux_represented_can nsubjpass_represented_properties advmod_represented_once nn_frames_subcategorization amod_frames_common poss_frames_their advmod_common_most prep_such_as_verbs_frames det_verbs_these prep_of_properties_verbs amod_properties_lexical det_properties_the advmod_once_only ccomp_occur_represented nsubj_occur_% prep_for_occur_instance amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the prep_of_sections_Treebank nn_sections_training det_sections_the appos_%_Marcus prep_in_%_sections prep_of_%_verbs num_%_38 quantmod_38_about
C08-1026	J93-2004	o	For example in the WSJ corpus part of the Penn Treebank 3 release -LRB- Marcus et al. 1993 -RRB- the string in -LRB- 1 -RRB- is a variation 12-gram since off is a variation nucleus that is tagged preposition -LRB- IN -RRB- in one corpus occurrence and particle -LRB- RP -RRB- in another .1 Dickinson -LRB- 2005 -RRB- shows that examining those cases with identical local contextin this case lookingat ward off aresultsinanestimated error detection precision of 92.5 %	num_%_92.5 prep_of_precision_% nn_precision_detection nn_precision_error amod_precision_aresultsinanestimated prep_off_ward_precision nn_ward_lookingat conj_case_ward det_case_this amod_contextin_local amod_contextin_identical det_cases_those dep_examining_case prep_with_examining_contextin dobj_examining_cases dep_that_examining prep_shows_that nsubj_shows_Dickinson mark_shows_another appos_Dickinson_2005 num_Dickinson_.1 appos_particle_RP conj_and_occurrence_particle nn_occurrence_corpus num_occurrence_one prepc_in_in_shows pobj_in_particle pobj_in_occurrence pcomp_IN_in prep_preposition_IN dobj_tagged_preposition auxpass_tagged_is nsubjpass_tagged_that rcmod_nucleus_tagged nn_nucleus_variation det_nucleus_a cop_nucleus_is advmod_nucleus_off prep_since_12-gram_nucleus nn_12-gram_variation det_12-gram_a nsubj_is_12-gram dep_is_1 prepc_in_string_is det_string_the dep_string_part prep_in_string_corpus prep_for_string_example amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_release_3 nn_release_Treebank nn_release_Penn det_release_the dep_part_Marcus prep_of_part_release nn_corpus_WSJ det_corpus_the
C08-1038	J93-2004	o	1999 -RRB- OpenCCG -LRB- White 2004 -RRB- and XLE -LRB- Crouch et al. 2007 -RRB- or created semi-automatically -LRB- Belz 2007 -RRB- or fully automatically extracted from annotated corpora like the HPSG -LRB- Nakanishi et al. 2005 -RRB- LFG -LRB- Cahill and van Genabith 2006 Hogan et al. 2007 -RRB- and CCG -LRB- White et al. 2007 -RRB- resources derived from the Penn-II Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn-II det_Treebank_the prep_from_derived_Treebank dep_resources_Marcus vmod_resources_derived nn_resources_CCG nn_resources_LFG nn_resources_created nn_resources_XLE nn_resources_White dep_White_2007 dep_White_al. nn_White_et num_Hogan_2007 nn_Hogan_al. nn_Hogan_et nn_Genabith_van dep_Cahill_Hogan dep_Cahill_2006 conj_and_Cahill_Genabith dep_LFG_Genabith dep_LFG_Cahill amod_Nakanishi_2005 dep_Nakanishi_al. nn_Nakanishi_et det_HPSG_the amod_corpora_annotated prep_from_extracted_corpora advmod_extracted_automatically advmod_automatically_fully amod_Belz_2007 dep_semi-automatically_Nakanishi prep_like_semi-automatically_HPSG conj_or_semi-automatically_extracted dep_semi-automatically_Belz advmod_created_extracted advmod_created_semi-automatically amod_Crouch_2007 dep_Crouch_al. nn_Crouch_et appos_XLE_Crouch dep_White_White conj_and_White_CCG conj_or_White_LFG conj_or_White_created conj_and_White_XLE num_White_2004 nn_White_OpenCCG conj_1999_resources
C08-1050	J93-2004	o	By habit most systems for automatic role-semantic analysis have used Pennstyle constituents -LRB- Marcus et al. 1993 -RRB- produced by Collins -LRB- 1997 -RRB- or Charniaks -LRB- 2000 -RRB- parsers	nn_parsers_Charniaks appos_Charniaks_2000 conj_or_Collins_parsers appos_Collins_1997 prep_by_produced_parsers prep_by_produced_Collins amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_constituents_Pennstyle dep_used_produced dep_used_Marcus dobj_used_constituents aux_used_have nsubj_used_systems prep_by_used_habit amod_analysis_role-semantic amod_analysis_automatic prep_for_systems_analysis amod_systems_most
C08-1050	J93-2004	o	Statistical dependency parsers of English must therefore rely on dependency structures automatically converted from a constituent corpus such as the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_such_as_corpus_Treebank nn_corpus_constituent det_corpus_a prep_from_converted_corpus advmod_converted_automatically nn_structures_dependency dep_rely_converted prep_on_rely_structures advmod_rely_therefore aux_rely_must nsubj_rely_parsers prep_of_parsers_English nn_parsers_dependency amod_parsers_Statistical
C08-1094	J93-2004	o	Hence our classifier evaluation omits those two word positions leading to n2 classifications for a string of length n. Table 1 shows statistics from sections 2-21 of the Penn WSJ Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_WSJ nn_Treebank_Penn det_Treebank_the prep_of_sections_Treebank num_sections_2-21 dep_statistics_Marcus prep_from_statistics_sections dobj_shows_statistics nsubj_shows_positions num_Table_1 nn_Table_n. nn_Table_length prep_of_string_Table det_string_a prep_for_classifications_string nn_classifications_n2 prep_to_leading_classifications vmod_positions_leading nn_positions_word num_positions_two det_positions_those ccomp_omits_shows nsubj_omits_evaluation advmod_omits_Hence nn_evaluation_classifier poss_evaluation_our
C08-1113	J93-2004	o	As mentioned in Section 2.2 there are words which have two or more candidate POS tags in the PTB corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_PTB det_corpus_the nn_tags_POS nn_tags_candidate num_tags_more num_tags_two conj_or_two_more prep_in_have_corpus dobj_have_tags nsubj_have_which dep_words_Marcus rcmod_words_have nsubj_are_words expl_are_there advcl_are_mentioned num_Section_2.2 prep_in_mentioned_Section mark_mentioned_As
C94-2149	J93-2004	o	Of the 1600 IBM sentences that have been parsed -LRB- those available from the Penn Treebank \ -LSB- Marcus et al. 19931 -RRB- only 67 overlapped with the IBM-manual treebank that was bracketed by University of Lancaster	prep_of_University_Lancaster agent_bracketed_University auxpass_bracketed_was nsubjpass_bracketed_that rcmod_treebank_bracketed amod_treebank_IBM-manual det_treebank_the prep_with_overlapped_treebank advmod_67_only amod_Marcus_19931 dep_Marcus_al. nn_Marcus_et nn_\_Treebank nn_\_Penn det_\_the prep_from_available_\ vmod_those_overlapped appos_those_67 dep_those_Marcus amod_those_available dobj_parsed_those auxpass_parsed_been aux_parsed_have nsubjpass_parsed_that rcmod_sentences_parsed nn_sentences_IBM num_sentences_1600 det_sentences_the prep_of_``_sentences
C94-2149	J93-2004	o	\ -LSB- Marcus et al. 1993 \ -RSB- Marcus M. Santorini B. and Malvinkiewicz M.A.	nn_B._Santorini conj_and_M._Malvinkiewicz conj_and_M._B. nn_M._Marcus dep_M._Marcus num_\_1993 dep_Marcus_\ dep_Marcus_al. nn_Marcus_et dep_\_M.A. dep_\_Malvinkiewicz dep_\_B. dep_\_M.
C96-1003	J93-2004	o	to estimale a model -LRB- clustering words -RRB- and measured the I -LRB- L distancd ~ between ` l'he K \ -RSB- distance -LRB- relative Clt | l Opy -RRB- which is widely used in information theory and sta tist ics is a nleasur ,2 of ` dista n < c ' l > ~ \ -LSB- wcen two distributions 5.2 Experiment 2 Qualitative Evaluation We extracted roughly 180,000 case fl anles from the bracketed WSJ -LRB- Wall Street Journal -RRB- corpus of the Penn Tree Bank -LRB- Marcus et al. 1993 -RRB- as co-occurrence data	nn_data_co-occurrence amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Bank_Tree nn_Bank_Penn det_Bank_the prep_of_corpus_Bank nn_corpus_WSJ amod_corpus_bracketed det_corpus_the nn_Journal_Street nn_Journal_Wall appos_WSJ_Journal prep_as_anles_data appos_anles_Marcus prep_from_anles_corpus dep_fl_anles dep_case_fl num_case_180,000 quantmod_180,000_roughly dobj_extracted_case nsubj_extracted_We rcmod_Evaluation_extracted amod_Evaluation_Qualitative num_Experiment_2 num_Experiment_5.2 dep_distributions_Experiment num_distributions_two dep_wcen_Evaluation dobj_wcen_distributions dep_\_wcen num_\_~ amod_\_> nn_\_l nn_\_c dep_<_\ vmod_n_< appos_,2_n prep_of_,2_dista amod_,2_nleasur det_,2_a cop_,2_is appos_tist_ics conj_and_theory_sta nn_theory_information prep_in_used_sta prep_in_used_theory advmod_used_widely auxpass_used_is nsubjpass_used_which dep_|_Opy appos_|_l nn_|_Clt amod_|_relative dep_distance_| appos_\_tist rcmod_\_used appos_\_distance nn_\_K amod_\_l'he prep_between_~_\ nn_~_distancd nn_~_L nn_~_I det_~_the dobj_measured_~ nn_words_clustering appos_model_words det_model_a dep_estimale_,2 conj_and_estimale_measured dobj_estimale_model aux_estimale_to ccomp_``_measured ccomp_``_estimale
C96-1003	J93-2004	o	In particular we used this method with WordNet -LRB- Miller et al. 1993 -RRB- and using the same training data	nn_data_training amod_data_same det_data_the dobj_using_data nsubj_using_we amod_Miller_1993 dep_Miller_al. nn_Miller_et dep_WordNet_Miller det_method_this conj_and_used_using prep_with_used_WordNet dobj_used_method nsubj_used_we prep_in_used_particular
C96-1003	J93-2004	o	have been proposed -LRB- Hindle 1990 Brown et al. 1992 Pereira et al. 1993 Tokunaga et al. 1995 -RRB-	num_Tokunaga_1995 nn_Tokunaga_al. nn_Tokunaga_et num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Hindle_Tokunaga dep_Hindle_Pereira dep_Hindle_Brown dep_Hindle_1990 dep_proposed_Hindle auxpass_proposed_been aux_proposed_have
C96-1020	J93-2004	o	Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers -LRB- Black et al. 1992 Brill 1994 Merialdo 1994 Weischedel et al. 1993 -RRB- and for statistical parsers -LRB- Black et al. 1993 Brill 1993 aelinek et al. 1994 Magerman 1995 Magerman and Marcus 1991 -RRB-	amod_Magerman_1991 conj_and_Magerman_Marcus dep_Magerman_Marcus dep_Magerman_Magerman num_Magerman_1995 nn_al._et nn_al._aelinek num_Brill_1993 dep_al._Magerman num_al._1994 dep_al._al. dep_al._Brill num_al._1993 nn_al._et amod_al._Black amod_parsers_statistical dep_for_al. pobj_for_parsers num_Weischedel_1993 nn_Weischedel_al. nn_Weischedel_et num_Merialdo_1994 conj_and_Brill_for dep_Brill_Weischedel conj_and_Brill_Merialdo num_Brill_1994 dep_al._for dep_al._Merialdo dep_al._Brill dep_al._1992 nn_al._et amod_al._Black nn_taggers_speech nn_taggers_og nn_taggers_part amod_taggers_statistical prep_for_data_taggers nn_data_training prep_of_source_data det_source_a nn_processing_language amod_processing_natural prep_of_field_processing det_field_the dep_used_al. prep_as_used_source prep_within_used_field auxpass_used_been aux_used_have nsubjpass_used_Treebanks
C96-1020	J93-2004	o	All of the features of the ATR/Lancaster Treebank that are described below represent a radical departure from extant large-scale -LRB- Eyes and Leech 1993 Garside and McEnery 1993 Marcus et al. 1993 -RRB- treebanks	nn_treebanks_Marcus num_Marcus_1993 nn_Marcus_al. nn_Marcus_et conj_and_Garside_treebanks conj_and_Garside_1993 conj_and_Garside_McEnery conj_and_Eyes_Leech amod_Eyes_large-scale amod_Eyes_extant dep_departure_treebanks dep_departure_1993 dep_departure_McEnery dep_departure_Garside dep_departure_1993 prep_from_departure_Leech prep_from_departure_Eyes amod_departure_radical det_departure_a dobj_represent_departure nsubj_represent_All advmod_described_below auxpass_described_are nsubjpass_described_that rcmod_Treebank_described nn_Treebank_ATR/Lancaster det_Treebank_the prep_of_features_Treebank det_features_the prep_of_All_features ccomp_``_represent
C96-1038	J93-2004	o	4 Experiments The Penn Treebank -LRB- Marcus et al. 1993 -RRB- is used as the testing corpus	nn_corpus_testing det_corpus_the prep_as_used_corpus auxpass_used_is nsubjpass_used_Treebank amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_The rcmod_Experiments_used num_Experiments_4
C96-1041	J93-2004	o	' \ -LSB- ` here are three main approaches in tagging problem rule-based approach -LRB- Klein and Simmons 1 % 3 Brodda 1982 Paulussen and Martin 1992 Brill et al. 1990 -RRB- statistical approach -LRB- Church :1988 Merialdo 1994 Foster 1991 Weischedel et al. 1993 Kupiec 1992 -RRB- and connectionist approach -LRB- Benello et al. 1989 Nakanmra et al. 1989 -RRB-	dep_al._1989 nn_al._et nn_al._Nakanmra num_al._1989 nn_al._et dep_Benello_al. dep_Benello_al. nn_approach_connectionist num_Kupiec_1992 num_al._1993 nn_al._et nn_al._Weischedel num_Foster_1991 num_Merialdo_1994 conj_Church_Kupiec conj_Church_al. conj_Church_Foster conj_Church_Merialdo num_Church_:1988 dep_approach_Church amod_approach_statistical dep_al._1990 nn_al._et nn_al._Brill dep_Paulussen_1992 conj_and_Paulussen_Martin num_Brodda_1982 num_%_3 num_%_1 nn_%_Simmons dep_Klein_al. conj_and_Klein_Martin conj_and_Klein_Paulussen conj_and_Klein_Brodda conj_and_Klein_% dep_approach_Benello conj_and_approach_approach conj_and_approach_approach appos_approach_Paulussen appos_approach_Brodda appos_approach_% appos_approach_Klein amod_approach_rule-based amod_problem_tagging dep_approaches_approach dep_approaches_approach dep_approaches_approach prep_in_approaches_problem amod_approaches_main num_approaches_three cop_approaches_are nsubj_approaches_here amod_approaches_\
C96-2114	J93-2004	o	-LRB- Marcus et al. 1993 316 -RRB-	num_1993_316 dep_al._1993 nn_al._et dep_Marcus_al. dep_''_Marcus
C96-2114	J93-2004	o	The tagger used is thus one that does not need tagged and disambiguated material to be trained on namely the XPOST originally constructed at Xerox Parc -LRB- Cutting et al. 1992 Cutting and Pedersen 1993 -RRB-	num_Pedersen_1993 advmod_1992_al. nn_al._et conj_and_Cutting_Pedersen conj_and_Cutting_Cutting dobj_Cutting_1992 nn_Parc_Xerox dep_constructed_Pedersen dep_constructed_Cutting dep_constructed_Cutting prep_at_constructed_Parc advmod_constructed_originally nsubj_constructed_XPOST advmod_constructed_namely det_XPOST_the prepc_on_trained_constructed auxpass_trained_be aux_trained_to amod_material_disambiguated amod_material_tagged conj_and_tagged_disambiguated xcomp_need_trained dobj_need_material neg_need_not aux_need_does nsubj_need_that rcmod_one_need advmod_one_thus cop_one_is nsubj_one_tagger vmod_tagger_used det_tagger_The
C96-2125	J93-2004	p	Recently we can see an important development in natural language processing and computational linguistics towards the use of empirical learning methods -LRB- for instance -LRB- Charniak 1993 Marcus et al. 1993 Wermter 11995 Jones 1995 Werml er et al. 1996 -RRB- -RRB-	num_er_1996 nn_er_al. nn_er_et num_Jones_1995 dep_Wermter_er conj_Wermter_Werml conj_Wermter_Jones conj_Wermter_11995 dep_Marcus_Wermter num_Marcus_1993 nn_Marcus_al. nn_Marcus_et dep_Charniak_Marcus appos_Charniak_1993 nn_methods_learning amod_methods_empirical prep_of_use_methods det_use_the amod_linguistics_computational conj_and_processing_linguistics nn_processing_language amod_processing_natural dep_development_Charniak prep_for_development_instance prep_towards_development_use prep_in_development_linguistics prep_in_development_processing amod_development_important det_development_an dobj_see_development aux_see_can nsubj_see_we advmod_see_Recently
C96-2185	J93-2004	o	4 Information Base 4.1 Text Corpus Text corpora are essential to statistical modeling in developing formal theories of the grammars investigating prosodic phenomena in speech and evaluating or comparing the adequacy of parsing models -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_models_parsing prep_of_adequacy_models det_adequacy_the dobj_evaluating_adequacy conj_or_evaluating_comparing prep_in_phenomena_speech amod_phenomena_prosodic dep_investigating_Marcus conj_and_investigating_comparing conj_and_investigating_evaluating dobj_investigating_phenomena ccomp_,_evaluating ccomp_,_investigating det_grammars_the prep_of_theories_grammars amod_theories_formal amod_theories_developing pobj_in_theories dep_,_in amod_modeling_statistical prep_to_essential_modeling cop_essential_are nsubj_essential_Information dep_essential_4 dobj_Text_corpora vmod_Corpus_Text dobj_Text_Corpus dep_4.1_Text num_Base_4.1 dep_Information_Base ccomp_``_essential
C96-2187	J93-2004	p	Successflfl examples of reuse of data resources include the WordNet thesaurus -LRB- Miller el al. 1993 -RRB- the Penn Tree Bank -LRB- Marcus et al. 1993 -RRB- the Longmans Dictionary of Contemporary English -LRB- Summers 1995 -RRB-	amod_Summers_1995 dep_English_Summers nn_English_Contemporary prep_of_Dictionary_English nn_Dictionary_Longmans det_Dictionary_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Bank_Marcus nn_Bank_Tree nn_Bank_Penn det_Bank_the dep_al._1993 dep_el_al. nn_el_Miller conj_thesaurus_Dictionary conj_thesaurus_Bank appos_thesaurus_el nn_thesaurus_WordNet det_thesaurus_the dep_include_thesaurus nsubj_include_examples nn_resources_data prep_of_reuse_resources prep_of_examples_reuse nn_examples_Successflfl
D07-1003	J93-2004	o	This sort of problem can be solved in principle by conditional variants of the Expectation-Maximization algorithm -LRB- Baum et al. 1970 Dempster et al. 1977 Meng and Rubin 1993 Jebara and Pentland 1999 -RRB-	amod_Jebara_1999 conj_and_Jebara_Pentland num_Meng_1993 conj_and_Meng_Rubin dep_Dempster_Pentland dep_Dempster_Jebara conj_Dempster_Rubin conj_Dempster_Meng num_Dempster_1977 nn_Dempster_al. nn_Dempster_et dep_Baum_Dempster appos_Baum_1970 dep_Baum_al. nn_Baum_et nn_algorithm_Expectation-Maximization det_algorithm_the prep_of_variants_algorithm amod_variants_conditional dep_solved_Baum agent_solved_variants prep_in_solved_principle auxpass_solved_be aux_solved_can nsubjpass_solved_sort prep_of_sort_problem det_sort_This ccomp_``_solved
D07-1003	J93-2004	o	Similarly Murdock and Croft -LRB- 2005 -RRB- adopted a simple translation model from IBM model 1 -LRB- Brown et al. 1990 Brown et al. 1993 -RRB- and applied it to QA	prep_to_applied_QA dobj_applied_it nsubj_applied_Murdock num_Brown_1993 nn_Brown_al. nn_Brown_et dep_Brown_Brown amod_Brown_1990 dep_Brown_al. nn_Brown_et num_model_1 nn_model_IBM dep_model_Brown prep_from_model_model nn_model_translation amod_model_simple det_model_a conj_and_adopted_applied dobj_adopted_model nsubj_adopted_Croft nsubj_adopted_Murdock advmod_adopted_Similarly appos_Croft_2005 conj_and_Murdock_Croft
D07-1003	J93-2004	o	The tree is produced by a state-of-the-art dependency parser -LRB- McDonald et al. 2005 -RRB- trained on the Wall Street Journal Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn nn_Treebank_Journal nn_Treebank_Street nn_Treebank_Wall det_Treebank_the prep_on_trained_Treebank amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et vmod_parser_trained appos_parser_McDonald nn_parser_dependency amod_parser_state-of-the-art det_parser_a dep_produced_Marcus agent_produced_parser auxpass_produced_is nsubjpass_produced_tree det_tree_The
D07-1018	J93-2004	o	For example given that each semantic class exhibits a particular syntactic behaviour information on the semantic class should improve POStagging for adjective-noun and adjective-participle ambiguities probably the most difficult distinctions both for humans and computers -LRB- Marcus et al. 1993 Brants 2000 -RRB-	dep_Brants_2000 dep_Marcus_Brants appos_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_humans_computers dep_distinctions_Marcus prep_for_distinctions_computers prep_for_distinctions_humans preconj_distinctions_both amod_distinctions_difficult det_distinctions_the advmod_distinctions_probably advmod_difficult_most amod_ambiguities_adjective-participle amod_ambiguities_adjective-noun conj_and_adjective-noun_adjective-participle prep_for_POStagging_ambiguities dobj_improve_distinctions dobj_improve_POStagging aux_improve_should dep_improve_given prep_for_improve_example amod_class_semantic det_class_the prep_on_information_class appos_behaviour_information nn_behaviour_syntactic amod_behaviour_particular det_behaviour_a dobj_exhibits_behaviour nsubj_exhibits_class mark_exhibits_that amod_class_semantic det_class_each ccomp_given_exhibits
D07-1023	J93-2004	o	We use as our English corpus the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the amod_corpus_English poss_corpus_our dep_use_Marcus dobj_use_portion prep_as_use_corpus nsubj_use_We
D07-1028	J93-2004	o	When tested on f-structures for all sentences from Section 23 of the Penn Wall Street Journal -LRB- WSJ -RRB- treebank -LRB- Mar267 cus et al. 1993 -RRB- the techniques described in this paper improve BLEU score from 66.52 to 68.82	nn_score_BLEU prep_to_improve_68.82 prep_from_improve_66.52 dobj_improve_score nsubj_improve_techniques advcl_improve_tested det_paper_this prep_in_described_paper vmod_techniques_described det_techniques_the amod_cus_1993 dep_cus_al. nn_cus_et nn_cus_Mar267 dep_treebank_cus nn_treebank_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall nn_Journal_Penn det_Journal_the prep_of_Section_treebank num_Section_23 prep_from_sentences_Section det_sentences_all prep_for_tested_sentences prep_on_tested_f-structures advmod_tested_When ccomp_``_improve
D07-1031	J93-2004	o	2 Evaluation All of the experiments described below have the same basic structure an estimator is used to infer a bitag HMM from the unsupervised training corpus -LRB- the words of Penn Treebank -LRB- PTB -RRB- Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB- -RRB- and then the resulting model is used to label each word of that corpus with one of the HMMs hidden states	amod_states_hidden dep_HMMs_states det_HMMs_the prep_of_one_HMMs det_corpus_that prep_of_word_corpus det_word_each prep_with_label_one dobj_label_word aux_label_to xcomp_used_label auxpass_used_is nsubjpass_used_model advmod_used_then amod_model_resulting det_model_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall nn_corpus_Treebank appos_Treebank_PTB nn_Treebank_Penn dep_words_Marcus prep_of_words_corpus det_words_the nn_corpus_training amod_corpus_unsupervised det_corpus_the appos_HMM_words prep_from_HMM_corpus nn_HMM_bitag det_HMM_a dobj_infer_HMM aux_infer_to conj_and_used_used xcomp_used_infer auxpass_used_is nsubjpass_used_estimator det_estimator_an amod_structure_basic amod_structure_same det_structure_the dobj_have_structure nsubj_have_All advmod_described_below vmod_experiments_described det_experiments_the prep_of_All_experiments dep_Evaluation_used dep_Evaluation_used rcmod_Evaluation_have num_Evaluation_2 dep_``_Evaluation
D07-1058	J93-2004	o	For a second set of parsing experiments we used the WSJ portion of the Penn Tree Bank -LRB- Marcus et al. 1993 -RRB- and Helmut Schmids enrichment program tmod -LRB- Schmid 2006 -RRB-	amod_Schmid_2006 dep_tmod_Schmid nn_tmod_program nn_tmod_enrichment nn_tmod_Schmids nn_tmod_Helmut conj_and_Marcus_tmod amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Bank_Tree nn_Bank_Penn det_Bank_the prep_of_portion_Bank nn_portion_WSJ det_portion_the dep_used_tmod dep_used_Marcus dobj_used_portion nsubj_used_we prep_for_used_set nn_experiments_parsing prep_of_set_experiments amod_set_second det_set_a
D07-1078	J93-2004	o	1 Introduction Syntax-based translation models -LRB- Eisner 2003 Galley et al. 2006 Marcu et al. 2006 -RRB- are usually built directly from Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB- style parse trees by composing treebank grammar rules	nn_rules_grammar amod_rules_treebank amod_rules_composing prep_by_parse_rules dobj_parse_trees dep_style_parse dep_style_Marcus dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn dep_built_style prep_from_built_Treebank advmod_built_directly advmod_built_usually auxpass_built_are nsubjpass_built_models num_Marcu_2006 nn_Marcu_al. nn_Marcu_et num_Galley_2006 nn_Galley_al. nn_Galley_et dep_Eisner_Marcu dep_Eisner_Galley appos_Eisner_2003 appos_models_Eisner nn_models_translation amod_models_Syntax-based nn_models_Introduction num_models_1 ccomp_``_built
D07-1082	J93-2004	o	6 Evaluation 6.1 Data The data used for our comparison experiments were developed as part of the OntoNotes project -LRB- Hovy et al. 2006 -RRB- which uses the WSJ part of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_part_Treebank nn_part_WSJ det_part_the dep_uses_Marcus dobj_uses_part nsubj_uses_which amod_Hovy_2006 dep_Hovy_al. nn_Hovy_et nn_project_OntoNotes det_project_the prep_of_part_project dep_developed_Hovy prep_as_developed_part auxpass_developed_were nsubjpass_developed_data nn_experiments_comparison poss_experiments_our prep_for_used_experiments vmod_data_used det_data_The rcmod_Data_uses rcmod_Data_developed num_Data_6.1 nn_Data_Evaluation num_Data_6 dep_``_Data
D07-1096	J93-2004	o	918 English For English we used the Wall Street Journal section of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_section_Treebank nn_section_Journal nn_section_Street nn_section_Wall det_section_the dep_used_Marcus dobj_used_section nsubj_used_we dep_used_English prep_for_English_English num_English_918
D07-1096	J93-2004	o	3.2 Domain Adaptation Track As mentioned previously the source data is drawn from a corpus of news specifically the Wall Street Journal section of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_section_Marcus prep_of_section_Treebank nn_section_Journal nn_section_Street nn_section_Wall det_section_the advmod_section_specifically prep_of_corpus_news det_corpus_a dep_drawn_section prep_from_drawn_corpus auxpass_drawn_is nsubjpass_drawn_data dep_drawn_Track nn_data_source det_data_the advmod_mentioned_previously mark_mentioned_As dep_Track_mentioned nn_Track_Adaptation nn_Track_Domain num_Track_3.2
D07-1097	J93-2004	o	1 Introduction In the multilingual track of the CoNLL 2007 shared task on dependency parsing a single parser must be trained to handle data from ten different languages Arabic -LRB- Hajic et al. 2004 -RRB- Basque -LRB- Aduriz et al. 2003 -RRB- Catalan -LRB- Mart et al. 2007 -RRB- Chinese -LRB- Chen et al. 2003 -RRB- Czech -LRB- Bohmova et al. 2003 -RRB- English -LRB- Marcus et al. 1993 Johansson and Nugues 2007 -RRB- Greek -LRB- Prokopidis et al. 2005 -RRB- Hungarian -LRB- Csendes et al. 2005 -RRB- Italian -LRB- Montemagni et al. 2003 -RRB- and Turkish -LRB- Oflazer et al. 2003 -RRB- .1 Our contribution is a study in multilingual parser optimization using the freely available MaltParser system which performs 1For more information about the task and the data sets see Nivre et al.	nn_al._et nn_al._Nivre dep_see_al. nn_sets_data det_sets_the conj_and_task_sets det_task_the prep_about_information_sets prep_about_information_task amod_information_more nn_information_1For dobj_performs_information nsubj_performs_which rcmod_system_performs nn_system_MaltParser amod_system_available det_system_the advmod_available_freely dobj_using_system nn_optimization_parser amod_optimization_multilingual amod_study_see vmod_study_using prep_in_study_optimization det_study_a cop_study_is nsubj_study_contribution poss_contribution_Our num_contribution_.1 amod_contribution_Turkish amod_contribution_Italian amod_contribution_Hungarian amod_contribution_Arabic dep_al._2003 nn_al._et advmod_Oflazer_al. dep_al._2003 nn_al._et advmod_Montemagni_al. dep_al._2005 nn_al._et advmod_Csendes_al. dep_Hungarian_Csendes amod_Prokopidis_2005 dep_Prokopidis_al. nn_Prokopidis_et dep_Johansson_2007 conj_and_Johansson_Nugues dep_Marcus_Nugues dep_Marcus_Johansson appos_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_Bohmova_2003 dep_Bohmova_al. nn_Bohmova_et amod_Chen_2003 dep_Chen_al. nn_Chen_et amod_Mart_2007 dep_Mart_al. nn_Mart_et amod_Aduriz_2003 dep_Aduriz_al. nn_Aduriz_et dep_al._2004 nn_al._et advmod_Hajic_al. dep_Arabic_Oflazer conj_and_Arabic_Turkish dep_Arabic_Montemagni conj_and_Arabic_Italian conj_and_Arabic_Hungarian dep_Arabic_Prokopidis dep_Arabic_Greek dep_Arabic_Marcus dep_Arabic_English dep_Arabic_Bohmova dep_Arabic_Czech dep_Arabic_Chen dep_Arabic_Chinese dep_Arabic_Mart dep_Arabic_Catalan dep_Arabic_Aduriz dep_Arabic_Basque dep_Arabic_Hajic amod_languages_different num_languages_ten prep_from_handle_languages dobj_handle_data aux_handle_to xcomp_trained_handle auxpass_trained_be aux_trained_must nsubjpass_trained_parser amod_parser_single det_parser_a nn_parsing_dependency prep_on_task_parsing amod_task_shared num_task_2007 nn_task_CoNLL det_task_the prep_of_track_task amod_track_multilingual det_track_the dep_Introduction_study rcmod_Introduction_trained prep_in_Introduction_track num_Introduction_1
D07-1099	J93-2004	o	4 Experiments We evaluated the ISBN parser on all the languages considered in the shared task -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bohmova et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Csendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB-	num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bohmova_2003 nn_Bohmova_al. nn_Bohmova_et dep_Chen_Oflazer conj_and_Chen_Montemagni conj_and_Chen_Csendes conj_and_Chen_Prokopidis num_Chen_2007 conj_and_Chen_Nugues conj_and_Chen_Johansson conj_and_Chen_Marcus conj_and_Chen_Bohmova num_Chen_2003 nn_Chen_al. nn_Chen_et dep_al._2007 nn_al._et nn_al._Mart num_al._2003 nn_al._et nn_al._Aduriz dep_al._Montemagni dep_al._Csendes dep_al._Prokopidis dep_al._Nugues dep_al._Johansson dep_al._Marcus dep_al._Bohmova dep_al._Chen dep_al._al. dep_al._al. num_al._2004 nn_al._et amod_al._Hajic dep_task_al. amod_task_shared det_task_the prep_in_considered_task vmod_languages_considered det_languages_the predet_languages_all nn_parser_ISBN det_parser_the prep_on_evaluated_languages dobj_evaluated_parser nsubj_evaluated_We rcmod_Experiments_evaluated num_Experiments_4
D07-1100	J93-2004	o	We participated in the multilingual track of the CoNLL 2007 shared task -LRB- Nivre et al. 2007 -RRB- and evaluated the system on data sets of 10 languages -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bohmova et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Csendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB-	num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bohmova_2003 nn_Bohmova_al. nn_Bohmova_et dep_Chen_Oflazer conj_and_Chen_Montemagni conj_and_Chen_Csendes conj_and_Chen_Prokopidis num_Chen_2007 conj_and_Chen_Nugues conj_and_Chen_Johansson conj_and_Chen_Marcus conj_and_Chen_Bohmova num_Chen_2003 nn_Chen_al. nn_Chen_et dep_al._2007 nn_al._et nn_al._Mart num_al._2003 nn_al._et nn_al._Aduriz dep_al._Montemagni dep_al._Csendes dep_al._Prokopidis dep_al._Nugues dep_al._Johansson dep_al._Marcus dep_al._Bohmova dep_al._Chen conj_al._al. conj_al._al. num_al._2004 nn_al._et amod_al._Hajic num_languages_10 dep_sets_al. prep_of_sets_languages nn_sets_data det_system_the prep_on_evaluated_sets dobj_evaluated_system nsubj_evaluated_We amod_Nivre_2007 dep_Nivre_al. nn_Nivre_et amod_task_shared num_task_2007 nn_task_CoNLL det_task_the prep_of_track_task amod_track_multilingual det_track_the conj_and_participated_evaluated dep_participated_Nivre prep_in_participated_track nsubj_participated_We
D07-1101	J93-2004	o	To train models we used projectivized versions of the training dependency trees .2 1We are grateful to the providers of the treebanks that constituted the data for the shared task -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bohmova et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Csendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB-	num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et dep_Johansson_Oflazer conj_and_Johansson_Montemagni conj_and_Johansson_Csendes conj_and_Johansson_Prokopidis conj_and_Johansson_2007 conj_and_Johansson_Nugues num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bohmova_2003 nn_Bohmova_al. nn_Bohmova_et num_Chen_2003 nn_Chen_al. nn_Chen_et num_Mart_2007 nn_Mart_al. nn_Mart_et num_Aduriz_2003 nn_Aduriz_al. nn_Aduriz_et num_al._2004 nn_al._et dep_Hajic_Montemagni dep_Hajic_Csendes dep_Hajic_Prokopidis dep_Hajic_2007 dep_Hajic_Nugues dep_Hajic_Johansson dep_Hajic_Marcus dep_Hajic_Bohmova dep_Hajic_Chen dep_Hajic_Mart dep_Hajic_Aduriz dep_Hajic_al. amod_task_shared det_task_the prep_for_data_task det_data_the dobj_constituted_data nsubj_constituted_that rcmod_treebanks_constituted det_treebanks_the prep_of_providers_treebanks det_providers_the dep_grateful_Hajic prep_to_grateful_providers cop_grateful_are csubj_grateful_used nn_1We_.2 dep_trees_1We nn_trees_dependency nn_trees_training det_trees_the prep_of_versions_trees amod_versions_projectivized dobj_used_versions nsubj_used_we advcl_used_train dobj_train_models aux_train_To
D07-1102	J93-2004	o	This task evaluated parsing performance on 10 languages Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian and Turkish using data originating from a wide variety of dependency treebanks and transformations of constituency-based treebanks -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bohmova et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Csendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB-	num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bohmova_2003 nn_Bohmova_al. nn_Bohmova_et dep_Chen_Oflazer conj_and_Chen_Montemagni conj_and_Chen_Csendes conj_and_Chen_Prokopidis num_Chen_2007 conj_and_Chen_Nugues conj_and_Chen_Johansson conj_and_Chen_Marcus conj_and_Chen_Bohmova num_Chen_2003 nn_Chen_al. nn_Chen_et dep_al._2007 nn_al._et nn_al._Mart num_al._2003 nn_al._et nn_al._Aduriz dep_al._Montemagni dep_al._Csendes dep_al._Prokopidis dep_al._Nugues dep_al._Johansson dep_al._Marcus dep_al._Bohmova dep_al._Chen dep_al._al. dep_al._al. num_al._2004 nn_al._et amod_al._Hajic dep_treebanks_al. amod_treebanks_constituency-based prep_of_transformations_treebanks nn_treebanks_dependency prep_of_variety_treebanks amod_variety_wide det_variety_a prep_from_originating_variety vmod_data_originating dobj_using_data conj_and_Arabic_transformations dep_Arabic_using conj_and_Arabic_Turkish conj_and_Arabic_Italian conj_and_Arabic_Hungarian conj_and_Arabic_Greek conj_and_Arabic_English conj_and_Arabic_Czech conj_and_Arabic_Chinese conj_and_Arabic_Catalan conj_and_Arabic_Basque num_languages_10 prep_on_performance_languages amod_performance_parsing dep_evaluated_transformations dep_evaluated_Turkish dep_evaluated_Italian dep_evaluated_Hungarian dep_evaluated_Greek dep_evaluated_English dep_evaluated_Czech dep_evaluated_Chinese dep_evaluated_Catalan dep_evaluated_Basque dep_evaluated_Arabic dobj_evaluated_performance nsubj_evaluated_task det_task_This ccomp_``_evaluated
D07-1111	J93-2004	o	In the multilingual parsing track participants train dependency parsers using treebanks provided for ten languages Arabic -LRB- Hajic et al. 2004 -RRB- Basque -LRB- Aduriz et al. 2003 -RRB- Catalan -LRB- Mart et al. 2007 -RRB- Chinese -LRB- Chen et al. 2003 -RRB- Czech -LRB- Bhmova et al. 2003 -RRB- English -LRB- Marcus et al. 1993 Johansson and Nugues 2007 -RRB- Greek -LRB- Prokopidis et al. 2005 -RRB- Hungarian -LRB- Czendes et al. 2005 -RRB- Italian -LRB- Montemagni et al. 2003 -RRB- and Turkish -LRB- Oflazer et al. 2003 -RRB-	dep_al._2003 nn_al._et advmod_Oflazer_al. dep_al._2003 nn_al._et advmod_Montemagni_al. dep_al._2005 nn_al._et advmod_Czendes_al. amod_Prokopidis_2005 dep_Prokopidis_al. nn_Prokopidis_et dep_Greek_Prokopidis dep_Johansson_2007 conj_and_Johansson_Nugues dep_Marcus_Nugues dep_Marcus_Johansson appos_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_Bhmova_2003 dep_Bhmova_al. nn_Bhmova_et amod_Chen_2003 dep_Chen_al. nn_Chen_et amod_Mart_2007 dep_Mart_al. nn_Mart_et appos_Catalan_Mart dep_2003_al. nn_al._et num_Aduriz_2003 dep_al._2004 nn_al._et advmod_Hajic_al. dep_Arabic_Oflazer conj_and_Arabic_Turkish dep_Arabic_Montemagni dep_Arabic_Italian dep_Arabic_Czendes appos_Arabic_Hungarian appos_Arabic_Greek appos_Arabic_Marcus dep_Arabic_English dep_Arabic_Bhmova dep_Arabic_Czech dep_Arabic_Chen conj_and_Arabic_Chinese conj_and_Arabic_Catalan appos_Arabic_Aduriz dep_Arabic_Basque dep_Arabic_Hajic dep_languages_Turkish dep_languages_Chinese dep_languages_Catalan dep_languages_Arabic num_languages_ten prep_for_provided_languages vmod_treebanks_provided dobj_using_treebanks nn_parsers_dependency xcomp_train_using dobj_train_parsers nsubj_train_participants prep_in_train_track nn_track_parsing amod_track_multilingual det_track_the
D07-1111	J93-2004	o	In the domain adaptation track participants were provided with English training data from the Wall Street Journal portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- converted to dependencies -LRB- Johansson and Nugues 2007 -RRB- to train parsers to be evaluated on material in the biological -LRB- development set -RRB- and chemical -LRB- test set -RRB- domains -LRB- Kulick et al. 2004 -RRB- and optionally on text from the CHILDES database -LRB- MacWhinney 2000 Brown 1973 -RRB-	amod_Brown_1973 dep_MacWhinney_Brown dep_MacWhinney_2000 appos_database_MacWhinney nn_database_CHILDES det_database_the prep_from_text_database prep_on_optionally_text amod_Kulick_2004 dep_Kulick_al. nn_Kulick_et amod_domains_chemical amod_domains_biological det_domains_the vmod_test_set vmod_development_set dep_biological_test conj_and_biological_chemical dep_biological_development prep_in_material_domains prep_on_evaluated_material auxpass_evaluated_be aux_evaluated_to dep_train_Kulick vmod_train_evaluated dobj_train_parsers aux_train_to amod_Johansson_2007 conj_and_Johansson_Nugues appos_dependencies_Nugues appos_dependencies_Johansson xcomp_converted_train prep_to_converted_dependencies nsubj_converted_Marcus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_Journal nn_portion_Street nn_portion_Wall det_portion_the prep_from_data_portion nn_data_training amod_data_English conj_and_provided_optionally ccomp_provided_converted prep_with_provided_data auxpass_provided_were nsubjpass_provided_participants prep_in_provided_track nn_track_adaptation nn_track_domain det_track_the
D07-1112	J93-2004	o	We were given around 15K sentences of labeled text from the Wall Street Journal -LRB- WSJ -RRB- -LRB- Marcus et al. 1993 Johansson and Nugues 2007 -RRB- as well as 200K unlabeled sentences	amod_sentences_unlabeled nn_sentences_200K conj_and_Johansson_sentences appos_Johansson_2007 conj_and_Johansson_Nugues dep_Marcus_sentences dep_Marcus_Nugues dep_Marcus_Johansson appos_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the prep_from_text_Journal amod_text_labeled prep_of_sentences_text num_sentences_15K dep_given_Marcus prep_around_given_sentences auxpass_given_were nsubjpass_given_We
D07-1112	J93-2004	o	The annotation guidelines for the Penn Treebank flattened noun phrases to simplify annotation -LRB- Marcus et al. 1993 -RRB- so there is no complex structure to NPs	prep_to_structure_NPs amod_structure_complex neg_structure_no nsubj_is_structure expl_is_there amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dobj_simplify_annotation aux_simplify_to nn_phrases_noun parataxis_flattened_is advmod_flattened_so dep_flattened_Marcus vmod_flattened_simplify dobj_flattened_phrases nsubj_flattened_guidelines nn_Treebank_Penn det_Treebank_the prep_for_guidelines_Treebank nn_guidelines_annotation det_guidelines_The ccomp_``_flattened
D07-1119	J93-2004	o	The following treebanks were used for training the parser -LRB- Aduriz et al. 2003 Bhmov et al. 2003 Chen et al. 2003 Haji et al. 2004 Marcus et al. 1993 Mart et al. 2002 Montemagni et al. 2003 Oflazer et al. 2003 Prokopidis et al. 2005 Csendes et al. 2005 -RRB-	num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et dep_al._2003 nn_al._et nn_al._Montemagni num_Mart_2002 nn_Mart_al. nn_Mart_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Haji_2004 nn_Haji_al. nn_Haji_et num_Chen_2003 nn_Chen_al. nn_Chen_et num_Bhmov_2003 nn_Bhmov_al. nn_Bhmov_et dep_Aduriz_Csendes dep_Aduriz_Prokopidis dep_Aduriz_Oflazer dep_Aduriz_al. dep_Aduriz_Mart dep_Aduriz_Marcus dep_Aduriz_Haji dep_Aduriz_Chen dep_Aduriz_Bhmov appos_Aduriz_2003 dep_Aduriz_al. nn_Aduriz_et det_parser_the dobj_training_parser dep_used_Aduriz prepc_for_used_training auxpass_used_were nsubjpass_used_treebanks amod_treebanks_following det_treebanks_The
D07-1121	J93-2004	o	-LRB- 1993 -RRB- Johansson and Nugues -LRB- 2007 -RRB- Prokopidis et al.	nn_al._et nn_al._Prokopidis appos_Nugues_2007 dep_Johansson_al. conj_and_Johansson_Nugues dep_Johansson_1993
D07-1122	J93-2004	o	We took part the Multilingual Track of all ten languages provided by the CoNLL-2007 shared task organizers -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bohmova et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Csendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB-	num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bohmova_2003 nn_Bohmova_al. nn_Bohmova_et dep_Chen_Oflazer conj_and_Chen_Montemagni conj_and_Chen_Csendes conj_and_Chen_Prokopidis num_Chen_2007 conj_and_Chen_Nugues conj_and_Chen_Johansson conj_and_Chen_Marcus conj_and_Chen_Bohmova num_Chen_2003 nn_Chen_al. nn_Chen_et dep_al._2007 nn_al._et nn_al._Mart num_al._2003 nn_al._et nn_al._Aduriz dep_al._Montemagni dep_al._Csendes dep_al._Prokopidis dep_al._Nugues dep_al._Johansson dep_al._Marcus dep_al._Bohmova dep_al._Chen conj_al._al. conj_al._al. num_al._2004 nn_al._et amod_al._Hajic dep_organizers_al. nn_organizers_task dobj_shared_organizers nsubj_shared_Track det_CoNLL-2007_the agent_provided_CoNLL-2007 vmod_languages_provided num_languages_ten det_languages_all prep_of_Track_languages amod_Track_Multilingual det_Track_the dep_took_shared dobj_took_part nsubj_took_We
D07-1124	J93-2004	o	5 Results and Discussion The system with online learning and Nivres parsing algorithm was trained on the data released by CoNLL Shared Task Organizers for all the ten languages -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bohmova et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Csendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB-	num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bohmova_2003 nn_Bohmova_al. nn_Bohmova_et dep_Chen_Oflazer conj_and_Chen_Montemagni conj_and_Chen_Csendes conj_and_Chen_Prokopidis num_Chen_2007 conj_and_Chen_Nugues conj_and_Chen_Johansson conj_and_Chen_Marcus conj_and_Chen_Bohmova num_Chen_2003 nn_Chen_al. nn_Chen_et dep_al._2007 nn_al._et nn_al._Mart num_al._2003 nn_al._et nn_al._Aduriz dep_al._Montemagni dep_al._Csendes dep_al._Prokopidis dep_al._Nugues dep_al._Johansson dep_al._Marcus dep_al._Bohmova dep_al._Chen dep_al._al. dep_al._al. appos_al._2004 nn_al._et amod_al._Hajic dep_languages_al. num_languages_ten det_languages_the predet_languages_all nn_Organizers_Task prep_for_Shared_languages dobj_Shared_Organizers agent_released_CoNLL vmod_data_released det_data_the dep_trained_Shared prep_on_trained_data auxpass_trained_was nsubjpass_trained_Discussion nsubjpass_trained_Results amod_algorithm_parsing nn_algorithm_Nivres conj_and_learning_algorithm amod_learning_online prep_with_system_algorithm prep_with_system_learning det_system_The dep_Discussion_system conj_and_Results_Discussion num_Results_5
D07-1125	J93-2004	o	Building heavily on the ideas of History-based parsing -LRB- Black et al. 1993 Nivre 2006 -RRB- training the parser means essentially running the parsing algorithms in a learning mode on the data in order to gather training instances for the memory-based learner	amod_learner_memory-based det_learner_the nn_instances_training prep_for_gather_learner dobj_gather_instances aux_gather_to dep_gather_order mark_gather_in det_data_the prep_on_mode_data nn_mode_learning det_mode_a nn_algorithms_parsing det_algorithms_the advcl_running_gather prep_in_running_mode dobj_running_algorithms advmod_running_essentially xcomp_means_running csubj_means_training dep_means_Building det_parser_the dobj_training_parser dep_Nivre_2006 dep_al._Nivre num_al._1993 nn_al._et amod_al._Black dep_parsing_al. amod_parsing_History-based prep_of_ideas_parsing det_ideas_the prep_on_Building_ideas advmod_Building_heavily
D07-1126	J93-2004	o	The pchemtb-closed shared task -LRB- Marcus et al. 1993 Johansson and Nugues 2007 Kulick et al. 2004 -RRB- is used to illustrate our models	poss_models_our dobj_illustrate_models aux_illustrate_to xcomp_used_illustrate auxpass_used_is nsubjpass_used_Nugues nsubjpass_used_Johansson num_Kulick_2004 nn_Kulick_al. nn_Kulick_et dep_Johansson_Kulick num_Johansson_2007 conj_and_Johansson_Nugues parataxis_Marcus_used appos_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_task_Marcus amod_task_shared amod_task_pchemtb-closed det_task_The dep_``_task
D07-1126	J93-2004	o	3 Experimental Results and Discussion We test our parsing models on the CONLL-2007 -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bohmova et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Csendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB- data set on various languages including Arabic Basque Catalan Chinese English Italian Hungarian and Turkish	conj_and_Arabic_Turkish conj_and_Arabic_Hungarian conj_and_Arabic_Italian conj_and_Arabic_English conj_and_Arabic_Chinese conj_and_Arabic_Catalan conj_and_Arabic_Basque prep_including_languages_Turkish prep_including_languages_Hungarian prep_including_languages_Italian prep_including_languages_English prep_including_languages_Chinese prep_including_languages_Catalan prep_including_languages_Basque prep_including_languages_Arabic amod_languages_various prep_on_set_languages vmod_data_set dep_Oflazer_data num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bohmova_2003 nn_Bohmova_al. nn_Bohmova_et dep_Chen_Oflazer conj_and_Chen_Montemagni conj_and_Chen_Csendes conj_and_Chen_Prokopidis num_Chen_2007 conj_and_Chen_Nugues conj_and_Chen_Johansson conj_and_Chen_Marcus conj_and_Chen_Bohmova num_Chen_2003 nn_Chen_al. nn_Chen_et nn_al._et nn_al._Mart nn_al._et nn_al._Aduriz dep_al._Montemagni dep_al._Csendes dep_al._Prokopidis dep_al._Nugues dep_al._Johansson dep_al._Marcus dep_al._Bohmova dep_al._Chen appos_al._2007 dep_al._al. num_al._2003 dep_al._al. num_al._2004 nn_al._et amod_al._Hajic det_CONLL-2007_the nn_models_parsing poss_models_our prep_on_test_CONLL-2007 dobj_test_models nsubj_test_We dep_Discussion_al. rcmod_Discussion_test conj_and_Results_Discussion amod_Results_Experimental num_Results_3
D07-1127	J93-2004	o	3 Experiments and Results All experiments were conducted on the treebanks provided in the shared task -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bhmov et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Csendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB-	num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Csendes_2005 nn_Csendes_al. nn_Csendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bhmov_2003 nn_Bhmov_al. nn_Bhmov_et dep_Chen_Oflazer conj_and_Chen_Montemagni conj_and_Chen_Csendes conj_and_Chen_Prokopidis num_Chen_2007 conj_and_Chen_Nugues conj_and_Chen_Johansson conj_and_Chen_Marcus conj_and_Chen_Bhmov num_Chen_2003 nn_Chen_al. nn_Chen_et dep_al._2007 nn_al._et nn_al._Mart num_al._2003 nn_al._et nn_al._Aduriz dep_al._Montemagni dep_al._Csendes dep_al._Prokopidis dep_al._Nugues dep_al._Johansson dep_al._Marcus dep_al._Bhmov dep_al._Chen dep_al._al. dep_al._al. num_al._2004 nn_al._et amod_al._Hajic dep_task_al. amod_task_shared det_task_the prep_in_provided_task vmod_treebanks_provided det_treebanks_the prep_on_conducted_treebanks auxpass_conducted_were nsubjpass_conducted_experiments det_experiments_All rcmod_Experiments_conducted conj_and_Experiments_Results num_Experiments_3
D07-1128	J93-2004	o	We have achieved average results in the CoNLL domain adaptation track open submission -LRB- Marcus et al. 1993 Johansson and Nugues 2007 Kulick et al. 2004 MacWhinney 2000 Brown 1973 -RRB-	amod_Brown_1973 dep_MacWhinney_Brown conj_MacWhinney_2000 num_Kulick_2004 nn_Kulick_al. nn_Kulick_et conj_and_Johansson_MacWhinney conj_and_Johansson_Kulick num_Johansson_2007 conj_and_Johansson_Nugues dep_Marcus_MacWhinney dep_Marcus_Kulick dep_Marcus_Nugues dep_Marcus_Johansson amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_submission_open dep_track_submission nn_track_adaptation nn_track_domain nn_track_CoNLL det_track_the prep_in_results_track amod_results_average dep_achieved_Marcus dobj_achieved_results aux_achieved_have nsubj_achieved_We
D07-1128	J93-2004	o	We use a hand-written competence grammar combined with performance-driven disambiguation obtained from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_obtained_Treebank vmod_disambiguation_obtained amod_disambiguation_performance-driven prep_with_combined_disambiguation vmod_grammar_combined nn_grammar_competence amod_grammar_hand-written det_grammar_a dep_use_Marcus dobj_use_grammar nsubj_use_We
D07-1129	J93-2004	o	4 Experiments Our experiments were conducted on CoNLL-2007 shared task domain adaptation track -LRB- Nivre et al. 2007 -RRB- using treebanks -LRB- Marcus et al. 1993 Johansson and Nugues 2007 Kulick et al. 2004 -RRB-	num_Kulick_2004 nn_Kulick_al. nn_Kulick_et dep_Johansson_Kulick num_Johansson_2007 conj_and_Johansson_Nugues dep_Marcus_Nugues dep_Marcus_Johansson amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_using_Marcus dobj_using_treebanks amod_Nivre_2007 dep_Nivre_al. nn_Nivre_et appos_track_Nivre nn_track_adaptation nn_track_domain nn_track_task xcomp_shared_using dobj_shared_track nsubj_shared_CoNLL-2007 mark_shared_on advcl_conducted_shared auxpass_conducted_were nsubjpass_conducted_experiments poss_experiments_Our dep_Experiments_conducted num_Experiments_4
D07-1131	J93-2004	o	In this year CoNLL-2007 shared task -LRB- Nivre et al. 2007 -RRB- focuses on multilingual dependency parsing based on ten different languages -LRB- Hajic et al. 2004 Aduriz et al. 2003 Mart et al. 2007 Chen et al. 2003 Bhmova et al. 2003 Marcus et al. 1993 Johansson and Nugues 2007 Prokopidis et al. 2005 Czendes et al. 2005 Montemagni et al. 2003 Oflazer et al. 2003 -RRB- and domain adaptation for English -LRB- Marcus et al. 1993 Johansson and Nugues 2007 Kulick et al. 2004 MacWhinney 2000 Brown 1973 -RRB- without taking the languagespecific knowledge into consideration	amod_knowledge_languagespecific det_knowledge_the prep_into_taking_consideration dobj_taking_knowledge dep_Brown_1973 prepc_without_MacWhinney_taking dep_MacWhinney_Brown conj_MacWhinney_2000 num_Kulick_2004 nn_Kulick_al. nn_Kulick_et dep_Johansson_MacWhinney conj_and_Johansson_Kulick num_Johansson_2007 conj_and_Johansson_Nugues dep_Marcus_Kulick dep_Marcus_Nugues dep_Marcus_Johansson amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_adaptation_domain num_Oflazer_2003 nn_Oflazer_al. nn_Oflazer_et num_Montemagni_2003 nn_Montemagni_al. nn_Montemagni_et num_Czendes_2005 nn_Czendes_al. nn_Czendes_et num_Prokopidis_2005 nn_Prokopidis_al. nn_Prokopidis_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Bhmova_2003 nn_Bhmova_al. nn_Bhmova_et appos_Chen_Marcus prep_for_Chen_English conj_and_Chen_adaptation conj_and_Chen_Oflazer conj_and_Chen_Montemagni conj_and_Chen_Czendes conj_and_Chen_Prokopidis num_Chen_2007 conj_and_Chen_Nugues conj_and_Chen_Johansson conj_and_Chen_Marcus conj_and_Chen_Bhmova num_Chen_2003 nn_Chen_al. nn_Chen_et dep_al._2007 nn_al._et nn_al._Mart num_al._2003 nn_al._et nn_al._Aduriz dep_al._adaptation dep_al._Oflazer dep_al._Montemagni dep_al._Czendes dep_al._Prokopidis dep_al._Nugues dep_al._Johansson dep_al._Marcus dep_al._Bhmova dep_al._Chen dep_al._al. dep_al._al. appos_al._2004 nn_al._et amod_al._Hajic dep_languages_al. amod_languages_different num_languages_ten nn_parsing_dependency amod_parsing_multilingual prep_based_on_focuses_languages prep_on_focuses_parsing amod_Nivre_2007 dep_Nivre_al. nn_Nivre_et dep_shared_focuses dep_shared_Nivre dobj_shared_task nsubj_shared_CoNLL-2007 prep_in_shared_year det_year_this
D08-1008	J93-2004	o	Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- although there has also been much research on the use of shallow syntax -LRB- Carreras and Mrquez 2004 -RRB- in SRL	dep_Carreras_2004 conj_and_Carreras_Mrquez appos_syntax_Mrquez appos_syntax_Carreras amod_syntax_shallow prep_in_use_SRL prep_of_use_syntax det_use_the prep_on_research_use amod_research_much cop_research_been advmod_research_also aux_research_has expl_research_there mark_research_although amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the pobj_in_Treebank pcomp_as_in amod_syntax_constituent advcl_used_research dep_used_Marcus prep_used_as dobj_used_syntax aux_used_have nsubj_used_systems amod_analysis_role-semantic amod_analysis_automatic prep_for_systems_analysis amod_systems_Most
D08-1050	J93-2004	o	1 Introduction Most state-of-the-art wide-coverage parsers are based on the Penn Treebank -LRB- Marcus et al. 1993 -RRB- making such parsers highly tuned to newspaper text	nn_text_newspaper prep_to_tuned_text advmod_tuned_highly nsubj_tuned_parsers amod_parsers_such ccomp_making_tuned amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the xcomp_based_making prep_on_based_Treebank auxpass_based_are nsubjpass_based_parsers nn_parsers_wide-coverage amod_parsers_state-of-the-art amod_parsers_Most nn_parsers_Introduction num_parsers_1 ccomp_``_based
D08-1056	J93-2004	o	These categories were automatically generated using the labeled parses in Penn Treebank -LRB- Marcus et al. 1993 -RRB- and the labeled semantic roles of PropBank -LRB- Kingsbury et al. 2002 -RRB-	amod_Kingsbury_2002 dep_Kingsbury_al. nn_Kingsbury_et dep_PropBank_Kingsbury prep_of_roles_PropBank amod_roles_semantic amod_roles_labeled det_roles_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn prep_in_parses_Treebank dep_labeled_parses det_labeled_the vmod_using_labeled conj_and_generated_roles dep_generated_Marcus xcomp_generated_using advmod_generated_automatically auxpass_generated_were nsubjpass_generated_categories det_categories_These
D08-1070	J93-2004	o	The model was trained on sections 221 from the English Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn nn_Treebank_English det_Treebank_the prep_from_sections_Treebank num_sections_221 dep_trained_Marcus prep_on_trained_sections auxpass_trained_was nsubjpass_trained_model det_model_The
D08-1071	J93-2004	o	4.1 Data Sets Our results are based on syntactic data drawn from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- specifically the portion used by CoNLL 2000 shared task -LRB- Tjong Kim Sang and Buchholz 2000 -RRB-	dep_Sang_2000 conj_and_Sang_Buchholz nn_Sang_Kim nn_Sang_Tjong appos_task_Buchholz appos_task_Sang amod_task_shared num_task_2000 nn_task_CoNLL agent_used_task vmod_portion_used det_portion_the pobj_specifically_portion amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_drawn_Treebank vmod_data_drawn amod_data_syntactic prep_on_based_data auxpass_based_are nsubjpass_based_results poss_results_Our prep_Sets_specifically dep_Sets_Marcus ccomp_Sets_based nsubj_Sets_Data num_Data_4.1 ccomp_``_Sets
D08-1071	J93-2004	o	We use 3500 sentences from CoNLL -LRB- Tjong Kim Sang and De Meulder 2003 -RRB- as the NER data and section 20-23 of the WSJ -LRB- Marcus et al. 1993 Ramshaw and Marcus 1995 -RRB- as the POS/chunk data -LRB- 8936 sentences -RRB-	num_sentences_8936 appos_data_sentences nn_data_POS/chunk det_data_the num_Ramshaw_1995 conj_and_Ramshaw_Marcus dep_Marcus_Marcus dep_Marcus_Ramshaw amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_WSJ_Marcus det_WSJ_the nn_20-23_section prep_as_data_data prep_of_data_WSJ conj_and_data_20-23 nn_data_NER det_data_the amod_Meulder_2003 nn_Meulder_De conj_and_Sang_Meulder nn_Sang_Kim nn_Sang_Tjong dep_CoNLL_Meulder dep_CoNLL_Sang num_sentences_3500 prep_as_use_20-23 prep_as_use_data prep_from_use_CoNLL dobj_use_sentences nsubj_use_We
D08-1091	J93-2004	o	Set Test Set ENGLISH-WSJ Sections Section 22 Section 23 -LRB- Marcus et al. 1993 -RRB- 2-21 ENGLISH-BROWN see 10 % of 10 % of the -LRB- Francis et al. 2002 -RRB- ENGLISH-WSJ the data6 the data6 FRENCH7 Sentences Sentences Sentences -LRB- Abeille et al. 2000 -RRB- 1-18 ,609 18,610-19 ,609 19,609-20 ,610 GERMAN Sentences Sentences Sentences -LRB- Skut et al. 1997 -RRB- 1-18 ,602 18,603-19 ,602 19,603-20 ,602 Table 1 Corpora and standard experimental setups	amod_setups_experimental amod_setups_standard conj_and_Corpora_setups num_Table_1 num_Table_,602 num_Table_19,603-20 dep_Table_,602 number_,602_18,603-19 dep_,602_Table number_,602_1-18 dep_Skut_setups dep_Skut_Corpora dep_Skut_,602 dep_Skut_1997 dep_Skut_al. nn_Skut_et dep_Sentences_Skut nn_Sentences_Sentences nn_Sentences_Sentences nn_Sentences_GERMAN num_Sentences_,610 dep_,610_19,609-20 dep_,610_,609 dep_,610_Abeille dep_19,609-20_,609 number_,609_18,610-19 number_,609_1-18 dep_Abeille_2000 dep_Abeille_al. nn_Abeille_et dep_Sentences_Sentences nn_Sentences_Sentences nn_Sentences_Sentences nn_Sentences_FRENCH7 nn_Sentences_data6 det_Sentences_the dep_data6_Sentences det_data6_the appos_ENGLISH-WSJ_Francis det_ENGLISH-WSJ_the dep_2002_al. num_Francis_2002 nn_Francis_et prep_of_%_ENGLISH-WSJ num_%_10 dep_%_data6 prep_of_%_% num_%_10 dobj_see_% nsubj_see_ENGLISH-BROWN num_ENGLISH-BROWN_2-21 dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_Section_23 num_Section_22 nn_Section_Section nn_Section_Sections nn_Section_ENGLISH-WSJ dep_Set_see dep_Set_Marcus dobj_Set_Section nsubj_Set_Test dep_Set_Set
D08-1093	J93-2004	o	The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks such as WSJ -LRB- Marcus et al. 1993 -RRB- and assume that the accuracy measure will carry over to the domains of interest	prep_of_domains_interest det_domains_the prep_to_carry_domains prt_carry_over aux_carry_will nsubj_carry_measure mark_carry_that nn_measure_accuracy det_measure_the ccomp_assume_carry amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_WSJ_Marcus amod_treebanks_existing prep_such_as_parser_WSJ prep_on_parser_treebanks det_parser_a prep_of_performance_parser det_performance_the conj_and_measure_assume dobj_measure_performance aux_measure_to xcomp_is_assume xcomp_is_measure nsubj_is_recipe amod_scale_large det_scale_a prep_on_used_scale advmod_used_currently auxpass_used_is nsubjpass_used_that rcmod_recipe_used amod_recipe_other det_recipe_The ccomp_``_is
D08-1105	J93-2004	o	Building on the annotations from the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- the project added several new layers of semantic annotations such as coreference information word senses etc. In its first release -LRB- LDC2007T21 -RRB- through the Linguistic Data Consortium -LRB- LDC -RRB- the project manually sense-tagged more than 40,000 examples belonging to hundreds of noun and verb types with an ITA of 90 % based on a coarse-grained sense inventory where each word has an average of only 3.2 senses	num_senses_3.2 quantmod_3.2_only prep_of_average_senses det_average_an dobj_has_average nsubj_has_word advmod_has_where det_word_each rcmod_inventory_has nn_inventory_sense amod_inventory_coarse-grained det_inventory_a prep_on_based_inventory num_%_90 prep_of_ITA_% det_ITA_an prep_with_verb_ITA dobj_verb_types nsubj_verb_project prep_of_hundreds_noun prep_to_belonging_hundreds vmod_examples_belonging num_examples_40,000 quantmod_40,000_than mwe_than_more vmod_sense-tagged_based conj_and_sense-tagged_verb dobj_sense-tagged_examples advmod_sense-tagged_manually nsubj_sense-tagged_project det_project_the appos_Consortium_LDC nn_Consortium_Data nn_Consortium_Linguistic det_Consortium_the prep_through_release_Consortium appos_release_LDC2007T21 amod_release_first poss_release_its nn_senses_word dep_information_etc. appos_information_senses nn_information_coreference prep_such_as_annotations_information amod_annotations_semantic prep_of_layers_annotations amod_layers_new amod_layers_several parataxis_added_verb parataxis_added_sense-tagged prep_in_added_release dobj_added_layers nsubj_added_project tmod_added_Building det_project_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the prep_from_annotations_portion det_annotations_the appos_Building_Marcus prep_on_Building_annotations
D09-1015	J93-2004	o	We removed all but the first two characters of each POS tag resulting in a set of 57 tags which more closely resembles that of the Penn TreeBank -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_TreeBank_Penn det_TreeBank_the prep_of_that_TreeBank xcomp_resembles_that advmod_resembles_closely nsubj_resembles_which advmod_closely_more rcmod_tags_resembles num_tags_57 prep_of_set_tags det_set_a prep_in_resulting_set nn_tag_POS det_tag_each prep_of_characters_tag num_characters_two amod_characters_first det_characters_the conj_but_all_characters dep_removed_Marcus vmod_removed_resulting dobj_removed_characters dobj_removed_all nsubj_removed_We
D09-1015	J93-2004	o	Our trees look just like syntactic constituency trees such as those in the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- 141 ROOT PROT PROT NN PEBP2 PROT NN alpha NN A1 PROT NN alpha NN B1 CC and PROT NN alpha NN B2 NNS proteins VBD bound DT the DNA PROT NN PEBP2 NN site IN within DT the DNA NN mouse PROT NN GM-CSF NN promoter Figure 1 An example of our tree representation over nested named entities	amod_entities_named amod_entities_nested prep_over_representation_entities nn_representation_tree poss_representation_our prep_of_example_representation det_example_An dep_Figure_example num_Figure_1 nn_promoter_NN nn_promoter_GM-CSF nn_promoter_NN nn_promoter_PROT nn_promoter_mouse nn_promoter_NN nn_promoter_DNA det_promoter_the pobj_within_DT pcomp_IN_within nn_site_NN nn_site_PEBP2 nn_site_NN nn_site_PROT nn_site_DNA det_site_the nn_site_DT dep_bound_promoter prep_bound_IN dobj_bound_site nsubj_bound_VBD rcmod_proteins_bound nn_proteins_NNS nn_proteins_B2 nn_proteins_NN nn_proteins_alpha nn_proteins_NN nn_proteins_PROT nn_proteins_CC conj_and_CC_PROT conj_B1_proteins nn_B1_NN nn_B1_alpha nn_B1_NN nn_B1_PROT dep_A1_Figure appos_A1_B1 nn_A1_NN nn_A1_alpha nn_A1_NN nn_A1_PROT nn_A1_PEBP2 nn_A1_NN nn_A1_PROT nn_A1_PROT nn_A1_ROOT num_A1_141 dep_,_A1 amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the prep_in_those_TreeBank dep_trees_Marcus prep_such_as_trees_those nn_trees_constituency amod_trees_syntactic prep_like_look_trees advmod_look_just nsubj_look_trees poss_trees_Our ccomp_``_look
D09-1031	J93-2004	o	For -LRB- 1 -RRB- the morphemes and labels for our task are -LRB- 2 -RRB- kita NEG tINC inE1S chabe VT j SC laj PREP inA1S yol S j SC iin PRON We also consider POS-tagging for Danish Dutch English and Swedish the English is from sections 00-05 -LRB- as training set -RRB- and 19-21 -LRB- as development set -RRB- of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and the other languages are from the CoNLL-X dependency parsing shared task -LRB- Buchholz and Marsi 2006 -RRB- .1 We split the original training data into training and development sets	nn_sets_development nn_sets_training conj_and_training_development prep_into_data_sets nn_data_training amod_data_original det_data_the dobj_split_data nsubj_split_We rcmod_.1_split dep_Buchholz_2006 conj_and_Buchholz_Marsi dep_task_.1 dep_task_Marsi dep_task_Buchholz dobj_shared_task nn_parsing_dependency nn_parsing_CoNLL-X det_parsing_the dep_are_shared prep_from_are_parsing nsubj_are_languages amod_languages_other det_languages_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the nn_set_development prep_of_19-21_Treebank prep_as_19-21_set nn_set_training pobj_as_set num_sections_00-05 dep_is_Marcus conj_and_is_19-21 dep_is_as prep_from_is_sections nsubj_is_English det_English_the conj_and_Danish_Swedish conj_and_Danish_English conj_and_Danish_Dutch prep_for_POS-tagging_Swedish prep_for_POS-tagging_English prep_for_POS-tagging_Dutch prep_for_POS-tagging_Danish dobj_consider_POS-tagging advmod_consider_also nsubj_consider_We rcmod_PRON_consider nn_PRON_iin nn_PRON_SC nn_PRON_j dep_S_PRON nn_S_yol nn_S_inA1S nn_S_PREP nn_S_laj nn_S_SC nn_S_j dep_VT_S nn_VT_chabe nn_VT_inE1S nn_VT_tINC nn_VT_NEG nn_VT_kita dep_VT_2 conj_and_are_are conj_and_are_19-21 conj_and_are_is conj_and_are_VT nsubj_are_labels nsubj_are_morphemes dep_are_1 mark_are_For poss_task_our prep_for_morphemes_task conj_and_morphemes_labels det_morphemes_the advcl_``_are advcl_``_is advcl_``_VT advcl_``_are
D09-1034	J93-2004	o	The modified version of the Roark parser trained on the Brown Corpus section of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- was used to parse the different narratives and produce the word by word measures	nn_measures_word det_word_the prep_by_produce_measures dobj_produce_word nsubj_produce_version amod_narratives_different det_narratives_the dobj_parse_narratives aux_parse_to conj_and_used_produce xcomp_used_parse auxpass_used_was nsubjpass_used_version amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_section_Treebank nn_section_Corpus amod_section_Brown det_section_the prep_on_trained_section nn_parser_Roark det_parser_the dep_version_Marcus vmod_version_trained prep_of_version_parser amod_version_modified det_version_The
D09-1047	J93-2004	o	The PropBank corpus adds a semantic layer to parse trees from the Wall Street Journal section of the Penn Treebank II corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_II nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_of_section_corpus nn_section_Journal nn_section_Street nn_section_Wall det_section_the prep_from_parse_section dobj_parse_trees aux_parse_to vmod_layer_parse amod_layer_semantic det_layer_a dep_adds_Marcus dobj_adds_layer nsubj_adds_corpus nn_corpus_PropBank det_corpus_The
D09-1059	J93-2004	p	In the statistical NLP community the most widely used grammatical resource is the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the cop_Treebank_is nsubj_Treebank_resource prep_in_Treebank_community amod_resource_grammatical amod_resource_used det_resource_the advmod_used_widely advmod_used_most nn_community_NLP amod_community_statistical det_community_the
D09-1060	J93-2004	o	For English we used the Penn Treebank -LRB- Marcus et al. 1993 -RRB- in our experiments and the tool Penn2Malt7 to convert the data into dependency structures using a standard set of head rules -LRB- Yamada and Matsumoto 2003 -RRB-	dep_Yamada_2003 conj_and_Yamada_Matsumoto appos_rules_Matsumoto appos_rules_Yamada nn_rules_head prep_of_set_rules amod_set_standard det_set_a dobj_using_set nn_structures_dependency det_data_the prep_into_convert_structures dobj_convert_data aux_convert_to vmod_Penn2Malt7_using vmod_Penn2Malt7_convert nn_Penn2Malt7_tool det_Penn2Malt7_the poss_experiments_our amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the conj_and_used_Penn2Malt7 prep_in_used_experiments dobj_used_Treebank nsubj_used_we prep_for_used_English
D09-1076	J93-2004	o	Though this model uses trees in the formal sense it does not create Penn Treebank -LRB- Marcus et al. 1993 -RRB- style linguistic trees but uses only one non-terminal label -LRB- X -RRB- to create those trees using six simple rule structures	nn_structures_rule amod_structures_simple num_structures_six dobj_using_structures det_trees_those xcomp_create_using dobj_create_trees aux_create_to appos_label_X amod_label_non-terminal num_label_one quantmod_one_only vmod_uses_create dobj_uses_label nsubj_uses_it amod_trees_linguistic nn_trees_style appos_trees_Marcus nn_trees_Treebank dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn conj_but_create_uses dobj_create_trees neg_create_not aux_create_does nsubj_create_it advcl_create_uses amod_sense_formal det_sense_the prep_in_uses_sense dobj_uses_trees nsubj_uses_model mark_uses_Though det_model_this
D09-1088	J93-2004	o	1 Introduction Parsing technology has come a long way since Charniak -LRB- 1996 -RRB- demonstrated that a simple treebank PCFG performs better than any other parser -LRB- with F175 accuracy -RRB- on parsing the WSJ Penn treebank -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_treebank_Penn nn_treebank_WSJ det_treebank_the dobj_parsing_treebank nn_accuracy_F175 prepc_on_parser_parsing prep_with_parser_accuracy amod_parser_other det_parser_any prep_than_better_parser acomp_performs_better nsubj_performs_PCFG mark_performs_that nn_PCFG_treebank amod_PCFG_simple det_PCFG_a dep_demonstrated_Marcus ccomp_demonstrated_performs csubj_demonstrated_come appos_Charniak_1996 prep_since_way_Charniak amod_way_long det_way_a dobj_come_way aux_come_has nsubj_come_technology nn_technology_Parsing nn_technology_Introduction num_technology_1
D09-1126	J93-2004	o	HockenmaierandSteedman -LRB- 2007 -RRB- showedthat a CCG corpus could be created by adapting the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dobj_adapting_Treebank agent_created_adapting auxpass_created_be aux_created_could nsubjpass_created_corpus nn_corpus_CCG det_corpus_a appos_showedthat_Marcus rcmod_showedthat_created nn_showedthat_HockenmaierandSteedman appos_HockenmaierandSteedman_2007
D09-1161	J93-2004	o	Averaged Perceptron Algorithm 5 Experiments We evaluate our method on both Chinese and English syntactic parsing task with the standard division on Chinese Penn Treebank Version 5.0 and WSJ English Treebank 3.0 -LRB- Marcus et al. 1993 -RRB- as shown in Table 1	num_Table_1 prep_in_shown_Table mark_shown_as dep_1993_al. nn_al._et num_Marcus_1993 dep_Treebank_shown appos_Treebank_Marcus num_Treebank_3.0 dep_English_Treebank dep_WSJ_English num_Version_5.0 nn_Version_Treebank nn_Version_Penn amod_Version_Chinese conj_and_division_WSJ prep_on_division_Version amod_division_standard det_division_the prep_with_task_WSJ prep_with_task_division nn_task_parsing nn_task_syntactic amod_task_English amod_task_Chinese conj_and_Chinese_English preconj_Chinese_both prep_on_method_task poss_method_our dobj_evaluate_method nsubj_evaluate_We rcmod_Experiments_evaluate num_Experiments_5 nn_Experiments_Algorithm nn_Experiments_Perceptron nn_Experiments_Averaged
E06-1015	J93-2004	o	4.1 Experimental Set-up We used two different corpora PropBank -LRB- www.cis.upenn.edu/ace -RRB- along with PennTree bank 2 -LRB- Marcus et al. 1993 -RRB- and FrameNet	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_bank_2 nn_bank_PennTree conj_and_PropBank_FrameNet dep_PropBank_Marcus pobj_PropBank_bank prepc_along_with_PropBank_with appos_PropBank_www.cis.upenn.edu/ace dep_corpora_FrameNet dep_corpora_PropBank amod_corpora_different num_corpora_two dobj_used_corpora nsubj_used_We rcmod_Set-up_used amod_Set-up_Experimental num_Set-up_4.1 dep_``_Set-up
E06-1034	J93-2004	o	For example in the WSJ corpus part of the Penn Treebank 3 release -LRB- Marcus et al. 1993 -RRB- the string in -LRB- 1 -RRB- is a variation 12-gram since off is a variation nucleus that in one corpus occurrence is tagged as a preposition -LRB- IN -RRB- while in another it is tagged as a particle -LRB- RP -RRB-	appos_particle_RP det_particle_a prep_as_tagged_particle auxpass_tagged_is nsubjpass_tagged_it dep_tagged_another mark_tagged_in mark_tagged_while dep_preposition_IN det_preposition_a prep_as_tagged_preposition auxpass_tagged_is prep_in_tagged_occurrence nsubjpass_tagged_that nn_occurrence_corpus num_occurrence_one advcl_nucleus_tagged rcmod_nucleus_tagged nn_nucleus_variation det_nucleus_a cop_nucleus_is advmod_nucleus_off dep_since_nucleus prep_12-gram_since nn_12-gram_variation det_12-gram_a nsubj_is_12-gram dep_is_1 prepc_in_string_is det_string_the dep_string_part prep_in_string_corpus prep_for_string_example amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_release_3 nn_release_Treebank nn_release_Penn det_release_the dep_part_Marcus prep_of_part_release nn_corpus_WSJ det_corpus_the
E09-1033	J93-2004	o	287 System Train + base Test + base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 -LRB- 5 trials/fold -RRB- 3 Contrastive 88.82 0.93 88.55 0.66 -LRB- greedy selection -RRB- Table 1 Average F1 of 7-way cross-validation To generate the alignments we used Model 4 -LRB- Brown et al. 1993 -RRB- as implemented in GIZA + + -LRB- Och and Ney 2003 -RRB-	num_Och_2003 conj_and_Och_Ney appos_+_Ney appos_+_Och conj_+_GIZA_+ prep_in_implemented_+ prep_in_implemented_GIZA mark_implemented_as amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_4 advcl_used_implemented dobj_used_Model nsubj_used_we nsubj_used_base nsubj_used_Test nsubj_used_Train det_alignments_the dobj_generate_alignments aux_generate_To amod_cross-validation_7-way vmod_F1_generate prep_of_F1_cross-validation amod_F1_Average num_Table_1 dep_Table_0.66 amod_selection_greedy appos_0.66_selection dep_0.66_88.55 dep_88.55_0.93 number_0.93_88.82 dep_Contrastive_F1 dep_Contrastive_Table dep_3_Contrastive num_trials/fold_5 dep_0.56_3 dep_0.56_trials/fold number_0.56_88.45 dep_0.56_0.82 number_0.82_88.70 dep_Contrastive_0.56 amod_2_Contrastive dep_87.89_2 number_87.89_87.89 amod_Baseline_87.89 num_Baseline_1 dep_base_Baseline nn_Test_base conj_+_Train_base conj_+_Train_Test nn_Train_System num_Train_287
E09-1033	J93-2004	o	Our test set is 3718 sentences from the English Penn treebank -LRB- Marcus et al. 1993 -RRB- which were translated into German	prep_into_translated_German auxpass_translated_were nsubjpass_translated_which rcmod_Marcus_translated amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_treebank_Penn nn_treebank_English det_treebank_the dep_sentences_Marcus prep_from_sentences_treebank num_sentences_3718 cop_sentences_is nsubj_sentences_set nn_set_test poss_set_Our
E09-1060	J93-2004	o	Corpora in various languages such as the English Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB- the Swedish Stockholm-Ume corpus -LRB- Ejerhed et al. 1992 -RRB- and the Icelandic Frequency Dictionary -LRB- IFD -RRB- corpus -LRB- Pind et al. 1991 -RRB- have been used to train -LRB- in the case of data-driven methods -RRB- and develop -LRB- in the case of linguistic rule-based methods -RRB- different taggers and to evaluate their accuracy e.g.	poss_accuracy_their dep_evaluate_e.g. dobj_evaluate_accuracy aux_evaluate_to amod_taggers_different amod_methods_rule-based amod_methods_linguistic prep_of_case_methods det_case_the prep_in_develop_case nsubj_develop_corpus amod_methods_data-driven prep_of_case_methods det_case_the dobj_used_taggers conj_and_used_develop prep_in_used_case prep_to_used_train auxpass_used_been aux_used_have nsubjpass_used_corpus amod_Pind_1991 dep_Pind_al. nn_Pind_et appos_corpus_Pind nn_corpus_IFD rcmod_Dictionary_develop rcmod_Dictionary_used nn_Dictionary_Frequency amod_Dictionary_Icelandic det_Dictionary_the dep_al._1992 nn_al._et advmod_Ejerhed_al. dep_corpus_Ejerhed nn_corpus_Stockholm-Ume nn_corpus_Swedish det_corpus_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn nn_corpus_English det_corpus_the amod_languages_various conj_and_Corpora_evaluate conj_and_Corpora_Dictionary appos_Corpora_corpus dep_Corpora_Marcus prep_such_as_Corpora_corpus prep_in_Corpora_languages
E09-1079	J93-2004	o	Purely syntactic categories lead to a smaller number of tags which also improves the accuracy of manual tagging 2 -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_tagging_2 amod_tagging_manual prep_of_accuracy_tagging det_accuracy_the dobj_improves_accuracy advmod_improves_also nsubj_improves_which rcmod_tags_improves prep_of_number_tags amod_number_smaller det_number_a dep_lead_Marcus prep_to_lead_number nsubj_lead_categories amod_categories_syntactic advmod_categories_Purely
E09-1080	J93-2004	o	4.3 Corpora The evaluations of the different models were carried out on the Penn Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB- for English and the Tiger treebank -LRB- Brants et al. 2002 -RRB- for German	amod_Brants_2002 dep_Brants_al. nn_Brants_et prep_for_treebank_German dep_treebank_Brants nn_treebank_Tiger det_treebank_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall nn_corpus_Penn det_corpus_the prep_for_carried_English dep_carried_Marcus prep_on_carried_corpus prt_carried_out auxpass_carried_were nsubjpass_carried_evaluations amod_models_different det_models_the prep_of_evaluations_models det_evaluations_The conj_and_Corpora_treebank rcmod_Corpora_carried num_Corpora_4.3 dep_``_treebank dep_``_Corpora
E09-1094	J93-2004	o	Our model uses an exemplar memory that consists of 133566 verb-role-noun triples extracted from the Wall Street Journal and Brown parts of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the nn_parts_Brown prep_of_Journal_Treebank conj_and_Journal_parts nn_Journal_Street nn_Journal_Wall det_Journal_the prep_from_extracted_parts prep_from_extracted_Journal vmod_triples_extracted amod_triples_verb-role-noun num_triples_133566 prep_of_consists_triples nsubj_consists_that rcmod_memory_consists nn_memory_exemplar det_memory_an dep_uses_Marcus dobj_uses_memory nsubj_uses_model poss_model_Our
E95-1015	J93-2004	o	4.1 The test environment For our experiments we used a manually corrected version of the Air Travel Information System -LRB- ATIS -RRB- spoken language corpus -LRB- Hemphill et al. 1990 -RRB- annotated in the Pennsylvania Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Pennsylvania det_Treebank_the dep_Hemphill_1990 dep_Hemphill_al. nn_Hemphill_et amod_corpus_annotated appos_corpus_Hemphill nn_corpus_language amod_corpus_spoken dep_System_corpus appos_System_ATIS nn_System_Information nn_System_Travel nn_System_Air det_System_the prep_in_version_Treebank prep_of_version_System amod_version_corrected det_version_a advmod_corrected_manually dep_used_Marcus dobj_used_version nsubj_used_we nsubj_used_environment poss_experiments_our prep_for_environment_experiments nn_environment_test det_environment_The num_environment_4.1
E95-1022	J93-2004	o	157 ena or the linguist 's abstraction capabilities -LRB- e.g. knowledge about what is relevant in the context -RRB- they tend to reach a 95-97 % accuracy in the analysis of several languages in particular English -LRB- Marshall 1983 Black et aL 1992 Church 1988 Cutting et al. 1992 de Marcken 1990 DeRose 1988 Hindle 1989 Merialdo 1994 Weischedel et al. 1993 Brill 1992 Samuelsson 1994 Eineborg and Gamb ~ ick 1994 etc. -RRB-	amod_1994_ick number_1994_~ num_Gamb_1994 dep_Eineborg_etc. conj_and_Eineborg_Gamb num_Samuelsson_1994 num_Brill_1992 num_al._1993 nn_al._et nn_al._Weischedel num_Merialdo_1994 num_Hindle_1989 dep_DeRose_Gamb dep_DeRose_Eineborg conj_DeRose_Samuelsson conj_DeRose_Brill dep_DeRose_al. dep_DeRose_Merialdo dep_DeRose_Hindle num_DeRose_1988 num_Marcken_1990 nn_Marcken_de num_al._1992 nn_al._et dobj_Cutting_DeRose advmod_Cutting_Marcken dobj_Cutting_al. num_Church_1988 num_aL_1992 dep_et_aL dep_Black_Cutting dep_Black_Church dep_Black_et dep_Marshall_Black num_Marshall_1983 amod_English_particular amod_languages_several prep_of_analysis_languages det_analysis_the prep_in_accuracy_analysis amod_accuracy_% det_accuracy_a number_%_95-97 dobj_reach_accuracy aux_reach_to dep_tend_Marshall prep_in_tend_English xcomp_tend_reach nsubj_tend_they det_context_the prep_in_relevant_context cop_relevant_is nsubj_relevant_what prepc_about_knowledge_relevant pobj_e.g._knowledge nn_capabilities_abstraction poss_capabilities_linguist det_linguist_the parataxis_ena_tend dep_ena_e.g. conj_or_ena_capabilities num_ena_157 dep_``_capabilities dep_``_ena
E95-1022	J93-2004	o	Note in passing that the ratio 1.04-1 .08 / 99.7 % compares very favourably with other systems c.f. 3.0 / 99.3 % by POST -LRB- Weischedel et al. 1993 -RRB- and 1.04 / 97.6 % or 1.09 / 98.6 % by de Marcken -LRB- 1990 -RRB-	appos_Marcken_1990 nn_Marcken_de prep_by_%_Marcken num_%_98.6 dep_%_% conj_or_%_1.09 num_%_97.6 dep_1993_al. nn_al._et num_Weischedel_1993 prep_by_%_POST num_%_99.3 dep_c.f._1.09 dep_c.f._% conj_and_c.f._1.04 appos_c.f._Weischedel dep_c.f._% num_c.f._3.0 amod_systems_other advmod_favourably_very prep_with_compares_systems advmod_compares_favourably dep_compares_.08 nsubj_compares_ratio dep_compares_the mark_compares_that num_%_99.7 dep_.08_% number_.08_1.04-1 ccomp_passing_compares dep_Note_1.04 dep_Note_c.f. prepc_in_Note_passing
E95-1029	J93-2004	o	Our results agree at least at the level of morphology with -LRB- Leech and Eyes 1993 Marcus et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Marcus num_Eyes_1993 conj_and_Leech_al. conj_and_Leech_Eyes prep_of_level_morphology det_level_the pobj_at_least prep_with_agree_al. prep_with_agree_Eyes prep_with_agree_Leech prep_at_agree_level advmod_agree_at nsubj_agree_results poss_results_Our
E95-1029	J93-2004	o	A more optimistic view can be found in -LRB- Leech and Eyes 1993 p. 39 Marcus et al. 1993 p. 328 -RRB- they argue that a near-100 % interjudge agreement is possible provided the part-of-speech annotation is done carefully by experts	agent_done_experts advmod_done_carefully auxpass_done_is nsubjpass_done_provided amod_annotation_part-of-speech det_annotation_the dobj_provided_annotation cop_possible_is nsubj_possible_agreement mark_possible_that nn_agreement_interjudge amod_agreement_% det_agreement_a number_%_near-100 parataxis_argue_done ccomp_argue_possible nsubj_argue_they num_p._328 dep_1993_al. appos_Marcus_p. num_Marcus_1993 nn_Marcus_et num_p._39 num_Eyes_1993 dep_Leech_Marcus conj_and_Leech_p. conj_and_Leech_Eyes dep_in_p. dep_in_Eyes dep_in_Leech parataxis_found_argue prep_found_in auxpass_found_be aux_found_can nsubjpass_found_view amod_view_optimistic det_view_A advmod_optimistic_more
E99-1031	J93-2004	o	Our experiments created translation modules for two evaluation corpora written news stories from the Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB- and spoken task-oriented dialogues from the TRAINS93 corpus -LRB- Heeman and Allen 1995 -RRB-	num_Heeman_1995 conj_and_Heeman_Allen appos_corpus_Allen appos_corpus_Heeman nn_corpus_TRAINS93 det_corpus_the amod_dialogues_task-oriented amod_dialogues_spoken amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_from_stories_corpus conj_and_stories_dialogues dep_stories_Marcus prep_from_stories_corpus nn_stories_news amod_stories_written nn_corpora_evaluation num_corpora_two dep_modules_dialogues dep_modules_stories prep_for_modules_corpora nn_modules_translation amod_modules_created nsubj_modules_experiments poss_experiments_Our
E99-1050	J93-2004	o	The tags sets we shall examine are the set used in the Penn Tree Bank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB- and the C5 tag-set used by the CLAWS part-of-speech tagger -LRB- Garside 1996 -RRB-	amod_Garside_1996 dep_tagger_Garside amod_tagger_part-of-speech nn_tagger_CLAWS det_tagger_the agent_used_tagger vmod_tag-set_used nn_tag-set_C5 det_tag-set_the conj_and_Marcus_tag-set amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Bank_PTB nn_Bank_Tree nn_Bank_Penn det_Bank_the prep_in_used_Bank dep_set_tag-set dep_set_Marcus vmod_set_used det_set_the cop_set_are csubj_set_sets aux_examine_shall nsubj_examine_we ccomp_sets_examine nsubj_sets_tags det_tags_The
H05-1035	J93-2004	o	The difference in accuracy between a SVM model applied to RRR dataset -LRB- RRR-basic experiment -RRB- and the same experiment applied to TB2 dataset -LRB- TB2278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human headwords -LRB- Ratnaparkhi et al. 1994 -RRB- 88.2 RRR Average human whole sentence -LRB- Ratnaparkhi et al. 1994 -RRB- 93.2 RRR Maximum Likelihood-based -LRB- Hindle and Rooth 1993 -RRB- 79.7 AP Maximum entropy words -LRB- Ratnaparkhi et al. 1994 -RRB- 77.7 RRR Maximum entropy words & classes -LRB- Ratnaparkhi et al. 1994 -RRB- 81.6 RRR Decision trees -LRB- Ratnaparkhi et al. 1994 -RRB- 77.7 RRR Transformation-Based Learning -LRB- Brill and Resnik 1994 -RRB- 81.8 WordNet Maximum-Likelihood based -LRB- Collins and Brooks 1995 -RRB- 84.5 RRR Maximum-Likelihood based -LRB- Collins and Brooks 1995 -RRB- 86.1 TB2 Decision trees & WSD -LRB- Stetina and Nagao 1997 -RRB- 88.1 RRR WordNet Memory-based Learning -LRB- Zavrel et al. 1997 -RRB- 84.4 RRR LexSpace Maximum entropy unsupervised -LRB- Ratnaparkhi 1998 -RRB- 81.9 Maximum entropy supervised -LRB- Ratnaparkhi 1998 -RRB- 83.7 RRR Neural Nets -LRB- Alegre et al. 1999 -RRB- 86.0 RRR WordNet Boosting -LRB- Abney et al. 1999 -RRB- 84.4 RRR Semi-probabilistic -LRB- Pantel and Lin 2000 -RRB- 84.31 RRR Maximum entropy ensemble -LRB- McLauchlan 2001 -RRB- 85.5 RRR LSA SVM -LRB- Vanschoenwinkel and Manderick 2003 -RRB- 84.8 RRR Nearest-neighbor -LRB- Zhao and Lin 2004 -RRB- 86.5 RRR DWS FN dataset w/o semantic features -LRB- FN-best-no-sem -RRB- 91.79 FN PR-WWW FN dataset w / semantic features -LRB- FN-best-sem -RRB- 92.85 FN PR-WWW TB2 dataset best feature set -LRB- TB2-best -RRB- 93.62 TB2 PR-WWW Table 5 Accuracy of PP-attachment ambiguity resolution -LRB- our results in bold -RRB- basic experiment -RRB- is 2.9 %	num_%_2.9 cop_%_is nsubj_%_dataset amod_experiment_basic amod_experiment_bold prep_in_results_experiment poss_results_our nn_resolution_ambiguity nn_resolution_PP-attachment dep_Accuracy_results prep_of_Accuracy_resolution num_Table_5 nn_Table_PR-WWW nn_Table_TB2 num_Table_93.62 dep_Table_TB2-best dep_set_Table nn_set_feature amod_set_best appos_dataset_set nn_dataset_TB2 nn_dataset_PR-WWW nn_dataset_FN num_dataset_92.85 dep_features_dataset appos_features_FN-best-sem amod_features_semantic dep_dataset_Accuracy dep_dataset_features appos_dataset_w nn_dataset_FN nn_dataset_PR-WWW nn_dataset_FN num_dataset_91.79 rcmod_features_% appos_features_FN-best-no-sem amod_features_semantic nn_features_w/o nn_dataset_FN nn_dataset_DWS nn_dataset_RRR num_dataset_86.5 num_Zhao_2004 conj_and_Zhao_Lin dep_Nearest-neighbor_dataset appos_Nearest-neighbor_Lin appos_Nearest-neighbor_Zhao nn_Nearest-neighbor_RRR num_Nearest-neighbor_84.8 dep_Vanschoenwinkel_2003 conj_and_Vanschoenwinkel_Manderick dep_SVM_Nearest-neighbor dep_SVM_Manderick dep_SVM_Vanschoenwinkel nn_SVM_LSA nn_SVM_RRR num_SVM_85.5 dep_McLauchlan_features dep_McLauchlan_SVM dep_McLauchlan_2001 dep_ensemble_McLauchlan appos_entropy_ensemble nn_entropy_Maximum nn_entropy_RRR num_entropy_84.31 amod_entropy_Semi-probabilistic nn_entropy_RRR num_entropy_84.4 amod_Pantel_2000 conj_and_Pantel_Lin dep_Semi-probabilistic_Lin dep_Semi-probabilistic_Pantel amod_Abney_1999 dep_Abney_al. nn_Abney_et dep_Boosting_entropy dep_Boosting_Abney nn_Boosting_WordNet nn_Boosting_RRR num_Boosting_86.0 amod_Alegre_1999 dep_Alegre_al. nn_Alegre_et dep_Nets_Boosting dep_Nets_Alegre amod_Nets_Neural nn_Nets_RRR num_Nets_83.7 dep_Ratnaparkhi_1998 dobj_supervised_Nets dep_supervised_Ratnaparkhi nn_entropy_Maximum num_entropy_81.9 amod_entropy_unsupervised amod_Ratnaparkhi_1998 dep_unsupervised_Ratnaparkhi vmod_entropy_supervised appos_entropy_entropy nn_entropy_Maximum nn_entropy_LexSpace nn_entropy_RRR num_entropy_84.4 dep_Zavrel_1997 dep_Zavrel_al. nn_Zavrel_et dep_Learning_entropy dep_Learning_Zavrel amod_Learning_Memory-based nn_Learning_WordNet nn_Learning_RRR num_Learning_88.1 dep_Stetina_1997 conj_and_Stetina_Nagao dep_trees_Learning dep_trees_Nagao dep_trees_Stetina conj_and_trees_WSD nn_trees_Decision nn_trees_TB2 num_trees_86.1 amod_Collins_1995 conj_and_Collins_Brooks dep_Maximum-Likelihood_WSD dep_Maximum-Likelihood_trees dep_Maximum-Likelihood_Brooks dep_Maximum-Likelihood_Collins vmod_Maximum-Likelihood_based nn_Maximum-Likelihood_RRR num_Maximum-Likelihood_84.5 dep_Collins_1995 conj_and_Collins_Brooks dep_Maximum-Likelihood_Maximum-Likelihood dep_Maximum-Likelihood_Brooks dep_Maximum-Likelihood_Collins vmod_Maximum-Likelihood_based nn_Maximum-Likelihood_WordNet num_Maximum-Likelihood_81.8 dep_Brill_1994 conj_and_Brill_Resnik dep_Learning_Maximum-Likelihood dep_Learning_Resnik dep_Learning_Brill amod_Learning_Transformation-Based nn_Learning_RRR num_Learning_77.7 amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_trees_Learning dep_trees_Ratnaparkhi nn_trees_Decision nn_trees_RRR num_trees_81.6 dep_Ratnaparkhi_trees amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_words_Ratnaparkhi conj_and_words_classes nn_entropy_Maximum nn_entropy_RRR num_entropy_77.7 appos_entropy_Ratnaparkhi nn_entropy_words amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et nn_entropy_Maximum nn_entropy_AP num_entropy_79.7 amod_entropy_Likelihood-based nn_entropy_Maximum nn_entropy_RRR num_entropy_93.2 dep_Hindle_1993 conj_and_Hindle_Rooth dep_Likelihood-based_Rooth dep_Likelihood-based_Hindle nn_al._et num_Ratnaparkhi_1994 advmod_Ratnaparkhi_al. dep_sentence_entropy appos_sentence_Ratnaparkhi amod_sentence_whole amod_sentence_human dep_sentence_RRR num_sentence_88.2 nn_sentence_headwords amod_human_Average amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_headwords_Ratnaparkhi dep_human_classes dep_human_words conj_human_entropy conj_human_sentence amod_human_Average amod_FN_human dep_81.73_FN dep_P_81.73 dep_each_P prep_for_likely_each advmod_likely_Most amod_TB2_likely num_TB2_72.30 nn_TB2_P det_TB2_each prep_for_likely_TB2 advmod_likely_Most amod_RRR_likely num_RRR_72.19 nn_RRR_P det_RRR_each prep_for_likely_RRR advmod_likely_Most amod_RRR_likely num_RRR_55.0 nn_RRR_noun nn_RRR_Always nn_RRR_Supervision nn_RRR_Extra nn_RRR_Data nn_RRR_Accuracy nn_RRR_Description nn_RRR_TB2278 appos_dataset_RRR nn_dataset_TB2 prep_to_applied_dataset amod_experiment_same det_experiment_the amod_experiment_RRR-basic conj_and_dataset_experiment appos_dataset_experiment nn_dataset_RRR dep_applied_applied prep_to_applied_experiment prep_to_applied_dataset nsubj_applied_difference nn_model_SVM det_model_a prep_between_difference_model prep_in_difference_accuracy det_difference_The
H05-1035	J93-2004	o	But if one limits the information used for disambiguation of the PPattachment to include only the verb the noun representing its object the preposition and the main noun in the PP the accuracy for human decision degrades from 93.2 % to 88.2 % -LRB- Ratnaparkhi et al. 1994 -RRB- on a dataset extracted from Penn Treebank -LRB- Marcus et 273 al. 1993 -RRB-	dep_al._1993 dep_273_al. dep_et_273 dep_Marcus_et nn_Treebank_Penn prep_from_extracted_Treebank vmod_dataset_extracted det_dataset_a appos_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_%_Ratnaparkhi number_%_88.2 num_%_93.2 prep_on_degrades_dataset prep_to_degrades_% prep_from_degrades_% nsubj_degrades_accuracy amod_decision_human prep_for_accuracy_decision det_accuracy_the det_PP_the prep_in_noun_PP amod_noun_main det_noun_the det_preposition_the poss_object_its dobj_representing_object vmod_noun_representing det_noun_the dep_verb_Marcus ccomp_verb_degrades conj_and_verb_noun conj_and_verb_preposition dobj_verb_noun det_verb_the advmod_verb_only dobj_include_noun dobj_include_preposition dobj_include_verb aux_include_to det_PPattachment_the prep_of_disambiguation_PPattachment xcomp_used_include prep_for_used_disambiguation vmod_information_used det_information_the dobj_limits_information nsubj_limits_one mark_limits_if advcl_But_limits advcl_``_But
H05-1066	J93-2004	o	In fact the largest source of English dependency trees is automatically generated from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and is by convention exclusively projective	advmod_projective_exclusively cop_projective_is nsubj_projective_source prep_by_is_convention amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the conj_and_generated_projective dep_generated_Marcus prep_from_generated_Treebank advmod_generated_automatically auxpass_generated_is nsubjpass_generated_source prep_in_generated_fact nn_trees_dependency nn_trees_English prep_of_source_trees amod_source_largest det_source_the
H05-1066	J93-2004	o	Table 2 shows the results for English projective dependency trees extracted from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- using the rules of Yamada and Matsumoto -LRB- 2003 -RRB-	appos_Matsumoto_2003 conj_and_Yamada_Matsumoto prep_of_rules_Matsumoto prep_of_rules_Yamada det_rules_the dobj_using_rules nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the prep_from_extracted_Treebank vmod_trees_extracted nn_trees_dependency amod_trees_projective nn_trees_English prep_for_results_trees det_results_the vmod_shows_using dep_shows_Marcus dobj_shows_results nsubj_shows_Table num_Table_2
H05-1070	J93-2004	o	Examples are the Penn Treebank -LRB- Marcus et al. 1993 -RRB- for American English annotated at the University of Pennsylvania the French treebank -LRB- Abeille and Clement 1999 -RRB- developed in Paris the TIGER Corpus -LRB- Brants et al. 2002 -RRB- for German annotated at the Universities of Saarbrcurrency1ucken and This research was funded by a German Science Foundation grant -LRB- DFG SFB441-6 -RRB-	nn_SFB441-6_DFG appos_grant_SFB441-6 nn_grant_Foundation nn_grant_Science amod_grant_German det_grant_a agent_funded_grant auxpass_funded_was nsubjpass_funded_research det_research_This prep_of_Universities_Saarbrcurrency1ucken det_Universities_the prep_at_annotated_Universities amod_annotated_German num_Brants_2002 dep_Brants_al. nn_Brants_et prep_for_Corpus_annotated appos_Corpus_Brants nn_Corpus_TIGER det_Corpus_the prep_in_developed_Paris dep_Abeille_1999 conj_and_Abeille_Clement vmod_treebank_developed dep_treebank_Clement dep_treebank_Abeille amod_treebank_French det_treebank_the prep_of_University_Pennsylvania det_University_the amod_English_annotated amod_English_American amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Treebank_funded conj_and_Treebank_Corpus dep_Treebank_treebank prep_at_Treebank_University prep_for_Treebank_English dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the cop_Treebank_are nsubj_Treebank_Examples
H05-1078	J93-2004	o	Statistical parsers trained on the Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB- produce trees annotated with bare phrase structure labels -LRB- Collins 1999 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak appos_Collins_1999 dep_labels_Collins nn_labels_structure nn_labels_phrase amod_labels_bare prep_with_annotated_labels amod_trees_annotated dobj_produce_trees nsubj_produce_parsers amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the prep_on_trained_Treebank appos_parsers_Marcus vmod_parsers_trained amod_parsers_Statistical
H05-1083	J93-2004	o	As for parser we train three off-shelf maximum-entropy parsers -LRB- Ratnaparkhi 1999 -RRB- using the Arabic Chinese and English Penn treebank -LRB- Maamouri and Bies 2004 Xia et al. 2000 Marcus et al. 1993 -RRB-	num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Xia_2000 nn_Xia_al. nn_Xia_et dep_Maamouri_Marcus conj_and_Maamouri_Xia conj_and_Maamouri_2004 conj_and_Maamouri_Bies nn_treebank_Penn nn_treebank_English dep_Arabic_Xia dep_Arabic_2004 dep_Arabic_Bies dep_Arabic_Maamouri conj_and_Arabic_treebank conj_and_Arabic_Chinese dep_the_treebank dep_the_Chinese dep_the_Arabic dobj_using_the amod_Ratnaparkhi_1999 vmod_parsers_using appos_parsers_Ratnaparkhi amod_parsers_maximum-entropy amod_parsers_off-shelf num_parsers_three dobj_train_parsers nsubj_train_we pobj_train_parser prepc_as_for_train_for
H05-1083	J93-2004	o	This is possible because of the availability of statistical parsers which can be trained on human-annotated treebanks -LRB- Marcus et al. 1993 Xia et al. 2000 Maamouri and Bies 2004 -RRB- for multiple languages -LRB- 2 -RRB- The binding theory is used as a guideline and syntactic structures are encoded as features in a maximum entropy coreference system -LRB- 3 -RRB- The syntactic features are evaluated on three languages Arabic Chinese and English -LRB- one goal is to see if features motivated by the English language can help coreference resolution in other languages -RRB-	amod_languages_other prep_in_resolution_languages nn_resolution_coreference dobj_help_resolution aux_help_can nsubj_help_features mark_help_if nn_language_English det_language_the agent_motivated_language vmod_features_motivated advcl_see_help aux_see_to xcomp_is_see nsubj_is_goal num_goal_one amod_goal_English amod_goal_Chinese amod_goal_Arabic conj_and_Arabic_English conj_and_Arabic_Chinese dep_languages_is num_languages_three prep_on_evaluated_languages auxpass_evaluated_are nsubjpass_evaluated_features dep_evaluated_3 amod_features_syntactic det_features_The nn_system_coreference nn_system_entropy nn_system_maximum det_system_a prep_in_features_system prep_as_encoded_features auxpass_encoded_are nsubjpass_encoded_structures nsubjpass_encoded_guideline mark_encoded_as amod_structures_syntactic conj_and_guideline_structures det_guideline_a advcl_used_encoded auxpass_used_is nsubjpass_used_theory dep_used_2 nn_theory_binding det_theory_The amod_languages_multiple dep_Maamouri_2004 conj_and_Maamouri_Bies prep_for_Xia_languages dep_Xia_Bies dep_Xia_Maamouri num_Xia_2000 nn_Xia_al. nn_Xia_et parataxis_Marcus_evaluated dep_Marcus_used dep_Marcus_Xia num_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_treebanks_human-annotated prep_on_trained_treebanks auxpass_trained_be aux_trained_can nsubjpass_trained_which amod_parsers_statistical dep_availability_Marcus rcmod_availability_trained prep_of_availability_parsers det_availability_the prep_because_of_possible_availability cop_possible_is nsubj_possible_This ccomp_``_possible
H05-1099	J93-2004	o	Li and Roth demonstrated that their shallow parser trained to label shallow constituents along the lines of the well-known CoNLL2000 task -LRB- Sang and Buchholz 2000 -RRB- outperformed the Collins parser in correctly identifying these constituents in the Penn Wall Street Journal -LRB- WSJ -RRB- Treebank -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall nn_Journal_Penn det_Journal_the det_constituents_these prep_in_identifying_Treebank dobj_identifying_constituents advmod_identifying_correctly nn_parser_Collins det_parser_the prepc_in_outperformed_identifying dobj_outperformed_parser nsubj_outperformed_parser mark_outperformed_that dep_Sang_2000 conj_and_Sang_Buchholz appos_task_Buchholz appos_task_Sang nn_task_CoNLL2000 amod_task_well-known det_task_the prep_of_lines_task det_lines_the amod_constituents_shallow prep_along_label_lines dobj_label_constituents aux_label_to xcomp_trained_label vmod_parser_trained amod_parser_shallow poss_parser_their dep_demonstrated_Marcus ccomp_demonstrated_outperformed nsubj_demonstrated_Roth nsubj_demonstrated_Li conj_and_Li_Roth
H94-1034	J93-2004	o	In a test set of 756 utterances containing 26 repairs -LRB- Dowding et al. 1993 -RRB- they obtained a detection recall rate of 42 % and a precision of 84.6 % for correction they obtained a recall rate of 30 % and a precision rate of 62 %	num_%_62 prep_of_rate_% nn_rate_precision det_rate_a num_%_30 conj_and_rate_rate prep_of_rate_% nn_rate_recall det_rate_a dobj_obtained_rate dobj_obtained_rate nsubj_obtained_they prep_for_obtained_correction num_%_84.6 prep_of_precision_% det_precision_a num_%_42 conj_and_rate_precision prep_of_rate_% nn_rate_recall nn_rate_detection det_rate_a parataxis_obtained_obtained dobj_obtained_precision dobj_obtained_rate nsubj_obtained_they prep_in_obtained_set amod_Dowding_1993 dep_Dowding_al. nn_Dowding_et dep_repairs_Dowding num_repairs_26 dobj_containing_repairs vmod_utterances_containing num_utterances_756 prep_of_set_utterances nn_set_test det_set_a
H94-1034	J93-2004	o	Good partof-speech results can be obtained using only the preceding category -LRB- Weischedel et al. 1993 -RRB- which is what we will be using	aux_using_be aux_using_will nsubj_using_we dobj_using_what ccomp_is_using nsubj_is_which amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et rcmod_category_is appos_category_Weischedel amod_category_preceding det_category_the advmod_category_only dobj_using_category xcomp_obtained_using auxpass_obtained_be aux_obtained_can nsubjpass_obtained_results amod_results_partof-speech amod_results_Good
I05-2019	J93-2004	o	eBonsai first performs syntactic analysis of a sentence using a parser based on GLR algorithm -LRB- MSLR parser -RRB- -LRB- Tanaka et al. 1993 -RRB- and provides candidates of its syntactic structure	amod_structure_syntactic poss_structure_its prep_of_candidates_structure dobj_provides_candidates nsubj_provides_eBonsai amod_Tanaka_1993 dep_Tanaka_al. nn_Tanaka_et nn_parser_MSLR appos_algorithm_parser nn_algorithm_GLR prep_on_based_algorithm vmod_parser_based det_parser_a dep_using_Tanaka dobj_using_parser det_sentence_a prep_of_analysis_sentence nn_analysis_syntactic conj_and_performs_provides xcomp_performs_using dobj_performs_analysis nsubj_performs_eBonsai amod_eBonsai_first
I05-2019	J93-2004	o	The MSLR parser -LRB- Tanaka et al. 1993 -RRB- performs syntactic analysis of the sentence	det_sentence_the prep_of_analysis_sentence amod_analysis_syntactic dobj_performs_analysis nsubj_performs_parser amod_Tanaka_1993 dep_Tanaka_al. nn_Tanaka_et dep_parser_Tanaka nn_parser_MSLR det_parser_The ccomp_``_performs
I05-2019	J93-2004	p	Particularly syntactically annotated corpora -LRB- treebanks -RRB- such as Penn Treebank -LRB- Marcus et al. 1993 -RRB- Negra Corpus -LRB- Skut et al. 1997 -RRB- and EDR Corpus -LRB- Jap 1994 -RRB- contribute to improve the performance of morpho-syntactic analysis systems	nn_systems_analysis amod_systems_morpho-syntactic prep_of_performance_systems det_performance_the dobj_improve_performance aux_improve_to xcomp_contribute_improve nsubj_contribute_corpora advmod_contribute_Particularly dep_Jap_1994 appos_Corpus_Jap nn_Corpus_EDR amod_Skut_1997 dep_Skut_al. nn_Skut_et dep_Corpus_Skut nn_Corpus_Negra amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Treebank_Corpus conj_and_Treebank_Corpus dep_Treebank_Marcus nn_Treebank_Penn prep_such_as_corpora_Corpus prep_such_as_corpora_Corpus prep_such_as_corpora_Treebank appos_corpora_treebanks amod_corpora_annotated advmod_annotated_syntactically
I05-2041	J93-2004	o	The first approaches are used for Penn Treebank -LRB- Marcus et al. 1993 -RRB- and the KAIST language resource -LRB- Lee et al. 1997 Choi 2001 -RRB-	amod_Choi_2001 dep_Lee_Choi appos_Lee_1997 dep_Lee_al. nn_Lee_et nn_resource_language nn_resource_KAIST det_resource_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn dep_used_Lee conj_and_used_resource dep_used_Marcus prep_for_used_Treebank auxpass_used_are nsubjpass_used_approaches amod_approaches_first det_approaches_The
I05-2041	J93-2004	o	However most parsers still tend to show low performance on the long sentences -LRB- Li et al. 1990 Doi et al. 1993 Kim et al. 2000 -RRB-	num_Kim_2000 nn_Kim_al. nn_Kim_et num_Doi_1993 nn_Doi_al. nn_Doi_et dep_Li_Kim dep_Li_Doi amod_Li_1990 dep_Li_al. nn_Li_et amod_sentences_long det_sentences_the prep_on_performance_sentences amod_performance_low dobj_show_performance aux_show_to dep_tend_Li xcomp_tend_show advmod_tend_still nsubj_tend_parsers advmod_tend_However amod_parsers_most
I05-2041	J93-2004	p	This kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering -LRB- Lee et al. 1997 Choi 2001 -RRB- and has also proved useful in theoretical linguistics research -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_research_linguistics amod_research_theoretical prep_in_useful_research acomp_proved_useful advmod_proved_also aux_proved_has nsubj_proved_kind dep_Choi_2001 dep_Lee_Choi amod_Lee_1997 dep_Lee_al. nn_Lee_et nn_answering_question conj_and_translation_answering nn_translation_machine prep_such_as_applications_answering prep_such_as_applications_translation nn_applications_linguistics amod_applications_computational prep_for_resource_applications amod_resource_valuable det_resource_an advmod_valuable_extremely dep_served_Marcus conj_and_served_proved dep_served_Lee prep_as_served_resource aux_served_has nsubj_served_kind prep_of_kind_corpus det_kind_This
I05-3005	J93-2004	o	-LRB- Ng and Low 2004 Toutanova et al 2003 Brants 2000 Ratnaparkhi 1996 Samuelsson 1993 -RRB-	num_Samuelsson_1993 num_Ratnaparkhi_1996 num_Brants_2000 nn_Toutanova_al nn_Toutanova_et amod_2004_Low appos_Ng_Samuelsson appos_Ng_Ratnaparkhi appos_Ng_Brants amod_Ng_2003 conj_and_Ng_Toutanova conj_and_Ng_2004 dep_''_Toutanova dep_''_2004 dep_''_Ng
I05-3016	J93-2004	o	The implementation of the algorithm is one that has a core of code that can run on either the Penn Treebank -LRB- Marcus et al. 1993 -RRB- or on the Chinese Treebank	amod_Treebank_Chinese det_Treebank_the pobj_on_Treebank dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_or_Treebank_on appos_Treebank_Marcus nn_Treebank_Penn det_Treebank_the preconj_Treebank_either prep_on_run_on prep_on_run_Treebank aux_run_can nsubj_run_that rcmod_core_run prep_of_core_code det_core_a dobj_has_core nsubj_has_that rcmod_one_has cop_one_is nsubj_one_implementation det_algorithm_the prep_of_implementation_algorithm det_implementation_The
I05-4002	J93-2004	o	For many languages large-scale syntactically annotated corpora have been built -LRB- e.g. the Penn Treebank -LRB- Marcus et al. 1993 -RRB- -RRB- and many parsing algorithms using CFGs have been proposed	auxpass_proposed_been aux_proposed_have nsubjpass_proposed_algorithms dobj_using_CFGs vmod_algorithms_using nn_algorithms_parsing amod_algorithms_many dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the pobj_e.g._Treebank conj_and_built_proposed dep_built_e.g. auxpass_built_been aux_built_have nsubjpass_built_corpora prep_for_built_languages amod_corpora_annotated amod_corpora_large-scale advmod_annotated_syntactically amod_languages_many
I05-6007	J93-2004	o	Since the texts in the RST Treebank are taken from the syntactically annotated Penn Treebank -LRB- Marcus et al. 1993 -RRB- it is natural to ask what the relation is between the discourse structures in the RST Treebank and the syntactic structures of the Penn Treebank	nn_Treebank_Penn det_Treebank_the prep_of_structures_Treebank amod_structures_syntactic det_structures_the conj_and_Treebank_structures nn_Treebank_RST det_Treebank_the prep_in_structures_structures prep_in_structures_Treebank nn_structures_discourse det_structures_the prep_between_is_structures nsubj_is_relation dobj_is_what det_relation_the ccomp_ask_is aux_ask_to xcomp_natural_ask cop_natural_is nsubj_natural_it advcl_natural_taken amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn amod_Treebank_annotated advmod_Treebank_syntactically det_Treebank_the dep_taken_Marcus prep_from_taken_Treebank auxpass_taken_are nsubjpass_taken_texts mark_taken_Since nn_Treebank_RST det_Treebank_the prep_in_texts_Treebank det_texts_the ccomp_``_natural
I08-2096	J93-2004	o	Evaluations are typically carried out on newspaper texts i.e. on section 23 of the Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the prep_of_section_Treebank num_section_23 pobj_on_section pcomp_i.e._on nn_texts_newspaper dep_carried_Marcus prep_carried_i.e. prep_on_carried_texts prt_carried_out advmod_carried_typically auxpass_carried_are nsubjpass_carried_Evaluations
I08-2099	J93-2004	p	Some notable efforts in this direction for other languages have been the Penn Tree Bank -LRB- Marcus et al. 1993 -RRB- for English and the Prague Dependency Bank -LRB- Hajicova 1998 -RRB- for Czech	amod_Hajicova_1998 prep_for_Bank_Czech dep_Bank_Hajicova nn_Bank_Dependency nn_Bank_Prague det_Bank_the num_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Bank_Bank prep_for_Bank_English dep_Bank_Marcus nn_Bank_Tree nn_Bank_Penn det_Bank_the cop_Bank_been aux_Bank_have nsubj_Bank_efforts amod_languages_other det_direction_this prep_for_efforts_languages prep_in_efforts_direction amod_efforts_notable det_efforts_Some
I08-2099	J93-2004	o	Other works based on this scheme like -LRB- Bharati et al. 1993 Bharati et al. 2002 Pedersen et al. 2004 -RRB- have shown promising results	amod_results_promising dobj_shown_results aux_shown_have nsubj_shown_works num_Pedersen_2004 nn_Pedersen_al. nn_Pedersen_et dep_Bharati_Pedersen num_Bharati_2002 nn_Bharati_al. nn_Bharati_et dep_Bharati_Bharati appos_Bharati_1993 dep_Bharati_al. nn_Bharati_et prep_like_scheme_Bharati det_scheme_this prep_on_based_scheme vmod_works_based amod_works_Other
J00-4003	J93-2004	o	Only recently have robust knowledge-based methods for some of these tasks begun to appear and their performance is still not very good as seen above in our discussion of using WordNet as a semantic network 33 as for checking the plausibility of a hypothesis on the basis of causal knowledge about the world we now have a much better theoretical grasp of how such inferences could be made -LRB- see for example Hobbs et al. \ -LSB- 1993 \ -RSB- and Lascarides and Asher \ -LSB- 1993 \ -RSB- -RRB- but we are still quite a long way from a general inference engine	nn_engine_inference amod_engine_general det_engine_a prep_from_way_engine amod_way_long det_way_a advmod_way_quite advmod_way_still cop_way_are nsubj_way_we num_\_1993 num_Asher_\ dep_Lascarides_\ conj_and_Lascarides_Asher num_\_1993 dep_\_\ nn_\_al. nn_\_Hobbs nn_al._et conj_and_see_Asher conj_and_see_Lascarides conj_and_see_\ prep_for_see_example dep_made_Lascarides dep_made_\ dep_made_see auxpass_made_be aux_made_could nsubjpass_made_inferences advmod_made_how amod_inferences_such prepc_of_grasp_made amod_grasp_theoretical amod_grasp_better det_grasp_a advmod_better_much conj_but_have_way dobj_have_grasp advmod_have_now nsubj_have_we dep_have_33 det_world_the prep_about_knowledge_world amod_knowledge_causal prep_of_basis_knowledge det_basis_the prep_on_hypothesis_basis det_hypothesis_a prep_of_plausibility_hypothesis det_plausibility_the dobj_checking_plausibility pcomp_33_checking prepc_as_for_33_for amod_network_semantic det_network_a prep_as_using_network dobj_using_WordNet prepc_of_discussion_using poss_discussion_our pobj_in_discussion pcomp_above_in prep_seen_above mark_seen_as advcl_good_seen advmod_good_very neg_good_not advmod_good_still cop_good_is nsubj_good_performance poss_performance_their aux_appear_to xcomp_begun_appear vmod_tasks_begun det_tasks_these prep_of_some_tasks prep_for_methods_some amod_methods_knowledge-based amod_methods_robust parataxis_have_way parataxis_have_have conj_and_have_good dobj_have_methods advmod_have_recently advmod_have_Only ccomp_``_good ccomp_``_have
J01-2004	J93-2004	o	It has been shown repeatedly -- e.g. Briscoe and Carroll -LRB- 1993 -RRB- Charniak -LRB- 1997 -RRB- Collins -LRB- 1997 -RRB- Inui et al.	nn_al._et nn_al._Inui dep_Collins_al. appos_Collins_1997 appos_Charniak_1997 appos_Carroll_1993 conj_and_e.g._Collins conj_and_e.g._Charniak conj_and_e.g._Carroll conj_and_e.g._Briscoe dep_shown_Collins dep_shown_Charniak dep_shown_Carroll dep_shown_Briscoe dep_shown_e.g. advmod_shown_repeatedly auxpass_shown_been aux_shown_has nsubjpass_shown_It
J01-4003	J93-2004	o	Next we use the conclusions from two psycholinguistic experiments on ranking the Cf-list the salience of discourse entities in prepended phrases -LRB- Gordon Grosz and Gilliom 1993 -RRB- and the ordering of possessor and possessed in complex NPs -LRB- Gordon et al. 1999 -RRB- to try to improve the performance of LRC	prep_of_performance_LRC det_performance_the dobj_improve_performance aux_improve_to xcomp_try_improve aux_try_to dep_try_possessed dep_try_use num_al._1999 nn_al._et nn_al._Gordon amod_NPs_complex prep_in_possessed_NPs nsubj_possessed_we prep_of_ordering_possessor det_ordering_the num_Gilliom_1993 conj_and_Gordon_Gilliom conj_and_Gordon_Grosz amod_phrases_prepended nn_entities_discourse dep_salience_Gilliom dep_salience_Grosz dep_salience_Gordon prep_in_salience_phrases prep_of_salience_entities det_salience_the conj_and_Cf-list_ordering conj_and_Cf-list_salience det_Cf-list_the dobj_ranking_ordering dobj_ranking_salience dobj_ranking_Cf-list prepc_on_experiments_ranking amod_experiments_psycholinguistic num_experiments_two det_conclusions_the dep_use_al. conj_and_use_possessed prep_from_use_experiments dobj_use_conclusions nsubj_use_we mark_use_Next
J06-1005	J93-2004	o	5 The SemCor collection -LRB- Miller et al. 1993 -RRB- is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns verbs adverbs and adjectives have been manually tagged with their corresponding WordNet senses and part-of-speech tags using Brills tagger -LRB- 1995 -RRB-	appos_tagger_1995 nn_tagger_Brills dobj_using_tagger amod_tags_part-of-speech conj_and_senses_tags nn_senses_WordNet amod_senses_corresponding poss_senses_their xcomp_tagged_using prep_with_tagged_tags prep_with_tagged_senses advmod_tagged_manually auxpass_tagged_been aux_tagged_have nsubjpass_tagged_adjectives nsubjpass_tagged_adverbs nsubjpass_tagged_verbs nsubjpass_tagged_nouns prep_in_tagged_which conj_and_nouns_adjectives conj_and_nouns_adverbs conj_and_nouns_verbs det_nouns_the rcmod_sets_tagged num_sets_three prep_into_distributed_sets vmod_articles_distributed nn_articles_news num_articles_352 prep_of_consists_articles nsubj_consists_collection amod_Corpus_Brown det_Corpus_the conj_and_subset_consists prep_of_subset_Corpus det_subset_a cop_subset_is nsubj_subset_collection amod_Miller_1993 dep_Miller_al. nn_Miller_et dep_collection_Miller nn_collection_SemCor det_collection_The num_collection_5
J07-3004	J93-2004	p	One of the largest and earliest such efforts is the Penn Treebank -LRB- Marcus Santorini and Marcinkiewicz 1993 Marcus et al. 1994 -RRB- which contains a one-million word Institute for Research in Cognitive Science University of Pennsylvania 3401 Walnut Street Suite 400A Philadelphia PA 19104-6228 USA	appos_PA_USA num_PA_19104-6228 nn_400A_Suite nn_Street_Walnut num_Street_3401 appos_University_PA appos_University_Philadelphia appos_University_400A appos_University_Street prep_of_University_Pennsylvania nn_Science_Cognitive prep_in_Institute_Science prep_for_Institute_Research nn_Institute_word amod_Institute_one-million det_Institute_a dobj_contains_Institute nsubj_contains_which dep_al._1994 nn_al._et nn_al._Marcus num_Marcinkiewicz_1993 appos_Marcus_University rcmod_Marcus_contains dep_Marcus_al. conj_and_Marcus_Marcinkiewicz conj_and_Marcus_Santorini dep_Treebank_Marcinkiewicz dep_Treebank_Santorini dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the cop_Treebank_is nsubj_Treebank_One amod_efforts_such amod_efforts_earliest amod_efforts_largest det_efforts_the conj_and_largest_earliest prep_of_One_efforts
J08-3003	J93-2004	o	Whereas most of the work on English has been based on constituency-based representations partly inuenced by the availability of data resources such as the Penn Treebank -LRB- Marcus Santorini and Marcinkiewicz 1993 -RRB- it has been argued that free constituent order languages can be analyzed more adequately using dependency-based representations which is also the kind of annotation found for example in the Prague Dependency Treebank of Czech -LRB- Haji c et al. 2001 -RRB-	advmod_2001_al. nn_al._et num_c_2001 nn_c_Haji appos_Czech_c prep_of_Treebank_Czech nn_Treebank_Dependency nn_Treebank_Prague det_Treebank_the vmod_kind_found prep_of_kind_annotation det_kind_the advmod_kind_also cop_kind_is nsubj_kind_which prep_in_representations_Treebank prep_for_representations_example rcmod_representations_kind amod_representations_dependency-based dobj_using_representations advmod_using_adequately advmod_adequately_more xcomp_analyzed_using auxpass_analyzed_be aux_analyzed_can nsubjpass_analyzed_languages mark_analyzed_that nn_languages_order nn_languages_constituent amod_languages_free ccomp_argued_analyzed auxpass_argued_been aux_argued_has nsubjpass_argued_it vmod_argued_inuenced advcl_argued_based num_Marcinkiewicz_1993 conj_and_Marcus_Marcinkiewicz conj_and_Marcus_Santorini dep_Treebank_Marcinkiewicz dep_Treebank_Santorini dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_such_as_resources_Treebank nn_resources_data prep_of_availability_resources det_availability_the agent_inuenced_availability advmod_inuenced_partly amod_representations_constituency-based prep_on_based_representations auxpass_based_been aux_based_has nsubjpass_based_most mark_based_Whereas det_work_the prep_on_most_English prep_of_most_work
J08-4003	J93-2004	o	Dependency Treebank -LRB- Hajic et al. 2001 Bohmova et al. 2003 -RRB- and in Figure 2 for an English sentence taken from the Penn Treebank -LRB- Marcus Santorini and Marcinkiewicz 1993 Marcus et al. 1994 -RRB-	dep_al._1994 nn_al._et nn_al._Marcus num_Marcinkiewicz_1993 dep_Marcus_al. conj_and_Marcus_Marcinkiewicz conj_and_Marcus_Santorini dep_Treebank_Marcinkiewicz dep_Treebank_Santorini dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_from_taken_Treebank vmod_sentence_taken amod_sentence_English det_sentence_an prep_for_,_sentence num_Figure_2 pobj_in_Figure dep_2003_al. nn_al._et num_Bohmova_2003 dep_Bohmova_Hajic num_al._2001 nn_al._et dep_Hajic_al. conj_and_Treebank_in appos_Treebank_Bohmova nn_Treebank_Dependency ccomp_``_in ccomp_``_Treebank
J94-4005	J93-2004	o	Lexical collocation functions especially those determined statistically have recently attracted considerable attention in computational linguistics -LRB- Calzolari and Bindi 1990 Church and Hanks 1990 Sekine et al. 1992 Hindle and Rooth 1993 -RRB- mainly though not exclusively for use in disambiguation	prep_in_use_disambiguation pobj_for_use ccomp_,_for neg_exclusively_not mark_exclusively_though advcl_,_exclusively num_Rooth_1993 conj_and_Hindle_Rooth dep_al._1992 nn_al._et nn_al._Sekine num_Church_1990 conj_and_Church_Hanks num_Bindi_1990 dep_Calzolari_mainly dep_Calzolari_Rooth dep_Calzolari_Hindle conj_and_Calzolari_al. conj_and_Calzolari_Hanks conj_and_Calzolari_Church conj_and_Calzolari_Bindi dep_linguistics_al. dep_linguistics_Church dep_linguistics_Bindi dep_linguistics_Calzolari amod_linguistics_computational amod_attention_considerable prep_in_attracted_linguistics dobj_attracted_attention advmod_attracted_recently aux_attracted_have nsubj_attracted_functions advmod_determined_statistically vmod_those_determined advmod_those_especially appos_functions_those nn_functions_collocation amod_functions_Lexical
J94-4005	J93-2004	o	More specifically the work on optimizing preference factors and semantic collocations was done as part of a project on spoken language translation in which the CLE was used for analysis and generation of both English and Swedish -LRB- AgnSs et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_AgnSs_1993 dep_English_AgnSs conj_and_English_Swedish preconj_English_both prep_of_analysis_Swedish prep_of_analysis_English conj_and_analysis_generation prep_for_used_generation prep_for_used_analysis auxpass_used_was nsubjpass_used_CLE prep_in_used_which det_CLE_the rcmod_translation_used nn_translation_language amod_translation_spoken prep_on_project_translation det_project_a prep_of_part_project prep_as_done_part auxpass_done_was nsubjpass_done_work advmod_done_specifically amod_collocations_semantic conj_and_factors_collocations nn_factors_preference amod_factors_optimizing prep_on_work_collocations prep_on_work_factors det_work_the dep_specifically_More ccomp_``_done
J95-2001	J93-2004	o	Rule-based taggers -LRB- Brill 1992 Elenius 1990 Jacobs and Zernik 1988 Karlsson 1990 Karlsson et al. 1991 Voutilainen Heikkila and Antitila 1992 Voutilainen and Tapanainen 1993 -RRB- use POS-dependent constraints defined by experienced linguists	amod_linguists_experienced agent_defined_linguists vmod_constraints_defined amod_constraints_POS-dependent dobj_use_constraints nsubj_use_taggers num_Tapanainen_1993 conj_and_Voutilainen_Tapanainen num_Antitila_1992 conj_and_Voutilainen_Antitila conj_and_Voutilainen_Heikkila dep_al._1991 nn_al._et nn_al._Karlsson num_Karlsson_1990 num_Zernik_1988 conj_and_Jacobs_Zernik num_Elenius_1990 dep_Brill_Tapanainen dep_Brill_Voutilainen dep_Brill_Antitila dep_Brill_Heikkila dep_Brill_Voutilainen dep_Brill_al. dep_Brill_Karlsson dep_Brill_Zernik dep_Brill_Jacobs dep_Brill_Elenius num_Brill_1992 appos_taggers_Brill amod_taggers_Rule-based
J95-2001	J93-2004	o	Stochastic taggers use both contextual and morphological information and the model parameters are usually defined or updated automatically from tagged texts -LRB- Cerf-Danon and E1-Beze 1991 Church 1988 Cutting et al. 1992 Dermatas and Kokkinakis 1988 1990 1993 1994 Garside Leech and Sampson 1987 Kupiec 1992 Maltese * Department of Electrical Engineering Wire Communications Laboratory -LRB- WCL -RRB- University of Patras 265 00 Patras Greece	appos_Patras_Greece num_Patras_00 number_00_265 appos_University_Patras prep_of_University_Patras appos_Laboratory_WCL nn_Laboratory_Communications nn_Laboratory_Wire amod_Engineering_Electrical appos_Department_University conj_Department_Laboratory prep_of_Department_Engineering dep_Department_* amod_Department_Maltese num_Kupiec_1992 num_Sampson_1987 conj_and_Garside_Sampson conj_and_Garside_Leech num_Kokkinakis_1994 num_Kokkinakis_1993 num_Kokkinakis_1990 num_Kokkinakis_1988 conj_and_Dermatas_Department conj_and_Dermatas_Kupiec conj_and_Dermatas_Sampson conj_and_Dermatas_Leech conj_and_Dermatas_Garside conj_and_Dermatas_Kokkinakis num_al._1992 nn_al._et dep_Cutting_Department dep_Cutting_Kupiec dep_Cutting_Garside dep_Cutting_Kokkinakis dep_Cutting_Dermatas dobj_Cutting_al. num_Church_1988 amod_1991_E1-Beze dep_Cerf-Danon_Cutting conj_and_Cerf-Danon_Church conj_and_Cerf-Danon_1991 dep_texts_Church dep_texts_1991 dep_texts_Cerf-Danon amod_texts_tagged nsubjpass_updated_parameters prep_from_defined_texts advmod_defined_automatically conj_or_defined_updated advmod_defined_usually auxpass_defined_are nsubjpass_defined_parameters nn_parameters_model det_parameters_the amod_information_morphological amod_information_contextual conj_and_contextual_morphological preconj_contextual_both conj_and_use_updated conj_and_use_defined dobj_use_information nsubj_use_taggers nn_taggers_Stochastic
J95-2001	J93-2004	o	-LRB- ~ -RRB- 1995 Association for Computational Linguistics Computational Linguistics Volume 21 Number 2 and Mancini 1991 Meteer Schwartz and Weischedel 1991 Merialdo 1991 Pelillo Moro and Refice 1992 Weischedel et al. 1993 Wothke et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Wothke num_al._1993 nn_al._et nn_al._Weischedel num_Refice_1992 conj_and_Pelillo_Refice conj_and_Pelillo_Moro num_Merialdo_1991 num_Weischedel_1991 conj_and_Meteer_Weischedel conj_and_Meteer_Schwartz num_Mancini_1991 conj_and_Number_Mancini num_Number_2 num_Volume_21 nn_Volume_Linguistics nn_Volume_Computational nn_Volume_Linguistics nn_Volume_Computational dep_Association_al. dep_Association_al. dep_Association_Refice dep_Association_Moro dep_Association_Pelillo dep_Association_Merialdo dep_Association_Weischedel dep_Association_Schwartz dep_Association_Meteer dep_Association_Mancini dep_Association_Number prep_for_Association_Volume num_Association_1995 appos_Association_~
J95-2001	J93-2004	o	Recently several solutions to the problem of tagging unknown words have been presented -LRB- Charniak et al. 1993 Meteer Schwartz and Weischedel 1991 -RRB-	num_Weischedel_1991 conj_and_Schwartz_Weischedel nn_Schwartz_Meteer dep_Charniak_Weischedel dep_Charniak_Schwartz dep_Charniak_1993 dep_Charniak_al. nn_Charniak_et dep_presented_Charniak auxpass_presented_been aux_presented_have nsubjpass_presented_solutions advmod_presented_Recently amod_words_unknown amod_words_tagging prep_of_problem_words det_problem_the prep_to_solutions_problem amod_solutions_several
J95-2001	J93-2004	o	Hypotheses for unknown words both stochastic -LRB- Dermatas and Kokkinakis 1993 1994 Maltese and Mancini 1991 Weischedel et al. 1993 -RRB- and connectionist -LRB- Eineborg and Gamback 1993 Elenius 1990 -RRB- have been applied to unlimited vocabulary taggers	nn_taggers_vocabulary amod_taggers_unlimited prep_to_applied_taggers auxpass_applied_been aux_applied_have nsubjpass_applied_connectionist nsubjpass_applied_both ccomp_applied_Hypotheses num_Elenius_1990 num_Gamback_1993 dep_Eineborg_Elenius conj_and_Eineborg_Gamback appos_connectionist_Gamback appos_connectionist_Eineborg dep_al._1993 nn_al._et nn_al._Weischedel num_Mancini_1991 conj_and_Maltese_Mancini num_Kokkinakis_1994 num_Kokkinakis_1993 dep_Dermatas_al. conj_and_Dermatas_Mancini conj_and_Dermatas_Maltese conj_and_Dermatas_Kokkinakis dep_stochastic_Maltese dep_stochastic_Kokkinakis dep_stochastic_Dermatas conj_and_both_connectionist amod_both_stochastic amod_words_unknown prep_for_Hypotheses_words
J95-2001	J93-2004	o	When the training text is adequate to estimate the tagger parameters more efficient stochastic taggers -LRB- Dermatas and Kokkinakis 1994 Maltese and Mancini 1991 Weischedel et al. 1993 -RRB- and training methods can be implemented -LRB- Merialdo 1994 -RRB-	num_Merialdo_1994 dep_implemented_Merialdo auxpass_implemented_be aux_implemented_can nsubjpass_implemented_methods nsubjpass_implemented_al. nsubjpass_implemented_Maltese nsubjpass_implemented_Kokkinakis nsubjpass_implemented_Dermatas nn_methods_training dep_al._1993 nn_al._et nn_al._Weischedel num_Mancini_1991 conj_and_Maltese_Mancini num_Kokkinakis_1994 conj_and_Dermatas_methods conj_and_Dermatas_al. conj_and_Dermatas_Mancini conj_and_Dermatas_Maltese conj_and_Dermatas_Kokkinakis rcmod_taggers_implemented amod_taggers_stochastic amod_taggers_efficient advcl_taggers_adequate advmod_efficient_more nn_parameters_tagger det_parameters_the dobj_estimate_parameters aux_estimate_to xcomp_adequate_estimate cop_adequate_is nsubj_adequate_text advmod_adequate_When nn_text_training det_text_the ccomp_``_taggers
J95-4004	J93-2004	o	A number of part-of-speech taggers are readily available and widely used all trained and retrainable on text corpora -LRB- Church 1988 Cutting et al. 1992 Brill 1992 Weischedel et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Weischedel dep_Brill_al. num_Brill_1992 num_al._1992 nn_al._et dep_Cutting_Brill dobj_Cutting_al. dep_Church_Cutting num_Church_1988 dep_corpora_Church nn_corpora_text prep_on_trained_corpora conj_and_trained_retrainable dep_trained_all advmod_used_widely nsubj_used_number dep_available_retrainable dep_available_trained conj_and_available_used advmod_available_readily cop_available_are nsubj_available_number amod_taggers_part-of-speech prep_of_number_taggers det_number_A
J95-4004	J93-2004	o	Endemic structural ambiguity which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence can be greatly reduced by adding empirically derived probabilities to grammar rules -LRB- Fujisaki et al. 1989 Sharman Jelinek and Mercer 1990 Black et al. 1993 -RRB- and by computing statistical measures of lexical association -LRB- Hindle and Rooth 1993 -RRB-	num_Rooth_1993 conj_and_Hindle_Rooth dep_association_Rooth dep_association_Hindle amod_association_lexical prep_of_measures_association amod_measures_statistical dobj_computing_measures pcomp_by_computing dep_al._1993 nn_al._et nn_al._Black num_Mercer_1990 conj_and_Sharman_Mercer conj_and_Sharman_Jelinek num_al._1989 nn_al._et dep_Fujisaki_al. dep_Fujisaki_Mercer dep_Fujisaki_Jelinek dep_Fujisaki_Sharman dep_Fujisaki_al. nn_rules_grammar prep_to_probabilities_rules amod_probabilities_derived advmod_derived_empirically dobj_adding_probabilities conj_and_reduced_by dep_reduced_Fujisaki agent_reduced_adding advmod_reduced_greatly auxpass_reduced_be aux_reduced_can nsubjpass_reduced_ambiguity det_sentence_a prep_to_assign_sentence aux_assign_can nsubj_assign_grammar mark_assign_that det_grammar_a amod_parses_possible prep_of_thousands_parses amod_thousands_many det_thousands_the ccomp_cope_assign prep_with_cope_thousands aux_cope_to xcomp_trying_cope prepc_as_difficulties_trying amod_difficulties_such prep_to_lead_difficulties aux_lead_can nsubj_lead_which rcmod_ambiguity_lead amod_ambiguity_structural amod_ambiguity_Endemic
J95-4004	J93-2004	o	Part-of-speech tagging is an active area of research a great deal of work has been done in this area over the past few years -LRB- e.g. Jelinek 1985 Church 1988 Derose 1988 Hindle 1989 DeMarcken 1990 Merialdo 1994 Brill 1992 Black et al. 1992 Cutting et al. 1992 Kupiec 1992 Charniak et al. 1993 Weischedel et al. 1993 Schutze and Singer 1994 -RRB-	num_Singer_1994 conj_and_Schutze_Singer num_al._1993 nn_al._et nn_al._Weischedel num_al._1993 nn_al._et nn_al._Charniak num_Kupiec_1992 nn_al._et parataxis_Cutting_Singer parataxis_Cutting_Schutze conj_Cutting_al. conj_Cutting_al. conj_Cutting_Kupiec tmod_Cutting_1992 dobj_Cutting_al. nn_al._et nn_al._Black num_Brill_1992 num_Merialdo_1994 num_DeMarcken_1990 num_Hindle_1989 num_Derose_1988 num_Church_1988 num_Jelinek_1985 dep_e.g._1992 dep_e.g._al. dep_e.g._Brill dep_e.g._Merialdo dep_e.g._DeMarcken dep_e.g._Hindle dep_e.g._Derose dep_e.g._Church dep_e.g._Jelinek amod_years_few amod_years_past det_years_the det_area_this dep_done_e.g. prep_over_done_years prep_in_done_area auxpass_done_been aux_done_has nsubjpass_done_deal prep_of_deal_work amod_deal_great det_deal_a dep_area_Cutting parataxis_area_done prep_of_area_research amod_area_active det_area_an cop_area_is nsubj_area_tagging amod_tagging_Part-of-speech
J95-4004	J93-2004	o	Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging -LRB- Jelinek 1985 Church 1988 Derose 1988 DeMarcken 1990 Merialdo 1994 Cutting et al. 1992 Kupiec 1992 Charniak et al. 1993 Weischedel et al. 1993 Schutze and Singer 1994 -RRB-	num_Singer_1994 conj_and_Schutze_Singer num_al._1993 nn_al._et nn_al._Weischedel num_al._1993 nn_al._et nn_al._Charniak dep_Kupiec_Singer dep_Kupiec_Schutze conj_Kupiec_al. conj_Kupiec_al. num_Kupiec_1992 num_al._1992 nn_al._et dep_Cutting_Kupiec dobj_Cutting_al. num_Merialdo_1994 num_DeMarcken_1990 num_Derose_1988 num_Church_1988 dep_Jelinek_Cutting dep_Jelinek_Merialdo dep_Jelinek_DeMarcken dep_Jelinek_Derose dep_Jelinek_Church num_Jelinek_1985 dep_tagging_Jelinek amod_tagging_based nn_tagging_Markovmodel dobj_exploring_tagging xcomp_further_exploring prep_on_been_further aux_been_has nsubj_been_work amod_taggers_part-of-speech amod_taggers_trained advmod_trained_automatically dobj_developing_taggers prepc_in_work_developing amod_work_recent det_work_all advmod_work_Almost
J97-3003	J93-2004	o	Such methods can achieve better performance reaching tagging accuracy of up to 85 % on unknown words for English -LRB- Brill 1994 Weischedel et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Weischedel dep_Brill_al. num_Brill_1994 dep_English_Brill prep_for_words_English amod_words_unknown num_%_85 dep_85_to quantmod_85_up prep_of_accuracy_% amod_accuracy_tagging prep_on_reaching_words dobj_reaching_accuracy amod_performance_better vmod_achieve_reaching dobj_achieve_performance aux_achieve_can nsubj_achieve_methods amod_methods_Such
J97-3003	J93-2004	o	It has been noticed -LRB- as in \ -LSB- Weischedel et al. 1993 \ -RSB- for example -RRB- that capitalized and hyphenated words have a different distribution from other words	amod_words_other prep_from_distribution_words amod_distribution_different det_distribution_a dobj_have_distribution nsubj_have_words amod_words_hyphenated conj_and_capitalized_have nsubj_capitalized_that num_\_1993 appos_Weischedel_\ dep_Weischedel_al. nn_Weischedel_et pobj_in_\ pcomp_as_in ccomp_noticed_have ccomp_noticed_capitalized prep_for_noticed_example dep_noticed_Weischedel prep_noticed_as auxpass_noticed_been aux_noticed_has nsubjpass_noticed_It
J98-2001	J93-2004	o	In the past two or three years this kind of verification has been attempted for other aspects of semantic interpretation by Passonneau and Litman -LRB- 1993 -RRB- for segmentation and by Kowtko Isard and Doherty -LRB- 1992 -RRB- and Carletta et al.	nn_al._et nn_al._Carletta appos_Doherty_1992 conj_and_Kowtko_al. conj_and_Kowtko_Doherty conj_and_Kowtko_Isard pobj_by_al. pobj_by_Doherty pobj_by_Isard pobj_by_Kowtko conj_and_for_by pobj_for_segmentation appos_Litman_1993 conj_and_Passonneau_Litman amod_interpretation_semantic prep_of_aspects_interpretation amod_aspects_other dep_attempted_by dep_attempted_for agent_attempted_Litman agent_attempted_Passonneau prep_for_attempted_aspects auxpass_attempted_been aux_attempted_has nsubjpass_attempted_kind prep_in_attempted_years prep_of_kind_verification det_kind_this num_years_three num_years_two amod_years_past det_years_the conj_or_two_three
J98-2002	J93-2004	o	Some of these methods make use of prior knowledge in the form of an existing thesaurus -LRB- Resnik 1993a 1993b Framis 1994 Almuallim et al. 1994 Tanaka 1996 Utsuro and Matsumoto 1997 -RRB- while others do not rely on any prior knowledge -LRB- Pereira Tishby and Lee 1993 Grishman and Sterling 1994 Tanaka 1994 -RRB-	num_Tanaka_1994 num_Sterling_1994 conj_and_Grishman_Sterling num_Lee_1993 dep_Pereira_Tanaka conj_and_Pereira_Sterling conj_and_Pereira_Grishman conj_and_Pereira_Lee conj_and_Pereira_Tishby appos_knowledge_Grishman appos_knowledge_Lee appos_knowledge_Tishby appos_knowledge_Pereira amod_knowledge_prior det_knowledge_any prep_on_rely_knowledge neg_rely_not aux_rely_do nsubj_rely_others mark_rely_while num_Matsumoto_1997 conj_and_Utsuro_Matsumoto num_Tanaka_1996 num_al._1994 nn_al._et nn_al._Almuallim num_Framis_1994 dep_1993a_Matsumoto dep_1993a_Utsuro dep_1993a_Tanaka dep_1993a_al. dep_1993a_Framis appos_1993a_1993b nn_1993a_Resnik appos_thesaurus_1993a amod_thesaurus_existing det_thesaurus_an prep_of_form_thesaurus det_form_the amod_knowledge_prior prep_in_use_form prep_of_use_knowledge advcl_make_rely dobj_make_use nsubj_make_Some det_methods_these prep_of_Some_methods
J98-2002	J93-2004	o	The second approach -LRB- Sekine et al. 1992 Chang Luo and Su 1992 Resnik 1993a Grishman and Sterling 1994 Alshawi and Carter 1994 -RRB- takes triples -LRB- verb prep noun2 -RRB- and -LRB- nounl prep noun2 -RRB- like those in Table 10 as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples	nn_disambiguation_PP-attachment prep_on_performs_quadruples dobj_performs_disambiguation amod_knowledge_semantic dobj_acquiring_knowledge prepc_for_data_acquiring nn_data_training num_Table_10 prep_in_those_Table appos_nounl_noun2 appos_nounl_prep conj_and_verb_performs prep_as_verb_data prep_like_verb_those conj_and_verb_nounl dep_verb_noun2 dobj_verb_prep dep_takes_performs dep_takes_nounl dep_takes_verb dobj_takes_triples nsubj_takes_approach num_Carter_1994 conj_and_Alshawi_Carter num_Sterling_1994 conj_and_Grishman_Sterling nn_1993a_Resnik num_Su_1992 conj_and_Chang_Su conj_and_Chang_Luo dep_al._1992 nn_al._et dep_Sekine_Carter dep_Sekine_Alshawi conj_Sekine_Sterling conj_Sekine_Grishman conj_Sekine_1993a dep_Sekine_Su dep_Sekine_Luo dep_Sekine_Chang advmod_Sekine_al. appos_approach_Sekine amod_approach_second det_approach_The ccomp_``_takes
J99-2004	J93-2004	o	2.2 Statistical Parsers Pioneered by the IBM natural language group -LRB- Fujisaki et al. 1989 -RRB- and later pursued by for example Schabes Roth and Osborne -LRB- 1993 -RRB- Jelinek et al.	nn_al._et nn_al._Jelinek appos_Osborne_1993 dep_Schabes_al. conj_and_Schabes_Osborne conj_and_Schabes_Roth dep_,_Osborne dep_,_Roth dep_,_Schabes prep_for_,_example dep_pursued_by advmod_pursued_later dep_1989_al. nn_al._et num_Fujisaki_1989 nn_group_language amod_group_natural nn_group_IBM det_group_the agent_Pioneered_group conj_and_Parsers_pursued appos_Parsers_Fujisaki vmod_Parsers_Pioneered amod_Parsers_Statistical num_Parsers_2.2 dep_``_pursued dep_``_Parsers
J99-2004	J93-2004	o	Supertags Part-of-speech disambiguation techniques -LRB- POS taggers -RRB- -LRB- Church 1988 Weischedel et al. 1993 Brill 1993 -RRB- are often used prior to parsing to eliminate -LRB- or substantially reduce -RRB- the part-of-speech ambiguity	amod_ambiguity_part-of-speech det_ambiguity_the advmod_reduce_substantially cc_reduce_or dobj_eliminate_ambiguity dep_eliminate_reduce aux_eliminate_to xcomp_parsing_eliminate prepc_prior_to_used_parsing advmod_used_often auxpass_used_are nsubjpass_used_techniques num_Brill_1993 num_al._1993 nn_al._et nn_al._Weischedel dep_Church_Brill dep_Church_al. num_Church_1988 nn_taggers_POS appos_techniques_Church appos_techniques_taggers nn_techniques_disambiguation amod_techniques_Part-of-speech nn_techniques_Supertags
J99-4003	J93-2004	o	These are the same distributions that are needed by previous POS-based language models -LRB- Equation 5 -RRB- and POS taggers -LRB- Church 1988 Charniak et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Charniak dep_Church_al. num_Church_1988 appos_taggers_Church nn_taggers_POS num_Equation_5 conj_and_models_taggers appos_models_Equation nn_models_language amod_models_POS-based amod_models_previous agent_needed_taggers agent_needed_models auxpass_needed_are nsubjpass_needed_that rcmod_distributions_needed amod_distributions_same det_distributions_the cop_distributions_are nsubj_distributions_These
J99-4003	J93-2004	o	This source is very important for repairs that do not have initial retracing and is the mainstay of the parser-first approach -LRB- e.g. 550 Heeman and Allen Modeling Speakers ' Utterances Dowding et al. 1993 -RRB- -- keep trying alternative corrections until one of them parses	nsubj_parses_one mark_parses_until prep_of_one_them amod_corrections_alternative dobj_trying_corrections advcl_keep_parses xcomp_keep_trying dep_al._1993 nn_al._et advmod_Dowding_al. amod_Utterances_Dowding poss_Utterances_Speakers nn_Speakers_Modeling nn_Speakers_Allen conj_and_Heeman_Utterances num_Heeman_550 dep_e.g._Utterances dep_e.g._Heeman dep_approach_e.g. amod_approach_parser-first det_approach_the prep_of_mainstay_approach det_mainstay_the cop_mainstay_is amod_retracing_initial dobj_have_retracing neg_have_not aux_have_do nsubj_have_that rcmod_repairs_have dep_important_keep conj_and_important_mainstay prep_for_important_repairs advmod_important_very cop_important_is nsubj_important_source det_source_This
J99-4003	J93-2004	o	In a test set containing 26 repairs Dowding et al. 1993 they obtained a detection recall rate of 42 % with a precision of 85 % and a correction recall rate of 31 % with a precision of 62 %	num_%_62 prep_of_precision_% det_precision_a num_%_31 prep_with_rate_precision prep_of_rate_% nn_rate_recall nn_rate_correction det_rate_a num_%_85 prep_of_precision_% det_precision_a num_%_42 prep_of_rate_% nn_rate_recall nn_rate_detection det_rate_a conj_and_obtained_rate prep_with_obtained_precision dobj_obtained_rate nsubj_obtained_they prep_in_obtained_set num_al._1993 dep_Dowding_al. nn_Dowding_et amod_repairs_Dowding num_repairs_26 dobj_containing_repairs vmod_set_containing nn_set_test det_set_a
L08-1018	J93-2004	o	http://duc.nist.gov </title> <date> 2004 </date> <journal> Journal of the Association for Computing Machinery </journal> <volume> 16 </volume> <pages> 264 -- 285 </pages> <contexts> <context> -LRB- Voorhees and Harman 1999 -RRB- Message Understanding Conferences -LRB- MUC -RRB- -LRB- Chinchor et al 1993 -RRB- TIPSTER SUMMAC Text Summarization Evaluation -LRB- Mani et al 1998 -RRB- Document Understanding Conference -LRB- DUC -RRB- -LRB- DUC 2004 -RRB- and Text Summarization Challenge -LRB- TSC -RRB- -LRB- Fukushima and Okumura 2001 -RRB- have attested the importance of this topic	det_topic_this prep_of_importance_topic det_importance_the dobj_attested_importance aux_attested_have dep_Fukushima_2001 conj_and_Fukushima_Okumura appos_Challenge_Okumura appos_Challenge_Fukushima appos_Challenge_TSC nn_Challenge_Summarization dobj_Text_Challenge dep_DUC_2004 appos_Conference_DUC appos_Conference_DUC dobj_Understanding_Conference vmod_Document_Understanding amod_Mani_1998 dep_Mani_al nn_Mani_et nn_Evaluation_Summarization dobj_Text_Evaluation nsubj_Text_SUMMAC nn_SUMMAC_TIPSTER amod_Chinchor_1993 dep_Chinchor_al nn_Chinchor_et appos_Conferences_MUC dobj_Understanding_Conferences dep_Message_Mani rcmod_Message_Text appos_Message_Chinchor vmod_Message_Understanding dep_Voorhees_1999 conj_and_Voorhees_Harman conj_and_<context>_Text appos_<context>_Document appos_<context>_Message appos_<context>_Harman appos_<context>_Voorhees nn_<context>_<contexts> nn_<context>_</pages> num_<context>_285 dep_<pages>_Text dep_<pages>_<context> num_<pages>_264 nn_<pages>_</volume> num_<pages>_16 nn_<pages>_<volume> nn_<pages>_</journal> nn_<pages>_Machinery nn_<pages>_Computing prep_for_Association_<pages> det_Association_the prep_of_Journal_Association nn_Journal_<journal> amod_Journal_</date> num_Journal_2004 dep_<date>_attested dobj_<date>_Journal nsubj_<date>_</title> nn_</title>_http://duc.nist.gov
L08-1018	J93-2004	o	264-285 </rawString> </citation> < citation valid = true > <authors> <author> T Fukushima </author> <author> M Okumura </author> </authors> <title> Text summarization challenge text summarization in Japan </title> <date> 2001 </date> <booktitle> in Proceedings of NAACL 2001 Workshop Automatic Summarization </booktitle> <pages> 51 -- 59 </pages> <contexts> <context> Conferences -LRB- MUC -RRB- -LRB- Chinchor et al 1993 -RRB- TIPSTER SUMMAC Text Summarization Evaluation -LRB- Mani et al 1998 -RRB- Document Understanding Conference -LRB- DUC -RRB- -LRB- DUC 2004 -RRB- and Text Summarization Challenge -LRB- TSC -RRB- -LRB- Fukushima and Okumura 2001 -RRB- have attested the importance of this topic	det_topic_this prep_of_importance_topic det_importance_the dobj_attested_importance aux_attested_have dep_Fukushima_2001 conj_and_Fukushima_Okumura appos_Challenge_Okumura appos_Challenge_Fukushima appos_Challenge_TSC nn_Challenge_Summarization dobj_Text_Challenge nsubj_Text_summarization dep_DUC_2004 appos_Conference_DUC appos_Conference_DUC dobj_Understanding_Conference vmod_Document_Understanding amod_Mani_1998 dep_Mani_al nn_Mani_et nn_Evaluation_Summarization dobj_Text_Evaluation nsubj_Text_SUMMAC nn_SUMMAC_TIPSTER amod_Chinchor_1993 dep_Chinchor_al nn_Chinchor_et dep_Conferences_Chinchor appos_Conferences_MUC nn_Conferences_<context> nn_Conferences_<contexts> nn_Conferences_</pages> num_Conferences_59 rcmod_<pages>_Text dep_<pages>_Conferences num_<pages>_51 nn_<pages>_</booktitle> nn_<pages>_Summarization nn_<pages>_Automatic nn_<pages>_Workshop num_<pages>_2001 nn_<pages>_NAACL prep_of_Proceedings_<pages> amod_<booktitle>_</date> num_<booktitle>_2001 conj_and_<date>_Text dep_<date>_Document dep_<date>_Mani prep_in_<date>_Proceedings dobj_<date>_<booktitle> nsubj_<date>_summarization nn_</title>_Japan prep_in_summarization_</title> nn_summarization_text dep_challenge_Text dep_challenge_<date> nn_challenge_summarization dep_Text_attested dobj_Text_challenge nsubj_Text_<title> nn_<title>_</authors> nn_<title>_</author> nn_<title>_Okumura nn_<title>_M nn_<title>_<author> nn_<title>_</author> nn_<title>_Fukushima nn_<title>_T nn_<title>_<author> nn_<title>_<authors> amod_<title>_> amod_<title>_true dep_true_= dep_true_valid dep_true_citation dep_true_< dep_true_</citation> dep_true_</rawString> dep_true_264-285
L08-1018	J93-2004	o	Statistics in linguistics Oxford. Basil Blackwell </rawString> </citation> < citation valid = true > <authors> <author> N Chinchor </author> </authors> <title> Evaluating message understanding systems an analysis of the third Message Understanding Conference -LRB- MUC-3 </title> <date> 1993 </date> <journal> Computational Linguistics </journal> <volume> 19 </volume> <pages> 409 -- 449 </pages> <marker> Chinchor 1993 </marker> <rawString> Chinchor N. et al 1993	dep_al_1993 nn_al_et advmod_Chinchor_al appos_Chinchor_N. nn_Chinchor_<rawString> nn_Chinchor_</marker> num_Chinchor_1993 appos_Chinchor_Chinchor nn_Chinchor_<marker> nn_Chinchor_</pages> num_Chinchor_449 dep_<pages>_Chinchor num_<pages>_409 nn_<pages>_</volume> num_<pages>_19 nn_<pages>_<volume> nn_<pages>_</journal> nn_<pages>_Linguistics nn_<pages>_Computational nn_<pages>_<journal> amod_<pages>_</date> num_<pages>_1993 dobj_<date>_<pages> nsubj_<date>_</title> nn_</title>_MUC-3 nn_</title>_Conference ccomp_Understanding_<date> vmod_Message_Understanding amod_Message_third det_Message_the prep_of_analysis_Message det_analysis_an nn_systems_understanding nn_systems_message dobj_Evaluating_systems dep_<title>_analysis vmod_<title>_Evaluating nn_<title>_</authors> nn_<title>_</author> nn_<title>_Chinchor nn_<title>_N nn_<title>_<author> nn_<title>_<authors> amod_<title>_> amod_<title>_true dep_true_= dep_true_valid dep_true_citation dep_true_< dep_true_</citation> dep_true_</rawString> dep_true_Blackwell nn_Blackwell_Basil appos_linguistics_Oxford. dep_Statistics_<title> prep_in_Statistics_linguistics
L08-1018	J93-2004	o	In acknowledgment of this fact a series of conferences like Text Retrieval Conferences -LRB- TREC -RRB- -LRB- Voorhees and Harman 1999 -RRB- Message Understanding Conferences -LRB- MUC -RRB- -LRB- Chinchor et al 1993 -RRB- TIPSTER SUMMAC Text Summarization Evaluation -LRB- Mani et al 1998 -RRB- Document Understanding Conference -LRB- DUC -RRB- -LRB- DUC 2004 -RRB- and Text Summar </context> </contexts> <marker> Voorhees Harman 1999 </marker> <rawString> Voorhees E. M. and Harman D. K. 1999	nn_K._D. nn_M._E. nn_Voorhees_<rawString> nn_Voorhees_</marker> num_Voorhees_1999 dep_Harman_1999 conj_and_Harman_K. conj_and_Harman_Harman conj_and_Harman_M. appos_Harman_Voorhees appos_Voorhees_K. appos_Voorhees_Harman appos_Voorhees_M. appos_Voorhees_Harman nn_Voorhees_<marker> nn_Voorhees_</contexts> nn_Voorhees_</context> amod_Voorhees_Summar dobj_Text_Voorhees dep_DUC_2004 appos_Conference_DUC appos_Conference_DUC dobj_Understanding_Conference vmod_Document_Understanding amod_Mani_1998 dep_Mani_al nn_Mani_et nn_Evaluation_Summarization dobj_Text_Evaluation nsubj_Text_SUMMAC dep_TIPSTER_Mani rcmod_TIPSTER_Text amod_Chinchor_1993 dep_Chinchor_al nn_Chinchor_et appos_Conferences_MUC dobj_Understanding_Conferences appos_Message_Chinchor vmod_Message_Understanding dep_Voorhees_1999 conj_and_Voorhees_Harman conj_and_Conferences_Text conj_and_Conferences_Document conj_and_Conferences_TIPSTER appos_Conferences_Message appos_Conferences_Harman appos_Conferences_Voorhees appos_Conferences_TREC nn_Conferences_Retrieval dobj_Text_Text dobj_Text_Document dobj_Text_TIPSTER dobj_Text_Conferences prepc_like_series_Text prep_of_series_conferences det_series_a det_fact_this prep_of_acknowledgment_fact dep_``_series prep_in_``_acknowledgment
M98-1009	J93-2004	o	Training Data Our source for syntactically annotated training data was the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the cop_Treebank_was nsubj_Treebank_source nn_data_training amod_data_annotated advmod_data_syntactically prep_for_source_data poss_source_Our nn_source_Data nn_source_Training
N01-1023	J93-2004	o	6 Experiment 6.1 Setup The experiments we report were done on the Penn Treebank WSJ Corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Corpus_WSJ nn_Corpus_Treebank nn_Corpus_Penn det_Corpus_the prep_on_done_Corpus auxpass_done_were nsubjpass_done_experiments nsubj_report_we rcmod_experiments_report det_experiments_The dep_Setup_Marcus rcmod_Setup_done num_Setup_6.1 nn_Setup_Experiment num_Setup_6 dep_``_Setup
N01-1023	J93-2004	o	They train from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- a collection of 40,000 sentences that are labeled with corrected parse trees -LRB- approximately a million word tokens -RRB-	nn_tokens_word num_tokens_million det_tokens_a advmod_tokens_approximately appos_trees_tokens nn_trees_parse amod_trees_corrected prep_with_labeled_trees auxpass_labeled_are nsubjpass_labeled_that rcmod_sentences_labeled num_sentences_40,000 prep_of_collection_sentences det_collection_a amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_train_collection dep_train_Marcus prep_from_train_Treebank nsubj_train_They
N03-1013	J93-2004	o	3 Building the CatVar The CatVar database was developed using a combination of resources and algorithms including the Lexical Conceptual Structure -LRB- LCS -RRB- Verb and Preposition Databases -LRB- Dorr 2001 -RRB- the Brown Corpus section of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- an English morphological analysis lexicon developed for PC-Kimmo -LRB- Englex -RRB- -LRB- Antworth 1990 -RRB- NOMLEX -LRB- Macleod et al. 1998 -RRB- Longman Dictionary of Contemporary English 2For a deeper discussion and classification of Porter stemmers errors see -LRB- Krovetz 1993 -RRB-	amod_Krovetz_1993 dep_see_Krovetz nsubj_see_CatVar dobj_see_Building amod_errors_stemmers nn_errors_Porter prep_of_discussion_errors conj_and_discussion_classification amod_discussion_deeper det_discussion_a nn_2For_English nn_2For_Contemporary dep_Dictionary_classification dep_Dictionary_discussion prep_of_Dictionary_2For nn_Dictionary_Longman amod_Macleod_1998 dep_Macleod_al. nn_Macleod_et dep_NOMLEX_Macleod amod_Antworth_1990 appos_PC-Kimmo_Dictionary conj_PC-Kimmo_NOMLEX dep_PC-Kimmo_Antworth appos_PC-Kimmo_Englex prep_for_developed_PC-Kimmo nsubj_developed_section nn_lexicon_analysis amod_lexicon_morphological amod_lexicon_English det_lexicon_an amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the appos_section_lexicon dep_section_Marcus prep_of_section_Treebank nn_section_Corpus nn_section_Brown det_section_the amod_Dorr_2001 nn_Databases_Preposition dep_Verb_Dorr conj_and_Verb_Databases dep_Structure_Databases dep_Structure_Verb appos_Structure_LCS nn_Structure_Conceptual nn_Structure_Lexical det_Structure_the prep_including_resources_Structure conj_and_resources_algorithms prep_of_combination_algorithms prep_of_combination_resources det_combination_a dobj_using_combination xcomp_developed_using auxpass_developed_was nsubjpass_developed_database nn_database_CatVar det_database_The rcmod_CatVar_developed rcmod_CatVar_developed det_CatVar_the num_Building_3 ccomp_``_see
N03-1013	J93-2004	o	Resources specifying the relations among lexical items such as WordNet -LRB- Fellbaum 1998 -RRB- and HowNet -LRB- Dong 2000 -RRB- -LRB- among others -RRB- have inspired the work of many researchers in NLP -LRB- Carpuat et al. 2002 Dorr et al. 2000 Resnik 1999 Hearst 1998 Voorhees 1993 -RRB-	amod_Voorhees_1993 num_Hearst_1998 num_Resnik_1999 dep_Dorr_Voorhees conj_Dorr_Hearst conj_Dorr_Resnik num_Dorr_2000 nn_Dorr_al. nn_Dorr_et dep_Carpuat_Dorr appos_Carpuat_2002 dep_Carpuat_al. nn_Carpuat_et amod_researchers_many prep_in_work_NLP prep_of_work_researchers det_work_the dep_inspired_Carpuat dobj_inspired_work aux_inspired_have nsubj_inspired_Resources dep_Dong_2000 appos_HowNet_Dong amod_Fellbaum_1998 prep_among_WordNet_others conj_and_WordNet_HowNet dep_WordNet_Fellbaum prep_such_as_items_HowNet prep_such_as_items_WordNet amod_items_lexical prep_among_relations_items det_relations_the dobj_specifying_relations vmod_Resources_specifying ccomp_``_inspired
N03-1014	J93-2004	o	6 The Experimental Results We used the Penn Treebank -LRB- Marcus et al. 1993 -RRB- to perform empirical experiments on this parsing model	nn_model_parsing det_model_this amod_experiments_empirical prep_on_perform_model dobj_perform_experiments aux_perform_to amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the vmod_used_perform dobj_used_Treebank nsubj_used_We rcmod_Results_used amod_Results_Experimental det_Results_The num_Results_6 dep_``_Results
N03-1030	J93-2004	p	1 Introduction By exploiting information encoded in human-produced syntactic trees -LRB- Marcus et al. 1993 -RRB- research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90 % accuracy -LRB- Charniak 2000 Collins 2000 -RRB-	amod_Collins_2000 dep_Charniak_Collins dep_Charniak_2000 appos_accuracy_Charniak dep_%_accuracy num_%_90 quantmod_90_about amod_parsers_syntactic prep_of_performance_parsers det_performance_the prep_to_driven_% dobj_driven_performance aux_driven_has nsubj_driven_Introduction prep_of_models_syntax amod_models_probabilistic prep_on_research_models amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_trees_syntactic amod_trees_human-produced prep_in_encoded_trees vmod_information_encoded dobj_exploiting_information appos_Introduction_research dep_Introduction_Marcus prepc_by_Introduction_exploiting num_Introduction_1
N03-1031	J93-2004	o	1 Introduction Current state-of-the-art statistical parsers -LRB- Collins 1999 Charniak 2000 -RRB- are trained on large annotated corpora such as the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_such_as_corpora_Treebank amod_corpora_annotated amod_corpora_large prep_on_trained_corpora auxpass_trained_are nsubjpass_trained_parsers dep_Charniak_2000 dep_Collins_Charniak amod_Collins_1999 appos_parsers_Collins amod_parsers_statistical amod_parsers_state-of-the-art amod_parsers_Current nn_parsers_Introduction num_parsers_1 ccomp_``_trained
N03-1033	J93-2004	o	Secondly while all taggers use lexical information and indeed it is well-known that lexical probabilities are much more revealing than tag sequence probabilities -LRB- Charniak et al. 1993 -RRB- most taggers make quite limited use of lexical probabilities -LRB- compared with for example the bilexical probabilities commonly used in current statistical parsers -RRB-	amod_parsers_statistical amod_parsers_current prep_in_used_parsers advmod_used_commonly vmod_probabilities_used amod_probabilities_bilexical det_probabilities_the pobj_for_example conj_with_probabilities conj_with_for prep_compared_with vmod_probabilities_compared amod_probabilities_lexical prep_of_use_probabilities amod_use_limited advmod_limited_quite dobj_make_use nsubj_make_taggers advcl_make_revealing amod_taggers_most amod_Charniak_1993 dep_Charniak_al. nn_Charniak_et dep_probabilities_Charniak nn_probabilities_sequence nn_probabilities_tag prep_than_revealing_probabilities advmod_revealing_more cop_revealing_are nsubj_revealing_probabilities mark_revealing_that advmod_more_much amod_probabilities_lexical dep_well-known_make cop_well-known_is nsubj_well-known_it advmod_well-known_indeed amod_information_lexical conj_and_use_well-known dobj_use_information nsubj_use_taggers mark_use_while det_taggers_all advcl_,_well-known advcl_,_use dep_``_Secondly
N03-3006	J93-2004	o	The parser has been trained developed and tested on a large collection of syntactically analyzed sentences the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the dep_,_Treebank amod_sentences_analyzed advmod_analyzed_syntactically prep_of_collection_sentences amod_collection_large det_collection_a prep_on_developed_collection conj_and_developed_tested ccomp_trained_tested ccomp_trained_developed auxpass_trained_been aux_trained_has nsubjpass_trained_parser det_parser_The dep_``_trained
N04-1016	J93-2004	o	Table 6 shows 3An exception is Golding -LRB- 1995 -RRB- who uses the entire Brown corpus for training -LRB- 1M words -RRB- and 3/4 of the Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB- for testing	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the nn_words_1M prep_of_training_corpus conj_and_training_3/4 appos_training_words amod_corpus_Brown amod_corpus_entire det_corpus_the prep_for_uses_testing dep_uses_Marcus prep_for_uses_3/4 prep_for_uses_training dobj_uses_corpus nsubj_uses_who rcmod_Golding_uses appos_Golding_1995 cop_Golding_is nsubj_Golding_exception amod_exception_3An rcmod_shows_Golding num_shows_6 dep_Table_shows
N04-1016	J93-2004	o	6 Bracketing of Compound Nouns The first analysis task we consider is the syntactic disambiguation of compound nouns which has received a fair amount of attention in the NLP literature -LRB- Pustejovsky et al. 1993 Resnik 1993 Lauer 1995 -RRB-	amod_Lauer_1995 dep_Resnik_Lauer num_Resnik_1993 dep_Pustejovsky_Resnik appos_Pustejovsky_1993 dep_Pustejovsky_al. nn_Pustejovsky_et nn_literature_NLP det_literature_the prep_of_amount_attention amod_amount_fair det_amount_a prep_in_received_literature dobj_received_amount aux_received_has nsubj_received_which rcmod_nouns_received nn_nouns_compound dep_disambiguation_Pustejovsky prep_of_disambiguation_nouns amod_disambiguation_syntactic det_disambiguation_the cop_disambiguation_is nsubj_disambiguation_task nsubj_consider_we rcmod_task_consider nn_task_analysis amod_task_first det_task_The nn_Nouns_Compound rcmod_Bracketing_disambiguation prep_of_Bracketing_Nouns num_Bracketing_6
N04-1016	J93-2004	o	The simplest model of compound noun disambiguation compares the frequencies of the two competing analyses and opts for the most frequent one -LRB- Pustejovsky et al. Model Alta BNC Baseline 63.93 63.93 f -LRB- n1 n2 -RRB- f -LRB- n2 n3 -RRB- 77.86 66.39 f -LRB- n1 n2 -RRB- f -LRB- n1 n3 -RRB- 78.68 # 65.57 f -LRB- n1 n2 -RRB- = f -LRB- n1 -RRB- f -LRB- n2 n3 -RRB- = f -LRB- n2 -RRB- 68.85 65.57 f -LRB- n1 n2 -RRB- = f -LRB- n2 -RRB- f -LRB- n2 n3 -RRB- = f -LRB- n3 -RRB- 70.49 63.11 f -LRB- n1 n2 -RRB- = f -LRB- n2 -RRB- f -LRB- n1 n3 -RRB- = f -LRB- n3 -RRB- 80.32 66.39 f -LRB- n1 n2 -RRB- f -LRB- n2 n3 -RRB- -LRB- NEAR -RRB- 68.03 63.11 f -LRB- n1 n2 -RRB- f -LRB- n1 n3 -RRB- -LRB- NEAR -RRB- 71.31 67.21 f -LRB- n1 n2 -RRB- = f -LRB- n1 -RRB- f -LRB- n2 n3 -RRB- = f -LRB- n2 -RRB- -LRB- NEAR -RRB- 61.47 62.29 f -LRB- n1 n2 -RRB- = f -LRB- n2 -RRB- f -LRB- n2 n3 -RRB- = f -LRB- n3 -RRB- -LRB- NEAR -RRB- 65.57 57.37 f -LRB- n1 n2 -RRB- = f -LRB- n2 -RRB- f -LRB- n1 n3 -RRB- = f -LRB- n3 -RRB- -LRB- NEAR -RRB- 75.40 68.03 # Table 8 Performance of Altavista counts and BNC counts for compound bracketing -LRB- data from Lauer 1995 -RRB- Model Accuracy Baseline 63.93 Best BNC 68.036 Lauer -LRB- 1995 -RRB- adjacency 68.90 Lauer -LRB- 1995 -RRB- dependency 77.50 Best Altavista 78.686 Lauer -LRB- 1995 -RRB- tuned 80.70 Upper bound 81.50 Table 9 Performance comparison with the literature for compound bracketing 1993 -RRB-	dobj_bracketing_1993 vmod_compound_bracketing prep_for_literature_compound det_literature_the prep_with_comparison_literature nn_comparison_Performance amod_comparison_tuned num_Table_9 num_Table_81.50 dobj_bound_Table nsubj_bound_Upper num_Upper_80.70 pcomp_tuned_bound dep_Lauer_comparison appos_Lauer_1995 num_Lauer_78.686 nn_Lauer_Altavista amod_Lauer_Best num_Lauer_77.50 nn_Lauer_dependency dep_Lauer_Lauer appos_Lauer_1995 num_Lauer_68.90 nn_Lauer_adjacency appos_Lauer_1995 num_Lauer_68.036 nn_Lauer_BNC amod_Lauer_Best num_Lauer_63.93 nn_Lauer_Baseline nn_Lauer_Accuracy nn_Lauer_Model num_Lauer_1995 dep_data_Lauer dep_data_Lauer prep_from_data_Lauer dep_bracketing_data nn_bracketing_compound nn_counts_BNC conj_and_counts_counts nn_counts_Altavista prep_for_Performance_bracketing prep_of_Performance_counts prep_of_Performance_counts num_Table_8 dep_Table_# dep_68.03_Table dep_75.40_68.03 dep_NEAR_Performance dep_NEAR_75.40 dep_n3_NEAR nn_n3_f amod_n3_= dep_n3_n1 dep_n3_f dep_n1_n3 nn_n2_f amod_n2_= dep_n2_f dep_n2_NEAR dep_n2_n3 dep_n1_n2 dep_f_n1 num_f_57.37 number_57.37_65.57 nn_n3_f amod_n3_= dep_n3_n2 dep_n3_f dep_n2_n3 nn_n2_f amod_n2_= dep_n2_f dep_n2_NEAR dep_n2_n2 dep_n1_n2 dep_f_n1 num_f_62.29 number_62.29_61.47 nn_n2_f amod_n2_= dep_n2_n2 dep_n2_f dep_n2_f dep_n2_n3 nn_n1_f amod_n1_= dep_n1_f dep_n1_n1 dep_n1_f dep_n1_n2 dep_f_n1 num_f_67.21 num_f_71.31 dep_n1_NEAR dep_n1_n3 dep_n1_n2 dep_f_n1 num_f_63.11 num_f_68.03 dep_n2_n1 dep_n2_f dep_n2_NEAR dep_n2_n3 dep_f_n2 dep_n1_n2 dep_f_n1 num_f_66.39 num_f_80.32 dep_n3_f nn_n3_f amod_n3_= dep_n3_n1 dep_n3_f dep_n1_n3 nn_n2_f amod_n2_= dep_n2_f dep_n2_n3 dep_n1_n2 dep_f_n1 num_f_63.11 num_f_70.49 nn_n3_f amod_n3_= dep_n3_n2 dep_n3_f dep_n2_n3 nn_n2_f amod_n2_= dep_n2_f dep_n2_n2 dep_n1_n2 dep_f_n1 num_f_65.57 num_f_68.85 nn_n2_f amod_n2_= dep_n2_n2 dep_n2_f dep_n2_n3 nn_n1_f amod_n1_= dep_n1_n2 dep_n1_f dep_n1_n2 nn_n1_f dep_n1_65.57 dep_n1_78.68 dep_n1_n1 dep_n1_f dep_65.57_# dep_n1_n3 dep_n1_n2 num_f_66.39 num_f_77.86 dep_n2_n1 dep_n2_n1 dep_n2_f dep_n2_n3 dep_n1_n2 dep_f_n1 num_f_63.93 num_f_63.93 dep_Baseline_f amod_BNC_Baseline dep_Alta_BNC dep_Model_Alta dep_Pustejovsky_n1 appos_Pustejovsky_Model dep_Pustejovsky_al. nn_Pustejovsky_et appos_one_Pustejovsky amod_one_frequent det_one_the advmod_frequent_most prep_for_analyses_one conj_and_analyses_opts amod_analyses_competing num_analyses_two det_analyses_the prep_of_frequencies_opts prep_of_frequencies_analyses det_frequencies_the dep_compares_n3 parataxis_compares_n2 parataxis_compares_n2 dep_compares_n3 parataxis_compares_n2 parataxis_compares_n2 dobj_compares_frequencies nsubj_compares_model nn_disambiguation_noun nn_disambiguation_compound prep_of_model_disambiguation amod_model_simplest det_model_The
N06-1020	J93-2004	o	3.3 Corpora Our labeled data comes from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and consists of about 40,000 sentences from Wall Street Journal -LRB- WSJ -RRB- articles 153 annotated with syntactic information	amod_information_syntactic prep_with_annotated_information amod_153_annotated dep_articles_153 nn_articles_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall prep_from_sentences_articles num_sentences_40,000 quantmod_40,000_about prep_of_consists_sentences nsubj_consists_data amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the conj_and_comes_consists dep_comes_Marcus prep_from_comes_Treebank nsubj_comes_data amod_data_labeled poss_data_Our nn_data_Corpora num_data_3.3
N06-1021	J93-2004	o	For English we used the Penn Treebank version 3.0 -LRB- Marcus et al. 1993 -RRB- and extracted dependency relations by applying the head-finding rules of -LRB- Yamada and Matsumoto 2003 -RRB-	dep_Yamada_2003 conj_and_Yamada_Matsumoto dep_of_Matsumoto dep_of_Yamada prep_rules_of amod_rules_head-finding det_rules_the dobj_applying_rules nn_relations_dependency amod_relations_extracted amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_version_3.0 nn_version_Treebank nn_version_Penn det_version_the prepc_by_used_applying conj_and_used_relations dep_used_Marcus dobj_used_version nsubj_used_we prep_for_used_English
N06-1031	J93-2004	o	The yield of this tree gives the target translation the gunman was killed by police The Penn English Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB- is our source of syntactic information largely due to the availability of reliable parsers	amod_parsers_reliable prep_of_availability_parsers det_availability_the amod_information_syntactic prep_due_to_source_availability advmod_source_largely prep_of_source_information poss_source_our cop_source_is csubj_source_gives amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_Marcus appos_Treebank_PTB nn_Treebank_English nn_Treebank_Penn det_Treebank_The agent_killed_police auxpass_killed_was nsubjpass_killed_gunman det_gunman_the nn_translation_target det_translation_the dobj_gives_Treebank parataxis_gives_killed dobj_gives_translation nsubj_gives_yield det_tree_this prep_of_yield_tree det_yield_The
N06-1040	J93-2004	o	Finally Section 4 reports the results of parsing experiments using our exhaustive k-best CYK parser with the concise PCFGs induced from the Penn WSJ treebank -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_treebank_WSJ nn_treebank_Penn det_treebank_the prep_from_induced_treebank vmod_PCFGs_induced amod_PCFGs_concise det_PCFGs_the nn_parser_CYK amod_parser_k-best amod_parser_exhaustive poss_parser_our prep_with_using_PCFGs dobj_using_parser nn_experiments_parsing vmod_results_using prep_of_results_experiments det_results_the dep_reports_Marcus dobj_reports_results nsubj_reports_Section advmod_reports_Finally num_Section_4
N06-1054	J93-2004	o	We use data from the CoNLL-2004 shared taskthe PropBank -LRB- Palmer et al. 2005 -RRB- annotations of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- with sections 1518 as the training set and section 20 as the development set	nn_set_development det_set_the num_section_20 prep_as_set_set conj_and_set_section nn_set_training det_set_the prep_as_sections_section prep_as_sections_set num_sections_1518 amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_annotations_Treebank nn_annotations_PropBank amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et appos_PropBank_Palmer nn_PropBank_taskthe amod_PropBank_shared dep_CoNLL-2004_annotations det_CoNLL-2004_the prep_with_use_sections dep_use_Marcus prep_from_use_CoNLL-2004 dobj_use_data nsubj_use_We
N06-2015	J93-2004	p	2 Treebanking The Penn Treebank -LRB- Marcus et al. 1993 -RRB- is annotated with information to make predicate-argument structure easy to decode including function tags and markers of empty categories that represent displaced constituents	amod_constituents_displaced dobj_represent_constituents nsubj_represent_that rcmod_categories_represent amod_categories_empty prep_of_markers_categories conj_and_tags_markers nn_tags_function prep_including_decode_markers prep_including_decode_tags aux_decode_to xcomp_easy_decode nsubj_easy_structure amod_structure_predicate-argument xcomp_make_easy aux_make_to vmod_information_make prep_with_annotated_information cop_annotated_is nsubj_annotated_Treebank amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_The amod_Treebank_Treebanking num_Treebank_2
N06-2019	J93-2004	o	For our out-of-domain training condition the parser was trained on sections 2-21 of the Wall Street Journal -LRB- WSJ -RRB- corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_corpus_Marcus nn_corpus_WSJ dep_Journal_corpus nn_Journal_Street nn_Journal_Wall det_Journal_the prep_of_sections_Journal num_sections_2-21 prep_on_trained_sections auxpass_trained_was nsubjpass_trained_parser prep_for_trained_condition det_parser_the nn_condition_training amod_condition_out-of-domain poss_condition_our rcmod_``_trained
N06-2025	J93-2004	o	4.2 Experiments on SRL dataset We used two different corpora PropBank -LRB- www.cis.upenn.edu/ace -RRB- along with Penn Treebank 2 -LRB- Marcus et al. 1993 -RRB- and FrameNet	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_Treebank_2 nn_Treebank_Penn conj_and_PropBank_FrameNet dep_PropBank_Marcus pobj_PropBank_Treebank prepc_along_with_PropBank_with appos_PropBank_www.cis.upenn.edu/ace dep_corpora_FrameNet dep_corpora_PropBank amod_corpora_different num_corpora_two dobj_used_corpora nsubj_used_We nn_dataset_SRL rcmod_Experiments_used prep_on_Experiments_dataset num_Experiments_4.2
N06-2026	J93-2004	o	PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_structures_Treebank amod_structures_syntactic det_structures_the nn_annotation_structure nn_annotation_argument prep_of_layer_annotation det_layer_a prep_to_adding_structures dobj_adding_layer amod_information_propositional dep_encodes_Marcus prepc_by_encodes_adding dobj_encodes_information nsubj_encodes_PropBank
N06-2033	J93-2004	o	Much of this work has been fueled by the availability of large corpora annotated with syntactic structures especially the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the advmod_Treebank_especially appos_structures_Treebank amod_structures_syntactic prep_with_annotated_structures amod_corpora_annotated amod_corpora_large prep_of_availability_corpora det_availability_the agent_fueled_availability auxpass_fueled_been aux_fueled_has nsubjpass_fueled_Much det_work_this prep_of_Much_work
N07-1049	J93-2004	o	Occasionally in 59 sentences out of 2416 on section 23 of the Wall Street Journal Penn Treebank -LRB- Marcus et al. 1993 -RRB- the shift-reduce parser fails to attach a node to a head producing a disconnected graph	amod_graph_disconnected det_graph_a dobj_producing_graph det_head_a det_node_a vmod_attach_producing prep_to_attach_head dobj_attach_node aux_attach_to xcomp_fails_attach nsubj_fails_parser advmod_fails_in advmod_fails_Occasionally amod_parser_shift-reduce det_parser_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn nn_Treebank_Journal nn_Treebank_Street nn_Treebank_Wall det_Treebank_the prep_of_section_Treebank num_section_23 pobj_sentences_2416 prepc_out_of_sentences_of num_sentences_59 dep_in_Marcus prep_on_in_section pobj_in_sentences
N07-1051	J93-2004	o	ENGLISH GERMAN CHINESE -LRB- Marcus et al. 1993 -RRB- -LRB- Skut et al. 1997 -RRB- -LRB- Xue et al. 2002 -RRB- TrainSet Section 2-21 Sentences 1-18 ,602 Articles 26-270 DevSet Section 22 18,603-19 ,602 Articles 1-25 TestSet Section 23 19,603-20 ,602 Articles 271-300 Table 3 Experimental setup	amod_setup_Experimental num_Table_3 num_Table_271-300 nn_Table_Articles num_Table_,602 num_Table_19,603-20 dep_Section_Table num_Section_23 nn_Section_TestSet num_Section_1-25 nn_Section_Articles appos_,602_Section dep_18,603-19_,602 num_Section_18,603-19 num_Section_22 nn_Section_DevSet num_Section_26-270 nn_Section_Articles num_Section_,602 dep_1-18_Section dep_Sentences_1-18 num_Sentences_2-21 nn_Sentences_Section nn_Sentences_TrainSet amod_Xue_2002 dep_Xue_al. nn_Xue_et amod_Skut_1997 dep_Skut_al. nn_Skut_et dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_CHINESE_setup dep_CHINESE_Sentences dep_CHINESE_Xue dep_CHINESE_Skut appos_CHINESE_Marcus amod_CHINESE_GERMAN amod_CHINESE_ENGLISH
N07-1058	J93-2004	p	The default training set of Penn Treebank -LRB- Marcus et al. 1993 -RRB- was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might be used	auxpass_used_be aux_used_might nsubjpass_used_predictor prep_on_used_which nn_learners_language amod_learners_second prep_for_predictor_learners nn_predictor_level nn_predictor_reading det_predictor_a rcmod_texts_used det_texts_the prep_of_domain_texts conj_and_domain_style det_domain_the advmod_well_fairly prep_with_matches_style prep_with_matches_domain advmod_matches_well advmod_matches_actually nsubj_matches_style nsubj_matches_domain mark_matches_because det_texts_those prep_of_domain_texts conj_and_domain_style det_domain_the det_parser_the advcl_used_matches prep_for_used_parser auxpass_used_was nsubjpass_used_set advmod_1993_al. nn_al._et num_Marcus_1993 nn_Treebank_Penn appos_set_Marcus prep_of_set_Treebank nn_set_training nn_set_default det_set_The
N07-2045	J93-2004	o	2.1 Training the model As with -LRB- Minnen et al. 2000 -RRB- we train the language model on the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_on_model_Treebank nn_model_language det_model_the dep_train_Marcus dobj_train_model nsubj_train_we ccomp_train_2.1 amod_Minnen_2000 dep_Minnen_al. nn_Minnen_et dep_with_Minnen pcomp_As_with det_model_the prep_Training_As dobj_Training_model vmod_2.1_Training
N07-2045	J93-2004	o	We also test our language model using leave-one-out cross-validation on the Penn Treebank -LRB- Marcus et al. 1993 -RRB- -LRB- WSJ -RRB- giving us 86.74 % accuracy -LRB- see Table 1 -RRB-	dep_Table_1 dobj_see_Table amod_accuracy_% number_%_86.74 dep_giving_see dobj_giving_accuracy iobj_giving_us dep_Marcus_giving appos_Marcus_WSJ dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the amod_cross-validation_leave-one-out prep_on_using_Treebank dobj_using_cross-validation vmod_model_using nn_model_language poss_model_our dep_test_Marcus dobj_test_model advmod_test_also nsubj_test_We
N09-1009	J93-2004	o	4 Experiments Our experiments involve data from two treebanks the Wall Street Journal Penn treebank -LRB- Marcus et al. 1993 -RRB- and the Chinese treebank -LRB- Xue et al. 2004 -RRB-	amod_Xue_2004 dep_Xue_al. nn_Xue_et dep_treebank_Xue amod_treebank_Chinese det_treebank_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_treebank_treebank appos_treebank_Marcus nn_treebank_Penn nn_treebank_Journal nn_treebank_Street nn_treebank_Wall det_treebank_the num_treebanks_two prep_from_data_treebanks dobj_involve_data nsubj_involve_experiments poss_experiments_Our dep_Experiments_treebank dep_Experiments_treebank rcmod_Experiments_involve num_Experiments_4
N09-1012	J93-2004	o	6 Results We trained on the standard Penn Treebank WSJ corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_WSJ nn_corpus_Treebank nn_corpus_Penn amod_corpus_standard det_corpus_the prep_on_trained_corpus nsubj_trained_We dep_Results_Marcus rcmod_Results_trained num_Results_6
N09-1013	J93-2004	o	Standard CI Model 1 training initialised with a uniform translation table so that t -LRB- ejf -RRB- is constant for all source/target word pairs -LRB- f e -RRB- was run on untagged data for 10 iterations in each direction -LRB- Brown et al. 1993 Deng and Byrne 2005b -RRB-	appos_Deng_2005b conj_and_Deng_Byrne conj_al._1993 nn_al._et dep_Brown_Byrne dep_Brown_Deng advmod_Brown_al. det_direction_each num_iterations_10 prep_for_data_iterations amod_data_untagged dep_run_Brown prep_in_run_direction prep_on_run_data auxpass_run_was nsubjpass_run_training dep_f_e appos_pairs_f nn_pairs_word amod_pairs_source/target det_pairs_all prep_for_constant_pairs cop_constant_is nsubj_constant_t mark_constant_that mark_constant_so appos_t_ejf nn_table_translation amod_table_uniform det_table_a advcl_initialised_constant prep_with_initialised_table vmod_training_initialised num_training_1 nn_training_Model nn_training_CI nn_training_Standard
N09-1013	J93-2004	o	2.1 EM parameter estimation We train using Expectation Maximisation -LRB- EM -RRB- optimising the log probability of the training setfe -LRB- s -RRB- f -LRB- s -RRB- gSs = 1 -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_=_1 dep_gSs_Brown amod_gSs_= dep_s_gSs nn_s_f appos_s_s nn_s_setfe nn_s_training det_s_the prep_of_probability_s nn_probability_log det_probability_the dobj_optimising_probability vmod_Maximisation_optimising appos_Maximisation_EM nn_Maximisation_Expectation dobj_using_Maximisation xcomp_train_using nsubj_train_We rcmod_estimation_train nn_estimation_parameter nn_estimation_EM num_estimation_2.1 dep_``_estimation
N09-1013	J93-2004	o	Then P -LRB- eI1jfj1 -RRB- = summationtextaI 1 P -LRB- eI1 aI1jfj1 -RRB- -LRB- Brown et al. 1993 -RRB-	amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_eI1_aI1jfj1 appos_P_Brown dep_P_eI1 num_P_1 nn_P_summationtextaI dobj_=_P nsubj_=_P advmod_=_Then appos_P_eI1jfj1
N09-1063	J93-2004	o	We used the Berkeley Parser 2 to train such grammars on sections 2-21 of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_sections_Treebank num_sections_2-21 amod_grammars_such prep_on_train_sections dobj_train_grammars aux_train_to num_Parser_2 nn_Parser_Berkeley det_Parser_the dep_used_Marcus vmod_used_train dobj_used_Parser nsubj_used_We
N09-1073	J93-2004	o	For this paper we use an exact inference -LRB- exhaustive search -RRB- CYK parser using a simple probabilistic context-free grammar -LRB- PCFG -RRB- induced from the Penn WSJ Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_WSJ nn_Treebank_Penn det_Treebank_the prep_from_induced_Treebank vmod_grammar_induced appos_grammar_PCFG amod_grammar_context-free amod_grammar_probabilistic amod_grammar_simple det_grammar_a dobj_using_grammar nn_parser_CYK nn_parser_inference amod_search_exhaustive appos_inference_search amod_inference_exact det_inference_an dep_use_Marcus vmod_use_using dobj_use_parser nsubj_use_we prep_for_use_paper det_paper_this
N09-2032	J93-2004	o	To determine the target distribution we classified 171 -LRB- approximately 5 % -RRB- randomly selected utterances from the TownInfo data that were used as a development set .2 In Table 1 we can see that 15.2 % of the trees in the artificial corpus will be NP NSUs .3 4 Data generation We constructed our artificial corpus from sections 2 to 21 of the Wall Street Journal -LRB- WSJ -RRB- section of the Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB- 2We discarded very short utterances -LRB- yes no and greetings -RRB- since they dont need parsing	nn_parsing_need dobj_dont_parsing nsubj_dont_they mark_dont_since advcl_no_dont conj_and_no_greetings discourse_no_yes amod_utterances_short advmod_short_very dobj_discarded_utterances nsubj_discarded_2We dep_discarded_Marcus advcl_discarded_see amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_of_section_corpus nn_section_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the dep_21_to number_21_2 prep_of_sections_section num_sections_21 amod_corpus_artificial poss_corpus_our prep_from_constructed_sections dobj_constructed_corpus nsubj_constructed_We rcmod_generation_constructed nn_generation_Data num_generation_4 nn_generation_.3 nn_generation_NSUs nn_generation_NP cop_generation_be aux_generation_will nsubj_generation_% mark_generation_that amod_corpus_artificial det_corpus_the det_trees_the prep_in_%_corpus prep_of_%_trees num_%_15.2 ccomp_see_generation aux_see_can nsubj_see_we num_Table_1 prep_in_.2_Table amod_.2_set dep_development_.2 det_development_a dep_used_greetings dep_used_no ccomp_used_discarded prep_as_used_development auxpass_used_were nsubjpass_used_that advcl_used_determine nn_data_TownInfo det_data_the amod_utterances_selected num_utterances_171 advmod_selected_randomly num_%_5 quantmod_5_approximately appos_171_% prep_from_classified_data dobj_classified_utterances nsubj_classified_we rcmod_distribution_classified nn_distribution_target det_distribution_the dobj_determine_distribution aux_determine_To ccomp_``_used
N09-2044	J93-2004	o	173 The standard features for genre classification models include words part-of-speech -LRB- POS -RRB- tags and punctuation -LRB- Kessler et al. 1997 Stamatatos et al. 2000 Lee and Myaeng 2002 Biber 1993 -RRB- but constituent-based syntactic categories have also been explored -LRB- Karlgren and Cutting 1994 -RRB-	dep_Karlgren_1994 conj_and_Karlgren_Cutting dep_explored_Cutting dep_explored_Karlgren auxpass_explored_been advmod_explored_also aux_explored_have nsubjpass_explored_categories nn_categories_syntactic amod_categories_constituent-based dep_Biber_1993 num_Lee_2002 conj_and_Lee_Myaeng num_Stamatatos_2000 nn_Stamatatos_al. nn_Stamatatos_et dep_Kessler_Biber conj_Kessler_Myaeng conj_Kessler_Lee conj_Kessler_Stamatatos amod_Kessler_1997 dep_Kessler_al. nn_Kessler_et nn_tags_part-of-speech appos_part-of-speech_POS dep_words_Kessler conj_and_words_punctuation conj_and_words_tags conj_but_include_explored dobj_include_punctuation dobj_include_tags dobj_include_words nsubj_include_features nn_models_classification nn_models_genre prep_for_features_models amod_features_standard det_features_The num_features_173
N09-2044	J93-2004	o	A similar approach is used here including a collapsed version of the Treebank POS tag set -LRB- Marcus et al. 1993 -RRB- with additions for specific words -LRB- e.g. personal pronouns and filled pause markers -RRB- compound punctuation -LRB- e.g. multiple exclamation marks -RRB- and a general emoticon tag resulting in a total of 41 tags	num_tags_41 prep_of_total_tags det_total_a prep_in_resulting_total nn_tag_emoticon amod_tag_general det_tag_a nn_marks_exclamation amod_marks_multiple pobj_e.g._marks prep_punctuation_e.g. nn_punctuation_compound prep_punctuation_e.g. nn_markers_pause amod_markers_filled conj_and_pronouns_markers amod_pronouns_personal pobj_e.g._markers pobj_e.g._pronouns dep_words_punctuation amod_words_specific prep_for_additions_words vmod_Marcus_resulting conj_and_Marcus_tag prep_with_Marcus_additions amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_set_tag nn_set_POS nn_set_Treebank det_set_the prep_of_version_set amod_version_collapsed det_version_a dep_used_tag dep_used_Marcus prep_including_used_version advmod_used_here auxpass_used_is nsubjpass_used_approach amod_approach_similar det_approach_A
N09-2061	J93-2004	o	2 Previous Work We briefly outline the most important existing methods and cite error rates on a standard English data set sections 03-06 of the Wall Street Journal -LRB- WSJ -RRB- corpus -LRB- Marcus et al. 1993 -RRB- containing nearly 27,000 examples	num_examples_27,000 quantmod_27,000_nearly dobj_containing_examples amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et vmod_corpus_containing appos_corpus_Marcus nn_corpus_WSJ dep_Journal_corpus nn_Journal_Street nn_Journal_Wall det_Journal_the prep_of_sections_Journal num_sections_03-06 vmod_data_set amod_data_English amod_data_standard det_data_a nn_rates_error prep_on_cite_data dobj_cite_rates nsubj_cite_We amod_methods_existing amod_methods_important det_methods_the advmod_important_most conj_and_outline_cite dobj_outline_methods advmod_outline_briefly nsubj_outline_We appos_Work_sections rcmod_Work_cite rcmod_Work_outline amod_Work_Previous num_Work_2
N09-3004	J93-2004	o	4 Experimental Set-up For the experiments we use the WSJ portion of the Penn tree bank -LRB- Marcus et al. 1993 -RRB- using the standard train/development/test splits viz 39,832 sentences from 2-21 sections 2416 sentences from section 23 for testing and 1,700 sentences from section 22 for development	num_section_22 num_sentences_1,700 prep_for_testing_development prep_from_testing_section conj_and_testing_sentences num_section_23 prep_for_sentences_sentences prep_for_sentences_testing prep_from_sentences_section num_sentences_2416 num_sections_2-21 dep_sentences_sentences prep_from_sentences_sections num_sentences_39,832 nn_sentences_viz dobj_splits_sentences csubj_splits_using amod_train/development/test_standard det_train/development/test_the dobj_using_train/development/test amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_bank_tree nn_bank_Penn det_bank_the prep_of_portion_bank nn_portion_WSJ det_portion_the dep_use_splits dep_use_Marcus dobj_use_portion nsubj_use_we dep_use_Set-up det_experiments_the prep_for_Set-up_experiments amod_Set-up_Experimental num_Set-up_4
N09-3017	J93-2004	o	OHara and Wiebe -LRB- 2003 -RRB- also make use of high level features in their case the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and FrameNet -LRB- Baker et al. 1998 -RRB- to classify prepositions	dobj_classify_prepositions aux_classify_to amod_Baker_1998 dep_Baker_al. nn_Baker_et amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et vmod_Treebank_classify dep_Treebank_Baker conj_and_Treebank_FrameNet dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the poss_case_their nn_features_level amod_features_high prep_of_use_features dep_make_FrameNet dep_make_Treebank prep_in_make_case dobj_make_use advmod_make_also nsubj_make_Wiebe nsubj_make_OHara appos_Wiebe_2003 conj_and_OHara_Wiebe
N09-3017	J93-2004	o	O'Hara and Wiebe -LRB- 2003 -RRB- make use of Penn Treebank -LRB- Marcus et al. 1993 -RRB- and FrameNet -LRB- Baker et al. 1998 -RRB- to classify prepositions	dobj_classify_prepositions aux_classify_to amod_Baker_1998 dep_Baker_al. nn_Baker_et nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn prep_of_use_Treebank xcomp_make_classify dep_make_Baker conj_and_make_FrameNet dep_make_Marcus dobj_make_use nsubj_make_Wiebe nsubj_make_O'Hara appos_Wiebe_2003 conj_and_O'Hara_Wiebe
P00-1061	J93-2004	o	In all of the cited approaches the Penn Wall Street Journal Treebank -LRB- Marcus et al. 1993 -RRB- is used the availability of whichobviates the standard eort required for treebank traininghandannotating large corpora of specic domains of specic languages with specic parse types	nn_types_parse amod_types_specic amod_languages_specic prep_of_domains_languages amod_domains_specic prep_with_corpora_types prep_of_corpora_domains amod_corpora_large dep_traininghandannotating_corpora nn_traininghandannotating_treebank prep_for_required_traininghandannotating vmod_eort_required amod_eort_standard det_eort_the dep_whichobviates_eort prep_of_availability_whichobviates det_availability_the dobj_used_availability auxpass_used_is nsubjpass_used_Treebank prep_in_used_all amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Journal nn_Treebank_Street nn_Treebank_Wall nn_Treebank_Penn det_Treebank_the amod_approaches_cited det_approaches_the prep_of_all_approaches
P01-1003	J93-2004	o	4 Experimental Work A part of the Wall Street Journal -LRB- WSJ -RRB- which had been processed in the Penn Treebanck Project -LRB- Marcus et al. 1993 -RRB- was used in the experiments	det_experiments_the prep_in_used_experiments auxpass_used_was nsubjpass_used_part amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Project_Treebanck nn_Project_Penn det_Project_the prep_in_processed_Project auxpass_processed_been aux_processed_had nsubjpass_processed_which rcmod_Journal_processed appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the dep_part_Marcus prep_of_part_Journal det_part_A rcmod_Work_used amod_Work_Experimental num_Work_4
P01-1010	J93-2004	o	While this technique has been successfully applied to parsing the ATIS portion in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- it is extremely time consuming	dep_time_consuming advmod_time_extremely cop_time_is nsubj_time_it advcl_time_applied dep_1993_al. nn_al._et num_Marcus_1993 nn_Treebank_Penn det_Treebank_the nn_portion_ATIS det_portion_the prep_in_parsing_Treebank dobj_parsing_portion dep_applied_Marcus prepc_to_applied_parsing advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_technique mark_applied_While det_technique_this
P01-1044	J93-2004	o	We used treebank grammars induced directly from the local trees of the entire WSJ section of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- -LRB- release 3 -RRB-	num_release_3 dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_section_Treebank nn_section_WSJ amod_section_entire det_section_the prep_of_trees_section amod_trees_local det_trees_the prep_from_induced_trees advmod_induced_directly vmod_grammars_induced amod_grammars_treebank dep_used_release dep_used_Marcus dobj_used_grammars nsubj_used_We
P02-1018	J93-2004	o	Evaluating the algorithm on the output of Charniaks parser -LRB- Charniak 2000 -RRB- and the Penn treebank -LRB- Marcus et al. 1993 -RRB- shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity	poss_simplicity_its pobj_given_simplicity amod_nodes_empty prep_of_types_nodes amod_types_occuring det_types_the advmod_occuring_frequently advmod_frequently_most advmod_well_surprisingly prep_algorithm_given prep_on_algorithm_types advmod_algorithm_well aux_algorithm_does nn_algorithm_patternmatching det_algorithm_the prep_that_shows_algorithm csubj_shows_Evaluating amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_treebank_Penn det_treebank_the amod_Charniak_2000 conj_and_parser_treebank dep_parser_Charniak nn_parser_Charniaks prep_of_output_treebank prep_of_output_parser det_output_the prep_on_algorithm_output det_algorithm_the dep_Evaluating_Marcus dobj_Evaluating_algorithm
P02-1026	J93-2004	o	Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB- sections 0-20 were used for training sections 2124 for testing	prep_for_sections_testing num_sections_2124 dep_used_sections prep_for_used_training auxpass_used_were nsubjpass_used_sections num_sections_0-20 nn_sections_corpus dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_corpus_Marcus nn_corpus_Treebank nn_corpus_Penn
P02-1034	J93-2004	o	The Penn Wall Street Journal treebank -LRB- Marcus et al. 1993 -RRB- was used as training and test data	nn_data_test nn_data_training conj_and_training_test prep_as_used_data auxpass_used_was nsubjpass_used_treebank advmod_1993_al. nn_al._et num_Marcus_1993 appos_treebank_Marcus nn_treebank_Journal nn_treebank_Street nn_treebank_Wall nn_treebank_Penn det_treebank_The
P02-1055	J93-2004	o	task -LRB- Church 1988 Brill 1993 Ratnaparkhi 1996 Daelemans et al. 1996 -RRB- and reported errors in the range of 26 % are common	cop_common_are nsubj_common_errors num_%_26 prep_of_range_% det_range_the prep_in_errors_range ccomp_reported_common num_Daelemans_1996 nn_Daelemans_al. nn_Daelemans_et num_Ratnaparkhi_1996 num_Brill_1993 conj_and_Church_reported conj_and_Church_Daelemans conj_and_Church_Ratnaparkhi conj_and_Church_Brill conj_and_Church_1988 nn_Church_task
P02-1055	J93-2004	o	In one experiment it has to be performed on the basis of the gold-standard assumed-perfect POS taken directly from the training data the Penn Treebank -LRB- Marcus et al. 1993 -RRB- so as to abstract from a particular POS tagger and to provide an upper bound	amod_bound_upper det_bound_an dep_provide_bound aux_provide_to nn_tagger_POS amod_tagger_particular det_tagger_a conj_and_from_provide pobj_from_tagger amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the nn_data_training det_data_the prep_from_taken_data advmod_taken_directly vmod_POS_taken amod_POS_assumed-perfect appos_gold-standard_Treebank appos_gold-standard_POS det_gold-standard_the prep_of_basis_gold-standard det_basis_the dep_performed_provide dep_performed_from pobj_performed_abstract prepc_as_to_performed_to advmod_performed_so prep_on_performed_basis auxpass_performed_be aux_performed_to xcomp_has_performed nsubj_has_it prep_in_has_experiment num_experiment_one
P02-1055	J93-2004	o	Our chunks and functions are based on the annotations in the third release of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the prep_of_release_Treebank amod_release_third det_release_the prep_in_annotations_release det_annotations_the dep_based_Marcus prep_on_based_annotations auxpass_based_are nsubjpass_based_functions nsubjpass_based_chunks conj_and_chunks_functions poss_chunks_Our ccomp_``_based
P03-1013	J93-2004	o	The annotation scheme -LRB- Skut et al. 1997 -RRB- is modeled to a certain extent on that of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- with crucial differences	amod_differences_crucial amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_that_Treebank prep_on_extent_that amod_extent_certain det_extent_a prep_with_modeled_differences dep_modeled_Marcus prep_to_modeled_extent auxpass_modeled_is nsubjpass_modeled_scheme amod_Skut_1997 dep_Skut_al. nn_Skut_et dep_scheme_Skut nn_scheme_annotation det_scheme_The
P03-1013	J93-2004	o	However most of the existing models have been developed for English and trained on the Penn Treebank -LRB- Marcus et al. 1993 -RRB- which raises the question whether these models generalize to other languages and to annotation schemes that differ from the Penn Treebank markup	nn_markup_Treebank nn_markup_Penn det_markup_the prep_from_differ_markup nsubj_differ_that rcmod_schemes_differ nn_schemes_annotation pobj_to_schemes amod_languages_other conj_and_generalize_to prep_to_generalize_languages nsubj_generalize_models mark_generalize_whether det_models_these dep_question_to dep_question_generalize det_question_the dobj_raises_question nsubj_raises_which amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_on_trained_Treebank nsubjpass_trained_most dep_developed_raises dep_developed_Marcus conj_and_developed_trained prep_for_developed_English auxpass_developed_been aux_developed_have nsubjpass_developed_most advmod_developed_However amod_models_existing det_models_the prep_of_most_models ccomp_``_trained ccomp_``_developed
P03-1055	J93-2004	o	We used the Wall Street Journal -LRB- WSJ -RRB- part of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- where extraction is represented by co-indexing an empty terminal element -LRB- henceforth EE -RRB- to its antecedent	poss_antecedent_its nn_EE_henceforth appos_element_EE amod_element_terminal amod_element_empty det_element_an prep_to_co-indexing_antecedent dobj_co-indexing_element agent_represented_co-indexing auxpass_represented_is nsubjpass_represented_extraction advmod_represented_where amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_part_Treebank nn_part_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the advcl_used_represented dep_used_Marcus dobj_used_part nsubj_used_We
P03-1069	J93-2004	o	It achieves 90.1 % average precision/recall for sentences with maximum length 40 and 89.5 % for sentences with maximum length 100 when trained and tested on the standard sections of the Wall Street Journal Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Journal nn_Treebank_Street nn_Treebank_Wall det_Treebank_the prep_of_sections_Treebank amod_sections_standard det_sections_the prep_on_trained_sections conj_and_trained_tested advmod_trained_when num_length_100 nn_length_maximum prep_with_sentences_length rcmod_%_tested rcmod_%_trained prep_for_%_sentences num_%_89.5 num_length_40 nn_length_maximum prep_with_sentences_length dep_precision/recall_Marcus conj_and_precision/recall_% prep_for_precision/recall_sentences amod_precision/recall_average amod_precision/recall_% number_%_90.1 dobj_achieves_% dobj_achieves_precision/recall nsubj_achieves_It
P03-2006	J93-2004	o	We performed experiments with two statistical classifiers the decision tree induction system C4 .5 -LRB- Quinlan 1993 -RRB- and the Tilburg Memory-Based Learner -LRB- TiMBL -RRB- -LRB- Daelemans et al. 2002 -RRB-	amod_Daelemans_2002 dep_Daelemans_al. nn_Daelemans_et appos_Learner_TiMBL nn_Learner_Memory-Based nn_Learner_Tilburg det_Learner_the amod_Quinlan_1993 dep_C4_Daelemans conj_and_C4_Learner dep_C4_Quinlan num_C4_.5 nn_C4_system nn_C4_induction nn_C4_tree nn_C4_decision det_C4_the amod_classifiers_statistical num_classifiers_two dep_performed_Learner dep_performed_C4 prep_with_performed_classifiers dobj_performed_experiments nsubj_performed_We
P03-2006	J93-2004	o	The corpus was automatically derived from the Penn Treebank II corpus -LRB- Marcus et al. 1993 -RRB- by means of the script chunklink.pl -LRB- Buchholz 2002 -RRB- that we modified to fit our purposes	poss_purposes_our dobj_fit_purposes aux_fit_to xcomp_modified_fit nsubj_modified_we mark_modified_that amod_Buchholz_2002 dep_chunklink.pl_Buchholz nn_chunklink.pl_script det_chunklink.pl_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_II nn_corpus_Treebank nn_corpus_Penn det_corpus_the ccomp_derived_modified prep_by_means_of_derived_chunklink.pl dep_derived_Marcus prep_from_derived_corpus advmod_derived_automatically auxpass_derived_was nsubjpass_derived_corpus det_corpus_The
P03-2036	J93-2004	o	We performed a comparison between the existing CFG filtering techniques for LTAG -LRB- Poller and Becker 1998 -RRB- and HPSG -LRB- Torisawa et al. 2000 -RRB- using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- into HPSG-style	prep_into_Marcus_HPSG-style amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_extracted_Treebank vmod_LTAGs_extracted dobj_converting_LTAGs agent_obtained_converting vmod_grammars_obtained amod_grammars_equivalent advmod_equivalent_strongly dobj_using_grammars amod_Torisawa_2000 dep_Torisawa_al. nn_Torisawa_et dep_Poller_1998 conj_and_Poller_Becker conj_and_LTAG_HPSG dep_LTAG_Becker dep_LTAG_Poller dep_techniques_Torisawa prep_for_techniques_HPSG prep_for_techniques_LTAG amod_techniques_filtering nn_techniques_CFG amod_techniques_existing det_techniques_the prep_between_comparison_techniques det_comparison_a dep_performed_Marcus vmod_performed_using dobj_performed_comparison nsubj_performed_We
P03-2036	J93-2004	o	-LRB- 2003 -RRB- from Sections 2-21 of the Wall Street Journal -LRB- WSJ -RRB- in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and its subsets .3 We then converted them into strongly equivalent HPSG-style grammars using the grammar conversion described in Section 2.1	num_Section_2.1 prep_in_described_Section vmod_conversion_described nn_conversion_grammar det_conversion_the dobj_using_conversion amod_grammars_HPSG-style amod_grammars_equivalent advmod_equivalent_strongly xcomp_converted_using prep_into_converted_grammars dobj_converted_them advmod_converted_then nsubj_converted_We rcmod_.3_converted nn_.3_subsets poss_.3_its amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the prep_of_Sections_Journal num_Sections_2-21 conj_and_2003_.3 dep_2003_Marcus prep_in_2003_Treebank prep_from_2003_Sections dep_''_.3 dep_''_2003
P04-1006	J93-2004	o	The first stage parser is a best-first PCFG parser trained on sections 2 through 22 and 24 of the Penn WSJ treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_treebank_WSJ nn_treebank_Penn det_treebank_the prep_of_24_treebank num_sections_2 prep_through_trained_22 prep_on_trained_sections dep_parser_Marcus conj_and_parser_24 vmod_parser_trained nn_parser_PCFG amod_parser_best-first det_parser_a cop_parser_is nsubj_parser_parser nn_parser_stage amod_parser_first det_parser_The
P04-1013	J93-2004	o	6 The Experiments We used the Penn Treebank -LRB- Marcus et al. 1993 -RRB- to perform empirical experiments on the proposed parsing models	nn_models_parsing amod_models_proposed det_models_the amod_experiments_empirical prep_on_perform_models dobj_perform_experiments aux_perform_to amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the vmod_used_perform dobj_used_Treebank nsubj_used_We rcmod_Experiments_used det_Experiments_The num_Experiments_6 dep_``_Experiments
P04-1043	J93-2004	o	4.1 Corpora set-up The above kernels were experimented over two corpora PropBank -LRB- www.cis.upenn.edu / ace -RRB- along with Penn TreeBank5 2 -LRB- Marcus et al. 1993 -RRB- and FrameNet	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_TreeBank5_2 nn_TreeBank5_Penn dep_www.cis.upenn.edu_ace conj_and_PropBank_FrameNet dep_PropBank_Marcus pobj_PropBank_TreeBank5 prepc_along_with_PropBank_with dep_PropBank_www.cis.upenn.edu dep_corpora_FrameNet dep_corpora_PropBank num_corpora_two prep_over_experimented_corpora auxpass_experimented_were nsubjpass_experimented_set-up prep_above_The_kernels dep_set-up_The nn_set-up_Corpora num_set-up_4.1
P04-1052	J93-2004	o	4 Evaluation As our algorithm works in open domains we were able to perform a corpus-based evaluation using the Penn WSJ Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_WSJ nn_Treebank_Penn det_Treebank_the dobj_using_Treebank amod_evaluation_corpus-based det_evaluation_a xcomp_perform_using dobj_perform_evaluation aux_perform_to xcomp_able_perform cop_able_were nsubj_able_we advcl_able_works amod_domains_open prep_in_works_domains nsubj_works_algorithm mark_works_As dep_works_Evaluation poss_algorithm_our num_Evaluation_4
P04-1052	J93-2004	o	Also attribute classi cation is a hard problem and there is no existing classi cation scheme that can be used for open domains like newswire for example WordNet -LRB- Miller et al. 1993 -RRB- organises adjectives as concepts that are related by the non-hierarchical relations of synonymy and antonymy -LRB- unlike nouns that are related through hierarchical links such as hyponymy hypernymy and metonymy -RRB-	conj_and_hyponymy_metonymy conj_and_hyponymy_hypernymy prep_such_as_links_metonymy prep_such_as_links_hypernymy prep_such_as_links_hyponymy amod_links_hierarchical prep_through_related_links auxpass_related_are nsubjpass_related_that rcmod_nouns_related prep_unlike_-LRB-_nouns conj_and_synonymy_antonymy prep_of_relations_antonymy prep_of_relations_synonymy amod_relations_non-hierarchical det_relations_the agent_related_relations auxpass_related_are nsubjpass_related_that rcmod_concepts_related prep_as_organises_concepts dobj_organises_adjectives nsubj_organises_WordNet prep_for_organises_example amod_Miller_1993 dep_Miller_al. nn_Miller_et dep_WordNet_Miller prep_like_domains_newswire amod_domains_open prep_for_used_domains auxpass_used_be aux_used_can nsubjpass_used_that rcmod_scheme_used nn_scheme_cation nn_scheme_classi amod_scheme_existing neg_scheme_no nsubj_is_scheme expl_is_there conj_and_problem_organises conj_and_problem_is amod_problem_hard det_problem_a cop_problem_is csubj_problem_attribute nn_cation_classi dobj_attribute_cation advmod_attribute_Also
P04-1082	J93-2004	o	Using linguistic principles to recover empty categories Richard CAMPBELL Microsoft Research One Microsoft Way Redmond WA 98052 USA richcamp@microsoft.com Abstract This paper describes an algorithm for detecting empty nodes in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- finding their antecedents and assigning them function tags without access to lexical information such as valency	prep_such_as_information_valency amod_information_lexical prep_to_access_information prep_without_function_access dobj_function_tags nsubj_function_them ccomp_assigning_function csubj_assigning_Using poss_antecedents_their dobj_finding_antecedents dep_al._1993 nn_al._et advmod_Marcus_al. nn_Treebank_Penn det_Treebank_the amod_nodes_empty prep_in_detecting_Treebank dobj_detecting_nodes prepc_for_algorithm_detecting det_algorithm_an conj_and_describes_assigning xcomp_describes_finding dep_describes_Marcus dobj_describes_algorithm csubj_describes_Using det_paper_This nn_paper_Abstract nn_paper_richcamp@microsoft.com nn_paper_USA num_paper_98052 nn_paper_WA appos_Redmond_paper nn_Redmond_Way nn_Redmond_Microsoft num_Redmond_One nn_Redmond_Research nn_Redmond_Microsoft nn_Redmond_CAMPBELL nn_Redmond_Richard dep_categories_Redmond amod_categories_empty dobj_recover_categories aux_recover_to amod_principles_linguistic vmod_Using_recover dobj_Using_principles
P04-1082	J93-2004	o	In the Penn Treebank -LRB- Marcus et al. 1993 -RRB- null elements or empty categories are used to indicate non-local dependencies discontinuous constituents and certain missing elements	amod_elements_missing amod_elements_certain amod_constituents_discontinuous conj_and_dependencies_elements conj_and_dependencies_constituents amod_dependencies_non-local dobj_indicate_elements dobj_indicate_constituents dobj_indicate_dependencies aux_indicate_to xcomp_used_indicate auxpass_used_are nsubjpass_used_categories nsubjpass_used_elements dep_used_Marcus prep_in_used_Treebank amod_categories_empty conj_or_elements_categories amod_elements_null nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the
P05-1012	J93-2004	o	3 Experiments We tested our methods experimentally on the English Penn Treebank -LRB- Marcus et al. 1993 -RRB- and on the Czech Prague Dependency Treebank -LRB- Hajic 1998 -RRB-	amod_Hajic_1998 dep_Treebank_Hajic nn_Treebank_Dependency nn_Treebank_Prague nn_Treebank_Czech det_Treebank_the pobj_on_Treebank amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn nn_Treebank_English det_Treebank_the poss_methods_our prep_on_tested_Treebank advmod_tested_experimentally dobj_tested_methods nsubj_tested_We conj_and_Experiments_on dep_Experiments_Marcus rcmod_Experiments_tested num_Experiments_3
P05-1023	J93-2004	o	5 The Experimental Results We used the Penn Treebank WSJ corpus -LRB- Marcus et al. 1993 -RRB- to perform empirical experiments on the proposed parsing models	nn_models_parsing amod_models_proposed det_models_the prep_on_experiments_models amod_experiments_empirical dobj_perform_experiments aux_perform_to nsubj_perform_Results amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_WSJ nn_corpus_Treebank nn_corpus_Penn det_corpus_the dobj_used_corpus nsubj_used_We dep_Results_Marcus rcmod_Results_used amod_Results_Experimental det_Results_The num_Results_5
P05-1023	J93-2004	o	The most sophisticated of these techniques -LRB- such as Support Vector Machines -RRB- are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_like_datasets_Treebank amod_datasets_large prep_on_used_datasets auxpass_used_be aux_used_to dep_expensive_Marcus xcomp_expensive_used advmod_expensive_computationally advmod_expensive_unfortunately cop_expensive_are nsubj_expensive_sophisticated advmod_computationally_too nn_Machines_Vector nn_Machines_Support prep_such_as_techniques_Machines det_techniques_these prep_of_sophisticated_techniques advmod_sophisticated_most det_sophisticated_The ccomp_``_expensive
P05-1025	J93-2004	o	Unlabeled dependencies can be readily obtained by processing constituent trees such as those in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- with a set of rules to determine the lexical heads of constituents	prep_of_heads_constituents amod_heads_lexical det_heads_the dobj_determine_heads aux_determine_to vmod_set_determine prep_of_set_rules det_set_a amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_in_those_Treebank prep_such_as_trees_those nn_trees_constituent dobj_processing_trees prep_with_obtained_set dep_obtained_Marcus agent_obtained_processing advmod_obtained_readily auxpass_obtained_be aux_obtained_can nsubjpass_obtained_dependencies amod_dependencies_Unlabeled
P05-1036	J93-2004	o	The K&M model creates a packed parse forest of all possible compressions that are grammatical with respect to the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_with_respect_to_grammatical_Treebank cop_grammatical_are nsubj_grammatical_that rcmod_compressions_grammatical amod_compressions_possible det_compressions_all prep_of_forest_compressions nn_forest_parse amod_forest_packed det_forest_a dep_creates_Marcus dobj_creates_forest nsubj_creates_model nn_model_K&M det_model_The
P05-1038	J93-2004	o	Compared to the Penn Treebank -LRB- PTB Marcus et al. 1993 -RRB- the POS tagset of the French Treebank is smaller -LRB- 13 tags vs. 36 tags -RRB- all punctuation marks are represented as the single PONCT tag there are no separate tags for modal verbs whwords and possessives	conj_and_verbs_possessives conj_and_verbs_whwords amod_verbs_modal prep_for_tags_possessives prep_for_tags_whwords prep_for_tags_verbs amod_tags_separate neg_tags_no nsubj_are_tags expl_are_there nn_tag_PONCT amod_tag_single det_tag_the prep_as_represented_tag auxpass_represented_are nsubjpass_represented_marks nn_marks_punctuation det_marks_all num_tags_36 conj_vs._tags_tags num_tags_13 parataxis_smaller_are dep_smaller_represented dep_smaller_tags dep_smaller_tags cop_smaller_is nsubj_smaller_tagset vmod_smaller_Compared amod_Treebank_French det_Treebank_the prep_of_tagset_Treebank nn_tagset_POS det_tagset_the dep_al._1993 nn_al._et nn_al._Marcus dep_PTB_al. appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the prep_to_Compared_Treebank
P05-1039	J93-2004	o	It is available in several formats and in this paper we use the Penn Treebank -LRB- Marcus et al. 1993 -RRB- format of NEGRA	prep_of_format_NEGRA dep_format_Marcus nn_format_Treebank dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dobj_use_format nsubj_use_we ccomp_use_in ccomp_use_available det_paper_this pobj_in_paper amod_formats_several conj_and_available_in prep_in_available_formats cop_available_is nsubj_available_It
P05-1040	J93-2004	o	-LRB- 2001 -RRB- compare taggers trained and tested on the Wall Street Journal -LRB- WSJ Marcus et al. 1993 -RRB- and the Lancaster-Oslo-Bergen -LRB- LOB Johansson 1986 -RRB- corpora and find that the results for the WSJ perform significantly worse	advmod_worse_significantly dobj_perform_worse nsubj_perform_results mark_perform_that det_WSJ_the prep_for_results_WSJ det_results_the ccomp_find_perform dep_LOB_1986 appos_LOB_Johansson dep_Lancaster-Oslo-Bergen_corpora dep_Lancaster-Oslo-Bergen_LOB det_Lancaster-Oslo-Bergen_the nn_Marcus_al. nn_Marcus_et num_WSJ_1993 appos_WSJ_Marcus appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the prep_on_trained_Journal conj_and_trained_tested conj_and_taggers_Lancaster-Oslo-Bergen amod_taggers_tested amod_taggers_trained conj_and_compare_find dobj_compare_Lancaster-Oslo-Bergen dobj_compare_taggers dep_compare_2001
P05-1040	J93-2004	o	Given the estimated 3 % error rate of the WSJ tagging -LRB- Marcus et al. 1993 -RRB- they argue that the difference in performance is not sufficient to establish which of the two taggers is actually better	advmod_better_actually cop_better_is nsubj_better_which num_taggers_two det_taggers_the prep_of_which_taggers ccomp_establish_better aux_establish_to xcomp_sufficient_establish neg_sufficient_not cop_sufficient_is nsubj_sufficient_difference mark_sufficient_that prep_in_difference_performance det_difference_the ccomp_argue_sufficient nsubj_argue_they dep_argue_Given nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_tagging_WSJ det_tagging_the prep_of_rate_tagging nn_rate_error amod_rate_% amod_rate_estimated det_rate_the number_%_3 dep_Given_Marcus pobj_Given_rate
P05-1040	J93-2004	o	On the other hand structural annotation such as that used in syntactic treebanks -LRB- e.g. Marcus et al. 1993 -RRB- assigns a syntactic category to a contiguous sequence of corpus positions	nn_positions_corpus prep_of_sequence_positions amod_sequence_contiguous det_sequence_a amod_category_syntactic det_category_a prep_to_assigns_sequence dobj_assigns_category nsubj_assigns_used mark_assigns_that nn_al._et nn_al._Marcus dep_e.g._1993 dep_e.g._al. dep_treebanks_e.g. amod_treebanks_syntactic prep_in_used_treebanks prepc_such_as_annotation_assigns amod_annotation_structural amod_hand_other det_hand_the dep_``_annotation prep_on_``_hand
P05-1073	J93-2004	o	In the February 2004 version of the PropBank corpus annotations are done on top of the Penn TreeBank II parse trees -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dobj_parse_trees vmod_II_parse nn_II_TreeBank nn_II_Penn det_II_the dep_done_Marcus prep_on_top_of_done_II auxpass_done_are nsubjpass_done_annotations prep_in_done_version nn_corpus_PropBank det_corpus_the prep_of_version_corpus num_version_2004 nn_version_February det_version_the
P05-2004	J93-2004	o	4.2 Data The data comes from the CoNLL 2000 shared task -LRB- Sang and Buchholz 2000 -RRB- which consists of sentences from the Penn Treebank Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_from_sentences_corpus prep_of_consists_sentences nsubj_consists_which dep_Sang_2000 conj_and_Sang_Buchholz rcmod_task_consists appos_task_Buchholz appos_task_Sang amod_task_shared num_task_2000 nn_task_CoNLL det_task_the prep_from_comes_task nsubj_comes_data det_data_The dep_Data_Marcus rcmod_Data_comes num_Data_4.2 dep_``_Data
P05-2010	J93-2004	o	4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification where an annotator decides for every test item on an appropriate tag out of the pre-specified set of tags -LRB- Poesio and Vieira 1998 Webber and Byron 2004 Hearst 1997 Marcus et al. 1993 -RRB-	num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Hearst_1997 num_Webber_2004 conj_and_Webber_Byron dep_Poesio_Marcus conj_and_Poesio_Hearst conj_and_Poesio_Byron conj_and_Poesio_Webber conj_and_Poesio_1998 conj_and_Poesio_Vieira dep_tags_Hearst dep_tags_Webber dep_tags_1998 dep_tags_Vieira dep_tags_Poesio prep_of_set_tags amod_set_pre-specified det_set_the pobj_tag_set prepc_out_of_tag_of amod_tag_appropriate det_tag_an nn_item_test det_item_every nsubj_decides_annotator advmod_decides_where det_annotator_an rcmod_classification_decides prep_of_framework_classification det_framework_the prep_on_is_tag prep_for_is_item prep_within_is_framework nsubj_is_Analysis amod_annotators_human dobj_uses_annotators nsubj_uses_that rcmod_linguistics_uses amod_linguistics_computational prep_in_research_linguistics amod_research_existing det_research_the prep_of_Most_research amod_Data_Most amod_Data_Experimental prep_of_Analysis_Data num_Analysis_4 ccomp_``_is
P05-3018	J93-2004	n	a time-consuming process -LRB- Litman and Pan 2002 Marcus et al. 1993 Xia et al. 2000 Wiebe 2002 -RRB-	amod_Wiebe_2002 num_Xia_2000 nn_Xia_al. nn_Xia_et num_Marcus_1993 nn_Marcus_al. nn_Marcus_et dep_Litman_Wiebe conj_and_Litman_Xia conj_and_Litman_Marcus conj_and_Litman_2002 conj_and_Litman_Pan dep_process_Xia dep_process_Marcus dep_process_2002 dep_process_Pan dep_process_Litman amod_process_time-consuming det_process_a
P06-1021	J93-2004	o	For instance the Penn Treebank policy -LRB- Marcus et al. 1993 Marcus et al. 1994 -RRB- is to annotate the lowest node that is unfinished with an UNF tag as in Figure 4 -LRB- a -RRB-	appos_Figure_a num_Figure_4 pobj_in_Figure pcomp_as_in prep_tag_as nn_tag_UNF det_tag_an prep_with_unfinished_tag cop_unfinished_is nsubj_unfinished_that rcmod_node_unfinished amod_node_lowest det_node_the dobj_annotate_node aux_annotate_to xcomp_is_annotate nsubj_is_policy prep_for_is_instance num_Marcus_1994 nn_Marcus_al. nn_Marcus_et dep_Marcus_Marcus appos_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_policy_Marcus nn_policy_Treebank nn_policy_Penn det_policy_the
P06-1023	J93-2004	o	In an evaluation on the PENN treebank -LRB- Marcus et al. 1993 -RRB- the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore	amod_fscore_bracketing amod_fscore_labeled prep_of_terms_fscore nn_parsers_PCFG amod_parsers_unlexicalized amod_parsers_other prep_in_outperformed_terms dobj_outperformed_parsers nsubj_outperformed_parser advmod_outperformed_In det_parser_the nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_treebank_PENN det_treebank_the prep_on_evaluation_treebank det_evaluation_an dep_In_Marcus pobj_In_evaluation
P06-1023	J93-2004	o	1 Introduction Empty categories -LRB- also called null elements -RRB- are used in the annotation of the PENN treebank -LRB- Marcus et al. 1993 -RRB- in order to represent syntactic phenomena like constituent movement -LRB- e.g. whextraction -RRB- discontinuous constituents and missing elements -LRB- PRO elements empty complementizers and relative pronouns -RRB-	amod_pronouns_relative amod_complementizers_empty conj_and_elements_pronouns conj_and_elements_complementizers nn_elements_PRO dep_elements_pronouns dep_elements_complementizers dep_elements_elements dobj_missing_elements nsubjpass_missing_categories amod_constituents_discontinuous dep_whextraction_e.g. dep_movement_whextraction nn_movement_constituent nn_phenomena_syntactic prep_like_represent_movement dobj_represent_phenomena aux_represent_to dep_represent_order mark_represent_in amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_treebank_PENN det_treebank_the prep_of_annotation_treebank det_annotation_the conj_and_used_missing conj_and_used_constituents advcl_used_represent dep_used_Marcus prep_in_used_annotation auxpass_used_are nsubjpass_used_categories amod_elements_null dep_called_elements advmod_called_also dep_categories_called amod_categories_Empty nn_categories_Introduction num_categories_1
P06-1043	J93-2004	o	But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain the Wall Street Journal -LRB- WSJ -RRB- section of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_section_Marcus prep_of_section_Treebank nn_section_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the det_domain_that nn_data_training dep_using_section prep_from_using_domain dobj_using_data amod_domain_single det_domain_a xcomp_performed_using prep_on_performed_domain auxpass_performed_is nsubjpass_performed_much advmod_performed_where amod_work_current det_work_the prep_on_much_parsing prep_of_much_work rcmod_situation_performed det_situation_a prep_to_led_situation aux_led_has nsubj_led_lack cc_led_But prep_of_lack_corpora det_lack_the
P06-1043	J93-2004	o	3.2 Wall Street Journal Our out-of-domain data is the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- which consists of about 40,000 sentences -LRB- one million words -RRB- annotated with syntactic information	amod_information_syntactic prep_with_annotated_information num_words_million number_million_one amod_sentences_annotated appos_sentences_words num_sentences_40,000 quantmod_40,000_about prep_of_consists_sentences nsubj_consists_which amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the rcmod_portion_consists appos_portion_Marcus prep_of_portion_Treebank dep_Journal_portion appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the cop_Journal_is nsubj_Journal_data amod_data_out-of-domain poss_data_Our rcmod_Journal_Journal nn_Journal_Street nn_Journal_Wall num_Journal_3.2 dep_``_Journal
P06-1060	J93-2004	o	There are cases though where the labels consist of several related but not entirely correlated properties examples include mention detectionthe task we are interested in syntactic parsing with functional tag assignment -LRB- besides identifying the syntactic parse also label the constituent nodes with their functional category as defined in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- -RRB- and to a lesser extent part-of-speech tagging in highly inflected languages .4 The particular type of mention detection that we are examining in this paper follows the ACE general definition each mention in the text -LRB- a reference to a real-world entity -RRB- is assigned three types of information :5 An entity type describing the type of the entity it points to -LRB- e.g. person location organization etc -RRB- An entity subtype further detailing the type -LRB- e.g. organizations can be commercial governmental and non-profit while locations can be a nation population center or an international region -RRB- A mention type specifying the way the entity is realized a mention can be named -LRB- e.g. John Smith -RRB- nominal -LRB- e.g. professor -RRB- or pronominal -LRB- e.g. she -RRB-	pobj_e.g._she prep_pronominal_e.g. nn_professor_e.g. dep_nominal_professor nn_Smith_John dep_Smith_e.g. dep_named_Smith auxpass_named_be aux_named_can nsubjpass_named_mention det_mention_a ccomp_realized_named auxpass_realized_is nsubjpass_realized_entity det_entity_the rcmod_way_realized det_way_the conj_or_specifying_pronominal conj_or_specifying_nominal dobj_specifying_way nn_type_mention det_type_A amod_region_international det_region_an nn_center_population det_nation_a cop_nation_be aux_nation_can nsubj_nation_locations mark_nation_while nsubj_non-profit_organizations nsubj_governmental_organizations conj_or_commercial_region conj_and_commercial_center advcl_commercial_nation conj_and_commercial_non-profit conj_and_commercial_governmental cop_commercial_be aux_commercial_can nsubj_commercial_organizations dep_commercial_e.g. dep_type_type dep_type_region dep_type_center dep_type_non-profit dep_type_governmental dep_type_commercial det_type_the dobj_detailing_type xcomp_further_detailing nn_subtype_entity det_subtype_An conj_location_etc conj_location_organization dep_person_pronominal dep_person_nominal dep_person_specifying dep_person_further dep_person_subtype dep_person_location dep_person_e.g. prep_to_points_person nsubj_points_it det_entity_the rcmod_type_points prep_of_type_entity det_type_the dobj_describing_type nn_type_entity det_type_An num_type_:5 dep_information_type prep_of_types_information num_types_three vmod_assigned_describing dobj_assigned_types auxpass_assigned_is nsubjpass_assigned_mention amod_entity_real-world det_entity_a prep_to_reference_entity det_reference_a dep_text_reference det_text_the prep_in_mention_text det_mention_each amod_definition_general nn_definition_ACE det_definition_the parataxis_follows_assigned dobj_follows_definition nsubj_follows_tagging nsubj_follows_to nsubj_follows_parsing advmod_follows_in det_paper_this prep_in_examining_paper aux_examining_are nsubj_examining_we mark_examining_that nn_detection_mention dep_type_examining prep_of_type_detection amod_type_particular det_type_The num_type_.4 dep_languages_type amod_languages_inflected advmod_inflected_highly prep_in_tagging_languages amod_tagging_part-of-speech amod_extent_lesser det_extent_a pobj_to_extent nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the prep_in_defined_Treebank mark_defined_as amod_category_functional poss_category_their nn_nodes_constituent det_nodes_the prep_with_label_category dobj_label_nodes advmod_label_also amod_parse_syntactic det_parse_the dobj_identifying_parse dep_besides_Marcus advcl_besides_defined conj_besides_label pcomp_besides_identifying nn_assignment_tag amod_assignment_functional conj_and_parsing_tagging conj_and_parsing_to dep_parsing_besides prep_with_parsing_assignment amod_parsing_syntactic dep_interested_follows cop_interested_are nsubj_interested_we rcmod_task_interested nn_task_detectionthe nn_task_mention dobj_include_task nsubj_include_examples advmod_correlated_entirely neg_entirely_not parataxis_related_include conj_but_related_properties conj_but_related_correlated amod_related_several prep_of_consist_properties prep_of_consist_correlated prep_of_consist_related nsubj_consist_labels advmod_consist_where det_labels_the dep_though_consist advmod_are_though nsubj_are_cases expl_are_There
P06-1063	J93-2004	o	Large treebanks are available for major languages however these are often based on a speci c text type or genre e.g. nancial newspaper text -LRB- the Penn-II Treebank -LRB- Marcus et al. 1993 -RRB- -RRB-	dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_Marcus nn_Treebank_Penn-II det_Treebank_the dep_text_Treebank nn_text_newspaper amod_text_nancial nn_text_e.g. appos_type_text conj_or_type_genre nn_type_text nn_type_c nn_type_speci det_type_a amod_languages_major prep_based_on_available_genre prep_based_on_available_type advmod_available_often auxpass_available_are nsubjpass_available_these advmod_available_however prep_for_available_languages cop_available_are nsubj_available_treebanks amod_treebanks_Large
P06-1064	J93-2004	o	1 Introduction A number of wide-coverage TAG CCG LFG and HPSG grammars -LRB- Xia 1999 Chen et al. 2005 Hockenmaier and Steedman 2002a ODonovan et al. 2005 Miyao et al. 2004 -RRB- have been extracted from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and have enabled the creation of widecoverage parsers for English which recover local and non-local dependencies that approximate the underlying predicate-argument structure -LRB- Hockenmaier and Steedman 2002b Clark and Curran 2004 Miyao and Tsujii 2005 Shen and Joshi 2005 -RRB-	amod_Miyao_2005 conj_and_Miyao_Joshi conj_and_Miyao_Shen conj_and_Miyao_2005 conj_and_Miyao_Tsujii num_Clark_2004 conj_and_Clark_Curran dep_Hockenmaier_Joshi dep_Hockenmaier_Shen dep_Hockenmaier_2005 dep_Hockenmaier_Tsujii dep_Hockenmaier_Miyao conj_and_Hockenmaier_Curran conj_and_Hockenmaier_Clark conj_and_Hockenmaier_2002b conj_and_Hockenmaier_Steedman dep_structure_Clark dep_structure_2002b dep_structure_Steedman dep_structure_Hockenmaier nn_structure_predicate-argument amod_structure_underlying det_structure_the amod_structure_approximate dep_that_structure dep_dependencies_that amod_dependencies_non-local amod_dependencies_local conj_and_local_non-local dobj_recover_dependencies nsubj_recover_which rcmod_English_recover prep_for_parsers_English nn_parsers_widecoverage prep_of_creation_parsers det_creation_the dobj_enabled_creation aux_enabled_have nsubjpass_enabled_number amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the conj_and_extracted_enabled dep_extracted_Marcus prep_from_extracted_Treebank auxpass_extracted_been aux_extracted_have nsubjpass_extracted_number num_Miyao_2004 nn_Miyao_al. nn_Miyao_et dep_ODonovan_Miyao num_ODonovan_2005 nn_ODonovan_al. nn_ODonovan_et conj_and_Hockenmaier_Steedman num_Chen_2005 nn_Chen_al. nn_Chen_et dep_Xia_ODonovan appos_Xia_2002a dep_Xia_Steedman dep_Xia_Hockenmaier dep_Xia_Chen dep_Xia_1999 appos_grammars_Xia nn_grammars_HPSG conj_and_TAG_grammars conj_and_TAG_LFG conj_and_TAG_CCG amod_TAG_wide-coverage prep_of_number_grammars prep_of_number_LFG prep_of_number_CCG prep_of_number_TAG det_number_A nn_number_Introduction num_number_1
P06-1123	J93-2004	o	3.3 Methods We parsed the English side of each bilingual bitext and both sides of each English/English bitext using an off-the-shelf syntactic parser -LRB- Bikel 2004 -RRB- which was trained on sections 02-21 of the Penn English Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_English nn_Treebank_Penn det_Treebank_the prep_of_sections_Treebank num_sections_02-21 prep_on_trained_sections auxpass_trained_was nsubjpass_trained_which dep_Bikel_2004 rcmod_parser_trained appos_parser_Bikel nn_parser_syntactic amod_parser_off-the-shelf det_parser_an dep_using_Marcus dobj_using_parser nsubj_using_sides amod_bitext_English/English det_bitext_each prep_of_sides_bitext det_sides_both amod_bitext_bilingual det_bitext_each conj_and_side_using prep_of_side_bitext amod_side_English det_side_the dobj_parsed_using dobj_parsed_side nsubj_parsed_We rcmod_Methods_parsed num_Methods_3.3 dep_``_Methods
P06-2002	J93-2004	o	2.2 Generalization pseudocode In order to identify the portions in common between the patterns and to generalize them we apply the following pseudocode -LRB- Ruiz-Casado et al. in press -RRB- 1All the PoS examples in this paper are done with Penn Treebank labels -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_labels_Treebank nn_labels_Penn dep_done_Marcus prep_with_done_labels auxpass_done_are nsubjpass_done_examples det_paper_this prep_in_examples_paper nn_examples_PoS det_examples_the nn_examples_1All prep_in_Ruiz-Casado_press dep_Ruiz-Casado_al. nn_Ruiz-Casado_et appos_pseudocode_Ruiz-Casado prep_following_the_pseudocode parataxis_apply_done dobj_apply_the nsubj_apply_we nsubj_apply_pseudocode dobj_generalize_them aux_generalize_to det_patterns_the prep_between_common_patterns prep_in_portions_common det_portions_the conj_and_identify_generalize dobj_identify_portions aux_identify_to dep_identify_order mark_identify_In dep_pseudocode_generalize dep_pseudocode_identify nn_pseudocode_Generalization num_pseudocode_2.2
P06-2004	J93-2004	o	With the exception of -LRB- Hindle and Rooth 1993 -RRB- most unsupervised work on PP attachment is based on superficial analysis of the unlabeled corpus without the use of partial parsing -LRB- Volk 2001 Calvo et al. 2005 -RRB-	num_Calvo_2005 nn_Calvo_al. nn_Calvo_et dep_Volk_Calvo amod_Volk_2001 appos_parsing_Volk amod_parsing_partial prep_of_use_parsing det_use_the amod_corpus_unlabeled det_corpus_the prep_without_analysis_use prep_of_analysis_corpus amod_analysis_superficial prep_on_based_analysis auxpass_based_is nsubjpass_based_work nn_attachment_PP prep_on_work_attachment amod_work_unsupervised amod_work_most rcmod_Hindle_based dep_Hindle_1993 conj_and_Hindle_Rooth prep_of_exception_Rooth prep_of_exception_Hindle det_exception_the pobj_With_exception dep_``_With
P06-2004	J93-2004	o	The labeled corpus is the Penn Wall Street Journal treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_treebank_Marcus nn_treebank_Journal nn_treebank_Street nn_treebank_Wall nn_treebank_Penn det_treebank_the cop_treebank_is nsubj_treebank_corpus amod_corpus_labeled det_corpus_The
P06-2004	J93-2004	o	1 Introduction The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and the prepositional phrase data set first described in -LRB- Ratnaparkhi et al. 1994 -RRB-	amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_in_Ratnaparkhi prep_described_in advmod_described_first vmod_set_described vmod_data_set nn_data_phrase amod_data_prepositional det_data_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Treebank_data dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_such_as_corpora_data prep_such_as_corpora_Treebank amod_corpora_annotated prep_on_training_corpora amod_training_supervised prep_on_based_training auxpass_based_are nsubjpass_based_systems nn_processing_language amod_processing_natural prep_in_tasks_processing amod_tasks_many prep_for_systems_tasks amod_systems_performing amod_systems_best det_systems_The rcmod_Introduction_based num_Introduction_1
P06-2009	J93-2004	o	4 Experiments and Results We use the standard corpus for this task the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the dep_,_Treebank det_task_this amod_corpus_standard det_corpus_the prep_for_use_task dobj_use_corpus nsubj_use_We rcmod_Experiments_use conj_and_Experiments_Results num_Experiments_4 dep_``_Results dep_``_Experiments
P06-2009	J93-2004	o	Policy #Shift #Left #Right Start over 156545 26351 27918 Stay 117819 26351 27918 Step back 43374 26351 27918 Table 1 The number of actions required to build all the trees for the sentences in section 23 of Penn Treebank -LRB- Marcus et al. 1993 -RRB- as a function of the focus point placement policy	nn_policy_placement nn_policy_point nn_policy_focus det_policy_the prep_of_function_policy det_function_a amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn prep_of_section_Treebank num_section_23 prep_in_sentences_section det_sentences_the det_trees_the predet_trees_all prep_for_build_sentences dobj_build_trees aux_build_to xcomp_required_build vmod_actions_required prep_as_number_function dep_number_Marcus prep_of_number_actions det_number_The num_Table_1 num_Table_27918 num_Table_26351 num_Table_43374 dep_Step_Table advmod_Step_back num_Step_27918 num_Step_26351 number_26351_117819 dep_26351_Stay number_26351_27918 number_26351_26351 number_26351_156545 dep_Start_number prep_over_Start_Step nn_Start_#Right nn_Start_#Left nn_Start_#Shift nn_Start_Policy
P06-2010	J93-2004	o	The data consist of sections of the Wall Street Journal -LRB- WSJ -RRB- part of the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- with information on predicate-argument structures extracted from the PropBank corpus -LRB- Palmer et al. 2005 -RRB-	amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et nn_corpus_PropBank det_corpus_the prep_from_extracted_corpus vmod_structures_extracted amod_structures_predicate-argument prep_on_information_structures amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the prep_of_part_TreeBank nn_part_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the prep_of_sections_part dep_consist_Palmer prep_with_consist_information dep_consist_Marcus prep_of_consist_sections nsubj_consist_data det_data_The
P06-2028	J93-2004	o	Typically the local context around the 215 word to be sense-tagged is used to disambiguate the sense -LRB- Yarowsky 1993 -RRB- and it is common for linguistic resources such as WordNet -LRB- Li et al. 1995 Mihalcea and Moldovan 1998 Ramakrishnan and Prithviraj 2004 -RRB- or bilingual data -LRB- Li and Li 2002 -RRB- to be employed as well as more longrange context	nn_context_longrange amod_context_more conj_and_employed_context auxpass_employed_be aux_employed_to amod_Li_2002 conj_and_Li_Li amod_data_bilingual dep_Ramakrishnan_2004 conj_and_Ramakrishnan_Prithviraj num_Mihalcea_1998 conj_and_Mihalcea_Moldovan vmod_Li_context vmod_Li_employed dep_Li_Li dep_Li_Li conj_or_Li_data dep_Li_Prithviraj dep_Li_Ramakrishnan conj_or_Li_Moldovan conj_or_Li_Mihalcea num_Li_1995 dep_Li_al. nn_Li_et dep_WordNet_data dep_WordNet_Mihalcea dep_WordNet_Li prep_such_as_resources_WordNet amod_resources_linguistic prep_for_common_resources cop_common_is nsubj_common_it dep_Yarowsky_1993 appos_sense_Yarowsky det_sense_the dobj_disambiguate_sense aux_disambiguate_to conj_and_used_common xcomp_used_disambiguate auxpass_used_is nsubjpass_used_context advmod_used_Typically cop_sense-tagged_be aux_sense-tagged_to vmod_word_sense-tagged num_word_215 det_word_the prep_around_context_word amod_context_local det_context_the
P06-2067	J93-2004	o	1 Introduction Robust statistical syntactic parsers made possible by new statistical techniques -LRB- Collins 1999 Charniak 2000 Bikel 2004 -RRB- and by the availability of large hand-annotated training corpora such as WSJ -LRB- Marcus et al. 1993 -RRB- and Switchboard -LRB- Godefrey et al. 1992 -RRB- have had a major impact on the field of natural language processing	nn_processing_language amod_processing_natural prep_of_field_processing det_field_the prep_on_impact_field amod_impact_major det_impact_a dobj_had_impact aux_had_have amod_Godefrey_1992 dep_Godefrey_al. nn_Godefrey_et amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_WSJ_Godefrey conj_and_WSJ_Switchboard dep_WSJ_Marcus prep_such_as_corpora_Switchboard prep_such_as_corpora_WSJ nn_corpora_training amod_corpora_hand-annotated amod_corpora_large prep_of_availability_corpora det_availability_the dep_Bikel_2004 appos_Charniak_2000 dep_Collins_Bikel dep_Collins_Charniak amod_Collins_1999 conj_and_techniques_availability appos_techniques_Collins amod_techniques_statistical amod_techniques_new dep_made_had prep_by_made_availability prep_by_made_techniques acomp_made_possible nsubj_made_parsers nn_parsers_syntactic amod_parsers_statistical amod_parsers_Robust nn_parsers_Introduction num_parsers_1
P06-2069	J93-2004	o	TB TBR JJ JJR JJS JJ RB RBR RBS RB CD LS CD CC CC DT WDT PDT DT FW FW MD VB VBD VBG VBN VBP VBZ VH VHD VHG VHN VHP VHZ MD NN NNS NP NPS NN PP WP PP$ WP$ EX WRB PP IN TO IN POS PO RP RP SYM SY UH UH VV VVD VVG VVN VVP VVZ VB -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_VB_Marcus nn_VB_VVZ appos_VVD_VB conj_VVD_VVP conj_VVD_VVN conj_VVD_VVG appos_VV_VVD nn_VV_UH nn_VV_UH nn_VV_SY nn_VV_SYM nn_VV_RP nn_VV_RP nn_VV_PO nn_VV_POS prep_in_TO_VV dep_IN_TO prep_PP_IN nn_PP_WRB nn_PP_NN nn_PP_NPS nn_NN_MD nn_NN_VHZ appos_VB_PP conj_VB_EX conj_VB_WP$ conj_VB_PP$ conj_VB_WP conj_VB_PP conj_VB_NP conj_VB_NNS conj_VB_NN conj_VB_VHP conj_VB_VHN conj_VB_VHG conj_VB_VHD conj_VB_VH conj_VB_VBZ conj_VB_VBP conj_VB_VBN conj_VB_VBG conj_VB_VBD appos_MD_VB nn_MD_FW nn_MD_FW nn_MD_DT nn_MD_PDT nn_DT_CC nn_DT_CC nn_DT_CD nn_DT_LS nn_CD_RB nn_CD_RBS appos_RB_MD appos_RB_WDT appos_RB_DT appos_RB_CD appos_RB_RBR nn_RB_JJ nn_RB_JJS appos_JJ_RB appos_JJ_JJR nn_JJ_TBR nn_JJ_TB
P06-2088	J93-2004	o	The experiment used all 578 sentences in the ATIS corpus with a parse tree in the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_Marcus_1993 nn_Treebank_Penn det_Treebank_the nn_tree_parse det_tree_a nn_corpus_ATIS det_corpus_the num_sentences_578 det_sentences_all dep_used_Marcus prep_in_used_Treebank prep_with_used_tree prep_in_used_corpus dobj_used_sentences nsubj_used_experiment det_experiment_The
P06-2089	J93-2004	p	However evaluations on the widely used WSJ corpus of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- show that the accuracy of these parsers still lags behind the state-of-theart	det_state-of-theart_the prep_behind_lags_state-of-theart advmod_lags_still nsubj_lags_accuracy mark_lags_that det_parsers_these prep_of_accuracy_parsers det_accuracy_the ccomp_show_lags nsubj_show_evaluations advmod_show_However amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_corpus_Treebank nn_corpus_WSJ amod_corpus_used det_corpus_the advmod_used_widely dep_evaluations_Marcus prep_on_evaluations_corpus
P06-2089	J93-2004	o	4 Experiments We evaluated our classifier-based best-first parser on the Wall Street Journal corpus of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- using the standard split sections 2-21 were used for training section 22 was used for development and tuning of parameters and features and section 23 was used for testing	prep_for_used_testing auxpass_used_was nsubjpass_used_section num_section_23 conj_and_parameters_features prep_of_development_features prep_of_development_parameters conj_and_development_tuning conj_and_used_used prep_for_used_tuning prep_for_used_development auxpass_used_was nsubjpass_used_section num_section_22 parataxis_used_used parataxis_used_used prep_for_used_training auxpass_used_were nsubjpass_used_sections num_sections_2-21 amod_split_standard det_split_the parataxis_using_used dobj_using_split amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_corpus_Treebank nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the amod_parser_best-first amod_parser_classifier-based poss_parser_our prep_on_evaluated_corpus dobj_evaluated_parser nsubj_evaluated_We dep_Experiments_using dep_Experiments_Marcus rcmod_Experiments_evaluated num_Experiments_4
P06-3014	J93-2004	p	1 Introduction Robust statistical syntactic parsers made possible by new statistical techniques -LRB- Collins 1999 Charniak 2000 Bikel 2004 -RRB- and by the availability of large hand-annotated training corpora such as WSJ -LRB- Marcus et al. 1993 -RRB- and Switchboard -LRB- Godefrey et al. 1992 -RRB- have had a major impact on the field of natural language processing	nn_processing_language amod_processing_natural prep_of_field_processing det_field_the prep_on_impact_field amod_impact_major det_impact_a dobj_had_impact aux_had_have amod_Godefrey_1992 dep_Godefrey_al. nn_Godefrey_et amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_WSJ_Godefrey conj_and_WSJ_Switchboard dep_WSJ_Marcus prep_such_as_corpora_Switchboard prep_such_as_corpora_WSJ nn_corpora_training amod_corpora_hand-annotated amod_corpora_large prep_of_availability_corpora det_availability_the dep_Bikel_2004 appos_Charniak_2000 dep_Collins_Bikel dep_Collins_Charniak amod_Collins_1999 conj_and_techniques_availability appos_techniques_Collins amod_techniques_statistical amod_techniques_new dep_made_had prep_by_made_availability prep_by_made_techniques acomp_made_possible nsubj_made_parsers nn_parsers_syntactic amod_parsers_statistical amod_parsers_Robust nn_parsers_Introduction num_parsers_1
P07-1026	J93-2004	o	The data consists of sections of the Wall Street Journal part of the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- with information on predicate-argument structures extracted from the PropBank corpus -LRB- Palmer et al. 2005 -RRB-	amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et nn_corpus_PropBank det_corpus_the prep_from_extracted_corpus vmod_structures_extracted amod_structures_predicate-argument prep_on_information_structures amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the prep_of_part_TreeBank nn_part_Journal nn_part_Street nn_part_Wall det_part_the prep_of_sections_part dep_consists_Palmer prep_with_consists_information dep_consists_Marcus prep_of_consists_sections nsubj_consists_data det_data_The
P07-1031	J93-2004	p	1 Introduction The Penn Treebank -LRB- Marcus et al. 1993 -RRB- is perhaps the most in uential resource in Natural Language Processing -LRB- NLP -RRB-	appos_Processing_NLP nn_Processing_Language amod_Processing_Natural prep_in_resource_Processing amod_resource_uential prep_in_the_resource advmod_the_most dep_perhaps_the advmod_is_perhaps amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_is dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_The dep_Introduction_Treebank num_Introduction_1
P07-1031	J93-2004	o	RECALL F-SCORE Brackets 89.17 87.50 88.33 Dependencies 96.40 96.40 96.40 Brackets revised 97.56 98.03 97.79 Dependencies revised 99.27 99.27 99.27 Table 1 Agreement between annotators few weeks and increased to about 1000 words per hour after gaining more experience -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_experience_more dobj_gaining_experience prep_per_words_hour num_words_1000 quantmod_1000_about prep_to_increased_words amod_weeks_few nn_weeks_annotators dep_Agreement_Marcus prepc_after_Agreement_gaining conj_and_Agreement_increased prep_between_Agreement_weeks dep_Table_increased dep_Table_Agreement num_Table_1 num_Table_99.27 num_Table_99.27 num_Table_99.27 dobj_revised_Table vmod_Dependencies_revised num_Dependencies_97.79 num_Dependencies_98.03 number_98.03_97.56 dobj_revised_Dependencies vmod_Brackets_revised num_Brackets_96.40 num_Brackets_96.40 nn_Brackets_Dependencies num_Brackets_88.33 num_Brackets_87.50 number_96.40_96.40 number_87.50_89.17 dep_Brackets_Brackets nn_Brackets_F-SCORE nn_Brackets_RECALL
P07-1035	J93-2004	o	For both experiments we used dependency trees extracted from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- using the head rules and dependency extractor from Yamada and Matsumoto -LRB- 2003 -RRB-	appos_Matsumoto_2003 conj_and_Yamada_Matsumoto nn_extractor_dependency conj_and_rules_extractor nn_rules_head det_rules_the prep_from_using_Matsumoto prep_from_using_Yamada dobj_using_extractor dobj_using_rules amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_extracted_Treebank vmod_trees_extracted nn_trees_dependency vmod_used_using dep_used_Marcus dobj_used_trees nsubj_used_we prep_for_used_experiments det_experiments_both
P07-1062	J93-2004	o	The RST-DT consists of 385 documents from the Wall Street Journal about 176,000 words which overlaps with the Penn Wall St. Journal -LRB- WSJ -RRB- Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Journal appos_Journal_WSJ nn_Journal_St. nn_Journal_Wall nn_Journal_Penn det_Journal_the prep_with_overlaps_Treebank nsubj_overlaps_which dep_words_Marcus rcmod_words_overlaps num_words_176,000 quantmod_176,000_about nn_Journal_Street nn_Journal_Wall det_Journal_the prep_from_documents_Journal num_documents_385 dobj_consists_words prep_of_consists_documents nsubj_consists_RST-DT det_RST-DT_The
P07-1071	J93-2004	o	The current version of the dataset gives semantic tags for the same sentencesas inthe PennTreebank -LRB- Marcuset al. 1993 -RRB- whichareexcerptsfromtheWallStreetJournal	amod_Marcuset_1993 dep_Marcuset_al. appos_PennTreebank_whichareexcerptsfromtheWallStreetJournal dep_PennTreebank_Marcuset nn_PennTreebank_inthe nn_PennTreebank_sentencesas amod_PennTreebank_same det_PennTreebank_the amod_tags_semantic prep_for_gives_PennTreebank dobj_gives_tags nsubj_gives_version det_dataset_the prep_of_version_dataset amod_version_current det_version_The
P07-1079	J93-2004	o	We created a dependency training corpus based on the Penn Treebank -LRB- Marcus et al. 1993 -RRB- or more specifically on the HPSG Treebank generated from the Penn Treebank -LRB- see section 2.2 -RRB-	num_section_2.2 dobj_see_section nn_Treebank_Penn det_Treebank_the prep_from_generated_Treebank nn_Treebank_HPSG det_Treebank_the pobj_on_Treebank advmod_on_specifically dep_specifically_more amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_or_Treebank_on dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_on_based_on prep_on_based_Treebank vmod_corpus_based nn_corpus_training nn_corpus_dependency det_corpus_a dep_created_see dep_created_generated dobj_created_corpus nsubj_created_We
P07-1079	J93-2004	o	4 Experiments We evaluate the accuracy of HPSG parsing with dependencyconstraintsontheHPSGTreebank -LRB- Miyao et al. 2003 -RRB- which is extracted from the Wall Street Journal portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- 1	num_Marcus_1 dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_Journal nn_portion_Street nn_portion_Wall det_portion_the prep_from_extracted_portion auxpass_extracted_is nsubjpass_extracted_which amod_Miyao_2003 dep_Miyao_al. nn_Miyao_et prep_with_parsing_dependencyconstraintsontheHPSGTreebank vmod_HPSG_parsing prep_of_accuracy_HPSG det_accuracy_the dobj_evaluate_accuracy nsubj_evaluate_We dep_Experiments_Marcus rcmod_Experiments_extracted dep_Experiments_Miyao rcmod_Experiments_evaluate num_Experiments_4
P07-1080	J93-2004	o	We used the Penn Treebank WSJ corpus -LRB- Marcus et al. 1993 -RRB- to perform the empirical evaluation of the considered approaches	amod_approaches_considered det_approaches_the prep_of_evaluation_approaches amod_evaluation_empirical det_evaluation_the dobj_perform_evaluation aux_perform_to amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_WSJ nn_corpus_Treebank nn_corpus_Penn det_corpus_the vmod_used_perform dep_used_Marcus dobj_used_corpus nsubj_used_We
P07-1120	J93-2004	o	First we trained a finitestate shallow parser on base phrases extracted from the Penn Wall St. Journal -LRB- WSJ -RRB- Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Journal appos_Journal_WSJ nn_Journal_St. nn_Journal_Wall nn_Journal_Penn det_Journal_the prep_from_extracted_Treebank vmod_phrases_extracted nn_phrases_base prep_on_parser_phrases amod_parser_shallow nn_parser_finitestate det_parser_a dobj_trained_parser nsubj_trained_we advmod_trained_First
P08-1039	J93-2004	n	This is because their training data the Penn Treebank -LRB- Marcus et al. 1993 -RRB- does not fully annotate NP structure	nn_structure_NP dobj_annotate_structure advmod_annotate_fully neg_annotate_not aux_annotate_does nsubj_annotate_data mark_annotate_because amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the appos_data_Treebank nn_data_training poss_data_their advcl_is_annotate nsubj_is_This
P08-1042	J93-2004	o	Treebank -LRB- Marcus et al. 1993 -RRB- six of which are errors	cop_errors_are nsubj_errors_six prep_of_six_which amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et rcmod_Treebank_errors dep_Treebank_Marcus
P08-1061	J93-2004	o	For experiment on English we used the English Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB- and the constituency structures were converted to dependency trees using the same rules as -LRB- Yamada and Matsumoto 2003 -RRB-	amod_Yamada_2003 conj_and_Yamada_Matsumoto dep_as_Matsumoto dep_as_Yamada amod_rules_same det_rules_the prep_using_as dobj_using_rules nn_trees_dependency xcomp_converted_using prep_to_converted_trees auxpass_converted_were nsubjpass_converted_structures nn_structures_constituency det_structures_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus appos_Treebank_PTB nn_Treebank_Penn nn_Treebank_English det_Treebank_the conj_and_used_converted dobj_used_Treebank nsubj_used_we prep_for_used_experiment prep_on_experiment_English
P08-1067	J93-2004	o	5 Experiments We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_English nn_Treebank_Penn det_Treebank_the amod_reranking_n-best nn_reranker_forest poss_reranker_our prep_on_performance_Treebank prep_against_performance_reranking prep_of_performance_reranker det_performance_the dobj_compare_performance nsubj_compare_We dep_Experiments_Marcus rcmod_Experiments_compare num_Experiments_5
P08-1068	J93-2004	o	We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and Prague Dependency Treebank -LRB- Hajic 1998 Hajic et al. 2001 -RRB- -LRB- see Sections 4.1 and 4.3 -RRB-	conj_and_4.1_4.3 dep_Sections_4.3 dep_Sections_4.1 dobj_see_Sections num_Hajic_2001 nn_Hajic_al. nn_Hajic_et dep_Hajic_Hajic dep_Hajic_1998 dep_Treebank_Hajic nn_Treebank_Dependency nn_Treebank_Prague dep_Marcus_see conj_and_Marcus_Treebank amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the nn_experiments_parsing prep_on_performing_Treebank dobj_performing_experiments amod_datasets_fixed dep_improvements_Treebank dep_improvements_Marcus prepc_by_improvements_performing prep_for_improvements_datasets nn_improvements_yields nn_improvements_approach amod_improvements_semi-supervised poss_improvements_our prep_that_show_improvements nsubj_show_We
P08-1068	J93-2004	o	The English experiments were performed on the Penn Treebank -LRB- Marcus et al. 1993 -RRB- using a standard set of head-selection rules -LRB- Yamada and Matsumoto 2003 -RRB- to convert the phrase structure syntax of the Treebank to a dependency tree representation .6 We split the Treebank into a training set -LRB- Sections 221 -RRB- a development set -LRB- Section 22 -RRB- and several test sets -LRB- Sections 0,7 1 23 and 24 -RRB-	number_1_0,7 conj_and_Sections_24 conj_and_Sections_23 num_Sections_1 dep_sets_24 dep_sets_23 dep_sets_Sections nn_sets_test amod_sets_several num_Section_22 appos_set_Section nn_set_development det_set_a num_Sections_221 conj_and_set_sets conj_and_set_set appos_set_Sections nn_set_training det_set_a det_Treebank_the prep_into_split_sets prep_into_split_set prep_into_split_set dobj_split_Treebank nsubj_split_We rcmod_representation_split num_representation_.6 nn_representation_tree nn_representation_dependency det_representation_a det_Treebank_the prep_of_syntax_Treebank dep_structure_syntax nn_structure_phrase det_structure_the prep_to_convert_representation dobj_convert_structure aux_convert_to dep_Yamada_2003 conj_and_Yamada_Matsumoto dep_rules_Matsumoto dep_rules_Yamada amod_rules_head-selection prep_of_set_rules amod_set_standard det_set_a vmod_using_convert dobj_using_set amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the xcomp_performed_using dep_performed_Marcus prep_on_performed_Treebank auxpass_performed_were nsubjpass_performed_experiments amod_experiments_English det_experiments_The
P08-1082	J93-2004	o	The text was split at the sentence level tokenized and PoS tagged in the style of the Wall Street Journal Penn TreeBank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn nn_TreeBank_Journal nn_TreeBank_Street nn_TreeBank_Wall det_TreeBank_the prep_of_style_TreeBank det_style_the nsubj_tagged_PoS nsubjpass_tokenized_text nn_level_sentence det_level_the dep_split_Marcus prep_in_split_style conj_and_split_tagged conj_and_split_tokenized prep_at_split_level auxpass_split_was nsubjpass_split_text det_text_The
P08-1082	J93-2004	o	This probability is computed using IBMs Model 1 -LRB- Brown et al. 1993 -RRB- P -LRB- Q | A -RRB- = productdisplay qQ P -LRB- q | A -RRB- -LRB- 3 -RRB- P -LRB- q | A -RRB- = -LRB- 1 -RRB- Pml -LRB- q | A -RRB- + Pml -LRB- q | C -RRB- -LRB- 4 -RRB- Pml -LRB- q | A -RRB- = summationdisplay aA -LRB- T -LRB- q | a -RRB- Pml -LRB- a | A -RRB- -RRB- -LRB- 5 -RRB- where the probability that the question term q is generated from answer A P -LRB- q | A -RRB- is smoothed using the prior probability that the term q is generated from the entire collection of answers C Pml -LRB- q | C -RRB-	nn_C_| amod_C_q appos_Pml_C appos_C_Pml dep_answers_C prep_of_collection_answers amod_collection_entire det_collection_the prep_from_generated_collection auxpass_generated_is nsubjpass_generated_q mark_generated_that nn_q_term det_q_the ccomp_probability_generated amod_probability_prior det_probability_the dobj_using_probability xcomp_smoothed_using auxpass_smoothed_is nsubjpass_smoothed_question mark_smoothed_that nn_A_| amod_A_q appos_P_A appos_A_P nn_A_answer prep_from_generated_A auxpass_generated_is nsubjpass_generated_q nn_q_term rcmod_question_generated det_question_the ccomp_probability_smoothed det_probability_the dep_where_probability nn_A_| det_A_a appos_Pml_A det_Pml_a dep_|_Pml amod_|_q nn_|_T dep_aA_where appos_aA_5 appos_aA_| nn_aA_summationdisplay dobj_=_aA nsubj_=_Pml dep_=_4 nn_A_| amod_A_q appos_Pml_A nn_C_| amod_C_q rcmod_Pml_= appos_Pml_C nn_A_| amod_A_q conj_+_Pml_Pml appos_Pml_A dobj_1_Pml dobj_1_Pml dep_=_1 nsubj_=_P dep_=_3 dep_=_P dep_=_= dep_=_P dep_=_Model dep_=_IBMs dep_=_using nn_A_| amod_A_q appos_P_A nn_A_| amod_A_q appos_P_A nn_P_qQ nn_P_productdisplay num_A_| nn_A_Q appos_P_A amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Brown num_Model_1 ccomp_computed_= auxpass_computed_is nsubjpass_computed_probability det_probability_This ccomp_``_computed
P08-1098	J93-2004	o	The WSJ corpus is based on the WSJ part of the PENN TREEBANK -LRB- Marcus et al. 1993 -RRB- we used the first 10,000 sentences of section 2-21 as the pool set and section 00 as evaluation set -LRB- 1,921 sentences -RRB-	num_sentences_1,921 appos_set_sentences nn_set_evaluation prep_as_section_set num_section_00 nn_set_pool det_set_the num_section_2-21 prep_of_sentences_section num_sentences_10,000 amod_sentences_first det_sentences_the conj_and_used_section prep_as_used_set dobj_used_sentences nsubj_used_we nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_TREEBANK_PENN det_TREEBANK_the prep_of_part_TREEBANK nn_part_WSJ det_part_the parataxis_based_section parataxis_based_used dep_based_Marcus prep_on_based_part auxpass_based_is nsubjpass_based_corpus nn_corpus_WSJ det_corpus_The
P08-1098	J93-2004	o	In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- sentences are annotated with POS tags parse trees as well as discourse annotation from the Penn Discourse Treebank -LRB- Miltsakaki et al. 2008 -RRB- while verbs and verb arguments are annotated with Propbank rolesets -LRB- Palmer et al. 2005 -RRB-	amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et amod_rolesets_Propbank dep_annotated_Palmer prep_with_annotated_rolesets cop_annotated_are nsubj_annotated_arguments mark_annotated_while nn_arguments_verb nn_arguments_verbs conj_and_verbs_verb amod_Miltsakaki_2008 dep_Miltsakaki_al. nn_Miltsakaki_et dep_Treebank_Miltsakaki nn_Treebank_Discourse nn_Treebank_Penn det_Treebank_the prep_from_annotation_Treebank nn_annotation_discourse conj_and_parse_annotation dobj_parse_trees nn_tags_POS prep_with_annotated_tags cop_annotated_are nsubj_annotated_sentences nn_al._et advcl_Marcus_annotated appos_Marcus_annotation appos_Marcus_parse conj_Marcus_annotated amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the prep_of_sections_Treebank nn_sections_WSJ det_sections_the prep_for_efforts_sections nn_efforts_annotation nn_efforts_UPenn nn_efforts_language amod_efforts_general det_efforts_the dep_``_Marcus prep_in_``_efforts
P08-1109	J93-2004	o	5 Experiments For all experiments we trained and tested on the Penn treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_treebank_PTB nn_treebank_Penn det_treebank_the nsubj_tested_we dep_trained_Marcus prep_on_trained_treebank conj_and_trained_tested nsubj_trained_we det_experiments_all dep_Experiments_tested dep_Experiments_trained prep_for_Experiments_experiments num_Experiments_5
P08-1117	J93-2004	o	In -LRB- Bayraktar et al. 1998 -RRB- the WSJ PennTreebank corpus -LRB- Marcus et al. 1993 -RRB- is analyzed and a very detailed list of syntactic patterns that correspond to different roles of commas is created	auxpass_created_is nsubjpass_created_list prep_of_roles_commas amod_roles_different prep_to_correspond_roles nsubj_correspond_that rcmod_patterns_correspond amod_patterns_syntactic prep_of_list_patterns amod_list_detailed det_list_a advmod_detailed_very conj_and_analyzed_created auxpass_analyzed_is nsubjpass_analyzed_Marcus prep_in_analyzed_corpus nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_corpus_PennTreebank nn_corpus_WSJ det_corpus_the appos_corpus_Bayraktar nn_al._et amod_Bayraktar_1998 dep_Bayraktar_al. ccomp_``_created ccomp_``_analyzed
P08-1117	J93-2004	o	4 Corpus Annotation For our corpus we selected 1,000 sentences containing at least one comma from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- WSJ section 00 and manually annotated them with comma information3	nn_information3_comma prep_with_annotated_information3 dobj_annotated_them advmod_annotated_manually nsubj_annotated_Annotation num_section_00 nn_section_WSJ amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_comma_Treebank num_comma_one quantmod_one_at mwe_at_least dobj_containing_comma vmod_sentences_containing num_sentences_1,000 conj_and_selected_annotated dep_selected_section dep_selected_Marcus dobj_selected_sentences nsubj_selected_we nsubj_selected_Annotation poss_corpus_our prep_for_Annotation_corpus nn_Annotation_Corpus num_Annotation_4
P08-2026	J93-2004	o	One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand-labeled trees are available -LRB- e.g. Wall Street Journal -LRB- M. Marcus et al. 1993 -RRB- -RRB- and then self-training on a second type of data in order to adapt the parser to the second domain	amod_domain_second det_domain_the det_parser_the prep_to_adapt_domain dobj_adapt_parser aux_adapt_to dep_adapt_order mark_adapt_in prep_of_type_data amod_type_second det_type_a advcl_self-training_adapt prep_on_self-training_type advmod_self-training_then amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Marcus_M. dep_Journal_Marcus nn_Journal_Street nn_Journal_Wall pobj_e.g._Journal dep_available_e.g. cop_available_are nsubj_available_trees prep_for_available_which amod_trees_hand-labeled rcmod_data_available prep_of_type_data num_type_one det_parser_the conj_and_training_self-training prep_on_training_type dobj_training_parser advmod_training_initially nn_adaptation_parser dep_is_self-training dep_is_training prep_for_is_adaptation nsubj_is_use det_technique_this prep_for_use_technique amod_use_possible num_use_One ccomp_``_is
P09-1006	J93-2004	o	1 Introduction The last few decades have seen the emergence of multiple treebanks annotated with different grammar formalisms motivated by the diversity of languages and linguistic theories which is crucial to the success of statistical parsing -LRB- Abeille et al. 2000 Brants et al. 1999 Bohmova et al. 2003 Han et al. 2002 Kurohashi and Nagao 1998 Marcus et al. 1993 Moreno et al. 2003 Xue et al. 2005 -RRB-	num_Xue_2005 nn_Xue_al. nn_Xue_et num_Moreno_2003 nn_Moreno_al. nn_Moreno_et dep_Marcus_Xue conj_Marcus_Moreno num_Marcus_1993 nn_Marcus_al. nn_Marcus_et conj_and_Kurohashi_1998 conj_and_Kurohashi_Nagao num_Han_2002 nn_Han_al. nn_Han_et num_Bohmova_2003 nn_Bohmova_al. nn_Bohmova_et num_Brants_1999 nn_Brants_al. nn_Brants_et dep_Abeille_Marcus dep_Abeille_1998 dep_Abeille_Nagao dep_Abeille_Kurohashi dep_Abeille_Han dep_Abeille_Bohmova dep_Abeille_Brants amod_Abeille_2000 dep_Abeille_al. nn_Abeille_et amod_parsing_statistical prep_of_success_parsing det_success_the prep_to_crucial_success cop_crucial_is nsubj_crucial_which amod_theories_linguistic conj_and_languages_theories rcmod_diversity_crucial prep_of_diversity_theories prep_of_diversity_languages det_diversity_the agent_motivated_diversity nn_formalisms_grammar amod_formalisms_different prep_with_annotated_formalisms amod_treebanks_annotated amod_treebanks_multiple prep_of_emergence_treebanks det_emergence_the dobj_seen_emergence aux_seen_have nsubj_seen_decades amod_decades_few amod_decades_last det_decades_The dep_Introduction_Abeille vmod_Introduction_motivated rcmod_Introduction_seen num_Introduction_1 dep_``_Introduction
P09-1033	J93-2004	o	2.1 Data and Semantic Role Annotation Proposition Bank -LRB- Palmer et al. 2005 -RRB- adds Levins style predicate-argument annotation and indication of verbs alternations to the syntactic structures of the Penn Treebank -LRB- Marcus et al. 289 1993 -RRB-	number_1993_289 num_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_structures_Treebank amod_structures_syntactic det_structures_the nn_alternations_verbs prep_to_indication_structures prep_of_indication_alternations conj_and_annotation_indication nn_annotation_predicate-argument nn_annotation_style nn_annotation_Levins dep_adds_Marcus dobj_adds_indication dobj_adds_annotation nsubj_adds_Bank nsubj_adds_Data amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et nn_Bank_Proposition nn_Bank_Annotation nn_Bank_Role amod_Bank_Semantic dep_Data_Palmer conj_and_Data_Bank num_Data_2.1
P09-1043	J93-2004	o	the Wall Street Journal -LRB- WSJ -RRB- sections of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- as training set tests on BROWN Sections typically result in a 6-8 % drop in labeled attachment scores although the average sentence length is much shorter in BROWN than that in WSJ	prep_in_that_WSJ prep_than_shorter_that prep_in_shorter_BROWN amod_shorter_much cop_shorter_is nsubj_shorter_length mark_shorter_although nn_length_sentence amod_length_average det_length_the nn_scores_attachment amod_scores_labeled prep_in_drop_scores amod_drop_% det_drop_a number_%_6-8 advcl_result_shorter prep_in_result_drop advmod_result_typically nsubj_result_tests nsubj_result_sections nn_Sections_BROWN prep_on_tests_Sections nn_set_training amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_as_sections_set dep_sections_Marcus prep_of_sections_Treebank nn_sections_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the
P09-1055	J93-2004	o	The unlabeled data for English we use is the union of the Penn Treebank tagged WSJ data -LRB- Marcus et al. 1993 -RRB- and the BLLIP corpus .5 For the rest of the languages we use only the text of George Orwells novel 1984 which is provided in morphologically disambiguated form as part of MultextEast -LRB- but we dont use the annotations -RRB-	det_annotations_the dobj_use_annotations dep_dont_use nsubj_dont_we prep_of_part_MultextEast amod_form_disambiguated advmod_disambiguated_morphologically conj_but_provided_dont prep_as_provided_part prep_in_provided_form auxpass_provided_is nsubjpass_provided_which amod_1984_novel num_Orwells_1984 rcmod_George_dont rcmod_George_provided dep_George_Orwells prep_of_text_George det_text_the advmod_text_only dobj_use_text nsubj_use_we det_languages_the prep_of_rest_languages det_rest_the prep_for_.5_rest nn_.5_corpus nn_.5_BLLIP det_.5_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_data_WSJ dobj_tagged_data nn_Treebank_Penn det_Treebank_the rcmod_union_use conj_and_union_.5 dep_union_Marcus vmod_union_tagged prep_of_union_Treebank det_union_the cop_union_is nsubj_union_data nsubj_use_we rcmod_data_use prep_for_data_English amod_data_unlabeled det_data_The
P09-1056	J93-2004	o	3.2 Rare Word Accuracy For these experiments we use the Wall Street Journal portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_Journal nn_portion_Street nn_portion_Wall det_portion_the dep_use_Marcus dobj_use_portion nsubj_use_we nsubj_use_Accuracy det_experiments_these prep_for_Accuracy_experiments nn_Accuracy_Word amod_Accuracy_Rare num_Accuracy_3.2
P09-1059	J93-2004	o	Penn Treebank -LRB- Marcus et al. 1993 -RRB- the HPSG LinGo Redwoods Treebank -LRB- Oepen et al. 2002 -RRB- and a smaller dependency treebank -LRB- Buchholz and Marsi 2006 -RRB-	amod_Buchholz_2006 conj_and_Buchholz_Marsi dep_treebank_Marsi dep_treebank_Buchholz nn_treebank_dependency amod_treebank_smaller det_treebank_a amod_Oepen_2002 dep_Oepen_al. nn_Oepen_et dep_Treebank_Oepen nn_Treebank_Redwoods nn_Treebank_LinGo nn_Treebank_HPSG det_Treebank_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Treebank_treebank dep_Treebank_Treebank dep_Treebank_Marcus nn_Treebank_Penn dep_``_treebank dep_``_Treebank
P09-1059	J93-2004	o	1 Introduction Much of statistical NLP research relies on some sort of manually annotated corpora to train their models but these resources are extremely expensive to build especially at a large scale for example in treebanking -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et prep_in_example_treebanking amod_scale_large det_scale_a prep_for_build_example prep_at_build_scale advmod_build_especially aux_build_to dep_expensive_Marcus xcomp_expensive_build advmod_expensive_extremely cop_expensive_are nsubj_expensive_resources det_resources_these poss_models_their dobj_train_models aux_train_to amod_corpora_annotated advmod_annotated_manually prep_of_sort_corpora det_sort_some conj_but_relies_expensive xcomp_relies_train prep_on_relies_sort nsubj_relies_Introduction nn_research_NLP amod_research_statistical prep_of_Much_research amod_Introduction_Much num_Introduction_1
P09-1108	J93-2004	o	We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_Sections_Treebank num_Sections_2-21 amod_grammars_such prep_from_learn_Sections dobj_learn_grammars aux_learn_to nn_Parser4_Berkeley det_Parser4_the dep_used_Marcus vmod_used_learn dobj_used_Parser4 nsubj_used_We
P09-2010	J93-2004	o	3.1 Data The English data set consists of the Wall Street Journal sections 2-24 of the Penn treebank -LRB- Marcus et al. 1993 -RRB- converted to dependency format	nn_format_dependency prep_to_converted_format nsubj_converted_Data amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_treebank_Penn det_treebank_the prep_of_2-24_treebank dep_sections_2-24 nn_sections_Journal nn_sections_Street nn_sections_Wall det_sections_the prep_of_consists_sections nsubj_consists_set nn_set_data amod_set_English det_set_The appos_Data_Marcus rcmod_Data_consists num_Data_3.1
P09-4003	J93-2004	p	1 Introduction Research in language processing has benefited greatly from the collection of large annotated corpora such as Penn PropBank -LRB- Kingsbury and Palmer 2002 -RRB- and Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn dep_Kingsbury_2002 conj_and_Kingsbury_Palmer conj_and_PropBank_Treebank dep_PropBank_Palmer dep_PropBank_Kingsbury nn_PropBank_Penn prep_such_as_corpora_Treebank prep_such_as_corpora_PropBank amod_corpora_annotated amod_corpora_large prep_of_collection_corpora det_collection_the prep_from_benefited_collection advmod_benefited_greatly aux_benefited_has nsubj_benefited_Research nn_processing_language prep_in_Research_processing nn_Research_Introduction num_Research_1 ccomp_``_benefited
P94-1034	J93-2004	p	One major resource for corpus-based research is the treebanks available in many research organizations \ -LSB- Marcus et al. 1993 \ -RSB- which carry skeletal syntactic structures or ` brackets ' that have been manually verified	advmod_verified_manually auxpass_verified_been aux_verified_have nsubjpass_verified_that conj_or_structures_brackets amod_structures_syntactic amod_structures_skeletal dep_carry_verified dobj_carry_brackets dobj_carry_structures nsubj_carry_which num_\_1993 nn_\_al. rcmod_Marcus_carry dep_Marcus_\ nn_Marcus_et nn_organizations_research amod_organizations_many dep_available_\ prep_in_available_organizations dep_treebanks_Marcus amod_treebanks_available det_treebanks_the cop_treebanks_is nsubj_treebanks_resource amod_research_corpus-based prep_for_resource_research amod_resource_major num_resource_One
P94-1034	J93-2004	o	Several frameworks for finding translation equivalents or translation units in machine translation such as \ -LSB- Chang and Su 1993 Isabelle et al. 1993 \ -RSB- and other example-based MT approaches might be used to select the preferred mapping	amod_mapping_preferred det_mapping_the dobj_select_mapping aux_select_to xcomp_used_select auxpass_used_be aux_used_might nsubjpass_used_frameworks nn_approaches_MT amod_approaches_example-based amod_approaches_other num_\_1993 dep_\_al. nn_\_Isabelle nn_al._et num_Su_1993 conj_and_Chang_\ conj_and_Chang_Su conj_and_\_approaches dep_\_\ dep_\_Su dep_\_Chang prep_such_as_translation_approaches prep_such_as_translation_\ nn_translation_machine nn_units_translation conj_or_equivalents_units nn_equivalents_translation prep_in_finding_translation dobj_finding_units dobj_finding_equivalents prepc_for_frameworks_finding amod_frameworks_Several
P94-1044	J93-2004	o	In addition corpus-based stochastic modelling of lexical patterns -LRB- see Weischedel et al. 1993 -RRB- may provide information about word sense frequency of the kind advocated since -LRB- Ford et al. 1982 -RRB-	dep_1982_al. nn_al._et nn_al._Ford dep_since_1982 prep_advocated_since det_kind_the prep_of_frequency_kind nn_frequency_sense nn_frequency_word prep_about_information_frequency dep_provide_advocated dobj_provide_information aux_provide_may nsubj_provide_modelling prep_in_provide_addition num_Weischedel_1993 nn_Weischedel_al. nn_Weischedel_et dobj_see_Weischedel amod_patterns_lexical dep_modelling_see prep_of_modelling_patterns amod_modelling_stochastic amod_modelling_corpus-based
P96-1043	J93-2004	o	A similar approach was taken in -LRB- Weischedel et al. 1993 -RRB- where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS its capitalisation feature and its ending	poss_ending_its conj_and_feature_ending nn_feature_capitalisation poss_feature_its amod_POS_particular det_POS_a dep_be_ending dep_be_feature prep_of_be_POS prep_to_word_be amod_word_unknown det_word_an prep_for_probabilities_word det_probabilities_the dobj_given_probabilities xcomp_guessed_given auxpass_guessed_was nsubjpass_guessed_word advmod_guessed_where amod_word_unknown det_word_an rcmod_Weischedel_guessed dep_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et prep_in_taken_Weischedel auxpass_taken_was nsubjpass_taken_approach amod_approach_similar det_approach_A
P96-1043	J93-2004	o	These texts were not seen at the training phase which means that neither the 6Since Brill 's tagger was trained on the Penn tag-set -LRB- Marcus et al. 1993 -RRB- we provided an additional mapping	amod_mapping_additional det_mapping_an dobj_provided_mapping nsubj_provided_we amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_tag-set_Penn det_tag-set_the prep_on_trained_tag-set auxpass_trained_was nsubjpass_trained_tagger cc_trained_neither mark_trained_that poss_tagger_Brill nn_Brill_6Since det_Brill_the ccomp_means_trained nsubj_means_which rcmod_phase_means nn_phase_training det_phase_the dep_seen_provided dep_seen_Marcus prep_at_seen_phase neg_seen_not auxpass_seen_were nsubjpass_seen_texts det_texts_These
P97-1024	J93-2004	o	3.5 Adding Context to the Model Next we added of a stochastic POS tagger -LRB- Charniak et al. 1993 -RRB- to provide a model of context	prep_of_model_context det_model_a dobj_provide_model aux_provide_to amod_Charniak_1993 dep_Charniak_al. nn_Charniak_et dep_tagger_Charniak nn_tagger_POS amod_tagger_stochastic det_tagger_a xcomp_added_provide prep_of_added_tagger nsubj_added_we nsubj_added_3.5 det_Model_the dep_Adding_Next prep_to_Adding_Model dobj_Adding_Context vmod_3.5_Adding
P97-1024	J93-2004	o	3.3 Accuracy Results -LRB- Weischedel et al. 1993 -RRB- describe a model for unknown words that uses four features but treats the features ms independent	amod_ms_independent dep_features_ms dep_the_features dobj_treats_the nsubj_treats_Results num_features_four dobj_uses_features nsubj_uses_that rcmod_words_uses amod_words_unknown prep_for_model_words det_model_a conj_but_describe_treats dobj_describe_model nsubj_describe_Results amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et dep_Results_Weischedel nn_Results_Accuracy num_Results_3.3
P97-1062	J93-2004	o	with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_from_derived_Treebank vmod_sentences_derived nn_sentences_Journal nn_sentences_Street nn_sentences_Wall num_sentences_40,000 prep_for_sequences_sentences nn_sequences_action nn_sequences_parse pobj_with_sequences dep_``_with
P98-1034	J93-2004	o	The bracketed portions of Figure 1 for example show the base NPs in one sentence from the Penn Treebank Wall Street Journal -LRB- WSJ -RRB- corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_corpus_Marcus nn_corpus_WSJ dep_Journal_corpus nn_Journal_Street nn_Journal_Wall nn_Journal_Treebank nn_Journal_Penn det_Journal_the prep_from_sentence_Journal num_sentence_one nn_NPs_base det_NPs_the prep_in_show_sentence dobj_show_NPs prep_for_show_example nsubj_show_portions num_Figure_1 prep_of_portions_Figure amod_portions_bracketed det_portions_The
P98-1083	J93-2004	o	The main reason behind this lies in the difference between the two corpora used Penn Treebank -LRB- Marcus et al. 1993 -RRB- and EDR corpus -LRB- EDR 1995 -RRB-	amod_EDR_1995 dep_corpus_EDR nn_corpus_EDR amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Treebank_corpus dep_Treebank_Marcus nn_Treebank_Penn vmod_corpora_used num_corpora_two det_corpora_the prep_between_difference_corpora det_difference_the dep_lies_corpus dep_lies_Treebank prep_in_lies_difference nsubj_lies_reason prep_behind_reason_this amod_reason_main det_reason_The
P98-1083	J93-2004	p	Penn Treebank -LRB- Marcus et al. 1993 -RRB- was also used to induce part-of-speech -LRB- POS -RRB- taggers because the corpus contains very precise and detailed POS markers as well as bracket annotations	appos_bracket_annotations nn_markers_POS amod_markers_detailed amod_markers_precise conj_and_precise_detailed advmod_precise_very conj_and_contains_bracket dobj_contains_markers nsubj_contains_corpus mark_contains_because det_corpus_the nn_taggers_part-of-speech appos_part-of-speech_POS advcl_induce_bracket advcl_induce_contains dobj_induce_taggers aux_induce_to xcomp_used_induce advmod_used_also auxpass_used_was nsubjpass_used_Treebank amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn
P98-2140	J93-2004	o	The simplest period-space-capital_letter approach works well for simple texts but is rather unreliable for texts with many proper names and abbreviations at the end of sentence as for instance the Wall Street Journal -LRB- WSJ -RRB- corpus -LRB- -LRB- Marcus et al. 1993 -RRB- -RRB-	dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_corpus_Marcus nn_corpus_WSJ appos_Journal_corpus nn_Journal_Street nn_Journal_Wall det_Journal_the dep_,_Journal prep_for_,_instance prep_of_end_sentence det_end_the conj_and_names_abbreviations amod_names_proper amod_names_many prep_with_texts_abbreviations prep_with_texts_names dep_unreliable_as prep_at_unreliable_end prep_for_unreliable_texts advmod_unreliable_rather cop_unreliable_is nsubj_unreliable_The amod_texts_simple conj_but_works_unreliable prep_for_works_texts advmod_works_well nsubj_works_approach nsubj_works_period-space-capital_letter dep_works_simplest nsubj_works_The rcmod_``_unreliable rcmod_``_works
P98-2182	J93-2004	o	To identify conjunctions lists and appositives we first parsed the corpus using an efficient statistical parser -LRB- Charniak et al. 1998 -RRB- trMned on the Penn Wall Street Journal Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Journal nn_Treebank_Street nn_Treebank_Wall nn_Treebank_Penn det_Treebank_the prep_on_trMned_Treebank amod_Charniak_1998 dep_Charniak_al. nn_Charniak_et vmod_parser_trMned appos_parser_Charniak amod_parser_statistical amod_parser_efficient det_parser_an dep_using_Marcus dobj_using_parser det_corpus_the vmod_parsed_using dobj_parsed_corpus advmod_parsed_first nsubj_parsed_we advcl_parsed_identify conj_and_conjunctions_appositives conj_and_conjunctions_lists dobj_identify_appositives dobj_identify_lists dobj_identify_conjunctions aux_identify_To ccomp_``_parsed
P98-2184	J93-2004	o	1 Introduction The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers -LRB- Charniak 1995 Collins 1996/1997 Joshi and Srinivas 1994 Kim Srinivas and Trueswell 1997 Stolcke et al. 1997 -RRB- and in psychological theories of language processing -LRB- Clifton et al. 1984 Ferfeira & McClure 1997 Gamsey et al. 1997 Jurafsky 1996 MacDonald 1994 Mitchell & Holmes 1985 Tanenhaus et al. 1990 Trueswell et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Trueswell num_al._1990 nn_al._et nn_al._Tanenhaus nn_1985_Holmes nn_1985_Mitchell conj_and_Mitchell_Holmes num_MacDonald_1994 num_Jurafsky_1996 num_al._1997 nn_al._et nn_al._Gamsey num_Ferfeira_1997 conj_and_Ferfeira_McClure appos_al._al. dep_al._al. num_al._1985 appos_al._MacDonald appos_al._Jurafsky dep_al._al. dep_al._McClure dep_al._Ferfeira num_al._1984 nn_al._et nn_al._Clifton nn_processing_language prep_of_theories_processing amod_theories_psychological dep_in_al. pobj_in_theories num_al._1997 nn_al._et nn_al._Stolcke num_Trueswell_1997 num_Srinivas_1994 num_Collins_1996/1997 dep_1995_al. conj_and_1995_Trueswell conj_and_1995_Srinivas conj_and_1995_Kim conj_and_1995_Srinivas conj_and_1995_Joshi conj_and_1995_Collins amod_1995_Charniak appos_supertaggers_Trueswell appos_supertaggers_Srinivas appos_supertaggers_Kim appos_supertaggers_Srinivas appos_supertaggers_Joshi appos_supertaggers_Collins appos_supertaggers_1995 conj_and_parsers_supertaggers amod_parsers_statistical amod_parsers_modern amod_role_important det_role_an conj_and_plays_in prep_in_plays_supertaggers prep_in_plays_parsers dobj_plays_role nsubj_plays_relation dep_plays_Introduction poss_arguments_their conj_and_verbs_arguments prep_between_relation_arguments prep_between_relation_verbs amod_relation_probabilistic det_relation_The num_Introduction_1
P98-2184	J93-2004	o	This can be done automatically with unparsed corpora -LRB- Briscoe and Carroll 1997 Manning 1993 Ushioda et al. 1993 -RRB- from parsed corpora such as Marcus et al. 's -LRB- 1993 -RRB- Treebank -LRB- Merlo 1994 Framis 1994 -RRB- or manually as was done for COMLEX -LRB- Macleod and Grishman 1994 -RRB-	num_Grishman_1994 conj_and_Macleod_Grishman dep_COMLEX_Grishman dep_COMLEX_Macleod prep_for_done_COMLEX auxpass_done_was advmod_done_as advmod_done_manually num_Framis_1994 appos_Merlo_Framis num_Merlo_1994 conj_or_Treebank_done dep_Treebank_Merlo dep_al._done dep_al._Treebank dep_al._1993 possessive_al._'s nn_al._et nn_al._Marcus prep_such_as_corpora_al. amod_corpora_parsed dep_al._1993 nn_al._et nn_al._Ushioda dobj_Manning_1993 num_Carroll_1997 dep_Briscoe_al. vmod_Briscoe_Manning conj_and_Briscoe_Carroll appos_corpora_Carroll appos_corpora_Briscoe amod_corpora_unparsed prep_from_done_corpora prep_with_done_corpora advmod_done_automatically auxpass_done_be aux_done_can nsubjpass_done_This
P98-2184	J93-2004	o	1984 -RRB- written discourse -LRB- Brown and WSJ from Penn Treebank Marcus et al. 1993 -RRB- and conversational data -LRB- Switchboard Godfrey et al. 1992 -RRB-	advmod_1992_al. nn_al._et num_Godfrey_1992 nn_Godfrey_Switchboard appos_data_Godfrey amod_data_conversational dep_al._1993 nn_al._et nn_al._Marcus nn_al._Treebank nn_al._Penn pobj_from_al. dep_Brown_from conj_and_Brown_WSJ dep_discourse_WSJ dep_discourse_Brown amod_discourse_written conj_and_1984_data conj_and_1984_discourse
P98-2201	J93-2004	o	21418 examples of structures of the kind ` VB N1 PREP N2 ' were extracted from the Penn-TreeBank Wall Street Journal -LRB- Marcus et al. 1993 -RRB-	dep_1993_al. nn_al._et num_Marcus_1993 nn_Journal_Street nn_Journal_Wall nn_Journal_Penn-TreeBank det_Journal_the dep_extracted_Marcus prep_from_extracted_Journal auxpass_extracted_were nsubjpass_extracted_examples nn_N2_PREP nn_N2_N1 nn_N2_VB dep_kind_N2 det_kind_the prep_of_structures_kind prep_of_examples_structures num_examples_21418
P98-2234	J93-2004	o	6 Experiments 6.1 Data preparation Our experiments were conducted with data made available through the Penn Treebank annotation effort -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_effort_annotation nn_effort_Treebank nn_effort_Penn det_effort_the prep_through_available_effort acomp_made_available vmod_data_made prep_with_conducted_data auxpass_conducted_were nsubjpass_conducted_experiments poss_experiments_Our rcmod_preparation_conducted nn_preparation_Data num_preparation_6.1 dep_Experiments_Marcus dep_Experiments_preparation dep_6_Experiments ccomp_``_6
P98-2234	J93-2004	o	By core phrases we mean the kind of nonrecursive simplifications of the NP and VP that in the literature go by names such as noun/verb groups -LRB- Appelt et al. 1993 -RRB- or chunks and base NPs -LRB- Ramshaw and Marcus 1995 -RRB-	amod_Ramshaw_1995 conj_and_Ramshaw_Marcus dep_NPs_Marcus dep_NPs_Ramshaw nn_NPs_base dep_Appelt_1993 dep_Appelt_al. nn_Appelt_et conj_or_groups_chunks dep_groups_Appelt amod_groups_noun/verb prep_such_as_names_chunks prep_such_as_names_groups conj_and_go_NPs prep_by_go_names prep_in_go_literature mark_go_that det_literature_the conj_and_NP_VP det_NP_the prep_of_simplifications_VP prep_of_simplifications_NP amod_simplifications_nonrecursive ccomp_kind_NPs ccomp_kind_go prep_of_kind_simplifications det_kind_the dobj_mean_kind nsubj_mean_we prep_by_mean_phrases nn_phrases_core
P98-2251	J93-2004	o	Charniak -LRB- Charniak et al. 1993 -RRB- gives a thorough explanation of the equations for an HMM model and Kupiec -LRB- Kupiec 1992 -RRB- describes an HMM tagging system in detail	amod_system_tagging nn_system_HMM det_system_an prep_in_describes_detail dobj_describes_system nsubj_describes_Kupiec dep_Kupiec_1992 appos_Kupiec_Kupiec nn_model_HMM det_model_an det_equations_the prep_for_explanation_model prep_of_explanation_equations amod_explanation_thorough det_explanation_a conj_and_gives_describes dobj_gives_explanation nsubj_gives_Charniak amod_Charniak_1993 dep_Charniak_al. nn_Charniak_et dep_Charniak_Charniak
P98-2251	J93-2004	o	Weischedel 's group -LRB- Weischedel et al. 1993 -RRB- examines unknown words in the context of part-of-speech tagging	amod_tagging_part-of-speech prep_of_context_tagging det_context_the prep_in_words_context amod_words_unknown dobj_examines_words nsubj_examines_group amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et dep_group_Weischedel poss_group_Weischedel
P98-2251	J93-2004	o	An example set of tags can be found in the Penn Treebank project -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_project_Treebank nn_project_Penn det_project_the dep_found_Marcus prep_in_found_project auxpass_found_be aux_found_can nsubjpass_found_set prep_of_set_tags nn_set_example det_set_An
P99-1009	J93-2004	o	1 To train their system R&M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal -LRB- Marcus et al. 1993 -RRB- tagged using a transformation-based tagger -LRB- Brill 1995 -RRB- and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics -LRB- like treating the possessive marker as the first word of a new base noun phrase -RRB- to flatten the recursive structure of the parse	det_parse_the prep_of_structure_parse amod_structure_recursive det_structure_the dobj_flatten_structure aux_flatten_to nn_phrase_noun nn_phrase_base amod_phrase_new det_phrase_a prep_of_word_phrase amod_word_first det_word_the amod_marker_possessive det_marker_the prep_as_treating_word dobj_treating_marker pcomp_like_treating det_heuristics_some det_data_the prep_with_processing_heuristics dobj_processing_data dep_further_like vmod_further_processing conj_and_phrases_further nn_phrases_noun amod_phrases_nested neg_phrases_no xcomp_contained_flatten dobj_contained_further dobj_contained_phrases nsubj_contained_that rcmod_phrases_contained nn_phrases_noun dobj_selecting_phrases poss_parses_its nn_phrases_noun nn_phrases_base amod_phrases_extracted amod_Brill_1995 conj_and_tagger_phrases dep_tagger_Brill amod_tagger_transformation-based det_tagger_a prepc_by_using_selecting prep_from_using_parses dobj_using_phrases dobj_using_tagger xcomp_tagged_using num_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Journal_Street nn_Journal_Wall nn_Journal_Parsed nn_Journal_Treebank nn_Journal_Penn det_Journal_the vmod_chunk_tagged appos_chunk_Marcus prep_of_chunk_Journal amod_chunk_200k-word det_chunk_a dobj_used_chunk nsubj_used_R&M dep_used_1 poss_system_their dobj_train_system aux_train_To vmod_1_train
P99-1010	J93-2004	o	The study is conducted on both a simple Air Travel Information System -LRB- ATIS -RRB- corpus -LRB- Hemphill et al. 1990 -RRB- and the more complex Wall Street Journal -LRB- WSJ -RRB- corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_corpus_Marcus nn_corpus_WSJ dep_Journal_corpus nn_Journal_Street nn_Journal_Wall amod_Journal_complex det_Journal_the advmod_complex_more amod_Hemphill_1990 dep_Hemphill_al. nn_Hemphill_et conj_and_corpus_Journal dep_corpus_Hemphill nn_corpus_ATIS dep_System_Journal dep_System_corpus nn_System_Information nn_System_Travel nn_System_Air amod_System_simple det_System_a preconj_System_both prep_on_conducted_System auxpass_conducted_is nsubjpass_conducted_study det_study_The
P99-1016	J93-2004	o	Some of the data comes from the parsed files 2-21 of the Wall Street Journal Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB- and additional parsed text was obtained by parsing the 1987 Wall Street Journal text using the parser described in Charniak et al.	nn_al._et nn_al._Charniak prep_in_described_al. vmod_parser_described det_parser_the dobj_using_parser nn_text_Journal nn_text_Street nn_text_Wall num_text_1987 det_text_the vmod_parsing_using dobj_parsing_text agent_obtained_parsing auxpass_obtained_was nsubjpass_obtained_text amod_text_parsed amod_text_additional amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_of_2-21_corpus amod_files_2-21 amod_files_parsed det_files_the conj_and_comes_obtained dep_comes_Marcus prep_from_comes_files nsubj_comes_Some det_data_the prep_of_Some_data
P99-1018	J93-2004	o	4 The Corpus We used two corpora for our analysis hospital discharge summaries from 1991 to 1997 from the Columbia-Presbyterian Medical Center and the January 1996 part of the Wall Street Journal corpus from the Penn TreeBank \ -LSB- Marcus et al. 1993 \ -RSB-	num_\_1993 nn_\_al. dep_Marcus_\ nn_Marcus_et nn_\_TreeBank nn_\_Penn det_\_the nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_from_part_\ prep_of_part_corpus num_part_1996 nn_part_January det_part_the nn_Center_Medical nn_Center_Columbia-Presbyterian det_Center_the dep_summaries_Marcus conj_and_summaries_part prep_from_summaries_Center prep_to_summaries_1997 prep_from_summaries_1991 nn_summaries_discharge nn_discharge_hospital poss_analysis_our num_corpora_two prep_for_used_analysis dobj_used_corpora nsubj_used_We dep_Corpus_part dep_Corpus_summaries rcmod_Corpus_used det_Corpus_The num_Corpus_4 dep_``_Corpus
P99-1018	J93-2004	o	In the future we will experiment with semantic -LRB- rather than positional -RRB- clustering of premoditiers using techniques such as those proposed in \ -LSB- Hatzivassiloglou and McKeown 1993 Pereira et al. 1993 \ -RSB-	num_\_1993 dep_al._\ nn_al._et nn_al._Pereira num_McKeown_1993 dep_Hatzivassiloglou_al. conj_and_Hatzivassiloglou_McKeown prep_in_proposed_\ dep_those_McKeown dep_those_Hatzivassiloglou vmod_those_proposed prep_such_as_techniques_those dobj_using_techniques prep_of_clustering_premoditiers pobj_than_positional advmod_than_rather dep_semantic_clustering dep_semantic_than vmod_experiment_using prep_with_experiment_semantic aux_experiment_will nsubj_experiment_we prep_in_experiment_future det_future_the ccomp_``_experiment
P99-1021	J93-2004	o	Both taggers used the Penn Treebank tagset and were trained on the Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_on_trained_corpus auxpass_trained_were nsubjpass_trained_taggers nn_tagset_Treebank nn_tagset_Penn det_tagset_the dep_used_Marcus conj_and_used_trained dobj_used_tagset nsubj_used_taggers det_taggers_Both
P99-1023	J93-2004	o	Much research has been done to improve tagging accuracy using several different models and methods including hidden Markov models -LRB- HMMs -RRB- -LRB- Kupiec 1992 -RRB- -LRB- Charniak et al. 1993 -RRB- rule-based systems -LRB- Brill 1994 -RRB- -LRB- Brill 1995 -RRB- memory-based systems -LRB- Daelemans et al. 1996 -RRB- maximum-entropy systems -LRB- Ratnaparkhi 1996 -RRB- path voting constraint systems -LRB- Tiir and Oflazer 1998 -RRB- linear separator systems -LRB- Roth and Zelenko 1998 -RRB- and majority voting systems -LRB- van Halteren et al. 1998 -RRB-	amod_Halteren_1998 dep_Halteren_al. nn_Halteren_et nn_Halteren_van dep_systems_Halteren nn_systems_voting nn_systems_majority amod_Roth_1998 conj_and_Roth_Zelenko dep_systems_Zelenko dep_systems_Roth nn_systems_separator amod_systems_linear dep_Tiir_1998 conj_and_Tiir_Oflazer dep_systems_Oflazer dep_systems_Tiir nn_systems_constraint nn_systems_voting nn_systems_path dep_Ratnaparkhi_1996 dep_systems_Ratnaparkhi amod_systems_maximum-entropy amod_Daelemans_1996 dep_Daelemans_al. nn_Daelemans_et dep_systems_Daelemans amod_systems_memory-based amod_Brill_1995 dep_Brill_1994 appos_systems_Brill appos_systems_Brill amod_systems_rule-based amod_Charniak_1993 dep_Charniak_al. nn_Charniak_et amod_Kupiec_1992 conj_and_models_systems conj_and_models_systems conj_and_models_systems conj_and_models_systems conj_and_models_systems conj_and_models_systems appos_models_Charniak dep_models_Kupiec appos_models_HMMs nn_models_Markov amod_models_hidden prep_including_models_systems prep_including_models_systems prep_including_models_systems prep_including_models_systems prep_including_models_systems prep_including_models_systems prep_including_models_models conj_and_models_methods amod_models_different amod_models_several dobj_using_methods dobj_using_models amod_accuracy_tagging xcomp_improve_using dobj_improve_accuracy aux_improve_to xcomp_done_improve auxpass_done_been aux_done_has nsubjpass_done_research amod_research_Much
P99-1023	J93-2004	o	The tagger was tested on two corpora-the Brown corpus -LRB- from the Treebank II CDROM -LRB- Marcus et al. 1993 -RRB- -RRB- and the Wall Street Journal corpus -LRB- from the same source -RRB-	amod_source_same det_source_the prep_from_corpus_source nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_CDROM_II nn_CDROM_Treebank det_CDROM_the dep_from_Marcus pobj_from_CDROM conj_and_corpus_corpus dep_corpus_from amod_corpus_Brown amod_corpus_corpora-the num_corpus_two prep_on_tested_corpus prep_on_tested_corpus auxpass_tested_was nsubjpass_tested_tagger det_tagger_The
P99-1023	J93-2004	o	The MBT -LRB- Daelemans et al. 1996 -RRB- 180 Tagger Type Standard Trigram -LRB- Weischedel et al. 1993 -RRB- MBT -LRB- Daelemans et al. 1996 -RRB- Rule-based -LRB- Brill 1994 -RRB- Maximum-Entropy -LRB- Ratnaparkhi 1996 -RRB- Full Second-Order HMM SNOW -LRB- Roth and Zelenko 1998 -RRB- Voting Constraints -LRB- Tiir and Oflazer 1998 -RRB- Full Second-Order HMM Known Unknown Overall Open/Closed Lexicon ?	amod_Lexicon_Open/Closed amod_Lexicon_Overall amod_Lexicon_Unknown amod_Lexicon_Known dep_HMM_Lexicon nn_HMM_Second-Order amod_HMM_Full dep_Tiir_1998 conj_and_Tiir_Oflazer dep_Constraints_HMM dep_Constraints_Oflazer dep_Constraints_Tiir nn_Constraints_Voting dep_Constraints_Zelenko dep_Constraints_Roth nn_Constraints_SNOW dep_Roth_1998 conj_and_Roth_Zelenko nn_SNOW_HMM nn_SNOW_Second-Order amod_SNOW_Full dep_SNOW_Ratnaparkhi nn_SNOW_Maximum-Entropy dep_Ratnaparkhi_1996 amod_Maximum-Entropy_Rule-based dep_Maximum-Entropy_Daelemans nn_Maximum-Entropy_MBT dep_Brill_1994 dep_Rule-based_Brill dep_Daelemans_1996 dep_Daelemans_al. nn_Daelemans_et appos_MBT_Weischedel nn_MBT_Trigram amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et nn_Trigram_Standard nn_Trigram_Type nn_Trigram_Tagger num_Trigram_180 amod_Daelemans_1996 dep_Daelemans_al. nn_Daelemans_et dep_MBT_Constraints dep_MBT_Daelemans det_MBT_The dep_``_MBT
P99-1023	J93-2004	o	Most work in the area of unknown words and tagging deals with predicting part-of-speech information based on word endings and affixation information as shown by work in -LRB- Mikheev 1996 -RRB- -LRB- Mikheev 1997 -RRB- -LRB- Weischedel et al. 1993 -RRB- and -LRB- Thede 1998 -RRB-	amod_Thede_1998 amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et dep_Mikheev_1997 conj_and_Mikheev_Thede appos_Mikheev_Weischedel appos_Mikheev_Mikheev amod_Mikheev_1996 prep_in_shown_Thede prep_in_shown_Mikheev prep_by_shown_work mark_shown_as advcl_,_shown nn_information_affixation conj_and_endings_information nn_endings_word prep_on_based_information prep_on_based_endings vmod_information_based amod_information_part-of-speech dobj_predicting_information prepc_with_deals_predicting amod_deals_tagging amod_words_unknown prep_of_area_words det_area_the conj_and_work_deals prep_in_work_area amod_work_Most dep_``_deals dep_``_work
P99-1023	J93-2004	o	The Penn Treebank documentation -LRB- Marcus et al. 1993 -RRB- defines a commonly used set of tags	prep_of_set_tags amod_set_used det_set_a advmod_used_commonly dobj_defines_set nsubj_defines_documentation amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_documentation_Marcus nn_documentation_Treebank nn_documentation_Penn det_documentation_The
P99-1032	J93-2004	o	Two disjoint corpora are used in steps 2 and 5 both consisting of complete articles taken from the Wall Street Journal Treebank Corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Corpus_Treebank nn_Corpus_Journal nn_Corpus_Street nn_Corpus_Wall det_Corpus_the prep_from_taken_Corpus vmod_articles_taken amod_articles_complete dep_consisting_Marcus prep_of_consisting_articles preconj_consisting_both conj_and_2_5 dep_steps_5 dep_steps_2 xcomp_used_consisting prep_in_used_steps auxpass_used_are nsubjpass_used_corpora nn_corpora_disjoint num_corpora_Two ccomp_``_used
P99-1051	J93-2004	o	For instance the to-PP frame is poorly ' represented in the syntactically annotated version of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_version_Treebank amod_version_annotated advmod_version_syntactically det_version_the dep_represented_Marcus prep_in_represented_version advmod_represented_poorly auxpass_represented_is nsubjpass_represented_frame prep_for_represented_instance nn_frame_to-PP det_frame_the
P99-1054	J93-2004	o	The grammars were induced from sections 2-21 of the Penn Wall St. Journal Treebank -LRB- Marcus et al. 1993 -RRB- and tested on section 23	num_section_23 prep_on_tested_section nsubjpass_tested_grammars amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Journal nn_Treebank_St. nn_Treebank_Wall nn_Treebank_Penn det_Treebank_the prep_of_sections_Treebank num_sections_2-21 conj_and_induced_tested dep_induced_Marcus prep_from_induced_sections auxpass_induced_were nsubjpass_induced_grammars det_grammars_The
P99-1079	J93-2004	o	3 Evaluation of Algorithms All four algorithms were run on a 3900 utterance subset of the Penn Treebank annotated corpus -LRB- Marcus et al. 1993 -RRB- provided by Charniak and Ge -LRB- 1998 -RRB-	appos_Ge_1998 conj_and_Charniak_Ge prep_by_provided_Ge prep_by_provided_Charniak amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_corpus_annotated nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_of_subset_corpus nn_subset_utterance num_subset_3900 det_subset_a dep_run_provided dep_run_Marcus prep_on_run_subset auxpass_run_were nsubjpass_run_Evaluation num_algorithms_four nn_algorithms_All nn_algorithms_Algorithms prep_of_Evaluation_algorithms num_Evaluation_3
W00-0709	J93-2004	o	4 Experiments The experiments described here were conducted using the Wall Street Journal Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the dobj_using_corpus xcomp_conducted_using auxpass_conducted_were nsubjpass_conducted_experiments advmod_described_here vmod_experiments_described det_experiments_The dep_Experiments_Marcus rcmod_Experiments_conducted num_Experiments_4
W00-0716	J93-2004	o	The syntactic and part-of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_project_Treebank nn_project_Penn det_project_the prep_in_processed_project vmod_corpus_processed det_corpus_the prep_of_part_corpus det_part_the dep_obtained_Marcus prep_from_obtained_part auxpass_obtained_were nsubjpass_obtained_informations amod_informations_part-of-speech amod_informations_syntactic det_informations_The conj_and_syntactic_part-of-speech
W00-0721	J93-2004	o	The data sets used are the standard data sets for this problem -LRB- Ramshaw and Maxcus 1995 Argamon et al. 1999 Mufioz et al. 1999 Tjong Kim Sang and Veenstra 1999 -RRB- taken from the Wall Street Journal corpus in the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_in_taken_Treebank prep_from_taken_corpus dep_Veenstra_1999 vmod_Sang_taken conj_and_Sang_Veenstra nn_Sang_Kim nn_Sang_Tjong num_Mufioz_1999 nn_Mufioz_al. nn_Mufioz_et num_Argamon_1999 nn_Argamon_al. nn_Argamon_et dep_Ramshaw_Marcus conj_and_Ramshaw_Veenstra conj_and_Ramshaw_Sang conj_and_Ramshaw_Mufioz conj_and_Ramshaw_Argamon conj_and_Ramshaw_1995 conj_and_Ramshaw_Maxcus det_problem_this dep_sets_Sang dep_sets_Mufioz dep_sets_Argamon dep_sets_1995 dep_sets_Maxcus dep_sets_Ramshaw prep_for_sets_problem nn_sets_data amod_sets_standard det_sets_the cop_sets_are nsubj_sets_sets vmod_sets_used nn_sets_data det_sets_The
W00-0725	J93-2004	o	The experiments were performed using the Wall Street Journal -LRB- WSJ -RRB- corpus of the University of Pennsylvania -LRB- Marcus et al. 1993 -RRB- modified as described in -LRB- Charniak 1996 -RRB- and -LRB- Johnson 1998 -RRB-	amod_Johnson_1998 conj_and_Charniak_Johnson amod_Charniak_1996 prep_in_described_Johnson prep_in_described_Charniak mark_described_as advcl_modified_described vmod_Marcus_modified amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et prep_of_University_Pennsylvania det_University_the prep_of_corpus_University nn_corpus_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the dobj_using_corpus dep_performed_Marcus xcomp_performed_using auxpass_performed_were nsubjpass_performed_experiments det_experiments_The
W00-0726	J93-2004	o	We have chosen to work with a corpus with parse information the Wall Street Journal WSJ part of the Penn Treebank II corpus -LRB- Marcus et al. 1993 -RRB- and to extract chunk information from the parse trees in this corpus	det_corpus_this nn_trees_parse det_trees_the nn_information_chunk prep_from_extract_trees dobj_extract_information aux_extract_to amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_II nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_in_part_corpus conj_and_part_extract appos_part_Marcus prep_of_part_corpus nn_part_WSJ dep_part_Journal ccomp_part_chosen nn_Journal_Street nn_Journal_Wall det_Journal_the nn_information_parse prep_with_corpus_information det_corpus_a prep_with_work_corpus aux_work_to xcomp_chosen_work aux_chosen_have nsubj_chosen_We
W00-0735	J93-2004	o	While the tag features containing WSJ paxt-ofspeech tags -LRB- Marcus et al. 1993 -RRB- have about 45 values the word features have more than 10,000 values	num_values_10,000 quantmod_10,000_than mwe_than_more dobj_have_values nsubj_have_features advcl_have_have nn_features_word det_features_the num_values_45 quantmod_45_about dobj_have_values nsubj_have_features mark_have_While amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_tags_paxt-ofspeech nn_tags_WSJ dobj_containing_tags dep_features_Marcus vmod_features_containing nn_features_tag det_features_the
W00-0905	J93-2004	o	1 Data Data for 64 verbs -LRB- shown in Table 1 -RRB- was collected from three corpora The British National Corpus -LRB- BNC -RRB- -LRB- http ` J/info ox.ac.uk / bnc/index html -RRB- the Penn Treehank parsed version of the Brown Corpus -LRB- Brown -RRB- and the Penn Treebank Wall Street Journal corpas -LRB- WSJ -RRB- -LRB- Marcus et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_Marcus_1993 dep_corpas_Marcus appos_corpas_WSJ nn_corpas_Journal nn_corpas_Street nn_corpas_Wall nn_corpas_Treebank nn_corpas_Penn det_corpas_the appos_Corpus_Brown amod_Corpus_Brown det_Corpus_the prep_of_version_Corpus dobj_parsed_version nsubj_parsed_Treehank nn_Treehank_Penn det_Treehank_the dep_bnc/index_html nn_bnc/index_ox.ac.uk dep_bnc/index_http nn_bnc/index_Corpus dep_http_J/info appos_Corpus_BNC nn_Corpus_National amod_Corpus_British det_Corpus_The num_corpora_three conj_and_collected_corpas conj_and_collected_parsed dep_collected_bnc/index prep_from_collected_corpora auxpass_collected_was parataxis_collected_shown nsubjpass_collected_Data dep_Table_1 prep_in_shown_Table num_verbs_64 prep_for_Data_verbs nn_Data_Data num_Data_1
W00-0905	J93-2004	o	Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications -LRB- e.g. Carroll Minnen and Briscoe 1998 Charniak 1997 Collins 1996/1997 Joshi and Srinivas 1994 Kim Srinivas and Tmeswell 1997 Stolcke et al. 1997 -RRB- and psycholinguisfic models of language processing -LRB- e.g. Boland 1997 Clifton et al. 1984 Ferreira & McClure 1997 Fodor 1978 Garnsey et al. 1997 Jurafsky 1996 MacDonald 1994 Mitchell & Holmes 1985 Tanenhaus et al. 1990 Trueswell et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Trueswell num_al._1990 nn_al._et nn_al._Tanenhaus num_MacDonald_1994 num_Jurafsky_1996 num_al._1997 nn_al._et nn_al._Garnsey num_Fodor_1978 num_McClure_1997 nn_1984_al. num_Clifton_1984 nn_Clifton_et dep_1997_al. dep_1997_al. dep_1997_1985 conj_and_1997_Holmes conj_and_1997_Mitchell conj_and_1997_MacDonald conj_and_1997_Jurafsky conj_and_1997_al. conj_and_1997_Fodor conj_and_1997_McClure conj_and_1997_Ferreira conj_and_1997_Clifton dep_1997_Boland dep_1997_e.g. nn_processing_language prep_of_models_processing amod_models_psycholinguisfic dep_al._1997 nn_al._et nn_al._Stolcke num_Tmeswell_1997 num_Srinivas_1994 num_Collins_1996/1997 num_Charniak_1997 num_Briscoe_1998 dep_Carroll_Holmes dep_Carroll_Mitchell dep_Carroll_MacDonald dep_Carroll_Jurafsky dep_Carroll_al. dep_Carroll_Fodor dep_Carroll_McClure dep_Carroll_Ferreira dep_Carroll_Clifton dep_Carroll_1997 conj_and_Carroll_models conj_and_Carroll_al. conj_and_Carroll_Tmeswell conj_and_Carroll_Srinivas conj_and_Carroll_Kim conj_and_Carroll_Srinivas conj_and_Carroll_Joshi conj_and_Carroll_Collins conj_and_Carroll_Charniak conj_and_Carroll_Briscoe conj_and_Carroll_Minnen pobj_e.g._models pobj_e.g._al. pobj_e.g._Tmeswell pobj_e.g._Srinivas pobj_e.g._Kim pobj_e.g._Srinivas pobj_e.g._Joshi pobj_e.g._Collins pobj_e.g._Charniak pobj_e.g._Briscoe pobj_e.g._Minnen pobj_e.g._Carroll dep_applications_e.g. amod_applications_linguistic amod_applications_computational preconj_applications_both amod_role_important det_role_an prep_in_play_applications dobj_play_role nsubj_play_probabilities nn_probabilities_subcategorizafion nn_probabilities_Verb nn_probabilities_Introduction
W00-1201	J93-2004	o	The success of statistical methods in particular has been quite evident in the area of syntactic parsing most recently with the outstanding results of -LRB- Charniak 2000 -RRB- and -LRB- Colhns 2000 -RRB- on the now-standard English test set of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_set_Treebank nn_set_test amod_set_English amod_set_now-standard det_set_the dep_Colhns_2000 dep_Charniak_Marcus prep_on_Charniak_set conj_and_Charniak_Colhns amod_Charniak_2000 prep_of_results_Colhns prep_of_results_Charniak amod_results_outstanding det_results_the advmod_recently_most amod_parsing_syntactic prep_of_area_parsing det_area_the prep_with_evident_results advmod_evident_recently prep_in_evident_area advmod_evident_quite cop_evident_been aux_evident_has nsubj_evident_success amod_methods_statistical prep_in_success_particular prep_of_success_methods det_success_The
W00-1205	J93-2004	p	Introduction The Penn Treebank -LRB- Marcus et al. 1993 -RRB- initiated a new paradigm in corpus-based research	amod_research_corpus-based prep_in_paradigm_research amod_paradigm_new det_paradigm_a dobj_initiated_paradigm nsubj_initiated_Treebank advmod_1993_al. nn_al._et num_Marcus_1993 appos_Treebank_Marcus nn_Treebank_Penn det_Treebank_The rcmod_Introduction_initiated
W00-1208	J93-2004	o	By comparing derivation trees for parallel sentences in two languages instances of structural divergences -LRB- Dorr 1993 Dorr 1994 Palmer et al. 1998 -RRB- can be automatically detected	advmod_detected_automatically auxpass_detected_be aux_detected_can agent_detected_comparing num_Palmer_1998 nn_Palmer_al. nn_Palmer_et num_Dorr_1994 dep_Dorr_Palmer conj_Dorr_Dorr conj_Dorr_1993 appos_divergences_Dorr amod_divergences_structural prep_of_instances_divergences appos_languages_instances num_languages_two prep_in_sentences_languages amod_sentences_parallel nn_trees_derivation prep_for_comparing_sentences dobj_comparing_trees
W00-1208	J93-2004	o	2.2 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II -LRB- Marcus et al. 1993 -RRB- the Chinese Penn Treebank -LRB- Xia et al. 2000b -RRB- and the Korean Penn Treebank -LRB- Chung-hye Han 2000 -RRB-	dep_Han_2000 amod_Han_Chung-hye dep_Treebank_Han nn_Treebank_Penn amod_Treebank_Korean det_Treebank_the appos_Xia_2000b dep_Xia_al. nn_Xia_et dep_Treebank_Xia nn_Treebank_Penn amod_Treebank_Chinese det_Treebank_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_II_Treebank conj_and_II_Treebank dep_II_Marcus nn_II_Treebank nn_II_Penn nn_II_English det_II_the cop_II_are nsubj_II_Treebanks det_paper_this prep_in_used_paper nsubj_used_we mark_used_that ccomp_Treebanks_used det_Treebanks_The dep_Treebanks_Treebanks dep_Treebanks_2.2 num_Treebanks_Three
W00-1301	J93-2004	o	The training and test set were derived by finding all instances of the confusable words in the Brown Corpus using the Penn Treebank parts of speech and tokenization -LRB- Marcus Santorini et al. 1993 -RRB- and then dividing this set into 80 % for training and 20 % for testing	prep_for_%_testing num_%_20 conj_and_%_% prep_for_%_training num_%_80 prep_into_set_% prep_into_set_% nsubj_set_this ccomp_dividing_set advmod_dividing_then dep_al._1993 nn_al._et nn_al._Santorini dep_Marcus_al. conj_and_speech_tokenization prep_of_parts_tokenization prep_of_parts_speech nn_parts_Treebank nn_parts_Penn det_parts_the conj_and_using_dividing appos_using_Marcus dobj_using_parts amod_Corpus_Brown det_Corpus_the prep_in_words_Corpus amod_words_confusable det_words_the prep_of_instances_words det_instances_all dobj_finding_instances dep_derived_dividing dep_derived_using agent_derived_finding auxpass_derived_were nsubjpass_derived_set nsubjpass_derived_training nn_set_test conj_and_training_set det_training_The
W00-1304	J93-2004	o	The corpus consists of sections 15-18 and section 20 of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and is pre-divided into a 8936-sentence -LRB- 211727 tokens -RRB- training set and a 2012-sentence -LRB- 47377 tokens -RRB- test set	nn_set_test amod_set_2012-sentence det_set_a num_tokens_47377 dep_2012-sentence_tokens conj_and_set_set nn_set_training nn_set_8936-sentence num_tokens_211727 appos_8936-sentence_tokens det_8936-sentence_a prep_into_pre-divided_set prep_into_pre-divided_set cop_pre-divided_is nsubj_pre-divided_corpus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the num_section_20 prep_of_sections_Treebank conj_and_sections_section num_sections_15-18 conj_and_consists_pre-divided dep_consists_Marcus prep_of_consists_section prep_of_consists_sections nsubj_consists_corpus det_corpus_The
W00-1306	J93-2004	o	This paper presents an empirical study measuring the effectiveness of our evaluation functions at selecting training sentences from the Wall Street Journal -LRB- WSJ -RRB- corpus -LRB- Marcuset al. 1993 -RRB- for inducing grammars	dobj_inducing_grammars dep_al._1993 nn_al._Marcuset prepc_for_corpus_inducing dep_corpus_al. nn_corpus_WSJ dep_Journal_corpus nn_Journal_Street nn_Journal_Wall det_Journal_the nn_sentences_training prep_from_selecting_Journal dobj_selecting_sentences prepc_at_functions_selecting nsubj_functions_study poss_evaluation_our prep_of_effectiveness_evaluation det_effectiveness_the dobj_measuring_effectiveness vmod_study_measuring amod_study_empirical det_study_an ccomp_presents_functions nsubj_presents_paper det_paper_This
W00-1307	J93-2004	o	3.5 The Experiments We have ran LexTract on the one-millionword English Penn Treebank -LRB- Marcus et al. 1993 -RRB- and got two Treebank grammars	amod_grammars_Treebank num_grammars_two dobj_got_grammars amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn nn_Treebank_English amod_Treebank_one-millionword det_Treebank_the prep_on_ran_Treebank dobj_ran_LexTract aux_ran_have nsubj_ran_We conj_and_Experiments_got dep_Experiments_Marcus rcmod_Experiments_ran det_Experiments_The num_Experiments_3.5 dep_``_got dep_``_Experiments
W00-1309	J93-2004	o	The data used for all our experiments is extracted from the PENN WSJ Treebank -LRB- Marcus et al. 1993 -RRB- by the program provided by Sabine Buchholz from Tilbug University	nn_University_Tilbug prep_from_Buchholz_University nn_Buchholz_Sabine agent_provided_Buchholz vmod_program_provided det_program_the advmod_1993_al. nn_al._et num_Marcus_1993 appos_Treebank_Marcus nn_Treebank_WSJ dep_PENN_Treebank det_PENN_the agent_extracted_program prep_from_extracted_PENN auxpass_extracted_is nsubjpass_extracted_data poss_experiments_our predet_experiments_all prep_for_used_experiments vmod_data_used det_data_The ccomp_``_extracted
W00-1320	J93-2004	o	2.2 Motivation from previous work 2.2.1 Parsing In recent years the success of statistical parsing techniques can be attributed to several factors such as the increasing size of computing machinery to accommodate larger models the availability of resources such as the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and the success of machine learning techniques for lowerlevel NLP problems such as part-of-speech tagging -LRB- Church 1988 Brill 1995 -RRB- and PPattachment -LRB- Brill and Resnik 1994 Collins and Brooks 1995 -RRB-	appos_Collins_1995 conj_and_Collins_Brooks conj_and_Brill_Brooks conj_and_Brill_Collins conj_and_Brill_1994 conj_and_Brill_Resnik dep_PPattachment_Collins dep_PPattachment_1994 dep_PPattachment_Resnik dep_PPattachment_Brill dep_Brill_1995 dep_Church_Brill appos_Church_1988 conj_and_tagging_PPattachment appos_tagging_Church amod_tagging_part-of-speech prep_such_as_problems_PPattachment prep_such_as_problems_tagging nn_problems_NLP amod_problems_lowerlevel prep_for_techniques_problems nn_techniques_learning nn_techniques_machine prep_of_success_techniques det_success_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Treebank_success dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_such_as_resources_success prep_such_as_resources_Treebank prep_of_availability_resources det_availability_the amod_models_larger dobj_accommodate_models aux_accommodate_to amod_machinery_computing prep_of_size_machinery amod_size_increasing det_size_the prep_as_factors_size mwe_factors_such amod_factors_several dobj_attributed_availability xcomp_attributed_accommodate prep_to_attributed_factors auxpass_attributed_be aux_attributed_can nsubjpass_attributed_success dep_attributed_Motivation nn_techniques_parsing amod_techniques_statistical prep_of_success_techniques det_success_the amod_years_recent num_Parsing_2.2.1 nn_Parsing_work amod_work_previous prep_in_Motivation_years prep_from_Motivation_Parsing num_Motivation_2.2
W00-1320	J93-2004	o	3.2 Probability structure of the original model We use p to denote the unlexicalized nonterminal corresponding to P and similarly for li ri and h We now present the top-level generation probabilities along with examples from 4The inclusion of the word feature in the BBN model was due to the work described in -LRB- Weischedel et al. 1993 -RRB- where word features helped reduce part of speech ambiguity for unknown words	amod_words_unknown prep_for_ambiguity_words nn_ambiguity_speech prep_of_part_ambiguity dobj_reduce_part ccomp_helped_reduce dep_features_helped vmod_word_features dep_where_word prep_Weischedel_where amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et prep_in_described_Weischedel vmod_work_described det_work_the nn_model_BBN det_model_the prep_in_feature_model nn_feature_word det_feature_the prep_of_inclusion_feature nn_inclusion_4The prep_from_examples_inclusion nn_probabilities_generation amod_probabilities_top-level det_probabilities_the prep_due_to_present_work cop_present_was pobj_present_examples prepc_along_with_present_with dobj_present_probabilities advmod_present_now nsubj_present_We prep_present_for prep_present_structure conj_and_li_h conj_and_li_ri pobj_for_h pobj_for_ri pobj_for_li advmod_for_similarly prep_to_corresponding_P vmod_nonterminal_corresponding amod_nonterminal_unlexicalized det_nonterminal_the dobj_denote_nonterminal aux_denote_to xcomp_use_denote dobj_use_p nsubj_use_We rcmod_model_use amod_model_original det_model_the conj_and_structure_for prep_of_structure_model nn_structure_Probability num_structure_3.2
W00-1427	J93-2004	o	The analyser -- and therefore the generator-includes exception lists derived from WordNet -LRB- version 1.5 Miller et al. 1993 -RRB-	num_Miller_1993 nn_Miller_al. nn_Miller_et dep_version_Miller num_version_1.5 dep_WordNet_version prep_from_derived_WordNet vmod_lists_derived nn_lists_exception nn_lists_generator-includes det_lists_the dep_therefore_lists cc_--_and det_analyser_The dep_``_analyser
W00-1427	J93-2004	o	corpus -LRB- Garside et al. 1987 -RRB- the Penn Treebank -LRB- Marcus et al. 1993 -RRB- the SUSANNE corpus -LRB- Sampson 1995 -RRB- the Spoken English Corpus -LRB- Taylor and Knowles 1988 -RRB- the Oxford Psycholinguistic Database -LRB- Quinlan 1992 -RRB- and the Computer-Usable version of the Oxford Advanced Learner 's Dictionary of Current English -LRB- OALDCE Mitton 1.9.92 -RRB-	amod_Mitton_1.9.92 dep_OALDCE_Mitton dep_English_OALDCE dep_Current_English prep_of_Dictionary_Current poss_Dictionary_Learner nn_Learner_Advanced nn_Learner_Oxford det_Learner_the prep_of_version_Dictionary amod_version_Computer-Usable det_version_the amod_Quinlan_1992 appos_Database_Quinlan nn_Database_Psycholinguistic nn_Database_Oxford det_Database_the dep_Taylor_1988 conj_and_Taylor_Knowles appos_Corpus_Knowles appos_Corpus_Taylor nn_Corpus_English nn_Corpus_Spoken det_Corpus_the appos_Sampson_1995 dep_corpus_Sampson nn_corpus_SUSANNE det_corpus_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the amod_Garside_1987 dep_Garside_al. nn_Garside_et conj_and_corpus_version appos_corpus_Database appos_corpus_Corpus appos_corpus_corpus appos_corpus_Treebank appos_corpus_Garside ccomp_``_version ccomp_``_corpus
W00-1427	J93-2004	o	2.5 Evaluation Minnen and Carroll -LRB- Under review -RRB- report an evaluation of the accuracy of the morphological generator with respect to the CELEX lexical database -LRB- version 2.5 Baayen et al. 1993 -RRB-	num_Baayen_1993 nn_Baayen_al. nn_Baayen_et dep_version_Baayen num_version_2.5 appos_database_version nn_database_lexical nn_database_CELEX det_database_the amod_generator_morphological det_generator_the prep_of_accuracy_generator det_accuracy_the prep_with_respect_to_evaluation_database prep_of_evaluation_accuracy det_evaluation_an dobj_report_evaluation prep_under_report_review nsubj_report_Carroll nsubj_report_Minnen conj_and_Minnen_Carroll nn_Minnen_Evaluation num_Minnen_2.5
W01-0702	J93-2004	o	The system is tested on base noun-phrase -LRB- NP -RRB- chunking using the Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the dobj_using_corpus xcomp_chunking_using vmod_noun-phrase_chunking appos_noun-phrase_NP nn_noun-phrase_base dep_tested_Marcus prep_on_tested_noun-phrase auxpass_tested_is nsubjpass_tested_system det_system_The
W01-0706	J93-2004	o	First it has been noted that in many natural language applications it is sufficient to use shallow parsing information information such as noun phrases -LRB- NPs -RRB- and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization -LRB- Grishman 1995 Appelt et al. 1993 -RRB-	num_Appelt_1993 nn_Appelt_al. nn_Appelt_et dep_Grishman_Appelt appos_Grishman_1995 nn_summarization_text dep_extraction_Grishman conj_and_extraction_summarization nn_extraction_information prep_including_applications_summarization prep_including_applications_extraction nn_applications_processing nn_applications_language amod_applications_large-scale amod_applications_many prep_in_useful_applications acomp_found_useful auxpass_found_been aux_found_have nsubjpass_found_information amod_sequences_syntactic amod_sequences_other conj_and_phrases_sequences appos_phrases_NPs nn_phrases_noun prep_such_as_information_sequences prep_such_as_information_phrases amod_information_parsing amod_information_shallow dobj_use_information aux_use_to xcomp_sufficient_use cop_sufficient_is nsubj_sufficient_it prep_in_sufficient_applications mark_sufficient_that nn_applications_language amod_applications_natural amod_applications_many parataxis_noted_found ccomp_noted_sufficient auxpass_noted_been aux_noted_has nsubjpass_noted_it advmod_noted_First
W01-0706	J93-2004	o	Parsers Precision -LRB- a4 -RRB- Recall -LRB- a4 -RRB- a5a7a6 -LRB- a4 -RRB- a8KM00 a9 93.45 93.51 93.48 a8Hal00 a9 93.13 93.51 93.32 a8CSCL a9 * 93.41 92.64 93.02 a8TKS00 a9 94.04 91.00 92.50 a8ZST00 a9 91.99 92.25 92.12 a8Dej00 a9 91.87 91.31 92.09 a8Koe00 a9 92.08 91.86 91.97 a8Osb00 a9 91.65 92.23 91.94 a8VB00 a9 91.05 92.03 91.54 a8PMP00 a9 90.63 89.65 90.14 a8Joh00 a9 86.24 88.25 87.23 a8VD00 a9 88.82 82.91 85.76 Baseline 72.58 82.14 77.07 2.2 Data Training was done on the Penn Treebank -LRB- Marcus et al. 1993 -RRB- Wall Street Journal data sections 02-21	num_sections_02-21 appos_data_sections nn_data_Journal nn_data_Street nn_data_Wall appos_data_Marcus dep_data_a9 nn_data_a9 nn_data_a8Hal00 num_data_93.48 num_data_93.51 amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_on_done_Treebank auxpass_done_was nsubjpass_done_93.02 dep_done_92.64 dep_done_* vmod_Data_Training num_Data_2.2 dep_77.07_Data number_77.07_82.14 number_77.07_72.58 dep_77.07_Baseline number_77.07_85.76 dep_77.07_82.91 number_82.91_88.82 dep_a9_77.07 dep_a8VD00_a9 appos_87.23_a8VD00 dep_88.25_87.23 number_88.25_86.24 num_a9_88.25 dep_a8Joh00_a9 dep_90.14_a8Joh00 dep_89.65_90.14 number_89.65_90.63 amod_a9_89.65 dep_a8PMP00_a9 appos_91.54_a8PMP00 dep_92.03_91.54 number_92.03_91.05 num_a9_92.03 dep_a8VB00_a9 dep_91.94_a8VB00 dep_92.23_91.94 number_92.23_91.65 amod_a9_92.23 dep_a8Osb00_a9 appos_91.97_a8Osb00 dep_91.86_91.97 number_91.86_92.08 num_a9_91.86 dep_a8Koe00_a9 dep_92.09_a8Koe00 dep_91.31_92.09 number_91.31_91.87 amod_a9_91.31 dep_a8Dej00_a9 appos_92.12_a8Dej00 dep_92.25_92.12 number_92.25_91.99 num_a9_92.25 dep_a8ZST00_a9 dep_92.50_a8ZST00 dep_91.00_92.50 number_91.00_94.04 amod_a9_91.00 dep_a8TKS00_a9 appos_93.02_a8TKS00 number_92.64_93.41 vmod_a9_done nn_a9_a8CSCL num_a9_93.32 num_a9_93.51 number_93.51_93.13 number_93.51_93.45 dep_a9_data nn_a9_a8KM00 dep_a5a7a6_a9 appos_a5a7a6_a4 nn_a5a7a6_a4 dobj_Recall_a5a7a6 nsubj_Recall_Precision appos_Precision_a4 nn_Precision_Parsers
W01-0712	J93-2004	o	We have used three different algorithms the nearest neighbour algorithm IB1IG which is part of the Timbl software package -LRB- Daelemans et al. 1999 -RRB- the decision tree learner IGTREE also from Timbl and C5 .0 a commercial version of the decision tree learner C4 .5 -LRB- Quinlan 1993 -RRB-	amod_Quinlan_1993 appos_C4_Quinlan num_C4_.5 nn_C4_learner nn_C4_tree nn_C4_decision det_C4_the prep_of_version_C4 amod_version_commercial det_version_a appos_C5_version num_C5_.0 nn_IGTREE_learner nn_IGTREE_tree nn_IGTREE_decision det_IGTREE_the amod_Daelemans_1999 dep_Daelemans_al. nn_Daelemans_et nn_package_software nn_package_Timbl det_package_the prep_of_part_package cop_part_is nsubj_part_which dep_IB1IG_Daelemans rcmod_IB1IG_part conj_and_algorithm_C5 prep_from_algorithm_Timbl advmod_algorithm_also appos_algorithm_IGTREE dep_algorithm_IB1IG nn_algorithm_neighbour amod_algorithm_nearest det_algorithm_the dep_algorithms_C5 dep_algorithms_algorithm amod_algorithms_different num_algorithms_three dobj_used_algorithms aux_used_have nsubj_used_We ccomp_``_used
W01-0712	J93-2004	o	It consists of sections 15-18 of the Wall Street Journal part of the Penn Treebank II -LRB- Marcus et al. 1993 -RRB- as training data -LRB- 211727 tokens -RRB- and section 20 as test data -LRB- 47377 tokens -RRB-	num_tokens_47377 appos_data_tokens nn_data_test prep_as_section_data num_section_20 num_tokens_211727 conj_and_data_section appos_data_tokens nn_data_training amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_II_Treebank nn_II_Penn det_II_the prep_of_part_II nn_part_Journal nn_part_Street nn_part_Wall det_part_the prep_of_sections_part num_sections_15-18 prep_as_consists_section prep_as_consists_data dep_consists_Marcus prep_of_consists_sections nsubj_consists_It
W01-0720	J93-2004	o	Here we present experiments performed using two complex corpora C1 and C2 extracted from the Penn Treebank -LRB- Marcus et al. 1993 Marcus et al. 1994 -RRB-	num_Marcus_1994 nn_Marcus_al. nn_Marcus_et dep_Marcus_Marcus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_extracted_Treebank conj_and_C1_C2 vmod_corpora_extracted appos_corpora_C2 appos_corpora_C1 amod_corpora_complex num_corpora_two dobj_using_corpora xcomp_performed_using vmod_experiments_performed dep_present_Marcus dobj_present_experiments nsubj_present_we advmod_present_Here
W01-0720	J93-2004	o	CLL has then been applied to a corpus of declarative sentences from the Penn Treebank -LRB- Marcus et al. 1993 Marcus et al. 1994 -RRB- on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems which are significantly more supervised and are applied to somewhat simpler problems	amod_problems_simpler advmod_simpler_somewhat prep_to_applied_problems auxpass_applied_are nsubjpass_applied_which conj_and_supervised_applied advmod_supervised_more advmod_supervised_significantly cop_supervised_are nsubj_supervised_which rcmod_systems_applied rcmod_systems_supervised amod_systems_plausible amod_systems_less advmod_plausible_psychologically advmod_less_much advmod_well_comparatively prep_with_respect_to_perform_systems advmod_perform_well aux_perform_to xcomp_shown_perform auxpass_shown_been aux_shown_has nsubjpass_shown_it prep_on_shown_which num_Marcus_1994 nn_Marcus_al. nn_Marcus_et nn_al._et rcmod_Marcus_shown dep_Marcus_Marcus appos_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the amod_sentences_declarative prep_from_corpus_Treebank prep_of_corpus_sentences det_corpus_a dep_applied_Marcus prep_to_applied_corpus auxpass_applied_been advmod_applied_then aux_applied_has nsubjpass_applied_CLL
W01-0904	J93-2004	o	For example the Penn Treebank -LRB- Marcus et al. 1993 Marcus et al. 1994 Bies et al. 1994 -RRB- provides a large corpus of syntactically annotated examples mostly from the Wall Street Journal	nn_Journal_Street nn_Journal_Wall det_Journal_the amod_examples_annotated advmod_examples_syntactically prep_of_corpus_examples amod_corpus_large det_corpus_a prep_from_provides_Journal advmod_provides_mostly dobj_provides_corpus nsubj_provides_Marcus num_Bies_1994 nn_Bies_al. nn_Bies_et dep_Marcus_Bies num_Marcus_1994 nn_Marcus_al. nn_Marcus_et parataxis_Marcus_provides num_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the dep_``_Treebank prep_for_``_example
W01-0904	J93-2004	o	Hockenmaier et al -LRB- Hockenmaier et al. 2000 -RRB- although to some extent following the approach of Xia -LRB- Xia 1999 -RRB- where LTAGs are extracted have pursued an alternative by extracting Combinatory Categorial Grammar -LRB- CCG -RRB- -LRB- Steedman 1993 Wood 1993 -RRB- lexicons from the Penn Treebank	nn_Treebank_Penn det_Treebank_the nn_lexicons_Grammar dep_Wood_1993 dep_Steedman_Wood dep_Steedman_1993 appos_Grammar_Steedman appos_Grammar_CCG amod_Grammar_Categorial amod_Grammar_Combinatory prep_from_extracting_Treebank dobj_extracting_lexicons det_alternative_an prepc_by_pursued_extracting dobj_pursued_alternative aux_pursued_have prep_following_pursued_approach prep_to_pursued_extent mark_pursued_although auxpass_extracted_are nsubjpass_extracted_LTAGs advmod_extracted_where dep_Xia_1999 rcmod_Xia_extracted dep_Xia_Xia prep_of_approach_Xia det_approach_the det_extent_some amod_Hockenmaier_2000 dep_Hockenmaier_al. nn_Hockenmaier_et dep_al_pursued dep_al_Hockenmaier nn_al_et nn_al_Hockenmaier dep_``_al
W01-0904	J93-2004	o	3.1 The Corpus The systems are applied to examples from the Penn Treebank -LRB- Marcus et al. 1993 Marcus et al. 1994 Bies et al. 1994 -RRB- a corpus of over 4.5 million words of American English annotated with both part-of-speech and syntactic tree information	nn_information_tree amod_information_syntactic amod_information_part-of-speech conj_and_part-of-speech_syntactic preconj_part-of-speech_both prep_with_annotated_information nsubj_annotated_English amod_annotated_American prep_of_words_annotated num_words_million number_million_4.5 quantmod_million_over prep_of_corpus_words det_corpus_a num_Bies_1994 nn_Bies_al. nn_Bies_et dep_Marcus_corpus dep_Marcus_Bies num_Marcus_1994 nn_Marcus_al. nn_Marcus_et nn_al._et dep_Marcus_Marcus appos_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the prep_from_examples_Treebank dep_applied_Marcus prep_to_applied_examples auxpass_applied_are nsubjpass_applied_systems det_systems_The nn_systems_Corpus num_systems_3.1 det_Corpus_The
W01-0904	J93-2004	o	However as Categorial Grammar formalisms do not usually change the lexical entries of words to deal with movement but use further rules -LRB- Wood 1993 Steedman 1993 Hockenmaier et al. 2000 -RRB- the lexicons learned here will be valid over corpora with movement	prep_with_corpora_movement prep_over_valid_corpora cop_valid_be aux_valid_will nsubj_valid_lexicons advcl_valid_use advcl_valid_change advmod_valid_However advmod_learned_here vmod_lexicons_learned det_lexicons_the nn_al._et nn_al._Hockenmaier num_Steedman_1993 amod_Wood_2000 dep_Wood_al. dep_Wood_Steedman dep_Wood_1993 dep_rules_Wood amod_rules_further dobj_use_rules nsubj_use_formalisms prep_with_deal_movement aux_deal_to prep_of_entries_words amod_entries_lexical det_entries_the conj_but_change_use vmod_change_deal dobj_change_entries advmod_change_usually neg_change_not aux_change_do nsubj_change_formalisms mark_change_as nn_formalisms_Grammar amod_formalisms_Categorial
W01-0904	J93-2004	o	Firstly there is also H -LRB- RB -RRB- A -LRB- ADVP -RRB- declined H -LRB- VBD -RRB- H -LRB- VP -RRB- the dollar A -LRB- DT -RRB- H -LRB- NN -RRB- C -LRB- NP-SBJ -RRB- H -LRB- VP -RRB- H -LRB- S -RRB- Figure 2 A tree with constituents marked the top-down method which is a version of the algorithm described by Hockenmaier et al -LRB- Hockenmaier et al. 2000 -RRB- but used for translating into simple -LRB- AB -RRB- CG rather than the Steedmans Combinatory Categorial Grammar -LRB- CCG -RRB- -LRB- Steedman 1993 -RRB-	amod_Steedman_1993 appos_Grammar_CCG nn_Grammar_Categorial nn_Grammar_Combinatory nn_Grammar_Steedmans det_Grammar_the dep_CG_Steedman conj_negcc_CG_Grammar amod_CG_simple dep_simple_AB prep_into_translating_Grammar prep_into_translating_CG prepc_for_used_translating nsubj_used_tree amod_Hockenmaier_2000 dep_Hockenmaier_al. nn_Hockenmaier_et dep_Hockenmaier_Hockenmaier dep_Hockenmaier_al nn_Hockenmaier_et agent_described_Hockenmaier vmod_algorithm_described det_algorithm_the prep_of_version_algorithm det_version_a cop_version_is nsubj_version_which rcmod_method_version amod_method_top-down det_method_the conj_but_marked_used dobj_marked_method nsubj_marked_tree prep_with_tree_constituents det_tree_A num_Figure_2 nn_Figure_H appos_H_S nn_H_H appos_H_VP nn_H_C appos_C_NP-SBJ nn_C_NN nn_C_H dep_A_Figure appos_A_DT nn_A_dollar det_A_the dep_H_A appos_H_VP nn_H_H appos_H_VBD parataxis_declined_used parataxis_declined_marked dobj_declined_H nsubj_declined_A appos_A_ADVP nn_A_H advmod_A_also appos_H_RB ccomp_is_declined expl_is_there advmod_is_Firstly
W01-0908	J93-2004	o	4.1 Data We used Penn-Treebank -LRB- Marcus et al. 1993 -RRB- data presented in Table 1	num_Table_1 prep_in_presented_Table appos_data_Marcus nn_data_Penn-Treebank dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et dobj_used_data nsubj_used_We vmod_Data_presented rcmod_Data_used num_Data_4.1 dep_``_Data
W01-1605	J93-2004	o	The resulting corpus contains 385 documents of American English selected from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- annotated in the framework of Rhetorical Structure Theory	nn_Theory_Structure amod_Theory_Rhetorical prep_of_framework_Theory det_framework_the prep_in_annotated_framework amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_selected_Treebank vmod_English_selected amod_English_American amod_documents_annotated dep_documents_Marcus prep_of_documents_English num_documents_385 dobj_contains_documents nsubj_contains_corpus amod_corpus_resulting det_corpus_The
W01-1605	J93-2004	o	Previous research has shown that RST trees can play a crucial role in building natural language generation systems -LRB- Hovy 1993 Moore and Paris 1993 Moore 1995 -RRB- and text summarization systems -LRB- Marcu 2000 -RRB- can be used to increase the naturalness of machine translation outputs -LRB- Marcu et al. 2000 -RRB- and can be used to build essayscoring systems that provide students with discourse-based feedback -LRB- Burstein et al. 2001 -RRB-	amod_Burstein_2001 dep_Burstein_al. nn_Burstein_et dep_feedback_Burstein amod_feedback_discourse-based prep_with_students_feedback dobj_provide_students nsubj_provide_that rcmod_systems_provide nn_systems_essayscoring dobj_build_systems aux_build_to xcomp_used_build auxpass_used_be aux_used_can nn_al._et dep_Marcu_2000 advmod_Marcu_al. nn_outputs_translation nn_outputs_machine prep_of_naturalness_outputs det_naturalness_the dobj_increase_naturalness aux_increase_to dep_used_Marcu xcomp_used_increase auxpass_used_be aux_used_can amod_Marcu_2000 nn_systems_summarization nn_systems_text dep_Moore_1995 dep_Moore_Marcu conj_and_Moore_systems conj_and_Moore_Moore conj_and_Moore_1993 conj_and_Moore_Paris conj_and_Hovy_used conj_and_Hovy_used conj_and_Hovy_systems conj_and_Hovy_Moore conj_and_Hovy_1993 conj_and_Hovy_Paris conj_and_Hovy_Moore appos_Hovy_1993 dep_systems_used dep_systems_used dep_systems_Moore dep_systems_Hovy nn_systems_generation nn_systems_language amod_systems_natural nn_systems_building amod_role_crucial det_role_a prep_in_play_systems dobj_play_role aux_play_can nsubj_play_trees mark_play_that nn_trees_RST ccomp_shown_play aux_shown_has nsubj_shown_research amod_research_Previous
W01-1626	J93-2004	o	One judge annotated allarticles in four datasets of the Wall Street Journal Treebank corpus -LRB- Marcus et al. 1993 -RRB- -LRB- W9-4 W9-10 W9-22 and W933 each approximately 160K words -RRB- as well as thecorpusofWall Street Journal articles used in -LRB- Wiebe et al. 1999 -RRB- -LRB- called WSJ-SE below -RRB-	npadvmod_below_WSJ-SE dep_called_below dep_Wiebe_called amod_Wiebe_1999 dep_Wiebe_al. nn_Wiebe_et prep_in_used_Wiebe vmod_articles_used nn_articles_Journal nn_articles_Street nn_articles_thecorpusofWall amod_words_160K det_words_each advmod_160K_approximately appos_W9-4_words conj_and_W9-4_W933 conj_and_W9-4_W9-22 conj_and_W9-4_W9-10 dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_of_datasets_corpus num_datasets_four conj_and_allarticles_articles dep_allarticles_W933 dep_allarticles_W9-22 dep_allarticles_W9-10 dep_allarticles_W9-4 appos_allarticles_Marcus prep_in_allarticles_datasets amod_allarticles_annotated nn_allarticles_judge num_allarticles_One dep_``_articles dep_``_allarticles
W01-1626	J93-2004	o	3 Previous Work on Subjectivity Tagging In previous work -LRB- Wiebe et al. 1999 Bruce and Wiebe 1999 -RRB- a corpus of sentences from the Wall Street Journal Treebank Corpus -LRB- Marcus et al. 1993 -RRB- was manually annotated with subjectivity classi cations bymultiplejudges	nn_bymultiplejudges_cations nn_bymultiplejudges_classi nn_bymultiplejudges_subjectivity prep_with_annotated_bymultiplejudges advmod_annotated_manually cop_annotated_was nsubj_annotated_Wiebe dep_annotated_Work amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Corpus_Treebank nn_Corpus_Journal nn_Corpus_Street nn_Corpus_Wall det_Corpus_the prep_from_corpus_Corpus prep_of_corpus_sentences det_corpus_a dep_Bruce_1999 conj_and_Bruce_Wiebe dep_Wiebe_Marcus dep_Wiebe_corpus dep_Wiebe_Wiebe dep_Wiebe_Bruce appos_Wiebe_1999 dep_Wiebe_al. nn_Wiebe_et amod_work_previous nn_Tagging_Subjectivity prep_in_Work_work prep_on_Work_Tagging amod_Work_Previous num_Work_3
W02-0817	J93-2004	o	One of the first large scale hand tagging efforts is reported in -LRB- Miller et al. 1993 -RRB- where a subset of the Brown corpus was tagged with WordNet July 2002 pp	appos_July_pp num_July_2002 tmod_WordNet_July prep_with_tagged_WordNet auxpass_tagged_was nsubjpass_tagged_subset advmod_tagged_where amod_corpus_Brown det_corpus_the prep_of_subset_corpus det_subset_a rcmod_Miller_tagged amod_Miller_1993 dep_Miller_al. nn_Miller_et prep_in_reported_Miller auxpass_reported_is nsubjpass_reported_One amod_efforts_tagging nn_efforts_hand nn_hand_scale amod_hand_large amod_hand_first det_hand_the prep_of_One_efforts ccomp_``_reported
W02-0817	J93-2004	o	3.1 Data The starting corpus we use is formed by a mix of three different sources of data namely the Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB- the Los Angeles Times collection as provided during TREC conferences1 and Open Mind Common Sense2 a collection of about 400,000 commonsense assertions in English as contributed by volunteers over the Web	det_Web_the prep_over_volunteers_Web prep_by_contributed_volunteers mark_contributed_as dep_assertions_contributed prep_in_assertions_English amod_assertions_commonsense num_assertions_400,000 quantmod_400,000_about prep_of_collection_assertions det_collection_a appos_Sense2_collection nn_Sense2_Common nn_Sense2_Mind dobj_Open_Sense2 nn_conferences1_TREC prep_during_provided_conferences1 mark_provided_as nn_collection_Times nn_collection_Angeles nn_collection_Los det_collection_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_corpus_Open advcl_corpus_provided appos_corpus_collection appos_corpus_Marcus nn_corpus_Treebank nn_corpus_Penn det_corpus_the advmod_corpus_namely prep_of_sources_data amod_sources_different num_sources_three prep_of_mix_sources det_mix_a agent_formed_mix auxpass_formed_is nsubjpass_formed_corpus nsubj_use_we rcmod_corpus_use amod_corpus_starting det_corpus_The appos_Data_Open appos_Data_corpus rcmod_Data_formed num_Data_3.1 dep_``_Data
W02-1009	J93-2004	o	Each dataset consisted of a collection of flat rules such as Sput!NP put NP PP extracted from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_extracted_Treebank vmod_PP_extracted nn_PP_NP dep_put_Marcus dobj_put_PP prep_such_as_rules_Sput!NP amod_rules_flat prep_of_collection_rules det_collection_a dep_consisted_put prep_of_consisted_collection nsubj_consisted_dataset det_dataset_Each
W02-1017	J93-2004	o	In one set of experiments we generated lexicons for PEOPLE and ORGANIZATIONS using 2500 Wall Street Journal articles from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_articles_Treebank nn_articles_Journal nn_articles_Street nn_articles_Wall num_articles_2500 dobj_using_articles conj_and_PEOPLE_ORGANIZATIONS prep_for_lexicons_ORGANIZATIONS prep_for_lexicons_PEOPLE dep_generated_Marcus vmod_generated_using dobj_generated_lexicons nsubj_generated_we prep_in_generated_set prep_of_set_experiments num_set_one
W02-1028	J93-2004	o	Even for relatively general texts such as the Wall Street Journal -LRB- Marcus et al. 1993 -RRB- or terrorism articles -LRB- MUC4 Proceedings 1992 -RRB- Roark and Charniak -LRB- Roark and Charniak 1998 -RRB- reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet	prep_in_present_WordNet neg_present_not cop_present_were nsubj_present_3 mark_present_that nn_learner_lexicon amod_learner_semantic poss_learner_their agent_generated_learner vmod_terms_generated num_terms_5 quantmod_5_every prep_of_3_terms ccomp_reported_present nsubj_reported_Charniak nsubj_reported_Roark prep_such_as_reported_articles prep_such_as_reported_Journal advmod_reported_Even amod_Roark_1998 conj_and_Roark_Charniak dep_Roark_Charniak dep_Roark_Roark conj_and_Roark_Charniak dep_Proceedings_1992 nn_Proceedings_MUC4 nn_articles_terrorism dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Journal_Proceedings conj_or_Journal_articles dep_Journal_Marcus nn_Journal_Street nn_Journal_Wall det_Journal_the amod_texts_general advmod_general_relatively prep_for_Even_texts
W02-1031	J93-2004	o	We have observed in several experiments that the number of SuperARVs does not grow signi cantly as training set size increases the moderate-sized Resource Management corpus -LRB- Price et al. 1988 -RRB- with 25,168 words produces 328 SuperARVs compared to 538 SuperARVs for the 1 million word Wall Street Journal -LRB- WSJ -RRB- Penn Treebank set -LRB- Marcus et al. 1993 -RRB- and 791 for the 37 million word training set of the WSJ continuous speech recognition task	nn_task_recognition nn_task_speech amod_task_continuous nn_task_WSJ det_task_the prep_of_set_task nn_set_training nn_set_word num_set_million det_set_the number_million_37 prep_for_791_set conj_and_Marcus_791 dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_set_Treebank nn_set_Penn nn_set_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall nn_Journal_word num_Journal_million det_Journal_the number_million_1 prep_for_SuperARVs_set num_SuperARVs_538 num_SuperARVs_328 dep_produces_791 dep_produces_Marcus pobj_produces_SuperARVs prepc_compared_to_produces_to dobj_produces_SuperARVs nsubj_produces_corpus num_words_25,168 appos_Price_1988 dep_Price_al. nn_Price_et prep_with_corpus_words dep_corpus_Price nn_corpus_Management nn_corpus_Resource amod_corpus_moderate-sized det_corpus_the nn_increases_size amod_increases_set nn_increases_training prep_as_cantly_increases advmod_signi_cantly dobj_grow_signi neg_grow_not aux_grow_does nsubj_grow_number mark_grow_that prep_of_number_SuperARVs det_number_the amod_experiments_several parataxis_observed_produces ccomp_observed_grow prep_in_observed_experiments aux_observed_have nsubj_observed_We
W02-1039	J93-2004	o	When an S alignment exists there will always also exist a P alignment such that P a65 S The English sentences were parsed using a state-of-the-art statistical parser -LRB- Charniak 2000 -RRB- trained on the University of Pennsylvania Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus dep_Treebank_University prep_of_University_Pennsylvania det_University_the prep_on_trained_Treebank dep_Charniak_2000 vmod_parser_trained appos_parser_Charniak amod_parser_statistical amod_parser_state-of-the-art det_parser_a dobj_using_parser xcomp_parsed_using auxpass_parsed_were nsubjpass_parsed_sentences amod_sentences_English det_sentences_The nn_S_a65 dep_S_P dep_S_that dep_S_such dep_alignment_parsed dep_alignment_S nn_alignment_P det_alignment_a dobj_exist_alignment advmod_exist_also advmod_exist_always aux_exist_will expl_exist_there advcl_exist_exists nsubj_exists_alignment advmod_exists_When nn_alignment_S det_alignment_an
W02-1039	J93-2004	o	The first work in SMT done at IBM -LRB- Brown et al. 1993 -RRB- developed a noisy-channel model factoring the translation process into two portions the translation model and the language model	nn_model_language det_model_the conj_and_model_model nn_model_translation det_model_the num_portions_two nn_process_translation det_process_the dep_factoring_model dep_factoring_model prep_into_factoring_portions dobj_factoring_process amod_model_noisy-channel det_model_a dobj_developed_model amod_Brown_1993 dep_Brown_al. nn_Brown_et appos_IBM_Brown dep_done_factoring dep_done_developed prep_at_done_IBM nsubj_done_work prep_in_work_SMT amod_work_first det_work_The
W02-1504	J93-2004	o	In this paper we give an overview of NLPWin a multi-application natural language analysis and generation system under development at Microsoft Research -LRB- Jensen et al. 1993 Gamon et al. 1997 Heidorn 2000 -RRB- incorporating analysis systems for 7 languages -LRB- Chinese English French German Japanese Korean and Spanish -RRB-	conj_and_Chinese_Spanish conj_and_Chinese_Korean conj_and_Chinese_Japanese conj_and_Chinese_German conj_and_Chinese_French conj_and_Chinese_English dep_languages_Spanish dep_languages_Korean dep_languages_Japanese dep_languages_German dep_languages_French dep_languages_English dep_languages_Chinese num_languages_7 nn_systems_analysis prep_for_incorporating_languages dobj_incorporating_systems num_Heidorn_2000 dep_Gamon_Heidorn num_Gamon_1997 nn_Gamon_al. nn_Gamon_et dep_Jensen_Gamon appos_Jensen_1993 dep_Jensen_al. nn_Jensen_et appos_Research_Jensen nn_Research_Microsoft nn_system_generation nn_analysis_language amod_analysis_natural amod_analysis_multi-application det_analysis_a conj_and_NLPWin_system conj_and_NLPWin_analysis prep_of_overview_system prep_of_overview_analysis prep_of_overview_NLPWin det_overview_an vmod_give_incorporating prep_at_give_Research prep_under_give_development dobj_give_overview nsubj_give_we prep_in_give_paper det_paper_this
W02-1504	J93-2004	o	consistency among raters who may have different levels of fluency in the source language raters are not shown the original French or Spanish sentence -LRB- for similar methodologies see Ringger et al. 2001 White et al. 1993 -RRB-	num_White_1993 nn_White_al. nn_White_et num_al._2001 nn_al._et nn_al._Ringger dep_see_al. amod_methodologies_similar dep_for_White conj_for_see pobj_for_methodologies prep_sentence_for amod_sentence_Spanish amod_sentence_French amod_sentence_original det_sentence_the conj_or_French_Spanish dobj_shown_sentence neg_shown_not auxpass_shown_are nsubjpass_shown_raters advcl_shown_consistency nn_language_source det_language_the prep_of_levels_fluency amod_levels_different prep_in_have_language dobj_have_levels aux_have_may nsubj_have_who rcmod_raters_have prep_among_consistency_raters ccomp_``_shown
W02-1504	J93-2004	o	The most common answer is component testing where the component is compared against a standard of goodness usually the Penn Treebank for English -LRB- Marcus et al. 1993 -RRB- allowing a numerical score of precision and recall -LRB- e.g. Collins 1997 -RRB-	num_Collins_1997 pobj_e.g._Collins conj_and_precision_recall prep_score_e.g. prep_of_score_recall prep_of_score_precision amod_score_numerical det_score_a dobj_allowing_score nsubj_allowing_Treebank advmod_allowing_usually amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus prep_for_Treebank_English nn_Treebank_Penn det_Treebank_the prep_of_standard_goodness det_standard_a prep_against_compared_standard auxpass_compared_is nsubjpass_compared_component advmod_compared_where det_component_the vmod_testing_allowing rcmod_testing_compared nn_testing_component cop_testing_is nsubj_testing_answer amod_answer_common det_answer_The advmod_common_most ccomp_``_testing
W02-1507	J93-2004	o	For instance -LRB- Chiang 2000 -RRB- -LRB- Xia 2001 -RRB- -LRB- Chen 2001 -RRB- all automatically acquire large TAGs for English from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_for_TAGs_English amod_TAGs_large prep_from_acquire_Treebank dobj_acquire_TAGs advmod_acquire_automatically dep_acquire_all nsubj_acquire_Xia dep_acquire_Chiang prep_for_acquire_instance dep_Chen_2001 dep_Xia_Chen dep_Xia_2001 amod_Chiang_2000
W02-1509	J93-2004	o	With the availability of large natural language corpora annotated for syntactic structure the treebanks e.g. -LRB- Marcus et al. 1993 -RRB- automatic grammar extraction became possible -LRB- Chen and VijayShanker 2000 Xia 1999 -RRB-	amod_Xia_1999 dep_Chen_Xia conj_and_Chen_2000 conj_and_Chen_VijayShanker dep_possible_2000 dep_possible_VijayShanker dep_possible_Chen acomp_became_possible nsubj_became_extraction dep_became_Marcus advmod_became_e.g. nsubj_became_treebanks prep_with_became_availability nn_extraction_grammar amod_extraction_automatic nn_al._et amod_Marcus_1993 dep_Marcus_al. det_treebanks_the amod_structure_syntactic prep_for_annotated_structure amod_corpora_annotated nn_corpora_language amod_corpora_natural amod_corpora_large prep_of_availability_corpora det_availability_the
W02-2001	J93-2004	o	Any linguistic annotation required during the extraction process therefore is produced through automatic means and it is only for reasons of accessibility and comparability with other research that we choose to work over the Wall Street Journal section of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_section_Treebank nn_section_Journal nn_section_Street nn_section_Wall det_section_the prep_over_work_section aux_work_to xcomp_choose_work nsubj_choose_we mark_choose_that amod_research_other prep_with_accessibility_research conj_and_accessibility_comparability prep_of_reasons_comparability prep_of_reasons_accessibility ccomp_only_choose prep_for_only_reasons cop_only_is nsubj_only_it amod_means_automatic dep_produced_Marcus conj_and_produced_only prep_through_produced_means auxpass_produced_is advmod_produced_therefore nsubjpass_produced_annotation nn_process_extraction det_process_the prep_during_required_process vmod_annotation_required amod_annotation_linguistic det_annotation_Any ccomp_``_only ccomp_``_produced
W02-2001	J93-2004	o	2.2 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types -LRB- % -RRB- Corpus frequency Figure 1 Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl = 1 Brill 135135 1.000 0.177 0.301 Penn 667800 0.834 0.565 0.673 Table 1 POS-based extraction results the WSJ section of the Penn Treebank we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar -LRB- Grover et al. 1993 -RRB- and did a manual corpus search for each	prep_for_search_each nn_search_corpus amod_search_manual det_search_a dobj_did_search nsubj_did_we amod_Grover_1993 dep_Grover_al. nn_Grover_et nn_grammar_Tools nn_grammar_Language nn_grammar_Natural nn_grammar_Alvey det_grammar_the num_VPCs_200 prep_of_sample_VPCs amod_sample_random det_sample_a conj_and_took_did dep_took_Grover prep_from_took_grammar dobj_took_sample nsubj_took_we nn_Treebank_Penn det_Treebank_the prep_of_section_Treebank nn_section_WSJ det_section_the dobj_results_section nsubj_results_extraction amod_extraction_POS-based num_Table_1 num_Table_0.673 num_Table_0.565 dep_0.834_Table number_0.834_667800 dep_Penn_0.834 dep_0.301_Penn dep_0.177_0.301 dep_1.000_0.177 number_1.000_135135 amod_Brill_1.000 num_Brill_1 dep_=_Brill amod_Ffl_= nn_Ffl_Rec nn_Ffl_Prec parataxis_correctextracted_did parataxis_correctextracted_took parataxis_correctextracted_results dobj_correctextracted_Ffl nsubj_correctextracted_distribution nn_Tagger_WSJ det_Tagger_the prep_in_distribution_Tagger prep_of_distribution_VPCs nn_distribution_Frequency num_Figure_1 nn_Figure_frequency nn_Figure_Corpus nn_Figure_types appos_types_% nn_types_VPC num_types_70 num_types_0 num_types_30 advmod_types_namely dep_70_60 dep_60_50 number_50_40 dep_50_30 dep_30_20 number_20_10 num_0_40 number_40_35 number_30_25 dep_30_20 dep_30_5 dep_20_15 number_15_10 number_5_0 dep_extraction_correctextracted appos_extraction_Figure prep_for_targeted_extraction vmod_corpus_targeted det_corpus_the prep_of_frequency_VPCs amod_frequency_relative det_frequency_the prep_in_feel_corpus prep_for_feel_frequency det_feel_a dobj_get_feel aux_get_to dep_get_order mark_get_In advcl_occurrence_get nn_occurrence_Corpus num_occurrence_2.2
W03-0310	J93-2004	n	This cost can often be substantial as with the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the pobj_with_Treebank pcomp_as_with dep_substantial_Marcus prep_substantial_as cop_substantial_be advmod_substantial_often aux_substantial_can nsubj_substantial_cost det_cost_This
W03-0402	J93-2004	o	In recent years reranking techniques have been successfully applied to the so-called history-based models -LRB- Black et al. 1993 -RRB- especially to parsing -LRB- Collins 2000 Collins and Duffy 2002 -RRB-	amod_Collins_2002 conj_and_Collins_Duffy dep_Collins_Duffy dep_Collins_Collins amod_Collins_2000 appos_parsing_Collins dep_al._1993 nn_al._et amod_al._Black dep_models_al. amod_models_history-based amod_models_so-called det_models_the prep_to_applied_parsing advmod_applied_especially prep_to_applied_models advmod_applied_successfully auxpass_applied_been aux_applied_have nsubjpass_applied_techniques prep_in_applied_years amod_techniques_reranking amod_years_recent
W03-0505	J93-2004	o	Table 3 compares precision recall and F scores for our system with CoNLL-2001 results training on sections 15-18 of the Penn Treebank and testing on section 21 -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_section_21 prep_on_testing_section nn_Treebank_Penn det_Treebank_the conj_and_sections_testing prep_of_sections_Treebank num_sections_15-18 prep_on_training_testing prep_on_training_sections dep_results_Marcus dobj_results_training prep_with_system_CoNLL-2001 poss_system_our nn_scores_F conj_and_precision_scores conj_and_precision_recall dep_compares_results prep_for_compares_system dobj_compares_scores dobj_compares_recall dobj_compares_precision nsubj_compares_Table num_Table_3
W03-0806	J93-2004	n	For example 10 million words of the American National Corpus -LRB- Ide et al. 2002 -RRB- will have manually corrected POS tags a tenfold increase over the Penn Treebank -LRB- Marcus et al. 1993 -RRB- currently used for training POS taggers	nn_taggers_POS nn_taggers_training prep_for_used_taggers advmod_used_currently amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the vmod_increase_used dep_increase_Marcus prep_over_increase_Treebank amod_increase_tenfold det_increase_a nn_tags_POS amod_tags_corrected advmod_corrected_manually dobj_have_increase dobj_have_tags aux_have_will nsubj_have_words prep_for_have_example amod_Ide_2002 dep_Ide_al. nn_Ide_et appos_Corpus_Ide nn_Corpus_National nn_Corpus_American det_Corpus_the prep_of_words_Corpus num_words_million number_million_10
W03-0806	J93-2004	o	Machine learning methods should be interchangeable Transformation-based learning -LRB- TBL -RRB- -LRB- Brill 1993 -RRB- and Memory-based learning -LRB- MBL -RRB- -LRB- Daelemans et al. 2002 -RRB- have been applied to many different problems so a single interchangeable component should be used to represent each method	det_method_each dobj_represent_method aux_represent_to xcomp_used_represent auxpass_used_be aux_used_should nsubjpass_used_component amod_component_interchangeable amod_component_single det_component_a advmod_component_so amod_problems_different amod_problems_many ccomp_applied_used prep_to_applied_problems auxpass_applied_been aux_applied_have nsubjpass_applied_learning nsubjpass_applied_learning amod_Daelemans_2002 dep_Daelemans_al. nn_Daelemans_et appos_learning_MBL amod_learning_Memory-based amod_Brill_1993 dep_learning_Daelemans conj_and_learning_learning dep_learning_Brill appos_learning_TBL amod_learning_Transformation-based parataxis_interchangeable_applied cop_interchangeable_be aux_interchangeable_should nsubj_interchangeable_methods nn_methods_learning nn_methods_Machine
W03-0902	J93-2004	o	Our work so far has focused on data in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- particularly the Brown corpus and some examples from the Wall Street Journal corpus	nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_from_examples_corpus det_examples_some conj_and_corpus_examples amod_corpus_Brown det_corpus_the advmod_corpus_particularly dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_in_data_Treebank appos_focused_examples appos_focused_corpus dep_focused_Marcus prep_on_focused_data aux_focused_has advmod_focused_far nsubj_focused_work advmod_far_so poss_work_Our ccomp_``_focused
W03-1002	J93-2004	o	2 Prior Work Statistical machine translation as pioneered by IBM -LRB- e.g. Brown et al. 1993 -RRB- is grounded in the noisy channel model	nn_model_channel amod_model_noisy det_model_the prep_in_grounded_model auxpass_grounded_is nsubjpass_grounded_e.g. num_Brown_1993 nn_Brown_al. nn_Brown_et dep_e.g._Brown dep_IBM_grounded prep_by_pioneered_IBM mark_pioneered_as advcl_,_pioneered nn_translation_machine amod_translation_Statistical nn_translation_Work amod_translation_Prior num_translation_2 dep_``_translation
W03-1002	J93-2004	o	POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit -LRB- Ngai and Florian 2001 -RRB- both were trained from the annotated Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn amod_corpus_annotated det_corpus_the dep_trained_Marcus prep_from_trained_corpus auxpass_trained_were dep_trained_both dep_Ngai_2001 conj_and_Ngai_Florian appos_Toolkit_Florian appos_Toolkit_Ngai nn_Toolkit_fnTBL det_Toolkit_the prep_with_provided_Toolkit vmod_systems_provided amod_systems_trained det_systems_the dobj_using_systems parataxis_done_trained xcomp_done_using auxpass_done_were nsubjpass_done_chunking nsubjpass_done_tagging nn_chunking_phrase prep_in_tagging_English conj_and_tagging_chunking nn_tagging_POS
W03-1006	J93-2004	o	The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 Marcus et al. 1994 -RRB-	num_Marcus_1994 nn_Marcus_al. nn_Marcus_et dep_Marcus_Marcus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the amod_structures_predicate-argument amod_structures_semantic prep_of_annotation_structures det_annotation_an dep_superimposes_Marcus prep_on_top_of_superimposes_Treebank dobj_superimposes_annotation nsubj_superimposes_PropBank det_PropBank_The
W03-1009	J93-2004	o	4.1 Experimental Setup We use the whole Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB- as our data set	nsubj_set_data mark_set_as poss_data_our amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn amod_corpus_whole det_corpus_the dobj_use_corpus nsubj_use_We advcl_Setup_set appos_Setup_Marcus rcmod_Setup_use amod_Setup_Experimental num_Setup_4.1 dep_``_Setup
W03-1707	J93-2004	p	The creation of the Penn English Treebank -LRB- Marcus et al. 1993 -RRB- a syntactically interpreted corpus played a crucial role in the advances in natural language parsing technology -LRB- Collins 1997 Collins 2000 Charniak 2000 -RRB- for English	dep_Charniak_2000 num_Collins_2000 prep_for_Collins_English dep_Collins_Charniak conj_Collins_Collins amod_Collins_1997 dep_technology_Collins nn_technology_parsing nn_technology_language amod_technology_natural prep_in_advances_technology det_advances_the amod_role_crucial det_role_a prep_in_played_advances dobj_played_role nsubj_played_creation amod_corpus_interpreted det_corpus_a advmod_interpreted_syntactically amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_English nn_Treebank_Penn det_Treebank_the appos_creation_corpus dep_creation_Marcus prep_of_creation_Treebank det_creation_The
W03-1712	J93-2004	o	Although few corpora annotated with semantic knowledge are available now there are some valuable lexical databases describing the lexical semantics in dictionary form for example English WordNet -LRB- Miller et al. 1993 -RRB- and Chinese HowNet -LRB- Dong and Dong 2001 -RRB-	amod_Dong_2001 conj_and_Dong_Dong dep_HowNet_Dong dep_HowNet_Dong amod_HowNet_Chinese amod_Miller_1993 dep_Miller_al. nn_Miller_et amod_WordNet_English nn_WordNet_example nn_form_dictionary prep_in_semantics_form amod_semantics_lexical det_semantics_the dobj_describing_semantics vmod_databases_describing amod_databases_lexical amod_databases_valuable det_databases_some conj_and_are_HowNet dep_are_Miller prep_for_are_WordNet nsubj_are_databases expl_are_there parataxis_available_HowNet parataxis_available_are advmod_available_now cop_available_are csubj_available_annotated amod_knowledge_semantic prep_with_annotated_knowledge nsubj_annotated_corpora mark_annotated_Although amod_corpora_few
W03-1712	J93-2004	o	For example the Penn Treebank -LRB- Marcus et al. 1993 -RRB- was annotated with skeletal syntactic structure and many syntactic parsers were evaluated and compared on the corpus	det_corpus_the pobj_on_corpus pcomp_compared_on nsubjpass_compared_parsers conj_and_evaluated_compared auxpass_evaluated_were nsubjpass_evaluated_parsers amod_parsers_syntactic amod_parsers_many nn_structure_syntactic amod_structure_skeletal conj_and_annotated_compared conj_and_annotated_evaluated prep_with_annotated_structure cop_annotated_was nsubj_annotated_Treebank prep_for_annotated_example amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the
W03-1903	J93-2004	o	Ontologies are formal specifications of a conceptualization -LRB- Gruber 1993 -RRB- so that it seems straightforward to formalize annotation schemes as ontologies and make use of semantic annotation tools such as OntoMat -LRB- Handschuh et al. 2001 -RRB- for the purpose of linguistic annotation	amod_annotation_linguistic prep_of_purpose_annotation det_purpose_the amod_Handschuh_2001 dep_Handschuh_al. nn_Handschuh_et dep_OntoMat_Handschuh prep_such_as_tools_OntoMat nn_tools_annotation amod_tools_semantic prep_for_use_purpose prep_of_use_tools dobj_make_use nn_schemes_annotation conj_and_formalize_make prep_as_formalize_ontologies dobj_formalize_schemes aux_formalize_to xcomp_seems_make xcomp_seems_formalize acomp_seems_straightforward nsubj_seems_it mark_seems_that ccomp_so_seems amod_Gruber_1993 dep_conceptualization_Gruber det_conceptualization_a dep_specifications_so prep_of_specifications_conceptualization amod_specifications_formal cop_specifications_are nsubj_specifications_Ontologies
W03-1903	J93-2004	o	Part-of-Speech -LRB- POS -RRB- annotation for example can be seen as the task of choosing the appropriate tag for a word from an ontology of word categories -LRB- compare for example the Penn Treebank POS tagset as described in -LRB- Marcus et al. 1993 -RRB- -RRB-	dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_in_Marcus prep_described_in mark_described_as nn_tagset_POS nn_tagset_Treebank nn_tagset_Penn det_tagset_the advcl_compare_described dobj_compare_tagset prep_for_compare_example nn_categories_word prep_of_ontology_categories det_ontology_an prep_from_word_ontology det_word_a prep_for_tag_word amod_tag_appropriate det_tag_the dobj_choosing_tag prepc_of_task_choosing det_task_the dep_seen_compare prep_as_seen_task auxpass_seen_be aux_seen_can nsubjpass_seen_annotation prep_for_annotation_example nn_annotation_Part-of-Speech appos_Part-of-Speech_POS
W03-2102	J93-2004	o	3 Previous Work on Subjectivity Tagging In previous work -LRB- Wiebe et al. 1999 -RRB- a corpus of sentences from the Wall Street Journal Treebank Corpus -LRB- Marcus et al. 1993 -RRB- was manually anno tated with subjectivity classifications by multiple judges	amod_judges_multiple nn_classifications_subjectivity prep_by_tated_judges prep_with_tated_classifications dep_anno_tated advmod_anno_manually cop_anno_was nsubj_anno_corpus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Corpus_Treebank nn_Corpus_Journal nn_Corpus_Street nn_Corpus_Wall det_Corpus_the appos_corpus_Marcus prep_from_corpus_Corpus prep_of_corpus_sentences det_corpus_a amod_Wiebe_1999 dep_Wiebe_al. nn_Wiebe_et amod_work_previous nn_Tagging_Subjectivity dep_Work_anno dep_Work_Wiebe prep_in_Work_work prep_on_Work_Tagging amod_Work_Previous num_Work_3
W04-0212	J93-2004	p	1 Introduction Large scale annotated corpora such as the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- have played a central role in speech and natural language research	nn_research_language amod_research_natural conj_and_speech_research amod_role_central det_role_a prep_in_played_research prep_in_played_speech dobj_played_role aux_played_have nsubj_played_corpora amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_TreeBank_Marcus nn_TreeBank_Penn det_TreeBank_the prep_such_as_corpora_TreeBank amod_corpora_annotated nn_corpora_scale amod_corpora_Large nn_corpora_Introduction num_corpora_1
W04-0212	J93-2004	o	However developing the PDTB may help facilitate the production of more such corpora through an initial pass of automatic annotation followed by manual correction much as was done in developing the PTB -LRB- Marcus et al. 1993 -RRB- -RRB-	num_Marcus_1993 dep_Marcus_al. nn_Marcus_et det_PTB_the dobj_developing_PTB prepc_in_done_developing auxpass_done_was mark_done_as advmod_done_much nn_correction_manual agent_followed_correction amod_annotation_automatic prep_of_pass_annotation amod_pass_initial det_pass_an amod_corpora_such amod_corpora_more prep_of_production_corpora det_production_the dep_facilitate_Marcus advcl_facilitate_done vmod_facilitate_followed prep_through_facilitate_pass dobj_facilitate_production ccomp_help_facilitate aux_help_may csubj_help_developing advmod_help_However det_PDTB_the dobj_developing_PDTB
W04-0214	J93-2004	o	Since parsing is just an initial stage of natural language understanding the project was focused not just on obtaining syntactic trees alone -LRB- as is done in many other parsed corpora for example Penn TreeBank -LRB- Marcus et al. 1993 -RRB- or Tiger -LRB- Brants and Plaehn 2000 -RRB- -RRB-	appos_Brants_2000 conj_and_Brants_Plaehn nn_Brants_Tiger dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_or_TreeBank_Plaehn conj_or_TreeBank_Brants dep_TreeBank_Marcus nn_TreeBank_Penn amod_corpora_parsed amod_corpora_other amod_corpora_many dep_done_Brants dep_done_TreeBank prep_for_done_example prep_in_done_corpora auxpass_done_is advmod_done_as advmod_trees_alone amod_trees_syntactic parataxis_obtaining_done dobj_obtaining_trees neg_just_not prepc_on_focused_obtaining preconj_focused_just auxpass_focused_was nsubjpass_focused_project advcl_focused_stage det_project_the nn_understanding_language amod_understanding_natural prep_of_stage_understanding amod_stage_initial det_stage_an advmod_stage_just cop_stage_is nsubj_stage_parsing mark_stage_Since
W04-0302	J93-2004	o	The elementary trees were extracted from the parse trees in sections 02-21 of the Wall Street Journal in Penn Treebank -LRB- Marcus et al. 1993 -RRB- which is transformed by using parent-child annotation and left factoring -LRB- Roark and Johnson 1999 -RRB-	dep_Roark_1999 conj_and_Roark_Johnson dep_factoring_Johnson dep_factoring_Roark dobj_left_factoring nsubjpass_left_which amod_annotation_parent-child dobj_using_annotation conj_and_transformed_left agent_transformed_using auxpass_transformed_is nsubjpass_transformed_which amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn prep_in_Journal_Treebank nn_Journal_Street nn_Journal_Wall det_Journal_the prep_of_sections_Journal num_sections_02-21 nn_trees_parse det_trees_the dep_extracted_left dep_extracted_transformed dep_extracted_Marcus prep_in_extracted_sections prep_from_extracted_trees auxpass_extracted_were nsubjpass_extracted_trees amod_trees_elementary det_trees_The
W04-0305	J93-2004	o	6 The Experiments To investigate the e ects of lookahead on our family of deterministic parsers we ran empirical experiments on the standard the Penn Treebank -LRB- Marcus et al. 1993 -RRB- datasets	dep_datasets_Marcus nn_datasets_Treebank dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_standard_datasets det_standard_the amod_experiments_empirical prep_on_ran_standard dobj_ran_experiments nsubj_ran_we nsubj_ran_Experiments amod_parsers_deterministic prep_of_family_parsers poss_family_our prep_of_ects_lookahead dep_ects_e det_ects_the prep_on_investigate_family dobj_investigate_ects aux_investigate_To vmod_Experiments_investigate det_Experiments_The num_Experiments_6
W04-0508	J93-2004	o	Although LDD annotation is actually provided in Treebanks such as the Penn Treebank -LRB- Marcus et al. 1993 -RRB- over which they are typically trained most probabilistic parsers largely or fully ignore this information	det_information_this dobj_ignore_information advmod_ignore_fully advmod_ignore_largely nsubj_ignore_parsers ccomp_ignore_trained conj_or_largely_fully amod_parsers_probabilistic amod_parsers_most advmod_trained_typically auxpass_trained_are nsubjpass_trained_they prep_over_trained_which advcl_trained_provided dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_such_as_Treebanks_Treebank prep_in_provided_Treebanks advmod_provided_actually auxpass_provided_is nsubjpass_provided_annotation mark_provided_Although nn_annotation_LDD
W04-0707	J93-2004	o	2 Detecting Discourse-New Definite Descriptions 2.1 Vieira and Poesio Poesio and Vieira -LRB- 1998 -RRB- carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- around 52 % of DDs are discourse-new -LRB- Prince 1992 -RRB- and another 15 % or so are bridging references for a total of about 66-67 % firstmention	dep_%_firstmention num_%_66-67 quantmod_66-67_about prep_of_total_% det_total_a dobj_bridging_references aux_bridging_are nsubj_bridging_so nsubj_bridging_% conj_or_%_so num_%_15 det_%_another dep_Prince_1992 conj_and_discourse-new_bridging appos_discourse-new_Prince cop_discourse-new_are nsubj_discourse-new_% prep_of_%_DDs num_%_52 quantmod_52_around prep_for_Marcus_total rcmod_Marcus_bridging rcmod_Marcus_discourse-new amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_Journal nn_portion_Street nn_portion_Wall det_portion_the prep_like_corpora_portion dep_that_Marcus prep_in_that_corpora dep_indicating_that nn_studies_corpus xcomp_carried_indicating dobj_carried_studies prt_carried_out csubj_carried_Detecting appos_Vieira_1998 conj_and_Poesio_Vieira nn_Poesio_Poesio conj_and_Vieira_Vieira conj_and_Vieira_Poesio num_Vieira_2.1 dep_Descriptions_Poesio dep_Descriptions_Vieira nn_Descriptions_Definite nn_Descriptions_Discourse-New dobj_Detecting_Descriptions rcmod_2_carried advcl_``_2
W04-1114	J93-2004	o	Word association norms mutual information and lexicography Computational Linguistics 16 -LRB- 1 -RRB- 22-29 Marcus M. et al. 1993	dep_1993_al. num_M._1993 nn_M._et appos_Marcus_M. num_Marcus_22-29 num_1_16 amod_Linguistics_Computational dep_lexicography_Marcus appos_lexicography_1 conj_lexicography_Linguistics amod_information_mutual conj_and_norms_lexicography conj_and_norms_information nn_norms_association nn_norms_Word
W04-1114	J93-2004	o	Collocation Dictionary of Modern Chinese Lexical Words Business Publisher China Yuan Liu et al. 1993	num_al._1993 nn_al._et appos_Liu_al. nn_Liu_Yuan nn_Liu_China nn_Publisher_Business appos_Words_Liu conj_Words_Publisher nn_Words_Lexical nn_Words_Chinese nn_Words_Modern prep_of_Dictionary_Words nn_Dictionary_Collocation
W04-1114	J93-2004	o	The segmentation is based on the guidelines given in the Chinese national standard GB13715 -LRB- Liu et al. 1993 -RRB- and the POS tagging specification was developed according to the Grammatical Knowledge-base of contemporary Chinese	amod_Chinese_contemporary prep_of_Knowledge-base_Chinese amod_Knowledge-base_Grammatical det_Knowledge-base_the pobj_developed_Knowledge-base prepc_according_to_developed_to auxpass_developed_was nsubjpass_developed_specification nsubjpass_developed_Liu amod_specification_tagging nn_specification_POS det_POS_the nn_al._et conj_and_Liu_specification dep_Liu_1993 advmod_Liu_al. amod_GB13715_standard amod_GB13715_national amod_GB13715_Chinese det_GB13715_the prep_in_given_GB13715 det_guidelines_the ccomp_based_developed conj_based_given prep_on_based_guidelines auxpass_based_is nsubjpass_based_segmentation det_segmentation_The
W04-1501	J93-2004	o	Also in the Penn Treebank -LRB- -LRB- Marcus et al. 1993 -RRB- -LRB- Marcus et al. 1994 -RRB- -RRB- a limited set of relations is placed over the constituencybased annotation in order to make explicit the -LRB- morpho-syntactic or semantic -RRB- roles that the constituents play	nsubj_play_constituents dobj_play_that det_constituents_the rcmod_roles_play dep_roles_semantic dep_roles_morpho-syntactic det_roles_the amod_roles_explicit conj_or_morpho-syntactic_semantic dobj_make_roles aux_make_to dep_make_order mark_make_in amod_annotation_constituencybased det_annotation_the advcl_placed_make prep_over_placed_annotation auxpass_placed_is nsubjpass_placed_set prep_in_placed_Treebank advmod_placed_Also prep_of_set_relations amod_set_limited det_set_a dep_set_Marcus amod_Marcus_1994 dep_Marcus_al. nn_Marcus_et dep_al._1993 nn_al._et appos_Marcus_Marcus dep_Marcus_al. nn_Treebank_Penn det_Treebank_the
W04-1602	J93-2004	o	-LRB- Marcus et al. 1993 -RRB- -LRB- Marcus et al. 1994 -RRB- In addition to the usual issues involved with the complex annotation of data we have come to terms with a number of issues that are specific to a highly inflected language with a rich history of traditional grammar	amod_grammar_traditional prep_of_history_grammar amod_history_rich det_history_a prep_with_language_history amod_language_inflected det_language_a advmod_inflected_highly prep_to_specific_language cop_specific_are nsubj_specific_that rcmod_issues_specific prep_of_number_issues det_number_a prep_with_come_number prep_to_come_terms aux_come_have nsubj_come_we prep_of_annotation_data nn_annotation_complex det_annotation_the prep_with_involved_annotation vmod_issues_involved amod_issues_usual det_issues_the nn_al._et num_Marcus_1994 appos_Marcus_al. nn_al._et rcmod_Marcus_come prep_in_addition_to_Marcus_issues appos_Marcus_Marcus amod_Marcus_1993 appos_Marcus_al. dep_''_Marcus
W04-1903	J93-2004	p	Annotated reference corpora such as the Brown Corpus -LRB- Kucera Francis 1967 -RRB- the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and the BNC -LRB- Leech et al. 2001 -RRB- have helped both the development of English computational linguistics tools and English corpus linguistics	nn_linguistics_corpus amod_linguistics_English conj_and_tools_linguistics nn_tools_linguistics amod_tools_computational nn_tools_English prep_of_development_linguistics prep_of_development_tools det_development_the preconj_development_both dobj_helped_development aux_helped_have nsubj_helped_BNC nsubj_helped_corpora amod_Leech_2001 dep_Leech_al. nn_Leech_et appos_BNC_Leech det_BNC_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the dep_Kucera_1967 appos_Kucera_Francis appos_Corpus_Kucera amod_Corpus_Brown det_Corpus_the conj_and_corpora_BNC appos_corpora_Treebank prep_such_as_corpora_Corpus nn_corpora_reference amod_corpora_Annotated
W04-2002	J93-2004	o	A quick search in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- shows that about 17 % of all sentences contain parentheticals or other sentence fragments interjections or unbracketable constituents	amod_constituents_unbracketable nn_fragments_sentence amod_fragments_other conj_or_parentheticals_constituents conj_or_parentheticals_interjections conj_or_parentheticals_fragments dobj_contain_constituents dobj_contain_interjections dobj_contain_fragments dobj_contain_parentheticals nsubj_contain_% mark_contain_that det_sentences_all prep_of_%_sentences num_%_17 quantmod_17_about ccomp_shows_contain nsubj_shows_search amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_search_Marcus prep_in_search_Treebank amod_search_quick det_search_A ccomp_``_shows
W04-2003	J93-2004	o	Although grammatical function and empty nodes annotation expressing long-distance dependencies are provided in Treebanks such as the Penn Treebank -LRB- Marcus et al. 1993 -RRB- most statistical Treebank trained parsers fully or largely ignore them 1 which entails two problems first the training can not profit from valuable annotation data	nn_data_annotation amod_data_valuable prep_from_profit_data neg_profit_not aux_profit_can nsubj_profit_training advmod_profit_first det_training_the dep_problems_profit num_problems_two dobj_entails_problems nsubj_entails_which rcmod_1_entails dep_them_1 dobj_ignore_them advmod_ignore_largely advmod_ignore_fully nsubj_ignore_parsers dep_ignore_Treebank advcl_ignore_provided conj_or_fully_largely amod_parsers_trained amod_Treebank_statistical amod_Treebank_most amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_such_as_Treebanks_Treebank prep_in_provided_Treebanks auxpass_provided_are nsubjpass_provided_nodes nsubjpass_provided_function mark_provided_Although nn_dependencies_long-distance dobj_expressing_dependencies vmod_annotation_expressing dep_nodes_annotation amod_nodes_empty conj_and_function_nodes amod_function_grammatical
W04-2208	J93-2004	p	On the other hand high-quality treebanks such as the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and the Kyoto University text corpus -LRB- Kurohashi and Nagao 1997 -RRB- have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structure analysis	nn_analysis_structure nn_analysis_syntactic conj_and_analysis_analysis amod_analysis_morphological prep_such_as_processing_analysis prep_such_as_processing_analysis nsubj_processing_language amod_language_natural prepc_for_techniques_processing amod_techniques_fundamental prep_of_accuracies_techniques det_accuracies_the dobj_improving_accuracies prepc_to_contributed_improving aux_contributed_have nsubj_contributed_treebanks prep_on_contributed_hand dep_Kurohashi_1997 conj_and_Kurohashi_Nagao appos_corpus_Nagao appos_corpus_Kurohashi nn_corpus_text nn_corpus_University nn_corpus_Kyoto det_corpus_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Treebank_corpus dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_such_as_treebanks_corpus prep_such_as_treebanks_Treebank amod_treebanks_high-quality amod_hand_other det_hand_the
W04-2208	J93-2004	o	The definitions of part-of-speech -LRB- POS -RRB- categories and syntactic labels follow those of the Treebank I style -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_style_I nn_style_Treebank det_style_the prep_of_those_style dep_follow_Marcus dobj_follow_those nsubj_follow_definitions amod_labels_syntactic conj_and_categories_labels nn_categories_part-of-speech appos_part-of-speech_POS prep_of_definitions_labels prep_of_definitions_categories det_definitions_The
W04-2403	J93-2004	o	4 The Experiments For the experiments we used PropBank -LRB- www.cis.upenn.edu/ace -RRB- along with PennTreeBank5 2 -LRB- www.cis.upenn.edu/treebank -RRB- -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_PennTreeBank5_www.cis.upenn.edu/treebank num_PennTreeBank5_2 appos_PropBank_www.cis.upenn.edu/ace dep_used_Marcus pobj_used_PennTreeBank5 prepc_along_with_used_with dobj_used_PropBank nsubj_used_we det_experiments_the dep_Experiments_used prep_for_Experiments_experiments det_Experiments_The num_Experiments_4 dep_``_Experiments
W04-2407	J93-2004	o	Thus the Penn Treebank of American English -LRB- Marcus et al. 1993 -RRB- has been used to train and evaluate the best available parsers of unrestricted English text -LRB- Collins 1999 Charniak 2000 -RRB-	amod_Charniak_2000 dep_Collins_Charniak appos_Collins_1999 dep_text_Collins amod_text_English amod_text_unrestricted prep_of_parsers_text amod_parsers_available det_parsers_the dep_available_best dobj_train_parsers conj_and_train_evaluate aux_train_to xcomp_used_evaluate xcomp_used_train auxpass_used_been aux_used_has nsubjpass_used_Treebank advmod_used_Thus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_English_American dep_Treebank_Marcus prep_of_Treebank_English nn_Treebank_Penn det_Treebank_the
W04-2412	J93-2004	o	3 Data The data consists of six sections of the Wall Street Journal part of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and follows the setting of past editions of the CoNLL shared task training set -LRB- sections 15-18 -RRB- development set -LRB- section 20 -RRB- and test set -LRB- section 21 -RRB-	num_section_21 appos_set_section nn_set_test num_section_20 conj_and_set_set appos_set_section nn_set_development num_sections_15-18 appos_set_set appos_set_set appos_set_sections nn_set_training dep_task_set dobj_shared_task nsubj_shared_setting det_CoNLL_the prep_of_editions_CoNLL amod_editions_past prep_of_setting_editions det_setting_the ccomp_follows_shared nsubj_follows_data amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_part_Treebank nn_part_Journal nn_part_Street nn_part_Wall det_part_the prep_of_sections_part num_sections_six conj_and_consists_follows dep_consists_Marcus prep_of_consists_sections nsubj_consists_data det_data_The nn_data_Data num_data_3
W04-2703	J93-2004	p	The Penn TreeBank -LRB- PTB -RRB- is an example of such a resource with worldwide impact on natural language processing -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_processing_language amod_processing_natural prep_on_impact_processing amod_impact_worldwide det_resource_a predet_resource_such dep_example_Marcus prep_with_example_impact prep_of_example_resource det_example_an cop_example_is nsubj_example_TreeBank appos_TreeBank_PTB nn_TreeBank_Penn det_TreeBank_The
W04-2708	J93-2004	n	Since Czech is a language with relatively high degree of word-order freedom and its sentences contain certain syntactic phenomena such as discontinuous constituents -LRB- non-projective constructions -RRB- which can not be straightforwardly handled using the annotation scheme of Penn Treebank -LRB- Marcus et al. 1993 Linguistic Data Consortium 1999 -RRB- based on phrase-structure trees we decided to adopt for the PCEDT the dependency-based annotation scheme of the Prague Dependency Treebank PDT -LRB- Linguistic Data Consortium 2001 -RRB-	dep_Consortium_2001 nn_Consortium_Data nn_Consortium_Linguistic nn_PDT_Treebank nn_PDT_Dependency nn_PDT_Prague det_PDT_the dep_scheme_Consortium prep_of_scheme_PDT nn_scheme_annotation amod_scheme_dependency-based det_scheme_the dep_PCEDT_scheme det_PCEDT_the prep_for_adopt_PCEDT aux_adopt_to xcomp_decided_adopt nsubj_decided_we amod_trees_phrase-structure prep_on_based_trees dep_Consortium_1999 nn_Consortium_Data nn_Consortium_Linguistic dep_Marcus_Consortium amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn prep_of_scheme_Treebank nn_scheme_annotation det_scheme_the dobj_using_scheme xcomp_handled_using advmod_handled_straightforwardly auxpass_handled_be neg_handled_not aux_handled_can nsubjpass_handled_which amod_constructions_non-projective dep_constituents_Marcus rcmod_constituents_handled appos_constituents_constructions amod_constituents_discontinuous vmod_phenomena_based prep_such_as_phenomena_constituents amod_phenomena_syntactic amod_phenomena_certain parataxis_contain_decided dobj_contain_phenomena nsubj_contain_sentences cc_contain_and advcl_contain_language poss_sentences_its nn_freedom_word-order prep_of_degree_freedom amod_degree_high advmod_high_relatively prep_with_language_degree det_language_a cop_language_is nsubj_language_Czech mark_language_Since
W05-0106	J93-2004	o	A model was trained using Maximum Likelihood from the UPenn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_UPenn det_Treebank_the nn_Likelihood_Maximum prep_from_using_Treebank dobj_using_Likelihood xcomp_trained_using auxpass_trained_was nsubjpass_trained_model det_model_A
W05-0302	J93-2004	p	Introduction The creation of the Penn Treebank -LRB- Marcus et al 1993 -RRB- and the word sense-annotated SEMCOR -LRB- Fellbaum 1997 -RRB- have shown how even limited amounts of annotated data can result in major improvements in complex natural language understanding systems	dobj_understanding_systems vmod_language_understanding amod_language_natural amod_language_complex prep_in_improvements_language amod_improvements_major prep_in_result_improvements aux_result_can nsubj_result_amounts dep_result_limited amod_data_annotated prep_of_amounts_data advmod_limited_even advmod_limited_how ccomp_shown_result aux_shown_have nsubj_shown_SEMCOR nsubj_shown_Introduction amod_Fellbaum_1997 appos_SEMCOR_Fellbaum amod_SEMCOR_sense-annotated nn_SEMCOR_word det_SEMCOR_the amod_Marcus_1993 dep_Marcus_al nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_creation_Treebank det_creation_The conj_and_Introduction_SEMCOR dep_Introduction_Marcus dep_Introduction_creation
W05-0305	J93-2004	o	1 Introduction The overall goal of the Penn Discourse Treebank -LRB- PDTB -RRB- is to annotate the million word WSJ corpus in the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- with a layer of discourse annotations	nn_annotations_discourse prep_of_layer_annotations det_layer_a amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the nn_corpus_WSJ nn_corpus_word num_corpus_million det_corpus_the prep_in_annotate_TreeBank dobj_annotate_corpus aux_annotate_to xcomp_is_annotate nsubj_is_goal appos_Treebank_PDTB nn_Treebank_Discourse nn_Treebank_Penn det_Treebank_the prep_of_goal_Treebank amod_goal_overall det_goal_The prep_with_Introduction_layer dep_Introduction_Marcus rcmod_Introduction_is num_Introduction_1
W05-0307	J93-2004	o	A third of this is syntactically parsed as part of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and has dialog act annotation -LRB- Shriberg et al. 1998 -RRB-	amod_Shriberg_1998 dep_Shriberg_al. nn_Shriberg_et dep_annotation_Shriberg nn_annotation_act nn_annotation_dialog dobj_has_annotation nsubj_has_third amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_part_Treebank conj_and_parsed_has dep_parsed_Marcus prep_as_parsed_part advmod_parsed_syntactically auxpass_parsed_is nsubjpass_parsed_third prep_of_third_this det_third_A ccomp_``_has ccomp_``_parsed
W05-0309	J93-2004	p	1 Introduction There is a pressing need for a consensus on a taskoriented level of semantic representation that can enable the development of powerful new semantic analyzers in the same way that the Penn Treebank -LRB- Marcus et al. 1993 -RRB- enabled the development of statistical syntactic parsers -LRB- Collins 1999 Charniak 2001 -RRB-	amod_Charniak_2001 dep_Collins_Charniak amod_Collins_1999 appos_parsers_Collins nn_parsers_syntactic amod_parsers_statistical prep_of_development_parsers det_development_the dobj_enabled_development nsubj_enabled_Treebank mark_enabled_that amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_Marcus nn_Treebank_Penn det_Treebank_the ccomp_way_enabled amod_way_same det_way_the amod_analyzers_semantic amod_analyzers_new amod_analyzers_powerful prep_of_development_analyzers det_development_the prep_in_enable_way dobj_enable_development aux_enable_can nsubj_enable_that amod_representation_semantic rcmod_level_enable prep_of_level_representation amod_level_taskoriented det_level_a prep_on_consensus_level det_consensus_a prep_for_need_consensus amod_need_pressing det_need_a nsubj_is_need expl_is_There rcmod_Introduction_is num_Introduction_1
W05-0310	J93-2004	o	6 Discussion Lack of interannotator agreement presents a significant problem in annotation efforts -LRB- see e.g. Marcus et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Marcus dep_see_al. dep_see_e.g. nn_efforts_annotation prep_in_problem_efforts amod_problem_significant det_problem_a dep_presents_see dobj_presents_problem nsubj_presents_Lack nn_agreement_interannotator prep_of_Lack_agreement nn_Lack_Discussion num_Lack_6
W05-0310	J93-2004	o	Post-editing of automatic annotation has been pursued in various projects -LRB- e.g. Brants 2000 and Marcus et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Marcus num_Brants_2000 conj_and_e.g._al. conj_and_e.g._Brants dep_projects_al. dep_projects_Brants dep_projects_e.g. amod_projects_various prep_in_pursued_projects auxpass_pursued_been aux_pursued_has nsubjpass_pursued_Post-editing amod_annotation_automatic prep_of_Post-editing_annotation
W05-0310	J93-2004	o	The latter group did an experiment early on in which they found that manual tagging took about twice as long as correcting -LSB- automated tagging -RSB- with about twice the interannotator disagreement rate and an error rate that was about 50 % higher -LRB- Marcus et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_Marcus_1993 npadvmod_higher_% cop_higher_was nsubj_higher_that num_%_50 quantmod_50_about rcmod_rate_higher nn_rate_error det_rate_an conj_and_rate_rate nn_rate_disagreement nn_rate_interannotator det_rate_the advmod_rate_twice amod_rate_about dep_tagging_Marcus prep_with_tagging_rate prep_with_tagging_rate amod_tagging_automated dobj_correcting_tagging advmod_long_as advmod_long_twice advmod_twice_about prepc_as_took_correcting dobj_took_long nsubj_took_tagging mark_took_that amod_tagging_manual ccomp_found_took nsubj_found_they prep_in_found_which prepc_on_experiment_found advmod_experiment_early det_experiment_an dobj_did_experiment nsubj_did_group amod_group_latter det_group_The
W05-0402	J93-2004	o	The list is obtained by first extracting the phrases with TMP function tags from the PennTree bank and taking the words in these phrases -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et det_phrases_these det_words_the dep_taking_Marcus prep_in_taking_phrases dobj_taking_words nn_bank_PennTree det_bank_the conj_and_tags_taking prep_from_tags_bank nn_tags_function nn_tags_TMP dep_with_taking dep_with_tags prep_phrases_with det_phrases_the dobj_extracting_phrases vmod_first_extracting agent_obtained_first auxpass_obtained_is nsubjpass_obtained_list det_list_The
W05-0404	J93-2004	o	In our framework we employ a simple HMM-based tagger where the most probable tag sequence a29a30 given the words a31 is output -LRB- Weischedel et al. 1993 -RRB- a29 a30 a20a22a32a34a33a36a35a38a37a39a32a41a40 a42 a43a45a44 a30a47a46 a31a49a48a17a20a22a32a34a33a50a35a38a37a39a32a41a40 a42 a43a45a44 a31 a46a30 a48 a43a51a44 a30 a48 Since we do not have enough data which is manually tagged with part-of-speech tags for our applications we used Penn Treebank -LRB- Marcus et al. 1994 -RRB- as our training set	nn_set_training poss_set_our amod_Marcus_1994 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn prep_as_used_set dobj_used_Treebank nsubj_used_we advcl_used_have nsubj_used_a48 poss_applications_our prep_for_tags_applications amod_tags_part-of-speech prep_with_tagged_tags advmod_tagged_manually auxpass_tagged_is nsubjpass_tagged_which rcmod_data_tagged amod_data_enough dobj_have_data neg_have_not aux_have_do nsubj_have_we mark_have_Since nn_a48_a30 nn_a48_a43a51a44 nn_a48_a48 nn_a48_a46a30 nn_a48_a31 nn_a48_a43a45a44 nn_a48_a42 nn_a48_a31a49a48a17a20a22a32a34a33a50a35a38a37a39a32a41a40 num_a48_a30a47a46 nn_a48_a43a45a44 nn_a48_a42 nn_a48_a20a22a32a34a33a36a35a38a37a39a32a41a40 nn_a48_a30 nn_a48_a29 amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et dep_output_Weischedel cop_output_is nsubj_output_sequence advmod_output_where appos_words_a31 det_words_the dobj_given_words vmod_sequence_given appos_sequence_a29a30 nn_sequence_tag amod_sequence_probable det_sequence_the advmod_probable_most rcmod_tagger_output amod_tagger_HMM-based amod_tagger_simple det_tagger_a parataxis_employ_used dobj_employ_tagger nsubj_employ_we prep_in_employ_framework poss_framework_our
W05-0407	J93-2004	o	As referring dataset we used the PropBank corpora available at www.cis.upenn.edu/ace along with the Penn TreeBank 2 -LRB- www.cis.upenn.edu/treebank -RRB- -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_TreeBank_www.cis.upenn.edu/treebank num_TreeBank_2 nn_TreeBank_Penn det_TreeBank_the prep_at_available_www.cis.upenn.edu/ace amod_corpora_available nn_corpora_PropBank det_corpora_the dep_used_Marcus pobj_used_TreeBank prepc_along_with_used_with dobj_used_corpora nsubj_used_we prepc_as_used_referring dobj_referring_dataset
W05-0620	J93-2004	o	2.2 Closed Challenge Setting The organization provided training development and test sets derived from the standard sections of the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- and PropBank -LRB- Palmer et al. 2005 -RRB- corpora	dep_corpora_Palmer dep_corpora_PropBank dep_corpora_Challenge amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the prep_of_sections_TreeBank amod_sections_standard det_sections_the prep_from_derived_sections vmod_sets_derived nn_sets_test conj_and_training_sets conj_and_training_development dobj_provided_sets dobj_provided_development dobj_provided_training vmod_organization_provided det_organization_The dobj_Setting_organization conj_and_Challenge_PropBank dep_Challenge_Marcus vmod_Challenge_Setting amod_Challenge_Closed num_Challenge_2.2
W05-0620	J93-2004	o	3 Data The data consists of sections of the Wall Street Journal part of the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- with information on predicate-argument structures extracted from the PropBank corpus -LRB- Palmer et al. 2005 -RRB-	amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et nn_corpus_PropBank det_corpus_the prep_from_extracted_corpus vmod_structures_extracted amod_structures_predicate-argument prep_on_information_structures amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the prep_of_part_TreeBank nn_part_Journal nn_part_Street nn_part_Wall det_part_the prep_of_sections_part dep_consists_Palmer prep_with_consists_information dep_consists_Marcus prep_of_consists_sections nsubj_consists_data det_data_The nn_data_Data num_data_3
W05-1002	J93-2004	o	PB available at www.cis.upenn.edu/ace is used along with the Penn TreeBank 2 -LRB- www.cis.upenn.edu / treebank -RRB- -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_www.cis.upenn.edu_treebank dep_TreeBank_www.cis.upenn.edu num_TreeBank_2 nn_TreeBank_Penn det_TreeBank_the dep_used_Marcus pobj_used_TreeBank prepc_along_with_used_with auxpass_used_is nsubjpass_used_PB prep_at_available_www.cis.upenn.edu/ace amod_PB_available
W05-1008	J93-2004	o	4.4 Corpora We ran the three syntactic preprocessors over a total of three corpora of varying size the Brown corpus -LRB- 460K tokens -RRB- and Wall Street Journal corpus -LRB- 1.2 M tokens -RRB- both derived from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- and the written component of the British National Corpus -LRB- 98M tokens Burnard -LRB- 2000 -RRB- -RRB-	appos_Burnard_2000 dep_tokens_Burnard nn_tokens_98M dep_Corpus_tokens nn_Corpus_National nn_Corpus_British det_Corpus_the prep_of_component_Corpus amod_component_written det_component_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_from_derived_Treebank vmod_both_derived nn_tokens_M num_tokens_1.2 appos_corpus_tokens nn_corpus_Journal nn_corpus_Street nn_corpus_Wall num_tokens_460K conj_and_corpus_component conj_and_corpus_both conj_and_corpus_corpus appos_corpus_tokens nn_corpus_Brown det_corpus_the amod_size_varying num_corpora_three prep_of_total_corpora det_total_a amod_preprocessors_syntactic num_preprocessors_three det_preprocessors_the prep_of_ran_size prep_over_ran_total dobj_ran_preprocessors nsubj_ran_We dep_Corpora_component dep_Corpora_both dep_Corpora_corpus dep_Corpora_corpus rcmod_Corpora_ran num_Corpora_4.4 dep_``_Corpora
W05-1506	J93-2004	o	For this experiment we used sections 02 21 of the Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB- as the training data and section 23 -LRB- 2416 sentences -RRB- for evaluation as is now standard	advmod_standard_now cop_standard_is advmod_standard_as num_sentences_2416 appos_section_sentences num_section_23 prep_for_data_evaluation conj_and_data_section nn_data_training det_data_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the number_21_02 prep_of_sections_Treebank num_sections_21 dep_used_standard prep_as_used_section prep_as_used_data dep_used_Marcus dobj_used_sections nsubj_used_we prep_for_used_experiment det_experiment_this
W05-1506	J93-2004	o	This paper however aims at the k-best tree algorithms whose packed representations are hypergraphs -LRB- Gallo et al. 1993 Klein and Manning 2001 -RRB- -LRB- equivalently and/or graphs or packed forests -RRB- which includes most parsers and parsing-based MT decoders	nn_decoders_MT amod_decoders_parsing-based conj_and_parsers_decoders advmod_parsers_most dobj_includes_decoders dobj_includes_parsers nsubj_includes_which amod_forests_packed conj_or_equivalently_forests conj_and/or_equivalently_graphs dep_Klein_2001 conj_and_Klein_Manning dep_Gallo_Manning dep_Gallo_Klein amod_Gallo_1993 dep_Gallo_al. nn_Gallo_et cop_hypergraphs_are nsubj_hypergraphs_representations amod_representations_packed poss_representations_whose rcmod_algorithms_includes dep_algorithms_forests dep_algorithms_graphs dep_algorithms_equivalently appos_algorithms_Gallo rcmod_algorithms_hypergraphs nn_algorithms_tree amod_algorithms_k-best det_algorithms_the prep_at_aims_algorithms advmod_aims_however nsubj_aims_paper det_paper_This
W05-1506	J93-2004	o	-LRB- 1993 -RRB- study the shortest hyperpath problem and Nielsen et al.	nn_al._et nn_al._Nielsen conj_and_problem_al. nn_problem_hyperpath amod_problem_shortest det_problem_the dep_study_al. dep_study_problem num_study_1993
W05-1506	J93-2004	o	3 Formulation Following Klein and Manning -LRB- 2001 -RRB- we use weighted directed hypergraphs -LRB- Gallo et al. 1993 -RRB- as an abstraction of the probabilistic parsing problem	nn_problem_parsing amod_problem_probabilistic det_problem_the prep_of_abstraction_problem det_abstraction_an amod_Gallo_1993 dep_Gallo_al. nn_Gallo_et dep_hypergraphs_Gallo amod_hypergraphs_directed amod_hypergraphs_weighted prep_as_use_abstraction dobj_use_hypergraphs nsubj_use_we appos_Manning_2001 conj_and_Klein_Manning dep_Formulation_use prep_following_Formulation_Manning prep_following_Formulation_Klein num_Formulation_3
W05-1509	J93-2004	o	State-of-the-art statistical parsers trained on the Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 -RRB- proS a8a8 a8a8a8 a72a72 a72a72a72 NP-SBJ a16a16a16 a80a80a80the authority VP a16a16a16 a16a16a16a16 a0 a0a0 a64 a64a64 a80a80a80 a80a80a80a80 VBD dropped PP-TMP a8a8 a72a72IN at NP NN midnight NP-TMP NNP Tuesday PP-DIR a8a8 a72a72TO to NP QP a16a16a16 a80a80a80 $ 2.80 trillion Figure 1 A sample syntactic structure with function labels	nn_labels_function prep_with_structure_labels nn_structure_syntactic nn_structure_sample det_structure_A num_Figure_1 amod_Figure_$ nn_Figure_a16a16a16 number_trillion_2.80 num_$_trillion number_$_a80a80a80 nn_a16a16a16_QP nn_a16a16a16_NP dep_a72a72TO_structure prep_to_a72a72TO_Figure num_a72a72TO_a8a8 nn_a72a72TO_PP-DIR nn_a72a72TO_Tuesday nn_NNP_NP-TMP nn_NNP_midnight nn_NNP_NN nn_NNP_NP nn_a72a72IN_a8a8 nn_a72a72IN_PP-TMP dep_dropped_a72a72TO prep_at_dropped_NNP dobj_dropped_a72a72IN nsubj_dropped_parsers nn_VBD_a80a80a80a80 nn_VBD_a80a80a80 nn_VBD_a64a64 nn_VBD_a64 nn_VBD_a0a0 nn_VBD_a0 nn_VBD_a16a16a16a16 nn_VBD_a16a16a16 nn_VBD_VP nn_VBD_authority nn_VBD_a80a80a80the nn_VBD_a16a16a16 nn_VBD_NP-SBJ nn_VBD_a72a72a72 nn_VBD_a72a72 nn_VBD_a8a8a8 num_VBD_a8a8 nn_VBD_proS dep_VBD_Marcus dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the prep_on_trained_Treebank dep_parsers_VBD vmod_parsers_trained amod_parsers_statistical amod_parsers_State-of-the-art
W05-1510	J93-2004	p	We evaluated the generator on the Penn Treebank -LRB- Marcus et al. 1993 -RRB- which is highly reliable corpus consisting of real-world texts	amod_texts_real-world prep_of_consisting_texts vmod_corpus_consisting amod_corpus_reliable cop_corpus_is nsubj_corpus_which advmod_reliable_highly dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_on_generator_Treebank det_generator_the dep_evaluated_corpus dep_evaluated_Marcus dobj_evaluated_generator nsubj_evaluated_We
W05-1511	J93-2004	o	Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed -LRB- Kasper et al. 1996 Briscoe and Carroll 1993 Kiefer et al. 2002 -RRB- and the most probable parse is found by PCFG parsing	nn_parsing_PCFG agent_found_parsing auxpass_found_is nsubjpass_found_parse nsubjpass_found_Kiefer nsubjpass_found_1993 nsubjpass_found_Carroll nsubjpass_found_Briscoe amod_parse_probable det_parse_the advmod_probable_most num_Kiefer_2002 nn_Kiefer_al. nn_Kiefer_et conj_and_Briscoe_parse conj_and_Briscoe_Kiefer conj_and_Briscoe_1993 conj_and_Briscoe_Carroll parataxis_Kasper_found appos_Kasper_1996 dep_Kasper_al. nn_Kasper_et dep_developed_Kasper auxpass_developed_been aux_developed_have nsubjpass_developed_models amod_grammar_unification-based det_grammar_the prep_of_backbone_grammar nn_backbone_CFG det_backbone_the prep_to_assigned_backbone auxpass_assigned_are nsubjpass_assigned_probabilities advmod_assigned_where rcmod_models_assigned nn_models_Probabilistic
W05-1511	J93-2004	o	Most of them were developed for exhaustive parsing i.e. producing all parse results that are given by the grammar -LRB- Matsumoto et al. 1983 Maxwell and Kaplan 1993 van Noord 1997 Kiefer et al. 1999 Malouf et al. 2000 Torisawa et al. 2000 Oepen et al. 2002 Penn and Munteanu 2003 -RRB-	num_Penn_2003 conj_and_Penn_Munteanu dep_al._2002 nn_al._et nn_al._Oepen dep_al._2000 nn_al._et nn_al._Torisawa dep_al._2000 nn_al._et nn_al._Malouf nn_al._et nn_al._Kiefer num_Noord_1997 nn_Noord_van dep_Maxwell_Munteanu dep_Maxwell_Penn conj_and_Maxwell_al. conj_and_Maxwell_al. conj_and_Maxwell_al. num_Maxwell_1999 dep_Maxwell_al. dep_Maxwell_Noord num_Maxwell_1993 conj_and_Maxwell_Kaplan dep_Matsumoto_al. dep_Matsumoto_al. dep_Matsumoto_al. dep_Matsumoto_Kaplan dep_Matsumoto_Maxwell appos_Matsumoto_1983 dep_Matsumoto_al. nn_Matsumoto_et det_grammar_the agent_given_grammar auxpass_given_are nsubjpass_given_that rcmod_results_given dobj_parse_results vmod_all_parse dep_producing_Matsumoto dobj_producing_all ccomp_,_producing prep_parsing_i.e. amod_parsing_exhaustive prep_for_developed_parsing auxpass_developed_were nsubjpass_developed_Most prep_of_Most_them
W05-1512	J93-2004	o	Data and Parameters To facilitate comparison with previous work we trained our models on sections 2-21 of the WSJ section of the Penn tree-bank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_tree-bank_Penn det_tree-bank_the prep_of_section_tree-bank nn_section_WSJ det_section_the prep_of_sections_section num_sections_2-21 poss_models_our dep_trained_Marcus prep_on_trained_sections dobj_trained_models nsubj_trained_we nsubj_trained_Parameters nsubj_trained_Data amod_work_previous prep_with_comparison_work dobj_facilitate_comparison aux_facilitate_To vmod_Data_facilitate conj_and_Data_Parameters
W05-1513	J93-2004	o	We trained and tested the parser on the Wall Street Journal corpus of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- using the standard split sections 2-21 were used for training section 22 was used for development and tuning of parameters and features and section 23 was used for testing	prep_for_used_testing auxpass_used_was nsubjpass_used_section num_section_23 conj_and_parameters_features prep_of_development_features prep_of_development_parameters conj_and_development_tuning conj_and_used_used prep_for_used_tuning prep_for_used_development auxpass_used_was nsubjpass_used_section num_section_22 parataxis_used_used parataxis_used_used prep_for_used_training auxpass_used_were nsubjpass_used_sections num_sections_2-21 amod_split_standard det_split_the parataxis_using_used dobj_using_split amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_corpus_Treebank nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_on_parser_corpus det_parser_the nsubj_tested_We dep_trained_using dep_trained_Marcus dobj_trained_parser conj_and_trained_tested nsubj_trained_We
W05-1619	J93-2004	o	For instance the HALOGEN statistical realizer -LSB- LangkildeGeary 2002 -RSB- underwent the most comprehensive evaluation of any surface realizer which was conducted by measuring sentences extracted from the Penn TreeBank -LSB- Marcus et al. 1993 -RSB- converting them into its input formalism and then producing output strings	nn_strings_output dobj_producing_strings advmod_producing_then nn_formalism_input poss_formalism_its prep_into_converting_formalism dobj_converting_them conj_and_Marcus_producing vmod_Marcus_converting amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the prep_from_extracted_TreeBank vmod_sentences_extracted dobj_measuring_sentences agent_conducted_measuring auxpass_conducted_was nsubjpass_conducted_which rcmod_realizer_conducted nn_realizer_surface det_realizer_any prep_of_evaluation_realizer amod_evaluation_comprehensive det_evaluation_the advmod_comprehensive_most dep_underwent_producing dep_underwent_Marcus dobj_underwent_evaluation nsubj_underwent_realizer prep_for_underwent_instance amod_LangkildeGeary_2002 dep_realizer_LangkildeGeary amod_realizer_statistical nn_realizer_HALOGEN det_realizer_the
W05-1619	J93-2004	o	Since text planners can not generate either the requisite syntactic variation or quantity of text -LSB- Langkilde-Geary 2002 -RSB- developed an evaluation strategy for HALOGEN employing a substitute sentence parses from the Penn TreeBank -LSB- Marcus et al. 1993 -RSB- a corpus that includes texts from newspapers such as the Wall Street Journal and which have been hand-annotated for syntax by linguists	prep_by_syntax_linguists prep_for_hand-annotated_syntax cop_hand-annotated_been aux_hand-annotated_have nsubj_hand-annotated_which conj_and_Journal_hand-annotated nn_Journal_Street nn_Journal_Wall det_Journal_the prep_such_as_newspapers_hand-annotated prep_such_as_newspapers_Journal prep_from_texts_newspapers dobj_includes_texts nsubj_includes_that rcmod_corpus_includes det_corpus_a appos_Marcus_corpus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the dep_parses_Marcus prep_from_parses_TreeBank nsubj_parses_sentence det_substitute_a dobj_employing_substitute nsubj_employing_HALOGEN prepc_for_strategy_employing nn_strategy_evaluation det_strategy_an parataxis_developed_parses dobj_developed_strategy nsubj_developed_Langkilde-Geary advcl_developed_generate amod_Langkilde-Geary_2002 prep_of_quantity_text conj_or_variation_quantity nn_variation_syntactic amod_variation_requisite det_variation_the preconj_variation_either dobj_generate_quantity dobj_generate_variation neg_generate_not aux_generate_can nsubj_generate_planners mark_generate_Since nn_planners_text
W06-0305	J93-2004	o	2 The Penn Discourse TreeBank -LRB- PDTB -RRB- The PDTB contains annotations of discourse relations and their arguments on the Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_on_arguments_corpus poss_arguments_their nn_relations_discourse conj_and_annotations_arguments prep_of_annotations_relations dobj_contains_arguments dobj_contains_annotations nsubj_contains_PDTB det_PDTB_The dep_TreeBank_Marcus rcmod_TreeBank_contains appos_TreeBank_PDTB nn_TreeBank_Discourse nn_TreeBank_Penn det_TreeBank_The num_TreeBank_2 dep_``_TreeBank
W06-0602	J93-2004	o	This corpus contains annotations of semantic PASs superimposed on the Penn Treebank -LRB- PTB -RRB- -LRB- Marcus et al. 1993 Marcus et al. 1994 -RRB-	num_Marcus_1994 nn_Marcus_al. nn_Marcus_et dep_Marcus_Marcus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the prep_on_superimposed_Treebank vmod_PASs_superimposed amod_PASs_semantic dep_annotations_Marcus prep_of_annotations_PASs dobj_contains_annotations nsubj_contains_corpus det_corpus_This
W06-0609	J93-2004	o	-LRB- Marcus et al. 1993 Santorini 1990 -RRB- The syntactic annotation task consists of marking constituent boundaries inserting empty categories -LRB- traces of movement PRO pro -RRB- showing the relationships between constituents -LRB- argument/adjunct structures -RRB- and specifying a particular subset of adverbial roles	amod_roles_adverbial prep_of_subset_roles amod_subset_particular det_subset_a dobj_specifying_subset amod_structures_argument/adjunct appos_constituents_structures prep_between_relationships_constituents det_relationships_the conj_and_showing_specifying dobj_showing_relationships dep_PRO_pro dep_movement_PRO vmod_traces_specifying vmod_traces_showing prep_of_traces_movement amod_categories_empty dep_inserting_traces dobj_inserting_categories amod_boundaries_constituent vmod_marking_inserting dobj_marking_boundaries prepc_of_consists_marking nsubj_consists_task dep_consists_Santorini nn_task_annotation amod_task_syntactic det_task_The num_Santorini_1990 num_al._1993 nn_al._et parataxis_Marcus_consists appos_Marcus_al. dep_''_Marcus
W06-0611	J93-2004	o	Section 4 concludes the paper with a critical assessment of the proposed approach and a discussion of the prospects for application in the construction of corpora comparable in size and quality to existing treebanks -LRB- such as for example the Penn Treebank for English -LRB- Marcus et al. 1993 -RRB- or the TIGER Treebank for German -LRB- Brants et al. 2002 -RRB- -RRB-	dep_Brants_2002 dep_Brants_al. nn_Brants_et prep_for_Treebank_German nn_Treebank_TIGER det_Treebank_the dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Brants conj_or_Treebank_Treebank dep_Treebank_Marcus prep_for_Treebank_English nn_Treebank_Penn det_Treebank_the dep_,_Treebank dep_,_Treebank pobj_for_example dep_,_for mwe_as_such prep_treebanks_as amod_treebanks_existing conj_and_size_quality prep_to_comparable_treebanks prep_in_comparable_quality prep_in_comparable_size amod_corpora_comparable prep_of_construction_corpora det_construction_the prep_in_application_construction prep_for_prospects_application det_prospects_the prep_of_discussion_prospects det_discussion_a conj_and_approach_discussion amod_approach_proposed det_approach_the prep_of_assessment_discussion prep_of_assessment_approach amod_assessment_critical det_assessment_a prep_with_paper_assessment det_paper_the dobj_concludes_paper nsubj_concludes_Section num_Section_4
W06-1205	J93-2004	o	1 Introduction A pain in the neck -LRB- Sag et al. 2002 -RRB- for NLP in languages of the Indo-Aryan family -LRB- e.g. Hindi-Urdu Bangla and Kashmiri -RRB- is the fact that most verbs -LRB- nearly half of all instances in Hindi -RRB- occur as complex predicates multi-word complexes which function as a single verbal unit in terms of argument and event structure -LRB- Hook 1993 Butt and Geuder 2003 Raina and Mukerjee 2005 -RRB-	dep_Butt_2005 conj_and_Butt_Mukerjee conj_and_Butt_Raina conj_and_Butt_2003 conj_and_Butt_Geuder dep_Hook_Mukerjee dep_Hook_Raina dep_Hook_2003 dep_Hook_Geuder dep_Hook_Butt appos_Hook_1993 dep_structure_Hook nn_structure_event conj_and_argument_structure prep_of_terms_structure prep_of_terms_argument amod_unit_verbal amod_unit_single det_unit_a prep_in_function_terms prep_as_function_unit nsubj_function_which rcmod_complexes_function amod_complexes_multi-word nn_complexes_predicates amod_complexes_complex prep_as_occur_complexes nsubj_occur_verbs mark_occur_that prep_in_instances_Hindi det_instances_all prep_of_half_instances quantmod_half_nearly appos_verbs_half amod_verbs_most ccomp_fact_occur det_fact_the cop_fact_is nsubj_fact_pain conj_and_Bangla_Kashmiri dep_Hindi-Urdu_Kashmiri dep_Hindi-Urdu_Bangla nn_Hindi-Urdu_e.g. nn_family_Indo-Aryan det_family_the prep_of_languages_family amod_Sag_2002 dep_Sag_al. nn_Sag_et det_neck_the dep_pain_Hindi-Urdu prep_in_pain_languages prep_for_pain_NLP dep_pain_Sag prep_in_pain_neck nn_pain_A nn_A_Introduction num_A_1
W06-1205	J93-2004	o	4.2 Word alignment We have used IBM models proposed by Brown -LRB- Brown et al. 1993 -RRB- for word aligning the parallel corpus	amod_corpus_parallel det_corpus_the dobj_aligning_corpus vmod_word_aligning amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Brown_Brown prep_for_proposed_word agent_proposed_Brown vmod_models_proposed nn_models_IBM dobj_used_models aux_used_have nsubj_used_We rcmod_alignment_used nn_alignment_Word num_alignment_4.2
W06-1601	J93-2004	o	5 Datasets and Evaluation We train our models with verb instances extracted from three parsed corpora -LRB- 1 -RRB- the Wall Street Journal section of the Penn Treebank -LRB- PTB -RRB- which was parsed by human annotators -LRB- Marcus et al. 1993 -RRB- -LRB- 2 -RRB- the Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text -LRB- BLLIP -RRB- which was parsed automatically by the Charniak parser -LRB- Charniak 2000 -RRB- and -LRB- 3 -RRB- the Gigaword corpus of raw newswire text -LRB- GW -RRB- which we parsed ourselves with the Stanford parser	nn_parser_Stanford det_parser_the prep_with_parsed_parser dobj_parsed_ourselves nsubj_parsed_we dobj_parsed_which appos_text_GW nn_text_newswire amod_text_raw rcmod_corpus_parsed prep_of_corpus_text nn_corpus_Gigaword det_corpus_the dobj_3_corpus dep_Charniak_2000 appos_parser_Charniak nn_parser_Charniak det_parser_the conj_and_parsed_3 agent_parsed_parser advmod_parsed_automatically auxpass_parsed_was nsubjpass_parsed_which appos_text_BLLIP nn_text_Journal nn_text_Street nn_text_Wall prep_of_corpus_text nn_corpus_Processing nn_corpus_Information nn_corpus_Linguistic rcmod_Laboratory_3 rcmod_Laboratory_parsed prep_for_Laboratory_corpus amod_Laboratory_Brown det_Laboratory_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_annotators_human agent_parsed_annotators auxpass_parsed_was nsubjpass_parsed_which rcmod_Treebank_parsed appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the dep_section_Laboratory appos_section_2 appos_section_Marcus prep_of_section_Treebank nn_section_Journal nn_section_Street nn_section_Wall det_section_the dep_1_section amod_corpora_parsed num_corpora_three prep_from_extracted_corpora vmod_instances_extracted dep_verb_1 dobj_verb_instances dep_with_verb poss_models_our prep_train_with dobj_train_models nsubj_train_We rcmod_Datasets_train conj_and_Datasets_Evaluation num_Datasets_5
W06-1608	J93-2004	o	The parser is trained on dependencies extracted from the English Penn Treebank version 3.0 -LRB- Marcus et al. 1993 -RRB- by using the head-percolation rules of -LRB- Yamada and Matsumoto 2003 -RRB-	dep_Yamada_2003 conj_and_Yamada_Matsumoto dep_of_Matsumoto dep_of_Yamada prep_rules_of amod_rules_head-percolation det_rules_the dobj_using_rules dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et num_version_3.0 nn_version_Treebank nn_version_Penn nn_version_English det_version_the prep_from_extracted_version vmod_dependencies_extracted agent_trained_using dep_trained_Marcus prep_on_trained_dependencies auxpass_trained_is nsubjpass_trained_parser det_parser_The ccomp_``_trained
W06-1612	J93-2004	o	A third of the corpus is syntactically parsed as part of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- 2This type corresponds to Princes -LRB- 1981 1992 -RRB- inferrables	dep_inferrables_1981 nn_inferrables_Princes dep_1981_1992 prep_to_corresponds_inferrables nsubj_corresponds_type nn_type_2This dep_type_Marcus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_part_Treebank ccomp_parsed_corresponds prep_as_parsed_part advmod_parsed_syntactically auxpass_parsed_is nsubjpass_parsed_third det_corpus_the prep_of_third_corpus det_third_A ccomp_``_parsed
W06-1615	J93-2004	o	5 Data Sets and Supervised Tagger 5.1 Source Domain WSJ We used sections 02-21 of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- for training	dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_sections_Marcus prep_of_sections_Treebank num_sections_02-21 prep_for_used_training dobj_used_sections nsubj_used_We rcmod_WSJ_used nn_Domain_Source num_Domain_5.1 nn_Domain_Tagger amod_Domain_Supervised dep_Sets_WSJ conj_and_Sets_Domain nsubj_Sets_Data num_Data_5 ccomp_``_Domain ccomp_``_Sets
W06-1615	J93-2004	o	There are many choices for modeling co-occurrence data -LRB- Brown et al. 1992 Pereira et al. 1993 Blei et al. 2003 -RRB-	appos_al._2003 nn_al._et nn_al._Blei nn_al._et nn_al._Pereira dep_Brown_al. amod_Brown_1993 dep_Brown_al. amod_Brown_1992 dep_Brown_al. nn_Brown_et nn_data_co-occurrence nn_data_modeling prep_for_choices_data amod_choices_many dep_are_Brown nsubj_are_choices expl_are_There
W06-1636	J93-2004	o	1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank -LRB- Marcus et al. 1993 -RRB- labels -LRB- e.g. S NP etc -RRB-	conj_S_etc conj_S_NP dep_e.g._S ccomp_-LRB-_e.g. appos_labels_Marcus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn amod_Treebank_standard prep_with_grammar_Treebank amod_grammar_context-free det_grammar_a advmod_grammar_only pobj_given_grammar prep_possible_given neg_possible_not cop_possible_is nsubj_possible_parsing dobj_possible_that nn_parsing_syntactic amod_parsing_accurate rcmod_knowledge_possible amod_knowledge_commonplace advmod_commonplace_now prep_by_is_knowledge nsubj_is_It amod_Research_Previous dep_Introduction_labels rcmod_Introduction_is conj_and_Introduction_Research num_Introduction_1
W06-1638	J93-2004	o	321 Jensen-Shannon divergence is defined as D -LRB- q r -RRB- = 12 parenleftbigg D parenleftbigg q | | q + r2 parenrightbigg + D parenleftbigg r | | q + r2 parenrightbiggparenrightbigg These experiments are a kind of poor mans version of the deterministic annealing clustering algorithm -LRB- Pereira et al. 1993 Rose 1998 -RRB- which gradually increases the number of clusters during the clustering process	nn_process_clustering det_process_the prep_of_number_clusters det_number_the prep_during_increases_process dobj_increases_number advmod_increases_gradually nsubj_increases_which dep_Rose_1998 dep_Pereira_Rose appos_Pereira_1993 dep_Pereira_al. nn_Pereira_et nn_algorithm_clustering nn_algorithm_annealing amod_algorithm_deterministic det_algorithm_the prep_of_version_algorithm nn_version_mans amod_version_poor rcmod_kind_increases appos_kind_Pereira prep_of_kind_version det_kind_a cop_kind_are csubj_kind_defined det_experiments_These nn_parenrightbiggparenrightbigg_r2 dep_q_experiments conj_+_q_parenrightbiggparenrightbigg num_q_| number_|_| dep_r_parenrightbiggparenrightbigg dep_r_q nn_r_parenleftbigg nn_r_D conj_+_parenrightbigg_r nn_parenrightbigg_r2 conj_+_q_r conj_+_q_parenrightbigg num_q_| number_|_| dobj_q_parenrightbigg dobj_q_q nsubj_q_parenleftbigg nn_parenleftbigg_D nn_parenleftbigg_parenleftbigg num_parenleftbigg_12 dep_=_q appos_q_r amod_D_= dep_D_q prep_as_defined_D auxpass_defined_is nsubjpass_defined_divergence nn_divergence_Jensen-Shannon num_divergence_321
W06-1638	J93-2004	o	We used sections 220 of the Penn Treebank 2 Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB- for training section 22 as development set and section 23 for testing	num_section_23 conj_and_set_section nn_set_development prep_for_section_testing prep_as_section_section prep_as_section_set num_section_22 dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall num_corpus_2 nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_of_sections_corpus num_sections_220 dep_used_section prep_for_used_training dep_used_Marcus dobj_used_sections nsubj_used_We ccomp_``_used
W06-1652	J93-2004	o	The OP data consists of 2,452 documents from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_documents_Treebank num_documents_2,452 dep_consists_Marcus prep_of_consists_documents nsubj_consists_data nn_data_OP det_data_The
W06-1666	J93-2004	o	5 Experimental Evaluation To perform empirical evaluations of the proposed methods we considered the task of parsing the Penn Treebank Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_corpus_Journal nn_corpus_Street nn_corpus_Wall nn_corpus_Treebank nn_corpus_Penn det_corpus_the dobj_parsing_corpus prepc_of_task_parsing det_task_the dep_considered_Marcus dobj_considered_task nsubj_considered_we nsubj_considered_Evaluation amod_methods_proposed det_methods_the prep_of_evaluations_methods amod_evaluations_empirical dobj_perform_evaluations aux_perform_To vmod_Evaluation_perform amod_Evaluation_Experimental num_Evaluation_5
W06-2110	J93-2004	o	4 Data Collection We evaluated out method by running RASP over Brown Corpus and Wall Street Journal as contained in the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_contained_Marcus prep_in_contained_Treebank mark_contained_as nn_Journal_Street nn_Journal_Wall conj_and_Corpus_Journal amod_Corpus_Brown prep_over_running_Journal prep_over_running_Corpus dobj_running_RASP prepc_by_evaluated_running dobj_evaluated_method prt_evaluated_out nsubj_evaluated_We dep_Collection_contained rcmod_Collection_evaluated nn_Collection_Data num_Collection_4 dep_``_Collection
W06-2112	J93-2004	o	Neither -LRB- Hindle and Rooth 1993 -RRB- with 67 % nor -LRB- Ratnaparkhi et al. 1994 -RRB- with 59 % noun attachment were anywhere close to this figure	det_figure_this prep_to_close_figure advmod_close_anywhere cop_close_were nsubj_close_Neither nn_attachment_noun amod_attachment_% number_%_59 pobj_with_attachment amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et num_%_67 pcomp_with_with conj_nor_with_Ratnaparkhi pobj_with_% amod_Hindle_1993 conj_and_Hindle_Rooth prep_Neither_Ratnaparkhi prep_Neither_with dep_Neither_Rooth dep_Neither_Hindle
W06-2112	J93-2004	o	But it makes obvious that -LRB- Ratnaparkhi et al. 1994 -RRB- were tackling a problem different from -LRB- Hindle and Rooth 1993 -RRB- given the fact that their baseline was at 59 % guessing noun attachment -LRB- rather than 67 % in the Hindle and Rooth experiments -RRB- .3 Of course the baseline is not a direct indicator of the difficulty of the disambiguation task	nn_task_disambiguation det_task_the prep_of_difficulty_task det_difficulty_the prep_of_indicator_difficulty amod_indicator_direct det_indicator_a neg_indicator_not cop_indicator_is nsubj_indicator_baseline det_baseline_the prep_of_.3_course nn_experiments_Rooth conj_and_Hindle_experiments det_Hindle_the rcmod_%_indicator dep_%_.3 prep_in_%_experiments prep_in_%_Hindle num_%_67 quantmod_67_rather mwe_rather_than nn_attachment_noun dobj_guessing_attachment vmod_%_guessing num_%_59 dep_was_% prep_at_was_% nsubj_was_baseline mark_was_that poss_baseline_their ccomp_fact_was det_fact_the pobj_given_fact dep_Hindle_1993 conj_and_Hindle_Rooth dep_from_Rooth dep_from_Hindle prep_different_given prep_different_from amod_problem_different det_problem_a dobj_tackling_problem aux_tackling_were nsubj_tackling_Ratnaparkhi mark_tackling_that amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et ccomp_obvious_tackling acomp_makes_obvious nsubj_makes_it cc_makes_But
W06-2112	J93-2004	o	3.1 Results for English We used sections 0 to 12 of the WSJ part of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- with a total of 24,618 sentences for our experiments	poss_experiments_our num_sentences_24,618 prep_for_total_experiments prep_of_total_sentences det_total_a prep_with_Marcus_total amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_part_Treebank nn_part_WSJ det_part_the prep_of_12_part num_sections_0 prep_to_used_12 dobj_used_sections nsubj_used_We dep_Results_Marcus rcmod_Results_used prep_for_Results_English num_Results_3.1
W06-2303	J93-2004	o	PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_structures_Treebank amod_structures_syntactic det_structures_the nn_annotation_structure nn_annotation_argument prep_of_layer_annotation det_layer_a prep_to_adding_structures dobj_adding_layer amod_information_propositional dep_encodes_Marcus prepc_by_encodes_adding dobj_encodes_information nsubj_encodes_PropBank
W06-2902	J93-2004	o	We use the Penn Treebank Wall Street Journal corpus as the large corpus and individual sections of the Brown corpus as the target corpora -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_corpora_Marcus nn_corpora_target det_corpora_the amod_corpus_Brown det_corpus_the prep_of_corpus_corpus dep_corpus_sections conj_and_corpus_individual amod_corpus_large det_corpus_the nn_corpus_Journal nn_corpus_Street nn_corpus_Wall nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_as_use_corpora prep_as_use_individual prep_as_use_corpus dobj_use_corpus nsubj_use_We
W06-2902	J93-2004	o	This research has focused mostly on the development of statistical parsers trained on large annotated corpora in particular the Penn Treebank WSJ corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_corpus_Marcus nn_corpus_WSJ nn_corpus_Treebank nn_corpus_Penn det_corpus_the dep_corpora_corpus prep_in_corpora_particular amod_corpora_annotated amod_corpora_large prep_on_trained_corpora vmod_parsers_trained amod_parsers_statistical prep_of_development_parsers det_development_the prep_on_focused_development advmod_focused_mostly aux_focused_has nsubj_focused_research det_research_This ccomp_``_focused
W06-3122	J93-2004	o	We retrained the parser on lowercased Penn Treebank II -LRB- Marcus et al. 1993 -RRB- to match the lowercased output of the MT decoder	nn_decoder_MT det_decoder_the prep_of_output_decoder amod_output_lowercased det_output_the dobj_match_output aux_match_to amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_II_Treebank nn_II_Penn amod_II_lowercased prep_on_parser_II det_parser_the vmod_retrained_match dep_retrained_Marcus dobj_retrained_parser nsubj_retrained_We ccomp_``_retrained
W06-3327	J93-2004	o	We measured the accuracy of the POS tagger trained in three settings Original The tagger is trained with the union of Wall Street Journal -LRB- WSJ -RRB- section of Penn Treebank -LRB- Marcus et al 1993 -RRB- GENIA and Penn BioIE	nn_BioIE_Penn nn_1993_al conj_and_Marcus_BioIE conj_and_Marcus_GENIA num_Marcus_1993 nn_Marcus_et nn_Treebank_Penn prep_of_section_Treebank nn_section_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall prep_of_union_section det_union_the dep_trained_BioIE dep_trained_GENIA dep_trained_Marcus prep_with_trained_union auxpass_trained_is nsubjpass_trained_tagger det_tagger_The parataxis_Original_trained num_settings_three prep_in_trained_settings vmod_tagger_trained nn_tagger_POS det_tagger_the prep_of_accuracy_tagger det_accuracy_the dep_measured_Original dobj_measured_accuracy nsubj_measured_We
W06-3604	J93-2004	o	As the third test set we selected all tokens of the Brown corpus part of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- a selected portion of the original one-million word Brown corpus -LRB- Kucera and Francis 1967 -RRB- a collection of samples of American English in many different genres from sources printed in 1961 we refer to this test set as BROWN	prep_as_set_BROWN vmod_test_set det_test_this prep_to_refer_test nsubj_refer_we prep_in_printed_1961 vmod_sources_printed amod_genres_different amod_genres_many amod_English_American prep_in_samples_genres prep_of_samples_English prep_of_collection_samples det_collection_a dep_Kucera_1967 conj_and_Kucera_Francis appos_corpus_Francis appos_corpus_Kucera amod_corpus_Brown nn_corpus_word amod_corpus_one-million amod_corpus_original det_corpus_the parataxis_portion_refer prep_from_portion_sources appos_portion_collection prep_of_portion_corpus amod_portion_selected det_portion_a amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_part_Treebank nn_part_corpus amod_part_Brown det_part_the prep_of_tokens_part det_tokens_all dobj_selected_portion dep_selected_Marcus dobj_selected_tokens nsubj_selected_we prep_as_selected_set nn_set_test amod_set_third det_set_the rcmod_``_selected
W07-0738	J93-2004	o	Tag sets for English are derived from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the prep_from_derived_Treebank auxpass_derived_are nsubjpass_derived_sets prep_for_sets_English nn_sets_Tag
W07-1001	J93-2004	o	The CDR -LRB- Morris 1993 -RRB- is assigned with access to clinical and cognitive test information independent of performance on the battery of neuropsychological tests used for this research study and has been shown to have high expert inter-annotator reliability -LRB- Morris et al. 1997 -RRB-	amod_Morris_1997 dep_Morris_al. nn_Morris_et dep_reliability_Morris amod_reliability_inter-annotator nn_reliability_expert amod_reliability_high dobj_have_reliability aux_have_to xcomp_shown_have auxpass_shown_been aux_shown_has nn_study_research det_study_this prep_for_used_study vmod_tests_used amod_tests_neuropsychological prep_of_battery_tests det_battery_the prep_on_performance_battery conj_and_independent_shown prep_of_independent_performance ccomp_,_shown ccomp_,_independent nn_information_test amod_information_cognitive amod_information_clinical conj_and_clinical_cognitive prep_to_access_information prep_with_assigned_access auxpass_assigned_is nsubjpass_assigned_CDR amod_Morris_1993 appos_CDR_Morris det_CDR_The dep_``_assigned
W07-1001	J93-2004	o	Narrative retellings provide a natural conversational speech sample that can be analyzed for many of the characteristics of speech and language that have been shown to discriminate between healthy and impaired subjects including syntactic complexity -LRB- Kemper et al. 1993 Lyons et al. 1994 -RRB- and mean pause duration -LRB- Singh et al. 2001 -RRB-	amod_Singh_2001 dep_Singh_al. nn_Singh_et nn_duration_pause dobj_mean_duration nsubj_mean_that num_Lyons_1994 nn_Lyons_al. nn_Lyons_et dep_Kemper_Lyons amod_Kemper_1993 dep_Kemper_al. nn_Kemper_et appos_complexity_Kemper amod_complexity_syntactic prep_including_subjects_complexity amod_subjects_impaired amod_subjects_healthy conj_and_healthy_impaired prep_between_discriminate_subjects aux_discriminate_to xcomp_shown_discriminate auxpass_shown_been aux_shown_have nsubjpass_shown_that conj_and_speech_language rcmod_characteristics_shown prep_of_characteristics_language prep_of_characteristics_speech det_characteristics_the prep_of_many_characteristics conj_and_analyzed_mean prep_for_analyzed_many auxpass_analyzed_be aux_analyzed_can nsubjpass_analyzed_that rcmod_sample_mean rcmod_sample_analyzed nn_sample_speech amod_sample_conversational amod_sample_natural det_sample_a dep_provide_Singh dobj_provide_sample nsubj_provide_retellings amod_retellings_Narrative
W07-1217	J93-2004	o	Empirical evaluation has been done with the ERG on a small set of texts from the Wall Street Journal Section 22 of the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_Section_Treebank num_Section_22 nn_Section_Journal nn_Section_Street nn_Section_Wall det_Section_the prep_from_set_Section prep_of_set_texts amod_set_small det_set_a prep_on_ERG_set det_ERG_the dep_done_Marcus prep_with_done_ERG auxpass_done_been aux_done_has nsubjpass_done_evaluation amod_evaluation_Empirical
W07-1502	J93-2004	p	After the success in syntactic -LRB- Penn TreeBank -LRB- Marcus et al. 1993 -RRB- -RRB- and propositional encodings -LRB- Penn PropBank -LRB- Palmer et al. 2005 -RRB- -RRB- more sophisticated semantic data -LRB- such as temporal -LRB- Pustejovsky et al. 2003 -RRB- or opinion annotations -LRB- Wiebe et al. 2005 -RRB- -RRB- and discourse data -LRB- e.g. for anaphora resolution -LRB- van Deemter and Kibble 2000 -RRB- and rhetorical parsing -LRB- Carlson et al. 2003 -RRB- -RRB- are being generated	auxpass_generated_being aux_generated_are nsubjpass_generated_data nsubjpass_generated_data prep_after_generated_success amod_Carlson_2003 dep_Carlson_al. nn_Carlson_et appos_parsing_Carlson amod_parsing_rhetorical dep_Deemter_2000 conj_and_Deemter_Kibble nn_Deemter_van conj_and_resolution_parsing dep_resolution_Kibble dep_resolution_Deemter nn_resolution_anaphora prep_for_e.g._parsing prep_for_e.g._resolution dep_data_e.g. nn_data_discourse amod_Wiebe_2005 dep_Wiebe_al. nn_Wiebe_et dep_annotations_Wiebe amod_annotations_opinion amod_annotations_temporal dep_Pustejovsky_2003 dep_Pustejovsky_al. nn_Pustejovsky_et conj_or_temporal_opinion dep_temporal_Pustejovsky conj_and_data_data prep_such_as_data_annotations amod_data_semantic amod_data_sophisticated advmod_sophisticated_more amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et dep_PropBank_Palmer nn_PropBank_Penn appos_encodings_PropBank amod_encodings_propositional dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_TreeBank_Marcus nn_TreeBank_Penn conj_and_syntactic_encodings appos_syntactic_TreeBank prep_in_success_encodings prep_in_success_syntactic det_success_the
W07-1502	J93-2004	p	While significant time savings have already been reported on the basis of automatic pre-tagging -LRB- e.g. for POS and parse tree taggings in the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- or named entity taggings for the Genia corpus -LRB- Ohta et al. 2002 -RRB- -RRB- this kind of pre-processing does not reduce the number of text tokens actually to be considered	auxpass_considered_be aux_considered_to advmod_considered_actually nn_tokens_text prep_of_number_tokens det_number_the vmod_reduce_considered dobj_reduce_number neg_reduce_not aux_reduce_does nsubj_reduce_kind advcl_reduce_reported prep_of_kind_pre-processing det_kind_this amod_Ohta_2002 dep_Ohta_al. nn_Ohta_et nn_corpus_Genia det_corpus_the dep_taggings_Ohta prep_for_taggings_corpus nn_taggings_entity dep_named_taggings num_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the prep_in_taggings_TreeBank nn_taggings_tree nn_taggings_parse nn_taggings_POS conj_and_POS_parse conj_or_e.g._named dep_e.g._Marcus prep_for_e.g._taggings dep_pre-tagging_named dep_pre-tagging_e.g. amod_pre-tagging_automatic prep_of_basis_pre-tagging det_basis_the prep_on_reported_basis auxpass_reported_been advmod_reported_already aux_reported_have nsubjpass_reported_savings mark_reported_While nn_savings_time amod_savings_significant
W07-1505	J93-2004	o	With respect to already available POS tagsets the scheme allows corresponding extensions of the supertype POSTag to e.g. PennPOSTag -LRB- for the Penn Tag Set -LRB- Marcus et al. 1993 -RRB- -RRB- or GeniaPOSTag -LRB- for the GENIA Tag Set -LRB- Ohta et al. 2002 -RRB- -RRB-	amod_Ohta_2002 dep_Ohta_al. nn_Ohta_et nsubj_Set_Tag mark_Set_for nn_Tag_GENIA det_Tag_the rcmod_GeniaPOSTag_Set amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Set_Marcus nn_Tag_Penn det_Tag_the dep_for_Ohta conj_or_for_GeniaPOSTag dep_for_Set pobj_for_Tag dep_PennPOSTag_GeniaPOSTag dep_PennPOSTag_for dep_e.g._PennPOSTag ccomp_,_e.g. amod_POSTag_supertype det_POSTag_the prep_of_extensions_POSTag amod_extensions_corresponding dep_allows_to dobj_allows_extensions nsubj_allows_scheme prep_with_respect_to_allows_tagsets det_scheme_the nn_tagsets_POS amod_tagsets_available advmod_available_already
W07-1505	J93-2004	o	Currently the scheme supports PhraseChunks with subtypes such as NP VP PP or ADJP -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_or_NP_ADJP conj_or_NP_PP conj_or_NP_VP prep_such_as_subtypes_ADJP prep_such_as_subtypes_PP prep_such_as_subtypes_VP prep_such_as_subtypes_NP dep_PhraseChunks_Marcus prep_with_PhraseChunks_subtypes dobj_supports_PhraseChunks nsubj_supports_scheme advmod_supports_Currently det_scheme_the
W07-1505	J93-2004	o	The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web .4 For -LRB- computational -RRB- linguistics proper syntactic annotation schemes such as the one from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- or semantic annotations such as the one underlying ACE -LRB- Doddington et al. 2004 -RRB- are increasingly being used in a quasi standard way	amod_way_standard amod_way_quasi det_way_a prep_in_used_way auxpass_used_being advmod_used_increasingly aux_used_are amod_Doddington_2004 dep_Doddington_al. nn_Doddington_et dep_ACE_Doddington amod_ACE_underlying num_ACE_one det_one_the amod_annotations_semantic amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_one_Treebank det_one_the nn_schemes_annotation amod_schemes_syntactic prep_such_as_linguistics_ACE conj_or_linguistics_annotations dep_linguistics_Marcus prep_such_as_linguistics_one appos_linguistics_schemes amod_linguistics_proper amod_linguistics_computational nn_.4_Web nn_.4_Semantic det_.4_the prep_for_standard_.4 nn_standard_facto nn_standard_de det_standard_a dep_established_used prep_for_established_annotations prep_for_established_linguistics dobj_established_standard nsubj_established_Initiative3 nn_Initiative3_Metadata nn_Initiative3_Core nn_Initiative3_Dublin det_Initiative3_The
W07-1517	J93-2004	o	The Penn Treebank annotation -LRB- Marcus et al. 1993 -RRB- was chosen to be the first among equals it is the starting point for the merger and data from other annotations are attached at tree nodes	nn_nodes_tree prep_at_attached_nodes auxpass_attached_are nsubjpass_attached_point amod_annotations_other prep_from_merger_annotations conj_and_merger_data det_merger_the prep_for_point_data prep_for_point_merger amod_point_starting det_point_the cop_point_is nsubj_point_it prep_among_first_equals det_first_the cop_first_be aux_first_to parataxis_chosen_attached xcomp_chosen_first auxpass_chosen_was nsubjpass_chosen_annotation amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_annotation_Marcus nn_annotation_Treebank nn_annotation_Penn det_annotation_The
W07-1524	J93-2004	o	Some of them are based upon syntactic structure with PropBank -LRB- Kingsbury and Palmer 2003 -RRB- being one of the most relevant building the annotation upon the syntactic representation of the TreeBank corpus -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_corpus_TreeBank det_corpus_the prep_of_representation_corpus amod_representation_syntactic det_representation_the det_annotation_the prep_upon_building_representation dobj_building_annotation advmod_relevant_most det_relevant_the vmod_one_building prep_of_one_relevant cop_one_being nsubj_one_PropBank dep_Kingsbury_2003 conj_and_Kingsbury_Palmer dep_PropBank_Palmer dep_PropBank_Kingsbury amod_structure_syntactic dep_based_Marcus prepc_with_based_one prep_upon_based_structure auxpass_based_are nsubjpass_based_Some prep_of_Some_them ccomp_``_based
W07-1530	J93-2004	o	6 Penn Discourse Treebank -LRB- Bonnie Webber Edinburgh -RRB- The Penn Discourse TreeBank -LRB- Miltsakaki et al. 2004 Prasad et al. 2004 Webber 2005 -RRB- annotates discourse relations over the Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB- in terms of discourse connectives and their arguments	poss_arguments_their nn_connectives_discourse prep_of_terms_connectives amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the nn_relations_discourse conj_and_annotates_arguments prep_in_annotates_terms dep_annotates_Marcus prep_over_annotates_corpus dobj_annotates_relations nsubj_annotates_Treebank dep_Webber_2005 dep_Prasad_Webber num_Prasad_2004 nn_Prasad_al. nn_Prasad_et dep_Miltsakaki_Prasad appos_Miltsakaki_2004 dep_Miltsakaki_al. nn_Miltsakaki_et dep_TreeBank_Miltsakaki nn_TreeBank_Discourse nn_TreeBank_Penn det_TreeBank_The appos_Webber_Edinburgh nn_Webber_Bonnie dep_Treebank_TreeBank dep_Treebank_Webber nn_Treebank_Discourse nn_Treebank_Penn num_Treebank_6 ccomp_``_arguments ccomp_``_annotates
W07-1530	J93-2004	n	It has been difficult to identify all and only those cases where a token functions as a discourse connective and in many cases the syntactic analysis in the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- provides no help	neg_help_no dobj_provides_help nsubj_provides_analysis amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the appos_analysis_Marcus prep_in_analysis_TreeBank amod_analysis_syntactic det_analysis_the amod_cases_many pobj_in_cases conj_and_connective_in amod_discourse_in amod_discourse_connective det_discourse_a rcmod_functions_provides prep_as_functions_discourse amod_functions_token det_functions_a dep_where_functions det_cases_those advmod_cases_only dep_all_where conj_and_all_cases dobj_identify_cases dobj_identify_all aux_identify_to xcomp_difficult_identify cop_difficult_been aux_difficult_has nsubj_difficult_It
W07-1602	J93-2004	o	For this reason each preposition and verb was assigned a weight based on the proportion of occurrences of that word in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- which are labelled with a spatial meaning	amod_meaning_spatial det_meaning_a prep_with_labelled_meaning auxpass_labelled_are nsubjpass_labelled_which amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_in_word_Treebank det_word_that prep_of_occurrences_word prep_of_proportion_occurrences det_proportion_the prep_on_based_proportion vmod_weight_based det_weight_a ccomp_assigned_labelled dep_assigned_Marcus dobj_assigned_weight auxpass_assigned_was nsubjpass_assigned_verb nsubjpass_assigned_preposition prep_for_assigned_reason conj_and_preposition_verb det_preposition_each det_reason_this rcmod_``_assigned
W07-2048	J93-2004	o	We trained the parser on the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the det_parser_the dep_trained_Marcus prep_on_trained_Treebank dobj_trained_parser nsubj_trained_We
W07-2052	J93-2004	o	We parsed the TimeEval data using MSTParser v0 .2 -LRB- McDonald and Pereira 2006 -RRB- which is trained with all Penn Treebank -LRB- Marcus et al. 1993 -RRB- without dependency label	nn_label_dependency amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_all prep_with_trained_Treebank auxpass_trained_is nsubjpass_trained_which dep_McDonald_2006 conj_and_McDonald_Pereira rcmod_.2_trained appos_.2_Pereira appos_.2_McDonald nn_.2_v0 nn_.2_MSTParser prep_without_using_label dep_using_Marcus dobj_using_.2 nn_data_TimeEval det_data_the xcomp_parsed_using dobj_parsed_data nsubj_parsed_We ccomp_``_parsed
W07-2204	J93-2004	o	The sentences included in the gold standard were chosen at random from the BNC subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et det_PTB_the prep_of_section_PTB nn_section_WSJ det_section_the prep_of_sections_section nn_sections_training det_sections_the prep_in_occur_sections neg_occur_not aux_occur_does nsubj_occur_which ccomp_verb_occur dep_a_Marcus amod_a_verb dobj_contain_a nsubj_contain_they mark_contain_that ccomp_condition_contain det_condition_the prep_to_subject_condition det_BNC_the prep_from_random_BNC advmod_chosen_subject prep_at_chosen_random auxpass_chosen_were amod_standard_gold det_standard_the dep_included_chosen prep_in_included_standard nsubj_included_sentences det_sentences_The
W07-2211	J93-2004	o	Examples of this work include a system by Liu et al -LRB- 1990 -RRB- and experiments by Hindle and Rooth -LRB- 1993 -RRB- and Resnik and Hearst -LRB- 1993 -RRB- .2 These efforts had mixed success suggesting that while multi-level preference scores are problematic integrating some corpus data does not solve the problems	det_problems_the dobj_solve_problems neg_solve_not aux_solve_does csubj_solve_integrating advcl_solve_problematic mark_solve_that nn_data_corpus det_data_some dobj_integrating_data cop_problematic_are nsubj_problematic_scores mark_problematic_while nn_scores_preference amod_scores_multi-level ccomp_suggesting_solve amod_success_mixed vmod_had_suggesting dobj_had_success nsubj_had_efforts nsubj_had_experiments det_efforts_These nn_efforts_.2 nn_efforts_Hearst nn_efforts_Resnik appos_Hearst_1993 conj_and_Resnik_Hearst appos_Rooth_1993 conj_and_Hindle_Rooth conj_and_experiments_efforts prep_by_experiments_Rooth prep_by_experiments_Hindle appos_Liu_1990 dep_Liu_al nn_Liu_et prep_by_system_Liu det_system_a conj_and_include_had dobj_include_system nsubj_include_Examples det_work_this prep_of_Examples_work
W07-2216	J93-2004	o	Figure 1 gives an example dependency graph for the sentence Mr. Tomash will remain as a director emeritus whichhasbeenextractedfromthe Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn nn_Treebank_whichhasbeenextractedfromthe appos_emeritus_Treebank nn_emeritus_director det_emeritus_a prep_as_remain_emeritus aux_remain_will nsubj_remain_Tomash nn_Tomash_Mr. rcmod_sentence_remain det_sentence_the nn_graph_dependency nn_graph_example det_graph_an prep_for_gives_sentence dobj_gives_graph nsubj_gives_Figure num_Figure_1
W07-2217	J93-2004	o	5 Parsing experiments 5.1 Data and setup We used the standard partitions of the Wall Street Journal Penn Treebank -LRB- Marcus et al. 1993 -RRB- i.e. sections 2-21 for training section 22 for development and section 23 for evaluation	num_section_23 conj_and_development_section prep_for_section_evaluation prep_for_section_section prep_for_section_development num_section_22 prep_for_sections_training num_sections_2-21 dep_i.e._section dep_i.e._sections amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn nn_Treebank_Journal nn_Treebank_Street nn_Treebank_Wall det_Treebank_the prep_of_partitions_Treebank amod_partitions_standard det_partitions_the dobj_used_partitions nsubj_used_We dep_Data_Marcus rcmod_Data_used conj_and_Data_setup num_Data_5.1 dep_experiments_i.e. dep_experiments_setup dep_experiments_Data nn_experiments_Parsing num_experiments_5 dep_``_experiments
W08-0614	J93-2004	p	1 Introduction Large scale annotated corpora e.g. the Penn TreeBank -LRB- PTB -RRB- project -LRB- Marcus et al. 1993 -RRB- have played an important role in text-mining	amod_role_important det_role_an prep_in_played_text-mining dobj_played_role aux_played_have nsubj_played_project dep_1993_al. nn_al._et num_Marcus_1993 appos_project_Marcus nsubj_project_TreeBank advmod_project_e.g. dep_project_corpora appos_TreeBank_PTB nn_TreeBank_Penn det_TreeBank_the amod_corpora_annotated nn_corpora_scale nn_corpora_Introduction num_corpora_1 amod_scale_Large
W08-0614	J93-2004	o	The current release of PDTB2 .0 contains the annotations of 1,808 Wall Street Journal articles -LRB- ~ 1 million words -RRB- from the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- II distribution and a total of 40,600 discourse connective tokens -LRB- Prasad et al. 2008b -RRB-	nn_2008b_al. nn_2008b_et dep_Prasad_2008b amod_tokens_connective nn_tokens_discourse num_tokens_40,600 prep_of_total_tokens det_total_a conj_and_distribution_total dep_1993_al. nn_al._et dep_Marcus_Prasad dep_Marcus_total dep_Marcus_distribution num_Marcus_II num_Marcus_1993 nn_TreeBank_Penn det_TreeBank_the num_words_million number_million_1 dep_~_words dep_articles_~ nn_articles_Journal nn_articles_Street nn_articles_Wall num_articles_1,808 prep_from_annotations_TreeBank prep_of_annotations_articles det_annotations_the dep_contains_Marcus dobj_contains_annotations nsubj_contains_release nn_.0_PDTB2 prep_of_release_.0 amod_release_current det_release_The
W08-1008	J93-2004	o	Other languagesfor which this is the case include English -LRB- with the Penn treebank -LRB- Marcus et al. 1993 -RRB- the Susanne Corpus -LRB- Sampson 1993 -RRB- and the British section of the ICE Corpus -LRB- Wallis and Nelson 2006 -RRB- -RRB- and Italian -LRB- with ISST -LRB- Montegmagni et al. 2000 -RRB- and TUT -LRB- Bosco et al. 2000 -RRB- -RRB-	amod_Bosco_2000 dep_Bosco_al. nn_Bosco_et amod_Montegmagni_2000 dep_Montegmagni_al. nn_Montegmagni_et dep_with_Bosco conj_and_with_TUT dep_with_Montegmagni pobj_with_ISST dep_Italian_TUT dep_Italian_with dep_Wallis_2006 conj_and_Wallis_Nelson dep_Corpus_Nelson dep_Corpus_Wallis nn_Corpus_ICE det_Corpus_the prep_of_section_Corpus amod_section_British det_section_the dep_Sampson_1993 appos_Corpus_Sampson nn_Corpus_Susanne det_Corpus_the amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_treebank_Penn det_treebank_the conj_and_with_Italian conj_and_with_section pobj_with_Corpus dep_with_Marcus pobj_with_treebank dep_English_Italian dep_English_section dep_English_with dobj_include_English nsubj_include_languagesfor det_case_the cop_case_is nsubj_case_this dobj_case_which rcmod_languagesfor_case amod_languagesfor_Other
W08-1301	J93-2004	o	First we noted how frequently WordNet -LRB- Fellbaum 1998 -RRB- gets used compared to other resources such as FrameNet -LRB- Fillmore et al. 2003 -RRB- or the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the num_Fillmore_2003 dep_Fillmore_al. nn_Fillmore_et conj_or_FrameNet_Treebank appos_FrameNet_Fillmore prep_such_as_resources_Treebank prep_such_as_resources_FrameNet amod_resources_other pobj_used_resources prepc_compared_to_used_to auxpass_used_gets nsubjpass_used_WordNet advmod_used_how amod_Fellbaum_1998 dep_WordNet_Fellbaum advmod_WordNet_frequently dep_noted_Marcus ccomp_noted_used nsubj_noted_we advmod_noted_First
W08-2101	J93-2004	o	2 The Data Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank -LRB- PTB -RRB- with PropBank -LRB- PRBK -RRB- -LRB- Marcus et al. 1993 Palmer et al. 2005 -RRB- as shown in Figure 1	num_Figure_1 prep_in_shown_Figure mark_shown_as num_Palmer_2005 nn_Palmer_al. nn_Palmer_et dep_Marcus_shown dep_Marcus_Palmer appos_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_PropBank_PRBK appos_Treebank_PTB nn_Treebank_Penn det_Treebank_the prep_with_merging_PropBank dobj_merging_Treebank agent_produced_merging advmod_produced_automatically auxpass_produced_is nsubjpass_produced_that nn_data_use nn_data_parsing amod_data_semantic rcmod_syntactic_produced conj_and_syntactic_data amod_syntactic_joint prep_on_experiments_data prep_on_experiments_syntactic poss_experiments_Our appos_Data_Marcus dep_Data_experiments det_Data_The dep_2_Data ccomp_``_2
W08-2121	J93-2004	o	html 162 3.1.1 Penn Treebank 3 The Penn Treebank 3 corpus -LRB- Marcus et al. 1993 -RRB- consists of hand-coded parses of the Wall Street Journal -LRB- test development and training -RRB- and a small subset of the Brown corpus -LRB- W. N. Francis and H. Kucera 1964 -RRB- -LRB- test only -RRB-	advmod_test_only nn_Kucera_H. dep_Francis_1964 conj_and_Francis_Kucera nn_Francis_N. nn_Francis_W. amod_corpus_Brown det_corpus_the appos_subset_test dep_subset_Kucera dep_subset_Francis prep_of_subset_corpus amod_subset_small det_subset_a conj_and_test_training conj_and_test_development conj_and_Journal_subset dep_Journal_training dep_Journal_development dep_Journal_test nn_Journal_Street nn_Journal_Wall det_Journal_the prep_of_parses_subset prep_of_parses_Journal amod_parses_hand-coded prep_of_consists_parses nsubj_consists_corpus amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_corpus_Marcus num_corpus_3 nn_corpus_Treebank nn_corpus_Penn det_corpus_The num_corpus_3 dep_corpus_Treebank num_corpus_162 dep_corpus_html nn_Treebank_Penn num_Treebank_3.1.1
W08-2121	J93-2004	o	3.2 Conversion to Dependencies 3.2.1 Syntactic Dependencies There exists no large-scale dependency treebank for English and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the amod_corpus_dependency-annotated det_corpus_a prep_from_construct_Treebank advmod_construct_automatically dobj_construct_corpus aux_construct_to xcomp_had_construct advmod_had_thus nsubj_had_we prep_for_treebank_English nn_treebank_dependency amod_treebank_large-scale neg_treebank_no conj_and_exists_had dobj_exists_treebank expl_exists_There nsubj_exists_Conversion amod_Dependencies_Syntactic num_Dependencies_3.2.1 nn_Dependencies_Dependencies prep_to_Conversion_Dependencies num_Conversion_3.2
W08-2122	J93-2004	o	In this vein the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies -LRB- extracted from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- -RRB- and semantic dependencies -LRB- extracted both from PropBank -LRB- Palmer et al. 2005 -RRB- c2008	appos_c2008_Palmer amod_Palmer_2005 dep_Palmer_al. nn_Palmer_et prep_from_both_PropBank conj_extracted_c2008 dep_extracted_both dep_dependencies_extracted amod_dependencies_semantic amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the dep_extracted_Marcus prep_from_extracted_Treebank conj_and_dependencies_dependencies dep_dependencies_extracted amod_dependencies_syntactic det_dependencies_both dobj_learning_dependencies dobj_learning_dependencies advmod_learning_jointly prepc_of_challenge_learning det_challenge_the dobj_sets_challenge nsubj_sets_task prep_in_sets_vein amod_task_shared num_task_2008 nn_task_CoNLL det_task_the det_vein_this
W09-0103	J93-2004	o	My guess is that the features used in e.g. the Collins -LRB- 2003 -RRB- or Charniak -LRB- 2000 -RRB- parsers are probably close to optimal for English Penn Treebank parsing -LRB- Marcus et al. 1993 -RRB- but that other features might improve parsing of other languages or even other English genres	nn_genres_English amod_genres_other advmod_genres_even amod_languages_other prep_of_parsing_languages conj_or_improve_genres dobj_improve_parsing aux_improve_might nsubj_improve_features mark_improve_that amod_features_other amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_parsing_Treebank nn_parsing_Penn nn_parsing_English prep_for_optimal_parsing nn_parsers_Charniak nn_parsers_Collins det_parsers_the appos_Charniak_2000 conj_or_Collins_Charniak appos_Collins_2003 prep_close_to_in_optimal advmod_in_probably cop_in_are nsubj_in_parsers advmod_in_e.g. prep_used_in conj_but_features_genres conj_but_features_improve dep_features_Marcus vmod_features_used det_features_the prep_that_is_improve prep_that_is_features nsubj_is_guess poss_guess_My ccomp_``_is
W09-0104	J93-2004	o	I have made a preliminary analysis of the inventory of syntactic categories used in the tagging for labelling trees in the 18 Penn Treebank -LRB- Marcus et al. 1993 -RRB- comparing them to the categories used in CGEL	prep_in_used_CGEL vmod_categories_used det_categories_the prep_to_comparing_categories dobj_comparing_them nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn num_Treebank_18 det_Treebank_the prep_in_labelling_Treebank dobj_labelling_trees prepc_for_tagging_labelling det_tagging_the prepc_in_used_tagging vmod_categories_used amod_categories_syntactic prep_of_inventory_categories det_inventory_the prep_of_analysis_inventory amod_analysis_preliminary det_analysis_a xcomp_made_comparing dep_made_Marcus dobj_made_analysis aux_made_have nsubj_made_I
W09-0608	J93-2004	o	However with their system trained on the medical corpus and then tested on the Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB- they achieve an overall prediction accuracy of only 54 %	num_%_54 quantmod_54_only prep_of_accuracy_% nn_accuracy_prediction amod_accuracy_overall det_accuracy_an dobj_achieve_accuracy nsubj_achieve_they prep_achieve_tested prep_achieve_with advmod_achieve_However amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_on_tested_corpus advmod_tested_then amod_corpus_medical det_corpus_the prep_on_trained_corpus vmod_system_trained poss_system_their dep_with_Marcus conj_and_with_tested pobj_with_system
W09-0905	J93-2004	o	Due to its popularity for unsupervised POS induction research -LRB- e.g. Goldberg et al. 2008 Goldwater and Griffiths 2007 Toutanova and Johnson 2008 -RRB- and its often-used tagset for our initial research we use the Wall Street Journal -LRB- WSJ -RRB- portion of the Penn Treebank -LRB- Marcus et al. 1993 -RRB- with 36 tags -LRB- plus 9 punctuation tags -RRB- and we use sections 00-18 leaving held-out data for future experiments .4 Defining frequent frames as those occurring at 4Even if we wanted child-directed speech the CHILDES database -LRB- MacWhinney 2000 -RRB- uses coarse POS tags	nn_tags_POS amod_tags_coarse dobj_uses_tags nsubj_uses_database advcl_uses_wanted amod_MacWhinney_2000 dep_database_MacWhinney nn_database_CHILDES det_database_the amod_speech_child-directed dobj_wanted_speech nsubj_wanted_we mark_wanted_if dep_wanted_use dep_wanted_tagset dep_wanted_Griffiths dep_wanted_Goldwater prep_at_occurring_4Even vmod_those_occurring prep_as_frames_those amod_frames_frequent amod_frames_Defining num_frames_.4 dep_experiments_frames amod_experiments_future amod_data_held-out prep_for_leaving_experiments dobj_leaving_data num_sections_00-18 dobj_use_sections nsubj_use_we nn_tags_punctuation num_tags_9 cc_tags_plus appos_tags_tags num_tags_36 dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_of_portion_Treebank nn_portion_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall det_Journal_the dobj_use_portion nsubj_use_we amod_research_initial poss_research_our amod_tagset_often-used poss_tagset_its dep_Toutanova_2008 conj_and_Toutanova_Johnson vmod_Goldwater_leaving conj_and_Goldwater_use prep_with_Goldwater_tags dep_Goldwater_Marcus rcmod_Goldwater_use prep_for_Goldwater_research conj_and_Goldwater_tagset dep_Goldwater_Johnson dep_Goldwater_Toutanova num_Goldwater_2007 conj_and_Goldwater_Griffiths parataxis_al._uses dep_al._2008 nn_al._et nn_al._Goldberg dep_al._e.g. dep_research_al. nn_research_induction nn_research_POS amod_research_unsupervised prep_for_popularity_research poss_popularity_its prep_due_to_``_popularity
W09-1007	J93-2004	o	For testing purposes we used the Wall Street Journal part of the Penn Treebank corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_of_part_corpus nn_part_Journal nn_part_Street nn_part_Wall det_part_the dep_used_Marcus dobj_used_part nsubj_used_we prep_for_used_purposes nn_purposes_testing
W09-1117	J93-2004	o	The Spanish corpus was parsed using the MST dependency parser -LRB- McDonald et al. 2005 -RRB- trained using dependency trees generated from the the English Penn Treebank -LRB- Marcus et al. 1993 -RRB- and Spanish CoNLL-X data -LRB- Buchholz and Marsi 2006 -RRB-	amod_Buchholz_2006 conj_and_Buchholz_Marsi dep_data_Marsi dep_data_Buchholz nn_data_CoNLL-X amod_data_Spanish amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et conj_and_Treebank_data dep_Treebank_Marcus nn_Treebank_Penn nn_Treebank_English det_Treebank_the det_Treebank_the prep_from_generated_data prep_from_generated_Treebank vmod_trees_generated nn_trees_dependency dobj_using_trees xcomp_trained_using amod_McDonald_2005 dep_McDonald_al. nn_McDonald_et vmod_parser_trained dep_parser_McDonald nn_parser_dependency nn_parser_MST det_parser_the dobj_using_parser xcomp_parsed_using auxpass_parsed_was nsubjpass_parsed_corpus amod_corpus_Spanish det_corpus_The
W09-1404	J93-2004	o	The parser expresses distinctions that are especially important for a predicate-argument based deep syntactic representation as far as they are expressed in the training data generated from the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_generated_Treebank vmod_data_generated nn_data_training det_data_the dep_expressed_Marcus prep_in_expressed_data auxpass_expressed_are nsubjpass_expressed_they mark_expressed_as advmod_expressed_far prepc_as_representation_expressed nn_representation_syntactic amod_representation_deep amod_representation_based dep_predicate-argument_representation det_predicate-argument_a prep_for_important_predicate-argument advmod_important_especially cop_important_are nsubj_important_that rcmod_distinctions_important dobj_expresses_distinctions nsubj_expresses_parser det_parser_The
W09-1805	J93-2004	o	A description of the flat featurized dependency-style syntactic representation we use is available in -LRB- Langkilde-Geary and Betteridge 2006 -RRB- which describes how the entire Penn Treebank -LRB- Marcus et al. 1993 -RRB- was converted to this representation	det_representation_this prep_to_converted_representation auxpass_converted_was nsubjpass_converted_Treebank advmod_converted_how amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn amod_Treebank_entire det_Treebank_the ccomp_describes_converted dep_Langkilde-Geary_2006 conj_and_Langkilde-Geary_Betteridge dep_available_describes prep_in_available_which dep_available_Betteridge dep_available_Langkilde-Geary cop_available_is nsubj_available_description nsubj_use_we nn_representation_syntactic amod_representation_dependency-style amod_representation_featurized amod_representation_flat det_representation_the rcmod_description_use prep_of_description_representation det_description_A
W09-2310	J93-2004	o	5.3 Experimental setup We used the Stanford Parser -LRB- Klein and Manning 2003 -RRB- for both languages Penn English Treebank -LRB- Marcus et al. 1993 -RRB- and Penn Arabic Treebank set -LRB- Kulick et al. 2006 -RRB-	amod_Kulick_2006 dep_Kulick_al. nn_Kulick_et nn_set_Treebank nn_set_Arabic nn_set_Penn amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Kulick conj_and_Treebank_set dep_Treebank_Marcus nn_Treebank_English nn_Treebank_Penn det_languages_both dep_Klein_2003 conj_and_Klein_Manning appos_Parser_Manning appos_Parser_Klein nn_Parser_Stanford det_Parser_the prep_for_used_languages dobj_used_Parser nsubj_used_We dep_setup_set dep_setup_Treebank rcmod_setup_used amod_setup_Experimental num_setup_5.3
W09-2406	J93-2004	o	3 Network Evaluation We present an evaluation which has been carried out on an initial set of annotations of English articles from The Wall Street Journal -LRB- covering those annotated at the syntactic level in the Penn Treebank -LRB- Marcus et al. 1993 -RRB- -RRB-	dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_in_level_Treebank amod_level_syntactic det_level_the prep_at_annotated_level amod_those_annotated dep_covering_Marcus dobj_covering_those nn_Journal_Street nn_Journal_Wall det_Journal_The prep_from_articles_Journal nn_articles_English prep_of_annotations_articles dep_set_covering prep_of_set_annotations amod_set_initial det_set_an prep_on_carried_set prt_carried_out auxpass_carried_been aux_carried_has nsubjpass_carried_which rcmod_evaluation_carried det_evaluation_an dobj_present_evaluation nsubj_present_We rcmod_Evaluation_present nn_Evaluation_Network num_Evaluation_3 dep_``_Evaluation
W09-2603	J93-2004	p	The Penn Treebank -LRB- Marcus et al. 1993 -RRB- has until recently been the only such corpus covering 4.5 M words in a single genre of financial reporting	amod_reporting_financial prep_of_genre_reporting amod_genre_single det_genre_a nn_words_M num_words_4.5 prep_in_covering_genre dobj_covering_words vmod_corpus_covering amod_corpus_such advmod_corpus_only det_corpus_the cop_corpus_been advmod_corpus_recently prepc_until_has_corpus nsubj_has_Treebank num_Marcus_1993 dep_Marcus_al. nn_Marcus_et appos_Treebank_Marcus nn_Treebank_Penn det_Treebank_The
W95-0101	J93-2004	o	Unsupervised Learning Results To test the effectiveness of the above unsupervised learning algorithm we ran a number of experiments using two different corpora and part of speech tag sets the Penn Treebank Wall Street Journal Corpus \ -LSB- Marcus et al. 1993 \ -RSB- and the original Brown Corpus \ -LSB- Francis and Kucera 1982 \ -RSB-	num_\_1982 dep_Francis_\ conj_and_Francis_Kucera appos_\_Kucera appos_\_Francis nn_\_Corpus amod_\_Brown amod_\_original det_\_the num_\_1993 appos_Marcus_\ dep_Marcus_al. nn_Marcus_et conj_and_\_\ dep_\_Marcus nn_\_Corpus nn_\_Journal nn_\_Street nn_\_Wall nn_\_Treebank nn_\_Penn det_\_the nn_sets_tag nn_sets_speech prep_of_part_sets conj_and_corpora_part amod_corpora_different num_corpora_two dobj_using_part dobj_using_corpora prep_of_number_experiments det_number_a dep_ran_\ dep_ran_\ xcomp_ran_using dobj_ran_number nsubj_ran_we nsubj_ran_Results nn_algorithm_learning amod_algorithm_unsupervised amod_algorithm_above det_algorithm_the prep_of_effectiveness_algorithm det_effectiveness_the dobj_test_effectiveness aux_test_To vmod_Results_test dep_Learning_ran amod_Learning_Unsupervised
W95-0101	J93-2004	o	\ -LSB- Francis and Kucera 1982 Marcus et al. 1993 \ -RSB- -RRB- training on a corpus of one type and then applying the tagger to a corpus of a different type usually results in a tagger with low accuracy \ -LSB- Weischedel et al. 1993 \ -RSB-	num_\_1993 appos_Weischedel_\ dep_Weischedel_al. nn_Weischedel_et nn_\_accuracy amod_\_low prep_with_tagger_\ det_tagger_a prep_in_results_tagger advmod_results_usually csubj_results_applying amod_type_different det_type_a prep_of_corpus_type det_corpus_a det_tagger_the prep_to_applying_corpus dobj_applying_tagger advmod_applying_then num_type_one prep_of_corpus_type det_corpus_a prep_on_training_corpus num_\_1993 appos_Marcus_\ nn_Marcus_al. nn_Marcus_et dep_Francis_Marcus num_Francis_1982 conj_and_Francis_Kucera dep_\_Weischedel conj_and_\_results conj_and_\_training appos_\_Kucera appos_\_Francis
W95-0101	J93-2004	o	Transformation-based error-driven learning has been applied to a number of natural language problems including part of speech tagging prepositional phrase attachment disambiguation speech generation and syntactic parsing \ -LSB- Brill 1992 Brill 1994 Ramshaw and Marcus 1994 Roche and Schabes 1995 Brill and Resnik 1994 Huang et al. 1994 Brill 1993a Brill 1993b \ -RSB-	num_\_1993b appos_Brill_\ appos_Brill_1993a num_Huang_1994 nn_Huang_al. nn_Huang_et conj_and_Ramshaw_1994 conj_and_Ramshaw_Resnik conj_and_Ramshaw_Brill conj_and_Ramshaw_1995 conj_and_Ramshaw_Schabes conj_and_Ramshaw_Roche conj_and_Ramshaw_1994 conj_and_Ramshaw_Marcus num_Brill_1994 dep_Brill_Brill dep_Brill_Brill dep_Brill_Huang dep_Brill_1994 dep_Brill_Resnik dep_Brill_Brill dep_Brill_1995 dep_Brill_Schabes dep_Brill_Roche dep_Brill_1994 dep_Brill_Marcus dep_Brill_Ramshaw dep_Brill_Brill dep_Brill_1992 nn_\_parsing nn_\_syntactic nn_generation_speech dep_disambiguation_Brill conj_and_disambiguation_\ conj_and_disambiguation_generation nn_disambiguation_attachment nn_disambiguation_phrase amod_disambiguation_prepositional nn_tagging_speech prep_of_part_tagging dep_problems_\ dep_problems_generation dep_problems_disambiguation prep_including_problems_part nn_problems_language amod_problems_natural prep_of_number_problems det_number_a prep_to_applied_number auxpass_applied_been aux_applied_has nsubjpass_applied_learning amod_learning_error-driven amod_learning_Transformation-based
W95-0101	J93-2004	o	Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging \ -LSB- Jelinek 1985 Church 1988 Derose 1988 DeMarcken 1990 Cutting et al. 1992 Kupiec 1992 Charniak et al. 1993 Weischedel et al. 1993 Schutze and Singer 1994 Lin et al. 1994 Elworthy 1994 Merialdo 1995 \ -RSB-	num_\_1995 appos_Merialdo_\ num_Elworthy_1994 num_Lin_1994 nn_Lin_al. nn_Lin_et num_Schutze_1994 conj_and_Schutze_Singer tmod_al._1993 nn_al._et nn_al._Weischedel dep_al._1993 nn_al._et nn_al._Charniak num_Kupiec_1992 conj_al._1992 nn_al._et dobj_Cutting_al. appos_DeMarcken_1990 num_Derose_1988 dep_Church_Merialdo conj_Church_Elworthy conj_Church_Lin conj_Church_Singer conj_Church_Schutze conj_Church_al. conj_Church_al. conj_Church_Kupiec conj_Church_Cutting conj_Church_DeMarcken conj_Church_Derose appos_Church_1988 dep_Jelinek_Church dep_Jelinek_1985 amod_\_tagging nn_\_speech prep_of_part_\ amod_part_based amod_part_Markov-model dep_explored_Jelinek dobj_explored_part aux_explored_has nsubj_explored_all amod_taggers_trained advmod_trained_automatically prep_of_area_taggers det_area_the prep_in_work_area det_work_the prep_of_all_work advmod_all_Almost ccomp_``_explored
W95-0101	J93-2004	o	Below is an example of the initial-state tagging of a sentence from the Penn Treebank \ -LSB- Marcus et al. 1993 \ -RSB- where an underscore is to be read as or	cc_as_or prep_read_as auxpass_read_be aux_read_to xcomp_is_read nsubj_is_underscore advmod_is_where det_underscore_an num_\_1993 appos_Marcus_\ dep_Marcus_al. nn_Marcus_et nn_\_Treebank nn_\_Penn det_\_the prep_from_sentence_\ det_sentence_a prep_of_tagging_sentence amod_tagging_initial-state det_tagging_the rcmod_example_is dep_example_Marcus prep_of_example_tagging det_example_an cop_example_is nsubj_example_Below
W95-0104	J93-2004	o	The performance figures given below are based on training each method on the 1-million-word Brown corpus \ -LSB- Ku ~ era and Francis 1967 \ -RSB- and testing it on a 3/4-million-word corpus of Wall Street Journal text \ -LSB- Marcus et al. 1993 \ -RSB-	num_\_1993 dep_Marcus_\ dep_Marcus_al. nn_Marcus_et dep_text_\ nn_text_Journal nn_text_Street nn_text_Wall prep_of_corpus_text amod_corpus_3/4-million-word det_corpus_a prep_on_testing_corpus dobj_testing_it num_\_1967 conj_and_era_Francis dep_Ku_Marcus conj_and_Ku_testing dep_Ku_\ dep_Ku_Francis dep_Ku_era num_Ku_~ nn_\_corpus amod_\_Brown amod_\_1-million-word det_\_the prep_on_method_\ det_method_each nn_method_training dep_based_testing dep_based_Ku prep_on_based_method auxpass_based_are nsubjpass_based_figures advmod_given_below vmod_figures_given nn_figures_performance det_figures_The
W95-0105	J93-2004	o	-LRB- 1993 -RRB- found that direct annotation takes twice as long as automatic tagging plus correction for partof-speech annotation -RRB- and the output quality reflects the difficulty of the task -LRB- inter-annotator disagreement is on the order of 10 % as contrasted with the approximately 3 % error rate reported for part-of-speech annotation by Marcus et al. -RRB-	nn_al._et nn_al._Marcus amod_annotation_part-of-speech agent_reported_al. prep_for_reported_annotation vmod_rate_reported nn_rate_error amod_rate_% det_rate_the number_%_3 quantmod_3_approximately prep_with_contrasted_rate mark_contrasted_as num_%_10 prep_of_order_% det_order_the prep_on_is_order dep_disagreement_contrasted dep_disagreement_is nn_disagreement_inter-annotator dep_task_disagreement det_task_the prep_of_difficulty_task det_difficulty_the dobj_reflects_difficulty nsubj_reflects_quality nn_quality_output det_quality_the amod_annotation_partof-speech conj_plus_tagging_correction amod_tagging_automatic prep_for_long_annotation prep_as_long_correction prep_as_long_tagging advmod_long_as advmod_long_twice dep_takes_long nsubj_takes_annotation mark_takes_that amod_annotation_direct conj_and_found_reflects ccomp_found_takes dep_found_1993
W95-0105	J93-2004	o	The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes -LRB- Lee et al. 1993 Rada et al. 1989 -RRB- also captures this albeit indirectly when the semantic network is just an IS-A hierarchy if the minimal path of IS-A links between two nodes is long that means it is necessary to go high in the taxonomy to more abstract concepts in order to find their least upper bound	dep_upper_bound amod_upper_least poss_upper_their dep_find_upper aux_find_to dep_find_order mark_find_in amod_concepts_abstract amod_concepts_more det_taxonomy_the prep_in_go_taxonomy acomp_go_high aux_go_to xcomp_necessary_go cop_necessary_is nsubj_necessary_it advcl_means_find prep_to_means_concepts ccomp_means_necessary nsubj_means_that advcl_means_long cop_long_is nsubj_long_path mark_long_if num_nodes_two prep_between_links_nodes nn_links_IS-A prep_of_path_links amod_path_minimal det_path_the ccomp_hierarchy_means amod_hierarchy_IS-A det_hierarchy_an advmod_hierarchy_just cop_hierarchy_is nsubj_hierarchy_network advmod_hierarchy_when amod_network_semantic det_network_the advcl_indirectly_hierarchy pcomp_albeit_indirectly prep_captures_albeit dobj_captures_this advmod_captures_also nsubj_captures_method num_Rada_1989 nn_Rada_al. nn_Rada_et dep_Lee_Rada num_Lee_1993 dep_Lee_al. nn_Lee_et num_nodes_two prep_between_length_nodes nn_length_path det_length_the dobj_measuring_length amod_network_semantic det_network_a prep_in_similarity_network prepc_by_evaluating_measuring dobj_evaluating_similarity dep_method_Lee prepc_of_method_evaluating amod_method_traditional det_method_The ccomp_``_captures
W95-0105	J93-2004	o	-LRB- Bensch and Savitch 1992 Brill 1991 Brown et al. 1992 Grefenstette 1994 McKcown and Hatzivassiloglou 1993 Pereira et al. 1993 Schtltze 1993 -RRB- -RRB-	dep_Schtltze_1993 num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_McKcown_1993 conj_and_McKcown_Hatzivassiloglou num_Grefenstette_1994 num_Brown_1992 nn_Brown_al. nn_Brown_et num_Brill_1991 dep_Bensch_Schtltze dep_Bensch_Pereira dep_Bensch_Hatzivassiloglou dep_Bensch_McKcown dep_Bensch_Grefenstette dep_Bensch_Brown dep_Bensch_Brill dep_Bensch_1992 conj_and_Bensch_Savitch dep_''_Savitch dep_''_Bensch
W95-0112	J93-2004	o	Furthermore training corpora for information extraction are typically annotated with domain-specific tags in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing -LRB- e.g. the Brown Corpus \ -LSB- Francis and Kucera 1982 \ -RSB- and the Penn Treebank \ -LSB- Marcus et al. 1993 \ -RSB- -RRB-	num_\_1993 appos_Marcus_\ dep_Marcus_al. nn_Marcus_et nn_\_Treebank nn_\_Penn det_\_the num_\_1982 dep_Francis_\ conj_and_Francis_Kucera dep_\_Marcus conj_and_\_\ dep_\_Kucera dep_\_Francis nn_\_Corpus amod_\_Brown det_\_the dep_e.g._\ dep_e.g._\ dep_bracketing_e.g. amod_bracketing_noun-phrase conj_or_tags_bracketing amod_tags_part-of-speech prep_such_as_annotations_bracketing prep_such_as_annotations_tags amod_annotations_general-purpose prep_to_contrast_annotations amod_tags_domain-specific prep_in_annotated_contrast prep_with_annotated_tags advmod_annotated_typically cop_annotated_are nsubj_annotated_corpora advmod_annotated_Furthermore nn_extraction_information prep_for_corpora_extraction nn_corpora_training
W95-0112	J93-2004	o	General purpose text annotations such as part-of-speech tags and noun-phrase bracketing are costly to obtain but have wide applicability and have been used successfully to develop statistical NLP systems -LRB- e.g. \ -LSB- Church 1989 Weischedel et al. 1993 \ -RSB- -RRB-	num_\_1993 nn_al._et nn_al._Weischedel appos_Church_\ dep_Church_al. dep_Church_1989 dep_\_Church dep_e.g._\ dep_systems_e.g. nn_systems_NLP amod_systems_statistical dobj_develop_systems aux_develop_to xcomp_used_develop advmod_used_successfully auxpass_used_been aux_used_have nsubjpass_used_annotations amod_applicability_wide dobj_have_applicability conj_but_obtain_have aux_obtain_to conj_and_costly_used xcomp_costly_have xcomp_costly_obtain cop_costly_are nsubj_costly_annotations amod_bracketing_noun-phrase conj_and_tags_bracketing amod_tags_part-of-speech prep_such_as_annotations_bracketing prep_such_as_annotations_tags nn_annotations_text nn_annotations_purpose nn_annotations_General
W96-0111	J93-2004	o	In previous work we tested the DOP method on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- achieving excellent test results -LRB- Bod 1993a b -RRB-	appos_Bod_b appos_Bod_1993a dep_results_Bod nn_results_test amod_results_excellent dobj_achieving_results amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_strings_Treebank amod_strings_part-of-speech amod_strings_analyzed prep_of_set_strings amod_set_cleaned-up det_set_a nn_method_DOP det_method_the vmod_tested_achieving dep_tested_Marcus prep_on_tested_set dobj_tested_method nsubj_tested_we prep_in_tested_work amod_work_previous
W96-0111	J93-2004	o	The latter approach has become increasingly popular -LRB- e.g. Schabes et al. 1993 Weischedel et al. 1993 Briscoe 1994 Magerman 1995 Collins 1996 -RRB-	amod_Collins_1996 dep_Magerman_Collins dep_Magerman_1995 dep_Briscoe_Magerman num_Briscoe_1994 nn_al._et nn_al._Weischedel nn_al._et nn_al._Schabes dep_e.g._Briscoe dep_e.g._1993 dep_e.g._al. dep_e.g._1993 dep_e.g._al. advmod_popular_increasingly dep_become_e.g. acomp_become_popular aux_become_has nsubj_become_approach amod_approach_latter det_approach_The ccomp_``_become
W96-0111	J93-2004	o	To deal with this question we use ATIS p-o-s trees as found in the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_in_found_Treebank mark_found_as nn_trees_p-o-s nn_trees_ATIS dep_use_Marcus advcl_use_found dobj_use_trees nsubj_use_we advcl_use_deal det_question_this prep_with_deal_question aux_deal_To
W96-0112	J93-2004	o	1993 Chang et al. 1992 Collins and Brooks 1995 Fujisaki 1989 Hindle and Rooth 1991 Hindle and Rooth 1993 Jelinek et al. 1990 Magerman and Marcus 1991 Magerman 1995 Ratnaparkhi et al. 1994 Resnik 1993 Su and Chang 1988 -RRB-	amod_Su_1988 conj_and_Su_Chang num_Resnik_1993 num_Ratnaparkhi_1994 nn_Ratnaparkhi_al. nn_Ratnaparkhi_et num_Magerman_1995 num_Magerman_1991 conj_and_Magerman_Marcus num_Jelinek_1990 nn_Jelinek_al. nn_Jelinek_et num_Hindle_1993 conj_and_Hindle_Rooth dep_1991_Chang dep_1991_Su conj_1991_Resnik conj_1991_Ratnaparkhi conj_1991_Magerman conj_1991_Marcus conj_1991_Magerman conj_1991_Jelinek conj_1991_Rooth conj_1991_Hindle dep_1991_Chang number_1991_1993 conj_and_Hindle_Rooth num_Fujisaki_1989 num_Collins_1995 conj_and_Collins_Brooks dep_Chang_Rooth dep_Chang_Hindle dep_Chang_Fujisaki dep_Chang_Brooks dep_Chang_Collins num_Chang_1992 dep_Chang_al. nn_Chang_et advcl_``_1991
W96-0112	J93-2004	o	We extracted 181,250 case frames from the WSJ -LRB- Wall Street Journal -RRB- bracketed corpus of the Penn Tree Bank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Bank_Tree nn_Bank_Penn det_Bank_the prep_of_corpus_Bank amod_corpus_bracketed nn_corpus_WSJ nn_Journal_Street nn_Journal_Wall appos_WSJ_Journal det_WSJ_the nn_frames_case num_frames_181,250 dep_extracted_Marcus prep_from_extracted_corpus dobj_extracted_frames nsubj_extracted_We ccomp_``_extracted
W96-0112	J93-2004	o	For subproblem -LRB- a -RRB- we have devised a new method based on LPR which has some good properties not shared by the methods proposed so far -LRB- Alshawi and Carter 1995 Chang et al. 1992 Collins and Brooks 1995 Hindle and Rooth 1991 Ratnaparkhi et al. 1994 Resnik 1993 -RRB-	amod_Resnik_1993 num_Ratnaparkhi_1994 nn_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_Hindle_Resnik conj_and_Hindle_Ratnaparkhi conj_and_Hindle_1991 conj_and_Hindle_Rooth num_Collins_1995 conj_and_Collins_Brooks num_Chang_1992 nn_Chang_al. nn_Chang_et dep_Alshawi_Ratnaparkhi dep_Alshawi_1991 dep_Alshawi_Rooth dep_Alshawi_Hindle conj_and_Alshawi_Brooks conj_and_Alshawi_Collins conj_and_Alshawi_Chang conj_and_Alshawi_1995 conj_and_Alshawi_Carter dep_far_Collins dep_far_Chang dep_far_1995 dep_far_Carter dep_far_Alshawi advmod_far_so advmod_proposed_far vmod_methods_proposed det_methods_the agent_shared_methods neg_shared_not vmod_properties_shared amod_properties_good det_properties_some dobj_has_properties nsubj_has_which rcmod_LPR_has prep_on_based_LPR vmod_method_based amod_method_new det_method_a dobj_devised_method aux_devised_have nsubj_devised_we prep_for_devised_subproblem appos_subproblem_a
W96-0203	J93-2004	o	Clusters are created by means of distributional techniques in -LRB- Ratnaparkhi et al 1994 -RRB- while in -LRB- Resnik and Hearst 1993 -RRB- low level synonim sets in WordNet are used	auxpass_used_are nsubjpass_used_in mark_used_while nn_sets_synonim nn_sets_level amod_sets_low dep_sets_Hearst dep_sets_Resnik num_Resnik_1993 conj_and_Resnik_Hearst prep_in_in_WordNet pobj_in_sets amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al nn_Ratnaparkhi_et dep_in_Ratnaparkhi prep_techniques_in amod_techniques_distributional advcl_created_used prep_by_means_of_created_techniques auxpass_created_are nsubjpass_created_Clusters
W96-0203	J93-2004	o	This method is described hereafter while the subsequent steps that use deeper -LRB- rulebased -RRB- levels of knowledge are implemented into the ARIOSTO_LEX lexical learning system described in -LRB- Basili et al. 1993b 1933c and 1996 -RRB-	conj_and_1933c_1996 appos_Basili_1996 appos_Basili_1933c appos_Basili_1993b dep_Basili_al. nn_Basili_et prep_in_described_Basili nn_system_learning amod_system_lexical nn_system_ARIOSTO_LEX det_system_the dep_implemented_described prep_into_implemented_system auxpass_implemented_are nsubjpass_implemented_steps mark_implemented_while prep_of_levels_knowledge amod_levels_deeper dep_deeper_rulebased dobj_use_levels nsubj_use_that rcmod_steps_use amod_steps_subsequent det_steps_the advcl_described_implemented advmod_described_hereafter auxpass_described_is nsubjpass_described_method det_method_This ccomp_``_described
W96-0203	J93-2004	o	The class based disambiguation operator is the Mutual Conditioned Plausibility -LRB- MCPI -RRB- -LRB- Basili et al. ,1993 a -RRB-	num_al._,1993 dep_Basili_a dep_Basili_al. nn_Basili_et appos_Plausibility_Basili appos_Plausibility_MCPI amod_Plausibility_Conditioned amod_Plausibility_Mutual det_Plausibility_the cop_Plausibility_is nsubj_Plausibility_operator nn_operator_disambiguation amod_operator_based nn_operator_class det_operator_The
W96-0203	J93-2004	o	In general the training set is the parsed Wall Street Journal -LRB- Marcus et al 1993 -RRB- with few exceptions and the size of the training samples is around 10-20 ,000 test cases	nn_cases_test num_cases_,000 number_,000_10-20 prep_around_is_cases nsubj_is_size nn_samples_training det_samples_the prep_of_size_samples det_size_the amod_exceptions_few amod_Marcus_1993 dep_Marcus_al nn_Marcus_et conj_and_Journal_is prep_with_Journal_exceptions dep_Journal_Marcus nn_Journal_Street nn_Journal_Wall amod_Journal_parsed det_Journal_the cop_Journal_is nsubj_Journal_set prep_in_Journal_general nn_set_training det_set_the
W96-0203	J93-2004	o	This incremental process can be iterated to the point that the system 1 It is not just a matter of time but also of required linguistic skills -LRB- see for example -LRB- Marcus et al 1993 -RRB- -RRB-	amod_Marcus_1993 dep_Marcus_al nn_Marcus_et dep_see_Marcus prep_for_see_example amod_skills_linguistic amod_skills_required prep_of_also_skills prep_of_matter_time det_matter_a advmod_matter_just neg_matter_not cop_matter_is nsubj_matter_It dep_matter_1 dep_system_see advmod_system_also cc_system_but dep_system_matter det_system_the prep_that_point_system det_point_the prep_to_iterated_point auxpass_iterated_be aux_iterated_can nsubjpass_iterated_process amod_process_incremental det_process_This
W96-0203	J93-2004	o	These later inductive phases may rely on some level of a priori knowledge like for example the naive case relations used in the ARIOSTO_LEX system -LRB- Basili et al 1993c 1996 -RRB-	dep_Basili_1996 appos_Basili_1993c dep_Basili_al nn_Basili_et nn_system_ARIOSTO_LEX det_system_the prep_in_used_system dep_relations_Basili vmod_relations_used nn_relations_case amod_relations_naive det_relations_the pobj_for_example pcomp_like_for nn_knowledge_priori det_knowledge_a prep_of_level_knowledge det_level_some dobj_rely_relations prep_rely_like prep_on_rely_level aux_rely_may nsubj_rely_phases amod_phases_inductive amod_phases_later det_phases_These
W96-0203	J93-2004	o	To simplify the plausibility of a detected esl is roughly inversely proportional to the number of mutually excluding syntactic structures in the text segment that generated the esl -LRB- see -LRB- Basili et al 1993a -RRB- for details -RRB-	appos_Basili_1993a dep_Basili_al nn_Basili_et prep_for_see_details dep_see_Basili det_esl_the dobj_generated_esl nsubj_generated_that rcmod_segment_generated nn_segment_text det_segment_the dep_structures_see prep_in_structures_segment amod_structures_syntactic dobj_excluding_structures advmod_excluding_mutually prepc_of_number_excluding det_number_the prep_to_proportional_number advmod_proportional_inversely advmod_proportional_roughly cop_proportional_is nsubj_proportional_plausibility advcl_proportional_simplify amod_esl_detected det_esl_a prep_of_plausibility_esl det_plausibility_the aux_simplify_To ccomp_``_proportional
W96-0208	J93-2004	o	Learning to Disambiguate Word Senses Several recent research projects have taken a corpus-based approach to lexical disambiguation -LRB- Brown Della-Pietra Della-Pietra & Mercer 1991 Gale Church & Yarowsky 1992b Leacock et al. 1993b Lehman 1994 -RRB-	amod_Lehman_1994 appos_Leacock_1993b dep_Leacock_al. nn_Leacock_et num_Gale_1992b conj_and_Gale_Yarowsky appos_Gale_Church dep_Brown_Lehman conj_and_Brown_Leacock conj_and_Brown_Yarowsky conj_and_Brown_Gale conj_and_Brown_1991 conj_and_Brown_Mercer conj_and_Brown_Della-Pietra conj_and_Brown_Della-Pietra dep_disambiguation_Leacock dep_disambiguation_Gale dep_disambiguation_1991 dep_disambiguation_Mercer dep_disambiguation_Della-Pietra dep_disambiguation_Della-Pietra dep_disambiguation_Brown amod_disambiguation_lexical prep_to_approach_disambiguation amod_approach_corpus-based det_approach_a dobj_taken_approach aux_taken_have nn_projects_research amod_projects_recent amod_projects_Several vmod_Senses_taken dobj_Senses_projects nsubj_Senses_Learning nn_Word_Disambiguate prep_to_Learning_Word ccomp_``_Senses
W96-0213	J93-2004	o	~ gtPdl = | & allm ~ WI.Lqlf IDW t ~ lIO r I ~ 1 ~ ~ ~ II Mlmulm IP il ~ lllb l ~ ~ I I I I I I I I I 0 200 400 600 800 1000 1200 1400 1600 1800 Article # 2000 Figure 1 Distribution of Tags for the word about vs. Article # Training Size -LRB- wrds -RRB- I Test571190 Size -LRB- wrds -RRB- I Baseline44478 97.04 % Specialized 197.13 % Table 10 Performance of Baseline ~ Specialized Model When Tested on Consistent Subset of Development Set 139 POS Tag 35 30 25 2O 15 10 5 0 1 I o. Oho m I I I B ~ m M I I I 2 3 4 Annotator Figure 2 Distribution of Tags for the word about vs. Annotator -LRB- Weischedel et al. 1993 -RRB- provide the results from a battery of tri-tag Markov Model experiments in which the probability P -LRB- W T -RRB- of observing a word sequence W = -LCB- wl w2 wn -RCB- together with a tag sequence T = -LCB- tl t2 tn -RCB- is given by P -LRB- TIW -RRB- P -LRB- W -RRB- p -LRB- tl -RRB- p -LRB- t21tl -RRB- H P -LRB- tilti-lti-2 -RRB- p -LRB- wilti i = 3 Furthermore p -LRB- wilti -RRB- for unknown words is computed by the following heuristic which uses a set of 35 pre-determined endings p -LRB- wilti -RRB- p -LRB- unknownwordlti -RRB- x p -LRB- capitalfeature \ -LSB- ti -RRB- x p -LRB- endings hypenationlti -RRB- This approximation works as well as the MaxEnt model giving 85 % unknown word accuracy -LRB- Weischedel et al. 1993 -RRB- on the Wall St. Journal but can not be generalized to handle more diverse information sources	nn_sources_information amod_sources_diverse amod_sources_more dobj_handle_sources aux_handle_to xcomp_generalized_handle auxpass_generalized_be neg_generalized_not aux_generalized_can nn_Journal_St. nn_Journal_Wall det_Journal_the amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et nn_accuracy_word amod_accuracy_unknown npadvmod_unknown_% num_%_85 prep_on_giving_Journal dep_giving_Weischedel dobj_giving_accuracy nn_model_MaxEnt det_model_the nsubj_works_approximation det_approximation_This appos_endings_hypenationlti dep_p_endings conj_and_\_model dep_\_works conj_x_\_p appos_\_ti nn_\_capitalfeature nn_\_p conj_but_p_generalized vmod_p_giving conj_x_p_model conj_x_p_p conj_x_p_\ appos_p_unknownwordlti nn_p_p appos_p_wilti amod_endings_pre-determined num_endings_35 prep_of_set_endings det_set_a dobj_uses_set nsubj_uses_which dep_heuristic_generalized dep_heuristic_\ dep_heuristic_p rcmod_heuristic_uses amod_heuristic_following det_heuristic_the agent_computed_heuristic auxpass_computed_is nsubjpass_computed_wilti amod_words_unknown prep_for_p_words appos_p_wilti advmod_3_Furthermore dep_=_3 advmod_=_i dep_wilti_p amod_wilti_= nn_p_P appos_P_tilti-lti-2 nn_P_H nn_P_p appos_p_t21tl nn_p_p appos_p_tl dep_P_computed dep_P_p appos_P_W nn_P_P appos_P_TIW dep_by_P prep_given_by auxpass_given_is nsubjpass_given_P prep_in_given_which appos_tl_tn conj_tl_t2 dep_=_tl amod_T_= nn_T_sequence nn_T_tag det_T_a appos_wl_wn conj_wl_w2 dep_=_wl advmod_W_together amod_W_= nn_W_sequence nn_W_word det_W_a prep_with_observing_T dobj_observing_W appos_W_T prepc_of_P_observing dep_P_W nn_P_probability det_P_the nn_experiments_Model nn_experiments_Markov nn_experiments_tri-tag rcmod_battery_given prep_of_battery_experiments det_battery_a prep_from_results_battery det_results_the dobj_provide_results amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et dep_Annotator_Weischedel pobj_vs._Annotator advmod_vs._about conj_word_provide prep_word_vs. det_word_the prep_for_Distribution_word prep_of_Distribution_Tags num_Figure_2 nn_Figure_Annotator dep_Figure_4 dep_Figure_I dep_Figure_I dep_Figure_I dep_Figure_M dep_Figure_m dep_Figure_~ dep_Figure_B dep_Figure_I dep_Figure_I dep_Figure_I dep_Figure_m dep_Figure_Oho dep_4_3 number_3_2 dep_o._Figure nsubj_o._I dep_o._1 dep_o._0 dep_o._2O dep_o._25 dep_o._30 nsubj_o._Tag dep_o._POS number_0_5 dep_0_10 number_10_15 number_30_35 rcmod_139_o. dobj_Set_139 prep_of_Subset_Development amod_Subset_Consistent dep_Tested_Set prep_on_Tested_Subset advmod_Tested_When nn_Model_Specialized nn_Model_~ nn_Model_Baseline dep_Performance_Distribution rcmod_Performance_Tested prep_of_Performance_Model dep_Table_Performance num_Table_10 nn_Table_% num_Table_197.13 nn_Table_Specialized dep_Table_% number_Table_97.04 dobj_Baseline44478_Table nsubj_Baseline44478_I rcmod_Size_Baseline44478 appos_Size_wrds nn_Size_Test571190 dep_I_Size dep_Size_I appos_Size_wrds dobj_Training_Size dep_Training_# vmod_Article_Training conj_vs._about_Article prep_word_Article prep_word_about det_word_the prep_for_Distribution_word prep_of_Distribution_Tags dep_Figure_Distribution num_Figure_1 num_Figure_2000 dep_#_Figure dep_Article_# dep_1800_Article vmod_1000 1200 1400 1600_1800 dep_1000 1200 1400 1600_200 400 600 800 number_200 400 600 800_0 amod_I_1000 1200 1400 1600 dep_I_I dep_I_I dep_I_I dep_I_I dep_I_I dep_I_I dep_I_I dep_I_I dep_~_I dep_~_~ dep_l_~ nn_~_il appos_~_l appos_~_lllb appos_~_~ appos_~_IP appos_~_Mlmulm num_~_II nn_~_~ num_~_~ dep_1_~ ccomp_``_1 nsubj_~_I nn_I_r num_lIO_~ nn_lIO_t dep_IDW_~ appos_IDW_lIO nn_IDW_WI.Lqlf nn_IDW_~ nn_IDW_allm conj_and_|_IDW amod_|_= nn_|_gtPdl dep_|_~
W96-0213	J93-2004	o	Previous uses of this model include language modeling -LRB- Lau et al. 1993 -RRB- machine translation -LRB- Berger et al. 1996 -RRB- prepositional phrase attachment -LRB- Ratnaparkhi et al. 1994 -RRB- and word morphology -LRB- Della Pietra et al. 1995 -RRB-	amod_Pietra_1995 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della dep_morphology_Pietra nn_morphology_word amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et appos_attachment_Ratnaparkhi nn_attachment_phrase amod_attachment_prepositional amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_translation_Berger nn_translation_machine amod_Lau_1993 dep_Lau_al. nn_Lau_et conj_and_modeling_morphology conj_and_modeling_attachment conj_and_modeling_translation dep_modeling_Lau nn_modeling_language dobj_include_morphology dobj_include_attachment dobj_include_translation dobj_include_modeling nsubj_include_uses det_model_this prep_of_uses_model amod_uses_Previous
W96-0213	J93-2004	o	In practice 7 / is very large and the model 's expectation Efj can not be computed directly so the following approximation -LRB- Lau et al. 1993 -RRB- is used n E fj ~ E15 -LRB- hi -RRB- p -LRB- tilhi -RRB- fj -LRB- hi ti -RRB- i = 1 where fi -LRB- hi -RRB- is the observed probability of the history hi in the training set	nn_set_training det_set_the det_history_the prep_of_probability_history amod_probability_observed det_probability_the cop_probability_is nsubj_probability_fi advmod_probability_where appos_fi_hi rcmod_1_probability prep_in_=_set advmod_=_hi dobj_=_1 nsubj_=_i dep_=_hi dep_hi_ti dep_fj_= nn_fj_p appos_p_tilhi nn_p_~ appos_~_hi num_~_E15 nn_fj_E nn_fj_n dep_fj_used auxpass_used_is nsubjpass_used_approximation mark_used_so amod_Lau_1993 dep_Lau_al. nn_Lau_et dep_approximation_Lau amod_approximation_following det_approximation_the advmod_computed_directly auxpass_computed_be neg_computed_not aux_computed_can nsubjpass_computed_Efj nn_Efj_expectation poss_Efj_model det_model_the conj_and_large_fj conj_and_large_fj conj_and_large_computed advmod_large_very cop_large_is prep_in_large_practice dep_is_7
W96-0213	J93-2004	o	Comparison With Previous Work Most of the recent corpus-based POS taggers in the literature are either statistically based and use Markov Model -LRB- Weischedel et al. 1993 Merialdo 1994 -RRB- or Statistical Decision Tree -LRB- Jelinek et al. 1994 Magerman 1995 -RRB- -LRB- SDT -RRB- techniques or are primarily rule based such as Drill 's Transformation Based Learner -LRB- Drill 1994 -RRB- -LRB- TBL -RRB-	dep_Drill_1994 appos_Learner_TBL dep_Learner_Drill dobj_Based_Learner nsubj_Based_Transformation mark_Based_as poss_Transformation_Drill mwe_as_such advcl_based_Based vmod_rule_based advmod_rule_primarily cop_rule_are conj_or_techniques_rule appos_techniques_SDT nn_techniques_Tree nn_techniques_use nn_techniques_Comparison dep_Jelinek_1995 appos_Jelinek_Magerman amod_Jelinek_1994 dep_Jelinek_al. nn_Jelinek_et nn_Tree_Decision amod_Tree_Statistical dep_Weischedel_1994 appos_Weischedel_Merialdo amod_Weischedel_1993 dep_Weischedel_al. nn_Weischedel_et nn_Model_Markov dobj_use_Model advmod_based_statistically auxpass_based_are nsubjpass_based_Most preconj_statistically_either det_literature_the prep_in_taggers_literature nn_taggers_POS amod_taggers_corpus-based amod_taggers_recent det_taggers_the prep_of_Most_taggers rcmod_Work_based amod_Work_Previous dep_Comparison_Jelinek conj_or_Comparison_Tree dep_Comparison_Weischedel conj_and_Comparison_use prep_with_Comparison_Work
W97-0105	J93-2004	o	Slrs Parse Base -LRB- Black et al. 1993a -RRB- is 1.76	cop_1.76_is nsubj_1.76_Parse appos_al._1993a nn_al._et amod_al._Black dep_Base_al. dobj_Parse_Base rcmod_Slrs_1.76
W97-0105	J93-2004	o	In all other respects our work departs from previous research on broad -- coverage 16 I t I I I I I i I i I I I I I I I I I I I i I 1 I. I I I I i I 1 I I I I probabilistic parsing which either attempts to learn to predict gr ~ rarn ~ tical structure of test data directly from a training treebank -LRB- Brill 1993 Collins 1996 Eisner 1996 Jelinek et al. 1994 Magerman 1995 S ~ kine and Orishman 1995 Sharman et al. 1990 -RRB- or employs a grammar and sometimes a dictionary to capture linguistic expertise directly -LRB- Black et al. 1993a GrinBerg et al. 1995 Schabes 1992 -RRB- but arguably at a less detailed and informative level than in the research reported here	advmod_reported_here vmod_research_reported det_research_the pobj_in_research pcomp_than_in prep_level_than amod_level_informative amod_level_detailed det_level_a conj_and_detailed_informative advmod_detailed_less pobj_at_level advmod_at_arguably dep_Schabes_1992 num_al._1995 nn_al._et nn_al._GrinBerg dep_al._Schabes dep_al._al. appos_al._1993a nn_al._et amod_al._Black amod_expertise_linguistic dep_capture_al. advmod_capture_directly dobj_capture_expertise aux_capture_to vmod_dictionary_capture det_dictionary_a advmod_dictionary_sometimes conj_and_grammar_dictionary det_grammar_a dobj_employs_dictionary dobj_employs_grammar nsubj_employs_work num_Sharman_1990 nn_Sharman_al. nn_Sharman_et conj_and_kine_Orishman nn_kine_~ nn_kine_S dep_Magerman_1995 num_Jelinek_1994 nn_Jelinek_al. nn_Jelinek_et num_Eisner_1996 num_Collins_1996 dep_Brill_Sharman dep_Brill_1995 dep_Brill_Orishman dep_Brill_kine dep_Brill_Magerman dep_Brill_Jelinek dep_Brill_Eisner dep_Brill_Collins dep_Brill_1993 appos_treebank_Brill nn_treebank_training det_treebank_a nn_data_test prep_of_structure_data amod_structure_tical nn_structure_~ nn_structure_rarn nn_structure_~ nn_structure_gr prep_from_predict_treebank advmod_predict_directly dobj_predict_structure aux_predict_to xcomp_learn_predict aux_learn_to xcomp_attempts_learn preconj_attempts_either nsubj_attempts_which rcmod_parsing_attempts amod_parsing_probabilistic dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_1 dep_parsing_I dep_parsing_i dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I nn_parsing_I. dep_parsing_I nn_parsing_i dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I dep_parsing_I nn_parsing_i dep_parsing_I dep_parsing_coverage amod_parsing_broad num_I_1 dep_i_I dep_i_I dep_i_I dep_i_I dep_i_I dep_i_t dep_i_I dep_i_16 dep_coverage_i prep_on_research_parsing amod_research_previous conj_but_departs_at conj_or_departs_employs prep_from_departs_research nsubj_departs_work prep_in_departs_respects poss_work_our amod_respects_other det_respects_all
W97-0105	J93-2004	o	Clearly the present research task is quite considerably harder than the parsing and tagging tasks undertaken in -LRB- Jelinek et al. 1994 Magerman 1995 Black et al. 1993b -RRB- which would seem to be the closest work to ours and any comparison between this work and ours must be approached with extreme caution	amod_caution_extreme prep_with_approached_caution auxpass_approached_be aux_approached_must nsubjpass_approached_ours nsubjpass_approached_comparison nsubjpass_approached_Magerman det_work_this prep_between_comparison_work det_comparison_any prep_to_work_ours amod_work_closest det_work_the cop_work_be aux_work_to xcomp_seem_work aux_seem_would nsubj_seem_which appos_Black_1993b dep_Black_al. nn_Black_et conj_and_Magerman_ours conj_and_Magerman_comparison rcmod_Magerman_seem dep_Magerman_Black num_Magerman_1995 dep_Jelinek_approached appos_Jelinek_1994 dep_Jelinek_al. nn_Jelinek_et prep_in_undertaken_Jelinek amod_tasks_tagging vmod_parsing_undertaken conj_and_parsing_tasks det_parsing_the prep_than_harder_tasks prep_than_harder_parsing advmod_harder_considerably advmod_harder_quite cop_harder_is nsubj_harder_task advmod_harder_Clearly nn_task_research amod_task_present det_task_the ccomp_``_harder
W97-0105	J93-2004	o	Table 3 shows the differences between the treebank ~ utilized in -LRB- Jelinek et al. 1994 -RRB- on the one hand and in the work reported here on the other is Table 4 shows relevant lSFigures for Average Sentence Length -LRB- ` l ~ raLuing Corpus -RRB- and Training Set Size for the IBM ManuaLs Corpus are approximate and cz ~ e fzom -LRB- Black et aL 1993a -RRB-	nn_1993a_aL amod_1993a_Black advmod_1993a_fzom nn_1993a_~ nn_aL_et dep_fzom_e nn_~_cz conj_and_approximate_1993a cop_approximate_are nsubj_approximate_shows nn_Corpus_ManuaLs nn_Corpus_IBM det_Corpus_the dobj_Set_Size dep_Training_Set nn_Corpus_raLuing nn_Corpus_~ nn_Corpus_l nn_Length_Sentence amod_Length_Average conj_and_lSFigures_Training appos_lSFigures_Corpus prep_for_lSFigures_Length amod_lSFigures_relevant dep_shows_Training dep_shows_lSFigures num_shows_4 dep_Table_shows cop_Table_is nsubj_Table_in nsubj_Table_Jelinek mark_Table_in det_other_the advmod_reported_here vmod_work_reported det_work_the pobj_in_work num_hand_one det_hand_the prep_on_Jelinek_other conj_and_Jelinek_in prep_on_Jelinek_hand amod_Jelinek_1994 dep_Jelinek_al. nn_Jelinek_et advcl_utilized_Table vmod_~_utilized nn_~_treebank det_~_the prep_between_differences_~ det_differences_the prep_for_shows_Corpus dobj_shows_differences nsubj_shows_Table num_Table_3
W97-0105	J93-2004	o	By labelling Treeb ~ n ~ nodes with Gr ~ ramar rule names and not with phrasal and clausal n ~ raes as in other -LRB- non-gr ~ rarnar-based -RRB- treebanks ' -LRB- Eyes and Leech 1993 Garside and McEnery 1993 Marcus et al. 1993 -RRB- we gain access to all information provided by the Grammar regarding each ~ reebank node	nn_node_reebank nn_node_~ det_node_each prep_regarding_Grammar_node det_Grammar_the agent_provided_Grammar vmod_information_provided det_information_all prep_to_access_information dobj_gain_access nsubj_gain_we num_Marcus_1993 nn_Marcus_al. nn_Marcus_et num_Garside_1993 conj_and_Garside_McEnery dep_Eyes_Marcus conj_and_Eyes_McEnery conj_and_Eyes_Garside conj_and_Eyes_1993 conj_and_Eyes_Leech rcmod_treebanks_gain dep_treebanks_Garside dep_treebanks_1993 dep_treebanks_Leech dep_treebanks_Eyes possessive_treebanks_' amod_treebanks_rarnar-based nn_treebanks_~ amod_treebanks_non-gr dep_other_treebanks pobj_in_other pcomp_as_in amod_raes_~ nn_raes_n amod_raes_clausal amod_raes_phrasal conj_and_phrasal_clausal prep_not_as prep_with_not_raes cc_not_and nn_names_rule nn_names_ramar nn_names_~ nn_names_Gr prep_with_nodes_names amod_nodes_~ dep_n_nodes nn_n_~ nn_n_Treeb dobj_labelling_n dep_By_not pcomp_By_labelling
W97-0105	J93-2004	o	For example the feature 1 On the ATR English Grammar see below for a detailed description of a precursor to the Gr-r ~ raar see -LRB- Black et al. 1993a -RRB-	nn_al._et dep_Black_1993a dep_Black_al. dep_see_Black prep_for_see_description nn_raar_~ nn_raar_Gr-r det_raar_the prep_to_precursor_raar det_precursor_a prep_of_description_precursor amod_description_detailed det_description_a parataxis_see_see advmod_see_below nsubj_see_feature prep_for_see_example nn_Grammar_English nn_Grammar_ATR det_Grammar_the prep_on_feature_Grammar num_feature_1 det_feature_the
W97-0121	J93-2004	o	We collected training samples from the Brown Corpus distributed with the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	dep_1993_al. nn_al._et num_Marcus_1993 nn_Treebank_Penn det_Treebank_the prep_with_distributed_Treebank amod_Corpus_Brown det_Corpus_the vmod_samples_distributed prep_from_samples_Corpus nn_samples_training dep_collected_Marcus dobj_collected_samples nsubj_collected_We
W97-0201	J93-2004	o	3 The Effect of Training Corpus Size A number of past research work on WSD such as -LRB- Leacock et al. 1993 Bruce and Wiebe 1994 Mooney 1996 -RRB- were tested on a small number of words like line and interest	dep_like_line prep_of_number_words amod_number_small det_number_a conj_and_tested_interest prep_tested_like prep_on_tested_number auxpass_tested_were nsubjpass_tested_Effect dep_Mooney_1996 num_Bruce_1994 conj_and_Bruce_Wiebe dep_Leacock_Mooney conj_Leacock_Wiebe conj_Leacock_Bruce amod_Leacock_1993 dep_Leacock_al. nn_Leacock_et dep_as_Leacock mwe_as_such prep_WSD_as nn_work_research amod_work_past prep_of_number_work det_number_A nn_number_Size nn_number_Corpus amod_number_Training prep_on_Effect_WSD prep_of_Effect_number det_Effect_The num_Effect_3 ccomp_``_interest ccomp_``_tested
W97-0202	J93-2004	o	edu Abstract This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus -LRB- Marcus et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_Marcus_1993 nn_corpus_Treebank nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_of_sentences_corpus num_sentences_12,925 amod_verbs_frequent det_verbs_the advmod_frequent_most prep_of_25_verbs prep_of_senses_25 det_senses_the prep_in_tagging_sentences dobj_tagging_senses vmod_hand_tagging nn_hand_experience poss_hand_our dep_reports_Marcus prep_on_reports_hand nsubj_reports_paper det_paper_This ccomp_Abstract_reports amod_edu_Abstract dep_``_edu
W97-0202	J93-2004	o	1 Introduction This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus -LRB- Marcus et al. 1993 -RRB-	advmod_1993_al. nn_al._et num_Marcus_1993 nn_corpus_Treebank nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_of_sentences_corpus num_sentences_12,925 amod_verbs_frequent det_verbs_the advmod_frequent_most prep_of_25_verbs prep_of_senses_25 det_senses_the prep_in_tagging_sentences dobj_tagging_senses vmod_hand_tagging nn_hand_experience poss_hand_our prep_on_reports_hand nsubj_reports_paper det_paper_This appos_Introduction_Marcus rcmod_Introduction_reports num_Introduction_1
W97-0209	J93-2004	o	In marked contrast to annotated training material for partof-speech tagging -LRB- a -RRB- there is no coarse-level set of sense distinctions widely agreed upon -LRB- whereas part-of-speech tag sets tend to differ in the details -RRB- -LRB- b -RRB- sense annotation has a comparatively high error rate -LRB- Miller personal communication reports an upper bound for human annotators of around 90 % for ambiguous cases using a non-blind evaluation method that may make even this estimate overly optimistic -RRB- and -LRB- c -RRB- no fully automatic method provides high enough quality output to support the annotate automatically correct manually methodology used to provide high volume annotation by data providers like the Penn Treebank project -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_project_Treebank nn_project_Penn det_project_the prep_like_providers_project nn_providers_data nn_annotation_volume amod_annotation_high prep_by_provide_providers dobj_provide_annotation aux_provide_to xcomp_used_provide vmod_methodology_used advmod_correct_manually dep_annotate_Marcus dep_annotate_methodology dep_annotate_correct advmod_annotate_automatically dep_annotate_the ccomp_support_annotate aux_support_to nn_output_quality amod_output_enough amod_output_high xcomp_provides_support dobj_provides_output nsubj_provides_method dep_provides_c amod_method_automatic neg_method_no advmod_automatic_fully advmod_optimistic_overly amod_estimate_optimistic det_estimate_this advmod_estimate_even dobj_make_estimate aux_make_may nsubj_make_that rcmod_method_make nn_method_evaluation amod_method_non-blind det_method_a dobj_using_method amod_cases_ambiguous num_%_90 quantmod_90_around prep_for_annotators_cases prep_of_annotators_% amod_annotators_human dep_bound_using prep_for_bound_annotators amod_bound_upper det_bound_an dep_reports_bound amod_communication_personal conj_and_Miller_provides appos_Miller_reports appos_Miller_communication dep_rate_provides dep_rate_Miller nn_rate_error amod_rate_high det_rate_a advmod_high_comparatively dobj_has_rate nsubj_has_annotation dep_has_b nn_annotation_sense det_details_the prep_in_differ_details aux_differ_to xcomp_tend_differ nsubj_tend_sets mark_tend_whereas nn_sets_tag amod_sets_part-of-speech parataxis_agreed_has advcl_agreed_tend prep_agreed_upon advmod_agreed_widely parataxis_agreed_is prep_in_agreed_contrast nn_distinctions_sense prep_of_set_distinctions amod_set_coarse-level neg_set_no nsubj_is_set expl_is_there dep_is_a amod_tagging_partof-speech prep_for_material_tagging nn_material_training amod_material_annotated prep_to_contrast_material amod_contrast_marked
W97-0209	J93-2004	o	Test and training materials were derived from the Brown corpus of American English all of which has been parsed and manually verified by the Penn T ~ eebank project -LRB- Marcus et al. 1993 -RRB- and parts of which have been manually sense-tagged by the WordNet group -LRB- Miller et al. 1993 -RRB-	amod_Miller_1993 dep_Miller_al. nn_Miller_et nn_group_WordNet det_group_the agent_sense-tagged_group advmod_sense-tagged_manually auxpass_sense-tagged_been aux_sense-tagged_have nsubjpass_sense-tagged_parts prep_of_parts_which amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_project_eebank nn_project_~ nn_project_T nn_project_Penn det_project_the prep_by_verified_project advmod_verified_manually nsubjpass_verified_all dep_parsed_Miller conj_and_parsed_sense-tagged dep_parsed_Marcus conj_and_parsed_verified auxpass_parsed_been aux_parsed_has nsubjpass_parsed_all prep_of_all_which amod_English_American prep_of_corpus_English amod_corpus_Brown det_corpus_the parataxis_derived_sense-tagged parataxis_derived_verified parataxis_derived_parsed prep_from_derived_corpus auxpass_derived_were nsubjpass_derived_materials nn_materials_training nn_materials_Test conj_and_Test_training
W97-0209	J93-2004	o	The approach combines statistical and knowledge-based methods but unlike many recent corpus-based approaches to sense disambiguation -LRB- arowsky 1993 Bruce and Wiebe 1994 Miller et al. 1994 -RRB- it takes as its starting point the assumption that senseannotated training text is not available	neg_available_not cop_available_is nsubj_available_text mark_available_that nn_text_training amod_text_senseannotated ccomp_assumption_available det_assumption_the amod_point_starting poss_point_its dobj_takes_assumption prep_as_takes_point nsubj_takes_it prep_unlike_takes_approaches num_Miller_1994 nn_Miller_al. nn_Miller_et num_Bruce_1994 conj_and_Bruce_Wiebe dep_arowsky_Miller conj_arowsky_Wiebe conj_arowsky_Bruce conj_arowsky_1993 appos_disambiguation_arowsky dobj_sense_disambiguation aux_sense_to vmod_approaches_sense amod_approaches_corpus-based amod_approaches_recent amod_approaches_many amod_methods_knowledge-based amod_methods_statistical conj_and_statistical_knowledge-based conj_but_combines_takes dobj_combines_methods nsubj_combines_approach det_approach_The
W97-0301	J93-2004	o	3 Probability Model This paper takes a history-based approach -LRB- Black et al. 1993 -RRB- where each tree-building procedure uses a probability model p -LRB- alb -RRB- derived from p -LRB- a b -RRB- to weight any action a based on the available context or history b. First we present a few simple categories of contextual predicates that capture any information in b that is useful for predicting a. Next the predicates are used to extract a set of features from a corpus of manually parsed sentences	amod_sentences_parsed advmod_sentences_manually prep_of_corpus_sentences det_corpus_a prep_from_set_corpus prep_of_set_features det_set_a dobj_extract_set aux_extract_to xcomp_used_extract auxpass_used_are nsubjpass_used_predicates det_predicates_the rcmod_Next_used nn_Next_a. dobj_predicting_Next prepc_for_useful_predicting cop_useful_is nsubj_useful_that rcmod_information_useful prep_in_information_b det_information_any dobj_capture_information nsubj_capture_that rcmod_predicates_capture amod_predicates_contextual prep_of_categories_predicates amod_categories_simple amod_categories_few det_categories_a dobj_present_categories nsubj_present_we rcmod_First_present nn_First_b. det_First_a conj_or_context_history amod_context_available det_context_the pobj_a_history pobj_a_context prepc_based_on_a_on dep_action_First det_action_any dobj_weight_action aux_weight_to det_b_a appos_p_b prep_from_derived_p vmod_p_derived appos_p_alb nn_p_model nn_p_probability det_p_a vmod_uses_weight dobj_uses_p nsubj_uses_procedure advmod_uses_where nn_procedure_tree-building det_procedure_each dep_al._1993 nn_al._et amod_al._Black amod_approach_history-based det_approach_a dep_takes_uses dep_takes_al. dobj_takes_approach nsubj_takes_paper det_paper_This rcmod_Model_takes nn_Model_Probability num_Model_3 dep_``_Model
W97-0308	J93-2004	o	We have processed the Susanne corpus -LRB- Sampson 1995 -RRB- and Penn treebank -LRB- Marcus et al 1993 -RRB- to provide tables of word and subtree alignments	nn_alignments_subtree nn_alignments_word conj_and_word_subtree prep_of_tables_alignments dobj_provide_tables aux_provide_to amod_Marcus_1993 dep_Marcus_al nn_Marcus_et nn_treebank_Penn amod_Sampson_1995 conj_and_corpus_treebank dep_corpus_Sampson nn_corpus_Susanne det_corpus_the vmod_processed_provide dep_processed_Marcus dobj_processed_treebank dobj_processed_corpus aux_processed_have nsubj_processed_We ccomp_``_processed
W97-1005	J93-2004	o	Both data were extracted from the Penn Treebank Wall Street Journal -LRB- WSJ -RRB- Corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Corpus_Journal appos_Journal_WSJ nn_Journal_Street nn_Journal_Wall nn_Journal_Treebank nn_Journal_Penn det_Journal_the dep_extracted_Marcus prep_from_extracted_Corpus auxpass_extracted_were nsubjpass_extracted_data det_data_Both ccomp_``_extracted
W97-1005	J93-2004	o	Statistical and information theoretic approaches -LRB- Hindle and Rooth 1993 -RRB- -LRB- Ratnaparkhi et al. 1994 -RRB- -LRB- Collins and Brooks 1995 -RRB- -LRB- Franz 1996 -RRB- Using lexical collocations to determine PPA with statistical techniques was first proposed by -LRB- Hindle and Rooth 1993 -RRB-	dep_Hindle_1993 conj_and_Hindle_Rooth dep_by_Rooth dep_by_Hindle prep_proposed_by vmod_first_proposed nsubj_was_first amod_techniques_statistical prep_with_PPA_techniques dobj_determine_PPA aux_determine_to amod_collocations_lexical vmod_Using_determine dobj_Using_collocations amod_Franz_1996 amod_Collins_1995 conj_and_Collins_Brooks amod_Ratnaparkhi_1994 dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_Hindle_1993 conj_and_Hindle_Rooth dep_approaches_was vmod_approaches_Using appos_approaches_Franz appos_approaches_Brooks appos_approaches_Collins dep_approaches_Ratnaparkhi dep_approaches_Rooth dep_approaches_Hindle amod_approaches_theoretic amod_approaches_information amod_approaches_Statistical conj_and_Statistical_information
W97-1502	J93-2004	p	Many mainstream systems and formalisms would satisfy these criteria including ones such as the University of Pennsylvania Treebank -LRB- Marcus et al 1993 -RRB- which are purely syntactic -LRB- though of course only syntactic properties could then be extracted -RRB-	auxpass_extracted_be advmod_extracted_then aux_extracted_could nsubjpass_extracted_properties prep_extracted_though amod_properties_syntactic advmod_properties_only pobj_of_course pcomp_though_of dep_syntactic_extracted advmod_syntactic_purely cop_syntactic_are nsubj_syntactic_which amod_Marcus_1993 dep_Marcus_al nn_Marcus_et rcmod_Treebank_syntactic dep_Treebank_Marcus dep_Treebank_University prep_of_University_Pennsylvania det_University_the prep_such_as_ones_Treebank prep_including_criteria_ones det_criteria_these dobj_satisfy_criteria aux_satisfy_would nsubj_satisfy_formalisms nsubj_satisfy_systems conj_and_systems_formalisms nn_systems_mainstream amod_systems_Many ccomp_``_satisfy
W98-0701	J93-2004	o	Because our algorithm does not consider the context given by the preceding sentences we have conducted the following experiment to see to what extent the discourse context could improve the performance of the wordsense disambiguation Using the semantic concordance files -LRB- Miller et al. 1993 -RRB- we have counted the occurrences of content words which previously appear in the same discourse file	nn_file_discourse amod_file_same det_file_the prep_in_appear_file advmod_appear_previously nsubj_appear_which rcmod_words_appear nn_words_content prep_of_occurrences_words det_occurrences_the dobj_counted_occurrences aux_counted_have nsubj_counted_we amod_Miller_1993 dep_Miller_al. nn_Miller_et nn_files_concordance amod_files_semantic det_files_the parataxis_Using_counted dep_Using_Miller dobj_Using_files nn_disambiguation_wordsense det_disambiguation_the prep_of_performance_disambiguation det_performance_the dobj_improve_performance aux_improve_could nsubj_improve_context rcmod_discourse_improve det_discourse_the dep_extent_discourse det_extent_what dep_see_Using prep_to_see_extent aux_see_to amod_experiment_following det_experiment_the dep_conducted_see dobj_conducted_experiment aux_conducted_have nsubj_conducted_we advcl_conducted_consider amod_sentences_preceding det_sentences_the agent_given_sentences vmod_context_given det_context_the dobj_consider_context neg_consider_not aux_consider_does nsubj_consider_algorithm mark_consider_Because poss_algorithm_our ccomp_``_conducted
W98-0701	J93-2004	o	Both for the training and for the testing of our algorithm we used the syntactically analysed sentences of the Brown Corpus -LRB- Marcus 1993 -RRB- which have been manually semantically tagged -LRB- Miller et al. 1993 -RRB- into semantic concordance files -LRB- SemCor -RRB-	appos_files_SemCor nn_files_concordance amod_files_semantic amod_Miller_1993 dep_Miller_al. nn_Miller_et advmod_tagged_semantically advmod_tagged_manually auxpass_tagged_been aux_tagged_have nsubjpass_tagged_which dep_Marcus_1993 rcmod_Corpus_tagged appos_Corpus_Marcus amod_Corpus_Brown det_Corpus_the prep_of_sentences_Corpus amod_sentences_analysed det_sentences_the advmod_analysed_syntactically prep_into_used_files dep_used_Miller dobj_used_sentences nsubj_used_we discourse_used_Both poss_algorithm_our prep_of_testing_algorithm det_testing_the conj_and_training_testing det_training_the prep_for_Both_testing prep_for_Both_training
W98-0717	J93-2004	o	-LRB- 1994 -RRB- from the Penn Treebank -LRB- Marcus et al. 1993 -RRB- WSJ corpus	nn_corpus_WSJ appos_corpus_Marcus num_corpus_1994 num_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_from_1994_Treebank
W98-1114	J93-2004	o	Systems which are able to acquire a small number of verbal subcategorisation classes automatically from corpus text have been described by Brent -LRB- 1991 1993 -RRB- and Ushioda et al.	nn_al._et nn_al._Ushioda dep_1991_1993 conj_and_Brent_al. dep_Brent_1991 agent_described_al. agent_described_Brent auxpass_described_been aux_described_have nsubjpass_described_Systems nn_text_corpus prep_from_automatically_text nn_classes_subcategorisation amod_classes_verbal prep_of_number_classes amod_number_small det_number_a advmod_acquire_automatically dobj_acquire_number aux_acquire_to xcomp_able_acquire cop_able_are nsubj_able_which rcmod_Systems_able
W98-1115	J93-2004	o	4 The Experiment For our experiment we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text -LRB- Marcus et al. 1993 -RRB- with section 22 reserved for testing	prep_for_reserved_testing vmod_section_reserved num_section_22 amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_text_Journal nn_text_Street nn_text_Wall nn_text_Penn det_text_the prep_of_sections_text num_sections_2-21 prep_from_induced_sections vmod_grammar_induced amod_grammar_tree-bank det_grammar_a prep_with_used_section dep_used_Marcus dobj_used_grammar nsubj_used_we nsubj_used_Experiment poss_experiment_our prep_for_Experiment_experiment det_Experiment_The num_Experiment_4
W98-1119	J93-2004	o	This program differs from earlier work in its almost complete lack of hand-crafting relying instead on a very small corpus of Penn Wall Street Journal Tree-bank text -LRB- Marcus et al. 1993 -RRB- that has been marked with co-reference information	nn_information_co-reference prep_with_marked_information auxpass_marked_been aux_marked_has nsubjpass_marked_that amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et amod_text_Tree-bank nn_text_Journal nn_text_Street nn_text_Wall nn_text_Penn prep_of_corpus_text amod_corpus_small det_corpus_a advmod_small_very rcmod_relying_marked dep_relying_Marcus prep_on_relying_corpus advmod_relying_instead prep_of_lack_hand-crafting amod_lack_complete poss_lack_its advmod_complete_almost prep_in_work_lack amod_work_earlier dep_differs_relying prep_from_differs_work nsubj_differs_program det_program_This
W98-1121	J93-2004	o	Dialogs Speakers Turns Words Fragments Distinct Words Distinct Words/POS Singleton Words Singleton Words/POS Intonational Phrases Speech Repairs 98 34 6163 58298 756 859 1101 252 350 10947 2396 Table 1 Size of the Trains Corpus 2.1 POS Annotations Our POS tagset is based on the Penn Treebank tagset -LRB- Marcus et al. 1993 -RRB- but modified to include tags for discourse markers and end-of-turns and to provide richer syntactic information -LRB- Heeman 1997 -RRB-	amod_Heeman_1997 dep_information_Heeman amod_information_syntactic amod_information_richer dobj_provide_information aux_provide_to conj_and_markers_end-of-turns nn_markers_discourse prep_for_tags_end-of-turns prep_for_tags_markers dobj_include_tags aux_include_to xcomp_modified_include amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_tagset_Treebank nn_tagset_Penn det_tagset_the prep_on_based_tagset auxpass_based_is nsubjpass_based_tagset nn_tagset_POS poss_tagset_Our rcmod_Annotations_based nn_Annotations_POS nn_Annotations_Corpus nn_Annotations_Trains det_Annotations_the num_Corpus_2.1 appos_Size_Marcus prep_of_Size_Annotations num_Table_1 num_Table_2396 dep_10947_Table dep_350_10947 dep_756 859 1101 252_350 number_756 859 1101 252_98 34 6163 58298 num_Repairs_756 859 1101 252 dep_Speech_Repairs dep_Phrases_Speech dep_Intonational_Size dep_Intonational_Phrases conj_but_Words/POS_modified dep_Words/POS_Intonational dep_Singleton_modified dep_Singleton_Words/POS dep_Words_Singleton conj_and_Singleton_provide dep_Singleton_Words dep_Words/POS_provide dep_Words/POS_Singleton dobj_Distinct_Words/POS dep_Words_Distinct dep_Distinct_Words amod_Fragments_Distinct dep_Words_Fragments dobj_Turns_Words nsubj_Turns_Speakers nn_Speakers_Dialogs
W98-1121	J93-2004	o	-LRB- Charniak et al. 1993 -RRB- -RRB- simplify these probability distributions as given in Equations 9 and 10	conj_and_9_10 dep_Equations_10 dep_Equations_9 prep_in_given_Equations mark_given_as nn_distributions_probability det_distributions_these advcl_simplify_given dobj_simplify_distributions nsubj_simplify_Charniak amod_Charniak_1993 dep_Charniak_al. nn_Charniak_et
W98-1126	J93-2004	o	The data consists of 2,544 main clauses from the Wall Street Journal Treebank corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the prep_from_clauses_corpus amod_clauses_main num_clauses_2,544 dep_consists_Marcus prep_of_consists_clauses nsubj_consists_data det_data_The
W99-0104	J93-2004	o	We use the finite-state parses of FaSTU $ -LRB- Appelt et al. 1993 -RRB- for recognizing these entities but the method extends to any basic phrasal parser 4	num_parser_4 amod_parser_phrasal amod_parser_basic det_parser_any prep_to_extends_parser nsubj_extends_method det_method_the det_entities_these dobj_recognizing_entities amod_Appelt_1993 dep_Appelt_al. nn_Appelt_et dep_$_Appelt nn_$_FaSTU prepc_for_parses_recognizing prep_of_parses_$ det_finite-state_the conj_but_use_extends dep_use_parses dobj_use_finite-state nsubj_use_We
W99-0104	J93-2004	o	This knowledge is represented in axiomatic form using the notation proposed in -LRB- Hobbs et al. 1993 -RRB- and previously implemented in TACITUS	prep_in_implemented_TACITUS advmod_implemented_previously conj_and_Hobbs_implemented amod_Hobbs_1993 dep_Hobbs_al. nn_Hobbs_et prep_in_proposed_implemented prep_in_proposed_Hobbs vmod_notation_proposed det_notation_the dobj_using_notation amod_form_axiomatic xcomp_represented_using prep_in_represented_form auxpass_represented_is nsubjpass_represented_knowledge det_knowledge_This
W99-0104	J93-2004	o	Such methods were presented in -LRB- Hoblm et al. 1993 -RRB- and ~ flensky 1978 -RRB-	dep_flensky_1978 nn_flensky_~ conj_and_Hoblm_flensky amod_Hoblm_1993 dep_Hoblm_al. nn_Hoblm_et prep_in_presented_flensky prep_in_presented_Hoblm auxpass_presented_were nsubjpass_presented_methods amod_methods_Such
W99-0104	J93-2004	o	The first one makes use of the advances in the parsing technology or on the availability of large parsed corpora -LRB- e.g. Trcebank -LRB- Marcus et al. 1993 -RRB- -RRB- to produce algorithms inspired by Hobbs ' baseline method -LRB- Hobbs 1978 -RRB-	amod_Hobbs_1978 dep_method_Hobbs nn_method_baseline poss_method_Hobbs agent_inspired_method vmod_algorithms_inspired dobj_produce_algorithms aux_produce_to dep_1993_al. nn_al._et num_Marcus_1993 dep_Trcebank_Marcus dep_Trcebank_e.g. amod_corpora_parsed amod_corpora_large prep_of_availability_corpora det_availability_the nn_technology_parsing det_technology_the vmod_advances_produce dep_advances_Trcebank prep_on_advances_availability prep_in_advances_technology conj_or_advances_advances det_advances_the prep_of_use_advances prep_of_use_advances dobj_makes_use nsubj_makes_one amod_one_first det_one_The
W99-0204	J93-2004	o	jp/et I/nl/GDA / t agset html 2http / / www.uic.edu :80 / orgs/tei / 25 ing insights from EAGLES s Penn TreeBank \ -LSB- Marcus et al. 1993 \ -RSB- and so forth	advmod_forth_so num_\_1993 appos_Marcus_\ dep_Marcus_al. nn_Marcus_et nn_\_TreeBank nn_\_Penn appos_s_\ nn_s_EAGLES prep_from_insights_s nn_insights_ing num_insights_25 advmod_www.uic.edu_forth cc_www.uic.edu_and dep_www.uic.edu_Marcus dep_www.uic.edu_insights amod_www.uic.edu_orgs/tei num_www.uic.edu_:80 nn_2http_html dep_agset_www.uic.edu appos_agset_2http nn_agset_t nn_agset_I/nl/GDA dobj_jp/et_agset ccomp_``_jp/et
W99-0301	J93-2004	o	html \ -RSB- provided by Lynette Hirschman syntactic structures in the style of the Penn TreeBank -LRB- Marcus et al. 1993 -RRB- provided by Ann Taylor and an alternative annotation for the F0 aspects of prosody known as Tilt -LRB- Taylor 1998 -RRB- and provided by its inventor Paul Taylor	nn_Taylor_Paul appos_inventor_Taylor poss_inventor_its prep_by_provided_inventor amod_Taylor_1998 dep_Tilt_Taylor prep_as_known_Tilt conj_and_prosody_provided vmod_prosody_known prep_of_aspects_provided prep_of_aspects_prosody nn_aspects_F0 det_aspects_the prep_for_annotation_aspects amod_annotation_alternative det_annotation_an nn_Taylor_Ann agent_provided_Taylor dep_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_TreeBank_Penn det_TreeBank_the prep_of_style_TreeBank det_style_the vmod_structures_provided dep_structures_Marcus prep_in_structures_style amod_structures_syntactic nn_Hirschman_Lynette agent_provided_Hirschman conj_and_\_annotation conj_and_\_structures vmod_\_provided nn_\_html
W99-0502	J93-2004	p	It us widely acknowledged that word sense d ~ samblguatmn -LRB- WSD -RRB- us a central problem m natural language processing In order for computers to be able to understand and process natural language beyond simple keyword matching the problem of d ~ samblguatmg word sense or dlscermng the meamng of a word m context must be effectively dealt with Advances in WSD v ill have slgmficant Impact on apphcatlons hke information retrieval and machine translation For natural language subtasks hke part-of-speech tagging or s -RRB- ntactm parsing there are relatlvely well defined and agreed-upon cnterm of what it means to have the correct part of speech or syntactic structure assigned to a word or sentence For instance the Penn Treebank corpus -LRB- Marcus et al 1993 -RRB- pro ~ ide ~ t large repo ~ tory of texts annotated w ~ th partof-speech and s -RCB- ntactm structure mformatlon Tv.o independent human annotators can achieve a high rate of agreement on assigning part-of-speech tags to words m a g ~ ven sentence Unfortunately th ~ s us not the case for word sense assignment F ~ rstly it is rarely the case that any two dictionaries will have the same set of sense defimtmns for a g ~ ven word Different d ~ ctlonanes tend to carve up the semantic space m a different way so to speak Secondly the hst of senses for a word m a typical dmtmnar ~ tend to be rather refined and comprehensive This is especmlly so for the commonly used words which have a large number of senses The sense dustmctmn between the different senses for a commonly used word m a d ~ ctmnary hke WoRDNET -LRB- Miller 1990 -RRB- tend to be rather fine Hence two human annotators may genuinely dusagree m their sense assignment to a word m context The agreement rate between human annotators on word sense assignment us an Important concern for the evaluatmn of WSD algorithms One would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human annotators The agreement rate between human annotators will then form the upper ceiling against whmh to compare the performance of WSD algorithms For instance the SENSEVAL exerclse has performed a detaded study to find out the raterannotator agreement among ~ ts lexicographers taggrog the word senses -LRB- Kllgamff 1998c Kllgarnff 1998a Kflgarrlff 1998b -RRB- 2 A Case Study In this-paper we examine the ~ ssue of raterannotator agreement by comparing the agreement rate of human annotators on a large sense-tagged corpus of more than 30,000 instances of the most frequently occurring nouns and verbs of Enghsh This corpus is the intersection of the WORDNET Semcor corpus -LRB- Miller et al 1993 -RRB- and the DSO corpus -LRB- Ng and Lee 1996 Ng 1997 -RRB- which has been independently tagged wlth the refined senses of WORDNET by two separate groups of human annotators The Semcor corpus us a subset of the Brown corpus tagged with ~ VoRDNET senses and consists of more than 670,000 words from 352 text files Sense taggmg was done on the content words -LRB- nouns ~ erbs adjectives and adverbs -RRB- m this subset The DSO corpus consists of sentences drawn from the Brown corpus and the Wall Street Journal For each word w from a hst of 191 frequently occurring words of Enghsh -LRB- 121 nouns and 70 verbs -RRB- sentences containing w -LRB- m singular or plural form and m its various reflectional verb form -RRB- are selected and each word occurrence w ~ s tagged w ~ th a sense from WoRDNET There ~ s a total of about 192,800 sentences in the DSO corpus m which one word occurrence has been sense-tagged m each sentence The intersection of the Semcor corpus and the DSO corpus thus consists of Brown corpus sentences m which a word occurrence w is sense-tagged m each sentence where w Is one of.the 191 frequently oc currmg English nouns or verbs Since this common pomon has been sense-tagged by two independent groups of human annotators ~ t serves as our data set for investigating inter-annotator agreement in this paper 3 Sentence Matching To determine the extent of inter-annotator agreement the first step ~ s to match each sentence m Semcor to its corresponding counterpart In the DSO corpus This step ~ s comphcated by the following factors 1 Although the intersected portion of both corpora came from Brown corpus they adopted different tokemzatmn convention and segmentartan into sentences differed sometimes 2 The latest versmn of Semcor makes use of the senses from WORDNET 1 6 whereas the senses used m the DSO corpus were from WoRDNET 15 1 To match the sentences we first converted the senses m the DSO corpus to those of WORDNET 1 6 We ignored all sentences m the DSO corpus m which a word is tagged with sense 0 or -1 -LRB- A word is tagged with sense 0 or -1 ff none of the given senses m WoRDNFT applies -RRB- 4 sentence from Semcor is considered to match one from the DSO corpus ff both sentences are exactl -RRB- ldent ~ cal or ff the ~ differ only m the pre ~ ence or absence of the characters -LRB- permd -RRB- or ' -LRB- hyphen -RRB- For each remaining Semcor sentence taking into account word ordering ff 75 % or more of the words m the sentence match those in a DSO corpus sentence then a potential match ~ s recorded These i kctua \ -LSB- ly the WORD ~ q ` ET senses used m the DSO corpus were from a shght variant of the official WORDNE'I 1 5 release Th ~ s ssas brought to our attention after the pubhc release of the DSO corpus potential matches are then manually verffied to ensure that they are true matches and to ~ eed out any false matches Using this method of matching a total of 13,188 sentence-palrs contasnmg nouns and 17,127 sentence-pa ~ rs containing verbs are found to match from both corpora ymldmg 30,315 sentences which form the intersected corpus used m our present study 4 The Kappa Statistic Suppose there are N sentences m our corpus where each sentence contains the word w Assume that w has M senses Let 4 be the number of sentences which are assigned identical sense b ~ two human annotators Then a simple measure to quantify the agreement rate between two human annotators Is Pc where Pc = A/N The drawback of this simple measure is that it does not take into account chance agreement between two annotators The Kappa statistic a -LRB- Cohen 1960 -RRB- is a better measure of rater-annotator agreement which takes into account the effect of chance agreement It has been used recently w ~ thm computatmnal hngu ~ stlcs to measure raterannotator agreement -LRB- Bruce and Wmbe 1998 Carletta 1996 Veroms 1998 -RRB- Let Cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 Then P ~ P ~ 1-P ~ where M j = l and Pe measures the chance agreement between two annotators A Kappa ~ alue of 0 indicates that the agreement is purely due to chance agreement whereas a Kappa ~ alue of 1 indicates perfect agreement A Kappa ~ alue of 0 8 and above is considered as mdmatmg good agreement -LRB- Carletta 1996 -RRB- Table 1 summarizes the inter-annotator agreement on the mtersected corpus The first -LRB- becond -RRB- row denotes agreement on the nouns -LRB- xerbs -RRB- wh ~ le the lass row denotes agreement on all words combined The a ~ erage ~ reported m the table is a s ~ mpie average of the individual ~ value of each word The agreement rate on the 30,315 sentences as measured by P = is 57 % This tallies with the figure reported ~ n our earlier paper -LRB- Ng and Lee 1996 -RRB- where we performed a quick test on a subset of 5,317 sentences n the intersection of both the Semcor corpus and the DSO corpus 10 \ -LSB- \ -RSB- mm m m m m m mm m m m m mm m m m Type Num of v ords A N \ -LSB- P ~ Avg Nouns 121 7,676 13,188 I 0 582 0 300 Verbs 70 9,520 17,127 I 0 555 0 347 All I 191 I 17,196 30,315 I 056T 0317 Table 1 Raw inter-annotator agreement 5 Algorithm Since the rater-annotator agreement on the intersected corpus is not high we would like to find out how the agreement rate would be affected if different sense classes were in use In this section we present a greedy search algorithm that can automatmalb derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achmve a higher agreement rate but we still maintain as many of the original sense classes as possible The algorithm is given m Figure 1 The algorithm operates on a set of sentences where each sentence contains an occurrence of the word w whmh has been sense-tagged by two human annotators At each Iteration of the algorithm tt finds the pair of sense classes Ct and Cj such that merging these two sense classes results in the highest t ~ value for the resulting merged group of sense classes It then proceeds to merge Cz and C ~ Thin process Is repeated until the ~ value reaches a satisfactory value ~ ~ t ~ which we set as 0 8 Note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any NLP tasks in which prior human agreement rate may not be high enough Such NLP tasks could be discourse tagging speech-act categorization etc 6 Results For each word w from the list of 121 nouns and 70 verbs ~ e applied the greedy search algorithm to each set of sentences in the intersected corpus contaming w For a subset of 95 words -LRB- 53 nouns and 42 verbs -RRB- the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting Kappa ~ alue reaches 0 8 or higher For the other 96 words m order for the Kappa value to reach 0 8 or higher the algorithm collapses all senses of the ~ ord to a single -LRB- trivial -RRB- class Table 2 and 3 summarizes the results for the set of 53 nouns and 42 ~ erbs respectively Table 2 md ~ cates that before the collapse of sense classes these 53 nouns have an average of 7 6 senses per noun There is a total of 5,339 sentences in the intersected corpus containing these nouns of which 3,387 sentences were assigned the same sense by the two groups of human annotators The average Kappa statistic -LRB- computed as a simple average of the Kappa statistic of ~ he mdlwdual nouns -RRB- is 0 463 After the collapse of sense classes by the greedy search algorithm the average number of senses per noun for these 53 nouns drops to 40 Howe ~ er the number of sentences which have been asmgned the same coarse sense by the annotators increases to 5,033 That is about 94 3 % of the sentences have been assigned the same coarse sense and that the average Kappa statistic has improved to 0 862 mgmfymg high rater-annotator agreement on the derived coarse senses Table3 gl ~ es the analogous figures for the 42 verbs agmn mdmatmg that high agreement is achieved on the coarse sense classes den ~ ed for verbs 7 Discussion Our findings on rater-annotator agreement for word sense tagging indicate that for average language users it is quite dl ~ cult to achieve high agreement when they are asked to assign refned sense tags -LRB- such as those found in WORDNET -RRB- given only the scanty definition entries m the WORDNET dlctionary and a few or no example sentences for the usage of each word sense Thin observation agrees wlth that obtmned m a recent study done by -LRB- Veroms 1998 -RRB- where the agreement on sense-tagging by naive users was also not hlgh Thus It appears that an average language user is able to process language wlthout needing to perform the task of dlsamblguatmg word sense to a very fine-grained resolutmn as formulated m a tradltlonal dmtlonary In contrast expert lexicographers tagged the ~ ord sense in the sentences used m the SENSEVAL exerclse where high rater-annotator agreement was reported There are also fuller dlctlonary entries m the HECTOR dlctlonary used and more e < amples showing the usage of each word sense m HECTOR These factors are likely to have contributed to the difference in rater-annotator agreement observed m the three studies conducted We also examined the coarse sense classes derived by the greedy search algorithm Vv ' e found some interesting groupings of coarse senses for nouns which ~ e hst in Table 4 From Table 4 it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human mtmtwe judgment of sense graz -RCB- ulanty It Is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of WoRDNET As an example there is a total Ii loop let Ct C M denote the current M sense classes ~ * + -- oo for all z ,3 such that 1 < < 3 < M let C \ -LSB- C ~ w _ 1 denote the resulting M 1 sense classes by mergmg C and C 3 compute ~ -LRB- C \ -LSB- C ~ / _ t -RRB- ff ~ -LRB- C C ~ 4_x -RRB- > ~ * then ~ + ~ -LRB- C ~ C ~ _ t -RRB- z * + ~ ~ * + end for merge the sense class C	nn_C_class nn_C_sense det_C_the dobj_merge_C dep_for_merge prep_*_for conj_+_*_end num_*_~ appos_*_end appos_*_* conj_+_*_~ dep_z_~ dep_z_* num_t__ nn_t_~ nn_t_C nn_~_C num_~_~ quantmod_~_then quantmod_~_* num_*_~ quantmod_~_> nn_4_x_~ nn_4_x_C dep_C_4_x vmod_~_z dep_~_t conj_+_~_~ dep_~_~ dep_~_C nn_~_ff nn_~_~ num_t__ dep_~_t nn_~_C nn_\_C nn_\_~ dep_compute_~ dep_compute_~ dobj_compute_\ nsubj_compute_~ nn_C_mergmg nn_classes_sense num_classes_1 nn_classes_M amod_classes_resulting det_classes_the prep_by_denote_C dobj_denote_classes nsubj_denote__ num___1 nn___w nn___~ nn___C amod_C_\ dobj_let_C nsubj_let_M advmod_let_< nsubj_let_1 mark_let_that num_M_3 quantmod_3_< quantmod_3_< ccomp_such_let num_,3_3 conj_and_,3_C rcmod_,3_denote amod_,3_such dep_,3_z dep_,3_all prep_for_oo_C prep_for_oo_,3 cc_*_+ dep_~_oo dep_~_* nn_classes_sense nn_classes_M amod_classes_current det_classes_the parataxis_denote_compute dobj_denote_classes nsubj_denote_M nn_M_C parataxis_Ct_denote dep_let_Ct dep_loop_let nn_loop_Ii amod_loop_total det_loop_a nsubj_is_loop expl_is_there det_example_an prep_of_senses_WoRDNET amod_senses_refined det_senses_the advmod_refined_overly prep_as_attributed_example prep_to_attributed_senses advmod_attributed_solely auxpass_attributed_be aux_attributed_can nsubjpass_attributed_some mark_attributed_that amod_annotators_human prep_of_groups_annotators num_groups_two det_groups_the prep_between_disagreement_groups det_disagreement_the prep_of_some_disagreement ccomp_clear_attributed cop_clear_Is nsubj_clear_It dep_clear_ulanty nn_graz_sense prep_of_judgment_graz nn_judgment_mtmtwe amod_judgment_human prep_to_correspond_judgment nsubj_correspond_that rcmod_senses_correspond nn_senses_word prep_of_groupings_senses amod_groupings_interesting dobj_derive_groupings aux_derive_can nsubj_derive_algorithm mark_derive_that nn_algorithm_search amod_algorithm_greedy det_algorithm_the ccomp_apparent_derive cop_apparent_is nsubj_apparent_it dep_apparent_hst dep_apparent_e num_Table_4 prep_from_Table_Table num_Table_4 prep_in_hst_Table dep_~_apparent nsubj_~_which rcmod_nouns_~ prep_for_senses_nouns amod_senses_coarse prep_of_groupings_senses amod_groupings_interesting det_groupings_some dobj_found_groupings nsubj_found_e nsubj_found_Vv rcmod_algorithm_found nn_algorithm_search amod_algorithm_greedy det_algorithm_the agent_derived_algorithm vmod_classes_derived nn_classes_sense amod_classes_coarse det_classes_the dobj_examined_classes advmod_examined_also nsubj_examined_We ccomp_conducted_examined vmod_studies_conducted num_studies_three det_studies_the nn_studies_m amod_studies_observed nn_studies_agreement amod_agreement_rater-annotator prep_in_difference_studies det_difference_the prep_to_contributed_difference aux_contributed_have aux_contributed_to parataxis_likely_clear xcomp_likely_contributed cop_likely_are csubj_likely_e csubj_likely_used det_factors_These nn_factors_HECTOR nn_factors_m nn_factors_sense nn_factors_word det_factors_each prep_of_usage_factors det_usage_the dobj_showing_usage vmod_amples_showing amod_amples_< advmod_e_more dobj_used_amples conj_and_used_e dep_used_dlctlonary dep_used_HECTOR dep_used_the dep_used_m dep_used_entries dep_used_dlctlonary dep_used_fuller parataxis_are_likely advmod_are_also expl_are_There ccomp_reported_are auxpass_reported_was nsubjpass_reported_agreement advmod_reported_where amod_agreement_rater-annotator amod_agreement_high rcmod_exerclse_reported nn_exerclse_SENSEVAL det_exerclse_the dep_m_exerclse dobj_used_m vmod_sentences_used det_sentences_the nn_sense_ord nn_sense_~ det_sense_the dep_tagged_is prep_in_tagged_sentences dobj_tagged_sense nsubj_tagged_lexicographers ccomp_tagged_m dep_tagged_formulated mark_tagged_as nn_lexicographers_expert prep_in_dmtlonary_contrast amod_dmtlonary_tradltlonal det_dmtlonary_a dep_m_dmtlonary amod_resolutmn_fine-grained det_resolutmn_a advmod_fine-grained_very nn_sense_word nn_sense_dlsamblguatmg prep_of_task_sense det_task_the dep_perform_tagged prep_to_perform_resolutmn dobj_perform_task aux_perform_to xcomp_needing_perform vmod_wlthout_needing nn_wlthout_language dobj_process_wlthout aux_process_to xcomp_able_process cop_able_is nsubj_able_user mark_able_that nn_user_language amod_user_average det_user_an ccomp_appears_able nsubj_appears_It parataxis_hlgh_appears advmod_hlgh_Thus neg_hlgh_not advmod_hlgh_also aux_hlgh_was nsubj_hlgh_agreement advmod_hlgh_where amod_users_naive prep_by_agreement_users prep_on_agreement_sense-tagging det_agreement_the rcmod_Veroms_hlgh amod_Veroms_1998 agent_done_Veroms vmod_study_done amod_study_recent det_study_a dep_m_study dobj_obtmned_m nsubj_obtmned_that rcmod_wlth_obtmned dobj_agrees_wlth nsubj_agrees_observation amod_observation_Thin rcmod_sense_agrees nn_sense_word det_sense_each prep_of_usage_sense det_usage_the nn_sentences_example prep_for_few_usage dep_few_sentences conj_or_few_no det_few_a nsubj_few_WORDNET conj_and_dlctionary_no conj_and_dlctionary_few nsubj_dlctionary_WORDNET det_WORDNET_the rcmod_m_few rcmod_m_dlctionary dep_entries_m dep_definition_entries dep_scanty_definition amod_the_scanty dep_only_the advmod_given_only prep_in_found_WORDNET vmod_those_found prep_such_as_tags_those nn_tags_sense amod_tags_refned vmod_assign_given dobj_assign_tags aux_assign_to xcomp_asked_assign auxpass_asked_are nsubjpass_asked_they advmod_asked_when amod_agreement_high advcl_achieve_asked dobj_achieve_agreement aux_achieve_to vmod_cult_achieve nn_cult_~ amod_cult_dl cop_cult_is nsubj_cult_it prep_for_cult_users mark_cult_that advmod_dl_quite nn_users_language amod_users_average ccomp_indicate_cult nn_sense_word amod_agreement_tagging prep_for_agreement_sense amod_agreement_rater-annotator prep_on_findings_agreement poss_findings_Our dep_Discussion_findings num_Discussion_7 appos_verbs_Discussion prep_for_ed_verbs nsubj_ed_den num_den_~ rcmod_classes_ed nn_classes_sense amod_classes_coarse det_classes_the prep_on_achieved_classes auxpass_achieved_is nsubjpass_achieved_agreement amod_agreement_high det_agreement_that rcmod_mdmatmg_achieved amod_mdmatmg_agmn appos_verbs_mdmatmg num_verbs_42 det_verbs_the prep_for_figures_verbs amod_figures_analogous det_figures_the dep_es_indicate dobj_es_figures nsubj_es_agreement num_gl_~ nn_gl_Table3 nn_gl_senses amod_gl_coarse amod_gl_derived det_gl_the prep_on_agreement_gl amod_agreement_rater-annotator amod_agreement_high nn_agreement_mgmfymg number_862_0 parataxis_improved_es prep_to_improved_862 aux_improved_has nsubj_improved_statistic mark_improved_that nn_statistic_Kappa amod_statistic_average det_statistic_the amod_sense_coarse amod_sense_same det_sense_the dobj_assigned_sense auxpass_assigned_been aux_assigned_have nsubjpass_assigned_% det_sentences_the prep_of_%_sentences num_%_3 num_%_94 quantmod_94_about conj_and_is_improved conj_and_is_assigned num_That_5,033 dep_increases_improved dep_increases_assigned dep_increases_is prep_to_increases_That nsubj_increases_number det_annotators_the amod_sense_coarse amod_sense_same det_sense_the agent_asmgned_annotators dobj_asmgned_sense auxpass_asmgned_been aux_asmgned_have nsubjpass_asmgned_which rcmod_sentences_asmgned prep_of_number_sentences det_number_the num_er_~ nn_er_Howe num_er_40 prep_to_drops_er nsubj_drops_number num_nouns_53 det_nouns_these prep_per_senses_noun prep_for_number_nouns prep_of_number_senses amod_number_average det_number_the nn_algorithm_search amod_algorithm_greedy det_algorithm_the nn_classes_sense prep_by_collapse_algorithm prep_of_collapse_classes det_collapse_the prep_after_463_collapse number_463_0 cop_463_is parataxis_463_is csubj_463_have amod_nouns_mdlwdual dep_nouns_he nn_nouns_~ prep_of_statistic_nouns nn_statistic_Kappa det_statistic_the prep_of_average_statistic amod_average_simple det_average_a prep_as_computed_average dep_statistic_computed nn_statistic_Kappa amod_statistic_average det_statistic_The amod_annotators_human prep_of_groups_annotators num_groups_two det_groups_the amod_sense_same det_sense_the dep_assigned_statistic agent_assigned_groups dobj_assigned_sense auxpass_assigned_were nsubjpass_assigned_sentences prep_of_assigned_which num_sentences_3,387 rcmod_nouns_assigned det_nouns_these dobj_containing_nouns vmod_corpus_containing amod_corpus_intersected det_corpus_the prep_in_sentences_corpus num_sentences_5,339 prep_of_total_sentences det_total_a nsubj_is_total expl_is_There prep_per_senses_noun num_senses_6 number_6_7 prep_of_average_senses det_average_an dobj_have_average nsubj_have_nouns prep_before_have_collapse mark_have_that num_nouns_53 det_nouns_these nn_classes_sense prep_of_collapse_classes det_collapse_the parataxis_cates_increases ccomp_cates_drops ccomp_cates_463 nsubj_cates_~ advmod_cates_respectively nn_~_md num_~_2 nn_~_Table nn_erbs_~ num_erbs_42 conj_and_nouns_erbs num_nouns_53 prep_of_set_erbs prep_of_set_nouns det_set_the prep_for_results_set det_results_the parataxis_summarizes_cates dobj_summarizes_results num_Table_2 nn_Table_class amod_Table_single det_Table_a dep_single_trivial nn_ord_~ det_ord_the prep_of_senses_ord det_senses_all prep_to_collapses_Table dobj_collapses_senses nsubj_collapses_algorithm nsubj_collapses_Results advmod_collapses_etc det_algorithm_the conj_or_8_higher num_8_0 dobj_reach_higher dobj_reach_8 aux_reach_to nn_value_Kappa det_value_the vmod_order_reach prep_for_order_value nn_order_m ccomp_order_able num_words_96 amod_words_other det_words_the conj_or_8_higher num_8_0 prep_for_reaches_words dobj_reaches_higher dobj_reaches_8 nsubj_reaches_alue mark_reaches_that dep_reaches_such nn_alue_~ nn_alue_Kappa amod_alue_resulting det_alue_the num_words_95 det_words_these rcmod_each_reaches prep_of_each_words num_senses_more num_senses_2 conj_or_2_more prep_for_set_each prep_of_set_senses amod_set_coarser det_set_a dobj_derive_set aux_derive_to xcomp_able_derive cop_able_was nsubj_able_algorithm det_algorithm_the num_verbs_42 conj_and_nouns_verbs num_nouns_53 dep_words_verbs dep_words_nouns num_words_95 prep_of_subset_words det_subset_a nn_w_contaming nn_w_corpus amod_w_intersected det_w_the prep_in_set_w prep_of_set_sentences det_set_each nn_algorithm_search amod_algorithm_greedy det_algorithm_the prep_for_applied_subset prep_to_applied_set dobj_applied_algorithm nsubj_applied_~ dep_~_e num_verbs_70 conj_and_nouns_verbs num_nouns_121 prep_of_list_verbs prep_of_list_nouns det_list_the nn_w_word det_w_each appos_Results_order rcmod_Results_applied prep_from_Results_list prep_for_Results_w num_Results_6 amod_categorization_speech-act conj_and_tagging_3 conj_and_tagging_collapses appos_tagging_categorization nn_tagging_discourse cop_tagging_be aux_tagging_could nsubj_tagging_~ nn_tasks_NLP amod_tasks_Such amod_tasks_enough amod_tasks_high cop_tasks_be neg_tasks_not aux_tasks_may dep_tasks_rate prep_in_tasks_which nn_rate_agreement amod_rate_human amod_rate_prior rcmod_tasks_tasks nn_tasks_NLP det_tasks_any prep_for_set_tasks amod_set_refined det_set_a prep_from_set_set prep_of_set_classes amod_set_coarser det_set_any dobj_deriving_set prepc_to_applicable_deriving advmod_applicable_also cop_applicable_is nsubj_applicable_algorithm mark_applicable_that det_algorithm_this num_Note_8 num_Note_0 ccomp_set_applicable prep_as_set_Note nsubj_set_we dobj_set_which nn_t_~ rcmod_~_set conj_~_~ conj_~_t rcmod_value_3 rcmod_value_collapses rcmod_value_tagging amod_value_satisfactory det_value_a dobj_reaches_value nsubj_reaches_value mark_reaches_until nn_value_~ det_value_the dep_repeated_summarizes advcl_repeated_reaches auxpass_repeated_Is csubjpass_repeated_sense-tagged nn_process_Thin num_process_~ nn_process_C conj_and_Cz_process dobj_merge_process dobj_merge_Cz aux_merge_to xcomp_proceeds_merge advmod_proceeds_then nsubj_proceeds_It nn_classes_sense prep_of_group_classes amod_group_merged amod_group_resulting det_group_the prep_for_value_group nn_value_~ nn_value_t amod_value_highest det_value_the ccomp_results_proceeds prep_in_results_value csubj_results_merging mark_results_that dep_results_such nn_classes_sense num_classes_two det_classes_these dobj_merging_classes dep_Ct_results conj_and_Ct_Cj prep_classes_Cj prep_classes_Ct dobj_sense_classes prep_of_pair_sense det_pair_the dobj_finds_pair nsubj_finds_tt det_algorithm_the prep_of_Iteration_algorithm det_Iteration_each rcmod_annotators_finds prep_at_annotators_Iteration amod_annotators_human num_annotators_two agent_sense-tagged_annotators auxpass_sense-tagged_been aux_sense-tagged_has nn_whmh_w nn_whmh_word det_whmh_the prep_of_occurrence_whmh det_occurrence_an dobj_contains_occurrence nsubj_contains_sentence advmod_contains_where det_sentence_each rcmod_sentences_contains prep_of_set_sentences det_set_a ccomp_operates_repeated prep_on_operates_set nsubj_operates_Figure det_algorithm_The dep_Figure_algorithm num_Figure_1 nn_Figure_m ccomp_given_operates auxpass_given_is nsubjpass_given_algorithm det_algorithm_The nn_classes_sense amod_classes_original det_classes_the prep_as_many_possible prep_of_many_classes ccomp_maintain_given prep_as_maintain_many advmod_maintain_still nsubj_maintain_we nn_rate_agreement amod_rate_higher det_rate_a conj_but_achmve_maintain dobj_achmve_rate nsubj_achmve_classes nn_classes_sense amod_classes_coarse amod_classes_derived amod_classes_resulting det_classes_The dep_annotators_maintain dep_annotators_achmve amod_annotators_human num_annotators_two agent_assigned_annotators vmod_tags_assigned nn_tags_sense det_tags_the prep_on_based_tags vmod_classes_based nn_classes_sense amod_classes_coarser dobj_derive_classes dep_automatmalb_derive aux_automatmalb_can nsubj_automatmalb_that rcmod_algorithm_automatmalb nn_algorithm_search amod_algorithm_greedy det_algorithm_a dobj_present_algorithm nsubj_present_we det_section_this prep_in_were_section prep_in_were_use nsubj_were_classes mark_were_if nn_classes_sense amod_classes_different advcl_affected_were auxpass_affected_be aux_affected_would nsubjpass_affected_rate advmod_affected_how nn_rate_agreement det_rate_the ccomp_find_affected prt_find_out aux_find_to xcomp_like_find aux_like_would nsubj_like_we neg_high_not cop_high_is nsubj_high_agreement mark_high_Since amod_corpus_intersected det_corpus_the prep_on_agreement_corpus amod_agreement_rater-annotator det_agreement_the num_Algorithm_5 dep_agreement_high dep_agreement_Algorithm amod_agreement_inter-annotator nn_agreement_Raw num_agreement_1 nn_agreement_Table num_agreement_0317 dobj_056T_agreement nsubj_056T_I num_I_30,315 number_30,315_17,196 dep_30,315_I number_30,315_191 rcmod_I_056T dep_All_I dep_347_All number_347_0 dep_347_555 dep_347_Verbs dep_347_0 dep_347_582 number_555_0 dep_555_I number_555_17,127 dep_555_9,520 number_9,520_70 num_Verbs_300 number_582_0 dep_582_I number_582_13,188 dep_582_7,676 number_7,676_121 advmod_Nouns_347 nn_Nouns_Avg num_Nouns_~ rcmod_P_present rcmod_P_like dep_P_Nouns dep_\_P nn_\_N det_\_A dep_ords_\ dep_v_ords prep_of_Num_v nn_Num_Type nn_Num_m nn_Num_m nn_Num_m nn_Num_mm nn_Num_m nn_Num_m nn_Num_m nn_Num_m nn_Num_mm nn_Num_m nn_Num_m nn_Num_m nn_Num_m nn_Num_m nn_Num_mm num_Num_\ num_\_10 nn_\_corpus nn_\_DSO det_\_the conj_and_corpus_\ nn_corpus_Semcor det_corpus_the preconj_corpus_both appos_intersection_Num prep_of_intersection_\ prep_of_intersection_corpus det_intersection_the nn_intersection_n num_sentences_5,317 prep_of_subset_sentences det_subset_a prep_on_test_subset amod_test_quick det_test_a dobj_performed_test nsubj_performed_we advmod_performed_where dep_Ng_1996 conj_and_Ng_Lee appos_paper_intersection rcmod_paper_performed dep_paper_Lee dep_paper_Ng amod_paper_earlier poss_paper_our dep_n_paper num_n_~ dobj_reported_n nsubj_reported_tallies det_figure_the prep_with_tallies_figure det_tallies_This rcmod_%_reported num_%_57 cop_%_is nsubj_%_rate amod_P_= prep_by_measured_P mark_measured_as num_sentences_30,315 det_sentences_the dep_rate_measured prep_on_rate_sentences nn_rate_agreement det_rate_The rcmod_word_% det_word_each prep_of_value_word nn_value_~ amod_value_individual det_value_the prep_of_average_value amod_average_mpie dobj_~_average dep_s_~ det_s_a cop_s_is nsubj_s_table det_table_the nn_table_m ccomp_reported_s nsubj_reported_~ nn_~_erage nn_~_~ det_~_a det_~_The ccomp_combined_reported vmod_words_combined det_words_all prep_on_agreement_words dobj_denotes_agreement nsubj_denotes_row nn_row_lass det_row_the predet_row_le nn_row_~ nn_row_wh appos_nouns_xerbs det_nouns_the dep_agreement_denotes prep_on_agreement_nouns dep_denotes_agreement vmod_row_denotes amod_row_first det_row_The dep_first_becond amod_corpus_mtersected det_corpus_the appos_agreement_row prep_on_agreement_corpus amod_agreement_inter-annotator det_agreement_the dobj_summarizes_agreement nsubj_summarizes_Carletta dep_summarizes_agreement mark_summarizes_as num_Table_1 dep_Carletta_Table dep_Carletta_1996 amod_agreement_good nn_agreement_mdmatmg advcl_considered_summarizes auxpass_considered_is advmod_considered_above num_8_0 conj_and_alue_considered prep_of_alue_8 nn_alue_~ nn_alue_Kappa det_alue_A nn_alue_agreement amod_alue_perfect dobj_indicates_considered dobj_indicates_alue nsubj_indicates_alue mark_indicates_whereas prep_of_alue_1 nn_alue_~ nn_alue_Kappa det_alue_a nn_agreement_chance prep_to_due_agreement advmod_due_purely cop_due_is nsubj_due_agreement mark_due_that det_agreement_the ccomp_indicates_due prep_of_alue_0 nn_alue_~ nn_alue_Kappa det_alue_A nn_alue_annotators num_alue_two prep_between_agreement_alue nn_agreement_chance det_agreement_the dobj_measures_agreement nsubj_measures_j dep_measures_M advmod_measures_where conj_and_l_Pe dep_=_Pe dep_=_l amod_j_= rcmod_~_measures nn_~_1-P num_~_~ nn_~_P dep_~_P advmod_~_Then dep_P_~ num_annotator_2 num_sense_3 dep_assigned_indicates dep_assigned_~ agent_assigned_annotator dobj_assigned_sense auxpass_assigned_been aux_assigned_have nsubjpass_assigned_whmh prep_of_number_sentences det_number_the conj_and_annotator_number num_annotator_1 num_sense_3 agent_assigned_number agent_assigned_annotator dobj_assigned_sense auxpass_assigned_been aux_assigned_have nsubjpass_assigned_which rcmod_sentences_assigned prep_of_number_sentences det_number_the prep_of_sum_number det_sum_the cop_sum_be nsubj_sum_Cj ccomp_Let_sum nsubj_Let_agreement amod_Bruce_1998 appos_Bruce_Veroms amod_Bruce_1996 appos_Bruce_Carletta amod_Bruce_1998 conj_and_Bruce_Wmbe dep_agreement_Wmbe dep_agreement_Bruce amod_agreement_raterannotator ccomp_measure_Let aux_measure_to nn_stlcs_~ nn_stlcs_hngu amod_stlcs_computatmnal nn_stlcs_thm nn_stlcs_~ amod_stlcs_w advmod_w_recently advcl_used_indicates dep_used_assigned vmod_used_measure dobj_used_stlcs auxpass_used_been aux_used_has nsubjpass_used_It nn_agreement_chance prep_of_effect_agreement det_effect_the parataxis_takes_used dobj_takes_effect prep_into_takes_account nsubj_takes_which amod_agreement_rater-annotator rcmod_measure_takes prep_of_measure_agreement amod_measure_better det_measure_a cop_measure_is nsubj_measure_a amod_Cohen_1960 appos_a_Cohen rcmod_statistic_measure nn_statistic_Kappa det_statistic_The dep_annotators_statistic num_annotators_two prep_between_agreement_annotators nn_agreement_chance nn_agreement_account prep_into_take_agreement neg_take_not aux_take_does nsubj_take_it mark_take_that ccomp_is_take nsubj_is_Pc advmod_is_where amod_measure_simple det_measure_this prep_of_drawback_measure det_drawback_The nn_drawback_A/N amod_drawback_= appos_Pc_drawback rcmod_Pc_is cop_Pc_Is nsubj_Pc_number amod_annotators_human num_annotators_two prep_between_rate_annotators nn_rate_agreement det_rate_the dobj_quantify_rate aux_quantify_to amod_measure_simple det_measure_a advmod_measure_Then amod_annotators_human num_annotators_two num_annotators_~ dep_b_measure dep_b_annotators nn_b_sense amod_b_identical xcomp_assigned_quantify dobj_assigned_b auxpass_assigned_are nsubjpass_assigned_which rcmod_sentences_assigned prep_of_number_sentences det_number_the cop_number_be nsubj_number_4 dep_Let_Pc nsubj_Let_senses nn_senses_M ccomp_has_Let nsubj_has_w mark_has_that ccomp_Assume_has nsubj_Assume_corpora mark_Assume_from dep_word_w det_word_the nsubj_contains_sentence advmod_contains_where det_sentence_each appos_corpus_word rcmod_corpus_contains poss_corpus_our dobj_m_corpus dep_sentences_m nn_sentences_N nsubj_are_sentences expl_are_there ccomp_Suppose_are nsubj_Suppose_Statistic nn_Statistic_Kappa det_Statistic_The num_Statistic_4 rcmod_study_Suppose amod_study_present poss_study_our appos_m_study dobj_used_m vmod_corpus_used amod_corpus_intersected det_corpus_the dobj_form_corpus nsubj_form_which rcmod_sentences_form num_sentences_30,315 nn_sentences_ymldmg appos_corpora_sentences preconj_corpora_both advcl_match_Assume aux_match_to xcomp_found_match auxpass_found_are nsubjpass_found_ly dobj_containing_verbs vmod_rs_containing num_rs_~ nn_rs_sentence-pa num_rs_17,127 conj_and_nouns_rs nn_nouns_contasnmg nn_nouns_sentence-palrs num_nouns_13,188 prep_of_total_rs prep_of_total_nouns det_total_a appos_matching_total prep_of_method_matching det_method_this dobj_Using_method amod_matches_false det_matches_any prep_out_~_matches dobj_~_eed aux_~_to nsubj_~_they vmod_matches_Using conj_and_matches_~ amod_matches_true cop_matches_are nsubj_matches_they mark_matches_that ccomp_ensure_~ ccomp_ensure_matches aux_ensure_to xcomp_verffied_ensure advmod_verffied_manually advmod_verffied_then auxpass_verffied_are nsubjpass_verffied_release mark_verffied_after amod_matches_potential nn_matches_corpus nn_matches_DSO det_matches_the prep_of_release_matches nn_release_pubhc det_release_the poss_attention_our advcl_brought_verffied prep_to_brought_attention nn_ssas_s num_ssas_~ nn_ssas_Th nn_ssas_release num_ssas_5 num_ssas_1 nn_ssas_WORDNE'I amod_ssas_official det_ssas_the prep_of_variant_ssas nn_variant_shght det_variant_a prep_from_were_variant nn_corpus_DSO det_corpus_the nn_corpus_m dobj_used_corpus nsubj_used_senses dep_used_q dep_used_~ dep_used_WORD dep_used_the nn_senses_ET dep_ly_brought dep_ly_were dep_ly_used rcmod_\_found nn_\_kctua dep_\_i dep_\_These dobj_recorded_\ aux_recorded_s nsubj_recorded_~ advmod_recorded_then nn_~_match amod_~_potential det_~_a nn_sentence_corpus nn_sentence_DSO det_sentence_a prep_in_those_sentence dep_match_those nn_match_sentence det_match_the nn_match_m dep_words_match det_words_the prep_of_%_words conj_or_%_more num_%_75 nn_%_ff vmod_word_ordering nn_word_account prep_into_taking_word nn_sentence_Semcor amod_sentence_remaining det_sentence_each vmod_permd_recorded dep_permd_more dep_permd_% vmod_permd_taking prep_for_permd_sentence appos_permd_hyphen cc_permd_or det_characters_the prep_of_ence_characters conj_or_ence_absence nn_ence_~ amod_ence_pre det_ence_the dep_m_absence dep_m_ence advmod_m_only dep_differ_permd dobj_differ_m nsubj_differ_ff nsubj_differ_cal det_~_the dep_cal_~ conj_or_cal_ff nn_cal_~ amod_cal_ldent cop_exactl_are nsubj_exactl_sentences preconj_sentences_both nn_ff_corpus nn_ff_DSO det_ff_the ccomp_match_differ parataxis_match_exactl prep_from_match_ff dobj_match_one aux_match_to xcomp_considered_match auxpass_considered_is nsubjpass_considered_sense prep_from_sentence_Semcor nsubj_applies_m nn_WoRDNFT_m nn_WoRDNFT_senses amod_WoRDNFT_given det_WoRDNFT_the prep_of_none_WoRDNFT nn_none_ff nn_none_sense num_ff_-1 conj_or_sense_ff num_sense_0 prep_with_tagged_none auxpass_tagged_is nsubjpass_tagged_word det_word_A conj_or_sense_-1 num_sense_0 prep_with_tagged_-1 prep_with_tagged_sense auxpass_tagged_is nsubjpass_tagged_word dobj_tagged_which det_word_a rcmod_m_tagged rcmod_m_tagged nn_m_corpus nn_m_DSO det_m_the dep_m_m rcmod_sentences_applies det_sentences_all dobj_ignored_sentences nsubj_ignored_We tmod_ignored_corpus number_6_1 num_WORDNET_6 prep_of_those_WORDNET prep_to_corpus_those nn_corpus_DSO det_corpus_the rcmod_m_ignored nn_m_senses det_m_the dobj_converted_m advmod_converted_first nsubj_converted_we advcl_converted_were nsubj_converted_m det_sentences_the dobj_match_sentences aux_match_To num_1_15 num_WoRDNET_1 xcomp_were_match prep_from_were_WoRDNET nsubj_were_senses mark_were_whereas nn_corpus_DSO det_corpus_the nn_corpus_m amod_corpus_used dep_senses_corpus det_senses_the num_WORDNET_6 num_WORDNET_1 det_senses_the prep_from_use_WORDNET prep_of_use_senses dobj_makes_use nsubj_makes_versmn advmod_makes_sometimes prep_of_versmn_Semcor amod_versmn_latest det_versmn_The num_versmn_2 ccomp_differed_makes dep_differed_Is nsubj_differed_w advmod_differed_where nn_convention_tokemzatmn amod_convention_different dobj_adopted_convention nsubj_adopted_they nn_corpus_Brown prep_from_came_corpus nsubj_came_portion mark_came_Although preconj_corpora_both prep_of_portion_corpora amod_portion_intersected det_portion_the num_factors_1 amod_factors_following det_factors_the agent_comphcated_factors advcl_s_came vmod_s_comphcated nn_s_~ nn_s_step det_s_This nn_corpus_DSO det_corpus_the prep_in_counterpart_corpus amod_counterpart_corresponding poss_counterpart_its nn_Semcor_m nn_Semcor_sentence det_Semcor_each dobj_match_s prep_to_match_counterpart dobj_match_Semcor aux_match_to xcomp_s_match nsubj_s_~ nn_~_step amod_~_first det_~_the amod_agreement_inter-annotator prep_of_extent_agreement det_extent_the dobj_determine_extent aux_determine_To xcomp_Matching_determine nsubj_Matching_Sentence num_Sentence_3 rcmod_paper_Matching det_paper_this prep_in_agreement_paper amod_agreement_inter-annotator dobj_investigating_agreement prepc_for_set_investigating vmod_data_set poss_data_our prep_as_serves_data nsubj_serves_verbs nsubj_serves_nouns nn_t_~ appos_annotators_t amod_annotators_human prep_of_groups_annotators amod_groups_independent num_groups_two agent_sense-tagged_groups auxpass_sense-tagged_been aux_sense-tagged_has nsubjpass_sense-tagged_pomon mark_sense-tagged_Since amod_pomon_common det_pomon_this dep_nouns_sense-tagged conj_or_nouns_verbs nn_nouns_English amod_nouns_currmg advmod_oc_frequently dep_191_oc conj_and_of.the_segmentartan rcmod_of.the_adopted rcmod_of.the_s rcmod_of.the_serves dep_of.the_191 num_of.the_one prep_into_Is_sentences nsubj_Is_segmentartan nsubj_Is_of.the det_sentence_each nn_sentence_m amod_sentence_sense-tagged cop_sentence_is nsubj_sentence_w dobj_sentence_which nn_w_occurrence nn_w_word det_w_a rcmod_m_differed rcmod_m_sentence rcmod_sentences_converted nn_sentences_corpus nn_sentences_Brown prep_of_consists_sentences advmod_consists_thus nsubj_consists_corpus nsubj_consists_intersection nn_corpus_DSO det_corpus_the nn_corpus_Semcor det_corpus_the conj_and_intersection_corpus prep_of_intersection_corpus det_intersection_The dep_sentence_consists det_sentence_each nn_sentence_m amod_sentence_sense-tagged cop_sentence_been aux_sentence_has nsubj_sentence_occurrence dep_sentence_which nn_occurrence_word num_occurrence_one rcmod_m_sentence nn_m_corpus nn_m_DSO det_m_the prep_in_sentences_m num_sentences_192,800 quantmod_192,800_about prep_of_total_sentences det_total_a dobj_s_total advmod_s_~ expl_s_There appos_sense_sentence num_sense_4 dep_sense_s prep_from_sense_WoRDNET det_sense_a det_sense_th rcmod_~_considered nn_~_w dobj_tagged_~ nsubj_tagged_s nn_s_~ nn_s_w nn_s_occurrence nn_s_word det_s_each auxpass_selected_are dep_selected_tagged csubjpass_selected_acknowledged amod_form_verb amod_form_reflectional amod_form_various poss_form_its dep_m_form conj_and_form_m amod_form_plural amod_form_singular nn_form_m nn_form_w conj_or_singular_plural dobj_containing_m dobj_containing_form vmod_sentences_containing num_verbs_70 conj_and_nouns_verbs num_nouns_121 dep_Enghsh_verbs dep_Enghsh_nouns prep_of_words_Enghsh amod_words_occurring num_words_191 advmod_occurring_frequently prep_of_hst_words det_hst_a nn_w_word det_w_each appos_Journal_sentences prep_from_Journal_hst prep_for_Journal_w nn_Journal_Street nn_Journal_Wall det_Journal_the amod_corpus_Brown det_corpus_the prep_from_drawn_corpus vmod_sentences_drawn conj_and_consists_Journal prep_of_consists_sentences nsubj_consists_corpus nn_corpus_DSO det_corpus_The rcmod_subset_Journal rcmod_subset_consists det_subset_this dep_m_subset nn_erbs_~ conj_and_nouns_adverbs conj_and_nouns_adjectives conj_and_nouns_erbs dep_words_m dep_words_adverbs dep_words_adjectives dep_words_erbs dep_words_nouns nn_words_content det_words_the prep_on_done_words auxpass_done_was nsubjpass_done_senses mark_done_wlth nn_taggmg_Sense dep_files_taggmg nn_files_text num_files_352 num_words_670,000 quantmod_670,000_than mwe_than_more prep_from_consists_files prep_of_consists_words nsubj_consists_us nn_senses_VoRDNET nn_senses_~ prep_with_tagged_senses amod_corpus_Brown det_corpus_the conj_and_subset_consists vmod_subset_tagged prep_of_subset_corpus det_subset_a nsubj_subset_us nn_us_corpus nn_us_Semcor det_us_The amod_annotators_human prep_of_groups_annotators amod_groups_separate num_groups_two rcmod_senses_consists rcmod_senses_subset prep_by_senses_groups prep_of_senses_WORDNET amod_senses_refined det_senses_the ccomp_tagged_done advmod_tagged_independently auxpass_tagged_been aux_tagged_has nsubjpass_tagged_which num_Ng_1997 appos_Ng_Ng conj_and_Ng_1996 conj_and_Ng_Lee appos_corpus_1996 appos_corpus_Lee appos_corpus_Ng nn_corpus_DSO det_corpus_the amod_Miller_1993 dep_Miller_al nn_Miller_et nn_corpus_Semcor nn_corpus_WORDNET det_corpus_the conj_and_intersection_tagged conj_and_intersection_selected conj_and_intersection_corpus dep_intersection_Miller prep_of_intersection_corpus det_intersection_the cop_intersection_is csubj_intersection_acknowledged det_corpus_This nn_corpus_Enghsh prep_of_nouns_corpus conj_and_nouns_verbs amod_nouns_occurring det_nouns_the advmod_occurring_frequently advmod_frequently_most prep_of_instances_verbs prep_of_instances_nouns num_instances_30,000 quantmod_30,000_than mwe_than_more prep_of_corpus_instances amod_corpus_sense-tagged amod_corpus_large det_corpus_a amod_annotators_human prep_of_rate_annotators nn_rate_agreement det_rate_the prep_on_comparing_corpus dobj_comparing_rate amod_agreement_raterannotator prep_of_ssue_agreement nn_ssue_~ det_ssue_the prepc_by_examine_comparing dobj_examine_ssue nsubj_examine_we prep_in_Study_this-paper nn_Study_Case det_Study_A num_Study_2 dep_Kllgamff_1998b conj_Kllgamff_Kflgarrlff conj_Kllgamff_1998a conj_Kllgamff_Kllgarnff conj_Kllgamff_1998c dep_senses_Study dep_senses_Kllgamff nn_senses_word det_senses_the dobj_taggrog_senses nn_lexicographers_ts nn_lexicographers_~ prep_among_agreement_lexicographers nn_agreement_raterannotator det_agreement_the dobj_find_agreement prt_find_out aux_find_to vmod_study_find amod_study_detaded det_study_a dobj_performed_study aux_performed_has nsubj_performed_exerclse nn_exerclse_SENSEVAL det_exerclse_the nn_algorithms_WSD prep_for_performance_instance prep_of_performance_algorithms det_performance_the dobj_compare_performance aux_compare_to amod_ceiling_upper det_ceiling_the xcomp_form_compare prep_against_form_whmh dobj_form_ceiling advmod_form_then aux_form_will nsubj_form_rate amod_annotators_human prep_between_rate_annotators nn_rate_agreement det_rate_The rcmod_annotators_form amod_annotators_human prep_between_agreement_annotators dobj_hlgh_agreement advmod_hlgh_reasonably nsubj_hlgh_us expl_hlgh_there prep_for_hlgh_which rcmod_task_hlgh nn_task_dusamblguatlon det_task_a dobj_define_task aux_define_to xcomp_prefer_define aux_prefer_would num_algorithms_One nn_algorithms_WSD prep_of_evaluatmn_algorithms det_evaluatmn_the prep_for_concern_evaluatmn amod_concern_Important det_concern_an dep_concern_us dep_assignment_concern nn_assignment_sense nn_assignment_word amod_annotators_human prep_between_rate_annotators nn_rate_agreement det_rate_The nn_rate_context nn_rate_m nn_rate_word det_rate_a nn_assignment_sense poss_assignment_their nn_assignment_m dep_dusagree_prefer prep_on_dusagree_assignment prep_to_dusagree_rate dobj_dusagree_assignment advmod_dusagree_genuinely aux_dusagree_may nsubj_dusagree_annotators amod_annotators_human num_annotators_two advmod_fine_Hence advmod_fine_rather cop_fine_be aux_fine_to xcomp_tend_fine nsubj_tend_WoRDNET dep_Miller_1990 appos_WoRDNET_Miller nn_WoRDNET_hke amod_WoRDNET_ctmnary nn_WoRDNET_~ nn_WoRDNET_d det_WoRDNET_a rcmod_m_tend nn_m_word amod_m_used det_m_a advmod_used_commonly prep_for_senses_m amod_senses_different det_senses_the prep_between_sense_senses dep_sense_dustmctmn det_sense_The dep_senses_sense prep_of_number_senses amod_number_large det_number_a dobj_have_number nsubj_have_which rcmod_words_have amod_words_used det_words_the advmod_used_commonly prep_for_so_words advmod_so_especmlly cop_so_is nsubj_so_This amod_This_comprehensive advmod_refined_rather auxpass_refined_be aux_refined_to conj_and_tend_so xcomp_tend_refined nsubj_tend_~ nn_~_dmtmnar amod_~_typical det_~_a rcmod_m_so rcmod_m_tend nn_m_word det_m_a prep_for_hst_m prep_of_hst_senses det_hst_the advmod_speak_Secondly aux_speak_to advmod_speak_so amod_way_different det_way_a npadvmod_way_m dep_way_repo amod_space_semantic det_space_the dobj_carve_space prt_carve_up aux_carve_to xcomp_tend_carve nn_ctlonanes_~ nn_ctlonanes_d amod_ctlonanes_Different nn_ctlonanes_word nn_ctlonanes_ven nn_ctlonanes_~ nn_ctlonanes_g det_ctlonanes_a nn_defimtmns_sense prep_of_set_defimtmns amod_set_same det_set_the dep_have_tend prep_for_have_ctlonanes dobj_have_set aux_have_will nsubj_have_dictionaries mark_have_that num_dictionaries_two det_dictionaries_any ccomp_case_have det_case_the advmod_case_rarely cop_case_is nsubj_case_sentence tmod_case_words dep_case_to advmod_~_rstly nn_~_F nn_~_assignment nn_~_sense nn_~_word prep_for_case_~ det_case_the neg_case_not dep_s_case dobj_s_us nsubj_s_~ det_~_th appos_sentence_it rcmod_sentence_s advmod_sentence_Unfortunately nn_sentence_ven nn_sentence_~ nn_sentence_g det_sentence_a nn_sentence_m amod_tags_part-of-speech amod_tags_assigning prep_on_agreement_tags prep_of_rate_agreement amod_rate_high det_rate_a xcomp_achieve_case dobj_achieve_rate aux_achieve_can nsubj_achieve_tory amod_annotators_human amod_annotators_independent nn_annotators_Tv.o nn_annotators_mformatlon nn_annotators_structure nn_annotators_ntactm amod_annotators_s amod_annotators_partof-speech det_annotators_th nn_annotators_~ nn_annotators_w amod_annotators_annotated nn_annotators_texts conj_and_partof-speech_s prep_of_tory_annotators nn_tory_~ dep_repo_achieve amod_repo_large nn_repo_t nn_~_ide nn_~_~ nn_~_pro num_Marcus_1993 dep_Marcus_al nn_Marcus_et advmod_corpus_~ appos_corpus_Marcus nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_for_word_instance conj_or_word_sentence det_word_a prep_to_assigned_sentence prep_to_assigned_word nn_structure_syntactic nn_structure_speech conj_or_speech_syntactic vmod_part_assigned prep_of_part_structure dep_part_correct det_correct_the dobj_have_part aux_have_to xcomp_means_have nsubj_means_it dobj_means_what prepc_of_cnterm_means amod_cnterm_agreed-upon dep_defined_taggrog ccomp_defined_performed parataxis_defined_dusagree dep_defined_hst dep_defined_speak dep_defined_way conj_and_defined_corpus conj_and_defined_cnterm advmod_defined_well advmod_defined_relatlvely dep_are_corpus dep_are_cnterm dep_are_defined expl_are_there nn_parsing_ntactm nn_parsing_s conj_or_tagging_parsing amod_tagging_part-of-speech dobj_hke_parsing dobj_hke_tagging nsubj_hke_subtasks mark_hke_For nn_subtasks_language amod_subtasks_natural nn_translation_machine conj_and_retrieval_translation nn_retrieval_information parataxis_hke_examine parataxis_hke_are advcl_hke_hke dobj_hke_translation dobj_hke_retrieval nsubj_hke_Impact aux_hke_have dep_hke_v nsubj_hke_WSD mark_hke_in prep_on_Impact_apphcatlons amod_Impact_slgmficant advmod_have_ill rcmod_Advances_hke prep_with_dealt_Advances advmod_dealt_effectively auxpass_dealt_be aux_dealt_must nsubjpass_dealt_samblguatmn mark_dealt_that nn_context_m nn_context_word det_context_a prep_of_meamng_context det_meamng_the dobj_dlscermng_meamng nn_sense_word nn_sense_samblguatmg nn_sense_~ nn_sense_d conj_or_problem_dlscermng prep_of_problem_sense det_problem_the amod_matching_keyword amod_matching_simple prep_beyond_language_matching amod_language_natural dobj_process_language conj_and_understand_process aux_understand_to xcomp_able_process xcomp_able_understand cop_able_be aux_able_to nsubj_able_computers mark_able_for dep_able_order dep_able_In nn_processing_language amod_processing_natural nn_processing_m nn_processing_problem amod_processing_central det_processing_a dep_processing_us appos_samblguatmn_dlscermng appos_samblguatmn_problem dep_samblguatmn_able dep_samblguatmn_processing appos_samblguatmn_WSD nn_samblguatmn_~ nn_samblguatmn_d nn_samblguatmn_sense nn_samblguatmn_word ccomp_acknowledged_dealt advmod_acknowledged_widely nsubj_acknowledged_us nsubj_acknowledged_It
W99-0606	J93-2004	o	Ralph Weischedel et al. 1993	num_al._1993 nn_al._et dep_Weischedel_al. nn_Weischedel_Ralph
W99-0606	J93-2004	o	3 Tagging 3.1 Corpus To facilitate comparison with previous results we used the UPenn Treebank corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Treebank nn_corpus_UPenn det_corpus_the dep_used_Marcus dobj_used_corpus nsubj_used_we dep_used_Corpus amod_results_previous prep_with_comparison_results dobj_facilitate_comparison aux_facilitate_To vmod_Corpus_facilitate num_Corpus_3.1 nn_Corpus_Tagging num_Corpus_3
W99-0611	J93-2004	o	-LRB- 1998 -RRB- present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_of_subset_corpus amod_subset_small det_subset_a prep_on_trained_subset vmod_resolution_trained nn_resolution_pronoun prep_for_model_resolution amod_model_probabilistic det_model_a dep_present_Marcus dobj_present_model nsubj_present_1998
W99-0621	J93-2004	o	These data sets were based on the Wall Street Journal corpus in the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	nn_al._et amod_Marcus_1993 dep_Marcus_al. nn_Treebank_Penn det_Treebank_the nn_corpus_Journal nn_corpus_Street nn_corpus_Wall det_corpus_the dep_based_Marcus prep_in_based_Treebank prep_on_based_corpus auxpass_based_were nsubjpass_based_sets nn_sets_data det_sets_These
W99-0622	J93-2004	o	As an example consider the fiat NP structures that are in the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_Treebank_Penn det_Treebank_the prep_in_are_Treebank nsubj_are_that dep_structures_Marcus rcmod_structures_are nn_structures_NP nn_structures_fiat det_structures_the dobj_consider_structures prep_as_consider_example det_example_an
W99-0623	J93-2004	o	These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et nn_corpus_Journal nn_corpus_Street nn_corpus_Wall nn_corpus_Treebank nn_corpus_Penn det_corpus_the prep_on_results_corpus nn_results_parsing amod_results_reported det_results_the dep_reported_best dep_given_Marcus dobj_given_results aux_given_have nsubj_given_parsers num_parsers_three det_parsers_These ccomp_``_given
W99-0628	J93-2004	o	Some works \ -LSB- Woods et al 1972 \ -RSB- \ -LSB- Boguraev 1979 \ -RSB- \ -LSB- Marcus et al. 1993 \ -RSB- suggested several strategies that based their 231 decision-making on the relationships existing between predicates and argumentswhat \ -LSB- Katz and Fodor 1963 \ -RSB- called selectional restrictions	amod_restrictions_selectional dobj_called_restrictions nsubj_called_Fodor nsubj_called_Katz num_\_1963 appos_Katz_\ conj_and_Katz_Fodor nn_\_argumentswhat conj_and_predicates_\ prep_between_existing_\ prep_between_existing_predicates vmod_relationships_existing det_relationships_the num_decision-making_231 poss_decision-making_their prep_on_based_relationships dobj_based_decision-making nsubj_based_that rcmod_strategies_based amod_strategies_several parataxis_suggested_called dobj_suggested_strategies nsubj_suggested_\ num_\_1993 nn_\_al. dep_Marcus_\ nn_Marcus_et appos_\_Marcus num_\_1979 appos_Boguraev_\ conj_\_\ dep_\_Boguraev num_\_1972 dep_Woods_\ dep_Woods_al nn_Woods_et parataxis_\_suggested dep_\_Woods nsubj_\_works det_works_Some ccomp_``_\
W99-0628	J93-2004	o	A very impor232 Author Best Hindle and Rooth -LRB- 1993 -RRB- 80.0 % Resnik and Hearst -LRB- 1993 -RRB- 83.9 % WN Resnik and Hearst -LRB- 1993 -RRB- 75.0 % Ratnaparkhi et al.	dep_Ratnaparkhi_al. nn_Ratnaparkhi_et dep_%_Ratnaparkhi number_%_75.0 dep_Hearst_% appos_Hearst_1993 conj_and_Resnik_Hearst nn_Resnik_WN amod_Resnik_% nn_Resnik_Hearst nn_Resnik_Resnik number_%_83.9 appos_Hearst_1993 conj_and_Resnik_Hearst dep_%_Hearst dep_%_Resnik num_%_80.0 dep_Rooth_1993 dep_Hindle_% conj_and_Hindle_Rooth amod_Hindle_Best dep_Author_Rooth dep_Author_Hindle amod_Author_impor232 det_Author_A advmod_impor232_very
W99-0628	J93-2004	o	In this data set the 4-tuples of the test and training sets were extracted from Penn Treebank Wall Street Journal \ -LSB- Marcus et al. 1993 \ -RSB-	num_\_1993 advmod_\_al. nn_al._et dep_Marcus_\ nn_\_Journal nn_\_Street nn_\_Wall nn_\_Treebank nn_\_Penn prep_from_extracted_\ auxpass_extracted_were nsubjpass_extracted_4-tuples nn_sets_training conj_and_test_sets det_test_the prep_of_4-tuples_sets prep_of_4-tuples_test det_4-tuples_the dep_set_Marcus ccomp_set_extracted prep_in_set_data det_data_this
W99-0629	J93-2004	o	The data for all our experiments was extracted from the Penn Treebank II Wall Street Journal -LRB- WSJ -RRB- corpus -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_corpus_Marcus nn_corpus_WSJ dep_Journal_corpus nn_Journal_Street nn_Journal_Wall nn_Journal_II nn_Journal_Treebank nn_Journal_Penn det_Journal_the prep_from_extracted_Journal auxpass_extracted_was nsubjpass_extracted_data poss_experiments_our predet_experiments_all prep_for_data_experiments det_data_The
W99-0701	J93-2004	o	Experiments We have conducted a series of lexical acquisition experiments with the above algorithm on largescale English corpora e.g. the Brown corpus \ -LSB- Francis and Kucera 1982 \ -RSB- and the PTB WSJ corpus \ -LSB- Marcus et al. 1993 \ -RSB-	num_\_1993 nn_\_al. dep_Marcus_\ nn_Marcus_et nn_\_corpus nn_\_WSJ nn_\_PTB det_\_the num_\_1982 nn_\_Kucera conj_and_Francis_\ dep_\_Marcus conj_and_\_\ dep_\_\ dep_\_Francis nn_\_corpus nn_\_Brown det_\_the amod_corpora_English amod_corpora_largescale dep_algorithm_\ dep_algorithm_\ dep_algorithm_e.g. prep_on_algorithm_corpora amod_algorithm_above det_algorithm_the nn_experiments_acquisition amod_experiments_lexical prep_of_series_experiments det_series_a prep_with_conducted_algorithm dobj_conducted_series aux_conducted_have nsubj_conducted_We rcmod_Experiments_conducted
W99-0704	J93-2004	o	The WSJNPVP set consists of part-of speech tagged Wall Street Journal material -LRB- Marcus Santorini & Marcinkiewicz 1993 -RRB- supplemented with syntactic tags indicating noun phrase and verb phrase boundaries -LRB- Daelemans et al 1999iii -RRB-	appos_Daelemans_1999iii dep_Daelemans_al nn_Daelemans_et nn_boundaries_phrase dep_verb_Daelemans dobj_verb_boundaries nn_phrase_noun dobj_indicating_phrase vmod_tags_indicating nn_tags_syntactic prep_with_supplemented_tags dep_Marcus_1993 conj_and_Marcus_Marcinkiewicz conj_and_Marcus_Santorini conj_and_material_verb vmod_material_supplemented appos_material_Marcinkiewicz appos_material_Santorini appos_material_Marcus nn_material_Journal nn_material_Street nn_material_Wall dobj_tagged_verb dobj_tagged_material amod_speech_part-of dep_consists_tagged prep_of_consists_speech nsubj_consists_set amod_set_WSJNPVP det_set_The
W99-0706	J93-2004	o	The figures given above were the original -LRB- 1998 -RRB- results for the system in \ -LSB- Argamon et al. 1998 \ -RSB- which came from training and testing on data derived from the Penn Treebank corpus \ -LSB- Marcus et al. 1993 \ -RSB- in which the added null elements -LRB- like null subjects -RRB- were left in	prep_left_in auxpass_left_were nsubjpass_left_elements prep_in_left_which amod_subjects_null prep_like_elements_subjects amod_elements_null amod_elements_added det_elements_the num_\_1993 rcmod_Marcus_left appos_Marcus_\ dep_Marcus_al. nn_Marcus_et nn_\_corpus nn_\_Treebank nn_\_Penn det_\_the prep_from_derived_\ vmod_data_derived conj_and_training_testing prep_on_came_data prep_from_came_testing prep_from_came_training nsubj_came_which num_\_1998 dep_al._\ nn_al._et dep_Argamon_al. prep_in_system_\ det_system_the dep_results_Marcus rcmod_results_came dep_results_Argamon prep_for_results_system vmod_1998_results amod_1998_original det_1998_the cop_1998_were nsubj_1998_figures advmod_given_above vmod_figures_given det_figures_The
W99-0706	J93-2004	p	Our training and test corpora for instance are lessthan-gargantuan compared to such collections as the Penn Treebank \ -LSB- Marcus et al. 1993 \ -RSB-	num_\_1993 appos_Marcus_\ dep_Marcus_al. nn_Marcus_et nn_\_Treebank nn_\_Penn det_\_the prep_as_collections_\ amod_collections_such dep_lessthan-gargantuan_Marcus pobj_lessthan-gargantuan_collections prepc_compared_to_lessthan-gargantuan_to cop_lessthan-gargantuan_are prep_for_lessthan-gargantuan_instance nsubj_lessthan-gargantuan_corpora nsubj_lessthan-gargantuan_training nn_corpora_test conj_and_training_corpora poss_training_Our
W99-0706	J93-2004	o	Many systems -LRB- e.g. the KERNEL system \ -LSB- Palmer et al. 1993 \ -RSB- -RRB- use these relationships as an intermediate form when determining the semantics of syntactically parsed text	amod_text_parsed advmod_text_syntactically prep_of_semantics_text det_semantics_the dobj_determining_semantics advmod_determining_when amod_form_intermediate det_form_an det_relationships_these advcl_use_determining prep_as_use_form dobj_use_relationships dep_use_e.g. nsubj_use_systems num_\_1993 appos_Palmer_\ dep_Palmer_al. nn_Palmer_et num_system_\ nn_system_KERNEL det_system_the dep_e.g._Palmer dep_e.g._system amod_systems_Many
W99-0707	J93-2004	o	The approach is evaluated by cross-validation on the WSJ treebank corpus \ -LSB- Marcus et al. 1993 \ -RSB-	num_\_1993 appos_Marcus_\ dep_Marcus_al. nn_Marcus_et nn_\_corpus nn_\_treebank nn_\_WSJ det_\_the prep_on_cross-validation_\ dep_evaluated_Marcus agent_evaluated_cross-validation auxpass_evaluated_is nsubjpass_evaluated_approach det_approach_The ccomp_``_evaluated
X98-1014	J93-2004	o	Training Data Our source for syntactically annotated training data was the Penn Treebank -LRB- Marcus et al. 1993 -RRB-	amod_Marcus_1993 dep_Marcus_al. nn_Marcus_et dep_Treebank_Marcus nn_Treebank_Penn det_Treebank_the cop_Treebank_was nsubj_Treebank_source nn_data_training amod_data_annotated advmod_data_syntactically prep_for_source_data poss_source_Our nn_source_Data nn_source_Training
A00-1019	J96-1002	o	Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work -LRB- Brown et al. 1993 Berger et al. 1996 Och and Weber 98 Wang and Waibel 98 Wu and Wong 98 -RRB-	amod_Wu_98 conj_and_Wu_Wong dep_Wang_Wong dep_Wang_Wu conj_and_Wang_98 conj_and_Wang_Waibel conj_and_Och_98 conj_and_Och_Waibel conj_and_Och_Wang conj_and_Och_98 conj_and_Och_Weber dep_Berger_Wang dep_Berger_98 dep_Berger_Weber dep_Berger_Och num_Berger_1996 nn_Berger_al. nn_Berger_et dep_al._Berger dep_al._1993 nn_al._et amod_al._Brown amod_work_recent dep_proposed_al. prep_in_proposed_work auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Techniques conj_and_1_2 dep_models_2 dep_models_1 nn_models_IBM det_models_the agent_made_models vmod_assumptions_made nn_assumptions_independence det_assumptions_the dobj_weakening_assumptions prepc_for_Techniques_weakening
A00-2026	J96-1002	o	Our approach differs from the corpus-based surface generation approaches of -LRB- Langkilde and Knight 1998 -RRB- and -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et conj_and_Langkilde_Berger amod_Langkilde_1998 conj_and_Langkilde_Knight prep_of_approaches_Berger prep_of_approaches_Knight prep_of_approaches_Langkilde nn_approaches_generation nn_approaches_surface amod_approaches_corpus-based det_approaches_the prep_from_differs_approaches nsubj_differs_approach poss_approach_Our ccomp_``_differs
A00-2026	J96-1002	o	There are more sophisticated surface generation packages such as FUF/SURGE -LRB- Elhadad and Robin 1996 -RRB- KPML -LRB- Bateman 1996 -RRB- MUMBLE -LRB- Meteer et al. 1987 -RRB- and RealPro -LRB- Lavoie and Rambow 1997 -RRB- which produce natural language text from an abstract semantic representation	amod_representation_semantic amod_representation_abstract det_representation_an nn_text_language amod_text_natural prep_from_produce_representation dobj_produce_text nsubj_produce_which dep_Lavoie_1997 conj_and_Lavoie_Rambow appos_RealPro_Rambow appos_RealPro_Lavoie amod_Meteer_1987 dep_Meteer_al. nn_Meteer_et dep_MUMBLE_Meteer amod_Bateman_1996 appos_KPML_Bateman amod_Elhadad_1996 conj_and_Elhadad_Robin conj_and_FUF/SURGE_RealPro conj_and_FUF/SURGE_MUMBLE conj_and_FUF/SURGE_KPML dep_FUF/SURGE_Robin dep_FUF/SURGE_Elhadad rcmod_packages_produce prep_such_as_packages_RealPro prep_such_as_packages_MUMBLE prep_such_as_packages_KPML prep_such_as_packages_FUF/SURGE nn_packages_generation nn_packages_surface amod_packages_sophisticated advmod_sophisticated_more nsubj_are_packages expl_are_There ccomp_``_are
A00-2026	J96-1002	o	The only trainable approaches -LRB- known to the author -RRB- to surface generation are the purely statistical machine translation -LRB- MT -RRB- systems such as -LRB- Berger et al. 1996 -RRB- and the corpus-based generation system described in -LRB- Langkilde and Knight 1998 -RRB-	amod_Langkilde_1998 conj_and_Langkilde_Knight dep_in_Knight dep_in_Langkilde prep_described_in vmod_system_described nn_system_generation amod_system_corpus-based det_system_the conj_and_Berger_system amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_such_as_systems_system prep_such_as_systems_Berger dep_systems_MT dep_translation_systems nn_translation_machine amod_translation_statistical advmod_translation_purely det_translation_the cop_translation_are nsubj_translation_approaches nn_generation_surface det_author_the prep_to_known_author prep_to_approaches_generation dep_approaches_known amod_approaches_trainable amod_approaches_only det_approaches_The
A00-2026	J96-1002	o	The MT systems of -LRB- Berger et al. 1996 -RRB- learn to generate text in the target language straight from the source language without the aid of an explicit semantic representation	amod_representation_semantic amod_representation_explicit det_representation_an prep_of_aid_representation det_aid_the nn_language_source det_language_the prep_from_straight_language nn_language_target det_language_the prep_in_text_language advmod_generate_straight dobj_generate_text aux_generate_to prep_without_learn_aid xcomp_learn_generate nsubj_learn_systems amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_of_systems_Berger nn_systems_MT det_systems_The
A00-2026	J96-1002	o	The form of the maximum entropy probability model is identical to the one used in -LRB- Berger et al. 1996 Ratnaparkhi 1998 -RRB- k f $ -LRB- wi,wi-1 wi-2 at ~ ri -RRB- YIj = I Otj p -LRB- wilwi-l wi-2 attri -RRB- = Z -LRB- Wi-l wi-2 attri -RRB- k to t j = l where wi ranges over V t3 stop	nn_t3_V prep_over_ranges_t3 nsubj_ranges_wi advmod_ranges_where rcmod_l_ranges dobj_=_l dep_j_= nn_j_t prep_to_k_j nn_k_attri appos_Wi-l_k appos_Wi-l_wi-2 nn_Wi-l_Z amod_Wi-l_= nn_Wi-l_p appos_wilwi-l_attri appos_wilwi-l_wi-2 dep_p_wilwi-l nn_p_Otj num_p_I dep_=_Wi-l amod_YIj_= num_ri_~ prep_at_wi,wi-1_ri appos_wi,wi-1_wi-2 dep_$_wi,wi-1 dep_$_f nn_f_k dep_Ratnaparkhi_1998 dep_Berger_YIj dep_Berger_$ dep_Berger_Ratnaparkhi appos_Berger_1996 dep_Berger_al. nn_Berger_et prep_in_used_Berger vmod_one_used det_one_the dep_identical_stop prep_to_identical_one cop_identical_is nsubj_identical_form nn_model_probability nn_model_entropy nn_model_maximum det_model_the prep_of_form_model det_form_The
A00-2026	J96-1002	o	The features used in NLG2 are described in the next section and the feature weights aj obtained from the Improved Iterative Scaling algorithm -LRB- Berger et al. 1996 -RRB- are set to maximize the likelihood of the training data	nn_data_training det_data_the prep_of_likelihood_data det_likelihood_the dobj_maximize_likelihood aux_maximize_to xcomp_set_maximize auxpass_set_are ccomp_set_weights ccomp_set_described amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_algorithm_Scaling nn_algorithm_Iterative nn_algorithm_Improved det_algorithm_the prep_from_obtained_algorithm vmod_aj_obtained dep_weights_aj nn_weights_feature det_weights_the amod_section_next det_section_the dep_described_Berger conj_and_described_weights prep_in_described_section auxpass_described_are nsubjpass_described_features prep_in_used_NLG2 vmod_features_used det_features_The
A00-2031	J96-1002	o	This is concordant with the usage in the maximum entropy literature -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_literature_entropy nn_literature_maximum det_literature_the prep_in_usage_literature det_usage_the dep_concordant_Berger prep_with_concordant_usage cop_concordant_is nsubj_concordant_This ccomp_``_concordant
A97-1056	J96-1002	o	However the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works -LRB- e.g. -LRB- Bruce and Wiebe 1994a -RRB- -LRB- Gale et al. 1992 -RRB- -LRB- Leacock et al. 1993 -RRB- and -LRB- Mooney 1996 -RRB- -RRB-	dep_Mooney_1996 num_Leacock_1993 dep_Leacock_al. nn_Leacock_et amod_Gale_1992 dep_Gale_al. nn_Gale_et dep_Bruce_1994a conj_and_Bruce_Wiebe conj_and_e.g._Mooney appos_e.g._Leacock appos_e.g._Gale appos_e.g._Wiebe appos_e.g._Bruce dep_works_Mooney dep_works_e.g. amod_works_other prep_of_variety_works det_variety_a pobj_in_variety conj_and_here_in preconj_here_both amod_disambiguation_word-sense advmod_perform_in advmod_perform_here prep_for_perform_disambiguation advmod_perform_well aux_perform_to xcomp_found_perform auxpass_found_been aux_found_has nsubjpass_found_classifier advmod_found_However nn_classifier_Bayes amod_classifier_Naive det_classifier_the
A97-1056	J96-1002	o	Maximum Entropy models have been used to express the interactions among multiple feature variables -LRB- e.g. -LRB- Berger et al. 1996 -RRB- -RRB- but within this framework no systematic study of interactions has been proposed	auxpass_proposed_been aux_proposed_has nsubjpass_proposed_study prep_within_proposed_framework prep_of_study_interactions amod_study_systematic neg_study_no det_framework_this amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_e.g._Berger dep_variables_e.g. nn_variables_feature amod_variables_multiple prep_among_interactions_variables det_interactions_the dobj_express_interactions aux_express_to conj_but_used_proposed xcomp_used_express auxpass_used_been aux_used_have nsubjpass_used_models nn_models_Entropy nn_models_Maximum
A97-1056	J96-1002	o	-LRB- Pedersen et al. 1996 -RRB- and -LRB- Zipf 1935 -RRB- -RRB-	dep_Zipf_1935 conj_and_Pedersen_Zipf amod_Pedersen_1996 dep_Pedersen_al. nn_Pedersen_et dep_''_Zipf dep_''_Pedersen
A97-1056	J96-1002	o	Because their joint distributions have such closed-form expressions the parameters can be estimated directly from the training data without the need for an iterative fitting procedure -LRB- as is required for example to estimate the parameters of maximum entropy models -LRB- Berger et al. 1996 -RRB- -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_models_entropy nn_models_maximum prep_of_parameters_models det_parameters_the dobj_estimate_parameters aux_estimate_to dep_required_Berger xcomp_required_estimate prep_for_required_example auxpass_required_is advmod_required_as amod_procedure_fitting amod_procedure_iterative det_procedure_an prep_for_need_procedure det_need_the nn_data_training det_data_the dep_estimated_required prep_without_estimated_need prep_from_estimated_data advmod_estimated_directly auxpass_estimated_be aux_estimated_can nsubjpass_estimated_parameters advcl_estimated_have det_parameters_the amod_expressions_closed-form amod_expressions_such dobj_have_expressions nsubj_have_distributions mark_have_Because amod_distributions_joint poss_distributions_their
A97-1056	J96-1002	o	The significance of G 2 based on the exact conditional distribution does not rely on an asymptotic approximation and is accurate for sparse and skewed data samples -LRB- Pedersen et al. 1996 -RRB- 4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression IC ~ = G 2 ~ x dof -LRB- 3 -RRB- where G ~ and dof are defined above	advmod_defined_above auxpass_defined_are nsubjpass_defined_dof nsubjpass_defined_~ advmod_defined_where conj_and_~_dof nn_~_G appos_dof_3 nn_dof_x num_dof_~ nn_dof_G number_~_2 dep_=_dof rcmod_~_defined amod_~_= conj_IC_~ amod_expression_following det_expression_the dobj_have_expression nsubj_have_criteria mark_have_as nn_criteria_information advcl_known_have vmod_criteria_known nn_criteria_evaluation nn_criteria_model dep_family_IC prep_of_family_criteria det_family_The dep_criteria_family nn_criteria_Information num_criteria_4.2 dep_Pedersen_criteria amod_Pedersen_1996 dep_Pedersen_al. nn_Pedersen_et nn_samples_data amod_samples_skewed amod_samples_sparse conj_and_sparse_skewed prep_for_accurate_samples cop_accurate_is nsubj_accurate_significance amod_approximation_asymptotic det_approximation_an dep_rely_Pedersen conj_and_rely_accurate prep_on_rely_approximation neg_rely_not aux_rely_does nsubj_rely_significance amod_distribution_conditional amod_distribution_exact det_distribution_the prep_on_based_distribution num_G_2 vmod_significance_based prep_of_significance_G det_significance_The
A97-1056	J96-1002	o	5 Experimental Data The sense-tagged text and feature set used in these experiments are the same as in -LRB- Bruce et al. 1996 -RRB-	dep_al._1996 nn_al._et nn_al._Bruce dep_in_al. pcomp_as_in prep_same_as det_same_the cop_same_are nsubj_same_Data det_experiments_these prep_in_used_experiments nn_set_feature vmod_text_used conj_and_text_set amod_text_sense-tagged det_text_The dep_Data_set dep_Data_text amod_Data_Experimental num_Data_5 ccomp_``_same
C00-1060	J96-1002	o	We report that our parsing framework achieved high accuracy -LRB- 88.6 % -RRB- in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar SLUNG -LRB- Mitsuishi et al. 1998 -RRB- and the maximum entropy method -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_method_Berger nn_method_entropy nn_method_maximum det_method_the amod_Mitsuishi_1998 dep_Mitsuishi_al. nn_Mitsuishi_et conj_and_SLUNG_method dep_SLUNG_Mitsuishi amod_grammar_Japanese amod_grammar_HPSG-based amod_grammar_underspecified det_grammar_an prep_of_combination_grammar det_combination_a prep_with_analysis_combination prep_of_analysis_Japanese nn_analysis_dependency num_%_88.6 appos_accuracy_% amod_accuracy_high dobj_achieved_method dobj_achieved_SLUNG prep_in_achieved_analysis dobj_achieved_accuracy nsubj_achieved_framework mark_achieved_that nn_framework_parsing poss_framework_our ccomp_report_achieved nsubj_report_We
C00-1060	J96-1002	o	2.2 Statistical Approaches with a grmnnmr There have been nlally l -RRB- rOl -RRB- osals tbr statistical t ` rameworks particularly designed tbr 1 -RRB- arsers with hand-crafted grmnmars -LRB- Schal -RRB- es 1992 Briscoe and Carroll 1993 Abney 1996 Inui et al. 1 -RRB- 97 -RRB-	num_Inui_97 num_Inui_1 nn_Inui_al. nn_Inui_et num_Abney_1996 dep_Briscoe_Inui conj_and_Briscoe_Abney conj_and_Briscoe_1993 conj_and_Briscoe_Carroll dep_es_Abney dep_es_1993 dep_es_Carroll dep_es_Briscoe conj_es_1992 dep_grmnmars_es appos_grmnmars_Schal amod_grmnmars_hand-crafted prep_with_arsers_grmnmars dep_arsers_tbr num_tbr_1 dobj_designed_arsers advmod_designed_particularly vmod_t_designed dep_t_rameworks amod_t_statistical nn_t_tbr nn_t_osals dep_rOl_t nn_rOl_l advmod_rOl_nlally cop_rOl_been aux_rOl_have expl_rOl_There det_grmnnmr_a dep_Approaches_rOl prep_with_Approaches_grmnnmr amod_Approaches_Statistical num_Approaches_2.2 dep_``_Approaches
C00-1061	J96-1002	o	1 2 show the examples of w ~ rious transliterations in KTSET 2.0 -LRB- Park et al. 1996 -RRB-	amod_Park_1996 dep_Park_al. nn_Park_et dep_KTSET_Park num_KTSET_2.0 amod_transliterations_rious nn_transliterations_~ nn_transliterations_w prep_in_examples_KTSET prep_of_examples_transliterations det_examples_the dobj_show_examples nsubj_show_2 dep_show_1
C00-1064	J96-1002	o	4 Maximum Entropy To explain our method we l -RRB- riefly des -LRB- ribe the con -LRB- ept of maximum entrol -RRB- y. Recently many al -RRB- lnoaches l -RRB- ased on the maximum entroi -RRB- y lnodel have t -RRB- een applied to natural language processing -LRB- Berger eL al. \ -RSB- 994 Berger et al. 1996 Pietra et al. 1997 -RRB-	num_Pietra_1997 nn_Pietra_al. nn_Pietra_et dep_Berger_Pietra num_Berger_1996 nn_Berger_al. nn_Berger_et appos_al._\ nn_al._eL nn_al._Berger num_processing_994 dep_processing_al. nn_processing_language amod_processing_natural prep_to_applied_processing nsubj_applied_een nn_een_t ccomp_have_applied nsubj_have_lnodel nn_lnodel_y nn_entroi_maximum det_entroi_the conj_ased_Berger parataxis_ased_have prep_on_ased_entroi nsubj_ased_l nn_l_lnoaches nn_l_des amod_al_many advmod_y._Recently nn_entrol_maximum prep_of_ept_entrol dep_con_y. dep_con_ept det_con_the dep_ribe_al dobj_ribe_con dep_des_ribe advmod_des_riefly nn_des_l dep_des_we dep_des_Entropy poss_method_our dobj_explain_method aux_explain_To vmod_Entropy_explain nn_Entropy_Maximum num_Entropy_4
C00-1064	J96-1002	o	We referred to the studies of -LRB- Berger et al. 1996 Pietra e.t al. 1997 -RRB-	appos_al._1997 nn_al._e.t nn_al._Pietra dep_Berger_al. amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_of_studies_Berger det_studies_the prep_to_referred_studies nsubj_referred_We
C00-1064	J96-1002	o	Wu -LRB- 1996 -RRB- adopted chammls that eliminate syntactically unlikely alignments and Wang et al.	dep_Wang_al. nn_Wang_et conj_and_alignments_Wang amod_alignments_unlikely advmod_unlikely_syntactically dobj_eliminate_Wang dobj_eliminate_alignments nsubj_eliminate_that rcmod_chammls_eliminate dobj_adopted_chammls nsubj_adopted_Wu appos_Wu_1996
C00-1064	J96-1002	o	Thus a lot of alignment techniques have been suggested at the sentence -LRB- Gale et al. 1993 -RRB- phrase -LRB- Shin et al. 1996 -RRB- nomt t -RRB- hrase -LRB- Kupiec 1993 -RRB- word -LRB- Brown et al. 1993 Berger et al. 1996 Melamed 1997 -RRB- collocation -LRB- Smadja et al. 1996 -RRB- and terminology level	nn_level_terminology amod_Smadja_1996 dep_Smadja_al. nn_Smadja_et dep_collocation_Smadja dep_Melamed_1997 conj_and_Berger_level conj_and_Berger_collocation dep_Berger_Melamed num_Berger_1996 nn_Berger_al. nn_Berger_et num_al._1993 nn_al._et dep_Brown_al. amod_word_Brown amod_Kupiec_1993 appos_hrase_word dep_hrase_Kupiec nn_t_nomt num_Shin_1996 dep_Shin_al. nn_Shin_et appos_phrase_Shin amod_Gale_1993 dep_Gale_al. nn_Gale_et dep_sentence_level dep_sentence_collocation dep_sentence_Berger dep_sentence_hrase appos_sentence_t appos_sentence_phrase dep_sentence_Gale det_sentence_the dep_suggested_sentence advmod_suggested_at auxpass_suggested_been aux_suggested_have nsubjpass_suggested_lot advmod_suggested_Thus nn_techniques_alignment prep_of_lot_techniques det_lot_a
C00-1082	J96-1002	o	a. 2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers -LRB- Berger et al. 1996 Ratnaparkhi 1996 Ratnaparkhi 1997 Borthwick el al. 1998 Uchimoto et al. 1999 -RRB-	num_Uchimoto_1999 nn_Uchimoto_al. nn_Uchimoto_et num_al._1998 nn_el_Borthwick num_Ratnaparkhi_1997 dep_Ratnaparkhi_Uchimoto conj_Ratnaparkhi_al. conj_Ratnaparkhi_el conj_Ratnaparkhi_Ratnaparkhi amod_Ratnaparkhi_1996 dep_Berger_Ratnaparkhi appos_Berger_1996 dep_Berger_al. nn_Berger_et dep_researchers_Berger amod_researchers_many agent_used_researchers auxpass_used_been aux_used_has nsubjpass_used_method nn_conditions_data amod_conditions_sparse conj_and_useful_used prep_with_useful_conditions cop_useful_is nsubj_useful_method amod_method_maximum-entropy det_method_The dep_method_used dep_method_useful amod_method_Maximum-entropy num_method_2 pobj_a._method dep_``_a.
C00-2124	J96-1002	o	For every class the weights of the active features are combined and the best scoring class is chosen -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et auxpass_chosen_is nsubjpass_chosen_class amod_class_scoring amod_class_best det_class_the conj_and_combined_chosen auxpass_combined_are nsubjpass_combined_weights amod_features_active det_features_the prep_of_weights_features det_weights_the rcmod_class_chosen rcmod_class_combined det_class_every dep_For_Berger pobj_For_class dep_``_For
C00-2126	J96-1002	o	This allows us to compute the conditional probability as follows -LRB- Berger et al. 1996 -RRB- ag ~ -LRB- h f -RRB- P -LRB- / Ih -RRB- 1L ' -LRB- 2 -RRB- Z -LRB- h -RRB- ct i	nn_i_ct nn_i_Z dep_i_2 appos_Z_h dep_P_1L dep_P_Ih dep_P_f dep_h_i dep_h_P dep_~_h nn_~_ag amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_follows_Berger mark_follows_as amod_probability_conditional det_probability_the advcl_compute_follows dobj_compute_probability aux_compute_to dep_allows_~ xcomp_allows_compute dobj_allows_us nsubj_allows_This
C02-1064	J96-1002	o	We implemented these models within an maximum entropy framework -LRB- Berger et al. 1996 Ristad 1997 Ristad 1998 -RRB-	amod_Ristad_1998 dep_Ristad_Ristad amod_Ristad_1997 dep_Berger_Ristad appos_Berger_1996 dep_Berger_al. nn_Berger_et nn_framework_entropy nn_framework_maximum det_framework_an det_models_these dep_implemented_Berger prep_within_implemented_framework dobj_implemented_models nsubj_implemented_We ccomp_``_implemented
C02-1143	J96-1002	o	We used a maximummatching algorithm and a dictionary compiled from the CTB -LRB- Sproat et al. 1996 Xue 2001 -RRB- to do segmentation and trained a maximum entropy part-ofspeech tagger -LRB- Ratnaparkhi 1998 -RRB- and TAG-based parser -LRB- Bikel and Chiang 2000 -RRB- on the CTB to do tagging and parsing .4 Then the same feature extraction and model-training was done for the PDN corpus as for the CTB	det_CTB_the nn_corpus_PDN det_corpus_the pobj_done_CTB prepc_as_for_done_for prep_for_done_corpus auxpass_done_was nsubjpass_done_trained nsubjpass_done_do conj_and_extraction_model-training nn_extraction_feature amod_extraction_same det_extraction_the advmod_extraction_Then num_extraction_.4 dobj_parsing_model-training dobj_parsing_extraction conj_and_tagging_parsing dobj_do_parsing dobj_do_tagging aux_do_to det_CTB_the dep_Bikel_2000 conj_and_Bikel_Chiang appos_parser_Chiang appos_parser_Bikel amod_parser_TAG-based amod_Ratnaparkhi_1998 conj_and_tagger_parser dep_tagger_Ratnaparkhi amod_tagger_part-ofspeech amod_tagger_entropy nn_tagger_maximum det_tagger_a vmod_trained_do prep_on_trained_CTB dobj_trained_parser dobj_trained_tagger conj_and_do_trained dobj_do_segmentation aux_do_to dep_Xue_2001 rcmod_Sproat_done dep_Sproat_Xue appos_Sproat_1996 dep_Sproat_al. nn_Sproat_et dep_CTB_Sproat det_CTB_the prep_from_compiled_CTB vmod_dictionary_compiled det_dictionary_a conj_and_algorithm_dictionary amod_algorithm_maximummatching det_algorithm_a dobj_used_dictionary dobj_used_algorithm nsubj_used_We
C02-1143	J96-1002	o	Under the maximum entropy framework -LRB- Berger et al. 1996 -RRB- evidence from different features can be combined with no assumptions of feature independence	nn_independence_feature prep_of_assumptions_independence neg_assumptions_no prep_with_combined_assumptions auxpass_combined_be aux_combined_can nsubjpass_combined_evidence dep_combined_Berger prep_under_combined_framework amod_features_different prep_from_evidence_features amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_framework_entropy nn_framework_maximum det_framework_the rcmod_``_combined
C02-2019	J96-1002	o	One is to find unknown words from corpora and put them into a dictionary -LRB- e.g. -LRB- Mori and Nagao 1996 -RRB- -RRB- and the other is to estimate a model that can identify unknown words correctly -LRB- e.g. -LRB- Kashioka et al. 1997 Nagata 1999 -RRB- -RRB-	dep_Nagata_1999 dep_Kashioka_Nagata amod_Kashioka_1997 dep_Kashioka_al. nn_Kashioka_et appos_e.g._Kashioka dep_correctly_e.g. amod_words_unknown advmod_identify_correctly dobj_identify_words aux_identify_can nsubj_identify_that rcmod_model_identify det_model_a dobj_estimate_model aux_estimate_to xcomp_is_estimate nsubj_is_other det_other_the dep_Mori_1996 conj_and_Mori_Nagao appos_e.g._Nagao appos_e.g._Mori dep_dictionary_e.g. det_dictionary_a prep_into_put_dictionary dobj_put_them nsubj_put_One prep_from_words_corpora amod_words_unknown dobj_find_words aux_find_to conj_and_is_is conj_and_is_put xcomp_is_find nsubj_is_One ccomp_``_is ccomp_``_put ccomp_``_is
C02-2019	J96-1002	o	-LRB- 1 -RRB- Here has -LRB- h x -RRB- is a binary function that returns true if the history h has feature x.Inour experiments we focused on such information as whether or not a string is found in a dictionary the length of the string what types of characters are used in the string and what part-of-speech the adjacent morpheme is Given a set of features and some training data the M.E. estimation process produces a model which is represented as follows -LRB- Berger et al. 1996 Ristad 1997 Ristad 1998 -RRB- P -LRB- f | h -RRB- = producttext i g i -LRB- h f -RRB- i Z -LRB- h -RRB- -LRB- 2 -RRB- Z -LRB- h -RRB- = summationdisplay f productdisplay i g i -LRB- h f -RRB- i.	nn_i._Z nn_i._g nsubj_f_Z dep_f_2 nn_h_i nn_h_g nn_g_i nn_g_productdisplay nn_g_f nn_g_summationdisplay dep_=_h amod_Z_= appos_Z_h dep_Z_f appos_Z_h nn_Z_i nn_Z_h appos_h_f nn_h_i nn_g_i nn_g_producttext dep_=_i. nn_h_| nn_h_f amod_P_= appos_P_h dep_Ristad_1998 dep_Ristad_P dep_Ristad_Ristad amod_Ristad_1997 dep_Berger_Ristad appos_Berger_1996 dep_Berger_al. nn_Berger_et dep_follows_Berger mark_follows_as advcl_represented_follows auxpass_represented_is nsubjpass_represented_which rcmod_model_represented det_model_a dobj_produces_model nsubj_produces_process prep_produces_Given nn_process_estimation nn_process_M.E. det_process_the nn_data_training det_data_some conj_and_set_data prep_of_set_features det_set_a pobj_Given_data pobj_Given_set amod_morpheme_adjacent det_morpheme_the dep_part-of-speech_is dobj_part-of-speech_morpheme nsubj_part-of-speech_what det_string_the prep_in_used_string auxpass_used_are nsubjpass_used_length advcl_used_found prep_of_types_characters det_types_what det_string_the appos_length_types prep_of_length_string det_length_the det_dictionary_a prep_in_found_dictionary auxpass_found_is nsubjpass_found_string mark_found_as det_string_a neg_string_not cc_string_or mark_string_whether rcmod_information_used amod_information_such parataxis_focused_produces conj_and_focused_part-of-speech prep_on_focused_information nsubj_focused_we ccomp_focused_function nn_experiments_x.Inour nn_experiments_feature dobj_has_experiments nsubj_has_h mark_has_if nn_h_history det_h_the advcl_returns_has acomp_returns_true nsubj_returns_that rcmod_function_returns amod_function_binary det_function_a cop_function_is nsubj_function_h aux_function_has appos_h_x advmod_has_Here dep_has_1
C04-1017	J96-1002	o	In previous research on splitting sentences many methods have been based on word-sequence characteristics like N-gram -LRB- Lavie et al. 1996 Berger et al. 1996 Nakajima and Yamamoto 2001 Gupta et al. 2002 -RRB-	num_Gupta_2002 nn_Gupta_al. nn_Gupta_et dep_Nakajima_Gupta conj_and_Nakajima_2001 conj_and_Nakajima_Yamamoto dep_Berger_2001 dep_Berger_Yamamoto dep_Berger_Nakajima num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Lavie_Berger appos_Lavie_1996 dep_Lavie_al. nn_Lavie_et prep_like_characteristics_N-gram nn_characteristics_word-sequence dep_based_Lavie prep_on_based_characteristics auxpass_based_been aux_based_have nsubjpass_based_methods prep_on_based_sentences prep_in_based_research amod_methods_many amod_sentences_splitting amod_research_previous
C04-1067	J96-1002	o	The candidates of unknown words can be generated by heuristic rules -LRB- Matsumoto et al. 2001 -RRB- or statistical word models which predict the probabilities for any strings to be unknown words -LRB- Sproat et al. 1996 Nagata 1999 -RRB-	amod_Nagata_1999 dep_Sproat_Nagata appos_Sproat_1996 dep_Sproat_al. nn_Sproat_et amod_words_unknown cop_words_be aux_words_to vmod_strings_words det_strings_any prep_for_probabilities_strings det_probabilities_the dobj_predict_probabilities nsubj_predict_which nn_models_word amod_models_statistical dep_Matsumoto_Sproat rcmod_Matsumoto_predict conj_or_Matsumoto_models dep_Matsumoto_2001 dep_Matsumoto_al. nn_Matsumoto_et nn_rules_heuristic dep_generated_models dep_generated_Matsumoto agent_generated_rules auxpass_generated_be aux_generated_can nsubjpass_generated_candidates amod_words_unknown prep_of_candidates_words det_candidates_The ccomp_``_generated
C04-1067	J96-1002	o	In the above equation P -LRB- ti -RRB- and P -LRB- wi t -RRB- are estimated by the maximum-likelihood method and the probability of a POC tag ti given a character wi -LRB- P -LRB- tijwi ti 2 TPOC -RRB- -RRB- is estimated using ME models -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_models_ME dobj_using_models xcomp_estimated_using auxpass_estimated_is nsubjpass_estimated_probability num_TPOC_2 nn_TPOC_ti dep_tijwi_TPOC dep_P_tijwi appos_wi_P nn_wi_character det_wi_a pobj_given_wi nn_ti_tag nn_ti_POC det_ti_a prep_probability_given prep_of_probability_ti det_probability_the nn_method_maximum-likelihood det_method_the dep_estimated_Berger conj_and_estimated_estimated agent_estimated_method auxpass_estimated_are nsubjpass_estimated_P nsubjpass_estimated_P prep_in_estimated_equation dep_wi_t dep_P_wi conj_and_P_P appos_P_ti amod_equation_above det_equation_the ccomp_``_estimated ccomp_``_estimated
C04-1112	J96-1002	o	The statistical classifier used in the experiments reported in this paper is a maximum entropy classifier -LRB- Berger et al. 1996 Ratnaparkhi 1997b -RRB-	appos_Ratnaparkhi_1997b dep_Berger_Ratnaparkhi appos_Berger_1996 dep_Berger_al. nn_Berger_et appos_classifier_Berger nn_classifier_entropy nn_classifier_maximum det_classifier_a cop_classifier_is nsubj_classifier_classifier det_paper_this prep_in_reported_paper vmod_experiments_reported det_experiments_the prep_in_used_experiments vmod_classifier_used amod_classifier_statistical det_classifier_The
C04-1112	J96-1002	p	Furthermore good results have been produced in other areas of NLP research using maximum entropy techniques -LRB- Berger et al. 1996 Koeling 2001 Ratnaparkhi 1997a -RRB-	appos_Ratnaparkhi_1997a dep_Koeling_Ratnaparkhi amod_Koeling_2001 dep_Berger_Koeling appos_Berger_1996 dep_Berger_al. nn_Berger_et nn_techniques_entropy nn_techniques_maximum dep_using_Berger dobj_using_techniques nn_research_NLP prep_of_areas_research amod_areas_other xcomp_produced_using prep_in_produced_areas auxpass_produced_been aux_produced_have nsubjpass_produced_results advmod_produced_Furthermore amod_results_good ccomp_``_produced
C04-1179	J96-1002	o	3 Maximum Entropy ME models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible -LRB- Berger et al. 1996 -RRB-	advmod_1996_al. nn_al._et num_Berger_1996 dep_possible_Berger dep_possible_as dep_is_possible prep_as_is_uniform advmod_is_otherwise nsubj_is_model det_evidence_the agent_imposed_evidence vmod_constraints_imposed prep_of_set_constraints det_set_the prep_with_consistent_set cop_consistent_is nsubj_consistent_that conj_but_one_is rcmod_one_consistent det_one_the cop_one_is nsubj_one_model mark_one_that amod_model_best det_model_the ccomp_intuition_is ccomp_intuition_one det_intuition_the dobj_implement_intuition nsubj_implement_models nn_models_ME nn_models_Entropy nn_models_Maximum num_models_3
C04-1204	J96-1002	o	Following recent research about disambiguation models on linguistic grammars -LRB- Abney 1997 Johnson et al. 1999 Riezler et al. 2002 Clark and Curran 2003 Miyao et al. 2003 Malouf and van Noord 2004 -RRB- we apply a log-linear model or maximum entropy model -LRB- Berger et al. 1996 -RRB- on HPSG derivations	nn_derivations_HPSG amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum conj_or_model_model amod_model_log-linear det_model_a prep_on_apply_derivations dep_apply_Berger dobj_apply_model dobj_apply_model nsubj_apply_we prep_following_apply_research nn_Noord_van dep_Malouf_2004 conj_and_Malouf_Noord num_Miyao_2003 nn_Miyao_al. nn_Miyao_et num_Clark_2003 conj_and_Clark_Curran num_Riezler_2002 nn_Riezler_al. nn_Riezler_et num_Johnson_1999 nn_Johnson_al. nn_Johnson_et dep_Abney_Noord dep_Abney_Malouf dep_Abney_Miyao dep_Abney_Curran dep_Abney_Clark dep_Abney_Riezler dep_Abney_Johnson dep_Abney_1997 appos_grammars_Abney amod_grammars_linguistic prep_on_models_grammars nn_models_disambiguation prep_about_research_models amod_research_recent
C08-1041	J96-1002	p	The maximum entropy approach -LRB- Berger et al. 1996 -RRB- is known to be well suited to solve the classification problem	nn_problem_classification det_problem_the dobj_solve_problem aux_solve_to xcomp_suited_solve advmod_suited_well auxpass_suited_be aux_suited_to xcomp_known_suited auxpass_known_is nsubjpass_known_approach amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_approach_Berger amod_approach_entropy nn_approach_maximum det_approach_The
C08-1079	J96-1002	o	3 Implementation 3.1 Pronoun resolution model We built a machine learning based pronoun resolution engine using a Maximum Entropy ranker model -LRB- Berger et al. 1996 -RRB- similar with Denis and Baldridges model -LRB- Denis and Baldridge 2007 -RRB-	dep_Denis_2007 conj_and_Denis_Baldridge dep_model_Baldridge dep_model_Denis nn_model_Baldridges conj_and_Denis_model prep_with_similar_model prep_with_similar_Denis amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_ranker nn_model_Entropy nn_model_Maximum det_model_a dobj_using_model amod_engine_similar dep_engine_Berger vmod_engine_using nn_engine_resolution nn_engine_pronoun amod_engine_based dep_learning_engine nn_learning_machine det_learning_a dobj_built_learning nsubj_built_We rcmod_model_built nn_model_resolution nn_model_Pronoun num_model_3.1 nn_model_Implementation num_model_3 dep_``_model
C08-1083	J96-1002	o	Preparing an aligned abbreviation corpus we obtain the optimal combination of the features by using the maximum entropy framework -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_framework_entropy nn_framework_maximum det_framework_the dobj_using_framework det_features_the prep_of_combination_features amod_combination_optimal det_combination_the dep_obtain_Berger prepc_by_obtain_using dobj_obtain_combination nsubj_obtain_we vmod_obtain_Preparing nn_corpus_abbreviation amod_corpus_aligned det_corpus_an dobj_Preparing_corpus
C08-1083	J96-1002	o	We directly model the conditional probability of the alignment a given x and y using the maximum entropy framework -LRB- Berger et al. 1996 -RRB- P -LRB- a | x y -RRB- = exp -LCB- F -LRB- a x y -RRB- -RCB- summationdisplay aC -LRB- x y -RRB- exp -LCB- F -LRB- a x y -RRB- -RCB-	nn_y_x det_y_a appos_F_y nn_F_exp nn_F_aC det_F_a appos_x_y dep_aC_x nn_aC_summationdisplay nn_y_x det_y_a dep_F_y nn_F_exp amod_F_= nn_F_P appos_x_y nn_x_| det_x_a dep_P_x amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_framework_entropy nn_framework_maximum det_framework_the dobj_using_framework conj_and_x_y vmod_given_using pobj_given_y pobj_given_x dep_a_F dep_a_Berger dep_a_given dep_alignment_F det_alignment_the prep_of_probability_alignment amod_probability_conditional det_probability_the dobj_model_probability advmod_model_directly nsubj_model_We ccomp_``_model
C08-1142	J96-1002	o	We utilize maximum entropy -LRB- MaxEnt -RRB- model -LRB- Berger et al. 1996 -RRB- to design the basic classifier used in active learning for WSD and TC tasks	nn_tasks_TC nn_tasks_WSD conj_and_WSD_TC prep_for_learning_tasks amod_learning_active prep_in_used_learning vmod_classifier_used amod_classifier_basic det_classifier_the dobj_design_classifier aux_design_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy appos_entropy_MaxEnt amod_entropy_maximum vmod_utilize_design dep_utilize_Berger dobj_utilize_model nsubj_utilize_We
C08-1143	J96-1002	o	6.2 Experimental Settings We utilize a maximum entropy -LRB- ME -RRB- model -LRB- Berger et al. 1996 -RRB- to design the basic classifier for WSD and TC tasks	nn_tasks_TC nn_tasks_WSD conj_and_WSD_TC prep_for_classifier_tasks amod_classifier_basic det_classifier_the dobj_design_classifier aux_design_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy appos_entropy_ME nn_entropy_maximum det_entropy_a dobj_utilize_model nsubj_utilize_We vmod_Settings_design dep_Settings_Berger rcmod_Settings_utilize amod_Settings_Experimental num_Settings_6.2 dep_``_Settings
C08-2016	J96-1002	o	When we have a junction tree for each document we can efficiently perform belief propagation in order to compute argmax in Equation -LRB- 1 -RRB- or the marginal probabilities of cliques and labels necessary for the parameter estimation of machine learning classifiers including perceptrons -LRB- Collins 2002 -RRB- and maximum entropy models -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_models_entropy nn_models_maximum amod_Collins_2002 appos_perceptrons_Collins prep_including_classifiers_perceptrons nn_classifiers_learning nn_classifiers_machine prep_of_estimation_classifiers nn_estimation_parameter det_estimation_the prep_for_necessary_estimation conj_and_cliques_labels prep_of_probabilities_labels prep_of_probabilities_cliques amod_probabilities_marginal det_probabilities_the conj_or_Equation_probabilities appos_Equation_1 prep_in_argmax_probabilities prep_in_argmax_Equation dep_compute_Berger conj_and_compute_models conj_and_compute_necessary dobj_compute_argmax aux_compute_to dep_compute_order mark_compute_in nn_propagation_belief advcl_perform_models advcl_perform_necessary advcl_perform_compute dobj_perform_propagation advmod_perform_efficiently aux_perform_can nsubj_perform_we advcl_perform_have det_document_each prep_for_tree_document nn_tree_junction det_tree_a dobj_have_tree nsubj_have_we advmod_have_When
C08-2016	J96-1002	o	In the following experiments we run two machine learning classifiers Bayes Point Machines -LRB- BPM -RRB- -LRB- Herbrich et al. 2001 -RRB- and the maximum entropy model -LRB- ME -RRB- -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_model_Berger appos_model_ME nn_model_entropy nn_model_maximum det_model_the amod_Herbrich_2001 dep_Herbrich_al. nn_Herbrich_et conj_and_Machines_model appos_Machines_Herbrich appos_Machines_BPM nn_Machines_Point nn_Machines_Bayes dep_classifiers_model dep_classifiers_Machines nn_classifiers_learning nn_classifiers_machine num_classifiers_two dobj_run_classifiers nsubj_run_we prep_in_run_the prep_following_the_experiments
D07-1019	J96-1002	o	One is how to learn a statistical model to estimate the conditional probability and the other is how to generate confusion set C of a given query q 4.1 Maximum Entropy Model for Query Spelling Correction We take a feature-based approach to model the posterior probability Specifically we use the maximum entropy model -LRB- Berger et al. 1996 -RRB- for this task = exp = 1 exp -LRB- -LRB- -RRB- = 1 -RRB- -LRB- 2 -RRB- where exp -LRB- -LRB- -RRB- = 1 -RRB- is the normalization factor is a feature function defined over query q and correction candidate c while is the corresponding feature weight	nn_weight_feature amod_weight_corresponding det_weight_the cop_weight_is nn_c_candidate nn_c_correction conj_and_q_c nn_q_query prep_over_defined_c prep_over_defined_q vmod_function_defined nn_function_feature det_function_a cop_function_is nn_factor_normalization det_factor_the cop_factor_is nsubj_factor_exp advmod_factor_where dep_factor_2 nn_factor_exp dobj_=_1 dep_exp_= dep_=_1 dep_exp_= num_exp_1 amod_exp_= appos_exp_exp dep_=_weight dep_=_while dep_=_function dobj_=_factor det_task_this amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_the dep_use_= prep_for_use_task dep_use_Berger dobj_use_model nsubj_use_we advmod_use_Specifically amod_probability_posterior det_probability_the dobj_model_probability aux_model_to amod_approach_feature-based det_approach_a vmod_take_model dobj_take_approach nsubj_take_We nn_Correction_Spelling nn_Correction_Query rcmod_Model_take prep_for_Model_Correction nn_Model_Entropy nn_Model_Maximum num_Model_4.1 dep_q_Model nn_q_query pobj_given_q prep_a_given prep_of_C_a nn_C_set nn_C_confusion dobj_generate_C aux_generate_to advmod_generate_how ccomp_is_generate nsubj_is_other det_other_the amod_probability_conditional det_probability_the dobj_estimate_probability aux_estimate_to amod_model_statistical det_model_a vmod_learn_estimate dobj_learn_model aux_learn_to advmod_learn_how dep_is_use conj_and_is_is ccomp_is_learn nsubj_is_One
D07-1051	J96-1002	o	optimization approaches which aim at selecting those examples that optimize some -LRB- algorithm-dependent -RRB- objective function such as prediction variance -LRB- Cohn et al. 1996 -RRB- and heuristic methods with uncertainty sampling -LRB- Lewis and Catlett 1994 -RRB- and query-by-committee -LRB- QBC -RRB- -LRB- Seung et al. 1992 -RRB- just to name the most prominent ones	amod_ones_prominent det_ones_the advmod_prominent_most dobj_name_ones aux_name_to advmod_name_just amod_Seung_1992 dep_Seung_al. nn_Seung_et appos_query-by-committee_QBC dep_Lewis_1994 conj_and_Lewis_Catlett conj_and_sampling_query-by-committee dep_sampling_Catlett dep_sampling_Lewis nn_sampling_uncertainty prep_with_methods_query-by-committee prep_with_methods_sampling nn_methods_heuristic amod_Cohn_1996 dep_Cohn_al. nn_Cohn_et dep_variance_Cohn nn_variance_prediction prep_such_as_function_variance amod_function_objective amod_function_algorithm-dependent det_function_some dobj_optimize_function nsubj_optimize_that rcmod_examples_optimize det_examples_those dobj_selecting_examples prepc_at_aim_selecting nsubj_aim_which vmod_approaches_name dep_approaches_Seung conj_and_approaches_methods rcmod_approaches_aim nn_approaches_optimization
D07-1051	J96-1002	o	AL has already been applied to several NLP tasks such as document classification -LRB- Schohn and Cohn 2000 -RRB- POS tagging -LRB- Engelson and Dagan 1996 -RRB- chunking -LRB- Ngai and Yarowsky 2000 -RRB- statistical parsing -LRB- Thompson et al. 1999 Hwa 2000 -RRB- and information extraction -LRB- Lewis and Catlett 1994 Thompson et al. 1999 -RRB-	num_Thompson_1999 nn_Thompson_al. nn_Thompson_et conj_and_Lewis_Thompson conj_and_Lewis_1994 conj_and_Lewis_Catlett dep_extraction_Thompson dep_extraction_1994 dep_extraction_Catlett dep_extraction_Lewis nn_extraction_information dep_Hwa_2000 dep_Thompson_Hwa amod_Thompson_1999 dep_Thompson_al. nn_Thompson_et amod_parsing_statistical dep_Ngai_2000 conj_and_Ngai_Yarowsky dep_chunking_Yarowsky dep_chunking_Ngai dep_Engelson_1996 conj_and_Engelson_Dagan appos_tagging_Dagan appos_tagging_Engelson nn_tagging_POS dep_Schohn_2000 conj_and_Schohn_Cohn dep_classification_Thompson conj_classification_parsing conj_classification_chunking conj_classification_tagging dep_classification_Cohn dep_classification_Schohn nn_classification_document prep_such_as_tasks_classification nn_tasks_NLP amod_tasks_several conj_and_applied_extraction prep_to_applied_tasks auxpass_applied_been advmod_applied_already aux_applied_has nsubjpass_applied_AL
D07-1051	J96-1002	o	4.2 Classifier and Features For our AL framework we decided to employ a Maximum Entropy -LRB- ME -RRB- classifier -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_classifier_Berger dep_Entropy_classifier appos_Entropy_ME nn_Entropy_Maximum det_Entropy_a dobj_employ_Entropy aux_employ_to xcomp_decided_employ nsubj_decided_we nn_framework_AL poss_framework_our rcmod_Classifier_decided prep_for_Classifier_framework conj_and_Classifier_Features num_Classifier_4.2 dep_``_Features dep_``_Classifier
D07-1077	J96-1002	o	2 Related Work A number of researchers -LRB- Brown et al. 1992 Berger et al. 1996 Niessen and Ney 2004 Xia and McCord 2004 Collins et al. 2005 -RRB- have described approaches that preprocess the source language input in SMT systems	nn_systems_SMT prep_in_input_systems nn_input_language nn_input_source det_input_the dobj_preprocess_input nsubj_preprocess_that rcmod_approaches_preprocess dobj_described_approaches aux_described_have nsubj_described_Collins nsubj_described_2004 nsubj_described_McCord nsubj_described_Xia nsubj_described_2004 nsubj_described_Ney nsubj_described_Niessen num_Collins_2005 nn_Collins_al. nn_Collins_et conj_and_Niessen_Collins conj_and_Niessen_2004 conj_and_Niessen_McCord conj_and_Niessen_Xia conj_and_Niessen_2004 conj_and_Niessen_Ney num_Berger_1996 nn_Berger_al. nn_Berger_et parataxis_Brown_described conj_Brown_Berger appos_Brown_1992 dep_Brown_al. nn_Brown_et dep_number_Brown prep_of_number_researchers det_number_A dep_Work_number amod_Work_Related num_Work_2 dep_``_Work
D07-1082	J96-1002	o	We utilize a maximum entropy -LRB- ME -RRB- model -LRB- Berger et al. 1996 -RRB- to design the basic classifier used in active learning for WSD	prep_for_learning_WSD amod_learning_active prep_in_used_learning vmod_classifier_used amod_classifier_basic det_classifier_the dobj_design_classifier aux_design_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy appos_entropy_ME nn_entropy_maximum det_entropy_a vmod_utilize_design dep_utilize_Berger dobj_utilize_model nsubj_utilize_We
D07-1111	J96-1002	o	The first LR model for each language uses maximum entropy classification -LRB- Berger et al. 1996 -RRB- to determine possible parser actions and their probabilities4	poss_probabilities4_their conj_and_actions_probabilities4 nn_actions_parser amod_actions_possible dobj_determine_probabilities4 dobj_determine_actions aux_determine_to amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_classification_entropy nn_classification_maximum vmod_uses_determine dep_uses_Berger dobj_uses_classification nsubj_uses_model det_language_each prep_for_model_language nn_model_LR amod_model_first det_model_The
D08-1047	J96-1002	o	-LRB- 1 -RRB- Here the candidate generator gen -LRB- s -RRB- enumerates candidates of destination -LRB- correct -RRB- strings and the scorer P -LRB- t | s -RRB- denotes the conditional probability of the string t for the given s The scorer was modeled by a noisy-channel model -LRB- Shannon 1948 Brill and Moore 2000 Ahmad and Kondrak 2005 -RRB- and maximum entropy framework -LRB- Berger et al. 1996 Li et al. 2006 Chen et al. 2007 -RRB-	num_Chen_2007 nn_Chen_al. nn_Chen_et dep_Li_Chen num_Li_2006 nn_Li_al. nn_Li_et dep_Berger_Li appos_Berger_1996 dep_Berger_al. nn_Berger_et amod_framework_entropy nn_framework_maximum dep_Ahmad_2005 conj_and_Ahmad_Kondrak dep_Brill_Berger conj_and_Brill_framework conj_and_Brill_Kondrak conj_and_Brill_Ahmad conj_and_Brill_2000 conj_and_Brill_Moore dep_Shannon_framework dep_Shannon_Ahmad dep_Shannon_2000 dep_Shannon_Moore dep_Shannon_Brill appos_Shannon_1948 dep_model_Shannon amod_model_noisy-channel det_model_a agent_modeled_model auxpass_modeled_was nsubjpass_modeled_scorer det_scorer_The amod_s_given det_s_the nn_t_string det_t_the prep_for_probability_s prep_of_probability_t amod_probability_conditional det_probability_the dobj_denotes_probability nsubj_denotes_P num_s_| nn_s_t appos_P_s nn_P_scorer det_P_the nn_strings_destination appos_destination_correct prep_of_candidates_strings parataxis_enumerates_modeled conj_and_enumerates_denotes dobj_enumerates_candidates nsubj_enumerates_gen advmod_enumerates_Here dep_enumerates_1 appos_gen_s nn_gen_generator nn_gen_candidate det_gen_the
D08-1063	J96-1002	p	The classification is performed with a statistical approach built around the maximum entropy -LRB- MaxEnt -RRB- principle -LRB- Berger et al. 1996 -RRB- that has the advantage of combining arbitrary types of information in making a classification decision	nn_decision_classification det_decision_a dobj_making_decision prep_of_types_information amod_types_arbitrary prepc_in_combining_making dobj_combining_types prepc_of_advantage_combining det_advantage_the dobj_has_advantage nsubj_has_that amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_principle_entropy appos_entropy_MaxEnt nn_entropy_maximum det_entropy_the prep_around_built_principle dep_approach_Berger vmod_approach_built amod_approach_statistical det_approach_a dep_performed_has prep_with_performed_approach auxpass_performed_is nsubjpass_performed_classification det_classification_The
D08-1063	J96-1002	o	The -LCB- ij -RCB- j = 1m weights are estimated during the training phase to maximize the likelihood of the data -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et det_data_the prep_of_likelihood_data det_likelihood_the dobj_maximize_likelihood aux_maximize_to vmod_phase_maximize nn_phase_training det_phase_the dep_estimated_Berger prep_during_estimated_phase auxpass_estimated_are nsubjpass_estimated_weights nn_weights_1m amod_weights_= nn_weights_j nn_j_ij det_j_The
D08-1097	J96-1002	p	2.2 Maximum Entropy Models Maximum entropy -LRB- ME -RRB- models -LRB- Berger et al. 1996 Manning and Klein 2003 -RRB- also known as 928 log-linear and exponential learning models provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification	prep_for_sources_classification nn_sources_information amod_sources_heterogeneous amod_sources_many prep_from_integrate_sources dobj_integrate_features aux_integrate_can nsubj_integrate_models amod_models_entropy nn_models_Maximum nn_etc._recognition nn_etc._entity dep_named_etc. nn_tagging_speech prep_of_part_tagging prep_including_processing_part nn_processing_language amod_processing_natural prep_to_applied_processing advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_which conj_and_classification_prediction prep_for_technique_prediction prep_for_technique_classification amod_technique_learning rcmod_machine_applied dep_machine_technique nn_machine_purpose amod_machine_general det_machine_a ccomp_provide_integrate vmod_provide_named dobj_provide_machine nsubj_provide_models nn_models_learning amod_models_exponential amod_models_log-linear num_models_928 conj_and_log-linear_exponential prep_as_known_models advmod_known_also dep_Manning_2003 conj_and_Manning_Klein dep_Berger_Klein dep_Berger_Manning appos_Berger_1996 dep_Berger_al. nn_Berger_et vmod_models_known appos_models_Berger nn_models_entropy nn_models_Models nn_models_Entropy nn_models_Maximum num_models_2.2 appos_entropy_ME nn_entropy_Maximum
D09-1003	J96-1002	p	To estimate the parameters of the MEMM + pred model we turn to the successful Maximum Entropy -LRB- Berger et al. 1996 -RRB- parameter estimation method	nn_method_estimation nn_method_parameter dep_method_Berger nn_method_Entropy dep_Berger_1996 dep_Berger_al. nn_Berger_et nn_Entropy_Maximum amod_Entropy_successful det_Entropy_the prep_to_turn_method nsubj_turn_we advcl_turn_estimate amod_model_pred det_MEMM_the conj_+_parameters_model prep_of_parameters_MEMM det_parameters_the dobj_estimate_model dobj_estimate_parameters aux_estimate_To
D09-1057	J96-1002	p	2 Maximum Entropy Models Maximum entropy -LRB- ME -RRB- models -LRB- Berger et al. 1996 Manning and Klein 2003 -RRB- also known as log-linear and exponential learning models provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification	prep_for_sources_classification nn_sources_information amod_sources_heterogeneous amod_sources_many prep_from_integrate_sources dobj_integrate_features aux_integrate_can nsubj_integrate_models dep_integrate_provideageneralpurposemachinelearningtechnique amod_models_entropy nn_models_Maximum nn_etc._recognition nn_etc._entity dep_named_etc. nn_tagging_speech prep_of_part_tagging prep_including_processing_part nn_processing_language amod_processing_natural prep_to_applied_processing advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_which conj_and_classification_prediction vmod_provideageneralpurposemachinelearningtechnique_named rcmod_provideageneralpurposemachinelearningtechnique_applied prep_for_provideageneralpurposemachinelearningtechnique_prediction prep_for_provideageneralpurposemachinelearningtechnique_classification nn_models_learning amod_models_exponential amod_models_log-linear conj_and_log-linear_exponential parataxis_known_integrate prep_as_known_models advmod_known_also dep_Manning_2003 conj_and_Manning_Klein dep_Berger_Klein dep_Berger_Manning appos_Berger_1996 dep_Berger_al. nn_Berger_et dep_models_known appos_models_Berger nn_models_entropy nn_models_Models nn_models_Entropy nn_models_Maximum num_models_2 appos_entropy_ME nn_entropy_Maximum
D09-1069	J96-1002	o	-LRB- 11 -RRB- -LRB- 13 -RRB- -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_11_Berger dep_11_13 dep_''_11
D09-1069	J96-1002	o	-LRB- 14 -RRB- where i is the parameter to be estimated and f i -LRB- a b -RRB- is a feature function corresponding to i -LRB- Berger et al. 1996 Ratnaparkhi 1997 -RRB- P -LRB- E P | E G -RRB- productdisplay i P -LRB- ep i | ep i1 ik eg i + k ik -RRB- -LRB- 11 -RRB- P -LRB- C P | E G E P -RRB- -LRB- 12 -RRB- productdisplay i P -LRB- cp i | cp i1 ik eg ep i + k ik -RRB- P -LRB- C G | E G E P C P -RRB- -LRB- 13 -RRB- productdisplay i P -LRB- cg i | cg i1 ik eg ep cp i + k ik -RRB- P -LRB- b | a -RRB- = exp -LRB- summationtext i i f i -LRB- a b -RRB- -RRB- summationtext b prime exp -LRB- summationtext i i f i -LRB- a b prime -RRB- -RRB- -LRB- 14 -RRB- f i -LRB- a b -RRB- is a binary function returning TRUE or FALSE based on context a and output b If f i -LRB- a b -RRB- = 1 its corresponding model parameter i contributes toward conditional probability P -LRB- b | a -RRB- -LRB- Berger et al. 1996 Ratnaparkhi 1997 -RRB-	amod_Ratnaparkhi_1997 dep_Berger_Ratnaparkhi appos_Berger_1996 dep_Berger_al. nn_Berger_et dep_b_a num_b_| appos_P_b nn_P_probability amod_P_conditional dobj_contributes_Berger prep_toward_contributes_P nsubj_contributes_P dep_contributes_P dep_contributes_i dep_contributes_to dep_contributes_corresponding nsubj_contributes_function dep_parameter_i nn_parameter_model amod_parameter_corresponding poss_parameter_its dep_=_1 advmod_=_i mark_=_If det_b_a appos_i_b nn_i_f nn_b_output conj_and_a_b dep_context_b dep_context_a pobj_TRUE_context prepc_based_on_TRUE_on conj_or_TRUE_FALSE dobj_returning_FALSE dobj_returning_TRUE vmod_function_returning amod_function_binary det_function_a cop_function_is nsubj_function_14 dep_function_b det_function_a det_b_a appos_i_b dep_14_i dep_14_f amod_b_prime nn_i_f nn_i_i nn_i_i dep_summationtext_function advmod_summationtext_i amod_exp_prime nn_exp_b nn_exp_summationtext dep_exp_summationtext det_b_a nn_i_f nn_i_i nn_i_i dep_summationtext_b advmod_summationtext_i dep_=_exp dep_b_a num_b_| dep_P_exp amod_P_= appos_P_b nn_ik_k nn_ik_i nn_ik_cp conj_+_i_k nn_ik_i1 nn_ik_cg nn_ik_| nn_ik_i appos_cg_ik appos_cg_ep dep_cg_eg dep_cg_ik dep_P_P dep_P_cg nn_P_i nn_P_productdisplay nn_P_P nn_P_C nn_P_E appos_G_P appos_G_P nn_G_E nn_G_| nn_G_G nn_G_C dep_P_13 dep_P_G nn_ik_k nn_ik_i nn_ik_ep conj_+_i_k nn_ik_i1 nn_ik_cp nn_ik_| nn_ik_i dep_cp_ik nn_P_i nn_P_productdisplay nn_P_E appos_G_P nn_G_E num_G_| nn_G_P nn_G_C dep_P_ik dep_P_eg dep_P_cp dep_P_P dep_P_12 dep_P_G num_P_11 nn_ik_k conj_+_i_ik dep_eg_ik dep_eg_i nn_ik_i1 nn_ik_ep nn_ik_| nn_ik_i amod_ep_eg dep_ep_ik appos_P_parameter dep_P_= dep_P_summationtext dep_P_P dep_P_P appos_P_ep nn_P_i nn_P_productdisplay nn_P_P nn_P_E nn_G_E num_G_| dep_P_G dep_Ratnaparkhi_1997 dep_Berger_Ratnaparkhi num_Berger_1996 dep_Berger_al. nn_Berger_et appos_i_Berger nn_function_feature det_function_a cop_function_is nsubj_function_14 det_b_a appos_i_b nn_i_f conj_and_estimated_i auxpass_estimated_be aux_estimated_to vmod_parameter_i vmod_parameter_estimated det_parameter_the cop_parameter_is nsubj_parameter_i advmod_parameter_where rcmod_14_parameter
D09-1123	J96-1002	o	DTM2 introduced in -LRB- Ittycheriah and Roukos 2007 -RRB- expresses the phrase-based translation task in a unified log-linear probabilistic framework consisting of three components -LRB- i -RRB- a prior conditional distribution P0 -LRB- | S -RRB- -LRB- ii -RRB- a number of feature functions i -LRB- -RRB- that capture the translation and language model effects and -LRB- iii -RRB- the weights of the features i that are estimated under MaxEnt -LRB- Berger et al. 1996 -RRB- as in -LRB- 1 -RRB- P -LRB- T | S -RRB- = P0 -LRB- T J | S -RRB- Z expsummationdisplay i ii -LRB- T J S -RRB- -LRB- 1 -RRB- Here J is the skip reordering factor for the phrase pair captured by i -LRB- -RRB- and represents the jump from the previous source word and Z is the per source sentence normalization term	nn_term_normalization nn_term_sentence nn_term_source nn_term_per det_term_the cop_term_is nsubj_term_Z nsubj_term_factor dep_term_1 nn_word_source amod_word_previous det_word_the prep_from_jump_word det_jump_the dobj_represents_jump cc_represents_and advmod_represents_i mark_represents_by advcl_captured_represents vmod_pair_captured nn_pair_phrase det_pair_the conj_and_factor_Z prep_for_factor_pair nn_factor_reordering dep_factor_skip det_factor_the cop_factor_is nsubj_factor_J advmod_factor_Here dep_factor_1 dep_factor_ii dep_1_J dep_1_T appos_J_S advmod_ii_i nn_ii_expsummationdisplay dep_ii_Z dep_ii_T num_S_| nn_S_J appos_T_S nn_T_P0 amod_T_= nn_T_P num_S_| nn_S_T appos_P_S pobj_in_term pcomp_as_in amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_under_estimated_MaxEnt auxpass_estimated_are nsubjpass_estimated_that appos_i_Berger rcmod_i_estimated dep_features_i det_features_the prep_weights_as prep_of_weights_features det_weights_the dep_iii_weights nn_effects_model nn_effects_language nn_effects_translation det_effects_the conj_and_translation_language conj_and_capture_iii dobj_capture_effects nsubj_capture_that advmod_capture_i nsubj_capture_functions mark_capture_of nn_functions_feature rcmod_number_iii rcmod_number_capture det_number_a num_S_| dep_P0_number appos_P0_ii appos_P0_S nn_P0_distribution amod_P0_conditional amod_P0_prior det_P0_a dep_i_P0 num_components_three prep_of_consisting_components vmod_framework_consisting amod_framework_probabilistic amod_framework_log-linear amod_framework_unified det_framework_a nn_task_translation amod_task_phrase-based det_task_the dep_expresses_i prep_in_expresses_framework dobj_expresses_task nsubj_expresses_DTM2 dep_Ittycheriah_2007 conj_and_Ittycheriah_Roukos dep_in_Roukos dep_in_Ittycheriah prep_introduced_in vmod_DTM2_introduced
D09-1128	J96-1002	o	3.4 Learning algorithm Maximum entropy -LRB- ME -RRB- models -LRB- Berger et al. 1996 Manning and Klein 2003 -RRB- also known as log-linear and exponential learning models has been adopted in the SC classification task	nn_task_classification nn_task_SC det_task_the prep_in_adopted_task auxpass_adopted_been aux_adopted_has nsubjpass_adopted_models nn_models_learning amod_models_exponential amod_models_log-linear conj_and_log-linear_exponential prep_as_known_models advmod_known_also dep_Manning_2003 conj_and_Manning_Klein dep_Berger_Klein dep_Berger_Manning appos_Berger_1996 dep_Berger_al. nn_Berger_et vmod_models_known appos_models_Berger nn_models_entropy appos_entropy_ME nn_entropy_Maximum nn_entropy_algorithm nn_entropy_Learning num_entropy_3.4
D09-1154	J96-1002	o	As described in Section 4 we define the problem of term variation identifica1484 tion as a binary classification task and build two types of classifiers according to the maximum entropy model -LRB- Berger et al. 1996 -RRB- and the MART algorithm -LRB- Friedman 2001 -RRB- where all term similarity metrics are incorporated as features and are jointly optimized	advmod_optimized_jointly auxpass_optimized_are nsubjpass_optimized_metrics conj_and_incorporated_optimized prep_as_incorporated_features auxpass_incorporated_are nsubjpass_incorporated_metrics advmod_incorporated_where nn_metrics_similarity nn_metrics_term det_metrics_all amod_Friedman_2001 rcmod_algorithm_optimized rcmod_algorithm_incorporated appos_algorithm_Friedman nn_algorithm_MART det_algorithm_the amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_the prep_of_types_classifiers num_types_two pobj_build_model prepc_according_to_build_to dobj_build_types nsubj_build_we nn_task_classification amod_task_binary det_task_a nn_tion_identifica1484 nn_tion_variation nn_tion_term prep_of_problem_tion det_problem_the conj_and_define_algorithm dep_define_Berger conj_and_define_build prep_as_define_task dobj_define_problem nsubj_define_we advcl_define_described num_Section_4 prep_in_described_Section mark_described_As
D09-1160	J96-1002	o	lscript1-regularized log-linear models -LRB- lscript1-LLMs -RRB- on the other hand provide sparse solutions in which weights of irrelevant features are exactly zero by assumingaLaplacianpriorontheweights -LRB- Tibshirani 1996 Kazama and Tsujii 2003 Goodman 2004 Gao et al. 2007 -RRB-	num_Gao_2007 nn_Gao_al. nn_Gao_et num_Goodman_2004 dep_Kazama_Gao conj_and_Kazama_Goodman conj_and_Kazama_2003 conj_and_Kazama_Tsujii dep_Tibshirani_Goodman dep_Tibshirani_2003 dep_Tibshirani_Tsujii dep_Tibshirani_Kazama appos_Tibshirani_1996 dep_assumingaLaplacianpriorontheweights_Tibshirani advmod_zero_exactly cop_zero_are nsubj_zero_weights prep_in_zero_which amod_features_irrelevant prep_of_weights_features rcmod_solutions_zero amod_solutions_sparse prep_by_provide_assumingaLaplacianpriorontheweights dobj_provide_solutions nsubj_provide_models amod_hand_other det_hand_the prep_on_models_hand appos_models_lscript1-LLMs amod_models_log-linear amod_models_lscript1-regularized ccomp_``_provide
D09-1160	J96-1002	o	2.1 Log-Linear Models The log-linear model -LRB- LLM -RRB- or also known as maximum-entropy model -LRB- Berger et al. 1996 -RRB- is a linear classifier widely used in the NLP literature	nn_literature_NLP det_literature_the prep_in_used_literature advmod_used_widely vmod_classifier_used amod_classifier_linear det_classifier_a cop_classifier_is nsubj_classifier_Models amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_model_maximum-entropy dep_known_Berger prep_as_known_model advmod_known_also conj_or_model_known appos_model_LLM amod_model_log-linear det_model_The dep_Models_known dep_Models_model nn_Models_Log-Linear num_Models_2.1
E06-2002	J96-1002	o	By introducing the hidden word alignment variable a the following approximate optimization criterion can be applied for that purpose e = argmaxe Pr -LRB- e | f -RRB- = argmaxe summationdisplay a Pr -LRB- e a | f -RRB- argmaxe a Pr -LRB- e a | f -RRB- Exploiting the maximum entropy -LRB- Berger et al. 1996 -RRB- framework the conditional distribution Pr -LRB- e a | f -RRB- can be determined through suitable real valued functions -LRB- called features -RRB- hr -LRB- e f a -RRB- r = 1R and takes the parametric form p -LRB- e a | f -RRB- exp Rsummationdisplay r = 1 rhr -LRB- e f a -RRB- -RCB- The ITC-irst system -LRB- Chen et al. 2005 -RRB- is based on a log-linear model which extends the original IBM Model 4 -LRB- Brown et al. 1993 -RRB- to phrases -LRB- Koehn et al. 2003 Federico and Bertoldi 2005 -RRB-	amod_Federico_2005 conj_and_Federico_Bertoldi dep_Koehn_Bertoldi dep_Koehn_Federico appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et amod_Brown_1993 dep_Brown_al. nn_Brown_et dep_Model_Koehn prep_to_Model_phrases dep_Model_Brown num_Model_4 dep_IBM_Model dep_original_IBM amod_the_original dobj_extends_the nsubj_extends_which rcmod_model_extends amod_model_log-linear det_model_a amod_Chen_2005 dep_Chen_al. nn_Chen_et dep_system_Chen amod_system_ITC-irst det_system_The dep_system_e nn_system_p dep_e_a dep_e_f num_rhr_1 dep_=_rhr dep_r_e amod_r_= nn_r_Rsummationdisplay dep_exp_r dep_exp_f dep_|_exp det_|_a appos_e_| amod_form_parametric det_form_the nsubj_takes_system dobj_takes_form dobj_=_1R npadvmod_=_r appos_e_a appos_e_f appos_hr_e prep_based_on_called_model auxpass_called_is nsubjpass_called_system conj_and_called_takes dep_called_= dep_called_hr dep_called_features dep_functions_takes dep_functions_called amod_functions_valued amod_functions_real amod_functions_suitable prep_through_determined_functions auxpass_determined_be aux_determined_can nsubjpass_determined_| dep_determined_e dep_|_f det_|_a vmod_Pr_determined nn_Pr_distribution amod_Pr_conditional det_Pr_the appos_framework_Pr dep_framework_Berger dep_framework_entropy dep_framework_Exploiting dep_framework_| dep_framework_e dep_Berger_1996 dep_Berger_al. nn_Berger_et nn_entropy_maximum det_entropy_the dep_Exploiting_f det_|_a dep_Pr_framework det_Pr_a appos_argmaxe_Pr dep_argmaxe_f dep_|_argmaxe det_|_a dep_e_| appos_Pr_e det_Pr_a nn_summationdisplay_argmaxe dobj_=_Pr iobj_=_summationdisplay dep_=_f dep_f_| dep_|_e dep_Pr_= amod_Pr_argmaxe dep_=_Pr dep_=_e prepc_by_=_introducing det_purpose_that prep_for_applied_purpose auxpass_applied_be aux_applied_can nsubjpass_applied_criterion dep_applied_a nn_criterion_optimization amod_criterion_approximate amod_criterion_following det_criterion_the dep_variable_applied nn_variable_alignment nn_variable_word amod_variable_hidden det_variable_the dobj_introducing_variable
E06-2002	J96-1002	o	Hence either the best translation hypothesis is directly extracted from the word graph and output or an N-best list of translations is computed -LRB- Tran et al. 1996 -RRB-	amod_Tran_1996 dep_Tran_al. nn_Tran_et dep_computed_Tran auxpass_computed_is nsubjpass_computed_list prep_of_list_translations amod_list_N-best det_list_an conj_and_graph_output nn_graph_word det_graph_the conj_or_extracted_computed prep_from_extracted_output prep_from_extracted_graph advmod_extracted_directly auxpass_extracted_is nsubjpass_extracted_hypothesis advmod_extracted_Hence nn_hypothesis_translation amod_hypothesis_best det_hypothesis_the preconj_hypothesis_either
E06-2015	J96-1002	o	2.2 Learning Algorithm For learning coreference decisions we used a Maximum Entropy -LRB- Berger et al. 1996 -RRB- model	dep_model_Berger dep_Berger_1996 dep_Berger_al. nn_Berger_et dep_Entropy_model nn_Entropy_Maximum det_Entropy_a dobj_used_Entropy nsubj_used_we nsubj_used_Algorithm nn_decisions_coreference dobj_learning_decisions prepc_for_Algorithm_learning nn_Algorithm_Learning num_Algorithm_2.2
E09-1012	J96-1002	p	In order to estimate the conditional distributions shown in Table 1 we use the general technique of choosing the MaxEnt distribution that properly estimates the average of each feature over the training data -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_data_training det_data_the det_feature_each prep_over_average_data prep_of_average_feature det_average_the dobj_estimates_average advmod_estimates_properly nsubj_estimates_that rcmod_distribution_estimates amod_distribution_MaxEnt det_distribution_the dobj_choosing_distribution prepc_of_technique_choosing amod_technique_general det_technique_the dep_use_Berger dobj_use_technique nsubj_use_we advcl_use_estimate num_Table_1 prep_in_shown_Table vmod_distributions_shown amod_distributions_conditional det_distributions_the dobj_estimate_distributions aux_estimate_to dep_estimate_order mark_estimate_In
E09-1012	J96-1002	o	These feature vectors and the associated parser actions are used to train maximum entropy models -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_models_entropy amod_models_maximum dobj_train_models aux_train_to dep_used_Berger xcomp_used_train auxpass_used_are nsubjpass_used_actions nsubjpass_used_vectors nn_actions_parser amod_actions_associated det_actions_the conj_and_vectors_actions nn_vectors_feature det_vectors_These
E09-1022	J96-1002	o	We chose to train maximum entropy models -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_models_entropy amod_models_maximum dobj_train_models aux_train_to dep_chose_Berger xcomp_chose_train nsubj_chose_We
E09-3005	J96-1002	o	So far most previous work on domain adaptation for parsing has focused on data-driven systems -LRB- Gildea 2001 Roark and Bacchiani 2003 McClosky et al. 2006 Shimizu and Nakagawa 2007 -RRB- i.e. systems employing -LRB- constituent or dependency based -RRB- treebank grammars -LRB- Charniak 1996 -RRB-	amod_Charniak_1996 dep_grammars_Charniak amod_grammars_treebank dep_grammars_dependency dep_grammars_constituent amod_grammars_employing dep_constituent_based conj_or_constituent_dependency dep_systems_grammars nn_systems_i.e. dep_Shimizu_2007 conj_and_Shimizu_Nakagawa num_McClosky_2006 nn_McClosky_al. nn_McClosky_et appos_Roark_systems dep_Roark_Nakagawa dep_Roark_Shimizu conj_and_Roark_McClosky conj_and_Roark_2003 conj_and_Roark_Bacchiani dep_Gildea_McClosky dep_Gildea_2003 dep_Gildea_Bacchiani dep_Gildea_Roark appos_Gildea_2001 dep_systems_Gildea amod_systems_data-driven prep_on_focused_systems aux_focused_has nsubj_focused_work advmod_focused_far prep_for_adaptation_parsing nn_adaptation_domain prep_on_work_adaptation amod_work_previous advmod_work_most advmod_far_So
E09-3005	J96-1002	o	The Maximum Entropy model -LRB- Berger et al. 1996 Ratnaparkhi 1997 Abney 1997 -RRB- is a conditional model that assigns a probability to every possible parse for a given sentence s The model consists of a set of m feature functions fj -LRB- -RRB- that describe properties of parses together with their associated weights j The denominator is a normalization term where Y -LRB- s -RRB- is the set of parses with yield s p -LRB- | s ;-RRB- = exp -LRB- summationtextm j = 1 jfj -LRB- -RRB- -RRB- summationtext yY -LRB- s -RRB- exp -LRB- summationtextm j = 1 jfj -LRB- y -RRB- -RRB- -RRB- -LRB- 1 -RRB- The parameters -LRB- weights -RRB- j can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus -LRB- Johnson et al. 1999 van Noord and Malouf 2005 -RRB- = argmax logL -LRB- -RRB- summationtextm j = 1 2j 22 -LRB- 2 -RRB- where L -LRB- -RRB- is the likelihood of the training data	nn_data_training det_data_the prep_of_likelihood_data det_likelihood_the cop_likelihood_is nn_likelihood_L advmod_likelihood_where dep_likelihood_2 dep_2j_likelihood num_2j_22 num_2j_1 dobj_=_2j dep_j_= nn_j_summationtextm dep_logL_j nn_logL_argmax amod_logL_= dep_Malouf_2005 conj_and_Noord_Malouf nn_Noord_van dep_Johnson_logL dep_Johnson_Malouf dep_Johnson_Noord amod_Johnson_1999 dep_Johnson_al. nn_Johnson_et nn_corpus_training det_corpus_a prep_of_likelihood_corpus amod_likelihood_conditional amod_likelihood_regularized det_likelihood_the dobj_maximizing_likelihood dep_estimated_Johnson agent_estimated_maximizing advmod_estimated_efficiently auxpass_estimated_be aux_estimated_can nsubjpass_estimated_j dep_j_weights rcmod_parameters_estimated det_parameters_The dep_parameters_1 appos_jfj_y num_jfj_1 dep_=_jfj amod_j_= nn_j_summationtextm dep_exp_j dep_yY_exp appos_yY_s nn_yY_summationtext appos_jfj_yY num_jfj_1 dep_=_jfj dep_j_parameters amod_j_= nn_j_summationtextm dep_exp_j dobj_=_exp amod_;-RRB-_= dep_s_;-RRB- num_s_| dep_p_s nn_s_yield prep_with_set_s prep_of_set_parses det_set_the cop_set_is nsubj_set_Y advmod_set_where appos_Y_s dep_term_p rcmod_term_set nn_term_normalization det_term_a cop_term_is nsubj_term_denominator det_denominator_The nn_j_weights amod_j_associated poss_j_their prep_of_properties_parses prep_together_with_describe_j dobj_describe_properties nsubj_describe_that dep_describe_fj dep_describe_functions nsubj_describe_feature dep_describe_m mark_describe_of dep_set_describe det_set_a parataxis_consists_term prep_of_consists_set nsubj_consists_model det_model_The nn_s_sentence amod_s_given det_s_a amod_parse_possible det_parse_every det_probability_a prep_for_assigns_s prep_to_assigns_parse dobj_assigns_probability nsubj_assigns_that dep_model_consists rcmod_model_assigns amod_model_conditional det_model_a cop_model_is nsubj_model_model dep_Abney_1997 dep_Ratnaparkhi_Abney amod_Ratnaparkhi_1997 dep_Berger_Ratnaparkhi appos_Berger_1996 dep_Berger_al. nn_Berger_et appos_model_Berger nn_model_Entropy nn_model_Maximum det_model_The
E99-1026	J96-1002	o	Other methods that have been proposed are one based on using the gain -LRB- Berger et al. 1996 -RRB- and an approximate method for selecting informative features -LRB- Shirai et al. 1998a -RRB- and several criteria for feature selection were proposed and compared with other criteria -LRB- Berger and Printz 1998 -RRB-	amod_Berger_1998 conj_and_Berger_Printz dep_criteria_Printz dep_criteria_Berger amod_criteria_other pobj_with_criteria pcomp_compared_with nsubjpass_compared_criteria conj_and_proposed_compared auxpass_proposed_were nsubjpass_proposed_criteria nn_selection_feature prep_for_criteria_selection amod_criteria_several appos_Shirai_1998a dep_Shirai_al. nn_Shirai_et amod_features_informative dobj_selecting_features prepc_for_method_selecting amod_method_approximate det_method_an amod_Berger_1996 dep_Berger_al. nn_Berger_et det_gain_the dobj_using_gain conj_and_one_compared conj_and_one_proposed dep_one_Shirai conj_and_one_method dep_one_Berger prepc_based_on_one_using cop_one_are nsubj_one_methods auxpass_proposed_been aux_proposed_have nsubjpass_proposed_that rcmod_methods_proposed amod_methods_Other ccomp_``_proposed ccomp_``_method ccomp_``_one
E99-1026	J96-1002	o	This allows us to compute the conditional probability as follows -LRB- Berger et al. 1996 -RRB- P -LRB- flh -RRB- YIia \ -LSB- ' -LRB- n ` l -RRB- z ~ -LRB- h -RRB- -LRB- 2 -RRB- ~ i -LRB- 3 -RRB- I i The maximum entropy estimation technique guarantees that for every feature gi the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus	nn_corpus_training det_corpus_the prep_of_expectation_gi amod_expectation_empirical det_expectation_the prep_in_equal_corpus dobj_equal_expectation aux_equal_will nsubj_equal_value prep_for_equal_gi nsubj_equal_that nn_model_M.E. det_model_the pobj_value_model prepc_according_to_value_to prep_of_value_gi amod_value_expected det_value_the nn_gi_feature det_gi_every rcmod_guarantees_equal nn_guarantees_technique nn_guarantees_estimation amod_guarantees_entropy nn_guarantees_maximum det_guarantees_The dep_guarantees_i nsubj_guarantees_I dep_guarantees_3 nn_guarantees_i nn_guarantees_~ nn_guarantees_\ dep_~_2 dep_~_z nn_h_~ appos_z_h appos_z_n dep_n_l nn_\_YIia nn_\_P appos_P_flh amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_follows_Berger mark_follows_as amod_probability_conditional det_probability_the advcl_compute_follows dobj_compute_probability aux_compute_to dep_allows_guarantees xcomp_allows_compute dobj_allows_us nsubj_allows_This
H05-1012	J96-1002	o	These IBM models and more recent refinements -LRB- Moore 2004 -RRB- as well as algorithms that bootstrap from these models like the HMM algorithm described in -LRB- Vogel et al. 1996 -RRB- are unsupervised algorithms	amod_algorithms_unsupervised cop_algorithms_are nsubj_algorithms_algorithms nsubj_algorithms_refinements nsubj_algorithms_models amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et dep_in_Vogel prep_described_in vmod_algorithm_described nn_algorithm_HMM det_algorithm_the det_models_these prep_like_bootstrap_algorithm prep_from_bootstrap_models nsubj_bootstrap_that rcmod_algorithms_bootstrap dep_Moore_2004 appos_refinements_Moore amod_refinements_recent amod_refinements_more conj_and_models_algorithms conj_and_models_refinements nn_models_IBM det_models_These
H05-1012	J96-1002	o	-LRB- Berger et al. 1996 -RRB- -RRB- 1We are overloading the word state to mean Arabic word position	nn_position_word amod_position_Arabic dobj_mean_position aux_mean_to nn_state_word det_state_the vmod_overloading_mean dobj_overloading_state aux_overloading_are nsubj_overloading_1We rcmod_Berger_overloading amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_''_Berger
H05-1022	J96-1002	o	We use a simple single parameter distribution with = 8.0 throughout P -LRB- K | m e -RRB- = P -LRB- K | m l -RRB- K Word-to-Phrase Alignment Alignment is a Markov process that specifies the lengths of phrases and their alignment with source words P -LRB- aK1 hK1 K1 | K m e -RRB- = Kproductdisplay k = 1 P -LRB- ak hk k | ak1 k1 e -RRB- = Kproductdisplay k = 1 p -LRB- ak | ak1 hk l -RRB- d -LRB- hk -RRB- n -LRB- k eak -RRB- The actual word-to-phrase alignment -LRB- ak -RRB- is a firstorder Markov process as in HMM-based word-toword alignment -LRB- Vogel et al. 1996 -RRB-	amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et amod_alignment_word-toword amod_alignment_HMM-based pobj_in_alignment pcomp_as_in nn_process_Markov nn_process_firstorder det_process_a cop_process_is nsubj_process_alignment appos_alignment_ak nn_alignment_word-to-phrase amod_alignment_actual det_alignment_The dep_k_eak dep_n_process appos_n_k nn_n_d appos_d_hk nn_d_p num_ak1_| dep_ak_l appos_ak_hk dep_ak_ak1 appos_p_ak num_p_1 dobj_=_n amod_k_= nn_k_Kproductdisplay dep_=_Vogel prep_=_as dobj_=_k dep_=_e num_ak1_| nn_ak1_k dep_ak_= appos_ak_k1 appos_ak_ak1 appos_ak_hk appos_P_ak num_P_1 dep_=_P amod_k_= nn_k_Kproductdisplay dobj_=_k num_K_| nn_K_K1 dep_aK1_= appos_aK1_e appos_aK1_m appos_aK1_K appos_aK1_hK1 dep_P_aK1 dep_words_P dep_source_words prep_with_alignment_source poss_alignment_their conj_and_lengths_alignment prep_of_lengths_phrases det_lengths_the dobj_specifies_alignment dobj_specifies_lengths nsubj_specifies_that rcmod_process_specifies nn_process_Markov det_process_a cop_process_is nsubj_process_m tmod_process_P mark_process_throughout nn_Alignment_Alignment amod_Alignment_Word-to-Phrase nn_Alignment_K nn_Alignment_P amod_Alignment_= dep_m_l num_m_| nn_m_K dep_P_m dep_m_Alignment appos_m_e num_m_| nn_m_K dep_=_process dep_=_8.0 nn_distribution_parameter amod_distribution_single amod_distribution_simple det_distribution_a prep_with_use_= dobj_use_distribution nsubj_use_We
H05-1022	J96-1002	o	The bigram translation probability t2 -LRB- f | f e -RRB- specifies the likelihood that target word f is to follow f in a phrase generated by source word e. 170 2.1 Properties of the Model and Prior Work The formulation of the WtoP alignment model was motivated by both the HMM word alignment model -LRB- Vogel et al. 1996 -RRB- and IBM Model-4 with the goal of building on the strengths of each	prep_of_strengths_each det_strengths_the prep_on_goal_strengths prep_of_goal_building det_goal_the prep_with_Model-4_goal nn_Model-4_IBM amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et conj_and_model_Model-4 dep_model_Vogel nn_model_alignment nn_model_word nn_model_HMM det_model_the preconj_model_both agent_motivated_Model-4 agent_motivated_model auxpass_motivated_was nsubjpass_motivated_formulation nn_model_alignment nn_model_WtoP det_model_the prep_of_formulation_model det_formulation_The rcmod_Work_motivated amod_Work_Prior det_Model_the conj_and_Properties_Work prep_of_Properties_Model num_Properties_2.1 num_Properties_170 nn_Properties_e. nn_Properties_word nn_Properties_source agent_generated_Work agent_generated_Properties vmod_phrase_generated det_phrase_a prep_in_follow_phrase dobj_follow_f aux_follow_to xcomp_is_follow nn_f_word dobj_target_f nsubj_target_that rcmod_likelihood_target det_likelihood_the dep_specifies_is dobj_specifies_likelihood dep_specifies_e advmod_specifies_| nsubj_specifies_t2 dep_|_f nn_|_f nn_t2_probability nn_t2_translation nn_t2_bigram det_t2_The
H05-1022	J96-1002	o	In fact the WtoP model is a segmental Hidden Markov Model -LRB- Ostendorf et al. 1996 -RRB- in which states emit observation sequences	nn_sequences_observation amod_sequences_emit dobj_states_sequences prep_in_states_which amod_Ostendorf_1996 dep_Ostendorf_al. nn_Ostendorf_et rcmod_Model_states dep_Model_Ostendorf nn_Model_Markov nn_Model_Hidden amod_Model_segmental det_Model_a cop_Model_is nsubj_Model_model prep_in_Model_fact nn_model_WtoP det_model_the
H05-1022	J96-1002	p	The bigram translation probability relies on word context known to be helpful in translation -LRB- Berger et al. 1996 -RRB- to improve the identification of target phrases	nn_phrases_target prep_of_identification_phrases det_identification_the dobj_improve_identification aux_improve_to amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_in_helpful_translation cop_helpful_be aux_helpful_to dep_known_Berger xcomp_known_helpful nn_context_word xcomp_relies_improve conj_relies_known prep_on_relies_context nsubj_relies_probability nn_probability_translation nn_probability_bigram det_probability_The
H05-1059	J96-1002	o	A common choice for the local probabilistic classifier is maximum entropy classifiers -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_classifiers_Berger nn_classifiers_entropy amod_classifiers_maximum cop_classifiers_is nsubj_classifiers_choice amod_classifier_probabilistic amod_classifier_local det_classifier_the prep_for_choice_classifier amod_choice_common det_choice_A
H05-1059	J96-1002	o	3 Maximum Entropy Classifier For local classifiers we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_processing_language amod_processing_natural prep_in_problems_processing nn_problems_classification prep_of_types_features amod_types_various prep_for_incorporating_problems dobj_incorporating_types prepc_for_choice_incorporating amod_choice_common det_choice_a cop_choice_is nsubj_choice_which rcmod_model_choice nn_model_entropy nn_model_maximum det_model_a dep_used_Berger dobj_used_model nsubj_used_we nsubj_used_Classifier amod_classifiers_local prep_for_Classifier_classifiers nn_Classifier_Entropy nn_Classifier_Maximum num_Classifier_3
H05-1083	J96-1002	o	660 2 Statistical Coreference Resolution Model Our coreference system uses a binary entity-mention model PL -LRB- je m -RRB- -LRB- henceforth link model -RRB- to score the action of linking a mention m to an entity e In our implementation the link model is computed as PL -LRB- L = 1je m -RRB- max mprimee PL -LRB- L = 1je mprime m -RRB- -LRB- 1 -RRB- where mprime is one mention in entity e and the basic model building block PL -LRB- L = 1je mprime m -RRB- is an exponential or maximum entropy model -LRB- Berger et al. 1996 -RRB- PL -LRB- Lje mprime m -RRB- = exp braceleftbig summationtext i igi -LRB- e m prime m L -RRB- bracerightbig Z -LRB- e mprime m -RRB- -LRB- 2 -RRB- where Z -LRB- e mprime m -RRB- is a normalizing factor to ensure that PL -LRB- je mprime m -RRB- is a probability fgi -LRB- e mprime m L -RRB- g are features and fig are feature weights	nn_weights_feature cop_weights_are nsubj_weights_fig nsubj_weights_features conj_and_features_fig cop_features_are nsubj_features_mprime dep_features_e nn_g_L nn_g_m conj_mprime_g rcmod_fgi_weights det_probability_a cop_probability_is nsubj_probability_PL mark_probability_that appos_je_m appos_je_mprime dep_PL_je ccomp_ensure_probability aux_ensure_to appos_factor_fgi vmod_factor_ensure nn_factor_normalizing det_factor_a cop_factor_is nsubj_factor_Z advmod_factor_where nn_m_mprime dep_m_e appos_Z_m nn_m_mprime dep_m_e dep_Z_factor appos_Z_2 appos_Z_m nn_Z_bracerightbig nn_Z_L nn_Z_m amod_Z_prime npadvmod_prime_m dep_e_Z nn_igi_i nn_igi_summationtext nn_igi_braceleftbig nn_igi_exp dep_=_igi appos_Lje_m appos_Lje_mprime dep_PL_e amod_PL_= dep_PL_Lje amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_model_PL dep_model_Berger amod_model_entropy amod_model_maximum amod_model_exponential det_model_an cop_model_is nsubj_model_PL conj_or_exponential_maximum dep_=_m dep_=_mprime dobj_=_1je nsubj_=_L dep_PL_= nn_PL_block nn_PL_building nn_PL_model amod_PL_basic det_PL_the nn_e_entity conj_and_mention_model prep_in_mention_e num_mention_one cop_mention_is nsubj_mention_mprime advmod_mention_where dep_1_model dep_1_mention dep_=_m dep_=_mprime dobj_=_1je nsubj_=_L dep_PL_= nn_PL_mprimee nn_PL_max dep_=_m dobj_=_1je nsubj_=_L dep_PL_PL dep_PL_= prep_as_computed_PL auxpass_computed_is nsubjpass_computed_model prep_in_computed_implementation dep_computed_e nn_model_link det_model_the poss_implementation_our vmod_entity_computed det_entity_an nn_m_mention det_m_a prep_to_linking_entity dobj_linking_m prepc_of_action_linking det_action_the dobj_score_action aux_score_to nn_model_link nn_model_henceforth appos_je_m appos_PL_model dep_PL_je nn_PL_model nn_PL_entity-mention amod_PL_binary det_PL_a vmod_uses_score dobj_uses_PL nsubj_uses_system nn_system_coreference poss_system_Our rcmod_Model_uses nn_Model_Resolution nn_Model_Coreference amod_Model_Statistical num_Model_2 appos_660_1 appos_660_Model ccomp_``_660
I05-2046	J96-1002	o	Given a set of features and a training corpus the ME estimation process produces a model in which every feature fi has a weight i. From -LRB- Berger et al. 1996 -RRB- we can compute the conditional probability as p -LRB- o | h -RRB- = 1Z -LRB- h -RRB- productdisplay i fi -LRB- h o -RRB- i -LRB- 2 -RRB- Z -LRB- h -RRB- = summationdisplay o productdisplay i fi -LRB- h o -RRB- i -LRB- 3 -RRB- The probability is given by multiplying the weights of active features -LRB- i.e. those fi -LRB- h o -RRB- = 1 -RRB-	dobj_=_1 nsubj_=_fi advmod_=_i.e. appos_h_o dep_fi_h det_fi_those dep_features_= amod_features_active prep_of_weights_features det_weights_the dobj_multiplying_weights agent_given_multiplying auxpass_given_is nsubjpass_given_probability dep_given_3 advmod_given_i det_probability_The nn_i_fi appos_h_o dep_fi_h nn_fi_i nn_fi_productdisplay nn_fi_o nn_fi_summationdisplay xcomp_=_given nsubj_=_Z appos_Z_h dep_Z_2 nn_Z_i nn_Z_fi appos_h_o dep_fi_h nn_fi_i rcmod_productdisplay_= nn_productdisplay_1Z appos_1Z_h dobj_=_productdisplay num_h_| dep_o_h dep_p_= dep_p_o dep_as_p amod_probability_conditional det_probability_the dep_compute_as dobj_compute_probability aux_compute_can nsubj_compute_we amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_From_Berger parataxis_i._compute prep_i._From nsubj_i._weight det_weight_a dep_has_i. nsubj_has_fi prep_in_has_which nn_fi_feature det_fi_every rcmod_model_has det_model_a dobj_produces_model nsubj_produces_process prep_produces_Given nn_process_estimation nn_process_ME det_process_the nn_corpus_training det_corpus_a conj_and_set_corpus prep_of_set_features det_set_a pobj_Given_corpus pobj_Given_set
I05-2046	J96-1002	o	The MBT POS tagger -LRB- Daelemans et al. 1996 -RRB- is used to provide POS information	nn_information_POS dobj_provide_information aux_provide_to xcomp_used_provide auxpass_used_is nsubjpass_used_tagger amod_Daelemans_1996 dep_Daelemans_al. nn_Daelemans_et appos_tagger_Daelemans nn_tagger_POS nn_tagger_MBT det_tagger_The
I05-3031	J96-1002	o	As the taskisanimportantprecursortomanynaturallanguage processing systems it receives a lot of attentions in the literature for the past decade -LRB- Wu and Tseng 1993 Sproat et al. 1996 -RRB-	nn_al._et nn_al._Sproat amod_Wu_1996 dep_Wu_al. num_Wu_1993 conj_and_Wu_Tseng amod_decade_past det_decade_the det_literature_the prep_in_lot_literature prep_of_lot_attentions det_lot_a dep_receives_Tseng dep_receives_Wu prep_for_receives_decade dobj_receives_lot nsubj_receives_it prep_as_receives_systems nn_systems_processing nn_systems_taskisanimportantprecursortomanynaturallanguage det_systems_the
I08-1008	J96-1002	o	3 MaxEnt Model and Features 3.1 MaxEnt Model for NOR The principle of maximum entropy -LRB- MaxEnt -RRB- model is that given a collection of facts choose a model consistent with all the facts but otherwise as uniform as possible -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_possible_Berger dep_possible_as prep_as_possible_uniform advmod_possible_otherwise nsubj_possible_principle det_facts_the predet_facts_all prep_with_consistent_facts amod_model_consistent det_model_a dobj_choose_model prep_choose_given mark_choose_that prep_of_collection_facts det_collection_a pobj_given_collection conj_but_is_possible ccomp_is_choose nsubj_is_principle nn_model_entropy appos_entropy_MaxEnt nn_entropy_maximum prep_of_principle_model det_principle_The dep_Model_possible dep_Model_is prep_for_Model_NOR nn_Model_MaxEnt num_Model_3.1 dep_Features_Model conj_and_Model_Features nn_Model_MaxEnt num_Model_3 dep_``_Features dep_``_Model
I08-1048	J96-1002	o	We utilize a maximum entropy -LRB- ME -RRB- model -LRB- Berger et al. 1996 -RRB- to design the basic classifier used in active learning for WSD	prep_for_learning_WSD amod_learning_active prep_in_used_learning vmod_classifier_used amod_classifier_basic det_classifier_the dobj_design_classifier aux_design_to amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy appos_entropy_ME nn_entropy_maximum det_entropy_a vmod_utilize_design dep_utilize_Berger dobj_utilize_model nsubj_utilize_We
I08-1060	J96-1002	o	There are other types of variations for phrases for example insertion deletion or substitution of words and permutation of words such as view point and point of view are such variations -LRB- Daille et al. 1996 -RRB-	amod_Daille_1996 dep_Daille_al. nn_Daille_et dep_variations_Daille amod_variations_such cop_variations_are nsubj_variations_permutation nsubj_variations_insertion prep_for_variations_example prep_of_point_view conj_and_point_point nn_point_view prep_such_as_words_point prep_such_as_words_point prep_of_permutation_words prep_of_deletion_words conj_or_deletion_substitution conj_and_insertion_permutation dep_insertion_substitution dep_insertion_deletion prep_for_variations_phrases prep_of_types_variations amod_types_other parataxis_are_variations nsubj_are_types expl_are_There ccomp_``_are
I08-1060	J96-1002	o	Then we build a classier learned by training data using a maximum entropy model -LRB- Berger et al. 1996 -RRB- and the features related to spelling variations in Table 3	num_Table_3 prep_in_variations_Table amod_variations_spelling prep_to_related_variations vmod_features_related det_features_the amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_entropy nn_model_maximum det_model_a dobj_using_model nn_data_training agent_learned_data conj_and_classier_features dep_classier_Berger ccomp_classier_using vmod_classier_learned dep_a_features dep_a_classier dobj_build_a nsubj_build_we advmod_build_Then
I08-2122	J96-1002	o	Uses Maximum Entropy -LRB- Berger et al. 1996 -RRB- classification trained on JNLPBA -LRB- Kim et al. 2004 -RRB- -LRB- NER -RRB-	num_Kim_2004 dep_Kim_al. nn_Kim_et prep_on_trained_JNLPBA dep_Berger_1996 dep_Berger_al. nn_Berger_et appos_Entropy_NER appos_Entropy_Kim vmod_Entropy_trained dep_Entropy_classification dep_Entropy_Berger nn_Entropy_Maximum dobj_Uses_Entropy
J00-3003	J96-1002	o	-LRB- 1996 -RRB- Warnke et al.	nn_al._et dep_Warnke_al. dep_Warnke_1996
J00-3003	J96-1002	o	The idea caught on very quickly Suhm and Waibel -LRB- 1994 -RRB- Mast et aL -LRB- 1996 -RRB- Warnke et al.	nn_al._et nn_al._Warnke dep_aL_1996 nn_aL_et nn_aL_Mast appos_Waibel_1994 appos_Suhm_al. conj_and_Suhm_aL conj_and_Suhm_Waibel dep_quickly_aL dep_quickly_Waibel dep_quickly_Suhm advmod_quickly_very pobj_on_quickly prep_caught_on vmod_idea_caught det_idea_The dep_``_idea
J00-3003	J96-1002	o	Computational approaches to prosodic modeling of DAs have aimed to automatically extract various prosodic parameters -- such as duration pitch and energy patterns -- from the speech signal -LRB- Yoshimura et al. \ -LSB- 1996 \ -RSB- Taylor et al. \ -LSB- 1997 \ -RSB- Kompe \ -LSB- 1997 \ -RSB- among others -RRB-	num_\_1997 prep_among_Kompe_others appos_Kompe_\ num_Kompe_\ num_\_1997 appos_\_\ dep_al._\ nn_al._et nn_al._Taylor num_\_1996 nn_al._et dep_Yoshimura_Kompe dep_Yoshimura_al. dep_Yoshimura_\ dep_Yoshimura_\ advmod_Yoshimura_al. nn_signal_speech det_signal_the nn_patterns_energy conj_and_duration_patterns conj_and_duration_pitch prep_such_as_parameters_patterns prep_such_as_parameters_pitch prep_such_as_parameters_duration amod_parameters_prosodic amod_parameters_various prep_from_extract_signal dobj_extract_parameters advmod_extract_automatically aux_extract_to dep_aimed_Yoshimura xcomp_aimed_extract aux_aimed_have nsubj_aimed_approaches prep_of_modeling_DAs amod_modeling_prosodic prep_to_approaches_modeling amod_approaches_Computational
J00-3003	J96-1002	o	Suhm and Waibel -LRB- 1994 -RRB- and Eckert Gallwitz and Niemann -LRB- 1996 -RRB- each condition a recognizer LM on left-to-right DA predictions and are able to 366 Stolcke et al. Dialogue Act Modeling show reductions in word error rate of 1 % on task-oriented corpora	amod_corpora_task-oriented num_%_1 prep_of_rate_% nn_rate_error nn_rate_word prep_on_reductions_corpora prep_in_reductions_rate nn_reductions_show amod_reductions_Modeling nn_reductions_Act nn_Act_Dialogue nn_al._et nn_al._Stolcke dep_al._366 dep_al._to dep_able_reductions dep_able_al. cop_able_are nsubj_able_condition nsubj_able_Gallwitz nsubj_able_Eckert nsubj_able_Waibel nsubj_able_Suhm nn_predictions_DA amod_predictions_left-to-right prep_on_LM_predictions nn_LM_recognizer det_LM_a dep_condition_LM det_condition_each nn_condition_Niemann appos_Niemann_1996 appos_Waibel_1994 conj_and_Suhm_condition conj_and_Suhm_Gallwitz conj_and_Suhm_Eckert conj_and_Suhm_Waibel
J00-3003	J96-1002	o	Automatic segmentation of spontaneous speech is an open research problem in its own right -LRB- Mast et al. 1996 Stolcke and Shriberg 1996 -RRB-	num_Shriberg_1996 conj_and_Stolcke_Shriberg num_al._1996 nn_al._et dep_Mast_Shriberg dep_Mast_Stolcke dep_Mast_al. dep_right_Mast amod_right_own poss_right_its prep_in_problem_right nn_problem_research amod_problem_open det_problem_an cop_problem_is nsubj_problem_segmentation amod_speech_spontaneous prep_of_segmentation_speech nn_segmentation_Automatic
J04-4002	J96-1002	o	Here we use the hidden Markov model -LRB- HMM -RRB- alignment model -LRB- Vogel Ney and Tillmann 1996 -RRB- and Model 4 of Brown et al.	nn_al._et nn_al._Brown prep_of_4_al. dep_Model_4 num_Tillmann_1996 conj_and_Vogel_Tillmann conj_and_Vogel_Ney dep_model_Tillmann dep_model_Ney dep_model_Vogel nn_model_alignment dep_model_model appos_model_HMM nn_model_Markov amod_model_hidden det_model_the conj_and_use_Model dobj_use_model nsubj_use_we advmod_use_Here
J05-1003	J96-1002	o	Feature selection methods have been proposed in the maximum-entropy literature by several authors -LRB- Ratnaparkhi Roukos and Ward 1994 Berger Della Pietra and Della Pietra 1996 Della Pietra Della Pietra and Lafferty 1997 Papineni Roukos and Ward 1997 1998 McCallum 2003 Zhou et al. 2003 Riezler and Vasserman 2004 -RRB-	num_Vasserman_2004 conj_and_Riezler_Vasserman num_al._2003 nn_al._et nn_al._Zhou num_McCallum_2003 num_Ward_1998 num_Ward_1997 conj_and_Papineni_Ward conj_and_Papineni_Roukos num_Lafferty_1997 nn_Pietra_Della conj_and_Pietra_Lafferty conj_and_Pietra_Pietra nn_Pietra_Della num_Pietra_1996 nn_Pietra_Della nn_Pietra_Della conj_and_Berger_Pietra conj_and_Berger_Pietra num_Ward_1994 dep_Ratnaparkhi_Vasserman dep_Ratnaparkhi_Riezler conj_and_Ratnaparkhi_al. conj_and_Ratnaparkhi_McCallum conj_and_Ratnaparkhi_Ward conj_and_Ratnaparkhi_Roukos conj_and_Ratnaparkhi_Papineni conj_and_Ratnaparkhi_Lafferty conj_and_Ratnaparkhi_Pietra conj_and_Ratnaparkhi_Pietra conj_and_Ratnaparkhi_Pietra conj_and_Ratnaparkhi_Pietra conj_and_Ratnaparkhi_Berger conj_and_Ratnaparkhi_Ward conj_and_Ratnaparkhi_Roukos dep_authors_al. dep_authors_McCallum dep_authors_Papineni dep_authors_Pietra dep_authors_Berger dep_authors_Ward dep_authors_Roukos dep_authors_Ratnaparkhi amod_authors_several amod_literature_maximum-entropy det_literature_the agent_proposed_authors prep_in_proposed_literature auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods nn_methods_selection nn_methods_Feature ccomp_``_proposed
J05-1003	J96-1002	o	6.4 Feature Selection Methods A number of previous papers -LRB- Berger Della Pietra and Della Pietra 1996 Ratnaparkhi 1998 Della Pietra Della Pietra and Lafferty 1997 McCallum 2003 Zhou et al. 2003 Riezler and Vasserman 2004 -RRB- describe feature selection approaches for log-linear models applied to NLP problems	nn_problems_NLP prep_to_applied_problems vmod_models_applied amod_models_log-linear prep_for_approaches_models nn_approaches_selection nn_approaches_feature dobj_describe_approaches nsubj_describe_number num_Vasserman_2004 conj_and_Riezler_Vasserman dep_al._2003 nn_al._et nn_al._Zhou num_McCallum_2003 num_Lafferty_1997 nn_Pietra_Della conj_and_Pietra_Lafferty conj_and_Pietra_Pietra nn_Pietra_Della num_Ratnaparkhi_1998 num_Pietra_1996 nn_Pietra_Della nn_Pietra_Della dep_Berger_Vasserman dep_Berger_Riezler dep_Berger_al. dep_Berger_McCallum dep_Berger_Lafferty dep_Berger_Pietra dep_Berger_Pietra conj_and_Berger_Ratnaparkhi conj_and_Berger_Pietra conj_and_Berger_Pietra amod_papers_previous appos_number_Ratnaparkhi appos_number_Pietra appos_number_Pietra appos_number_Berger prep_of_number_papers det_number_A rcmod_Methods_describe dep_Selection_Methods dep_Feature_Selection dep_6.4_Feature ccomp_``_6.4
J05-1003	J96-1002	n	More recent work -LRB- McCallum 2003 Zhou et al. 2003 Riezler and Vasserman 2004 -RRB- has considered methods for speeding up the feature selection methods described in Berger Della Pietra and Della Pietra -LRB- 1996 -RRB- Ratnaparkhi -LRB- 1998 -RRB- and Della Pietra Della Pietra and Lafferty -LRB- 1997 -RRB-	appos_Lafferty_1997 nn_Pietra_Della nn_Pietra_Della appos_Ratnaparkhi_1998 appos_Pietra_1996 nn_Pietra_Della nn_Pietra_Della conj_and_Berger_Lafferty conj_and_Berger_Pietra conj_and_Berger_Pietra conj_and_Berger_Ratnaparkhi conj_and_Berger_Pietra conj_and_Berger_Pietra prep_in_described_Lafferty prep_in_described_Pietra prep_in_described_Pietra prep_in_described_Ratnaparkhi prep_in_described_Pietra prep_in_described_Pietra prep_in_described_Berger vmod_methods_described nn_methods_selection nn_methods_feature det_methods_the dobj_speeding_methods prt_speeding_up prepc_for_methods_speeding dobj_considered_methods aux_considered_has nsubj_considered_work num_Vasserman_2004 conj_and_Riezler_Vasserman num_al._2003 nn_al._et nn_al._Zhou dep_McCallum_Vasserman dep_McCallum_Riezler conj_McCallum_al. num_McCallum_2003 dep_work_McCallum amod_work_recent advmod_recent_More
J99-1004	J96-1002	o	The theory has been applied in probabilistic language modeling -LRB- Mark Miller and Grenander 1996 Mark et al. 1996 Johnson 1998 -RRB- natural language processing -LRB- Berger Della Pietra and Della Pietra 1996 Della Pietra Della Pietra and Lafferty 1997 -RRB- as well as computational vision -LRB- Zhu Wu and Mumford 1997 -RRB-	num_Mumford_1997 conj_and_Zhu_Mumford appos_Zhu_Wu dep_vision_Mumford dep_vision_Zhu amod_vision_computational num_Lafferty_1997 nn_Pietra_Della conj_and_Pietra_Lafferty conj_and_Pietra_Pietra nn_Pietra_Della num_Pietra_1996 nn_Pietra_Della nn_Pietra_Della conj_and_Berger_vision conj_and_Berger_Lafferty conj_and_Berger_Pietra conj_and_Berger_Pietra conj_and_Berger_Pietra conj_and_Berger_Pietra dep_processing_vision dep_processing_Pietra dep_processing_Pietra dep_processing_Pietra dep_processing_Berger nn_processing_language amod_processing_natural num_Johnson_1998 num_al._1996 nn_al._et nn_al._Mark num_Grenander_1996 dep_Mark_Johnson conj_and_Mark_al. conj_and_Mark_Grenander conj_and_Mark_Miller appos_modeling_processing appos_modeling_al. appos_modeling_Grenander appos_modeling_Miller appos_modeling_Mark nn_modeling_language amod_modeling_probabilistic prep_in_applied_modeling auxpass_applied_been aux_applied_has nsubjpass_applied_theory det_theory_The
J99-1004	J96-1002	o	Among the most widely studied is the Gibbs distribution -LRB- Mark Miller and Grenander 1996 Mark et al. 1996 Mark 1997 Abney 1997 -RRB-	num_Abney_1997 num_Mark_1997 dep_al._1996 nn_al._et nn_al._Mark num_Grenander_1996 dep_Mark_Abney dep_Mark_Mark dep_Mark_al. conj_and_Mark_Grenander conj_and_Mark_Miller appos_distribution_Grenander appos_distribution_Miller appos_distribution_Mark nn_distribution_Gibbs det_distribution_the nsubj_is_distribution prep_among_is_studied advmod_studied_widely det_studied_the advmod_widely_most
N03-1004	J96-1002	o	These distributions are modeled using a maximum entropy formulation -LRB- Berger et al. 1996 -RRB- using training data which consists of human judgments of question answer pairs	nn_pairs_answer dep_question_pairs prep_of_judgments_question amod_judgments_human prep_of_consists_judgments nsubj_consists_which rcmod_data_consists nn_data_training dobj_using_data amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_formulation_entropy nn_formulation_maximum det_formulation_a dobj_using_formulation xcomp_modeled_using dep_modeled_Berger xcomp_modeled_using auxpass_modeled_are nsubjpass_modeled_distributions det_distributions_These
N03-1028	J96-1002	o	The sequential classi cation approach can handle many correlated features as demonstrated in work on maximum-entropy -LRB- McCallum et al. 2000 Ratnaparkhi 1996 -RRB- and a variety of other linear classi ers including winnow -LRB- Punyakanok and Roth 2001 -RRB- AdaBoost -LRB- Abney et al. 1999 -RRB- and support-vector machines -LRB- Kudo and Matsumoto 2001 -RRB-	dep_Kudo_2001 conj_and_Kudo_Matsumoto amod_machines_support-vector amod_Abney_1999 dep_Abney_al. nn_Abney_et dep_AdaBoost_Matsumoto dep_AdaBoost_Kudo conj_and_AdaBoost_machines dep_AdaBoost_Abney dep_Punyakanok_2001 conj_and_Punyakanok_Roth appos_winnow_Roth appos_winnow_Punyakanok dep_ers_machines dep_ers_AdaBoost prep_including_ers_winnow nn_ers_classi amod_ers_linear amod_ers_other prep_of_variety_ers det_variety_a dep_Ratnaparkhi_1996 dep_McCallum_Ratnaparkhi appos_McCallum_2000 dep_McCallum_al. nn_McCallum_et conj_and_maximum-entropy_variety dep_maximum-entropy_McCallum prep_on_work_variety prep_on_work_maximum-entropy prep_in_demonstrated_work mark_demonstrated_as advcl_correlated_demonstrated dobj_correlated_features nsubj_correlated_many ccomp_handle_correlated aux_handle_can nsubj_handle_approach nn_approach_cation nn_approach_classi amod_approach_sequential det_approach_The
N03-1028	J96-1002	o	-LRB- 2001 -RRB- used iterative scaling algorithms for CRF training following earlier work on maximumentropy models for natural language -LRB- Berger et al. 1996 Della Pietra et al. 1997 -RRB-	num_Pietra_1997 nn_Pietra_al. nn_Pietra_et dep_Della_Pietra dep_Berger_Della appos_Berger_1996 dep_Berger_al. nn_Berger_et amod_language_natural prep_for_models_language amod_models_maximumentropy prep_on_work_models amod_work_earlier nn_training_CRF prep_for_algorithms_training nn_algorithms_scaling amod_algorithms_iterative dep_used_Berger prep_following_used_work dobj_used_algorithms dep_used_2001
N03-2008	J96-1002	o	-LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_''_Berger
N04-1001	J96-1002	o	For mention detection we use approaches based on Maximum Entropy -LRB- MaxEnt henceforth -RRB- -LRB- Berger et al. 1996 -RRB- and Robust Risk Minimization -LRB- RRM henceforth -RRB- 1For a description of the ACE program see http://www.nist.gov/speech/tests/ace/	dobj_see_http://www.nist.gov/speech/tests/ace/ nsubj_see_description nn_program_ACE det_program_the prep_of_description_program det_description_a rcmod_1For_see nn_1For_Minimization nn_henceforth_RRM appos_Minimization_henceforth nn_Minimization_Risk amod_Minimization_Robust amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_henceforth_MaxEnt conj_and_Entropy_1For dep_Entropy_Berger appos_Entropy_henceforth nn_Entropy_Maximum prep_on_based_1For prep_on_based_Entropy vmod_approaches_based dobj_use_approaches nsubj_use_we prep_for_use_detection nn_detection_mention
N04-1001	J96-1002	o	Algorithm 1 The RRM Decoding Algorithm foreacha26a29a27a67a42 foreacha68 a1a20a23a69a10a11a10a12a10a45 a60 a48a22a70a26a22a71 a1a73a72a2a25 a57a38a50 a7 a56 a48a54a57 a64a74a30 a57 a31a33a26a17a34 a5a11a75 a60a77a76a74a76 a31a78a26a35a34a66a79a81a80a83a82a38a84a69a85a86a80a24a87a88a48 a60 a48 a70a26a61a71 Somewhat similarly the MaxEnt algorithm has an associated set of weights a31a33a89 a48a54a57 a34a48a90a50 a7a53a52a54a52a54a52a15 a57a38a50 a7a58a52a54a52a54a52 a25 which are estimated during the training phase so as to maximize the likelihood of the data -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et det_data_the prep_of_likelihood_data det_likelihood_the dep_maximize_Berger dobj_maximize_likelihood aux_maximize_to nn_phase_training det_phase_the prepc_as_estimated_maximize advmod_estimated_so prep_during_estimated_phase auxpass_estimated_are nsubjpass_estimated_which rcmod_a7a58a52a54a52a54a52_estimated num_a7a58a52a54a52a54a52_a25 nn_a7a58a52a54a52a54a52_a57a38a50 nn_a7a58a52a54a52a54a52_a7a53a52a54a52a54a52a15 nn_a7a58a52a54a52a54a52_a34a48a90a50 num_a7a58a52a54a52a54a52_a48a54a57 number_a48a54a57_a31a33a89 dobj_weights_a7a58a52a54a52a54a52 prep_of_set_weights amod_set_associated det_set_an dobj_has_set rcmod_algorithm_has nn_algorithm_MaxEnt det_algorithm_the advmod_similarly_Somewhat appos_a70a26a61a71_algorithm advmod_a70a26a61a71_similarly nn_a70a26a61a71_a48 nn_a70a26a61a71_a60 nn_a70a26a61a71_a31a78a26a35a34a66a79a81a80a83a82a38a84a69a85a86a80a24a87a88a48 nn_a70a26a61a71_a60a77a76a74a76 nn_a70a26a61a71_a5a11a75 nn_a70a26a61a71_a31a33a26a17a34 nn_a70a26a61a71_a57 nn_a70a26a61a71_a64a74a30 num_a70a26a61a71_a48a54a57 nn_a70a26a61a71_a56 nn_a70a26a61a71_a7 nn_a70a26a61a71_a57a38a50 num_a70a26a61a71_a1a73a72a2a25 nn_a70a26a61a71_a48a22a70a26a22a71 nn_a70a26a61a71_a60 nn_a70a26a61a71_a1a20a23a69a10a11a10a12a10a45 nn_a70a26a61a71_foreacha68 nn_a70a26a61a71_foreacha26a29a27a67a42 nn_a70a26a61a71_Algorithm nn_a70a26a61a71_Decoding nn_a70a26a61a71_RRM det_a70a26a61a71_The num_a70a26a61a71_1 nn_a70a26a61a71_Algorithm
N04-1037	J96-1002	o	Maximum Entropy Modeling As previously indicated the weight-based scheme of L&L suggests MaxEnt modeling -LRB- Berger et al. 1996 -RRB- as a particularly natural choice for a machine learning approach	amod_approach_learning nsubj_approach_machine det_machine_a prepc_for_choice_approach amod_choice_natural det_choice_a advmod_natural_particularly amod_Berger_1996 dep_Berger_al. nn_Berger_et prep_as_modeling_choice dep_modeling_Berger nn_modeling_MaxEnt dobj_suggests_modeling nsubj_suggests_Entropy prep_of_scheme_L&L amod_scheme_weight-based det_scheme_the advmod_indicated_previously mark_indicated_As advcl_Modeling_indicated appos_Entropy_scheme vmod_Entropy_Modeling nn_Entropy_Maximum
N04-1039	J96-1002	o	1 Introduction Conditional Maximum Entropy -LRB- maxent -RRB- models have been widely used for a variety of tasks including language modeling -LRB- Rosenfeld 1994 -RRB- part-of-speech tagging prepositional phrase attachment and parsing -LRB- Ratnaparkhi 1998 -RRB- word selection for machine translation -LRB- Berger et al. 1996 -RRB- and finding sentence boundaries -LRB- Reynar and Ratnaparkhi 1997 -RRB-	amod_Reynar_1997 conj_and_Reynar_Ratnaparkhi dep_boundaries_Ratnaparkhi dep_boundaries_Reynar nn_boundaries_sentence dobj_finding_boundaries amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_translation_machine dep_selection_Berger prep_for_selection_translation nn_selection_word amod_Ratnaparkhi_1998 dep_parsing_Ratnaparkhi nn_attachment_phrase amod_attachment_prepositional amod_tagging_part-of-speech amod_Rosenfeld_1994 conj_and_modeling_parsing conj_and_modeling_attachment conj_and_modeling_tagging dep_modeling_Rosenfeld nn_modeling_language conj_and_tasks_finding conj_and_tasks_selection prep_including_tasks_parsing prep_including_tasks_attachment prep_including_tasks_tagging prep_including_tasks_modeling prep_of_variety_finding prep_of_variety_selection prep_of_variety_tasks det_variety_a prep_for_used_variety advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_models dep_models_maxent nn_models_Entropy nn_Entropy_Maximum amod_Entropy_Conditional nn_Entropy_Introduction num_Entropy_1
N04-2003	J96-1002	o	A major issue in MaxEnt training is how to select proper features and determine the feature targets -LRB- Berger et al. 1996 Jebara and Jaakkola 2000 -RRB-	amod_Jebara_2000 conj_and_Jebara_Jaakkola dep_Berger_Jaakkola dep_Berger_Jebara appos_Berger_1996 dep_Berger_al. nn_Berger_et dep_targets_Berger nn_targets_feature det_targets_the dobj_determine_targets amod_features_proper conj_and_select_determine dobj_select_features aux_select_to advmod_select_how ccomp_is_determine ccomp_is_select nsubj_is_issue nn_training_MaxEnt prep_in_issue_training amod_issue_major det_issue_A
N04-2003	J96-1002	o	3 Feature selection Berger et al -LRB- 1996 -RRB- proposed an iterative procedure of adding news features to feature set driven by data	agent_driven_data vmod_set_driven dep_feature_set nn_features_news prep_to_adding_feature dobj_adding_features prepc_of_procedure_adding amod_procedure_iterative det_procedure_an dobj_proposed_procedure advmod_proposed_al nsubj_proposed_Berger appos_al_1996 nn_al_et nn_Berger_selection nn_Berger_Feature num_Berger_3
N06-1013	J96-1002	o	Given a collection of facts ME chooses a model consistent with all the facts but otherwise as uniform as possible -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_possible_Berger dep_possible_as advmod_possible_otherwise nsubj_possible_ME prep_as_otherwise_uniform det_facts_the predet_facts_all prep_with_consistent_facts amod_model_consistent det_model_a conj_but_chooses_possible dobj_chooses_model nsubj_chooses_ME prep_chooses_Given prep_of_collection_facts det_collection_a pobj_Given_collection
N06-1013	J96-1002	o	Maximum entropy -LRB- ME -RRB- models have been used in bilingual sense disambiguation word reordering and sentence segmentation -LRB- Berger et al. 1996 -RRB- parsing POS tagging and PP attachment -LRB- Ratnaparkhi 1998 -RRB- machine translation -LRB- Och and Ney 2002 -RRB- and FrameNet classification -LRB- Fleischman et al. 2003 -RRB-	amod_Fleischman_2003 dep_Fleischman_al. nn_Fleischman_et nn_classification_FrameNet dep_Och_2002 conj_and_Och_Ney appos_translation_Ney appos_translation_Och nn_translation_machine amod_Ratnaparkhi_1998 nn_attachment_PP nn_tagging_POS dep_parsing_Fleischman conj_and_parsing_classification conj_and_parsing_translation dep_parsing_Ratnaparkhi conj_and_parsing_attachment conj_and_parsing_tagging amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_segmentation_Berger nn_segmentation_sentence nn_reordering_word nn_disambiguation_sense amod_disambiguation_bilingual conj_and_used_classification conj_and_used_translation conj_and_used_attachment conj_and_used_tagging conj_and_used_parsing conj_and_used_segmentation conj_and_used_reordering prep_in_used_disambiguation auxpass_used_been aux_used_have nsubjpass_used_models nn_models_entropy appos_entropy_ME nn_entropy_Maximum
N06-1025	J96-1002	o	3.2 Learning Algorithm For learning coreference decisions we used a Maximum Entropy -LRB- Berger et al. 1996 -RRB- model	dep_model_Berger dep_Berger_1996 dep_Berger_al. nn_Berger_et dep_Entropy_model nn_Entropy_Maximum det_Entropy_a dobj_used_Entropy nsubj_used_we nsubj_used_Algorithm nn_decisions_coreference dobj_learning_decisions prepc_for_Algorithm_learning nn_Algorithm_Learning num_Algorithm_3.2
N06-1026	J96-1002	o	Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible -LRB- Berger et al. 1996 -RRB-	dep_al._1996 nn_al._et advmod_Berger_al. dep_possible_Berger dep_possible_as dep_is_possible prep_as_is_uniform advmod_is_otherwise nsubj_is_that det_evidence_the agent_imposed_evidence vmod_constraints_imposed prep_of_set_constraints det_set_the conj_but_consistent_is prep_with_consistent_set cop_consistent_is nsubj_consistent_that rcmod_one_is rcmod_one_consistent det_one_the cop_one_is nsubj_one_model mark_one_that amod_model_best det_model_the ccomp_intuition_one det_intuition_the dobj_implement_intuition nsubj_implement_models nn_models_Entropy nn_models_Maximum
N06-2036	J96-1002	o	The algorithm employs the OpenNLP MaxEnt implementation of the maximum entropy classification algorithm -LRB- Berger et al. 1996 -RRB- to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs	nsubj_occurs_lemma prep_in_occurs_which det_lemma_the rcmod_context_occurs det_context_the det_lemma_the prep_for_sense_lemma amod_sense_likely det_sense_the advmod_likely_most pobj_predicts_context prepc_according_to_predicts_to dobj_predicts_sense nsubj_predicts_which rcmod_lemma_predicts det_lemma_each prep_for_signatures_lemma nn_signatures_recognition nn_signatures_sense nn_signatures_word dobj_develop_signatures aux_develop_to dep_al._1996 nn_al._et advmod_Berger_al. nn_algorithm_classification nn_algorithm_entropy nn_algorithm_maximum det_algorithm_the prep_of_implementation_algorithm nn_implementation_MaxEnt nn_implementation_OpenNLP det_implementation_the vmod_employs_develop dep_employs_Berger dobj_employs_implementation nsubj_employs_algorithm det_algorithm_The
N07-1001	J96-1002	o	The best prosodic label sequence is then L = argmax L nproductdisplay i P -LRB- li | -RRB- -LRB- 6 -RRB- To estimate the conditional distribution P -LRB- li | -RRB- we use the general technique of choosing the maximum entropy -LRB- maxent -RRB- distribution that estimates the average of each feature over the training data -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_data_Berger nn_data_training det_data_the det_feature_each prep_of_average_feature det_average_the prep_over_estimates_data dobj_estimates_average nsubj_estimates_that rcmod_distribution_estimates nn_distribution_entropy appos_entropy_maxent nn_entropy_maximum det_entropy_the dobj_choosing_distribution prepc_of_technique_choosing amod_technique_general det_technique_the dobj_use_technique nsubj_use_we nn_|_li rcmod_P_use appos_P_| nn_P_distribution amod_P_conditional det_P_the dobj_estimate_P aux_estimate_To nn_|_li appos_P_6 appos_P_| nn_P_i nn_P_nproductdisplay nn_P_L nn_P_argmax vmod_=_estimate nsubj_=_P nsubj_=_L ccomp_=_is advmod_is_then nsubj_is_sequence nn_sequence_label amod_sequence_prosodic amod_sequence_best det_sequence_The
N07-1001	J96-1002	o	We report results on the Boston University -LRB- BU -RRB- Radio Speech Corpus -LRB- Ostendorf et al. 1995 -RRB- and Boston Directions Corpus -LRB- BDC -RRB- -LRB- Hirschberg and Nakatani 1996 -RRB- two publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling	nn_labeling_prosody amod_labeling_automatic prep_in_experiments_labeling prep_for_intended_experiments vmod_annotations_intended nn_annotations_ToBI amod_annotations_manual prep_with_corpora_annotations nn_corpora_speech amod_corpora_available num_corpora_two advmod_available_publicly dep_Hirschberg_1996 conj_and_Hirschberg_Nakatani appos_Corpus_BDC nn_Corpus_Directions nn_Corpus_Boston amod_Ostendorf_1995 dep_Ostendorf_al. nn_Ostendorf_et conj_and_Corpus_Corpus dep_Corpus_Ostendorf nn_Corpus_Speech nn_Corpus_Radio appos_University_Nakatani appos_University_Hirschberg dep_University_Corpus dep_University_Corpus appos_University_BU nn_University_Boston det_University_the prep_on_results_University dep_report_corpora dobj_report_results nsubj_report_We
N07-1009	J96-1002	n	But without the global normalization the maximumlikelihood criterion motivated by the maximum entropy principle -LRB- Berger et al. 1996 -RRB- is no longer a feasible option as an optimization criterion	nn_criterion_optimization det_criterion_an prep_as_option_criterion amod_option_feasible det_option_a advmod_option_longer cop_option_is nsubj_option_criterion prep_without_option_normalization cc_option_But neg_longer_no amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_principle_entropy nn_principle_maximum det_principle_the agent_motivated_principle appos_criterion_Berger vmod_criterion_motivated nn_criterion_maximumlikelihood det_criterion_the amod_normalization_global det_normalization_the
N07-1010	J96-1002	o	3 Implementation 3.1 Feature Structure To implement the twin model we adopt the log linear or maximum entropy -LRB- MaxEnt -RRB- model -LRB- Berger et al. 1996 -RRB- for its flexibility of combining diverse sources of information	prep_of_sources_information amod_sources_diverse dobj_combining_sources prepc_of_flexibility_combining poss_flexibility_its amod_Berger_1996 dep_Berger_al. nn_Berger_et appos_entropy_MaxEnt nn_entropy_maximum dep_linear_model conj_or_linear_entropy nn_linear_log det_linear_the prep_for_adopt_flexibility dep_adopt_Berger dobj_adopt_entropy dobj_adopt_linear nsubj_adopt_we nsubj_adopt_Structure amod_model_twin det_model_the dobj_implement_model aux_implement_To vmod_Structure_implement nn_Structure_Feature num_Structure_3.1 nn_Structure_Implementation num_Structure_3
N07-1010	J96-1002	o	Once the set of features functions are selected algorithm such as improved iterative scaling -LRB- Berger et al. 1996 -RRB- or sequential conditional generalized iterative scaling -LRB- Goodman 2002 -RRB- can be used to find the optimal parameter values of fkg and fig	conj_and_fkg_fig prep_of_values_fig prep_of_values_fkg nn_values_parameter amod_values_optimal det_values_the dobj_find_values aux_find_to xcomp_used_find auxpass_used_be aux_used_can nsubjpass_used_algorithm amod_Goodman_2002 amod_scaling_iterative amod_scaling_generalized amod_scaling_conditional amod_scaling_sequential amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_scaling_Goodman conj_or_scaling_scaling dep_scaling_Berger amod_scaling_iterative amod_scaling_improved prep_such_as_algorithm_scaling prep_such_as_algorithm_scaling ccomp_selected_used auxpass_selected_are nsubjpass_selected_set nn_functions_features prep_of_set_functions det_set_the advmod_set_Once
N07-1030	J96-1002	o	Model parameters are estimated using maximum entropy -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_entropy_Berger nn_entropy_maximum dobj_using_entropy xcomp_estimated_using auxpass_estimated_are nsubjpass_estimated_parameters nn_parameters_Model
N07-1046	J96-1002	o	With hand-labeled data -LCB- m -RCB- can be learnt via generalized iterative scaling algorithm -LRB- GIS -RRB- -LRB- Darroch and Ratcliff 1972 -RRB- or improved iterative scaling -LRB- IIS -RRB- -LRB- Berger 367 et al. 1996 -RRB-	nn_al._et num_al._367 amod_Berger_1996 dep_Berger_al. appos_scaling_IIS amod_scaling_iterative amod_scaling_improved dep_Darroch_1972 conj_and_Darroch_Ratcliff conj_or_algorithm_scaling appos_algorithm_Ratcliff appos_algorithm_Darroch appos_algorithm_GIS amod_algorithm_scaling amod_algorithm_iterative amod_algorithm_generalized dep_learnt_Berger prep_via_learnt_scaling prep_via_learnt_algorithm auxpass_learnt_be aux_learnt_can nsubjpass_learnt_m prep_with_learnt_data amod_data_hand-labeled
N07-1046	J96-1002	o	This sequential property is well suited to HMMs -LRB- Vogel et al. 1996 -RRB- in which the jumps from the current aligned position can only be forward	advmod_be_forward advmod_be_only aux_be_can nsubj_be_jumps prep_in_be_which amod_position_aligned amod_position_current det_position_the prep_from_jumps_position det_jumps_the amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et parataxis_suited_be dep_suited_Vogel prep_to_suited_HMMs advmod_suited_well auxpass_suited_is nsubjpass_suited_property amod_property_sequential det_property_This
N07-2043	J96-1002	o	To reduce the knowledge engineering burden on the user in constructing and porting an IE system unsupervised learning has been utilized e.g. Riloff -LRB- 1996 -RRB- Yangarber et al.	nn_al._et nn_al._Yangarber dep_Riloff_1996 dep_e.g._Riloff conj_utilized_al. conj_utilized_e.g. auxpass_utilized_been aux_utilized_has nsubjpass_utilized_learning advcl_utilized_reduce amod_learning_unsupervised amod_system_IE det_system_an dobj_constructing_system conj_and_constructing_porting det_user_the prep_on_burden_user nn_burden_engineering nn_burden_knowledge det_burden_the prepc_in_reduce_porting prepc_in_reduce_constructing dobj_reduce_burden aux_reduce_To
N07-2043	J96-1002	o	For the classifier we used the OpenNLP MaxEnt implementation -LRB- maxent.sourceforge.net -RRB- of the maximum entropy classification algorithm -LRB- Berger et al. 1996 -RRB-	advmod_1996_al. nn_al._et num_Berger_1996 nn_algorithm_classification nn_algorithm_entropy nn_algorithm_maximum det_algorithm_the prep_of_implementation_algorithm appos_implementation_maxent.sourceforge.net nn_implementation_MaxEnt nn_implementation_OpenNLP det_implementation_the dep_used_Berger dobj_used_implementation nsubj_used_we prep_for_used_classifier det_classifier_the
N09-1013	J96-1002	o	Since it is not feasible to maximise the likelihood of the observations directly we maximise the expected log likelihood by considering the EM auxiliary function in a similar manner to that used for modelling contextual variations of phones for ASR -LRB- Young et al. 1994 Singer and Ostendorf 1996 -RRB-	appos_Singer_1996 conj_and_Singer_Ostendorf dep_Young_Ostendorf dep_Young_Singer appos_Young_1994 dep_Young_al. nn_Young_et prep_for_variations_ASR prep_of_variations_phones amod_variations_contextual amod_variations_modelling prep_for_used_variations vmod_that_used prep_to_manner_that amod_manner_similar det_manner_a dep_,_Young prep_in_,_manner nn_function_auxiliary nn_function_EM det_function_the dobj_considering_function nn_likelihood_log amod_likelihood_expected det_likelihood_the prepc_by_maximise_considering dobj_maximise_likelihood nsubj_maximise_we advcl_maximise_feasible det_observations_the prep_of_likelihood_observations det_likelihood_the advmod_maximise_directly dobj_maximise_likelihood aux_maximise_to xcomp_feasible_maximise neg_feasible_not cop_feasible_is nsubj_feasible_it mark_feasible_Since ccomp_``_maximise
N09-1022	J96-1002	o	3.5 Maximum Entropy Model In order to build a unified probabilistic query alteration model we used the maximum entropy approach of -LRB- Beger et al. 1996 -RRB- which Li et al.	nn_al._et nn_al._Li dep_al._which dep_Beger_al. amod_Beger_1996 dep_Beger_al. nn_Beger_et prep_of_approach_Beger amod_approach_entropy nn_approach_maximum det_approach_the dobj_used_approach nsubj_used_we advcl_used_build nsubj_used_Model nn_model_alteration nn_model_query amod_model_probabilistic amod_model_unified det_model_a dobj_build_model aux_build_to dep_build_order mark_build_In nn_Model_Entropy nn_Model_Maximum num_Model_3.5
N09-1046	J96-1002	p	We decided to use the class of maximum entropy models which are probabilistically sound can make use of possibly many overlapping features and can be trained efficiently -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_trained_Berger advmod_trained_efficiently auxpass_trained_be aux_trained_can amod_features_overlapping amod_features_many advmod_many_possibly prep_of_use_features conj_and_make_trained dobj_make_use aux_make_can advmod_sound_probabilistically cop_sound_are nsubj_sound_which nn_models_entropy nn_models_maximum rcmod_class_sound prep_of_class_models det_class_the dobj_use_class aux_use_to dep_decided_trained dep_decided_make xcomp_decided_use nsubj_decided_We
N09-1065	J96-1002	o	2.1 The Standard Machine Learning Approach We use maximum entropy -LRB- MaxEnt -RRB- classification -LRB- Berger et al. 1996 -RRB- in conjunction with the 33 features described in Ng -LRB- 2007 -RRB- to acquire a model PC for determining the probability that two mentions mi and mj are coreferent	cop_coreferent_are ccomp_coreferent_2.1 conj_and_mi_mj dep_mentions_mj dep_mentions_mi nsubj_mentions_two dobj_mentions_that rcmod_probability_mentions det_probability_the dobj_determining_probability prepc_for_model_determining appos_model_PC det_model_a dobj_acquire_model aux_acquire_to appos_Ng_2007 xcomp_described_acquire prep_in_described_Ng vmod_features_described num_features_33 det_features_the prep_with_conjunction_features amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_classification_entropy appos_entropy_MaxEnt amod_entropy_maximum dobj_use_classification nsubj_use_We prep_in_Approach_conjunction dep_Approach_Berger rcmod_Approach_use nn_Approach_Learning nn_Approach_Machine amod_Approach_Standard det_Approach_The dep_2.1_Approach
N09-1065	J96-1002	o	Research in the first category aims to identify specific types of nonanaphoric phrases with some identifying pleonastic it -LRB- using heuristics -LSB- e.g. Paice and Husk -LRB- 1987 -RRB- Lappin and Leass -LRB- 1994 -RRB- Kennedy and Boguraev -LRB- 1996 -RRB- -RSB- supervised approaches -LSB- e.g. Evans -LRB- 2001 -RRB- Muller -LRB- 2006 -RRB- Versley et al.	nn_al._et nn_al._Versley appos_Muller_2006 dep_Evans_al. conj_Evans_Muller appos_Evans_2001 appos_e.g._Evans prep_approaches_e.g. amod_approaches_supervised appos_Boguraev_1996 conj_and_Kennedy_Boguraev appos_Leass_1994 appos_Husk_1987 conj_and_Paice_Leass conj_and_Paice_Lappin conj_and_Paice_Husk dep_e.g._Boguraev dep_e.g._Kennedy dep_e.g._Leass dep_e.g._Lappin dep_e.g._Husk dep_e.g._Paice dobj_using_approaches dep_using_e.g. dobj_using_heuristics dep_it_using dep_pleonastic_it dobj_identifying_pleonastic vmod_some_identifying amod_phrases_nonanaphoric prep_of_types_phrases amod_types_specific dobj_identify_types aux_identify_to prep_with_aims_some xcomp_aims_identify nsubj_aims_Research amod_category_first det_category_the prep_in_Research_category
N09-2014	J96-1002	o	Our approach is to use maximum entropy models -LRB- Berger et al. 1996 -RRB- to learn a suitable mapping from features derived from the words in the ASR output to semantic frames	amod_frames_semantic prep_to_output_frames nn_output_ASR det_output_the prep_in_words_output det_words_the prep_from_derived_words vmod_features_derived prep_from_mapping_features amod_mapping_suitable det_mapping_a dobj_learn_mapping aux_learn_to amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_models_entropy amod_models_maximum vmod_use_learn dep_use_Berger dobj_use_models aux_use_to xcomp_is_use nsubj_is_approach poss_approach_Our ccomp_``_is
N09-2026	J96-1002	o	In -LRB- Teevan et al. 1996 -RRB- it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search	amod_search_repeated det_search_a prep_to_associated_search auxpass_associated_are nsubjpass_associated_percent mark_associated_that nn_engine_search det_engine_a prep_in_user_engine det_user_a agent_made_user vmod_queries_made det_queries_the prep_of_percent_queries amod_percent_significant det_percent_a ccomp_observed_associated auxpass_observed_was nsubjpass_observed_it prep_observed_In nn_al._et amod_Teevan_1996 dep_Teevan_al. dep_In_Teevan
N09-3017	J96-1002	p	2.3 Classifier Training We chose maximum entropy -LRB- Berger et al. 1996 -RRB- as our primary classifier since it had been successfully applied by the highest performing systems in both the SemEval-2007 preposition sense disambiguation task -LRB- Ye and Baldwin 2007 -RRB- and the general word sense disambiguation task -LRB- Tratz et al. 2007 -RRB-	amod_Tratz_2007 dep_Tratz_al. nn_Tratz_et nn_task_disambiguation nn_task_sense nn_task_word amod_task_general det_task_the dep_Ye_2007 conj_and_Ye_Baldwin conj_and_task_task dep_task_Baldwin dep_task_Ye nn_task_disambiguation nn_task_sense nn_task_preposition nn_task_SemEval-2007 det_task_the preconj_task_both prep_in_systems_task prep_in_systems_task amod_systems_performing amod_systems_highest det_systems_the agent_applied_systems advmod_applied_successfully auxpass_applied_been aux_applied_had nsubjpass_applied_it mark_applied_since amod_classifier_primary poss_classifier_our amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_entropy_Berger nn_entropy_maximum advcl_chose_applied prep_as_chose_classifier dobj_chose_entropy nsubj_chose_We ccomp_Training_chose dep_Classifier_Tratz vmod_Classifier_Training num_Classifier_2.3 dep_``_Classifier
N09-3017	J96-1002	p	2.3 Classifier Training We chose maximum entropy -LRB- Berger 1996 -RRB- as our primary classifier because the highest performing systems in both the SemEval-2007 preposition sense disambiguation task -LRB- Ye and Baldwin 2007 -RRB- and the general word sense disambiguation task -LRB- Tratz et al. 2007 -RRB- used it	dobj_used_it nsubj_used_systems mark_used_because amod_Tratz_2007 dep_Tratz_al. nn_Tratz_et nn_task_disambiguation nn_task_sense nn_task_word amod_task_general det_task_the dep_Ye_2007 conj_and_Ye_Baldwin conj_and_task_task dep_task_Baldwin dep_task_Ye nn_task_disambiguation nn_task_sense nn_task_preposition nn_task_SemEval-2007 det_task_the preconj_task_both dep_systems_Tratz prep_in_systems_task prep_in_systems_task amod_systems_performing amod_systems_highest det_systems_the amod_classifier_primary poss_classifier_our amod_Berger_1996 dep_entropy_Berger nn_entropy_maximum advcl_chose_used prep_as_chose_classifier dobj_chose_entropy nsubj_chose_We ccomp_Training_chose vmod_Classifier_Training num_Classifier_2.3 dep_``_Classifier
P01-1003	J96-1002	o	Using the ME principle we can combine information from a variety of sources into the same language model -LRB- Berger et al. 1996 Rosenfeld 1996 -RRB-	amod_Rosenfeld_1996 dep_Berger_Rosenfeld appos_Berger_1996 dep_Berger_al. nn_Berger_et nn_model_language amod_model_same det_model_the prep_of_variety_sources det_variety_a prep_from_information_variety dep_combine_Berger prep_into_combine_model dobj_combine_information aux_combine_can nsubj_combine_we vmod_combine_Using nn_principle_ME det_principle_the dobj_Using_principle
P01-1027	J96-1002	o	-LRB- Berger et al. 1996 -RRB- applies this approach to the so-called IBM Candide system to build context dependent models compute automatic sentence splitting and to improve word reordering in translation	prep_in_reordering_translation nn_reordering_word dobj_improve_reordering aux_improve_to nn_splitting_sentence amod_sentence_automatic dobj_compute_splitting amod_models_dependent nn_models_context conj_and_build_improve conj_and_build_compute dobj_build_models aux_build_to nn_system_Candide nn_system_IBM amod_system_so-called det_system_the det_approach_this vmod_applies_improve vmod_applies_compute vmod_applies_build prep_to_applies_system dobj_applies_approach nsubj_applies_Berger amod_Berger_1996 dep_Berger_al. nn_Berger_et
P01-1027	J96-1002	o	Similar techniques are used in -LRB- Papineni et al. 1996 Papineni et al. 1998 -RRB- for socalled direct translation models instead of those proposed in -LRB- Brown et al. 1993 -RRB-	num_al._1993 nn_al._et amod_al._Brown dep_in_al. prep_proposed_in vmod_those_proposed conj_negcc_models_those nn_models_translation amod_models_direct amod_models_socalled num_Papineni_1998 nn_Papineni_al. nn_Papineni_et dep_Papineni_Papineni appos_Papineni_1996 dep_Papineni_al. nn_Papineni_et prep_for_used_those prep_for_used_models prep_in_used_Papineni auxpass_used_are nsubjpass_used_techniques amod_techniques_Similar
P01-1027	J96-1002	o	Other authors have applied this approach to language modeling -LRB- Rosenfeld 1996 Martin et al. 1999 Peters and Klakow 1999 -RRB-	appos_Peters_1999 conj_and_Peters_Klakow num_Martin_1999 nn_Martin_al. nn_Martin_et dep_Rosenfeld_Klakow dep_Rosenfeld_Peters conj_Rosenfeld_Martin amod_Rosenfeld_1996 dep_modeling_Rosenfeld nn_modeling_language det_approach_this prep_to_applied_modeling dobj_applied_approach aux_applied_have nsubj_applied_authors amod_authors_Other ccomp_``_applied
P01-1027	J96-1002	o	The resulting model has an exponential form with free parameters a102 a91 a24a94a93 a8 a87 a24 a10a11a10a11a10 a24a46a95 The parameter values which maximize the likelihood for a given training corpus can be computed with the socalled GIS algorithm -LRB- general iterative scaling -RRB- or its improved version IIS -LRB- Pietra et al. 1997 Berger et al. 1996 -RRB-	num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Pietra_Berger appos_Pietra_1997 dep_Pietra_al. nn_Pietra_et dep_IIS_Pietra nn_IIS_version amod_IIS_improved poss_IIS_its amod_scaling_iterative amod_scaling_general conj_or_algorithm_IIS appos_algorithm_scaling nn_algorithm_GIS amod_algorithm_socalled det_algorithm_the prep_with_computed_IIS prep_with_computed_algorithm auxpass_computed_be aux_computed_can nsubjpass_computed_values nn_corpus_training amod_corpus_given det_corpus_a prep_for_likelihood_corpus det_likelihood_the dobj_maximize_likelihood nsubj_maximize_which rcmod_values_maximize nn_values_parameter det_values_The nn_a24a46a95_a10a11a10a11a10 nn_a24a46a95_a24 nn_a24a46a95_a87 nn_a24a46a95_a8 nn_a24a46a95_a24a94a93 nn_a24a46a95_a91 dobj_a102_a24a46a95 nsubj_a102_form amod_parameters_free prep_with_form_parameters amod_form_exponential det_form_an parataxis_has_computed ccomp_has_a102 nsubj_has_model amod_model_resulting det_model_The
P01-1027	J96-1002	o	In this work we use the following contextual information a3 Target context As in -LRB- Berger et al. 1996 -RRB- we consider a window of 3 words to the left and to the right of the target word considered	vmod_word_considered nn_word_target det_word_the prep_of_right_word det_right_the pobj_to_right det_left_the num_words_3 prep_of_window_words det_window_a conj_and_consider_to prep_to_consider_left dobj_consider_window nsubj_consider_we dep_consider_Berger mark_consider_in amod_Berger_1996 dep_Berger_al. nn_Berger_et prepc_as_context_to prepc_as_context_consider nn_context_Target nn_context_a3 amod_information_contextual amod_information_following det_information_the dep_use_context dobj_use_information nsubj_use_we prep_in_use_work det_work_this
P01-1042	J96-1002	o	In statistical computational linguistics maximum conditional likelihood estimators have mostly been used with general exponential or maximum entropy models because standard maximum likelihood estimation is usually computationally intractable -LRB- Berger et al. 1996 Della Pietra et al. 1997 Jelinek 1997 -RRB-	amod_Jelinek_1997 dep_Pietra_Jelinek num_Pietra_1997 nn_Pietra_al. nn_Pietra_et dep_Della_Pietra dep_Berger_Della appos_Berger_1996 dep_Berger_al. nn_Berger_et advmod_intractable_computationally advmod_intractable_usually cop_intractable_is nsubj_intractable_estimation mark_intractable_because nn_estimation_likelihood nn_estimation_maximum amod_estimation_standard amod_models_entropy amod_models_maximum conj_or_exponential_models amod_exponential_general dep_used_Berger advcl_used_intractable prep_with_used_models prep_with_used_exponential auxpass_used_been advmod_used_mostly aux_used_have nsubjpass_used_estimators prep_in_used_linguistics nn_estimators_likelihood amod_estimators_conditional nn_estimators_maximum amod_linguistics_computational amod_linguistics_statistical
P02-1002	J96-1002	o	1 Introduction Conditional Maximum Entropy models have been used for a variety of natural language tasks including Language Modeling -LRB- Rosenfeld 1994 -RRB- partof-speech tagging prepositional phrase attachment and parsing -LRB- Ratnaparkhi 1998 -RRB- word selection for machine translation -LRB- Berger et al. 1996 -RRB- and finding sentence boundaries -LRB- Reynar and Ratnaparkhi 1997 -RRB-	amod_Reynar_1997 conj_and_Reynar_Ratnaparkhi dep_boundaries_Ratnaparkhi dep_boundaries_Reynar nn_boundaries_sentence dobj_finding_boundaries amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_translation_machine dep_selection_Berger prep_for_selection_translation nn_selection_word amod_Ratnaparkhi_1998 dep_parsing_Ratnaparkhi nn_attachment_phrase amod_attachment_prepositional amod_tagging_partof-speech amod_Rosenfeld_1994 conj_and_Modeling_parsing conj_and_Modeling_attachment conj_and_Modeling_tagging dep_Modeling_Rosenfeld nn_Modeling_Language conj_and_tasks_finding conj_and_tasks_selection prep_including_tasks_parsing prep_including_tasks_attachment prep_including_tasks_tagging prep_including_tasks_Modeling nn_tasks_language amod_tasks_natural prep_of_variety_finding prep_of_variety_selection prep_of_variety_tasks det_variety_a prep_for_used_variety auxpass_used_been aux_used_have nsubjpass_used_models nn_models_Entropy nn_models_Maximum amod_models_Conditional nn_models_Introduction num_models_1
P02-1025	J96-1002	o	One solution would be to apply the maximum entropy estimation technique -LRB- MaxEnt -LRB- Berger et al. 1996 -RRB- -RRB- to all of the three components of the SLM or at least to the CONSTRUCTOR	det_CONSTRUCTOR_the pobj_to_CONSTRUCTOR advmod_to_at pobj_at_least det_SLM_the prep_of_components_SLM num_components_three det_components_the prep_of_all_components amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_MaxEnt_Berger dep_technique_MaxEnt nn_technique_estimation amod_technique_entropy nn_technique_maximum det_technique_the conj_or_apply_to prep_to_apply_all dobj_apply_technique aux_apply_to xcomp_be_to xcomp_be_apply aux_be_would nsubj_be_solution num_solution_One ccomp_``_be
P02-1038	J96-1002	o	An especially well-founded framework for doing this is maximum entropy -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_entropy_Berger amod_entropy_maximum cop_entropy_is nsubj_entropy_framework dobj_doing_this prepc_for_framework_doing amod_framework_well-founded det_framework_An advmod_well-founded_especially
P02-1063	J96-1002	o	Various learning models have been studied such as Hidden Markov models -LRB- HMMs -RRB- -LRB- Rabiner and Juang 1993 -RRB- decision trees -LRB- Breiman et al. 1984 -RRB- and maximum entropy models -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_models_entropy nn_models_maximum amod_Breiman_1984 dep_Breiman_al. nn_Breiman_et dep_trees_Breiman nn_trees_decision dep_Rabiner_1993 conj_and_Rabiner_Juang conj_and_models_models conj_and_models_trees appos_models_Juang appos_models_Rabiner appos_models_HMMs nn_models_Markov nn_models_Hidden dep_studied_Berger prep_such_as_studied_models prep_such_as_studied_trees prep_such_as_studied_models auxpass_studied_been aux_studied_have nsubjpass_studied_models nn_models_learning amod_models_Various
P03-1012	J96-1002	p	Maximum entropy can be used to improve IBM-style translation probabilities by using features such as improvements to P -LRB- f | e -RRB- in -LRB- Berger et al. 1996 -RRB-	dep_Berger_al. nn_Berger_et dep_|_1996 dep_|_Berger dep_|_in dep_|_e nn_|_f dep_P_| prep_to_improvements_P prep_such_as_features_improvements dobj_using_features nn_probabilities_translation amod_probabilities_IBM-style prepc_by_improve_using dobj_improve_probabilities aux_improve_to xcomp_used_improve auxpass_used_be aux_used_can nsubjpass_used_entropy nn_entropy_Maximum
P03-1012	J96-1002	o	For example alignments can be used to learn translation lexicons -LRB- Melamed 1996 -RRB- transfer rules -LRB- Carbonell et al. 2002 Menezes and Richardson 2001 -RRB- and classifiers to find safe sentence segmentation points -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et nn_points_segmentation nn_points_sentence amod_points_safe dobj_find_points aux_find_to vmod_classifiers_find dep_Menezes_2001 conj_and_Menezes_Richardson dep_Carbonell_Richardson dep_Carbonell_Menezes appos_Carbonell_2002 dep_Carbonell_al. nn_Carbonell_et dep_rules_Berger conj_and_rules_classifiers appos_rules_Carbonell nn_rules_transfer amod_Melamed_1996 dep_lexicons_Melamed nn_lexicons_translation dobj_learn_lexicons aux_learn_to parataxis_used_classifiers parataxis_used_rules xcomp_used_learn auxpass_used_be aux_used_can nsubjpass_used_alignments prep_for_used_example
P03-1012	J96-1002	o	It has been observed that words close to each other in the source language tend to remain close to each other in the translation -LRB- Vogel et al. 1996 Ker and Change 1997 -RRB-	appos_Ker_1997 conj_and_Ker_Change dep_Vogel_Change dep_Vogel_Ker amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et det_translation_the det_other_each prep_to_close_other prep_in_remain_translation acomp_remain_close aux_remain_to xcomp_tend_remain nsubj_tend_words mark_tend_that nn_language_source det_language_the det_other_each prep_to_close_other prep_in_words_language advmod_words_close dep_observed_Vogel ccomp_observed_tend auxpass_observed_been aux_observed_has nsubjpass_observed_It
P03-1015	J96-1002	o	We used the Maximum Entropy approach5 -LRB- Berger et al. 1996 -RRB- as a machine learner for this task	det_task_this prep_for_learner_task nn_learner_machine det_learner_a amod_Berger_1996 dep_Berger_al. nn_Berger_et dep_approach5_Berger nn_approach5_Entropy nn_approach5_Maximum det_approach5_the prep_as_used_learner dobj_used_approach5 nsubj_used_We
P03-1055	J96-1002	o	The other main difference is the apparently nonlocal nature of the problem which motivates our choice of a Maximum Entropy -LRB- ME -RRB- model for the tagging task -LRB- Berger et al. 1996 -RRB-	amod_Berger_1996 dep_Berger_al. nn_Berger_et amod_task_tagging det_task_the prep_for_model_task amod_Entropy_model appos_Entropy_ME nn_Entropy_Maximum det_Entropy_a prep_of_choice_Entropy poss_choice_our dobj_motivates_choice nsubj_motivates_which rcmod_problem_motivates det_problem_the dep_nature_Berger prep_of_nature_problem amod_nature_nonlocal det_nature_the cop_nature_is nsubj_nature_difference advmod_nonlocal_apparently amod_difference_main amod_difference_other det_difference_The
P03-1061	J96-1002	o	One is to find unknown words from corpora and put them into a dictionary -LRB- e.g. -LRB- Mori and Nagao 1996 -RRB- -RRB- and the other is to estimate a model that can identify unknown words correctly -LRB- e.g. -LRB- Kashioka et al. 1997 Nagata 1999 -RRB- -RRB-	dep_Nagata_1999 dep_Kashioka_Nagata amod_Kashioka_1997 dep_Kashioka_al. nn_Kashioka_et appos_e.g._Kashioka dep_correctly_e.g. amod_words_unknown advmod_identify_correctly dobj_identify_words aux_identify_can nsubj_identify_that rcmod_model_identify det_model_a dobj_estimate_model aux_estimate_to xcomp_is_estimate nsubj_is_other det_other_the dep_Mori_1996 conj_and_Mori_Nagao appos_e.g._Nagao appos_e.g._Mori dep_dictionary_e.g. det_dictionary_a prep_into_put_dictionary dobj_put_them nsubj_put_One prep_from_words_corpora amod_words_unknown dobj_find_words aux_find_to conj_and_is_is conj_and_is_put xcomp_is_find nsubj_is_One ccomp_``_is ccomp_``_put ccomp_``_is
P03-1061	J96-1002	o	We implemented this model within an ME modeling framework -LRB- Jaynes 1957 Jaynes 1979 Berger et al. 1996 -RRB-	num_Berger_1996 nn_Berger_al. nn_Berger_et dep_Jaynes_Berger num_Jaynes_1979 dep_Jaynes_Jaynes appos_Jaynes_1957 dep_framework_Jaynes nn_framework_modeling nn_framework_ME det_framework_an det_model_this prep_within_implemented_framework dobj_implemented_model nsubj_implemented_We
P04-1014	J96-1002	o	They use a conditional model based on Collins -LRB- 1996 -RRB- which as the authors acknowledge has a number of theoretical deficiencies thus the results of Clark et al. provide a useful baseline for the new models presented here	advmod_presented_here vmod_models_presented amod_models_new det_models_the prep_for_baseline_models amod_baseline_useful det_baseline_a dobj_provide_baseline nsubj_provide_results advmod_provide_thus dep_Clark_al. nn_Clark_et prep_of_results_Clark det_results_the amod_deficiencies_theoretical prep_of_number_deficiencies det_number_a dobj_has_number parataxis_has_acknowledge nsubj_has_which nsubj_acknowledge_authors mark_acknowledge_as det_authors_the rcmod_Collins_has appos_Collins_1996 prep_on_based_Collins vmod_model_based amod_model_conditional det_model_a parataxis_use_provide dobj_use_model nsubj_use_They