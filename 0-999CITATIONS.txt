A00-1043	A00-2024	o	We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences including removing extraneous phrases from an extracted sentence combining a reduced sentence with other sentences syntactic transformation substituting phrases in an extracted sentence with their paraphrases substituting phrases with more general or specific descriptions and reordering the extracted sentences -LRB- Jing and McKeown 1999 Jing and McKeown 2000 -RRB-	amod_Jing_2000 conj_and_Jing_McKeown dep_Jing_McKeown dep_Jing_Jing conj_and_Jing_1999 conj_and_Jing_McKeown appos_sentences_1999 appos_sentences_McKeown appos_sentences_Jing amod_sentences_extracted det_sentences_the dobj_reordering_sentences dep_general_descriptions conj_or_general_specific amod_general_more prep_with_substituting_specific prep_with_substituting_general dobj_substituting_phrases poss_paraphrases_their prep_with_sentence_paraphrases amod_sentence_extracted det_sentence_an prep_in_substituting_sentence dobj_substituting_phrases conj_and_transformation_reordering vmod_transformation_substituting vmod_transformation_substituting amod_transformation_syntactic amod_sentences_other prep_with_sentence_sentences amod_sentence_reduced det_sentence_a dobj_combining_sentence amod_sentence_extracted det_sentence_an amod_phrases_extraneous prep_from_removing_sentence dobj_removing_phrases appos_sentences_reordering appos_sentences_transformation vmod_sentences_combining prepc_including_sentences_removing amod_sentences_extracted det_sentences_the nn_sentences_editing prep_for_used_sentences auxpass_used_be aux_used_can nsubjpass_used_that amod_operations_major num_operations_six amod_operations_identified rcmod_set_used conj_and_set_operations prep_of_set_articles det_set_a dobj_analyzed_operations dobj_analyzed_set nsubj_analyzed_We
H05-1033	A00-2024	o	Table 3 Example compressions Compression AvgLen Rating Baseline 9.70 1.93 BT-2-Step 22.06 3.21 Spade 19.09 3.10 Humans 20.07 3.83 Table 4 Mean ratings for automatic compressions nally we added a simple baseline compression algorithm proposed by Jing and McKeown -LRB- 2000 -RRB- which removed all prepositional phrases clauses toinfinitives and gerunds	conj_and_clauses_gerunds conj_and_clauses_toinfinitives amod_phrases_prepositional det_phrases_all dobj_removed_phrases nsubj_removed_which appos_McKeown_2000 conj_and_Jing_McKeown agent_proposed_McKeown agent_proposed_Jing appos_algorithm_gerunds appos_algorithm_toinfinitives appos_algorithm_clauses rcmod_algorithm_removed vmod_algorithm_proposed nn_algorithm_compression nn_algorithm_baseline amod_algorithm_simple det_algorithm_a dobj_added_algorithm nsubj_added_we ccomp_added_Table advmod_compressions_nally amod_compressions_automatic prep_for_ratings_compressions nn_ratings_Mean num_Table_4 num_Table_3.83 num_Table_20.07 dep_Humans_Table nn_Humans_Spade num_Humans_3.21 num_Humans_22.06 nn_Humans_BT-2-Step num_Humans_1.93 num_Humans_9.70 number_3.10_19.09 num_Spade_3.10 dep_Baseline_Humans dep_Rating_Baseline amod_AvgLen_Rating dep_Compression_AvgLen dep_compressions_Compression dep_Example_ratings dep_Example_compressions dep_Table_Example num_Table_3
I05-2009	A00-2024	o	5.3 Related works and discussion Our two-step model essentially belongs to the same category as the works of -LRB- Mani et al. 1999 -RRB- and -LRB- Jing and McKeown 2000 -RRB-	dep_Jing_2000 conj_and_Jing_McKeown conj_and_Mani_McKeown conj_and_Mani_Jing amod_Mani_1999 dep_Mani_al. nn_Mani_et prep_of_works_Jing prep_of_works_Mani det_works_the prep_as_category_works amod_category_same det_category_the prep_to_belongs_category advmod_belongs_essentially nsubj_belongs_model amod_model_two-step poss_model_Our rcmod_discussion_belongs conj_and_works_discussion amod_works_Related num_works_5.3
I05-2009	A00-2024	o	-LRB- 1999 -RRB- proposed a summarization system based on the draft and revision Jing and McKeown -LRB- 2000 -RRB- proposed a system based on extraction and cut-and-paste generation Our abstractors performed the same cut-and-paste operations that Jing and McKeown noted in their work and we think that our two-step model will be a reasonable starting point for our subsequent research	amod_research_subsequent poss_research_our prep_for_point_research amod_point_starting amod_point_reasonable det_point_a cop_point_be aux_point_will nsubj_point_model mark_point_that amod_model_two-step poss_model_our ccomp_think_point nsubj_think_we poss_work_their prep_in_noted_work nsubj_noted_McKeown nsubj_noted_Jing dobj_noted_that conj_and_Jing_McKeown rcmod_operations_noted amod_operations_cut-and-paste amod_operations_same det_operations_the dobj_performed_operations nsubj_performed_abstractors poss_abstractors_Our amod_generation_cut-and-paste conj_and_extraction_generation prep_on_based_generation prep_on_based_extraction vmod_system_based det_system_a conj_and_proposed_think parataxis_proposed_performed dobj_proposed_system nsubj_proposed_McKeown nsubj_proposed_Jing appos_McKeown_2000 conj_and_Jing_McKeown conj_and_draft_revision det_draft_the prep_on_based_revision prep_on_based_draft vmod_system_based nn_system_summarization det_system_a parataxis_proposed_think parataxis_proposed_proposed dobj_proposed_system dep_proposed_1999
I05-2009	A00-2024	o	We found that the deletion of lead parts did not occur very often in our summary unlike the case of Jing and McKeown -LRB- 2000 -RRB-	appos_McKeown_2000 conj_and_Jing_McKeown prep_of_case_McKeown prep_of_case_Jing det_case_the poss_summary_our advmod_often_very prep_unlike_occur_case prep_in_occur_summary advmod_occur_often neg_occur_not aux_occur_did nsubj_occur_deletion mark_occur_that nn_parts_lead prep_of_deletion_parts det_deletion_the ccomp_found_occur nsubj_found_We
I08-1016	A00-2024	o	Automatic text summarization approaches have offered reasonably well-performing approximations for identifiying important sentences -LRB- Lin and Hovy 2002 Schiffman et al. 2002 Erkan and Radev 2004 Mihalcea and Tarau 2004 Daume III and Marcu 2006 -RRB- but not surprisingly text -LRB- re -RRB- generation has been a major challange despite some work on sub-sentential modification -LRB- Jing and McKeown 2000 Knight and Marcu 2000 Barzilay and McKeown 2005 -RRB-	dep_Knight_2005 conj_and_Knight_McKeown conj_and_Knight_Barzilay conj_and_Knight_2000 conj_and_Knight_Marcu dep_Jing_McKeown dep_Jing_Barzilay dep_Jing_2000 dep_Jing_Marcu dep_Jing_Knight conj_and_Jing_2000 conj_and_Jing_McKeown appos_modification_2000 appos_modification_McKeown appos_modification_Jing amod_modification_sub-sentential prep_on_work_modification det_work_some prep_despite_challange_work amod_challange_major det_challange_a cop_challange_been aux_challange_has nsubj_challange_generation dep_challange_Marcu dep_challange_III nn_generation_text appos_text_re neg_surprisingly_not advmod_III_surprisingly appos_III_2006 conj_and_III_Marcu nn_III_Daume num_Mihalcea_2004 conj_and_Mihalcea_Tarau num_Erkan_2004 conj_and_Erkan_Radev num_Schiffman_2002 nn_Schiffman_al. nn_Schiffman_et dep_Lin_challange conj_and_Lin_Tarau conj_and_Lin_Mihalcea conj_and_Lin_Radev conj_and_Lin_Erkan conj_and_Lin_Schiffman num_Lin_2002 conj_and_Lin_Hovy dep_sentences_Mihalcea dep_sentences_Erkan dep_sentences_Schiffman dep_sentences_Hovy dep_sentences_Lin amod_sentences_important amod_sentences_identifiying amod_approximations_well-performing advmod_well-performing_reasonably prep_for_offered_sentences dobj_offered_approximations aux_offered_have nsubj_offered_approaches nn_approaches_summarization nn_approaches_text nn_approaches_Automatic
I08-2101	A00-2024	p	al. 1994 -RRB- compression of sentences with Automatic Translation approaches -LRB- Knight and Marcu 2000 -RRB- Hidden Markov Model -LRB- Jing and McKeown 2000 -RRB- Topic Signatures based methods -LRB- Lin and Hovy 2000 Lacatusu et al. 2006 -RRB- are among the most popular techniques that have been used in the summarization systems of this category	det_category_this prep_of_systems_category nn_systems_summarization det_systems_the prep_in_used_systems auxpass_used_been aux_used_have nsubjpass_used_that rcmod_techniques_used amod_techniques_popular det_techniques_the advmod_popular_most prep_among_are_techniques nsubj_are_compression nn_Lacatusu_al. nn_Lacatusu_et dep_Lin_2006 appos_Lin_Lacatusu amod_Lin_2000 conj_and_Lin_Hovy dep_methods_Hovy dep_methods_Lin amod_methods_based dep_Signatures_methods nn_Signatures_Topic dep_Jing_2000 conj_and_Jing_McKeown dep_Model_McKeown dep_Model_Jing nn_Model_Markov nn_Model_Hidden dep_Knight_2000 conj_and_Knight_Marcu dep_approaches_Marcu dep_approaches_Knight nn_approaches_Translation nn_approaches_Automatic prep_with_sentences_approaches appos_compression_Signatures conj_compression_Model prep_of_compression_sentences rcmod_1994_are dep_1994_al.
J02-4002	A00-2024	o	Because of this it is generally accepted that some kind of postprocessing should be performed to improve the final result by shortening fusing or otherwise revising the material -LRB- Grefenstette 1998 Mani Gates and Bloedorn 1999 Jing and McKeown 2000 Barzilay et al. 2000 Knight and Marcu 2000 -RRB-	num_Knight_2000 conj_and_Knight_Marcu dep_al._2000 nn_al._et nn_al._Barzilay num_McKeown_2000 conj_and_Jing_McKeown num_Bloedorn_1999 conj_and_Mani_Bloedorn conj_and_Mani_Gates dep_Grefenstette_Marcu dep_Grefenstette_Knight dep_Grefenstette_al. dep_Grefenstette_McKeown dep_Grefenstette_Jing dep_Grefenstette_Bloedorn dep_Grefenstette_Gates dep_Grefenstette_Mani num_Grefenstette_1998 appos_material_Grefenstette det_material_the dobj_revising_material advmod_revising_otherwise conj_or_shortening_revising conj_or_shortening_fusing amod_result_final det_result_the dobj_improve_result aux_improve_to xcomp_performed_improve auxpass_performed_be aux_performed_should nsubjpass_performed_kind mark_performed_that prep_of_kind_postprocessing det_kind_some agent_accepted_revising agent_accepted_fusing agent_accepted_shortening ccomp_accepted_performed advmod_accepted_generally auxpass_accepted_is nsubjpass_accepted_it prep_because_of_accepted_this ccomp_``_accepted
J02-4004	A00-2024	o	Additionally some research has explored cutting and pasting segments of text from the full document to generate a summary -LRB- Jing and McKeown 2000 -RRB-	num_McKeown_2000 conj_and_Jing_McKeown dep_summary_McKeown dep_summary_Jing det_summary_a dobj_generate_summary aux_generate_to amod_document_full det_document_the prep_of_segments_text vmod_cutting_generate prep_from_cutting_document dobj_cutting_segments conj_and_cutting_pasting xcomp_explored_pasting xcomp_explored_cutting aux_explored_has nsubj_explored_research advmod_explored_Additionally det_research_some
J02-4005	A00-2024	p	But in fact the issue of editing in text summarization has usually been neglected notable exceptions being the works by Jing and McKeown -LRB- 2000 -RRB- and Mani Gates and Bloedorn -LRB- 1999 -RRB-	appos_Bloedorn_1999 dep_McKeown_2000 conj_and_Jing_Bloedorn conj_and_Jing_Gates conj_and_Jing_Mani conj_and_Jing_McKeown agent_works_Bloedorn agent_works_Gates agent_works_Mani agent_works_McKeown agent_works_Jing det_works_the cop_works_being vmod_exceptions_works amod_exceptions_notable dobj_neglected_exceptions auxpass_neglected_been advmod_neglected_usually aux_neglected_has nsubjpass_neglected_issue prep_in_neglected_fact cc_neglected_But nn_summarization_text prep_in_issue_summarization prep_of_issue_editing det_issue_the
J02-4005	A00-2024	o	Jing and McKeown -LRB- 2000 -RRB- and Jing -LRB- 2000 -RRB- propose a cut-and-paste strategy as a computational process of automatic abstracting and a sentence reduction strategy to produce concise sentences	amod_sentences_concise dobj_produce_sentences aux_produce_to nn_strategy_reduction nn_strategy_sentence det_strategy_a conj_and_abstracting_strategy dep_automatic_strategy dep_automatic_abstracting vmod_process_produce prep_of_process_automatic amod_process_computational det_process_a prep_as_strategy_process amod_strategy_cut-and-paste det_strategy_a dobj_propose_strategy nsubj_propose_Jing nsubj_propose_McKeown nsubj_propose_Jing appos_Jing_2000 appos_McKeown_2000 conj_and_Jing_Jing conj_and_Jing_McKeown
J02-4005	A00-2024	o	Our work in sentence reformulation is different from cut-and-paste summarization -LRB- Jing and McKeown 2000 -RRB- in many ways	amod_ways_many num_McKeown_2000 conj_and_Jing_McKeown dep_summarization_McKeown dep_summarization_Jing amod_summarization_cut-and-paste prep_in_different_ways prep_from_different_summarization cop_different_is nsubj_different_work nn_reformulation_sentence prep_in_work_reformulation poss_work_Our
J02-4005	A00-2024	n	Jing and McKeown -LRB- 2000 -RRB- have proposed a rule-based algorithm for sentence combination but no results have been reported	auxpass_reported_been aux_reported_have nsubjpass_reported_results neg_results_no nn_combination_sentence prep_for_algorithm_combination amod_algorithm_rule-based det_algorithm_a conj_but_proposed_reported dobj_proposed_algorithm aux_proposed_have nsubj_proposed_McKeown nsubj_proposed_Jing appos_McKeown_2000 conj_and_Jing_McKeown ccomp_``_reported ccomp_``_proposed
J05-3002	A00-2024	o	As previously observed in the literature -LRB- Mani Gates and Bloedorn 1999 Jing and McKeown 2000 -RRB- such components include a clause in the clause conjunction relative clauses and some elements within a clause -LRB- such as adverbs and prepositions -RRB-	conj_and_adverbs_prepositions prep_such_as_clause_prepositions prep_such_as_clause_adverbs det_clause_a prep_within_elements_clause det_elements_some conj_and_clauses_elements amod_clauses_relative nn_conjunction_clause det_conjunction_the prep_in_clause_conjunction det_clause_a dobj_include_elements dobj_include_clauses dobj_include_clause nsubj_include_components advcl_include_observed amod_components_such num_McKeown_2000 conj_and_Jing_McKeown num_Bloedorn_1999 dep_Mani_McKeown dep_Mani_Jing conj_and_Mani_Bloedorn conj_and_Mani_Gates appos_literature_Bloedorn appos_literature_Gates appos_literature_Mani det_literature_the prep_in_observed_literature advmod_observed_previously mark_observed_As
J05-3002	A00-2024	o	In addition to sentence fusion compression algorithms -LRB- Chandrasekar Doran and Bangalore 1996 Grefenstette 1998 Mani Gates and Bloedorn 1999 Knight and Marcu 2002 Jing and McKeown 2000 Reizler et al. 2003 -RRB- and methods for expansion of a multiparallel corpus -LRB- Pang Knight and Marcu 2003 -RRB- are other instances of such methods	amod_methods_such prep_of_instances_methods amod_instances_other cop_instances_are nsubj_instances_methods nsubj_instances_Knight nsubj_instances_Mani nsubj_instances_Grefenstette nsubj_instances_Bangalore nsubj_instances_Doran nsubj_instances_Chandrasekar dep_instances_algorithms num_Marcu_2003 conj_and_Pang_Marcu conj_and_Pang_Knight appos_corpus_Marcu appos_corpus_Knight appos_corpus_Pang amod_corpus_multiparallel det_corpus_a prep_of_expansion_corpus prep_for_methods_expansion dep_al._2003 nn_al._et nn_al._Reizler num_McKeown_2000 conj_and_Jing_McKeown num_Knight_2002 conj_and_Knight_Marcu num_Bloedorn_1999 conj_and_Mani_Bloedorn conj_and_Mani_Gates num_Grefenstette_1998 num_Bangalore_1996 conj_and_Chandrasekar_methods dep_Chandrasekar_al. dep_Chandrasekar_McKeown dep_Chandrasekar_Jing conj_and_Chandrasekar_Marcu conj_and_Chandrasekar_Knight conj_and_Chandrasekar_Bloedorn conj_and_Chandrasekar_Gates conj_and_Chandrasekar_Mani conj_and_Chandrasekar_Grefenstette conj_and_Chandrasekar_Bangalore conj_and_Chandrasekar_Doran nn_algorithms_compression dep_fusion_instances nn_fusion_sentence prep_to_addition_fusion pobj_In_addition dep_``_In
J05-3002	A00-2024	o	While earlier approaches for text compression were based on symbolic reduction rules -LRB- Grefenstette 1998 Mani Gates and Bloedorn 1999 -RRB- more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced -LRB- Knight and Marcu 2002 Jing and McKeown 2000 Reizler et al. 2003 -RRB-	dep_al._2003 nn_al._et nn_al._Reizler num_McKeown_2000 conj_and_Jing_McKeown num_Marcu_2002 dep_Knight_al. conj_and_Knight_McKeown conj_and_Knight_Jing conj_and_Knight_Marcu dep_reduced_Jing dep_reduced_Marcu dep_reduced_Knight auxpass_reduced_be aux_reduced_can nsubjpass_reduced_constituents det_constituents_which ccomp_determine_reduced aux_determine_to vmod_summaries_determine amod_summaries_written amod_summaries_human poss_summaries_their conj_and_corpus_summaries prep_of_corpus_documents amod_corpus_aligned det_corpus_an dobj_use_summaries dobj_use_corpus nsubj_use_approaches amod_approaches_recent advmod_recent_more num_Bloedorn_1999 conj_and_Mani_Bloedorn conj_and_Mani_Gates dep_Grefenstette_Bloedorn dep_Grefenstette_Gates dep_Grefenstette_Mani num_Grefenstette_1998 rcmod_rules_use appos_rules_Grefenstette nn_rules_reduction amod_rules_symbolic prep_on_based_rules auxpass_based_were nsubjpass_based_approaches mark_based_While nn_compression_text prep_for_approaches_compression amod_approaches_earlier advcl_``_based
J05-3002	A00-2024	o	While this approach exploits only syntactic and lexical information Jing and McKeown -LRB- 2000 -RRB- also rely on cohesion information derived from word distribution in a text Phrases that are linked to a local context are retained while phrases that have no such links are dropped	auxpass_dropped_are nsubjpass_dropped_phrases mark_dropped_while amod_links_such neg_links_no dobj_have_links nsubj_have_that rcmod_phrases_have advcl_retained_dropped auxpass_retained_are nsubjpass_retained_Phrases amod_context_local det_context_a prep_to_linked_context auxpass_linked_are nsubjpass_linked_that rcmod_Phrases_linked det_text_a prep_in_distribution_text nn_distribution_word prep_from_derived_distribution vmod_information_derived nn_information_cohesion parataxis_rely_retained prep_on_rely_information advmod_rely_also nsubj_rely_McKeown nsubj_rely_Jing advcl_rely_exploits appos_McKeown_2000 conj_and_Jing_McKeown amod_information_lexical amod_information_syntactic conj_and_syntactic_lexical advmod_syntactic_only dobj_exploits_information nsubj_exploits_approach mark_exploits_While det_approach_this
J05-3002	A00-2024	o	In addition to reducing the original sentences Jing and McKeown -LRB- 2000 -RRB- use a number of manually compiled rules to aggregate reduced sentences for example reduced clauses might be conjoined with and	cc_with_and prep_conjoined_with auxpass_conjoined_be aux_conjoined_might dep_reduced_conjoined dobj_reduced_clauses prep_for_reduced_example dobj_reduced_sentences amod_rules_compiled advmod_compiled_manually prep_of_number_rules det_number_a parataxis_use_reduced dep_use_reduced prep_to_use_aggregate dobj_use_number nsubj_use_McKeown nsubj_use_Jing prepc_in_addition_to_use_reducing appos_McKeown_2000 conj_and_Jing_McKeown amod_sentences_original det_sentences_the dobj_reducing_sentences
W02-0404	A00-2024	o	Previous research has addressed revision in single-document summaries -LSB- Jing & McKeown 2000 -RSB- -LSB- Mani et al 1999 -RSB- and has suggested that revising summaries can make them more informative and correct errors	amod_errors_correct advmod_informative_more nsubj_informative_them conj_and_make_errors xcomp_make_informative aux_make_can csubj_make_revising mark_make_that dobj_revising_summaries ccomp_suggested_errors ccomp_suggested_make aux_suggested_has nsubj_suggested_research amod_Mani_1999 dep_Mani_al nn_Mani_et dep_Jing_2000 conj_and_Jing_McKeown amod_summaries_single-document prep_in_revision_summaries conj_and_addressed_suggested dep_addressed_Mani dep_addressed_McKeown dep_addressed_Jing dobj_addressed_revision aux_addressed_has nsubj_addressed_research amod_research_Previous
W02-0404	A00-2024	o	To contrast -LSB- Jing & McKeown 2000 -RSB- concentrated on analyzing human-written summaries in order to determine how professionals construct summaries	dobj_construct_summaries nsubj_construct_professionals advmod_construct_how ccomp_determine_construct aux_determine_to dep_determine_order mark_determine_in dep_summaries_determine amod_summaries_human-written dobj_analyzing_summaries prepc_on_concentrated_analyzing nsubj_concentrated_McKeown nsubj_concentrated_Jing prep_to_concentrated_contrast dep_Jing_2000 conj_and_Jing_McKeown
W03-1004	A00-2024	o	1 Introduction Text-to-text generation is an emerging area of research in NLP -LRB- Chandrasekar and Bangalore 1997 Caroll et al. 1999 Knight and Marcu 2000 Jing and McKeown 2000 -RRB-	dep_Knight_2000 conj_and_Knight_McKeown conj_and_Knight_Jing conj_and_Knight_2000 conj_and_Knight_Marcu num_Caroll_1999 nn_Caroll_al. nn_Caroll_et dep_Chandrasekar_McKeown dep_Chandrasekar_Jing dep_Chandrasekar_2000 dep_Chandrasekar_Marcu dep_Chandrasekar_Knight conj_and_Chandrasekar_Caroll conj_and_Chandrasekar_1997 conj_and_Chandrasekar_Bangalore dep_NLP_Caroll dep_NLP_1997 dep_NLP_Bangalore dep_NLP_Chandrasekar prep_in_area_NLP prep_of_area_research amod_area_emerging det_area_an cop_area_is nsubj_area_generation nn_generation_Text-to-text nn_generation_Introduction num_generation_1
W03-1102	A00-2024	p	The recent approach for editing extracted text spans -LRB- Jing and McKeown 2000 -RRB- may also produce improvement for our algorithm	poss_algorithm_our prep_for_improvement_algorithm dobj_produce_improvement advmod_produce_also aux_produce_may nsubj_produce_approach dep_Jing_2000 conj_and_Jing_McKeown dep_spans_McKeown dep_spans_Jing nn_spans_text amod_spans_extracted nn_spans_editing prep_for_approach_spans amod_approach_recent det_approach_The
W09-0604	A00-2024	o	First splitting and merging of sentences -LRB- Jing and McKeown 2000 -RRB- which seems related to content planning and aggregation	conj_and_planning_aggregation amod_planning_content prep_to_related_aggregation prep_to_related_planning acomp_seems_related nsubj_seems_which dep_Jing_2000 conj_and_Jing_McKeown appos_sentences_McKeown appos_sentences_Jing prep_of_merging_sentences rcmod_splitting_seems conj_and_splitting_merging advmod_splitting_First
W09-0604	A00-2024	o	1 Introduction The task of sentence compression -LRB- or sentence reduction -RRB- can be defined as summarizing a single sentence by removing information from it -LRB- Jing and McKeown 2000 -RRB-	amod_Jing_2000 conj_and_Jing_McKeown dep_it_McKeown dep_it_Jing prep_from_removing_it dobj_removing_information amod_sentence_single det_sentence_a prepc_by_summarizing_removing dobj_summarizing_sentence prepc_as_defined_summarizing auxpass_defined_be aux_defined_can nsubjpass_defined_task nn_reduction_sentence cc_reduction_or appos_compression_reduction nn_compression_sentence prep_of_task_compression det_task_The rcmod_Introduction_defined num_Introduction_1
W09-0604	A00-2024	o	One of the applications is in automatic summarization in order to compress sentences extracted for the summary -LRB- Lin 2003 Jing and McKeown 2000 -RRB-	amod_Jing_2000 conj_and_Jing_McKeown dep_Lin_McKeown dep_Lin_Jing num_Lin_2003 appos_summary_Lin det_summary_the prep_for_extracted_summary vmod_sentences_extracted dobj_compress_sentences aux_compress_to dep_compress_order mark_compress_in amod_summarization_automatic advcl_is_compress prep_in_is_summarization nsubj_is_One det_applications_the prep_of_One_applications ccomp_``_is
W09-2807	A00-2024	o	In cut-and-paste summarization -LRB- Jing and McKeown 2000 -RRB- sentence combination operations were implemented manually following the study of a set of professionally written abstracts however the particular pasting operation presented here was not implemented	neg_implemented_not auxpass_implemented_was nsubjpass_implemented_operation advmod_implemented_however advmod_presented_here vmod_operation_presented nn_operation_pasting amod_operation_particular det_operation_the amod_abstracts_written advmod_written_professionally prep_of_set_abstracts det_set_a prep_of_study_set det_study_the parataxis_implemented_implemented prep_following_implemented_study advmod_implemented_manually auxpass_implemented_were nsubjpass_implemented_operations prep_in_implemented_summarization nn_operations_combination nn_operations_sentence dep_Jing_2000 conj_and_Jing_McKeown appos_summarization_McKeown appos_summarization_Jing amod_summarization_cut-and-paste
W09-2807	A00-2024	o	Close to the problem studied here is Jing and McKeowns -LRB- Jing and McKeown 2000 -RRB- cut-and-paste method founded on EndresNiggemeyers observations	nn_observations_EndresNiggemeyers prep_on_founded_observations vmod_method_founded amod_method_cut-and-paste nn_method_McKeowns dep_Jing_2000 conj_and_Jing_McKeown dep_McKeowns_McKeown dep_McKeowns_Jing conj_and_Jing_method cop_Jing_is csubj_Jing_Close advmod_studied_here vmod_problem_studied det_problem_the prep_to_Close_problem
W09-2808	A00-2024	o	Jing and McKeown -LRB- 1999 2000 -RRB- found that human summarization can be traced back to six cut-andpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part	nn_part_extraction nn_part_sentence det_part_a nn_modules_combination prep_with_reduction_part conj_and_reduction_modules nn_reduction_sentence prep_of_consisting_modules prep_of_consisting_reduction vmod_method_consisting nn_method_revision det_method_a dobj_proposed_method nsubjpass_proposed_summarization det_text_a prep_of_operations_text amod_operations_cut-andpaste num_operations_six prep_to_back_operations conj_and_traced_proposed advmod_traced_back auxpass_traced_be aux_traced_can nsubjpass_traced_summarization mark_traced_that amod_summarization_human ccomp_found_proposed ccomp_found_traced nsubj_found_McKeown nsubj_found_Jing dep_1999_2000 dep_McKeown_1999 conj_and_Jing_McKeown
W09-2808	A00-2024	o	Like the work of Jing and McKeown -LRB- 2000 -RRB- and Mani et al.	nn_al._et nn_al._Mani dep_McKeown_2000 conj_and_Jing_al. conj_and_Jing_McKeown prep_of_work_al. prep_of_work_McKeown prep_of_work_Jing det_work_the pobj_Like_work
A00-1026	A92-1018	o	The SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger -LRB- Cutting et al. 1992 -RRB-	nn_al._et tmod_Cutting_1992 dobj_Cutting_al. dep_tagger_Cutting amod_tagger_stochastic nn_tagger_Xerox det_tagger_the conj_and_Lexicon_tagger nn_Lexicon_SPECIALIST det_Lexicon_the prep_on_relies_tagger prep_on_relies_Lexicon nsubj_relies_parser nn_parser_commitment amod_parser_minimal nn_parser_SPECIALIST det_parser_The
A00-1031	A92-1018	p	Recent comparisons of approaches that can be trained on corpora -LRB- van Halteren et al. 1998 Volk and Schneider 1998 -RRB- have shown that in most cases statistical aproaches -LRB- Cutting et al. 1992 Schmid 1995 Ratnaparkhi 1996 -RRB- yield better results than finite-state rule-based or memory-based taggers -LRB- Brill 1993 Daelemans et al. 1996 -RRB-	num_Daelemans_1996 nn_Daelemans_al. nn_Daelemans_et dep_Brill_Daelemans dep_Brill_1993 appos_taggers_Brill amod_taggers_memory-based amod_taggers_rule-based amod_taggers_finite-state conj_or_finite-state_memory-based conj_or_finite-state_rule-based prep_than_results_taggers amod_results_better dobj_yield_results num_Ratnaparkhi_1996 dep_Schmid_yield dep_Schmid_Ratnaparkhi num_Schmid_1995 conj_al._1992 nn_al._et dep_Cutting_Schmid dobj_Cutting_al. amod_aproaches_statistical nn_aproaches_cases amod_aproaches_most dep_that_Cutting prep_in_that_aproaches dep_shown_that aux_shown_have nsubj_shown_comparisons amod_Volk_1998 conj_and_Volk_Schneider dep_Halteren_Schneider dep_Halteren_Volk amod_Halteren_1998 dep_Halteren_al. nn_Halteren_et nn_Halteren_van appos_corpora_Halteren prep_on_trained_corpora auxpass_trained_be aux_trained_can nsubjpass_trained_that rcmod_approaches_trained prep_of_comparisons_approaches amod_comparisons_Recent
A94-1008	A92-1018	o	The two systems we use are ENGCG -LRB- Karlsson et al. 1994 -RRB- and the Xerox Tagger -LRB- Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_Tagger_Cutting nn_Tagger_Xerox det_Tagger_the amod_Karlsson_1994 dep_Karlsson_al. nn_Karlsson_et conj_and_ENGCG_Tagger dep_ENGCG_Karlsson cop_ENGCG_are nsubj_ENGCG_systems nsubj_use_we rcmod_systems_use num_systems_two det_systems_The
A94-1008	A92-1018	o	2.2 Xerox Tagger The Xerox Tagger 1 XT -LRB- Cutting et al. 1992 -RRB- is a statistical tagger made by Doug Cutting Julian Kupiec Jan Pedersen and Penelope Sibun in Xerox PARC	nn_PARC_Xerox nn_Sibun_Penelope nn_Pedersen_Jan nn_Kupiec_Julian conj_and_Doug_Sibun conj_and_Doug_Pedersen conj_and_Doug_Kupiec vmod_Doug_Cutting prep_in_made_PARC agent_made_Sibun agent_made_Pedersen agent_made_Kupiec agent_made_Doug vmod_tagger_made amod_tagger_statistical det_tagger_a cop_tagger_is nsubj_tagger_Tagger nn_al._et dobj_Cutting_1992 dobj_Cutting_al. appos_Tagger_Cutting appos_Tagger_XT num_Tagger_1 nn_Tagger_Xerox nn_Tagger_The nn_Tagger_Tagger nn_Tagger_Xerox num_Tagger_2.2
A94-1009	A92-1018	p	One of the most effective taggers based on a pure HMM is that developed at Xerox -LRB- Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_developed_Cutting prep_at_developed_Xerox dep_that_developed dep_is_that amod_HMM_pure det_HMM_a prep_on_based_HMM amod_taggers_effective det_taggers_the advmod_effective_most dep_One_is vmod_One_based prep_of_One_taggers
A94-1009	A92-1018	o	The Xerox experiments -LRB- Cutting et al. 1992 -RRB- correspond to something between D1 and D2 and between TO and T1 in that there is some initial biasing of the probabilities	det_probabilities_the prep_of_biasing_probabilities amod_biasing_initial det_biasing_some nsubj_is_biasing expl_is_there prep_in_is_that rcmod_TO_is conj_and_TO_T1 dep_between_T1 dep_between_TO conj_and_D1_D2 prep_between_something_D2 prep_between_something_D1 conj_and_correspond_between prep_to_correspond_something nsubj_correspond_experiments nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_experiments_Cutting nn_experiments_Xerox det_experiments_The
A94-1027	A92-1018	o	All 8,907 articles were tagged by the Xerox Part-ofSpeech Tagger -LRB- Cutting et al. 1992 -RRB- 4	dep_al._1992 nn_al._et dobj_Cutting_4 advmod_Cutting_al. nn_Tagger_Part-ofSpeech nn_Tagger_Xerox det_Tagger_the dep_tagged_Cutting agent_tagged_Tagger auxpass_tagged_were nsubjpass_tagged_articles num_articles_8,907 det_articles_All
A97-1004	A92-1018	o	-LRB- Cutting et al. 1992 -RRB- -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_''_Cutting
A97-1014	A92-1018	o	-LRB- Cutting et al. 1992 -RRB- and -LRB- Feldweg 1995 -RRB- -RRB-	dep_Feldweg_1995 nn_al._et conj_and_Cutting_Feldweg dobj_Cutting_1992 dobj_Cutting_al. dep_''_Feldweg dep_''_Cutting
A97-1017	A92-1018	o	For Czech we created a prototype of the first step of this process the part-of-speech -LRB- POS -RRB- tagger using Rank Xerox tools -LRB- Tapanainen 1995 -RRB- -LRB- Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. amod_Tapanainen_1995 dep_tools_Tapanainen nn_tools_Xerox nn_tools_Rank parataxis_using_Cutting dobj_using_tools dep_tagger_using amod_tagger_part-of-speech det_tagger_the dep_part-of-speech_POS det_process_this prep_of_step_process amod_step_first det_step_the dep_prototype_tagger prep_of_prototype_step det_prototype_a dobj_created_prototype nsubj_created_we prep_for_created_Czech
C00-1004	A92-1018	o	5 Related work Cutting introduced grouping of words into equiva.lence classes based on the set of possible tags to reduce the number of the parameters -LRB- Cutting et al. 1992 -RRB- Schmid used tile equivaleuce classes for smoothing	amod_classes_equivaleuce nn_classes_tile prep_for_used_smoothing dobj_used_classes nsubj_used_Schmid dep_used_work nn_al._et dobj_Cutting_1992 dobj_Cutting_al. det_parameters_the prep_of_number_parameters det_number_the dobj_reduce_number aux_reduce_to amod_tags_possible vmod_set_reduce prep_of_set_tags det_set_the prep_on_based_set amod_classes_equiva.lence prep_of_grouping_words amod_grouping_introduced parataxis_Cutting_Cutting vmod_Cutting_based prep_into_Cutting_classes dobj_Cutting_grouping dep_work_Cutting amod_work_Related num_work_5
C08-1026	A92-1018	p	4.1 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in words with the same category possibilities are grouped together .4 And ambiguity classes have been shown to be successfully employed in a variety of ways to improve POS tagging -LRB- e.g. Cutting et al. 1992 Daelemans et al. 1996 Dickinson 2007 Goldberg et al. 2008 Tseng et al. 2005 -RRB-	num_Tseng_2005 nn_Tseng_al. nn_Tseng_et nn_al._et nn_al._Goldberg num_Dickinson_2007 nn_al._et nn_al._Daelemans num_al._1992 nn_al._et dobj_Cutting_al. dep_e.g._Tseng dep_e.g._2008 dep_e.g._al. dep_e.g._Dickinson dep_e.g._1996 dep_e.g._al. vmod_e.g._Cutting dep_tagging_e.g. nn_tagging_POS dobj_improve_tagging aux_improve_to prep_of_variety_ways det_variety_a advmod_employed_successfully auxpass_employed_be aux_employed_to xcomp_shown_improve prep_in_shown_variety xcomp_shown_employed auxpass_shown_been aux_shown_have num_classes_ambiguity num_classes_.4 conj_and_.4_ambiguity advmod_.4_together advcl_grouped_shown xcomp_grouped_classes auxpass_grouped_are nsubjpass_grouped_words nn_possibilities_category amod_possibilities_same det_possibilities_the prep_with_words_possibilities dep_in_grouped prep_interested_in cop_interested_are nsubj_interested_we amod_property_relevant det_property_the dep_capture_interested dobj_capture_property nsubj_capture_classes nn_classes_Ambiguity nn_classes_classes nn_classes_ambiguity amod_classes_Complete num_classes_4.1
C94-1027	A92-1018	o	In tabh 2 the accuracy rate of the Net-Tagger is cOrolLated to that of a trigram l -RRB- msed tagger -LRB- Kempe 1993 -RRB- and a lIidden Markov Model tagger -LRB- Cutting et al. 1992 -RRB- which were	nsubj_were_which nn_al._et dep_Cutting_1992 dobj_Cutting_al. dep_tagger_Cutting nn_tagger_Model nn_tagger_Markov nn_tagger_lIidden det_tagger_a amod_Kempe_1993 rcmod_tagger_were conj_and_tagger_tagger dep_tagger_Kempe amod_tagger_msed dep_l_tagger dep_l_tagger nn_l_trigram det_l_a prep_of_that_l prep_to_cOrolLated_that auxpass_cOrolLated_is nsubjpass_cOrolLated_rate det_Net-Tagger_the prep_of_rate_Net-Tagger nn_rate_accuracy det_rate_the ccomp_2_cOrolLated prep_in_2_tabh ccomp_``_2
C94-1027	A92-1018	o	In this paper a new part-of-speech tagging method hased on neural networks -LRB- Net-Tagger -RRB- is presented and its performance is compared to that of a llMM-tagger -LRB- Cutting et al. 1992 -RRB- and a trigrambased tagger -LRB- Kempe 1993 -RRB-	amod_Kempe_1993 dep_tagger_Kempe amod_tagger_trigrambased det_tagger_a dep_al._1992 nn_al._et conj_and_Cutting_tagger advmod_Cutting_al. det_llMM-tagger_a prep_of_that_llMM-tagger prep_to_compared_that auxpass_compared_is nsubjpass_compared_performance poss_performance_its dep_presented_tagger dep_presented_Cutting conj_and_presented_compared auxpass_presented_is nsubjpass_presented_method prep_in_presented_paper appos_networks_Net-Tagger amod_networks_neural prep_on_hased_networks vmod_method_hased nn_method_tagging amod_method_part-of-speech amod_method_new det_method_a det_paper_this
C94-1027	A92-1018	o	The performance of tl e presented tagger is measured and compared to that of two other taggers -LRB- Cutting et al. 1992 Kempe 1993 -RRB-	num_Kempe_1993 conj_al._Kempe conj_al._1992 nn_al._et dobj_Cutting_al. amod_taggers_other num_taggers_two prep_of_that_taggers prep_to_compared_that nsubjpass_compared_performance dep_measured_Cutting conj_and_measured_compared auxpass_measured_is nsubjpass_measured_performance amod_tagger_presented dep_tagger_e appos_performance_tagger prep_of_performance_tl det_performance_The
C94-1027	A92-1018	o	No documentation of tile construction algorithm of the su \ -LSB- lix lexicon in -LRB- Cutting et al. 1992 -RRB- was available	cop_available_was prep_available_in num_al._1992 nn_al._et amod_al._Cutting dep_in_al. rcmod_lexicon_available nn_lexicon_lix dep_\_lexicon nn_\_su det_\_the prep_of_algorithm_\ nn_algorithm_construction nn_algorithm_tile prep_of_documentation_algorithm neg_documentation_No
C96-1036	A92-1018	o	Language models such as N-gram class models -LRB- Brown et al. 1992 -RRB- and Ergodic Hidden Markov Models -LRB- Kuhn el al. 1994 -RRB- were proposed and used in applications such as syntactic class -LRB- POS -RRB- tagging for English -LRB- Cutting et al. 1992 -RRB- clustering and scoring of recognizer sentence hypotheses	nn_hypotheses_sentence nn_hypotheses_recognizer prep_of_scoring_hypotheses conj_and_clustering_scoring nn_al._et dobj_Cutting_1992 dobj_Cutting_al. prep_for_tagging_English vmod_class_tagging appos_class_POS amod_class_syntactic prep_such_as_applications_class prep_in_used_applications nsubjpass_used_models dep_proposed_scoring dep_proposed_clustering dep_proposed_Cutting conj_and_proposed_used auxpass_proposed_were nsubjpass_proposed_models amod_el_1994 dep_el_al. nn_el_Kuhn nn_Models_Markov nn_Models_Hidden nn_Models_Ergodic amod_Brown_1992 dep_Brown_al. nn_Brown_et conj_and_models_Models dep_models_Brown nn_models_class nn_models_N-gram dep_models_el prep_such_as_models_Models prep_such_as_models_models nn_models_Language
C96-2114	A92-1018	o	The tagger used is thus one that does not need tagged and disambiguated material to be trained on namely the XPOST originally constructed at Xerox Parc -LRB- Cutting et al. 1992 Cutting and Pedersen 1993 -RRB-	num_Pedersen_1993 advmod_1992_al. nn_al._et conj_and_Cutting_Pedersen conj_and_Cutting_Cutting dobj_Cutting_1992 nn_Parc_Xerox dep_constructed_Pedersen dep_constructed_Cutting dep_constructed_Cutting prep_at_constructed_Parc advmod_constructed_originally nsubj_constructed_XPOST advmod_constructed_namely det_XPOST_the prepc_on_trained_constructed auxpass_trained_be aux_trained_to amod_material_disambiguated amod_material_tagged conj_and_tagged_disambiguated xcomp_need_trained dobj_need_material neg_need_not aux_need_does nsubj_need_that rcmod_one_need advmod_one_thus cop_one_is nsubj_one_tagger vmod_tagger_used det_tagger_The
C96-2136	A92-1018	o	It is used as tagging mode \ -LSB- in English -LRB- Church 1988 Cutting et al. 1992 -RRB- and morphological analysis nlodel -LRB- word segmentation and tagging -RRB- in Japanese -LRB- Nagata 1994 -RRB-	amod_Nagata_1994 dep_Japanese_Nagata conj_and_segmentation_tagging nn_segmentation_word dep_nlodel_tagging dep_nlodel_segmentation nn_nlodel_analysis amod_nlodel_morphological nn_al._et num_Cutting_1992 dobj_Cutting_al. num_Church_1988 nn_Church_English prep_in_in_Japanese conj_and_in_nlodel dep_in_Cutting pobj_in_Church nn_\_mode nn_\_tagging dep_used_nlodel dep_used_in prep_as_used_\ auxpass_used_is nsubjpass_used_It
C96-2136	A92-1018	o	It is a natural extension of the Viteri > i algorithm -LRB- Church 1 < -RRB- 88 Cutting et al. 1992 -RRB- for those languages that do not have delimiters between words and it can generate N-best morphological analysis hypotheses like tree trellis search -LRB- Soong and l \ -LSB- uang 1991 -RRB-	amod_uang_1991 nn_\_l dep_Soong_uang conj_and_Soong_\ dep_search_\ dep_search_Soong nn_search_trellis nn_search_tree prep_like_hypotheses_search nn_hypotheses_analysis amod_hypotheses_morphological nn_hypotheses_N-best dobj_generate_hypotheses aux_generate_can nsubj_generate_it prep_between_delimiters_words dobj_have_delimiters neg_have_not aux_have_do nsubj_have_that rcmod_languages_have det_languages_those nn_al._et conj_and_Cutting_generate prep_for_Cutting_languages dobj_Cutting_1992 dobj_Cutting_al. dep_88_Church num_<_1 dep_Church_< nn_algorithm_i amod_algorithm_> num_Viteri_88 dep_Viteri_algorithm det_Viteri_the dep_extension_generate dep_extension_Cutting prep_of_extension_Viteri amod_extension_natural det_extension_a cop_extension_is nsubj_extension_It
C96-2192	A92-1018	o	-LRB- DeRose 1988 Cutting et al 1992 Merialdo 1994 -RRB-	num_Merialdo_1994 num_al_1992 nn_al_et parataxis_Cutting_Merialdo dobj_Cutting_al dep_DeRose_Cutting num_DeRose_1988
E06-1034	A92-1018	o	5.2 Assigning complex ambiguity tags In the tagging literature -LRB- e.g. Cutting et al -LRB- 1992 -RRB- -RRB- an ambiguity class is often composed of the set of every possible tag for a word	det_word_a amod_tag_possible det_tag_every prep_for_set_word prep_of_set_tag det_set_the prep_of_composed_set advmod_composed_often auxpass_composed_is nsubjpass_composed_class nn_class_ambiguity det_class_an dep_al_1992 nn_al_et advmod_Cutting_al vmod_e.g._Cutting dep_literature_e.g. amod_literature_tagging det_literature_the rcmod_tags_composed prep_in_tags_literature nn_tags_ambiguity nn_tags_complex nn_tags_Assigning num_tags_5.2
E95-1014	A92-1018	o	The corpus lines retained are part-of-speech tagged -LRB- Cutting et al. 1992 -RRB-	dep_al._1992 nn_al._et advmod_Cutting_al. dep_tagged_Cutting dep_tagged_part-of-speech auxpass_tagged_are nsubjpass_tagged_lines vmod_lines_retained nn_lines_corpus det_lines_The
E95-1014	A92-1018	o	This text was part-of-speech tagged using the Xerox HMM tagger -LRB- Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. nn_tagger_HMM nn_tagger_Xerox det_tagger_the dep_using_Cutting dobj_using_tagger xcomp_tagged_using dep_part-of-speech_tagged cop_part-of-speech_was nsubj_part-of-speech_text det_text_This
E95-1020	A92-1018	o	No pretagged text is necessary for Hidden Markov Models -LRB- Jelinek 1985 Cutting et al. 1991 Kupiec 1992 -RRB-	amod_Kupiec_1992 dep_1991_Kupiec nn_al._et tmod_Cutting_1991 dobj_Cutting_al. dep_Jelinek_Cutting appos_Jelinek_1985 dep_Models_Jelinek nn_Models_Markov nn_Models_Hidden prep_for_necessary_Models cop_necessary_is nsubj_necessary_text amod_text_pretagged neg_text_No
E95-1020	A92-1018	o	We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot -LRB- Cutting et al. 1992 -RRB- -LRB- group average agglomeration applied to a sample -RRB-	det_sample_a prep_to_applied_sample nsubj_applied_agglomeration amod_agglomeration_average nn_agglomeration_group nn_agglomeration_Buckshot nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_Buckshot_Cutting rcmod_algorithm_applied nn_algorithm_clustering amod_algorithm_fast det_algorithm_the dobj_using_algorithm num_classes_200 vmod_clustered_using prep_into_clustered_classes dobj_clustered_them nsubj_clustered_We det_SVD_the amod_vectors_reduced amod_vectors_50-dimensional num_vectors_47,025 conj_and_obtained_clustered prep_from_obtained_SVD dobj_obtained_vectors nsubj_obtained_We
E95-1021	A92-1018	o	3 The statistical model We use the Xerox part-of-speech tagger -LRB- Cutting et al. 1992 -RRB- a statistical tagger made at the Xerox Palo Alto Research Center	nn_Center_Research nn_Center_Alto nn_Center_Palo nn_Center_Xerox det_Center_the prep_at_made_Center vmod_tagger_made amod_tagger_statistical det_tagger_a nn_al._et dobj_Cutting_1992 dobj_Cutting_al. amod_tagger_part-of-speech nn_tagger_Xerox det_tagger_the dobj_use_tagger nsubj_use_We appos_model_tagger dep_model_Cutting rcmod_model_use amod_model_statistical det_model_The num_model_3 dep_``_model
E95-1022	A92-1018	o	This corpus-based information typically concerns sequences of 1-3 tags or words -LRB- with some well-known exceptions e.g. Cutting et al. 1992 -RRB-	advmod_1992_al. nn_al._et dobj_Cutting_1992 pcomp_e.g._Cutting amod_exceptions_well-known det_exceptions_some prep_with_tags_exceptions conj_or_tags_words num_tags_1-3 prep_sequences_e.g. prep_of_sequences_words prep_of_sequences_tags dobj_concerns_sequences advmod_concerns_typically nsubj_concerns_information amod_information_corpus-based det_information_This ccomp_``_concerns
E95-1022	A92-1018	o	157 ena or the linguist 's abstraction capabilities -LRB- e.g. knowledge about what is relevant in the context -RRB- they tend to reach a 95-97 % accuracy in the analysis of several languages in particular English -LRB- Marshall 1983 Black et aL 1992 Church 1988 Cutting et al. 1992 de Marcken 1990 DeRose 1988 Hindle 1989 Merialdo 1994 Weischedel et al. 1993 Brill 1992 Samuelsson 1994 Eineborg and Gamb ~ ick 1994 etc. -RRB-	amod_1994_ick number_1994_~ num_Gamb_1994 dep_Eineborg_etc. conj_and_Eineborg_Gamb num_Samuelsson_1994 num_Brill_1992 num_al._1993 nn_al._et nn_al._Weischedel num_Merialdo_1994 num_Hindle_1989 dep_DeRose_Gamb dep_DeRose_Eineborg conj_DeRose_Samuelsson conj_DeRose_Brill dep_DeRose_al. dep_DeRose_Merialdo dep_DeRose_Hindle num_DeRose_1988 num_Marcken_1990 nn_Marcken_de num_al._1992 nn_al._et dobj_Cutting_DeRose advmod_Cutting_Marcken dobj_Cutting_al. num_Church_1988 num_aL_1992 dep_et_aL dep_Black_Cutting dep_Black_Church dep_Black_et dep_Marshall_Black num_Marshall_1983 amod_English_particular amod_languages_several prep_of_analysis_languages det_analysis_the prep_in_accuracy_analysis amod_accuracy_% det_accuracy_a number_%_95-97 dobj_reach_accuracy aux_reach_to dep_tend_Marshall prep_in_tend_English xcomp_tend_reach nsubj_tend_they det_context_the prep_in_relevant_context cop_relevant_is nsubj_relevant_what prepc_about_knowledge_relevant pobj_e.g._knowledge nn_capabilities_abstraction poss_capabilities_linguist det_linguist_the parataxis_ena_tend dep_ena_e.g. conj_or_ena_capabilities num_ena_157 dep_``_capabilities dep_``_ena
E99-1018	A92-1018	o	As a common strategy POS guessers examine the endings of unknown words -LRB- Cutting et al. 1992 -RRB- along with their capitalization or consider the distribution of unknown words over specific parts-of-speech -LRB- Weischedel et aL 1993 -RRB-	dep_aL_1993 nn_aL_et dep_Weischedel_aL amod_parts-of-speech_specific amod_words_unknown prep_over_distribution_parts-of-speech prep_of_distribution_words det_distribution_the dobj_consider_distribution poss_capitalization_their advmod_1992_al. nn_al._et dep_Cutting_Weischedel conj_or_Cutting_consider pobj_Cutting_capitalization prepc_along_with_Cutting_with dobj_Cutting_1992 amod_words_unknown prep_of_endings_words det_endings_the dep_examine_consider dep_examine_Cutting dobj_examine_endings nsubj_examine_guessers prep_as_examine_strategy nn_guessers_POS amod_strategy_common det_strategy_a
E99-1018	A92-1018	o	On the other hand according to the data-driven approach a frequency-based language model is acquired from corpora and has the forms of ngrams -LRB- Church 1988 Cutting et al. 1992 -RRB- rules -LRB- Hindle 1989 Brill 1995 -RRB- decision trees -LRB- Cardie 1994 Daelemans et al. 1996 -RRB- or neural networks -LRB- Schmid 1994 -RRB-	dep_Schmid_1994 amod_networks_neural num_Daelemans_1996 nn_Daelemans_al. nn_Daelemans_et dep_Cardie_Daelemans appos_Cardie_1994 appos_trees_Cardie nn_trees_decision dep_Brill_1995 dep_Hindle_Brill appos_Hindle_1989 dep_rules_Hindle nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_Church_Schmid conj_or_Church_networks conj_or_Church_trees conj_or_Church_rules dep_Church_Cutting appos_Church_1988 dep_ngrams_networks dep_ngrams_trees dep_ngrams_rules dep_ngrams_Church prep_of_forms_ngrams det_forms_the dobj_has_forms nsubj_has_model conj_and_acquired_has prep_from_acquired_corpora auxpass_acquired_is nsubjpass_acquired_model pobj_acquired_approach prepc_according_to_acquired_to prep_on_acquired_hand nn_model_language amod_model_frequency-based det_model_a amod_approach_data-driven det_approach_the amod_hand_other det_hand_the
H05-1052	A92-1018	o	In the absence of an annotated corpus dependencies can be derived by other means e.g. part413 of-speech probabilities can be approximated from a raw corpus as in -LRB- Cutting et al. 1992 -RRB- word-sense dependencies can be derived as definition-based similarities etc. Label dependencies are set as weights on the arcs drawn between corresponding labels	amod_labels_corresponding prep_between_drawn_labels vmod_arcs_drawn det_arcs_the prep_on_weights_arcs prep_as_set_weights auxpass_set_are nsubjpass_set_dependencies nn_dependencies_Label dep_similarities_etc. amod_similarities_definition-based prep_as_derived_similarities auxpass_derived_be aux_derived_can nsubjpass_derived_dependencies vmod_derived_approximated dep_derived_means mark_derived_by amod_dependencies_word-sense num_al._1992 nn_al._et amod_al._Cutting dep_in_al. pcomp_as_in amod_corpus_raw det_corpus_a prep_approximated_as prep_from_approximated_corpus auxpass_approximated_be aux_approximated_can nsubjpass_approximated_probabilities amod_probabilities_of-speech nn_probabilities_part413 nn_probabilities_e.g. amod_means_other parataxis_derived_set advcl_derived_derived auxpass_derived_be aux_derived_can nsubjpass_derived_dependencies prep_in_derived_absence amod_corpus_annotated det_corpus_an prep_of_absence_corpus det_absence_the
I08-3015	A92-1018	o	There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning -LRB- Brill 1995 -RRB- decision trees -LRB- Black et al. 1992 -RRB- Markov model -LRB- Cutting et al. 1992 -RRB- maximum entropy methods -LRB- Ratnaparkhi 1996 -RRB- etc for English	prep_for_etc_English dep_Ratnaparkhi_1996 dep_methods_etc appos_methods_Ratnaparkhi nn_methods_entropy nn_methods_maximum nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_model_Cutting nn_model_Markov num_al._1992 nn_al._et amod_al._Black dep_trees_al. nn_trees_decision dep_Brill_1995 conj_learning_methods conj_learning_model conj_learning_trees dep_learning_Brill amod_learning_error-driven amod_learning_transformation-based prep_such_as_languages_learning amod_languages_major amod_languages_many prep_for_techniques_languages amod_techniques_different dobj_using_techniques xcomp_developed_using nsubj_developed_taggers nn_taggers_POS amod_taggers_many ccomp_are_developed expl_are_There ccomp_``_are
J02-1004	A92-1018	o	Our statistical tagging model is modified from the standard bigrams -LRB- Cutting et al. 1992 -RRB- using Viterbi search plus onthe-fly extra computing of lexical probabilities for unknown morphemes	amod_morphemes_unknown prep_for_probabilities_morphemes amod_probabilities_lexical amod_computing_extra amod_computing_onthe-fly prep_of_search_probabilities conj_plus_search_computing nn_search_Viterbi dobj_using_computing dobj_using_search nn_al._et xcomp_Cutting_using tmod_Cutting_1992 dobj_Cutting_al. amod_bigrams_standard det_bigrams_the xcomp_modified_Cutting prep_from_modified_bigrams auxpass_modified_is nsubjpass_modified_model nn_model_tagging amod_model_statistical poss_model_Our
J02-1004	A92-1018	o	POS disambiguation has usually been performed by statistical approaches mainly using the hidden Markov model -LRB- HMM -RRB- in English research communities -LRB- Cutting et al. 1992 Kupiec 1992 Weischedel et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Weischedel dep_Kupiec_al. num_Kupiec_1992 num_al._1992 nn_al._et dep_Cutting_Kupiec dobj_Cutting_al. nn_communities_research nn_communities_English prep_in_model_communities appos_model_HMM nn_model_Markov amod_model_hidden det_model_the dep_using_Cutting dobj_using_model advmod_using_mainly amod_approaches_statistical dep_performed_using agent_performed_approaches auxpass_performed_been advmod_performed_usually aux_performed_has nsubjpass_performed_disambiguation nn_disambiguation_POS
J93-1002	A92-1018	o	The main application of these techniques to written input has been in the robust lexical tagging of corpora with part-of-speech labels -LRB- e.g. Garside Leech and Sampson 1987 de Rose 1988 Meteer Schwartz and Weischedel 1991 Cutting et al. 1992 -RRB-	advmod_1992_al. nn_al._et dobj_Cutting_1992 num_Weischedel_1991 conj_and_Schwartz_Weischedel nn_Schwartz_Meteer dep_Rose_Cutting conj_Rose_Weischedel conj_Rose_Schwartz num_Rose_1988 nn_Rose_de prep_Rose_e.g. num_Sampson_1987 conj_and_Garside_Sampson conj_and_Garside_Leech pobj_e.g._Sampson pobj_e.g._Leech pobj_e.g._Garside dep_labels_Rose amod_labels_part-of-speech prep_of_tagging_corpora amod_tagging_lexical amod_tagging_robust det_tagging_the prep_with_been_labels prep_in_been_tagging aux_been_has nsubj_been_application amod_input_written det_techniques_these prep_to_application_input prep_of_application_techniques amod_application_main det_application_The ccomp_``_been
J94-2001	A92-1018	o	Two main approaches have generally been considered rule-based -LRB- Klein and Simmons 1963 Brodda 1982 Paulussen and Martin 1992 Brill et al. 1990 -RRB- probabilistic -LRB- Bahl and Mercer 1976 Debili 1977 Stolz Tannenbaum and Carstensen 1965 Marshall 1983 Leech Garside and Atwell 1983 Derouault and Merialdo 1986 DeRose 1988 Church 1989 Beale 1988 Marcken 1990 Merialdo 1991 Cutting et al. 1992 -RRB-	nn_al._et tmod_Cutting_1992 dobj_Cutting_al. num_Merialdo_1991 num_Marcken_1990 num_Beale_1988 num_Church_1989 num_DeRose_1988 num_Merialdo_1986 conj_and_Derouault_Merialdo num_Atwell_1983 conj_and_Leech_Atwell conj_and_Leech_Garside num_Marshall_1983 num_Carstensen_1965 conj_and_Stolz_Carstensen conj_and_Stolz_Tannenbaum amod_1977_Debili num_Mercer_1976 dep_Bahl_Cutting conj_and_Bahl_Merialdo conj_and_Bahl_Marcken conj_and_Bahl_Beale conj_and_Bahl_Church conj_and_Bahl_DeRose conj_and_Bahl_Merialdo conj_and_Bahl_Derouault conj_and_Bahl_Atwell conj_and_Bahl_Garside conj_and_Bahl_Leech conj_and_Bahl_Marshall conj_and_Bahl_Carstensen conj_and_Bahl_Tannenbaum conj_and_Bahl_Stolz conj_and_Bahl_1977 conj_and_Bahl_Mercer dep_probabilistic_Merialdo dep_probabilistic_Marcken dep_probabilistic_Beale dep_probabilistic_Church dep_probabilistic_DeRose dep_probabilistic_Derouault dep_probabilistic_Leech dep_probabilistic_Marshall dep_probabilistic_Stolz dep_probabilistic_1977 dep_probabilistic_Mercer dep_probabilistic_Bahl num_probabilistic_1990 rcmod_al._probabilistic nn_al._et nn_al._Brill dep_Paulussen_1992 conj_and_Paulussen_Martin num_Brodda_1982 num_Simmons_1963 dep_Klein_al. conj_and_Klein_Martin conj_and_Klein_Paulussen conj_and_Klein_Brodda conj_and_Klein_Simmons dep_rule-based_Paulussen dep_rule-based_Brodda dep_rule-based_Simmons dep_rule-based_Klein dep_considered_rule-based auxpass_considered_been advmod_considered_generally aux_considered_have nsubjpass_considered_approaches amod_approaches_main num_approaches_Two
J95-2001	A92-1018	o	Stochastic taggers use both contextual and morphological information and the model parameters are usually defined or updated automatically from tagged texts -LRB- Cerf-Danon and E1-Beze 1991 Church 1988 Cutting et al. 1992 Dermatas and Kokkinakis 1988 1990 1993 1994 Garside Leech and Sampson 1987 Kupiec 1992 Maltese * Department of Electrical Engineering Wire Communications Laboratory -LRB- WCL -RRB- University of Patras 265 00 Patras Greece	appos_Patras_Greece num_Patras_00 number_00_265 appos_University_Patras prep_of_University_Patras appos_Laboratory_WCL nn_Laboratory_Communications nn_Laboratory_Wire amod_Engineering_Electrical appos_Department_University conj_Department_Laboratory prep_of_Department_Engineering dep_Department_* amod_Department_Maltese num_Kupiec_1992 num_Sampson_1987 conj_and_Garside_Sampson conj_and_Garside_Leech num_Kokkinakis_1994 num_Kokkinakis_1993 num_Kokkinakis_1990 num_Kokkinakis_1988 conj_and_Dermatas_Department conj_and_Dermatas_Kupiec conj_and_Dermatas_Sampson conj_and_Dermatas_Leech conj_and_Dermatas_Garside conj_and_Dermatas_Kokkinakis num_al._1992 nn_al._et dep_Cutting_Department dep_Cutting_Kupiec dep_Cutting_Garside dep_Cutting_Kokkinakis dep_Cutting_Dermatas dobj_Cutting_al. num_Church_1988 amod_1991_E1-Beze dep_Cerf-Danon_Cutting conj_and_Cerf-Danon_Church conj_and_Cerf-Danon_1991 dep_texts_Church dep_texts_1991 dep_texts_Cerf-Danon amod_texts_tagged nsubjpass_updated_parameters prep_from_defined_texts advmod_defined_automatically conj_or_defined_updated advmod_defined_usually auxpass_defined_are nsubjpass_defined_parameters nn_parameters_model det_parameters_the amod_information_morphological amod_information_contextual conj_and_contextual_morphological preconj_contextual_both conj_and_use_updated conj_and_use_defined dobj_use_information nsubj_use_taggers nn_taggers_Stochastic
J95-2004	A92-1018	o	Unlike stochastic approaches to part-of-speech tagging -LRB- Church 1988 Kupiec 1992 Cutting et al. 1992 Merialdo 1990 DeRose 1988 Weischedel et al. 1993 -RRB- up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired	advmod_acquired_automatically neg_acquired_not auxpass_acquired_was csubjpass_acquired_Cutting conj_and_handcrafted_acquired auxpass_handcrafted_been aux_handcrafted_has csubjpass_handcrafted_Cutting amod_taggers_finite-state prep_in_found_taggers vmod_knowledge_found det_knowledge_the pobj_to_now prep_up_to dep_al._1993 nn_al._et nn_al._Weischedel num_DeRose_1988 dep_Merialdo_al. dep_Merialdo_DeRose num_Merialdo_1990 num_al._1992 nn_al._et dobj_Cutting_knowledge advmod_Cutting_up dep_Cutting_Merialdo dobj_Cutting_al. num_Kupiec_1992 dep_Church_acquired dep_Church_handcrafted dep_Church_Kupiec num_Church_1988 dep_tagging_Church amod_tagging_part-of-speech prep_to_approaches_tagging amod_approaches_stochastic pobj_Unlike_approaches dep_``_Unlike
J95-2004	A92-1018	o	7 Independently Cutting et aL -LRB- 1992 -RRB- quote a performance of 800 words per second for their part-of-speech tagger based on hidden Markov models	nn_models_Markov amod_models_hidden amod_tagger_part-of-speech poss_tagger_their prep_per_words_second num_words_800 prep_of_performance_words det_performance_a pobj_quote_models prepc_based_on_quote_on prep_for_quote_tagger dobj_quote_performance csubj_quote_Cutting advmod_quote_Independently nsubj_quote_7 appos_aL_1992 nn_aL_et dobj_Cutting_aL
J95-3004	A92-1018	o	These methods have reported performance in the range of 95-99 % correct by word -LRB- DeRose 1988 Cutting et al. 1992 Jelinek Mercer and Roukos 1992 Kupiec 1992 -RRB-	num_Kupiec_1992 num_Roukos_1992 dep_Jelinek_Kupiec conj_and_Jelinek_Roukos conj_and_Jelinek_Mercer num_al._1992 nn_al._et dep_Cutting_Roukos dep_Cutting_Mercer dep_Cutting_Jelinek dobj_Cutting_al. dep_DeRose_Cutting num_DeRose_1988 dep_word_DeRose prep_by_correct_word dep_correct_performance num_%_95-99 prep_of_range_% det_range_the prep_in_performance_range dep_reported_correct aux_reported_have nsubj_reported_methods det_methods_These ccomp_``_reported
J95-4004	A92-1018	p	A number of part-of-speech taggers are readily available and widely used all trained and retrainable on text corpora -LRB- Church 1988 Cutting et al. 1992 Brill 1992 Weischedel et al. 1993 -RRB-	dep_al._1993 nn_al._et nn_al._Weischedel dep_Brill_al. num_Brill_1992 num_al._1992 nn_al._et dep_Cutting_Brill dobj_Cutting_al. dep_Church_Cutting num_Church_1988 dep_corpora_Church nn_corpora_text prep_on_trained_corpora conj_and_trained_retrainable dep_trained_all advmod_used_widely nsubj_used_number dep_available_retrainable dep_available_trained conj_and_available_used advmod_available_readily cop_available_are nsubj_available_number amod_taggers_part-of-speech prep_of_number_taggers det_number_A
J95-4004	A92-1018	o	Part-of-speech tagging is an active area of research a great deal of work has been done in this area over the past few years -LRB- e.g. Jelinek 1985 Church 1988 Derose 1988 Hindle 1989 DeMarcken 1990 Merialdo 1994 Brill 1992 Black et al. 1992 Cutting et al. 1992 Kupiec 1992 Charniak et al. 1993 Weischedel et al. 1993 Schutze and Singer 1994 -RRB-	num_Singer_1994 conj_and_Schutze_Singer num_al._1993 nn_al._et nn_al._Weischedel num_al._1993 nn_al._et nn_al._Charniak num_Kupiec_1992 nn_al._et parataxis_Cutting_Singer parataxis_Cutting_Schutze conj_Cutting_al. conj_Cutting_al. conj_Cutting_Kupiec tmod_Cutting_1992 dobj_Cutting_al. nn_al._et nn_al._Black num_Brill_1992 num_Merialdo_1994 num_DeMarcken_1990 num_Hindle_1989 num_Derose_1988 num_Church_1988 num_Jelinek_1985 dep_e.g._1992 dep_e.g._al. dep_e.g._Brill dep_e.g._Merialdo dep_e.g._DeMarcken dep_e.g._Hindle dep_e.g._Derose dep_e.g._Church dep_e.g._Jelinek amod_years_few amod_years_past det_years_the det_area_this dep_done_e.g. prep_over_done_years prep_in_done_area auxpass_done_been aux_done_has nsubjpass_done_deal prep_of_deal_work amod_deal_great det_deal_a dep_area_Cutting parataxis_area_done prep_of_area_research amod_area_active det_area_an cop_area_is nsubj_area_tagging amod_tagging_Part-of-speech
J95-4004	A92-1018	o	Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging -LRB- Jelinek 1985 Church 1988 Derose 1988 DeMarcken 1990 Merialdo 1994 Cutting et al. 1992 Kupiec 1992 Charniak et al. 1993 Weischedel et al. 1993 Schutze and Singer 1994 -RRB-	num_Singer_1994 conj_and_Schutze_Singer num_al._1993 nn_al._et nn_al._Weischedel num_al._1993 nn_al._et nn_al._Charniak dep_Kupiec_Singer dep_Kupiec_Schutze conj_Kupiec_al. conj_Kupiec_al. num_Kupiec_1992 num_al._1992 nn_al._et dep_Cutting_Kupiec dobj_Cutting_al. num_Merialdo_1994 num_DeMarcken_1990 num_Derose_1988 num_Church_1988 dep_Jelinek_Cutting dep_Jelinek_Merialdo dep_Jelinek_DeMarcken dep_Jelinek_Derose dep_Jelinek_Church num_Jelinek_1985 dep_tagging_Jelinek amod_tagging_based nn_tagging_Markovmodel dobj_exploring_tagging xcomp_further_exploring prep_on_been_further aux_been_has nsubj_been_work amod_taggers_part-of-speech amod_taggers_trained advmod_trained_automatically dobj_developing_taggers prepc_in_work_developing amod_work_recent det_work_all advmod_work_Almost
J97-3003	A92-1018	o	As the baseline standard we took the ending-guessing rule set supplied with the Xerox tagger -LRB- Cutting et al. 1992 -RRB-	advmod_1992_al. nn_al._et dobj_Cutting_1992 nn_tagger_Xerox det_tagger_the prep_with_supplied_tagger dep_set_Cutting vmod_set_supplied amod_rule_ending-guessing det_rule_the dep_took_set dobj_took_rule nsubj_took_we prep_as_took_standard nn_standard_baseline det_standard_the
J97-3003	A92-1018	o	The Xerox tagger -LRB- Cutting et al. 1992 -RRB- comes with a set of rules that assign an unknown word a set of possible pos-tags -LRB- i.e. POS-class -RRB- on the basis of its ending segment	amod_segment_ending poss_segment_its prep_of_basis_segment det_basis_the dep_POS-class_i.e. dep_pos-tags_POS-class amod_pos-tags_possible prep_on_set_basis prep_of_set_pos-tags det_set_a dep_word_set amod_word_unknown det_word_an dobj_assign_word nsubj_assign_that rcmod_rules_assign prep_of_set_rules det_set_a prep_with_comes_set nsubj_comes_tagger nn_al._et tmod_Cutting_1992 dobj_Cutting_al. dep_tagger_Cutting nn_tagger_Xerox det_tagger_The
N01-1023	A92-1018	p	-LRB- Cutting et al. 1992 -RRB- reported very high results -LRB- 96 % on the Brown corpus -RRB- for unsupervised POS tagging using Hidden Markov Models -LRB- HMMs -RRB- by exploiting hand-built tag dictionaries and equivalence classes	amod_classes_equivalence conj_and_dictionaries_classes nn_dictionaries_tag amod_dictionaries_hand-built dobj_exploiting_classes dobj_exploiting_dictionaries appos_Models_HMMs nn_Models_Markov nn_Models_Hidden prepc_by_using_exploiting dobj_using_Models xcomp_tagging_using vmod_POS_tagging amod_POS_unsupervised amod_corpus_Brown det_corpus_the prep_on_%_corpus num_%_96 prep_for_results_POS dep_results_% amod_results_high advmod_high_very dobj_reported_results parataxis_reported_Cutting nn_al._et tmod_Cutting_1992 dobj_Cutting_al.
N06-1042	A92-1018	o	It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm -LRB- Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. nn_algorithm_maximization nn_algorithm_expectation det_algorithm_the amod_data_unlabeled prep_with_using_algorithm dobj_using_data amod_models_statistical parataxis_train_Cutting vmod_train_using dobj_train_models aux_train_to ccomp_possible_train advmod_possible_also cop_possible_is nsubj_possible_It
P06-2100	A92-1018	o	For English there are many POS taggers employing machine learning techniques like transformation-based error-driven learning -LRB- Brill 1995 -RRB- decision trees -LRB- Black et al. 1992 -RRB- markov model -LRB- Cutting et al. 1992 -RRB- maximum entropy methods -LRB- Ratnaparkhi 1996 -RRB- etc. There are also taggers which are hybrid using both stochastic and rule-based approaches such as CLAWS -LRB- Garside and Smith 1997 -RRB-	dep_Garside_1997 conj_and_Garside_Smith dep_CLAWS_Smith dep_CLAWS_Garside prep_such_as_approaches_CLAWS amod_approaches_rule-based amod_approaches_stochastic conj_and_stochastic_rule-based preconj_stochastic_both dobj_using_approaches xcomp_hybrid_using cop_hybrid_are nsubj_hybrid_which rcmod_taggers_hybrid nsubj_are_taggers advmod_are_also expl_are_There parataxis_Ratnaparkhi_are dep_Ratnaparkhi_etc. dep_Ratnaparkhi_1996 dep_methods_Ratnaparkhi nn_methods_entropy nn_methods_maximum nn_al._et tmod_Cutting_1992 dobj_Cutting_al. dep_model_Cutting nn_model_markov num_al._1992 nn_al._et amod_al._Black appos_trees_methods appos_trees_model dep_trees_al. nn_trees_decision dep_Brill_1995 appos_learning_Brill amod_learning_error-driven amod_learning_transformation-based prep_like_techniques_learning nn_techniques_learning nn_techniques_machine dobj_employing_techniques nn_taggers_POS amod_taggers_many nsubj_are_trees xcomp_are_employing nsubj_are_taggers expl_are_there prep_for_are_English rcmod_``_are
P07-2056	A92-1018	o	In such cases additional information may be coded into the HMM model to achieve higher accuracy -LRB- Cutting et al. 1992 -RRB-	dep_al._1992 nn_al._et advmod_Cutting_al. amod_accuracy_higher parataxis_achieve_Cutting dobj_achieve_accuracy aux_achieve_to nn_model_HMM det_model_the xcomp_coded_achieve prep_into_coded_model auxpass_coded_be aux_coded_may nsubjpass_coded_information prep_in_coded_cases amod_information_additional amod_cases_such
P07-2056	A92-1018	p	Stochastic models -LRB- Cutting et al. 1992 Dermatas et al. 1995 Brants 2000 -RRB- have been widely used in POS tagging for simplicity and language independence of the models	det_models_the nn_independence_language prep_of_simplicity_models conj_and_simplicity_independence prep_for_tagging_independence prep_for_tagging_simplicity vmod_POS_tagging prep_in_used_POS advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_models dep_Brants_2000 dep_Dermatas_Brants num_Dermatas_1995 nn_Dermatas_al. nn_Dermatas_et num_al._1992 nn_al._et dep_Cutting_Dermatas dobj_Cutting_al. dep_models_Cutting nn_models_Stochastic
P93-1003	A92-1018	o	This situation is very similar to that involved in training HMM text taggers where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech and the rest of the words in the sentence are also generated -LRB- e.g. \ -LSB- Cutting et al. 1992 \ -RSB- -RRB-	num_\_1992 nn_al._et dobj_Cutting_\ dobj_Cutting_al. dep_\_Cutting dep_\_e.g. dep_generated_\ advmod_generated_also auxpass_generated_are nsubjpass_generated_similar det_sentence_the prep_in_words_sentence det_words_the prep_of_rest_words det_rest_the amod_part-ofspeech_particular det_part-ofspeech_a prep_to_corresponds_part-ofspeech nsubj_corresponds_word mark_corresponds_that amod_word_particular det_word_a ccomp_computed_corresponds auxpass_computed_are nsubjpass_computed_probabilities advmod_computed_where amod_probabilities_joint conj_and_taggers_rest rcmod_taggers_computed nn_taggers_text nn_taggers_HMM nn_taggers_training prep_in_involved_rest prep_in_involved_taggers vmod_that_involved prep_to_similar_that advmod_similar_very cop_similar_is nsubj_similar_situation det_situation_This
P95-1039	A92-1018	p	1 Motivation Statistical part-of-speech disambiguation can be efficiently done with n-gram models -LRB- Church 1988 Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_Church_Cutting appos_Church_1988 dep_models_Church nn_models_n-gram prep_with_done_models advmod_done_efficiently auxpass_done_be aux_done_can nsubjpass_done_disambiguation nn_disambiguation_part-of-speech amod_disambiguation_Statistical nn_disambiguation_Motivation num_disambiguation_1
P96-1006	A92-1018	o	Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96 % are readily available to assign POS to unrestricted English sentences -LRB- Brill 1992 Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_Brill_Cutting appos_Brill_1992 dep_sentences_Brill amod_sentences_English amod_sentences_unrestricted prep_to_assign_sentences dobj_assign_POS aux_assign_to xcomp_available_assign advmod_available_readily cop_available_are nsubj_available_taggers mark_available_since num_%_96 prep_of_accuracy_% dobj_achieve_accuracy aux_achieve_can nsubj_achieve_that rcmod_taggers_achieve nn_taggers_POS advcl_reasonable_available cop_reasonable_is csubj_reasonable_Making det_assumption_an predet_assumption_such dobj_Making_assumption ccomp_``_reasonable
P96-1030	A92-1018	o	-LRB- DeRose 1988 Cutting et al. 1992 Church 1988 -RRB-	amod_Church_1988 conj_al._1992 nn_al._et dep_Cutting_Church dobj_Cutting_al. dep_DeRose_Cutting appos_DeRose_1988 dep_''_DeRose
P97-1029	A92-1018	o	There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques e.g. -LRB- Church 1988 Cutting et al. 1992 DeRose 1988 -RRB- constraint-based techniques -LRB- Karlsson et al. 1995 Voutilainen 1995b Voutilainen Heikkil/i and Anttila 1992 Voutilainen and Tapanainen 1993 Oflazer and KuruSz 1994 Oflazer and Till 1996 -RRB- and transformation-based techniques -LRB- Brilt 1992 Brill 1994 Brill 1995 -RRB-	amod_Brill_1995 dep_Brill_Brill num_Brill_1994 dep_Brilt_Brill appos_Brilt_1992 dep_techniques_Brilt pobj_Till_1996 conj_and_Voutilainen_Till conj_and_Voutilainen_Oflazer conj_and_Voutilainen_1994 conj_and_Voutilainen_KuruSz conj_and_Voutilainen_Oflazer conj_and_Voutilainen_1993 conj_and_Voutilainen_Tapanainen num_Anttila_1992 conj_and_Voutilainen_Anttila conj_and_Voutilainen_Heikkil/i dep_Voutilainen_Till dep_Voutilainen_Oflazer dep_Voutilainen_1994 dep_Voutilainen_KuruSz dep_Voutilainen_Oflazer dep_Voutilainen_1993 dep_Voutilainen_Tapanainen dep_Voutilainen_Voutilainen dep_Voutilainen_Anttila dep_Voutilainen_Heikkil/i dep_Voutilainen_Voutilainen num_Voutilainen_1995b dep_Karlsson_Voutilainen appos_Karlsson_1995 dep_Karlsson_al. nn_Karlsson_et dep_techniques_Karlsson amod_techniques_constraint-based dep_DeRose_1988 conj_al._1992 nn_al._et dobj_Cutting_techniques conj_and_Cutting_transformation-based conj_and_Cutting_techniques dep_Cutting_DeRose dobj_Cutting_al. dep_Church_transformation-based dep_Church_techniques dep_Church_Cutting appos_Church_1988 dep_techniques_Church conj_techniques_e.g. amod_techniques_statistical prep_such_as_techniques_techniques amod_techniques_various dobj_using_techniques vmod_disambiguation_using dep_tagging_disambiguation conj_and_tagging_morphological prep_in_number_morphological prep_in_number_tagging prep_of_number_studies amod_number_large det_number_a cop_number_been aux_number_has expl_number_There
P97-1031	A92-1018	o	Second the automatic approach in which the model is automatically obtained from corpora -LRB- either raw or annotated -RRB- 1 and consists of n-grams -LRB- Garside et al. 1987 Cutting et ah 1992 -RRB- rules -LRB- Hindle 1989 -RRB- or neural nets -LRB- Schmid 1994 -RRB-	amod_Schmid_1994 amod_nets_neural dep_Hindle_1989 dep_rules_Schmid conj_or_rules_nets dep_rules_Hindle dep_ah_1992 nn_ah_et advmod_Cutting_ah dep_Garside_nets dep_Garside_rules dep_Garside_Cutting appos_Garside_1987 dep_Garside_al. nn_Garside_et dep_consists_Garside prep_of_consists_n-grams conj_or_raw_annotated preconj_raw_either num_corpora_1 dep_corpora_annotated dep_corpora_raw prep_from_obtained_corpora advmod_obtained_automatically auxpass_obtained_is nsubjpass_obtained_model prep_in_obtained_which det_model_the conj_and_approach_consists rcmod_approach_obtained amod_approach_automatic det_approach_the dep_Second_consists dep_Second_approach advcl_``_Second
P97-1032	A92-1018	o	Cutting et al. 1992 -RRB- local rules -LRB- e.g. Hindle 1989 -RRB- and neural networks -LRB- e.g. Schmid 1994 -RRB-	dep_1994_Schmid dep_1994_e.g. dep_networks_1994 amod_networks_neural num_Hindle_1989 conj_and_e.g._networks dep_e.g._Hindle dep_rules_networks dep_rules_e.g. amod_rules_local num_rules_1992 advmod_1992_al. amod_1992_Cutting nn_al._et ccomp_``_rules
W03-1314	A92-1018	o	-LRB- 2000 -RRB- that draws on a stochastic tagger -LRB- see -LRB- Cutting et al. 1992 -RRB- for details -RRB- as well as the SPECIALIST Lexicon5 a large syntactic lexicon of both general and medical English that is distributed with the UMLS	det_UMLS_the prep_with_distributed_UMLS auxpass_distributed_is nsubjpass_distributed_that rcmod_general_distributed dep_general_English conj_and_general_medical preconj_general_both prep_of_lexicon_medical prep_of_lexicon_general amod_lexicon_syntactic amod_lexicon_large det_lexicon_a appos_Lexicon5_lexicon nn_Lexicon5_SPECIALIST det_Lexicon5_the nn_al._et dobj_Cutting_1992 dobj_Cutting_al. prep_for_see_details dep_see_Cutting dep_tagger_see amod_tagger_stochastic det_tagger_a conj_and_on_Lexicon5 pobj_on_tagger prep_draws_Lexicon5 prep_draws_on nsubj_draws_that rcmod_2000_draws dep_''_2000
W04-1211	A92-1018	o	The prime public domain examples of such implementations include the TrigramsnTags tagger -LRB- Brandts 2000 -RRB- Xerox tagger -LRB- Cutting et al. 1992 -RRB- and LT POS tagger -LRB- Mikheev 1997 -RRB-	num_Mikheev_1997 appos_tagger_Mikheev nn_tagger_POS amod_tagger_LT nn_al._et tmod_Cutting_1992 dobj_Cutting_al. dep_tagger_Cutting nn_tagger_Xerox num_Brandts_2000 conj_and_tagger_tagger conj_and_tagger_tagger appos_tagger_Brandts nn_tagger_TrigramsnTags det_tagger_the dobj_include_tagger dobj_include_tagger dobj_include_tagger nsubj_include_examples amod_implementations_such prep_of_examples_implementations nn_examples_domain amod_examples_public amod_examples_prime det_examples_The
W04-2010	A92-1018	o	The XEROX tagger comes with a list of built-in ending guessing rules -LRB- Cutting et al. ,1992 -RRB-	nn_al._et tmod_Cutting_,1992 dobj_Cutting_al. dobj_guessing_rules xcomp_ending_guessing dep_built-in_Cutting vmod_built-in_ending prep_of_list_built-in det_list_a prep_with_comes_list nsubj_comes_tagger nn_tagger_XEROX det_tagger_The
W04-2602	A92-1018	p	It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model -LRB- Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_model_Cutting nn_model_Markov amod_model_hidden det_model_a conj_and_tagging_model amod_tagging_partial prep_with_realized_model prep_with_realized_tagging auxpass_realized_be aux_realized_can nsubjpass_realized_performance amod_performance_good det_performance_that rcmod_years_realized det_years_some prep_for_known_years auxpass_known_been aux_known_has nsubjpass_known_It
W04-2611	A92-1018	o	This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger -LRB- Cutting et al. 1992 -RRB- and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap -LRB- Aronson 2001 -RRB-	amod_Aronson_2001 dep_MetaMap_Aronson dobj_using_MetaMap nn_Metathesaurus_UMLS det_Metathesaurus_the prep_in_concepts_Metathesaurus xcomp_mapped_using prep_to_mapped_concepts auxpass_mapped_are nsubjpass_mapped_that rcmod_phrases_mapped nn_phrases_noun amod_phrases_simple dobj_provides_phrases nsubj_provides_analysis dep_al._1992 nn_al._et advmod_Cutting_al. amod_tagger_part-of-speech nn_tagger_Xerox det_tagger_the conj_and_Lexicon_tagger nn_Lexicon_SPECIALIST det_Lexicon_the conj_and_depends_provides dep_depends_Cutting prep_on_depends_tagger prep_on_depends_Lexicon nsubj_depends_analysis det_analysis_This
W04-3112	A92-1018	o	The initial phase relies on a parser that draws on the SPECIALIST Lexicon -LRB- McCray et al. 1994 -RRB- and the Xerox Part-of-Speech Tagger -LRB- Cutting et al. 1992 -RRB- to produce an underspecified categorial analysis	nn_analysis_categorial amod_analysis_underspecified det_analysis_an dobj_produce_analysis aux_produce_to nn_al._et tmod_Cutting_1992 dobj_Cutting_al. dep_Tagger_Cutting nn_Tagger_Part-of-Speech nn_Tagger_Xerox det_Tagger_the dep_1994_al. nn_al._et num_McCray_1994 conj_and_Lexicon_Tagger appos_Lexicon_McCray nn_Lexicon_SPECIALIST det_Lexicon_the xcomp_draws_produce prep_on_draws_Tagger prep_on_draws_Lexicon nsubj_draws_that rcmod_parser_draws det_parser_a prep_on_relies_parser nsubj_relies_phase amod_phase_initial det_phase_The
W05-0708	A92-1018	n	Many approaches for POS tagging have been developed in the past including rule-based tagging -LRB- Brill 1995 -RRB- HMM taggers -LRB- Brants 2000 Cutting and others 1992 -RRB- maximum-entropy models -LRB- Rathnaparki 1996 -RRB- cyclic dependency networks -LRB- Toutanova et al. 2003 -RRB- memory-based learning -LRB- Daelemans et al. 1996 -RRB- etc. All of these approaches require either a large amount of annotated training data -LRB- for supervised tagging -RRB- or a lexicon listing all possible tags for each word -LRB- for unsupervised tagging -RRB-	amod_tagging_unsupervised det_word_each prep_for_tags_tagging prep_for_tags_word amod_tags_possible det_tags_all dobj_listing_tags vmod_lexicon_listing det_lexicon_a conj_or_tagging_lexicon amod_tagging_supervised pobj_for_lexicon pobj_for_tagging ccomp_-LRB-_for nn_data_training amod_data_annotated prep_of_amount_data amod_amount_large det_amount_a preconj_amount_either dobj_require_amount nsubj_require_All det_approaches_these prep_of_All_approaches amod_Daelemans_1996 dep_Daelemans_al. nn_Daelemans_et dep_learning_Daelemans amod_learning_memory-based amod_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et nn_networks_dependency amod_networks_cyclic amod_Rathnaparki_1996 dep_models_Rathnaparki amod_models_maximum-entropy dep_others_1992 conj_and_Cutting_others dep_Brants_others dep_Brants_Cutting appos_Brants_2000 appos_taggers_Brants nn_taggers_HMM amod_Brill_1995 conj_tagging_etc. conj_tagging_learning dep_tagging_Toutanova conj_tagging_networks conj_tagging_models conj_tagging_taggers dep_tagging_Brill amod_tagging_rule-based det_past_the parataxis_developed_require prep_including_developed_tagging prep_in_developed_past auxpass_developed_been aux_developed_have nsubjpass_developed_approaches nn_tagging_POS prep_for_approaches_tagging amod_approaches_Many
W94-0111	A92-1018	n	Brill 's results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging -LRB- Jelinek 1985 Church 1988 DeRose 1988 Cutting et al. 1992 Weischedel et al. 1993 -RRB- as well as showing promise for other applications	amod_applications_other prep_for_promise_applications dobj_showing_promise nsubjpass_showing_that num_Weischedel_1993 nn_Weischedel_al. nn_Weischedel_et amod_1992_Cutting nn_al._et advmod_Cutting_al. num_DeRose_1988 appos_Church_1988 dep_Jelinek_Weischedel conj_Jelinek_1992 dep_Jelinek_DeRose dep_Jelinek_Church dep_Jelinek_1985 appos_tagging_Jelinek amod_tagging_part-of-speech conj_and_used_showing prep_for_used_tagging advmod_used_frequently auxpass_used_are nsubjpass_used_that ccomp_approaches_showing ccomp_approaches_used nn_Model_Markov nn_Model_Hidden det_Model_the dobj_outperform_Model aux_outperform_can nsubj_outperform_approach mark_outperform_that det_approach_this ccomp_demonstrate_approaches ccomp_demonstrate_outperform nsubj_demonstrate_results poss_results_Brill
W95-0101	A92-1018	o	It is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the Baum-Welch algorithm \ -LSB- Baum 1972 Jelinek 1985 Cutting et al. 1992 Kupiec 1992 Elworthy 1994 Merialdo 1995 \ -RSB-	num_\_1995 appos_Merialdo_\ dep_Elworthy_Merialdo num_Elworthy_1994 dep_Kupiec_Elworthy num_Kupiec_1992 conj_al._1992 nn_al._et dep_Cutting_Kupiec dobj_Cutting_al. num_Jelinek_1985 dep_Baum_Cutting dep_Baum_Jelinek dep_Baum_1972 dep_\_Baum vmod_algorithm_\ nn_algorithm_Baum-Welch det_algorithm_the dobj_using_algorithm amod_corpus_annotated det_corpus_a advmod_annotated_manually prep_for_need_corpus det_need_the amod_taggers_stochastic prepc_by_train_using prep_without_train_need dobj_train_taggers aux_train_to xcomp_learning_train vmod_unsupervised_learning dobj_use_unsupervised aux_use_to xcomp_possible_use cop_possible_is nsubj_possible_It
W95-0101	A92-1018	o	This method is employed in \ -LSB- Kupiec 1992 Cutting et al. 1992 \ -RSB-	num_\_1992 amod_\_Cutting nn_al._et dobj_Cutting_al. dep_Kupiec_\ appos_Kupiec_1992 dep_employed_Kupiec prep_in_employed_\ auxpass_employed_is nsubjpass_employed_method det_method_This ccomp_``_employed
W95-0101	A92-1018	o	Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging \ -LSB- Jelinek 1985 Church 1988 Derose 1988 DeMarcken 1990 Cutting et al. 1992 Kupiec 1992 Charniak et al. 1993 Weischedel et al. 1993 Schutze and Singer 1994 Lin et al. 1994 Elworthy 1994 Merialdo 1995 \ -RSB-	num_\_1995 appos_Merialdo_\ num_Elworthy_1994 num_Lin_1994 nn_Lin_al. nn_Lin_et num_Schutze_1994 conj_and_Schutze_Singer tmod_al._1993 nn_al._et nn_al._Weischedel dep_al._1993 nn_al._et nn_al._Charniak num_Kupiec_1992 conj_al._1992 nn_al._et dobj_Cutting_al. appos_DeMarcken_1990 num_Derose_1988 dep_Church_Merialdo conj_Church_Elworthy conj_Church_Lin conj_Church_Singer conj_Church_Schutze conj_Church_al. conj_Church_al. conj_Church_Kupiec conj_Church_Cutting conj_Church_DeMarcken conj_Church_Derose appos_Church_1988 dep_Jelinek_Church dep_Jelinek_1985 amod_\_tagging nn_\_speech prep_of_part_\ amod_part_based amod_part_Markov-model dep_explored_Jelinek dobj_explored_part aux_explored_has nsubj_explored_all amod_taggers_trained advmod_trained_automatically prep_of_area_taggers det_area_the prep_in_work_area det_work_the prep_of_all_work advmod_all_Almost ccomp_``_explored
W96-0101	A92-1018	o	1 Introduction In the part-of-speech hterature whether taggers are based on a rule-based approach -LRB- Klein and Simmons 1963 -RRB- -LRB- Brill 1992 -RRB- -LRB- Voutilainen 1993 -RRB- or on a statistical one -LRB- Bahl and Mercer 1976 -RRB- -LRB- Leech et al. 1983 -RRB- -LRB- Merialdo 1994 -RRB- -LRB- DeRose 1988 -RRB- -LRB- Church 1989 -RRB- -LRB- Cutting et al. 1992 -RRB- there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones	amod_ones_contextual conj_negcc_probabilities_ones amod_probabilities_lexical prep_to_paid_ones prep_to_paid_probabilities auxpass_paid_be aux_paid_should nsubjpass_paid_attention mark_paid_whether amod_attention_more pcomp_debate_paid prepc_as_to_debate_to det_debate_a dobj_is_debate expl_is_there dep_is_on dep_is_Brill prep_based_on_is_approach auxpass_is_are nsubjpass_is_taggers mark_is_whether nsubj_is_Introduction nn_al._et dobj_Cutting_1992 dobj_Cutting_al. appos_Church_1989 dep_DeRose_1988 dep_Merialdo_1994 amod_Leech_1983 dep_Leech_al. nn_Leech_et dep_Bahl_1976 conj_and_Bahl_Mercer dep_one_Mercer dep_one_Bahl amod_one_statistical det_one_a pobj_on_one dep_Voutilainen_1993 dep_Brill_Cutting appos_Brill_Church appos_Brill_DeRose appos_Brill_Merialdo appos_Brill_Leech conj_or_Brill_on appos_Brill_Voutilainen amod_Brill_1992 dep_Klein_1963 conj_and_Klein_Simmons appos_approach_Simmons appos_approach_Klein amod_approach_rule-based det_approach_a amod_hterature_part-of-speech det_hterature_the prep_in_Introduction_hterature num_Introduction_1 ccomp_``_is
W96-0101	A92-1018	o	5 Comparison with other approaches In some sense this approach is similar to the notion of ambiguity classes explained in -LRB- Kupiec 1992 -RRB- and -LRB- Cutting et al. 1992 -RRB- where words that belong to the same part-of-speech figure together	advmod_figure_together nn_figure_part-of-speech amod_figure_same det_figure_the prep_to_belong_figure nsubj_belong_that rcmod_words_belong dep_where_words nn_al._et dep_Cutting_where dobj_Cutting_1992 dobj_Cutting_al. conj_and_Kupiec_Cutting amod_Kupiec_1992 prep_in_explained_Cutting prep_in_explained_Kupiec vmod_classes_explained nn_classes_ambiguity prep_notion_of det_notion_the dep_similar_classes prep_to_similar_notion cop_similar_is nsubj_similar_approach advcl_similar_Comparison det_approach_this det_sense_some prep_in_approaches_sense amod_approaches_other prep_with_Comparison_approaches num_Comparison_5 ccomp_``_similar
W96-0101	A92-1018	o	-LRB- Chanod and Tapanainen 1995 -RRB- compare two tagging frameworks for tagging French one that is statistical built upon the Xerox tagger -LRB- Cutting et al. 1992 -RRB- and another based on linguistic constraints only	advmod_constraints_only amod_constraints_linguistic pobj_another_constraints prepc_based_on_another_on nn_al._et dobj_Cutting_1992 dobj_Cutting_al. nn_tagger_Xerox det_tagger_the dep_built_Cutting prep_upon_built_tagger cop_statistical_is nsubj_statistical_that rcmod_one_statistical conj_and_French_another conj_and_French_built appos_French_one dobj_tagging_another dobj_tagging_built dobj_tagging_French amod_frameworks_tagging num_frameworks_two prepc_for_compare_tagging dobj_compare_frameworks nsubj_compare_Tapanainen nsubj_compare_Chanod amod_Chanod_1995 conj_and_Chanod_Tapanainen
W96-0102	A92-1018	o	Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers -LRB- e.g. Church 1988 DeRose 1988 Cutting et al. 1992 Merialdo 1994 etc. -RRB-	amod_Merialdo_etc. num_Merialdo_1994 num_al._1992 nn_al._et parataxis_Cutting_Merialdo dobj_Cutting_al. num_DeRose_1988 dep_Church_Cutting dep_Church_DeRose dep_Church_1988 nn_Church_e.g. amod_taggers_Model-based nn_taggers_Markov nn_taggers_Hidden conj_or_models_taggers amod_models_n-gram dep_used_Church dobj_used_taggers dobj_used_models aux_used_has nsubj_used_work amod_methods_statistical prep_on_work_methods amod_work_Most
W96-0113	A92-1018	o	Kupiec -LRB- 1992 -RRB- has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm -LRB- Rabiner et al. 1994 -RRB- from an untagged corpus and Cutting et al.	nn_al._et dobj_Cutting_al. nsubj_Cutting_Kupiec amod_corpus_untagged det_corpus_an appos_Rabiner_1994 dep_Rabiner_al. nn_Rabiner_et nn_algorithm_reestimation amod_algorithm_Baum-Welch det_algorithm_the dobj_using_algorithm nn_model_language nn_model_N-gram det_model_the dep_method_Rabiner vmod_method_using prep_for_method_model nn_method_estimation det_method_an conj_and_proposed_Cutting prep_from_proposed_corpus dobj_proposed_method aux_proposed_has nsubj_proposed_Kupiec appos_Kupiec_1992 ccomp_``_Cutting ccomp_``_proposed
W96-0205	A92-1018	o	Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers the maximization of Equation -LRB- 1 -RRB- to get the most likely tag sequence is accomplished by the Viterbi algorithm -LRB- Church 1988 -RRB- and the maximum likelihood estimates of the parameters of Equation -LRB- 2 -RRB- are obtained from untagged corpus by the ForwardBackward algorithm -LRB- Cutting et al. 1992 -RRB-	dep_al._1992 nn_al._et advmod_Cutting_al. nn_algorithm_ForwardBackward det_algorithm_the amod_corpus_untagged dep_obtained_Cutting agent_obtained_algorithm prep_from_obtained_corpus auxpass_obtained_are nsubjpass_obtained_estimates appos_Equation_2 prep_of_parameters_Equation det_parameters_the prep_of_estimates_parameters nn_estimates_likelihood nn_estimates_maximum det_estimates_the appos_Church_1988 dep_algorithm_Church nn_algorithm_Viterbi det_algorithm_the conj_and_accomplished_obtained agent_accomplished_algorithm auxpass_accomplished_is nsubjpass_accomplished_Generalization nn_sequence_tag amod_sequence_likely det_sequence_the advmod_likely_most dobj_get_sequence aux_get_to appos_Equation_1 vmod_maximization_get prep_of_maximization_Equation det_maximization_the nn_taggers_speech prep_of_part_taggers advmod_Algorithm_Viterbi advmod_Algorithm_Forward det_Algorithm_the conj_and_Forward_Viterbi appos_Generalization_maximization dep_Generalization_part prep_in_Generalization_English prep_of_Generalization_Algorithm nn_Generalization_Reestimation advmod_Generalization_Backward nn_Generalization_Forward nn_Generalization_Generalized
W96-0206	A92-1018	o	The accuracy of the derived model depends heavily on the initial bias but with a good choice results are comparable to those of method three -LRB- Cutting et al. 1992 -RRB-	nn_al._et dobj_Cutting_1992 dobj_Cutting_al. num_method_three prep_of_those_method dep_comparable_Cutting prep_to_comparable_those cop_comparable_are prep_with_comparable_results nsubj_comparable_accuracy nn_results_choice amod_results_good det_results_a amod_bias_initial det_bias_the conj_but_depends_comparable prep_on_depends_bias advmod_depends_heavily nsubj_depends_accuracy amod_model_derived det_model_the prep_of_accuracy_model det_accuracy_The
W97-0307	A92-1018	o	-LRB- Cutting et al. 1992 Feldweg 1995 -RRB- -RRB- the tagger for grammatical functions works with lexical and contextual probability measures Pq -LRB- -RRB-	dep_Pq_-LRB- nn_Pq_measures nn_Pq_probability amod_Pq_contextual amod_Pq_lexical conj_and_lexical_contextual prep_with_works_Pq nsubj_works_tagger parataxis_works_Cutting amod_functions_grammatical prep_for_tagger_functions det_tagger_the dep_Feldweg_1995 conj_al._1992 nn_al._et dep_Cutting_Feldweg dobj_Cutting_al.
W97-0811	A92-1018	o	In our experiments we used the Hidden Markov Model -LRB- HMM -RRB- tagging method described in \ -LSB- Cutting et aL 1992 \ -RSB-	num_\_1992 appos_aL_\ nn_aL_et dobj_Cutting_aL prep_in_described_\ vmod_method_described amod_method_tagging nn_method_Model appos_Model_HMM nn_Model_Markov nn_Model_Hidden det_Model_the dep_used_Cutting dobj_used_method nsubj_used_we prep_in_used_experiments poss_experiments_our
W98-1110	A92-1018	o	The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model -LRB- HMM -RRB- -LRB- Cutting et al. 1992 Kupiec	conj_al._Kupiec conj_al._1992 nn_al._et dobj_Cutting_al. appos_model_HMM nn_model_markov amod_model_hidden dep_using_Cutting dobj_using_model advmod_using_mainly amod_approaches_statistical xcomp_performed_using agent_performed_approaches auxpass_performed_been advmod_performed_usually aux_performed_has nsubjpass_performed_disambiguation nn_disambiguation_POS det_disambiguation_The
W98-1110	A92-1018	o	Our statistical tagging model is adjusted from standard bi-grams using the Viterbi-search -LRB- Cutting et al. 1992 -RRB- plus on-the-fly extra computing of lexical probabilities for unknown morphemes	amod_morphemes_unknown prep_for_probabilities_morphemes amod_probabilities_lexical prep_of_computing_probabilities amod_computing_extra amod_computing_on-the-fly amod_computing_Viterbi-search det_computing_the dep_al._1992 nn_al._et advmod_Cutting_al. conj_plus_Viterbi-search_on-the-fly dep_Viterbi-search_Cutting dobj_using_computing amod_bi-grams_standard xcomp_adjusted_using prep_from_adjusted_bi-grams auxpass_adjusted_is nsubjpass_adjusted_model nn_model_tagging amod_model_statistical poss_model_Our
W98-1207	A92-1018	o	-LRB- Cutting et al. 1992 Feldweg 1995 -RRB- -RRB- the tagger for grammatical functions works with lexical -LRB- 1 -RRB- Selbst besucht ADV VVPP himself visited hat Peter Sabine VAFIN NE NE has Peter Sabine ` Peter never visited Sabine himself ' l hie ADV never Figure 2 Example sentence and contextual probability measures PO -LRB- ' -RRB- depending on the category of a mother node -LRB- Q -RRB-	appos_node_Q nn_node_mother det_node_a prep_of_category_node det_category_the nn_PO_measures nn_PO_probability amod_PO_contextual conj_and_sentence_PO nn_sentence_Example pobj_Figure_category prepc_depending_on_Figure_on dep_Figure_PO dep_Figure_sentence num_Figure_2 neg_Figure_never dep_ADV_Figure nn_ADV_hie nn_ADV_l nn_ADV_Sabine npadvmod_Sabine_himself dobj_visited_ADV neg_visited_never nsubj_visited_Peter aux_visited_has parataxis_visited_works nn_Peter_Sabine nn_Peter_Peter nn_NE_NE nn_NE_VAFIN nn_NE_Sabine nn_NE_Peter nn_NE_hat dobj_visited_NE nsubj_visited_himself rcmod_VVPP_visited nn_VVPP_ADV nn_VVPP_besucht nn_VVPP_Selbst dep_VVPP_1 amod_VVPP_lexical prep_with_works_VVPP nsubj_works_tagger parataxis_works_Cutting amod_functions_grammatical prep_for_tagger_functions det_tagger_the dep_Feldweg_1995 conj_al._1992 nn_al._et dep_Cutting_Feldweg dobj_Cutting_al.
W99-0608	A92-1018	o	2.2 STT A Statistical Tree-based Tagger The aim of statistical or probabilistic tagging -LRB- Church 1988 Cutting et al. 1992 -RRB- is to assign the most likely sequence of tags given the observed sequence of words	prep_of_sequence_words amod_sequence_observed det_sequence_the pobj_given_sequence prep_tags_given prep_of_sequence_tags amod_sequence_likely det_sequence_the advmod_likely_most dobj_assign_sequence aux_assign_to aux_assign_is nsubj_assign_Church nn_al._et dobj_Cutting_1992 dobj_Cutting_al. dep_Church_Cutting appos_Church_1988 amod_tagging_probabilistic amod_tagging_statistical conj_or_statistical_probabilistic prepc_of_aim_assign det_aim_The dep_Tagger_aim amod_Tagger_Tree-based amod_Tagger_Statistical det_Tagger_A dep_STT_Tagger num_STT_2.2 dep_``_STT
C04-1147	C02-1007	o	Examples of such affinities include synonyms -LRB- Terra and Clarke 2003 -RRB- verb similarities -LRB- Resnik and Diab 2000 -RRB- and word associations -LRB- Rapp 2002 -RRB-	amod_Rapp_2002 dep_associations_Rapp nn_associations_word dep_Resnik_2000 conj_and_Resnik_Diab conj_and_similarities_associations dep_similarities_Diab dep_similarities_Resnik dobj_verb_associations dobj_verb_similarities num_Terra_2003 conj_and_Terra_Clarke dep_synonyms_verb appos_synonyms_Clarke appos_synonyms_Terra dobj_include_synonyms nsubj_include_Examples amod_affinities_such prep_of_Examples_affinities
D08-1096	C02-1007	n	Several papers have looked at higher-order representations but have not examined the equivalence of syn/para distributions when formalized as Markov chains -LRB- Schutze and Pedersen 1993 Lund and Burgess 1996 Edmonds 1997 Rapp 2002 Biemann et al. 2004 Lemaire and Denhi`ere 2006 -RRB-	amod_Lemaire_2006 conj_and_Lemaire_Denhi`ere num_Biemann_2004 nn_Biemann_al. nn_Biemann_et num_Rapp_2002 appos_Edmonds_1997 dep_Schutze_Denhi`ere dep_Schutze_Lemaire conj_and_Schutze_Biemann conj_and_Schutze_Rapp conj_and_Schutze_Edmonds conj_and_Schutze_1996 conj_and_Schutze_Burgess conj_and_Schutze_Lund conj_and_Schutze_1993 conj_and_Schutze_Pedersen appos_chains_Biemann appos_chains_Rapp appos_chains_Edmonds appos_chains_1996 appos_chains_Burgess appos_chains_Lund appos_chains_1993 appos_chains_Pedersen appos_chains_Schutze nn_chains_Markov prep_as_formalized_chains advmod_formalized_when nn_distributions_syn/para prep_of_equivalence_distributions det_equivalence_the advcl_examined_formalized dobj_examined_equivalence neg_examined_not aux_examined_have nsubj_examined_papers amod_representations_higher-order conj_but_looked_examined prep_at_looked_representations aux_looked_have nsubj_looked_papers amod_papers_Several
D09-1066	C02-1007	o	Roughly in keeping with -LRB- Rapp 2002 -RRB- we hereby regard paradigmatic assocations as those based largely on word similarity -LRB- i.e. including those typically classed as synonyms antonyms hypernyms hyponyms etc -RRB- whereas syntagmatic associations are all those words which strongly invoke one another yet which can not readily be said to be similar	cop_similar_be aux_similar_to xcomp_said_similar auxpass_said_be advmod_said_readily neg_said_not aux_said_can nsubjpass_said_which advmod_said_yet rcmod_another_said num_another_one dobj_invoke_another advmod_invoke_strongly nsubj_invoke_which rcmod_words_invoke det_words_those predet_words_all cop_words_are nsubj_words_associations mark_words_whereas amod_associations_syntagmatic nn_etc_hyponyms conj_synonyms_etc conj_synonyms_hypernyms conj_synonyms_antonyms prep_as_classed_synonyms advmod_classed_typically vmod_those_classed prep_including_i.e._those dep_similarity_i.e. nn_similarity_word prep_on_based_similarity advmod_based_largely vmod_those_based amod_assocations_paradigmatic advcl_regard_words prep_as_regard_those dobj_regard_assocations advmod_regard_hereby nsubj_regard_we prepc_in_regard_keeping advmod_regard_Roughly appos_Rapp_2002 prep_with_keeping_Rapp
D09-1066	C02-1007	o	Then by using evaluations similar to those described in -LRB- Baroni et al. 2008 -RRB- and by Rapp -LRB- 2002 -RRB- we show that the best distance-based measures correlate better overall with human association scores than do the best window based configurations -LRB- see Section 4 -RRB- and that they also serve as better predictors of the strongest human associations -LRB- see Section 5 -RRB-	num_Section_5 dobj_see_Section dep_associations_see amod_associations_human amod_associations_strongest det_associations_the prep_of_predictors_associations amod_predictors_better advmod_better_as dobj_serve_predictors advmod_serve_also nsubj_serve_they mark_serve_that nsubj_4_Section conj_and_see_serve dep_see_4 dep_configurations_serve dep_configurations_see amod_configurations_based dep_window_configurations amod_window_best det_window_the dobj_do_window mark_do_than nn_scores_association amod_scores_human prep_with_overall_scores advcl_better_do amod_better_overall dobj_correlate_better nsubj_correlate_measures mark_correlate_that amod_measures_distance-based amod_measures_best det_measures_the ccomp_show_correlate nsubj_show_we appos_Rapp_2002 pobj_by_Rapp rcmod_Baroni_show conj_and_Baroni_by amod_Baroni_2008 dep_Baroni_al. nn_Baroni_et prep_in_described_by prep_in_described_Baroni vmod_those_described prep_to_similar_those amod_evaluations_similar dobj_using_evaluations pcomp_by_using ccomp_,_by dep_``_Then
D09-1066	C02-1007	o	While choosing an optimum window size for an application is often subject to trial and error there are some generally recognized trade-offs between small versus large windows such as the impact of data-sparseness and the nature of the associations retrieved -LRB- Church and Hanks 1989 Church and Hanks 1991 Rapp 2002 -RRB- Measures based on distance between words in the text	det_text_the prep_in_words_text prep_between_distance_words prep_on_based_distance vmod_Measures_based dep_Measures_Rapp dep_Rapp_2002 conj_and_Church_Measures conj_and_Church_1991 conj_and_Church_Hanks dep_Church_Measures dep_Church_1991 dep_Church_Hanks dep_Church_Church conj_and_Church_1989 conj_and_Church_Hanks dep_retrieved_1989 dep_retrieved_Hanks dep_retrieved_Church vmod_associations_retrieved det_associations_the prep_of_nature_associations det_nature_the prep_of_impact_data-sparseness det_impact_the conj_and_windows_nature prep_such_as_windows_impact amod_windows_large amod_windows_small conj_versus_small_large prep_between_trade-offs_nature prep_between_trade-offs_windows dobj_recognized_trade-offs advmod_recognized_generally nsubj_recognized_some dep_are_recognized expl_are_there ccomp_are_subject conj_and_trial_error prep_to_subject_error prep_to_subject_trial advmod_subject_often cop_subject_is csubj_subject_choosing det_application_an nn_size_window nn_size_optimum det_size_an prep_for_choosing_application dobj_choosing_size mark_choosing_While
D09-1066	C02-1007	o	3 Methodology Similar to -LRB- Rapp 2002 Baroni et al. 2008 among others -RRB- we use comparison to human assocation datasets as a test bed for the scores produced by computational association measures	nn_measures_association amod_measures_computational agent_produced_measures vmod_scores_produced det_scores_the prep_for_bed_scores nn_bed_test det_bed_a nn_datasets_assocation amod_datasets_human prep_to_comparison_datasets prep_as_use_bed dobj_use_comparison nsubj_use_we ccomp_use_Methodology prep_among_Baroni_others num_Baroni_2008 nn_Baroni_al. nn_Baroni_et dep_Rapp_Baroni dep_Rapp_2002 dep_to_Rapp prep_Similar_to amod_Methodology_Similar num_Methodology_3
D09-1066	C02-1007	o	We use evaluations similar to those used before -LRB- Rapp 2002 Pado and Lapata 2007 Baroni et al. 2008 among others -RRB-	prep_among_Baroni_others num_Baroni_2008 nn_Baroni_al. nn_Baroni_et dep_Pado_Baroni conj_and_Pado_2007 conj_and_Pado_Lapata dep_Rapp_2007 dep_Rapp_Lapata dep_Rapp_Pado appos_Rapp_2002 prep_before_used_Rapp vmod_those_used prep_to_similar_those amod_evaluations_similar dobj_use_evaluations nsubj_use_We
E09-1098	C02-1007	o	At the present time given the key role of window size in determining the selection and apparent strength of associations under the conventional co-occurrence model highlighted here and in the works of Church et al -LRB- 1991 -RRB- Rapp -LRB- 2002 -RRB- Wang -LRB- 2005 -RRB- and Schulte im Walde & Melinger -LRB- 2008 -RRB- we would urge that this is an issue which window-driven studies continue to conscientiously address at the very least scale is a parameter which findings dependent on distributional phenomena must be qualified in light of	prep_light_of prep_in_qualified_light auxpass_qualified_be aux_qualified_must nsubjpass_qualified_findings dobj_qualified_which amod_phenomena_distributional prep_on_dependent_phenomena amod_findings_dependent rcmod_parameter_qualified det_parameter_a cop_parameter_is nsubj_parameter_scale advmod_parameter_at advmod_least_very det_least_the pobj_at_least advmod_address_conscientiously aux_address_to xcomp_continue_address nsubj_continue_studies dobj_continue_which amod_studies_window-driven rcmod_issue_continue det_issue_an cop_issue_is nsubj_issue_this mark_issue_that ccomp_urge_issue aux_urge_would nsubj_urge_we dep_urge_Melinger dep_urge_Walde appos_Melinger_2008 conj_and_Walde_Melinger nn_Walde_im nn_Walde_Schulte appos_Wang_2005 appos_Rapp_2002 appos_al_1991 appos_Church_Wang appos_Church_Rapp dep_Church_al nn_Church_et prep_of_works_Church det_works_the advmod_highlighted_here vmod_model_highlighted nn_model_co-occurrence amod_model_conventional det_model_the amod_strength_apparent prep_of_selection_associations conj_and_selection_strength det_selection_the conj_and_determining_works prep_under_determining_model dobj_determining_strength dobj_determining_selection nn_size_window prepc_in_role_works prepc_in_role_determining prep_of_role_size amod_role_key det_role_the parataxis_given_parameter conj_and_given_urge pobj_given_role ccomp_,_urge ccomp_,_given amod_time_present det_time_the pobj_At_time dep_``_At
E09-1098	C02-1007	o	As Rapp -LRB- 2002 -RRB- observes choosing a window size involves making a trade-off between various qualities	amod_qualities_various prep_between_trade-off_qualities det_trade-off_a dobj_making_trade-off xcomp_involves_making nn_size_window det_size_a dep_choosing_involves dobj_choosing_size xcomp_observes_choosing nsubj_observes_Rapp advmod_observes_As appos_Rapp_2002
E09-1098	C02-1007	o	Rapp -LRB- 2002 -RRB- calls this trade-off specificity equivalent observations were made by Church & Hanks -LRB- 1989 -RRB- and Church et al -LRB- 1991 -RRB- who refer to the tendency for large windows to wash out smear or defocus those associations exhibited at smaller scales	amod_scales_smaller prep_at_exhibited_scales det_associations_those dobj_wash_associations conj_or_wash_defocus conj_or_wash_smear prt_wash_out aux_wash_to vmod_windows_defocus vmod_windows_smear vmod_windows_wash amod_windows_large prep_for_tendency_windows det_tendency_the dep_refer_exhibited prep_to_refer_tendency nsubj_refer_who dep_al_1991 nn_al_et nn_al_Church conj_and_Church_al dep_Church_1989 conj_and_Church_Hanks dep_made_refer agent_made_al agent_made_Hanks agent_made_Church auxpass_made_were nsubjpass_made_observations amod_observations_equivalent nn_specificity_trade-off det_specificity_this parataxis_calls_made dobj_calls_specificity nsubj_calls_Rapp appos_Rapp_2002
E09-1098	C02-1007	o	2.1 Scale-dependence It has been shown that varying the size of the context considered for a word can impact upon the performance of applications -LRB- Rapp 2002 Yarowsky & Florian 2002 -RRB- there being no ideal window size for all applications	det_applications_all prep_for_size_applications nn_size_window nn_size_ideal neg_size_no amod_size_being expl_being_there dep_being_Scale-dependence dep_Yarowsky_2002 conj_and_Yarowsky_Florian dep_Rapp_Florian dep_Rapp_Yarowsky appos_Rapp_2002 appos_applications_Rapp prep_of_performance_applications det_performance_the prep_upon_impact_performance aux_impact_can csubj_impact_varying mark_impact_that det_word_a prep_for_considered_word det_context_the vmod_size_considered prep_of_size_context det_size_the dobj_varying_size ccomp_shown_impact auxpass_shown_been aux_shown_has nsubjpass_shown_It rcmod_Scale-dependence_shown num_Scale-dependence_2.1 dep_``_size
E09-1098	C02-1007	o	2.2 Data sparseness Another facet of the general trade-off identified by Rapp -LRB- 2002 -RRB- pertains to how limitations in862 herent in the combination of data and cooccurrence retrieval method are manifest	cop_manifest_are csubj_manifest_sparseness nn_method_retrieval nn_method_cooccurrence conj_and_data_method prep_of_combination_method prep_of_combination_data det_combination_the prep_in_in862_combination advmod_in862_herent nsubj_in862_limitations advmod_in862_how nn_pertains_Rapp appos_Rapp_2002 prepc_to_identified_in862 agent_identified_pertains vmod_trade-off_identified amod_trade-off_general det_trade-off_the prep_of_facet_trade-off det_facet_Another dobj_sparseness_facet nsubj_sparseness_Data num_Data_2.2
E09-1098	C02-1007	o	This is one manifestation of what is commonly referred to as the data sparseness problem and was discussed by Rapp -LRB- 2002 -RRB- as a side-effect of specificity	prep_of_side-effect_specificity det_side-effect_a appos_Rapp_2002 prep_as_discussed_side-effect agent_discussed_Rapp auxpass_discussed_was nsubjpass_discussed_This nn_problem_sparseness nn_problem_data det_problem_the prep_as_referred_problem prep_referred_to advmod_referred_commonly auxpass_referred_is nsubjpass_referred_what conj_and_manifestation_discussed prepc_of_manifestation_referred num_manifestation_one cop_manifestation_is nsubj_manifestation_This
P04-3026	C02-1007	o	Whereas until recently the focus of research had been on sense disambiguation papers like Pantel & Lin -LRB- 2002 -RRB- Neill -LRB- 2002 -RRB- and Rapp -LRB- 2003 -RRB- give evidence that sense induction now also attracts attention	dobj_attracts_attention advmod_attracts_also advmod_attracts_now nsubj_attracts_induction mark_attracts_that nn_induction_sense ccomp_evidence_attracts dobj_give_evidence nsubj_give_papers advcl_give_been mark_give_Whereas appos_Rapp_2003 appos_Neill_2002 conj_and_Pantel_Rapp appos_Pantel_Neill appos_Pantel_2002 conj_and_Pantel_Lin prep_like_papers_Rapp prep_like_papers_Lin prep_like_papers_Pantel nn_disambiguation_sense prep_on_been_disambiguation aux_been_had nsubj_been_focus mark_been_until prep_of_focus_research det_focus_the advmod_focus_recently advcl_``_give
P04-3026	C02-1007	o	3 Algorithm As in previous work -LRB- Rapp 2002 -RRB- our computations are based on a partially lemmatized version of the British National Corpus -LRB- BNC -RRB- which has the function words removed	nsubj_removed_words nn_words_function det_words_the ccomp_has_removed nsubj_has_which rcmod_Corpus_has appos_Corpus_BNC nn_Corpus_National nn_Corpus_British det_Corpus_the prep_of_version_Corpus amod_version_lemmatized det_version_a advmod_lemmatized_partially prep_on_based_version auxpass_based_are nsubjpass_based_computations poss_computations_our dep_Rapp_2002 dep_work_Rapp amod_work_previous pobj_in_work pcomp_As_in rcmod_Algorithm_based prep_Algorithm_As num_Algorithm_3
P04-3026	C02-1007	o	We used the procedure described in Rapp -LRB- 2002 -RRB- with the only modification being the multiplication of the loglikelihood values with a triangular function that depends on the logarithm of a words frequency	nn_frequency_words det_frequency_a prep_of_logarithm_frequency det_logarithm_the prep_on_depends_logarithm nsubj_depends_that rcmod_function_depends amod_function_triangular det_function_a nn_values_loglikelihood det_values_the prep_with_multiplication_function prep_of_multiplication_values det_multiplication_the cop_multiplication_being nsubj_multiplication_modification amod_modification_only det_modification_the appos_Rapp_2002 prep_in_described_Rapp vmod_procedure_described det_procedure_the prepc_with_used_multiplication dobj_used_procedure nsubj_used_We
P09-1051	C02-1007	o	-LRB- Ruge 1992 Rapp 2002 -RRB- -RRB-	dep_Rapp_2002 dep_Ruge_Rapp dep_Ruge_1992 dep_''_Ruge
W04-2117	C02-1007	o	Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments -LRB- Landauer et al. 1998 Rapp 2002 -RRB- there has not been any research on the usage of a digital network-based dictionary reflecting the organisation of the mental lexicon to our knowledge	poss_knowledge_our amod_lexicon_mental det_lexicon_the prep_of_organisation_lexicon det_organisation_the prep_to_reflecting_knowledge dobj_reflecting_organisation vmod_dictionary_reflecting amod_dictionary_network-based amod_dictionary_digital det_dictionary_a prep_of_usage_dictionary det_usage_the prep_on_research_usage det_research_any cop_research_been neg_research_not aux_research_has expl_research_there advcl_research_are dep_Rapp_2002 dep_Landauer_Rapp appos_Landauer_1998 dep_Landauer_al. nn_Landauer_et amod_experiments_psycholinguistic nn_norms_association nn_norms_word prep_from_measures_experiments prep_with_measures_norms nn_measures_association amod_measures_computed advmod_computed_statistically det_results_the prep_from_compare_measures dobj_compare_results nsubj_compare_that appos_studies_Landauer rcmod_studies_compare det_studies_some nsubj_are_studies expl_are_there mark_are_though advmod_are_Even
W04-2117	C02-1007	o	There are several other approaches such as Ji and Ploux -LRB- 2003 -RRB- and the already mentioned Rapp -LRB- 2002 -RRB-	appos_Rapp_2002 amod_Rapp_mentioned det_Rapp_the advmod_mentioned_already appos_Ploux_2003 conj_and_Ji_Rapp conj_and_Ji_Ploux prep_such_as_approaches_Rapp prep_such_as_approaches_Ploux prep_such_as_approaches_Ji amod_approaches_other amod_approaches_several nsubj_are_approaches expl_are_There
C08-1040	C04-1162	o	For example it has been used to measure centrality in hyperlinked web pages networks -LRB- Brin and Page 1998 Kleinberg 1998 -RRB- lexical networks -LRB- Erkan and Radev 2004 Mihalcea and Tarau 2004 Kurland and Lee 2005 Kurland and Lee 2006 -RRB- and semantic networks -LRB- Mihalcea et al. 2004 -RRB-	amod_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et amod_networks_semantic num_Kurland_2006 conj_and_Kurland_Lee num_Kurland_2005 conj_and_Kurland_Lee dep_Mihalcea_Mihalcea conj_and_Mihalcea_networks conj_and_Mihalcea_Lee conj_and_Mihalcea_Kurland conj_and_Mihalcea_Lee conj_and_Mihalcea_Kurland conj_and_Mihalcea_2004 conj_and_Mihalcea_Tarau conj_and_Erkan_networks conj_and_Erkan_Kurland conj_and_Erkan_Kurland conj_and_Erkan_2004 conj_and_Erkan_Tarau conj_and_Erkan_Mihalcea conj_and_Erkan_2004 conj_and_Erkan_Radev dep_networks_Mihalcea dep_networks_2004 dep_networks_Radev dep_networks_Erkan amod_networks_lexical dep_Kleinberg_1998 conj_and_Brin_networks conj_and_Brin_Kleinberg conj_and_Brin_1998 conj_and_Brin_Page dep_networks_networks dep_networks_Kleinberg dep_networks_1998 dep_networks_Page dep_networks_Brin nn_networks_pages nn_networks_web amod_networks_hyperlinked prep_in_measure_networks dobj_measure_centrality aux_measure_to xcomp_used_measure auxpass_used_been aux_used_has nsubjpass_used_it prep_for_used_example rcmod_``_used
C08-1040	C04-1162	o	Our method is based on the ones described in -LRB- Erkan and Radev 2004 Mihalcea and Tarau 2004 Fader et al. 2007 -RRB- The objective of this paper is to dynamically rank speakers or participants in a discussion	det_discussion_a conj_or_speakers_participants prep_in_rank_discussion dobj_rank_participants dobj_rank_speakers advmod_rank_dynamically aux_rank_to xcomp_is_rank nsubj_is_Fader nsubj_is_2004 nsubj_is_Tarau nsubj_is_Mihalcea det_paper_this prep_of_objective_paper det_objective_The num_Fader_2007 nn_Fader_al. nn_Fader_et appos_Mihalcea_objective conj_and_Mihalcea_Fader conj_and_Mihalcea_2004 conj_and_Mihalcea_Tarau dep_Erkan_is conj_and_Erkan_2004 conj_and_Erkan_Radev prep_in_described_2004 prep_in_described_Radev prep_in_described_Erkan vmod_ones_described det_ones_the prep_on_based_ones auxpass_based_is nsubjpass_based_method poss_method_Our ccomp_``_based
D07-1069	C04-1162	p	Eigenvector centrality in particular has been successfully applied to many different types of networks including hyperlinked web pages -LRB- Brin and Page 1998 Kleinberg 1998 -RRB- lexical networks -LRB- Erkan and Radev 2004 Mihalcea and Tarau 2004 Kurland and Lee 2005 Kurland and Lee 2006 -RRB- and semantic networks -LRB- Mihalcea et al. 2004 -RRB-	amod_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et amod_networks_semantic num_Kurland_2006 conj_and_Kurland_Lee num_Kurland_2005 conj_and_Kurland_Lee dep_Mihalcea_Mihalcea conj_and_Mihalcea_networks conj_and_Mihalcea_Lee conj_and_Mihalcea_Kurland conj_and_Mihalcea_Lee conj_and_Mihalcea_Kurland conj_and_Mihalcea_2004 conj_and_Mihalcea_Tarau conj_and_Erkan_networks conj_and_Erkan_Kurland conj_and_Erkan_Kurland conj_and_Erkan_2004 conj_and_Erkan_Tarau conj_and_Erkan_Mihalcea conj_and_Erkan_2004 conj_and_Erkan_Radev dep_networks_Mihalcea dep_networks_2004 dep_networks_Radev dep_networks_Erkan amod_networks_lexical dep_Kleinberg_1998 dep_Brin_Kleinberg conj_and_Brin_1998 conj_and_Brin_Page appos_pages_networks appos_pages_1998 appos_pages_Page appos_pages_Brin nn_pages_web amod_pages_hyperlinked prep_including_types_pages prep_of_types_networks amod_types_different amod_types_many prep_to_applied_types advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_centrality prep_in_centrality_particular nn_centrality_Eigenvector
E09-3009	C04-1162	o	Still it is in our next plans and part of our future work to embed in our model some of the interesting WSD approaches like knowledgebased -LRB- Sinha and Mihalcea 2007 Brody et al. 2006 -RRB- corpus-based -LRB- Mihalcea and Csomai 2005 McCarthy et al. 2004 -RRB- or combinations with very high accuracy -LRB- Montoyo et al. 2005 -RRB-	amod_Montoyo_2005 dep_Montoyo_al. nn_Montoyo_et amod_accuracy_high advmod_high_very prep_with_combinations_accuracy num_McCarthy_2004 nn_McCarthy_al. nn_McCarthy_et conj_and_Mihalcea_McCarthy conj_and_Mihalcea_2005 conj_and_Mihalcea_Csomai dep_corpus-based_McCarthy dep_corpus-based_2005 dep_corpus-based_Csomai dep_corpus-based_Mihalcea num_Brody_2006 nn_Brody_al. nn_Brody_et dep_Sinha_Brody dep_Sinha_2007 conj_and_Sinha_Mihalcea conj_or_knowledgebased_combinations conj_or_knowledgebased_corpus-based dep_knowledgebased_Mihalcea dep_knowledgebased_Sinha nn_approaches_WSD amod_approaches_interesting det_approaches_the dep_some_Montoyo prep_like_some_combinations prep_like_some_corpus-based prep_like_some_knowledgebased prep_of_some_approaches poss_model_our dobj_embed_some prep_in_embed_model aux_embed_to amod_work_future poss_work_our prep_of_part_work conj_and_plans_part amod_plans_next poss_plans_our xcomp_is_embed prep_in_is_part prep_in_is_plans nsubj_is_it advmod_is_Still
E09-3009	C04-1162	n	This method was preferred against other related methods like the one introduced in -LRB- Mihalcea et al. 2004 -RRB- since it embeds all the available semantic information existing in WordNet even edges that cross POS thus offering a richer semantic representation	amod_representation_semantic amod_representation_richer det_representation_a dobj_offering_representation advmod_offering_thus vmod_POS_offering nn_POS_cross prep_that_edges_POS advmod_edges_even prep_in_existing_WordNet vmod_information_existing amod_information_semantic amod_information_available det_information_the predet_information_all dep_embeds_edges dobj_embeds_information nsubj_embeds_it mark_embeds_since rcmod_Mihalcea_embeds amod_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et prep_in_introduced_Mihalcea vmod_one_introduced det_one_the amod_methods_related amod_methods_other prep_like_preferred_one prep_against_preferred_methods auxpass_preferred_was nsubjpass_preferred_method det_method_This ccomp_``_preferred
H05-1052	C04-1162	o	417 structure of semantic networks was proposed in -LRB- Mihalcea et al. 2004 -RRB- with a disambiguation accuracy of 50.9 % measured on all the words in the SENSEVAL-2 data set	nn_set_data nn_set_SENSEVAL-2 det_set_the prep_in_words_set det_words_the predet_words_all prep_on_measured_words vmod_%_measured num_%_50.9 prep_of_accuracy_% nn_accuracy_disambiguation det_accuracy_a amod_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et dep_in_Mihalcea prep_with_proposed_accuracy prep_proposed_in auxpass_proposed_was nsubjpass_proposed_structure amod_networks_semantic prep_of_structure_networks num_structure_417
I05-2004	C04-1162	o	Previous approaches include supervised learning -LRB- Hirao et al. 2002 -RRB- -LRB- Teufel and Moens 1997 -RRB- vectorial similarity computed between an initial abstract and sentences in the given document intradocument similarities -LRB- Salton et al. 1997 -RRB- or graph algorithms -LRB- Mihalcea and Tarau 2004 -RRB- -LRB- Erkan and Radev 2004 -RRB- -LRB- Wolf and Gibson 2004 -RRB-	amod_Wolf_2004 conj_and_Wolf_Gibson dep_Erkan_2004 conj_and_Erkan_Radev dep_Mihalcea_2004 conj_and_Mihalcea_Tarau appos_algorithms_Tarau appos_algorithms_Mihalcea nn_algorithms_graph amod_Salton_1997 dep_Salton_al. nn_Salton_et dep_similarities_Salton nn_similarities_intradocument amod_document_given det_document_the appos_abstract_Gibson appos_abstract_Wolf appos_abstract_Radev appos_abstract_Erkan conj_or_abstract_algorithms conj_and_abstract_similarities prep_in_abstract_document conj_and_abstract_sentences amod_abstract_initial det_abstract_an prep_between_computed_algorithms prep_between_computed_similarities prep_between_computed_sentences prep_between_computed_abstract nsubj_computed_similarity amod_similarity_vectorial dep_Teufel_1997 conj_and_Teufel_Moens amod_Hirao_2002 dep_Hirao_al. nn_Hirao_et rcmod_learning_computed appos_learning_Moens appos_learning_Teufel dep_learning_Hirao amod_learning_supervised dobj_include_learning dep_approaches_include amod_approaches_Previous
I05-2004	C04-1162	o	Ranking algorithms such as Kleinbergs HITS algorithm -LRB- Kleinberg 1999 -RRB- or Googles PageRank -LRB- Brin and Page 1998 -RRB- have been traditionally and successfully used in Web-link analysis -LRB- Brin and Page 1998 -RRB- social networks and more recently in text processing applications -LRB- Mihalcea and Tarau 2004 -RRB- -LRB- Mihalcea et al. 2004 -RRB- -LRB- Erkan and Radev 2004 -RRB-	amod_Erkan_2004 conj_and_Erkan_Radev amod_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et dep_Mihalcea_2004 conj_and_Mihalcea_Tarau appos_applications_Tarau appos_applications_Mihalcea nn_applications_processing nn_applications_text prep_in_recently_applications advmod_recently_more amod_networks_social dep_Brin_1998 conj_and_Brin_Page dep_analysis_Page dep_analysis_Brin amod_analysis_Web-link dep_used_Radev dep_used_Erkan dep_used_Mihalcea conj_and_used_recently conj_and_used_networks prep_in_used_analysis advmod_used_successfully advmod_used_traditionally auxpass_used_been aux_used_have nsubjpass_used_algorithms conj_and_traditionally_successfully dep_Brin_1998 conj_and_Brin_Page appos_PageRank_Page appos_PageRank_Brin nn_PageRank_Googles amod_Kleinberg_1999 conj_or_algorithm_PageRank dep_algorithm_Kleinberg nn_algorithm_HITS nn_algorithm_Kleinbergs prep_such_as_algorithms_PageRank prep_such_as_algorithms_algorithm nn_algorithms_Ranking
N06-1027	C04-1162	o	Inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graph-based approaches on keyword selection -LRB- Mihalcea and Tarau 2004 -RRB- text summarization -LRB- Erkan and Radev 2004 Mihalcea 2004 -RRB- word sense disambiguation -LRB- Mihalcea et al. 2004 Mihalcea 2005 -RRB- sentiment analysis -LRB- Pang and Lee 2004 -RRB- and sentence retrieval for question answering -LRB- Otterbacher et al. 2005 -RRB-	amod_Otterbacher_2005 dep_Otterbacher_al. nn_Otterbacher_et dep_answering_Otterbacher dep_question_answering nn_retrieval_sentence dep_Pang_2004 conj_and_Pang_Lee appos_analysis_Lee appos_analysis_Pang nn_analysis_sentiment dep_Mihalcea_2005 dep_Mihalcea_Mihalcea appos_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et appos_disambiguation_Mihalcea nn_disambiguation_sense nn_disambiguation_word dep_Mihalcea_2004 dep_Erkan_Mihalcea conj_and_Erkan_2004 conj_and_Erkan_Radev appos_summarization_2004 appos_summarization_Radev appos_summarization_Erkan nn_summarization_text dep_Mihalcea_2004 conj_and_Mihalcea_Tarau dep_selection_Tarau dep_selection_Mihalcea amod_selection_keyword conj_and_approaches_retrieval conj_and_approaches_analysis conj_and_approaches_disambiguation conj_and_approaches_summarization prep_on_approaches_selection amod_approaches_graph-based prep_for_applied_question dobj_applied_retrieval dobj_applied_analysis dobj_applied_disambiguation dobj_applied_summarization dobj_applied_approaches aux_applied_have nsubj_applied_candidate nn_community_language amod_community_natural det_community_the prep_in_efforts_community nn_efforts_research appos_candidate_efforts amod_candidate_best det_candidate_the ccomp_select_applied conj_and_rank_select advmod_rank_collectively aux_rank_to vmod_algorithms_select vmod_algorithms_rank dobj_based_algorithms vmod_idea_based prep_of_idea_graph det_idea_the prep_by_Inspired_idea ccomp_``_Inspired
P04-3020	C04-1162	o	Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases to extractive summarization and word sense disambiguation -LRB- Mihalcea et al. 2004 -RRB-	amod_Mihalcea_2004 dep_Mihalcea_al. nn_Mihalcea_et nn_disambiguation_sense nn_disambiguation_word conj_and_summarization_disambiguation amod_summarization_extractive prep_of_extraction_keyphrases amod_extraction_automated prep_from_ranging_extraction vmod_tasks_ranging dep_applied_Mihalcea prep_to_applied_disambiguation prep_to_applied_summarization prep_to_applied_tasks auxpass_applied_be aux_applied_can nsubjpass_applied_methods amod_methods_ranking amod_methods_text-oriented amod_methods_Such
W06-3811	C04-1162	o	Using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation -LRB- Veronis and Ide 1990 H.Kozima and Furugori 1993 Niwa and Nitta 1994 -RRB- before losing ground to statistical approaches even though -LRB- Gaume et al. 2004 Mihalcea et al. 2004 -RRB- tried a revival of such methods	amod_methods_such prep_of_revival_methods det_revival_a dobj_tried_revival nsubj_tried_Gaume mark_tried_though advmod_tried_even num_Mihalcea_2004 nn_Mihalcea_al. nn_Mihalcea_et dep_Gaume_Mihalcea dep_Gaume_2004 dep_Gaume_al. nn_Gaume_et amod_approaches_statistical advcl_losing_tried prep_to_losing_approaches dobj_losing_ground dep_H.Kozima_1994 conj_and_H.Kozima_Nitta conj_and_H.Kozima_Niwa conj_and_H.Kozima_1993 conj_and_H.Kozima_Furugori dep_Veronis_Nitta dep_Veronis_Niwa dep_Veronis_1993 dep_Veronis_Furugori dep_Veronis_H.Kozima conj_and_Veronis_1990 conj_and_Veronis_Ide appos_disambiguation_1990 appos_disambiguation_Ide appos_disambiguation_Veronis nn_disambiguation_sense nn_disambiguation_word prepc_before_popular_losing prep_for_popular_disambiguation advmod_popular_quite cop_popular_been aux_popular_has nsubj_popular_network mark_popular_as conj_or_items_senses amod_items_lexical prep_of_network_senses prep_of_network_items advcl_Using_popular dobj_Using_dictionaries ccomp_``_Using
D09-1114	C08-1005	o	Although various approaches to SMT system combination have been explored including enhanced combination model structure -LRB- Rosti et al. 2007 -RRB- better word alignment between translations -LRB- Ayan et al. 2008 He et al. 2008 -RRB- and improved confusion network construction -LRB- Rosti et al. 2008 -RRB- most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way	amod_way_principled det_way_a det_ensemble_the prep_in_obtain_way dobj_obtain_ensemble aux_obtain_to advmod_obtain_how prepc_of_issue_obtain det_issue_the dobj_tackle_issue neg_tackle_not aux_tackle_did nsubj_tackle_work prep_at_models_hand conj_and_models_paradigms amod_models_different prep_on_based_paradigms prep_on_based_models nn_systems_SMT prep_of_ensemble_systems det_ensemble_the conj_and_used_tackle vmod_used_based dobj_used_ensemble advmod_used_simply nsubj_used_work dep_used_construction dep_used_He amod_work_previous amod_work_most amod_Rosti_2008 dep_Rosti_al. nn_Rosti_et dep_construction_Rosti nn_construction_network nn_construction_confusion amod_construction_improved dep_al._2008 nn_al._et conj_and_He_construction dep_He_al. parataxis_Ayan_tackle parataxis_Ayan_used appos_Ayan_2008 dep_Ayan_al. nn_Ayan_et dep_translations_Ayan prep_between_alignment_translations nn_alignment_word amod_alignment_better amod_Rosti_2007 dep_Rosti_al. nn_Rosti_et dep_structure_Rosti nn_structure_model nn_structure_combination amod_structure_enhanced dep_,_alignment prep_including_,_structure auxpass_explored_been aux_explored_have nsubjpass_explored_approaches mark_explored_Although nn_combination_system nn_combination_SMT prep_to_approaches_combination amod_approaches_various advcl_``_explored
P09-1066	C08-1005	o	Most of the work focused on seeking better word alignment for consensus-based confusion network decoding -LRB- Matusov et al. 2006 -RRB- or word-level system combination -LRB- He et al. 2008 Ayan et al. 2008 -RRB-	appos_al._2008 nn_al._et nn_al._Ayan dep_al._al. dep_al._2008 nn_al._et nsubj_al._He dep_combination_al. nn_combination_system amod_combination_word-level amod_Matusov_2006 dep_Matusov_al. nn_Matusov_et nn_decoding_network nn_decoding_confusion amod_decoding_consensus-based conj_or_alignment_combination dep_alignment_Matusov prep_for_alignment_decoding nn_alignment_word amod_alignment_better dobj_seeking_combination dobj_seeking_alignment prepc_on_focused_seeking nsubj_focused_Most det_work_the prep_of_Most_work
D09-1073	C08-1027	o	However one of the major limitations of these advances is the structured syntactic knowledge which is important to global reordering -LRB- Li et al. 2007 Elming 2008 -RRB- has not been well exploited	advmod_exploited_well auxpass_exploited_been neg_exploited_not aux_exploited_has nsubjpass_exploited_knowledge amod_Elming_2008 dep_Li_Elming appos_Li_2007 dep_Li_al. nn_Li_et amod_reordering_global dep_important_Li prep_to_important_reordering cop_important_is nsubj_important_which rcmod_knowledge_important nn_knowledge_syntactic amod_knowledge_structured det_knowledge_the cop_knowledge_is nsubj_knowledge_one advmod_knowledge_However det_advances_these prep_of_limitations_advances amod_limitations_major det_limitations_the prep_of_one_limitations
D09-1008	C08-1041	o	-LRB- He et al. 2008 -RRB-	dep_al._2008 nn_al._et nsubj_al._He dep_''_al.
D09-1008	C08-1041	o	Please note that our approach is very different from other approaches to context dependent rule selection such as -LRB- Ittycheriah and Roukos 2007 -RRB- and -LRB- He et al. 2008 -RRB-	dep_al._2008 nn_al._et nsubj_al._He conj_and_Ittycheriah_al. dep_Ittycheriah_2007 conj_and_Ittycheriah_Roukos nn_selection_rule amod_selection_dependent npadvmod_dependent_context prep_such_as_approaches_al. prep_such_as_approaches_Roukos prep_such_as_approaches_Ittycheriah prep_to_approaches_selection amod_approaches_other prep_from_different_approaches advmod_different_very cop_different_is nsubj_different_approach mark_different_that poss_approach_our ccomp_note_different discourse_note_Please
D09-1008	C08-1041	o	Thus we can compute the source dependency LM score in the same way we compute the target side score using a procedure described in -LRB- Shen et al. 2008 -RRB-	amod_Shen_2008 dep_Shen_al. nn_Shen_et dep_in_Shen prep_described_in vmod_procedure_described det_procedure_a dobj_using_procedure nn_score_side nn_score_target det_score_the vmod_compute_using dobj_compute_score nsubj_compute_we amod_way_same det_way_the nn_score_LM nn_score_dependency nn_score_source det_score_the dep_compute_compute prep_in_compute_way dobj_compute_score aux_compute_can nsubj_compute_we advmod_compute_Thus
D09-1008	C08-1041	o	Due to the lack of a good Arabic parser compatible with the Sakhr tokenization that we used on the source side we did not test the source dependency LM for Arabic-to-English MT. When extracting rules with source dependency structures we applied the same well-formedness constraint on the source side as we did on the target side using a procedure described by -LRB- Shen et al. 2008 -RRB-	amod_Shen_2008 dep_Shen_al. nn_Shen_et dep_by_Shen prep_described_by vmod_procedure_described det_procedure_a dobj_using_procedure nn_side_target det_side_the prep_on_did_side nsubj_did_we mark_did_as nn_side_source det_side_the prep_on_constraint_side nn_constraint_well-formedness amod_constraint_same det_constraint_the advcl_applied_did dobj_applied_constraint nsubj_applied_we nn_structures_dependency nn_structures_source prep_with_rules_structures dobj_extracting_rules advmod_extracting_When nn_MT._Arabic-to-English prep_for_LM_MT. nn_LM_dependency nn_LM_source det_LM_the xcomp_test_using parataxis_test_applied advcl_test_extracting dobj_test_LM neg_test_not aux_test_did nsubj_test_we prep_due_to_test_lack nn_side_source det_side_the prep_on_used_side nsubj_used_we mark_used_that nn_tokenization_Sakhr det_tokenization_the ccomp_compatible_used prep_with_compatible_tokenization amod_parser_compatible amod_parser_Arabic amod_parser_good det_parser_a prep_of_lack_parser det_lack_the
D09-1008	C08-1041	o	A remedy is to aggressively limit the feature space e.g. to syntactic labels or a small fraction of the bi-lingual features available as in -LRB- Chiang et al. 2008 Chiang et al. 2009 -RRB- but that reduces the benefit of lexical features	amod_features_lexical prep_of_benefit_features det_benefit_the dobj_reduces_benefit nsubj_reduces_that num_Chiang_2009 nn_Chiang_al. nn_Chiang_et dep_Chiang_Chiang appos_Chiang_2008 dep_Chiang_al. nn_Chiang_et dep_in_Chiang conj_but_as_reduces pcomp_as_in amod_features_available amod_features_bi-lingual det_features_the prep_of_fraction_features amod_fraction_small det_fraction_a conj_or_labels_fraction amod_labels_syntactic pobj_to_fraction pobj_to_labels pcomp_e.g._to nn_space_feature det_space_the dobj_limit_space advmod_limit_aggressively aux_limit_to prep_is_reduces prep_is_as prep_is_e.g. xcomp_is_limit nsubj_is_remedy det_remedy_A ccomp_``_is
D09-1008	C08-1041	o	In -LRB- Post and Gildea 2008 Shen et al. 2008 -RRB- target trees were employed to improve the scoring of translation theories	nn_theories_translation prep_of_scoring_theories vmod_the_scoring dobj_improve_the aux_improve_to xcomp_employed_improve auxpass_employed_were nsubjpass_employed_trees prep_employed_In nn_trees_target num_Shen_2008 nn_Shen_al. nn_Shen_et dep_Post_Shen num_Post_2008 conj_and_Post_Gildea dep_In_Gildea dep_In_Post
D09-1008	C08-1041	o	A few studies -LRB- Carpuat and Wu 2007 Ittycheriah and Roukos 2007 He et al. 2008 Hasan et al. 2008 -RRB- addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence	nn_sentence_input det_sentence_the prep_in_context_sentence poss_context_its prep_on_based_context vmod_span_based nn_span_input det_span_an nn_rules_translation amod_rules_appropriate det_rules_the prep_for_selecting_span dobj_selecting_rules det_defect_this prepc_by_addressed_selecting dobj_addressed_defect nsubj_addressed_studies num_Hasan_2008 nn_Hasan_al. nn_Hasan_et advmod_2008_al. dep_2008_He nn_al._et num_Ittycheriah_2007 conj_and_Ittycheriah_Roukos dep_Carpuat_Hasan conj_and_Carpuat_2008 conj_and_Carpuat_Roukos conj_and_Carpuat_Ittycheriah conj_and_Carpuat_2007 conj_and_Carpuat_Wu dep_studies_2008 dep_studies_Ittycheriah dep_studies_2007 dep_studies_Wu dep_studies_Carpuat amod_studies_few det_studies_A ccomp_``_addressed
D09-1008	C08-1041	o	The other approach is to estimate a single score or likelihood of a translation with rich features for example with the maximum entropy -LRB- MaxEnt -RRB- method as in -LRB- Carpuat and Wu 2007 Ittycheriah and Roukos 2007 He et al. 2008 -RRB-	dep_al._2008 nn_al._et dep_He_al. num_Ittycheriah_2007 conj_and_Ittycheriah_Roukos conj_and_Carpuat_He conj_and_Carpuat_Roukos conj_and_Carpuat_Ittycheriah conj_and_Carpuat_2007 conj_and_Carpuat_Wu pobj_in_He pobj_in_Ittycheriah pobj_in_2007 pobj_in_Wu pobj_in_Carpuat pcomp_as_in prep_method_as nn_method_entropy appos_entropy_MaxEnt nn_entropy_maximum det_entropy_the amod_features_rich prep_with_translation_features det_translation_a prep_of_score_translation conj_or_score_likelihood amod_score_single det_score_a prep_with_estimate_method prep_for_estimate_example dobj_estimate_likelihood dobj_estimate_score aux_estimate_to xcomp_is_estimate nsubj_is_approach amod_approach_other det_approach_The
D09-1008	C08-1041	o	In -LRB- He et al. 2008 -RRB- lexical 72 features were limited on each single side due to the feature space problem	nn_problem_space nn_problem_feature det_problem_the amod_side_single det_side_each prep_due_to_limited_problem prep_on_limited_side auxpass_limited_were nsubjpass_limited_features prep_limited_In num_features_72 amod_features_lexical dep_al._2008 nn_al._et nsubj_al._He dep_In_al. rcmod_``_limited
D09-1008	C08-1041	o	Similar ideas were explored in -LRB- He et al. 2008 -RRB-	num_al._2008 nn_al._et nsubj_al._He dep_in_al. prep_explored_in auxpass_explored_were nsubjpass_explored_ideas amod_ideas_Similar
D09-1008	C08-1041	o	-LRB- Carpuat and Wu 2007 -RRB- and -LRB- He et al. 2008 -RRB- the specific technique we used by means of a context language model is rather different	advmod_different_rather cop_different_is nsubj_different_technique dep_different_al. dep_different_Wu dep_different_Carpuat nn_model_language nn_model_context det_model_a prep_by_means_of_used_model nsubj_used_we rcmod_technique_used amod_technique_specific det_technique_the dep_al._2008 nn_al._et nsubj_al._He conj_and_Carpuat_al. amod_Carpuat_2007 conj_and_Carpuat_Wu
D09-1008	C08-1041	o	73 1.2.2 Baseline System and Experimental Setup We take BBNs HierDec a string-to-dependency decoder as described in -LRB- Shen et al. 2008 -RRB- as our baseline for the following two reasons It provides a strong baseline which ensures the validity of the improvement we would obtain	aux_obtain_would nsubj_obtain_we det_improvement_the prep_of_validity_improvement det_validity_the parataxis_ensures_obtain dobj_ensures_validity nsubj_ensures_which rcmod_baseline_ensures amod_baseline_strong det_baseline_a dobj_provides_baseline nsubj_provides_It num_reasons_two amod_reasons_following det_reasons_the prep_for_baseline_reasons poss_baseline_our dep_Shen_provides prep_as_Shen_baseline amod_Shen_2008 dep_Shen_al. nn_Shen_et prep_in_described_Shen mark_described_as dep_decoder_described nn_decoder_string-to-dependency det_decoder_a appos_HierDec_decoder nn_HierDec_BBNs dobj_take_HierDec nsubj_take_We amod_Setup_Experimental rcmod_System_take conj_and_System_Setup nn_System_Baseline num_System_1.2.2 num_System_73 dep_``_Setup dep_``_System
D09-1008	C08-1041	o	2 Linguistic and Context Features 2.1 Non-terminal Labels In the original string-to-dependency model -LRB- Shen et al. 2008 -RRB- a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side	nn_side_target det_side_the nn_structure_dependency amod_structure_well-formed det_structure_a conj_and_side_structure nn_side_source det_side_the conj_and_words_non-terminals prep_of_string_non-terminals prep_of_string_words det_string_a prep_on_composed_side prep_on_composed_structure prep_on_composed_side prep_of_composed_string auxpass_composed_is nsubjpass_composed_rule nn_rule_translation det_rule_a amod_Shen_2008 dep_Shen_al. nn_Shen_et nn_model_string-to-dependency amod_model_original det_model_the amod_Labels_Non-terminal num_Labels_2.1 ccomp_Features_composed dep_Features_Shen prep_in_Features_model dobj_Features_Labels nsubj_Features_Context nsubj_Features_Linguistic conj_and_Linguistic_Context num_Linguistic_2 ccomp_``_Features
E09-1044	C08-1064	o	Previously published approaches to reducing the rule set include enforcing a minimum span of two words per non-terminal -LRB- Lopez 2008 -RRB- which would reduce our set to 115M rules or a minimum count -LRB- mincount -RRB- threshold -LRB- Zollmann et al. 2008 -RRB- which would reduce our set to 78M -LRB- mincount = 2 -RRB- or 57M -LRB- mincount = 3 -RRB- rules	nn_rules_57M nn_rules_78M dep_=_3 amod_mincount_= dep_57M_mincount dep_=_2 amod_mincount_= conj_or_78M_57M dep_78M_mincount prep_to_set_rules vmod_our_set dobj_reduce_our aux_reduce_would nsubj_reduce_which amod_Zollmann_2008 dep_Zollmann_al. nn_Zollmann_et dep_threshold_Zollmann nn_threshold_count appos_count_mincount nn_count_minimum det_count_a nn_rules_115M prep_to_set_rules vmod_our_set ccomp_reduce_reduce conj_or_reduce_threshold dobj_reduce_our aux_reduce_would nsubj_reduce_which dep_Lopez_2008 appos_non-terminal_Lopez prep_per_words_non-terminal num_words_two prep_of_span_words amod_span_minimum det_span_a ccomp_enforcing_threshold ccomp_enforcing_reduce dobj_enforcing_span parataxis_include_enforcing nsubj_include_approaches vmod_rule_set det_rule_the dobj_reducing_rule prepc_to_approaches_reducing amod_approaches_published advmod_published_Previously
E09-1044	C08-1064	o	Lopez -LRB- 2008 -RRB- explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems	amod_systems_phrase-based prep_over_improvements_systems dobj_explains_improvements nsubj_explains_discontiguity nsubj_explains_reordering mark_explains_whether amod_rules_hierarchical prep_in_inherent_rules nn_discontiguity_phrase det_discontiguity_the dep_reordering_inherent conj_or_reordering_discontiguity amod_reordering_lexical ccomp_explores_explains nsubj_explores_Lopez appos_Lopez_2008
N09-1049	C08-1064	o	Lopez -LRB- 2008 -RRB- explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems	amod_systems_phrase-based prep_over_improvements_systems dobj_explains_improvements nsubj_explains_discontiguity nsubj_explains_reordering mark_explains_whether amod_rules_hierarchical prep_in_inherent_rules nn_discontiguity_phrase det_discontiguity_the dep_reordering_inherent conj_or_reordering_discontiguity amod_reordering_lexical ccomp_explores_explains nsubj_explores_Lopez appos_Lopez_2008
W09-0426	C08-1064	p	First such a system makes use of lexical information when modeling reordering -LRB- Lopez 2008 -RRB- which has previously been shown to be useful in German-to-English translation -LRB- Koehn et al. 2008 -RRB-	amod_Koehn_2008 dep_Koehn_al. nn_Koehn_et amod_translation_German-to-English prep_in_useful_translation cop_useful_be aux_useful_to xcomp_shown_useful auxpass_shown_been advmod_shown_previously aux_shown_has nsubjpass_shown_which dep_Lopez_2008 dep_reordering_Koehn rcmod_reordering_shown appos_reordering_Lopez nn_reordering_modeling dep_when_reordering amod_information_lexical prep_of_use_information dep_makes_when dobj_makes_use nsubj_makes_system advmod_makes_First det_system_a predet_system_such
W09-0437	C08-1064	o	2 Models Search Spaces and Errors A translation model consists of two distinct elements an unweighted ruleset and a parameterization -LRB- Lopez 2008a 2009 -RRB-	dep_Lopez_2009 appos_Lopez_2008a appos_parameterization_Lopez det_parameterization_a conj_and_ruleset_parameterization amod_ruleset_unweighted det_ruleset_an dep_elements_parameterization dep_elements_ruleset amod_elements_distinct num_elements_two prep_of_consists_elements nsubj_consists_model nsubj_consists_Search nsubj_consists_Models nn_model_translation det_model_A nn_model_Errors dobj_Search_Spaces conj_and_Models_model conj_and_Models_Search num_Models_2 ccomp_``_consists
W09-0437	C08-1064	o	Lopez -LRB- 2008b -RRB- gives indirect experimental evidence that this difference affects performance	dobj_affects_performance nsubj_affects_difference mark_affects_that det_difference_this ccomp_evidence_affects amod_evidence_experimental amod_evidence_indirect dobj_gives_evidence nsubj_gives_Lopez appos_Lopez_2008b
W09-0437	C08-1064	o	Our hierarchical system is Hiero -LRB- Chiang 2007 -RRB- modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez -LRB- 2008b -RRB-	appos_Lopez_2008b prep_by_described_Lopez mark_described_as prep_in_phrase_training nn_phrase_source det_phrase_each prep_of_occurrences_phrase prep_of_sample_occurrences amod_sample_small det_sample_a advcl_construct_described prep_from_construct_sample dobj_construct_rules aux_construct_to xcomp_modified_construct dep_Chiang_2007 vmod_Hiero_modified dep_Hiero_Chiang cop_Hiero_is nsubj_Hiero_system amod_system_hierarchical poss_system_Our
E09-1057	C08-1067	o	In a next step chunk information was added by a rule-based language-independent chunker -LRB- Macken et al. 2008 -RRB- that contains distituency rules which implies that chunk boundaries are added between two PoS codes that can not occur in the same constituent	amod_constituent_same det_constituent_the prep_in_occur_constituent neg_occur_not aux_occur_can nsubj_occur_that rcmod_codes_occur nn_codes_PoS num_codes_two prep_between_added_codes auxpass_added_are nsubjpass_added_boundaries mark_added_that nn_boundaries_chunk ccomp_implies_added nsubj_implies_which rcmod_rules_implies nn_rules_distituency dobj_contains_rules nsubj_contains_that amod_Macken_2008 dep_Macken_al. nn_Macken_et rcmod_chunker_contains appos_chunker_Macken amod_chunker_language-independent amod_chunker_rule-based det_chunker_a agent_added_chunker auxpass_added_was nsubjpass_added_information prep_in_added_step nn_information_chunk amod_step_next det_step_a
E09-1057	C08-1067	p	-LRB- Macken et al. 2008 -RRB- showed that the results for French-English were competitive to state-of-the-art alignment systems	nn_systems_alignment amod_systems_state-of-the-art prep_to_competitive_systems cop_competitive_were nsubj_competitive_results mark_competitive_that prep_for_results_French-English det_results_the ccomp_showed_competitive nsubj_showed_Macken dep_Macken_2008 dep_Macken_al. nn_Macken_et
D09-1125	C08-1074	p	Then the same system weights are applied to both IncHMM and Joint Decoding based approaches and the feature weights of them are trained using the max-BLEU training method proposed by Och -LRB- 2003 -RRB- and refined by Moore and Quirk -LRB- 2008 -RRB-	appos_Quirk_2008 conj_and_Moore_Quirk prep_by_refined_Quirk prep_by_refined_Moore appos_Och_2003 conj_and_proposed_refined prep_by_proposed_Och dep_method_refined dep_method_proposed nn_method_training amod_method_max-BLEU det_method_the dobj_using_method xcomp_trained_using auxpass_trained_are prep_of_weights_them nn_weights_feature det_weights_the amod_approaches_based nn_Decoding_Joint conj_and_IncHMM_weights conj_and_IncHMM_approaches conj_and_IncHMM_Decoding preconj_IncHMM_both dep_applied_trained prep_to_applied_weights prep_to_applied_approaches prep_to_applied_Decoding prep_to_applied_IncHMM auxpass_applied_are nsubjpass_applied_weights advmod_applied_Then nn_weights_system amod_weights_same det_weights_the ccomp_``_applied
W09-0439	C08-1074	o	Previouswork eg -LRB- Moore and Quirk 2008 Cer et al. 2008 -RRB- has focusedonimprovingtheperformanceofPowells algorithm	nn_algorithm_focusedonimprovingtheperformanceofPowells dobj_has_algorithm advmod_has_eg nsubj_has_Previouswork num_Cer_2008 nn_Cer_al. nn_Cer_et dep_Moore_Cer num_Moore_2008 conj_and_Moore_Quirk dep_eg_Quirk dep_eg_Moore
W09-0439	C08-1074	o	Moore and Quirk -LRB- 2008 -RRB- share the goal underlying our own research improving rather than replacing Ochs MERT procedure	nn_procedure_MERT nn_procedure_Ochs conj_negcc_improving_procedure conj_negcc_improving_replacing amod_research_own poss_research_our dobj_underlying_research vmod_goal_underlying det_goal_the dep_share_procedure dep_share_replacing dep_share_improving dobj_share_goal nsubj_share_Quirk nsubj_share_Moore appos_Quirk_2008 conj_and_Moore_Quirk
E09-1071	C08-1114	o	One such relational reasoning task is the problem of compound noun interpretation which has received a great deal of attention in recent years -LRB- Girju et al. 2005 Turney 2006 Butnariu and Veale 2008 -RRB-	dep_Butnariu_2008 conj_and_Butnariu_Veale dep_Turney_Veale dep_Turney_Butnariu num_Turney_2006 dep_Girju_Turney appos_Girju_2005 dep_Girju_al. nn_Girju_et amod_years_recent prep_of_deal_attention amod_deal_great det_deal_a dep_received_Girju prep_in_received_years dobj_received_deal aux_received_has nsubj_received_which nn_interpretation_noun nn_interpretation_compound rcmod_problem_received prep_of_problem_interpretation det_problem_the cop_problem_is nsubj_problem_task nn_task_reasoning amod_task_relational amod_task_such num_task_One
E09-1071	C08-1114	o	Turney -LRB- 2008 -RRB- has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass	dep_called_PairClass vmod_classification_called amod_classification_analogical prep_for_algorithm_classification amod_algorithm_SVM-based amod_algorithm_simpler det_algorithm_a dobj_proposed_algorithm advmod_proposed_recently aux_proposed_has nsubj_proposed_Turney appos_Turney_2008
E09-1071	C08-1114	o	Turney -LRB- 2008 -RRB- argues that many NLP tasks can be formulated in terms of analogical reasoning and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests synonym/antonym classification and distinction between semantically similar and semantically associated words	amod_words_associated amod_words_similar advmod_associated_semantically conj_and_similar_associated advmod_similar_semantically prep_between_distinction_words nn_classification_synonym/antonym conj_and_tests_distinction conj_and_tests_classification nn_tests_analogy amod_tests_verbal nn_tests_SAT prep_including_problems_distinction prep_including_problems_classification prep_including_problems_tests prep_of_number_problems det_number_a nn_algorithm_PairClass poss_algorithm_his prep_to_applies_number dobj_applies_algorithm nsubj_applies_he amod_reasoning_analogical prep_of_terms_reasoning prep_in_formulated_terms auxpass_formulated_be aux_formulated_can nsubjpass_formulated_tasks mark_formulated_that nn_tasks_NLP amod_tasks_many conj_and_argues_applies ccomp_argues_formulated nsubj_argues_Turney appos_Turney_2008
E09-1071	C08-1114	o	AnalternativeembeddingisthatusedbyTurney -LRB- 2008 -RRB- in his PairClass system -LRB- see Section 6 -RRB-	num_Section_6 dobj_see_Section nn_system_PairClass poss_system_his dep_AnalternativeembeddingisthatusedbyTurney_see prep_in_AnalternativeembeddingisthatusedbyTurney_system appos_AnalternativeembeddingisthatusedbyTurney_2008
N09-1058	C08-1114	o	Language modeling -LRB- Chen and Goodman 1996 -RRB- noun-clustering -LRB- Ravichandran et al. 2005 -RRB- constructing syntactic rules for SMT -LRB- Galley et al. 2004 -RRB- and finding analogies -LRB- Turney 2008 -RRB- are examples of some of the problems where we need to compute relative frequencies	amod_frequencies_relative dobj_compute_frequencies aux_compute_to xcomp_need_compute nsubj_need_we advmod_need_where rcmod_problems_need det_problems_the prep_of_some_problems prep_of_examples_some cop_examples_are nsubj_examples_finding nsubj_examples_modeling amod_Turney_2008 appos_analogies_Turney dobj_finding_analogies amod_Galley_2004 dep_Galley_al. nn_Galley_et prep_for_rules_SMT amod_rules_syntactic dobj_constructing_rules dep_al._2005 nn_al._et advmod_Ravichandran_al. dep_noun-clustering_Ravichandran dep_Chen_1996 conj_and_Chen_Goodman conj_and_modeling_finding dep_modeling_Galley vmod_modeling_constructing appos_modeling_noun-clustering appos_modeling_Goodman appos_modeling_Chen nn_modeling_Language
N09-1058	C08-1114	o	In NLP community it has been shown that having more data results in better performance -LRB- Ravichandran et al. 2005 Brants et al. 2007 Turney 2008 -RRB-	amod_Turney_2008 dep_Brants_Turney num_Brants_2007 nn_Brants_al. nn_Brants_et dep_Ravichandran_Brants appos_Ravichandran_2005 dep_Ravichandran_al. nn_Ravichandran_et amod_performance_better prep_in_results_performance nn_results_data amod_results_more dep_having_Ravichandran dobj_having_results dep_that_having prep_shown_that auxpass_shown_been aux_shown_has nsubjpass_shown_it prep_in_shown_community nn_community_NLP
W09-0201	C08-1114	o	In Table 6 we report our results together with the state-of-the-art from the ACL wiki5 and the scores of Turney -LRB- 2008 -RRB- -LRB- PairClass -RRB- and from Amac Herdagdelens PairSpace system that was trained on ukWaC	prep_on_trained_ukWaC auxpass_trained_was nsubjpass_trained_that nn_system_PairSpace nn_system_Herdagdelens nn_system_Amac pobj_from_system appos_Turney_PairClass appos_Turney_2008 prep_of_scores_Turney det_scores_the nn_wiki5_ACL det_wiki5_the rcmod_state-of-the-art_trained conj_and_state-of-the-art_from conj_and_state-of-the-art_scores prep_from_state-of-the-art_wiki5 det_state-of-the-art_the prep_together_with_results_from prep_together_with_results_scores prep_together_with_results_state-of-the-art poss_results_our dobj_report_results nsubj_report_we prep_in_report_Table num_Table_6
W09-0201	C08-1114	o	2 Related work Turney -LRB- 2008 -RRB- recently advocated the need for a uniform approach to corpus-based semantic tasks	amod_tasks_semantic amod_tasks_corpus-based prep_to_approach_tasks amod_approach_uniform det_approach_a prep_for_need_approach det_need_the dobj_advocated_need advmod_advocated_recently nsubj_advocated_Turney appos_Turney_2008 nn_Turney_work amod_Turney_Related num_Turney_2
W09-0201	C08-1114	o	Such tasks will require an extension of the current framework of Turney -LRB- 2008 -RRB- beyond evidence from the direct cooccurrence of target word pairs	nn_pairs_word nn_pairs_target prep_of_cooccurrence_pairs amod_cooccurrence_direct det_cooccurrence_the prep_from_evidence_cooccurrence prep_beyond_Turney_evidence appos_Turney_2008 prep_of_framework_Turney amod_framework_current det_framework_the prep_of_extension_framework det_extension_an dobj_require_extension aux_require_will nsubj_require_tasks amod_tasks_Such
W09-0205	C08-1114	o	Turney -LRB- 2008 -RRB- is the first to the best of our knowledge to raise the issue of a unified approach	amod_approach_unified det_approach_a prep_of_issue_approach det_issue_the dobj_raise_issue aux_raise_to poss_knowledge_our prep_of_best_knowledge det_best_the vmod_first_raise prep_to_first_best det_first_the cop_first_is nsubj_first_Turney appos_Turney_2008
W09-0205	C08-1114	o	The algorithm proposed by Turney -LRB- 2008 -RRB- is labeled as Turney-PairClass	prep_as_labeled_Turney-PairClass auxpass_labeled_is nsubjpass_labeled_algorithm appos_Turney_2008 agent_proposed_Turney vmod_algorithm_proposed det_algorithm_The
W09-0205	C08-1114	o	Building on a recent proposal in this direction by Turney -LRB- 2008 -RRB- we propose a generic method of this sort and we test it on a set of unrelated tasks reporting good performance across the board with very little task-specific tweaking	amod_tweaking_task-specific amod_tweaking_little advmod_little_very det_board_the prep_across_performance_board amod_performance_good prep_with_reporting_tweaking dobj_reporting_performance amod_tasks_unrelated prep_of_set_tasks det_set_a prep_on_test_set dobj_test_it nsubj_test_we det_sort_this prep_of_method_sort amod_method_generic det_method_a vmod_propose_reporting conj_and_propose_test nsubj_propose_method nsubj_propose_we dep_propose_Building appos_Turney_2008 det_direction_this prep_by_proposal_Turney prep_in_proposal_direction amod_proposal_recent det_proposal_a prep_on_Building_proposal
W09-0205	C08-1114	o	We adopt a similar approach to the one used in Turney -LRB- 2008 -RRB- and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs	amod_pairs_unknown num_pairs_5 conj_and_instance_pairs nn_instance_training amod_instance_positive num_instance_one prep_with_problem_pairs prep_with_problem_instance nn_problem_classification amod_problem_binary amod_problem_separate det_problem_a det_question_each prep_as_consider_problem dobj_consider_question nsubj_consider_We appos_Turney_2008 prep_in_used_Turney vmod_one_used det_one_the prep_to_approach_one amod_approach_similar det_approach_a conj_and_adopt_consider dobj_adopt_approach nsubj_adopt_We
W09-0419	C08-1115	o	They are part of an effort to better integrate a linguistic rule-based system and the statistical correcting layer also illustrated in -LRB- Ueffing et al. 2008 -RRB-	num_al._2008 nn_al._et amod_al._Ueffing dep_in_al. prep_illustrated_in advmod_illustrated_also nsubj_illustrated_layer amod_layer_correcting amod_layer_statistical det_layer_the amod_system_rule-based amod_system_linguistic det_system_a dobj_integrate_system conj_and_better_illustrated ccomp_better_integrate prep_to_effort_illustrated prep_to_effort_better det_effort_an prep_of_part_effort cop_part_are nsubj_part_They
D09-1079	C08-1125	o	3.5 Domain adaptation in Machine Translation Within MT there has been a variety of approaches dealing with domain adaption -LRB- for example -LRB- Wu et al. 2008 Koehn and Schroeder 2007 -RRB-	amod_Koehn_2007 conj_and_Koehn_Schroeder dep_Wu_Schroeder dep_Wu_Koehn num_Wu_2008 dep_Wu_al. nn_Wu_et nn_adaption_domain dep_dealing_Wu prep_for_dealing_example prep_with_dealing_adaption vmod_approaches_dealing prep_of_variety_approaches det_variety_a cop_variety_been aux_variety_has expl_variety_there dep_variety_adaptation nn_Translation_Machine prep_within_adaptation_MT prep_in_adaptation_Translation nn_adaptation_Domain num_adaptation_3.5
P09-1036	C08-1127	o	This unfortunately significantly jeopardizes performance -LRB- Koehn et al. 2003 Xiong et al. 2008 -RRB- because by integrating syntactic constraint into decoding as a hard constraint it simply prohibits any other useful non-syntactic translations which violate constituent boundaries	amod_boundaries_constituent dobj_violate_boundaries nsubj_violate_which rcmod_translations_violate amod_translations_non-syntactic amod_translations_useful amod_translations_other det_translations_any dobj_prohibits_translations advmod_prohibits_simply nsubj_prohibits_it prepc_by_prohibits_integrating mwe_prohibits_because amod_constraint_hard det_constraint_a prep_as_decoding_constraint nn_constraint_syntactic prepc_into_integrating_decoding dobj_integrating_constraint num_Xiong_2008 nn_Xiong_al. nn_Xiong_et dep_Koehn_Xiong appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et parataxis_jeopardizes_prohibits dep_jeopardizes_Koehn dobj_jeopardizes_performance advmod_jeopardizes_significantly advmod_jeopardizes_unfortunately nsubj_jeopardizes_This ccomp_``_jeopardizes
N09-1061	C08-1136	o	Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar -LRB- SCFG -RRB- -LRB- Uno and Yagiura 2000 Zhang et al. 2008 -RRB-	num_Zhang_2008 nn_Zhang_al. nn_Zhang_et dep_Uno_Zhang amod_Uno_2000 conj_and_Uno_Yagiura appos_Grammar_Yagiura appos_Grammar_Uno appos_Grammar_SCFG nn_Grammar_Context-Free amod_Grammar_Synchronous det_Grammar_a prep_of_size_rules det_size_the prep_in_minimising_Grammar dobj_minimising_size prepc_for_exist_minimising nsubj_exist_algorithms amod_algorithms_Optimal
P09-1088	C08-1136	o	The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs doing away with the separate word alignment step -LRB- Marcu and Wong 2002 Cherry and Lin 2007 Zhang et al. 2008b Blunsom et al. 2008 -RRB-	num_Blunsom_2008 nn_Blunsom_al. nn_Blunsom_et appos_Zhang_2008b dep_Zhang_al. nn_Zhang_et conj_and_Cherry_2007 conj_and_Cherry_Lin dep_Marcu_Blunsom conj_and_Marcu_Zhang conj_and_Marcu_2007 conj_and_Marcu_Lin conj_and_Marcu_Cherry conj_and_Marcu_2002 conj_and_Marcu_Wong appos_step_Zhang appos_step_Cherry appos_step_2002 appos_step_Wong appos_step_Marcu nn_step_alignment nn_step_word amod_step_separate det_step_the prep_with_doing_step prt_doing_away nn_pairs_sentence amod_pairs_aligned nn_transducer_string amod_transducer_phrase-based det_transducer_a prep_from_learn_pairs advmod_learn_directly dobj_learn_transducer aux_learn_to vmod_attempts_learn amod_attempts_various xcomp_littered_doing prep_with_littered_attempts auxpass_littered_is nsubjpass_littered_literature nn_literature_translation nn_literature_machine det_literature_The dep_``_littered
P09-1088	C08-1136	o	The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions -LRB- e.g. constraints drawn from a wordalignment -LRB- Cherry and Lin 2007 Zhang et al. 2008b -RRB- or a grammar fixed a priori -LRB- Blunsom et al. 1f and e are the input and output sentences respectively	nn_sentences_output advmod_input_respectively conj_and_input_sentences det_input_the cop_input_are nsubj_input_e nsubj_input_1f nsubj_input_Blunsom dep_input_constraints dep_input_e.g. conj_and_Blunsom_e conj_and_Blunsom_1f dep_Blunsom_al. nn_Blunsom_et det_priori_a dep_fixed_priori vmod_grammar_fixed det_grammar_a nn_al._et nn_al._Zhang appos_Cherry_2008b dep_Cherry_al. amod_Cherry_2007 conj_and_Cherry_Lin conj_or_wordalignment_grammar dep_wordalignment_Lin dep_wordalignment_Cherry det_wordalignment_a prep_from_drawn_grammar prep_from_drawn_wordalignment vmod_constraints_drawn dep_restrictions_sentences dep_restrictions_input amod_restrictions_arbitrary prep_to_recourse_restrictions nn_units_translation amod_units_possible prep_of_space_units amod_space_infinite det_space_the prep_without_reasons_recourse prep_over_reasons_space nn_reasons_sampler det_reasons_The dep_``_reasons
P09-1088	C08-1136	o	Following the broad shift in the field from finite state transducers to grammar transducers -LRB- Chiang 2007 -RRB- recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference -LRB- Wu 1997 783 Cherry and Lin 2007 Zhang et al. 2008b Blunsom et al. 2008 -RRB-	nn_al._et nn_al._Blunsom dep_Zhang_al. nn_Zhang_et amod_Cherry_2007 conj_and_Cherry_Lin num_Cherry_783 amod_Wu_2008 dep_Wu_al. appos_Wu_2008b dep_Wu_Zhang dep_Wu_Lin dep_Wu_Cherry dep_Wu_1997 appos_inference_Wu nn_inference_time amod_inference_polynomial dobj_permitting_inference vmod_formalisms_permitting nn_formalisms_grammar amod_formalisms_synchronous dobj_used_formalisms aux_used_have dep_used_Following amod_alignment_phrase-based prep_to_approaches_alignment amod_approaches_recent dep_Chiang_2007 appos_transducers_approaches appos_transducers_Chiang nn_transducers_grammar nn_transducers_state amod_transducers_finite det_field_the prep_to_shift_transducers prep_from_shift_transducers prep_in_shift_field amod_shift_broad det_shift_the dobj_Following_shift
P09-1111	C08-1136	o	Other linear time algorithms for rank reduction are found in the literature -LRB- Zhang et al. 2008 -RRB- but they are restricted to the case of synchronous context-free grammars a strict subclass of the LCFRS with f = 2	num_=_2 amod_f_= det_LCFRS_the prep_with_subclass_f prep_of_subclass_LCFRS amod_subclass_strict det_subclass_a ccomp_,_subclass amod_grammars_context-free amod_grammars_synchronous prep_of_case_grammars det_case_the prep_to_restricted_case cop_restricted_are nsubj_restricted_they amod_Zhang_2008 dep_Zhang_al. nn_Zhang_et det_literature_the prep_in_found_literature auxpass_found_are nsubjpass_found_algorithms amod_reduction_rank prep_for_algorithms_reduction conj_but_time_restricted dep_time_Zhang rcmod_time_found amod_time_linear amod_time_Other dep_``_restricted dep_``_time
D09-1108	C08-1138	o	In the SMT research community the second step has been well studied and many methods have been proposed to speed up the decoding process such as node-based or span-based beam search with different pruning strategies -LRB- Liu et al. 2006 Zhang et al. 2008a 2008b -RRB- and cube pruning -LRB- Huang and Chiang 2007 Mi et al. 2008 -RRB-	num_Mi_2008 nn_Mi_al. nn_Mi_et dep_Huang_Mi conj_and_Huang_2007 conj_and_Huang_Chiang dep_pruning_2007 dep_pruning_Chiang dep_pruning_Huang nn_pruning_cube appos_Zhang_2008b appos_Zhang_2008a dep_Zhang_al. nn_Zhang_et dep_Liu_Zhang appos_Liu_2006 dep_Liu_al. nn_Liu_et nn_strategies_pruning amod_strategies_different prep_with_search_strategies nn_search_beam amod_search_span-based amod_search_node-based conj_or_node-based_span-based prep_such_as_process_search nn_process_decoding det_process_the conj_and_speed_pruning dep_speed_Liu dobj_speed_process prt_speed_up aux_speed_to xcomp_proposed_pruning xcomp_proposed_speed auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods amod_methods_many conj_and_studied_proposed advmod_studied_well auxpass_studied_been aux_studied_has nsubjpass_studied_step prep_in_studied_community amod_step_second det_step_the nn_community_research nn_community_SMT det_community_the
D09-1108	C08-1138	o	3.1 Exhaustive search by tree fragments This method generates all possible tree fragments rooted by each node in the source parse tree or forest and then matches all the generated tree fragments against the source parts -LRB- left hand side -RRB- of translation rules to extract the useful rules -LRB- Zhang et al. 2008a -RRB-	appos_Zhang_2008a dep_Zhang_al. nn_Zhang_et amod_rules_useful det_rules_the dobj_extract_rules aux_extract_to nn_rules_translation nn_side_hand amod_side_left prep_of_parts_rules appos_parts_side nn_parts_source det_parts_the prep_against_fragments_parts nn_fragments_tree amod_fragments_generated det_fragments_the predet_fragments_all vmod_matches_extract dobj_matches_fragments advmod_matches_then conj_or_tree_forest dobj_parse_forest dobj_parse_tree vmod_source_parse det_source_the det_node_each agent_rooted_node prep_in_fragments_source vmod_fragments_rooted nn_fragments_tree amod_fragments_possible det_fragments_all dobj_generates_fragments nsubj_generates_method det_method_This nn_fragments_tree dep_search_Zhang conj_and_search_matches rcmod_search_generates prep_by_search_fragments amod_search_Exhaustive num_search_3.1
D09-1108	C08-1138	p	1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation -LRB- SMT -RRB- -LRB- Galley et al. 2004 Liu et al. 2006 2007 Zhang et al. 2007 2008a Mi et al. 2008 Mi and Huang 2008 Zhang et al. 2009 -RRB-	num_Zhang_2009 nn_Zhang_al. nn_Zhang_et num_Huang_2008 num_Mi_2008 nn_Mi_al. nn_Mi_et appos_Zhang_2008a num_Zhang_2007 nn_Zhang_al. nn_Zhang_et conj_and_Liu_Huang conj_and_Liu_Mi conj_and_Liu_Mi conj_and_Liu_Zhang num_Liu_2007 num_Liu_2006 nn_Liu_al. nn_Liu_et dep_Galley_Zhang dep_Galley_Huang dep_Galley_Mi dep_Galley_Mi dep_Galley_Zhang dep_Galley_Liu appos_Galley_2004 dep_Galley_al. nn_Galley_et appos_translation_SMT nn_translation_machine amod_translation_statistical amod_success_great dep_achieved_Galley prep_in_achieved_translation dobj_achieved_success aux_achieved_has nsubj_achieved_method nn_method_translation amod_method_syntax-based amod_method_linguistically-motivated advmod_method_Recently dep_method_Introduction num_Introduction_1
P09-1020	C08-1138	o	4 Training This section discusses how to extract our translation rules given a triple nullnull null null nullnull As we know the traditional tree-to-string rules can be easily extracted from nullnull null null nullnull using the algorithm of Mi and Huang -LRB- 2008 -RRB- 2 We would like 2 Mi and Huang -LRB- 2008 -RRB- extend the tree-based rule extraction algorithm -LRB- Galley et al. 2004 -RRB- to forest-based by introducing non-deterministic mechanism	amod_mechanism_non-deterministic dobj_introducing_mechanism amod_Galley_2004 dep_Galley_al. nn_Galley_et nn_algorithm_extraction nn_algorithm_rule amod_algorithm_tree-based det_algorithm_the prepc_by_extend_introducing prep_to_extend_forest-based dep_extend_Galley dobj_extend_algorithm appos_Huang_2008 conj_and_Mi_Huang num_Mi_2 dep_like_extend dobj_like_Huang dobj_like_Mi aux_like_would nsubj_like_We dep_2_2008 num_Huang_2 conj_and_Mi_Huang prep_of_algorithm_Huang prep_of_algorithm_Mi det_algorithm_the dobj_using_algorithm vmod_nullnull_using parataxis_null_like appos_null_nullnull amod_null_null prep_from_extracted_nullnull advmod_extracted_easily auxpass_extracted_be aux_extracted_can nsubjpass_extracted_rules advcl_extracted_know amod_rules_tree-to-string amod_rules_traditional det_rules_the nsubj_know_we mark_know_As amod_null_null appos_nullnull_nullnull conj_nullnull_null amod_nullnull_triple det_nullnull_a pobj_given_nullnull prep_rules_given nn_rules_translation poss_rules_our dobj_extract_rules aux_extract_to advmod_extract_how dobj_discusses_null parataxis_discusses_extracted ccomp_discusses_extract nsubj_discusses_section det_section_This amod_section_Training num_section_4
P09-1020	C08-1138	p	Among these advances forest-based modeling -LRB- Mi et al. 2008 Mi and Huang 2008 -RRB- and tree sequence-based modeling -LRB- Liu et al. 2007 Zhang et al. 2008a -RRB- are two interesting modeling methods with promising results reported	nsubj_reported_methods amod_results_promising prep_with_methods_results nn_methods_modeling amod_methods_interesting num_methods_two cop_methods_are nsubj_methods_modeling nsubj_methods_modeling prep_among_methods_advances amod_2008a_Zhang dep_Zhang_al. nn_Zhang_et dep_Liu_2008a appos_Liu_2007 dep_Liu_al. nn_Liu_et amod_modeling_sequence-based nn_modeling_tree dep_Mi_2008 conj_and_Mi_Huang dep_Mi_Huang dep_Mi_Mi appos_Mi_2008 dep_Mi_al. nn_Mi_et dep_modeling_Liu conj_and_modeling_modeling dep_modeling_Mi amod_modeling_forest-based det_advances_these
P09-1020	C08-1138	o	Motivated by the fact that non-syntactic phrases make non-trivial contribution to phrase-based SMT the tree sequencebased translation model is proposed -LRB- Liu et al. 2007 Zhang et al. 2008a -RRB- that uses tree sequence as the basic translation unit rather than using single sub-tree as in the STSG	det_STSG_the pobj_in_STSG pcomp_as_in amod_sub-tree_single prep_using_as dobj_using_sub-tree conj_negcc_unit_using nn_unit_translation amod_unit_basic det_unit_the nn_sequence_tree prep_as_uses_using prep_as_uses_unit dobj_uses_sequence nsubj_uses_that amod_2008a_Zhang dep_Zhang_al. nn_Zhang_et rcmod_Liu_uses dep_Liu_2008a appos_Liu_2007 dep_Liu_al. nn_Liu_et dep_proposed_Liu auxpass_proposed_is nsubjpass_proposed_Motivated nn_model_translation amod_model_sequencebased nn_model_tree det_model_the appos_SMT_model amod_SMT_phrase-based prep_to_contribution_SMT amod_contribution_non-trivial dobj_make_contribution nsubj_make_phrases mark_make_that amod_phrases_non-syntactic ccomp_fact_make det_fact_the prep_by_Motivated_fact ccomp_``_proposed
P09-1020	C08-1138	o	-LRB- 2008a -RRB- propose a tree sequence-based tree to tree translation model and Zhang et al.	dep_Zhang_al. nn_Zhang_et conj_and_model_Zhang nn_model_translation nn_model_tree prep_to_tree_Zhang prep_to_tree_model amod_tree_sequence-based nn_tree_tree det_tree_a dobj_propose_tree nsubj_propose_2008a
P09-1020	C08-1138	o	Therefore structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT -LRB- Zhang et al. 2008a Mi et al. 2008 -RRB-	num_Mi_2008 nn_Mi_al. nn_Mi_et dep_Zhang_Mi appos_Zhang_2008a dep_Zhang_al. nn_Zhang_et amod_SMT_syntax-based prep_of_performance_SMT det_performance_the dobj_compromise_performance advmod_compromise_largely aux_compromise_may nsubj_compromise_that rcmod_issues_compromise amod_issues_major det_issues_the dep_two_Zhang prep_of_two_issues cop_two_are nsubj_two_parse nsubj_two_divergence advmod_two_Therefore dobj_parse_errors conj_and_divergence_parse nn_divergence_structure
P09-1020	C08-1138	o	A tree sequence to string rule 174 A tree-sequence to string translation rule in a forest is a triple < L R A > where L is the tree sequence in source language R is the string containing words and variables in target language and A is the alignment between the leaf nodes of L and R This definition is similar to that of -LRB- Liu et al. 2007 Zhang et al. 2008a -RRB- except our treesequence is defined in forest	prep_in_defined_forest auxpass_defined_is nsubjpass_defined_treesequence mark_defined_except poss_treesequence_our dep_al._2008a nn_al._et amod_Zhang_al. appos_Liu_Zhang dep_Liu_2007 dep_Liu_al. nn_Liu_et prep_of_that_Liu advcl_similar_defined prep_to_similar_that cop_similar_is nsubj_similar_definition det_definition_This conj_and_L_R prep_of_nodes_R prep_of_nodes_L nn_nodes_leaf det_nodes_the prep_between_alignment_nodes det_alignment_the cop_alignment_is nsubj_alignment_A nn_language_target prep_in_words_language conj_and_words_variables dobj_containing_variables dobj_containing_words conj_and_string_alignment vmod_string_containing det_string_the cop_string_is nsubj_string_R amod_R_> det_R_A nn_language_source prep_in_sequence_language nn_sequence_tree det_sequence_the cop_sequence_is nsubj_sequence_L advmod_sequence_where rcmod_>_sequence rcmod_L_similar parataxis_L_alignment parataxis_L_string appos_L_R amod_L_< det_L_a cop_L_is nsubj_L_sequence amod_<_triple det_forest_a nn_rule_translation prep_in_string_forest dobj_string_rule aux_string_to vmod_tree-sequence_string det_tree-sequence_A num_tree-sequence_174 dep_rule_tree-sequence dobj_string_rule aux_string_to vmod_sequence_string nn_sequence_tree det_sequence_A
P09-1103	C08-1138	o	To address this issue many syntax-based approaches -LRB- Yamada and Knight 2001 Eisner 2003 Gildea 2003 Ding and Palmer 2005 Quirk et al 2005 Zhang et al 2007 2008a Bod 2007 Liu et al 2006 2007 Hearne and Way 2003 -RRB- tend to integrate more syntactic information to enhance the non-contiguous phrase modeling	nn_modeling_phrase amod_modeling_non-contiguous det_modeling_the dobj_enhance_modeling aux_enhance_to amod_information_syntactic amod_information_more vmod_integrate_enhance dobj_integrate_information aux_integrate_to xcomp_tend_integrate nsubj_tend_approaches advcl_tend_address num_al_2007 num_al_2006 nn_al_et nn_al_Liu appos_Bod_2007 appos_al_2008a num_al_2007 nn_al_et nn_al_Zhang appos_al_2005 nn_al_et nn_al_Quirk dep_Ding_2003 conj_and_Ding_Way conj_and_Ding_Hearne conj_and_Ding_al conj_and_Ding_Bod conj_and_Ding_al conj_and_Ding_al conj_and_Ding_2005 conj_and_Ding_Palmer num_Gildea_2003 num_Eisner_2003 dep_Yamada_Way dep_Yamada_Hearne dep_Yamada_al dep_Yamada_Bod dep_Yamada_al dep_Yamada_al dep_Yamada_2005 dep_Yamada_Palmer dep_Yamada_Ding conj_and_Yamada_Gildea conj_and_Yamada_Eisner conj_and_Yamada_2001 conj_and_Yamada_Knight appos_approaches_Gildea appos_approaches_Eisner appos_approaches_2001 appos_approaches_Knight appos_approaches_Yamada amod_approaches_syntax-based amod_approaches_many det_issue_this dobj_address_issue aux_address_To
P09-1103	C08-1138	o	Nevertheless the generated rules are strictly required to be derived from the contiguous translational equivalences -LRB- Galley et al 2006 Marcu et al 2006 Zhang et al 2007 2008a 2008b Liu et al 2006 2007 -RRB-	amod_al_2007 num_al_2006 nn_al_et nn_al_Liu dep_2008a_al conj_2008a_2008b dep_al_2008a amod_al_2007 nn_al_et nn_al_Zhang appos_al_2006 nn_al_et nn_al_Marcu dep_Galley_al conj_Galley_al appos_Galley_2006 dep_Galley_al nn_Galley_et dep_equivalences_Galley amod_equivalences_translational amod_equivalences_contiguous det_equivalences_the prep_from_derived_equivalences auxpass_derived_be aux_derived_to xcomp_required_derived advmod_required_strictly auxpass_required_are nsubjpass_required_rules advmod_required_Nevertheless amod_rules_generated det_rules_the advcl_``_required
P09-1103	C08-1138	o	2 We illustrate the rule extraction with an example from the tree-to-tree translation model based on tree sequence alignment -LRB- Zhang et al 2008a -RRB- without losing of generality to most syntactic tree based models	amod_models_based nn_models_tree amod_tree_syntactic advmod_syntactic_most prep_to_losing_models prep_of_losing_generality appos_Zhang_2008a dep_Zhang_al nn_Zhang_et dep_alignment_Zhang nn_alignment_sequence nn_alignment_tree prepc_without_based_losing prep_on_based_alignment vmod_model_based nn_model_translation amod_model_tree-to-tree det_model_the prep_from_example_model det_example_an nn_extraction_rule det_extraction_the prep_with_illustrate_example dobj_illustrate_extraction nsubj_illustrate_We rcmod_2_illustrate ccomp_``_2
P09-1103	C08-1138	o	The proposed synchronous grammar is able to cover the previous proposed grammar based on tree -LRB- STSG Eisner 2003 Zhang et al 2007 -RRB- and tree sequence -LRB- STSSG Zhang et al 2008a -RRB- alignment	nn_alignment_sequence nn_alignment_tree nn_Zhang_al nn_Zhang_et appos_STSSG_2008a appos_STSSG_Zhang dep_sequence_STSSG nn_sequence_tree amod_al_2007 nn_al_et nn_al_Zhang dep_STSG_al appos_STSG_2003 appos_STSG_Eisner conj_and_tree_sequence dep_tree_STSG prep_on_based_alignment vmod_grammar_based amod_grammar_proposed amod_grammar_previous det_grammar_the dobj_cover_grammar aux_cover_to xcomp_able_cover cop_able_is nsubj_able_grammar amod_grammar_synchronous amod_grammar_proposed det_grammar_The
D09-1024	C08-1139	o	Word alignment is also a required first step in other algorithms such as for learning sub-sentential phrase pairs -LRB- Lavie et al. 2008 -RRB- or the generation of parallel treebanks -LRB- Zhechev and Way 2002 -RRB-	dep_Zhechev_2002 conj_and_Zhechev_Way appos_treebanks_Way appos_treebanks_Zhechev amod_treebanks_parallel prep_of_generation_treebanks det_generation_the amod_Lavie_2008 dep_Lavie_al. nn_Lavie_et nn_pairs_phrase amod_pairs_sub-sentential dobj_learning_pairs pcomp_algorithms_learning prepc_as_for_algorithms_for mwe_algorithms_such amod_algorithms_other conj_or_step_generation dep_step_Lavie prep_in_step_algorithms amod_step_first amod_step_required det_step_a advmod_step_also cop_step_is nsubj_step_alignment nn_alignment_Word
E09-1044	C08-1144	o	Previously published approaches to reducing the rule set include enforcing a minimum span of two words per non-terminal -LRB- Lopez 2008 -RRB- which would reduce our set to 115M rules or a minimum count -LRB- mincount -RRB- threshold -LRB- Zollmann et al. 2008 -RRB- which would reduce our set to 78M -LRB- mincount = 2 -RRB- or 57M -LRB- mincount = 3 -RRB- rules	nn_rules_57M nn_rules_78M dep_=_3 amod_mincount_= dep_57M_mincount dep_=_2 amod_mincount_= conj_or_78M_57M dep_78M_mincount prep_to_set_rules vmod_our_set dobj_reduce_our aux_reduce_would nsubj_reduce_which amod_Zollmann_2008 dep_Zollmann_al. nn_Zollmann_et dep_threshold_Zollmann nn_threshold_count appos_count_mincount nn_count_minimum det_count_a nn_rules_115M prep_to_set_rules vmod_our_set ccomp_reduce_reduce conj_or_reduce_threshold dobj_reduce_our aux_reduce_would nsubj_reduce_which dep_Lopez_2008 appos_non-terminal_Lopez prep_per_words_non-terminal num_words_two prep_of_span_words amod_span_minimum det_span_a ccomp_enforcing_threshold ccomp_enforcing_reduce dobj_enforcing_span parataxis_include_enforcing nsubj_include_approaches vmod_rule_set det_rule_the dobj_reducing_rule prepc_to_approaches_reducing amod_approaches_published advmod_published_Previously
E09-1044	C08-1144	o	-LRB- Zollmann et al. 2008 -RRB-	amod_Zollmann_2008 dep_Zollmann_al. nn_Zollmann_et dep_''_Zollmann
E09-1044	C08-1144	o	This is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance -LRB- Shen et al. 2008 Zollmann et al. 2008 -RRB-	num_Zollmann_2008 nn_Zollmann_al. nn_Zollmann_et dep_Shen_Zollmann appos_Shen_2008 dep_Shen_al. nn_Shen_et amod_performance_degraded prep_to_lead_performance nsubj_lead_strategies prep_in_lead_which amod_strategies_filtering amod_strategies_other rcmod_results_lead amod_results_reported amod_results_recent prep_to_contrast_results amod_contrast_direct dep_is_Shen prep_in_is_contrast nsubj_is_This
N09-1049	C08-1144	o	Extensions to Hiero Several authors describe extensions to Hiero to incorporate additional syntactic information -LRB- Zollmann and Venugopal 2006 Zhang and Gildea 2006 Shen et al. 2008 Marton and Resnik 2008 -RRB- or to combine it with discriminative latent models -LRB- Blunsom et al. 2008 -RRB-	amod_Blunsom_2008 dep_Blunsom_al. nn_Blunsom_et amod_models_latent amod_models_discriminative dep_combine_Blunsom prep_with_combine_models dobj_combine_it aux_combine_to dep_Marton_2008 conj_and_Marton_Resnik num_Shen_2008 nn_Shen_al. nn_Shen_et num_Zhang_2006 conj_and_Zhang_Gildea dep_Zollmann_Resnik dep_Zollmann_Marton conj_and_Zollmann_Shen conj_and_Zollmann_Gildea conj_and_Zollmann_Zhang conj_and_Zollmann_2006 conj_and_Zollmann_Venugopal conj_or_information_combine appos_information_Shen appos_information_Zhang appos_information_2006 appos_information_Venugopal appos_information_Zollmann amod_information_syntactic amod_information_additional dobj_incorporate_combine dobj_incorporate_information aux_incorporate_to prep_to_extensions_Hiero vmod_describe_incorporate dobj_describe_extensions nsubj_describe_Extensions amod_authors_Several dobj_Hiero_authors aux_Hiero_to vmod_Extensions_Hiero
E09-1017	C08-1145	p	The fluency models hold promise for actual improvements in machine translation output quality -LRB- Zwarts and Dras 2008 -RRB-	dep_Zwarts_2008 conj_and_Zwarts_Dras appos_quality_Dras appos_quality_Zwarts nn_quality_output nn_quality_translation nn_quality_machine prep_in_improvements_quality amod_improvements_actual prep_for_promise_improvements dobj_hold_promise nsubj_hold_models nn_models_fluency det_models_The
A97-1055	C94-2113	o	-LRB- Dolan 1994 -RRB- and -LRB- Krovetz and Croft 1992 -RRB- claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications	amod_applications_many amod_value_practical prep_for_be_applications prep_of_be_value aux_be_to xcomp_unlikely_be cop_unlikely_are nsubj_unlikely_distinctions mark_unlikely_that amod_distinctions_semantic amod_distinctions_fine-grained ccomp_claim_unlikely nsubj_claim_Krovetz nsubj_claim_Dolan dep_Krovetz_1992 conj_and_Krovetz_Croft conj_and_Dolan_Croft conj_and_Dolan_Krovetz amod_Dolan_1994
D07-1107	C94-2113	o	Much work has gone into methods for measuring synset similarity early work in this direction includes -LRB- Dolan 1994 -RRB- which attempted to discover sense similarities between dictionary senses	nn_senses_dictionary prep_between_similarities_senses nn_similarities_sense dobj_discover_similarities aux_discover_to xcomp_attempted_discover nsubj_attempted_which rcmod_Dolan_attempted dep_Dolan_1994 dep_includes_Dolan nsubj_includes_work det_direction_this prep_in_work_direction amod_work_early nn_similarity_synset dobj_measuring_similarity prepc_for_methods_measuring parataxis_gone_includes prep_into_gone_methods aux_gone_has nsubj_gone_work amod_work_Much
J98-1001	C94-2113	o	Recognizing this Dolan -LRB- 1994 -RRB- proposes a method for ambiguating dictionary senses by combining them to create grosser sense distinctions	nn_distinctions_sense nn_distinctions_grosser dobj_create_distinctions aux_create_to vmod_combining_create dobj_combining_them prepc_by_senses_combining nn_senses_dictionary amod_senses_ambiguating prep_for_method_senses det_method_a dobj_proposes_method nsubj_proposes_this appos_Dolan_1994 appos_this_Dolan ccomp_Recognizing_proposes ccomp_``_Recognizing
J98-1003	C94-2113	o	Various approaches to word sense division have been proposed in the literature on WSD including -LRB- 1 -RRB- sense numbers in every-day dictionaries -LRB- Lesk 1986 Cowie Guthrie and Guthrie 1992 -RRB- -LRB- 2 -RRB- automatic or hand-crafted clusters of dictionary senses -LRB- Dolan 1994 Bruce and Wiebe 1995 Luk * Department of Computer Science National Tsing Hua University Hsinchu 30043 Taiwan ROC	num_Hsinchu_30043 nn_University_Hua nn_University_Tsing nn_University_National nn_Science_Computer appos_Department_ROC appos_Department_Taiwan appos_Department_Hsinchu appos_Department_University prep_of_Department_Science dep_Department_* nn_Department_Luk num_Bruce_1995 conj_and_Bruce_Wiebe dep_Dolan_Department dep_Dolan_Wiebe dep_Dolan_Bruce num_Dolan_1994 dep_senses_Dolan nn_senses_dictionary prep_of_clusters_senses amod_clusters_hand-crafted amod_clusters_automatic conj_or_automatic_hand-crafted dep_2_clusters num_Guthrie_1992 conj_and_Cowie_Guthrie conj_and_Cowie_Guthrie dep_Lesk_Guthrie dep_Lesk_Guthrie dep_Lesk_Cowie num_Lesk_1986 appos_dictionaries_Lesk amod_dictionaries_every-day prep_in_numbers_dictionaries nn_numbers_sense dep_numbers_1 dep_WSD_2 prep_including_WSD_numbers prep_on_literature_WSD det_literature_the prep_in_proposed_literature auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches nn_division_sense nn_division_word prep_to_approaches_division amod_approaches_Various
J98-1003	C94-2113	o	Furthermore as pointed out in Dolan -LRB- 1994 -RRB- the sense division in an MRD is frequently too fine-grained for the purpose of WSD	prep_of_purpose_WSD det_purpose_the prep_for_fine-grained_purpose advmod_fine-grained_too advmod_fine-grained_frequently cop_fine-grained_is nsubj_fine-grained_division advcl_fine-grained_pointed advmod_fine-grained_Furthermore det_MRD_an prep_in_division_MRD nn_division_sense det_division_the appos_Dolan_1994 prep_in_pointed_Dolan prt_pointed_out mark_pointed_as
J98-1003	C94-2113	o	82 Chen and Chang Topical Clustering Dolan -LRB- 1994 -RRB- maintains the position that intersense relations are mostly idiosyncratical thereby making it difficult to characterize them in a general way so as to identify them	dobj_identify_them aux_identify_to amod_way_general det_way_a prepc_as_characterize_identify advmod_characterize_so prep_in_characterize_way dobj_characterize_them aux_characterize_to dep_characterize_difficult nsubj_characterize_it xcomp_making_characterize advmod_making_thereby xcomp_idiosyncratical_making advmod_idiosyncratical_mostly cop_idiosyncratical_are nsubj_idiosyncratical_relations mark_idiosyncratical_that amod_relations_intersense ccomp_position_idiosyncratical det_position_the dobj_maintains_position nsubj_maintains_Dolan nsubj_maintains_Chen appos_Dolan_1994 nn_Dolan_Clustering nn_Dolan_Topical nn_Dolan_Chang conj_and_Chen_Dolan num_Chen_82
J98-1003	C94-2113	o	However they do not elaborate on how the comparisons are done or on how effective the program is Dolan -LRB- 1994 -RRB- describes a heuristic approach to forming unlabeled clusters of closely related senses in an MRD	det_MRD_an prep_in_senses_MRD amod_senses_related advmod_related_closely prep_of_clusters_senses amod_clusters_unlabeled dobj_forming_clusters prepc_to_approach_forming nn_approach_heuristic det_approach_a dobj_describes_approach nsubj_describes_Dolan appos_Dolan_1994 nsubj_is_program dep_is_effective det_program_the advmod_effective_how pcomp_on_is conj_or_done_on auxpass_done_are nsubjpass_done_comparisons advmod_done_how det_comparisons_the parataxis_elaborate_describes prepc_on_elaborate_on prepc_on_elaborate_done neg_elaborate_not aux_elaborate_do nsubj_elaborate_they advmod_elaborate_However
J98-1003	C94-2113	o	As noted in Dolan -LRB- 1994 -RRB- it is possible to run a sense-clustering algorithm on several MRDs to build an integrated lexical database with more complete coverage of word senses	nn_senses_word prep_of_coverage_senses amod_coverage_complete amod_coverage_more nn_database_lexical amod_database_integrated det_database_an prep_with_build_coverage dobj_build_database aux_build_to amod_MRDs_several amod_algorithm_sense-clustering det_algorithm_a vmod_run_build prep_on_run_MRDs dobj_run_algorithm aux_run_to xcomp_possible_run cop_possible_is nsubj_possible_it advcl_possible_noted appos_Dolan_1994 prep_in_noted_Dolan mark_noted_As ccomp_``_possible
J98-1003	C94-2113	o	These relations are then used for various tasks ranging from the interpretation of a noun sequence -LRB- Vanderwende 1994 -RRB- or a prepositional phrase -LRB- Ravin 1990 -RRB- to resolving structural ambiguity -LRB- Jenson and Binot 1987 -RRB- to merging dictionary senses for WSD -LRB- Dolan 1994 -RRB-	num_Dolan_1994 appos_WSD_Dolan nn_senses_dictionary prep_for_merging_WSD dobj_merging_senses num_Binot_1987 conj_and_Jenson_Binot dep_ambiguity_Binot dep_ambiguity_Jenson amod_ambiguity_structural dobj_resolving_ambiguity num_Ravin_1990 appos_phrase_Ravin amod_phrase_prepositional det_phrase_a dep_Vanderwende_1994 conj_or_sequence_phrase dep_sequence_Vanderwende nn_sequence_noun det_sequence_a prep_of_interpretation_phrase prep_of_interpretation_sequence det_interpretation_the prep_from_ranging_interpretation amod_tasks_various prepc_to_used_merging prepc_to_used_resolving vmod_used_ranging prep_for_used_tasks advmod_used_then auxpass_used_are nsubjpass_used_relations det_relations_These ccomp_``_used
P06-1014	C94-2113	o	5 Related Work Dolan -LRB- 1994 -RRB- describes a method for clustering word senses with the use of information provided in the electronic version of LDOCE -LRB- textual definitions semantic relations domain labels etc. -RRB-	nn_labels_domain amod_relations_semantic appos_definitions_etc. conj_definitions_labels conj_definitions_relations amod_definitions_textual nn_definitions_LDOCE prep_of_version_definitions amod_version_electronic det_version_the prep_in_provided_version vmod_information_provided prep_of_use_information det_use_the prep_with_senses_use nn_senses_word nn_senses_clustering prep_for_method_senses det_method_a dobj_describes_method nsubj_describes_Dolan appos_Dolan_1994 nn_Dolan_Work amod_Dolan_Related num_Dolan_5
W00-0103	C94-2113	p	This approach took inspiration from the pioneering work by -LRB- Dolan 1994 -RRB- but it is also fundamentally different because instead of grouping similar senses together the CoreLex approach groups together words according to all of their senses	poss_senses_their prep_of_all_senses pobj_words_all prepc_according_to_words_to dep_groups_words advmod_groups_together nn_groups_approach nn_groups_CoreLex det_groups_the amod_senses_similar dep_grouping_groups advmod_grouping_together dobj_grouping_senses prepc_instead_of_different_grouping mwe_different_because advmod_different_fundamentally advmod_different_also cop_different_is nsubj_different_it conj_but_Dolan_different num_Dolan_1994 amod_work_pioneering det_work_the prep_by_took_different prep_by_took_Dolan prep_from_took_work dobj_took_inspiration nsubj_took_approach det_approach_This
W06-2503	C94-2113	o	There is also work on grouping senses of other inventories using information in the inventory -LRB- Dolan 1994 -RRB- along with information retrieval techniques -LRB- Chen and Chang 1998 -RRB-	amod_Chen_1998 conj_and_Chen_Chang dep_techniques_Chang dep_techniques_Chen nn_techniques_retrieval nn_techniques_information dep_Dolan_1994 det_inventory_the prep_in_information_inventory pobj_using_techniques prepc_along_with_using_with dep_using_Dolan dobj_using_information amod_inventories_other prep_of_senses_inventories amod_senses_grouping xcomp_work_using prep_on_work_senses nsubj_is_work advmod_is_also expl_is_There ccomp_``_is
W96-0305	C94-2113	o	Recently various approaches -LRB- Dolan 1994 Luk 1995 Yarowsky 1992 Dagan et al. 1991 Dagan and Itai 1994 -RRB- to word sense division have been used in WSD research	nn_research_WSD prep_in_used_research auxpass_used_been aux_used_have nsubjpass_used_approaches advmod_used_Recently nn_division_sense nn_division_word dep_Dagan_1994 conj_and_Dagan_Itai dep_al._1991 nn_al._et nn_al._Dagan num_Yarowsky_1992 num_Luk_1995 dep_Dolan_Itai dep_Dolan_Dagan dep_Dolan_al. dep_Dolan_Yarowsky dep_Dolan_Luk num_Dolan_1994 prep_to_approaches_division dep_approaches_Dolan amod_approaches_various
W96-0305	C94-2113	o	Zero derivation Dolan -LRB- 1994 -RRB- pointed out that it is helpful to identify zero-derived noun/verb pairs for such tasks as normalization of the semantics of expressions that are only superficially different	advmod_different_superficially cop_different_are nsubj_different_that advmod_superficially_only rcmod_expressions_different prep_of_semantics_expressions det_semantics_the prep_of_normalization_semantics prep_as_tasks_normalization amod_tasks_such nn_pairs_noun/verb amod_pairs_zero-derived prep_for_identify_tasks dobj_identify_pairs aux_identify_to xcomp_helpful_identify cop_helpful_is nsubj_helpful_it mark_helpful_that ccomp_pointed_helpful prt_pointed_out nsubj_pointed_Dolan appos_Dolan_1994 nn_Dolan_derivation nn_Dolan_Zero
W96-0305	C94-2113	o	Dolan -LRB- 1994 -RRB- described a heuristic approach to forming unlabeled clusters of closely related senses in a MRD	det_MRD_a prep_in_senses_MRD amod_senses_related advmod_related_closely prep_of_clusters_senses amod_clusters_unlabeled dobj_forming_clusters prepc_to_approach_forming nn_approach_heuristic det_approach_a dobj_described_approach nsubj_described_Dolan appos_Dolan_1994
W96-0305	C94-2113	o	Dolan -LRB- 1994 -RRB- observed that sense division in MRD is frequently too free for the purpose of WSD	prep_of_purpose_WSD det_purpose_the prep_for_free_purpose advmod_free_too advmod_free_frequently cop_free_is nsubj_free_division mark_free_that prep_in_division_MRD nn_division_sense ccomp_observed_free nsubj_observed_Dolan appos_Dolan_1994
W99-0505	C94-2113	o	Towards a Meaning-Full Comparison of Lexieal Resources Kenneth C Lltkowska CL Research 9208 Gue Road Damascus MD 20872 ken@clres corn http / / www tires tom Abstract The mapping from WordNet to Hector senses m Senseval provides a gold standard against wluch to judge our ability to compare lexlcal resources The gold standard is provided through a word overlap analysis -LRB- with and without a stop list -RRB- for flus mapping achieving at most a 36 percent correct mapping -LRB- inflated by 9 percent from empty assignments -RRB- An alternaUve componenttal analysis of the defimtaons using syntacUc collocatmnal and semantac component and relation identification -LRB- through the use ofdefimng patterns integrated seamlessly mto the parsing thclaonary -RRB- provides an almost 41 percent correct mapping with an additaonal 4 percent by recogmzmg semantic components not used in the Senseval mapping Defimtion sets of the Senseval words from three pubhshed thclaonanes and Dorr 's lextcal knowledge base were added to WordNet and the Hector database to exanune the nature of the mapping process between defimtton sets of more and less sco \ -LSB- ~ e The tecbauques described here consUtute only an maaal implementation of the componenUal analysis approach and suggests that considerable further improvements can be aclueved Introduction The difficulty of companng lemcal resources long a s ~ gnfficant challenge in computauonal hnguistlcs -LRB- Atlans 1991 -RRB- came to the fore in the recent Senseval competatton -LRB- IOlgarnff 1998 -RRB- when some systems that relied heavily on the WordNet -LRB- Miller et al 1990 -RRB- sense inventory were faced with the necessity of using another sense inventory -LRB- Hecto0 A hasty solutaon to the problem was the development of a map between the two inventories but some part ~ cipants expressed concerns that use of flus map may have degraded their performance to an unknown degree Although there were disclaimers about the WordNet-Hector map it nonetheless stands as a usable gold standard for efforts to compare lexical resources Moreover we have a usable baseline -LRB- a word overlap method suggested m -LRB- Lesk 1986 -RRB- -RRB- against which to compare whether we are able to make improvements m the mapping -LRB- since flus method has been shown to perform not as well as expected -LRB- Krovetz 1992 -RRB- -RRB- We first describe the lextcal resources used m the study -LRB- Hector WordNet other dicUonanes and a lex ~ cal knowledge base -RRB- first characterizing them in terms ofpolysemy and the types of leracal mformaUon each contmns -LRB- syntacUc properties and features semantac components and relaUons and collocaUonal properties -RRB- We then present results of perfornung the word overlap analysis of the 18 verbs used m Senseval analyzing the definitions m WordNet and Hector We then expand our analysis to include other dictionaries We describe our methods of analysis particularly the methods of parsing defimtaons and identff -RRB- qng semantic relations -LRB- semrels -RRB- based on defimng patterns essentially takang first steps m Implementing the program described by Atkms and focusmg on the use of meamng full mformataon rather than statistical mformaUon We identify the results that have been achieved thus far and outline further steps that may add more meanmg to the analysis IAll analyses described m this paper were performed automatically using functlonahty incorporated m DIMAP -LRB- Dictionary Maintenance Programs -RRB- -LRB- available for immediate download at -LRB- CL Research 1999a -RRB- -RRB- This includes automatac extracuon of WordNet reformation for the selected words -LRB- mtegrated m DIMAP -RRB- Hector defimtlons were uploaded into DIMAP dicUonanes after use of a conversmn program Defimtlons for other 30 The Lexical Resources Tlus analysis focuses on the mmn verb senses used In Senseval -LRB- not ichoms and phrases -RRB- specifically the followmg AMAZE BAND BET BOTHER BURY CALCULATE CONSUME DERIVE FLOAT HURDLE INVADE PROMISE SACK SANCTION SCRAP SEIZE SHAKE SLIGHT The Hector database used In Senseval consists of a tree of senses each of which contains defimttons syntactic properties example usages and clues -LRB- collocational information about the syntactic and semantic enwronment in wluch a word appears in the spectfic sense -RRB- The WordNet database contmns synonyms -LRB- synsets -RRB- perhaps a defimtton or example usages -LRB- gloss -RRB- some syntactic mformaUon -LRB- verb frames -RRB- hypernyms hyponyms and some other semrels -LRB- ENTAILS CAUSES -RRB- To extend our analysis In order to look at other issues of lexacal resource comparison we have included the defirauons or leracal information from the following additional sources Webster 's 3 ra New International Dictionary -LRB- W3 -RRB- Oxford Advanced l.earners D ~ ctlonary -LRB- OALD -RRB- American Hentage DlcUonary -LRB- AI-ID -RRB- Dorr 's Lexacal Knowledge Base -LRB- Dorr -RRB- We used only the defimuons from W3 OALD and AHD -LRB- which also contmn sample usages and some collocattonal information m the form of usage notes not used at the present tame -RRB- Dorr 's database contains thematic grids wluch characterize the thematic roles of obligatory and optional semanuc components frequently identifying accompanying preposmons -LRB- Olsen et al 1998 -RRB- The following table identities the number of senses and average overall polysemy for each of these resources dictionaries were entered by hand Word amaze band bet bother bury calculate consume denve float hurdle invade pronuse sack sanction scrap seize shake shght Average Polysemy o o o 1 2 4 2 3 1 II 4 4 2 5 5 7 6 9 7 12 6 14 5 5 5 10 9 6 6 8 8 6 5 15 5 16 4 41 14 2 1 4 3 6 2 10 5 5 4 7 4 4 4 6 3 2 2 5 2 3 1 3 3 11 6 21 13 8 8 37 17 1 1 6 3 O 1 2 2 4 1 3 4 4 8 1 3 1 3 1 3 2 10 5 1 0 3 1 3 2 2 0 1 1 1 0 7 1 7 12 I 0 57 37 120 62 34 22 Word Overlap Analysis We first estabhsh a baseline for automatic replication of the lexicographer 's mappmg from WordNet 1 6 to Hector using a s ~ mple word overlap analysis smular to -LRB- Lesk 1986 -RRB- The lextcographer mapped the 66 WordNet senses -LRB- each synset m which a test occurred -RRB- Into 102 Hector senses A total of 86 assignments were made 9 WordNet senses were gwen no assignments 40 recewed exactly one and 17 senses received 2 or 3 asssgnments The WordNet senses contained 348 words -LRB- about half of wluch were common words appeanng on our stop list which contained 165 words mostly preposmons pronouns and conjunctions -RRB- The Hector senses selected m the word overlap analysis contained about 960 words -LRB- all Hector senses contained 1878 words -RRB- We performed a strict word overlap analysts -LRB- with and wsthout a stop hst -RRB- between tile definlUons in WordNet and the Hector senses that is we did not attempt to ldenttfy root forms of Inflected words We took each word m a WordNet sense and determined whether ~ t appeared in a Hector sense we selected a Hector sense based on the highest percentage of words over all Hector senses An 31 empty selection was made ff all the words in the WordNet sense did not appear in any Hector sense only content words were considered when the stop hst was used For example for bet WordNet sense 2 -LRB- stake -LRB- money -RRB- on the outcome of an issue -RRB- mapped into Hector sense 4 -LRB- -LRB- of a person -RRB- to risk -LRB- a sum of money or property -RRB- m thts way -RRB- In this case there was an overlap on two words -LRB- money 039 in the Hector defimtlon -LRB- 0 13 of its 15 words -RRB- without the stop list When the stop list was invoked there was an overlap of only one word -LRB- money 0 07 of the Hector defimtion -RRB- In this case the lexicographer had made three assignments -LRB- Hector senses 2 3 and 4 -RRB- our scoring method treated flus as only 1 out of 3 correct -LRB- not using the relaxed method employed in Senseval of treating flus as completely correct -RRB- Without the stop hst our selections matched the lexicographer 's in 28 of 86 cases -LRB- 32 6 % -RRB- using the stop list we were successful in 31 of 86 cases -LRB- 36 1 % -RRB- The improvement arising when the stop list was used is deceptive where 8 cases were due to empty assignments -LRB- so that only 23 cases 26 7 % were due to matching content words -RRB- Overall only 41 content words were involved in these 23 successes when the stop list was used an average of I 8 content words To summanze the word overlap analysis -LRB- 1 -RRB- despite a ncher set of defimtions in Hector 9 of 66 WordNet senses -LRB- 13 6 % -RRB- could not be assigned -LRB- 2 -RRB- despite the greater detail in Hector senses compared to WordNet senses -LRB- 2 8 times as many words -RRB- only 1 8 content words participated in the assignments and -LRB- 3 -RRB- therefore the defimng vocabulary between these two definition sets seems to be somewhat divergent Although it might appear as if the word overlap analysis does not perform well this is not the case The analysis provides a broad overview of the defimuon companson process between two definmon sets and frames a deeper analysis of the differences Moreover it appears that the accuracy of a gold standard mapping is not crucially important The quality of the mapping may help frame the subsequent analysis more precisely but it seems sufficient that any reasonable mapping will suffice This will be discussed further after presenting the results of the componentlal analysis of the defimtlons 32 Meaning-Full Analysis of Definitions The deeper analysis of the mapping between two defimtion sets relies primarily on two major steps -LRB- 1 -RRB- parsing definitions and using defimng patterns to identify semrels present m the definitions and -LRB- 2 -RRB- relaxing values to these relations by allowing synonymic substitution -LRB- using WordNet -RRB- Thus for example ffwe identify hypernyms or instruments from parsing a defimtion we would say that the defimtions are equal not just ffthe hypernym or instrument is the same word but also Lf the hypernyms or instruments are members of the same synset This approach is based on the finding -LRB- Litkowski 1978 -RRB- that a dictionary induces a semantic network where nodes represent concepts that may be lexicahzed and verbalized in more than one way This finding implies in general the absence of true synonyms and instead the kind of concept embodied in WordNet synsets -LRB- with several lexical items and phraseologles -RRB- A slmdar approach parsing defimtlons and relaxing semrel values was followed in -LRB- Dolan 1994 -RRB- for clnstenng related senses w ~ thin a single dictionary The ideal toward which this approach strives is a complete identification of the meamng components included in a defimtion The meaning components can include syntactic features and charactenstlcs -LRB- including subcategonzation patterns -RRB- semantm components -LRB- realized through identification of semrels -RRB- selectional restrictions and coUocational specifications The first stage of the analysis parses the definitions -LRB- CL Research 1999b Litkowski to appear -RRB- and uses the parse results to extract -LRB- via defining patterns -RRB- semrels Since definitions have many idiosyncrasies -LRB- that do not follow ordinary text -RRB- an important first step in this stage is preprocessmg the definition text to put it into a sentence frame that facilitates the extraction of semrels 2 2Note that the stop hst is not applicable to the definition parsing The parser is a full-scale sentence parser where prepositmns and other words on the stop list are necessary for successful parsing Moreover inclusion of the prepositions is cmcml to the method since they are the bearers of much semrel information The extractmn of semrels examines the parse results a e a tree whose mtermedaate nodes represent non-ternunals and whose leaves represent the lextcal atems that compnse the defimuons where any node may also include annotations such as characterizations of number and tense For all noun or verb defimttons flus includes Identification of the head noun -LRB- with recogmtton of empty heads -RRB- or verb for verbs we signal whether the defimtaon contmned any selecttonal restnctmus -LRB- that as pamcular parenthesazed expressaons -RRB- for the subject and object We then exanune preposattonal phrases In the defimUon and deterrmne whether we have a defining pattern for the preposaUon whach we can use as mdacaUve of a partacular semrel We also identify adverbs m the parse tree and look these up in WordNet to adentffy an adjecuve synset from wluch they are derived -LRB- if one is gwen -RRB- The defimng pattems are actually part of the dictionary used by the parser That is we do not have to develop specafic routines to look for specLfic patterns A defimng pattern ~ s a regular expressaon that arlaculates a syntactac pattern to be matched Thus to recograze a manner semrel we have the foUowmg entry for m m -LRB- dpat -LRB- -LRB- ~ rep0 l -LRB- det -LRB- 0 -RRB- -RRB- adj manner -LRB- 0 -RRB- st -LRB- manner -RRB- -RRB- -RRB- -RRB- This allows us to recognize m as possibly gwmg rise to a manner component where we recogmze m -LRB- the tdde which allows us to specify partacular elements before the m as well -RRB- vath a noun phrase that consasts of 0 or 1 determiner an adjectwe and the lateral manner The ' 0 after the detenmner and the hteral mdacate that these words are not copied into the value for a manner role so that the value to the manner semrel becomes only the adjectwe that as recogmzed The second stage of the analysis uses the populated lexacal database to compare senses and make the selectaons This process follows the general methodology used m Senseval -LRB- Lltkowska to appear -RRB- Specifically m the defimtaon comparison we first exanune exclusaon cntena to rule out specific mappings These criteria include syntacUc properUes -LRB- e g a verb sense that Is only transluve can not map into one that Is only mtransRave -RRB- and collocataonal propertaes -LRB- e g a sense that is used with a parUcle can not map into one that uses a different particle -RRB- At the present tune these are used only rmmmally 33 We next score each viable sense based on rots semrels We increment the score ff the senses have a common hypernym or If a sense 's hypernyms belong to the same synset as the other sense 's hypernyms If a parUcular sense con ~ ns a large number of synonyms -LRB- that as no differentiae on the hypernym -RRB- and they overlap consaderably m the synsets they evoke the score can be increased substanUally Currently we add 5 points for each match 3 We increment the score based on common semrels In tins amtml tmplementaUon we have defimng patterns -LRB- usually qmte nummal -RRB- for recogmzmg Instrument means location purpose source manner has-constituents has-members is-part-of locale and goal 4 We Increment the score by 2 points when we have a common semrel and then by another 5 points when the value Is ~ dentacal or m the same synset After all possable increments to the scores have been made we then select the sense -LRB- s -RRB- w ~ th the lughest score Finally we compare our selecuon with that of the gold standard to assess our mapping over all senses Another way an wluch our methodology follows the Senseval process as that at proceeds incrementally Thus ~ t ms not necessary to have a final perfect parse and mapping rouUne We can make conUnual refinements at any stage of the process and exarmne the overall effect As m Senseval we may make changes to deal wath a particular phenomenon with the result that overall performance dechnes but w ~ th a sounder basis for making subsequent amprovements Results of Componential Analysis The gold standard analysis Involves mapping 66 WordNet senses with 348 words into 102 Hector senses with 1878 words Using the method described above we obtained 35 out of 86 correct 3At the present tame we use WordNet to adentffy semreis We envaslon usmg the full semanlac network created by parsing all a dlcUonary 's defimtaons Thas would include a richer set of semrels than currently included m WordNet 4The defimng patterns are developed by hand We have onlyJust begun this effort so the current set ms somewhat Impoverished mappmgs -LRB- 407 % -RRB- a shght improvement over the 31 correct assignments usmg the stop-last word overlap techmque However as mentioned above the stophst techmque had aclueved 8 of its successes by matclung null assignments Consadered on tlus basins ~ t seems that the componentaal analysis techmque provides substantial ~ mprovement In addition our technique erred on 4 cases by malang assagnments where none were made by the leracographer We suggest that these cases do con ~ n some common elements of meaning and may conceivably not be construed as errors The mapping from WordNet to Hector had relatavely few empty mappings senses for wtuch It was not possable to make an assignment These are the cases where at appears that the chetmnanes do not overlap and thus prowde a tentative mdacataon of where two dictionaries may have different coverage The cases of multiple assignments mchcate the degree ofamblgmty m the mapping The average m both darecUons between Hector and WordNet were donunated by the mabdaty to obtain good dascnnunatton for the word semze Thus tlus method identifies individual words where the & scnnunatwe ablhty needs to be further refined Perhaps more importantly the componentml analysis method exploits consaderably more WordNet Hector mformauon than the word overlap methods Whereas the stop-hst word overlap mapping was based on only 41 content words the componenual ~ approach -LRB- In the selected mappings -RRB- had 228 hits in ~ ~ developing ats scores with only a small number of ~ ~ ~ defining patterns Comparison of Dictionaries tel O ~ 3 0 ' 3 We next exanuned the nature of the mterrelalaons between parrs of chctaonanes w ~ thout use of a gold standard to assess the process of mapping For t/us purpose we mapped m both & recttons between the paars -LCB- WordNet Hector -RCB- -LCB- W3 OALD -RCB- and -LCB- W3 AHD We exanune Dorr 's lexacal knowledge base for the amphcatlons It may have m the mapping process Neither WordNet nor Hector are properly v ~ ewed as chcuonanes since there was no mtenuon to pubhsh them as such WordNet glosses are generally smaller -LRB- 53 words per sense -RRB- compared to Hector -LRB- 184 words per sense -RRB- whach contains many words specff3nng selectmnal restnct ~ ons on the subject and object of the verbs Hector was used primarily for a large-scale sense tagging project The three formal d ~ ctmnanes were subject to rigorous pubhslung and style standards The average number of words per sense were 87 -LRB- OALD -RRB- 7 1 -LRB- AHD -RRB- and 9 9 -LRB- W3 -RRB- w ~ th an average of 3 4 62 and 120 senses per word Each table shows the average number of senses being mapped the average number of assignments m the target dlCtmnary the average number of senses for which no assagnment could be made the average number of mulUple assignments per word and the average score of the assignments that were made WN-Hector 37 47 06 17 119 Hector-WN 57 64 14 22 113 These points are further emphasized m the mapping between W3 and OALD where the disparity between the empty and mulUple assagnments indicate that we are mapping between dictionaries qmte disparate This tends to be the case not only for the enUre set of words but also is evident for individual words where there is a considerable d ~ spanty m the number of senses wtuch then dominate the overall dlspanty Thus for example W3 has 41 defimUons for float while OALD has 10 We tend to be unable to find the specific sense m going from W3 to OALD because at is likely that we have many more specific defimtlons that are not present In the other direction we are hkely to have considerable ambiguity and multiple assignments W3-OALD OALD-W3 W3 OALD 120 78 60 18 99 34 60 07 32 86 34 A Between W3 and AHD there ss less overall daspanty between the defimtaon sets although since W3 Is tmabndged we stall have a relatavely lugh number of senses m W3 that do not appear to be present m AHD Finally It should be noted that the scores for the published dictaonanes tend to be a little lower than for WordNet and Hector Tlus reflects the hkehhood that we have not extracted as much mformataon as we dad m parsing and analyzmg the defimtaon sets used m Senseval W3 AHD oJ ' q O W3-AHD 120 115 40 36 90 AHD-W3 6 2 9 1 1 2 4 1 9 1 We next considered Dorr 's lexacal database We first transformed her theta grids to syntactic spectflcataons -LRB- transttave or lntransmttve -RRB- and identtficataon of semreis -LRB- e g where she Identified an instr component we added such a semrel to the DIMAP sense -RRB- We were able to identify a mappmg from WordNet to her senses for two words -LRB- float and shake -RRB- for wluch Dorr has several entries However smce she has considerably more semanuc components than we are currently able to recogmze we dad not pursue this avenue any further at flus time More important than just mappmg between two words Dorr 's data mdacates the posstbday of further exploitation of a richer set of semanUc components Spectfically as reported m -LRB- Olsen et al 1998 -RRB- m descnbmg procedures for automatically acqumng thematic grids for Mandann Chinese ~ t was noted that verbs that incorporate themaUc elements m their meamng would not allow that element to appear m the complement structure Thus by usmg Dorr 's thematic grids when verb are parsed m defimtaons it ~ s possible to ~ dentffy where partacular semantac components are lexicahzed and which others are transnutted through to the themaUc grid -LRB- complement or subcategonzataon pattern -RRB- for the defimendum The transmiss ~ on of semantic components to the thematic gnd ~ s also reflected overtly m many defimtlons For example shake has one definition to bnng to a specified condatton by or as ffby repeated qmck jerky movements We would thus expect that the thematac grid for this defimtaon should include a goal And deed Dorr 's database has two senses whch reqmre a goal as part of their thematic grid Smularly for many defimtaons m the sample set we ~ dentLfied a source defimng pattern based on the word from frequently the object of the preposmon was the word source ttseff mdacatmg that the subcategonzaUon properties of the defimendum should elude a source component Discussion Wlule the improvement m mapping by using the componentaal analysis techmque -LRB- over the word overlap methods -RRB- is modest we consider these results qmte slgmficant m wew of the very small number of defimng patterns we have Implemented Most of the improvement stems from the word substatuUon pnnclple described earlier -LRB- as ewdenced by the preponderance of 5 point scores -RRB- This techmque also provides a mechamsm for bnngmg back the stop words wz the preposmons wluch are the careers of mformatmn about semrels -LRB- the 2 point scores -RRB- The more general conclusion -LRB- from the word subsutuuon -RRB- is that the success arises from no longer considenng a defimtmn m ~ solation The proper context for a word and its defimtions consists not lUSt of the words that make up the definition but also the total semantac network represented by the dictaonary We have aclueved our results by explomng only a small part of that network We have moved only a few steps to that network beyond the mdawdual words and their definitions We would expect that further expansmn first by the addon of further and ~ mproved semrel defining patterns and second through the identaficataon of more pnmmve semanuc components will add considerably to our abflay to map between lexacal resources We also expect ~ mprovements from consideration of other techniques such as attempts at ontology ahgnment -LRB- Hovy 1998 -RRB- Although tile definition analysis provlded here was performed on definmons with a stogie language the vanous meamng components m m m m m m m m 35 correspond to those used in an Interhngua The use of the exUncuon method -LRB- developed m order to charactenze verbs m another language Clunese -RRB- can frmtfully be applied here as well Two further observaUons about tlus process can be made The first is that rchance on a wellestablished semantic network such as WordNet s not necessary The componenUal analysis method rehes on the local neighborhood of words m the defimUons not on the completeness of the network Indeed the network ~ tsel can be bootstrapped based on the parsing results The method can work vath any semanUc network or ontology and may be used to refine or flesh out the network or ontology The second observation is that it is not necessary to have a well-estabhshed gold standard Any mapping vail do All that Is necessary is for any mvesugator -LRB- lemcographer or not -RRB- to create a judgmental mappmg The methods employed here can then quanufy ttus mapping based on a word overlap analysis and then further examine tt based on the componenaal analysis The componenUal analysis method can then be used to exanune underlying subtleUes and nuances tn the defimUous wluch a lemcographer or analyst can then examine m further detail to assess the mapping Future Work Tlus work has marked the first ume that all the necessary mfrastructure has been combmed tn a rudimentary form Because of its rudimentary status the opportumUes for improvement are quite extensive In addlUon there are many opportumUes for using the techmques descnbed here m further NLP apphcatlons First the techmques described here have immediate apphcabtllty as part of a lexicographer 's workstaUon When defimUons are parsed and semrels are zdenttfied the resulUng data structures can be apphed against a corpus of instances for parUcular words -LRB- as m Senseval -RRB- for improving word-sense disamblguaUon The techmques will also permit comparing an entry vath Itself to deternune the mterrelattonshtps among ~ ts defimUons and of companng the defimUons of two synonyms to deternune the amount of overlap between them on a defimtlon by defimUon bas ~ s Although the analys s here has focused on the parsing of defimUous the development of defimng patterns clearly extends to generalized text parsing since the defimng patterns have been incorporated mto the same chcttonary used for parsing free text the patterns can be used threctly to identify the presence of parUcular semrels among sentenual consUtuents We are working to integrate th ~ s funcUonahty into our word-sense & sambiguaUon techruques -LRB- both the defimng patterns and the semrels -RRB- Even further mt seems that matclung defimng patterns in free text can be used for lextcal acquisition Textual matenal that contains these patterns could concewably be flagged as providing defimUonal matenal which can then be compared to emstmg defimUons to assess whether their use ts cous stent vath these defimUons and ff not at least to flag the inconsistency The tecluuques descnbed here can be apphed directly to the fields of ontology development and analysis of ternunologlcal databases For ontoiogles vath or w ~ thout defimuons the methods employed can be used to compare entries m dai'erent ontologles based pnmanly on the relattous m the ontology both luerarclucal and other For ternunologlcal databases the methods descnbed here can be used to exanune the set of conceptual relaUons lmphed by the defimtmus The defimuon parsing wall facd ~ tate the development of the termmolog ~ ca I network tn the pamcular field covered by the database The componenUal analysts methods result m a richer semantic network that can be used m other apphcattous Thus for example ~ t ts possible to extend the leracal chatmng methods described m -LRB- Green 1997 -RRB- which are based on the semrels used m WordNet The semrels developed with the componenttal analysis method would provide additional detad available for apphcauon of lexlcal cohesion methods In particular addtUonal relattous would penmt some structunng wmthm the individual leracal chams rather than just consldenng each cham as an amorphous set -LRB- Green 1999 -RRB- Finally we are currently investigating the use of the componenUal analysts techmque for mformauon extracUon The techmque identifies -LRB- from defimtlous -RRB- slots that can be used as slots or fields m template generataon Once these slots are identified we wall be attemptmg to extract slot values from Items m large catalog databases -LRB- mdhons of items -RRB- 36 In conclusion it would seem that instead of a paucity of tnformation allovang us to compare lexmal resources by bnngmg m the full semantic network of the lexicon we are overwhelmed with a plethora of data Acknowledgments I would like to thank Bonnie Dorr Chnstiane Fellbaum Steve Green Ed Hovy Ramesh Knshnamurthy Bob Krovetz Thomas Potter Lucy Vanderwende and an anonymous reviewer for their comments on an earlier draft of this paper References Atlans B T S -LRB- 1991 -RRB- Bmldmga lexicon The contribution of lexicography lnternattonal Journal of Lextcography 4 -LRB- 3 -RRB- 167-204 CL Research -LRB- 1999a -RRB- CL Research Demos http / / www clres com/Demo html CL Research -LRB- 1999b -RRB- Dmtlonary Parsing Project http / / www clres com/dpp html Dolan W B -LRB- 1994 5-9 Aug -RRB- Word Sense Amblguation Chistenng Related Senses COLING-94 The 15th International Conference on Computational Linguistics Kyoto Japan Green S J -LRB- 1997 -RRB- Automatically generating hypertext by computing semantic smulanty \ -LSB- Dlss \ -RSB- Toronto Canada Umverstty of Toronto Green S J -LRB- Sjgreen@mn mq edu au -RRB- -LRB- 1999 1 June -RRB- -LRB- Rich semantic networks -RRB- Hovy E -LRB- 1998 May -RRB- Combining and Standardizing Large-Scale Practical Ontologms for Machine Translation and Other Uses Language Resources and Evaluation Conference Granada Spam Kalgarnff A -LRB- 1998 -RRB- SENSEVAL Home Page http / / www itn bton ac uk/events/senseval / Krovetz R -LRB- 1992 June -RRB- Sense-Linking m a Machine Readable Dictionary 30th Annual Meeting of the Association for Computational Lmgu ~ stics Newark Delaware Association for Computational Lmgtustics Lesk M -LRB- 1986 -RRB- Automatic Sense Dlsamblguation Using Machine Readable Dmttonanes How to Tell a Pine Cone from an Ice Cream Cone Proceechngs of SIGDOC Lttkowski K C -LRB- 1978 -RRB- Models of the semantic structure of dictionaries American Journal of Computattonal Lmgutsttcs Atf 81 25-74 Lttkowskl K C -LRB- to appear -RRB- SENSEVAL The CL Research Expenence Computers and the Humamttes Mtller G A Beckwlth R Fellbaum C Gross D & Miller K J -LRB- 1990 -RRB- Introduction to WordNet An on-hne lexical database lnternatwnal Journal of Lexicography 3 -LRB- 4 -RRB- 235-244 Olsen M B Dorr B J & Thomas S C -LRB- 1998 28-31 October -RRB- Enhancmg Automatic Acqulsmon of Thematic Structure in a Large-Scale Lexacon for Mandann Chinese Tlurd Conference of the Association for Machine Translation m the Americas AMTA-98 Langhorne PA	nn_Langhorne_AMTA-98 appos_Americas_PA appos_Americas_Langhorne det_Americas_the dep_m_Americas nn_m_Translation nn_m_Machine det_Association_the prep_of_Conference_Association nn_Conference_Tlurd nn_Conference_Chinese nn_Conference_Mandann prep_for_Lexacon_Conference amod_Lexacon_Large-Scale det_Lexacon_a nn_Structure_Thematic prep_of_Acqulsmon_Structure nn_Acqulsmon_Automatic nn_Acqulsmon_Enhancmg num_October_28-31 dep_1998_October dep_C_Acqulsmon dep_C_1998 nn_C_S nn_J_B nn_B_M prep_in_Olsen_Lexacon appos_Olsen_C conj_and_Olsen_Thomas conj_and_Olsen_J conj_and_Olsen_Dorr conj_and_Olsen_B num_Olsen_235-244 appos_3_4 prep_for_Lexicography_m dep_Lexicography_Thomas dep_Lexicography_J dep_Lexicography_Dorr dep_Lexicography_B dep_Lexicography_Olsen dep_Lexicography_3 prep_of_Journal_Lexicography amod_Journal_lnternatwnal nn_Journal_database nn_Journal_lexical amod_Journal_on-hne det_Journal_An dep_WordNet_Journal prep_to_Introduction_WordNet nn_Introduction_J appos_J_1990 nn_J_K conj_and_Fellbaum_Miller conj_and_Fellbaum_D conj_and_Fellbaum_Gross conj_and_Fellbaum_C nn_A_G nn_Mtller_Humamttes det_Mtller_the appos_Computers_Introduction conj_and_Computers_Miller conj_and_Computers_D conj_and_Computers_Gross conj_and_Computers_C conj_and_Computers_Fellbaum conj_and_Computers_R conj_and_Computers_Beckwlth conj_and_Computers_A conj_and_Computers_Mtller nn_Computers_Expenence nn_Computers_Research nn_Computers_CL det_Computers_The dep_SENSEVAL_Fellbaum dep_SENSEVAL_R dep_SENSEVAL_Beckwlth dep_SENSEVAL_A dep_SENSEVAL_Mtller dep_SENSEVAL_Computers dep_appear_SENSEVAL aux_appear_to vmod_C_appear nn_C_K num_Lttkowskl_25-74 appos_Atf_C appos_Atf_Lttkowskl num_Atf_81 nn_Lmgutsttcs_Computattonal appos_Journal_Atf prep_of_Journal_Lmgutsttcs amod_Journal_American nn_Journal_dictionaries prep_of_structure_Journal amod_structure_semantic det_structure_the prep_of_Models_structure nn_Models_C appos_C_1978 nn_C_K appos_Lttkowski_Models nn_Lttkowski_SIGDOC prep_of_Proceechngs_Lttkowski nn_Proceechngs_Cone nn_Proceechngs_Cream nn_Proceechngs_Ice det_Proceechngs_an nn_Cone_Pine det_Cone_a prep_from_Tell_Proceechngs dobj_Tell_Cone aux_Tell_to advmod_Tell_How amod_Dmttonanes_Readable nn_Dmttonanes_Machine vmod_Using_Tell dobj_Using_Dmttonanes vmod_Dlsamblguation_Using nn_Dlsamblguation_Sense nn_Dlsamblguation_Automatic nn_Dlsamblguation_M dep_M_1986 appos_Lesk_Dlsamblguation nn_Lesk_Lmgtustics nn_Lesk_Computational prep_for_Association_Lesk nn_Association_Delaware appos_Newark_Association dep_stics_Newark nn_stics_~ nn_stics_Lmgu nn_stics_Computational prep_for_Association_stics det_Association_the prep_of_Meeting_Association amod_Meeting_Annual xcomp_30th_Meeting amod_Dictionary_30th dep_Readable_Dictionary amod_Machine_Readable det_Machine_a dep_m_Machine nn_m_Sense-Linking nn_m_R dep_1992_June dep_R_1992 appos_Krovetz_m dep_uk/events/senseval_Krovetz amod_ac_uk/events/senseval dep_bton_ac dep_itn_bton dep_www_itn dep_http_www nsubj_http_Page nn_Page_Home nn_Page_SENSEVAL dep_Page_1998 nn_Page_A nn_Kalgarnff_Spam nn_Granada_Conference nn_Granada_Evaluation conj_and_Resources_http conj_and_Resources_Kalgarnff conj_and_Resources_Granada nn_Resources_Language dobj_Uses_http dobj_Uses_Kalgarnff dobj_Uses_Granada dobj_Uses_Resources nsubj_Uses_Ontologms conj_and_Translation_Other nn_Translation_Machine prep_for_Ontologms_Other prep_for_Ontologms_Translation nn_Ontologms_Practical conj_Large-Scale_Uses dep_Standardizing_Large-Scale nn_Combining_E appos_1998_May dep_E_1998 conj_and_Hovy_Standardizing conj_and_Hovy_Combining dep_networks_Standardizing dep_networks_Combining dep_networks_Hovy amod_networks_semantic amod_networks_Rich nn_networks_J num_June_1 num_June_1999 nn_au_edu nn_au_mq nn_au_Sjgreen@mn appos_J_June appos_J_au nn_J_S nn_Green_Toronto prep_of_Umverstty_Green nn_Umverstty_Canada nn_\_Dlss appos_\_networks appos_\_Umverstty appos_\_Toronto appos_\_\ amod_\_smulanty amod_\_semantic dobj_computing_\ agent_generating_computing dobj_generating_hypertext advmod_generating_Automatically vmod_J_generating appos_J_1997 nn_J_S nn_Green_Japan nn_Kyoto_Linguistics nn_Kyoto_Computational prep_on_Conference_Kyoto nn_Conference_International amod_Conference_15th det_Conference_The conj_COLING-94_J conj_COLING-94_Green conj_COLING-94_Conference dobj_Senses_COLING-94 nsubj_Senses_Related nn_Related_Chistenng nn_Related_Amblguation nn_Related_Sense nn_Related_Word nn_Related_B num_Aug_5-9 num_Aug_1994 appos_B_Aug nn_B_W rcmod_Dolan_Senses nn_Dolan_html dobj_com/dpp_Dolan nsubj_com/dpp_clres nn_clres_www parataxis_http_com/dpp nsubj_http_Project nn_Project_Parsing nn_Project_Dmtlonary nn_Project_Research appos_Research_1999b nn_Research_CL nn_Research_html nn_Research_com/Demo nn_Research_clres nn_Research_www parataxis_http_http nsubj_http_Lextcography mark_http_of nn_Demos_Research nn_Demos_CL nn_Demos_Research appos_Research_1999a nn_Research_CL num_Research_167-204 appos_4_3 dep_Lextcography_Demos dep_Lextcography_4 dep_Journal_http amod_Journal_lnternattonal nn_Journal_lexicography prep_of_contribution_Journal det_contribution_The dep_lexicon_contribution nn_lexicon_Bmldmga nn_lexicon_S appos_S_1991 nn_S_T nn_S_B nn_Atlans_References nn_Atlans_paper det_Atlans_this prep_of_draft_Atlans amod_draft_earlier det_draft_an prep_on_comments_draft poss_comments_their appos_reviewer_lexicon prep_for_reviewer_comments amod_reviewer_anonymous det_reviewer_an nn_Vanderwende_Lucy nn_Potter_Thomas nn_Krovetz_Bob nn_Knshnamurthy_Ramesh nn_Hovy_Ed nn_Green_Steve nn_Fellbaum_Chnstiane conj_and_Dorr_reviewer conj_and_Dorr_Vanderwende conj_and_Dorr_Potter conj_and_Dorr_Krovetz conj_and_Dorr_Knshnamurthy conj_and_Dorr_Hovy conj_and_Dorr_Green conj_and_Dorr_Fellbaum nn_Dorr_Bonnie dobj_thank_reviewer dobj_thank_Vanderwende dobj_thank_Potter dobj_thank_Krovetz dobj_thank_Knshnamurthy dobj_thank_Hovy dobj_thank_Green dobj_thank_Fellbaum dobj_thank_Dorr aux_thank_to xcomp_like_thank aux_like_would nsubj_like_I rcmod_Acknowledgments_like dep_data_Acknowledgments prep_of_plethora_data det_plethora_a prep_with_overwhelmed_plethora auxpass_overwhelmed_are nsubjpass_overwhelmed_we det_lexicon_the rcmod_network_overwhelmed prep_of_network_lexicon amod_network_semantic amod_network_full det_network_the nn_m_bnngmg amod_resources_lexmal dep_compare_network prep_by_compare_m dobj_compare_resources aux_compare_to npadvmod_allovang_us nn_allovang_tnformation prep_of_paucity_allovang det_paucity_a dep_of_compare pobj_of_paucity advmod_of_instead ccomp_,_of dep_seem_that aux_seem_would nsubj_seem_it prep_of_mdhons_items num_databases_36 appos_databases_mdhons nn_databases_catalog amod_databases_large nn_databases_m nn_databases_Items nn_values_slot nn_values_extract dep_attemptmg_seem prep_in_attemptmg_conclusion prep_from_attemptmg_databases prep_to_attemptmg_values cop_attemptmg_be aux_attemptmg_wall nsubj_attemptmg_we auxpass_identified_are nsubjpass_identified_slots mark_identified_Once det_slots_these nn_generataon_template nn_generataon_m nn_generataon_fields conj_or_slots_generataon advcl_used_identified prep_as_used_generataon prep_as_used_slots auxpass_used_be aux_used_can nsubjpass_used_that rcmod_slots_used dep_identifies_attemptmg dep_identifies_slots prep_from_identifies_defimtlous nsubj_identifies_techmque det_techmque_The nn_extracUon_mformauon rcmod_techmque_identifies prep_for_techmque_extracUon dep_analysts_techmque amod_analysts_componenUal det_analysts_the prep_of_use_analysts det_use_the dobj_investigating_use advmod_investigating_currently aux_investigating_are nsubj_investigating_we dep_Green_1999 appos_set_Green amod_set_amorphous det_set_an det_cham_each advmod_consldenng_Finally prep_as_consldenng_set dobj_consldenng_cham advmod_consldenng_just conj_negcc_chams_consldenng amod_chams_leracal amod_chams_individual det_chams_the dep_wmthm_consldenng dep_wmthm_chams nn_wmthm_structunng det_wmthm_some dobj_penmt_wmthm aux_penmt_would nsubj_penmt_relattous vmod_penmt_providing mark_penmt_as amod_relattous_addtUonal nn_methods_cohesion amod_methods_lexlcal prep_of_apphcauon_methods prep_for_available_apphcauon amod_detad_available amod_detad_additional prep_in_provide_particular dobj_provide_detad aux_provide_would nsubj_provide_cous nn_method_analysis amod_method_componenttal det_method_the prep_with_developed_method nsubj_developed_semrels det_semrels_The rcmod_WordNet_developed nn_WordNet_m dobj_used_WordNet vmod_semrels_used det_semrels_the prep_on_based_semrels auxpass_based_are nsubjpass_based_which dep_Green_1997 rcmod_m_based appos_m_Green dobj_described_m nsubj_described_network nn_methods_chatmng amod_methods_leracal det_methods_the dobj_extend_methods aux_extend_to xcomp_possible_extend amod_ts_possible nn_ts_t nn_ts_~ amod_apphcattous_other nn_apphcattous_m prep_for_used_example advmod_used_Thus dobj_used_apphcattous auxpass_used_be aux_used_can nsubjpass_used_that appos_network_ts rcmod_network_used amod_network_semantic amod_network_richer det_network_a rcmod_m_described dobj_result_m nsubj_result_methods nn_methods_analysts amod_methods_componenUal det_methods_The rcmod_database_result det_database_the agent_covered_database vmod_field_covered amod_field_pamcular det_field_the nn_field_tn dobj_network_field nsubj_network_I aux_network_ca nn_~_termmolog det_~_the prep_of_development_~ det_development_the dobj_tate_development nsubj_tate_facd num_facd_~ nn_facd_wall nn_facd_parsing nn_facd_defimuon det_facd_The rcmod_defimtmus_tate det_defimtmus_the agent_lmphed_defimtmus vmod_relaUons_lmphed amod_relaUons_conceptual prep_of_set_relaUons det_set_the dobj_exanune_set aux_exanune_to xcomp_used_exanune auxpass_used_be aux_used_can nsubjpass_used_ontology advmod_descnbed_here vmod_methods_descnbed det_methods_the amod_databases_ternunologlcal prep_for_luerarclucal_databases conj_and_luerarclucal_other preconj_luerarclucal_both appos_ontology_methods amod_ontology_other amod_ontology_luerarclucal det_ontology_the rcmod_m_used amod_m_relattous det_m_the prep_on_based_m advmod_based_pnmanly vmod_ontologles_based amod_ontologles_dai'erent nn_ontologles_m nn_ontologles_entries dobj_compare_ontologles aux_compare_to xcomp_used_compare auxpass_used_be aux_used_can nsubjpass_used_ff nsubjpass_used_defimUons vmod_methods_employed det_methods_the nn_defimuons_thout nn_defimuons_~ nn_defimuons_w conj_or_ontoiogles_defimuons conj_or_ontoiogles_vath amod_databases_ternunologlcal prep_for_analysis_defimuons prep_for_analysis_vath prep_for_analysis_ontoiogles prep_of_analysis_databases amod_development_ontology conj_and_fields_analysis prep_of_fields_development det_fields_the prep_to_apphed_analysis prep_to_apphed_fields advmod_apphed_directly auxpass_apphed_be aux_apphed_can nsubjpass_apphed_tecluuques prep_at_apphed_inconsistency prep_to_apphed_flag mwe_apphed_least neg_apphed_not advmod_descnbed_here vmod_tecluuques_descnbed det_tecluuques_The det_inconsistency_the vmod_ff_apphed appos_defimUons_methods conj_and_defimUons_ff det_defimUons_these dep_vath_network rcmod_vath_used amod_vath_stent conj_cous_vath ccomp_ts_provide nsubj_ts_use mark_ts_whether poss_use_their ccomp_assess_ts aux_assess_to vmod_emstmg_assess dobj_emstmg_defimUons aux_emstmg_to xcomp_compared_emstmg auxpass_compared_be advmod_compared_then aux_compared_can nsubjpass_compared_which rcmod_matenal_compared amod_matenal_defimUonal dobj_providing_matenal advcl_flagged_penmt auxpass_flagged_be advmod_flagged_concewably aux_flagged_could det_patterns_these dobj_contains_patterns nsubj_contains_that rcmod_matenal_contains amod_matenal_Textual nn_matenal_acquisition amod_matenal_lextcal dep_used_flagged prep_for_used_matenal auxpass_used_be aux_used_can nsubjpass_used_patterns mark_used_that amod_text_free prep_in_patterns_text nn_patterns_defimng nn_patterns_matclung ccomp_seems_used nsubj_seems_mt advmod_further_Even det_semrels_the conj_and_patterns_semrels nn_patterns_defimng det_patterns_the preconj_patterns_both nn_techruques_sambiguaUon conj_and_word-sense_techruques poss_word-sense_our parataxis_s_investigating conj_s_seems advmod_s_further dep_s_semrels dep_s_patterns prep_into_s_techruques prep_into_s_word-sense dobj_s_funcUonahty det_~_th dep_integrate_s dobj_integrate_~ aux_integrate_to xcomp_working_integrate aux_working_are nsubj_working_We amod_consUtuents_sentenual amod_semrels_parUcular prep_among_presence_consUtuents prep_of_presence_semrels det_presence_the parataxis_identify_working dobj_identify_presence aux_identify_to xcomp_used_identify advmod_used_threctly auxpass_used_be aux_used_can nsubjpass_used_patterns det_patterns_the amod_text_free amod_text_parsing prep_for_used_text rcmod_chcttonary_used vmod_chcttonary_used amod_chcttonary_same det_chcttonary_the prep_mto_incorporated_chcttonary auxpass_incorporated_been aux_incorporated_have nsubjpass_incorporated_patterns mark_incorporated_since nn_patterns_defimng det_patterns_the nn_parsing_text amod_parsing_generalized advcl_extends_incorporated prep_to_extends_parsing advmod_extends_clearly nsubj_extends_development nn_patterns_defimng prep_of_development_patterns det_development_the prep_of_parsing_defimUous det_parsing_the prep_on_focused_parsing aux_focused_has nsubj_focused_analys mark_focused_Although advmod_s_here appos_analys_s det_analys_the parataxis_s_extends advcl_s_focused nn_~_bas nn_~_defimUon prep_by_defimtlon_~ det_defimtlon_a dep_overlap_s prep_on_overlap_defimtlon prep_between_overlap_them nsubj_overlap_amount prep_amount_of det_amount_the ccomp_deternune_overlap aux_deternune_to num_synonyms_two prep_of_defimUons_synonyms det_defimUons_the dobj_companng_defimUons pcomp_of_companng nn_defimUons_ts nn_defimUons_~ prep_among_mterrelattonshtps_defimUons det_mterrelattonshtps_the xcomp_deternune_deternune conj_and_deternune_of dobj_deternune_mterrelattonshtps aux_deternune_to nn_Itself_vath nn_Itself_entry det_Itself_an vmod_comparing_of vmod_comparing_deternune dobj_comparing_Itself xcomp_permit_comparing advmod_permit_also aux_permit_will nsubj_permit_techmques det_techmques_The amod_disamblguaUon_word-sense ccomp_improving_permit dobj_improving_disamblguaUon nn_Senseval_m prep_as_words_Senseval amod_words_parUcular prep_of_corpus_instances det_corpus_a prepc_for_apphed_improving prep_for_apphed_words prep_against_apphed_corpus auxpass_apphed_be aux_apphed_can nsubjpass_apphed_structures nn_structures_data nn_structures_resulUng det_structures_the auxpass_zdenttfied_are nsubjpass_zdenttfied_semrels conj_and_parsed_zdenttfied auxpass_parsed_are nsubjpass_parsed_defimUons advmod_parsed_When poss_workstaUon_lexicographer det_lexicographer_a prep_of_part_workstaUon amod_apphcabtllty_immediate advcl_have_zdenttfied advcl_have_parsed prep_as_have_part dobj_have_apphcabtllty nsubj_have_techmques advmod_described_here vmod_techmques_described det_techmques_the advmod_apphcatlons_First nn_apphcatlons_NLP amod_apphcatlons_further nn_apphcatlons_m amod_apphcatlons_descnbed advmod_descnbed_here dep_techmques_apphcatlons det_techmques_the dobj_using_techmques rcmod_opportumUes_apphed rcmod_opportumUes_have prepc_for_opportumUes_using amod_opportumUes_many nsubj_are_opportumUes expl_are_there prep_in_extensive_addlUon advmod_extensive_quite cop_extensive_are nsubj_extensive_opportumUes prep_for_opportumUes_improvement det_opportumUes_the amod_status_rudimentary poss_status_its amod_form_rudimentary det_form_a parataxis_combmed_are parataxis_combmed_extensive prep_because_of_combmed_status prep_tn_combmed_form auxpass_combmed_been aux_combmed_has nsubjpass_combmed_mfrastructure mark_combmed_that amod_mfrastructure_necessary det_mfrastructure_the predet_mfrastructure_all dep_ume_combmed amod_ume_first det_ume_the dobj_marked_ume aux_marked_has nn_work_Tlus nn_work_Work amod_work_Future nn_work_mapping det_work_the dobj_assess_work aux_assess_to amod_detail_further nn_detail_m dep_examine_marked vmod_examine_assess dobj_examine_detail advmod_examine_then aux_examine_can nsubj_examine_tn dep_examine_examine conj_or_lemcographer_analyst det_lemcographer_a dobj_wluch_analyst dobj_wluch_lemcographer det_defimUous_the conj_tn_wluch dobj_tn_defimUous conj_and_subtleUes_nuances dobj_underlying_nuances dobj_underlying_subtleUes xcomp_exanune_underlying aux_exanune_to xcomp_used_exanune auxpass_used_be advmod_used_then aux_used_can nsubjpass_used_method nn_method_analysis amod_method_componenUal det_method_The rcmod_analysis_used amod_analysis_componenaal det_analysis_the prep_on_based_analysis vmod_tt_based dobj_examine_tt advmod_examine_further advmod_examine_then conj_and_overlap_examine dobj_overlap_analysis nsubj_overlap_order det_word_a prep_on_based_word nn_mapping_ttus vmod_quanufy_based dobj_quanufy_mapping advmod_quanufy_then aux_quanufy_can nsubj_quanufy_methods advmod_employed_here vmod_methods_employed det_methods_The rcmod_mappmg_quanufy amod_mappmg_judgmental det_mappmg_a dobj_create_mappmg aux_create_to conj_or_lemcographer_not vmod_mvesugator_create appos_mvesugator_not appos_mvesugator_lemcographer det_mvesugator_any prep_for_is_mvesugator nsubj_is_All cop_necessary_Is nsubj_necessary_that rcmod_All_necessary ccomp_do_is nsubj_do_vail nn_vail_mapping det_vail_Any amod_standard_gold amod_standard_well-estabhshed det_standard_a dobj_have_standard aux_have_to dep_necessary_do xcomp_necessary_have neg_necessary_not cop_necessary_is nsubj_necessary_it mark_necessary_that ccomp_is_necessary nsubj_is_rchance mark_is_that amod_observation_second det_observation_The conj_or_network_ontology det_network_the dobj_refine_observation prep_out_refine_ontology prep_out_refine_network conj_or_refine_flesh aux_refine_to xcomp_used_flesh xcomp_used_refine auxpass_used_be aux_used_may conj_and_network_used conj_or_network_ontology nn_network_semanUc det_network_any dep_vath_used dep_vath_ontology dep_vath_network dobj_work_vath aux_work_can nsubj_work_method det_method_The ccomp_results_work nsubj_results_defimUons tmod_results_m dep_results_words mark_results_of det_parsing_the prep_on_based_parsing dep_bootstrapped_based auxpass_bootstrapped_be aux_bootstrapped_can nsubjpass_bootstrapped_tsel nn_tsel_~ nn_tsel_network det_tsel_the det_network_the prep_of_completeness_network det_completeness_the rcmod_defimUons_bootstrapped advmod_defimUons_Indeed prep_on_defimUons_completeness neg_defimUons_not det_defimUons_the vmod_neighborhood_results amod_neighborhood_local det_neighborhood_the prep_on_rehes_neighborhood nn_rehes_method nn_rehes_analysis amod_rehes_componenUal det_rehes_The dobj_necessary_rehes neg_necessary_not acomp_s_necessary prep_such_as_network_WordNet amod_network_semantic amod_network_wellestablished det_network_a vmod_rchance_s prep_on_rchance_network ccomp_is_is nsubj_is_first det_first_The xcomp_made_is auxpass_made_be aux_made_can nsubjpass_made_observaUons advmod_made_as amod_process_tlus prep_about_observaUons_process amod_observaUons_further num_observaUons_Two quantmod_Two_well ccomp_applied_made advmod_applied_here auxpass_applied_be advmod_applied_frmtfully aux_applied_can nsubjpass_applied_verbs appos_language_Clunese det_language_another dep_m_language dep_verbs_m rcmod_charactenze_applied prep_to_order_charactenze nn_order_m amod_order_developed rcmod_method_examine rcmod_method_overlap nn_method_exUncuon det_method_the prep_of_use_method det_use_The det_Interhngua_an dobj_used_use prep_in_used_Interhngua vmod_those_used prep_to_correspond_those nsubj_correspond_m advcl_correspond_performed nsubj_correspond_attempts mark_correspond_as num_m_35 nn_m_m nn_m_m nn_m_m nn_m_m nn_m_m nn_m_m nn_m_m nn_m_components nn_m_meamng amod_m_vanous det_m_the nn_language_stogie det_language_a prep_with_definmons_language prep_on_performed_definmons auxpass_performed_was nsubjpass_performed_analysis mark_performed_Although advmod_provlded_here vmod_analysis_provlded nn_analysis_definition nn_analysis_tile amod_Hovy_1998 dep_ahgnment_Hovy amod_ahgnment_ontology prep_at_attempts_ahgnment mwe_as_such conj_techniques_correspond amod_techniques_other prep_of_consideration_techniques nn_mprovements_~ prep_from_expect_consideration dobj_expect_mprovements advmod_expect_also nsubj_expect_We amod_resources_lexacal ccomp_map_expect prep_between_map_resources aux_map_to poss_abflay_our xcomp_add_map prep_to_add_abflay advmod_add_considerably aux_add_will nsubj_add_second nsubj_add_network advmod_add_also nn_components_semanuc nn_components_pnmmve amod_components_more prep_of_identaficataon_components det_identaficataon_the amod_patterns_defining nn_patterns_semrel amod_semrel_mproved amod_semrel_~ amod_semrel_further conj_and_further_~ prep_of_addon_patterns det_addon_the amod_expansmn_further det_expansmn_that dobj_expect_expansmn aux_expect_would nsubj_expect_We rcmod_definitions_expect poss_definitions_their amod_words_mdawdual det_words_the det_network_that prep_beyond_steps_words prep_to_steps_network amod_steps_few det_steps_a advmod_steps_only dobj_moved_steps aux_moved_have nsubj_moved_We rcmod_network_moved det_network_that conj_and_part_definitions prep_of_part_network amod_part_small det_part_a advmod_part_only dobj_explomng_definitions dobj_explomng_part poss_results_our prepc_by_aclueved_explomng dobj_aclueved_results aux_aclueved_have nsubj_aclueved_We ccomp_dictaonary_aclueved det_dictaonary_the agent_represented_dictaonary prep_through_network_identaficataon conj_and_network_second prep_by_network_addon advmod_network_first vmod_network_represented amod_network_semantac amod_network_total det_network_the det_definition_the dobj_make_definition prt_make_up nsubj_make_that rcmod_words_make det_words_the conj_but_lUSt_add prep_of_lUSt_words advmod_consists_not nsubj_consists_context poss_defimtions_its conj_and_word_defimtions det_word_a prep_for_context_defimtions prep_for_context_word amod_context_proper det_context_The rcmod_solation_consists nn_solation_~ nn_solation_m nn_solation_defimtmn det_solation_a dep_considenng_solation dep_longer_considenng neg_longer_no prep_from_arises_longer nsubj_arises_success mark_arises_that det_success_the ccomp_is_arises nsubj_is_conclusion nn_subsutuuon_word det_subsutuuon_the prep_from_conclusion_subsutuuon amod_conclusion_general amod_conclusion_more det_conclusion_The nn_scores_point num_scores_2 det_scores_the appos_semrels_scores prep_about_mformatmn_semrels rcmod_careers_is prep_of_careers_mformatmn det_careers_the cop_careers_are nsubj_careers_wluch det_preposmons_the dep_words_add dep_words_lUSt rcmod_words_careers appos_words_preposmons appos_words_wz nn_words_stop det_words_the dep_bnngmg_words advmod_bnngmg_back prep_for_mechamsm_bnngmg det_mechamsm_a dobj_provides_mechamsm advmod_provides_also nsubj_provides_techmque det_techmque_This nn_scores_point num_scores_5 prep_of_preponderance_scores det_preponderance_the dep_earlier_provides prep_by_earlier_preponderance prep_as_earlier_ewdenced dep_described_earlier vmod_pnnclple_described nn_pnnclple_substatuUon nn_pnnclple_word det_pnnclple_the prep_from_stems_pnnclple nsubj_stems_Most det_improvement_the prep_of_Most_improvement ccomp_Implemented_stems aux_Implemented_have nsubj_Implemented_we nn_patterns_defimng prep_of_number_patterns amod_number_small det_number_the advmod_small_very prep_of_wew_number nn_wew_m nn_wew_slgmficant dep_qmte_Implemented dobj_qmte_wew nsubj_qmte_results det_results_these ccomp_consider_qmte nsubj_consider_we cop_modest_is nsubj_modest_mdacatmg dobj_overlap_methods prep_over_overlap_word nsubj_overlap_techmque det_word_the nn_techmque_analysis amod_techmque_componentaal det_techmque_the ccomp_using_overlap prepc_by_mapping_using nn_mapping_m nn_mapping_improvement det_mapping_the dep_Wlule_mapping nn_Wlule_Discussion nn_Wlule_component nn_Wlule_source det_Wlule_a dobj_elude_Wlule aux_elude_should nsubj_elude_subcategonzaUon mark_elude_that det_defimendum_the prep_of_properties_defimendum appos_subcategonzaUon_properties det_subcategonzaUon_the ccomp_mdacatmg_elude rcmod_source_consider dep_source_modest dep_source_ttseff dep_source_word det_word_the cop_word_was nsubj_word_object advmod_word_frequently det_preposmon_the prep_of_object_preposmon det_object_the prep_from_word_word det_word_the prep_on_based_source vmod_pattern_based nn_pattern_defimng nn_pattern_source det_pattern_a amod_pattern_dentLfied dobj_~_pattern dep_we_~ nn_set_sample det_set_the nn_set_m nn_set_defimtaons amod_set_many dep_for_we pobj_for_set ccomp_,_for amod_grid_thematic poss_grid_their advmod_part_Smularly prep_of_part_grid pobj_as_part det_goal_a nn_goal_reqmre dobj_whch_goal nsubj_whch_senses num_senses_two ccomp_has_whch nsubj_has_database poss_database_Dorr rcmod_deed_has det_goal_a dep_include_as conj_and_include_deed dobj_include_goal aux_include_should nsubj_include_grid mark_include_that det_defimtaon_this prep_for_grid_defimtaon amod_grid_thematac det_grid_the ccomp_expect_deed ccomp_expect_include advmod_expect_thus aux_expect_would nsubj_expect_We dep_expect_defimtlons nn_movements_jerky nn_movements_qmck amod_movements_repeated amod_movements_ffby pobj_as_movements conj_or_by_as amod_condatton_specified det_condatton_a prep_bnng_as prep_bnng_by prep_to_bnng_condatton aux_bnng_to num_definition_one dobj_has_definition nsubj_has_shake vmod_defimtlons_bnng rcmod_defimtlons_has prep_for_defimtlons_example amod_defimtlons_many nn_defimtlons_m advmod_defimtlons_overtly ccomp_reflected_expect advmod_reflected_also nsubj_reflected_~ nn_s_~ nn_s_gnd amod_s_thematic det_s_the amod_components_semantic prep_to_on_s prep_of_on_components prep_~_on amod_~_transmiss det_~_The rcmod_defimendum_reflected det_defimendum_the nn_pattern_subcategonzataon conj_or_complement_pattern dep_grid_pattern dep_grid_complement nn_grid_themaUc det_grid_the pobj_to_grid pcomp_through_to prep_transnutted_through auxpass_transnutted_are nsubjpass_transnutted_others dobj_transnutted_which prep_for_lexicahzed_defimendum conj_and_lexicahzed_transnutted auxpass_lexicahzed_are nsubjpass_lexicahzed_components advmod_lexicahzed_where nn_components_semantac amod_components_partacular rcmod_dentffy_transnutted rcmod_dentffy_lexicahzed nn_dentffy_~ prep_to_possible_dentffy amod_s_possible dobj_~_s nsubj_~_it dep_defimtaons_~ nn_defimtaons_m amod_defimtaons_parsed cop_defimtaons_are nsubj_defimtaons_verb advmod_defimtaons_when amod_grids_thematic poss_grids_Dorr nn_Dorr_usmg advcl_by_defimtaons pobj_by_grids dep_Thus_by ccomp_''_Thus nn_structure_complement det_structure_the nn_structure_m dobj_appear_structure aux_appear_to vmod_element_appear det_element_that dobj_allow_element neg_allow_not aux_allow_would nsubj_allow_meamng dep_allow_m dep_allow_elements dep_allow_themaUc poss_meamng_their dep_incorporate_allow nsubj_incorporate_that rcmod_verbs_incorporate dep_that_verbs dep_noted_that auxpass_noted_was nsubjpass_noted_m mark_noted_as num_t_~ appos_Chinese_t nn_Chinese_Mandann prep_for_grids_Chinese amod_grids_thematic amod_grids_acqumng advmod_acqumng_automatically prep_for_procedures_grids nn_procedures_descnbmg nn_procedures_m nn_al_et amod_Olsen_1998 appos_Olsen_al conj_m_procedures dep_m_Olsen amod_m_reported advmod_components_Spectfically nn_components_semanUc prep_of_set_components amod_set_richer det_set_a prep_of_exploitation_set amod_exploitation_further prep_of_posstbday_exploitation det_posstbday_the advcl_mdacates_noted dobj_mdacates_posstbday nsubj_mdacates_data poss_data_Dorr dep_words_mdacates num_words_two prep_between_mappmg_words advmod_mappmg_just prep_than_important_mappmg advmod_important_More amod_time_flus amod_further_important prep_at_further_time amod_any_further det_avenue_this dobj_pursue_any dobj_pursue_avenue neg_pursue_not nsubj_pursue_dad dep_pursue_we advcl_pursue_able prep_to_able_recogmze advmod_able_currently cop_able_are nsubj_able_we mark_able_than amod_components_semanuc advmod_components_more advmod_more_considerably dobj_has_components nsubj_has_she dep_smce_pursue rcmod_smce_has advmod_entries_However amod_entries_several conj_has_smce dobj_has_entries nsubj_has_hkehhood nn_Dorr_wluch dep_shake_float cc_float_and dep_words_shake num_words_two poss_senses_her prep_from_mappmg_WordNet det_mappmg_a prep_for_identify_words prep_to_identify_senses dobj_identify_mappmg aux_identify_to xcomp_able_identify cop_able_were nsubj_able_We nn_sense_DIMAP det_sense_the det_semrel_a predet_semrel_such prep_to_added_sense dobj_added_semrel nsubj_added_we nn_component_instr det_component_an dobj_Identified_component nsubj_Identified_she advmod_Identified_where dep_g_added rcmod_g_Identified dep_g_e dep_identtficataon_g prep_of_identtficataon_semreis conj_or_transttave_lntransmttve conj_and_spectflcataons_identtficataon dep_spectflcataons_lntransmttve dep_spectflcataons_transttave amod_spectflcataons_syntactic nn_grids_theta poss_grids_her parataxis_transformed_able prep_to_transformed_identtficataon prep_to_transformed_spectflcataons dobj_transformed_grids advmod_transformed_first nsubj_transformed_We rcmod_database_transformed nn_database_lexacal poss_database_Dorr dobj_considered_database advmod_considered_next nsubj_considered_We rcmod_1_considered dep_9_1 number_9_1 number_9_4 dep_9_2 number_2_1 number_2_1 dep_9_9 number_9_2 number_9_6 dep_9_AHD-W3 dep_9_115 dep_AHD-W3_90 dep_90_36 number_36_40 number_115_120 dep_W3-AHD_9 nn_W3-AHD_O dobj_q_W3-AHD nsubj_q_oJ nn_oJ_AHD nn_oJ_W3 nn_oJ_Senseval nn_oJ_m prep_for_used_Dorr dep_used_q vmod_sets_used nn_sets_defimtaon det_sets_the nn_sets_analyzmg conj_and_parsing_sets nn_parsing_m nn_parsing_dad dep_we_sets dep_we_parsing prep_as_mformataon_we amod_mformataon_much advmod_mformataon_as dobj_extracted_mformataon neg_extracted_not aux_extracted_have nsubj_extracted_we mark_extracted_that ccomp_hkehhood_extracted det_hkehhood_the ccomp_reflects_has nn_Tlus_Hector conj_and_WordNet_Tlus pobj_for_Tlus pobj_for_WordNet pcomp_than_for prep_lower_than npadvmod_lower_little cop_lower_be aux_lower_to det_little_a xcomp_tend_lower nsubj_tend_scores mark_tend_that amod_dictaonanes_published det_dictaonanes_the prep_for_scores_dictaonanes det_scores_the dep_noted_reflects ccomp_noted_tend auxpass_noted_be aux_noted_should nsubjpass_noted_It advmod_AHD_Finally nn_AHD_m amod_AHD_present cop_AHD_be aux_AHD_to xcomp_appear_AHD neg_appear_not aux_appear_do nsubj_appear_that rcmod_W3_appear nn_W3_m nn_W3_senses prep_of_number_W3 nn_number_lugh amod_number_relatavely det_number_a dobj_have_number ccomp_stall_have nsubj_stall_we advcl_stall_tmabndged mark_stall_although cop_tmabndged_Is nsubj_tmabndged_W3 mark_tmabndged_since nn_sets_defimtaon det_sets_the prep_between_daspanty_sets amod_daspanty_overall amod_daspanty_less advcl_ss_stall dobj_ss_daspanty expl_ss_there ccomp_ss_evident ccomp_ss_for conj_and_W3_AHD prep_between_A_AHD prep_between_A_W3 num_A_34 dep_86_A number_86_32 dep_86_07 dep_86_34 number_07_60 number_34_99 dep_34_18 number_18_60 dep_78_86 number_78_120 amod_OALD_78 nn_OALD_W3 nn_OALD_OALD-W3 amod_OALD_W3-OALD nn_OALD_assignments amod_OALD_multiple conj_and_ambiguity_OALD amod_ambiguity_considerable dobj_have_OALD dobj_have_ambiguity aux_have_to xcomp_hkely_have cop_hkely_are nsubj_hkely_we advcl_hkely_has nsubj_hkely_number amod_direction_other det_direction_the prep_in_present_direction neg_present_not cop_present_are nsubj_present_that rcmod_defimtlons_present amod_defimtlons_specific amod_defimtlons_more amod_defimtlons_many dobj_have_defimtlons nsubj_have_we mark_have_that ccomp_likely_have cop_likely_is prep_to_going_OALD prep_from_going_W3 vmod_m_going nn_m_sense amod_m_specific det_m_the dobj_find_m aux_find_to xcomp_unable_find cop_unable_be aux_unable_to xcomp_tend_unable nsubj_tend_We rcmod_10_tend prepc_at_has_likely advmod_has_because dobj_has_10 nsubj_has_OALD mark_has_while dep_for_float num_defimUons_41 prep_has_for dobj_has_defimUons nsubj_has_W3 amod_dlspanty_overall det_dlspanty_the advmod_dominate_Thus dobj_dominate_dlspanty advmod_dominate_then nsubj_dominate_wtuch rcmod_number_has prep_for_number_example rcmod_number_dominate prep_of_number_senses det_number_the rcmod_m_hkely nn_m_spanty nn_m_~ nn_m_d amod_m_considerable det_m_a nsubj_is_m expl_is_there advmod_is_where rcmod_words_is amod_words_individual prep_for_evident_words cop_evident_is prep_of_set_words amod_set_enUre det_set_the conj_and_for_evident pobj_for_set preconj_for_only neg_only_not rcmod_case_ss det_case_the cop_case_be aux_case_to parataxis_tends_noted xcomp_tends_case nsubj_tends_This amod_This_disparate dep_qmte_tends dep_mapping_qmte prep_between_mapping_dictionaries aux_mapping_are nsubj_mapping_we mark_mapping_that ccomp_indicate_mapping nsubj_indicate_disparity advmod_indicate_where amod_assagnments_mulUple amod_assagnments_empty det_assagnments_the conj_and_empty_mulUple prep_between_disparity_assagnments det_disparity_the rcmod_W3_indicate conj_and_W3_OALD prep_between_mapping_OALD prep_between_mapping_W3 det_mapping_the dep_m_mapping dobj_emphasized_m dep_further_emphasized dep_are_further det_points_These num_points_113 dep_points_22 nn_points_Hector-WN dep_points_119 num_points_47 amod_points_WN-Hector number_22_14 dep_22_64 number_64_57 dep_119_17 number_17_06 number_47_37 dobj_made_points auxpass_made_were nsubjpass_made_that rcmod_assignments_made det_assignments_the prep_of_score_assignments amod_score_average det_score_the prep_per_assignments_word amod_assignments_mulUple prep_of_number_assignments amod_number_average det_number_the auxpass_made_be aux_made_could nsubjpass_made_assagnment prep_for_made_which neg_assagnment_no rcmod_senses_made prep_of_number_senses amod_number_average det_number_the dep_target_are conj_and_target_score appos_target_number appos_target_number dep_target_dlCtmnary det_target_the dep_m_score dep_m_target dep_assignments_m prep_of_number_assignments amod_number_average det_number_the auxpass_mapped_being vmod_senses_mapped prep_of_number_senses amod_number_average det_number_the dobj_shows_number nsubj_shows_average det_table_Each num_table_senses num_table_62 num_table_4 prep_per_senses_word num_senses_120 conj_and_4_senses conj_and_4_62 number_4_3 prep_of_average_table det_average_an det_average_th rcmod_~_shows nn_~_w num_W3_9 number_9_9 number_1_7 conj_and_87_W3 appos_87_AHD num_87_1 appos_87_OALD conj_were_number conj_were_~ dep_were_W3 dep_were_87 prep_per_words_sense dep_number_were prep_of_number_words amod_number_average det_number_The dep_standards_number dep_pubhslung_standards conj_and_pubhslung_style amod_pubhslung_rigorous prep_to_subject_style prep_to_subject_pubhslung cop_subject_were nsubj_subject_ctmnanes nn_ctmnanes_~ nn_ctmnanes_d amod_ctmnanes_formal num_ctmnanes_three det_ctmnanes_The rcmod_project_subject amod_project_tagging dep_sense_project amod_sense_large-scale det_sense_a prep_for_used_sense advmod_used_primarily auxpass_used_was nsubjpass_used_Hector rcmod_verbs_used det_verbs_the prep_of_subject_verbs conj_and_subject_object det_subject_the nn_ons_~ nn_ons_restnct amod_ons_selectmnal prep_on_specff3nng_object prep_on_specff3nng_subject dobj_specff3nng_ons vmod_words_specff3nng amod_words_many dobj_contains_words nsubj_contains_whach nsubj_contains_Hector dep_contains_to dep_contains_compared dep_contains_have csubj_contains_exanune dep_contains_AHD nsubj_contains_W3 prep_per_words_sense num_words_184 appos_Hector_words prep_per_words_sense num_words_53 dep_smaller_words advmod_smaller_generally cop_smaller_are dep_smaller_was nsubj_smaller_v cop_smaller_are nsubj_smaller_Hector nsubj_smaller_WordNet dep_glosses_WordNet mark_glosses_as amod_WordNet_such advcl_pubhsh_glosses dobj_pubhsh_them aux_pubhsh_to vmod_mtenuon_pubhsh neg_mtenuon_no nsubj_was_mtenuon expl_was_there mark_was_since prep_as_ewed_chcuonanes nn_ewed_~ advmod_v_ewed advmod_v_properly conj_nor_WordNet_Hector preconj_WordNet_Neither rcmod_process_smaller nn_process_mapping det_process_the nn_process_m dobj_have_process aux_have_may nsubj_have_It det_amphcatlons_the prep_for_base_amphcatlons nn_base_knowledge amod_base_lexacal poss_base_Dorr dobj_exanune_base nsubj_exanune_We appos_W3_OALD appos_WordNet_Hector dep_paars_WordNet det_paars_the conj_and_both_recttons prep_between_m_paars preconj_m_recttons preconj_m_both dobj_mapped_m nsubj_mapped_we nn_purpose_t/us prep_for_process_purpose prep_of_process_mapping det_process_the dobj_assess_process aux_assess_to amod_standard_gold det_standard_a prep_of_use_standard nn_use_thout nn_use_~ dobj_w_use prep_of_parrs_chctaonanes prep_between_mterrelalaons_parrs det_mterrelalaons_the prep_of_nature_mterrelalaons det_nature_the dobj_exanuned_nature advmod_exanuned_next nsubj_exanuned_We number_0_3 num_~_3 num_~_0 nn_~_O nn_~_tel nn_~_Dictionaries rcmod_Comparison_exanuned prep_of_Comparison_~ nn_Comparison_patterns xcomp_defining_assess dep_defining_w dobj_defining_Comparison nsubj_defining_~ nn_~_~ prep_of_number_~ amod_number_small det_number_a advmod_number_only nn_scores_ats dobj_developing_scores vmod_~_developing num_hits_228 dep_had_defining prep_with_had_number dobj_had_~ prep_in_had_~ dobj_had_hits prep_in_had_mappings amod_mappings_selected det_mappings_the rcmod_approach_had nn_approach_~ amod_approach_componenual det_approach_the nn_words_content num_words_41 amod_words_only prep_on_based_words auxpass_based_was dobj_overlap_mapping nsubj_overlap_word mark_overlap_Whereas amod_word_stop-hst det_word_the advcl_overlap_overlap dobj_overlap_methods nsubj_overlap_word mark_overlap_than det_word_the ccomp_mformauon_overlap nn_mformauon_Hector nn_mformauon_WordNet amod_mformauon_more advmod_more_consaderably dep_exploits_based dobj_exploits_mformauon nsubj_exploits_method nn_method_analysis nn_method_componentml det_method_the advmod_importantly_more advmod_importantly_Perhaps advmod_refined_importantly amod_refined_further cop_refined_be aux_refined_to xcomp_needs_refined nsubj_needs_ablhty advmod_needs_where det_ablhty_scnnunatwe det_ablhty_the conj_and_the_scnnunatwe rcmod_words_needs amod_words_individual dobj_identifies_words nsubj_identifies_m nn_method_tlus dep_word_semze det_word_the amod_dascnnunatton_good advmod_obtain_Thus prep_for_obtain_word dobj_obtain_dascnnunatton aux_obtain_to det_mabdaty_the xcomp_donunated_obtain agent_donunated_mabdaty auxpass_donunated_were nsubjpass_donunated_darecUons conj_and_Hector_WordNet prep_between_darecUons_WordNet prep_between_darecUons_Hector preconj_darecUons_both appos_m_method rcmod_m_donunated amod_m_average det_m_The appos_mapping_approach rcmod_mapping_exploits rcmod_mapping_identifies det_mapping_the rcmod_m_mapped dep_m_mapping nn_m_ofamblgmty nn_m_degree det_m_the parataxis_mchcate_contains cc_mchcate_and dep_mchcate_W3 dobj_mchcate_m nsubj_mchcate_cases amod_assignments_multiple prep_of_cases_assignments det_cases_The amod_coverage_different dep_have_mchcate dobj_have_coverage aux_have_may nsubj_have_dictionaries advmod_have_where num_dictionaries_two prepc_of_mdacataon_have amod_mdacataon_tentative det_mdacataon_a dobj_prowde_mdacataon advmod_prowde_thus nsubj_prowde_chetmnanes conj_and_overlap_prowde neg_overlap_not aux_overlap_do nsubj_overlap_chetmnanes mark_overlap_that det_chetmnanes_the ccomp_appears_prowde ccomp_appears_overlap dep_appears_at advmod_appears_where rcmod_cases_appears det_cases_the cop_cases_are nsubj_cases_These rcmod_assignment_cases det_assignment_an dobj_make_assignment aux_make_to xcomp_possable_make neg_possable_not cop_possable_was nsubj_possable_It ccomp_wtuch_possable prep_for_senses_wtuch appos_mappings_senses amod_mappings_empty amod_mappings_few advmod_mappings_relatavely dobj_had_mappings nsubj_had_mapping prep_to_mapping_Hector prep_from_mapping_WordNet det_mapping_The rcmod_errors_had prep_as_construed_errors auxpass_construed_be neg_construed_not advmod_construed_conceivably aux_construed_may conj_and_elements_construed prep_of_elements_meaning amod_elements_common det_elements_some dep_n_construed dep_n_elements num_n_~ pobj_con_n prep_do_con vmod_cases_do det_cases_these prep_that_suggest_cases nsubj_suggest_We det_leracographer_the dep_made_suggest agent_made_leracographer auxpass_made_were nsubjpass_made_none advmod_made_where rcmod_assagnments_made nn_assagnments_malang num_cases_4 prep_by_erred_assagnments prep_on_erred_cases nsubj_erred_technique dep_erred_seems csubj_erred_overlap dep_erred_onlyJust aux_erred_have nsubj_erred_We poss_technique_our nn_mprovement_~ amod_mprovement_substantial prep_in_provides_addition dobj_provides_mprovement nsubj_provides_techmque mark_provides_that nn_techmque_analysis amod_techmque_componentaal det_techmque_the ccomp_seems_provides nn_t_~ appos_basins_t amod_basins_tlus prep_on_Consadered_basins vmod_assignments_Consadered amod_assignments_null dobj_matclung_assignments poss_successes_its prep_of_8_successes prepc_by_aclueved_matclung dobj_aclueved_8 aux_aclueved_had nsubj_aclueved_techmque nn_techmque_stophst det_techmque_the advmod_mentioned_above mark_mentioned_as advmod_techmque_However ccomp_overlap_aclueved advcl_overlap_mentioned advmod_overlap_techmque nsubj_overlap_ms amod_word_stop-last det_word_the dobj_usmg_word nsubj_usmg_improvement amod_assignments_correct num_assignments_31 det_assignments_the prep_over_improvement_assignments nn_improvement_shght det_improvement_a num_%_407 appos_mappmgs_% amod_mappmgs_Impoverished advmod_Impoverished_somewhat rcmod_ms_usmg dep_ms_mappmgs nn_ms_set amod_ms_current det_ms_the det_effort_this dobj_begun_effort dep_onlyJust_so dep_onlyJust_begun ccomp_developed_erred agent_developed_hand auxpass_developed_are nsubjpass_developed_patterns nn_patterns_defimng nn_patterns_4The nn_patterns_WordNet nn_patterns_m dep_included_developed advmod_included_currently mark_included_than vmod_set_included prep_of_set_semrels amod_set_richer det_set_a dobj_include_set aux_include_would nsubj_include_network nn_Thas_defimtaons poss_Thas_dlcUonary predet_Thas_all det_dlcUonary_a dobj_parsing_Thas agent_created_parsing vmod_network_created amod_network_semanlac amod_network_full det_network_the rcmod_usmg_include dobj_envaslon_usmg nsubj_envaslon_We amod_semreis_adentffy dep_use_envaslon prep_to_use_semreis dobj_use_WordNet nsubj_use_we amod_tame_present det_tame_the npadvmod_tame_3At dep_correct_tame amod_86_correct pobj_35_86 prepc_out_of_35_of dobj_obtained_35 nsubj_obtained_we ccomp_obtained_w ccomp_obtained_dechnes mark_obtained_that advmod_described_above vmod_method_described det_method_the dobj_Using_method vmod_words_Using num_words_1878 nn_senses_Hector num_senses_102 prep_with_words_words prep_into_words_senses num_words_348 nn_senses_WordNet num_senses_66 prep_with_mapping_words dobj_mapping_senses xcomp_Involves_mapping nsubj_Involves_analysis nn_analysis_standard amod_standard_gold det_standard_The nn_Analysis_Componential rcmod_Results_Involves prep_of_Results_Analysis nn_Results_amprovements amod_Results_subsequent dobj_making_Results prepc_for_basis_making amod_basis_sounder det_basis_a det_basis_th num_basis_~ dep_w_basis conj_but_dechnes_w nn_dechnes_performance amod_dechnes_overall ccomp_result_obtained det_result_the amod_phenomenon_particular det_phenomenon_a prep_with_wath_result dobj_wath_phenomenon ccomp_deal_wath aux_deal_to vmod_changes_deal dobj_make_changes aux_make_may nsubj_make_we nsubj_make_we nn_Senseval_m amod_effect_overall det_effect_the prep_as_exarmne_Senseval dobj_exarmne_effect nsubj_exarmne_We det_process_the prep_of_stage_process det_stage_any amod_refinements_conUnual conj_and_make_exarmne prep_at_make_stage dobj_make_refinements aux_make_can nsubj_make_We nn_rouUne_mapping rcmod_parse_exarmne rcmod_parse_make conj_and_parse_rouUne amod_parse_perfect det_final_a dobj_have_final aux_have_to xcomp_necessary_have neg_necessary_not dobj_ms_rouUne dobj_ms_parse dep_ms_necessary nsubj_ms_t advmod_ms_Thus nsubj_ms_that mark_ms_as nn_t_~ advmod_Thus_incrementally prep_at_that_proceeds amod_process_Senseval det_process_the advcl_follows_ms dobj_follows_process nsubj_follows_wluch poss_methodology_our dep_wluch_methodology det_wluch_an rcmod_way_follows det_way_Another det_senses_all prep_over_mapping_senses poss_mapping_our dobj_assess_way dobj_assess_mapping aux_assess_to amod_standard_gold det_standard_the prep_of_that_standard poss_selecuon_our vmod_compare_assess prep_with_compare_that dobj_compare_selecuon nsubj_compare_we ccomp_compare_w dep_compare_add nsubj_compare_ff dep_compare_increment dep_compare_We dep_compare_semrels dep_compare_on dep_compare_based nsubj_compare_sense dep_compare_score dep_compare_We dep_compare_33 advmod_compare_rmmmally advmod_compare_only amod_score_lughest det_score_the det_score_th num_score_~ advmod_w_Finally dobj_w_score appos_sense_s det_sense_the dobj_select_sense advmod_select_then nsubj_select_we auxpass_made_been aux_made_have nsubjpass_made_Is nsubjpass_made_value advmod_made_when det_scores_the prep_to_increments_scores amod_increments_possable det_increments_all amod_synset_same det_synset_the dep_dentacal_synset conj_or_dentacal_m amod_~_m amod_~_dentacal prep_after_Is_increments nsubj_Is_~ det_value_the rcmod_points_made num_points_5 det_points_another pobj_by_points advmod_by_then amod_semrel_common det_semrel_a conj_and_have_by dobj_have_semrel nsubj_have_we advmod_have_when num_points_2 det_score_the advcl_Increment_by advcl_Increment_have prep_by_Increment_points dobj_Increment_score nsubj_Increment_We num_goal_4 rcmod_location_Increment conj_and_location_goal conj_and_location_locale conj_and_location_is-part-of conj_and_location_has-members conj_and_location_has-constituents conj_and_location_manner conj_and_location_source conj_and_location_purpose nn_Instrument_recogmzmg dep_qmte_nummal advmod_qmte_usually amod_patterns_defimng prep_for_have_Instrument dep_have_qmte dobj_have_patterns nsubj_have_we prep_amtml_tins_tmplementaUon prep_in_semrels_tins amod_semrels_common pobj_on_semrels pcomp_based_on prep_score_based det_score_the dep_increment_score dep_We_increment dep_3_We num_match_3 det_match_each num_points_5 parataxis_add_select dobj_add_goal dobj_add_locale dobj_add_is-part-of dobj_add_has-members dobj_add_has-constituents dobj_add_manner dobj_add_source dobj_add_purpose dobj_add_location dep_add_means parataxis_add_have prep_for_add_match dobj_add_points nsubj_add_we dep_add_overlap dep_add_differentiae advmod_add_as nsubj_add_that nn_Currently_substanUally amod_Currently_increased auxpass_increased_be aux_increased_can nsubjpass_increased_score dep_increased_synsets advmod_increased_consaderably det_score_the nsubj_evoke_they rcmod_synsets_evoke det_synsets_the nn_synsets_m dobj_overlap_Currently nsubj_overlap_they det_hypernym_the conj_and_differentiae_overlap prep_on_differentiae_hypernym neg_differentiae_no prep_of_number_synonyms amod_number_large det_number_a dobj_ns_number nsubj_ns_con mark_ns_If num_con_~ nn_con_sense amod_con_parUcular det_con_a poss_hypernyms_sense amod_sense_other det_sense_the prep_as_synset_hypernyms amod_synset_same det_synset_the advcl_belong_ns prep_to_belong_synset nsubj_belong_hypernyms mark_belong_If poss_hypernyms_sense det_sense_a amod_hypernym_common det_hypernym_a conj_or_have_belong dobj_have_hypernym nsubj_have_senses det_senses_the rcmod_ff_belong rcmod_ff_have nn_ff_score det_ff_the nn_semrels_rots amod_sense_viable det_sense_each amod_score_next ccomp_used_compare auxpass_used_are nsubjpass_used_these amod_tune_present det_tune_the amod_particle_different det_particle_a dobj_uses_particle nsubj_uses_that rcmod_one_uses prep_into_map_one neg_map_not aux_map_can nsubjpass_map_g nsubj_map_propertaes det_parUcle_a prep_with_used_parUcle auxpass_used_is nsubjpass_used_that rcmod_sense_used det_sense_a appos_g_sense dep_g_e amod_propertaes_collocataonal advmod_mtransRave_only cop_mtransRave_Is nsubj_mtransRave_that rcmod_one_mtransRave prep_into_map_one neg_map_not aux_map_can nsubj_map_transluve aux_map_Is mark_map_that advmod_transluve_only ccomp_sense_map amod_sense_verb det_sense_a appos_g_sense dep_g_e conj_and_properUes_map dep_properUes_g nn_properUes_syntacUc dobj_include_map dobj_include_properUes nsubj_include_criteria det_criteria_These rcmod_mappings_include amod_mappings_specific dobj_rule_mappings prt_rule_out aux_rule_to vmod_cntena_rule nn_cntena_exclusaon amod_cntena_exanune advmod_cntena_first rcmod_we_used prep_at_we_tune dep_we_cntena rcmod_comparison_use rcmod_comparison_make nn_comparison_defimtaon det_comparison_the nn_comparison_m aux_appear_to vmod_Lltkowska_appear conj_Senseval_comparison advmod_Senseval_Specifically appos_Senseval_Lltkowska nn_Senseval_m amod_Senseval_used dep_methodology_Senseval amod_methodology_general det_methodology_the dobj_follows_methodology nsubj_follows_process nsubj_follows_selectaons det_process_This det_selectaons_the ccomp_make_follows conj_and_compare_make dobj_compare_senses aux_compare_to nn_database_lexacal amod_database_populated det_database_the vmod_uses_make vmod_uses_compare dobj_uses_database nsubj_uses_stage mark_uses_as det_analysis_the prep_of_stage_analysis amod_stage_second det_stage_The amod_stage_recogmzed ccomp_that_uses dep_adjectwe_that det_adjectwe_the advmod_adjectwe_only acomp_becomes_adjectwe nsubj_becomes_semrel mark_becomes_that advmod_becomes_so nn_semrel_value det_manner_the prep_to_value_manner det_value_the nn_role_mdacate nn_role_m det_manner_a det_value_the prep_for_copied_manner prep_into_copied_value neg_copied_not auxpass_copied_are nsubjpass_copied_words mark_copied_that det_words_these dep_mdacate_copied amod_mdacate_hteral det_mdacate_the det_detenmner_the det_0_The amod_manner_lateral det_manner_the det_adjectwe_an conj_and_determiner_manner appos_determiner_adjectwe num_determiner_1 num_determiner_0 conj_or_0_1 prep_of_consasts_manner prep_of_consasts_determiner nsubj_consasts_that rcmod_phrase_consasts nn_phrase_noun det_phrase_a dobj_vath_phrase advmod_well_as det_m_the amod_elements_partacular prep_before_specify_m dobj_specify_elements aux_specify_to xcomp_allows_specify dobj_allows_us nsubj_allows_which dep_tdde_well rcmod_tdde_allows det_tdde_the conj_and_m_mdacate prep_after_m_detenmner num_m_0 dep_m_vath appos_m_tdde dobj_recogmze_role nsubj_recogmze_we advmod_recogmze_where dep_component_becomes rcmod_component_recogmze nn_component_expressaons det_manner_a prep_to_rise_manner amod_rise_gwmg advmod_gwmg_possibly prep_as_m_rise dobj_recognize_m aux_recognize_to xcomp_allows_recognize dobj_allows_us nsubj_allows_This appos_st_manner num_st_0 dep_manner_st nn_manner_adj dep_manner_det appos_det_0 nn_det_l nn_det_rep0 num_det_~ dep_dpat_manner nn_dpat_m nn_dpat_m rcmod_entry_allows dep_entry_dpat prep_entry_for nn_entry_foUowmg det_entry_the dobj_have_entry nsubj_have_we det_manner_a dobj_recograze_semrel dobj_recograze_manner aux_recograze_to xcomp_matched_recograze advmod_matched_Thus auxpass_matched_be aux_matched_to vmod_pattern_matched amod_pattern_syntactac det_pattern_a dobj_arlaculates_pattern nsubj_arlaculates_that rcmod_expressaon_arlaculates amod_expressaon_regular det_expressaon_a dobj_s_expressaon nsubj_s_~ nn_~_pattern amod_~_defimng det_~_A rcmod_patterns_s amod_patterns_specLfic prep_for_look_patterns aux_look_to amod_routines_specafic vmod_develop_look dobj_develop_routines aux_develop_to xcomp_have_develop neg_have_not aux_have_do nsubj_have_we advcl_have_have nsubj_is_That rcmod_parser_is det_parser_the agent_used_parser vmod_dictionary_used det_dictionary_the prep_of_part_dictionary advmod_part_actually cop_part_are nsubj_part_pattems amod_pattems_defimng det_pattems_The auxpass_gwen_is nsubjpass_gwen_one mark_gwen_if ccomp_derived_part advcl_derived_gwen auxpass_derived_are nsubjpass_derived_they amod_synset_adjecuve det_synset_an parataxis_adentffy_derived prep_from_adentffy_wluch dobj_adentffy_synset aux_adentffy_to xcomp_look_adentffy prep_in_look_WordNet prt_look_up dobj_look_these nsubj_look_We nn_tree_parse det_tree_the nn_tree_m nn_tree_adverbs conj_and_identify_look dobj_identify_tree advmod_identify_also nsubj_identify_We rcmod_semrel_look rcmod_semrel_identify amod_semrel_partacular det_semrel_a prep_of_mdacaUve_semrel prep_as_use_mdacaUve aux_use_can nsubj_use_we nn_whach_preposaUon det_whach_the amod_pattern_defining det_pattern_a ccomp_have_use prep_for_have_whach dobj_have_pattern nsubj_have_we mark_have_whether conj_and_defimUon_deterrmne det_defimUon_the amod_phrases_preposattonal prep_in_exanune_deterrmne prep_in_exanune_defimUon dobj_exanune_phrases advmod_exanune_then nsubj_exanune_We rcmod_subject_exanune conj_and_subject_object det_subject_the parataxis_expressaons_have rcmod_expressaons_have prep_for_expressaons_object prep_for_expressaons_subject amod_expressaons_parenthesazed amod_expressaons_pamcular advmod_expressaons_as dep_that_component ccomp_-LRB-_that amod_restnctmus_selecttonal det_restnctmus_any dobj_contmned_restnctmus nsubj_contmned_defimtaon mark_contmned_whether det_defimtaon_the ccomp_signal_contmned nsubj_signal_we prep_for_verb_verbs conj_or_heads_verb amod_heads_empty prep_of_recogmtton_verb prep_of_recogmtton_heads prep_with_noun_recogmtton nn_noun_head det_noun_the rcmod_Identification_signal prep_of_Identification_noun dobj_includes_Identification nsubj_includes_flus dep_noun_defimttons conj_or_noun_verb det_noun_all prep_for_tense_verb prep_for_tense_noun conj_and_characterizations_tense prep_of_characterizations_number prep_as_annotations_tense prep_as_annotations_characterizations mwe_annotations_such dobj_include_annotations advmod_include_also aux_include_may nsubj_include_node advmod_include_where det_node_any rcmod_defimuons_includes rcmod_defimuons_include det_defimuons_the dobj_compnse_defimuons nsubj_compnse_that rcmod_atems_compnse amod_atems_lextcal det_atems_the dobj_represent_atems dep_leaves_represent nsubj_leaves_whose conj_and_represent_leaves dobj_represent_non-ternunals nsubj_represent_nodes nn_nodes_mtermedaate poss_nodes_whose rcmod_tree_leaves rcmod_tree_represent det_tree_a dep_e_tree amod_a_e appos_results_a amod_results_parse det_results_the dobj_examines_results nsubj_examines_extractmn prep_of_extractmn_semrels det_extractmn_The nn_information_semrel amod_information_much rcmod_bearers_examines prep_of_bearers_information det_bearers_the cop_bearers_are nsubj_bearers_they mark_bearers_since det_method_the advcl_cmcml_bearers prep_to_cmcml_method cop_cmcml_is nsubj_cmcml_inclusion det_prepositions_the prep_of_inclusion_prepositions advmod_parsing_Moreover dep_successful_parsing prepc_for_necessary_successful cop_necessary_are nsubj_necessary_words nsubj_necessary_prepositmns advmod_necessary_where nn_list_stop det_list_the amod_words_other prep_on_prepositmns_list conj_and_prepositmns_words parataxis_parser_cmcml rcmod_parser_necessary nn_parser_sentence amod_parser_full-scale det_parser_a cop_parser_is nsubj_parser_parser det_parser_The nn_parsing_definition det_parsing_the parataxis_applicable_parser prep_to_applicable_parsing neg_applicable_not cop_applicable_is nsubj_applicable_hst mark_applicable_that nn_hst_stop det_hst_the num_2Note_2 nn_2Note_semrels dep_extraction_applicable prep_of_extraction_2Note det_extraction_the dobj_facilitates_extraction nsubj_facilitates_that rcmod_frame_facilitates nn_frame_sentence det_frame_a prep_into_put_frame dobj_put_it aux_put_to vmod_text_put nn_text_definition det_text_the dobj_preprocessmg_text aux_preprocessmg_is nsubj_preprocessmg_semrels dep_preprocessmg_via det_stage_this prep_in_step_stage amod_step_first amod_step_important det_step_an amod_text_ordinary dobj_follow_text neg_follow_not aux_follow_do nsubj_follow_that dep_idiosyncrasies_follow amod_idiosyncrasies_many dobj_have_idiosyncrasies nsubj_have_definitions mark_have_Since appos_semrels_step advcl_semrels_have amod_patterns_defining pobj_via_patterns advcl_results_preprocessmg prep_to_results_extract nsubj_results_specifications nsubj_results_restrictions nsubj_results_components nsubj_results_charactenstlcs nsubj_results_features det_parse_the dobj_uses_parse nsubj_uses_stage aux_appear_to vmod_Research_appear appos_Research_Litkowski appos_Research_1999b nn_Research_CL dep_definitions_Research det_definitions_the conj_and_parses_uses dobj_parses_definitions nsubj_parses_stage det_analysis_the prep_of_stage_analysis amod_stage_first det_stage_The rcmod_specifications_uses rcmod_specifications_parses amod_specifications_coUocational amod_restrictions_selectional prep_of_identification_semrels prep_through_realized_identification dep_components_realized nn_components_semantm nn_patterns_subcategonzation prep_including_charactenstlcs_patterns conj_and_features_specifications conj_and_features_restrictions conj_and_features_components conj_and_features_charactenstlcs nn_features_syntactic ccomp_include_results aux_include_can nsubj_include_components nn_components_meaning det_components_The det_defimtion_a ccomp_included_include prep_in_included_defimtion nsubj_included_identification nn_components_meamng det_components_the prep_of_identification_components amod_identification_complete det_identification_a cop_identification_is ccomp_strives_included nsubj_strives_approach prep_toward_strives_which det_approach_this ccomp_ideal_strives det_ideal_The amod_dictionary_ideal amod_dictionary_single det_dictionary_a dep_thin_dictionary amod_~_thin dobj_w_~ amod_senses_related nn_senses_clnstenng dep_Dolan_w prep_for_Dolan_senses dep_Dolan_1994 prep_in_followed_Dolan auxpass_followed_was nn_values_semrel dobj_relaxing_values amod_defimtlons_parsing amod_approach_slmdar det_approach_A nn_approach_phraseologles conj_and_items_relaxing conj_and_items_defimtlons conj_and_items_approach amod_items_lexical amod_items_several prep_with_synsets_relaxing prep_with_synsets_defimtlons prep_with_synsets_approach prep_with_synsets_items nn_synsets_WordNet prep_in_embodied_synsets vmod_concept_embodied dep_concept_kind dep_concept_absence prep_kind_of det_kind_the advmod_kind_instead amod_synonyms_true conj_and_absence_kind prep_of_absence_synonyms det_absence_the dep_implies_followed advcl_implies_concept prep_in_implies_general nsubj_implies_finding det_finding_This num_way_one quantmod_one_than mwe_than_more nsubjpass_verbalized_that dep_lexicahzed_implies prep_in_lexicahzed_way conj_and_lexicahzed_verbalized auxpass_lexicahzed_be aux_lexicahzed_may nsubjpass_lexicahzed_that rcmod_concepts_verbalized rcmod_concepts_lexicahzed dobj_represent_concepts nsubj_represent_nodes advmod_represent_where rcmod_network_represent amod_network_semantic det_network_a dobj_induces_network nsubj_induces_dictionary mark_induces_that det_dictionary_a amod_Litkowski_1978 dep_finding_induces dep_finding_Litkowski det_finding_the prep_on_based_finding auxpass_based_is nsubjpass_based_approach det_approach_This amod_synset_same det_synset_the ccomp_members_based prep_of_members_synset cop_members_are nsubj_members_Lf conj_or_hypernyms_instruments det_hypernyms_the dobj_Lf_instruments dobj_Lf_hypernyms advmod_Lf_also amod_word_same det_word_the cop_word_is csubj_word_successful conj_or_hypernym_instrument amod_hypernym_ffthe advmod_ffthe_just neg_ffthe_not dep_equal_instrument dep_equal_hypernym cop_equal_are nsubj_equal_defimtions mark_equal_that det_defimtions_the ccomp_say_equal aux_say_would nsubj_say_we ccomp_say_seems ccomp_say_appears ccomp_say_case ccomp_say_seems ccomp_say_participated ccomp_say_assigned dep_say_overlap advmod_say_so dep_say_due det_defimtion_a dobj_parsing_defimtion conj_or_hypernyms_instruments prepc_from_identify_parsing dobj_identify_instruments dobj_identify_hypernyms nsubj_identify_ffwe prep_for_identify_example advmod_identify_Thus nsubj_identify_substitution dobj_using_WordNet vmod_substitution_using amod_substitution_synonymic ccomp_allowing_identify det_relations_these prep_to_values_relations prepc_by_relaxing_allowing dobj_relaxing_values dep_relaxing_2 det_definitions_the nn_definitions_m amod_definitions_present nn_definitions_semrels dobj_identify_definitions aux_identify_to amod_patterns_defimng conj_and_using_relaxing vmod_using_identify dobj_using_patterns nsubj_using_analysis amod_definitions_parsing nn_definitions_steps appos_steps_1 amod_steps_major num_steps_two conj_and_relies_relaxing conj_and_relies_using prep_on_relies_definitions advmod_relies_primarily nsubj_relies_analysis amod_sets_defimtion num_sets_two prep_between_mapping_sets det_mapping_the prep_of_analysis_mapping amod_analysis_deeper det_analysis_The dep_Definitions_using dep_Definitions_relies prep_of_Analysis_Definitions nn_Analysis_Meaning-Full num_Analysis_32 nn_Analysis_defimtlons det_Analysis_the prep_of_analysis_Analysis amod_analysis_componentlal det_analysis_the prep_of_results_analysis det_results_the dobj_presenting_results prepc_after_discussed_presenting advmod_discussed_further auxpass_discussed_be aux_discussed_will nsubjpass_discussed_This ccomp_suffice_discussed aux_suffice_will nsubj_suffice_mapping mark_suffice_that amod_mapping_reasonable det_mapping_any ccomp_sufficient_suffice acomp_seems_sufficient nsubj_seems_it advmod_precisely_more amod_analysis_subsequent det_analysis_the advmod_frame_precisely dobj_frame_analysis xcomp_help_frame aux_help_may csubj_help_important det_mapping_the prep_of_quality_mapping det_quality_The dobj_important_quality advmod_important_crucially neg_important_not cop_important_is nsubj_important_mapping nsubj_important_accuracy mark_important_that amod_standard_gold det_standard_a prep_of_accuracy_standard det_accuracy_the ccomp_appears_help nsubj_appears_it advmod_differences_Moreover det_differences_the prep_of_analysis_differences amod_analysis_deeper det_analysis_a dobj_frames_analysis conj_and_sets_frames nn_sets_definmon num_sets_two prep_between_process_frames prep_between_process_sets nn_process_companson nn_process_defimuon det_process_the prep_of_overview_process amod_overview_broad det_overview_a dobj_provides_overview nsubj_provides_analysis det_analysis_The rcmod_case_provides det_case_the neg_case_not cop_case_is nsubj_case_this advmod_perform_well neg_perform_not aux_perform_does nsubj_perform_analysis ccomp_overlap_perform nsubj_overlap_word mark_overlap_if mark_overlap_as det_word_the advcl_appear_overlap aux_appear_might nsubj_appear_it mark_appear_Although advmod_divergent_somewhat cop_divergent_be aux_divergent_to advcl_seems_appear xcomp_seems_divergent nsubj_seems_vocabulary advmod_seems_therefore dep_seems_3 nn_sets_definition num_sets_two det_sets_these prep_between_vocabulary_sets amod_vocabulary_defimng det_vocabulary_the det_assignments_the prep_in_participated_assignments nsubj_participated_words prep_despite_participated_detail dep_participated_2 nn_words_content num_words_8 number_8_1 quantmod_8_only amod_words_many prep_as_times_words number_times_8 number_times_2 dep_senses_times nn_senses_WordNet nn_senses_Hector pobj_detail_senses prepc_compared_to_detail_to prep_in_detail_senses amod_detail_greater det_detail_the conj_but_assigned_seems conj_and_assigned_appears conj_and_assigned_case conj_and_assigned_seems conj_and_assigned_participated auxpass_assigned_be neg_assigned_not aux_assigned_could num_%_6 num_%_13 appos_senses_% nn_senses_WordNet num_senses_66 prep_of_9_senses appos_Hector_9 prep_in_set_Hector prep_of_set_defimtions amod_set_ncher det_set_a appos_analysis_1 prep_despite_overlap_set dobj_overlap_analysis nsubj_overlap_average advcl_overlap_used nsubj_overlap_content det_word_the dobj_summanze_word aux_summanze_To vmod_words_summanze nn_words_content num_words_8 dep_words_I prep_of_average_words det_average_an auxpass_used_was nsubjpass_used_list advmod_used_when nn_list_stop det_list_the num_successes_23 det_successes_these prep_in_involved_successes auxpass_involved_were nsubjpass_involved_words rcmod_content_involved num_content_41 quantmod_41_only amod_words_content advmod_matching_Overall dobj_matching_words prepc_to_due_matching cop_due_were nsubj_due_cases mark_due_that num_%_7 num_%_26 appos_cases_% num_cases_23 advmod_cases_only ccomp_so_due amod_assignments_empty prep_to_due_assignments cop_due_were nsubj_due_cases advmod_due_where num_cases_8 ccomp_deceptive_say cop_deceptive_is nsubj_deceptive_improvement auxpass_used_was nsubjpass_used_list advmod_used_when nn_list_stop det_list_the advcl_arising_used vmod_improvement_arising det_improvement_The num_%_1 num_%_36 appos_cases_% num_cases_86 prep_of_31_cases ccomp_successful_deceptive prep_in_successful_31 cop_successful_were nsubj_successful_we nn_list_stop det_list_the dobj_using_list num_%_6 num_%_32 appos_cases_% num_cases_86 prep_of_28_cases prep_in_lexicographer_28 possessive_lexicographer_'s det_lexicographer_the conj_but_matched_members ccomp_matched_word vmod_matched_using dobj_matched_lexicographer nsubj_matched_selections poss_selections_our nn_hst_stop det_hst_the advmod_correct_completely mark_correct_as advcl_treating_correct dobj_treating_flus prepc_of_Senseval_treating prep_in_employed_Senseval vmod_method_employed amod_method_relaxed det_method_the dobj_using_method neg_using_not dep_correct_using num_correct_3 prep_of_out_correct advmod_1_out quantmod_1_only dep_treated_members dep_treated_matched prep_without_treated_hst prep_as_treated_1 dobj_treated_flus nsubj_treated_method amod_method_scoring poss_method_our conj_and_senses_4 amod_senses_3 num_senses_2 nn_senses_Hector appos_assignments_4 appos_assignments_senses num_assignments_three parataxis_made_treated dobj_made_assignments aux_made_had nsubj_made_lexicographer det_lexicographer_the det_case_this nn_defimtion_Hector det_defimtion_the prep_of_07_defimtion number_07_0 appos_money_07 dep_word_money num_word_one quantmod_one_only parataxis_overlap_made prep_in_overlap_case prep_of_overlap_word nsubj_overlap_an dep_was_overlap expl_was_there auxpass_invoked_was nsubjpass_invoked_list advmod_invoked_When nn_list_stop det_list_the nn_list_stop det_list_the num_words_15 poss_words_its prep_of_13_words num_13_0 dep_defimtlon_13 nn_defimtlon_Hector det_defimtlon_the prep_in_039_defimtlon rcmod_money_was rcmod_money_invoked prep_without_money_list appos_money_039 dep_words_money num_words_two prep_on_overlap_words nsubj_overlap_an ccomp_was_overlap expl_was_there det_case_this nn_way_thts nn_way_m dep_way_sum conj_or_money_property prep_of_sum_property prep_of_sum_money det_sum_a dobj_risk_way aux_risk_to det_person_a prep_of_sense_person num_sense_4 nn_sense_Hector prep_in_mapped_case xcomp_mapped_risk prep_into_mapped_sense nsubj_mapped_sense parataxis_mapped_considered dep_mapped_appear dep_mapped_selected parataxis_mapped_is det_issue_an prep_of_outcome_issue det_outcome_the prep_on_stake_outcome appos_stake_money appos_sense_stake num_sense_2 nn_sense_WordNet prep_for_used_example auxpass_used_was nsubjpass_used_hst advmod_used_when nn_hst_stop det_hst_the prep_for_considered_bet advcl_considered_used auxpass_considered_were nsubjpass_considered_words amod_words_content advmod_content_only nn_sense_Hector det_sense_any prep_in_appear_sense neg_appear_not aux_appear_did nn_sense_WordNet det_sense_the prep_in_words_sense det_words_the predet_words_all nn_words_ff dobj_made_words auxpass_made_was nsubjpass_made_selection amod_selection_empty num_selection_31 det_selection_An rcmod_senses_made nn_senses_Hector det_senses_all prep_over_percentage_senses prep_of_percentage_words amod_percentage_highest det_percentage_the prep_on_based_percentage nn_sense_Hector det_sense_a vmod_selected_based dobj_selected_sense nsubj_selected_we nn_sense_Hector det_sense_a prep_in_appeared_sense nsubj_appeared_t mark_appeared_whether nn_t_~ ccomp_determined_appeared nsubj_determined_We nn_sense_WordNet det_sense_a dep_m_sense nn_m_word det_m_each conj_and_took_determined dobj_took_m nsubj_took_We amod_words_Inflected prep_of_forms_words nn_forms_root amod_forms_ldenttfy ccomp_attempt_determined ccomp_attempt_took prep_to_attempt_forms neg_attempt_not aux_attempt_did nsubj_attempt_we ccomp_is_attempt nsubj_is_that nn_senses_Hector det_senses_the conj_and_WordNet_senses prep_in_definlUons_senses prep_in_definlUons_WordNet nn_definlUons_tile nn_hst_stop det_hst_a pobj_with_hst conj_and_with_wsthout prep_between_analysts_definlUons prep_analysts_wsthout prep_analysts_with parataxis_overlap_was dep_overlap_mapped dobj_overlap_analysts amod_word_strict det_word_a dobj_performed_word nsubj_performed_We num_words_1878 dobj_contained_words nsubj_contained_senses nn_senses_Hector det_senses_all rcmod_words_contained num_words_960 quantmod_960_about dobj_contained_words nsubj_contained_senses dep_contained_conjunctions dep_contained_pronouns dep_contained_preposmons dep_contained_words dobj_overlap_analysis nsubj_overlap_word det_word_the rcmod_m_overlap dobj_selected_m vmod_senses_selected nn_senses_Hector det_senses_The conj_and_preposmons_conjunctions conj_and_preposmons_pronouns advmod_preposmons_mostly num_words_165 dobj_contained_words nsubj_contained_which nn_list_stop poss_list_our prep_on_appeanng_list nn_appeanng_words amod_appeanng_common cop_appeanng_were nsubj_appeanng_half prep_of_half_wluch quantmod_half_about rcmod_words_contained rcmod_words_appeanng num_words_348 ccomp_contained_contained nsubj_contained_senses amod_senses_WordNet det_senses_The rcmod_asssgnments_contained num_asssgnments_3 num_asssgnments_2 conj_or_2_3 dobj_received_asssgnments nsubj_received_senses num_senses_17 advmod_one_exactly dep_one_recewed number_one_40 conj_and_assignments_received conj_and_assignments_one neg_assignments_no dobj_gwen_received dobj_gwen_one dobj_gwen_assignments auxpass_gwen_were nn_senses_WordNet num_senses_9 auxpass_made_were nsubjpass_made_total num_assignments_86 prep_of_total_assignments det_total_A appos_senses_senses rcmod_senses_made nn_senses_Hector num_senses_102 dep_Into_senses nsubj_occurred_test dobj_occurred_which det_test_a rcmod_m_occurred nn_m_synset det_m_each prep_senses_Into appos_senses_m nn_senses_WordNet num_senses_66 det_senses_the dobj_mapped_senses nsubj_mapped_lextcographer dep_mapped_Lesk dep_mapped_to dep_mapped_smular dep_mapped_analysis det_lextcographer_The dep_Lesk_1986 parataxis_overlap_overlap ccomp_overlap_performed dep_overlap_gwen dep_overlap_mapped csubj_overlap_using amod_word_mple dobj_~_word vmod_s_~ det_s_a dobj_using_s number_6_1 num_WordNet_6 poss_mappmg_lexicographer det_lexicographer_the prep_to_replication_Hector prep_from_replication_WordNet prep_of_replication_mappmg amod_replication_automatic prep_for_baseline_replication det_baseline_a parataxis_estabhsh_overlap dobj_estabhsh_baseline advmod_estabhsh_first nsubj_estabhsh_We rcmod_Analysis_estabhsh nn_Analysis_Overlap dep_Analysis_Word dep_Word_22 dep_22_34 dep_22_37 dep_22_I number_22_12 dep_22_7 dep_34_62 number_62_120 dep_37_57 number_57_0 number_7_1 dep_7_7 number_7_0 dep_1_Analysis number_1_1 dep_1_1 number_1_0 dep_1_2 number_2_2 dep_2_3 number_3_1 dep_3_1 number_3_0 dep_3_1 dep_3_2 dep_1_5 number_5_10 number_2_3 number_2_1 dep_2_3 number_3_1 dep_3_3 number_3_1 number_3_8 dep_3_4 number_4_4 number_4_3 dep_4_1 number_1_4 dep_2_3 number_2_2 number_2_1 dep_O_2 dep_3_O dep_6_3 number_6_1 dep_6_1 number_1_17 dep_37_6 number_37_8 number_37_8 dep_37_13 number_13_21 number_13_6 dep_13_11 number_11_3 dep_3_37 number_3_1 dep_3_3 number_3_2 dep_5_3 number_5_2 dep_5_2 dep_5_4 dep_5_7 number_2_3 number_2_6 dep_4_4 number_4_4 number_7_4 dep_7_5 dep_7_2 dep_5_5 number_5_10 number_2_6 number_2_3 dep_2_4 number_4_1 number_4_2 number_4_14 number_4_41 dep_4_4 number_4_16 dep_5_5 number_5_15 dep_5_5 dep_5_9 number_5_6 number_5_8 number_5_8 dep_5_6 number_6_6 number_9_10 dep_9_5 number_5_5 dep_5_5 number_5_14 number_5_6 dep_5_12 dep_5_5 dep_12_7 number_7_9 dep_7_6 number_6_7 dep_5_5 number_5_2 amod_4_5 number_4_4 dep_4_II dep_4_2 number_II_1 number_II_3 number_2_4 dep_2_2 number_2_1 dep_o_4 nn_o_o nn_o_o nn_o_Polysemy amod_o_Average nn_o_shght nn_o_shake dobj_seize_o nsubj_seize_scrap ccomp_sanction_seize nn_sanction_sack dep_pronuse_sanction dobj_invade_pronuse nsubj_invade_hurdle nn_hurdle_float nn_hurdle_denve ccomp_consume_invade dep_calculate_consume ccomp_bury_calculate ccomp_bother_bury nsubj_bother_bet nn_bet_band ccomp_amaze_bother nn_Word_hand xcomp_entered_amaze agent_entered_Word auxpass_entered_were nsubjpass_entered_polysemy nsubjpass_entered_number prep_following_entered_identities nn_dictionaries_resources det_dictionaries_these prep_of_each_dictionaries prep_for_polysemy_each amod_polysemy_overall amod_polysemy_average conj_and_number_polysemy prep_of_number_senses det_number_the nn_identities_table rcmod_The_entered nn_al_et amod_Olsen_1998 appos_Olsen_al dep_preposmons_The dep_preposmons_Olsen dobj_accompanying_preposmons vmod_identifying_accompanying advmod_identifying_frequently nn_components_semanuc amod_components_optional amod_components_obligatory conj_and_obligatory_optional vmod_roles_identifying prep_of_roles_components amod_roles_thematic det_roles_the dobj_characterize_roles nsubj_characterize_wluch amod_grids_thematic dep_contains_characterize dobj_contains_grids nsubj_contains_database poss_database_Dorr dep_tame_contains amod_tame_present det_tame_the prep_at_used_tame neg_used_not nn_notes_usage vmod_form_used prep_of_form_notes det_form_the dep_m_form dep_information_m dep_collocattonal_information amod_some_collocattonal nn_usages_sample conj_and_contmn_some dobj_contmn_usages advmod_contmn_also nsubj_contmn_which dep_AHD_some dep_AHD_contmn conj_and_W3_AHD conj_and_W3_OALD det_defimuons_the advmod_defimuons_only prep_from_used_AHD prep_from_used_OALD prep_from_used_W3 dobj_used_defimuons nsubj_used_We rcmod_Base_used appos_Base_Dorr nn_Base_Knowledge amod_Base_Lexacal poss_Base_Dorr nn_Dorr_DlcUonary appos_DlcUonary_AI-ID nn_DlcUonary_Hentage amod_DlcUonary_American amod_DlcUonary_ctlonary nn_DlcUonary_~ nn_DlcUonary_D dep_ctlonary_OALD dobj_l.earners_Base vmod_Advanced_l.earners nn_Advanced_Oxford dep_Dictionary_Advanced appos_Dictionary_W3 nn_Dictionary_International nn_Dictionary_New nn_Dictionary_ra num_Dictionary_3 poss_Dictionary_Webster dep_sources_Dictionary amod_sources_additional prep_following_the_sources amod_information_leracal prep_from_defirauons_the conj_or_defirauons_information det_defirauons_the dobj_included_information dobj_included_defirauons aux_included_have nsubj_included_we nn_comparison_resource amod_comparison_lexacal prep_of_issues_comparison amod_issues_other prep_at_look_issues aux_look_to dep_look_order mark_look_In poss_analysis_our advcl_extend_look dobj_extend_analysis aux_extend_To dep_ENTAILS_CAUSES appos_semrels_ENTAILS amod_semrels_other det_semrels_some dobj_verb_frames conj_and_mformaUon_semrels appos_mformaUon_hyponyms appos_mformaUon_hypernyms dep_mformaUon_verb amod_mformaUon_syntactic det_mformaUon_some appos_usages_gloss dep_defimtton_usages conj_or_defimtton_example det_defimtton_a advmod_defimtton_perhaps vmod_synonyms_extend appos_synonyms_semrels appos_synonyms_mformaUon appos_synonyms_example appos_synonyms_defimtton appos_synonyms_synsets nn_synonyms_contmns nn_synonyms_database nn_synonyms_WordNet det_synonyms_The amod_sense_spectfic det_sense_the prep_in_appears_sense nsubj_appears_word det_word_a amod_enwronment_semantic prep_in_syntactic_wluch conj_and_syntactic_enwronment det_syntactic_the parataxis_information_included dep_information_synonyms dep_information_appears prep_about_information_enwronment prep_about_information_syntactic amod_information_collocational nn_usages_example amod_properties_syntactic conj_and_defimttons_clues conj_and_defimttons_usages conj_and_defimttons_properties dep_contains_information dobj_contains_clues dobj_contains_usages dobj_contains_properties dobj_contains_defimttons nsubj_contains_each prep_of_each_which prep_of_tree_senses det_tree_a prep_of_consists_tree parataxis_used_contains dep_used_consists prep_in_used_Senseval nsubj_used_database nn_database_Hector det_database_The nn_database_SLIGHT conj_SHAKE_used nsubj_SHAKE_AMAZE advmod_SHAKE_specifically appos_AMAZE_SEIZE conj_AMAZE_SCRAP conj_AMAZE_SANCTION conj_AMAZE_SACK conj_AMAZE_PROMISE conj_AMAZE_INVADE conj_AMAZE_HURDLE conj_AMAZE_FLOAT conj_AMAZE_DERIVE conj_AMAZE_CONSUME conj_AMAZE_CALCULATE conj_AMAZE_BURY conj_AMAZE_BOTHER conj_AMAZE_BET conj_AMAZE_BAND amod_AMAZE_followmg det_AMAZE_the conj_and_ichoms_phrases neg_ichoms_not dep_Senseval_phrases dep_Senseval_ichoms prep_in_used_Senseval vmod_senses_used dobj_verb_senses vmod_mmn_verb det_mmn_the parataxis_focuses_SHAKE prep_on_focuses_mmn nsubj_focuses_m nn_analysis_Tlus nn_analysis_Resources nn_analysis_Lexical det_analysis_The num_analysis_30 amod_30_other nn_Defimtlons_program nn_Defimtlons_conversmn det_Defimtlons_a prep_for_use_analysis prep_of_use_Defimtlons nn_dicUonanes_DIMAP prep_after_uploaded_use prep_into_uploaded_dicUonanes auxpass_uploaded_were nsubjpass_uploaded_defimtlons nn_defimtlons_Hector nn_DIMAP_m amod_DIMAP_mtegrated rcmod_words_uploaded appos_words_DIMAP amod_words_selected det_words_the nn_reformation_WordNet prep_for_extracuon_words prep_of_extracuon_reformation amod_extracuon_automatac dobj_includes_extracuon nsubj_includes_This appos_Research_1999a nn_Research_CL amod_download_immediate prep_at_available_Research prep_for_available_download nn_Programs_Maintenance nn_Programs_Dictionary dep_DIMAP_available appos_DIMAP_Programs nn_DIMAP_m amod_DIMAP_incorporated nn_DIMAP_functlonahty dep_using_includes dobj_using_DIMAP advmod_using_automatically xcomp_performed_using auxpass_performed_were nsubjpass_performed_paper det_paper_this rcmod_m_performed dep_described_focuses vmod_analyses_described nn_analyses_IAll nn_analyses_analysis det_analyses_the prep_to_add_analyses dobj_add_meanmg dobj_add_more aux_add_may nsubj_add_that rcmod_steps_add amod_steps_further dobj_outline_steps nsubj_outline_that advmod_far_thus conj_and_achieved_outline advmod_achieved_far auxpass_achieved_been aux_achieved_have nsubjpass_achieved_that rcmod_results_outline rcmod_results_achieved det_results_the dobj_identify_results nsubj_identify_We tmod_identify_m dep_identify_steps advmod_identify_first dep_identify_takang nsubj_identify_methods dep_identify_dictionaries amod_mformaUon_statistical amod_mformataon_full conj_negcc_meamng_mformaUon dep_meamng_mformataon prep_of_use_mformaUon prep_of_use_meamng det_use_the conj_and_Atkms_focusmg prep_on_described_use agent_described_focusmg agent_described_Atkms vmod_program_described det_program_the dobj_Implementing_program vmod_m_Implementing advmod_takang_essentially nn_patterns_defimng pobj_on_patterns pcomp_based_on prep_relations_based appos_relations_semrels amod_relations_semantic nn_relations_qng dep_defimtaons_relations conj_and_defimtaons_identff dobj_parsing_identff dobj_parsing_defimtaons prepc_of_methods_parsing det_methods_the advmod_methods_particularly prep_of_methods_analysis poss_methods_our dobj_describe_methods nsubj_describe_We rcmod_dictionaries_describe amod_dictionaries_other ccomp_include_identify aux_include_to poss_analysis_our vmod_expand_include dobj_expand_analysis advmod_expand_then nsubj_expand_We nsubj_expand_WordNet nn_We_Hector conj_and_WordNet_We nn_WordNet_m rcmod_definitions_expand det_definitions_the dobj_analyzing_definitions vmod_Senseval_analyzing nn_Senseval_m amod_Senseval_used dep_verbs_Senseval num_verbs_18 det_verbs_the prep_of_analysis_verbs dobj_overlap_analysis nsubj_overlap_first det_word_the dobj_perfornung_word prepc_of_results_perfornung amod_results_present dep_results_We nn_results_contmns advmod_present_then nn_properties_collocaUonal amod_components_semantac conj_and_properties_properties conj_and_properties_relaUons conj_and_properties_components conj_and_properties_features nn_properties_syntacUc appos_contmns_properties appos_contmns_relaUons appos_contmns_components appos_contmns_features appos_contmns_properties det_contmns_each nn_contmns_mformaUon amod_contmns_leracal prep_of_types_results det_types_the conj_and_ofpolysemy_types nn_ofpolysemy_terms prep_in_characterizing_types prep_in_characterizing_ofpolysemy dobj_characterizing_them vmod_first_characterizing nn_base_knowledge amod_base_cal nn_base_~ nn_base_lex det_base_a amod_dicUonanes_other conj_and_WordNet_base conj_and_WordNet_dicUonanes nn_WordNet_Hector rcmod_study_overlap dep_study_base dep_study_dicUonanes dep_study_WordNet det_study_the nn_study_m dobj_used_study vmod_resources_used amod_resources_lextcal det_resources_the dobj_describe_resources advmod_describe_first nsubj_describe_We dep_Krovetz_1992 mark_expected_as advcl_well_expected advmod_well_as neg_well_not advmod_perform_well aux_perform_to dep_shown_Krovetz xcomp_shown_perform auxpass_shown_been aux_shown_has nsubjpass_shown_method mark_shown_since nn_method_flus rcmod_mapping_describe dep_mapping_shown det_mapping_the dep_m_mapping dep_improvements_m dobj_make_improvements aux_make_to xcomp_able_make cop_able_are nsubj_able_we mark_able_whether ccomp_compare_able aux_compare_to prep_against_compare_which dep_Lesk_1986 dep_m_Lesk xcomp_suggested_compare dep_suggested_m vmod_method_suggested dobj_overlap_method nsubj_overlap_word det_word_a dep_baseline_overlap amod_baseline_usable det_baseline_a dobj_have_baseline nsubj_have_we amod_resources_lexical dobj_compare_resources aux_compare_to vmod_efforts_compare prep_for_standard_efforts nn_standard_gold amod_standard_usable det_standard_a advmod_stands_Moreover prep_as_stands_standard amod_map_WordNet-Hector det_map_the prep_about_disclaimers_map nsubj_were_disclaimers expl_were_there mark_were_Although amod_degree_unknown det_degree_an poss_performance_their parataxis_degraded_have dep_degraded_stands advmod_degraded_nonetheless dobj_degraded_it advcl_degraded_were prep_to_degraded_degree dobj_degraded_performance aux_degraded_have aux_degraded_may nsubj_degraded_use mark_degraded_that amod_map_flus prep_of_use_map ccomp_concerns_degraded dobj_expressed_concerns nsubj_expressed_cipants nn_cipants_~ nn_cipants_part det_cipants_some num_inventories_two det_inventories_the prep_between_map_inventories det_map_a prep_of_development_map det_development_the cop_development_was nsubj_development_solutaon det_problem_the prep_to_solutaon_problem amod_solutaon_hasty nn_solutaon_A nn_solutaon_Hecto0 rcmod_inventory_development nn_inventory_sense det_inventory_another dobj_using_inventory prepc_of_necessity_using det_necessity_the prep_with_faced_necessity auxpass_faced_were nsubjpass_faced_systems advmod_faced_when nn_inventory_sense dep_inventory_Miller nn_inventory_WordNet det_inventory_the nn_al_et amod_Miller_1990 dep_Miller_al prep_on_relied_inventory advmod_relied_heavily nsubj_relied_that rcmod_systems_relied det_systems_some amod_IOlgarnff_1998 conj_but_competatton_expressed rcmod_competatton_faced dep_competatton_IOlgarnff amod_competatton_Senseval amod_competatton_recent det_competatton_the det_fore_the prep_in_came_expressed prep_in_came_competatton prep_to_came_fore amod_Atlans_1991 appos_hnguistlcs_Atlans amod_hnguistlcs_computauonal amod_challenge_gnfficant conj_~_came prep_in_~_hnguistlcs dobj_~_challenge nsubj_~_s advmod_~_long det_s_a amod_resources_lemcal nn_resources_companng dep_difficulty_~ prep_of_difficulty_resources det_difficulty_The dobj_Introduction_difficulty dep_aclueved_Introduction auxpass_aclueved_be aux_aclueved_can nsubjpass_aclueved_improvements mark_aclueved_that amod_improvements_further amod_improvements_considerable ccomp_suggests_aclueved nn_approach_analysis amod_approach_componenUal det_approach_the prep_of_implementation_approach amod_implementation_maaal det_implementation_an advmod_implementation_only nn_implementation_consUtute amod_implementation_described advmod_described_here conj_and_tecbauques_suggests dep_tecbauques_implementation det_tecbauques_The dep_tecbauques_e num_tecbauques_~ dep_\_suggests dep_\_tecbauques nn_\_sco amod_\_less amod_\_more conj_and_more_less prep_of_sets_\ amod_sets_defimtton prep_between_process_sets nn_process_mapping det_process_the prep_of_nature_process det_nature_the dobj_exanune_nature aux_exanune_to nn_database_Hector det_database_the conj_and_WordNet_database xcomp_added_exanune prep_to_added_database prep_to_added_WordNet auxpass_added_were dep_added_inflated nsubjpass_added_mapping nn_base_knowledge amod_base_lextcal poss_base_Dorr conj_and_thclaonanes_base amod_thclaonanes_pubhshed num_thclaonanes_three amod_words_Senseval det_words_the prep_of_sets_words nn_sets_Defimtion nn_sets_mapping amod_sets_Senseval det_sets_the prep_from_used_base prep_from_used_thclaonanes prep_in_used_sets neg_used_not amod_components_semantic nn_components_recogmzmg vmod_percent_used prep_by_percent_components num_percent_4 amod_percent_additaonal det_percent_an amod_mapping_correct dep_percent_mapping num_percent_41 det_percent_an quantmod_41_almost dobj_provides_percent amod_parsing_thclaonary dep_the_parsing prep_mto_seamlessly_the advmod_integrated_seamlessly vmod_patterns_integrated nn_patterns_ofdefimng nn_patterns_use det_patterns_the nn_identification_relation nn_component_semantac conj_and_syntacUc_identification conj_and_syntacUc_component conj_and_syntacUc_collocatmnal prep_through_using_patterns dobj_using_identification dobj_using_component dobj_using_collocatmnal dobj_using_syntacUc det_defimtaons_the prep_of_analysis_defimtaons amod_analysis_componenttal nn_analysis_alternaUve det_analysis_An dep_empty_assignments dep_empty_from num_percent_9 prep_with_inflated_percent conj_inflated_provides vmod_inflated_using dobj_inflated_analysis dep_inflated_empty prep_by_inflated_percent amod_mapping_correct vmod_percent_added num_percent_36 det_percent_a dep_most_percent prep_at_achieving_most vmod_flus_mapping nn_list_stop det_list_a prep_for_analysis_flus prep_without_analysis_list prep_with_analysis_list conj_and_analysis_analysis vmod_overlap_achieving dobj_overlap_analysis dobj_overlap_analysis det_word_a dep_provided_overlap prep_through_provided_word auxpass_provided_is dep_provided_judge dep_provided_provides amod_standard_gold det_standard_The amod_resources_lexlcal dobj_compare_resources aux_compare_to vmod_ability_compare poss_ability_our dep_judge_standard dobj_judge_ability aux_judge_to amod_standard_gold det_standard_a prep_against_provides_wluch dobj_provides_standard csubj_provides_tom nn_Senseval_m nn_Senseval_senses nn_Senseval_Hector prep_to_mapping_Senseval prep_from_mapping_WordNet det_mapping_The amod_mapping_Abstract dobj_tom_mapping nsubj_tom_tires prep_towards_tom_Comparison nn_tires_www nn_http_corn nn_http_ken@clres num_http_20872 nn_http_MD appos_Damascus_http nn_Damascus_Road nn_Damascus_Gue num_Damascus_9208 dep_Research_Damascus nn_Research_CL nn_Research_Lltkowska nn_Research_C nn_Research_Kenneth nn_Research_Resources nn_Research_Lexieal prep_of_Comparison_Research nn_Comparison_Meaning-Full det_Comparison_a
C08-1009	C98-2122	o	On the British National Corpus -LRB- BNC -RRB- using Lins -LRB- 1998 -RRB- similarity method we retrieve the following neighbors for the first and second sense respectively 1	amod_sense_second amod_sense_first det_sense_the conj_and_first_second prep_for_neighbors_sense amod_neighbors_following det_neighbors_the dep_retrieve_1 advmod_retrieve_respectively dobj_retrieve_neighbors nsubj_retrieve_we vmod_retrieve_using prep_on_retrieve_Corpus nn_method_similarity nn_method_Lins appos_Lins_1998 dobj_using_method appos_Corpus_BNC nn_Corpus_National nn_Corpus_British det_Corpus_the
C08-1009	C98-2122	o	As described in Section 3 we retrieved neighbors using Lins -LRB- 1998 -RRB- similarity measure on a RASP parsed -LRB- Briscoe and Carroll 2002 -RRB- version of the BNC	det_BNC_the prep_of_version_BNC dep_version_Carroll dep_version_Briscoe amod_version_parsed dep_Briscoe_2002 conj_and_Briscoe_Carroll det_RASP_a nn_measure_similarity nn_measure_Lins appos_Lins_1998 dep_using_version prep_on_using_RASP dobj_using_measure xcomp_retrieved_using dobj_retrieved_neighbors nsubj_retrieved_we advcl_retrieved_described num_Section_3 prep_in_described_Section mark_described_As
C08-1009	C98-2122	o	The best accuracies are observed when the labelsarecreatedfromdistributionallysimilarwords using Lins -LRB- 1998 -RRB- dependency-based similarity measure -LRB- Depend -RRB-	appos_measure_Depend nn_measure_similarity amod_measure_dependency-based nn_measure_Lins appos_Lins_1998 dobj_using_measure vmod_labelsarecreatedfromdistributionallysimilarwords_using det_labelsarecreatedfromdistributionallysimilarwords_the dep_when_labelsarecreatedfromdistributionallysimilarwords prep_observed_when auxpass_observed_are nsubjpass_observed_accuracies amod_accuracies_best det_accuracies_The
C08-1009	C98-2122	p	Lins -LRB- 1998 -RRB- information-theoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD -LRB- McCarthy et al. 2004 -RRB-	amod_McCarthy_2004 dep_McCarthy_al. nn_McCarthy_et amod_WSD_unsupervised prep_in_performance_WSD amod_performance_good dobj_demonstrated_performance aux_demonstrated_has nsubjpass_demonstrated_measure nn_tasks_acquisition nn_tasks_lexicon dep_used_McCarthy conj_and_used_demonstrated prep_in_used_tasks advmod_used_commonly auxpass_used_is nsubjpass_used_measure nn_measure_similarity amod_measure_information-theoretic nn_measure_Lins dep_Lins_1998
C08-1009	C98-2122	n	A potential caveat with Lins -LRB- 1998 -RRB- distributional similarity measure is its reliance on syntactic information for obtaining dependency relations	nn_relations_dependency dobj_obtaining_relations amod_information_syntactic prepc_for_reliance_obtaining prep_on_reliance_information poss_reliance_its cop_reliance_is nsubj_reliance_caveat nn_measure_similarity amod_measure_distributional nn_measure_Lins appos_Lins_1998 prep_with_caveat_measure amod_caveat_potential det_caveat_A
C08-1029	C98-2122	p	Point-wise mutual information -LRB- Lin 1998 -RRB- and Relative Feature Focus -LRB- Geffet and Dagan 2004 -RRB- are well-known examples	amod_examples_well-known cop_examples_are nsubj_examples_Focus nsubj_examples_information dep_Geffet_2004 conj_and_Geffet_Dagan appos_Focus_Dagan appos_Focus_Geffet nn_Focus_Feature amod_Focus_Relative amod_Lin_1998 conj_and_information_Focus dep_information_Lin amod_information_mutual amod_information_Point-wise
C08-1029	C98-2122	o	Feature comparison measures to convert two feature sets into a scalar value several measures have been proposed such as cosine Lins measure -LRB- Lin 1998 -RRB- Kullback-Leibler -LRB- KL -RRB- divergence and its variants	poss_variants_its nn_divergence_Kullback-Leibler appos_Kullback-Leibler_KL amod_Lin_1998 dep_measure_Lin nn_measure_Lins conj_and_cosine_variants conj_and_cosine_divergence conj_and_cosine_measure prep_such_as_proposed_variants prep_such_as_proposed_divergence prep_such_as_proposed_measure prep_such_as_proposed_cosine auxpass_proposed_been aux_proposed_have nsubjpass_proposed_measures amod_measures_several amod_value_scalar det_value_a prep_into_sets_value nsubj_sets_feature num_feature_two parataxis_convert_proposed ccomp_convert_sets aux_convert_to dep_measures_convert nn_measures_comparison nn_measures_Feature
C08-1029	C98-2122	o	Lins measure Lin -LRB- 1998 -RRB- proposed a symmetrical measure Par Lin -LRB- s t -RRB- = summationtext fF s F t -LRB- w -LRB- s f -RRB- + w -LRB- t f -RRB- -RRB- summationtext fF s w -LRB- s f -RRB- + summationtext fF t w -LRB- t f -RRB- where F s and F t denote sets of features with positive weights for words s and t respectively	conj_and_s_t nn_s_words prep_for_weights_t prep_for_weights_s amod_weights_positive prep_with_features_weights prep_of_sets_features dobj_denote_sets nsubj_denote_t nsubj_denote_s advmod_denote_where nn_t_F conj_and_s_t nn_s_F amod_t_f advmod_w_respectively rcmod_w_denote appos_w_t nn_w_t nn_w_fF nn_w_summationtext conj_+_f_w nsubj_f_Lin nn_s_w dep_s_s nsubj_s_fF dep_s_t nn_fF_summationtext nn_t_w dep_s_f appos_w_f conj_+_w_t appos_w_s dep_t_t dep_t_w nn_t_F dobj_s_s nsubj_s_fF nn_fF_summationtext dep_=_s nn_t_s amod_Lin_= appos_Lin_t nn_Lin_Par dep_measure_w dep_measure_f amod_measure_symmetrical det_measure_a dobj_proposed_measure nsubj_proposed_Lin appos_Lin_1998 ccomp_measure_proposed nsubj_measure_Lins
C08-1051	C98-2122	o	Three K-means algorithms using different distributional similarity or dissimilarity measures cosine skew divergence -LRB- Lee 1999 -RRB- 4 and Lins similarity -LRB- Lin 1998 -RRB-	amod_Lin_1998 dep_similarity_Lin nn_similarity_Lins dep_Lee_1999 conj_and_skew_similarity dep_skew_4 dep_skew_Lee dobj_skew_divergence dep_cosine_similarity dep_cosine_skew nn_measures_dissimilarity conj_or_similarity_measures amod_similarity_distributional amod_similarity_different dobj_using_measures dobj_using_similarity dep_algorithms_cosine vmod_algorithms_using nn_algorithms_K-means num_algorithms_Three ccomp_''_algorithms
C08-1051	C98-2122	o	Others proposed distributional similarity measures between words -LRB- Hindle 1990 Lin 1998 Lee 1999 Weeds et al. 2004 -RRB-	amod_Weeds_2004 dep_Weeds_al. nn_Weeds_et num_Lee_1999 dep_Lin_Weeds conj_Lin_Lee num_Lin_1998 dep_Hindle_Lin appos_Hindle_1990 dep_words_Hindle prep_between_measures_words nn_measures_similarity amod_measures_distributional dobj_proposed_measures nsubj_proposed_Others ccomp_``_proposed
C08-1051	C98-2122	o	405 PRF 1 proposed .383 .437 .408 multinomial mixture .360 .374 .367 Newman -LRB- 2004 -RRB- .318 .353 .334 cosine .603 .114 .192 skew divergence -LRB- Lee 1999 -RRB- .730 .155 .255 Lins similarity -LRB- Lin 1998 -RRB- .691 .096 .169 CBC -LRB- Lin and Pantel 2002 -RRB- .981 .060 .114 Table 3 Precision recall and F-measure	conj_and_Precision_F-measure conj_and_Precision_recall num_Table_3 num_Table_.114 num_Table_.060 num_Table_.981 dep_Lin_2002 conj_and_Lin_Pantel dep_CBC_Table dep_CBC_Pantel dep_CBC_Lin num_CBC_.169 number_.169_.096 number_.169_.691 dep_Lin_1998 dep_similarity_CBC dep_similarity_Lin nn_similarity_Lins num_similarity_.255 num_similarity_.155 num_similarity_.730 dep_Lee_1999 dep_skew_F-measure dep_skew_recall dep_skew_Precision dep_skew_similarity dep_skew_Lee dobj_skew_divergence num_.192_.114 number_.114_.603 num_cosine_.192 dep_.334_cosine dep_.353_.334 number_.353_.318 dep_2004_.353 appos_Newman_2004 num_Newman_.367 num_Newman_.374 num_Newman_.360 dep_mixture_Newman amod_mixture_multinomial num_mixture_.408 dep_.437_mixture dep_.383_.437 dep_proposed_.383 nsubj_proposed_1 dep_PRF_skew rcmod_PRF_proposed num_PRF_405 dep_``_PRF
C08-1051	C98-2122	o	Applications of word clustering include language modeling -LRB- Brown et al. 1992 -RRB- text classification -LRB- Baker and McCallum 1998 -RRB- thesaurus construction -LRB- Lin 1998 -RRB- and so on	advmod_on_so dep_Lin_1998 appos_construction_Lin nn_construction_thesaurus dep_Baker_1998 conj_and_Baker_McCallum dep_classification_McCallum dep_classification_Baker nn_classification_text dep_al._1992 nn_al._et amod_al._Brown conj_and_modeling_on conj_and_modeling_construction appos_modeling_classification dep_modeling_al. nn_modeling_language dobj_include_on dobj_include_construction dobj_include_modeling nsubj_include_Applications nn_clustering_word prep_of_Applications_clustering
C08-1054	C98-2122	o	-LRB- 2005 -RRB- applied the distributional similarity proposed by Lin -LRB- 1998 -RRB- to coordination disambiguation	nn_disambiguation_coordination appos_Lin_1998 agent_proposed_Lin prep_to_similarity_disambiguation vmod_similarity_proposed amod_similarity_distributional det_similarity_the dobj_applied_similarity nsubj_applied_2005
C08-1058	C98-2122	o	One is automatic thesaurus acquisition that is to identify synonyms or topically related words from corpora based on various measures of similarity -LRB- e.g. Riloff and Shepherd 1997 Lin 1998 Caraballo 1999 Thelen and Riloff 2002 You and Chen 2006 -RRB-	dep_You_2006 conj_and_You_Chen num_Thelen_2002 conj_and_Thelen_Riloff num_Caraballo_1999 dep_Lin_Chen dep_Lin_You conj_Lin_Riloff conj_Lin_Thelen conj_Lin_Caraballo num_Lin_1998 num_Shepherd_1997 dep_Riloff_Lin conj_and_Riloff_Shepherd nn_Riloff_e.g. dep_measures_Shepherd dep_measures_Riloff prep_of_measures_similarity amod_measures_various amod_words_related advmod_related_topically conj_or_synonyms_words pobj_identify_measures prepc_based_on_identify_on prep_from_identify_corpora dobj_identify_words dobj_identify_synonyms aux_identify_to aux_identify_is nsubj_identify_that rcmod_acquisition_identify nn_acquisition_thesaurus amod_acquisition_automatic cop_acquisition_is nsubj_acquisition_One
C08-1086	C98-2122	o	By no means an exhaustive list the most commonly cited ranking and scoring algorithms are HITS -LRB- Kleinberg 1998 -RRB- and PageRank -LRB- Page et al. 1998 -RRB- which rank hyperlinked documents using the concepts of hubs and authorities	conj_and_hubs_authorities prep_of_concepts_authorities prep_of_concepts_hubs det_concepts_the dobj_using_concepts vmod_documents_using amod_documents_hyperlinked dobj_rank_documents nsubj_rank_which advmod_1998_al. nn_al._et num_Page_1998 num_Kleinberg_1998 dep_HITS_Page conj_and_HITS_PageRank dep_HITS_Kleinberg cop_HITS_are nsubj_HITS_algorithms nsubj_HITS_ranking amod_algorithms_scoring conj_and_ranking_algorithms amod_ranking_cited det_ranking_the advmod_cited_commonly advmod_cited_most rcmod_list_rank rcmod_list_PageRank rcmod_list_HITS amod_list_exhaustive det_list_an dobj_means_list prep_by_means_no
C08-1086	C98-2122	o	Within the NLP community n-best list ranking has been looked at carefully in parsing extractive summarization -LRB- Barzilay et al. 1999 Hovy and Lin 1998 -RRB- and machine translation -LRB- Zhang et al. 2006 -RRB- to name a few	det_few_a dobj_name_few aux_name_to dep_2006_al. nn_al._et num_Zhang_2006 nn_translation_machine num_Lin_1998 conj_and_Hovy_Lin dep_Barzilay_Lin dep_Barzilay_Hovy dep_Barzilay_1999 dep_Barzilay_al. nn_Barzilay_et vmod_summarization_name appos_summarization_Zhang conj_and_summarization_translation appos_summarization_Barzilay amod_summarization_extractive pobj_in_parsing advmod_in_carefully pcomp_at_in dep_looked_translation dep_looked_summarization prep_looked_at auxpass_looked_been aux_looked_has nsubjpass_looked_ranking prep_within_looked_community nn_ranking_list amod_ranking_n-best nn_community_NLP det_community_the
C08-1086	C98-2122	o	Following Lin -LRB- 1998 -RRB- we use syntactic dependencies between words to model their semantic properties	amod_properties_semantic poss_properties_their dobj_model_properties aux_model_to vmod_words_model amod_dependencies_syntactic prep_between_use_words dobj_use_dependencies nsubj_use_we prep_following_use_Lin appos_Lin_1998
C08-1100	C98-2122	o	For each word in the LDV we consulted three existing thesauri Rogets Thesaurus -LRB- Roget 1995 -RRB- Collins COBUILD Thesaurus -LRB- Collins 2002 -RRB- and WordNet -LRB- Fellbaum 1998 -RRB-	amod_Fellbaum_1998 dep_WordNet_Fellbaum dep_Collins_2002 appos_Thesaurus_Collins nn_Thesaurus_COBUILD nn_Thesaurus_Collins amod_Roget_1995 conj_and_Thesaurus_WordNet conj_and_Thesaurus_Thesaurus dep_Thesaurus_Roget nn_Thesaurus_Rogets dep_thesauri_WordNet dep_thesauri_Thesaurus dep_thesauri_Thesaurus amod_thesauri_existing num_thesauri_three dobj_consulted_thesauri nsubj_consulted_we prep_for_consulted_word det_LDV_the prep_in_word_LDV det_word_each
C08-1100	C98-2122	o	Various methods -LRB- Hindle 1990 Lin 1998 -RRB- of automatically acquiring synonyms have been proposed	auxpass_proposed_been aux_proposed_have nsubjpass_proposed_Hindle dobj_acquiring_synonyms advmod_acquiring_automatically num_Lin_1998 prepc_of_Hindle_acquiring dep_Hindle_Lin appos_Hindle_1990 rcmod_methods_proposed amod_methods_Various
C08-1100	C98-2122	p	4.1 Features We used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies -LRB- Ruge 1997 Lin 1998 -RRB-	dep_Lin_1998 dep_Ruge_Lin appos_Ruge_1997 dep_studies_Ruge amod_studies_past det_studies_the prep_in_information_studies amod_information_contextual amod_information_performing amod_information_best det_information_the prep_of_one_information conj_and_used_one advmod_used_widely det_used_the auxpass_used_is nsubjpass_used_it mark_used_because advmod_widely_most det_context_the nn_structure_dependency det_structure_a advcl_used_one advcl_used_used prep_for_used_words prep_as_used_context dobj_used_structure nsubj_used_We rcmod_Features_used num_Features_4.1
C08-1107	C98-2122	o	Given a wordq its set of featuresFq and feature weightswq -LRB- f -RRB- for f Fq a common symmetric similarity measure is Lin similarity -LRB- Lin 1998a -RRB- Lin -LRB- u v -RRB- = summationtext fFuFv -LSB- wu -LRB- f -RRB- + wv -LRB- f -RRB- -RSB- summationtext fFu wu -LRB- f -RRB- + summationtext fFv wv -LRB- f -RRB- where the weight of each feature is the pointwise mutual information -LRB- pmi -RRB- between the word and the feature wq -LRB- f -RRB- = log -LSB- Pr -LRB- f | q -RRB- Pr -LRB- f -RRB- -RSB-	dep_Pr_f nn_Pr_Pr nn_q_| dep_f_q appos_Pr_f nn_Pr_log dep_=_Pr dep_f_= dep_wq_f det_feature_the conj_and_word_feature det_word_the dep_information_wq prep_between_information_feature prep_between_information_word dep_information_pmi amod_information_mutual amod_information_pointwise det_information_the cop_information_is nsubj_information_weight advmod_information_where det_feature_each prep_of_weight_feature det_weight_the dep_f_information appos_wv_f nn_wv_fFv nn_wv_summationtext pobj_+_wv cc_f_+ dep_wu_f nn_wu_fFu nn_wu_summationtext dep_wu_wv dep_wv_f conj_+_f_wu dep_wu_wu dep_wu_f dep_fFuFv_wu nn_fFuFv_summationtext dobj_=_fFuFv dep_u_v dep_Lin_= dep_Lin_u appos_Lin_1998a dep_similarity_Lin dep_similarity_Lin nn_similarity_Lin cop_similarity_is nsubj_similarity_measure nn_measure_similarity amod_measure_symmetric amod_measure_common det_measure_a nn_Fq_f nn_weightswq_feature dep_featuresFq_f conj_and_featuresFq_weightswq dep_set_similarity prep_for_set_Fq prep_of_set_weightswq prep_of_set_featuresFq poss_set_its advcl_,_set det_wordq_a pobj_Given_wordq dep_``_Given
C08-1107	C98-2122	o	Texts are represented by dependency parse trees -LRB- using the Minipar parser -LRB- Lin 1998b -RRB- -RRB- and templates by parse sub-trees	nn_sub-trees_parse prep_by_templates_sub-trees dep_Lin_1998b dep_parser_Lin nn_parser_Minipar det_parser_the conj_and_using_templates dobj_using_parser nn_trees_parse nn_trees_dependency dep_represented_templates dep_represented_using agent_represented_trees auxpass_represented_are nsubjpass_represented_Texts
C08-1117	C98-2122	p	Among these measures the most important are Wu & Palmers -LRB- Wu and Palmer 1994 -RRB- Resniks -LRB- Resnik 1995 -RRB- and Lins -LRB- Lin 1998 -RRB-	amod_Lin_1998 dep_Lins_Lin amod_Resnik_1995 dep_Resniks_Resnik dep_Wu_1994 conj_and_Wu_Palmer conj_and_Wu_Lins conj_and_Wu_Resniks dep_Wu_Palmer dep_Wu_Wu conj_and_Wu_Palmers cop_Wu_are nsubj_Wu_important prep_among_Wu_measures advmod_important_most det_important_the det_measures_these
C08-1117	C98-2122	o	Where Pantel and Lin use Lins -LRB- 1998 -RRB- measure we use Wu and Palmers -LRB- 1994 -RRB- measure	nn_measure_Palmers nn_measure_Wu appos_Palmers_1994 conj_and_Wu_Palmers dobj_use_measure nsubj_use_we advcl_use_use nn_measure_Lins appos_Lins_1998 dobj_use_measure nsubj_use_Lin nsubj_use_Pantel advmod_use_Where conj_and_Pantel_Lin
C08-1117	C98-2122	p	One of the most important is Lins -LRB- 1998 -RRB-	appos_Lins_1998 cop_Lins_is nsubj_Lins_One advmod_important_most det_important_the prep_of_One_important
D08-1007	C98-2122	o	4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus -LRB- Voorhees 2002 -RRB- using Minipar -LRB- Lin 1998b -RRB- and collected verb-object and verb-subject frequencies building an empirical MI model from this data	det_data_this prep_from_model_data nn_model_MI amod_model_empirical det_model_an dobj_building_model amod_frequencies_verb-subject amod_frequencies_verb-object amod_frequencies_collected conj_and_verb-object_verb-subject appos_Lin_1998b conj_and_Minipar_frequencies dep_Minipar_Lin vmod_using_building dobj_using_frequencies dobj_using_Minipar dep_Voorhees_2002 appos_corpus_Voorhees nn_corpus_AQUAINT nn_corpus_GB num_corpus_3 det_corpus_the vmod_parsed_using dobj_parsed_corpus nsubj_parsed_We ccomp_Set_parsed prt_Set_up nsubj_Set_4.1 rcmod_Results_Set conj_and_Experiments_Results num_Experiments_4
D08-1007	C98-2122	o	Lin -LRB- 1998a -RRB- s similar word list for eat misses these but includes sleep -LRB- ranked 6 -RRB- and sit -LRB- ranked 14 -RRB- because these have similar subjects to eat	aux_eat_to vmod_subjects_eat amod_subjects_similar dobj_have_subjects nsubj_have_these mark_have_because dobj_ranked_14 dep_sit_ranked dobj_ranked_6 conj_and_sleep_sit dep_sleep_ranked dobj_includes_sit dobj_includes_sleep conj_but_these_includes dep_eat_includes dep_eat_these dobj_eat_misses nn_list_word amod_list_similar advcl_s_have prepc_for_s_eat dobj_s_list nsubj_s_Lin appos_Lin_1998a
D08-1007	C98-2122	o	Discriminative context-specific training seems to yield a better set of similar predicates e.g. the highest-ranked contexts for DSPcooc on the verb join ,3 lead 1.42 rejoin 1.39 form 1.34 belong to 1.31 found 1.31 quit 1.29 guide 1.19 induct 1.19 launch -LRB- subj -RRB- 1.18 work at 1.14 give a better SIMS -LRB- join -RRB- for Equation -LRB- 1 -RRB- than the top similarities returned by -LRB- Lin 1998a -RRB- participate 0.164 lead 0.150 return to 0.148 say 0.143 rejoin 0.142 sign 0.142 meet 0.142 include 0.141 leave 0.140 work 0.137 Other features are also weighted intuitively	advmod_weighted_intuitively advmod_weighted_also cop_weighted_are nsubj_weighted_features amod_features_Other num_features_0.137 nn_features_work rcmod_leave_weighted dobj_leave_0.140 dobj_include_0.141 dobj_meet_0.142 nsubj_meet_0.142 num_sign_0.142 appos_0.142_sign ccomp_rejoin_meet dep_say_leave conj_say_include conj_say_rejoin dobj_say_0.143 nsubj_say_0.150 prep_to_return_0.148 appos_0.150_return dep_lead_say dep_participate_lead dobj_participate_0.164 appos_Lin_1998a agent_returned_Lin dep_similarities_participate vmod_similarities_returned amod_similarities_top det_similarities_the appos_Equation_1 appos_SIMS_join amod_SIMS_better det_SIMS_a prep_than_give_similarities prep_for_give_Equation dobj_give_SIMS nsubj_give_quit parataxis_give_seems prep_at_work_1.14 num_launch_1.18 appos_launch_subj dobj_induct_1.19 num_guide_1.19 conj_quit_work conj_quit_launch conj_quit_induct conj_quit_guide dobj_quit_1.29 dobj_found_1.31 prep_to_belong_1.31 nsubj_belong_rejoin num_form_1.34 appos_1.39_form dobj_rejoin_1.39 amod_1.42_lead num_1.42_,3 dobj_join_1.42 ccomp_verb_join amod_the_verb prep_on_DSPcooc_the prep_for_contexts_DSPcooc amod_contexts_highest-ranked det_contexts_the pobj_e.g._contexts amod_predicates_similar prep_of_set_predicates amod_set_better det_set_a dobj_yield_set aux_yield_to xcomp_seems_found ccomp_seems_belong prep_seems_e.g. xcomp_seems_yield nsubj_seems_training advmod_seems_Discriminative amod_training_context-specific
D08-1007	C98-2122	o	We also test an MI model inspired by Erk -LRB- 2007 -RRB- MISIM -LRB- n v -RRB- = log summationdisplay nSIMS -LRB- n -RRB- Sim -LRB- n n -RRB- Pr -LRB- v n -RRB- Pr -LRB- v -RRB- Pr -LRB- n -RRB- We gather similar words using Lin -LRB- 1998a -RRB- mining similar verbs from a comparable-sized parsed corpus and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata -LRB- 2003 -RRB- s approach to obtaining web-counts	dobj_obtaining_web-counts prepc_to_approach_obtaining nn_approach_s appos_Lapata_2003 conj_and_Keller_Lapata dobj_use_approach dobj_use_Lapata dobj_use_Keller advmod_use_also nsubj_use_We prep_from_use_corpus num_text_.4 nn_text_English prep_of_corpus_text nn_corpus_GB num_corpus_10 amod_corpus_broader det_corpus_a amod_nouns_similar amod_corpus_parsed amod_corpus_comparable-sized det_corpus_a prep_from_verbs_corpus amod_verbs_similar nn_verbs_mining appos_Lin_verbs appos_Lin_1998a rcmod_using_use dobj_using_nouns conj_and_using_collecting dobj_using_Lin amod_words_similar xcomp_gather_collecting xcomp_gather_using dobj_gather_words nsubj_gather_We rcmod_Pr_gather appos_Pr_n dep_Pr_v dep_Pr_Pr dep_Pr_v dep_v_n appos_n_n dep_Sim_n nn_Sim_nSIMS appos_nSIMS_n nn_nSIMS_summationdisplay nn_nSIMS_log dep_=_Pr dep_=_Sim dep_n_Pr dep_n_= dep_n_v dep_MISIM_n dep_Erk_MISIM appos_Erk_2007 agent_inspired_Erk vmod_model_inspired nn_model_MI det_model_an dobj_test_model advmod_test_also nsubj_test_We ccomp_``_test
D08-1007	C98-2122	p	Erk -LRB- 2007 -RRB- compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin -LRB- 1998a -RRB- s information-theoretic metric work best	amod_work_metric amod_work_information-theoretic advmod_s_best dobj_s_work nsubj_s_Lin nsubj_s_coefficient mark_s_that appos_Lin_1998a conj_and_coefficient_Lin nn_coefficient_Jaccard det_coefficient_the preconj_coefficient_both ccomp_found_s amod_sets_similar-word dobj_creating_sets prepc_for_number_creating prep_of_number_techniques det_number_a pobj_compared_number conj_and_Erk_found prep_Erk_compared appos_Erk_2007
D08-1048	C98-2122	o	They have been successfully applied in several tasks such as information retrieval -LRB- Salton et al. 1975 -RRB- and harvesting thesauri -LRB- Lin 1998 -RRB-	amod_Lin_1998 dep_thesauri_Lin dobj_harvesting_thesauri amod_Salton_1975 dep_Salton_al. nn_Salton_et conj_and_retrieval_harvesting dep_retrieval_Salton nn_retrieval_information prep_such_as_tasks_harvesting prep_such_as_tasks_retrieval amod_tasks_several prep_in_applied_tasks advmod_applied_successfully auxpass_applied_been aux_applied_have nsubjpass_applied_They
D08-1048	C98-2122	o	Two LUs close in the space are likely to be in a paradigmatic relation i.e. to be close in a is-a hierarchy -LRB- Budanitsky and Hirst 2006 Lin 1998 Pado 2007 -RRB-	amod_Pado_2007 num_Lin_1998 dep_Budanitsky_Pado conj_and_Budanitsky_Lin conj_and_Budanitsky_2006 conj_and_Budanitsky_Hirst amod_hierarchy_is-a det_hierarchy_a prep_in_close_hierarchy cop_close_be aux_close_to dep_i.e._Lin dep_i.e._2006 dep_i.e._Hirst dep_i.e._Budanitsky vmod_i.e._close amod_relation_paradigmatic det_relation_a prep_in_be_relation aux_be_to dep_likely_i.e. xcomp_likely_be cop_likely_are nsubj_likely_LUs det_space_the prep_in_close_space amod_LUs_close num_LUs_Two
D08-1084	C98-2122	p	This similarity score is computed as a max over a number of component scoring functions some based on external lexical resources including various string similarity functions of which most are applied to word lemmas measures of synonymy hypernymy antonymy and semantic relatedness including a widelyused measure due to Jiang and Conrath -LRB- 1997 -RRB- based on manually constructed lexical resources such as WordNet and NomBank a function based on the well-known distributional similarity metric of Lin -LRB- 1998 -RRB- which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources both manually and automatically constructedis critical to the success of MANLI	prep_of_success_MANLI det_success_the prep_to_critical_success amod_constructedis_critical dep_manually_constructedis conj_and_manually_automatically preconj_manually_both amod_resources_lexical amod_resources_external nn_resources_leverage advmod_ability_automatically advmod_ability_manually prep_to_ability_resources det_ability_The nn_text_English prep_of_corpus_text amod_corpus_large det_corpus_a advmod_large_very poss_distributions_their conj_and_words_phrases prep_from_similarity_distributions prep_of_similarity_phrases prep_of_similarity_words dep_infers_ability prep_in_infers_corpus dobj_infers_similarity advmod_infers_automatically nsubj_infers_which rcmod_Lin_infers appos_Lin_1998 prep_of_metric_Lin nn_metric_similarity amod_metric_distributional amod_metric_well-known det_metric_the prep_on_based_metric det_function_a dep_WordNet_function conj_and_WordNet_NomBank vmod_resources_based prep_such_as_resources_NomBank prep_such_as_resources_WordNet amod_resources_lexical amod_resources_constructed advmod_constructed_manually prep_on_based_resources appos_Conrath_1997 conj_and_Jiang_Conrath vmod_measure_based prep_due_to_measure_Conrath prep_due_to_measure_Jiang amod_measure_widelyused det_measure_a amod_relatedness_semantic conj_and_synonymy_relatedness conj_and_synonymy_antonymy conj_and_synonymy_hypernymy prep_including_measures_measure prep_of_measures_relatedness prep_of_measures_antonymy prep_of_measures_hypernymy prep_of_measures_synonymy nn_measures_lemmas nn_measures_word prep_to_applied_measures auxpass_applied_are advmod_applied_most prep_of_applied_which rcmod_functions_applied nn_functions_similarity nn_functions_string amod_functions_various prep_including_resources_functions amod_resources_lexical amod_resources_external prep_on_based_resources vmod_some_based dobj_scoring_functions vmod_component_scoring prep_of_number_component det_number_a prep_over_max_number det_max_a xcomp_computed_some prep_as_computed_max auxpass_computed_is nsubjpass_computed_score nn_score_similarity det_score_This
D08-1103	C98-2122	o	Distributional measures of distance such as those proposed by Lin -LRB- 1998 -RRB- quantify how similar the two sets of contexts of a target word pair are	nsubj_are_sets dep_are_similar nn_pair_word nn_pair_target det_pair_a prep_of_contexts_pair prep_of_sets_contexts num_sets_two det_sets_the advmod_similar_how ccomp_quantify_are nsubj_quantify_measures appos_Lin_1998 agent_proposed_Lin vmod_those_proposed prep_such_as_measures_those prep_of_measures_distance amod_measures_Distributional
D08-1103	C98-2122	o	For each word pair from the antonym set we calculated the distributional distance between each of their senses using Mohammad and Hirsts -LRB- 2006 -RRB- method of concept distance along with the modified form of Lins -LRB- 1998 -RRB- distributional measure -LRB- equation 2 -RRB-	num_equation_2 amod_measure_distributional nn_measure_Lins appos_Lins_1998 appos_form_equation prep_of_form_measure amod_form_modified det_form_the nn_distance_concept pobj_method_form prepc_along_with_method_with prep_of_method_distance nsubj_method_Hirsts nsubj_method_Mohammad appos_Hirsts_2006 conj_and_Mohammad_Hirsts dobj_using_method poss_senses_their vmod_each_using prep_of_each_senses prep_between_distance_each amod_distance_distributional det_distance_the dobj_calculated_distance nsubj_calculated_we prep_for_calculated_pair nn_set_antonym det_set_the prep_from_pair_set nn_pair_word det_pair_each
D08-1103	C98-2122	o	Again we used Mohammad and Hirsts -LRB- 2006 -RRB- method along with Lins -LRB- 1998 -RRB- distributional measure to determine the distributional closeness of two thesaurus concepts	nn_concepts_thesaurus num_concepts_two prep_of_closeness_concepts amod_closeness_distributional det_closeness_the dobj_determine_closeness aux_determine_to amod_measure_distributional nn_measure_Lins appos_Lins_1998 pobj_method_measure prepc_along_with_method_with nn_method_Hirsts nn_method_Mohammad appos_Hirsts_2006 conj_and_Mohammad_Hirsts vmod_used_determine dobj_used_method nsubj_used_we advmod_used_Again
D09-1028	C98-2122	o	Curran -LRB- 2002 -RRB- and Lin -LRB- 1998 -RRB- use syntactic features in the vector definition	nn_definition_vector det_definition_the amod_features_syntactic prep_in_use_definition dobj_use_features nsubj_use_Lin nsubj_use_Curran appos_Lin_1998 conj_and_Curran_Lin appos_Curran_2002
D09-1084	C98-2122	o	Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation -LRB- Resnik 1995 -RRB- synonym extraction -LRB- Lin 1998a -RRB- and automatic thesauri generation -LRB- Curran 2002 -RRB-	amod_Curran_2002 dep_generation_Curran nn_generation_thesauri amod_generation_automatic appos_Lin_1998a dep_extraction_Lin nn_extraction_synonym amod_Resnik_1995 conj_and_disambiguation_generation conj_and_disambiguation_extraction dep_disambiguation_Resnik nn_disambiguation_sense nn_disambiguation_word prep_such_as_processing_generation prep_such_as_processing_extraction prep_such_as_processing_disambiguation vmod_language_processing amod_language_natural prep_in_tasks_language amod_tasks_numerous prep_for_important_tasks cop_important_is nsubj_important_measurement conj_or_words_phrases prep_such_as_units_phrases prep_such_as_units_words amod_units_lexical prep_between_similarity_units amod_similarity_semantic prep_of_measurement_similarity amod_measurement_Accurate
D09-1084	C98-2122	o	Method Correlation Edge-counting 0.664 Jiang & Conrath -LRB- 1998 -RRB- 0.848 Lin -LRB- 1998a -RRB- 0.822 Resnik -LRB- 1995 -RRB- 0.745 Li et al.	nn_al._et advmod_Li_al. num_Li_0.745 dep_Resnik_Li dep_Resnik_1995 num_Resnik_0.822 dep_Lin_Resnik appos_Lin_1998a num_Lin_0.848 nn_Lin_Conrath nn_Lin_Jiang appos_Jiang_1998 conj_and_Jiang_Conrath num_Jiang_0.664 nn_Jiang_Edge-counting nn_Jiang_Correlation nn_Jiang_Method
D09-1084	C98-2122	o	-LRB- Strube and Ponzetto 2006 -RRB- 0.19-0 .48 Leacock & Chodrow -LRB- 1998 -RRB- 0.36 Lin -LRB- 1998b -RRB- 0.36 Resnik -LRB- 1995 -RRB- 0.37 Proposed 0.504 7 Conclusion We proposed a relational model to measure the semantic similarity between two words	num_words_two prep_between_similarity_words amod_similarity_semantic det_similarity_the dobj_measure_similarity aux_measure_to vmod_model_measure amod_model_relational det_model_a dobj_proposed_model nsubj_proposed_We rcmod_Conclusion_proposed num_Conclusion_7 amod_Conclusion_Proposed num_Conclusion_0.37 nn_Conclusion_Resnik number_7_0.504 appos_Resnik_1995 num_Resnik_0.36 dep_1998b_Conclusion dep_Lin_1998b num_Lin_0.36 dep_Leacock_Lin dep_Leacock_1998 conj_and_Leacock_Chodrow num_Leacock_.48 dep_Leacock_Ponzetto dep_Leacock_Strube number_.48_0.19-0 dep_Strube_2006 conj_and_Strube_Ponzetto
D09-1084	C98-2122	o	Lin -LRB- 1998b -RRB- defined the similarity between two concepts as the information that is in common to both concepts and the information contained in each individual concept	amod_concept_individual det_concept_each prep_in_contained_concept nsubj_contained_similarity det_information_the conj_and_concepts_information preconj_concepts_both prep_to_common_information prep_to_common_concepts prep_in_is_common nsubj_is_that rcmod_information_is det_information_the prep_as_concepts_information num_concepts_two prep_between_similarity_concepts det_similarity_the ccomp_defined_contained nsubj_defined_Lin appos_Lin_1998b
D09-1089	C98-2122	o	Pereira et al. -LRB- 1993 -RRB- Curran and Moens -LRB- 2002 -RRB- and Lin -LRB- 1998 -RRB- use syntactic features in the vector definition	nn_definition_vector det_definition_the amod_features_syntactic prep_in_use_definition dobj_use_features nsubj_use_Lin nsubj_use_Moens nsubj_use_Curran appos_Lin_1998 appos_Moens_2002 conj_and_Curran_Lin conj_and_Curran_Moens rcmod_al._use appos_al._1993 nn_al._et nn_al._Pereira dep_``_al.
E09-1077	C98-2122	o	Wiebe -LRB- 2000 -RRB- uses Lin -LRB- 1998a -RRB- style distributionally similar adjectives in a cluster-and-label process to generate sentiment lexicon of adjectives	prep_of_lexicon_adjectives nn_lexicon_sentiment dobj_generate_lexicon aux_generate_to vmod_process_generate amod_process_cluster-and-label det_process_a amod_adjectives_similar advmod_adjectives_distributionally nn_adjectives_style nn_style_Lin appos_Lin_1998a prep_in_uses_process dobj_uses_adjectives nsubj_uses_Wiebe appos_Wiebe_2000
E09-1077	C98-2122	o	3http / / www.openoffice.org Another corpora based method due to Turney and Littman -LRB- 2003 -RRB- tries to measure the semantic orientation O -LRB- t -RRB- for a term t by O -LRB- t -RRB- = summationdisplay tiS + PMI -LRB- t ti -RRB- summationdisplay tjS PMI -LRB- t tj -RRB- where S + and S are minimal sets of polar terms that contain prototypical positive and negative terms respectively and PMI -LRB- t ti -RRB- is the pointwise mutual information -LRB- Lin 1998b -RRB- between the terms t and ti	conj_and_t_ti nn_t_terms det_t_the appos_Lin_1998b prep_between_information_ti prep_between_information_t dep_information_Lin amod_information_mutual amod_information_pointwise det_information_the cop_information_is csubj_information_tries appos_t_ti dep_PMI_t amod_terms_negative amod_terms_positive amod_terms_prototypical conj_and_positive_negative advmod_contain_respectively dobj_contain_terms nsubj_contain_that rcmod_terms_contain amod_terms_polar conj_and_sets_PMI prep_of_sets_terms amod_sets_minimal cop_sets_are nsubj_sets_S nsubj_sets_S advmod_sets_where conj_and_S_S appos_t_tj rcmod_PMI_PMI rcmod_PMI_sets dep_PMI_t nn_PMI_tjS nn_PMI_summationdisplay nn_PMI_PMI nn_PMI_tiS appos_t_ti dep_tiS_t conj_+_tiS_PMI nn_tiS_summationdisplay dobj_=_PMI amod_O_= appos_O_t prep_by_t_O nn_t_term det_t_a appos_O_t nn_O_orientation amod_O_semantic det_O_the prep_for_measure_t dobj_measure_O aux_measure_to xcomp_tries_measure nsubj_tries_method appos_Littman_2003 conj_and_Turney_Littman prep_due_to_method_Littman prep_due_to_method_Turney amod_method_based nn_method_corpora nn_method_www.openoffice.org det_corpora_Another parataxis_3http_information
I08-1021	C98-2122	o	Our approach to STC uses a thesaurus based on corpus statistics -LRB- Lin 1998 -RRB- for real-valued similarity calculation	nn_calculation_similarity amod_calculation_real-valued dep_Lin_1998 prep_for_statistics_calculation dep_statistics_Lin nn_statistics_corpus pobj_thesaurus_statistics prepc_based_on_thesaurus_on det_thesaurus_a dobj_uses_thesaurus nsubj_uses_approach prep_to_approach_STC poss_approach_Our ccomp_``_uses
I08-1060	C98-2122	o	Some researchers -LRB- Hindle 1990 Grefenstette 1994 Lin 1998 -RRB- classify terms by similarities based on their distributional syntactic patterns	nn_patterns_syntactic amod_patterns_distributional poss_patterns_their prep_on_based_patterns vmod_similarities_based prep_by_classify_similarities dobj_classify_terms nsubj_classify_Grefenstette dep_Lin_1998 dep_Grefenstette_Lin appos_Grefenstette_1994 parataxis_Hindle_classify appos_Hindle_1990 dep_researchers_Hindle det_researchers_Some ccomp_``_researchers
I08-1072	C98-2122	o	A wide range of contextual information such as surrounding words -LRB- Lowe and McDonald 2000 Curran and Moens 2002a -RRB- dependency or case structure -LRB- Hindle 1990 Ruge 1997 Lin 1998 -RRB- and dependency path -LRB- Lin and Pantel 2001 Pado and Lapata 2007 -RRB- has been utilized for similarity calculation and achieved considerable success	amod_success_considerable dobj_achieved_success nn_calculation_similarity prep_for_utilized_calculation auxpass_utilized_been aux_utilized_has nsubjpass_utilized_Hindle amod_Pado_2007 conj_and_Pado_Lapata dep_Lin_Lapata dep_Lin_Pado num_Lin_2001 conj_and_Lin_Pantel appos_path_Pantel appos_path_Lin nn_path_dependency num_Lin_1998 conj_and_Ruge_path conj_and_Ruge_Lin appos_Ruge_1997 dep_Hindle_path dep_Hindle_Lin dep_Hindle_Ruge appos_Hindle_1990 rcmod_structure_utilized nn_structure_case nn_structure_dependency conj_or_dependency_case appos_Lowe_2002a conj_and_Lowe_Moens conj_and_Lowe_Curran conj_and_Lowe_2000 conj_and_Lowe_McDonald appos_words_structure dep_words_Moens dep_words_Curran dep_words_2000 dep_words_McDonald dep_words_Lowe amod_words_surrounding prep_such_as_information_words amod_information_contextual conj_and_range_achieved prep_of_range_information amod_range_wide det_range_A
I08-1072	C98-2122	o	3.1 Context Extraction We adopted dependency structure as the context of words since it is the most widely used and wellperforming contextual information in the past studies -LRB- Ruge 1997 Lin 1998 -RRB-	dep_Lin_1998 dep_Ruge_Lin appos_Ruge_1997 dep_studies_Ruge amod_studies_past det_studies_the prep_in_information_studies amod_information_contextual amod_information_wellperforming amod_information_used det_information_the cop_information_is nsubj_information_it mark_information_since conj_and_used_wellperforming advmod_used_widely advmod_widely_most prep_of_context_words det_context_the nn_structure_dependency advcl_adopted_information prep_as_adopted_context dobj_adopted_structure nsubj_adopted_We rcmod_Extraction_adopted nn_Extraction_Context num_Extraction_3.1
I08-1072	C98-2122	o	For each word in LDV three existing thesauri are consulted Rogets Thesaurus -LRB- Roget 1995 -RRB- Collins COBUILD Thesaurus -LRB- Collins 2002 -RRB- and WordNet -LRB- Fellbaum 1998 -RRB-	amod_Fellbaum_1998 dep_WordNet_Fellbaum dep_Collins_2002 appos_Thesaurus_Collins nn_Thesaurus_COBUILD nn_Thesaurus_Collins amod_Roget_1995 conj_and_Thesaurus_WordNet conj_and_Thesaurus_Thesaurus dep_Thesaurus_Roget nn_Thesaurus_Rogets dep_consulted_WordNet dep_consulted_Thesaurus dep_consulted_Thesaurus auxpass_consulted_are nsubjpass_consulted_thesauri prep_for_consulted_word amod_thesauri_existing num_thesauri_three prep_in_word_LDV det_word_each
I08-1073	C98-2122	o	We propose using distributional similarity -LRB- using -LRB- Lin 1998 -RRB- -RRB- as an approximation of semantic distancebetweenthewordsinthetwoglosses rather than requiring an exact match	amod_match_exact det_match_an dobj_requiring_match amod_distancebetweenthewordsinthetwoglosses_semantic prep_of_approximation_distancebetweenthewordsinthetwoglosses det_approximation_an dep_Lin_1998 dep_using_Lin amod_similarity_distributional conj_negcc_using_requiring prep_as_using_approximation dep_using_using dobj_using_similarity ccomp_propose_requiring ccomp_propose_using nsubj_propose_We ccomp_``_propose
I08-1073	C98-2122	o	We adopt the similarity score proposed by Lin -LRB- 1998 -RRB- as the distributional similarity score and use 50 nearest neighbours in line with McCarthy et al. For the random baseline we select one word sense at random for each word token and average the precision over 100 trials	num_trials_100 det_precision_the amod_precision_average amod_precision_token nn_precision_word det_precision_each conj_and_token_average prep_at_sense_random nn_sense_word num_sense_one prep_over_select_trials prep_for_select_precision dobj_select_sense nsubj_select_we amod_baseline_random det_baseline_the dep_McCarthy_al. nn_McCarthy_et prep_with_line_McCarthy amod_neighbours_nearest num_neighbours_50 nn_neighbours_use conj_and_score_neighbours nn_score_similarity amod_score_distributional det_score_the appos_Lin_1998 prep_in_proposed_line prep_as_proposed_neighbours prep_as_proposed_score agent_proposed_Lin vmod_score_proposed nn_score_similarity det_score_the ccomp_adopt_select prep_for_adopt_baseline dobj_adopt_score nsubj_adopt_We
I08-1073	C98-2122	o	2 Related Work ThisworkbuildsuponthatofMcCarthyetal -LRB- 2004 -RRB- which acquires predominant senses for target words from a large sample of text using distributional similarity -LRB- Lin 1998 -RRB- to provide evidence for predominance	prep_for_evidence_predominance dobj_provide_evidence aux_provide_to amod_Lin_1998 dep_similarity_Lin amod_similarity_distributional vmod_using_provide dobj_using_similarity prep_of_sample_text amod_sample_large det_sample_a vmod_words_using prep_from_words_sample nn_words_target amod_senses_predominant prep_for_acquires_words dobj_acquires_senses nsubj_acquires_which rcmod_ThisworkbuildsuponthatofMcCarthyetal_acquires dep_ThisworkbuildsuponthatofMcCarthyetal_2004 nn_ThisworkbuildsuponthatofMcCarthyetal_Work amod_ThisworkbuildsuponthatofMcCarthyetal_Related num_ThisworkbuildsuponthatofMcCarthyetal_2
I08-1073	C98-2122	o	In this approach we extend the denition overlap by considering the distributional similarity -LRB- Lin 1998 -RRB- rather than identify of the words in the two denitions	num_denitions_two det_denitions_the prep_in_words_denitions det_words_the prep_of_identify_words num_Lin_1998 conj_negcc_similarity_identify appos_similarity_Lin amod_similarity_distributional det_similarity_the dobj_considering_identify dobj_considering_similarity prepc_by_overlap_considering det_denition_the dep_extend_overlap dobj_extend_denition nsubj_extend_we prep_in_extend_approach det_approach_this
I08-1073	C98-2122	o	McCarthy et al. use a distributional similarity thesaurus acquired from corpus data using the method of Lin -LRB- 1998 -RRB- for nding the predominant sense of a word where the senses are dened by WordNet	agent_dened_WordNet auxpass_dened_are nsubjpass_dened_senses advmod_dened_where det_senses_the rcmod_word_dened det_word_a prep_of_sense_word amod_sense_predominant det_sense_the dobj_nding_sense appos_Lin_1998 prepc_for_method_nding prep_of_method_Lin det_method_the dobj_using_method nn_data_corpus prep_from_acquired_data vmod_thesaurus_acquired nn_thesaurus_similarity amod_thesaurus_distributional det_thesaurus_a vmod_use_using dobj_use_thesaurus nsubj_use_al. nn_al._et nn_al._McCarthy
I08-1073	C98-2122	o	Let w be a target word and Nw = fn1 n2nkg be the ordered set of the top scoring k neighbours of w from the thesaurus with associated distributional similarity scores fdss -LRB- w n1 -RRB- dss -LRB- w n2 -RRB- dss -LRB- w nk -RRB- g using -LRB- Lin 1998 -RRB-	amod_Lin_1998 dep_using_Lin vmod_g_using nn_g_dss appos_w_nk dep_dss_w appos_w_n2 appos_dss_g dep_dss_w appos_w_n1 dep_fdss_dss dep_fdss_w nsubj_fdss_fn1 nn_scores_similarity amod_scores_distributional amod_scores_associated prep_with_thesaurus_scores det_thesaurus_the prep_of_neighbours_w nn_neighbours_k dobj_scoring_neighbours prep_from_top_thesaurus dep_top_scoring amod_the_top prep_of_set_the amod_set_ordered det_set_the cop_set_be nsubj_set_n2nkg rcmod_fn1_set dep_=_fdss amod_Nw_= conj_and_word_Nw nn_word_target det_word_a cop_word_be nsubj_word_w ccomp_Let_Nw ccomp_Let_word
I08-2102	C98-2122	o	We use the similarity proposed by Lin -LRB- 1998 -RRB-	appos_Lin_1998 agent_proposed_Lin vmod_similarity_proposed det_similarity_the dobj_use_similarity nsubj_use_We
N09-2059	C98-2122	o	The thesaurus was produced using the metric described by Lin -LRB- 1998 -RRB- with input from the grammatical relation data extracted using the 90 million words of written English from the British National Corpus -LRB- BNC -RRB- -LRB- Leech 1992 -RRB- using the RASP parser -LRB- Briscoe and Carroll 2002 -RRB-	amod_Briscoe_2002 conj_and_Briscoe_Carroll appos_parser_Carroll appos_parser_Briscoe nn_parser_RASP det_parser_the dobj_using_parser dep_Leech_1992 appos_Corpus_Leech appos_Corpus_BNC nn_Corpus_National nn_Corpus_British det_Corpus_the amod_English_written prep_from_words_Corpus prep_of_words_English num_words_million det_words_the number_million_90 dobj_using_words xcomp_extracted_using vmod_data_extracted nn_data_relation amod_data_grammatical det_data_the vmod_input_using prep_from_input_data appos_Lin_1998 prep_with_described_input agent_described_Lin vmod_metric_described det_metric_the dobj_using_metric xcomp_produced_using auxpass_produced_was nsubjpass_produced_thesaurus det_thesaurus_The
P09-1031	C98-2122	o	The common types of features include contextual -LRB- Lin 1998 -RRB- co-occurrence -LRB- Yang and Callan 2008 -RRB- and syntactic dependency -LRB- Pantel and Lin 2002 Pantel and Ravichandran 2004 -RRB-	amod_Pantel_2004 conj_and_Pantel_Ravichandran dep_Pantel_Ravichandran dep_Pantel_Pantel conj_and_Pantel_2002 conj_and_Pantel_Lin dep_dependency_2002 dep_dependency_Lin dep_dependency_Pantel nn_dependency_syntactic num_Yang_2008 conj_and_Yang_Callan nn_Yang_co-occurrence amod_Lin_1998 conj_and_contextual_dependency conj_and_contextual_Callan conj_and_contextual_Yang dep_contextual_Lin dobj_include_dependency dobj_include_Yang dobj_include_contextual nsubj_include_types prep_of_types_features amod_types_common det_types_The
P09-1031	C98-2122	o	Inspired by the conjunction and appositive structures Riloff and Shepherd -LRB- 1997 -RRB- Roark and Charniak -LRB- 1998 -RRB- used cooccurrence statistics in local context to discover sibling relations	nn_relations_sibling dobj_discover_relations aux_discover_to nsubj_discover_statistics nsubj_discover_Roark nsubj_discover_Shepherd nsubj_discover_Riloff vmod_discover_Inspired amod_context_local nn_statistics_cooccurrence amod_statistics_used nn_statistics_Charniak dep_Charniak_1998 appos_Shepherd_1997 prep_in_Riloff_context conj_and_Riloff_statistics conj_and_Riloff_Roark conj_and_Riloff_Shepherd amod_structures_appositive conj_and_conjunction_structures det_conjunction_the agent_Inspired_structures agent_Inspired_conjunction
P09-1031	C98-2122	o	Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors -LRB- Brown et al. 1992 Lin 1998 -RRB-	dep_Lin_1998 dep_Brown_Lin appos_Brown_1992 dep_Brown_al. nn_Brown_et det_vectors_the dep_similarities_Brown prep_of_similarities_vectors prep_on_based_similarities vmod_vectors_based dep_vectors_words conj_and_vectors_cluster nn_contexts_word prep_as_represent_cluster prep_as_represent_vectors dobj_represent_contexts advmod_represent_usually nsubj_represent_approaches amod_approaches_Clustering-based
P09-1051	C98-2122	o	The second uses Lin dependency similarity a syntacticdependency based distributional word similarity resource described in -LRB- Lin 1998a -RRB- 9	num_Lin_9 appos_Lin_1998a prep_in_described_Lin vmod_resource_described nn_resource_similarity nn_resource_word amod_resource_distributional pobj_based_resource prep_syntacticdependency_based det_syntacticdependency_a appos_similarity_syntacticdependency nn_similarity_dependency nn_similarity_Lin nn_similarity_uses amod_similarity_second det_similarity_The
P09-1051	C98-2122	o	While Kazama and Torisawa used a chunker we parsed the definition sentence using Minipar -LRB- Lin 1998b -RRB-	appos_Lin_1998b dep_Minipar_Lin dobj_using_Minipar vmod_sentence_using nn_sentence_definition det_sentence_the dobj_parsed_sentence nsubj_parsed_we advcl_parsed_used det_chunker_a dobj_used_chunker nsubj_used_Torisawa nsubj_used_Kazama mark_used_While conj_and_Kazama_Torisawa
P09-1052	C98-2122	o	Syntactic context information is used -LRB- Hindle 1990 Ruge 1992 Lin 1998 -RRB- to compute term similarities based on which similar words to a particular word can directly be returned	auxpass_returned_be advmod_returned_directly aux_returned_can nsubjpass_returned_words prep_on_returned_which amod_word_particular det_word_a prep_to_words_word amod_words_similar pcomp_based_returned prep_similarities_based nn_similarities_term dobj_compute_similarities aux_compute_to num_Lin_1998 appos_Ruge_1992 dep_Hindle_Lin dep_Hindle_Ruge dep_Hindle_1990 xcomp_used_compute dep_used_Hindle auxpass_used_is nsubjpass_used_information nn_information_context amod_information_Syntactic
P09-2062	C98-2122	o	Semantic DSN The construction of this network is inspired by -LRB- Lin 1998 -RRB-	amod_Lin_1998 dep_by_Lin prep_inspired_by auxpass_inspired_is nsubjpass_inspired_construction det_network_this prep_of_construction_network det_construction_The dep_DSN_inspired amod_DSN_Semantic
W08-1901	C98-2122	o	corpora and corpus query tools has been particularly significant in the area of compiling and developing lexicographic materials -LRB- Kilgarriff and Rundell 2002 -RRB- and in the area of creating various kinds of lexical resources such as WordNet -LRB- Fellbaum 1998 -RRB- and FrameNet -LRB- Atkins et al. 2003 Fillmore et al. 2003 -RRB-	num_Fillmore_2003 nn_Fillmore_al. nn_Fillmore_et dep_Atkins_Fillmore amod_Atkins_2003 dep_Atkins_al. nn_Atkins_et amod_Fellbaum_1998 appos_WordNet_Atkins conj_and_WordNet_FrameNet dep_WordNet_Fellbaum prep_such_as_resources_FrameNet prep_such_as_resources_WordNet amod_resources_lexical prep_of_kinds_resources amod_kinds_various dobj_creating_kinds prepc_of_area_creating det_area_the pobj_in_area dep_Kilgarriff_2002 conj_and_Kilgarriff_Rundell appos_materials_Rundell appos_materials_Kilgarriff amod_materials_lexicographic dobj_compiling_materials conj_and_compiling_developing prepc_of_area_developing prepc_of_area_compiling det_area_the conj_and_significant_in prep_in_significant_area advmod_significant_particularly cop_significant_been aux_significant_has nsubj_significant_tools nn_tools_query nn_tools_corpus nn_tools_corpora conj_and_corpora_corpus
W08-1901	C98-2122	o	This approach is similar to conventional techniques for automatic thesaurus construction -LRB- Lin 1998 -RRB-	amod_Lin_1998 dep_construction_Lin nn_construction_thesaurus amod_construction_automatic prep_for_techniques_construction amod_techniques_conventional prep_to_similar_techniques cop_similar_is nsubj_similar_approach det_approach_This ccomp_``_similar
W08-1902	C98-2122	o	Our next steps will be to take a closer look at the following work clustering of similar words -LRB- Lin 1998 -RRB- topic signatures -LRB- Lin and Hovy 2000 -RRB- and Kilgariffs sketch engine -LRB- Kilgarriff et al. 2004 -RRB-	amod_Kilgarriff_2004 dep_Kilgarriff_al. nn_Kilgarriff_et nn_engine_sketch nn_engine_Kilgariffs dep_Lin_2000 conj_and_Lin_Hovy dep_signatures_Hovy dep_signatures_Lin nn_signatures_topic amod_Lin_1998 conj_and_words_engine conj_and_words_signatures dep_words_Lin amod_words_similar dep_clustering_Kilgarriff prep_of_clustering_engine prep_of_clustering_signatures prep_of_clustering_words amod_work_following det_work_the prep_at_look_work advmod_look_closer det_look_a dep_take_clustering dep_take_look aux_take_to xcomp_be_take aux_be_will nsubj_be_steps amod_steps_next poss_steps_Our ccomp_``_be
W08-2005	C98-2122	o	The earliest work in this direction are those of -LRB- Hindle 1990 -RRB- -LRB- Lin 1998 -RRB- -LRB- Dagan et al. 1999 -RRB- -LRB- Chen and Chen 2000 -RRB- -LRB- Geffet and Dagan 2004 -RRB- and -LRB- Weeds and Weir 2005 -RRB-	dep_Weeds_2005 conj_and_Weeds_Weir conj_and_Geffet_Dagan amod_Chen_2000 conj_and_Chen_Chen amod_Dagan_1999 dep_Dagan_al. nn_Dagan_et amod_Lin_1998 conj_and_Hindle_Weir conj_and_Hindle_Weeds dep_Hindle_2004 appos_Hindle_Dagan appos_Hindle_Geffet appos_Hindle_Chen appos_Hindle_Chen appos_Hindle_Dagan appos_Hindle_Lin dep_Hindle_1990 prep_of_those_Weeds prep_of_those_Hindle cop_those_are nsubj_those_work det_direction_this prep_in_work_direction amod_work_earliest det_work_The ccomp_``_those
W08-2005	C98-2122	o	Lin -LRB- 1998 -RRB- proposed a word similarity measure based on the distributio nal pattern of words which allows to construct a thesaurus using a parsed corpus	amod_corpus_parsed det_corpus_a dobj_using_corpus vmod_thesaurus_using det_thesaurus_a dobj_construct_thesaurus aux_construct_to xcomp_allows_construct nsubj_allows_which rcmod_pattern_allows prep_of_pattern_words nn_pattern_nal nn_pattern_distributio det_pattern_the prep_on_based_pattern vmod_measure_based nn_measure_similarity nn_measure_word det_measure_a dobj_proposed_measure nsubj_proposed_Lin appos_Lin_1998
W08-2211	C98-2122	o	-LRB- 2004 -RRB- we use k = 50 and obtain our thesaurus using the distributional similarity metric described by Lin -LRB- 1998 -RRB-	appos_Lin_1998 agent_described_Lin vmod_metric_described amod_similarity_metric amod_similarity_distributional det_similarity_the dobj_using_similarity poss_thesaurus_our dobj_obtain_thesaurus nsubj_obtain_we dep_=_50 amod_k_= xcomp_use_using conj_and_use_obtain dobj_use_k nsubj_use_we dep_use_2004
W08-2211	C98-2122	o	Thus we rank each sense wsi WSw using Prevalence Score wsi = -LRB- 11 -RRB- njNw dssnj wnss -LRB- wsi nj -RRB- wsiWSw wnss -LRB- wsi nj -RRB- where the WordNet similarity score -LRB- wnss -RRB- is defined as wnss -LRB- wsi nj -RRB- = max nsxNSnj -LRB- wnss -LRB- wsi nsx -RRB- -RRB- 2.2 Building the Thesaurus The thesaurus was acquired using the method described by Lin -LRB- 1998 -RRB-	appos_Lin_1998 agent_described_Lin vmod_method_described det_method_the dobj_using_method xcomp_acquired_using auxpass_acquired_was nsubjpass_acquired_thesaurus det_thesaurus_The rcmod_Thesaurus_acquired det_Thesaurus_the dep_Building_Thesaurus num_Building_2.2 nn_Building_nsxNSnj appos_wsi_nsx dep_wnss_wsi dep_nsxNSnj_wnss nn_nsxNSnj_max dep_=_Building appos_wsi_nj amod_wnss_= dep_wnss_wsi dep_as_wnss prep_defined_as auxpass_defined_is nsubjpass_defined_score advmod_defined_where appos_score_wnss nn_score_similarity nn_score_WordNet det_score_the appos_wsi_nj dep_wnss_wsi nn_wnss_wsiWSw nn_wnss_wnss appos_wsi_nj dep_wnss_wsi nn_wnss_dssnj nn_wnss_njNw advcl_=_defined dep_=_wnss dep_=_11 dep_wsi_= nn_wsi_Score nn_wsi_Prevalence dobj_using_wsi vmod_WSw_using nn_WSw_wsi nn_WSw_sense det_WSw_each dobj_rank_WSw nsubj_rank_we advmod_rank_Thus
W08-2211	C98-2122	o	For every pair of nouns where each noun had a total frequency in the triple data of 10 or more we computed their distributional similarity using the measure given by Lin -LRB- 1998 -RRB-	appos_Lin_1998 agent_given_Lin vmod_measure_given det_measure_the dobj_using_measure vmod_similarity_using amod_similarity_distributional poss_similarity_their dobj_computed_similarity nsubj_computed_we prep_for_computed_pair conj_or_10_more prep_of_data_more prep_of_data_10 amod_data_triple det_data_the amod_frequency_total det_frequency_a prep_in_had_data dobj_had_frequency nsubj_had_noun advmod_had_where det_noun_each rcmod_nouns_had prep_of_pair_nouns det_pair_every
W09-0201	C98-2122	o	Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information -LRB- Lin 1998 Curran and Moens 2002 -RRB-	amod_Curran_2002 conj_and_Curran_Moens dep_Lin_Moens dep_Lin_Curran amod_Lin_1998 appos_information_Lin nn_information_dependency prep_with_typed_information auxpass_typed_are nsubjpass_typed_that rcmod_words_typed nn_words_context prep_with_vectors_words prep_of_vectors_co-occurrence agent_measured_vectors advmod_measured_often auxpass_measured_is nsubjpass_measured_similarity nn_similarity_Concept
W09-0203	C98-2122	p	Whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems -LRB- Pad and Lapata 2007 Lin 1998 -RRB- for the task of categorisation simple pattern based spaces have been shown to perform equally good if not better -LRB- Poesio and Almuhareb 2005b Almuhareb and Poesio 2005b -RRB-	appos_Almuhareb_2005b conj_and_Almuhareb_Poesio dep_Poesio_Poesio dep_Poesio_Almuhareb conj_and_Poesio_2005b conj_and_Poesio_Almuhareb dep_better_2005b dep_better_Almuhareb dep_better_Poesio neg_better_not mark_better_if advcl_good_better advmod_good_equally acomp_perform_good aux_perform_to xcomp_shown_perform auxpass_shown_been aux_shown_have nsubjpass_shown_2007 nsubjpass_shown_Lapata nsubjpass_shown_Pad amod_spaces_based amod_pattern_simple nn_pattern_categorisation dep_task_spaces prep_of_task_pattern det_task_the dep_Lin_1998 prep_for_Pad_task dep_Pad_Lin conj_and_Pad_2007 conj_and_Pad_Lapata rcmod_problems_shown prep_of_number_problems det_number_a prep_for_models_number nn_models_space nn_models_word amod_models_other dobj_surpass_models aux_surpass_to xcomp_shown_surpass auxpass_shown_been aux_shown_have nsubjpass_shown_spaces mark_shown_Whereas amod_spaces_semantic amod_spaces_based nn_spaces_dependency advcl_``_shown
W09-0203	C98-2122	o	In particular we work with dependency paths that can reach beyond direct dependencies as opposed to Lin -LRB- 1998 -RRB- but in the line of Pado and Lapata -LRB- 2007 -RRB-	appos_Lapata_2007 conj_and_Pado_Lapata prep_of_line_Lapata prep_of_line_Pado det_line_the pobj_in_line appos_Lin_1998 conj_but_opposed_in prep_to_opposed_Lin mark_opposed_as amod_dependencies_direct advcl_reach_in advcl_reach_opposed prep_beyond_reach_dependencies aux_reach_can nsubj_reach_that rcmod_paths_reach nn_paths_dependency prep_with_work_paths nsubj_work_we prep_in_work_particular
W09-0203	C98-2122	o	As a basis mapping function we used a generalisation of the one used by Grefenstette -LRB- 1994 -RRB- and Lin -LRB- 1998 -RRB-	appos_Lin_1998 conj_and_Grefenstette_Lin appos_Grefenstette_1994 agent_used_Lin agent_used_Grefenstette vmod_one_used det_one_the prep_of_generalisation_one det_generalisation_a dobj_used_generalisation nsubj_used_we prep_as_used_function nn_function_mapping nn_function_basis det_function_a
W09-0805	C98-2122	o	Example of such algorithms are -LRB- Pereira et al. 1993 -RRB- and -LRB- Lin 1998 -RRB- that use syntactic features in the vector definition	nn_definition_vector det_definition_the amod_features_syntactic prep_in_use_definition dobj_use_features nsubj_use_that amod_Lin_1998 amod_Pereira_1993 dep_Pereira_al. nn_Pereira_et dep_are_use conj_and_are_Lin dep_are_Pereira amod_algorithms_such dep_Example_Lin dep_Example_are prep_of_Example_algorithms
W09-1108	C98-2122	o	Pereira -LRB- 1993 -RRB- Curran -LRB- 2002 -RRB- and Lin -LRB- 1998 -RRB- use syntactic features in the vector definition	nn_definition_vector det_definition_the amod_features_syntactic prep_in_use_definition dobj_use_features nsubj_use_Pereira appos_Lin_1998 conj_and_Curran_Lin appos_Curran_2002 appos_Pereira_Lin appos_Pereira_Curran appos_Pereira_1993
W09-1316	C98-2122	o	In particular this method has been used for word sense disambiguation -LRB- Lin 1997 -RRB- and thesaurus construction -LRB- Lin 1998 -RRB-	amod_Lin_1998 nn_construction_thesaurus amod_Lin_1997 dep_disambiguation_Lin conj_and_disambiguation_construction dep_disambiguation_Lin nn_disambiguation_sense nn_disambiguation_word prep_for_used_construction prep_for_used_disambiguation auxpass_used_been aux_used_has nsubjpass_used_method prep_in_used_particular det_method_this
D07-1008	D07-1001	o	We used an implementation of McDonald -LRB- 2006 -RRB- forcomparisonofresults -LRB- ClarkeandLapata 2007 -RRB-	amod_ClarkeandLapata_2007 dep_forcomparisonofresults_ClarkeandLapata num_forcomparisonofresults_2006 nn_forcomparisonofresults_McDonald prep_of_implementation_forcomparisonofresults det_implementation_an dobj_used_implementation nsubj_used_We
D08-1057	D07-1001	o	More recently Clarke and Lapata -LRB- 2007 -RRB- use Centering Theory -LRB- Grosz et al. 1995 -RRB- and Lexical Chains -LRB- Morris and Hirst 1991 -RRB- to identify which information to prune	aux_prune_to vmod_information_prune det_information_which dobj_identify_information aux_identify_to dep_Morris_1991 conj_and_Morris_Hirst appos_Chains_Hirst appos_Chains_Morris amod_Chains_Lexical amod_Grosz_1995 dep_Grosz_al. nn_Grosz_et conj_and_Theory_Chains dep_Theory_Grosz nn_Theory_Centering vmod_use_identify dobj_use_Chains dobj_use_Theory nsubj_use_Lapata nsubj_use_Clarke advmod_use_recently appos_Lapata_2007 conj_and_Clarke_Lapata advmod_recently_More
N09-2058	D07-1001	o	-LRB- Elhadad et al. 2001 Clarke and Lapata 2007 Madnani et al. 2007 -RRB- -RRB-	appos_al._2007 nn_al._et nn_al._Madnani dep_Clarke_al. num_Clarke_2007 conj_and_Clarke_Lapata dep_Elhadad_Lapata dep_Elhadad_Clarke appos_Elhadad_2001 dep_Elhadad_al. nn_Elhadad_et dep_''_Elhadad
P09-1024	D07-1001	o	This framework is 211 commonly used in generation and summarization applications where the selection process is driven by multiple constraints -LRB- Marciniak and Strube 2005 Clarke and Lapata 2007 -RRB-	appos_Clarke_2007 conj_and_Clarke_Lapata conj_and_Marciniak_Lapata conj_and_Marciniak_Clarke conj_and_Marciniak_2005 conj_and_Marciniak_Strube dep_constraints_Clarke dep_constraints_2005 dep_constraints_Strube dep_constraints_Marciniak amod_constraints_multiple agent_driven_constraints auxpass_driven_is nsubjpass_driven_process advmod_driven_where nn_process_selection det_process_the rcmod_applications_driven nn_applications_summarization nn_applications_generation conj_and_generation_summarization prep_in_used_applications advmod_used_commonly vmod_211_used nsubj_is_211 dep_framework_is det_framework_This
P09-1024	D07-1001	o	In prior research ILP was used as a postprocessing step to remove redundancy and make other global decisions about parameters -LRB- McDonald 2007 Marciniak and Strube 2005 Clarke and Lapata 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Lapata conj_and_McDonald_Clarke conj_and_McDonald_2005 conj_and_McDonald_Strube conj_and_McDonald_Marciniak conj_and_McDonald_2007 dep_parameters_Lapata dep_parameters_Clarke dep_parameters_2005 dep_parameters_Strube dep_parameters_Marciniak dep_parameters_2007 dep_parameters_McDonald prep_about_decisions_parameters amod_decisions_global amod_decisions_other dobj_make_decisions conj_and_remove_make dobj_remove_redundancy aux_remove_to vmod_step_make vmod_step_remove amod_step_postprocessing det_step_a prep_as_used_step auxpass_used_was nsubjpass_used_ILP prep_in_used_research amod_research_prior
P09-2026	D07-1001	o	Clarke and Lapata -LRB- 2007 -RRB- included discourse level features in their framework to leverage context for enhancing coherence	dobj_enhancing_coherence nn_context_leverage poss_framework_their nn_features_level nn_features_discourse prepc_for_included_enhancing prep_to_included_context prep_in_included_framework dobj_included_features nsubj_included_Lapata nsubj_included_Clarke appos_Lapata_2007 conj_and_Clarke_Lapata
P09-2026	D07-1001	o	4.1 Corpora Sentence compression systems have been tested on product review data from the Ziff-Davis -LRB- ZD henceforth -RRB- Corpus by Knight and Marcu -LRB- 2000 -RRB- general news articles by Clarke and Lapata -LRB- CL henceforth -RRB- corpus -LRB- 2007 -RRB- and biomedical articles -LRB- Lin and Wilbur 2007 -RRB-	amod_Lin_2007 conj_and_Lin_Wilbur dep_articles_Wilbur dep_articles_Lin amod_articles_biomedical appos_corpus_2007 dep_corpus_CL nn_corpus_Lapata dep_CL_henceforth conj_and_Clarke_articles conj_and_Clarke_corpus prep_by_articles_articles prep_by_articles_corpus prep_by_articles_Clarke nn_articles_news amod_articles_general appos_Marcu_2000 conj_and_Knight_Marcu appos_ZD_henceforth dep_Ziff-Davis_Corpus dep_Ziff-Davis_ZD det_Ziff-Davis_the prep_from_data_Ziff-Davis nn_data_review nn_data_product dep_tested_articles agent_tested_Marcu agent_tested_Knight prep_on_tested_data auxpass_tested_been aux_tested_have nsubjpass_tested_systems nn_systems_compression nn_systems_Sentence nn_systems_Corpora num_systems_4.1
D09-1024	D07-1006	o	In the first set of experiments we compare two settings of our UALIGN system with other aligners GIZA + + -LRB- Union -RRB- -LRB- Och and Ney 2003 -RRB- and LEAF -LRB- with 2 iterations -RRB- -LRB- Fraser and Marcu 2007 -RRB-	amod_Fraser_2007 conj_and_Fraser_Marcu num_iterations_2 prep_with_LEAF_iterations num_Och_2003 conj_and_Och_Ney appos_+_Union dep_GIZA_Marcu dep_GIZA_Fraser conj_and_GIZA_LEAF dep_GIZA_Ney dep_GIZA_Och conj_+_GIZA_+ amod_aligners_other nn_system_UALIGN poss_system_our prep_of_settings_system num_settings_two dobj_compare_LEAF dobj_compare_+ dobj_compare_GIZA prep_with_compare_aligners dobj_compare_settings nsubj_compare_we prep_in_compare_set prep_of_set_experiments amod_set_first det_set_the
D09-1024	D07-1006	o	Besides precision recall and -LRB- balanced -RRB- F-measure we also include an F-measure variant strongly biased towards recall -LRB- # 0B = 0.1 -RRB- which -LRB- Fraser and Marcu 2007 -RRB- found to be best to tune their LEAF aligner for maximum MT accuracy	nn_accuracy_MT nn_accuracy_maximum nn_aligner_LEAF poss_aligner_their prep_for_tune_accuracy dobj_tune_aligner aux_tune_to xcomp_best_tune cop_best_be aux_best_to xcomp_found_best dep_found_Marcu dep_found_Fraser dobj_found_which dep_Fraser_2007 conj_and_Fraser_Marcu dep_=_0.1 nsubj_=_0B dep_=_# rcmod_recall_found dep_recall_= prep_towards_biased_recall advmod_biased_strongly dep_variant_biased amod_F-measure_variant dep_an_F-measure dobj_include_an advmod_include_also nsubj_include_we prep_besides_include_F-measure prep_besides_include_recall prep_besides_include_precision dep_F-measure_balanced conj_and_precision_F-measure conj_and_precision_recall
D09-1024	D07-1006	o	1 Introduction Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research for example -LRB- Brown et al. 1993 Ittycheriah and Roukos 2005 Fraser and Marcu 2007 -RRB- including work leveraging syntactic parse trees e.g. -LRB- Cherry and Lin 2006 DeNero and Klein 2007 Fossum et al. 2008 -RRB-	nn_al._et nn_al._Fossum num_DeNero_2007 conj_and_DeNero_Klein amod_Cherry_2008 dep_Cherry_al. dep_Cherry_Klein dep_Cherry_DeNero num_Cherry_2006 conj_and_Cherry_Lin appos_e.g._Lin appos_e.g._Cherry nn_trees_parse amod_trees_syntactic dobj_leveraging_trees vmod_work_leveraging dep_Ittycheriah_2007 conj_and_Ittycheriah_Marcu conj_and_Ittycheriah_Fraser conj_and_Ittycheriah_2005 conj_and_Ittycheriah_Roukos dep_Brown_Marcu dep_Brown_Fraser dep_Brown_2005 dep_Brown_Roukos dep_Brown_Ittycheriah amod_Brown_1993 dep_Brown_al. nn_Brown_et prep_of_amount_research amod_amount_significant det_amount_a prep_received_e.g. prep_including_received_work appos_received_Brown prep_for_received_example dobj_received_amount aux_received_has nsubj_received_alignment nn_systems_translation nn_systems_machine amod_systems_statistical nn_systems_training conj_and_component_received prep_in_component_systems amod_component_critical det_component_a cop_component_is nsubj_component_alignment nn_alignment_Word nn_alignment_Introduction num_alignment_1
D09-1076	D07-1006	o	The training data is aligned using the LEAF technique -LRB- Fraser and Marcu 2007 -RRB-	amod_Fraser_2007 conj_and_Fraser_Marcu dep_technique_Marcu dep_technique_Fraser nn_technique_LEAF det_technique_the dobj_using_technique xcomp_aligned_using auxpass_aligned_is nsubjpass_aligned_data nn_data_training det_data_The
W08-0306	D07-1006	o	1.2 Related Work Recently discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments -LRB- Liu et al. 2005 Ittycheriah and Roukos 2005 Taskar et al. 2005 Moore et al. 2006 Fraser and Marcu 2007b -RRB-	appos_Fraser_2007b conj_and_Fraser_Marcu num_Moore_2006 nn_Moore_al. nn_Moore_et num_Taskar_2005 nn_Taskar_al. nn_Taskar_et num_Ittycheriah_2005 conj_and_Ittycheriah_Roukos dep_Liu_Marcu dep_Liu_Fraser dep_Liu_Moore dep_Liu_Taskar dep_Liu_Roukos dep_Liu_Ittycheriah num_Liu_2005 dep_Liu_al. nn_Liu_et num_alignments_4 nn_alignments_Model nn_alignments_IBM prep_of_quality_alignments det_quality_the dep_rivaled_Liu dobj_rivaled_quality aux_rivaled_have nsubj_rivaled_methods prep_for_methods_alignment amod_methods_discriminative rcmod_Work_rivaled advmod_Work_Recently amod_Work_Related num_Work_1.2
W08-0306	D07-1006	p	However except for -LRB- Fraser and Marcu 2007b -RRB- none of these advances in alignment quality has improved translation quality of a state-of-the-art system	amod_system_state-of-the-art det_system_a prep_of_quality_system nn_quality_translation dobj_improved_quality aux_improved_has nsubj_improved_none nn_quality_alignment prep_in_advances_quality det_advances_these prep_of_none_advances rcmod_Fraser_improved dep_Fraser_2007b conj_and_Fraser_Marcu pobj_for_Marcu pobj_for_Fraser pcomp_except_for ccomp_,_except dep_``_However
W08-0306	D07-1006	o	In contrast to the semi-supervised LEAF alignment algorithm of -LRB- Fraser and Marcu 2007b -RRB- which requires 1,5002,000 CPU days per iteration to align 8.4 M ChineseEnglish sentences -LRB- anonymous p.c. -RRB- link deletion requires only 450 CPU hours to re-align such a corpus -LRB- after initial alignment by GIZA + + which requires 20-24 CPU days -RRB-	nn_days_CPU num_days_20-24 dobj_requires_days nsubj_requires_which rcmod_GIZA_requires conj_+_GIZA_+ prep_by_alignment_+ prep_by_alignment_GIZA amod_alignment_initial det_corpus_a predet_corpus_such prep_after_re-align_alignment dobj_re-align_corpus aux_re-align_to nn_hours_CPU num_hours_450 quantmod_450_only xcomp_requires_re-align dobj_requires_hours nsubj_requires_deletion nn_deletion_link dep_anonymous_p.c. dep_sentences_anonymous amod_sentences_ChineseEnglish nn_sentences_M num_sentences_8.4 dobj_align_sentences aux_align_to prep_per_days_iteration nn_days_CPU num_days_1,5002,000 xcomp_requires_align dobj_requires_days nsubj_requires_which rcmod_Fraser_requires rcmod_Fraser_requires dep_Fraser_2007b conj_and_Fraser_Marcu prep_of_algorithm_Marcu prep_of_algorithm_Fraser nn_algorithm_alignment nn_algorithm_LEAF amod_algorithm_semi-supervised det_algorithm_the prep_to_contrast_algorithm prep_in_``_contrast
W08-0306	D07-1006	o	However -LRB- Fraser and Marcu 2007a -RRB- show that in phrase-based translation improvements in AER or f-measure do not necessarily correlate with improvements in BLEU score	nn_score_BLEU prep_in_improvements_score prep_with_correlate_improvements advmod_correlate_necessarily neg_correlate_not aux_correlate_do nsubj_correlate_improvements prep_in_correlate_translation mark_correlate_that conj_or_AER_f-measure prep_in_improvements_f-measure prep_in_improvements_AER amod_translation_phrase-based ccomp_show_correlate nsubj_show_Marcu nsubj_show_Fraser advmod_show_However dep_Fraser_2007a conj_and_Fraser_Marcu
W08-0306	D07-1006	o	They propose two modifications to f-measure varying the precision/recall tradeoff and fully-connecting the alignment links before computing f-measure .11 Weighted Fully-Connected F-Measure Given a hypothesized set of alignment links H and a goldstandard set of alignment links G we define H + = fullyConnect -LRB- H -RRB- and G + = fullyConnect -LRB- G -RRB- and then compute f-measure -LRB- H + -RRB- = 1 precision -LRB- H + -RRB- + 1 recall -LRB- H + -RRB- For phrase-based Chinese-English and ArabicEnglish translation tasks -LRB- Fraser and Marcu 2007a -RRB- obtain the closest correlation between weighted fully-connected alignment f-measure and BLEU score using = 0.5 and = 0.1 respectively	dep_=_0.1 advmod_=_respectively conj_and_=_= dep_=_0.5 dobj_using_= dobj_using_= nn_score_BLEU conj_and_f-measure_score nn_f-measure_alignment amod_f-measure_fully-connected amod_f-measure_weighted prep_between_correlation_score prep_between_correlation_f-measure amod_correlation_closest det_correlation_the xcomp_obtain_using dobj_obtain_correlation nsubj_obtain_f-measure dep_Fraser_2007a conj_and_Fraser_Marcu nn_tasks_translation amod_tasks_ArabicEnglish amod_tasks_Chinese-English amod_tasks_phrase-based conj_and_Chinese-English_ArabicEnglish cc_H_+ appos_recall_H num_recall_1 cc_H_+ conj_+_precision_recall appos_precision_H num_precision_1 prep_for_=_tasks dobj_=_recall dobj_=_precision cc_H_+ appos_f-measure_Marcu appos_f-measure_Fraser dep_f-measure_= appos_f-measure_H ccomp_compute_obtain advmod_compute_then nsubj_compute_we appos_fullyConnect_G dep_=_fullyConnect conj_+_fullyConnect_= conj_and_fullyConnect_G appos_fullyConnect_H dep_=_= dep_=_G dep_=_fullyConnect conj_+_H_= conj_and_define_compute dobj_define_= dobj_define_H nsubj_define_we vmod_define_fully-connecting vmod_define_varying nn_G_links nn_G_alignment prep_of_set_G nn_set_goldstandard det_set_a conj_and_H_set nn_H_links nn_H_alignment prep_of_set_set prep_of_set_H amod_set_hypothesized det_set_a pobj_Given_set prep_F-Measure_Given amod_F-Measure_Fully-Connected amod_F-Measure_Weighted num_F-Measure_.11 nn_F-Measure_f-measure amod_F-Measure_computing nn_links_alignment det_links_the prep_before_fully-connecting_F-Measure dobj_fully-connecting_links nn_tradeoff_precision/recall det_tradeoff_the conj_and_varying_fully-connecting dobj_varying_tradeoff prep_to_modifications_f-measure num_modifications_two parataxis_propose_compute parataxis_propose_define dobj_propose_modifications nsubj_propose_They
W09-1804	D07-1006	o	Probabilistic generative models like IBM 1-5 -LRB- Brown et al. 1993 -RRB- HMM -LRB- Vogel et al. 1996 -RRB- ITG -LRB- Wu 1997 -RRB- and LEAF -LRB- Fraser and Marcu 2007 -RRB- define formulas for P -LRB- f | e -RRB- or P -LRB- e f -RRB- with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1 Word alignment exercise -LRB- Knight 1997 -RRB-	dep_Knight_1997 appos_exercise_Knight nn_exercise_alignment nn_exercise_Word num_Figure_1 nn_Figure_gat nn_Figure_vat nn_Figure_arrat nn_Figure_forat nn_Figure_nnat dobj_wat_Figure nsubj_wat_mok nn_mok_hihok nn_mok_izok nn_mok_nok nn_mok_rarok nn_mok_lalok nn_mok_zanzanat nn_mok_mat nn_mok_arrat nsubj_wat_zanzanok nn_zanzanok_yorok nn_zanzanok_hihok nn_zanzanok_crrrok nn_zanzanok_nok nn_zanzanok_lalok nn_zanzanok_hilat nn_zanzanok_bat nn_zanzanok_mat nn_zanzanok_gat nn_zanzanok_nnat amod_zanzanok_wat nn_clok_ghirok nn_clok_yorok nn_clok_nok nn_clok_mok nn_clok_lalok nn_clok_at-yurp nn_clok_oloat nn_clok_quat nn_clok_nnat nn_clok_totat nn_clok_ok-yurp nn_clok_kantok nn_clok_izok nn_clok_nok nn_clok_wiwok nn_clok_nnat nn_clok_rrat nn_clok_pippat nn_clok_lat nn_clok_iat nn_clok_nok nn_clok_plok nn_clok_anok nn_clok_brok nn_clok_lalok nn_clok_eneat nn_clok_vat nn_clok_dat dobj_wat_clok nsubj_wat_bichat nn_bichat_jjat amod_bichat_wat nn_enemok_izok nn_enemok_sprok nn_enemok_lalok nn_enemok_ororok nn_enemok_farok nn_enemok_lalok nn_enemok_cat nn_enemok_quat nn_enemok_krat nn_enemok_dat dep_wat_exercise parataxis_wat_wat dep_wat_nnat dep_wat_wat dep_wat_wat dobj_wat_enemok nsubj_wat_stok nn_stok_jok nn_stok_izok nn_stok_sprok nn_stok_lalok nn_stok_cat nn_stok_quat nn_stok_jjat nn_stok_totat nn_stok_stok nn_stok_izok nn_stok_farok nn_stok_wiwok nn_stok_lat dep_sat_wat nn_pippat_krat nn_pippat_at-voon nn_pippat_jok nn_pippat_brok nn_pippat_drok nn_pippat_anok nn_pippat_ok-voon nn_pippat_dat nn_pippat_rrat nn_pippat_pippat nn_pippat_at-voon nn_pippat_at-drubel nn_pippat_sprok nn_pippat_plok nn_pippat_anok nn_pippat_ok-voon amod_pippat_ok-drubel nn_pippat_hilat nn_pippat_vat nn_pippat_arrat nn_pippat_dat nn_pippat_totat nn_pippat_ghirok nn_pippat_hihok nn_pippat_izok nn_pippat_sprok nn_pippat_erok nn_pippat_dat nn_pippat_bichat nn_pippat_at-voon nn_pippat_sprok nn_pippat_ororok amod_pippat_ok-voon dep_|_f dep_|_e conj_or_|_P dep_|_e nn_|_f dep_P_P dep_P_| prep_for_formulas_P dep_define_sat prep_with_define_pippat dobj_define_formulas nsubj_define_models dep_Fraser_2007 conj_and_Fraser_Marcu appos_LEAF_Marcu appos_LEAF_Fraser dep_Wu_1997 appos_ITG_Wu amod_Vogel_1996 dep_Vogel_al. nn_Vogel_et conj_and_HMM_LEAF conj_and_HMM_ITG dep_HMM_Vogel amod_Brown_1993 dep_Brown_al. nn_Brown_et num_IBM_1-5 appos_models_LEAF appos_models_ITG appos_models_HMM dep_models_Brown prep_like_models_IBM amod_models_generative nn_models_Probabilistic
C08-1041	D07-1007	o	Carpuat and Wu -LRB- 2007b -RRB- integrated a WSD system into a phrase-based SMT system Pharaoh -LRB- Koehn 2004a -RRB-	appos_Koehn_2004a dep_Pharaoh_Koehn nn_system_SMT amod_system_phrase-based det_system_a nn_system_WSD det_system_a amod_system_integrated appos_Wu_2007b dep_Carpuat_Pharaoh prep_into_Carpuat_system dep_Carpuat_system conj_and_Carpuat_Wu ccomp_``_Wu ccomp_``_Carpuat
C08-1041	D07-1007	o	Furthermore they extended WSD to phrase sense disambiguation -LRB- PSD -RRB- -LRB- Carpuat and Wu 2007a -RRB-	appos_Carpuat_2007a conj_and_Carpuat_Wu dep_disambiguation_Wu dep_disambiguation_Carpuat appos_disambiguation_PSD nn_disambiguation_sense nn_disambiguation_phrase prep_to_extended_disambiguation dobj_extended_WSD nsubj_extended_they advmod_extended_Furthermore
D08-1010	D07-1007	o	Carpuat and Wu -LRB- 2007b -RRB- and Chan et al.	dep_Chan_al. nn_Chan_et dep_Wu_2007b conj_and_Carpuat_Chan conj_and_Carpuat_Wu
D08-1010	D07-1007	p	Similar to WSD Carpuat and Wu -LRB- 2007a -RRB- used contextual information to solve the ambiguity problem for phrases	prep_for_problem_phrases nn_problem_ambiguity det_problem_the dobj_solve_problem aux_solve_to amod_information_contextual amod_information_used appos_Wu_2007a dep_WSD_information conj_and_WSD_Wu conj_and_WSD_Carpuat xcomp_Similar_solve prep_to_Similar_Wu prep_to_Similar_Carpuat prep_to_Similar_WSD dep_``_Similar
D08-1039	D07-1007	p	Recently word-sense disambiguation -LRB- WSD -RRB- methods have been shown to improve translation quality -LRB- Chan et al. 2007 Carpuat and Wu 2007 -RRB-	amod_Carpuat_2007 conj_and_Carpuat_Wu dep_Chan_Wu dep_Chan_Carpuat num_Chan_2007 dep_Chan_al. nn_Chan_et nn_quality_translation dobj_improve_quality aux_improve_to dep_shown_Chan xcomp_shown_improve auxpass_shown_been aux_shown_have nsubjpass_shown_methods advmod_shown_Recently nn_methods_disambiguation appos_disambiguation_WSD amod_disambiguation_word-sense
D08-1039	D07-1007	p	In Carpuat and Wu -LRB- 2007 -RRB- anotherstate-of-the-artWSDengine -LRB- acombination of naive Bayes maximum entropy boosting and Kernel PCA models -RRB- is used to dynamically determine the score of a phrase pair under consideration and thus let the phrase selection adapt to the context of the sentence	det_sentence_the prep_of_context_sentence det_context_the prep_to_adapt_context nsubj_adapt_selection nn_selection_phrase det_selection_the ccomp_let_adapt nn_pair_phrase det_pair_a prep_of_score_pair det_score_the ccomp_determine_let advmod_determine_thus cc_determine_and prep_under_determine_consideration dobj_determine_score advmod_determine_dynamically aux_determine_to xcomp_used_determine auxpass_used_is nsubjpass_used_anotherstate-of-the-artWSDengine prep_in_used_Wu prep_in_used_Carpuat nn_models_PCA nn_models_Kernel conj_and_boosting_models nn_entropy_maximum amod_Bayes_naive dep_acombination_models dep_acombination_boosting appos_acombination_entropy prep_of_acombination_Bayes appos_anotherstate-of-the-artWSDengine_acombination appos_Wu_2007 conj_and_Carpuat_Wu
D08-1105	D07-1007	p	WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation -LRB- MT -RRB- -LRB- Chan et al. 2007a Carpuat and Wu 2007 -RRB- information retrieval -LRB- IR -RRB- etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label -LRB- from a pre-defined sense inventory -RRB- during the disambiguation process	nn_process_disambiguation det_process_the nn_inventory_sense amod_inventory_pre-defined det_inventory_a prep_from_label_inventory nn_label_sense det_label_a prep_during_assigned_process dobj_assigned_label auxpass_assigned_is nsubjpass_assigned_word advmod_assigned_where amod_word_ambiguous det_word_each rcmod_problem_assigned nn_problem_classification det_problem_a prep_as_viewed_problem advmod_viewed_typically auxpass_viewed_is nsubjpass_viewed_WSD nn_WSD_etc. rcmod_retrieval_viewed appos_retrieval_IR dep_information_retrieval amod_Carpuat_2007 conj_and_Carpuat_Wu dep_Chan_Wu dep_Chan_Carpuat appos_Chan_2007a dep_Chan_al. nn_Chan_et appos_translation_Chan appos_translation_MT nn_translation_machine prep_such_as_applications_translation prep_for_important_applications cop_important_is nsubj_important_WSD nn_processing_language amod_processing_natural prep_in_problems_processing amod_problems_fundamental det_problems_the dep_one_information conj_and_one_important prep_of_one_problems cop_one_is nsubj_one_WSD ccomp_``_important ccomp_``_one
D09-1022	D07-1007	o	Another WSD approach incorporating context-dependent phrasal translation lexicons is given in -LRB- Carpuat and Wu 2007 -RRB- and has been evaluated on several translation tasks	nn_tasks_translation amod_tasks_several prep_on_evaluated_tasks auxpass_evaluated_been aux_evaluated_has nsubjpass_evaluated_approach amod_Carpuat_2007 conj_and_Carpuat_Wu dep_in_Wu dep_in_Carpuat conj_and_given_evaluated prep_given_in auxpass_given_is nsubjpass_given_approach nn_lexicons_translation amod_lexicons_phrasal amod_lexicons_context-dependent dobj_incorporating_lexicons vmod_approach_incorporating nn_approach_WSD det_approach_Another
D09-1022	D07-1007	o	Second instead of disambiguating phrase senses as in -LRB- Carpuat and Wu 2007 -RRB- we model word selection independently of the phrases used in the MT models	nn_models_MT det_models_the prep_in_used_models vmod_phrases_used det_phrases_the prep_of_independently_phrases advmod_selection_independently nn_selection_word dobj_model_selection nsubj_model_we prep_instead_of_model_senses advmod_model_Second amod_Carpuat_2007 conj_and_Carpuat_Wu dep_in_Wu dep_in_Carpuat pcomp_as_in prep_senses_as nn_senses_phrase amod_senses_disambiguating
D09-1046	D07-1007	o	The senses are 1 material from cellulose 2 report 3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object inventory is suitable for which application other than cross-lingual applications where the inventory can be determined from parallel data -LRB- Carpuat and Wu 2007 Chan et al. 2007 -RRB-	num_Chan_2007 nn_Chan_al. nn_Chan_et dep_Carpuat_Chan conj_and_Carpuat_2007 conj_and_Carpuat_Wu dep_data_2007 dep_data_Wu dep_data_Carpuat amod_data_parallel prep_from_determined_data auxpass_determined_be aux_determined_can nsubjpass_determined_inventory advmod_determined_where det_inventory_the rcmod_applications_determined amod_applications_cross-lingual prep_than_other_applications amod_application_other dep_suitable_application prep_for_suitable_which cop_suitable_is nsubj_suitable_material nn_inventory_object amod_inventory_physical num_inventory_7 nn_inventory_firm nn_inventory_publishing num_inventory_6 amod_inventory_scientific num_inventory_5 dobj_writing_inventory prepc_for_medium_writing num_medium_4 nn_medium_publication num_medium_3 dep_report_medium num_report_2 nn_report_cellulose prep_from_material_report num_material_1 parataxis_are_suitable nsubj_are_senses det_senses_The rcmod_``_are
I08-1073	D07-1007	p	There has been considerable skepticism over whether WSD will actually improve performance of applications but we are now starting to see improvement in performance due to WSD in cross-lingual information retrieval -LRB- Clough and Stevenson 2004 Vossen et al. 2006 -RRB- and machine translation -LRB- Carpuat and Wu 2007 Chan et al. 2007 -RRB- and we hope that other applications such as question-answering text simplication and summarisation might also benet as WSD methods improve	nsubj_improve_methods mark_improve_as nn_methods_WSD advcl_benet_improve advmod_benet_also aux_benet_might nsubj_benet_applications mark_benet_that nn_simplication_text conj_and_question-answering_summarisation conj_and_question-answering_simplication prep_such_as_applications_summarisation prep_such_as_applications_simplication prep_such_as_applications_question-answering amod_applications_other ccomp_hope_benet nsubj_hope_we num_Chan_2007 nn_Chan_al. nn_Chan_et conj_and_Carpuat_Chan conj_and_Carpuat_2007 conj_and_Carpuat_Wu dep_translation_Chan dep_translation_2007 dep_translation_Wu dep_translation_Carpuat nn_translation_machine num_Vossen_2006 nn_Vossen_al. nn_Vossen_et dep_Clough_Vossen num_Clough_2004 conj_and_Clough_Stevenson conj_and_retrieval_translation appos_retrieval_Stevenson appos_retrieval_Clough nn_retrieval_information amod_retrieval_cross-lingual prep_in_WSD_translation prep_in_WSD_retrieval prep_due_to_performance_WSD prep_in_improvement_performance dobj_see_improvement aux_see_to conj_and_starting_hope xcomp_starting_see advmod_starting_now aux_starting_are nsubj_starting_we prep_of_performance_applications dobj_improve_performance advmod_improve_actually aux_improve_will nsubj_improve_WSD mark_improve_whether conj_but_skepticism_hope conj_but_skepticism_starting prepc_over_skepticism_improve amod_skepticism_considerable cop_skepticism_been aux_skepticism_has expl_skepticism_There
P08-1024	D07-1007	p	Promising features might include those over source side reordering rules -LRB- Wang et al. 2007 -RRB- or source context features -LRB- Carpuat and Wu 2007 -RRB-	amod_Carpuat_2007 conj_and_Carpuat_Wu dep_features_Wu dep_features_Carpuat nn_features_context amod_Wang_2007 dep_Wang_al. nn_Wang_et nn_rules_reordering nn_rules_side nn_rules_source dep_those_features conj_or_those_source dep_those_Wang prep_over_those_rules dobj_include_source dobj_include_those aux_include_might nsubj_include_features amod_features_Promising
P08-1049	D07-1007	p	On the other hand integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation -LRB- WSD -RRB- into SMT systems different ways of integration lead to conflicting conclusions on whether WSD helps MT performance -LRB- Chan et al. 2007 Carpuat and Wu 2007 -RRB-	amod_Carpuat_2007 conj_and_Carpuat_Wu dep_Chan_Wu dep_Chan_Carpuat appos_Chan_2007 dep_Chan_al. nn_Chan_et nn_performance_MT dobj_helps_performance nsubj_helps_WSD mark_helps_whether amod_conclusions_conflicting nn_lead_integration dep_ways_Chan prepc_on_ways_helps prep_to_ways_conclusions prep_of_ways_lead amod_ways_different nn_systems_SMT appos_disambiguation_WSD nn_disambiguation_sense nn_disambiguation_word prep_into_integrating_systems dobj_integrating_disambiguation prepc_on_research_integrating det_research_the prep_in_evident_research mark_evident_as dep_tricky_ways advcl_tricky_evident advmod_tricky_notoriously cop_tricky_is csubj_tricky_integrating prep_on_tricky_hand nn_system_SMT nn_system_baseline det_system_a amod_component_additional det_component_an prep_into_integrating_system dobj_integrating_component amod_hand_other det_hand_the
P08-1087	D07-1007	o	Carpuat and Wu -LRB- 2007 -RRB- approached the issue as a Word Sense Disambiguation problem	nn_problem_Disambiguation nn_problem_Sense nn_problem_Word det_problem_a det_issue_the prep_as_approached_problem dobj_approached_issue nsubj_approached_Wu nsubj_approached_Carpuat appos_Wu_2007 conj_and_Carpuat_Wu
W08-0302	D07-1007	o	Carpuat and Wu -LRB- 2007 -RRB- and Chan et al.	dep_Chan_al. nn_Chan_et appos_Wu_2007 conj_and_Carpuat_Chan conj_and_Carpuat_Wu
W08-0404	D07-1007	o	Maximum entropy estimation for translation of individual words dates back to Berger et al -LRB- 1996 -RRB- and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings -LRB- Chan et al 2007 Carpuat and Wu 2007a Carpuat and Wu 2007b -RRB-	nn_2007b_Wu conj_and_Carpuat_2007b nn_2007a_Wu nn_2007a_Carpuat conj_and_Carpuat_Wu dep_Chan_2007b dep_Chan_Carpuat conj_Chan_2007a dep_Chan_2007 dep_Chan_al nn_Chan_et prep_to_generalized_substrings nsubj_generalized_estimation dep_reintroducedundertherubricofwordsensedisambiguation_Chan conj_and_reintroducedundertherubricofwordsensedisambiguation_generalized advmod_reintroducedundertherubricofwordsensedisambiguation_recently cop_reintroducedundertherubricofwordsensedisambiguation_been aux_reintroducedundertherubricofwordsensedisambiguation_has nsubj_reintroducedundertherubricofwordsensedisambiguation_idea nsubj_reintroducedundertherubricofwordsensedisambiguation_estimation nn_estimates_frequency amod_estimates_relative prep_through_made_estimates advmod_made_normally vmod_predictions_made dobj_sharpen_predictions aux_sharpen_to amod_classifiers_multi-class vmod_using_sharpen dobj_using_classifiers prepc_of_idea_using det_idea_the dep_al_1996 dep_Berger_al nn_Berger_et prep_to_back_Berger advmod_dates_back nn_dates_words amod_dates_individual prep_of_translation_dates conj_and_estimation_idea prep_for_estimation_translation amod_estimation_entropy nn_estimation_Maximum
W08-0404	D07-1007	o	4 are equivalent to a maximum entropy variant of the phrase sense disambiguation approach studied by Carpuat & Wu -LRB- 2007b -RRB-	appos_Carpuat_2007b conj_and_Carpuat_Wu agent_studied_Wu agent_studied_Carpuat vmod_approach_studied nn_approach_disambiguation nn_approach_sense nn_approach_phrase det_approach_the prep_of_variant_approach nn_variant_entropy nn_variant_maximum det_variant_a prep_to_equivalent_variant cop_equivalent_are nsubj_equivalent_4
W09-2404	D07-1007	p	In Statistical Machine Translation -LRB- SMT -RRB- recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories -LRB- Carpuat and Wu 2007 Chan et al. 2007 Gimenez and M`arquez 2007 -RRB-	num_Chan_2007 nn_Chan_al. nn_Chan_et dep_Carpuat_2007 conj_and_Carpuat_M`arquez conj_and_Carpuat_Gimenez conj_and_Carpuat_Chan conj_and_Carpuat_2007 conj_and_Carpuat_Wu dep_inventories_M`arquez dep_inventories_Gimenez dep_inventories_Chan dep_inventories_2007 dep_inventories_Wu dep_inventories_Carpuat nn_inventories_sense nn_candidates_translation prep_as_uses_inventories dobj_uses_candidates advmod_uses_directly nsubj_uses_system advmod_uses_when nn_system_WSD det_system_the nn_quality_translation advcl_helps_uses dobj_helps_quality nsubj_helps_WSD mark_helps_that ccomp_shows_helps nsubj_shows_work prep_in_shows_Translation amod_work_recent appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical
W09-2404	D07-1007	o	Even the recent generation of SMT models that explicitly use WSD modeling to perform lexical choice rely on sentence context rather than wider document context and translate sentences in isolation -LRB- Carpuat and Wu 2007 Chan et al. 2007 Gimenez and M`arquez 2007 Stroppa et al. 2007 Specia et al. 2008 -RRB-	num_Specia_2008 nn_Specia_al. nn_Specia_et num_Stroppa_2007 nn_Stroppa_al. nn_Stroppa_et dep_Gimenez_Specia conj_and_Gimenez_Stroppa conj_and_Gimenez_2007 conj_and_Gimenez_M`arquez num_Chan_2007 nn_Chan_al. nn_Chan_et dep_Carpuat_Stroppa dep_Carpuat_2007 dep_Carpuat_M`arquez dep_Carpuat_Gimenez conj_and_Carpuat_Chan conj_and_Carpuat_2007 conj_and_Carpuat_Wu dep_isolation_Chan dep_isolation_2007 dep_isolation_Wu dep_isolation_Carpuat prep_in_translate_isolation dobj_translate_sentences nsubj_translate_generation nn_context_document amod_context_wider nn_context_sentence conj_negcc_on_context pobj_on_context conj_and_rely_translate prep_rely_context prep_rely_on nsubj_rely_generation amod_choice_lexical dobj_perform_choice aux_perform_to nn_modeling_WSD vmod_use_perform dobj_use_modeling advmod_use_explicitly nsubj_use_that rcmod_models_use nn_models_SMT prep_of_generation_models amod_generation_recent det_generation_the advmod_generation_Even
W09-2410	D07-1007	p	We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation -LRB- Carpuat and Wu 2007 Chan et al. 2007 -RRB-	num_Chan_2007 nn_Chan_al. nn_Chan_et dep_Carpuat_Chan conj_and_Carpuat_2007 conj_and_Carpuat_Wu dep_Translation_2007 dep_Translation_Wu dep_Translation_Carpuat nn_Translation_Machine prep_such_as_applications_Translation nn_applications_NLP prep_in_effect_applications prep_of_effect_WSD amod_effect_positive det_effect_a prep_of_beginnings_effect det_beginnings_the dobj_see_beginnings aux_see_to xcomp_starting_see aux_starting_are nsubj_starting_We ccomp_``_starting
W09-2412	D07-1007	o	Unlike a full blown machine translation task -LRB- Carpuat and Wu 2007 -RRB- annotators and systems will not be required to translate the whole context but just the target word	nn_word_target det_word_the advmod_word_just amod_context_whole det_context_the conj_but_translate_word dobj_translate_context aux_translate_to xcomp_required_word xcomp_required_translate auxpass_required_be neg_required_not aux_required_will nsubjpass_required_systems nsubjpass_required_annotators prep_unlike_required_task conj_and_annotators_systems amod_Carpuat_2007 conj_and_Carpuat_Wu dep_task_Wu dep_task_Carpuat nn_task_translation nn_task_machine amod_task_blown amod_task_full det_task_a
W09-2413	D07-1007	p	Several studies have demonstrated that for instance Statistical Machine Translation -LRB- SMT -RRB- benefits from incorporating a dedicated WSD module -LRB- Chan et al. 2007 Carpuat and Wu 2007 -RRB-	amod_Carpuat_2007 conj_and_Carpuat_Wu dep_Chan_Wu dep_Chan_Carpuat num_Chan_2007 dep_Chan_al. nn_Chan_et appos_module_Chan nn_module_WSD amod_module_dedicated det_module_a dobj_incorporating_module nn_benefits_Translation appos_Translation_SMT nn_Translation_Machine amod_Translation_Statistical nn_Translation_instance prepc_from_that_incorporating prep_for_that_benefits dep_demonstrated_that aux_demonstrated_have nsubj_demonstrated_studies amod_studies_Several
C08-1015	D07-1013	o	A simple example is shown in Figure 1 where the arc between a and hat indicates that hat is the head of a. Current statistical dependency parsers perform better if the dependency lengthes are shorter -LRB- McDonald and Nivre 2007 -RRB-	dep_McDonald_2007 conj_and_McDonald_Nivre cop_shorter_are nsubj_shorter_lengthes mark_shorter_if nn_lengthes_dependency det_lengthes_the dep_perform_Nivre dep_perform_McDonald advcl_perform_shorter dobj_perform_better nn_parsers_dependency amod_parsers_statistical nn_parsers_Current nn_parsers_a. dep_head_perform prep_of_head_parsers det_head_the cop_head_is nsubj_head_hat mark_head_that ccomp_indicates_head nsubj_indicates_arc advmod_indicates_where conj_and_a_hat prep_between_arc_hat prep_between_arc_a det_arc_the rcmod_Figure_indicates num_Figure_1 prep_in_shown_Figure auxpass_shown_is nsubjpass_shown_example amod_example_simple det_example_A ccomp_``_shown
C08-1081	D07-1013	o	The corresponding unlabeled figures are 73.3 and 33.4.3 This confirms the results of previous studies showing that the pseudo-projective parsing technique used by MaltParser tends to give high precision given that non-projective dependencies are among the most difficult to parse correctly but rather low recall -LRB- McDonald and Nivre 2007 -RRB-	dep_McDonald_2007 conj_and_McDonald_Nivre appos_recall_Nivre appos_recall_McDonald amod_recall_low advmod_low_rather conj_but_parse_recall advmod_parse_correctly aux_parse_to xcomp_difficult_recall xcomp_difficult_parse advmod_difficult_most det_difficult_the prep_among_are_difficult nsubj_are_dependencies mark_are_that amod_dependencies_non-projective pcomp_given_are amod_precision_high prep_give_given dobj_give_precision aux_give_to xcomp_tends_give nsubj_tends_technique mark_tends_that agent_used_MaltParser vmod_technique_used nn_technique_parsing amod_technique_pseudo-projective det_technique_the ccomp_showing_tends vmod_studies_showing amod_studies_previous prep_of_results_studies det_results_the dobj_confirms_results nsubj_confirms_This rcmod_73.3_confirms conj_and_73.3_33.4.3 cop_73.3_are nsubj_73.3_figures amod_figures_unlabeled amod_figures_corresponding det_figures_The ccomp_``_33.4.3 ccomp_``_73.3
C08-1081	D07-1013	o	3 MaltParser MaltParser -LRB- Nivre et al. 2007b -RRB- is a languageindependent system for data-driven dependency parsing based on a transition-based parsing model -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_model_Nivre dep_model_McDonald nn_model_parsing amod_model_transition-based det_model_a prep_on_based_model nn_parsing_dependency amod_parsing_data-driven vmod_system_based prep_for_system_parsing amod_system_languageindependent det_system_a cop_system_is nsubj_system_MaltParser appos_Nivre_2007b dep_Nivre_al. nn_Nivre_et dep_MaltParser_Nivre nn_MaltParser_MaltParser num_MaltParser_3
D07-1096	D07-1013	o	We then describe the two main paradigms for learning and inference in this years shared task as well as in last years which we call transition-based parsers -LRB- section 5.2 -RRB- and graph-based parsers -LRB- section 5.3 -RRB- adopting the terminology of McDonald and Nivre -LRB- 2007 -RRB- .5 Finally we give an overview of the domain adaptation methods that were used -LRB- section 5.4 -RRB-	num_section_5.4 dep_used_section auxpass_used_were nsubjpass_used_that rcmod_methods_used nn_methods_adaptation nn_methods_domain det_methods_the prep_of_overview_methods det_overview_an dobj_give_overview nsubj_give_we advmod_.5_Finally dep_2007_.5 dep_Nivre_2007 conj_and_McDonald_Nivre prep_of_terminology_Nivre prep_of_terminology_McDonald det_terminology_the dobj_adopting_terminology num_section_5.3 appos_parsers_section amod_parsers_graph-based num_section_5.2 conj_and_parsers_parsers appos_parsers_section amod_parsers_transition-based dobj_call_parsers dobj_call_parsers nsubj_call_we dobj_call_which amod_years_last pobj_in_years rcmod_task_give vmod_task_adopting rcmod_task_call conj_and_task_in amod_task_shared dep_years_in dep_years_task det_years_this prep_in_learning_years conj_and_learning_inference amod_paradigms_main num_paradigms_two det_paradigms_the prepc_for_describe_inference prepc_for_describe_learning dobj_describe_paradigms advmod_describe_then nsubj_describe_We ccomp_``_describe
D07-1097	D07-1013	o	As shown by McDonald and Nivre -LRB- 2007 -RRB- the Single Malt parser tends to suffer from two problems error propagation due to the deterministic parsing strategy typicallyaffectinglongdependenciesmorethan short ones and low precision on dependencies originating in the artificial root node due to fragmented parses .9 The question is which of these problems is alleviatedbythemultipleviewsgivenbythecomponent parsers in the Blended system	amod_system_Blended det_system_the prep_in_parsers_system amod_parsers_alleviatedbythemultipleviewsgivenbythecomponent cop_parsers_is nsubj_parsers_which det_problems_these prep_of_which_problems ccomp_is_parsers dep_question_is det_question_The num_question_.9 dobj_parses_question dep_fragmented_parses prep_due_to_node_fragmented nn_node_root amod_node_artificial det_node_the prep_in_originating_node vmod_dependencies_originating prep_on_precision_dependencies amod_precision_low amod_ones_short nn_ones_typicallyaffectinglongdependenciesmorethan nn_strategy_parsing amod_strategy_deterministic det_strategy_the prep_to_due_strategy conj_and_propagation_precision conj_and_propagation_ones amod_propagation_due nn_propagation_error dep_problems_precision dep_problems_ones dep_problems_propagation num_problems_two prep_from_suffer_problems aux_suffer_to xcomp_tends_suffer nsubj_tends_parser advcl_tends_shown nn_parser_Malt amod_parser_Single det_parser_the appos_Nivre_2007 conj_and_McDonald_Nivre prep_by_shown_Nivre prep_by_shown_McDonald mark_shown_As
D08-1017	D07-1013	p	A solution that leverages the complementary strengths of these two approachesdescribed in detail by McDonald and Nivre -LRB- 2007 -RRB- was recently and successfully explored by Nivre and McDonald -LRB- 2008 -RRB-	appos_McDonald_2008 conj_and_Nivre_McDonald agent_explored_McDonald agent_explored_Nivre advmod_explored_successfully advmod_explored_recently auxpass_explored_was nsubjpass_explored_two conj_and_recently_successfully appos_Nivre_2007 conj_and_McDonald_Nivre agent_approachesdescribed_Nivre agent_approachesdescribed_McDonald prep_in_approachesdescribed_detail vmod_two_approachesdescribed rcmod_these_explored prep_of_strengths_these amod_strengths_complementary det_strengths_the dobj_leverages_strengths nsubj_leverages_that rcmod_solution_leverages det_solution_A
D08-1059	D07-1013	o	However they make different types of errors which can be seen as a reflection of their theoretical differences -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_differences_Nivre dep_differences_McDonald amod_differences_theoretical poss_differences_their prep_of_reflection_differences det_reflection_a prep_as_seen_reflection auxpass_seen_be aux_seen_can nsubjpass_seen_which rcmod_types_seen prep_of_types_errors amod_types_different dobj_make_types nsubj_make_they advmod_make_However
D08-1059	D07-1013	o	-LRB- 2007 -RRB- and Nivre and McDonald -LRB- 2008 -RRB- can be seen as methods to combine separately defined models	amod_models_defined advmod_defined_separately dobj_combine_models aux_combine_to vmod_methods_combine prep_as_seen_methods auxpass_seen_be aux_seen_can nsubjpass_seen_McDonald nsubjpass_seen_Nivre nsubjpass_seen_2007 appos_McDonald_2008 conj_and_2007_McDonald conj_and_2007_Nivre
D08-1059	D07-1013	o	The terms graph-based and transition-based were used by McDonald and Nivre -LRB- 2007 -RRB- to describe the difference between MSTParser -LRB- McDonald and Pereira 2006 -RRB- which is a graph-based parser with an exhaustive search decoder and MaltParser -LRB- Nivre et al. 2006 -RRB- which is a transition-based parser with a greedy search decoder	nn_decoder_search amod_decoder_greedy det_decoder_a prep_with_parser_decoder amod_parser_transition-based det_parser_a cop_parser_is nsubj_parser_which amod_Nivre_2006 dep_Nivre_al. nn_Nivre_et nn_decoder_search amod_decoder_exhaustive det_decoder_an prep_with_parser_decoder amod_parser_graph-based det_parser_a cop_parser_is nsubj_parser_which dep_McDonald_2006 conj_and_McDonald_Pereira conj_and_MSTParser_MaltParser rcmod_MSTParser_parser appos_MSTParser_Pereira appos_MSTParser_McDonald prep_between_difference_MaltParser prep_between_difference_MSTParser det_difference_the dobj_describe_difference aux_describe_to appos_Nivre_2007 conj_and_McDonald_Nivre dep_used_parser dep_used_Nivre xcomp_used_describe agent_used_Nivre agent_used_McDonald auxpass_used_were nsubjpass_used_terms conj_and_graph-based_transition-based amod_terms_transition-based amod_terms_graph-based det_terms_The
D08-1059	D07-1013	o	McDonald and Nivre -LRB- 2007 -RRB- showed that the MSTParser and MaltParser produce different errors	amod_errors_different dobj_produce_errors nsubj_produce_MaltParser nsubj_produce_MSTParser mark_produce_that conj_and_MSTParser_MaltParser det_MSTParser_the ccomp_showed_produce nsubj_showed_Nivre nsubj_showed_McDonald appos_Nivre_2007 conj_and_McDonald_Nivre
D09-1121	D07-1013	o	In the field of parsing McDonald and Nivre -LRB- 2007 -RRB- compared parsing errors between graphbased and transition-based parsers	amod_parsers_transition-based amod_parsers_graphbased conj_and_graphbased_transition-based prep_between_errors_parsers amod_errors_parsing dobj_compared_errors nsubj_compared_Nivre nsubj_compared_McDonald prep_in_compared_field appos_Nivre_2007 conj_and_McDonald_Nivre prep_of_field_parsing det_field_the
D09-1121	D07-1013	o	In examining the combination of the two types of parsing McDonald and Nivre -LRB- 2007 -RRB- utilized similar approaches to our empirical analysis	amod_analysis_empirical poss_analysis_our amod_approaches_similar prep_to_utilized_analysis dobj_utilized_approaches prepc_in_utilized_examining appos_Nivre_2007 conj_and_parsing_Nivre conj_and_parsing_McDonald prep_of_types_Nivre prep_of_types_McDonald prep_of_types_parsing num_types_two det_types_the prep_of_combination_types det_combination_the dobj_examining_combination
E09-1023	D07-1013	o	also McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_also_Nivre dep_also_McDonald dep_``_also
I08-1012	D07-1013	o	F 1 = 2precisionrecall / -LRB- precision + recall -RRB- the figure we find that F 1 score decreases when dependency length increases as -LRB- McDonald and Nivre 2007 -RRB- found	dep_found_Nivre dep_found_McDonald mark_found_as dep_McDonald_2007 conj_and_McDonald_Nivre advcl_increases_found nsubj_increases_length advmod_increases_when nn_length_dependency advcl_decreases_increases nsubj_decreases_score mark_decreases_that num_score_1 nn_score_F ccomp_find_decreases nsubj_find_we nsubj_find_figure det_figure_the amod_figure_2precisionrecall amod_figure_= num_figure_1 nn_figure_F conj_+_precision_recall dep_2precisionrecall_recall dep_2precisionrecall_precision
I08-1012	D07-1013	o	The reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronouns modifying their direct neighbors while longer dependencies typically represent modifiers of the root or the main verb in a sentence -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre appos_sentence_Nivre appos_sentence_McDonald det_sentence_a prep_in_verb_sentence dep_main_verb det_main_the conj_or_root_main det_root_the prep_of_modifiers_main prep_of_modifiers_root dobj_represent_modifiers advmod_represent_typically nsubj_represent_dependencies mark_represent_while amod_dependencies_longer amod_neighbors_direct poss_neighbors_their dobj_modifying_neighbors vmod_determiners_modifying conj_or_determiners_pronouns conj_or_determiners_adjectives prep_such_as_nouns_pronouns prep_such_as_nouns_adjectives prep_such_as_nouns_determiners advcl_modifier_represent prep_of_modifier_nouns advmod_modifier_often cop_modifier_are nsubj_modifier_dependencies mark_modifier_that amod_dependencies_shorter ccomp_be_modifier aux_be_may nsubj_be_reason det_reason_The ccomp_``_be
I08-1012	D07-1013	o	However current statistical dependency parsers provide worse results if the dependency length becomes longer -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_becomes_Nivre dep_becomes_McDonald acomp_becomes_longer nsubj_becomes_length mark_becomes_if nn_length_dependency det_length_the amod_results_worse advcl_provide_becomes dobj_provide_results nsubj_provide_parsers advmod_provide_However nn_parsers_dependency amod_parsers_statistical amod_parsers_current
I08-2097	D07-1013	o	sentence length The longer the sentence is the poorer the parser performs -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre nsubj_performs_parser det_parser_the ccomp_poorer_performs det_poorer_the dep_sentence_Nivre dep_sentence_McDonald dep_sentence_poorer dep_sentence_is det_sentence_the dep_sentence_longer det_longer_The dep_length_sentence nn_length_sentence
I08-2097	D07-1013	o	dependency lengths Long-distance dependencies exhibit bad performance -LRB- McDonald and Nivre 2007 -RRB-	dep_McDonald_2007 conj_and_McDonald_Nivre dep_performance_Nivre dep_performance_McDonald amod_performance_bad dobj_exhibit_performance nsubj_exhibit_dependencies amod_dependencies_Long-distance dep_lengths_exhibit nn_lengths_dependency
J08-4003	D07-1013	o	Looking rst at learning times it is obvious that learning time depends primarily on the number of training instances which is why we can observe a difference of several orders of magnitude in learning time between the biggest training set -LRB- Czech -RRB- and the smallest training set -LRB- Slovene -RRB- 14 This is shown by Nivre and Scholz -LRB- 2004 -RRB- in comparison to the iterative arc-standard algorithm of Yamada and Matsumoto -LRB- 2003 -RRB- and by McDonald and Nivre -LRB- 2007 -RRB- in comparison to the spanning tree algorithm of McDonald Lerman and Pereira -LRB- 2006 -RRB-	appos_Pereira_2006 conj_and_McDonald_Pereira conj_and_McDonald_Lerman prep_of_algorithm_Pereira prep_of_algorithm_Lerman prep_of_algorithm_McDonald nn_algorithm_tree amod_algorithm_spanning det_algorithm_the prep_to_comparison_algorithm appos_Nivre_2007 conj_and_McDonald_Nivre appos_Matsumoto_2003 conj_and_Yamada_Matsumoto prep_of_algorithm_Matsumoto prep_of_algorithm_Yamada amod_algorithm_arc-standard amod_algorithm_iterative det_algorithm_the prep_by_comparison_Nivre prep_by_comparison_McDonald prep_to_comparison_algorithm conj_and_comparison_comparison appos_Scholz_2004 conj_and_Nivre_Scholz prep_in_shown_comparison prep_in_shown_comparison prep_in_shown_comparison agent_shown_Scholz agent_shown_Nivre auxpass_shown_is nsubjpass_shown_This rcmod_set_shown num_set_14 appos_set_Slovene nn_set_training amod_set_smallest det_set_the conj_and_set_set appos_set_Czech nn_set_training amod_set_biggest det_set_the prep_between_time_set prep_between_time_set amod_time_learning prep_of_orders_magnitude amod_orders_several prep_in_difference_time prep_of_difference_orders det_difference_a dobj_observe_difference aux_observe_can nsubj_observe_we advmod_observe_why advcl_is_observe nsubj_is_which nn_instances_training rcmod_number_is prep_of_number_instances det_number_the prep_on_depends_number advmod_depends_primarily nsubj_depends_time mark_depends_that amod_time_learning ccomp_obvious_depends cop_obvious_is nsubj_obvious_it vmod_obvious_Looking amod_times_learning prep_at_Looking_times dobj_Looking_rst
N09-2066	D07-1013	o	c 2009 Association for Computational Linguistics Reverse Revision and Linear Tree Combination for Dependency Parsing Giuseppe Attardi Dipartimento di Informatica Universit ` a di Pisa Pisa Italy attardi@di.unipi.it Felice DellOrletta Dipartimento di Informatica Universit ` a di Pisa Pisa Italy felice.dellorletta@di.unipi.it 1 Introduction Deterministic transition-based Shift/Reduce dependency parsers make often mistakes in the analysis of long span dependencies -LRB- McDonald & Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_dependencies_Nivre dep_dependencies_McDonald nn_dependencies_span amod_dependencies_long prep_of_analysis_dependencies det_analysis_the advmod_mistakes_often prep_in_make_analysis dobj_make_mistakes nsubj_make_parsers nn_parsers_dependency nn_parsers_Shift/Reduce amod_parsers_transition-based amod_parsers_Deterministic nn_parsers_Introduction num_Introduction_1 nn_Introduction_felice.dellorletta@di.unipi.it nn_Introduction_Italy rcmod_Pisa_make nn_Pisa_Pisa nn_Pisa_di det_Pisa_a dep_`_Pisa nn_Universit_Informatica nn_Universit_di nn_Universit_Dipartimento nn_Universit_DellOrletta nn_Universit_Felice nn_Universit_attardi@di.unipi.it nn_Universit_Italy appos_Pisa_Universit nn_Pisa_Pisa nn_Pisa_di det_Pisa_a dep_Universit_Pisa nn_Universit_Informatica nn_Universit_di nn_Universit_Dipartimento nn_Universit_Attardi nn_Universit_Giuseppe nn_Universit_Parsing nn_Universit_Dependency nn_Combination_Tree nn_Combination_Linear prep_for_Revision_Universit conj_and_Revision_Combination amod_Revision_Reverse nn_Revision_Linguistics nn_Revision_Computational prep_for_Association_Combination prep_for_Association_Revision num_Association_2009 nn_Association_c
P08-1108	D07-1013	o	Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_graph-based_Nivre dep_graph-based_McDonald conj_or_graph-based_transitionbased preconj_graph-based_either prep_as_described_transitionbased prep_as_described_graph-based auxpass_described_be aux_described_can nsubjpass_described_models amod_years_recent nn_parsing_dependency prep_in_proposed_years prep_for_proposed_parsing auxpass_proposed_been aux_proposed_have nsubjpass_proposed_that rcmod_models_proposed amod_models_data-driven det_models_all advmod_models_Practically
P08-1108	D07-1013	o	In order to get a better understanding of these matters we replicate parts of the error analysis presented by McDonald and Nivre -LRB- 2007 -RRB- where parsing errors are related to different structural properties of sentences and their dependency graphs	nn_graphs_dependency poss_graphs_their conj_and_properties_graphs prep_of_properties_sentences amod_properties_structural amod_properties_different prep_to_related_graphs prep_to_related_properties cop_related_are nsubj_related_errors advmod_related_where amod_errors_parsing appos_Nivre_2007 rcmod_McDonald_related conj_and_McDonald_Nivre agent_presented_Nivre agent_presented_McDonald nn_analysis_error det_analysis_the vmod_parts_presented prep_of_parts_analysis dobj_replicate_parts nsubj_replicate_we advcl_replicate_get det_matters_these prep_of_understanding_matters amod_understanding_better det_understanding_a dobj_get_understanding aux_get_to dep_get_order mark_get_In
P08-1108	D07-1013	o	As expected Malt and MST have very similar accuracy for short sentences but Malt degrades more rapidly with increasing sentence length because of error propagation -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_propagation_Nivre dep_propagation_McDonald nn_propagation_error nn_length_sentence amod_length_increasing advmod_rapidly_more prep_because_of_degrades_propagation prep_with_degrades_length advmod_degrades_rapidly nsubj_degrades_Malt amod_sentences_short amod_accuracy_similar advmod_similar_very conj_but_have_degrades prep_for_have_sentences dobj_have_accuracy nsubj_have_MST nsubj_have_Malt advcl_have_expected conj_and_Malt_MST mark_expected_As
P08-1108	D07-1013	o	First the graph-based models have better precision than the transition-based models when predicting long arcs which is compatible with the results of McDonald and Nivre -LRB- 2007 -RRB-	appos_Nivre_2007 conj_and_McDonald_Nivre prep_of_results_Nivre prep_of_results_McDonald det_results_the prep_with_compatible_results cop_compatible_is nsubj_compatible_which rcmod_arcs_compatible amod_arcs_long dobj_predicting_arcs advmod_predicting_when amod_models_transition-based det_models_the prep_than_precision_models amod_precision_better advcl_have_predicting dobj_have_precision nsubj_have_models advmod_have_First amod_models_graph-based det_models_the
P08-1108	D07-1013	o	Again we find the clearest patterns in the graphs for precision where Malt has very low precision near the root but improves with increasing depth while MST shows the opposite trend -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_trend_Nivre dep_trend_McDonald amod_trend_opposite det_trend_the dobj_shows_trend nsubj_shows_MST mark_shows_while amod_depth_increasing prep_with_improves_depth nsubj_improves_Malt det_root_the amod_precision_low advmod_low_very conj_but_has_improves prep_near_has_root dobj_has_precision nsubj_has_Malt advmod_has_where rcmod_precision_improves rcmod_precision_has prep_for_graphs_precision det_graphs_the prep_in_patterns_graphs amod_patterns_clearest det_patterns_the advcl_find_shows dobj_find_patterns nsubj_find_we advmod_find_Again
P08-1108	D07-1013	o	As expected we see that MST does better than Malt for all categories except nouns and pronouns -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_pronouns_Nivre dep_pronouns_McDonald conj_and_nouns_pronouns det_categories_all prep_for_Malt_categories prep_than_better_Malt prep_except_does_pronouns prep_except_does_nouns dobj_does_better dep_MST_does dep_that_MST prep_see_that nsubj_see_we advcl_see_expected mark_expected_As
P08-1108	D07-1013	o	Both models have been used to achieve state-of-the-art accuracy for a wide range of languages as shown in the CoNLL shared tasks on dependency parsing -LRB- Buchholz and Marsi 2006 Nivre et al. 2007 -RRB- but McDonald and Nivre -LRB- 2007 -RRB- showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models	num_models_two det_models_the prep_with_associated_models vmod_errors_associated prep_of_distribution_errors det_distribution_the prep_in_differences_distribution amod_differences_important dobj_reveals_differences nsubj_reveals_analysis mark_reveals_that nn_analysis_error amod_analysis_detailed det_analysis_a ccomp_showed_reveals nsubj_showed_Nivre nsubj_showed_McDonald appos_Nivre_2007 conj_and_McDonald_Nivre num_Nivre_2007 nn_Nivre_al. nn_Nivre_et dep_Buchholz_Nivre dep_Buchholz_2006 conj_and_Buchholz_Marsi appos_parsing_Marsi appos_parsing_Buchholz nn_parsing_dependency prep_on_tasks_parsing dobj_shared_tasks det_CoNLL_the dep_shown_shared prep_in_shown_CoNLL mark_shown_as prep_of_range_languages amod_range_wide det_range_a prep_for_accuracy_range amod_accuracy_state-of-the-art dobj_achieve_accuracy aux_achieve_to conj_but_used_showed advcl_used_shown xcomp_used_achieve auxpass_used_been aux_used_have nsubjpass_used_models det_models_Both
P08-1108	D07-1013	o	This difference was highlighted in the 3http / / w3.msi.vxu.se / jha/maltparser / studyofMcDonaldandNivre -LRB- 2007 -RRB- whichshowed that the difference is reflected directly in the error distributions of the parsers	det_parsers_the prep_of_distributions_parsers nn_distributions_error det_distributions_the prep_in_reflected_distributions advmod_reflected_directly auxpass_reflected_is nsubjpass_reflected_difference mark_reflected_that det_difference_the ccomp_whichshowed_reflected vmod_studyofMcDonaldandNivre_whichshowed appos_studyofMcDonaldandNivre_2007 nn_studyofMcDonaldandNivre_jha/maltparser dep_w3.msi.vxu.se_studyofMcDonaldandNivre det_3http_the dep_highlighted_w3.msi.vxu.se prep_in_highlighted_3http auxpass_highlighted_was nsubjpass_highlighted_difference det_difference_This
P08-1110	D07-1013	o	7An alternative framework that formally describes some dependency parsers is that of transition systems -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_systems_Nivre dep_systems_McDonald nn_systems_transition prep_of_that_systems cop_that_is nsubj_that_framework nn_parsers_dependency det_parsers_some dobj_describes_parsers advmod_describes_formally nsubj_describes_that rcmod_framework_describes nn_framework_alternative amod_framework_7An
P09-1007	D07-1013	o	The experimental results in -LRB- McDonald and Nivre 2007 -RRB- show a negative impact on the parsing accuracy from too long dependency relation	nn_relation_dependency amod_relation_long advmod_long_too nn_accuracy_parsing det_accuracy_the prep_from_impact_relation prep_on_impact_accuracy amod_impact_negative det_impact_a dobj_show_impact nsubj_show_results dep_McDonald_2007 conj_and_McDonald_Nivre dep_in_Nivre dep_in_McDonald prep_results_in amod_results_experimental det_results_The
P09-1007	D07-1013	o	4 Dependency Parsing Baseline 4.1 Learning Model and Features According to -LRB- McDonald and Nivre 2007 -RRB- all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based	conj_or_graph-based_transition-based preconj_graph-based_either prep_as_described_transition-based prep_as_described_graph-based auxpass_described_be aux_described_can nsubjpass_described_models amod_years_recent prep_in_proposed_years auxpass_proposed_been aux_proposed_have nsubjpass_proposed_that rcmod_parsing_proposed nn_parsing_dependency prep_for_models_parsing amod_models_data-driven det_models_all vmod_McDonald_described dep_McDonald_2007 conj_and_McDonald_Nivre pobj_Features_Nivre pobj_Features_McDonald prepc_according_to_Features_to conj_and_Model_Features nn_Model_Learning num_Model_4.1 nn_Model_Baseline dep_Parsing_Features dep_Parsing_Model nn_Parsing_Dependency num_Parsing_4 dep_``_Parsing
P09-3002	D07-1013	o	-LRB- Kuhlmann and Mohl 2007 McDonald and Nivre 2007 Nivre et al. 2007 -RRB- Hindi is a verb final flexible word order language and therefore has frequent occurrences of non-projectivity in its dependency structures	nn_structures_dependency poss_structures_its prep_in_occurrences_structures prep_of_occurrences_non-projectivity amod_occurrences_frequent dobj_has_occurrences advmod_language_therefore cc_language_and nn_language_order nn_language_word amod_language_flexible amod_language_final amod_language_verb det_language_a cop_language_is nsubj_language_Hindi nn_Hindi_Nivre num_Nivre_2007 nn_Nivre_al. nn_Nivre_et dep_McDonald_has parataxis_McDonald_language conj_and_McDonald_2007 conj_and_McDonald_Nivre dep_Kuhlmann_2007 dep_Kuhlmann_Nivre dep_Kuhlmann_McDonald dep_Kuhlmann_2007 conj_and_Kuhlmann_Mohl dep_''_Mohl dep_''_Kuhlmann
W07-2220	D07-1013	o	The majority of these systems used models belonging to one of the twodominantapproachesindata-drivendependency parsinginrecentyears -LRB- McDonaldandNivre ,2007 -RRB- In graph-based models every possible dependency graph for a given input sentence is given a score that decomposes into scores for the arcs of the graph	det_graph_the prep_of_arcs_graph det_arcs_the prep_for_scores_arcs prep_into_decomposes_scores nsubj_decomposes_that rcmod_score_decomposes det_score_a dobj_given_score auxpass_given_is nsubjpass_given_graph prep_in_given_models nn_sentence_input amod_sentence_given det_sentence_a prep_for_graph_sentence nn_graph_dependency amod_graph_possible det_graph_every amod_models_graph-based num_McDonaldandNivre_,2007 appos_parsinginrecentyears_McDonaldandNivre nn_parsinginrecentyears_twodominantapproachesindata-drivendependency det_parsinginrecentyears_the prep_of_one_parsinginrecentyears prep_to_belonging_one dep_models_given vmod_models_belonging amod_models_used dep_systems_models det_systems_these prep_of_majority_systems det_majority_The dep_``_majority
W07-2220	D07-1013	o	Acknowledgments I want to thank my fellow organizers of the shared task Johan Hall Sandra Kubler Ryan McDonald Jens Nilsson Sebastian Riedel and Deniz Yuret whoarealsoco-authorsofthelongerpaperonwhich this paper is partly based -LRB- Nivre et al. 2007 -RRB-	amod_Nivre_2007 dep_Nivre_al. nn_Nivre_et dep_based_Nivre advmod_based_partly auxpass_based_is nsubjpass_based_Yuret det_paper_this amod_paper_whoarealsoco-authorsofthelongerpaperonwhich appos_Yuret_paper nn_Yuret_Deniz nn_Riedel_Sebastian nn_Nilsson_Jens nn_McDonald_Ryan nn_Kubler_Sandra conj_and_Hall_based conj_and_Hall_Riedel conj_and_Hall_Nilsson conj_and_Hall_McDonald conj_and_Hall_Kubler nn_Hall_Johan dep_Hall_Acknowledgments amod_task_shared det_task_the prep_of_organizers_task amod_organizers_fellow poss_organizers_my dobj_thank_organizers aux_thank_to xcomp_want_thank nsubj_want_I rcmod_Acknowledgments_want
W08-2104	D07-1013	o	There are also attempts at a more fine-grained analysis of accuracy targeting specific linguistic constructions or grammatical functions -LRB- Carroll and Briscoe 2002 Kubler and Prokic 2006 McDonald and Nivre 2007 -RRB-	amod_Kubler_2007 conj_and_Kubler_Nivre conj_and_Kubler_McDonald conj_and_Kubler_2006 conj_and_Kubler_Prokic dep_Carroll_Nivre dep_Carroll_McDonald dep_Carroll_2006 dep_Carroll_Prokic dep_Carroll_Kubler conj_and_Carroll_2002 conj_and_Carroll_Briscoe amod_functions_grammatical conj_or_constructions_functions amod_constructions_linguistic amod_constructions_specific dep_targeting_2002 dep_targeting_Briscoe dep_targeting_Carroll dobj_targeting_functions dobj_targeting_constructions prep_of_analysis_accuracy amod_analysis_fine-grained det_analysis_a advmod_fine-grained_more prep_at_attempts_analysis advmod_attempts_also dep_are_targeting dep_are_attempts expl_are_There ccomp_``_are
W09-1104	D07-1013	o	5 Data-driven Dependency Parsing Models for data-driven dependency parsing can be roughly divided into two paradigms Graph-based and transition-based models -LRB- McDonald and Nivre 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Nivre dep_models_Nivre dep_models_McDonald amod_models_transition-based amod_models_Graph-based conj_and_Graph-based_transition-based dep_paradigms_models num_paradigms_two prep_into_divided_paradigms advmod_divided_roughly auxpass_divided_be aux_divided_can nsubjpass_divided_Models nn_parsing_dependency amod_parsing_data-driven prep_for_Models_parsing nn_Models_Parsing nn_Models_Dependency amod_Models_Data-driven num_Models_5
D07-1015	D07-1014	o	Two other groups of authors have independently and simultaneously proposed adaptations of the Matrix-Tree Theorem for structured inference on directed spanning trees -LRB- McDonald and Satta 2007 SmithandSmith ,2007 -RRB-	num_SmithandSmith_,2007 dep_McDonald_SmithandSmith conj_and_McDonald_2007 conj_and_McDonald_Satta appos_trees_2007 appos_trees_Satta appos_trees_McDonald dobj_spanning_trees xcomp_directed_spanning prepc_on_inference_directed amod_inference_structured nn_Theorem_Matrix-Tree det_Theorem_the prep_for_adaptations_inference prep_of_adaptations_Theorem amod_adaptations_proposed advmod_proposed_simultaneously advmod_proposed_independently conj_and_independently_simultaneously dobj_have_adaptations nsubj_have_groups prep_of_groups_authors amod_groups_other num_groups_Two
D07-1015	D07-1014	o	Second McDonald and Satta -LRB- 2007 -RRB- propose an O -LRB- n5 -RRB- algorithm for computing the marginals as opposed to the O -LRB- n3 -RRB- matrix-inversion approach used by Smith and Smith -LRB- 2007 -RRB- and ourselves	appos_Smith_2007 conj_and_Smith_Smith agent_used_Smith agent_used_Smith vmod_approach_used nn_approach_matrix-inversion nn_approach_O det_approach_the appos_O_n3 conj_and_opposed_ourselves prep_to_opposed_approach mark_opposed_as det_marginals_the dobj_computing_marginals prepc_for_algorithm_computing nn_algorithm_O det_algorithm_an appos_O_n5 advcl_propose_ourselves advcl_propose_opposed dobj_propose_algorithm nsubj_propose_Satta nsubj_propose_McDonald dep_propose_Second appos_Satta_2007 conj_and_McDonald_Satta
D07-1015	D07-1014	o	For example both papers propose minimum-risk decoding and McDonald and Satta -LRB- 2007 -RRB- discuss unsupervised learning and language modeling while Smith and Smith -LRB- 2007 -RRB- define hiddenvariable models based on spanning trees	dobj_spanning_trees prepc_on_based_spanning vmod_models_based amod_models_hiddenvariable dobj_define_models nsubj_define_Smith nsubj_define_Smith mark_define_while appos_Smith_2007 conj_and_Smith_Smith nn_modeling_language conj_and_learning_modeling amod_learning_unsupervised dobj_discuss_modeling dobj_discuss_learning nsubj_discuss_Satta nsubj_discuss_McDonald appos_Satta_2007 conj_and_McDonald_Satta amod_decoding_minimum-risk advcl_propose_define conj_and_propose_discuss dobj_propose_decoding nsubj_propose_papers prep_for_propose_example preconj_papers_both
D07-1015	D07-1014	o	Similar adaptations of the Matrix-Tree Theorem have been developed independently and simultaneouslybySmithandSmith -LRB- 2007 -RRB- andMcDonaldand Satta -LRB- 2007 -RRB- see Section 5 for more discussion	amod_discussion_more prep_for_Section_discussion num_Section_5 dobj_see_Section appos_Satta_2007 nn_Satta_andMcDonaldand appos_simultaneouslybySmithandSmith_2007 dep_independently_Satta conj_and_independently_simultaneouslybySmithandSmith parataxis_developed_see advmod_developed_simultaneouslybySmithandSmith advmod_developed_independently auxpass_developed_been aux_developed_have nsubjpass_developed_adaptations nn_Theorem_Matrix-Tree det_Theorem_the prep_of_adaptations_Theorem amod_adaptations_Similar
D07-1070	D07-1014	o	For nonprojective parsing the analogy to the inside algorithm is the O -LRB- n3 -RRB- matrix-tree algorithm which is dominated asymptotically by a matrix determinant -LRB- Smith and Smith 2007 Koo et al. 2007 McDonald and Satta 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Satta num_Koo_2007 nn_Koo_al. nn_Koo_et dep_Smith_Satta dep_Smith_McDonald conj_and_Smith_Koo conj_and_Smith_2007 conj_and_Smith_Smith dep_determinant_Koo dep_determinant_2007 dep_determinant_Smith dep_determinant_Smith nn_determinant_matrix det_determinant_a agent_dominated_determinant advmod_dominated_asymptotically auxpass_dominated_is nsubjpass_dominated_which rcmod_algorithm_dominated amod_algorithm_matrix-tree nn_algorithm_O det_algorithm_the cop_algorithm_is nsubj_algorithm_analogy prep_for_algorithm_parsing appos_O_n3 amod_algorithm_inside det_algorithm_the prep_to_analogy_algorithm det_analogy_the amod_parsing_nonprojective
D07-1102	D07-1014	o	We can sum over all non-projective spanning trees by taking the determinant of the Kirchhoff matrix of the graph defined above minus the row and column corresponding to the root node -LRB- Smith and Smith 2007 -RRB-	amod_Smith_2007 conj_and_Smith_Smith appos_node_Smith appos_node_Smith nn_node_root det_node_the prep_to_corresponding_node vmod_row_corresponding conj_and_row_column det_row_the conj_minus_above_column conj_minus_above_row dobj_defined_row dobj_defined_above vmod_graph_defined det_graph_the prep_of_matrix_graph nn_matrix_Kirchhoff det_matrix_the prep_of_determinant_matrix det_determinant_the dobj_taking_determinant nn_trees_spanning amod_trees_non-projective det_trees_all prepc_by_sum_taking prep_over_sum_trees aux_sum_can nsubj_sum_We
D08-1016	D07-1014	o	-LRB- 2007 -RRB- and Smith and Smith -LRB- 2007 -RRB- show how to employ the matrix-tree theorem	nn_theorem_matrix-tree det_theorem_the dobj_employ_theorem aux_employ_to advmod_employ_how ccomp_show_employ nsubj_show_Smith nsubj_show_Smith nsubj_show_2007 appos_Smith_2007 conj_and_2007_Smith conj_and_2007_Smith
D08-1065	D07-1014	p	The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition -LRB- Goel and Byrne 2000 -RRB- machine translation -LRB- Kumar and Byrne 2004 Zhang and Gildea 2008 -RRB- bilingual word alignment -LRB- Kumar and Byrne 2002 -RRB- andparsing -LRB- Goodman 1996 TitovandHenderson 2006 Smith and Smith 2007 -RRB-	appos_Smith_2007 conj_and_Smith_Smith conj_TitovandHenderson_2006 dep_Goodman_Smith dep_Goodman_Smith dep_Goodman_TitovandHenderson amod_Goodman_1996 dep_andparsing_Goodman dep_Kumar_2002 conj_and_Kumar_Byrne appos_alignment_Byrne appos_alignment_Kumar nn_alignment_word amod_alignment_bilingual dep_Zhang_2008 conj_and_Zhang_Gildea dep_Kumar_Gildea dep_Kumar_Zhang conj_and_Kumar_2004 conj_and_Kumar_Byrne appos_translation_2004 appos_translation_Byrne appos_translation_Kumar nn_translation_machine dep_Goel_2000 conj_and_Goel_Byrne conj_recognition_andparsing conj_recognition_alignment conj_recognition_translation dep_recognition_Byrne dep_recognition_Goel nn_recognition_speech amod_recognition_automatic prep_including_processing_recognition nn_processing_language amod_processing_natural prep_of_areas_processing amod_areas_many nn_classifier_MAP det_classifier_the prep_over_improvements_classifier prep_in_give_areas dobj_give_improvements aux_give_to xcomp_shown_give auxpass_shown_been aux_shown_has nsubjpass_shown_approach det_approach_The
D09-1058	D07-1014	o	It is often straightforward to obtain large amounts of unlabeled data making semi-supervised approaches appealing previous work on semisupervised methods for dependency parsing includes -LRB- Smith and Eisner 2007 Koo et al. 2008 Wang et al. 2008 -RRB-	num_Wang_2008 nn_Wang_al. nn_Wang_et num_Koo_2008 nn_Koo_al. nn_Koo_et dep_Smith_Wang conj_and_Smith_Koo conj_and_Smith_2007 conj_and_Smith_Eisner dep_includes_Koo dep_includes_2007 dep_includes_Eisner dep_includes_Smith nsubj_includes_work nn_parsing_dependency prep_for_methods_parsing amod_methods_semisupervised prep_on_work_methods amod_work_previous nsubj_appealing_approaches amod_approaches_semi-supervised xcomp_making_appealing amod_data_unlabeled prep_of_amounts_data amod_amounts_large dobj_obtain_amounts aux_obtain_to parataxis_straightforward_includes xcomp_straightforward_making xcomp_straightforward_obtain advmod_straightforward_often cop_straightforward_is nsubj_straightforward_It ccomp_``_straightforward
D09-1058	D07-1014	o	We used a non-projective model trained using an application of the matrix-tree theorem -LRB- Koo et al. 2007 Smith and Smith 2007 McDonald and Satta 2007 -RRB- for the first-order Czech models and projective parsers for all other models	amod_models_other det_models_all prep_for_parsers_models amod_parsers_projective nn_models_Czech amod_models_first-order det_models_the amod_Smith_2007 conj_and_Smith_Satta conj_and_Smith_McDonald conj_and_Smith_2007 conj_and_Smith_Smith conj_and_Koo_parsers prep_for_Koo_models dep_Koo_Satta dep_Koo_McDonald dep_Koo_2007 dep_Koo_Smith dep_Koo_Smith appos_Koo_2007 dep_Koo_al. nn_Koo_et nn_theorem_matrix-tree det_theorem_the prep_of_application_theorem det_application_an dobj_using_application xcomp_trained_using dep_model_parsers dep_model_Koo vmod_model_trained amod_model_non-projective det_model_a dobj_used_model nsubj_used_We
D09-1058	D07-1014	o	Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm -LRB- Baker 1979 -RRB- applied to the -LRB- Eisner 1996 -RRB- dependency-parsing data structures -LRB- Paskin 2001 -RRB- for projective dependency structures or the matrix-tree theorem -LRB- Koo et al. 2007 Smith and Smith 2007 McDonald and Satta 2007 -RRB- for nonprojective dependency structures	nn_structures_dependency amod_structures_nonprojective dep_Smith_2007 conj_and_Smith_Satta conj_and_Smith_McDonald conj_and_Smith_2007 conj_and_Smith_Smith dep_Koo_Satta dep_Koo_McDonald dep_Koo_2007 dep_Koo_Smith dep_Koo_Smith appos_Koo_2007 dep_Koo_al. nn_Koo_et prep_for_theorem_structures appos_theorem_Koo nn_theorem_matrix-tree det_theorem_the conj_or_structures_theorem nn_structures_dependency amod_structures_projective dep_Paskin_2001 dep_structures_Paskin nn_structures_data amod_structures_dependency-parsing dep_structures_Eisner det_structures_the dep_Eisner_1996 prep_for_applied_theorem prep_for_applied_structures prep_to_applied_structures dep_Baker_1979 amod_algorithm_inside-outside det_algorithm_the vmod_variant_applied appos_variant_Baker prep_of_variant_algorithm det_variant_a dobj_using_variant amod_counts_expected det_counts_these xcomp_calculate_using dobj_calculate_counts aux_calculate_to xcomp_straightforward_calculate cop_straightforward_is nsubj_straightforward_it mark_straightforward_that ccomp_Note_straightforward
P09-1041	D07-1014	o	Then the method of Smith and Smith -LRB- 2007 -RRB- can be used to compute the probability of every possible edge conditioned on the presence of ki p -LRB- yiprime = kprime | yi = k x -RRB- using K1ki Multiplying this probability by p -LRB- yi = k | x -RRB- yields the desired two edge marginal	amod_edge_marginal num_edge_two dobj_desired_edge vmod_the_desired dobj_yields_the nsubj_yields_p num_x_| nn_x_k dobj_=_x nsubj_=_yi dep_p_= det_probability_this agent_Multiplying_p dobj_Multiplying_probability dobj_using_K1ki appos_k_x dep_=_k amod_yi_= num_yi_| nn_yi_kprime dobj_=_yi nsubj_=_yiprime vmod_p_Multiplying vmod_p_using dep_p_= prep_of_presence_ki det_presence_the prep_on_conditioned_presence vmod_edge_conditioned amod_edge_possible det_edge_every prep_of_probability_edge det_probability_the dobj_compute_probability aux_compute_to parataxis_used_yields xcomp_used_compute auxpass_used_be aux_used_can nsubjpass_used_method advmod_used_Then appos_Smith_2007 conj_and_Smith_Smith prep_of_method_Smith prep_of_method_Smith det_method_the
P09-1041	D07-1014	n	Unfortunately there is no straightforward generalization of the method of Smith and Smith -LRB- 2007 -RRB- to the two edge marginal problem	amod_problem_marginal nn_problem_edge num_problem_two det_problem_the appos_Smith_2007 conj_and_Smith_Smith prep_of_method_Smith prep_of_method_Smith det_method_the prep_to_generalization_problem prep_of_generalization_method amod_generalization_straightforward neg_generalization_no nsubj_is_generalization expl_is_there advmod_is_Unfortunately
P09-1041	D07-1014	o	-LRB- Smith and Smith 2007 -RRB- -RRB-	amod_Smith_2007 conj_and_Smith_Smith dep_''_Smith dep_''_Smith
P09-1041	D07-1014	o	This weak supervision has been encoded using priors and initializations -LRB- Klein and Manning 2004 Smith 2006 -RRB- specialized models -LRB- Klein and Manning 2004 Seginer 2007 Bod 2006 -RRB- and implicit negative evidence -LRB- Smith 2006 -RRB-	amod_Smith_2006 dep_evidence_Smith amod_evidence_negative amod_evidence_implicit dep_Bod_2006 num_Seginer_2007 dep_Klein_Bod conj_and_Klein_Seginer conj_and_Klein_2004 conj_and_Klein_Manning appos_models_Seginer appos_models_2004 appos_models_Manning appos_models_Klein amod_models_specialized amod_Smith_2006 conj_and_Klein_evidence conj_and_Klein_models conj_and_Klein_Smith conj_and_Klein_2004 conj_and_Klein_Manning dep_priors_evidence dep_priors_models dep_priors_Smith dep_priors_2004 dep_priors_Manning dep_priors_Klein conj_and_priors_initializations dobj_using_initializations dobj_using_priors xcomp_encoded_using auxpass_encoded_been aux_encoded_has nsubjpass_encoded_supervision amod_supervision_weak det_supervision_This
P09-1041	D07-1014	o	In this paper we use a non-projective dependency tree CRF -LRB- Smith and Smith 2007 -RRB-	amod_Smith_2007 conj_and_Smith_Smith appos_CRF_Smith appos_CRF_Smith nn_CRF_tree nn_CRF_dependency amod_CRF_non-projective det_CRF_a dobj_use_CRF nsubj_use_we prep_in_use_paper det_paper_this
P09-1041	D07-1014	o	This generates tens of millions features so we prune those features that occur fewer than 10 total times as in -LRB- Smith and Eisner 2007 -RRB-	amod_Smith_2007 conj_and_Smith_Eisner dep_in_Eisner dep_in_Smith pobj_as_in amod_times_total num_times_10 quantmod_10_than mwe_than_fewer tmod_occur_times nsubj_occur_that rcmod_features_occur det_features_those prep_prune_as dobj_prune_features nsubj_prune_we mark_prune_so nn_features_millions prep_of_tens_features advcl_generates_prune dobj_generates_tens nsubj_generates_This
P09-1041	D07-1014	o	Smith and Eisner -LRB- 2007 -RRB- apply entropy regularization to dependency parsing	nn_parsing_dependency nn_regularization_entropy prep_to_apply_parsing dobj_apply_regularization nsubj_apply_Eisner nsubj_apply_Smith appos_Eisner_2007 conj_and_Smith_Eisner
P09-1041	D07-1014	o	-LRB- McDonald and Satta 2007 Smith and Smith 2007 -RRB-	amod_Smith_2007 conj_and_Smith_Smith dep_McDonald_Smith dep_McDonald_Smith conj_and_McDonald_2007 conj_and_McDonald_Satta dep_''_2007 dep_''_Satta dep_''_McDonald
P09-1041	D07-1014	p	Smith and Smith -LRB- 2007 -RRB- describe a more efficient algorithm that can compute all edge expectations in O -LRB- n3 -RRB- time using the inverse of the Kirchoff matrix K1	nn_K1_matrix nn_K1_Kirchoff det_K1_the prep_of_inverse_K1 det_inverse_the dobj_using_inverse nn_time_O appos_O_n3 nn_expectations_edge det_expectations_all xcomp_compute_using prep_in_compute_time dobj_compute_expectations aux_compute_can nsubj_compute_that rcmod_algorithm_compute amod_algorithm_efficient det_algorithm_a advmod_efficient_more dobj_describe_algorithm nsubj_describe_Smith nsubj_describe_Smith appos_Smith_2007 conj_and_Smith_Smith
P09-1064	D07-1014	p	Minimizing risk has been shown to improve performance for MT -LRB- Kumar and Byrne 2004 -RRB- as well as other language processing tasks -LRB- Goodman 1996 Goel and Byrne 2000 Kumar and Byrne 2002 Titov and Henderson 2006 Smith and Smith 2007 -RRB-	appos_Smith_2007 conj_and_Smith_Smith conj_and_Titov_2006 conj_and_Titov_Henderson conj_and_Kumar_Byrne conj_and_Goel_2000 conj_and_Goel_Byrne dep_Goodman_Smith dep_Goodman_Smith dep_Goodman_2006 dep_Goodman_Henderson dep_Goodman_Titov amod_Goodman_2002 dep_Goodman_Byrne dep_Goodman_Kumar dep_Goodman_2000 dep_Goodman_Byrne dep_Goodman_Goel amod_Goodman_1996 dep_tasks_Goodman nn_tasks_processing nn_tasks_language amod_tasks_other dep_Kumar_2004 conj_and_Kumar_Byrne conj_and_MT_tasks appos_MT_Byrne appos_MT_Kumar prep_for_performance_tasks prep_for_performance_MT dobj_improve_performance aux_improve_to xcomp_shown_improve auxpass_shown_been aux_shown_has nsubjpass_shown_risk amod_risk_Minimizing
W07-2216	D07-1014	o	FollowingtheworkofKooetal -LRB- 2007 -RRB- andSmith and Smith -LRB- 2007 -RRB- it is possible to compute all expectations in O -LRB- n3 + | L | n2 -RRB- through matrix inversion	nn_inversion_matrix num_n2_| nn_n2_L num_n2_| conj_+_n3_n2 dep_O_n2 dep_O_n3 prep_in_expectations_O det_expectations_all prep_through_compute_inversion dobj_compute_expectations aux_compute_to xcomp_possible_compute cop_possible_is nsubj_possible_it appos_Smith_2007 conj_and_andSmith_Smith rcmod_FollowingtheworkofKooetal_possible dep_FollowingtheworkofKooetal_Smith dep_FollowingtheworkofKooetal_andSmith dep_FollowingtheworkofKooetal_2007
W07-2216	D07-1014	o	-LRB- 2007 -RRB- and Smith and Smith -LRB- 2007 -RRB- showed that the MatrixTree Theorem can be used to train edge-factored log-linearmodelsofdependencyparsing	amod_log-linearmodelsofdependencyparsing_edge-factored dobj_train_log-linearmodelsofdependencyparsing aux_train_to xcomp_used_train auxpass_used_be aux_used_can nsubjpass_used_Theorem mark_used_that nn_Theorem_MatrixTree det_Theorem_the ccomp_showed_used nsubj_showed_Smith nsubj_showed_Smith nsubj_showed_2007 appos_Smith_2007 conj_and_2007_Smith conj_and_2007_Smith
D07-1070	D07-1015	o	For nonprojective parsing the analogy to the inside algorithm is the O -LRB- n3 -RRB- matrix-tree algorithm which is dominated asymptotically by a matrix determinant -LRB- Smith and Smith 2007 Koo et al. 2007 McDonald and Satta 2007 -RRB-	amod_McDonald_2007 conj_and_McDonald_Satta num_Koo_2007 nn_Koo_al. nn_Koo_et dep_Smith_Satta dep_Smith_McDonald conj_and_Smith_Koo conj_and_Smith_2007 conj_and_Smith_Smith dep_determinant_Koo dep_determinant_2007 dep_determinant_Smith dep_determinant_Smith nn_determinant_matrix det_determinant_a agent_dominated_determinant advmod_dominated_asymptotically auxpass_dominated_is nsubjpass_dominated_which rcmod_algorithm_dominated amod_algorithm_matrix-tree nn_algorithm_O det_algorithm_the cop_algorithm_is nsubj_algorithm_analogy prep_for_algorithm_parsing appos_O_n3 amod_algorithm_inside det_algorithm_the prep_to_analogy_algorithm det_analogy_the amod_parsing_nonprojective
D09-1058	D07-1015	o	It is often straightforward to obtain large amounts of unlabeled data making semi-supervised approaches appealing previous work on semisupervised methods for dependency parsing includes -LRB- Smith and Eisner 2007 Koo et al. 2008 Wang et al. 2008 -RRB-	num_Wang_2008 nn_Wang_al. nn_Wang_et num_Koo_2008 nn_Koo_al. nn_Koo_et dep_Smith_Wang conj_and_Smith_Koo conj_and_Smith_2007 conj_and_Smith_Eisner dep_includes_Koo dep_includes_2007 dep_includes_Eisner dep_includes_Smith nsubj_includes_work nn_parsing_dependency prep_for_methods_parsing amod_methods_semisupervised prep_on_work_methods amod_work_previous nsubj_appealing_approaches amod_approaches_semi-supervised xcomp_making_appealing amod_data_unlabeled prep_of_amounts_data amod_amounts_large dobj_obtain_amounts aux_obtain_to parataxis_straightforward_includes xcomp_straightforward_making xcomp_straightforward_obtain advmod_straightforward_often cop_straightforward_is nsubj_straightforward_It ccomp_``_straightforward
D09-1058	D07-1015	o	We used a non-projective model trained using an application of the matrix-tree theorem -LRB- Koo et al. 2007 Smith and Smith 2007 McDonald and Satta 2007 -RRB- for the first-order Czech models and projective parsers for all other models	amod_models_other det_models_all prep_for_parsers_models amod_parsers_projective nn_models_Czech amod_models_first-order det_models_the amod_Smith_2007 conj_and_Smith_Satta conj_and_Smith_McDonald conj_and_Smith_2007 conj_and_Smith_Smith conj_and_Koo_parsers prep_for_Koo_models dep_Koo_Satta dep_Koo_McDonald dep_Koo_2007 dep_Koo_Smith dep_Koo_Smith appos_Koo_2007 dep_Koo_al. nn_Koo_et nn_theorem_matrix-tree det_theorem_the prep_of_application_theorem det_application_an dobj_using_application xcomp_trained_using dep_model_parsers dep_model_Koo vmod_model_trained amod_model_non-projective det_model_a dobj_used_model nsubj_used_We
D09-1058	D07-1015	o	Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm -LRB- Baker 1979 -RRB- applied to the -LRB- Eisner 1996 -RRB- dependency-parsing data structures -LRB- Paskin 2001 -RRB- for projective dependency structures or the matrix-tree theorem -LRB- Koo et al. 2007 Smith and Smith 2007 McDonald and Satta 2007 -RRB- for nonprojective dependency structures	nn_structures_dependency amod_structures_nonprojective dep_Smith_2007 conj_and_Smith_Satta conj_and_Smith_McDonald conj_and_Smith_2007 conj_and_Smith_Smith dep_Koo_Satta dep_Koo_McDonald dep_Koo_2007 dep_Koo_Smith dep_Koo_Smith appos_Koo_2007 dep_Koo_al. nn_Koo_et prep_for_theorem_structures appos_theorem_Koo nn_theorem_matrix-tree det_theorem_the conj_or_structures_theorem nn_structures_dependency amod_structures_projective dep_Paskin_2001 dep_structures_Paskin nn_structures_data amod_structures_dependency-parsing dep_structures_Eisner det_structures_the dep_Eisner_1996 prep_for_applied_theorem prep_for_applied_structures prep_to_applied_structures dep_Baker_1979 amod_algorithm_inside-outside det_algorithm_the vmod_variant_applied appos_variant_Baker prep_of_variant_algorithm det_variant_a dobj_using_variant amod_counts_expected det_counts_these xcomp_calculate_using dobj_calculate_counts aux_calculate_to xcomp_straightforward_calculate cop_straightforward_is nsubj_straightforward_it mark_straightforward_that ccomp_Note_straightforward
N09-3002	D07-1020	o	For example the topics Sport and Education are important cues for differentiating mentions of Michael Jordan which may refer to a basketball player a computer science professor etc. Second as noted in the top WePS run -LRB- Chen and Martin 2007 -RRB- feature development is important in achieving good coreference performance	nn_performance_coreference amod_performance_good dobj_achieving_performance prepc_in_important_achieving cop_important_is nsubj_important_development advcl_important_noted tmod_important_Second nn_development_feature amod_Chen_2007 conj_and_Chen_Martin dep_run_Martin dep_run_Chen nn_run_WePS amod_run_top det_run_the prep_in_noted_run mark_noted_as advmod_professor_etc. nn_professor_science nn_professor_computer det_professor_a appos_player_professor nn_player_basketball det_player_a prep_to_refer_player aux_refer_may nsubj_refer_which rcmod_Jordan_refer nn_Jordan_Michael prep_of_mentions_Jordan ccomp_differentiating_mentions parataxis_cues_important prepc_for_cues_differentiating amod_cues_important cop_cues_are nsubj_cues_topics prep_for_cues_example conj_and_Sport_Education dep_topics_Education dep_topics_Sport det_topics_the
P09-1047	D07-1020	o	-LRB- Mann and Yarowsky 2003 Chen and Martin 2007 Baron and Freedman 2008 -RRB-	appos_Chen_2008 conj_and_Chen_Freedman conj_and_Chen_Baron conj_and_Chen_2007 conj_and_Chen_Martin dep_Mann_Freedman dep_Mann_Baron dep_Mann_2007 dep_Mann_Martin dep_Mann_Chen conj_and_Mann_2003 conj_and_Mann_Yarowsky dep_''_2003 dep_''_Yarowsky dep_''_Mann
P09-2090	D07-1020	o	We base our work partly on previous work done by Bagga and Baldwin -LRB- Bagga and Baldwin 1998 -RRB- which has also been used in later work -LRB- Chen and Martin 2007 -RRB-	amod_Chen_2007 conj_and_Chen_Martin dep_work_Martin dep_work_Chen amod_work_later prep_in_used_work auxpass_used_been advmod_used_also aux_used_has nsubjpass_used_which dep_Bagga_1998 conj_and_Bagga_Baldwin rcmod_Bagga_used dep_Bagga_Baldwin dep_Bagga_Bagga conj_and_Bagga_Baldwin agent_done_Baldwin agent_done_Bagga amod_work_previous poss_work_our vmod_base_done prep_on_base_work advmod_base_partly dobj_base_work nsubj_base_We
P09-3011	D07-1020	o	Chen and Martin -LRB- 2007 -RRB- explored the use of a range of syntactic and semantic features in unsupervised clustering of documents	prep_of_clustering_documents amod_clustering_unsupervised amod_features_semantic amod_features_syntactic conj_and_syntactic_semantic prep_of_range_features det_range_a prep_in_use_clustering prep_of_use_range det_use_the dobj_explored_use nsubj_explored_Martin nsubj_explored_Chen appos_Martin_2007 conj_and_Chen_Martin
W07-2024	D07-1020	o	For more detail see Chen & Martin -LRB- 2007 -RRB-	appos_Chen_2007 conj_and_Chen_Martin dobj_see_Martin dobj_see_Chen prep_for_see_detail amod_detail_more
W07-2024	D07-1020	o	Chen & Martin -LRB- 2007 -RRB- introduced one of those similarity schemes ? two-level SoftTFIDF ??	nn_??_SoftTFIDF amod_??_two-level nn_schemes_similarity det_schemes_those prep_of_one_schemes dep_introduced_?? dobj_introduced_one nsubj_introduced_Martin nsubj_introduced_Chen appos_Chen_2007 conj_and_Chen_Martin
C08-1008	D07-1031	o	Standard sequence prediction models are highly effective for supertagging including Hidden Markov Models -LRB- Bangalore and Joshi 1999 Nielsen 2002 -RRB- Maximum Entropy Markov Models -LRB- Clark 2002 Hockenmaier et al. 2004 Clark and Curran 2007 -RRB- and Conditional Random Fields -LRB- Blunsom and Baldwin 2006 -RRB-	dep_Blunsom_2006 conj_and_Blunsom_Baldwin dep_Fields_Baldwin dep_Fields_Blunsom amod_Fields_Random amod_Fields_Conditional num_Clark_2007 conj_and_Clark_Curran num_Hockenmaier_2004 nn_Hockenmaier_al. nn_Hockenmaier_et dep_Clark_Curran dep_Clark_Clark conj_Clark_Hockenmaier amod_Clark_2002 appos_Models_Clark nn_Models_Markov nn_Models_Entropy nn_Models_Maximum num_Nielsen_2002 dep_Bangalore_Nielsen conj_and_Bangalore_1999 conj_and_Bangalore_Joshi appos_Models_1999 appos_Models_Joshi appos_Models_Bangalore nn_Models_Markov nn_Models_Hidden prep_including_supertagging_Models conj_and_effective_Fields conj_and_effective_Models prep_for_effective_supertagging advmod_effective_highly cop_effective_are nsubj_effective_models nn_models_prediction nn_models_sequence amod_models_Standard
C08-1008	D07-1031	o	Recentworkconsidersadamagedtagdictionary by assuming that tags are known only for words that occur more than once or twice -LRB- Toutanova and Johnson 2007 -RRB-	num_Toutanova_2007 conj_and_Toutanova_Johnson dep_once_Johnson dep_once_Toutanova conj_or_once_twice pobj_than_twice pobj_than_once prep_more_than advmod_occur_more nsubj_occur_that rcmod_words_occur prep_for_known_words advmod_known_only auxpass_known_are nsubjpass_known_tags mark_known_that ccomp_assuming_known prepc_by_Recentworkconsidersadamagedtagdictionary_assuming dep_``_Recentworkconsidersadamagedtagdictionary
C08-1008	D07-1031	o	Other work aims to do truly unsupervised learning of taggers such as Goldwater and Griffiths -LRB- 2007 -RRB- and Johnson -LRB- 2007 -RRB-	appos_Johnson_2007 appos_Griffiths_2007 conj_and_Goldwater_Johnson conj_and_Goldwater_Griffiths prep_such_as_taggers_Johnson prep_such_as_taggers_Griffiths prep_such_as_taggers_Goldwater prep_of_learning_taggers amod_learning_unsupervised advmod_unsupervised_truly dobj_do_learning aux_do_to xcomp_aims_do nsubj_aims_work amod_work_Other
C08-1008	D07-1031	o	Dirichlet priors can be used to bias HMMs toward more skewed distributions -LRB- Goldwater and Griffiths 2007 Johnson 2007 -RRB- which is especially useful in the weakly supervised setting consideredhere	nn_consideredhere_setting amod_consideredhere_supervised amod_consideredhere_weakly det_consideredhere_the prep_in_useful_consideredhere advmod_useful_especially cop_useful_is nsubj_useful_which dep_Johnson_2007 dep_Goldwater_Johnson amod_Goldwater_2007 conj_and_Goldwater_Griffiths rcmod_distributions_useful appos_distributions_Griffiths appos_distributions_Goldwater amod_distributions_skewed amod_distributions_more nn_HMMs_bias prep_toward_used_distributions prep_to_used_HMMs auxpass_used_be aux_used_can nsubjpass_used_priors amod_priors_Dirichlet
C08-1008	D07-1031	o	FollowingJohnson -LRB- 2007 -RRB- Iusevariational Bayes EM -LRB- Beal 2003 -RRB- during the M-step for the transition distribution l +1 j | i = f -LRB- E -LSB- ni j -RSB- + i -RRB- f -LRB- E -LSB- n i -RSB- + | C | i -RRB- -LRB- 3 -RRB- f -LRB- v -RRB- = exp -LRB- -LRB- v -RRB- -RRB- -LRB- 4 -RRB- 60 -LRB- v -RRB- = braceleftBigg g -LRB- v 1 2 -RRB- ifv > 7 -LRB- v + 1 -RRB- 1v o.w.	dep_o.w._1v dep_1v_1 cc_1v_+ dep_1v_v dep_1v_7 quantmod_7_> dep_ifv_o.w. dep_ifv_2 num_ifv_1 dep_v_ifv dep_g_v nn_g_braceleftBigg dep_=_g dep_=_v dep_=_60 dep_60_4 dep_4_v dep_4_exp parataxis_=_= dep_=_v dep_=_f dep_=_3 num_i_| dep_C_= dep_C_i nn_C_| dep_n_i conj_+_E_C appos_E_n dep_f_C dep_f_E dep_i_f dep_i_+ dep_i_E appos_ni_j dep_E_ni nn_E_f amod_E_= nn_E_i nn_E_| nn_E_j num_E_+1 nn_E_l nn_E_FollowingJohnson nn_distribution_transition det_distribution_the prep_for_M-step_distribution det_M-step_the amod_Beal_2003 dep_EM_Beal nn_EM_Bayes nn_EM_Iusevariational prep_during_FollowingJohnson_M-step appos_FollowingJohnson_EM appos_FollowingJohnson_2007
C08-1042	D07-1031	o	1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text both parts-of-speech -LRB- Johnson 2007 Goldwater and Griffiths 2007 Biemann 2006 Dasgupta and Ng 2007 -RRB- and deeper grammatical structure like constituency and dependency trees -LRB- Klein and Manning 2004 Smith 2006 Bod 2006 Seginer 2007 Van Zaanen 2001 -RRB-	amod_Zaanen_2001 nn_Zaanen_Van dep_Seginer_Zaanen num_Seginer_2007 num_Bod_2006 num_Smith_2006 dep_Klein_Seginer conj_and_Klein_Bod conj_and_Klein_Smith conj_and_Klein_2004 conj_and_Klein_Manning nn_trees_dependency conj_and_constituency_trees amod_structure_grammatical amod_structure_deeper dep_Dasgupta_2007 conj_and_Dasgupta_Ng appos_Biemann_2006 dep_Goldwater_Bod dep_Goldwater_Smith dep_Goldwater_2004 dep_Goldwater_Manning dep_Goldwater_Klein prep_like_Goldwater_trees prep_like_Goldwater_constituency conj_and_Goldwater_structure conj_and_Goldwater_Ng conj_and_Goldwater_Dasgupta conj_and_Goldwater_Biemann conj_and_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Johnson_structure dep_Johnson_Dasgupta dep_Johnson_Biemann dep_Johnson_2007 dep_Johnson_Griffiths dep_Johnson_Goldwater amod_Johnson_2007 dep_parts-of-speech_Johnson preconj_parts-of-speech_both amod_structure_syntactic prep_from_discovery_text prep_of_discovery_structure amod_discovery_unsupervised det_discovery_the prep_in_interest_discovery amod_interest_recent conj_deal_parts-of-speech prep_of_deal_interest amod_deal_great det_deal_a cop_deal_been aux_deal_has expl_deal_There dep_deal_Introduction num_Introduction_1
C08-1042	D07-1031	o	For an HMM with a set of states T and a set of output symbols V t T t Dir -LRB- 1 | T | -RRB- -LRB- 1 -RRB- t T t Dir -LRB- 1 | V | -RRB- -LRB- 2 -RRB- ti | ti1 ti1 Multi -LRB- ti1 -RRB- -LRB- 3 -RRB- wi | ti ti Multi -LRB- ti -RRB- -LRB- 4 -RRB- One advantage of the Bayesian approach is that the prior allows us to bias learning toward sparser structures by setting the Dirichlet hyperparameters to a value less than one -LRB- Johnson 2007 Goldwater and Griffiths 2007 -RRB-	amod_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Johnson_Griffiths dep_Johnson_Goldwater num_Johnson_2007 appos_one_Johnson quantmod_one_than advmod_than_less det_value_a amod_hyperparameters_Dirichlet det_hyperparameters_the dobj_setting_hyperparameters dep_by_one prep_to_by_value pcomp_by_setting amod_structures_sparser prep_toward_learning_structures nn_learning_bias prep_to_allows_learning dobj_allows_us nsubj_allows_prior mark_allows_that det_prior_the dep_is_by ccomp_is_allows nsubj_is_advantage advmod_is_ti dep_is_3 nsubj_is_ti1 dep_is_2 amod_approach_Bayesian det_approach_the prep_of_advantage_approach num_advantage_One dep_advantage_4 nn_advantage_Multi appos_Multi_ti nn_Multi_ti nn_ti_| nn_ti_wi appos_Multi_ti1 nn_Multi_ti1 appos_ti1_Multi num_ti1_| nn_ti1_ti nn_|_V num_|_| num_|_1 dep_Dir_is appos_Dir_| nn_Dir_t nn_Dir_T nn_Dir_t dep_Dir_1 nn_Dir_Dir nn_|_T num_|_| num_|_1 appos_Dir_| nn_Dir_t nn_Dir_T nn_Dir_t dep_V_Dir dep_symbols_V dep_output_symbols prep_of_set_output det_set_a conj_and_T_set nn_T_states prep_of_set_set prep_of_set_T det_set_a prep_with_HMM_set det_HMM_an pobj_For_HMM dep_``_For
C08-1042	D07-1031	o	There is evidence that this leads to better performance on some part-of-speech induction metrics -LRB- Johnson 2007 Goldwater and Griffiths 2007 -RRB-	amod_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Johnson_Griffiths dep_Johnson_Goldwater appos_Johnson_2007 dep_metrics_Johnson nn_metrics_induction amod_metrics_part-of-speech det_metrics_some prep_on_performance_metrics amod_performance_better prep_to_leads_performance nsubj_leads_this mark_leads_that ccomp_evidence_leads nsubj_is_evidence expl_is_There ccomp_``_is
C08-1042	D07-1031	o	Johnson -LRB- 2007 -RRB- evaluates both estimation techniques on the Bayesian bitag model Goldwater and Griffiths -LRB- 2007 -RRB- emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model yielding a tagging supported by many different parameter settings	nn_settings_parameter amod_settings_different amod_settings_many agent_supported_settings vmod_tagging_supported vmod_a_tagging dobj_yielding_a nn_model_tritag det_model_a prep_in_parameters_model nn_parameters_HMM det_parameters_the dobj_integrating_parameters prt_integrating_out prepc_of_approach_integrating nn_approach_MCMC det_approach_the prep_in_advantage_approach det_advantage_the vmod_emphasize_yielding dobj_emphasize_advantage nsubj_emphasize_Griffiths nsubj_emphasize_Goldwater appos_Griffiths_2007 conj_and_Goldwater_Griffiths nn_model_bitag amod_model_Bayesian det_model_the nn_techniques_estimation det_techniques_both parataxis_evaluates_emphasize prep_on_evaluates_model dobj_evaluates_techniques nsubj_evaluates_Johnson appos_Johnson_2007
C08-1042	D07-1031	o	Following the setup in Johnson -LRB- 2007 -RRB- we initialize the transition and emission distributions to be uniform with a small amount of noise and run EM and VB for 1000 iterations	num_iterations_1000 conj_and_EM_VB prep_for_run_iterations dobj_run_VB dobj_run_EM nsubj_run_we prep_of_amount_noise amod_amount_small det_amount_a prep_with_uniform_amount cop_uniform_be aux_uniform_to vmod_transition_uniform dep_transition_distributions conj_and_transition_emission det_transition_the conj_and_initialize_run dobj_initialize_emission dobj_initialize_transition nsubj_initialize_we prep_following_initialize_setup appos_Johnson_2007 prep_in_setup_Johnson det_setup_the
C08-1042	D07-1031	o	In our VB experiments we set i = j = 0.1 i -LCB- 1 | T | -RCB- j -LCB- 1 | V | -RCB- which yielded the best performance on most reported metrics in Johnson -LRB- 2007 -RRB-	appos_Johnson_2007 prep_in_metrics_Johnson amod_metrics_reported amod_metrics_most prep_on_performance_metrics amod_performance_best det_performance_the dobj_yielded_performance nsubj_yielded_which num_V_| appos_|_V rcmod_1_yielded dep_1_| dep_j_1 nn_|_T num_|_| dep_1_| nn_1_i dep_=_0.1 appos_j_j appos_j_1 amod_j_= dep_=_j dep_i_= dobj_set_i nsubj_set_we prep_in_set_experiments nn_experiments_VB poss_experiments_our
C08-1042	D07-1031	o	We use maximum marginal decoding which Johnson -LRB- 2007 -RRB- reports performs better than Viterbi decoding	nn_decoding_Viterbi prep_than_better_decoding advmod_performs_better nsubj_performs_reports nn_reports_Johnson det_reports_which appos_Johnson_2007 rcmod_decoding_performs amod_decoding_marginal amod_decoding_maximum dobj_use_decoding nsubj_use_We
C08-1042	D07-1031	o	One option is what Johnson -LRB- 2007 -RRB- calls many-to-one -LRB- M-to-1 -RRB- accuracy in which each induced tag is labeled with its most frequent gold tag	nn_tag_gold amod_tag_frequent poss_tag_its advmod_frequent_most prep_with_labeled_tag auxpass_labeled_is nsubjpass_labeled_tag prep_in_labeled_which amod_tag_induced det_tag_each rcmod_accuracy_labeled nn_accuracy_many-to-one appos_many-to-one_M-to-1 dep_calls_accuracy nsubj_calls_Johnson dobj_calls_what appos_Johnson_2007 ccomp_is_calls nsubj_is_option num_option_One
C08-1042	D07-1031	o	In cases where the number of gold tags is different than the number of induced tags some must necessarily remain unassigned -LRB- Johnson 2007 -RRB-	amod_Johnson_2007 dep_remain_Johnson acomp_remain_unassigned advmod_remain_necessarily aux_remain_must nsubj_remain_some prep_in_remain_cases amod_tags_induced prep_of_number_tags det_number_the prep_than_different_number cop_different_is nsubj_different_number advmod_different_where nn_tags_gold prep_of_number_tags det_number_the rcmod_cases_different
D08-1036	D07-1031	o	Finally following Haghighi and Klein -LRB- 2006 -RRB- and Johnson -LRB- 2007 -RRB- we can instead insist that at most one HMM state can be mapped to any part-of-speech tag	amod_tag_part-of-speech det_tag_any prep_to_mapped_tag auxpass_mapped_be aux_mapped_can nsubjpass_mapped_that nn_state_HMM num_state_one advmod_state_most prep_at_that_state ccomp_insist_mapped advmod_insist_instead aux_insist_can nsubj_insist_we prep_following_insist_Johnson prep_following_insist_Klein prep_following_insist_Haghighi advmod_insist_Finally appos_Johnson_2007 appos_Klein_2006 conj_and_Haghighi_Johnson conj_and_Haghighi_Klein
D08-1036	D07-1031	o	The studies presented by Goldwater and Griffiths -LRB- 2007 -RRB- and Johnson -LRB- 2007 -RRB- differed in the number of states that they used	nsubj_used_they mark_used_that prep_of_number_states det_number_the ccomp_differed_used prep_in_differed_number nsubj_differed_studies appos_Johnson_2007 appos_Griffiths_2007 conj_and_Goldwater_Johnson conj_and_Goldwater_Griffiths agent_presented_Johnson agent_presented_Griffiths agent_presented_Goldwater vmod_studies_presented det_studies_The
D08-1036	D07-1031	o	Goldwater and Griffiths -LRB- 2007 -RRB- evaluated against the reduced tag set of 17 tags developed by Smith and Eisner -LRB- 2005 -RRB- while Johnson -LRB- 2007 -RRB- evaluated against the full Penn Treebank tag set	nn_set_tag nn_set_Treebank nn_set_Penn amod_set_full det_set_the prep_against_evaluated_set nsubj_evaluated_Johnson mark_evaluated_while appos_Johnson_2007 appos_Eisner_2005 conj_and_Smith_Eisner agent_developed_Eisner agent_developed_Smith vmod_tags_developed num_tags_17 prep_of_set_tags nn_set_tag amod_set_reduced det_set_the prep_against_evaluated_set appos_Griffiths_2007 advcl_Goldwater_evaluated vmod_Goldwater_evaluated conj_and_Goldwater_Griffiths
D08-1036	D07-1031	o	The largest corpus that Goldwater and Griffiths -LRB- 2007 -RRB- studied contained 96,000 words while Johnson -LRB- 2007 -RRB- used all of the 1,173,766 words in the full Penn WSJ treebank	nn_treebank_WSJ nn_treebank_Penn amod_treebank_full det_treebank_the prep_in_words_treebank num_words_1,173,766 det_words_the prep_of_all_words dobj_used_all nsubj_used_Johnson mark_used_while appos_Johnson_2007 num_words_96,000 dobj_contained_words nsubj_contained_Griffiths nsubj_contained_Goldwater dobj_contained_that appos_Griffiths_2007 vmod_Goldwater_studied conj_and_Goldwater_Griffiths advcl_corpus_used rcmod_corpus_contained amod_corpus_largest det_corpus_The ccomp_``_corpus
D08-1036	D07-1031	o	We ran each estimator with the eight different combinations of values for the hyperparameters and prime listed below which include the optimal values for the hyperparameters found by Johnson -LRB- 2007 -RRB- and report results for the best combination for each estimator below 1	prep_below_estimator_1 det_estimator_each prep_for_combination_estimator amod_combination_best det_combination_the nn_results_report conj_and_Johnson_results appos_Johnson_2007 prep_for_found_combination agent_found_results agent_found_Johnson vmod_hyperparameters_found det_hyperparameters_the prep_for_values_hyperparameters amod_values_optimal det_values_the dobj_include_values nsubj_include_which advmod_listed_below dep_prime_listed rcmod_hyperparameters_include conj_and_hyperparameters_prime det_hyperparameters_the prep_for_values_prime prep_for_values_hyperparameters prep_of_combinations_values amod_combinations_different num_combinations_eight det_combinations_the det_estimator_each prep_with_ran_combinations dobj_ran_estimator nsubj_ran_We
D08-1036	D07-1031	o	prime 1 1 1 0.5 0.5 1 0.5 0.5 0.1 0.1 0.1 0.0001 0.0001 0.1 0.0001 0.0001 Further we ran each setting of each estimator at least 10 times -LRB- from randomly jittered initial starting points -RRB- for at least 1,000 iterations as Johnson -LRB- 2007 -RRB- showed that some estimators require many iterations to converge	aux_converge_to amod_iterations_many xcomp_require_converge dobj_require_iterations nsubj_require_estimators mark_require_that det_estimators_some ccomp_showed_require nsubj_showed_Johnson mark_showed_as dep_showed_from dep_showed_times csubj_showed_ran appos_Johnson_2007 num_iterations_1,000 quantmod_1,000_at mwe_at_least amod_points_starting amod_points_initial amod_points_jittered advmod_jittered_randomly prep_for_from_iterations pobj_from_points num_times_10 quantmod_10_at mwe_at_least det_estimator_each prep_of_setting_estimator det_setting_each dobj_ran_setting nsubj_ran_we advmod_ran_prime amod_0.0001_Further number_0.0001_0.0001 dep_0.0001_0.1 dep_0.0001_0.1 dep_0.0001_0.5 dep_0.1_0.0001 number_0.0001_0.0001 number_0.1_0.1 dep_0.1_0.1 number_0.1_0.5 number_0.5_1 dep_0.5_0.5 dep_0.5_1 dep_0.5_0.5 number_0.5_1 number_1_1 dep_prime_0.0001
D08-1036	D07-1031	o	Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy confirming the results reported by Johnson -LRB- 2007 -RRB-	appos_Johnson_2007 agent_reported_Johnson vmod_results_reported det_results_the dobj_confirming_results ccomp_,_confirming amod_accuracy_cross-validation prep_of_terms_accuracy pobj_at_least amod_estimators_Bayesian det_estimators_the prep_with_competitive_estimators cop_competitive_is nn_sets_data amod_sets_larger prep_on_well_sets advmod_well_surprisingly prep_in_does_terms advmod_does_at conj_and_does_competitive advmod_does_well dep_Maximization_competitive dep_Maximization_does nn_Maximization_Expectation
D08-1036	D07-1031	o	Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers -LRB- Goldwater and Griffiths 2007 Johnson 2007 -RRB-	dep_Johnson_2007 dep_Goldwater_Johnson conj_and_Goldwater_2007 conj_and_Goldwater_Griffiths dep_taggers_2007 dep_taggers_Griffiths dep_taggers_Goldwater nn_taggers_POS nn_taggers_HMM amod_taggers_unsupervised prep_of_inference_taggers amod_inference_Bayesian prep_to_applied_inference auxpass_applied_been aux_applied_have nsubjpass_applied_that rcmod_methods_applied nn_methods_inference amod_methods_approximate prep_of_kinds_methods num_kinds_two cop_kinds_are nsubj_kinds_Bayes nsubj_kinds_methods nn_Bayes_Variational conj_and_methods_Bayes nn_methods_sampling nn_methods_Carlo nn_methods_Monte
D08-1036	D07-1031	o	Johnson -LRB- 2007 -RRB- compared two Bayesian inference algorithms Variational Bayes and what we call here a point-wise collapsed Gibbs sampler and found that Variational Bayes produced the best solution and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM	prep_than_solution_EM amod_solution_worse det_solution_a dobj_produced_solution nsubj_produced_sampler aux_converge_to conj_and_slow_produced xcomp_slow_converge advmod_slow_extremely cop_slow_was nsubj_slow_sampler mark_slow_that nn_sampler_Gibbs det_sampler_the amod_solution_best det_solution_the conj_and_produced_produced conj_and_produced_slow dobj_produced_solution nsubj_produced_Bayes mark_produced_that nn_Bayes_Variational ccomp_found_slow ccomp_found_produced nsubj_found_Johnson nn_sampler_Gibbs dobj_collapsed_sampler nsubj_collapsed_point-wise det_point-wise_a ccomp_call_collapsed advmod_call_here nsubj_call_we dobj_call_what nn_Bayes_Variational conj_and_algorithms_call conj_and_algorithms_Bayes nn_algorithms_inference amod_algorithms_Bayesian num_algorithms_two conj_and_compared_found dobj_compared_call dobj_compared_Bayes dobj_compared_algorithms nsubj_compared_Johnson appos_Johnson_2007
D08-1036	D07-1031	o	The samplers that Goldwater and Griffiths -LRB- 2007 -RRB- and Johnson -LRB- 2007 -RRB- describe are pointwise collapsed Gibbs samplers	nn_samplers_Gibbs amod_samplers_collapsed dep_pointwise_samplers cop_pointwise_are nsubj_pointwise_samplers nsubj_describe_Johnson nsubj_describe_Griffiths nsubj_describe_Goldwater dobj_describe_that appos_Johnson_2007 appos_Griffiths_2007 conj_and_Goldwater_Johnson conj_and_Goldwater_Griffiths rcmod_samplers_describe det_samplers_The
D08-1109	D07-1031	o	Recent advances in these approaches include the use of a fully Bayesian HMM -LRB- Johnson 2007 Goldwater and Griffiths 2007 -RRB-	amod_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Johnson_Griffiths dep_Johnson_Goldwater amod_Johnson_2007 appos_HMM_Johnson amod_HMM_Bayesian det_HMM_a advmod_Bayesian_fully prep_of_use_HMM det_use_the dobj_include_use nsubj_include_advances det_approaches_these prep_in_advances_approaches amod_advances_Recent
D09-1071	D07-1031	o	Recent work -LRB- Johnson 2007 Goldwater and Griffiths 2007 Gao and Johnson 2008 -RRB- explored the task of part-of-speech tagging -LRB- PoS -RRB- using unsupervised Hidden Markov Models -LRB- HMMs -RRB- with encouraging results	amod_results_encouraging appos_Models_HMMs nn_Models_Markov nn_Models_Hidden amod_Models_unsupervised prep_with_using_results dobj_using_Models appos_tagging_PoS amod_tagging_part-of-speech vmod_task_using prep_of_task_tagging det_task_the dobj_explored_task nsubj_explored_work dep_Goldwater_2008 conj_and_Goldwater_Johnson conj_and_Goldwater_Gao conj_and_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Johnson_Johnson dep_Johnson_Gao dep_Johnson_2007 dep_Johnson_Griffiths dep_Johnson_Goldwater amod_Johnson_2007 appos_work_Johnson amod_work_Recent
D09-1071	D07-1031	o	Recent work -LRB- Goldwater and Griffiths 2007 Johnson 2007 Gao and Johnson 2008 -RRB- on this task explored a variety of methodologies to address this issue	det_issue_this dobj_address_issue aux_address_to prep_of_variety_methodologies det_variety_a vmod_explored_address dobj_explored_variety nsubj_explored_work det_task_this dep_Gao_2008 conj_and_Gao_Johnson num_Johnson_2007 dep_Goldwater_Johnson dep_Goldwater_Gao dep_Goldwater_Johnson amod_Goldwater_2007 conj_and_Goldwater_Griffiths prep_on_work_task dep_work_Griffiths dep_work_Goldwater amod_work_Recent
D09-1071	D07-1031	o	Johnson -LRB- 2007 -RRB- and Gao & Johnson -LRB- 2008 -RRB- assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags	nn_tags_POS prep_with_correlate_tags advmod_correlate_strongly nsubj_correlate_states mark_correlate_that amod_states_resulting det_states_the ccomp_find_correlate nsubj_find_words nn_model_Markov amod_model_hidden det_model_a conj_and_generated_find agent_generated_model auxpass_generated_are nsubjpass_generated_words mark_generated_that ccomp_assume_find ccomp_assume_generated nsubj_assume_Gao nsubj_assume_Johnson appos_Gao_2008 conj_and_Gao_Johnson conj_and_Johnson_Johnson conj_and_Johnson_Gao appos_Johnson_2007
D09-1071	D07-1031	o	The fact that different authors use different versions of the same gold standard to evaluate similar experiments -LRB- e.g. Goldwater & Griffiths -LRB- 2007 -RRB- versus Johnson -LRB- 2007 -RRB- -RRB- supports this claim	det_claim_this dobj_supports_claim prep_supports_e.g. appos_Johnson_2007 conj_versus_Goldwater_Johnson dep_Goldwater_2007 conj_and_Goldwater_Griffiths pobj_e.g._Johnson pobj_e.g._Griffiths pobj_e.g._Goldwater dep_experiments_supports amod_experiments_similar dobj_evaluate_experiments aux_evaluate_to vmod_standard_evaluate amod_standard_gold amod_standard_same det_standard_the prep_of_versions_standard amod_versions_different dobj_use_versions nsubj_use_authors mark_use_that amod_authors_different ccomp_fact_use det_fact_The
D09-1071	D07-1031	o	Johnson -LRB- 2007 -RRB- reports results for different numbers of hidden states but it is unclear how to make this choice a priori while Goldwater & Griffiths -LRB- 2007 -RRB- leave this question as future work	amod_work_future det_question_this prep_as_leave_work dobj_leave_question nsubj_leave_Griffiths nsubj_leave_Goldwater mark_leave_while appos_Goldwater_2007 conj_and_Goldwater_Griffiths det_priori_a nsubj_priori_choice det_choice_this xcomp_make_priori aux_make_to advmod_make_how ccomp_unclear_make cop_unclear_is nsubj_unclear_it amod_states_hidden prep_of_numbers_states amod_numbers_different advcl_results_leave conj_but_results_unclear prep_for_results_numbers nsubj_results_reports nn_reports_Johnson appos_Johnson_2007
D09-1071	D07-1031	p	Given the parameters -LCB- pi0 pi K -RCB- of the HMM the joint distribution over hidden states s and observationsy can be written -LRB- with s0 = 0 -RRB- p -LRB- s y | pi0 pi K -RRB- = Tproductdisplay t = 1 p -LRB- st | st1 -RRB- p -LRB- yt | st -RRB- As Johnson -LRB- 2007 -RRB- clearly explained training the HMM with EM leads to poor results in PoS tagging	nn_tagging_PoS prep_in_results_tagging amod_results_poor prep_to_leads_results nsubj_leads_p tmod_leads_p dep_leads_= nsubj_leads_t det_HMM_the prep_with_training_EM dobj_training_HMM xcomp_explained_training advmod_explained_clearly nsubj_explained_Johnson mark_explained_As appos_Johnson_2007 num_st_| nn_st_yt advcl_p_explained appos_p_st nn_p_st num_st1_| appos_st_st1 num_p_1 nn_t_Tproductdisplay ccomp_=_leads npadvmod_=_K num_pi0_| nn_pi0_y dep_s_= conj_s_pi conj_s_pi0 nn_s_p dobj_=_0 nsubj_=_s0 dep_with_= dep_written_s dep_written_with auxpass_written_be aux_written_can nsubjpass_written_distribution prep_written_Given conj_and_s_observationsy nn_s_states amod_s_hidden prep_over_distribution_observationsy prep_over_distribution_s amod_distribution_joint det_distribution_the det_HMM_the appos_pi0_K appos_pi0_pi prep_of_parameters_HMM dep_parameters_pi0 det_parameters_the pobj_Given_parameters
D09-1075	D07-1031	o	4.1 Variational Bayes Beal -LRB- 2003 -RRB- and Johnson -LRB- 2007 -RRB- describe variational Bayes for hidden Markov model in detail which can be directly applied to our bilingual model	amod_model_bilingual poss_model_our prep_to_applied_model advmod_applied_directly auxpass_applied_be aux_applied_can nsubjpass_applied_which nn_model_Markov amod_model_hidden prep_for_Bayes_model amod_Bayes_variational ccomp_describe_applied prep_in_describe_detail dobj_describe_Bayes nsubj_describe_Johnson nsubj_describe_Beal appos_Johnson_2007 conj_and_Beal_Johnson dep_Beal_2003 nn_Beal_Bayes nn_Beal_Variational num_Beal_4.1
D09-1075	D07-1031	o	Johnson -LRB- 2007 -RRB- and Zhang et al.	nn_al._et nn_al._Zhang conj_and_Johnson_al. appos_Johnson_2007
E09-1042	D07-1031	o	Importantly this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories -LRB- Johnson 2007 -RRB-	amod_Johnson_2007 dep_categories_Johnson amod_categories_lexical prep_to_tokens_categories prep_of_distribution_tokens amod_distribution_practical det_distribution_a advmod_practical_more prep_in_result_distribution nsubj_result_that rcmod_priors_result amod_priors_sparse prep_of_incorporation_priors det_incorporation_the dobj_facilitates_incorporation nsubj_facilitates_approach advmod_facilitates_Importantly amod_approach_Bayesian det_approach_this
E09-1042	D07-1031	o	Similar to Goldwater and Griffiths -LRB- 2007 -RRB- and Johnson -LRB- 2007 -RRB- Toutanova and Johnson -LRB- 2007 -RRB- also use Bayesian inference for POS tagging	nn_tagging_POS amod_inference_Bayesian prep_for_use_tagging dobj_use_inference advmod_use_also nsubj_use_Similar appos_Johnson_2007 appos_Johnson_2007 appos_Griffiths_2007 conj_and_Goldwater_Johnson conj_and_Goldwater_Toutanova conj_and_Goldwater_Johnson conj_and_Goldwater_Griffiths prep_to_Similar_Johnson prep_to_Similar_Toutanova prep_to_Similar_Johnson prep_to_Similar_Griffiths prep_to_Similar_Goldwater
E09-1042	D07-1031	o	Nevertheless EM sometimes fails to find good parameter values .2 The reason is that EM tries to assign roughly the same number of word tokens to each of the hidden states -LRB- Johnson 2007 -RRB-	amod_Johnson_2007 dep_states_Johnson amod_states_hidden det_states_the prep_of_each_states nn_tokens_word prep_of_number_tokens amod_number_same det_number_the advmod_number_roughly prep_to_assign_each dobj_assign_number aux_assign_to xcomp_tries_assign nsubj_tries_EM mark_tries_that ccomp_is_tries nsubj_is_reason det_reason_The nn_reason_.2 rcmod_values_is nn_values_parameter amod_values_good dobj_find_values aux_find_to xcomp_fails_find advmod_fails_sometimes nsubj_fails_EM advmod_fails_Nevertheless
N09-1009	D07-1031	o	There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings ranging from putting priors over grammar probabilities -LRB- Johnson et al. 2007 -RRB- to putting non-parametric priors over derivations -LRB- Johnson et al. 2006 -RRB- to learning the set of states in a grammar -LRB- Finkel et al. 2007 Liang et al. 2007 -RRB-	num_Liang_2007 nn_Liang_al. nn_Liang_et dep_Finkel_Liang appos_Finkel_2007 dep_Finkel_al. nn_Finkel_et det_grammar_a prep_of_set_states det_set_the prep_in_learning_grammar dobj_learning_set amod_Johnson_2006 dep_Johnson_al. nn_Johnson_et amod_priors_non-parametric prep_over_putting_derivations dobj_putting_priors dep_Johnson_Finkel prepc_to_Johnson_learning dep_Johnson_Johnson prepc_to_Johnson_putting amod_Johnson_2007 dep_Johnson_al. nn_Johnson_et nn_probabilities_grammar prep_over_putting_probabilities dobj_putting_priors prepc_from_ranging_putting amod_settings_different prep_in_grammars_settings amod_grammars_probabilistic prep_for_modeling_grammars amod_modeling_Bayesian dobj_employing_modeling dep_interest_Johnson vmod_interest_ranging prepc_in_interest_employing advmod_interest_recently amod_interest_increased det_interest_an cop_interest_been aux_interest_has expl_interest_There
N09-1009	D07-1031	o	Mostcommonlyvariational -LRB- Johnson 2007 Kurihara and Sato 2006 -RRB- or sampling techniques are applied -LRB- Johnson et al. 2006 -RRB-	amod_Johnson_2006 dep_Johnson_al. nn_Johnson_et dep_applied_Johnson auxpass_applied_are nsubjpass_applied_techniques amod_techniques_sampling amod_techniques_Mostcommonlyvariational dep_Kurihara_2006 conj_and_Kurihara_Sato dep_Johnson_Sato dep_Johnson_Kurihara amod_Johnson_2007 conj_or_Mostcommonlyvariational_sampling dep_Mostcommonlyvariational_Johnson
N09-1009	D07-1031	o	They are most commonly used for parsing and linguistic analysis -LRB- Charniak and Johnson 2005 Collins 2003 -RRB- but are now commonly seen in applications like machine translation -LRB- Wu 1997 -RRB- and question answering -LRB- Wang et al. 2007 -RRB-	amod_Wang_2007 dep_Wang_al. nn_Wang_et dep_answering_Wang nn_answering_question num_Wu_1997 conj_and_translation_answering appos_translation_Wu nn_translation_machine prep_like_applications_answering prep_like_applications_translation prep_in_seen_applications advmod_seen_commonly advmod_seen_now auxpass_seen_are nsubjpass_seen_They dep_Collins_2003 dep_Charniak_Collins conj_and_Charniak_2005 conj_and_Charniak_Johnson appos_analysis_2005 appos_analysis_Johnson appos_analysis_Charniak nn_analysis_linguistic nn_analysis_parsing conj_and_parsing_linguistic conj_but_used_seen prep_for_used_analysis advmod_used_commonly auxpass_used_are nsubjpass_used_They advmod_commonly_most
N09-1009	D07-1031	o	For example if we make a mean-field assumption with respect to hidden structure and weights the variationalalgorithmforapproximatelyinferringthe distribution over and trees y resembles the traditional EM algorithm very closely -LRB- Johnson 2007 -RRB-	amod_Johnson_2007 advmod_closely_very dep_algorithm_Johnson advmod_algorithm_closely nn_algorithm_EM amod_algorithm_traditional det_algorithm_the dobj_resembles_algorithm dep_y_resembles nsubj_y_distribution prep_with_respect_to_y_weights prep_with_respect_to_y_structure advcl_y_make prep_for_y_example conj_and_over_trees prep_distribution_trees prep_distribution_over nn_distribution_variationalalgorithmforapproximatelyinferringthe det_distribution_the conj_and_structure_weights amod_structure_hidden amod_assumption_mean-field det_assumption_a dobj_make_assumption nsubj_make_we mark_make_if
N09-1069	D07-1031	o	For instance on unsupervised part-ofspeech tagging EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal -LRB- Johnson 2007 -RRB-	dep_Johnson_2007 appos_Journal_Johnson nn_Journal_Wall-Street det_Journal_the prep_on_performance_Journal amod_performance_peak poss_performance_its dobj_reach_performance aux_reach_to vmod_iterations_reach num_iterations_100 prep_over_requires_iterations nsubj_requires_EM prep_on_requires_tagging prep_for_requires_instance amod_tagging_part-ofspeech amod_tagging_unsupervised
P08-1012	D07-1031	n	Unlike Johnson -LRB- 2007 -RRB- who found optimal performance when was approximately 104 we observed monotonic increases in performance as dropped	mark_dropped_as prep_in_increases_performance amod_increases_monotonic advcl_observed_dropped dobj_observed_increases nsubj_observed_we prep_unlike_observed_Johnson advmod_104_approximately cop_104_was advmod_104_when amod_performance_optimal advcl_found_104 dobj_found_performance nsubj_found_who rcmod_Johnson_found appos_Johnson_2007
P08-1012	D07-1031	o	3 Variational Bayes for ITG Goldwater and Griffiths -LRB- 2007 -RRB- and Johnson -LRB- 2007 -RRB- show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging	amod_tagging_part-of-speech amod_tagging_unsupervised prep_for_accuracy_tagging amod_accuracy_improved prep_to_leads_accuracy csubj_leads_modifying mark_leads_that amod_estimation_Bayesian dobj_using_estimation poss_parameters_its prep_over_sparse_parameters advmod_sparse_prior det_sparse_a conj_and_include_using dobj_include_sparse aux_include_to det_HMM_an xcomp_modifying_using xcomp_modifying_include dobj_modifying_HMM ccomp_show_leads nsubj_show_Bayes appos_Johnson_2007 appos_Griffiths_2007 conj_and_Goldwater_Johnson conj_and_Goldwater_Griffiths nn_Goldwater_ITG prep_for_Bayes_Johnson prep_for_Bayes_Griffiths prep_for_Bayes_Goldwater nn_Bayes_Variational num_Bayes_3
P08-1012	D07-1031	o	However in experiments in unsupervised POS tag learning using HMM structured models Johnson -LRB- 2007 -RRB- shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipfs law which is prominent in natural languages	amod_languages_natural prep_in_prominent_languages cop_prominent_is nsubj_prominent_which rcmod_law_prominent nn_law_Zipfs det_law_the prep_with_agree_law nsubj_agree_that rcmod_distributions_agree dobj_approaching_distributions prepc_in_sampling_approaching nn_sampling_Gibbs prep_than_effective_sampling advmod_effective_more cop_effective_is nsubj_effective_VB mark_effective_that ccomp_shows_effective nsubj_shows_learning appos_Johnson_2007 appos_models_Johnson amod_models_structured nn_models_HMM dobj_using_models vmod_learning_using rcmod_tag_shows nn_tag_POS amod_tag_unsupervised prep_in_experiments_tag pobj_in_experiments ccomp_,_in dep_``_However
P08-1012	D07-1031	o	As pointed out by Johnson -LRB- 2007 -RRB- in effect this expression adds to c a small value that asymptotically approaches 0.5 as c approaches and 0 as c approaches 0	num_approaches_0 nn_approaches_c prep_as_0_approaches nn_approaches_c prep_as_approaches_approaches dobj_approaches_0.5 advmod_approaches_asymptotically nsubj_approaches_that rcmod_value_approaches amod_value_small det_value_a nn_value_c conj_and_adds_0 prep_to_adds_value nsubj_adds_expression prep_in_adds_effect advcl_adds_pointed det_expression_this appos_Johnson_2007 prep_by_pointed_Johnson prt_pointed_out mark_pointed_As
P08-1100	D07-1031	o	Bayesian approaches can also improve performance -LRB- Goldwater and Griffiths 2007 Johnson 2007 Kurihara and Sato 2006 -RRB-	appos_Kurihara_2006 conj_and_Kurihara_Sato num_Johnson_2007 dep_Goldwater_Sato dep_Goldwater_Kurihara conj_and_Goldwater_Johnson conj_and_Goldwater_2007 conj_and_Goldwater_Griffiths dep_performance_Johnson dep_performance_2007 dep_performance_Griffiths dep_performance_Goldwater dobj_improve_performance advmod_improve_also aux_improve_can nsubj_improve_approaches amod_approaches_Bayesian
P09-1056	D07-1031	p	Recent projects in semisupervised -LRB- Toutanova and Johnson 2007 -RRB- and unsupervised -LRB- Biemann et al. 2007 Smith and Eisner 2005 -RRB- tagging also show significant progress	amod_progress_significant dobj_show_progress advmod_show_also nsubj_show_tagging num_tagging_2005 dep_tagging_Eisner dep_tagging_Smith conj_and_Smith_Eisner parataxis_al._show conj_al._2007 nn_al._et dep_Biemann_al. conj_and_Toutanova_2007 conj_and_Toutanova_Johnson dep_semisupervised_Biemann conj_and_semisupervised_unsupervised dep_semisupervised_2007 dep_semisupervised_Johnson dep_semisupervised_Toutanova prep_in_projects_unsupervised prep_in_projects_semisupervised amod_projects_Recent dep_``_projects
P09-1056	D07-1031	o	HMMs have been used many times for POS tagging and chunking in supervised semisupervised and in unsupervised settings -LRB- Banko and Moore 2004 Goldwater and Griffiths 2007 Johnson 2007 Zhou 2004 -RRB-	dep_Zhou_2004 appos_Johnson_2007 dep_Goldwater_Zhou conj_and_Goldwater_Johnson conj_and_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Banko_Johnson dep_Banko_2007 dep_Banko_Griffiths dep_Banko_Goldwater conj_and_Banko_2004 conj_and_Banko_Moore dep_settings_2004 dep_settings_Moore dep_settings_Banko amod_settings_unsupervised pobj_in_settings conj_and_supervised_in conj_and_supervised_semisupervised conj_and_tagging_chunking nn_tagging_POS amod_times_many prepc_in_used_in prepc_in_used_semisupervised prepc_in_used_supervised prep_for_used_chunking prep_for_used_tagging tmod_used_times auxpass_used_been aux_used_have nsubjpass_used_HMMs
P09-1057	D07-1031	o	6 Smaller Tagset and Incomplete Dictionaries Previously researchers working on this task have also reported results for unsupervised tagging with a smaller tagset -LRB- Smith and Eisner 2005 Goldwater and Griffiths 2007 Toutanova and Johnson 2008 Goldberg et al. 2008 -RRB-	num_Goldberg_2008 nn_Goldberg_al. nn_Goldberg_et dep_Goldwater_Goldberg conj_and_Goldwater_2008 conj_and_Goldwater_Johnson conj_and_Goldwater_Toutanova conj_and_Goldwater_2007 conj_and_Goldwater_Griffiths dep_Smith_2008 dep_Smith_Johnson dep_Smith_Toutanova dep_Smith_2007 dep_Smith_Griffiths dep_Smith_Goldwater conj_and_Smith_2005 conj_and_Smith_Eisner dep_tagset_2005 dep_tagset_Eisner dep_tagset_Smith amod_tagset_smaller det_tagset_a prep_with_tagging_tagset amod_unsupervised_tagging prep_for_results_unsupervised amod_results_reported advmod_reported_also aux_reported_have nsubj_reported_researchers ccomp_reported_Dictionaries ccomp_reported_Tagset dep_reported_6 det_task_this prep_on_working_task vmod_researchers_working nn_Dictionaries_Incomplete advmod_Tagset_Previously conj_and_Tagset_Dictionaries amod_Tagset_Smaller
P09-1057	D07-1031	o	The overall POS tag distribution learnt by EM is relatively uniform as noted by Johnson -LRB- 2007 -RRB- and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed	advmod_skewed_highly cop_skewed_is nsubj_skewed_distribution mark_skewed_whereas nn_distribution_tag amod_distribution_real det_distribution_the nn_label_tag det_label_each prep_of_number_tokens amod_number_equal advcl_assign_skewed prep_to_assign_label dobj_assign_number aux_assign_to xcomp_tends_assign nsubj_tends_it appos_Johnson_2007 conj_and_noted_tends prep_by_noted_Johnson mark_noted_as advcl_uniform_tends advcl_uniform_noted advmod_uniform_relatively cop_uniform_is nsubj_uniform_distribution agent_learnt_EM vmod_distribution_learnt nn_distribution_tag nn_distribution_POS amod_distribution_overall det_distribution_The
E09-1048	D07-1047	p	This is also the main reason why most summarization systems applied to news articles do not outperform a simple baseline that just uses the first 100 words of an article -LRB- Svore et al. 2007 Nenkova 2005 -RRB-	amod_Nenkova_2005 dep_Svore_Nenkova appos_Svore_2007 dep_Svore_al. nn_Svore_et appos_article_Svore det_article_an prep_of_words_article num_words_100 amod_words_first det_words_the dobj_uses_words advmod_uses_just nsubj_uses_that rcmod_baseline_uses amod_baseline_simple det_baseline_a dobj_outperform_baseline neg_outperform_not aux_outperform_do nn_articles_news prep_to_applied_articles nsubj_applied_systems advmod_applied_why amod_systems_summarization advmod_systems_most ccomp_reason_outperform rcmod_reason_applied amod_reason_main det_reason_the advmod_reason_also cop_reason_is nsubj_reason_This
E09-1048	D07-1047	n	5Since the test data of -LRB- Svore et al. 2007 -RRB- is not publicly available we were unable to carry out a more detailed comparison	amod_comparison_detailed det_comparison_a advmod_detailed_more dobj_carry_comparison prt_carry_out aux_carry_to xcomp_unable_carry cop_unable_were nsubj_unable_we ccomp_available_unable advmod_available_publicly neg_available_not cop_available_is nsubj_available_data amod_Svore_2007 dep_Svore_al. nn_Svore_et prep_of_data_Svore nn_data_test det_data_the nn_data_5Since
E09-1048	D07-1047	n	Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of -LRB- Svore et al. 2007 -RRB- without requiring their third-party data resources	nn_resources_data amod_resources_third-party poss_resources_their dobj_requiring_resources prepc_without_Svore_requiring amod_Svore_2007 dep_Svore_al. nn_Svore_et prep_of_approach_Svore det_approach_the amod_performance_similar prep_to_achieved_approach dobj_achieved_performance advmod_achieved_also nsubj_achieved_approach amod_baseline_difficult det_baseline_a advmod_difficult_notoriously conj_but_outperformed_achieved dobj_outperformed_baseline advmod_outperformed_only neg_outperformed_not nsubj_outperformed_approach poss_approach_Our
D08-1104	D07-1050	o	They are not used in LN but they are known to be useful for WSD -LRB- Tanaka et al. 2007 Magnini et al. 2002 -RRB-	num_Magnini_2002 nn_Magnini_al. nn_Magnini_et dep_Tanaka_Magnini amod_Tanaka_2007 dep_Tanaka_al. nn_Tanaka_et prep_for_useful_WSD cop_useful_be aux_useful_to xcomp_known_useful auxpass_known_are nsubjpass_known_they dep_used_Tanaka conj_but_used_known prep_in_used_LN neg_used_not auxpass_used_are nsubjpass_used_They
D09-1124	D07-1060	o	Although to a lesser extent measures of word relatedness have also been applied on other languages including German -LRB- Zesch et al. 2007 Zesch et al. 2008 Mohammad et al. 2007 -RRB- Chinese -LRB- Wang et al. 2008 -RRB- Dutch -LRB- Heylen et al. 2008 -RRB- and others	amod_Heylen_2008 dep_Heylen_al. nn_Heylen_et dep_Dutch_Heylen amod_Wang_2008 dep_Wang_al. nn_Wang_et dep_Chinese_Wang num_Mohammad_2007 nn_Mohammad_al. nn_Mohammad_et conj_and_Zesch_others conj_and_Zesch_Dutch conj_and_Zesch_Chinese dep_Zesch_Mohammad num_Zesch_2008 nn_Zesch_al. nn_Zesch_et dep_Zesch_others dep_Zesch_Dutch dep_Zesch_Chinese dep_Zesch_Zesch appos_Zesch_2007 dep_Zesch_al. nn_Zesch_et dep_German_Zesch prep_including_languages_German amod_languages_other prep_on_applied_languages auxpass_applied_been advmod_applied_also aux_applied_have nsubjpass_applied_measures prep_to_applied_extent mark_applied_Although nn_relatedness_word prep_of_measures_relatedness amod_extent_lesser det_extent_a advcl_``_applied
D09-1124	D07-1060	o	Also related are the areas of word alignment for machine translation -LRB- Och and Ney 2000 -RRB- induction of translation lexicons -LRB- Schafer and Yarowsky 2002 -RRB- and cross-language annotation projections to a second language -LRB- Riloff et al. 2002 Hwa et al. 2002 Mohammad et al. 2007 -RRB-	num_Mohammad_2007 nn_Mohammad_al. nn_Mohammad_et dep_Hwa_Mohammad num_Hwa_2002 nn_Hwa_al. nn_Hwa_et dep_Riloff_Hwa appos_Riloff_2002 dep_Riloff_al. nn_Riloff_et dep_language_Riloff amod_language_second det_language_a prep_to_projections_language nn_projections_annotation amod_projections_cross-language dep_Schafer_2002 conj_and_Schafer_Yarowsky appos_lexicons_Yarowsky appos_lexicons_Schafer nn_lexicons_translation prep_of_induction_lexicons dep_Och_2000 conj_and_Och_Ney appos_translation_Ney appos_translation_Och nn_translation_machine nn_alignment_word conj_and_areas_projections conj_and_areas_induction prep_for_areas_translation prep_of_areas_alignment det_areas_the cop_areas_are nsubj_areas_related advmod_related_Also
D09-1124	D07-1060	o	Measures of cross-language relatedness are useful for a large number of applications including cross-language information retrieval -LRB- Nie et al. 1999 Monz and Dorr 2005 -RRB- cross-language text classification -LRB- Gliozzo and Strapparava 2006 -RRB- lexical choice in machine translation -LRB- Och and Ney 2000 Bangalore et al. 2007 -RRB- induction of translation lexicons -LRB- Schafer and Yarowsky 2002 -RRB- cross-language annotation and resource projections to a second language -LRB- Riloff et al. 2002 Hwa et al. 2002 Mohammad et al. 2007 -RRB-	num_Mohammad_2007 nn_Mohammad_al. nn_Mohammad_et num_Hwa_2002 nn_Hwa_al. nn_Hwa_et dep_Riloff_Mohammad conj_Riloff_Hwa appos_Riloff_2002 dep_Riloff_al. nn_Riloff_et amod_language_second det_language_a dep_annotation_Riloff prep_to_annotation_language dep_annotation_projections conj_and_annotation_resource amod_annotation_cross-language dep_Schafer_2002 conj_and_Schafer_Yarowsky appos_lexicons_resource appos_lexicons_annotation appos_lexicons_Yarowsky appos_lexicons_Schafer nn_lexicons_translation prep_of_induction_lexicons num_Bangalore_2007 nn_Bangalore_al. nn_Bangalore_et dep_Och_Bangalore conj_and_Och_2000 conj_and_Och_Ney appos_translation_2000 appos_translation_Ney appos_translation_Och nn_translation_machine prep_in_choice_translation amod_choice_lexical dep_Gliozzo_2006 conj_and_Gliozzo_Strapparava appos_classification_Strapparava appos_classification_Gliozzo nn_classification_text amod_classification_cross-language dep_Monz_2005 conj_and_Monz_Dorr dep_Nie_Dorr dep_Nie_Monz appos_Nie_1999 dep_Nie_al. nn_Nie_et conj_retrieval_induction conj_retrieval_choice conj_retrieval_classification appos_retrieval_Nie nn_retrieval_information amod_retrieval_cross-language prep_including_number_retrieval prep_of_number_applications amod_number_large det_number_a prep_for_useful_number cop_useful_are nsubj_useful_Measures amod_relatedness_cross-language prep_of_Measures_relatedness
C08-1036	D07-1061	o	2 Related Work The most commonly used similarity measures are based on the WordNet lexical database -LRB- eg Budanitsky and Hirst 2006 Hughes and Ramage 2007 -RRB- and a number of such measures have been made publicly available -LRB- Pedersen et-al 2004 -RRB-	dobj_et-al_2004 nsubj_et-al_Pedersen dep_available_et-al advmod_available_publicly acomp_made_available auxpass_made_been aux_made_have nsubjpass_made_Work amod_measures_such prep_of_number_measures det_number_a num_Ramage_2007 num_Hirst_2006 conj_and_Budanitsky_Ramage conj_and_Budanitsky_Hughes conj_and_Budanitsky_Hirst nn_Budanitsky_eg conj_and_database_number dep_database_Ramage dep_database_Hughes dep_database_Hirst dep_database_Budanitsky nn_database_lexical nn_database_WordNet det_database_the prep_on_based_number prep_on_based_database auxpass_based_are nsubjpass_based_measures nn_measures_similarity amod_measures_used det_measures_The advmod_used_commonly advmod_used_most rcmod_Work_based amod_Work_Related num_Work_2
D08-1095	D07-1061	o	et al. 2004 Collins-Thompson and Callan 2005 Hughes and Ramage 2007 -RRB-	amod_Collins-Thompson_2007 conj_and_Collins-Thompson_Ramage conj_and_Collins-Thompson_Hughes conj_and_Collins-Thompson_2005 conj_and_Collins-Thompson_Callan dep_2004_Ramage dep_2004_Hughes dep_2004_2005 dep_2004_Callan dep_2004_Collins-Thompson dep_2004_al. nn_al._et
D08-1095	D07-1061	o	For instance Hughes and Ramage -LRB- 2007 -RRB- constructed a graph which represented various types of word relations from WordNet and compared random-walk similarity to similarity assessments from humansubject trials	amod_trials_humansubject prep_from_assessments_trials nn_assessments_similarity prep_to_similarity_assessments nn_similarity_random-walk amod_similarity_compared prep_from_relations_WordNet nn_relations_word prep_of_types_relations amod_types_various dobj_represented_types nsubj_represented_which rcmod_graph_represented det_graph_a conj_and_constructed_similarity dobj_constructed_graph nsubj_constructed_Ramage nsubj_constructed_Hughes prep_for_constructed_instance appos_Ramage_2007 conj_and_Hughes_Ramage
D09-1124	D07-1061	o	0 500 1000 1500 2000 5000 10000 15000 20000 25000 30000 Number of interlanguage links Vector length aren ares arro enes enro esro Figure 5 Number of interlanguage links vs. vector length for the Miller-Charles data set 0 500 1000 1500 2000 2500 3000 3500 4000 5000 10000 15000 20000 25000 30000 Number of interlanguage links Vector length aren ares arro enes enro esro Figure 6 Number of interlanguage links vs. vector length for the WordSimilarity-353 data set edge bases -LRB- Lesk 1986 Wu and Palmer 1994 Resnik 1995 Jiang and Conrath 1997 Hughes and Ramage 2007 -RRB- or on large corpora -LRB- Salton et al. 1997 Landauer et al. 1998 Turney 2001 Gabrilovich and Markovitch 2007 -RRB-	dep_Gabrilovich_2007 conj_and_Gabrilovich_Markovitch dep_Turney_Markovitch dep_Turney_Gabrilovich num_Turney_2001 dep_Landauer_Turney num_Landauer_1998 nn_Landauer_al. nn_Landauer_et dep_Salton_Landauer num_Salton_1997 dep_Salton_al. nn_Salton_et amod_corpora_large pobj_on_corpora num_Resnik_1995 conj_and_Wu_2007 conj_and_Wu_Ramage conj_and_Wu_Hughes conj_and_Wu_1997 conj_and_Wu_Conrath conj_and_Wu_Jiang conj_and_Wu_Resnik num_Wu_1994 conj_and_Wu_Palmer dep_Lesk_Salton conj_or_Lesk_on dep_Lesk_2007 dep_Lesk_Ramage dep_Lesk_Hughes dep_Lesk_1997 dep_Lesk_Conrath dep_Lesk_Jiang dep_Lesk_Resnik dep_Lesk_Palmer dep_Lesk_Wu appos_Lesk_1986 dep_bases_on dep_bases_Lesk nn_bases_edge nn_bases_set nn_bases_data amod_bases_WordSimilarity-353 det_bases_the nn_length_vector prep_for_links_bases conj_vs._links_length nn_links_interlanguage prep_of_Number_length prep_of_Number_links dep_Figure_Number num_Figure_6 nn_Figure_esro nn_Figure_enro dobj_enes_Figure nsubj_enes_arro ccomp_ares_enes vmod_aren_ares dep_length_aren nn_length_Vector nn_length_links nn_length_interlanguage prep_of_Number_length num_Number_30000 num_Number_0 dep_30000_25000 number_25000_20000 dep_25000_15000 dep_25000_5000 dep_25000_2500 number_15000_10000 number_5000_4000 dep_5000_3500 number_3500_3000 number_2500_2000 dep_2500_1500 dep_1500_1000 number_1000_500 dep_set_Number nn_set_data nn_set_Miller-Charles det_set_the nn_length_vector prep_for_links_set conj_vs._links_length nn_links_interlanguage prep_of_Number_length prep_of_Number_links dep_Figure_Number num_Figure_5 nn_Figure_esro nn_Figure_enro dobj_enes_Figure nsubj_enes_arro ccomp_ares_enes vmod_aren_ares dep_length_aren nn_length_Vector nn_length_links nn_length_interlanguage prep_of_Number_length num_Number_30000 num_Number_0 dep_30000_25000 number_25000_20000 dep_25000_15000 dep_25000_5000 number_15000_10000 number_5000_2000 dep_5000_1500 dep_1500_1000 number_1000_500 dep_``_Number
D09-1124	D07-1061	o	The dataset is available only in English and has been widely used in previous semantic relatedness evaluations -LRB- e.g. -LRB- Resnik 1995 Hughes and Ramage 2007 Zesch et al. 2008 -RRB- -RRB-	num_Zesch_2008 nn_Zesch_al. nn_Zesch_et num_Hughes_2007 conj_and_Hughes_Ramage dep_Resnik_Zesch dep_Resnik_Ramage dep_Resnik_Hughes dep_Resnik_1995 dep_,_Resnik dep_-LRB-_e.g. nn_evaluations_relatedness amod_evaluations_semantic amod_evaluations_previous prep_in_used_evaluations advmod_used_widely auxpass_used_been aux_used_has nsubjpass_used_dataset conj_and_available_used prep_in_available_English advmod_available_only cop_available_is nsubj_available_dataset det_dataset_The ccomp_``_used ccomp_``_available
N09-1003	D07-1061	o	Method Source Spearman -LRB- Strube and Ponzetto 2006 -RRB- Wikipedia 0.190.48 -LRB- Jarmasz 2003 -RRB- WordNet 0.330.35 -LRB- Jarmasz 2003 -RRB- Rogets 0.55 -LRB- Hughes and Ramage 2007 -RRB- WordNet 0.55 -LRB- Finkelstein et al. 2002 -RRB- Web corpus WN 0.56 -LRB- Gabrilovich and Markovitch 2007 -RRB- ODP 0.65 -LRB- Gabrilovich and Markovitch 2007 -RRB- Wikipedia 0.75 SVM Web corpus WN 0.78 Table 9 Comparison with previous work for WordSim353	prep_for_work_WordSim353 amod_work_previous prep_with_Comparison_work num_Table_9 num_Table_0.78 nn_Table_WN nn_corpus_Web nn_corpus_SVM num_corpus_0.75 nn_corpus_Wikipedia dep_corpus_Markovitch dep_corpus_Gabrilovich nn_corpus_ODP nn_corpus_WN dep_Gabrilovich_2007 conj_and_Gabrilovich_Markovitch num_ODP_0.65 dep_Gabrilovich_2007 conj_and_Gabrilovich_Markovitch dep_WN_Markovitch dep_WN_Gabrilovich num_WN_0.56 dep_corpus_Comparison conj_corpus_Table conj_corpus_corpus nn_corpus_Web dep_corpus_Finkelstein nn_corpus_WordNet dep_corpus_Ramage dep_corpus_Hughes nn_corpus_Rogets dep_corpus_Jarmasz nn_corpus_WordNet dep_corpus_Jarmasz nn_corpus_Wikipedia nn_corpus_Spearman dep_Finkelstein_2002 dep_Finkelstein_al. nn_Finkelstein_et num_WordNet_0.55 dep_Hughes_2007 conj_and_Hughes_Ramage num_Rogets_0.55 dep_Jarmasz_2003 num_WordNet_0.330.35 dep_Jarmasz_2003 num_Wikipedia_0.190.48 dep_Strube_2006 conj_and_Strube_Ponzetto appos_Spearman_Ponzetto appos_Spearman_Strube nn_Spearman_Source nn_Spearman_Method
N09-1003	D07-1061	n	We want to note that our WordNetbased method outperforms that of Hughes and Ramage -LRB- 2007 -RRB- which uses a similar method	amod_method_similar det_method_a dobj_uses_method nsubj_uses_which mark_uses_that appos_Ramage_2007 conj_and_Hughes_Ramage prep_of_that_Ramage prep_of_that_Hughes ccomp_outperforms_uses nsubj_outperforms_method mark_outperforms_that amod_method_WordNetbased poss_method_our ccomp_note_outperforms aux_note_to xcomp_want_note nsubj_want_We
N09-1003	D07-1061	p	Our similarity method is similar but simpler to that used by -LRB- Hughes and Ramage 2007 -RRB- which report very good results on similarity datasets	nn_datasets_similarity prep_on_results_datasets amod_results_good advmod_good_very dobj_report_results nsubj_report_which rcmod_Hughes_report dep_Hughes_2007 conj_and_Hughes_Ramage agent_used_Ramage agent_used_Hughes vmod_that_used nsubj_simpler_method prep_to_similar_that conj_but_similar_simpler cop_similar_is nsubj_similar_method nn_method_similarity poss_method_Our ccomp_``_simpler ccomp_``_similar
N09-1003	D07-1061	o	The techniques used to solve this problem can be roughly classified into two main categories those relying on pre-existing knowledge resources -LRB- thesauri semantic networks taxonomies or encyclopedias -RRB- -LRB- Alvarez and Lim 2007 Yang and Powers 2005 Hughes and Ramage 2007 -RRB- and those inducing distributional properties of words from corpora -LRB- Sahami and Heilman 2006 Chen et al. 2006 Bollegala et al. 2007 -RRB-	num_Bollegala_2007 nn_Bollegala_al. nn_Bollegala_et num_Chen_2006 nn_Chen_al. nn_Chen_et dep_Sahami_Bollegala conj_and_Sahami_Chen conj_and_Sahami_2006 conj_and_Sahami_Heilman appos_corpora_Chen appos_corpora_2006 appos_corpora_Heilman appos_corpora_Sahami prep_from_words_corpora prep_of_properties_words amod_properties_distributional dobj_inducing_properties vmod_those_inducing dep_Alvarez_2007 conj_and_Alvarez_Ramage conj_and_Alvarez_Hughes conj_and_Alvarez_2005 conj_and_Alvarez_Powers conj_and_Alvarez_Yang conj_and_Alvarez_2007 conj_and_Alvarez_Lim amod_networks_semantic conj_or_thesauri_encyclopedias conj_or_thesauri_taxonomies conj_or_thesauri_networks dep_resources_encyclopedias dep_resources_taxonomies dep_resources_networks dep_resources_thesauri nn_resources_knowledge amod_resources_pre-existing prep_on_relying_resources conj_and_those_those appos_those_Ramage appos_those_Hughes appos_those_2005 appos_those_Powers appos_those_Yang appos_those_2007 appos_those_Lim appos_those_Alvarez vmod_those_relying dep_categories_those dep_categories_those amod_categories_main num_categories_two prep_into_classified_categories advmod_classified_roughly auxpass_classified_be aux_classified_can nsubjpass_classified_techniques det_problem_this dobj_solve_problem aux_solve_to xcomp_used_solve vmod_techniques_used det_techniques_The ccomp_``_classified
N09-2060	D07-1061	o	Hughes and Ramage -LRB- 2007 -RRB- present a lexical similarity model based on random walks on graphs derived from WordNet Rao et al.	dep_Rao_al. nn_Rao_et prep_from_derived_WordNet vmod_graphs_derived prep_on_walks_graphs nsubj_walks_random mark_walks_on advcl_based_walks vmod_model_based nn_model_similarity amod_model_lexical det_model_a dep_present_Rao dobj_present_model nsubj_present_Ramage nsubj_present_Hughes appos_Ramage_2007 conj_and_Hughes_Ramage
N09-2060	D07-1061	o	This is similartothegraphconstructionmethodofHughes and Ramage -LRB- 2007 -RRB- and Rao et al.	dep_Rao_al. nn_Rao_et appos_Ramage_2007 conj_and_similartothegraphconstructionmethodofHughes_Rao conj_and_similartothegraphconstructionmethodofHughes_Ramage cop_similartothegraphconstructionmethodofHughes_is nsubj_similartothegraphconstructionmethodofHughes_This
W08-2006	D07-1061	o	7.1.3 Similarity via pagerank Pagerank -LRB- Page et al. 1998 -RRB- is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization -LRB- Erkan and Radev 2004 -RRB- to opinion mining -LRB- Esuli and Sebastiani 2007 -RRB- to our task of lexical relatedness -LRB- Hughes and Ramage 2007 -RRB-	amod_Hughes_2007 conj_and_Hughes_Ramage amod_relatedness_lexical prep_of_task_relatedness poss_task_our dep_Esuli_2007 conj_and_Esuli_Sebastiani dep_mining_Sebastiani dep_mining_Esuli nn_mining_opinion dep_Erkan_2004 conj_and_Erkan_Radev dep_summarization_Radev dep_summarization_Erkan prep_from_problems_summarization nn_problems_language amod_problems_natural amod_problems_several dep_applied_Ramage dep_applied_Hughes prep_to_applied_task prep_to_applied_mining prep_to_applied_problems auxpass_applied_been aux_applied_has nsubjpass_applied_that rcmod_algorithm_applied amod_algorithm_ranking nn_algorithm_citation amod_algorithm_celebrated det_algorithm_the cop_algorithm_is nsubj_algorithm_Similarity amod_Page_1998 dep_Page_al. nn_Page_et nn_Pagerank_pagerank dep_Similarity_Page prep_via_Similarity_Pagerank num_Similarity_7.1.3
W08-2006	D07-1061	o	We further note that our results are different from that of -LRB- Hughes and Ramage 2007 -RRB- as they use extensive feature engineering and weight tuning during the graph generation process that we have not been able to reproduce	aux_reproduce_to xcomp_able_reproduce cop_able_been neg_able_not aux_able_have nsubj_able_we mark_able_that ccomp_process_able nn_process_generation nn_process_graph det_process_the nn_tuning_weight conj_and_engineering_tuning nn_engineering_feature amod_engineering_extensive prep_during_use_process dobj_use_tuning dobj_use_engineering nsubj_use_they mark_use_as amod_Hughes_2007 conj_and_Hughes_Ramage prep_of_that_Ramage prep_of_that_Hughes advcl_different_use prep_from_different_that cop_different_are nsubj_different_results mark_different_that poss_results_our ccomp_note_different advmod_note_further nsubj_note_We
W09-1126	D07-1061	o	-LRB- Hughes and Ramage 2007 -RRB- described the use of a biased PageRank over the WordNet graph to compute word pair semantic relatedness using the divergence of the probability values over the graph created by each word	det_word_each agent_created_word vmod_graph_created det_graph_the prep_over_values_graph nn_values_probability det_values_the prep_of_divergence_values det_divergence_the dobj_using_divergence amod_relatedness_semantic nn_relatedness_pair nn_relatedness_word vmod_compute_using dobj_compute_relatedness aux_compute_to nn_graph_WordNet det_graph_the prep_over_PageRank_graph amod_PageRank_biased det_PageRank_a prep_of_use_PageRank det_use_the vmod_described_compute dobj_described_use nsubj_described_Ramage nsubj_described_Hughes amod_Hughes_2007 conj_and_Hughes_Ramage
E09-1041	D07-1068	o	4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction -LRB- Ponzetto and Strube 2007a -RRB- coreference resolution -LRB- Ponzetto and Strube 2007b -RRB- and English NER -LRB- e.g. Bunescu and Pasca -LRB- 2006 -RRB- Cucerzan -LRB- 2007 -RRB- Kazama and Torisawa -LRB- 2007 -RRB- Watanabe et al.	nn_al._et nn_al._Watanabe ccomp_al._used appos_Torisawa_2007 appos_Cucerzan_2007 appos_Pasca_2006 conj_and_Bunescu_Pasca conj_and_NER_Torisawa conj_and_NER_Kazama appos_NER_Cucerzan appos_NER_Pasca appos_NER_Bunescu dep_NER_e.g. nn_NER_English appos_Ponzetto_2007b conj_and_Ponzetto_Strube dep_resolution_Strube dep_resolution_Ponzetto nn_resolution_coreference appos_Ponzetto_2007a conj_and_Ponzetto_Strube conj_and_construction_Torisawa conj_and_construction_Kazama conj_and_construction_NER conj_and_construction_resolution dep_construction_Strube dep_construction_Ponzetto nn_construction_taxonomy prep_including_tasks_NER prep_including_tasks_resolution prep_including_tasks_construction nn_tasks_processing nn_tasks_language amod_tasks_various prep_for_source_tasks nn_source_knowledge det_source_a prep_as_used_source auxpass_used_been advmod_used_recently aux_used_has nsubjpass_used_Induction nn_Wikipedia_Wikipedia prep_from_Induction_Wikipedia nn_Induction_Class nn_Induction_Semantic num_Induction_4
D09-1058	D07-1070	o	It is often straightforward to obtain large amounts of unlabeled data making semi-supervised approaches appealing previous work on semisupervised methods for dependency parsing includes -LRB- Smith and Eisner 2007 Koo et al. 2008 Wang et al. 2008 -RRB-	num_Wang_2008 nn_Wang_al. nn_Wang_et num_Koo_2008 nn_Koo_al. nn_Koo_et dep_Smith_Wang conj_and_Smith_Koo conj_and_Smith_2007 conj_and_Smith_Eisner dep_includes_Koo dep_includes_2007 dep_includes_Eisner dep_includes_Smith nsubj_includes_work nn_parsing_dependency prep_for_methods_parsing amod_methods_semisupervised prep_on_work_methods amod_work_previous nsubj_appealing_approaches amod_approaches_semi-supervised xcomp_making_appealing amod_data_unlabeled prep_of_amounts_data amod_amounts_large dobj_obtain_amounts aux_obtain_to parataxis_straightforward_includes xcomp_straightforward_making xcomp_straightforward_obtain advmod_straightforward_often cop_straightforward_is nsubj_straightforward_It ccomp_``_straightforward
D09-1058	D07-1070	o	Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm -LRB- Baker 1979 -RRB- applied to the -LRB- Eisner 1996 -RRB- dependency-parsing data structures -LRB- Paskin 2001 -RRB- for projective dependency structures or the matrix-tree theorem -LRB- Koo et al. 2007 Smith and Smith 2007 McDonald and Satta 2007 -RRB- for nonprojective dependency structures	nn_structures_dependency amod_structures_nonprojective dep_Smith_2007 conj_and_Smith_Satta conj_and_Smith_McDonald conj_and_Smith_2007 conj_and_Smith_Smith dep_Koo_Satta dep_Koo_McDonald dep_Koo_2007 dep_Koo_Smith dep_Koo_Smith appos_Koo_2007 dep_Koo_al. nn_Koo_et prep_for_theorem_structures appos_theorem_Koo nn_theorem_matrix-tree det_theorem_the conj_or_structures_theorem nn_structures_dependency amod_structures_projective dep_Paskin_2001 dep_structures_Paskin nn_structures_data amod_structures_dependency-parsing dep_structures_Eisner det_structures_the dep_Eisner_1996 prep_for_applied_theorem prep_for_applied_structures prep_to_applied_structures dep_Baker_1979 amod_algorithm_inside-outside det_algorithm_the vmod_variant_applied appos_variant_Baker prep_of_variant_algorithm det_variant_a dobj_using_variant amod_counts_expected det_counts_these xcomp_calculate_using dobj_calculate_counts aux_calculate_to xcomp_straightforward_calculate cop_straightforward_is nsubj_straightforward_it mark_straightforward_that ccomp_Note_straightforward
D09-1086	D07-1070	o	One option would be to leverage unannotated text -LRB- McClosky et al. 2006 Smith and Eisner 2007 -RRB-	amod_Smith_2007 conj_and_Smith_Eisner dep_McClosky_Eisner dep_McClosky_Smith dep_McClosky_2006 dep_McClosky_al. nn_McClosky_et amod_text_unannotated nn_text_leverage dep_be_McClosky prep_to_be_text aux_be_would nsubj_be_option num_option_One
P09-1041	D07-1070	o	This generates tens of millions features so we prune those features that occur fewer than 10 total times as in -LRB- Smith and Eisner 2007 -RRB-	amod_Smith_2007 conj_and_Smith_Eisner dep_in_Eisner dep_in_Smith pobj_as_in amod_times_total num_times_10 quantmod_10_than mwe_than_fewer tmod_occur_times nsubj_occur_that rcmod_features_occur det_features_those prep_prune_as dobj_prune_features nsubj_prune_we mark_prune_so nn_features_millions prep_of_tens_features advcl_generates_prune dobj_generates_tens nsubj_generates_This
P09-1041	D07-1070	o	Smith and Eisner -LRB- 2007 -RRB- apply entropy regularization to dependency parsing	nn_parsing_dependency nn_regularization_entropy prep_to_apply_parsing dobj_apply_regularization nsubj_apply_Eisner nsubj_apply_Smith appos_Eisner_2007 conj_and_Smith_Eisner
D08-1082	D07-1071	o	Finally recent work has explored learning to map sentences to lambda-calculus meaning representations -LRB- Wong and Mooney 2007 Zettlemoyer and Collins 2005 Zettlemoyer and Collins 2007 -RRB-	num_Wong_2007 conj_and_Wong_Collins conj_and_Wong_Zettlemoyer conj_and_Wong_2005 conj_and_Wong_Collins conj_and_Wong_Zettlemoyer conj_and_Wong_2007 conj_and_Wong_Mooney appos_representations_Collins appos_representations_Zettlemoyer appos_representations_2005 appos_representations_Collins appos_representations_Zettlemoyer appos_representations_2007 appos_representations_Mooney appos_representations_Wong nn_representations_meaning amod_representations_lambda-calculus prep_to_map_representations dobj_map_sentences aux_map_to xcomp_learning_map xcomp_explored_learning aux_explored_has nsubj_explored_work advmod_explored_Finally amod_work_recent
D09-1001	D07-1071	o	Recently a number of machine learning approaches have been proposed -LRB- Zettlemoyer and Collins 2005 Mooney 2007 -RRB-	amod_Mooney_2007 dep_Zettlemoyer_Mooney conj_and_Zettlemoyer_2005 conj_and_Zettlemoyer_Collins dep_proposed_2005 dep_proposed_Collins dep_proposed_Zettlemoyer auxpass_proposed_been aux_proposed_have nsubjpass_proposed_number advmod_proposed_Recently nn_approaches_learning nn_approaches_machine prep_of_number_approaches det_number_a
D09-1001	D07-1071	o	For example when applying their approach to a different domain with somewhat less rigid syntax Zettlemoyer and Collins -LRB- 2007 -RRB- need to introduce new combinators and new forms of candidate lexical entries	amod_entries_lexical nn_entries_candidate prep_of_forms_entries amod_forms_new conj_and_combinators_forms amod_combinators_new dobj_introduce_forms dobj_introduce_combinators aux_introduce_to xcomp_need_introduce csubj_need_applying prep_for_need_example appos_Collins_2007 conj_and_syntax_Collins conj_and_syntax_Zettlemoyer amod_syntax_rigid amod_syntax_less advmod_less_somewhat amod_domain_different det_domain_a prep_to_approach_domain poss_approach_their prep_with_applying_Collins prep_with_applying_Zettlemoyer prep_with_applying_syntax dobj_applying_approach advmod_applying_when
E09-1052	D07-1071	o	There has thus been a trend recently towards robust wide-coverage semantic construction -LRB- e.g. -LRB- Bos et al. 2004 Zettlemoyer and Collins 2007 -RRB- -RRB-	dep_Zettlemoyer_2007 conj_and_Zettlemoyer_Collins dep_Bos_Collins dep_Bos_Zettlemoyer num_Bos_2004 dep_Bos_al. nn_Bos_et appos_e.g._Bos dep_construction_e.g. amod_construction_semantic amod_construction_wide-coverage amod_construction_robust prep_towards_trend_construction advmod_trend_recently det_trend_a cop_trend_been advmod_trend_thus aux_trend_has expl_trend_There
P08-1038	D07-1071	o	It has been used for a variety of tasks such as wide-coverage parsing -LRB- Hockenmaier and Steedman 2002 Clark and Curran 2007 -RRB- sentence realization -LRB- White 2006 -RRB- learning semantic parsers -LRB- Zettlemoyer and Collins 2007 -RRB- dialog systems -LRB- Kruijff et al. 2007 -RRB- grammar engineering -LRB- Beavers 2004 Baldridge et al. 2007 -RRB- and modeling syntactic priming -LRB- Reitter et al. 2006 -RRB-	amod_Reitter_2006 dep_Reitter_al. nn_Reitter_et nn_priming_syntactic nn_priming_modeling num_Baldridge_2007 nn_Baldridge_al. nn_Baldridge_et dep_Beavers_Baldridge appos_Beavers_2004 appos_engineering_Beavers nn_engineering_grammar amod_Kruijff_2007 dep_Kruijff_al. nn_Kruijff_et dep_systems_Kruijff nn_systems_dialog dep_Zettlemoyer_2007 conj_and_Zettlemoyer_Collins conj_and_parsers_priming conj_and_parsers_engineering appos_parsers_systems appos_parsers_Collins appos_parsers_Zettlemoyer amod_parsers_semantic dep_learning_Reitter dobj_learning_priming dobj_learning_engineering dobj_learning_parsers amod_White_2006 dep_realization_White nn_realization_sentence dep_Clark_2007 conj_and_Clark_Curran dep_Hockenmaier_Curran dep_Hockenmaier_Clark conj_and_Hockenmaier_2002 conj_and_Hockenmaier_Steedman vmod_parsing_learning appos_parsing_realization appos_parsing_2002 appos_parsing_Steedman appos_parsing_Hockenmaier amod_parsing_wide-coverage prep_such_as_tasks_parsing prep_of_variety_tasks det_variety_a prep_for_used_variety auxpass_used_been aux_used_has nsubjpass_used_It
P09-1011	D07-1071	o	1 Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations -LRB- e.g. some logical form -RRB- given aligned sentence/meaning pairs as training data -LRB- Ge and Mooney 2005 Zettlemoyer and Collins 2005 Zettlemoyer and Collins 2007 Lu et al. 2008 -RRB-	num_Lu_2008 nn_Lu_al. nn_Lu_et num_Zettlemoyer_2007 conj_and_Zettlemoyer_Collins conj_and_Zettlemoyer_Zettlemoyer conj_and_Zettlemoyer_2005 conj_and_Zettlemoyer_Collins dep_Ge_Lu conj_and_Ge_Collins conj_and_Ge_Zettlemoyer conj_and_Ge_2005 conj_and_Ge_Collins conj_and_Ge_Zettlemoyer conj_and_Ge_2005 conj_and_Ge_Mooney dep_data_Zettlemoyer dep_data_2005 dep_data_Mooney dep_data_Ge nn_data_training amod_pairs_sentence/meaning amod_pairs_aligned prep_as_given_data dobj_given_pairs amod_form_logical det_form_some dep_e.g._form vmod_representations_given dep_representations_e.g. dobj_meaning_representations prepc_to_mapping_meaning dobj_mapping_sentences prepc_on_focused_mapping aux_focused_has nsubj_focused_work amod_semantics_learning prep_in_work_semantics amod_work_Recent nn_work_Introduction num_work_1 ccomp_``_focused
P09-1069	D07-1071	o	available -RRB- SCISSOR -LRB- Ge and Mooney 2005 -RRB- an integrated syntactic-semantic parser KRISP -LRB- Kate and Mooney 2006 -RRB- an SVM-based parser using string kernels WASP -LRB- Wong and Mooney 2006 Wong and Mooney 2007 -RRB- a system based on synchronous grammars Z&C -LRB- Zettlemoyer and Collins 2007 -RRB- 3 a probabilistic parser based on relaxed CCG grammars and LU -LRB- Lu et al. 2008 -RRB- a generative model with discriminative reranking	amod_reranking_discriminative prep_with_model_reranking amod_model_generative det_model_a amod_Lu_2008 dep_Lu_al. nn_Lu_et appos_LU_model dep_LU_Lu nn_grammars_CCG dobj_relaxed_grammars prepc_on_based_relaxed vmod_parser_based amod_parser_probabilistic det_parser_a dep_Zettlemoyer_2007 conj_and_Zettlemoyer_Collins conj_and_Z&C_LU appos_Z&C_parser dep_Z&C_3 dep_Z&C_Collins dep_Z&C_Zettlemoyer amod_grammars_synchronous prep_on_based_grammars vmod_system_based det_system_a num_Wong_2006 conj_and_Wong_Mooney nn_Wong_WASP nn_kernels_string dobj_using_kernels vmod_parser_using amod_parser_SVM-based det_parser_an dep_Kate_2006 conj_and_Kate_Mooney appos_KRISP_parser appos_KRISP_Mooney appos_KRISP_Kate amod_parser_syntactic-semantic amod_parser_integrated det_parser_an dep_Ge_2005 conj_and_Ge_Mooney dep_SCISSOR_2007 conj_and_SCISSOR_Mooney conj_and_SCISSOR_Wong conj_and_SCISSOR_Mooney conj_and_SCISSOR_Wong conj_and_SCISSOR_KRISP appos_SCISSOR_parser appos_SCISSOR_Mooney appos_SCISSOR_Ge dep_available_LU dep_available_Z&C dep_available_system dep_available_Mooney dep_available_Wong dep_available_Wong dep_available_KRISP dep_available_SCISSOR
P09-1069	D07-1071	o	A number of systems for automatically learning semantic parsers have been proposed -LRB- Ge and Mooney 2005 Zettlemoyer and Collins 2005 Wong and Mooney 2007 Lu et al. 2008 -RRB-	num_Lu_2008 nn_Lu_al. nn_Lu_et dep_Zettlemoyer_Lu conj_and_Zettlemoyer_2007 conj_and_Zettlemoyer_Mooney conj_and_Zettlemoyer_Wong conj_and_Zettlemoyer_2005 conj_and_Zettlemoyer_Collins dep_Ge_2007 dep_Ge_Mooney dep_Ge_Wong dep_Ge_2005 dep_Ge_Collins dep_Ge_Zettlemoyer conj_and_Ge_2005 conj_and_Ge_Mooney dep_proposed_2005 dep_proposed_Mooney dep_proposed_Ge auxpass_proposed_been aux_proposed_have nsubjpass_proposed_number amod_parsers_semantic dobj_learning_parsers advmod_learning_automatically prepc_for_number_learning prep_of_number_systems det_number_A
P09-1110	D07-1071	o	1 Introduction Recently researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning -LRB- He and Young 2006 Wong and Mooney 2007 Zettlemoyer and Collins 2005 -RRB-	num_Zettlemoyer_2005 conj_and_Zettlemoyer_Collins dep_Wong_Collins dep_Wong_Zettlemoyer num_Wong_2007 conj_and_Wong_Mooney dep_He_Mooney dep_He_Wong conj_and_He_2006 conj_and_He_Young dep_meaning_2006 dep_meaning_Young dep_meaning_He amod_meaning_underlying poss_meaning_their prep_of_representations_meaning nn_sentences_language amod_sentences_natural prep_to_map_representations dobj_map_sentences aux_map_to xcomp_learn_map nsubj_learn_that rcmod_algorithms_learn dep_developed_algorithms aux_developed_have nsubj_developed_researchers ccomp_developed_Introduction advmod_Introduction_Recently num_Introduction_1
W09-0508	D07-1071	o	Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars -LRB- Zettlemoyer and Collins 2007 -RRB- -LRB- ZC07 -RRB-	dep_Zettlemoyer_2007 conj_and_Zettlemoyer_Collins appos_grammars_ZC07 appos_grammars_Collins appos_grammars_Zettlemoyer nn_grammars_CCG dobj_relaxed_grammars dep_using_relaxed xcomp_parsing_using conj_and_Zettlemoyer_Collins prepc_on_done_parsing agent_done_Collins agent_done_Zettlemoyer vmod_work_done det_work_the cop_work_is nsubj_work_point amod_point_starting poss_point_Our
W09-0508	D07-1071	o	Practically the grammar relaxation is done via the introduction of non-standard CCG rules -LRB- Zettlemoyer and Collins 2007 -RRB-	dep_Zettlemoyer_2007 conj_and_Zettlemoyer_Collins appos_rules_Collins appos_rules_Zettlemoyer nn_rules_CCG amod_rules_non-standard prep_of_introduction_rules det_introduction_the prep_via_done_introduction auxpass_done_is nsubjpass_done_relaxation advmod_done_Practically nn_relaxation_grammar det_relaxation_the
W09-0508	D07-1071	p	Albeit simple the algorithm has proven to be very efficient and accurate for the task of parse selection -LRB- Collins and Roark 2004 Collins 2004 Zettlemoyer and Collins 2005 Zettlemoyer and Collins 2007 -RRB-	num_Zettlemoyer_2007 conj_and_Zettlemoyer_Collins conj_and_Zettlemoyer_Collins conj_and_Zettlemoyer_Zettlemoyer conj_and_Zettlemoyer_2005 conj_and_Zettlemoyer_Collins num_Collins_2004 dep_Collins_Zettlemoyer dep_Collins_2005 dep_Collins_Collins dep_Collins_Zettlemoyer conj_and_Collins_Collins conj_and_Collins_2004 conj_and_Collins_Roark nn_selection_parse prep_of_task_selection det_task_the prep_for_efficient_task conj_and_efficient_accurate advmod_efficient_very cop_efficient_be aux_efficient_to dep_proven_Collins dep_proven_2004 dep_proven_Roark dep_proven_Collins xcomp_proven_accurate xcomp_proven_efficient aux_proven_has nsubj_proven_algorithm ccomp_proven_simple det_algorithm_the advmod_simple_Albeit
D08-1091	D07-1072	o	The parameters of the refined productions Ax By Cz where Ax is a subcategory of A By of B and Cz of C can then be estimated in various ways past work has included both generative -LRB- Matsuzaki et al. 2005 Liang et al. 2007 -RRB- and discriminative approaches -LRB- Petrov and Klein 2008 -RRB-	amod_Petrov_2008 conj_and_Petrov_Klein dep_approaches_Klein dep_approaches_Petrov amod_approaches_discriminative num_Liang_2007 nn_Liang_al. nn_Liang_et dep_Matsuzaki_Liang appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et conj_and_generative_approaches dep_generative_Matsuzaki preconj_generative_both dobj_included_approaches dobj_included_generative aux_included_has nsubj_included_work amod_work_past amod_ways_various parataxis_estimated_included prep_in_estimated_ways auxpass_estimated_be advmod_estimated_then aux_estimated_can nsubjpass_estimated_Cz nsubjpass_estimated_parameters prep_of_Cz_C pobj_of_B pcomp_By_of prep_of_subcategory_A det_subcategory_a cop_subcategory_is nsubj_subcategory_Ax advmod_subcategory_where rcmod_Cz_subcategory nn_Ax_productions amod_Ax_refined det_Ax_the conj_and_parameters_Cz prep_parameters_By prep_by_parameters_Cz prep_of_parameters_Ax det_parameters_The
D09-1071	D07-1072	o	A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting -LRB- Finkel et al. 2007 Liang et al. 2007 -RRB-	num_Liang_2007 nn_Liang_al. nn_Liang_et dep_Finkel_Liang appos_Finkel_2007 dep_Finkel_al. nn_Finkel_et dep_statesplitting_Finkel aux_statesplitting_is nsubj_statesplitting_approach amod_models_Bayesian amod_models_nonparametric prep_for_evaluating_NLP dobj_evaluating_models prepc_in_approach_evaluating amod_approach_different det_approach_A
N09-1009	D07-1072	o	There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings ranging from putting priors over grammar probabilities -LRB- Johnson et al. 2007 -RRB- to putting non-parametric priors over derivations -LRB- Johnson et al. 2006 -RRB- to learning the set of states in a grammar -LRB- Finkel et al. 2007 Liang et al. 2007 -RRB-	num_Liang_2007 nn_Liang_al. nn_Liang_et dep_Finkel_Liang appos_Finkel_2007 dep_Finkel_al. nn_Finkel_et det_grammar_a prep_of_set_states det_set_the prep_in_learning_grammar dobj_learning_set amod_Johnson_2006 dep_Johnson_al. nn_Johnson_et amod_priors_non-parametric prep_over_putting_derivations dobj_putting_priors dep_Johnson_Finkel prepc_to_Johnson_learning dep_Johnson_Johnson prepc_to_Johnson_putting amod_Johnson_2007 dep_Johnson_al. nn_Johnson_et nn_probabilities_grammar prep_over_putting_probabilities dobj_putting_priors prepc_from_ranging_putting amod_settings_different prep_in_grammars_settings amod_grammars_probabilistic prep_for_modeling_grammars amod_modeling_Bayesian dobj_employing_modeling dep_interest_Johnson vmod_interest_ranging prepc_in_interest_employing advmod_interest_recently amod_interest_increased det_interest_an cop_interest_been aux_interest_has expl_interest_There
N09-1019	D07-1072	o	In addition to the block sampler used by Bhattacharya and Getoor -LRB- 2006 -RRB- we are investigating general-purpose splitmerge samplers -LRB- Jain and Neal 2000 -RRB- and the permutation sampler -LRB- Liang et al. 2007a -RRB-	appos_Liang_2007a dep_Liang_al. nn_Liang_et dep_sampler_Liang nn_sampler_permutation det_sampler_the amod_Jain_2000 conj_and_Jain_Neal conj_and_samplers_sampler dep_samplers_Neal dep_samplers_Jain nn_samplers_splitmerge amod_samplers_general-purpose dobj_investigating_sampler dobj_investigating_samplers aux_investigating_are nsubj_investigating_we prep_in_addition_to_investigating_sampler appos_Getoor_2006 conj_and_Bhattacharya_Getoor agent_used_Getoor agent_used_Bhattacharya vmod_sampler_used nn_sampler_block det_sampler_the rcmod_``_investigating
N09-1019	D07-1072	o	-LRB- General grammars with infinite numbers of nonterminals were studied by -LRB- Liang et al. 2007b -RRB- -RRB-	appos_Liang_2007b dep_Liang_al. nn_Liang_et agent_studied_Liang auxpass_studied_were nsubjpass_studied_grammars prep_of_numbers_nonterminals amod_numbers_infinite prep_with_grammars_numbers nn_grammars_General
N09-1062	D07-1072	o	Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework .4 Models of grammar refinement -LRB- Petrov et al. 2006 Liang et al. 2007 Finkel et al. 2007 -RRB- also aim to automatically learn latent structure underlying treebanked data	amod_data_treebanked amod_data_underlying nn_data_structure amod_structure_latent dobj_learn_data advmod_learn_automatically aux_learn_to vmod_aim_learn advmod_aim_also nsubj_aim_Models num_Finkel_2007 nn_Finkel_al. nn_Finkel_et num_Liang_2007 nn_Liang_al. nn_Liang_et dep_Petrov_Finkel dep_Petrov_Liang appos_Petrov_2006 dep_Petrov_al. nn_Petrov_et nn_refinement_grammar appos_Models_Petrov prep_of_Models_refinement num_Models_.4 rcmod_framework_aim amod_framework_Bayesian det_framework_a prep_within_grammars_framework prep_over_a_grammars advmod_a_prior dobj_model_a advmod_model_explicitly nsubj_model_we prep_in_approaches_that amod_approaches_previous det_approaches_these parataxis_differs_model prep_from_differs_approaches nsubj_differs_work poss_work_Our ccomp_``_differs
P08-1046	D07-1072	o	Wed like to learn the number of paradigm classes from the data but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the iHMM and iPCFG -LRB- Liang et al. 2007 -RRB-	amod_Liang_2007 dep_Liang_al. nn_Liang_et dep_iHMM_Liang conj_and_iHMM_iPCFG dep_the_iPCFG dep_the_iHMM prep_in_found_the vmod_statesplitting_found amod_statesplitting_adaptive prep_of_kind_statesplitting det_kind_the dobj_incorporate_kind aux_incorporate_to nn_grammars_adaptor vmod_extending_incorporate dobj_extending_grammars xcomp_require_extending advmod_require_probably aux_require_would csubj_require_doing dobj_doing_this det_data_the nn_classes_paradigm prep_of_number_classes det_number_the conj_but_learn_require prep_from_learn_data dobj_learn_number aux_learn_to prepc_like_Wed_require prepc_like_Wed_learn ccomp_``_Wed
P08-1046	D07-1072	o	First we can construct an infinite number of more specialized PCFGs by splitting or refining the PCFGs nonterminals into increasingly finer states this leads to the iPCFG or infinite PCFG -LRB- Liang et al. 2007 -RRB-	amod_Liang_2007 dep_Liang_al. nn_Liang_et amod_PCFG_infinite conj_or_iPCFG_PCFG det_iPCFG_the dep_leads_Liang prep_to_leads_PCFG prep_to_leads_iPCFG nsubj_leads_this amod_states_finer advmod_states_increasingly nn_nonterminals_PCFGs det_nonterminals_the prep_into_splitting_states dobj_splitting_nonterminals conj_or_splitting_refining amod_PCFGs_specialized amod_PCFGs_more prep_of_number_PCFGs amod_number_infinite det_number_an parataxis_construct_leads prepc_by_construct_refining prepc_by_construct_splitting dobj_construct_number aux_construct_can nsubj_construct_we advmod_construct_First
P09-2085	D07-1072	o	Recently methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks including language modeling word and morpheme segmentation parsing and machine translation -LRB- Teh et al. 2006 Goldwater et al. 2006a Goldwater et al. 2006b Liang et al. 2007 Finkel et al. 2007 DeNero et al. 2008 -RRB-	num_DeNero_2008 nn_DeNero_al. nn_DeNero_et num_Finkel_2007 nn_Finkel_al. nn_Finkel_et num_Liang_2007 nn_Liang_al. nn_Liang_et appos_Goldwater_2006b dep_Goldwater_al. nn_Goldwater_et appos_Goldwater_2006a dep_Goldwater_al. nn_Goldwater_et dep_Teh_DeNero conj_Teh_Finkel conj_Teh_Liang conj_Teh_Goldwater conj_Teh_Goldwater appos_Teh_2006 dep_Teh_al. nn_Teh_et nn_translation_machine nn_segmentation_morpheme conj_and_modeling_translation conj_and_modeling_parsing conj_and_modeling_segmentation conj_and_modeling_word nn_modeling_language prep_including_tasks_translation prep_including_tasks_parsing prep_including_tasks_segmentation prep_including_tasks_word prep_including_tasks_modeling prep_of_variety_tasks det_variety_a prep_for_learning_variety amod_learning_unsupervised nn_learning_approach prep_to_way_learning det_way_a dep_gaining_Teh prep_as_gaining_way dobj_gaining_popularity aux_gaining_been aux_gaining_have nsubj_gaining_methods advmod_gaining_Recently amod_statistics_Bayesian amod_statistics_nonparametric prep_from_methods_statistics ccomp_``_gaining
W08-0704	D07-1072	o	First we can let the number of nonterminals grow unboundedly as in the Infinite PCFG where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG -LRB- Liang et al. 2007 -RRB-	amod_Liang_2007 dep_Liang_al. nn_Liang_et appos_PCFG_Liang nn_PCFG_base det_PCFG_a prep_of_versions_PCFG dobj_refined_versions advmod_refined_indefinitely auxpass_refined_be aux_refined_can nsubjpass_refined_nonterminals advmod_refined_where det_grammar_the prep_of_nonterminals_grammar det_nonterminals_the rcmod_PCFG_refined amod_PCFG_Infinite det_PCFG_the pobj_in_PCFG pcomp_as_in prep_grow_as advmod_grow_unboundedly nsubj_grow_number prep_of_number_nonterminals det_number_the ccomp_let_grow aux_let_can nsubj_let_we advmod_let_First
E09-1041	D07-1073	o	4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction -LRB- Ponzetto and Strube 2007a -RRB- coreference resolution -LRB- Ponzetto and Strube 2007b -RRB- and English NER -LRB- e.g. Bunescu and Pasca -LRB- 2006 -RRB- Cucerzan -LRB- 2007 -RRB- Kazama and Torisawa -LRB- 2007 -RRB- Watanabe et al.	nn_al._et nn_al._Watanabe ccomp_al._used appos_Torisawa_2007 appos_Cucerzan_2007 appos_Pasca_2006 conj_and_Bunescu_Pasca conj_and_NER_Torisawa conj_and_NER_Kazama appos_NER_Cucerzan appos_NER_Pasca appos_NER_Bunescu dep_NER_e.g. nn_NER_English appos_Ponzetto_2007b conj_and_Ponzetto_Strube dep_resolution_Strube dep_resolution_Ponzetto nn_resolution_coreference appos_Ponzetto_2007a conj_and_Ponzetto_Strube conj_and_construction_Torisawa conj_and_construction_Kazama conj_and_construction_NER conj_and_construction_resolution dep_construction_Strube dep_construction_Ponzetto nn_construction_taxonomy prep_including_tasks_NER prep_including_tasks_resolution prep_including_tasks_construction nn_tasks_processing nn_tasks_language amod_tasks_various prep_for_source_tasks nn_source_knowledge det_source_a prep_as_used_source auxpass_used_been advmod_used_recently aux_used_has nsubjpass_used_Induction nn_Wikipedia_Wikipedia prep_from_Induction_Wikipedia nn_Induction_Class nn_Induction_Semantic num_Induction_4
E09-1064	D07-1073	p	Wikipedia first sentence -LRB- WikiFS -RRB- Kazama and Torisawa -LRB- 2007 -RRB- used Wikipedia as an external knowledge to improve Named Entity Recognition	nn_Recognition_Entity amod_Recognition_Named dobj_improve_Recognition aux_improve_to vmod_knowledge_improve amod_knowledge_external det_knowledge_an amod_Wikipedia_used appos_Torisawa_2007 prep_as_Kazama_knowledge dep_Kazama_Wikipedia conj_and_Kazama_Torisawa dep_sentence_Torisawa dep_sentence_Kazama appos_sentence_WikiFS amod_sentence_first nn_sentence_Wikipedia
E09-1064	D07-1073	o	Recently Wikipedia is emerging as a source for extracting semantic relationships -LRB- Suchanek et al. 2007 Kazama and Torisawa 2007 -RRB-	dep_Kazama_2007 conj_and_Kazama_Torisawa dep_Suchanek_Torisawa dep_Suchanek_Kazama appos_Suchanek_2007 dep_Suchanek_al. nn_Suchanek_et amod_relationships_semantic dobj_extracting_relationships prepc_for_source_extracting det_source_a dep_emerging_Suchanek prep_as_emerging_source aux_emerging_is nsubj_emerging_Wikipedia advmod_emerging_Recently ccomp_``_emerging
E09-1070	D07-1073	p	Kazama and Torisawa -LRB- 2007 -RRB- improve their F-score by 3 % by including a Wikipedia-based feature in their machine learner	nn_learner_machine poss_learner_their prep_in_feature_learner amod_feature_Wikipedia-based det_feature_a pobj_including_feature num_%_3 poss_F-score_their prepc_by_improve_including prep_by_improve_% dobj_improve_F-score nsubj_improve_Torisawa nsubj_improve_Kazama appos_Torisawa_2007 conj_and_Kazama_Torisawa
I08-2126	D07-1073	o	4.1 Extraction from Definition Sentences Definition sentences in the Wikipedia article were used for acquiring hyponymy relations by -LRB- Kazama and Torisawa 2007 -RRB- for named entity recognition	nn_recognition_entity dep_named_recognition prepc_for_Kazama_named dep_Kazama_2007 conj_and_Kazama_Torisawa amod_relations_hyponymy prep_by_acquiring_Torisawa prep_by_acquiring_Kazama dobj_acquiring_relations prepc_for_used_acquiring auxpass_used_were nsubjpass_used_Extraction nn_article_Wikipedia det_article_the prep_in_sentences_article nn_sentences_Definition nn_sentences_Sentences nn_sentences_Definition prep_from_Extraction_sentences num_Extraction_4.1
I08-2126	D07-1073	o	Hyponymy relations were extracted from definition sentences -LRB- Herbelot and Copestake 2006 Kazama and Torisawa 2007 -RRB-	amod_Kazama_2007 conj_and_Kazama_Torisawa dep_Herbelot_Torisawa dep_Herbelot_Kazama conj_and_Herbelot_2006 conj_and_Herbelot_Copestake appos_sentences_2006 appos_sentences_Copestake appos_sentences_Herbelot nn_sentences_definition prep_from_extracted_sentences auxpass_extracted_were nsubjpass_extracted_relations amod_relations_Hyponymy
P08-1001	D07-1073	o	The most relevant to our work are Kazama and Torisawa -LRB- 2007 -RRB- Toral and Muoz -LRB- 2006 -RRB- and Cucerzan -LRB- 2007 -RRB-	appos_Cucerzan_2007 appos_Muoz_2006 conj_and_Toral_Muoz appos_Torisawa_2007 conj_and_Kazama_Cucerzan dep_Kazama_Muoz dep_Kazama_Toral conj_and_Kazama_Torisawa cop_Kazama_are nsubj_Kazama_relevant poss_work_our prep_to_relevant_work advmod_relevant_most det_relevant_The
P08-1001	D07-1073	o	Similarly Kazama and Torisawa -LRB- 2007 -RRB- used Wikipedia particularly the first sentence of each article to create lists of entities	prep_of_lists_entities dobj_create_lists aux_create_to det_article_each prep_of_sentence_article amod_sentence_first det_sentence_the advmod_sentence_particularly vmod_Wikipedia_create appos_Wikipedia_sentence dobj_used_Wikipedia nsubj_used_Torisawa nsubj_used_Kazama advmod_used_Similarly appos_Torisawa_2007 conj_and_Kazama_Torisawa
P08-1047	D07-1073	n	Although this Wikipedia gazetteer is much smaller than the English version used by Kazama and Torisawa -LRB- 2007 -RRB- that has over 2,000,000 entries it is the largest gazetteer that can be freely used for Japanese NER	amod_NER_Japanese prep_for_used_NER advmod_used_freely auxpass_used_be aux_used_can nsubjpass_used_that rcmod_gazetteer_used amod_gazetteer_largest det_gazetteer_the cop_gazetteer_is nsubj_gazetteer_it advcl_gazetteer_smaller num_entries_2,000,000 quantmod_2,000,000_over dobj_has_entries nsubj_has_that appos_Torisawa_2007 conj_and_Kazama_Torisawa agent_used_Torisawa agent_used_Kazama rcmod_version_has vmod_version_used nn_version_English det_version_the prep_than_smaller_version advmod_smaller_much cop_smaller_is nsubj_smaller_gazetteer mark_smaller_Although nn_gazetteer_Wikipedia det_gazetteer_this
P08-1047	D07-1073	o	We follow the method used by Kazama and Torisawa -LRB- 2007 -RRB- which encodes the matching with a gazetteer entity using IOB tags with the modication for Japanese	prep_for_modication_Japanese det_modication_the nn_tags_IOB dobj_using_tags vmod_entity_using nn_entity_gazetteer det_entity_a prep_with_matching_modication prep_with_matching_entity amod_the_matching dobj_encodes_the nsubj_encodes_which appos_Torisawa_2007 conj_and_Kazama_Torisawa agent_used_Torisawa agent_used_Kazama rcmod_method_encodes vmod_method_used det_method_the dobj_follow_method nsubj_follow_We
P08-1047	D07-1073	o	The small differences from their work are -LRB- 1 -RRB- We used characters as the unit as we described above -LRB- 2 -RRB- While Kazama and Torisawa -LRB- 2007 -RRB- checked only the word sequences that start with a capitalized word and thus exploitedthecharacteristicsofEnglishlanguage we checked the matching at every character -LRB- 3 -RRB- We used a TRIE to make the look-up efcient	amod_efcient_look-up det_efcient_the dobj_make_efcient aux_make_to det_TRIE_a vmod_used_make dobj_used_TRIE nsubj_used_We dep_used_3 det_character_every det_matching_the parataxis_checked_used prep_at_checked_character dobj_checked_matching nsubj_checked_we advcl_checked_checked dep_checked_2 advmod_exploitedthecharacteristicsofEnglishlanguage_thus conj_and_word_exploitedthecharacteristicsofEnglishlanguage amod_word_capitalized det_word_a prep_with_start_exploitedthecharacteristicsofEnglishlanguage prep_with_start_word nsubj_start_that rcmod_sequences_start nn_sequences_word det_sequences_the advmod_sequences_only dobj_checked_sequences nsubj_checked_Torisawa nsubj_checked_Kazama mark_checked_While appos_Torisawa_2007 conj_and_Kazama_Torisawa advmod_described_above nsubj_described_we mark_described_as det_unit_the ccomp_used_checked advcl_used_described prep_as_used_unit dobj_used_characters nsubj_used_We dep_used_1 parataxis_are_used nsubj_are_differences poss_work_their prep_from_differences_work amod_differences_small det_differences_The ccomp_``_are
P08-1047	D07-1073	p	For instance Kazama and Torisawa -LRB- 2007 -RRB- used the hyponymy relations extracted from Wikipedia for the English NER and reported improved accuracies with such a gazetteer	det_gazetteer_a predet_gazetteer_such prep_with_accuracies_gazetteer amod_accuracies_improved dobj_reported_accuracies nsubj_reported_Kazama nn_NER_English det_NER_the prep_for_extracted_NER prep_from_extracted_Wikipedia vmod_relations_extracted amod_relations_hyponymy det_relations_the conj_and_used_reported dobj_used_relations nsubj_used_Torisawa nsubj_used_Kazama prep_for_used_instance appos_Torisawa_2007 conj_and_Kazama_Torisawa
P08-1047	D07-1073	o	First the Wikipedia gazetteer improved the accuracy as expected i.e. it reproduced the result of Kazama and Torisawa -LRB- 2007 -RRB- for Japanese NER	amod_NER_Japanese appos_Torisawa_2007 conj_and_Kazama_Torisawa prep_of_result_Torisawa prep_of_result_Kazama det_result_the prep_for_reproduced_NER dobj_reproduced_result nsubj_reproduced_it dep_expected_i.e. mark_expected_as det_accuracy_the parataxis_improved_reproduced advcl_improved_expected dobj_improved_accuracy nsubj_improved_gazetteer advmod_improved_First nn_gazetteer_Wikipedia det_gazetteer_the
P08-1047	D07-1073	o	6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER -LRB- Shinzato et al. 2006 Talukdar et al. 2006 Nadeau et al. 2006 Kazama and Torisawa 2007 -RRB-	dep_Kazama_2007 conj_and_Kazama_Torisawa num_Nadeau_2006 nn_Nadeau_al. nn_Nadeau_et dep_Talukdar_Torisawa dep_Talukdar_Kazama conj_Talukdar_Nadeau num_Talukdar_2006 nn_Talukdar_al. nn_Talukdar_et dep_Shinzato_Talukdar appos_Shinzato_2006 dep_Shinzato_al. nn_Shinzato_et amod_gazetteers_extracted advmod_extracted_automatically prep_for_used_NER dobj_used_gazetteers nsubj_used_that dep_studies_Shinzato rcmod_studies_used amod_studies_several nsubj_are_studies expl_are_There dep_Work_are conj_and_Work_Discussion amod_Work_Related num_Work_6 dep_``_Discussion dep_``_Work
P08-1047	D07-1073	o	On the other hand Kazama and Torisawa -LRB- 2007 -RRB- extracted hyponymy relations which are independent of the NE categories from Wikipedia and utilized it as a gazetteer	det_gazetteer_a prep_as_utilized_gazetteer dobj_utilized_it nsubj_utilized_Kazama nn_categories_NE det_categories_the prep_of_independent_categories cop_independent_are nsubj_independent_which prep_from_relations_Wikipedia rcmod_relations_independent amod_relations_hyponymy conj_and_extracted_utilized dobj_extracted_relations nsubj_extracted_Torisawa nsubj_extracted_Kazama prep_on_extracted_hand appos_Torisawa_2007 conj_and_Kazama_Torisawa amod_hand_other det_hand_the
P08-1047	D07-1073	o	The previous studies with the exception of Kazama and Torisawa -LRB- 2007 -RRB- used smaller gazetteers than ours	prep_than_gazetteers_ours amod_gazetteers_smaller dobj_used_gazetteers appos_Torisawa_2007 conj_and_Kazama_Torisawa prep_of_exception_Torisawa prep_of_exception_Kazama det_exception_the dep_studies_used prep_with_studies_exception amod_studies_previous det_studies_The
P08-1047	D07-1073	o	We also compared the cluster gazetteers with the Wikipedia gazetteer constructed by following the method of -LRB- Kazama and Torisawa 2007 -RRB-	num_Kazama_2007 conj_and_Kazama_Torisawa dep_of_Torisawa dep_of_Kazama prep_method_of det_method_the pobj_following_method agent_constructed_following vmod_gazetteer_constructed nn_gazetteer_Wikipedia det_gazetteer_the prep_with_gazetteers_gazetteer nn_gazetteers_cluster det_gazetteers_the dobj_compared_gazetteers advmod_compared_also nsubj_compared_We ccomp_``_compared
P08-1047	D07-1073	o	Kazama and Torisawa -LRB- 2007 -RRB- extracted hyponymyrelationsfromtherstsentences -LRB- i.e. dening sentences -RRB- of Wikipedia articles and then used them as a gazetteer for NER	prep_for_gazetteer_NER det_gazetteer_a prep_as_used_gazetteer dobj_used_them advmod_used_then nn_articles_Wikipedia amod_sentences_dening advmod_sentences_i.e. amod_hyponymyrelationsfromtherstsentences_extracted nn_hyponymyrelationsfromtherstsentences_Torisawa appos_Torisawa_2007 conj_and_Kazama_used prep_of_Kazama_articles dep_Kazama_sentences conj_and_Kazama_hyponymyrelationsfromtherstsentences
P08-1047	D07-1073	o	The method described by Kazama and Torisawa -LRB- 2007 -RRB- is to rst extract the rst -LRB- base -RRB- noun phrase after the rst is was are or were in the rst sentence of a Wikipedia article	nn_article_Wikipedia det_article_a prep_of_sentence_article nn_sentence_rst det_sentence_the prep_in_were_sentence conj_or_are_were parataxis_are_was dep_are_is nsubj_is_rst mark_is_after det_rst_the nn_phrase_noun nn_phrase_base nn_phrase_rst det_phrase_the nn_phrase_extract advcl_rst_is dobj_rst_phrase aux_rst_to xcomp_is_rst nsubj_is_method appos_Torisawa_2007 conj_and_Kazama_Torisawa agent_described_Torisawa agent_described_Kazama vmod_method_described det_method_The
P09-1049	D07-1073	o	Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations -LRB- Herbelot and Copestake 2006 Kazama and Torisawa 2007 Ruiz-casado et al. 2005 Nastase and Strube 2008 Sumida et al. 2008 Suchanek et al. 2007 -RRB-	num_Suchanek_2007 nn_Suchanek_al. nn_Suchanek_et num_Sumida_2008 nn_Sumida_al. nn_Sumida_et dep_Nastase_Suchanek conj_and_Nastase_Sumida conj_and_Nastase_2008 conj_and_Nastase_Strube num_al._2005 nn_al._et amod_al._Ruiz-casado num_Kazama_2007 conj_and_Kazama_Torisawa dep_Herbelot_Sumida dep_Herbelot_2008 dep_Herbelot_Strube dep_Herbelot_Nastase conj_and_Herbelot_al. conj_and_Herbelot_Torisawa conj_and_Herbelot_Kazama conj_and_Herbelot_2006 conj_and_Herbelot_Copestake dep_relations_al. dep_relations_Kazama dep_relations_2006 dep_relations_Copestake dep_relations_Herbelot amod_relations_semantic dobj_acquire_relations aux_acquire_to vmod_rules_acquire amod_rules_machine-learned amod_rules_hand-crafted amod_rules_applied conj_or_hand-crafted_machine-learned det_corpora_the conj_and_Wikipedia_rules prep_as_Wikipedia_corpora amod_Wikipedia_regarded det_Wikipedia_Some ccomp_``_rules ccomp_``_Wikipedia
P09-1051	D07-1073	o	The second baseline is our implementation of the relevant part of the Wikipedia extraction in -LRB- Kazama and Torisawa 2007 -RRB- taking the first noun after a be verb in the definition sentence denoted as WikiBL	prep_as_denoted_WikiBL nn_sentence_definition det_sentence_the prep_in_verb_sentence auxpass_verb_be dep_verb_a mark_verb_after amod_noun_first det_noun_the vmod_taking_denoted advcl_taking_verb dobj_taking_noun vmod_Kazama_taking dep_Kazama_2007 conj_and_Kazama_Torisawa nn_extraction_Wikipedia det_extraction_the prep_of_part_extraction amod_part_relevant det_part_the prep_in_implementation_Torisawa prep_in_implementation_Kazama prep_of_implementation_part poss_implementation_our cop_implementation_is nsubj_implementation_baseline amod_baseline_second det_baseline_The
P09-1051	D07-1073	o	Kazama and Torisawa -LRB- 2007 -RRB- explores the first sentence of an article and identifies the first noun phrase following the verb be as a label for the article title	nn_title_article det_title_the prep_for_label_title det_label_a prep_as_be_label ccomp_verb_be det_verb_the dobj_following_verb vmod_phrase_following nn_phrase_noun amod_phrase_first det_phrase_the dobj_identifies_phrase nsubj_identifies_Kazama det_article_an prep_of_sentence_article amod_sentence_first det_sentence_the conj_and_explores_identifies dobj_explores_sentence nsubj_explores_Torisawa nsubj_explores_Kazama appos_Torisawa_2007 conj_and_Kazama_Torisawa
P09-1051	D07-1073	o	As the most concise definition we take the first sentence of each article following -LRB- Kazama and Torisawa 2007 -RRB-	dep_Kazama_2007 conj_and_Kazama_Torisawa dep_following_Torisawa dep_following_Kazama ccomp_,_following det_article_each prep_of_sentence_article amod_sentence_first det_sentence_the dobj_take_sentence nsubj_take_we rcmod_definition_take amod_definition_concise det_definition_the advmod_concise_most pobj_As_definition dep_``_As
P09-1051	D07-1073	o	Be-Comp Following the general idea in -LRB- Kazama and Torisawa 2007 -RRB- we identify the ISA pattern in the definition sentence by extracting nominal complements of the verb be taking 451 No	num_No_451 dobj_taking_No aux_taking_be xcomp_verb_taking det_verb_the prepc_of_complements_verb amod_complements_nominal dobj_extracting_complements nn_sentence_definition det_sentence_the nn_pattern_ISA det_pattern_the prepc_by_identify_extracting prep_in_identify_sentence dobj_identify_pattern nsubj_identify_we nsubj_identify_Be-Comp num_Kazama_2007 conj_and_Kazama_Torisawa prep_in_idea_Torisawa prep_in_idea_Kazama amod_idea_general det_idea_the prep_following_Be-Comp_idea
W09-1119	D07-1073	o	It turns out that while problems of coverage and ambiguity prevent straightforward lookup injection of gazetteer matches as features in machine-learning based approaches is critical for good performance -LRB- Cohen 2004 Kazama and Torisawa 2007a Toral and Munoz 2006 Florian et al. 2003 -RRB-	num_Florian_2003 nn_Florian_al. nn_Florian_et dep_Kazama_Florian conj_and_Kazama_2006 conj_and_Kazama_Munoz conj_and_Kazama_Toral conj_and_Kazama_2007a conj_and_Kazama_Torisawa dep_Cohen_2006 dep_Cohen_Munoz dep_Cohen_Toral dep_Cohen_2007a dep_Cohen_Torisawa dep_Cohen_Kazama amod_Cohen_2004 amod_performance_good prep_for_critical_performance cop_critical_is nsubj_critical_approaches amod_approaches_based dep_features_Cohen rcmod_features_critical prep_in_features_machine-learning prep_as_matches_features nsubj_matches_injection advcl_matches_prevent mark_matches_that prep_of_injection_gazetteer amod_lookup_straightforward dobj_prevent_lookup nsubj_prevent_ambiguity nsubj_prevent_problems mark_prevent_while conj_and_problems_ambiguity prep_of_problems_coverage ccomp_turns_matches prt_turns_out nsubj_turns_It
W09-1119	D07-1073	p	Recently -LRB- Toral and Munoz 2006 Kazama and Torisawa 2007a -RRB- have successfully constructed high quality and high coverage gazetteers from Wikipedia	nn_gazetteers_coverage amod_gazetteers_high prep_from_quality_Wikipedia conj_and_quality_gazetteers amod_quality_high dobj_constructed_gazetteers dobj_constructed_quality advmod_constructed_successfully aux_constructed_have nsubj_constructed_Munoz nsubj_constructed_Toral advmod_constructed_Recently appos_Kazama_2007a conj_and_Kazama_Torisawa dep_Toral_Torisawa dep_Toral_Kazama dep_Toral_2006 conj_and_Toral_Munoz
W09-1119	D07-1073	o	For example the entry about the Microsoft in Wikipedia has the following categories Companies listed on NASDAQ Cloud computing vendors etc. Both -LRB- Toral and Munoz 2006 -RRB- and -LRB- Kazama and Torisawa 2007a -RRB- used the free-text description of the Wikipedia entity to reason about the entity type	nn_type_entity det_type_the prep_about_reason_type nn_entity_Wikipedia det_entity_the prep_of_description_entity amod_description_free-text det_description_the prep_to_used_reason dobj_used_description nsubj_used_Kazama nsubj_used_vendors nsubj_used_Companies appos_Kazama_2007a conj_and_Kazama_Torisawa dep_Toral_2006 conj_and_Toral_Munoz preconj_Toral_Both dep_vendors_Munoz dep_vendors_Toral dep_vendors_etc. amod_vendors_computing nn_vendors_Cloud prep_on_listed_NASDAQ conj_and_Companies_Torisawa conj_and_Companies_Kazama conj_and_Companies_vendors vmod_Companies_listed dep_categories_used amod_categories_following det_categories_the dobj_has_categories nsubj_has_entry prep_for_has_example prep_in_Microsoft_Wikipedia det_Microsoft_the prep_about_entry_Microsoft det_entry_the
W09-1119	D07-1073	o	NER proves to be a knowledgeintensive task and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia Nonlocal Features Word-class Model 90.80 -LRB- Suzuki and Isozaki 2008 -RRB- Semi-supervised on 1Gword unlabeled data 89.92 -LRB- Ando and Zhang 2005 -RRB- Semi-supervised on 27Mword unlabeled data 89.31 -LRB- Kazama and Torisawa 2007a -RRB- Wikipedia 88.02 -LRB- Krishnan and Manning 2006 -RRB- Non-local Features 87.24 -LRB- Kazama and Torisawa 2007b -RRB- Non-local Features 87.17 + -LRB- Finkel et al. 2005 -RRB- Non-local Features 86.86 Table 7 Results for CoNLL03 data reported in the literature	det_literature_the prep_in_reported_literature vmod_data_reported nn_data_CoNLL03 prep_for_Results_data num_Table_7 num_Table_86.86 nn_Table_Features amod_Table_Non-local num_Table_Finkel num_Table_87.17 amod_Finkel_2005 dep_Finkel_al. nn_Finkel_et conj_+_87.17_Finkel dep_Features_Table amod_Features_Non-local conj_and_Kazama_2007b conj_and_Kazama_Torisawa dep_Features_Features dep_Features_2007b dep_Features_Torisawa dep_Features_Kazama num_Features_87.24 amod_Features_Non-local dep_Features_Manning dep_Features_Krishnan dep_Krishnan_2006 conj_and_Krishnan_Manning dep_88.02_Features nn_88.02_Wikipedia nn_88.02_89.31 dep_Kazama_2007a conj_and_Kazama_Torisawa appos_89.31_Torisawa appos_89.31_Kazama nn_89.31_data amod_89.31_unlabeled nn_89.31_27Mword amod_Ando_2005 conj_and_Ando_Zhang amod_89.92_Semi-supervised appos_89.92_Zhang appos_89.92_Ando nn_89.92_data amod_89.92_unlabeled nn_89.92_1Gword dep_Suzuki_2008 conj_and_Suzuki_Isozaki prep_on_Model_88.02 prep_on_Model_89.92 amod_Model_Semi-supervised appos_Model_Isozaki appos_Model_Suzuki num_Model_90.80 amod_Model_Word-class amod_Features_Nonlocal appos_Wikipedia_Model appos_Wikipedia_Features nn_Wikipedia_LBJ-NER nn_Wikipedia_F1 conj_+_F1_LBJ-NER dobj_Used_Wikipedia nsubj_Used_Resources mark_Used_that nn_Resources_System ccomp_observe_Used aux_observe_to xcomp_reassuring_observe aux_reassuring_was nsubj_reassuring_it amod_task_knowledgeintensive det_task_a cop_task_be aux_task_to dep_proves_Results conj_and_proves_reassuring xcomp_proves_task nsubj_proves_NER rcmod_``_reassuring rcmod_``_proves
W09-1119	D07-1073	o	Systems based on perceptron have been shown to be competitive in NER and text chunking -LRB- Kazama and Torisawa 2007b Punyakanok and Roth 2001 Carreras et al. 2003 -RRB- We specify the model and the features with the LBJ -LRB- Rizzolo and Roth 2007 -RRB- modeling language	nn_language_modeling dep_language_Roth dep_language_Rizzolo dep_language_LBJ dep_language_with dep_Rizzolo_2007 conj_and_Rizzolo_Roth det_LBJ_the rcmod_features_language det_features_the conj_and_model_features det_model_the dobj_specify_features dobj_specify_model nsubj_specify_We num_Carreras_2003 nn_Carreras_al. nn_Carreras_et rcmod_Punyakanok_specify dep_Punyakanok_Carreras conj_and_Punyakanok_2001 conj_and_Punyakanok_Roth dep_Kazama_2001 dep_Kazama_Roth dep_Kazama_Punyakanok conj_and_Kazama_2007b conj_and_Kazama_Torisawa nn_chunking_text conj_and_NER_chunking dep_competitive_2007b dep_competitive_Torisawa dep_competitive_Kazama prep_in_competitive_chunking prep_in_competitive_NER cop_competitive_be aux_competitive_to xcomp_shown_competitive auxpass_shown_been aux_shown_have nsubjpass_shown_Systems prep_on_based_perceptron vmod_Systems_based
D07-1047	D07-1074	o	We perform term disambiguation on each document using an entity extractor -LRB- Cucerzan 2007 -RRB-	amod_Cucerzan_2007 dep_extractor_Cucerzan nn_extractor_entity det_extractor_an dobj_using_extractor det_document_each nn_disambiguation_term xcomp_perform_using prep_on_perform_document dobj_perform_disambiguation nsubj_perform_We
D09-1029	D07-1074	o	Even if the idea of using Wikipedia links for disambiguation is not novel -LRB- Cucerzan 2007 -RRB- it is applied for the first time to FrameNet lexical units considering a frame as a sense definition	nn_definition_sense det_definition_a det_frame_a prep_as_considering_definition dobj_considering_frame amod_units_lexical nn_units_FrameNet prep_to_time_units amod_time_first det_time_the xcomp_applied_considering prep_for_applied_time auxpass_applied_is nsubjpass_applied_it advcl_applied_novel dep_Cucerzan_2007 dep_novel_Cucerzan neg_novel_not cop_novel_is nsubj_novel_idea mark_novel_if advmod_novel_Even prep_for_links_disambiguation nn_links_Wikipedia dobj_using_links prepc_of_idea_using det_idea_the
D09-1056	D07-1074	o	Some researchers -LRB- Cucerzan 2007 Nguyen and Cao 2008 -RRB- have explored the use of Wikipedia information to improve the disambiguation process	nn_process_disambiguation det_process_the dobj_improve_process aux_improve_to nn_information_Wikipedia vmod_use_improve prep_of_use_information det_use_the dobj_explored_use aux_explored_have nsubj_explored_researchers dep_Nguyen_2008 conj_and_Nguyen_Cao dep_Cucerzan_Cao dep_Cucerzan_Nguyen appos_Cucerzan_2007 appos_researchers_Cucerzan det_researchers_Some ccomp_``_explored
E09-1007	D07-1074	o	However most of them do not build a NEs resource but exploit external gazetteers -LRB- Bunescu and Pasca 2006 -RRB- -LRB- Cucerzan 2007 -RRB-	amod_Cucerzan_2007 dep_Bunescu_2006 conj_and_Bunescu_Pasca appos_gazetteers_Cucerzan appos_gazetteers_Pasca appos_gazetteers_Bunescu amod_gazetteers_external dobj_exploit_gazetteers nsubj_exploit_most nn_resource_NEs det_resource_a conj_but_build_exploit dobj_build_resource neg_build_not aux_build_do nsubj_build_most advmod_build_However prep_of_most_them
E09-1035	D07-1074	p	An important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature -LRB- Mann and Yarowsky 2003 Artiles et al. 2007 Cucerzan 2007 -RRB-	amod_Artiles_2007 appos_Artiles_Cucerzan num_Artiles_2007 nn_Artiles_al. nn_Artiles_et dep_Mann_Artiles conj_and_Mann_2003 conj_and_Mann_Yarowsky dep_literature_2003 dep_literature_Yarowsky dep_literature_Mann det_literature_the nn_disambiguation_name nn_disambiguation_person nn_disambiguation_web prep_in_focusing_literature prep_on_focusing_disambiguation vmod_efforts_focusing amod_efforts_multiple prep_to_leading_efforts vmod_name_leading amod_name_same det_name_the prep_with_people_name prep_among_distinguishing_people nn_results_search prepc_by_narrow_distinguishing dobj_narrow_results prt_narrow_down aux_narrow_to xcomp_able_narrow cop_able_be aux_able_to xcomp_is_able nsubj_is_aspect nn_search_web prep_of_aspect_search amod_aspect_important det_aspect_An ccomp_``_is
E09-1041	D07-1074	o	4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction -LRB- Ponzetto and Strube 2007a -RRB- coreference resolution -LRB- Ponzetto and Strube 2007b -RRB- and English NER -LRB- e.g. Bunescu and Pasca -LRB- 2006 -RRB- Cucerzan -LRB- 2007 -RRB- Kazama and Torisawa -LRB- 2007 -RRB- Watanabe et al.	nn_al._et nn_al._Watanabe ccomp_al._used appos_Torisawa_2007 appos_Cucerzan_2007 appos_Pasca_2006 conj_and_Bunescu_Pasca conj_and_NER_Torisawa conj_and_NER_Kazama appos_NER_Cucerzan appos_NER_Pasca appos_NER_Bunescu dep_NER_e.g. nn_NER_English appos_Ponzetto_2007b conj_and_Ponzetto_Strube dep_resolution_Strube dep_resolution_Ponzetto nn_resolution_coreference appos_Ponzetto_2007a conj_and_Ponzetto_Strube conj_and_construction_Torisawa conj_and_construction_Kazama conj_and_construction_NER conj_and_construction_resolution dep_construction_Strube dep_construction_Ponzetto nn_construction_taxonomy prep_including_tasks_NER prep_including_tasks_resolution prep_including_tasks_construction nn_tasks_processing nn_tasks_language amod_tasks_various prep_for_source_tasks nn_source_knowledge det_source_a prep_as_used_source auxpass_used_been advmod_used_recently aux_used_has nsubjpass_used_Induction nn_Wikipedia_Wikipedia prep_from_Induction_Wikipedia nn_Induction_Class nn_Induction_Semantic num_Induction_4
I08-1071	D07-1074	o	ca -LRB- 2006 -RRB- and Cucerzan -LRB- 2007 -RRB- in mining relationships between named entities or in extracting useful facet terms from news articles -LRB- e.g. Dakka and Ipeirotis 2008 -RRB-	amod_Dakka_2008 conj_and_Dakka_Ipeirotis dep_e.g._Ipeirotis dep_e.g._Dakka ccomp_-LRB-_e.g. nn_articles_news nn_terms_facet amod_terms_useful prep_from_extracting_articles dobj_extracting_terms pcomp_in_extracting conj_or_entities_in dep_named_in dep_named_entities prepc_between_relationships_named nn_relationships_mining pobj_in_relationships ccomp_,_in appos_Cucerzan_2007 conj_and_ca_Cucerzan dep_ca_2006 advcl_``_Cucerzan advcl_``_ca
I08-1071	D07-1074	o	Some of these have been previously employed for various tasks by Gabrilovich and Markovitch -LRB- 2006 -RRB- Overell and Ruger -LRB- 2006 -RRB- Cucerzan -LRB- 2007 -RRB- and Suchanek et al.	nn_al._et nn_al._Suchanek appos_Cucerzan_2007 appos_Ruger_2006 conj_and_Overell_al. conj_and_Overell_Cucerzan conj_and_Overell_Ruger conj_and_Gabrilovich_Markovitch prep_by_tasks_Markovitch prep_by_tasks_Gabrilovich amod_tasks_various dep_employed_al. dep_employed_Cucerzan dep_employed_Ruger dep_employed_Overell parataxis_employed_2006 prep_for_employed_tasks advmod_employed_previously auxpass_employed_been aux_employed_have nsubjpass_employed_Some prep_of_Some_these
N09-1019	D07-1074	p	Much later work -LRB- Evans 2003 Etzioni et al. 2005 Cucerzan 2007 Pasca 2004 -RRB- relies on the use of extremely large corpora which allow very precise but sparse features	amod_features_sparse advmod_precise_very acomp_allow_precise nsubj_allow_which amod_corpora_large advmod_large_extremely rcmod_use_allow prep_of_use_corpora det_use_the conj_but_relies_features prep_on_relies_use nsubj_relies_work dep_Pasca_2004 num_Cucerzan_2007 num_Etzioni_2005 nn_Etzioni_al. nn_Etzioni_et dep_Evans_Pasca dep_Evans_Cucerzan dep_Evans_Etzioni amod_Evans_2003 appos_work_Evans amod_work_later advmod_work_Much
P08-1001	D07-1074	o	The most relevant to our work are Kazama and Torisawa -LRB- 2007 -RRB- Toral and Muoz -LRB- 2006 -RRB- and Cucerzan -LRB- 2007 -RRB-	appos_Cucerzan_2007 appos_Muoz_2006 conj_and_Toral_Muoz appos_Torisawa_2007 conj_and_Kazama_Cucerzan dep_Kazama_Muoz dep_Kazama_Toral conj_and_Kazama_Torisawa cop_Kazama_are nsubj_Kazama_relevant poss_work_our prep_to_relevant_work advmod_relevant_most det_relevant_The
P08-1001	D07-1074	o	Cucerzan -LRB- 2007 -RRB- by contrast to the above used Wikipedia primarily for Named Entity Disambiguation following the path of Bunescu and Paca -LRB- 2006 -RRB-	appos_Paca_2006 conj_and_Bunescu_Paca prep_of_path_Paca prep_of_path_Bunescu det_path_the nn_Disambiguation_Entity amod_Disambiguation_Named prep_following_Wikipedia_path prep_for_Wikipedia_Disambiguation advmod_Wikipedia_primarily amod_Wikipedia_used det_above_the prep_to_contrast_above dep_,_Wikipedia prep_by_,_contrast appos_Cucerzan_2007 ccomp_``_Cucerzan
W08-2231	D07-1074	o	Wu and Weld -LRB- 2007 -RRB- and Cucerzan -LRB- 2007 -RRB- calculate the overlap between contexts of named entities and candidate articles from Wikipedia using overlap ratios or similarity scores in a vector space model respectively	nn_model_space nn_model_vector det_model_a nn_scores_similarity prep_in_ratios_model conj_or_ratios_scores advmod_overlap_respectively dobj_overlap_scores dobj_overlap_ratios dep_using_overlap prep_from_articles_Wikipedia nn_articles_candidate nn_articles_entities conj_and_entities_candidate dobj_named_articles prepc_of_contexts_named vmod_overlap_using prep_between_overlap_contexts nsubj_overlap_the ccomp_calculate_overlap nsubj_calculate_Cucerzan nsubj_calculate_Weld nsubj_calculate_Wu appos_Cucerzan_2007 appos_Weld_2007 conj_and_Wu_Cucerzan conj_and_Wu_Weld
P08-1076	D07-1083	o	CRF -LRB- baseline -RRB- -RSB- 97.18 97.21 Table 7 POS tagging results of the previous top systems for PTB III data evaluated by label accuracy system test additional resources JESS-CM -LRB- CRF/HMM -RRB- 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data -LRB- Ando and Zhang 2005 -RRB- 94.39 15M-word unlabeled data -LRB- Suzuki et al. 2007 -RRB- 94.36 17M-word unlabeled data -LRB- Zhang et al. 2002 -RRB- 94.17 full parser output -LRB- Kudo and Matsumoto 2001 -RRB- 93.91 -LSB- supervised CRF -LRB- baseline -RRB- -RSB- 93.88 Table 8 Syntactic chunking results of the previous top systems for CoNLL00 shared task data -LRB- F = 1 score -RRB- 30-31 Aug. 1996 and 6-7 Dec. 1996 Reuters news articles respectively	nn_articles_news nn_articles_Reuters nn_articles_Dec. num_articles_6-7 num_Dec._1996 conj_and_Aug._articles num_Aug._1996 num_Aug._30-31 num_score_1 advmod_=_respectively dep_=_articles dep_=_Aug. dobj_=_score nsubj_=_F dep_data_= nn_data_task dobj_shared_data nsubj_shared_results prep_for_systems_CoNLL00 amod_systems_top amod_systems_previous det_systems_the prep_of_results_systems amod_results_chunking amod_results_Syntactic num_Table_8 num_Table_93.88 appos_CRF_baseline dep_supervised_Table dobj_supervised_CRF dep_Kudo_2001 conj_and_Kudo_Matsumoto num_output_93.91 dep_output_Matsumoto dep_output_Kudo nn_output_parser amod_output_full num_output_94.17 dep_Zhang_2002 dep_Zhang_al. nn_Zhang_et dep_data_output dep_data_Zhang amod_data_unlabeled nn_data_17M-word num_data_94.36 amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et dep_data_data dep_data_Suzuki amod_data_unlabeled nn_data_15M-word num_data_94.39 parataxis_Ando_shared vmod_Ando_supervised dep_Ando_data amod_Ando_2005 conj_and_Ando_Zhang dep_data_Zhang dep_data_Ando amod_data_unlabeled nn_data_15M-word num_data_94.67 dep_data_data amod_data_unlabeled nn_data_1G-word num_data_95.15 dep_JESS-CM_data appos_JESS-CM_CRF/HMM nn_JESS-CM_resources amod_JESS-CM_additional nn_JESS-CM_test dep_system_JESS-CM amod_accuracy_system dep_label_accuracy agent_evaluated_label vmod_data_evaluated num_data_III nn_data_PTB prep_for_systems_data amod_systems_top amod_systems_previous det_systems_the prep_of_results_systems nn_results_tagging nn_results_POS dep_Table_results num_Table_7 num_Table_97.21 num_Table_97.18 dep_Table_CRF appos_CRF_baseline
P08-1076	D07-1083	o	test additional resources JESS-CM -LRB- CRF/HMM -RRB- 94.48 89.92 1G-word unlabeled data 93.66 89.36 37M-word unlabeled data -LRB- Ando and Zhang 2005 -RRB- 93.15 89.31 27M-word unlabeled data -LRB- Florian et al. 2003 -RRB- 93.87 88.76 own large gazetteers 2M-word labeled data -LRB- Suzuki et al. 2007 -RRB- N/A 88.41 27M-word unlabeled data -LSB- sup	dep_data_sup amod_data_unlabeled amod_data_27M-word num_data_88.41 nn_data_N/A dep_data_Suzuki nn_data_data amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et amod_data_labeled amod_data_2M-word amod_gazetteers_large amod_gazetteers_own num_gazetteers_88.76 number_88.76_93.87 dep_Florian_2003 dep_Florian_al. nn_Florian_et dep_data_gazetteers dep_data_Florian amod_data_unlabeled amod_data_27M-word num_data_89.31 number_89.31_93.15 dep_Ando_data dep_Ando_data amod_Ando_2005 conj_and_Ando_Zhang dep_data_Zhang dep_data_Ando amod_data_unlabeled nn_data_37M-word num_data_89.36 num_data_93.66 dep_data_data amod_data_unlabeled nn_data_1G-word num_data_89.92 number_89.92_94.48 dep_CRF/HMM_data dep_JESS-CM_CRF/HMM nn_JESS-CM_resources amod_JESS-CM_additional nn_JESS-CM_test
P08-1076	D07-1083	o	As our approach for incorporating unlabeled data we basically follow the idea proposed in -LRB- Suzuki et al. 2007 -RRB-	amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et dep_in_Suzuki prep_proposed_in vmod_idea_proposed det_idea_the dobj_follow_idea advmod_follow_basically nsubj_follow_we prep_as_follow_approach amod_data_unlabeled dobj_incorporating_data prepc_for_approach_incorporating poss_approach_our
P08-1076	D07-1083	o	Following this idea there have been introduced a parameter estimation approach for non-generative approaches that can effectively incorporate unlabeled data -LRB- Suzuki et al. 2007 -RRB-	amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et amod_data_unlabeled dobj_incorporate_data advmod_incorporate_effectively aux_incorporate_can nsubj_incorporate_that rcmod_approaches_incorporate amod_approaches_non-generative appos_approach_Suzuki prep_for_approach_approaches nn_approach_estimation nn_approach_parameter det_approach_a dobj_introduced_approach auxpass_introduced_been aux_introduced_have expl_introduced_there prep_following_introduced_idea det_idea_this
P08-1076	D07-1083	o	In addition the calculation cost for estimating parameters of embedded joint PMs -LRB- HMMs -RRB- is independent of the number of HMMs J that we used -LRB- Suzuki et al. 2007 -RRB-	amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et dep_used_Suzuki nsubj_used_we mark_used_that appos_HMMs_J prep_of_number_HMMs det_number_the ccomp_independent_used prep_of_independent_number cop_independent_is nsubj_independent_cost prep_in_independent_addition appos_PMs_HMMs amod_PMs_joint amod_PMs_embedded prep_of_parameters_PMs dobj_estimating_parameters prepc_for_cost_estimating nn_cost_calculation det_cost_the
P08-1076	D07-1083	o	2.4 Comparison with Hybrid Model SSL based on a hybrid generative/discriminative approach proposed in -LRB- Suzuki et al. 2007 -RRB- has been defined as a log-linear model that discriminatively combines several discriminative models pDi and generative models pGj such that R -LRB- y | x -RRB- = producttext i p Di -LRB- y | x i -RRB- i producttext j p Gj -LRB- xj y j -RRB- j summationtext y producttext i p Di -LRB- y | x i -RRB- i producttext j p Gj -LRB- xj y j -RRB- j where = -LCB- i -RCB- Ii = 1 and ={ -LCB- i -RCB- Ii = 1 -LCB- j -RCB- I+J j = I +1 -RCB-	number_+1_I dep_=_+1 amod_j_= nn_j_I+J nn_j_j dep_=_1 appos_Ii_j amod_Ii_= dep_i_Ii dep_={_i conj_and_=_={ dobj_=_1 amod_Ii_={ amod_Ii_= dep_=_Ii dep_=_i advmod_=_where dep_j_= dep_xj_j appos_xj_y dep_Gj_j dep_Gj_xj nn_Gj_p nn_Gj_j nn_Gj_producttext nn_Gj_i dep_Gj_i num_x_| dep_y_Gj dep_y_x dep_Di_y nn_Di_p nn_Di_i nn_Di_producttext nn_Di_y nn_Di_summationtext nn_Di_j dep_xj_j appos_xj_y dep_Gj_Di dep_Gj_xj nn_Gj_p nn_Gj_j nn_Gj_producttext nn_Gj_i dep_Gj_i num_x_| dep_y_Gj dep_y_x dep_Di_y nn_Di_p nn_Di_i nn_Di_producttext dep_=_Di nsubj_=_R mark_=_that dep_=_such num_x_| dep_y_x appos_R_y amod_models_= appos_models_pGj amod_models_generative conj_and_models_models conj_and_models_pDi amod_models_discriminative amod_models_several dobj_combines_models dobj_combines_pDi dobj_combines_models advmod_combines_discriminatively nsubj_combines_that rcmod_model_combines amod_model_log-linear det_model_a prep_as_defined_model auxpass_defined_been aux_defined_has nsubjpass_defined_Comparison amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et prep_in_proposed_Suzuki vmod_approach_proposed amod_approach_generative/discriminative nn_approach_hybrid det_approach_a prep_on_based_approach nn_SSL_Model nn_SSL_Hybrid vmod_Comparison_based prep_with_Comparison_SSL num_Comparison_2.4
P08-1076	D07-1083	o	As a solution a given amount of labeled training data is divided into two distinct sets i.e. 4/5 for estimating and the 667 remaining 1/5 for estimating -LRB- Suzuki et al. 2007 -RRB-	amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et appos_1/5_Suzuki prep_for_1/5_estimating amod_1/5_remaining num_1/5_667 det_1/5_the prep_for_4/5_estimating pobj_i.e._4/5 conj_and_sets_1/5 prep_sets_i.e. amod_sets_distinct num_sets_two prep_into_divided_1/5 prep_into_divided_sets auxpass_divided_is nsubjpass_divided_amount prep_as_divided_solution nn_data_training amod_data_labeled prep_of_amount_data amod_amount_given det_amount_a det_solution_a
P08-1076	D07-1083	n	Surprisingly although JESS-CM is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure JESS-CM provides F-scores of 94.45 and 88.03 for CoNLL00 and 03 data respectively which are 0.15 and 0.83 points higher than those reported in -LRB- Suzuki et al. 2007 -RRB- for the same configurations	amod_configurations_same det_configurations_the prep_for_Suzuki_configurations amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et prep_in_reported_Suzuki vmod_those_reported prep_than_higher_those dep_higher_points amod_0.15_higher conj_and_0.15_0.83 cop_0.15_are nsubj_0.15_which nn_data_03 nn_data_CoNLL00 conj_and_CoNLL00_03 conj_and_94.45_88.03 prep_for_F-scores_data prep_of_F-scores_88.03 prep_of_F-scores_94.45 ccomp_provides_0.83 ccomp_provides_0.15 advmod_provides_respectively dobj_provides_F-scores nsubj_provides_JESS-CM advcl_provides_procedure advcl_provides_version advmod_provides_Surprisingly nn_procedure_estimation nn_procedure_parameter nn_structure_model prep_of_terms_structure nn_model_hybrid det_model_the conj_and_version_procedure prep_in_version_terms prep_of_version_model amod_version_simpler det_version_a cop_version_is nsubj_version_JESS-CM mark_version_although
W08-2103	D07-1083	o	Networks -LRB- Toutanova et al. 2003 -RRB- 97.24 SVM -LRB- Gimenez and M`arquez 2003 -RRB- 97.05 ME based a bidirectional inference -LRB- Tsuruoka and Tsujii 2005 -RRB- 97.15 Guided learning for bidirectional sequence classification -LRB- Shen et al. 2007 -RRB- 97.33 AdaBoost.SDF with candidate features -LRB- = 2 = 1 = 100 W-dist -RRB- 97.32 AdaBoost.SDF with candidate features -LRB- = 2 = 10 = 10 F-dist -RRB- 97.32 SVM with candidate features -LRB- C = 0.1 d = 2 -RRB- 97.32 Text Chunking F = 1 Regularized Winnow + full parser output -LRB- Zhang et al. 2001 -RRB- 94.17 SVM-voting -LRB- Kudo and Matsumoto 2001 -RRB- 93.91 ASO + unlabeled data -LRB- Ando and Zhang 2005 -RRB- 94.39 CRF+R eranking -LRB- Kudo et al. 2005 -RRB- 94.12 ME based a bidirectional inference -LRB- Tsuruoka and Tsujii 2005 -RRB- 93.70 LaSo -LRB- Approximate Large Margin Update -RRB- -LRB- Daume III and Marcu 2005 -RRB- 94.4 HySOL -LRB- Suzuki et al. 2007 -RRB- 94.36 AdaBoost.SDF with candidate featuers -LRB- = 2 = 1 = W-dist -RRB- 94.32 AdaBoost.SDF with candidate featuers -LRB- = 2 = 10 = 10,W-dist -RRB- 94.30 SVM with candidate features -LRB- C = 1 d = 2 -RRB- 94.31 One of the reasons that boosting-based classifiers realize faster classification speed is sparseness of rules	prep_of_sparseness_rules cop_sparseness_is nsubj_sparseness_speed nn_speed_classification advmod_speed_faster ccomp_realize_sparseness nsubj_realize_classifiers dobj_realize_that amod_classifiers_boosting-based rcmod_reasons_realize det_reasons_the prep_of_One_reasons num_One_94.31 dobj_=_2 nsubj_=_d dep_=_1 rcmod_C_= amod_C_= dep_features_One dep_features_C nn_features_candidate prep_with_SVM_features num_SVM_94.30 dep_=_SVM dep_=_10,W-dist dep_=_10 dep_=_= dep_=_= dep_=_2 dep_featuers_= nn_featuers_candidate prep_with_AdaBoost.SDF_featuers num_AdaBoost.SDF_94.32 dep_=_1 dep_=_W-dist dep_=_= dep_=_= dep_=_2 dep_featuers_AdaBoost.SDF dep_featuers_= nn_featuers_candidate prep_with_AdaBoost.SDF_featuers num_AdaBoost.SDF_94.36 amod_Suzuki_2007 dep_Suzuki_al. nn_Suzuki_et dep_HySOL_AdaBoost.SDF dep_HySOL_Suzuki num_HySOL_94.4 dep_III_2005 conj_and_III_Marcu nn_III_Daume nn_Update_Margin nn_Update_Approximate amod_Margin_Large dep_LaSo_HySOL dep_LaSo_Marcu dep_LaSo_III appos_LaSo_Update num_LaSo_93.70 dep_Tsuruoka_LaSo dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii dep_inference_Tsujii dep_inference_Tsuruoka amod_inference_bidirectional det_inference_a pobj_based_inference num_ME_94.12 amod_Kudo_2005 dep_Kudo_al. nn_Kudo_et dep_eranking_ME dep_eranking_Kudo nn_eranking_CRF+R num_eranking_94.39 amod_Ando_2005 conj_and_Ando_Zhang amod_data_unlabeled dep_ASO_eranking dep_ASO_Zhang dep_ASO_Ando conj_+_ASO_data num_ASO_93.91 conj_and_Kudo_Matsumoto num_SVM-voting_94.17 amod_Zhang_2001 dep_Zhang_al. nn_Zhang_et nn_output_parser amod_output_full conj_+_Winnow_output amod_Winnow_Regularized num_Winnow_1 dep_=_output dep_=_Winnow amod_F_= dobj_Chunking_F vmod_Text_Chunking vmod_97.32_Text dobj_=_2 nsubj_=_d dep_=_0.1 dep_C_= amod_C_= dep_features_97.32 dep_features_C nn_features_candidate prep_with_SVM_features num_SVM_97.32 dep_=_10 dep_=_10 dep_=_F-dist dep_=_= dep_=_= dep_=_2 dep_features_SVM dep_features_= nn_features_candidate prep_with_AdaBoost.SDF_features nn_AdaBoost.SDF_W-dist num_W-dist_97.32 num_W-dist_100 dep_=_AdaBoost.SDF dep_=_1 dep_=_2001 dep_=_Matsumoto dep_=_Kudo dep_=_SVM-voting dep_=_Zhang dep_=_= dep_=_= dep_=_2 dep_features_data dep_features_ASO dep_features_= nn_features_candidate prep_AdaBoost.SDF_based prep_with_AdaBoost.SDF_features num_AdaBoost.SDF_97.33 dep_AdaBoost.SDF_Shen amod_Shen_2007 dep_Shen_al. nn_Shen_et nn_classification_sequence amod_classification_bidirectional prep_for_learning_classification xcomp_Guided_learning dep_Tsuruoka_2005 conj_and_Tsuruoka_Tsujii dep_inference_AdaBoost.SDF vmod_inference_Guided num_inference_97.15 appos_inference_Tsujii appos_inference_Tsuruoka amod_inference_bidirectional det_inference_a pobj_based_inference prep_ME_based num_ME_97.05 dep_Gimenez_2003 conj_and_Gimenez_M`arquez dep_SVM_ME dep_SVM_M`arquez dep_SVM_Gimenez num_SVM_97.24 dep_SVM_Toutanova nn_SVM_Networks dep_Toutanova_2003 dep_Toutanova_al. nn_Toutanova_et
D09-1014	D07-1087	o	In the first a separate language model is trained on each column of the database and these models are then used to segment and label a given text sequence -LRB- Agichtein and Ganti 2004 Canisius and Sporleder 2007 -RRB-	amod_Canisius_2007 conj_and_Canisius_Sporleder dep_Agichtein_Sporleder dep_Agichtein_Canisius conj_and_Agichtein_2004 conj_and_Agichtein_Ganti dep_sequence_2004 dep_sequence_Ganti dep_sequence_Agichtein nn_sequence_text amod_sequence_given det_sequence_a conj_and_segment_label dobj_used_sequence prep_to_used_label prep_to_used_segment advmod_used_then auxpass_used_are nsubjpass_used_models det_models_these det_database_the prep_of_column_database det_column_each conj_and_trained_used prep_on_trained_column auxpass_trained_is nsubjpass_trained_model prep_in_trained_first nn_model_language amod_model_separate det_model_a det_first_the
D09-1014	D07-1087	o	These records are also known as field books and reference sets in literature -LRB- Canisius and Sporleder 2007 Michelson and Knoblock 2008 -RRB-	amod_Michelson_2008 conj_and_Michelson_Knoblock dep_Canisius_Knoblock dep_Canisius_Michelson conj_and_Canisius_2007 conj_and_Canisius_Sporleder dep_literature_2007 dep_literature_Sporleder dep_literature_Canisius nn_sets_reference prep_in_books_literature conj_and_books_sets nn_books_field prep_as_known_sets prep_as_known_books advmod_known_also auxpass_known_are nsubjpass_known_records det_records_These
D09-1014	D07-1087	o	Both Agichtein and Ganti -LRB- 2004 -RRB- and Canisius and Sporleder -LRB- 2007 -RRB- train a language model for each database column	nn_column_database det_column_each prep_for_model_column nn_model_language det_model_a nn_train_Sporleder appos_Sporleder_2007 conj_and_Canisius_train appos_Ganti_2004 dep_Agichtein_model conj_and_Agichtein_train conj_and_Agichtein_Canisius conj_and_Agichtein_Ganti preconj_Agichtein_Both
C08-2005	D07-1090	o	In the second pass 5-gram and 6-gram zero-cutoff stupid-backoff -LRB- Brants et al. 2007 -RRB- language models estimated using 4.7 billion words of English newswire text are used to generate lattices for phrasal segmentation model rescoring	nn_rescoring_model nn_rescoring_segmentation amod_rescoring_phrasal prep_for_lattices_rescoring dobj_generate_lattices aux_generate_to xcomp_used_generate auxpass_used_are nsubjpass_used_models prep_in_used_pass nn_text_newswire amod_text_English prep_of_words_text num_words_billion number_billion_4.7 dobj_using_words xcomp_estimated_using vmod_models_estimated nn_models_language nn_models_stupid-backoff nn_models_5-gram dep_Brants_2007 dep_Brants_al. nn_Brants_et nn_stupid-backoff_zero-cutoff amod_stupid-backoff_6-gram dep_5-gram_Brants conj_and_5-gram_stupid-backoff amod_pass_second det_pass_the
D07-1005	D07-1090	o	5-gram word language models in English are trained on a variety of monolingual corpora -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et amod_corpora_monolingual prep_of_variety_corpora det_variety_a dep_trained_Brants prep_on_trained_variety auxpass_trained_are nsubjpass_trained_models prep_in_models_English nn_models_language nn_models_word amod_models_5-gram
D07-1105	D07-1090	o	For instance word alignment models are often trained using the GIZA + + toolkit -LRB- Och and Ney 2003 -RRB- error minimizing training criteria such as the Minimum Error Rate Training -LRB- Och 2003 -RRB- are employed in order to learn feature function weights for log-linear models and translation candidates are produced using phrase-based decoders -LRB- Koehn et al. 2003 -RRB- in combination with n-gram language models -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_models_Brants nn_models_language amod_models_n-gram prep_with_combination_models amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_decoders_Koehn amod_decoders_phrase-based prep_in_using_combination dobj_using_decoders xcomp_produced_using auxpass_produced_are csubjpass_produced_trained nn_candidates_translation amod_models_log-linear nn_weights_function nn_weights_feature prep_for_learn_models dobj_learn_weights aux_learn_to dep_learn_order mark_learn_in advcl_employed_learn auxpass_employed_are nsubjpass_employed_error amod_Och_2003 dep_Training_Och nn_Training_Rate nn_Training_Error nn_Training_Minimum det_Training_the prep_such_as_criteria_Training nn_criteria_training dobj_minimizing_criteria vmod_error_minimizing dep_Och_2003 conj_and_Och_Ney appos_toolkit_Ney appos_toolkit_Och pobj_+_toolkit conj_and_GIZA_candidates conj_+_GIZA_employed conj_+_GIZA_+ det_GIZA_the dobj_using_candidates dobj_using_employed dobj_using_+ dobj_using_GIZA xcomp_trained_using advmod_trained_often auxpass_trained_are nsubjpass_trained_models prep_for_trained_instance nn_models_alignment nn_models_word
D08-1044	D07-1090	o	Of course many applications require smoothing of the estimated distributionsthis problem also has known solutions in MapReduce -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_MapReduce_Brants prep_in_solutions_MapReduce dobj_known_solutions aux_known_has advmod_known_also nn_problem_distributionsthis amod_problem_estimated det_problem_the prep_of_smoothing_problem dep_require_known dobj_require_smoothing nsubj_require_applications prep_of_require_course amod_applications_many
D08-1085	D07-1090	p	We conclude by noting that English language models currently used in speech recognition -LRB- Chelba and Jelinek 1999 -RRB- and automated language translation -LRB- Brants et al. 2007 -RRB- are much more powerful employing for example 7-gram word models -LRB- not letter models -RRB- trained on trillions of words	prep_of_trillions_words prep_on_trained_trillions nn_models_letter neg_models_not vmod_models_trained appos_models_models nn_models_word amod_models_7-gram dobj_powerful_models prep_for_powerful_example xcomp_powerful_employing advmod_powerful_more advmod_powerful_much cop_powerful_are nsubj_powerful_translation nsubj_powerful_models mark_powerful_that amod_Brants_2007 dep_Brants_al. nn_Brants_et appos_translation_Brants nn_translation_language amod_translation_automated dep_Chelba_1999 conj_and_Chelba_Jelinek dep_recognition_Jelinek dep_recognition_Chelba nn_recognition_speech prep_in_used_recognition advmod_used_currently conj_and_models_translation vmod_models_used nn_models_language nn_models_English ccomp_noting_powerful prepc_by_conclude_noting nsubj_conclude_We
D09-1078	D07-1090	o	Since that time however increasingly large amounts of language model training data have become available ranging from approximately one billion words -LRB- the Gigaword corpora from the Linguistic Data Consortium -RRB- to trillions of words -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et prep_of_trillions_words nn_Consortium_Data nn_Consortium_Linguistic det_Consortium_the prep_from_corpora_Consortium nn_corpora_Gigaword det_corpora_the appos_words_corpora num_words_billion number_billion_one quantmod_billion_approximately prep_to_ranging_trillions prep_from_ranging_words vmod_available_ranging dep_become_Brants acomp_become_available aux_become_have nsubj_become_amounts advmod_become_however prep_since_become_time nn_data_training nn_data_model nn_data_language prep_of_amounts_data amod_amounts_large advmod_large_increasingly det_time_that
D09-1079	D07-1090	o	All the TB-LMs and O-RLMs were unpruned 5gram models and used Stupid-backoff smoothing -LRB- Brants et al. 2007 -RRB- 2 with the backoff parameter set to 0.4 as suggested	mark_suggested_as prep_to_set_0.4 vmod_parameter_set nn_parameter_backoff det_parameter_the amod_Brants_2007 dep_Brants_al. nn_Brants_et prep_with_smoothing_parameter dep_smoothing_2 dep_smoothing_Brants amod_smoothing_Stupid-backoff amod_smoothing_used dep_models_suggested conj_and_models_smoothing nn_models_5gram amod_models_unpruned cop_models_were nsubj_models_O-RLMs nsubj_models_TB-LMs dep_models_All conj_and_TB-LMs_O-RLMs det_TB-LMs_the
D09-1079	D07-1090	o	-LRB- 2007 -RRB- looked at Golomb Coding and Brants et al.	nn_al._et nn_al._Brants conj_and_Coding_al. nn_Coding_Golomb prep_at_looked_al. prep_at_looked_Coding dep_looked_2007
D09-1093	D07-1090	o	3.3 Language Model We estimate P -LRB- s -RRB- using n-gram LMs trained on data from the Web using Stupid Backoff -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_Backoff_Brants amod_Backoff_Stupid dobj_using_Backoff det_Web_the prep_from_data_Web prep_on_trained_data vmod_LMs_trained nn_LMs_n-gram dobj_using_LMs appos_P_s xcomp_estimate_using xcomp_estimate_using dobj_estimate_P nsubj_estimate_We rcmod_Model_estimate nn_Model_Language num_Model_3.3 dep_``_Model
E09-1019	D07-1090	o	This was expected as it has been observed before that very simple smoothing techniques can perform well on large data sets such as web data -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_data_Brants nn_data_web prep_such_as_sets_data nn_sets_data amod_sets_large prep_on_perform_sets advmod_perform_well aux_perform_can nsubj_perform_techniques mark_perform_that nn_techniques_smoothing amod_techniques_simple advmod_simple_very prepc_before_observed_perform auxpass_observed_been aux_observed_has nsubjpass_observed_it mark_observed_as advcl_expected_observed auxpass_expected_was nsubjpass_expected_This ccomp_``_expected
E09-1044	D07-1090	o	We build sentencespecific zero-cutoff stupid-backoff -LRB- Brants et al. 2007 -RRB- 5-gram language models estimated using 4.7 B words of English newswire text and apply them to rescore each 10000-best list	amod_list_10000-best det_list_each dobj_rescore_list aux_rescore_to xcomp_apply_rescore dobj_apply_them nsubj_apply_We nn_text_newswire amod_text_English prep_of_words_text nn_words_B num_words_4.7 dobj_using_words xcomp_estimated_using nn_models_language amod_models_5-gram nn_models_stupid-backoff dep_Brants_2007 dep_Brants_al. nn_Brants_et appos_stupid-backoff_Brants nn_stupid-backoff_zero-cutoff amod_stupid-backoff_sentencespecific conj_and_build_apply dep_build_estimated dobj_build_models nsubj_build_We ccomp_``_apply ccomp_``_build
I08-2089	D07-1090	o	A recent trend is to store the LM in a distributed cluster of machines which are queried via network requests -LRB- Brants et al. 2007 Emami et al. 2007 -RRB-	num_Emami_2007 nn_Emami_al. nn_Emami_et dep_Brants_Emami appos_Brants_2007 dep_Brants_al. nn_Brants_et appos_requests_Brants nn_requests_network prep_via_queried_requests auxpass_queried_are nsubjpass_queried_which rcmod_cluster_queried prep_of_cluster_machines amod_cluster_distributed det_cluster_a det_LM_the prep_in_store_cluster dobj_store_LM aux_store_to xcomp_is_store nsubj_is_trend amod_trend_recent det_trend_A ccomp_``_is
N09-1049	D07-1090	o	We build sentencespecific zero-cutoff stupid-backoff -LRB- Brants et al. 2007 -RRB- 5-gram language models estimated using 4.7 B words of English newswire text and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST	agent_generated_HiFST nn_lattices_word vmod_HCP_generated conj_or_HCP_lattices agent_generated_lattices agent_generated_HCP vmod_lists_generated amod_lists_10000-best preconj_lists_either dobj_rescore_lists aux_rescore_to xcomp_apply_rescore dobj_apply_them nsubj_apply_We nn_text_newswire amod_text_English prep_of_words_text nn_words_B num_words_4.7 dobj_using_words xcomp_estimated_using nn_models_language amod_models_5-gram nn_models_stupid-backoff dep_Brants_2007 dep_Brants_al. nn_Brants_et appos_stupid-backoff_Brants nn_stupid-backoff_zero-cutoff amod_stupid-backoff_sentencespecific conj_and_build_apply dep_build_estimated dobj_build_models nsubj_build_We ccomp_``_apply ccomp_``_build
N09-1058	D07-1090	o	In NLP community it has been shown that having more data results in better performance -LRB- Ravichandran et al. 2005 Brants et al. 2007 Turney 2008 -RRB-	amod_Turney_2008 dep_Brants_Turney num_Brants_2007 nn_Brants_al. nn_Brants_et dep_Ravichandran_Brants appos_Ravichandran_2005 dep_Ravichandran_al. nn_Ravichandran_et amod_performance_better prep_in_results_performance nn_results_data amod_results_more dep_having_Ravichandran dobj_having_results dep_that_having prep_shown_that auxpass_shown_been aux_shown_has nsubjpass_shown_it prep_in_shown_community nn_community_NLP
N09-1058	D07-1090	o	-LRB- Brants et al. 2007 Emami et al. 2007 -RRB- built 5-gram LMs over web using distributed cluster of machines and queried them via network requests	nn_requests_network prep_via_queried_requests dobj_queried_them nsubj_queried_Emami prep_of_cluster_machines amod_cluster_distributed dobj_using_cluster prep_over_LMs_web amod_LMs_5-gram conj_and_built_queried xcomp_built_using dobj_built_LMs nsubj_built_Emami num_Emami_2007 nn_Emami_al. nn_Emami_et parataxis_Brants_queried parataxis_Brants_built appos_Brants_2007 dep_Brants_al. nn_Brants_et dep_''_Brants
N09-1059	D07-1090	p	1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural languageprocessing -LRB- NLP -RRB- applications suchasprepositional phrase -LRB- PP -RRB- attachment other-anaphora resolution spellingcorrection confusablewordsetdisambiguation and machine translation -LRB- Volk 2001 Modjeska et al. 2003 Lapata and Keller 2005 Atterer and Schutze 2006 Brants et al. 2007 -RRB-	num_Brants_2007 nn_Brants_al. nn_Brants_et dep_Atterer_Brants conj_and_Atterer_2006 conj_and_Atterer_Schutze num_Lapata_2005 conj_and_Lapata_Keller dep_Modjeska_2006 dep_Modjeska_Schutze dep_Modjeska_Atterer conj_Modjeska_Keller conj_Modjeska_Lapata num_Modjeska_2003 nn_Modjeska_al. nn_Modjeska_et dep_Volk_Modjeska appos_Volk_2001 dep_translation_Volk nn_translation_machine nn_resolution_other-anaphora conj_and_attachment_translation conj_and_attachment_confusablewordsetdisambiguation conj_and_attachment_spellingcorrection conj_and_attachment_resolution nn_attachment_phrase appos_phrase_PP amod_phrase_suchasprepositional nn_applications_languageprocessing appos_languageprocessing_NLP amod_languageprocessing_natural amod_languageprocessing_many prep_for_utilized_applications advmod_utilized_successfully auxpass_utilized_been aux_utilized_have nsubjpass_utilized_corpora det_Web_the prep_from_obtained_Web vmod_corpora_obtained amod_corpora_large advmod_large_Very dep_Introduction_translation dep_Introduction_confusablewordsetdisambiguation dep_Introduction_spellingcorrection dep_Introduction_resolution dep_Introduction_attachment rcmod_Introduction_utilized num_Introduction_1
P08-1058	D07-1090	p	To scale LMs to larger corpora with higher-order dependencies researchers Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models -LRB- Brown et al. 1992 -RRB- model reduction techniques such as entropy-based pruning -LRB- Stolcke 1998 -RRB- novel represention schemes such as suffix arrays -LRB- Emami et al. 2007 -RRB- Golomb Coding -LRB- Church et al. 2007 -RRB- and distributed language models that scale more readily -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_readily_more dep_scale_Brants advmod_scale_readily nsubj_scale_that nn_models_language amod_models_distributed amod_Church_2007 dep_Church_al. nn_Church_et dep_Coding_Church nn_Coding_Golomb amod_Emami_2007 dep_Emami_al. nn_Emami_et conj_and_arrays_models conj_and_arrays_Coding dep_arrays_Emami nn_arrays_suffix prep_such_as_schemes_models prep_such_as_schemes_Coding prep_such_as_schemes_arrays nn_schemes_represention amod_schemes_novel amod_Stolcke_1998 rcmod_pruning_scale conj_pruning_schemes dep_pruning_Stolcke amod_pruning_entropy-based prep_such_as_techniques_pruning nn_techniques_reduction amod_techniques_model amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_models_Brown amod_models_class-based appos_parameterizations_techniques prep_as_parameterizations_models mwe_parameterizations_such amod_parameterizations_alternative dobj_considered_parameterizations aux_considered_have nn_Inc._Google prep_at_was_Inc. nsubj_was_author mark_was_while det_author_this dep_completed_considered advcl_completed_was prep_Work_completed nsubj_Work_researchers advcl_Work_scale amod_dependencies_higher-order prep_with_corpora_dependencies amod_corpora_larger prep_to_scale_corpora dobj_scale_LMs aux_scale_To
P08-1058	D07-1090	p	Here we choose to work with stupid backoff smoothing -LRB- Brants et al. 2007 -RRB- since this is significantly more efficient to train and deploy in a distributed framework than a contextdependent smoothing scheme such as Kneser-Ney	prep_such_as_scheme_Kneser-Ney nn_scheme_smoothing amod_scheme_contextdependent det_scheme_a amod_framework_distributed det_framework_a prep_in_train_framework conj_and_train_deploy aux_train_to prep_than_efficient_scheme xcomp_efficient_deploy xcomp_efficient_train advmod_efficient_more advmod_efficient_significantly cop_efficient_is nsubj_efficient_this mark_efficient_since amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_smoothing_Brants nn_smoothing_backoff amod_smoothing_stupid advcl_work_efficient prep_with_work_smoothing aux_work_to xcomp_choose_work nsubj_choose_we advmod_choose_Here
P08-1058	D07-1090	o	Previous work -LRB- Brants et al. 2007 -RRB- has shown it to be appropriate to large-scale language modeling	nn_modeling_language amod_modeling_large-scale prep_to_appropriate_modeling cop_appropriate_be aux_appropriate_to xcomp_shown_appropriate dobj_shown_it aux_shown_has nsubj_shown_work amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_work_Brants amod_work_Previous
P08-1058	D07-1090	o	Table 2 shows the total space and number of bytes required per n-gram to encode the model under different schemes LDC gzipd is the size of the files as delivered by LDC Trie uses a compact trie representation -LRB- e.g. -LRB- Clarkson et al. 1997 Church et al. 2007 -RRB- -RRB- with 3 byte word ids 1 byte values and 3 byte indices Block encoding is the encoding used in -LRB- Brants et al. 2007 -RRB- and randomized uses our novel randomized scheme with 12 error bits	nn_bits_error num_bits_12 amod_scheme_randomized amod_scheme_novel poss_scheme_our prep_with_uses_bits dobj_uses_scheme nsubj_uses_randomized appos_Brants_2007 dep_Brants_al. nn_Brants_et prep_in_used_Brants conj_and_encoding_uses conj_and_encoding_used amod_the_uses amod_the_used amod_the_encoding nsubj_is_the dep_Block_is amod_Block_encoding nn_indices_byte num_indices_3 nn_values_byte num_values_1 conj_and_ids_indices conj_and_ids_values nn_ids_word nn_ids_byte num_ids_3 num_Church_2007 nn_Church_al. nn_Church_et dep_Clarkson_Church amod_Clarkson_1997 dep_Clarkson_al. nn_Clarkson_et appos_e.g._Clarkson prep_with_representation_indices prep_with_representation_values prep_with_representation_ids dep_representation_e.g. nn_representation_trie amod_representation_compact det_representation_a dobj_uses_representation nsubj_uses_Trie agent_delivered_LDC mark_delivered_as det_files_the vmod_size_delivered prep_of_size_files det_size_the cop_size_is nsubj_size_gzipd nn_gzipd_LDC amod_schemes_different det_model_the prep_under_encode_schemes dobj_encode_model aux_encode_to xcomp_required_encode prep_per_required_n-gram vmod_bytes_required dep_space_size prep_of_space_bytes conj_and_space_number amod_space_total det_space_the dep_shows_Block parataxis_shows_uses dobj_shows_number dobj_shows_space nsubj_shows_Table num_Table_2
P08-1058	D07-1090	o	-LRB- Emami et al. 2007 -RRB- -LRB- Brants et al. 2007 -RRB- -LRB- Church et al. 2007 -RRB-	amod_Church_2007 dep_Church_al. nn_Church_et amod_Brants_2007 dep_Brants_al. nn_Brants_et appos_Emami_Church appos_Emami_Brants amod_Emami_2007 dep_Emami_al. nn_Emami_et dep_''_Emami
P08-1075	D07-1090	o	There is a vast literature on language modeling see e.g. -LRB- Rosenfeld 2000 Chen and Goodman 1999 Brants et al. 2007 Roark et al. 2007 -RRB-	num_Roark_2007 nn_Roark_al. nn_Roark_et num_Brants_2007 nn_Brants_al. nn_Brants_et num_Chen_1999 conj_and_Chen_Goodman dep_Rosenfeld_Roark dep_Rosenfeld_Brants dep_Rosenfeld_Goodman dep_Rosenfeld_Chen amod_Rosenfeld_2000 appos_see_Rosenfeld dep_see_e.g. nn_modeling_language prep_on_literature_modeling amod_literature_vast det_literature_a parataxis_is_see nsubj_is_literature expl_is_There ccomp_``_is
P08-1086	D07-1090	o	We use the distributed training and application infrastructure described in -LRB- Brants et al. 2007 -RRB- with modifications to allow the training of predictive class-based models and their application in the decoder of the machine translation system	nn_system_translation nn_system_machine det_system_the prep_of_decoder_system det_decoder_the prep_in_application_decoder poss_application_their amod_models_class-based amod_models_predictive conj_and_training_application prep_of_training_models det_training_the dobj_allow_application dobj_allow_training aux_allow_to vmod_modifications_allow amod_Brants_2007 dep_Brants_al. nn_Brants_et prep_with_described_modifications prep_in_described_Brants nn_infrastructure_application vmod_training_described conj_and_training_infrastructure amod_training_distributed det_training_the dobj_use_infrastructure dobj_use_training nsubj_use_We
P08-1086	D07-1090	o	759 For all models used in our experiments both wordand class-based the smoothing method used was Stupid Backoff -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_Backoff_Brants amod_Backoff_Stupid cop_Backoff_was nsubj_Backoff_method ccomp_Backoff_class-based num_Backoff_759 vmod_method_used nn_method_smoothing det_method_the amod_class-based_wordand preconj_class-based_both poss_experiments_our prep_in_used_experiments vmod_models_used det_models_all prep_for_759_models
P08-1086	D07-1090	o	Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams -LRB- Goodman and Gao 2000 -RRB- and even despite the increasing size and decreasing sparsity of language model training corpora -LRB- Brants et al. 2007 -RRB- class-based n-gram models might lead to improvements when increasing the n-gram order	nn_order_n-gram det_order_the dobj_increasing_order advmod_increasing_when advcl_lead_increasing prep_to_lead_improvements aux_lead_might nsubj_lead_models advcl_lead_decreasing advcl_lead_despite advcl_lead_scaling nn_models_n-gram amod_models_class-based amod_Brants_2007 dep_Brants_al. nn_Brants_et nn_corpora_training nn_corpora_model nn_corpora_language prep_of_sparsity_corpora dobj_decreasing_sparsity amod_size_increasing det_size_the pobj_despite_size advmod_despite_even dep_Goodman_2000 conj_and_Goodman_Gao appos_n-grams_Gao appos_n-grams_Goodman amod_n-grams_higher-order dep_scaling_Brants conj_and_scaling_decreasing conj_and_scaling_despite prep_to_scaling_n-grams advmod_scaling_when prep_of_number_parameters amod_number_reduced poss_number_their prep_from_benefit_number aux_benefit_to ccomp_shown_lead xcomp_shown_benefit auxpass_shown_been advmod_shown_also aux_shown_have nsubjpass_shown_models nn_models_n-gram amod_models_Class-based
P09-1087	D07-1090	p	Indeed researchers have shown that gigantic language models are key to state-ofthe-art performance -LRB- Brants et al. 2007 -RRB- and the ability of phrase-based decoders to handle large-size high-order language models with no consequence on asymptotic running time during decoding presents a compelling advantage over CKYdecoders whosetimecomplexitygrowsprohibitively large with higher-order language models	nn_models_language amod_models_higher-order prep_with_large_models advmod_large_whosetimecomplexitygrowsprohibitively nsubj_large_models prep_over_advantage_CKYdecoders amod_advantage_compelling det_advantage_a dobj_presents_advantage nsubj_presents_ability amod_time_running amod_time_asymptotic prep_on_consequence_time neg_consequence_no prep_during_models_decoding prep_with_models_consequence nn_models_language amod_models_high-order conj_large-size_models dobj_handle_large-size aux_handle_to amod_decoders_phrase-based vmod_ability_handle prep_of_ability_decoders det_ability_the appos_Brants_2007 dep_Brants_al. nn_Brants_et amod_performance_state-ofthe-art conj_and_key_large conj_and_key_presents dep_key_Brants prep_to_key_performance cop_key_are nsubj_key_models mark_key_that nn_models_language amod_models_gigantic ccomp_shown_large ccomp_shown_presents ccomp_shown_key aux_shown_have nsubj_shown_researchers advmod_shown_Indeed
P09-2086	D07-1090	o	Either pruning -LRB- Stolcke 1998 Church et al. 2007 -RRB- or lossy randomizing approaches -LRB- Talbot and Brants 2008 -RRB- may result in a compact representation for the application run-time	nn_run-time_application det_run-time_the prep_for_representation_run-time amod_representation_compact det_representation_a prep_in_result_representation aux_result_may nsubj_result_approaches nsubj_result_pruning amod_Talbot_2008 conj_and_Talbot_Brants dep_approaches_Brants dep_approaches_Talbot amod_approaches_randomizing amod_approaches_lossy num_Church_2007 nn_Church_al. nn_Church_et dep_Stolcke_Church dep_Stolcke_1998 conj_or_pruning_approaches appos_pruning_Stolcke preconj_pruning_Either
P09-2086	D07-1090	o	To support distributed computation -LRB- Brants et al. 2007 -RRB- we further split the N-gram data into shards by hash values of the first bigram	amod_bigram_first det_bigram_the prep_of_values_bigram nn_values_hash nn_data_N-gram det_data_the prep_by_split_values prep_into_split_shards dobj_split_data advmod_split_further nsubj_split_we advcl_split_support amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_computation_Brants dep_distributed_computation vmod_support_distributed aux_support_To
P09-2086	D07-1090	o	We implemented an N-gram indexer/estimator using MPI inspired by the MapReduce implementation of N-gram language model indexing/estimation pipeline -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et nn_pipeline_indexing/estimation nn_pipeline_model nn_pipeline_language nn_pipeline_N-gram prep_of_implementation_pipeline nn_implementation_MapReduce det_implementation_the agent_inspired_implementation vmod_MPI_inspired dobj_using_MPI dep_indexer/estimator_Brants vmod_indexer/estimator_using nn_indexer/estimator_N-gram det_indexer/estimator_an dobj_implemented_indexer/estimator nsubj_implemented_We ccomp_``_implemented
W08-0302	D07-1090	o	Phrase-based MT systems are straightforward to train from parallel corpora -LRB- Koehn et al. 2003 -RRB- and like the original IBM models -LRB- Brown et al. 1990 -RRB- benefit from standard language models built on large monolingual target-language corpora -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et dep_corpora_Brants amod_corpora_target-language appos_monolingual_corpora amod_monolingual_large prep_on_built_monolingual vmod_models_built nn_models_language amod_models_standard prep_from_benefit_models amod_Brown_1990 dep_Brown_al. nn_Brown_et nn_models_IBM amod_models_original det_models_the dep_like_Brown pobj_like_models amod_Koehn_2003 dep_Koehn_al. nn_Koehn_et dep_corpora_Koehn amod_corpora_parallel conj_and_train_benefit conj_and_train_like prep_from_train_corpora aux_train_to xcomp_straightforward_benefit xcomp_straightforward_like xcomp_straightforward_train cop_straightforward_are nsubj_straightforward_systems nn_systems_MT amod_systems_Phrase-based
W08-0302	D07-1090	o	The recent emphasis on improving these components of a translation system -LRB- Brants et al. 2007 -RRB- is likely due in part to the widespread availability of NLP tools for the language that is most frequently the target English	det_target_the advmod_target_frequently cop_target_is nsubj_target_that advmod_frequently_most rcmod_language_target det_language_the nn_tools_NLP prep_for_availability_language prep_of_availability_tools amod_availability_widespread det_availability_the prep_to_due_availability prep_in_due_part dep_likely_English prep_likely_due cop_likely_is nsubj_likely_emphasis amod_Brants_2007 dep_Brants_al. nn_Brants_et nn_system_translation det_system_a prep_of_components_system det_components_these dobj_improving_components dep_emphasis_Brants prepc_on_emphasis_improving amod_emphasis_recent det_emphasis_The ccomp_``_likely
W08-0316	D07-1090	o	For our contrast submission we rescore the first-pass translation lattices with a large zero-cutoff stupid-backoff -LRB- Brants et al. 2007 -RRB- language model estimated over approximately five billion words of newswire text	nn_text_newswire prep_of_words_text num_words_billion amod_words_estimated number_billion_five quantmod_billion_approximately quantmod_billion_over dep_model_words nn_model_language dep_model_Brants dep_Brants_2007 dep_Brants_al. nn_Brants_et dep_stupid-backoff_model nn_stupid-backoff_zero-cutoff amod_stupid-backoff_large det_stupid-backoff_a nn_lattices_translation amod_lattices_first-pass det_lattices_the prep_with_rescore_stupid-backoff dobj_rescore_lattices nsubj_rescore_we prep_for_rescore_submission nn_submission_contrast poss_submission_our
W08-0402	D07-1090	o	It is therefore desirable to have dedicated servers to load parts of the LM3 an idea that has been exploited by -LRB- Zhang et al. 2006 Emami et al. 2007 Brants et al. 2007 -RRB-	num_Brants_2007 nn_Brants_al. nn_Brants_et num_Emami_2007 nn_Emami_al. nn_Emami_et dep_Zhang_Brants dep_Zhang_Emami amod_Zhang_2006 dep_Zhang_al. nn_Zhang_et dep_by_Zhang prep_exploited_by auxpass_exploited_been aux_exploited_has nsubjpass_exploited_that rcmod_idea_exploited det_idea_an dep_LM3_idea det_LM3_the prep_of_parts_LM3 dobj_load_parts aux_load_to vmod_dedicated_load dobj_dedicated_servers aux_dedicated_have aux_dedicated_to xcomp_desirable_dedicated advmod_desirable_therefore cop_desirable_is nsubj_desirable_It
W09-0423	D07-1090	o	These findings are somehow surprising since it was eventually believed by the community that adding large amounts of bitexts should improve the translation model as it is usually observed for the language model -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et nn_model_language det_model_the dep_observed_Brants prep_for_observed_model advmod_observed_usually auxpass_observed_is nsubjpass_observed_it mark_observed_as nn_model_translation det_model_the dobj_improve_model aux_improve_should prep_of_amounts_bitexts amod_amounts_large dobj_adding_amounts nsubj_adding_that rcmod_community_adding det_community_the agent_believed_community advmod_believed_eventually auxpass_believed_was nsubjpass_believed_it mark_believed_since advcl_surprising_observed dep_surprising_improve advcl_surprising_believed advmod_surprising_somehow cop_surprising_are nsubj_surprising_findings det_findings_These
W09-1505	D07-1090	o	We have also used TPTs to encode n-gram count databases such as the Google 1T web n-gram database -LRB- Brants and Franz 2006 -RRB- but are not able to provide detailed results within the space limitations of this paper .4 5.1 Perplexity computation with 5-gram language models We compared the performance of TPT-encoded language models against three other language model implementations the SRI language modeling toolkit -LRB- Stolcke 2002 -RRB- IRSTLM -LRB- Federico and Cettolo 2007 -RRB- and the language model implementation currently used in the Portage SMT system -LRB- Badr et al. 2007 -RRB- which uses a pointer-based implementation but is able to perform fast LM filtering at load time	nn_time_load prep_at_filtering_time nsubj_filtering_LM rcmod_fast_filtering dobj_perform_fast aux_perform_to xcomp_able_perform cop_able_is nsubj_able_which amod_implementation_pointer-based det_implementation_a conj_but_uses_able dobj_uses_implementation nsubj_uses_which amod_Badr_2007 dep_Badr_al. nn_Badr_et nn_system_SMT nn_system_Portage det_system_the prep_in_used_system advmod_used_currently dep_implementation_Badr vmod_implementation_used nn_implementation_model nn_implementation_language det_implementation_the dep_Federico_2007 conj_and_Federico_Cettolo appos_IRSTLM_Cettolo appos_IRSTLM_Federico amod_Stolcke_2002 rcmod_toolkit_able rcmod_toolkit_uses conj_and_toolkit_implementation conj_and_toolkit_IRSTLM dep_toolkit_Stolcke nn_toolkit_modeling nn_toolkit_language nn_toolkit_SRI det_toolkit_the nn_implementations_model nn_implementations_language amod_implementations_other num_implementations_three nn_models_language amod_models_TPT-encoded dep_performance_implementation dep_performance_IRSTLM dep_performance_toolkit prep_against_performance_implementations prep_of_performance_models det_performance_the dobj_compared_performance nsubj_compared_We nn_models_language amod_models_5-gram nn_computation_Perplexity num_computation_5.1 nn_computation_.4 nn_computation_paper det_computation_this prep_with_limitations_models prep_of_limitations_computation nn_limitations_space det_limitations_the prep_within_results_limitations amod_results_detailed dobj_provide_results aux_provide_to dep_able_compared xcomp_able_provide neg_able_not cop_able_are nsubj_able_We dep_Brants_2006 conj_and_Brants_Franz appos_database_Franz appos_database_Brants nn_database_n-gram nn_database_web nn_database_1T nn_database_Google det_database_the prep_such_as_databases_database nn_databases_count nn_databases_n-gram dobj_encode_databases aux_encode_to conj_but_used_able xcomp_used_encode dobj_used_TPTs advmod_used_also aux_used_have nsubj_used_We ccomp_``_able ccomp_``_used
W09-2012	D07-1090	o	In this study we use the Google Web 1T 5gram Corpus -LRB- Brants et al. 2007 -RRB-	amod_Brants_2007 dep_Brants_al. nn_Brants_et nn_Corpus_5gram nn_Corpus_1T nn_Corpus_Web nn_Corpus_Google det_Corpus_the dep_use_Brants dobj_use_Corpus nsubj_use_we prep_in_use_study det_study_this
C08-1127	D07-1091	o	2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems either from the target side -LRB- Marcu et al. 2006 Hassan et al. 2007 Zollmann and Venugopal 2006 -RRB- the source side -LRB- Quirk et al. 2005 Liu et al. 2006 Huang et al. 2006 -RRB- or both sides -LRB- Eisner 2003 Ding et al. 2005 Koehn and Hoang 2007 -RRB- just to name a few	det_few_a dobj_name_few aux_name_to advmod_name_just dep_Koehn_2007 conj_and_Koehn_Hoang num_Ding_2005 nn_Ding_al. nn_Ding_et dep_Eisner_Hoang dep_Eisner_Koehn dep_Eisner_Ding dep_Eisner_2003 det_sides_both num_Huang_2006 nn_Huang_al. nn_Huang_et num_Liu_2006 nn_Liu_al. nn_Liu_et dep_Quirk_Huang conj_Quirk_Liu appos_Quirk_2005 dep_Quirk_al. nn_Quirk_et dep_side_Quirk nn_side_source det_side_the dep_Zollmann_2006 conj_and_Zollmann_Venugopal vmod_Hassan_name appos_Hassan_Eisner conj_or_Hassan_sides conj_or_Hassan_side dep_Hassan_Venugopal dep_Hassan_Zollmann num_Hassan_2007 nn_Hassan_al. nn_Hassan_et dep_Marcu_sides dep_Marcu_side dep_Marcu_Hassan appos_Marcu_2006 dep_Marcu_al. nn_Marcu_et nn_side_target det_side_the nn_systems_SMT amod_knowledge_linguistic prep_from_integrate_side preconj_integrate_either prep_into_integrate_systems dobj_integrate_knowledge aux_integrate_to dep_efforts_Marcu vmod_efforts_integrate amod_efforts_various cop_efforts_been aux_efforts_have expl_efforts_There rcmod_Work_efforts amod_Work_Related num_Work_2
D07-1049	D07-1091	o	Decoding is carried-out using the Moses decoder -LRB- Koehn and Hoang 2007 -RRB-	amod_Koehn_2007 conj_and_Koehn_Hoang dep_decoder_Hoang dep_decoder_Koehn nn_decoder_Moses det_decoder_the dobj_using_decoder xcomp_carried-out_using cop_carried-out_is nsubj_carried-out_Decoding
D09-1008	D07-1091	o	In -LRB- Koehn and Hoang 2007 -RRB- shallow syntactic analysis such as POS tagging and morphological analysis were incorporated in a phrasal decoder	amod_decoder_phrasal det_decoder_a prep_in_incorporated_decoder auxpass_incorporated_were nsubjpass_incorporated_analysis prep_incorporated_In amod_analysis_morphological conj_and_tagging_analysis nn_tagging_POS prep_such_as_analysis_analysis prep_such_as_analysis_tagging nn_analysis_syntactic amod_analysis_shallow dep_Koehn_2007 conj_and_Koehn_Hoang dep_In_Hoang dep_In_Koehn
D09-1079	D07-1091	o	5 SMT Experiments 5.1 Experimental Setup We used publicly available resources for all our tests for decoding we used Moses -LRB- Koehn and Hoang 2007 -RRB- and our parallel data was taken from the Spanish-English section of Europarl	prep_of_section_Europarl amod_section_Spanish-English det_section_the prep_from_taken_section auxpass_taken_was prepc_for_taken_decoding nsubjpass_taken_Setup amod_data_parallel poss_data_our dep_Koehn_2007 conj_and_Koehn_Hoang conj_and_Moses_data appos_Moses_Hoang appos_Moses_Koehn dobj_used_data dobj_used_Moses nsubj_used_we ccomp_decoding_used poss_tests_our predet_tests_all amod_resources_available prep_for_used_tests dobj_used_resources advmod_used_publicly nsubj_used_We rcmod_Setup_used amod_Setup_Experimental num_Setup_5.1 rcmod_Experiments_taken nn_Experiments_SMT num_Experiments_5
E09-1011	D07-1091	o	Koehn and Hoang -LRB- 2007 -RRB- propose Factored Translation Models which extend phrase-based statistical machine translation by allowing the integration of additional morphological features at the word level	nn_level_word det_level_the amod_features_morphological amod_features_additional prep_at_integration_level prep_of_integration_features det_integration_the dobj_allowing_integration nn_translation_machine amod_translation_statistical amod_translation_phrase-based prepc_by_extend_allowing dobj_extend_translation nsubj_extend_which rcmod_Models_extend nn_Models_Translation nn_Models_Factored dobj_propose_Models nsubj_propose_Hoang nsubj_propose_Koehn appos_Hoang_2007 conj_and_Koehn_Hoang
E09-1043	D07-1091	o	c2009 Association for Computational Linguistics Improving Mid-Range Reordering using Templates of Factors Hieu Hoang School of Informatics University of Edinburgh h.hoang@sms.ed.ac.uk Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk Abstract We extend the factored translation model -LRB- Koehn and Hoang 2007 -RRB- to allow translations of longer phrases composed of factors such as POS and morphological tags to act as templates for the selection and reordering of surface phrase translation	nn_translation_phrase nn_translation_surface prep_of_selection_translation conj_and_selection_reordering det_selection_the prep_for_templates_reordering prep_for_templates_selection prep_as_act_templates aux_act_to vmod_tags_act nn_tags_morphological nn_tags_POS conj_and_POS_morphological prep_such_as_factors_tags prep_of_composed_factors vmod_phrases_composed amod_phrases_longer prep_of_translations_phrases dobj_allow_translations aux_allow_to dep_Koehn_2007 conj_and_Koehn_Hoang dep_model_Hoang dep_model_Koehn nn_model_translation amod_model_factored det_model_the vmod_extend_allow dobj_extend_model nsubj_extend_We nn_Abstract_pkoehn@inf.ed.ac.uk nn_Abstract_Edinburgh prep_of_University_Abstract nn_University_Informatics prep_of_School_University nn_School_Koehn nn_School_Philipp nn_School_h.hoang@sms.ed.ac.uk nn_School_Edinburgh prep_of_University_School nn_University_Informatics prep_of_School_University nn_School_Hoang nn_School_Hieu nn_School_Factors prep_of_Templates_School ccomp_using_extend dobj_using_Templates nn_Reordering_Mid-Range nn_Reordering_Improving nn_Reordering_Linguistics nn_Reordering_Computational vmod_Association_using prep_for_Association_Reordering nn_Association_c2009
E09-1043	D07-1091	o	2.4 Factor Model Decomposition Factored translation models -LRB- Koehn and Hoang 2007 -RRB- extend the phrase-based model by integrating word level factors into the decoding process	nn_process_decoding det_process_the nn_factors_level nn_factors_word prep_into_integrating_process dobj_integrating_factors amod_model_phrase-based det_model_the prepc_by_extend_integrating dobj_extend_model nsubj_extend_models amod_Koehn_2007 conj_and_Koehn_Hoang appos_models_Hoang appos_models_Koehn nn_models_translation nn_models_Factored nn_models_Decomposition nn_models_Model nn_models_Factor num_models_2.4
E09-1043	D07-1091	o	-LRB- Koehn and Hoang 2007 -RRB- describes various strategies for the decomposition of the decoding into multiple translation models using the Moses decoder	nn_decoder_Moses det_decoder_the dobj_using_decoder nn_models_translation amod_models_multiple xcomp_decoding_using prep_into_decoding_models det_decoding_the prepc_of_decomposition_decoding det_decomposition_the prep_for_strategies_decomposition amod_strategies_various dobj_describes_strategies nsubj_describes_Hoang nsubj_describes_Koehn amod_Koehn_2007 conj_and_Koehn_Hoang
E09-1043	D07-1091	o	4.1 Training The training procedure is identical to the factored phrase-based training described in -LRB- Koehn and Hoang 2007 -RRB-	amod_Koehn_2007 conj_and_Koehn_Hoang dep_in_Hoang dep_in_Koehn prep_described_in vmod_training_described amod_training_phrase-based amod_training_factored det_training_the prep_to_identical_training cop_identical_is nsubj_identical_procedure nn_procedure_training det_procedure_The amod_procedure_Training num_procedure_4.1
E09-3008	D07-1091	o	In a factored translation model other factors than surface form can be used such as lemma or part-of-speech -LRB- Koehn and Hoang 2007 -RRB-	amod_Koehn_2007 conj_and_Koehn_Hoang dep_part-of-speech_Hoang dep_part-of-speech_Koehn conj_or_lemma_part-of-speech prep_such_as_used_part-of-speech prep_such_as_used_lemma auxpass_used_be aux_used_can nsubjpass_used_factors nn_form_surface prep_than_factors_form amod_factors_other rcmod_model_used nn_model_translation amod_model_factored det_model_a pobj_In_model dep_``_In
I08-1067	D07-1091	o	Recent work by Koehn and Hoang -LRB- 2007 -RRB- pro514 poses factored translation models that combine feature functions to handle syntactic morphological and other linguistic information in a log-linear model	amod_model_log-linear det_model_a prep_in_information_model amod_information_linguistic amod_information_other conj_and_syntactic_information conj_and_syntactic_morphological dobj_handle_information dobj_handle_morphological dobj_handle_syntactic aux_handle_to nn_functions_feature xcomp_combine_handle dobj_combine_functions nsubj_combine_that rcmod_models_combine nn_models_translation amod_models_factored dobj_poses_models nsubj_poses_work appos_Hoang_2007 dep_Koehn_pro514 conj_and_Koehn_Hoang prep_by_work_Hoang prep_by_work_Koehn amod_work_Recent
N09-1021	D07-1091	o	In particular we adopt the approach of phrase-based statistical machine translation -LRB- Koehn et al. 2003 Koehn and Hoang 2007 -RRB-	dep_Koehn_2007 conj_and_Koehn_Hoang dep_Koehn_Hoang dep_Koehn_Koehn appos_Koehn_2003 dep_Koehn_al. nn_Koehn_et nn_translation_machine amod_translation_statistical amod_translation_phrase-based prep_of_approach_translation det_approach_the dep_adopt_Koehn dobj_adopt_approach nsubj_adopt_we prep_in_adopt_particular
N09-1021	D07-1091	o	The reader is referred to -LRB- Koehn and Hoang 2007 Koehn et al. 2007 -RRB- for detailed information about phrase-based statistical machine translation	nn_translation_machine amod_translation_statistical amod_translation_phrase-based prep_about_information_translation amod_information_detailed num_Koehn_2007 nn_Koehn_al. nn_Koehn_et dep_Koehn_Koehn conj_and_Koehn_2007 conj_and_Koehn_Hoang prep_for_referred_information prep_to_referred_2007 prep_to_referred_Hoang prep_to_referred_Koehn auxpass_referred_is nsubjpass_referred_reader det_reader_The
N09-1058	D07-1091	o	The publicly available Moses4 decoder is used for training and decoding -LRB- Koehn and Hoang 2007 -RRB-	dep_Koehn_2007 conj_and_Koehn_Hoang dep_training_Hoang dep_training_Koehn conj_and_training_decoding prep_for_used_decoding prep_for_used_training auxpass_used_is nsubjpass_used_decoder nn_decoder_Moses4 amod_decoder_available det_decoder_The advmod_available_publicly
N09-2019	D07-1091	o	Factored models are introduced in -LRB- Koehn and Hoang 2007 -RRB- for better integration of morphosyntactic information	amod_information_morphosyntactic prep_of_integration_information amod_integration_better dep_Koehn_2007 conj_and_Koehn_Hoang prep_for_introduced_integration prep_in_introduced_Hoang prep_in_introduced_Koehn auxpass_introduced_are nsubjpass_introduced_models amod_models_Factored
P07-1065	D07-1091	o	Decoding is carried-out using the Moses decoder -LRB- Koehn and Hoang 2007 -RRB-	amod_Koehn_2007 conj_and_Koehn_Hoang dep_decoder_Hoang dep_decoder_Koehn nn_decoder_Moses det_decoder_the dobj_using_decoder xcomp_carried-out_using cop_carried-out_is nsubj_carried-out_Decoding
P07-2045	D07-1091	o	Initial results show the potential benefit of factors for statistical machine translation -LRB- Koehn et al. 2006 -RRB- and -LRB- Koehn and Hoang 2007 -RRB-	num_Hoang_2007 conj_and_Koehn_Hoang dep_2006_al. nn_al._et num_Koehn_2006 nn_translation_machine amod_translation_statistical prep_for_benefit_translation prep_of_benefit_factors amod_benefit_potential det_benefit_the conj_and_show_Hoang conj_and_show_Koehn dep_show_Koehn dobj_show_benefit nsubj_show_results amod_results_Initial
P08-1059	D07-1091	o	In recent work Koehn and Hoang -LRB- 2007 -RRB- proposed a general framework for including morphological features in a phrase-based SMT system by factoring the representation of words into a vector of morphological features and allowing a phrase-based MT system to work on any of the factored representations which is implemented in the Moses system	nn_system_Moses det_system_the prep_in_implemented_system auxpass_implemented_is nsubjpass_implemented_which rcmod_representations_implemented amod_representations_factored det_representations_the prep_of_any_representations prep_on_work_any aux_work_to nn_system_MT amod_system_phrase-based det_system_a xcomp_allowing_work dobj_allowing_system amod_features_morphological prep_of_vector_features det_vector_a prep_of_representation_words det_representation_the conj_and_factoring_allowing prep_into_factoring_vector dobj_factoring_representation nn_system_SMT amod_system_phrase-based det_system_a prep_in_features_system amod_features_morphological pobj_including_features prepc_for_framework_including amod_framework_general det_framework_a prepc_by_proposed_allowing prepc_by_proposed_factoring dobj_proposed_framework nsubj_proposed_Hoang nsubj_proposed_Koehn prep_in_proposed_work appos_Hoang_2007 conj_and_Koehn_Hoang amod_work_recent
P08-1059	D07-1091	o	Though our motivation is similar to that of Koehn and Hoang -LRB- 2007 -RRB- we chose to build an independent component for inflection prediction in isolation rather than folding morphological information into the main translation model	nn_model_translation amod_model_main det_model_the prep_into_information_model amod_information_morphological amod_information_folding prep_in_prediction_isolation nn_prediction_inflection conj_negcc_for_information pobj_for_prediction amod_component_independent det_component_an prep_build_information prep_build_for dobj_build_component aux_build_to xcomp_chose_build nsubj_chose_we advcl_chose_similar appos_Hoang_2007 conj_and_Koehn_Hoang prep_of_that_Hoang prep_of_that_Koehn prep_to_similar_that cop_similar_is nsubj_similar_motivation mark_similar_Though poss_motivation_our
P08-1087	D07-1091	o	In their presentation of the factored SMT models Koehn and Hoang -LRB- 2007 -RRB- describe experiments for translating from English to German Spanish and Czech using morphology tags added on the morphologically rich side along with POS tags	nn_tags_POS amod_side_rich det_side_the advmod_rich_morphologically pobj_added_tags prepc_along_with_added_with prep_on_added_side nn_tags_morphology dep_using_added dobj_using_tags conj_and_German_Czech conj_and_German_Spanish prep_to_translating_Czech prep_to_translating_Spanish prep_to_translating_German prep_from_translating_English vmod_describe_using prepc_for_describe_translating dobj_describe_experiments prep_in_describe_presentation appos_Hoang_2007 conj_and_Koehn_Hoang appos_models_Hoang appos_models_Koehn nn_models_SMT amod_models_factored det_models_the prep_of_presentation_models poss_presentation_their
P08-1087	D07-1091	o	The model is defined mathematically -LRB- Koehn and Hoang 2007 -RRB- as following p -LRB- f | e -RRB- = 1Zexp nsummationdisplay i = 1 ihi -LRB- f e -RRB- -LRB- 1 -RRB- where i is a vector of weights determined during a tuning process and hi is the feature function	nn_function_feature det_function_the cop_function_is nsubj_function_hi nsubj_function_= dep_function_e nn_function_p nn_process_tuning det_process_a prep_during_determined_process vmod_vector_determined prep_of_vector_weights det_vector_a cop_vector_is nsubj_vector_i advmod_vector_where dep_f_e appos_ihi_f num_ihi_1 dep_=_vector dep_=_1 dep_=_ihi dep_i_= dep_nsummationdisplay_i amod_1Zexp_nsummationdisplay conj_and_=_hi dobj_=_1Zexp advmod_e_| nn_|_f dep_following_function amod_Koehn_2007 conj_and_Koehn_Hoang dep_mathematically_Hoang dep_mathematically_Koehn prepc_as_defined_following advmod_defined_mathematically auxpass_defined_is nsubjpass_defined_model det_model_The ccomp_``_defined
P08-1114	D07-1091	o	Chiang -LRB- 2005 -RRB- distinguishes statistical MT approaches that are syntactic in a formal sense going beyond the nite-state underpinnings of phrasebased models from approaches that are syntactic in a linguistic sense i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking .1 The two forms of syntactic modeling are doubly dissociable current research frameworks include systems that are nite state but informed by linguistic annotation prior to training -LRB- e.g. -LRB- Koehn and Hoang 2007 Birch et al. 2007 Hassan et al. 2007 -RRB- -RRB- and also include systems employing contextfree models trained on parallel text without bene t of any prior linguistic analysis -LRB- e.g.	amod_analysis_linguistic amod_analysis_prior det_analysis_any prep_of_t_analysis nn_t_bene dep_text_e.g. prep_without_text_t amod_text_parallel prep_on_trained_text vmod_models_trained amod_models_contextfree dobj_employing_models vmod_systems_employing dobj_include_systems num_Hassan_2007 nn_Hassan_al. nn_Hassan_et num_Birch_2007 nn_Birch_al. nn_Birch_et dep_Koehn_Hassan conj_and_Koehn_Birch conj_and_Koehn_2007 conj_and_Koehn_Hoang dep_e.g._include conj_and_e.g._also appos_e.g._Birch appos_e.g._2007 appos_e.g._Hoang appos_e.g._Koehn dep_training_also dep_training_e.g. amod_annotation_linguistic prep_by_informed_annotation nsubj_informed_that prep_prior_to_state_training conj_but_state_informed amod_state_nite cop_state_are nsubj_state_that rcmod_systems_informed rcmod_systems_state dobj_include_systems nsubj_include_frameworks nn_frameworks_research amod_frameworks_current parataxis_dissociable_include advmod_dissociable_doubly cop_dissociable_are csubj_dissociable_taking amod_modeling_syntactic num_forms_two det_forms_The num_forms_.1 nn_forms_treebanking prep_of_analysis_modeling conj_or_analysis_forms amod_analysis_linguistic amod_analysis_human prep_from_derived_forms prep_from_derived_analysis vmod_annotations_derived prep_of_form_annotations det_form_the prep_in_knowledge_form nn_knowledge_language nn_knowledge_priori det_knowledge_a prep_of_advantage_knowledge dobj_taking_advantage advmod_taking_i.e. amod_sense_linguistic det_sense_a prep_in_syntactic_sense cop_syntactic_are nsubj_syntactic_that rcmod_approaches_syntactic amod_models_phrasebased prep_of_underpinnings_models amod_underpinnings_nite-state det_underpinnings_the parataxis_going_dissociable prep_from_going_approaches prep_beyond_going_underpinnings amod_sense_formal det_sense_a prep_in_syntactic_sense cop_syntactic_are nsubj_syntactic_that rcmod_approaches_syntactic nn_approaches_MT amod_approaches_statistical dep_distinguishes_going dobj_distinguishes_approaches nsubj_distinguishes_Chiang appos_Chiang_2005
P08-2039	D07-1091	o	We also report on applying Factored Translation Models -LRB- Koehn and Hoang 2007 -RRB- for English-to-Arabic translation	amod_translation_English-to-Arabic dep_Koehn_2007 conj_and_Koehn_Hoang appos_Models_Hoang appos_Models_Koehn nn_Models_Translation nn_Models_Factored prep_for_applying_translation dobj_applying_Models prepc_on_report_applying advmod_report_also nsubj_report_We
P08-2039	D07-1091	o	Koehn and Hoang -LRB- 2007 -RRB- present Factored Translation Models as an extension to phrase-based statistical machine translation models	nn_models_translation nn_models_machine amod_models_statistical amod_models_phrase-based prep_to_extension_models det_extension_an prep_as_Models_extension nn_Models_Translation nn_Models_Factored amod_Models_present nn_Models_Hoang nn_Models_Koehn appos_Hoang_2007 conj_and_Koehn_Hoang
P09-1090	D07-1091	o	Koehn and Hoang -LRB- 2007 -RRB- propose factored translation models that combine feature functions to handle syntactic morphological and other linguistic information in a log-linear model	amod_model_log-linear det_model_a prep_in_information_model amod_information_linguistic amod_information_other conj_and_syntactic_information conj_and_syntactic_morphological dobj_handle_information dobj_handle_morphological dobj_handle_syntactic aux_handle_to nn_functions_feature xcomp_combine_handle dobj_combine_functions nsubj_combine_that rcmod_models_combine nn_models_translation amod_models_factored dobj_propose_models nsubj_propose_Hoang nsubj_propose_Koehn appos_Hoang_2007 conj_and_Koehn_Hoang
W08-0305	D07-1091	o	Any way to enforce linguistic constraints will result in a reduced need for data and ultimately in more complete models given the same amount of data -LRB- Koehn and Hoang 2007 -RRB-	amod_Koehn_2007 conj_and_Koehn_Hoang dep_data_Hoang dep_data_Koehn prep_of_amount_data amod_amount_same det_amount_the pobj_given_amount ccomp_,_given amod_models_complete amod_models_more prep_in_ultimately_models prep_for_need_data amod_need_reduced det_need_a conj_and_result_ultimately prep_in_result_need aux_result_will nsubj_result_way amod_constraints_linguistic dobj_enforce_constraints aux_enforce_to vmod_way_enforce det_way_Any
W08-0310	D07-1091	o	4.1 Overview In this work factored models -LRB- Koehn and Hoang 2007 -RRB- are experimented with three factors the surface form the lemma and the part of speech -LRB- POS -RRB-	appos_speech_POS prep_of_part_speech det_part_the det_lemma_the conj_and_form_part conj_and_form_lemma nn_form_surface det_form_the num_factors_three dep_experimented_part dep_experimented_lemma dep_experimented_form prep_with_experimented_factors auxpass_experimented_are nsubjpass_experimented_Overview amod_Koehn_2007 conj_and_Koehn_Hoang dep_models_Hoang dep_models_Koehn amod_models_factored det_work_this appos_Overview_models prep_in_Overview_work num_Overview_4.1
W08-0310	D07-1091	o	Therefore including a model based on surface forms as suggested -LRB- Koehn and Hoang 2007 -RRB- is also necessary	advmod_necessary_also cop_necessary_is parataxis_necessary_suggested prep_including_necessary_model advmod_necessary_Therefore dep_Koehn_2007 conj_and_Koehn_Hoang dep_suggested_Hoang dep_suggested_Koehn mark_suggested_as nn_forms_surface prep_on_based_forms vmod_model_based det_model_a
W08-0318	D07-1091	o	+ truecase 20.7 -LRB- +0.4 -RRB- 27.8 -LRB- +0.2 -RRB- Table 2 Impact of truecasing on case-sensitive BLEU In a more integrated approach factored translation models -LRB- Koehn and Hoang 2007 -RRB- allow us to consider grammatical coherence in form of partof-speech language models	nn_models_language amod_models_partof-speech prep_of_form_models amod_coherence_grammatical prep_in_consider_form dobj_consider_coherence aux_consider_to xcomp_allow_consider dobj_allow_us nsubj_allow_27.8 amod_Koehn_2007 conj_and_Koehn_Hoang dep_models_Hoang dep_models_Koehn nn_models_translation amod_models_factored amod_approach_integrated det_approach_a advmod_integrated_more amod_BLEU_case-sensitive prep_in_truecasing_approach prep_on_truecasing_BLEU prepc_of_Impact_truecasing appos_27.8_models dep_27.8_Impact dep_27.8_2 dep_27.8_Table dep_27.8_+0.2 num_27.8_+0.4 rcmod_20.7_allow amod_20.7_truecase cc_20.7_+
W08-0319	D07-1091	o	3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features .3 We can consider the attributes as individual factors -LRB- Koehn and Hoang 2007 -RRB-	dep_Koehn_2007 conj_and_Koehn_Hoang dep_factors_Hoang dep_factors_Koehn amod_factors_individual prep_as_attributes_factors det_attributes_the dobj_consider_attributes aux_consider_can nsubj_consider_We rcmod_.3_consider dep_features_.3 amod_features_linguistic amod_features_various dobj_representing_features vmod_attributes_representing num_attributes_20 quantmod_20_than mwe_than_more prep_of_consist_attributes nsubj_consist_Labels conj_but_atomic_consist neg_atomic_not cop_atomic_are nsubj_atomic_Labels det_t-layer_the prep_at_Labels_t-layer prep_of_Labels_nodes nn_Labels_Translation nn_Labels_Treelet nn_Labels_Factored num_Labels_3.2.1
W08-0319	D07-1091	o	In order to generate a value for each target-side factor we use a sequence of mapping steps similar to Koehn and Hoang -LRB- 2007 -RRB-	appos_Hoang_2007 conj_and_Koehn_Hoang prep_to_similar_Hoang prep_to_similar_Koehn amod_steps_similar dobj_mapping_steps prepc_of_sequence_mapping det_sequence_a dobj_use_sequence nsubj_use_we advcl_use_generate nn_factor_target-side det_factor_each prep_for_value_factor det_value_a dobj_generate_value aux_generate_to dep_generate_order mark_generate_In
W08-0322	D07-1091	p	Furthermore the BLEU score performance suggests that our model is not very powerful but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses -LRB- Koehn and Hoang 2007 -RRB- based on various evaluation metrics including BLEU score NIST score -LRB- Doddington 2002 -RRB- METEOR -LRB- Banerjee and Lavie 2005 -RRB- TER -LRB- Snover et al. 2006 -RRB- WER and PER	amod_Snover_2006 dep_Snover_al. nn_Snover_et dep_TER_Snover dep_Banerjee_2005 conj_and_Banerjee_Lavie appos_METEOR_Lavie appos_METEOR_Banerjee amod_Doddington_2002 dep_score_Doddington nn_score_NIST conj_and_score_PER conj_and_score_WER conj_and_score_TER appos_score_METEOR appos_score_score nn_score_BLEU prep_including_metrics_PER prep_including_metrics_WER prep_including_metrics_TER prep_including_metrics_score nn_metrics_evaluation amod_metrics_various dep_Koehn_2007 conj_and_Koehn_Hoang pobj_Moses_metrics prepc_based_on_Moses_on dep_Moses_Hoang dep_Moses_Koehn dep_system_Moses amod_system_state-of-the-art det_system_a prep_to_model_system nn_model_language amod_model_5-gram det_model_a prep_with_method_model poss_method_our dobj_compare_method nsubj_compare_we advmod_compare_when num_Table_3 advcl_found_compare prep_in_found_Table auxpass_found_be aux_found_can nsubjpass_found_hints amod_hints_interesting det_hints_some conj_but_powerful_found advmod_powerful_very neg_powerful_not cop_powerful_is nsubj_powerful_model mark_powerful_that poss_model_our ccomp_suggests_found ccomp_suggests_powerful nsubj_suggests_performance advmod_suggests_Furthermore nn_performance_score nn_performance_BLEU det_performance_the
W08-0410	D07-1091	p	For example factored translation models -LRB- Koehn and Hoang 2007 -RRB- retain the simplicity of phrase-based SMT while adding the ability to incorporate additional features	amod_features_additional dobj_incorporate_features aux_incorporate_to vmod_ability_incorporate det_ability_the dobj_adding_ability mark_adding_while amod_SMT_phrase-based prep_of_simplicity_SMT det_simplicity_the advcl_retain_adding dobj_retain_simplicity nsubj_retain_models prep_for_retain_example dep_Koehn_2007 conj_and_Koehn_Hoang appos_models_Hoang appos_models_Koehn nn_models_translation amod_models_factored
W08-0510	D07-1091	o	Some research into factored machine translation has been published by -LRB- Koehn and Hoang 2007 -RRB-	num_Hoang_2007 conj_and_Koehn_Hoang agent_published_Hoang agent_published_Koehn auxpass_published_been aux_published_has nsubjpass_published_research nn_translation_machine amod_translation_factored prep_into_research_translation det_research_Some
W08-2119	D07-1091	o	We believe that other kinds of translationunit such as n-gram -LRB- Jos et al. 2006 -RRB- factoredphrasaltranslation -LRB- Koehn and Hoang 2007 -RRB- or treelet -LRB- Quirk et al. 2005 -RRB- can be used in this method	det_method_this prep_in_used_method auxpass_used_be aux_used_can nsubjpass_used_kinds mark_used_that amod_Quirk_2005 dep_Quirk_al. nn_Quirk_et dep_Koehn_2007 conj_and_Koehn_Hoang appos_factoredphrasaltranslation_Hoang appos_factoredphrasaltranslation_Koehn amod_Jos_2006 dep_Jos_al. nn_Jos_et conj_or_n-gram_treelet conj_or_n-gram_factoredphrasaltranslation dep_n-gram_Jos prep_such_as_translationunit_treelet prep_such_as_translationunit_factoredphrasaltranslation prep_such_as_translationunit_n-gram dep_kinds_Quirk prep_of_kinds_translationunit amod_kinds_other ccomp_believe_used nsubj_believe_We ccomp_``_believe
W09-0418	D07-1091	o	A tight integration of morphosyntactic information into the translation model was proposed by -LRB- Koehn and Hoang 2007 -RRB- where lemma and morphological information are translated separately and this information is combined on the output side to generate the translation	det_translation_the dobj_generate_translation aux_generate_to nn_side_output det_side_the xcomp_combined_generate prep_on_combined_side auxpass_combined_is nsubjpass_combined_information det_information_this conj_and_translated_combined advmod_translated_separately auxpass_translated_are nsubjpass_translated_information nsubjpass_translated_lemma advmod_translated_where amod_information_morphological conj_and_lemma_information rcmod_Koehn_combined rcmod_Koehn_translated dep_Koehn_2007 conj_and_Koehn_Hoang agent_proposed_Hoang agent_proposed_Koehn auxpass_proposed_was nsubjpass_proposed_integration nn_model_translation det_model_the amod_information_morphosyntactic prep_into_integration_model prep_of_integration_information amod_integration_tight det_integration_A ccomp_``_proposed
W09-0427	D07-1091	o	Unlike with factored models -LRB- Koehn and Hoang 2007 -RRB- or additional translation lexicons -LRB- Schwenk et al. 2008 -RRB- we do not generate the surface form back from the lemma translation which means that tense gender and number information are 151 news-dev2009a representation OOV % METEOR BLEU NIST baseline surface form only 2.24 49.05 20.45 6.135 decoding lemma backoff 2.13 49.12 20.44 6.143 word alignment lemma + POS for all 2.24 48.87 20.36 6.145 lemma + POS for adj 2.25 48.94 20.46 6.131 lemma + POS for verbs 2.21 49.05 20.47 6.137 decoding + alignment backoff + all 2.10 48.97 20.36 6.147 backoff + adj 2.12 49.05 20.48 6.140 backoff + verbs 2.08 49.15 20.50 6.148 news-dev2009b representation OOV % METEOR BLEU NIST baseline surface form only 2.52 49.60 21.10 6.211 decoding lemma backoff 2.43 49.66 21.02 6.210 word alignment lemma + POS for all 2.53 49.56 21.03 6.199 lemma + POS for adj 2.52 49.74 21.00 6.213 lemma + POS for verbs 2.47 49.73 21.10 6.217 decoding + alignment backoff + all 2.44 49.59 20.92 6.194 backoff + adj 2.43 49.80 21.03 6.217 backoff + verbs 2.39 49.80 21.03 6.217 Table 2 Evaluation of the decoding backoff strategy the modified word alignment strategy and their combination Input Meme sil demissionnait la situation ne changerait pas	nn_pas_changerait nn_pas_ne nn_pas_situation det_pas_la nn_demissionnait_sil nn_demissionnait_Meme nn_demissionnait_Input nn_demissionnait_combination poss_demissionnait_their appos_strategy_pas conj_and_strategy_demissionnait nn_strategy_alignment nn_strategy_word amod_strategy_modified det_strategy_the num_strategy_49.60 nn_strategy_backoff amod_strategy_decoding det_strategy_the prep_of_Evaluation_strategy num_Table_2 num_Table_6.217 num_Table_21.03 dep_49.80_Table dep_2.39_49.80 dep_backoff_2.39 conj_+_backoff_verbs dep_backoff_6.217 number_6.217_21.03 dep_49.80_verbs dep_49.80_backoff number_49.80_2.43 dep_backoff_49.80 conj_+_backoff_adj dep_6.194_Evaluation dep_6.194_adj dep_6.194_backoff number_6.194_20.92 dep_6.194_49.59 number_49.59_2.44 dep_all_6.194 conj_+_backoff_all dep_decoding_all dep_decoding_backoff conj_+_decoding_alignment num_decoding_6.217 dep_21.10_alignment dep_21.10_decoding number_21.10_49.73 number_21.10_2.47 dep_verbs_21.10 conj_+_lemma_POS dep_6.213_POS dep_6.213_lemma number_6.213_21.00 dep_6.213_49.74 number_49.74_2.52 prep_for_adj_verbs dep_adj_6.213 conj_+_lemma_POS prep_for_6.199_adj dep_6.199_POS dep_6.199_lemma number_6.199_21.03 dep_6.199_49.56 number_49.56_2.53 dep_all_6.199 conj_+_lemma_POS dep_alignment_POS dep_alignment_lemma nn_alignment_word num_alignment_6.210 dep_6.210_21.02 number_6.210_2.43 number_21.02_49.66 dep_backoff_alignment nn_backoff_lemma nn_backoff_decoding dep_backoff_6.211 number_6.211_21.10 prep_for_49.60_all dep_49.60_backoff number_49.60_2.52 quantmod_49.60_only dep_form_demissionnait dep_form_strategy nn_form_surface nn_form_baseline nn_form_NIST nn_form_BLEU nn_form_METEOR amod_form_% nn_form_representation nn_form_backoff number_%_OOV nn_representation_news-dev2009b dep_representation_6.148 dep_representation_49.15 dep_representation_verbs dep_representation_backoff dep_representation_6.140 dep_representation_adj dep_representation_backoff dep_representation_6.147 dep_representation_all number_6.148_20.50 number_49.15_2.08 conj_+_backoff_verbs number_6.140_20.48 dep_6.140_49.05 number_49.05_2.12 conj_+_backoff_adj number_6.147_20.36 dep_6.147_48.97 number_48.97_2.10 conj_+_backoff_representation dep_decoding_form conj_+_decoding_alignment num_decoding_6.137 dep_20.47_alignment dep_20.47_decoding number_20.47_49.05 number_20.47_2.21 dep_verbs_20.47 conj_+_lemma_POS dep_6.131_POS dep_6.131_lemma number_6.131_20.46 dep_6.131_48.94 number_48.94_2.25 prep_for_adj_verbs dep_adj_6.131 conj_+_lemma_POS prep_for_6.145_adj dep_6.145_POS dep_6.145_lemma number_6.145_20.36 dep_6.145_48.87 number_48.87_2.24 dep_all_6.145 conj_+_lemma_POS dep_alignment_POS dep_alignment_lemma nn_alignment_word num_alignment_6.143 dep_6.143_20.44 number_6.143_2.13 number_20.44_49.12 dep_backoff_alignment nn_backoff_lemma nn_backoff_decoding dep_backoff_6.135 number_6.135_20.45 prep_for_49.05_all dep_49.05_backoff number_49.05_2.24 quantmod_49.05_only dep_form_49.05 nn_form_surface nn_form_baseline nn_form_NIST nn_form_BLEU nn_form_METEOR dep_%_form num_%_OOV dep_representation_% amod_representation_news-dev2009a num_representation_151 cop_representation_are nsubj_representation_information advmod_representation_tense mark_representation_that nn_information_number nn_information_gender conj_and_gender_number ccomp_means_representation nsubj_means_which rcmod_translation_means nn_translation_lemma det_translation_the prep_from_back_translation nn_form_surface det_form_the advmod_generate_back dobj_generate_form neg_generate_not aux_generate_do nsubj_generate_we prep_generate_Unlike amod_Schwenk_2008 dep_Schwenk_al. nn_Schwenk_et nn_lexicons_translation amod_lexicons_additional dep_Koehn_2007 conj_and_Koehn_Hoang conj_or_models_lexicons appos_models_Hoang appos_models_Koehn amod_models_factored pobj_with_lexicons pobj_with_models dep_Unlike_Schwenk pcomp_Unlike_with
W09-0427	D07-1091	o	Many strategies have been proposed to integrate morphology information in SMT including factored translation models -LRB- Koehn and Hoang 2007 -RRB- adding a translation dictionary containing inflected forms to the training data -LRB- Schwenk et al. 2008 -RRB- entirely replacing surface forms by representations built on lemmas and POS tags -LRB- Popovic and Ney 2004 -RRB- morphemes learned in an unsupervised manner -LRB- Virpojia et al. 2007 -RRB- and using Porter stems and even 4-letter prefixes for word alignment -LRB- Watanabe et al. 2006 -RRB-	amod_Watanabe_2006 dep_Watanabe_al. nn_Watanabe_et nn_alignment_word advmod_4-letter_even dep_stems_Watanabe prep_for_stems_alignment dep_stems_prefixes conj_and_stems_4-letter dep_using_4-letter dep_using_stems dobj_using_Porter nsubj_using_morphemes amod_Virpojia_2007 dep_Virpojia_al. nn_Virpojia_et amod_manner_unsupervised det_manner_an conj_and_learned_using dep_learned_Virpojia prep_in_learned_manner nsubj_learned_morphemes vmod_learned_adding ccomp_learned_proposed dep_Popovic_2004 conj_and_Popovic_Ney nn_tags_POS dep_lemmas_Ney dep_lemmas_Popovic conj_and_lemmas_tags prep_on_built_tags prep_on_built_lemmas vmod_representations_built nn_forms_surface agent_replacing_representations dobj_replacing_forms advmod_replacing_entirely amod_Schwenk_2008 dep_Schwenk_al. nn_Schwenk_et nn_data_training det_data_the amod_forms_inflected prep_to_containing_data dobj_containing_forms vmod_dictionary_containing nn_dictionary_translation det_dictionary_a vmod_adding_replacing dep_adding_Schwenk dobj_adding_dictionary dep_Koehn_2007 conj_and_Koehn_Hoang appos_models_Hoang appos_models_Koehn nn_models_translation amod_models_factored prep_including_information_models prep_in_information_SMT nn_information_morphology dobj_integrate_information aux_integrate_to xcomp_proposed_integrate auxpass_proposed_been aux_proposed_have nsubjpass_proposed_strategies amod_strategies_Many
W09-0429	D07-1091	o	part-of-speech language model We use factored translation models -LRB- Koehn and Hoang 2007 -RRB- to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them	nn_model_n-gram amod_model_second det_model_a prep_over_run_them dobj_run_model nn_mapping_phrase amod_mapping_single det_mapping_a det_word_each conj_and_tags_run prep_in_tags_mapping prep_with_tags_word amod_tags_part-of-speech nn_tags_output advmod_tags_also aux_tags_to dep_Koehn_2007 conj_and_Koehn_Hoang appos_models_Hoang appos_models_Koehn nn_models_translation amod_models_factored dep_use_run dep_use_tags dobj_use_models nsubj_use_We rcmod_model_use nn_model_language amod_model_part-of-speech
C08-1015	D07-1112	o	4.3 Adaptation for unknown word2 The unknown word problem is an important issue for domain adaptation -LRB- Dredze et al. 2007 -RRB-	amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et nn_adaptation_domain dep_issue_Dredze prep_for_issue_adaptation amod_issue_important det_issue_an cop_issue_is nsubj_issue_problem dep_issue_Adaptation nn_problem_word amod_problem_unknown det_problem_The amod_word2_unknown prep_for_Adaptation_word2 num_Adaptation_4.3
C08-1015	D07-1112	o	Without specific knowledge of the target domains annotation standards significant improvement can not be made -LRB- Dredze et al. 2007 -RRB-	nn_al._et amod_Dredze_2007 dep_Dredze_al. dep_made_Dredze auxpass_made_be neg_made_not aux_made_can nsubjpass_made_improvement prep_without_made_knowledge amod_improvement_significant nn_standards_annotation nn_standards_domains nn_standards_target det_standards_the prep_of_knowledge_standards amod_knowledge_specific
C08-1015	D07-1112	o	-LRB- 2007 -RRB- and Dredze et al.	nn_al._et nn_al._Dredze conj_and_2007_al. dep_''_al. dep_''_2007
D07-1096	D07-1112	o	Instead of assigning HEAD and DEPREL in a single step some systems use a two-stage approach for attaching and labeling dependencies -LRB- Chen et al. 2007 Dredze et al. 2007 -RRB-	num_Dredze_2007 nn_Dredze_al. nn_Dredze_et dep_Chen_Dredze amod_Chen_2007 dep_Chen_al. nn_Chen_et dobj_attaching_dependencies conj_and_attaching_labeling prepc_for_approach_labeling prepc_for_approach_attaching amod_approach_two-stage det_approach_a dep_use_Chen dobj_use_approach nsubj_use_systems prepc_instead_of_use_assigning det_systems_some amod_step_single det_step_a conj_and_HEAD_DEPREL prep_in_assigning_step dobj_assigning_DEPREL dobj_assigning_HEAD
D07-1096	D07-1112	o	In order to calculate a global score or probability for a transition sequence two systems used a Markov chain approach -LRB- Duan et al. 2007 Sagae and Tsujii 2007 -RRB-	dep_Sagae_2007 conj_and_Sagae_Tsujii dep_Duan_Tsujii dep_Duan_Sagae appos_Duan_2007 dep_Duan_al. nn_Duan_et nn_approach_chain nn_approach_Markov det_approach_a dep_used_Duan dobj_used_approach nsubj_used_systems advcl_used_calculate num_systems_two nn_sequence_transition det_sequence_a prep_for_score_sequence conj_or_score_probability amod_score_global det_score_a dobj_calculate_probability dobj_calculate_score aux_calculate_to dep_calculate_order mark_calculate_In
D07-1096	D07-1112	o	5.4 Domain Adaptation 5.4.1 Feature-Based Approaches Onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well -LRB- Dredze et al. 2007 -RRB-	amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et dep_transfer_Dredze advmod_transfer_well aux_transfer_to xcomp_expected_transfer auxpass_expected_are nsubjpass_expected_that rcmod_features_expected dobj_include_features advmod_include_only aux_include_to xcomp_is_include nsubj_is_Adaptation amod_data_unlabeled det_data_any dobj_using_data vmod_Onewayofadaptingalearnertoanewdomainwithout_using nn_Onewayofadaptingalearnertoanewdomainwithout_Approaches amod_Onewayofadaptingalearnertoanewdomainwithout_Feature-Based num_Onewayofadaptingalearnertoanewdomainwithout_5.4.1 appos_Adaptation_Onewayofadaptingalearnertoanewdomainwithout nn_Adaptation_Domain num_Adaptation_5.4 ccomp_``_is
D07-1096	D07-1112	o	Another technique used was to filter sentences of the out-of-domain corpus based on their similarity to the target domain as predicted by a classifier -LRB- Dredze et al. 2007 -RRB-	amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et dep_classifier_Dredze det_classifier_a prep_by_predicted_classifier mark_predicted_as nn_domain_target det_domain_the prep_to_similarity_domain poss_similarity_their amod_corpus_out-of-domain det_corpus_the prep_of_sentences_corpus nn_sentences_filter advcl_was_predicted pobj_was_similarity prepc_based_on_was_on prep_to_was_sentences nsubj_was_technique vmod_technique_used det_technique_Another
D09-1086	D07-1112	o	As with many domain adaptation problems it is quite helpful to have some annotated target data especially when annotation styles vary -LRB- Dredze et al. 2007 -RRB-	amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et dep_vary_Dredze nsubj_vary_styles advmod_vary_when nn_styles_annotation advmod_when_especially nn_data_target amod_data_annotated det_data_some dobj_have_data aux_have_to advcl_helpful_vary xcomp_helpful_have advmod_helpful_quite cop_helpful_is nsubj_helpful_it prep_helpful_As nn_problems_adaptation nn_problems_domain amod_problems_many pobj_with_problems pcomp_As_with
E09-3005	D07-1112	o	The problem itself has started to get attention only recently -LRB- Roark and Bacchiani 2003 Hara et al. 2005 Daume III and Marcu 2006 Daume III 2007 Blitzer et al. 2006 McClosky et al. 2006 Dredze et al. 2007 -RRB-	num_Dredze_2007 nn_Dredze_al. nn_Dredze_et num_McClosky_2006 nn_McClosky_al. nn_McClosky_et num_Blitzer_2006 nn_Blitzer_al. nn_Blitzer_et appos_III_2007 nn_III_Daume dep_III_Dredze conj_and_III_McClosky conj_and_III_Blitzer conj_and_III_III conj_and_III_2006 conj_and_III_Marcu nn_III_Daume num_Hara_2005 nn_Hara_al. nn_Hara_et dep_Roark_McClosky dep_Roark_Blitzer dep_Roark_III dep_Roark_2006 dep_Roark_Marcu dep_Roark_III conj_and_Roark_Hara conj_and_Roark_2003 conj_and_Roark_Bacchiani advmod_recently_only dep_get_Hara dep_get_2003 dep_get_Bacchiani dep_get_Roark advmod_get_recently dobj_get_attention aux_get_to xcomp_started_get aux_started_has nsubj_started_problem npadvmod_problem_itself det_problem_The
E09-3005	D07-1112	o	In contrast semi-supervised domain adaptation -LRB- Blitzer et al. 2006 McClosky et al. 2006 Dredze et al. 2007 -RRB- is the scenario in which in addition to the labeled source data we only have unlabeled and no labeled target domain data	nn_data_domain nn_data_target amod_data_labeled dep_unlabeled_data conj_and_unlabeled_no dobj_have_no dobj_have_unlabeled advmod_have_only nsubj_have_we prep_in_addition_to_have_data prep_in_have_which nn_data_source amod_data_labeled det_data_the rcmod_scenario_have det_scenario_the cop_scenario_is nsubj_scenario_adaptation prep_in_scenario_contrast num_Dredze_2007 nn_Dredze_al. nn_Dredze_et dep_McClosky_Dredze num_McClosky_2006 nn_McClosky_al. nn_McClosky_et dep_Blitzer_McClosky appos_Blitzer_2006 dep_Blitzer_al. nn_Blitzer_et appos_adaptation_Blitzer nn_adaptation_domain amod_adaptation_semi-supervised
E09-3005	D07-1112	o	2 Motivation and Prior Work While several authors have looked at the supervised adaptation case there are less -LRB- and especially less successful -RRB- studies on semi-supervised domain adaptation -LRB- McClosky et al. 2006 Blitzer et al. 2006 Dredze et al. 2007 -RRB-	num_Dredze_2007 nn_Dredze_al. nn_Dredze_et dep_Blitzer_Dredze num_Blitzer_2006 nn_Blitzer_al. nn_Blitzer_et dep_McClosky_Blitzer appos_McClosky_2006 dep_McClosky_al. nn_McClosky_et nn_adaptation_domain amod_adaptation_semi-supervised dep_studies_McClosky prep_on_studies_adaptation amod_studies_less dep_less_successful advmod_less_especially cc_less_and dep_less_less nsubj_are_studies expl_are_there ccomp_are_Work ccomp_are_Motivation nn_case_adaptation amod_case_supervised det_case_the prep_at_looked_case aux_looked_have nsubj_looked_authors mark_looked_While amod_authors_several amod_Work_Prior dep_Motivation_looked conj_and_Motivation_Work num_Motivation_2 ccomp_``_are
E09-3005	D07-1112	o	However based on annotation differences in the datasets -LRB- Dredze et al. 2007 -RRB- and a bug in their system -LRB- Shimizu and Nakagawa 2007 -RRB- their results are inconclusive .1 Thus the effectiveness of SCL is rather unexplored for parsing	prep_for_unexplored_parsing advmod_unexplored_rather cop_unexplored_is nsubj_unexplored_effectiveness advmod_unexplored_Thus prep_of_effectiveness_SCL det_effectiveness_the dep_.1_unexplored amod_.1_inconclusive cop_.1_are nsubj_.1_results pobj_.1_differences prepc_based_on_.1_on advmod_.1_However poss_results_their amod_Shimizu_2007 conj_and_Shimizu_Nakagawa poss_system_their dep_bug_Nakagawa dep_bug_Shimizu prep_in_bug_system det_bug_a amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et conj_and_datasets_bug dep_datasets_Dredze det_datasets_the prep_in_differences_bug prep_in_differences_datasets nn_differences_annotation
I08-2097	D07-1112	o	84.12 only PTB -LRB- baseline -RRB- 83.58 1st -LRB- Sagae and Tsujii 2007 -RRB- 83.42 2nd -LRB- Dredze et al. 2007 -RRB- 83.38 3rd -LRB- Attardi et al. 2007 -RRB- 83.08 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task	amod_task_shared num_task_2007 nn_task_CoNLL det_task_the prep_of_track_task nn_track_adaptation nn_track_domain det_track_the prep_of_scores_track amod_scores_highest num_scores_three det_scores_the dobj_lists_scores nsubj_lists_row advmod_lists_84.12 amod_row_third num_row_83.08 amod_row_3rd amod_row_2nd amod_row_PTB dep_Attardi_2007 dep_Attardi_al. nn_Attardi_et dep_3rd_Attardi num_3rd_83.38 dep_al._2007 nn_al._et advmod_Dredze_al. dep_2nd_Dredze num_2nd_83.42 num_Sagae_2007 conj_and_Sagae_Tsujii number_1st_83.58 dep_PTB_Tsujii dep_PTB_Sagae num_PTB_1st appos_PTB_baseline advmod_PTB_only
I08-2097	D07-1112	o	This was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data -LRB- Dredze et al. 2007 -RRB-	amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et amod_data_unlabeled prep_from_gains_data amod_gains_meaningful det_gains_any dobj_obtain_gains aux_obtain_to xcomp_failed_obtain nsubj_failed_participants mark_failed_as det_task_the prep_in_participants_task amod_participants_many dep_challenge_Dredze advcl_challenge_failed nn_challenge_difcult det_challenge_a cop_challenge_was nsubj_challenge_This
I08-2097	D07-1112	o	Dredze et al. yielded the second highest score1 in the domain adaptation track -LRB- Dredze et al. 2007 -RRB-	amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et dep_track_Dredze nn_track_adaptation nn_track_domain det_track_the prep_in_score1_track amod_score1_highest amod_score1_second det_score1_the dobj_yielded_score1 nn_al._et dep_Dredze_yielded dobj_Dredze_al. ccomp_``_Dredze
I08-2097	D07-1112	o	Dredze et al. also indicated that unlabeled dependency parsing is not robust to domain adaptation -LRB- Dredze et al. 2007 -RRB-	dep_al._2007 nn_al._et advmod_Dredze_al. nn_adaptation_domain prep_to_robust_adaptation neg_robust_not cop_robust_is nsubj_robust_parsing mark_robust_that nn_parsing_dependency amod_parsing_unlabeled dep_indicated_Dredze ccomp_indicated_robust advmod_indicated_also nsubj_indicated_Dredze nn_al._et dobj_Dredze_al.
P08-1082	D07-1112	o	It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes since the data has huge variability in terms of quality style genres domains etc. and domain adaptation for the NLP tasks involved is still an open problem -LRB- Dredze et al. 2007 -RRB-	amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et dep_problem_Dredze amod_problem_open det_problem_an advmod_problem_still cop_problem_is csubj_problem_has vmod_tasks_involved nn_tasks_NLP det_tasks_the nn_adaptation_domain dep_domains_etc. conj_and_quality_adaptation conj_and_quality_domains conj_and_quality_genres conj_and_quality_style prep_for_terms_tasks prep_of_terms_adaptation prep_of_terms_domains prep_of_terms_genres prep_of_terms_style prep_of_terms_quality amod_variability_huge prep_in_has_terms dobj_has_variability nsubj_has_data mark_has_since det_data_the prep_of_plenty_mistakes dobj_contains_plenty nsubj_contains_output conj_and_noisy_problem conj_and_noisy_contains cop_noisy_is nsubj_noisy_output mark_noisy_that nn_steps_processing amod_steps_mentioned det_steps_all prep_of_output_steps det_output_the ccomp_realize_problem ccomp_realize_contains ccomp_realize_noisy aux_realize_to xcomp_important_realize cop_important_is nsubj_important_It
W09-2205	D07-1112	o	Based on annotation differences in the datasets -LRB- Dredze et al. 2007 -RRB- and a bug in their system -LRB- Shimizu and Nakagawa 2007 -RRB- their results are inconclusive	cop_inconclusive_are nsubj_inconclusive_results pobj_inconclusive_differences prepc_based_on_inconclusive_on poss_results_their amod_Shimizu_2007 conj_and_Shimizu_Nakagawa poss_system_their dep_bug_Nakagawa dep_bug_Shimizu prep_in_bug_system det_bug_a amod_Dredze_2007 dep_Dredze_al. nn_Dredze_et conj_and_datasets_bug dep_datasets_Dredze det_datasets_the prep_in_differences_bug prep_in_differences_datasets nn_differences_annotation
C08-1052	D07-1113	o	As well as the sentiment expressions leading to evaluations there are many semantic aspects to be extracted from documents which contain writers opinions such as subjectivity -LRB- Wiebe and Mihalcea 2006 -RRB- comparative sentences -LRB- Jindal and Liu 2006 -RRB- or predictive expressions -LRB- Kim and Hovy 2007 -RRB-	dep_Kim_2007 conj_and_Kim_Hovy dep_expressions_Hovy dep_expressions_Kim amod_expressions_predictive dep_Jindal_2006 conj_and_Jindal_Liu appos_sentences_Liu appos_sentences_Jindal amod_sentences_comparative dep_Wiebe_2006 conj_and_Wiebe_Mihalcea conj_or_subjectivity_expressions conj_or_subjectivity_sentences appos_subjectivity_Mihalcea appos_subjectivity_Wiebe prep_such_as_opinions_expressions prep_such_as_opinions_sentences prep_such_as_opinions_subjectivity nn_opinions_writers dobj_contain_opinions nsubj_contain_which rcmod_documents_contain prep_from_extracted_documents auxpass_extracted_be aux_extracted_to vmod_aspects_extracted amod_aspects_semantic amod_aspects_many dobj_are_aspects expl_are_there advcl_are_well prep_to_leading_evaluations vmod_expressions_leading nn_expressions_sentiment det_expressions_the dep_well_expressions mwe_well_as advmod_well_As
C08-1060	D07-1113	o	Specifically Kim and Hovy -LRB- 2007 -RRB- identify which political candidate is predicted to win by an opinion posted on a message board and aggregate opinions to correctly predict an election result	nn_result_election det_result_an dobj_predict_result advmod_predict_correctly aux_predict_to nn_opinions_aggregate conj_and_board_opinions nn_board_message det_board_a xcomp_posted_predict prep_on_posted_opinions prep_on_posted_board det_opinion_an prep_by_win_opinion aux_win_to xcomp_predicted_win auxpass_predicted_is nsubjpass_predicted_candidate amod_candidate_political det_candidate_which dep_identify_posted ccomp_identify_predicted nsubj_identify_Hovy nsubj_identify_Kim advmod_identify_Specifically appos_Hovy_2007 conj_and_Kim_Hovy
C08-1060	D07-1113	o	Opinion forecasting differs from that of opinion analysis such as extracting opinions evaluating sentiment and extracting predictions -LRB- Kim and Hovy 2007 -RRB-	amod_Kim_2007 conj_and_Kim_Hovy dep_predictions_Hovy dep_predictions_Kim dobj_extracting_predictions dobj_evaluating_sentiment conj_and_extracting_extracting conj_and_extracting_evaluating dobj_extracting_opinions nn_analysis_opinion prep_of_that_analysis prepc_such_as_differs_extracting prepc_such_as_differs_evaluating prepc_such_as_differs_extracting prep_from_differs_that nsubj_differs_forecasting nn_forecasting_Opinion
C08-1060	D07-1113	o	Kim and Hovy -LRB- 2007 -RRB- make a similar assumption	amod_assumption_similar det_assumption_a dobj_make_assumption nsubj_make_Hovy nsubj_make_Kim appos_Hovy_2007 conj_and_Kim_Hovy
C08-1101	D07-1113	o	An application of the idea of alternative targets can be seen in Kim and Hovys -LRB- 2007 -RRB- work on election prediction	nn_prediction_election nn_work_Hovys nn_work_Kim appos_Hovys_2007 conj_and_Kim_Hovys prep_on_seen_prediction prep_in_seen_work auxpass_seen_be aux_seen_can nsubjpass_seen_application amod_targets_alternative prep_of_idea_targets det_idea_the prep_of_application_idea det_application_An
P09-1026	D07-1113	o	Kim and Hovy -LRB- 2007 -RRB- predict the results of an election by analyzing forums discussing the elections	det_elections_the dobj_discussing_elections vmod_forums_discussing dobj_analyzing_forums det_election_an prep_of_results_election det_results_the prepc_by_predict_analyzing dobj_predict_results nsubj_predict_Hovy nsubj_predict_Kim appos_Hovy_2007 conj_and_Kim_Hovy
D09-1031	D08-1027	o	Examples of the latter include providing suggestions from a machine labeler and using extremely cheap human labelers e.g. with the Amazon Mechanical Turk -LRB- Snow et al. 2008 -RRB-	amod_Snow_2008 dep_Snow_al. nn_Snow_et appos_Turk_Snow amod_Turk_Mechanical nn_Turk_Amazon det_Turk_the prep_with_e.g._Turk amod_labelers_human amod_labelers_cheap advmod_cheap_extremely dep_using_e.g. dobj_using_labelers nn_labeler_machine det_labeler_a prep_from_suggestions_labeler conj_and_providing_using dobj_providing_suggestions xcomp_include_using xcomp_include_providing nsubj_include_Examples det_latter_the prep_of_Examples_latter
P09-1032	D08-1027	p	While this is certainly a daunting task it is possible that for annotation studies that do not require expert annotators and extensive annotator training the newly available access to a large pool of inexpensive annotators such as the Amazon Mechanical Turk scheme -LRB- Snow et al. 2008 -RRB- ,4 or embedding the task in an online game played by volunteers -LRB- Poesio et al. 2008 von Ahn 2006 -RRB- could provide some solutions	det_solutions_some dobj_provide_solutions aux_provide_could nsubj_provide_access prep_for_provide_studies mark_provide_that amod_Ahn_2006 nn_Ahn_von dep_Poesio_Ahn appos_Poesio_2008 dep_Poesio_al. nn_Poesio_et agent_played_volunteers vmod_game_played amod_game_online det_game_an det_task_the dobj_embedding_task conj_or_,4_embedding amod_Snow_2008 dep_Snow_al. nn_Snow_et dep_scheme_embedding dep_scheme_,4 dep_scheme_Snow nn_scheme_Turk amod_scheme_Mechanical nn_scheme_Amazon det_scheme_the prep_such_as_annotators_scheme amod_annotators_inexpensive prep_of_pool_annotators amod_pool_large det_pool_a dep_access_Poesio prep_in_access_game prep_to_access_pool amod_access_available det_access_the advmod_available_newly nn_training_annotator amod_training_extensive conj_and_annotators_training amod_annotators_expert dobj_require_training dobj_require_annotators neg_require_not aux_require_do nsubj_require_that rcmod_studies_require nn_studies_annotation ccomp_possible_provide cop_possible_is nsubj_possible_it advcl_possible_task amod_task_daunting det_task_a advmod_task_certainly cop_task_is nsubj_task_this mark_task_While
P09-2078	D08-1027	o	Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources -LRB- Snow et al. 2008 -RRB-	amod_Snow_2008 dep_Snow_al. nn_Snow_et dep_sources_Snow amod_sources_trusted prep_with_quality_sources nsubj_comparable_data prep_in_reliable_quality conj_and_reliable_comparable cop_reliable_is nsubj_reliable_data mark_reliable_that nn_service_Turk amod_service_Mechanical det_service_the prep_through_collected_service vmod_data_collected ccomp_shown_comparable ccomp_shown_reliable aux_shown_has nsubj_shown_work amod_work_Previous
W09-1904	D08-1027	o	Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk a marketplace for recruiting online workers -LRB- Su et al. 2007 Kaisser et al. 2008 Kittur et al. 2008 Sheng et al. 2008 Snow et al. 2008 Sorokin and Forsyth 2008 -RRB-	amod_Sorokin_2008 conj_and_Sorokin_Forsyth dep_Snow_Forsyth dep_Snow_Sorokin num_Snow_2008 nn_Snow_al. nn_Snow_et num_al._2008 nn_al._et nn_al._Sheng num_al._2008 nn_al._et nn_al._Kittur nn_al._et nn_al._Kaisser dep_al._Snow conj_al._al. conj_al._al. appos_al._2008 dep_al._al. dep_al._2007 nn_al._et nn_al._Su dep_workers_al. amod_workers_online dobj_recruiting_workers prepc_for_marketplace_recruiting det_marketplace_a appos_Turk_marketplace amod_Turk_Mechanical nn_Turk_Amazon prep_from_obtained_Turk vmod_annotations_obtained prep_of_use_annotations det_use_the dobj_studied_use aux_studied_have nsubj_studied_papers amod_papers_recent amod_papers_Several ccomp_``_studied
W09-1908	D08-1027	o	With the success of collaborative sites like Amazons Mechanical Turk 1 one 1http / / www.mturk.com / 59 can provide the task of annotation to multiple oracles on the internet -LRB- Snow et al. 2008 -RRB-	amod_Snow_2008 dep_Snow_al. nn_Snow_et det_internet_the prep_on_oracles_internet amod_oracles_multiple prep_of_task_annotation det_task_the dep_provide_Snow prep_to_provide_oracles dobj_provide_task aux_provide_can nsubj_provide_59 dep_www.mturk.com_provide prep_with_www.mturk.com_success num_1http_one appos_Turk_1http num_Turk_1 amod_Turk_Mechanical nn_Turk_Amazons prep_like_sites_Turk amod_sites_collaborative prep_of_success_sites det_success_the
D09-1008	D08-1039	o	A few studies -LRB- Carpuat and Wu 2007 Ittycheriah and Roukos 2007 He et al. 2008 Hasan et al. 2008 -RRB- addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence	nn_sentence_input det_sentence_the prep_in_context_sentence poss_context_its prep_on_based_context vmod_span_based nn_span_input det_span_an nn_rules_translation amod_rules_appropriate det_rules_the prep_for_selecting_span dobj_selecting_rules det_defect_this prepc_by_addressed_selecting dobj_addressed_defect nsubj_addressed_studies num_Hasan_2008 nn_Hasan_al. nn_Hasan_et advmod_2008_al. dep_2008_He nn_al._et num_Ittycheriah_2007 conj_and_Ittycheriah_Roukos dep_Carpuat_Hasan conj_and_Carpuat_2008 conj_and_Carpuat_Roukos conj_and_Carpuat_Ittycheriah conj_and_Carpuat_2007 conj_and_Carpuat_Wu dep_studies_2008 dep_studies_Ittycheriah dep_studies_2007 dep_studies_Wu dep_studies_Carpuat amod_studies_few det_studies_A ccomp_``_addressed
D09-1022	D08-1039	o	We chose this inverse direction since it can be integrated directly into the decoder and thus does not rely on a two-pass approach using reranking as it is the case for -LRB- Hasan et al. 2008 -RRB-	amod_Hasan_2008 dep_Hasan_al. nn_Hasan_et dep_for_Hasan prep_case_for det_case_the cop_case_is nsubj_case_it mark_case_as dobj_using_reranking vmod_approach_using amod_approach_two-pass det_approach_a advcl_rely_case prep_on_rely_approach neg_rely_not aux_rely_does advmod_rely_thus nsubj_rely_it det_decoder_the conj_and_integrated_rely prep_into_integrated_decoder advmod_integrated_directly auxpass_integrated_be aux_integrated_can nsubjpass_integrated_it mark_integrated_since amod_direction_inverse det_direction_this advcl_chose_rely advcl_chose_integrated dobj_chose_direction nsubj_chose_We
D09-1022	D08-1039	o	The trigger-based lexicon model used in this work follows the training procedure introduced in -LRB- Hasan et al. 2008 -RRB- and is integrated directly in the decoder instead of being applied in n-best list reranking	nn_reranking_list amod_reranking_n-best prep_in_applied_reranking auxpass_applied_being det_decoder_the prep_in_integrated_decoder advmod_integrated_directly auxpass_integrated_is conj_negcc_Hasan_applied conj_and_Hasan_integrated amod_Hasan_2008 dep_Hasan_al. nn_Hasan_et prep_in_introduced_applied prep_in_introduced_integrated prep_in_introduced_Hasan vmod_procedure_introduced nn_procedure_training det_procedure_the dobj_follows_procedure nsubj_follows_model det_work_this prep_in_used_work vmod_model_used nn_model_lexicon amod_model_trigger-based det_model_The
D08-1067	H05-1013	o	-LRB- 2005 -RRB- Ponzetto and Strube -LRB- 2006 -RRB- -RRB- and the exploitation of advanced techniques that involve joint learning -LRB- e.g. Daume III and Marcu -LRB- 2005 -RRB- -RRB- and joint inference -LRB- e.g. Denis and Baldridge -LRB- 2007 -RRB- -RRB- for coreference resolution and a related extraction task	nn_task_extraction amod_task_related det_task_a nn_resolution_coreference appos_Baldridge_2007 conj_and_Denis_task prep_for_Denis_resolution conj_and_Denis_Baldridge dep_e.g._task dep_e.g._Baldridge dep_e.g._Denis dep_inference_e.g. amod_inference_joint appos_Marcu_2005 conj_and_III_inference conj_and_III_Marcu nn_III_Daume dep_,_inference dep_,_Marcu dep_,_III dep_-LRB-_e.g. amod_learning_joint dobj_involve_learning nsubj_involve_that rcmod_techniques_involve amod_techniques_advanced prep_of_exploitation_techniques det_exploitation_the dep_Strube_2006 conj_and_Ponzetto_exploitation conj_and_Ponzetto_Strube dep_Ponzetto_2005
D08-1069	H05-1013	o	While early machine learning approaches for the task relied on local discriminative classifiers -LRB- Soon et al. 2001 Ng and Cardie 2002b Morton 2000 Kehler et al. 2004 -RRB- more recent approaches use joint and/or global models -LRB- McCallum and Wellner 2004 Ng 2004 Daume III and Marcu 2005 Denis and Baldridge 2007a -RRB-	nn_III_Daume num_Ng_2004 appos_McCallum_2007a conj_and_McCallum_Baldridge conj_and_McCallum_Denis conj_and_McCallum_2005 conj_and_McCallum_Marcu conj_and_McCallum_III conj_and_McCallum_Ng conj_and_McCallum_2004 conj_and_McCallum_Wellner dep_models_Baldridge dep_models_Denis dep_models_2005 dep_models_Marcu dep_models_III dep_models_Ng dep_models_2004 dep_models_Wellner dep_models_McCallum amod_models_global amod_models_joint conj_and/or_joint_global dobj_use_models vmod_approaches_use amod_approaches_recent advmod_recent_more num_Kehler_2004 nn_Kehler_al. nn_Kehler_et num_Morton_2000 appos_Ng_approaches conj_and_Ng_Kehler conj_and_Ng_Morton conj_and_Ng_2002b conj_and_Ng_Cardie dep_al._Kehler dep_al._Morton dep_al._2002b dep_al._Cardie dep_al._Ng appos_al._2001 nn_al._et advmod_al._Soon amod_classifiers_local dep_local_discriminative prep_on_relied_classifiers vmod_task_relied det_task_the dep_approaches_al. prep_for_approaches_task amod_approaches_learning nn_approaches_machine amod_machine_early pobj_While_approaches dep_``_While
D09-1128	H05-1013	o	The most similar work to ours is -LRB- Daume III and Marcu 2005 -RRB- in which two most common synsets from WordNet for all words in an NP and their hypernyms are extracted as features	prep_as_extracted_features auxpass_extracted_are nsubjpass_extracted_synsets prep_in_extracted_which poss_hypernyms_their conj_and_NP_hypernyms det_NP_an prep_in_words_hypernyms prep_in_words_NP det_words_all prep_for_synsets_words prep_from_synsets_WordNet amod_synsets_common num_synsets_two advmod_common_most dep_III_2005 conj_and_III_Marcu nn_III_Daume rcmod_is_extracted dep_is_Marcu dep_is_III dep_work_is prep_to_work_ours amod_work_similar det_work_The advmod_similar_most dep_``_work
D09-1128	H05-1013	o	Other similar work includes the mention detection -LRB- MD -RRB- task -LRB- Florian et al. 2006 -RRB- and joint probabilistic model of coreference -LRB- Daume III and Marcu 2005 -RRB-	amod_Daume_2005 conj_and_Daume_Marcu num_Daume_III dep_coreference_Marcu dep_coreference_Daume prep_of_model_coreference amod_model_probabilistic amod_model_joint amod_Florian_2006 dep_Florian_al. nn_Florian_et conj_and_task_model dep_task_Florian nn_task_detection appos_detection_MD nn_detection_mention det_detection_the dobj_includes_model dobj_includes_task nsubj_includes_work amod_work_similar amod_work_Other
N07-1010	H05-1013	o	See -LRB- Luo and Zitouni 2005 -RRB- and -LRB- Daume III and Marcu 2005 -RRB-	dep_III_2005 conj_and_III_Marcu nn_III_Daume dep_Luo_2005 conj_and_Luo_Zitouni conj_and_See_Marcu conj_and_See_III dep_See_Zitouni dep_See_Luo ccomp_``_III ccomp_``_See
N07-1011	H05-1013	o	7 Related Work There has been a recent interest in training methods that enable the use of first-order features -LRB- Paskin 2002 Daume III and Marcu 2005b Richardson and Domingos 2006 -RRB-	amod_Richardson_2006 conj_and_Richardson_Domingos dep_III_Domingos dep_III_Richardson conj_and_III_2005b conj_and_III_Marcu nn_III_Daume dep_Paskin_2005b dep_Paskin_Marcu dep_Paskin_III dep_Paskin_2002 dep_features_Paskin amod_features_first-order prep_of_use_features det_use_the dobj_enable_use nsubj_enable_that rcmod_methods_enable nn_methods_training prep_in_interest_methods amod_interest_recent det_interest_a cop_interest_been aux_interest_has expl_interest_There rcmod_Work_interest amod_Work_Related num_Work_7 ccomp_``_Work
N07-1011	H05-1013	o	Perhaps the most related is 86 learning as search optimization -LRB- LASO -RRB- -LRB- Daume III and Marcu 2005b Daume III and Marcu 2005a -RRB-	appos_III_2005a conj_and_III_Marcu nn_III_Daume dep_III_Marcu dep_III_III appos_III_2005b conj_and_III_Marcu nn_III_Daume dep_optimization_Marcu dep_optimization_III appos_optimization_LASO nn_optimization_search prep_as_learning_optimization num_learning_86 cop_learning_is nsubj_learning_related advmod_learning_Perhaps advmod_related_most det_related_the
P09-1024	H05-1013	o	We implement this algorithm using the perceptron framework as it can be easily modified for structured prediction while preserving convergence guarantees -LRB- Daume III and Marcu 2005 Snyder and Barzilay 2007 -RRB-	amod_Snyder_2007 conj_and_Snyder_Barzilay dep_III_Barzilay dep_III_Snyder conj_and_III_2005 conj_and_III_Marcu nn_III_Daume dep_guarantees_2005 dep_guarantees_Marcu dep_guarantees_III nn_guarantees_convergence dobj_preserving_guarantees amod_prediction_structured prepc_while_modified_preserving prep_for_modified_prediction advmod_modified_easily auxpass_modified_be aux_modified_can nsubjpass_modified_it mark_modified_as nn_framework_perceptron det_framework_the dobj_using_framework det_algorithm_this advcl_implement_modified xcomp_implement_using dobj_implement_algorithm nsubj_implement_We
P09-1024	H05-1013	o	Training Procedure Our algorithm is a modification of the perceptron ranking algorithm -LRB- Collins 2002 -RRB- which allows for joint learning across several ranking problems -LRB- Daume III and Marcu 2005 Snyder and Barzilay 2007 -RRB-	amod_Snyder_2007 conj_and_Snyder_Barzilay dep_III_Barzilay dep_III_Snyder conj_and_III_2005 conj_and_III_Marcu nn_III_Daume dep_problems_2005 dep_problems_Marcu dep_problems_III amod_problems_ranking amod_problems_several prep_across_learning_problems amod_learning_joint prep_for_allows_learning nsubj_allows_which dep_Collins_2002 rcmod_algorithm_allows appos_algorithm_Collins amod_algorithm_ranking nn_algorithm_perceptron det_algorithm_the prep_of_modification_algorithm det_modification_a cop_modification_is nsubj_modification_algorithm poss_algorithm_Our nn_algorithm_Procedure amod_algorithm_Training
P09-5006	H05-1013	o	We finally move on to present more complex models which attempt to model coreference as a global discourse phenomenon -LRB- Yang et al. 2003 Luo et al. 2004 Daume III & Marcu 2005 inter alia -RRB-	nn_alia_inter appos_III_alia amod_III_2005 conj_and_III_Marcu nn_III_Daume num_Luo_2004 nn_Luo_al. nn_Luo_et dep_Yang_Marcu dep_Yang_III dep_Yang_Luo amod_Yang_2003 dep_Yang_al. nn_Yang_et nn_phenomenon_discourse amod_phenomenon_global det_phenomenon_a prep_as_model_phenomenon dobj_model_coreference aux_model_to xcomp_attempt_model nsubj_attempt_which rcmod_models_attempt amod_models_complex advmod_complex_more dobj_present_models aux_present_to dep_move_Yang prepc_on_move_present advmod_move_finally nsubj_move_We
W06-1633	H05-1013	o	In contrast globally optimized clustering decisions were reported in -LRB- Luo et al. 2004 -RRB- and -LRB- DaumeIII and Marcu 2005a -RRB- where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization -LRB- LaSO -RRB- framework -LRB- DaumeIII and Marcu 2005b -RRB- respectively but the first search is partial and driven by heuristics and the second one only looks back in text	prep_in_looks_text advmod_looks_back advmod_looks_only nsubj_looks_one amod_one_second det_one_the conj_and_heuristics_looks prep_by_driven_looks prep_by_driven_heuristics nsubj_driven_search conj_and_partial_driven cop_partial_is nsubj_partial_search amod_search_first det_search_the appos_DaumeIII_2005b conj_and_DaumeIII_Marcu advmod_framework_respectively dep_framework_Marcu dep_framework_DaumeIII nn_framework_LaSO nn_framework_Optimization dobj_Search_framework det_Learning_the prepc_as_using_Search dobj_using_Learning nn_representation_tree nn_representation_Bell det_representation_a prepc_by_searching_using prep_on_searching_representation conj_or_searching_searching agent_considered_searching agent_considered_searching auxpass_considered_are nsubjpass_considered_possibilities advmod_considered_where nn_possibilities_clustering det_possibilities_all appos_DaumeIII_2005a conj_and_DaumeIII_Marcu conj_but_Luo_driven conj_but_Luo_partial rcmod_Luo_considered conj_and_Luo_Marcu conj_and_Luo_DaumeIII amod_Luo_2004 dep_Luo_al. nn_Luo_et prep_in_reported_partial prep_in_reported_DaumeIII prep_in_reported_Luo auxpass_reported_were nsubjpass_reported_decisions prep_in_reported_contrast nn_decisions_clustering amod_decisions_optimized advmod_optimized_globally
W06-1633	H05-1013	o	-LRB- DaumeIII and Marcu 2005a -RRB- use the Learning as Search Optimization framework to take into account the non-locality behavior of the coreference features	nn_features_coreference det_features_the prep_of_behavior_features amod_behavior_non-locality det_behavior_the dobj_take_behavior prep_into_take_account aux_take_to nn_framework_Optimization vmod_Search_take dobj_Search_framework dep_as_Search det_Learning_the prep_use_as dobj_use_Learning nsubj_use_Marcu nsubj_use_DaumeIII appos_DaumeIII_2005a conj_and_DaumeIII_Marcu
D07-1014	H05-1064	o	We have already shown in Section 3 how to solve -LRB- a -RRB- here we avoid -LRB- b -RRB- by maximizing conditional likelihood marginalizing out the hidden variable denotedz max vector summationdisplay x y p -LRB- x y -RRB- log summationdisplay z pvector -LRB- y z | x -RRB- -LRB- 17 -RRB- This sort of conditional training with hidden variables was carried out by Koo and Collins -LRB- 2005 -RRB- for example in reranking it is related to the information bottleneck method -LRB- Tishby et al. 1999 -RRB- and contrastive estimation -LRB- Smith and Eisner 2005 -RRB-	amod_Smith_2005 conj_and_Smith_Eisner dep_estimation_Eisner dep_estimation_Smith amod_estimation_contrastive amod_Tishby_1999 dep_Tishby_al. nn_Tishby_et nn_method_bottleneck nn_method_information det_method_the conj_and_related_estimation dep_related_Tishby prep_to_related_method cop_related_is nsubj_related_it appos_Collins_2005 conj_and_Koo_Collins prep_in_carried_reranking prep_for_carried_example agent_carried_Collins agent_carried_Koo prt_carried_out auxpass_carried_was nsubjpass_carried_sort amod_variables_hidden prep_with_training_variables amod_training_conditional prep_of_sort_training det_sort_This num_x_| dep_z_x nn_y_pvector dep_y_z nn_y_summationdisplay nn_y_log nn_y_p appos_x_y dep_p_x nn_p_y dep_x_z appos_x_y nn_x_summationdisplay nn_x_vector nn_x_max dep_denotedz_x rcmod_variable_carried num_variable_17 appos_variable_denotedz amod_variable_hidden det_variable_the dobj_marginalizing_variable prt_marginalizing_out amod_likelihood_conditional dobj_maximizing_likelihood parataxis_b_estimation parataxis_b_related vmod_b_marginalizing prepc_by_b_maximizing dep_avoid_b nsubj_avoid_we advmod_avoid_here dep_solve_a aux_solve_to advmod_solve_how num_Section_3 parataxis_shown_avoid ccomp_shown_solve prep_in_shown_Section advmod_shown_already aux_shown_have nsubj_shown_We
D08-1091	H05-1064	o	Our method is based on the ones described in -LRB- Erkan and Radev 2004 Mihalcea and Tarau 2004 Fader et al. 2007 -RRB- The objective of this paper is to dynamically rank speakers or participants in a discussion	det_discussion_a conj_or_speakers_participants prep_in_rank_discussion dobj_rank_participants dobj_rank_speakers advmod_rank_dynamically aux_rank_to xcomp_is_rank nsubj_is_Fader nsubj_is_2004 nsubj_is_Tarau nsubj_is_Mihalcea det_paper_this prep_of_objective_paper det_objective_The num_Fader_2007 nn_Fader_al. nn_Fader_et appos_Mihalcea_objective conj_and_Mihalcea_Fader conj_and_Mihalcea_2004 conj_and_Mihalcea_Tarau dep_Erkan_is conj_and_Erkan_2004 conj_and_Erkan_Radev prep_in_described_2004 prep_in_described_Radev prep_in_described_Erkan vmod_ones_described det_ones_the prep_on_based_ones auxpass_based_is nsubjpass_based_method poss_method_Our ccomp_``_based
D08-1091	H05-1064	o	Discriminative parsing has been investigated before such as in Johnson -LRB- 2001 -RRB- Clark and Curran -LRB- 2004 -RRB- Henderson -LRB- 2004 -RRB- Koo and Collins -LRB- 2005 -RRB- Turian et al.	nn_al._et amod_al._Turian prep_in_al._Collins prep_in_al._Koo prep_in_al._Henderson prep_in_al._Curran prep_in_al._Clark prep_in_al._Johnson appos_Collins_2005 appos_Henderson_2004 appos_Curran_2004 conj_and_Johnson_Collins conj_and_Johnson_Koo conj_and_Johnson_Henderson conj_and_Johnson_Curran conj_and_Johnson_Clark appos_Johnson_2001 dep_as_al. mwe_as_such advmod_investigated_as advmod_investigated_before auxpass_investigated_been aux_investigated_has nsubjpass_investigated_parsing amod_parsing_Discriminative
D09-1023	H05-1064	o	1113 Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out .10 Optimization problems of this form are by now widely known in NLP -LRB- Koo and Collins 2005 -RRB- and have recently been used for machinetranslationaswell -LRB- Blunsometal. ,2008 -RRB-	num_Blunsometal._,2008 appos_machinetranslationaswell_Blunsometal. prep_for_used_machinetranslationaswell auxpass_used_been advmod_used_recently aux_used_have dep_Koo_2005 conj_and_Koo_Collins appos_NLP_Collins appos_NLP_Koo prep_in_known_NLP advmod_known_widely advmod_known_now conj_and_are_used prepc_by_are_known det_form_this prep_of_problems_form nn_problems_Optimization num_problems_.10 dobj_marginalized_problems prt_marginalized_out auxpass_marginalized_be aux_marginalized_to vmod_variable_marginalized amod_variable_hidden det_variable_a dep_treated_used dep_treated_are prep_as_treated_variable auxpass_treated_are nsubjpass_treated_equations nn_alignments_a. nn_alignments_t conj_and_t_a. prep_over_summing_alignments prepc_for_equations_summing nn_equations_DP amod_equations_Recursive num_equations_1113
E09-1037	H05-1064	o	Meanwhile some learning algorithms like maximum likelihood for conditional log-linear models -LRB- Lafferty et al. 2001 -RRB- unsupervised models -LRB- Pereira and Schabes 1992 -RRB- and models with hidden variables -LRB- Koo and Collins 2005 Wang et al. 2007 Blunsom et al. 2008 -RRB- require summing over the scores of many structures to calculate marginals	dobj_calculate_marginals aux_calculate_to amod_structures_many prep_of_scores_structures det_scores_the prep_over_summing_scores xcomp_require_calculate dobj_require_summing nsubj_require_models nsubj_require_models nsubj_require_algorithms advmod_require_Meanwhile num_Blunsom_2008 nn_Blunsom_al. nn_Blunsom_et num_Wang_2007 nn_Wang_al. nn_Wang_et dep_Koo_Blunsom conj_and_Koo_Wang conj_and_Koo_2005 conj_and_Koo_Collins appos_variables_Wang appos_variables_2005 appos_variables_Collins appos_variables_Koo amod_variables_hidden prep_with_models_variables dep_Pereira_1992 conj_and_Pereira_Schabes appos_models_Schabes appos_models_Pereira amod_models_unsupervised amod_Lafferty_2001 dep_Lafferty_al. nn_Lafferty_et amod_models_log-linear amod_models_conditional prep_for_likelihood_models nn_likelihood_maximum conj_and_algorithms_models conj_and_algorithms_models dep_algorithms_Lafferty prep_like_algorithms_likelihood nn_algorithms_learning det_algorithms_some
P06-1096	H05-1064	o	Discriminative training with hidden variables has been handled in this probabilistic framework -LRB- Quattoni et al. 2004 Koo and Collins 2005 -RRB- but we choose Equation 3 for efficiency	prep_for_Equation_efficiency num_Equation_3 dobj_choose_Equation nsubj_choose_we num_Koo_2005 conj_and_Koo_Collins dep_Quattoni_Collins dep_Quattoni_Koo appos_Quattoni_2004 dep_Quattoni_al. nn_Quattoni_et amod_framework_probabilistic det_framework_this conj_but_handled_choose dep_handled_Quattoni prep_in_handled_framework auxpass_handled_been aux_handled_has nsubjpass_handled_training amod_variables_hidden prep_with_training_variables amod_training_Discriminative
P07-1078	H05-1064	o	A reranking parser -LRB- see also -LRB- Koo and Collins 2005 -RRB- -RRB- is a layered model the base layer is a generative statistical PCFG parser that creates a ranked list of k parses -LRB- say 50 -RRB- and the second layer is a reranker that reorders these parses using more detailed features	amod_features_detailed amod_features_more dobj_using_features det_parses_these xcomp_reorders_using dobj_reorders_parses nsubj_reorders_that rcmod_reranker_reorders det_reranker_a cop_reranker_is nsubj_reranker_layer amod_layer_second det_layer_the dep_say_50 dep_parses_say prep_of_list_k amod_list_ranked det_list_a dobj_creates_list nsubj_creates_that conj_and_parser_reranker dep_parser_parses rcmod_parser_creates nn_parser_PCFG amod_parser_statistical amod_parser_generative det_parser_a cop_parser_is nsubj_parser_layer nn_layer_base det_layer_the dep_model_reranker dep_model_parser amod_model_layered det_model_a cop_model_is nsubj_model_parser dep_Koo_2005 conj_and_Koo_Collins dep_see_Collins dep_see_Koo advmod_see_also dep_parser_see nn_parser_reranking det_parser_A
P07-1078	H05-1064	p	1 Introduction State of the art statistical parsers -LRB- Collins 1999 Charniak 2000 Koo and Collins 2005 Charniak and Johnson 2005 -RRB- are trained on manually annotated treebanks that are highly expensive to create	aux_create_to xcomp_expensive_create advmod_expensive_highly cop_expensive_are nsubj_expensive_that rcmod_treebanks_expensive amod_treebanks_annotated advmod_annotated_manually prep_on_trained_treebanks auxpass_trained_are nsubjpass_trained_State dep_Koo_2005 conj_and_Koo_Johnson conj_and_Koo_Charniak conj_and_Koo_2005 conj_and_Koo_Collins num_Charniak_2000 dep_Collins_Johnson dep_Collins_Charniak dep_Collins_2005 dep_Collins_Collins dep_Collins_Koo conj_Collins_Charniak amod_Collins_1999 appos_parsers_Collins amod_parsers_statistical nn_parsers_art det_parsers_the prep_of_State_parsers nn_State_Introduction num_State_1
P07-1080	H05-1064	o	5 Related Work There has not been much previous work on graphical models for full parsing although recently several latent variable models for parsing have been proposed -LRB- Koo and Collins 2005 Matsuzaki et al. 2005 Riezler et al. 2002 -RRB-	num_Riezler_2002 nn_Riezler_al. nn_Riezler_et num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Koo_Riezler conj_and_Koo_Matsuzaki conj_and_Koo_2005 conj_and_Koo_Collins dep_proposed_Matsuzaki dep_proposed_2005 dep_proposed_Collins dep_proposed_Koo auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models mark_proposed_although prepc_for_models_parsing amod_models_variable amod_models_latent amod_models_several advmod_models_recently amod_parsing_full amod_models_graphical prep_for_work_parsing prep_on_work_models amod_work_previous amod_work_much cop_work_been neg_work_not aux_work_has expl_work_There advcl_Work_proposed rcmod_Work_work amod_Work_Related num_Work_5 dep_``_Work
P07-1080	H05-1064	o	In -LRB- Koo and Collins 2005 -RRB- an undirected graphical model is used for parse reranking	nn_reranking_parse prep_for_used_reranking auxpass_used_is nsubjpass_used_model prep_used_In amod_model_graphical amod_model_undirected det_model_an dep_Koo_2005 conj_and_Koo_Collins dep_In_Collins dep_In_Koo
P07-1080	H05-1064	o	-LRB- Koo and Collins 2005 Matsuzaki et al. 2005 Riezler et al. 2002 -RRB- -RRB-	num_Riezler_2002 nn_Riezler_al. nn_Riezler_et dep_Matsuzaki_Riezler num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Koo_Matsuzaki appos_Koo_2005 conj_and_Koo_Collins dep_''_Collins dep_''_Koo
P08-1068	H05-1064	o	Previous research in this area includes several models which incorporate hidden variables -LRB- Matsuzaki et al. 2005 Koo and Collins 2005 Petrov et al. 2006 Titov and Henderson 2007 -RRB-	num_Petrov_2006 nn_Petrov_al. nn_Petrov_et appos_Koo_2007 conj_and_Koo_Henderson conj_and_Koo_Titov conj_and_Koo_Petrov num_Koo_2005 conj_and_Koo_Collins dep_Matsuzaki_Henderson dep_Matsuzaki_Titov dep_Matsuzaki_Petrov dep_Matsuzaki_Collins dep_Matsuzaki_Koo appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et amod_variables_hidden dobj_incorporate_variables nsubj_incorporate_which rcmod_models_incorporate amod_models_several dep_includes_Matsuzaki dobj_includes_models nsubj_includes_research det_area_this prep_in_research_area amod_research_Previous
W06-1666	H05-1064	o	Use of probability estimates is not a serious limitation of this approach because in practice candidates are normally provided by some probabilistic model and its probability estimates are used as additional features in the reranker -LRB- Collins and Koo 2005 Shen and Joshi 2003 Henderson and Titov 2005 -RRB-	appos_Henderson_2005 conj_and_Henderson_Titov num_Shen_2003 conj_and_Shen_Joshi dep_Collins_Titov dep_Collins_Henderson conj_and_Collins_Joshi conj_and_Collins_Shen amod_Collins_2005 conj_and_Collins_Koo dep_reranker_Shen dep_reranker_Koo dep_reranker_Collins det_reranker_the prep_in_features_reranker amod_features_additional prep_as_used_features auxpass_used_are nn_estimates_probability poss_estimates_its conj_and_model_estimates amod_model_probabilistic det_model_some dep_provided_used agent_provided_estimates agent_provided_model advmod_provided_normally auxpass_provided_are prep_in_provided_candidates mark_provided_because nn_candidates_practice det_approach_this advcl_limitation_provided prep_of_limitation_approach amod_limitation_serious det_limitation_a neg_limitation_not cop_limitation_is nsubj_limitation_Use nn_estimates_probability prep_of_Use_estimates
W06-1666	H05-1064	o	1 Introduction The reranking approach is widely used in parsing -LRB- Collins and Koo 2005 Koo and Collins 2005 Henderson and Titov 2005 Shen and Joshi 2003 -RRB- as well as in other structured classification problems	nn_problems_classification amod_problems_structured amod_problems_other pobj_in_problems num_Shen_2003 conj_and_Shen_Joshi conj_and_Koo_in dep_Koo_Joshi dep_Koo_Shen conj_and_Koo_2005 conj_and_Koo_Titov conj_and_Koo_Henderson conj_and_Koo_2005 conj_and_Koo_Collins dep_Collins_in dep_Collins_2005 dep_Collins_Titov dep_Collins_Henderson dep_Collins_2005 dep_Collins_Collins dep_Collins_Koo amod_Collins_2005 conj_and_Collins_Koo dep_parsing_Koo dep_parsing_Collins prep_in_used_parsing advmod_used_widely auxpass_used_is nsubjpass_used_approach nn_approach_reranking det_approach_The rcmod_Introduction_used num_Introduction_1
W06-1670	H05-1064	p	In syntactic parse re-ranking supersenses have been used to build useful latent semantic features -LRB- Koo and Collins 2005 -RRB-	dep_Koo_2005 conj_and_Koo_Collins dep_features_Collins dep_features_Koo amod_features_semantic amod_features_latent amod_features_useful dobj_build_features aux_build_to xcomp_used_build auxpass_used_been aux_used_have prep_in_used_supersenses nn_supersenses_re-ranking nn_supersenses_parse amod_supersenses_syntactic
W06-2902	H05-1064	o	-LRB- Matsuzaki et al. 2005 Koo and Collins 2005 -RRB- -RRB-	dep_Koo_2005 conj_and_Koo_Collins dep_Matsuzaki_Collins dep_Matsuzaki_Koo appos_Matsuzaki_2005 dep_Matsuzaki_al. nn_Matsuzaki_et dep_''_Matsuzaki
W07-2217	H05-1064	o	Collins and Koo -LRB- Collins & Koo 2005 -RRB- introduced an improved reranking model for parsing which includes a hidden layer of semantic features	amod_features_semantic prep_of_layer_features amod_layer_hidden det_layer_a dobj_includes_layer nsubj_includes_which rcmod_parsing_includes prep_for_model_parsing nn_model_reranking amod_model_improved det_model_an dobj_introduced_model nsubj_introduced_Koo nsubj_introduced_Collins dep_Collins_2005 conj_and_Collins_Koo appos_Collins_Koo appos_Collins_Collins conj_and_Collins_Koo
W07-2218	H05-1064	o	Recently several latent variable models for constituent parsing have been proposed -LRB- Koo and Collins 2005 Matsuzaki et al. 2005 Prescher 2005 Riezler et al. 2002 -RRB-	num_Riezler_2002 nn_Riezler_al. nn_Riezler_et num_Prescher_2005 num_Matsuzaki_2005 nn_Matsuzaki_al. nn_Matsuzaki_et dep_Koo_Riezler conj_and_Koo_Prescher conj_and_Koo_Matsuzaki conj_and_Koo_2005 conj_and_Koo_Collins dep_proposed_Prescher dep_proposed_Matsuzaki dep_proposed_2005 dep_proposed_Collins dep_proposed_Koo auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models nn_parsing_constituent prep_for_models_parsing amod_models_variable amod_models_latent amod_models_several advmod_models_Recently
W07-2218	H05-1064	o	In -LRB- Koo and Collins 2005 -RRB- an undirected graphical model for constituent parse reranking uses dependency relations to define the edges	det_edges_the dobj_define_edges aux_define_to nn_relations_dependency vmod_uses_define dobj_uses_relations nsubj_uses_model prep_uses_In nn_reranking_parse amod_reranking_constituent prep_for_model_reranking amod_model_graphical amod_model_undirected det_model_an dep_Koo_2005 conj_and_Koo_Collins dep_In_Collins dep_In_Koo
C08-1121	H05-1083	o	Some researchers -LRB- Lappin and Leass 1994 Kennedy and Boguraev 1996 -RRB- use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun while others use features determined over the parse tree in a machine-learning approach -LRB- Aone and Bennett 1995 Yang et al. 2004 Luo and Zitouni 2005 -RRB-	num_Zitouni_2005 conj_and_Luo_Zitouni dep_Yang_2004 dep_Yang_al. nn_Yang_et num_Bennett_1995 dep_Aone_Zitouni dep_Aone_Luo conj_and_Aone_Yang conj_and_Aone_Bennett appos_approach_Yang appos_approach_Bennett appos_approach_Aone amod_approach_machine-learning det_approach_a nn_tree_parse det_tree_the prep_in_determined_approach prep_over_determined_tree vmod_features_determined dobj_use_features nsubj_use_others mark_use_while det_pronoun_the conj_and_candidate_pronoun det_candidate_the prep_between_relations_pronoun prep_between_relations_candidate amod_relations_governing det_relations_the amod_candidates_antecedent det_candidates_the conj_and_role_relations prep_of_role_candidates amod_role_grammatical det_role_the dobj_take_relations dobj_take_role prep_into_take_account aux_take_to amod_rules_designed advmod_designed_manually advcl_use_use vmod_use_take dobj_use_rules nsubj_use_researchers num_Boguraev_1996 conj_and_Kennedy_Boguraev num_Leass_1994 dep_Lappin_Boguraev dep_Lappin_Kennedy conj_and_Lappin_Leass appos_researchers_Leass appos_researchers_Lappin det_researchers_Some ccomp_``_use
D08-1031	H05-1083	o	-LRB- 2007 -RRB- and Luo and Zitouni -LRB- 2005 -RRB-	appos_Zitouni_2005 conj_and_Luo_Zitouni conj_and_2007_Zitouni conj_and_2007_Luo
N07-1010	H05-1083	o	For example syntactic features -LRB- Ng and Cardie 2002b Luo and Zitouni 2005 -RRB- can be computed this way and are used in our system	poss_system_our prep_in_used_system auxpass_used_are nsubjpass_used_features det_way_this conj_and_computed_used dobj_computed_way auxpass_computed_be aux_computed_can nsubjpass_computed_features prep_for_computed_example dep_Ng_2005 conj_and_Ng_Zitouni conj_and_Ng_Luo conj_and_Ng_2002b conj_and_Ng_Cardie appos_features_Zitouni appos_features_Luo appos_features_2002b appos_features_Cardie appos_features_Ng nn_features_syntactic rcmod_``_used rcmod_``_computed
N07-1010	H05-1083	o	See -LRB- Luo and Zitouni 2005 -RRB- and -LRB- Daume III and Marcu 2005 -RRB-	dep_III_2005 conj_and_III_Marcu nn_III_Daume dep_Luo_2005 conj_and_Luo_Zitouni conj_and_See_Marcu conj_and_See_III dep_See_Zitouni dep_See_Luo ccomp_``_III ccomp_``_See
N09-2051	H05-1083	o	The coreference resolution system employs a variety of lexical semantic distance and syntactic features -LRB- Luo et al. 2004 Luo and Zitouni 2005 -RRB-	dep_Luo_2005 conj_and_Luo_Zitouni dep_Luo_Zitouni dep_Luo_Luo appos_Luo_2004 dep_Luo_al. nn_Luo_et amod_features_syntactic amod_features_distance amod_features_semantic amod_features_lexical conj_and_lexical_syntactic conj_and_lexical_distance conj_and_lexical_semantic prep_of_variety_features det_variety_a dep_employs_Luo dobj_employs_variety nsubj_employs_system nn_system_resolution nn_system_coreference det_system_The
P06-1006	H05-1083	o	These features are calculated by mining the parse trees and then could be used for resolution by using manually designed rules -LRB- Lappin and Leass 1994 Kennedy and Boguraev 1996 Mitkov 1998 -RRB- or using machine-learning methods -LRB- Aone and Bennett 1995 Yang et al. 2004 Luo and Zitouni 2005 -RRB-	amod_Luo_2005 conj_and_Luo_Zitouni num_Yang_2004 nn_Yang_al. nn_Yang_et dep_Aone_Zitouni dep_Aone_Luo conj_and_Aone_Yang conj_and_Aone_1995 conj_and_Aone_Bennett appos_methods_Yang appos_methods_1995 appos_methods_Bennett appos_methods_Aone amod_methods_machine-learning dobj_using_methods dep_Mitkov_1998 conj_or_Kennedy_using conj_and_Kennedy_Mitkov conj_and_Kennedy_1996 conj_and_Kennedy_Boguraev dep_Lappin_using dep_Lappin_Mitkov dep_Lappin_1996 dep_Lappin_Boguraev dep_Lappin_Kennedy conj_and_Lappin_1994 conj_and_Lappin_Leass dep_rules_1994 dep_rules_Leass dep_rules_Lappin amod_rules_designed advmod_designed_manually dobj_using_rules agent_used_using prep_for_used_resolution auxpass_used_be aux_used_could advmod_used_then nsubjpass_used_features nn_trees_parse det_trees_the dobj_mining_trees conj_and_calculated_used agent_calculated_mining auxpass_calculated_are nsubjpass_calculated_features det_features_These
P06-1006	H05-1083	o	We also tested the flat syntactic feature set proposed in Luo and Zitouni -LRB- 2005 -RRB- s work	dobj_s_work appos_Zitouni_2005 conj_and_Luo_Zitouni prep_in_proposed_Zitouni prep_in_proposed_Luo dep_set_s vmod_set_proposed dep_feature_set nn_feature_syntactic amod_feature_flat det_feature_the dobj_tested_feature advmod_tested_also nsubj_tested_We
P06-1006	H05-1083	o	In line with the reports in -LRB- Luo and Zitouni 2005 -RRB- we do observe the performance improvement against the baseline -LRB- NORM -RRB- for all the domains	det_domains_the predet_domains_all appos_baseline_NORM det_baseline_the prep_for_improvement_domains prep_against_improvement_baseline nn_improvement_performance det_improvement_the dobj_observe_improvement aux_observe_do nsubj_observe_we prep_in_observe_line dep_Luo_2005 conj_and_Luo_Zitouni prep_in_reports_Zitouni prep_in_reports_Luo det_reports_the prep_with_line_reports ccomp_``_observe
P06-1006	H05-1083	o	Luo and Zitouni -LRB- 2005 -RRB- proposed a coreference resolution approach which also explores the information from the syntactic parse trees	nn_trees_parse amod_trees_syntactic det_trees_the prep_from_information_trees det_information_the dobj_explores_information advmod_explores_also nsubj_explores_which rcmod_approach_explores nn_approach_resolution nn_approach_coreference det_approach_a dobj_proposed_approach nsubj_proposed_Zitouni nsubj_proposed_Luo appos_Zitouni_2005 conj_and_Luo_Zitouni
I08-2116	H05-1087	o	The training methods of LRM-F and SVM-F were useful to improve the F M scores of LRM and SVM as reported in -LRB- Jansche 2005 Joachims 2005 -RRB-	amod_Joachims_2005 dep_Jansche_Joachims appos_Jansche_2005 prep_in_reported_Jansche mark_reported_as conj_and_LRM_SVM prep_of_scores_SVM prep_of_scores_LRM dep_M_scores nn_M_F det_M_the advcl_improve_reported dobj_improve_M aux_improve_to xcomp_useful_improve cop_useful_were nsubj_useful_methods conj_and_LRM-F_SVM-F prep_of_methods_SVM-F prep_of_methods_LRM-F nn_methods_training det_methods_The ccomp_``_useful
I08-2116	H05-1087	o	Recently methods for training binary classifiers to maximize the F 1 score have been proposed for SVM -LRB- Joachims 2005 -RRB- and LRM -LRB- Jansche 2005 -RRB-	amod_Jansche_2005 dep_LRM_Jansche amod_Joachims_2005 conj_and_SVM_LRM dep_SVM_Joachims prep_for_proposed_LRM prep_for_proposed_SVM auxpass_proposed_been aux_proposed_have nsubjpass_proposed_methods advmod_proposed_Recently dep_F_score num_F_1 det_F_the dobj_maximize_F aux_maximize_to vmod_classifiers_maximize amod_classifiers_binary nn_classifiers_training prep_for_methods_classifiers
I08-2116	H05-1087	o	To estimate combination weights we extend the F 1 score maximization training algorithm for LRM described in -LRB- Jansche 2005 -RRB-	amod_Jansche_2005 dep_in_Jansche prep_described_in vmod_LRM_described nn_algorithm_training nn_algorithm_maximization nn_algorithm_score prep_for_F_LRM dep_F_algorithm num_F_1 det_F_the dobj_extend_F nsubj_extend_we advcl_extend_estimate nn_weights_combination dobj_estimate_weights aux_estimate_To
I08-2116	H05-1087	o	2 F 1 score Maximization Training of LRM We first review the F 1 score maximization training method for linear models using a logistic function described in -LRB- Jansche 2005 -RRB-	amod_Jansche_2005 dep_in_Jansche prep_described_in vmod_function_described amod_function_logistic det_function_a dobj_using_function amod_models_linear nn_method_training nn_method_maximization nn_method_score vmod_F_using prep_for_F_models dep_F_method num_F_1 dep_the_F dobj_review_the advmod_review_first nsubj_review_We rcmod_LRM_review prep_of_Training_LRM nn_Training_Maximization nn_Training_score dep_F_Training num_F_1 num_F_2 dep_``_F
I08-2116	H05-1087	o	By contrast in the training method proposed by -LRB- Jansche 2005 -RRB- the discriminative function f -LRB- x w -RRB- is estimated to maximize the F 1 score of training dataset D This training method employs an approximate form of the F 1 score obtained by using a logistic function	amod_function_logistic det_function_a dobj_using_function agent_obtained_using vmod_score_obtained num_F_1 det_F_the dep_form_score prep_of_form_F amod_form_approximate det_form_an dobj_employs_form nsubj_employs_method prep_in_employs_method prep_by_employs_contrast nn_method_training det_method_This nn_D_dataset nn_D_training prep_of_score_D dep_F_score num_F_1 det_F_the dobj_maximize_F aux_maximize_to xcomp_estimated_maximize auxpass_estimated_is nsubjpass_estimated_Jansche mark_estimated_by dep_x_w nn_x_f dep_function_x amod_function_discriminative det_function_the appos_Jansche_function dep_Jansche_2005 advcl_proposed_estimated vmod_method_proposed nn_method_training det_method_the
I08-2116	H05-1087	o	C A and B are computed for training dataset D as C = summationtext M m = 1 y -LRB- m -RRB- y -LRB- m -RRB- A = summationtext M m = 1 y -LRB- m -RRB- and B = summationtext M m = 1 y -LRB- m -RRB- In -LRB- Jansche 2005 -RRB- y -LRB- m -RRB- was approximated by using the discriminative and logistic functions shown in Eqs	prep_in_shown_Eqs vmod_functions_shown amod_functions_logistic amod_functions_discriminative det_functions_the conj_and_discriminative_logistic dobj_using_functions agent_approximated_using auxpass_approximated_was nsubjpass_approximated_y prep_approximated_In appos_y_m amod_Jansche_2005 dep_In_Jansche appos_y_m num_y_1 dep_=_y amod_m_= nn_m_M nn_m_summationtext dobj_=_m amod_B_= appos_y_m num_y_1 dep_=_y conj_and_m_B amod_m_= nn_m_M nn_m_summationtext dobj_=_B dobj_=_m dep_A_= rcmod_y_approximated appos_y_A appos_y_m nn_y_m dep_y_y num_y_1 dep_=_y amod_m_= nn_m_M nn_m_summationtext dep_=_m amod_C_= nn_D_dataset nn_D_training prep_as_computed_C prep_for_computed_D auxpass_computed_are nsubjpass_computed_B nsubjpass_computed_A conj_and_A_B nn_A_C
P06-1028	H05-1087	o	Moreover an F-score optimization method for logistic regression has also been proposed -LRB- Jansche 2005 -RRB-	amod_Jansche_2005 dep_proposed_Jansche auxpass_proposed_been advmod_proposed_also aux_proposed_has nsubjpass_proposed_method advmod_proposed_Moreover amod_regression_logistic prep_for_method_regression nn_method_optimization nn_method_F-score det_method_an ccomp_``_proposed
P07-1093	H05-1087	o	-LRB- 2006 -RRB- and Jansche -LRB- 2005 -RRB- who discuss maximum expected F-score training of decision trees and logistic regression models	nn_models_regression amod_models_logistic nn_trees_decision conj_and_training_models prep_of_training_trees amod_training_F-score dobj_expected_models dobj_expected_training nsubj_expected_maximum ccomp_discuss_expected nsubj_discuss_who appos_Jansche_2005 rcmod_2006_discuss conj_and_2006_Jansche dep_''_Jansche dep_''_2006
P07-1093	H05-1087	o	For example the constrained optimization method of -LRB- Mozer et al. 2001 -RRB- relies on approximations of sensitivity -LRB- which they call CA -RRB- and specificity2 -LRB- their CR -RRB- related techniques -LRB- Gao et al. 2006 Jansche 2005 -RRB- rely on approximations of true positives false positives and false negatives and indirectly recall and precision	conj_and_recall_precision dep_indirectly_precision dep_indirectly_recall advmod_negatives_indirectly cc_negatives_and amod_negatives_false amod_positives_false amod_positives_true prep_of_approximations_positives conj_and_rely_negatives conj_and_rely_positives prep_on_rely_approximations nsubj_rely_techniques dep_Jansche_2005 dep_Gao_Jansche appos_Gao_2006 dep_Gao_al. nn_Gao_et dep_techniques_Gao amod_techniques_related poss_CR_their appos_specificity2_CR dobj_call_CA nsubj_call_they dobj_call_which conj_and_sensitivity_specificity2 dep_sensitivity_call prep_of_approximations_specificity2 prep_of_approximations_sensitivity parataxis_relies_negatives parataxis_relies_positives parataxis_relies_rely prep_on_relies_approximations nsubj_relies_method prep_for_relies_example amod_Mozer_2001 dep_Mozer_al. nn_Mozer_et prep_of_method_Mozer nn_method_optimization amod_method_constrained det_method_the
P07-1093	H05-1087	o	-LRB- 2001 -RRB- whose constrained optimization technique is similar to those in -LRB- Gao et al. 2006 Jansche 2005 -RRB-	amod_Jansche_2005 dep_Gao_Jansche appos_Gao_2006 dep_Gao_al. nn_Gao_et prep_in_those_Gao prep_to_similar_those cop_similar_is nsubj_similar_technique nn_technique_optimization amod_technique_constrained poss_technique_whose rcmod_2001_similar dep_''_2001
D07-1070	J04-3004	o	In his analysis of Yarowsky -LRB- 1995 -RRB- Abney -LRB- 2004 -RRB- formulates several variants of bootstrapping	prep_of_variants_bootstrapping amod_variants_several dobj_formulates_variants nsubj_formulates_Abney appos_Abney_2004 rcmod_Yarowsky_formulates appos_Yarowsky_1995 prep_of_analysis_Yarowsky poss_analysis_his pobj_In_analysis dep_``_In
D07-1070	J04-3004	o	Drawing on Abneys -LRB- 2004 -RRB- analysis of the Yarowsky algorithm we perform bootstrapping by entropy regularization we maximize a linear combination of conditional likelihood on labeled data and confidence -LRB- negative Renyi entropy -RRB- on unlabeled data	amod_data_unlabeled nn_entropy_Renyi amod_entropy_negative prep_on_data_data appos_data_entropy conj_and_data_confidence dobj_labeled_confidence dobj_labeled_data amod_likelihood_conditional prepc_on_combination_labeled prep_of_combination_likelihood amod_combination_linear det_combination_a dobj_maximize_combination nsubj_maximize_we nn_regularization_entropy prep_by_bootstrapping_regularization parataxis_perform_maximize xcomp_perform_bootstrapping nsubj_perform_we vmod_perform_Drawing nn_algorithm_Yarowsky det_algorithm_the prep_of_analysis_algorithm nn_analysis_Abneys appos_Abneys_2004 prep_on_Drawing_analysis
D08-1106	J04-3004	o	Abney -LRB- 2004 -RRB- presented a thorough discussion on the Yarowsky algorithm	nn_algorithm_Yarowsky det_algorithm_the prep_on_discussion_algorithm amod_discussion_thorough det_discussion_a dobj_presented_discussion nsubj_presented_Abney appos_Abney_2004
D09-1134	J04-3004	o	This approach however does not have a theoretical guarantee on optimality unless certain nontrivial conditions are satisfied -LRB- Abney 2004 -RRB-	amod_Abney_2004 dep_satisfied_Abney cop_satisfied_are nsubj_satisfied_conditions mark_satisfied_unless amod_conditions_nontrivial amod_conditions_certain prep_on_guarantee_optimality amod_guarantee_theoretical det_guarantee_a advcl_have_satisfied dobj_have_guarantee neg_have_not aux_have_does advmod_have_however nsubj_have_approach det_approach_This
P06-1027	J04-3004	o	5.1 Comparison to self-training For completeness we also compared our results to the self-learning algorithm which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of Yarowsky in word sense disambiguation -LRB- Abney 2004 Yarowsky 1995 -RRB-	num_Yarowsky_1995 dep_Abney_Yarowsky num_Abney_2004 appos_disambiguation_Abney nn_disambiguation_sense nn_disambiguation_word prep_of_work_Yarowsky det_work_the prep_in_popularized_disambiguation prep_by_popularized_work advmod_popularized_originally nsubjpass_popularized_which nn_processing_language amod_processing_natural prep_in_bootstrapping_processing conj_and_referred_popularized pobj_referred_bootstrapping prepc_as_to_referred_as auxpass_referred_been advmod_referred_commonly aux_referred_has nsubjpass_referred_which rcmod_algorithm_popularized rcmod_algorithm_referred amod_algorithm_self-learning det_algorithm_the prep_to_results_algorithm poss_results_our dobj_compared_results advmod_compared_also nsubj_compared_we tmod_compared_Comparison prep_for_Comparison_completeness prep_to_Comparison_self-training num_Comparison_5.1
P07-1004	J04-3004	o	3 The Framework 3.1 The Algorithm Our transductive learning algorithm Algorithm 1 is inspired by the Yarowsky algorithm -LRB- Yarowsky 1995 Abney 2004 -RRB-	amod_Abney_2004 dep_Yarowsky_Abney appos_Yarowsky_1995 dep_algorithm_Yarowsky nn_algorithm_Yarowsky det_algorithm_the agent_inspired_algorithm auxpass_inspired_is nsubjpass_inspired_algorithm num_Algorithm_1 appos_algorithm_Algorithm nn_algorithm_learning amod_algorithm_transductive poss_algorithm_Our nn_algorithm_Algorithm det_algorithm_The num_algorithm_3.1 dep_algorithm_Framework num_algorithm_3 det_Framework_The
P07-1004	J04-3004	o	Under certain precise conditions as described in -LRB- Abney 2004 -RRB- we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U. However this is true only when the functions Estimate Score and Select have very prescribed definitions	amod_definitions_prescribed advmod_prescribed_very dobj_have_definitions nsubj_have_Select nsubj_have_Score nsubj_have_Estimate advmod_have_when conj_and_Estimate_Select conj_and_Estimate_Score nn_Estimate_functions det_Estimate_the advmod_when_only advcl_true_have cop_true_is nsubj_true_this prep_of_translations_U. det_distribution_the prep_of_entropy_distribution det_entropy_the advmod_minimizing_However prep_over_minimizing_translations dobj_minimizing_entropy num_Algorithm_1 ccomp_analyze_true prepc_as_analyze_minimizing dobj_analyze_Algorithm aux_analyze_can nsubj_analyze_we advcl_analyze_described prep_under_analyze_conditions amod_Abney_2004 dep_in_Abney prep_described_in mark_described_as amod_conditions_precise amod_conditions_certain
P08-1061	J04-3004	o	More recently Haffari and Sarkar -LRB- 2007 -RRB- have extended the work of Abney -LRB- 2004 -RRB- and given a better mathematical understanding of self-training algorithms	amod_algorithms_self-training prep_of_understanding_algorithms amod_understanding_mathematical amod_understanding_better det_understanding_a pobj_given_understanding nsubj_given_Haffari appos_Abney_2004 prep_of_work_Abney det_work_the conj_and_extended_given dobj_extended_work aux_extended_have nsubj_extended_Sarkar nsubj_extended_Haffari advmod_extended_recently appos_Sarkar_2007 conj_and_Haffari_Sarkar advmod_recently_More
W06-2207	J04-3004	o	Although a rich literature covers bootstrapping methods applied to natural language problems -LRB- Yarowsky 1995 Riloff 1996 Collins and Singer 1999 Yangarber et al. 2000 Yangarber 2003 Abney 2004 -RRB- several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition	nn_acquisition_pattern amod_acquisition_semantic amod_acquisition_syntactic conj_or_syntactic_semantic prep_to_applied_acquisition auxpass_applied_are nsubjpass_applied_methods advmod_applied_when det_methods_these advcl_remain_applied acomp_remain_unanswered nsubj_remain_questions amod_questions_several dep_Abney_2004 num_Yangarber_2003 num_Yangarber_2000 nn_Yangarber_al. nn_Yangarber_et dep_Riloff_remain dep_Riloff_Abney conj_and_Riloff_Yangarber conj_and_Riloff_Yangarber num_Riloff_1999 conj_and_Riloff_Singer conj_and_Riloff_Collins conj_and_Riloff_1996 dep_Yarowsky_Yangarber dep_Yarowsky_Yangarber dep_Yarowsky_Singer dep_Yarowsky_Collins dep_Yarowsky_1996 dep_Yarowsky_Riloff appos_Yarowsky_1995 dep_problems_Yarowsky nn_problems_language amod_problems_natural prep_to_applied_problems nsubj_applied_methods nn_methods_bootstrapping ccomp_covers_applied nsubj_covers_literature mark_covers_Although amod_literature_rich det_literature_a advcl_``_covers
W06-2207	J04-3004	o	Previous studies called the class of algorithms illustrated in Figure 2 cautious or sequential because in each iteration they acquire 1 or a small set of rules -LRB- Abney 2004 Collins and Singer 1999 -RRB-	amod_Collins_1999 conj_and_Collins_Singer dep_Abney_Singer dep_Abney_Collins dep_Abney_2004 prep_of_set_rules amod_set_small det_set_a conj_or_1_set dobj_acquire_set dobj_acquire_1 nsubj_acquire_they prep_in_acquire_iteration mark_acquire_because det_iteration_each advcl_cautious_acquire conj_or_cautious_sequential dep_2_Abney amod_2_sequential amod_2_cautious dobj_Figure_2 prep_in_illustrated_Figure nsubj_illustrated_class prep_of_class_algorithms det_class_the dep_called_illustrated nsubj_called_studies amod_studies_Previous
W06-2207	J04-3004	o	Thispaperfocusesontheframeworkintroduced in Figure 2 for two reasons -LRB- a -RRB- cautious al50 gorithms were shown to perform best for several NLP problems -LRB- including acquisition of IE patterns -RRB- and -LRB- b -RRB- it has nice theoretical properties Abney -LRB- 2004 -RRB- showed that regardless of the selection procedure sequential bootstrapping algorithms converge to a local minimum of K where K is an upper bound of the negative log likelihood of the data	det_data_the prep_of_likelihood_data nn_likelihood_log amod_likelihood_negative det_likelihood_the prep_of_bound_likelihood amod_bound_upper det_bound_an auxpass_bound_is nsubjpass_bound_K advmod_bound_where rcmod_K_bound prep_of_minimum_K amod_minimum_local det_minimum_a prep_to_converge_minimum nsubj_converge_algorithms prep_regardless_of_converge_procedure nsubj_converge_that nn_algorithms_bootstrapping amod_algorithms_sequential nn_procedure_selection det_procedure_the ccomp_showed_converge nsubj_showed_Abney appos_Abney_2004 amod_properties_theoretical amod_properties_nice parataxis_has_showed dobj_has_properties nsubj_has_it dep_has_b amod_patterns_IE prep_of_acquisition_patterns prep_including_problems_acquisition nn_problems_NLP amod_problems_several prep_for_perform_problems dobj_perform_best aux_perform_to conj_and_shown_has xcomp_shown_perform auxpass_shown_were nsubjpass_shown_gorithms nn_gorithms_al50 amod_gorithms_cautious dep_gorithms_a num_reasons_two num_Figure_2 dep_Thispaperfocusesontheframeworkintroduced_has dep_Thispaperfocusesontheframeworkintroduced_shown prep_for_Thispaperfocusesontheframeworkintroduced_reasons prep_in_Thispaperfocusesontheframeworkintroduced_Figure
W07-2060	J04-3004	n	Unsupervised methods have been developed for WSD but despite modest success have not always been well understood statistically -LRB- Abney 2004 -RRB-	amod_Abney_2004 dep_statistically_Abney advmod_understood_statistically advmod_understood_well auxpass_understood_been advmod_understood_always neg_understood_not aux_understood_have nsubjpass_understood_success mark_understood_despite amod_success_modest conj_but_developed_understood prep_for_developed_WSD auxpass_developed_been aux_developed_have nsubjpass_developed_methods amod_methods_Unsupervised
C08-1082	J05-4002	o	Many 649 similarity measures and weighting functions have been proposed for distributional vectors comparative studies include Lee -LRB- 1999 -RRB- Curran -LRB- 2003 -RRB- and Weeds and Weir -LRB- 2005 -RRB-	appos_Weir_2005 appos_Curran_2003 conj_and_Lee_Weir conj_and_Lee_Weeds conj_and_Lee_Curran appos_Lee_1999 dobj_include_Weir dobj_include_Weeds dobj_include_Curran dobj_include_Lee nsubj_include_studies amod_studies_comparative amod_vectors_distributional parataxis_proposed_include prep_for_proposed_vectors auxpass_proposed_been aux_proposed_have nsubjpass_proposed_functions nsubjpass_proposed_measures nn_functions_weighting conj_and_measures_functions nn_measures_similarity num_measures_649 amod_measures_Many ccomp_``_proposed
C08-1082	J05-4002	o	The linear kernel derived from the L1 distance is the same as the difference-weighted token-based similarity measure of Weeds and Weir -LRB- 2005 -RRB-	appos_Weir_2005 conj_and_Weeds_Weir prep_of_measure_Weir prep_of_measure_Weeds nn_measure_similarity amod_measure_token-based amod_measure_difference-weighted det_measure_the prep_as_same_measure det_same_the cop_same_is nsubj_same_kernel nn_distance_L1 det_distance_the prep_from_derived_distance vmod_kernel_derived amod_kernel_linear det_kernel_The
D07-1052	J05-4002	o	Weeds and Weir -LRB- 2005 -RRB- discuss the influence of bias towards highor low-frequency items for different tasks -LRB- correlation with WordNet-derived neighbour sets and pseudoword disambiguation -RRB- and it would not be surprising if the different high-frequency bias were leading to different results	amod_results_different prep_to_leading_results aux_leading_were nsubj_leading_different mark_leading_if dobj_high-frequency_bias dep_different_high-frequency det_different_the advcl_surprising_leading cop_surprising_be neg_surprising_not aux_surprising_would nsubj_surprising_it nn_disambiguation_pseudoword conj_and_sets_disambiguation nn_sets_neighbour amod_sets_WordNet-derived prep_with_correlation_disambiguation prep_with_correlation_sets amod_tasks_different nn_items_low-frequency nn_items_highor prep_towards_bias_items prep_for_influence_tasks prep_of_influence_bias det_influence_the conj_and_discuss_surprising dep_discuss_correlation dobj_discuss_influence nsubj_discuss_Weir nsubj_discuss_Weeds appos_Weir_2005 conj_and_Weeds_Weir
D07-1052	J05-4002	o	See Weeds and Weir -LRB- 2005 -RRB- for an overview of other measures	amod_measures_other prep_of_overview_measures det_overview_an appos_Weir_2005 conj_and_Weeds_Weir prep_for_See_overview dobj_See_Weir dobj_See_Weeds
D07-1061	J05-4002	o	A variety of other measures of semantic relatedness have been proposed including distributional similarity measures based on co-occurrence in a body of text see -LRB- Weeds and Weir 2005 -RRB- for a survey	det_survey_a dep_Weeds_2005 conj_and_Weeds_Weir prep_for_see_survey dep_see_Weir dep_see_Weeds prep_of_body_text det_body_a prep_in_co-occurrence_body prep_on_based_co-occurrence vmod_measures_based nn_measures_similarity amod_measures_distributional dep_proposed_see prep_including_proposed_measures auxpass_proposed_been aux_proposed_have nsubjpass_proposed_variety amod_relatedness_semantic prep_of_measures_relatedness amod_measures_other prep_of_variety_measures det_variety_A
J06-1003	J05-4002	o	Formally by distributional similarity -LRB- or co-occurrence similarity -RRB- of two words w 1 and w 2 we mean that they tend to occur in similar contexts for some definition of context or that the set of words that w 1 tends to co-occur with is similar to the set that w 2 tends to co-occur with or that if w 1 is substituted for w 2 in a context its plausibility -LRB- Weeds 2003 Weeds and Weir 2005 -RRB- is unchanged	cop_unchanged_is nsubj_unchanged_plausibility advcl_unchanged_substituted mark_unchanged_that num_Weir_2005 conj_and_Weeds_Weir dep_Weeds_Weir dep_Weeds_Weeds num_Weeds_2003 dep_plausibility_Weeds poss_plausibility_its det_context_a num_w_2 prep_in_substituted_context prep_for_substituted_w auxpass_substituted_is nsubjpass_substituted_w mark_substituted_if num_w_1 prep_co-occur_with aux_co-occur_to xcomp_tends_co-occur nsubj_tends_w mark_tends_that num_w_2 ccomp_set_tends vmod_the_set prep_to_similar_the cop_similar_is mark_similar_with advcl_co-occur_similar aux_co-occur_to xcomp_tends_co-occur nsubj_tends_set mark_tends_that dobj_w_1 nsubj_w_that rcmod_words_w prep_of_set_words det_set_the prep_of_definition_context det_definition_some amod_contexts_similar prep_in_occur_contexts aux_occur_to prep_for_tend_definition xcomp_tend_occur nsubj_tend_they mark_tend_that conj_or_mean_unchanged conj_or_mean_tends ccomp_mean_tend nsubj_mean_we prep_by_mean_similarity advmod_mean_Formally num_w_2 conj_and_w_w num_w_1 dep_words_w dep_words_w dep_two_words nn_similarity_co-occurrence cc_similarity_or prep_of_similarity_two dep_similarity_similarity amod_similarity_distributional
J06-1003	J05-4002	o	For example Weeds -LRB- 2003 Weeds and Weir 2005 -RRB- -LRB- see below -RRB- took verbs as contexts for nouns in object position so they regarded two nouns to be similar to the extent that they occur as direct objects of the same set of verbs	prep_of_set_verbs amod_set_same det_set_the prep_of_objects_set amod_objects_direct prep_as_occur_objects nsubj_occur_they mark_occur_that ccomp_extent_occur det_extent_the prep_to_similar_extent cop_similar_be aux_similar_to num_nouns_two xcomp_regarded_similar dobj_regarded_nouns nsubj_regarded_they mark_regarded_so nn_position_object prep_in_nouns_position prep_for_contexts_nouns advcl_took_regarded prep_as_took_contexts dobj_took_verbs nsubj_took_Weeds prep_for_took_example prep_see_below dep_2003_2005 conj_and_2003_Weir conj_and_2003_Weeds dep_Weeds_see dep_Weeds_Weir dep_Weeds_Weeds dep_Weeds_2003
J06-1003	J05-4002	o	If distributional similarity is conceived of as substitutability as Weeds and Weir -LRB- 2005 -RRB- and Lee -LRB- 1999 -RRB- emphasize then asymmetries arise when one word appears in a subset of the contexts in which the other appears for example the adjectives that typically modify apple are a subset of those that modify fruit sofruit substitutes for apple better than apple substitutes for fruit	prep_for_substitutes_fruit nsubj_substitutes_apple mark_substitutes_than ccomp_better_substitutes amod_apple_better prep_for_substitutes_apple amod_substitutes_sofruit dobj_modify_fruit nsubj_modify_that rcmod_those_modify appos_subset_substitutes prep_of_subset_those det_subset_a cop_subset_are nsubj_subset_adjectives prep_for_subset_example dobj_modify_apple advmod_modify_typically nsubj_modify_that rcmod_adjectives_modify det_adjectives_the nsubj_appears_other prep_in_appears_which det_other_the rcmod_contexts_appears det_contexts_the prep_of_subset_contexts det_subset_a prep_in_appears_subset nsubj_appears_word advmod_appears_when num_word_one parataxis_arise_subset advcl_arise_appears nsubj_arise_asymmetries advmod_arise_then parataxis_arise_emphasize advcl_arise_conceived prep_as_emphasize_Lee prep_as_emphasize_Weir prep_as_emphasize_Weeds appos_Lee_1999 appos_Weir_2005 conj_and_Weeds_Lee conj_and_Weeds_Weir pobj_conceived_substitutability prepc_as_of_conceived_as auxpass_conceived_is nsubjpass_conceived_similarity mark_conceived_If amod_similarity_distributional
J07-4005	J05-4002	p	However the study of Weeds and Weir -LRB- 2005 -RRB- provides interesting insights into what makes a good distributional similarity measure in the contexts of semantic similarity prediction and language modeling	nn_modeling_language conj_and_prediction_modeling nn_prediction_similarity amod_prediction_semantic prep_of_contexts_modeling prep_of_contexts_prediction det_contexts_the nn_measure_similarity amod_measure_distributional amod_measure_good det_measure_a prep_in_makes_contexts dobj_makes_measure nsubj_makes_what prepc_into_insights_makes amod_insights_interesting dobj_provides_insights nsubj_provides_study advmod_provides_However appos_Weir_2005 conj_and_Weeds_Weir prep_of_study_Weir prep_of_study_Weeds det_study_the
J07-4005	J05-4002	o	Further it has been shown -LRB- Weeds et al. 2005 Weeds and Weir 2005 -RRB- that performance of Lins distributional similarity score decreases more significantly than other measures for low frequency nouns	nn_nouns_frequency amod_nouns_low prep_for_measures_nouns amod_measures_other prep_than_significantly_measures advmod_significantly_more advmod_decreases_significantly nsubj_decreases_performance dobj_decreases_that nn_score_similarity amod_score_distributional nn_score_Lins prep_of_performance_score num_Weir_2005 conj_and_Weeds_Weir nn_al._et dep_Weeds_Weir dep_Weeds_Weeds dep_Weeds_2005 dep_Weeds_al. rcmod_shown_decreases dep_shown_Weeds auxpass_shown_been aux_shown_has nsubjpass_shown_it advmod_shown_Further
N07-1024	J05-4002	o	We then compute the weight of a context word w in context c W -LRB- w c -RRB- using mutual information and t-test which were reported by Weeds and Weir -LRB- 2005 -RRB- to perform the best on a pseudo-disambiguation task	nn_task_pseudo-disambiguation det_task_a det_best_the prep_on_perform_task dobj_perform_best aux_perform_to appos_Weir_2005 conj_and_Weeds_Weir xcomp_reported_perform agent_reported_Weir agent_reported_Weeds auxpass_reported_were nsubjpass_reported_which rcmod_information_reported conj_and_information_t-test amod_information_mutual dobj_using_t-test dobj_using_information appos_w_c vmod_W_using dep_W_w nn_c_context nn_w_word nn_w_context det_w_a prep_of_weight_w det_weight_the parataxis_compute_W prep_in_compute_c dobj_compute_weight advmod_compute_then nsubj_compute_We ccomp_``_compute
P06-1046	J05-4002	o	This is important when LARGE CUT-OFF 0 5 100 NAIVE 541,721 184,493 35,617 SASH 10,599 8,796 6,231 INDEX 5,844 13,187 32,663 Table 4 Average number of comparisons per term considering that different tasks may require different weights and measures -LRB- Weeds and Weir 2005 -RRB-	amod_Weeds_2005 conj_and_Weeds_Weir dep_weights_Weir dep_weights_Weeds conj_and_weights_measures amod_weights_different dobj_require_measures dobj_require_weights aux_require_may nsubj_require_tasks mark_require_that amod_tasks_different ccomp_considering_require prep_per_comparisons_term vmod_number_considering prep_of_number_comparisons amod_number_Average dep_4_Table number_4_10,599 num_Table_32,663 num_Table_13,187 num_Table_5,844 nn_Table_INDEX dep_Table_6,231 number_6,231_8,796 num_SASH_4 dep_35,617_SASH dep_184,493_35,617 number_184,493_541,721 dep_NAIVE_184,493 dep_100_NAIVE dep_5_100 number_5_0 dep_CUT-OFF_number dep_CUT-OFF_5 nn_CUT-OFF_LARGE dep_when_CUT-OFF dep_important_when cop_important_is nsubj_important_This ccomp_``_important
P07-2011	J05-4002	o	Following initial work by -LRB- Sparck Jones 1964 -RRB- and -LRB- Grefenstette 1994 -RRB- an early online distributional thesaurus presented in -LRB- Lin 1998 -RRB- has been widely used and cited and numerous authors since have explored thesaurus properties and parameters see survey component of -LRB- Weeds and Weir 2005 -RRB-	amod_Weeds_2005 conj_and_Weeds_Weir dep_of_Weir dep_of_Weeds prep_component_of nn_component_survey dobj_see_component dep_properties_see conj_and_properties_parameters nn_properties_thesaurus dobj_explored_parameters dobj_explored_properties aux_explored_have prepc_since_authors_explored amod_authors_numerous nsubjpass_cited_Jones conj_and_used_authors conj_and_used_cited advmod_used_widely auxpass_used_been aux_used_has nsubjpass_used_Grefenstette nsubjpass_used_Jones mark_used_by amod_Lin_1998 prep_in_presented_Lin vmod_thesaurus_presented amod_thesaurus_distributional amod_thesaurus_online amod_thesaurus_early det_thesaurus_an dep_Grefenstette_1994 appos_Jones_thesaurus conj_and_Jones_Grefenstette amod_Jones_1964 amod_Jones_Sparck advcl_work_authors advcl_work_cited advcl_work_used amod_work_initial pobj_Following_work ccomp_``_Following
P07-2011	J05-4002	p	It is explored extensively in -LRB- Curran 2004 Weeds and Weir 2005 -RRB-	amod_Weeds_2005 conj_and_Weeds_Weir dep_Curran_Weir dep_Curran_Weeds appos_Curran_2004 prep_in_explored_Curran advmod_explored_extensively auxpass_explored_is nsubjpass_explored_It
W06-1104	J05-4002	o	2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed e.g. dictionary-based -LRB- Lesk 1986 -RRB- ontology-based -LRB- Wu and Palmer 1994 Leacock and Chodorow 1998 -RRB- information-based -LRB- Resnik 1995 Jiang and Conrath 1997 -RRB- or distributional -LRB- Weeds and Weir 2005 -RRB-	dep_Weeds_2005 conj_and_Weeds_Weir dep_Jiang_1997 conj_and_Jiang_Conrath dep_Resnik_Weir dep_Resnik_Weeds conj_or_Resnik_distributional dep_Resnik_Conrath dep_Resnik_Jiang appos_Resnik_1995 dep_information-based_distributional dep_information-based_Resnik dep_Wu_1998 conj_and_Wu_Chodorow conj_and_Wu_Leacock conj_and_Wu_1994 conj_and_Wu_Palmer dep_ontology-based_Chodorow dep_ontology-based_Leacock dep_ontology-based_1994 dep_ontology-based_Palmer dep_ontology-based_Wu dep_Lesk_1986 conj_dictionary-based_information-based conj_dictionary-based_ontology-based dep_dictionary-based_Lesk dep_e.g._dictionary-based prep_proposed_e.g. auxpass_proposed_been aux_proposed_have nsubjpass_proposed_approaches conj_or_words_concepts prep_of_relatedness_concepts prep_of_relatedness_words amod_relatedness_semantic amod_relatedness_computing prep_for_approaches_relatedness amod_approaches_Various rcmod_measures_proposed amod_measures_SR dobj_Evaluating_measures vmod_2_Evaluating ccomp_``_2
W08-2005	J05-4002	o	The earliest work in this direction are those of -LRB- Hindle 1990 -RRB- -LRB- Lin 1998 -RRB- -LRB- Dagan et al. 1999 -RRB- -LRB- Chen and Chen 2000 -RRB- -LRB- Geffet and Dagan 2004 -RRB- and -LRB- Weeds and Weir 2005 -RRB-	dep_Weeds_2005 conj_and_Weeds_Weir conj_and_Geffet_Dagan amod_Chen_2000 conj_and_Chen_Chen amod_Dagan_1999 dep_Dagan_al. nn_Dagan_et amod_Lin_1998 conj_and_Hindle_Weir conj_and_Hindle_Weeds dep_Hindle_2004 appos_Hindle_Dagan appos_Hindle_Geffet appos_Hindle_Chen appos_Hindle_Chen appos_Hindle_Dagan appos_Hindle_Lin dep_Hindle_1990 prep_of_those_Weeds prep_of_those_Hindle cop_those_are nsubj_those_work det_direction_this prep_in_work_direction amod_work_earliest det_work_The ccomp_``_those
W09-1706	J05-4002	o	They generally perform less well on low-frequency words -LRB- Weeds and Weir 2005 van der Plas 2008 -RRB-	amod_Plas_2008 nn_Plas_der nn_Plas_van conj_and_Weeds_Plas conj_and_Weeds_2005 conj_and_Weeds_Weir dep_words_Plas dep_words_2005 dep_words_Weir dep_words_Weeds nn_words_low-frequency advmod_well_less prep_on_perform_words advmod_perform_well advmod_perform_generally nsubj_perform_They
D08-1057	J05-4004	o	This upper bound is consistent with the upper limit of 50 % found by Daume III and Marcu -LRB- 2005 -RRB- which takes into account stemming differences	amod_differences_stemming nn_differences_account prep_into_takes_differences nsubj_takes_which appos_Marcu_2005 rcmod_III_takes conj_and_III_Marcu nn_III_Daume agent_found_Marcu agent_found_III vmod_%_found num_%_50 prep_of_limit_% amod_limit_upper det_limit_the prep_with_consistent_limit cop_consistent_is nsubj_consistent_bound amod_bound_upper det_bound_This
D08-1057	J05-4004	o	The closest work is that of Jing and McKeown -LRB- 1999 -RRB- and Daume III and Marcu -LRB- 2005 -RRB- in which multiple sentences are processed with fragments within them being recycled to generate the novel generated text	amod_text_generated amod_text_novel det_text_the dobj_generate_text aux_generate_to xcomp_recycled_generate auxpass_recycled_being nsubjpass_recycled_fragments prep_within_fragments_them prepc_with_processed_recycled auxpass_processed_are nsubjpass_processed_sentences prep_in_processed_which amod_sentences_multiple appos_Marcu_2005 nn_III_Daume dep_McKeown_1999 conj_and_Jing_Marcu conj_and_Jing_III conj_and_Jing_McKeown rcmod_of_processed pobj_of_Marcu pobj_of_III pobj_of_McKeown pobj_of_Jing dep_that_of prep_is_that nsubj_is_work amod_work_closest det_work_The ccomp_``_is
D08-1057	J05-4004	o	Daume III and Marcu -LRB- 2005 -RRB- propose a model that encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle	aux_recycle_to conj_and_words_phrases xcomp_reach_recycle dobj_reach_phrases dobj_reach_words aux_reach_to xcomp_skipped_reach auxpass_skipped_are nsubjpass_skipped_spans mark_skipped_that prep_of_spans_text amod_spans_sized amod_spans_different ccomp_is_skipped nsubj_is_it dep_is_likely advmod_likely_how ccomp_encodes_is nsubj_encodes_that rcmod_model_encodes det_model_a dobj_propose_model nsubj_propose_Marcu nsubj_propose_III appos_Marcu_2005 conj_and_III_Marcu nn_III_Daume
C08-1082	J06-3003	o	3.2 Compound Noun Interpretation The task of interpreting the semantics of noun compounds is one which has recently received considerable attention -LRB- Lauer 1995 Girju et al. 2005 Turney 2006 -RRB-	amod_Turney_2006 dep_Girju_Turney num_Girju_2005 nn_Girju_al. nn_Girju_et dep_Lauer_Girju appos_Lauer_1995 dep_attention_Lauer amod_attention_considerable dobj_received_attention advmod_received_recently aux_received_has nsubj_received_which rcmod_one_received cop_one_is nsubj_one_task nn_compounds_noun prep_of_semantics_compounds det_semantics_the dobj_interpreting_semantics prepc_of_task_interpreting det_task_The dep_Interpretation_one nn_Interpretation_Noun nn_Interpretation_Compound num_Interpretation_3.2 ccomp_``_Interpretation
C08-1114	J06-3003	p	The best previous result is an accuracy of 56.1 % -LRB- Turney 2006 -RRB-	dep_Turney_2006 appos_%_Turney num_%_56.1 prep_of_accuracy_% det_accuracy_an cop_accuracy_is nsubj_accuracy_result amod_result_previous amod_result_best det_result_The
C08-1114	J06-3003	o	The average senior high school student achieves 57 % correct -LRB- Turney 2006 -RRB-	dep_Turney_2006 dep_%_Turney amod_%_correct num_%_57 dobj_achieves_% nsubj_achieves_student nn_student_school amod_student_high amod_student_senior amod_student_average det_student_The
C08-1114	J06-3003	o	Turney -LRB- 2006 -RRB- later addressed the same problem using 8000 automatically generated patterns	amod_patterns_generated num_patterns_8000 advmod_generated_automatically dobj_using_patterns amod_problem_same det_problem_the xcomp_addressed_using dobj_addressed_problem advmod_addressed_later nsubj_addressed_Turney appos_Turney_2006
C08-1114	J06-3003	o	PairClass is most similar to the algorithm of Turney -LRB- 2006 -RRB- but it differs in the following ways PairClass does not use a lexicon to find synonyms for the input word pairs	nn_pairs_word nn_pairs_input det_pairs_the prep_for_synonyms_pairs dobj_find_synonyms aux_find_to det_lexicon_a vmod_use_find dobj_use_lexicon neg_use_not aux_use_does nsubj_use_PairClass amod_ways_following det_ways_the prep_in_differs_ways nsubj_differs_it appos_Turney_2006 prep_of_algorithm_Turney det_algorithm_the parataxis_similar_use conj_but_similar_differs prep_to_similar_algorithm advmod_similar_most cop_similar_is nsubj_similar_PairClass
C08-1114	J06-3003	o	PairClass generates probability estimates whereas Turney -LRB- 2006 -RRB- uses a cosine measure of similarity	prep_of_measure_similarity nn_measure_cosine det_measure_a dobj_uses_measure nsubj_uses_Turney mark_uses_whereas appos_Turney_2006 nn_estimates_probability advcl_generates_uses dobj_generates_estimates nsubj_generates_PairClass
C08-1114	J06-3003	n	The automatically generated patterns in PairClass are slightly more general than the patterns of Turney -LRB- 2006 -RRB-	appos_Turney_2006 prep_of_patterns_Turney det_patterns_the prep_than_general_patterns advmod_general_more cop_general_are nsubj_general_patterns advmod_more_slightly prep_in_patterns_PairClass amod_patterns_generated det_patterns_The advmod_generated_automatically
C08-1114	J06-3003	n	The morphological processing in PairClass -LRB- Minnen et al. 2001 -RRB- is more sophisticated than in Turney -LRB- 2006 -RRB-	appos_Turney_2006 pobj_in_Turney pcomp_than_in prep_sophisticated_than advmod_sophisticated_more cop_sophisticated_is nsubj_sophisticated_processing amod_Minnen_2001 dep_Minnen_al. nn_Minnen_et dep_processing_Minnen prep_in_processing_PairClass amod_processing_morphological det_processing_The
C08-1114	J06-3003	p	Veale -LRB- 2004 -RRB- used WordNet to answer 374 multiple-choice SAT analogy questions achieving an accuracy of 43 % but the best corpus-based approach attains an accuracy of 56 % -LRB- Turney 2006 -RRB-	dep_Turney_2006 appos_%_Turney num_%_56 prep_of_accuracy_% det_accuracy_an dobj_attains_accuracy nsubj_attains_approach amod_approach_corpus-based amod_approach_best det_approach_the num_%_43 prep_of_accuracy_% det_accuracy_an conj_but_achieving_attains dobj_achieving_accuracy ccomp_,_attains ccomp_,_achieving nn_questions_analogy nn_questions_SAT amod_questions_multiple-choice num_questions_374 dobj_answer_questions aux_answer_to vmod_WordNet_answer amod_WordNet_used nn_WordNet_Veale dep_Veale_2004 advcl_``_WordNet
C08-1114	J06-3003	o	The template we use here is similar to Turney -LRB- 2006 -RRB- but we have added extra context words before the X and after the Y Our morphological processing also differs from Turney -LRB- 2006 -RRB-	appos_Turney_2006 prep_from_differs_Turney advmod_differs_also nsubj_differs_processing amod_processing_morphological poss_processing_Our det_Y_the pobj_after_Y det_X_the nn_words_context amod_words_extra conj_and_added_after prep_before_added_X dobj_added_words aux_added_have nsubj_added_we appos_Turney_2006 parataxis_similar_differs conj_but_similar_after conj_but_similar_added prep_to_similar_Turney cop_similar_is nsubj_similar_template advmod_use_here nsubj_use_we rcmod_template_use det_template_The
C08-1114	J06-3003	o	Turney -LRB- 2006 -RRB- also selects patterns based on the number of pairs that generate them but the number of selected patterns is a constant -LRB- 8000 -RRB- independent of the number of input word pairs	nn_pairs_word nn_pairs_input prep_of_number_pairs det_number_the prep_of_independent_number dep_constant_independent dep_constant_8000 amod_a_constant nsubj_is_a amod_patterns_selected dep_number_is prep_of_number_patterns det_number_the dobj_generate_them nsubj_generate_that rcmod_pairs_generate prep_of_number_pairs det_number_the pobj_patterns_number prepc_based_on_patterns_on conj_but_selects_number dobj_selects_patterns advmod_selects_also nsubj_selects_Turney appos_Turney_2006 ccomp_``_number ccomp_``_selects
C08-1114	J06-3003	o	Turney -LRB- 2006 -RRB- used a corpus-based algorithm	amod_algorithm_corpus-based det_algorithm_a dobj_used_algorithm nsubj_used_Turney appos_Turney_2006
E09-1071	J06-3003	o	One such relational reasoning task is the problem of compound noun interpretation which has received a great deal of attention in recent years -LRB- Girju et al. 2005 Turney 2006 Butnariu and Veale 2008 -RRB-	dep_Butnariu_2008 conj_and_Butnariu_Veale dep_Turney_Veale dep_Turney_Butnariu num_Turney_2006 dep_Girju_Turney appos_Girju_2005 dep_Girju_al. nn_Girju_et amod_years_recent prep_of_deal_attention amod_deal_great det_deal_a dep_received_Girju prep_in_received_years dobj_received_deal aux_received_has nsubj_received_which nn_interpretation_noun nn_interpretation_compound rcmod_problem_received prep_of_problem_interpretation det_problem_the cop_problem_is nsubj_problem_task nn_task_reasoning amod_task_relational amod_task_such num_task_One
E09-1071	J06-3003	o	Turney -LRB- 2006 -RRB- describes a method -LRB- Latent Relational Analysis -RRB- that extracts subsequence patterns for noun pairs from a large corpus using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space	nn_space_feature det_space_the prep_of_complexity_space det_complexity_the dobj_reduce_complexity aux_reduce_to nn_reduction_dimensionality nn_selection_feature conj_and_search_reduction conj_and_search_selection det_search_the prep_of_recall_reduction prep_of_recall_selection prep_of_recall_search det_recall_the vmod_increase_reduce dobj_increase_recall aux_increase_to nn_expansion_query vmod_using_increase dobj_using_expansion amod_corpus_large det_corpus_a nn_pairs_noun prep_for_patterns_pairs nn_patterns_subsequence prep_from_extracts_corpus dobj_extracts_patterns nsubj_extracts_that amod_Analysis_Relational amod_Analysis_Latent rcmod_method_extracts appos_method_Analysis det_method_a vmod_describes_using dobj_describes_method nsubj_describes_Turney appos_Turney_2006
E09-1071	J06-3003	o	The distinction between lexical and relational similarity for word pair comparison is recognised byTurney -LRB- 2006 -RRB- -LRB- hecallstheformer attributional similarity -RRB- though the methods he presents focus on relational similarity	amod_similarity_relational prep_on_focus_similarity dobj_presents_focus nsubj_presents_he rcmod_methods_presents det_methods_the nn_similarity_attributional nn_similarity_hecallstheformer prep_though_byTurney_methods appos_byTurney_similarity appos_byTurney_2006 amod_byTurney_recognised cop_byTurney_is nsubj_byTurney_distinction nn_comparison_pair nn_comparison_word prep_for_similarity_comparison amod_similarity_relational amod_similarity_lexical conj_and_lexical_relational prep_between_distinction_similarity det_distinction_The
W09-0201	J06-3003	o	in at with use teacher school 11894.47020.1 28.9 0.0 teacher handbook 2.5 0.0 3.2 10.1 soldier gun 2.8 10.3 105.9 41.0 Table 5 A fragment of the CCxL space We use this space to measure relational similarity -LRB- Turney 2006 -RRB- of concept pairs e.g. finding that the relation between teachers and handbooks is more similar to the one between soldiers and guns than to the one between teachers and schools	conj_and_teachers_schools prep_between_one_schools prep_between_one_teachers det_one_the pobj_to_one pcomp_than_to conj_and_soldiers_guns prep_between_one_guns prep_between_one_soldiers det_one_the prep_to_similar_one advmod_similar_more cop_similar_is nsubj_similar_relation mark_similar_that conj_and_teachers_handbooks prep_between_relation_handbooks prep_between_relation_teachers det_relation_the prep_finding_than ccomp_finding_similar advmod_finding_e.g. nn_pairs_concept dep_Turney_2006 vmod_similarity_finding prep_of_similarity_pairs dep_similarity_Turney amod_similarity_relational dobj_measure_similarity aux_measure_to det_space_this vmod_use_measure dobj_use_space nsubj_use_We nn_space_CCxL det_space_the rcmod_fragment_use prep_of_fragment_space det_fragment_A num_Table_5 num_Table_41.0 num_Table_105.9 dep_10.3_Table dep_2.8_10.3 dep_gun_2.8 nn_gun_soldier num_gun_10.1 num_gun_3.2 number_3.2_0.0 number_3.2_2.5 dep_handbook_gun dep_teacher_handbook dep_0.0_teacher dep_28.9_0.0 dep_11894.47020.1_28.9 dep_school_11894.47020.1 nn_school_teacher nn_school_use pobj_with_school pcomp_at_with dep_in_fragment prep_in_at dep_``_in
W09-0201	J06-3003	o	The Attr cells summarize the performance of the 6 models on the wiki table that are based on attributional similarity only -LRB- Turney 2006 -RRB-	amod_Turney_2006 dep_only_Turney advmod_similarity_only amod_similarity_attributional prep_on_based_similarity auxpass_based_are nsubjpass_based_that rcmod_table_based nn_table_wiki det_table_the num_models_6 det_models_the prep_on_performance_table prep_of_performance_models det_performance_the dobj_summarize_performance nsubj_summarize_cells nn_cells_Attr det_cells_The
W09-0201	J06-3003	o	In particular we need to develop a backoff strategy for unseen pairs in the relational similarity tasks that following Turney -LRB- 2006 -RRB- could be based on constructing surrogate pairs of taxonomically similar words found in the CxLC space	nn_space_CxLC det_space_the prep_in_found_space vmod_words_found amod_words_similar advmod_words_taxonomically prep_of_pairs_words amod_pairs_surrogate dobj_constructing_pairs prepc_on_based_constructing auxpass_based_be aux_based_could prep_following_based_Turney mark_based_that appos_Turney_2006 nn_tasks_similarity amod_tasks_relational det_tasks_the amod_pairs_unseen prep_for_strategy_pairs nn_strategy_backoff det_strategy_a prep_in_develop_tasks dobj_develop_strategy aux_develop_to ccomp_need_based xcomp_need_develop nsubj_need_we prep_in_need_particular
W09-0201	J06-3003	o	We solve SAT analogies with a simplified version of the method of Turney -LRB- 2006 -RRB-	appos_Turney_2006 prep_of_method_Turney det_method_the prep_of_version_method amod_version_simplified det_version_a nn_analogies_SAT prep_with_solve_version dobj_solve_analogies nsubj_solve_We
W09-0201	J06-3003	o	1 Introduction Corpus-derived distributional semantic spaces have proved valuable in tackling a variety of tasks ranging from concept categorization to relation extraction to many others -LRB- Sahlgren 2006 Turney 2006 Pado and Lapata 2007 -RRB-	dep_Pado_2007 conj_and_Pado_Lapata num_Turney_2006 dep_Sahlgren_Lapata dep_Sahlgren_Pado dep_Sahlgren_Turney dep_Sahlgren_2006 appos_others_Sahlgren amod_others_many nn_extraction_relation nn_categorization_concept prep_to_ranging_others prep_to_ranging_extraction prep_from_ranging_categorization prep_of_variety_tasks det_variety_a vmod_tackling_ranging dobj_tackling_variety prepc_in_proved_tackling acomp_proved_valuable aux_proved_have nsubj_proved_spaces amod_spaces_semantic amod_spaces_distributional amod_spaces_Corpus-derived nn_spaces_Introduction num_spaces_1 ccomp_``_proved
W09-0205	J06-3003	o	The literature on relational similarity on the other hand has focused on pairs of words devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest -LRB- Turney 2006 Pantel and Pennacchiotti 2006 -RRB-	amod_Pantel_2006 conj_and_Pantel_Pennacchiotti dep_Turney_Pennacchiotti dep_Turney_Pantel appos_Turney_2006 appos_interest_Turney prep_of_relation_interest det_relation_a dobj_instantiate_relation nsubj_instantiate_that rcmod_pairs_instantiate amod_pairs_other prep_of_contexts_pairs det_contexts_the prep_to_are_contexts nsubj_are_contexts dep_are_similar nsubj_appear_pairs prep_in_appear_which nn_pairs_target rcmod_contexts_appear det_contexts_the advmod_similar_how ccomp_compare_are aux_compare_to amod_methods_various vmod_devising_compare dobj_devising_methods prep_of_pairs_words xcomp_focused_devising prep_on_focused_pairs aux_focused_has prep_on_focused_hand nsubj_focused_literature amod_hand_other det_hand_the amod_similarity_relational prep_on_literature_similarity det_literature_The
W09-0205	J06-3003	p	1 Introduction Co-occurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts -LRB- Sahlgren 2006 Turney 2006 -RRB-	amod_Turney_2006 dep_Sahlgren_Turney appos_Sahlgren_2006 appos_concepts_Sahlgren conj_or_words_concepts num_words_two prep_between_relation_concepts prep_between_relation_words amod_relation_semantic det_relation_the prep_of_identification_relation det_identification_the dobj_involve_identification nsubj_involve_that rcmod_tasks_involve prep_of_range_tasks amod_range_wide det_range_a amod_performance_good nn_lead_corpora prep_on_extracted_range prep_to_extracted_performance prep_from_extracted_lead vmod_statistics_extracted nn_statistics_Co-occurrence nn_statistics_Introduction num_statistics_1 dep_``_statistics
W09-1111	J06-3003	o	In computational linguistics our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs -LRB- -LRB- Hearst 1998 Chklovski and Pantel 2004 Etzioni et al. 2004 Turney 2006 Davidov and Rappoport 2008 -RRB- inter alia -RRB-	nn_alia_inter dep_alia_Rappoport dep_alia_Davidov dep_Davidov_2008 conj_and_Davidov_Rappoport num_Turney_2006 num_Etzioni_2004 nn_Etzioni_al. nn_Etzioni_et dep_Chklovski_alia conj_and_Chklovski_Turney conj_and_Chklovski_Etzioni conj_and_Chklovski_2004 conj_and_Chklovski_Pantel dep_Hearst_Turney dep_Hearst_Etzioni dep_Hearst_2004 dep_Hearst_Pantel dep_Hearst_Chklovski amod_Hearst_1998 conj_or_nouns_verbs prep_between_relations_verbs prep_between_relations_nouns amod_relations_semantic dep_indicators_Hearst prep_of_indicators_relations nn_patterns_surface prep_as_use_indicators dobj_use_patterns nsubj_use_that rcmod_approaches_use amod_approaches_previous prep_over_extends_approaches nsubj_extends_procedure prep_in_extends_linguistics nn_procedure_discovery nn_procedure_pattern poss_procedure_our amod_linguistics_computational
A92-1013	J90-1003	o	Congress of the Italian Association for Artificial Intelligence Palermo 1991 B. Boguraev Building a Lexicon the Contribution of Computers IBM Report T.J. Watson Research Center 1991 M. Brent Automatic Aquisition of Subcategorization frames from Untagged Texts in -LRB- ACL 1991 -RRB- N. Calzolari R. Bindi Acquisition of Lexical Information from Corpus in -LRB- COLING 1990 -RRB- K. W. Church P. Hanks Word Association Norms Mutual Information and Lexicography Computational Linguistics vol	nn_Linguistics_Computational nn_Information_Mutual nn_Norms_Association nn_Norms_Word nn_Hanks_P. nn_Church_W. nn_Church_K. dep_Church_1990 amod_1990_COLING nn_Information_Lexical prep_from_Acquisition_Corpus prep_of_Acquisition_Information nn_Bindi_R. appos_Calzolari_vol conj_and_Calzolari_Linguistics conj_and_Calzolari_Lexicography conj_and_Calzolari_Information conj_and_Calzolari_Norms conj_and_Calzolari_Hanks prep_in_Calzolari_Church conj_and_Calzolari_Acquisition conj_and_Calzolari_Bindi nn_Calzolari_N. dep_Calzolari_ACL dep_ACL_1991 amod_Texts_Untagged nn_frames_Subcategorization prep_from_Aquisition_Texts prep_of_Aquisition_frames nn_Aquisition_Automatic nn_Brent_M. num_Brent_1991 nn_Center_Research nn_Center_Watson nn_Center_T.J. nn_Report_IBM prep_in_Contribution_Linguistics prep_in_Contribution_Lexicography prep_in_Contribution_Information prep_in_Contribution_Norms prep_in_Contribution_Hanks prep_in_Contribution_Acquisition prep_in_Contribution_Bindi prep_in_Contribution_Calzolari conj_Contribution_Aquisition conj_Contribution_Brent conj_Contribution_Center conj_Contribution_Report prep_of_Contribution_Computers det_Contribution_the det_Lexicon_a nn_Lexicon_Building nn_Boguraev_B. num_Boguraev_1991 dep_Palermo_Contribution appos_Palermo_Lexicon appos_Palermo_Boguraev appos_Intelligence_Palermo nn_Intelligence_Artificial prep_for_Association_Intelligence amod_Association_Italian det_Association_the prep_of_Congress_Association
A92-1013	J90-1003	o	The results of these studies have important applications in lexicography to detect lexicosyntactic regularities -LRB- Church and Hanks 19901 -LRB- Calzolari and Bindi ,1990 -RRB- such as for example ~ support verbs -LRB- e.g. make-decision -RRB- prepositional verbs -LRB- e.g. rely-upon -RRB- idioms semantic relations -LRB- e.g. part_of -RRB- and fixed expressions -LRB- e.g. kick the bucket -RRB-	det_bucket_the dobj_kick_bucket dep_e.g._kick dep_expressions_e.g. amod_expressions_fixed dep_part_of_e.g. dep_relations_part_of amod_relations_semantic conj_and_idioms_expressions conj_and_idioms_relations dep_idioms_rely-upon dep_rely-upon_e.g. dep_rely-upon_verbs dep_rely-upon_verbs dep_rely-upon_Hanks dep_rely-upon_Church dep_rely-upon_regularities amod_verbs_prepositional dep_make-decision_e.g. dep_verbs_make-decision nn_verbs_support nn_verbs_~ prep_for_verbs_example mwe_as_such num_Bindi_,1990 conj_and_Calzolari_Bindi dep_Church_as dep_Church_Bindi dep_Church_Calzolari num_Church_19901 conj_and_Church_Hanks amod_regularities_lexicosyntactic dobj_detect_expressions dobj_detect_relations dobj_detect_idioms aux_detect_to prep_in_applications_lexicography amod_applications_important vmod_have_detect dobj_have_applications nsubj_have_results det_studies_these prep_of_results_studies det_results_The
A92-1013	J90-1003	o	In -LRB- Calzolari and Bindi 1990 -RRB- -LRB- Church and Hanks 1990 -RRB- the significance of an association -LRB- x y -RRB- is measured by the mutual information I -LRB- x y -RRB- i.e. the probability of observing x and y together compared with the probability of observing x and y independently	conj_and_x_y advmod_observing_independently dobj_observing_y dobj_observing_x prepc_of_probability_observing det_probability_the conj_and_x_y pobj_observing_probability prepc_compared_with_observing_with advmod_observing_together dobj_observing_y dobj_observing_x prepc_of_probability_observing det_probability_the pobj_i.e._probability appos_x_y dep_I_x nn_I_information amod_I_mutual det_I_the prep_measured_i.e. agent_measured_I auxpass_measured_is nsubjpass_measured_significance prep_measured_In appos_x_y dep_association_x det_association_an prep_of_significance_association det_significance_the dep_significance_Hanks dep_significance_Church dep_Church_1990 conj_and_Church_Hanks dep_Calzolari_1990 conj_and_Calzolari_Bindi dep_In_Bindi dep_In_Calzolari
A94-1006	J90-1003	o	In particular mutual information -LRB- Church and Hanks 1990 Wu and Su 1993 -RRB- and other statistical methods such as -LRB- Smadja 1993 -RRB- and frequency-based methods such as -LRB- Justeson and Katz 1993 -RRB- exclude infrequent phrases because they tend to introduce too much noise	amod_noise_much advmod_much_too dobj_introduce_noise aux_introduce_to xcomp_tend_introduce nsubj_tend_they mark_tend_because amod_phrases_infrequent advcl_exclude_tend dobj_exclude_phrases nsubj_exclude_methods nsubj_exclude_information prep_in_exclude_particular dep_Justeson_1993 conj_and_Justeson_Katz prep_such_as_methods_Katz prep_such_as_methods_Justeson amod_methods_frequency-based conj_and_Smadja_methods amod_Smadja_1993 prep_such_as_methods_methods prep_such_as_methods_Smadja amod_methods_statistical amod_methods_other dep_Wu_1993 conj_and_Wu_Su conj_and_Church_Su conj_and_Church_Wu conj_and_Church_1990 conj_and_Church_Hanks conj_and_information_methods dep_information_Wu dep_information_1990 dep_information_Hanks dep_information_Church amod_information_mutual
A97-1021	J90-1003	o	Previous research in automatic acquisition focuses primarily on the use of statistical techniques such as bilingual alignment -LRB- Church and Hanks 1990 Klavans and Tzoukermann 1995 Wu and Xia 1995 -RRB- or extraction of syntactic constructions from online dictionaries and corpora -LRB- Brent 1993 -RRB-	amod_Brent_1993 dep_corpora_Brent conj_and_dictionaries_corpora amod_dictionaries_online amod_constructions_syntactic dep_Wu_1995 conj_and_Wu_Xia prep_from_Klavans_corpora prep_from_Klavans_dictionaries prep_of_Klavans_constructions conj_or_Klavans_extraction conj_and_Klavans_Xia conj_and_Klavans_Wu conj_and_Klavans_1995 conj_and_Klavans_Tzoukermann dep_Church_extraction dep_Church_Wu dep_Church_1995 dep_Church_Tzoukermann dep_Church_Klavans conj_and_Church_1990 conj_and_Church_Hanks dep_alignment_1990 dep_alignment_Hanks dep_alignment_Church amod_alignment_bilingual prep_such_as_techniques_alignment amod_techniques_statistical prep_of_use_techniques det_use_the prep_on_focuses_use advmod_focuses_primarily nsubj_focuses_research amod_acquisition_automatic prep_in_research_acquisition amod_research_Previous ccomp_``_focuses
A97-1045	J90-1003	o	Church and Hanks -LRB- 1990 -RRB- introduced a statistical measurement called mutual information for extracting strongly associated or collocated words	amod_words_collocated amod_words_associated conj_or_associated_collocated advmod_associated_strongly dobj_extracting_words prepc_for_information_extracting amod_information_mutual dobj_called_information vmod_measurement_called amod_measurement_statistical det_measurement_a dobj_introduced_measurement nsubj_introduced_Hanks nsubj_introduced_Church appos_Hanks_1990 conj_and_Church_Hanks
C00-1059	J90-1003	o	Word association norms based on co-occurrence information have been proposed by -LRB- Church and Hanks 1990 -RRB-	num_Hanks_1990 conj_and_Church_Hanks agent_proposed_Hanks agent_proposed_Church auxpass_proposed_been aux_proposed_have nsubjpass_proposed_norms nn_information_co-occurrence prep_on_based_information vmod_norms_based nn_norms_association nn_norms_Word
C00-1059	J90-1003	o	2.1.3 Correlation analysis As a correlation measure between terms we use mutual information -LRB- Church and Hanks 1990 -RRB-	num_Hanks_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual dobj_use_information nsubj_use_we nsubj_use_analysis prep_between_measure_terms nn_measure_correlation det_measure_a prep_as_analysis_measure nn_analysis_Correlation num_analysis_2.1.3
C00-2128	J90-1003	o	A large corpus is vahmble as a source of such nouns -LRB- Church and Hanks 1990 Brown et al. 1992 -RRB-	tmod_al._1992 nn_al._et nn_al._Brown conj_and_Church_al. conj_and_Church_1990 conj_and_Church_Hanks dep_nouns_al. dep_nouns_1990 dep_nouns_Hanks dep_nouns_Church amod_nouns_such prep_of_source_nouns det_source_a prep_as_vahmble_source cop_vahmble_is nsubj_vahmble_corpus amod_corpus_large det_corpus_A
C02-1033	J90-1003	o	Collocations were extracted according to the method described in -LRB- Church and Hanks 1990 -RRB- by moving a window on texts	det_window_a prep_on_moving_texts dobj_moving_window dep_Church_1990 conj_and_Church_Hanks agent_described_moving prep_in_described_Hanks prep_in_described_Church vmod_method_described det_method_the pobj_extracted_method prepc_according_to_extracted_to auxpass_extracted_were nsubjpass_extracted_Collocations
C02-1086	J90-1003	o	Mutual information MI -LRB- x y -RRB- is defined as following -LRB- Church and Hanks 1990 -RRB- -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- log -RRB- -LRB- -RRB- -LRB- -RRB- -LRB- log -RRB- -LRB- 22 yfxf yxfN ypxp yxp yxMI == -LRB- 4 -RRB- where f -LRB- x -RRB- and f -LRB- y -RRB- are frequency of term x and term y respectively	nn_y_term conj_and_x_y nn_x_term advmod_frequency_respectively prep_of_frequency_y prep_of_frequency_x cop_frequency_are nsubj_frequency_y nsubj_frequency_f advmod_frequency_where nn_y_f conj_and_f_y appos_f_x rcmod_==_frequency appos_==_4 nn_==_yxMI nn_==_yxp nn_==_ypxp nn_==_yxfN nn_==_yfxf num_==_22 appos_log_== dep_log_log dep_Church_1990 conj_and_Church_Hanks dep_following_Hanks dep_following_Church dep_defined_log prepc_as_defined_following auxpass_defined_is nsubjpass_defined_MI pobj_x_y dep_MI_x nn_MI_information amod_MI_Mutual
C02-1086	J90-1003	o	One way of resolving query ambiguities is to use the statistics such as mutual information -LRB- Church and Hanks 1990 -RRB- to measure associations of query terms on the basis of existing corpora -LRB- Jang et al 1999 -RRB-	amod_Jang_1999 dep_Jang_al nn_Jang_et dep_corpora_Jang amod_corpora_existing prep_of_basis_corpora det_basis_the nn_terms_query prep_of_associations_terms dobj_measure_associations aux_measure_to appos_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual prep_such_as_statistics_information det_statistics_the vmod_use_measure dobj_use_statistics aux_use_to prep_on_is_basis xcomp_is_use nsubj_is_way nn_ambiguities_query dobj_resolving_ambiguities prepc_of_way_resolving num_way_One ccomp_``_is
C04-1105	J90-1003	o	The mutual information of a pair of words is defined in terms of their co-occurrence frequency and respective occurrence frequencies -LRB- Church and Hanks 1990 -RRB-	num_Hanks_1990 conj_and_Church_Hanks nn_frequencies_occurrence amod_frequencies_respective dep_frequency_Hanks dep_frequency_Church conj_and_frequency_frequencies nn_frequency_co-occurrence poss_frequency_their prep_of_terms_frequencies prep_of_terms_frequency prep_in_defined_terms auxpass_defined_is nsubjpass_defined_information prep_of_pair_words det_pair_a prep_of_information_pair amod_information_mutual det_information_The
C04-1141	J90-1003	o	5 Related Work Although there have been many studies on collocation extraction and mining using only statistical approaches -LRB- Church and Hanks 1990 Ikehara et al. 1996 -RRB- there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations	prep_with_associated_collocations advmod_associated_typically vmod_properties_associated amod_properties_linguistic det_properties_the dobj_takes_properties prep_into_takes_account nsubj_takes_which rcmod_acquisition_takes nn_acquisition_collocation prep_on_work_acquisition advmod_work_less cop_work_been aux_work_has expl_work_there ccomp_work_Work advmod_less_much tmod_al._1996 nn_al._et nn_al._Ikehara dep_Church_al. appos_Church_1990 conj_and_Church_Hanks appos_approaches_Hanks appos_approaches_Church amod_approaches_statistical advmod_approaches_only dobj_using_approaches conj_and_extraction_mining nn_extraction_collocation dep_studies_using prep_on_studies_mining prep_on_studies_extraction amod_studies_many cop_studies_been aux_studies_have expl_studies_there mark_studies_Although dep_Work_studies amod_Work_Related num_Work_5
C04-1194	J90-1003	o	As -LRB- Church and Hanks 1990 -RRB- we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence	det_cooccurrence_each prep_of_measure_cooccurrence nn_measure_cohesion det_measure_a amod_information_mutual prep_as_evaluation_measure prep_of_evaluation_information det_evaluation_an dobj_adopted_evaluation nsubj_adopted_we prep_adopted_As amod_Church_1990 conj_and_Church_Hanks dep_As_Hanks dep_As_Church
C08-1117	J90-1003	o	The initial vectors to be clustered are adapted with pointwise mutual information -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual amod_information_pointwise prep_with_adapted_information auxpass_adapted_are nsubjpass_adapted_vectors auxpass_clustered_be aux_clustered_to vmod_vectors_clustered amod_vectors_initial det_vectors_The
C92-1033	J90-1003	o	a Hindle and Rooth -LRB- 1991 -RRB- and Church and Hanks -LRB- 1990 -RRB- used partial parses generated by Fidditch to study word ~ urrt.nc patterns m syntactic contexts	nn_contexts_syntactic nn_contexts_m nn_contexts_patterns nn_contexts_urrt.nc nn_contexts_~ nn_contexts_word dobj_study_contexts aux_study_to agent_generated_Fidditch vmod_parses_study vmod_parses_generated amod_parses_partial amod_parses_used dep_parses_Hanks dep_parses_Church dep_parses_Rooth dep_parses_Hindle det_parses_a appos_Hanks_1990 appos_Rooth_1991 conj_and_Hindle_Hanks conj_and_Hindle_Church conj_and_Hindle_Rooth
C94-1074	J90-1003	o	In -LRB- Zernik 1990 Calzolari and Bindi 1990 Smadja 1989 Church and Hanks 1990 -RRB- associations are detected in a 5 window	num_window_5 det_window_a prep_in_detected_window auxpass_detected_are nsubjpass_detected_associations mark_detected_In dep_associations_Zernik dep_Church_1990 conj_and_Church_Hanks num_Smadja_1989 num_Bindi_1990 conj_and_Calzolari_Bindi dep_Zernik_Hanks dep_Zernik_Church dep_Zernik_Smadja dep_Zernik_Bindi dep_Zernik_Calzolari num_Zernik_1990
C94-1084	J90-1003	o	In the field of eomputationa .1 linguistics mutual information \ -LSB- Brown et al. 1988 \ -RSB- 2 \ -LSB- Church and Hanks 1990 \ -RSB- or a likelihood ratio test \ -LSB- Dunning 199a \ -RSB- are suggested	auxpass_suggested_are nsubjpass_suggested_\ nsubjpass_suggested_\ prep_in_suggested_field num_\_199a appos_Dunning_\ dep_\_Dunning nn_\_test nn_\_ratio nn_\_likelihood det_\_a num_\_1990 dep_Church_\ conj_and_Church_Hanks number_\_2 num_\_1988 appos_al._\ nn_al._et dep_Brown_al. conj_or_\_\ dep_\_Hanks dep_\_Church dep_\_\ dep_\_Brown nn_\_information amod_\_mutual num_linguistics_.1 nn_linguistics_eomputationa prep_of_field_linguistics det_field_the
C96-1055	J90-1003	o	Previous research in automatic acquisition focuscs primarily on the use of statistical techniques such as bilingual alignment -LRB- Church and Hanks 1990 Klavans and Tzoukermann 1996 Wu and Xia 1995 -RRB- or extraction of syntactic constructions from online dictionaries and corpora -LRB- Brant 1993 Dorr Garman and Weinberg 1995 -RRB-	amod_Weinberg_1995 conj_and_Dorr_Weinberg conj_and_Dorr_Garman dep_Brant_Weinberg dep_Brant_Garman dep_Brant_Dorr appos_Brant_1993 dep_corpora_Brant conj_and_dictionaries_corpora amod_dictionaries_online amod_constructions_syntactic prep_from_extraction_corpora prep_from_extraction_dictionaries prep_of_extraction_constructions dep_Wu_1995 conj_and_Wu_Xia conj_or_Klavans_extraction conj_and_Klavans_Xia conj_and_Klavans_Wu conj_and_Klavans_1996 conj_and_Klavans_Tzoukermann dep_Church_extraction dep_Church_Wu dep_Church_1996 dep_Church_Tzoukermann dep_Church_Klavans conj_and_Church_1990 conj_and_Church_Hanks dep_alignment_1990 dep_alignment_Hanks dep_alignment_Church amod_alignment_bilingual prep_such_as_techniques_alignment amod_techniques_statistical prep_of_use_techniques det_use_the nn_focuscs_acquisition amod_focuscs_automatic prep_on_research_use advmod_research_primarily prep_in_research_focuscs amod_research_Previous ccomp_``_research
C96-1083	J90-1003	o	Hindle uses the observed frequencies within a specific syntactic pattern -LRB- subject/verb and verb/object -RRB- to derive a cooccu > rence score which is an estimate of mutual information -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual prep_of_estimate_information det_estimate_an cop_estimate_is nsubj_estimate_which rcmod_score_estimate nn_score_rence amod_score_> appos_cooccu_score det_cooccu_a dobj_derive_cooccu aux_derive_to conj_and_subject/verb_verb/object dep_pattern_verb/object dep_pattern_subject/verb nn_pattern_syntactic amod_pattern_specific det_pattern_a amod_frequencies_observed det_frequencies_the vmod_uses_derive prep_within_uses_pattern dobj_uses_frequencies nsubj_uses_Hindle
C96-1083	J90-1003	p	In the past five years important research on the automatic acquisition of word classes based on lexical distribution has been published -LRB- Church and Hanks 1990 Hindle 1990 Smadja 1993 Grei ~ nstette 1994 Grishman and Sterling 1994 -RRB-	num_Grishman_1994 conj_and_Grishman_Sterling appos_nstette_1994 num_nstette_~ nn_nstette_Grei num_Smadja_1993 num_Hindle_1990 dep_Church_Sterling dep_Church_Grishman conj_and_Church_nstette conj_and_Church_Smadja conj_and_Church_Hindle conj_and_Church_1990 conj_and_Church_Hanks dep_published_nstette dep_published_Smadja dep_published_Hindle dep_published_1990 dep_published_Hanks dep_published_Church auxpass_published_been aux_published_has nsubjpass_published_research prep_in_published_years amod_distribution_lexical prep_on_based_distribution nn_classes_word prep_of_acquisition_classes amod_acquisition_automatic det_acquisition_the vmod_research_based prep_on_research_acquisition amod_research_important num_years_five amod_years_past det_years_the
C96-1097	J90-1003	o	There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words -LRB- Church and Hanks 1990 -RRB- the distance between words -LRB- Smadja and Makeown 1990 -RRB- and the number of combined words and frequency of appearance -LRB- Kita 1993 1994 -RRB-	amod_Kita_1994 num_Kita_1993 prep_of_words_appearance conj_and_words_frequency amod_words_combined dep_number_Kita prep_of_number_frequency prep_of_number_words det_number_the amod_1990_Makeown conj_and_Smadja_1990 dep_words_1990 dep_words_Smadja conj_and_distance_number prep_between_distance_words det_distance_the num_Hanks_1990 conj_and_Church_Hanks appos_words_Hanks appos_words_Church num_words_two prep_of_strength_words nn_strength_binding det_strength_the prep_on_focusing_strength prepc_of_method_focusing det_method_a prep_such_as_corpora_method prep_from_expressions_corpora amod_expressions_rigid dobj_extract_expressions aux_extract_to xcomp_proposed_extract vmod_method_proposed amod_method_many parataxis_are_number parataxis_are_distance nsubj_are_method expl_are_There ccomp_``_are
C96-2100	J90-1003	o	-LRB- Church & Hanks 1990 :p .24 -RRB- Merkel Nilsson & Ahrenberg -LRB- 1994 -RRB- have constructed a system that uses frequency of recurrent segments to determine long phrases	amod_phrases_long dobj_determine_phrases aux_determine_to amod_segments_recurrent prep_of_frequency_segments vmod_uses_determine dobj_uses_frequency nsubj_uses_that rcmod_system_uses det_system_a dobj_constructed_system aux_constructed_have nsubj_constructed_Ahrenberg nsubj_constructed_Merkel appos_Ahrenberg_1994 conj_and_Merkel_Ahrenberg appos_Merkel_Nilsson appos_Merkel_Hanks appos_Merkel_Church num_:p_.24 num_:p_1990 appos_Church_:p conj_and_Church_Hanks
C96-2100	J90-1003	o	Given this the mutual information ratio -LRB- Church & Hanks 1990 Church & Mercer 1993 Steier & Belew 1991 -RRB- is expressed by Formula 1	num_Formula_1 agent_expressed_Formula auxpass_expressed_is nsubjpass_expressed_1993 nsubjpass_expressed_Mercer nsubjpass_expressed_Church dep_Steier_1991 conj_and_Steier_Belew dep_Church_Belew dep_Church_Steier conj_and_Church_1993 conj_and_Church_Mercer parataxis_Church_expressed appos_Church_1990 conj_and_Church_Hanks dep_ratio_Hanks dep_ratio_Church nn_ratio_information amod_ratio_mutual det_ratio_the dep_,_ratio pobj_Given_this dep_``_Given
C96-2163	J90-1003	o	In the field of statistical analysis of natural language data it is common to use measures of lexical association such as the informationtheoretic measure of mutual information to extract useful relationships between words -LRB- e.g. Church and Hanks -LRB- 1990 -RRB- -RRB-	appos_Hanks_1990 conj_and_Church_Hanks dep_Church_e.g. dep_words_Hanks dep_words_Church amod_relationships_useful prep_between_extract_words dobj_extract_relationships aux_extract_to amod_information_mutual prep_of_measure_information amod_measure_informationtheoretic det_measure_the prep_such_as_association_measure amod_association_lexical prep_of_measures_association vmod_use_extract dobj_use_measures aux_use_to xcomp_common_use cop_common_is nsubj_common_it prep_in_common_field nn_data_language amod_data_natural prep_of_analysis_data amod_analysis_statistical prep_of_field_analysis det_field_the
C96-2208	J90-1003	o	More rare words rather than common words are found even in standard dictionaries -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_dictionaries_Hanks dep_dictionaries_Church amod_dictionaries_standard prep_in_found_dictionaries advmod_found_even auxpass_found_are nsubjpass_found_words nsubjpass_found_words amod_words_common conj_negcc_words_words amod_words_rare amod_words_More ccomp_``_found
D07-1039	J90-1003	o	pointwise mutual information -LRB- Church and Hanks 1990 -RRB- 3	dep_Church_1990 conj_and_Church_Hanks dep_information_3 dep_information_Hanks dep_information_Church amod_information_mutual amod_information_pointwise
D08-1007	J90-1003	o	We measure this association using pointwise Mutual Information -LRB- MI -RRB- -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_Information_Hanks dep_Information_Church appos_Information_MI amod_Information_Mutual amod_Information_pointwise dobj_using_Information det_association_this xcomp_measure_using dobj_measure_association nsubj_measure_We
D08-1044	J90-1003	o	This task is quite common in corpus linguistics and provides the starting point to many other algorithms e.g. for computing statistics such as pointwise mutual information -LRB- Church and Hanks 1990 -RRB- for unsupervised sense clustering -LRB- Schutze 1998 -RRB- and more generally a large body of work in lexical semantics based on distributional profiles dating back to Firth -LRB- 1957 -RRB- and Harris -LRB- 1968 -RRB-	appos_Harris_1968 conj_and_Firth_Harris appos_Firth_1957 prep_to_back_Harris prep_to_back_Firth advmod_dating_back amod_profiles_distributional prep_on_based_profiles amod_semantics_lexical vmod_body_dating vmod_body_based prep_in_body_semantics prep_of_body_work amod_body_large det_body_a dep_generally_body advmod_generally_more appos_Schutze_1998 dep_clustering_Schutze nn_clustering_sense amod_clustering_unsupervised appos_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual amod_information_pointwise prep_such_as_statistics_information amod_statistics_computing prep_for_algorithms_clustering prep_for_algorithms_statistics dep_algorithms_e.g. amod_algorithms_other amod_algorithms_many amod_point_starting det_point_the prep_to_provides_algorithms dobj_provides_point nsubj_provides_task nn_linguistics_corpus advmod_common_generally conj_and_common_provides prep_in_common_linguistics advmod_common_quite cop_common_is nsubj_common_task det_task_This ccomp_``_provides ccomp_``_common
D09-1051	J90-1003	o	Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts -LRB- Choueka et al. 1983 Church and Hanks 1990 Smadja 1993 Dunning 1993 Pearce 2002 Evert 2004 -RRB-	amod_Evert_2004 num_Pearce_2002 num_Dunning_1993 num_Smadja_1993 dep_Church_Evert conj_and_Church_Pearce conj_and_Church_Dunning conj_and_Church_Smadja conj_and_Church_1990 conj_and_Church_Hanks dep_Choueka_Pearce dep_Choueka_Dunning dep_Choueka_Smadja dep_Choueka_1990 dep_Choueka_Hanks dep_Choueka_Church dep_Choueka_1983 dep_Choueka_al. nn_Choueka_et prep_in_pairs_texts nn_pairs_word det_pairs_the prep_of_frequencies_pairs amod_frequencies_co-occurring dep_carried_Choueka prep_based_on_carried_frequencies prt_carried_out auxpass_carried_are nsubjpass_carried_studies nn_extraction_collocation prep_on_studies_extraction amod_studies_Many ccomp_``_carried
E95-1037	J90-1003	o	1989 -RRB- e.g lexicography -LRB- Church and Hanks 1990 -RRB- information retrieval -LRB- Salton 1986a -RRB- text input -LRB- Yamashina and Obashi 1988 -RRB- etc. This paper will touch on its feasibility in topic identification	nn_identification_topic prep_in_feasibility_identification poss_feasibility_its prep_on_touch_feasibility aux_touch_will nsubj_touch_paper det_paper_This dep_Yamashina_1988 conj_and_Yamashina_Obashi appos_input_Obashi appos_input_Yamashina nn_input_text appos_Salton_1986a dep_retrieval_touch conj_retrieval_etc. conj_retrieval_input dep_retrieval_Salton nn_retrieval_information nn_Church_lexicography dep_e.g_retrieval dep_e.g_1990 conj_and_e.g_Hanks conj_and_e.g_Church dep_1989_Hanks dep_1989_Church dep_1989_e.g ccomp_,_1989
E99-1005	J90-1003	n	2Mutual information though potentially of interest as a measure of collocational status was not tested due to its well-known property of overemphasising the significance of rare events -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_events_Hanks dep_events_Church amod_events_rare prep_of_significance_events det_significance_the dobj_overemphasising_significance prepc_of_property_overemphasising amod_property_well-known poss_property_its prep_due_to_tested_property neg_tested_not auxpass_tested_was prep_tested_though amod_status_collocational prep_of_measure_status det_measure_a prep_as_of_measure pobj_of_interest advmod_of_potentially pcomp_though_of rcmod_information_tested amod_information_2Mutual
E99-1013	J90-1003	o	Mutual information compares the probability of the co-occurence of words a and b with the independent probabilities of occurrence of a and b -LRB- Church and Hanks 1990 -RRB-	nn_Church_b appos_a_1990 conj_and_a_Hanks conj_and_a_Church prep_of_occurrence_Hanks prep_of_occurrence_Church prep_of_occurrence_a prep_of_probabilities_occurrence amod_probabilities_independent det_probabilities_the prep_with_a_probabilities conj_and_a_b dep_words_b dep_words_a prep_of_co-occurence_words det_co-occurence_the prep_of_probability_co-occurence det_probability_the dobj_compares_probability nsubj_compares_information amod_information_Mutual
H92-1040	J90-1003	o	IC function is a derivative of Fano 's mutual information formula recently used by Church and Hanks -LRB- 1990 -RRB- to compute word co-occurrence patterns in a 44 million word corpus of Associated Press news stories	nn_stories_news nn_stories_Press nn_stories_Associated prep_of_corpus_stories nn_corpus_word num_corpus_million det_corpus_a number_million_44 nn_patterns_co-occurrence nn_patterns_word prep_in_compute_corpus dobj_compute_patterns aux_compute_to appos_Hanks_1990 conj_and_Church_Hanks xcomp_used_compute agent_used_Hanks agent_used_Church advmod_used_recently nn_formula_information amod_formula_mutual poss_formula_Fano vmod_derivative_used prep_of_derivative_formula det_derivative_a cop_derivative_is nsubj_derivative_function nn_function_IC
H92-1047	J90-1003	o	Using techniques described in Church and Hindle -LRB- 1990 -RRB- Church and Hanks -LRB- 1990 -RRB- and Hindle and Rooth -LRB- 1991 -RRB- below are some examples of the most frequent V-O pairs from the AP corpus	nn_corpus_AP det_corpus_the prep_from_pairs_corpus nn_pairs_V-O amod_pairs_frequent det_pairs_the advmod_frequent_most prep_of_examples_pairs det_examples_some cop_examples_are nsubj_examples_below dep_examples_Rooth dep_examples_Hindle dep_examples_Hanks dep_examples_Church dep_examples_Using appos_Rooth_1991 appos_Hanks_1990 conj_and_Church_Rooth conj_and_Church_Hindle conj_and_Church_Hanks appos_Hindle_1990 conj_and_Church_Hindle prep_in_described_Hindle prep_in_described_Church vmod_techniques_described dobj_Using_techniques
H93-1049	J90-1003	o	Church K. and Hanks P. -LRB- 1990 -RRB- Word Association Norms Mutual Information and Lexicography Computational Linguistics Vol	nn_Vol_Linguistics nn_Vol_Computational nn_Vol_Lexicography nn_Vol_Information nn_Vol_P. nn_Vol_K. nn_Information_Mutual nn_Norms_Association nn_Norms_Word appos_P._1990 nn_P._Hanks conj_and_K._Lexicography conj_and_K._Information dep_K._Norms conj_and_K._P. nn_K._Church
I08-1014	J90-1003	o	2 Related Works Some of the most common measures of unithood include pointwise mutual information -LRB- MI -RRB- -LRB- Church and Hanks 1990 -RRB- and log-likelihood ratio -LRB- Dunning 1994 -RRB-	amod_Dunning_1994 dep_ratio_Dunning amod_ratio_log-likelihood dep_Church_1990 conj_and_Church_Hanks conj_and_information_ratio dep_information_Hanks dep_information_Church appos_information_MI amod_information_mutual nn_information_pointwise dobj_include_ratio dobj_include_information nsubj_include_Some prep_of_measures_unithood amod_measures_common det_measures_the advmod_common_most prep_of_Some_measures rcmod_Works_include amod_Works_Related num_Works_2
J00-3001	J90-1003	o	While we have observed reasonable results with both G 2 and Fisher 's exact test we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information -LRB- MI -RRB- measure -LRB- Church and Hanks 1990 -RRB- I -LRB- x y -RRB- -- log 2 P -LRB- x y -RRB- -LRB- 4 -RRB- P -LRB- x -RRB- P -LRB- y -RRB- In -LRB- 4 -RRB- y is the seed term and x a potential target word	nn_word_target amod_word_potential det_word_a dep_term_word conj_and_term_x nn_term_seed det_term_the cop_term_is nsubj_term_y prep_term_In dep_In_4 appos_P_y nn_P_P appos_P_x appos_x_y dep_P_P appos_P_4 dep_P_x num_P_2 nn_P_log appos_x_y dep_I_x dep_I_term dep_I_P dep_I_x num_Hanks_1990 conj_and_Church_Hanks dep_measure_Hanks dep_measure_Church dep_information_measure appos_information_MI amod_information_mutual det_information_the nn_linguistics_corpus pobj_used_information prepc_based_on_used_on prep_in_used_linguistics advmod_used_commonly vmod_technique_used det_technique_a prep_with_obtained_technique auxpass_obtained_be aux_obtained_can nsubjpass_obtained_that rcmod_results_obtained det_results_the prep_to_compare_results nsubj_compare_results advmod_compare_how det_results_these dep_discussed_I ccomp_discussed_compare advmod_discussed_yet neg_discussed_not aux_discussed_have nsubj_discussed_we advcl_discussed_observed amod_test_exact poss_test_Fisher conj_and_G_test num_G_2 preconj_G_both prep_with_results_test prep_with_results_G amod_results_reasonable dobj_observed_results aux_observed_have nsubj_observed_we mark_observed_While
J00-3001	J90-1003	p	Church and Hanks -LRB- 1990 -RRB- use mutual information to identify collocations a method they claim is reasonably effective for words with a frequency of not less than five	prep_than_less_five neg_less_not prep_of_frequency_less det_frequency_a prep_with_words_frequency prep_for_effective_words advmod_effective_reasonably cop_effective_is csubj_effective_use nsubj_claim_they rcmod_method_claim det_method_a appos_collocations_method dobj_identify_collocations aux_identify_to amod_information_mutual vmod_use_identify dobj_use_information nsubj_use_Hanks nsubj_use_Church appos_Hanks_1990 conj_and_Church_Hanks
J00-3001	J90-1003	o	org/pubs/citations / j ournals/toms/1986 -12 -2 / p154-meht a / Mutual Information Given the definition of Mutual Information -LRB- Church and Hanks 1990 -RRB- I -LRB- x y -RRB- = log 2 P -LRB- x y -RRB- P -LRB- x -RRB- P -LRB- y -RRB- we consider the distribution of a window word according to the contingency table -LRB- a -RRB- in Table 4	num_Table_4 prep_in_table_Table appos_table_a nn_table_contingency det_table_the nn_word_window det_word_a prep_of_distribution_word det_distribution_the pobj_consider_table prepc_according_to_consider_to dobj_consider_distribution nsubj_consider_we nsubj_consider_j dep_consider_org/pubs/citations appos_P_y nn_P_P appos_P_x appos_x_y dep_P_P appos_P_x num_P_2 nn_P_log dobj_=_P appos_x_y nn_x_I num_Hanks_1990 conj_and_Church_Hanks dep_Information_Hanks dep_Information_Church amod_Information_Mutual prep_of_definition_Information det_definition_the pobj_Given_definition prep_Information_Given amod_Information_Mutual det_Information_a num_Information_p154-meht number_-2_-12 dep_ournals/toms/1986_= dep_ournals/toms/1986_x dep_ournals/toms/1986_Information num_ournals/toms/1986_-2 dep_j_ournals/toms/1986
J02-2003	J90-1003	o	Equation -LRB- 10 -RRB- is of interest because the ratio p -LRB- C | v r -RRB- / p -LRB- C | r -RRB- can be interpreted as a measure of association between the verb v and class C This ratio is similar to pointwise mutual information -LRB- Church and Hanks 1990 -RRB- and also forms part of Resniks association score which will be introduced in Section 6	num_Section_6 prep_in_introduced_Section auxpass_introduced_be aux_introduced_will nsubjpass_introduced_which rcmod_score_introduced nn_score_association nn_score_Resniks prep_of_part_score dobj_forms_part advmod_forms_also nsubj_forms_ratio num_Hanks_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual amod_information_pointwise conj_and_similar_forms prep_to_similar_information cop_similar_is nsubj_similar_ratio dep_similar_C dep_similar_v det_ratio_This nn_C_class conj_and_v_C ccomp_verb_forms ccomp_verb_similar vmod_the_verb prep_between_association_the prep_of_measure_association det_measure_a prep_as_interpreted_measure auxpass_interpreted_be aux_interpreted_can nsubjpass_interpreted_p dep_interpreted_v nsubjpass_interpreted_C dobj_interpreted_p mark_interpreted_because num_r_| nn_r_C appos_p_r dep_p_r num_C_| nn_p_ratio det_p_the advcl_is_interpreted prep_of_is_interest nsubj_is_Equation appos_Equation_10
J04-3002	J90-1003	o	303 Wiebe Wilson Bruce Bell and Martin Learning Subjective Language While it is common in studies of collocations to omit low-frequency words and expressions from analysis because they give rise to invalid or unrealistic statistical measures -LRB- Church and Hanks 1990 -RRB- we are able to identify higher-precision collocations by including placeholders for unique words -LRB- i.e. the ugen-n-grams -RRB-	det_ugen-n-grams_the dep_ugen-n-grams_i.e. dep_words_ugen-n-grams amod_words_unique prep_for_placeholders_words pobj_including_placeholders amod_collocations_higher-precision prepc_by_identify_including dobj_identify_collocations aux_identify_to xcomp_able_identify cop_able_are nsubj_able_we advcl_able_give dep_Church_1990 conj_and_Church_Hanks dep_measures_Hanks dep_measures_Church amod_measures_statistical amod_measures_unrealistic amod_measures_invalid conj_or_invalid_unrealistic prep_to_rise_measures dobj_give_rise nsubj_give_they mark_give_because conj_and_words_expressions nn_words_low-frequency prep_from_omit_analysis dobj_omit_expressions dobj_omit_words aux_omit_to vmod_studies_omit prep_of_studies_collocations prep_in_common_studies cop_common_is nsubj_common_it mark_common_While nn_Language_Subjective nn_Language_Learning nn_Language_Martin rcmod_Wiebe_able dep_Wiebe_common conj_and_Wiebe_Language conj_and_Wiebe_Bell conj_and_Wiebe_Bruce conj_and_Wiebe_Wilson num_Wiebe_303 dep_``_Language dep_``_Bell dep_``_Bruce dep_``_Wilson dep_``_Wiebe
J09-3004	J90-1003	o	This does not seem to be the case however for common feature weighting functions such as Point-wise Mutual Information -LRB- Church and Patrick 1990 Hindle 1990 -RRB-	num_Hindle_1990 num_Patrick_1990 dep_Church_Hindle conj_and_Church_Patrick dep_Information_Patrick dep_Information_Church amod_Information_Mutual amod_Information_Point-wise prep_such_as_functions_Information nn_functions_weighting nn_functions_feature amod_functions_common det_case_the cop_case_be aux_case_to prep_for_seem_functions advmod_seem_however xcomp_seem_case neg_seem_not aux_seem_does nsubj_seem_This ccomp_``_seem
J09-3004	J90-1003	p	Probably the most widely used feature weighting function is -LRB- point-wise -RRB- Mutual Information -LRB- MI -RRB- -LRB- Church and Patrick 1990 Hindle 1990 Luk 1995 Lin 1998 Gauch Wang and Rachakonda 1999 Dagan 2000 Baroni and Vegnaduzzo 2004 Chklovski and Pantel 2004 Pantel and Ravichandran 2004 Pantel Ravichandran and Hovy 2004 Weeds Weir and McCarthy 2004 -RRB- dened by weight MI -LRB- w f -RRB- = log 2 P -LRB- w f -RRB- P -LRB- w -RRB- P -LRB- f -RRB- -LRB- 1 -RRB- We calculate the MI weights by the following statistics in the space of co-occurrence instances S weight MI -LRB- w f -RRB- = log 2 count -LRB- w f -RRB- nrels count -LRB- w -RRB- count -LRB- f -RRB- -LRB- 2 -RRB- where count -LRB- w f -RRB- is the frequency of the co-occurrence pair w f in S count -LRB- w -RRB- and count -LRB- f -RRB- are the independent frequencies of w and f in S andnrels is the size of S.High MI weights are assumed to correspond to strong wordfeature associations	nn_associations_wordfeature amod_associations_strong prep_to_correspond_associations aux_correspond_to xcomp_assumed_correspond auxpass_assumed_are nsubjpass_assumed_f nsubjpass_assumed_frequencies advcl_assumed_frequency dep_assumed_2 nn_weights_MI nn_weights_S.High prep_of_size_weights det_size_the cop_size_is nsubj_size_andnrels prep_in_f_S rcmod_frequencies_size conj_and_frequencies_f prep_of_frequencies_w amod_frequencies_independent det_frequencies_the cop_frequencies_are nsubj_frequencies_count nsubj_frequencies_count dep_frequencies_f appos_count_f conj_and_count_count appos_count_w prep_in_f_S nn_w_pair nn_w_co-occurrence det_w_the prep_of_frequency_w det_frequency_the cop_frequency_is nsubj_frequency_count advmod_frequency_where dep_w_f appos_count_w rcmod_count_assumed appos_count_f nn_count_count appos_count_w nn_count_nrels nn_count_count dep_w_f appos_count_w num_count_2 nn_count_log dobj_=_count dep_w_f dep_MI_= appos_MI_w nn_MI_weight nn_S_instances nn_S_co-occurrence prep_of_space_S det_space_the prep_in_statistics_space amod_statistics_following det_statistics_the nn_weights_MI det_weights_the prep_by_calculate_statistics dobj_calculate_weights nsubj_calculate_We dep_P_calculate appos_P_1 appos_P_f nn_P_P appos_P_w nn_P_P dep_w_f appos_P_w num_P_2 nn_P_log dep_=_P dep_w_f dep_MI_MI amod_MI_= appos_MI_w nn_MI_weight prep_by_dened_MI num_McCarthy_2004 conj_and_Weeds_McCarthy conj_and_Weeds_Weir num_Hovy_2004 conj_and_Pantel_Hovy conj_and_Pantel_Ravichandran num_Ravichandran_2004 conj_and_Pantel_Ravichandran num_Pantel_2004 conj_and_Chklovski_Pantel num_Vegnaduzzo_2004 conj_and_Baroni_Vegnaduzzo num_Dagan_2000 num_Rachakonda_1999 conj_and_Gauch_Rachakonda conj_and_Gauch_Wang num_Lin_1998 num_Luk_1995 num_Hindle_1990 num_Patrick_1990 dep_Church_McCarthy dep_Church_Weir dep_Church_Weeds dep_Church_Hovy dep_Church_Ravichandran dep_Church_Pantel dep_Church_Ravichandran dep_Church_Pantel dep_Church_Pantel dep_Church_Chklovski dep_Church_Vegnaduzzo dep_Church_Baroni dep_Church_Dagan dep_Church_Rachakonda dep_Church_Wang dep_Church_Gauch conj_and_Church_Lin conj_and_Church_Luk conj_and_Church_Hindle conj_and_Church_Patrick dep_Information_Lin dep_Information_Luk dep_Information_Hindle dep_Information_Patrick dep_Information_Church appos_Information_MI amod_Information_Mutual dep_point-wise_dened dep_point-wise_Information dep_is_point-wise dep_function_is nn_function_weighting nn_function_feature amod_function_used det_function_the advmod_used_widely advmod_widely_most dep_Probably_function dep_``_Probably
J92-1001	J90-1003	o	Church and Hanks 1990 Smadja and McKeown 1990 -RRB-	num_McKeown_1990 conj_and_Smadja_McKeown dep_Church_McKeown dep_Church_Smadja dep_Church_1990 conj_and_Church_Hanks
J93-1006	J90-1003	o	7 This discussion could also be cast in an information theoretic framework using the notion of mutual information -LRB- Fano 1961 -RRB- estimating the variance of the degree of match in order to find a frequency-threshold -LRB- see Church and Hanks 1990 -RRB-	num_Hanks_1990 conj_and_Church_Hanks dobj_see_Hanks dobj_see_Church dep_frequency-threshold_see det_frequency-threshold_a dobj_find_frequency-threshold aux_find_to dep_find_order mark_find_in prep_of_degree_match det_degree_the prep_of_variance_degree det_variance_the advcl_estimating_find dobj_estimating_variance num_Fano_1961 amod_information_mutual prep_of_notion_information det_notion_the dobj_using_notion vmod_framework_using amod_framework_theoretic nn_framework_information det_framework_an xcomp_cast_estimating dep_cast_Fano prep_in_cast_framework auxpass_cast_be advmod_cast_also aux_cast_could nsubjpass_cast_discussion det_discussion_This num_discussion_7
J93-2005	J90-1003	o	Using techniques described in Church and Hindle -LRB- 1990 -RRB- Church and Hanks -LRB- 1990 -RRB- and Hindle and Rooth -LRB- 1991 -RRB- Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus	nn_corpus_AP det_corpus_the prep_from_pairs_corpus nn_pairs_V-O amod_pairs_frequent det_pairs_the advmod_frequent_most prep_of_examples_pairs det_examples_some dobj_shows_examples nsubj_shows_Figure nsubj_shows_Rooth nsubj_shows_Hindle nsubj_shows_Hanks nsubj_shows_Church vmod_shows_Using num_Figure_4 appos_Rooth_1991 appos_Hanks_1990 conj_and_Church_Figure conj_and_Church_Rooth conj_and_Church_Hindle conj_and_Church_Hanks appos_Hindle_1990 conj_and_Church_Hindle prep_in_described_Hindle prep_in_described_Church vmod_techniques_described dobj_Using_techniques
J93-2005	J90-1003	o	Church and Hanks 1990 Klavans Chodorow and Wacholder 1990 Wilks et al. 1993 Smadja 1991a 1991b Calzolari and Bindi 1990 -RRB-	num_Bindi_1990 conj_and_Calzolari_Bindi appos_1991a_1991b nn_1991a_Smadja num_al._1993 nn_al._et nn_al._Wilks num_Wacholder_1990 conj_and_Klavans_Wacholder conj_and_Klavans_Chodorow dep_Church_Bindi dep_Church_Calzolari conj_and_Church_1991a dep_Church_al. dep_Church_Wacholder dep_Church_Chodorow dep_Church_Klavans dep_Church_1990 conj_and_Church_Hanks
J93-3004	J90-1003	o	For example Church and Hanks -LRB- 1990 -RRB- describe the use of the mutual information index for this purpose -LRB- cf.	det_purpose_this nn_index_information amod_index_mutual det_index_the prep_for_use_purpose prep_of_use_index det_use_the dep_describe_cf. dobj_describe_use nsubj_describe_Hanks nsubj_describe_Church prep_for_describe_example appos_Hanks_1990 conj_and_Church_Hanks
J93-3004	J90-1003	o	These tools are important in that the strongest collocational associations often represent different word senses and thus ` they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags ' -LRB- Church and Hanks 1990 p. 28 -RRB-	num_p._28 num_Hanks_1990 appos_Church_p. conj_and_Church_Hanks dep_tags_Hanks dep_tags_Church possessive_tags_' amod_tags_semantic prep_of_set_tags det_set_a dobj_choosing_set prepc_in_accounted_choosing prep_accounted_for auxpass_accounted_be aux_accounted_to xcomp_needs_accounted nsubj_needs_what det_lexicographer_the prep_to_suggestions_lexicographer prep_of_set_suggestions amod_set_powerful det_set_a prepc_for_provide_needs dobj_provide_set nsubj_provide_they advmod_provide_thus nn_senses_word amod_senses_different conj_and_represent_provide dobj_represent_senses advmod_represent_often nsubj_represent_associations mark_represent_that amod_associations_collocational amod_associations_strongest det_associations_the prepc_in_important_provide prepc_in_important_represent cop_important_are nsubj_important_tools det_tools_These ccomp_``_important
J93-3004	J90-1003	o	1 Church and Hanks -LRB- 1990 Church et al. 1991 -RRB- thus emphasize the importance of human judgment used in conjunction with these tools	det_tools_these prep_with_conjunction_tools prep_in_used_conjunction vmod_judgment_used amod_judgment_human prep_of_importance_judgment det_importance_the dobj_emphasize_importance advmod_emphasize_thus nsubj_emphasize_Hanks nsubj_emphasize_Church dep_al._1991 nn_al._et nn_al._Church dep_1990_al. appos_Hanks_1990 conj_and_Church_Hanks num_Church_1
J94-4003	J90-1003	o	The use of such relations -LRB- mainly relations between verbs or nouns and their arguments and modifiers -RRB- for various purposes has received growing attention in recent research -LRB- Church and Hanks 1990 Zernik and Jacobs 1990 Hindle 1990 Smadja 1993 -RRB-	num_Smadja_1993 num_Hindle_1990 num_Jacobs_1990 conj_and_Zernik_Jacobs num_Hanks_1990 dep_Church_Smadja conj_and_Church_Hindle conj_and_Church_Jacobs conj_and_Church_Zernik conj_and_Church_Hanks dep_research_Hindle dep_research_Zernik dep_research_Hanks dep_research_Church amod_research_recent amod_attention_growing prep_in_received_research dobj_received_attention aux_received_has nsubj_received_use amod_purposes_various poss_arguments_their conj_and_verbs_modifiers conj_and_verbs_arguments conj_or_verbs_nouns prep_between_relations_modifiers prep_between_relations_arguments prep_between_relations_nouns prep_between_relations_verbs advmod_relations_mainly amod_relations_such prep_for_use_purposes dep_use_relations prep_of_use_relations det_use_The
J94-4003	J90-1003	o	Statistics on co-occurrence of words in a local context were used recently for monolingual word sense disambiguation -LRB- Gale Church and Yarowsky 1992b 1993 Sch6tze 1992 1993 -RRB- -LRB- see Section 7 for more details and Church and Hanks 1990 Smadja 1993 for other applications of these statistics -RRB-	det_statistics_these prep_of_applications_statistics amod_applications_other prep_for_Smadja_applications num_Smadja_1993 num_Hanks_1990 conj_and_details_Hanks conj_and_details_Church amod_details_more num_Section_7 parataxis_see_Smadja prep_for_see_Hanks prep_for_see_Church prep_for_see_details dobj_see_Section num_Sch6tze_1993 num_Sch6tze_1992 appos_1992b_1993 nn_1992b_Yarowsky dep_Gale_see dep_Gale_Sch6tze conj_and_Gale_1992b conj_and_Gale_Church dep_disambiguation_1992b dep_disambiguation_Church dep_disambiguation_Gale nn_disambiguation_sense nn_disambiguation_word amod_disambiguation_monolingual prep_for_used_disambiguation advmod_used_recently auxpass_used_were nsubjpass_used_Statistics amod_context_local det_context_a prep_of_co-occurrence_words prep_in_Statistics_context prep_on_Statistics_co-occurrence
J94-4005	J90-1003	o	Lexical collocation functions especially those determined statistically have recently attracted considerable attention in computational linguistics -LRB- Calzolari and Bindi 1990 Church and Hanks 1990 Sekine et al. 1992 Hindle and Rooth 1993 -RRB- mainly though not exclusively for use in disambiguation	prep_in_use_disambiguation pobj_for_use ccomp_,_for neg_exclusively_not mark_exclusively_though advcl_,_exclusively num_Rooth_1993 conj_and_Hindle_Rooth dep_al._1992 nn_al._et nn_al._Sekine num_Church_1990 conj_and_Church_Hanks num_Bindi_1990 dep_Calzolari_mainly dep_Calzolari_Rooth dep_Calzolari_Hindle conj_and_Calzolari_al. conj_and_Calzolari_Hanks conj_and_Calzolari_Church conj_and_Calzolari_Bindi dep_linguistics_al. dep_linguistics_Church dep_linguistics_Bindi dep_linguistics_Calzolari amod_linguistics_computational amod_attention_considerable prep_in_attracted_linguistics dobj_attracted_attention advmod_attracted_recently aux_attracted_have nsubj_attracted_functions advmod_determined_statistically vmod_those_determined advmod_those_especially appos_functions_those nn_functions_collocation amod_functions_Lexical
N03-1032	J90-1003	o	The window size may vary Church and Hanks -LRB- 1990 -RRB- used windows of size 2 and 5	num_size_2 conj_and_windows_5 prep_of_windows_size dobj_used_5 dobj_used_windows nsubj_used_Hanks nsubj_used_Church appos_Hanks_1990 conj_and_Church_Hanks ccomp_vary_used aux_vary_may nsubj_vary_size nn_size_window det_size_The
N03-1032	J90-1003	o	2.1.1 Pointwise Mutual Information This measure for word similarity was first used in this context by Church and Hanks -LRB- 1990 -RRB-	appos_Hanks_1990 conj_and_Church_Hanks det_context_this agent_used_Hanks agent_used_Church prep_in_used_context advmod_used_first auxpass_used_was nsubjpass_used_measure nn_similarity_word prep_for_measure_similarity det_measure_This rcmod_Information_used nn_Information_Mutual nn_Information_Pointwise num_Information_2.1.1
N03-1032	J90-1003	o	1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts -LRB- Dunning 1993 Church and Hanks 1990 Dagan et al. 1999 -RRB-	appos_al._1999 nn_al._et nn_al._Dagan conj_and_Church_al. conj_and_Church_1990 conj_and_Church_Hanks dep_Dunning_al. dep_Dunning_1990 dep_Dunning_Hanks dep_Dunning_Church appos_Dunning_1993 dep_texts_Dunning nn_texts_language amod_texts_natural nn_association_word prep_in_similarity_texts conj_or_similarity_association nn_similarity_word prep_of_strength_association prep_of_strength_similarity det_strength_the dobj_measure_strength aux_measure_to xcomp_proposed_measure auxpass_proposed_been aux_proposed_have nsubjpass_proposed_tests amod_tests_statistical amod_tests_different amod_tests_Many nn_tests_Introduction num_tests_1 ccomp_``_proposed
N09-1032	J90-1003	o	The value of fj is calculated by Mutual Information -LRB- Church and Hanks 1990 -RRB- between xi and fj	conj_and_xi_fj appos_Church_1990 conj_and_Church_Hanks dep_Information_Hanks dep_Information_Church amod_Information_Mutual prep_between_calculated_fj prep_between_calculated_xi agent_calculated_Information auxpass_calculated_is nsubjpass_calculated_value prep_of_value_fj det_value_The
P01-1059	J90-1003	o	Strength of association between subject i and verb j is measured using mutual information -LRB- Church and Hanks 1990 -RRB- -RRB- ln -LRB- -RRB- -LRB- ji ij tftf tfNjiMI = Here tfij is the maximum frequency of subject-verb pair ij in the Reuters corpus tfi is the frequency of subject head noun i in the corpus tfj is the frequency of verb j in the corpus and N is the number of terms in the corpus	det_corpus_the prep_in_number_corpus prep_of_number_terms det_number_the cop_number_is nsubj_number_N det_corpus_the conj_and_verb_number prep_in_verb_corpus dobj_verb_j prep_of_frequency_number prep_of_frequency_verb det_frequency_the cop_frequency_is nsubj_frequency_tfj dep_frequency_i nn_frequency_noun det_corpus_the prep_in_i_corpus nn_noun_head amod_noun_subject prep_of_frequency_frequency det_frequency_the cop_frequency_is nsubj_frequency_tfi nn_corpus_Reuters det_corpus_the nn_ij_pair amod_ij_subject-verb parataxis_frequency_frequency prep_in_frequency_corpus prep_of_frequency_ij amod_frequency_maximum det_frequency_the cop_frequency_is nsubj_frequency_tfij advmod_frequency_Here parataxis_=_frequency dep_=_tfNjiMI dep_=_tftf dep_=_ij dep_=_ji dep_=_ln dep_=_measured num_Hanks_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual dobj_using_information xcomp_measured_using auxpass_measured_is nsubjpass_measured_Strength dep_j_verb conj_and_i_j amod_i_subject prep_between_association_j prep_between_association_i prep_of_Strength_association
P04-1022	J90-1003	o	The former extracts collocations within a fixed window -LRB- Church and Hanks 1990 Smadja 1993 -RRB-	amod_Smadja_1993 num_Hanks_1990 dep_Church_Smadja conj_and_Church_Hanks appos_window_Hanks appos_window_Church amod_window_fixed det_window_a prep_within_collocations_window nn_collocations_extracts amod_collocations_former det_collocations_The dep_``_collocations
P04-3019	J90-1003	n	Hanks and Church -LRB- 1990 -RRB- proposed using pointwise mutual information to identify collocations in lexicography however the method may result in unacceptable collocations for low-count pairs	amod_pairs_low-count prep_for_collocations_pairs amod_collocations_unacceptable prep_in_result_collocations aux_result_may nsubj_result_method advmod_result_however nsubj_result_Church nsubj_result_Hanks det_method_the prep_in_collocations_lexicography dobj_identify_collocations aux_identify_to amod_information_mutual amod_information_pointwise vmod_using_identify dobj_using_information xcomp_proposed_using appos_Church_1990 vmod_Hanks_proposed conj_and_Hanks_Church
P05-1014	J90-1003	p	The most widely used association weight function is -LRB- point-wise -RRB- Mutual Information -LRB- MI -RRB- -LRB- Church and Hanks 1990 Lin 1998 Dagan 2000 Weeds et al. 2004 -RRB-	amod_Weeds_2004 dep_Weeds_al. nn_Weeds_et dep_Dagan_Weeds num_Dagan_2000 dep_Lin_Dagan num_Lin_1998 dep_Church_Lin appos_Church_1990 conj_and_Church_Hanks dep_Information_Hanks dep_Information_Church appos_Information_MI amod_Information_Mutual dep_point-wise_Information dep_is_point-wise dep_function_is nn_function_weight nn_function_association amod_function_used det_function_The advmod_used_widely advmod_used_most
P05-1014	J90-1003	o	Concrete similarity measures compare a pair of weighted context feature vectors that characterize two words -LRB- Church and Hanks 1990 Ruge 1992 Pereira et al. 1993 Grefenstette 1994 Lee 1997 Lin 1998 Pantel and Lin 2002 Weeds and Weir 2003 -RRB-	num_Lin_1998 num_Lee_1997 num_Grefenstette_1994 num_Pereira_1993 nn_Pereira_al. nn_Pereira_et dep_Ruge_2003 conj_and_Ruge_Weir conj_and_Ruge_Weeds conj_and_Ruge_2002 conj_and_Ruge_Lin conj_and_Ruge_Pantel conj_and_Ruge_Lin conj_and_Ruge_Lee conj_and_Ruge_Grefenstette conj_and_Ruge_Pereira num_Ruge_1992 dep_Church_Weir dep_Church_Weeds dep_Church_2002 dep_Church_Lin dep_Church_Pantel dep_Church_Lin dep_Church_Lee dep_Church_Grefenstette dep_Church_Pereira dep_Church_Ruge appos_Church_1990 conj_and_Church_Hanks dep_words_Hanks dep_words_Church num_words_two dobj_characterize_words nsubj_characterize_that rcmod_vectors_characterize nn_vectors_feature nn_vectors_context amod_vectors_weighted prep_of_pair_vectors det_pair_a dobj_compare_pair nsubj_compare_measures nn_measures_similarity amod_measures_Concrete
P05-1075	J90-1003	o	METRIC FORMULA Frequency -LRB- Guiliano 1964 -RRB- x yf Pointwise Mutual Information -LSB- PMI -RSB- -LRB- Church & Hanks 1990 -RRB- -LRB- -RRB- xy x y2log / P P P True Mutual Information -LSB- TMI -RSB- -LRB- Manning 1999 -RRB- -LRB- -RRB- xy 2 xy x ylog / P P P P Chi-Squared -LRB- 2 -RRB- -LRB- Church and Gale 1991 -RRB- -LCB- -RCB- -LCB- -RCB- 2 -LRB- -RRB- i X X Y Y i j i j i j j f T-Score -LRB- Church & Hanks 1990 -RRB- 1 2 2 2 1 2 1 2 x x s s n n + C-Values4 -LRB- Frantzi Anadiou & Mima 2000 -RRB- 2 is not nested 2 log -LRB- -RRB- log -LRB- -RRB- 1 -LRB- -RRB- -LRB- -RRB- a a b T a f f f b P T where is the candidate string f -LRB- -RRB- is its frequency in the corpus T is the set of candidate terms that contain P -LRB- T -RRB- is the number of these candidate terms 609 1,700 of the three-word phrases are attested in the Lexile corpus	nn_corpus_Lexile det_corpus_the prep_in_attested_corpus auxpass_attested_are nsubjpass_attested_T amod_phrases_three-word det_phrases_the number_1,700_609 prep_of_terms_phrases num_terms_1,700 nn_terms_candidate det_terms_these prep_of_number_terms det_number_the cop_number_is nsubj_number_is advmod_number_where appos_P_T dobj_contain_P nsubj_contain_that rcmod_terms_contain nn_terms_candidate prep_of_set_terms det_set_the cop_set_is nsubj_set_frequency nn_T_corpus det_T_the prep_in_frequency_T poss_frequency_its cop_frequency_is rcmod_string_set dep_string_f nn_string_candidate det_string_the nsubj_is_string rcmod_T_number nn_T_P nn_T_b nn_T_f nn_T_f nn_T_f det_T_a nn_T_T nn_T_b det_T_a det_T_a num_T_1 nn_T_log nn_log_log num_log_2 parataxis_nested_attested neg_nested_not cop_nested_is nsubj_nested_ylog nsubj_nested_xy num_Mima_2000 conj_and_Frantzi_Mima conj_and_Frantzi_Anadiou num_n_2 appos_n_Mima appos_n_Anadiou appos_n_Frantzi conj_+_n_C-Values4 nn_n_n tmod_s_C-Values4 tmod_s_n vmod_s_s nn_s_x nn_s_x dep_s_2 num_s_2 dep_s_Hanks dep_s_Church number_2_1 number_2_2 dep_2_1 number_1_2 number_2_2 number_2_1 dep_Church_1990 conj_and_Church_Hanks dep_T-Score_s nn_T-Score_f nn_T-Score_j nn_T-Score_j nn_T-Score_i nn_T-Score_j nn_T-Score_i nn_T-Score_j nn_T-Score_i nn_T-Score_Y nn_T-Score_Y nn_T-Score_X nn_T-Score_X nn_T-Score_i num_T-Score_2 amod_T-Score_Chi-Squared nn_T-Score_P nn_T-Score_P nn_T-Score_P nn_T-Score_P dep_Church_1991 conj_and_Church_Gale dep_Chi-Squared_Gale dep_Chi-Squared_Church dep_Chi-Squared_2 dep_xy_T-Score conj_x_xy_ylog num_xy_2 nn_xy_xy appos_xy_Information appos_xy_y2log appos_xy_xy appos_Manning_1999 dep_Information_Manning appos_Information_TMI amod_Information_Mutual amod_Information_True nn_Information_P nn_Information_P nn_Information_P conj_x_xy_Information conj_x_xy_y2log dep_Church_1990 conj_and_Church_Hanks appos_Information_PMI nn_Information_Mutual nn_Information_Pointwise nn_Information_yf dep_Guiliano_1964 rcmod_Frequency_nested appos_Frequency_Hanks appos_Frequency_Church conj_x_Frequency_Information dep_Frequency_Guiliano nn_Frequency_FORMULA nn_Frequency_METRIC
P05-1075	J90-1003	o	A variety of methods have been applied ranging from simple frequency -LRB- Justeson & Katz 1995 -RRB- modified frequency measures such as c-values -LRB- Frantzi Anadiou & Mima 2000 Maynard & Anadiou 2000 -RRB- and standard statistical significance tests such as the t-test the chi-squared test and loglikelihood -LRB- Church and Hanks 1990 Dunning 1993 -RRB- and information-based methods e.g. pointwise mutual information -LRB- Church & Hanks 1990 -RRB-	dep_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual nn_information_pointwise pobj_e.g._information amod_methods_information-based num_Dunning_1993 num_Hanks_1990 appos_Church_Dunning conj_and_Church_Hanks dep_loglikelihood_Hanks dep_loglikelihood_Church amod_test_chi-squared det_test_the det_t-test_the prep_such_as_tests_t-test nn_tests_significance amod_tests_statistical amod_tests_standard num_Frantzi_2000 conj_and_Frantzi_Anadiou conj_and_Frantzi_Maynard dep_Frantzi_2000 conj_and_Frantzi_Mima conj_and_Frantzi_Anadiou conj_and_c-values_tests appos_c-values_Anadiou appos_c-values_Maynard appos_c-values_Mima appos_c-values_Anadiou appos_c-values_Frantzi prep_measures_e.g. conj_and_measures_methods conj_and_measures_loglikelihood conj_and_measures_test prep_such_as_measures_tests prep_such_as_measures_c-values nn_measures_frequency amod_measures_modified dep_Justeson_1995 conj_and_Justeson_Katz appos_frequency_Katz appos_frequency_Justeson amod_frequency_simple prep_from_ranging_frequency dobj_applied_methods dobj_applied_loglikelihood dobj_applied_test dobj_applied_measures xcomp_applied_ranging auxpass_applied_been aux_applied_have nsubjpass_applied_variety prep_of_variety_methods det_variety_A
P06-1036	J90-1003	o	To this end we follow the method introduced by -LRB- Church and Hanks 1990 -RRB- i.e. by sliding a window of a given size over some texts	det_texts_some amod_size_given det_size_a prep_of_window_size det_window_a prep_over_sliding_texts dobj_sliding_window prepc_by_i.e._sliding dep_Church_i.e. dep_Church_1990 conj_and_Church_Hanks agent_introduced_Hanks agent_introduced_Church vmod_method_introduced det_method_the dobj_follow_method nsubj_follow_we prep_to_follow_end det_end_this
P06-1036	J90-1003	o	Like -LRB- Church and Hanks 1990 -RRB- we used mutual information to measure the cohesion between two words	num_words_two det_cohesion_the prep_between_measure_words dobj_measure_cohesion aux_measure_to amod_information_mutual vmod_used_measure dobj_used_information nsubj_used_we prep_used_Like amod_Church_1990 conj_and_Church_Hanks dep_Like_Hanks dep_Like_Church
P06-2033	J90-1003	o	The information content of this set is defined as mutual information I -LRB- F -LRB- w -RRB- -RRB- -LRB- Church and Hanks 1990 -RRB-	amod_Hanks_1990 conj_and_Church_Hanks nn_Church_I nn_Church_information amod_Church_mutual appos_F_w dep_I_F prep_as_defined_Hanks prep_as_defined_Church auxpass_defined_is nsubjpass_defined_content det_set_this prep_of_content_set nn_content_information det_content_The
P06-2069	J90-1003	o	One can also examine the distribution of character or word ngrams e.g. Language Modeling -LRB- Croft and Lafferty 2003 -RRB- phrases -LRB- Church and Hanks 1990 Lewis 1992 -RRB- and so on	advmod_on_so num_Lewis_1992 conj_and_Church_Lewis conj_and_Church_1990 conj_and_Church_Hanks appos_phrases_Lewis appos_phrases_1990 appos_phrases_Hanks appos_phrases_Church dep_Croft_2003 conj_and_Croft_Lafferty conj_and_Modeling_on conj_and_Modeling_phrases appos_Modeling_Lafferty appos_Modeling_Croft nn_Modeling_Language pobj_e.g._on pobj_e.g._phrases pobj_e.g._Modeling nn_ngrams_word nn_ngrams_character conj_or_character_word prep_distribution_e.g. prep_of_distribution_ngrams det_distribution_the dobj_examine_distribution advmod_examine_also aux_examine_can nsubj_examine_One
P08-1013	J90-1003	o	To extract such word clusters we used suffix arrays proposed in Yamamoto and Church -LRB- 2001 -RRB- and the pointwise mutual information measure see Church and Hanks -LRB- 1990 -RRB-	appos_Hanks_1990 conj_and_Church_Hanks dobj_see_Hanks dobj_see_Church vmod_measure_see nn_measure_information amod_measure_mutual amod_measure_pointwise det_measure_the appos_Church_2001 conj_and_Yamamoto_Church prep_in_proposed_Church prep_in_proposed_Yamamoto vmod_arrays_proposed nn_arrays_suffix conj_and_used_measure dobj_used_arrays nsubj_used_we nn_clusters_word amod_clusters_such dep_extract_measure dep_extract_used dobj_extract_clusters aux_extract_To
P08-2046	J90-1003	o	To examine the effects of including some known AMs on the performance the following AMs had a 50 % chance of being included in the initial population pointwise mutual information -LRB- Church and Hanks 1990 -RRB- the Dice coefficient and the heuristic measure defined in -LRB- Petrovic et al. 2006 -RRB- H -LRB- a b c -RRB- = 2log f -LRB- abc -RRB- f -LRB- a -RRB- f -LRB- c -RRB- if POS -LRB- b -RRB- = X log f -LRB- abc -RRB- f -LRB- a -RRB- f -LRB- b -RRB- f -LRB- c -RRB- otherwise	nsubj_otherwise_f dep_otherwise_X dep_otherwise_= dep_otherwise_POS mark_otherwise_if appos_f_c nn_f_f nn_b_f det_b_a appos_f_b nn_f_f nn_f_log appos_f_abc appos_POS_b nn_c_f det_c_a appos_f_c nn_f_f appos_f_abc nn_f_2log dep_=_f nn_c_b det_c_a advcl_H_otherwise amod_H_= appos_H_c num_al._2006 nn_al._et amod_al._Petrovic dep_in_al. prep_defined_in dep_measure_H vmod_measure_defined nn_measure_heuristic det_measure_the nn_coefficient_Dice det_coefficient_the appos_Church_1990 conj_and_Church_Hanks conj_and_information_measure appos_information_coefficient dep_information_Hanks dep_information_Church amod_information_mutual nn_information_pointwise dep_:_measure dep_:_information amod_population_initial det_population_the prep_in_included_population auxpass_included_being prepc_of_chance_included amod_chance_% det_chance_a number_%_50 dobj_had_chance nsubj_had_AMs amod_AMs_following det_AMs_the det_performance_the rcmod_AMs_had prep_on_AMs_performance amod_AMs_known det_AMs_some pobj_including_AMs prepc_of_effects_including det_effects_the dobj_examine_effects aux_examine_To advcl_``_examine
P09-1072	J90-1003	o	Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs -LRB- Church & Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_co-occurs_Hanks dep_co-occurs_Church advmod_co-occurs_commonly dep_it_co-occurs dep_words_it prep_with_words_which conj_and_words_phrases prep_of_distribution_phrases prep_of_distribution_words det_distribution_the det_extent_some agent_captured_distribution prep_to_captured_extent auxpass_captured_is nsubjpass_captured_meaning rcmod_words_captured det_words_a prep_that_demonstrated_words aux_demonstrated_have nsubj_demonstrated_linguists amod_linguists_Computational
P09-1072	J90-1003	o	4 Using vector-based models of semantic representation to account for the systematic variances in neural activity 4.1 Lexical Semantic Representation Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_co-occurs_Hanks dep_co-occurs_Church advmod_co-occurs_commonly nsubj_co-occurs_it prep_with_co-occurs_which rcmod_words_co-occurs conj_and_words_phrases prep_of_distribution_phrases prep_of_distribution_words det_distribution_the det_extent_some agent_captured_distribution prep_to_captured_extent auxpass_captured_is nsubjpass_captured_meaning rcmod_words_captured det_words_a prep_that_demonstrated_words aux_demonstrated_have nsubj_demonstrated_4 amod_linguists_Computational nn_linguists_Representation nn_linguists_Semantic nn_linguists_Lexical num_linguists_4.1 dep_activity_linguists amod_activity_neural prep_in_variances_activity amod_variances_systematic det_variances_the prep_for_account_variances aux_account_to amod_representation_semantic prep_of_models_representation amod_models_vector-based vmod_Using_account dobj_Using_models vmod_4_Using ccomp_``_demonstrated
P91-1017	J90-1003	o	The use of such relations -LRB- mainly relations between verbs or nouns and their arguments and modifiers -RRB- for various purposes has received growing attention in recent research -LRB- Church and Hanks 1990 Zernik and Jacobs 1990 Hindle 1990 -RRB-	amod_Hindle_1990 dep_Zernik_Hindle conj_and_Zernik_1990 conj_and_Zernik_Jacobs conj_and_Church_1990 conj_and_Church_Jacobs conj_and_Church_Zernik conj_and_Church_1990 conj_and_Church_Hanks dep_research_Zernik dep_research_1990 dep_research_Hanks dep_research_Church amod_research_recent amod_attention_growing prep_in_received_research dobj_received_attention aux_received_has nsubj_received_use amod_purposes_various poss_arguments_their conj_and_verbs_modifiers conj_and_verbs_arguments conj_or_verbs_nouns prep_between_relations_modifiers prep_between_relations_arguments prep_between_relations_nouns prep_between_relations_verbs advmod_relations_mainly amod_relations_such prep_for_use_purposes dep_use_relations prep_of_use_relations det_use_The ccomp_``_received
P91-1017	J90-1003	o	In this case it is possible to perform the correct selection if we used only statistics about the cooccurrences of ` corruption ' with either ` investigator ' or ` researcher ' without looking for any syntactic relation -LRB- as in Church and Hanks -LRB- 1990 -RRB- -RRB-	appos_Hanks_1990 conj_and_Church_Hanks pobj_in_Hanks pobj_in_Church pcomp_as_in prep_relation_as nn_relation_syntactic det_relation_any prep_for_looking_relation conj_or_investigator_researcher preconj_investigator_either prep_with_corruption_researcher prep_with_corruption_investigator prep_of_cooccurrences_corruption det_cooccurrences_the prep_about_statistics_cooccurrences advmod_statistics_only prepc_without_used_looking dobj_used_statistics nsubj_used_we mark_used_if amod_selection_correct det_selection_the dobj_perform_selection aux_perform_to advcl_possible_used xcomp_possible_perform cop_possible_is nsubj_possible_it prep_in_possible_case det_case_this rcmod_``_possible
P91-1019	J90-1003	o	INTRODUCTION Word associations have been studied for some time in the fields of psycholinguistics -LRB- by testing human subjects on words -RRB- linguistics -LRB- where meaning is often based on how words co-occur with each other -RRB- and more recently by researchers in natural language processing -LRB- Church and Hanks 1990 Hindle and Rooth 1990 Dagan 1990 McDonald et al. 1990 Wilks et al. 1990 -RRB- using statistical measures to identify sets of associated words for use in various natural language processing tasks	nn_tasks_processing nn_tasks_language amod_tasks_natural amod_tasks_various prep_in_use_tasks prep_for_words_use amod_words_associated prep_of_sets_words dobj_identify_sets aux_identify_to amod_measures_statistical vmod_using_identify dobj_using_measures num_Wilks_1990 nn_Wilks_al. nn_Wilks_et num_McDonald_1990 nn_McDonald_al. nn_McDonald_et num_Dagan_1990 conj_and_Church_Wilks conj_and_Church_McDonald conj_and_Church_Dagan conj_and_Church_1990 conj_and_Church_Rooth conj_and_Church_Hindle conj_and_Church_1990 conj_and_Church_Hanks dep_processing_Wilks dep_processing_McDonald dep_processing_Dagan dep_processing_1990 dep_processing_Rooth dep_processing_Hindle dep_processing_1990 dep_processing_Hanks dep_processing_Church nn_processing_language amod_processing_natural prep_in_researchers_processing dep_recently_using prep_by_recently_researchers advmod_recently_more det_other_each prep_with_co-occur_other nsubj_co-occur_words advmod_co-occur_how prepc_based_on_linguistics_co-occur advmod_linguistics_often auxpass_linguistics_is nsubjpass_linguistics_meaning advmod_linguistics_where prep_on_subjects_words amod_subjects_human dobj_testing_subjects pcomp_by_testing prep_of_fields_psycholinguistics det_fields_the prep_in_time_fields det_time_some conj_and_studied_recently conj_and_studied_linguistics dep_studied_by prep_for_studied_time auxpass_studied_been aux_studied_have nsubjpass_studied_associations nn_associations_Word nn_associations_INTRODUCTION
P91-1027	J90-1003	o	Three recent papers in this area are Church and Hanks -LRB- 1990 -RRB- Hindle -LRB- 1990 -RRB- and Smadja and McKeown -LRB- 1990 -RRB-	appos_McKeown_1990 conj_and_Smadja_McKeown appos_Hindle_1990 appos_Hanks_1990 conj_and_Church_McKeown conj_and_Church_Smadja conj_and_Church_Hindle conj_and_Church_Hanks cop_Church_are nsubj_Church_papers det_area_this prep_in_papers_area amod_papers_recent num_papers_Three
P92-1014	J90-1003	o	In addition IC is stable even for relatively low frequency words which can be contrasted with Fano 's mutual information formula recently used by Church and Hanks -LRB- 1990 -RRB- to compute word cooccurrence patterns in a 44 million word corpus of Associated Press news stories	nn_stories_news nn_stories_Press nn_stories_Associated prep_of_corpus_stories nn_corpus_word num_corpus_million det_corpus_a number_million_44 nn_patterns_cooccurrence nn_patterns_word prep_in_compute_corpus dobj_compute_patterns aux_compute_to appos_Hanks_1990 conj_and_Church_Hanks agent_used_Hanks agent_used_Church advmod_used_recently vmod_formula_used nn_formula_information amod_formula_mutual poss_formula_Fano xcomp_contrasted_compute prep_with_contrasted_formula auxpass_contrasted_be aux_contrasted_can nsubjpass_contrasted_which rcmod_words_contrasted nn_words_frequency amod_words_low advmod_low_relatively prep_for_stable_words advmod_stable_even cop_stable_is nsubj_stable_IC prep_in_stable_addition
P92-1052	J90-1003	p	Researchers such as -LRB- Evans et al. 1991 -RRB- and -LRB- Church and Hanks 1990 -RRB- have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subject-verb verb-object pairs	amod_pairs_verb-object appos_subject-verb_pairs conj_and_phrases_subject-verb nn_phrases_noun amod_phrases_interesting nn_phrases_extract amod_corpora_large amod_techniques_statistical conj_and_grammars_techniques amod_grammars_robust prep_to_applied_subject-verb prep_to_applied_phrases prep_over_applied_corpora dobj_applied_techniques dobj_applied_grammars aux_applied_have nsubj_applied_Church nsubj_applied_Evans mark_applied_as num_Hanks_1990 conj_and_Church_Hanks dep_1991_al. nn_al._et conj_and_Evans_Hanks conj_and_Evans_Church num_Evans_1991 mwe_as_such advcl_Researchers_applied
P93-1022	J90-1003	o	Statistical data about these various cooccurrence relations is employed for a variety of applications such as speech recognition -LRB- Jelinek 1990 -RRB- language generation -LRB- Smadja and McKeown 1990 -RRB- lexicography -LRB- Church and Hanks 1990 -RRB- machine translation -LRB- Brown et al. Sadler 1989 -RRB- information retrieval -LRB- Maarek and Smadja 1989 -RRB- and various disambiguation tasks -LRB- Dagan et al. 1991 Hindle and Rooth 1991 Grishman et al. 1986 Dagan and Itai 1990 -RRB-	dep_Dagan_1990 conj_and_Dagan_Itai num_Grishman_1986 nn_Grishman_al. nn_Grishman_et dep_Hindle_Itai dep_Hindle_Dagan conj_and_Hindle_Grishman conj_and_Hindle_1991 conj_and_Hindle_Rooth dep_Dagan_Grishman dep_Dagan_1991 dep_Dagan_Rooth dep_Dagan_Hindle appos_Dagan_1991 dep_Dagan_al. nn_Dagan_et dep_tasks_Dagan nn_tasks_disambiguation amod_tasks_various conj_and_Maarek_1989 conj_and_Maarek_Smadja dep_retrieval_1989 dep_retrieval_Smadja dep_retrieval_Maarek nn_retrieval_information dep_Sadler_1989 dep_al._Sadler nn_al._et amod_al._Brown dep_translation_al. nn_translation_machine amod_Church_1990 conj_and_Church_Hanks appos_lexicography_Hanks appos_lexicography_Church dep_Smadja_1990 conj_and_Smadja_McKeown appos_generation_McKeown appos_generation_Smadja nn_generation_language dep_Jelinek_1990 conj_and_recognition_tasks conj_and_recognition_retrieval conj_and_recognition_translation conj_and_recognition_lexicography conj_and_recognition_generation dep_recognition_Jelinek nn_recognition_speech prep_such_as_applications_tasks prep_such_as_applications_retrieval prep_such_as_applications_translation prep_such_as_applications_lexicography prep_such_as_applications_generation prep_such_as_applications_recognition prep_of_variety_applications det_variety_a prep_for_employed_variety auxpass_employed_is nsubjpass_employed_data nn_relations_cooccurrence amod_relations_various det_relations_these prep_about_data_relations amod_data_Statistical ccomp_``_employed
P93-1022	J90-1003	o	The mutual information of a cooccurrence pair which measures the degree of association between the two words -LRB- Church and Hanks 1990 -RRB- is defined as -LRB- Fano 1961 -RRB- P -LRB- xly -RRB- I -LRB- x y -RRB- log 2 P -LRB- x y -RRB- _ log 2 -LRB- 1 -RRB- P -LRB- x -RRB- P -LRB- y -RRB- P -LRB- x -RRB- = log 2 P -LRB- y \ -LSB- x -RRB- P -LRB- Y -RRB- where P -LRB- x -RRB- and P -LRB- y -RRB- are the probabilities of the events x and y -LRB- occurrences of words in our case -RRB- and P -LRB- x y -RRB- is the probability of the joint event -LRB- a cooccurrence pair -RRB-	nn_pair_cooccurrence det_pair_a appos_event_pair amod_event_joint det_event_the prep_of_probability_event det_probability_the cop_probability_is nsubj_probability_y dobj_probability_P appos_x_y dep_P_x poss_case_our prep_in_occurrences_case prep_of_occurrences_words conj_and_x_y nn_x_events det_x_the conj_and_probabilities_P dep_probabilities_occurrences prep_of_probabilities_y prep_of_probabilities_x det_probabilities_the cop_probabilities_are nsubj_probabilities_P nsubj_probabilities_P advmod_probabilities_where appos_P_y conj_and_P_P appos_P_x appos_P_Y rcmod_y_P rcmod_y_probabilities dep_y_P appos_y_x num_y_\ num_P_2 nn_P_log dep_=_probability amod_P_= appos_P_x nn_P_P appos_P_y dep_P_P appos_P_x num_P_1 npadvmod_log_P num_log_2 num_log__ appos_x_y dep_P_log dep_P_x num_P_2 nn_P_log appos_x_y dep_I_P dep_I_x nn_I_P appos_P_xly amod_Fano_1961 dep_as_Fano dep_defined_I prep_defined_as auxpass_defined_is nsubjpass_defined_information amod_Church_1990 conj_and_Church_Hanks appos_words_Hanks appos_words_Church num_words_two det_words_the prep_between_association_words prep_of_degree_association det_degree_the dobj_measures_degree nsubj_measures_which rcmod_pair_measures nn_pair_cooccurrence det_pair_a prep_of_information_pair amod_information_mutual det_information_The
P93-1043	J90-1003	o	In the results we describe here we use mutual information -LRB- Fano 1961 27-28 Church and Hanks 1990 -RRB- as the metric for neighbourhood pruning pruning which occurs as the network is being generated	auxpass_generated_being aux_generated_is nsubjpass_generated_network mark_generated_as det_network_the advcl_occurs_generated nsubj_occurs_which rcmod_pruning_occurs appos_pruning_pruning nn_pruning_neighbourhood det_metric_the prep_for_-RRB-_pruning prep_as_-RRB-_metric dep_Church_1990 conj_and_Church_Hanks dep_Fano_Hanks dep_Fano_Church num_Fano_27-28 num_Fano_1961 dep_-LRB-_Fano amod_information_mutual dobj_use_information nsubj_use_we advcl_use_describe advmod_describe_here nsubj_describe_we prep_in_describe_results det_results_the
P97-1007	J90-1003	o	Thus given a hyponym definition -LRB- O -RRB- and a set of candidate hypernym definitions this method selects the candidate hypernym definition -LRB- E -RRB- which returns the maximum score given by formula -LRB- 1 -RRB- SC -LRB- O E -RRB- E cw -LRB- wi wj -RRB- -LRB- I -RRB- ` wIEOAwj6E The cooccurrence weight -LRB- cw -RRB- between two words can be given by Cooccurrence Frequency Mutual Information -LRB- Church and Hanks 1990 -RRB- or Association Ratio -LRB- Resnik 1992 -RRB-	amod_Resnik_1992 dep_Ratio_Resnik nn_Ratio_Association dep_Church_1990 conj_and_Church_Hanks dep_Information_Hanks dep_Information_Church nn_Information_Mutual conj_or_Frequency_Ratio conj_or_Frequency_Information nn_Frequency_Cooccurrence agent_given_Ratio agent_given_Information agent_given_Frequency auxpass_given_be aux_given_can nsubjpass_given_wIEOAwj6E num_words_two prep_between_weight_words appos_weight_cw nn_weight_cooccurrence det_weight_The dobj_wIEOAwj6E_weight appos_wi_wj dep_cw_given appos_cw_I dep_cw_wi nn_cw_E appos_O_E dep_SC_cw dep_SC_O dep_formula_SC appos_formula_1 agent_given_formula vmod_score_given amod_score_maximum det_score_the dobj_returns_score nsubj_returns_which rcmod_definition_returns appos_definition_E nn_definition_hypernym nn_definition_candidate det_definition_the dobj_selects_definition nsubj_selects_method prep_selects_given advmod_selects_Thus det_method_this nn_definitions_hypernym nn_definitions_candidate prep_of_set_definitions det_set_a conj_and_definition_set appos_definition_O nn_definition_hyponym det_definition_a pobj_given_set pobj_given_definition
P97-1024	J90-1003	o	In each experiment performance IMutu ' d Information provides an estimate of the magnitude of the ratio t -RRB- ctw -LRB- -LRB- n the joint prol -RRB- ability P -LRB- verb/noun ,1 -RRB- reposition -RRB- and the joint probability a. ~ suming indcpendcnce P -LRB- verb/noun -RRB- P -LRB- prcl -RRB- osition -RRB- s -LRB- :-LRB- -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks appos_:-LRB-_Hanks appos_:-LRB-_Church dep_s_:-LRB- nn_s_osition nn_s_P nn_s_P appos_P_prcl appos_P_verb/noun nn_P_indcpendcnce amod_P_suming nn_P_~ prep_a._probability_s amod_probability_joint det_probability_the amod_,1_verb/noun appos_P_,1 dep_ability_reposition dep_ability_P dep_ability_n amod_prol_joint det_prol_the dep_n_prol appos_ctw_ability dep_t_ctw nn_t_ratio det_t_the prep_of_magnitude_t det_magnitude_the prep_of_estimate_magnitude det_estimate_an conj_and_provides_probability dobj_provides_estimate nsubj_provides_IMutu dep_provides_performance prep_in_provides_experiment nn_Information_d conj_IMutu_Information det_experiment_each
P98-1065	J90-1003	o	The collocations have been calculated according to the method described in Church and Hanks -LRB- 1990 -RRB- by moving a window on the texts	det_texts_the det_window_a prep_on_moving_texts dobj_moving_window appos_Hanks_1990 conj_and_Church_Hanks agent_described_moving prep_in_described_Hanks prep_in_described_Church vmod_method_described det_method_the pobj_calculated_method prepc_according_to_calculated_to auxpass_calculated_been aux_calculated_have nsubjpass_calculated_collocations det_collocations_The
P98-1065	J90-1003	o	The cohesion between two words is measured as in Church and Hanks -LRB- 1990 -RRB- by an estimation of the mutual information based on their collocation frequency	nn_frequency_collocation poss_frequency_their amod_information_mutual det_information_the pobj_estimation_frequency prepc_based_on_estimation_on prep_of_estimation_information det_estimation_an pobj_by_estimation appos_Hanks_1990 conj_and_Church_Hanks pcomp_as_by prep_in_as_Hanks prep_in_as_Church prep_measured_as auxpass_measured_is nsubjpass_measured_cohesion num_words_two prep_between_cohesion_words det_cohesion_The
P98-1100	J90-1003	o	Collocation Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio -LRB- Church and Hanks 1990 -RRB- and outputted to a lexicon	det_lexicon_a prep_to_outputted_lexicon nsubjpass_outputted_Collocations amod_Church_1990 conj_and_Church_Hanks appos_ratio_Hanks appos_ratio_Church nn_ratio_association det_ratio_the dobj_using_ratio nn_Corpus_Language nn_Corpus_English nn_Corpus_Longman det_Corpus_the vmod_sample_using prep_of_sample_Corpus nn_sample_word num_sample_million det_sample_a number_million_seven conj_and_extracted_outputted prep_from_extracted_sample auxpass_extracted_were nsubjpass_extracted_Collocations dep_Collocation_outputted dep_Collocation_extracted
P98-2231	J90-1003	o	For instance Church and Hanks -LRB- 1990 -RRB- calculated SA in terms of mutual information between two words wl and w2 N * f -LRB- wl w2 -RRB- I -LRB- wl w2 -RRB- = log2 -LRB- 1 -RRB- f -LRB- wl -RRB- f -LRB- w2 -RRB- here N is the size of the corpus used in the estimation f -LRB- Wl w2 -RRB- is the frequency of the cooccurrence f -LRB- wl -RRB- and f -LRB- w2 -RRB- that of each word	det_word_each prep_of_that_word nn_w2_f dep_wl_that conj_and_wl_w2 nn_wl_f det_cooccurrence_the appos_frequency_w2 appos_frequency_wl prep_of_frequency_cooccurrence det_frequency_the cop_frequency_is nsubj_frequency_size appos_Wl_w2 nn_Wl_f det_estimation_the prep_in_used_estimation vmod_corpus_used det_corpus_the appos_size_Wl prep_of_size_corpus det_size_the cop_size_is nsubj_size_N advmod_size_here nn_size_f amod_size_= nn_size_I appos_f_w2 nn_f_f nn_f_log2 appos_f_wl num_f_1 appos_wl_w2 dep_I_wl nn_I_f appos_wl_w2 dep_f_wl dep_f_* nn_f_N conj_and_wl_w2 dep_words_w2 dep_words_wl num_words_two prep_between_information_words amod_information_mutual prep_of_terms_information parataxis_calculated_frequency prep_in_calculated_terms dobj_calculated_SA nsubj_calculated_Hanks nsubj_calculated_Church prep_for_calculated_instance appos_Hanks_1990 conj_and_Church_Hanks
P98-2243	J90-1003	o	The cohesion between words has been evaluated with the mutual information measure as in -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_in_Hanks dep_in_Church pcomp_as_in nn_measure_information amod_measure_mutual det_measure_the prep_evaluated_as prep_with_evaluated_measure auxpass_evaluated_been aux_evaluated_has nsubjpass_evaluated_cohesion prep_between_cohesion_words det_cohesion_The
P99-1004	J90-1003	p	Arguably the most widely used is the mutual information -LRB- Hindle 1990 Church and Hanks 1990 Dagan et al. 1995 Luk 1995 D. Lin 1998a -RRB-	appos_Lin_1998a nn_Lin_D. dep_Luk_1995 dep_al._1995 nn_al._et nn_al._Dagan dep_Church_Lin conj_and_Church_Luk conj_and_Church_al. conj_and_Church_1990 conj_and_Church_Hanks dep_Hindle_Luk dep_Hindle_al. dep_Hindle_1990 dep_Hindle_Hanks dep_Hindle_Church appos_Hindle_1990 dep_information_Hindle amod_information_mutual det_information_the cop_information_is nsubj_information_used advmod_used_widely det_used_the advmod_used_Arguably advmod_widely_most
P99-1029	J90-1003	o	We then propose a relatively simple yet effective method for resolving translation disambiguation using mutual information -LRB- MI -RRB- -LRB- Church and Hanks 1990 -RRB- statistics obtained only from the target document collection	nn_collection_document nn_collection_target det_collection_the prep_from_obtained_collection advmod_obtained_only vmod_statistics_obtained nn_statistics_information dep_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church appos_information_MI amod_information_mutual dobj_using_statistics nn_disambiguation_translation vmod_resolving_using dobj_resolving_disambiguation prepc_for_method_resolving amod_method_effective amod_method_simple det_method_a advmod_effective_yet advmod_simple_relatively dobj_propose_method advmod_propose_then nsubj_propose_We ccomp_``_propose
P99-1029	J90-1003	o	The mutual information Ml -LRB- x y -RRB- is defined as the following formula -LRB- Church and Hanks 1990 -RRB-	amod_Hanks_1990 conj_and_Church_Hanks nn_Church_formula amod_Church_following det_Church_the prep_as_defined_Hanks prep_as_defined_Church auxpass_defined_is nsubjpass_defined_Ml appos_x_y dep_Ml_x nn_Ml_information amod_Ml_mutual det_Ml_The
P99-1051	J90-1003	o	We preferred the log-likelihood ratio to other statistical scores such as the association ratio -LRB- Church and Hanks 1990 -RRB- or -LRB- 2 since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpussize -LRB- Dunning 1993 Daille 1996 -RRB-	amod_Daille_1996 dep_Dunning_Daille appos_Dunning_1993 appos_corpussize_Dunning conj_and_events_corpussize amod_events_rare prep_to_sensitive_corpussize prep_to_sensitive_events advmod_sensitive_less cop_sensitive_is nsubj_sensitive_it amod_words_co-occurring det_words_the prep_of_frequency_words det_frequency_the conj_and_takes_sensitive dobj_takes_frequency prep_into_takes_account advmod_takes_adequately nsubj_takes_it mark_takes_since rcmod_2_sensitive rcmod_2_takes dep_Church_1990 conj_and_Church_Hanks conj_or_ratio_2 appos_ratio_Hanks appos_ratio_Church nn_ratio_association det_ratio_the prep_such_as_scores_2 prep_such_as_scores_ratio amod_scores_statistical amod_scores_other prep_to_ratio_scores amod_ratio_log-likelihood det_ratio_the dobj_preferred_ratio nsubj_preferred_We ccomp_``_preferred
W00-1106	J90-1003	o	For mutual information -LRB- MI -RRB- we use two different equations one for two-element compound nouns -LRB- Church and Hanks 1990 -RRB- and the other for three-element compound nouns -LRB- Suet al. 1994 -RRB-	dep_al._1994 nn_al._Suet nn_nouns_compound amod_nouns_three-element prep_for_other_nouns det_other_the amod_Church_1990 conj_and_Church_Hanks nn_nouns_compound amod_nouns_two-element dep_one_al. conj_and_one_other dep_one_Hanks dep_one_Church prep_for_one_nouns dep_equations_other dep_equations_one amod_equations_different num_equations_two dobj_use_equations nsubj_use_we prep_for_use_information appos_information_MI amod_information_mutual
W00-1313	J90-1003	o	The association relationship between two words can be indicated by their mutual information which can be further used to discover phrases \ -LSB- Church & Hanks -LRB- 1990 -RRB- \ -RSB-	dep_\_1990 nn_\_Hanks conj_and_Church_\ dep_\_\ dep_\_Church amod_phrases_\ dobj_discover_phrases aux_discover_to xcomp_used_discover advmod_used_further auxpass_used_be aux_used_can nsubjpass_used_which rcmod_information_used amod_information_mutual poss_information_their agent_indicated_information auxpass_indicated_be aux_indicated_can nsubjpass_indicated_relationship num_words_two prep_between_relationship_words nn_relationship_association det_relationship_The
W01-0513	J90-1003	o	We then rank-order the P X | Y MI XY M Z Pr Z | Y MI ZY G092log -LSB- P X P Y P X P Y -RSB- f Y -LSB- P XY P XY -RSB- f XY -LSB- P XY P XY -RSB- f XY M iG13X X -RCB- jG13Y Y -RCB- -LRB- f ij G09 ij -RRB- 2 ij f XY G09 XY XY -LRB- 1G09 -LRB- XY / N -RRB- -RRB- f XY G09 XY f XY -LRB- 1G09 -LRB- f XY / N -RRB- -RRB- Table 1 Probabilistic Approaches METHOD FORMULA Frequency -LRB- Guiliano 1964 -RRB- f XY Pointwise Mutual Information -LRB- MI -RRB- -LRB- Fano 1961 Church and Hanks 1990 -RRB- log -LRB- P / PP -RRB- 2XY XY Selectional Association -LRB- Resnik 1996 -RRB- Symmetric Conditional Probability -LRB- Ferreira and Pereira 1999 -RRB- P / PP XY X Y 2 Dice Formula -LRB- Dice 1945 -RRB- 2 f / -LRB- f + f -RRB- XY X Y Log-likelihood -LRB- Dunning 1993 -LRB- Daille 1996 -RRB-	amod_Daille_1996 dep_Dunning_Daille appos_Dunning_1993 dep_Log-likelihood_Dunning nn_Log-likelihood_Y nn_Log-likelihood_X nn_Log-likelihood_XY dep_Log-likelihood_f dep_f_+ dep_f_f dep_f_P nsubj_f_Probability dep_f_Fano dep_f_Information dep_f_f dep_f_Frequency num_f_2 dep_Dice_1945 dep_Formula_f dep_Formula_Dice nn_Formula_Dice num_Formula_2 dep_Y_Formula nn_Y_X nn_Y_XY nn_Y_PP dep_P_Y dep_Ferreira_1999 conj_and_Ferreira_Pereira appos_Probability_Pereira appos_Probability_Ferreira amod_Probability_Conditional amod_Probability_Symmetric nn_Probability_Association dep_Resnik_1996 dep_Association_Resnik nn_Association_Selectional nn_Association_XY nn_Association_2XY dep_Association_P nn_Association_log dep_P_PP dep_Church_1990 conj_and_Church_Hanks dep_Fano_Hanks dep_Fano_Church appos_Fano_1961 appos_Information_MI nn_Information_Mutual nn_Information_Pointwise nn_Information_XY dep_Guiliano_1964 appos_Frequency_Guiliano nn_Frequency_FORMULA nn_Frequency_METHOD nn_Frequency_Approaches nn_Frequency_Probabilistic dep_Table_Log-likelihood num_Table_1 nn_Table_XY dep_XY_N nn_XY_f dep_1G09_XY appos_XY_1G09 nn_XY_f nn_XY_XY nn_XY_G09 nn_XY_XY dep_f_Table dep_XY_N dep_1G09_XY amod_XY_f appos_XY_1G09 nn_XY_XY nn_XY_G09 nn_XY_XY nn_XY_f nn_XY_ij num_XY_2 dep_ij_XY nn_ij_G09 nn_ij_ij nn_ij_f nn_jG13Y_X nn_iG13X_M nn_iG13X_XY nn_iG13X_f appos_XY_Y conj_XY_jG13Y dep_XY_iG13X nn_XY_P nn_XY_XY nn_XY_P nn_XY_f nn_XY_Y nn_XY_Z det_XY_the nn_XY_P nn_XY_XY nn_XY_P appos_Y_XY nn_Y_f nn_Y_G092log nn_Y_P nn_Y_X nn_Y_P nn_Y_Y nn_Y_P nn_Y_X nn_Y_P appos_G092log_Y nn_G092log_ZY nn_G092log_MI nn_G092log_Y num_G092log_| nn_Z_Pr nn_Z_Z nn_Z_M nn_Z_XY nn_Z_MI nn_Z_Y num_Z_| nn_Z_X nn_Z_P dep_rank-order_ij dep_rank-order_XY dobj_rank-order_XY advmod_rank-order_then nsubj_rank-order_We ccomp_``_rank-order
W01-0513	J90-1003	o	Since we need knowledge-poor Daille 1996 -RRB- induction we can not use human-suggested filtering Chi-squared -LRB- G24 -RRB- 2 -LRB- Church and Gale 1991 -RRB- Z-Score -LRB- Smadja 1993 Fontenelle et al. 1994 -RRB- Students t-Score -LRB- Church and Hanks 1990 -RRB- n-gram list in accordance to each probabilistic algorithm	amod_algorithm_probabilistic det_algorithm_each prep_to_list_algorithm prep_in_list_accordance nn_list_n-gram dep_list_Hanks dep_list_Church nn_list_t-Score nn_list_Students dep_Church_1990 conj_and_Church_Hanks dep_al._1994 nn_al._et dep_Fontenelle_al. dep_Smadja_1993 nn_Smadja_Z-Score dep_Smadja_Gale dep_Gale_1991 conj_and_Church_Smadja num_Church_2 amod_Church_Chi-squared amod_Church_filtering amod_Church_human-suggested appos_Chi-squared_G24 dobj_use_Smadja dobj_use_Church neg_use_not aux_use_can nsubj_use_we dep_induction_list dep_induction_Fontenelle rcmod_induction_use dep_induction_need dep_Daille_1996 amod_Daille_knowledge-poor dobj_need_Daille nsubj_need_we mark_need_Since
W02-1115	J90-1003	p	There are several distance measures suitable for this purpose such as the mutual information -LRB- Church and Hanks 1990 -RRB- the dice coefficient -LRB- Manning and Schueutze 8.5 1999 -RRB- the phi coefficient -LRB- Manning and Schuetze 5.3.3 1999 -RRB- the cosine measure -LRB- Manning and Schueutze 8.5 1999 -RRB- and the confidence -LRB- Arrawal and Srikant 1995 -RRB-	amod_Srikant_1995 conj_and_Arrawal_Srikant dep_confidence_Srikant dep_confidence_Arrawal det_confidence_the num_Schueutze_8.5 conj_and_Manning_1999 conj_and_Manning_Schueutze dep_measure_1999 dep_measure_Schueutze dep_measure_Manning nn_measure_cosine det_measure_the num_Schuetze_5.3.3 dep_Manning_1999 conj_and_Manning_Schuetze dep_coefficient_Schuetze dep_coefficient_Manning nn_coefficient_phi det_coefficient_the num_Schueutze_8.5 dep_Manning_1999 conj_and_Manning_Schueutze conj_and_coefficient_confidence appos_coefficient_measure appos_coefficient_coefficient appos_coefficient_Schueutze appos_coefficient_Manning nn_coefficient_dice det_coefficient_the appos_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual det_information_the det_purpose_this prep_for_suitable_purpose appos_measures_confidence appos_measures_coefficient prep_such_as_measures_information amod_measures_suitable nn_measures_distance amod_measures_several nsubj_are_measures expl_are_There ccomp_``_are
W03-1805	J90-1003	o	3 Related work Word collocation Various collocation metrics have been proposed including mean and variance -LRB- Smadja 1994 -RRB- the t-test -LRB- Church et al. 1991 -RRB- the chi-square test pointwise mutual information -LRB- MI -RRB- -LRB- Church and Hanks 1990 -RRB- and binomial loglikelihood ratio test -LRB- BLRT -RRB- -LRB- Dunning 1993 -RRB-	amod_Dunning_1993 dep_test_Dunning appos_test_BLRT nn_test_ratio nn_test_loglikelihood amod_test_binomial dep_Church_1990 conj_and_Church_Hanks appos_information_Hanks appos_information_Church appos_information_MI amod_information_mutual amod_information_pointwise amod_test_chi-square det_test_the amod_Church_1991 dep_Church_al. nn_Church_et conj_and_t-test_test conj_and_t-test_information appos_t-test_test dep_t-test_Church det_t-test_the dep_,_test dep_,_information dep_,_t-test amod_Smadja_1994 dep_mean_Smadja conj_and_mean_variance prep_including_proposed_variance prep_including_proposed_mean auxpass_proposed_been aux_proposed_have nsubjpass_proposed_metrics nn_metrics_collocation amod_metrics_Various nn_metrics_collocation nn_metrics_Word nn_metrics_work amod_metrics_Related num_metrics_3 ccomp_``_proposed
W03-1805	J90-1003	o	a11a29a9 thea13 thea15 a1a4a3a6a5 a11a29a9 thea13 thea15 a11a29a9 thea15 a11a29a9 thea15a1a0 a2 since a11a2a9 thea13 thea15a4a3 a11a29a9 thea15 a11a29a9 thea15 Also note that in the case of phraseness of a bigram the equation looks similar to pointwise mutual information -LRB- Church and Hanks 1990 -RRB- but they are different	cop_different_are nsubj_different_they appos_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual amod_information_pointwise prep_to_similar_information acomp_looks_similar nsubj_looks_equation prep_in_looks_case mark_looks_that det_equation_the det_bigram_a prep_of_phraseness_bigram prep_of_case_phraseness det_case_the conj_but_note_different ccomp_note_looks advmod_note_Also nsubj_note_a11a29a9 nn_thea15_a11a29a9 prep_thea15_a11a29a9_thea15 nn_a11a29a9_thea15a4a3 nn_a11a29a9_thea13 dep_a11a2a9_a11a29a9 prep_since_a2_a11a2a9 nn_a2_thea15a1a0 nn_a2_a11a29a9 amod_a2_thea15 nn_a2_a11a29a9 num_a11a29a9_thea15 prep_thea13_a11a29a9_a2 nn_a11a29a9_a1a4a3a6a5 prep_thea15_thea13_a11a29a9 dep_a11a29a9_thea13
W04-1113	J90-1003	o	Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction -LRB- Church and Hanks 1990 Smadja 1993 Choueka 1993 Lin 1998 -RRB-	num_Lin_1998 num_Choueka_1993 num_Smadja_1993 num_Hanks_1990 appos_Church_Lin appos_Church_Choueka appos_Church_Smadja conj_and_Church_Hanks dep_extraction_Hanks dep_extraction_Church nn_extraction_collocation prep_in_faced_extraction vmod_issues_faced det_issues_the prep_to_insights_issues det_insights_some dobj_gained_insights aux_gained_has nsubj_gained_Study amod_statistics_lexical dobj_using_statistics nn_extraction_collocation vmod_Study_using prep_in_Study_extraction
W04-1113	J90-1003	o	Church and Hanks -LRB- Church and Hanks 1990 -RRB- employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window	amod_window_fixed-size det_window_a prep_within_co-occur_window aux_co-occur_to xcomp_tend_co-occur nsubj_tend_that rcmod_bi-grams_tend amod_bi-grams_distant amod_bi-grams_adjacent conj_and_adjacent_distant preconj_adjacent_both dobj_extract_bi-grams aux_extract_to vmod_information_extract amod_information_mutual dobj_employed_information nsubj_employed_Hanks nsubj_employed_Church num_Hanks_1990 conj_and_Church_Hanks dep_Church_Hanks dep_Church_Church conj_and_Church_Hanks
W04-1114	J90-1003	o	The typical problems like doctor-nurse -LRB- Church and Hanks 1990 -RRB- could be avoided by using such information	amod_information_such dobj_using_information agent_avoided_using auxpass_avoided_be aux_avoided_could nsubjpass_avoided_problems num_Hanks_1990 conj_and_Church_Hanks nn_Church_doctor-nurse prep_like_problems_Hanks prep_like_problems_Church amod_problems_typical det_problems_The
W04-2105	J90-1003	o	There are several basic methods for evaluating associations between words based on frequency counts -LRB- Choueka 1988 Wettler and Rapp 1993 -RRB- information theoretic -LRB- Church and Hanks 1990 -RRB- and statistical significance -LRB- Smadja 1993 -RRB-	amod_Smadja_1993 amod_significance_statistical dep_Church_1990 conj_and_Church_Hanks dep_theoretic_Hanks dep_theoretic_Church dep_information_Smadja conj_and_information_significance amod_information_theoretic dep_Wettler_1993 conj_and_Wettler_Rapp dep_Choueka_Rapp dep_Choueka_Wettler appos_Choueka_1988 appos_counts_significance appos_counts_information appos_counts_Choueka nn_counts_frequency prep_on_based_counts prep_between_associations_words dobj_evaluating_associations vmod_methods_based prepc_for_methods_evaluating amod_methods_basic amod_methods_several nsubj_are_methods expl_are_There ccomp_``_are
W05-0101	J90-1003	o	After building the chunker students were asked to 4 choose a verb and then analyze verb-argument structure -LRB- they were provided with two relevant papers -LRB- Church and Hanks 1990 Chklovski and Pantel 2004 -RRB- -RRB-	appos_Chklovski_2004 conj_and_Chklovski_Pantel conj_and_Church_Pantel conj_and_Church_Chklovski conj_and_Church_1990 conj_and_Church_Hanks dep_papers_Chklovski dep_papers_1990 dep_papers_Hanks dep_papers_Church amod_papers_relevant num_papers_two prep_with_provided_papers auxpass_provided_were nsubjpass_provided_they amod_structure_verb-argument dep_analyze_provided dobj_analyze_structure dep_verb_analyze conj_and_verb_then det_verb_a dep_choose_then dep_choose_verb dep_asked_choose prep_to_asked_4 auxpass_asked_were nsubjpass_asked_students prepc_after_asked_building det_chunker_the dobj_building_chunker
W05-0829	J90-1003	o	In the following sections we will use 2 statistics to measure the the mutual translation likelihood -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_likelihood_Hanks dep_likelihood_Church nn_likelihood_translation amod_likelihood_mutual det_likelihood_the dep_the_likelihood dobj_measure_the aux_measure_to num_statistics_2 vmod_use_measure dobj_use_statistics aux_use_will nsubj_use_we prep_in_use_the prep_following_the_sections
W06-0308	J90-1003	o	PMI -LRB- Church and Hanks 1990 -RRB- between two phrases is de ned as log2 prob -LRB- ph1 is near ph2 -RRB- prob -LRB- ph 1 -RRB- prob -LRB- ph2 -RRB- PMI is positive when two phrases tend to co-occur and negative when they tend to be in a complementary distribution	amod_distribution_complementary det_distribution_a prep_in_be_distribution aux_be_to xcomp_tend_be nsubj_tend_they advmod_tend_when conj_and_co-occur_negative aux_co-occur_to advcl_tend_tend xcomp_tend_negative xcomp_tend_co-occur nsubj_tend_phrases advmod_tend_when num_phrases_two advcl_positive_tend cop_positive_is nsubj_positive_PMI nn_PMI_prob appos_prob_ph2 nn_prob_prob num_ph_1 appos_prob_ph nn_prob_prob prep_near_is_ph2 dep_ph1_is appos_prob_ph1 nn_prob_log2 prepc_as_ned_positive prep_de_is_ned num_phrases_two dep_Church_is prep_between_Church_phrases dep_Church_1990 conj_and_Church_Hanks nn_Church_PMI
W06-1101	J90-1003	o	Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist J.R. Firth You shall know a word by the company it keeps -LRB- Firth 1957 p. 11 -RRB- Context similarity has been used as a means of extracting collocations from corpora e.g. by Church & Hanks -LRB- 1990 -RRB- and by Dunning -LRB- 1993 -RRB- of identifying word senses e.g. by Yarowski -LRB- 1995 -RRB- and by Schutze -LRB- 1998 -RRB- of clustering verb classes e.g. by Schulte im Walde -LRB- 2003 -RRB- and of inducing selectional restrictions of verbs e.g. by Resnik -LRB- 1993 -RRB- by Abe & Li -LRB- 1996 -RRB- by Rooth et al.	nn_al._et nn_al._Rooth appos_Abe_1996 conj_and_Abe_Li prep_by_by_al. pobj_by_Li pobj_by_Abe appos_Resnik_1993 dep_e.g._by prep_by_e.g._Resnik dep_,_e.g. prep_of_restrictions_verbs amod_restrictions_selectional dobj_inducing_restrictions pcomp_of_inducing conj_and_Walde_of appos_Walde_2003 nn_Walde_im nn_Walde_Schulte prep_by_e.g._of prep_by_e.g._Walde dep_verb_e.g. nsubj_verb_classes prep_of_verb_clustering appos_Schutze_1998 conj_and_Yarowski_Schutze appos_Yarowski_1995 prep_by_e.g._Schutze prep_by_e.g._Yarowski nn_senses_word dobj_identifying_senses parataxis_of_verb dep_of_e.g. pcomp_of_identifying appos_Dunning_1993 pobj_by_Dunning dep_Church_1990 conj_and_Church_Hanks conj_and_by_by pobj_by_Hanks pobj_by_Church dep_e.g._of dep_e.g._by dep_e.g._by prep_from_extracting_corpora dobj_extracting_collocations prepc_of_means_extracting det_means_a prep_used_e.g. prep_as_used_means auxpass_used_been aux_used_has nsubjpass_used_similarity nn_similarity_Context num_p._11 dep_Firth_p. dep_Firth_1957 nsubj_keeps_it rcmod_company_keeps det_company_the prep_by_word_company det_word_a dobj_know_word aux_know_shall nsubj_know_You nn_Firth_J.R. nn_Firth_linguist num_Firth_3 amod_Firth_British det_Firth_the prep_of_dictum_Firth amod_dictum_famous det_dictum_the prep_in_best_dictum parataxis_summarized_know dobj_summarized_best nn_meaning_word prep_to_approach_meaning nn_approach_empiricist det_approach_the parataxis_follow_used dep_follow_Firth dep_follow_summarized dobj_follow_approach nsubj_follow_studies amod_studies_Such
W06-1605	J90-1003	p	Usually in 1 In our experiments we set negative PMI values to 0 because Church and Hanks -LRB- 1990 -RRB- in their seminal paper on word association ratio show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus	amod_corpus_large det_corpus_an advmod_large_extremely prep_from_made_corpus auxpass_made_are nsubjpass_made_counts mark_made_unless nn_counts_co-occurrence advcl_accurate_made cop_accurate_be aux_accurate_to xcomp_expected_accurate neg_expected_not auxpass_expected_are nsubjpass_expected_values mark_expected_that nn_values_PMI amod_values_negative ccomp_show_expected prep_on_show_ratio prep_in_show_paper nsubj_show_Hanks nsubj_show_Church mark_show_because nn_ratio_association nn_ratio_word amod_paper_seminal poss_paper_their appos_Hanks_1990 conj_and_Church_Hanks nn_values_PMI amod_values_negative advcl_set_show prep_to_set_0 dobj_set_values nsubj_set_we advcl_set_in poss_experiments_our prep_in_in_experiments pobj_in_1 advmod_in_Usually
W06-3307	J90-1003	o	To compute the degree of interaction between two proteins D4 BD and D4 BE we use the information-theoretic measure of pointwise mutual information -LRB- Church and Hanks 1990 Manning and Schutze 1999 -RRB- which is computed based on the following quantities 1	amod_quantities_following det_quantities_the prep_based_on_computed_quantities auxpass_computed_is nsubjpass_computed_which dep_Manning_1999 conj_and_Manning_Schutze dep_Church_Schutze dep_Church_Manning conj_and_Church_1990 conj_and_Church_Hanks appos_information_1990 appos_information_Hanks appos_information_Church amod_information_mutual amod_information_pointwise dep_measure_1 rcmod_measure_computed prep_of_measure_information amod_measure_information-theoretic det_measure_the dobj_use_measure nsubj_use_we ccomp_use_BE csubj_BE_compute conj_and_BD_D4 nn_BD_D4 nn_BD_proteins num_BD_two prep_between_interaction_D4 prep_between_interaction_BD prep_of_degree_interaction det_degree_the dobj_compute_degree aux_compute_To
W07-2201	J90-1003	o	Pointwise mutual information -LRB- Fano 1961 -RRB- was used to measure strength of selection restrictions for instance by Church and Hanks -LRB- 1990 -RRB-	appos_Hanks_1990 conj_and_Church_Hanks prep_for_restrictions_instance nn_restrictions_selection prep_of_strength_restrictions prep_by_measure_Hanks prep_by_measure_Church dobj_measure_strength aux_measure_to xcomp_used_measure auxpass_used_was nsubjpass_used_information amod_Fano_1961 dep_information_Fano amod_information_mutual amod_information_Pointwise
W07-2201	J90-1003	o	andw2 iscomputedusinganassociationscorebased on pointwise mutual information asdefinedbyFano -LRB- 1961 -RRB- and used for a similar purpose in Church and Hanks -LRB- 1990 -RRB- as well as in many other studies in corpus linguistics	nn_linguistics_corpus prep_in_studies_linguistics amod_studies_other amod_studies_many pobj_in_studies appos_Hanks_1990 conj_and_Church_Hanks prep_in_purpose_Hanks prep_in_purpose_Church amod_purpose_similar det_purpose_a prep_for_used_purpose appos_asdefinedbyFano_1961 conj_and_information_used conj_and_information_asdefinedbyFano amod_information_mutual amod_information_pointwise prep_on_iscomputedusinganassociationscorebased_used prep_on_iscomputedusinganassociationscorebased_asdefinedbyFano prep_on_iscomputedusinganassociationscorebased_information conj_and_andw2_in vmod_andw2_iscomputedusinganassociationscorebased
W08-1901	J90-1003	o	Indeed as Sinopalnikova and Pavel -LRB- 2004 -RRB- note Deese -LRB- 1965 -RRB- was the first to conduct linguistic analyses of word association norms such as measurements of semantic similarity based on his convictions that similar words evoke similar word association responsesan approach that is somewhat reminiscent of Church and Hanks -LRB- 1990 -RRB- notion of mutual information	amod_information_mutual prep_of_notion_information nn_notion_Hanks nn_notion_Church appos_Hanks_1990 conj_and_Church_Hanks prep_of_reminiscent_notion advmod_reminiscent_somewhat cop_reminiscent_is nsubj_reminiscent_that rcmod_approach_reminiscent nn_approach_responsesan nn_approach_association nn_approach_word amod_approach_similar dobj_evoke_approach nsubj_evoke_words nsubj_evoke_that amod_words_similar rcmod_convictions_evoke poss_convictions_his pobj_similarity_convictions prepc_based_on_similarity_on amod_similarity_semantic prep_of_measurements_similarity prep_such_as_norms_measurements nn_norms_association nn_norms_word prep_of_analyses_norms amod_analyses_linguistic dobj_conduct_analyses aux_conduct_to xcomp_first_conduct det_first_the cop_first_was nsubj_first_Deese prep_as_first_note prep_as_first_Sinopalnikova advmod_first_Indeed appos_Deese_1965 nn_note_Pavel appos_Pavel_2004 conj_and_Sinopalnikova_note
W08-1914	J90-1003	o	Following Church & Hanks -LRB- 1990 -RRB- Rapp -LRB- 2004 -RRB- and Wettler et al.	nn_al._et nn_al._Wettler appos_Rapp_2004 conj_and_Church_al. conj_and_Church_Rapp appos_Church_1990 conj_and_Church_Hanks pobj_Following_al. pobj_Following_Rapp pobj_Following_Hanks pobj_Following_Church ccomp_``_Following
W08-2005	J90-1003	o	Mutual Informatio n Church and Hanks -LRB- 1990 -RRB- discussed the use of the mutual information statistics as a way to identify a variety of interesting linguistic phenomena ranging from semanti c relations of the doctor/nurse type -LRB- content word/content word -RRB- to lexico-syntactic co-occurrence preferences between verbs and prepositions -LRB- content word/function word -RRB-	nn_word_word/function nn_word_content appos_prepositions_word conj_and_verbs_prepositions prep_between_preferences_prepositions prep_between_preferences_verbs nn_preferences_co-occurrence amod_preferences_lexico-syntactic amod_word_word/content nn_word_content appos_type_word nn_type_doctor/nurse det_type_the prep_of_relations_type nn_relations_c nn_relations_semanti prep_to_ranging_preferences prep_from_ranging_relations vmod_phenomena_ranging amod_phenomena_linguistic amod_phenomena_interesting prep_of_variety_phenomena det_variety_a dobj_identify_variety aux_identify_to vmod_way_identify det_way_a nn_statistics_information amod_statistics_mutual det_statistics_the prep_of_use_statistics det_use_the prep_as_discussed_way dobj_discussed_use nsubj_discussed_Hanks nsubj_discussed_Church appos_Hanks_1990 conj_and_Church_Hanks nn_Church_n nn_Church_Informatio amod_Church_Mutual
W09-0211	J90-1003	o	The tensor has been adapted with a straightforward extension of pointwise mutual information -LRB- Church and Hanks 1990 -RRB- for three-way cooccurrences following equation 4	num_equation_4 amod_cooccurrences_three-way appos_Church_1990 conj_and_Church_Hanks dep_information_Hanks dep_information_Church amod_information_mutual amod_information_pointwise prep_for_extension_cooccurrences prep_of_extension_information amod_extension_straightforward det_extension_a prep_following_adapted_equation prep_with_adapted_extension auxpass_adapted_been aux_adapted_has nsubjpass_adapted_tensor det_tensor_The
W09-0304	J90-1003	o	The first adaptation includes theswap-operation -LRB- WagnerandLowrance ,1975 -RRB- whilethesecondadaptationincludesphoneticsegment distances which are generated by applying an iterative pointwise mutual information -LRB- PMI -RRB- procedure -LRB- Churchand Hanks 1990 -RRB-	amod_Hanks_1990 nn_Hanks_Churchand nn_procedure_information appos_information_PMI amod_information_mutual nn_information_pointwise amod_information_iterative det_information_an dobj_applying_procedure dep_generated_Hanks agent_generated_applying auxpass_generated_are nsubjpass_generated_which nn_distances_whilethesecondadaptationincludesphoneticsegment num_WagnerandLowrance_,1975 rcmod_theswap-operation_generated appos_theswap-operation_distances appos_theswap-operation_WagnerandLowrance dobj_includes_theswap-operation nsubj_includes_adaptation amod_adaptation_first det_adaptation_The
W09-0304	J90-1003	o	We used pointwise mutual information -LRB- PMI Church and Hanks 1990 -RRB- to obtain these distances	det_distances_these dobj_obtain_distances aux_obtain_to dep_Church_1990 conj_and_Church_Hanks dep_PMI_Hanks dep_PMI_Church appos_information_PMI amod_information_mutual amod_information_pointwise vmod_used_obtain dobj_used_information nsubj_used_We
W91-0211	J90-1003	o	A broad view of the possible scope of lexical semantics would thus be one which tries to chart out the systematic generalizable aspects of word meanings and of the relations between words drawing on readily accessible sources of lexical knowledge such as machine readable dictionaries encyclopedias and representative corpora coupled with the kind of analytic apparatus that is needed to fruitfully explore such sources for instance custom-built parsers to cope with dictionary definitions -LRB- Vossen 1990 -RRB- statistical programs to deal with the distributional properties of lexical items in large corpora -LRB- Church & Hanks 1990 -RRB- etc. At the same time this kind of massive data-acquisition should be made sensitive to the borders between perceptual experience lexical knowledge and expert knowledge	nn_knowledge_expert conj_and_knowledge_knowledge amod_knowledge_lexical amod_experience_perceptual prep_between_borders_experience det_borders_the prep_to_sensitive_borders dobj_made_knowledge dobj_made_knowledge xcomp_made_sensitive auxpass_made_be aux_made_should nsubjpass_made_one amod_data-acquisition_massive prep_of_kind_data-acquisition det_kind_this prep_at_kind_time nsubj_kind_of nsubj_kind_aspects amod_kind_systematic det_kind_the amod_time_same det_time_the nn_etc._corpora num_Church_1990 conj_and_Church_Hanks appos_corpora_Hanks appos_corpora_Church amod_corpora_large prep_in_items_etc. amod_items_lexical prep_of_properties_items amod_properties_distributional det_properties_the prep_with_deal_properties aux_deal_to vmod_programs_deal amod_programs_statistical dep_Vossen_1990 dep_definitions_Vossen nn_definitions_dictionary prep_with_cope_definitions aux_cope_to vmod_parsers_cope amod_parsers_custom-built nn_parsers_instance amod_sources_such dobj_explore_sources advmod_explore_fruitfully aux_explore_to xcomp_needed_explore auxpass_needed_is nsubjpass_needed_that amod_apparatus_analytic rcmod_kind_needed prep_of_kind_apparatus det_kind_the prep_with_coupled_kind amod_corpora_representative conj_and_dictionaries_corpora conj_and_dictionaries_encyclopedias amod_dictionaries_readable nn_dictionaries_machine amod_knowledge_lexical appos_sources_programs prep_for_sources_parsers vmod_sources_coupled prep_such_as_sources_corpora prep_such_as_sources_encyclopedias prep_such_as_sources_dictionaries prep_of_sources_knowledge amod_sources_accessible advmod_accessible_readily prep_on_drawing_sources prep_between_relations_words det_relations_the pobj_of_relations nn_meanings_word vmod_aspects_drawing conj_and_aspects_of prep_of_aspects_meanings amod_aspects_generalizable dobj_chart_kind prt_chart_out aux_chart_to xcomp_tries_chart nsubj_tries_which rcmod_one_tries cop_one_be advmod_one_thus aux_one_would nsubj_one_view amod_semantics_lexical prep_of_scope_semantics amod_scope_possible det_scope_the prep_of_view_scope amod_view_broad det_view_A
W93-0111	J90-1003	o	In our approach we take into account both the relative positions of the nearby context words as well as the mutual information -LRB- Church & Hanks 1990 -RRB- associated with the occurrence of a particular context word	nn_word_context amod_word_particular det_word_a prep_of_occurrence_word det_occurrence_the prep_with_associated_occurrence dep_Church_1990 conj_and_Church_Hanks vmod_information_associated dep_information_Hanks dep_information_Church amod_information_mutual det_information_the conj_and_words_information nn_words_context amod_words_nearby det_words_the prep_of_positions_information prep_of_positions_words amod_positions_relative det_positions_the preconj_positions_both dobj_take_positions prep_into_take_account nsubj_take_we prep_in_take_approach poss_approach_our
W93-0310	J90-1003	o	-LRB- 1989 -RRB- Wettler & Rapp -LRB- 1989 -RRB- and Church & Hanks -LRB- 1990 -RRB- describe algorithms which do this	dobj_do_this nsubj_do_which rcmod_algorithms_do dobj_describe_algorithms nsubj_describe_Church nsubj_describe_Rapp nsubj_describe_Wettler dep_describe_1989 dep_Church_1990 conj_and_Church_Hanks conj_and_Wettler_Hanks conj_and_Wettler_Church dep_Wettler_1989 conj_and_Wettler_Rapp
W95-0111	J90-1003	o	Other representative collocation research can be found in Church and Hanks -LRB- 1990 -RRB- and Smadja -LRB- 1993 -RRB-	appos_Smadja_1993 appos_Hanks_1990 conj_and_Church_Smadja conj_and_Church_Hanks prep_in_found_Smadja prep_in_found_Hanks prep_in_found_Church auxpass_found_be aux_found_can nsubjpass_found_research nn_research_collocation amod_research_representative amod_research_Other
W95-0111	J90-1003	p	Unlike Choueka -LRB- 1988 -RRB- Church and Hanks -LRB- 1990 -RRB- identify as collocations both interrupted and uninterrupted sequences of words	prep_of_sequences_words amod_sequences_uninterrupted amod_sequences_interrupted nn_sequences_collocations conj_and_interrupted_uninterrupted preconj_interrupted_both prep_as_identify_sequences nsubj_identify_Choueka mark_identify_Unlike appos_Hanks_1990 conj_and_Church_Hanks appos_Choueka_Hanks appos_Choueka_Church appos_Choueka_1988 advcl_``_identify
W95-0111	J90-1003	n	Unlike Church and Hanks -LRB- 1990 -RRB- Smadja -LRB- 1993 -RRB- goes beyond the two-word limitation and deals with collocations of arbitrary length	amod_length_arbitrary prep_of_collocations_length conj_and_limitation_deals amod_limitation_two-word det_limitation_the prep_with_goes_collocations prep_beyond_goes_deals prep_beyond_goes_limitation nsubj_goes_Smadja prep_unlike_goes_Hanks prep_unlike_goes_Church appos_Smadja_1993 appos_Hanks_1990 conj_and_Church_Hanks
W95-0111	J90-1003	o	Following Church and Hanks -LRB- 1990 -RRB- they use mutual information to select significant two-word patterns but at the same time a lexical inductive process is incorporated which as they claim can improve the collection of domain-specific terms	amod_terms_domain-specific prep_of_collection_terms det_collection_the dobj_improve_collection aux_improve_can parataxis_improve_claim nsubj_improve_which nsubj_claim_they mark_claim_as ccomp_incorporated_improve auxpass_incorporated_is nsubjpass_incorporated_process prep_at_incorporated_time amod_process_inductive amod_process_lexical det_process_a amod_time_same det_time_the amod_patterns_two-word amod_patterns_significant dobj_select_patterns aux_select_to amod_information_mutual parataxis_use_incorporated cc_use_but vmod_use_select dobj_use_information nsubj_use_they prep_following_use_Hanks prep_following_use_Church appos_Hanks_1990 conj_and_Church_Hanks
W96-0103	J90-1003	o	We used * TH * = 3 following a very rough rule of thumb used for word-based mutual information in -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks prep_in_information_Hanks prep_in_information_Church amod_information_mutual amod_information_word-based prep_for_used_information prep_of_rule_thumb amod_rule_rough det_rule_a advmod_rough_very dobj_=_3 amod_*_= nn_*_TH dep_*_* dep_used_used prep_following_used_rule dobj_used_* nsubj_used_We
W96-0103	J90-1003	o	Most of the previously proposed methods to extract compounds or to measure word association using mutual information -LRB- MI -RRB- either ignore or penalize items with low co-occurrence counts -LRB- Church and Hanks 1990 Su Wu and Chang 1994 -RRB- because MI becomes unstable when the co-occurrence counts are very small	advmod_small_very cop_small_are nsubj_small_counts advmod_small_when nn_counts_co-occurrence det_counts_the advcl_becomes_small acomp_becomes_unstable nsubj_becomes_MI mark_becomes_because num_Chang_1994 num_Hanks_1990 conj_and_Church_Chang conj_and_Church_Wu conj_and_Church_Su conj_and_Church_Hanks dep_counts_Chang dep_counts_Wu dep_counts_Su dep_counts_Hanks dep_counts_Church nn_counts_co-occurrence amod_counts_low prep_with_items_counts dobj_ignore_items conj_or_ignore_penalize preconj_ignore_either appos_information_MI amod_information_mutual dep_using_penalize dep_using_ignore dobj_using_information nn_association_word xcomp_measure_using dobj_measure_association aux_measure_to nsubj_measure_Most advcl_extract_becomes conj_or_extract_measure dobj_extract_compounds aux_extract_to nsubj_extract_Most amod_methods_proposed det_methods_the advmod_proposed_previously prep_of_Most_methods
W96-0306	J90-1003	o	Previous research in automatic acquisition focuses primarily on the use of statistical techniques such as bilingual alignment -LRB- Church and Hanks 1990 Klavans and Tzoukermann 1996 Wu and Xia 1995 -RRB- or extraction of syntactic constructions from online dictionaries and corpora -LRB- Brent 1993 Dorr Garman and Weinberg 1995 -RRB-	amod_Weinberg_1995 nn_Garman_Dorr conj_and_Brent_Weinberg conj_and_Brent_Garman dep_Brent_1993 dep_corpora_Weinberg dep_corpora_Garman dep_corpora_Brent conj_and_dictionaries_corpora amod_dictionaries_online amod_constructions_syntactic prep_from_extraction_corpora prep_from_extraction_dictionaries prep_of_extraction_constructions dep_Wu_1995 conj_and_Wu_Xia conj_or_Klavans_extraction conj_and_Klavans_Xia conj_and_Klavans_Wu conj_and_Klavans_1996 conj_and_Klavans_Tzoukermann dep_Church_extraction dep_Church_Wu dep_Church_1996 dep_Church_Tzoukermann dep_Church_Klavans conj_and_Church_1990 conj_and_Church_Hanks dep_alignment_1990 dep_alignment_Hanks dep_alignment_Church amod_alignment_bilingual prep_such_as_techniques_alignment amod_techniques_statistical prep_of_use_techniques det_use_the prep_on_focuses_use advmod_focuses_primarily nsubj_focuses_research amod_acquisition_automatic prep_in_research_acquisition amod_research_Previous ccomp_``_focuses
W97-0205	J90-1003	o	The classifier uses mutual information -LRB- MI -RRB- scores rather than the raw frequences of the occurring patterns -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_patterns_Hanks dep_patterns_Church amod_patterns_occurring det_patterns_the prep_of_frequences_patterns amod_frequences_raw det_frequences_the nn_scores_information appos_information_MI amod_information_mutual conj_negcc_uses_frequences dobj_uses_scores nsubj_uses_classifier det_classifier_The
W97-0709	J90-1003	o	s e the window to consider when extracting words related to word w should span from postttuon w-5 to w +5 Maarek also defines the resolwng power of a parr m a document d as P = ~ ` Pd log Pc where Pd is the observed probabshty of appearance of the pan m document d Pc the observed probabdny of the pmr recorpus and log Pc the quantity of mformauon assocmted to the pmr It Is easdy seen that p wall be h | gher the higher the frequency of the pmr m the document and the lower sts frequency m the corpus which agrees wlth the sdea presented at the begmnmg of this sectton Church and Hanks -LRB- 1990 -RRB- propose the apphcatlon of the concept of mutual mformatton e -LRB- x y -RRB- ~ -LRB- x.y -RRB- = hog2 ecx -RRB- e -LRB- y -RRB- 51 to the retrieval ro a corpus of pairs of lextcally related words They alsoconslder a word span of e5 words and observe that roterestrog pmr s generally present a mutual mformatxon above 3 Salton and.Allan -LRB- 1995 -RRB- foc ~ as on paragraph level Each paragraph Is represented by a weighed vector where each element is a term -LRB- typically	advmod_term_typically det_term_a cop_term_is nsubj_term_element advmod_term_where det_element_each rcmod_vector_term amod_vector_weighed det_vector_a agent_represented_vector auxpass_represented_Is nsubjpass_represented_paragraph det_paragraph_Each nn_level_paragraph pobj_on_level pcomp_as_on nn_~_foc nn_~_and.Allan appos_and.Allan_1995 nn_and.Allan_Salton num_and.Allan_3 prep_above_mformatxon_~ amod_mformatxon_mutual det_mformatxon_a parataxis_present_represented prep_present_as dobj_present_mformatxon advmod_present_generally nsubj_present_pmr conj_pmr_s dep_pmr_roterestrog mark_roterestrog_that rcmod_words_present conj_and_words_observe nn_words_e5 prep_of_span_observe prep_of_span_words nn_span_word det_span_a dobj_alsoconslder_span nsubj_alsoconslder_They rcmod_words_alsoconslder amod_words_related advmod_related_lextcally prep_of_pairs_words det_corpus_a nn_corpus_ro det_retrieval_the prep_of_51_pairs appos_51_corpus prep_to_51_retrieval dep_51_e appos_e_y nn_ecx_hog2 dep_=_ecx amod_~_= appos_~_x.y dep_~_e appos_x_y dep_e_x dep_mformatton_~ amod_mformatton_mutual prep_of_concept_mformatton det_concept_the dep_apphcatlon_51 prep_of_apphcatlon_concept det_apphcatlon_the dobj_propose_apphcatlon nsubj_propose_sdea appos_Hanks_1990 conj_and_Church_Hanks nn_Church_sectton det_Church_this prep_of_begmnmg_Hanks prep_of_begmnmg_Church det_begmnmg_the prep_at_presented_begmnmg vmod_sdea_presented det_sdea_the rcmod_wlth_propose dobj_agrees_wlth nsubj_agrees_which det_corpus_the rcmod_m_agrees dep_m_corpus nn_m_frequency nn_m_sts amod_m_lower det_m_the conj_and_document_m det_document_the nn_m_pmr det_m_the dep_frequency_m dep_frequency_document prep_of_frequency_m det_frequency_the dep_higher_frequency det_higher_the dep_gher_higher nn_gher_| nn_gher_h cop_gher_be nsubj_gher_wall mark_gher_that nn_wall_p ccomp_seen_gher nsubjpass_seen_easdy auxpass_seen_Is nsubjpass_seen_It rcmod_pmr_seen det_pmr_the prep_to_assocmted_pmr vmod_mformauon_assocmted prep_of_quantity_mformauon det_quantity_the dep_Pc_quantity nn_Pc_log nn_recorpus_pmr det_recorpus_the prep_of_probabdny_recorpus amod_probabdny_observed det_probabdny_the nn_probabdny_Pc conj_and_d_Pc conj_and_d_probabdny nn_d_document nn_d_m nn_d_pan det_pan_the prep_of_appearance_Pc prep_of_appearance_probabdny prep_of_appearance_d prep_of_probabshty_appearance amod_probabshty_observed det_probabshty_the cop_probabshty_is nsubj_probabshty_Pd advmod_probabshty_where rcmod_Pc_probabshty nn_Pc_log nn_Pc_Pd num_Pc_~ dobj_=_Pc nsubj_=_P mark_=_as rcmod_d_= nn_d_document det_d_a dep_m_d nn_m_parr det_m_a prep_of_power_m amod_power_resolwng det_power_the dobj_defines_power advmod_defines_also csubj_defines_w num_Maarek_+5 dobj_w_Maarek aux_w_to amod_w-5_postttuon ccomp_span_defines prep_from_span_w-5 aux_span_should nn_w_word prep_to_related_w vmod_words_related dep_extracting_span dobj_extracting_words advmod_extracting_when advcl_consider_extracting aux_consider_to vmod_window_consider det_window_the dep_e_window amod_s_e
W97-0711	J90-1003	o	of the works of -LRB- Kuplec Pedersen and Chen 1995 -RRB- and -LRB- Brandow Mltze and Ran 1995 -RRB- and advances summarmatlon technology by applynag corpus-based statistical NLP teehmques robust information extraction and readily avaalable on-hne resources Our prehxmnary experiments with combining different summarization features have been reported and our current effort to learn to combine these features to produce the best summaries has been described The features derived by these robust NLP techmques were also utihzed m presentmg multiple summary.vtews to the user m a novel way References Advanced Research Projects Agency 1995 Proceed rigs of S zth Message Understanding Conference -LRB- MUC-6 -RRB- Morgan Kanfmann Pubhshers Brandow Ron Karl Mltze and Lisa Ran 1995 Automatic condensation of electromc pubhcatlous by sentence selection Information Processing and Management 31 forthcoming Bull Eric 1993 A Comps-based Approach to Language Learning Ph D thesm Umverslty of Pennsylvania Church Kenneth and Patrick Hanks 1990 Word Aesoclatlon Norrns Mutual Information and Lexicography Computational Lmgmstscs 16 -LRB- 1 -RRB- Church Kenneth W 1995 One term or two 9 In Proceedings of the 17th Annual International SIGIR Conference on Research and Development In Informatzon Retrzeral pages 310-318 Edmundson H P 1969 New methods m automatic abstracting Journal of the ACM 16 -LRB- 2 -RRB- 264-228 Fum Dando Glovanm Gmda and Carlo Tasso 1985 Evalutatmg Importance A step towards text surnmarlzatlon In I3CAI85 pages 840-844IJCAi AAAI Hahn Udo 1990 Topic parsing Accounting for text macro structures m full-text analysm In format on Processing and Management 26 -LRB- 1 -RRB- 135170 Harman Donna 1991 How effective is suttixang ~ Journal of the Amerlcan Sot cry for Informatwn Sc ence 42 -LRB- 1 -RRB- 7-15 Harman Donna 1996 Overview of the fifth text retrieval conference -LRB- tree-5 -RRB- In TREC-5 Conference Proceedings Jmg Y and B Croft 1994 An Assoc atwn Thesaurns for Informatzon Retrseval Umass Techmcal Report 94-I7 Center for Intelligent Information Retrieval University of Massachusetts Johnson F C C D Prate W J Black and A P Neal 1993	num_Neal_1993 dep_P_Neal dep_A_P nn_Black_J nn_Black_W nn_Prate_D nn_Prate_C nn_C_F conj_and_Johnson_A appos_Johnson_Black appos_Johnson_Prate appos_Johnson_C dep_Johnson_University prep_of_University_Massachusetts appos_Retrieval_A appos_Retrieval_Johnson nn_Retrieval_Information nn_Retrieval_Intelligent prep_for_Center_Retrieval nn_Center_94-I7 nn_Center_Report nn_Center_Techmcal nn_Center_Umass nn_Center_Retrseval nn_Center_Informatzon prep_for_Thesaurns_Center nn_Thesaurns_atwn det_Assoc_An num_Assoc_1994 dep_Croft_Assoc dep_Y_Croft conj_and_Y_B dep_Jmg_B dep_Jmg_Y nn_Jmg_Proceedings nn_Jmg_Conference nn_Jmg_TREC-5 appos_conference_tree-5 nn_conference_retrieval nn_conference_text amod_conference_fifth det_conference_the prep_in_Overview_Jmg prep_of_Overview_conference num_Overview_1996 nn_Overview_Donna dep_Harman_Thesaurns appos_Harman_Overview num_Harman_7-15 num_Harman_1 dep_42_Harman appos_ence_42 amod_Sc_Informatwn dep_cry_ence prep_for_cry_Sc nn_Sot_Amerlcan det_Sot_the prep_of_Journal_Sot nn_Journal_~ nn_Journal_suttixang cop_Journal_is nsubj_Journal_Harman dep_Journal_26 advmod_effective_How amod_Donna_effective num_Donna_1991 appos_Harman_Donna num_Harman_135170 num_Harman_1 conj_and_Processing_Management dep_on_cry dep_on_Journal pobj_on_Management pobj_on_Processing amod_analysm_full-text nn_analysm_m nn_analysm_structures nn_analysm_macro nn_analysm_text prep_for_Accounting_analysm amod_Accounting_parsing dep_Topic_Accounting num_Topic_1990 nn_Topic_Udo nn_Hahn_AAAI nn_840-844IJCAi_pages nn_surnmarlzatlon_text prep_towards_step_surnmarlzatlon nn_step_A nn_step_Importance nn_step_Evalutatmg num_step_1985 nn_step_Tasso nn_step_Carlo nn_Gmda_Glovanm dep_Fum_on prep_in_Fum_format appos_Fum_Topic appos_Fum_Hahn appos_Fum_840-844IJCAi prep_in_Fum_I3CAI85 conj_and_Fum_step appos_Fum_Gmda appos_Fum_Dando num_Fum_264-228 num_Fum_2 appos_16_step appos_16_Fum det_ACM_the dep_Journal_16 prep_of_Journal_ACM amod_Journal_abstracting amod_Journal_automatic nn_Journal_m dep_methods_Journal nn_methods_New num_methods_1969 dep_P_methods nn_P_H num_Edmundson_310-318 nn_Edmundson_pages nn_Retrzeral_Informatzon conj_and_Research_Development prep_in_Conference_Retrzeral prep_on_Conference_Development prep_on_Conference_Research nn_Conference_SIGIR nn_Conference_International amod_Conference_Annual amod_Conference_17th det_Conference_the prep_of_Proceedings_Conference num_9_two prep_in_term_Proceedings conj_or_term_9 num_term_One number_One_1995 dep_W_9 dep_W_term nn_W_Kenneth appos_Church_P appos_Church_Edmundson appos_Church_W num_Church_1 dep_16_Church appos_Lmgmstscs_16 nn_Lmgmstscs_Computational nn_Lmgmstscs_Lexicography nn_Information_Mutual conj_and_Norrns_Lmgmstscs conj_and_Norrns_Information nn_Norrns_Aesoclatlon nn_Norrns_Word num_Norrns_1990 dep_Hanks_Lmgmstscs dep_Hanks_Information dep_Hanks_Norrns nn_Hanks_Patrick conj_and_Kenneth_Hanks nn_Church_Pennsylvania prep_of_Umverslty_Church appos_thesm_Hanks appos_thesm_Kenneth appos_thesm_Umverslty nn_thesm_D nn_thesm_Ph nn_thesm_Learning nn_thesm_Language prep_to_Approach_thesm amod_Approach_Comps-based det_Approach_A num_Approach_1993 nn_Approach_Eric amod_Bull_forthcoming conj_and_Processing_Approach conj_and_Processing_Bull conj_and_Processing_31 conj_and_Processing_Management nn_Processing_Information nn_Processing_selection nn_Processing_sentence amod_pubhcatlous_electromc prep_of_condensation_pubhcatlous nn_condensation_Automatic num_condensation_1995 dep_Ran_condensation nn_Ran_Lisa nn_Mltze_Karl conj_and_Ron_Ran conj_and_Ron_Mltze appos_Brandow_Ran appos_Brandow_Mltze appos_Brandow_Ron nn_Brandow_Pubhshers nn_Brandow_Kanfmann nn_Brandow_Morgan dep_Brandow_MUC-6 nn_Brandow_Conference agent_Understanding_Approach agent_Understanding_Bull agent_Understanding_31 agent_Understanding_Management agent_Understanding_Processing dobj_Understanding_Brandow vmod_Message_Understanding dep_zth_Message prep_of_rigs_S dep_Proceed_rigs num_Proceed_1995 nn_Proceed_Agency nn_Proceed_Projects nn_Proceed_Research nn_Proceed_Advanced nn_Proceed_References dep_way_zth dep_way_Proceed amod_way_novel det_way_a dep_m_way nn_m_user det_m_the prep_to_summary.vtews_m amod_summary.vtews_multiple nn_summary.vtews_presentmg nn_summary.vtews_m amod_summary.vtews_utihzed advmod_summary.vtews_also cop_summary.vtews_were nsubj_summary.vtews_features nn_techmques_NLP amod_techmques_robust det_techmques_these agent_derived_techmques vmod_features_derived det_features_The ccomp_described_summary.vtews auxpass_described_been aux_described_has nsubjpass_described_effort amod_summaries_best det_summaries_the dobj_produce_summaries aux_produce_to det_features_these xcomp_combine_produce dobj_combine_features aux_combine_to xcomp_learn_combine aux_learn_to vmod_effort_learn amod_effort_current poss_effort_our conj_and_reported_described auxpass_reported_been aux_reported_have nsubjpass_reported_technology nn_features_summarization amod_features_different dobj_combining_features prepc_with_experiments_combining amod_experiments_prehxmnary poss_experiments_Our dep_resources_experiments amod_resources_on-hne amod_resources_avaalable advmod_avaalable_readily nn_extraction_information amod_extraction_robust conj_and_teehmques_resources conj_and_teehmques_extraction nn_teehmques_NLP amod_teehmques_statistical amod_teehmques_corpus-based nn_teehmques_applynag prep_by_technology_resources prep_by_technology_extraction prep_by_technology_teehmques dep_summarmatlon_described dep_summarmatlon_reported conj_and_summarmatlon_advances prep_of_summarmatlon_works dep_Ran_1995 appos_Brandow_Mltze dep_Chen_1995 conj_and_Kuplec_Ran conj_and_Kuplec_Brandow conj_and_Kuplec_Chen conj_and_Kuplec_Pedersen prep_of_works_Ran prep_of_works_Brandow prep_of_works_Chen prep_of_works_Pedersen prep_of_works_Kuplec det_works_the
W97-0711	J90-1003	p	robust mforrmatlon extractlon and readlly-avmlable on-hne NLP resources These techtuques and resources allow us to create a richer indexed source of Imgmstlc and domain knowledge than other frequency approaches Our approach attempts to apprommate text dlscourse structure through these multlple layers of mformatlon ohtinned from automated methods m contrast to labor-lntenslve discourse-based approaches Moreover our planned training methodology will also allow us to explmt thin productlve infrastructure m ways whlch model human performance whde avoidmg hand-crafting domain-dependent rules of the knowledge-based approaches Our ultlmate goal m to make our summarlzatlon system scalable and portable by learning summarization rules from easily extractable text features 2 System Description Our summarization system DlmSum consmts of the Summarization Server and the Summarlzatzon Chent The Server extracts features -LRB- the Feature Extractor -RRB- from a document using various robust NLP techmques described In Sectzon 2 1 and combines these features -LRB- the Feature Combiner -RRB- to basehne multiple combinations of features as described m Section 2 2 Our work m progress to automattcally tram the Feature Combiner based upon user and apphcatlon needs m presented in Section 2 2 2 The Java-based Chent which wdl be dmcnssed In Section 4 provides a graphical user interface -LRB- GUI -RRB- for the end user to cnstomlze the summamzatlon preferences and see multiple views of generated sumInarles 2.1 Extracting Stlmmarization Features In this section we describe how we apply robust NLP technology to extract summarization features Our goal IS to add more mtelhgence to frequencybased approaches to acqmre domain knowledge In a more automated fashion and to apprommate text structure by recogmzing sources of dmcourse cohesion and coherence 2.1.1 Going Beyond a Word Frequency-based summarization systems typically use a single word stnng as a umt for counting frequencies Whde such a method IS very robust it ignores the semantic content of words and their potential membership m multi-word phrases For example zt does not dmtmgumh between bill m Bdl Table 1 Collocations with chlps -LCB- potato tortdla corn chocolate b ~ gle -RCB- chips -LCB- computer pentmm Intel macroprocessor memory -RCB- chips -LCB- wood oak plastlc -RCB- cchlps bsrgmmng clups blue clups mr chips Clmton and bill in reform bill This may introduce noise m frequency counting as the same strmgs are treated umformly no matter how the context may have dmamblguated the sense or regardless of membership in multl-word phrases For DlrnSum we use term frequency based on tf * Idf -LRB- Salton and McGdl 1983 Brandow Mitze and Rau 1995 -RRB- to derive ssgnature words as one of the summarization features If single words were the sole basra of countmg for our summarization application nome would be introduced both m term frequency and reverse document frequency However recent advances in statmtlcal NLP and information extraction make it possible to utilize features which go beyond the single word level Our approach is to extract multi-word phrases automatlcally with high accuracy and use them as the basic unit in the summarization process including frequency calculation Ftrst just as word association methods have proven effective m lemcal analysis e g -LRB- Church and Hanks 1990 -RRB- we are exploring whether frequently occurring Collocatlonal reformation can improve on simple word-based approaches We have preprocessed about 800 MB of LA tlmes/Wastnngton Post newspaper articles nsmg a POS tagger -LRB- Bnll 1993 -RRB- and derived two-word noun collocations using mutual information The	amod_information_mutual dep_using_The dobj_using_information nn_collocations_noun amod_collocations_two-word amod_collocations_derived amod_Bnll_1993 conj_and_tagger_collocations dep_tagger_Bnll nn_tagger_POS det_tagger_a vmod_nsmg_using dobj_nsmg_collocations dobj_nsmg_tagger nn_articles_newspaper nn_articles_Post nn_articles_tlmes/Wastnngton nn_articles_LA prep_of_MB_articles num_MB_800 quantmod_800_about dobj_preprocessed_MB aux_preprocessed_have nsubj_preprocessed_We rcmod_approaches_preprocessed amod_approaches_word-based amod_approaches_simple dep_improve_nsmg prep_on_improve_approaches aux_improve_can nsubj_improve_reformation mark_improve_whether nn_reformation_Collocatlonal amod_reformation_occurring advmod_occurring_frequently ccomp_exploring_improve aux_exploring_are nsubj_exploring_we amod_Church_1990 conj_and_Church_Hanks dep_g_Hanks dep_g_Church dep_g_e amod_analysis_lemcal nn_analysis_m amod_analysis_effective dobj_proven_analysis aux_proven_have nsubj_proven_methods mark_proven_as advmod_proven_just nn_methods_association nn_methods_word nn_Ftrst_calculation nn_Ftrst_frequency nn_process_summarization det_process_the prep_including_unit_Ftrst prep_in_unit_process amod_unit_basic det_unit_the advcl_use_proven prep_as_use_unit dobj_use_them amod_accuracy_high prep_with_automatlcally_accuracy advmod_phrases_automatlcally amod_phrases_multi-word parataxis_extract_exploring conj_and_extract_g conj_and_extract_use dobj_extract_phrases aux_extract_to xcomp_is_g xcomp_is_use xcomp_is_extract nsubj_is_approach poss_approach_Our nn_level_word amod_level_single det_level_the ccomp_go_is prep_beyond_go_level nsubj_go_which rcmod_features_go dobj_utilize_features aux_utilize_to dep_utilize_possible nsubj_utilize_it xcomp_make_utilize nsubj_make_advances nn_extraction_information conj_and_NLP_extraction amod_NLP_statmtlcal prep_in_advances_extraction prep_in_advances_NLP amod_advances_recent nn_frequency_document dobj_reverse_frequency nsubj_reverse_nome nn_frequency_term nn_frequency_m det_frequency_both advmod_introduced_However conj_and_introduced_reverse dobj_introduced_frequency auxpass_introduced_be aux_introduced_would nsubjpass_introduced_nome advcl_introduced_basra nn_application_summarization poss_application_our prep_for_basra_application prep_of_basra_countmg amod_basra_sole det_basra_the cop_basra_were nsubj_basra_words mark_basra_If amod_words_single dep_features_make ccomp_features_reverse ccomp_features_introduced det_summarization_the prep_of_one_summarization nn_words_ssgnature dobj_derive_words aux_derive_to dep_Salton_1995 conj_and_Salton_Rau appos_Salton_Mitze appos_Salton_Brandow amod_Salton_1983 conj_and_Salton_McGdl vmod_Idf_derive appos_Idf_Rau appos_Idf_McGdl appos_Idf_Salton dep_Idf_* prep_as_tf_one dobj_tf_Idf prep_on_based_tf vmod_frequency_based nn_frequency_term dep_use_features dobj_use_frequency nsubj_use_we ccomp_use_introduce prep_for_phrases_DlrnSum amod_phrases_multl-word prep_in_membership_phrases prep_of_regardless_membership conj_or_sense_regardless det_sense_the dobj_dmamblguated_regardless dobj_dmamblguated_sense aux_dmamblguated_have aux_dmamblguated_may nsubj_dmamblguated_context advmod_dmamblguated_how det_context_the rcmod_matter_dmamblguated neg_matter_no advmod_matter_umformly dobj_treated_matter auxpass_treated_are nsubjpass_treated_strmgs mark_treated_as amod_strmgs_same det_strmgs_the nn_counting_frequency nn_counting_m nn_counting_noise advcl_introduce_treated dobj_introduce_counting aux_introduce_may nsubj_introduce_This nn_bill_reform dep_bill_chlps dep_bill_with nn_Clmton_chips nn_Clmton_mr nn_Clmton_clups amod_Clmton_blue nn_Clmton_clups nn_Clmton_bsrgmmng dobj_cchlps_Clmton nsubj_cchlps_chips nn_plastlc_oak nn_plastlc_wood appos_chips_plastlc nn_chips_memory nn_chips_macroprocessor nn_chips_Intel nn_chips_pentmm nn_chips_computer rcmod_chips_cchlps nn_chips_gle nn_chips_~ nn_chips_b nn_chips_chocolate nn_chips_corn nn_chips_tortdla nn_chips_potato cc_chlps_and dep_chlps_chips num_Collocations_1 nn_Collocations_Table nn_Collocations_Bdl nn_Collocations_m nn_Collocations_bill amod_bill_between dep_dmtmgumh_bill dobj_dmtmgumh_Collocations neg_dmtmgumh_not aux_dmtmgumh_does nsubj_dmtmgumh_zt prep_for_phrases_example amod_phrases_multi-word nn_phrases_m nn_phrases_membership amod_phrases_potential poss_phrases_their conj_and_content_phrases prep_of_content_words amod_content_semantic det_content_the ccomp_ignores_use prep_in_ignores_bill parataxis_ignores_dmtmgumh dobj_ignores_phrases dobj_ignores_content nsubj_ignores_it advcl_ignores_allow advmod_robust_very cop_robust_IS nsubj_robust_method dep_robust_Whde det_method_a predet_method_such amod_frequencies_robust dobj_counting_frequencies prepc_for_umt_counting det_umt_a nn_stnng_word amod_stnng_single det_stnng_a prep_as_use_umt dobj_use_stnng advmod_use_typically dep_use_Going nsubj_use_sources mark_use_by nn_systems_summarization amod_systems_Frequency-based nn_systems_Word det_systems_a prep_beyond_Going_systems num_coherence_2.1.1 conj_and_cohesion_coherence amod_cohesion_dmcourse prep_of_sources_coherence prep_of_sources_cohesion amod_sources_recogmzing nn_structure_text advcl_apprommate_use dobj_apprommate_structure aux_apprommate_to amod_fashion_automated det_fashion_a advmod_automated_more nn_knowledge_domain amod_knowledge_acqmre amod_approaches_frequencybased amod_mtelhgence_more conj_and_add_apprommate prep_in_add_fashion prep_to_add_knowledge prep_to_add_approaches dobj_add_mtelhgence aux_add_to xcomp_IS_apprommate xcomp_IS_add nsubj_IS_goal poss_goal_Our nn_features_summarization ccomp_extract_IS dobj_extract_features aux_extract_to nn_technology_NLP amod_technology_robust xcomp_apply_extract dobj_apply_technology nsubj_apply_we advmod_apply_how ccomp_describe_apply nsubj_describe_we nsubj_describe_approaches vmod_describe_ohtinned det_section_this nn_Features_Stlmmarization amod_Features_Extracting num_Features_2.1 nn_Features_sumInarles amod_Features_generated prep_of_views_Features amod_views_multiple prep_in_see_section dobj_see_views nn_preferences_summamzatlon det_preferences_the conj_and_cnstomlze_see dobj_cnstomlze_preferences aux_cnstomlze_to vmod_user_see vmod_user_cnstomlze nn_user_end det_user_the appos_interface_GUI nn_interface_user amod_interface_graphical det_interface_a prep_for_provides_user dobj_provides_interface prep_as_provides_progress nsubj_provides_the num_Section_4 prep_in_dmcnssed_Section auxpass_dmcnssed_be aux_dmcnssed_wdl nsubjpass_dmcnssed_which rcmod_Chent_dmcnssed amod_Chent_Java-based det_Chent_The num_Chent_2 nn_Chent_Section num_2_2 number_2_2 prep_in_presented_Chent vmod_m_presented dobj_needs_m nsubj_needs_apphcatlon nsubj_needs_user mark_needs_upon conj_and_user_apphcatlon advcl_based_needs nn_Combiner_Feature det_Combiner_the vmod_tram_based dobj_tram_Combiner advmod_tram_automattcally aux_tram_to vmod_progress_tram nn_progress_m nn_progress_work poss_progress_Our nn_progress_Section nn_progress_m amod_progress_described number_2_2 num_Section_2 prep_of_combinations_features amod_combinations_multiple dobj_basehne_combinations aux_basehne_to nsubj_basehne_combines nsubj_basehne_Summarlzatzon nn_Combiner_Feature det_Combiner_the appos_features_Combiner det_features_these number_1_2 num_Sectzon_1 prep_in_described_Sectzon vmod_techmques_described nn_techmques_NLP amod_techmques_robust amod_techmques_various dobj_using_techmques det_document_a nn_Extractor_Feature det_Extractor_the vmod_features_using prep_from_features_document appos_features_Extractor dep_extracts_features nn_extracts_Server nn_extracts_The dep_Chent_extracts dep_Summarlzatzon_features conj_and_Summarlzatzon_combines dep_Summarlzatzon_Chent vmod_the_basehne nn_Server_Summarization det_Server_the conj_and_consmts_provides prep_of_consmts_Server nn_consmts_DlmSum dep_system_provides dep_system_consmts nn_system_summarization poss_system_Our dep_Description_system nn_Description_System num_Description_2 dep_features_Description nn_features_text amod_features_extractable advmod_extractable_easily nn_rules_summarization prep_from_learning_features dobj_learning_rules nsubj_portable_system conj_and_scalable_portable nsubj_scalable_system nn_system_summarlzatlon poss_system_our prepc_by_make_learning xcomp_make_portable xcomp_make_scalable aux_make_to vmod_goal_make dep_goal_m nn_goal_ultlmate poss_goal_Our dep_approaches_goal amod_approaches_knowledge-based det_approaches_the prep_of_rules_approaches amod_rules_domain-dependent amod_rules_hand-crafting nn_rules_avoidmg nn_rules_whde nn_rules_performance amod_rules_human nn_rules_model dobj_whlch_rules nn_ways_m nn_ways_infrastructure nn_ways_productlve amod_ways_thin dobj_explmt_ways aux_explmt_to dep_allow_whlch xcomp_allow_explmt dobj_allow_us advmod_allow_also aux_allow_will nsubj_allow_methodology nn_methodology_training amod_methodology_planned poss_methodology_our rcmod_approaches_allow advmod_approaches_Moreover amod_approaches_discourse-based prep_to_contrast_labor-lntenslve nn_contrast_m nn_contrast_methods amod_contrast_automated prep_from_ohtinned_contrast prep_of_layers_mformatlon amod_layers_multlple det_layers_these nn_structure_dlscourse nn_structure_text parataxis_apprommate_describe prep_through_apprommate_layers dobj_apprommate_structure aux_apprommate_to xcomp_attempts_apprommate nsubj_attempts_approach poss_approach_Our nn_approaches_frequency amod_approaches_other nn_knowledge_domain nn_knowledge_Imgmstlc conj_and_Imgmstlc_domain prep_than_source_approaches prep_of_source_knowledge amod_source_indexed amod_source_richer det_source_a ccomp_create_attempts dobj_create_source aux_create_to xcomp_allow_create dobj_allow_us nsubj_allow_resources nsubj_allow_extractlon conj_and_techtuques_resources dep_These_resources dep_These_techtuques dep_resources_These nn_resources_NLP amod_resources_on-hne amod_resources_readlly-avmlable conj_and_extractlon_resources nn_extractlon_mforrmatlon amod_extractlon_robust
W98-1104	J90-1003	o	RIDF is like MI but different References Church K. and P. Hanks -LRB- 1990 -RRB- Word association norms mutual information and lexicography Computational Linguistics 16:1 pp	appos_Linguistics_pp appos_Linguistics_16:1 amod_Linguistics_Computational nn_Linguistics_lexicography amod_information_mutual nn_norms_association nn_norms_Word dep_norms_1990 nn_norms_Hanks nn_Hanks_P. conj_and_Church_Linguistics conj_and_Church_information conj_and_Church_norms conj_and_Church_K. nn_Church_References amod_Church_different conj_but_is_Linguistics conj_but_is_information conj_but_is_norms conj_but_is_K. conj_but_is_Church prep_like_is_MI nsubj_is_RIDF
W98-1104	J90-1003	o	l -LRB- x y -RRB- = log -LRB- P -LRB- x y -RRB- / e -LRB- x -RRB- e -LRB- y -RRB- -RRB- MI has been used to identify a variety of interesting linguistic phenomena ranging from semantic relations of the doctor/nurse type to lexico-syntactic co-occurrence preferences of the save/from type -LRB- Church and Hanks 1990 -RRB-	amod_Church_1990 conj_and_Church_Hanks dep_type_Hanks dep_type_Church nn_type_save/from det_type_the prep_of_preferences_type nn_preferences_co-occurrence amod_preferences_lexico-syntactic nn_type_doctor/nurse det_type_the prep_of_relations_type amod_relations_semantic prep_to_ranging_preferences prep_from_ranging_relations vmod_phenomena_ranging amod_phenomena_linguistic amod_phenomena_interesting prep_of_variety_phenomena det_variety_a dobj_identify_variety aux_identify_to xcomp_used_identify auxpass_used_been aux_used_has nsubjpass_used_P dep_MI_e dep_y_e nn_y_x dep_e_y appos_x_y dep_P_MI dep_P_x nn_P_log amod_P_= nn_P_l dep_x_y appos_l_x
H05-1025	J92-1002	o	A standard solution is to use a weighted linear mixture of N-gram models 1 n N -LRB- Brown et al. 1992 -RRB-	amod_Brown_1992 dep_Brown_al. nn_Brown_et nn_N_n num_N_1 nn_models_N-gram appos_mixture_Brown appos_mixture_N prep_of_mixture_models amod_mixture_linear amod_mixture_weighted det_mixture_a dobj_use_mixture aux_use_to xcomp_is_use nsubj_is_solution amod_solution_standard det_solution_A ccomp_``_is
H05-1025	J92-1002	o	Previous studies have shed light on the predictability of the next unix command that a user will enter -LRB- Motoda and Yoshida 1997 Davison and Hirsch 1998 -RRB- the next keystrokes on a small input device such as a PDA -LRB- Darragh and Witten 1992 -RRB- and of the translation that a human translator will choose for a given foreign sentence -LRB- Nepveu et al. 2004 -RRB-	amod_Nepveu_2004 dep_Nepveu_al. nn_Nepveu_et amod_sentence_foreign amod_sentence_given det_sentence_a dep_choose_Nepveu prep_for_choose_sentence aux_choose_will nsubj_choose_translator mark_choose_that amod_translator_human det_translator_a det_translation_the dep_of_choose pobj_of_translation dep_Darragh_1992 conj_and_Darragh_Witten appos_PDA_Witten appos_PDA_Darragh det_PDA_a prep_such_as_device_PDA nn_device_input amod_device_small det_device_a conj_and_keystrokes_of prep_on_keystrokes_device amod_keystrokes_next det_keystrokes_the dep_,_of dep_,_keystrokes num_Davison_1998 conj_and_Davison_Hirsch conj_and_Motoda_Hirsch conj_and_Motoda_Davison conj_and_Motoda_1997 conj_and_Motoda_Yoshida dep_enter_Davison dep_enter_1997 dep_enter_Yoshida dep_enter_Motoda aux_enter_will nsubj_enter_user mark_enter_that det_user_a amod_command_unix amod_command_next det_command_the ccomp_predictability_enter prep_of_predictability_command det_predictability_the prep_on_shed_predictability dobj_shed_light aux_shed_have nsubj_shed_studies amod_studies_Previous ccomp_``_shed
I08-5010	J92-1002	o	This is due to the reason that Telugu -LRB- Entropy = 15.625 bits per character -RRB- -LRB- Bharati et al. 1998 -RRB- is comparitively a high entropy language than English -LRB- Brown and Pietra 1992 -RRB-	amod_Brown_1992 conj_and_Brown_Pietra dep_English_Pietra dep_English_Brown prep_than_language_English nn_language_entropy amod_language_high det_language_a advmod_language_comparitively cop_language_is nsubj_language_Telugu dobj_language_that amod_Bharati_1998 dep_Bharati_al. nn_Bharati_et prep_per_bits_character num_bits_15.625 dep_=_bits amod_Entropy_= dep_Telugu_Bharati dep_Telugu_Entropy rcmod_reason_language det_reason_the prep_to_due_reason cop_due_is nsubj_due_This ccomp_``_due
J93-1001	J92-1002	o	As a result the empirical approach has been adopted by almost all contemporary part-of-speech programs Bahl and Mercer -LRB- 1976 -RRB- Leech Garside and Atwell -LRB- 1983 -RRB- Jelinek -LRB- 1985 -RRB- Deroualt and Merialdo -LRB- 1986 -RRB- Garside Leech and Sampson -LRB- 1987 -RRB- Church -LRB- 1988 -RRB- DeRose -LRB- 1988 -RRB- Hindle -LRB- 1989 -RRB- Kupiec -LRB- 1989 1992 -RRB- Ayuso et al.	nn_al._et nn_al._Ayuso dep_1989_1992 dep_Kupiec_1989 appos_Hindle_1989 appos_DeRose_1988 appos_Church_1988 appos_Sampson_1987 conj_and_Garside_Sampson conj_and_Garside_Leech appos_Merialdo_1986 conj_and_Deroualt_Merialdo dep_Jelinek_al. appos_Jelinek_Kupiec appos_Jelinek_Hindle appos_Jelinek_DeRose appos_Jelinek_Church appos_Jelinek_Sampson appos_Jelinek_Leech appos_Jelinek_Garside appos_Jelinek_Merialdo appos_Jelinek_Deroualt appos_Jelinek_1985 appos_Atwell_Jelinek appos_Atwell_1983 appos_Mercer_1976 conj_and_Bahl_Atwell conj_and_Bahl_Garside conj_and_Bahl_Leech conj_and_Bahl_Mercer amod_programs_part-of-speech amod_programs_contemporary det_programs_all advmod_programs_almost dep_adopted_Atwell dep_adopted_Garside dep_adopted_Leech dep_adopted_Mercer dep_adopted_Bahl agent_adopted_programs auxpass_adopted_been aux_adopted_has nsubjpass_adopted_approach prep_as_adopted_result amod_approach_empirical det_approach_the det_result_a
J93-1001	J92-1002	o	Model Bits / Character ASCII Huffman code each char Lempel-Ziv -LRB- Unix TM compress -RRB- Unigram -LRB- Huffman code each word -RRB- Trigram Human Performance 8 5 4.43 2.1 -LRB- Brown personal communication -RRB- 1.76 -LRB- Brown et al. 1992 -RRB- 1.25 -LRB- Shannon 1951 -RRB- The cross entropy H of a code and a source is given by H -LRB- source code -RRB- = ~ ~ Pr -LRB- s h I source -RRB- log 2 Pr -LRB- s I h code -RRB- s h where Pr -LRB- s h I source -RRB- is the joint probability of a symbol s following a history h given the source	det_source_the pobj_given_source prep_h_given nn_h_history det_h_a prep_following_s_h nsubj_s_s dep_s_Pr det_symbol_a prep_of_probability_symbol amod_probability_joint det_probability_the cop_probability_is nsubj_probability_s advmod_probability_where nn_probability_h dep_source_I nn_source_h dep_s_source nn_s_Pr dobj_s_probability nn_code_h dep_code_I dep_s_code appos_Pr_s num_Pr_2 nn_Pr_log nn_Pr_source dep_Pr_I nn_Pr_h vmod_s_s dep_s_Pr num_Pr_~ num_Pr_~ dep_=_s appos_source_code amod_H_= dep_H_source agent_given_H auxpass_given_is nsubjpass_given_code dep_given_Unigram det_source_a det_code_a conj_and_entropy_source prep_of_entropy_code appos_entropy_H nn_entropy_cross det_entropy_The num_entropy_1.25 num_Shannon_1951 appos_1.25_Shannon number_1.25_1.76 dep_al._1992 nn_al._et advmod_Brown_al. appos_1.76_Brown amod_communication_personal appos_Brown_communication number_2.1_4.43 dep_2.1_5 number_5_8 dep_Performance_source dep_Performance_entropy dep_Performance_Brown num_Performance_2.1 nn_Performance_Human nn_Performance_Trigram det_word_each dep_code_Performance dep_code_word nn_code_Huffman dep_TM_compress nn_TM_Unix dep_Lempel-Ziv_given appos_Lempel-Ziv_TM nn_Lempel-Ziv_char det_Lempel-Ziv_each dobj_code_Lempel-Ziv nsubj_code_Huffman nn_Huffman_ASCII nn_Huffman_Character dep_Bits_code dep_Model_Bits
J96-2003	J92-1002	o	Illustrative clusterings of this type can also be found in Pereira Tishby and Lee -LRB- 1993 -RRB- Brown Della Pietra Mercer Della Pietra and Lai -LRB- 1992 -RRB- Kneser and Ney -LRB- 1993 -RRB- and Brill et al.	nn_al._et nn_al._Brill appos_Ney_1993 appos_Lai_1992 nn_Pietra_Della nn_Pietra_Della appos_Lee_Pietra appos_Lee_Mercer appos_Lee_Pietra appos_Lee_Brown appos_Lee_1993 conj_and_Pereira_Ney conj_and_Pereira_Kneser conj_and_Pereira_Lai conj_and_Pereira_Lee conj_and_Pereira_Tishby conj_and_found_al. prep_in_found_Ney prep_in_found_Kneser prep_in_found_Lai prep_in_found_Lee prep_in_found_Tishby prep_in_found_Pereira auxpass_found_be advmod_found_also aux_found_can nsubjpass_found_clusterings det_type_this prep_of_clusterings_type amod_clusterings_Illustrative
J96-2003	J92-1002	o	Successful approaches aimed at trying to overcome the sparse data limitation include backoff -LRB- Katz 1987 -RRB- Turing-Good variants -LRB- Good 1953 Church and Gale 1991 -RRB- interpolation -LRB- Jelinek 1985 -RRB- deleted estimation -LRB- Jelinek 1985 Church and Gale 1991 -RRB- similarity-based models -LRB- Dagan Pereira and Lee 1994 Essen and Steinbiss 1992 -RRB- Pos-language models -LRB- Derouault and Merialdo 1986 -RRB- and decision tree models -LRB- Bahl et al. 1989 Black Garside and Leech 1993 Magerman 1994 -RRB-	num_Magerman_1994 num_Leech_1993 conj_and_Black_Leech conj_and_Black_Garside dep_Bahl_Magerman dep_Bahl_Leech dep_Bahl_Garside dep_Bahl_Black dep_Bahl_1989 dep_Bahl_al. nn_Bahl_et nn_models_tree nn_models_decision num_Merialdo_1986 conj_and_Derouault_Merialdo dep_models_Merialdo dep_models_Derouault amod_models_Pos-language num_Steinbiss_1992 conj_and_Essen_Steinbiss num_Lee_1994 dep_Dagan_Steinbiss dep_Dagan_Essen conj_and_Dagan_Lee conj_and_Dagan_Pereira appos_models_Lee appos_models_Pereira appos_models_Dagan amod_models_similarity-based num_Church_1991 conj_and_Church_Gale dep_Jelinek_Gale dep_Jelinek_Church num_Jelinek_1985 appos_estimation_Jelinek amod_estimation_deleted num_Jelinek_1985 appos_interpolation_Jelinek dep_Church_1991 conj_and_Church_Gale dep_1953_Gale dep_1953_Church amod_1953_Good dep_variants_1953 amod_variants_Turing-Good num_Katz_1987 appos_backoff_Bahl conj_and_backoff_models conj_and_backoff_models conj_and_backoff_models conj_and_backoff_estimation conj_and_backoff_interpolation conj_and_backoff_variants appos_backoff_Katz dobj_include_models dobj_include_models dobj_include_models dobj_include_estimation dobj_include_interpolation dobj_include_variants dobj_include_backoff nsubj_include_approaches nn_limitation_data amod_limitation_sparse det_limitation_the dobj_overcome_limitation aux_overcome_to xcomp_trying_overcome prepc_at_aimed_trying vmod_approaches_aimed amod_approaches_Successful
J96-2003	J92-1002	o	Much research has been carried out recently in this area -LRB- Hughes and Atwell 1994 Finch and Chater 1994 Redington Chater and Finch 1993 Brill et al. 1990 Kiss 1973 Pereira and Tishby 1992 Resnik 1993 Ney Essen and Kneser 1994 Matsukawa 1993 -RRB-	num_Matsukawa_1993 num_Kneser_1994 conj_and_Ney_Kneser conj_and_Ney_Essen num_Resnik_1993 num_Tishby_1992 conj_and_Pereira_Tishby dobj_Kiss_1973 dep_al._1990 nn_al._et nn_al._Brill num_Finch_1993 conj_and_Redington_Finch conj_and_Redington_Chater num_Chater_1994 conj_and_Finch_Chater num_Atwell_1994 dep_Hughes_Matsukawa dep_Hughes_Kneser dep_Hughes_Essen dep_Hughes_Ney dep_Hughes_Resnik dep_Hughes_Tishby dep_Hughes_Pereira dep_Hughes_Kiss dep_Hughes_al. dep_Hughes_Finch dep_Hughes_Chater dep_Hughes_Redington conj_and_Hughes_Chater conj_and_Hughes_Finch conj_and_Hughes_Atwell det_area_this dep_carried_Finch dep_carried_Atwell dep_carried_Hughes prep_in_carried_area advmod_carried_recently prt_carried_out auxpass_carried_been aux_carried_has nsubjpass_carried_research amod_research_Much
J96-2003	J92-1002	o	Introduction Many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings that is by using statistical language model information -LRB- Church et al. 1991 Church and Mercer 1993 Gale Church and Yarowsky 1992 Liddy and Paik 1992 -RRB-	num_Paik_1992 conj_and_Liddy_Paik num_Yarowsky_1992 conj_and_Gale_Yarowsky conj_and_Gale_Church num_Church_1993 conj_and_Church_Mercer num_al._1991 dep_Church_Paik dep_Church_Liddy dep_Church_Yarowsky dep_Church_Church dep_Church_Gale dep_Church_Mercer dep_Church_Church dep_Church_al. nn_Church_et dep_information_Church nn_information_model nn_information_language amod_information_statistical dobj_using_information pcomp_by_using ccomp_,_by dep_that_is nn_strings_word prep_of_probabilities_strings det_probabilities_the prep_about_information_probabilities dobj_incorporating_information dep_enhanced_that agent_enhanced_incorporating auxpass_enhanced_be aux_enhanced_can nsubjpass_enhanced_applications amod_language_natural dobj_process_language nsubj_process_that rcmod_applications_process amod_applications_Many nn_applications_Introduction
P09-2087	J92-1002	o	Dependency models -LRB- Rosenfeld 2000 -RRB- use the parsed dependency structure of sentences to build the language model as in grammatical trigrams -LRB- Lafferty et al. 1992 -RRB- structured language models -LRB- Chelba and Jelinek 2000 -RRB- and dependency language models -LRB- Chelba et al. 1997 -RRB-	amod_Chelba_1997 dep_Chelba_al. nn_Chelba_et nn_models_language nn_models_dependency dep_Chelba_2000 conj_and_Chelba_Jelinek dep_models_Jelinek dep_models_Chelba nn_models_language amod_models_structured amod_Lafferty_1992 dep_Lafferty_al. nn_Lafferty_et amod_trigrams_grammatical pobj_in_trigrams pcomp_as_in nn_model_language det_model_the dep_build_Chelba conj_and_build_models conj_and_build_models dep_build_Lafferty prep_build_as dobj_build_model aux_build_to prep_of_structure_sentences nn_structure_dependency amod_structure_parsed det_structure_the vmod_use_models vmod_use_models vmod_use_build dobj_use_structure nsubj_use_models amod_Rosenfeld_2000 dep_models_Rosenfeld nn_models_Dependency
P99-1036	J92-1002	o	In order to estimate the entropy of English -LRB- Brown et al. 1992 -RRB- approximated P -LRB- kI <UNK> -RRB- by a Poisson distribution whose parameter is the average word length A in the training corpus and P -LRB- cz cklk <UNK> -RRB- by the product of character zerogram probabilities	nn_probabilities_zerogram nn_probabilities_character prep_of_product_probabilities det_product_the appos_cklk_<UNK> nn_cklk_cz prep_by_P_product dep_P_cklk nn_corpus_training det_corpus_the prep_in_A_corpus nn_A_length nn_A_word amod_A_average det_A_the cop_A_is nsubj_A_parameter poss_parameter_whose rcmod_distribution_A nn_distribution_Poisson det_distribution_a nn_<UNK>_kI conj_and_P_P prep_by_P_distribution appos_P_<UNK> amod_P_approximated appos_P_Brown dep_P_estimate amod_Brown_1992 dep_Brown_al. nn_Brown_et prep_of_entropy_English det_entropy_the dobj_estimate_entropy aux_estimate_to dep_estimate_order mark_estimate_In
W97-0506	J92-1002	o	It is sometimes assumed that estimates of entropy -LRB- e.g. Shannon 's estimate that English is 75 % redundant Brown et al 's -LRB- 1992 -RRB- upper bound of 1.75 bits per character for printed English -RRB- are directly 3There are some cases where words are deliberately misspelled in order to get better output from the synthesizer such as coyote spelled kiote	dobj_spelled_kiote nsubj_spelled_coyote mark_spelled_as mwe_as_such dep_synthesizer_spelled det_synthesizer_the prep_from_output_synthesizer amod_output_better dobj_get_output aux_get_to dep_get_order mark_get_in advcl_misspelled_get advmod_misspelled_deliberately auxpass_misspelled_are nsubjpass_misspelled_words advmod_misspelled_where rcmod_cases_misspelled det_cases_some cop_cases_are nsubj_cases_3There advmod_3There_directly cop_3There_are nsubj_3There_estimates mark_3There_that amod_English_printed prep_per_bits_character num_bits_1.75 prep_for_bound_English prep_of_bound_bits dep_bound_upper vmod_al_bound dep_al_1992 possessive_al_'s nn_al_et amod_al_Brown dep_redundant_al npadvmod_redundant_% cop_redundant_is nsubj_redundant_English mark_redundant_that num_%_75 ccomp_estimate_redundant poss_estimate_Shannon appos_e.g._estimate dep_entropy_e.g. prep_of_estimates_entropy ccomp_assumed_cases advmod_assumed_sometimes auxpass_assumed_is nsubjpass_assumed_It ccomp_``_assumed
W97-0506	J92-1002	o	Work at the University of Dundee -LRB- e.g. Aim et al 1992 Todman and Alm this volume -RRB- has shown that the extensive use of fixed text for sequences such as greetings and prestored narratives is beneficial in AAC	prep_in_beneficial_AAC cop_beneficial_is nsubj_beneficial_narratives nsubj_beneficial_use mark_beneficial_that amod_narratives_prestored prep_such_as_sequences_greetings amod_text_fixed conj_and_use_narratives prep_for_use_sequences prep_of_use_text amod_use_extensive det_use_the ccomp_shown_beneficial aux_shown_has prep_shown_e.g. nsubj_shown_Work det_volume_this dep_Todman_volume conj_and_Todman_Alm dep_al_1992 nn_al_et nn_al_Aim dep_e.g._Alm dep_e.g._Todman conj_e.g._al prep_of_University_Dundee det_University_the prep_at_Work_University
W98-1217	J92-1002	o	-LRB- Farach et al. 1995 Wyner in press -RRB- describe a novel algorithm for entropy estimation for which they claim very fast convergence time using no more than about five pages of text they can achieve nearly the same accuracy as -LRB- Brown et al. 1992 -RRB-	num_al._1992 nn_al._et amod_al._Brown dep_as_al. amod_accuracy_as amod_accuracy_same det_accuracy_the advmod_accuracy_nearly dobj_achieve_accuracy aux_achieve_can nsubj_achieve_they vmod_achieve_using prep_of_pages_text num_pages_five quantmod_five_about prep_than_more_pages neg_more_no dobj_using_more nn_time_convergence amod_time_fast advmod_fast_very dobj_claim_time nsubj_claim_they prep_for_claim_which rcmod_estimation_claim amod_estimation_entropy prep_for_algorithm_estimation amod_algorithm_novel det_algorithm_a parataxis_describe_achieve dobj_describe_algorithm nsubj_describe_Farach prep_in_Wyner_press dep_Farach_Wyner appos_Farach_1995 dep_Farach_al. nn_Farach_et
C00-2121	J92-4003	o	Methods that use bigrams -LRB- Brown et al. 1992 -RRB- or trigrams -LRB- Martin et al. 1998 -RRB- cluster words considering as a word 's context the one or two immediately adjacent words and employ as clustering criteria the minimal loss of average 836 nmtual information and the perplexity improvement respectively	advmod_improvement_respectively nn_improvement_perplexity det_improvement_the amod_information_nmtual num_information_836 amod_information_average conj_and_loss_improvement prep_of_loss_information amod_loss_minimal det_loss_the dep_criteria_improvement dep_criteria_loss nn_criteria_clustering prep_as_employ_criteria amod_words_adjacent advmod_words_immediately num_words_two det_one_the conj_or_context_words num_context_one poss_context_word det_word_a prep_as_considering_words prep_as_considering_context conj_and_words_employ vmod_words_considering nn_words_cluster nn_words_trigrams nn_words_Methods amod_Martin_1998 dep_Martin_al. nn_Martin_et amod_Brown_1992 dep_Brown_al. nn_Brown_et dobj_use_bigrams nsubj_use_that dep_Methods_Martin conj_or_Methods_trigrams dep_Methods_Brown rcmod_Methods_use
C00-2121	J92-4003	o	Most approaches -LRB- Brown et al. 1992 Li & Abe 1997 -RRB- inherently extract semantic knowledge in the abstracted form of semantic clusters	amod_clusters_semantic prep_of_form_clusters amod_form_abstracted det_form_the prep_in_knowledge_form amod_knowledge_semantic nn_knowledge_extract advmod_extract_inherently num_Li_1997 conj_and_Li_Abe dep_Brown_Abe dep_Brown_Li appos_Brown_1992 dep_Brown_al. nn_Brown_et dep_approaches_knowledge dep_approaches_Brown amod_approaches_Most
C00-2128	J92-4003	o	Previous approaches to processing lnetonymy have used hand-constructed ontologies or semantic networks -LRB- \ -RSB- ? ass 1988 Iverson and Hehnreich 1992 B -LRB- maud et al. 1996 Fass 1997 -RRB-	amod_Fass_1997 dep_maud_Fass appos_maud_1996 dep_maud_al. nn_maud_et appos_B_maud conj_and_ass_B conj_and_ass_1992 conj_and_ass_Hehnreich conj_and_ass_Iverson conj_and_ass_1988 amod_networks_semantic dep_ontologies_\ conj_or_ontologies_networks amod_ontologies_hand-constructed dep_used_B dep_used_1992 dep_used_Hehnreich dep_used_Iverson dep_used_1988 dep_used_ass dobj_used_networks dobj_used_ontologies aux_used_have nsubj_used_approaches nn_lnetonymy_processing prep_to_approaches_lnetonymy amod_approaches_Previous
C00-2128	J92-4003	o	A large corpus is vahmble as a source of such nouns -LRB- Church and Hanks 1990 Brown et al. 1992 -RRB-	tmod_al._1992 nn_al._et nn_al._Brown conj_and_Church_al. conj_and_Church_1990 conj_and_Church_Hanks dep_nouns_al. dep_nouns_1990 dep_nouns_Hanks dep_nouns_Church amod_nouns_such prep_of_source_nouns det_source_a prep_as_vahmble_source cop_vahmble_is nsubj_vahmble_corpus amod_corpus_large det_corpus_A
C02-1012	J92-4003	o	Brown et al -LRB- 1992 -RRB- put forward and discussed n-gram models based on classes of words	prep_of_classes_words prep_on_based_classes vmod_models_based nn_models_n-gram amod_models_discussed conj_and_put_models advmod_put_forward nsubj_put_al appos_al_1992 nn_al_et nn_al_Brown
C04-1014	J92-4003	o	Among all the language modeling approaches ngram models have been most widely used in speech recognition -LRB- Jelinek 1990 Gale and Church 1990 Brown et al. 1992 Yang et al. 1996 -RRB- and other applications	amod_applications_other dep_al._1996 nn_al._et nn_al._Yang num_al._1992 dep_Brown_al. nn_Brown_et num_Church_1990 conj_and_Gale_Church dep_Jelinek_al. dep_Jelinek_Brown dep_Jelinek_Church dep_Jelinek_Gale num_Jelinek_1990 conj_and_recognition_applications appos_recognition_Jelinek nn_recognition_speech prep_in_used_applications prep_in_used_recognition advmod_used_widely auxpass_used_been aux_used_have nsubjpass_used_models prep_among_used_approaches advmod_widely_most nn_models_ngram nn_approaches_modeling nn_approaches_language det_approaches_the predet_approaches_all
C04-1146	J92-4003	p	Similarity-based smoothing -LRB- Brown et al. 1992 Dagan et al. 1999 -RRB- is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events	amod_events_similar advmod_events_distributionally prep_of_co-occurrences_events dobj_seen_co-occurrences prepc_of_probabilities_seen prep_from_estimated_probabilities auxpass_estimated_are nsubjpass_estimated_probabilities advmod_estimated_where amod_co-occurrences_unseen prep_of_probabilities_co-occurrences det_problem_this advcl_approach_estimated prep_to_approach_problem amod_approach_appealing det_approach_an cop_approach_is nsubj_approach_smoothing advmod_appealing_intuitively nn_al._et nn_al._Dagan num_al._1999 dep_al._al. num_al._1992 nn_al._et amod_al._Brown dep_smoothing_al. amod_smoothing_Similarity-based
C08-1017	J92-4003	o	-LRB- 1992 -RRB- describe one application of MI to identify word collocations Kashioka et al.	nn_al._et nn_al._Kashioka nn_collocations_word dobj_identify_collocations aux_identify_to vmod_application_identify prep_of_application_MI num_application_one dep_describe_al. dobj_describe_application nsubj_describe_1992
C08-1017	J92-4003	o	We believe the benefit to limiting the size of n is connected to Brown et al. s -LRB- 1992 470 -RRB- observation that as n increases the accuracy of an n-gram model increases but the reliability of our parameter estimates drawn as they must be from a limited training text decreases	nsubj_decreases_reliability nsubj_decreases_accuracy prep_as_decreases_increases mark_decreases_that nn_text_training amod_text_limited det_text_a prep_from_be_text aux_be_must nsubj_be_they mark_be_as advcl_drawn_be vmod_estimates_drawn nn_estimates_parameter poss_estimates_our prep_of_reliability_estimates det_reliability_the nn_increases_model nn_increases_n-gram det_increases_an conj_but_accuracy_reliability prep_of_accuracy_increases det_accuracy_the nn_increases_n ccomp_observation_decreases nn_observation_s dep_1992_470 dep_s_1992 nn_s_al. nn_s_et nn_s_Brown prep_to_connected_observation auxpass_connected_is nsubjpass_connected_benefit prep_of_size_n det_size_the dobj_limiting_size prepc_to_benefit_limiting det_benefit_the ccomp_believe_connected nsubj_believe_We ccomp_``_believe
C08-1051	J92-4003	o	Applications of word clustering include language modeling -LRB- Brown et al. 1992 -RRB- text classification -LRB- Baker and McCallum 1998 -RRB- thesaurus construction -LRB- Lin 1998 -RRB- and so on	advmod_on_so dep_Lin_1998 appos_construction_Lin nn_construction_thesaurus dep_Baker_1998 conj_and_Baker_McCallum dep_classification_McCallum dep_classification_Baker nn_classification_text dep_al._1992 nn_al._et amod_al._Brown conj_and_modeling_on conj_and_modeling_construction appos_modeling_classification dep_modeling_al. nn_modeling_language dobj_include_on dobj_include_construction dobj_include_modeling nsubj_include_Applications nn_clustering_word prep_of_Applications_clustering
C94-2198	J92-4003	o	We have used a state-of-the-art Chinese handwriting recognizer -LRB- Li et al. 1992 -RRB- developed by ATC CCL ITRI Taiwan as the basis of our experiments	poss_experiments_our prep_of_basis_experiments det_basis_the prep_as_Taiwan_basis appos_ATC_Taiwan conj_ATC_ITRI conj_ATC_CCL agent_developed_ATC amod_Li_1992 dep_Li_al. nn_Li_et vmod_recognizer_developed dep_recognizer_Li nn_recognizer_handwriting amod_recognizer_Chinese amod_recognizer_state-of-the-art det_recognizer_a dobj_used_recognizer aux_used_have nsubj_used_We ccomp_``_used
C94-2198	J92-4003	o	For a class bigram model find V -- + C to maximize ~ -LRB- T -RRB- = ~ I/L = I p -LRB- wi I -LRB- wl -RRB- -RRB- p -LRB- -LRB- wi -RRB- l -LRB- wi-1 -RRB- -RRB- -RRB- -RRB- Alternatively perplexity -LRB- Jardino an d Adda 1993 -RRB- or average mutual information -LRB- Brown et al. 1992 -RRB- can be used as the characteristic value for optimization	prep_for_value_optimization amod_value_characteristic det_value_the prep_as_used_value auxpass_used_be aux_used_can nsubjpass_used_information nsubjpass_used_perplexity advmod_used_Alternatively nsubjpass_used_p dep_used_= dep_used_I/L dep_used_= dep_used_~ dep_used_maximize dep_used_C dep_used_V num_al._1992 nn_al._et amod_al._Brown dep_information_al. amod_information_mutual amod_information_average dep_Adda_1993 nn_Adda_d det_Adda_an dep_Jardino_Adda conj_or_perplexity_information dep_perplexity_Jardino dep_Alternatively_wi appos_l_wi-1 dep_wi_l nn_p_p appos_wi_wl num_wi_I dep_p_wi num_p_I num_I/L_~ appos_~_T aux_maximize_to conj_+_V_C parataxis_find_used prep_for_find_model nn_model_bigram nn_model_class det_model_a
C94-2198	J92-4003	o	INTRODUCTION Class-based language models -LRB- Brown et al. 1992 -RRB- have been proposed for dealing with two problems confronted by the well-known word n-gram language models -LRB- 1 -RRB- data sparseness the amount of training data is insufficient for estimating the huge number of parameters and -LRB- 2 -RRB- domain robustness the model is not adaptable to new application domains	nn_domains_application amod_domains_new prep_to_adaptable_domains neg_adaptable_not cop_adaptable_is nsubj_adaptable_model det_model_the dep_robustness_adaptable nn_robustness_domain dep_robustness_2 prep_of_number_parameters amod_number_huge det_number_the dobj_estimating_number conj_and_insufficient_robustness prepc_for_insufficient_estimating cop_insufficient_is nsubj_insufficient_amount nn_data_training prep_of_amount_data det_amount_the nn_sparseness_data dep_models_sparseness appos_models_1 nn_models_language nn_models_n-gram nn_models_word amod_models_well-known det_models_the agent_confronted_models vmod_problems_confronted num_problems_two prep_with_dealing_problems parataxis_proposed_robustness parataxis_proposed_insufficient prepc_for_proposed_dealing auxpass_proposed_been aux_proposed_have nsubjpass_proposed_models amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_models_Brown nn_models_language amod_models_Class-based nn_models_INTRODUCTION
C96-1003	J92-4003	o	have been proposed -LRB- Hindle 1990 Brown et al. 1992 Pereira et al. 1993 Tokunaga et al. 1995 -RRB-	num_Tokunaga_1995 nn_Tokunaga_al. nn_Tokunaga_et num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Hindle_Tokunaga dep_Hindle_Pereira dep_Hindle_Brown dep_Hindle_1990 dep_proposed_Hindle auxpass_proposed_been aux_proposed_have
C96-1036	J92-4003	o	Language models such as N-gram class models -LRB- Brown et al. 1992 -RRB- and Ergodic Hidden Markov Models -LRB- Kuhn el al. 1994 -RRB- were proposed and used in applications such as syntactic class -LRB- POS -RRB- tagging for English -LRB- Cutting et al. 1992 -RRB- clustering and scoring of recognizer sentence hypotheses	nn_hypotheses_sentence nn_hypotheses_recognizer prep_of_scoring_hypotheses conj_and_clustering_scoring nn_al._et dobj_Cutting_1992 dobj_Cutting_al. prep_for_tagging_English vmod_class_tagging appos_class_POS amod_class_syntactic prep_such_as_applications_class prep_in_used_applications nsubjpass_used_models dep_proposed_scoring dep_proposed_clustering dep_proposed_Cutting conj_and_proposed_used auxpass_proposed_were nsubjpass_proposed_models amod_el_1994 dep_el_al. nn_el_Kuhn nn_Models_Markov nn_Models_Hidden nn_Models_Ergodic amod_Brown_1992 dep_Brown_al. nn_Brown_et conj_and_models_Models dep_models_Brown nn_models_class nn_models_N-gram dep_models_el prep_such_as_models_Models prep_such_as_models_models nn_models_Language
C96-2205	J92-4003	o	Brown et al. proposed a class-based n-gram model which generalizes the n-gram model to predict a word from previous words in a text -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown det_text_a amod_words_previous prep_in_word_text prep_from_word_words det_word_a dobj_predict_word aux_predict_to nn_model_n-gram det_model_the dobj_generalizes_model nsubj_generalizes_which dep_model_al. vmod_model_predict rcmod_model_generalizes nn_model_n-gram amod_model_class-based det_model_a amod_model_proposed dep_model_Brown advmod_proposed_al. nn_al._et
D08-1006	J92-4003	o	In future work we plan to experiment with richer representations e.g. including long-range n-grams -LRB- Rosenfeld 1996 -RRB- class n-grams -LRB- Brown et al. 1992 -RRB- grammatical features -LRB- Amaya and Benedy 2001 -RRB- etc '	dep_Amaya_2001 conj_and_Amaya_Benedy appos_features_Benedy appos_features_Amaya amod_features_grammatical dep_al._1992 nn_al._et amod_al._Brown dep_n-grams_al. nn_n-grams_class amod_Rosenfeld_1996 dep_n-grams_etc appos_n-grams_features appos_n-grams_n-grams dep_n-grams_Rosenfeld amod_n-grams_long-range prep_including_e.g._n-grams dep_representations_e.g. amod_representations_richer prep_with_experiment_representations prep_to_plan_experiment nsubj_plan_we prep_in_plan_work amod_work_future
D08-1096	J92-4003	o	Of particular relevance are class-based language models -LRB- e.g. -LRB- Saul and Pereira 1997 Brown et al. 1992 -RRB- -RRB-	num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Saul_Brown conj_and_Saul_1997 conj_and_Saul_Pereira appos_e.g._1997 appos_e.g._Pereira appos_e.g._Saul dep_models_e.g. nn_models_language amod_models_class-based cop_models_are prep_of_models_relevance amod_relevance_particular
D09-1003	J92-4003	o	-LRB- 2008 -RRB- who employ clusters of related words constructed by the Brown clustering algorithm -LRB- Brown et al. 1992 -RRB- for syntactic processing of texts	prep_of_processing_texts nn_processing_syntactic num_al._1992 nn_al._et amod_al._Brown nn_algorithm_clustering amod_algorithm_Brown det_algorithm_the agent_constructed_algorithm vmod_words_constructed amod_words_related prep_of_clusters_words prep_for_employ_processing dep_employ_al. dobj_employ_clusters nsubj_employ_who dep_employ_2008
D09-1003	J92-4003	n	This method was shown to outperform the class based model proposed in -LRB- Brown et al. 1992 -RRB- and can thus be expected to discover better clusters of words	prep_of_clusters_words amod_clusters_better dobj_discover_clusters aux_discover_to xcomp_expected_discover auxpass_expected_be advmod_expected_thus aux_expected_can nsubjpass_expected_method num_al._1992 nn_al._et amod_al._Brown dep_in_al. amod_proposed_in vmod_model_proposed amod_model_based dep_class_model det_class_the dobj_outperform_class aux_outperform_to conj_and_shown_expected xcomp_shown_outperform auxpass_shown_was nsubjpass_shown_method det_method_This
D09-1058	J92-4003	o	First hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm -LRB- Brown et al. 1992 -RRB-	num_al._1992 nn_al._et amod_al._Brown nn_algorithm_clustering nn_algorithm_al. nn_algorithm_et nn_algorithm_Brown det_algorithm_the dobj_using_algorithm vmod_data_using amod_data_unlabeled dep_derived_al. prep_from_derived_data auxpass_derived_are nsubjpass_derived_clusters advmod_derived_First nn_clusters_word amod_clusters_hierarchical ccomp_``_derived
D09-1116	J92-4003	o	Models of this type include -LRB- Brown et al. 1992 Zitouni 2007 -RRB- which use semantic word clustering and -LRB- Bahl et al. 1990 -RRB- which uses variablelength context	nn_context_variablelength dobj_uses_context nsubj_uses_which rcmod_Bahl_uses dep_Bahl_1990 dep_Bahl_al. nn_Bahl_et nn_clustering_word amod_clustering_semantic dobj_use_clustering nsubj_use_which dep_Zitouni_2007 conj_and_Brown_Bahl rcmod_Brown_use dep_Brown_Zitouni amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_include_Bahl dep_include_Brown nsubj_include_Models det_type_this prep_of_Models_type
E06-1050	J92-4003	o	Many methods exist for clustering e.g. -LRB- Brown et al. 1990 Cutting et al. 1992 Pereira et al. 1993 Karypis et al. 1999 -RRB-	dep_al._1999 nn_al._et nn_al._Karypis dep_al._1993 nn_al._et nn_al._Pereira amod_1992_Cutting nn_al._et advmod_Cutting_al. dep_Brown_al. conj_Brown_al. conj_Brown_1992 appos_Brown_1990 dep_Brown_al. nn_Brown_et appos_clustering_Brown dep_clustering_e.g. prep_for_exist_clustering nsubj_exist_methods amod_methods_Many ccomp_``_exist
E95-1039	J92-4003	o	Introduction There has been considerable recent interest in the use of statistical methods for grouping words in large on-line corpora into categories which capture some of our intuitions about the reference of the words we use and the relationships between them -LRB- e.g. Brown et al. 1992 Schiitze 1993 -RRB-	num_Schiitze_1993 conj_al._Schiitze conj_al._1992 nn_al._et dep_Brown_al. pobj_e.g._Brown dep_relationships_e.g. prep_between_relationships_them det_relationships_the conj_and_use_relationships nsubj_use_we rcmod_words_relationships rcmod_words_use det_words_the prep_of_reference_words det_reference_the poss_intuitions_our prep_of_some_intuitions prep_about_capture_reference dobj_capture_some nsubj_capture_which rcmod_categories_capture amod_corpora_on-line amod_corpora_large prep_into_words_categories prep_in_words_corpora amod_words_grouping amod_methods_statistical prep_of_use_methods det_use_the prep_for_interest_words prep_in_interest_use amod_interest_recent amod_interest_considerable cop_interest_been aux_interest_has expl_interest_There dep_Introduction_interest
E99-1010	J92-4003	o	Various clustering techniques have been proposed -LRB- Brown et al. 1992 Jardino and Adda 1993 Martin et al. 1998 -RRB- which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms	nn_algorithms_clustering amod_algorithms_iterative amod_criterion_maximum-likelihood det_criterion_a prep_with_optimizing_algorithms dobj_optimizing_criterion vmod_clustering_optimizing nn_clustering_word amod_clustering_automatic dobj_perform_clustering nsubj_perform_which num_Martin_1998 nn_Martin_al. nn_Martin_et num_Jardino_1993 conj_and_Jardino_Adda dep_Brown_Martin dep_Brown_Adda dep_Brown_Jardino amod_Brown_1992 dep_Brown_al. nn_Brown_et ccomp_proposed_perform dep_proposed_Brown auxpass_proposed_been aux_proposed_have nsubjpass_proposed_techniques nn_techniques_clustering amod_techniques_Various
H05-1026	J92-4003	o	decades like n-gram back-off word models -LRB- Katz 1987 -RRB- class models -LRB- Brown et al. 1992 -RRB- structured language models -LRB- Chelba and Jelinek 2000 -RRB- or maximum entropy language models -LRB- Rosenfeld 1996 -RRB-	amod_Rosenfeld_1996 dep_models_Rosenfeld nn_models_language amod_models_entropy nn_models_maximum dep_Chelba_2000 conj_and_Chelba_Jelinek dep_models_Jelinek dep_models_Chelba nn_models_language amod_models_structured amod_Brown_1992 dep_Brown_al. nn_Brown_et conj_or_models_models conj_or_models_models dep_models_Brown nn_models_class amod_Katz_1987 dep_models_Katz nn_models_word amod_models_back-off amod_models_n-gram dep_decades_models dep_decades_models dep_decades_models prep_like_decades_models
H05-1028	J92-4003	o	More specifically we use a class-based bigram model from -LRB- Brown et al 1992 -RRB- -RRB- | -LRB- -RRB- | -LRB- -RRB- | -LRB- 11 = iiiiii ccPcwPwwP -LRB- 3 -RRB- In Equation -LRB- 3 -RRB- c i is the class of the word w i which could be a syntactic class or a semantic class	amod_class_semantic det_class_a conj_or_class_class amod_class_syntactic det_class_a cop_class_be aux_class_could nsubj_class_which dep_w_i nn_w_word det_w_the prep_of_class_w det_class_the cop_class_is nsubj_class_i nn_i_c appos_Equation_3 dep_ccPcwPwwP_class dep_ccPcwPwwP_class rcmod_ccPcwPwwP_class prep_in_ccPcwPwwP_Equation dep_ccPcwPwwP_3 nn_ccPcwPwwP_iiiiii amod_ccPcwPwwP_= num_ccPcwPwwP_11 dep_|_ccPcwPwwP nn_|_| nn_|_| dep_al_1992 nn_al_et advmod_Brown_al dep_from_Brown nn_model_bigram amod_model_class-based det_model_a dobj_use_| prep_use_from dobj_use_model nsubj_use_we advmod_use_specifically dep_specifically_More
H93-1036	J92-4003	n	This is in contrast to purely statistical systems -LRB- e.g. \ -LSB- Brown et al. 1992 \ -RSB- -RRB- which are difficult to inspect and modify	conj_and_inspect_modify aux_inspect_to ccomp_difficult_modify ccomp_difficult_inspect cop_difficult_are nsubj_difficult_which num_\_1992 appos_al._\ nn_al._et dep_Brown_al. rcmod_\_difficult dep_\_Brown dep_e.g._\ dep_systems_e.g. amod_systems_statistical advmod_statistical_purely prep_to_contrast_systems prep_in_is_contrast nsubj_is_This
H93-1036	J92-4003	o	There has been considerable use in the NLP community of both WordNet -LRB- e.g. \ -LSB- Lehman et al. 1992 Resnik 1992 \ -RSB- -RRB- and LDOCE -LRB- e.g \ -LSB- Liddy et aL 1992 Wilks et al. 1990 \ -RSB- -RRB- but no one has merged the two in order to combine their strengths	poss_strengths_their dobj_combine_strengths aux_combine_to dep_combine_order mark_combine_in det_two_the advcl_merged_combine dobj_merged_two aux_merged_has nsubj_merged_one neg_one_no num_\_1990 appos_Wilks_\ dep_Wilks_al. nn_Wilks_et conj_aL_Wilks conj_aL_1992 nn_aL_et conj_but_Liddy_merged dep_Liddy_aL dep_e.g_merged dep_e.g_Liddy appos_e.g_\ dep_LDOCE_e.g num_\_1992 appos_Resnik_\ dep_Lehman_Resnik appos_Lehman_1992 dep_Lehman_al. nn_Lehman_et conj_and_\_LDOCE dep_\_Lehman dep_\_e.g. dep_WordNet_LDOCE dep_WordNet_\ preconj_WordNet_both prep_of_community_WordNet nn_community_NLP det_community_the prep_in_use_community amod_use_considerable cop_use_been aux_use_has expl_use_There
J02-3004	J92-4003	o	The smoothing methods proposed in the literature -LRB- overviews are provided by Dagan Lee and Pereira -LRB- 1999 -RRB- and Lee -LRB- 1999 -RRB- -RRB- can be generally divided into three types discounting -LRB- Katz 1987 -RRB- class-based smoothing -LRB- Resnik 1993 Brown et al. 1992 364 Computational Linguistics Volume 28 Number 3 Pereira Tishby and Lee 1993 -RRB- and distance-weighted averaging -LRB- Grishman and Sterling 1994 Dagan Lee and Pereira 1999 -RRB-	num_Pereira_1999 conj_and_Dagan_Pereira conj_and_Dagan_Lee num_Sterling_1994 dep_Grishman_Pereira dep_Grishman_Lee dep_Grishman_Dagan conj_and_Grishman_Sterling dep_averaging_Sterling dep_averaging_Grishman amod_averaging_distance-weighted num_Lee_1993 conj_and_Pereira_Lee appos_Pereira_Tishby num_Pereira_3 nn_Pereira_Number appos_Volume_Lee appos_Volume_Pereira num_Volume_28 nn_Volume_Linguistics amod_Volume_Computational num_Volume_364 num_al._1992 nn_al._et nn_al._Brown dep_Resnik_Volume dep_Resnik_al. num_Resnik_1993 appos_smoothing_Resnik amod_smoothing_class-based num_Katz_1987 conj_and_discounting_averaging conj_and_discounting_smoothing appos_discounting_Katz num_types_three dep_divided_averaging dep_divided_smoothing dep_divided_discounting prep_into_divided_types advmod_divided_generally auxpass_divided_be aux_divided_can nsubjpass_divided_methods appos_Lee_1999 conj_and_Pereira_Lee appos_Pereira_1999 conj_and_Dagan_Lee conj_and_Dagan_Pereira conj_and_Dagan_Lee agent_provided_Pereira agent_provided_Lee agent_provided_Dagan auxpass_provided_are nsubjpass_provided_overviews det_literature_the prep_in_proposed_literature dep_methods_provided vmod_methods_proposed nn_methods_smoothing det_methods_The ccomp_``_divided
J02-3004	J92-4003	o	Classes can be induced directly from the corpus using distributional clustering -LRB- Pereira Tishby and Lee 1993 Brown et al. 1992 Lee and Pereira 1999 -RRB- or taken from a manually crafted taxonomy -LRB- Resnik 1993 -RRB-	num_Resnik_1993 appos_taxonomy_Resnik amod_taxonomy_crafted det_taxonomy_a advmod_crafted_manually prep_from_taken_taxonomy num_Pereira_1999 conj_and_Lee_Pereira dep_al._1992 nn_al._et nn_al._Brown num_Lee_1993 conj_or_Pereira_taken dep_Pereira_Pereira dep_Pereira_Lee conj_and_Pereira_al. conj_and_Pereira_Lee conj_and_Pereira_Tishby dep_clustering_taken dep_clustering_al. dep_clustering_Lee dep_clustering_Tishby dep_clustering_Pereira amod_clustering_distributional dobj_using_clustering det_corpus_the xcomp_induced_using prep_from_induced_corpus advmod_induced_directly auxpass_induced_be aux_induced_can nsubjpass_induced_Classes
J03-1005	J92-4003	o	For this purpose we present a data-driven beam search algorithm similar to the one used in speech recognition search algorithms -LRB- Ney et al. 1992 -RRB-	advmod_1992_al. nn_al._et num_Ney_1992 nn_algorithms_search nn_algorithms_recognition nn_algorithms_speech prep_in_used_algorithms vmod_one_used det_one_the prep_to_similar_one amod_algorithm_similar nn_algorithm_search nn_algorithm_beam amod_algorithm_data-driven det_algorithm_a dep_present_Ney dobj_present_algorithm nsubj_present_we prep_for_present_purpose det_purpose_this
J03-1005	J92-4003	o	The distortion probabilities are class-based They depend on the word class F -LRB- f -RRB- of a covered source word f as well as on the word class E -LRB- e -RRB- of the previously generated target word e The classes are automatically trained -LRB- Brown et al. 1992 -RRB-	dep_1992_al. num_Brown_1992 nn_Brown_et advmod_trained_automatically auxpass_trained_are nsubjpass_trained_classes dep_trained_e dep_trained_word dep_trained_target dep_trained_generated dep_trained_the dep_trained_of det_classes_The advmod_generated_previously appos_E_Brown vmod_E_trained appos_E_e nn_E_class nn_E_word det_E_the pobj_on_E conj_and_f_on nn_f_word nn_f_source amod_f_covered det_f_a prep_of_F_on prep_of_F_f appos_F_f nn_F_class nn_F_word det_F_the prep_on_depend_F nsubj_depend_They parataxis_class-based_depend cop_class-based_are nsubj_class-based_probabilities nn_probabilities_distortion det_probabilities_The
J05-4002	J92-4003	p	Similarity-based smoothing -LRB- Hindle 1990 Brown et al. 1992 Dagan Marcus and Markovitch 1993 Pereira Tishby and Lee 1993 Dagan Lee and Pereira 1999 -RRB- provides an intuitively appealing approach to language modeling	nn_modeling_language prep_to_approach_modeling amod_approach_appealing det_approach_an advmod_appealing_intuitively dobj_provides_approach nsubj_provides_smoothing num_Pereira_1999 conj_and_Dagan_Pereira conj_and_Dagan_Lee num_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby num_Markovitch_1993 conj_and_Dagan_Markovitch conj_and_Dagan_Marcus nn_1992_al. num_Brown_1992 nn_Brown_et dep_1990_Pereira dep_1990_Lee dep_1990_Dagan conj_1990_Lee conj_1990_Tishby conj_1990_Pereira conj_1990_Markovitch conj_1990_Marcus conj_1990_Dagan conj_1990_Brown amod_1990_Hindle dep_smoothing_1990 amod_smoothing_Similarity-based
J05-4002	J92-4003	o	5.2 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique -LRB- Gale Church and Yarowsky 1992 Sch utze 1992 Pereira Tishby and Lee 1993 Sch utze 1998 Lee 1999 Dagan Lee and Pereira 1999 Golding and Roth 1999 Rooth et al. 1999 EvenZohar and Roth 2000 Lee 2001 Clark and Weir 2002 -RRB- and in the current setting we may use a nouns neighbors to decide which of two co-occurrences is the most likely	advmod_likely_most det_likely_the cop_likely_is nsubj_likely_which num_co-occurrences_two prep_of_which_co-occurrences ccomp_decide_likely aux_decide_to nn_neighbors_nouns det_neighbors_a xcomp_use_decide dobj_use_neighbors aux_use_may nsubj_use_we ccomp_use_in ccomp_use_technique amod_setting_current det_setting_the pobj_in_setting num_Clark_2002 conj_and_Clark_Weir num_Lee_2001 num_EvenZohar_2000 conj_and_EvenZohar_Roth dep_al._1999 nn_al._et nn_al._Rooth dep_Golding_1999 conj_and_Golding_Roth num_Pereira_1999 conj_and_Dagan_Pereira conj_and_Dagan_Lee num_Lee_1999 amod_1998_utze num_Sch_1998 num_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby amod_1992_utze num_Sch_1992 num_Yarowsky_1992 conj_and_Gale_Weir conj_and_Gale_Clark conj_and_Gale_Lee conj_and_Gale_Roth conj_and_Gale_EvenZohar conj_and_Gale_al. conj_and_Gale_Roth conj_and_Gale_Golding conj_and_Gale_Pereira conj_and_Gale_Lee conj_and_Gale_Dagan conj_and_Gale_Lee conj_and_Gale_Sch conj_and_Gale_Lee conj_and_Gale_Tishby conj_and_Gale_Pereira conj_and_Gale_Sch conj_and_Gale_Yarowsky conj_and_Gale_Church conj_and_technique_in appos_technique_Clark appos_technique_Lee appos_technique_EvenZohar appos_technique_al. appos_technique_Golding appos_technique_Dagan appos_technique_Lee appos_technique_Sch appos_technique_Pereira appos_technique_Sch appos_technique_Yarowsky appos_technique_Church appos_technique_Gale nn_technique_evaluation amod_technique_standard det_technique_a xcomp_become_use aux_become_have nsubj_become_tasks nn_tasks_Pseudo-disambiguation nn_tasks_Task nn_tasks_Pseudo-Disambiguation num_tasks_5.2
J95-3002	J92-4003	o	In addition explicitly using the left context symbols allows easy use of smoothing techniques such as deleted interpolation -LRB- Bahl Jelinek and Mercer 1983 -RRB- clustering techniques -LRB- Brown et al. 1992 -RRB- and model refinement techniques -LRB- Lin Chiang and Su 1994 -RRB- to estimate the probabilities more reliably by changing the window sizes of the context and weighting the various estimates dynamically	advmod_estimates_dynamically amod_estimates_various det_estimates_the conj_and_context_weighting det_context_the prep_of_sizes_weighting prep_of_sizes_context nn_sizes_window det_sizes_the dep_changing_estimates dobj_changing_sizes advmod_reliably_more prepc_by_probabilities_changing advmod_probabilities_reliably det_probabilities_the dobj_estimate_probabilities aux_estimate_to dep_1994_Su advmod_Chiang_1994 cc_Chiang_and nn_Chiang_Lin appos_techniques_Chiang nn_techniques_refinement nn_techniques_model dep_al._1992 nn_al._et advmod_Brown_al. appos_techniques_Brown nn_techniques_clustering num_Mercer_1983 conj_and_Bahl_Mercer conj_and_Bahl_Jelinek conj_and_interpolation_techniques conj_and_interpolation_techniques dep_interpolation_Mercer dep_interpolation_Jelinek dep_interpolation_Bahl amod_interpolation_deleted prep_such_as_techniques_techniques prep_such_as_techniques_techniques prep_such_as_techniques_interpolation nn_techniques_smoothing prep_of_use_techniques amod_use_easy xcomp_allows_estimate dobj_allows_use nn_symbols_context amod_symbols_left det_symbols_the dep_using_allows dobj_using_symbols advmod_using_explicitly prep_in_using_addition rcmod_``_using
J95-3002	J92-4003	o	Previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications -LRB- Su Chiang and Lin 1992 Chen et al. 1991 Su and Chang 1990 -RRB-	dep_Su_1990 conj_and_Su_Chang nn_1991_al. num_Chen_1991 nn_Chen_et num_Lin_1992 dep_Chiang_Chang dep_Chiang_Su conj_and_Chiang_Chen conj_and_Chiang_Lin nn_Chiang_Su dep_applications_Chen dep_applications_Lin dep_applications_Chiang prep_of_variety_applications det_variety_a prep_for_power_variety nn_power_discrimination amod_power_high dobj_provide_power aux_provide_to xcomp_able_provide cop_able_is nsubj_able_function mark_able_that amod_function_scoring det_function_this ccomp_demonstrated_able aux_demonstrated_has nsubj_demonstrated_work amod_work_Previous
J95-3002	J92-4003	o	This scoring function has been successfully applied to resolve ambiguity problems in an English-to-Chinese machine translation system -LRB- BehaviorTran -RRB- -LRB- Chen et al. 1991 -RRB- and a spoken language processing system -LRB- Su Chiang and Lin 1991 1992 -RRB-	num_Lin_1991 dep_Su_1992 conj_and_Su_Lin appos_Su_Chiang dep_system_Lin dep_system_Su nn_system_processing nn_system_language amod_system_spoken det_system_a dep_1991_al. nn_al._et num_Chen_1991 appos_system_BehaviorTran nn_system_translation nn_system_machine amod_system_English-to-Chinese det_system_an nn_problems_ambiguity prep_in_resolve_system dobj_resolve_problems aux_resolve_to conj_and_applied_system dep_applied_Chen xcomp_applied_resolve advmod_applied_successfully auxpass_applied_been aux_applied_has nsubjpass_applied_function amod_function_scoring det_function_This
J96-2003	J92-4003	o	Illustrative clusterings of this type can also be found in Pereira Tishby and Lee -LRB- 1993 -RRB- Brown Della Pietra Mercer Della Pietra and Lai -LRB- 1992 -RRB- Kneser and Ney -LRB- 1993 -RRB- and Brill et al.	nn_al._et nn_al._Brill appos_Ney_1993 appos_Lai_1992 nn_Pietra_Della nn_Pietra_Della appos_Lee_Pietra appos_Lee_Mercer appos_Lee_Pietra appos_Lee_Brown appos_Lee_1993 conj_and_Pereira_Ney conj_and_Pereira_Kneser conj_and_Pereira_Lai conj_and_Pereira_Lee conj_and_Pereira_Tishby conj_and_found_al. prep_in_found_Ney prep_in_found_Kneser prep_in_found_Lai prep_in_found_Lee prep_in_found_Tishby prep_in_found_Pereira auxpass_found_be advmod_found_also aux_found_can nsubjpass_found_clusterings det_type_this prep_of_clusterings_type amod_clusterings_Illustrative
J96-2003	J92-4003	o	Successful approaches aimed at trying to overcome the sparse data limitation include backoff -LRB- Katz 1987 -RRB- Turing-Good variants -LRB- Good 1953 Church and Gale 1991 -RRB- interpolation -LRB- Jelinek 1985 -RRB- deleted estimation -LRB- Jelinek 1985 Church and Gale 1991 -RRB- similarity-based models -LRB- Dagan Pereira and Lee 1994 Essen and Steinbiss 1992 -RRB- Pos-language models -LRB- Derouault and Merialdo 1986 -RRB- and decision tree models -LRB- Bahl et al. 1989 Black Garside and Leech 1993 Magerman 1994 -RRB-	num_Magerman_1994 num_Leech_1993 conj_and_Black_Leech conj_and_Black_Garside dep_Bahl_Magerman dep_Bahl_Leech dep_Bahl_Garside dep_Bahl_Black dep_Bahl_1989 dep_Bahl_al. nn_Bahl_et nn_models_tree nn_models_decision num_Merialdo_1986 conj_and_Derouault_Merialdo dep_models_Merialdo dep_models_Derouault amod_models_Pos-language num_Steinbiss_1992 conj_and_Essen_Steinbiss num_Lee_1994 dep_Dagan_Steinbiss dep_Dagan_Essen conj_and_Dagan_Lee conj_and_Dagan_Pereira appos_models_Lee appos_models_Pereira appos_models_Dagan amod_models_similarity-based num_Church_1991 conj_and_Church_Gale dep_Jelinek_Gale dep_Jelinek_Church num_Jelinek_1985 appos_estimation_Jelinek amod_estimation_deleted num_Jelinek_1985 appos_interpolation_Jelinek dep_Church_1991 conj_and_Church_Gale dep_1953_Gale dep_1953_Church amod_1953_Good dep_variants_1953 amod_variants_Turing-Good num_Katz_1987 appos_backoff_Bahl conj_and_backoff_models conj_and_backoff_models conj_and_backoff_models conj_and_backoff_estimation conj_and_backoff_interpolation conj_and_backoff_variants appos_backoff_Katz dobj_include_models dobj_include_models dobj_include_models dobj_include_estimation dobj_include_interpolation dobj_include_variants dobj_include_backoff nsubj_include_approaches nn_limitation_data amod_limitation_sparse det_limitation_the dobj_overcome_limitation aux_overcome_to xcomp_trying_overcome prepc_at_aimed_trying vmod_approaches_aimed amod_approaches_Successful
J96-2003	J92-4003	o	Much research has been carried out recently in this area -LRB- Hughes and Atwell 1994 Finch and Chater 1994 Redington Chater and Finch 1993 Brill et al. 1990 Kiss 1973 Pereira and Tishby 1992 Resnik 1993 Ney Essen and Kneser 1994 Matsukawa 1993 -RRB-	num_Matsukawa_1993 num_Kneser_1994 conj_and_Ney_Kneser conj_and_Ney_Essen num_Resnik_1993 num_Tishby_1992 conj_and_Pereira_Tishby dobj_Kiss_1973 dep_al._1990 nn_al._et nn_al._Brill num_Finch_1993 conj_and_Redington_Finch conj_and_Redington_Chater num_Chater_1994 conj_and_Finch_Chater num_Atwell_1994 dep_Hughes_Matsukawa dep_Hughes_Kneser dep_Hughes_Essen dep_Hughes_Ney dep_Hughes_Resnik dep_Hughes_Tishby dep_Hughes_Pereira dep_Hughes_Kiss dep_Hughes_al. dep_Hughes_Finch dep_Hughes_Chater dep_Hughes_Redington conj_and_Hughes_Chater conj_and_Hughes_Finch conj_and_Hughes_Atwell det_area_this dep_carried_Finch dep_carried_Atwell dep_carried_Hughes prep_in_carried_area advmod_carried_recently prt_carried_out auxpass_carried_been aux_carried_has nsubjpass_carried_research amod_research_Much
J96-2003	J92-4003	o	Introduction Many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings that is by using statistical language model information -LRB- Church et al. 1991 Church and Mercer 1993 Gale Church and Yarowsky 1992 Liddy and Paik 1992 -RRB-	num_Paik_1992 conj_and_Liddy_Paik num_Yarowsky_1992 conj_and_Gale_Yarowsky conj_and_Gale_Church num_Church_1993 conj_and_Church_Mercer num_al._1991 dep_Church_Paik dep_Church_Liddy dep_Church_Yarowsky dep_Church_Church dep_Church_Gale dep_Church_Mercer dep_Church_Church dep_Church_al. nn_Church_et dep_information_Church nn_information_model nn_information_language amod_information_statistical dobj_using_information pcomp_by_using ccomp_,_by dep_that_is nn_strings_word prep_of_probabilities_strings det_probabilities_the prep_about_information_probabilities dobj_incorporating_information dep_enhanced_that agent_enhanced_incorporating auxpass_enhanced_be aux_enhanced_can nsubjpass_enhanced_applications amod_language_natural dobj_process_language nsubj_process_that rcmod_applications_process amod_applications_Many nn_applications_Introduction
J96-4003	J92-4003	o	Furthermore our model is not necessarily nativist these biases may be innate but they may also be the product of some other earlier learning algorithm as the results of Ellison -LRB- 1992 -RRB- and Brown et al.	nn_al._et nn_al._Brown conj_and_Ellison_al. appos_Ellison_1992 prep_of_results_al. prep_of_results_Ellison det_results_the nn_algorithm_learning amod_algorithm_earlier amod_algorithm_other det_algorithm_some prep_as_product_results prep_of_product_algorithm det_product_the cop_product_be advmod_product_also aux_product_may nsubj_product_they conj_but_innate_product cop_innate_be aux_innate_may nsubj_innate_biases det_biases_these dep_nativist_product dep_nativist_innate advmod_nativist_necessarily neg_nativist_not cop_nativist_is nsubj_nativist_model advmod_nativist_Furthermore poss_model_our
J97-2004	J92-4003	o	Notice that most in-context and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the LLOCE -LRB- McArthur 1992 -RRB- and CILIN -LRB- Mei et al. 1993 -RRB-	nn_al._et dep_Mei_1993 advmod_Mei_al. dep_CILIN_Mei num_McArthur_1992 conj_and_LLOCE_CILIN dep_LLOCE_McArthur det_LLOCE_the prep_such_as_thesaurus_CILIN prep_such_as_thesaurus_LLOCE amod_thesaurus_typical det_thesaurus_a prep_in_category_thesaurus amod_category_same det_category_the prep_within_bounded_category auxpass_bounded_are nsubjpass_bounded_translations mark_bounded_that nn_words_source prep_of_translations_words amod_translations_dictionary amod_translations_in-context amod_translations_most conj_and_in-context_dictionary ccomp_Notice_bounded
J98-1001	J92-4003	o	Several authors -LRB- for example Krovetz and Croft \ -LSB- 1989 \ -RSB- Guthrie et al. \ -LSB- 1991 \ -RSB- Slator \ -LSB- 1992 \ -RSB- Cowie Guthrie and Guthrie \ -LSB- 1992 \ -RSB- Janssen \ -LSB- 1992 \ -RSB- Braden-Harder \ -LSB- 1993 \ -RSB- Liddy and Paik \ -LSB- 1993 \ -RSB- -RRB- have attempted to improve results by using supplementary fields of information in the electronic version of the Longman Dictionary of Contemporary English -LRB- LDOCE -RRB- in particular the box codes and subject codes provided for each sense	det_sense_each prep_for_provided_sense amod_codes_subject vmod_codes_provided conj_and_codes_codes nn_codes_box det_codes_the appos_English_LDOCE nn_English_Contemporary prep_of_Dictionary_English nn_Dictionary_Longman det_Dictionary_the prep_of_version_Dictionary amod_version_electronic det_version_the prep_of_fields_information amod_fields_supplementary prep_in_using_version dobj_using_fields prepc_by_improve_using dobj_improve_results aux_improve_to dep_attempted_codes dep_attempted_codes prep_in_attempted_particular xcomp_attempted_improve aux_attempted_have nsubj_attempted_authors num_\_1993 appos_Paik_\ num_Paik_\ num_\_1993 appos_\_\ nn_\_Braden-Harder num_\_1992 appos_Janssen_\ num_Janssen_\ num_\_1992 appos_Guthrie_\ num_Guthrie_\ num_\_1992 appos_Slator_\ num_Slator_\ num_\_1991 appos_\_\ nn_\_al. nn_\_Guthrie nn_al._et num_\_1989 appos_Croft_\ num_Croft_\ conj_and_Krovetz_Paik conj_and_Krovetz_Liddy conj_and_Krovetz_\ conj_and_Krovetz_Janssen conj_and_Krovetz_Guthrie conj_and_Krovetz_Guthrie conj_and_Krovetz_Cowie conj_and_Krovetz_Slator conj_and_Krovetz_\ conj_and_Krovetz_Croft dep_for_Paik dep_for_Liddy dep_for_\ dep_for_Janssen dep_for_Guthrie dep_for_Guthrie dep_for_Cowie dep_for_Slator dep_for_\ dep_for_Croft dep_for_Krovetz pobj_for_example dep_authors_for amod_authors_Several ccomp_``_attempted
J98-1001	J92-4003	o	Since then supervised learning from sense-tagged corpora has since been used by several researchers Zernik -LRB- 1990 1991 -RRB- Hearst -LRB- 1991 -RRB- Leacock Towell and Voorhees -LRB- 1993 -RRB- Gale Church and Yarowsky -LRB- 1992d 1993 -RRB- Bruce and Wiebe -LRB- 1994 -RRB- Miller et al.	dep_Miller_al. nn_Miller_et appos_Wiebe_1994 dep_1992d_1993 dep_Yarowsky_1992d appos_Voorhees_1993 appos_Hearst_1991 num_1991_1990 appos_Zernik_Miller conj_and_Zernik_Wiebe conj_and_Zernik_Bruce conj_and_Zernik_Yarowsky conj_and_Zernik_Church conj_and_Zernik_Gale conj_and_Zernik_Voorhees conj_and_Zernik_Towell conj_and_Zernik_Leacock conj_and_Zernik_Hearst appos_Zernik_1991 amod_researchers_several dobj_used_Wiebe dobj_used_Bruce dobj_used_Yarowsky dobj_used_Church dobj_used_Gale dobj_used_Voorhees dobj_used_Towell dobj_used_Leacock dobj_used_Hearst dobj_used_Zernik agent_used_researchers auxpass_used_been mark_used_since aux_used_has amod_corpora_sense-tagged dep_learning_used prep_from_learning_corpora amod_learning_supervised dep_Since_learning pcomp_Since_then dep_``_Since
J98-1001	J92-4003	o	A similar view underlies the class-based methods cited in Section 2.4.3 -LRB- Brown et al. 1992 Pereira and Tishby 1992 Pereira Tishby and Lee 1993 -RRB-	num_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby num_Tishby_1992 conj_and_Pereira_Tishby dep_Brown_Lee dep_Brown_Tishby dep_Brown_Pereira dep_Brown_Tishby dep_Brown_Pereira dep_Brown_1992 dep_Brown_al. nn_Brown_et num_Section_2.4.3 prep_in_cited_Section vmod_methods_cited amod_methods_class-based det_methods_the dep_underlies_Brown dobj_underlies_methods nsubj_underlies_view amod_view_similar det_view_A ccomp_``_underlies
J98-1004	J92-4003	o	This set of context vectors is then clustered into a predetermined number of coherent clusters or context groups using Buckshot -LRB- Cutting et al. 1992 -RRB- a combination of the EM algorithm and agglomerative clustering	amod_clustering_agglomerative conj_and_algorithm_clustering nn_algorithm_EM det_algorithm_the prep_of_combination_clustering prep_of_combination_algorithm det_combination_a nn_al._et tmod_Cutting_1992 dobj_Cutting_al. dobj_using_Buckshot nn_groups_context conj_or_clusters_groups amod_clusters_coherent vmod_number_using prep_of_number_groups prep_of_number_clusters amod_number_predetermined det_number_a npadvmod_clustered_combination dep_clustered_Cutting prep_into_clustered_number advmod_clustered_then auxpass_clustered_is nsubjpass_clustered_set nn_vectors_context prep_of_set_vectors det_set_This
J98-1004	J92-4003	o	Regardless of whether it takes the form of dictionaries -LRB- Lesk 1986 Guthrie et al. 1991 Dagan Itai and Schwall 1991 Karov and Edelman 1996 -RRB- thesauri -LRB- Yarowsky 1992 Walker and Amsler 1986 -RRB- bilingual corpora -LRB- Brown et al. 1991 Church and Gale 1991 -RRB- or hand-labeled training sets -LRB- Hearst 1991 Leacock Towell and Voorhees 1993 Niwa and Nitta 1994 Bruce and Wiebe 1994 -RRB- providing information for sense definitions can be a considerable burden	amod_burden_considerable det_burden_a cop_burden_be aux_burden_can nsubj_burden_sets nsubj_burden_Brown nn_definitions_sense prep_for_providing_definitions dobj_providing_information num_Wiebe_1994 conj_and_Bruce_Wiebe num_Nitta_1994 conj_and_Niwa_Nitta num_Voorhees_1993 conj_and_Leacock_Voorhees conj_and_Leacock_Towell dep_Hearst_Wiebe dep_Hearst_Bruce dep_Hearst_Nitta dep_Hearst_Niwa dep_Hearst_Voorhees dep_Hearst_Towell dep_Hearst_Leacock num_Hearst_1991 appos_sets_Hearst nn_sets_training amod_sets_hand-labeled num_Church_1991 conj_and_Church_Gale num_al._1991 nn_al._et vmod_Brown_providing conj_or_Brown_sets dep_Brown_Gale dep_Brown_Church tmod_Brown_al. dep_corpora_burden amod_corpora_bilingual num_Amsler_1986 conj_and_Walker_Amsler dep_Yarowsky_Amsler dep_Yarowsky_Walker num_Yarowsky_1992 appos_thesauri_Yarowsky dep_Karov_1996 conj_and_Karov_Edelman num_Schwall_1991 conj_and_Dagan_Schwall conj_and_Dagan_Itai dep_al._1991 nn_al._et nn_al._Guthrie dep_Lesk_Edelman dep_Lesk_Karov dep_Lesk_Schwall dep_Lesk_Itai dep_Lesk_Dagan dep_Lesk_al. num_Lesk_1986 appos_dictionaries_corpora appos_dictionaries_thesauri appos_dictionaries_Lesk prep_of_form_dictionaries det_form_the dobj_takes_form nsubj_takes_it mark_takes_whether prepc_of_Regardless_takes ccomp_``_Regardless
J98-1004	J92-4003	o	Another body of related work is the literature on word clustering in computational linguistics -LRB- Brown et al. 1992 Finch 1993 Pereira Tishby and Lee 1993 Grefenstette 1994a -RRB- and document clustering in information retrieval -LRB- van Rijsbergen 1979 Willett 1988 Sparck-Jones 1991 Cutting et al. 1992 -RRB-	nn_al._et tmod_Cutting_1992 dobj_Cutting_al. amod_1991_Sparck-Jones num_Willett_1988 dep_Rijsbergen_Cutting dep_Rijsbergen_1991 dep_Rijsbergen_Willett num_Rijsbergen_1979 nn_Rijsbergen_van dep_retrieval_Rijsbergen nn_retrieval_information prep_in_clustering_retrieval nn_clustering_document nn_1994a_Grefenstette num_Lee_1993 conj_and_Pereira_Lee conj_and_Pereira_Tishby num_Finch_1993 dep_Brown_1994a dep_Brown_Lee dep_Brown_Tishby dep_Brown_Pereira dep_Brown_Finch dep_Brown_1992 dep_Brown_al. nn_Brown_et amod_linguistics_computational nn_clustering_word conj_and_literature_clustering dep_literature_Brown prep_in_literature_linguistics prep_on_literature_clustering det_literature_the cop_literature_is nsubj_literature_body amod_work_related prep_of_body_work det_body_Another
J98-2002	J92-4003	o	The second approach -LRB- Sekine et al. 1992 Chang Luo and Su 1992 Resnik 1993a Grishman and Sterling 1994 Alshawi and Carter 1994 -RRB- takes triples -LRB- verb prep noun2 -RRB- and -LRB- nounl prep noun2 -RRB- like those in Table 10 as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples	nn_disambiguation_PP-attachment prep_on_performs_quadruples dobj_performs_disambiguation amod_knowledge_semantic dobj_acquiring_knowledge prepc_for_data_acquiring nn_data_training num_Table_10 prep_in_those_Table appos_nounl_noun2 appos_nounl_prep conj_and_verb_performs prep_as_verb_data prep_like_verb_those conj_and_verb_nounl dep_verb_noun2 dobj_verb_prep dep_takes_performs dep_takes_nounl dep_takes_verb dobj_takes_triples nsubj_takes_approach num_Carter_1994 conj_and_Alshawi_Carter num_Sterling_1994 conj_and_Grishman_Sterling nn_1993a_Resnik num_Su_1992 conj_and_Chang_Su conj_and_Chang_Luo dep_al._1992 nn_al._et dep_Sekine_Carter dep_Sekine_Alshawi conj_Sekine_Sterling conj_Sekine_Grishman conj_Sekine_1993a dep_Sekine_Su dep_Sekine_Luo dep_Sekine_Chang advmod_Sekine_al. appos_approach_Sekine amod_approach_second det_approach_The ccomp_``_takes
J98-2002	J92-4003	o	It is potentially useful in other natural language processing tasks such as the problem of estimating n-gram models -LRB- Brown et al. 1992 -RRB- or the problem of semantic tagging -LRB- Cucchiarelli and Velardi 1997 -RRB-	num_Velardi_1997 conj_and_Cucchiarelli_Velardi dep_tagging_Velardi dep_tagging_Cucchiarelli amod_tagging_semantic prep_of_problem_tagging det_problem_the nn_1992_al. num_Brown_1992 nn_Brown_et nn_models_n-gram dobj_estimating_models prepc_of_problem_estimating det_problem_the prep_such_as_tasks_problem nn_tasks_processing nn_tasks_language amod_tasks_natural amod_tasks_other conj_or_useful_problem dep_useful_Brown prep_in_useful_tasks advmod_useful_potentially cop_useful_is nsubj_useful_It
J99-4003	J92-4003	o	3 2.4 Intonation Annotations For our intonation annotation we have annotated the intonational phrase boundaries using the ToBI -LRB- Tones and Break Indices -RRB- definition -LRB- Silverman et al. 1992 -RRB-	advmod_1992_al. nn_al._et num_Silverman_1992 nn_definition_Indices amod_definition_Break dep_Tones_Silverman conj_and_Tones_definition dep_ToBI_definition dep_ToBI_Tones det_ToBI_the dobj_using_ToBI nn_boundaries_phrase amod_boundaries_intonational det_boundaries_the amod_boundaries_annotated vmod_have_using dobj_have_boundaries nsubj_have_we ccomp_have_Annotations nn_annotation_intonation poss_annotation_our prep_for_Annotations_annotation nn_Annotations_Intonation num_Annotations_2.4 num_Annotations_3
J99-4003	J92-4003	o	-LRB- 1992 -RRB- and Magerman -LRB- 1994 -RRB- used the clustering algorithm of Brown et al.	dep_Brown_al. nn_Brown_et prep_of_algorithm_Brown nn_algorithm_clustering det_algorithm_the dobj_used_algorithm nsubj_used_Magerman nsubj_used_1992 appos_Magerman_1994 conj_and_1992_Magerman
J99-4003	J92-4003	o	For handling word identities one could follow the approach used for handling the POS tags -LRB- e.g. Black et al. 1992 Magerman 1994 -RRB- and view the POS tags and word identities as two separate sources of information	prep_of_sources_information amod_sources_separate num_sources_two nn_identities_word conj_and_tags_identities nn_tags_POS det_tags_the prep_as_view_sources dobj_view_identities dobj_view_tags nsubj_view_approach num_Magerman_1994 num_al._1992 nn_al._et nn_al._Black dep_e.g._Magerman conj_e.g._al. nn_tags_POS det_tags_the dep_handling_e.g. dobj_handling_tags conj_and_used_view prepc_for_used_handling nsubj_used_approach det_approach_the ccomp_follow_view ccomp_follow_used aux_follow_could nsubj_follow_one prepc_for_follow_handling nn_identities_word dobj_handling_identities
N03-1032	J92-4003	o	In information retrieval word similarity can be used to identify terms for pseudo-relevance feedback -LRB- Harman 1992 Buckley et al. 1995 Xu and Croft 2000 Vechtomova and Robertson 2000 -RRB-	amod_Xu_2000 conj_and_Xu_Robertson conj_and_Xu_Vechtomova conj_and_Xu_2000 conj_and_Xu_Croft num_Buckley_1995 nn_Buckley_al. nn_Buckley_et dep_Harman_Robertson dep_Harman_Vechtomova dep_Harman_2000 dep_Harman_Croft dep_Harman_Xu conj_Harman_Buckley conj_Harman_1992 dep_feedback_Harman nn_feedback_pseudo-relevance prep_for_identify_feedback dobj_identify_terms aux_identify_to xcomp_used_identify auxpass_used_be aux_used_can nsubjpass_used_similarity prep_in_used_retrieval nn_similarity_word nn_retrieval_information
N04-4034	J92-4003	o	A re nement of this model is the class-based n-gram where the words are partitioned into equivalence classes -LRB- Brown et al. 1992 -RRB-	amod_Brown_1992 dep_Brown_al. nn_Brown_et amod_classes_equivalence prep_into_partitioned_classes auxpass_partitioned_are nsubjpass_partitioned_words advmod_partitioned_where det_words_the dep_n-gram_Brown rcmod_n-gram_partitioned amod_n-gram_class-based det_n-gram_the cop_n-gram_is nsubj_n-gram_nement det_model_this prep_of_nement_model nn_nement_re det_nement_A
N04-4034	J92-4003	o	In addition we developed a word clustering procedure -LRB- based on a standard approach -LRB- Brown et al. 1992 -RRB- -RRB- that optimizes conditional word clusters	nn_clusters_word amod_clusters_conditional dobj_optimizes_clusters nsubj_optimizes_that num_al._1992 nn_al._et amod_al._Brown dep_approach_al. amod_approach_standard det_approach_a rcmod_procedure_optimizes prep_based_on_procedure_approach nn_procedure_clustering nn_procedure_word det_procedure_a dobj_developed_procedure nsubj_developed_we prep_in_developed_addition
N04-4034	J92-4003	o	Table 2 Three types of class-based MSLMs on Switchboard-I -LRB- swbd -RRB- and ICSI Meeting -LRB- mr -RRB- corpora # of swbd mr classes BROWN MMI MCMI BROWN MMI MCMI 100 68.9 0.3 68.4 0.3 68.2 0.3 78.9 3.0 77.3 2.8 76.8 2.8 500 68.9 0.3 68.3 0.3 67.9 0.3 78.7 3.1 77.1 2.8 76.7 2.8 1000 68.9 0.3 68.2 0.3 67.9 0.3 79.0 3.1 77.2 2.7 76.9 2.8 1500 69.0 0.3 68.2 0.3 68.0 0.3 79.6 3.1 77.4 2.7 77.4 2.7 2000 69.0 0.3 68.3 0.3 68.0 0.3 80.1 3.1 77.6 2.7 77.9 2.7 jV j 68.5 0.3 78.3 2.7 Table 3 Class-based MSLM on Switchboard Eval-2003 size 10050010001500 2000 jV j 3-gram 4-gram ppl 65.8 65.5 65.6 65.7 66.1 67.9 72.1 76.3 % reduction 8.6 8.9 8.8 8.7 8.3 5.8 0 -5.8 Class-based language models -LRB- Brown et al. 1992 Whittaker and Woodland 2003 -RRB- yield great bene ts when data sparseness abounds	nsubj_abounds_sparseness advmod_abounds_when nn_sparseness_data rcmod_ts_abounds nn_ts_bene amod_ts_great nn_ts_yield dep_Whittaker_2003 conj_and_Whittaker_Woodland dep_Brown_Woodland dep_Brown_Whittaker amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_models_ts appos_models_Brown nn_models_language amod_models_Class-based num_models_-5.8 num_models_0 dep_models_5.8 num_models_8.9 number_5.8_8.3 dep_5.8_8.7 number_8.7_8.8 number_8.9_8.6 dep_reduction_models amod_reduction_% num_reduction_67.9 number_%_76.3 number_%_72.1 appos_66.1_reduction dep_65.7_66.1 number_65.7_65.6 dep_65.5_65.7 number_65.5_65.8 dep_ppl_65.5 nn_ppl_4-gram amod_ppl_3-gram nn_ppl_j nn_ppl_jV num_ppl_2000 number_2000_10050010001500 dep_size_ppl nn_size_Eval-2003 nn_size_Switchboard prep_on_MSLM_size amod_MSLM_Class-based num_Table_3 num_Table_2.7 num_Table_78.3 number_78.3_0.3 number_78.3_68.5 dep_j_MSLM dep_j_Table dep_jV_j dep_2.7_jV dep_77.9_2.7 number_77.9_2.7 dep_77.9_77.6 dep_77.9_68.0 dep_77.9_69.0 dep_77.9_2.7 dep_77.9_0.3 dep_77.9_0.3 dep_77.9_76.9 dep_77.9_79.0 dep_77.9_68.2 dep_77.9_2.8 dep_77.9_3.1 dep_77.9_0.3 dep_77.9_500 dep_77.9_77.3 dep_77.9_68.2 dep_77.9_68.9 number_77.6_3.1 dep_77.6_80.1 number_80.1_0.3 number_68.0_0.3 dep_68.0_68.3 number_68.3_0.3 number_69.0_2000 dep_69.0_2.7 number_2.7_77.4 number_2.7_77.4 dep_2.7_3.1 number_3.1_79.6 number_0.3_68.0 dep_0.3_0.3 number_0.3_68.2 number_0.3_69.0 dep_0.3_1500 number_1500_2.8 number_76.9_2.7 dep_76.9_77.2 number_77.2_3.1 number_79.0_0.3 dep_79.0_67.9 number_67.9_0.3 number_68.2_0.3 dep_68.2_68.9 number_68.9_1000 number_2.8_76.7 dep_2.8_2.8 number_2.8_77.1 number_3.1_78.7 dep_3.1_0.3 number_0.3_67.9 number_0.3_68.3 dep_0.3_0.3 number_0.3_68.9 number_500_2.8 dep_500_76.8 number_76.8_2.8 number_77.3_3.0 dep_77.3_78.9 number_78.9_0.3 number_68.2_0.3 dep_68.2_68.4 number_68.4_0.3 number_68.9_100 dep_MCMI_77.9 nn_MCMI_MMI nn_MCMI_BROWN nn_MCMI_MCMI nn_MCMI_MMI nn_MCMI_BROWN dep_classes_MCMI dep_mr_classes dep_swbd_mr prep_of_#_swbd dep_corpora_# dep_mr_corpora dobj_Meeting_mr vmod_ICSI_Meeting conj_and_Switchboard-I_ICSI appos_Switchboard-I_swbd amod_MSLMs_class-based prep_on_types_ICSI prep_on_types_Switchboard-I prep_of_types_MSLMs num_types_Three dep_Table_types num_Table_2
N04-4034	J92-4003	o	SRILM -LRB- Stolcke 2002 -RRB- can produce classes to maximize the mutual information between the classes I -LRB- C -LRB- wt -RRB- C -LRB- wt 1 -RRB- -RRB- as described in -LRB- Brown et al. 1992 -RRB-	num_al._1992 nn_al._et amod_al._Brown dep_in_al. prep_described_in mark_described_as dep_wt_1 advcl_C_described appos_C_wt dep_C_C appos_C_wt dep_I_C dep_classes_I det_classes_the prep_between_information_classes amod_information_mutual det_information_the dobj_maximize_information aux_maximize_to xcomp_produce_maximize dobj_produce_classes aux_produce_can nsubj_produce_SRILM amod_Stolcke_2002 dep_SRILM_Stolcke
N04-4034	J92-4003	o	To compare different clustering algorithms results with the standard method of -LRB- Brown et al. 1992 -RRB- -LRB- SRILMs ngram-class -RRB- are also reported	advmod_reported_also auxpass_reported_are nsubjpass_reported_SRILMs dep_reported_al. amod_SRILMs_ngram-class num_al._1992 nn_al._et amod_al._Brown prepc_of_method_reported nn_method_standard det_method_the prep_with_results_method advcl_results_compare nn_algorithms_clustering amod_algorithms_different dobj_compare_algorithms aux_compare_To
N06-1058	J92-4003	o	4.1.3 Alternative Paraphrasing Techniques To investigate the effect of paraphrase quality on automatic evaluation we consider two alternative paraphrasing resources Latent Semantic Analysis -LRB- LSA -RRB- and Brown clustering -LRB- Brown et al. 1992 -RRB-	dep_al._1992 nn_al._et amod_al._Brown dep_clustering_al. nn_clustering_Brown conj_and_Analysis_clustering appos_Analysis_LSA amod_Analysis_Semantic amod_Analysis_Latent amod_resources_paraphrasing amod_resources_alternative num_resources_two dep_consider_clustering dep_consider_Analysis dobj_consider_resources nsubj_consider_we nsubj_consider_Techniques amod_evaluation_automatic amod_quality_paraphrase prep_on_effect_evaluation prep_of_effect_quality det_effect_the dobj_investigate_effect aux_investigate_To vmod_Techniques_investigate amod_Techniques_Paraphrasing amod_Techniques_Alternative num_Techniques_4.1.3
N06-2001	J92-4003	o	This can also be interpreted as a generalization of standard class-based models -LRB- Brown et al. 1992 -RRB-	amod_Brown_1992 dep_Brown_al. nn_Brown_et amod_models_class-based amod_models_standard prep_of_generalization_models det_generalization_a dep_interpreted_Brown prep_as_interpreted_generalization auxpass_interpreted_be advmod_interpreted_also aux_interpreted_can nsubjpass_interpreted_This
N09-1051	J92-4003	o	-LRB- 4 -RRB- can be used to motivate a novel class-based language model and a regularized version of minimum discrimination information -LRB- MDI -RRB- models -LRB- Della Pietra et al. 1992 -RRB-	amod_Pietra_1992 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della nn_models_information appos_information_MDI nn_information_discrimination amod_information_minimum dep_version_Pietra prep_of_version_models amod_version_regularized det_version_a conj_and_model_version nn_model_language amod_model_class-based amod_model_novel det_model_a dobj_motivate_version dobj_motivate_model aux_motivate_to xcomp_used_motivate auxpass_used_be aux_used_can dep_used_4
N09-1051	J92-4003	o	We consider three class models models S M and L defined as pS -LRB- cj | c1cj1 w1wj1 -RRB- = png -LRB- cj | cj2cj1 -RRB- pS -LRB- wj | c1cj w1wj1 -RRB- = png -LRB- wj | cj -RRB- pM -LRB- cj | c1cj1 w1wj1 -RRB- = png -LRB- cj | cj2cj1 wj2wj1 -RRB- pM -LRB- wj | c1cj w1wj1 -RRB- = png -LRB- wj | wj2wj1cj -RRB- pL -LRB- cj | c1cj1 w1wj1 -RRB- = png -LRB- cj | wj2cj2wj1cj1 -RRB- pL -LRB- wj | c1cj w1wj1 -RRB- = png -LRB- wj | wj2cj2wj1cj1cj -RRB- Model S is an exponential version of the class-based n-gram model from -LRB- Brown et al. 1992 -RRB- model M is a novel model introduced in -LRB- Chen 2009 -RRB- and model L is an exponential version of the model indexpredict from -LRB- Goodman 2001 -RRB-	amod_Goodman_2001 dep_from_Goodman nn_indexpredict_model det_indexpredict_the prep_version_from prep_of_version_indexpredict amod_version_exponential det_version_an cop_version_is nsubj_version_L nn_L_model dep_Chen_2009 prep_in_introduced_Chen vmod_model_introduced amod_model_novel det_model_a cop_model_is nsubj_model_M nn_M_model num_al._1992 nn_al._et amod_al._Brown dep_from_al. prep_model_from nn_model_n-gram amod_model_class-based det_model_the prep_of_version_model amod_version_exponential det_version_an cop_version_is nsubj_version_pS dep_version_cj2cj1 nn_version_cj nn_version_png amod_version_= nn_version_pS nn_S_Model dep_S_wj2cj2wj1cj1cj nn_S_wj nn_S_png amod_S_= nn_S_pL dep_S_wj2cj2wj1cj1 nn_S_cj nn_S_png amod_S_= nn_S_pL dep_S_wj2wj1cj nn_S_wj nn_S_png amod_S_= nn_S_pM num_wj2cj2wj1cj1cj_| appos_c1cj_w1wj1 num_c1cj_| nn_c1cj_wj dep_pL_c1cj num_wj2cj2wj1cj1_| appos_c1cj1_w1wj1 num_c1cj1_| nn_c1cj1_cj dep_pL_c1cj1 num_wj2wj1cj_| appos_c1cj_w1wj1 num_c1cj_| nn_c1cj_wj dep_pM_c1cj nn_pM_png appos_cj2cj1_wj2wj1 num_cj2cj1_| nn_cj2cj1_cj dep_png_cj2cj1 dep_=_S appos_c1cj1_w1wj1 num_c1cj1_| nn_c1cj1_cj amod_pM_= dep_pM_c1cj1 nn_pM_png num_cj_| nn_cj_wj appos_png_cj dep_=_pM appos_c1cj_w1wj1 num_c1cj_| nn_c1cj_wj amod_pS_= dep_pS_c1cj num_cj2cj1_| appos_c1cj1_w1wj1 num_c1cj1_| nn_c1cj1_cj dep_pS_c1cj1 prep_as_defined_version conj_and_S_L conj_and_S_M dep_models_L dep_models_M dep_models_S vmod_models_defined appos_models_models nn_models_class num_models_three conj_and_consider_version parataxis_consider_model dobj_consider_models nsubj_consider_We
N09-1051	J92-4003	o	4.2 Models with Prior Distributions Minimum discrimination information models -LRB- Della Pietra et al. 1992 -RRB- are exponential models with a prior distribution q -LRB- y | x -RRB- p -LRB- y | x -RRB- = q -LRB- y | x -RRB- exp -LRB- summationtextF i = 1 ifi -LRB- x y -RRB- -RRB- Z -LRB- x -RRB- -LRB- 14 -RRB- The central issue in performance prediction for MDI models is whether q -LRB- y | x -RRB- needs to be accounted for	prep_accounted_for auxpass_accounted_be aux_accounted_to xcomp_needs_accounted nsubj_needs_q mark_needs_whether num_x_| nn_x_y appos_q_x ccomp_is_needs nsubj_is_p nn_models_MDI prep_for_prediction_models nn_prediction_performance amod_issue_central det_issue_The nn_issue_Z appos_Z_14 appos_Z_x nn_Z_exp appos_x_y dep_ifi_x num_ifi_1 dep_=_ifi dep_i_= dep_summationtextF_i appos_exp_summationtextF nn_exp_y nn_exp_q num_x_| appos_y_x dep_=_issue num_x_| nn_x_y prep_in_p_prediction amod_p_= appos_p_x num_x_| nn_x_y appos_q_x nn_q_distribution amod_q_prior det_q_a parataxis_models_is prep_with_models_q amod_models_exponential cop_models_are nsubj_models_Models amod_Pietra_1992 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della dep_models_Pietra nn_models_information nn_models_discrimination nn_models_Minimum nn_models_Distributions amod_models_Prior prep_with_Models_models num_Models_4.2
N09-1051	J92-4003	o	The most popular non-data-splitting methods for predicting test set cross-entropy -LRB- or likelihood -RRB- are AIC and variants such as AICc quasi-AIC -LRB- QAIC -RRB- and QAICc -LRB- Akaike 1973 Hurvich and Tsai 1989 Lebreton et al. 1992 -RRB-	num_Lebreton_1992 nn_Lebreton_al. nn_Lebreton_et dep_Hurvich_Lebreton conj_and_Hurvich_1989 conj_and_Hurvich_Tsai dep_Akaike_1989 dep_Akaike_Tsai dep_Akaike_Hurvich appos_Akaike_1973 dep_QAICc_Akaike appos_quasi-AIC_QAIC conj_and_AICc_QAICc conj_and_AICc_quasi-AIC prep_such_as_variants_QAICc prep_such_as_variants_quasi-AIC prep_such_as_variants_AICc conj_and_AIC_variants cop_AIC_are nsubj_AIC_methods cc_likelihood_or dep_cross-entropy_likelihood dobj_set_cross-entropy nsubj_set_test ccomp_predicting_set prepc_for_methods_predicting amod_methods_non-data-splitting amod_methods_popular det_methods_The advmod_popular_most ccomp_``_variants ccomp_``_AIC
N09-1053	J92-4003	o	We compare the following model types conventional -LRB- i.e. non-exponential -RRB- word n-gram models conventional IBM class n-gram models interpolated with conventional word n-gram models -LRB- Brown et al. 1992 -RRB- and model M. All conventional n-gram models are smoothed with modified Kneser-Ney smoothing -LRB- Chen and Goodman 1998 -RRB- except we also evaluate word n-gram models with Katz smoothing -LRB- Katz 1987 -RRB-	amod_Katz_1987 dep_smoothing_Katz nn_smoothing_Katz nn_models_n-gram nn_models_word prep_with_evaluate_smoothing dobj_evaluate_models advmod_evaluate_also nsubj_evaluate_we mark_evaluate_except dep_Chen_1998 conj_and_Chen_Goodman appos_smoothing_Goodman appos_smoothing_Chen nn_smoothing_Kneser-Ney amod_smoothing_modified advcl_smoothed_evaluate prep_with_smoothed_smoothing auxpass_smoothed_are nsubjpass_smoothed_models nsubjpass_smoothed_models nsubjpass_smoothed_models nn_models_n-gram amod_models_conventional det_models_All nn_models_M. nn_models_model amod_Brown_1992 dep_Brown_al. nn_Brown_et appos_models_Brown nn_models_n-gram nn_models_word amod_models_conventional prep_with_interpolated_models vmod_models_interpolated nn_models_n-gram nn_models_class nn_models_IBM amod_models_conventional conj_and_models_models conj_and_models_models nn_models_n-gram nn_models_word amod_models_conventional dep_i.e._non-exponential dep_conventional_i.e. nn_types_model amod_types_following det_types_the parataxis_compare_smoothed dobj_compare_types nsubj_compare_We
N09-1053	J92-4003	o	While we can only compare class models with word models on the largest training set for this training set model M outperforms the baseline Katzsmoothed word trigram model by 1.9 % absolute .6 4 Domain Adaptation In this section we introduce another heuristic for improving exponential models and show how this heuristic can be used to motivate a regularized version of minimum discrimination information -LRB- MDI -RRB- models -LRB- Della Pietra et al. 1992 -RRB-	amod_Pietra_1992 dep_Pietra_al. nn_Pietra_et nn_Pietra_Della nn_models_information appos_information_MDI nn_information_discrimination amod_information_minimum dep_version_Pietra prep_of_version_models amod_version_regularized det_version_a dobj_motivate_version aux_motivate_to xcomp_used_motivate auxpass_used_be aux_used_can nsubjpass_used_heuristic advmod_used_how det_heuristic_this ccomp_show_used nsubj_show_we amod_models_exponential dobj_improving_models det_heuristic_another conj_and_introduce_show prepc_for_introduce_improving dobj_introduce_heuristic nsubj_introduce_we det_section_this prep_in_Adaptation_section nn_Adaptation_Domain num_Adaptation_4 num_Adaptation_.6 amod_Adaptation_absolute dep_%_Adaptation num_%_1.9 nn_model_trigram nn_model_word nn_model_Katzsmoothed nn_model_baseline det_model_the parataxis_outperforms_show parataxis_outperforms_introduce prep_by_outperforms_% dobj_outperforms_model nsubj_outperforms_M nsubj_outperforms_training mark_outperforms_for advcl_outperforms_compare nn_M_model amod_M_set det_training_this nn_set_training amod_set_largest det_set_the prep_on_models_set nn_models_word nn_models_class prep_with_compare_models dobj_compare_models advmod_compare_only aux_compare_can nsubj_compare_we mark_compare_While
P01-1046	J92-4003	o	-LRB- 1999 -RRB- and Lee -LRB- 1999 -RRB- -RRB- can be generally divided into three types discounting -LRB- Katz 1987 -RRB- class-based smoothing -LRB- Resnik 1993 Brown et al. 1992 Pereira et al. 1993 -RRB- and distance-weighted averaging -LRB- Grishman and Sterling 1994 Dagan et al. 1999 -RRB-	num_Dagan_1999 nn_Dagan_al. nn_Dagan_et dep_Grishman_Dagan conj_and_Grishman_1994 conj_and_Grishman_Sterling dep_averaging_1994 dep_averaging_Sterling dep_averaging_Grishman amod_averaging_distance-weighted num_Pereira_1993 nn_Pereira_al. nn_Pereira_et num_Brown_1992 nn_Brown_al. nn_Brown_et dep_Resnik_Pereira dep_Resnik_Brown dep_Resnik_1993 appos_smoothing_Resnik amod_smoothing_class-based dep_Katz_1987 conj_and_discounting_averaging conj_and_discounting_smoothing dep_discounting_Katz num_types_three dep_divided_averaging dep_divided_smoothing dep_divided_discounting prep_into_divided_types advmod_divided_generally auxpass_divided_be aux_divided_can nsubjpass_divided_Lee nsubjpass_divided_1999 appos_Lee_1999 conj_and_1999_Lee
P01-1046	J92-4003	o	Classes can be induced directly from the corpus -LRB- Pereira et al. 1993 Brown et al. 1992 -RRB- or taken from a manually crafted taxonomy -LRB- Resnik 1993 -RRB-	amod_Resnik_1993 dep_taxonomy_Resnik amod_taxonomy_crafted det_taxonomy_a advmod_crafted_manually prep_from_taken_taxonomy num_Brown_1992 nn_Brown_al. nn_Brown_et conj_or_Pereira_taken dep_Pereira_Brown appos_Pereira_1993 dep_Pereira_al. nn_Pereira_et det_corpus_the dep_induced_taken dep_induced_Pereira prep_from_induced_corpus advmod_induced_directly auxpass_induced_be aux_induced_can nsubjpass_induced_Classes
P01-1068	J92-4003	o	And we consider that word pairs that have a small distance between vectors also have similar word neighboring characteristics -LRB- Brown et al. 1992 -RRB- -LRB- Bai et al. 1998 -RRB-	amod_Bai_1998 dep_Bai_al. nn_Bai_et amod_Brown_1992 dep_Brown_al. nn_Brown_et dep_characteristics_Bai dep_characteristics_Brown amod_characteristics_neighboring dep_word_characteristics amod_word_similar dobj_have_word prep_between_distance_vectors amod_distance_small det_distance_a dobj_have_distance nsubj_have_that rcmod_pairs_have nn_pairs_word det_pairs_that parataxis_consider_have advmod_consider_also dobj_consider_pairs nsubj_consider_we cc_consider_And
P02-1016	J92-4003	o	Words are encoded through an automatic clustering algorithm -LRB- Brown et al. 1992 -RRB- while tags labels and extensions are normally encoded using diagonal bits	amod_bits_diagonal dobj_using_bits xcomp_encoded_using advmod_encoded_normally auxpass_encoded_are nsubjpass_encoded_extensions nsubjpass_encoded_labels nsubjpass_encoded_tags mark_encoded_while conj_and_tags_extensions conj_and_tags_labels dep_al._1992 nn_al._et amod_al._Brown nn_algorithm_clustering amod_algorithm_automatic det_algorithm_an advcl_encoded_encoded dep_encoded_al. prep_through_encoded_algorithm auxpass_encoded_are nsubjpass_encoded_Words