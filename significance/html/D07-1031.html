<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
D07-1031 <div class="dstPaperTitle">Why Doesn't EM Find Good HMM POS-Taggers?</div><div class="dstPaperAuthors">Johnson, Mark;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1008
Weakly Supervised Supertagging with Grammar-Informed Initialization
Baldridge, Jason;"></td>
	<td class="line x" title="1:175	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 5764 Manchester, August 2008 Weakly supervised supertagging with grammar-informed initialization Jason Baldridge Department of Linguistics The University of Texas at Austin jbaldrid@mail.utexas.edu Abstract Muchpreviousworkhasinvestigatedweak supervision with HMMs and tag dictionaries for part-of-speech tagging, but there have been no similar investigations for the harder problem of supertagging." ></td>
	<td class="line x" title="2:175	Here, I show that weak supervision for supertagging does work, but that it is subject to severe performance degradation when the tag dictionary is highly ambiguous." ></td>
	<td class="line x" title="3:175	I show that lexical category complexity and information about how supertags may combine syntactically can be used to initialize the transition distributions of a first-order Hidden Markov Model for weakly supervised learning." ></td>
	<td class="line x" title="4:175	This initialization proves more effective than starting with uniform transitions, especially when the tag dictionary is highly ambiguous." ></td>
	<td class="line x" title="5:175	1 Introduction Supertagging involves assigning words lexical entries based on a lexicalized grammatical theory, such as Combinatory Categorial Grammar (CCG) (Steedman, 2000) Tree-adjoining Grammar (Joshi, 1988), or Head-driven Phrase Structure Grammar (Pollard and Sag, 1994)." ></td>
	<td class="line x" title="6:175	Supertag sets are larger than part-of-speech (POS) tag sets and their elements are generally far more articulated." ></td>
	<td class="line x" title="7:175	For example, the English verb join has the POS VB and the CCG category ((Sb\NP)/PP)/NP in CCGbank (Hockenmaier and Steedman, 2007)." ></td>
	<td class="line x" title="8:175	This category indicates that join requires a noun phrase c2008." ></td>
	<td class="line x" title="9:175	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:175	Some rights reserved." ></td>
	<td class="line x" title="11:175	to its left, another to its right, and a prepositional phrase to the right of that." ></td>
	<td class="line x" title="12:175	Supertags convey such detailed syntactic subcategorization information that supertag disambiguation is referred to as almost parsing (Bangalore and Joshi, 1999)." ></td>
	<td class="line x" title="13:175	Standard sequence prediction models are highly effective for supertagging, including Hidden Markov Models (Bangalore and Joshi, 1999; Nielsen, 2002), Maximum Entropy Markov Models (Clark, 2002; Hockenmaier et al., 2004; Clark and Curran, 2007), and Conditional Random Fields (Blunsom and Baldwin, 2006)." ></td>
	<td class="line x" title="14:175	The original motivation for supertagsparse prefiltering for lexicalized grammarsof Bangalore and Joshi (1999) has been realized to good effect: the supertagger of Clark and Curran (2007) provides stagedn-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy." ></td>
	<td class="line x" title="15:175	Espinosa et al.(2008) have shown that hypertagging (predicting the supertag associated with a logical form) can improve both speed and accuracy of wide-coverage sentence realization with CCG." ></td>
	<td class="line x" title="17:175	Supertags have gained further relevance as they are increasingly used as features for other tasks, including machine translation (Birch et al., 2007; Hassan et al., 2007)." ></td>
	<td class="line x" title="18:175	Supertaggers typically rely on a significant amount of carefully annotated sentences." ></td>
	<td class="line x" title="19:175	As with many problems, there is pressing need to find strategies for reducing the amount of supervision required for producing accurate supertaggers, but as yet, no one has explored the use of weak supervision for the task." ></td>
	<td class="line x" title="20:175	In particular, there are many dialog systems which rely on hand-crafted lexicons that both provide a starting point for bootstrapping a supertagger and which could benefit greatly from supertag pre-parse filter." ></td>
	<td class="line x" title="21:175	For example, the dialog system used by Kruijff et al.(2007) uses a hand57 crafted CCG grammar for OpenCCG (White and Baldridge, 2003)." ></td>
	<td class="line x" title="23:175	Itisimportanttostressthatthere are many such uses of CCG and related frameworks which do not rely on first annotating (even a small number of) sentences in a corpus: these define a lexicon that maps from words to categories (supertags) for a particular domain/application." ></td>
	<td class="line x" title="24:175	This scenario is a natural fit for learning taggers from tag dictionaries using hidden Markov models with Expectation-Maximization (EM)." ></td>
	<td class="line x" title="25:175	Here, I investigate such weakly supervised learning for supertagging and demonstrate the importance of proper initialization of the tag transition distributions of the HMM." ></td>
	<td class="line x" title="26:175	In particular, such initialization can be done using inherent properties of the CCG formalism itself regarding how categories1 may combine." ></td>
	<td class="line x" title="27:175	Informed initialization should help with supertagging for two reasons." ></td>
	<td class="line x" title="28:175	First, categories have structurelacking in POS tagswaiting to be exploited." ></td>
	<td class="line x" title="29:175	For example, it is far more likely a priori to see the category sequence (S\NP)/NP NP/N than the sequence S/S NP\NP." ></td>
	<td class="line x" title="30:175	Given the categories for a word, this information can be used to influence our expectations about categories for adjacent words." ></td>
	<td class="line x" title="31:175	Second, this kind of information truly matters for the task: a key aspect of supertagging that differentiates it from POS tagging is that thecontextualinformationismuchmoreimportant for the former." ></td>
	<td class="line x" title="32:175	Lexical probabilities handle most of the ambiguity for POS tagging, but supertags are inherently about context and, furthermore, lexical ambiguity is much greater for supertagging, making lexical probabilities less effective." ></td>
	<td class="line x" title="33:175	I start by defining a distribution over lexical categories and then use this distribution as part of creating a CCG-informed transition distribution that appropriately breaks the symmetry of uniform HMM initialization." ></td>
	<td class="line x" title="34:175	After describing how these components are included in the HMM, I describe experiments with CCGbank varying the ambiguity of the lexicon provided." ></td>
	<td class="line x" title="35:175	I show that using knowledge about the formalism consistently improves performance, and is especially important as categorial ambiguity increases." ></td>
	<td class="line x" title="36:175	2 Lexical category distribution The categories of CCG are an inductively defined set containing elements that are either atomic elements or (curried) functions specifying the canon1For the rest of the paper, I will refer to categories rather supertags, but will still refer to the task as supertagging." ></td>
	<td class="line x" title="37:175	ical linear direction in which they seek their arguments." ></td>
	<td class="line x" title="38:175	Some example entries from CCGbank are: the := NPnb/N of := (NP\NP)/NP of := ((S\NP)\(S\NP)/NP were := (Sdcl\NP)/(Spss\NP) buy := (Sdcl\NP)/NP buy := ((((Sb\NP)/PP)/PP)/(Sadj\NP))/NP Words can be associated with multiple categories; the distribution over these categories is typically quite skewed." ></td>
	<td class="line x" title="39:175	For example, the first entry for buy occurs 33 times in CCGbank, compared with just once for the second." ></td>
	<td class="line x" title="40:175	That the simpler category is more prevalent is unsurprising: a general strategy whencreatingCCGlexiconsistousesimplercategories whenever possible." ></td>
	<td class="line x" title="41:175	This points to the possibility of defining distributions over CCG lexicons based on measures of the complexity of categories." ></td>
	<td class="line x" title="42:175	I use a simple distribution here: given a lexicon L, the probability of a category i is inversely proportional to its complexity: i = 1 complexity(ci)summationtext jL 1complexity(cj) (1) Here, a very simple complexity measure is assumed: the number of subcategories (tokens) contained in a category.2 For example, ((S\NP)\(S\NP)/NP contains 9: S (twice), NP (thrice), S\NP (twice), (S\NP)\(S\NP), and ((S\NP)\(S\NP)/NP." ></td>
	<td class="line x" title="43:175	Thetagtransitiondistributiondefinedinthenext section uses  to bias transitions toward simpler categories, e.g., preferring the first category for buy over the second." ></td>
	<td class="line x" title="44:175	Performance when using  is compared to using a uniform distribution." ></td>
	<td class="line x" title="45:175	Other distributions could be given, e.g., one which gives more mass to adjunct categories such as (S\NP)\(S\NP) than to ones which are otherwise similar but do not display such symmetry, like (S/NP)\(NP\S)." ></td>
	<td class="line x" title="46:175	However, the most important thing for present purposes is that simpler categories are more likely than more complex ones." ></td>
	<td class="line x" title="47:175	This distribution imposes no internal structure on the likelihood of a lexicon." ></td>
	<td class="line x" title="48:175	As far as  is concerned, lexicons can as well have the category (S\NP)\NP for transitive verbs and ((S/NP)/NP)/NP for ditransitive verbs, even though this is a highly unlikely pattern since we 2This worked better than using category arity or number of unique subcategory types." ></td>
	<td class="line x" title="49:175	58 Vinken will join the board as nonexecutive director S/(S\NP) (S\NP)/(S\NP) ((S\NP)/PP)/NP NP/N N PP/NP NP/N N > >NP NP > >(S\NP)/PP PP >S\NP >S\NP >S Figure 1: Normal form CCG derivation, using only application rules." ></td>
	<td class="line x" title="50:175	would expect both types of verbs to seek their arguments in the same direction." ></td>
	<td class="line x" title="51:175	Languages also tend to prefer lexicons with one or the other slash direction predominating (Villavicencio, 2002)." ></td>
	<td class="line x" title="52:175	In the future, it would be interesting to consider Bayesian approaches that could encode more complex structure and assign priors over distributions over lexicons, building on these observations." ></td>
	<td class="line x" title="53:175	An aspect of CCGbank that relevant for i is that some categories actually are not true categories." ></td>
	<td class="line x" title="54:175	For example, many punctuation categories are given as LRB, ., :, etc. In most grammars, the category of . is usually assumed to be S\S. The grammatical behavior of such pseudo-categories is handled via special rules in the parsers of Hockenmaier and Steedman (2007) and Clark and Curran (2007)." ></td>
	<td class="line x" title="55:175	I relabeled three of these: , to NP\NP, . to S\S and ; to (S\S)/S. A single best change was not clear for others such as LRB and :, so they were left as is. 3 Category transition distribution CCG analyses of sentences are built up from lexical categories combining to form derived categories, until an entire sentence is reduced to a single derived category with corresponding dependencies." ></td>
	<td class="line x" title="56:175	One of CCGs most interesting linguistic properties is it allows alternative constituents." ></td>
	<td class="line x" title="57:175	Consider the derivations in Figures 1 and 2, which show a normal form derivation (Eisner, 1996) and fully incremental derivation, respectively." ></td>
	<td class="line x" title="58:175	Both produce the same dependencies, guaranteed by the semantic consistency of CCGs rules (Steedman, 2000)." ></td>
	<td class="line x" title="59:175	This property of CCG of supporting multiple derivations of the same analysis has been termed spurious ambiguity." ></td>
	<td class="line x" title="60:175	However, the extra constituents are anything but spurious: they are implicated in a range of CCG (along with other forms of categorial grammar) linguistic analyses, including coordination, long-distance extraction, intonation, and incremental processing." ></td>
	<td class="line x" title="61:175	This all boils down to associativity: just as (1 + (4 + 2)) = ((1 + 4) + 2) = 7, CCG ensures that (Ed(sawTed)) = ((Edsaw)Ted) = S Such multiplederivationsarisewhenadjacentcategories can combine through either application or composition." ></td>
	<td class="line x" title="62:175	Thus, we would expect that the lexical categories needed to analyze an entire sentence will more often than not be able to combine with their immediate neighbors." ></td>
	<td class="line x" title="63:175	For example, six of seven pairs of adjacent lexical categories in the sentence in Figure 1 can combine." ></td>
	<td class="line x" title="64:175	Only NPP/NP of board as cannot.3 This observation can be used in different ways by different models for CCG supertagging." ></td>
	<td class="line x" title="65:175	For example, discriminative tagging models could include features that capture whether or not the current supertag can combine with the previous one and possibly via which CCG rule." ></td>
	<td class="line x" title="66:175	Here, I show how it can be used to provide a non-uniform starting point for the transition distributions j|i in a first-order Hidden Markov Model." ></td>
	<td class="line x" title="67:175	This is similar to how Grenager et al.(2005) use diagonal initialization in an HMM for field segmentation to encourage the model to remain in the same state (and thus predict the same label for adjacent words)." ></td>
	<td class="line x" title="69:175	For CCG supertagging, the initialization should discourage diagonalization and establish a preference for some transitions over others." ></td>
	<td class="line x" title="70:175	There are many ways to define such a starting point." ></td>
	<td class="line x" title="71:175	The simplest would be to reserve a small part of the mass spread uniformly over category pairs which cannot combine and then spread the rest of the mass uniformly over those which can." ></td>
	<td class="line x" title="72:175	However, we can provide a more refined distribution, j|i, by incorporating the lexical category distribution i defined in the previous section to weight these transitions according to this further information." ></td>
	<td class="line x" title="73:175	In a similar manner to Grenager et al.(2005), I define  as follows: 3I make the standard assumption that type-raising is performed in the lexicon, so the possibility of combining these through type-raising plus composition is not available." ></td>
	<td class="line x" title="75:175	59 Vinken will join the board as nonexecutive director S/(S\NP) (S\NP)/(S\NP) ((S\NP)/PP)/NP NP/N N PP/NP NP/N N >BS/(S\NP) >B2(S/PP)/NP >B(S/PP)/N >S/PP >BS/NP >BS/N >S Figure 2: Incremental CCG derivation, using both application and composition (B) rules." ></td>
	<td class="line x" title="76:175	j|i = (1)j +(i,j)  jsummationtext kL|(i,k) k (2) where (i,j) is an indicator function that returns 1 if categories ci and cj can combine when ci immediately precedescj, is a global parameter that specifying the total probability of transitions that are combinable from i. Each j receives a proportion of  according to its lexical prior probability over the sum of the lexical prior probabilities for all categories that combine with i. For the experiments in this paper, was set to.95." ></td>
	<td class="line x" title="77:175	For the models referred to as U and U-EM in section 5, the uniform lexical probability 1/|C| is used for i. For (i,j), I use the standard rules assumed for CCGbank parsers: forward and backward application (>, <), order-preserving composition (>B, <B), and backward crossed composition (<B) for S-rooted categories." ></td>
	<td class="line x" title="78:175	Thus, (NP,S\NP)=1, (S/NP,NP/N)=1, ((S\NP)/NP,(S\NP)\(S\NP))=1 and (S/NP,NP\NP)=0." ></td>
	<td class="line x" title="79:175	For application, leftward and rightward arguments are handled separately by assuming that it would be possible to consume all preceding arguments of the first category and all following arguments of the second." ></td>
	<td class="line x" title="80:175	So, ((S/NP)\S,NP/N)=1 and (NP,(S\NP)/NP)=1." ></td>
	<td class="line x" title="81:175	Unification on categories is standard (so (NP[nb],S\NP)=1), except that N unifies with NP only when N is the argument: (N,S\NP)=1, but (NP/N,NP)=0." ></td>
	<td class="line x" title="82:175	This is to deal with the fact that CCGbank represents many words with N (e.g., Mr.|N/N Vinken|N is|(S[dcl]\NP)/NP) and assumes that a parser will include the unary type changing rule NNP." ></td>
	<td class="line x" title="83:175	The HMM also has initial and final probabilities; distributions can be defined based on which categories are likely to start or end a sentence." ></td>
	<td class="line x" title="84:175	For this, I assume only that categories which seek arguments to the left (e.g., S\NP) are less likely at the beginning of a sentence and those which seek rightward arguments are less likely at the end." ></td>
	<td class="line x" title="85:175	The initializations for these are defined similarly to the transition distribution, substituting functions noLeftArgs(i) andnoRightArgs(i) for(i,j)." ></td>
	<td class="line x" title="86:175	4 Model A first-order Hidden Markov Model (bitag HMM) is used for bootstrapping a supertagger from a lexicon." ></td>
	<td class="line x" title="87:175	See Rabiner (1989) for an extensive introduction to and discussion of HMMs." ></td>
	<td class="line x" title="88:175	There are several reasons why this is an attractive tagging model here." ></td>
	<td class="line x" title="89:175	First, though extra context in the form of tritag transition distributions or other techniques can improve supervised POS tagging accuracy,theaccuracyofbitagHMMsisnotfarbehind." ></td>
	<td class="line x" title="90:175	The goal here is to investigate the relative gains of using CCG-based information in weakly supervised HMM learning." ></td>
	<td class="line x" title="91:175	Second, the expectationmaximization algorithm for bitag HMMs is efficient and has been shown to be quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions(BankoandMoore,2004)." ></td>
	<td class="line x" title="92:175	Third,themodels simplicity makes it straightforward to test the idea of CCG-initialization on tag transitions." ></td>
	<td class="line pc" title="93:175	Dirichlet priors can be used to bias HMMs toward more skewed distributions (Goldwater and Griffiths, 2007; Johnson, 2007), which is especially useful in the weakly supervised setting consideredhere." ></td>
	<td class="line oc" title="94:175	FollowingJohnson(2007),Iusevariational Bayes EM (Beal, 2003) during the M-step for the transition distribution: l+1j|i = f(E[ni,j] +i)f(E[n i] + |C|i) (3) f(v) = exp((v)) (4) 60 (v) = braceleftBigg g(v 1 2) ifv> 7 (v+ 1)  1v o.w." ></td>
	<td class="line x" title="95:175	(5) g(x)  log(x) + 124x2  7960x4 + 318064x6  12730720x8 (6) where V is the set of word types,is the digamma function (which is approximated by g), and i is the hyperparameter of the Dirichlet priors." ></td>
	<td class="line x" title="96:175	In all experiments, the i parameters were set symmetrically to.005." ></td>
	<td class="line x" title="97:175	For experiments using the transition prior j|i, the initial expectations of the model were set as E[ni,j] = |Ei|  j|i and E[ni] = |Ei|, where Ei is the set of emissions for category ci." ></td>
	<td class="line x" title="98:175	The uniform probability 1|C| was used in place of j|i for standard HMM initialization." ></td>
	<td class="line x" title="99:175	The emission distributions use standard EM expectations with more mass reserved for unknowns for tags with more emissions as follows:4 l+1k|i = E[ni,k] + |Ei| 1|V| E[ni] + |Ei| (7) The Viterbi algorithm is used for decoding." ></td>
	<td class="line x" title="100:175	5 Experiments CCGbank (Hockenmaier and Steedman, 2007) is a translation of phrase structure analyses of the Penn Treebank into CCG analyses." ></td>
	<td class="line x" title="101:175	Here, I consider only the lexical category annotations and ignore derivations." ></td>
	<td class="line x" title="102:175	The standard split used for weakly supervised HMM tagging experiments (Banko and Moore, 2004; Wang and Schuurmans, 2005) is used: sections 0-18 for training (train), 19-21 for development(dev), and22-24fortesting(test)." ></td>
	<td class="line x" title="103:175	All parameters and models were developed using dev." ></td>
	<td class="line x" title="104:175	The test set was used only once to obtain the performance figures reported here." ></td>
	<td class="line x" title="105:175	Counts for word types, word tokens and sentences for each data set are given in Table 1." ></td>
	<td class="line x" title="106:175	In train, there are 1241 distinct categories, the ambiguity per word type is 1.69, and the maximum number of categories for a single word type is 126." ></td>
	<td class="line x" title="107:175	This is much greater than for POS tags in CCGbank, for which there are 48 POS tags with an av4I also experimented with a Dirichlet prior on the emissions, but it performed worse." ></td>
	<td class="line x" title="108:175	Using a symmetric prior was actually detrimental, while performance within a percent of those achieved with the above update was achieved with Dirichlet hyperparameters set relative to |Ei|/|V|." ></td>
	<td class="line x" title="109:175	Dataset Types Tokens Sentences train 43063 893k 38,015 dev 14961 128k 5484 test 13898 127k 5435 Table 1: Basic statistics for the datasets." ></td>
	<td class="line x" title="110:175	erage ambiguity of 1.17 per word and a maximum of 7 tags in train.5 The set of supertags was not reduced: any category found in the data used to initialize a lexicon was considered." ></td>
	<td class="line x" title="111:175	This is one of the advantages of the HMM over using discriminative models, where typically only supertags seen at least 10 times in the training material are utilized for efficiency (Clark and Curran, 2007)." ></td>
	<td class="line x" title="112:175	Ignoring some supertags makes sense when building supervised supertaggers for pre-parse filtering, but not for learning from lexicons, where we cannot assume we have such frequencies." ></td>
	<td class="line x" title="113:175	For supervised training with the HMM on train, the performance is 87.6%." ></td>
	<td class="line x" title="114:175	This compares to 91.4% for the C&C supertagger." ></td>
	<td class="line x" title="115:175	The accuracy of the HMM, though quite a bit lower than that of C&C, is still quite good, indicating that it is an adequate model for the task." ></td>
	<td class="line x" title="116:175	Note also that it uses only the words themselves and does not rely on POS tags." ></td>
	<td class="line x" title="117:175	The performance of the C&C tagger was obtained by training the C&C POS tagger on the given dataset and tagging the evaluation material with it." ></td>
	<td class="line x" title="118:175	Finally, the HMM trains in just a few seconds as opposed to over an hour.6 Five different weakly supervised scenarios are evaluated: (1) standard EM with 50 iterations (EM), (2)  initialization with uniform lexical probabilities w/o EM (U), (3)  with  probabilities w/o EM (), (4)  with uniform lexical probabilities and 10 EM iterations, and (5)  with  and 10 EM iterations.7 These scenarios compare the effectiveness of standard EM with the use of grammar informed transitions; these in turn are of two varieties  one using a uniform lexical prior or one that is biased in favor of less complex categories according to ." ></td>
	<td class="line x" title="119:175	As Banko and Moore (2004) discovered when 5Note that the POS tag information is not used in these experiments, except for by the C&C tagger." ></td>
	<td class="line x" title="120:175	6It should be stressed that the goal of this paper is not to compete on supervised performance with C&C; instead, this comparison shows that the HMM supervised performance is reasonable and is thus relevant for bootstrapping." ></td>
	<td class="line x" title="121:175	7The number of iterations for standard and grammar informed iteration were determined by performance on dev." ></td>
	<td class="line x" title="122:175	61 reimplementing several previous HMMs for POS tagging, the lexicons had been limited to contain only tags occurring above a particular frequency." ></td>
	<td class="line x" title="123:175	For POS tagging, this keeps a cleaner lexicon that avoids errors in annotated data (such as the tagged as VB) and rare tags (such as a tagged as SYM)." ></td>
	<td class="line x" title="124:175	Whenlearningfromalexiconalone,suchelements receive the same weight as their other (correct or more fundamental) tags in initializing the HMM." ></td>
	<td class="line x" title="125:175	The problem of rare tags turns out to be very important for weakly supervised CCG supertagging.8 To consider the effect of the CCG-based initialization for lexicons with differing ambiguity, I use tagcutoffsthatremoveanylexicalentrycontaining a category that appears with a particular word less than X% of the time (Banko and Moore, 2004), as well as using no cutoffs at all." ></td>
	<td class="line x" title="126:175	Recall that the goal of these experiments is to investigate the relative difference in performance between using the grammar-based initialization or not, given some (possibly hand-crafted) lexicon." ></td>
	<td class="line x" title="127:175	Lexicon cutoffs actually constitute a strong source of supervision because they use tag frequencies (which would not be known for a hand-crafted lexicon), so it should be stressed that they are used here only so that this relative performance can be measured for different ambiguity levels." ></td>
	<td class="line x" title="128:175	Table2providesaccuracyforambiguouswords (and not including punctuation) for the five scenarios, varying the cutoff to measure the effect of progressively allowing more lexical ambiguity (and muchrarercategories)." ></td>
	<td class="line x" title="129:175	Thenumberofambiguous, non-punctuation tokens is 101,167." ></td>
	<td class="line x" title="130:175	The first thing to note is performance given only the lexicon and the U or  initialization of the transitions." ></td>
	<td class="line x" title="131:175	These correspond to taggers which have only been given the lexicon and have not utilized any data to improve their estimates of the transitionandemissionprobabilities." ></td>
	<td class="line x" title="132:175	Interestingly, both do quite well with a clean lexicon: see the columns under U and ." ></td>
	<td class="line x" title="133:175	These indicate that initializing the transitions based on whether categories can combine does indeed appropriately capture key aspects of category transitions." ></td>
	<td class="line x" title="134:175	Furthermore, using the lexical category distribution () to create the transition initialization provides a better starting point than the uniform one (U), espe8CCGbank actually corrects many errors in the Penn Treebank, and does not suffer as much from mistagged examples." ></td>
	<td class="line x" title="135:175	However, there were two instances of an ill-formed category ((S[b]\NP)/NP)/inwsj 0595forthewordsownandkeep." ></td>
	<td class="line x" title="136:175	These were corrected to (S[b]\NP)/NP." ></td>
	<td class="line x" title="137:175	Cutoff EM U  U-EM -EM .1 77.4 73.1 74.7 80.0 79.6 .05 69.1 70.6 72.5 79.2 79.2 .01 60.2 62.2 65.0 75.4 76.7 .005 52.2 57.8 59.0 72.5 73.8 .001 41.3 45.5 48.2 63.0 67.6 None 33.0 33.9 37.8 52.9 56.1 Table 2: Performance on ambiguous word types of the HMM with standard EM (uniform starting transitions), just the initial  transitions (U and ), and EM initialized with U and , for lexicons with varied cutoffs." ></td>
	<td class="line x" title="138:175	Note also that these scores do not include punctuation." ></td>
	<td class="line x" title="139:175	cially as lexical ambiguity increases." ></td>
	<td class="line x" title="140:175	Next, note that both U-EM and -EM beat the randomly initialized EM for all cutoff levels." ></td>
	<td class="line x" title="141:175	For the 10% tag cutoff (the first row), there is an absolute difference of over 2% for both.9 As the ambiguity increases, the grammar-informed initialization has a much stronger effect." ></td>
	<td class="line x" title="142:175	In the extreme case of using no cutoff at all (the None row ofTable2), U-EM and-EM beat EM by19.9% and 23.1%, respectively." ></td>
	<td class="line x" title="143:175	Finally, using the lexical category distribution  instead of a uniform one is much more effective when there is more lexical ambiguity (e.g., compare the .01 through None rows of the U-EM and -EM columns), but has a negligible effect with less ambiguity (rows .05 and .01)." ></td>
	<td class="line x" title="144:175	This demonstrates that the grammarbased initialization can be effectively exploited  it is in fact crucial for improving performance when we are given much more ambiguous lexicons." ></td>
	<td class="line x" title="145:175	The majority of errors with -EM involve marking adjectives (N/N) as nouns (N) or vice versa, and assigning the wrong prepositional category (usually the simpler noun phrase postmodifier (NP\NP)/NP instead of the verb phrase modifier ((S\NP)\(S\NP))/NP." ></td>
	<td class="line x" title="146:175	Both of these kinds of errors, and others, could potentially be corrected if the categories proposed by the tagger were further filtered by an attempt to parse each sentence with the categories." ></td>
	<td class="line x" title="147:175	6 Related work The idea of using knowledge from the formalism for constraining supertagging originates with Ban9For comparison with the performance of 87.6% for the fully supervised HMM on all tokens, -EM achieves 82.1% and 58.9% using a cutoff of .1 or no cutoff, respectively." ></td>
	<td class="line x" title="148:175	62 galore and Joshi (1999)." ></td>
	<td class="line x" title="149:175	They used constraints based on how elementary trees of Tree-Adjoining Grammar could or could not combine as filters to block out tags that do not fit in certain locations in the string." ></td>
	<td class="line x" title="150:175	My approach is different is several ways." ></td>
	<td class="line x" title="151:175	First, they dealt with fully supervised supertagging; here I show that using this knowledge is important for weakly supervised supertagging where we are given only a tag dictionary (lexicon)." ></td>
	<td class="line x" title="152:175	Second, my approach encodes grammarbased cues only as an initial bias, so categories are never explicitly filtered." ></td>
	<td class="line x" title="153:175	Finally, I use CCG rather than TAG, which makes it possible to exploit a much higher degree of associativity in derivations." ></td>
	<td class="line x" title="154:175	This in turn makes it easier to utilize prior knowledge about adjacent contexts  precisely what is neededforusingthegrammartoinfluencethetransition probabilities of a bigram HMM." ></td>
	<td class="line x" title="155:175	On the other hand, Bangalore and Joshi (1999) use constraints that act at greater distances than I have considered here." ></td>
	<td class="line x" title="156:175	For example, if one wishes to provide a word with the category ((S\NP)/PP)/NP, then there should be a word with a category which results in a PP two or more words to its right  this is something which the bigram transitions considered here cannot capture." ></td>
	<td class="line x" title="157:175	An interesting way to extend the present approach would be to enforce such patterns as posterior constraints during EM (Graca et al., 2007)." ></td>
	<td class="line xc" title="158:175	Recentworkconsidersadamagedtagdictionary by assuming that tags are known only for words that occur more than once or twice (Toutanova and Johnson, 2007)." ></td>
	<td class="line x" title="159:175	A very interesting aspect of this work is that they explicitly model ambiguity classes to exploit commonality in the lexicon between different word forms, which could be even more useful for supertagging." ></td>
	<td class="line x" title="160:175	In a grammar development context, it is often thecasethatonlysomeofthecategoriesforaword have been assigned." ></td>
	<td class="line x" title="161:175	This is the scenario considered by Haghighi and Klein (2006) for POS tagging: how to construct an accurate tagger given a set of tags and a few example words for each of those tags." ></td>
	<td class="line x" title="162:175	They use distributional similarity of wordstodefinefeaturesfortaggingthateffectively allow such prototype words to stand in for others." ></td>
	<td class="line x" title="163:175	This idea could be used with my approach as well; the most obvious way would be to use prototype words to suggest extra categories (beyond the tag dictionary) for known words and a reduced set of categories for unknown words." ></td>
	<td class="line oc" title="164:175	Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007)." ></td>
	<td class="line o" title="165:175	No tag dictionaries areassumed, andthemodelsareparametrizedwith Dirichlet priors." ></td>
	<td class="line n" title="166:175	The states of these models implicitly represent tags; however, it actually is not clear whatthestatesinsuchmodelstrulyrepresent: they are (probably interesting) clusters that may or may not correspond to what we normally think of as parts-of-speech." ></td>
	<td class="line x" title="167:175	POS tags are relatively inert, passive elements in a grammar, whereas CCG categories are the very drivers of grammatical analysis." ></td>
	<td class="line x" title="168:175	That is, syntax is projected, quite locally, by lexical categories." ></td>
	<td class="line o" title="169:175	It would thus be interesting to considertheinductionofcategorieswithgrammarbased priors with such models." ></td>
	<td class="line x" title="170:175	7 Conclusion I have shown that weakly supervised learning can indeed be used to induce supertaggers from a lexicon mapping words to their possible categories, but that the extra ambiguity in the supertagging task over that of POS tagging makes performance much more sensitive to rare categories that occur in larger, more ambiguous lexicons." ></td>
	<td class="line x" title="171:175	However, I have also shown that the CCG formalism itself can provide the basis for useful distributions over lexical categories and tag transitions in a bitag HMM." ></td>
	<td class="line x" title="172:175	By using these distributions to initialize the HMM, it is possible to improve performance regardless of the underlying ambiguity." ></td>
	<td class="line x" title="173:175	This is especially important for reducing error when the lexicon used for bootstrapping is highly ambiguous and contains very rare categories." ></td>
	<td class="line x" title="174:175	Acknowledgments Thanks to the UT Austin Natural Language Learning group and three anonymous reviewers for useful comments and feedback." ></td>
	<td class="line x" title="175:175	This work was supported by NSF grant BCS-0651988 and a Faculty Research Assignment from UT Austin." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1042
Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction
Headden III, William P.;McClosky, David;Charniak, Eugene;"></td>
	<td class="line x" title="1:202	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 329336 Manchester, August 2008 Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction William P. Headden III, David McClosky, Eugene Charniak Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence, RI 02912 {headdenw,dmcc,ec}@cs.brown.edu Abstract This paper explores the relationship between various measures of unsupervised part-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags." ></td>
	<td class="line x" title="2:202	We find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance." ></td>
	<td class="line oc" title="3:202	1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (Johnson, 2007; Goldwater and Griffiths, 2007; Biemann, 2006; Dasgupta and Ng, 2007) and deeper grammatical structure like constituency and dependency trees (Klein and Manning, 2004; Smith, 2006; Bod, 2006; Seginer, 2007; Van Zaanen, 2001)." ></td>
	<td class="line x" title="4:202	While some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging." ></td>
	<td class="line x" title="5:202	Meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the Penn Treebank." ></td>
	<td class="line x" title="6:202	In this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction." ></td>
	<td class="line x" title="7:202	Using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results." ></td>
	<td class="line x" title="8:202	c2008." ></td>
	<td class="line x" title="9:202	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:202	Some rights reserved." ></td>
	<td class="line x" title="11:202	We then explore the relationship between the performance on common unsupervised tagging metrics and the performance of resulting grammar induction systems." ></td>
	<td class="line x" title="12:202	Disconcertingly we find that they bear little to no relationship." ></td>
	<td class="line x" title="13:202	This paper is organized as follows." ></td>
	<td class="line x" title="14:202	In Section 2 we discuss unsupervised part-of-speech induction systems and common methods of evaluation." ></td>
	<td class="line x" title="15:202	In Section 3, we describe grammar induction in general and discuss the systems with which we evaluate taggings." ></td>
	<td class="line x" title="16:202	We present our experiments in Section 4, and finally conclude in Section 5." ></td>
	<td class="line x" title="17:202	2 Part-of-speech Tag Induction Part-of-speech tag induction can be thought of as a clustering problem where, given a corpus of words, we aim to group word tokens into syntactic classes." ></td>
	<td class="line x" title="18:202	Two tasks are commonly labeled unsupervised part-of-speech induction." ></td>
	<td class="line x" title="19:202	In the first, tag induction systems are allowed the use of a tagging dictionary, which specifies for each word a set of possible parts-of-speech (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007)." ></td>
	<td class="line x" title="20:202	In the second, only the word tokens and sentence boundaries are given." ></td>
	<td class="line x" title="21:202	In this work we focus on this latter task to explore grammar induction in a maximally unsupervised context." ></td>
	<td class="line x" title="22:202	Tag induction systems typically focus on two sorts of features: distributional and morphological. Distributional refers to what sorts of words appear in close proximity to the word in question, while morphological refers to modeling the internal structure of a word." ></td>
	<td class="line x" title="23:202	All the systems below make use of distributional information, whereas only two use morphological features." ></td>
	<td class="line x" title="24:202	We primarily focus on the metrics used to evaluate induced taggings." ></td>
	<td class="line x" title="25:202	The catalogue of recent partof-speech systems is large, and we can only test 329 the tagging metrics using a few systems." ></td>
	<td class="line x" title="26:202	Recent work that we do not explore explicitly includes (Biemann, 2006; Dasgupta and Ng, 2007; Freitag, 2004; Smith and Eisner, 2005)." ></td>
	<td class="line x" title="27:202	We have selected a few systems, described below, that represent a broad range of features and techniques to make our evaluation of the metrics as broad as possible." ></td>
	<td class="line x" title="28:202	2.1 Clustering using SVD and K-means Schutze (1995) presents a series of part-of-speech inducers based on distributional clustering." ></td>
	<td class="line x" title="29:202	We implement the baseline system, which Klein and Manning (2002) use for their grammar induction experiments with induced part-of-speech tags." ></td>
	<td class="line x" title="30:202	For each word type w in the vocabulary V , the system forms a feature row vector consisting of the number of times each of the F most frequent words occur to the left of w and to the right of w. It normalizes these row vectors and assembles them into a |V|2F matrix." ></td>
	<td class="line x" title="31:202	It then performs a Singular Value Decomposition on the matrix and rank reduces it to decrease its dimensionality to d principle components (d < 2F)." ></td>
	<td class="line x" title="32:202	This results in a representation of each word as a point in a d dimensional space." ></td>
	<td class="line x" title="33:202	We follow Klein and Manning (2002) in using Kmeans to cluster the d dimensional word vectors into parts-of-speech." ></td>
	<td class="line x" title="34:202	We use the F = 500 most frequent words as left and right context features, and reduce to a dimensionality of d = 50." ></td>
	<td class="line x" title="35:202	We refer to this system as SVD in our experiments." ></td>
	<td class="line x" title="36:202	The other systems described in Schutze (1995) make use of more complicated feature models." ></td>
	<td class="line x" title="37:202	We chose the baseline system primarily to match previous evaluations of grammar induction using induced tags (Klein and Manning, 2002)." ></td>
	<td class="line x" title="38:202	2.2 Hidden Markov Models One simple family of models for part-of-speech induction are the Hidden Markov Models (HMMs), in which there is a sequence of hidden state variables t1tn (for us, the part-of-speech tags)." ></td>
	<td class="line x" title="39:202	Each state ti is conditioned on the previous n 1 states ti1tin+1, and every ti emits an observed word wi conditioned on ti." ></td>
	<td class="line x" title="40:202	There is a single start state that emits nothing, as well as a single stop state, which emits an end-of-sentence marker with probability 1 and does not transition further." ></td>
	<td class="line x" title="41:202	In our experiments we use the bitag HMM, in which each state ti depends only on state ti1." ></td>
	<td class="line x" title="42:202	The classic method of training HMMs for partof-speech induction is the Baum-Welch (Baum, 1972) variant of the Expectation-Maximization (EM) algorithm, which searches for a local maximum in the likelihood of the observed words." ></td>
	<td class="line x" title="43:202	Other methods approach the problem from a Bayesian perspective." ></td>
	<td class="line x" title="44:202	These methods place Dirichlet priors over the parameters of each transition and emission multinomial." ></td>
	<td class="line pc" title="45:202	For an HMM with a set of states T and a set of output symbols V : t  T t  Dir(1,|T|) (1) t  T t  Dir(1,|V |) (2) ti|ti1, ti1  Multi(ti1) (3) wi|ti, ti  Multi(ti) (4) One advantage of the Bayesian approach is that the prior allows us to bias learning toward sparser structures, by setting the Dirichlet hyperparameters , to a value less than one (Johnson, 2007; Goldwater and Griffiths, 2007)." ></td>
	<td class="line x" title="46:202	This increases the probability of multinomial distributions which put most of their mass on a few events, instead of distributing them broadly across many events." ></td>
	<td class="line pc" title="47:202	There is evidence that this leads to better performance on some part-of-speech induction metrics (Johnson, 2007; Goldwater and Griffiths, 2007)." ></td>
	<td class="line x" title="48:202	There are both MCMC and variational approaches to estimating HMMs with sparse Dirichlet priors; we chose the latter (Variational Bayes or VB) due to its simple implementation as a minor modification to Baum-Welch." ></td>
	<td class="line oc" title="49:202	Johnson (2007) evaluates both estimation techniques on the Bayesian bitag model; Goldwater and Griffiths (2007) emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model, yielding a tagging supported by many different parameter settings." ></td>
	<td class="line oc" title="50:202	Following the setup in Johnson (2007), we initialize the transition and emission distributions to be uniform with a small amount of noise, and run EM and VB for 1000 iterations." ></td>
	<td class="line x" title="51:202	We label these systems as HMM-EM and HMM-VB respectively in our experiments." ></td>
	<td class="line oc" title="52:202	In our VB experiments we set i = j = 0.1,i  {1,,|T|},j  {1,,|V |}, which yielded the best performance on most reported metrics in Johnson (2007)." ></td>
	<td class="line pc" title="53:202	We use maximum marginal decoding, which Johnson (2007) reports performs better than Viterbi decoding." ></td>
	<td class="line x" title="54:202	2.3 Systems with Morphology Clark (2003) presents several part-of-speech induction systems which incorporate morphological as well as distributional information." ></td>
	<td class="line x" title="55:202	We use the 330 implementation found on his website.1 2.3.1 Ney-Essen with Morphology The simplest model is based on work by (Ney et al., 1994)." ></td>
	<td class="line x" title="56:202	It uses a bitag HMM, with the restriction that each word type in the vocabulary can only be generated by a single part-of-speech." ></td>
	<td class="line x" title="57:202	Thus the tag induction task here reduces to finding a multiway partition of the vocabulary." ></td>
	<td class="line x" title="58:202	The learning algorithm greedily reassigns each word type to the part-of-speech that results in the greatest increase in likelihood." ></td>
	<td class="line x" title="59:202	In order to incorporate morphology, Clark (2003) associates with each part-of-speech a HMM with letter emissions." ></td>
	<td class="line x" title="60:202	The vocabulary is generated by generating a series of word types from the letter HMM of each part-of-speech." ></td>
	<td class="line x" title="61:202	These can model very basic concatenative morphology." ></td>
	<td class="line x" title="62:202	The parameters of the HMMs are estimated by running a single iteration of Forward-Backward after each round of reassigning words to tags." ></td>
	<td class="line x" title="63:202	In our experiments we evaluate both the model without morphology (NE in our experiments), and the morphological model, trying both 5 and 10 states in the letter HMM (NEMorph5, NEMorph10 respectively)." ></td>
	<td class="line x" title="64:202	2.3.2 Two-Level HMM The final part-of-speech inducer we try from Clark (2003) is a two-level HMM." ></td>
	<td class="line x" title="65:202	This is similar to the previous model, except it lifts the restriction that a word appear under only one part-of-speech." ></td>
	<td class="line x" title="66:202	Alternatively, one could think of this model as a standard HMM, whose emission distributions incorporate a mixture of a letter HMM and a standard multinomial." ></td>
	<td class="line x" title="67:202	Training uses a simple variation of Forward-Backward." ></td>
	<td class="line x" title="68:202	In the experiments in this paper, we initialize the mixture parameters to .5, and try 5 states in the letter HMM." ></td>
	<td class="line x" title="69:202	We refer to this model as 2HMM." ></td>
	<td class="line x" title="70:202	2.4 Tag Evaluation Objective evaluation in any clustering task is always difficult, since there are many ways to define good clusters." ></td>
	<td class="line x" title="71:202	Typically it involves a mixture of subjective evaluation and a comparison of the clusters to those found by human annotators." ></td>
	<td class="line x" title="72:202	In the realm of part-of-speech induction, there are several common ways of doing the latter." ></td>
	<td class="line x" title="73:202	These split into two groups: accuracy and informationtheoretic criteria." ></td>
	<td class="line x" title="74:202	1http://www.cs.rhul.ac.uk/home/alexc/pos.tar.gz Accuracy, given some mapping between the set of induced classes and the gold standard labels, is the number of words in the corpus that have been marked with the correct gold label divided by the total number of word tokens." ></td>
	<td class="line x" title="75:202	The main challenge facing these metrics is deciding how to to map each induced part-of-speech class to a gold tag." ></td>
	<td class="line oc" title="76:202	One option is what Johnson (2007) calls many-to-one (M-to-1) accuracy, in which each induced tag is labeled with its most frequent gold tag." ></td>
	<td class="line p" title="77:202	Although this results in a situation where multiple induced tags may share a single gold tag, it does not punish a system for providing tags of a finer granularity than the gold standard." ></td>
	<td class="line x" title="78:202	In contrast, one-to-one (1-to-1) accuracy restricts each gold tag to having a single induced tag." ></td>
	<td class="line x" title="79:202	The mapping typically is made to try to give the most favorable mapping in terms of accuracy, typically using a greedy assignment (Haghighi and Klein, 2006)." ></td>
	<td class="line oc" title="80:202	In cases where the number of gold tags is different than the number of induced tags, some must necessarily remain unassigned (Johnson, 2007)." ></td>
	<td class="line x" title="81:202	In addition to accuracy, there are several information theoretic criteria presented in the literature." ></td>
	<td class="line x" title="82:202	These escape the problem of trying to find an appropriate mapping between induced and gold tags, at the expense of perhaps being less intuitive." ></td>
	<td class="line x" title="83:202	Let TI be the tag assignments to the words in the corpus created by an unsupervised tagger, and let TG be the gold standard tag assignments." ></td>
	<td class="line x" title="84:202	Clark (2003) uses Shannons conditional entropy of the gold tagging given the induced tagging H(TG|TI)." ></td>
	<td class="line x" title="85:202	Lower entropy indicates less uncertainty in the gold tagging if we already know the induced tagging." ></td>
	<td class="line x" title="86:202	Freitag (2004) uses the similar cluster-conditional tag perplexity which is merely exp(H(TG|TI))2." ></td>
	<td class="line x" title="87:202	Since cluster-conditional tag perplexity is a monotonic function of H(TG|TI), we only report the latter." ></td>
	<td class="line x" title="88:202	Goldwater and Griffiths (2007) propose using the Variation of Information of Meila (2003): V I(TG;TI) = H(TG|TI) + H(TI|TG) VI represents the change in information when going from one clustering to another." ></td>
	<td class="line x" title="89:202	It holds the nice properties of being nonnegative, symmetric, as well as fulfilling the triangle inequality." ></td>
	<td class="line x" title="90:202	2Freitag (2004) measures entropy in nats, while we use bits." ></td>
	<td class="line x" title="91:202	The difference is a constant factor." ></td>
	<td class="line x" title="92:202	331 3 Grammar Induction In addition to parts-of-speech, we also want to discover deeper syntactic relationships." ></td>
	<td class="line x" title="93:202	Grammar induction is the problem of determining these relationships in an unsupervised fashion." ></td>
	<td class="line x" title="94:202	This can be thought of more concretely as an unsupervised parsing task." ></td>
	<td class="line x" title="95:202	As there are many languages and domains with few treebank resources, systems that can learn syntactic structure from unlabeled data would be valuable." ></td>
	<td class="line x" title="96:202	Most work on this problem has focused on either dependency induction, which we discuss in Section 3.2, or on constituent induction, which we examine in the next section." ></td>
	<td class="line x" title="97:202	The Grammar Induction systems we use to evaluate the above taggers are the Constituent-Context Model (CCM), the Dependency Model with Valence (DMV), and a model which combines the two (CCM+DMV) outlined in (Klein and Manning, 2002; Klein and Manning, 2004)." ></td>
	<td class="line x" title="98:202	3.1 Constituent Grammar Induction Klein and Manning (2002) present a generative model for inducing constituent boundaries from part-of-speech tagged text." ></td>
	<td class="line x" title="99:202	The model first generates a bracketing B = {Bij}1ijn, which specifies whether each span (i,j) in the sentence is a constituent or a distituent." ></td>
	<td class="line x" title="100:202	Next, given the constituency or distituency of the span Bij, the model generates the part-of-speech yield of the span titj, and the one-tag context window of the span ti1,tj+1." ></td>
	<td class="line x" title="101:202	P(titj|Bij) and P(ti1,tj+1|Bij) are multinomial distributions." ></td>
	<td class="line x" title="102:202	The model is trained using EM." ></td>
	<td class="line x" title="103:202	We evaluate induced constituency trees against those of the Penn Treebank using the versions of unlabeled precision, recall, and F-score used by Klein and Manning (2002)." ></td>
	<td class="line x" title="104:202	These ignore trivial brackets and multiple constituents spanning the same bracket." ></td>
	<td class="line x" title="105:202	They evaluate their CCM system on the Penn Treebank WSJ sentences of length 10 or less, using part-of-speech tags induced by the baseline system of Schutze (1995)." ></td>
	<td class="line x" title="106:202	They report that switching to induced tags decreases the overall bracketing F-score from 71.1 to 63.2, although the recall of VP and S constituents actually improves." ></td>
	<td class="line x" title="107:202	Additionally, they find that NP and PP recall decreases substantially with induced tags." ></td>
	<td class="line x" title="108:202	They attribute this to the fact that nouns end up in many induced tags." ></td>
	<td class="line x" title="109:202	There has been quite a bit of other work on constituency induction." ></td>
	<td class="line x" title="110:202	Smith and Eisner (2004) present an alternative estimation technique for CCM which uses annealing to try to escape local maxima." ></td>
	<td class="line x" title="111:202	Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing framework." ></td>
	<td class="line x" title="112:202	Several approaches try to learn structure directly from raw text." ></td>
	<td class="line x" title="113:202	Seginer (2007) has an incremental parsing approach using a novel representation called common-cover-links, which can be converted to constituent brackets." ></td>
	<td class="line x" title="114:202	Van Zaanen (2001)s ABL attempts to align sentences to determine what sequences of words are substitutable." ></td>
	<td class="line x" title="115:202	The work closest in spirit to this paper is Cramer (2007), who evaluates several grammar induction systems on the Eindhoven corpus (Dutch)." ></td>
	<td class="line x" title="116:202	One of his experiments compares the grammar induction performance of these systems starting with tags induced using the system described by Biemann (2006), to the performance of the systems on manually-marked tags." ></td>
	<td class="line x" title="117:202	However he does not evaluate to what degree better tagging performance leads to improvement in these systems." ></td>
	<td class="line x" title="118:202	3.2 Dependency Grammar Induction A dependency tree is a directed graph whose nodes are words in the sentence." ></td>
	<td class="line x" title="119:202	A directed edge exists between two words if the target word (argument) is a dependent of the source word (head)." ></td>
	<td class="line x" title="120:202	Each word token may be the argument of only one head, but a head may have several arguments." ></td>
	<td class="line x" title="121:202	One word is the head of the sentence, and is often thought of as the argument of a virtual Root node." ></td>
	<td class="line x" title="122:202	Klein and Manning (2004) present their Dependency Model with Valence (DMV) for the unsupervised induction of dependencies." ></td>
	<td class="line x" title="123:202	Like the constituency model, DMV works from parts-ofspeech." ></td>
	<td class="line x" title="124:202	Under this model, for a given head, h, they first generate the parts-of-speech of the arguments to the right of h, and then those to the left." ></td>
	<td class="line x" title="125:202	Generating the arguments in a particular direction breaks down into two parts: deciding whether to stop generating in this direction, and if not, what part-of-speech to generate as the argument." ></td>
	<td class="line x" title="126:202	The argument decision conditions on h and the direction." ></td>
	<td class="line x" title="127:202	The stopping decision conditions on this and also on whether h has already generated an argument in this direction, thereby capturing the limited notion of valence from which the model takes its name." ></td>
	<td class="line x" title="128:202	It is worth noting that this model can only represent projective dependency trees, i.e. those without crossing edges." ></td>
	<td class="line x" title="129:202	Dependencies are typically evaluated using di332 Tagging Metrics Grammar Induction Metrics Tagger No." ></td>
	<td class="line x" title="130:202	Tags CCM CCM+DMV DMV 1-to-1 H(TG|TI) M-to-1 VI UF1 DA UA UF1 DA UA Gold 1.00 0.00 1.00 0.00 71.50 52.90 67.60 56.50 45.40 63.80 HMM-EM 10 0.39 2.67 0.41 4.39 58.89 40.12 59.26 59.43 36.77 57.37 HMM-EM 20 0.43 2.28 0.48 4.54 57.31 51.16 64.66 61.33 38.65 58.57 HMM-EM 50 0.36 1.83 0.58 4.92 56.56 48.03 63.84 58.02 39.30 58.84 HMM-VB 10 0.40 2.75 0.41 4.42 39.05 27.72 52.84 58.64 23.94 51.64 HMM-VB 20 0.40 2.63 0.43 4.65 37.60 33.77 55.97 40.30 30.36 51.53 HMM-VB 50 0.38 2.70 0.42 5.01 34.68 37.29 57.72 39.82 29.03 50.50 NE 10 0.34 2.74 0.40 4.32 28.80 20.70 50.60 32.70 26.20 48.90 NE 20 0.48 2.02 0.55 3.76 32.50 36.00 59.30 40.60 32.80 54.00 NEMorph10 10 0.44 2.46 0.47 3.74 29.03 25.99 53.80 34.58 26.98 48.72 NEMorph10 20 0.48 1.94 0.56 3.65 31.95 35.85 57.93 38.22 30.45 50.72 NEMorph10 50 0.47 1.24 0.72 3.60 31.07 36.29 57.76 39.28 31.50 52.83 NEMorph5 10 0.45 2.50 0.47 3.76 29.04 22.72 51.58 32.67 23.62 47.89 NEMorph5 20 0.44 2.02 0.56 3.80 31.94 24.17 52.43 32.90 22.41 47.17 NEMorph5 50 0.47 1.27 0.72 3.64 31.39 38.63 59.44 40.23 34.26 54.63 2HMM 10 0.38 2.78 0.41 4.55 31.63 36.35 58.87 44.97 28.43 49.32 2HMM 20 0.41 2.35 0.48 4.71 42.39 43.91 60.74 50.85 29.32 50.69 2HMM 50 0.37 1.92 0.58 5.11 41.18 49.94 64.87 57.84 39.24 59.14 SVD 10 0.31 3.07 0.34 4.99 37.77 27.64 49.56 36.46 20.74 45.52 SVD 20 0.33 2.73 0.40 4.99 37.17 30.14 51.66 37.66 22.24 46.25 SVD 50 0.34 2.37 0.47 5.18 36.87 37.66 56.49 52.83 22.50 46.52 SVD 100 0.34 2.03 0.53 5.37 45.46 41.68 58.83 64.20 20.81 44.36 SVD 200 0.32 1.72 0.59 5.59 61.90 34.79 52.25 59.93 22.66 42.30 Table 1: The performance of the taggers regarding both tag and grammar induction metrics on WSJ sections 0-10, averaged over 10 runs." ></td>
	<td class="line x" title="131:202	Bold indicates the result was within 10 percent of the best-scoring induced system for a given metric." ></td>
	<td class="line x" title="132:202	rected and undirected accuracy." ></td>
	<td class="line x" title="133:202	These are the total number of proposed edges that appear in the gold tree divided by the total number of edges (the number of words in the sentence)." ></td>
	<td class="line x" title="134:202	Directed accuracy gives credit to a proposed edge if it is in the gold tree and is in the correct direction, while undirected accuracy ignores the direction." ></td>
	<td class="line x" title="135:202	Klein and Manning (2004) also present a model which combines CCM and DMV into a single model, which we show as CCM+DMV." ></td>
	<td class="line x" title="136:202	In their experiments, this model performed better on both the constituency and dependency induction tasks." ></td>
	<td class="line x" title="137:202	As with CCM, Klein and Manning (2004) similarly evaluate the combined CCM+DMV system using tags induced with the same method." ></td>
	<td class="line x" title="138:202	Again they find that overall bracketing F-score decreases from 77.6 to 72.9 and directed dependency accuracy measures decreases from 47.5 to 42.3 when switching to induced tags from gold." ></td>
	<td class="line x" title="139:202	However for each metric, the systems still do quite well with induced tags." ></td>
	<td class="line x" title="140:202	As in the constituency case, Smith (2006) presents several alternative estimation procedures for DMV, which try to minimize the local maximum problems inherent in EM." ></td>
	<td class="line x" title="141:202	It is thus possible these methods might yield better performance for the models when run off of induced tags." ></td>
	<td class="line x" title="142:202	4 Experiments We induce tags with each system on the Penn Treebank Wall Street Journal (Marcus et al., 1994), sections 0-10, which contain 20,260 sentences." ></td>
	<td class="line x" title="143:202	We vary the number of tags (10, 20, 50) and run each system 10 times for a given setting." ></td>
	<td class="line x" title="144:202	The result of each run is used as the input to the CCM, DMV, and CCM+DMV systems." ></td>
	<td class="line x" title="145:202	While the tags are induced from all sentences in the section, following the practice in (Klein and Manning, 2002; Klein and Manning, 2004), we remove punctuation, and consider only sentences of length not greater than 10 in our grammar induction experiments." ></td>
	<td class="line x" title="146:202	Taggings are evaluated after punctuation is removed, but before filtering for length." ></td>
	<td class="line x" title="147:202	To explore the relationship between tagging metrics and the resulting performance of grammar induction systems, we examine each pair of tagging and grammar induction metrics." ></td>
	<td class="line x" title="148:202	Consider the following two examples: DMV directed accuracy vs. H(TG|TI) (Figure 1), and CCM f-score vs. variation of information (Figure 2)." ></td>
	<td class="line x" title="149:202	These were selected because they have relatively high magnitude s. From these plots it is clear that although there 333 may be a slight correspondence, the relationships are weak at best." ></td>
	<td class="line x" title="150:202	Each tagging and grammar induction metric gives us a ranking over the set of taggings of the data generated over the course of our experiments." ></td>
	<td class="line x" title="151:202	These are ordered from best to worst according to the metric, so for instance H(TG|TI) would give highest rank to its lowest value." ></td>
	<td class="line x" title="152:202	We can compare the two rankings using Kendalls  (see Lapata (2006) for an overview), a nonparametric measure of correspondence for rankings." ></td>
	<td class="line x" title="153:202	 measures the difference between the number of concordant pairs (items the two rankings place in the same order) and discordant pairs (those the rankings place in opposite order), divided by the total number of pairs." ></td>
	<td class="line x" title="154:202	A value of 1 indicates the rankings have perfect correspondence, -1 indicates they are in the opposite order, and 0 indicates they are independent." ></td>
	<td class="line x" title="155:202	The  values are shown in Table 2." ></td>
	<td class="line x" title="156:202	The scatter-plot in Figure 1 shows the  with the greatest magnitude." ></td>
	<td class="line x" title="157:202	However, we can see that even these rankings have barely any relationship." ></td>
	<td class="line x" title="158:202	An objection one might raise is that the lack of correspondence reflects poorly not on these metrics, but upon the grammar induction systems we use to evaluate them." ></td>
	<td class="line x" title="159:202	There might be something about these models in particular which yields these low correlations." ></td>
	<td class="line x" title="160:202	For instance these grammar inducers all estimate their models using EM, which can get caught easily in a local maximum." ></td>
	<td class="line x" title="161:202	To this possibility, we respond by pointing to performance on gold tags, which is consistently high for all grammar induction metrics." ></td>
	<td class="line x" title="162:202	There is clearly some property of the gold tags which is exploited by the grammar induction systems even in the absence of better estimation procedures." ></td>
	<td class="line x" title="163:202	This property is not reflected in the tagging metrics." ></td>
	<td class="line x" title="164:202	The scores for each system for tagging and grammar induction, averaged over the 10 runs, are shown in Table 1." ></td>
	<td class="line x" title="165:202	Additionally, we included runs of the SVD-tagger for 100 and 200 tags, since running this system is still practical with these numbers of tags." ></td>
	<td class="line x" title="166:202	The Ney-Essen with Morphology taggers perform at or near the top on the various tagging metrics, but not well on the grammar induction tasks on average." ></td>
	<td class="line x" title="167:202	HMM-EM seems to perform on average quite well on all the grammar induction tasks, while the SVD-based systems yield the top bracketing F-scores, making use of larger numbers of tags." ></td>
	<td class="line x" title="168:202	Grammar Induction Metrics Tagging CCM CCM+DMV DMV Metrics UF1 DA UA UF1 DA UA 1-to-1 -0.22 -0.04 0.05 -0.13 0.13 0.12 M-to-1 -0.09 0.17 0.24 0.03 0.26 0.25 H(TG|TI) 0.01 0.21 0.27 0.07 0.29 0.28 VI -0.25 -0.17 -0.06 -0.20 0.07 0.07 Table 2: Kendalls , between tag and grammar induction criteria." ></td>
	<td class="line x" title="169:202	4.1 Supervised Experiments One question we might ask is whether these tagging metrics capture information relevant to any parsing task." ></td>
	<td class="line x" title="170:202	We explored this by experimenting with a supervised parser, training off trees where the gold parts-of-speech have been removed and replaced with induced tags." ></td>
	<td class="line x" title="171:202	Our expectation was that the brackets, the head propagation paths, and the phrasal categories in the training trees would be sufficient to overcome any loss in information that the gold tags might provide." ></td>
	<td class="line x" title="172:202	Additionally it was possible the induced tags would ignore rare parts-of-speech such as FW, and make better use of the available tags, perhaps using new distributional clues not in the original tags." ></td>
	<td class="line x" title="173:202	To this end we modified the Charniak Parser (Charniak, 2000) to train off induced parts-ofspeech." ></td>
	<td class="line x" title="174:202	The Charniak parser is a lexicalized PCFG parser for which the part-of-speech of a head word is a key aspect of its model." ></td>
	<td class="line x" title="175:202	During training, the head-paths from the gold part-of-speech tags are retained, but we replace the tags themselves." ></td>
	<td class="line x" title="176:202	We ran experiments using the bitag HMM from Section 2.2 trained using EM, as well as with the Schutze SVD tagger from Section 2.1." ></td>
	<td class="line x" title="177:202	The parser was trained on sections 2-21 of the Penn Treebank for training and section 24 was used for evaluation." ></td>
	<td class="line x" title="178:202	As before we calculated  scores between each tagging metric and supervised f-score." ></td>
	<td class="line x" title="179:202	Unlike the unsupervised evaluation where we used the metric UF1, we use the standard EVALB calculation of unlabeled f-score." ></td>
	<td class="line x" title="180:202	The results are shown in Table 3." ></td>
	<td class="line x" title="181:202	The contrast with the unsupervised case is vast, with very high s for both accuracy metrics." ></td>
	<td class="line x" title="182:202	Consider f-score vs. many-to-one, plotted in Figure 3." ></td>
	<td class="line x" title="183:202	The correspondence here is very clear: taggings with high accuracy do actually reflect on better parser performance." ></td>
	<td class="line x" title="184:202	Note, however, that the correspondence between the information theoretic measures and parsing performance is still rather weak." ></td>
	<td class="line x" title="185:202	334 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5  15 20 25 30 35 40 45 50 55 DMV DA NEMorph HMM-VB 2HMM NE SVD HMM-EM Gold Figure 1: DMV Directed Accuracy vs. H(TG|TI) 0 1 2 3 4 5 6  VI 20 30 40 50 60 70 80 CCM UF1 NEMorph HMM-VB 2HMM NE SVD HMM-EM Gold Figure 2: CCM fscore vs. tagging variation of information." ></td>
	<td class="line x" title="186:202	0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0  M-to-1 0.78 0.80 0.82 0.84 0.86 0.88 0.90 Charniak parser F1 SVD 10 tags SVD 50 tags Gold HMM-EMbitag 10 tags HMM-EMbitag 50 tags Figure 3: Supervised parsing f-score vs. tagging many-to-one accuracy." ></td>
	<td class="line x" title="187:202	Tagging Metric Supervised F1 1-to-1 0.62 M-to-1 0.83 H(TG|TI) -0.19 VI 0.25 Table 3: Kendalls , between tag induction criteria and supervised parsing unlabeled bracketing F-score." ></td>
	<td class="line x" title="188:202	Interestingly, parsing performance and speed does degrade considerably when training off induced tags." ></td>
	<td class="line x" title="189:202	We are not sure what causes this." ></td>
	<td class="line x" title="190:202	One possibility is in the lexicalized stage of the parser, where the probability of a head word is smoothed primarily by its part-of-speech tag." ></td>
	<td class="line x" title="191:202	This requires that the tag be a good proxy for the syntactic role of the head." ></td>
	<td class="line x" title="192:202	In any case this warrants further investigation." ></td>
	<td class="line x" title="193:202	5 Conclusion and Future Work In this work, we found that none of the most common part-of-speech tagging metrics bear a strong relationship to good grammar induction performance." ></td>
	<td class="line x" title="194:202	Although our experiments only involve English, the poor correspondence we find between the various tagging metrics and grammar induction performance raises concerns about their relationship more broadly." ></td>
	<td class="line x" title="195:202	We additionally found that while tagging accuracy measures do correlate with better supervised parsing, common information theoretic ones do not strongly predict better performance on either task." ></td>
	<td class="line x" title="196:202	Furthermore, the supervised experiments indicate that informative part-of-speech tags are important for good parsing." ></td>
	<td class="line x" title="197:202	The next step is to explore better tagging metrics that correspond more strongly to better grammar induction performance." ></td>
	<td class="line x" title="198:202	A good metric should use all the information we have, including the gold trees, to evaluate." ></td>
	<td class="line x" title="199:202	Finally, we should explore grammar induction schemes that do not rely on prior parts-of-speech, instead learning them from raw text at the same time as deeper structure." ></td>
	<td class="line x" title="200:202	Acknowledgments We thank Dan Klein for his grammar induction code, as well as Matt Lease and other members of BLLIP for their feedback." ></td>
	<td class="line x" title="201:202	This work was partially supported by DARPA GALE contract HR0011-062-0001 and NSF award 0631667." ></td>
	<td class="line x" title="202:202	335" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1036
A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers
Gao, Jianfeng;Johnson, Mark;"></td>
	<td class="line x" title="1:151	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 344352, Honolulu, October 2008." ></td>
	<td class="line x" title="2:151	c2008 Association for Computational Linguistics A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers Jianfeng Gao Microsoft Research Redmond, WA, USA jfgao@microsoft.com Mark Johnson Brown Univeristy Providence, RI, USA MarkJohnson@Brown.edu Abstract There is growing interest in applying Bayesian techniques to NLP problems." ></td>
	<td class="line x" title="3:151	There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on." ></td>
	<td class="line x" title="4:151	This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes." ></td>
	<td class="line x" title="5:151	Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM." ></td>
	<td class="line x" title="6:151	We invesigate a variety of samplers for HMMs, including some that these earlier papers did not study." ></td>
	<td class="line x" title="7:151	We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers." ></td>
	<td class="line x" title="8:151	In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets." ></td>
	<td class="line x" title="9:151	1 Introduction Probabilistic models now play a central role in computational linguistics." ></td>
	<td class="line x" title="10:151	These models define a probability distribution P(x) over structures or analyses x. For example, in the part-of-speech (POS) tagging application described in this paper, which involves predicting the part-of-speech tag ti of each word wi in the sentence w = (w1,,wn), the structure x = (w,t) consists of the words w in a sentence together with their corresponding parts-ofspeech t = (t1,,tn)." ></td>
	<td class="line x" title="11:151	In general the probabilistic models used in computational linguistics have adjustable parameters  which determine the distribution P(x | )." ></td>
	<td class="line x" title="12:151	In this paper we focus on bitag Hidden Markov Models (HMMs)." ></td>
	<td class="line x" title="13:151	Since our goal here is to compare algorithms rather than achieve the best performance, we keep the models simple by ignoring morphology and capitalization (two very strong cues in English) and treat each word as an atomic entity." ></td>
	<td class="line x" title="14:151	This means that the model parameters  consist of the HMM stateto-state transition probabilities and the state-to-word emission probabilities." ></td>
	<td class="line x" title="15:151	In virtually all statistical approaches the parameters  are chosen or estimated on the basis of training data d. This paper studies unsupervised estimation, so d = w = (w1,,wn) consists of a sequence of words wi containing all of the words of training corpus appended into a single string, as explained below." ></td>
	<td class="line x" title="16:151	Maximum Likelihood (ML) is the most common estimation method in computational linguistics." ></td>
	<td class="line x" title="17:151	A Maximum Likelihood estimator sets the parameters to the value  that makes the likelihood Ld of the data d as large as possible: Ld() = P(d | )  = argmax  Ld() In this paper we use the Inside-Outside algorithm, which is a specialized form of Expectation344 Maximization, to find HMM parameters which (at least locally) maximize the likelihood function Ld." ></td>
	<td class="line x" title="18:151	Recently there is increasing interest in Bayesian methods in computational linguistics, and the primary goal of this paper is to compare the performance of various Bayesian estimators with each other and with EM." ></td>
	<td class="line x" title="19:151	A Bayesian approach uses Bayes theorem to factorize the posterior distribution P( | d) into the likelihood P(d | ) and the prior P()." ></td>
	<td class="line x" title="20:151	P( | d)  P(d | ) P() Priors can be useful because they can express preferences for certain types of models." ></td>
	<td class="line x" title="21:151	To take an example from our POS-tagging application, most words belong to relatively few parts-of-speech (e.g., most words belong to a single POS, and while there are some words which are both nouns and verbs, very few are prepositions and adjectives as well)." ></td>
	<td class="line x" title="22:151	One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words." ></td>
	<td class="line x" title="23:151	An appropriate Dirichlet prior can express this preference." ></td>
	<td class="line x" title="24:151	While it is possible to use Bayesian inference to find a single model, such as the Maximum A Posteriori or MAP value of  which maximizes the posterior P( | d), this is not necessarily the best approach (Bishop, 2006; MacKay, 2003)." ></td>
	<td class="line x" title="25:151	Instead, rather than commiting to a single value for the parameters  many Bayesians often prefer to work with the full posterior distribution P( | d), as this naturally reflects the uncertainty in s value." ></td>
	<td class="line x" title="26:151	In all but the simplest models there is no known closed form for the posterior distribution." ></td>
	<td class="line x" title="27:151	However, the Bayesian literature describes a number of methods for approximating the posterior P( | d)." ></td>
	<td class="line oc" title="28:151	Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths, 2007; Johnson, 2007)." ></td>
	<td class="line n" title="29:151	These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t | w) of POS tags (i.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods." ></td>
	<td class="line oc" title="30:151	Johnson (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM." ></td>
	<td class="line n" title="31:151	On the other hand, Goldwater and Griffiths (2007) reported that the same kind of Gibbs sampler produced much better results than EM on their unsupervised POS tagging task." ></td>
	<td class="line x" title="32:151	One of the primary motivations for this paper was to understand and resolve the difference in these results." ></td>
	<td class="line o" title="33:151	We replicate the results of both papers and show that the difference in their results stems from differences in the sizes of the training data and numbers of states in their models." ></td>
	<td class="line o" title="34:151	It turns out that the Gibbs sampler used in these earlier papers is not the only kind of sampler for HMMs." ></td>
	<td class="line x" title="35:151	This paper compares the performance of four different kinds of Gibbs samplers, Variational Bayes and Expectation Maximization on unsupervised POS tagging problems of various sizes." ></td>
	<td class="line x" title="36:151	Our goal here is to try to learn how the performance of these different estimators varies as we change the number of hidden states in the HMMs and the size of the training data." ></td>
	<td class="line x" title="37:151	In theory, the Gibbs samplers produce streams of samples that eventually converge on the true posterior distribution, while the Variational Bayes (VB) estimator only produces an approximation to the posterior." ></td>
	<td class="line x" title="38:151	However, as the size of the training data distribution increases the likelihood function and therefore the posterior distribution becomes increasingly peaked, so one would expect this variational approximation to become increasingly accurate." ></td>
	<td class="line x" title="39:151	Further the Gibbs samplers used in this paper should exhibit reduced mobility as the size of training data increases, so as the size of the training data increases eventually the Variational Bayes estimator should prove to be superior." ></td>
	<td class="line x" title="40:151	However the two point-wise Gibbs samplers investigated here, which resample the label of each word conditioned on the labels of its neighbours (amongst other things) only require O(m) steps per sample (where m is the number of HMM states), while EM, VB and the sentence-blocked Gibbs samplers require O(m2) steps per sample." ></td>
	<td class="line x" title="41:151	Thus for HMMs with many states it is possible to perform one or two orders of magnitude more iterations of the 345 point-wise Gibbs samplers in the same run-time as the other samplers, so it is plausible that they would yield better results." ></td>
	<td class="line x" title="42:151	2 Inference for HMMs There are a number of excellent textbook presentations of Hidden Markov Models (Jelinek, 1997; Manning and Schutze, 1999), so we do not present them in detail here." ></td>
	<td class="line x" title="43:151	Conceptually, a Hidden Markov Model uses a Markov model to generate the sequence of states t = (t1,,tn) (which will be interpreted as POS tags), and then generates each word wi conditioned on the corresponding state ti." ></td>
	<td class="line x" title="44:151	We insert endmarkers at the beginning and end of the corpus and between sentence boundaries, and constrain the estimators to associate endmarkers with a special HMM state that never appears elsewhere in the corpus (we ignore these endmarkers during evaluation)." ></td>
	<td class="line x" title="45:151	This means that we can formally treat the training corpus as one long string, yet each sentence can be processed independently by a firstorder HMM." ></td>
	<td class="line x" title="46:151	In more detail, the HMM is specified by a pair of multinomials t and t associated with each state t, where t specifies the distribution over states tprime following t and t specifies the distribution over words w given state t. ti | ti1 = t  Multi(t) wi | ti = t  Multi(t) (1) The Bayesian model we consider here puts a fixed uniform Dirichlet prior on these multinomials." ></td>
	<td class="line x" title="47:151	Because Dirichlets are conjugate to multinomials, this greatly simplifies inference." ></td>
	<td class="line x" title="48:151	t |   Dir() t | prime  Dir(prime) A multinomial  is distributed according to the Dirichlet distribution Dir() iff: P( | )  mproductdisplay j=1 j1j In our experiments we set  and prime to the uniform values (i.e., all components have the same value  or prime), but it is possible to estimate these as well (Goldwater and Griffiths, 2007)." ></td>
	<td class="line x" title="49:151	Informally,  controls the sparsity of the state-to-state transition probabilities while prime controls the sparsity of the state-toword emission probabilities." ></td>
	<td class="line x" title="50:151	As prime approaches zero the prior strongly prefers models in which each state emits as few words as possible, capturing the intuition that most word types only belong to one POS mentioned earlier." ></td>
	<td class="line x" title="51:151	2.1 Expectation Maximization Expectation-Maximization is a procedure that iteratively re-estimates the model parameters (,), converging on a local maximum of the likelihood." ></td>
	<td class="line x" title="52:151	Specifically, if the parameter estimate at iteration lscript is ((lscript),(lscript)), then the re-estimated parameters at iteration lscript+1 are: (lscript+1)tprime|t = E[ntprime,t]/E[nt] (2) (lscript+1)w|t = E[nprimew,t]/E[nt] where nprimew,t is the number of times word w occurs with state t, ntprime,t is the number of times state tprime follows t and nt is the number of occurences of state t; all expectations are taken with respect to the model ((lscript),(lscript))." ></td>
	<td class="line x" title="53:151	The experiments below used the ForwardBackward algorithm (Jelinek, 1997), which is a dynamic programming algorithm for calculating the likelihood and the expectations in (2) in O(nm2) time, where n is the number of words in the training corpus and m is the number of HMM states." ></td>
	<td class="line x" title="54:151	2.2 Variational Bayes Variational Bayesian inference attempts to find a function Q(t,,) that minimizes an upper bound (3) to the negative log likelihood." ></td>
	<td class="line x" title="55:151	logP(w) = log integraldisplay Q(t,,)P(w,t,,)Q(t,,) dtdd  integraldisplay Q(t,,)log P(w,t,,)Q(t,,) dtdd (3) The upper bound (3) is called the Variational Free Energy." ></td>
	<td class="line x" title="56:151	We make a mean-field assumption that the posterior can be well approximated by a factorized model Q in which the state sequence t does not covary with the model parameters ,: P(t,, | w)  Q(t,,) = Q1(t)Q2(,) 346 P(ti|w,ti,,prime)  parenleftbiggnprime wi,ti + prime nti + mprimeprime parenrightbigg parenleftbiggn ti,ti1 +  nti1 + m parenrightbigg parenleftbiggn ti+1,ti +I(ti1 = ti = ti+1) +  nti +I(ti1 = ti)+ m parenrightbigg Figure 1: The conditional distribution for state ti used in the pointwise collapsed Gibbs sampler, which conditions on all states ti except ti (i.e., the counts n do not include ti)." ></td>
	<td class="line x" title="57:151	Here mprime is the size of the vocabulary, m is the number of HMM states and I() is the indicator function (i.e., equal to one if its argument is true and zero otherwise), The calculus of variations is used to minimize the KL divergence between the desired posterior distribution and the factorized approximation." ></td>
	<td class="line x" title="58:151	It turns out that if the likelihood and conjugate prior belong to exponential families then the optimal Q1 and Q2 do too, and there is an EM-like iterative procedure that finds locally-optimal model parameters (Bishop, 2006)." ></td>
	<td class="line x" title="59:151	This procedure is especially attractive for HMM inference, since it involves only a minor modification to the M-step of the Forward-Backward algorithm." ></td>
	<td class="line x" title="60:151	MacKay (1997) and Beal (2003) describe Variational Bayesian (VB) inference for HMMs." ></td>
	<td class="line x" title="61:151	In general, the E-step for VB inference for HMMs is the same as in EM, while the M-step is as follows: (lscript+1)tprime|t = f(E[ntprime,t] + )/f(E[nt] +m) (4) (lscript+1)w|t = f(E[nprimew,t]+ prime)/f(E[nt]+ mprimeprime) f(v) = exp((v)) where mprime and m are the number of word types and states respectively,  is the digamma function and the remaining quantities are as in (2)." ></td>
	<td class="line x" title="62:151	This means that a single iteration can be performed in O(nm2) time, just as for the EM algorithm." ></td>
	<td class="line x" title="63:151	2.3 MCMC sampling algorithms The goal of Markov Chain Monte Carlo (MCMC) algorithms is to produce a stream of samples from the posterior distribution P(t | w,)." ></td>
	<td class="line x" title="64:151	Besag (2004) provides a tutorial on MCMC techniques for HMM inference." ></td>
	<td class="line x" title="65:151	A Gibbs sampler is a simple kind of MCMC algorithm that is well-suited to sampling highdimensional spaces." ></td>
	<td class="line x" title="66:151	A Gibbs sampler for P(z) where z = (z1,,zn) proceeds by sampling and updating each zi in turn from P(zi | zi), where zi = (z1,,zi1,zi+1,,zn), i.e., all of the z except zi (Geman and Geman, 1984; Robert and Casella, 2004)." ></td>
	<td class="line x" title="67:151	We evaluate four different Gibbs samplers in this paper, which vary along two dimensions." ></td>
	<td class="line x" title="68:151	First, the sampler can either be pointwise or blocked." ></td>
	<td class="line x" title="69:151	A pointwise sampler resamples a single state ti (labeling a single word wi) at each step, while a blocked sampler resamples the labels for all of the words in a sentence at a single step using a dynamic programming algorithm based on the Forward-Backward algorithm." ></td>
	<td class="line x" title="70:151	(In principle it is possible to use block sizes other than the sentence, but we did not explore this here)." ></td>
	<td class="line x" title="71:151	A pointwise sampler requires O(nm) time per iteration, while a blocked sampler requires O(nm2) time per iteration, where m is the number of HMM states and n is the length of the training corpus." ></td>
	<td class="line x" title="72:151	Second, the sampler can either be explicit or collapsed." ></td>
	<td class="line x" title="73:151	An explicit sampler represents and samples the HMM parameters  and  in addition to the states t, while in a collapsed sampler the HMM parameters are integrated out, and only the states t are sampled." ></td>
	<td class="line oc" title="74:151	The difference between explicit and collapsed samplers corresponds exactly to the difference between the two PCFG sampling algorithms presented in Johnson et al.(2007)." ></td>
	<td class="line x" title="76:151	An iteration of the pointwise explicit Gibbs sampler consists of resampling  and  given the stateto-state transition counts n and state-to-word emission counts nprime using (5), and then resampling each state ti given the corresponding word wi and the neighboring states ti1 and ti+1 using (6)." ></td>
	<td class="line oc" title="77:151	t | nt,  Dir(nt +) t | nprimet,prime  Dir(nprimet +prime) (5) P(ti | wi,ti,,)  ti|ti1wi|titi+1|ti (6) The Dirichlet distributions in (5) are non-uniform; nt is the vector of state-to-state transition counts in t leaving state t in the current state vector t, while 347 nprimet is the vector of state-to-word emission counts for state t. See Johnson et al.(2007) for a more detailed explanation, as well as an algorithm for sampling from the Dirichlet distributions in (5)." ></td>
	<td class="line oc" title="79:151	The samplers that Goldwater and Griffiths (2007) and Johnson (2007) describe are pointwise collapsed Gibbs samplers." ></td>
	<td class="line x" title="80:151	Figure 1 gives the sampling distribution for this sampler." ></td>
	<td class="line oc" title="81:151	As Johnson et al.(2007) explains, samples of the HMM parameters  and  can be obtained using (5) if required." ></td>
	<td class="line x" title="83:151	The blocked Gibbs samplers differ from the pointwise Gibbs samplers in that they resample the POS tags for an entire sentence at a time." ></td>
	<td class="line x" title="84:151	Besag (2004) describes the well-known dynamic programming algorithm (based on the Forward-Backward algorithm) for sampling a state sequence t given the words w and the transition and emission probabilities  and ." ></td>
	<td class="line x" title="85:151	At each iteration the explicit blocked Gibbs sampler resamples  and  using (5), just as the explicit pointwise sampler does." ></td>
	<td class="line x" title="86:151	Then it uses the new HMM parameters to resample the states t for the training corpus using the algorithm just mentioned." ></td>
	<td class="line x" title="87:151	This can be done in parallel for each sentence in the training corpus." ></td>
	<td class="line oc" title="88:151	The collapsed blocked Gibbs sampler is a straight-forward application of the Metropoliswithin-Gibbs approach proposed by Johnson et al.(2007) for PCFGs, so we only sketch it here." ></td>
	<td class="line x" title="90:151	We iterate through the sentences of the training data, resampling the states for each sentence conditioned on the state-to-state transition counts n and stateto-word emission counts nprime for the other sentences in the corpus." ></td>
	<td class="line x" title="91:151	This is done by first computing the parameters star and star of a proposal HMM using (7)." ></td>
	<td class="line x" title="92:151	startprime|t = ntprime,t + n t + m (7) starw|t = n primew,t + prime nt + mprime Then we use the dynamic programming sampler described above to produce a proposal state sequence tstar for the words in the sentence." ></td>
	<td class="line x" title="93:151	Finally, we use a Metropolis-Hastings accept-reject step to decide whether to update the current state sequence for the sentence with the proposal tstar, or whether to keep the current state sequence." ></td>
	<td class="line x" title="94:151	In practice, with all but the very smallest training corpora the acceptance rate is very high; the acceptance rate for all of our collapsed blocked Gibbs samplers was over 99%." ></td>
	<td class="line x" title="95:151	3 Evaluation The previous section described six different unsupervised estimators for HMMs." ></td>
	<td class="line x" title="96:151	In this section we compare their performance for English part-ofspeech tagging." ></td>
	<td class="line x" title="97:151	One of the difficulties in evaluating unsupervised taggers such as these is mapping the systems states to the gold-standard partsof-speech." ></td>
	<td class="line x" title="98:151	Goldwater and Griffiths (2007) proposed an information-theoretic measure known as the Variation of Information (VI) described by Meila (2003) as an evaluation of an unsupervised tagging." ></td>
	<td class="line x" title="99:151	However as Goldwater (p.c.) points out, this may not be an ideal evaluation measure; e.g., a tagger which assigns all words the same single part-of-speech tag does disturbingly well under Variation of Information, suggesting that a poor tagger may score well under VI." ></td>
	<td class="line x" title="100:151	In order to avoid this problem we focus here on evaluation measures that construct an explicit mapping between the gold-standard part-of-speech tags and the HMMs states." ></td>
	<td class="line x" title="101:151	Perhaps the most straightforward approach is to map each HMM state to the part-of-speech tag it co-occurs with most frequently, and use this mapping to map each HMM state sequence t to a sequence of part-of-speech tags." ></td>
	<td class="line x" title="102:151	But as Clark (2003) observes, this approach has several defects." ></td>
	<td class="line x" title="103:151	If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state." ></td>
	<td class="line x" title="104:151	We can partially address this by cross-validation." ></td>
	<td class="line x" title="105:151	We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech." ></td>
	<td class="line x" title="106:151	We call the accuracy of the resulting tagging the crossvalidation accuracy." ></td>
	<td class="line oc" title="107:151	Finally, following Haghighi and Klein (2006) and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag." ></td>
	<td class="line o" title="108:151	Following these authors, we used a greedy algorithm to associate states with POS tags; the accuracy of the resulting tagging is called the greedy 1-to-1 348 All50 All17 120K50 120K17 24K50 24K17 EM 0.40527 0.43101 0.29303 0.35202 0.18618 0.28165 VB 0.46123 0.51379 0.34679 0.36010 0.23823 0.36599 GSe,p 0.47826 0.43424 0.36984 0.44125 0.29953 0.36811 GSe,b 0.49371 0.46568 0.38888 0.44341 0.34404 0.37032 GSc,p 0.49910star 0.45028 0.42785 0.43652 0.39182 0.39164 GSc,b 0.49486star 0.46193 0.41162 0.42278 0.38497 0.36793 Figure 2: Average greedy 1-to-1 accuracy of state sequences produced by HMMs estimated by the various estimators." ></td>
	<td class="line x" title="109:151	The column heading indicates the size of the corpus and the number of HMM states." ></td>
	<td class="line x" title="110:151	In the Gibbs sampler (GS) results the subscript e indicates that the parameters  and  were explicitly sampled while the subscript c indicates that they were integrated out, and the subscript p indicates pointwise sampling, while b indicates sentence-blocked sampling." ></td>
	<td class="line x" title="111:151	Entries tagged with a star indicate that the estimator had not converged after weeks of run-time, but was still slowly improving." ></td>
	<td class="line x" title="112:151	All50 All17 120K 50 120K 17 24K50 24K17 EM 0.62115 0.64651 0.44135 0.56215 0.28576 0.46669 VB 0.60484 0.63652 0.48427 0.36458 0.35946 0.36926 GSe,p 0.64190 0.63057 0.53571 0.46986 0.41620 0.37165 GSe,b 0.65953 0.65606 0.57918 0.48975 0.47228 0.37311 GSc,p 0.61391star 0.67414 0.65285 0.65012 0.58153 0.62254 GSc,b 0.60551star 0.65516 0.62167 0.58271 0.55006 0.58728 Figure 3: Average cross-validation accuracy of state sequences produced by HMMs estimated by the various estimators." ></td>
	<td class="line x" title="113:151	The table headings follow those used in Figure 2." ></td>
	<td class="line x" title="114:151	All50 All17 120K50 120K17 24K50 24K17 EM 4.47555 3.86326 6.16499 4.55681 7.72465 5.42815 VB 4.27911 3.44029 5.00509 3.19670 4.80778 3.14557 GSe,p 4.24919 3.53024 4.30457 3.23082 4.24368 3.17076 GSe,b 4.04123 3.46179 4.22590 3.20276 4.29474 3.10609 GSc,p 4.03886star 3.52185 4.21259 3.17586 4.30928 3.18273 GSc,b 4.11272star 3.61516 4.36595 3.23630 4.32096 3.17780 Figure 4: Average Variation of Information between the state sequences produced by HMMs estimated by the various estimators and the gold tags (smaller is better)." ></td>
	<td class="line x" title="115:151	The table headings follow those used in Figure 2." ></td>
	<td class="line x" title="116:151	All50 All17 120K50 120K17 24K50 24K17 EM 558 346 648 351 142 125 VB 473 123 337 24 183 20 GSe,p 2863 382 3709 63 2500 177 GSe,b 3846 286 5169 154 4856 139 GSc,p star 34325 44864 40088 45285 43208 GSc,b star 6948 7502 7782 7342 7985 Figure 5: Average number of iterations until the negative logarithm of the posterior probability (or likelihood) changes by less than 0.5% (smaller is better) per at least 2,000 iterations." ></td>
	<td class="line x" title="117:151	No annealing was used." ></td>
	<td class="line x" title="118:151	349 explicit, pointwise explicit, blocked collapsed, pointwise collapsed,blocked All data, 50 states,  = prime = 0.1 computing time (seconds) l og po ste rio rp rob ab ilit y 50000400003000020000100000 8.1e+06 8.05e+06 8e+06 7.95e+06 7.9e+06 7.85e+06 explicit, pointwise explicit, blocked collapsed, pointwise collapsed,blocked All data, 50 states,  = prime = 0.1 computing time (seconds) Gr eed y1 -to -1 acc ura cy 50000400003000020000100000 0.58 0.56 0.54 0.52 0.5 0.48 0.46 0.44 0.42 0.4 Figure 6: Variation in (a) negative log likelihood and (b) 1-to-1 accuracy as a function of running time on a 3GHz dual quad-core Pentium for the four different Gibbs samplers on all data and 50 hidden states." ></td>
	<td class="line x" title="119:151	Each iteration took approximately 96 sec." ></td>
	<td class="line x" title="120:151	for the collapsed blocked sampler, 7.5 sec." ></td>
	<td class="line x" title="121:151	for the collapsed pointwise sampler, 25 sec." ></td>
	<td class="line x" title="122:151	for the explicit blocked sampler and 4.4 sec." ></td>
	<td class="line x" title="123:151	for the explicit pointwise sampler." ></td>
	<td class="line x" title="124:151	350 accuracy." ></td>
	<td class="line oc" title="125:151	The studies presented by Goldwater and Griffiths (2007) and Johnson (2007) differed in the number of states that they used." ></td>
	<td class="line oc" title="126:151	Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set." ></td>
	<td class="line x" title="127:151	We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set)." ></td>
	<td class="line o" title="128:151	Also, the studies differed in the size of the corpora used." ></td>
	<td class="line oc" title="129:151	The largest corpus that Goldwater and Griffiths (2007) studied contained 96,000 words, while Johnson (2007) used all of the 1,173,766 words in the full Penn WSJ treebank." ></td>
	<td class="line x" title="130:151	For that reason we ran all our estimators on corpora containing 24,000 words and 120,000 words as well as the full treebank." ></td>
	<td class="line oc" title="131:151	We ran each estimator with the eight different combinations of values for the hyperparameters  and prime listed below, which include the optimal values for the hyperparameters found by Johnson (2007), and report results for the best combination for each estimator below 1." ></td>
	<td class="line oc" title="132:151	 prime 1 1 1 0.5 0.5 1 0.5 0.5 0.1 0.1 0.1 0.0001 0.0001 0.1 0.0001 0.0001 Further, we ran each setting of each estimator at least 10 times (from randomly jittered initial starting points) for at least 1,000 iterations, as Johnson (2007) showed that some estimators require many iterations to converge." ></td>
	<td class="line x" title="133:151	The results of our experiments are summarized in Figures 25." ></td>
	<td class="line x" title="134:151	1We found that on some data sets the results are sensitive to the values of the hyperparameters." ></td>
	<td class="line x" title="135:151	So, there is a bit uncertainty in our comparison results because it is possible that the values we tried were good for one estimator and bad for others." ></td>
	<td class="line x" title="136:151	Unfortunately, we do not know any efficient way of searching the optimal hyperparameters in a much wider and more fine-grained space." ></td>
	<td class="line x" title="137:151	We leave it to future work." ></td>
	<td class="line x" title="138:151	4 Conclusion and future work As might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear." ></td>
	<td class="line x" title="139:151	On small data sets all of the Bayesian estimators strongly outperform EM (and, to a lesser extent, VB) with respect to all of our evaluation measures, confirming the results reported in Goldwater and Griffiths (2007)." ></td>
	<td class="line x" title="140:151	This is perhaps not too surprising, as the Bayesian prior plays a comparatively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets." ></td>
	<td class="line x" title="141:151	But on larger data sets, which Goldwater et al did not study, the results are much less clear, and depend on which evaluation measure is used." ></td>
	<td class="line pc" title="142:151	Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson (2007)." ></td>
	<td class="line x" title="143:151	Variational Bayes converges faster than all of the other estimators we examined here." ></td>
	<td class="line x" title="144:151	We found that the speed of convergence of our samplers depends to a large degree upon the values of the hyperparameters  and prime, with larger values leading to much faster convergence." ></td>
	<td class="line x" title="145:151	This is not surprising, as the  and prime specify how likely the samplers are to consider novel tags, and therefore directly influence the samplers mobility." ></td>
	<td class="line x" title="146:151	However, in our experiments the best results are obtained in most settings with small values for  and prime, usually between 0.1 and 0.0001." ></td>
	<td class="line x" title="147:151	In terms of time to convergence, on larger data sets we found that the blocked samplers were generally faster than the pointwise samplers, and that the explicit samplers (which represented and sampled  and ) were faster than the collapsed samplers, largely because the time saved in not computing probabilities on the fly overwhelmed the time spent resampling the parameters." ></td>
	<td class="line x" title="148:151	Of course these experiments only scratch the surface of what is possible." ></td>
	<td class="line x" title="149:151	Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers." ></td>
	<td class="line x" title="150:151	Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here." ></td>
	<td class="line x" title="151:151	351" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1109
Unsupervised Multilingual Learning for POS Tagging
Snyder, Benjamin;Naseem, Tahira;Eisenstein, Jacob;Barzilay, Regina;"></td>
	<td class="line x" title="1:232	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 10411050, Honolulu, October 2008." ></td>
	<td class="line x" title="2:232	c2008 Association for Computational Linguistics Unsupervised Multilingual Learning for POS Tagging Benjamin Snyder and Tahira Naseem and Jacob Eisenstein and Regina Barzilay Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology 77 Massachusetts Ave., Cambridge MA 02139 {bsnyder, tahira, jacobe, regina}@csail.mit.edu Abstract We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging." ></td>
	<td class="line x" title="3:232	The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent." ></td>
	<td class="line x" title="4:232	We formulate a hierarchical Bayesian model for jointly predicting bilingual streams of part-of-speech tags." ></td>
	<td class="line x" title="5:232	The model learns language-specific features while capturing cross-lingual patterns in tag distribution for aligned words." ></td>
	<td class="line x" title="6:232	Once the parameters of our model have been learned on bilingual parallel data, we evaluate its performance on a held-out monolingual test set." ></td>
	<td class="line x" title="7:232	Our evaluation on six pairs of languages shows consistent and significant performance gains over a state-of-the-art monolingual baseline." ></td>
	<td class="line x" title="8:232	For one language pair, we observe a relative reduction in error of 53%." ></td>
	<td class="line x" title="9:232	1 Introduction In this paper, we explore the application of multilingual learning to part-of-speech tagging when no annotation is available." ></td>
	<td class="line x" title="10:232	This core task has been studied in an unsupervised monolingual framework for over a decade and is still an active area of research." ></td>
	<td class="line x" title="11:232	In this paper, we demonstrate the effectiveness of multilingual learning when applied to both closely related and distantly related language pairs." ></td>
	<td class="line x" title="12:232	We further analyze the language features which lead to robust bilingual performance." ></td>
	<td class="line x" title="13:232	The fundamental idea upon which our work is based is that the patterns of ambiguity inherent in part-of-speech tag assignments differ across languages." ></td>
	<td class="line x" title="14:232	At the lexical level, a word with part-ofspeech tag ambiguity in one language may correspond to an unambiguous word in the other language." ></td>
	<td class="line x" title="15:232	For example, the word can in English may function as an auxiliary verb, a noun, or a regular verb." ></td>
	<td class="line x" title="16:232	However, each of the corresponding functions in Serbian is expressed with a distinct lexical item." ></td>
	<td class="line x" title="17:232	Languages also differ in their patterns of structural ambiguity." ></td>
	<td class="line x" title="18:232	For example, the presence of an article in English greatly reduces the ambiguity of the succeeding tag." ></td>
	<td class="line x" title="19:232	In Serbian, a language without articles, this constraint is obviously absent." ></td>
	<td class="line x" title="20:232	The key idea of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent." ></td>
	<td class="line x" title="21:232	While multilingual learning can address ambiguities in each language, it must be flexible enough to accommodate cross-lingual variations such as tag inventory and syntactic structure." ></td>
	<td class="line x" title="22:232	As a result of such variations, two languages often select and order their tags differently even when expressing the same meaning." ></td>
	<td class="line x" title="23:232	A key challenge of multilingual learning is to model language-specific structure while allowing information to flow between languages." ></td>
	<td class="line x" title="24:232	We jointly model bilingual part-of-speech tag sequences in a hierarchical Bayesian framework." ></td>
	<td class="line x" title="25:232	For each word, we posit a hidden tag state which generates the word as well as the succeeding tag." ></td>
	<td class="line x" title="26:232	In addition, the tags of words with common semantic or syntactic function in parallel sentences are combined into bilingual nodes representing the tag pair." ></td>
	<td class="line x" title="27:232	These joined nodes serve as anchors that create probabilistic dependencies between the tag se1041 quences in each language." ></td>
	<td class="line x" title="28:232	We use standard tools from machine translation to discover aligned wordpairs, and thereafter our model treats the alignments as observed data." ></td>
	<td class="line x" title="29:232	Our model structure allows language-specific tag inventories." ></td>
	<td class="line x" title="30:232	Additionally, it assumes only that the tags at joined nodes are correlated; they need not be identical." ></td>
	<td class="line x" title="31:232	We factor the conditional probabilities of joined nodes into two individual transition probabilities as well as a coupling probability." ></td>
	<td class="line x" title="32:232	We define priors over the transition, emission, and coupling parameters and perform Bayesian inference using Gibbs sampling and the Metropolis-Hastings algorithm." ></td>
	<td class="line x" title="33:232	We evaluate our model on a parallel corpus of four languages: English, Bulgarian, Serbian, and Slovene." ></td>
	<td class="line x" title="34:232	For each of the six language pairs, we train a bilingual model on this corpus, and evaluate it on held-out monolingual test sets." ></td>
	<td class="line x" title="35:232	Our results show consistent improvement over a monolingual baseline for all languages and all pairings." ></td>
	<td class="line x" title="36:232	In fact, for one language pair  Serbian and Slovene  the error is reduced by over 53%." ></td>
	<td class="line x" title="37:232	Moreover, the multilingual model significantly reduces the gap between unsupervised and supervised performance." ></td>
	<td class="line x" title="38:232	For instance, in the case of Slovene this gap is reduced by 71%." ></td>
	<td class="line x" title="39:232	We also observe significant variation in the level of improvement across language pairs." ></td>
	<td class="line x" title="40:232	We show that a cross-lingual entropy measure corresponds with the observed differentials in performance." ></td>
	<td class="line x" title="41:232	2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation." ></td>
	<td class="line x" title="42:232	Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005)." ></td>
	<td class="line x" title="43:232	Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003)." ></td>
	<td class="line x" title="44:232	When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pado and Lapata, 2006; Xi and Hwa, 2005)." ></td>
	<td class="line x" title="45:232	In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006)." ></td>
	<td class="line x" title="46:232	In contrast, our own work assumes that annotations exist for neither language." ></td>
	<td class="line x" title="47:232	Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008)." ></td>
	<td class="line x" title="48:232	In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging." ></td>
	<td class="line x" title="49:232	Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004)." ></td>
	<td class="line oc" title="50:232	Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007)." ></td>
	<td class="line x" title="51:232	In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features." ></td>
	<td class="line x" title="52:232	In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006)." ></td>
	<td class="line x" title="53:232	Our focus is on developing a simple model that effectively incorporates multilingual evidence." ></td>
	<td class="line x" title="54:232	We view this direction as orthogonal to refining monolingual tagging models for any particular language." ></td>
	<td class="line x" title="55:232	3 Model We propose a bilingual model for unsupervised partof-speech tagging that jointly tags parallel streams of text in two languages." ></td>
	<td class="line x" title="56:232	Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set." ></td>
	<td class="line x" title="57:232	Our key hypothesis is that the patterns of ambiguity found in each language at the part-of-speech level will differ in systematic ways; by considering multiple language simultaneously, the total inherent ambiguity can be reduced in each language." ></td>
	<td class="line x" title="58:232	The model is designed to permit information to flow across the 1042 I love fish J' adore les poissons x1 y1 x2 y2 y3 y4 x3 I love fish J' adore les poissons x1/y1 x1/y1 x1/y1 y3 (a) (b) Figure 1: (a) Graphical structure of two standard monolingual HMMs." ></td>
	<td class="line x" title="59:232	(b) Graphical structure of our bilingual model based on word alignments." ></td>
	<td class="line x" title="60:232	language barrier, while respecting language-specific idiosyncrasies such as tag inventory, selection, and order." ></td>
	<td class="line x" title="61:232	We assume that for pairs of words that share similar semantic or syntactic function, the associated tags will be statistically correlated, though not necessarily identical." ></td>
	<td class="line x" title="62:232	We use such word pairs as the bilingual anchors of our model, allowing crosslingual information to be shared via joint tagging decisions." ></td>
	<td class="line x" title="63:232	We use standard tools from machine translation to identify these aligned words, and thereafter our model treats them as fixed and observed data." ></td>
	<td class="line x" title="64:232	To avoid cycles, we remove crossing edges from the alignments." ></td>
	<td class="line x" title="65:232	For unaligned parts of the sentence, the tag and word selections are identical to standard monolingual HMMs. Figure 1 shows an example of the bilingual graphical structure we use, in comparison to two independent monolingual HMMs. We formulate a hierarchical Bayesian model that exploits both language-specific and cross-lingual patterns to explain the observed bilingual sentences." ></td>
	<td class="line x" title="66:232	We present a generative story in which the observed words are produced by the hidden tags and model parameters." ></td>
	<td class="line x" title="67:232	In Section 4, we describe how to infer the posterior distribution over these hidden variables, given the observations." ></td>
	<td class="line x" title="68:232	3.1 Generative Model Our generative model assumes the existence of two tagsets, T and T, and two vocabularies W and W, one of each for each language." ></td>
	<td class="line x" title="69:232	For ease of exposition, we formulate our model with bigram tag dependencies." ></td>
	<td class="line x" title="70:232	However, in our experiments we used a trigram model, which is a trivial extension of the model discussed here and in the next section." ></td>
	<td class="line x" title="71:232	1." ></td>
	<td class="line x" title="72:232	For each tag t  T, draw a transition distribution t over tags T, and an emission distribution t over words W, both from symmetric Dirichlet priors.1 2." ></td>
	<td class="line x" title="73:232	For each tag t  T, draw a transition distribution t over tags T, and an emission distribution t over words W, both from symmetric Dirichlet priors." ></td>
	<td class="line x" title="74:232	3." ></td>
	<td class="line x" title="75:232	Draw a bilingual coupling distribution  over tag pairs T  T from a symmetric Dirichlet prior." ></td>
	<td class="line x" title="76:232	4." ></td>
	<td class="line x" title="77:232	For each bilingual parallel sentence: (a) Draw an alignment a from an alignment distribution A (see the following paragraph for formal definitions of a and A), (b) Draw a bilingual sequence of part-ofspeech tags (x1,,xm), (y1,,yn) according to: P(x1,,xm, y1,,yn|a,,,)." ></td>
	<td class="line x" title="78:232	2 This joint distribution is given in equation 1." ></td>
	<td class="line x" title="79:232	1The Dirichlet is a probability distribution over the simplex, and is conjugate to the multinomial (Gelman et al., 2004)." ></td>
	<td class="line x" title="80:232	2Note that we use a special end state rather than explicitly modeling sentence length." ></td>
	<td class="line x" title="81:232	Thus the values of m and n depend on the draw." ></td>
	<td class="line x" title="82:232	1043 (c) For each part-of-speech tag xi in the first language, emit a word from W: ei  xi, (d) For each part-of-speech tag yj in the second language, emit a word from W: fj  yj . We define an alignment a to be a set of one-toone integer pairs with no crossing edges." ></td>
	<td class="line x" title="83:232	Intuitively, each pair (i,j)  a indicates that the words ei and fj share some common role in the bilingual parallel sentences." ></td>
	<td class="line x" title="84:232	In our experiments, we assume that alignments are directly observed and we hold them fixed." ></td>
	<td class="line x" title="85:232	From the perspective of our generative model, we treat alignments as drawn from a distribution A, about which we remain largely agnostic." ></td>
	<td class="line x" title="86:232	We only require that A assign zero probability to alignments which either: (i) align a single index in one language to multiple indices in the other language or (ii) contain crossing edges." ></td>
	<td class="line x" title="87:232	The resulting alignments are thus one-to-one, contain no crossing edges, and may be sparse or even possibly empty." ></td>
	<td class="line x" title="88:232	Our technique for obtaining alignments that display these properties is described in Section 5." ></td>
	<td class="line x" title="89:232	Given an alignment a and sets of transition parameters  and , we factor the conditional probability of a bilingual tag sequence (x1,xm), (y1,,yn) into transition probabilities for unaligned tags, and joint probabilities over aligned tag pairs: P(x1,,xm, y1,,yn|a,,,) =productdisplay unaligned i xi1(xi)  productdisplay unaligned j yj1(yj)  productdisplay (i,j)a P(xi,yj|xi1,yj1,,,) (1) Because the alignment contains no crossing edges, we can model the tags as generated sequentially by a stochastic process." ></td>
	<td class="line x" title="90:232	We define the distribution over aligned tag pairs to be a product of each languages transition probability and the coupling probability: P(xi,yj|xi1,yj1,,,) = xi1(xi) yj1(yj) (xi,yj) Z (2) The normalization constant here is defined as: Z = summationdisplay x,y xi1(x) yj1(y) (x,y) This factorization allows the language-specific transition probabilities to be shared across aligned and unaligned tags." ></td>
	<td class="line x" title="91:232	In the latter case, the addition of the coupling parameter  gives the tag pair an additional role: that of multilingual anchor." ></td>
	<td class="line x" title="92:232	In essence, the probability of the aligned tag pair is a product of three experts: the two transition parameters and the coupling parameter." ></td>
	<td class="line x" title="93:232	Thus, the combination of a high probability transition in one language and a high probability coupling can resolve cases of inherent transition uncertainty in the other language." ></td>
	<td class="line x" title="94:232	In addition, any one of the three parameters can veto a tag pair to which it assigns low probability." ></td>
	<td class="line x" title="95:232	To perform inference in this model, we predict the bilingual tag sequences with maximal probability given the observed words and alignments, while integrating over the transition, emission, and coupling parameters." ></td>
	<td class="line x" title="96:232	To do so, we use a combination of sampling-based techniques." ></td>
	<td class="line x" title="97:232	4 Inference The core element of our inference procedure is Gibbs sampling (Geman and Geman, 1984)." ></td>
	<td class="line x" title="98:232	Gibbs sampling begins by randomly initializing all unobserved random variables; at each iteration, each random variable zi is sampled from the conditional distribution P(zi|zi), where zi refers to all variables other than zi." ></td>
	<td class="line x" title="99:232	Eventually, the distribution over samples drawn from this process will converge to the unconditional joint distribution P(z) of the unobserved variables." ></td>
	<td class="line x" title="100:232	When possible, we avoid explicitly sampling variables which are not of direct interest, but rather integrate over themthis technique is known as collapsed sampling, and can reduce variance (Liu, 1994)." ></td>
	<td class="line x" title="101:232	We sample: (i) the bilingual tag sequences (x,y), (ii) the two sets of transition parameters  and , and (iii) the coupling parameter ." ></td>
	<td class="line x" title="102:232	We integrate over the emission parameters  and , whose priors are Dirichlet distributions with hyperparameters 0 and 0." ></td>
	<td class="line x" title="103:232	The resulting emission distribution over words ei, given the other words ei, the tag sequences x 1044 and the emission prior 0, can easily be derived as: P(ei|x,ei,0) = integraldisplay xi xi(ei)P(xi|0) dxi = n(xi,ei) + 0n(x i) + Wxi0 (3) Here, n(xi) is the number of occurrences of the tag xi in xi, n(xi,ei) is the number of occurrences of the tag-word pair (xi,ei) in (xi,ei), and Wxi is the number of word types in the vocabulary W that can take tag xi." ></td>
	<td class="line x" title="104:232	The integral is tractable due to Dirichlet-multinomial conjugacy (Gelman et al., 2004)." ></td>
	<td class="line x" title="105:232	We will now discuss, in turn, each of the variables that we sample." ></td>
	<td class="line x" title="106:232	Note that in all cases we condition on the other sampled variables as well as the observed words and alignments, e, f and a, which are kept fixed throughout." ></td>
	<td class="line x" title="107:232	4.1 Sampling Part-of-speech Tags This section presents the conditional distributions that we sample from to obtain the part-of-speech tags." ></td>
	<td class="line x" title="108:232	Depending on the alignment, there are several scenarios." ></td>
	<td class="line x" title="109:232	In the simplest case, both the tag to be sampled and its succeeding tag are not aligned to any tag in the other language." ></td>
	<td class="line x" title="110:232	If so, the sampling distribution is identical to the monolingual case, including only terms for the emission (defined in equation 3), and the preceding and succeeding transitions: P(xi|xi,y,e,f,a,,,,0,0)  P(ei|x,ei,0) xi1(xi) xi(xi+1)." ></td>
	<td class="line x" title="111:232	For an aligned tag pair (xi,yj), we sample the identity of the tags jointly." ></td>
	<td class="line x" title="112:232	By applying the chain rule we obtain terms for the emissions in both languages and a joint term for the transition probabilities: P(xi,yj|xi,yj,e,f,a,,,,0,0)  P(ei|x,ei,0)P(fj|y,fj,0) P(xi,yj|xi,yj,a,,,) The expansion of the joint term depends on the alignment of the succeeding tags." ></td>
	<td class="line x" title="113:232	In the case that the successors are not aligned, we have a product of the bilingual coupling probability and four transition probabilities (preceding and succeeding transitions in each language): P(xi,yj|xi,yj,a,,,)  (xi,yj)xi1(xi) yj1(yj) xi(xi+1) yj(yj+1) Whenever one or more of the succeeding tags is aligned, the sampling formulas must account for the effect of the sampled tag on the joint probability of the succeeding tags, which is no longer a simple multinomial transition probability." ></td>
	<td class="line x" title="114:232	We give the formula for one such casewhen we are sampling an aligned tag pair (xi,yj), whose succeeding tags (xi+1,yj+1) are also aligned to one another: P(xi,yj|xi,yj,a,,,)  (xi,yj) xi1(xi)yj1(yj) bracketleftBigg  xi(xi+1)yj(yj+1)summationtext x,y xi(x)yj(y)(x,y) bracketrightBigg Similar equations can be derived for cases where the succeeding tags are not aligned to each other, but to other tags." ></td>
	<td class="line x" title="115:232	4.2 Sampling Transition Parameters and the Coupling Parameter When computing the joint probability of an aligned tag pair (Equation 2), we employ the transition parameters ,  and the coupling parameter  in a normalized product." ></td>
	<td class="line x" title="116:232	Because of this, we can no longer regard these parameters as simple multinomials, and thus can no longer sample them using the standard closed formulas." ></td>
	<td class="line x" title="117:232	Instead, to resample these parameters, we resort to the Metropolis-Hastings algorithm as a subroutine within Gibbs sampling (Hastings, 1970)." ></td>
	<td class="line x" title="118:232	Metropolis-Hastings is a Markov chain sampling technique that can be used when it is impossible to directly sample from the posterior." ></td>
	<td class="line x" title="119:232	Instead, samples are drawn from a proposal distribution and then stochastically accepted or rejected on the basis of: their likelihood, their probability under the proposal distribution, and the likelihood and proposal probability of the previous sample." ></td>
	<td class="line x" title="120:232	We use a form of Metropolis-Hastings known as an independent sampler." ></td>
	<td class="line x" title="121:232	In this setup, the proposal distribution does not depend on the value of the previous sample, although the accept/reject decision 1045 does depend on the previous model likelihood." ></td>
	<td class="line x" title="122:232	More formally, if we denote the proposal distribution as Q(z), the target distribution as P(z), and the previous sample as z, then the probability of accepting a new sample z  Q is set at: min braceleftbigg 1, P(z ) Q(z) P(z) Q(z) bracerightbigg Theoretically any non-degenerate proposal distribution may be used." ></td>
	<td class="line x" title="123:232	However, a higher acceptance rate and faster convergence is achieved when the proposal Q is a close approximation of P. For a particular transition parameter x, we define our proposal distribution Q to be Dirichlet with parameters set to the bigram counts of the tags following x in the sampled tag data." ></td>
	<td class="line x" title="124:232	Thus, the proposal distribution for x has a mean proportional to these counts, and is thus likely to be a good approximation to the target distribution." ></td>
	<td class="line x" title="125:232	Likewise for the coupling parameter , we define a Dirichlet proposal distribution." ></td>
	<td class="line x" title="126:232	This Dirichlet is parameterized by the counts of aligned tag pairs (x,y) in the current set of tag samples." ></td>
	<td class="line x" title="127:232	Since this sets the mean of the proposal to be proportional to these counts, this too is likely to be a good approximation to the target distribution." ></td>
	<td class="line x" title="128:232	4.3 Hyperparameter Re-estimation After every iteration of Gibbs sampling the hyperparameters 0 and 0 are re-estimated using a single Metropolis-Hastings move." ></td>
	<td class="line x" title="129:232	The proposal distribution is set to a Gaussian with mean at the current value and variance equal to one tenth of the mean." ></td>
	<td class="line x" title="130:232	5 Experimental Set-Up Our evaluation framework follows the standard procedures established for unsupervised part-of-speech tagging." ></td>
	<td class="line x" title="131:232	Given a tag dictionary (i.e., a set of possible tags for each word type), the model has to select the appropriate tag for each token occurring in a text." ></td>
	<td class="line x" title="132:232	We also evaluate tagger performance when only incomplete dictionaries are available (Smith and Eisner, 2005; Goldwater and Griffiths, 2007)." ></td>
	<td class="line x" title="133:232	In both scenarios, the model is trained only using untagged text." ></td>
	<td class="line x" title="134:232	In this section, we first describe the parallel data and part-of-speech annotations used for system evaluation." ></td>
	<td class="line x" title="135:232	Next we describe a monolingual baseline and our procedures for initialization and hyperparameter setting." ></td>
	<td class="line x" title="136:232	Data As a source of parallel data, we use Orwells novel Nineteen Eighty Four in the original English as well as translations to three Slavic languages  Bulgarian, Serbian and Slovene." ></td>
	<td class="line x" title="137:232	This data is distributed as part of the Multext-East corpus which is publicly available." ></td>
	<td class="line x" title="138:232	The corpus provides detailed morphological annotation at the world level, including part-of-speech tags." ></td>
	<td class="line x" title="139:232	In addition a lexicon for each language is provided." ></td>
	<td class="line x" title="140:232	We obtain six parallel corpora by considering all pairings of the four languages." ></td>
	<td class="line x" title="141:232	We compute word level alignments for each language pair using Giza++." ></td>
	<td class="line x" title="142:232	To generate one-to-one alignments at the word level, we intersect the one-to-many alignments going in each direction and automatically remove crossing edges in the order in which they appear left to right." ></td>
	<td class="line x" title="143:232	This process results in alignment of about half the tokens in each bilingual parallel corpus." ></td>
	<td class="line x" title="144:232	We treat the alignments as fixed and observed variables throughout the training procedure." ></td>
	<td class="line x" title="145:232	The corpus consists of 94,725 English words (see Table 2)." ></td>
	<td class="line x" title="146:232	For every language, a random three quarters of the data are used for learning the model while the remaining quarter is used for testing." ></td>
	<td class="line x" title="147:232	In the test set, only monolingual information is made available to the model, in order to simulate future performance on non-parallel data." ></td>
	<td class="line x" title="148:232	Tokens Tags/Token SR 89,051 1.41 SL 91,724 1.40 BG 80,757 1.34 EN 94,725 2.58 Table 2: Corpus statistics: SR=Serbian, SL=Slovene, EN=English, BG=Bulgarian Tagset The Multext-East corpus is manually annotated with detailed morphosyntactic information." ></td>
	<td class="line x" title="149:232	In our experiments, we focus on the main syntactic category encoded as a first letter of the labels." ></td>
	<td class="line x" title="150:232	The annotation distinguishes between 13 parts-ofspeech, of which 11 are common for all languages 1046 Random Monolingual Unsupervised Monolingual Supervised Trigram Entropy EN 56.24 90.71 96.97 1.558 BG 82.68 88.88 96.96 1.708 SL 84.70 87.41 97.31 1.703 SR 83.41 85.05 96.72 1.789 Table 1: Monolingual tagging accuracy for English, Bulgarian, Slovene, and Serbian for two unsupervised baselines (random tag selection and a Bayesian HMM (Goldwater and Griffiths, 2007)) as well as a supervised HMM." ></td>
	<td class="line x" title="151:232	In addition, the trigram part-of-speech tag entropy is given for each language." ></td>
	<td class="line x" title="152:232	in our experiments.3 In the Multext-East corpus, punctuation marks are not annotated." ></td>
	<td class="line x" title="153:232	We expand the tag repository by defining a separate tag for all punctuation marks." ></td>
	<td class="line x" title="154:232	This allows the model to make use of any transition or coupling patterns involving punctuation marks." ></td>
	<td class="line x" title="155:232	We do not consider punctuation tokens when computing model accuracy." ></td>
	<td class="line x" title="156:232	Table 2 shows the tag/token ratio for these languages." ></td>
	<td class="line x" title="157:232	For Slavic languages, we use the tag dictionaries provided with the corpus." ></td>
	<td class="line x" title="158:232	For English, we use a different process for dictionary construction." ></td>
	<td class="line x" title="159:232	Using the original dictionary would result in the tag/token ratio of 1.5, in comparison to the ratio of 2.3 observed in the Wall Street Journal (WSJ) corpus." ></td>
	<td class="line x" title="160:232	To make our results on English tagging more comparable to previous benchmarks, we expand the original dictionary of English tags by merging it with the tags from the WSJ dictionary." ></td>
	<td class="line x" title="161:232	This process results in a tag/token ratio of 2.58, yielding a slightly more ambiguous dictionary than the one used in previous tagging work." ></td>
	<td class="line x" title="162:232	4 Monolingual Baseline As our monolingual baseline we use the unsupervised Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1)." ></td>
	<td class="line x" title="163:232	This model modifies the standard HMM by adding priors and by performing Bayesian inference." ></td>
	<td class="line x" title="164:232	Its is in line with state-of-the-art unsupervised models." ></td>
	<td class="line x" title="165:232	This model is a particulary informative baseline, since our model reduces to this baseline model when there are no alignments in the data." ></td>
	<td class="line x" title="166:232	This implies that any performance gain over the baseline can only be at3The remaining two tags are Particle and Determiner; The English tagset does not include Particle while the other three languages Serbian, Slovene and Bulgarian do not have Determiner in their tagset." ></td>
	<td class="line x" title="167:232	4We couldnt perform the same dictionary expansion for the Slavic languages due to a lack of additional annotated resources." ></td>
	<td class="line x" title="168:232	tributed to the multilingual aspect of our model." ></td>
	<td class="line x" title="169:232	We used our own implementation after verifying that its performance on WSJ was identical to that reported in (Goldwater and Griffiths, 2007)." ></td>
	<td class="line x" title="170:232	Supervised Performance In order to provide a point of comparison, we also provide supervised results when an annotated corpus is provided." ></td>
	<td class="line x" title="171:232	We use the standard supervised HMM with Viterbi decoding." ></td>
	<td class="line x" title="172:232	Training and Testing Framework Initially, all words are assigned tags randomly from their tag dictionaries." ></td>
	<td class="line x" title="173:232	During each iteration of the sampler, aligned tag pairs and unaligned tags are sampled from their respective distributions given in Section 4.1 above." ></td>
	<td class="line x" title="174:232	The hyperparameters 0 and 0 are initialized with the values learned during monolingual training." ></td>
	<td class="line x" title="175:232	They are re-estimated after every iteration of the sampler using the Metropolis Hastings algorithm." ></td>
	<td class="line x" title="176:232	The parameters  and  are initially set to trigram counts and the  parameter is set to tag pair counts of aligned pairs." ></td>
	<td class="line x" title="177:232	After every 40 iterations of the sampler, a Metropolis Hastings subroutine is invoked that re-estimates these parameters based on the current counts." ></td>
	<td class="line x" title="178:232	Overall, the algorithm is run for 1000 iterations of tag sampling, by which time the resulting log-likelihood converges to stable values." ></td>
	<td class="line x" title="179:232	Each Metropolis Hastings subroutine samples 20 values, with an acceptance ratio of around 1/6, in line with the standard recommended values." ></td>
	<td class="line x" title="180:232	After training, trigram and word emission probabilities are computed based on the counts of tags assigned in the final iteration." ></td>
	<td class="line x" title="181:232	For smoothing, the final sampled values of the hyperparameters are used." ></td>
	<td class="line x" title="182:232	The highest probability tag sequences for each monolingual test set are then predicted using trigram Viterbi decoding." ></td>
	<td class="line x" title="183:232	We report results averaged over five complete runs of all experiments." ></td>
	<td class="line x" title="184:232	1047 6 Results Complete Tag Dictionary In our first experiment, we assume that a complete dictionary listing the possible tags for every word is provided in each language." ></td>
	<td class="line x" title="185:232	Table 1 shows the monolingual results of a random baseline, an unsupervised Bayesian HMM and a supervised HMM." ></td>
	<td class="line x" title="186:232	Table 3 show the results of our bilingual models for different language pairings while repeating the monolingual unsupervised results from Table 1 for easy comparison." ></td>
	<td class="line x" title="187:232	The final column indicates the absolute gain in performance over this monolingual baseline." ></td>
	<td class="line x" title="188:232	Across all language pairs, the bilingual model consistently outperforms the monolingual baseline." ></td>
	<td class="line x" title="189:232	All the improvements are statistically significant by a Fisher sign test at p < 0.05." ></td>
	<td class="line x" title="190:232	For some language pairs, the gains are quite high." ></td>
	<td class="line x" title="191:232	For instance, the pairing of Serbian and Slovene (two closely related languages) yields absolute improvements of 6.7 and 7.7 percentage points, corresponding to relative reductions in error of 51.4% and 53.2%." ></td>
	<td class="line x" title="192:232	Pairing Bulgarian and English (two distantly related languages) also yields large gains: 5.6 and 1.3 percentage points, corresponding to relative reductions in error of 50% and 14%, respectively.5 When we compare the best bilingual result for each language (Table 3, in bold) to the monolingual supervised results (Table 1), we find that for all languages the gap between supervised and unsupervised learning is reduced significantly." ></td>
	<td class="line x" title="193:232	For English, this gap is reduced by 21%." ></td>
	<td class="line x" title="194:232	For the Slavic languages, the supervised-unsupervised gap is reduced by even larger amounts: 57%, 69%, and 78% for Serbian, Bulgarian, and Slovene respectively." ></td>
	<td class="line x" title="195:232	While all the languages benefit from the bilingual learning framework, some language combinations are more effective than others." ></td>
	<td class="line x" title="196:232	Slovene, for instance, achieves a large improvement when paired with Serbian (+7.7), a closely related Slavic language, but only a minor improvement when coupled 5The accuracy of the monolingual English tagger is relatively high compared to the 87% reported by (Goldwater and Griffiths, 2007) on the WSJ corpus." ></td>
	<td class="line x" title="197:232	We attribute this discrepancy to the slight differences in tag inventory used in our dataset." ></td>
	<td class="line x" title="198:232	For example, when Particles and Prepositions are merged in the WSJ corpus (as they happen to be in our tag inventory and corpus), the performance of Goldwaters model on WSJ is similar to what we report here." ></td>
	<td class="line x" title="199:232	Entropy MonoBilingual Absolute lingual Gain EN 0.566 90.71 91.01 +0.30 SR 0.554 85.05 90.06 +5.03 EN 0.578 90.71 92.00 +1.29 BG 0.543 88.88 94.48 +5.61 EN 0.571 90.71 92.01 +1.30 SL 0.568 87.41 88.54 +1.13 SL 0.494 87.41 95.10 +7.69 SR 0.478 85.05 91.75 +6.70 BG 0.568 88.88 91.95 +3.08 SR 0.588 85.05 86.58 +1.53 BG 0.579 88.88 90.91 +2.04 SL 0.609 87.41 88.20 +0.79 Table 3: The tagging accuracy of our bilingual models on different language pairs, when a full tag dictionary is provided." ></td>
	<td class="line x" title="200:232	The Monolingual Unsupervised results from Table 1 are repeated for easy comparison." ></td>
	<td class="line x" title="201:232	The first column shows the cross-lingual entropy of a tag when the tag of the aligned word in the other language is known." ></td>
	<td class="line x" title="202:232	The final column shows the absolute improvement over the monolingual Bayesian HMM." ></td>
	<td class="line x" title="203:232	The best result for each language is shown in boldface." ></td>
	<td class="line x" title="204:232	with English (+1.3)." ></td>
	<td class="line x" title="205:232	On the other hand, for Bulgarian, the best performance is achieved when coupling with English (+5.6) rather than with closely related Slavic languages (+3.1 and +2.4)." ></td>
	<td class="line x" title="206:232	As these results show, an optimal pairing cannot be predicted based solely on the family connection of paired languages." ></td>
	<td class="line x" title="207:232	To gain a better understanding of this variation in performance, we measured the internal tag entropy of each language as well as the cross-lingual tag entropy of language pairs." ></td>
	<td class="line x" title="208:232	For the first measure, we computed the conditional entropy of a tag decision given the previous two tags." ></td>
	<td class="line x" title="209:232	Intuitively, this should correspond to the inherent structural uncertainty of part-of-speech decisions in a language." ></td>
	<td class="line x" title="210:232	In fact, as Table 1 shows, the trigram entropy is a good indicator of the relative performance of the monolingual baseline." ></td>
	<td class="line x" title="211:232	To measure the cross-lingual tag entropies of language pairs, we considered all bilingual aligned tag pairs, and computed the conditional entropy of the tags in one language given the tags in the other language." ></td>
	<td class="line x" title="212:232	This measure should indicate the amount of information that one language in a pair can provide the other." ></td>
	<td class="line x" title="213:232	The results of this anal1048 MonoBilingual Absolute lingual Gain EN 63.57 68.22 +4.66 SR 41.14 54.73 +13.59 EN 63.57 71.34 +7.78 BG 53.19 62.55 +9.37 EN 63.57 66.48 +2.91 SL 49.90 53.77 +3.88 SL 49.90 59.68 +9.78 SR 41.14 54.08 +12.94 BG 53.19 54.22 +1.04 SR 41.14 56.91 +15.77 BG 53.19 55.88 +2.70 SL 49.90 58.50 +8.60 Table 4: Tagging accuracy for Bilingual models with reduced dictionary: Lexicon entries are available for only the 100 most frequent words, while all other words become fully ambiguous." ></td>
	<td class="line x" title="214:232	The improvement over the monolingual Bayesian HMM trained under similar circumstances is shown." ></td>
	<td class="line x" title="215:232	The best result for each language is shown in boldface." ></td>
	<td class="line x" title="216:232	ysis are given in the first column of Table 3." ></td>
	<td class="line x" title="217:232	We observe that the cross-lingual entropy is lowest for the Serbian and Slovene pair, corresponding with their large gain in performance." ></td>
	<td class="line x" title="218:232	Bulgarian, on the other hand, has lowest cross-lingual entropy when paired with English." ></td>
	<td class="line x" title="219:232	This corresponds with the fact that English provides Bulgarian with its largest performance gain." ></td>
	<td class="line x" title="220:232	In general, we find that the largest performance gain for any language is achieved when minimizing its cross-lingual entropy." ></td>
	<td class="line x" title="221:232	Reduced Tag Dictionary We also conducted experiments to investigate the impact of the dictionary size on the performance of the bilingual model." ></td>
	<td class="line x" title="222:232	Here, we provide results for the realistic scenario where only a very small dictionary is present." ></td>
	<td class="line x" title="223:232	Table 4 shows the performance when a tag dictionary for the 100 most frequent words is present in each language." ></td>
	<td class="line x" title="224:232	The bilingual models results are consistently and significantly better than the monolingual baseline for all language pairs." ></td>
	<td class="line x" title="225:232	7 Conclusion We have demonstrated the effectiveness of multilingual learning for unsupervised part-of-speech tagging." ></td>
	<td class="line x" title="226:232	The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent." ></td>
	<td class="line x" title="227:232	We formulated a hierarchical Bayesian model for jointly predicting bilingual streams of tags." ></td>
	<td class="line x" title="228:232	The model learns language-specific features while capturing cross-lingual patterns in tag distribution." ></td>
	<td class="line x" title="229:232	Our evaluation shows significant performance gains over a state-of-the-art monolingual baseline." ></td>
	<td class="line x" title="230:232	Acknowledgments The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168 and grant IIS-0835445) and the Microsoft Research Faculty Fellowship." ></td>
	<td class="line x" title="231:232	Thanks to Michael Collins, Amir Globerson, Lillian Lee, Yoong Keok Lee, Maria Polinsky and the anonymous reviewers for helpful comments and suggestions." ></td>
	<td class="line x" title="232:232	Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1012
Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing
Zhang, Hao;Quirk, Chris;Moore, Robert C.;Gildea, Daniel;"></td>
	<td class="line x" title="1:198	Proceedings of ACL-08: HLT, pages 97105, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:198	c2008 Association for Computational Linguistics Bayesian Learning of Non-compositional Phrases with Synchronous Parsing Hao Zhang Computer Science Department University of Rochester Rochester, NY 14627 zhanghao@cs.rochester.edu Chris Quirk Microsoft Research One Microsoft Way Redmond, WA 98052 USA chrisq@microsoft.com Robert C. Moore Microsoft Research One Microsoft Way Redmond, WA 98052 USA bobmoore@microsoft.com Daniel Gildea Computer Science Department University of Rochester Rochester, NY 14627 gildea@cs.rochester.edu Abstract We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs." ></td>
	<td class="line x" title="3:198	The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large." ></td>
	<td class="line x" title="4:198	Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results." ></td>
	<td class="line x" title="5:198	Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment." ></td>
	<td class="line x" title="6:198	This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches." ></td>
	<td class="line x" title="7:198	1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments." ></td>
	<td class="line x" title="8:198	These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al.(1993) and Vogel et al.(1996)." ></td>
	<td class="line x" title="11:198	As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments." ></td>
	<td class="line x" title="12:198	Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process." ></td>
	<td class="line x" title="13:198	While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems." ></td>
	<td class="line x" title="14:198	A natural solution to several of these issues is unite the word-level and phrase-level models into one learning procedure." ></td>
	<td class="line x" title="15:198	Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words." ></td>
	<td class="line x" title="16:198	Furthermore it would obviate the need for heuristic combination of word alignments." ></td>
	<td class="line x" title="17:198	A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words." ></td>
	<td class="line x" title="18:198	In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting." ></td>
	<td class="line x" title="19:198	Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair." ></td>
	<td class="line x" title="20:198	In this paper, we attempt to address these two issues in order to apply EM above the word level." ></td>
	<td class="line x" title="21:198	97 We attack computational complexity by adopting the polynomial-time Inversion Transduction Grammar framework, and by only learning small noncompositional phrases." ></td>
	<td class="line x" title="22:198	We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases." ></td>
	<td class="line x" title="23:198	We test our model by extracting longer phrases from our models alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments." ></td>
	<td class="line x" title="24:198	2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework." ></td>
	<td class="line x" title="25:198	Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs." ></td>
	<td class="line x" title="26:198	There are three rules with X on the left-hand side: X  [X X], X  X X, X  C. The first two rules are the straight rule and inverted rule respectively." ></td>
	<td class="line x" title="27:198	They split the left-hand side constituent which represents a phrase pair into two smaller phrase pairs on the right-hand side and order them according to one of the two possible permutations." ></td>
	<td class="line x" title="28:198	The rewriting process continues until the third rule is invoked." ></td>
	<td class="line x" title="29:198	C is our unique pre-terminal for generating terminal multi-word pairs: C  e/f. We parameterize our probabilistic model in the manner of a PCFG: we associate a multinomial distribution with each nonterminal, where each outcome in this distribution corresponds to an expansion of that nonterminal." ></td>
	<td class="line x" title="30:198	Specifically, we place one multinomial distribution X over the three expansions of the nonterminalX, and another multinomial distribution C over the expansions of C. Thus, the parameters in our model can be listed as X = (P,P[],PC), where P is for the inverted rule, P[] for the straight rule,PC for the third rule, satisfyingP+P[]+PC = 1, and C = (P(e/f), P(e/f),), where summationtexte/f P(e/f) = 1 is a multinomial distribution over phrase pairs." ></td>
	<td class="line x" title="31:198	This is our model in a nutshell." ></td>
	<td class="line x" title="32:198	We can train this model using a two-dimensional extension of the inside-outside algorithm on bilingual data, assuming every phrase pair that can appear as a leaf in a parse tree of the grammar a valid candidate." ></td>
	<td class="line x" title="33:198	However, it is easy to show that the maximum likelihood training will lead to the saturated solution where PC = 1  each sentence pair is generated by a single phrase spanning the whole sentence." ></td>
	<td class="line x" title="34:198	From the computational point of view, the full EM algorithm runs in O(n6) where n is the average length of the two input sentences, which is too slow in practice." ></td>
	<td class="line x" title="35:198	The key is to control the number of parameters, and therefore the size of the set of candidate phrases." ></td>
	<td class="line x" title="36:198	We deal with this problem in two directions." ></td>
	<td class="line x" title="37:198	First we change the objective function by incorporating a prior over the phrasal parameters." ></td>
	<td class="line x" title="38:198	This has the effect of preferring parameter vectors in C with fewer non-zero values." ></td>
	<td class="line x" title="39:198	Our second approach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training." ></td>
	<td class="line x" title="40:198	First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model." ></td>
	<td class="line x" title="41:198	Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model." ></td>
	<td class="line oc" title="42:198	3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging." ></td>
	<td class="line x" title="43:198	In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior." ></td>
	<td class="line x" title="44:198	The traditional estimation method for word 98 alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data." ></td>
	<td class="line x" title="45:198	The drawback of maximum likelihood is obvious for phrase-based models." ></td>
	<td class="line x" title="46:198	If we do not put any constraint on the distribution of phrases, EM overfits the data by memorizing every sentence pair." ></td>
	<td class="line x" title="47:198	A sparse prior over a multinomial distribution such as the distribution of phrase pairs may bias the estimator toward skewed distributions that generalize better." ></td>
	<td class="line x" title="48:198	In the context of phrasal models, this means learning the more representative phrases in the space of all possible phrases." ></td>
	<td class="line x" title="49:198	The Dirichlet distribution, which is parameterized by a vector of real values often interpreted as pseudo-counts, is a natural choice for the prior, for two main reasons." ></td>
	<td class="line x" title="50:198	First, the Dirichlet is conjugate to the multinomial distribution, meaning that if we select a Dirichlet prior and a multinomial likelihood function, the posterior distribution will again be a Dirichlet." ></td>
	<td class="line x" title="51:198	This makes parameter estimation quite simple." ></td>
	<td class="line x" title="52:198	Second, Dirichlet distributions with small, non-zero parameters place more probability mass on multinomials on the edges or faces of the probability simplex, distributions with fewer non-zero parameters." ></td>
	<td class="line x" title="53:198	Starting from the model from Section 2, we propose the following Bayesian extension, where A  Dir(B) means the random variable A is distributed according to a Dirichlet with parameter B: X |X  Dir(X), C |C  Dir(C), [X X] X X C X  Multi(X), e/f |C  Multi(C)." ></td>
	<td class="line x" title="54:198	The parametersX andC control the sparsity of the two distributions in our model." ></td>
	<td class="line x" title="55:198	One is the distribution of the three possible branching choices." ></td>
	<td class="line x" title="56:198	The other is the distribution of the phrase pairs." ></td>
	<td class="line x" title="57:198	C is crucial, since the multinomial it is controlling has a high dimension." ></td>
	<td class="line x" title="58:198	By adjusting C to a very small number, we hope to place more posterior mass on parsimonious solutions with fewer but more confident and general phrase pairs." ></td>
	<td class="line x" title="59:198	Having defined the Bayesian model, it remains to decide the inference procedure." ></td>
	<td class="line x" title="60:198	We chose Variational Bayes, for its procedural similarity to EM and ease of implementation." ></td>
	<td class="line x" title="61:198	Another potential option would be Gibbs sampling (or some other sampling technique)." ></td>
	<td class="line oc" title="62:198	However, in experiments in unsupervised POS tag learning using HMM structured models, Johnson (2007) shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipfs law, which is prominent in natural languages." ></td>
	<td class="line x" title="63:198	Kurihara and Sato (2006) describe VB for PCFGs, showing the only need is to change the M step of the EM algorithm." ></td>
	<td class="line x" title="64:198	As in the case of maximum likelihood estimation, Bayesian estimation for ITGs is very similar to PCFGs, which follows due to the strong isomorphism between the two models." ></td>
	<td class="line x" title="65:198	Specific to our ITG case, the M step becomes: P(l+1)[] = exp((E(X  [X X]) +X)) exp((E(X) +sX)) , P(l+1) = exp((E(X  X X) +X)) exp((E(X) +sX)) , P(l+1)C = exp((E(X C) +X)) exp((E(X) +sX)) , P(l+1)(e/f) = exp((E(e/f) +C)) exp((E(C) +mC)), where  is the digamma function (Beal, 2003), s = 3 is the number of right-hand-sides for X, and m is the number of observed phrase pairs in the data." ></td>
	<td class="line x" title="66:198	The sole difference between EM and VB with a sparse prior  is that the raw fractional counts c are replaced by exp((c+)), an operation that resembles smoothing." ></td>
	<td class="line oc" title="67:198	As pointed out by Johnson (2007), in effect this expression adds to c a small value that asymptotically approaches  0.5 as c approaches , and 0 as c approaches 0." ></td>
	<td class="line x" title="68:198	For small values of  the net effect is the opposite of typical smoothing, since it tends to redistribute probably mass away from unlikely events onto more likely ones." ></td>
	<td class="line x" title="69:198	4 Bitext Pruning Strategy ITG is slow mainly because it considers every pair of spans in two sentences as a possible chart element." ></td>
	<td class="line x" title="70:198	In reality, the set of useful chart elements is much 99 smaller than the possible scriptO(n4), where n is the average sentence length." ></td>
	<td class="line x" title="71:198	Pruning the span pairs (bitext cells) that can participate in a tree (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space." ></td>
	<td class="line x" title="72:198	Given a bitext cell defined by the four boundary indices (i,j,l,m) as shown in Figure 1a, we prune based on a figure of merit V(i,j,l,m) approximating the utility of that cell in a full ITG parse." ></td>
	<td class="line x" title="73:198	The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005)." ></td>
	<td class="line x" title="74:198	Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases." ></td>
	<td class="line x" title="75:198	The total score is the product of the Model 1 probabilities for each column; inside columns in the range [l,m] are scored according to the sum (or maximum) of Model 1 probabilities for [i,j], and outside columns use the sum (or maximum) of all probabilities not in the range [i,j]." ></td>
	<td class="line x" title="76:198	Our pruning differs from Zhang and Gildea (2005) in two major ways." ></td>
	<td class="line x" title="77:198	First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V, we have two: VF and VB." ></td>
	<td class="line x" title="78:198	Only those spans that pass the pruning threshold in both directions are kept." ></td>
	<td class="line x" title="79:198	Second, we allow whole spans to be pruned." ></td>
	<td class="line x" title="80:198	The figure of merit for a span is VF(i,j) = maxl,mVF(i,j,l,m)." ></td>
	<td class="line x" title="81:198	Only spans that are within some threshold of the unrestricted Model 1 scores VF and VB are kept: VF(i,j) VF s and VB(l,m) VB s. Amongst those spans retained by this first threshold, we keep only those bitext cells satisfying both VF(i,j,l,m) VF(i,j) b and VB(i,j,l,m) VB(l,m) b. 4.1 Fast Tic-tac-toe Pruning The tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O(n4) time." ></td>
	<td class="line x" title="82:198	However, even this can be slow for large values of n. Therefore we describe an Figure 1: (a) shows the original tic-tac-toe score for a bitext cell (i,j,l,m)." ></td>
	<td class="line x" title="83:198	(b) demonstrates the finite state representation using the machine in (c), assuming a fixed source span (i,j)." ></td>
	<td class="line x" title="84:198	improved algorithm with best case n3 performance." ></td>
	<td class="line x" title="85:198	Although the worst case performance is also O(n4), in practice it is significantly faster." ></td>
	<td class="line x" title="86:198	To begin, let us restrict our attention to the forward direction for a fixed source span (i,j)." ></td>
	<td class="line x" title="87:198	Pruning bitext spans and cells requiresVF(i,j), the score of the best bitext cell within a given span, as well as all cells within a given threshold of that best score." ></td>
	<td class="line x" title="88:198	For a fixed i and j, we need to search over the starting and ending points l and m of the inside region." ></td>
	<td class="line x" title="89:198	Note that there is an isomorphism between the set of spans and a simple finite state machine: any span (l,m) can be represented by a sequence ofl OUTSIDE columns, followed byml+1 INSIDE columns, followed by n  m + 1 OUTSIDE columns." ></td>
	<td class="line x" title="90:198	This simple machine has the restricted form described in Figure 1c: it has three states, L, M, and R; each transition generates either an OUTSIDE column O or an INSIDE column I. The cost of generating an OUTSIDE at position a is O(a) = P(ta|NULL) +summationtextbnegationslash[i,j]P(ta|sb); likewise the cost of generating an INSIDE column is I(a) = P(ta|NULL) + summationtextb[i,j]P(ta|sb), with 100 O(0) = O(n+ 1) = 1 and I(0) = I(n+ 1) = 0." ></td>
	<td class="line x" title="91:198	Directly computing O and I would take time O(n2) for each source span, leading to an overall runtime of O(n4)." ></td>
	<td class="line x" title="92:198	Luckily there are faster ways to find the inside and outside scores." ></td>
	<td class="line x" title="93:198	First we can precompute following arrays in O(n2) time and space: pre[0,l] := P(tl|NULL) pre[i,l] := pre[i1,l] +P(tl|si) suf[n+ 1,l] := 0 suf[i,l] := suf[i+ 1,l] +P(tl|si) Then for any (i,j), O(a) = P(ta|NULL) +summationtext bnegationslash[i,j]P(ta|sb) = pre[i  1,a] + suf[j + 1,a]." ></td>
	<td class="line x" title="94:198	I(a) can be incrementally updated as the source span varies: when i = j, I(a) = P(ta|NULL) + P(ta|si)." ></td>
	<td class="line x" title="95:198	As j is incremented, we add P(ta|sj) to I(a)." ></td>
	<td class="line x" title="96:198	Thus we have linear time updates for O and I. We can then find the best scoring sequence using the familiar Viterbi algorithm." ></td>
	<td class="line x" title="97:198	Let[a,] be the cost of the best scoring sequence ending at in state  at time a: [0,] := 1 if  = L; 0 otherwise [a,L] := [a1,L]O(a) [a,M] := max L,M {[a1,]}I(a) [a,R] := max M,R {[a1,]}O(a) Then VF(i,j) = [n + 1,R], using the isomorphism between state sequences and spans." ></td>
	<td class="line x" title="98:198	This linear time algorithm allows us to compute span pruning in O(n3) time." ></td>
	<td class="line x" title="99:198	The same algorithm may be performed using the backward figure of merit after transposing rows and columns." ></td>
	<td class="line x" title="100:198	Having cast the problem in terms of finite state automata, we can use finite state algorithms for pruning." ></td>
	<td class="line x" title="101:198	For instance, fixing a source span we can enumerate the target spans in decreasing order by score (Soong and Huang, 1991), stopping once we encounter the first span below threshold." ></td>
	<td class="line x" title="102:198	In practice the overhead of maintaining the priority queue outweighs any benefit, as seen in Figure 2." ></td>
	<td class="line x" title="103:198	An alternate approach that avoids this overhead is to enumerate spans by position." ></td>
	<td class="line x" title="104:198	Note that [m,R] producttext n a=m+1O(a) is within threshold iff there is a span with right boundary m < m within threshold." ></td>
	<td class="line x" title="105:198	Furthermore if [m,M]  producttextna=m+1O(a) is  0  100  200  300  400  500  600  700  800  900  10  20  30  40  50 Pruning time (thousands of seconds) Average sentence length Baseline k-best Fast Figure 2: Speed comparison of the O(n4) tic-tac-toe pruning algorithm, the A* top-x algorithm, and the fast tic-tac-toe pruning." ></td>
	<td class="line x" title="106:198	All produce the same set of bitext cells, those within threshold of the best bitext cell." ></td>
	<td class="line x" title="107:198	within threshold, thenmis the right boundary within threshold." ></td>
	<td class="line x" title="108:198	Using these facts, we can gradually sweep the right boundary m from n toward 1 until the first condition fails to hold." ></td>
	<td class="line x" title="109:198	For each value where the second condition holds, we pause to search for the set of left boundaries within threshold." ></td>
	<td class="line x" title="110:198	Likewise for the left edge, [l,M]producttextma=l+1I(a)producttext n a=m+1O(a) is within threshold iff there is some l < l identifying a span (l,m) within threshold." ></td>
	<td class="line x" title="111:198	Finally if V(i,j,l,m) = [l  1,L] producttextma=lI(a) producttext n a=m+1O(a) is within threshold, then (i,j,l,m) is a bitext cell within threshold." ></td>
	<td class="line x" title="112:198	For right edges that are known to be within threshold, we can sweep the left edges leftward until the first condition no longer holds, keeping only those spans for which the second condition holds." ></td>
	<td class="line x" title="113:198	The filtering algorithm behaves extremely well." ></td>
	<td class="line x" title="114:198	Although the worst case runtime is still O(n4), the best case has improved ton3; empirically it seems to significantly reduce the amount of time spent exploring spans." ></td>
	<td class="line x" title="115:198	Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005)." ></td>
	<td class="line x" title="116:198	101 Figure 3: Example output from the ITG using non-compositional phrases." ></td>
	<td class="line x" title="117:198	(a) is the Viterbi alignment from the wordbased ITG." ></td>
	<td class="line x" title="118:198	The shaded regions indicate phrasal alignments that are allowed by the non-compositional constraint; all other phrasal alignments will not be considered." ></td>
	<td class="line x" title="119:198	(b) is the Viterbi alignment from the phrasal ITG, with the multi-word alignments highlighted." ></td>
	<td class="line x" title="120:198	5 Bootstrapping Phrasal ITG from Word-based ITG This section introduces a technique that bootstraps candidate phrase pairs for phrase-based ITG from word-based ITG Viterbi alignments." ></td>
	<td class="line x" title="121:198	The wordbased ITG uses the same expansions for the nonterminal X, but the expansions of C are limited to generate only 1-1, 1-0, and 0-1 alignments: C  e/f, C  e/, C  /f where  indicates that no word was generated." ></td>
	<td class="line x" title="122:198	Broadly speaking, the goal of this section is the same as the previous section, namely, to limit the set of phrase pairs that needs to be considered in the training process." ></td>
	<td class="line x" title="123:198	The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area." ></td>
	<td class="line x" title="124:198	In this part, we use word-based ITG alignments as anchor points in the alignment space to pin down the potential phrases." ></td>
	<td class="line x" title="125:198	The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-toone word alignments." ></td>
	<td class="line x" title="126:198	The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007)." ></td>
	<td class="line x" title="127:198	Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the bitext space to constraint ITG phrases." ></td>
	<td class="line x" title="128:198	We use ITG Viterbi alignments instead." ></td>
	<td class="line x" title="129:198	The benefit is two-fold." ></td>
	<td class="line x" title="130:198	First of all, we do not have to run a GIZA++ aligner." ></td>
	<td class="line x" title="131:198	Second, we do not need to worry about non-ITG word alignments, such as the (2,4,1,3) permutation patterns." ></td>
	<td class="line x" title="132:198	GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG." ></td>
	<td class="line x" title="133:198	Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link." ></td>
	<td class="line x" title="134:198	Mathematically, let e(i,j) count the number of word links that are emitted from the substring eij, and f(l,m) count the number of word links emitted from the substring flm." ></td>
	<td class="line x" title="135:198	The non-compositional phrase pairs satisfy e(i,j) = f(l,m)  1." ></td>
	<td class="line x" title="136:198	Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair." ></td>
	<td class="line x" title="137:198	6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main contributions of this paper: Variational Bayes, tic-tactoe pruning, and word-to-phrase bootstrapping." ></td>
	<td class="line x" title="138:198	We 102 start from sentence-aligned bilingual data and run IBM Model 1 in both directions to obtain two translation tables." ></td>
	<td class="line x" title="139:198	Then we use the efficient bidirectional tic-tac-toe pruning to prune the bitext space within each of the sentence pairs; ITG parsing will be carried out on only this this sparse set of bitext cells." ></td>
	<td class="line x" title="140:198	The first stage of training is word-based ITG, using the standard iterative training procedure, except VB replaces EM to focus on a sparse prior." ></td>
	<td class="line x" title="141:198	After several training iterations, we obtain the Viterbi alignments on the training data according to the final model." ></td>
	<td class="line x" title="142:198	Now we transition into the second stage  the phrasal training." ></td>
	<td class="line x" title="143:198	Before the training starts, we apply the non-compositional constraints over the pruned bitext space to further constrain the space of phrase pairs." ></td>
	<td class="line x" title="144:198	Finally, we run phrasal ITG iterative training using VB for a certain number of iterations." ></td>
	<td class="line x" title="145:198	In the end, a Viterbi pass for the phrasal ITG is executed to produce the non-compositional phrasal alignments." ></td>
	<td class="line x" title="146:198	From this alignment, phrase pairs are extracted in the usual manner, and a phrase-based translation system is trained." ></td>
	<td class="line x" title="147:198	7 Experiments The training data was a subset of 175K sentence pairs from the NIST Chinese-English training data, automatically selected to maximize character-level overlap with the source side of the test data." ></td>
	<td class="line x" title="148:198	We put a length limit of 35 on both sides, producing a training set of 141K sentence pairs." ></td>
	<td class="line x" title="149:198	500 Chinese-English pairs from this set were manually aligned and used as a gold standard." ></td>
	<td class="line x" title="150:198	7.1 Word Alignment Evaluation First, using evaluations of alignment quality, we demonstrate the effectiveness of VB over EM, and explore the effect of the prior." ></td>
	<td class="line x" title="151:198	Figure 4 examines the difference between EM and VB with varying sparse priors for the word-based model of ITG on the 500 sentence pairs, both after 10 iterations of training." ></td>
	<td class="line x" title="152:198	Using EM, because of overfitting, AER drops first and increases again as the number of iterations varies from 1 to 10." ></td>
	<td class="line x" title="153:198	The lowest AER using EM is achieved after the second iteration, which is .40." ></td>
	<td class="line x" title="154:198	At iteration 10, AER for EM increases to .42." ></td>
	<td class="line x" title="155:198	On the other hand, using VB, AER decreases monotonically over the 10 iterations and  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  1e-009  1e-006  0.001  1 AER Prior value VB EM Figure 4: AER drops as C approaches zero; a more sparse solution leads to better results." ></td>
	<td class="line x" title="156:198	stabilizes at iteration 10." ></td>
	<td class="line x" title="157:198	When C is 1e 9, VB gets AER close to .35 at iteration 10." ></td>
	<td class="line x" title="158:198	As we increase the bias toward sparsity, the AER decreases, following a long slow plateau." ></td>
	<td class="line x" title="159:198	Although the magnitude of improvement is not large, the trend is encouraging." ></td>
	<td class="line x" title="160:198	These experiments also indicate that a very sparse prior is needed for machine translation tasks." ></td>
	<td class="line nc" title="161:198	Unlike Johnson (2007), who found optimal performance when  was approximately 104, we observed monotonic increases in performance as  dropped." ></td>
	<td class="line x" title="162:198	The dimensionality of this MT problem is significantly larger than that of the sequence problem, though, therefore it may take a stronger push from the prior to achieve the desired result." ></td>
	<td class="line x" title="163:198	7.2 End-to-end Evaluation Given an unlimited amount of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU." ></td>
	<td class="line x" title="164:198	Unfortunately these experiments are very slow." ></td>
	<td class="line x" title="165:198	Since we observed monotonic increases in alignment performance with smaller values of C, we simply fixed the prior at a very small value (10100) for all translation experiments." ></td>
	<td class="line x" title="166:198	We do compare VB against EM in terms of final BLEU scores in the translation experiments to ensure that this sparse prior has a sig103 nificant impact on the output." ></td>
	<td class="line x" title="167:198	We also trained a baseline model with GIZA++ (Och and Ney, 2003) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4." ></td>
	<td class="line x" title="168:198	We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1." ></td>
	<td class="line x" title="169:198	These values were used to perform tic-tac-toe pruning with b = 1103 and s = 1106." ></td>
	<td class="line x" title="170:198	Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB." ></td>
	<td class="line x" title="171:198	The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model." ></td>
	<td class="line x" title="172:198	Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB, and extracted the Viterbi alignments." ></td>
	<td class="line x" title="173:198	For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004)." ></td>
	<td class="line x" title="174:198	The output of the word alignment systems (GIZA++ or ITG) were fed to a standard phrase extraction procedure that extracted all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies." ></td>
	<td class="line x" title="175:198	Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units." ></td>
	<td class="line x" title="176:198	In addition the phrases were annotated with lexical weights using the IBM Model 1 tables." ></td>
	<td class="line x" title="177:198	The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features." ></td>
	<td class="line x" title="178:198	Minimum Error Rate training (Och, 2003) over BLEU was used to optimize the weights for each of these models over the development test data." ></td>
	<td class="line x" title="179:198	We used the NIST 2002 evaluation datasets for tuning and evaluation; the 10-reference development set was used for minimum error rate training, and the 4-reference test set was used for evaluation." ></td>
	<td class="line x" title="180:198	We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method." ></td>
	<td class="line x" title="181:198	Table 1 compares the four systems: the GIZA++ baseline, the ITG word-based model, the ITG multiword model using EM training, and the ITG multiword model using VB training." ></td>
	<td class="line x" title="182:198	ITG-mwm-VB is our best model." ></td>
	<td class="line x" title="183:198	We see an improvement of nearly Development Test GIZA++ 37.46 28.24 ITG-word 35.47 26.55 ITG-mwm (VB) 39.21 29.02 ITG-mwm (EM) 39.15 28.47 Table 1: Translation results on Chinese-English, using the subset of training data (141K sentence pairs) that have length limit 35 on both sides." ></td>
	<td class="line x" title="184:198	(No length limit in translation." ></td>
	<td class="line x" title="185:198	) 2 points dev set and nearly 1 point of improvement on the test set." ></td>
	<td class="line x" title="186:198	We also observe the consistent superiority of VB over EM." ></td>
	<td class="line x" title="187:198	The gain is especially large on the test data set, indicating VB is less prone to overfitting." ></td>
	<td class="line x" title="188:198	8 Conclusion We have presented an improved and more efficient method of estimating phrase pairs directly." ></td>
	<td class="line x" title="189:198	By both changing the objective function to include a bias toward sparser models and improving the pruning techniques and efficiency, we achieve significant gains on test data with practical speed." ></td>
	<td class="line x" title="190:198	In addition, these gains were shown without resorting to external models, such as GIZA++." ></td>
	<td class="line x" title="191:198	We have shown that VB is both practical and effective for use in MT models." ></td>
	<td class="line x" title="192:198	However, our best system does not apply VB to a single probability model, as we found an appreciable benefit from bootstrapping each model from simpler models, much as the IBM word alignment models are usually trained in succession." ></td>
	<td class="line x" title="193:198	We find that VB alone is not sufficient to counteract the tendency of EM to prefer analyses with smaller trees using fewer rules and longer phrases." ></td>
	<td class="line x" title="194:198	Both the tic-tac-toe pruning and the non-compositional constraint address this problem by reducing the space of possible phrase pairs." ></td>
	<td class="line x" title="195:198	On top of these hard constraints, the sparse prior of VB helps make the model less prone to overfitting to infrequent phrase pairs, and thus improves the quality of the phrase pairs the model learns." ></td>
	<td class="line x" title="196:198	Acknowledgments This work was done while the first author was at Microsoft Research; thanks to Xiaodong He, Mark Johnson, and Kristina Toutanova." ></td>
	<td class="line x" title="197:198	The last author was supported by NSF IIS-0546554." ></td>
	<td class="line x" title="198:198	104" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1100
Analyzing the Errors of Unsupervised Learning
Liang, Percy;Klein, Dan;"></td>
	<td class="line x" title="1:214	Proceedings of ACL-08: HLT, pages 879887, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:214	c2008 Association for Computational Linguistics Analyzing the Errors of Unsupervised Learning Percy Liang Dan Klein Computer Science Division, EECS Department University of California at Berkeley Berkeley, CA 94720 {pliang,klein}@cs.berkeley.edu Abstract We identify four types of errors that unsupervised induction systems make and study each one in turn." ></td>
	<td class="line x" title="3:214	Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples." ></td>
	<td class="line x" title="4:214	We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model." ></td>
	<td class="line x" title="5:214	1 Introduction The unsupervised induction of linguistic structure from raw text is an important problem both for understanding language acquisition and for building language processing systems such as parsers from limited resources." ></td>
	<td class="line x" title="6:214	Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima." ></td>
	<td class="line x" title="7:214	Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective." ></td>
	<td class="line x" title="8:214	Since then, there has been a large body of work addressing the flaws of the EM-based approach." ></td>
	<td class="line x" title="9:214	Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004)." ></td>
	<td class="line x" title="10:214	Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure." ></td>
	<td class="line oc" title="11:214	Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006)." ></td>
	<td class="line n" title="12:214	Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches." ></td>
	<td class="line x" title="13:214	It is therefore important to better understand the behavior of unsupervised induction systems in general." ></td>
	<td class="line x" title="14:214	In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction." ></td>
	<td class="line x" title="15:214	We identify four types of error that a system can make: approximation, identifiability, estimation, and optimization errors (see Figure 1)." ></td>
	<td class="line x" title="16:214	We try to isolate each one in turn and study its properties." ></td>
	<td class="line x" title="17:214	Approximation error is caused by a mis-match between the likelihood objective optimized by EM and the true relationship between sentences and their syntactic structures." ></td>
	<td class="line x" title="18:214	Our key idea for understanding this mis-match is to cheat and initialize EM with the true relationship and then study the ways in which EM repurposes our desired syntactic structures to increase likelihood." ></td>
	<td class="line x" title="19:214	We present a metamodel of the changes that EM makes and show how this tool can shed some light on the undesired biases of the HMM, the PCFG, and the dependency model with valence (Klein and Manning, 2004)." ></td>
	<td class="line x" title="20:214	Identifiability error can be incurred when two distinct parameter settings yield the same probability distribution over sentences." ></td>
	<td class="line x" title="21:214	One type of nonidentifiability present in HMMs and PCFGs is label symmetry, which even makes computing a meaningful distance between parameters NP-hard." ></td>
	<td class="line x" title="22:214	We present a method to obtain lower and upper bounds on such a distance." ></td>
	<td class="line x" title="23:214	Estimation error arises from having too few training examples, and optimization error stems from 879 EM getting stuck in local optima." ></td>
	<td class="line x" title="24:214	While it is to be expected that estimation error should decrease as the amount of data increases, we show that optimization error can also decrease." ></td>
	<td class="line x" title="25:214	We present striking experiments showing that if our data actually comes from the model family we are learning with, we can sometimes recover the true parameters by simply running EM without clever initialization." ></td>
	<td class="line x" title="26:214	This result runs counter to the conventional attitude that EM is doomed to local optima; it suggests that increasing the amount of data might be an effective way to partially combat local optima." ></td>
	<td class="line x" title="27:214	2 Unsupervised models Let x denote an input sentence and y denote the unobserved desired output (e.g., a parse tree)." ></td>
	<td class="line x" title="28:214	We consider a model familyP = {p(x,y) :  }." ></td>
	<td class="line x" title="29:214	For example, if P is the set of all PCFGs, then the parameters  would specify all the rule probabilities of a particular grammar." ></td>
	<td class="line x" title="30:214	We sometimes use  and p interchangeably to simplify notation." ></td>
	<td class="line x" title="31:214	In this paper, we analyze the following three model families: In the HMM, the input x is a sequence of words and the output y is the corresponding sequence of part-of-speech tags." ></td>
	<td class="line x" title="32:214	In the PCFG, the input x is a sequence of POS tags and the output y is a binary parse tree with yield x. We represent y as a multiset of binary rewrites of the form (yy1y2), where y is a nonterminal and y1,y2 can be either nonterminals or terminals." ></td>
	<td class="line x" title="33:214	In the dependency model with valence (DMV) (Klein and Manning, 2004), the input x = (x1,,xm) is a sequence of POS tags and the output y specifies the directed links of a projective dependency tree." ></td>
	<td class="line x" title="34:214	The generative model is as follows: for each head xi, we generate an independent sequence of arguments to the left and to the right from a direction-dependent distribution over tags." ></td>
	<td class="line x" title="35:214	At each point, we stop with a probability parametrized by the direction and whether any arguments have already been generated in that direction." ></td>
	<td class="line x" title="36:214	See Klein and Manning (2004) for a formal description." ></td>
	<td class="line x" title="37:214	In all our experiments, we used the Wall Street Journal (WSJ) portion of the Penn Treebank." ></td>
	<td class="line x" title="38:214	We binarized the PCFG trees and created gold dependency trees according to the Collins head rules." ></td>
	<td class="line x" title="39:214	We trained 45-state HMMs on all 49208 sentences, 11-state PCFGs on WSJ-10 (7424 sentences) and DMVs on WSJ-20 (25523 sentences) (Klein and Manning, 2004)." ></td>
	<td class="line x" title="40:214	We ran EM for 100 iterations with the parameters initialized uniformly (always plus a small amount of random noise)." ></td>
	<td class="line x" title="41:214	We evaluated the HMM and PCFG by mapping model states to Treebank tags to maximize accuracy." ></td>
	<td class="line x" title="42:214	3 Decomposition of errors Now we will describe the four types of errors (Figure 1) more formally." ></td>
	<td class="line x" title="43:214	Let p(x,y) denote the distribution which governs the true relationship between the input x and output y. In general, p does not live in our model familyP." ></td>
	<td class="line x" title="44:214	We are presented with a set of n unlabeled examples x(1),,x(n) drawn i.i.d. from the true p." ></td>
	<td class="line x" title="45:214	In unsupervised induction, our goal is to approximatepby some modelpP in terms of strong generative capacity." ></td>
	<td class="line x" title="46:214	A standard approach is to use the EM algorithm to optimize the empirical likelihood Elogp(x).1 However, EM only finds a local maximum, which we denote EM, so there is a discrepancy between what we get (pEM) and what we want (p)." ></td>
	<td class="line x" title="47:214	We will define this discrepancy later, but for now, it suffices to remark that the discrepancy depends on the distribution over y whereas learning depends only on the distribution over x. This is an important property that distinguishes unsupervised induction from more standard supervised learning or density estimation scenarios." ></td>
	<td class="line x" title="48:214	Now let us walk through the four types of error bottom up." ></td>
	<td class="line x" title="49:214	First, EM, the local maximum found by EM, is in general different from   argmax Elogp(x), any global maximum, which we could find given unlimited computational resources." ></td>
	<td class="line x" title="50:214	Optimization error refers to the discrepancy between  and EM." ></td>
	<td class="line x" title="51:214	Second, our training data is only a noisy sample from the true p." ></td>
	<td class="line x" title="52:214	If we had infinite data, we would choose an optimal parameter setting under the model, 2  argmaxElogp(x), where now the expectation E is taken with respect to the true p instead of the training data." ></td>
	<td class="line x" title="53:214	The discrepancy between 2 and  is the estimation error." ></td>
	<td class="line x" title="54:214	Note that 2 might not be unique." ></td>
	<td class="line x" title="55:214	Let 1 denote 1Here, the expectation Ef(x) def= 1 n Pn i=1f(x (i)) denotes averaging some function f over the training data." ></td>
	<td class="line x" title="56:214	880 p =truemodel Approximationerror(Section4)   1 =Best(argmax Elogp (x)) Identi abilityerror(Section5) 2 2argmax Elogp (x) Estimationerror(Section6)^  2argmax ^Elogp (x) Optimizationerror(Section7)^  EM=EM(^Elogp (x)) P Figure 1: The discrepancy between what we get (EM) and what we want (p) can be decomposed into four types of errors." ></td>
	<td class="line x" title="57:214	The box represents our model familyP, which is the set of possible parametrized distributions we can represent." ></td>
	<td class="line x" title="58:214	Best(S) returns theS which has the smallest discrepancy with p." ></td>
	<td class="line x" title="59:214	the maximizer of Elogp(x) that has the smallest discrepancy with p." ></td>
	<td class="line x" title="60:214	Since 1 and 2 have the same value under the objective function, we would not be able to choose 1 over 2, even with infinite data or unlimited computation." ></td>
	<td class="line x" title="61:214	Identifiability error refers to the discrepancy between 1 and 2." ></td>
	<td class="line x" title="62:214	Finally, the model familyP has fundamental limitations." ></td>
	<td class="line x" title="63:214	Approximation error refers to the discrepancy between p and p1." ></td>
	<td class="line x" title="64:214	Note that 1 is not necessarily the best in P. If we had labeled data, we could find a parameter setting in P which is closer to p by optimizing joint likelihood Elogp(x,y) (generative training) or even conditional likelihood Elogp(y|x) (discriminative training)." ></td>
	<td class="line x" title="65:214	In the remaining sections, we try to study each of the four errors in isolation." ></td>
	<td class="line x" title="66:214	In practice, since it is difficult to work with some of the parameter settings that participate in the error decomposition, we use computationally feasible surrogates so that the error under study remains the dominant effect." ></td>
	<td class="line x" title="67:214	4 Approximation error We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been dis20 40 60 80100iteration -18.4-18.0 -17.6-17.2 -16.7 log-lik eliho od 20 40 60 80100iteration 0.20.4 0.60.8 1.0 Lab eled F1 Figure 2: For the PCFG, when we initialize EM with the supervised estimate gen, the likelihood increases but the accuracy decreases." ></td>
	<td class="line x" title="68:214	cussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 gen = argmax Elogp(x,y), which acts as a surrogate for p." ></td>
	<td class="line x" title="69:214	As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly)." ></td>
	<td class="line x" title="70:214	We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models." ></td>
	<td class="line x" title="71:214	However, EM is changing hundreds of thousands of parameters at once in a non-trivial way, so we need a way of characterizing the important changes." ></td>
	<td class="line x" title="72:214	One broad observation we can make is that the first iteration of EM reinforces the systematic mistakes of the supervised initializer." ></td>
	<td class="line x" title="73:214	In the first E-step, the posterior counts that are computed summarize the predictions of the supervised system." ></td>
	<td class="line x" title="74:214	If these match the empirical counts, then the M-step does not change the parameters." ></td>
	<td class="line x" title="75:214	But if the supervised system predicts too many JJs, for example, then the M-step will update the parameters to reinforce this bias." ></td>
	<td class="line x" title="76:214	4.1 A meta-model for analyzing EM We would like to go further and characterize the specific changes EM makes." ></td>
	<td class="line x" title="77:214	An initial approach is to find the parameters that changed the most during the first iteration (weighted by the correspond2Here, we think of discrepancy betweenpandpprime as the error incurred when using pprime for prediction on examples generated from p; in symbols, E(x,y)ploss(y,argmaxyprime pprime(yprime | x))." ></td>
	<td class="line x" title="78:214	3For all our models, the supervised estimate is solved in closed form by taking ratios of counts." ></td>
	<td class="line x" title="79:214	881 ing expected counts computed in the E-step)." ></td>
	<td class="line x" title="80:214	For the HMM, the three most changed parameters are the transitions 2:DT8:JJ, START0:NNP, and 8:JJ3:NN.4 If we delve deeper, we can see that 2:DT3:NN (the parameter with the 10th largest change) fell and 2:DT8:JJ rose." ></td>
	<td class="line x" title="81:214	After checking with a few examples, we can then deduce that some nouns were retagged as adjectives." ></td>
	<td class="line x" title="82:214	Unfortunately, this type of ad-hoc reasoning requires considerable manual effort and is rather subjective." ></td>
	<td class="line x" title="83:214	Instead, we propose using a general meta-model to analyze the changes EM makes in an automatic and objective way." ></td>
	<td class="line x" title="84:214	Instead of treating parameters as the primary object of study, we look at predictions made by the model and study how they change over time." ></td>
	<td class="line x" title="85:214	While a model is a distribution over sentences, a meta-model is a distribution over how the predictions of the model change." ></td>
	<td class="line x" title="86:214	Let R(y) denote the set of parts of a prediction y that we are interested in tracking." ></td>
	<td class="line x" title="87:214	Each part (c,l)R(y) consists of a configuration c and a location l. For a PCFG, we define a configuration to be a rewrite rule (e.g., c = PPIN NP), and a location l = [i,k,j] to be a span [i,j] split at k, where the rewrite c is applied." ></td>
	<td class="line x" title="88:214	In this work, each configuration is associated with a parameter of the model, but in general, a configuration could be a larger unit such as a subtree, allowing one to track more complex changes." ></td>
	<td class="line x" title="89:214	The size of a configuration governs how much the meta-model generalizes from individual examples." ></td>
	<td class="line x" title="90:214	Let y(i,t) denote the model prediction on the i-th training example after t iterations of EM." ></td>
	<td class="line x" title="91:214	To simplify notation, we write Rt = R(y(i,t))." ></td>
	<td class="line x" title="92:214	The metamodel explains how Rt became Rt+1.5 In general, we expect a part in Rt+1 to be explained by a part in Rt that has a similar location and furthermore, we expect the locations of the two parts to be related in some consistent way." ></td>
	<td class="line x" title="93:214	The metamodel uses two notions to formalize this idea: a distance d(l,lprime) and a relation r(l,lprime)." ></td>
	<td class="line x" title="94:214	For the PCFG, d(l,lprime) is the number of positions among i,j,k that are the same as the corresponding ones in lprime, and r((i,k,j),(iprime,kprime,jprime)) = (sign(i  iprime),sign(j  4Here 2:DT means state 2 of the HMM, which was greedily mapped to DT." ></td>
	<td class="line x" title="95:214	5If the same part appears in both Rt and Rt+1, we remove it from both sets." ></td>
	<td class="line x" title="96:214	jprime),sign(kkprime)) is one of 33 values." ></td>
	<td class="line x" title="97:214	We define a migration as a triple (c,cprime,r(l,lprime)); this is the unit of change we want to extract from the meta-model." ></td>
	<td class="line x" title="98:214	Our meta-model provides the following generative story of how Rt becomes Rt+1: each new part (cprime,lprime) Rt+1 chooses an old part (c,l) Rt with some probability that depends on (1) the distance between the locations l and lprime and (2) the likelihood of the particular migration." ></td>
	<td class="line x" title="99:214	Formally: pmeta(Rt+1|Rt) =productdisplay (cprime,lprime)Rt+1 summationdisplay (c,l)Rt Z1lprime ed(l,lprime)p(cprime|c,r(l,lprime)), where Zl = summationtext(c,l)Rt ed(l,lprime) is a normalization constant, and  is a hyperparameter controlling the possibility of distant migrations (set to 3 in our experiments)." ></td>
	<td class="line x" title="100:214	We learn the parameters of the meta-model with an EM algorithm similar to the one for IBM model 1." ></td>
	<td class="line x" title="101:214	Fortunately, the likelihood objective is convex, so we need not worry about local optima." ></td>
	<td class="line x" title="102:214	4.2 Results of the meta-model We used our meta-model to analyze the approximation errors of the HMM, DMV, and PCFG." ></td>
	<td class="line x" title="103:214	For these models, we initialized EM with the supervised estimate gen and collected the model predictions as EM ran." ></td>
	<td class="line x" title="104:214	We then trained the meta-model on the predictions between successive iterations." ></td>
	<td class="line x" title="105:214	The metamodel gives us an expected count for each migration." ></td>
	<td class="line x" title="106:214	Figure 3 lists the migrations with the highest expected counts." ></td>
	<td class="line x" title="107:214	From these migrations, we can see that EM tries to explain x better by making the corresponding y more regular." ></td>
	<td class="line x" title="108:214	In fact, many of the HMM migrations on the first iteration attempt to resolve inconsistencies in gold tags." ></td>
	<td class="line x" title="109:214	For example, noun adjuncts (e.g., stock-index), tagged as both nouns and adjectives in the Treebank, tend to become consolidated under adjectives, as captured by migration (B)." ></td>
	<td class="line x" title="110:214	EM also re-purposes under-utilized states to better capture distributional similarities." ></td>
	<td class="line x" title="111:214	For example, state 24 has migrated to state 40 (N), both of which are now dominated by proper nouns." ></td>
	<td class="line x" title="112:214	State 40 initially contained only #, but was quickly overrun with distributionally similar proper nouns such as Oct. and Chapter, which also precede numbers, just as # does." ></td>
	<td class="line x" title="113:214	882 Figure 3: We show the prominent migrations that occur during the first 5 iterations of EM for the HMM, DMV, and PCFG, as recovered by our meta-model." ></td>
	<td class="line x" title="114:214	We sort the migrations across each iteration by their expected counts under the meta-model and show the top 3." ></td>
	<td class="line x" title="115:214	Iteration 0 corresponds to the correct outputs." ></td>
	<td class="line x" title="116:214	Blue indicates the new iteration, red indicates the old." ></td>
	<td class="line x" title="117:214	DMV migrations also try to regularize model predictions, but in a different wayin terms of the number of arguments." ></td>
	<td class="line x" title="118:214	Because the stop probability is different for adjacent and non-adjacent arguments, it is statistically much cheaper to generate one argument rather than two or more." ></td>
	<td class="line x" title="119:214	For example, if we train a DMV on only DT JJ NN, it can fit the data perfectly by using a chain of single arguments, but perfect fit is not possible if NN generates both DT and JJ (which is the desired structure); this explains migration (J)." ></td>
	<td class="line x" title="120:214	Indeed, we observed that the variance of the number of arguments decreases with more EM iterations (for NN, from 1.38 to 0.41)." ></td>
	<td class="line x" title="121:214	In general, low-entropy conditional distributions are preferred." ></td>
	<td class="line x" title="122:214	Migration (H) explains how adverbs now consistently attach to verbs rather than modals." ></td>
	<td class="line x" title="123:214	After a few iterations, the modal has committed itself to generating exactly one verb to the right, which is statistically advantageous because there must be a verb after a modal, while the adverb is optional." ></td>
	<td class="line x" title="124:214	This leaves the verb to generate the adverb." ></td>
	<td class="line x" title="125:214	The PCFG migrations regularize categories in a manner similar to the HMM, but with the added complexity of changing bracketing structures." ></td>
	<td class="line x" title="126:214	For example, sentential adverbs are re-analyzed as VP adverbs (A)." ></td>
	<td class="line x" title="127:214	Sometimes, multiple migrations explain the same phenomenon.6 For example, migrations (B) and (C) indicate that PPs that previously attached to NPs are now raised to the verbal level." ></td>
	<td class="line x" title="128:214	Tree rotation is another common phenomenon, leading to many left-branching structures (D,G,H)." ></td>
	<td class="line x" title="129:214	The migrations that happen during one iteration can also trigger additional migrations in the next." ></td>
	<td class="line x" title="130:214	For example, the raising of the PP (B,C) inspires more of the 6We could consolidate these migrations by using larger configurations, but at the risk of decreased generalization." ></td>
	<td class="line x" title="131:214	883 same raising (E)." ></td>
	<td class="line x" title="132:214	As another example, migration (I) regularizes TO VB infinitival clauses into PPs, and this momentum carries over to the next iteration with even greater force (J)." ></td>
	<td class="line x" title="133:214	In summary, the meta-model facilitates our analyses by automatically identifying the broad trends." ></td>
	<td class="line x" title="134:214	We believe that the central idea of modeling the errors of a system is a powerful one which can be used to analyze a wide range of models, both supervised and unsupervised." ></td>
	<td class="line x" title="135:214	5 Identifiability error While approximation error is incurred when likelihood diverges from accuracy, identifiability error is concerned with the case where likelihood is indifferent to accuracy." ></td>
	<td class="line x" title="136:214	We say a set of parameters S is identifiable (in terms of x) if p(x) negationslash= pprime(x) for every ,prime  S where  negationslash= prime.7 In general, identifiability error is incurred when the set of maximizers of Elogp(x) is non-identifiable.8 Label symmetry is perhaps the most familiar example of non-identifiability and is intrinsic to models with hidden labels (HMM and PCFG, but not DMV)." ></td>
	<td class="line x" title="137:214	We can permute the hidden labels without changing the objective function or even the nature of the solution, so there is no reason to prefer one permutation over another." ></td>
	<td class="line x" title="138:214	While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1)." ></td>
	<td class="line x" title="139:214	Grenager et al.(2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q. Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution." ></td>
	<td class="line x" title="141:214	This is a case where our notion of desired structure is absent in the likelihood, and a prior over parameters could help break ties." ></td>
	<td class="line x" title="142:214	7For our three model families,  is identifiable in terms of (x,y), but not in terms of x alone." ></td>
	<td class="line x" title="143:214	8We emphasize that non-identifiability is in terms of x, so two parameter settings could still induce the same marginal distribution on x (weak generative capacity) while having different joint distributions on (x,y) (strong generative capacity)." ></td>
	<td class="line x" title="144:214	Recall that discrepancy depends on the latter." ></td>
	<td class="line x" title="145:214	The above non-identifiabilities apply to all parameter settings, but another type of non-identifiability concerns only the maximizers of Elogp(x)." ></td>
	<td class="line x" title="146:214	Suppose the true data comes from a K-state HMM." ></td>
	<td class="line x" title="147:214	If we attempt to fit an HMM with K + 1 states, we can split any one of the K states and maintain the same distribution on x. Or, if we learn a PCFG on the same HMM data, then we can choose either the leftor right-branching chain structures, which both mimic the true HMM equally well." ></td>
	<td class="line x" title="148:214	5.1 Permutation-invariant distance KL-divergence is a natural measure of discrepancy between two distributions, but it is somewhat nontrivial to computefor our three recursive models, it requires solving fixed point equations, and becomes completely intractable in face of label symmetry." ></td>
	<td class="line x" title="149:214	Thus we propose a more manageable alternative: d(||prime) def= summationtext jj|j primej| summationtext jj , (1) where we weight the difference between the j-th component of the parameter vectors by j, the jth expected sufficient statistic with respect to p (the expected counts computed in the E-step).9 Unlike KL, our distance d is only defined on distributions in the model family and is not invariant to reparametrization." ></td>
	<td class="line x" title="150:214	Like KL, d is asymmetric, with the first argument holding the status of being the true parameter setting." ></td>
	<td class="line x" title="151:214	In our case, the parameters are conditional probabilities, so 0d(||prime)1, so we can interpret d as an expected difference between these probabilities." ></td>
	<td class="line x" title="152:214	Unfortunately, label symmetry can wreak havoc on our distance measure d." ></td>
	<td class="line x" title="153:214	Suppose we want to measure the distance between  and prime." ></td>
	<td class="line x" title="154:214	If prime is simply  with the labels permuted, then d(||prime) would be substantial even though the distance ought to be zero." ></td>
	<td class="line x" title="155:214	We define a revised distance to correct for this by taking the minimum distance over all label permutations: D(||prime) = minpi d(||pi(prime)), (2) 9Without this factor, rarely used components could contribute to the sum as much as frequently used ones, thus, making the distance overly pessimistic." ></td>
	<td class="line x" title="156:214	884 where pi(prime) denotes the parameter setting resulting from permuting the labels according to pi." ></td>
	<td class="line x" title="157:214	(The DMV has no label symmetries, so just d works.)" ></td>
	<td class="line x" title="158:214	For mixture models, we can compute D(||prime) efficiently as follows." ></td>
	<td class="line x" title="159:214	Note that each term in the summation of (1) is associated with one of the K labels." ></td>
	<td class="line x" title="160:214	We can form aKK matrixM, where each entry Mij is the distance between the parameters involving label i of  and label j of prime." ></td>
	<td class="line x" title="161:214	D(||prime) can then be computed by finding a maximum weighted bipartite matching on M using the O(K3) Hungarian algorithm (Kuhn, 1955)." ></td>
	<td class="line x" title="162:214	For models such as the HMM and PCFG, computingD is NP-hard, since the summation ind (1) contains both first-order terms which depend on one label (e.g., emission parameters) and higher-order terms which depend on more than one label (e.g., transitions or rewrites)." ></td>
	<td class="line x" title="163:214	We cannot capture these problematic higher-order dependencies in M. However, we can bound D(||prime) as follows." ></td>
	<td class="line x" title="164:214	We create M using only first-order terms and find the best matching (permutation) to obtain a lower bound D and an associated permutation pi0 achieving it." ></td>
	<td class="line x" title="165:214	Since D(||prime) takes the minimum over all permutations, d(||pi(prime)) is an upper bound for any pi, in particular for pi = pi0." ></td>
	<td class="line x" title="166:214	We then use a local search procedure that changes pi to further tighten the upper bound." ></td>
	<td class="line x" title="167:214	Let D denote the final value." ></td>
	<td class="line x" title="168:214	6 Estimation error Thus far, we have considered approximation and identifiability errors, which have to do with flaws of the model." ></td>
	<td class="line x" title="169:214	The remaining errors have to do with how well we can fit the model." ></td>
	<td class="line x" title="170:214	To focus on these errors, we consider the case where the true model is in our family (pP)." ></td>
	<td class="line x" title="171:214	To keep the setting as realistic as possible, we do supervised learning on real labeled data to obtain  = argmax Elogp(x,y)." ></td>
	<td class="line x" title="172:214	We then throw away our real data and let p = p." ></td>
	<td class="line x" title="173:214	Now we start anew: sample new artificial data from , learn a model using this artificial data, and see how close we get to recovering ." ></td>
	<td class="line x" title="174:214	In order to compute estimation error, we need to compare with , the global maximizer of the likelihood on our generated data." ></td>
	<td class="line x" title="175:214	However, we cannot compute  exactly." ></td>
	<td class="line x" title="176:214	Let us therefore first consider the simpler supervised scenario." ></td>
	<td class="line x" title="177:214	Here, gen has a closed form solution, so there is no optimization error." ></td>
	<td class="line x" title="178:214	Using our distanceD (defined in Section 5.1) to quantify estimation error, we see that, for the HMM, gen quickly approaches  as we increase the amount of data (Table 1)." ></td>
	<td class="line x" title="179:214	# examples 500 5K 50K 500K D(||gen) 0.003 6.3e-4 2.7e-4 8.5e-5 D(||gen) 0.005 0.001 5.2e-4 1.7e-4 D(||gen-EM) 0.022 0.018 0.008 0.002 D(||gen-EM) 0.049 0.039 0.016 0.004 Table 1: Lower and upper bounds on the distance from the true  for the HMM as we increase the number of examples." ></td>
	<td class="line x" title="180:214	In the unsupervised case, we use the following procedure to obtain a surrogate for : initialize EM with the supervised estimate gen and run EM for 100 iterations." ></td>
	<td class="line x" title="181:214	Let gen-EM denote the final parameters, which should be representative of ." ></td>
	<td class="line x" title="182:214	Table 1 shows that the estimation error of gen-EM is an order of magnitude higher than that of gen, which is to expected since gen-EM does not have access to labeled data." ></td>
	<td class="line x" title="183:214	However, this error can also be driven down given a moderate number of examples." ></td>
	<td class="line x" title="184:214	7 Optimization error Finally, we study optimization error, which is the discrepancy between the global maximizer  and EM, the result of running EM starting from a uniform initialization (plus some small noise)." ></td>
	<td class="line x" title="185:214	As before, we cannot compute , so we use gen-EM as a surrogate." ></td>
	<td class="line x" title="186:214	Also, instead of comparing gen-EM and  with each other, we compare each of their discrepancies with respect to ." ></td>
	<td class="line x" title="187:214	Let us first consider optimization error in terms of prediction error." ></td>
	<td class="line x" title="188:214	The first observation is that there is a gap between the prediction accuracies of gen-EM and EM, but this gap shrinks considerably as we increase the number of examples." ></td>
	<td class="line x" title="189:214	Figures 4(a,b,c) support this for all three model families: for the HMM, both gen-EM and EM eventually achieve around 90% accuracy; for the DMV, 85%." ></td>
	<td class="line x" title="190:214	For the PCFG, EM still lags gen-EM by 10%, but we believe that more data can further reduce this gap." ></td>
	<td class="line x" title="191:214	Figure 4(d) shows that these trends are not particular to artificial data." ></td>
	<td class="line x" title="192:214	On real WSJ data, the gap 885 5005K 50K500K#examples 0.60.7 0.80.9 1.0 Accuracy 5005K 50K500K#examples 0.60.7 0.80.9 1.0 Directed F1 500 5K 50K#examples 0.50.6 0.80.9 1.0 Lab eled F1 1K 3K 10K40K#examples 0.30.4 0.60.7 0.8 Accuracy (a)HMM(arti cialdata)(b)DMV(arti cialdata)(c)PCFG(arti cialdata)(d)HMM(realdata) 5005K 50K500K#examples 0.020.05 0.070.1 0.12 D ( jj ) ^ gen-EM^ EM(rand1)^ EM(rand2)^ EM(rand3) 20 40 60 80 100 iteration -173.3-171.4 -169.4-167.4 -165.5 log-lik eliho od 20 40 60 80 100iteration 0.20.4 0.60.8 1.0 Accuracy Sup.init.Unif.init." ></td>
	<td class="line x" title="193:214	(e)HMM(arti cialdata) (f)HMMlog-likelihood/accuracyon500Kexamples Figure 4: Compares the performance of EM (EM with a uniform initialization) against gen-EM (EM initialized with the supervised estimate) on (ac) various models, (d) real data." ></td>
	<td class="line x" title="194:214	(e) measures distance instead of accuracy and (f) shows a sample EM run." ></td>
	<td class="line x" title="195:214	between gen-EM and EM also diminishes for the HMM." ></td>
	<td class="line x" title="196:214	To reaffirm the trends, we also measure distance D." ></td>
	<td class="line x" title="197:214	Figure 4(e) shows that the distance from EM to the true parameters  decreases, but the gap between gen-EM and EM does not close as decisively as it did for prediction error." ></td>
	<td class="line x" title="198:214	It is quite surprising that by simply running EM with a neutral initialization, we can accurately learn a complex model with thousands of parameters." ></td>
	<td class="line x" title="199:214	Figures 4(f,g) show how both likelihood and accuracy, which both start quite low, improve substantially over time for the HMM on artificial data." ></td>
	<td class="line x" title="200:214	Carroll and Charniak (1992) report that EM fared poorly with local optima." ></td>
	<td class="line x" title="201:214	We do not claim that there are no local optima, but only that the likelihood surface that EM is optimizing can become smoother with more examples." ></td>
	<td class="line x" title="202:214	With more examples, there is less noise in the aggregate statistics, so it might be easier for EM to pick out the salient patterns." ></td>
	<td class="line x" title="203:214	Srebro et al.(2006) made a similar observation in the context of learning Gaussian mixtures." ></td>
	<td class="line x" title="205:214	They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data)." ></td>
	<td class="line x" title="206:214	There is also a rich body of theoretical work on learning latent-variable models." ></td>
	<td class="line x" title="207:214	Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005)." ></td>
	<td class="line x" title="208:214	But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV." ></td>
	<td class="line x" title="209:214	8 Conclusion In recent years, many methods have improved unsupervised induction, but these methods must still deal with the four types of errors we have identified in this paper." ></td>
	<td class="line x" title="210:214	One of our main contributions of this paper is the idea of using the meta-model to diagnose the approximation error." ></td>
	<td class="line x" title="211:214	Using this tool, we can better understand model biases and hopefully correct for them." ></td>
	<td class="line x" title="212:214	We also introduced a method for measuring distances in face of label symmetry and ran experiments exploring the effectiveness of EM as a function of the amount of data." ></td>
	<td class="line x" title="213:214	Finally, we hope that setting up the general framework to understand the errors of unsupervised induction systems will aid the development of better methods and further analyses." ></td>
	<td class="line x" title="214:214	886" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1071
The infinite HMM for unsupervised PoS tagging
Van Gael, Jurgen;Vlachos, Andreas;Ghahramani, Zoubin;"></td>
	<td class="line x" title="1:228	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678687, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:228	c 2009 ACL and AFNLP The infinite HMM for unsupervised PoS tagging Jurgen Van Gael Department of Engineering University of Cambridge jv249@cam.ac.uk Andreas Vlachos Computer Laboratory University of Cambridge av308@cl.cam.ac.uk Zoubin Ghahramani Department of Engineering University of Cambridge zoubin@eng.cam.ac.uk Abstract We extend previous work on fully unsupervised part-of-speech tagging." ></td>
	<td class="line x" title="3:228	Using a non-parametric version of the HMM, called the infinite HMM (iHMM), we address the problem of choosing the number of hidden states in unsupervised Markov models for PoS tagging." ></td>
	<td class="line x" title="4:228	We experiment with two non-parametric priors, the Dirichlet and Pitman-Yor processes, on the Wall Street Journal dataset using a parallelized implementation of an iHMM inference algorithm." ></td>
	<td class="line x" title="5:228	We evaluate the results with a variety of clustering evaluation metrics and achieve equivalent or better performances than previously reported." ></td>
	<td class="line x" title="6:228	Building on this promising result we evaluate the output of the unsupervised PoS tagger as a direct replacement for the output of a fully supervised PoS tagger for the task of shallow parsing and compare the two evaluations." ></td>
	<td class="line x" title="7:228	1 Introduction Many Natural Language Processing (NLP) tasks are commonly tackled using supervised learning approaches." ></td>
	<td class="line x" title="8:228	These learning methods rely on the availability of labeled datasets which are usually produced by expensive manual annotation." ></td>
	<td class="line x" title="9:228	For some tasks, we have the choice to use unsupervised learning approaches." ></td>
	<td class="line x" title="10:228	While they do not necessarily achieve the same level of performance, they are appealing as unlabeled data is usually abundant." ></td>
	<td class="line x" title="11:228	In particular, for the purpose of exploring new domains and languages, obtainining labeled material can be prohibitively expensive and unsupervised learning methods are a very attractive choice." ></td>
	<td class="line pc" title="12:228	Recent work (Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008) explored the task of part-of-speech tagging (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results." ></td>
	<td class="line x" title="13:228	PoS tagging is a standard component in many linguistic processing pipelines, so any improvement on its performance is likely to impact a wide range of tasks." ></td>
	<td class="line x" title="14:228	It is important to point out that a completely unsupervised learning method will discover the statistics of a dataset according to a particular model choice but these statistics might not correspond exactly to our intuition about PoS tags." ></td>
	<td class="line oc" title="15:228	Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags." ></td>
	<td class="line x" title="16:228	Nonetheless, identifying the HMM states with appropriate POS tags is hard." ></td>
	<td class="line x" title="17:228	Because many evaluation methods often require POS tags (rather than HMM states) this identification problem makes unsupervised systems difficult to evaluate." ></td>
	<td class="line x" title="18:228	One potential solution is to add a small amount of supervision as in Goldwater & Griffiths (2007) who assume a dictionary of frequent words associated with possible PoS tags extracted from a labeled corpus." ></td>
	<td class="line x" title="19:228	Although this technique improves performance, in this paper we explore the completely unsupervised approach." ></td>
	<td class="line x" title="20:228	The reason for this is that better unsupervised approaches provide us with better starting points from which to explore how and where to incorporate supervision." ></td>
	<td class="line x" title="21:228	In previous work on unsupervised PoS tagging a main question was how to set the number of hidden states appropriately." ></td>
	<td class="line nc" title="22:228	Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work." ></td>
	<td class="line x" title="23:228	It is not uncommon in statistical machine learning to distinguish between parameters of a model and the capacity of a model." ></td>
	<td class="line x" title="24:228	E.g. in a clustering context, the choice for the number of clusters (capacity) and the parameters of each cluster are often 678 treated differently: the latter are estimated using algorithms like EM, MCMC or Variational Bayes while the former is chosen using common sense, heuristics or in a Bayesian framework maybe using evidence maximization." ></td>
	<td class="line x" title="25:228	Non-parametric Bayesian methods are a class of probability distributions which explicitly treat the capacity of a model as just another parameter." ></td>
	<td class="line x" title="26:228	Potential advantages are  the model capacity can automatically adjust to the amount of data: e.g. when clustering a very small dataset, it is unlikely that many fine grained clusters can be distinguished,  inference can be more efficient: e.g. instead of running full inference for different model capacities and then choosing the best capacity (according to some choice of best), inference in non-parametric Bayesian methods integrates the capacity search in one algorithm." ></td>
	<td class="line x" title="27:228	This is particularly advantageous when parameters other than capacity need to be explored, since it reduces signifcantly the number of experiments needed." ></td>
	<td class="line x" title="28:228	None of these potential advantages are guaranteed and in this paper we investigate these two aspects for the task of unsupervised PoS tagging." ></td>
	<td class="line x" title="29:228	The contributions in this paper extend previous work on unsupervised PoS tagging in five ways." ></td>
	<td class="line x" title="30:228	First, we introduce the use of a non-parametric version of the HMM, namely the infinite HMM (iHMM) (Beal et al., 2002) for unsupervised PoS tagging." ></td>
	<td class="line x" title="31:228	This answers an open problem from Goldwater & Griffiths (2007)." ></td>
	<td class="line x" title="32:228	Second, we carefully implemented a parallelized version of the inference algorithms for the iHMM so we could use it on the Wall Street Journal Penn Treebank dataset." ></td>
	<td class="line x" title="33:228	Third, we introduce a new variant of the iHMM that builds on the Pitman-Yor process." ></td>
	<td class="line x" title="34:228	Fourth, we evaluate the results with a variety of clustering evaluation methods and achieve equivalent or better performances than previously reported." ></td>
	<td class="line x" title="35:228	Finally, building on this promising result we use the output of the unsupervised PoS tagger as a direct replacement for the output of a fully supervised PoS tagger for the task of shallow parsing." ></td>
	<td class="line x" title="36:228	This evaluation enables us to assess the applicability of an unsupervised PoS tagging method and provides us with means of comparing its performance against a supervised PoS tagger." ></td>
	<td class="line x" title="37:228	The rest of the paper is structured as follows: in section 2 we introduce the iHMM as a nonparametric version of the Bayesian HMM used in previous work on unsupervised PoS tagging." ></td>
	<td class="line x" title="38:228	Then, in section 3 we describe some details of our implementation of the iHMM." ></td>
	<td class="line x" title="39:228	In section 4 we present a variety of evaluation metrics to compare our results with previous work." ></td>
	<td class="line x" title="40:228	Finally, in section 5 we report our experimental results." ></td>
	<td class="line x" title="41:228	We conclude this paper with a discussion of ongoing work and experiments." ></td>
	<td class="line x" title="42:228	2 The Infinite HMM In this section, we describe a non-parametric hidden Markov model known as the infinite HMM (iHMM) (Beal et al., 2002; Teh et al., 2006)." ></td>
	<td class="line x" title="43:228	As we show below, this model is flexible in the number of hidden states which it can accomodate." ></td>
	<td class="line x" title="44:228	In other words, the capacity is an uncertain quantity with an a priori infinite range that is a posteriori inferred by the data." ></td>
	<td class="line x" title="45:228	It is instructive to first review the finite HMM and its Bayesian treatment: for one, it is the model that has been used in previous work on unsupervised PoS tagging, secondly it allows us to better understand the iHMM." ></td>
	<td class="line x" title="46:228	The Bayesian HMM A finite first-order HMM consists of a hidden state sequence s = (s1,s2,,sT) and a corresponding observation sequence y = (y1,y2,,yT)." ></td>
	<td class="line x" title="47:228	Each state variable st can take on a finite number of states, say 1K." ></td>
	<td class="line x" title="48:228	Transitions between states are governed by Markov dynamics parameterized by the transition matrix pi, where piij = p(st = j|st1 = i), while the initial state probabilities are pi0i = p(s1 = i)." ></td>
	<td class="line x" title="49:228	For each state st {1K} there is a parameter st which parameterizes the observation likelihood for that state: yt|st  F(st)." ></td>
	<td class="line pc" title="50:228	Given the parameters{pi0,pi,,K}of the HMM, the joint distribution over hidden states s and observationsy can be written (with s0 = 0): p(s,y|pi0,pi,,K) = Tproductdisplay t=1 p(st|st1)p(yt|st) As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging." ></td>
	<td class="line x" title="51:228	However, we can easily treat the HMM in a fully Bayesian way (MacKay, 1997) by introducing priors on the parameters of the HMM." ></td>
	<td class="line x" title="52:228	With no further prior knowledge, a typical prior for the transition (and initial) probabilities are symmetric Dirichlet distributions." ></td>
	<td class="line x" title="53:228	This corresponds to our 679 belief that, a priori, each state is equally likely to transition to every other state." ></td>
	<td class="line x" title="54:228	Also, it is commonly known that the parameter of a Dirichlet distribution controls how sparse its samples are." ></td>
	<td class="line x" title="55:228	In other words, by making the hyperprior on the Dirichlet distribution for the rows of the transition matrix small, we can encode our belief that any state (corresponding to a PoS tag in this application context) will only be followed by a small number of other states." ></td>
	<td class="line x" title="56:228	As we explain below, we will be able to include this desirable property in the non-parametric model as well." ></td>
	<td class="line x" title="57:228	Secondly, we need to introduce a prior on the observation parameters k. Without any further prior knowledge, a convenient choice here is another symmetric Dirichlet distribution with sparsity inducing hyperprior." ></td>
	<td class="line x" title="58:228	This encodes our belief that only a subset of the words correspond to a particular state." ></td>
	<td class="line x" title="59:228	The Infinite HMM A first nave way to obtain a non-parametric HMM with an infinite number of states might be to use symmetric Dirichlet priors over the transition probabilities with parameter /K and take K ." ></td>
	<td class="line x" title="60:228	This approach unfortunately does not work: /K  0 when K  and hence the rows of the matrix will become infinitely sparse." ></td>
	<td class="line x" title="61:228	Since the sum of the entries must sum to one, the rows of the transition matrix will be zero everywhere and all its mass in a random location." ></td>
	<td class="line x" title="62:228	Unfortunately, this random location is out of an infinite number of possible locations and hence with probability 1 will be different for all the rows." ></td>
	<td class="line x" title="63:228	As a consequence, at each timestep the HMM moves to a new state and will never revisit old states." ></td>
	<td class="line x" title="64:228	As we shall see shortly, we can fix this by using a hierarchical Bayesian formalism where the Dirichlet priors on the rows have a shared parameter." ></td>
	<td class="line x" title="65:228	Before moving on to the iHMM, let us look at the finite HMM from a different perspective." ></td>
	<td class="line x" title="66:228	The finite HMM of length T with K hidden states can be seen as a sequence of T finite mixture models." ></td>
	<td class="line x" title="67:228	The following equation illustrates this idea: conditioned on the previous state st1, the marginal probability of observation yt can be written as: p(yt|st1 = k) = Ksummationdisplay st=1 p(st|st1 = k)p(yt|st), = Ksummationdisplay st=1 pik,stp(yt|st)." ></td>
	<td class="line x" title="68:228	(1) The variable st1 = k specifies the mixing weights pik, for the mixture distribution, while st indexes the mixture component generating the observation yt." ></td>
	<td class="line x" title="69:228	In other words, equation (1) says that each row of the transition matrixpi specifies a different mixture distribution over the same set of K mixture components." ></td>
	<td class="line x" title="70:228	Our second attempt to define a non-parametric version of the hidden Markov model is to replace the finite mixture by an infinite mixture." ></td>
	<td class="line x" title="71:228	The theory of Dirichlet process mixtures (Antoniak, 1974) tells us exactly how to do this." ></td>
	<td class="line x" title="72:228	A draw G  DP(,H) from a Dirichlet process (DP) with base measure H and concentration parameter   0 is a discrete distribution which can be written as an infinite mixture of atoms G() = summationdisplay i=1 piii() where the i are i.i.d. draws from the base measure H, i() represents a point distribution at i and pii = viproducttexti1l=1(1vl) where each vl  Beta(1,)." ></td>
	<td class="line x" title="73:228	The distribution over pii is called a stick breaking construction and is essentially an infinite dimensional version of the Dirichlet distribution." ></td>
	<td class="line x" title="74:228	We refer to Teh et al.(2006) for more details." ></td>
	<td class="line x" title="76:228	Switching back to the iHMM our next step is to introduce a DP Gj for each state j {1}; we write Gj() = summationtexti=1pijij i ()." ></td>
	<td class="line x" title="77:228	There is now a parameter for each state j and each index i  {1,2,,}." ></td>
	<td class="line x" title="78:228	Next, we draw the datapoint at timestep t given that the previous datapoint was in state st1 by drawing from DP Gst1." ></td>
	<td class="line x" title="79:228	We first select a mixture componentst from the vectorpist1, and then sample a datapoint yt F(st1,st) so we get the following distribution for yt p(yt|,st1) = summationdisplay st=1 pist1,stp(yt|st1,st)." ></td>
	<td class="line x" title="80:228	This is almost the non-parametric equivalent of equation (1) but there is a subtle difference: each Gj selects their own set of parameters j." ></td>
	<td class="line x" title="81:228	This is unfortunate as it means that the output distribution would not be the same for each state, it would depend on which state we were moving to!" ></td>
	<td class="line x" title="82:228	Luckily, we can easily fix this: by introducing an intermediate distribution G0  DP(,H) and let Gj DP(,G0) we enforce that the i.i.d. draws j are draws from a discrete distribution (sinceG0 680 is a draw from a Dirichlet process) and hence all Gj will share the same infinite set of atoms as chosen byG0." ></td>
	<td class="line x" title="83:228	Figure 1 illustrates the graphical model for the iHMM." ></td>
	<td class="line x" title="84:228	The iHMM with Pitman-Yor Prior The Dirichlet process described above defines a very specific distribution over the number of states in the iHMM." ></td>
	<td class="line x" title="85:228	One particular generalization of the Dirichlet process that has been studied in the NLP literature before is the Pitman-Yor process." ></td>
	<td class="line x" title="86:228	Goldwater et al.(2006) have shown that the Pitman-Yor distribution can more accurately capture power-law like distributions that frequently occur in natural language." ></td>
	<td class="line x" title="88:228	More specifically, a draw G  PY(d,,H) from a Pitman-Yor process (PY) with base measure H, discount parameter 0 d < 1 and concentration parameter  >d is a discrete distribution which can be written as an infinite mixture of atoms G() = summationdisplay i=1 piii() where the i are i.i.d. draws from the base measure H, i() represents a point distribution at i and pii = viproducttexti1l=1(1vl) where each vl  Beta(1d,+ld)." ></td>
	<td class="line x" title="89:228	Note the similarity to the DP: in fact, the DP is a special case of PY with d = 0." ></td>
	<td class="line x" title="90:228	In our experiments, we constructed an iHMM where the DP(,H) base measure G0 is replaced with its two parameter generalization PY(d,,H)." ></td>
	<td class="line x" title="91:228	Because the Dirichlet and PitmanYor processes only differ in the way pi is constructed, without loss of generality we will describe hyper-parameter choice and inference in the context of the iHMM with Dirichlet process base measure." ></td>
	<td class="line x" title="92:228	Hyperparameter Choice The description above shows that there are 4 parameters which we must specify: the base measure H, the output distribution p(yt|st), the discount1 and concentration2 parameters d, for G0 and the concentration parameterfor the DPsGj." ></td>
	<td class="line x" title="93:228	Just as in the finite case, the base measure H is the prior distribution on the parameter  of p(yt|st)." ></td>
	<td class="line x" title="94:228	We chose to use a symmetric Dirichlet distribution with parameter  over the word types in our corpus." ></td>
	<td class="line x" title="95:228	Since we do not know the sparsity level  of the output distributions we decided to learn this 1for Pitman-Yor base measure 2for both Dirichlet and Pitman-Yor base measures parameter from the data." ></td>
	<td class="line x" title="96:228	We initially set a vague Gamma prior over  but soon realized that as we expect hidden states in the iHMM to correspond to PoS tags, it is unrealistic to expect each state to have the same sparsity level." ></td>
	<td class="line x" title="97:228	Hence we chose a Dirichlet process as the prior for ; this way we end up with a small discrete set of sparsity levels: e.g. we can learn that states corresponding to verbs and nouns share one sparsity level while states correpsonding to determiners have their own (much sparser) sparsity level." ></td>
	<td class="line x" title="98:228	For the output distribution p(yt|st) we chose a simple multinomial distribution." ></td>
	<td class="line x" title="99:228	The hyperparametersdand mostly control the number of states in the iHMM while as we discussed above  controls the sparsity of the transition matrix." ></td>
	<td class="line x" title="100:228	In the experiments below we report both fixing the two parameters and learning them by sampling (using vague Gamma hyperpriors)." ></td>
	<td class="line x" title="101:228	Because of computational constraints, we chose to use vague Bayesian priors for all hyperparameters rather than run the whole experiment over a grid of reasonable parameter settings and use the best ones according to cross validation." ></td>
	<td class="line x" title="102:228	3 Inference The Wall Street Journal part of the Penn Treebank that was used for our experiments contains about one million words." ></td>
	<td class="line x" title="103:228	In the non-parametric Bayesian literature not many algorithms have been described that scale into this regime." ></td>
	<td class="line x" title="104:228	In this section we describe our parallel implementation of the iHMM which can easily handle a dataset of this scale." ></td>
	<td class="line x" title="105:228	There is a wealth of evidence (Scott, 2002; Gao and Johnson, 2008) in the machine learning literature that Gibbs sampling for Markov models leads to slow mixing times." ></td>
	<td class="line x" title="106:228	Hence we decided our starting point for inference needs to be based on dynamic programming." ></td>
	<td class="line x" title="107:228	Because we didnt have a good idea for the number of states that we were going to end up with, we prefered the beam sampler of Van Gael et al.(2008) over a finite truncation of the iHMM." ></td>
	<td class="line x" title="109:228	Moreover, the beam sampler also introduces a certain amount of sparsity in the dynamic program which can speed up computations (potentially at the cost of slower mixing)." ></td>
	<td class="line x" title="110:228	The beam sampler is a blocked Gibbs sampler where we alternate between sampling the parameters (transition matrix, output parameters), the state sequence and the hyperparameters." ></td>
	<td class="line x" title="111:228	Sam681 k=11 s01s2ykHFigure 1: The graphical model for the iHMM." ></td>
	<td class="line x" title="112:228	The variable  represents the mixture for the DP G0.pling the transition matrix and output distribu-tion parameters requires computing their sufficientstatistics and sampling from a Dirichlet distribu-tion; we refer to the beam sampling paper for de-tails." ></td>
	<td class="line x" title="113:228	For the hyperparameters we use standardGibbs sampling." ></td>
	<td class="line x" title="114:228	We briefly sketch the resam-pling step for the state sequence for a single se-quence of data (sentence of words)." ></td>
	<td class="line x" title="115:228	Running stan-dard dynamic programming is prohibitive becausethe state space of the iHMM is infinitely large.The central idea of the beam sampler is to adap-tively truncate the state space of the iHMM andrun dynamic programming." ></td>
	<td class="line x" title="116:228	In order to truncatethe state space, we sample an auxilary variable utfor each word in the sequence from the distribu-tion ut Uniform(0,pist1st) where pi representsthe transition matrix.Intuitively, when we sample u1:T|s1:T accord-ing to the distribution above, the only valid sam-ples are those for which the ut are smaller thanthe transition probabilities of the state sequences1:T. This means that when we sample s1:T|u1:Tat a later point, it must be the case that the utsare still smaller than the new transition probabil-ities." ></td>
	<td class="line x" title="117:228	This significantly reduces the set of validstate sequences that we need to consider." ></td>
	<td class="line x" title="118:228	Morespecifically, Van Gael et al.(2008) show that wecan compute p(st|y1:t,u1:t) using the followingdynamic programming recursionp(st|y1:t,u1:t) = p(yt|st) summationdisplay st1:ut<pist1,st p(st1|y1:t1,u1:t1)." ></td>
	<td class="line x" title="120:228	The summationsummationtextst1:ut<pis t1,st ensures that this computation remains finite." ></td>
	<td class="line x" title="121:228	When we compute p(st|y1:t,u1:t) for t  {1T}, we can easily sample sT and using Bayes rule backtrack sample every other st. It can be shown that this procedure produces samples from the exact posterior." ></td>
	<td class="line x" title="122:228	Notice that the dynamic program only needs to perform computation when ut <pist1,st. A careful implementation of the beam sampler consists of preprocessing the transition matrix pi and sorting its elements in descending order." ></td>
	<td class="line x" title="123:228	We can then iterate over the elements of the transition matrix starting from the largest element and stop once we reach the first element of the transition matrix smaller than ut. In our experiments we found that this optimization reduces the amount of computation per sentence by an order of magnitutde." ></td>
	<td class="line x" title="124:228	A second optimization which we introduced is to use the map-reduce paradigm (Dean and Ghemawat, 2004) to parallelize our computations." ></td>
	<td class="line x" title="125:228	More specifically, after we preprocess the transition matrix, the dynamic program computations are independent for each sentence in the dataset." ></td>
	<td class="line x" title="126:228	This means we can perform each dynamic program in parallel; in other words our map consists of running the dynamic program on one sentence in the dataset." ></td>
	<td class="line x" title="127:228	Next, we need to resample the transition matrix and output distribution parameters." ></td>
	<td class="line x" title="128:228	In order to do so we need to compute their sufficient statistics: the number of transitions from state to state and the number of emissions of each word out of each state." ></td>
	<td class="line x" title="129:228	Our reduce function consists of computing the sufficient statistics for each sentence and then aggregating the statistics for the whole dataset." ></td>
	<td class="line x" title="130:228	Our implementation runs on a quad-core shared memory architecture and we find an almost linear speedup going from one to four cores." ></td>
	<td class="line x" title="131:228	4 Evaluation Evaluating unsupervised PoS tagging is rather difficult mainly due to the fact that the output of such 682 systems are not actual PoS tags but state identifiers." ></td>
	<td class="line x" title="132:228	Therefore it is impossible to evaluate performance against a manually annotated gold standard using accuracy." ></td>
	<td class="line oc" title="133:228	Recent work (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) on this task explored a variety of methodologies to address this issue." ></td>
	<td class="line x" title="134:228	The most common approach followed in previous work is to evaluate unsupervised PoS tagging as clustering against a gold standard using the Variation of Information (VI) (Meila, 2007)." ></td>
	<td class="line x" title="135:228	VI assesses homogeneity and completeness using the quantities H(C|K) (the conditional entropy of the class distribution in the gold standard given the clustering) and H(K|C) (the conditional entropy of clustering given the class distribution in the gold standard)." ></td>
	<td class="line x" title="136:228	However, as Gao & Johnson (2008) point out, VI is biased towards clusterings with a small number of clusters." ></td>
	<td class="line x" title="137:228	A different evaluation measure that uses the same quantities but weighs them differently is the V-measure (Rosenberg and Hirschberg, 2007), which is defined in Equation 2 by setting the parameter  to 1." ></td>
	<td class="line x" title="138:228	h = 1H(C|K)H(C) c = 1H(K|C)H(K) V = (1 +)hc(h) +c (2) Vlachos et al.(2009) noted that V-measure favors clusterings with a large number of clusters." ></td>
	<td class="line x" title="140:228	Both of these biases become crucial in our experiments, since the number of clusters (states of the iHMM) is not fixed in advance." ></td>
	<td class="line x" title="141:228	Vlachos et al. proposed a variation of the V-measure, V-beta, that adjusts the balance between homogeneity and completeness using the parameter  in Eq." ></td>
	<td class="line x" title="142:228	2." ></td>
	<td class="line x" title="143:228	It is worth mentioning that, unlike V-measure and V-beta, VI scores are not normalized and therefore they are difficult to interpret." ></td>
	<td class="line x" title="144:228	Meila (2007) presented two normalizations, acknowledging the potential disadvantages they have." ></td>
	<td class="line x" title="145:228	The first one normalizes VI by 2log(max(|K|,|C|)), which is inappropriate when the number of clusters discovered |K| changes between experiments." ></td>
	<td class="line x" title="146:228	The second normalization involves the quantity logN which is appropriate when comparing different algorithms on the same dataset (N is the number of instances)." ></td>
	<td class="line x" title="147:228	However, this quantity depends exclusively on the size of the dataset and hence if the dataset is very large it can result in normalized VI scores misleadingly close to 100%." ></td>
	<td class="line x" title="148:228	This does not affect rankings, i.e. a better VI score will also be translated into a better normalized VI score." ></td>
	<td class="line x" title="149:228	In our experiments, we report results only with the un-normalized VI scores, V-measure and V-beta." ></td>
	<td class="line x" title="150:228	All the evaluation measures mentioned so far evaluate PoS tagging as a clustering task against a manually annotated gold standard." ></td>
	<td class="line x" title="151:228	While this is reasonable, it still does not provide means of assessing the performance in a way that would allow comparisons with supervised methods that output actual PoS tags." ></td>
	<td class="line x" title="152:228	Even for the normalized measures V-measure and V-beta, it is unclear how their values relate to accuracy levels." ></td>
	<td class="line x" title="153:228	Gao & Johnson (2008) partially addressed this issue by mapping states to PoS tags following two different strategies, cross-validation accuracy, and greedy 1-to-1 mapping, which both have shortcomings." ></td>
	<td class="line x" title="154:228	We argue that since an unsupervised PoS tagger is trained without taking any gold standard into account, it is not appropriate to evaluate against a particular gold standard, or at least this should not be the sole criterion." ></td>
	<td class="line oc" title="155:228	The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwater & Griffiths (2007) versus Johnson (2007)) supports this claim." ></td>
	<td class="line x" title="156:228	Furthermore, PoS tagging is seldomly a goal in itself, but it is a component in a linguistic pipeline." ></td>
	<td class="line x" title="157:228	In order to address these issues, we perform an extrinsic evaluation using a well-explored task that involves PoS tags." ></td>
	<td class="line x" title="158:228	While PoS tagging is considered a pre-processing step in many natural language processing pipelines, the choice of task is restricted by the lack of real PoS tags in the output of our system." ></td>
	<td class="line x" title="159:228	For our purposes we need a task that relies on discriminating between PoS tags rather than the PoS tag semantics themselves, in other words, a task in which knowing whether a word is tagged as noun instead of a verb is equivalent to knowing it is tagged as state 1 instead of state 2." ></td>
	<td class="line x" title="160:228	Taking these considerations into account, in Section 5 we experiment with shallow parsing in the context of the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) in which very good performances were achieved using only the words with their PoS tags." ></td>
	<td class="line x" title="161:228	Our intuition is that if the iHMM (or any unsupervised PoS tagging 683 method) has a reasonable level of performance, it should improve on the performance of a system that does not use PoS tags." ></td>
	<td class="line x" title="162:228	Moreover, if the performance is very good indeed, it should get close to the performance of a system that uses real PoS tags, provided either by human annotation or by a good supervised system." ></td>
	<td class="line x" title="163:228	Similar extrinsic evaluation was performed by Biemann et al.(2007)." ></td>
	<td class="line x" title="165:228	It is of interest to compare the results between the clustering evaluation and the extrinsic one." ></td>
	<td class="line x" title="166:228	A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting (Finkel et al., 2007; Liang et al., 2007)." ></td>
	<td class="line x" title="167:228	In this setting, the model is used in order to refine existing annotation of the dataset." ></td>
	<td class="line x" title="168:228	While this approach can provide us with some insights and interpretable results, the use of existing annotation influences the output of the model." ></td>
	<td class="line x" title="169:228	In this work, we want to verify whether the output of the iHMM (without any supervision) can be used instead of that of a supervised system." ></td>
	<td class="line x" title="170:228	5 Experiments In all our experiments, the Wall Street Journal (WSJ) part of the Penn Treebank was used." ></td>
	<td class="line x" title="171:228	As explained in Section 4, we evaluate the output of the iHMM in two ways, as clustering with respect to a gold standard and as direct replacement of the PoS tags in the task of shallow parsing." ></td>
	<td class="line x" title="172:228	In each experiment, we obtain a sample from the iHMM over all the sections of WSJ." ></td>
	<td class="line x" title="173:228	The states for sections 15-18 and 20 of the WSJ (training and testing sets respectvely in the CoNLL shared task) are used for the evaluation based on shallow parsing, while the remaining sections are used for evaluation against the WSJ gold standard PoS tags using clustering evaluation measures." ></td>
	<td class="line x" title="174:228	As described in Section 2 we performed three runs with the iHMM: one run with DP prior and fixed ,, one with PY prior and fixed d,, and one with DP prior but where we learn the hyperparameters,from the data." ></td>
	<td class="line x" title="175:228	Our inference algorithm uses 1000 burn-in iterations after which we collect a sample every 1000 iterations." ></td>
	<td class="line x" title="176:228	Our inference procedure is annealed during the first 1000 burnin and 2400 iterations by powering the likelihood of the output distribution with a number that smoothly increases from 0.4 to 1.0 over the 3400 first iterations." ></td>
	<td class="line x" title="177:228	The numbers of iterations reported in the remainder of the section refer to the iterations after burn-in." ></td>
	<td class="line x" title="178:228	We initialized the sampler by: a) sampling the hyperparameters from the prior where applicable, b) uniformly assign each word one out of 20 iHMM states." ></td>
	<td class="line x" title="179:228	For the DP run with fixed parameters, we chose  = 0.8 to encourage some sparsity in the transition matrix and  = 5.0 to allow for enough hidden states." ></td>
	<td class="line x" title="180:228	For the PY run with fixed parameters, we chose  = 0.8 for similar reasons andd = 0.1 and = 1.0." ></td>
	<td class="line x" title="181:228	We point out that one weakness of MCMC methods is that they are hard to test for convergence." ></td>
	<td class="line x" title="182:228	We chose to run the simulations until they became prohibitively expensive to obtain a new sample." ></td>
	<td class="line x" title="183:228	First, we present results using clustering evaluation measures which appear in the figures of Table 1." ></td>
	<td class="line x" title="184:228	The three runs exhibit different behavior." ></td>
	<td class="line x" title="185:228	The number of states reached by the iHMM with fixed parameters using the DP prior stabilizes close to 50 states, while for the experiment with learnt hyperparameters the number of states grows more rapidly, reaching 194 states after 8,000 iterations." ></td>
	<td class="line x" title="186:228	With the PY prior, the number of states reached grows less rapidly reaching 90 states." ></td>
	<td class="line x" title="187:228	All runs achieve better performances with respect to all the measures used as the number of iterations grows." ></td>
	<td class="line x" title="188:228	An exception is that VI scores tend to increase (lower VI scores are better) when the number of states grows larger than the gold standard." ></td>
	<td class="line x" title="189:228	It is interesting to notice how the measures exhibit different biases, in particular that VI penalizes the larger numbers of states discovered in the DP run with learnt parameters as well as the run with the PY prior, compared to the more lenient scores provided by V-measure and V-beta." ></td>
	<td class="line x" title="190:228	The latter though assigns lower scores to the DP run with learnt parameters because it takes into account that the high homogeneity is achieved using even more states." ></td>
	<td class="line x" title="191:228	Finally, the interpretability of these scores presents some interest." ></td>
	<td class="line x" title="192:228	For example, in the run with fixed parameters using the DP prior, after burn-in VI was 4.6, which corresponds to 76.65% normalized VI score, while V-measure and V-beta were 12.7% and 9% respectively." ></td>
	<td class="line x" title="193:228	In 8,000 iterations after burnin, VI was 3.94 (80.3% when normalized), while V-measure and V-beta were 53.3%, since the number of states was almost the same as the number of unique PoS tags in the gold standard." ></td>
	<td class="line x" title="194:228	The closest experiment to ours is the one by Gao & Johnson (2008) who run their Bayesian HMM over the whole WSJ and evaluated against the full gold standard, the only difference being is that we exclude the CoNLL shared task sec684  0  20  40  60  80  100  120  140  160  180  200  0  1  2  3  4  5  6  7  8 states DP-learnt DP-fixed PY-fixed  0  10  20  30  40  50  60  70  0  1  2  3  4  5  6  7  8 homogeneity DP-learnt DP-fixed PY-fixed  25  30  35  40  45  50  55  60  0  1  2  3  4  5  6  7  8 completeness DP-learnt DP-fixed PY-fixed  3.6  3.8  4  4.2  4.4  4.6  4.8  5  5.2  0  1  2  3  4  5  6  7  8 VI DP-learnt DP-fixed PY-fixed  10  15  20  25  30  35  40  45  50  55  60  0  1  2  3  4  5  6  7  8 V-measure DP-learnt DP-fixed PY-fixed  5  10  15  20  25  30  35  40  45  50  55  60  0  1  2  3  4  5  6  7  8 V-beta DP-learnt DP-fixed PY-fixed Table 1: Performance of the three iHMM runs according to clustering evaluation measures against number of iteretions (in thousands)." ></td>
	<td class="line x" title="195:228	93.2  93.4  93.6  93.8  94  94.2  94.4  94.6  0  1  2  3  4  5  6  7  8 accuracy DP-learnt DP-fixed PY-fixed  88.5  89  89.5  90  90.5  91  0  1  2  3  4  5  6  7  8 F-score DP-learnt DP-fixed PY-fixed Table 2: Performance of the output of the three iHMM runs when used in shallow parsing against number of iteretions (in thousands)." ></td>
	<td class="line x" title="196:228	tions from our evaluation, which leaves us with 19 sections instead of 24." ></td>
	<td class="line x" title="197:228	Their best VI score was 4.03886 which they achieved using the collapsed, sentence-blocked Gibbs sampler with the number of states fixed to 50." ></td>
	<td class="line x" title="198:228	The VI score achieved by the iHMM with fixed parameters using the PY prior reaches 3.73, while using the DP prior VI reaches 4.32 with learnt parameters and 3.93 with fixed 685 parameters." ></td>
	<td class="line x" title="199:228	These results, even if they are not directly comparable, are on par with the state-ofthe-art, which encouraged us to proceed with the extrinsic evaluation." ></td>
	<td class="line x" title="200:228	For the experiments with shallow parsing we used the CRF++ toolkit3 which has an efficient implementation of the model introduced by Sha & Pereira (2003) for this task." ></td>
	<td class="line x" title="201:228	First we ran an experiment using the words and the PoS tags provided in the shared task data and the performances obtained were 96.07% accuracy and 93.81% F-measure." ></td>
	<td class="line x" title="202:228	The PoS tags were produced using the Brill tagger (Brill, 1994) which employs tranformationbased learning and was trained using the WSJ corpus." ></td>
	<td class="line x" title="203:228	Then we ran an experiment removing the PoS tags altogether, and the performances were 93.25% accuracy and 88.58% F-measure respectively." ></td>
	<td class="line x" title="204:228	This gave us some indication as to what the contribution of the PoS tags is in the context of the shallow parsing task at hand." ></td>
	<td class="line x" title="205:228	The experiments using the output of the iHMM as PoS tags for shallow parsing are presented in Table 2." ></td>
	<td class="line x" title="206:228	The best performance achieved was 94.48% and 90.98% in accuracy and F-measure, which is 1.23% and 2.4% better respectively than just using words, but worse by 1.57% and 2.83% compared to using the supervised PoS tagger output." ></td>
	<td class="line x" title="207:228	Given that the latter is trained on WSJ we believe that this is a good result." ></td>
	<td class="line x" title="208:228	Interestingly, this was obtained by using the last sample from the iHMM run using the DP prior with learnt parameters which has worse overall clustering evaluation scores, especially in terms of VI." ></td>
	<td class="line x" title="209:228	This sample though has the best homogeneity score (69.39%)." ></td>
	<td class="line x" title="210:228	We believe that homogeneity is more important than the overall clustering score due to the fact that, in the application considered, it is probably worse to assign tokens that belong to different PoS tags to the same state, e.g. verb and adverbs, rather than generate more than one state for the same PoS." ></td>
	<td class="line x" title="211:228	This is likely to be the case in tasks where we are interested in distinguishing between PoS tags rather than the actual tag itself." ></td>
	<td class="line x" title="212:228	Also, clustering evaluation measures tend to score leniently consistent mixing of members of different classes in the same cluster." ></td>
	<td class="line x" title="213:228	However, such mixing results in consistent noise when the clustering output becomes input to a machine learning method, which is harder to deal with." ></td>
	<td class="line x" title="214:228	3http://crfpp.sourceforge.net/ 6 Conclusions Future Work In the context of shallow parsing we saw that the performance of the iHMM does not match the performance of a supervised PoS tagger but does lead to a performance increase over a model using only words as features." ></td>
	<td class="line x" title="215:228	Given that it was constructed without any need for human annotation, we believe this is a good result." ></td>
	<td class="line x" title="216:228	At the same time though, it suggests that it is still some way from being a direct drop-in replacement for a supervised method." ></td>
	<td class="line x" title="217:228	We argue that the extrinsic evaluation of unsupervised PoS tagging performed in this paper is quite informative as it allowed us to assess our results in a more realistic context." ></td>
	<td class="line x" title="218:228	In this work we used shallow parsing for this, but we are considering other tasks in which we hope that PoS tagging performance will be more crucial." ></td>
	<td class="line x" title="219:228	Our experiments also suggest that the number of states in a Bayesian non-parametric model can be rather unpredictable." ></td>
	<td class="line x" title="220:228	On one hand, this is a strong warning towards inference algorithms which perform finite truncation of non-parametric models." ></td>
	<td class="line x" title="221:228	On the other hand, the remarkable difference in behavior between the DP with fixed and learned priors suggests that more research is needed towards understanding the influence of hyperparameters in Bayesian non-parametric models." ></td>
	<td class="line x" title="222:228	We are currently experimenting with a semisupervised PoS tagger where we let the transition matrix of the iHMM depend on annotated PoS tags." ></td>
	<td class="line x" title="223:228	This model allows us to: a) use annotations whenever they are available and do unsupervised learning otherwise; b) use the power of non-parametric methods to possibly learn more fine grained statistical structure than tag sets created manually." ></td>
	<td class="line x" title="224:228	On the implementation side, it would be interesting to see how our methods scale in a distributed map-reduce architecture where network communication overhead becomes an issue." ></td>
	<td class="line x" title="225:228	Finally, the ultimate goal of our investigation is to do unsupervised PoS tagging using web-scale datasets." ></td>
	<td class="line x" title="226:228	Although the WSJ corpus is reasonably sized, our computational methods do not currently scale to problems with one or two order of magnitude more data." ></td>
	<td class="line x" title="227:228	We will need new breakthroughs to unleash the full potential of unsupervised learning for NLP." ></td>
	<td class="line x" title="228:228	686" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1075
Unsupervised Tokenization for Machine Translation
Chung, Tagyoung;Gildea, Daniel;"></td>
	<td class="line x" title="1:203	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 718726, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:203	c 2009 ACL and AFNLP Unsupervised Tokenization for Machine Translation Tagyoung Chung and Daniel Gildea Computer Science Department University of Rochester Rochester, NY 14627 Abstract Training a statistical machine translation starts with tokenizing a parallel corpus." ></td>
	<td class="line x" title="3:203	SomelanguagessuchasChinesedonotincorporate spacing in their writing system, whichcreatesa challengefortokenization." ></td>
	<td class="line x" title="4:203	Moreover,morphologicallyrichlanguages such as Korean present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear." ></td>
	<td class="line x" title="5:203	Both rule-based solutions and statistical solutions are currently used." ></td>
	<td class="line x" title="6:203	In this paper, we present unsupervised methods to solve tokenization problem." ></td>
	<td class="line x" title="7:203	Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation." ></td>
	<td class="line x" title="8:203	1 Introduction Tokenizing a parallel corpus is usually the first step of training a statistical machine translation system." ></td>
	<td class="line x" title="9:203	With languages such as Chinese, which has no spaces in its writing system, the main challenge is to segment sentences into appropriate tokens." ></td>
	<td class="line x" title="10:203	With languages such as Korean and Hungarian, although the writing systems of both languages incorporate spaces between words, the granularity is too coarse compared with languages such as English." ></td>
	<td class="line x" title="11:203	A single word in these languages is composed of several morphemes, which often correspond to separate words in English." ></td>
	<td class="line x" title="12:203	These languages also form compound nouns more freely." ></td>
	<td class="line x" title="13:203	Ideally, we want to find segmentations for source and target languages that create a one-toone mapping of words." ></td>
	<td class="line x" title="14:203	However, this is not always straightforward for two major reasons." ></td>
	<td class="line x" title="15:203	First, what the optimal tokenization for machine translation should be is not always clear." ></td>
	<td class="line x" title="16:203	Zhang et al.(2008b) and Chang et al.(2008) show that getting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems." ></td>
	<td class="line x" title="19:203	Second, even statistical methods requirehand-annotatedtrainingdata,whichmeans thatinresource-poorlanguages,goodtokenization is hard to achieve." ></td>
	<td class="line x" title="20:203	In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation." ></td>
	<td class="line x" title="21:203	We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language." ></td>
	<td class="line x" title="22:203	Unsupervisedmonolingualsegmentationhasbeenstudied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001)." ></td>
	<td class="line x" title="23:203	Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs(KikuiandYamamoto,2002),andforfinding good segmentation for Chinese in machine translation using Gibbs sampling (Xu et al., 2008)." ></td>
	<td class="line x" title="24:203	In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation." ></td>
	<td class="line x" title="25:203	We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization problem of normalizing between languages that include more or less information in a word as defined by the writing system, using Korean-English for our experiments." ></td>
	<td class="line x" title="26:203	Here too, we find a benefit from using bilingual information, with unsupervised segmentation rivaling and in some cases surpassing supervised segmentation." ></td>
	<td class="line x" title="27:203	On the modeling side, we use dynamic programming-based variational Bayes, making Gibbs sampling unnecessary." ></td>
	<td class="line x" title="28:203	We also develop and compare various factors in the model to control the length of the tokens learned, and find a benefit from adjusting these parameters directly to optimize the end-to-end translation quality." ></td>
	<td class="line x" title="29:203	718 2 Tokenization Tokenization is breaking down text into lexemes  a unit of morphological analysis." ></td>
	<td class="line x" title="30:203	For relatively isolatinglanguagessuchasEnglishandChinese, a wordgenerallyequalsasingletoken,whichisusually a clearly identifiable unit." ></td>
	<td class="line x" title="31:203	English, especially, incorporates spaces between words in its writing system, which makes tokenization in English usually trivial." ></td>
	<td class="line x" title="32:203	The Chinese writing system does not have spaces between words, but there is less ambiguity where word boundaries lie in a given sentence compared to more agglutinative languages." ></td>
	<td class="line x" title="33:203	In languages such as Hungarian, Japanese, and Korean, what constitutes an optimal token boundary is more ambiguous." ></td>
	<td class="line x" title="34:203	While two tokens are usuallyconsideredtwoseparatewordsinEnglish,this may be not be the case in agglutinative languages." ></td>
	<td class="line x" title="35:203	Although what is considered a single morphological unit is different from language to language, if someone were given a task to align words between two languages, it is desirable to have oneto-one token mapping between two languages in order to have the optimal problem space." ></td>
	<td class="line x" title="36:203	For machine translation,one token should not necessarily correspond to one morphological unit, but rather should reflect the morphological units and writing system of the other language involved in translation." ></td>
	<td class="line x" title="37:203	For example, consider a Korean word meokeoss-da, which means ate." ></td>
	<td class="line x" title="38:203	It is written as a single word in Korean but consists of three morphemes eat-past-indicative." ></td>
	<td class="line x" title="39:203	If one uses morphological analysis as the basis for Korean tokenization, meok-eoss-da would be split into three tokens, which is not desirable if we are translating Korean to English, since English does not have these morphological counterparts." ></td>
	<td class="line x" title="40:203	However, aHungarianwordszekrenyemben,whichmeansin my closet, consists of three morphemes closet-myinessive that are distinct words in English." ></td>
	<td class="line x" title="41:203	In this case, we do want our tokenizer to split this word into three morphemes szekreny em ben." ></td>
	<td class="line x" title="42:203	In this paper, we use segmentation and tokenization interchangeably as blanket terms to cover the two different problems we have presented here." ></td>
	<td class="line x" title="43:203	The problem of segmenting Chinese sentences into words and the problem of segmenting Korean or Hungarian words into tokens of rightgranularityaredifferentintheirnature." ></td>
	<td class="line x" title="44:203	However, our models presented in section 3 handle the both problems." ></td>
	<td class="line x" title="45:203	3 Models We present two different methods for unsupervised tokenization." ></td>
	<td class="line x" title="46:203	Both are essentially unigram tokenization models." ></td>
	<td class="line x" title="47:203	In the first method, we try learning tokenization from word alignments with a model that bears resemblance to Hidden Markov models." ></td>
	<td class="line x" title="48:203	WeuseIBMModel1(Brownetal., 1993) for the word alignment model." ></td>
	<td class="line x" title="49:203	The second model is a relatively simpler monolingual tokenization model based on counts of substrings which serves as a baseline of unsupervised tokenization." ></td>
	<td class="line x" title="50:203	3.1 Learning tokenization from alignment We use expectation maximization as our primary tools in learning tokenization form parallel text." ></td>
	<td class="line x" title="51:203	Here, the observed data provided to the algorithm are the tokenized English string en1 and the untokenized string of foreign characters cm1 . The unobserved variables are both the word-level alignments between the two strings, and the tokenization of the foreign string." ></td>
	<td class="line x" title="52:203	We represent the tokenizationwithastringsm1 ofbinaryvariables,with si = 1 indicating that the ith character is the final character in a word." ></td>
	<td class="line x" title="53:203	The string of foreign words f1 can be thought of as the result of applying the tokenization s to the character string c: f = s c where  = msummationdisplay i=1 si We use IBM Model 1 as our word-level alignment model, following its assumptions that each foreign word is generated independently from one English word: P(f|e) = summationdisplay a P(f,a | e) = summationdisplay a productdisplay i P(fi |eai)P(a) = productdisplay i summationdisplay j P(fi |ej)P(ai = j) and that all word-level alignments a are equally likely: P(a) = 1n for all positions." ></td>
	<td class="line x" title="54:203	While Model 1 has a simple EM update rule to compute posteriors for the alignment variables a and from them learn the lexical translation parameters P(f | e), we cannot apply it directly here because f itself is unknown, and ranges over an exponential number ofpossibilitiesdependingonthehiddensegmentation s. Thiscanbeaddressedbyapplyingdynamic programing over the sequence s. We compute the 719 posterior probability of a word beginning at positioni, endingatpositionj, andbeinggeneratedby English wordk: P(sij = (1,0,,0,1),a= k | e) = (i)P(f |ek)P(a = k)(j)P(c | e) where f = cicj is the word formed by concatenating characters i through j, and a is a variable indicating which English position generated f. Hereand are defined as: (i) = P(ci1,si = 1 | e) (j) = P(cmj+1,sj = 1 | e) These quantities resemble forward and backward probabilities of hidden Markov models, and can be computed with similar dynamic programming recursions: (i) = Lsummationdisplay =1 (i) summationdisplay a P(a)P(cii |ea) (j) = Lsummationdisplay =1 summationdisplay a P(a)P(cj+j |ea)(j +) where L is the maximum character length for a word." ></td>
	<td class="line x" title="55:203	Then, we can calculate the expected counts of individual word pairs being aligned (cji,ek) by accumulating these posteriors over the data: ec(cji,ek) += (i)P(a)P(c j i |ek)(j) (m) The M step simply normalizes the counts: P(f |e) = ec(f,e)summationtext eec(f,e) OurmodelcanbecomparedtoahiddenMarkov model in the following way: a target word generates a source token which spans a zeroth order Markov chain of characters in source sentence, whereatransitionrepresentsasegmentationand a emission represents an alignment." ></td>
	<td class="line x" title="56:203	The model uses HMM-like dynamic programming to do inference." ></td>
	<td class="line x" title="57:203	For the convenience, we refer to this model as the bilingual model in the rest of the paper." ></td>
	<td class="line x" title="58:203	Figure 1 illustrates our first model with an small example." ></td>
	<td class="line x" title="59:203	Under this model we are not learning segmentation directly, but rather we are learning alignments between two sentences." ></td>
	<td class="line x" title="60:203	The c1 c2 c3 c4 f1 f2 e1 e2 Figure 1: The figure shows a source sentence f = f1,f2 = s c1c4 where s = (0,0,1,1) and a target sentence e = e1,e2." ></td>
	<td class="line x" title="61:203	There is a segmentation between c3 and c4; thus c1, c2, c3 form f1 andc3 formsf2." ></td>
	<td class="line x" title="62:203	f1 is generated bye2 andf2 is generated bye1." ></td>
	<td class="line x" title="63:203	segmentation is by-product of learning the alignment." ></td>
	<td class="line x" title="64:203	We can find the optimal segmentation of a new source language sentence using the Viterbi algorithm." ></td>
	<td class="line x" title="65:203	Given two sentences e and f, a = argmax a P(f,a | e) and segmentation s implied by alignment a is theoptimalsegmentationoff foundbythismodel." ></td>
	<td class="line x" title="66:203	3.2 Learning tokenization from substring counts The second tokenization model we propose is much simpler." ></td>
	<td class="line x" title="67:203	More sophisticated unsupervised monolingual tokenization models using hierarchical Bayesian models (Goldwater et al., 2006) and using the minimum description length principle (Goldsmith, 2001; de Marcken, 1996) have been studied." ></td>
	<td class="line x" title="68:203	Our model is meant to serve as a computationally efficient baseline for unsupervised monolingual tokenization." ></td>
	<td class="line x" title="69:203	Given a corpus of only source language of unknown tokenization, we want to find the optimal s given c  s that gives us thehighestP(s | c)." ></td>
	<td class="line x" title="70:203	AccordingtoBayes rule, P(s | c) P(c | s)P(s) Again, we assume that allP(s) are equally likely." ></td>
	<td class="line x" title="71:203	Let f = sc = f1f, wherefi is a word under some possiblesegmentation s. We want tofind the s that maximizesP(f)." ></td>
	<td class="line x" title="72:203	We assume that P(f) = P(f1)P(f) To calculate P(fi), we count every possible 720 substring  every possible segmentation of characters  from the sentences." ></td>
	<td class="line x" title="73:203	We assume that P(fi) = count(fi)summationtext kcount(fk) We can compute these counts by making a single pass through the corpus." ></td>
	<td class="line x" title="74:203	As in the bilingual model, we limit the maximum size of f for practical reasons and to prevent our model from learning unnecessarily long f. With P(f), given a sequence of characters c, we can calculate the most likely segmentation using the Viterbi algorithm." ></td>
	<td class="line x" title="75:203	s = argmax s P(f) Our rationale for this model is that if a span of charactersf = cicj is an independent token, it will occur often enough in different contexts that such a span of characters will have higher probability than other spans of characters that are not meaningful." ></td>
	<td class="line x" title="76:203	For the rest of the paper, this model will be referred to as the monolingual model." ></td>
	<td class="line x" title="77:203	3.3 Tokenizing new data Since the monolingual tokenization only uses information from a monolingual corpus, tokenizing new data is not a problem." ></td>
	<td class="line x" title="78:203	However, with the bilingual model, we are learningP(f |e)." ></td>
	<td class="line x" title="79:203	We are relying on information available from e to get the best tokenization for f. However, the parallel sentences will not be available for new data we want to translate." ></td>
	<td class="line x" title="80:203	Therefore, for the new data, we have to rely only on P(f) to tokenize any new data, which can be obtained by calculating P(f) = summationdisplay e P(f |e)P(e) With P(f) from the bilingual model, we can run theViterbialgorithminthesamemannerasmonolingual tokenization model for monolingual data." ></td>
	<td class="line x" title="81:203	We hypothesize that we can learn valuable information on which token boundaries are preferable in language f when creating a statistical machine translation system that translates from languagef to languagee." ></td>
	<td class="line x" title="82:203	4 Preventing overfitting We introduce two more refinements to our wordalignment induced tokenization model and monolingual tokenization model." ></td>
	<td class="line x" title="83:203	Since we are considering every possible token f that can be guessed from our corpus, the data is very sparse." ></td>
	<td class="line x" title="84:203	For the bilingual model, we are also using the EM algorithm to learn P(f | e), which means there is a danger of the EM algorithm memorizing the trainingdataandtherebyoverfitting." ></td>
	<td class="line x" title="85:203	WeputaDirichlet prior on our multinomial parameter for P(f | e) to control this situation." ></td>
	<td class="line x" title="86:203	For both models, we also want a way to control the distribution of token length after tokenization." ></td>
	<td class="line x" title="87:203	We address this problem by adding a length factor to our models." ></td>
	<td class="line oc" title="88:203	4.1 Variational Bayes Beal (2003) and Johnson (2007) describe variational Bayes for hidden Markov model in detail, which can be directly applied to our bilingual model." ></td>
	<td class="line o" title="89:203	WiththisBayesianextension,theemission probability of our first model can be summarized as follows: e | Dir(), fi |ei = e Multi(e)." ></td>
	<td class="line oc" title="90:203	Johnson (2007) and Zhang et al.(2008a) show having small  helps to control overfitting." ></td>
	<td class="line x" title="92:203	Following this, we set our Dirichlet prior to be as sparse as possible." ></td>
	<td class="line x" title="93:203	It is set at = 106, the number we used as floor of our probability." ></td>
	<td class="line x" title="94:203	For the model incorporating the length factor, which is described in the next section, we do not place a prior on our transition probability, since there are only two possible states, i.e. P(s = 1) andP(s = 0)." ></td>
	<td class="line x" title="95:203	This distribution is not as sparse as the emission probability." ></td>
	<td class="line x" title="96:203	Comparing variational Bayes to the traditional EM algorithm, the E step stays the same but the M step for calculating the emission probability changes as follows: P(f |e) = exp((ec(f,e) +)) exp((summationtexteec(f,e) +s)) whereis the digamma function, andsis the size of the vocabulary from which f is drawn." ></td>
	<td class="line x" title="97:203	Since we do not accurately know s, we set s to be the numberofallpossibletokens." ></td>
	<td class="line x" title="98:203	Ascanbeseenfrom the equation, by settingto a small value, we are discounting the expected count with help of the digamma function." ></td>
	<td class="line x" title="99:203	Thus, having lower  leads to a sparser solution." ></td>
	<td class="line x" title="100:203	4.2 Token length We now add a parameter that can adjust the tokenizers preference for longer or shorter tokens." ></td>
	<td class="line x" title="101:203	721  0  0.1  0.2  0.3  0.4  0.5  0.6  1  2  3  4  5  6 ref P(s)=0.55 lambda=3.16  0  0.1  0.2  0.3  0.4  0.5  0.6  1  2  3  4  5  6 ref P(s)=0.58 lambda=2.13 Figure 2: Distribution of token length for (from left to right) Chinese, and Korean." ></td>
	<td class="line x" title="102:203	ref is the empirical distribution from supervised tokenization." ></td>
	<td class="line x" title="103:203	Two length factors  1 and 2 are also shown." ></td>
	<td class="line x" title="104:203	For 1, the parameter to geometric distributionP(s) is set to the value learned from our bilingual model." ></td>
	<td class="line x" title="105:203	For2, is set using the criterion described in the experiment section." ></td>
	<td class="line x" title="106:203	This parameter is beneficial because we want our distribution of token length after tokenization to resembletherealdistributionoftokenlength." ></td>
	<td class="line x" title="107:203	This parameter is also useful because we also want to incorporate information on the number of tokens intheotherlanguageintheparallelcorpus." ></td>
	<td class="line x" title="108:203	This is based on the assumption that, if tokenization creates a one-to-one mapping, the number of tokens in both languages should be roughly the same." ></td>
	<td class="line x" title="109:203	We canforcethetwolanguagestohaveaboutthesame number of tokens by adjusting this parameter." ></td>
	<td class="line x" title="110:203	The third reason is to further control overfitting." ></td>
	<td class="line x" title="111:203	Our observation is that certain morphemes are very common, such that they will be always observed attachedto other morphemes." ></td>
	<td class="line x" title="112:203	For example, in Korean, a nounattachedwithnominative casemarker is very common." ></td>
	<td class="line x" title="113:203	Our model is likely to learn a noun attached with the morpheme  nominative case marker  rather than noun itself." ></td>
	<td class="line x" title="114:203	This is not desirable when the noun occurs with less common morphemes; in these cases the morpheme will be split off creating inconsistencies." ></td>
	<td class="line x" title="115:203	Wehaveexperimentedwithtwodifferentlength factors, each with one adjustable parameter: 1() = P(s)(1P(s))1 2() = 2 The first, 1, is the geometric distribution, where l is length of a token and P(s) is probability of segmentation between two characters." ></td>
	<td class="line x" title="116:203	The second length factor 2 was acquired through several experiments and was found to work well." ></td>
	<td class="line x" title="117:203	As can been seen from Figure 2, the second factor discounts longer tokens more heavily than the geometric distribution." ></td>
	<td class="line x" title="118:203	We can adjust the value of  andP(s) toincreaseor decreasenumber of tokens after segmentation." ></td>
	<td class="line x" title="119:203	For ourmonolingualmodel, incorporatingthese factors is straightforward." ></td>
	<td class="line x" title="120:203	We assume that P(f) P(f1)(1)P(fn)(n) wherei is the lengthoffi." ></td>
	<td class="line x" title="121:203	Then, we use the same Viterbi algorithm to select the f1fn that maximizes P(f), thereby selecting the optimal s according to our monolingual model with a length factor." ></td>
	<td class="line x" title="122:203	We pick the value of  and P(s) that produces about the same number of tokens in the source side as in the target side, thereby incorporatingsome informationabout the targetlanguage." ></td>
	<td class="line x" title="123:203	For our bilingual model, we modify our model slightly to incorporate 1, creating a hybrid model." ></td>
	<td class="line x" title="124:203	Now, our forward probability of forwardbackward algorithm is: (i) = Lsummationdisplay =1 (il)1() summationdisplay a P(a)P(cii |ea) and the expected count of (cji,ek) is ec(cji,ek) += (i)P(a)P(c j i |ek)(j)1(ji) (m) For 1, we can learn P(s) for the geometric distribution from the model itself:1 P(s) = 1m msummationdisplay i (i)(i) (m) 1The equation is for one sentence, but in practice, we sum over all sentences in the training data to calculate P(s)." ></td>
	<td class="line x" title="125:203	722 We can also fixP(s) instead of learning it through EM." ></td>
	<td class="line x" title="126:203	We incorporate 2 into the bilingual model as follows: after learningP(f) from the bilingual model, we pick the  in the same manner as the monolingual model and run the Viterbi algorithm." ></td>
	<td class="line x" title="127:203	After applying the length factor, what we have is a log-linear model for tokenization, with two feature functions with equal weights: the length factor andP(f) learned from model." ></td>
	<td class="line x" title="128:203	5 Experiments 5.1 Data We tested our tokenization methods on two different language pairs: Chinese-English, and KoreanEnglish." ></td>
	<td class="line x" title="129:203	For Chinese-English, we used FBIS newswire data." ></td>
	<td class="line x" title="130:203	The Korean-English parallel data was collected from news websites and sentencealigned using two different tools described by Moore (2002) and Melamed (1999)." ></td>
	<td class="line x" title="131:203	We used subsetsofeachparallelcorpusconsistingofabout2M words and 60K sentences on the English side." ></td>
	<td class="line x" title="132:203	For our development set and test set, Chinese-English had about 1000 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation." ></td>
	<td class="line x" title="133:203	For Korean-English, 2200 sentence pairs were randomly sampled from the parallel corpus, and held out from the training data." ></td>
	<td class="line x" title="134:203	These were divided in half and used for test set and development set respectively." ></td>
	<td class="line x" title="135:203	For all language pairs, very minimal tokenization  splitting off punctuation  was done on the English side." ></td>
	<td class="line x" title="136:203	5.2 Experimental setup We used Moses (Koehn et al., 2007) to train machine translation systems." ></td>
	<td class="line x" title="137:203	Default parameters were used for all experiments except for the numberofiterationsforGIZA++(OchandNey, 2003)." ></td>
	<td class="line x" title="138:203	GIZA++ was run until the perplexity on development set stopped decreasing." ></td>
	<td class="line x" title="139:203	For practical reasons, the maximum size of a token was set at three for Chinese, andfour forKorean.2 Minimum error rate training (Och, 2003) was run on each system afterwardsand BLEU score (Papineni et al., 2002) was calculated on the test sets." ></td>
	<td class="line x" title="140:203	For the monolingual model, we tested two versions with the length factor1, and2." ></td>
	<td class="line x" title="141:203	We picked andP(s) so that the number of tokens on source side (Chinese, and Korean) will be about the same 2In the Korean writing system, one character is actually one syllable block." ></td>
	<td class="line x" title="142:203	We do not decompose syllable blocks into individual consonants and vowels." ></td>
	<td class="line x" title="143:203	as the number of tokens in the target side (English)." ></td>
	<td class="line x" title="144:203	For the bilingual model, as explained in the model section, we are learningP(f | e), but only P(f) is available for tokenizing any new data." ></td>
	<td class="line x" title="145:203	We compared two conditions: using only the source data to tokenize the source language training data according to P(f) (which is consistent with the conditions at test time), and using both the source and English data to tokenize the source language training data (which might produce better tokenization by using more information)." ></td>
	<td class="line x" title="146:203	For the first length factor 1, we ran an experiment where the model learns P(s) as described in the model section, andwe alsohadexperimentswhereP(s)was pre-setat0.9, 0.7, 0.5, and0.3forcomparison." ></td>
	<td class="line x" title="147:203	We also ran an experiment with the second length factor2 where was picked as the same manner as the monolingual model." ></td>
	<td class="line x" title="148:203	We varied tokenization of development set and test set to match the training data for each experiment." ></td>
	<td class="line x" title="149:203	However, as we have implied in the previous paragraph, in the one experiment where P(f | e) was used to segment training data, directly incorporating information from target corpus, tokenization for test and development set is not exactly consistent with tokenization of training corpus." ></td>
	<td class="line x" title="150:203	Since we assume only source corpus is available at the test time, the test and the development set was tokenized only using information fromP(f)." ></td>
	<td class="line x" title="151:203	We also trained MT systems using supervised tokenizations and tokenization requiring a minimal effortfor the eachlanguagepair." ></td>
	<td class="line x" title="152:203	For ChineseEnglish, the minimal effort tokenization is maximal tokenization where every Chinese character is segmented." ></td>
	<td class="line x" title="153:203	Since a number of Chinese tokenizers are available, we have tried four different tokenizations for the supervised tokenizations." ></td>
	<td class="line x" title="154:203	The first one is the LDC Chinese tokenizer available at the LDC website3, which is compiled by Zhibiao Wu." ></td>
	<td class="line x" title="155:203	The second tokenizer is a maxent-based tokenizer described by Xue (2003)." ></td>
	<td class="line x" title="156:203	The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al.(2008)." ></td>
	<td class="line x" title="158:203	The difference between third and fourth tokenization comes from the different gold standard, the third one is based on Beijing Universitys segmentation (pku) and the fourth one is based on Chinese Treebank (ctb)." ></td>
	<td class="line x" title="159:203	For Korean3http://projects.ldc.upenn.edu/Chinese/LDC ch.htm 723 Chinese Korean BLEU F-score BLEU Supervised Rule-based morphological analyzer 7.27 LDC segmenter 20.03 0.94 Xues segmenter 23.02 0.96 Stanford segmenter (pku) 21.69 0.96 Stanford segmenter (ctb) 22.45 1.00 Unsupervised Splitting punctuation only 6.04 Maximal (Character-based MT) 20.32 0.75 BilingualP(f |e) with1 P(s) = learned 19.25 6.93 BilingualP(f) with1 P(s) = learned 20.04 0.80 7.06 BilingualP(f) with1 P(s) = 0.9 20.75 0.87 7.46 BilingualP(f) with1 P(s) = 0.7 20.59 0.81 7.31 BilingualP(f) with1 P(s) = 0.5 19.68 0.80 7.18 BilingualP(f) with1 P(s) = 0.3 20.02 0.79 7.38 BilingualP(f) with2 22.31 0.88 7.35 MonolingualP(f) with1 20.93 0.83 6.76 MonolingualP(f) with2 20.72 0.85 7.02 Table 1: BLEU score results for Chinese-English and Korean-English experiments and F-score of segmentation compared against Chinese Treebank standard." ></td>
	<td class="line x" title="160:203	The highest unsupervised score is highlighted." ></td>
	<td class="line x" title="161:203	English, the minimal effort tokenization splitting off punctuation and otherwise respectingthe spacing in the Korean writing system." ></td>
	<td class="line x" title="162:203	A Korean morphologicalanalysistool4 wasusedtocreatethesupervised tokenization." ></td>
	<td class="line x" title="163:203	For Chinese-English, since a gold standard for Chinese segmentationis available, we ran an additional evaluation of tokenization from each methods we have tested." ></td>
	<td class="line x" title="164:203	We tokenized the raw text of Chinese Treebank (Xia et al., 2000) using all of themethods(supervised/unsupervised)we have described in this section except for the bilingual tokenization using P(f | e) because the English translation of the Chinese Treebank data was not available." ></td>
	<td class="line x" title="165:203	We compared the result against the gold standard segmentation and calculated the F-score." ></td>
	<td class="line x" title="166:203	6 Results ResultsfromChinese-EnglishandKorean-English experiments are presented in Table 1." ></td>
	<td class="line x" title="167:203	Note that nature of data and number of references are different for the two language pairs, and therefore the BLEU scores are not comparable." ></td>
	<td class="line x" title="168:203	For both language pairs, our models perform equally well as supervised baselines, or even better." ></td>
	<td class="line x" title="169:203	We can 4http://nlp.kookmin.ac.kr/HAM/eng/main-e.html observe three things from the result." ></td>
	<td class="line x" title="170:203	First, tokenization of training data usingP(f |e) tested on a test set tokenized with P(f) performed worse than any other experiments." ></td>
	<td class="line x" title="171:203	This affirms our belief that consistency in tokenization is important formachinetranslation,whichwasalsomentioned by Chang et al.(2008)." ></td>
	<td class="line x" title="173:203	Secondly, we are learning valuable information by looking at the target language." ></td>
	<td class="line x" title="174:203	Compare the result of the bilingual model with 2 as the length factor to the result of the monolingual model with the same length factor." ></td>
	<td class="line x" title="175:203	The bilingual version consistently performed better than the monolingual model in all language pairs." ></td>
	<td class="line x" title="176:203	This tells us we can learn better token boundaries by using information from the target language." ></td>
	<td class="line x" title="177:203	Thirdly, our hypothesis on the need for heavy discount for longer tokens is confirmed." ></td>
	<td class="line x" title="178:203	The valueforP(s)learnedbythe model was 0.55, and0.58forChinese,andKoreanrespectively." ></td>
	<td class="line x" title="179:203	For both language pairs, this accurately reflects the empirical distribution of token length, as can be seen in Figure 2." ></td>
	<td class="line x" title="180:203	However, experiments where P(s) was directly optimized performed better, indicating that this parameter should be optimized within the context of a complete system." ></td>
	<td class="line x" title="181:203	The second length factor 2, which discounts longer tokens even more heavily, generally performed bet724 English the two presidents will hold a joint press conference at the end of their summit talks . Untokenized Korean uni396Auni3476uni3A41uni385Cuni39F9uni3CE7uni35BAuni3A0Buni351Auni3526uni360Funi3457uni35F4uni349Funi3A19uni3CE7uni343Euni39FAuni340Duni344Cuni3CE7uni35BAuni3440uni3459uni36F5uni3457uni38E9uni378Cuni3C76uni3CA2uni35B0." ></td>
	<td class="line x" title="182:203	Supervised uni396Auni3476uni3A41uni385C uni39F9uni3CE7uni35BA uni3A0Buni351Auni3523 uni3134uni360Funi3457uni35F4uni349Funi3A19uni3CE7uni343E uni39FAuni340D uni344Cuni3CE7uni35BAuni3440uni3459 uni36F5uni3457uni38E9uni378Cuni3C76 uni3CA0 uni3134uni35B0." ></td>
	<td class="line x" title="183:203	Bilingual P(f | e) with 1 uni396Auni3476uni3A41uni385Cuni39F9uni3CE7uni35BA uni3A0Buni351Auni3526uni360Funi3457uni35F4uni349Funi3A19uni3CE7uni343E uni39FAuni340Duni344Cuni3CE7uni35BAuni3440uni3459 uni36F5uni3457uni38E9uni378Cuni3C76uni3CA2 uni35B0." ></td>
	<td class="line x" title="184:203	Bilingual P(f) with 2 uni396Auni3476uni3A41uni385C uni39F9uni3CE7uni35BA uni3A0Buni351Auni3526uni360Funi3457uni35F4uni349Funi3A19uni3CE7uni343E uni39FAuni340Duni344Cuni3CE7uni35BAuni3440uni3459 uni36F5uni3457uni38E9uni378Cuni3C76 uni3CA2uni35B0." ></td>
	<td class="line x" title="185:203	Monolingual P(f) with 1 uni396Auni3476uni3A41 uni385C uni39F9uni3CE7uni35BA uni3A0Buni351Auni3526uni360Funi3457uni35F4uni349Funi3A19uni3CE7uni343E uni39FAuni340Duni344Cuni3CE7uni35BAuni3440uni3459uni36F5uni3457uni38E9uni378Cuni3C76uni3CA2 uni35B0." ></td>
	<td class="line x" title="186:203	Monolingual P(f) with 2 uni396Auni3476uni3A41uni385C uni39F9uni3CE7uni35BA uni3A0Buni351Auni3526uni360Funi3457uni35F4uni349Funi3A19uni3CE7uni343E uni39FAuni340Duni344Cuni3CE7uni35BAuni3440uni3459uni36F5uni3457uni38E9uni378Cuni3C76 uni3CA2uni35B0." ></td>
	<td class="line x" title="187:203	Figure 3: Sample tokenization results for Korean-English data." ></td>
	<td class="line x" title="188:203	The underscores are added to clearly visualize where the breaks are." ></td>
	<td class="line x" title="189:203	ter than the first length factor when used in conjunctionwiththe bilingualmodel." ></td>
	<td class="line x" title="190:203	Lastly, F-scores of Chinese segmentations compared against the goldstandardshowshighersegmentationaccuracy does not necessarily lead to higher BLEU score." ></td>
	<td class="line x" title="191:203	F-scorespresentedinTable 1are not directlycomparable for all different experiments because the test data (Chinese Treebank) is used in trainingfor someofthesupervisedsegmenters,butthesenumbers do show how close unsupervised segmentations are to the gold standard." ></td>
	<td class="line x" title="192:203	It is interesting to note that our highest unsupervised segmentation result does make use of bilingual information." ></td>
	<td class="line x" title="193:203	Sample tokenization results for Korean-English experimentsarepresentedinFigure3." ></td>
	<td class="line x" title="194:203	Weobserve that different configurations produce different tokenizations, and the bilingual model produced generally better tokenizations for translation compared to the monolingual models or the supervised tokenizer." ></td>
	<td class="line x" title="195:203	In this example, the tokenization obtained from the supervised tokenizer, although morphologicallycorrect,istoofine-grainedforthe purpose of translation to English." ></td>
	<td class="line x" title="196:203	For example, it correctly tokenized the attributive suffix uni3134 -n however, this is not desirable since English has no such counterpart." ></td>
	<td class="line x" title="197:203	Both variations of the monolingual tokenization have errors such as incorrectly not segmenting uni3440uni3459uni36F5 gyeol-gwa-reul, which is a compound of a noun and a case marker, intouni3440 uni3459 uni36F5gyeol-gwa reul as the bilingual model was able to do." ></td>
	<td class="line x" title="198:203	6.1 Conclusion and future work We have shown that unsupervised tokenization for machine translationis feasibleandcan outperform rule-based methods that rely on lexical analysis, or supervised statistical segmentations." ></td>
	<td class="line x" title="199:203	The approach can be applied both to morphological analysis of Korean and the segmentation of sentences into words for Chinese, which may at first glace appear to be quite different problems." ></td>
	<td class="line x" title="200:203	We have only shown how our methods can be applied to one language of the pair, where one language is generally isolating and the other is generally synthetic." ></td>
	<td class="line x" title="201:203	However, our methods could be extended to tokenization for both languages by iterating between languages." ></td>
	<td class="line x" title="202:203	We also used the most simple word-alignment model, but more complex word alignment models could be incorporated into our bilingual model." ></td>
	<td class="line x" title="203:203	Acknowledgments This work was supportedby NSF grants IIS-0546554 and ITR-0428020." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1042
Weakly Supervised Part-of-Speech Tagging for Morphologically-Rich, Resource-Scarce Languages
Hasan, Kazi Saidul;Ng, Vincent;"></td>
	<td class="line x" title="1:182	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 363371, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:182	c2009 Association for Computational Linguistics Weakly Supervised Part-of-Speech Tagging for Morphologically-Rich, Resource-Scarce Languages Kazi Saidul Hasan and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {saidul,vince}@hlt.utdallas.edu Abstract This paper examines unsupervised approaches to part-of-speech (POS) tagging for morphologically-rich, resource-scarce languages, with an emphasis on Goldwater and Griffithss (2007) fully-Bayesian approach originally developed for English POS tagging." ></td>
	<td class="line x" title="3:182	We argue that existing unsupervised POS taggers unrealistically assume as input a perfect POS lexicon, and consequently, we propose a weakly supervised fully-Bayesian approach to POS tagging, which relaxes the unrealistic assumption by automatically acquiring the lexicon from a small amount of POS-tagged data." ></td>
	<td class="line x" title="4:182	Since such relaxation comes at the expense of a drop in tagging accuracy, we propose two extensions to the Bayesian framework and demonstrate that they are effective in improving a fully-Bayesian POS tagger for Bengali, our representative morphologicallyrich, resource-scarce language." ></td>
	<td class="line x" title="5:182	1 Introduction Unsupervised POS tagging requires neither manual encoding of tagging heuristics nor the availability of data labeled with POS information." ></td>
	<td class="line x" title="6:182	Rather, an unsupervised POS tagger operates by only assuming as input a POS lexicon, which consists of a list of possible POS tags for each word." ></td>
	<td class="line x" title="7:182	As we can see from the partial POS lexicon for English in Figure 1, the is unambiguous with respect to POS tagging, since it can only be a determiner (DT), whereas sting is ambiguous, since it can be a common noun (NN), a proper noun (NNP) or a verb (VB)." ></td>
	<td class="line x" title="8:182	In other words, the lexicon imposes constraints on the possible POS tags Word POS tag(s)   running NN, JJ sting NN, NNP, VB the DT   Figure 1: A partial lexicon for English of each word, and such constraints are then used by an unsupervised tagger to label a new sentence." ></td>
	<td class="line x" title="9:182	Conceivably, tagging accuracy decreases with the increase in ambiguity: unambiguous words such as the will always be tagged correctly; on the other hand, unseen words (or words not present in the POS lexicon) are among the most ambiguous words, since they are not constrained at all and therefore can receive any of the POS tags." ></td>
	<td class="line x" title="10:182	Hence, unsupervised POS tagging can present significant challenges to natural language processing researchers, especially when a large fraction of the words are ambiguous." ></td>
	<td class="line x" title="11:182	Nevertheless, the development of unsupervised taggers potentially allows POS tagging technologies to be applied to a substantially larger number of natural languages, most of which are resource-scarce and, in particular, have little or no POS-tagged data." ></td>
	<td class="line x" title="12:182	The most common approach to unsupervised POS tagging to date has been to train a hidden Markov model (HMM) in an unsupervised manner to maximize the likelihood of an unannotated corpus, using a special instance of the expectationmaximization (EM) algorithm (Dempster et al., 1977) known as Baum-Welch (Baum, 1972)." ></td>
	<td class="line x" title="13:182	More recently, a fully-Bayesian approach to unsupervised POS tagging has been developed by Goldwater and Griffiths (2007) [henceforth G&G] as a viable alternative to the traditional maximumlikelihood-based HMM approach." ></td>
	<td class="line x" title="14:182	While unsupervised POS taggers adopting both approaches have 363 demonstrated promising results, it is important to note that they are typically evaluated by assuming the availability of a perfect POS lexicon." ></td>
	<td class="line x" title="15:182	This assumption, however, is fairly unrealistic in practice, as a perfect POS lexicon can only be constructed by having a linguist manually label each word in a language with its possible POS tags.1 In other words, the labor-intensive POS lexicon construction process renders unsupervised POS taggers a lot less unsupervised than they appear." ></td>
	<td class="line x" title="16:182	To make these unsupervised taggers practical, one could attempt to automatically construct a POS lexicon, a task commonly known as POS induction." ></td>
	<td class="line x" title="17:182	However, POS induction is by no means an easy task, and it is not clear how well unsupervised POS taggers work when used in combination with an automatically constructed POS lexicon." ></td>
	<td class="line x" title="18:182	The goals of this paper are three-fold." ></td>
	<td class="line x" title="19:182	First, motivated by the successes of unsupervised approaches to English POS tagging, we aim to investigate whether such approaches, especially G&Gs fully-Bayesian approach, can deliver similar performance for Bengali, our representative resourcescarce language." ></td>
	<td class="line x" title="20:182	Second, to relax the unrealistic assumption of employing a perfect lexicon as in existing unsupervised POS taggers, we propose a weakly supervised fully-Bayesian approach to POS tagging, where we automatically construct a POS lexicon from a small amount of POS-tagged data." ></td>
	<td class="line x" title="21:182	Hence, unlike a perfect POS lexicon, our automatically constructed lexicon is necessarily incomplete, yielding a large number of words that are completely ambiguous." ></td>
	<td class="line x" title="22:182	The high ambiguity rate inherent in our weakly supervised approach substantially complicates the POS tagging process." ></td>
	<td class="line x" title="23:182	Consequently, our third goal of this paper is to propose two potentially performance-enhancing extensions to G&Gs Bayesian POS tagging approach, which exploit morphology and techniques successfully used in supervised POS tagging." ></td>
	<td class="line x" title="24:182	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="25:182	Section 2 presents related work on unsupervised approaches to POS tagging." ></td>
	<td class="line x" title="26:182	Section 3 gives an introduction to G&Gs fully-Bayesian approach to unsupervised POS tagging." ></td>
	<td class="line x" title="27:182	In Section 4, we describe our two extensions to G&Gs approach." ></td>
	<td class="line x" title="28:182	Section 5 presents experimental results on Bengali POS tagging, focusing on evaluating the effective1When evaluating an unsupervised POS tagger, researchers typically construct a pseudo-perfect POS lexicon by collecting the possible POS tags of a word directly from the corpus on which the tagger is to be evaluated." ></td>
	<td class="line x" title="29:182	ness of our two extensions in improving G&Gs approach." ></td>
	<td class="line x" title="30:182	Finally, we conclude in Section 6." ></td>
	<td class="line x" title="31:182	2 Related Work With the notable exception of Synder et al.s (2008; 2009) recent work on unsupervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data." ></td>
	<td class="line x" title="32:182	For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM." ></td>
	<td class="line x" title="33:182	Schutze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters." ></td>
	<td class="line x" title="34:182	Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words." ></td>
	<td class="line x" title="35:182	Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e)." ></td>
	<td class="line x" title="36:182	Wang and Schuurmans (2005) improve an unsupervised HMM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters." ></td>
	<td class="line x" title="37:182	As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible parameter values instead of fixing a set of parameter values for unsupervised sequence learning." ></td>
	<td class="line oc" title="38:182	Importantly, this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories (Johnson, 2007)." ></td>
	<td class="line oc" title="39:182	Similar to Goldwater and Griffiths (2007) and Johnson (2007), Toutanova and Johnson (2007) also use Bayesian inference for POS tagging." ></td>
	<td class="line x" title="40:182	However, their work departs from existing Bayesian approaches to POS tagging in that they (1) introduce a new sparse prior on the distribution over tags for each word, (2) extend the Latent Dirichlet Allocation model, and (3) explicitly model ambiguity class." ></td>
	<td class="line x" title="41:182	While their tagging model, like Goldwater and Griffithss, assumes as input an incomplete POS lexicon and a large unlabeled corpus, they consider their approach semisupervised simply because of the human knowledge involved in constructing the POS lexicon." ></td>
	<td class="line x" title="42:182	364 3 A Fully Bayesian Approach 3.1 Motivation As mentioned in the introduction, the most common approach to unsupervised POS tagging is to train an HMM on an unannotated corpus using the Baum-Welch algorithm so that the likelihood of the corpus is maximized." ></td>
	<td class="line x" title="43:182	To understand what the HMM parameters are, let us revisit how an HMM simultaneously generates an output sequence w = (w0,w1,,wn) and the associated hidden state sequence t = (t0,t1,,tn)." ></td>
	<td class="line x" title="44:182	In the context of POS tagging, each state of the HMM corresponds to a POS tag, the output sequence w is the given word sequence, and the hidden state sequence t is the associated POS tag sequence." ></td>
	<td class="line x" title="45:182	To generate w and t, the HMM begins by guessing a state t0 and then emitting w0 from t0 according to a state-specific output distribution over word tokens." ></td>
	<td class="line x" title="46:182	After that, we move to the next state t1, the choice of which is based on t0s transition distribution, and emit w1 according to t1s output distribution." ></td>
	<td class="line x" title="47:182	This generation process repeats until the end of the word sequence is reached." ></td>
	<td class="line x" title="48:182	In other words, the parameters of an HMM, , are composed of a set of statespecific (1) output distributions (over word tokens) and (2) transition distributions, both of which can be learned using the EM algorithm." ></td>
	<td class="line x" title="49:182	Once learning is complete, we can use the resulting set of parameters to find the most likely hidden state sequence given a word sequence using the Viterbi algorithm." ></td>
	<td class="line oc" title="50:182	Nevertheless, EM sometimes fails to find good parameter values.2 The reason is that EM tries to assign roughly the same number of word tokens to each of the hidden states (Johnson, 2007)." ></td>
	<td class="line x" title="51:182	In practice, however, the distribution of word tokens to POS tags is highly skewed (i.e., some POS categories are more populated with tokens than others)." ></td>
	<td class="line x" title="52:182	This motivates a fully-Bayesian approach, which, rather than committing to a particular set of parameter values as in an EM-based approach, integrates over all possible values of  and, most importantly, allows the use of priors to favor the learning of the skewed distributions, through the use of the term P(|w) in the following equation: P(t|w) = integraldisplay P(t|w,)P(|w)d (1) The question, then, is: which priors on  would allow the acquisition of skewed distributions?" ></td>
	<td class="line x" title="53:182	To 2When given good parameter initializations, however, EM can find good parameter values for an HMM-based POS tagger." ></td>
	<td class="line x" title="54:182	See Goldberg et al.(2008) for details." ></td>
	<td class="line x" title="56:182	answer this question, recall that in POS tagging,  is composed of a set of tag transition distributions and output distributions." ></td>
	<td class="line x" title="57:182	Each such distribution is a multinomial (i.e., each trial produces exactly one of some finite number of possible outcomes)." ></td>
	<td class="line x" title="58:182	For a multinomial with K outcomes, a K-dimensional Dirichlet distribution, which is conjugate to the multinomial, is a natural choice of prior." ></td>
	<td class="line x" title="59:182	For simplicity, we assume that a distribution in  is drawn from a symmetric Dirichlet with a certain hyperparameter (see Teh et al.(2006) for details)." ></td>
	<td class="line x" title="61:182	The value of a hyperparameter, , affects the skewness of the resulting distribution, as it assigns different probabilities to different distributions." ></td>
	<td class="line x" title="62:182	For instance, when  < 1, higher probabilities are assigned to sparse multinomials (i.e., multinomials in which only a few entries are nonzero)." ></td>
	<td class="line x" title="63:182	Intuitively, the tag transition distributions and the output distributions in an HMM-based POS tagger are sparse multinomials." ></td>
	<td class="line x" title="64:182	As a result, it is logical to choose a Dirichlet prior with  < 1." ></td>
	<td class="line x" title="65:182	By integrating over all possible parameter values, the probability that i-th outcome, yi, takes the value k, given the previous i  1 outcomes yi= (y1,y2,,yi1), is P(k|yi,) = integraldisplay P(k|)P(|yi,)d(2) = nk +i1+K (3) where nk is the frequency of k in yi. See MacKay and Peto (1995) for the derivation." ></td>
	<td class="line x" title="66:182	3.2 Model Our baseline POS tagging model is a standard trigram HMM with tag transition distributions and output distributions, each of which is a sparse multinomial that is learned by applying a symmetric Dirichlet prior: ti | ti1,ti2,(ti1,ti2)  Mult((ti1,ti2)) wi | ti,(ti)  Mult((ti)) (ti1,ti2) |   Dirichlet() (ti) |   Dirichlet() where wi and ti denote the i-th word and tag." ></td>
	<td class="line x" title="67:182	With a tagset of size T (including a special tag used as sentence delimiter), each of the tag transition distributions has T components." ></td>
	<td class="line x" title="68:182	For the output symbols, each of the (ti) has Wti components, where Wti denotes the number of word types that can be emitted from the state corresponding to ti." ></td>
	<td class="line x" title="69:182	365 From the closed form in Equation 3, given previous outcomes, we can compute the tag transition and output probabilities of the model as follows: P(ti|ti,) = n(ti2,ti1,ti) +n (ti2,ti1) +T (4) P(wi|ti,ti,wi,) = n(ti,wi) +n ti +Wti (5) where n(ti2,ti1,ti) and n(ti,wi) are the frequencies of observing the tag trigram (ti2,ti1,ti) and the tag-word pair (ti,wi), respectively." ></td>
	<td class="line x" title="70:182	These counts are taken from the i  1 tags and words generated previously." ></td>
	<td class="line x" title="71:182	The inference procedure described next exploits the property that trigrams (and outputs) are exchangeable; that is, the probability of a set of trigrams (and outputs) does not depend on the order in which it was generated." ></td>
	<td class="line x" title="72:182	3.3 Inference Procedure We perform inference using Gibbs sampling (Geman and Geman, 1984), using the following posterior distribution to generate samples: P(t|w,,)  P(w|t,)P(t|) Starting with a random assignment of a POS tag to each word (subject to the constraints in the POS lexicon), we resample each POS tag, ti, according to the conditional distribution shown in Figure 2." ></td>
	<td class="line x" title="73:182	Note that the current counts of other trigrams and outputs can be used as previous observations due to the property of exchangeability." ></td>
	<td class="line x" title="74:182	Following G&G, we use simulated annealing to find the MAP tag sequence." ></td>
	<td class="line x" title="75:182	The temperature decreases by a factor of exp(log( 2 1 ) N1 ) after each iteration, where 1 is the initial temperature and 2 is the temperature after N sampling iterations." ></td>
	<td class="line x" title="76:182	4 Two Extensions In this section, we present two extensions to G&Gs fully-Bayesian framework to unsupervised POS tagging, namely, induced suffix emission and discriminative prediction." ></td>
	<td class="line x" title="77:182	4.1 Induced Suffix Emission For morphologically-rich languages like Bengali, a lot of grammatical information (e.g., POS) is expressed via suffixes." ></td>
	<td class="line x" title="78:182	In fact, several approaches to unsupervised POS induction for morphologicallyrich languages have exploited the observation that some suffixes can only be associated with a small number of POS tags (e.g., Clark (2003), Dasgupta and Ng (2007))." ></td>
	<td class="line x" title="79:182	To exploit suffixes in HMMbased POS tagging, one can (1) convert the wordbased POS lexicon to a suffix-based POS lexicon, which lists the possible POS tags for each suffix; and then (2) have the HMM emit suffixes rather than words, subject to the constraints in the suffixbased POS lexicon." ></td>
	<td class="line x" title="80:182	Such a suffix-based HMM, however, may suffer from over-generalization." ></td>
	<td class="line x" title="81:182	To prevent over-generalization and at the same time exploit suffixes, we propose as our first extension to G&Gs framework a hybrid approach to word/suffix emission: a word is emitted if it is present in the word-based POS lexicon; otherwise, its suffix is emitted." ></td>
	<td class="line x" title="82:182	In other words, our approach imposes suffix-based constraints on the tagging of words that are unseen w.r.t. the word-based POS lexicon." ></td>
	<td class="line x" title="83:182	Below we show how to induce the suffix of a word and create the suffix-based POS lexicon." ></td>
	<td class="line x" title="84:182	Inducing suffixes To induce suffixes, we rely on Keshava and Pitlers (2006) method." ></td>
	<td class="line x" title="85:182	Assume that (1) V is a vocabulary (i.e., a set of distinct words) extracted from a large, unannotated corpus, (2) C1 and C2 are two character sequences, and (3) C1C2 is the concatenation of C1 and C2." ></td>
	<td class="line x" title="86:182	If C1C2 and C1 are found in V , we extract C2 as a suffix." ></td>
	<td class="line x" title="87:182	However, this unsupervised suffix induction method is arguably overly simplistic and hence many of the induced affixes could be spurious." ></td>
	<td class="line x" title="88:182	To identify suffixes that are likely to be correct, we employ a simple procedure: we (1) score each suffix by multiplying its frequency (i.e., the number of distinct words in V to which each suffix attaches) and its length3, and (2) select only those whose score is above a certain threshold." ></td>
	<td class="line x" title="89:182	In our experiments, we set this threshold to 50, and generate our vocabulary from five years of articles taken from the Bengali newspaper Prothom Alo." ></td>
	<td class="line x" title="90:182	This enables us to induce 975 suffixes." ></td>
	<td class="line x" title="91:182	Constructing a suffix-based POS lexicon Next, we construct a suffix-based POS lexicon." ></td>
	<td class="line x" title="92:182	For each word w in the original word-based POS lexicon, we (1) use the induced suffix list obtained in the previous step to identify the longest-matching suffix of w, and then (2) assign all the POS tags associated with w to this suffix." ></td>
	<td class="line x" title="93:182	Incorporating suffix-based output distributions Finally, we extend our trigram model by introduc3The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001))." ></td>
	<td class="line x" title="94:182	366 P(ti|ti,w,,)  n(ti,wi) +n ti +Wti .n(ti2,ti1,ti) +n (ti2,ti1) +T .n(ti1,ti,ti+1) +I(ti2 = ti1 = ti = ti+1) +n (ti1,ti) +I(ti2 = ti1 = ti)+T .n(ti,ti+1,ti+2) +I(ti2 = ti = ti+2,ti1 = ti+1) +I(ti1 = ti = ti+1 = ti+2) +n (ti,ti+1) +I(ti2 = ti,ti1 = ti+1) +I(ti1 = ti = ti+1)+T Figure 2: The sampling distribution for ti (taken directly from Goldwater and Griffiths (2007))." ></td>
	<td class="line x" title="95:182	All nx values are computed from the current values of all tags except for ti." ></td>
	<td class="line x" title="96:182	Here, I(arg) is a function that returns 1 if arg is true and 0 otherwise, and ti refers to the current values of all tags except for ti." ></td>
	<td class="line x" title="97:182	ing a state-specific probability distribution over induced suffixes." ></td>
	<td class="line x" title="98:182	Specifically, if the current word is present in the word-based POS lexicon, or if we cannot find any suffix for the word using the induced suffix list, then we emit the word." ></td>
	<td class="line x" title="99:182	Otherwise, we emit its suffix according to a suffix-based output distribution, which is drawn from a symmetric Dirichlet with hyperparameter : si | ti,(ti)  Mult((ti)) (ti) |   Dirichlet() where si denotes the induced suffix of the i-th word." ></td>
	<td class="line x" title="100:182	The distribution, (ti), has Sti components, where Sti denotes the number of induced suffixes that can be emitted from the state corresponding to ti." ></td>
	<td class="line x" title="101:182	We compute the induced suffix emission probabilities of the model as follows: P(si|ti,ti,si,) = n(ti,si) +n ti +Sti (6) where n(ti,si) is the frequency of observing the tag-suffix pair (ti,si)." ></td>
	<td class="line x" title="102:182	This extension requires that we slightly modify the inference procedure." ></td>
	<td class="line x" title="103:182	Specifically, if the current word is unseen (w.r.t. the word-based POS lexicon) and has a suffix (according to the induced suffix list), then we sample from a distribution that is almost identical to the one shown in Figure 2, except that we replace the first fraction (i.e., the fraction involving the emission counts) with the one shown in Equation (6)." ></td>
	<td class="line x" title="104:182	Otherwise, we simply sample from the distribution in Figure 2." ></td>
	<td class="line x" title="105:182	4.2 Discriminative Prediction As mentioned in the introduction, the (wordbased) POS lexicons used in existing approaches to unsupervised POS tagging were created somewhat unrealistically by collecting the possible POS tags of a word directly from the corpus on which the tagger is to be evaluated." ></td>
	<td class="line x" title="106:182	To make the lexicon formation process more realistic, we propose a weakly supervised approach to Bayesian POS tagging, in which we automatically create the word-based POS lexicon from a small set of POStagged sentences that is disjoint from the test data." ></td>
	<td class="line x" title="107:182	Adopting a weakly supervised approach has an additional advantage: the presence of POS-tagged sentences makes it possible to exploit techniques developed for supervised POS tagging, which is the idea behind discriminative prediction, our second extension to G&Gs framework." ></td>
	<td class="line x" title="108:182	Given a small set of POS-tagged sentences L, discriminative prediction uses the statistics collected from L to predict the POS of a word in a discriminative fashion whenever possible." ></td>
	<td class="line x" title="109:182	More specifically, discriminative prediction relies on two simple ideas typically exploited by supervised POS tagging algorithms: (1) if the target word (i.e., the word whose POS tag is to be predicted) appears in L, we can label the word with its POS tag in L; and (2) if the target word does not appear in L but its context does, we can use its context to predict its POS tag." ></td>
	<td class="line x" title="110:182	In bigram and trigram POS taggers, the context of a word is represented using the preceding one or two words." ></td>
	<td class="line x" title="111:182	Nevertheless, since L is typically small in a weakly supervised setting, it is common for a target word not to satisfy any of the two conditions above." ></td>
	<td class="line x" title="112:182	Hence, if it is not possible to predict a target word in a discriminative fashion (due to the limited size of L), we resort to the sampling equation in Figure 2." ></td>
	<td class="line x" title="113:182	To incorporate the above discriminative decision steps into G&Gs fully-Bayesian framework for POS tagging, the algorithm estimates three types of probability distributions from L. First, to capture context, it computes (1) a distribution over the POS tags following a word bigram, (wi2,wi1), that appears in L [henceforth D1(wi2,wi1)] and (2) a distribution over the POS tags following a word unigram, wi1, that appears in L [henceforth D2(wi1)]." ></td>
	<td class="line x" title="114:182	Then, to cap367 Algorithm 1 Algorithm for incorporating discriminative prediction Input: wi: current word wi1: previous word wi2: second previous word L: a set of POS-tagged sentences Output: Predicted tag, ti 1: if wi L then 2: ti Tag drawn from the distribution of wis candidate tags 3: else if (wi2,wi1)L then 4: ti Tag drawn from the distribution of the POS tags following the word bigram (wi2,wi1) 5: else if wi1L then 6: ti Tag drawn from the distribution of the POS tags following the word unigram wi1 7: else 8: ti Tag obtained using the sampling equation 9: end if ture the fact that a word can have more than one POS tag, it also estimates a distribution over POS tags for each word wi that appears in L [henceforth D3(wi)]." ></td>
	<td class="line x" title="115:182	Implemented as a set of if-else clauses, the algorithm uses these three types of distributions to tag a target word, wi, in a discriminative manner." ></td>
	<td class="line x" title="116:182	First, it checks whether wi appears in L (line 1)." ></td>
	<td class="line x" title="117:182	If so, it tags wi according to D3(wi)." ></td>
	<td class="line x" title="118:182	Otherwise, it attempts to label wi based on its context." ></td>
	<td class="line x" title="119:182	Specifically, if (wi2,wi1), the word bigram preceding wi, appears in L (line 3), then wi is tagged according to D1(wi2,wi1)." ></td>
	<td class="line x" title="120:182	Otherwise, it backs off to a unigram distribution: if wi1, the word preceding wi, appears in L (line 5), then wi is tagged according to D2(wi1)." ></td>
	<td class="line x" title="121:182	Finally, if it is not possible to tag the word discriminatively (i.e., if all the above cases fail), it resorts to the sampling equation (lines 78)." ></td>
	<td class="line x" title="122:182	We apply simulated annealing to all four cases in this iterative tagging procedure." ></td>
	<td class="line x" title="123:182	5 Evaluation 5.1 Experimental Setup Corpus Our evaluation corpus is the one used in the shared task of the IJCNLP-08 Workshop on NER for South and South East Asian Languages.4 Specifically, we use the portion of the Bengali dataset that is manually POS-tagged." ></td>
	<td class="line x" title="124:182	IIIT Hyderabads POS tagset5, which consists of 26 tags specifically developed for Indian languages, has been used to annotate the data." ></td>
	<td class="line x" title="125:182	The corpus is composed of a training set and a test set with approxi4The corpus is available from http://ltrc.iiit.ac.in/ner-ssea08/index.cgi?topic=5." ></td>
	<td class="line x" title="126:182	5http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf mately 50K and 30K tokens, respectively." ></td>
	<td class="line x" title="127:182	Importantly, all our POS tagging results will be reported using only the test set; the training set will be used for lexicon construction, as we will see shortly." ></td>
	<td class="line x" title="128:182	Tagset We collapse the set of 26 POS tags into 15 tags." ></td>
	<td class="line x" title="129:182	Specifically, while we retain the tags corresponding to the major POS categories, we merge some of the infrequent tags designed to capture Indian language specific structure (e.g., reduplication, echo words) into a category called OTHERS." ></td>
	<td class="line x" title="130:182	Hyperparameter settings Recall that our tagger consists of three types of distributions  tag transition distributions, word-based output distributions, and suffix-based output distributions  drawn from a symmetric Dirichlet with , , and  as the underlying hyperparameters, respectively." ></td>
	<td class="line x" title="131:182	We automatically determine the values of these hyperparameters by (1) randomly initializing them and (2) resampling their values by using a Metropolis-Hastings update (Gilks et al., 1996) at the end of each sampling iteration." ></td>
	<td class="line x" title="132:182	Details of this update process can be found in G&G. Inference Inference is performed by running a Gibbs sampler for 5000 iterations." ></td>
	<td class="line x" title="133:182	The initial temperature is set to 2.0, which is gradually lowered to 0.08 over the iterations." ></td>
	<td class="line x" title="134:182	Owing to the randomness involved in hyperparameter initialization, all reported results are averaged over three runs." ></td>
	<td class="line x" title="135:182	Lexicon construction methods To better understand the role of a POS lexicon in tagging performance, we evaluate each POS tagging model by employing lexicons constructed by three methods." ></td>
	<td class="line x" title="136:182	The first lexicon construction method, arguably the most unrealistic among the three, follows that of G&G: for each word, w, in the test set, we (1) collect from each occurrence of w in the training set and the test set its POS tag, and then (2) insert w and all the POS tags collected for w into the POS lexicon." ></td>
	<td class="line x" title="137:182	This method is unrealistic because (1) in practice, a human needs to list all possible POS tags for each word in order to construct this lexicon, thus rendering the resulting tagger considerably less unsupervised than it appears; and (2) constructing the lexicon using the dataset on which the tagger is to be evaluated implies that there is no unseen word w.r.t. the lexicon, thus unrealistically simplifies the POS tagging task." ></td>
	<td class="line x" title="138:182	To make the method more realistic, G&G also create a set of relaxed lexicons." ></td>
	<td class="line x" title="139:182	Each of these lexicons includes the tags for only the words that appear at least d times in the test corpus, where d ranges 368 1 2 3 4 5 6 7 8 9 1030 40 50 60 70 80 90 d Accuracy (%) (a) Lexicon 1   MLHMM BHMM BHMM+IS 1 2 3 4 5 6 7 8 9 1030 35 40 45 50 55 60 65 70 75 d Accuracy (%) (b) Lexicon 2   MLHMM BHMM BHMM+IS Figure 3: Accuracies of POS tagging models using (a) Lexicon 1 and (b) Lexicon 2 from 1 to 10 in our experiments." ></td>
	<td class="line x" title="140:182	Any unseen (i.e., out-of-dictionary) word is ambiguous among the 15 possible tags." ></td>
	<td class="line x" title="141:182	Not surprisingly, both ambiguity and the unseen word rate increase with d. For instance, the ambiguous token rate increases from 40.0% with 1.7 tags/token (d=1) to 77.7% with 8.1 tags/token (d=10)." ></td>
	<td class="line x" title="142:182	Similarly, the unseen word rate increases from 16% (d=2) to 46% (d=10)." ></td>
	<td class="line x" title="143:182	We will refer to this set of tag dictionaries as Lexicon 1." ></td>
	<td class="line x" title="144:182	The second method generates a set of relaxed lexicons, Lexicon 2, in essentially the same way as the first method, except that these lexicons include only the words that appear at least d times in the training data." ></td>
	<td class="line x" title="145:182	Importantly, the words that appear solely in the test data are not included in any of these relaxed POS lexicons." ></td>
	<td class="line x" title="146:182	This makes Lexicon 2 a bit more realistic than Lexicon 1 in terms of the way they are constructed." ></td>
	<td class="line x" title="147:182	As a result, in comparison to Lexicon 1, Lexicon 2 has a considerably higher ambiguous token rate and unseen word rate: its ambiguous token rate ranges from 64.3% with 5.3 tags/token (d=1) to 80.5% with 8.6 tags/token (d=10), and its unseen word rate ranges from 25% (d=1) to 50% (d=10)." ></td>
	<td class="line x" title="148:182	The third method, arguably the most realistic among the three, is motivated by our proposed weakly supervised approach." ></td>
	<td class="line x" title="149:182	In this method, we (1) form ten different datasets from the (labeled) training data of sizes 5K words, 10K words, , 50K words, and then (2) create one POS lexicon from each dataset L by listing, for each word w in L, all the tags associated with w in L. This set of tag dictionaries, which we will refer to as Lexicon 3, has an ambiguous token rate that ranges from 57.7% with 5.1 tags/token (50K) to 61.5% with 8.1 tags/token (5K), and an unseen word rate that ranges from 25% (50K) to 50% (5K)." ></td>
	<td class="line x" title="150:182	5.2 Results and Discussion 5.2.1 Baseline Systems We use as our first baseline system G&Gs Bayesian POS tagging model, as our goal is to evaluate the effectiveness of our two extensions in improving their model." ></td>
	<td class="line x" title="151:182	To further gauge the performance of G&Gs model, we employ another baseline commonly used in POS tagging experiments, which is an unsupervised trigram HMM trained by running EM to convergence." ></td>
	<td class="line x" title="152:182	As mentioned previously, we evaluate each tagging model by employing the three POS lexicons described in the previous subsection." ></td>
	<td class="line x" title="153:182	Figure 3(a) shows how the tagging accuracy varies with d when Lexicon 1 is used." ></td>
	<td class="line x" title="154:182	Perhaps not surprisingly, the trigram HMM (MLHMM) and G&Gs Bayesian model (BHMM) achieve almost identical accuracies when d=1 (i.e., the complete lexicon with a zero unseen word rate)." ></td>
	<td class="line x" title="155:182	As d increases, both ambiguity and the unseen word rate increase; as a result, the tagging accuracy decreases." ></td>
	<td class="line x" title="156:182	Also, consistent with G&Gs results, BHMM outperforms MLHMM by a large margin (47%)." ></td>
	<td class="line x" title="157:182	Similar performance trends can be observed when Lexicon 2 is used (see Figure 3(b))." ></td>
	<td class="line x" title="158:182	However, both baselines achieve comparatively lower tagging accuracies, as a result of the higher unseen word rate associated with Lexicon 2." ></td>
	<td class="line x" title="159:182	369 5 10 15 20 25 30 35 40 45 5045 50 55 60 65 70 75 80 Training data (K) Accuracy (%) Lexicon 3   SHMM BHMM BHMM+IS BHMM+IS+DP Figure 4: Accuracies of the POS tagging models using Lexicon 3 Results using Lexicon 3 are shown in Figure 4." ></td>
	<td class="line x" title="160:182	Owing to the availability of POS-tagged sentences, we replace MLHMM with its supervised counterpart that is trained on the available labeled data, yielding the SHMM baseline." ></td>
	<td class="line x" title="161:182	The accuracies of SHMM range from 48% to 67%, outperforming BHMM as the amount of labeled data increases." ></td>
	<td class="line x" title="162:182	5.2.2 Adding Induced Suffix Emission Next, we augment BHMM with our first extension, induced suffix emission, yielding BHMM+IS. For Lexicon 1, BHMM+IS achieves the same accuracy as the two baselines when d=1." ></td>
	<td class="line x" title="163:182	The reason is simple: as all the test words are in the POS lexicon, the tagger never emits an induced suffix." ></td>
	<td class="line x" title="164:182	More importantly, BHMM+IS beats BHMM and MLHMM by 49% and 1014%, respectively." ></td>
	<td class="line x" title="165:182	Similar trends are observed for Lexicon 2, where BHMM+IS outperforms BHMM and MLHMM by a larger margin of 510% and 1216%, respectively." ></td>
	<td class="line x" title="166:182	For Lexicon 3, BHMM+IS outperforms SHMM, the stronger baseline, by 6 11%." ></td>
	<td class="line x" title="167:182	Overall, these results suggest that induced suffix emission is a strong performance-enhancing extension to G&Gs approach." ></td>
	<td class="line x" title="168:182	5.2.3 Adding Discriminative Prediction Finally, we augment BHMM+IS with discriminative prediction, yielding BHMM+IS+DP." ></td>
	<td class="line x" title="169:182	Since this extension requires labeled data, it can only be applied in combination with Lexicon 3." ></td>
	<td class="line x" title="170:182	As seen in Figure 4, BHMM+IS+DP outperforms SHMM by 1014%." ></td>
	<td class="line x" title="171:182	Its discriminative nature proves to be Predicted Tag Correct Tag % of Error NN NNP 8.4 NN JJ 6.9 VM VAUX 5.9 Table 1: Most frequent POS tagging errors for BHMM+IS+DP on the 50K-word training set strong as it even beats BHMM+IS by 34%." ></td>
	<td class="line x" title="172:182	5.2.4 Error Analysis Table 1 lists the most common types of errors made by the best-performing tagging model, BHMM+IS+DP (50K-word labeled data)." ></td>
	<td class="line x" title="173:182	As we can see, common nouns and proper nouns (row 1) are difficult to distinguish, due in part to the case insensitivity of Bengali." ></td>
	<td class="line x" title="174:182	Also, it is difficult to distinguish Bengali common nouns and adjectives (row 2), as they are distributionally similar to each other." ></td>
	<td class="line x" title="175:182	The confusion between main verbs [VM] and auxiliary verbs [VAUX] (row 3) arises from the fact that certain Bengali verbs can serve as both a main verb and an auxiliary verb, depending on the role the verb plays in the verb sequence." ></td>
	<td class="line x" title="176:182	6 Conclusions While Goldwater and Griffithss fully-Bayesian approach and the traditional maximum-likelihood parameter-based approach to unsupervised POS tagging have offered promising results for English, we argued in this paper that such results were obtained under the unrealistic assumption that a perfect POS lexicon is available, which renders these taggers less unsupervised than they appear." ></td>
	<td class="line x" title="177:182	As a result, we investigated a weakly supervised fullyBayesian approach to POS tagging, which relaxes the unrealistic assumption by automatically acquiring the lexicon from a small amount of POStagged data." ></td>
	<td class="line x" title="178:182	Since such relaxation comes at the expense of a drop in tagging accuracy, we proposed two performance-enhancing extensions to the Bayesian framework, namely, induced suffix emission and discriminative prediction, which effectively exploit morphology and techniques from supervised POS tagging, respectively." ></td>
	<td class="line x" title="179:182	Acknowledgments We thank the three anonymous reviewers and Sajib Dasgupta for their comments." ></td>
	<td class="line x" title="180:182	We also thank CRBLP, BRAC University, Bangladesh, for providing us with Bengali resources and Taufiq Hasan Al Banna for his MATLAB code." ></td>
	<td class="line x" title="181:182	This work was supported in part by NSF Grant IIS-0812261." ></td>
	<td class="line x" title="182:182	370" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1009
Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction
Cohen, Shay B.;Smith, Noah A.;"></td>
	<td class="line x" title="1:240	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 7482, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:240	c 2009 Association for Computational Linguistics Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction Shay B. Cohen and Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {scohen,nasmith}@cs.cmu.edu Abstract We present a family of priors over probabilisticgrammarweights, calledthesharedlogistic normal distribution." ></td>
	<td class="line x" title="3:240	This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar." ></td>
	<td class="line x" title="4:240	We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors." ></td>
	<td class="line x" title="5:240	We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus." ></td>
	<td class="line x" title="6:240	1 Introduction Probabilistic grammars have become an important tool in natural language processing." ></td>
	<td class="line xc" title="7:240	They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007)." ></td>
	<td class="line x" title="8:240	An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learningboth from labeled and unlabeled data." ></td>
	<td class="line x" title="9:240	Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors." ></td>
	<td class="line oc" title="10:240	There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007)." ></td>
	<td class="line x" title="11:240	Bayesian methods offer an elegant framework for combining prior knowledge with data." ></td>
	<td class="line x" title="12:240	The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, whichisgenerallyintractable." ></td>
	<td class="line oc" title="13:240	Mostcommonlyvariational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006)." ></td>
	<td class="line x" title="14:240	Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, moreprecisely,acollectionofDirichlets)isanatural candidate for probabilistic grammars because of its conjugacytothemultinomialfamily." ></td>
	<td class="line x" title="15:240	Conjugacyimplies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability." ></td>
	<td class="line x" title="16:240	Following work by Blei and Lafferty (2006) for topic models, Cohen et al.(2008) proposed an alternative to Dirichlet priors for probabilistic grammars, based on the logistic normal (LN) distribution over the probability simplex." ></td>
	<td class="line x" title="18:240	Cohen et al. used this prior tosoftlytiegrammarweightsthroughthecovariance parameters of the LN." ></td>
	<td class="line x" title="19:240	The prior encodes information about which grammar rules weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.1 The contribution of this paper is two-fold." ></td>
	<td class="line x" title="20:240	First, from the modeling perspective, we present a generalization of the LN prior of Cohen et al.(2008), showing how to extend the use of the LN prior to 1Although the task, underlying model, and weights being tied were different, Eisner (2002) also showed evidence for the efficacy of parameter tying in grammar learning." ></td>
	<td class="line x" title="22:240	74 tie between any grammar weights in a probabilistic grammar (instead of only allowing weights within the same multinomial distribution to covary)." ></td>
	<td class="line x" title="23:240	Second, from the experimental perspective, we show how such flexibility in parameter tying can help in unsupervised grammar learning in the well-known monolingual setting and in a new bilingual setting where grammars for two languages are learned at once (without parallel corpora)." ></td>
	<td class="line x" title="24:240	Our method is based on a distribution which we call the shared logistic normal distribution, which is a distribution over a collection of multinomials from different probability simplexes." ></td>
	<td class="line x" title="25:240	We provide a variational EM algorithm for inference." ></td>
	<td class="line x" title="26:240	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="27:240	In 2,wegiveabriefexplanationofprobabilisticgrammars and introduce some notation for the specific type of dependency grammar used in this paper, due to Klein and Manning (2004)." ></td>
	<td class="line x" title="28:240	In3, we present our modelandavariationalinferencealgorithmforit." ></td>
	<td class="line x" title="29:240	In 4, we report on experiments for both monolingual settingsandabilingualsettinganddiscussthem." ></td>
	<td class="line x" title="30:240	We discuss future work (5) and conclude in6." ></td>
	<td class="line x" title="31:240	2 Probabilistic Grammars and Dependency Grammar Induction A probabilistic grammar defines a probability distribution over grammatical derivations generated through a step-by-step process." ></td>
	<td class="line x" title="32:240	HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state." ></td>
	<td class="line x" title="33:240	Each step of the walk and each symbol emission corresponds to one derivation step." ></td>
	<td class="line x" title="34:240	PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of child symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM)." ></td>
	<td class="line x" title="35:240	Each step or emission of an HMM and each rewriting operation of a PCFG is conditionally independent of the other rewriting operations given a single structural element (one HMM or PCFG state); this Markov property permits efficient inference for the probability distribution defined by the probabilistic grammar." ></td>
	<td class="line x" title="36:240	In general, a probabilistic grammar defines the joint probability of a string x and a grammatical derivation y: p(x,y|) = Kproductdisplay k=1 Nkproductdisplay i=1 fk,i(x,y)k,i (1) = exp Ksummationdisplay k=1 Nksummationdisplay i=1 fk,i(x,y)logk,i where fk,i is a function that counts the number of times the kth distributions ith event occurs in the derivation." ></td>
	<td class="line x" title="37:240	The  are a collection of K multinomials1,,K, the kth of which includes Nk events." ></td>
	<td class="line x" title="38:240	Note that there may be many derivations y for a given string xperhaps even infinitely many in some kinds of grammars." ></td>
	<td class="line x" title="39:240	2.1 Dependency Model with Valence HMMs and PCFGs are the best-known probabilistic grammars, but there are many others." ></td>
	<td class="line x" title="40:240	In this paper, we use the dependency model with valence (DMV), due to Klein and Manning (2004)." ></td>
	<td class="line x" title="41:240	DMV defines a probabilistic grammar for unlabeled, projective dependency structures." ></td>
	<td class="line x" title="42:240	Klein and Manning (2004) achieved their best results with a combination of DMV with a model known as the constituent-context model (CCM)." ></td>
	<td class="line x" title="43:240	We do not experiment with CCM in this paper, because it does notfit directlyina Bayesiansetting(itis highlydeficient) and because state-of-the-art unsupervised dependency parsing results have been achieved with DMV alone (Smith, 2006)." ></td>
	<td class="line x" title="44:240	Using the notation above, DMV defines x = x1,x2,,xn to be a sentence." ></td>
	<td class="line x" title="45:240	x0 is a special wall symbol, $, on the left of every sentence." ></td>
	<td class="line x" title="46:240	A tree y is defined by a pair of functions yleft and yright (both{0,1,2,,n} 2{1,2,,n}) that map each word to its sets of left and right dependents, respectively." ></td>
	<td class="line x" title="47:240	Here, the graph is constrained to be a projective tree rooted atx0 = $: each word except $ has a single parent, and there are no cycles or crossing dependencies." ></td>
	<td class="line x" title="48:240	yleft(0) is taken to be empty, and yright(0) contains the sentences single head." ></td>
	<td class="line x" title="49:240	Let y(i) denote the subtree rooted at position i. The probability P(y(i) | xi,) of generating this subtree, given its head word xi, is defined recursively, as described in Fig." ></td>
	<td class="line x" title="50:240	1 (Eq." ></td>
	<td class="line x" title="51:240	2)." ></td>
	<td class="line x" title="52:240	The probability of the entire tree is given by p(x,y |) = P(y(0) | $,)." ></td>
	<td class="line x" title="53:240	The  are the multinomial distributions s(|,,) and c(|,)." ></td>
	<td class="line x" title="54:240	To 75 P(y(i)|xi,) = producttextD{left,right}s(stop|xi,D,[yD(i) =]) (2) producttextjyD(i)s(stop|xi,D,firsty(j))c(xj |xi,D)P(y(j)|xj,) Figure 1: The dependency model with valence recursive equation." ></td>
	<td class="line x" title="55:240	firsty(j) is a predicate defined to be true iffxj is the closest child (on either side) to its parentxi." ></td>
	<td class="line x" title="56:240	The probability of the treep(x,y|) = P(y(0)|$,)." ></td>
	<td class="line x" title="57:240	follow the general setting of Eq." ></td>
	<td class="line x" title="58:240	1, we index these distributions as1,,K. Headden et al.(2009) extended DMV so that the distributions c condition on the valence as well, with smoothing, and showed significant improvements for short sentences." ></td>
	<td class="line x" title="60:240	Our experiments found that these improvements do not hold on longer sentences." ></td>
	<td class="line x" title="61:240	Here we experiment only with DMV, but note that our techniques are also applicable to richer probabilistic grammars like that of Headden et al. 2.2 Learning DMV Klein and Manning (2004) learned the DMV probabilities  from a corpus of part-of-speech-tagged sentences using the EM algorithm." ></td>
	<td class="line x" title="62:240	EM manipulates  to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y)." ></td>
	<td class="line x" title="63:240	The likelihood surface is not globally concave, so EM only locally optimizes the surface." ></td>
	<td class="line x" title="64:240	Klein and Mannings initialization, though reasonable and language-independent, was an important factor in performance." ></td>
	<td class="line x" title="65:240	Various alternatives to EM were explored by Smith (2006), achieving substantially more accurate parsing models by altering the objective function." ></td>
	<td class="line x" title="66:240	Smiths methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters." ></td>
	<td class="line x" title="67:240	In this paper, we consider only fullyunsupervisedmethods,thoughwetheBayesian ideas explored here might be merged with the biasing approaches of Smith (2006) for further benefit." ></td>
	<td class="line x" title="68:240	3 Parameter Tying in the Bayesian Setting As stated above,  comprises a collection of multinomials that weights the grammar." ></td>
	<td class="line x" title="69:240	Taking the Bayesianapproach,wewishtoplaceaprioronthose multinomials, and the Dirichlet family is a natural candidate for such a prior because of its conjugacy, which makes inference algorithms easier to derive." ></td>
	<td class="line oc" title="70:240	For example, if we make a mean-field assumption, with respect to hidden structure and weights, the variationalalgorithmforapproximatelyinferringthe distribution over  and trees y resembles the traditional EM algorithm very closely (Johnson, 2007)." ></td>
	<td class="line x" title="71:240	In fact, variational inference in this case takes an actionsimilartosmoothingthecountsusingtheexp- functionduringtheE-step." ></td>
	<td class="line x" title="72:240	Variationalinferencecan beembeddedinanempiricalBayessetting, inwhich weoptimizethevariationalboundwithrespecttothe hyperparameters as well, repeating the process until convergence." ></td>
	<td class="line x" title="73:240	3.1 Logistic Normal Distributions While Dirichlet priors over grammar probabilities make learning algorithms easy, they are limiting." ></td>
	<td class="line x" title="74:240	In particular, as noted by Blei and Lafferty (2006), there is no explicit flexible way for the Dirichlets parameters to encode beliefs about covariance between the probabilities of two events." ></td>
	<td class="line x" title="75:240	To illustrate this point, we describe how a multinomial  of dimensiondis generated from a Dirichlet distribution with parameters=1,,d: 1." ></td>
	<td class="line x" title="76:240	Generate j  (j,1) independently for j  {1,,d}." ></td>
	<td class="line x" title="77:240	2." ></td>
	<td class="line x" title="78:240	j j/summationtextii. where (,1) is a Gamma distribution with shape and scale 1." ></td>
	<td class="line x" title="79:240	Correlation among i and j, i negationslash= j, cannot be modeled directly, only through the normalization in step 2." ></td>
	<td class="line x" title="80:240	In contrast, LN distributions (Aitchison, 1986) provide a natural way to model such correlation." ></td>
	<td class="line x" title="81:240	The LN draws a multinomialas follows: 1." ></td>
	<td class="line x" title="82:240	GenerateNormal(,)." ></td>
	<td class="line x" title="83:240	2." ></td>
	<td class="line x" title="84:240	j exp(j)/summationtexti exp(i)." ></td>
	<td class="line x" title="85:240	76 I1 = {1:2,3:6,7:9} = { I1,1, I1,2, I1,L1 } I2 = {1:2,3:6} = { I2,1, I2,L2 } I3 = {1:4,5:7} = { I3,1, I3,L3 } IN = {1:2} = { I4,L4 } J1 J2 JK      partition struct." ></td>
	<td class="line x" title="86:240	S 1 = 1,1,1,2, 1,3,1,4,1,5,1,6, 1,7,1,8,1,lscript1  Normal(1,1) 2 = 2,1,2,2, 2,3,2,4,2,5,2,lscript2  Normal(2,2) 3 = 3,1,3,2,3,3,3,4, 3,5,3,6,3,lscript3  Normal(3,3) 4 = 4,1,4,lscript4  Normal(4,4)    sample 1 = 131,1 +2,1 +4,1, 1,2 +2,2 +4,2 2 = 131,3 +2,3 +3,1, 1,4 +2,4 +3,2, 1,5 +2,5 +3,3, 1,6 +2,6 +3,4 3 = 121,7 +3,5, 1,8 +3,6, 1,9 +3,7    combine 1 = (exp 1) slashBigsummationtextN 1 iprime=1 exp 1,iprime 2 = (exp 2) slashBigsummationtextN 2 iprime=1 exp 2,iprime 3 = (exp 3) slashBigsummationtextN 3 iprime=1 exp 3,iprime      softmax Figure 2: An example of a shared logistic normal distribution, illustrating Def." ></td>
	<td class="line x" title="87:240	1." ></td>
	<td class="line x" title="88:240	N = 4 experts are used to sample K = 3 multinomials; L1 = 3, L2 = 2, L3 = 2, L4 = 1, lscript1 = 9, lscript2 = 6, lscript3 = 7, lscript4 = 2, N1 = 2, N2 = 4, and N3 = 3." ></td>
	<td class="line x" title="89:240	This figure is best viewed in color." ></td>
	<td class="line x" title="90:240	Blei and Lafferty (2006) defined correlated topic models by replacing the Dirichlet in latent Dirichlet allocation models (Blei et al., 2003) with a LN distribution." ></td>
	<td class="line x" title="91:240	Cohen et al.(2008) compared Dirichlet and LN distributions for learning DMV using empirical Bayes, finding substantial improvements for English using the latter." ></td>
	<td class="line x" title="93:240	In that work, we obtained improvements even without specifying exactly which grammar probabilities covaried." ></td>
	<td class="line x" title="94:240	While empirical Bayes learning permits these covariances to be discovered without supervision, we found that by initializing the covariance to encode beliefs about which grammar probabilities should covary, further improvements were possible." ></td>
	<td class="line x" title="95:240	Specifically, we grouped the Penn Treebank part-of-speech tags into coarse groups based on the treebank annotation guidelines and biased the initial covariance matrix for each child distribution c( | ,) so that the probabilities of child tags from the same coarse group covaried." ></td>
	<td class="line x" title="96:240	For example, the probability that a past-tense verb (VBD) has a singular noun (NN) as a right child may be correlated with the probability that it has a plural noun (NNS) as a right child." ></td>
	<td class="line x" title="97:240	Hence linguistic knowledgespecifically, a coarse grouping of word classescan be encoded in the prior." ></td>
	<td class="line x" title="98:240	A per-distribution LN distribution only permits probabilities within a multinomial to covary." ></td>
	<td class="line x" title="99:240	We will generalize the LN to permit covariance among any probabilities in , throughout the model." ></td>
	<td class="line x" title="100:240	For example, the probability of a past-tense verb (VBD) having a noun as a right child might correlate with theprobabilitythatotherkindsofverbs(VBZ,VBN, etc.) have a noun as a right child." ></td>
	<td class="line x" title="101:240	The partitioned logistic normal distribution (PLN) is a generalization of the LN distribution that takes the first step towards our goal (Aitchison, 1986)." ></td>
	<td class="line x" title="102:240	Generating from PLN involves drawing a random vector from a multivariate normal distribution, but the logistic transformation is applied to different parts of the vector, leading to sampled multinomial distributions of the required lengths from different probability simplices." ></td>
	<td class="line x" title="103:240	This is in principle what is required for arbitrary covariance between grammar probabilities, except that DMV has O(t2) weights for a part-of-speech vocabulary of sizet, requiring a very large multivariate normal distribution withO(t4) covariance parameters." ></td>
	<td class="line x" title="104:240	77 3.2 Shared Logistic Normal Distributions To solve this problem, we suggest a refinement of the class of PLN distributions." ></td>
	<td class="line x" title="105:240	Instead of using a single normal vector for all of the multinomials, we use several normal vectors, partition each one and then recombine parts which correspond to the same multinomial, as a mixture." ></td>
	<td class="line x" title="106:240	Next, we apply the logisitic transformation on the mixed vectors (each of which is normally distributed as well)." ></td>
	<td class="line x" title="107:240	Fig." ></td>
	<td class="line x" title="108:240	2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts." ></td>
	<td class="line x" title="109:240	We now formalize this notion." ></td>
	<td class="line x" title="110:240	For a natural number N, we denote by 1:N the set{1,,N}." ></td>
	<td class="line x" title="111:240	For a vector in v  RN and a set I  1:N, we denote by vI to be the vector created from v by using the coordinates in I. Recall that K is the number of multinomials in the probabilistic grammar, and Nk is the number of events in thekth multinomial." ></td>
	<td class="line x" title="112:240	Definition 1." ></td>
	<td class="line x" title="113:240	We define a shared logistic normal distribution with N experts over a collection of K multinomial distributions." ></td>
	<td class="line x" title="114:240	Let n  Normal(n,n) be a set of multivariate normal variables for n  1:N, where the length of n is denoted lscriptn." ></td>
	<td class="line x" title="115:240	Let In = {In,j}Lnj=1 be a partition of 1:lscriptn into Ln sets, such that Lnj=1In,j = 1:lscriptn and In,j In,jprime =  for j negationslash= jprime." ></td>
	<td class="line x" title="116:240	Let Jk for k  1:K be a collection of (disjoint) subsets of {In,j | n  1:N,j  1:lscriptn,|In,j| = Nk}, such that all sets in Jk are of the same size, Nk." ></td>
	<td class="line x" title="117:240	Let k = 1|Jk| summationtextIn,jJk n,In,j, and k,i = exp(k,i)slashbigsummationtextiprime exp(k,iprime)." ></td>
	<td class="line x" title="118:240	We then saydistributes according to the shared logistic normal distribution with partition structure S = ({In}Nn=1,{Jk}Kk=1) and normal experts{(n,n)}Nn=1 and denote it by SLN(,,S)." ></td>
	<td class="line x" title="119:240	The partitioned LN distribution in Aitchison (1986)canbeformulatedasasharedLNdistribution where N = 1." ></td>
	<td class="line x" title="120:240	The LN collection used by Cohen et al.(2008) is the special case where N = K, each Ln = 1, eachlscriptk = Nk, and eachJk ={Ik,1}." ></td>
	<td class="line x" title="122:240	Thecovarianceamongarbitraryk,i isnotdefined directly; it is implied by the definition of the normal experts n,In,j, for each In,j  Jk." ></td>
	<td class="line x" title="123:240	We note that a SLN can be represented as a PLN by relying on the distributivity of the covariance operator, and merging all the partition structure into one (perhaps sparse) covariance matrix." ></td>
	<td class="line x" title="124:240	However, if we are interested in keeping a factored structure on the covariance matrices which generate the grammar weights, we cannot represent every SLN as a PLN." ></td>
	<td class="line x" title="125:240	It is convenient to think of each i,j as a weight associated with a unique events probability, a certain outcome of a certain multinomial in the probabilistic grammar." ></td>
	<td class="line x" title="126:240	By letting different i,j covary with each other, we loosen the relationships among k,j and permit the modelat least in principle to learn patterns from the data." ></td>
	<td class="line x" title="127:240	Def." ></td>
	<td class="line x" title="128:240	1 also implies that we multiply several multinomials together in a product-of-experts style (Hinton, 1999), because the exponentialofamixtureofnormalsbecomesaproduct of (unnormalized) probabilities." ></td>
	<td class="line x" title="129:240	Our extension to the model in Cohen et al.(2008) follows naturally after we have defined the shared LN distribution." ></td>
	<td class="line x" title="131:240	The generative story for this model is as follows: 1." ></td>
	<td class="line x" title="132:240	GenerateSLN(,,S), whereis a collection of vectorsk,k = 1,,K. 2." ></td>
	<td class="line x" title="133:240	Generatexandy fromp(x,y|) (i.e., sample from the probabilistic grammar)." ></td>
	<td class="line x" title="134:240	3.3 Inference In this work, the partition structure S is known, the sentences x are observed, the trees y and the grammar weightsare hidden, and the parameters of the shared LN distributionand  are learned.2 Our inference algorithm aims to find the posterior over the grammar probabilitiesand the hidden structures (grammar trees y)." ></td>
	<td class="line x" title="135:240	To do that, we use variational approximation techniques (Jordan et al., 1999), which treat the problem of finding the posterior as an optimization problem aimed to find the best approximation q(,y) of the posterior p(,y| x,,,S)." ></td>
	<td class="line x" title="136:240	The posteriorq needs to be constrained to be within a family of tractable and manageable distributions, yet rich enough to represent good approximations of the true posterior." ></td>
	<td class="line x" title="137:240	Best approximation is defined as the KL divergence between q(,y) andp(,y|x,,,S)." ></td>
	<td class="line x" title="138:240	Our variational inference algorithm uses a meanfield assumption: q(,y) = q()q(y)." ></td>
	<td class="line x" title="139:240	The distributionq() is assumed to be a LN distribution with 2In future work, we might aim to learn S. 78 logp(x|,,S)  parenleftBigsummationtextN n=1Eq [logp(k |k,k)] parenrightBig + parenleftBigsummationtextK k=1 summationtextNk i=1 fk,i k,i parenrightBig +H(q) bracehtipupleft bracehtipdownrightbracehtipdownleft bracehtipupright B (3) fk,i defines summationtextyq(y)fk,i(x,y) (4) k,i defines Ck,ilog k + 1 1 k summationtextNk iprime=1 exp parenleftbigg Ck,i + ( C k,i) 2 2 parenrightbigg (5) Ck defines 1|Jk| summationtextIr,jJk r,Ir,j (6) (Ck )2 defines 1|J k|2 summationtext Ir,jJk  2 r,Ir,j (7) Figure 3: Variational inference bound." ></td>
	<td class="line x" title="140:240	Eq." ></td>
	<td class="line x" title="141:240	3 is the bound itself, using notation defined in Eqs." ></td>
	<td class="line x" title="142:240	47 for clarity." ></td>
	<td class="line x" title="143:240	Eq." ></td>
	<td class="line x" title="144:240	4 defines expected counts of the grammar events under the variational distributionq(y), calculated using dynamic programming." ></td>
	<td class="line x" title="145:240	Eq." ></td>
	<td class="line x" title="146:240	5 describes the weights for the weighted grammar defined byq(y)." ></td>
	<td class="line x" title="147:240	Eq." ></td>
	<td class="line x" title="148:240	6 and Eq." ></td>
	<td class="line x" title="149:240	7 describe the mean and the variance, respectively, for the multivariate normal eventually used with the weighted grammar." ></td>
	<td class="line x" title="150:240	These values are based on the parameterization of q() by i,j and 2i,j. An additional set of variational parameters is k, which helps resolve the non-conjugacy of the LN distribution through a first order Taylor approximation." ></td>
	<td class="line x" title="151:240	all off-diagonal covariances fixed at zero (i.e., the variational parameters consist of a single mean k,i and a single variance 2k,i for each k,i)." ></td>
	<td class="line x" title="152:240	There is an additional variational parameter, k per multinomial, which is the result of an additional variational approximation because of the lack of conjugacy of the LN distribution to the multinomial distribution." ></td>
	<td class="line x" title="153:240	The distribution q(y) is assumed to be defined by a DMV with unnormalized probabilities ." ></td>
	<td class="line x" title="154:240	Inference optimizes the bound B given in Fig." ></td>
	<td class="line x" title="155:240	3 (Eq." ></td>
	<td class="line x" title="156:240	3) with respect to the variational parameters." ></td>
	<td class="line x" title="157:240	Our variational inference algorithm is derived similarly to that of Cohen et al.(2008)." ></td>
	<td class="line x" title="159:240	Because we wish tolearnthevaluesofand,weembedvariational inference as the E step within a variational EM algorithm, shown schematically in Fig." ></td>
	<td class="line x" title="160:240	4." ></td>
	<td class="line x" title="161:240	In our experiments, we use this variational EM algorithm on a training set, and then use the normal experts means to get a point estimate for , the grammar weights." ></td>
	<td class="line x" title="162:240	This is called empirical Bayesian estimation." ></td>
	<td class="line x" title="163:240	Our approachdiffersfrommaximum a posteriori (MAP) estimation, since we re-estimate the parameters of the normal experts." ></td>
	<td class="line x" title="164:240	Exact MAP estimation is probably not feasible; a variational algorithm like ours might be applied, though better performance is expected from adjusting the SLN to fit the data." ></td>
	<td class="line x" title="165:240	4 Experiments Our experiments involve data from two treebanks: the Wall Street Journal Penn treebank (Marcus et al., 1993) and the Chinese treebank (Xue et al., 2004)." ></td>
	<td class="line x" title="166:240	In both cases, following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure." ></td>
	<td class="line x" title="167:240	For English, we trainon221, tuneon22(withoutusingannotated data), and report final results on23." ></td>
	<td class="line x" title="168:240	For Chinese, we train on1270, use3011151 for development and report testing results on271300.3 To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus." ></td>
	<td class="line x" title="169:240	This performance measure is also known as attachment accuracy." ></td>
	<td class="line x" title="170:240	We considered two parsing methods after extracting a point estimate for the grammar: the most probable Viterbi parse (argmaxyp(y|x,)) and the minimum Bayes risk (MBR) parse (argminyEp(yprime|x,)[lscript(y;x,yprime)]) with dependency attachment error as the loss function (Goodman, 1996)." ></td>
	<td class="line x" title="171:240	Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing." ></td>
	<td class="line x" title="172:240	4.1 Nouns, Verbs, and Adjectives In this paper, we use a few simple heuristics to decide which partition structure S to use." ></td>
	<td class="line x" title="173:240	Our heuris3Unsupervised training for these datasets can be costly, and requires iteratively running a cubic-time inside-outside dynamic programming algorithm, so we follow Klein and Manning (2004) in restricting the training set to sentences of ten or fewer words in length." ></td>
	<td class="line x" title="174:240	Short sentences are also less structurally ambiguous and may therefore be easier to learn from." ></td>
	<td class="line x" title="175:240	79 Input: initial parameters(0), (0), partition structure S, observed data x, number of iterationsT Output: learned parameters,  t1 ; whiletT do E-step (forlscript = 1,,M) do: repeat optimizeB w.r.t. lscript,(t)r ,r = 1,,N; optimizeB w.r.t. lscript,(t)r ,r = 1,,N; update lscript,(t)r ,r = 1,,N; update lscript,(t)r ,r = 1,,N; compute countsflscript,(t)r ,r = 1,,N; until convergence ofB ; M-step: optimizeB w.r.t.(t) and (t); tt+ 1; end return(T), (T) Figure 4: Main details of the variational inference EM algorithm with empirical Bayes estimation of  and ." ></td>
	<td class="line x" title="176:240	B is the bound defined in Fig." ></td>
	<td class="line x" title="177:240	3 (Eq." ></td>
	<td class="line x" title="178:240	3)." ></td>
	<td class="line x" title="179:240	N is the number of normal experts for the SLN distribution defining the prior." ></td>
	<td class="line x" title="180:240	M is the number of training examples." ></td>
	<td class="line x" title="181:240	The full algorithm is given in Cohen and Smith (2009)." ></td>
	<td class="line x" title="182:240	tics rely mainly on the centrality of content words: nouns,verbs,andadjectives." ></td>
	<td class="line x" title="183:240	Forexample,intheEnglish treebank, the most common attachment errors (with the LN prior from Cohen et al., 2008) happen with a noun (25.9%) or a verb (16.9%) parent." ></td>
	<td class="line x" title="184:240	In the Chinese treebank, the most common attachment errors happen with noun (36.0%) and verb (21.2%) parents as well." ></td>
	<td class="line x" title="185:240	The errors being governed by such attachments are the direct result of nouns and verbs being the most common parents in these data sets." ></td>
	<td class="line x" title="186:240	Following this observation, we compare four different settings in our experiments (all SLN settings include one normal expert for each multinomial on its own, equivalent to the regular LN setting from Cohen et al.):  TIEV: We add normal experts that tie all probabilities corresponding to a verbal parent (any parent, using the coarse tags of Cohen et al., 2008)." ></td>
	<td class="line x" title="187:240	LetV be the set of part-of-speech tags which belong to the verb category." ></td>
	<td class="line x" title="188:240	For each direction D (left or right), the set of multinomials of the form c(|v,D), forvV, all share a normal expert." ></td>
	<td class="line x" title="189:240	For each direction D and each boolean value B of the predicate firsty(), the set of multinomials s(|x,D,v), forvV share a normal expert." ></td>
	<td class="line x" title="190:240	 TIEN: This is the same as TIEV, only for nominal parents." ></td>
	<td class="line x" title="191:240	 TIEV&N: Tie both verbs and nouns (in separate partitions)." ></td>
	<td class="line x" title="192:240	This is equivalent to taking the union of the partition structures of the above two settings." ></td>
	<td class="line x" title="193:240	 TIEA: This is the same as TIEV, only for adjectival parents." ></td>
	<td class="line x" title="194:240	Since inference for a model with parameter tying can be computationally intensive, we first run the inference algorithm without parameter tying, and then add parameter tying to the rest of the inference algorithms execution until convergence." ></td>
	<td class="line x" title="195:240	Initialization is important for the inference algorithm, because the variational bound is a nonconcave function." ></td>
	<td class="line x" title="196:240	For the expected values of the normalexperts, weusetheinitializerfromKleinand Manning (2004)." ></td>
	<td class="line x" title="197:240	For the covariance matrices, we follow the setting in Cohen et al.(2008) in our experiments also described in3.1." ></td>
	<td class="line x" title="199:240	For each treebank, we divide the tags into twelve disjoint tag families.4 The covariance matrices for all dependency distributions were initialized with 1 on the diagonal, 0.5 between tags which belong to the same family, and 0 otherwise." ></td>
	<td class="line x" title="200:240	This initializer has been shown to be more successful than an identity covariance matrix." ></td>
	<td class="line x" title="201:240	4.2 Monolingual Experiments We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above." ></td>
	<td class="line x" title="202:240	The attachment accuracy for this set of experiments is described in Table 1." ></td>
	<td class="line x" title="203:240	The baselines include right attachment (where each word is attached to the word to its right), MLE via EM (Klein and Manning, 2004), and empirical Bayes with Dirichlet and LN priors (Cohen et al., 2008)." ></td>
	<td class="line x" title="204:240	We also include a ceiling(DMVtrainedusingsupervisedMLEfrom the training sentences trees)." ></td>
	<td class="line x" title="205:240	For English, we see that tying nouns, verbs or adjectives improves performance compared to the LN baseline." ></td>
	<td class="line x" title="206:240	Tying both nouns and verbs improves performance a bit more." ></td>
	<td class="line x" title="207:240	4These are simply coarser tags: adjective, adverb, conjunction, foreign word, interjection, noun, number, particle, preposition, pronoun, proper noun, verb." ></td>
	<td class="line x" title="208:240	80 attachment acc." ></td>
	<td class="line x" title="209:240	(%) 10 20 all English Attach-Right 38.4 33.4 31.7 EM (K&M, 2004) 46.1 39.9 35.9 Dirichlet 46.1 40.6 36.9 LN (CG&S, 2008) 59.4 45.9 40.5 SLN, TIEV 60.2 46.2 40.0 SLN, TIEN 60.2 46.7 40.9 SLN, TIEV&N 61.3 47.4 41.4 SLN, TIEA 59.9 45.8 40.9 Biling." ></td>
	<td class="line x" title="210:240	SLN, TIEV 61.6 47.6 41.7 Biling." ></td>
	<td class="line x" title="211:240	SLN, TIEN 61.8 48.1 42.1 Biling." ></td>
	<td class="line x" title="212:240	SLN, TIEV&N 62.0 48.0 42.2 Biling." ></td>
	<td class="line x" title="213:240	SLN, TIEA 61.3 47.6 41.7 Supervised MLE 84.5 74.9 68.8 Chinese Attach-Right 34.9 34.6 34.6 EM (K&M, 2004) 38.3 36.1 32.7 Dirichlet 38.3 35.9 32.4 LN 50.1 40.5 35.8 SLN, TIEV 51.9 42.0 35.8 SLN, TIEN 43.0 38.4 33.7 SLN, TIEV&N 45.0 39.2 34.2 SLN, TIEA 47.4 40.4 35.2 Biling." ></td>
	<td class="line x" title="214:240	SLN, TIEV 51.9 42.0 35.8 Biling." ></td>
	<td class="line x" title="215:240	SLN, TIEN 48.0 38.9 33.8 Biling." ></td>
	<td class="line x" title="216:240	SLN, TIEV&N 51.5 41.7 35.3 Biling." ></td>
	<td class="line x" title="217:240	SLN, TIEA 52.0 41.3 35.2 Supervised MLE 84.3 66.1 57.6 Table 1: Attachment accuracy of different models, on test data from the Penn Treebank and the Chinese Treebank of varying levels of difficulty imposed through a length filter." ></td>
	<td class="line x" title="218:240	Attach-Right attaches each word to the word on its right and the last word to $." ></td>
	<td class="line x" title="219:240	Bold marks best overall accuracy per length bound, andmarks figures that are not significantly worse (binomial sign test,p< 0.05)." ></td>
	<td class="line x" title="220:240	4.3 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008)." ></td>
	<td class="line x" title="221:240	Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly." ></td>
	<td class="line x" title="222:240	Shar5Haghighi et al.(2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora." ></td>
	<td class="line x" title="224:240	ing information between those two models is done by softly tying grammar weights in the two hidden grammars." ></td>
	<td class="line x" title="225:240	We first merge the models for English and Chinese by taking a union of the multinomial families of each and the corresponding prior parameters." ></td>
	<td class="line x" title="226:240	We then add a normal expert that ties between the parts of speech in the respective partition structures for both grammars together." ></td>
	<td class="line x" title="227:240	Parts of speech are matched through the single coarse tagset (footnote 4)." ></td>
	<td class="line x" title="228:240	For example, with TIEV, let V =VEngVChi be the set of part-of-speech tags which belong to the verb category for either treebank." ></td>
	<td class="line x" title="229:240	Then, we tie parameters for all part-of-speech tags in V. We tested this joint model for each of TIEV, TIEN, TIEV&N, and TIEA." ></td>
	<td class="line x" title="230:240	After running the inference algorithm which learns the two models jointly, we use unseen data to test each learned model separately." ></td>
	<td class="line x" title="231:240	Table 1 includes the results for these experiments." ></td>
	<td class="line x" title="232:240	The performance on English improved significantly in the bilingual setting, achieving highest performance with TIEV&N. Performance with Chinese is also the highest in the bilingual setting, with TIEA and TIEV&N. 5 Future Work In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for thesyntacticpatternsofwords." ></td>
	<td class="line x" title="233:240	Nonparametricmodels (Teh, 2006) may be appropriate." ></td>
	<td class="line x" title="234:240	We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration." ></td>
	<td class="line x" title="235:240	6 Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar." ></td>
	<td class="line x" title="236:240	We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results." ></td>
	<td class="line x" title="237:240	We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data." ></td>
	<td class="line x" title="238:240	Acknowledgments This research was supported by NSF IIS-0836431." ></td>
	<td class="line x" title="239:240	The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments." ></td>
	<td class="line x" title="240:240	81" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1069
Online EM for Unsupervised Models
Liang, Percy;Klein, Dan;"></td>
	<td class="line x" title="1:198	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 611619, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:198	c 2009 Association for Computational Linguistics Online EM for Unsupervised Models Percy Liang Dan Klein Computer Science Division, EECS Department University of California at Berkeley Berkeley, CA 94720 {pliang,klein}@cs.berkeley.edu Abstract The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence." ></td>
	<td class="line x" title="3:198	In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM." ></td>
	<td class="line x" title="4:198	We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment." ></td>
	<td class="line x" title="5:198	1 Introduction In unsupervised NLP tasks such as tagging, parsing, and alignment, one wishes to induce latent linguistic structures from raw text." ></td>
	<td class="line x" title="6:198	Probabilistic modeling has emerged as a dominant paradigm for these problems, and the EM algorithm has been a driving force for learning models in a simple and intuitive manner." ></td>
	<td class="line x" title="7:198	However, on some tasks, EM can converge slowly." ></td>
	<td class="line oc" title="8:198	For instance, on unsupervised part-ofspeech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007)." ></td>
	<td class="line x" title="9:198	The slowness of EM is mainly due to its batch nature: Parameters are updated only once after each pass through the data." ></td>
	<td class="line x" title="10:198	When parameter estimates are still rough or if there is high redundancy in the data, computing statistics on the entire dataset just to make one update can be wasteful." ></td>
	<td class="line x" title="11:198	In this paper, we investigate two flavors of online EMincremental EM (Neal and Hinton, 1998) and stepwise EM (Sato and Ishii, 2000; Cappe and Moulines, 2009), both of which involve updating parameters after each example or after a mini-batch (subset) of examples." ></td>
	<td class="line x" title="12:198	Online algorithms have the potential to speed up learning by making updates more frequently." ></td>
	<td class="line x" title="13:198	However, these updates can be seen as noisy approximations to the full batch update, and this noise can in fact impede learning." ></td>
	<td class="line x" title="14:198	This tradeoff between speed and stability is familiar to online algorithms for convex supervised learning problemse.g., Perceptron, MIRA, stochastic gradient, etc. Unsupervised learning raises two additional issues: (1) Since the EM objective is nonconvex, we often get convergence to different local optima of varying quality; and (2) we evaluate on accuracy metrics which are at best loosely correlated with the EM likelihood objective (Liang and Klein, 2008)." ></td>
	<td class="line x" title="15:198	We will see that these issues can lead to surprising results." ></td>
	<td class="line x" title="16:198	In Section 4, we present a thorough investigation of online EM, mostly focusing on stepwise EM since it dominates incremental EM." ></td>
	<td class="line x" title="17:198	For stepwise EM, we find that choosing a good stepsize and mini-batch size is important but can fortunately be done adequately without supervision." ></td>
	<td class="line x" title="18:198	With a proper choice, stepwise EM reaches the same performance as batch EM, but much more quickly." ></td>
	<td class="line x" title="19:198	Moreover, it can even surpass the performance of batch EM." ></td>
	<td class="line x" title="20:198	Our results are particularly striking on part-of-speech tagging: Batch EM crawls to an accuracy of 57.3% after 100 iterations, whereas stepwise EM shoots up to 65.4% after just two iterations." ></td>
	<td class="line x" title="21:198	2 Tasks, models, and datasets In this paper, we focus on unsupervised induction via probabilistic modeling." ></td>
	<td class="line x" title="22:198	In particular, we define a probabilistic model p(x,z;) of the input x (e.g., 611 a sentence) and hidden output z (e.g., a parse tree) with parameters  (e.g., rule probabilities)." ></td>
	<td class="line x" title="23:198	Given a set of unlabeled examples x(1),,x(n), the standard training objective is to maximize the marginal log-likelihood of these examples: lscript() = nsummationdisplay i=1 logp(x(i);)." ></td>
	<td class="line x" title="24:198	(1) A trained model  is then evaluated on the accuracy of its predictions: argmaxzp(z|x(i); ) against the true output z(i); the exact evaluation metric depends on the task." ></td>
	<td class="line x" title="25:198	What makes unsupervised induction hard at best and ill-defined at worst is that the training objective (1) does not depend on the true outputs at all." ></td>
	<td class="line x" title="26:198	We ran experiments on four tasks described below." ></td>
	<td class="line x" title="27:198	Two of these taskspart-of-speech tagging and document classificationare clustering tasks." ></td>
	<td class="line x" title="28:198	For these, the output z consists of labels; for evaluation, we map each predicted label to the true label that maximizes accuracy." ></td>
	<td class="line x" title="29:198	The other two tasks segmentation and alignmentonly involve unlabeled combinatorial structures, which can be evaluated directly." ></td>
	<td class="line x" title="30:198	Part-of-speech tagging For each sentence x = (x1,,xlscript), represented as a sequence of words, we wish to predict the corresponding sequence of partof-speech (POS) tags z = (z1,,zlscript)." ></td>
	<td class="line x" title="31:198	We used a simple bigram HMM trained on the Wall Street Journal (WSJ) portion of the Penn Treebank (49208 sentences, 45 tags)." ></td>
	<td class="line x" title="32:198	No tagging dictionary was used." ></td>
	<td class="line x" title="33:198	We evaluated using per-position accuracy." ></td>
	<td class="line x" title="34:198	Document classification For each document x = (x1,,xlscript) consisting of lscript words,1 we wish to predict the document class z{1,,20}." ></td>
	<td class="line x" title="35:198	Each document x is modeled as a bag of words drawn independently given the class z. We used the 20 Newsgroups dataset (18828 documents, 20 classes)." ></td>
	<td class="line x" title="36:198	We evaluated on class accuracy." ></td>
	<td class="line x" title="37:198	Word segmentation For each sentence x = (x1,,xlscript), represented as a sequence of English phonemes or Chinese characters without spaces separating the words, we would like to predict 1We removed the 50 most common words and words that occurred fewer than 5 times." ></td>
	<td class="line x" title="38:198	a segmentation of the sequence into words z = (z1,,z|z|), where each segment (word) zi is a contiguous subsequence of 1,,lscript." ></td>
	<td class="line x" title="39:198	Since the nave unigram model has a degenerate maximum likelihood solution that makes each sentence a separate word, we incorporate a penalty for longer segments: p(x,z;)  producttext|z|k=1p(xzk;)e|zk|, where  > 1 determines the strength of the penalty." ></td>
	<td class="line x" title="40:198	For English, we used  = 1.6; Chinese,  = 2.5." ></td>
	<td class="line x" title="41:198	To speed up inference, we restricted the maximum segment length to 10 for English and 5 for Chinese." ></td>
	<td class="line x" title="42:198	We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al.(2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences)." ></td>
	<td class="line x" title="44:198	We evaluated using F1 on word tokens." ></td>
	<td class="line x" title="45:198	To the best of our knowledge, our penalized unigram model is new and actually beats the more complicated model of Johnson (2008) 83.5% to 78%, which had been the best published result on this task." ></td>
	<td class="line x" title="46:198	Word alignment For each pair of translated sentences x = (e1,,ene,f1,,fnf), we wish to predict the word alignments z  {0,1}nenf." ></td>
	<td class="line x" title="47:198	We trained two IBM model 1s using agreement-based learning (Liang et al., 2008)." ></td>
	<td class="line x" title="48:198	We used the first 30K sentence pairs of the English-French Hansards data from the NAACL 2003 Shared Task, 447+37 of which were hand-aligned (Och and Ney, 2003)." ></td>
	<td class="line x" title="49:198	We evaluated using the standard alignment error rate (AER)." ></td>
	<td class="line x" title="50:198	3 EM algorithms Given a probabilistic model p(x,z;) and unlabeled examples x(1),,x(n), recall we would like to maximize the marginal likelihood of the data (1)." ></td>
	<td class="line x" title="51:198	Let (x,z) denote a mapping from a fullylabeled example (x,z) to a vector of sufficient statistics (counts in the case of multinomials) for the model." ></td>
	<td class="line x" title="52:198	For example, one component of this vector for HMMs would be the number of times state 7 emits the word house in sentence x with state sequence z. Given a vector of sufficient statistics , let() denote the maximum likelihood estimate." ></td>
	<td class="line x" title="53:198	In our case, () are simply probabilities obtained by normalizing each block of counts." ></td>
	<td class="line x" title="54:198	This closed-form 612 Batch EM initialization for each iterationt= 1,,T: prime 0 for each examplei= 1,,n: sprimeisummationtextzp(z|x(i);())(x(i),z) [inference] prime prime +sprimei [accumulatenew] prime [replaceold with new] solution is one of the features that makes EM (both batch and online) attractive." ></td>
	<td class="line x" title="55:198	3.1 Batch EM In the (batch) EM algorithm, we alternate between the E-step and the M-step." ></td>
	<td class="line x" title="56:198	In the E-step, we compute the expected sufficient statistics prime across all the examples based on the posterior over z under the current parameters ()." ></td>
	<td class="line x" title="57:198	In all our models, this step can be done via a dynamic program (for example, forward-backward for POS tagging)." ></td>
	<td class="line x" title="58:198	In the M-step, we use these sufficient statistics prime to re-estimate the parameters." ></td>
	<td class="line x" title="59:198	Since the M-step is trivial, we represent it implicitly by () in order to concentrate on the computation of the sufficient statistics." ></td>
	<td class="line x" title="60:198	This focus will be important for online EM, so writing batch EM in this way accentuates the parallel between batch and online." ></td>
	<td class="line x" title="61:198	3.2 Online EM To obtain an online EM algorithm, we store a single set of sufficient statistics  and update it after processing each example." ></td>
	<td class="line x" title="62:198	For the i-th example, we compute sufficient statistics sprimei." ></td>
	<td class="line x" title="63:198	There are two main variants of online EM algorithms which differ in exactly how the new sprimei is incorporated into ." ></td>
	<td class="line x" title="64:198	The first is incremental EM (iEM) (Neal and Hinton, 1998), in which we not only keep track of  but also the sufficient statistics s1,,sn for each example ( =summationtextni=1si)." ></td>
	<td class="line x" title="65:198	When we process example i, we subtract out the old si and add the new sprimei." ></td>
	<td class="line x" title="66:198	Sato and Ishii (2000) developed another variant, later generalized by Cappe and Moulines (2009), which we call stepwise EM (sEM)." ></td>
	<td class="line x" title="67:198	In sEM, we interpolate between  and sprimei based on a stepsize k (k is the number of updates made to  so far)." ></td>
	<td class="line x" title="68:198	The two algorithms are motivated in different ways." ></td>
	<td class="line x" title="69:198	Recall that the log-likelihood can be lower IncrementalEM (iEM) siinitializationfori= 1,,n summationtextni=1si for each iterationt= 1,,T: for each examplei= 1,,nin randomorder: sprimeisummationtextzp(z|x(i);())(x(i),z) [inference] +sprimeisi; sisprimei [replaceold with new] StepwiseEM (sEM) initialization;k = 0 for each iterationt= 1,,T: for each examplei= 1,,nin randomorder: sprimeisummationtextzp(z|x(i);())(x(i),z) [inference] (1k)+ksprimei;kk+1 [towards new] bounded as follows (Neal and Hinton, 1998): lscript()L(q1,,qn,) (2) def= nsummationdisplay i=1 bracketleftBigsummationdisplay z qi(z|x(i))logp(x(i),z;) +H(qi) bracketrightBig , where H(qi) is the entropy of the distribution qi(z| x(i))." ></td>
	<td class="line x" title="70:198	Batch EM alternates between optimizing L with respect to q1,,qn in the E-step (represented implicitly via sufficient statistics prime) and with respect to  in the M-step." ></td>
	<td class="line x" title="71:198	Incremental EM alternates between optimizing with respect to a singleqi and." ></td>
	<td class="line x" title="72:198	Stepwise EM is motivated from the stochastic approximation literature, where we think of approximating the update prime in batch EM with a single sample sprimei." ></td>
	<td class="line x" title="73:198	Since one sample is a bad approximation, we interpolate between sprimei and the current ." ></td>
	<td class="line x" title="74:198	Thus, sEM can be seen as stochastic gradient in the space of sufficient statistics." ></td>
	<td class="line x" title="75:198	Stepsize reduction power  Stepwise EM leaves open the choice of the stepsize k. Standard results from the stochastic approximation literature state that summationtextk=0k =  and summationtextk=02k <  are sufficient to guarantee convergence to a local optimum." ></td>
	<td class="line x" title="76:198	In particular, if we take k = (k + 2), then any 0.5 < 1 is valid." ></td>
	<td class="line x" title="77:198	The smaller the , the larger the updates, and the more quickly we forget (decay) our old sufficient statistics." ></td>
	<td class="line x" title="78:198	This can lead to swift progress but also generates instability." ></td>
	<td class="line x" title="79:198	Mini-batch size m We can add some stability to sEM by updating on multiple examples at once 613 instead of just one." ></td>
	<td class="line x" title="80:198	In particular, partition the n examples into mini-batches of size m and run sEM, treating each mini-batch as a single example." ></td>
	<td class="line x" title="81:198	Formally, for each i = 0,m,2m,3m,, first compute the sufficient statistics sprimei+1,,sprimei+m on x(i+1),,x(i+m) and then update  using sprimei+1 +  + sprimei+m. The larger the m, the less frequent the updates, but the more stable they are." ></td>
	<td class="line x" title="82:198	In this way, mini-batches interpolate between a pure online (m = 1) and a pure batch (m = n) algorithm.2 Fast implementation Due to sparsity in NLP, the sufficient statistics of an example sprimei are nonzero for a small fraction of its components." ></td>
	<td class="line x" title="83:198	For iEM, the time required to update  with sprimei depends only on the number of nonzero components of sprimei." ></td>
	<td class="line x" title="84:198	However, the sEM update is(1k)+ksprimei, and a nave implementation would take time proportional to the total number of components." ></td>
	<td class="line x" title="85:198	The key to a more efficient solution is to note that() is invariant to scaling of ." ></td>
	<td class="line x" title="86:198	Therefore, we can store S = Q j<k(1j)instead of  and make the following sparse update: S S + kQ jk(1j) sprimei, taking comfort in the fact that () = (S)." ></td>
	<td class="line x" title="87:198	For both iEM and sEM, we also need to efficiently compute ()." ></td>
	<td class="line x" title="88:198	We can do this by maintaining the normalizer for each multinomial block (sum of the components in the block)." ></td>
	<td class="line x" title="89:198	This extra maintenance only doubles the number of updates we have to make but allows us to fetch any component of() in constant time by dividing out the normalizer." ></td>
	<td class="line x" title="90:198	3.3 Incremental versus stepwise EM Incremental EM increases L monotonically after each update by virtue of doing coordinate-wise ascent and thus is guaranteed to converge to a local optimum of bothLand lscript (Neal and Hinton, 1998)." ></td>
	<td class="line x" title="91:198	However, lscript is not guaranteed to increase after each update." ></td>
	<td class="line x" title="92:198	Stepwise EM might not increase eitherLor lscript after each update, but it is guaranteed to converge to a local optimum of lscript given suitable conditions on the stepsize discussed earlier." ></td>
	<td class="line x" title="93:198	Incremental and stepwise EM actually coincide under the following setting (Cappe and Moulines, 2Note that running sEM with m = n is similar but not equivalent to batch EM since old sufficient statistics are still interpolated rather than replaced." ></td>
	<td class="line x" title="94:198	2009): If we set (,m) = (1,1) for sEM and initialize allsi = 0 for iEM, then both algorithms make the same updates on the first pass through the data." ></td>
	<td class="line x" title="95:198	They diverge thereafter as iEM subtracts out old sis, while sEM does not even remember them." ></td>
	<td class="line x" title="96:198	One weakness of iEM is that its memory requirements grow linearly with the number of examples due to storing s1,,sn." ></td>
	<td class="line x" title="97:198	For large datasets, these sis might not even fit in memory, and resorting to physical disk would be very slow." ></td>
	<td class="line x" title="98:198	In contrast, the memory usage of sEM does not depend on n. The relationship between iEM and sEM (with m = 1) is analogous to the one between exponentiated gradient (Collins et al., 2008) and stochastic gradient for supervised learning of log-linear models." ></td>
	<td class="line x" title="99:198	The former maintains the sufficient statistics of each example and subtracts out old ones whereas the latter does not." ></td>
	<td class="line x" title="100:198	In the supervised case, the added stability of exponentiated gradient tends to yield better performance." ></td>
	<td class="line x" title="101:198	For the unsupervised case, we will see empirically that remembering the old sufficient statistics offers no benefit, and much better performance can be obtained by properly setting (,m) for sEM (Section 4)." ></td>
	<td class="line x" title="102:198	4 Experiments We now present our empirical results for batch EM and online EM (iEM and sEM) on the four tasks described in Section 2: part-of-speech tagging, document classification, word segmentation (English and Chinese), and word alignment." ></td>
	<td class="line x" title="103:198	We used the following protocol for all experiments: We initialized the parameters to a neutral setting plus noise to break symmetries.3 Training was performed for 20 iterations.4 No parameter smoothing was used." ></td>
	<td class="line x" title="104:198	All runs used a fixed random seed for initializing the parameters and permuting the examples at the beginning of each iteration." ></td>
	<td class="line x" title="105:198	We report two performance metrics: log-likelihood normalized by the number of examples and the task-specific accuracy metric (see Section 2)." ></td>
	<td class="line x" title="106:198	All numbers are taken from the final iteration." ></td>
	<td class="line x" title="107:198	3Specifically, for each block of multinomial probabilities 1,,K, we set k  exp{103(1 + ak)}, where ak  U[0,1]." ></td>
	<td class="line x" title="108:198	Exception: for batch EM on POS tagging, we used 1 instead of 103; more noise worked better." ></td>
	<td class="line x" title="109:198	4Exception: for batch EM on POS tagging, 100 iterations was needed to get satisfactory performance." ></td>
	<td class="line x" title="110:198	614 Stepwise EM (sEM) requires setting two optimization parameters: the stepsize reduction power  and the mini-batch size m (see Section 3.2)." ></td>
	<td class="line x" title="111:198	As Section 4.3 will show, these two parameters can have a large impact on performance." ></td>
	<td class="line x" title="112:198	As a default rule of thumb, we chose (,m)  {0.5,0.6,0.7,0.8,0.9,1.0}  {1,3,10,30,100,300,1K,3K,10K} to maximize log-likelihood; let sEMlscript denote stepwise EM with this setting." ></td>
	<td class="line x" title="113:198	Note that this setting requires no labeled data." ></td>
	<td class="line x" title="114:198	We will also consider fixing (,m) = (1,1) (sEMi) and choosing (,m) to maximize accuracy (sEMa)." ></td>
	<td class="line x" title="115:198	In the results to follow, we first demonstrate that online EM is faster (Section 4.1) and sometimes leads to higher accuracies (Section 4.2)." ></td>
	<td class="line x" title="116:198	Next, we explore the effect of the optimization parameters (,m) (Section 4.3), briefly revisiting the connection between incremental and stepwise EM." ></td>
	<td class="line x" title="117:198	Finally, we show the stability of our results under different random seeds (Section 4.4)." ></td>
	<td class="line x" title="118:198	4.1 Speed One of the principal motivations for online EM is speed, and indeed we found this motivation to be empirically well-justified." ></td>
	<td class="line x" title="119:198	Figure 1 shows that, across all five datasets, sEMlscript converges to a solution with at least comparable log-likelihood and accuracy with respect to batch EM, but sEMlscript does it anywhere from about 2 (word alignment) to 10 (POS tagging) times faster." ></td>
	<td class="line x" title="120:198	This supports our intuition that more frequent updates lead to faster convergence." ></td>
	<td class="line x" title="121:198	At the same time, note that the other two online EM variants in Figure 1 (iEM and sEMi) are prone to catastrophic failure." ></td>
	<td class="line x" title="122:198	See Section 4.3 for further discussion on this issue." ></td>
	<td class="line x" title="123:198	4.2 Performance It is fortunate but perhaps not surprising that stepwise EM is faster than batch EM." ></td>
	<td class="line x" title="124:198	But Figure 1 also shows that, somewhat surprisingly, sEMlscript can actually converge to a solution with higher accuracy, in particular on POS tagging and document classification." ></td>
	<td class="line x" title="125:198	To further explore the accuracy-increasing potential of sEM, consider choosing (,m) to maximize accuracy (sEMa)." ></td>
	<td class="line x" title="126:198	Unlike sEMlscript, sEMa does require labeled data." ></td>
	<td class="line x" title="127:198	In practice, (,m) can be tuned EM sEMlscript sEMa lscript mlscript a ma POS 57.3 59.6 66.7 0.7 3 0.5 3 DOC 39.1 47.8 49.9 0.8 1K 0.5 3K SEG(en) 80.5 80.7 83.5 0.7 1K 1.0 100 SEG(ch) 78.2 77.2 78.1 0.6 10K 1.0 10K ALIGN 78.8 78.9 78.9 0.7 10K 0.7 10K Table 1: Accuracy of batch EM and stepwise EM, where the optimization parameters (,m) are tuned to either maximize log-likelihood (sEMlscript) or accuracy (sEMa)." ></td>
	<td class="line x" title="128:198	With an appropriate setting of (,m), stepwise EM outperforms batch EM significantly on POS tagging and document classification." ></td>
	<td class="line x" title="129:198	on a small labeled set along with any model hyperparameters." ></td>
	<td class="line x" title="130:198	Table 1 shows that sEMa improves the accuracy compared to batch EM even more than sEMlscript." ></td>
	<td class="line x" title="131:198	The result for POS is most vivid: After one iteration of batch EM, the accuracy is only at 24.0% whereas sEMa is already at 54.5%, and after two iterations, at 65.4%." ></td>
	<td class="line x" title="132:198	Not only is this orders of magnitude faster than batch EM, batch EM only reaches 57.3% after 100 iterations." ></td>
	<td class="line x" title="133:198	We get a similarly striking result for document classification, but the results for word segmentation and word alignment are more modest." ></td>
	<td class="line x" title="134:198	A full understanding of this phenomenon is left as an open problem, but we will comment on one difference between the tasks where sEM improves accuracy and the tasks where it doesnt. The former are clustering tasks (POS tagging and document classification), while the latter are structural tasks (word segmentation and word alignment)." ></td>
	<td class="line x" title="135:198	Learning of clustering models centers around probabilities over words given a latent cluster label, whereas in structural models, there are no cluster labels, and it is the combinatorial structure (the segmentations and alignments) that drives the learning." ></td>
	<td class="line x" title="136:198	Likelihood versus accuracy From Figure 1, we see that stepwise EM (sEMlscript) can outperform batch EM in both likelihood and accuracy." ></td>
	<td class="line x" title="137:198	This suggests that stepwise EM is better at avoiding local minima, perhaps leveraging its stochasticity to its advantage." ></td>
	<td class="line x" title="138:198	However, on POS tagging, tuning sEM to maximize accuracy (sEMa) results in a slower increase in likelihood: compare sEMa in Figure 2 with sEMlscript in Figure 1(a)." ></td>
	<td class="line x" title="139:198	This shouldnt surprise us too much given that likelihood and accuracy are only loosely 615 20 40 60 80iterations 0.2 0.4 0.6 0.8 1.0 accuracy 20 40 60 80iterations -9.8 -8.8 -7.8 -6.9 -5.9 log-lik eliho od EMsEM isEM  2 4 6 8 10iterations 0.2 0.4 0.6 0.8 1.0 accuracy 2 4 6 8 10iterations -9.8 -9.3 -8.8 -8.3 -7.8 log-lik eliho od EMiEM sEMisEM  (a)POStagging (b)Documentclassi cation 2 4 6 8 10iterations 0.2 0.4 0.6 0.8 1.0 F1 2 4 6 8 10iterations -4.8 -4.6 -4.4 -4.2 -4.0 log-lik eliho od EMiEM sEMisEM  2 4 6 8 10iterations 0.2 0.4 0.6 0.8 1.0 F1 2 4 6 8 10iterations -9.5 -8.9 -8.4 -7.8 -7.2 log-lik eliho od EMiEM sEMisEM  (c)Wordsegmentation(English) (d)Wordsegmentation(Chinese) 2 4 6 8 10iterations 0.2 0.4 0.6 0.8 1.0 1 AER 2 4 6 8 10iterations -10.9 -9.4 -7.9 -6.5 -5.0 log-lik eliho od EMiEM sEMisEM  accuracylog-likelihoodEMsEM  EMsEMpos 57.359.6-6.03-6.08 doc 39.147.8-7.96-7.88seg(en)80.580.7-4.11-4.11 seg(ch)78.277.2-7.27-7.28align78.878.9-5.05-5.12 (e)Wordalignment (f)Resultsafterconvergence Figure 1: Accuracy and log-likelihood plots for batch EM, incremental EM, and stepwise EM across all five datasets." ></td>
	<td class="line x" title="140:198	sEMlscript outperforms batch EM in terms of convergence speed and even accuracy and likelihood; iEM and sEMi fail in some cases." ></td>
	<td class="line x" title="141:198	We did not run iEM on POS tagging due to memory limitations; we expect the performance would be similar to sEMi, which is not very encouraging (Section 4.3)." ></td>
	<td class="line x" title="142:198	correlated (Liang and Klein, 2008)." ></td>
	<td class="line x" title="143:198	But it does suggest that stepwise EM is injecting a bias that favors accuracy over likelihooda bias not at all reflected in the training objective." ></td>
	<td class="line x" title="144:198	We can create a hybrid (sEMa+EM) that combines the strengths of both sEMa and EM: First run sEMa for 5 iterations, which quickly takes us to a part of the parameter space yielding good accuracies; then run EM, which quickly improves the likelihood." ></td>
	<td class="line x" title="145:198	Fortunately, accuracy does not degrade as likelihood increases (Figure 2)." ></td>
	<td class="line x" title="146:198	4.3 Varying the optimization parameters Recall that stepwise EM requires setting two optimization parameters: the stepsize reduction power and the mini-batch size m. We now explore the effect of (,m) on likelihood and accuracy." ></td>
	<td class="line x" title="147:198	As mentioned in Section 3.2, larger mini-batches (increasing m) stabilize parameter updates, while larger stepsizes (decreasing ) provide swifter 616 Figure 3: Effect of optimization parameters (stepsize reduction power  and mini-batch size m) on accuracy and likelihood." ></td>
	<td class="line x" title="148:198	Numerical results are shown for document classification." ></td>
	<td class="line x" title="149:198	In the interest of space, the results for each task are compressed into two gray scale images, one for accuracy (top) and one for log-likelihood (bottom), where darker shades represent larger values." ></td>
	<td class="line x" title="150:198	Bold (red) numbers denote the best  for a given m. 20 40 60 80iterations 0.2 0.4 0.6 0.8 1.0 accuracy 20 40 60 80iterations -12.7 -11.0 -9.3 -7.6 -5.9 log-lik eliho od EMsEM asEM a+EM Figure 2: sEMa quickly obtains higher accuracy than batch EM but suffers from a slower increase in likelihood." ></td>
	<td class="line x" title="151:198	The hybrid sEMa+EM (5 iterations of EMa followed by batch EM) increases both accuracy and likelihood sharply." ></td>
	<td class="line x" title="152:198	progress." ></td>
	<td class="line x" title="153:198	Remember that since we are dealing with a nonconvex objective, the choice of stepsize not only influences how fast we converge, but also the quality of the solution that we converge to." ></td>
	<td class="line x" title="154:198	Figure 3 shows the interaction between  and m in terms of likelihood and accuracy." ></td>
	<td class="line x" title="155:198	In general, the best (,m) depends on the task and dataset." ></td>
	<td class="line x" title="156:198	For example, for document classification, larger m is critical for good performance; for POS tagging, it is better to use smaller values of  and m. Fortunately, there is a range of permissible settings (corresponding to the dark regions in Figure 3) that lead to reasonable performance." ></td>
	<td class="line x" title="157:198	Furthermore, the settings that perform well on likelihood generally correspond to ones that perform well on accuracy, which justifies using sEMlscript." ></td>
	<td class="line x" title="158:198	A final observation is that as we use larger minibatches (larger m), decreasing the stepsize more gradually (smaller ) leads to better performance." ></td>
	<td class="line x" title="159:198	Intuitively, updates become more reliable with larger m, so we can afford to trust them more and incorporate them more aggressively." ></td>
	<td class="line x" title="160:198	Stepwise versus incremental EM In Section 3.2, we mentioned that incremental EM can be made equivalent to stepwise EM with  = 1 and m = 1 (sEMi)." ></td>
	<td class="line x" title="161:198	Figure 1 provides the empirical support: iEM and sEMi have very similar training curves." ></td>
	<td class="line x" title="162:198	Therefore, keeping around the old sufficient statistics does not provide any advantage and still requires a substantial storage cost." ></td>
	<td class="line x" title="163:198	As mentioned before, setting (,m) properly is crucial." ></td>
	<td class="line x" title="164:198	While we could simulate mini-batches with iEM by updating multiple coordinates simultaneously, iEM is not capable of exploiting the behavior of < 1." ></td>
	<td class="line x" title="165:198	4.4 Varying the random seed All our results thus far represent single runs with a fixed random seed." ></td>
	<td class="line x" title="166:198	We now investigate the impact of randomness on our results." ></td>
	<td class="line x" title="167:198	Recall that we use randomness for two purposes: (1) initializing the parameters (affects both batch EM and online EM), 617 accuracy log-likelihood EM sEMlscript EM sEMlscript POS 56.21.36 58.80.73,1.41 6.01 6.09 DOC 41.21.97 51.40.97,2.82 7.93 7.88 SEG(en) 80.50.0 81.00.0,0.42 4.1 4.1 SEG(ch) 78.20.0 77.20.0,0.04 7.26 7.27 ALIGN 79.00.14 78.80.14,0.25 5.04 5.11 Table 2: Mean and standard deviation over different random seeds." ></td>
	<td class="line x" title="168:198	For EM and sEM, the first number after  is the standard deviation due to different initializations of the parameters." ></td>
	<td class="line x" title="169:198	For sEM, the second number is the standard deviation due to different permutations of the examples." ></td>
	<td class="line x" title="170:198	Standard deviation for log-likelihoods are all < 0.01 and therefore left out due to lack of space." ></td>
	<td class="line x" title="171:198	and (2) permuting the examples at the beginning of each iteration (affects only online EM)." ></td>
	<td class="line x" title="172:198	To separate these two purposes, we used two different seeds, Si  {1,2,3,4,5} and Sp  {1,2,3,4,5}for initializing and permuting, respectively." ></td>
	<td class="line x" title="173:198	Let X be a random variable denoting either log-likelihood or accuracy." ></td>
	<td class="line x" title="174:198	We define the variance due to initialization as var(E(X |Si)) (E averages over Sp for each fixed Si) and the variance due to permutation as E(var(X|Si)) (E averages overSi)." ></td>
	<td class="line x" title="175:198	These two variances provide an additive decomposition of the total variance: var(X) = var(E(X | Si)) +E(var(X|Si))." ></td>
	<td class="line x" title="176:198	Table 2 summarizes the results across the 5 trials for EM and 25 for sEMlscript." ></td>
	<td class="line x" title="177:198	Since we used a very small amount of noise to initialize the parameters, the variance due to initialization is systematically smaller than the variance due to permutation." ></td>
	<td class="line x" title="178:198	sEMlscript is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples." ></td>
	<td class="line x" title="179:198	Overall, the accuracy of sEMlscript is more variable than that of EM, but not by a large amount." ></td>
	<td class="line x" title="180:198	5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years." ></td>
	<td class="line x" title="181:198	One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few." ></td>
	<td class="line x" title="182:198	Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008)." ></td>
	<td class="line x" title="183:198	However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008)." ></td>
	<td class="line x" title="184:198	Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima." ></td>
	<td class="line x" title="185:198	Stepwise EM provides finer control via its optimization parameters and has proven quite successful." ></td>
	<td class="line x" title="186:198	One family of methods that resembles incremental EM includes collapsed samplers for Bayesian modelsfor example, Goldwater et al.(2006) and Goldwater and Griffiths (2007)." ></td>
	<td class="line x" title="188:198	These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM." ></td>
	<td class="line x" title="189:198	In contrast, stepwise EM does not require this storage and operates more in the spirit of a truly online algorithm." ></td>
	<td class="line x" title="190:198	Besides speed, online algorithms are of interest for two additional reasons." ></td>
	<td class="line x" title="191:198	First, in some applications, we receive examples sequentially and would like to estimate a model in real-time, e.g., in the clustering of news articles." ></td>
	<td class="line x" title="192:198	Second, since humans learn sequentially, studying online EM might suggest new connections to cognitive mechanisms." ></td>
	<td class="line x" title="193:198	6 Conclusion We have explored online EM on four tasks and demonstrated how to use stepwise EM to overcome the dangers of stochasticity and reap the benefits of frequent updates and fast learning." ></td>
	<td class="line x" title="194:198	We also discovered that stepwise EM can actually improve accuracy, a phenomenon worthy of further investigation." ></td>
	<td class="line x" title="195:198	This paper makes some progress on elucidating the properties of online EM." ></td>
	<td class="line x" title="196:198	With this increased understanding, online EM, like its batch cousin, could become a mainstay for unsupervised learning." ></td>
	<td class="line x" title="197:198	5Other types of learning methods have been employed successfully, for example, Venkataraman (2001) and Seginer (2007)." ></td>
	<td class="line x" title="198:198	618" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1056
Distributional Representations for Handling Sparsity in Supervised Sequence-Labeling
Huang, Fei;Yates, Alexander;"></td>
	<td class="line x" title="1:216	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 495503, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:216	c2009 ACL and AFNLP Distributional Representations for Handling Sparsity in Supervised Sequence-Labeling Fei Huang Temple University 1805 N. Broad St. Wachman Hall 324 tub58431@temple.edu Alexander Yates Temple University 1805 N. Broad St. Wachman Hall 324 yates@temple.edu Abstract Supervised sequence-labeling systems in natural language processing often suffer from data sparsity because they use word types as features in their prediction tasks." ></td>
	<td class="line x" title="3:216	Consequently, they have difficulty estimating parameters for types which appear in the test set, but seldom (or never) appear in the training set." ></td>
	<td class="line x" title="4:216	We demonstrate that distributional representations of word types, trained on unannotated text, can be used to improve performance on rare words." ></td>
	<td class="line x" title="5:216	We incorporate aspects of these representations into the feature space of our sequence-labeling systems." ></td>
	<td class="line x" title="6:216	In an experiment on a standard chunking dataset, our best technique improves a chunker from 0.76 F1 to 0.86 F1 on chunks beginning with rare words." ></td>
	<td class="line x" title="7:216	On the same dataset, it improves our part-of-speech tagger from 74% to 80% accuracy on rare words." ></td>
	<td class="line x" title="8:216	Furthermore, our system improves significantly over a baseline system when applied to text from a different domain, and it reduces the sample complexity of sequence labeling." ></td>
	<td class="line x" title="9:216	1 Introduction Data sparsity and high dimensionality are the twin curses of statistical natural language processing (NLP)." ></td>
	<td class="line x" title="10:216	In many traditional supervised NLP systems, the feature space includes dimensions for each word type in the data, or perhaps even combinations of word types." ></td>
	<td class="line x" title="11:216	Since vocabularies can be extremely large, this leads to an explosion in the number of parameters." ></td>
	<td class="line x" title="12:216	To make matters worse, language is Zipf-distributed, so that a large fraction of any training data set will be hapax legomena, very many word types will appear only a few times, and many word types will be left out of the training set altogether." ></td>
	<td class="line x" title="13:216	As a consequence, for many word types supervised NLP systems have very few, or even zero, labeled examples from which to estimate parameters." ></td>
	<td class="line x" title="14:216	The negative effects of data sparsity have been well-documented in the NLP literature." ></td>
	<td class="line x" title="15:216	The performance of state-of-the-art, supervised NLP systems like part-of-speech (POS) taggers degrades significantly on words that do not appear in the training data, or out-of-vocabulary (OOV) words (Lafferty et al., 2001)." ></td>
	<td class="line x" title="16:216	Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth, rare words) (Blitzer et al., 2006; Daume III and Marcu, 2006; Chelba and Acero, 2004)." ></td>
	<td class="line x" title="17:216	We investigate the use of distributional representations, which model the probability distribution of a words context, as techniques for finding smoothed representations of word sequences." ></td>
	<td class="line x" title="18:216	That is, we use the distributional representations to share information across unannotated examples of the same word type." ></td>
	<td class="line x" title="19:216	We then compute features of the distributional representations, and provide them as input to our supervised sequence labelers." ></td>
	<td class="line x" title="20:216	Our technique is particularly well-suited to handling data sparsity because it is possible to improve performance on rare words by supplementing the training data with additional unannotated text containing more examples of the rare words." ></td>
	<td class="line x" title="21:216	We provide empirical evidence that shows how distributional representations improve sequencelabeling in the face of data sparsity." ></td>
	<td class="line x" title="22:216	Specifically, we investigate empirically the effects of our smoothing techniques on two sequence-labeling tasks, POS tagging and chunking, to answer the following: 1." ></td>
	<td class="line x" title="23:216	What is the effect of smoothing on sequencelabeling accuracy for rare word types?" ></td>
	<td class="line x" title="24:216	Our best smoothing technique improves a POS tagger by 11% on OOV words, and a chunker by an impressive 21% on OOV words." ></td>
	<td class="line x" title="25:216	495 2." ></td>
	<td class="line x" title="26:216	Can smoothing improve adaptability to new domains?" ></td>
	<td class="line x" title="27:216	After training our chunker on newswire text, we apply it to biomedical texts." ></td>
	<td class="line x" title="28:216	Remarkably, we find that the smoothed chunker achieves a higher F1 on the new domain than the baseline chunker achieves on a test set from the original newswire domain." ></td>
	<td class="line x" title="29:216	3." ></td>
	<td class="line x" title="30:216	How does our smoothing technique affect sample complexity?" ></td>
	<td class="line x" title="31:216	We show that smoothing drastically reduces sample complexity: our smoothed chunker requires under 100 labeled samples to reach 85% accuracy, whereas the unsmoothed chunker requires 3500 samples to reach the same level of performance." ></td>
	<td class="line x" title="32:216	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="33:216	Section 2 discusses the smoothing problem for word sequences, and introduces three smoothing techniques." ></td>
	<td class="line x" title="34:216	Section 3 presents our empirical study of the effects of smoothing on two sequencelabeling tasks." ></td>
	<td class="line x" title="35:216	Section 4 describes related work, and Section 5 concludes and suggests items for future work." ></td>
	<td class="line x" title="36:216	2 Smoothing Natural Language Sequences To smooth a dataset is to find an approximation of it that retains the important patterns of the original data while hiding the noise or other complicating factors." ></td>
	<td class="line x" title="37:216	Formally, we define the smoothing task as follows: let D = {(x,z)|x is a word sequence, z is a label sequence} be a labeled dataset of word sequences, and let M be a machine learning algorithm that will learn a function f to predict the correct labels." ></td>
	<td class="line x" title="38:216	The smoothing task is to find a function g such that when M is applied to Dprime = {(g(x),z)|(x,z)  D}, it produces a function fprime that is more accurate than f. For supervised sequence-labeling problems in NLP, the most important complicating factor that we seek to avoid through smoothing is the data sparsity associated with word-based representations." ></td>
	<td class="line x" title="39:216	Thus, the task is to find g such that for every word x, g(x) is much less sparse, but still retains the essential features of x that are useful for predicting its label." ></td>
	<td class="line x" title="40:216	As an example, consider the string Researchers test reformulated gasolines on newer engines. In a common dataset for NP chunking, the word reformulated never appears in the training data, but appears four times in the test set as part of the NP reformulated gasolines. Thus, a learning algorithm supplied with word-level features would have a difficult time determining that reformulated is the start of a NP." ></td>
	<td class="line x" title="41:216	Character-level features are of little help as well, since the -ed suffix is more commonly associated with verb phrases." ></td>
	<td class="line x" title="42:216	Finally, context may be of some help, but test is ambiguous between a noun and verb, and gasolines is only seen once in the training data, so there is no guarantee that context is sufficient to make a correct judgment." ></td>
	<td class="line x" title="43:216	On the other hand, some of the other contexts in which reformulated appears in the test set, such as testing of reformulated gasolines, provide strong evidence that it can start a NP, since of is a highly reliable indicator that a NP is to follow." ></td>
	<td class="line x" title="44:216	This example provides the intuition for our approach to smoothing: we seek to share information about the contexts of a word across multiple instances of the word, in order to provide more information about words that are rarely or never seen in training." ></td>
	<td class="line x" title="45:216	In particular, we seek to represent each word by a distribution over its contexts, and then provide the learning algorithm with features computed from this distribution." ></td>
	<td class="line x" title="46:216	Importantly, we seek distributional representations that will provide features that are common in both training and test data, to avoid data sparsity." ></td>
	<td class="line x" title="47:216	In the next three sections, we develop three techniques for smoothing text using distributional representations." ></td>
	<td class="line x" title="48:216	2.1 Multinomial Representation In its simplest form, the context of a word may be represented as a multinomial distribution over the terms that appear on either side of the word." ></td>
	<td class="line x" title="49:216	If V is the vocabulary, or the set of word types, and X is a sequence of random variables over V, the left and right context of Xi = v may each be represented as a probability distribution over V: P(Xi1|Xi = v) and P(Xi+1|X = v) respectively." ></td>
	<td class="line x" title="50:216	We learn these distributions from unlabeled texts in two different ways." ></td>
	<td class="line x" title="51:216	The first method computes word count vectors for the left and right contexts of each word type in the vocabulary of the training and test texts." ></td>
	<td class="line x" title="52:216	We also use a large collection of additional text to determine the vectors." ></td>
	<td class="line x" title="53:216	We then normalize each vector to form a probability distribution." ></td>
	<td class="line x" title="54:216	The second technique first applies TF-IDF weighting to each vector, where the context words of each word type constitute a document, before applying normalization." ></td>
	<td class="line x" title="55:216	This gives greater weight to words with more idiosyncratic distributions and may improve the informativeness of a distributional representation." ></td>
	<td class="line x" title="56:216	We refer to these techniques as TF and TF-IDF." ></td>
	<td class="line x" title="57:216	496 To supply a sequence-labeling algorithm with information from these distributional representations, we compute real-valued features of the context distributions." ></td>
	<td class="line x" title="58:216	In particular, for every word xi in a sequence, we provide the sequence labeler with a set of features of the left and right contexts indexed by v  V: Fleftv (xi) = P(Xi1 = v|xi) and Frightv (xi) = P(Xi+1 = v|xi)." ></td>
	<td class="line x" title="59:216	For example, the left context for reformulated in our example above would contain a nonzero probability for the word of. Using the features F(xi), a sequence labeler can learn patterns such as, if xi has a high probability of following of, it is a good candidate for the start of a noun phrase." ></td>
	<td class="line x" title="60:216	These features provide smoothing by aggregating information across multiple unannotated examples of the same word." ></td>
	<td class="line x" title="61:216	2.2 LSA Model One drawback of the multinomial representation is that it does not handle sparsity well enough, because the multinomial distributions themselves are so high-dimensional." ></td>
	<td class="line x" title="62:216	For example, the two phrases red lamp and magenta tablecloth share no words in common." ></td>
	<td class="line x" title="63:216	If magenta is never observed in training, the fact that tablecloth appears in its right context is of no help in connecting it with the phrase red lamp. But if we can group similar context words together, putting lamp and tablecloth into a category for household items, say, then these two adjectives will share that category in their context distributions." ></td>
	<td class="line x" title="64:216	Any patterns learned for the more common red lamp will then also apply to the less common magenta tablecloth. Our second distributional representation aggregates information from multiple context words by grouping together the distributions P(xi1 = v|xi = w) and P(xi1 = vprime|xi = w) if v and vprime appear together with many of the same words w. Aggregating counts in this way smooths our representations even further, by supplying better estimates when the data is too sparse to estimate P(xi1|xi) accurately." ></td>
	<td class="line x" title="65:216	Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a widely-used technique for computing dimensionality-reduced representations from a bag-of-words model." ></td>
	<td class="line x" title="66:216	We apply LSA to the set of right context vectors and the set of left context vectors separately, to find compact versions of each vector, where each dimension represents a combination of several context word types." ></td>
	<td class="line x" title="67:216	We normalize each vector, and then calculate features as above." ></td>
	<td class="line x" title="68:216	After experimenting with different choices for the number of dimensions to reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes the performance of our supervised sequence labelers on held-out data." ></td>
	<td class="line x" title="69:216	2.3 Latent Variable Language Model Representation To take smoothing one step further, we present a technique that aggregates context distributions both for similar context words xi1 = v and vprime, and for similar words xi = w and wprime." ></td>
	<td class="line x" title="70:216	Latent variable language models (LVLMs) can be used to produce just such a distributional representation." ></td>
	<td class="line x" title="71:216	We use Hidden Markov Models (HMMs) as the main example in the discussion and as the LVLMs in our experiments, but the smoothing technique can be generalized to other forms of LVLMs, such as factorial HMMs and latent variable maximum entropy models (Ghahramani and Jordan, 1997; Smith and Eisner, 2005)." ></td>
	<td class="line x" title="72:216	An HMM is a generative probabilistic model that generates each word xi in the corpus conditioned on a latent variable Yi." ></td>
	<td class="line x" title="73:216	Each Yi in the model takes on integral values from 1 to S, and each one is generated by the latent variable for the preceding word, Yi1." ></td>
	<td class="line x" title="74:216	The distribution for a corpus x = (x1,,xN) given a set of state vectors y = (y1,,yN) is given by: P(x|y) = productdisplay i P(xi|yi)P(yi|yi1) Using Expectation-Maximization (Dempster et al., 1977), it is possible to estimate the distributions for P(xi|yi) and P(yi|yi1) from unlabeled data." ></td>
	<td class="line x" title="75:216	We use a trained HMM to determine the optimal sequence of latent states yi using the wellknown Viterbi algorithm (Rabiner, 1989)." ></td>
	<td class="line x" title="76:216	The output of this process is an integer (ranging from 1 to S) for every word xi in the corpus; we include a new boolean feature for each possible value of yi in our sequence labelers." ></td>
	<td class="line x" title="77:216	To compare our models, note that in the multinomial representation we directly model the probability that a word v appears before a word w: P(xi1 = v|xi = w))." ></td>
	<td class="line x" title="78:216	In our LSA model, we find latent categories of context words z, and model the probability that a category appears before the current word w: P(xi1 = z|xi = w)." ></td>
	<td class="line x" title="79:216	The HMM finds (probabilistic) categories Y for both the current word xi and the context word xi1, and models the probability that one category follows the 497 other: P(Yi|Yi1)." ></td>
	<td class="line x" title="80:216	Thus the HMM is our most extreme smoothing model, as it aggregates information over the greatest number of examples: for a given consecutive pair of words xi1,xi in the test set, it aggregates over all pairs of consecutive words xprimei1,xprimei where xprimei1 is similar to xi1 and xprimei is similar to xi." ></td>
	<td class="line x" title="81:216	3 Experiments We tested the following hypotheses in our experiments: 1." ></td>
	<td class="line x" title="82:216	Smoothing can improve the performance of a supervised sequence labeling system on words that are rare or nonexistent in the training data." ></td>
	<td class="line x" title="83:216	2." ></td>
	<td class="line x" title="84:216	A supervised sequence labeler achieves greater accuracy on new domains with smoothing." ></td>
	<td class="line x" title="85:216	3." ></td>
	<td class="line x" title="86:216	A supervised sequence labeler has a better sample complexity with smoothing." ></td>
	<td class="line x" title="87:216	3.1 Experimental Setup We investigate the use of smoothing in two test systems, conditional random field (CRF) models for POS tagging and chunking." ></td>
	<td class="line x" title="88:216	To incorporate smoothing into our models, we follow the following general procedure: first, we collect a set of unannotated text from the same domain as the test data set." ></td>
	<td class="line x" title="89:216	Second, we train a smoothing model on the text of the training data, the test data, and the additional collection." ></td>
	<td class="line x" title="90:216	We then automatically annotate both the training and test data with features calculated from the distributional representation." ></td>
	<td class="line x" title="91:216	Finally, we train the CRF model on the annotated training set and apply it to the test set." ></td>
	<td class="line x" title="92:216	We use an open source CRF software package designed by Sunita Sajarwal and William W. Cohen to implement our CRF models.1 We use a set of boolean features listed in Table 1." ></td>
	<td class="line x" title="93:216	Our baseline CRF system for POS tagging follows the model described by Lafferty et al.(2001)." ></td>
	<td class="line x" title="94:216	We include transition features between pairs of consecutive tag variables, features between tag variables and words, and a set of orthographic features that Lafferty et al. found helpful for performance on OOV words." ></td>
	<td class="line x" title="95:216	Our smoothed models add features computed from the distributional representations, as discussed above." ></td>
	<td class="line x" title="96:216	Our chunker follows the system described by Sha and Pereira (2003)." ></td>
	<td class="line x" title="97:216	In addition to the transition, word-level, and orthographic features, we include features relating automatically-generated POS tags and the chunk labels." ></td>
	<td class="line x" title="98:216	Unlike Sha and 1Available from http://sourceforge.net/projects/crf/ CRF Feature Set Transition zi=z zi=z and zi1=zprime Word xi=w and zi=z POS ti=t and zi=z Orthography for every s  {-ing, -ogy, ed, -s, -ly, -ion, -tion, -ity}, suffix(xi)= s and zi=z xi is capitalized and zi = z xi has a digit and zi = z TF, TF-IDF, and LSA features for every context type v, Fleftv (xi) and Frightv (xi) HMM features yi=y and zi = z Table 1: Features used in our CRF systems." ></td>
	<td class="line x" title="99:216	zi variables represent labels to be predicted, ti represent tags (for the chunker), and xi represent word tokens." ></td>
	<td class="line x" title="100:216	All features are boolean except for the TF, TF-IDF, and LSA features." ></td>
	<td class="line x" title="101:216	Pereira, we exclude features relating consecutive pairs of words and a chunk label, or features relating consecutive tag labels and a chunk label, in order to expedite our experiments." ></td>
	<td class="line x" title="102:216	We found that including such features does improve chunking F1 by approximately 2%, but it also significantly slows down CRF training." ></td>
	<td class="line x" title="103:216	3.2 Rare Word Accuracy For these experiments, we use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993)." ></td>
	<td class="line x" title="104:216	Following the CoNLL shared task from 2000, we use sections 15-18 of the Penn Treebank for our labeled training data for the supervised sequence labeler in all experiments (Tjong et al., 2000)." ></td>
	<td class="line x" title="105:216	For the tagging experiments, we train and test using the gold standard POS tags contained in the Penn Treebank." ></td>
	<td class="line x" title="106:216	For the chunking experiments, we train and test with POS tags that are automatically generated by a standard tagger (Brill, 1994)." ></td>
	<td class="line x" title="107:216	We tested the accuracy of our models for chunking and POS tagging on section 20 of the Penn Treebank, which corresponds to the test set from the CoNLL 2000 task." ></td>
	<td class="line x" title="108:216	Our distributional representations are trained on sections 2-22 of the Penn Treebank." ></td>
	<td class="line x" title="109:216	Because we include the text from the train and test sets in our training data for the distributional representations, we do not need to worry about smoothing them  when they are decoded on the test set, they 498 Freq: 0 1 2 0-2 all #Samples 438 508 588 1534 46661 Baseline .62 .77 .81 .74 .93 TF .76 .72 .77 .75 .92 TF-IDF .82 .75 .76 .78 .94 LSA .78 .80 .77 .78 .94 HMM .73 .81 .86 .80 .94 Table 2: POS tagging accuracy: our HMM-smoothed tagger outperforms the baseline tagger by 6% on rare words." ></td>
	<td class="line x" title="110:216	Differences between the baseline and the HMM are statistically significant at p < 0.01 for the OOV, 0-2, and all cases using the two-tailed Chi-squared test with 1 degree of freedom." ></td>
	<td class="line x" title="111:216	will not encounter any previously unseen words." ></td>
	<td class="line x" title="112:216	However, to speed up training during our experiments and, in some cases, to avoid running out of memory, we replaced words appearing twice or fewer times in the data with the special symbol *UNKNOWN*." ></td>
	<td class="line x" title="113:216	In addition, all numbers were replaced with another special symbol." ></td>
	<td class="line x" title="114:216	For the LSA model, we had to use a more drastic cutoff to fit the singular value decomposition computation into memory: we replaced words appearing 10 times or fewer with the *UNKNOWN* symbol." ></td>
	<td class="line x" title="115:216	We initialize our HMMs randomly." ></td>
	<td class="line x" title="116:216	We run EM ten times and take the model with the best cross-entropy on a held-out set." ></td>
	<td class="line x" title="117:216	After experimenting with different variations of HMM models, we settled on a model with 80 latent states as a good compromise between accuracy and efficiency." ></td>
	<td class="line x" title="118:216	For our POS tagging experiments, we measured the accuracy of the tagger on rare words, or words that appear at most twice in the training data." ></td>
	<td class="line x" title="119:216	For our chunking experiments, we focus on chunks that begin with rare words, as we found that those were the most difficult for the chunker to identify correctly." ></td>
	<td class="line x" title="120:216	So we define rare chunks as those that begin with words appearing at most twice in training data." ></td>
	<td class="line x" title="121:216	To ensure that our smoothing models have enough training data for our test set, we further narrow our focus to those words that appear rarely in the labeled training data, but appear at least ten times in sections 2-22." ></td>
	<td class="line x" title="122:216	Tables 2 and 3 show the accuracy of our smoothed models and the baseline model on tagging and chunking, respectively." ></td>
	<td class="line x" title="123:216	The line for all in both tables indicates results on the complete test set." ></td>
	<td class="line x" title="124:216	Both our baseline tagger and chunker achieve respectable results on their respective tasks for all words, and the results were good enough for Freq: 0 1 2 0-2 all #Samples 133 199 231 563 21900 Baseline .69 .75 .81 .76 .90 TF .70 .82 .79 .77 .89 TF-IDF .77 .77 .80 .78 .90 LSA .84 .82 .83 .84 .90 HMM .90 .85 .85 .86 .93 Table 3: Chunking F1: our HMM-smoothed chunker outperforms the baseline CRF chunker by 0.21 on chunks that begin with OOV words, and 0.10 on chunks that begin with rare words." ></td>
	<td class="line x" title="125:216	us to be satisfied that performance on rare words closely follows how a state-of-the-art supervised sequence-labeler behaves." ></td>
	<td class="line x" title="126:216	The chunkers accuracy is roughly in the middle of the range of results for the original CoNLL 2000 shared task (Tjong et al., 2000) . While several systems have achieved slightly higher accuracy on supervised POS tagging, they are usually trained on larger training sets." ></td>
	<td class="line x" title="127:216	As expected, the drop-off in the baseline systems performance from all words to rare words is impressive for both tasks." ></td>
	<td class="line x" title="128:216	Comparing performance on all terms and OOV terms, the baseline taggers accuracy drops by 0.31, and the baseline chunkers F1 drops by 0.21." ></td>
	<td class="line x" title="129:216	Comparing performance on all terms and rare terms, the drop is less severe but still dramatic: 0.19 for tagging and 0.15 for chunking." ></td>
	<td class="line x" title="130:216	Our hypothesis that smoothing would improve performance on rare terms is validated by these experiments." ></td>
	<td class="line x" title="131:216	In fact, the more aggregation a smoothing model performs, the better it appears to be at smoothing." ></td>
	<td class="line x" title="132:216	The HMM-smoothed system outperforms all other systems in all categories except tagging on OOV words, where TF-IDF performs best." ></td>
	<td class="line x" title="133:216	And in most cases, the clear trend is for HMM smoothing to outperform LSA, which in turn outperforms TF and TF-IDF." ></td>
	<td class="line x" title="134:216	HMM tagging performance on OOV terms improves by 11%, and chunking performance by 21%." ></td>
	<td class="line x" title="135:216	Tagging performance on all of the rare terms improves by 6%, and chunking by 10%." ></td>
	<td class="line x" title="136:216	In chunking, there is a clear trend toward larger increases in performance as words become rarer in the labeled data set, from a 0.02 improvement on words of frequency 2, to an improvement of 0.21 on OOV words." ></td>
	<td class="line x" title="137:216	Because the test data for this experiment is drawn from the same domain (newswire) as the 499 training data, the rare terms make up a relatively small portion of the overall dataset (approximately 4% of both the tagged words and the chunks)." ></td>
	<td class="line x" title="138:216	Still, the increased performance by the HMMsmoothed model on the rare-word subset contributes in part to an increase in performance on the overall dataset of 1% for tagging and 3% for chunking." ></td>
	<td class="line x" title="139:216	In our next experiment, we consider a common scenario where rare terms make up a much larger fraction of the test data." ></td>
	<td class="line x" title="140:216	3.3 Domain Adaptation For our experiment on domain adaptation, we focus on NP chunking and POS tagging, and we use the labeled training data from the CoNLL 2000 shared task as before." ></td>
	<td class="line x" title="141:216	For NP chunking, we use 198 sentences from the biochemistry domain in the Open American National Corpus (OANC) (Reppen et al., 2005) as or our test set." ></td>
	<td class="line x" title="142:216	We manually tagged the test set with POS tags and NP chunk boundaries." ></td>
	<td class="line x" title="143:216	The test set contains 5330 words and a total of 1258 NP chunks." ></td>
	<td class="line x" title="144:216	We used sections 15-18 of the Penn Treebank as our labeled training set, including the gold standard POS tags." ></td>
	<td class="line x" title="145:216	We use our best-performing smoothing model, the HMM, and train it on sections 13 through 19 of the Penn Treebank, plus the written portion of the OANC that contains journal articles from biochemistry (40,727 sentences)." ></td>
	<td class="line x" title="146:216	We focus on chunks that begin with words appearing 0-2 times in the labeled training data, and appearing at least ten times in the HMMs training data." ></td>
	<td class="line x" title="147:216	Table 4 contains our results." ></td>
	<td class="line x" title="148:216	For our POS tagging experiments, we use 561 MEDLINE sentences (9576 words) from the Penn BioIE project (PennBioIE, 2005), a test set previously used by Blitzer et al.(2006)." ></td>
	<td class="line x" title="149:216	We use the same experimental setup as Blitzer et al.: 40,000 manually tagged sentences from the Penn Treebank for our labeled training data, and all of the unlabeled text from the Penn Treebank plus their MEDLINE corpus of 71,306 sentences to train our HMM." ></td>
	<td class="line x" title="150:216	We report on tagging accuracy for all words and OOV words in Table 5." ></td>
	<td class="line x" title="151:216	This table also includes results for two previous systems as reported by Blitzer et al.(2006): the semi-supervised Alternating Structural Optimization (ASO) technique and the Structural Correspondence Learning (SCL) technique for domain adaptation." ></td>
	<td class="line x" title="153:216	Note that this test set for NP chunking contains a much higher proportion of rare and OOV words: 23% of chunks begin with an OOV word, and 29% begin with a rare word, as compared with Baseline HMM Freq." ></td>
	<td class="line x" title="154:216	# R P F1 R P F1 0 284 .74 .70 .72 .80 .89 .84 1 39 .85 .87 .86 .92 .88 .90 2 39 .79 .86 .83 .92 .90 .91 0-2 362 .75 .73 .74 .82 .89 .85 all 1258 .86 .87 .86 .91 .90 .91 Table 4: On biochemistry journal data from the OANC, our HMM-smoothed NP chunker outperforms the baseline CRF chunker by 0.12 (F1) on chunks that begin with OOV words, and by 0.05 (F1) on all chunks." ></td>
	<td class="line x" title="155:216	Results in bold are statistically significantly different from the baseline results at p < 0.05 using the two-tailed Fishers exact test." ></td>
	<td class="line x" title="156:216	We did not perform significance tests for F1." ></td>
	<td class="line x" title="157:216	All Unknown Model words words Baseline 88.3 67.3 ASO 88.4 70.9 SCL 88.9 72.0 HMM 90.5 75.2 Table 5: On biomedical data from the Penn BioIE project, our HMM-smoothed tagger outperforms the SCL tagger by 3% (accuracy) on OOV words, and by 1.6% (accuracy) on all words." ></td>
	<td class="line x" title="158:216	Differences between the smoothed tagger and the SCL tagger are significant at p < .001 for all words and for OOV words, using the Chi-squared test with 1 degree of freedom." ></td>
	<td class="line x" title="159:216	1% and 4%, respectively, for NP chunks in the test set from the original domain." ></td>
	<td class="line x" title="160:216	The test set for tagging also contains a much higher proportion: 23% OOV words, as compared with 1% in the original domain." ></td>
	<td class="line x" title="161:216	Because of the increase in the number of rare words, the baseline chunkers overall performance drops by 4% compared with performance on WSJ data, and the baseline taggers overall performance drops by 5% in the new domain." ></td>
	<td class="line x" title="162:216	The performance improvements for both the smoothed NP chunker and tagger are again impressive: there is a 12% improvement on OOV words, and a 10% overall improvement on rare words for chunking; the tagger shows an 8% improvement on OOV words compared to out baseline and a 3% improvement on OOV words compared to the SCL model." ></td>
	<td class="line x" title="163:216	The resulting performance of the smoothed NP chunker is almost identical to its performance on the WSJ data." ></td>
	<td class="line x" title="164:216	Through smoothing, the chunker not only improves by 5% 500 in F1 over the baseline system on all words, it in fact outperforms our baseline NP chunker on the WSJ data." ></td>
	<td class="line x" title="165:216	60% of this improvement comes from improved accuracy on rare words." ></td>
	<td class="line x" title="166:216	The performance of our HMM-smoothed chunker caused us to wonder how well the chunker could work without some of its other features." ></td>
	<td class="line x" title="167:216	We removed all tag features and all features for word types that appear fewer than 20 times in training." ></td>
	<td class="line x" title="168:216	This chunker achieves 0.91 F1 on OANC data, and 0.93 F1 on WSJ data, outperforming the baseline system in both cases." ></td>
	<td class="line x" title="169:216	It has only 20% as many features as the baseline chunker, greatly improving its training time." ></td>
	<td class="line x" title="170:216	Thus our smoothing features are more valuable to the chunker than features from POS tags and features for all but the most common words." ></td>
	<td class="line x" title="171:216	Our results point to the exciting possibility that with smoothing, we may be able to train a sequence-labeling system on a small labeled sample, and have it apply generally to other domains." ></td>
	<td class="line x" title="172:216	Exactly what size training set we need is a question that we address next." ></td>
	<td class="line x" title="173:216	3.4 Sample Complexity Our complete system consists of two learned components, a supervised CRF system and an unsupervised smoothing model." ></td>
	<td class="line x" title="174:216	We measure the sample complexity of each component separately." ></td>
	<td class="line x" title="175:216	To measure the sample complexity of the supervised CRF, we use the same experimental setup as in the chunking experiment on WSJ text, but we vary the amount of labeled data available to the CRF." ></td>
	<td class="line x" title="176:216	We take ten random samples of a fixed size from the labeled training set, train a chunking model on each subset, and graph the F1 on the labeled test set, averaged over the ten runs, in Figure 1." ></td>
	<td class="line x" title="177:216	To measure the sample complexity of our HMM with respect to unlabeled text, we use the full labeled training set and vary the amount of unlabeled text available to the HMM." ></td>
	<td class="line x" title="178:216	At minimum, we use the text available in the labeled training and test sets, and then add random subsets of the Penn Treebank, sections 2-22." ></td>
	<td class="line x" title="179:216	For each subset size, we take ten random samples of the unlabeled text, train an HMM and then a chunking model, and graph the F1 on the labeled test set averaged over the ten runs in Figure 2." ></td>
	<td class="line x" title="180:216	The results from our labeled sample complexity experiment indicate that sample complexity is drastically reduced by HMM smoothing." ></td>
	<td class="line x" title="181:216	On rare chunks, the smoothed system reaches 0.78 F1 using only 87 labeled training sentences, a level that the baseline system never reaches, even with 6933 baseline (all) HMM (all) HMM (rare) 0.6 0.7 0.8 0.9 1 F1  (C hu nk ing ) Labeled Sample Complexity baseline (rare) 0.2 0.3 0.4 0.5 1 10 100 1000 10000 F1  (C hu nk ing ) Number of Labeled Sentences (log scale) Figure 1: The smoothed NP chunker requires less than 10% of the samples needed by the baseline chunker to achieve .83 F1, and the same for .88 F1." ></td>
	<td class="line x" title="182:216	Baseline (all) HMM (all) HMM (rare) 0.80 0.85 0.90 0.95 F1  (C hu nk ing ) Unlabeled Sample Complexity Baseline (rare)0.70 0.75 0 10000 20000 30000 40000 F1  (C hu nk ing ) Number of Unannotated Sentences Figure 2: By leveraging plentiful unannotated text, the smoothed chunker soon outperforms the baseline." ></td>
	<td class="line x" title="183:216	labeled sentences." ></td>
	<td class="line x" title="184:216	On the overall data set, the smoothed system reaches 0.83 F1 with 50 labeled sentences, which the baseline does not reach until it has 867 labeled sentences." ></td>
	<td class="line x" title="185:216	With 434 labeled sentences, the smoothed system reaches 0.88 F1, which the baseline system does not reach until it has 5200 labeled samples." ></td>
	<td class="line x" title="186:216	Our unlabeled sample complexity results show that even with access to a small amount of unlabeled text, 6000 sentences more than what appears in the training and test sets, smoothing using the HMM yields 0.78 F1 on rare chunks." ></td>
	<td class="line x" title="187:216	However, the smoothed system requires 25,000 more sentences before it outperforms the baseline system on all chunks." ></td>
	<td class="line x" title="188:216	No peak in performance is reached, so further improvements are possible with more unlabeled data." ></td>
	<td class="line x" title="189:216	Thus smoothing is optimizing performance for the case where unlabeled data is plentiful and labeled data is scarce, as we would hope." ></td>
	<td class="line x" title="190:216	4 Related Work To our knowledge, only one previous system  the REALM system for sparse information extrac501 tion  has used HMMs as a feature representation for other applications." ></td>
	<td class="line x" title="191:216	REALM uses an HMM trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007)." ></td>
	<td class="line x" title="192:216	We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance." ></td>
	<td class="line x" title="193:216	Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data." ></td>
	<td class="line x" title="194:216	Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005)." ></td>
	<td class="line xc" title="195:216	Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress." ></td>
	<td class="line x" title="196:216	Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity." ></td>
	<td class="line oc" title="197:216	HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004)." ></td>
	<td class="line x" title="198:216	We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that tokens contexts." ></td>
	<td class="line x" title="199:216	Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories." ></td>
	<td class="line x" title="200:216	Smoothing in NLP usually refers to the problem of smoothing n-gram models." ></td>
	<td class="line x" title="201:216	Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity." ></td>
	<td class="line x" title="202:216	Our task differs in that we are primarily concerned with the case where even the unigram model (single word) is rarely or never observed in the labeled training data." ></td>
	<td class="line x" title="203:216	Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models." ></td>
	<td class="line x" title="204:216	While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variable probabilistic models that rival and even surpass n-gram models in accuracy (Blitzer et al., 2005; Mnih and Hinton, 2007)." ></td>
	<td class="line x" title="205:216	Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005)." ></td>
	<td class="line x" title="206:216	One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently." ></td>
	<td class="line x" title="207:216	Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daume III, 2007; Chelba and Acero, 2004; Daume III and Marcu, 2006))." ></td>
	<td class="line x" title="208:216	Our technique uses unlabeled training data from the target domain, and is thus applicable more generally, including in web processing, where the domain and vocabulary is highly variable, and it is extremely difficult to obtain labeled data that is representative of the test distribution." ></td>
	<td class="line x" title="209:216	When labeled target-domain data is available, instance weighting and similar techniques can be used in combination with our smoothing technique to improve our results further, although this has not yet been demonstrated empirically." ></td>
	<td class="line x" title="210:216	HMM-smoothing improves on the most closely related work, the Structural Correspondence Learning technique for domain adaptation (Blitzer et al., 2006), in experiments." ></td>
	<td class="line x" title="211:216	5 Conclusion and Future Work Our study of smoothing techniques demonstrates that by aggregating information across many unannotated examples, it is possible to find accurate distributional representations that can provide highly informative features to supervised sequence labelers." ></td>
	<td class="line x" title="212:216	These features help improve sequence labeling performance on rare word types, on domains that differ from the training set, and on smaller training sets." ></td>
	<td class="line x" title="213:216	Further experiments are of course necessary to investigate distributional representations as smoothing techniques." ></td>
	<td class="line x" title="214:216	One particularly promising area for further study is the combination of smoothing and instance weighting techniques for domain adaptation." ></td>
	<td class="line x" title="215:216	Whether the current techniques are applicable to structured prediction tasks, like parsing and relation extraction, also deserves future attention." ></td>
	<td class="line x" title="216:216	502" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1057
Minimized Models for Unsupervised Part-of-Speech Tagging
Ravi, Sujith;Knight, Kevin;"></td>
	<td class="line x" title="1:223	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 504512, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:223	c2009 ACL and AFNLP Minimized Models for Unsupervised Part-of-Speech Tagging Sujith Ravi and Kevin Knight University of Southern California Information Sciences Institute Marina del Rey, California 90292 {sravi,knight}@isi.edu Abstract We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values." ></td>
	<td class="line x" title="3:223	We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings." ></td>
	<td class="line x" title="4:223	1 Introduction In recent years, we have seen increased interest in using unsupervised methods for attacking different NLP tasks like part-of-speech (POS) tagging." ></td>
	<td class="line x" title="5:223	The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods." ></td>
	<td class="line x" title="6:223	In this paper, we develop new methods for unsupervised part-of-speech tagging." ></td>
	<td class="line x" title="7:223	We adopt the problem formulation of Merialdo (1994), in which we are given a raw word sequence and a dictionary of legal tags for each word type." ></td>
	<td class="line x" title="8:223	The goal is to tag each word token so as to maximize accuracy against a gold tag sequence." ></td>
	<td class="line x" title="9:223	Whether this is a realistic problem set-up is arguable, but an interesting collection of methods and results has accumulated around it, and these can be clearly compared with one another." ></td>
	<td class="line x" title="10:223	We use the standard test set for this task, a 24,115-word subset of the Penn Treebank, for which a gold tag sequence is available." ></td>
	<td class="line x" title="11:223	There are 5,878 word types in this test set." ></td>
	<td class="line x" title="12:223	We use the standard tag dictionary, consisting of 57,388 word/tag pairs derived from the entire Penn Treebank.1 8,910 dictionary entries are relevant to the 5,878 word types in the test set." ></td>
	<td class="line x" title="13:223	Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data." ></td>
	<td class="line x" title="14:223	There are 45 distinct grammatical tags." ></td>
	<td class="line x" title="15:223	In this set-up, there are no unknown words." ></td>
	<td class="line x" title="16:223	Figure 1 shows prior results for this problem." ></td>
	<td class="line x" title="17:223	While the methods are quite different, they all make use of two common model elements." ></td>
	<td class="line x" title="18:223	One is a probabilistic n-gram tag model P(ti|tin+1ti1), which we call the grammar." ></td>
	<td class="line x" title="19:223	The other is a probabilistic word-given-tag model P(wi|ti), which we call the dictionary." ></td>
	<td class="line x" title="20:223	The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: P(w1wn) = summationdisplay t1tn P(t1tn)P(w1wn|t1tn)  summationdisplay t1tn nproductdisplay i=1 P(ti|ti2 ti1)P(wi|ti) Goldwater and Griffiths (2007) report 74.5% accuracy for EM with a 3-gram tag model, which we confirm by replication." ></td>
	<td class="line x" title="21:223	They improve this to 83.9% by employing a fully Bayesian approach which integrates over all possible parameter values, rather than estimating a single distribution." ></td>
	<td class="line x" title="22:223	They further improve this to 86.8% by using priors that favor sparse distributions." ></td>
	<td class="line x" title="23:223	Smith and Eisner (2005) employ a contrastive estimation tech1As (Banko and Moore, 2004) point out, unsupervised tagging accuracy varies wildly depending on the dictionary employed." ></td>
	<td class="line x" title="24:223	We follow others in using a fat dictionary (with 49,206 distinct word types), rather than a thin one derived only from the test set." ></td>
	<td class="line x" title="25:223	504 System Tagging accuracy (%) on 24,115-word corpus 1." ></td>
	<td class="line x" title="26:223	Random baseline (for each word, pick a random tag from the alternatives given by the word/tag dictionary) 64.6 2." ></td>
	<td class="line x" title="27:223	EM with 2-gram tag model 81.7 3." ></td>
	<td class="line x" title="28:223	EM with 3-gram tag model 74.5 4a." ></td>
	<td class="line x" title="29:223	Bayesian method (Goldwater and Griffiths, 2007) 83.9 4b." ></td>
	<td class="line x" title="30:223	Bayesian method with sparse priors (Goldwater and Griffiths, 2007) 86.8 5." ></td>
	<td class="line x" title="31:223	CRF model trained using contrastive estimation (Smith and Eisner, 2005) 88.6 6." ></td>
	<td class="line x" title="32:223	EM-HMM tagger provided with good initial conditions (Goldberg et al., 2008) 91.4* (*uses linguistic constraints and manual adjustments to the dictionary) Figure 1: Previous results on unsupervised POS tagging using a dictionary (Merialdo, 1994) on the full 45-tag set." ></td>
	<td class="line x" title="33:223	All other results reported in this paper (unless specified otherwise) are on the 45-tag set as well." ></td>
	<td class="line x" title="34:223	nique, in which they automatically generate negative examples and use CRF training." ></td>
	<td class="line x" title="35:223	In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes." ></td>
	<td class="line x" title="36:223	They show considerable improvements in tagging accuracy when using a coarser-grained version (with 17-tags) of the tag set from the Penn Treebank." ></td>
	<td class="line x" title="37:223	Goldberg et al.(2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions." ></td>
	<td class="line x" title="39:223	They use language specific information (like word contexts, syntax and morphology) for learning initial P(t|w) distributions and also use linguistic knowledge to apply constraints on the tag sequences allowed by their models (e.g., the tag sequence V V is disallowed)." ></td>
	<td class="line x" title="40:223	Also, they make other manual adjustments to reduce noise from the word/tag dictionary (e.g., reducing the number of tags for the from six to just one)." ></td>
	<td class="line x" title="41:223	In contrast, we keep all the original dictionary entries derived from the Penn Treebank data for our experiments." ></td>
	<td class="line x" title="42:223	The literature omits one other baseline, which is EM with a 2-gram tag model." ></td>
	<td class="line x" title="43:223	Here we obtain 81.7% accuracy, which is better than the 3-gram model." ></td>
	<td class="line x" title="44:223	It seems that EM with a 3-gram tag model runs amok with its freedom." ></td>
	<td class="line x" title="45:223	For the rest of this paper, we will limit ourselves to a 2-gram tag model." ></td>
	<td class="line x" title="46:223	2 What goes wrong with EM?" ></td>
	<td class="line x" title="47:223	We analyze the tag sequence output produced by EM and try to see where EM goes wrong." ></td>
	<td class="line oc" title="48:223	The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed." ></td>
	<td class="line x" title="49:223	The Bayesian methods overcome this effect by using priors which favor sparser distributions." ></td>
	<td class="line x" title="50:223	But it is not easy to model such priors into EM learning." ></td>
	<td class="line x" title="51:223	As a result, EM exploits a lot of rare tags (like FW = foreign word, or SYM = symbol) and assigns them to common word types (in, of, etc.)." ></td>
	<td class="line x" title="52:223	We can compare the tag assignments from the gold tagging and the EM tagging (Viterbi tag sequence)." ></td>
	<td class="line x" title="53:223	The table below shows tag assignments (and their counts in parentheses) for a few word types which occur frequently in the test corpus." ></td>
	<td class="line x" title="54:223	word/tagdictionary Goldtagging EMtagging in{IN,RP, RB,NN,FW, RBR} IN (355) IN (0) RP(3) RP(0) FW(0) FW(358) of{IN,RP, RB} IN (567) IN (0) RP(0) RP(567) on{IN,RP, RB} RP(5) RP(127) IN (129) IN (0) RB(0) RB(7) a{DT, JJ, IN,LS,FW, SYM,NNP} DT(517) DT(0) SYM(0) SYM(517) We see how the rare tag labels (like FW, SYM, etc.) are abused by EM." ></td>
	<td class="line x" title="55:223	As a result, many word tokens which occur very frequently in the corpus are incorrectly tagged with rare tags in the EM tagging output." ></td>
	<td class="line x" title="56:223	We also look at things more globally." ></td>
	<td class="line x" title="57:223	We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence." ></td>
	<td class="line x" title="58:223	We call this the observed grammar size, and it is 915." ></td>
	<td class="line x" title="59:223	That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45  45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we 2We contrast observed size with the model size for the grammar, which we define as the number of P(t2|t1) entries in EMs trained tag model that exceed 0.0001 probability." ></td>
	<td class="line x" title="60:223	505 L8 L0 they    can          fish       .       I        fish L1 L2 L3 L4 L6 L5 L7 L9 L10 L11 START PRO AUX V N PUNC d1 PRO-theyd2 AUX-can d3 V-cand4 N-fish d5 V-fishd6 PUNC-." ></td>
	<td class="line x" title="61:223	d7 PRO-I g1 PRO-AUXg2 PRO-V g3 AUX-Ng4  AUX-V g5 V-Ng6 V-V g7 N-PUNCg8 V-PUNC g9 PUNC-PROg10 PRO-N dictionary variables grammar variables Integer Program Minimize: i=110 gi Constraints: 1." ></td>
	<td class="line x" title="62:223	Single left-to-right path (at each node, flow in = flow out)e.g.,  L 0 = 1L 1 = L3 + L4 2." ></td>
	<td class="line x" title="63:223	Path consistency constraints (chosen path respects chosen dictionary & grammar) e.g., L0d1L 1g1 IP formulation training text link variables Figure 2: Integer Programming formulation for finding the smallest grammar that explains a given word sequence." ></td>
	<td class="line x" title="64:223	Here, we show a sample word sequence and the corresponding IP network generated for that sequence." ></td>
	<td class="line x" title="65:223	can compare it with the gold taggings observed grammar size, which is 760." ></td>
	<td class="line x" title="66:223	So we can safely say that EM is learning a grammar that is too big, still abusing its freedom." ></td>
	<td class="line x" title="67:223	3 Small Models Bayesian sparse priors aim to create small models." ></td>
	<td class="line x" title="68:223	We take a different tack in the paper and directly ask: What is the smallest model that explains the text?" ></td>
	<td class="line x" title="69:223	Our approach is related to minimum description length (MDL)." ></td>
	<td class="line x" title="70:223	We formulate our question precisely by asking which tag sequence (of the 106425 available) has the smallest observed grammar size." ></td>
	<td class="line x" title="71:223	The answer is 459." ></td>
	<td class="line x" title="72:223	That is, there exists a tag sequence that contains 459 distinct tag bigrams, and no other tag sequence contains fewer." ></td>
	<td class="line x" title="73:223	We obtain this answer by formulating the problem in an integer programming (IP) framework." ></td>
	<td class="line x" title="74:223	Figure 2 illustrates this with a small sample word sequence." ></td>
	<td class="line x" title="75:223	We create a network of possible taggings, and we assign a binary variable to each link in the network." ></td>
	<td class="line x" title="76:223	We create constraints to ensure that those link variables receiving a value of 1 form a left-to-right path through the tagging network, and that all other link variables receive a value of 0." ></td>
	<td class="line x" title="77:223	We accomplish this by requiring the sum of the links entering each node to equal to the sum of the links leaving each node." ></td>
	<td class="line x" title="78:223	We also create variables for every possible tag bigram and word/tag dictionary entry." ></td>
	<td class="line x" title="79:223	We constrain link variable assignments to respect those grammar and dictionary variables." ></td>
	<td class="line x" title="80:223	For example, we do not allow a link variable to activate unless the corresponding grammar variable is also activated." ></td>
	<td class="line x" title="81:223	Finally, we add an objective function that minimizes the number of grammar variables that are assigned a value of 1." ></td>
	<td class="line x" title="82:223	Figure 3 shows the IP solution for the example word sequence from Figure 2." ></td>
	<td class="line x" title="83:223	Of course, a small grammar size does not necessarily correlate with higher tagging accuracy." ></td>
	<td class="line x" title="84:223	For the small toy example shown in Figure 3, the correct tagging is PRO AUX V . PRO V (with 5 tag pairs), whereas the IP tries to minimize the grammar size and picks another solution instead." ></td>
	<td class="line x" title="85:223	For solving the integer program, we use CPLEX software (a commercial IP solver package)." ></td>
	<td class="line x" title="86:223	Alternatively, there are other programs such as lp solve, which are free and publicly available for use." ></td>
	<td class="line x" title="87:223	Once we create an integer program for the full test corpus, and pass it to CPLEX, the solver returns an 506 word sequence: they can fish . I fish Tagging GrammarSize PRO AUX N . PRO N 5 PRO AUX V . PRO N 5 PRO AUX N . PRO V 5 PRO AUX V . PRO V 5 PRO V N . PRO N 5 PRO V V . PRO N 5 PRO V N . PRO V 4 PRO V V . PRO V 4 Figure 3: Possible tagging solutions and corresponding grammar sizes for the sample word sequence from Figure 2 using the given dictionary and grammar." ></td>
	<td class="line x" title="88:223	The IP solver finds the smallest grammar set that can explain the given word sequence." ></td>
	<td class="line x" title="89:223	In this example, there exist two solutions that each contain only 4 tag pair entries, and IP returns one of them." ></td>
	<td class="line x" title="90:223	objective function value of 459.3 CPLEX also returns a tag sequence via assignments to the link variables." ></td>
	<td class="line x" title="91:223	However, there are actually 104378 tag sequences compatible with the 459-sized grammar, and our IP solver just selects one at random." ></td>
	<td class="line x" title="92:223	We find that of all those tag sequences, the worst gives an accuracy of 50.8%, and the best gives an accuracy of 90.3%." ></td>
	<td class="line x" title="93:223	We also note that CPLEX takes 320 seconds to return the optimal solution for the integer program corresponding to this particular test data (24,115 tokens with the 45-tag set)." ></td>
	<td class="line x" title="94:223	It might be interesting to see how the performance of the IP method (in terms of time complexity) is affected when scaling up to larger data and bigger tagsets." ></td>
	<td class="line x" title="95:223	We leave this as part of future work." ></td>
	<td class="line x" title="96:223	But we do note that it is possible to obtain less than optimal solutions faster by interrupting the CPLEX solver." ></td>
	<td class="line x" title="97:223	4 Fitting the Model Our IP formulation can find us a small model, but it does not attempt to fit the model to the data." ></td>
	<td class="line x" title="98:223	Fortunately, we can use EM for that." ></td>
	<td class="line x" title="99:223	We still give EM the full word/tag dictionary, but now we constrain its initial grammar model to the 459 tag bigrams identified by IP." ></td>
	<td class="line x" title="100:223	Starting with uniform probabilities, EM finds a tagging that is 84.5% accurate, substantially better than the 81.7% originally obtained with the fully-connected grammar." ></td>
	<td class="line x" title="101:223	So we see a benefit to our explicit small-model approach." ></td>
	<td class="line x" title="102:223	While EM does not find the most accurate 3Note that the grammar identified by IP is not uniquely minimal." ></td>
	<td class="line x" title="103:223	For the same word sequence, there exist other minimal grammars having the same size (459 entries)." ></td>
	<td class="line x" title="104:223	In our experiments, we choose the first solution returned by CPLEX." ></td>
	<td class="line x" title="105:223	in on IN IN RP RP word/tag dictionary RB RB NN FW RBR observed EM dictionary FW (358) RP (127) RB (7) observed IP+EM dictionary IN (349) IN (126) RB (9) RB (8) observed gold dictionary IN (355) IN (129) RB (3) RP (5) Figure 4: Examples of tagging obtained from different systems for prepositions in and on." ></td>
	<td class="line x" title="106:223	sequence consistent with the IP grammar (90.3%), it finds a relatively good one." ></td>
	<td class="line x" title="107:223	The IP+EM tagging (with 84.5% accuracy) has some interesting properties." ></td>
	<td class="line x" title="108:223	First, the dictionary we observe from the tagging is of higher quality (with fewer spurious tagging assignments) than the one we observe from the original EM tagging." ></td>
	<td class="line x" title="109:223	Figure 4 shows some examples." ></td>
	<td class="line x" title="110:223	We also measure the quality of the two observed grammars/dictionaries by computing their precision and recall against the grammar/dictionary we observe in the gold tagging.4 We find that precision of the observed grammar increases from 0.73 (EM) to 0.94 (IP+EM)." ></td>
	<td class="line x" title="111:223	In addition to removing many bad tag bigrams from the grammar, IP minimization also removes some of the good ones, leading to lower recall (EM = 0.87, IP+EM = 0.57)." ></td>
	<td class="line x" title="112:223	In the case of the observed dictionary, using a smaller grammar model does not affect the precision (EM = 0.91, IP+EM = 0.89) or recall (EM = 0.89, IP+EM = 0.89)." ></td>
	<td class="line x" title="113:223	During EM training, the smaller grammar with fewer bad tag bigrams helps to restrict the dictionary model from making too many bad choices that EM made earlier." ></td>
	<td class="line x" title="114:223	Here are a few examples of bad dictionary entries that get removed when we use the minimized grammar for EM training: in  FW a  SYM of  RP In  RBR During EM training, the minimized grammar 4For any observed grammar or dictionary X, Precision (X) = |{X}{observedgold}||{X}| Recall (X) = |{X}{observedgold}||{observedgold}| 507 Model Taggingaccuracy Observedsize Modelsize on 24,115-word corpus grammar(G),dictionary(D) grammar(G),dictionary(D) 1." ></td>
	<td class="line x" title="115:223	EM baselinewith full grammar+ full dictionary 81.7 G=915,D=6295 G=935,D=6430 2." ></td>
	<td class="line x" title="116:223	EM constrainedwith minimizedIP-grammar + fulldictionary 84.5 G=459,D=6318 G=459,D=6414 3." ></td>
	<td class="line x" title="117:223	EM constrainedwith full grammar+ dictionaryfrom(2) 91.3 G=606,D=6245 G=612,D=6298 4." ></td>
	<td class="line x" title="118:223	EMconstrainedwithgrammarfrom(3) + full dictionary 91.5 G=593,D=6285 G=600,D=6373 5." ></td>
	<td class="line x" title="119:223	EM constrainedwith full grammar+ dictionaryfrom(4) 91.6 G=603,D=6280 G=618,D=6337 Figure 5: Percentage of word tokens tagged correctly by different models." ></td>
	<td class="line x" title="120:223	The observed sizes and model sizes of grammar (G) and dictionary (D) produced by these models are shown in the last two columns." ></td>
	<td class="line x" title="121:223	helps to eliminate many incorrect entries (i.e., zero out model parameters) from the dictionary, thereby yielding an improved dictionary model." ></td>
	<td class="line x" title="122:223	So using the minimized grammar (which has higher precision) helps to improve the quality of the chosen dictionary (examples shown in Figure 4)." ></td>
	<td class="line x" title="123:223	This in turn helps improve the tagging accuracy from 81.7% to 84.5%." ></td>
	<td class="line x" title="124:223	It is clear that the IP-constrained grammar is a better choice to run EM on than the full grammar." ></td>
	<td class="line x" title="125:223	Note that we used a very small IP-grammar (containing only 459 tag bigrams) during EM training." ></td>
	<td class="line x" title="126:223	In the process of minimizing the grammar size, IP ends up removing many good tag bigrams from our grammar set (as seen from the low measured recall of 0.57 for the observed grammar)." ></td>
	<td class="line x" title="127:223	Next, we proceed to recover some good tag bigrams and expand the grammar in a restricted fashion by making use of the higher-quality dictionary produced by the IP+EM method." ></td>
	<td class="line x" title="128:223	We now run EM again on the full grammar (all possible tag bigrams) in combination with this good dictionary (containing fewer entries than the full dictionary)." ></td>
	<td class="line x" title="129:223	Unlike the original training with full grammar, where EM could choose any tag bigram, now the choice of grammar entries is constrained by the good dictionary model that we provide EM with." ></td>
	<td class="line x" title="130:223	This allows EM to recover some of the good tag pairs, and results in a good grammardictionary combination that yields better tagging performance." ></td>
	<td class="line x" title="131:223	With these improvements in mind, we embark on an alternating scheme to find better models and taggings." ></td>
	<td class="line x" title="132:223	We run EM for multiple passes, and in each pass we alternately constrain either the grammar model or the dictionary model." ></td>
	<td class="line x" title="133:223	The procedure is simple and proceeds as follows: 1." ></td>
	<td class="line x" title="134:223	Run EM constrained to the last trained dictionary, but provided with a full grammar.5 2." ></td>
	<td class="line x" title="135:223	Run EM constrained to the last trained grammar, but provided with a full dictionary." ></td>
	<td class="line x" title="136:223	3." ></td>
	<td class="line x" title="137:223	Repeat steps 1 and 2." ></td>
	<td class="line x" title="138:223	We notice significant gains in tagging performance when applying this technique." ></td>
	<td class="line x" title="139:223	The tagging accuracy increases at each step and finally settles at a high of 91.6%, which outperforms the existing state-of-the-art systems for the 45-tag set." ></td>
	<td class="line x" title="140:223	The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al.(2008) without using any additional linguistic constraints or manual cleaning of the dictionary." ></td>
	<td class="line x" title="142:223	Figure 5 shows the tagging performance achieved at each step." ></td>
	<td class="line x" title="143:223	We found that it is the elimination of incorrect entries from the dictionary (and grammar) and not necessarily the initialization weights from previous EM training, that results in the tagging improvements." ></td>
	<td class="line x" title="144:223	Initializing the last trained dictionary or grammar at each step with uniform weights also yields the same tagging improvements as shown in Figure 5." ></td>
	<td class="line x" title="145:223	We find that the observed grammar also improves, growing from 459 entries to 603 entries, with precision increasing from 0.94 to 0.96, and recall increasing from 0.57 to 0.76." ></td>
	<td class="line x" title="146:223	The figure also shows the models internal grammar and dictionary sizes." ></td>
	<td class="line x" title="147:223	Figure 6 and 7 show how the precision/recall of the observed grammar and dictionary varies for different models from Figure 5." ></td>
	<td class="line x" title="148:223	In the case of the observed grammar (Figure 6), precision increases 5For all experiments, EM training is allowed to run for 40 iterations or until the likelihood ratios between two subsequent iterations reaches a value of 0.99999, whichever occurs earlier." ></td>
	<td class="line x" title="149:223	508  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 Precision / Recall of observed grammar Tagging ModelModel 1Model 2Model 3Model 4Model 5 PrecisionRecall Figure 6: Comparison of observed grammars from the model tagging vs. gold tagging in terms of precision and recall measures." ></td>
	<td class="line x" title="150:223	0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 Precision / Recall of observed dictionary Tagging ModelModel 1Model 2Model 3Model 4Model 5 PrecisionRecall Figure 7: Comparison of observed dictionaries from the model tagging vs. gold tagging in terms of precision and recall measures." ></td>
	<td class="line x" title="151:223	Model Tagging accuracy on 24,115-wordcorpus no-restarts with100restarts 1." ></td>
	<td class="line x" title="152:223	Model1 (EMbaseline) 81.7 83.8 2." ></td>
	<td class="line x" title="153:223	Model2 84.5 84.5 3." ></td>
	<td class="line x" title="154:223	Model3 91.3 91.8 4." ></td>
	<td class="line x" title="155:223	Model4 91.5 91.8 5." ></td>
	<td class="line x" title="156:223	Model5 91.6 91.8 Figure 8: Effect of random restarts (during EM training) on tagging accuracy." ></td>
	<td class="line x" title="157:223	at each step, whereas recall drops initially (owing to the grammar minimization) but then picks up again." ></td>
	<td class="line x" title="158:223	The precision/recall of the observed dictionary on the other hand, is not affected by much." ></td>
	<td class="line x" title="159:223	5 Restarts and More Data Multiple random restarts for EM, while not often emphasized in the literature, are key in this domain." ></td>
	<td class="line x" title="160:223	Recall that our original EM tagging with a fully-connected 2-gram tag model was 81.7% accurate." ></td>
	<td class="line x" title="161:223	When we execute 100 random restarts and select the model with the highest data likelihood, we get 83.8% accuracy." ></td>
	<td class="line x" title="162:223	Likewise, when we extend our alternating EM scheme to 100 random restarts at each step, we improve our tagging accuracy from 91.6% to 91.8% (Figure 8)." ></td>
	<td class="line x" title="163:223	As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models." ></td>
	<td class="line x" title="164:223	Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al.(2008)." ></td>
	<td class="line x" title="166:223	But previous results from Smith and Eisner (2005) and Goldwater and Griffiths (2007) show that their models do not benefit from using more unlabeled training data." ></td>
	<td class="line x" title="167:223	Because EM is efficient, we can extend our word-sequence training data from the 24,115-token set to the entire Penn Treebank (973k tokens)." ></td>
	<td class="line x" title="168:223	We run EM training again for Model 5 (the best model from Figure 5) but this time using 973k word tokens, and further increase our accuracy to 92.3%." ></td>
	<td class="line x" title="169:223	This is our final result on the 45-tagset, and we note that it is higher than previously reported results." ></td>
	<td class="line x" title="170:223	6 Smaller Tagset and Incomplete Dictionaries Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008)." ></td>
	<td class="line x" title="171:223	Their systems were shown to obtain considerable improvements in accuracy when using a 17-tagset (a coarsergrained version of the tag labels from the Penn Treebank) instead of the 45-tagset." ></td>
	<td class="line x" title="172:223	When tagging the same standard test corpus with the smaller 17-tagset, our method is able to achieve a substantially high accuracy of 96.8%, which is the best result reported so far on this task." ></td>
	<td class="line x" title="173:223	The table in Figure 9 shows a comparison of different systems for which tagging accuracies have been reported previously for the 17-tagset case (Goldberg et al., 2008)." ></td>
	<td class="line x" title="174:223	The first row in the table compares tagging results when using a full dictionary (i.e., a lexicon containing entries for 49,206 word types)." ></td>
	<td class="line x" title="175:223	The InitEM-HMM system from Goldberg et al.(2008) reports an accuracy of 93.8%, followed by the LDA+AC model (Latent Dirichlet Allocation model with a strong Ambiguity Class component) from Toutanova and Johnson (2008)." ></td>
	<td class="line x" title="177:223	In comparison, the Bayesian HMM (BHMM) model from Goldwater et al.(2007) and 509 Dict IP+EM(24k) InitEM-HMM LDA+AC CE+spl BHMM Full(49206words) 96.8(96.8) 93.8 93.4 88.7 87.3 2 (2141words) 90.6(90.0) 89.4 91.2 79.5 79.6 3 (1249words) 88.0(86.1) 87.4 89.7 78.4 71 Figure 9: Comparison of different systems for English unsupervised POS tagging with 17 tags." ></td>
	<td class="line x" title="179:223	the CE+spl model (Contrastive Estimation with a spelling model) from Smith and Eisner (2005) report lower accuracies (87.3% and 88.7%, respectively)." ></td>
	<td class="line x" title="180:223	Our system (IP+EM) which uses integer programming and EM, gets the highest accuracy (96.8%)." ></td>
	<td class="line x" title="181:223	The accuracy numbers reported for Init-HMM and LDA+AC are for models that are trained on all the available unlabeled data from the Penn Treebank." ></td>
	<td class="line x" title="182:223	The IP+EM models used in the 17-tagset experiments reported here were not trained on the entire Penn Treebank, but instead used a smaller section containing 77,963 tokens for estimating model parameters." ></td>
	<td class="line x" title="183:223	We also include the accuracies for our IP+EM model when using only the 24,115 token test corpus for EM estimation (shown within parenthesis in second column of the table in Figure 9)." ></td>
	<td class="line x" title="184:223	We find that our performance does not degrade when the parameter estimation is done using less data, and our model still achieves a high accuracy of 96.8%." ></td>
	<td class="line x" title="185:223	6.1 Incomplete Dictionaries and Unknown Words The literature also includes results reported in a different setting for the tagging problem." ></td>
	<td class="line x" title="186:223	In some scenarios, a complete dictionary with entries for all word types may not be readily available to us and instead, we might be provided with an incomplete dictionary that contains entries for only frequent word types." ></td>
	<td class="line x" title="187:223	In such cases, any word not appearing in the dictionary will be treated as an unknown word, and can be labeled with any of the tags from given tagset (i.e., for every unknown word, there are 17 tag possibilities)." ></td>
	<td class="line x" title="188:223	Some previous approaches (Toutanova and Johnson, 2008; Goldberg et al., 2008) handle unknown words explicitly using ambiguity class components conditioned on various morphological features, and this has shown to produce good tagging results, especially when dealing with incomplete dictionaries." ></td>
	<td class="line x" title="189:223	We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word." ></td>
	<td class="line x" title="190:223	We first identify the top-100 suffixes (up to 3 characters) for words in the dictionary." ></td>
	<td class="line x" title="191:223	Using the word/tag pairs from the dictionary, we train a simple probabilistic model that predicts the tag given a particular suffix (e.g., P(VBG|ing) = 0.97, P(N|ing) = 0.0001, )." ></td>
	<td class="line x" title="192:223	Next, for every unknown word w, the trained P(tag|suffix) model is used to predict the top 3 tag possibilities for w (using only its suffix information), and subsequently this word along with its 3 tags are added as a new entry to the lexicon." ></td>
	<td class="line x" title="193:223	We do this for every unknown word, and eventually we have a dictionary containing entries for all the words." ></td>
	<td class="line x" title="194:223	Once the completed lexicon (containing both correct entries for words in the lexicon and the predicted entries for unknown words) is available, we follow the same methodology from Sections 3 and 4 using integer programming to minimize the size of the grammar and then applying EM to estimate parameter values." ></td>
	<td class="line x" title="195:223	Figure 9 shows comparative results for the 17tagset case when the dictionary is incomplete." ></td>
	<td class="line x" title="196:223	The second and third rows in the table shows tagging accuracies for different systems when a cutoff of 2 (i.e., all word types that occur with frequency counts < 2 in the test corpus are removed) and a cutoff of 3 (i.e., all word types occurring with frequency counts < 3 in the test corpus are removed) is applied to the dictionary." ></td>
	<td class="line x" title="197:223	This yields lexicons containing 2,141 and 1,249 words respectively, which are much smaller compared to the original 49,206 word dictionary." ></td>
	<td class="line x" title="198:223	As the results in Figure 9 illustrate, the IP+EM method clearly does better than all the other systems except for the LDA+AC model." ></td>
	<td class="line x" title="199:223	The LDA+AC model from Toutanova and Johnson (2008) has a strong ambiguity class component and uses more features to handle the unknown words better, and this contributes to the slightly higher performance in the incomplete dictionary cases, when compared to the IP+EM model." ></td>
	<td class="line x" title="200:223	7 Discussion The method proposed in this paper is simple once an integer program is produced, there are solvers available which directly give us the solution." ></td>
	<td class="line x" title="201:223	In addition, we do not require any complex parameter estimation techniques; we train our models using simple EM, which proves to be efficient for this task." ></td>
	<td class="line x" title="202:223	While some previous methods 510 wordtype Goldtag Automatictag #oftokenstaggedincorrectly s POS VBZ 173be VB VBP 67 that IN WDT 54New NNP NNPS 33 U.S. NNP JJ 31up RP RB 28 more RBR JJR 27and CC IN 23 have VB VBP 20first JJ JJS 20 to TO IN 19out RP RB 17 there EX RB 15stock NN JJ 15 what WP WDT 14one CD NN 14  POS : 14as RB IN 14 all DT RB 14that IN RB 13 Figure 10: Most frequent mistakes observed in the model tagging (using the best model, which gives 92.3% accuracy) when compared to the gold tagging." ></td>
	<td class="line x" title="203:223	introduced for the same task have achieved big tagging improvements using additional linguistic knowledge or manual supervision, our models are not provided with any additional information." ></td>
	<td class="line x" title="204:223	Figure 10 illustrates for the 45-tag set some of the common mistakes that our best tagging model (92.3%) makes." ></td>
	<td class="line x" title="205:223	In some cases, the model actually gets a reasonable tagging but is penalized perhaps unfairly." ></td>
	<td class="line x" title="206:223	For example, to is tagged as IN by our model sometimes when it occurs in the context of a preposition, whereas in the gold tagging it is always tagged as TO." ></td>
	<td class="line x" title="207:223	The model also gets penalized for tagging the word U.S. as an adjective (JJ), which might be considered valid in some cases such as the U.S. State Department." ></td>
	<td class="line x" title="208:223	In other cases, the model clearly produces incorrect tags (e.g., New gets tagged incorrectly as NNPS)." ></td>
	<td class="line x" title="209:223	Our method resembles the classic Minimum Description Length (MDL) approach for model selection (Barron et al., 1998)." ></td>
	<td class="line x" title="210:223	In MDL, there is a single objective function to (1) maximize the likelihood of observing the data, and at the same time (2) minimize the length of the model description (which depends on the model size)." ></td>
	<td class="line x" title="211:223	However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results." ></td>
	<td class="line x" title="212:223	In the past, only a few approaches utilizing MDL have been shown to work for natural language applications." ></td>
	<td class="line x" title="213:223	These approaches employ heuristic search methods with MDL for the task of unsupervised learning of morphology of natural languages (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2005)." ></td>
	<td class="line x" title="214:223	The method proposed in this paper is the first application of the MDL idea to POS tagging, and the first to use an integer programming formulation rather than heuristic search techniques." ></td>
	<td class="line x" title="215:223	We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007)." ></td>
	<td class="line x" title="216:223	8 Conclusion We presented a novel method for attacking dictionary-based unsupervised part-of-speech tagging." ></td>
	<td class="line x" title="217:223	Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset." ></td>
	<td class="line x" title="218:223	The method works by explicitly minimizing the grammar size using integer programming, and then using EM to estimate parameter values." ></td>
	<td class="line x" title="219:223	The entire process is fully automated and yields better performance than any existing state-of-the-art system, even though our models were not provided with any additional linguistic knowledge (for example, explicit syntactic constraints to avoid certain tag combinations such as V V, etc.)." ></td>
	<td class="line x" title="220:223	However, it is easy to model some of these linguistic constraints (both at the local and global levels) directly using integer programming, and this may result in further improvements and lead to new possibilities for future research." ></td>
	<td class="line x" title="221:223	For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task." ></td>
	<td class="line x" title="222:223	9 Acknowledgements This research was supported by the Defense Advanced Research Projects Agency under SRI Internationals prime Contract Number NBCHD040058." ></td>
	<td class="line x" title="223:223	511" ></td>
</tr></table>
</div
</body></html>
