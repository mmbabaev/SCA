<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
W04-1013 <div class="dstPaperTitle">ROUGE: A Package For Automatic Evaluation Of Summaries</div><div class="dstPaperAuthors">Lin, Chin Yew;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-1077
Automatic Evaluation Of Machine Translation Quality Using Longest Common Subsequence And Skip-Bigram Statistics
Lin, Chin Yew;Och, Franz Josef;"></td>
	<td class="line x" title="1:211	Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics Chin-Yew Lin and Franz Josef Och Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292, USA {cyl,och}@isi.edu Abstract In this paper we describe two new objective automatic evaluation methods for machine translation." ></td>
	<td class="line x" title="2:211	The first method is based on longest common subsequence between a candidate translation and a set of reference translations." ></td>
	<td class="line x" title="3:211	Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring insequence n-grams automatically." ></td>
	<td class="line x" title="4:211	The second method relaxes strict n-gram matching to skipbigram matching." ></td>
	<td class="line x" title="5:211	Skip-bigram is any pair of words in their sentence order." ></td>
	<td class="line x" title="6:211	Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations." ></td>
	<td class="line x" title="7:211	The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency." ></td>
	<td class="line x" title="8:211	1 Introduction Using objective functions to automatically evaluate machine translation quality is not new." ></td>
	<td class="line x" title="9:211	Su et al.(1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations." ></td>
	<td class="line x" title="11:211	Akiba et al.(2001) extended the idea to accommodate multiple references." ></td>
	<td class="line x" title="13:211	Nieen et al.(2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations." ></td>
	<td class="line x" title="15:211	Leusch et al.(2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead." ></td>
	<td class="line x" title="17:211	Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995)." ></td>
	<td class="line x" title="18:211	An n-gram co-occurrence measure, BLEU, proposed by Papineni et al.(2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential." ></td>
	<td class="line x" title="20:211	A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations." ></td>
	<td class="line x" title="21:211	Recently, Turian et al.(2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation." ></td>
	<td class="line x" title="23:211	However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n > 1) matches achieved similar performance as Bleu." ></td>
	<td class="line x" title="24:211	Since unigram matches do not distinguish words in consecutive positions from words in the wrong order, measures based on position-independent unigram matches are not sensitive to word order and sentence level structure." ></td>
	<td class="line x" title="25:211	Therefore, systems optimized for these unigram-based measures might generate adequate but not fluent target language." ></td>
	<td class="line x" title="26:211	Since BLEU has been used to report the performance of many machine translation systems and it has been shown to correlate well with human judgments, we will explain BLEU in more detail and point out its limitations in the next section." ></td>
	<td class="line x" title="27:211	We then introduce a new evaluation method called ROUGE-L that measures sentence-to-sentence similarity based on the longest common subsequence statistics between a candidate translation and a set of reference translations in Section 3." ></td>
	<td class="line x" title="28:211	Section 4 describes another automatic evaluation method called ROUGE-S that computes skipbigram co-occurrence statistics." ></td>
	<td class="line x" title="29:211	Section 5 presents the evaluation results of ROUGE-L, and ROUGES and compare them with BLEU, GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency." ></td>
	<td class="line x" title="30:211	We conclude this paper and discuss extensions of the current work in Section 6." ></td>
	<td class="line x" title="31:211	2 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001)." ></td>
	<td class="line x" title="32:211	In two recent large-scale machine translation evaluations sponsored by NIST, a closely related automatic evaluation method, simply called NIST score, was used." ></td>
	<td class="line x" title="33:211	The NIST (NIST 2002) scoring method is based on BLEU." ></td>
	<td class="line x" title="34:211	The main idea of BLEU is to measure the similarity between a candidate translation and a set of reference translations with a numerical metric." ></td>
	<td class="line x" title="35:211	They used a weighted average of variable length ngram matches between system translations and a set of human reference translations and showed that the weighted average metric correlating highly with human assessments." ></td>
	<td class="line x" title="36:211	BLEU measures how well a machine translation overlaps with multiple human translations using ngram co-occurrence statistics." ></td>
	<td class="line x" title="37:211	N-gram precision in BLEU is computed as follows:       = }{ }{ )( )( CandidatesCCgramn CandidatesCCgramn clip n gramnCount gramnCount p (1) Where Count clip (n-gram) is the maximum number of n-grams co-occurring in a candidate translation and a reference translation, and Count(ngram) is the number of n-grams in the candidate translation." ></td>
	<td class="line x" title="38:211	To prevent very short translations that try to maximize their precision scores, BLEU adds a brevity penalty, BP, to the formula: )2( 1 |)|/||1(        > =  rcife rcif BP cr Where |c| is the length of the candidate translation and |r| is the length of the reference translation." ></td>
	<td class="line x" title="39:211	The BLEU formula is then written as follows: )3(logexp 1       =  = N n nn pwBPBLEU The weighting factor, w n, is set at 1/N. Although BLEU has been shown to correlate well with human assessments, it has a few things that can be improved." ></td>
	<td class="line x" title="40:211	First the subjective application of the brevity penalty can be replaced with a recall related parameter that is sensitive to reference length." ></td>
	<td class="line x" title="41:211	Although brevity penalty will penalize candidate translations with low recall by a factor of e (1|r|/|c|), it would be nice if we can use the traditional recall measure that has been a well known measure in NLP as suggested by Melamed (2003)." ></td>
	<td class="line x" title="42:211	Of course we have to make sure the resulting composite function of precision and recall is still correlates highly with human judgments." ></td>
	<td class="line x" title="43:211	Second, although BLEU uses high order n-gram (n>1) matches to favor candidate sentences with consecutive word matches and to estimate their fluency, it does not consider sentence level structure." ></td>
	<td class="line x" title="44:211	For example, given the following sentences: S1." ></td>
	<td class="line x" title="45:211	police killed the gunman S2." ></td>
	<td class="line x" title="46:211	police kill the gunman 1 S3." ></td>
	<td class="line x" title="47:211	the gunman kill police We only consider BLEU with unigram and bigram, i.e. N=2, for the purpose of explanation and call this BLEU-2." ></td>
	<td class="line x" title="48:211	Using S1 as the reference and S2 and S3 as the candidate translations, S2 and S3 would have the same BLEU-2 score, since they both have one bigram and three unigram matches 2." ></td>
	<td class="line x" title="49:211	However, S2 and S3 have very different meanings." ></td>
	<td class="line x" title="50:211	Third, BLEU is a geometric mean of unigram to N-gram precisions." ></td>
	<td class="line x" title="51:211	Any candidate translation without a N-gram match has a per-sentence BLEU score of zero." ></td>
	<td class="line x" title="52:211	Although BLEU is usually calculated over the whole test corpus, it is still desirable to have a measure that works reliably at sentence level for diagnostic and introspection purpose." ></td>
	<td class="line x" title="53:211	To address these issues, we propose three new automatic evaluation measures based on longest common subsequence statistics and skip bigram co-occurrence statistics in the following sections." ></td>
	<td class="line x" title="54:211	3 Longest Common Subsequence 3.1 ROUGE-L A sequence Z = [z 1, z 2,  , z n ] is a subsequence of another sequence X = [x 1, x 2,  , x m ], if there exists a strict increasing sequence [i 1, i 2,  , i k ] of indices of X such that for all j = 1, 2,  , k, we have x ij = z j (Cormen et al. 1989)." ></td>
	<td class="line x" title="55:211	Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length." ></td>
	<td class="line x" title="56:211	We can find the LCS of two sequences of length m and n using standard dynamic programming technique in O(mn) time." ></td>
	<td class="line x" title="57:211	LCS has been used to identify cognate candidates during construction of N-best translation lexicons from parallel text." ></td>
	<td class="line x" title="58:211	Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them." ></td>
	<td class="line x" title="59:211	He used as an approximate string matching algorithm." ></td>
	<td class="line x" title="60:211	Saggion et al.(2002) used normalized pairwise LCS (NP-LCS) to compare similarity between two texts in automatic summarization evaluation." ></td>
	<td class="line x" title="62:211	NP-LCS can be shown as a special case of Equation (6) with  = 1." ></td>
	<td class="line x" title="63:211	However, they did not provide the correlation analysis of NP-LCS with 1 This is a real machine translation output." ></td>
	<td class="line x" title="64:211	2 The kill in S2 or S3 does not match with killed in S1 in strict word-to-word comparison." ></td>
	<td class="line x" title="65:211	human judgments and its effectiveness as an automatic evaluation measure." ></td>
	<td class="line x" title="66:211	To apply LCS in machine translation evaluation, we view a translation as a sequence of words." ></td>
	<td class="line x" title="67:211	The intuition is that the longer the LCS of two translations is, the more similar the two translations are." ></td>
	<td class="line x" title="68:211	We propose using LCS-based F-measure to estimate the similarity between two translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, as follows: R lcs m YXLCS ),( = (4) P lcs n YXLCS ),( = (5) F lcs lcslcs lcslcs PR PR 2 2 )1(   + + = (6) Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and  = P lcs /R lcs when F lcs /R lcs _=_F lcs /P lcs . We call the LCS-based Fmeasure, i.e. Equation 6, ROUGE-L." ></td>
	<td class="line x" title="69:211	Notice that ROUGE-L is 1 when X = Y since LCS(X,Y) = m or n; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen 1979)." ></td>
	<td class="line x" title="70:211	The composite factors are LCS-based recall and precision in this case." ></td>
	<td class="line x" title="71:211	Melamed et al.(2003) used unigram F-measure to estimate machine translation quality and showed that unigram F-measure was as good as BLEU." ></td>
	<td class="line x" title="73:211	One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as ngrams." ></td>
	<td class="line x" title="74:211	The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary." ></td>
	<td class="line x" title="75:211	ROUGE-L as defined in Equation 6 has the property that its value is less than or equal to the minimum of unigram F-measure of X and Y. Unigram recall reflects the proportion of words in X (reference translation) that are also present in Y (candidate translation); while unigram precision is the proportion of words in Y that are also in X. Unigram recall and precision count all co-occurring words regardless their orders; while ROUGE-L counts only in-sequence co-occurrences." ></td>
	<td class="line x" title="76:211	By only awarding credit to in-sequence unigram matches, ROUGE-L also captures sentence level structure in a natural way." ></td>
	<td class="line x" title="77:211	Consider again the example given in Section 2 that is copied here for convenience: S1." ></td>
	<td class="line x" title="78:211	police killed the gunman S2." ></td>
	<td class="line x" title="79:211	police kill the gunman S3." ></td>
	<td class="line x" title="80:211	the gunman kill police As we have shown earlier, BLEU-2 cannot differentiate S2 from S3." ></td>
	<td class="line x" title="81:211	However, S2 has a ROUGE-L score of 3/4 = 0.75 and S3 has a ROUGE-L score of 2/4 = 0.5, with  = 1." ></td>
	<td class="line x" title="82:211	Therefore S2 is better than S3 according to ROUGE-L." ></td>
	<td class="line x" title="83:211	This example also illustrated that ROUGE-L can work reliably at sentence level." ></td>
	<td class="line x" title="84:211	However, LCS only counts the main in-sequence words; therefore, other longest common subsequences and shorter sequences are not reflected in the final score." ></td>
	<td class="line x" title="85:211	For example, consider the following candidate sentence: S4." ></td>
	<td class="line x" title="86:211	the gunman police killed Using S1 as its reference, LCS counts either the gunman or police killed, but not both; therefore, S4 has the same ROUGE-L score as S3." ></td>
	<td class="line x" title="87:211	BLEU-2 would prefer S4 over S3." ></td>
	<td class="line x" title="88:211	In Section 4, we will introduce skip-bigram co-occurrence statistics that do not have this problem while still keeping the advantage of in-sequence (not necessary consecutive) matching that reflects sentence level word order." ></td>
	<td class="line x" title="89:211	3.2 Multiple References So far, we only demonstrated how to compute ROUGE-L using a single reference." ></td>
	<td class="line x" title="90:211	When multiple references are used, we take the maximum LCS matches between a candidate translation, c, of n words and a set of u reference translations of m j words." ></td>
	<td class="line x" title="91:211	The LCS-based F-measure can be computed as follows: R lcs-multi         = = j ju j m crLCS ),( max 1 (7) P lcs-multi         = = n crLCS ju j ),( max 1 (8) F lcs-multi multilcsmultilcs multilcsmultilcs PR PR   + + = 2 2 )1(   (9) where  = P lcs-multi /R lcs-multi when F lcs-multi /R lcsmulti _=_F lcs-multi /P lcs-multi." ></td>
	<td class="line x" title="92:211	This procedure is also applied to computation of ROUGE-S when multiple references are used." ></td>
	<td class="line x" title="93:211	In the next section, we introduce the skip-bigram cooccurrence statistics." ></td>
	<td class="line x" title="94:211	In the next section, we describe how to extend ROUGE-L to assign more credits to longest common subsequences with consecutive words." ></td>
	<td class="line x" title="95:211	3.3 ROUGE-W: Weighted Longest Common Subsequence LCS has many nice properties as we have described in the previous sections." ></td>
	<td class="line x" title="96:211	Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences." ></td>
	<td class="line x" title="97:211	For example, given a reference sequence X and two candidate sequences Y 1 and Y 2 as follows: X: [A B C D E F G] Y 1 : [A B C D H I K] Y 2 : [A H B K C I D] Y 1 and Y 2 have the same ROUGE-L score." ></td>
	<td class="line x" title="98:211	However, in this case, Y 1 should be the better choice than Y 2 because Y 1 has consecutive matches." ></td>
	<td class="line x" title="99:211	To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS." ></td>
	<td class="line x" title="100:211	We call this weighted LCS (WLCS) and use k to indicate the length of the current consecutive matches ending at words x i and y j . Given two sentences X and Y, the WLCS score of X and Y can be computed using the following dynamic programming procedure: (1) For (i = 0; i <=m; i++) c(i,j) = 0 // initialize c-table w(i,j) = 0 // initialize w-table (2) For (i = 1; i <= m; i++) For (j = 1; j <= n; j++) If x i = y j Then // the length of consecutive matches at // position i-1 and j-1 k = w(i-1,j-1) c(i,j) = c(i-1,j-1) + f(k+1)  f(k) // remember the length of consecutive // matches at position i, j w(i,j) = k+1 Otherwise If c(i-1,j) > c(i,j-1) Then c(i,j) = c(i-1,j) w(i,j) = 0 // no match at i, j Else c(i,j) = c(i,j-1) w(i,j) = 0 // no match at i, j (3) WLCS(X,Y) = c(m,n) Where c is the dynamic programming table, c(i,j) stores the WLCS score ending at word x i of X and y j of Y, w is the table storing the length of consecutive matches ended at c table position i and j, and f is a function of consecutive matches at the table position, c(i,j)." ></td>
	<td class="line x" title="101:211	Notice that by providing different weighting function f, we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches." ></td>
	<td class="line x" title="102:211	The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y. In other words, consecutive matches are awarded more scores than non-consecutive matches." ></td>
	<td class="line x" title="103:211	For example, f(k)-=-k   when k >= 0, and ,  > 0." ></td>
	<td class="line x" title="104:211	This function charges a gap penalty of  for each non-consecutive n-gram sequences." ></td>
	<td class="line x" title="105:211	Another possible function family is the polynomial family of the form k  where - > 1." ></td>
	<td class="line x" title="106:211	However, in order to normalize the final ROUGE-W score, we also prefer to have a function that has a close form inverse function." ></td>
	<td class="line x" title="107:211	For example, f(k)-=-k 2 has a close form inverse function f -1 (k)-=-k 1/2 . F-measure based on WLCS can be computed as follows, given two sequences X of length m and Y of length n: R wlcs         =  )( ),( 1 mf YXWLCS f (10) P wlcs         =  )( ),( 1 nf YXWLCS f (11) F wlcs wlcswlcs wlcswlcs PR PR 2 2 )1(   + + = (12) Where f -1 is the inverse function of f. We call the WLCS-based F-measure, i.e. Equation 12, ROUGE-W." ></td>
	<td class="line x" title="108:211	Using Equation 12 and f(k)-=-k 2 as the weighting function, the ROUGE-W scores for sequences Y 1 and Y 2 are 0.571 and 0.286 respectively." ></td>
	<td class="line x" title="109:211	Therefore, Y 1 would be ranked higher than Y 2 using WLCS." ></td>
	<td class="line x" title="110:211	We use the polynomial function of the form k  in the ROUGE evaluation package." ></td>
	<td class="line x" title="111:211	In the next section, we introduce the skip-bigram cooccurrence statistics." ></td>
	<td class="line x" title="112:211	4 ROUGE-S: Skip-Bigram Co-Occurrence Statistics Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps." ></td>
	<td class="line x" title="113:211	Skipbigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations." ></td>
	<td class="line x" title="114:211	Using the example given in Section 3.1: S1." ></td>
	<td class="line x" title="115:211	police killed the gunman S2." ></td>
	<td class="line x" title="116:211	police kill the gunman S3." ></td>
	<td class="line x" title="117:211	the gunman kill police S4." ></td>
	<td class="line x" title="118:211	the gunman police killed Each sentence has C(4,2) 3 = 6 skip-bigrams." ></td>
	<td class="line x" title="119:211	For example, S1 has the following skip-bigrams: 3 Combination: C(4,2) = 4!/(2!*2)!" ></td>
	<td class="line x" title="120:211	= 6." ></td>
	<td class="line x" title="121:211	(police killed, police the, police gunman, killed the, killed gunman, the gunman) S2 has three skip-bigram matches with S1 (police the, police gunman, the gunman), S3 has one skip-bigram match with S1 (the gunman), and S4 has two skip-bigram matches with S1 (police killed, the gunman)." ></td>
	<td class="line x" title="122:211	Given translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, we compute skip-bigram-based F-measure as follows: R skip2 )2,( ),(2 mC YXSKIP = (13) P skip2 )2,( ),(2 nC YXSKIP = (14) F skip2 2 2 2 22 2 )1( skipskip skipskip PR PR   + + = (15) Where SKIP2(X,Y) is the number of skip-bigram matches between X and Y,  = P skip2 /R skip2 when F skip2 /R skip2 _=_F skip2 /P skip2, and C is the combination function." ></td>
	<td class="line x" title="123:211	We call the skip-bigram-based Fmeasure, i.e. Equation 15, ROUGE-S." ></td>
	<td class="line x" title="124:211	Using Equation 15 with  = 1 and S1 as the reference, S2s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333." ></td>
	<td class="line x" title="125:211	Therefore, S2 is better than S3 and S4, and S4 is better than S3." ></td>
	<td class="line x" title="126:211	This result is more intuitive than using BLEU-2 and ROUGE-L." ></td>
	<td class="line x" title="127:211	One advantage of skip-bigram vs. BLEU is that it does not require consecutive matches but is still sensitive to word order." ></td>
	<td class="line x" title="128:211	Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence." ></td>
	<td class="line x" title="129:211	We can limit the maximum skip distance, d skip, between two in-order words that is allowed to form a skip-bigram." ></td>
	<td class="line x" title="130:211	Applying such constraint, we limit skip-bigram formation to a fix window size." ></td>
	<td class="line x" title="131:211	Therefore, computation time can be reduced and hopefully performance can be as good as the version without such constraint." ></td>
	<td class="line x" title="132:211	For example, if we set d skip to 0 then ROUGE-S is equivalent to bigram overlap." ></td>
	<td class="line x" title="133:211	If we set d skip to 4 then only word pairs of at most 4 words apart can form skip-bigrams." ></td>
	<td class="line x" title="134:211	Adjusting Equations 13, 14, and 15 to use maximum skip distance limit is straightforward: we only count the skip-bigram matches, SKIP2(X,Y), within the maximum skip distance and replace denominators of Equations 13, C(m,2), and 14, C(n,2), with the actual numbers of within distance skip-bigrams from the reference and the candidate respectively." ></td>
	<td class="line x" title="135:211	In the next section, we present the evaluations of ROUGE-L, ROUGE-S, and compare their performance with other automatic evaluation measures." ></td>
	<td class="line x" title="136:211	5 Evaluations One of the goals of developing automatic evaluation measures is to replace labor-intensive human evaluations." ></td>
	<td class="line x" title="137:211	Therefore the first criterion to assess the usefulness of an automatic evaluation measure is to show that it correlates highly with human judgments in different evaluation settings." ></td>
	<td class="line x" title="138:211	However, high quality large-scale human judgments are hard to come by." ></td>
	<td class="line x" title="139:211	Fortunately, we have access to eight MT systems outputs, their human assessment data, and the reference translations from 2003 NIST Chinese MT evaluation (NIST 2002a)." ></td>
	<td class="line x" title="140:211	There were 919 sentence segments in the corpus." ></td>
	<td class="line x" title="141:211	We first computed averages of the adequacy and fluency scores of each system assigned by human evaluators." ></td>
	<td class="line x" title="142:211	For the input of automatic evaluation methods, we created three evaluation sets from the MT outputs: 1." ></td>
	<td class="line x" title="143:211	Case set: The original system outputs with case information." ></td>
	<td class="line x" title="144:211	2." ></td>
	<td class="line x" title="145:211	NoCase set: All words were converted into lower case, i.e. no case information was used." ></td>
	<td class="line x" title="146:211	This set was used to examine whether human assessments were affected by case information since not all MT systems generate properly cased output." ></td>
	<td class="line x" title="147:211	3." ></td>
	<td class="line x" title="148:211	Stem set: All words were converted into lower case and stemmed using the Porter stemmer (Porter 1980)." ></td>
	<td class="line o" title="149:211	Since ROUGE computed similarity on surface word level, stemmed version allowed ROUGE to perform more lenient matches." ></td>
	<td class="line x" title="150:211	To accommodate multiple references, we use a Jackknifing procedure." ></td>
	<td class="line x" title="151:211	Given N references, we compute the best score over N sets of N-1 references." ></td>
	<td class="line x" title="152:211	The final score is the average of the N best scores using N different sets of N-1 references." ></td>
	<td class="line x" title="153:211	The Jackknifing procedure is adopted since we often need to compare system and human performance and the reference translations are usually the only human translations available." ></td>
	<td class="line x" title="154:211	Using this procedure, we are able to estimate average human performance by averaging N best scores of one reference vs. the rest N-1 references." ></td>
	<td class="line x" title="155:211	We then computed average BLEU1-12 4, GTM with exponents of 1.0, 2.0, and 3.0, NIST, WER, and PER scores over these three sets." ></td>
	<td class="line x" title="156:211	Finally we applied ROUGE-L, ROUGE-W with weighting function k 1.2, and ROUGE-S without skip distance 4 BLEUN computes BLEU over n-grams up to length N. Only BLEU1, BLEU4, and BLEU12 are shown in Table 1." ></td>
	<td class="line x" title="157:211	limit and with skip distant limits of 0, 4, and 9." ></td>
	<td class="line x" title="158:211	Correlation analysis based on two different correlation statistics, Pearsons  and Spearmans , with respect to adequacy and fluency are shown in Table 1." ></td>
	<td class="line x" title="159:211	The Pearsons correlation coefficient 5 measures the strength and direction of a linear relationship between any two variables, i.e. automatic metric score and human assigned mean coverage score in our case." ></td>
	<td class="line x" title="160:211	It ranges from +1 to -1." ></td>
	<td class="line x" title="161:211	A correlation of 1 means that there is a perfect positive linear relationship between the two variables, a correlation of -1 means that there is a perfect negative linear relationship between them, and a correlation of 0 means that there is no linear relationship between them." ></td>
	<td class="line x" title="162:211	Since we would like to use automatic evaluation metric not only in comparing systems 5 For a quick overview of the Pearsons coefficient, see: http://davidmlane.com/hyperstat/A34739.html." ></td>
	<td class="line x" title="163:211	but also in in-house system development, a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores." ></td>
	<td class="line x" title="164:211	Therefore, Pearsons correlation coefficient is a good measure to look at." ></td>
	<td class="line x" title="165:211	Spearmans correlation coefficient 6 is also a measure of correlation between two variables." ></td>
	<td class="line x" title="166:211	It is a non-parametric measure and is a special case of the Pearsons correlation coefficient when the values of data are converted into ranks before computing the coefficient." ></td>
	<td class="line x" title="167:211	Spearmans correlation coefficient does not assume the correlation between the variables is linear." ></td>
	<td class="line x" title="168:211	Therefore it is a useful correlation indicator even when good linear correlation, for example, according to Pearsons correlation coefficient between two variables could 6 For a quick overview of the Spearmans coefficient, see: http://davidmlane.com/hyperstat/A62436.html." ></td>
	<td class="line x" title="169:211	Adequacy Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U BLEU1 0.86 0.83 0.89 0.80 0.71 0.90 0.87 0.84 0.90 0.76 0.67 0.89 0.91 0.89 0.93 0.85 0.76 0.95 BLEU4 0.77 0.72 0.81 0.77 0.71 0.89 0.79 0.75 0.82 0.67 0.55 0.83 0.82 0.78 0.85 0.76 0.67 0.89 BLEU12 0.66 0.60 0.72 0.53 0.44 0.65 0.72 0.57 0.81 0.65 0.25 0.88 0.72 0.58 0.81 0.66 0.28 0.88 NIST 0.89 0.86 0.92 0.78 0.71 0.89 0.87 0.85 0.90 0.80 0.74 0.92 0.90 0.88 0.93 0.88 0.83 0.97 WER 0.47 0.41 0.53 0.56 0.45 0.74 0.43 0.37 0.49 0.66 0.60 0.82 0.48 0.42 0.54 0.66 0.60 0.81 PER 0.67 0.62 0.72 0.56 0.48 0.75 0.63 0.58 0.68 0.67 0.60 0.83 0.72 0.68 0.76 0.69 0.62 0.86 ROUGE-L 0.87 0.84 0.90 0.84 0.79 0.93 0.89 0.86 0.92 0.84 0.71 0.94 0.92 0.90 0.94 0.87 0.76 0.95 ROUGE-W 0.84 0.81 0.87 0.83 0.74 0.90 0.85 0.82 0.88 0.77 0.67 0.90 0.89 0.86 0.91 0.86 0.76 0.95 ROUGE-S* 0.85 0.81 0.88 0.83 0.76 0.90 0.90 0.88 0.93 0.82 0.70 0.92 0.95 0.93 0.97 0.85 0.76 0.94 ROUGE-S0 0.82 0.78 0.85 0.82 0.71 0.90 0.84 0.81 0.87 0.76 0.67 0.90 0.87 0.84 0.90 0.82 0.68 0.90 ROUGE-S4 0.82 0.78 0.85 0.84 0.79 0.93 0.87 0.85 0.90 0.83 0.71 0.90 0.92 0.90 0.94 0.84 0.74 0.93 ROUGE-S9 0.84 0.80 0.87 0.84 0.79 0.92 0.89 0.86 0.92 0.84 0.76 0.93 0.94 0.92 0.96 0.84 0.76 0.94 GTM10 0.82 0.79 0.85 0.79 0.74 0.83 0.91 0.89 0.94 0.84 0.79 0.93 0.94 0.92 0.96 0.84 0.79 0.92 GTM20 0.77 0.73 0.81 0.76 0.69 0.88 0.79 0.76 0.83 0.70 0.55 0.83 0.83 0.79 0.86 0.80 0.67 0.90 GTM30 0.74 0.70 0.78 0.73 0.60 0.86 0.74 0.70 0.78 0.63 0.52 0.79 0.77 0.73 0.81 0.64 0.52 0.80 Fluency Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U BLEU1 0.81 0.75 0.86 0.76 0.62 0.90 0.73 0.67 0.79 0.70 0.62 0.81 0.70 0.63 0.77 0.79 0.67 0.90 BLEU4 0.86 0.81 0.90 0.74 0.62 0.86 0.83 0.78 0.88 0.68 0.60 0.81 0.83 0.78 0.88 0.70 0.62 0.81 BLEU12 0.87 0.76 0.93 0.66 0.33 0.79 0.93 0.81 0.97 0.78 0.44 0.94 0.93 0.84 0.97 0.80 0.49 0.94 NIST 0.81 0.75 0.87 0.74 0.62 0.86 0.70 0.64 0.77 0.68 0.60 0.79 0.68 0.61 0.75 0.77 0.67 0.88 WER 0.69 0.62 0.75 0.68 0.57 0.85 0.59 0.51 0.66 0.70 0.57 0.82 0.60 0.52 0.68 0.69 0.57 0.81 PER 0.79 0.74 0.85 0.67 0.57 0.82 0.68 0.60 0.73 0.69 0.60 0.81 0.70 0.63 0.76 0.65 0.57 0.79 ROUGE-L 0.83 0.77 0.88 0.80 0.67 0.90 0.76 0.69 0.82 0.79 0.64 0.90 0.73 0.66 0.80 0.78 0.67 0.90 ROUGE-W 0.85 0.80 0.90 0.79 0.63 0.90 0.78 0.73 0.84 0.72 0.62 0.83 0.77 0.71 0.83 0.78 0.67 0.90 ROUGE-S* 0.84 0.78 0.89 0.79 0.62 0.90 0.80 0.74 0.86 0.77 0.64 0.90 0.78 0.71 0.84 0.79 0.69 0.90 ROUGE-S0 0.87 0.81 0.91 0.78 0.62 0.90 0.83 0.78 0.88 0.71 0.62 0.82 0.82 0.77 0.88 0.76 0.62 0.90 ROUGE-S4 0.84 0.79 0.89 0.80 0.67 0.90 0.82 0.77 0.87 0.78 0.64 0.90 0.81 0.75 0.86 0.79 0.67 0.90 ROUGE-S9 0.84 0.79 0.89 0.80 0.67 0.90 0.81 0.76 0.87 0.79 0.69 0.90 0.79 0.73 0.85 0.79 0.69 0.90 GTM10 0.73 0.66 0.79 0.76 0.60 0.87 0.71 0.64 0.78 0.80 0.67 0.90 0.66 0.58 0.74 0.80 0.64 0.90 GTM20 0.86 0.81 0.90 0.80 0.67 0.90 0.83 0.77 0.88 0.69 0.62 0.81 0.83 0.77 0.87 0.74 0.62 0.89 GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83 With Case Information (Case) Lower Case (NoCase) Lower Case & Stemmed (Stem) With Case Information (Case) Lower Case (NoCase) Lower Case & Stemmed (Stem) Table 1." ></td>
	<td class="line x" title="170:211	Pearsons  and Spearmans  correlations of automatic evaluation measures vs. adequacy and fluency: BLEU1, 4, and 12 are BLEU with maximum of 1, 4, and 12 grams, NIST is the NIST score, ROUGE-L is LCS-based F-measure ( = 1), ROUGE-W is weighted LCS-based F-measure ( = 1)." ></td>
	<td class="line x" title="171:211	ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGESN is skip-bigram-based F-measure ( = 1) with maximum skip distance of N, PER is position independent word error rate, and WER is word error rate." ></td>
	<td class="line x" title="172:211	GTM 10, 20, and 30 are general text matcher with exponents of 1.0, 2.0, and 3.0." ></td>
	<td class="line x" title="173:211	(Note, only BLEU1, 4, and 12 are shown here to preserve space)." ></td>
	<td class="line x" title="174:211	not be found." ></td>
	<td class="line x" title="175:211	It also suits the NIST MT evaluation scenario where multiple systems are ranked according to some performance metrics." ></td>
	<td class="line x" title="176:211	To estimate the significance of these correlation statistics, we applied bootstrap resampling, generating random samples of the 919 different sentence segments." ></td>
	<td class="line x" title="177:211	The lower and upper values of 95% confidence interval are also shown in the table." ></td>
	<td class="line x" title="178:211	Dark (green) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories." ></td>
	<td class="line x" title="179:211	Analyzing all runs according to the adequacy and fluency table, we make the following observations: Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for BLEU7-12 (only BLEU12 is shown)." ></td>
	<td class="line x" title="180:211	For example, the Pearsons  (P) correlation of ROUGE-S* with adequacy increases from 0.85 (Case) to 0.95 (Stem) while its Pearsons  correlation with fluency drops from 0.84 (Case) to 0.78 (Stem)." ></td>
	<td class="line x" title="181:211	We will focus our discussions on the Stem set in adequacy and Case set in fluency." ></td>
	<td class="line x" title="182:211	The Pearson's  correlation values in the Stem set of the Adequacy Table, indicates that ROUGEL and ROUGE-S with a skip distance longer than 0 correlate highly and linearly with adequacy and outperform BLEU and NIST." ></td>
	<td class="line x" title="183:211	ROUGE-S* achieves that best correlation with a Pearsons  of 0.95." ></td>
	<td class="line x" title="184:211	Measures favoring consecutive matches, i.e. BLEU4 and 12, ROUGE-W, GTM20 and 30, ROUGE-S0 (bigram), and WER have lower Pearsons  . Among them WER (0.48) that tends to penalize small word movement is the worst performer." ></td>
	<td class="line x" title="185:211	One interesting observation is that longer BLEU has lower correlation with adequacy." ></td>
	<td class="line x" title="186:211	Spearmans  values generally agree with Pearson's  but have more equivalents." ></td>
	<td class="line x" title="187:211	The Pearson's  correlation values in the Stem set of the Fluency Table, indicates that BLEU12 has the highest correlation (0.93) with fluency." ></td>
	<td class="line x" title="188:211	However, it is statistically indistinguishable with 95% confidence from all other metrics shown in the Case set of the Fluency Table except for WER and GTM10." ></td>
	<td class="line x" title="189:211	GTM10 has good correlation with human judgments in adequacy but not fluency; while GTM20 and GTM30, i.e. GTM with exponent larger than 1.0, has good correlation with human judgment in fluency but not adequacy." ></td>
	<td class="line x" title="190:211	ROUGE-L and ROUGE-S*, 4, and 9 are good automatic evaluation metric candidates since they perform as well as BLEU in fluency correlation analysis and outperform BLEU4 and 12 significantly in adequacy." ></td>
	<td class="line x" title="191:211	Among them, ROUGE-L is the best metric in both adequacy and fluency correlation with human judgment according to Spearmans correlation coefficient and is statistically indistinguishable from the best metrics in both adequacy and fluency correlation with human judgment according to Pearsons correlation coefficient." ></td>
	<td class="line x" title="192:211	6 Conclusion In this paper we presented two new objective automatic evaluation methods for machine translation, ROUGE-L based on longest common subsequence (LCS) statistics between a candidate translation and a set of reference translations." ></td>
	<td class="line x" title="193:211	Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence ngrams automatically while this is a free parameter in BLEU." ></td>
	<td class="line x" title="194:211	To give proper credit to shorter common sequences that are ignored by LCS but still retain the flexibility of non-consecutive matches, we proposed counting skip bigram co-occurrence." ></td>
	<td class="line o" title="195:211	The skip-bigram-based ROUGE-S* (without skip distance restriction) had the best Pearson's  correlation of 0.95 in adequacy when all words were lower case and stemmed." ></td>
	<td class="line o" title="196:211	ROUGE-L, ROUGE-W, ROUGE-S*, ROUGE-S4, and ROUGE-S9 were equal performers to BLEU in measuring fluency." ></td>
	<td class="line x" title="197:211	However, they have the advantage that we can apply them on sentence level while longer BLEU such as BLEU12 would not differentiate any sentences with length shorter than 12 words (i.e. no 12-gram matches)." ></td>
	<td class="line x" title="198:211	We plan to explore their correlation with human judgments on sentence-level in the future." ></td>
	<td class="line x" title="199:211	We also confirmed empirically that adequacy and fluency focused on different aspects of machine translations." ></td>
	<td class="line x" title="200:211	Adequacy placed more emphasis on terms co-occurred in candidate and reference translations as shown in the higher correlations in Stem set than Case set in Table 1; while the reverse was true in the terms of fluency." ></td>
	<td class="line p" title="201:211	The evaluation results of ROUGE-L, ROUGEW, and ROUGE-S in machine translation evaluation are very encouraging." ></td>
	<td class="line x" title="202:211	However, these measures in their current forms are still only applying string-to-string matching." ></td>
	<td class="line x" title="203:211	We have shown that better correlation with adequacy can be reached by applying stemmer." ></td>
	<td class="line x" title="204:211	In the next step, we plan to extend them to accommodate synonyms and paraphrases." ></td>
	<td class="line x" title="205:211	For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms." ></td>
	<td class="line x" title="206:211	Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003)." ></td>
	<td class="line x" title="207:211	Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches." ></td>
	<td class="line x" title="208:211	In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased." ></td>
	<td class="line pc" title="209:211	ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004)." ></td>
	<td class="line oc" title="210:211	In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement." ></td>
	<td class="line p" title="211:211	According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1019
Kernel-Based Approach For Automatic Evaluation Of Natural Language Generation Technologies: Application To Automatic Summarization
Hirao, Tsutomu;Okumura, Manabu;Isozaki, Hideki;"></td>
	<td class="line x" title="1:179	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 145152, Vancouver, October 2005." ></td>
	<td class="line x" title="2:179	c2005 Association for Computational Linguistics Kernel-based Approach for Automatic Evaluation of Natural Language Generation Technologies: Application to Automatic Summarization Tsutomu Hirao NTT Communication Science Labs." ></td>
	<td class="line x" title="3:179	NTT Corp. hirao@cslab.kecl.ntt.co.jp Manabu Okumura Precision and Intelligence Labs." ></td>
	<td class="line x" title="4:179	Tokyo Institute of Technology oku@pi.titech.ac.jp Hideki Isozaki NTT Communication Science Labs." ></td>
	<td class="line x" title="5:179	NTT Corp. isozaki@cslab.kecl.ntt.co.jp Abstract In order to promote the study of automatic summarization and translation, we need an accurate automatic evaluation method that is close to human evaluation." ></td>
	<td class="line x" title="6:179	In this paper, we present an evaluation method that is based on convolution kernels that measure the similarities between texts considering their substructures." ></td>
	<td class="line x" title="7:179	We conducted an experiment using automatic summarization evaluation data developed for Text Summarization Challenge 3 (TSC-3)." ></td>
	<td class="line x" title="8:179	A comparison with conventional techniques shows that our method correlates more closely with human evaluations and is more robust." ></td>
	<td class="line x" title="9:179	1 Introduction Automatic summarization, machine translation, and paraphrasing have attracted much attention recently." ></td>
	<td class="line x" title="10:179	These tasks include text-to-text language generation." ></td>
	<td class="line x" title="11:179	Evaluation workshops are held in the U.S. and Japan, e.g., the Document Understanding Conference (DUC)1, NIST Machine Translation Evaluation2 as part of the TIDES project, the Text Summarization Challenge (TSC)3 of the NTCIR project, and the International Workshop on Spoken Language Translation (IWSLT)4." ></td>
	<td class="line x" title="12:179	These evaluation workshops employ human evaluations, which are essential in terms of achieving 1http://duc.nist.gov 2http://www.nist.gov/speech/tests/mt/ 3http://www.lr.titech.ac.jp/tsc 4http://www.slt.atr.co.jp/IWSLT2004 high quality evaluations results." ></td>
	<td class="line x" title="13:179	However, human evaluations require a huge effort and the cost is considerable." ></td>
	<td class="line x" title="14:179	Moreover, we cannot automatically evaluate a new system even if we use the corpora built for these workshops, and we cannot conduct reevaluation experiments." ></td>
	<td class="line x" title="15:179	To cope with this situation, there is a particular need to establish a high quality automatic evaluation method." ></td>
	<td class="line x" title="16:179	Once this is done, we can expect great progress to be made on natural language generation." ></td>
	<td class="line x" title="17:179	In this paper, we propose a novel automatic evaluation method for natural language generation technologies." ></td>
	<td class="line x" title="18:179	Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al. , 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001)." ></td>
	<td class="line x" title="19:179	ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations." ></td>
	<td class="line x" title="20:179	We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al. , 2004a)." ></td>
	<td class="line oc" title="21:179	The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust." ></td>
	<td class="line x" title="22:179	2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes." ></td>
	<td class="line oc" title="23:179	One is the longest common subsequence (LCS) based approach (Hori et al. , 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004)." ></td>
	<td class="line x" title="24:179	The other is the N-gram based approach (Papineni et al. , 145 Table 1: Components of vectors corresponding to S1 and S2." ></td>
	<td class="line x" title="25:179	Bold subsequences are common to S1 and S2." ></td>
	<td class="line oc" title="26:179	a0 subsequence S1 S2 a0 subsequence S1 S2 a0 subsequence S1 S2 Becoming 1 1 Becoming-is a1 a2 a1 a2 astronaut-DREAM 0 a1 a2 DREAM 1 1 Becoming-my a1a4a3a5a1a4a3 astronaut-ambition 0 a1 a2 SPACEMAN 1 1 SPACEMAN-DREAM a1a4a3a5a1 a2 astronaut-is 0 1 a 1 0 SPACEMAN-ambition 0 a1 a2 astronaut-my 0 a1 ambition 0 1 SPACEMAN-dream a1 a3 0 cosmonaut-DREAM a1 a3 0 1 an 0 1 SPACEMAN-great a1 a2 0 cosmonaut-dream a1 a3 0 astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great a1 a2 0 cosmonaut 1 0 SPACEMAN-my a1a6a1 cosmonaut-is 1 0 dream 1 0 a-DREAM a1 a7 0 cosmonaut-my a1 0great 1 0 a-SPACEMAN 1 0 great-DREAM 1 0 is 1 1 2 a-cosmonaut 1 0 2 great-dream 1 0 my 1 1 a-dream a1 a7 0 is-DREAM a1 a2 a1 Becoming-DREAM a1a4a8a5a1 a7 a-great a1 a3 0 is-ambition 0 a1 Becoming-SPACEMAN a1a6a1 a-is a1 0 is-dream a1 a2 0 Becoming-a 1 0 a-my a1 a2 0 is-great a1 0 Becoming-ambition 0 a1 a7 an-DREAM 0 a1 a3 is-my 1 1 2 Becoming-an 0 1 an-SPACEMAN 0 1 my-DREAM a1 1 Becoming-astronaut 0 a1 an-ambition 0 a1 a3 my-ambition 0 1 Becoming-cosmonaut a1 0 an-astronaut 0 1 my-dream a1 0 Becoming-dream a1a4a8 0 an-is 0 a1 my-great 1 0 Becoming-great a1 a7 0 an-my 0 a1 a2 2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004)." ></td>
	<td class="line x" title="27:179	Hori et." ></td>
	<td class="line x" title="28:179	al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy." ></td>
	<td class="line x" title="29:179	They reported that their method is superior to BLEU (Papineni et al. , 2002) in terms of the correlation between human assessment and automatic evaluation." ></td>
	<td class="line oc" title="30:179	Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L." ></td>
	<td class="line o" title="31:179	They applied ROUGE-L to the evaluation of summarization and machine translation." ></td>
	<td class="line x" title="32:179	The results showed that the LCS-based measure is comparable to Ngram-based automatic evaluation methods." ></td>
	<td class="line x" title="33:179	However, these methods tend to be strongly influenced by word order." ></td>
	<td class="line x" title="34:179	Various N-gram-based methods have been proposed since BLEU, which is now widely used for the evaluation of machine translation." ></td>
	<td class="line x" title="35:179	Lin et al.(2003) proposed a recall-oriented measure, ROUGE-N, whereas BLEU is precision-oriented." ></td>
	<td class="line x" title="37:179	They reported that ROUGE-N performed well as regards automatic summarization." ></td>
	<td class="line o" title="38:179	In particular, ROUGE-1, i.e., unigram matching, provides the best correlation with human evaluation." ></td>
	<td class="line x" title="39:179	Soricut et." ></td>
	<td class="line x" title="40:179	al (2004) proposed a unified measure." ></td>
	<td class="line x" title="41:179	They integrated a precisionoriented measure with a recall-oriented measure by using an extension of the harmonic mean formula." ></td>
	<td class="line x" title="42:179	It performs well in evaluations of machine translation, automatic summarization, and question answering." ></td>
	<td class="line x" title="43:179	However, N-gram based methods have a critical problem; they cannot consider co-occurrences with gaps, although the LCS-based method can deal with them." ></td>
	<td class="line oc" title="44:179	Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation." ></td>
	<td class="line x" title="45:179	However, they did not consider longer skip-n-grams such as skip-trigrams." ></td>
	<td class="line x" title="46:179	Moreover, their method does not distinguish between bigrams and skip-bigrams." ></td>
	<td class="line x" title="47:179	3 Kernel-based Automatic Evaluation The above N-gram-based methods correlated closely with human evaluations." ></td>
	<td class="line x" title="48:179	However, we think some skip-n-grams (na9a11a10 ) are useful." ></td>
	<td class="line x" title="49:179	In this paper, we employ the Extended String Subsequence Kernel (ESK), which considers both n-grams and skip-n-grams." ></td>
	<td class="line x" title="50:179	In addition, the ESK allows us to add word senses to each word." ></td>
	<td class="line x" title="51:179	The use of word senses enables flexible matching even when paraphrasing is used." ></td>
	<td class="line x" title="52:179	The ESK is a kind of convolution kernel (Collins and Duffy, 2001)." ></td>
	<td class="line x" title="53:179	Convolution kernels have recently attracted attention as a novel similarity measure in natural language processing." ></td>
	<td class="line x" title="54:179	3.1 ESK The ESK is an extension of the String Subsequence Kernel (SSK) (Lodhi et al. , 2002) and the Word Sequence Kernel (WSK) (Cancedda et al. , 2003)." ></td>
	<td class="line x" title="55:179	The ESK receives two node sequences as inputs 146 and maps each of them into a high-dimensional vector space." ></td>
	<td class="line x" title="56:179	The kernels value is simply the inner product of the two vectors in the vector space." ></td>
	<td class="line x" title="57:179	In order to discount long-skip-n-grams, the decay parameter a12 is introduced." ></td>
	<td class="line x" title="58:179	We explain the computation of the ESKs value whose inputs are the sentences (S1 and S2) shown below." ></td>
	<td class="line x" title="59:179	In the example, word senses are shown in braces." ></td>
	<td class="line x" title="60:179	S1 Becoming a cosmonaut:a13 SPACEMANa14 is my great dream:a13 DREAMa14 S2 Becoming an astronaut:a13 SPACEMANa14 is my ambition:a13 DREAMa14 In this case, cosmonaut and astronaut share the same sense a15 SPACEMANa16 and ambition and dream also share the same sense a15 DREAMa16." ></td>
	<td class="line x" title="61:179	We can use WordNet for English and Goitaikei (Ikehara et al. , 1997) for Japanese." ></td>
	<td class="line x" title="62:179	Table 1 shows the subsequences derived from S1 and S2 and its weights." ></td>
	<td class="line x" title="63:179	Note that the subsequence length is two or less." ></td>
	<td class="line x" title="64:179	From the table, there are fifteen subsequences5 that are common to S1 and S2." ></td>
	<td class="line x" title="65:179	Therefore, a17a19a18a21a20a23a22a25a24a27a26a4a28a29a18a31a30a33a32a34a18a21a35a37a36a39a38a41a40a23a42a43a12a44a42a45a35a37a12 a26 a42a43a12a47a46a23a42 a12a47a48a49a42a41a12a51a50a49a42a52a12a51a53a49a42a54a12a51a55 . For reference, there are three unigrams, one bigram, zero trigrams and three skipbigrams common to S1 and S2." ></td>
	<td class="line x" title="66:179	Formally, the ESK is defined as follows." ></td>
	<td class="line x" title="67:179	a56 and a57 are node sequences." ></td>
	<td class="line x" title="68:179	ESKa58a29a59a61a60a63a62a29a64a66a65a68a67 a58 a69a71a70a73a72 a74a76a75a78a77a80a79 a81a83a82a83a77a85a84 a86 a69 a59a61a87a89a88a90a62a90a91a47a92a83a65 (1) a86 a69 a59a61a87a93a88a90a62a90a91a47a92a94a65a68a67 a95a34a96 a97 a59a61a87a89a88a98a62a90a91a47a92a29a65 if a99a100a67a102a101 a86a104a103 a69a106a105a27a72 a59a61a87a93a88a90a62a90a91a51a92a29a65a47a107 a95a34a96 a97 a59a61a87a93a88a90a62a90a91a51a92a29a65 otherwise (2) Here, a108 is the upper bound of the subsequence length and a109a111a110a112 a28a78a113a29a114a93a32a93a115a117a116a118a36 is defined as follows." ></td>
	<td class="line x" title="69:179	a113a94a114 is the a119 -th node of a56 . a115a120a116 is the a121 -th node of a57 . The function a122a120a123a27a124 a28a29a125a25a32a93a113a34a36 returns the number of attributes common to given nodes a125 and a113 . a86 a103 a69 a59a61a87a93a88a90a62a90a91a51a92a83a65a68a67 a126 if a127a85a67a39a101 a1 a86 a103 a69 a59a61a87 a88 a62a98a91 a92 a105a47a72 a65a51a128 a86 a103a103 a69 a59a61a87 a88 a62a90a91 a92 a105a27a72 a65 otherwise (3) a109a111a110a110a112 a28a78a113a94a114a93a32a93a115a120a116a25a36 is defined as follows: a86 a103a103 a69 a59a61a87a93a88a90a62a90a91a51a92a83a65a68a67 a126 if a129a130a67a102a101 a1 a86a104a103a103 a69 a59a61a87a93a88 a105a27a72 a62a90a91a47a92a83a65a51a128 a86 a69 a59a61a87a93a88 a105a27a72 a62a98a91a47a92a83a65a132a131 (4) 5Bold subsequences in Table 1." ></td>
	<td class="line x" title="70:179	Finally, we define the similarity measure between a56 and a57 by normalizing ESK." ></td>
	<td class="line x" title="71:179	This similarity can be regarded as an extension of the cosine measure." ></td>
	<td class="line x" title="72:179	Sima58a133a90a134a136a135 a59a61a60a63a62a94a64a66a65a68a67 ESKa58 a59a61a60a137a62a93a64a66a65 ESKa58 a59a61a60a63a62a90a60a19a65 ESKa58 a59a132a64a138a62a94a64a66a65 a131 (5) 3.2 Automatic Evaluation based on ESK Suppose, a139 is a system output, which consists of a140 sentences, and a141 is a human written reference, which consists of a142 sentences." ></td>
	<td class="line x" title="73:179	a143a94a114 is a sentence in a139, and a144 a116 is a sentence in a141 . We define two scoring functions for automatic evaluation." ></td>
	<td class="line x" title="74:179	First, we define a precision-oriented measure as follows: a145 a58 a133a90a134a136a135 a59a61a146a31a62a90a147a49a65a68a67 a101 a148 a149 a88 a70a73a72a151a150a49a152a132a153 a72a68a154 a92 a154a4a69 Sima58a133a98a134a61a135 a59a61a155 a88 a62a90a156 a92 a65 (6) Symmetrically, we define a recall-oriented measure as follows: a157 a58 a133a98a134a61a135 a59a61a146a31a62a98a147a49a65a68a67 a101 a99 a69 a92 a70a73a72 a150a49a152a132a153 a72a68a154 a88 a154 a149 Sima58a133a90a134a136a135 a59a61a155a158a88a78a62a98a156a159a92a29a65 (7) Finally, we define a unified measure, i.e., Fmeasure, as follows: a160 a58 a133a98a134a61a135 a59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a117a163 a157 a133a90a134a136a135 a59a61a146a31a62a90a147a49a65a117a163 a145 a133a90a134a136a135 a59a61a146a31a62a164a147a49a65 a157 a133a98a134a61a135 a59a61a146a31a62a164a147a49a65a47a128a49a162 a2 a163 a145 a133a98a134a61a135 a59a61a146a31a62a164a147a49a65 (8) a165 is a cost parameter for a166a168a167a132a169a78a170 and a171a138a167a132a169a78a170 . a165 s value is selected depending on the evaluation task." ></td>
	<td class="line x" title="75:179	Since summary should not miss important information given in the human reference, recall is more important than precision." ></td>
	<td class="line x" title="76:179	Therefore,a large a165 will yield good results." ></td>
	<td class="line x" title="77:179	3.3 Extension for Multiple References When multiple human references (correct answers) are available, we define a simple function for multiple references as follows: a160a23a172 a133a90a173a68a174 a133a98a134a61a135 a59a61a146a31a62 a157 a65a68a67 a101 a175 a176 a88 a70a73a72 a160 a133a90a134a61a135 a59a61a146a31a62a90a147 a88 a65a132a62 (9) Here, equation (9) gives the average score." ></td>
	<td class="line x" title="78:179	a166 indicates a set of references; a166a177a38a54a15a159a141a179a178a80a32a34a180a34a180a34a180a85a32a93a141a182a181a138a16 . 4 Experimental Evaluation To confirm and discuss the effectiveness of our method, we conducted an experimental evaluation using TSC-3 multiple document summarization 147 evaluation data and our additional data." ></td>
	<td class="line x" title="79:179	4.1 Task and Evaluation Metrics in TSC-3 The task of TSC-3 is multiple document summarization." ></td>
	<td class="line x" title="80:179	Participants were given a set of documents about a certain event and required to generate two different length summaries for the entire document set." ></td>
	<td class="line x" title="81:179	The lengths were about 5% and 10% of the total number of characters in the document set, respectively." ></td>
	<td class="line x" title="82:179	Thirty document sets were provided for the official run evaluation." ></td>
	<td class="line x" title="83:179	There were ten participant systems; one provided by the TSC organizers as a baseline system." ></td>
	<td class="line x" title="84:179	The evaluation metric follows DUCs SEE evaluation scheme (Harman and Over, 2004)." ></td>
	<td class="line x" title="85:179	For each document set, one human subject makes a reference summary and uses it as a basis for evaluating ten system outputs." ></td>
	<td class="line x" title="86:179	This human evaluation procedure consists of the following steps: Step 1 For each reference sentence a144a183a116a85a28a29a184a185a141a179a36, repeat Steps 2 and 3." ></td>
	<td class="line x" title="87:179	Step 2 For a144a25a116, the human assessor finds the most relevant sentence set a186 from the system output." ></td>
	<td class="line x" title="88:179	Step 3 The assessor assigns a score, a187a118a28a78a144a183a116a25a32a34a186a39a36, a188 a32 a188a37a189 a30a37a32a34a180a34a180a34a180a183a32a34a30 a189a76a188a37a189 1.0 means perfect." ></td>
	<td class="line x" title="89:179	in terms of how much of the content of a144a25a116 can be reproduced by using only sentences in a186 . Step 4 Finally, the evaluation score of output a139 for reference a141 is defined a190 a28a78a141a191a32a93a139a102a36a29a38 a116 a187a25a28a78a144 a116 a32a34a186a39a36a29a192a31a193a141a45a193." ></td>
	<td class="line x" title="90:179	The final score of a system is calculated by applying the above procedure and normalized by the number of topics, i.e., a46a29a194a195 a24 a178 a190 a28a78a141 a195 a32a93a139 a195 a36a29a192a37a10 a188 . When multiple references a166a11a28a29a38a49a15a159a141a191a30a37a32a34a180a34a180a34a180a183a32a93a141a111a181a138a16a37a36 are available, the scores are given as follows: a190a151a196 a167a130a197a29a198 a28a29a166a11a32a93a139a102a36a29a38 a199 a190 a28a78a141 a199 a32a93a139a102a36a29a192a31a193a166a5a193." ></td>
	<td class="line x" title="91:179	4.2 Variation of Human Assessors In TSC-3s official run evaluation, system outputs were compared with one human written reference summary for each topic." ></td>
	<td class="line x" title="92:179	There were five topic sets and five human assessors (A-E in Table 2) for each topic set." ></td>
	<td class="line x" title="93:179	Before we use the one human written reference summary as the gold-standard-reference, to examine variations among human assessors, we prepared two additional human summaries for each topic sets." ></td>
	<td class="line x" title="94:179	Table 2: The relationship between topics and reference summary creators, i.e., human assessors." ></td>
	<td class="line x" title="95:179	a186a39a28a29a200a201a36 indicates a subject As evaluation score for all systems for corresponding topics." ></td>
	<td class="line x" title="96:179	topic-ID a202 a72 a202 a2 a202 a3 a202a23a203a89a204a93a205 1 6 a206 (A) a206 (E) a206 (C) mean(a206 (A),a206 (E),a206 (C))7 12 a206 (B) a206 (A) a206 (D) mean(a206 (B),a206 (A),a206 (D))13 18 a206 (C) a206 (B) a206 (E) mean(a206 (C),a206 (B),a206 (E))19 24 a206 (D) a206 (C) a206 (A) mean(a206 (D),a206 (C),a206 (A))25 30 a206 (E) a206 (D) a206 (B) mean(a206 (E),a206 (D),a206 (B)) Table 3: Correlations between human judgments." ></td>
	<td class="line x" title="97:179	correlation rank correlationcoefficient ( a156 ) coefficient (a207 )short a202 a72 a202 a2 a202 a3 a202 a173a98a208a89a209 a202 a72 a202 a2 a202 a3 a202 a173a98a208a89a209 a202 a72 1.00 .968 .902 .988 1.00 .976 .697 .988 a202 a2 a210 1.00 .910 .996 a210 1.00 .733 .988 a202 a3 a210 a210 1.00 .914 a210 a210 1.00 .758 a202 a173a98a208a89a209 a210 a210 a210 1.00 a210 a210 a210 1.00 long a202 a72 a202 a2 a202 a3 a202 a173a98a208a89a209 a202 a72 a202 a2 a202 a3 a202 a173a98a208a89a209 a202 a72 1.00 .908 .822 .964 1.00 .964 .939 .964 a202 a2 a210 1.00 .963 .987 a210 1.00 .952 1.00 a202 a3 a210 a210 1.00 .931 a210 a210 1.00 .932 a202 a173a98a208a89a209 a210 a210 a210 1.00 a210 a210 a210 1.00 Therefore, we obtained three reference summaries and evaluation results for each topic sets (Table 2)." ></td>
	<td class="line x" title="98:179	Moreover, we prepared unified evaluation results of three human judgment as a211 a197a89a212a158a213, which is calculated as the average of three human scores." ></td>
	<td class="line x" title="99:179	The relationship between topics and human assessors is shown in Table 2." ></td>
	<td class="line x" title="100:179	For example, subject B generates summaries and evaluates all systems for topics 7-12, 13-18 and 25-30 on a211 a178, a211 a26, and a211 a46respectively." ></td>
	<td class="line x" title="101:179	Note that each human subject, A to E, was a retired professional journalist; that is, they shared a common background." ></td>
	<td class="line x" title="102:179	Table 3 shows the Pearsons correlation coefficient (a144 ) and Spearmans rank correlation coefficient a214 for the human subjects." ></td>
	<td class="line x" title="103:179	The results show that every pair has a high correlation." ></td>
	<td class="line x" title="104:179	Therefore, changing the human subject has little influence as regards creating references and evaluating system summaries." ></td>
	<td class="line x" title="105:179	The evaluation by human subjects is stable." ></td>
	<td class="line x" title="106:179	This result agrees with DUCs additional evaluation results (Harman and Over, 2004)." ></td>
	<td class="line x" title="107:179	However, the behavior of the correlations between humans with different backgrounds is uncertain." ></td>
	<td class="line x" title="108:179	The correlation might be fragile if we introduce a human subject whose background is different from the others." ></td>
	<td class="line o" title="109:179	148 4.3 Compared Automatic Evaluation Methods We compared our method with ROUGE-N and ROUGE-L described below." ></td>
	<td class="line o" title="110:179	We used only content words to calculate the ROUGE scores because the correlation coefficient decreased if we did not remove functional words." ></td>
	<td class="line x" title="111:179	WSK-based method We use WSK instead of ESK in equation (6)-(8)." ></td>
	<td class="line oc" title="112:179	ROUGE-N ROUGE-N is an N-gram-based evaluation measure defined as follows (Lin, 2004b): ROUGE-Na59a61a146a31a62a90a147a49a65a68a67 a215 a77a83a216 a209a68a217a61a173 a172a27a218 a77 a215 a219a27a220a158a221a183a222a85a223 a172 a173a78a224a76a225a164a226 a59a136a227a158a228 a152a130a150a104a229 a65 a215 a77a29a216 a209a68a217a76a173 a172 a218 a77 a215 a219a27a220a159a221a183a222a85a223 a59a136a227a158a228 a152a130a150 a229 a65 (10) Here, a230a66a231a37a232a21a233a27a234a118a28a78a235a37a236a25a237a37a238a11a239a168a36 is the number of an N-gram and a230a66a231a37a232a21a233a27a234 a196 a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36 denotes the number of ngram co-occurrences in a system output and the reference." ></td>
	<td class="line oc" title="113:179	ROUGE-S ROUGE-S is an extension of ROUGE-2 defined as follows (Lin, 2004b): ROUGE-Sa59a61a146a31a62a98a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a161a163 a157 a134a61a135a93a245a246 a2 a59a61a146a31a62a98a147a49a65a161a163 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a164a147a49a65 a157 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a51a128a104a162 a2 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a98a147a49a65 (11) Where a166a168a169a78a170a248a247a250a249 a26 and a171a138a169a90a170a158a247a250a249 a26 are defined as follows: a251 a134a61a135a89a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a248a253a85a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2a23a147 (12) a3 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a83a253a118a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2 a146 (13) Here, function Skip2 returns the number of skipbi-grams that are common to a141 and a139 . ROUGE-SU ROUGE-SU is an extension of ROUGE-S, which includes unigrams as a feature defined as follows (Lin, 2004b): ROUGE-SUa59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a117a163 a157 a134a5a4 a59a61a146a31a62a98a147a49a65a71a163 a145 a134a6a4 a59a61a146a31a62a98a147a49a65 a157 a134a5a4 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145 a134a5a4 a59a61a146a31a62a164a147a49a65 (14) Where a166 a169a8a7 and a171 a169a8a7 are defined as follows: a251 a134a5a4 a59a61a146a31a62a98a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 (# of skip bigrams + # of unigrams) a2 a147 (15) a3 a134a5a4 a59a61a146a31a62a90a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 (# of skip bigrams + # of unigrams) a2 a146 (16) Here, function SU returns the number of skip-bigrams and unigrams that are common to a141 and a139 . ROUGE-L ROUGE-L is an LCS-based evaluation measure defined as follows (Lin, 2004b): ROUGE-La59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a161a163 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a161a163 a145a12a10 a225a90a134 a59a61a146a31a62a98a147a49a65 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145a12a10 a225a98a134 a59a61a146a31a62a90a147a49a65 (17) where a166a14a13a250a241a132a169 and a171a15a13a250a241a130a169 are defined as follows: a157a11a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a91 a16 a75 a77a29a216 LCSa17a244a59a61a156 a88 a62a90a146a21a65 (18) a145a18a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a95 a16 a75a78a77a83a216 LCSa17 a59a61a156a34a88a78a62a98a146a21a65 (19) Here, LCSa19a244a28a78a144a183a114a93a32a93a139a102a36 is the LCS score of the union longest common subsequence between reference sentences a144a25a114 and a139 . a115 and a122 are the number of words contained in a141, and a139, respectively." ></td>
	<td class="line x" title="114:179	The multiple reference version of ROUGE-N S, SU or L, RNa196 a167a132a197a29a198 a32 RSa196 a167a132a197a29a198 a32 RSUa196 a167a130a197a29a198 a32 RLa196 a167a130a197a29a198 can be defined in accordance with equation (9)." ></td>
	<td class="line x" title="115:179	4.4 Evaluation Measures We evaluate automatic evaluation methods by using Pearsons correlation coefficient (a144 ) and Spearmans rank correlation coefficient (a214 )." ></td>
	<td class="line x" title="116:179	Since we have ten systems, we make a vector a20a66a38a49a28a8a21a102a178a80a32a22a21 a26 a32a34a180a34a180a34a180a118a32a22a21a117a114a94a32a34a180a34a180a34a180a85a32a22a21a66a178 a194 a36 from the results of an automatic evaluation." ></td>
	<td class="line x" title="117:179	Here, a21a120a114a93a38a49a30a37a192a37a10 a188 a46a29a194 a195 a24 a178 a23 a28a78a141 a195 a32a93a139a244a114a25a24 a195 a36 . a141 a195 indicates a reference for the a113 -th topic." ></td>
	<td class="line o" title="118:179	a23 indicates an automatic evaluation function such as a26a138a167a132a169a90a170, a26a15a27a21a169a78a170, ROUGE-N, ROUGE-S, ROUGE-SU and ROUGE-L." ></td>
	<td class="line x" title="119:179	Next, we make another vector a28a39a38a104a28a8a29 a178 a32a22a29 a26 a32a34a180a34a180a34a180a85a32a22a29 a114 a32a34a180a34a180a34a180a118a32a22a29 a178 a194 a36 from the human evaluation results." ></td>
	<td class="line x" title="120:179	Here, a29a51a114a93a38a11a30a37a192a37a10 a188 a46a29a194 a195 a24 a178 a190 a28a78a141 a195 a32a93a139a73a114a25a24 a195 a36 . Finally, we compute a144 and a214 between a20 and a28 6." ></td>
	<td class="line x" title="121:179	4.5 Evaluation Results and Discussions Table 4 shows the evaluation results obtained by using Pearsons correlation coefficient a144 . Table 5 shows the evaluation results obtained with Spearmans rank correlation coefficient a214 . The ta6When using multiple references, functions a30 and a31 for making vectors a32 and a33 are substituted for a30 a172 a133a90a173a68a174 and a31 a172 a133a78a173a68a174, respectively." ></td>
	<td class="line x" title="122:179	149 Table 4: Results obtained with Pearsons correlation coefficient.stop indicates with stop word exclusion, case indicates w/o stop word exclusion." ></td>
	<td class="line o" title="123:179	short long a202 a72 a202 a2 a202 a3 a202 a173a164a208a93a209 a202 a72 a202 a2 a202 a3 a202 a173a164a208a93a209 stop case stop case stop case stop case stop case stop case stop case stop case ROUGE-1 .965 .884 .931 .888 .937 .879 .956 .906 .906 .876 .919 .916 .897 .891 .918 .948 ROUGE-2 .943 .960 .836 .880 .861 .906 .904 .937 .886 .930 .788 .941 .834 .616 .856 .929 ROUGE-3 .906 .936 .759 .814 .786 .846 .862 .900 .873 .909 .717 .849 .826 .431 .844 .885 ROUGE-4 .878 .914 .725 .752 .729 .794 .837 .871 .850 .890 .651 .787 .836 .292 .836 .865 ROUGE-L .919 .777 .789 .683 .875 .867 .898 .852 .917 .840 .861 .812 .847 .829 .910 .848 ROUGE-S(a34 ) .934 .914 .805 .888 .872 .938 .867 .917 .812 .863 .744 .954 .709 .547 .757 .900 ROUGE-S(9) .929 .935 .783 .899 .808 .917 .856 .939 .840 .903 .735 .951 .730 .617 .787 .927 ROUGE-S(4) .936 .943 .802 .891 .839 .917 .877 .940 .876 .920 .778 .945 .814 .663 .840 .932 ROUGE-SU(a34 ) .934 .914 .805 .887 .872 .937 .867 .917 .811 .864 .743 .954 .707 .547 .756 .900 ROUGE-SU(9) .926 .938 .765 .890 .789 .906 .845 .936 .829 .904 .705 .948 .701 .586 .766 .925 ROUGE-SU(4) .930 .945 .772 .865 .810 .889 .861 .927 .868 .921 .730 .928 .785 .620 .818 .925 a160 a58 a70 a2 a133a98a134a61a135 a59a61a162a138a67 a1 a65 .942 .927 .921 .957 .941 .957 .967 .969 a160 a58 a70 a2 a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .929 .943 .928 .965 .939 .962 .959 .967 a160 a58 a70 a3a133a98a134a61a135 a59a61a162a138a67 a1 a65 .939 .923 .919 .962 .926 .954 .953 .966 a160 a58 a70 a3a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .927 .933 .920 .964 .920 .947 .904 .949 a160 a58 a70 a7 a133a98a134a61a135 a59a61a162a138a67 a1 a65 .921 .900 .897 .955 .900 .932 .890 .946 a160 a58 a70 a7 a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .909 .900 .888 .950 .892 .921 .819 .922 a160 a58 a70 a2 a37 a134a136a135 a59a61a162a138a67 a1 a65 .939 .900 .897 .942 .931 .923 .936 .939 a160 a58 a70 a2 a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .928 .921 .909 .958 .932 .939 .950 .950 a160 a58 a70 a3a37 a134a136a135 a59a61a162a138a67 a1 a65 .938 .902 .886 .947 .924 .921 .934 .944 a160 a58 a70 a3a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .928 .922 .895 .960 .920 .929 .919 .942 a160 a58 a70 a7 a37 a134a136a135 a59a61a162a138a67 a1 a65 .929 .896 .873 .947 .910 .913 .908 .938 a160 a58 a70 a7 a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .918 .915 .879 .956 .903 .913 .865 .925 bles show results obtained with and without stop word exclusion for the entire ROUGE family." ></td>
	<td class="line oc" title="124:179	For ROUGE-S and ROUGE-SU, we use three variations following (Lin, 2004b): the maximum skip distances are 4, 9 and infinity 7." ></td>
	<td class="line x" title="125:179	In addition, we examine a165 a38 a35 anda10 for the ESK-based and WSK-based methods." ></td>
	<td class="line x" title="126:179	The decay parameter a12 for a26a138a167a132a169a90a170 and a26a38a27a21a169a78a170 is set at 0.5." ></td>
	<td class="line x" title="127:179	We will discuss these parameter values in Section 4.6." ></td>
	<td class="line x" title="128:179	From the tables, ROUGE-Ns a144 and a214 decrease monotonically with N when we exclude stop words." ></td>
	<td class="line x" title="129:179	In most cases, the performance is improved by including stop words for N (a9a49a35 )." ></td>
	<td class="line x" title="130:179	There is a large difference between ROUGE-1 and ROUGE-4." ></td>
	<td class="line x" title="131:179	The ROUGE-S family is comparable to the ROUGE-SU family and their performance is close to ROUGE1 without stop words and ROUGE-2 with stop words." ></td>
	<td class="line x" title="132:179	ROUGE-L is better than both ROUGE-3 and ROUGE-4 but worse than ROUGE-1 or ROUGE-2." ></td>
	<td class="line x" title="133:179	On the other hand, a26a138a167a132a169a78a170 s correlation coefficients (a144 ) do not change very much with respect to a108 . Even if a108 is set at 4, we can obtain good correlations." ></td>
	<td class="line x" title="134:179	The behavior of rank correlation coefficients (a214 ) is 7We use a162 =1,2, and 3." ></td>
	<td class="line x" title="135:179	However there are little difference among correlation coefficient regardless of a162 because the number of the words in reference and the number of the words in system output are almost the same." ></td>
	<td class="line x" title="136:179	similar to the above." ></td>
	<td class="line x" title="137:179	The difference between the ROUGE family and our method is particularly large for long summaries." ></td>
	<td class="line x" title="138:179	By setting a108a47a38a49a35, our method gives the good results." ></td>
	<td class="line x" title="139:179	The optimal a165 is varied in the data sets." ></td>
	<td class="line x" title="140:179	However, the difference betweena165 a38a54a35 and a165 a38a49a10 is small." ></td>
	<td class="line x" title="141:179	For a214, our method outperforms the ROUGE family except for a211a151a178 . By contrast, we can see a108a47a38a49a10 or a108a51a38a40a39 provided the best results." ></td>
	<td class="line x" title="142:179	The differences between our method and the ROUGE family are larger than for a144 . For both a144 and a214, when multiple references are available, our method outperforms the ROUGE family." ></td>
	<td class="line x" title="143:179	Although ROUGE-1 sometimes provides better results than our method for short summaries, it has a critical problem; ROUGE-1 disregards word sequences making it easy to cheat." ></td>
	<td class="line x" title="144:179	For instance, we can easily obtain a high ROUGE-1 score by using a sequence of high Inverse Document Frequency (IDF) words." ></td>
	<td class="line x" title="145:179	Such a summary is incomprehensible and meaningless but we obtain a good ROUGE-1 score comparable to those of the top TSC-3 systems." ></td>
	<td class="line x" title="146:179	By contrast, it is difficult to cheat other members of the ROUGE family or our method." ></td>
	<td class="line x" title="147:179	Our evaluation results imply that a26a31a167a132a169a78a170 is robust 150 Table 5: Results obtained with Spearmans correlation coefficient." ></td>
	<td class="line x" title="148:179	stop indicates with stop word exclusion, case indicates w/o stop word exclusion." ></td>
	<td class="line x" title="149:179	short long a202 a72 a202 a2 a202 a3 a202 a173a164a208a93a209 a202 a72 a202 a2 a202 a3 a202 a173a164a208a93a209 stop case stop case stop case stop case stop case stop case stop case stop case ROUGE-1 .988 .964 .842 .891 .842 .855 .927 .903 .818 .830 .903 .806 .867 .855 .842 .915 ROUGE-2 .927 .976 .770 .794 .855 .842 .879 .903 .721 .891 .721 .855 .794 .648 .818 .903 ROUGE-3 .879 .927 .588 .697 .818 .818 .867 .927 .758 .842 .636 .745 .806 .564 .709 .855 ROUGE-4 .818 .879 .721 .697 .745 .745 .867 .867 .685 .794 .564 .612 .830 .455 .709 .758 ROUGE-L .927 .830 .661 .600 .806 .818 .879 .806 .842 .770 .576 .612 .636 .709 .879 .697 ROUGE-S(a34 ) .939 .939 .673 .818 .794 .818 .818 .927 .770 .879 .636 .818 .697 .527 .709 .867 ROUGE-S(9) .879 .952 .600 .745 .721 .794 .733 .939 .758 .806 .576 .806 .673 .564 .745 .855 ROUGE-S(4) .891 .964 .600 .794 .794 .794 .794 .939 .709 .842 .576 .770 .770 .733 .758 .842 ROUGE-SU(a34 ) .939 .939 .673 .818 .794 .818 .818 .927 .770 .879 .636 .818 .697 .553 .709 .867 ROUGE-SU(9) .879 .964 .600 .745 .721 .794 .745 .939 .745 .806 .576 .758 .612 .564 .745 .903 ROUGE-SU(4) .879 .988 .600 .745 .721 .770 .794 .903 .758 .855 .576 .794 .709 .612 .794 .842 a160 a58 a70 a2 a133a98a134a61a135 a59a61a162a138a67 a1 a65 .952 .879 .855 .939 .842 .927 .903 .903 a160 a58 a70 a3a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .952 .915 .891 .939 .855 .903 .903 .903 a160 a58 a70 a3a133a98a134a61a135 a59a61a162a138a67 a1 a65 .964 .867 .867 .976 .818 .927 .879 .879 a160 a58 a70 a3a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .964 .891 .915 .976 .758 .903 .709 .891 a160 a58 a70 a7 a133a98a134a61a135 a59a61a162a138a67 a1 a65 .927 .830 .867 .952 .661 .903 .733 .915 a160 a58 a70 a7 a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .927 .842 .842 .988 .588 .903 .673 .891 a160 a58 a70 a2 a37 a134a136a135 a59a61a162a138a67 a1 a65 .976 .794 .830 .952 .818 .867 .806 .891 a160 a58 a70 a2 a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .952 .842 .830 .952 .818 .867 .794 .903 a160 a58 a70 a3a37 a134a136a135 a59a61a162a138a67 a1 a65 .976 .794 .818 .939 .806 .855 .733 .879 a160 a58 a70 a3a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .976 .879 .855 .952 .806 .818 .794 .915 a160 a58 a70 a7 a37 a134a136a135 a59a61a162a138a67 a1 a65 .964 .794 .818 .939 .806 .855 .697 .915 a160 a58 a70 a7 a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .964 .867 .855 .976 .745 .855 .770 .915 Table 6: Best scores for each data set.Pearsons Correlation Coefficient Length a202 a72 a202 a2 a202 a3 a202 a173a98a208a89a209 short .945 .946 .933 .967( a0 a62a94a1a37a62a98a162 ) (2,0.7,2) (2,0.7,4) (2,0.1,3) (2,0.7,3)long .941 .962 .971 .972 (a0 a62a94a1a37a62a98a162 ) (2,0.6,2) (2,0.6,3) (2,0.7,2) (2,0.8,2) Spearmans Rank Correlation Coefficient Length a202 a72 a202 a2 a202 a3 a202 a173a98a208a89a209 short .964 .915 .915 .988( a0 a62a94a1a37a62a98a162 ) (3,0.9,4) (2,0.3,4) (3,0.5,3) (4,0.7,4)long .855 .927 .915 .939 (a0 a62a94a1a37a62a98a162 ) (2,0.8,4) (3,0.5,2) (2,0.5,4) (2,0.8,3) for a108 and length of summary and correlates closely with human evaluation results." ></td>
	<td class="line x" title="150:179	Moreover, it includes no trivial way of obtaining a good score." ></td>
	<td class="line x" title="151:179	These are significant advantages over ROUGE family." ></td>
	<td class="line x" title="152:179	In addition, our method outperformed the WSK-based method in most cases." ></td>
	<td class="line x" title="153:179	This result confirms the effectiveness of semantic information and the significant advantage of the ESK." ></td>
	<td class="line x" title="154:179	4.6 Effects of Parameters Our method has three parameters, a108a51a32a34a12, and a165 . In this section, we discuss the effects of these parameters." ></td>
	<td class="line x" title="155:179	Figure 1 shows a144 and a214 for various a12 and a165 values with respect to a211 a197a89a212a158a213 . Note that we set a108 at 2 in the figure because the tendency is similar when we use other values, namely a108a51a28a29a38a49a10a168a231a37a236a41a39a31a36 . From Fig." ></td>
	<td class="line x" title="156:179	1, we can see that a165 a38a49a30 is not good." ></td>
	<td class="line x" title="157:179	With automatic summarization, precision is not necessarily a good evaluation measure because highly redundant summaries may obtain a very high precision." ></td>
	<td class="line x" title="158:179	On the other hand, recall is not good when a systems output is redundant." ></td>
	<td class="line x" title="159:179	Therefore, equal treatment of precision and recall does not give a good evaluation measure." ></td>
	<td class="line x" title="160:179	The figure shows that a165 a38a104a35a37a32a34a10 and 5 are good for a144 and a165 a38a104a10a37a32a22a39a31a32a43a42 and infinity are good for a214 . Moreover, we can see a significant differences between a12a182a38a41a30 and others from the figure." ></td>
	<td class="line x" title="161:179	This implies an advantage of our method compared to ROUGE-S and ROUGE-SU, which cannot handle decay factor for skip-n-grams." ></td>
	<td class="line x" title="162:179	From Fig." ></td>
	<td class="line x" title="163:179	1, we can see thata214 is more sensitive to a165 than a144 . Here, a165 a38a49a10a37a32a22a39a31a32a43a42 and infinity obtained the best results." ></td>
	<td class="line x" title="164:179	a165 a38a49a30 was again the worst." ></td>
	<td class="line x" title="165:179	This result indicates that we have to determine the parameter value properly for different tasks." ></td>
	<td class="line x" title="166:179	a12 does not greatly affect the correlation for a108a51a38a104a10a37a32a22a39a31a32a43a42 and infinity as regards the middle range." ></td>
	<td class="line x" title="167:179	Table 6 show the best results when we examined all parameter combinations." ></td>
	<td class="line x" title="168:179	In the brackets, we show the best settings of these parameter combinations." ></td>
	<td class="line x" title="169:179	For a144, a108a51a38a41a35 provides the best result and middle range a12 anda165 a38a49a35 or 3 are good in most cases." ></td>
	<td class="line x" title="170:179	On the other hand, the best settings for a214 vary with 151 0.8 0.85 0.9 0.95 1.0 0 0.5 1.0 Correlation Coefficient  =1 =2 =3 =4 =5 =inf." ></td>
	<td class="line x" title="171:179	0.7 0.75 0.8 0.85 0.9 0.95 1.0 0 0.5 1.0 Rank Correlation Coefficient =1 =2 =3 =4 =5 =inf." ></td>
	<td class="line x" title="172:179	 Figure 1: Correlation coefficients for various values of a44 and a45 on a46a48a47a22a49a51a50 . the data set." ></td>
	<td class="line x" title="173:179	a52a54a53a56a55 is not always good for a57 . In short, we can see that the decay parameter for skips is significant and long skip-n-grams are effective especially a57 . These results show that our method has an advantage over the ROUGE family." ></td>
	<td class="line x" title="174:179	In addition, our method is robust and sufficiently good even if close attention is not paid to the parameters." ></td>
	<td class="line x" title="175:179	5 Conclusion In this paper, we described an automatic evaluation method based on the ESK, which is a method for measuring the similarities between texts based on sequences of words and word senses." ></td>
	<td class="line x" title="176:179	Our experiments showed that our method is comparable to ROUGE family for short summaries and outperforms it for long summaries." ></td>
	<td class="line x" title="177:179	In order to prove that our method is language independent, we will conduct an experimental evaluation by using DUCs evaluation data." ></td>
	<td class="line x" title="178:179	We believe that our method will also be useful for other natural language generation tasks." ></td>
	<td class="line x" title="179:179	We are now planning to apply our method to an evaluation of machine translation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I05-2027
Machine Learning Approach to Augmenting News Headline Generation
Wang, Ruichao;Dunnion, John;Carthy, Joe;"></td>
	<td class="line x" title="1:123	Machine Learning Approach To Augmenting News Headline Generation Ruichao Wang Dept. of Computer Science University College Dublin Ireland rachel@ucd.ie John Dunnion Dept. of Computer Science University College Dublin Ireland John.Dunnion@ucd.ie Joe Carthy Dept. of Computer Science University College Dublin Ireland Joe.Carthy@ucd.ie Abstract In this paper, we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text." ></td>
	<td class="line x" title="2:123	We compare our system with the Topiary system which, in contrast, uses a statistical learning approach to finding topic descriptors for headlines." ></td>
	<td class="line x" title="3:123	The Topiary system, developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004." ></td>
	<td class="line x" title="4:123	Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story." ></td>
	<td class="line x" title="5:123	The Topiary system uses a statistical learning approach to finding topic labels." ></td>
	<td class="line o" title="6:123	The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection." ></td>
	<td class="line x" title="7:123	1 Introduction In this paper we present an approach to headline generation for a single document." ></td>
	<td class="line x" title="8:123	This headline generation task was added to the annual summ5arisation evaluation in the Document Understanding Conference (DUC) 2003." ></td>
	<td class="line o" title="9:123	It was also included in the DUC 2004 evaluation plan where summary quality was automatically judged using a set of n-gram word overlap metrics called ROUGE (Lin and Hovy, 2003)." ></td>
	<td class="line x" title="10:123	Eighteen research groups participated in the headline generation task at DUC 2004, i.e. Task 1: very short summary generation." ></td>
	<td class="line x" title="11:123	The Topiary system was the top performing headline system at DUC 2004." ></td>
	<td class="line x" title="12:123	It generated headlines by combining a set of topic descriptors with a compressed version of the lead sentence, e.g. KURDISH TURKISH SYRIA: Turkey sent 10,000 troops to southeastern border." ></td>
	<td class="line x" title="13:123	These topic descriptors were automatically identified using a statistical approach called Unsupervised Topic Discovery (UTD) (Zajic et al. , 2004)." ></td>
	<td class="line x" title="14:123	The disadvantage of this technique is that meaningful topic descriptors will only be identified if this technique is trained on the corpus containing the news stories that are to be summarised." ></td>
	<td class="line x" title="15:123	In addition, the corpus must contain clusters of related news stories to ensure that reliable cooccurrence statistics are generated." ></td>
	<td class="line x" title="16:123	In this paper we compare the UTD method with an alternative topic label identifier that can be trained on an auxiliary news corpus, and observe the effect of these labels on summary quality when combined with compressed lead sentences." ></td>
	<td class="line x" title="17:123	Our topic labeling technique works by combining linguistic and statistical information about terms using the C5.0 (Quinlan, 1998) machine learning algorithm, to predict which words in the source text should be included in the resultant gist with the compressed lead sentence." ></td>
	<td class="line x" title="18:123	In this paper, we compare the performance of this system, HybridTrim, with the Topiary system and a number of other baseline gisting systems on a collection of news documents from the DUC 2004 corpus (DUC, 2003)." ></td>
	<td class="line x" title="19:123	155 2 Topiary System In this section, we describe the Topiary system developed at the University of Maryland with BBN Technologies." ></td>
	<td class="line x" title="20:123	As already stated, this system was the top performing headline generation system at DUC 2004." ></td>
	<td class="line x" title="21:123	A Topiarystyle headline consists of a set of topic labels followed by a compressed version of the lead sentence." ></td>
	<td class="line x" title="22:123	Hence, the Topiary system views headline generation as a two-step process: first, create a compressed version of the lead sentence of the source text, and second, find a set of topic descriptors that adequately describe the general topic of the news story." ></td>
	<td class="line x" title="23:123	We will now look at each of these steps in more detail." ></td>
	<td class="line x" title="24:123	Dorr et al.(2003) stated that when human subjects were asked to write titles by selecting words in order of occurrence in the source text, 86.8% of these headline words occurred in the first sentence of the news story." ></td>
	<td class="line x" title="26:123	Based on this result Dorr, Zajic and Schwartz, concluded that compressing the lead sentence was sufficient when generating titles for news stories." ></td>
	<td class="line x" title="27:123	Consequently, their DUC 2003 system HedgeTrimmer used linguistically-motivated heuristics to remove constituents that could be eliminated from a parse tree representation of the lead sentence without affecting the factual correctness or grammaticality of the sentence." ></td>
	<td class="line x" title="28:123	These linguistically-motivated trimming rules (Dorr et al. , 2003; Zajic et al. , 2004) iteratively remove constituents until a desired sentence compression rate is reached." ></td>
	<td class="line x" title="29:123	The compression algorithm begins by removing determiners, time expressions and other low content words." ></td>
	<td class="line x" title="30:123	More drastic compression rules are then applied to remove larger constituents of the parse tree until the required headline length is achieved." ></td>
	<td class="line x" title="31:123	For the DUC 2004 headline generation task systems were required to produce headlines no longer than 75 bytes, i.e. about 10 words." ></td>
	<td class="line x" title="32:123	The following worked example helps to illustrate the sentence compression process." ></td>
	<td class="line x" title="33:123	1 1 The part of speech tags in the example are explained as follows: S represents a simple declarative clause; SBAR represents a clause introduced by a (possibly empty) subordinating conjunction; NP is a noun phrase; VP is a verb phrase; ADVP is an adverbial phrase." ></td>
	<td class="line x" title="34:123	Lead Sentence: The U.S. space shuttle Discovery returned home this morning after astronauts successfully ended their 10-day Hubble Space telescope service mission." ></td>
	<td class="line x" title="35:123	Parse: (S (NP (NP The U.S. space shuttle) Discovery) (VP returned (NP home) (NP this morning)) (SBAR after (S (NP astronauts) (VP (ADVP successfully) ended (NP their 10-day Hubble Space telescope service mission))))) 1." ></td>
	<td class="line x" title="36:123	Choose leftmost S of parse tree and remove all determiners, time expressions and low content units such as quantifiers (e.g. each, many, some), possessive pronouns (e.g. their, ours, hers) and deictics (e.g. this, tese, those): Before: (S (NP (NP The U.S. space shuttle) Discovery) (VP returned (NP home) (NP this morning)) (SBAR after (S (NP astronauts) (VP (ADVP successfully) ended (NP their 10-day Hubble Space telescope service mission))))) After: (S (NP (NP U.S. space shuttle) Discovery) (VP returned (NP home)) (SBAR after (S (NP astronauts) (VP (ADVP successfully) ended (NP 10-day Hubble Space telescope service mission))))) 2." ></td>
	<td class="line x" title="37:123	The next step iteratively removes constituents until the desired length is reached." ></td>
	<td class="line x" title="38:123	In this instance the algorithm will remove the trailing SBAR." ></td>
	<td class="line x" title="39:123	Before: (S (NP (NP U.S. space shuttle) Discovery) (VP returned (NP home)) (SBAR after (S (NP astronauts) (VP (ADVP successfully) ended (NP 10-day Hubble Space telescope service mission))))) After: U.S. space shuttle Discovery returned home." ></td>
	<td class="line x" title="40:123	Like the trailing SBAR rule, the other iterative rules identify and remove non-essential relative clauses and subordinate clauses from the lead sentence." ></td>
	<td class="line o" title="41:123	A more detailed description of these rules can be found in Dorr et al.(2003) and Zajic et al.(2004) In this example, we can see that after compression the lead sentence reads 156 more like a headline." ></td>
	<td class="line x" title="44:123	The readability of the sentence in this case could be further improved by replacing the past tense verb returned with its present tense form; however, this refinement is not currently implemented by the Topiary system or by our implementation of this compression algorithm." ></td>
	<td class="line x" title="45:123	As stated earlier, a list of relevant topic words is also concatenated with this compressed sentence resulting in the final headline." ></td>
	<td class="line x" title="46:123	The topic labels are generated by the UTD (Unsupervised Topic Discovery) algorithm (Zajic et al. , 2004)." ></td>
	<td class="line x" title="47:123	This unsupervised information extraction algorithm creates a short list of useful topic labels by identifying commonly occurring words and phrases in the DUC corpus." ></td>
	<td class="line x" title="48:123	So for each document in the corpus it identifies an initial set of important topic names for the document using a modified version of the tf.idf metric." ></td>
	<td class="line x" title="49:123	Topic models are then created from these topic names using the OnTopic software package." ></td>
	<td class="line x" title="50:123	The list of topic labels associated with the topic models closest in content to the source document are then added to the beginning of the compressed lead sentence produced in the previous step, resulting in a Topiary-style summary." ></td>
	<td class="line x" title="51:123	One of the problems with this approach is that it will only produce meaningful topic models and labels if they are generated from a corpus containing additional on-topic documents on the news story being summarised." ></td>
	<td class="line x" title="52:123	In the next section, we explore two alternative techniques for identifying topic labels, where useful summary words are identified locally by analysing the source document rather than globally using the entire DUC corpus, i.e. the UTD method." ></td>
	<td class="line x" title="53:123	3 C5.0 C5.0 (Quinlan, 1998) is a commercial machine learning program developed by RuleQuest Research and is the successor of the widely used ID3 (Quinlan, 1983) and C4.5 (Quinlan, 1993) algorithms developed by Ross Quinlan." ></td>
	<td class="line x" title="54:123	C5.0 is a tool for detecting patterns that delineate categories." ></td>
	<td class="line x" title="55:123	It subsequently generates decision trees based on these patterns." ></td>
	<td class="line x" title="56:123	A decision tree is a classifier represented as a tree structure, where each node is either a leaf node, a classification that applies to all instances that reach the leaf (Witten, 2000), or a non-leaf node, some test is carried out on a single attribute-value, with one branch and sub-tree for each possible outcome of the test." ></td>
	<td class="line x" title="57:123	A decision tree is a powerful and popular tool for classification and prediction and can be used to classify an instance by starting at the root of the tree and moving down the tree branch until reaching a leaf node." ></td>
	<td class="line x" title="58:123	However, a decision tree may not be very easy to understand." ></td>
	<td class="line x" title="59:123	An important feature of C5.0 is that it can convert trees into collections of rules called rulesets." ></td>
	<td class="line x" title="60:123	C5.0 rulesets consist of unordered collections of simple if-then rules." ></td>
	<td class="line x" title="61:123	It is easy to read a set of rules directly from a decision tree." ></td>
	<td class="line x" title="62:123	One rule is generated for each leaf." ></td>
	<td class="line x" title="63:123	The antecedent of the rule includes a condition for every node on the path from the root to that leaf, and the consequent of the rule is the class assigned by the leaf." ></td>
	<td class="line x" title="64:123	This process produces rules that are unambiguous in that the order in which they are executed is irrelevant (Witten, 2000)." ></td>
	<td class="line x" title="65:123	C5.0 has been used for text classification in a number of research projects." ></td>
	<td class="line x" title="66:123	For example, Akhtar et al.(2001) used C5.0 for automatically marking up XML documents, Newman et al.(2005) used it for generating multi-document summary, while Zhang et al.(2004) applied this approach to World Wide Web site summarisation." ></td>
	<td class="line x" title="70:123	4 HybridTrim System The HybridTrim system uses our implementation of the Hedge Trimmer algorithm and the C5.0 (Quinlan, 1998) machine learning algorithm to create a decision tree capable of predicting which words in the source text should be included in the resultant gist." ></td>
	<td class="line x" title="71:123	To identify pertinent topic labels the algorithm follows a two-step process: the first step involves creating an intermediate representation of a source text, and the second involves transforming this representation into a summary text." ></td>
	<td class="line x" title="72:123	The intermediate representation we have chosen is a set of features, that we feel are good indicators of possible summary words." ></td>
	<td class="line x" title="73:123	We focus our efforts on the content words of a document, i.e. the nouns, verbs and adjectives that occur within the document." ></td>
	<td class="line x" title="74:123	For each occurrence of a term in a document, we calculate several features: the tf, or term 157 frequency of the word in the document; the idf, or inverse document frequency of the term taken from an auxiliary corpus (TDT, 2004); and the relative position of a word with respect to the start of the document in terms of word distance." ></td>
	<td class="line x" title="75:123	We also include binary features indicating whether a word is a noun, verb or adjective and whether it occurs in a noun or proper noun phrase." ></td>
	<td class="line x" title="76:123	The final feature is a lexical cohesion score calculated with the aid of a linguistic technique called lexical chaining." ></td>
	<td class="line x" title="77:123	Lexical chaining is a method of clustering words in a document that are semantically similar with the aid of a thesaurus, in our case WordNet." ></td>
	<td class="line x" title="78:123	Our chaining method identifies the following word relationship (in order of strength): repetition, synonymy, specialisation and generalisation, and part/whole relationships." ></td>
	<td class="line x" title="79:123	Once all lexical chains have been created for a text then a score is assigned to each chained word based on the strength of the chain in which it occurs." ></td>
	<td class="line x" title="80:123	More specifically, as shown in Equation (1), the chain strength score is the sum of each strength score assigned to each word pair in the chain." ></td>
	<td class="line x" title="81:123	where reps i is the frequency of word i in the text, and rel(i,j) is a score assigned based on the strength of the relationship between word i and j. More information on the chaining process and cohesion score can be found in Doran et al.(2004a) and Stokes (2004)." ></td>
	<td class="line x" title="83:123	Using the DUC 2003 corpus as the training data for our classifier, we then assigned each word a set of values for each of these features, which are then used with a set of gold standard human-generated summaries to train a decision tree summarisation model using the C5.0 machine learning algorithm." ></td>
	<td class="line x" title="84:123	The DUC 2003 evaluation provides four human summaries for each document, where words in the source text that occur in these model summaries are considered to be positive training examples, while document words that do not occur in these summaries are considered to be negative examples." ></td>
	<td class="line x" title="85:123	Further use is made of these four summaries, where the model is trained to classify a word based on its summarisation potential." ></td>
	<td class="line x" title="86:123	More specifically, the appropriateness of a word as a summary term is determined based on the class assigned to it by the decision tree." ></td>
	<td class="line x" title="87:123	These classes are ordered from strongest to weakest as follows: occurs in 4 summaries, occurs in 3 summaries, occurs in 2 summaries, occurs in 1 summary, occurs in none of the summaries." ></td>
	<td class="line x" title="88:123	If the classifier predicts that a word will occur in all four of the human generated summaries, then it is considered to be a more appropriate summary word than a word predicted to occur in only three of the model summaries." ></td>
	<td class="line x" title="89:123	This resulted in a total of 103267 training cases, where 5762 instances occurred in one summary, 1791 in two, 1111 in three, 726 in four, and finally 93877 instances were negative." ></td>
	<td class="line x" title="90:123	A decision tree classifier was then produced by the C5.0 algorithm based on this training data." ></td>
	<td class="line x" title="91:123	To gauge the accuracy of our decision tree topic label classifier, we used a training/test data split of 90%/10%, and found that on this test set the classifier had a precision (true positives divided by true positives and false positives) of 63% and recall (true positives divided by true positives and false negatives) of 20%." ></td>
	<td class="line x" title="92:123	5 Evaluation and Results In this section we present the results of our headline generation experiments on the DUC 2004 corpus." ></td>
	<td class="line x" title="93:123	2 We use the ROUGE (RecallOriented Understudy for Gisting Evaluation) metrics to evaluate the quality of our automatically generated headlines." ></td>
	<td class="line x" title="94:123	In DUC 2004 task 1, participants were asked to generate very short (<=75 bytes) single-document summaries for documents on TDT-defined events." ></td>
	<td class="line x" title="95:123	The DUC 2004 corpus consists of 500 Associated Press and New York Times newswire documents." ></td>
	<td class="line o" title="96:123	The headline-style summaries created by each system were evaluated against a set of human generated (or model) summaries using the ROUGE metrics." ></td>
	<td class="line o" title="97:123	The format of the evaluation was based on six scoring metrics: ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, ROUGE-LCS and ROUGE-W." ></td>
	<td class="line o" title="98:123	The first four metrics are based on the average n-gram match between a set of model summaries and the system-generated summary for each document in the corpus." ></td>
	<td class="line o" title="99:123	ROUGE-LCS calculated the longest common 2 Details of our official DUC 2004 headline generation system can be found in Doran et al.(2004b)." ></td>
	<td class="line x" title="101:123	This system returned a list of keywords rather than a sentence + keywords as a headline." ></td>
	<td class="line x" title="102:123	It used a decision tree classifier to identify appropriate summary terms in the news story based on a number of linguistic and statistical word features." ></td>
	<td class="line o" title="103:123	 += )),(*)(()( jirelrepsrepschainScore ji (1) 158 sub-string between the system summaries and the models, and ROUGE-W is a weighted version of the LCS measure." ></td>
	<td class="line o" title="104:123	So for all ROUGE metrics, the higher the ROUGE value the better the performance of the summarisation system, since high ROUGE scores indicate greater overlap between the system summaries and their respective models." ></td>
	<td class="line x" title="105:123	Lin and Hovy (2003) have shown that these metrics correlated well with human judgments of summary quality, and the summarisation community is now accepting these metrics as a credible and less timeconsuming alternative to manual summary evaluation." ></td>
	<td class="line o" title="106:123	In the official DUC 2004 evaluation all summary words were stemmed before the ROUGE metrics were calculated; however, stopwords were not removed." ></td>
	<td class="line x" title="107:123	No manual evaluation of headlines was performed." ></td>
	<td class="line o" title="108:123	5.1 ROUGE Evaluation Results Table 1 shows the results of our headline generation experiments on the DUC 2004 collection." ></td>
	<td class="line x" title="109:123	Seven systems in total took part in this evaluation, three Topiary-style headline generation systems and four baselines: the goal of our experiments was to evaluate linguistically-motivated heuristic approaches to title generation, and establish which of our alternative techniques for padding Topiary-style headlines with topic labels works best." ></td>
	<td class="line pc" title="110:123	Since the DUC 2004 evaluation, Lin (2004) has concluded that certain ROUGE metrics correlate better with human judgments than others, depending on the summarisation task being evaluated, i.e. single document, headline, or multi-document summarisation." ></td>
	<td class="line p" title="111:123	In the case of headline generation, Lin found that ROUGE-1, ROUGE-L and ROUGE-W scores worked best and so only these scores are included in Table 1." ></td>
	<td class="line x" title="112:123	Table 1." ></td>
	<td class="line o" title="113:123	ROUGE scores for headline generation systems As the results show the best performing topic labeling techniques are the TF and Hybrid systems." ></td>
	<td class="line x" title="114:123	TF system is a baseline system that chooses high frequency content words as topic descriptors." ></td>
	<td class="line x" title="115:123	Hybrid system is our decision tree classifier described in the previous section." ></td>
	<td class="line x" title="116:123	Both of these systems outperform the Topiary's UTD method." ></td>
	<td class="line x" title="117:123	The top three performing systems in this table combine topic labels with a compressed version of the lead sentence." ></td>
	<td class="line x" title="118:123	Comparing these results to the Trim system (that returns the reduced lead sentence only), it is clear that the addition of topic descriptors greatly improves summary quality." ></td>
	<td class="line o" title="119:123	The performance of the baseline TFTrim system and the HybridTrim system are very similar for all Rouge metrics; however, both systems outperform the Topiary headline generator." ></td>
	<td class="line x" title="120:123	6 Conclusions and Future work The results of our experiment have shown the TFTrim system (the simplest of the three Topiary-style headline generators examined in this paper) is the most appropriate headline approach because it yields high quality short summaries and, unlike the Topiary and HybridTrim systems, it requires no prior training." ></td>
	<td class="line x" title="121:123	This is an interesting result as it shows that a simple tf weighting scheme can produce as good, if not better, topic descriptors than the statistical UTD method employed by the University of Maryland and our own statistical/linguistic approach to topic label identification." ></td>
	<td class="line x" title="122:123	In future work, we intend to proceed by improving the sentence compression procedure described in this paper." ></td>
	<td class="line x" title="123:123	We are currently working on the use of term frequency information as a means of improving the performance of the Hedge Trimmer algorithm by limiting the elimination of important parse tree components during sentence compression." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0901
A Methodology For Extrinsic Evaluation Of Text Summarization: Does ROUGE Correlate?
Dorr, Bonnie Jean;Monz, Christof;President, Stacy;Schwartz, Richard M.;Zajic, David;"></td>
	<td class="line x" title="1:220	Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 18, Ann Arbor, June 2005." ></td>
	<td class="line o" title="2:220	c2005 Association for Computational Linguistics A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?" ></td>
	<td class="line x" title="3:220	Bonnie J. Dorr and Christof Monz and Stacy President and Richard Schwartz and David Zajic Department of Computer Science and UMIACS University of Maryland College Park, MD 20742 {bonnie,christof,stacypre,dmzajic}@umiacs.umd.edu BBN Technologies 9861 Broken Land Parkway Columbia, Maryland 21046 schwartz@bbn.com Abstract This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment based on a new method for measuring agreement, Relevance-Prediction, which compares subjects judgments on summaries with their own judgments on full text documents." ></td>
	<td class="line x" title="4:220	We demonstrate that, because this measure is more reliable than previous gold-standard measures, we are able to make stronger statistical statements about the benefits of summarization." ></td>
	<td class="line o" title="5:220	We found positive correlations between ROUGE scores and two different summary types, where only weak or negative correlations were found using other agreement measures." ></td>
	<td class="line n" title="6:220	However, we show that ROUGE may be sensitive to the choice of summarization style." ></td>
	<td class="line x" title="7:220	We discuss the importance of these results and the implications for future summarization evaluations." ></td>
	<td class="line x" title="8:220	1 Introduction People often prefer to read a summary of a text document, e.g., news headlines, scientific abstracts, movie previews and reviews, and meeting minutes." ></td>
	<td class="line x" title="9:220	Correspondingly, the explosion of online textual material has prompted advanced research in document summarization." ></td>
	<td class="line x" title="10:220	Although researchers have demonstrated that users can read summaries faster than full text (Mani et al. , 2002) with some loss of accuracy, researchers have found it difficult to draw strong conclusions about the usefulness of summarization due to the low level of interannotator agreement in the gold standards that they have used." ></td>
	<td class="line x" title="11:220	Definitive conclusions about the usefulness of summaries would provide justification for continued research and development of new summarization methods." ></td>
	<td class="line x" title="12:220	To investigate the question of whether text summarization is useful in an extrinsic task, we examined human performance in a relevance assessment task using a human text surrogate (i.e. text intended to stand in the place of a document)." ></td>
	<td class="line x" title="13:220	We use single-document English summaries as these are sufficient for investigating task-based usefulness, although more elaborate surrogates are possible, e.g., those that span more than one document (Radev and McKeown, 1998; Mani and Bloedorn, 1998)." ></td>
	<td class="line x" title="14:220	The next section motivates the need for developing a new framework for measuring task-based usefulness." ></td>
	<td class="line x" title="15:220	Section 3 presents a novel extrinsic measure called Relevance-Prediction." ></td>
	<td class="line x" title="16:220	Section 4 demonstrates that this is a more reliable measure than that of previous gold standard methods, e.g., the LDC-Agreement method used for SUMMAC-style evaluations, and that this reliability allows us to make stronger statistical statements about the benefits of summarization." ></td>
	<td class="line x" title="17:220	We expect these findings to be important for future summarization evaluations." ></td>
	<td class="line n" title="18:220	Section 5 presents the results of correlation between task usefulness and the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric (Lin and Hovy, 2003).1 While we show that ROUGE correlates with task usefulness (using our Relevance-Prediction measure), we detect a slight difference between informative, extractive headlines (containing words from the full document) and less informative, non-extractive eye-catchers (containing words that might not appear in the full document, and intended to entice a reader to read the entire document)." ></td>
	<td class="line x" title="19:220	Section 6 further highlights the importance of this point and discusses the implications for automatic evaluation of non-extractive summaries." ></td>
	<td class="line x" title="20:220	To evaluate nonextractive summaries reliably, an automatic measure may require knowledge of sophisticated meaning units.2 It is our hope that the conclusions drawn herein will prompt investigation into more sophisticated automatic metrics as researchers shift their focus to non-extractive summaries." ></td>
	<td class="line n" title="21:220	1ROUGE has been previously used as the primary automatic evaluation metric by NIST in the 2003 and 2004 DUC Evaluations." ></td>
	<td class="line x" title="22:220	2The content units proposed in recent methods (Nenkova and Passonneau, 2004) are a first step in this direction." ></td>
	<td class="line x" title="23:220	1 2 Background In the past, assessments of usefulness involved a wide range of both intrinsic and extrinsic (task-based) measures (Sparck-Jones and Gallier, 1996)." ></td>
	<td class="line x" title="24:220	Intrinsic evaluations focus on coherence and informativeness (Jing et al. , 1998) and often involve quality comparisons between automatic summaries and reference summaries that are pre-determined to be of high quality." ></td>
	<td class="line x" title="25:220	Human intrinsic measures determine quality by assessing document accuracy, fluency, and clarity." ></td>
	<td class="line o" title="26:220	Automatic intrinsic measures such as ROUGE use n-gram scoring to produce rankings of summarization methods." ></td>
	<td class="line x" title="27:220	Extrinsic evaluations concentrate on the use of summaries in a specific task, e.g., executing instructions, information retrieval, question answering, and relevance assessments (Mani, 2001)." ></td>
	<td class="line x" title="28:220	In relevance assessments, a user reads a topic or event description and judges relevance of a document to the topic/event based solely on its summary.3 These have been used in many large-scale extrinsic evaluations, e.g., SUMMAC (Mani et al. , 2002) and the Document Understanding Conference (DUC) (Harman and Over, 2004)." ></td>
	<td class="line x" title="29:220	The task chosen for such evaluations must support a very high degree of interannotator agreement, i.e., consistent relevance decisions across subjects with respect to a predefined gold standard." ></td>
	<td class="line x" title="30:220	Unfortunately, a consistent gold standard has not yet been reported." ></td>
	<td class="line x" title="31:220	For example, in two previous studies (Mani, 2001; Tombros and Sanderson, 1998), users judgments were compared to gold standard judgments produced by members of the University of Pennsylvanias Linguistic Data Consortium." ></td>
	<td class="line x" title="32:220	Although these judgments were supposed to represent the correct relevance judgments for each of the documents associated with an event, both studies reported that annotators judgments varied greatly and that this was a significant issue for the evaluations." ></td>
	<td class="line x" title="33:220	In the SUMMAC experiments, the Kappa score (Carletta, 1996; Eugenio and Glass, 2004) for interannotator agreement was reported to be 0.38 (Mani et al. , 2002)." ></td>
	<td class="line x" title="34:220	In fact, large variations have been found in the initial summary scoring of an individual participant and a subsequent scoring that occurs a few weeks later (Mani, 2001; van Halteren and Teufel, 2003)." ></td>
	<td class="line x" title="35:220	This paper attempts to overcome the problem of interannotator inconsistency by measuring summary effectiveness in an extrinsic task using a much more consistent form of user judgment instead of a gold standard." ></td>
	<td class="line x" title="36:220	Using Relevance-Prediction increases the confidence in our results and strengthens the statistical statements we can make about the benefits of summarization." ></td>
	<td class="line x" title="37:220	The next section describes an alternative approach to measuring task-based usefulness, where the usage of external judgments as a gold standard is replaced by the 3A topic is an event or activity, along with all directly related events and activities." ></td>
	<td class="line x" title="38:220	An event is something that happens at some specific time and place, and the unavoidable consequences." ></td>
	<td class="line x" title="39:220	users own decisions on the full text." ></td>
	<td class="line x" title="40:220	Following the lead of earlier evaluations (Oka and Ueda, 2000; Mani et al. , 2002; Sakai and Sparck-Jones, 2001), we focus on relevance assessment as our extrinsic task." ></td>
	<td class="line x" title="41:220	3 Evaluation of Usefulness of Summaries We define a new extrinsic measure of task-based usefulness called Relevance-Prediction, where we compare a summary-based decision to the subjects own full-text decision rather than to a different subjects decision." ></td>
	<td class="line x" title="42:220	Our findings differ from that of the SUMMAC results (Mani et al. , 2002) in that using Relevance-Prediction as an alternative to comparision to a gold standard is a more realistic agreement measure for assessing usefulness in a relevance assessment task." ></td>
	<td class="line x" title="43:220	For example, users performing browsing tasks must examine document surrogates, but open the full-text only if they expect the document to be interesting to them." ></td>
	<td class="line x" title="44:220	They are not trying to decide if the document will be interesting to someone else." ></td>
	<td class="line x" title="45:220	To determine the usefulness of summarization, we focus on two questions:  Can users make judgments on summaries that are consistent with their full-text judgments?" ></td>
	<td class="line x" title="46:220	 Can users make judgments on summaries more quickly than on full document text?" ></td>
	<td class="line x" title="47:220	First we describe the Relevance-Prediction measure for determining whether users can make accurate judgments with a summary." ></td>
	<td class="line x" title="48:220	Following this, we describe our experiments and results using this measure, including the timing results of summaries compared to full documents." ></td>
	<td class="line x" title="49:220	3.1 Relevance-Prediction Measure To answer the first question above, we define a measure called Relevance-Prediction, where subjects build their own gold standard based on the full-text documents." ></td>
	<td class="line x" title="50:220	Agreement is measured by comparing subjects surrogate-based judgments against their own judgments on the corresponding texts." ></td>
	<td class="line x" title="51:220	The subjects judgment is assigned a value of 1 if his/her surrogate judgment is the same as the corresponding full-text judgment, and 0 otherwise." ></td>
	<td class="line x" title="52:220	These values were summed over all judgments for a surrogate type and were divided by the total number of judgments for that surrogate type to determine the effectiveness of the associated summary method." ></td>
	<td class="line x" title="53:220	Formally, given a summary/document pair (s,d), if subjects make the same judgment on s that they did on d, we say j(s,d) = 1." ></td>
	<td class="line x" title="54:220	If subjects change their judgment between s and d, we say j(s,d) = 0." ></td>
	<td class="line x" title="55:220	Given a set of summary/document pairs DSi associated with event i, the Relevance-Prediction score is computed as follows: Relevance-Prediction(i) = summationtext s,dDSij(s,d) |DSi| This approach provides a more reliable comparison mechanism than gold standard judgments provided by 2 other individuals." ></td>
	<td class="line x" title="56:220	Specifically, Relevance-Prediction is more helpful in illuminating the usefulness of summaries for a real-world scenario, e.g., a browsing environment, where credit is given when an individual subject would choose (or reject) a document under both conditions." ></td>
	<td class="line x" title="57:220	To our knowledge, this subject-driven approach to testing usefulness has never before been used." ></td>
	<td class="line x" title="58:220	3.2 Experiment Design Ten human subjects were recruited to evaluate full-text documents and two summary types.4 The original text documents were taken from the Topic Detection and Tracking 3 (TDT-3) corpus (Allan et al. , 1999) which contains news stories and headlines, topic and event descriptions, and a mapping between news stories and their related topic and/or events." ></td>
	<td class="line x" title="59:220	Although the TDT-3 collection contains transcribed speech documents, our investigation was restricted to documents that were originally text, i.e., newspaper or newswire, not broadcast news." ></td>
	<td class="line x" title="60:220	For our experiment we selected three distinct events and related document sets5 from TDT-3." ></td>
	<td class="line x" title="61:220	For each event, the subjects were given a description of the event (written by LDC) and then asked to judge relevance of a set of 20 documents associated with that event (using three different presentation types to be discussed below)." ></td>
	<td class="line x" title="62:220	The events used from the TDT data set were events from world news occurring in 1998." ></td>
	<td class="line x" title="63:220	It is possible that the subjects had some prior knowledge about the events, yet we believe that this would not affect their ability to complete the task." ></td>
	<td class="line x" title="64:220	Subjects background knowledge of an event can also make this task more similar to real-world browsing tasks, in which subjects are often familiar with the event or topic they are searching for." ></td>
	<td class="line x" title="65:220	The 20 documents were retrieved by a search engine." ></td>
	<td class="line x" title="66:220	We used a constrained subset where exactly half (10) were judged relevant by the LDC annotators." ></td>
	<td class="line x" title="67:220	Because all 20 documents were somewhat similar to the event, this approach ensured that our task would be more difficult than it would be if we had chosen documents from completely unrelated events (where the choice of relevance would be obvious even from a poorly written summary)." ></td>
	<td class="line x" title="68:220	Each document was pre-annotated with the headline associated with the original newswire source." ></td>
	<td class="line x" title="69:220	These headlines were used as the first summary type." ></td>
	<td class="line x" title="70:220	We refer to them as HEAD (Headline Surrogate)." ></td>
	<td class="line x" title="71:220	The average length of the HEAD surrogates was 53 characters." ></td>
	<td class="line x" title="72:220	In addition, we commissioned human-generated summaries6 of each document as the second summary type; we refer 4We required all human subjects to be native-English speakers to ensure that the accuracy of judgments was not degraded by language barriers." ></td>
	<td class="line x" title="73:220	5The three event and related document sets contained enough data points to achieve statistically significant results." ></td>
	<td class="line x" title="74:220	6The human summarizers were instructed to create a summary no greater than 75 characters for each specified full text document." ></td>
	<td class="line x" title="75:220	The summaries were not compared for writing style or quality." ></td>
	<td class="line x" title="76:220	to this as HUM (Human Surrogate)." ></td>
	<td class="line x" title="77:220	The average length of the HUM surrogates was 72 characters." ></td>
	<td class="line x" title="78:220	Although neither of these summaries was produced automatically, our experiment allowed us to focus on the question of summary usefulness and to learn about the differences in presentation style as a first step toward experimentation with the output of automatic summarization systems." ></td>
	<td class="line x" title="79:220	Two main factors were measured: (1) differences in judgments for the three presentation types (HEAD, HUM, and the full-text document) and (2) judgment time." ></td>
	<td class="line x" title="80:220	Each subject made a total of 60 judgments for each presentation type since there were 3 distinct events and 20 documents per event." ></td>
	<td class="line x" title="81:220	To facilitate the analysis of the data, the subjects judgments were constrained to two possibilities, relevant or not relevant.7 Although the HEAD and HUM surrogates were both produced by humans, they differed in style." ></td>
	<td class="line x" title="82:220	The HEAD surrogates were shorter than the HUM surrogates by 26%." ></td>
	<td class="line x" title="83:220	Many of these were eye-catchers designed to entice the reader to examine the entire document (i.e. , purchase the newspaper); that is, the HEAD surrogates were not intended to stand in the place of the full document." ></td>
	<td class="line x" title="84:220	By contrast, the writers of the HUM surrogates were instructed to write text that conveyed what happened in the full document." ></td>
	<td class="line x" title="85:220	We observed that the HUM surrogates used more words and phrases extracted from the full documents than the HEAD surrogates." ></td>
	<td class="line x" title="86:220	Experiments were conducted using a web browser (Internet Explorer) on a PC in the presence of the experimenter." ></td>
	<td class="line x" title="87:220	Subjects were given written and verbal instructions for completing their task and were asked to make relevance judgments on a practice event set." ></td>
	<td class="line x" title="88:220	The judgments from the practice event set were not included in our experimental results or used in our analyses." ></td>
	<td class="line x" title="89:220	The written instructions were given to aid subjects in determining requirements for relevance." ></td>
	<td class="line x" title="90:220	For example, in an Election event documents describing new people in office, new public officials, change in governments or parliaments were suggested as evidence for relevance." ></td>
	<td class="line x" title="91:220	Each of the ten subjects made judgments on 20 documents for each of three different events." ></td>
	<td class="line x" title="92:220	After reading each document or summary, the subjects clicked on a radio button corresponding to their judgment and clicked a submit button to move to the next document description." ></td>
	<td class="line x" title="93:220	Subjects were not allowed to move to the next summary/document until a valid selection was made." ></td>
	<td class="line x" title="94:220	No backing up was allowed." ></td>
	<td class="line x" title="95:220	Judgment time was computed as the number of seconds it took the subject to read the full text document or surrogate, comprehend it, compare it to the event description, and make a judgment (timed up until the subject clicked the submit button)." ></td>
	<td class="line x" title="96:220	7If we allowed subjects to make additional judgments such as somewhat relevant, this could possibly encourage subjects to always choose this when they were the least bit unsure." ></td>
	<td class="line x" title="97:220	Previous experiments indicate that this additional selection method may increase the level of variability in judgments (Zajic et al. , 2004)." ></td>
	<td class="line x" title="98:220	3 3.3 Order of Document/Surrogate Presentation One concern with our evaluation methodology was the issue of possible memory effects or priming: if the same subjects saw a summary and a full document about the same event, their answers might be tainted." ></td>
	<td class="line x" title="99:220	Thus, prior to the full experiment, we conducted pre-experiments (using 4 participants) with an extreme form of influence: we presented the summary and full text in immediate succession." ></td>
	<td class="line x" title="100:220	In these experiments, we compared two document presentation approaches, termed Drill Down and Complete Set. In the Drill Down document presentation approach all three presentation types were shown for each document, in sequence: first a single HEAD surrogate, followed by the corresponding HUM surrogate, followed by the full text document." ></td>
	<td class="line x" title="101:220	This process was repeated 10 times." ></td>
	<td class="line x" title="102:220	In the Complete Set document-presentation approach we presented the complete set of documents using one surrogate type, followed by the complete set using another surrogate type, and so on." ></td>
	<td class="line x" title="103:220	That is, the 10 HEAD surrogates were displayed all at once, followed by the corresponding 10 HUM surrogates, followed by the corresponding 10 full-text documents." ></td>
	<td class="line x" title="104:220	The results indicated that there was almost no effect between the two document-presentation approaches." ></td>
	<td class="line x" title="105:220	The performance varied only slightly and neither approach consistently allowed subjects to perform better than the other." ></td>
	<td class="line x" title="106:220	Therefore, we determined that the subjects were not associating a given summary with its corresponding full-text documents." ></td>
	<td class="line x" title="107:220	This may be due, in part, to the fact that all 20 documents were related to the eventand according to the LDC relevance judgments half of these were actually about the same event." ></td>
	<td class="line x" title="108:220	Given that the variations were insignificant in these pre-experiments, we selected only the Complete-Set approach (no Drill-Down) for the full experiment." ></td>
	<td class="line x" title="109:220	However, we still needed to vary the ordering for the two surrogate presentation types associated with each full-text document." ></td>
	<td class="line x" title="110:220	Thus, each 20-document set was divided in half for each subject." ></td>
	<td class="line x" title="111:220	In the first half, the subject saw the first 10 documents as: (1) HEAD surrogates, then HUM surrogates and then the full-text document; or (2) HUM surrogates, then HEAD surrogates, and then the full-text document." ></td>
	<td class="line x" title="112:220	In the second half, the subject saw the alternative ordering, e.g., if a subject saw HEAD surrogates before HUM surrogates in the first half, he/she saw the HUM surrogates before HEAD surrogates for the second half." ></td>
	<td class="line x" title="113:220	Either way, the full-text document was always shown last so as not to introduce judgment effects associated with reading the entire document before either surrogate type." ></td>
	<td class="line x" title="114:220	In addition to varying the ordering for the surrogate type, the ordering of the surrogates and full documents within the events were also varied." ></td>
	<td class="line x" title="115:220	The subjects were grouped in pairs, and each pair viewed the surrogates and documents in a different order than the other pairs." ></td>
	<td class="line x" title="116:220	3.4 Experimental Hypotheses We hypothesized that the summaries would allow subjects to achieve a Relevance-Prediction rate of 7090%." ></td>
	<td class="line x" title="117:220	Since these summaries were significantly shorter than the original document text, we expected that the rate would not be 100% compared to the judgments made on the full document text." ></td>
	<td class="line x" title="118:220	However, we expected higher than a 50% ratio, i.e., higher than that of random judgments on all of the surrogates." ></td>
	<td class="line x" title="119:220	We also expected high performance because the meaning of the original document text is best preserved when written by a human (Mani, 2001)." ></td>
	<td class="line x" title="120:220	A second hypothesis is that the HEAD surrogates would yield a significantly lower agreement rate than that of the HUM surrogates." ></td>
	<td class="line x" title="121:220	Our commissioned HUM surrogates were written to stand in place of the full document, whereas the HEAD surrogates were written to catch a readers interest." ></td>
	<td class="line x" title="122:220	This suggests that the HEAD surrogates might not provide as informative a description of the original documents as the HUM surrogates." ></td>
	<td class="line x" title="123:220	We also tested a third hypothesis: that our RelevancePrediction measure would be more reliable than that of the LDC-Agreement method used for SUMMAC-style evaluations (thus providing a more stable framework for evaluating summarization techniques)." ></td>
	<td class="line x" title="124:220	LDC-Agreement compares a subjects judgment on a surrogate or full text against the correct judgments as assigned by the TDT corpus annotators (Linguistic Data Consortium 2001)." ></td>
	<td class="line x" title="125:220	Finally, we tested the hypothesis that using a text summary for judging relevance would take considerably less time than using the corresponding full-text document." ></td>
	<td class="line x" title="126:220	4 Experimental Results Table 1 shows the subjects judgments using both Relevance-Prediction and LDC-Agreement for each of three events." ></td>
	<td class="line x" title="127:220	Using our Relevance-Prediction measure, the HUM surrogates yield averages between 79% and 86%, with an overall average of 81%, thus confirming our first hypothesis." ></td>
	<td class="line x" title="128:220	However, we failed to confirm our second hypothesis." ></td>
	<td class="line x" title="129:220	The HEAD Relevance-Prediction rates were between 71% and 82%, with an overall average of 76%, which was lower than the rates for HUM, but the difference was not statistically significant." ></td>
	<td class="line x" title="130:220	It appeared that subjects were able to make consistent relevance decisions from the non-extractive HEAD surrogates, even though these were shorter and less informative than the HUM surrogates." ></td>
	<td class="line x" title="131:220	A closer look reveals that the HEAD summaries sometimes contained enough information to judge relevance, yielding almost the same number of true positives (and true negatives) as the HUM summaries." ></td>
	<td class="line x" title="132:220	For example, a document about the formation of a coalition government to avoid violence in Cambodia has the HEAD surrogate Cambodians hope new government can avoid past mistakes." ></td>
	<td class="line x" title="133:220	By contrast, the HUM surrogate for this same event was Rival parties to form a coalition government to avoid violence in Cambodia." ></td>
	<td class="line x" title="134:220	Although the HEAD surrogate 4 Surrogate EVENT 1 EVENT 2 EVENT 3 Overall Avg Avg Time LDC RP LDC RP LDC RP LDC RP (seconds) HEAD 67% 76% 66% 71% 70% 82% 67% 76% 4.60 HUM 69% 80% 73% 86% 62% 79% 68% 81% 4.57 DOC         13.38 Table 1: Relevance-Prediction (RP) and LDC-Agreement (LDC) Rates for HEAD and HUM Surrogates for each Event uses words that do not appear in the original document (hope and mistakes), the subject may infer the relevance of this surrogate by relating hope to the notion of forming a coalition government and mistakes to violence." ></td>
	<td class="line x" title="135:220	On the other hand, we found that the lower degree of informativeness of HEAD surrogates gave rise to over 50% more false negatives than the HUM summaries." ></td>
	<td class="line x" title="136:220	This statistically significant difference will be discussed further in Section 6." ></td>
	<td class="line x" title="137:220	As for our third hypothesis, Table 1 illustrates a substantial difference between the two agreement measures." ></td>
	<td class="line x" title="138:220	For each of the three events, the RelevancePrediction rate is at least five percent higher than that of the LDC-Agreement approach, with an average of 8.8% increase for the HEAD summary and a 13.3% average increase for the HUM summary." ></td>
	<td class="line x" title="139:220	The average rates across events show a statistically significant difference between LDC-Agreement and Relevance-Prediction for both HUM summaries with p<0.01 and HEAD summaries with p<0.05." ></td>
	<td class="line x" title="140:220	This significance was determined through use of a single factor ANOVA statistical analysis." ></td>
	<td class="line x" title="141:220	The higher Relevance-Prediction rate supports our statement that this approach provides a more stable framework for evaluating different summarization techniques." ></td>
	<td class="line x" title="142:220	Finally, the average timing results shown in Table 1 confirm our fourth hypothesis." ></td>
	<td class="line x" title="143:220	The subjects took 4-5 seconds (on average) to make judgments on both the HEAD and HUM summaries, as compared to about 13.4 seconds to make judgments on full text documents." ></td>
	<td class="line x" title="144:220	This shows that it takes subjects almost 3 times longer to make judgments on full text documents as it took to make judgments on the summaries (HEAD and HUM)." ></td>
	<td class="line x" title="145:220	This finding is not surprising since text summaries are an order of magnitude shorter than full-text documents." ></td>
	<td class="line o" title="146:220	5 Correlation with Intrinsic Evaluation Metric: ROUGE We now turn to the task of correlating our extrinsic task performance with scores produced by an intrinsic evaluation measure." ></td>
	<td class="line o" title="147:220	We used the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric version 1.2.1." ></td>
	<td class="line n" title="148:220	In previous studies (Dorr et al. , 2004) ROUGE was shown to have a very low correlation with the LDC-Agreement measurement results of the extrinsic task." ></td>
	<td class="line x" title="149:220	This was attributed to low interannotator agreement in the gold standard." ></td>
	<td class="line o" title="150:220	Our goal was to test whether our new RelevancePrediction technique would allow us to induce higher correlations with ROUGE." ></td>
	<td class="line o" title="151:220	5.1 Extrinsic Agreement Data To reduce the effect of outliers on the correlation between ROUGE and the human judgments, we averaged over all judgments for each subject (20 judgments3 events) to produce 60 data points." ></td>
	<td class="line x" title="152:220	These data points were then partitioned into either 1, 2, or 4 partitions of equal size." ></td>
	<td class="line x" title="153:220	(Partitions of size four have 15 data points, partitions of size two have 30 data points, and partitions of size one have 60 data points per subjector a total of 600 datapoints across all 10 subjects)." ></td>
	<td class="line x" title="154:220	To ensure that the correlation did not depend on a specific partition, we repeated this same process using 10,000 different (randomly generated) partitions for each of the three partition sizes." ></td>
	<td class="line x" title="155:220	Partitioned data points of size four provided a high degree of noise reduction without compromising the size of the data set (15 points)." ></td>
	<td class="line x" title="156:220	Larger partition sizes would result in too few data points and compromise the statistical significance of our correlation results." ></td>
	<td class="line x" title="157:220	In order to show the variation within a single partition, we used the partitioning of size 4 with the smallest mean square error on the human headline compared to the other partitionings as a representative partition." ></td>
	<td class="line x" title="158:220	For this representative partitioning, the individual data points P1P15 of that partition are shown for each of the two agreement measures in Tables 2 and 3." ></td>
	<td class="line x" title="159:220	This shows that, across partitions, the maximum and minimum Relevance-Prediction rates for HEAD (93% and 60%) are higher than the corresponding LDC-Agreement rates (85% and 50%)." ></td>
	<td class="line x" title="160:220	The same trend is seen with the HUM surrogates: RelevancePrediction maximum of 98%, minimum of 68%; and LDC-Agreement maximum 88%, minimum of 55%." ></td>
	<td class="line o" title="161:220	5.2 Intrinsic ROUGE Score To correlate the partitioned agreement scores above with our intrinsic measure, we first ran ROUGE on all 120 surrogates in our experiment (i.e. , the HUM and HEAD surrogates for each of the 60 event/document pairs) and then averaged the ROUGE scores for all surrogates belonging to the same partitions (for each of the three partition sizes)." ></td>
	<td class="line o" title="162:220	These partitioned ROUGE values were then used for detecting correlations with the corresponding partitioned agreement scores described above." ></td>
	<td class="line o" title="163:220	Table 4 shows the ROUGE scores, based on 3 reference summaries per document, for partitions P1P15 used in the previous tables.8 For brevity, we include 8We commissioned a total of 180 human-generated reference summaries (3 for each of 60 documents) (in addition to the human generated summaries used in the experiment)." ></td>
	<td class="line o" title="164:220	5 Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 HEAD 80% 80% 85% 70% 73% 60% 80% 75% 60% 75% 88% 68% 80% 93% 83% HUM 83% 88% 85% 68% 75% 75% 93% 75% 98% 90% 75% 70% 80% 90% 78% Table 2: Relevance-Prediction Rates for HEAD and HUM Surrogates (Representative Partition of Size 4) Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 HEAD 70% 73% 85% 70% 63% 60% 60% 85% 50% 73% 70% 78% 65% 63% 73% HUM 68% 75% 58% 68% 75% 70% 68% 80% 88% 58% 63% 55% 55% 60% 78% Table 3: LDC-Agreement Rates for HEAD and HUM Surrogates (Representative Partition of Size 4) Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 Avg HEAD.10 .23 .13 .27 .20 .24 .26 .22 .13 .08 .30 .16 .26 .27 .30 .211 HUM .16 .22 .17 .23 .19 .36 .39 .29 .28 .25 .37 .22 .22 .39 .27 .269 Table 4: Average Rouge-1 Scores for HEAD and HUM Surrogates (Representative Partition of Size 4) only ROUGE 1-gram measurement (R1).9 The ROUGE scores for HEAD surrogates were slightly lower than those for HUM surrogates." ></td>
	<td class="line x" title="165:220	This is consistent with our statements earlier about the difference between nonextractive eye-catchers and informative headlines." ></td>
	<td class="line o" title="166:220	Because ROUGE measures whether a particular summary has the same words (or n-grams) as a reference summary, a more constrained choice of words (as found in the extractive HUM surrogates) makes it more likely that the summary would match the reference." ></td>
	<td class="line x" title="167:220	A summary in which the word choice is less constrainedas in the non-extractive HEAD surrogatesis less likely to share n-grams with the reference." ></td>
	<td class="line x" title="168:220	Thus, we may see non-extractive summaries that have almost identical meanings, but very different words." ></td>
	<td class="line n" title="169:220	This raises the concern that ROUGE may be sensitive to the style of summarization that is used." ></td>
	<td class="line x" title="170:220	Section 6 discusses this point further." ></td>
	<td class="line o" title="171:220	5.3 Intrinsic and Extrinsic Correlation To test whether ROUGE correlates more highly with Relevance-Prediction than with LDC-Agreement, we calculated the correlation for the results of both techniques using Pearsons r (Siegel and Castellan, 1988): summationtextn i=1(rir)(sis)radicalbigsummationtext n i=1(rir)2 radicalbigsummationtextn i=1(sis)2 where ri is the ROUGE score of surrogate i, r is the average ROUGE score of all data points, si is the agreement score of summary i (using Relevance-Prediction or LDC-Agreement), and s is the average agreement score." ></td>
	<td class="line oc" title="172:220	Pearsons statistics is commonly used in summarization and machine translation evaluation, see e.g.(Lin, 2004; Lin and Och, 2004)." ></td>
	<td class="line o" title="174:220	As one might expect, there is some variability in the correlation between ROUGE and human judgments for 9We also computed ROUGE 2-gram, ROUGE L and ROUGE W, but the trend for these did not differ from ROUGE1." ></td>
	<td class="line x" title="175:220	Figure 1: Distribution of the Correlation Variation for Relevance-Prediction on HEAD and HUM the different partitions." ></td>
	<td class="line x" title="176:220	However, the boxplots for both HEAD and HUM indicate that the first and third quartile were relatively close to the median (see Figure 1)." ></td>
	<td class="line o" title="177:220	Table 5 shows the Pearson Correlations with ROUGE1 using Relevance-Prediction and LDC-Agreement." ></td>
	<td class="line x" title="178:220	For Relevance-Prediction, we observed a positive correlation for both surrogate types, with a slightly higher correlation for HEAD than HUM." ></td>
	<td class="line o" title="179:220	For LDC-Agreement, we observed no correlation (or a minimally negative one) with ROUGE-1 scores, for both the HEAD and HUM surrogates." ></td>
	<td class="line x" title="180:220	The highest correlation was observed for Relevance-Prediction on HEAD." ></td>
	<td class="line o" title="181:220	We conclude that ROUGE correlates more highly with the Relevance-Prediction measurement than the LDCAgreement measurement, although we should add that none of the correlations in Table 5 were statistically significant atp< 0.05." ></td>
	<td class="line o" title="182:220	The low LDC-Agreement scores are consistent with previous studies where poor correlations 6 Surrogate P = 1 P = 2 P = 4 HEAD (RP) 0.1270 0.1943 0.3140 HUM (RP) 0.0632 0.1096 0.1391 HEAD (LDC) -0.0968 -0.0660 -0.0099 HUM (LDC) -0.0395 -0.0236 -0.0187 Table 5: Pearson Correlations with ROUGE-1 for Relevance-Prediction (RP) and LDC-Agreement (LDC), where Partition size (P) = 1, 2, and 4 were attributed to low interannotator agreement rates." ></td>
	<td class="line n" title="183:220	6 Discussion Our results suggest that ROUGE may be sensitive to the style of summarization that is used." ></td>
	<td class="line x" title="184:220	As we observed above, many of the HEAD surrogates were not actually summaries of the full text, but were eye-catchers." ></td>
	<td class="line x" title="185:220	Often, these surrogates did not allow the subject to judge relevance correctly, resulting in lower agreement." ></td>
	<td class="line o" title="186:220	In addition, these same surrogates often did not use a high percentage of words that were actually from the story, resulting in low ROUGE scores." ></td>
	<td class="line x" title="187:220	(We noticed that most words in the HUM surrogates appeared in the corresponding stories)." ></td>
	<td class="line o" title="188:220	There were three consequences of this difference between HEAD and HUM: (1) The rate of agreement was lower for HEAD than for HUM; (2) The average ROUGE score was lower for HEAD than for HUM; and (3) The correlation of ROUGE scores with agreement was higher for HEAD than for HUM." ></td>
	<td class="line o" title="189:220	A further analysis supports the (somewhat counterintuitive) third point above." ></td>
	<td class="line o" title="190:220	Although the ROUGE scores of true positives (and true negatives) were significantly lower for HEAD surrogates (0.2127 and 0.2162) than for HUM surrogates (0.2696 and 0.2715), the number of false negatives was substantially higher for HEAD surrogates than for HUM surrogates." ></td>
	<td class="line o" title="191:220	These cases corresponded to much lower ROUGE scores for HEAD surrogates (0.1996) than for HUM (0.2586) surrogates." ></td>
	<td class="line o" title="192:220	A summary of this analysis is given in Table 6, where true positives and negatives are indicated by Rel/Rel and NonRel/NonRel, respectively, and false positives and negatives are indicated by Rel/NonRel and NonRel/Rel, respectively.10 The numbers in parentheses after each ROUGE score refer to the standard deviation for that 10We also included (average) elapsed times for summary judgments in each of the four categories." ></td>
	<td class="line x" title="193:220	One might expect a relevant judgment to be much quicker than a non-relevant judgment (since the latter might require reading the full summary)." ></td>
	<td class="line x" title="194:220	However, it turned out non-relevant judgments did not always take longer." ></td>
	<td class="line x" title="195:220	In fact, the NonRel/NonRel cases took considerably less time than the Rel/Rel and Rel/NonRel cases." ></td>
	<td class="line x" title="196:220	On the other hand, the NonRel/Rel cases took considerably more timealmost as much time as reading the full text documents an indication that the subjects may have re-read the summary a number of times, perhaps vacillating back and forth." ></td>
	<td class="line x" title="197:220	Still, the overall time savings was significant, given that the vast majority of the non-relevant judgments were in the NonRel/NonRel category." ></td>
	<td class="line x" title="198:220	score." ></td>
	<td class="line x" title="199:220	This was computed as follows: Std.-Dev." ></td>
	<td class="line o" title="200:220	= radicalBiggsummationtext N i=1(xix)2 N where N is the number of surrogates in a particular judgment category (e.g. , N = 245 for the HEAD-based NonRel/Rel judgments), xi is the ROUGE score for the ith surrogate, and r is the average of all ROUGE scores in that category." ></td>
	<td class="line x" title="201:220	Although there were very few false positives (less than 6% for both HEAD and HUM), the number of false negatives (NonRel/Rel) was particularly high for HEAD (50% higher than for HUM)." ></td>
	<td class="line x" title="202:220	This difference was statistically significant at p<0.01 using the t-test." ></td>
	<td class="line x" title="203:220	The large number of false negatives with HEAD may be attributed to the eye-catching nature of these surrogates." ></td>
	<td class="line x" title="204:220	A subject may be misled into thinking that this surrogate is not related to an event because the surrogate does not contain words from the event description and is too broad for the subject to extract definitive information (e.g. , the surrogate There he goes again!)." ></td>
	<td class="line o" title="205:220	Because the false negatives were associated with the lowest average ROUGE score (0.1996), we speculate that, if a correlation exists between RelevancePrediction and ROUGE, the false negatives may be a major contributing factor." ></td>
	<td class="line n" title="206:220	Based on this experiment, we conjecture that ROUGE may not be a good method for measuring the usefulness of summaries when the summaries are not extractive." ></td>
	<td class="line n" title="207:220	That is, if someone intentionally writes summaries that contain different words than the story, the summaries will also likely contain different words than a reference summary, resulting in low ROUGE scores." ></td>
	<td class="line x" title="208:220	However, the summaries, if well-written, could still result in high agreement with the judgments made on the full text." ></td>
	<td class="line x" title="209:220	7 Conclusion We have shown that two types of human summaries, HEAD and HUM, can be useful for relevance assessment in that they help a user achieve 70-85% agreement in relevance judgments." ></td>
	<td class="line x" title="210:220	We observed a 65% reduction in judgment time between full texts and summaries." ></td>
	<td class="line x" title="211:220	These findings are important in that they establish the usefulness of summarization and they support research and development of additional summarization methods, including automatic methods." ></td>
	<td class="line x" title="212:220	We introduced a new method for measuring agreement, Relevance-Prediction, which takes a subjects full-text judgment as the standard against which the same subjects summary judgment is measured." ></td>
	<td class="line x" title="213:220	Because Relevance-Prediction was more reliable than LDCAgreement judgments, we encourage others to use this measure in future summarization evaluations." ></td>
	<td class="line o" title="214:220	Using this new method, we were able to find positive correlations between relevance assessments and ROUGE scores for HUM and HEAD surrogates, where only 7 Judgment HEAD HUM (Surr/Doc) Raw R1-Avg Avg Time Raw R1-Avg Avg Time Rel/Rel 211 (35%) 0.2127 (0.120) 4.6 251 (42%) 0.2696 (0.130) 4.2 Rel/NonRel 27 (5%) 0.2115 (0.110) 7.1 35 (6%) 0.2725 (0.131) 4.6 NonRel/Rel 117 (19%) 0.1996 (0.127) 8.5 77 (13%) 0.2586 (0.120) 13.8 NonRel/NonRel 245 (41%) 0.2162 (0.126) 2.5 237 (39%) 0.2715 (0.131) 1.9 TOTAL 600 (100%) 0.2115 (0.124) 4.6 600 (100%) 0.2691 (0.129) 4.6 Table 6: Subjects Judgments and Corresponding Average ROUGE 1 Scores negative correlations were found using LDC-Agreement scores." ></td>
	<td class="line o" title="215:220	We found that both the Relevance-Prediction and the ROUGE-1 scores were higher for human-generated summaries than for the original headlines." ></td>
	<td class="line o" title="216:220	It appears that most of the difference is induced by surrogates that are eye-catchers (rather than true summaries), where both agreement and ROUGE scores are low." ></td>
	<td class="line x" title="217:220	Our future work will include further experimentation with automatic summarization methods to determine the level of Relevance-Prediction." ></td>
	<td class="line x" title="218:220	We aim to determine how well automatic summarizers help users complete tasks, and to investigate which automatic summarizers perform better than others." ></td>
	<td class="line o" title="219:220	We also plan to test for correlations between ROUGE and human task performance with automatic summaries, to further investigate whether ROUGE is a good predictor of human task performance." ></td>
	<td class="line x" title="220:220	Acknowledgements This work was supported in part by DARPA TIDES Cooperative Agreement N66001-00-2-8910." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0907
Evaluating DUC 2004 Tasks With The QARLA Framework
Amigó, Enrique;Gonzalo, Julio;Penas, Anselmo;Verdejo, M. Felisa;"></td>
	<td class="line x" title="1:132	Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 4956, Ann Arbor, June 2005." ></td>
	<td class="line xc" title="2:132	c2005 Association for Computational Linguistics Evaluating DUC 2004 Tasks with the QARLA Framework Enrique Amigo, Julio Gonzalo, Anselmo Penas, Felisa Verdejo Departamento de Lenguajes y Sistemas Informaticos Universidad Nacional de Educacion a Distancia c/Juan del Rosal, 16 28040 Madrid Spain {enrique,julio,anselmo,felisa}@lsi.uned.es Abstract This papers reports the application of the QARLA evaluation framework to the DUC 2004 testbed (tasks 2 and 5)." ></td>
	<td class="line x" title="3:132	Our experiment addresses two issues: how well QARLA evaluation measures correlate with human judgements, and what additional insights can be provided by the QARLA framework to the DUC evaluation exercises." ></td>
	<td class="line x" title="4:132	1 Introduction QARLA (Amigo et al. , 2005) is a framework that uses similarity to models as a building block for the evaluation of automatic summarisation systems." ></td>
	<td class="line x" title="5:132	The input of QARLA is a summarisation task, a set of test cases, a set of similarity metrics, and sets of models and automatic summaries (peers) for each test case." ></td>
	<td class="line x" title="6:132	With such a testbed, QARLA provides:  A measure, QUEEN, which combines assorted similarity metrics to estimate the quality of automatic summarisers." ></td>
	<td class="line x" title="7:132	 A measure, KING, to select the best combination of similarity metrics." ></td>
	<td class="line x" title="8:132	 An estimation, JACK, of the reliability of the testbed for evaluation purposes." ></td>
	<td class="line x" title="9:132	The QARLA framework does not rely on human judges." ></td>
	<td class="line x" title="10:132	It is interesting, however, to find out how wellanevaluationusingQARLAcorrelateswithhuman judges, and whether QARLA can provide additional insights into an evaluation based on human assessments." ></td>
	<td class="line x" title="11:132	In this paper, we apply the QARLA framework (QUEEN, KING and JACK measures) to the output of two different evaluation exercises: DUC 2004 tasks 2 and 5 (Over and Yen, 2004)." ></td>
	<td class="line x" title="12:132	Task 2 requires short (one-hundred word) summaries for assorted document sets; Task 5 consists of generating a short summary in response to a Who is question." ></td>
	<td class="line x" title="13:132	In Section 2, we summarise the QARLA evaluation framework; in Section 3, we describe the similarity metrics used in the experiments." ></td>
	<td class="line x" title="14:132	Section 4 discusses the results of the QARLA framework using such metrics on the DUC testbeds." ></td>
	<td class="line x" title="15:132	Finally, Section 5 draws some conclusions." ></td>
	<td class="line x" title="16:132	2 The QARLA evaluation framework QARLA uses similarity to models for the evaluation of automatic summarisation systems." ></td>
	<td class="line x" title="17:132	Here we summarise its main features; the reader may refer to (Amigo et al. , 2005) for details." ></td>
	<td class="line x" title="18:132	The input of the framework is:  A summarisation task (e.g. topic oriented, informative multi-document summarisation on a given domain/corpus)." ></td>
	<td class="line x" title="19:132	 A set T of test cases (e.g. topic/document set pairs for the example above)  A set of summaries M produced by humans (models), and a set of automatic summaries A (peers), for every test case." ></td>
	<td class="line x" title="20:132	 A set X of similarity metrics to compare summaries." ></td>
	<td class="line x" title="21:132	With this input, QARLA provides three main measures that we describe below." ></td>
	<td class="line x" title="22:132	49 2.1 QUEEN: Estimating the quality of an automatic summary QUEEN operates under the assumption that a summary is better if it is closer to the model summaries according to all metrics; it is defined as the probability, measured onM M M, that for every metric in X the automatic summary a is closer to a model than two models to each other: QUEENX,M(a)  P(x  X.x(a,m)  x(mprime,mprimeprime)) where a is the automatic summary being evaluated, m,mprime,mprimeprime are three models in M, and x(a,m) stands for the similarity ofmtoa." ></td>
	<td class="line x" title="23:132	QUEEN is stated as a probability, and therefore its range of values is [0,1]." ></td>
	<td class="line x" title="24:132	We can think of the QUEEN measure as using a set of tests (every similarity metric in X) to falsify the hypothesis that a given summary a is a model." ></td>
	<td class="line x" title="25:132	Givena,m,mprime,mprimeprime,wetestx(a,m)  x(mprime,mprimeprime) for each metric x. a is accepted as a model only if it passes the test for every metric." ></td>
	<td class="line x" title="26:132	QUEEN(a) is, then, the probability of acceptance for a in the sample space M M M. This measure has some interesting properties: (i) it is able to combine different similarity metrics into a single evaluation measure; (ii) it is not affected by the scale properties of individual metrics, i.e. it does not require metric normalisation and it is not affected by metric weighting." ></td>
	<td class="line x" title="27:132	(iii) Peers which are very far from the set of models all receive QUEEN=0." ></td>
	<td class="line x" title="28:132	Inotherwords, QUEENdoesnotdistinguish between very poor summarisation strategies." ></td>
	<td class="line x" title="29:132	(iv) The value of QUEEN is maximised for peers that merge with the models under all metrics inX." ></td>
	<td class="line x" title="30:132	(v) The universal quantifier on the metric parameter x implies that adding redundant metrics do not bias the result of QUEEN." ></td>
	<td class="line x" title="31:132	Now the question is: which similarity metrics are adequate to evaluate summaries?" ></td>
	<td class="line x" title="32:132	Imagine that we use a similarity metric based on sentence coselection; it might happen that humans do not agree on which sentences to select, and therefore emulating their sentence selection behaviour is both easy (nobody agrees with each other) and useless." ></td>
	<td class="line x" title="33:132	We need to take into account which are the features that human summaries do share, and evaluate according to them." ></td>
	<td class="line x" title="34:132	This is provided by the KING measure." ></td>
	<td class="line x" title="35:132	2.2 KING: estimating the quality of similarity metrics The measure KINGM,A(X) estimates the quality of a set of similarity metrics X using a set of models M and a set of peers A. KING is defined as the probability that a model has higher QUEEN value than any peer in a test sample." ></td>
	<td class="line x" title="36:132	Formally: KINGM,A(X)  P(a  A,QUEENM,X(m) > QUEENM,X(a)) For example, an ideal metric -that puts all models together-would give QUEEN(m) = 1 for all models, and QUEEN(a) = 0 for all peers which are not put together with the models, obtaining KING = 1." ></td>
	<td class="line x" title="37:132	KING satisfies several interesting properties: (i) KING does not depend on the scale properties of the metric; (ii) Adding repeated or very similar peers do not alter the KING measure, which avoids one way of biasing the measure." ></td>
	<td class="line x" title="38:132	(iii) the KING value of random and constant metrics is zero or close to zero." ></td>
	<td class="line x" title="39:132	2.3 JACK: reliability of the peer set Once we detect a difference in quality between two summarisation systems, the question is now whether this result is reliable." ></td>
	<td class="line x" title="40:132	Would we get the same results using a different test set (different examples, different human summarisers (models) or different baseline systems)?" ></td>
	<td class="line x" title="41:132	The first step is obviously to apply statistical significance tests to the results." ></td>
	<td class="line x" title="42:132	But even if they give a positive result, it might be insufficient." ></td>
	<td class="line x" title="43:132	The problem is that the estimation of the probabilities in KING assumes that the sample sets M,A are not biased." ></td>
	<td class="line x" title="44:132	If M,A are biased, the results can be statistically significant and yet unreliable." ></td>
	<td class="line x" title="45:132	The set of examples and the behaviour of human summarisers (models) should be somehow controlled either for homogeneity (if the intended profile of examples and/or users is narrow) or representativity (if it is wide)." ></td>
	<td class="line x" title="46:132	But how to know whether the set of automatic summaries is representative and therefore is not penalising certain automatic summarisation strategies?" ></td>
	<td class="line x" title="47:132	This is addressed by the JACK measure: 50 JACK(X,M,A)  P(a,aprime  A| x  X.x(a,aprime)  x(a,m) x(aprime,a)  x(aprime,m) QUEEN(a) > 0 QUEEN(aprime) > 0) i.e. theprobabilityoverallmodelsummariesmof finding a couple of automatic summariesa,aprime which are closer to m than to each other according to all metrics." ></td>
	<td class="line x" title="48:132	This measure satisfies three desirable properties: (i) it can be enlarged by increasing the similarity of the peers to the models (the x(m,a) factor in the inequalities), i.e. enhancing the quality of the peer set; (ii) it can also be enlarged by decreasing the similarity between automatic summaries (the x(a,aprime) factor in the inequality), i.e. augmenting the diversity of (independent) automatic summarisation strategies represented in the test bed; (iii) adding elements to A cannot diminish the JACK value, because of the existential quantifier on a,aprime." ></td>
	<td class="line x" title="49:132	3 Selection of similarity metrics Each different similarity metric characterises different features of a summary." ></td>
	<td class="line x" title="50:132	Our first objective is to select the best set of metrics, that is, the metrics which bestcharacterise thehuman summaries(models) as opposed to automatic summaries." ></td>
	<td class="line x" title="51:132	The second objective is to obtain as much information as possible about the behaviour of automatic summaries." ></td>
	<td class="line x" title="52:132	In this Section, we begin by describing a set of 59 metrics used as a starting point." ></td>
	<td class="line x" title="53:132	Some of them provide overlapping information; the second step is then to select a subset of metrics that minimises redundancy and, at the same time, maximises quality (KING values)." ></td>
	<td class="line x" title="54:132	Finally, we analyse the characteristics of the selected metrics." ></td>
	<td class="line o" title="55:132	3.1 Similarity metrics For this work, we have considered the following similarity metrics: ROUGE based metrics (R): ROUGE (Lin and Hovy, 2003) estimates the quality of an automatic summary on the basis of the n-gram coverage related to a set of human summaries (models)." ></td>
	<td class="line o" title="56:132	Although ROUGE is an evaluation metric, we can adapt it to behave as a similarity metric between pairs of summaries if we consider only one model in the computation." ></td>
	<td class="line oc" title="57:132	There are different kinds of ROUGE metrics such as ROUGE-W, ROUGE-L, ROUGE1, ROUGE-2, ROUGE-3, ROUGE-4, etc.(Lin, 2004b)." ></td>
	<td class="line o" title="59:132	Each of these metrics has been applied over summaries with three preprocessing options: with stemming and stopword removal (type c); only with stopwords removal (type b); or without any kind of preprocessing (type a)." ></td>
	<td class="line o" title="60:132	All these combinations give 24 similarity metrics based on ROUGE." ></td>
	<td class="line o" title="61:132	Inverted ROUGE based metrics (Rpre): ROUGE metrics are recall oriented." ></td>
	<td class="line o" title="62:132	If we reverse the directionofthesimilaritycomputation, weobtain precision oriented metrics (i.e. Rpre(a,b) = R(b,a))." ></td>
	<td class="line o" title="63:132	In this way, we generate another 24 metrics based on inverted ROUGE." ></td>
	<td class="line x" title="64:132	TruncatedVectModel (TVMn): This family of metrics compares the distribution of the n most relevant terms from original documents in the summaries." ></td>
	<td class="line x" title="65:132	The process is the following: (1) obtaining thenmost frequent lemmas ignoring stopwords; (2) generating a vector with the relative frequency of each term in the summary; (3) calculating the similarity between two vectors as the inverse of the Euclidean distance." ></td>
	<td class="line x" title="66:132	We have used 9 variants of this measure with n = 1,4,8,16,32,64,128,256,512." ></td>
	<td class="line x" title="67:132	AveragedSentencelengthSim (AVLS): This is a very simple metric that compares the average length of the sentences in two summaries." ></td>
	<td class="line x" title="68:132	It can be useful to compare the degree of abstraction of the summaries." ></td>
	<td class="line x" title="69:132	GRAMSIM: This similarity metric compares the distribution of the part-of-speech tags in the two summaries." ></td>
	<td class="line x" title="70:132	The processing is the following: (1) part-of-speech tagging of summaries using TreeTagger ; (2) generation of a vector with the tags frequency for each summary; (3) calculation of the similarity between two vectors as the inverse of the Euclidean distance." ></td>
	<td class="line x" title="71:132	This similarity metric is not content oriented, but syntax-oriented." ></td>
	<td class="line x" title="72:132	51 Figure 1: Similarity Metric Clusters 3.2 Clustering similarity metrics From the set of metrics described above we have 57 (24+24+9) content oriented metrics, plus two metrics based on stylistic features (AVLS and GRAMSIM)." ></td>
	<td class="line x" title="73:132	However, the 57 metrics characterising summary contents are highly redundant." ></td>
	<td class="line x" title="74:132	Thus, clustering similar metrics seems desirable." ></td>
	<td class="line x" title="75:132	We perform an automatic clustering process using the following notion of proximity between two metric sets: sim(X,Xprime)  Prob[H(X)  H(Xprime)] where H(X)  x  X.x(a,m)  x(mprime,mprimeprime) Two metrics sets are similar, according to the formula, if they behave similarly with respect to the QUEEN condition (H predicate in the formula), i.e. the probability that the two sets of metrics discriminate the same automatic summaries when they are compared to the same pair of models." ></td>
	<td class="line x" title="76:132	Figure 1 shows the clustering of similarity metrics for the DUC 2004 Task 2." ></td>
	<td class="line x" title="77:132	The number of clusters was fixed in 10." ></td>
	<td class="line o" title="78:132	After the clustering process, the 48 ROUGE metrics are grouped in 7 sets, and the 9 TVM metrics are grouped in 3 sets." ></td>
	<td class="line o" title="79:132	In each cluster, the metric with highest KING has been marked in boldface." ></td>
	<td class="line o" title="80:132	Note that the ROUGE-c metrics (with stemming)withhighestKINGarethosebasedonrecall whereas the ROUGE-a/b metrics (without stemming)arethosebasedonprecision." ></td>
	<td class="line x" title="81:132	RegardingTVM clusters, themetricswithhighestKINGineachcluster are those based on a higher number of terms." ></td>
	<td class="line x" title="82:132	Finally, we select the metric with highest KING in each group, obtaining the 10 most representative metrics." ></td>
	<td class="line x" title="83:132	3.3 Best evaluation metric: KING values Figure 2 shows the KING values for the selected similaritymetrics, whichrepresenthoweverymetric characterises model summaries as opposed to automatic summaries." ></td>
	<td class="line x" title="84:132	These are the main results:  The last column shows the best metric set, considering all possible metric combinations." ></td>
	<td class="line x" title="85:132	In both DUC tasks, the best combination is {Rpre-W-1.2.b,TVM.512." ></td>
	<td class="line x" title="86:132	This metric set gets better KING values than any individual metric inisolation(17%betterthanthesecondbestfor task 2, and 23% better for task 5)." ></td>
	<td class="line x" title="87:132	This is an interesting result confirming that we can improve our ability to characterise human summaries just by combining standard similarity metrics in the QARLA framework." ></td>
	<td class="line x" title="88:132	Note also that both metrics in the best set are content-oriented." ></td>
	<td class="line p" title="89:132	 Rpre-W.1.2.b (inverted ROUGE measure, using non-contiguous word sequences, removing stopwords, without stemming) obtains the highest individual KING for task 2, and is one of the best in task 5, confirming that ROUGEbased metrics are a robust way of evaluating summaries, and indicating that non-contiguous word sequences can be more useful for evaluation purposes than n-grams." ></td>
	<td class="line x" title="90:132	52 Figure 2: Similarity Metric quality  TVM metrics get higher values when considering more terms (TVM.512), confirming that comparing with just a few terms (e.g. TVM.4) is not informative enough." ></td>
	<td class="line x" title="91:132	 Overall, KING values are higher for task 5, suggesting that there is more agreement between human summaries in topic-oriented tasks." ></td>
	<td class="line x" title="92:132	3.4 Reliability of the results The JACK measure estimates the reliability of QARLA results, and is correlated with the diversity of automatic summarisation strategies included in thetestbed." ></td>
	<td class="line x" title="93:132	Inprinciple, thelargerthenumberofautomatic summaries, the higher the JACK values we should obtain." ></td>
	<td class="line x" title="94:132	The important point is to determine when JACK values tend to stabilise; at this point, it is not useful to add more automatic summaries without introducing new summarisation strategies." ></td>
	<td class="line x" title="95:132	Figure 3 shows how JACKRpre-W,TVM.512 values grow when adding automatic summaries." ></td>
	<td class="line x" title="96:132	For more than 10 systems, JACK values grow slower in both tasks." ></td>
	<td class="line x" title="97:132	Absolute JACK values are higher in Task 2 than in task 5, indicating that systems tend to produce more similar summaries in Task 5 (perhaps becauseitisatopic-orientedtask)." ></td>
	<td class="line x" title="98:132	Thisresultsuggests that we should incorporate more diverse summarisation strategies in Task 5 to enhance the reliability of the testbed for evaluation purposes with QARLA." ></td>
	<td class="line x" title="99:132	4 Evaluation of automatic summarisers: QUEEN values The QUEEN measure provides two kinds of information to compare automatic summarisation systems: which are the best systems -according to the best metric set-, and which are the individual features of every automatic summariser -according to individual similarity metrics-." ></td>
	<td class="line x" title="100:132	4.1 System ranking The best metric combination for both tasks was {Rpre-W,TVM.512}; therefore, our global system evaluation uses this combination of content-oriented metrics." ></td>
	<td class="line x" title="101:132	Figure 4 shows the QUEENRpre-W,TVM.512} values for each participating system in DUC 2004, also including the model summaries." ></td>
	<td class="line x" title="102:132	As expected, model summaries obtain the highest QUEEN values in both DUC tasks, with a significant distance with respect to the automatic summaries." ></td>
	<td class="line x" title="103:132	4.2 Correlation with human judgements The manual ranking generated in DUC is based on a set of human-produced evaluation criteria, whereas the QARLA framework gives more weight to the aspects that characterise model summaries as opposed to automatic summaries." ></td>
	<td class="line x" title="104:132	It is interesting, however, to find out whether both evaluation methodologies are correlated." ></td>
	<td class="line x" title="105:132	Indeed, this is the case: the Pearson correlation between manual and QUEEN rankings is 0.92 for the Task 2 and 0.96 for the Task 5." ></td>
	<td class="line x" title="106:132	Of course, QUEEN values depend on the chosen metric set X; it is also interesting to check whether 53 Figure 3: JACK vs. Number of Automatic Summaries Figure 4: QUEEN system ranking for the best metric set (A-H are models) Figure 5: Correlation Between DUC and QARLA results 54 Figure 6: QUEEN values over GRAMSIM metrics with higher KING values lead to QUEEN rankings more similar to human judgements." ></td>
	<td class="line x" title="107:132	Figure 5 shows the Pearson correlation between manual and QUEEN rankings for 1024 metric combinations with different KING values." ></td>
	<td class="line x" title="108:132	The figure confirms that higher KING values are associated with rankings closer to human judgements." ></td>
	<td class="line x" title="109:132	4.3 Stylistic features The best metric combination leaves out similarity metrics based on stylistic features." ></td>
	<td class="line x" title="110:132	It is interesting, however, to see how automatic summaries behave with respect to this kind of features." ></td>
	<td class="line x" title="111:132	Perhaps the most remarkable fact about stylistic similarities is that, in the case of the GRAMSIM metric, task 2 and task 5 exhibit a rather different behaviour (see Figure 6)." ></td>
	<td class="line x" title="112:132	In task 2, systems merge with the models, while in task 5 the QUEEN values of the systems are inferior to the models." ></td>
	<td class="line x" title="113:132	This suggests that there is some stylistic component in models that systems are not capturing in the topic-oriented task." ></td>
	<td class="line oc" title="114:132	5 Related work The methodology which is closest to our framework is ORANGE (Lin, 2004a), which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set." ></td>
	<td class="line x" title="115:132	As in our framework, ORANGE performs an automatic meta-evaluation, there is no need for human assessments, and it does not depend on the scale properties of the metric being evaluated (because changes of scale preserve rankings)." ></td>
	<td class="line x" title="116:132	The ORANGE approach is, indeed, intimately related to the original QARLA measure introduced in (Amigo et al. , 2004)." ></td>
	<td class="line x" title="117:132	There are several approaches to the automatic evaluation of summarisation and Machine Translation systems (Culy and Riehemann, 2003; Coughlin, 2003)." ></td>
	<td class="line x" title="118:132	Probably the most significant improvement over ORANGE is the ability to combine automatically the information of different metrics." ></td>
	<td class="line x" title="119:132	Our impression is that a comprehensive automatic evaluation of a summary must necessarily capture different aspects of the problem with different metrics, and that the results of every individual checking (metric) should not be combined in any prescribed algebraic way (such as a linear weighted combination)." ></td>
	<td class="line x" title="120:132	Our framework satisfies this condition." ></td>
	<td class="line x" title="121:132	ORANGE, however, has also an advantage over the QARLA framework, namely that it can be used for evaluation metrics which are not based on similarity between model/peer pairs." ></td>
	<td class="line o" title="122:132	For instance, ROUGE can be applied directly in the ORANGE framework without any reformulation." ></td>
	<td class="line x" title="123:132	6 Conclusions The application of the QARLA evaluation framework to the DUC testbed provides some useful insights into the problem of evaluating text summarisation systems:  The results show that a combination of similarity metrics behaves better than any metric in isolation." ></td>
	<td class="line x" title="124:132	ThebestmetricsetisRpre-W,TVM.512}, a combination of content-oriented metrics." ></td>
	<td class="line x" title="125:132	Un55 surprisingly, stylistic similarity is less useful for evaluation purposes." ></td>
	<td class="line x" title="126:132	 The evaluation provided by QARLA correlates well with the rankings provided by DUC human judges." ></td>
	<td class="line o" title="127:132	For both tasks, metric sets with higher KING values slightly outperforms the best ROUGE evaluation measure." ></td>
	<td class="line x" title="128:132	 QARLA measures show that DUC tasks 2 and 5are quitedifferentin nature." ></td>
	<td class="line x" title="129:132	In Task5, human summaries are more similar, and the automatic summarisation strategies evaluated are less diverse." ></td>
	<td class="line x" title="130:132	Acknowledgements We are indebted to Ed Hovy, Donna Harman, Paul Over, Hoa Dang and Chin-Yew Lin for their inspiring and generous feedback at different stages in the development of QARLA." ></td>
	<td class="line x" title="131:132	We are also indebted to NIST for hosting Enrique Amigo as a visitor and for providing the DUC test beds." ></td>
	<td class="line x" title="132:132	This work has been partially supported bythe Spanishgovernment, project R2D2 (TIC-2003-7180)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-2006
Class Model Adaptation For Speech Summarisation
Chatain, Pierre;Whittaker, Edward W. D.;Mrozinski, Joanna;Furui, Sadaoki;"></td>
	<td class="line x" title="1:71	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 2124, New York, June 2006." ></td>
	<td class="line x" title="2:71	c2006 Association for Computational Linguistics Class Model Adaptation for Speech Summarisation Pierre Chatain, Edward W.D. Whittaker, Joanna Mrozinski and Sadaoki Furui Dept. of Computer Science Tokyo Institute of Technology 2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552, Japan {pierre, edw, mrozinsk, furui}@furui.cs.titech.ac.jp Abstract The performance of automatic speech summarisation has been improved in previous experiments by using linguistic model adaptation." ></td>
	<td class="line x" title="3:71	We extend such adaptation to the use of class models, whose robustness further improves summarisation performance on a wider variety of objective evaluation metrics such as ROUGE-2 and ROUGE-SU4 used in the text summarisation literature." ></td>
	<td class="line x" title="4:71	Summaries made from automatic speech recogniser transcriptions benefit from relative improvements ranging from 6.0% to 22.2% on all investigated metrics." ></td>
	<td class="line x" title="5:71	1 Introduction Techniques for automatically summarising written text have been actively investigated in the field of natural language processing, and more recently new techniques have been developed for speech summarisation (Kikuchi et al. , 2003)." ></td>
	<td class="line x" title="6:71	However it is still very hard to obtain good quality summaries." ></td>
	<td class="line x" title="7:71	Moreover, recognition accuracy is still around 30% on spontaneous speech tasks, in contrast to speech read from text such as broadcast news." ></td>
	<td class="line x" title="8:71	Spontaneous speech is characterised by disfluencies, repetitions, repairs, and fillers, all of which make recognition and consequently speech summarisation more difficult (Zechner, 2002)." ></td>
	<td class="line x" title="9:71	In a previous study (Chatain et al. , 2006), linguistic model (LiM) adaptation using different types of word models has proved useful in order to improve summary quality." ></td>
	<td class="line x" title="10:71	However sparsity of the data available for adaptation makes it difficult to obtain reliable estimates of word n-gram probabilities." ></td>
	<td class="line x" title="11:71	In speech recognition, class models are often used in such cases to improve model robustness." ></td>
	<td class="line x" title="12:71	In this paper we extend the work previously done on adapting the linguistic model of the speech summariser by investigating class models." ></td>
	<td class="line x" title="13:71	We also use a wider variety of objective evaluation metrics to corroborate results." ></td>
	<td class="line x" title="14:71	2 Summarisation Method The summarisation system used in this paper is essentially the same as the one described in (Kikuchi et al. , 2003), which involves a two step summarisation process, consisting of sentence extraction and sentence compaction." ></td>
	<td class="line x" title="15:71	Practically, only the sentence extraction part was used in this paper, as preliminary experiments showed that compaction had little impact on results for the data used in this study." ></td>
	<td class="line x" title="16:71	Important sentences are first extracted according to the following score for each sentence W = w1,w2,,wn, obtained from the automatic speech recognition output: S(W) = 1N Nsummationdisplay i=1 {CC(wi)+II(wi)+LL(wi)}, (1) where N is the number of words in the sentence W, and C(wi), I(wi) and L(wi) are the confidence score, the significance score and the linguistic score of word wi, respectively." ></td>
	<td class="line x" title="17:71	C, I and L are the respective weighting factors of those scores, determined experimentally." ></td>
	<td class="line x" title="18:71	For each word from the automatic speech recogni21 tion transcription, a logarithmic value of its posterior probability, the ratio of a word hypothesis probability to that of all other hypotheses, is calculated using a word graph obtained from the speech recogniser and used as a confidence score." ></td>
	<td class="line x" title="19:71	For the significance score, the frequencies of occurrence of 115k words were found using the WSJ and the Brown corpora." ></td>
	<td class="line x" title="20:71	In the experiments in this paper we modified the linguistic component to use combinations of different linguistic models." ></td>
	<td class="line x" title="21:71	The linguistic component gives the linguistic likelihood of word strings in the sentence." ></td>
	<td class="line x" title="22:71	Starting with a baseline LiM (LiMB) we perform LiM adaptation by linearly interpolating the baseline model with other component models trained on different data." ></td>
	<td class="line x" title="23:71	The probability of a given n-gram sequence then becomes: P(wi|win+1wi1) = 1P1(wi|win+1wi1) + + nPn(wi|win+1wi1), (2) wheresummationtextk k = 1 and k and Pk are the weight and the probability assigned by model k. In the case of a two-sided class-based model, Pk(wi|win+1wi1) = Pk(wi|C(wi))  Pk(C(wi)|C(win+1)C(wi1)), (3) where Pk(wi|C(wi)) is the probability of the word wi belonging to a given class C, and Pk(C(wi)|C(win+1)C(wi1)) the probability of a certain word class C(wi) to appear after a history of word classes, C(win+1),,C(wi1)." ></td>
	<td class="line x" title="24:71	Different types of component LiM are built, coming from different sources of data, either as word or class models." ></td>
	<td class="line x" title="25:71	The LiMB and component LiMs are then combined for adaptation using linear interpolation as in Equation (2)." ></td>
	<td class="line x" title="26:71	The linguistic score is then computed using this modified probability as in Equation (4): L(wi) = logP(wi|win+1wi1)." ></td>
	<td class="line x" title="27:71	(4) 3 Evaluation Criteria 3.1 Summarisation Accuracy To automatically evaluate the summarised speeches, correctly transcribed talks were manually summarised, and used as the correct targets for evaluation." ></td>
	<td class="line x" title="28:71	Variations of manual summarisation results are merged into a word network, which is considered to approximately express all possible correct summarisations covering subjective variations." ></td>
	<td class="line x" title="29:71	The word accuracy of automatic summarisation is calculated as the summarisation accuracy (SumACCY) using the word network (Hori et al. , 2003): Accuracy = (LenSubInsDel)/Len100[%], (5) where Sub is the number of substitution errors, Ins is the number of insertion errors, Del is the number of deletion errors, and Len is the number of words in the most similar word string in the network." ></td>
	<td class="line oc" title="30:71	3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results." ></td>
	<td class="line o" title="31:71	ROUGE F-measure scores are given for ROUGE2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4 (skip-bigram), using the model average (average score across all references) metric." ></td>
	<td class="line x" title="32:71	4 Experimental Setup Experiments were performed on spontaneous speech, using 9 talks taken from the Translanguage English Database (TED) corpus (Lamel et al. , 1994; Wolfel and Burger, 2005), each transcribed and manually summarised by nine different humans for both 10% and 30% summarization ratios." ></td>
	<td class="line x" title="33:71	Speech recognition transcriptions (ASR) were obtained for each talk, with an average word error rate of 33.3%." ></td>
	<td class="line x" title="34:71	A corpus consisting of around ten years of conference proceedings (17.8M words) on the subject of speech and signal processing is used to generate the LiMB and word classes using the clustering algorithm in (Ney et al. , 1994)." ></td>
	<td class="line x" title="35:71	Different types of component LiM are built and combined for adaptation as described in Section 2." ></td>
	<td class="line x" title="36:71	The first type of component linguistic models are built on the small corpus of hand-made summaries described above, made for the same summarisation ratio as the one we are generating." ></td>
	<td class="line x" title="37:71	For each talk the hand-made summaries of the other eight talks (i.e. 72 summaries) were used as the LiM training corpus." ></td>
	<td class="line x" title="38:71	This type of LiM is expected to help generate automatic summaries in the same style as those made manually." ></td>
	<td class="line x" title="39:71	22 Baseline Adapted SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4 10% Random 34.4 0.104 0.055 0.142 Word 63.1 0.186 0.130 0.227 67.8 0.193 0.140 0.228 Class 65.1 0.195 0.131 0.226 72.6 0.210 0.143 0.234 Mixed 63.6 0.186 0.128 0.218 71.8 0.211 0.139 0.231 30% Random 71.2 0.294 0.198 0.331 Word 81.6 0.365 0.271 0.395 83.3 0.365 0.270 0.392 Class 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442 Mixed 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442 Table 1: TRS baseline and adapted results." ></td>
	<td class="line x" title="40:71	The second type of component linguistic models are built from the papers in the conference proceedings for the talk we want to summarise." ></td>
	<td class="line x" title="41:71	This type of LiM, used for topic adaptation, is investigated because key words and important sentences that appear in the associated paper are expected to have a high information value and should be selected during the summarisation process." ></td>
	<td class="line x" title="42:71	Three sets of experiments were made: in the first experiment (referred to as Word), LiMB and both component models are word models, as introduced in (Chatain et al. , 2006)." ></td>
	<td class="line x" title="43:71	For the second one (Class), both LiMB and the component models are class models built using exactly the same data as the word models." ></td>
	<td class="line x" title="44:71	For the third experiment (Mixed), the LiMB is an interpolation of class and word models, while the component LiMs are class models." ></td>
	<td class="line x" title="45:71	To optimise use of the available data, a rotating form of cross-validation (Duda and Hart, 1973) is used: all talks but one are used for development, the remaining talk being used for testing." ></td>
	<td class="line x" title="46:71	Summaries from the development talks are generated automatically by the system using different sets of parameters and the LiMB." ></td>
	<td class="line x" title="47:71	These summaries are evaluated and the set of parameters which maximises the development score for the LiMB is selected for the remaining talk." ></td>
	<td class="line x" title="48:71	The purpose of the development phase is to choose the most effective combination of weights C, I and L. The summary generated for each talk using its set of optimised parameters is then evaluated using the same metric, which gives us our baseline for this talk." ></td>
	<td class="line x" title="49:71	Using the same parameters as those that were selected for the baseline, we generate summaries for the lectures in the development set for different LiM interpolation weights k. Values between 0 and 1 in steps of 0.1, were investigated for the latter, and an optimal set of k is selected." ></td>
	<td class="line x" title="50:71	Using these interpolation weights, as well as the set of parameters determined for the baseline, we generate a summary of the test talk, which is evaluated using the same evaluation metric, giving us our final adapted result for this talk." ></td>
	<td class="line x" title="51:71	Averaging those results over the test set (i.e. all talks) gives us our final adapted result." ></td>
	<td class="line x" title="52:71	This process is repeated for all evaluation metrics, and all three experiments (Word, Class, and Mixed)." ></td>
	<td class="line x" title="53:71	Lower bound results are given by random summarisation (Random) i.e. randomly extracting sentences and words, without use of the scores present in Equation (1) for appropriate summarisation ratios." ></td>
	<td class="line x" title="54:71	5 Results 5.1 TRS Results Initial experiments were made on the human transcriptions (TRS), and results are given in Table 1." ></td>
	<td class="line x" title="55:71	Experiments on word models (Word) show relative improvements in terms of SumACCY of 7.5% and 2.1% for the 10% and 30% summarisation ratios, respectively." ></td>
	<td class="line o" title="56:71	ROUGE metrics, however, do not show any significant improvement." ></td>
	<td class="line o" title="57:71	Using class models (Class and Mixed), for all ROUGE metrics, relative improvements range from 3.5% to 13.4% for the 10% summarisation ratio, and from 8.6% to 16.5% on the 30% summarisation ratio." ></td>
	<td class="line x" title="58:71	For SumACCY, relative improvements between 11.5% to 12.9% are observed." ></td>
	<td class="line x" title="59:71	5.2 ASR Results ASR results for each experiment are given in Table 2 for appropriate summarisation ratios." ></td>
	<td class="line x" title="60:71	As for 23 Baseline Adapted SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4 10% Random 33.9 0.095 0.042 0.140 Word 48.6 0.143 0.064 0.182 49.8 0.129 0.060 0.173 Class 50.0 0.133 0.063 0.170 55.1 0.156 0.077 0.193 Mixed 48.5 0.134 0.068 0.176 56.2 0.142 0.077 0.191 30% Random 56.1 0.230 0.124 0.283 Word 66.7 0.265 0.157 0.314 68.7 0.271 0.161 0.328 Class 66.1 0.277 0.165 0.324 71.1 0.300 0.180 0.348 Mixed 64.9 0.268 0.160 0.312 70.5 0.304 0.192 0.351 Table 2: ASR baseline and adapted results." ></td>
	<td class="line o" title="61:71	the TRS, LiM adaptation showed improvements in terms of SumACCY, but ROUGE metrics do not corroborate those results for the 10% summarisation ratio." ></td>
	<td class="line o" title="62:71	Using class models, for all ROUGE metrics, relative improvements range from 6.0% to 22.2% and from 7.4% to 20.0% for the 10% and 30% summarisation ratios, respectively." ></td>
	<td class="line x" title="63:71	SumACCY relative improvements range from 7.6% to 15.9%." ></td>
	<td class="line o" title="64:71	6 Discussion Compared to previous experiments using only word models, improvements obtained using class models are larger and more significant for both ROUGE and SumACCY metrics." ></td>
	<td class="line x" title="65:71	This can be explained by the fact that the data we are performing adaptation on is very sparse, and that the nine talks used in these experiments are quite different from each other, especially since the speakers also vary in style." ></td>
	<td class="line x" title="66:71	Class models are more robust to this spontaneous speech aspect than word models, since they generalise better to unseen word sequences." ></td>
	<td class="line x" title="67:71	There is little difference between the Class and Mixed results, since the development phase assigned most weight to the class model component in the Mixed experiment, making the results quite similar to those of the Class experiment." ></td>
	<td class="line x" title="68:71	7 Conclusion In this paper we have investigated linguistic model adaptation using different sources of data for an automatic speech summarisation system." ></td>
	<td class="line x" title="69:71	Class models have proved to be much more robust than word models for this process, and relative improvements ranging from 6.0% to 22.2% were obtained on a variety of evaluation metrics on summaries generated from automatic speech recogniser transcriptions." ></td>
	<td class="line x" title="70:71	Acknowledgements: The authors would like to thank M. Wolfel for the recogniser transcriptions and C. Hori for her work on two stage summarisation and gathering the TED corpus data." ></td>
	<td class="line x" title="71:71	This work is supported by the 21st Century COE Programme." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1139
Stochastic Language Generation Using WIDL-Expressions And Its Application In Machine Translation And Summarization
Soricut, Radu;Marcu, Daniel;"></td>
	<td class="line x" title="1:231	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 11051112, Sydney, July 2006." ></td>
	<td class="line x" title="2:231	c2006 Association for Computational Linguistics Stochastic Language Generation Using WIDL-expressions and its Application in Machine Translation and Summarization Radu Soricut Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 radu@isi.edu Daniel Marcu Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 marcu@isi.edu Abstract We propose WIDL-expressions as a flexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications." ></td>
	<td class="line x" title="3:231	WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions." ></td>
	<td class="line x" title="4:231	We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation." ></td>
	<td class="line x" title="5:231	1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al. , 2002), etc. However, when it comes to end-to-end, text-totext applications  Machine Translation, Summarization, Question Answering  these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al. , 2002; Habash, 2003)." ></td>
	<td class="line x" title="6:231	We believe two reasons explain this state of affairs." ></td>
	<td class="line x" title="7:231	First, these generic NLG systems use input representation languages with complex syntax and semantics." ></td>
	<td class="line x" title="8:231	These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGENT, PATIENT, etc. , for Penman and FUF), syntactic relations (such as subject, object, premod, etc. , for HALogen), or lexical dependencies (Fergus, Amalgam)." ></td>
	<td class="line x" title="9:231	Such inputs cannot be accurately produced by state-of-the-art analysis components from arbitrary textual input in the context of text-to-text applications." ></td>
	<td class="line x" title="10:231	Second, most of the recent systems (starting with Nitrogen) have adopted a hybrid approach to generation, which has increased their robustness." ></td>
	<td class="line x" title="11:231	These hybrid systems use, in a first phase, symbolic knowledge to (over)generate a large set of candidate realizations, and, in a second phase, statistical knowledge about the target language (such as stochastic language models) to rank the candidate realizations and find the best scoring one." ></td>
	<td class="line x" title="12:231	The disadvantage of the hybrid approach  from the perspective of integrating these systems within end-to-end applications  is that the two generation phases cannot be tightly coupled." ></td>
	<td class="line x" title="13:231	More precisely, input-driven preferences and target languagedriven preferences cannot be integrated in a true probabilistic model that can be trained and tuned for maximum performance." ></td>
	<td class="line x" title="14:231	In this paper, we propose WIDL-expressions (WIDL stands for Weighted Interleave, Disjunction, and Lock, after the names of the main operators) as a representation formalism that facilitates the integration of a generic sentence realization system within end-to-end language applications." ></td>
	<td class="line x" title="15:231	The WIDL formalism, an extension of the IDL-expressions formalism of Nederhof and Satta (2004), has several crucial properties that differentiate it from previously-proposed NLG representation formalisms." ></td>
	<td class="line x" title="16:231	First, it has a simple syntax (expressions are built using four operators) and a simple, formal semantics (probability distributions over finite sets of strings)." ></td>
	<td class="line x" title="17:231	Second, it is a compact representation that grows linearly 1105 in the number of words available for generation (see Section 2)." ></td>
	<td class="line x" title="18:231	(In contrast, representations such as word lattices (Knight and Hatzivassiloglou, 1995) or non-recursive CFGs (Langkilde-Geary, 2002) require exponential space in the number of words available for generation (Nederhof and Satta, 2004))." ></td>
	<td class="line x" title="19:231	Third, it has good computational properties, such as optimal algorithms for intersection with a0 -gram language models (Section 3)." ></td>
	<td class="line x" title="20:231	Fourth, it is flexible with respect to the amount of linguistic processing required to produce WIDLexpressions directly from text (Sections 4 and 5)." ></td>
	<td class="line x" title="21:231	Fifth, it allows for a tight integration of inputspecific preferences and target-language preferences via interpolation of probability distributions using log-linear models." ></td>
	<td class="line x" title="22:231	We show the effectiveness of our proposal by directly employing a generic WIDL-based generation system in two end-to-end tasks: machine translation and automatic headline generation." ></td>
	<td class="line x" title="23:231	2 The WIDL Representation Language 2.1 WIDL-expressions In this section, we introduce WIDL-expressions, a formal language used to compactly represent probability distributions over finite sets of strings." ></td>
	<td class="line x" title="24:231	Given a finite alphabet of symbols a1, atomic WIDL-expressions are of the form a2, with a2a4a3 a1." ></td>
	<td class="line x" title="25:231	For a WIDL-expression a5a7a6a8a2, its semantics is a probability distribution a9a11a10a13a12a15a14a17a16a19a18a20a5a22a21a24a23a26a25a28a27a30a29a32a31a34a33 a35a36a30a37a39a38a41a40, where a25a42a27a30a29 a31 a6a44a43a45a2a13a46 and a9a47a10a13a12a15a14a17a16a48a18a20a5a22a21a17a18a48a2a49a21a50a6 a38 . Complex WIDL-expressions are created from other WIDL-expressions, by employing the following four operators, as well as operator distribution functions a51a53a52 from an alphabet a54 . Weighted Disjunction." ></td>
	<td class="line x" title="26:231	If a5a56a55 a37a53a57a53a57a53a57a58a37 a5a60a59 are WIDL-expressions, then a5a61a6 a62a42a63a19a64a65a18a20a5a42a55 a37a53a57a53a57a53a57a45a37 a5a66a59a30a21, with a51a17a67 a23a44a43 a38a68a37a53a57a53a57a53a57a58a37 a0 a46 a33 a35a36a30a37a39a38a41a40, specified such that a1a70a69a45a71a73a72a75a74a77a76a60a78a63a64a75a79a51a17a67a80a18a82a81a47a21a83a6 a38, is a WIDLexpression." ></td>
	<td class="line x" title="27:231	Its semantics is a probability distribution a9a13a10a13a12a15a14a17a16a48a18a20a5a22a21a84a23a85a25a28a27a30a29 a31 a33 a35a36a30a37a39a38a41a40, where a25a28a27a30a29 a31 a6 a86 a59 a52a88a87a60a55 a25a42a27a30a29 a31a80a89, and the probability values are induced by a51a39a67 and a9a49a10a11a12a15a14a41a16a19a18a20a5a66a52a82a21, a38a91a90a93a92a94a90 a0 . For example, if a5a95a6 a62a28a63a19a64a68a18a48a96 a37a98a97 a21, a99 a64a101a100a94a102a17a103a32a104a106a105a39a107a108a45a109a111a110a112a104a106a105a39a107a110a41a113, its semantics is a probability distribution a9a13a10a11a12a15a14a41a16a82a18a20a5a22a21 over a25a28a27a30a29 a31 a6a106a43a45a96 a37a98a97 a46, defined by a114a68a115a68a116a117a119a118a88a120a122a121a124a123a119a120a88a125a41a123 a100 a99 a64 a120a103 a123 a100 a105a39a107a108 and a114 a115a73a116a117a126a118a120a122a121a124a123a119a120a128a127a65a123 a100 a99 a64 a120 a110 a123 a100a4a105a39a107a110 . Precedence." ></td>
	<td class="line x" title="28:231	If a5a22a55 a37 a5a66a129 are WIDL-expressions, then a5a130a6 a5 a55a132a131 a5 a129 is a WIDL-expression." ></td>
	<td class="line x" title="29:231	Its semantics is a probability distribution a9a124a10a13a12a15a14a17a16a82a18a20a5a22a21a4a23 a25a28a27a30a29a32a31a83a33 a35a36a30a37a39a38a41a40, where a25a28a27a30a29a133a31 is the set of all strings that obey the precedence imposed over the arguments, and the probability values are induced by a9a47a10a13a12a15a14a17a16a48a18a20a5a134a55a41a21 and a9a47a10a13a12a15a14a17a16a82a18a20a5a66a129a45a21 . For example, if a5a134a55a22a6a135a62a136a63a98a137a53a18a48a96 a37a98a97 a21, a99 a137 a100a26a102a17a103a66a104a91a105a45a107a108a45a109a13a110a28a104a138a105a39a107a110a17a113, and a5a136a129a133a6 a62a136a63a140a139a58a18a48a141 a37a75a142 a21, a99 a139a134a100a50a102a17a103a70a104a143a105a39a107a144a39a109a111a110a32a104a143a105a45a107a145a39a113, then a5a135a6a146a5a134a55 a131 a5a136a129 represents a probability distribution a9a147a10a13a12a15a14a17a16a19a18a20a5a22a21 over the set a25a28a27a30a29a112a31a148a6 a43a45a96a65a141 a37 a96 a142a149a37a98a97 a141 a37a98a97a124a142 a46, defined by a114 a115a73a116a117a126a118a120a122a121a124a123a119a120a88a125a77a150a98a123 a100 a99 a137 a120a103 a123a99 a139 a120a103 a123 a100a24a105a39a107a145a17a108, a114 a115a68a116a117a126a118a120a122a121a149a123a119a120a88a125a77a151a73a123 a100 a99 a137 a120 a103 a123 a99 a139 a120 a110 a123 a100a4a105a45a107a152a17a110, etc. Weighted Interleave." ></td>
	<td class="line x" title="30:231	If a5a56a55 a37a53a57a53a57a53a57a58a37 a5a60a59 are WIDLexpressions, then a5a153a6a95a154a41a63a19a64a65a18a20a5a42a55 a37 a5a66a129 a37a53a57a53a57a53a57a58a37 a5a60a59a30a21, with a99 a64a136a155a53a156a11a157a147a102a98a158a77a159a140a160a162a161a77a163a39a164a17a161a77a163a88a165a167a166a140a113a41a157a11a102a75a166a119a160a53a168a98a169a22a161a119a166a119a113a136a104a171a170a105a39a109a98a103a119a172, a173a175a174a24a176a49a177a68a178a19a29a112a59, specified such that a1a101a179 a71a73a72a75a74a77a76a66a78a63a19a64 a79a51a17a67a65a18a48a2a49a21a146a6 a38, is a WIDL-expression." ></td>
	<td class="line x" title="31:231	Its semantics is a probability distribution a9a13a10a13a12a15a14a17a16a82a18a20a5a22a21a180a23a85a25a42a27a30a29 a31 a33 a35a36a30a37a39a38a41a40, where a25a28a27a30a29 a31 consists of all the possible interleavings of strings from a25a28a27a30a29 a31a80a89, a38a181a90a182a92a183a90 a0, and the probability values are induced by a51a45a67 and a9a49a10a11a12a15a14a41a16a19a18a20a5a66a52a82a21 . The distribution function a51 a67 is defined either explicitly, over a173a180a174a184a176a49a177a68a178a185a29a112a59 (the set of all permutations of a0 elements), or implicitly, as a51a45a67a80a18a48a27a68a186a77a187a73a177a68a178a189a188a80a177a68a178a185a29a32a190a77a21 . Because the set of argument permutations is a subset of all possible interleavings, a51a45a67 also needs to specify the probability mass for the strings that are not argument permutations, a51a45a67a80a18a191a190a53a187a193a192a39a194a195a177a17a190a41a21 . For example, if a5 a6 a154a77a63a64 a18a48a96 a131 a97a167a37 a141a45a21, a99 a64 a100a95a102a53a103a196a110a24a104 a105a45a107a108a41a105a39a109a42a158a77a159a119a160a162a161a162a163a53a164a17a161a77a163a88a165a167a166a198a197a200a199a20a201a202a203a200a204a205a104 a105a45a107a206a103a75a207a53a109a140a166a140a160a53a168a98a169a56a161a119a166a196a197a20a199a200a201a202a203a200a204a205a104 a105a45a107a105a41a207a17a113, its semantics is a probability distribution a9a124a10a13a12a15a14a17a16a48a18a20a5a22a21, with domain a25a28a27a30a29 a31 a6a93a43a45a96 a97 a141 a37 a141a53a96 a97a167a37 a96a65a141 a97 a46, defined by a114 a115a68a116a117a126a118a120a122a121a149a123a119a120a88a125a162a127a65a150a98a123 a100 a99 a64 a120 a103a56a110 a123 a100a91a105a39a107a108a41a105, a114 a115a73a116a117a126a118a120a122a121a124a123a119a120a128a150a98a125a162a127a65a123 a100 a208a48a209a98a210 a203a88a211a206a212a128a213a88a204a82a214a200a213a88a204a205a53a215a122a216 a137 a100a4a105a45a107a206a103a98a207, a114a68a115a73a116a117a126a118a128a120a122a121a124a123a119a120a88a125a77a150a119a127a68a123 a100 a208a48a209a98a210 a215a217a212a20a197a219a218a68a213a122a215a219a216 a137 a100a4a105a39a107a105a41a207 . Lock." ></td>
	<td class="line x" title="32:231	If a5a42a220 is a WIDL-expression, then a5a93a6 a221 a18a20a5a28a220a88a21 is a WIDL-expression." ></td>
	<td class="line x" title="33:231	The semantic mapping a9a47a10a13a12a15a14a17a16a48a18a20a5a22a21 is the same as a9a13a10a13a12a15a14a17a16a82a18a20a5 a220a21, except that a25a28a27a30a29 a31 contains strings in which no additional symbol can be interleaved." ></td>
	<td class="line x" title="34:231	For example, if a5 a6 a154a77a63a19a64a65a18 a221 a18a48a96 a131 a97 a21 a37 a141a45a21, a99 a64a91a100a44a102a17a103a222a110a184a104 a105a45a107a108a41a105a39a109a124a158a162a159a119a160a162a161a77a163a53a164a41a161a77a163a88a165a167a166a223a104a182a105a45a107a110a77a105a17a113, its semantics is a probability distribution a9a13a10a13a12a15a14a17a16a82a18a20a5a22a21, with domain a25a28a27a30a29 a31 a6 a43a45a141a53a96 a97a111a37 a96 a97 a141a68a46, defined by a114 a115a68a116a117a126a118a120a122a121a149a123a119a120a88a125a162a127a65a150a98a123 a100 a99 a64 a120 a103a56a110 a123 a100 a105a45a107a108a41a105, a114 a115a73a116a117a126a118a120a122a121a124a123a119a120a128a150a98a125a77a127a73a123 a100 a208a48a209a162a210 a203a200a211a15a212a88a213a88a204a82a214a88a213a88a204a205a17a215a224a216 a137 a100a4a105a45a107a110a77a105 . In Figure 1, we show a more complex WIDLexpression." ></td>
	<td class="line x" title="35:231	The probability distribution a51 a55 associated with the operator a154a41a63a126a137 assigns probability 0.2 to the argument order a225 a38a111a226 ; from a probability mass of 0.7, it assigns uniformly, for each of the remaining a226a189a227a147a228a135a38 a6a230a229 argument permutations, a permutation probability value of a67a41a231a232a233 a6 a36a30a57a88a38a17a234 . The 1106 a154a162a63a126a137a58a18 a221 a18a191a186a77a192a193a178a1a0a3a2a190a53a187 a131a5a4 a27a3a6a73a177a68a178a1a7a193a29a133a177a8a7a45a186a162a21 a37 a62a136a63a140a139a65a18 a221 a18a119a178a200a177a8a9a80a177a8a10a190 a131a12a11a13a4 a187a58a186a14a2a15a7 a4 a21 a37 a221 a18a17a16a45a186a119a186a18a16a20a19a8a0a77a177a22a21 a131 a178a20a177a8a9a80a177a8a10a190a77a21a98a21 a37 a2a23a7 a131 a2a122a178a24a16a3a25a193a21 a37 a51a58a55a22a6a135a43a58a225 a38a111a226 a33 a36a30a57 a225 a37 a27a68a186a41a187a58a177a68a178a189a188a80a177a68a178a19a29a32a190a27a26a14a28 a29a30 a74a18a31a219a76 a33 a36a30a57a23a32a193a37 a190a53a187a193a192a39a194a195a177a17a190a27a26a33a28 a29a30 a74a33a31a219a76 a33 a36a30a57a88a38 a46 a37 a51a17a129a32a6a135a43 a38 a33 a36a30a57a35a34 a229 a37 a225 a33 a36a30a57a217a226 a229a193a46 Figure 1: An example of a WIDL-expression." ></td>
	<td class="line x" title="36:231	remaining probability mass of 0.1 is left for the 12 shuffles associated with the unlocked expression a2a15a7 a131 a2a122a178a24a16a3a25, for a shuffle probability of a67a41a231a15a55 a55a185a129 a6 a36a30a57a206a36a65a36a37a36 . The list below enumerates some of the a38 a190a98a186a41a178a17a2a23a7 a4 a37a40a39 a18a191a190a98a186a41a178a17a2a23a7 a4 a21a18a41 pairs that belong to the probability distribution defined by our example: rebels fighting turkish government in iraq 0.130 in iraq attacked rebels turkish goverment 0.049 in turkish goverment iraq rebels fighting 0.005 The following result characterizes an important representation property for WIDL-expressions." ></td>
	<td class="line x" title="37:231	Theorem 1 A WIDL-expression a5 over a1 and a54 using a0 atomic expressions has space complexity O(a0 ), if the operator distribution functions of a5 have space complexity at most O(a0 )." ></td>
	<td class="line x" title="38:231	For proofs and more details regarding WIDLexpressions, we refer the interested reader to (Soricut, 2006)." ></td>
	<td class="line x" title="39:231	Theorem 1 ensures that highcomplexity hypothesis spaces can be represented efficiently by WIDL-expressions (Section 5)." ></td>
	<td class="line x" title="40:231	2.2 WIDL-graphs and Probabilistic Finite-State Acceptors WIDL-graphs." ></td>
	<td class="line x" title="41:231	Equivalent at the representation level with WIDL-expressions, WIDL-graphs allow for formulations of algorithms that process them." ></td>
	<td class="line x" title="42:231	For each WIDL-expression a5, there exists an equivalent WIDL-graph a42 a31 . As an example, we illustrate in Figure 2(a) the WIDL-graph corresponding to the WIDL-expression in Figure 1." ></td>
	<td class="line x" title="43:231	WIDL-graphs have an initial vertex a43a45a44 and a final vertex a43a37a46 . Vertices a43a80a67, a43a48a47, and a43a65a129a98a67 with in-going edges labeled a49 a55 a63 a137, a49 a129 a63 a137, and a49a51a50 a63 a137, respectively, and vertices a43 a233, a43a189a55a53a52, and a43a65a129 a50 with out-going edges labeled a54 a55 a63a126a137, a54 a129 a63a98a137, and a54a51a50 a63a126a137, respectively, result from the expansion of the a154a41a63a126a137 operator." ></td>
	<td class="line x" title="44:231	Vertices a43a193a232 and a43 a55 a50 with in-going edges labeled a18 a55 a63a185a139, a18a129 a63a185a139, respectively, and vertices a43a49a55a185a129 and a43a189a55a53a55 with out-going edges labeled a21 a55 a63a140a139, a21a129 a63a140a139, respectively, result from the expansion of the a62a28a63a185a139 operator." ></td>
	<td class="line x" title="45:231	With each WIDL-graph a42 a31, we associate a probability distribution." ></td>
	<td class="line x" title="46:231	The domain of this distribution is the finite collection of strings that can be generated from the paths of a WIDL-specific traversal of a42 a31, starting from a43a56a44 and ending in a43a37a46 . Each path (and its associated string) has a probability value induced by the probability distribution functions associated with the edge labels of a42 a31 . A WIDL-expression a5 and its corresponding WIDLgraph a42 a31 are said to be equivalent because they represent the same distribution a9a11a10a11a12a15a14a41a16a19a18a20a5a22a21 . WIDL-graphs and Probabilistic FSA." ></td>
	<td class="line x" title="47:231	Probabilistic finite-state acceptors (pFSA) are a wellknown formalism for representing probability distributions (Mohri et al. , 2002)." ></td>
	<td class="line x" title="48:231	For a WIDLexpression a5, we define a mapping, called UNFOLD, between the WIDL-graph a42 a31 and a pFSA a57 a31 . A state a58 in a57 a31 is created for each set of WIDL-graph vertices that can be reached simultaneously when traversing the graph." ></td>
	<td class="line x" title="49:231	State a58 records, in what we call a a154 -stack (interleave stack), the order in which a49 a52 a63,a54 a52 a63 bordered subgraphs are traversed." ></td>
	<td class="line x" title="50:231	Consider Figure 2(b), in which state a35a43a80a67a59a43a48a52a60a43a68a129 a50 a37 a43 a38 a63a98a137 a226 a225a193a46 a40 (at the bottom) corresponds to reaching vertices a43a193a67 a37 a43a8a52, and a43a65a129 a50 (see the WIDL-graph in Figure 2(a)), by first reaching vertex a43a80a129 a50 (inside the a49 a50 a63a126a137, a54 a50 a63a126a137 bordered subgraph), and then reaching vertex a43 a52 (inside the a49 a129 a63a98a137, a54 a129 a63a98a137 bordered sub-graph)." ></td>
	<td class="line x" title="51:231	A transition labeled a2 between two a57 a31 states a58a80a55 and a58a58a129 in a57 a31 exists if there exists a vertex a43a3a61 in the description of a58 a55 and a vertex a43 a69 in the description of a58a73a129 such that there exists a path in a42 a31 between a43a5a61 and a43 a69, and a2 is the only a1 -labeled transitions in this path." ></td>
	<td class="line x" title="52:231	For example, transition a35 a43a65a67a59a43a48a52a60a43a68a129 a50 a37 a43 a38 a63a98a137 a226 a225a193a46 a40 a31a63a62a65a64a18a62a67a66a68 a33 a35 a43a65a67a60a43a189a55a53a52a60a43a65a129 a50 a37 a43 a38 a63a126a137 a226 a225a193a46 a40 (Figure 2(b)) results from unfolding the path a43a56a52a70a69a33 a43a189a55a185a67 a31a35a62a67a64a18a62a65a66a68 a33 a43a189a55a98a55a71a69a33a72a43a189a55a185a129 a79 a137 a208 a139 a33a73a43a189a55a53a52 (Figure 2(a))." ></td>
	<td class="line x" title="53:231	A transition labeled a74 between two a57a112a31 states a58 a55 and a58 a129 in a57 a31 exists if there exists a vertex a43a3a61 in the description of a58 a55 and vertices a43 a55a69 a37a53a57a53a57a53a57a45a37 a43 a59a69 in the description of a58a58a129, such that a43a5a61a76a75 a89 a208 a33a77a43 a52 a69 a3a78a42 a31, a38a4a90a171a92 a90 a0 (see transition a35a43a79a44 a37a126a40 a69a33 a35a43a65a67a60a43a8a47a59a43a65a129a98a67 a37 a43 a38 a63a126a137a59a41a119a63a126a137a17a46 a40 ), or if there exists vertices a43 a55 a61 a37a53a57a53a57a53a57a45a37 a43 a59 a61 in the description of a58 a55 and vertex a43 a69 in the description of a58 a129, such that a43 a52 a61 a80a58a89 a208 a33a81a43 a69 a3a82a42 a31, a38 a90a180a92a70a90 a0 . The a74 -transitions 1107 a0 a0a1 a1 a2 a2a3 a3 a4 a4a5 a5 a6 a6a7 a7 a8 a8a9 a9 a10 a10a11 a11 a12 a12a13 a13 a14 a14a15 a15 a16 a16a17 a17 a18 a18a19 a19 a20 a20a21 a21 a22 a22a23 a23 a24 a24a25 a25 a26 a26a27 a27 a28 a28a29 a29 a30 a30a31 a31 a32 a32a33 a33 a34 a34a35 a35 a36 a36a37 a37 a38 a38a39 a39 a40 a40a41 a41 a42 a42a43 a43 a44 a44a45 a45 a46 a46a47 a47 a48 a48a49 a49 a50 a50a51 a51 a52a53a52a53a52 a52a53a52a53a52 a52a53a52a53a52 a52a53a52a53a52 a52a53a52a53a52 a52a53a52a53a52 a54a53a54a53a54 a54a53a54a53a54 a54a53a54a53a54 a54a53a54a53a54 a54a53a54a53a54 a54a53a54a53a54 a55a53a55a53a55 a55a53a55a53a55 a55a53a55a53a55 a55a53a55a53a55 a55a53a55a53a55 a55a53a55a53a55 a55a53a55a53a55 a56a53a56a53a56 a56a53a56a53a56 a56a53a56a53a56 a56a53a56a53a56 a56a53a56a53a56 a56a53a56a53a56 a57a53a57a53a57 a57a53a57a53a57 a57a53a57a53a57 a57a53a57a53a57 a57a53a57a53a57 a57a53a57a53a57 a58a53a58a53a58 a58a53a58a53a58 a58a53a58a53a58 a58a53a58a53a58 a58a53a58a53a58 a58a53a58a53a58 a59a53a59a53a59a53a59 a59a53a59a53a59a53a59 a59a53a59a53a59a53a59 a59a53a59a53a59a53a59 a59a53a59a53a59a53a59 a59a53a59a53a59a53a59 a59a53a59a53a59a53a59 a60a53a60a53a60a53a60 a60a53a60a53a60a53a60 a60a53a60a53a60a53a60 a60a53a60a53a60a53a60 a60a53a60a53a60a53a60 a60a53a60a53a60a53a60 attacked attacked attacked attacked attacked rebels rebels rebels fighting rebels rebels rebels rebels rebels fighting fighting fighting fighting turkish turkish turkish turkish turkish turkish turkish government government government government government government in iraq in in in in in iraq iraq iraq iraq iraq   1 government turkish :0.3 attacked :0.1 :0.3 :1 :1 rebels :0.2 :1 fighting :1rebels :1 1 :0.18 :0.18 :1rebels :1rebels :1  0 6 21 0 6 023 9 23 9 0 2111 0 209 0 1520 6 202 0 21 s e (b)(a) rebels rebels fighting ( ( )2 1 1 1 1 1 1 2 2 2 1 2 3 2 11 3 )12 attacked in iraq         turkish government1 1 1 1 1 1 1 1 1 1 1 2v v v v v v v v v v v v v v v v v v v v v v v v 1 v s e 0 1 2 3 4 v6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 232221 5 0 6 20 0 12319 0 2319 [v, ] 0 12319 0 2319 [v v v,<32][v v v,<32][v v v,<3] [v v v,<3] [v v v,<32] [v v v,<0] [v v v,<32] [v v v,<2] [v v v,<2] [v, ] [v v v,<1 ]   1 11 111 1 1 1 1 0.1 }shuffles0.7,1= { 2 1 3 0.2, other perms 2 = { 1 0.35 }0.65, 2 1[v v v,< > ]1 [v v v,< 0 > ] [v v v,< 321 > ]11 Figure 2: The WIDL-graph corresponding to the WIDL-expression in Figure 1 is shown in (a)." ></td>
	<td class="line x" title="54:231	The probabilistic finite-state acceptor (pFSA) that corresponds to the WIDL-graph is shown in (b)." ></td>
	<td class="line x" title="55:231	are responsible for adding and removing, respectively, the a38 a63,a41a126a63 symbols in the a154 -stack." ></td>
	<td class="line x" title="56:231	The probabilities associated with a57 a31 transitions are computed using the vertex set and the a154 -stack of each a57a32a31 state, together with the distribution functions of the a62 and a154 operators." ></td>
	<td class="line x" title="57:231	For a detailed presentation of the UNFOLD relation we refer the reader to (Soricut, 2006)." ></td>
	<td class="line x" title="58:231	3 Stochastic Language Generation from WIDL-expressions 3.1 Interpolating Probability Distributions in a Log-linear Framework Let us assume a finite set a61 of strings over a finite alphabet a1, representing the set of possible sentence realizations." ></td>
	<td class="line x" title="59:231	In a log-linear framework, we have a vector of feature functions a62 a6 a38 a62 a67 a62 a55 a57a53a57a53a57 a62a64a63 a41, and a vector of parameters a65a182a6 a38 a65a13a67a53a65a124a55 a57a53a57a53a57 a65 a63 a41 . For any a66 a3 a61, the interpolated probability a67a222a18a68a66a58a21 can be written under a log-linear model as in Equation 1: a67a183a18a68a66a73a21a42a6 a69a71a70a73a72 a35a1 a63 a74 a87a124a67 a65 a74 a62 a74 a18a68a66a73a21 a40 a1 a46a76a75 a69a71a70a73a72 a35a1 a63 a74 a87a124a67 a65 a74 a62 a74 a18a68a66 a220a21 a40 (1) We can formulate the search problem of finding the most probable realization a66 under this model as shown in Equation 2, and therefore we do not need to be concerned about computing expensive normalization factors." ></td>
	<td class="line x" title="60:231	a96a78a77a80a79a82a81a195a96 a70 a46 a67a222a18a68a66a58a21a42a6a84a96a78a77a80a79a83a81a195a96 a70 a46 a69a71a70a73a72 a35a1 a63 a74 a87a124a67 a65 a74 a62 a74 a18a68a66a58a21 a40 (2) For a given WIDL-expression a5 over a1, the set a61 is defined by a21 a27a30a29a196a18a48a9a47a10a13a12a15a14a17a16a19a18a20a5a22a21a98a21, and feature function a62a13a67 is taken to be a9a13a10a13a12a15a14a17a16a82a18a20a5a22a21 . Any language model we want to employ may be added in Equation 2 as a feature function a62a13a52, a92a85a84a24a38 . 3.2 Algorithms for Intersecting WIDL-expressions with Language Models Algorithm WIDL-NGLM-Aa86 (Figure 3) solves the search problem defined by Equation 2 for a WIDL-expression a5 (which provides feature function a62 a67 ) and a87 a0 -gram language models (which provide feature functions a62a167a55 a37a53a57a53a57a53a57a58a37 a62 a63 a21 . It does so by incrementally computing UNFOLD for a42 a31 (i.e. , on-demand computation of the corresponding pFSA a57 a31 ), by keeping track of a set of active states, called a88a90a89a92a91a94a93a96a95a78a97 . The set of newly UNFOLDed states is called a98a100a99a53a101a71a102a78a103a105a104 . Using Equation 1 (unnormalized), we EVALUATE the current a67a183a18a68a66a58a21 scores for the a98a73a99a92a101a106a102a78a103a107a104 states." ></td>
	<td class="line x" title="61:231	Additionally, EVALUATE uses an admissible heuristic function to compute future (admissible) scores for the a98a73a99a92a101a71a102a108a103a107a104 states." ></td>
	<td class="line x" title="62:231	The algorithm PUSHes each state from the current a98a100a99a53a101a71a102a78a103a105a104 into a priority queue a109, which sorts the states according to their total score (current a110 admissible)." ></td>
	<td class="line x" title="63:231	In the next iteration, a88a90a89a53a91a94a93a111a95a108a97 is a singleton set containing the state POPed out from the top of a109 . The admissible heuristic function we use is the one defined in (Soricut and Marcu, 2005), using Equation 1 (unnormalized) for computing the event costs." ></td>
	<td class="line x" title="64:231	Given the existence of the admissible heuristic and the monotonicity property of the unfolding provided by the priority queue a109, the proof for Aa86 optimality (Russell and Norvig, 1995) guarantees that WIDL-NGLM-Aa86 finds a path in a57a113a112 that provides an optimal solution." ></td>
	<td class="line x" title="65:231	1108 WIDL-NGLM-Aa86a30a18a1a42 a31 a37 a62 a37 a65a147a21 1 a88a90a89a53a91a94a93a111a95a108a97a1a0 a43 a35a43a37a44 a37 a43a65a46 a40a46 2 a2 a88a4a3a5a0 a38 3 while a2 a88a4a3 4 do a98a73a99a92a101a106a102a78a103a107a104a6a0 UNFOLDa18a1a42 a31 a37 a88a90a89a53a91a94a93a111a95a108a97a68a21 5 EVALUATEa18 a98a100a99a53a101a71a102a78a103a105a104 a37 a62 a37 a65a124a21 6 if a88a90a89a92a91a94a93a96a95a78a97 a6a135a43 a35a43a48a46 a37 a43a65a46 a40a46 7 then a2 a88a4a3a5a0 a36 8 for each a7 a91 a88a108a91 a97 in a98a73a99a92a101a106a102a78a103a105a104 do PUSHa18 a109 a37 a7 a91 a88a108a91 a97a80a21 a88 a89a92a91a94a93a96a95a78a97a8a0 POPa18 a109 a21 9 return a88a90a89a53a91a94a93a111a95a108a97 Figure 3: Aa86 algorithm for interpolating WIDLexpressions with a0 -gram language models." ></td>
	<td class="line x" title="66:231	An important property of the WIDL-NGLM-Aa86 algorithm is that the UNFOLD relation (and, implicitly, the a57a112a31 acceptor) is computed only partially, for those states for which the total cost is less than the cost of the optimal path." ></td>
	<td class="line x" title="67:231	This results in important savings, both in space and time, over simply running a single-source shortest-path algorithm for directed acyclic graphs (Cormen et al. , 2001) over the full acceptor a57a133a31 (Soricut and Marcu, 2005)." ></td>
	<td class="line x" title="68:231	4 Headline Generation using WIDL-expressions We employ the WIDL formalism (Section 2) and the WIDL-NGLM-Aa86 algorithm (Section 3) in a summarization application that aims at producing both informative and fluent headlines." ></td>
	<td class="line x" title="69:231	Our headlines are generated in an abstractive, bottom-up manner, starting from words and phrases." ></td>
	<td class="line x" title="70:231	A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al. , 2003) and annotated with additional information (Zajic et al. , 2004)." ></td>
	<td class="line x" title="71:231	Automatic Creation of WIDL-expressions for Headline Generation." ></td>
	<td class="line x" title="72:231	We generate WIDLexpressions starting from an input document." ></td>
	<td class="line x" title="73:231	First, we extract a weighted list of topic keywords from the input document using the algorithm of Zhou and Hovy (2003)." ></td>
	<td class="line x" title="74:231	This list is enriched with phrases created from the lexical dependencies the topic keywords have in the input document." ></td>
	<td class="line x" title="75:231	We associate probability distributions with these phrases using their frequency (we assume Keywords a43 iraq 0.32, syria 0.25, rebels 0.22, kurdish 0.17, turkish 0.14, attack 0.10a46 Phrases iraq a43 in iraq 0.4, northern iraq 0.5,iraq and iran 0.1a46, syria a43 into syria 0.6, and syria 0.4 a46 rebels a43 attacked rebels 0.7,rebels fighting 0.3a46 . . ." ></td>
	<td class="line x" title="76:231	a9 WIDL-expression & trigram interpolation TURKISH GOVERNMENT ATTACKED REBELS IN IRAQ AND SYRIA Figure 4: Input and output for our automatic headline generation system." ></td>
	<td class="line x" title="77:231	that higher frequency is indicative of increased importance) and their position in the document (we assume that proximity to the beginning of the document is also indicative of importance)." ></td>
	<td class="line x" title="78:231	In Figure 4, we present an example of input keywords and lexical-dependency phrases automatically extracted from a document describing incidents at the Turkey-Iraq border." ></td>
	<td class="line x" title="79:231	The algorithm for producing WIDLexpressions combines the lexical-dependency phrases for each keyword using a a62 operator with the associated probability values for each phrase multiplied with the probability value of each topic keyword." ></td>
	<td class="line x" title="80:231	It then combines all the a62 -headed expressions into a single WIDL-expression using a a154 operator with uniform probability." ></td>
	<td class="line x" title="81:231	The WIDLexpression in Figure 1 is a (scaled-down) example of the expressions created by this algorithm." ></td>
	<td class="line x" title="82:231	On average, a WIDL-expression created by this algorithm, using a10 a6 a34 keywords and an average of a81a195a6 a234 lexical-dependency phrases per keyword, compactly encodes a candidate set of about 3 million possible realizations." ></td>
	<td class="line x" title="83:231	As the specification of the a154a77a63 operator takes space a11a195a18 a38 a21 for uniform a51, Theorem 1 guarantees that the space complexity of these expressions is a11a183a18a12a10 a81a47a21 . Finally, we generate headlines from WIDLexpressions using the WIDL-NGLM-Aa86 algorithm, which interpolates the probability distributions represented by the WIDL-expressions with a0 -gram language model distributions." ></td>
	<td class="line x" title="84:231	The output presented in Figure 4 is the most likely headline realization produced by our system." ></td>
	<td class="line x" title="85:231	Headline Generation Evaluation." ></td>
	<td class="line x" title="86:231	To evaluate the accuracy of our headline generation system, we use the documents from the DUC 2003 evaluation competition." ></td>
	<td class="line x" title="87:231	Half of these documents are used as development set (283 documents), 1109 ALG a0 (uni) a0 (bi) Len." ></td>
	<td class="line o" title="88:231	Rougea1 Rougea2 Extractive Lead10 458 114 9.9 20.8 11.1 HedgeTrimmera3 399 104 7.4 18.1 9.9 Topiarya4 576 115 9.9 26.2 12.5 Abstractive Keywords 585 22 9.9 26.6 5.5 Webcl 311 76 7.3 14.1 7.5 WIDL-Aa5 562 126 10.0 25.5 12.9 Table 1: Headline generation evaluation." ></td>
	<td class="line x" title="89:231	We compare extractive algorithms against abstractive algorithms, including our WIDL-based algorithm." ></td>
	<td class="line x" title="90:231	and the other half is used as test set (273 documents)." ></td>
	<td class="line oc" title="91:231	We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGEa129 (Lin, 2004)." ></td>
	<td class="line x" title="92:231	For each input document, we train two language models, using the SRI Language Model Toolkit (with modified Kneser-Ney smoothing)." ></td>
	<td class="line x" title="93:231	A general trigram language model, trained on 170M English words from the Wall Street Journal, is used to model fluency." ></td>
	<td class="line x" title="94:231	A document-specific trigram language model, trained on-the-fly for each input document, accounts for both fluency and content validity." ></td>
	<td class="line x" title="95:231	We also employ a word-count model (which counts the number of words in a proposed realization) and a phrase-count model (which counts the number of phrases in a proposed realization), which allow us to learn to produce headlines that have restrictions in the number of words allowed (10, in our case)." ></td>
	<td class="line o" title="96:231	The interpolation weights a65 (Equation 2) are trained using discriminative training (Och, 2003) using ROUGEa129 as the objective function, on the development set." ></td>
	<td class="line x" title="97:231	The results are presented in Table 1." ></td>
	<td class="line x" title="98:231	We compare the performance of several extractive algorithms (which operate on an extracted sentence to arrive at a headline) against several abstractive algorithms (which create headlines starting from scratch)." ></td>
	<td class="line x" title="99:231	For the extractive algorithms, Lead10 is a baseline which simply proposes as headline the lead sentence, cut after the first 10 words." ></td>
	<td class="line x" title="100:231	HedgeTrimmera6 is our implementation of the Hedge Trimer system (Dorr et al. , 2003), and Topiarya7 is our implementation of the Topiary system (Zajic et al. , 2004)." ></td>
	<td class="line x" title="101:231	For the abstractive algorithms, Keywords is a baseline that proposes as headline the sequence of topic keywords, Webcl is the system THREE GORGES PROJECT IN CHINA HAS WON APPROVAL WATER IS LINK BETWEEN CLUSTER OF E. COLI CASES SRI LANKA S JOINT VENTURE TO EXPAND EXPORTS OPPOSITION TO EUROPEAN UNION SINGLE CURRENCY EURO OF INDIA AND BANGLADESH WATER BARRAGE Figure 5: Headlines generated automatically using a WIDL-based sentence realization system." ></td>
	<td class="line x" title="102:231	described in (Zhou and Hovy, 2003), and WIDLAa8 is the algorithm described in this paper." ></td>
	<td class="line x" title="103:231	This evaluation shows that our WIDL-based approach to generation is capable of obtaining headlines that compare favorably, in both content and fluency, with extractive, state-of-the-art results (Zajic et al. , 2004), while it outperforms a previously-proposed abstractive system by a wide margin (Zhou and Hovy, 2003)." ></td>
	<td class="line x" title="104:231	Also note that our evaluation makes these results directly comparable, as they use the same parsing and topic identification algorithms." ></td>
	<td class="line x" title="105:231	In Figure 5, we present a sample of headlines produced by our system, which includes both good and not-so-good outputs." ></td>
	<td class="line x" title="106:231	5 Machine Translation using WIDL-expressions We also employ our WIDL-based realization engine in a machine translation application that uses a two-phase generation approach: in a first phase, WIDL-expressions representing large sets of possible translations are created from input foreignlanguage sentences." ></td>
	<td class="line x" title="107:231	In a second phase, we use our generic, WIDL-based sentence realization engine to intersect WIDL-expressions with an a0 gram language model." ></td>
	<td class="line x" title="108:231	In the experiments reported here, we translate between Chinese (source language) and English (target language)." ></td>
	<td class="line x" title="109:231	Automatic Creation of WIDL-expressions for MT. We generate WIDL-expressions from Chinese strings by exploiting a phrase-based translation table (Koehn et al. , 2003)." ></td>
	<td class="line x" title="110:231	We use an algorithm resembling probabilistic bottom-up parsing to build a WIDL-expression for an input Chinese string: each contiguous span a18a92a75a37a10a9 a21 over a Chinese string a11a22a52a13a12a61 is considered a possible constituent, and the non-terminals associated with each constituent are the English phrase translations a61 a69 a52a13a12a61 that correspond in the translation table to the Chinese string a11a56a52a13a12a61 . Multiple-word English phrases, such as a14a16a15a17a14a19a18a20a14a22a21, are represented as WIDL-expressions using the precedence (a131) and 1110 a0a2a1 a1a4a3a6a5 a1 a2a7a3a9a8a11a10a13a12a13a14a16a15a13a12a18a17a11a19a16a3a21a20a23a22a25a24a27a26a28a8a18a10a13a12a29a14a30a24a25a12a25a31a25a17a32a8a18a10a13a12a29a14a30a24a25a12a25a31a25a17 a5 a1a34a33 a3a25a19a16a3a35a22a2a15a37a36a25a24a38a26a18a39a13a24a34a24a2a12a28a31a25a17a2a39a28a24a37a40 a12a25a8a41a17a42a19a16a3a43a22a28a15a25a44a45a26a18a39a13a24a34a24a25a12a13a31a25a17a23a46a47a24a2a48 a24a29a17a23a46a49a15a25a50a35a31a25a17 a19a51a3a43a39a34a52a51a26a18a53a28a54a18a55a40a56a32a24a25a31a2a17a37a5 a1a35a57 a3a35a58a37a40 a55 a55 a17a28a58a37a40 a55a55a24a32a44a6a17a2a58a37a40 a55 a55a40 a12a25a8a41a31a25a17a13a59a60a31 a61 a1a45a62a64a63 a53a13a24a2a48a21a14a30a50a47a65 a66a68a67 a69a18a70a9a71 a67a9a65a9a72 a73 a74 a59 a75a41a76 a61 a2a77a62a78a63 a74a79a73 a75a6a59 a80a18a81a38a75a6a59 a75a18a80a30a75a82a59 a74a13a83 a75a6a59 a84a18a81a41a17 a61 a33 a62a64a63 a74a77a73 a75a82a59 a74 a80a38a75a6a59 a74a28a85 a75a6a59 a74a28a86 a75a6a59 a75a18a87a82a17 a84 a73 a75a6a59 a80a29a81a30a75a82a59 a74 a75a27a75a6a59 a80a18a84a27a75a6a59 a80a29a88a82a17 a84 a73 a75a6a59 a85 a87a27a75a6a59 a74 a81a30a75a6a59 a80a13a75a30a75a82a59 a85 a88a41a17 a80 a73 a75a6a59 a80a13a75a16a75a82a59 a86 a87a38a75a6a59 a85a11a83 a75a6a59 a80 a83 a76 a80 a73 a75a6a59 a74a28a85 a75a6a59 a74a29a74 a75a6a59 a74a28a86 a75a82a59 a75a11a87a41a17 a61 a57 a62a78a63 a74a79a73 a75a6a59 a88a18a81a38a75a6a59 a80a29a80a30a75a82a59 a84a18a81a38a75a6a59 a88a18a80a41a17 a85a89a73 a75a6a59 a74 a88a27a75a6a59 a84a29a87a30a75a6a59 a84 a85 a75a82a59 a84 a85 a17 a84 a73 a75a6a59 a84a29a80a30a75a82a59 a81a29a75a27a75a6a59 a81a18a80a27a75a6a59 a84a29a88a82a17 a81 a73 a75a6a59 a74 a75a30a75a6a59 a80a29a80a30a75a6a59 a74 a75a30a75a82a59 a74 a88a11a76 a80 a73 a75a6a59 a74 a84a30a75a82a59 a74 a88a38a75a6a59 a84a18a84a27a75a6a59 a74a29a74 a76 a9 WIDL-expression & trigram interpolation gunman was killed by police . Figure 6: A Chinese string is converted into a WIDL-expression, which provides a translation as the best scoring hypothesis under the interpolation with a trigram language model." ></td>
	<td class="line x" title="111:231	lock (a221 ) operators, as a221 a18 a14 a15a134a131 a14 a18a56a131 a14 a21 a21 . To limit the number of possible translations a61 a69 a52a13a12a61 corresponding to a Chinese span a11a56a52a13a12a61, we use a probabilistic beam a90 and a histogram beam a58 to beam out low probability translation alternatives." ></td>
	<td class="line x" title="112:231	At this point, each a11 a52 a12a61 span is tiled with likely translations a61 a69 a52a13a12a61 taken from the translation table." ></td>
	<td class="line x" title="113:231	Tiles that are adjacent are joined together in a larger tile by a a154a77a63 operator, where a51 a6 a43a80a188a80a177a68a178a19a29a133a190 a62a25a91a82a92 a76a149a74a11a93 a92a18a62 a28a33 a38 a46 . That is, reordering of the component tiles are permitted by the a154a53a63 operators (assigned non-zero probability), but the longer the movement from the original order of the tiles, the lower the probability." ></td>
	<td class="line x" title="114:231	(This distortion model is similar with the one used in (Koehn, 2004))." ></td>
	<td class="line x" title="115:231	When multiple tiles are available for the same span a18a92a75a37a10a9 a21, they are joined by a a62a42a63 operator, where a51 is specified by the probability distributions specified in the translation table." ></td>
	<td class="line x" title="116:231	Usually, statistical phrase-based translation tables specify not only one, but multiple distributions that account for context preferences." ></td>
	<td class="line x" title="117:231	In our experiments, we consider four probability distributions: a39 a18 a94a96a95 a66a68a21 a37a40a39 a18 a66a16a95 a94a111a21 a37a40a39a89a97 a46a29a98 a18 a94a96a95 a66a68a21, and a39a99a97 a46a13a98 a18 a66a51a95 a94a124a21, where a94 and a66 are Chinese-English phrase translations as they appear in the translation table." ></td>
	<td class="line x" title="118:231	In Figure 6, we show an example of WIDL-expression created by this algorithm1." ></td>
	<td class="line x" title="119:231	On average, a WIDL-expression created by this algorithm, using an average of a10 a6 a226a48a36 tiles per sentence (for an average input sentence length of 30 words) and an average of a81a183a6a101a100 possible translations per tile, encodes a candidate set of about 10a233 a67 possible translations." ></td>
	<td class="line x" title="120:231	As the specification of the a154a162a63 operators takes space a11a195a18 a38 a21, Theorem 1 1English reference: the gunman was shot dead by the police." ></td>
	<td class="line x" title="121:231	guarantees that these WIDL-expressions encode compactly these huge spaces in a11a183a18a12a10 a81a47a21 . In the second phase, we employ our WIDLbased realization engine to interpolate the distribution probabilities of WIDL-expressions with a trigram language model." ></td>
	<td class="line x" title="122:231	In the notation of Equation 2, we use four feature functions a62a124a67 a37a53a57a53a57a53a57a58a37 a62 a50 for the WIDL-expression distributions (one for each probability distribution encoded); a feature function a62a79a102 for a trigram language model; a feature function a62 a233 for a word-count model, and a feature function a62 a47 for a phrase-count model." ></td>
	<td class="line x" title="123:231	As acknowledged in the Machine Translation literature (Germann et al. , 2003), full Aa86 search is not usually possible, due to the large size of the search spaces." ></td>
	<td class="line x" title="124:231	We therefore use an approximation algorithm, called WIDL-NGLM-Aa86a69, which considers for unfolding only the nodes extracted from the priority queue a109 which already unfolded a path of length greater than or equal to the maximum length already unfolded minus a81 (we used a81a195a6a84a225 in the experiments reported here)." ></td>
	<td class="line x" title="125:231	MT Performance Evaluation." ></td>
	<td class="line x" title="126:231	When evaluated against the state-of-the-art, phrase-based decoder Pharaoh (Koehn, 2004), using the same experimental conditions  translation table trained on the FBIS corpus (7.2M Chinese words and 9.2M English words of parallel text), trigram language model trained on 155M words of English newswire, interpolation weights a65 (Equation 2) trained using discriminative training (Och, 2003) (on the 2002 NIST MT evaluation set), probabilistic beam a90 set to 0.01, histogram beam a58 set to 10  and BLEU (Papineni et al. , 2002) as our metric, the WIDL-NGLM-Aa86 a129 algorithm produces translations that have a BLEU score of 0.2570, while Pharaoh translations have a BLEU score of 0.2635." ></td>
	<td class="line x" title="127:231	The difference is not statistically significant at 95% confidence level." ></td>
	<td class="line x" title="128:231	These results show that the WIDL-based approach to machine translation is powerful enough to achieve translation accuracy comparable with state-of-the-art systems in machine translation." ></td>
	<td class="line x" title="129:231	6 Conclusions The approach to sentence realization we advocate in this paper relies on WIDL-expressions, a formal language with convenient theoretical properties that can accommodate a wide range of generation scenarios." ></td>
	<td class="line x" title="130:231	In the worst case, one can work with simple bags of words that encode no context 1111 preferences (Soricut and Marcu, 2005)." ></td>
	<td class="line x" title="131:231	One can also work with bags of words and phrases that encode context preferences, a scenario that applies to current approaches in statistical machine translation (Section 5)." ></td>
	<td class="line x" title="132:231	And one can also encode context and ordering preferences typically used in summarization (Section 4)." ></td>
	<td class="line x" title="133:231	The generation engine we describe enables a tight coupling of content selection with sentence realization preferences." ></td>
	<td class="line x" title="134:231	Its algorithm comes with theoretical guarantees about its optimality." ></td>
	<td class="line x" title="135:231	Because the requirements for producing WIDLexpressions are minimal, our WIDL-based generation engine can be employed, with state-of-the-art results, in a variety of text-to-text applications." ></td>
	<td class="line x" title="136:231	Acknowledgments This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No." ></td>
	<td class="line x" title="137:231	HR0011-06-C-0022." ></td>
	<td class="line x" title="138:231	References Srinivas Bangalore and Owen Rambow." ></td>
	<td class="line x" title="139:231	2000." ></td>
	<td class="line x" title="140:231	Using TAG, a tree model, and a language model for generation." ></td>
	<td class="line x" title="141:231	In Proceedings of the Fifth International Workshop on Tree-Adjoining Grammars (TAG+)." ></td>
	<td class="line x" title="142:231	Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein." ></td>
	<td class="line x" title="143:231	2001." ></td>
	<td class="line x" title="144:231	Introduction to Algorithms." ></td>
	<td class="line x" title="145:231	The MIT Press and McGraw-Hill." ></td>
	<td class="line x" title="146:231	Simon Corston-Oliver, Michael Gamon, Eric K. Ringger, and Robert Moore." ></td>
	<td class="line x" title="147:231	2002." ></td>
	<td class="line x" title="148:231	An overview of Amalgam: A machine-learned generation module." ></td>
	<td class="line x" title="149:231	In Proceedings of the INLG." ></td>
	<td class="line x" title="150:231	Bonnie Dorr, David Zajic, and Richard Schwartz." ></td>
	<td class="line x" title="151:231	2003." ></td>
	<td class="line x" title="152:231	Hedge trimmer: a parse-and-trim approach to headline generation." ></td>
	<td class="line x" title="153:231	In Proceedings of the HLTNAACL Text Summarization Workshop, pages 18." ></td>
	<td class="line x" title="154:231	Michael Elhadad." ></td>
	<td class="line x" title="155:231	1991." ></td>
	<td class="line x" title="156:231	FUF User manual  version 5.0." ></td>
	<td class="line x" title="157:231	Technical Report CUCS-038-91, Department of Computer Science, Columbia University." ></td>
	<td class="line x" title="158:231	Ulrich Germann, Mike Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada." ></td>
	<td class="line x" title="159:231	2003." ></td>
	<td class="line x" title="160:231	Fast decoding and optimal decoding for machine translation." ></td>
	<td class="line x" title="161:231	Artificial Intelligence, 154(12):127-143." ></td>
	<td class="line x" title="162:231	Nizar Habash." ></td>
	<td class="line x" title="163:231	2003." ></td>
	<td class="line x" title="164:231	Matador: A large-scale SpanishEnglish GHMT system." ></td>
	<td class="line x" title="165:231	In Proceedings of AMTA." ></td>
	<td class="line x" title="166:231	J. Hajic, M. Cmejrek, B. Dorr, Y. Ding, J. Eisner, D. Gildea, T. Koo, K. Parton, G. Penn, D. Radev, and O. Rambow." ></td>
	<td class="line x" title="167:231	2002." ></td>
	<td class="line x" title="168:231	Natural language generation in the context of machine translation." ></td>
	<td class="line x" title="169:231	Summer workshop final report, Johns Hopkins University." ></td>
	<td class="line x" title="170:231	K. Knight and V. Hatzivassiloglou." ></td>
	<td class="line x" title="171:231	1995." ></td>
	<td class="line x" title="172:231	Two level, many-path generation." ></td>
	<td class="line x" title="173:231	In Proceedings of the ACL." ></td>
	<td class="line x" title="174:231	Philipp Koehn, Franz J. Och, and Daniel Marcu." ></td>
	<td class="line x" title="175:231	2003." ></td>
	<td class="line x" title="176:231	Statistical phrase based translation." ></td>
	<td class="line x" title="177:231	In Proceedings of the HLT-NAACL, pages 127133." ></td>
	<td class="line x" title="178:231	Philipp Koehn." ></td>
	<td class="line x" title="179:231	2004." ></td>
	<td class="line x" title="180:231	Pharaoh: a beam search decoder for phrase-based statistical machine transltion models." ></td>
	<td class="line x" title="181:231	In Proceedings of the AMTA, pages 115124." ></td>
	<td class="line x" title="182:231	I. Langkilde-Geary." ></td>
	<td class="line x" title="183:231	2002." ></td>
	<td class="line x" title="184:231	A foundation for generalpurpose natural language generation: sentence realization using probabilistic models of language." ></td>
	<td class="line x" title="185:231	Ph.D. thesis, University of Southern California." ></td>
	<td class="line x" title="186:231	Chin-Yew Lin." ></td>
	<td class="line x" title="187:231	2004." ></td>
	<td class="line x" title="188:231	ROUGE: a package for automatic evaluation of summaries." ></td>
	<td class="line x" title="189:231	In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004)." ></td>
	<td class="line x" title="190:231	Christian Matthiessen and John Bateman." ></td>
	<td class="line x" title="191:231	1991." ></td>
	<td class="line x" title="192:231	Text Generation and Systemic-Functional Linguistic." ></td>
	<td class="line x" title="193:231	Pinter Publishers, London." ></td>
	<td class="line x" title="194:231	Mehryar Mohri, Fernando Pereira, and Michael Riley." ></td>
	<td class="line x" title="195:231	2002." ></td>
	<td class="line x" title="196:231	Weighted finite-state transducers in speech recognition." ></td>
	<td class="line x" title="197:231	Computer Speech and Language, 16(1):6988." ></td>
	<td class="line x" title="198:231	Mark-Jan Nederhof and Giorgio Satta." ></td>
	<td class="line x" title="199:231	2004." ></td>
	<td class="line x" title="200:231	IDLexpressions: a formalism for representing and parsing finite languages in natural language processing." ></td>
	<td class="line x" title="201:231	Journal of Artificial Intelligence Research, pages 287317." ></td>
	<td class="line x" title="202:231	Franz Josef Och." ></td>
	<td class="line x" title="203:231	2003." ></td>
	<td class="line x" title="204:231	Minimum error rate training in statistical machine translation." ></td>
	<td class="line x" title="205:231	In Proceedings of the ACL, pages 160167." ></td>
	<td class="line x" title="206:231	Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu." ></td>
	<td class="line x" title="207:231	2002." ></td>
	<td class="line x" title="208:231	BLEU: a method for automatic evaluation of machine translation." ></td>
	<td class="line x" title="209:231	In In Proceedings of the ACL, pages 311318." ></td>
	<td class="line x" title="210:231	Stuart Russell and Peter Norvig." ></td>
	<td class="line x" title="211:231	1995." ></td>
	<td class="line x" title="212:231	Artificial Intelligence." ></td>
	<td class="line x" title="213:231	A Modern Approach." ></td>
	<td class="line x" title="214:231	Prentice Hall." ></td>
	<td class="line x" title="215:231	Radu Soricut and Daniel Marcu." ></td>
	<td class="line x" title="216:231	2005." ></td>
	<td class="line x" title="217:231	Towards developing generation algorithms for text-to-text applications." ></td>
	<td class="line x" title="218:231	In Proceedings of the ACL, pages 6674." ></td>
	<td class="line x" title="219:231	Radu Soricut." ></td>
	<td class="line x" title="220:231	2006." ></td>
	<td class="line x" title="221:231	Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation." ></td>
	<td class="line x" title="222:231	Ph.D. thesis, University of Southern California." ></td>
	<td class="line x" title="223:231	David Zajic, Bonnie J. Dorr, and Richard Schwartz." ></td>
	<td class="line x" title="224:231	2004." ></td>
	<td class="line x" title="225:231	BBN/UMD at DUC-2004: Topiary." ></td>
	<td class="line x" title="226:231	In Proceedings of the NAACL Workshop on Document Understanding, pages 112119." ></td>
	<td class="line x" title="227:231	Liang Zhou and Eduard Hovy." ></td>
	<td class="line x" title="228:231	2003." ></td>
	<td class="line x" title="229:231	Headline summarization at ISI." ></td>
	<td class="line x" title="230:231	In Proceedings of the NAACL Workshop on Document Understanding." ></td>
	<td class="line x" title="231:231	1112" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2078
An Automatic Method For Summary Evaluation Using Multiple Evaluation Results By A Manual Method
Nanba, Hidetsugu;Okumura, Manabu;"></td>
	<td class="line x" title="1:204	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 603610, Sydney, July 2006." ></td>
	<td class="line x" title="2:204	c2006 Association for Computational Linguistics An Automatic Method for Summary Evaluation Using Multiple Evaluation Results by a Manual Method Hidetsugu Nanba Faculty of Information Sciences, Hiroshima City University 3-4-1 Ozuka, Hiroshima, 731-3194 Japan nanba@its.hiroshima-cu.ac.jp Manabu Okumura Precision and Intelligence Laboratory, Tokyo Institute of Technology 4259 Nagatsuta, Yokohama, 226-8503 Japan oku@pi.titech.ac.jp Abstract To solve a problem" ></td>
	<td class="line x" title="3:204	of how to evaluate computer-produced summaries, a number of automatic" ></td>
	<td class="line x" title="4:204	and manual methods have been proposed." ></td>
	<td class="line x" title="5:204	Manual methods evaluate summaries correctly, because humans evaluate them, but are costly." ></td>
	<td class="line x" title="6:204	On the other hand, automatic methods, which use evaluation tools or programs, are low cost, although these methods cannot evaluate summaries as accurately as manual methods." ></td>
	<td class="line x" title="7:204	In this paper, we investigate an automatic evaluation method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually." ></td>
	<td class="line x" title="8:204	We conducted some experiments using the data of the Text Summarization Challenge 2 (TSC-2)." ></td>
	<td class="line x" title="9:204	A comparison with conventional automatic methods shows that our method outperforms other methods usually used." ></td>
	<td class="line x" title="10:204	1 Introduction Recently, the evaluation of computer-produced summaries has" ></td>
	<td class="line x" title="11:204	become recognized as one of the problem areas that must be addressed in the field of automatic summarization." ></td>
	<td class="line x" title="12:204	To solve this problem, a number of automatic" ></td>
	<td class="line oc" title="13:204	(Donaway et al. , 2000, Hirao et al. , 2005, Lin et al. , 2003, Lin, 2004, Hori et al. , 2003) and manual methods" ></td>
	<td class="line x" title="14:204	(Nenkova et al. , 2004, Teufel et al. , 2004) have been proposed." ></td>
	<td class="line x" title="15:204	Manual methods evaluate summaries correctly, because humans evaluate them, but are costly." ></td>
	<td class="line x" title="16:204	On the other hand, automatic methods, which use evaluation tools or programs, are low cost, although these methods cannot evaluate summaries as accurately as manual methods." ></td>
	<td class="line x" title="17:204	In this paper, we investigate an automatic method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually." ></td>
	<td class="line x" title="18:204	Unlike other automatic methods, our method estimates manual evaluation scores." ></td>
	<td class="line x" title="19:204	Therefore, our method makes it possible to compare a new system with other systems that have been evaluated manually." ></td>
	<td class="line x" title="20:204	There are two research studies related to our work (Kazawa et al. , 2003, Yasuda et al. , 2003)." ></td>
	<td class="line x" title="21:204	Kazawa et al.(2003) proposed an automatic evaluation method using multiple evaluation results from a manual method." ></td>
	<td class="line x" title="23:204	In the field of machine translation, Yasuda et al.(2003) proposed an automatic method that gives an evaluation result of a translation system as a score for the Test of English for International Communication (TOEIC)." ></td>
	<td class="line x" title="25:204	Although the effectiveness of both methods was confirmed experimentally, further discussion of four points, which we describe in Section 3, is necessary for a more accurate summary evaluation." ></td>
	<td class="line x" title="26:204	In this paper, we address three of these points based on Kazawas and Yasudas methods." ></td>
	<td class="line x" title="27:204	We also investigate whether these methods can outperform other automatic methods." ></td>
	<td class="line x" title="28:204	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="29:204	Section 2 describes related work." ></td>
	<td class="line x" title="30:204	Section 3 describes our method." ></td>
	<td class="line x" title="31:204	To investigate the effectiveness of our method, we conducted some examinations and Section 4 reports on these." ></td>
	<td class="line x" title="32:204	We present some conclusions in Section 5." ></td>
	<td class="line x" title="33:204	2 Related Work Generally, similar summaries are considered to obtain similar evaluation results." ></td>
	<td class="line x" title="34:204	If there is a set of summaries (pooled summaries) produced from a document (or multiple documents) and if these are evaluated manually, then we can estimate a manual evaluation score for any summary to be evaluated with the evaluation results for those pooled summaries." ></td>
	<td class="line x" title="35:204	Based on this idea, Kazawa et 603 al." ></td>
	<td class="line x" title="36:204	(2003) proposed an automatic method using multiple evaluation results from a manual method." ></td>
	<td class="line x" title="37:204	First, n summaries for each document, m, were prepared." ></td>
	<td class="line x" title="38:204	A summarization system generated summaries from m documents." ></td>
	<td class="line x" title="39:204	Here, we represent the i th summary for the j th document and its evaluation score as x ij and y ij, respectively." ></td>
	<td class="line x" title="40:204	The system was evaluated using Equation 1." ></td>
	<td class="line x" title="41:204	 == += m i ijij n j j bxxSimywxscr 11 ),()( (1) The evaluation score of summary x was obtained by summing parameter b for all the subscores calculated for each pooled summary, x ij." ></td>
	<td class="line x" title="42:204	A subscore was obtained by multiplying a parameter w j, by the evaluation score y ij, and the similarity between x and x ij . In the field of machine translation, there is another related study." ></td>
	<td class="line x" title="43:204	Yasuda et al.(2003) proposed an automatic method that gives an evaluation result of a translation system as a score for TOEIC." ></td>
	<td class="line x" title="45:204	They prepared 29 human subjects, whose TOEIC scores were from 300s to 800s, and asked them to translate 23 Japanese conversations into English." ></td>
	<td class="line x" title="46:204	They also generated translations using a system for each conversation." ></td>
	<td class="line x" title="47:204	Then, they evaluated both translations using an automatic method, and obtained W H, which indicated the ratio of system translations that were superior to human translations." ></td>
	<td class="line x" title="48:204	Yasuda et al. calculated W H for each subject and plotted the values along with their corresponding TOEIC scores to produce a regression line." ></td>
	<td class="line x" title="49:204	Finally, they defined a point where the regression line crossed W H = 0.5 to provide the TOEIC score for the system." ></td>
	<td class="line x" title="50:204	Though, the effectiveness of Kazawas and Yasudas methods were confirmed experimentally, further discussions of four points, which we describe in the next section, are necessary for a more accurate summary evaluation." ></td>
	<td class="line x" title="51:204	3 Investigation of an Automatic Method using Multiple Manual Evaluation Results 3.1 Overview of Our Evaluation Method and Essential Points to be Discussed We investigate an automatic method using multiple evaluation results by a manual method based on Kazawas and Yasudas method." ></td>
	<td class="line x" title="52:204	The procedure of our evaluation method is shown as follows; (Step 1) Prepare summaries and their evaluation results by a manual method (Step 2) Calculate the similarities between a summary to be evaluated and the pooled summaries (Step 3) Combine manual scores of pooled summaries in proportion to their similarities to the summary to be evaluated For each step, we need to discuss the following points." ></td>
	<td class="line x" title="53:204	(Step 1) 1." ></td>
	<td class="line x" title="54:204	How many summaries, and what type (variety) of summaries should be prepared?" ></td>
	<td class="line x" title="55:204	Kazawa et al. prepared 6 summaries for each document, and Yasuda et al. prepared 29 translations for each conversation." ></td>
	<td class="line x" title="56:204	However, they did not examine about the number and the type of pooled summaries required to the evaluation." ></td>
	<td class="line x" title="57:204	(Step 2) 2." ></td>
	<td class="line x" title="58:204	Which measure is better for calculating the similarities between a summary to be evaluated and the pooled summaries?" ></td>
	<td class="line x" title="59:204	Kazawa et al. used Equation 2 to calculate similarities." ></td>
	<td class="line x" title="60:204	|)||,min(| || ),( xx xx xxSim ij ij ij  = (2) where xx ij  indicates the number of discourse units 1 that appear in both x ij and x, and | x | represents the number of words in x. However, there are many other measures that can be used to calculate the topical similarities between two documents (or passages)." ></td>
	<td class="line x" title="61:204	As well as Yasudas method does, using W H is another way to calculate similarities between a summary to be evaluated and pooled summaries indirectly." ></td>
	<td class="line x" title="62:204	Yasuda et al.(2003) tested DP matching (Su et al. , 1992), BLEU (Papineni et al. , 2002), and NIST 2, for the calculation of W H . However there are many other measures for summary evaluation." ></td>
	<td class="line x" title="64:204	1 Rhetorical Structure Theory Discourse Treebank." ></td>
	<td class="line x" title="65:204	www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalog Id=LDC2002T07 Linguistic Data Consortium." ></td>
	<td class="line x" title="66:204	2 http://www.nist.gov/speech/tests/mt/mt2001/resource/ 604 3." ></td>
	<td class="line x" title="67:204	How many summaries should be used to calculate the score of a summary to be evaluated?" ></td>
	<td class="line x" title="68:204	Kazawa et al. used all the pooled summaries but this does not ensure the best performance of their evaluation method." ></td>
	<td class="line x" title="69:204	(Step 3) 4." ></td>
	<td class="line x" title="70:204	How to combine the manual scores of the pooled summaries?" ></td>
	<td class="line x" title="71:204	Kazawa et al. calculated the score of a summary as a weighted linear sum of the manual scores." ></td>
	<td class="line x" title="72:204	Applying regression analysis (Yasuda et al. , 2003) is another method of combining several manual scores." ></td>
	<td class="line x" title="73:204	3.2 Three Points Addressed in Our Study We address the second, third and fourth points in Section 3.1." ></td>
	<td class="line x" title="74:204	(Point 2) A measure for calculating similarities between a summary to be evaluated and pooled summaries: There are many measures that can calculate the topical similarities between two documents (or passages)." ></td>
	<td class="line oc" title="75:204	We tested several measures, such as ROUGE (Lin, 2004) and the cosine distance." ></td>
	<td class="line x" title="76:204	We describe these measures in detail in Section 4.2." ></td>
	<td class="line x" title="77:204	(Point 3) The number of summaries used to calculate the score of a summary to be evaluated: We use summaries whose similarities to a summary to be evaluated are higher than a threshold value." ></td>
	<td class="line x" title="78:204	(Point 4) Combination of manual scores: We used both Kazawas and Yasudas methods." ></td>
	<td class="line x" title="79:204	4 Experiments 4.1 Experimental Methods To investigate the three points described in Section 3.2, we conducted the following four experiments." ></td>
	<td class="line x" title="80:204	z Exp-1: We examined Points 2 and 3 based on Kazawas method." ></td>
	<td class="line x" title="81:204	We tested threshold values from 0 to 1 at 0.005 intervals." ></td>
	<td class="line x" title="82:204	We also tested several similarity measures, such as cosine distance and 11 kinds of ROUGE." ></td>
	<td class="line x" title="83:204	z Exp-2: In order to investigate whether the evaluation based on Kazawas method can outperform other automatic methods, we compared the evaluation with other automatic methods." ></td>
	<td class="line x" title="84:204	In this experiment, we used the similarity measure, which obtain the best performance in Exp-1." ></td>
	<td class="line x" title="85:204	z Exp-3: We also examined Point 2 based on Yasudas method." ></td>
	<td class="line o" title="86:204	As a similarity measure, we tested cosine distance and 11 kinds of ROUGE." ></td>
	<td class="line x" title="87:204	Then, we examined Point 4 by comparing the result of Yasudas method with that of Kazawas. z Exp-4: In the same way as Exp-2, we compared the evaluation with other automatic methods, which we describe in the next section, to investigate whether the evaluation based on Yasudas method can outperform other automatic methods." ></td>
	<td class="line x" title="88:204	4.2 Automatic Evaluation Methods Used in the Experiments In the following, we show the automatic evaluation methods used in our experiments." ></td>
	<td class="line x" title="89:204	Content-based evaluation (Donaway et al. , 2000) This measure evaluates summaries by comparing their content words with those of the humanproduced extracts." ></td>
	<td class="line x" title="90:204	The score of the contentbased measure is obtained by computing the similarity between the term vector using tf*idf weighting of a computer-produced summary and the term vector of a human-produced summary by cosine distance." ></td>
	<td class="line oc" title="91:204	ROUGE-N (Lin, 2004) This measure compares n-grams of two summaries, and counts the number of matches." ></td>
	<td class="line o" title="92:204	The measure is defined by Equation 3." ></td>
	<td class="line o" title="93:204	     = RSSgram N RSSgram Nmatch N N gramCount gramCount NROUGE )( )( (3) where Count(gram N ) is the number of an N-gram and Count match (gram N ) denotes the number of ngram co-occurrences in two summaries." ></td>
	<td class="line oc" title="94:204	ROUGE-L (Lin, 2004) This measure evaluates summaries by longest common subsequence (LCS) defined by Equation 4." ></td>
	<td class="line o" title="95:204	m CrLCS LROUGE u ii i =  = ),( (4) where LCS U (r i,C) is the LCS score of the unions longest common subsequence between reference sentences r i and the summary to be evaluated, and m is the number of words contained in a reference summary." ></td>
	<td class="line oc" title="96:204	605 ROUGE-S (Lin, 2004) Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps." ></td>
	<td class="line o" title="97:204	ROUGE-S measures the overlap of skip-bigrams in a candidate summary and a reference summary." ></td>
	<td class="line o" title="98:204	Several variations of ROUGE-S are possible by limiting the maximum skip distance between the two in-order words that are allowed to form a skip-bigram." ></td>
	<td class="line oc" title="99:204	In the following, ROUGE-SN denotes ROUGE-S with maximum skip distance N. ROUGE-SU (Lin, 2004) This measure is an extension of ROUGE-S; it adds a unigram as a counting unit." ></td>
	<td class="line o" title="100:204	In the following, ROUGE-SUN denotes ROUGE-SU with maximum skip distance N. 4.3 Evaluation Methods In the following, we elaborate on the evaluation methods for each experiment." ></td>
	<td class="line x" title="101:204	Exp-1: An experiment for Points 2 and 3 based on Kazawas method We evaluated Kazawas method from the viewpoint of Gap." ></td>
	<td class="line x" title="102:204	Differing from other automatic methods, the method uses multiple manual evaluation results and estimates the manual scores of the summaries to be evaluated or the summarization systems." ></td>
	<td class="line x" title="103:204	We therefore evaluated the automatic methods using Gap, which manually indicates the difference between the scores from a manual method and each automatic method that estimates the scores." ></td>
	<td class="line x" title="104:204	First, an arbitrary summary is selected from the 10 summaries in a dataset, which we describe in Section 4.4, and an evaluation score is calculated by Kazawas method using the other nine summaries." ></td>
	<td class="line x" title="105:204	The score is compared with a manual score of the summary by Gap, which is defined by Equation 5." ></td>
	<td class="line x" title="106:204	nm yxscr Gap m k n l klkl   =  ==11 |)('| (5) where x kl is the k th systems l th summary, and y kl is the score from a manual evaluation method for the k th systems l th summary." ></td>
	<td class="line x" title="107:204	To distinguish our evaluation function from Kazawas, we denote it as scr(x)." ></td>
	<td class="line o" title="108:204	As a similarity measure in scr(x), we tested ROUGE and the cosine distance." ></td>
	<td class="line x" title="109:204	We also tested the coverage of the automatic method." ></td>
	<td class="line x" title="110:204	The method cannot calculate scores if there are no similar summaries above a given threshold value." ></td>
	<td class="line x" title="111:204	Therefore, we checked the coverage of the method, which is defined by Equation 6." ></td>
	<td class="line x" title="112:204	summariesgivenofnumberThe methodthebyevaluated summariesofnumberThe Coverage = (6) Exp-2: Comparison of Kazawas method with other automatic methods Traditionally, automatic methods have been evaluated by Ranking." ></td>
	<td class="line x" title="113:204	This means that summarization systems are ranked based on the results of the automatic and manual methods." ></td>
	<td class="line x" title="114:204	Then, the effectiveness of the automatic method is evaluated by the number of matches between both rankings" ></td>
	<td class="line oc" title="115:204	using Spearmans rank correlation coefficient and Pearsons rank correlation coefficient (Lin et al. , 2003, Lin, 2004, Hirao et al. , 2005)." ></td>
	<td class="line x" title="116:204	However, we did not use both correlation coefficients, because evaluation scores are not always calculated by a Kazawabased method, which we described in Exp-1." ></td>
	<td class="line x" title="117:204	Therefore, we ranked the summaries instead of the summarization systems." ></td>
	<td class="line x" title="118:204	Two arbitrary summaries from the 10 summaries in a dataset were selected and ranked by Kazawas method." ></td>
	<td class="line x" title="119:204	Then, Kazawas method was evaluated using Precision, which calculates the percentage of cases where the order of the manual method of the two summaries matches the order of their ranks calculated by Kazawas method." ></td>
	<td class="line o" title="120:204	The two summaries were also ranked by ROUGE and by cosine distance, and both Precision values were calculated." ></td>
	<td class="line o" title="121:204	Finally, the Precision value of Kazawas method was compared with those of ROUGE and cosine distance." ></td>
	<td class="line x" title="122:204	Exp-3: An experiment for Point 2 based on Yasudas method An arbitrary system was selected from the 10 systems, and Yasudas method estimated its manual score from the other nine systems." ></td>
	<td class="line x" title="123:204	Yasudas method was evaluated by Gap, which is defined by Equation 7." ></td>
	<td class="line x" title="124:204	m yxs Gap m k kk =  = 1 |)(| (7) where x k is the k th system, s(x k ) is a score of x k by Yasudas method, and y k is the manual score for the k th system." ></td>
	<td class="line x" title="125:204	Yasuda et al.(2003) tested DP matching (Su et al. , 1992), BLEU (Papineni et al. , 2002), and NIST 3, as automatic methods used in their evaluation." ></td>
	<td class="line o" title="127:204	Instead of those methods, we 3 http://www.nist.gov/speech/tests/mt/mt2001/resource/ 606 tested ROUGE and cosine distance, both of which have been used for summary evaluation." ></td>
	<td class="line x" title="128:204	If a score by Yasudas method exceeds the range of the manual score, the score is modified to be within the range." ></td>
	<td class="line x" title="129:204	In our experiments, we used evaluation by revision (Fukushima et al. , 2002) as the manual evaluation method." ></td>
	<td class="line x" title="130:204	The range of the score of this method is between zero and 0.5." ></td>
	<td class="line x" title="131:204	If the score is less than zero, it is changed to zero and if greater than 0.5 it is changed to 0.5." ></td>
	<td class="line x" title="132:204	Exp-4: Comparison of Yasudas method and other automatic methods In the same way as for the evaluation of Kazawas method in Exp-2, we evaluated Yasudas method by Precision." ></td>
	<td class="line x" title="133:204	Two arbitrary summaries from the 10 summaries in a dataset were selected, and ranked by Yasudas method." ></td>
	<td class="line x" title="134:204	Then, Yasudas method was evaluated using Precision." ></td>
	<td class="line o" title="135:204	Two summaries were also ranked by ROUGE and by cosine distance and both Precision values were calculated." ></td>
	<td class="line o" title="136:204	Finally, the Precision value of Yasudas method was compared with those of ROUGE and cosine distance." ></td>
	<td class="line x" title="137:204	4.4 The Data Used in Our Experiments We used the TSC-2 data (Fukushima" ></td>
	<td class="line x" title="138:204	et al. , 2002) in our examinations." ></td>
	<td class="line x" title="139:204	The data consisted of human-produced extracts (denoted as PART), human-produced abstracts (denoted as FREE), computer-produced summaries (eight systems and a baseline system using the lead method (denoted as LEAD)) 4, and their evaluation results by two manual methods." ></td>
	<td class="line x" title="140:204	All the summaries were derived from 30 newspaper articles, written in Japanese, and were extracted from the Mainichi newspaper database for the years 1998 and 1999." ></td>
	<td class="line x" title="141:204	Two tasks were conducted in TSC-2, and we used the data from a single document summarization task." ></td>
	<td class="line x" title="142:204	In this task, participants were asked to produce summaries in plain text in the ratios of 20% and 40%." ></td>
	<td class="line x" title="143:204	Summaries were evaluated using a ranking evaluation method and the revision method evaluation." ></td>
	<td class="line x" title="144:204	In our experiments, we used the results of evaluation from the revision method." ></td>
	<td class="line x" title="145:204	This method evaluates summaries by measuring the degree to which computer-produced summaries are revised." ></td>
	<td class="line x" title="146:204	The judges read the 4 In Exp-2 and 4, we evaluated PART, LEAD, and eight systems (candidate summaries) by automatic methods using FREE as the reference summaries." ></td>
	<td class="line x" title="147:204	original texts and revised the computer-produced summaries in terms of their content and readability." ></td>
	<td class="line x" title="148:204	The human revisions were made with only three editing operations (insertion, deletion, replacement)." ></td>
	<td class="line x" title="149:204	The degree of the human revision, called the edit distance, is computed from the number of revised characters divided by the number of characters in the original summary." ></td>
	<td class="line x" title="150:204	If the summarys quality was so low that a revision of more than half of the original summary was required, the judges stopped the revision and a score of 0.5 was given." ></td>
	<td class="line x" title="151:204	The effectiveness of evaluation by the revision method was confirmed in our previous work (Nanba et al. , 2004)." ></td>
	<td class="line x" title="152:204	We compared evaluation by revision with ranking evaluation." ></td>
	<td class="line oc" title="153:204	We also tested other automatic methods: content-based evaluation, BLEU (Papineni et al. , 2001) and ROUGE-1 (Lin, 2004), and compared their results with that of evaluation by revision as reference." ></td>
	<td class="line x" title="154:204	As a result, we found that evaluation by revision is effective for recognizing slight differences between computer-produced summaries." ></td>
	<td class="line x" title="155:204	4.5 Experimental Results and Discussion Exp-1: An experiment for Points 2 and 3 based on Kazawas method To address Points 2 and 3, we evaluated summaries by the method based on Kazawas method using 12 measures, described in Section 4.4, as measures to calculate topical similarities between summaries, and compared these measures by Gap." ></td>
	<td class="line x" title="156:204	The experimental results for summarization ratios of 40%" ></td>
	<td class="line x" title="157:204	and 20% are shown in Tables 1 and 2, respectively." ></td>
	<td class="line x" title="158:204	Tables show the Gap values of 12 measures for each Coverage value from 0.2 to 1.0 at 0.1 intervals." ></td>
	<td class="line x" title="159:204	Average values of Gap for each measure are also shown in these tables." ></td>
	<td class="line x" title="160:204	As can be seen from Tables 1" ></td>
	<td class="line x" title="161:204	and 2, the larger the threshold value, the smaller the value of Gap." ></td>
	<td class="line x" title="162:204	From the result, we can conclude for Point 3 that more accurate evaluation is possible when we use similar pooled summaries (Point 2)." ></td>
	<td class="line x" title="163:204	However, the number of summaries that can be evaluated by this method was limited when the threshold value was large." ></td>
	<td class="line p" title="164:204	Of the 12 measures, unigram-based methods, such as cosine distance and ROUGE-1, produced good results." ></td>
	<td class="line o" title="165:204	However, there were no significant differences between measures except for when ROUGE-L was used." ></td>
	<td class="line x" title="166:204	607 Table 1 Comparison of Gap values for several measures (ratio: 40%) Coverage Measure 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 Average R-1 0.080 0.070 0.067 0.057 0.064 0.062 0.058 0.045 0.041 0.062 R-2 0.082 0.074 0.070 0.070 0.069 0.063 0.059 0.051 0.042 0.065 R-3 0.083 0.074 0.075 0.071 0.069 0.063 0.059 0.051 0.045 0.066 R-4 0.085 0.078 0.076 0.073 0.069 0.064 0.060 0.051 0.043 0.067 R-L 0.102 0.100 0.097 0.094 0.091 0.090 0.089 0.082 0.078 0.091 R-S 0.083 0.077 0.073 0.073 0.069 0.067 0.064 0.060 0.045 0.068 R-S4 0.083 0.072 0.071 0.069 0.066 0.066 0.060 0.054 0.044 0.065 R-S9 0.083 0.075 0.069 0.070 0.067 0.066 0.066 0.057 0.046 0.067 R-SU 0.083 0.077 0.070 0.071 0.069 0.068 0.064 0.057 0.043 0.067 R-SU4 0.082 0.073 0.069 0.069 0.065 0.068 0.063 0.051 0.043 0.065 R-SU9 0.083 0.074 0.070 0.068 0.066 0.067 0.066 0.054 0.046 0.066 Cosine 0.081 0.074 0.065 0.062 0.059 0.056 0.057 0.039 0.043 0.059 Threshold Small Large Table 2 Comparison of Gap values for several measures (ratio: 20%) Coverage Measure 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 Average R-1 0.129 0.104 0.102 0.976 0.090 0.089 0.089 0.083 0.082 0.096 R-2 0.132 0.115 0.107 0.109 0.096 0.093 0.079 0.081 0.082 0.099 R-3 0.132 0.115 0.116 0.111 0.102 0.092 0.080 0.078 0.079 0.101 R-4 0.134 0.121 0.121 0.112 0.103 0.090 0.080 0.080 0.078 0.102 R-L 0.140 0.135 0.134 0.125 0.117 0.110 0.105 0.769 0.060 0.111 R-S 0.130 0.119 0.113 0.106 0.098 0.099 0.089 0.089 0.087 0.103 R-S4 0.130 0.114 0.109 0.105 0.102 0.092 0.085 0.088 0.085 0.101 R-S9 0.130 0.119 0.113 0.105 0.095 0.097 0.095 0.085 0.084 0.103 R-SU 0.130 0.118 0.109 0.109 0.097 0.098 0.088 0.089 0.079 0.102 R-SU4 0.130 0.111 0.107 0.106 0.100 0.090 0.086 0.084 0.087 0.100 R-SU9 0.130 0.116 0.108 0.105 0.096 0.090 0.085 0.085 0.082 0.099 Cosine 0.128 0.106 0.102 0.094 0.091 0.090 0.079 0.080 0.057 0.092 Threshold Small Large Exp-2: Comparison of Kazawas method with other automatic methods (Point 2) In Exp-1, cosine distance outperformed the other 11 measures." ></td>
	<td class="line x" title="167:204	We therefore used cosine distance in Kazawas method in Exp-2." ></td>
	<td class="line o" title="168:204	We ranked summaries by Kazawas method, ROUGE and cosine distance, calculated using Precision." ></td>
	<td class="line x" title="169:204	The results of the evaluation by Precision for summarization ratios of 40% and 20% are shown in Figures 1 and 2, respectively." ></td>
	<td class="line x" title="170:204	We plotted the Precision value of Kazawas method by changing the threshold value from 0 to 1 at 0.05 intervals." ></td>
	<td class="line o" title="171:204	We also plotted the Precision values of ROUGE2 as dotted lines." ></td>
	<td class="line p" title="172:204	ROUGE-2 was superior to the other 11 measures in terms of Ranking." ></td>
	<td class="line x" title="173:204	The X and Y axes in Figures 1 and 2 show the threshold value of Kazawas method and the Precision values, respectively." ></td>
	<td class="line n" title="174:204	From the result shown in Figure 1, we found that Kazawas method outperformed ROUGE-2, when the threshold value was greater than 0.968." ></td>
	<td class="line x" title="175:204	The Coverage value of this point was 0.203." ></td>
	<td class="line x" title="176:204	In Figure 2, the Precision curve of Kazawas method crossed the dotted line at a threshold value of 0.890." ></td>
	<td class="line x" title="177:204	The Coverage value of this point was 0.405." ></td>
	<td class="line x" title="178:204	To improve these Coverage values, we need to prepare more summaries and their manual evaluation results, because the Coverage is critically dependent on the number and variety of pooled summaries." ></td>
	<td class="line x" title="179:204	This is exactly the first point in Section 3.1, which we do not address in this paper." ></td>
	<td class="line x" title="180:204	We will investigate this point as the next step in our future work." ></td>
	<td class="line o" title="181:204	608 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 threshold value pr e c i s i o n Kazawa's method R-2 Figure 1 Comparison of Kazawas method and ROUGE-2 (ratio: 40%) 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 threshold value p r e c is io n Kazawa's method R-2 Figure 2 Comparison of Kazawas method and ROUGE-2 (ratio: 20%) Exp-3: An experiment for Point 3 based on Yasudas method For Point 2 in Section 3.2, we also examined Yasudas method." ></td>
	<td class="line x" title="182:204	The experimental result by Gap is shown in Table 3." ></td>
	<td class="line o" title="183:204	When the ratio is 20%, ROUGE-SU4 is the best." ></td>
	<td class="line x" title="184:204	The N-gram and the skip-bigram are both useful when the summarization ratio is low." ></td>
	<td class="line x" title="185:204	For Point 4, we compared the result by Yasudas method (Table 3) with that of Kazawas method (in Tables 1 and 2)." ></td>
	<td class="line x" title="186:204	Yasudas method could accurately estimate manual scores." ></td>
	<td class="line o" title="187:204	In particular, the Gap values of 0.023 by ROUGE-2 and by ROUGE-3 are smaller than those produced by Kazawas method with a threshold value of 0.9 (Tables 1 and 2)." ></td>
	<td class="line x" title="188:204	This indicates that regression analysis used in Yasudas method is superior to that used in Kazawas method." ></td>
	<td class="line x" title="189:204	Table 3 Gap between the manual method and Yasudas method Ratio 20% 40% Average Cosine 0.037 0.031 0.035 R-1 0.033 0.022 0.028 R-2 0.028 0.023 0.025 R-3 0.028 0.023 0.025 R-4 0.036 0.024 0.030 R-L 0.040 0.038 0.039 R-S() 0.051 0.060 0.055 R-S4 0.025 0.040 0.033 R-S9 0.042 0.052 0.047 R-SU() 0.027 0.055 0.041 R-SU4 0.022 0.037 0.029 R-SU9 0.023 0.048 0.036 Exp-4: Comparison of Yasudas method with other automatic methods We also evaluated Yasudas method by comparison with other automatic methods in terms of Ranking." ></td>
	<td class="line p" title="190:204	We evaluated 10 systems by Yasudas method with ROUGE-3, which produced the best results in Exp-3." ></td>
	<td class="line x" title="191:204	We also evaluated the systems by ROUGE and cosine distance, and compared the results." ></td>
	<td class="line x" title="192:204	The results are shown in Table 4." ></td>
	<td class="line x" title="193:204	Table 4 Comparison between Yasudas method and automatic methods Ratio 20% 40% Average Yasuda 0.867 0.844 0.856 Cosine 0.844 0.800 0.822 R-1 0.822 0.778 0.800 R-2 0.844 0.800 0.822 R-3 0.822 0.800 0.811 R-4 0.822 0.844 0.833 R-L 0.822 0.800 0.811 R-S() 0.667 0.689 0.678 R-S4 0.800 0.756 0.778 R-S9 0.733 0.689 0.711 R-SU() 0.711 0.711 0.711 R-SU4 0.800 0.822 0.811 R-SU9 0.756 0.711 0.733 As can be seen from Table 4, Yasudas method produced the best results for the ratios of 20% and 40%." ></td>
	<td class="line n" title="194:204	Of the automatic methods compared, ROUGE-4 was the best." ></td>
	<td class="line o" title="195:204	609 As evaluation scores by Yasudas method were calculated based on ROUGE-3, there were no striking differences between Yasudas method and the others except for the integration process of evaluation scores for each summary." ></td>
	<td class="line x" title="196:204	Yasudas method uses a regression analysis, whereas the other methods average the scores for each summary." ></td>
	<td class="line n" title="197:204	Yasudas method using ROUGE-3 outperformed the original ROUGE-3 for both ratios, 20% and 40%." ></td>
	<td class="line x" title="198:204	5 Conclusions We have investigated an automatic method that uses several evaluation results from a manual method" ></td>
	<td class="line x" title="199:204	based on Kazawas and Yasudas methods." ></td>
	<td class="line x" title="200:204	From the experimental results based on Kazawas method, we found that limiting the number of pooled summaries could produce better results than using all the pooled summaries." ></td>
	<td class="line x" title="201:204	However, the number of summaries that can be evaluated by this method was limited." ></td>
	<td class="line x" title="202:204	To improve the Coverage of Kazawas method, more summaries and their evaluation results are required, because the Coverage is critically dependent on the number and variety of pooled summaries." ></td>
	<td class="line p" title="203:204	We also investigated an automatic method based on Yasudas method and found that the method using ROUGE-2 and -3 could accurately estimate manual scores, and could outperform Kazawas method and the other automatic methods tested." ></td>
	<td class="line p" title="204:204	From these results, we can conclude that the automatic method performed the best when ROUGE-2 or 3 is used as a similarity measure, and a regression analysis is used for combining manual method." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2109
Trimming CFG Parse Trees For Sentence Compression Using Machine Learning Approaches
Unno, Yuya;Ninomiya, Takashi;Miyao, Yusuke;Tsujii, Jun'ichi;"></td>
	<td class="line x" title="1:230	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850857, Sydney, July 2006." ></td>
	<td class="line x" title="2:230	c2006 Association for Computational Linguistics Trimming CFG Parse Trees for Sentence Compression Using Machine Learning Approaches Yuya Unno1 Takashi Ninomiya2 Yusuke Miyao1 Junichi Tsujii134 1Department of Computer Science, University of Tokyo 2Information Technology Center, University of Tokyo 3School of Informatics, University of Manchester 4SORST, JST Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan {unno, yusuke, tsujii}@is.s.u-tokyo.ac.jp ninomi@r.dl.itc.u-tokyo.ac.jp Abstract Sentence compression is a task of creating a short grammatical sentence by removing extraneous words or phrases from an original sentence while preserving its meaning." ></td>
	<td class="line x" title="3:230	Existing methods learn statistics on trimming context-free grammar (CFG) rules." ></td>
	<td class="line x" title="4:230	However, these methods sometimes eliminate the original meaning by incorrectly removing important parts of sentences, because trimming probabilities only depend on parents and daughters non-terminals in applied CFG rules." ></td>
	<td class="line x" title="5:230	We apply a maximum entropy model to the above method." ></td>
	<td class="line x" title="6:230	Our method can easily include various features, for example, other parts of a parse tree or words the sentences contain." ></td>
	<td class="line x" title="7:230	We evaluated the method using manually compressed sentences and human judgments." ></td>
	<td class="line x" title="8:230	We found that our method produced more grammatical and informative compressed sentences than other methods." ></td>
	<td class="line x" title="9:230	1 Introduction In most automatic summarization approaches, text is summarized by extracting sentences from a given document without modifying the sentences themselves." ></td>
	<td class="line x" title="10:230	Although these methods have been signi cantly improved to extract good sentences as summaries, they are not intended to shorten sentences; i.e., the output often has redundant words or phrases." ></td>
	<td class="line x" title="11:230	These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news (Dorr et al. , 2003) or messages for the small screens of mobile devices." ></td>
	<td class="line x" title="12:230	We need to compress sentences to obtain short and useful summaries." ></td>
	<td class="line x" title="13:230	This task is called sentence compression." ></td>
	<td class="line x" title="14:230	While several methods have been proposed for sentence compression (Witbrock and Mittal, 1999; Jing and McKeown, 1999; Vandeghinste and Pan, 2004), this paper focuses on Knight and Marcus noisy-channel model (Knight and Marcu, 2000) and presents an extension of their method." ></td>
	<td class="line x" title="15:230	They developed a probabilistic model for trimming a CFG parse tree of an input sentence." ></td>
	<td class="line x" title="16:230	Their method drops words of input sentences but does not change their order or change the words." ></td>
	<td class="line x" title="17:230	They use a parallel corpus that contains pairs of original and compressed sentences." ></td>
	<td class="line x" title="18:230	The method makes CFG parse trees of both original and compressed sentences and learns trimming probabilities from these pairs." ></td>
	<td class="line x" title="19:230	Although their method is concise and well-de ned, its accuracy is still unsatisfactory." ></td>
	<td class="line x" title="20:230	Their method has two problems." ></td>
	<td class="line x" title="21:230	One is that probabilities are calculated only from the frequencies of applied CFG rules, and other characteristics like whether the phrase includes negative words cannot be introduced." ></td>
	<td class="line x" title="22:230	The other problem is that the parse trees of original and compressed sentences sometimes do not correspond." ></td>
	<td class="line x" title="23:230	To solve the former problem, we apply a maximum entropy model to Knight and Marcus model to introduce machine learning features that are dened not only for CFG rules but also for other characteristics in a parse tree, such as the depth from the root node or words it contains." ></td>
	<td class="line x" title="24:230	To solve the latter problem, we introduce a novel matching method, the bottom-up method, to learn complicated relations of two unmatched trees." ></td>
	<td class="line x" title="25:230	We evaluated each algorithm using the ZiffDavis corpus, which has long and short sentence pairs." ></td>
	<td class="line x" title="26:230	We compared our method with Knight and Marcus method in terms of F-measures, bigram F-measures, BLEU scores and human judgments." ></td>
	<td class="line x" title="27:230	850 2 Background 2.1 The Noisy-Channel Model for Sentence Compression Knight and Marcu proposed a sentence compression method using a noisy-channel model (Knight and Marcu, 2000)." ></td>
	<td class="line x" title="28:230	This model assumes that a long sentence was originally a short one and that the longer sentence was generated because some unnecessary words were added." ></td>
	<td class="line x" title="29:230	Given a long sentence l, it nds a short sentence s that maximizes P(sjl)." ></td>
	<td class="line x" title="30:230	This is equivalent to nding the s that maximizes P(s) P(ljs) in Bayes Rule." ></td>
	<td class="line x" title="31:230	The expression P(s) is the source model, which gives the probability that s is the original short string." ></td>
	<td class="line x" title="32:230	When s is ungrammatical, P(s) becomes small." ></td>
	<td class="line x" title="33:230	The expression P(ljs) is the channel model, which gives the probability that s is expanded to l. When s does not include important words of l, P(ljs) has a low value." ></td>
	<td class="line x" title="34:230	In the Knight and Marcus model, a probabilistic context-free grammar (PCFG) score and a word-bigram score are incorporated as the source model." ></td>
	<td class="line x" title="35:230	To estimate the channel model, Knight and Marcu used the Ziff-Davis parallel corpus, which contains long sentences and corresponding short sentences compressed by humans." ></td>
	<td class="line x" title="36:230	Note that each compressed sentence is a subsequence of the corresponding original sentence." ></td>
	<td class="line x" title="37:230	They rst parse both the original and compressed sentences using a CFG parser to create parse trees." ></td>
	<td class="line x" title="38:230	When two nodes of the original and compressed trees have the same non-terminals, and the daughter nodes of the compressed tree are a subsequence of the original tree, they count the node pair as a joint event." ></td>
	<td class="line x" title="39:230	For example, in Figure 1, the original parse tree contains a rule rl = (B ! D E F), and the compressed parse tree contains rs = (B ! D F)." ></td>
	<td class="line x" title="40:230	They assume that rs was expanded into rl, and count the node pairs as joint events." ></td>
	<td class="line x" title="41:230	The expansion probability of two rules is given by: Pexpand (rljrs) = count(joint(rl,rs))count(r s)." ></td>
	<td class="line x" title="42:230	Finally, new subtrees grow from new daughter nodes in each expanded node." ></td>
	<td class="line x" title="43:230	In Figure 1, (E (G g) (H h)) grows from E. The PCFG scores, Pcfg, of these subtrees are calculated." ></td>
	<td class="line x" title="44:230	Then, each probability is assumed to be independent of the others, and the channel model, P(ljs), is calculated as the product of all expansion probabilities of joint events and PCFG scores of new A B C E FD d g h f c A B C FD d f c G H Figure 1: Examples of original and compressed parse trees." ></td>
	<td class="line x" title="45:230	subtrees: P(ljs) = productdisplay (rl,rs)R Pexpand(rljrs) productdisplay rRprime Pcfg(r), where R is the set of rule pairs, and Rprime is the set of generation rules in new subtrees." ></td>
	<td class="line x" title="46:230	To compress an input sentence, they create a tree with the highest score of all possible trees." ></td>
	<td class="line x" title="47:230	They pack all possible trees in a shared-forest structure (Langkilde, 2000)." ></td>
	<td class="line x" title="48:230	The forest structure is represented by an AND-OR tree, and it contains many tree structures." ></td>
	<td class="line x" title="49:230	The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations." ></td>
	<td class="line x" title="50:230	They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence." ></td>
	<td class="line x" title="51:230	Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data." ></td>
	<td class="line x" title="52:230	However their model also has the same problem." ></td>
	<td class="line x" title="53:230	McDonald (McDonald, 2006) independently proposed a new machine learning approach." ></td>
	<td class="line x" title="54:230	He does not trim input parse trees but uses rich features about syntactic trees and improved performance." ></td>
	<td class="line x" title="55:230	2.2 Maximum Entropy Model The maximum entropy model (Berger et al. , 1996) estimates a probability distribution from training data." ></td>
	<td class="line x" title="56:230	The model creates the most uniform distribution within the constraints given by users." ></td>
	<td class="line x" title="57:230	The distribution with the maximum entropy is considered the most uniform." ></td>
	<td class="line x" title="58:230	Given two nite sets of event variables, X and Y, we estimate their joint probability distribution, P(x,y)." ></td>
	<td class="line x" title="59:230	An output, y (2 Y), is produced, and 851 contextual information, x (2 X), is observed." ></td>
	<td class="line x" title="60:230	To represent whether the event (x,y) satis es a certain feature, we introduce a feature function." ></td>
	<td class="line x" title="61:230	A feature function fi returns 1 iff the event (x,y) satis es the feature i and returns 0 otherwise." ></td>
	<td class="line x" title="62:230	Given training data f(x1,y1),, (xn,yn)g, we assume that the expectation of fi on the distribution of the model conforms to that on the empirical probability distribution P(x,y)." ></td>
	<td class="line x" title="63:230	We select the probability distribution that satis es these constraints of all feature functions and maximizes its entropy, H(P) = summationtextx,y P(x,y) log (P(x,y))." ></td>
	<td class="line x" title="64:230	3 Methods 3.1 Maximum Entropy Model for Sentence Compression We describe a maximum entropy method as a natural extension of Knight and Marcus noisychannel model (Knight and Marcu, 2000)." ></td>
	<td class="line x" title="65:230	Knight and Marcus method uses only mother and daughter local relations in CFG parse trees." ></td>
	<td class="line x" title="66:230	Therefore, it sometimes eliminates the meanings of the original sentences." ></td>
	<td class="line x" title="67:230	For example, their method cannot distinguish never and always because these two adverbs are assigned the same non-terminals in parse trees." ></td>
	<td class="line x" title="68:230	However, if never is removed from a sentence, the meaning of the sentence completely changes." ></td>
	<td class="line x" title="69:230	Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcus algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem." ></td>
	<td class="line x" title="70:230	We use other information as feature functions of the maximum entropy model, and this model can deal with many features more appropriately than using simple frequency." ></td>
	<td class="line x" title="71:230	Suppose that we trim a node in the original full parse tree." ></td>
	<td class="line x" title="72:230	For example, suppose we have a mother node A and daughter nodes (B C D) that are derived using a CFG rule." ></td>
	<td class="line x" title="73:230	We must leave at least one non-terminal in the daughter nodes." ></td>
	<td class="line x" title="74:230	The trim candidates of this rule are the members of the set of subsequences, Y, of (B C D), or the seven nonterminal sequences below: Y = fB,C,D,BC,BD,CD,BCDg." ></td>
	<td class="line x" title="75:230	For each y (2 Y), such as (B C), the trimming probability, P(yjY) = Ptrim(A ! B CjA ! B C D), is calculated by using the maximum entropy model." ></td>
	<td class="line x" title="76:230	We assume that these joint events are independent of each other and calculate the probability that an original sentence, l, is compressed to Description 1 the mother node 2 the current node 3 the daughter node sequence in the original sentence and which daughters are removed 4 the daughter node sequence in the compressed sentence 5 the number of daughter nodes 6 the depth from the root 7 the daughter non-terminals that are removed 8 the daughter terminals that are removed 9 whether the daughters are negative adverbs, and removed 10 tri-gram of daughter nodes 11 only one daughter exists, and its non-terminal is the same as that of the current node 12 only one daughter exists, and its non-terminal is the same as that of the mother node 13 how many daughter nodes are removed 14 the number of terminals the current node contains 15 whether the head daughter is removed 16 the left-most and the right-most daughters 17 the left and the right siblings Table 1: Features for maximum entropy model." ></td>
	<td class="line x" title="77:230	s as the product of all trimming probabilities, like in Knight and Marcus method." ></td>
	<td class="line x" title="78:230	P(sjl) = productdisplay (rs,rl)R Ptrim(rsjrl), where R is the set of compressed and original rule pairs in joint events." ></td>
	<td class="line x" title="79:230	Note that our model does not use Bayes Rule or any language models." ></td>
	<td class="line x" title="80:230	For example, in Figure 1, the trimming probability is calculated as below: P(sjl) = Ptrim(A ! B CjA ! B C) Ptrim(B ! D FjB ! D E F)." ></td>
	<td class="line x" title="81:230	To represent all summary candidates, we create a compression forest as Knight and Marcu did." ></td>
	<td class="line x" title="82:230	We select the tree assigned the highest probability from the forest." ></td>
	<td class="line x" title="83:230	Features in the maximum entropy model are dened for a tree node and its surroundings." ></td>
	<td class="line x" title="84:230	When we process one node, or one non-terminal x, we call it the current node." ></td>
	<td class="line x" title="85:230	We focus on not only x and its daughter nodes, but its mother node, its sibling nodes, terminals of its subtree and so on." ></td>
	<td class="line x" title="86:230	The features we used are listed in Table 1." ></td>
	<td class="line x" title="87:230	Knight and Marcu divided the log probabilities by the length of the summary." ></td>
	<td class="line x" title="88:230	We extend this idea so that we can change the output length exibly." ></td>
	<td class="line x" title="89:230	We introduce a length parameter, , and de ne a score S as S(s) = length(s) log P(sjl), where l is an input sentence to be shortened, and s is a 852 summary candidate." ></td>
	<td class="line x" title="90:230	Because log P(sjl) is negative, short sentences obtain a high score for large , and long ones get a low score." ></td>
	<td class="line x" title="91:230	The parameter  can be negative or positive, and we can use it to control the average length of outputs." ></td>
	<td class="line x" title="92:230	3.2 Bottom-Up Method As explained in Section 2.1, in Knight and Marcus method, both original and compressed sentences are parsed, and correspondences of CFG rules are identi ed." ></td>
	<td class="line x" title="93:230	However, when the daughter nodes of a compressed rule are not a subsequence of the daughter nodes in the original one, the method cannot learn this joint event." ></td>
	<td class="line x" title="94:230	A complex sentence is a typical example." ></td>
	<td class="line x" title="95:230	A complex sentence is a sentence that includes another sentence as a part." ></td>
	<td class="line x" title="96:230	An example of a parse tree of a complex sentence and its compressed version is shown in Figure 2." ></td>
	<td class="line x" title="97:230	When we extract joint events from these two trees, we cannot match the two root nodes because the sequence of the daughter nodes of the root node of the compressed parse tree, (NP ADVP VP .), is not a subsequence of the daughter nodes of the original parse tree, (S, NP VP .)." ></td>
	<td class="line x" title="98:230	Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label." ></td>
	<td class="line x" title="99:230	However, there are several types of such problems like Figure 2." ></td>
	<td class="line x" title="100:230	We need to extract these structures from a training corpus." ></td>
	<td class="line x" title="101:230	We propose a bottom-up method to solve the problem explained above." ></td>
	<td class="line x" title="102:230	In our method, only original sentences are parsed, and the parse trees of compressed sentences are extracted from the original parse trees." ></td>
	<td class="line x" title="103:230	An example of this method is shown in Figure 3." ></td>
	<td class="line x" title="104:230	The original sentence is d g h f c, and its compressed sentence is d g c." ></td>
	<td class="line x" title="105:230	First, each terminal in the parse tree of the original sentence is marked if it exists in the compressed sentence." ></td>
	<td class="line x" title="106:230	In the gure, the marked terminals are represented by circles." ></td>
	<td class="line x" title="107:230	Second, each non-terminal in the original parse tree is marked if it has at least one marked terminal in its sub-trees." ></td>
	<td class="line x" title="108:230	These are represented as bold boxes in the gure." ></td>
	<td class="line x" title="109:230	If nonterminals contain marked non-terminals in their sub-trees, these non-terminals are also marked recursively." ></td>
	<td class="line x" title="110:230	These marked non-terminals and terminals compose a tree structure like that on the righthand side in the gure." ></td>
	<td class="line x" title="111:230	These non-terminals represent joint events at each node." ></td>
	<td class="line x" title="112:230	S S,, NP VP I said . . S . . NP VPADVP I never think soNP VPADVP I never think so top top Figure 2: Example of parse tree pair that cannot be matched." ></td>
	<td class="line x" title="113:230	A B C E FD G H h f A B C ED d g c d g c G Figure 3: Example of bottom-up method." ></td>
	<td class="line x" title="114:230	Note that this tree is not guaranteed to be a grammatical parse tree by the CFG grammar. For example, from the tree of Figure 2, (S (S ) (,, ) (NP I) (VP said) (." ></td>
	<td class="line x" title="115:230	.)), a new tree, (S (S ) (." ></td>
	<td class="line x" title="116:230	.)), is extracted." ></td>
	<td class="line x" title="117:230	However, the rule (S ! S )." ></td>
	<td class="line x" title="118:230	is ungrammatical." ></td>
	<td class="line x" title="119:230	4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F-measures, bigram F-measures, and BLEU scores (Papineni et al. , 2002)." ></td>
	<td class="line x" title="120:230	BLEU scores are usually used for evaluating machine translation quality." ></td>
	<td class="line x" title="121:230	A BLEU score is de ned as the weighted geometric average of n-gram precisions with length penalties." ></td>
	<td class="line x" title="122:230	We used from unigram to 4-gram precisions and uniform weights for the BLEU scores." ></td>
	<td class="line oc" title="123:230	ROUGE (Lin, 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks." ></td>
	<td class="line o" title="124:230	ROUGE-N uses average N-gram recall, and ROUGE-1 is word recall." ></td>
	<td class="line o" title="125:230	ROUGE-L uses the length of the longest common subsequence (LCS) of the original and summarized sentences." ></td>
	<td class="line o" title="126:230	In our model, the length of the LCS is equal to the number of common words, and ROUGE-L is equal to the unigram F-measure because words are not rearranged." ></td>
	<td class="line oc" title="127:230	ROUGE-L and ROUGE-1 are supposed to be appropriate for the headline gener853 ation task (Lin, 2004)." ></td>
	<td class="line x" title="128:230	This is not our task, but it is the most similar task in his paper." ></td>
	<td class="line x" title="129:230	We also evaluated the methods using human judgments." ></td>
	<td class="line x" title="130:230	The evaluator is not the author but not a native English speaker." ></td>
	<td class="line x" title="131:230	The judgment used the same criteria as those in Knight and Marcus methods." ></td>
	<td class="line x" title="132:230	We performed two experiments." ></td>
	<td class="line x" title="133:230	In the rst experiment, evaluators scored from 1 to 5 points the grammaticality of the compressed sentence." ></td>
	<td class="line x" title="134:230	In the second one, they scored from 1 to 5 points how well the compressed sentence contained the important words of the original one." ></td>
	<td class="line x" title="135:230	We used the parallel corpus used in Ref." ></td>
	<td class="line x" title="136:230	(Knight and Marcu, 2000)." ></td>
	<td class="line x" title="137:230	This corpus consists of sentence pairs extracted automatically from the ZiffDavis corpus, a set of newspaper articles about computer products." ></td>
	<td class="line x" title="138:230	This corpus has 1087 sentence pairs." ></td>
	<td class="line x" title="139:230	Thirty-two of these sentences were used for the human judgments in Knight and Marcus experiment, and the same sentences were used for our human judgments." ></td>
	<td class="line x" title="140:230	The rest of the sentences were randomly shuf ed, and 527 sentence pairs were used as a training corpus, 263 pairs as a development corpus, and 264 pairs as a test corpus." ></td>
	<td class="line x" title="141:230	To parse these corpora, we used Charniak and Johnsons parser (Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="142:230	4.2 Settings of Two Experiments We experimented with/without goal sentence length for summaries." ></td>
	<td class="line x" title="143:230	In the rst experiment, the system was given only a sentence and no sentence length information." ></td>
	<td class="line x" title="144:230	The sentence compression problem without the length information is a general task, but evaluating it is dif cult because the correct length of a summary is not generally de ned even by humans." ></td>
	<td class="line x" title="145:230	The following example shows this." ></td>
	<td class="line x" title="146:230	Original: A font, on the other hand, is a subcategory of a typeface, such as Helvetica Bold or Helvetica Medium." ></td>
	<td class="line x" title="147:230	Human: A font is a subcategory of a typeface, such as Helvetica Bold." ></td>
	<td class="line x" title="148:230	System: A font is a subcategory of a typeface." ></td>
	<td class="line x" title="149:230	The such as phrase is removed in this system output, but it is not removed in the human summary." ></td>
	<td class="line x" title="150:230	Neither result is wrong, but in such situations, the evaluation score of the system decreases." ></td>
	<td class="line x" title="151:230	This is because the compression rate of each algorithm is different, and evaluation scores are affected by the lengths of system outputs." ></td>
	<td class="line x" title="152:230	For this reason, results with different lengths cannot be 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 F-measure Compression ratio Noisy-channel ME ME + bottom-up Figure 4: F-measures and compression ratios." ></td>
	<td class="line x" title="153:230	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Bigram F-measure Compression ratio Noisy-channel ME ME + bottom-up Figure 5: Bigram F-measures and compression ratios." ></td>
	<td class="line x" title="154:230	compared easily." ></td>
	<td class="line x" title="155:230	We therefore examined the relations between the average compression ratios and evaluation scores for all methods by changing the system summary length with the different length parameter  introduced in Section 3.1." ></td>
	<td class="line x" title="156:230	In the second experiment, the system was given a sentence and the length for the compressed sentence." ></td>
	<td class="line x" title="157:230	We compressed each input sentence to the length of the sentence in its goal summary." ></td>
	<td class="line x" title="158:230	This sentence compression problem is easier than that in which the system can generate sentences of any length." ></td>
	<td class="line x" title="159:230	We selected the highest-scored sentence from the sentences of length l. Note that the recalls, precisions and F-measures have the same scores in this setting." ></td>
	<td class="line x" title="160:230	4.3 Results of Experiments The results of the experiment without the sentence length information are shown in Figure 4, 5 and 6." ></td>
	<td class="line x" title="161:230	Noisy-channel indicates the results of the noisy-channel model, ME indicates the results of the maximum-entropy method, and ME + bottomup indicates the results of the maximum-entropy 854 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 BLEU score Compression ratio Noisy-channel ME ME + bottom-up Figure 6: BLEU scores and compression ratios." ></td>
	<td class="line x" title="162:230	0.4 0.5 0.6 0.7 0.8 0.9 Noisy-channel ME ME+bottom-up F-measure bigram-F-measure BLEU Figure 7: Results of experiments with length information." ></td>
	<td class="line x" title="163:230	method with the bottom-up method." ></td>
	<td class="line x" title="164:230	We used the length parameter, , introduced in Section 3.1, and obtained a set of summaries with different average lengths." ></td>
	<td class="line x" title="165:230	We plotted the compression ratios and three scores in the gures." ></td>
	<td class="line x" title="166:230	In these gures, a compression ratio is the ratio of the total number of words in compressed sentences to the total number of words in the original sentences." ></td>
	<td class="line x" title="167:230	In these gures, our maximum entropy methods obtained higher scores than the noisy-channel model at all compression ratios." ></td>
	<td class="line x" title="168:230	The maximum entropy method with the bottom-up method obtain the highest scores on these three measures." ></td>
	<td class="line x" title="169:230	The results of the experiment with the sentence length information are shown in Figure 7." ></td>
	<td class="line x" title="170:230	In this experiment, the scores of the maximum entropy methods were higher than the scores of the noisychannel model." ></td>
	<td class="line x" title="171:230	The maximum entropy method with the bottom-up method achieved the highest scores on each measure." ></td>
	<td class="line x" title="172:230	The results of the human judgments are shown in Table 2." ></td>
	<td class="line x" title="173:230	In this experiment, each length of output is same as the length of goal sentence." ></td>
	<td class="line x" title="174:230	The Method Grammar Importance Human 4.94 4.31 Noisy-channel 3.81 3.38 ME 3.88 3.38 ME + bottom-up 4.22 4.06 Table 2: Results of human judgments." ></td>
	<td class="line x" title="175:230	maximum entropy with the bottom-up method obtained the highest scores of the three methods." ></td>
	<td class="line x" title="176:230	We did t-tests (5% signi cance)." ></td>
	<td class="line x" title="177:230	Between the noisychannel model and the maximum entropy with the bottom-up method, importance is signi cantly different but grammaticality is not." ></td>
	<td class="line x" title="178:230	Between the human and the maximum entropy with the bottomup method, grammaticality is signi cantly different but importance is not." ></td>
	<td class="line x" title="179:230	There are no signi cant differences between the noisy-channel model and the maximum entropy model." ></td>
	<td class="line x" title="180:230	4.3.1 Problem of Negative Adverbs One problem of the noisy-channel model is that it cannot distinguish the meanings of removed words." ></td>
	<td class="line x" title="181:230	That is, it sometimes removes semantically important words, such as not and never, because the expansion probability depends only on non-terminals of parent and daughter nodes." ></td>
	<td class="line x" title="182:230	For example, our test corpus includes 15 sentences that contain not . The noisy-channel model removed six not s, and the meanings of the sentences were reversed." ></td>
	<td class="line x" title="183:230	However, the two maximum entropy methods removed only one not because they have negative adverb as a feature in their models." ></td>
	<td class="line x" title="184:230	The rst example in Table 3 shows one of these sentences." ></td>
	<td class="line x" title="185:230	In this example, only Noisy-channel removed not . 4.3.2 Effect of Bottom-Up Method Our bottom-up method achieved the highest accuracy, in terms of F-measures, bigram Fmeasures, BLEU scores and human judgments." ></td>
	<td class="line x" title="186:230	The results were fairly good, especially when it summarized complex sentences, which have sentences as parts." ></td>
	<td class="line x" title="187:230	The second example in Table 3 is a typical complex sentence." ></td>
	<td class="line x" title="188:230	In this example, only ME + bottom-up correctly remove he said . Most of the complex sentences were correctly compressed by the bottom-up method, but a few sentences like the third example in Table 3 were not." ></td>
	<td class="line x" title="189:230	In this example, the original sentence was parsed as shown in Figure 8 (left)." ></td>
	<td class="line x" title="190:230	If this sentence is compressed to the human output, its parse tree has to be like that in Figure 8 (middle) using 855 Original a file or application  alias  similar in effect to the ms-dos path statement provides a visible icon in folders where an aliased application does not actually reside . Human a file or application alias provides a visible icon in folders where an aliased application does not actually reside . Noisychannel a similar in effect to ms-dos statement provides a visible icon in folders where an aliased application does reside . ME a or application alias statement provides a visible icon in folders where an aliased application does not actually reside . ME + bottom-up a file or application statement provides a visible icon in folders where an aliased application does not actually reside . Original the user can then abort the transmission, he said . Human the user can then abort the transmission . Noisychannel the user can abort the transmission said . ME the user can abort the transmission said . ME + bottom-up the user can then abort the transmission . Original it is likely that both companies will work on integrating multimedia with database technologies . Human both companies will work on integrating multimedia with database technologies . Noisychannel it is likely that both companies will work on integrating . ME it is likely that both companies will work on integrating . ME + bottom-up it is will work on integrating multimedia with database technologies . Table 3: Examples of compressed sentences." ></td>
	<td class="line x" title="191:230	our method." ></td>
	<td class="line x" title="192:230	When a parse tree is too long from the root to the leaves like this, some nodes are trimmed but others are not because we assume that each trimming probability is independent." ></td>
	<td class="line x" title="193:230	The compressed sentence is ungrammatical, as in the third example in Table 3." ></td>
	<td class="line x" title="194:230	We have to constrain such ungrammatical sentences or introduce another rule that reconstructs a short tree as in Figure 8 (right)." ></td>
	<td class="line x" title="195:230	That is, we introduce a new transformation rule that compresses (A1 (B (C (A2 )))) to (A2 )." ></td>
	<td class="line x" title="196:230	4.4 Comparison with Original Results We compared our results with Knight and Marcus original results." ></td>
	<td class="line x" title="197:230	They implemented two methods: one is the noisy-channel model and the other is a decision-based model." ></td>
	<td class="line x" title="198:230	Each model produced 32 compressed sentences, and we calculated Fmeasures, bigram F-measures, and BLEU scores." ></td>
	<td class="line x" title="199:230	We used the length parameter  = 0.5 for the maximum-entropy method and  = 0.25 for S VP is ADJP SBAR likely that S both companies will  S It both companies will  S VP SBAR S both companies will  (left) (middle) (right) Figure 8: Parse trees of complicated complex sentences." ></td>
	<td class="line x" title="200:230	Method Comp." ></td>
	<td class="line x" title="201:230	F-measure bigram Fmeasure BLEU Noisychannel 70.19% 68.80 55.96 44.54 Decisionbased 57.26% 71.25 61.93 58.21 ME 66.51% 73.10 62.86 53.51 ME + bottom-up 58.14% 78.58 70.30 65.26 Human 53.59% Table 4: Comparison with original results." ></td>
	<td class="line x" title="202:230	the maximum-entropy method with the bottom-up method." ></td>
	<td class="line x" title="203:230	These two values were determined using experiments on the development set, which did not contain the 32 test sentences." ></td>
	<td class="line x" title="204:230	The results are shown in Table 4." ></td>
	<td class="line x" title="205:230	Noisy-channel indicates the results of Knight and Marcus noisychannel model, and Decision-based indicates the results of Knight and Marcus decision-based model." ></td>
	<td class="line x" title="206:230	Comp." ></td>
	<td class="line x" title="207:230	indicates the compression ratio of each result." ></td>
	<td class="line x" title="208:230	Our two methods achieved higher accuracy than the noisy-channel model." ></td>
	<td class="line x" title="209:230	The results of the decision-based model and our maximumentropy method were not signi cantly different." ></td>
	<td class="line x" title="210:230	Our maximum-entropy method with the bottomup method achieved the highest accuracy." ></td>
	<td class="line x" title="211:230	4.5 Corpus Size and Output Accuracy In general, using more training data improves the accuracy of outputs and using less data results in low accuracy." ></td>
	<td class="line x" title="212:230	Our experiment has the problem that the training corpus was small." ></td>
	<td class="line x" title="213:230	To study the relation between training corpus size and accuracy, we experimented using different training corpus sizes and compared accuracy of the output." ></td>
	<td class="line x" title="214:230	Figure 9 shows the relations between training corpus size and three scores, F-measures, bigram F-measures and BLEU scores, when we used the maximum entropy method with the bottom-up method." ></td>
	<td class="line x" title="215:230	This graph suggests that the accuracy in856 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0 100 200 300 400 500 600 700 800 Score Size of training corpus BLEU score F-measure bigram F-measure Figure 9: Relation between training corpus size and evaluation score." ></td>
	<td class="line x" title="216:230	creases when the corpus size is increased." ></td>
	<td class="line x" title="217:230	Over about 600 sentences, the increase becomes slower." ></td>
	<td class="line x" title="218:230	The graph shows that the training corpus was large enough for this study." ></td>
	<td class="line x" title="219:230	However, if we introduced other speci c features, such as lexical features, a larger corpus would be required." ></td>
	<td class="line x" title="220:230	5 Conclusion We presented a maximum entropy model to extend the sentence compression methods described by Knight and Marcu (Knight and Marcu, 2000)." ></td>
	<td class="line x" title="221:230	Our proposals are two-fold." ></td>
	<td class="line x" title="222:230	First, our maximum entropy model allows us to incorporate various characteristics, such as a mother node or the depth from a root node, into a probabilistic model for determining which part of an input sentence is removed." ></td>
	<td class="line x" title="223:230	Second, our bottom-up method of matching original and compressed parse trees can match tree structures that cannot be matched using Knight and Marcus method." ></td>
	<td class="line x" title="224:230	The experimental results show that our maximum entropy method improved the accuracy of sentence compression as determined by three evaluation criteria: F-measures, bigram F-measures and BLEU scores." ></td>
	<td class="line x" title="225:230	Using our bottom-up method further improved accuracy and produced short summaries that could not be produced by previous methods." ></td>
	<td class="line x" title="226:230	However, we need to modify this model to appropriately process more complicated sentences because some sentences were not correctly summarized." ></td>
	<td class="line x" title="227:230	Human judgments showed that the maximum entropy model with the bottomup method provided more grammatical and more informative summaries than other methods." ></td>
	<td class="line x" title="228:230	Though our training corpus was small, our experiments demonstrated that the data was suf cient." ></td>
	<td class="line x" title="229:230	To improve our approaches, we can introduce more feature functions, especially more semantic or lexical features, and to deal with these features, we need a larger corpus." ></td>
	<td class="line x" title="230:230	Acknowledgements We would like to thank Prof. Kevin Knight and Prof. Daniel Marcu for providing their parallel corpus and the experimental results." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0707
DUC 2005: Evaluation Of Question-Focused Summarization Systems
Dang, Hoa Trang;"></td>
	<td class="line x" title="1:158	Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 4855, Sydney, July 2006." ></td>
	<td class="line x" title="2:158	c2006 Association for Computational Linguistics DUC 2005: Evaluation of Question-Focused Summarization Systems Hoa Trang Dang Information Access Division National Institute of Standards and Technology 100 Bureau Drive Gaithersburg, MD, 20899 hoa.dang@nist.gov Abstract The Document Understanding Conference (DUC) 2005 evaluation had a single useroriented, question-focused summarization task, which was to synthesize from a set of 25-50 documents a well-organized, fluent answer to a complex question." ></td>
	<td class="line x" title="3:158	The evaluation shows that the best summarization systems have difficulty extracting relevant sentences in response to complex questions (as opposed to representative sentences that might be appropriate to a generic summary)." ></td>
	<td class="line x" title="4:158	The relatively generous allowance of 250 words for each answer also reveals how difficult it is for current summarization systems to produce fluent text from multiple documents." ></td>
	<td class="line x" title="5:158	1 Introduction The Document Understanding Conference (DUC) is a series of evaluations of automatic text summarization systems." ></td>
	<td class="line x" title="6:158	It is organized by the National Institute of Standards of Technology with the goals of furthering progress in automatic summarization and enabling researchers to participate in large-scale experiments." ></td>
	<td class="line x" title="7:158	In DUC 2001-2004 a growing number of research groups participated in the evaluation of generic and focused summaries of English newspaper and newswire data." ></td>
	<td class="line x" title="8:158	Various target sizes were used (10-400 words) and both singledocument summaries and summaries of multiple documents were evaluated (around 10 documents per set)." ></td>
	<td class="line x" title="9:158	Summaries were manually judged for both content and readability." ></td>
	<td class="line x" title="10:158	To evaluate content, each peer (human or automatic) summary was compared against a single model (human) summary using SEE (http://www.isi.edu/ cyl/SEE/) to estimate the percentage of information in the model that was covered in the peer." ></td>
	<td class="line oc" title="11:158	Additionally, automatic evaluation of content coverage using ROUGE (Lin, 2004) was explored in 2004." ></td>
	<td class="line x" title="12:158	Human summaries vary in both writing style and content." ></td>
	<td class="line x" title="13:158	For example, (Harman and Over, 2004) noted that a human summary can vary in its level of granularity, whether the summary has a very high-level analysis or primarily contains details." ></td>
	<td class="line x" title="14:158	They analyzed the effects of human variaion in the DUC evaluations and concluded that despite large variation in model summaries, the rankings of the systems when compared against a single model for each document set remained stable when averaged over a large number of document sets and human assessors." ></td>
	<td class="line x" title="15:158	The use of a large test set to smooth over natural human variation is not a new technique; it is the approach that has been taken in TREC (Text Retrieval Conference) for many years (Voorhees and Buckley, 2002)." ></td>
	<td class="line x" title="16:158	While evaluators can achieve stable overall system rankings by averaging scores over a large number of document sets, system builders are still faced with the challenge of producing a summary for a given document set that is most likely to satisfy any human user (since they cannot know ahead of time which human will be using or judging the summary)." ></td>
	<td class="line x" title="17:158	Thus, system developers desire an evaluation methodology that takes into account human variation in summaries for any given document set." ></td>
	<td class="line x" title="18:158	DUC 2005 marked a major change in direction from previous years." ></td>
	<td class="line x" title="19:158	The road mapping committee had strongly recommended that new tasks be undertaken that were strongly tied to a clear user application." ></td>
	<td class="line x" title="20:158	At the same time, the program committee wanted to work on new evaluation methodologies and metrics that would take into 48 account variation of content in human-authored summaries." ></td>
	<td class="line x" title="21:158	Therefore, DUC 2005 had a single user-oriented system task that allowed the community to put some time and effort into helping with a new evaluation framework." ></td>
	<td class="line x" title="22:158	The system task modeled realworld complex question answering (Amigo et al. , 2004)." ></td>
	<td class="line x" title="23:158	Systems were to synthesize from a set of 25-50 documents a brief, well-organized, fluent answer to a need for information that could not be met by just stating a name, date, quantity, etc. Summaries were evaluated for both content and readability." ></td>
	<td class="line x" title="24:158	The task design attempted to constrain two parameters that could produce summaries with widely different content: focus and granularity." ></td>
	<td class="line x" title="25:158	Having a question to focus the summary was intended to improve agreement in content between the model summaries." ></td>
	<td class="line x" title="26:158	Additionally, the assessor who developed each topic specified the desired granularity (level of generalization) of the summary." ></td>
	<td class="line x" title="27:158	Granularity was a way to express one type of user preference; one user might want a general background or overview summary, while another user might want specific details that would allow him to answer questions about specific events or situations." ></td>
	<td class="line x" title="28:158	Because it is both impossible and unnatural to eliminate all human variation, our assessors created as many manual summaries as feasible for each topic, to provide examples of the range of normal human variability in the summarization task." ></td>
	<td class="line x" title="29:158	These multiple models would provide more representative training data to system developers, while enabling additional experiments to investigate the effect of human variability on the evaluation of summarization systems." ></td>
	<td class="line x" title="30:158	As in past DUCs, assessors manually evaluated each summary for readability using a set of linguistic quality questions." ></td>
	<td class="line x" title="31:158	Summary content was manually evaluated using the pseudoextrinsic measure of responsiveness, which does not attempt pairwise comparison of peers against a model summary but gives a coarse ranking of all the summaries based on responsiveness of the summary to the topic." ></td>
	<td class="line x" title="32:158	In parallel, ISI and Columbia University led the summarization research community in two exploratory efforts at intrinsic evaluation of summary content; these evaluations compared peer summaries against multiple reference summaries, using Basic Elements at ISI and Pyramids at Columbia University." ></td>
	<td class="line x" title="33:158	This paper describes the DUC 2005 task and the results of our evaluations of summary content and readability." ></td>
	<td class="line x" title="34:158	(Hovy et al. , 2005) and (Passonneau et al. , 2005) provide additional details and results of the evaluations of summary content using Basic Elements and Pyramids." ></td>
	<td class="line x" title="35:158	2 Task Description The DUC 2005 task was a complex questionfocused summarization task that required summarizers to piece together information from multiple documents to answer a question or set of questions as posed in a topic." ></td>
	<td class="line x" title="36:158	Assessors developed a total of 50 topics to be used as test data." ></td>
	<td class="line x" title="37:158	For each topic, the assessor selected 25-50 related documents from the Los Angeles Times and Financial Times of London and formulated a topic statement, which was a request for information that could be answered using the selected documents." ></td>
	<td class="line x" title="38:158	The topic statement could be in the form of a question or set of related questions and could include background information that the assessor thought would help clarify his/her information need." ></td>
	<td class="line x" title="39:158	The assessor also indicated the granularity of the desired response for each topic." ></td>
	<td class="line x" title="40:158	That is, they indicated whether they wanted the answer to their question(s) to name specific events, people, places, etc. , or whether they wanted a general, high-level answer." ></td>
	<td class="line x" title="41:158	Only one value of granularity was given for each topic, since the goal was not to measure the effect of different granularities on system performance for a given topic, but to provide additional information about the users preferences to both human and automatic summarizers." ></td>
	<td class="line x" title="42:158	An example DUC topic follows: num: D345 title: American Tobacco Companies Overseas narr: In the early 1990s, American tobacco companies tried to expand their business overseas." ></td>
	<td class="line x" title="43:158	What did these companies do or try to do and where?" ></td>
	<td class="line x" title="44:158	How did their parent companies fare?" ></td>
	<td class="line x" title="45:158	granularity: specific The summarization task was the same for both human and automatic summarizers: Given a DUC topic with granularity specification and a set of documents relevant to the topic, the summarization task was to create from the documents a brief, 49 well-organized, fluent summary that answers the need for information expressed in the topic, at the specified level of granularity." ></td>
	<td class="line x" title="46:158	The summary could be no longer than 250 words (whitespacedelimited tokens)." ></td>
	<td class="line x" title="47:158	Summaries over the size limit were truncated, and no bonus was given for creating a shorter summary." ></td>
	<td class="line x" title="48:158	No specific formatting other than linear was allowed." ></td>
	<td class="line x" title="49:158	The summary should include (in some form or other) all the information in the documents that contributed to meeting the information need." ></td>
	<td class="line x" title="50:158	Ten assessors produced a total of 9 human summaries for each of 20 topics, and 4 human summaries for each of the remaining 30 topics." ></td>
	<td class="line x" title="51:158	The summarization task was a relatively difficult task, requiring about 5 hours to manually create each summary." ></td>
	<td class="line x" title="52:158	Thus, there would be a real benefit to users if the task could be performed automatically." ></td>
	<td class="line x" title="53:158	3 Participants There was much interest in the longer, questionfocused summaries required in the DUC 2005 task." ></td>
	<td class="line x" title="54:158	31 participants submitted runs to the evaluation; they are identified by numeric Run IDs (2-32) in the remainder of this paper." ></td>
	<td class="line x" title="55:158	We also developed a simple baseline system that returned the first 250 words of the most recent document for each topic (Run ID = 1)." ></td>
	<td class="line x" title="56:158	In addition to the automatic peers, there were 10 human peers, assigned alphabetic Run IDs, A-J." ></td>
	<td class="line x" title="57:158	Most system developers treated the summarization task as a passage retrieval task." ></td>
	<td class="line x" title="58:158	Sentences were ranked according to relevance to the topic." ></td>
	<td class="line x" title="59:158	The most relevant sentences were then selected for inclusion in the summary while minimizing redundancy within the summary, up to the maximum 250-word allowance." ></td>
	<td class="line x" title="60:158	A significant minority of systems first decomposed the topic narrative into a set of simpler questions, and then extracted sentences to answer each subquestion." ></td>
	<td class="line x" title="61:158	Systems differed in the approach taken to compute relevance and redundancy, using similarity metrics ranging from simple term frequency to semantic graph matching." ></td>
	<td class="line x" title="62:158	In order to include more relevant information in the summary, systems attempted withinsentence compression by removing phrases such as parentheticals and relative clauses." ></td>
	<td class="line x" title="63:158	Many systems simply ignored the granularity specification." ></td>
	<td class="line x" title="64:158	The systems that addressed granularity did so by preferring to extract sentences that contained proper names for topics with a specific granularity but not for topics with general granularity." ></td>
	<td class="line x" title="65:158	Cross-sentence dependencies had to be handled, including anaphora." ></td>
	<td class="line x" title="66:158	Strategies for dealing with pronouns that occurred in relevant sentences included co-reference resolution, including the previous sentence for additional context, or simply excluding all sentences containing any pronouns." ></td>
	<td class="line x" title="67:158	Most systems made no attempt to reword the extracted sentences to improve the readability of the final summary." ></td>
	<td class="line x" title="68:158	Although some systems grouped related sentences together to improve cohesion, the most common heuristic to improve readability was simply to order the extracted sentences by document date and position in the document." ></td>
	<td class="line x" title="69:158	System 12 achieved high readability scores by choosing a single representative document and extracting sentences in the order of appearance in that document." ></td>
	<td class="line x" title="70:158	This approach is similar to the baseline summarizer and produces summaries that are more fluent than those constructed from multiple documents." ></td>
	<td class="line x" title="71:158	4 Evaluation Results Summaries were manually evaluated by 10 assessors." ></td>
	<td class="line x" title="72:158	All summaries for a given topic were judged by a single assessor (who was usually the same as the topic developer)." ></td>
	<td class="line x" title="73:158	In all cases, the assessor was one of the summarizers for the topic." ></td>
	<td class="line x" title="74:158	All summaries for the topic (including the one written by the assessor) were anonymously presented to the assessor, in a random order, and the ssessor judged each summary for readability and responsiveness to the topic, giving separate scores for responsiveness and each of 5 linguistic qualities." ></td>
	<td class="line x" title="75:158	This allowed participants who could not work on optimizing all 6 manual scores, to focus on only the elements that they were interested in or had the resources to address." ></td>
	<td class="line x" title="76:158	No single score was reported that reflected a combination of readability and content." ></td>
	<td class="line x" title="77:158	In previous years, responsiveness considered both the content and readability of the summary." ></td>
	<td class="line x" title="78:158	While it tracked SEE coverage, responsiveness could not be seen as a direct measure of content due to possible effects of readability on the score." ></td>
	<td class="line x" title="79:158	Because we needed an inexpensive manual measure of coverage, we revised the definition of responsiveness in 2005 so that it considered only the information content and not the readability of the summary, to the extent possible." ></td>
	<td class="line x" title="80:158	50 4.1 Evaluation of Readability The readability of the summaries was assessed using five linguistic quality questions which measured qualities of the summary that do not involve comparison with a reference summary or DUC topic." ></td>
	<td class="line x" title="81:158	The linguistic qualities measured were Grammaticality, Non-redundancy, Referential clarity, Focus, and Structure and coherence." ></td>
	<td class="line x" title="82:158	Q1: Grammaticality The summary should have no datelines, system-internal formatting, capitalization errors or obviously ungrammatical sentences (e.g. , fragments, missing components) that make the text difficult to read." ></td>
	<td class="line x" title="83:158	Q2: Non-redundancy There should be no unnecessary repetition in the summary." ></td>
	<td class="line x" title="84:158	Unnecessary repetition might take the form of whole sentences that are repeated, or repeated facts, or the repeated use of a noun or noun phrase (e.g. , Bill Clinton) when a pronoun (he) would suffice." ></td>
	<td class="line x" title="85:158	Q3: Referential clarity It should be easy to identify who or what the pronouns and noun phrases in the summary are referring to." ></td>
	<td class="line x" title="86:158	If a person or other entity is mentioned, it should be clear what their role in the story is. So, a reference would be unclear if an entity is referenced but its identity or relation to the story remains unclear." ></td>
	<td class="line x" title="87:158	Q4: Focus The summary should have a focus; sentences should only contain information that is related to the rest of the summary." ></td>
	<td class="line x" title="88:158	Q5: Structure and Coherence The summary should be well-structured and well-organized." ></td>
	<td class="line x" title="89:158	The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic." ></td>
	<td class="line x" title="90:158	Each linguistic quality question was assessed on a five-point scale: 1." ></td>
	<td class="line x" title="91:158	Very Poor 2." ></td>
	<td class="line x" title="92:158	Poor 3." ></td>
	<td class="line x" title="93:158	Barely Acceptable 4." ></td>
	<td class="line x" title="94:158	Good 5." ></td>
	<td class="line x" title="95:158	Very Good Table 1 shows the distribution of the scores across all the summaries, broken down by the type of summarizer (Human, Baseline, or Participants)." ></td>
	<td class="line x" title="96:158	All summarizers generally performed well on the first two linguistic qualities." ></td>
	<td class="line x" title="97:158	The high scores on non-redundancy show that most participants have Humans Q1 Frequency 1 2 3 4 5 0 50 100 150 200 250 Baseline Q1 Frequency 1 2 3 4 5 0 5 10 15 20 Participants Q1 Frequency 1 2 3 4 5 0 100 300 500 Q1: Grammaticality Humans Q2 Frequency 1 2 3 4 5 0 50 100 150 200 250 Baseline Q2 Frequency 1 2 3 4 5 0 10 20 30 40 Participants Q2 Frequency 1 2 3 4 5 0 200 400 600 800 1000 Q2: Non-redundancy Humans Q3 Frequency 1 2 3 4 5 0 50 100 150 200 250 300 Baseline Q3 Frequency 1 2 3 4 5 0 10 20 30 Participants Q3 Frequency 1 2 3 4 5 0 100 200 300 400 Q3: Referential Clarity Humans Q4 Frequency 1 2 3 4 5 0 50 100 150 200 250 Baseline Q4 Frequency 1 2 3 4 5 0 10 20 30 Participants Q4 Frequency 1 2 3 4 5 0 100 200 300 400 500 Q4: Focus Humans Q5 Frequency 1 2 3 4 5 0 50 100 150 200 250 Baseline Q5 Frequency 1 2 3 4 5 0 5 10 15 20 Participants Q5 Frequency 1 2 3 4 5 0 100 300 500 Q5: Structure and Coherence Table 1: Frequency of scores for each linguistic quality, broken down by source of summary (Humans, Baseline, Participants)." ></td>
	<td class="line x" title="98:158	51 successfully achieved this capability." ></td>
	<td class="line x" title="99:158	Humans and the baseline system also scored well on the last 3 linguistic qualities." ></td>
	<td class="line x" title="100:158	The multi-document summarization systems submitted by participants, on the other hand, still struggle with referential clarity and focus, and perform very poorly on structure and coherence." ></td>
	<td class="line x" title="101:158	4.1.1 Comparison by system For each linguistic quality question, we performed a multiple comparison test between the scores of all peers using Tukeys honestly significant difference criterion." ></td>
	<td class="line x" title="102:158	A multiple comparison test between all human and automatic peers was performed using the Kruskall-Wallis test, to see how the individual automatic peers performed relative to human peers." ></td>
	<td class="line x" title="103:158	For grammaticality, the best human summarizer is significantly better than 28 of the 32 systems; the worst human summarizer is better than 8 systems." ></td>
	<td class="line x" title="104:158	For non-redundancy, the two best humans are significantly better than 6 systems, and the two worst humans are not significantly different from any system." ></td>
	<td class="line x" title="105:158	For referential clarity, all humans are significantly better than all but 2 automatic peers (baseline and System 12)." ></td>
	<td class="line x" title="106:158	For focus, the best human is significantly better than all automatic peers except the baseline; all other humans are significantly better than all automatic peers except the baseline and System 12." ></td>
	<td class="line x" title="107:158	For structure and coherence, the two best humans are significantly better than 31 systems (all automatic peers except the baseline); all humans are better than 30 of the automatic peers (all automatic peers except baseline and System 12)." ></td>
	<td class="line x" title="108:158	4.2 Evaluation of Content We performed manual pseudo-extrinsic evaluation of peer summaries in the form of assessment of responsiveness." ></td>
	<td class="line x" title="109:158	Responsiveness is different from SEE coverage in that it does not compare a peer summary against a single reference; however, responsiveness tracked SEE coverage in DUC 2003 and 2004, and was used to provide a coarsegrained measure of content in 2005." ></td>
	<td class="line o" title="110:158	We also computed ROUGE scores as was done in DUC 2004." ></td>
	<td class="line x" title="111:158	4.2.1 Responsiveness Assessors assigned a raw responsiveness score to each summary." ></td>
	<td class="line x" title="112:158	The score provides a coarse ranking of the summaries for each topic, according to the amount of information in the summary that helps to satisfy the information need expressed in the topic statement, at the level of granularity requested in the user profile." ></td>
	<td class="line x" title="113:158	The score was an integer between 1 and 5, with 1 being least responsive and 5 being most responsive." ></td>
	<td class="line x" title="114:158	For a given topic, some summary was required to receive each of the five possible scores, but no distribution was specified for how many summaries had to receive each score." ></td>
	<td class="line x" title="115:158	The number of human summaries scored per topic also varied." ></td>
	<td class="line x" title="116:158	Therefore, raw responsiveness scores should not be directly added and compared across topics." ></td>
	<td class="line x" title="117:158	Assigning responsiveness scores can be seen as a clustering task in which peers are partitioned into exactly 5 clusters, where members of a cluster are more similar to each other in quality." ></td>
	<td class="line x" title="118:158	RunID 10 A 5 A 4 A B 15 A B C 29 A B C D 11 A B C D 17 A B C D 8 A B C D 7 A B C D E 14 A B C D E 6 A B C D E 28 A B C D E F 21 A B C D E F 19 A B C D E F 24 A B C D E F 9 A B C D E F 16 A B C D E F 32 A B C D E F 12 A B C D E F 25 A B C D E F 18 A B C D E F 27 A B C D E F 20 A B C D E F 3 A B C D E F 2 B C D E F 13 C D E F 30 D E F 22 E F 1 E F 26 F 31 F G 23 G Table 2: Multiple comparison of systems based on Friedmans test on responsiveness For each topic, we computed the scaled responsiveness score for each summary, such that the sum of the scaled responsiveness score is proportional to the number of summaries for the topic." ></td>
	<td class="line x" title="119:158	The scaled responsiveness is the rank of the summary based on the raw responsiveness score." ></td>
	<td class="line x" title="120:158	We computed the average scaled responsiveness score of each summarizer across all topics." ></td>
	<td class="line x" title="121:158	Since the 52 number of human summaries varied across topics, we also computed the average scaled responsiveness score of only the automatic summaries (ignoring the human summaries in scaling responsiveness)." ></td>
	<td class="line x" title="122:158	Table 2 shows the results of a multiple comparison of scaled responsiveness of the automatic peers using Tukeys honestly significant criterion and Friedmans test, with the best peers on top; peers not sharing a common letter are significantly different at the 95.5% confidence level." ></td>
	<td class="line x" title="123:158	None of the automatic peers performed significantly better than the majority of the remaining peers, and only eight of the automatic peers performed significantly better than the simple baseline." ></td>
	<td class="line x" title="124:158	In multiple comparison of all peers using the Kruskal-Wallis test, all human peers were significantly better than all the automatic peers." ></td>
	<td class="line o" title="125:158	4.2.2 ROUGE We computed two ROUGE scores: ROUGE-2 and ROUGE-SU4 recall, both with stemming and implementing jackknifing for each [peer, topic] pair so that human and automatic peers could be compared." ></td>
	<td class="line o" title="126:158	Since the number of ROUGE evaluations per topic varied depending on the number of reference summaries, we computed a macroaverage of each score for each peer, where the macro-average score is the mean over all topics of the mean per-topic score for the peer." ></td>
	<td class="line p" title="127:158	Unlike responsiveness and linguistic quality scores, which are ordinal data and are best suited for non-parametric analyses, ROUGE scores, can be measured on an interval scale and are suitable for parametric analysis." ></td>
	<td class="line o" title="128:158	Analysis of variance showed significant effects from peer and topic (p = 0 for each factor) for both ROUGE-2 and ROUGE-SU4 recall." ></td>
	<td class="line o" title="129:158	To see which peers were different, a multiple comparison of population marginal means (PMM) was performed for each type of ROUGE score." ></td>
	<td class="line x" title="130:158	The population marginal means remove any effect of an unbalanced design (since not all human peers created summaries for all topics) by fixing the values of the peer factor, and averaging out the effects of the topic factor as if each factor combination occurred the same number of times." ></td>
	<td class="line x" title="131:158	Table 3 shows multiple comparison of all peers based on ANOVA of ROUGE-2 recall (ROUGESU4 shows similar results)." ></td>
	<td class="line o" title="132:158	ROUGE-2 and ROUGE-SU4 both distinguish human peers from automatic ones." ></td>
	<td class="line o" title="133:158	The difference in the ROUGE-2 5 10 15 20 25 30 35 10 15 20 25 30 35 Average scaled responsiveness (primary) Average scaled responsiveness (secondary) Figure 1: Primary vs. secondary average scaled responsiveness score of the best system and worst human is not considered significant (possibly due to the very conservative nature of the multiple comparison test) but is still relatively large." ></td>
	<td class="line o" title="134:158	On the other hand, ANOVA of ROUGE-2 found more significant differences between the automatic peers than did Friedmans test of responsiveness." ></td>
	<td class="line x" title="135:158	4.3 Correlation A metric must produce stable rankings of systems in the face of human variation." ></td>
	<td class="line o" title="136:158	Intrinsic measures like ROUGE rely on multiple model summaries to take into account human variation (although Pyramids add another level of human variation in the manual pyramid and peer annotation)." ></td>
	<td class="line x" title="137:158	For a metric like responsiveness, which does not depend on comparison of peer summaries against a model or set of model summaries, it is appropriate to consider the stability of the measure across different assessors." ></td>
	<td class="line x" title="138:158	A secondary assessment was done on responsiveness for the 20 topics that had 9 summaries each." ></td>
	<td class="line x" title="139:158	The secondary assessor had written a summary for the topic but was generally not the same person who developed the topic." ></td>
	<td class="line x" title="140:158	As seen in Figure 1, average scaled responsiveness scores from the two sets of assessments (averaged over the 20 topics) track each other very well." ></td>
	<td class="line x" title="141:158	The human summaries are clustered on the upper right side of the graph, while the automatic summaries form a second cluster on the lower left side." ></td>
	<td class="line x" title="142:158	The actual responsiveness scores for each system and each topic do vary between assessors, but this variation in human judgment is smoothed out by averaging over multiple topics." ></td>
	<td class="line o" title="143:158	Table 4 shows that the correlation between the primary and sec53 RunID PMM of R2 C 0.1172 A A 0.1156 A B I 0.1023 A B C B 0.1014 A B C J 0.1012 A B C E 0.1009 A B C D 0.0986 A B C G 0.0970 B C F 0.0947 C H 0.0897 C D 15 0.0725 D E 17 0.0717 E 10 0.0698 E F 8 0.0696 E F 4 0.0686 E F G 5 0.0675 E F G 11 0.0643 E F G H 14 0.0635 E F G H I 16 0.0633 E F G H I 19 0.0632 E F G H I 7 0.0628 E F G H I J 9 0.0625 E F G H I J 29 0.0609 E F G H I J K 25 0.0609 E F G H I J K 6 0.0609 E F G H I J K 24 0.0597 E F G H I J K 28 0.0594 E F G H I J K 3 0.0594 E F G H I J K 21 0.0573 E F G H I J K 12 0.0563 F G H I J K 18 0.0553 F G H I J K L 26 0.0547 F G H I J K L 27 0.0546 F G H I J K L 32 0.0534 G H I J K L 20 0.0515 H I J K L 13 0.0497 H I J K L 30 0.0496 H I J K L 31 0.0487 I J K L 2 0.0478 J K L 22 0.0462 K L 1 0.0403 L M 23 0.0256 M Table 3: Multiple comparison of all peers based on ANOVA of ROUGE-2 recall 54 Spearman Pearson All peers 0.900 0.976 [0.960, 1.000] Auto peers 0.775 0.822 [0.695, 1.000] Table 4: Correlation between primary and secondary average scaled responsiveness (20 topics), with 95% confidence intervals for Pearsons r. ondary average scaled responsiveness scores is respectable despite the low number of topics." ></td>
	<td class="line x" title="144:158	The correlation suggests that responsiveness would give a stable ranking of the systems when averaged over the entire set of 50 topics." ></td>
	<td class="line o" title="145:158	Table 5 shows that there is high correlation between macro-average ROUGE scores (intrinsic measures) and average scaled responsiveness (a pseudo-extrinisic measure)." ></td>
	<td class="line x" title="146:158	The correlation is high even when the human summaries are ignored." ></td>
	<td class="line o" title="147:158	Metric Spearman Pearson ROUGE-2 (all) 0.951 0.972 [0.953, 1.000] ROUGE-SU4 (all) 0.942 0.958 [0.930, 1.000] ROUGE-2 (auto) 0.901 0.928 [0.872, 1.000] ROUGE-SU4 (auto) 0.872 0.919 [0.855, 1.000] Table 5: Correlation between average scaled responsiveness and macro-average ROUGE recall over all topics and either all peers or only automatic peers." ></td>
	<td class="line x" title="148:158	5 Conclusion The DUC 2005 task was to summarize the answer to a complex question, as found in a set of documents." ></td>
	<td class="line x" title="149:158	The evaluation showed that only the top systems are able to extract sentences whose information content is more responsive to the question than a simple baseline." ></td>
	<td class="line x" title="150:158	Additionally, systems require much additional work to produce coherent, well-structured text, which is apparent in the longer summary sizes of DUC 2005." ></td>
	<td class="line x" title="151:158	On the other hand, systems do well on non-redundancy, since text summarization has historically been formulated as a text compression task." ></td>
	<td class="line x" title="152:158	Since DUC 2005 is the first time question-focused summarization has been evaluated on a large-scale, we have repeated the task in 2006, with some modifications." ></td>
	<td class="line x" title="153:158	We eliminated the granularity specification in DUC 2006." ></td>
	<td class="line x" title="154:158	Assessors had appreciated the theory behind the granularity specification, but found that the size limit for the summaries was a much bigger factor in determining what information to include; some specific summaries ended up being very general given the large amount of information and limited space allowed." ></td>
	<td class="line x" title="155:158	From a human perspective, the actual granularity of the resulting summary mostly fell out naturally from the topic question and the content that was available in the source documents." ></td>
	<td class="line x" title="156:158	The definition of responsiveness scores was meant to yield a coarse ranking of the peer summaries into 5 ordered clusters." ></td>
	<td class="line x" title="157:158	However, assessors found it difficult to form these 5 clusters because of the large number (36+) of summaries that needed to be compared with one another, and the impression that many sets of human and automatic summaries could not be separated into as many as 5 groups." ></td>
	<td class="line x" title="158:158	We therefore changed the scoring of responsiveness in 2006 so that it is based on the same scale as the linguistic quality questions; this may reduce the discriminative power of the responsiveness measure but should produce scores that more accurately reflect the true differences between summaries." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1401
Lessons Learned From Large Scale Evaluation Of Systems That Produce Text: Nightmares And Pleasant Surprises
McKeown, Kathleen R.;"></td>
	<td class="line x" title="1:60	Proceedings of the Fourth International Natural Language Generation Conference, pages 35, Sydney, July 2006." ></td>
	<td class="line x" title="2:60	c2006 Association for Computational Linguistics Lessons Learned from Large Scale Evaluation of Systems that Produce Text: Nightmares and Pleasant Surprises Kathleen R. McKeown Department of Computer Science Columbia University New York, NY 10027 kathy@cs.columbia.edu Extended Abstract As the language generation community explores the possibility of an evaluation program for language generation, it behooves us to examine our experience in evaluation of other systems that produce text as output." ></td>
	<td class="line x" title="3:60	Large scale evaluation of summarization systems and of question answering systems has been carried out for several years now." ></td>
	<td class="line x" title="4:60	Summarization and question answering systems produce text output given text as input, while language generation produces text from a semantic representation." ></td>
	<td class="line x" title="5:60	Given that the output has the same properties, we can learn from the mistakes and the understandings gained in earlier evaluations." ></td>
	<td class="line x" title="6:60	In this invited talk, I will discuss what we have learned in the large scale summarization evaluations carried out in the Document Understanding Conferences (DUC) from 2001 to present, and in the large scale question answering evaluations carried out in TREC (e.g. , the definition pilot) as well as the new large scale evaluations being carried out in the DARPA GALE (Global Autonomous Language Environment) program." ></td>
	<td class="line x" title="7:60	DUC was developed and run by NIST and provides a forum for regular evaluation of summarization systems." ></td>
	<td class="line x" title="8:60	NIST oversees the gathering of data, including both input documents and gold standard summaries, some of which is done by NIST and some of which is done by LDC." ></td>
	<td class="line x" title="9:60	Each year, some 30 to 50 document sets were gathered as test data and somewhere between two to nine summaries were written for each of the input sets." ></td>
	<td class="line x" title="10:60	NIST has carried out both manual and automatic evaluation by comparing system output against the gold standard summaries written by humans." ></td>
	<td class="line x" title="11:60	The results are made public at the annual conference." ></td>
	<td class="line x" title="12:60	In the most recent years, the number of participants has grown to 25 or 30 sites from all over the world." ></td>
	<td class="line x" title="13:60	TREC is also run by NIST and provides an annual opportunity for evaluating the output of question-answering (QA) systems." ></td>
	<td class="line x" title="14:60	Of the various QA evaluations, the one that is probably most illuminating for language generation is the definition pilot." ></td>
	<td class="line x" title="15:60	In this evaluation, systems generated long answers (e.g. , paragraph length or lists of facts) in response to a request for a definition." ></td>
	<td class="line x" title="16:60	In contrast to DUC, no model answers were developed." ></td>
	<td class="line x" title="17:60	Instead, system output was pooled and human judges determined which facts within the output were necessary (termed vital nuggets) and which were helpful, but not absolutely necessary (termed OK nuggets)." ></td>
	<td class="line x" title="18:60	Systems could then be scored on their recall of nuggets and precision of their response." ></td>
	<td class="line x" title="19:60	DARPA GALE is a new program funded by DARPA that is running its own evaluation, carried out by BAE Systems, an independent contractor." ></td>
	<td class="line x" title="20:60	Evaluation more closely resembles that done in TREC, but the systems scores will be compared against the scores of human distillers who carry out the same task." ></td>
	<td class="line x" title="21:60	Thus, final numbers will report percent of human performance." ></td>
	<td class="line x" title="22:60	In the DARPA GALE evaluation, which is a future event at the time of this writing, in addition to measuring properties such as precision and recall, BAE will also measure systems ability to find all occurrences of the same fact in the input (redundancy)." ></td>
	<td class="line x" title="23:60	One consideration for an evaluation program is the feel of the program." ></td>
	<td class="line x" title="24:60	Does the evaluation program motivate researchers or does it cause headaches?" ></td>
	<td class="line x" title="25:60	I liken Columbias experience in DUC and currently in GALE to that of Max in Where the Wild Things Are by Maurice Sendak." ></td>
	<td class="line x" title="26:60	We began with punishment (i.e. , if you dont do well, your funding will be in jeopardy), encounter monsters along the way (seemingly arbitrary methods for 3 measuring output quality), finally tame the monsters and sail back peacefully across time." ></td>
	<td class="line x" title="27:60	DUC has reached the peaceful stage, but GALE has not." ></td>
	<td class="line x" title="28:60	The TREC definition pilot had less of a threat of punishment." ></td>
	<td class="line x" title="29:60	Evaluation in all of these programs began at the request of the funders, with the goal of comparing how well different funded systems perform." ></td>
	<td class="line x" title="30:60	Improvement over the years is also measured in order to determine if funding is well spent." ></td>
	<td class="line x" title="31:60	This kind of goal creates anxiety in participants and makes it most important to get the details of the evaluation right; errors in how evaluation is carried out can have great consequences." ></td>
	<td class="line x" title="32:60	Coming to agreement on the metrics used, the methodology for measuring output and the tasks on which performance is measured can be difficult; the environment does not feel friendly." ></td>
	<td class="line x" title="33:60	Even if evaluation within the language generation community was not initiated with the same goals, I think it is reasonable to expect a certain amount of disagreement as the program gets off the ground." ></td>
	<td class="line x" title="34:60	However, over time, researchers come to agreement on some portion of the task and these features become accepted." ></td>
	<td class="line x" title="35:60	At this point in time, it is possible to see the benefits of the program." ></td>
	<td class="line x" title="36:60	Certainly, within DUC, we are at this stage." ></td>
	<td class="line x" title="37:60	DUC has generated large amounts of data, including both input document sets and multiple models of good output for each input set, which has spurred studies both on evaluation and summarization." ></td>
	<td class="line x" title="38:60	Halteren and Teufel, for example, provide a method for annotation of content units and study consensus across summarizers (van Halteren and Teufel, 2003; Teufel and van Halteren, 2004b)." ></td>
	<td class="line x" title="39:60	Nenkova studies significant differences across DUC04 systems (Nenkova, 2005) as well as the properties of human and system summaries (Nenkova, 2006)." ></td>
	<td class="line pc" title="40:60	We can credit DUC with the emergence of automatic methods for evaluation such as ROUGE (Lin and Hovy, 2003; Lin, 2004) which allow quick measurement of systems during development and enable evaluation of larger amounts of data." ></td>
	<td class="line x" title="41:60	We have seen the development of manual methods for evaluation developed both within DUC (Harman and Over, 2004) and without." ></td>
	<td class="line x" title="42:60	The Pyramid method (Nenkova and Passonneau, 2004) provides a annotation method and metric that addresses the issues of reliability and stability of scoring." ></td>
	<td class="line x" title="43:60	Thus, research on evaluation of summarization has become a field in its own right resulting in greater understanding of the effect of different metrics and methodologies." ></td>
	<td class="line x" title="44:60	From DUC and TREC, we have learned important characteristics of a large-scale evaluation, of which the top three might be:  Output can be measured by comparison against a human model, but we know that this comparison will only be valid if multiple models are used." ></td>
	<td class="line x" title="45:60	There are multiple good summaries of the same input and if system output is compared against just one, the results will be biased." ></td>
	<td class="line x" title="46:60	 If the task is appealing to a wide audience, the evaluation will spur research and motivate researchers to join in." ></td>
	<td class="line x" title="47:60	We have seen this with growth of participation in DUC." ></td>
	<td class="line x" title="48:60	One benefit of summarization and QA is that the task is domain-independent and thus, no one site has an advantage over others through experience with a particular domain." ></td>
	<td class="line x" title="49:60	 Given the different ways in which evaluation can be carried out and the fact that different researchers may be biased towards methods which favor their own approach, it is important the evaluation be overseen by a neutral party which is not deeply involved in research on the task itself." ></td>
	<td class="line x" title="50:60	On the other hand, some knowledge is necessary if the evaluation is to be well-designed." ></td>
	<td class="line x" title="51:60	While my talk will focus on large scale evaluation programs that feature quantitative evaluation through comparison with a gold standard, there has been work on task-based evaluation of summarization (McKeown et al, 2005)." ></td>
	<td class="line x" title="52:60	Task-based evaluation is more intensive and to date, has not been done on a large scale across sites, but shows potential for indicating the usefulness of summarization systems." ></td>
	<td class="line x" title="53:60	In this brief abstract, Ive suggested some of the topics that will be covered in my talk, which will tour the land of the wild things for evaluation, illuminating monsters and highlighting events that will allow more peaceful sailing." ></td>
	<td class="line x" title="54:60	Evaluation can be a nightmare, but over time and particularly if carried out away from the influence of funding pressures, it can nurture a community of researchers with common goals." ></td>
	<td class="line x" title="55:60	4 Acknowledgments This material is based upon work supported in part by the ARDA AQUAINT program (Contract No." ></td>
	<td class="line x" title="56:60	MDA908-02-C-0008 and Contract No." ></td>
	<td class="line x" title="57:60	NBCHC040040) and the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="58:60	HR0011-06-C-0023 and Contract No." ></td>
	<td class="line x" title="59:60	N66001-00-1-8919." ></td>
	<td class="line x" title="60:60	Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the DARPA or ARDA." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1643
A Skip-Chain Conditional Random Field For Ranking Meeting Utterances By Importance
Galley, Michel;"></td>
	<td class="line x" title="1:186	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 364372, Sydney, July 2006." ></td>
	<td class="line x" title="2:186	c2006 Association for Computational Linguistics A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance Michel Galley Columbia University Department of Computer Science New York, NY 10027, USA galley@cs.columbia.edu Abstract We describe a probabilistic approach to content selection for meeting summarization." ></td>
	<td class="line x" title="3:186	We use skipchain Conditional Random Fields (CRF) to model non-local pragmatic dependencies between paired utterances such as QUESTION-ANSWER that typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task." ></td>
	<td class="line x" title="4:186	We also discuss different approaches for ranking all utterances in a sequence using CRFs." ></td>
	<td class="line x" title="5:186	Our best performing system achieves 91.3% of human performance when evaluated with the Pyramid evaluation metric, which represents a 3.9% absolute increase compared to our most competitive non-sequential classifier." ></td>
	<td class="line x" title="6:186	1 Introduction Summarizationofmeetingsfacesmanychallenges not found in texts, i.e., high word error rates, absenceofpunctuation,andsometimeslackofgrammaticality and coherent ordering." ></td>
	<td class="line x" title="7:186	On the other hand, meetings present a rich source of structural and pragmatic information that makes summarization of multi-party speech quite unique." ></td>
	<td class="line x" title="8:186	In particular, our analyses of patterns in the verbal exchange between participants found that adjacency pairs (AP), a concept drawn from the conversational analysis literature (Schegloff and Sacks, 1973),haveparticularrelevancetosummarization." ></td>
	<td class="line x" title="9:186	APs are pairs of utterances such as QUESTIONANSWER or OFFER-ACCEPT, inwhichthesecond utteranceissaidtobeconditionallyrelevantonthe first." ></td>
	<td class="line x" title="10:186	We show that there is a strong correlation between the two elements of an AP in summarization, and that one is unlikely to be included if the other element is not present in the summary." ></td>
	<td class="line x" title="11:186	Most current statistical sequence models in natural language processing (NLP), such as hidden This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grants No." ></td>
	<td class="line x" title="12:186	IIS-0121396 and IIS-05-34871, and the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="13:186	HR0011-06-C-0023." ></td>
	<td class="line x" title="14:186	Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the NSF or DARPA." ></td>
	<td class="line x" title="15:186	Markov models (HMMs) (Rabiner, 1989), are linear chains that only encode local dependencies between utterances to be labeled." ></td>
	<td class="line x" title="16:186	In multi-party speech, the two elements of an AP are generally arbitrarily distant, and such models can only poorly account for dependencies underlying APs in summarization." ></td>
	<td class="line x" title="17:186	We use instead skip-chain sequence models (Sutton and McCallum, 2004), which allow us to explicitly model dependencies between distant utterances, and turn out to be particularly effective in the summarization task." ></td>
	<td class="line x" title="18:186	In this paper, we compare two types of network structureslinear-chain and skip-chainand two types of network semanticsBayesian Networks (BNs) and Conditional Random Fields (CRFs)." ></td>
	<td class="line x" title="19:186	We discuss the problem of estimating the class posterior probability of each utterance in a sequence in order to extract the N most probable ones, and show that the cost assigned by a CRF to each utterance needs to be locally normalized in order to outperform BNs." ></td>
	<td class="line x" title="20:186	After analyzing the predictive power of a large set of durational, acoustical, lexical, structural, and information retrieval features, we perform feature selection to have a competitive set of predictors to test the different models." ></td>
	<td class="line pc" title="21:186	Empirical evaluations using two standard summarization metricsthe Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%." ></td>
	<td class="line x" title="22:186	2 Corpus The work presented here was applied to the ICSI Meeting Corpus (Janin et al. , 2003), a corpus of naturally-occurring meetings, i.e. meetings that would have taken place anyway." ></td>
	<td class="line x" title="23:186	Their style is quite informal, and topics are primarily concerned with speech, natural language, artificial 364 intelligence, and networking research." ></td>
	<td class="line x" title="24:186	The corpus contains 75 meetings, which are 60 minutes long on average, and involve a number of participants ranging from 3 to 10 (6 on average)." ></td>
	<td class="line x" title="25:186	The total number of unique speakers is 60, including 26 non-native English speakers." ></td>
	<td class="line x" title="26:186	Experiments in this paper are based either on human orthographic transcriptions or automatic speech recognition output, which were available for all meetings." ></td>
	<td class="line x" title="27:186	Forautomaticrecognition, weusedtheICSISRI-UW speech recognition system (Mirghafori et al. , 2004), a state-of-the-art conversational telephone speech (CTS) recognizer whose language and acoustic models were adapted to the meeting domain." ></td>
	<td class="line x" title="28:186	It achieves 34.8% WER on the ICSI corpus, which is indicative of the difficulty involved in processing meetings automatically." ></td>
	<td class="line x" title="29:186	We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al. , 2004)." ></td>
	<td class="line x" title="30:186	Dialog act (DA) labels describe the pragmatic function of utterances, e.g. a STATEMENT or a BACKCHANNEL." ></td>
	<td class="line x" title="31:186	This auxiliary corpus consists of over 180,000 human-annotated dialog act labels ( =.8), for which so-called adjacency pair (AP) relations (e.g. , APOLOGYDOWNPLAY) were also labeled." ></td>
	<td class="line x" title="32:186	This latter annotation was used to train an AP classifier that is instrumental in automatically determining the structure of our sequence models." ></td>
	<td class="line x" title="33:186	Note that, in the case of three or more speakers, adjacency pair is admittedly an unfortunate term, since labeled APs are generally not adjacent (e.g. , see Table 1), but we will nevertheless use the same terminology to enforce consistency with previous work." ></td>
	<td class="line x" title="34:186	To train and evaluate our summarizer, we used a corpus of extractive summaries produced at the University of Edinburgh (Murray et al. , 2005)." ></td>
	<td class="line x" title="35:186	For each of the 75 meetings, human judges were asked toselecttranscriptionutterancessegmentedbyDA to include in summaries, resulting in an average compression ratio of 6.26% (though no strict limit was imposed)." ></td>
	<td class="line x" title="36:186	Inter-labeler agreement was measured using six meetings that were summarized by multiple coders (average  = .323)." ></td>
	<td class="line x" title="37:186	While this level of agreement is quite low, this situation is not uncommon to summarization, since there may be many good summaries for a given document; a main challenge lies in using evaluation schemes that properly accounts for this diversity." ></td>
	<td class="line x" title="38:186	3 Content selection State sequence Markov models such as hidden Markov models (Rabiner, 1989) have been highly successful in many speech and natural language processing applications, including summarization." ></td>
	<td class="line x" title="39:186	Following an intuition that the probability of a given sentence may be locally conditioned on the previous one, Conroy (2004) built a HMM-based summarizer that consistently ranked among the top systems in recent Document Understanding Conference (DUC) evaluations." ></td>
	<td class="line x" title="40:186	Inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties." ></td>
	<td class="line x" title="41:186	In the case of summarization of conversational speech, Zechner (2002) found, for instance, that a simple technique consisting of linking together questions and answers in summariesand thus preventing the selection of orphan questions or answerssignificantly improved their readability according to various human summary evaluations." ></td>
	<td class="line x" title="42:186	In email summarization (Rambow et al. , 2004), ShresthaandMcKeown(2004)obtainedgoodperformance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange." ></td>
	<td class="line x" title="43:186	In a combined chat and email summarization task, a technique (Zhou and Hovy, 2005) consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines." ></td>
	<td class="line x" title="44:186	The need to model pragmatic influences, such asbetweenaquestionandananswer,isalsoprevalent in meeting summarization." ></td>
	<td class="line x" title="45:186	In fact, questionanswer pairs are not the only discourse relations that we need to preserve in order to create coherent summaries, and, as we will see, most instances of APs would need to be preserved together, either inside or outside the summary." ></td>
	<td class="line x" title="46:186	Table 1 displays an AP construction with one statement (A part) and three respondents (B parts)." ></td>
	<td class="line x" title="47:186	This example illustrates that the number of turns between constituents of APs is variable and thus difficult to model with standard sequence models." ></td>
	<td class="line x" title="48:186	This example also illustrates some of the predictors investigated in this paper." ></td>
	<td class="line x" title="49:186	First, many speakers respond to As utterance, which is generally a strong indicator that the A utterance should be included." ></td>
	<td class="line x" title="50:186	Secondly, while APs are generally characterized in terms of pre-defined dialog acts, such 365 Time Speaker AP Transcript 1480.85-1493.91 1 A are are those ddelays adjustable?" ></td>
	<td class="line x" title="51:186	see a lot of people who actually build stuff with human computer interfaces understand that delay, and and so when you by the time you click it itll be right on because itll go back in time to put the 1489.71-1489.94 2 yeah." ></td>
	<td class="line x" title="52:186	1493.95-1495.41 3 B yeah, uh, not in this case." ></td>
	<td class="line x" title="53:186	1494.31-1495.83 2 B it could do that, couldnt it." ></td>
	<td class="line x" title="54:186	1495.1-1497.07 4 B we could program that pretty easily, couldnt we?" ></td>
	<td class="line x" title="55:186	Table 1: Snippet of a meeting displaying an AP construction, where a question (A) initiates three responses (B)." ></td>
	<td class="line x" title="56:186	Sentences in italic are not present in the reference summary." ></td>
	<td class="line x" title="57:186	as OFFER-ACCEPT, we found that the type of dialog act has much less importance than the existence of the AP connection itself (APs in the data represent a great variety of DA pairs, including many that are not characterized as APs in the litteraturee.g. , STATEMENT-STATEMENT in the table)." ></td>
	<td class="line x" title="58:186	Since DAs seem to matter less than adjacency pairs, the aim will be to build techniques to automatically identify such relations and exploit them in utterance selection." ></td>
	<td class="line x" title="59:186	In the current work, we use skip-chain sequence models (Sutton and McCallum, 2004) to represent dependencies between both contiguous utterances and paired utterances appearing in the same AP constructions." ></td>
	<td class="line x" title="60:186	The graphical representations of skip-chain models, such as the CRF represented in Figure 1, are composed of two types of edges: linear-chain and skip-chain edges." ></td>
	<td class="line x" title="61:186	The latter edges model AP links, which we represent as a set of (s,d) index pairs (note that no more than one AP may share the same second element d)." ></td>
	<td class="line x" title="62:186	The intuition that the summarization labels (1 or 1) are highly correlated with APs is confirmed in Table 2." ></td>
	<td class="line x" title="63:186	While contiguous labels yt1 and yt seem to seldom influence each other, the correlation between AP elements ys and yd is particularly strong, and they have a tendency to be either both included or both excluded." ></td>
	<td class="line x" title="64:186	Note that the second table is not symmetric, because the data allows an A part to be linked to multiple B parts, but not vice-versa." ></td>
	<td class="line x" title="65:186	While counts in Table 2 reflect human labels, we only use automatically predicted (s,d) pairs in the experiments of the remaining part of this paper." ></td>
	<td class="line x" title="66:186	To find these pairs automatically, wetrainedanon-sequentiallog-linearmodel that achieves a .902 accuracy (Galley et al. , 2004)." ></td>
	<td class="line x" title="67:186	4 Skip-Chain Sequence Models In this paper, we investigate conditional models for paired sequences of observations and labels." ></td>
	<td class="line x" title="68:186	In the case of utterance selection, the observation sequence x = x1:T = (x1,,xT) represents local c53c74c61c74c65c6dc65c6ec74c78 c31 c78 c32 c78 c33 c78 c34 c78 c35 c42c61c63c6bc43c68c61c6ec6ec65c6c c53c74c61c74c65c6dc65c6ec74 c53c74c61c74c65c6dc65c6ec74 c53c74c61c74c65c6dc65c6ec74 c79 c31 c79 c32 c79 c33 c79 c34 c79 c35 Figure 1: A skip-chain CRF with pragmatic-level links." ></td>
	<td class="line x" title="69:186	Linear-chain edges yt = 1 yt = 1 yt1 = 1 529 7742 yt1 = 1 7742 116040 Skip-chain edges yd = 1 yd = 1 ys = 1 6792 2191 ys = 1 1479 121591 Table 2: Contingency tables: while the correlation between adjacent labels yt1 and yt is not significant (2 = 2.3, p > .05), empirical evidence clearly shows that ys and yd influence each other (2 = 78948, p < .001)." ></td>
	<td class="line x" title="70:186	summarization predictors (see Section 6), and the binary sequence y = y1:T = (y1,,yT) (where yt  {1,1}) determines which utterances must be included in the summary." ></td>
	<td class="line x" title="71:186	In a discriminative framework, we concentrate our modeling effort on estimating p(y|x) from data, and do not explicitly model the prior probability p(x), since x is fixed during testing anyway." ></td>
	<td class="line x" title="72:186	Many probabilistic approaches to modeling sequences have relied on directed graphical models, also known as Bayesian networks (BN),1 in particular hidden Markov models (Rabiner, 1989) and conditional Markov models (McCallum et al. , 2000)." ></td>
	<td class="line x" title="73:186	However, prominent recent approaches have focused on undirected graphical models, in particular conditional random fields (CRF) (Lafferty et al. , 2001), and provided state-of-the-art performance in many NLP tasks." ></td>
	<td class="line x" title="74:186	In our work, we will provide empirical results for state sequence models of both semantics, and we will now de1Intheexistingliterature,sequencemodelsthatsatisfythe Markovian conditioni.e. , the state of the system at time t depend only on its immediate past tk:t1 (typically just t1)are generally termed dynamic Bayesian networks (DBN)." ></td>
	<td class="line x" title="75:186	Since the particular models under investigation, i.e. skip-chain models, do not have this property, we will simply refer to them as Bayesian networks." ></td>
	<td class="line x" title="76:186	366 scribe skip-chain models for both BNs and CRFs." ></td>
	<td class="line x" title="77:186	In a BN, the probability of the sequence y factorizesasaproductofprobabilitiesoflocalpredictions yt conditioned on their parents pi(yt) (Equation1)." ></td>
	<td class="line x" title="78:186	InaCRF,theprobabilityofthesequencey factorizes according to a set of clique potentials {c}cC, where C is represents the cliques of the underlying graphical model (Equation 2)." ></td>
	<td class="line x" title="79:186	pBN(y|x) = Tproductdisplay i=1 pBN(yt|x,pi(yt)) (1) pCRF(y|x)  productdisplay cC c(xc,yc) (2) We parameterize these BNs and CRFs as loglinear models, and factorize both BNs local prediction probabilities and CRFs clique potentials using two types of feature functions." ></td>
	<td class="line x" title="80:186	Linear-chain feature functions fj(ytk:t,x,t) represent local dependencies that are consistent with an order-k Markov assumption." ></td>
	<td class="line x" title="81:186	For instance, one such function could be a predicate that is true if and only if yt1 = 1, yt = 1, and (xt1,xt) indicates that both utterances are produced by the same speaker." ></td>
	<td class="line x" title="82:186	Given a set of skip edges S = {(st,t)} specifying source and destination indices, skip-chain feature functions gj(yst,yt,x,st,t) exploit dependencies between variables that are arbitrarily distant in the chain." ></td>
	<td class="line x" title="83:186	For instance, the finding that OFFERREJECT pairs are often linked in summaries might be encoded as a skip-chain feature predicate that is true if and only if yst = 1, yt = 1, and the first word of the t-th utterance is no." ></td>
	<td class="line x" title="84:186	Log-linear models for skip-chain sequence models are defined in terms of weights {k} and {k}, one for each feature function." ></td>
	<td class="line x" title="85:186	In the case of BNs, we write: logpBN(yt|x,pi(yt))  Jsummationdisplay j=1 jfj(x,ytk:t,t) + Jprimesummationdisplay j=1 jgj(x,yst,yt,st,t) We can reduce a particular skip-chain CRF to represent only the set of cliques along (yt1,yt) adjacency edges and (yst,yt) skip edges, resulting in only two potential functions: logLIN(x,ytk:t,t) = Jsummationdisplay j=1 jfj(x,ytk:t,t) logSKIP(x,yst,yt,t) = Jprimesummationdisplay j=1 jgj(x,yst,yt,st,t) 4.1 Inference and Parameter Estimation Our CRF and BN models were designed using MALLET (McCallum, 2002), which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training dataD = (x(i),y(i))Ni=1, andprovidesprobabilisticinferencealgorithmsfor linear-chain BNs and CRFs." ></td>
	<td class="line x" title="86:186	Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al. , 2005)." ></td>
	<td class="line x" title="87:186	Approximation is needed when the junction tree of a graphical model is associated with prohibitively large cliques." ></td>
	<td class="line x" title="88:186	For example, the worse case reported in (Sutton and McCallum, 2004) is a clique of 61 nodes." ></td>
	<td class="line x" title="89:186	In the case of skip-chain models representing APs, the inference problem is somewhat simpler: loops in the graph are relatively short, 98% of AP edges span no more than 5 time slices, and the maximum clique size in the entire data is 5." ></td>
	<td class="line x" title="90:186	While exact inference might be possible in our case, we used the simpler approach of adapting standard inference algorithms for linear-chain models." ></td>
	<td class="line x" title="91:186	Specifically, to account for skip-edges, we used a technique inspired by (Sha and Pereira, 2003), in which multiple state dependencies, such as an order-2 Markov model, are encoded using auxiliary tags." ></td>
	<td class="line x" title="92:186	For instance, an order-2 Markov model isparameterizedusingstatetriplesyt2:t,andeach possible triple is converted to a label zt = yt2:t. Using these auxiliary labels only, we can then use the standard forward-backward algorithm for computing marginal distributions in linear-chain CRFs, and Viterbi decoding in linear-chain CRFs and BNs." ></td>
	<td class="line x" title="93:186	The only requirement is to ensure that a transition between zt and zt+1 is forbidden if the sub-states yt1:t common to both states differ, i.e., is assigned an infinite cost." ></td>
	<td class="line x" title="94:186	This approach can be extended to the case of skip-chain transitions." ></td>
	<td class="line x" title="95:186	For instance, an order-1 Markov model with skipedgescanbeconstructedusingzt = (yst,yt1,yt) triples, where the first element yst represents the label at the source of the skip-edge." ></td>
	<td class="line x" title="96:186	Similarly to the case of order-2 Markov models, we need to ensure that only valid sequences of labels are considered, which is trivial to enforce if we assume that no skip edge ranges more than a predefined threshold of k time slices." ></td>
	<td class="line x" title="97:186	Whilethisapproachisnotexact, itstillprovides 367 competitive performance as we will see in Section 8." ></td>
	<td class="line x" title="98:186	In future work, we plan to explore more accurate probabilistic inference techniques." ></td>
	<td class="line x" title="99:186	5 Ranking Utterances by Importance As we will see in Section 8, using the actual {1,1} label predictions of our BNs and CRFs leads to significantly sub-optimal results, which mightbeexplainedbythefollowingreasons." ></td>
	<td class="line x" title="100:186	First, our models are optimized to maximize the conditional log-likelihood of the training data, a measure that does not correlate well with utility measures generally used in retrieval oriented tasks such as summarization, especially when faced with a significant class imbalance (only 6.26% of reference instances are positive)." ></td>
	<td class="line x" title="101:186	Second, the MAP decision rule doesnt give us the freedom to select an arbitrary number of sentences in order to satisfy any constraint on length." ></td>
	<td class="line x" title="102:186	Instead of using actual predictions, it seems more reasonable to compute the posterior probability of each local prediction yt, and extract the N most probable summary sentences (yr1,,yrk), where N may depend on a length expressed in number of words, as it is the case in our evaluation in Section 7." ></td>
	<td class="line x" title="103:186	BNs assign probability distributions over entire sequencesbyestimatingtheprobabilityofeachindividual instance yt in the sequence (Equation 1), and seem thus particularly suited for ranking utterances." ></td>
	<td class="line x" title="104:186	A first approach is then to rank utterances according to the cost of predicting yt = 1 at each time step on the Viterbi path." ></td>
	<td class="line x" title="105:186	While these costs are well-formed (negative log) probabilities in the case of BNs, they cannot be interpreted as such in the case of CRFs, and turn out to produce poor results with CRFs." ></td>
	<td class="line x" title="106:186	Indeed, the set of CRF potentials associated with each time step have no immediate probabilistic interpretation, and cannot be used directly to rank sentences." ></td>
	<td class="line x" title="107:186	Since BNs and CRFs are here parameterized as log-linear models and rely on the same set of feature functions, a second approach is to use CRF-trained model parameters to build a BN classifier that assigns a probability to each yt." ></td>
	<td class="line x" title="108:186	Specifically, the CRF model is first used to generate label predicitons y, from which the locally-normalized model estimates the cost of predicting yt = 1 given a label history y1:t1." ></td>
	<td class="line x" title="109:186	This ensures that we have a well-formed probability distribution at each time slice, while capitalizing on the good performance of CRF models." ></td>
	<td class="line x" title="110:186	Lexical features:  n-grams (n  3)  number of words  number of digits  number of consecutive repeats Information retrieval features:  max/sum/mean frequency of all terms in ut  max/sum/mean idf score  max/sum/mean tfidf score  cosine similarity between word vector of ut with centroid of of the meeting  scores of LSA with 5, 10, 50, 100, 200, 300 concepts Acoustic features:  seconds of silence before/during/after the turn  speech rate  min/max/mean/median/stddev/onset/outset f0 of utterance t, and of first and last word  min/max/mean/stddev energy  .05, .25, .5, .75, .95 quantiles of f0 and energy  pitch range  f0 mean absolute slope Durational and structural features:  duration of the previous/current/next utterance  relative position within meeting (i.e. , index t)  relative position within speaker turn  large number of structural predicates, i.e. is the previous utterance of the same speaker?  number of APs initiated in yt Discourse features:  lexical cohesion score (for topic shifts) (Hearst, 1994)  first and second word of utterance, if in cue word list  number of pronouns  numberoffillersandfluencydevices(e.g. , uh, um)  number of backchannel and acknowledgment tokens (e.g. , uh-huh, ok, right) Table 3: Features for extractive summarization." ></td>
	<td class="line x" title="111:186	Unless otherwise mentioned, we refer to features of utterance t whose label yt we are trying to predict." ></td>
	<td class="line x" title="112:186	6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al. , 2004; Maskey and Hirschberg, 2005; Murray et al. , 2005) or text summarization (Mani and Maybury, 1999)." ></td>
	<td class="line x" title="113:186	Our goal is to build a very competitive feature set that capitalizesonrecentadvancesinsummarizationof both genres." ></td>
	<td class="line x" title="114:186	Table 3 lists some important features." ></td>
	<td class="line x" title="115:186	There is strong evidence that lexical cues such as significant and great are strong predictors in many summarization tasks (Edmundson, 1968)." ></td>
	<td class="line x" title="116:186	Such cues are admittedly quite genre specific, so we did not want to commit ourselves to any specific list, which may not carry over well to our specific speech domain, and we automatically selected a list of n-grams (n  3) using crossvalidation on the training data." ></td>
	<td class="line x" title="117:186	More specifically, we computed the mutual information of each n368 c54c72c61c6e c73c63c72c69c70c74c3a c49c20c74c68c69c6ec6bc20c2d c6fc6ec65c20c74c68c69c6ec67c20c74 c68c61c74c20c6dc61c6bc65c73 c20c61c20c64c69c66c66c65c72c65c6ec63c65c20c69c73c20c74c68c69c73c20c44c43c20c6fc66c66c73c65c74c20c63c6fc6dc70c65c6ec73c61c74c69c6fc6ec2e c31c2dc31c33 c44c69c64c20c79c6fc75c20c68c61c76c65c20c61c20c6cc6fc6fc6b c20c61c74c20c6dc65c65c74c69 c6ec67c20c64c69c67c69c74c73c20c69c66c20c74 c68c65c79c20c68c61c76c65c20c61c20c74c68c65c6dc3f c31c34c2dc32c36 c49c20c64c69c64c6ec27c74c2ec20c4ec6fc2e c32c37c2dc32c39 c48c6dc6dc2e c33c30 c4ec6fc2ec20c54c68c65c20c44 c43c20c63c6fc6dc70c6fc6ec65c6ec74c20c69c73c20c6ec65c67c6c c69c67c69c62c6cc65c2ec20c41c6cc6cc20c6dc69c6bc65c73c20c68c61c76 c65c20c44c43c20c72c65c6dc6fc76c61c6cc2e c33c31c2dc34c31 c59c65c61c68c2e c34c32 c42c65c63c61c75c73c65c20c74 c68c65c72c65c27c73c20c61c20c73 c61c6dc70c6cc65c20c61c6ec64c20c68c6fc6cc64c20c69c6ec20c74 c68c65c20c41c2dc74c6fc2dc44c2e c34c33c2dc35c31 c41c6ec64c20c49c20c61c6cc73c6fc2cc20c75c6dc2cc20c64c69c64c20c73 c6fc6dc65c20c65c78c70c65c72c69c6dc65c6ec74c73c20c61c62c6fc75c74c20c6ec6fc72c6dc61c6cc69c7ac69c6ec67c20c74c68c65c20c70c68c61c73c65c2e c35c32c2dc36c32 c41c6ec64c20c63c61c6dc65c20c75c70c20c77c69c74c68c20c61c20c77c65c62c20c70c61c67c65c20c70c65c6fc70c6c c65c20c63c61c6ec20c74c61c6bc65c20c61c20c6cc6fc6fc6bc20c61c74c2e c36c33c2dc37c35 c4dc6fc64c65 c6cc20c31c20c28c6cc65 c6ec3dc32c30c29c3a c33c31c2dc34c31 c34c33c2dc35c31 c4dc6fc64c65 c6cc20c32c20c28c6cc65 c6ec3dc32c32c29c3a c33c31c2dc34c31 c35c32c2dc36c32 c4dc6fc64c65 c6cc20c33c20c28c6cc65 c6ec3dc32c34c29c3a c35c32c2dc36c32 c36c33c2dc37c35 c50c65c65c72c20c28c6cc65c6ec3dc32 c32c29c3a c31c2dc31c33 c34c33c2dc35c31 c4fc70c74c69c6dc61c6cc20c28c6cc65c6ec3dc32c32c29c3a c33c31c2dc34c31 c35c32c2dc36c32 c31 c31 c32 c33 c34 c33 c33 c32 c32 c53c70c65c61c6bc65c72c3a Figure 2: Model, peer, and optimal summaries are all extracts taken from the same transcription." ></td>
	<td class="line x" title="118:186	gram with the class variable, and selected for each n the 200 best scoring n-grams." ></td>
	<td class="line x" title="119:186	Other lexical features include: the number of digits, which is helpful for identifying sections of the meetings where participants collect data by recording digits; the number of repeats, which may indicate the kind of hesitations and disfluencies that negatively correlates with what is included in the summary." ></td>
	<td class="line x" title="120:186	The information retrieval feature set contains many features that are generally found helpful in summarization, in particular tfidf and scores derived from centroid methods." ></td>
	<td class="line x" title="121:186	In particular, we used the latent semantic analysis (LSA) feature discussed in (Murray et al. , 2005), which attempts to determine sentence importance through singular value decomposition, and whose resulting singular values and singular vectors can be exploited toassociateeachutteranceadegreeofrelevanceto oneofthetop-nconceptsofthemeetings(wheren represents the number of dimensions in the LSA)." ></td>
	<td class="line x" title="122:186	We used the same scoring mechanism as (Murray et al. , 2005), though we extracted features for many different n values." ></td>
	<td class="line x" title="123:186	Acoustic features extracted with Praat (Boersma and Weenink, 2006) were normalized by channel and speaker, including many raw features such as f0 and energy." ></td>
	<td class="line x" title="124:186	Structural features listed in the table are those computed from the sequence model before decoding, e.g., the duration that separates the two elements of an AP." ></td>
	<td class="line x" title="125:186	Finally, discourse features represent predictors that may substitute to DA labels." ></td>
	<td class="line x" title="126:186	While DA tagging is not directly our concern, it is presumably helpful to capitalize on discourse characteristics of utterances involved in adjacency pairs, since different types of dialog acts may be unequally likely to appear in a summary." ></td>
	<td class="line x" title="127:186	7 Evaluation Evaluating summarization is a difficult problem and there is no broad consensus on how to best perform this task." ></td>
	<td class="line pc" title="128:186	Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)." ></td>
	<td class="line o" title="129:186	Pyramid and ROUGE are techniques looking for content units repeated in different model summaries, i.e.,summarycontentunits(SCUs)suchasclauses and noun phrases for the Pyramid method, and ngrams for ROUGE." ></td>
	<td class="line x" title="130:186	The underlying hypothesis is that different model sentences, clauses, or phrases may convey the same meaning, which is a reasonableassumptionwhendealingwithreferencesummaries produced by different authors, since it is quite unlikely that any two abstractors would use the exact same words to convey the same idea." ></td>
	<td class="line x" title="131:186	Our situation is however quite different, since all model summaries of a given document are utterance extracts of that same document, as this can been seen in the excerpt of Figure 2." ></td>
	<td class="line x" title="132:186	In our own annotation of three meetings with SCUs defined as in (Nenkova and Passonneau, 2004a), we found that repetitions and reformulation of the same information are particularly infrequent, and that textual units that express the same content among model summaries are generally originating from the same document sentence (e.g. , in the figure, the first sentence in model 1 and 2 emanate from the same document sentence)." ></td>
	<td class="line x" title="133:186	Very short SCUs (e.g. , base noun phrases) sometimes appeared in different locations of a meeting, but we think it is problematic to assume that connections between such short units are indicative of any similarity of sentential meaning: the contexts are different, and words may be uttered by different speakers, which may lead to unrelated or conflicting pragmatic forces." ></td>
	<td class="line x" title="134:186	For instance, an SCU realized as DC offset and DC component appears in two different sentences in the figure, i.e. those identified as 1-13 and 31-41." ></td>
	<td class="line x" title="135:186	However, the two sentences have contradictory meanings, and it would be unfortunate to increase the score of a peer summary containing the former sentence because the 369 latter is included in some model summaries." ></td>
	<td class="line x" title="136:186	For all these reasons, we believe that summarization evaluation in our case should rely on the following restrictive matching: two summary units should be considered equivalent if and only if they are extracted from the same location in the original document (e.g. , the DC appearing in models 1 and 2 is not the same as the DC in the peer summary, since they are extracted from different sentences)." ></td>
	<td class="line x" title="137:186	This constraint on the matching is reflected in our Pyramid evaluation, and we define an SCU as a word and its document position, which lets us distinguish (DC,11) from (DC,33)." ></td>
	<td class="line x" title="138:186	While this restriction on SCUs forces us to disregard scarcely occurring paraphrases and repetitions of the same information, it provides the benefit of automated evaluation." ></td>
	<td class="line x" title="139:186	Once all SCUs have been identified, the Pyramid method is applied as in (Nenkova and Passonneau, 2004b): wecomputeascoreD byaddingfor each SCU present in the summary a score equal to the number of model summaries in which that SCU appears." ></td>
	<td class="line x" title="140:186	The Pyramid score P is computed by dividing D by the maximum D value that is obtainable given the constraint on length." ></td>
	<td class="line x" title="141:186	For instance, the peer summary in the figure gets a score D = 9 (since the 9 SCUs in range 43-51 occur in one model), and the maximum obtainable score is D = 44 (all SCUs of the optimal summary appear in exactly two model summaries), hence the peer summarys score is P = .204." ></td>
	<td class="line x" title="142:186	While our evaluation scheme is similar to comparing the binary predictions of model and peer summarieseach prediction determining whether a given transcription word is included or not andaveragingprecisionscoresoverallpeer-model pairs, the Pyramid evaluation differs on an important point, which makes us prefer the Pyramid evaluation method: the maximum possible Pyramid score is always guaranteed to be 1, but average precision scores can become arbitrarily low as the consensus between summary annotators decreases." ></td>
	<td class="line x" title="143:186	For instance, the average precision score of the optimal summary in the figure is PR = 23.2 2Precision scores of the optimal summary compared against the the three model summaries are .5, 1, and .5, respectively, and hence average 23." ></td>
	<td class="line x" title="144:186	We can show that P = PR/PR, where PR is the average precision of the optimal summary." ></td>
	<td class="line x" title="145:186	Lack of space prevent us from providing a proof, so we will just show that the equality holds in our example: since the peer summarys precision scores against the three model summaries are respectively 922, 0, and 0, we have PR/PR = ( 966)/(23) = 944 = P. FEATURE F=1 1 utterance duration .246 2 100-dimension LSA .268 3 duration of utterance t1 .275 4 time between utterances s and d = t .281 5 IDF mean .284 6 meeting position .286 7 number of APs initiated in t .288 8 duration of utterance t + 1 .288 9 number of fillers .289 10 .25-quantile of energy .290 11 number of lexical repeats .292 12 lexical cohesion score .294 13 f0 mean of last word of utterance t .294 14 LSA 50 dimensions .295 15 utterances (t,t + 1) by same speaker .298 16 speech rate .302 17 is that .303 18 for the .303 19 (ut1,ut) by same speaker .305 20 to try .305 21 meetings .305 22 utterance starts with and .306 23 we have .306 24 new .307 25 utterance starts with what .307 Table 4: Forward feature selection." ></td>
	<td class="line x" title="146:186	In the case of the six test meetings, which all have either 3 or 4 model summaries, the maximum possible average precision is .6405." ></td>
	<td class="line x" title="147:186	8 Experiments We follow (Murray et al. , 2005) in using the same six meetings as test data, since each of these meetings has multiple reference summaries." ></td>
	<td class="line x" title="148:186	The remaining69meetingswereusedfortraining,which represent in total more than 103,000 training instances (or DA units), of which 6,464 are positives (6.24%)." ></td>
	<td class="line x" title="149:186	The multi-reference test set contains more than 28,000 instances." ></td>
	<td class="line x" title="150:186	The goal of a preliminary experiment was to devise a set of useful predictors from a full set of 1171." ></td>
	<td class="line x" title="151:186	We performed feature selection by incrementally growing a log-linear model with order0 features f(x,yt) using a forward feature selection procedure similar to (Berger et al. , 1996)." ></td>
	<td class="line x" title="152:186	Probably due to the imbalance between positive and negative samples, we found it more effective to rank candidate features by gains in F-measure (through5-foldcrossvalidationontheentiretrainingset)." ></td>
	<td class="line x" title="153:186	TheincreaseinF1 byaddingnewfeatures to the model is displayed in Table 4; this greedy search resulted in a set S of 217 features." ></td>
	<td class="line x" title="154:186	We now analyze the performance of different sequence models on our test set." ></td>
	<td class="line x" title="155:186	The target length of each summary was set to 12.7% of the number of words of the full document, which is the aver370 age on the entire training data (the average on the test data is 12.9%)." ></td>
	<td class="line x" title="156:186	In Table 5, we use an order-0 CRF to compare S against all features and various categorical groupings." ></td>
	<td class="line x" title="157:186	Overall, we notice lexical predictors and statistics derived from them (e.g. LSA features) represent the most helpful feature group (.497), though all other features combined achieve a competitive performance (.476)." ></td>
	<td class="line x" title="158:186	Table 6 displays performance for sequence models incorporating linear-chain features of increasing order k. Its second column indicates what criterion was used to rank utterances." ></td>
	<td class="line x" title="159:186	In the case of pred, we used actual model {1,1} predictions, which in all cases generated summaries much shorted than the allowable length, and produced poor performance." ></td>
	<td class="line x" title="160:186	Costs and norm-CRF refer to the two ranking criteria presented in Section 5, and it is clear that the performance of CRFs degrades with increasing orders without local normalization." ></td>
	<td class="line x" title="161:186	While the contingency counts in Table 2 only hinted a limited benefit of linear-chain features, empirical results show the contrary especially for order k = 2." ></td>
	<td class="line x" title="162:186	However, the further increase of k causes overfitting, and skip-chain features seem a better way to capture non-local dependencies while keeping the number of model parameters relatively small." ></td>
	<td class="line x" title="163:186	Overall, the addition of skip-chain edges to linear-chain models provide noticeable improvement in Pyramid scores." ></td>
	<td class="line x" title="164:186	Our system that performed best on cross-validation data is an order-2 CRF with skip-chain transitions, which achieves a Pyramid score of P = .554." ></td>
	<td class="line x" title="165:186	We now assess the significance of our results by comparing our best system against: (1) a lead summarizer that always selects the first N utterances to match the predefined length; (2) human performance, which is obtained by leave-one-out comparisons among references (Table 7); (3) optimal summaries generated using the procedure explained in (Nenkova and Passonneau, 2004b) by ranking document utterances by the number of model summaries in which they appear." ></td>
	<td class="line x" title="166:186	It appears that our system is considerably better than the baseline, and achieves 91.3% of human performance in terms of Pyramid scores, and 83% if using ASR transcription." ></td>
	<td class="line x" title="167:186	This last result is particularly positive if we consider our strong reliance on lexical features." ></td>
	<td class="line o" title="168:186	For completeness, we also included standard ROUGE (1, 2, and L) scores in Table 7, which were obtained using parameters defined for the FEATURE SET P lexical .471 IR .415 lexical + IR .497 acoustic .407 structural/durational .478 acoustic + structural/durational .476 all features .507 selected features (S) .515 Table 5: Pyramid score for each feature set." ></td>
	<td class="line x" title="169:186	MODEL RANKING k = 1 2 3 linear-chain BN pred .241 .267 .269 linear-chain BN costs .512 .519 .525 skip-chain BN costs .543 .549 .542 linear-chain CRF pred .326 .36 .348 linear-chain CRF costs .508 .475 .447 linear-chain CRF norm-CRF .53 .548 .54 skip-chain CRF norm-CRF .541 .554 .559 Table6: Pyramidscoresfordifferentsequencemodels,where k stands for the order of linear-chain features." ></td>
	<td class="line x" title="170:186	The value in bold is the performance of the model that was selected after a 5-fold cross validation on the training data, which obtained the highest F1 score." ></td>
	<td class="line o" title="171:186	SUMMARIZER P R-1 R-2 R-L baseline .188 .501 .210 .495 skip-chain CRF (transcript) .554 .715 .442 .709 skip-chain CRF (ASR) .504 .714 .42 .706 human .607 .720 .477 .715 optimal 1 .791 .648 .788 Table 7: Pyramid, and average ROUGE scores for summaries produces by a baseline (lead summarizer), our best system, humans, and the optimal summarizer." ></td>
	<td class="line x" title="172:186	DUC-05 evaluation." ></td>
	<td class="line o" title="173:186	Since system summaries have on average approximately the same length as references, we only report recall measures of ROUGE (precision and F averages are within  .002).3 It may come as a surprise that our best system (both with ASR and true words) performs almost as well as humans; it seems more reasonable to conclude that, in our case, ROUGE has trouble discriminating between systems with moderately close performance." ></td>
	<td class="line x" title="174:186	This seems to confirm our impression that content evaluation in our task should be based on exact matches." ></td>
	<td class="line x" title="175:186	We performed a last experiment to compare our bestsystemagainstMurrayetal.(2005), whoused the same test data, but constrained summary sizes in terms of number of DA units instead of words." ></td>
	<td class="line x" title="176:186	In their experiments, 10% of DAs had to be selected." ></td>
	<td class="line x" title="177:186	Our system achieves .91 recall, .5 precision, and .64 F1 with the same length constraint." ></td>
	<td class="line o" title="178:186	3Human performance with ROUGE was assessed by cross-validating reference summaries of each meeting (i.e. , n references for a given meeting resulted in n evaluations against the other references)." ></td>
	<td class="line x" title="179:186	We used the same leave-oneout procedure with other summarizers, in order to get results comparable to humans." ></td>
	<td class="line x" title="180:186	371 The discrepancy between recall and precision is largely due to the fact that generated summaries areonaveragemuchlongerthanmodelsummaries (10% vs. 6.26% of DAs), which explains why our precision is relatively low in this last evaluation." ></td>
	<td class="line o" title="181:186	The best ROUGE-1 measure reported in (Murray et al. , 2005) is .69 recall, which is significantly lower than ours according to confidence intervals." ></td>
	<td class="line x" title="182:186	9 Conclusion An order-2 CRF with skip-chain dependencies derived from the automatic analysis of participant interaction was shown to outperform linear-chain BNs and CRFs, despite the incorporation in all cases of the same competitive set of predictors resulting from cross-validated feature selection." ></td>
	<td class="line x" title="183:186	Compared to an order-0 CRF model, the absolute increase in performance is 3.9% (7.5% relative increase), which indicates that it is helpful to use skip-chain sequence models in the summarization task." ></td>
	<td class="line x" title="184:186	Our best performing system reaches 91.3% of human performance, and scales relatively well on automatic speech recognition output." ></td>
	<td class="line x" title="185:186	Acknowledgments This work has benefited greatly from suggestions andadvicefromKathleenMcKeown." ></td>
	<td class="line x" title="186:186	Ialsowould like to thank Jean Carletta, Steve Renals and Gabriel Murray for giving me access to their summarization corpus, Ani Nenkova for helpful discussionsaboutsummarizationevaluation, Michael Collins, Daniel Ellis, Julia Hirschberg, and Owen Rambow for useful preliminary discussions, and three anonymous reviewers for their insightful comments on an earlier version of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1005
Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-Goals
Uchimoto, Kiyotaka;Kotani, Katsunori;Zhang, Yujie;Isahara, Hitoshi;"></td>
	<td class="line x" title="1:194	Proceedings of NAACL HLT 2007, pages 3340, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:194	c2007 Association for Computational Linguistics Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara National Institute of Information and Communications Technology 3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp Abstract The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate." ></td>
	<td class="line x" title="3:194	We propose a method for automatically evaluating the quality of each translation." ></td>
	<td class="line x" title="4:194	In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality." ></td>
	<td class="line x" title="5:194	In EnglishJapanese translation, for example, prepositions and infinitives must be appropriately translated." ></td>
	<td class="line x" title="6:194	We show several procedures that enable evaluating the quality of a translated sentence more appropriately than using conventional methods." ></td>
	<td class="line x" title="7:194	The first procedure is constructing a test set where the conditions are assigned to each test-set sentence in the form of yes/no questions." ></td>
	<td class="line x" title="8:194	The second procedure is developing a system that determines an answer to each question." ></td>
	<td class="line x" title="9:194	The third procedure is combining a measure based on the questions and conventional measures." ></td>
	<td class="line x" title="10:194	We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals." ></td>
	<td class="line x" title="11:194	Promising results are shown." ></td>
	<td class="line x" title="12:194	1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue." ></td>
	<td class="line oc" title="13:194	In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al. , 2000; Akiba et al. , 2001; Papineni et al. , 2002; NIST, 2002; Leusch et al. , 2003; Turian et al. , 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gimenez et al. , 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently." ></td>
	<td class="line x" title="14:194	For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003)." ></td>
	<td class="line x" title="15:194	This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves." ></td>
	<td class="line x" title="16:194	MT systems can be ranked if a set of MT results for each system and their reference translations are given." ></td>
	<td class="line x" title="17:194	Usually, about 300 or more sentences are used to automatically rank MT systems (Koehn, 2004)." ></td>
	<td class="line x" title="18:194	However, the quality of a sentence translated by an MT system is difficult to evaluate." ></td>
	<td class="line x" title="19:194	For example, the results of five MTs into Japanese of the sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers. are shown in Table 1." ></td>
	<td class="line x" title="20:194	A conventional automatic evaluation method ranks the fifth MT result first although its human subjective evaluation is the lowest." ></td>
	<td class="line x" title="21:194	This is because conventional methods are based on the similarity between a translated sentence and its reference translation, and they give the translated sentence a high score when the two sentences are globally similar to each other in terms of lexical overlap." ></td>
	<td class="line x" title="22:194	However, in the case of the above example, 33 Table 1: Examples of conventional automatic evaluations." ></td>
	<td class="line x" title="23:194	Original sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers." ></td>
	<td class="line x" title="24:194	reference translation (in Japanese) roudousha no igan no wariai wa, asubesuto roudousha no tame ni saikou to naru youda." ></td>
	<td class="line x" title="25:194	System MT results BLEU NIST Fluency Adequacy 1 roudousha no aida no igan no paasenteeji wa, donoyouna ishiwata roudousha no tame ni demo mottomo ookii youdearu . 0.2111 2.1328 2 3 2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto roudousha no tame ni mottomo takai youni omowa re masu . 0.2572 2.1234 2 3 3 roudousha no aida no igan no paasenteeji wa donna asubesuto no tame ni mo mottomo takai youni mie masu 0 1.8094 1 2 4 roudousha no aida no igan no paasenteeji wa ninino ishiwata ni wa mottomo takaku mie masu . 0 1.5902 1 2 5 roudousha no naka no igan no wariai wa donna asubesuto ni mo mottomo takai youni mieru . 0.2692 2.2640 1 2 the most important thing to maintain a high translation quality is to correctly translate for into the target language, and it would be difficult to detect the importance just by comparing an MT result and its reference translations even if the number of reference translations is increased." ></td>
	<td class="line x" title="26:194	In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality." ></td>
	<td class="line x" title="27:194	In this paper, we show that constructing a test set where the conditions that are mainly established from a linguistic point of view are assigned to each test-set sentence in the form of yes/no questions, developing a system that determines an answer to each question, and combining a measure based on the questions and conventional measures enable the evaluation of the quality of a translated sentence more appropriately than using conventional methods." ></td>
	<td class="line x" title="28:194	We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals." ></td>
	<td class="line x" title="29:194	2 Test Set for Evaluating Machine Translation Quality 2.1 Test Set Two main types of data are used for evaluating MT quality." ></td>
	<td class="line x" title="30:194	One type of data is constructed by arbitrarily collecting sentence pairs in the sourceand target-languages, and the other is constructed by intensively collecting sentence pairs that include linguistic phenomena that are difficult to automatically translate." ></td>
	<td class="line x" title="31:194	Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation 1, NIST Machine Translation Evaluation 2, and HTRDP Evaluation 3 were organized to support the improvement of MT techniques." ></td>
	<td class="line x" title="32:194	The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conversation data for fair evaluation." ></td>
	<td class="line x" title="33:194	They are classified as the former type of data mentioned above." ></td>
	<td class="line x" title="34:194	On the other hand, the data provided by NTT (Ikehara et al. , 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type." ></td>
	<td class="line x" title="35:194	Almost all the data mentioned above consist of only parallel translations in two languages." ></td>
	<td class="line x" title="36:194	Data with information for evaluating MT results, such as JEIDAs are rarely found." ></td>
	<td class="line x" title="37:194	In this paper, we call data that consist of parallel translations collected for MT evaluation and that the information for MT evaluation is assigned to, a test set." ></td>
	<td class="line x" title="38:194	The most characteristic information assigned to the JEIDA test set is the yes/no question for assessing the translation results." ></td>
	<td class="line x" title="39:194	For example, a yes/no question such as Is for translated into an expression representing a cause/reason such as de? (in Japanese) is assigned to a test-set sentence." ></td>
	<td class="line x" title="40:194	We can evaluate MT results objectively by answering the question." ></td>
	<td class="line x" title="41:194	An example of a test-set sample consisting of an ID, a source-language sample sentence, its reference translation, and a question is as follows." ></td>
	<td class="line x" title="42:194	1 http://www.slt.atr.jp/IWSLT2006/ 2 http://www.nist.gov/speech/tests/mt/index.htm 3 http://www.863data.org.cn/ 34 ID 1.1.7.1.3-1 Sample sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers." ></td>
	<td class="line x" title="43:194	reference translation (in Japanese) roudousha no igan no wariai wa, asubesuto roudousha no tame ni saikou to naru youda . Question Is appear to translated into an auxiliary verb such as youda?" ></td>
	<td class="line x" title="44:194	The questions are classified mainly in terms of grammar, and the numbers to the left of the hyphenation of each ID such as 1.1.7.1.3 represent the categories of the questions." ></td>
	<td class="line x" title="45:194	For example, the above question is related to catenative verbs." ></td>
	<td class="line x" title="46:194	The JEIDA test set consists of two parts, one for the evaluation of English-Japanese MT and the other for that of Japanese-English MT. We focused on the part for English-Japanese MT. This part consists of 769 sample sentences, each of which has a yes/no question." ></td>
	<td class="line x" title="47:194	The 769 sentences were translated by using five commercial MT systems to investigate the relationship between subjective evaluation based on yes/no questions and conventional subjective evaluation based on fluency and adequacy." ></td>
	<td class="line x" title="48:194	The instruction for the subjective evaluation based on fluency and adequacy followed that given in the TIDES specification (TIDES, 2002)." ></td>
	<td class="line x" title="49:194	The subjective evaluation based on yes/no questions was done by manually answering each question for each translation." ></td>
	<td class="line x" title="50:194	The subjective evaluation based on the yes/no questions was stable; namely, it was almost independent of the human subjects in our preliminary investigation." ></td>
	<td class="line x" title="51:194	There were only two questions for which the answers generated inconsistency in the subjective evaluation when 1,500 question-answer pairs were randomly sampled and evaluated by two human subjects." ></td>
	<td class="line x" title="52:194	Then, we investigated the correlation between the two types of subjective evaluation." ></td>
	<td class="line x" title="53:194	The correlation coefficients mentioned in this paper are statistically significant at the 1% or less significance level." ></td>
	<td class="line x" title="54:194	The Spearman rank-order correlation coefficient is used in this paper." ></td>
	<td class="line x" title="55:194	In the subjective evaluation based on yes/no questions, yes and no were numerically transformed into 1 and 1." ></td>
	<td class="line x" title="56:194	For 3,845 translations obtained by using five MT systems, the correlation coefficients between the subjective evaluations based on yes/no questions and based on fluency and adequacy were 0.48 for fluency and 0.63 for adequacy." ></td>
	<td class="line x" title="57:194	These results indicate that the two subjective evaluations have relatively strong correlations." ></td>
	<td class="line x" title="58:194	The correlation is especially strong between the subjective evaluation based on yes/no questions and adequacy." ></td>
	<td class="line x" title="59:194	2.2 Expansion of JEIDA Test Set Each sample sentence in the JEIDA test set has only one question." ></td>
	<td class="line x" title="60:194	Therefore, in the subjective evaluation using the JEIDA test set, translation errors that do not involve the pre-assigned question are ignored even if they are serious." ></td>
	<td class="line x" title="61:194	Therefore, translations that have serious errors that are not related to the question tend to be evaluated as being of high quality." ></td>
	<td class="line x" title="62:194	To solve this problem, we expanded the test set by adding new questions about translations with the serious errors." ></td>
	<td class="line x" title="63:194	Sentences whose average grades were three or less for fluency and adequacy for the translation results of the five MT systems were selected for the expansion." ></td>
	<td class="line x" title="64:194	Besides them, sentences whose average grades were more than three for fluency and adequacy for the translation results of the five MT systems were selected when a majority of evaluation results based on yes/no questions about the translations of the five MT systems were no." ></td>
	<td class="line x" title="65:194	The number of selected sentences was 150." ></td>
	<td class="line x" title="66:194	The expansion was manually performed using the following steps." ></td>
	<td class="line x" title="67:194	1." ></td>
	<td class="line x" title="68:194	Serious translation errors are extracted from the MT results." ></td>
	<td class="line x" title="69:194	2." ></td>
	<td class="line x" title="70:194	For each extracted error, questions strongly related to the error are searched for in the test set." ></td>
	<td class="line x" title="71:194	If related questions are found, the same types of questions are generated for the selected sentence, and the same ID as that of the related question is assigned to each generated question." ></td>
	<td class="line x" title="72:194	Otherwise, questions are newly generated, and a new ID is assigned to each generated question." ></td>
	<td class="line x" title="73:194	3." ></td>
	<td class="line x" title="74:194	Each MT result is evaluated according to each added question." ></td>
	<td class="line x" title="75:194	Eventually, one or more questions were assigned to each selected sentence in the test set." ></td>
	<td class="line x" title="76:194	Among the 150 35 Table 2: Expanded test-set samples." ></td>
	<td class="line x" title="77:194	ID 1.1.7.1.3-1 Original Sample sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers." ></td>
	<td class="line x" title="78:194	reference translation (in Japanese) roudousha no igan no wariai wa, asubesuto roudousha no tame ni saikou to naru youda . Question (Q-0) Is appear to translated into an auxiliary verb such as youda?" ></td>
	<td class="line x" title="79:194	ID 1.1.6.1.3-5 Expanded Translation error For is not translated appropriately." ></td>
	<td class="line x" title="80:194	Question-1 (Q-1) Is for translated into an expression representing a cause/reason such as de?" ></td>
	<td class="line x" title="81:194	ID Additional-1 Expanded Translation error Some expressions are not translated." ></td>
	<td class="line x" title="82:194	Question-2 (Q-2) Are all English words translated into Japanese?" ></td>
	<td class="line x" title="83:194	Table 3: Examples of subjective evaluations based on yes/no questions." ></td>
	<td class="line x" title="84:194	Answer System MT results Q-0 Q-1 Q-2 Fluency Adequacy 1 roudousha no aida no igan no paasenteeji wa, donoyouna ishiwata roudousha no tame ni demo mottomo ookii youdearu . YesNoYes 2 3 2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto roudousha no tame ni mottomo takai youni omowa re masu . Yes Yes Yes 2 3 3 roudousha no aida no igan no paasenteeji wa donna asubesuto no tame ni mo mottomo takai youni mie masu Yes No No 1 2 4 roudousha no aida no igan no paasenteeji wa ninino ishiwata ni wa mottomo takaku mie masu . Yes No No 1 2 5 roudousha no naka no igan no wariai wa donna asubesuto ni mo mottomo takai youni mieru . Yes No No 1 2 selected sentences, questions were newly assigned to 103 sentences." ></td>
	<td class="line x" title="85:194	The number of added questions was 148." ></td>
	<td class="line x" title="86:194	The maximum number of questions added to a sentence was five." ></td>
	<td class="line x" title="87:194	After expanding the test set, the correlation coefficients between the subjective evaluations based on yes/no questions and based on fluency and adequacy increased from 0.48 to 0.51 for fluency and from 0.63 to 0.66 for adequacy." ></td>
	<td class="line x" title="88:194	The differences between the correlation coefficients obtained before and after the expansion are statistically significant at the 5% or less significance level for adequacy." ></td>
	<td class="line x" title="89:194	These results indicate that the expansion of the test set significantly improves the correlation between the subjective evaluations based on yes/no questions and based on adequacy." ></td>
	<td class="line x" title="90:194	When two or more questions were assigned to a test-set sentence, the subjective evaluation based on the questions was decided by the majority answer." ></td>
	<td class="line x" title="91:194	The majority answers, yes and no, were numerically transformed into 1 and 1." ></td>
	<td class="line x" title="92:194	Ties between yes and no were transformed into 0." ></td>
	<td class="line x" title="93:194	Examples of added questions and the subjective evaluations based on the questions are shown in Tables 2 and 3." ></td>
	<td class="line x" title="94:194	3 Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals 3.1 A New Measure for Evaluating Machine Translation Quality The JEIDA test set was not designed for automatic evaluation but for human subjective evaluation." ></td>
	<td class="line x" title="95:194	However, a measure for automatic MT evaluation that strongly correlates fluency and adequacy is likely to be established because the subjective evaluation based on yes/no questions has a relatively strong correlation with the subjective evaluation based on fluency and adequacy, as mentioned in Section 2." ></td>
	<td class="line x" title="96:194	In this section, we describe a method for automatically evaluating MT quality by predicting an answer to each yes/no question and using those answers." ></td>
	<td class="line x" title="97:194	Hereafter, we assume that each yes/no question is defined as a sub-goal that a given translation should satisfy and that the sub-goal is accomplished if the answer to the corresponding yes/no question to the sub-goal is yes." ></td>
	<td class="line x" title="98:194	We also assume that the sub-goal is unaccomplished if the answer is no." ></td>
	<td class="line x" title="99:194	A new evaluation score, A, is defined based on a multiple lin36 Table 4: Examples of Patterns." ></td>
	<td class="line x" title="100:194	Sample sentence She lived there by herself." ></td>
	<td class="line x" title="101:194	Question Is by herself translated as hitori de?" ></td>
	<td class="line x" title="102:194	Pattern The answer is yes if the pattern [hitori dake de|hitori kiri de |tandoku de|tanshin de] is included in a translation." ></td>
	<td class="line x" title="103:194	Otherwise, the answer is no." ></td>
	<td class="line x" title="104:194	Sample sentence They speak English in New Zealand." ></td>
	<td class="line x" title="105:194	Question The personal pronoun they is omitted in a translation like nyuujiilando de wa eigo wo hanasu?" ></td>
	<td class="line x" title="106:194	Pattern The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation." ></td>
	<td class="line x" title="107:194	Otherwise, the answer is no." ></td>
	<td class="line x" title="108:194	ear regression model as follows using the rate of accomplishment of the sub-goals and the similarities between a given translation and its reference translation." ></td>
	<td class="line x" title="109:194	The best-fitted line for the observed data is calculated by the method of least-squares (Draper and Smith, 1981)." ></td>
	<td class="line x" title="110:194	A = m summationdisplay i=1  S i  S i (1) + n summationdisplay j=1 ( Q j  Q j +  Q prime j  Q prime j )+ epsilon1 Q j = braceleftBigg 1 : if subgoal is accomplished 0:otherwise (2) Q prime j = braceleftBigg 1 : if subgoal is unaccomplished 0:otherwise (3) Here, the term Q j corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and  Q j is a weight for the rate of accomplishment." ></td>
	<td class="line x" title="111:194	The term Q prime j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and  Q prime j is a weight for the rate of unaccomplishment." ></td>
	<td class="line x" title="112:194	The value n indicates the number of types of sub-goals." ></td>
	<td class="line x" title="113:194	The term  epsilon1 is constant." ></td>
	<td class="line x" title="114:194	The term S i indicates a similarity between a translated sentence and its reference translation, and  S i is a weight for the similarity." ></td>
	<td class="line oc" title="115:194	Many methods for calculating the similarity have been proposed (Niessen et al. , 2000; Akiba et al. , 2001; Papineni et al. , 2002; NIST, 2002; Leusch et al. , 2003; Turian et al. , 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gimenez et al. , 2005)." ></td>
	<td class="line oc" title="116:194	In our research, 23 scores, namely BLEU (Papineni et al. , 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al. , 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al. , 2000), PER (Leusch et al. , 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S,SU, W-1.2), were used to calculate each similarity S i . Therefore, the value of m in Eq." ></td>
	<td class="line x" title="117:194	(1) was 23." ></td>
	<td class="line x" title="118:194	Japanese word segmentation was performed by using JUMAN 4 in our experiments." ></td>
	<td class="line x" title="119:194	As you can see, the definition of our new measure is based on a combination of an evaluation measure focusing on local information and that focusing on global information." ></td>
	<td class="line x" title="120:194	3.2 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is estimated by determining the answer to each question as yes or no." ></td>
	<td class="line x" title="121:194	This section describes a method based on simple patterns for determining the answers." ></td>
	<td class="line x" title="122:194	An answer to each question is automatically determined by checking whether patterns are included in a translation or not." ></td>
	<td class="line x" title="123:194	The patterns are constructed for each question." ></td>
	<td class="line x" title="124:194	All of the patterns are expressed in hiragana characters." ></td>
	<td class="line x" title="125:194	Before applying the patterns to a given translation, the translation is transformed into hiragana characters, and all punctuation is eliminated." ></td>
	<td class="line x" title="126:194	The transformation to hiragana characters was performed by using JUMAN in our experiments." ></td>
	<td class="line x" title="127:194	Test-set sentences, the questions assigned to them, and the patterns constructed for the questions are shown in Table 4." ></td>
	<td class="line x" title="128:194	In the patterns, the symbol | represents OR." ></td>
	<td class="line x" title="129:194	3.3 Automatic Sub-goal Generation and Automatic Estimation of Rate of Accomplishment of Sub-goals We found that expressions important for maintaining a high translation quality were often commonly 4 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html 37 included in the reference translations for each testset sentence." ></td>
	<td class="line x" title="130:194	We also found that the expression was also related to the yes/no question assigned to the test-set sentence." ></td>
	<td class="line x" title="131:194	Therefore, we automatically generate yes/no questions in the following steps." ></td>
	<td class="line x" title="132:194	1." ></td>
	<td class="line x" title="133:194	For each test-set sentence, a set of words commonly appearing in the reference translations are extracted." ></td>
	<td class="line x" title="134:194	2." ></td>
	<td class="line x" title="135:194	For each combination of n words in the set of words extracted in the first step, skip word n-grams commonly appearing in the reference translations in the same word order are selected as a set of common skip word n-grams." ></td>
	<td class="line x" title="136:194	3." ></td>
	<td class="line x" title="137:194	For each test-set sentence, the sub-goal is defined as the yes/no question Are all of the common skip word n-grams included in the translation? If no common skip word n-grams are found, the yes/no question is not generated." ></td>
	<td class="line x" title="138:194	The answer to the yes/no question is determined to be yes if all of the common skip word n-grams are included in a translation." ></td>
	<td class="line x" title="139:194	Otherwise, the answer is determined to be no." ></td>
	<td class="line x" title="140:194	This scheme assigns greater weight to important phrases that should be included in the translation to maintain a high translation quality." ></td>
	<td class="line x" title="141:194	Our observation is that those important phrases are often common between human translations." ></td>
	<td class="line x" title="142:194	A similar scheme was proposed by Babych and Hartley (Babych and Hartley, 2004) for BLEU." ></td>
	<td class="line x" title="143:194	In their scheme, greater weight is assigned to components that are salient throughout the document." ></td>
	<td class="line x" title="144:194	Therefore, their scheme focuses on global context while our scheme focuses on local context." ></td>
	<td class="line x" title="145:194	We believe that the two schemes are complementary to each other." ></td>
	<td class="line x" title="146:194	4 Experiments and Discussion In our experiments, the translation results of three MT systems and their subjective evaluation results were used as a development set for constructing the patterns described in Section 3.2 and for tuning the parameters  S i,  Q j,  Q prime j, and  epsilon1 in Eq." ></td>
	<td class="line x" title="147:194	(1)." ></td>
	<td class="line x" title="148:194	The translations and evaluation results of the remaining two MT systems were used as an evaluation set for testing." ></td>
	<td class="line x" title="149:194	In the development set, each test-set sentence has at least one question, at least one reference translation, three MT results, and subjective evaluation results of the three MT results." ></td>
	<td class="line x" title="150:194	The patterns for determining yes/no answers were manually constructed for the questions assigned to the 769 test-set sentences." ></td>
	<td class="line x" title="151:194	There were 917 questions assigned to them." ></td>
	<td class="line x" title="152:194	Among them, the patterns could be constructed for 898 questions assigned to 767 test-set sentences." ></td>
	<td class="line x" title="153:194	The remaining 19 questions were skipped because making simple patterns as described in Section 3.2 was difficult; for example, one of the questions was Is the whole sentence translated into one sentence?." ></td>
	<td class="line x" title="154:194	The yes/no answer determination accuracies obtained by using the patterns are shown in Table 5." ></td>
	<td class="line x" title="155:194	Table 5: Results of yes/no answer determination." ></td>
	<td class="line x" title="156:194	Test set Accuracy Development 97.6% (2,629/2,694) Evaluation 82.8% (1,487/1,796) We investigated the correlation between the evaluation score, A in Eq." ></td>
	<td class="line x" title="157:194	(1) and the subjective evaluations, fluency and adequacy, for the 769 test-set sentences." ></td>
	<td class="line x" title="158:194	First, to maximize the correlation coefficients between the evaluation score, A, and the human subjective evaluations, fluency and adequacy, the optimal values of  S i,  Q j,  Q prime j, and  epsilon1 in Eq." ></td>
	<td class="line x" title="159:194	(1) were investigated using the development set within a framework of multiple linear regression modeling (Draper and Smith, 1981)." ></td>
	<td class="line x" title="160:194	Then, the correlation coefficients were investigated by using the optimal value set." ></td>
	<td class="line x" title="161:194	The results are shown in Table 6, 7, and 8." ></td>
	<td class="line x" title="162:194	In these tables, Conventional method indicates the correlation coefficients obtained when A was calculated by using only similarities S i . Conventional method (combination) is a combination of existing automatic evaluation methods from the literature." ></td>
	<td class="line x" title="163:194	Our method (automatic) indicates the correlation coefficients obtained when the results of the automatic determination of yes/no answers were used to calculate Q j and Q prime j in Eq." ></td>
	<td class="line x" title="164:194	(1)." ></td>
	<td class="line x" title="165:194	For the 19 questions for which the patterns could not be constructed, Q j was set at 0." ></td>
	<td class="line x" title="166:194	Our method (full automatic) indicates the correlation coefficients obtained when the results of the automatic sub-goal generation and determination of rate of accomplish38 Table 6: Coefficients of correlation between evaluation score A and fluency/adequacy." ></td>
	<td class="line x" title="167:194	(A reference translation is used to calculate S i .) Method fluency adequacy Development set Evaluation set Development set Evaluation set Conventional method (WER) 0.43 0.48 0.42 0.48 Conventional method (combination) 0.52 0.51 0.49 0.47 Our method (automatic) 0.90 0.59 0.89 0.62 Our method (upper bound) 0.90 0.62 0.90 0.68 Table 7: Coefficients of correlation between evaluation score A and fluency/adequacy." ></td>
	<td class="line x" title="168:194	(Three reference translations are used to calculate S i .) Method fluency adequacy Development set Evaluation set Development set Evaluation set Conventional method (WER) 0.47 0.51 0.45 0.51 Conventional method (combination) 0.54 0.54 0.51 0.52 Our method (automatic) 0.90 0.60 0.90 0.64 Our method (full automatic) 0.85 0.58 0.84 0.60 Our method (upper bound) 0.90 0.62 0.90 0.69 Table 8: Coefficients of correlation between evaluation score A and fluency/adequacy." ></td>
	<td class="line x" title="169:194	(Five reference translations are used to calculate S i .) Method fluency adequacy Development set Evaluation set Development set Evaluation set Conventional method (WER) 0.49 0.53 0.46 0.53 Conventional method (combination) 0.56 0.56 0.52 0.54 Our method (automatic) 0.90 0.60 0.90 0.63 Our method (full automatic) 0.86 0.59 0.85 0.60 Our method (upper bound) 0.91 0.63 0.90 0.69 In these tables,  indicates significance at the 5% or less significance level." ></td>
	<td class="line x" title="170:194	ment of sub-goals were used to calculate Q j and Q prime j in Eq." ></td>
	<td class="line x" title="171:194	(1)." ></td>
	<td class="line x" title="172:194	Skip word trigrams, skip word bigrams, and skip word unigrams were used for generating the sub-goals according to our preliminary experiments." ></td>
	<td class="line x" title="173:194	Our method (upper bound) indicates the correlation coefficients obtained when human judgments on the questions were used to calculate Q j and Q prime j . As shown in Table 6, 7, and 8, our methods significantly outperform the conventional methods from literature." ></td>
	<td class="line x" title="174:194	Note that WER outperformed other individual measures like BLEU and NIST in our experiments, and the combination of existing automatic evaluation methods from the literature outperformed individual lexical similarity measures by themselves in almost all cases." ></td>
	<td class="line x" title="175:194	The differences between the correlation coefficients obtained using our method and the conventional methods are statistically significant at the 5% or less significance level for fluency and adequacy, even if the number of reference translations increases, except in three cases shown in Table 7 and 8." ></td>
	<td class="line x" title="176:194	This indicates that considering the rate of accomplishment of sub-goals to automatically evaluate the quality of each translation is useful, especially when the number of reference translations is small." ></td>
	<td class="line x" title="177:194	The differences between the correlation coefficients obtained using two automatic methods are not significant." ></td>
	<td class="line x" title="178:194	These results indicate that we can reduce the development cost for constructing sub-goals." ></td>
	<td class="line x" title="179:194	However, there are still significant gaps between the correlation coefficients obtained using a fully automatic method and upper bounds." ></td>
	<td class="line x" title="180:194	These gaps indicate that we need further improvement in automatic sub-goal generation and automatic estimation of rate of accomplishment of sub-goals, which is our future work." ></td>
	<td class="line x" title="181:194	Human judgments of adequacy and fluency are known to be noisy, with varying levels of intercoder agreement." ></td>
	<td class="line x" title="182:194	Recent work has tended to apply crossjudge normalization to address this issue (Blatz et al. , 2003)." ></td>
	<td class="line x" title="183:194	We would like to evaluate against the normalized data in the future." ></td>
	<td class="line x" title="184:194	39 5 Conclusion and Future Work We demonstrated that the quality of a translated sentence can be evaluated more appropriately than by using conventional methods." ></td>
	<td class="line x" title="185:194	That was demonstrated by constructing a test set where the conditions that should be satisfied to maintain a high translation quality are assigned to each test-set sentence in the form of a question, by developing a system that determines an answer to each question, and by combining a measure based on the questions and conventional measures." ></td>
	<td class="line x" title="186:194	We also presented a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals." ></td>
	<td class="line x" title="187:194	Promising results were obtained." ></td>
	<td class="line x" title="188:194	In the near future, we would like to expand the test set to improve the upper bound obtained by our method." ></td>
	<td class="line x" title="189:194	We are also planning to expand the method and improve the accuracy of the automatic sub-goal generation and determination of the rate of accomplishment of sub-goals." ></td>
	<td class="line x" title="190:194	The sub-goals of a given sentence should be generated by considering the complexity of the sentence and the alignment information between the original source-language sentence and its translation." ></td>
	<td class="line x" title="191:194	Further advanced generation and estimation would give us information about the erroneous parts of MT results and their quality." ></td>
	<td class="line x" title="192:194	We believe that future research would allow us to develop high-quality MT systems by tuning the system parameters based on the automatic MT evaluation measures." ></td>
	<td class="line x" title="193:194	Acknowledgments The guideline for expanding the test set is based on that constructed by the Technical Research Committee of the AAMT (Asia-Pacific Association for Machine Translation) The authors would like to thank the committee members, especially, Mr. Kentaro Ogura, Ms. Miwako Shimazu, Mr. Tatsuya Sukehiro, Mr. Masaru Fuji, and Ms. Yoshiko Matsukawa for their cooperation." ></td>
	<td class="line x" title="194:194	This research is partially supported by special coordination funds for promoting science and technology." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-2049
Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization
Gupta, Surabhi;Nenkova, Ani;Jurafsky, Daniel;"></td>
	<td class="line x" title="1:91	Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193??96, Prague, June 2007." ></td>
	<td class="line x" title="2:91	c2007 Association for Computational Linguistics Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization Surabhi Gupta and Ani Nenkova and Dan Jurafsky Stanford University Stanford, CA 94305 surabhi@cs.stanford.edu, {anenkova,jurafsky}@stanford.edu Abstract The increasing complexity of summarization systems makes it difficult to analyze exactly which modules make a difference in performance." ></td>
	<td class="line x" title="3:91	We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization: raw frequency (word probability) and log-likelihood ratio." ></td>
	<td class="line x" title="4:91	We demonstrate that the advantages of log-likelihood ratio come from its known distributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input." ></td>
	<td class="line x" title="5:91	We also find that LLR is more suitable for query-focused summarization since, unlike raw frequency, it is more sensitive to the integration of the information need defined by the user." ></td>
	<td class="line x" title="6:91	1 Introduction Recently the task of multi-document summarization in response to a complex user query has received considerable attention." ></td>
	<td class="line x" title="7:91	In generic summarization, the summary is meant to give an overview of the information in the documents." ></td>
	<td class="line x" title="8:91	By contrast, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging." ></td>
	<td class="line x" title="9:91	In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario." ></td>
	<td class="line x" title="10:91	First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al. , 2006; Vanderwende et al. , 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al. , 2006; Lacatusu et al. , 2006)." ></td>
	<td class="line x" title="11:91	While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ." ></td>
	<td class="line x" title="12:91	Second, there are many little-understood aspects of the differences between generic and query-focused summarization." ></td>
	<td class="line x" title="13:91	For example, we?d like to know if a particular word weighting scheme is more suitable for focused summarization than others." ></td>
	<td class="line x" title="14:91	More significantly, previous studies show that generic and focused systems perform very similarly to each other in query-focused summarization (Nenkova, 2005) and it is of interest to find out why." ></td>
	<td class="line x" title="15:91	To address these questions we examine the two weighting schemes: raw frequency (or word probability estimated from the input), and log-likelihood ratio (LLR) and two of its variants." ></td>
	<td class="line x" title="16:91	These metrics are used to assign importance to individual content words in the input, as we discuss below." ></td>
	<td class="line x" title="17:91	Word probability R(w) = nN, where n is the number of times the word w appeared in the input and N is the total number of words in the input." ></td>
	<td class="line x" title="18:91	Log-likelihood ratio (LLR) The likelihood ratio  (Manning and Schutze, 1999) uses a background corpus to estimate the importance of a word and it is proportional to the mutual information between a word w and the input to be summarized; (w) is defined as the ratio between the probability (under a binomial distribution) of observing w in the input and the background corpus assuming equal probability of occurrence of w in both and the probability of the data assuming different probabilities for w in the input and the background corpus." ></td>
	<td class="line x" title="19:91	LLR with cut-off (LLR(C)) A useful property of the log-likelihood ratio is that the quantity 193 2 log() is asymptotically well approximated by ?2 distribution." ></td>
	<td class="line x" title="20:91	A word appears in the input significantly more often than in the background corpus when 2 log() > 10." ></td>
	<td class="line x" title="21:91	Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization." ></td>
	<td class="line x" title="22:91	Each descriptive word is assigned an equal weight and the rest of the words have a weight of zero: R(w) = 1 if ( 2 log((w)) > 10), 0 otherwise." ></td>
	<td class="line x" title="23:91	This weighting scheme has been adopted in several recent generic and topic-focused summarizers (Conroy et al. , 2006; Lacatusu et al. , 2006)." ></td>
	<td class="line x" title="24:91	LLR(CQ) The above three weighting schemes assign a weight to words regardless of the user query and are most appropriate for generic summarization." ></td>
	<td class="line x" title="25:91	When a user query is available, it should inform the summarizer to make the summary more focused." ></td>
	<td class="line x" title="26:91	In Conroy et al.(2006) such query sensititivity is achieved by augmenting LLR(C) with all content words from the user query, each assigned a weight of 1 equal to the weight of words defined by LLR(C) as topic words from the input to the summarizer." ></td>
	<td class="line x" title="28:91	2 Data We used the data from the 2005 Document Understanding Conference (DUC) for our experiments." ></td>
	<td class="line x" title="29:91	The task is to produce a 250-word summary in response to a topic defined by a user for a total of 50 topics with approximately 25 documents for each marked as relevant by the topic creator." ></td>
	<td class="line x" title="30:91	In computing LLR, the remaining 49 topics were used as a background corpus as is often done by DUC participants." ></td>
	<td class="line x" title="31:91	A sample topic (d301) shows the complexity of the queries: Identify and describe types of organized crime that crosses borders or involves more than one country." ></td>
	<td class="line x" title="32:91	Name the countries involved." ></td>
	<td class="line x" title="33:91	Also identify the perpetrators involved with each type of crime, including both individuals and organizations if possible." ></td>
	<td class="line x" title="34:91	3 The Experiment In the summarizers we compare here, the various weighting methods we describe above are used to assign importance to individual content words in the input." ></td>
	<td class="line o" title="35:91	The weight or importance of a sentence S in GENERIC FOCUSED Frequency 0.11972 0.11795 (0.11168??.12735) (0.11010??.12521) LLR 0.11223 0.11600 (0.10627??.11873) (0.10915??.12281) LLR(C) 0.11949 0.12201 (0.11249??.12724) (0.11507??.12950) LLR(CQ) not app 0.12546 (.11884??13247) Table 1: SU4 ROUGE recall (and 95% confidence intervals) for runs on the entire input (GENERIC) and on relevant sentences (FOCUSED)." ></td>
	<td class="line x" title="36:91	the input is defined as WeightR(S) = summationdisplay w?S R(w) (1) where R(w) assigns a weight for each word w. For GENERIC summarization, the top scoring sentences in the input are taken to form a generic extractive summary." ></td>
	<td class="line x" title="37:91	In the computation of sentence importance, only nouns, verbs, adjectives and adverbs are considered and a short list of light verbs are excluded: ?has, was, have, are, will, were, do, been, say, said, says??" ></td>
	<td class="line x" title="38:91	For FOCUSED summarization, we modify this algorithm merely by running the sentence selection algorithm on only those sentences in the input that are relevent to the user query." ></td>
	<td class="line x" title="39:91	In some previous DUC evaluations, relevant sentences are explicitly marked by annotators and given to systems." ></td>
	<td class="line x" title="40:91	In our version here, a sentence in the input is considered relevant if it contains at least one word from the user query." ></td>
	<td class="line oc" title="41:91	For evaluation we use ROUGE (Lin, 2004) SU4 recall metric1, which was among the official automatic evaluation metrics for DUC." ></td>
	<td class="line x" title="42:91	4 Results The results are shown in Table 1." ></td>
	<td class="line x" title="43:91	The focused summarizer using LLR(CQ) is the best, and it significantly outperforms the focused summarizer based on frequency." ></td>
	<td class="line x" title="44:91	Also, LLR (using log-likelihood ratio to assign weights to all words) perfroms significantly worse than LLR(C)." ></td>
	<td class="line x" title="45:91	We can observe some trends even from the results for which there is no significance." ></td>
	<td class="line x" title="46:91	Both LLR and LLR(C) are sensitive to the introduction of topic relevance, producing somewhat better summaries in the FOCUSED scenario 1-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d 194 compared to the GENERIC scenario." ></td>
	<td class="line x" title="47:91	This is not the case for the frequency summarizer, where using only the relevant sentences has a negative impact." ></td>
	<td class="line x" title="48:91	4.1 Focused summarization: do we need query expansion?" ></td>
	<td class="line x" title="49:91	In the FOCUSED condition there was little (for LLR weighting) or no (for frequency) improvement over GENERIC." ></td>
	<td class="line x" title="50:91	One possible explanation for the lack of clear improvement in the FOCUSED setting is that there are not enough relevant sentences, making it impossible to get stable estimates of word importance." ></td>
	<td class="line x" title="51:91	Alternatively, it could be the case that many of the sentences are relevant, so estimates from the relevant portion of the input are about the same as those from the entire input." ></td>
	<td class="line x" title="52:91	To distinguish between these two hypotheses, we conducted an oracle experiment." ></td>
	<td class="line x" title="53:91	We modified the FOCUSED condition by expanding the topic words from the user query with all content words from any of the human-written summaries for the topic." ></td>
	<td class="line x" title="54:91	This increases the number of relevant sentences for each topic." ></td>
	<td class="line o" title="55:91	No automatic method for query expansion can be expected to give more accurate results, since the content of the human summaries is a direct indication of what information in the input was important and relevant and, moreover, the ROUGE evaluation metric is based on direct n-gram comparison with these human summaries." ></td>
	<td class="line x" title="56:91	Even under these conditions there was no significant improvement for the summarizers, each getting better by 0.002: the frequency summarizer gets R-SU4 of 0.12048 and the LLR(CQ) summarizer achieves R-SU4 of 0.12717." ></td>
	<td class="line x" title="57:91	These results seem to suggest that considering the content words in the user topic results in enough relevant sentences." ></td>
	<td class="line x" title="58:91	Indeed, Table 2 shows the minimum, maximum and average percentage of relevant sentences in the input (containing at least one content words from the user the query), both as defined by the original query and by the oracle query expansion." ></td>
	<td class="line x" title="59:91	It is clear from the table that, on average, over half of the input comprises sentences that are relevant to the user topic." ></td>
	<td class="line x" title="60:91	Oracle query expansion makes the number of relevant sentences almost equivalent to the input size and it is thus not surprising that the corresponding results for content selection are nearly identical to the query independent Original query Oracle query expansion Min 13% 52% Average 57% 86% Max 82% 98% Table 2: Percentage of relevant sentences (containing words from the user query) in the input." ></td>
	<td class="line x" title="61:91	The oracle query expansion considers all content words form human summaries of the input as query words." ></td>
	<td class="line x" title="62:91	runs of generic summaries for the entire input." ></td>
	<td class="line x" title="63:91	These numbers indictate that rather than finding ways for query expansion, it might instead be more important to find techniques for constraining the query, determining which parts of the input are directly related to the user questions." ></td>
	<td class="line x" title="64:91	Such techniques have been described in the recent multi-strategy approach of Lacatusu et al.(2006) for example, where one of the strategies breaks down the user topic into smaller questions that are answered using robust question-answering techniques." ></td>
	<td class="line x" title="66:91	4.2 Why is log-likelihood ratio better than frequency?" ></td>
	<td class="line x" title="67:91	Frequency and log-likelihood ratio weighting for content words produce similar results when applied to rank all words in the input, while the cut-off for topicality in LLR(C) does have a positive impact on content selection." ></td>
	<td class="line x" title="68:91	A closer look at the two weighting schemes confirms that when cut-off is not used, similar weighting of content words is produced." ></td>
	<td class="line x" title="69:91	The Spearman correlation coefficient between the weights for words assigned by the two schemes is on average 0.64." ></td>
	<td class="line x" title="70:91	At the same time, it is likely that the weights of sentences are dominated by only the top most highly weighted words." ></td>
	<td class="line x" title="71:91	In order to see to what extent the two schemes identify the same or different words as the most important ones, we computed the overlap between the 250 most highly weighted words according to LLR and frequency." ></td>
	<td class="line x" title="72:91	The average overlap across the 50 sets was quite large, 70%." ></td>
	<td class="line x" title="73:91	To illustrate the degree of overlap, we list below are the most highly weighted words according to each weighting scheme for our sample topic concerning crimes across borders." ></td>
	<td class="line x" title="74:91	LLR drug, cocaine, traffickers, cartel, police, crime, enforcement, u.s., smuggling, trafficking, arrested, government, seized, year, drugs, organised, heroin, criminal, cartels, last, 195 official, country, law, border, kilos, arrest, more, mexican, laundering, officials, money, accounts, charges, authorities, corruption, anti-drug, international, banks, operations, seizures, federal, italian, smugglers, dealers, narcotics, criminals, tons, most, planes, customs Frequency drug, cocaine, officials, police, more, last, government, year, cartel, traffickers, u.s., other, drugs, enforcement, crime, money, country, arrested, federal, most, now, trafficking, seized, law, years, new, charges, smuggling, being, official, organised, international, former, authorities, only, criminal, border, people, countries, state, world, trade, first, mexican, many, accounts, according, bank, heroin, cartels It becomes clear that the advantage of likelihood ratio as a weighting scheme does not come from major differences in overall weights it assigns to words compared to frequency." ></td>
	<td class="line x" title="75:91	It is the significance cut-off for the likelihood ratio that leads to noticeable improvement (see Table 1)." ></td>
	<td class="line x" title="76:91	When this weighting scheme is augmented by adding a score of 1 for content words that appear in the user topic, the summaries improve even further (LLR(CQ))." ></td>
	<td class="line x" title="77:91	Half of the improvement can be attributed to the cut-off (LLR(C)), and the other half to focusing the summary using the information from the user query (LLR(CQ))." ></td>
	<td class="line x" title="78:91	The advantage of likelihood ratio comes from its providing a principled criterion for deciding which words are truly descriptive of the input and which are not." ></td>
	<td class="line x" title="79:91	Raw frequency provides no such cut-off." ></td>
	<td class="line x" title="80:91	5 Conclusions In this paper we examined two weighting schemes for estimating word importance that have been successfully used in current systems but have not todate been directly compared." ></td>
	<td class="line x" title="81:91	Our analysis confirmed that log-likelihood ratio leads to better results, but not because it defines a more accurate assignment of importance than raw frequency." ></td>
	<td class="line x" title="82:91	Rather, its power comes from the use of a known distribution that makes it possible to determine which words are truly descriptive of the input." ></td>
	<td class="line x" title="83:91	Only when such words are viewed as equally important in defining the topic does this weighting scheme show improved performance." ></td>
	<td class="line x" title="84:91	Using the significance cut-off and considering all words above it equally important is key." ></td>
	<td class="line x" title="85:91	Log-likelihood ratio summarizer is more sensitive to topicality or relevance and produces summaries that are better when it take the user request into account than when it does not." ></td>
	<td class="line x" title="86:91	This is not the case for a summarizer based on frequency." ></td>
	<td class="line x" title="87:91	At the same time it is noteworthy that the generic summarizers perform about as well as their focused counterparts." ></td>
	<td class="line x" title="88:91	This may be related to our discovery that on average 57% of the sentences in the document are relevant and that ideal query expansion leads to a situation in which almost all sentences in the input become relevant." ></td>
	<td class="line x" title="89:91	These facts could be an unplanned side-effect from the way the test topics were produced: annotators might have been influenced by information in the input to be summarizied when defining their topic." ></td>
	<td class="line x" title="90:91	Such observations also suggest that a competitive generic summarizer would be an appropriate baseline for the topicfocused task in future DUCs." ></td>
	<td class="line x" title="91:91	In addition, including some irrelavant documents in the input might make the task more challenging and allow more room for advances in query expansion and other summary focusing techniques." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-1411
A Perspective-Based Approach for Solving Textual Entailment Recognition
Ferrandez, Oscar;Micol, Daniel;Rafael, MuÃ±;Palomar, Manuel;"></td>
	<td class="line x" title="1:142	Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 66??1, Prague, June 2007." ></td>
	<td class="line x" title="2:142	c2007 Association for Computational Linguistics A Perspective-Based Approach for Solving Textual Entailment Recognition Oscar Ferrandez, Daniel Micol, Rafael Mu?noz, and Manuel Palomar Natural Language Processing and Information Systems Group Department of Computing Languages and Systems University of Alicante San Vicente del Raspeig, Alicante 03690, Spain {ofe, dmicol, rafael, mpalomar}@dlsi.ua.es Abstract The textual entailment recognition system that we discuss in this paper represents a perspective-based approach composed of two modules that analyze text-hypothesis pairs from a strictly lexical and syntactic perspectives, respectively." ></td>
	<td class="line x" title="3:142	We attempt to prove that the textual entailment recognition task can be overcome by performing individual analysis that acknowledges us of the maximum amount of information that each single perspective can provide." ></td>
	<td class="line x" title="4:142	We compare this approach with the system we presented inthepreviouseditionofPASCALRecognising Textual Entailment Challenge, obtaining an accuracy rate 17.98% higher." ></td>
	<td class="line x" title="5:142	1 Introduction Textual entailment recognition has become a popular Natural Language Processing task within the last few years." ></td>
	<td class="line x" title="6:142	It consists in determining whether one text snippet (hypothesis) entails another one (text) (Glickman, 2005)." ></td>
	<td class="line x" title="7:142	To overcome this problem several approaches have been studied, being the Recognising Textual Entailment Challenge (RTE) (BarHaim et al. , 2006; Dagan et al. , 2006) the most referred source for determining which one is the most accurate." ></td>
	<td class="line x" title="8:142	Many of the participating groups in previous editions of RTE, including ourselves (Ferrandez et al. , 2006), designed systems that combined a variety of lexical, syntactic and semantic techniques." ></td>
	<td class="line x" title="9:142	In our contribution to RTE-3 we attempt to solve the textual entailment recognition task by analyzing two different perspectives separately, in order to acknowledge the amount of information that an individual perspective can provide." ></td>
	<td class="line x" title="10:142	Later on, we combine both modules to obtain the highest possible accuracy rate." ></td>
	<td class="line x" title="11:142	For this purpose, we analyze the provided corpora by using a lexical module, namely DLSITE-1, and a syntactic one, namely DLSITE-2." ></td>
	<td class="line x" title="12:142	Once all results have been obtained we perform a voting process in order to take into account all system?s judgments." ></td>
	<td class="line x" title="13:142	The remainder of this paper is structured as follows." ></td>
	<td class="line x" title="14:142	Section two describes the system we have built, providing details of the lexical and syntactic perspectives, and explains the difference with the one we presented in RTE-2." ></td>
	<td class="line x" title="15:142	Third section presents the experimental results, and the fourth one provides our conclusions and describes possible future work." ></td>
	<td class="line x" title="16:142	2 System Specification Thissectiondescribesthesystemwehavedeveloped in order to participate in RTE-3." ></td>
	<td class="line x" title="17:142	It is based on surface techniques of lexical and syntactic analysis." ></td>
	<td class="line x" title="18:142	As the starting point we have used our previous system presented in the second edition of the RTE Challenge (Ferrandez et al. , 2006)." ></td>
	<td class="line x" title="19:142	We have enriched it with two independent modules that are intended to detect some misinterpretations performed by this system." ></td>
	<td class="line x" title="20:142	Moreover, these new modules can also recognize entailment relations by themselves." ></td>
	<td class="line x" title="21:142	The performance of each separate module and their combination with our previous system will be detailed in section three." ></td>
	<td class="line x" title="22:142	Next, Figure 1 represents a schematic view of the system we have developed." ></td>
	<td class="line x" title="23:142	66 Figure 1: System architecture." ></td>
	<td class="line x" title="24:142	As we can see in the previous Figure, our system is composed of three modules that are coordinated by an input scheduler." ></td>
	<td class="line x" title="25:142	Its commitment is to provide the text-hypothesis pairs to each module in order to extract their corresponding similarity rates." ></td>
	<td class="line x" title="26:142	Once all rates for a given text-hypothesis pair have been calculated, they will be processed by an output gatherer that will provide the final judgment." ></td>
	<td class="line x" title="27:142	The method used to calculate the final entailment decision consists in combining the outputs of both lexical and syntactic modules, and these outputs with our RTE-2 system?s judgment." ></td>
	<td class="line x" title="28:142	The output gatherer will be detailed later in this paper when we describe the experimental results." ></td>
	<td class="line x" title="29:142	2.1 RTE-2 System Theapproachwepresentedinthepreviouseditionof RTE attempts to recognize textual entailment by determining whether the text and the hypothesis are related using their respective derived logic forms, and by finding relations between their predicates using WordNet (Miller et al. , 1990)." ></td>
	<td class="line x" title="30:142	These relations have a specific weight that provide us a score representing the similarity of the derived logic forms and determining whether they are related or not." ></td>
	<td class="line x" title="31:142	For our participation in RTE-3 we decided to apply our previous system because it allows us to handle some kinds of information that are not correctly managed by the new approaches developed for the current RTE edition." ></td>
	<td class="line x" title="32:142	2.2 Lexical Module This method relies on the computation of a wide variety of lexical measures, which basically consists of overlap metrics." ></td>
	<td class="line x" title="33:142	Although in other related work this kind of metrics have already been used (Nicholson et al. , 2006), the main contribution of this module is the fact that it only deals with lexical features without taking into account any syntactic nor semantic information." ></td>
	<td class="line x" title="34:142	The following paragraphs list the considered lexical measures." ></td>
	<td class="line x" title="35:142	Simple matching: initialized to zero." ></td>
	<td class="line x" title="36:142	A boolean value is set to one if the hypothesis word appears in the text." ></td>
	<td class="line x" title="37:142	The final weight is calculated as the sum of all boolean values and normalized dividing it by the length of the hypothesis." ></td>
	<td class="line x" title="38:142	Levenshtein distance: it is similar to simple matching." ></td>
	<td class="line x" title="39:142	However, in this case we use the mentioned distance as the similarity measure between words." ></td>
	<td class="line x" title="40:142	When the distance is zero, the increment value is one." ></td>
	<td class="line x" title="41:142	On the other hand, if such value is equal to one, theincrementis0.9." ></td>
	<td class="line x" title="42:142	Otherwise, itwillbetheinverse of the obtained distance." ></td>
	<td class="line x" title="43:142	Consecutive subsequence matching: this measure assigns the highest relevance to the appearance of consecutive subsequences." ></td>
	<td class="line x" title="44:142	In order to perform this, we have generated all possible sets of consecutive subsequences, from length two until the length in words, from the text and the hypothesis." ></td>
	<td class="line x" title="45:142	If we proceed as mentioned, the sets of length two extracted from the hypothesis will be compared to the sets of the same length from the text." ></td>
	<td class="line x" title="46:142	If the same element is present in both the text and the hypothesis set, then a unit is added to the accumulated weight." ></td>
	<td class="line x" title="47:142	This procedure is applied for all sets of different length extracted from the hypothesis." ></td>
	<td class="line x" title="48:142	Finally, the sum of the weight obtained from each set of a specific length is normalized by the number of sets corresponding to 67 this length, and the final accumulated weight is also normalized by the length of the hypothesis in words minus one." ></td>
	<td class="line x" title="49:142	This measure is defined as follows: CSmatch = |H|summationdisplay i=2 f(SHi) |H| ??1 (1) where SHi contains the hypothesis??subsequences of lengthi, andf(SHi) is defined as follows: f(SHi) = summationdisplay j?SHi match(j) |H| ?i+ 1 (2) being match(j) equal to one if there exists an elementkthat belongs to the set that contains the text?s subsequences of lengthi, such thatk = j. One should note that this measure does not consider non-consecutive subsequences." ></td>
	<td class="line x" title="50:142	In addition, it assigns the same relevance to all consecutive subsequences with the same length." ></td>
	<td class="line x" title="51:142	Furthermore, the longer the subsequence is, the more relevant it will be considered." ></td>
	<td class="line x" title="52:142	Tri-grams: two sets containing tri-grams of letters belonging to the text and the hypothesis were created." ></td>
	<td class="line x" title="53:142	All the occurrences in the hypothesis??trigrams set that also appear in the text?s will increase the accumulated weight in a factor of one unit." ></td>
	<td class="line x" title="54:142	The weight is normalized by the size of the hypothesis??" ></td>
	<td class="line x" title="55:142	tri-grams set." ></td>
	<td class="line p" title="56:142	ROUGE measures: considering the impact of ngram overlap metrics in textual entailment, we believethattheideaofintegratingthesemeasures1 into our system is very appealing." ></td>
	<td class="line oc" title="57:142	We have implemented them as defined in (Lin, 2004)." ></td>
	<td class="line o" title="58:142	Eachmeasureisappliedtothewords, lemmasand stems belonging to the text-hypothesis pair." ></td>
	<td class="line o" title="59:142	Within the entire set of measures, each one of them is considered as a feature for the training and test stages of a machine learning algorithm." ></td>
	<td class="line x" title="60:142	The selected one was a Support Vector Machine due to the fact that its properties are suitable for recognizing entailment." ></td>
	<td class="line o" title="61:142	2.3 Syntactic Module The syntactic module we have built is composed of few submodules that operate collaboratively in order 1The considered measures were ROUGE-N with n=2 and n=3, ROUGE-L, ROUGE-W and ROUGE-S with s=2 and s=3." ></td>
	<td class="line x" title="62:142	toobtainthehighestpossibleaccuracybyusingonly syntactic information." ></td>
	<td class="line x" title="63:142	The commitment of the first two submodules is to generate an internal representation of the syntactic dependency trees generated by MINIPAR (Lin, 1998)." ></td>
	<td class="line x" title="64:142	For this purpose we obtain the output of such parser for the text-hypothesis pairs, and then process it to generate an on-memory internal representation of the mentioned trees." ></td>
	<td class="line x" title="65:142	In order to reduce our system?s noise and increase its accuracy rate, we only keep the relevant words and discard the ones that we believe do not provide useful information, such as determinants and auxiliary verbs." ></td>
	<td class="line x" title="66:142	After this step has been performed we can proceed to compare the generated syntactic dependency trees of the text and the hypothesis." ></td>
	<td class="line x" title="67:142	The graph node matching, termed alignment, between both the text and the hypothesis consists in finding pairs of words in both trees whose lemmas are identical, no matter whether they are in the same position within the tree." ></td>
	<td class="line x" title="68:142	Some authors have already designed similar matching techniques, such as the onedescribed in(Snow etal., 2006)." ></td>
	<td class="line x" title="70:142	However, these include semantic constraints that we have decided nottoconsider." ></td>
	<td class="line x" title="71:142	Thereasonofthisdecisionisthatwe desired to overcome the textual entailment recognition from an exclusively syntactic perspective." ></td>
	<td class="line x" title="72:142	The formula that provides the similarity rate between the dependency trees of the text and the hypothesis in our system, denoted by the symbol ?, is shown in Equation 3: ?(?,) = summationdisplay ?? ?() (3) where ? and  represent the text?s and hypothesis??" ></td>
	<td class="line x" title="73:142	syntactic dependency trees, respectively, andis the set that contains all synsets present in both trees, being = ? ??? ???, ??." ></td>
	<td class="line x" title="74:142	As we can observe in Equation 3,?" ></td>
	<td class="line x" title="75:142	depends on another function, denoted by the symbol ?, which provides the relevance of a synset." ></td>
	<td class="line x" title="76:142	Such a weight factor will depend on the grammatical category and relation of the synset." ></td>
	<td class="line x" title="77:142	In addition, we believe that the most relevant words of a phrase occupy the highest positions in the dependency tree, so we desired to assign different weights depending on the depth of the synset." ></td>
	<td class="line x" title="78:142	With all these factors we define the relevance of a word as shown 68 in Equation 4: ?() =  ???(4) where  is a synset present in both ? and ,  represents the weight assigned to ?s grammatical category (Table 1), ? the weight of ?s grammatical relationship (Table 2),  an empirically calculated value that represents the weight difference between treelevels, and thedepthofthenodethatcontains the synset in." ></td>
	<td class="line x" title="79:142	The performed experiments reveal that the optimal value foris 1.1." ></td>
	<td class="line x" title="80:142	Grammatical category Weight Verbs, verbs with one argument, verbs with two arguments, verbs taking clause as complement 1.0 Nouns, numbers 0.75 Be used as a linking verb 0.7 Adjectives, adverbs, noun-noun modifiers 0.5 Verbs Have and Be 0.3 Table 1: Weights assigned to the relevant grammatical categories." ></td>
	<td class="line x" title="81:142	Grammatical relationship Weight Subject of verbs, surface subject, object of verbs, second object of ditransitive verbs 1.0 The rest 0.5 Table 2: Weights assigned to the grammatical relationships." ></td>
	<td class="line x" title="82:142	We would like to point out that a requirement of our system?s similarity measure is to be independent of the hypothesis length." ></td>
	<td class="line x" title="83:142	Therefore, we must define the normalized similarity rate, as represented in Equation 5: ?(?,) = summationdisplay ?? ?() summationdisplay ?? ?() (5) Once the similarity value has been calculated, it will be provided to the user together with the corresponding text-hypothesis pair identifier." ></td>
	<td class="line x" title="84:142	It will be his responsibility to choose an appropriate threshold that will represent the minimum similarity rate to be considered as entailment between text and hypothesis." ></td>
	<td class="line x" title="85:142	All values that are under such a threshold will be marked as not entailed." ></td>
	<td class="line x" title="86:142	3 System Evaluation In order to evaluate our system we have generated several results using different combinations of all three mentioned modules." ></td>
	<td class="line x" title="87:142	Since the lexical one uses a machine learning algorithm, it has to be run within a training environment." ></td>
	<td class="line x" title="88:142	For this purpose we have trained our system with the corpora provided in the previous editions of RTE, and also with the development corpus from the current RTE-3 challenge." ></td>
	<td class="line x" title="89:142	On the other hand, for the remainder modules the development corpora was used to set the thresholds that determine if the entailment holds." ></td>
	<td class="line x" title="90:142	The performed tests have been obtained by performing different combinations of the described modules." ></td>
	<td class="line x" title="91:142	First, we have calculated the accuracy rates using only each single module separately." ></td>
	<td class="line x" title="92:142	Later on we have combined those developed by our research group for this year?s RTE challenge, which are DLSITE-1 (the lexical one) and DLSITE-2 (the syntactic one)." ></td>
	<td class="line x" title="93:142	Finally we have performed a voting process between these two systems and the one we presented in RTE-2." ></td>
	<td class="line x" title="94:142	The combination of DLSITE-1 and DLSITE-2 is describedasfollows." ></td>
	<td class="line x" title="95:142	Ifbothmodulesagree, thenthe judgement is straightforward, but if they do not, we then decide the judgment depending on the accuracy of each one for true and false entailment situations." ></td>
	<td class="line x" title="96:142	In our case, DLSITE-1 performs better while dealing with negative examples, so its decision will prevail overtherest." ></td>
	<td class="line x" title="97:142	Regardingthecombinationofthethree approaches, we have developed a voting strategy." ></td>
	<td class="line x" title="98:142	The results obtained by our system are represented in Table 3." ></td>
	<td class="line x" title="99:142	As it is reflected in such table, the highest accuracy rate obtained using the RTE-3 test corpus was achieved applying only the lexical module, namely DLSITE-1." ></td>
	<td class="line x" title="100:142	On the other hand, the syntactic one had a significantly lower rate, and the same happened with the system we presented in RTE-2." ></td>
	<td class="line x" title="101:142	Therefore, a combination of them will most likely produce less accurate results than the lexical module, as it is shown in Table 3." ></td>
	<td class="line x" title="102:142	However, we would like to point out that these results depend heavily on the corpus idiosyncrasy." ></td>
	<td class="line x" title="103:142	This can be proven with the results obtained for the RTE-2 test corpus, where the grouping of the three modules provided the highest accuracy rates of all possible combinations." ></td>
	<td class="line x" title="104:142	69 RTE-2 test RTE-3 dev RTE-3 test Overall Overall Overall IE IR QA SUM RTE-2 system 0.5563 0.5523 0.5400 0.4900 0.6050 0.5100 0.5550 DLSITE-1 0.6188 0.7012 0.6563 0.5150 0.7350 0.7950 0.5800 DLSITE-2 0.6075 0.6450 0.5925 0.5050 0.6350 0.6300 0.6000 DLSITE-1&2 0.6212 0.6900 0.6375 0.5150 0.7150 0.7400 0.5800 Voting 0.6300 0.6900 0.6375 0.5250 0.7050 0.7200 0.6000 Table 3: Results obtained with the corpora from RTE-2 and RTE-3." ></td>
	<td class="line x" title="105:142	3.1 Results Analysis We will now perform an analysis of the results shown in the previous section." ></td>
	<td class="line x" title="106:142	First, we would like to mention the fact that our system does not behave correctly when it has to deal with long texts." ></td>
	<td class="line x" title="107:142	Roughly 11% and 13% of the false positives of DLSITE-1 and DLSITE-2, respectively, are caused by misinterpretations of long texts." ></td>
	<td class="line x" title="108:142	The underlying reason of these failures is the fact that it is easier to find a lexical and syntactic match when a long text is present in the pair, even if there is not entailment." ></td>
	<td class="line x" title="109:142	In addition, we consider very appealing to show the accuracy rates corresponding to true and false entailmentpairsindividually." ></td>
	<td class="line x" title="110:142	Figure2representsthe mentioned rates for all system combinations that we displayed in Table 3." ></td>
	<td class="line x" title="111:142	Figure 2: Accuracy rates obtained for true and false entailments using the RTE-3 test corpus." ></td>
	<td class="line x" title="112:142	As we can see in Figure 2, the accuracy rates for true and false entailment pairs vary significantly." ></td>
	<td class="line x" title="113:142	The modules we built for our participation in RTE-3 obtainedhighaccuracyratesfortrueentailmenttexthypothesis pairs, but in contrast they behaved worse in detecting false entailment pairs." ></td>
	<td class="line x" title="114:142	This is the oppositetothesystemwepresentedinRTE-2, sinceithas a much higher accuracy rate for false cases than true ones." ></td>
	<td class="line x" title="115:142	WhenwecombinedDLSITE-1andDLSITE-2, their accuracy rate for true entailments diminished, although, on the other hand, the rate for false ones raised." ></td>
	<td class="line x" title="116:142	The voting between all three modules providedahigheraccuracyrateforfalseentailmentsbecause the system we presented at RTE-2 performed well in these cases." ></td>
	<td class="line x" title="117:142	Finally, we would like to discuss some examples that lead to failures and correct forecasts by our two new approaches." ></td>
	<td class="line x" title="118:142	Pair 246 entailment=YES task=IR T: Overall the accident rate worldwide for commercial aviation has been falling fairly dramatically especially during the period between 1950 and 1970, largely due to the introduction of new technology during this period." ></td>
	<td class="line x" title="119:142	H: Airplane accidents are decreasing." ></td>
	<td class="line x" title="120:142	Pair 246 is incorrectly classified by DLSITE-1 due to the fact that some words of the hypothesis do not appear in the same manner in the text, although they have similar meaning (e.g. airplane and aviation)." ></td>
	<td class="line x" title="121:142	However, DLSITE-2 is able to establish a true entailment for this pair, since the hypothesis??" ></td>
	<td class="line x" title="122:142	syntactic dependency tree can be matched within the text?s, and the similarity measure applied between lemmas obtains a high score." ></td>
	<td class="line x" title="123:142	This fact produces that, in this case, the voting also achieves a correct prediction for pair 246." ></td>
	<td class="line x" title="124:142	Pair 736 entailment=YES task=SUM T: In a security fraud case, Michael Milken was sentenced to 10 years in prison." ></td>
	<td class="line x" title="125:142	H: Milken was imprisoned for security fraud." ></td>
	<td class="line x" title="126:142	Pair 736 is correctly classified by DLSITE-1 since there are matches for all hypothesis??words (except imprisoned) and some subsequences." ></td>
	<td class="line x" title="127:142	In contrast, DLSITE-2 does not behave correctly with this example because the main verbs do not match, being this fact a considerable handicap for the overall score." ></td>
	<td class="line x" title="128:142	70 4 Conclusions and Future Work This research provides independent approaches considering mainly lexical and syntactic information." ></td>
	<td class="line x" title="129:142	In order to achieve this, we expose and analyze a wide varietyoflexicalmeasuresaswellassyntacticstructurecomparisonsthatattempttosolvethetextualentailment recognition task." ></td>
	<td class="line x" title="130:142	In addition, we propose several combinations between these two approaches and integrate them with our previous RTE-2 system by using a voting strategy." ></td>
	<td class="line x" title="131:142	The results obtained reveal that, although the combined approach provided the highest accuracy rates for the RTE-2 corpora, it has not accomplished the expected reliability in the RTE-3 challenge." ></td>
	<td class="line x" title="132:142	Nevertheless, in both cases the lexical-based moduleachievedbetterresultsthantherestoftheindividual approaches, being the optimal for our participation in RTE-3, and obtaining an accuracy rate of about 70% and 65% for the development and test corpus, respectively." ></td>
	<td class="line x" title="133:142	One should note that these results depend on the idiosyncrasies of the RTE corpora." ></td>
	<td class="line x" title="134:142	However, these corpora are the most reliable ones for evaluating textual entailment recognizers." ></td>
	<td class="line x" title="135:142	Future work can be related to the development of a semantic module." ></td>
	<td class="line x" title="136:142	Our system achieves good lexical and syntactic comparisons between texts, but we believe that we should take advantage of the semantic resources in order to achieve higher accuracy rates." ></td>
	<td class="line x" title="137:142	For this purpose we plan to build a module that constructs characterized representations based on the text using named entities and role labeling in order to extract semantic information from a texthypothesis pair." ></td>
	<td class="line x" title="138:142	Another future research line could consist in applying different recognition techniques depending on the type of entailment task." ></td>
	<td class="line x" title="139:142	We have noticed that the accuracy of our approach differs when the entailment is produced mainly by lexical or syntactic implications." ></td>
	<td class="line x" title="140:142	We intend to establish an entailment typology and tackle each type by means of different points of view or approaches." ></td>
	<td class="line x" title="141:142	Acknowledgments This research has been partially funded by the QALL-ME consortium, which is a 6th Framework Research Programme of the European Union (EU), contract number FP6-IST-033860 and by the Spanish Government under the project CICyT number TIN2006-1526-C06-01." ></td>
	<td class="line x" title="142:142	It has also been supported by the undergraduate research fellowships financed by the Spanish Ministry of Education and Science, and the project ACOM06/90 financed by the Spanish Generalitat Valenciana." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1019
Mind the Gap: Dangers of Divorcing Evaluations of Summary Content from Linguistic Quality
Conroy, John M.;Dang, Hoa Trang;"></td>
	<td class="line x" title="1:164	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 145152 Manchester, August 2008 Mind the Gap: Dangers of Divorcing Evaluations of Summary Content from Linguistic Quality John M. Conroy IDA/Center for Computing Sciences Bowie, Maryland, USA conroy@super.org Hoa Trang Dang Information Access Division National Institute of Standards and Technology Gaithersburg, Maryland, USA hoa.dang@nist.gov Abstract In this paper, we analyze the state of current human and automatic evaluation of topic-focused summarization in the Document Understanding Conference main task for 2005-2007." ></td>
	<td class="line n" title="2:164	The analyses show that while ROUGE has very strong correlation with responsiveness for both human and automatic summaries, there is a significant gap in responsiveness between humans and systems which is not accounted for by the ROUGE metrics." ></td>
	<td class="line x" title="3:164	In addition to teasing out gaps in the current automatic evaluation, we propose a method to maximize the strength of current automatic evaluations by using the method of canonical correlation." ></td>
	<td class="line o" title="4:164	We apply this new evaluation method, which we call ROSE (ROUGE Optimal Summarization Evaluation), to find the optimal linear combination of ROUGE scores to maximize correlation with human responsiveness." ></td>
	<td class="line oc" title="5:164	1 Introduction ROUGE (Lin, 2004) and its linguisticallymotivated descendent, Basic Elements (BE) (Hovy et al., 2005), evaluate a summary by computing its overlap with a set of model (human) summaries; ROUGE considers lexical n-grams as the unit for comparing the overlap between summaries, while Basic Elements uses larger units of comparison based on the output of syntactic parsers." ></td>
	<td class="line p" title="6:164	The ROUGE/BE toolkit has become the standard automatic method for evaluating the content of c2008." ></td>
	<td class="line x" title="7:164	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="8:164	Some rights reserved." ></td>
	<td class="line x" title="9:164	machine-generated summaries, but the correlation of these automatic scores with human evaluation metrics has not always been consistent." ></td>
	<td class="line x" title="10:164	In this paper, we analyze the state of current human and automatic evaluation of topic-focused summarization." ></td>
	<td class="line o" title="11:164	Using the results of the Document Understanding Conference main task for 20052007 we explore the correlation between variants of ROUGE and the human metrics of responsiveness and linguistic quality." ></td>
	<td class="line x" title="12:164	The analyses expose a number of challenges and several surprising results." ></td>
	<td class="line n" title="13:164	In particular, while ROUGE has very strong correlation with responsiveness for both human and system summaries, there is a significant gap in responsiveness between humans and systems which is not accounted for by the ROUGE metrics." ></td>
	<td class="line n" title="14:164	One cause of the gap is that many automatic summarizers truncate the last sentence of their summary, which shows significant reduction in the responsiveness score but does not result in a statistically significant drop in ROUGE scores." ></td>
	<td class="line x" title="15:164	In addition to teasing out gaps in the current automatic evaluation, we propose a method to maximize the strength of current automatic evaluations by using the method of canonical correlation." ></td>
	<td class="line o" title="16:164	We apply this new evaluation method, which we call ROSE (ROUGE Optimal Summarization Evaluation), to find the optimal linear combination of ROUGE metrics to maximize correlation with human responsiveness." ></td>
	<td class="line x" title="17:164	2 DUC 2005-2007 Task and Evaluation The main task for DUC 2005-2007 was a complex question-focused summarization task that required summarizers to piece together information from multiple documents to answer a question or set of questions as posed in a DUC topic statement." ></td>
	<td class="line x" title="18:164	The topic statement was a request for infor145 mationthatcouldnotbemetbyjuststatinganame, date,quantity,etc. Thesummarizationtaskwasthe same for both human and automatic summarizers: Given a topic statement and a set of 25-50 relevant newswire documents, the summarization task was to create from the documents a brief, wellorganized, fluent summary that answered the need for information expressed in the topic statement." ></td>
	<td class="line x" title="19:164	The summary could be no longer than 250 words." ></td>
	<td class="line x" title="20:164	Summaries over the size limit were truncated, and no bonus was given for creating a shorter summary." ></td>
	<td class="line x" title="21:164	NIST Assessors developed the DUC topics used as test data." ></td>
	<td class="line x" title="22:164	There were 50 DUC topics each year in 2005-2006, and 45 topics in DUC 2007." ></td>
	<td class="line x" title="23:164	Each year, 10 NIST assessors produced a total of 4 human summaries for each of the topics." ></td>
	<td class="line x" title="24:164	The assessor who developed a particular topic always wrote one of the 4 summaries for that topic." ></td>
	<td class="line x" title="25:164	NIST manually assessed each summary for both content and readability." ></td>
	<td class="line x" title="26:164	Readability was assessed using a set of linguistic quality questions; summary content was assessed using the pseudoextrinsic measure of content responsiveness." ></td>
	<td class="line x" title="27:164	All summaries for a given topic were judged by a single assessor who was usually the same as the topic developer." ></td>
	<td class="line x" title="28:164	In all cases, the assessor was one of the summarizers for the topic." ></td>
	<td class="line x" title="29:164	Assessors first judged each summary for a topic for readability, assigning a separate score for each of 5 linguistic qualities; each summary for the topic was then judged for content responsiveness." ></td>
	<td class="line x" title="30:164	Each of these manual evaluations was based on a five-point scale (1=very poor, 5=very good), resulting in 6 scores for each summary." ></td>
	<td class="line x" title="31:164	2.1 Evaluation of Readability The readability of the summaries was assessed using five linguistic quality questions which measured qualities of the summary that did not involve comparison with a reference summary or DUC topic." ></td>
	<td class="line x" title="32:164	The linguistic qualities measured were Q1: Grammaticality, Q2: Non-redundancy, Q3: Referential clarity, Q4: Focus, and Q5: Structure and coherence." ></td>
	<td class="line x" title="33:164	2.2 Evaluation of Content NIST performed manual pseudo-extrinsic evaluation of peer summaries in the form of assessment of responsiveness." ></td>
	<td class="line oc" title="34:164	Responsiveness differs from other measures of summary content such as SEE coverage (Lin and Hovy, 2002) and Pyramid scores (Nenkova and Passonneau, 2004) in that it does not compare a peer summary against a set of known human summaries." ></td>
	<td class="line x" title="35:164	Rather, the assessor is given a list of randomly ordered, unlabeled summaries (both human and system-generated) for a topic, and must assign a responsiveness score to eachsummary(afterhavingreadallthesummaries first)." ></td>
	<td class="line x" title="36:164	In DUC 2005-2007, NIST assessors assigned a content responsiveness score to each summary; content responsiveness indicated the amount of information in the summary that helped to satisfy the information need expressed in the topic statement." ></td>
	<td class="line x" title="37:164	For content responsiveness, the linguistic quality of the summary was to play a role in the assessment only insofar as it interfered with the expression of information and reduced the amount of information that was conveyed." ></td>
	<td class="line x" title="38:164	In DUC 2006, assessors assigned an additional overall responsiveness score, which was based on both information content and readability." ></td>
	<td class="line x" title="39:164	Assessors judged overall responsiveness only after judging all their topics for readability and content responsiveness; however, they were not given direct access to these previously assigned scores, but were told to give their gut reaction to the overall responsiveness of each summary." ></td>
	<td class="line x" title="40:164	The content responsiveness score provides a coarse manual measure of information coverage; overall responsiveness reflects a combination of readability and content." ></td>
	<td class="line x" title="41:164	Content responsiveness was largely responsible for determining how assessors perceived the overall quality of a summary, butreadabilityalsoplayedanimportantrole." ></td>
	<td class="line x" title="42:164	While poor readability could downgrade the overall responsiveness of a summary that had very good content responsiveness, very good readability could sometimes bolster the overall responsiveness score of a less information-laden summary (Dang, 2006)." ></td>
	<td class="line x" title="43:164	Attempts at greater readability in 2006 paid off among the peers with the best overall responsiveness scores." ></td>
	<td class="line x" title="44:164	However, the automatic peers generally had poor readability, and the average overall responsiveness for each peer was generally much lower than its average content responsiveness." ></td>
	<td class="line o" title="45:164	In addition to the human assessment of responsiveness, NIST computed three official automatic scores using ROUGE and Basic Elements: ROUGE-2, ROUGE-SU4, and ROUGE-BE recall." ></td>
	<td class="line x" title="46:164	For the BE evaluation, summaries were parsed 146 withMinipar(Lin,2005),andBE-Fwereextracted and matched using the Head-Modifier criterion." ></td>
	<td class="line x" title="47:164	Jackknifing was used for each [peer,topic] pair so that human and automatic peers could be compared." ></td>
	<td class="line x" title="48:164	3 An Analysis of the Metrics Figure 1 shows the average scores for each summarizer for DUC 2005, 2006, and 2007." ></td>
	<td class="line x" title="49:164	For each year we report the Pearson correlation coefficient for ROUGE-2, ROUGE-SU4, and ROUGEBE (denoted R2,SU4 and BE), against content responsiveness." ></td>
	<td class="line x" title="50:164	This correlation is computed including just the systems as the human summarizers are clearly distributed differently.1 To highlight the trend in the correlation we fit the systems data using robust linear regression." ></td>
	<td class="line o" title="51:164	This line could be used to extrapolate the system performance if ROUGE scores were to increase." ></td>
	<td class="line o" title="52:164	As seen in Figure 1, while both the manual and the automatic ROUGE scores of the human summarizers remained relatively constant over the years, the systems made significant progress in their automatic scores, with the top systems performing within statistical confidence of the human summarizers in the ROUGE metrics as reported by Conroy et al.(2007)." ></td>
	<td class="line x" title="54:164	While the content responsiveness scores of the systems also increased as a group over the years, all systems performed significantly worse than humans in content responsiveness as measured by Tukeys honestly significant difference criterion (Conroy et al., 2007)." ></td>
	<td class="line n" title="55:164	Thus, there is not only a gap in performance between humans and systems on this task as measured manually by content responsiveness, but there is also a metric gap in using any single variant of ROUGE to predict content responsiveness." ></td>
	<td class="line n" title="56:164	This metric gap becomes more pronounced as system performance improves to the point where ROUGE is unable to distinguish between systems and humans." ></td>
	<td class="line x" title="57:164	Weturnnexttoananalysisof sourcesoftheperformance gap and metric gap." ></td>
	<td class="line x" title="58:164	Responsiveness is a subjective measure, and because NIST uses the same humans both to generate abstracts and to evaluate the abstracts, there is the possibility that humans may give high scores to their own abstract 1One system each year in 2005-2007 had formatting problems in their summaries which resulted in abnormally low ROUGE-BE scores." ></td>
	<td class="line x" title="59:164	While these systems are included in the scatter plot, they are not included in the correlation coefficient computation." ></td>
	<td class="line o" title="60:164	Figure 1: Scatter plot of average manual content responsiveness vs. automatic ROUGE scores (ROUGE-BE, ROUGE-2, and ROUGE-SU4) for humans (filled points) and systems (unfilled points), for DUC 2005-2007." ></td>
	<td class="line x" title="61:164	147 just because it not surprisingly says what they would say. To test this hypothesis we performed one-side Student Ttest, testing if the group of Self-Assessed abstracts had significantly higher responsiveness for DUC 2005-2007." ></td>
	<td class="line x" title="62:164	Indeed, as Table 1 shows in each year and for both content and overall responsiveness, humans gave significantly higher scores to their own abstracts than the other human abstracts." ></td>
	<td class="line x" title="63:164	This bias adds to the gap in content responsiveness between the human and automatic summarizers." ></td>
	<td class="line x" title="64:164	Fortunately, this effect is dampened by the fact that NIST used 10 assessors and on average a human got to assess their own abstract only 25% of the time." ></td>
	<td class="line x" title="65:164	It is noteworthy to add that at the Multi-lingual Summarization Evaluation of 2006, the human assessors were not the abstractors." ></td>
	<td class="line x" title="66:164	This and other factors, notably an easier task, lead to there being no gap in performance between the human and the top scoring system (Schlesinger et al., 2008)." ></td>
	<td class="line x" title="67:164	Table 1: Mean responsiveness assessment by humans for their own (Self) vs. Other abstracts." ></td>
	<td class="line x" title="68:164	Data Self Other Signif DUC 2005 Content 4.88 4.61 0.00277 DUC 2006 Content 4.96 4.68 0.00052 DUC 2006 Overall 4.94 4.67 0.00326 DUC 2007 Content 4.87 4.65 0.01931 We next examine the correlation between responsiveness and each of the five (manual) metricsforlinguisticquality." ></td>
	<td class="line x" title="69:164	Wedividethecorrelation into three groups: Human (the group of 10 human summarizers), Systems (the automatic systems enteredintoDUC),andCombined(theunionofthese two groups)." ></td>
	<td class="line x" title="70:164	Table 2 gives the Pearson correlation coefficient and the pvalue of statistical significance between content responsiveness and each of the five linguistic quality questions for DUC 20052007." ></td>
	<td class="line x" title="71:164	For DUC 2005 there is no significant correlation between the average score of a human or automaticsummarizeronlinguisticquestionsandthe content responsiveness score." ></td>
	<td class="line x" title="72:164	The fact that there is a significant correlation in the Combined case is primarily due to the fact that the human summarizers scored higher as a group than the systems in the content metric as well as the linguistic metrics." ></td>
	<td class="line x" title="73:164	In DUC 2006 and 2007, the linguistic question which rewards summaries for not having redundancy (Q2) has a significant negative correlation with content responsiveness in the group of systems." ></td>
	<td class="line x" title="74:164	Thisnegativecorrelationisduelargelytothe fact that a number of low scoring systems (including the baseline) have no significant redundancy." ></td>
	<td class="line x" title="75:164	Rarely does any system have sentences which are near duplicates." ></td>
	<td class="line x" title="76:164	However, many systems, even those with relatively high responsiveness scores, still suffer from clause level redundancy, much of it in the form of noun phrases for which a human summarizer would employ pronouns." ></td>
	<td class="line x" title="77:164	Table 3 gives additional correlations between overall responsiveness and the linguistic questions for DUC 2006." ></td>
	<td class="line x" title="78:164	We contrast the correlations for DUC 2006 in Table 2 vs. those in Table 3." ></td>
	<td class="line x" title="79:164	Not surprisingly, overall responsiveness, which intentionally penalizes summaries for linguistic problems, does correlate more strongly with the linguistic questions than content responsiveness." ></td>
	<td class="line x" title="80:164	Also, we note that the DUC 2007 correlations for content responsiveness appear more like those for DUC 2006 overall responsiveness than the corresponding correlations for DUC 2006 content responsiveness." ></td>
	<td class="line x" title="81:164	NIST did not have sufficient time in 2007 to perform an overall responsiveness evaluation." ></td>
	<td class="line x" title="82:164	We hypothesize that the assessors, many of whom worked on DUC 2006, may have inadvertently taken linguistic quality into account more in 2007 than in 2006 for the content responsiveness, since only one measure was done in 2007." ></td>
	<td class="line x" title="83:164	Finally, it was hypothesized at the DUC 2006 workshop2 that the human assessors penalize systems in content responsiveness which end with a sentence fragment, more than could be accounted for by the missing content of the sentence fragment." ></td>
	<td class="line o" title="84:164	We tested the hypothesis by comparing the average grammaticality (Q1), content responsiveness, and ROUGE scores of the 15 systems in DUC 2007 that ended their summaries with a complete sentence, against the 17 systems whose summaries ended with a sentence fragment." ></td>
	<td class="line x" title="85:164	Table 4 gives a summary of the results." ></td>
	<td class="line o" title="86:164	As measured by a Student Ttest, systems that ended their summaries with a complete sentence had significantly higher content responsiveness scores than those that did not; however, there was no significant difference in ROUGE scores." ></td>
	<td class="line x" title="87:164	The table lists ROUGE-2 as an example; these results are consistent with both ROUGE-BE and ROUGE-SU4." ></td>
	<td class="line x" title="88:164	Because linguistic quality clearly influences content responsiveness, automatic methods of evaluating summary content that try to maximize 2Lucy Vanderwende, personal communication 148 Table 2: Correlation and p-values between Content Responsiveness and Linguistic Quality Questions, DUC 2005-2007 Year Group Q1 Q2 Q3 Q4 Q5 Grammar Non-redund." ></td>
	<td class="line x" title="89:164	Refer." ></td>
	<td class="line x" title="90:164	Clarity Focus Structure/Coherence 2005 Humans -0.10( 0.78) 0.03( 0.94) 0.06( 0.87) 0.23( 0.53) 0.31( 0.39) 2005 Systems -0.05( 0.79) 0.15( 0.42) 0.19( 0.29) 0.30( 0.10) 0.08( 0.66) 2005 Combined 0.72( 0.00) 0.75( 0.00) 0.87( 0.00) 0.90( 0.00) 0.91( 0.00) 2006 Humans 0.26( 0.47) 0.15( 0.69) 0.04( 0.91) 0.64( 0.05) 0.40( 0.26) 2006 Systems 0.33( 0.05) -0.38( 0.03) 0.27( 0.11) 0.41( 0.01) 0.16( 0.35) 2006 Combined 0.74( 0.00) 0.68( 0.00) 0.86( 0.00) 0.87( 0.00) 0.89( 0.00) 2007 Humans 0.80( 0.01) 0.73( 0.02) 0.24( 0.51) 0.57( 0.09) 0.47( 0.17) 2007 Systems 0.60( 0.00) -0.43( 0.01) 0.59( 0.00) 0.71( 0.00) 0.49( 0.00) 2007 Combined 0.77( 0.00) 0.72( 0.00) 0.85( 0.00) 0.92( 0.00) 0.90( 0.00) Table 3: Correlation and p-values between Overall Responsiveness and Linguistic Quality Questions, DUC 2006 Group Q1 Q2 Q3 Q4 Q5 Grammar Non-redund." ></td>
	<td class="line x" title="91:164	Refer." ></td>
	<td class="line x" title="92:164	Clarity Focus Structure/Coherence Humans 0.60( 0.06) 0.27( 0.45) 0.39( 0.26) 0.74( 0.01) 0.82( 0.00) Systems 0.49( 0.00) -0.23( 0.19) 0.55( 0.00) 0.64( 0.00) 0.49( 0.00) Combined 0.77( 0.00) 0.72( 0.00) 0.89( 0.00) 0.89( 0.00) 0.93( 0.00) Table 4: Average scores of DUC 2007 systems ending with a complete sentence vs. those ending with a fragment." ></td>
	<td class="line x" title="93:164	Metric Sentence Fragment Signif Grammaticality 3.88 3.24 0.011 Content Resp." ></td>
	<td class="line x" title="94:164	2.79 2.46 0.021 ROUGE-2 0.098 0.092 0.408 correlation with content responsiveness should attempt to include some measures of linguistic quality." ></td>
	<td class="line p" title="95:164	We hypothesize that different variants of ROUGE may capture different qualities of a summary; for example, ROUGE-1 may be a good indicator of the relevance of summary content, but ROUGE variants that take into account larger contexts may capture linguistic qualities of the summary." ></td>
	<td class="line x" title="96:164	Hence, a combination of scores (including measures of linguistic quality) would be a better predictor of content responsiveness.3 In the 3An additional weakness in the automatic metrics, which we do not attempt to address in our current work, is their inability to adequately handle the generalizations that are often made in model summaries (Dang, 2006), which are abstractive as opposed to the extractive summaries of most systems." ></td>
	<td class="line o" title="97:164	next section, we present a new evaluation metric that finds a linear combination of ROUGE metrics which, in general, has stronger correlation with content responsiveness than any of the current ROUGE metrics." ></td>
	<td class="line o" title="98:164	4 ROSE: Un Melange de ROUGEs We developed an automatic content evaluation model which combines multiple ROUGE scores using canonical correlation (Hotelling, 1935)." ></td>
	<td class="line o" title="99:164	Canonical correlation finds the linear combination of ROUGE scores that has maximum correlation with human responsiveness on a given data set." ></td>
	<td class="line o" title="100:164	As this family of models is a blend of ROUGE scores we call this metric ROSE, for ROUGE Optimal Summarization Evaluation." ></td>
	<td class="line x" title="101:164	We first apply canonical correlation for each year of DUC using a Monte Carlo method." ></td>
	<td class="line x" title="102:164	We then report on preliminary experiments that use ROSE models from one year to predict content responsiveness in subsequent years." ></td>
	<td class="line o" title="103:164	4.1 Blending ROUGE Scoring with a Canonical Correlation Model Suppose we are given a set of ROUGE scores and the corresponding content responsiveness scores." ></td>
	<td class="line o" title="104:164	149 We let aij, for i = 1,,m and j = 1,,n, be the ROUGE score of type j for the summarizer i, and bi the human content evaluation metric." ></td>
	<td class="line x" title="105:164	Canonical correlation finds an nlong vector x such that x = argmax ( nsummationdisplay j=1 aijxj,bi), (1) where (x,y) is the Pearson correlation between x and y. A similar approach has been used by Liu and Gildea (2007) in the application of machine translation metrics, where they use a gradient optimization method to solve the maximization problem." ></td>
	<td class="line x" title="106:164	Canonical correlation actually solves a more general correlation optimization problem, where the goal is to find two linear combinations of variables to maximize the correlation between two sub-spaces." ></td>
	<td class="line x" title="107:164	In the application of document summarization, we may wish to consider a matrix B of humanevaluationmetricswherebij isthejthhuman evaluation for the ith summarizer." ></td>
	<td class="line x" title="108:164	We could include, for example, content and overall responsiveness or linguistic questions." ></td>
	<td class="line x" title="109:164	Here we solve for (x,y) in the equation below: (x,y) = argmax ( nsummationdisplay j=1 aijxj, ksummationdisplay j=1 bijyj)." ></td>
	<td class="line x" title="110:164	(2) This maximization procedure can be solved via a generalized eigenvalue problem, which we computed in Matlab using a routine distributed by Borga (2000)." ></td>
	<td class="line x" title="111:164	For the case studied here, as given inEquation(1),thegeneralizedeigenvaluereduces to a linear least squares problem." ></td>
	<td class="line x" title="112:164	Tofindstrongcanonicalcorrelationswedecided to explore a large space of metrics." ></td>
	<td class="line o" title="113:164	To this end, we included in our optimization 7 ROUGE automatic metrics: ROUGE-1,2,3,4,L,SU4, and BE to predict content responsiveness and (for DUC 2006) overall responsiveness." ></td>
	<td class="line x" title="114:164	As our analyses of the previous section indicated for DUC 2006 and 2007 there was a significant correlation between the linguistic questions and content responsiveness." ></td>
	<td class="line x" title="115:164	We add questions 1 and 4 to our canonical correlation model to see to what extent these questions could improve the correlation with content responsiveness." ></td>
	<td class="line o" title="116:164	While the linguistic questions evaluation scores are manually generated we combine them with the automatic methods of ROUGE in an attempt see to what extent these non-content scores can better model both content and overall responsiveness." ></td>
	<td class="line x" title="117:164	Thus, in all we consider 9 variables to predict responsiveness." ></td>
	<td class="line x" title="118:164	In order to perform an evaluation that would avoid over-fitting the data we used a Monte Carlo method of resampling to evaluate which of the 29 1 = 511 combinations of variables (canonical variates) to include in the model." ></td>
	<td class="line x" title="119:164	4 In each experiment of the Monte Carlo method we randomly held back 1/4 of the data (human and system summarizers) for testing and used 3/4 of the data to build a canonical variate model." ></td>
	<td class="line x" title="120:164	We found 4000 random samples sufficient to achieve accuracy within at least 2 digits." ></td>
	<td class="line x" title="121:164	For each of the canonical variate models, 4000 trials are performed and then the computed model is applied to the held-back portion of the data and its Pearson correlation and p-value is reported." ></td>
	<td class="line x" title="122:164	These 4000 correlations (and p-values) are then used to estimate the median correlation for a canonical variate." ></td>
	<td class="line x" title="123:164	The median is computed from the subset of 4000 experiments with statistically significant correlations on the testing data (95% confidence, a p-value less then 0.05)." ></td>
	<td class="line o" title="124:164	The canonical variate with the highest estimated median correlation is then compared with the best performing ROUGE method." ></td>
	<td class="line o" title="125:164	We compare the best of 504=511-7 canonical variates with the best of the 7 ROUGE variants by using the Mann-Whitney Utest, which tests for equal medians." ></td>
	<td class="line x" title="126:164	The procedure is then repeated using only the systems to find the ROSE model that gives the best prediction for just machine summarizers." ></td>
	<td class="line x" title="127:164	Table 5 gives the results of the Monte Carlo experiments." ></td>
	<td class="line o" title="128:164	In each case the best canonical variate and the estimated median correlation are reported over the set of ROUGE scores and the ROUGE scores in union with the linguistic questions." ></td>
	<td class="line x" title="129:164	As these results are based on 4000 trials they are more reliable than the simple correlation analysis done using the three official DUC automatic metrics, ROUGE-2, SU4, and BE." ></td>
	<td class="line x" title="130:164	We note, in particular, that occasionally ROUGE-1 and ROUGE-L were found to be the best predictor even when linguistic questions were allowed in the model." ></td>
	<td class="line x" title="131:164	Not surprisingly, the human evaluation of overall responsiveness was harder to predict and the optimal variants included both linguistic questions 1 and 4." ></td>
	<td class="line x" title="132:164	The ROSE models give the best combinations 4We also removed one system each year that had a poor ROUGE-BE score due to formatting problems." ></td>
	<td class="line x" title="133:164	150 Table 5: Monte Carlo Results for Canonical Correlation Model." ></td>
	<td class="line o" title="134:164	A * by a variant indicates that it differs significantly from the best single ROUGE correlation with a p-value of 107 or less as measured by a Mann Whitney U-test." ></td>
	<td class="line o" title="135:164	Year Metric Summarizer Best ROUGE Corr." ></td>
	<td class="line o" title="136:164	ROSEROUGE Corr." ></td>
	<td class="line x" title="137:164	ROSE(ROUGE,Q) Corr." ></td>
	<td class="line x" title="138:164	2005 Content All BE 0.976 R1,R2,R4,SU4,BE* 0.981 R1,R2,R3,RL,SU4,BE,Q4* 0.986 2005 Content Systems R2 0.939 R1,R2,RL 0.940 R2,RL,SU4,Q4 0.941 2006 Content All RL 0.928 R1,R2,R3,R4* 0.942 RL,Q1* 0.960 2006 Content Systems R1 0.900 R1 0.900 R1 0.900 2007 Content All BE 0.937 R1,R4,RL,BE 0.940 BE,Q4* 0.966 2007 Content Systems R3 0.906 RL,BE* 0.915 R1,RL,BE,Q1* 0.929 2006 Overall All BE 0.893 R1,R2,R3,R4* 0.913 R3,R4,Q1,Q4* 0.946 2006 Overall Systems RL 0.854 RL 0.854 R1,R3,SU4,Q1,Q4* 0.894 of ROUGE scores to give maximum correlation with the human judgement of content or overall responsiveness." ></td>
	<td class="line o" title="139:164	The ROSE models based on just ROUGE for the automatic summarizers are an appropriate method to use to compare systems that did not compete in DUC with those that did." ></td>
	<td class="line x" title="140:164	4.2 Applying ROSE across the Years To further evaluate the generality of the ROSE model we apply DUC 2005 canonical correlation models to DUC 2006 and DUC 2007, and similarly apply the DUC 2006 model to the DUC 2007 data." ></td>
	<td class="line x" title="141:164	In these experiments we measure the stability of a ROSE model from one year to the next." ></td>
	<td class="line x" title="142:164	(Note, we have also computed a model based on the combined data of DUC 2005 and DUC 2006 for use with DUC 2007 and these results are comparable to those presented.)" ></td>
	<td class="line o" title="143:164	Here, for simplicity, we restrict the ROSE model to use only the official ROUGE metrics to build a model based on a givenyearandthenevaluatethatmodelonasubsequent year." ></td>
	<td class="line x" title="144:164	Table 6 gives results for ROSE models constructed from only ROUGE-2, ROUGE-SU4, ROUGE-BE, and content responsiveness to create the ROSE model for each year; results are also given for ROSE models (ROSE+Q1,4) which also includesthelinguisticquestionsongrammaticality (Q1) and focus (Q4)." ></td>
	<td class="line n" title="145:164	The ROSE models built from only ROUGE scores had mixed results, sometimes performing worse than a single ROUGE score (e.g., the ROSE model trained on DUC 2005 and evaluated on DUC 2006), but in other cases performing as well as or better than single ROUGE scores." ></td>
	<td class="line x" title="146:164	These preliminary results with ROSE illustrate the difficulty in finding a single canonical variate that can be used from year to year to build ROSE models based on previous years data." ></td>
	<td class="line x" title="147:164	We hypothesize that the task is made more difficult due to humans changing their criteria for judging content responsiveness over the years." ></td>
	<td class="line x" title="148:164	On the other hand, ROSE+Q1,4 models that included the linguistic questions Q1 and Q4 always yielded the best correlation with content responsiveness both for the systems and for the group of combined systems and human summarizers." ></td>
	<td class="line x" title="149:164	5 Conclusions We analyzed the results of the topic-focused summarization task using the data from DUC 20052007." ></td>
	<td class="line x" title="150:164	Our main concern was to expose causes of thegapthatcurrentlyexistsbetweenautomaticand human evaluation of summary content." ></td>
	<td class="line n" title="151:164	As the automatic ROUGE scores of system summaries approaches that of human summaries, the disparity between automatic and manual measures of summary content becomes a more important concern." ></td>
	<td class="line x" title="152:164	Wefindthatthereisaslightbiasinthehumanevaluation: humans give their own summaries significantly higher scores." ></td>
	<td class="line x" title="153:164	Furthermore, the responsiveness metric appears to be time varying, i.e., the humans changed their standards for judging responsiveness over the years, making it difficult to use automatic scores from one year to predict responsiveness in another year." ></td>
	<td class="line x" title="154:164	Assessors naturally tend toward taking linguisticqualityintoaccountwhenassessingsummaries." ></td>
	<td class="line x" title="155:164	The instructions for assessing content responsiveness implicitly acknowledges this; what is surprising is the extent to which linguistic quality does influence content responsiveness." ></td>
	<td class="line x" title="156:164	In particular, we demonstrated that content responsiveness in DUC 2006 and 2007 correlated with the linguistic quality questions of grammar (Q1) and focus (Q4), and that systems were significantly penalized in content responsiveness when their summary ended 151 Table 6: Correlation and p-values between content responsiveness and various metrics for each Test year of DUC." ></td>
	<td class="line x" title="157:164	ROSE models were constructed using DUC data from Train year and evaluated on data from Test year." ></td>
	<td class="line x" title="158:164	Train/Test Summarizer R2 SU4 BE Q1 Q4 ROSE ROSE+Q1,4 2005/2006 Humans 0.64(0.05) 0.69(0.03) 0.57(0.09) 0.26(0.47) 0.64(0.05) 0.59 (0.07) 0.61(0.06) 2005/2006 Systems 0.83(0.00) 0.85(0.00) 0.85(0.00) 0.33(0.06) 0.41(0.02) 0.83 (0.00) 0.85(0.00) 2005/2006 All 0.90(0.00) 0.88(0.00) 0.90(0.00) 0.74(0.00) 0.87(0.00) 0.90 (0.00) 0.93(0.00) 2005/2007 Humans 0.41(0.24) 0.26(0.47) 0.55(0.10) 0.80(0.01) 0.57(0.09) 0.53 (0.12) 0.57(0.09) 2005/2007 Systems 0.88(0.00) 0.84(0.00) 0.89(0.00) 0.56(0.00) 0.68(0.00) 0.90 (0.00) 0.92(0.00) 2005/2007 All 0.91(0.00) 0.88(0.00) 0.92(0.00) 0.77(0.00) 0.92(0.00) 0.92 (0.00) 0.94(0.00) 2006/2007 Humans 0.41(0.24) 0.26(0.47) 0.55(0.10) 0.80(0.01) 0.57(0.09) 0.52(0.12) 0.67(0.03) 2006/2007 Systems 0.88(0.00) 0.84(0.00) 0.89(0.00) 0.56(0.00) 0.68(0.00) 0.89 (0.00) 0.90(0.00) 2006/2007 All 0.91(0.00) 0.88(0.00) 0.92(0.00) 0.77(0.00) 0.92(0.00) 0.92( 0.00) 0.96(0.00) with a sentence fragment even though the automatic content measures did not show a statistically significant difference." ></td>
	<td class="line x" title="159:164	The influence of linguistic quality on content responsiveness contributes to the evaluation gap that we see between ROUGE/BE and this coarse human measure of summary content." ></td>
	<td class="line x" title="160:164	Automatic methods of evaluating summary content that try to maximize correlation with content responsiveness should therefore attempt to include some measures of linguistic quality." ></td>
	<td class="line o" title="161:164	We found that a blending of ROUGE scores using canonical correlation gave higher correlations with content and overall responsiveness." ></td>
	<td class="line x" title="162:164	When the linguistic questions Q1 and Q4 were added to the ROSE model, correlations of up to 0.96 were observed." ></td>
	<td class="line x" title="163:164	This result leads to a natural question: What automatic methods could be used to approximate the linguistic questions?" ></td>
	<td class="line x" title="164:164	The work of Barzilay and Lapata (2005) on local coherence might be a possible candidate for estimating focus (Q4), while an automatic parser could be run on the summaries and the induced score could be used as a surrogate for grammaticality (Q1)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1023
Pedagogically Useful Extractive Summaries for Science Education
Chica, Sebastian;Ahmad, Faisal;Martin, James H.;Sumner, Tamara;"></td>
	<td class="line x" title="1:183	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 177184 Manchester, August 2008 Pedagogically Useful Extractive Summaries for Science Education Sebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara Sumner Institute of Cognitive Science Department of Computer Science University of Colorado at Boulder sebastian.delachica, faisal.ahmad, james.martin, tamara.sumner@colorado.edu    Abstract This paper describes the design and evaluation of an extractive summarizer for educational science content called COGENT." ></td>
	<td class="line x" title="2:183	COGENT extends MEAD based on strategies elicited from an empirical study with science domain and instructional design experts." ></td>
	<td class="line x" title="3:183	COGENT identifies sentences containing pedagogically relevant concepts for a specific science domain." ></td>
	<td class="line x" title="4:183	The algorithms pursue a hybrid approach integrating both domain independent bottom-up sentence scoring features and domain-aware top-down features." ></td>
	<td class="line x" title="5:183	Evaluation results indicate that COGENT outperforms existing summarizers and generates summaries that closely resemble those generated by human experts." ></td>
	<td class="line x" title="6:183	COGENT concept inventories appear to also support the computational identification of student misconceptions about earthquakes and plate tectonics." ></td>
	<td class="line x" title="7:183	1 Introduction Multidocument summarization (MDS) research efforts have resulted in significant advancements in algorithm and system design (Mani, 2001)." ></td>
	<td class="line x" title="8:183	Many of these efforts have focused on summarizing news articles, but not significantly explored the research issues arising from processing educational content to support pedagogical applications." ></td>
	<td class="line x" title="9:183	This paper describes our research into the application of MDS techniques to educational     2008." ></td>
	<td class="line x" title="10:183	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/)." ></td>
	<td class="line x" title="11:183	Some rights reserved." ></td>
	<td class="line x" title="12:183	science content to generate pedagogically useful summaries." ></td>
	<td class="line x" title="13:183	Knowledge maps are graphical representations of domain information laid out as networks of nodes containing rich concept descriptions interconnected using a fixed set of relationship types (Holley and Dansereau, 1984)." ></td>
	<td class="line x" title="14:183	Knowledge maps are a variant of the concept maps used to capture, assess, and track student knowledge in education research (Novak and Gowin, 1984)." ></td>
	<td class="line x" title="15:183	Learning research indicates that knowledge maps may be useful cognitive scaffolds, helping users lacking domain expertise to understand the macro-level structure of an information space (O'Donnell et al., 2002)." ></td>
	<td class="line x" title="16:183	Knowledge maps have emerged as an effective representation to generate conceptual browsers that help students navigate educational digital libraries, such as the Digital Library for Earth System Education (DLESE.org) (Butcher et al., 2006)." ></td>
	<td class="line x" title="17:183	In addition, knowledge maps have proven useful for domain and instructional experts to capture domain knowledge from digital library resources and to analyze student understanding for the purposes of providing formative assessments (Ahmad et al., 2007)." ></td>
	<td class="line x" title="18:183	Knowledge maps have proven useful both as representations of knowledge for assessment purposes and as learning resources for presentation to students." ></td>
	<td class="line x" title="19:183	However, domain knowledge map construction by experts is an expensive knowledge engineering activity." ></td>
	<td class="line x" title="20:183	In this paper, we describe our progress towards the automated generation of pedagogically useful extractive summaries from educational texts about a science domain." ></td>
	<td class="line x" title="21:183	In the context of automated knowledge map generation, summary sentences correspond to concepts." ></td>
	<td class="line x" title="22:183	While the detection of relationships between concepts is also part of our overall research agenda, this paper focuses solely on concept identification using MDS techniques." ></td>
	<td class="line x" title="23:183	The remainder of this paper is organized as fol177 lows." ></td>
	<td class="line x" title="24:183	First, we review related work in the areas of automated concept extraction from texts and extractive summarization." ></td>
	<td class="line x" title="25:183	We then describe the empirical study we have conducted to understand how domain and instructional design experts identify pedagogically important science concepts in educational digital library resources." ></td>
	<td class="line x" title="26:183	Next, we provide a detailed description of the algorithms we have designed based on expert strategies elicited from our empirical study." ></td>
	<td class="line x" title="27:183	We then present and discuss our evaluation results using automated summarization metrics and human judgments." ></td>
	<td class="line x" title="28:183	Finally, we present our conclusions and future work in this area." ></td>
	<td class="line x" title="29:183	2 Related Work Our work is informed by efforts to automate the acquisition of ontology concepts from text." ></td>
	<td class="line x" title="30:183	OntoLearn (Navigli and Velardi, 2004) extracts domain terminology from a collection of texts using a syntactic parse to identify candidate terms that are filtered based on domain relevance and connected using a semantic interpretation based on word sense disambiguation." ></td>
	<td class="line x" title="31:183	The newly identified concepts and relationships are used to update an existing ontology." ></td>
	<td class="line x" title="32:183	Knowledge Puzzle focuses on n-grams to produce candidate terms filtered based on term frequency in the input documents and on the number of relationships associated with a given term (Zouaq et al., 2007)." ></td>
	<td class="line x" title="33:183	This approach leverages pattern extraction techniques to identify concepts and relationships." ></td>
	<td class="line x" title="34:183	While these approaches produce ontologies useful for computational purposes, the identified concepts are of a very fine granularity and therefore may yield graphs not suitable for identifying student misconceptions or for presentation back to the student." ></td>
	<td class="line x" title="35:183	Clustering by committee has also been used to discover concepts from a text by grouping terms into conceptually related clusters (Lin and Pantel, 2002)." ></td>
	<td class="line x" title="36:183	The resulting clusters appear to be tightly related, but operate at a very fine level of granularity." ></td>
	<td class="line x" title="37:183	Our approach focuses on sentences as units of knowledge to produce concise representations that may be useful both as computational objects and as learning resources to present back to the student." ></td>
	<td class="line x" title="38:183	Therefore, extractive summarization research also informs our work." ></td>
	<td class="line x" title="39:183	Topic representation and topic themes have been used to explore promising MDS techniques (Harabagiu and Lacatusu, 2005)." ></td>
	<td class="line x" title="40:183	Recent efforts in graph-based MDS have integrated sentence affinity, information richness and diversity penalties to produce very promising results (Wan and Yang, 2006)." ></td>
	<td class="line x" title="41:183	Finally, MEAD is a widely used multi-document summarization and evaluation platform (Radev et al., 2000)." ></td>
	<td class="line x" title="42:183	MEAD research efforts have resulted in significant contributions to support the development of summarization applications (Radev et al., 2000)." ></td>
	<td class="line x" title="43:183	While all these systems have produced promising results in automated evaluations, none have directly targeted educational content as input or the generation of pedagogically useful summaries." ></td>
	<td class="line x" title="44:183	We are directly building upon MEAD due its focus on sentence extraction and its high degree of modularization." ></td>
	<td class="line x" title="45:183	3 Empirical Study We have conducted a study to capture how human experts construct and use knowledge maps." ></td>
	<td class="line x" title="46:183	In this 10-month study, we examined how experts created knowledge maps from educational digital libraries and how they used the maps to assess student work and provide personalized feedback." ></td>
	<td class="line x" title="47:183	In this paper, we are focusing on the knowledge map construction aspects of the study." ></td>
	<td class="line x" title="48:183	Four geology and instructional design experts collaboratively selected 20 resources from DLESE to construct a domain knowledge map on earthquakes and plates tectonics for high school age learners." ></td>
	<td class="line x" title="49:183	The experts independently created knowledge maps of individual resources which they collaboratively merged into the final domain knowledge map in a one-day workshop." ></td>
	<td class="line x" title="50:183	The resulting domain knowledge map consisted of 564 nodes containing domain concepts and 578 relationships." ></td>
	<td class="line x" title="51:183	The concepts consist of 7,846 words, or 5% of the total number of words in the original resources." ></td>
	<td class="line x" title="52:183	Figure 1 shows a fragment of the domain knowledge map created by our experts." ></td>
	<td class="line x" title="53:183	Figure 1." ></td>
	<td class="line x" title="54:183	Fragment of domain  knowledge map created by domain and instructional experts  Experts created nodes containing concepts of varying granularity, including nouns, noun phrases, partial sentences, single sentences, and 178 multiple sentences." ></td>
	<td class="line x" title="55:183	Our analysis of this domain knowledge map indicates that experts relied on copying-and-pasting (58%) and paraphrasing (37%) to create most domain concepts." ></td>
	<td class="line x" title="56:183	Only 5% of the nodes could not be traced directly to the original resources." ></td>
	<td class="line x" title="57:183	Experts used relationship types in a Zipf-like distribution with the top 10 relationship types accounting for 64% of all relationships." ></td>
	<td class="line x" title="58:183	The top 2 relationship types each accounted for more than 10% of all relationships: elaborations (19% or 110 links) and examples (14% or 78 links)." ></td>
	<td class="line x" title="59:183	We have established the completeness of this domain knowledge map by asking a domain expert to assess its content coverage of nationallyrecognized educational goals on earthquakes and plate tectonics for high school age learners using the American Association for the Advancement of Science (AAAS) Benchmarks (Project 2061, 1993)." ></td>
	<td class="line x" title="60:183	The results indicate adequate content coverage of the relevant AAAS Benchmarks achieved through 82 of the concepts (15%) with the remaining 482 concepts (85%) providing very detailed elaborations of the associated learning goals." ></td>
	<td class="line x" title="61:183	Qualitative analysis of the verbal protocols captured during the study indicates that all experts used external sources to construct the domain knowledge map." ></td>
	<td class="line x" title="62:183	Experts made references to their own knowledge (e.g., I know that), to content learned or taught in geology courses, to other resources used in the study, and to the National Science Education Standards (NSES), a comprehensive collection of nationallyrecognized science learning goals for K-12 students (National Research Council, 1996)." ></td>
	<td class="line x" title="63:183	We have examined sentence extraction agreement between experts using a kappa measure that accounts for prevalence of judgments and conflicting biases amongst experts, called PABAkappa (Byrt et al., 1993)." ></td>
	<td class="line x" title="64:183	The average PABAkappa value of 0.62 indicates that our experts substantially agree on sentence extraction from digital library resources." ></td>
	<td class="line x" title="65:183	While this study was not designed as an annotation project to support summarization evaluation, this level of agreement indicates that the concepts selected by the experts may serve as the reference summary to evaluate the performance of our summarizer." ></td>
	<td class="line x" title="66:183	4 Summarizer for Science Education Creating a knowledge map from a collection of input texts involves identifying sentences containing important domain concepts, linking concepts, and labeling those links." ></td>
	<td class="line x" title="67:183	This paper focuses solely on identifying and extracting pedagogically relevant sentences as domain concepts." ></td>
	<td class="line x" title="68:183	We have designed and implemented an extractive summarizer for educational science content, called COGENT, based on MEAD version 3.11 (Radev et al., 2000)." ></td>
	<td class="line x" title="69:183	COGENT processes a collection of educational digital library resources by first preprocessing each resource using Tidy (tidy.sourceforge.net) to fix improperly formatted HTML code." ></td>
	<td class="line x" title="70:183	COGENT then merges multiple web pages into a single HTML document and extracts the contents of each resource into a plain text file." ></td>
	<td class="line x" title="71:183	We have extended MEAD with sentence scoring features based on domain content, document structure, and sentence length." ></td>
	<td class="line x" title="72:183	4.1 Domain Content We have designed two sentence-scoring features that aim to capture the domain content relevance of each sentence: the educational standards feature and the gazetteer feature." ></td>
	<td class="line x" title="73:183	We have developed a feature that models how human experts used external sources to identify and extract concepts." ></td>
	<td class="line x" title="74:183	The educational standards feature uses the textual description of the relevant AAAS Benchmarks on earthquakes and plate tectonics for high-school age learners and the associated NSES." ></td>
	<td class="line x" title="75:183	Each sentence receives a score based on its similarity to the text contents of the learning goals and educational standards computed using a TFIDF (Term FrequencyInverse Document Frequency) approach  (Salton and Buckley, 1988)." ></td>
	<td class="line x" title="76:183	We have used KinoSearch, a Perl implementation of the Lucene search engine (lucene.apache.org), to create an index that includes the AAAS Benchmarks learning goal description (boosted by 2), subject (boosted by 8), and keywords (boosted by 2), plus the text of the associated national standards (not boosted)." ></td>
	<td class="line x" title="77:183	Sentence scores are based on the similarity scores generated by KinoSearch in response to a query consisting of the sentence text." ></td>
	<td class="line x" title="78:183	To account for the large number of examples used by the experts in the domain knowledge map (14% of all links), we have developed a feature that reflects the number and relevance of the geographical names in each sentence." ></td>
	<td class="line x" title="79:183	Earth science examples often refer to names of geographical places, including geological formations on the planet." ></td>
	<td class="line x" title="80:183	The gazetteer feature leverages the Alexandria Digital Library (ADL) Gazetteer service (Hill, 2000) to check whether named entities identified in each sentence match 179 entries in the ADL Gazetteer." ></td>
	<td class="line x" title="81:183	A gazetteer is a georeferencing resource containing information about locations and place-names, including latitude and longitude as well as type information about the corresponding geographical feature." ></td>
	<td class="line x" title="82:183	Each sentence receives a score based on a TFIDF approach where the TF is the number of times a particular location name appears in the sentence and the IDF is the inverse of the count of gazetteer entries matching the location name." ></td>
	<td class="line x" title="83:183	If the ADL Gazetteer returns a large number of results for a given place-name, it means there are many geographical locations identified by that name." ></td>
	<td class="line x" title="84:183	Our assumption is that unique names may be more pedagogically relevant." ></td>
	<td class="line x" title="85:183	For example, Ohio receives an IDF score of 0.0625 because the ADL Gazetteer contains 16 entries so named, while the Mid-Atlantic Ridge, the distinctive underwater mountain range dividing the Atlantic Ocean, receives a score of 1.0 as it appears only once." ></td>
	<td class="line x" title="86:183	4.2 Document Structure Based on the intuition that the HTML structure of a web site reflects content relevancy, we have developed the hypertext feature." ></td>
	<td class="line x" title="87:183	The hypertext feature assigns a higher score to sentences contained under higher level HTML headings." ></td>
	<td class="line x" title="88:183	Heading Bonus H1 1/1 = 1.00 H2 1/2 = 0.50 H3 1/3 = 0.33 H4 1/4 = 0.25 H5 1/5 = 0.20 H6 1/6 = 0.17 Table 1." ></td>
	<td class="line x" title="89:183	Hypertext feature heading bonus  Within a given heading level, the hypertext feature assigns a higher score to sentences that appear earlier within that level based on both relative paragraph order within the heading and relative sentence position within each paragraph." ></td>
	<td class="line x" title="90:183	The equation used to compute the hypertext score for a sentence is 44 _1 * _1 * _  _ nosentnoparbonusheadingscorehypertext = where heading_bonus is obtained from Table 1, par_no is the paragraph number within the heading, and sent_no is the sentence number within the paragraph." ></td>
	<td class="line x" title="91:183	We use the 4 1 x   function to attenuate the contributions to the feature score of later paragraphs and sentences." ></td>
	<td class="line x" title="92:183	Initially, we used the same function MEAD uses to modulate its position feature ( 2 1 x ), but initial experimentation indicated this function decayed too rapidly, resulting in later sentences being over-penalized." ></td>
	<td class="line x" title="93:183	4.3 Sentence Length To promote the extraction of sentences containing scientific concepts, we have developed the content word density feature." ></td>
	<td class="line x" title="94:183	This feature makes a cut-off decision based on the ratio of content words to function words in a sentence." ></td>
	<td class="line x" title="95:183	The content word density feature uses a pre-populated list of function words (a stopword list) to calculate the ratio of content to function words within each sentence, keeping sentences that meet or exceed the ratio of 50%." ></td>
	<td class="line x" title="96:183	This cut-off value implies that the extracted sentences contain relatively more content words than function words." ></td>
	<td class="line x" title="97:183	4.4 Sentence Scoring and Selection We compute the final score of each sentence by adding the scores obtained for the MEAD default configuration features (centroid and position) to the scores for the COGENT features (educational standards, gazetteer, and hypertext)." ></td>
	<td class="line x" title="98:183	After the sentences have been sorted according to their cumulative scores, we keep sentences that pass the cut-off constraints, including the MEAD length feature equal or greater than 9 and COGENT content word density equal or greater than 50%." ></td>
	<td class="line x" title="99:183	We use the MEAD cosine re-ranker to eliminate redundant sentences based on a cutoff similarity value of 0.7." ></td>
	<td class="line x" title="100:183	Since human experts used only 5% of the total word count in the resources, we have configured MEAD to use a 5% word compression rate." ></td>
	<td class="line x" title="101:183	5 Evaluation We have evaluated COGENT by processing the 20 digital library resources used in the empirical study and comparing its output against the concepts identified by the experts." ></td>
	<td class="line x" title="102:183	5.1 Quality To assess the quality of the generated summaries, we have examined three configurations: Random, Default, and COGENT." ></td>
	<td class="line x" title="103:183	The Random configuration extracts a random collection of sentences from the input texts." ></td>
	<td class="line x" title="104:183	The Default configuration uses the MEAD default centroid, position and length (cut-off value of 9) sentence scoring features." ></td>
	<td class="line x" title="105:183	Finally, the COGENT configuration includes the MEAD default features and the COGENT features." ></td>
	<td class="line x" title="106:183	The Default and COGENT configurations use the MEAD cosine function with a threshold of 0.7 to eliminate redundant sen180 tences." ></td>
	<td class="line x" title="107:183	All three configurations use a word compression factor of 5% resulting in summaries of very similar length." ></td>
	<td class="line x" title="108:183	For this evaluation, we leverage ROUGE (Lin and Hovy, 2003) to address the relative quality of the generated summaries based on common ngram counts and longest common subsequence (LCS)." ></td>
	<td class="line oc" title="109:183	We report on ROUGE-1 (unigrams), ROUGE-2 (bigrams), ROUGE W-1.2 (weighted LCS), and ROUGE-S* (skip bigrams) as they appear to correlate well with human judgments for longer multi-document summaries, particularly ROUGE-1 (Lin, 2004)." ></td>
	<td class="line o" title="110:183	Table 2 shows the results of this ROUGE-based evaluation including recall (R), precision (P), and balanced fmeasure (F)." ></td>
	<td class="line x" title="111:183	Random Default COGENT R 0.4855 0.4976 0.6073 P 0.5026 0.5688 0.6034 R-1 F 0.4939 0.5308 0.6054 R 0.0972 0.1321 0.1907 P 0.1006 0.1510 0.1895 R-2 F 0.0989 0.1409 0.1901 R 0.0929 0.0951 0.1185 P 0.1533 0.1733 0.1877 R-W-1.2 F 0.1157 0.1228 0.1453 R 0.2481 0.2620 0.3820 P 0.2657 0.3424 0.3772 R-S* F 0.2566 0.2969 0.3796 Table 2." ></td>
	<td class="line o" title="112:183	Quality evaluation results (5% word compression)  COGENT consistently outperforms the Random and Default baselines based on all four reported ROUGE measures." ></td>
	<td class="line x" title="113:183	Given that much of the original research efforts on MEAD have centered on news articles, this result is not surprising." ></td>
	<td class="line x" title="114:183	Pedagogical content, such as the educational digital library resources used in our work, differs in rhetorical intent, structure and terminology from the news articles leveraged by the MEAD researchers." ></td>
	<td class="line x" title="115:183	However, the COGENT features described here are complementary to the default MEAD configuration." ></td>
	<td class="line x" title="116:183	COGENT can best be characterized as a hybrid MDS, integrating bottom-up (centroid, position, length, hypertext, and content word density) and top-down (educational standards and gazetteer) sentence scoring features." ></td>
	<td class="line x" title="117:183	This hybrid approach reflects our findings from observing expert behaviors for identifying concepts from educational digital library resources." ></td>
	<td class="line x" title="118:183	We believe the overall improvement in quality scores may be due to the COGENT features targeting different dimensions of what constitutes a pedagogically effective summary than the default MEAD features." ></td>
	<td class="line x" title="119:183	To characterize the COGENT summary contents, one of our research team members manually generated a summary corresponding to the best case for an extractive summarizer." ></td>
	<td class="line x" title="120:183	This Best Case summary comprises the sentences from the digital library resources that align to the concepts selected by the human experts in our empirical study." ></td>
	<td class="line x" title="121:183	Since the experts created concepts of varying granularity, this alignment produces the list of sentences that the experts would have produced if they had only selected single sentences to create concepts for their domain knowledge map." ></td>
	<td class="line x" title="122:183	This summary comprises 621 sentences consisting of 13,116 words, or about a 9% word compression." ></td>
	<td class="line o" title="123:183	For this aspect of the evaluation, we have used ROUGE-L, an LCS metric computed using ROUGE." ></td>
	<td class="line o" title="124:183	The ROUGE-L computation examines the union LCS between each reference sentence and all the sentences in the candidate summary." ></td>
	<td class="line p" title="125:183	We believe this metric may be well-suited to reflect the degree of linguistic surface structure similarity between summaries." ></td>
	<td class="line p" title="126:183	We postulate that ROUGE-L may be able to account for the explicitly copy-pasted concepts and to detect the more subtle similarities with paraphrased concepts in the expert-generated domain knowledge map." ></td>
	<td class="line x" title="127:183	We have also used the content-based evaluation capabilities of MEAD to report on a cosine measure to capture similarity between the candidate summaries and the reference." ></td>
	<td class="line x" title="128:183	Table 3 shows the results of this aspect of the evaluation including recall (R), precision (P), and balanced fmeasure (F)." ></td>
	<td class="line x" title="129:183	Random (5%) Default (5%) COGENT (5%) Best Case (9%) R 0.4814 0.4919 0.6021 0.9669 P 0.4982 0.5623 0.5982 0.6256 R-L F 0.4897 0.5248 0.6001 0.7597 Cosine 0.5382 0.6748 0.8325 0.9323 Table 3." ></td>
	<td class="line o" title="130:183	Content-based evaluation results (word compression in parentheses)  COGENT consistently outperforms the Random and Default baselines on both the ROUGEL and cosine measures." ></td>
	<td class="line x" title="131:183	Given the cosine value of 0.8325, it appears COGENT extracts sentences containing similar terms  in very similar frequency distribution as the experts." ></td>
	<td class="line o" title="132:183	The ROUGE-L scores also consistently indicate that the COGENT summary may be closer to the reference summary in relative word order181 ing than either the Random or Default configurations." ></td>
	<td class="line x" title="133:183	However, the scores for the Best Case summary reveal two interesting points." ></td>
	<td class="line o" title="134:183	First, the ROUGE-L recall score for COGENT (R=0." ></td>
	<td class="line x" title="135:183	6021) is lower than that obtained by the Best Case summary (R=0.9669), meaning our summarizer appears to be extracting different sentences than those selected by the experts." ></td>
	<td class="line x" title="136:183	Given the high cosine similarity with the reference summary (0.8325), we hypothesize that COGENT may be selecting sentences that cover very similar concepts to those selected by the experts only expressed differently." ></td>
	<td class="line o" title="137:183	Second, we would have expected the ROUGE-L precision score for the Best Case configuration to be closer to 1.0." ></td>
	<td class="line x" title="138:183	Instead, the Best Case precision score is 0.6256, only a minor improvement over COGENT (P=0.5982)." ></td>
	<td class="line o" title="139:183	Since the sentences in the Best Case summary come directly from the digital library resources, we hypothesize that experts may have used extensive linguistic transformations for paraphrased concepts, resulting in structures that ROUGE-L could not identify as similar." ></td>
	<td class="line o" title="140:183	Given the difference in word compression for the Best Case summary, we have performed an incremental analysis using the ROUGE-L measure shown in Figure 2." ></td>
	<td class="line o" title="141:183	ROUGE-L COGENT Evaluation 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 0 5 10 15 20 25 30 MEAD Word Percent Compression Recall Precision F-Measure Figure 2." ></td>
	<td class="line o" title="142:183	COGENT ROUGE-L results at different word compression rates  This graph shows improved COGENT performance in ROUGE-L recall as the length of the summary increases, while both precision and fmeasure degrade." ></td>
	<td class="line x" title="143:183	COGENT can match the recall scores of the Best Case summary (R=0.9669) by making the generated summary longer (30% word compression rate or 32,619 words), but the precision would suffer a sizeable decay (P=0.1558)." ></td>
	<td class="line x" title="144:183	For educational applications, more comprehensive concept inventories (longer summaries) may be better suited for computational purposes, such as pedagogical reasoning about student understanding, while more succinct inventories (shorter summaries) may be more appropriate for display to the student." ></td>
	<td class="line x" title="145:183	5.2 Pedagogical Utility We have evaluated COGENTs pedagogical utility in the context of computationally identifying student scientific misconceptions." ></td>
	<td class="line x" title="146:183	We have developed algorithms that reliably detect incorrect statements in student essays by comparing an expert-created domain knowledge map to an expert-created knowledge map of an essay." ></td>
	<td class="line x" title="147:183	These algorithms use textual entailment techniques based on a shallow linguistic analysis of knowledge map concepts to identify sentences that contradict concepts in the domain knowledge map." ></td>
	<td class="line x" title="148:183	Initial evaluation results indicate that these algorithms identify incorrect statements nearly as adeptly as human experts." ></td>
	<td class="line x" title="149:183	Manual Expert Agreement Expert Knowledge Maps COGENT Concept Inventory Recall 0.69 0.87 0.93 Precision 0.69 0.57 0.57 F-Measure 0.69 0.68 0.69 Table 4." ></td>
	<td class="line x" title="150:183	Incorrect statement identification evaluation results  As shown in Table 4, the algorithms detect 87% of all incorrect statements identified by experts and 57% of the reported incorrect statements agree with human judgments on the same task." ></td>
	<td class="line x" title="151:183	By comparison, experts show 69% overlap on average along both dimensions." ></td>
	<td class="line x" title="152:183	Introducing the COGENT concept inventory in place of the expert-created domain knowledge map improves recall performance, as the algorithms return 93% of all incorrect statements reported by the experts, while preserving 57% precision." ></td>
	<td class="line x" title="153:183	These results indicate that the generated summary covers the necessary pedagogical concepts to computationally identify student scientific misconceptions." ></td>
	<td class="line x" title="154:183	Informal sampling of the sentences selected by COGENT shows the following three important science concepts receiving the highest scores: 1." ></td>
	<td class="line x" title="155:183	Earthquakes are the result of forces deep within the Earth's interior that continuously affect the surface of the Earth." ></td>
	<td class="line x" title="156:183	2." ></td>
	<td class="line x" title="157:183	Scientists believed that the movement of the Earth's plates bends and squeezes the rocks at the edges of the plates." ></td>
	<td class="line x" title="158:183	3." ></td>
	<td class="line x" title="159:183	In particular, four major scientific developments spurred the formulation of the plate182 tectonics theory: (1) demonstration of the ruggedness and youth of the ocean floor; (2) confirmation of repeated reversals of the Earth magnetic field in the geologic past; (3) emergence of the seafloor-spreading hypothesis and associated recycling of oceanic crust; and (4) precise documentation that the world's earthquake and volcanic activity is concentrated along oceanic trenches and submarine mountain ranges." ></td>
	<td class="line x" title="160:183	For a more rigorous analysis of the pedagogical utility of the COGENT concepts, we asked an instructional expert with domain expertise in geology to evaluate the 326 sentences returned by COGENT." ></td>
	<td class="line x" title="161:183	The expert used a 5-point Likert scale to judge whether each concept would be pedagogically useful in the context of a concept inventory on earthquakes and plate tectonics knowledge for high school age learners." ></td>
	<td class="line x" title="162:183	The expert agreed or strongly agreed that 60% of the sentences would be pedagogically useful, with 30% of the sentences being potentially useful and only 10% of the sentences being judged as not useful." ></td>
	<td class="line x" title="163:183	These results indicate that COGENT appears to perform quite well at identifying sentences that contain information relevant for learning about the domain." ></td>
	<td class="line x" title="164:183	We have also completed an ablation study to identify the relative contribution of the COGENT features to the quality of the summary." ></td>
	<td class="line x" title="165:183	We have focused on the cosine metric to capture the overall similarity between the COGENT concept inventory and the concepts from the expert-created knowledge map." ></td>
	<td class="line x" title="166:183	Features Cosine All Features 0.8325 (Gazetteer) 0.5545 (Hypertext) 0.5575 (Educational Standards) 0.8083 (Content Word Density) 0.8271  Table 5." ></td>
	<td class="line x" title="167:183	Feature ablation evaluation results for COGENT  Table 5 shows the cosine similarity between the concept inventory generated after taking the feature shown in parentheses out of the summarizer." ></td>
	<td class="line x" title="168:183	The results are ordered from low-to-high such that the feature contributing the most to the all-features cosine score appears at the top of the table." ></td>
	<td class="line x" title="169:183	Removing either the gazetteer or the hypertext feature causes the largest drops in similarity indicating the importance of the use of examples and the relevance of document structure for the quality of the COGENT-generated summary." ></td>
	<td class="line x" title="170:183	Meanwhile both the educational standards and content word density appear to provide modest but useful improvements to the quality of the COGENT summary." ></td>
	<td class="line x" title="171:183	Given that our algorithms have only been evaluated on the topic of earthquakes and plate tectonics for high school age learners, COGENT may be limited in its ability to transcend domains due to its reliance on two domain-aware sentence scoring features: educational standards and gazetteer." ></td>
	<td class="line x" title="172:183	However, the educational standards feature may be applicable across other science topics because the AAAS Benchmarks and NSES provide very thorough and detailed coverage of a wide range of topics for the Science, Technology, Engineering, and Math disciplines for grades K-12." ></td>
	<td class="line x" title="173:183	Only the gazetteer feature would need to be replaced, especially given its significant contribution to the quality of the generated summary as indicated by the results of the ablation study." ></td>
	<td class="line x" title="174:183	We believe these results highlight the need to generalize our approach, perhaps using a classifier for identifying examples in educational texts without resorting to overly domain-specific language resources, such as the ADL Gazetteer." ></td>
	<td class="line x" title="175:183	Overall, the evaluation results indicate that our approach holds promise for effectively identifying concepts for inclusion in the construction of a pedagogically useful domain knowledge map from educational science content." ></td>
	<td class="line x" title="176:183	6 Conclusions and Future Work In this paper, we have presented a multidocument summarization system, COGENT, that integrates bottom-up and top-down sentence scoring features to identify pedagogically relevant concepts from educational digital library resources." ></td>
	<td class="line x" title="177:183	Our results indicate that COGENT generates concept inventories that resemble those identified by experts and outperforms existing multi-document summarization systems." ></td>
	<td class="line x" title="178:183	We have also used the COGENT concept inventory as input to our  misconception identification algorithms and the evaluation results indicate the algorithms perform as well as when using an expert-created domain knowledge map." ></td>
	<td class="line x" title="179:183	In the context of generating domain knowledge maps, our next step is to explore how machine learning techniques may be employed to connect concepts with links." ></td>
	<td class="line x" title="180:183	Automating the process of creating inventories of important pedagogical concepts represents an important step towards creating scalable intelli183 gent learning and tutoring systems." ></td>
	<td class="line x" title="181:183	We hope our progress in this direction may contribute to increase the interest within the computational linguistics research community in novel educational technology research." ></td>
	<td class="line x" title="182:183	Acknowledgments This research is funded in part by the National Science Foundation under NSF IIS/ALT Award 0537194." ></td>
	<td class="line x" title="183:183	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-2006
A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization
Boudin, Florian;El-Bèze, Marc;Torres-Moreno, Juan-Manuel;"></td>
	<td class="line x" title="1:98	Coling 2008: Companion volume  Posters and Demonstrations, pages 2326 Manchester, August 2008 A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization Florian Boudin natural and Marc El-B`eze natural natural Laboratoire Informatique dAvignon 339 chemin des Meinajaries, BP1228, 84911 Avignon Cedex 9, France." ></td>
	<td class="line x" title="2:98	florian.boudin@univ-avignon.fr marc.elbeze@univ-avignon.fr Juan-Manuel Torres-Moreno natural,flat flat Ecole Polytechnique de Montreal CP 6079 Succ." ></td>
	<td class="line x" title="3:98	Centre Ville H3C 3A7 Montreal (Quebec), Canada." ></td>
	<td class="line x" title="4:98	juan-manuel.torres@univ-avignon.fr Abstract We present SMMR, a scalable sentence scoring method for query-oriented update summarization." ></td>
	<td class="line x" title="5:98	Sentences are scored thanks to a criterion combining query relevance and dissimilarity with already read documents (history)." ></td>
	<td class="line x" title="6:98	As the amount of data in history increases, non-redundancy is prioritized over query-relevance." ></td>
	<td class="line x" title="7:98	We show that SMMR achieves promising results on the DUC 2007 update corpus." ></td>
	<td class="line x" title="8:98	1 Introduction Extensive experiments on query-oriented multidocument summarization have been carried out over the past few years." ></td>
	<td class="line x" title="9:98	Most of the strategies to produce summaries are based on an extraction method, which identifies salient textual segments, most often sentences, in documents." ></td>
	<td class="line x" title="10:98	Sentences containing the most salient concepts are selected, ordered and assembled according to their relevance to produce summaries (also called extracts) (Mani and Maybury, 1999)." ></td>
	<td class="line x" title="11:98	Recently emerged from the Document Understanding Conference (DUC) 20071, update summarization attempts to enhance summarization when more information about knowledge acquired by the user is available." ></td>
	<td class="line x" title="12:98	It asks the following question: has the user already read documents on the topic?" ></td>
	<td class="line x" title="13:98	In the case of a positive answer, producing an extract focusing on only new facts is of interest." ></td>
	<td class="line x" title="14:98	In this way, an important issue is introduced: c2008." ></td>
	<td class="line x" title="15:98	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="16:98	Some rights reserved." ></td>
	<td class="line x" title="17:98	1Document Understanding Conferences are conducted since 2000 by the National Institute of Standards and Technology (NIST), http://www-nlpir.nist.gov redundancy with previously read documents (history) has to be removed from the extract." ></td>
	<td class="line x" title="18:98	A natural way to go about update summarization would be extracting temporal tags (dates, elapsed times, temporal expressions) (Mani and Wilson, 2000) or to automatically construct the timeline from documents (Swan and Allan, 2000)." ></td>
	<td class="line x" title="19:98	These temporal marks could be used to focus extracts on the most recently written facts." ></td>
	<td class="line x" title="20:98	However, most recently written facts are not necessarily new facts." ></td>
	<td class="line x" title="21:98	Machine Reading (MR) was used by (Hickl et al., 2007) to construct knowledge representations from clusters of documents." ></td>
	<td class="line x" title="22:98	Sentences containing new information (i.e. that could not be inferred by any previously considered document) are selected to generate summary." ></td>
	<td class="line x" title="23:98	However, this highly efficient approach (best system in DUC 2007 update) requires large linguistic resources." ></td>
	<td class="line x" title="24:98	(Witte et al., 2007) propose a rule-based system based on fuzzy coreference cluster graphs." ></td>
	<td class="line x" title="25:98	Again, this approach requires to manually write the sentence ranking scheme." ></td>
	<td class="line x" title="26:98	Several strategies remaining on post-processing redundancy removal techniques have been suggested." ></td>
	<td class="line x" title="27:98	Extracts constructed from history were used by (Boudin and TorresMoreno, 2007) to minimize historys redundancy." ></td>
	<td class="line x" title="28:98	(Lin et al., 2007) have proposed a modified Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) re-ranker during sentence selection, constructing the summary by incrementally re-ranking sentences." ></td>
	<td class="line x" title="29:98	In this paper, we propose a scalable sentence scoring method for update summarization derived from MMR." ></td>
	<td class="line x" title="30:98	Motivated by the need for relevant novelty, candidate sentences are selected according to a combined criterion of query relevance and dissimilarity with previously read sentences." ></td>
	<td class="line x" title="31:98	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="32:98	Section 2 23 introduces our proposed sentence scoring method and Section 3 presents experiments and evaluates our approach." ></td>
	<td class="line x" title="33:98	2 Method The underlying idea of our method is that as the number of sentences in the history increases, the likelihood to have redundant information within candidate sentences also increases." ></td>
	<td class="line x" title="34:98	We propose a scalable sentence scoring method derived from MMR that, as the size of the history increases, gives more importance to non-redundancy that to query relevance." ></td>
	<td class="line x" title="35:98	We define H to represent the previously read documents (history), Q to represent the query and s the candidate sentence." ></td>
	<td class="line x" title="36:98	The following subsections formally define the similarity measures and the scalable MMR scoring method." ></td>
	<td class="line x" title="37:98	2.1 A query-oriented multi-document summarizer We have first started by implementing a simple summarizer for which the task is to produce queryfocused summaries from clusters of documents." ></td>
	<td class="line x" title="38:98	Each document is pre-processed: documents are segmented into sentences, sentences are filtered (words which do not carry meaning are removed such as functional words or common words) and normalized using a lemmas database (i.e. inflected forms go, goes, went, gone are replaced by go)." ></td>
	<td class="line x" title="39:98	An N-dimensional term-space , where N is the number of different terms found in the cluster, is constructed." ></td>
	<td class="line x" title="40:98	Sentences are represented in  by vectors in which each component is the term frequency within the sentence." ></td>
	<td class="line x" title="41:98	Sentence scoring can be seen as a passage retrieval task in Information Retrieval (IR)." ></td>
	<td class="line x" title="42:98	Each sentencesis scored by computing a combination of two similarity measures between the sentence and the query." ></td>
	<td class="line x" title="43:98	The first measure is the well known cosine angle (Salton et al., 1975) between the sentence and the query vectorial representations in  (denoted respectively vectors and vectorQ)." ></td>
	<td class="line x" title="44:98	The second similarity measure is based on the Jaro-Winkler distance (Winkler, 1999)." ></td>
	<td class="line x" title="45:98	The original Jaro-Winkler measure, denoted JW, uses the number of matching characters and transpositions to compute a similarity score between two terms, giving more favourable ratings to terms that match from the beginning." ></td>
	<td class="line x" title="46:98	We have extended this measure to calculate the similarity between the sentence s and the query Q: JWe(s,Q) = 1|Q| summationdisplay qQ max mSprime JW(q,m) (1) where Sprime is the term set of s in which the terms m that already have maximized JW(q,m) are removed." ></td>
	<td class="line x" title="47:98	The use of JWe smooths normalization and misspelling errors." ></td>
	<td class="line x" title="48:98	Each sentencesis scored using the linear combination: Sim1(s,Q) = cosine(vectors,vectorQ) + (1)JWe(s,Q) (2) where  = 0.7, optimally tuned on the past DUCs data (2005 and 2006)." ></td>
	<td class="line x" title="49:98	The system produces a list of ranked sentences from which the summary is constructed by arranging the high scored sentences until the desired size is reached." ></td>
	<td class="line x" title="50:98	2.2 A scalable MMR approach MMR re-ranking algorithm has been successfully used in query-oriented summarization (Ye et al., 2005)." ></td>
	<td class="line x" title="51:98	It strives to reduce redundancy while maintaining query relevance in selected sentences." ></td>
	<td class="line x" title="52:98	The summary is constructed incrementally from a list of ranked sentences, at each iteration the sentence which maximizes MMR is chosen: MMR = argmax sS [Sim1(s,Q) (1) max sjE Sim2(s,sj) ] (3) where S is the set of candidates sentences and E is the set of selected sentences." ></td>
	<td class="line x" title="53:98	 represents an interpolation coefficient between sentences relevance and non-redundancy." ></td>
	<td class="line x" title="54:98	Sim2(s,sj) is a normalized Longest Common Substring (LCS) measure between sentences s and sj." ></td>
	<td class="line x" title="55:98	Detecting sentence rehearsals, LCS is well adapted for redundancy removal." ></td>
	<td class="line x" title="56:98	We propose an interpretation of MMR to tackle the update summarization issue." ></td>
	<td class="line x" title="57:98	Since Sim1 and Sim2 are ranged in [0,1], they can be seen as probabilities even though they are not." ></td>
	<td class="line x" title="58:98	Just as rewriting (3) as (NR stands for Novelty Relevance): NR = argmax sS [Sim1(s,Q) + (1) (1max shH Sim2(s,sh)) ] (4) We can understand that (4) equates to an OR combination." ></td>
	<td class="line x" title="59:98	But as we are looking for a more intuitive AND and since the similarities are independent, we have to use the product combination." ></td>
	<td class="line x" title="60:98	The 24 scoring method defined in (2) is modified into a double maximization criterion in which the best ranked sentence will be the most relevant to the query AND the most different to the sentences in H. SMMR(s) =Sim1(s,Q)  parenleftbigg 1max shH Sim2(s,sh) parenrightbiggf(H) (5) Decreasing  in (3) with the length of the summary was suggested by (Murray et al., 2005) and successfully used in the DUC 2005 by (Hachey et al., 2005), thereby emphasizing the relevance at the outset but increasingly prioritizing redundancy removal as the process continues." ></td>
	<td class="line x" title="61:98	Similarly, we propose to follow this assumption in SMMR using a function denoted f that as the amount of data in history increases, prioritize nonredundancy (f(H)0)." ></td>
	<td class="line x" title="62:98	3 Experiments The method described in the previous section has been implemented and evaluated by using the DUC 2007 update corpus2." ></td>
	<td class="line x" title="63:98	The following subsections present details of the different experiments we have conducted." ></td>
	<td class="line x" title="64:98	3.1 The DUC 2007 update corpus We used for our experiments the DUC 2007 update competition data set." ></td>
	<td class="line x" title="65:98	The corpus is composed of 10 topics, with 25 documents per topic." ></td>
	<td class="line x" title="66:98	The update task goal was to produce short (100 words) multi-document update summaries of newswire articles under the assumption that the user has already read a set of earlier articles." ></td>
	<td class="line x" title="67:98	The purpose of each update summary will be to inform the reader of new information about a particular topic." ></td>
	<td class="line x" title="68:98	Given a DUC topic and its 3 document clusters: A (10 documents), B (8 documents) and C (7 documents), the task is to create from the documents three brief, fluent summaries that contribute to satisfying the information need expressed in the topic statement." ></td>
	<td class="line x" title="69:98	1." ></td>
	<td class="line x" title="70:98	A summary of documents in cluster A. 2." ></td>
	<td class="line x" title="71:98	An update summary of documents in B, under the assumption that the reader has already read documents in A. 2More information about the DUC 2007 corpus is available at http://duc.nist.gov/." ></td>
	<td class="line x" title="72:98	3." ></td>
	<td class="line x" title="73:98	An update summary of documents in C, under the assumption that the reader has already read documents in A and B. Within a topic, the document clusters must be processed in chronological order." ></td>
	<td class="line x" title="74:98	Our system generates a summary for each cluster by arranging the high ranked sentences until the limit of 100 words is reached." ></td>
	<td class="line x" title="75:98	3.2 Evaluation Most existing automated evaluation methods work by comparing the generated summaries to one or more reference summaries (ideally, produced by humans)." ></td>
	<td class="line oc" title="76:98	To evaluate the quality of our generated summaries, we choose to use the ROUGE3 (Lin, 2004) evaluation toolkit, that has been found to be highly correlated with human judgments." ></td>
	<td class="line x" title="77:98	ROUGEN is a n-gram recall measure calculated between a candidate summary and a set of reference summaries." ></td>
	<td class="line o" title="78:98	In our experiments ROUGE-1, ROUGE-2 and ROUGE-SU4 will be computed." ></td>
	<td class="line x" title="79:98	3.3 Results Table 1 reports the results obtained on the DUC 2007 update data set for different sentence scoring methods." ></td>
	<td class="line x" title="80:98	cosine + JWe stands for the scoring method defined in (2) and NR improves it with sentence re-ranking defined in equation (4)." ></td>
	<td class="line x" title="81:98	SMMR is the combined adaptation we have proposed in (5)." ></td>
	<td class="line x" title="82:98	The function f(H) used in SMMR is the simple rational function 1H , where H increases with the number of previous clusters (f(H) = 1 for cluster A, 12 for cluster B and 13 for cluster C)." ></td>
	<td class="line x" title="83:98	This function allows to simply test the assumption that non-redundancy have to be favoured as the size of history grows." ></td>
	<td class="line x" title="84:98	Baseline results are obtained on summaries generated by taking the leading sentences of the most recent documents of the cluster, up to 100 words (official baseline of DUC)." ></td>
	<td class="line x" title="85:98	The table also lists the three top performing systems at DUC 2007 and the lowest scored human reference." ></td>
	<td class="line x" title="86:98	As we can see from these results, SMMR outperforms the other sentence scoring methods." ></td>
	<td class="line x" title="87:98	By ways of comparison our system would have been ranked second at the DUC 2007 update competition." ></td>
	<td class="line x" title="88:98	Moreover, no post-processing was applied to the selected sentences leaving an important margin of progress." ></td>
	<td class="line o" title="89:98	Another interesting result is the high performance of the non-update specific method (cosine+ JWe) that could be due to the small size 3ROUGE is available at http://haydn.isi.edu/ROUGE/." ></td>
	<td class="line x" title="90:98	25 of the corpus (little redundancy between clusters)." ></td>
	<td class="line o" title="91:98	ROUGE-1 ROUGE-2 ROUGE-SU4 Baseline 0.26232 0.04543 0.08247 3rd system 0.35715 0.09622 0.13245 2nd system 0.36965 0.09851 0.13509 cosine+ JWe 0.35905 0.10161 0.13701 NR 0.36207 0.10042 0.13781 SMMR 0.36323 0.10223 0.13886 1st system 0.37032 0.11189 0.14306 Worst human 0.40497 0.10511 0.14779 Table 1: ROUGE average recall scores computed on the DUC 2007 update corpus." ></td>
	<td class="line x" title="92:98	4 Discussion and Future Work In this paper we have described SMMR, a scalable sentence scoring method based on MMR that achieves very promising results." ></td>
	<td class="line x" title="93:98	An important aspect of our sentence scoring method is that it does not requires re-ranking nor linguistic knowledge, which makes it a simple and fast approach to the issue of update summarization." ></td>
	<td class="line x" title="94:98	It was pointed out at the DUC 2007 workshop that Question Answering and query-oriented summarization have been converging on a common task." ></td>
	<td class="line x" title="95:98	The value added by summarization lies in the linguistic quality." ></td>
	<td class="line x" title="96:98	Approaches mixing IR techniques are well suited for query-oriented summarization but they require intensive work for making the summary fluent and coherent." ></td>
	<td class="line x" title="97:98	Among the others, this is a point that we think is worthy of further investigation." ></td>
	<td class="line x" title="98:98	Acknowledgments This work was supported by the Agence Nationale de la Recherche, France, project RPM2." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1032
Selecting Sentences for Answering Complex Questions
Chali, Yllias;Joty, Shafiq R.;"></td>
	<td class="line x" title="1:217	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304313, Honolulu, October 2008." ></td>
	<td class="line x" title="2:217	c2008 Association for Computational Linguistics Selecting Sentences for Answering Complex Questions Yllias Chali University of Lethbridge 4401 University Drive Lethbridge, Alberta, Canada, T1K 3M4 chali@cs.uleth.ca Shafiq R. Joty University of British Columbia 2366 Main Mall Vancouver, B.C. Canada V6T 1Z4 rjoty@cs.ubc.ca Abstract Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topicoriented, informative multi-document summarization." ></td>
	<td class="line x" title="3:217	In this paper, we have experimented with one empirical and two unsupervised statistical machine learning techniques: kmeans and Expectation Maximization (EM), for computing relative importance of the sentences." ></td>
	<td class="line x" title="4:217	However, the performance of these approaches depends entirely on the feature set used and the weighting of these features." ></td>
	<td class="line x" title="5:217	We extracted different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query." ></td>
	<td class="line x" title="6:217	We used a local search technique to learn the weights of the features." ></td>
	<td class="line x" title="7:217	For all our methods of generating summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) features." ></td>
	<td class="line x" title="8:217	1 Introduction After having made substantial headway in factoid and list questions, researchers have turned their attention to more complex information needs that cannot be answered by simply extracting named entities (persons, organizations, locations, dates, etc.) from documents." ></td>
	<td class="line x" title="9:217	For example, the question: Describe the after-effects of cyclone Sidr-Nov 2007 in Bangladesh requires inferencing and synthesizing information from multiple documents." ></td>
	<td class="line x" title="10:217	This information synthesis in NLP can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information." ></td>
	<td class="line x" title="11:217	In this paper, we experimented with one empirical and two well-known unsupervised statistical machine learning techniques: k-means and EM and evaluated their performance in generating topicoriented summaries." ></td>
	<td class="line x" title="12:217	However, the performance of these approaches depends entirely on the feature set used and the weighting of these features." ></td>
	<td class="line x" title="13:217	We extracted different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query." ></td>
	<td class="line x" title="14:217	We have used a gradient descent local search technique to learn the weights of the features." ></td>
	<td class="line x" title="15:217	Traditionally, information extraction techniques are based on the BOW approach augmented by language modeling." ></td>
	<td class="line x" title="16:217	But when the task requires the use of more complex semantics, the approaches based on only BOW are often inadequate to perform fine-level textual analysis." ></td>
	<td class="line x" title="17:217	Some improvements on BOW are given by the use of dependency trees and syntactic parse trees (Hirao et al., 2004), (Punyakanok et al., 2004), (Zhang and Lee, 2003), but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs." ></td>
	<td class="line x" title="18:217	Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007)." ></td>
	<td class="line x" title="19:217	Attempting an application of 304 syntactic and semantic information to complex QA hence seems natural, as pinpointing the answer to a question relies on a deep understanding of the semantics of both." ></td>
	<td class="line x" title="20:217	In more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions), to our knowledge no study uses tree kernel functions to encode syntactic/semantic information." ></td>
	<td class="line x" title="21:217	For all our methods of generating summaries (i.e. empirical, k-means and EM), we have shown the effects of syntactic and shallow-semantic features over the BOW features." ></td>
	<td class="line x" title="22:217	This paper is organized as follows: Section 2 focuses on the related work, Section 3 describes how the features are extracted, Section 4 discusses the scoring approaches, Section 5 discusses how we remove the redundant sentences before adding them to the summary, Section 6 describes our experimental study." ></td>
	<td class="line x" title="23:217	We conclude and give future directions in Section 7." ></td>
	<td class="line x" title="24:217	2 Related Work Researchers all over the world working on querybased summarization are trying different directions to see which methods provide the best results." ></td>
	<td class="line x" title="25:217	The LexRank method addressed in (Erkan and Radev, 2004) was very successful in generic multi-document summarization." ></td>
	<td class="line x" title="26:217	A topic-sensitive LexRank is proposed in (Otterbacher et al., 2005)." ></td>
	<td class="line x" title="27:217	As in LexRank, the set of sentences in a document cluster is represented as a graph, where nodes are sentences and links between the nodes are induced by a similarity relation between the sentences." ></td>
	<td class="line x" title="28:217	Then the system ranked the sentences according to a random walk model defined in terms of both the intersentence similarities and the similarities of the sentences to the topic description or question." ></td>
	<td class="line x" title="29:217	The summarization methods based on lexical chain first extract the nouns, compound nouns and named entities as candidate words (Li et al., 2007)." ></td>
	<td class="line x" title="30:217	Then using WordNet, the systems find the semantic similarity between the nouns and compound nouns." ></td>
	<td class="line x" title="31:217	After that, lexical chains are built in two steps: 1) Building single document strong chains while disambiguating the senses of the words and, 2) building multi-chain by merging the strongest chains of the single documents into one chain." ></td>
	<td class="line x" title="32:217	The systems rank sentences using a formula that involves a) the lexical chain, b) keywords from query and c) named entities." ></td>
	<td class="line x" title="33:217	(Harabagiu et al., 2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Summarization (MDS) techniques." ></td>
	<td class="line x" title="34:217	The question decomposition procedure operates on a Marcov chain, by following a random walk with mixture model on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations." ></td>
	<td class="line x" title="35:217	Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a MDS system." ></td>
	<td class="line x" title="36:217	They show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions." ></td>
	<td class="line x" title="37:217	There are approaches that are based on probabilistic models (Pingali et al., 2007) (Toutanova et al., 2007)." ></td>
	<td class="line x" title="38:217	(Pingali et al., 2007) rank the sentences based on a mixture model where each component of the model is a statistical model: Score(s) = QIScore(s)+(1)QFocus(s,Q) Where, Score(s) is the score for sentence s. Queryindependent score (QIScore) and query-dependent score (QFocus) are calculated based on probabilistic models." ></td>
	<td class="line o" title="39:217	(Toutanova et al., 2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency." ></td>
	<td class="line x" title="40:217	The scoring function is learned by fitting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion." ></td>
	<td class="line x" title="41:217	The scoring function is further adapted to apply to summaries rather than sentences and to take into account redundancy among sentences." ></td>
	<td class="line x" title="42:217	There are approaches in Recognizing Textual Entailment, Sentence Alignment and Question Answering that use syntactic and/or semantic information in order to measure the similarity between two textual units." ></td>
	<td class="line x" title="43:217	(MacCartney et al., 2006) use typed dependency graphs (same as dependency trees) to represent the text and the hypothesis." ></td>
	<td class="line x" title="44:217	Then they try to find a good partial alignment between the typed dependency graphs representing the hypothesis and the text in a search space of O((m + 1)n) 305 where hypothesis graph containsnnodes and a text graph contains m nodes." ></td>
	<td class="line x" title="45:217	(Hirao et al., 2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information." ></td>
	<td class="line x" title="46:217	They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences." ></td>
	<td class="line x" title="47:217	They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs." ></td>
	<td class="line x" title="48:217	(Kouylekov and Magnini, 2005) use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment." ></td>
	<td class="line x" title="49:217	According to this approach, a text T entails a hypothesis H if there exists a sequence of transformations (i.e. deletion, insertion and substitution) applied to T such that we can obtain H with an overall cost below a certain threshold." ></td>
	<td class="line x" title="50:217	(Punyakanok et al., 2004) represent the question and the sentence containing answer with their dependency trees." ></td>
	<td class="line x" title="51:217	They add semantic information (i.e. named entity, synonyms and other related words) in the dependency trees." ></td>
	<td class="line x" title="52:217	They apply the approximate tree matching in order to decide how similar any given pair of trees are." ></td>
	<td class="line x" title="53:217	They also use the edit distance as the matching criteria in the approximate tree matching." ></td>
	<td class="line x" title="54:217	All these methods show the improvement over the BOW scoring methods." ></td>
	<td class="line x" title="55:217	Our Basic Element (BE)-based feature used the dependency tree to extract the BEs (i.e. head-modifier-relation) and ranked the BEs based on their log-likelihood ratios." ></td>
	<td class="line x" title="56:217	For syntactic feature, we extracted the syntactic trees for the sentence as well as for the query using the Charniak parser and measured the similarity between the two trees using the tree kernel function." ></td>
	<td class="line x" title="57:217	We used the ASSERT semantic role labeler system to parse the sentence as well as the query semantically and used the shallow semantic tree kernel to measure the similarity between the two shallow-semantic trees." ></td>
	<td class="line x" title="58:217	3 Feature Extraction The sentences in the document collection are analyzed in various levels and each of the document-sentences is represented as a vector of feature-values." ></td>
	<td class="line x" title="59:217	The features can be divided into several categories: 3.1 Lexical Features 3.1.1 N-gram Overlap N-gram overlap measures the overlapping word sequences between the candidate sentence and the query sentence." ></td>
	<td class="line x" title="60:217	With the view to measure the N-gram (N=1,2,3,4) overlap scores, a query pool and a sentence pool are created." ></td>
	<td class="line x" title="61:217	In order to create the query (or sentence) pool, we took the query (or document) sentence and created a set of related sentences by replacing its important words1 by their first-sense synonyms." ></td>
	<td class="line x" title="62:217	For example given 1hence forth important words are the nouns, verbs, adverbs and adjectives a stemmed document-sentence: John write a poem, the sentence pool contains: John compose a poem, John write a verse form along with the given sentence." ></td>
	<td class="line x" title="63:217	We measured the recall based n-gram scores for a sentenceP using the following formula: n-gramScore(P) = maxi(maxj N-gram(si,qj)) N-gram(S,Q) = summationtext gramnSCountmatch(gramn)summationtext gramnSCount(gramn) Where, n stands for the length of the n-gram (n = 1,2,3,4) and Countmatch (gramn) is the number of n-grams co-occurring in the query and the candidate sentence, qj is the jth sentence in the query pool and si is the ith sentence in the sentence pool of sentence P. 3.1.2 LCS, WLCS and Skip-Bigram A sequence W = [w1,w2,,wn] is a subsequence of another sequence X = [x1,x2,,xm], if there exists a strict increasing sequence [i1,i2,,ik] of indices of X such that for all j = 1,2,,k we have xij = wj." ></td>
	<td class="line x" title="64:217	Given two sequences, S1 and S2, the Longest Common Subsequence (LCS) of S1 andS2 is a common subsequence with maximum length." ></td>
	<td class="line x" title="65:217	The longer the LCS of two sentences is, the more similar the two sentences are." ></td>
	<td class="line oc" title="66:217	The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences (Lin, 2004)." ></td>
	<td class="line x" title="67:217	To improve the basic LCS method, we can remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS." ></td>
	<td class="line x" title="68:217	We call this weighted LCS (WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj." ></td>
	<td class="line oc" title="69:217	Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated in (Lin, 2004)." ></td>
	<td class="line oc" title="70:217	We computed the LCS and WLCS-based F-measure following (Lin, 2004) using both the query pool and the sentence pool as in the previous section." ></td>
	<td class="line x" title="71:217	Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps." ></td>
	<td class="line x" title="72:217	Skip-bigram measures the overlap of skip-bigrams between a candidate sentence and a query sentence." ></td>
	<td class="line oc" title="73:217	Following (Lin, 2004), we computed the skip bi-gram score using both the sentence pool and the query pool." ></td>
	<td class="line x" title="74:217	306 3.1.3 Head and Head Related-words Overlap The number of head words common in between two sentences can indicate how much they are relevant to each other." ></td>
	<td class="line x" title="75:217	In order to extract the heads from the sentence (or query), the sentence (or query) is parsed by Minipar 2 and from the dependency tree we extract the heads which we call exact head words." ></td>
	<td class="line x" title="76:217	For example, the head word of the sentence: John eats rice is eat." ></td>
	<td class="line x" title="77:217	We take the synonyms, hyponyms and hypernyms3 of both the query-head words and the sentence-head words and form a set of words which we call head-related words." ></td>
	<td class="line x" title="78:217	We measured the exact head score and the head-related score as follows: ExactHeadScore = summationtext w1HeadSetCountmatch(w1)summationtext w1HeadSetCount(w1) HeadRelatedScore = summationtext w1HeadRelSetCountmatch(w1)summationtext w1HeadRelSetCount(w1) Where HeadSet is the set of head words in the sentence and Countmatch is the number of matches between the HeadSet of the query and the sentence." ></td>
	<td class="line x" title="79:217	HeadRelSet is the set of synonyms, hyponyms and hypernyms of head words in the sentence and Countmatch is the number of matches between the head-related words of the query and the sentence." ></td>
	<td class="line x" title="80:217	3.2 Lexical Semantic Features We form a set of words which we call QueryRelatedWords by taking the important words from the query, their first-sense synonyms, the nouns hypernyms/hyponyms and important words from the nouns gloss definitions." ></td>
	<td class="line x" title="81:217	Synonym overlap measure is the overlap between the list of synonyms of the important words extracted from the candidate sentence and the QueryRelatedWords." ></td>
	<td class="line x" title="82:217	Hypernym/hyponym overlap measure is the overlap between the list of hypernyms and hyponyms of the nouns extracted from the sentence and the QueryRelatedWords, and gloss overlap measure is the overlap between the list of important words that are extracted from the gloss definitions of the nouns of the sentence and the QueryRelatedWords." ></td>
	<td class="line x" title="83:217	2http://www.cs.ualberta.ca/ lindek/minipar.htm 3hypernym and hyponym levels are restricted to 2 and 3 respectively 3.3 Statistical Similarity Measures Statistical similarity measures are based on the co-occurance of similar words in a corpus." ></td>
	<td class="line x" title="84:217	We have used two statistical similarity measures: 1." ></td>
	<td class="line x" title="85:217	Dependency-based similarity measure and 2." ></td>
	<td class="line x" title="86:217	Proximity-based similarity measure." ></td>
	<td class="line x" title="87:217	Dependency-based similarity measure uses the dependency relations among words in order to measure the similarity." ></td>
	<td class="line x" title="88:217	It extracts the dependency triples then uses statistical approach to measure the similarity." ></td>
	<td class="line x" title="89:217	Proximity-based similarity measure is computed based on the linear proximity relationship between words only." ></td>
	<td class="line x" title="90:217	It uses the information theoretic definition of similarity to measure the similarity." ></td>
	<td class="line x" title="91:217	We used the data provided by Dr. Dekang Lin4." ></td>
	<td class="line x" title="92:217	Using the data, one can retrieve most similar words for a given word." ></td>
	<td class="line x" title="93:217	The similar words are grouped into clusters." ></td>
	<td class="line x" title="94:217	Note that, for a word there can be more than one cluster." ></td>
	<td class="line x" title="95:217	Each cluster represents the sense of the word and its similar words for that sense." ></td>
	<td class="line x" title="96:217	For each query word, we extract all of its clusters from the data." ></td>
	<td class="line x" title="97:217	Now, in order to determine the right cluster for a query word, we measure the overlap score between the QueryRelatedWords and the clusters of words." ></td>
	<td class="line x" title="98:217	The hypothesis is that, the cluster that has more words common with the QueryRelatedWords is the right cluster." ></td>
	<td class="line x" title="99:217	We chose the cluster for a word which has the highest overlap score." ></td>
	<td class="line x" title="100:217	Once we get the clusters for the query words, we measured the overlap between the cluster words and the sentence words as follows: Measure = summationtext w1SenWordsCountmatch(w1)summationtext w1SenWordsCount(w1) Where, SenWords is the set of important words extracted from the sentence and Countmatch is the number of matches between the sentence words and the clusters of similar words of the query words." ></td>
	<td class="line x" title="101:217	3.4 Graph-based Similarity Measure In LexRank (Erkan and Radev, 2004), the concept of graph-based centrality is used to rank a set of sentences, in producing generic multi-document summaries." ></td>
	<td class="line x" title="102:217	A similarity graph is produced for the sentences in the document collection." ></td>
	<td class="line x" title="103:217	In the graph, each node represents a sentence." ></td>
	<td class="line x" title="104:217	The edges between the nodes measure the cosine similarity between the respective pair of sentences." ></td>
	<td class="line x" title="105:217	The degree of a given node is an indication of how much important the sentence is. Once the similarity graph is 4http://www.cs.ualberta.ca/ lindek/downloads.htm 307 constructed, the sentences are then ranked according to their eigenvector centrality." ></td>
	<td class="line x" title="106:217	To apply LexRank to queryfocused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005)." ></td>
	<td class="line x" title="107:217	We followed a similar approach in order to calculate this feature." ></td>
	<td class="line x" title="108:217	The score of a sentence is determined by a mixture model of the relevance of the sentence to the query and the similarity of the sentence to other high-scoring sentences." ></td>
	<td class="line x" title="109:217	3.5 Syntactic and Semantic Features: So far, we have included the features of type Bag of Words (BOW)." ></td>
	<td class="line x" title="110:217	The task like query-based summarization that requires the use of more complex syntactic and semantics, the approaches with only BOW are often inadequate to perform fine-level textual analysis." ></td>
	<td class="line x" title="111:217	We extracted three features that incorporate syntactic/semantic information." ></td>
	<td class="line x" title="112:217	3.5.1 Basic Element (BE) Overlap Measure The head-modifier-relation triples, extracted from the dependency trees are considered as BEs in our experiment." ></td>
	<td class="line x" title="113:217	The triples encode some syntactic/semantic information and one can quite easily decide whether any two units match or notconsiderably more easily than with longer units (Zhou et al., 2005)." ></td>
	<td class="line x" title="114:217	We used the BE package distributed by ISI5 to extract the BEs for the sentences." ></td>
	<td class="line x" title="115:217	Once we get the BEs for a sentence, we computed the Likelihood Ratio (LR) for each BE following (Zhou et al., 2005)." ></td>
	<td class="line x" title="116:217	Sorting BEs according to their LR scores produced a BE-ranked list." ></td>
	<td class="line x" title="117:217	Our goal is to generate a summary that will answer the user questions." ></td>
	<td class="line x" title="118:217	The ranked list of BEs in this way contains important BEs at the top which may or may not be relevant to the user questions." ></td>
	<td class="line x" title="119:217	We filter those BEs by checking whether they contain any word which is a query word or a QueryRelatedWords (defined in Section 3.2)." ></td>
	<td class="line x" title="120:217	The score of a sentence is the sum of its BE scores divided by the number of BEs in the sentence." ></td>
	<td class="line x" title="121:217	3.5.2 Syntactic Feature Encoding syntactic structure is easier and straight forward." ></td>
	<td class="line x" title="122:217	Given a sentence (or query), we first parse it into a syntactic tree using a syntactic parser (i.e. Charniak parser) and then we calculate the similarity between the two trees using the tree kernel defined in (Collins and Duffy, 2001)." ></td>
	<td class="line x" title="123:217	3.5.3 Shallow-semantic Feature Though introducing BE and syntactic information gives an improvement on BOW by the use of dependency/syntactic parses, but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even 5BE website:http://www.isi.edu/ cyl/BE Figure 1: Example of semantic trees paragraphs." ></td>
	<td class="line x" title="124:217	Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007)." ></td>
	<td class="line x" title="125:217	Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003)." ></td>
	<td class="line x" title="126:217	For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like Figure 1 which we call Semantic Tree (ST)." ></td>
	<td class="line x" title="127:217	In the semantic tree, arguments are replaced with the most important word-often referred to as the semantic head." ></td>
	<td class="line x" title="128:217	The sentences may contain one or more subordinate clauses." ></td>
	<td class="line x" title="129:217	For example the sentence, the Vatican, located wholly within Italy uses the Italian lira as their currency. gives the STs as in Figure 2." ></td>
	<td class="line x" title="130:217	As we can see in Figure 2(A), when an argument node corresponds to an entire subordinate clause, we label its leaf with ST, e.g. the leaf of ARG0." ></td>
	<td class="line x" title="131:217	Such ST node is actually the root of the subordinate clause in Figure 2(B)." ></td>
	<td class="line x" title="132:217	If taken separately, such STs do not express the whole meaning of the sentence, hence it is more accurate to define a single structure encoding the dependency between the two predicates as in Figure 2(C)." ></td>
	<td class="line x" title="133:217	We refer to this kind of nested STs as STNs." ></td>
	<td class="line x" title="134:217	Note that, the tree kernel (TK) function defined in (Collins and Duffy, 2001) computes the number of common subtrees between two trees." ></td>
	<td class="line x" title="135:217	Such subtrees are subject to the constraint that their nodes are taken with all or none of the children they have in the original tree." ></td>
	<td class="line x" title="136:217	308 Figure 2: Two STs composing a STN Though, this definition of subtrees makes the TK function appropriate for syntactic trees but at the same time makes it not well suited for the semantic trees (ST) defined above." ></td>
	<td class="line x" title="137:217	For instance, although the two STs of Figure 1 share most of the subtrees rooted in the ST node, the kernel defined above computes only one match (ST ARG0 TARGET ARG1 ARG2) which is not useful." ></td>
	<td class="line x" title="138:217	The critical aspect of the TK function is that the productions of two evaluated nodes have to be identical to allow the match of further descendants." ></td>
	<td class="line x" title="139:217	This means that common substructures cannot be composed by a node with only some of its children as an effective ST representation would require." ></td>
	<td class="line x" title="140:217	(Moschitti et al., 2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a ST. We followed the similar approach to compute the SSTK." ></td>
	<td class="line x" title="141:217	4 Ranking Sentences In this section, we describe the scoring techniques in detail." ></td>
	<td class="line x" title="142:217	4.1 Learning Feature-weights: A Local Search Strategy In order to fine-tune the weights of the features, we used a local search technique with simulated annealing to find the global maximum." ></td>
	<td class="line x" title="143:217	Initially, we set all the featureweights, w1,,wn, as equal values (i.e. 0.5) (see Algorithm 1)." ></td>
	<td class="line x" title="144:217	Based on the current weights we score the sentences and generate summaries accordingly." ></td>
	<td class="line oc" title="145:217	We evaluate the summaries using the automatic evaluation tool ROUGE (Lin, 2004) (described in Section 6) and the ROUGE value works as the feedback to our learning loop." ></td>
	<td class="line o" title="146:217	Our learning system tries to maximize the ROUGE score in every step by changing the weights individually by a specific step size (i.e. 0.01)." ></td>
	<td class="line x" title="147:217	That means, to learn weight wi, we change the value of wi keeping all other weight values (wjjnegationslash=i) stagnant." ></td>
	<td class="line o" title="148:217	For each weight wi, the algorithm achieves the local maximum of ROUGE value." ></td>
	<td class="line x" title="149:217	In order to find the global maximum we ran this algorithm multiple times with different random choices of initial values (i.e. simulated annealing)." ></td>
	<td class="line o" title="150:217	Input: Stepsize l, Weight Initial Value v Output: A vector vectorw of learned weights Initialize the weight values wi to v. for i 1 ton do rg1 = rg2 = prev = 0 while (true) do scoreSentences(vectorw) generateSummaries() rg2 = evaluateROUGE() if rg1rg2 thenprev = w i wi+ = l rg1 = rg2 else break end end end return vectorwAlgorithm 1: Tuning weights using Local Search technique Once we have learned the feature-weights, our empirical method computes the final scores for the sentences using the formula: scorei = vectorxi.vectorw (1) Where, vectorxi is the feature vector for i-th sentence, vectorw is the weight vector and scorei is the score of i-th sentence." ></td>
	<td class="line x" title="151:217	4.2 K-means Learning We start with a set of initial cluster centers and go through several iterations of assigning each object to the cluster whose center is closest." ></td>
	<td class="line x" title="152:217	After all objects have been assigned, we recompute the center of each cluster as the centroid or mean () of its members." ></td>
	<td class="line x" title="153:217	Once we have learned the means of the clusters using the k-means algorithm, our next task is to rank the sentences according to a probability model." ></td>
	<td class="line x" title="154:217	We have used Bayesian model in order to do so." ></td>
	<td class="line x" title="155:217	Bayes law says: P(qk|vectorx,) = p(vectorx|qk,)P(qk|)summationtextK k=1p(vectorx|qk,)p(qk|) (2) where qk is a class, vectorx is a feature vector representing a sentence and  is the parameter set of all class models." ></td>
	<td class="line x" title="156:217	We set the weights of the clusters as equiprobable (i.e. P(qk|) = 1/K)." ></td>
	<td class="line x" title="157:217	We calculated 309 p(x|qk,) using the gaussian probability distribution." ></td>
	<td class="line x" title="158:217	The gaussian probability density function (pdf) for the d-dimensional random variablevectorxis given by: p(,)(vectorx) = e 1 2 (vectorx) T1(vectorx) 2pidradicalbigdet() (3) where , the mean vector and , the covariance matrix are the parameters of the gaussian distribution." ></td>
	<td class="line x" title="159:217	We get the means () from the k-means algorithm and we calculate the covariance matrix using the unbiased covariance estimation:  = 1 N1 Nsummationdisplay i=1 (xjj)(xii)T (4) 4.3 EM Learning EM is an iterative two step procedure: 1." ></td>
	<td class="line x" title="160:217	Expectation-step and 2." ></td>
	<td class="line x" title="161:217	Maximization-step." ></td>
	<td class="line x" title="162:217	In the expectation step, we compute expected values for the hidden variables hi,j which are cluster membership probabilities." ></td>
	<td class="line x" title="163:217	Given the current parameters, we compute how likely an object belongs to any of the clusters." ></td>
	<td class="line x" title="164:217	The maximization step computes the most likely parameters of the model given the cluster membership probabilities." ></td>
	<td class="line x" title="165:217	The data-points are considered to be generated by a mixture model of k-gaussians of the form: P(vectorx) = ksummationdisplay i=1 P(C = i)P(vectorx|i,i) (5) Where the total likelihood of model  with k components given the observed data points, X = x1,,xn is: L(|X) = nproductdisplay i=1 ksummationdisplay j=1 P(C = j)P(xi|j) = nproductdisplay i=1 ksummationdisplay j=1 wjP(xi|j,j)  nsummationdisplay i=1 log ksummationdisplay j=1 wjP(xi|j,j) where P is the probability density function (i.e. eq 3)." ></td>
	<td class="line x" title="166:217	j and j are the mean and covariance matrix of component j, respectively." ></td>
	<td class="line x" title="167:217	Each component contributes a proportion, wj, of the total population, such that: summationtextKj=1wj = 1." ></td>
	<td class="line x" title="168:217	However, a significant problem with the EM algorithm is that it converges to a local maximum of the likelihood function and hence the quality of the result depends on the initialization." ></td>
	<td class="line x" title="169:217	In order to get good results from using random starting values, we can run the EM algorithm several times and choose the initial configuration for which we get the maximum log likelihood among all configurations." ></td>
	<td class="line x" title="170:217	Choosing the best one among several runs is very computer intensive process." ></td>
	<td class="line x" title="171:217	So, to improve the outcome of the EM algorithm on gaussian mixture models it is necessary to find a better method of estimating initial means for the components." ></td>
	<td class="line x" title="172:217	To achieve this aim we explored the widely used k-means algorithm as a cluster (means) finding method." ></td>
	<td class="line x" title="173:217	That means, the means found by kmeans clustering above will be utilized as the initial means for EM and we calculate the initial covariance matrices using the unbiased covariance estimation procedure (eq:4)." ></td>
	<td class="line x" title="174:217	Once the sentences are clustered by EM algorithm, we filter out the sentences which are not query-relevant by checking their probabilities, P(qr|xi,) where, qr denotes the cluster queryrelevant." ></td>
	<td class="line x" title="175:217	If for a sentence xi, P(qr|xi,) > 0.5 then xi is considered to be query-relevant." ></td>
	<td class="line x" title="176:217	Our next task is to rank the query-relevant sentences in order to include them in the summary." ></td>
	<td class="line x" title="177:217	This can be done easily by multiplying the feature vector vectorxi with the weight vector vectorw that we learned by the local search technique (eq:1)." ></td>
	<td class="line x" title="178:217	5 Redundancy Checking When many of the competing sentences are included in the summary, the issue of information overlap between parts of the output comes up, and a mechanism for addressing redundancy is needed." ></td>
	<td class="line x" title="179:217	Therefore, our summarization systems employ a final level of analysis: before being added to the final output, the sentences deemed to be important are compared to each other and only those that are not too similar to other candidates are included in the final answer or summary." ></td>
	<td class="line x" title="180:217	Following (Zhou et al., 2005), we modeled this by BE overlap between an intermediate summary and a to-be-added candidate summary 310 sentence." ></td>
	<td class="line x" title="181:217	We call this overlap ratio R, where R is between 0 and 1 inclusively." ></td>
	<td class="line x" title="182:217	Setting R = 0.7 means that a candidate summary sentence, s, can be added to an intermediate summary, S, if the sentence has a BE overlap ratio less than or equal to 0.7." ></td>
	<td class="line x" title="183:217	6 Experimental Evaluation 6.1 Evaluation Setup We used the main task of Document Understanding Conference (DUC) 2007 for evaluation." ></td>
	<td class="line x" title="184:217	The task was: Given a complex question (topic description) and a collection of relevant documents, the task is to synthesize a fluent, well-organized 250-word summary of the documents that answers the question(s) in the topic. NIST assessors developed topics of interest to them and choose a set of 25 documents relevant (document cluster) to each topic." ></td>
	<td class="line x" title="185:217	Each topic and its document cluster were given to 4 different NIST assessors." ></td>
	<td class="line x" title="186:217	The assessor created a 250-word summary of the document cluster that satisfies the information need expressed in the topic statement." ></td>
	<td class="line x" title="187:217	These multiple reference summaries are used in the evaluation of summary content." ></td>
	<td class="line pc" title="188:217	We carried out automatic evaluation of our summaries using ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation." ></td>
	<td class="line o" title="189:217	It measures summary quality by counting overlapping units such as the n-grams (ROUGE-N), word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary." ></td>
	<td class="line o" title="190:217	ROUGE parameters were set as the same as DUC 2007 evaluation setup." ></td>
	<td class="line x" title="191:217	One purpose of our experiments is to study the impact of different features for complex question answering task." ></td>
	<td class="line x" title="192:217	To accomplish this, we generated summaries for the topics of DUC 2007 by each of our seven systems defined as below: The LEX system generates summaries based on only lexical features: n-gram (n=1,2,3,4), LCS, WLCS, skip bi-gram, head, head synonym." ></td>
	<td class="line x" title="193:217	The LSEM system considers only lexical semantic features: synonym, hypernym/hyponym, gloss, dependency-based and proximity-based similarity." ></td>
	<td class="line x" title="194:217	The COS system generates summary based on the graph-based method." ></td>
	<td class="line x" title="195:217	The SYS1 system considers all the features except the BE, syntactic and semantic features." ></td>
	<td class="line x" title="196:217	The SYS2 system considers all the features except the syntactic and semantic features." ></td>
	<td class="line x" title="197:217	The SYS3 considers all the features except the semantic and the ALL6 system generates summaries taking all the features into account." ></td>
	<td class="line x" title="198:217	6.2 Evaluation Results Table 17 to Table 3, Table 4 to Table 6 and Table 7 to Table 9 show the evaluation measures for k-means, EM and empirical approaches respectively." ></td>
	<td class="line o" title="199:217	As Table 1 shows, in k-means, SYS2 gets 0-21%, SYS3 gets 4-32% and ALL gets 3-36% improvement in ROUGE-2 scores over the SYS1 system." ></td>
	<td class="line o" title="200:217	We get best ROUGE-W (Table 2) scores for SYS2 (i.e. including BE) but SYS3 and ALL do not perform well in this case." ></td>
	<td class="line o" title="201:217	SYS2 improves the ROUGE-W F-score by 1% over SYS1." ></td>
	<td class="line o" title="202:217	We do not get any improvement in ROUGE-SU (Table 3) scores when we include any kind of syntactic/semantic structures." ></td>
	<td class="line x" title="203:217	The case is different for EM and empirical approaches." ></td>
	<td class="line x" title="204:217	Here, in every case we get a significant amount of improvement when we include the syntactic and/or semantic features." ></td>
	<td class="line x" title="205:217	For EM (Table 4 to Table 6), the ratio of improvement in F-scores over SYS1 is: 1-3% for SYS2, 3-15% for SYS3 and 224% for ALL." ></td>
	<td class="line x" title="206:217	In our empirical approach (Table 7 to Table 9), SYS2, SYS3 and ALL improve the Fscores by 3-11%, 7-15% and 8-19% over SYS1 respectively." ></td>
	<td class="line x" title="207:217	These results clearly indicate the positive impact of the syntactic/semantic features for complex question answering task." ></td>
	<td class="line o" title="208:217	Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.074 0.077 0.086 0.075 0.075 0.078 0.077 P 0.081 0.084 0.093 0.081 0.098 0.107 0.110 F 0.078 0.080 0.089 0.078 0.085 0.090 0.090 Table 1: ROUGE-2 measures in k-means learning Table 10 shows the F-scores of the ROUGE measures for one baseline system, the best system in DUC 2007 and our three scoring techniques considering all features." ></td>
	<td class="line o" title="209:217	The baseline system gener6SYS2, SYS3 and ALL systems show the impact of BE, syntactic and semantic features respectively 7R stands for Recall, P stands for Precision and F stands for F-score 311 Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.098 0.097 0.101 0.099 0.101 0.097 0.097 P 0.195 0.194 0.200 0.237 0.233 0.241 0.237 F 0.130 0.129 0.134 0.140 0.141 0.139 0.138 Table 2: ROUGE-W measures in k-means learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.131 0.127 0.139 0.136 0.135 0.135 0.135 P 0.155 0.152 0.162 0.176 0.171 0.174 0.174 F 0.142 0.139 0.150 0.153 0.151 0.152 0.152 Table 3: ROUGE-SU in k-means learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.089 0.080 0.087 0.085 0.085 0.089 0.091 P 0.096 0.087 0.094 0.092 0.095 0.116 0.138 F 0.092 0.083 0.090 0.088 0.090 0.101 0.109 Table 4: ROUGE-2 measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.103 0.096 0.101 0.102 0.101 0.102 0.101 P 0.205 0.193 0.200 0.203 0.218 0.222 0.223 F 0.137 0.128 0.134 0.136 0.138 0.139 0.139 Table 5: ROUGE-W measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.146 0.128 0.138 0.143 0.144 0.145 0.144 P 0.171 0.153 0.162 0.168 0.177 0.186 0.185 F 0.157 0.140 0.149 0.154 0.159 0.163 0.162 Table 6: ROUGE-SU measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.086 0.080 0.087 0.087 0.090 0.095 0.099 P 0.093 0.087 0.094 0.094 0.112 0.115 0.116 F 0.089 0.083 0.090 0.090 0.100 0.104 0.107 Table 7: ROUGE-2 in empirical approach Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.102 0.096 0.101 0.102 0.102 0.104 0.105 P 0.203 0.193 0.200 0.204 0.239 0.246 0.247 F 0.135 0.128 0.134 0.137 0.143 0.147 0.148 Table 8: ROUGE-W in empirical approach Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.144 0.129 0.138 0.145 0.146 0.149 0.150 P 0.169 0.153 0.162 0.171 0.182 0.195 0.197 F 0.155 0.140 0.150 0.157 0.162 0.169 0.170 Table 9: ROUGE-SU in empirical approach ates summaries by returning all the leading sentences (up to 250 words) in the TEXT field of the most recent document(s)." ></td>
	<td class="line x" title="210:217	It shows that the empirical approach outperforms the other two learning techniques and EM performs better than k-means algorithm." ></td>
	<td class="line x" title="211:217	EM improves the F-scores over k-means by 0.7-22.5%." ></td>
	<td class="line x" title="212:217	Empirical approach improves the Fscores over k-means and EM by 5.9-20.2% and 3.56.5% respectively." ></td>
	<td class="line o" title="213:217	Comparing with the DUC 2007 participants our systems achieve top scores and for some ROUGE measures there is no statistically significant difference between our system and the best DUC 2007 system." ></td>
	<td class="line o" title="214:217	System ROUGE1 ROUGE2 ROUGEW ROUGESU Baseline 0.335 0.065 0.114 0.113 Best 0.438 0.122 0.153 0.174 k-means 0.390 0.090 0.138 0.152 EM 0.399 0.109 0.139 0.162 Empirical 0.413 0.107 0.148 0.170 Table 10: F-measures for different systems 7 Conclusion and Future Work Our experiments show the following: (a) our approaches achieve promising results, (b) empirical approach outperforms the other two learning and EM performs better than the k-means algorithm for this particular task, and (c) our systems achieve better results when we include BE, syntactic and semantic features." ></td>
	<td class="line x" title="215:217	In future, we have the plan to decompose the complex questions into several simple questions before measuring the similarity between the document sentence and the query sentence." ></td>
	<td class="line x" title="216:217	We expect that by decomposing complex questions into the sets of subquestions that they entail, systems can improve the average quality of answers returned and achieve better coverage for the question as a whole." ></td>
	<td class="line x" title="217:217	312" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1035
Automatic Extraction of Briefing Templates
Das, Dipanjan;Kumar, Mohit;Rudnicky, Alexander I.;"></td>
	<td class="line x" title="1:182	Automatic Extraction of Briefing Templates Dipanjan Das Mohit Kumar Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213, USA fdipanjan, mohitkum, airg@cs.cmu.edu Alexander I. Rudnicky Abstract An approach to solving the problem of automatic briefing generation from non-textual events can be segmenting the task into two major steps, namely, extraction of briefing templates and learning aggregators that collate information from events and automatically fill up the templates." ></td>
	<td class="line x" title="2:182	In this paper, we describe two novel unsupervised approaches for extracting briefing templates from human written reports." ></td>
	<td class="line x" title="3:182	Since the problem is non-standard, we define our own criteria for evaluating the approaches and demonstrate that both approaches are effective in extracting domain relevant templates with promising accuracies." ></td>
	<td class="line x" title="4:182	1 Introduction Automated briefing generation from non-textual events is an unsolved problem that currently lacks a standard approach in the NLP community." ></td>
	<td class="line x" title="5:182	Broadly, it intersects the problem of language generation from structured data and summarization." ></td>
	<td class="line x" title="6:182	The problem is relevant in several domains where the user has to repeatedly write reports based on events in the domain, for example, weather reports (Reiter et al., 2005), medical reports (Elhadad et al., 2005), weekly class project reports (Kumar et al., 2007) and so forth." ></td>
	<td class="line x" title="7:182	On observing the data from these domains, we notice a templatized nature of report items." ></td>
	<td class="line x" title="8:182	Examples (1)-(3) demonstrate equivalents in a particular domain (Reiter et al., 2005)." ></td>
	<td class="line x" title="9:182	(1) [A warm front] from [Iceland] to [northern Scotland] will move [SE] across [the northern North Sea] [today and tomorrow] (2) [A warm front] from [Iceland] to [the Faeroes] will move [ENE] across [the Norwegian Sea] [this evening] (3) [A ridge] from [the British Isles] to [Iceland] will move [NE] across [the North Sea] [today] In each sentence, the phrases in square brackets at the same relative positions form the slots that take up different values at different occasions." ></td>
	<td class="line x" title="10:182	The corresponding template is shown in (4) with slots containing their respective domain entity types." ></td>
	<td class="line x" title="11:182	Instantiations of (4) may produce (1)-(3) and similar sentences." ></td>
	<td class="line x" title="12:182	This kind of sentence structure motivates an approach of segmenting the problem of closed domain summarization into two major steps of automatic template extraction and learning aggregators, which are pattern detectors that assimilate information from the events, to populate these templates." ></td>
	<td class="line x" title="13:182	(4) [PRESSURE ENTITY] from [LOCATION] to [LOCATION] will move [DIRECTION] across [LOCATION] [TIME] In the current work we address the first problem of automatically extracting domain templates from human written reports." ></td>
	<td class="line x" title="14:182	We take a two-step approach to the problem; first, we cluster report sentences based on similarity and second, we extract template(s) corresponding to each cluster by aligning the instances in the cluster." ></td>
	<td class="line oc" title="15:182	We experimented with two independent, arguably complementary techniques for clustering and aligning  a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE (Lin, 2004) based 265 approach that can extract templates containing multiple verbs." ></td>
	<td class="line p" title="16:182	As we will see below, both approaches show promise." ></td>
	<td class="line x" title="17:182	2 Related Work There has been instances of template based summarization in popular Information Extraction (IE) evaluations like MUC (Marsh & Perzanowski, 1998; Onyshkevych, 1994) and ACE (ACE, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation." ></td>
	<td class="line x" title="18:182	(Riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the patterns do not resemble the templates that we aim to extract." ></td>
	<td class="line x" title="19:182	(Yangarber et al., 2000) describe another system called ExDisco, that extracts event patterns from un-annotated text starting from seed patterns." ></td>
	<td class="line x" title="20:182	Once again, the text analysis is not deep and the patterns extracted are not sentence surface forms." ></td>
	<td class="line x" title="21:182	(Collier, 1998) proposed automatic domain template extraction for IE purposes where MUC type templates for particular types of events were constructed." ></td>
	<td class="line x" title="22:182	The method relies on the idea from (Luhn, 1958) where statistically significant words of a corpus were extracted." ></td>
	<td class="line x" title="23:182	Based on these words, sentences containing them were chosen and aligned using subject-object-verb patterns." ></td>
	<td class="line x" title="24:182	However, this method did not look at arbitrary syntactic patterns." ></td>
	<td class="line x" title="25:182	(Filatova et al., 2006) improved the paradigm by looking at the most frequent verbs occurring in a corpus and aligning subtrees containing the verb, by using the syntactic parses as a similarity metric." ></td>
	<td class="line x" title="26:182	However, long distance dependencies of verbs with constituents were not looked at and deep semantic analysis was not performed on the sentences to find out similar verb subcategorization frames." ></td>
	<td class="line x" title="27:182	In contrast, in our predicate argument based approach we look into deeper semantic structures, and align sentences not only based on similar syntactic parses, but also based on the constituents roles with respect to the main predicate." ></td>
	<td class="line x" title="28:182	Also, they relied on typical Named Entities (NEs) like location, organization, person etc. and included another entity that they termed as NUMBER." ></td>
	<td class="line x" title="29:182	However, for specific domains like weather forecasts, medical reports or student reports, more varied domain entities form slots in templates, as we observe in our data; hence, existence of a module handling domain specific entities become essential for such a task." ></td>
	<td class="line x" title="30:182	(Surdeanu et al., 2003) identify arguments for predicates in a sentence and emphasize how semantic role information may assist in IE related tasks, but their primary focus remained on the extraction of PropBank (Kingsbury et al., 2002) type semantic roles." ></td>
	<td class="line o" title="31:182	To our knowledge, the ROUGE metric has not been used for automatic extraction of templates." ></td>
	<td class="line x" title="32:182	3 The Data 3.1 Data Description Since our focus is on creating summary items from events or structured data rather than from text, we used a corpus from the domain of weather forecasts (Reiter et al., 2005)." ></td>
	<td class="line x" title="33:182	This is a freely available parallel corpus1 consisting of weather data and human written forecasts describing them." ></td>
	<td class="line x" title="34:182	The dataset showed regularity in sentence structure and belonged to a closed domain, making the variations in surface forms more constrained than completely free text." ></td>
	<td class="line x" title="35:182	After sentence segmentation we arrived at a set of 3262 sentences." ></td>
	<td class="line x" title="36:182	From this set, we selected 3000 for template extraction and kept aside 262 sentences for testing." ></td>
	<td class="line x" title="37:182	3.2 Preprocessing For semantic analysis, we used the ASSERT toolkit (Pradhan et al., 2004) that produces shallow semantic parses using the PropBank conventions." ></td>
	<td class="line x" title="38:182	As a by product, it also produces syntactic parses of sentences, using the Charniak parser (Charniak, 2001)." ></td>
	<td class="line x" title="39:182	For each sentence, we maintained a part-of-speech tagged (leaves of the parse tree), parsed, baseNP2 tagged and semantic role tagged version." ></td>
	<td class="line x" title="40:182	The baseNPs were retrieved by pruning the parse trees and not by using a separate NP chunker." ></td>
	<td class="line x" title="41:182	The reason for having a baseNP tagged corpus will become clear as we go into the detail of our template extraction techniques." ></td>
	<td class="line x" title="42:182	Figure 1 shows a typical output from the Charniak parser and Figure 2 shows the same tree with nodes under the baseNPs pruned." ></td>
	<td class="line x" title="43:182	We identified the need to have a domain entity tagger for matching constituents in the sentences." ></td>
	<td class="line x" title="44:182	1http://www.csd.abdn.ac.uk/research/sumtime/ 2A baseNP is a noun-phrase with no internal noun-phrase 266 ADVP IN A low theoverNorwegianSeawillmoveNorthandweaken DTNN DT JJ NNMDVBRBCCVB NP NP PP NP S VP VP VP VP Figure 1: Parse tree for a sentence in the data." ></td>
	<td class="line x" title="45:182	ADVP IN A low theoverNorwegianSeawillmoveNorthandweaken MDVBRBCCVB NP NP PP NP S VP VP VP VP Figure 2: Pruned parse tree for a sentence in the corpus Any tagger for named entities was not suitable for weather forecasts since unique constituent types assumed significance unlike newswire data." ></td>
	<td class="line x" title="46:182	Since the development of such a tagger was beyond the scope of the present work, we developed a module that took baseNP tagged sentences as input and produced tags across words and baseNPs that were domain entities." ></td>
	<td class="line x" title="47:182	The development of such a module by hand was easy because of a limited vocabulary (< 1000 words) of the data and the closed set nature of most entity types (e.g the direction entity could take up a finite set of values)." ></td>
	<td class="line x" title="48:182	From inspection, thirteen distinct entity types were recognized in the domain." ></td>
	<td class="line x" title="49:182	Figure 3 shows an example output from the entity recognizer with the sentence from Figure 2 as input." ></td>
	<td class="line x" title="50:182	[ A low ] DIRECTIONand weaken A low over the Norwegian Sea will move North and weaken ENTITY RECOGNIZER LOCATIONover [ the Norwegian Sea ]PRESSURE ENTITY will move [ North ] Figure 3: Example output of the entity recognizer We now provide a detailed description of our clustering and template extraction algorithms." ></td>
	<td class="line x" title="51:182	4 Approach and Experiments We adopted two parallel approaches." ></td>
	<td class="line x" title="52:182	First, we investigated a predicate-argument based approach where we consider the set of all propositions in our dataset, and cluster them based on their verb subcategorization frame." ></td>
	<td class="line o" title="53:182	Second, we used ROUGE, a summarization evaluation metric that is generally used to compare machine generated and human written summaries." ></td>
	<td class="line o" title="54:182	We uniquely used this metric for clustering similar summary items, after abstracting the surface forms to a representation that facilitates comparison of a pair of sentences." ></td>
	<td class="line o" title="55:182	The following subsections detail both the techniques." ></td>
	<td class="line x" title="56:182	4.1 A Predicate-Argument Based Approach Analysis of predicate-argument structures seemed appropriate for template extraction for a few reasons: Firstly, complicated sentences with multiple verbs are broken down into propositions by a semantic role labeler." ></td>
	<td class="line x" title="57:182	The propositions3 are better generalizable units than whole sentences across a corpus." ></td>
	<td class="line x" title="58:182	Secondly, long distance dependencies of constituents with a particular verb, are captured well by a semantic role labeler." ></td>
	<td class="line x" title="59:182	Finally, if verbs are considered to be the center of events, then groups of sentences with the same semantic role sequences seemed to form clusters conveying similar meaning." ></td>
	<td class="line x" title="60:182	We explain the complete algorithm for template extraction in the following subsections." ></td>
	<td class="line x" title="61:182	(5) [ARG0 A low over the Norwegian Sea] [AGM-MOD will] [TARGET move ] [ARGM-DIR North ] and weaken (6) [ARG0 A high pressure area ] [AGM-MOD will ] [TARGET move] [ARGM-DIR southwestwards] and build on Sunday." ></td>
	<td class="line x" title="62:182	4.1.1 Verb based clustering We performed a verb based clustering as the first step." ></td>
	<td class="line x" title="63:182	Instead of considering a unique set of verbs, we considered related verbs as a single verb type." ></td>
	<td class="line x" title="64:182	The relatedness of verbs was derived from Wordnet (Fellbaum, 1998), by merging verbs that appear in the same synset." ></td>
	<td class="line x" title="65:182	This kind of clustering is not 3sentence fragments with one verb 267 ideal in a corpus containing a huge variation in event streams, like newswire." ></td>
	<td class="line x" title="66:182	However, the results were good for the weather domain where the number of verbs used is limited." ></td>
	<td class="line x" title="67:182	The grouping procedure resulted in a set of 82 clusters with 6632 propositions." ></td>
	<td class="line x" title="68:182	4.1.2 Matching Role Sequences Each verb cluster was considered next." ></td>
	<td class="line x" title="69:182	Instead of finding structural similarities of the propositions in one go, we first considered the semantic role sequences for each proposition." ></td>
	<td class="line x" title="70:182	We searched for propositions that had exactly similar role sequences and grouped them together." ></td>
	<td class="line x" title="71:182	To give an example, both sentences 5 and 6 have the matching role sequence ARG0ARGM-MODTARGETARGMDIR." ></td>
	<td class="line x" title="72:182	The intuition behind such clustering is straightforward." ></td>
	<td class="line x" title="73:182	Propositions with a matching verb type with the same set of roles arranged in a similar fashion would convey similar meaning." ></td>
	<td class="line x" title="74:182	We observed that this was indeed true for sentences tagged with correct semantic role labels." ></td>
	<td class="line x" title="75:182	Instead of considering matching role sequences for a set of propositions, we could as well have considered matching bag of roles." ></td>
	<td class="line x" title="76:182	However, for the present corpus, we decided to use strict role sequence instead because of the sentences rigid structure and absence of any passive sentences." ></td>
	<td class="line x" title="77:182	This subclustering step resulted in smaller clusters, and many of them contained a single proposition." ></td>
	<td class="line x" title="78:182	We threw out these clusters on the assumption that the human summarizers did not necessarily have a template in mind while writing those summary items." ></td>
	<td class="line x" title="79:182	As a result, many verb types were eliminated and only 33 verb-type clusters containing several subclusters each were produced." ></td>
	<td class="line x" title="80:182	4.1.3 Looking inside Roles Groups of propositions with the same verb-type and semantic role sequences were considered in this step." ></td>
	<td class="line x" title="81:182	For each group, we looked at individual semantic roles to find out similarity between them." ></td>
	<td class="line x" title="82:182	We decided at first to look at syntactic parse tree similarities between constituents." ></td>
	<td class="line x" title="83:182	However, there is a need to decide at what level of abstraction should one consider matching the parse trees." ></td>
	<td class="line x" title="84:182	After considerable speculation, we decided on pruning the constituents parse trees till the level of baseNPs and then match the resulting tag sequences." ></td>
	<td class="line x" title="85:182	Scotland IN A low theover Sea NP NP PP NP NP NP PP NP Norwegian A frontal troughINacross Figure 4: Matching ARG0s for two propositions LOCATIONIN A low theover SeaNorwegianA frontal troughINacrossScotland PRESSURE ENTITY LOCATION PRESSURE ENTITY Figure 5: Abstracted tag sequences for two constituents The parses with pruned trees from the preprocessing steps provide the necessary information for constituent matching." ></td>
	<td class="line x" title="86:182	Figure 4 shows matching syntactic trees for two ARG0s from two propositions of a cluster." ></td>
	<td class="line x" title="87:182	It is at this step that we use the domain entity tags to abstract away the constituents syntactic tags." ></td>
	<td class="line x" title="88:182	Figure 5 shows the constituents of Figure 4 with the tree structure reduced to tag sequences and domain entity types replacing the tags whenever necessary." ></td>
	<td class="line x" title="89:182	This abstraction step produces a number of unique domain entity augmented tag sequences for a particular semantic role." ></td>
	<td class="line x" title="90:182	As a final step of template generation, we concatenate these abstracted constituent types for all the semantic roles in the given group." ></td>
	<td class="line x" title="91:182	To focus on template-like structures we only consider tag sequences that occur twice or more in the group." ></td>
	<td class="line x" title="92:182	The templates produced at the end of this step are essentially tag sequences interspersed with domain entities." ></td>
	<td class="line x" title="93:182	In our definition of templates, the slots are the entity types and the fixed parts are constituted by word(s) used by the human experts for a particular tag sequence." ></td>
	<td class="line x" title="94:182	Figure 6 shows some example templates." ></td>
	<td class="line x" title="95:182	The upper case words in the figure correspond to the domain entities identified by the entity tagger and they form the slots in the templates." ></td>
	<td class="line x" title="96:182	A total of 209 templates were produced." ></td>
	<td class="line x" title="97:182	268 PRESSURE_ENTITY to DIRECTION of LOCATION will drift slowly WAVE will run_0.5/move_0.5 DIRECTION then DIRECTION Associated PRESSURE_ENTITYwill move DIRECTION across LOCATION TIME PRESSURE_ENTITY expected over LOCATION by_0.5/on_0.5 DAY Figure 6: Example Templates." ></td>
	<td class="line x" title="98:182	Upper case tokens correspond to slots." ></td>
	<td class="line x" title="99:182	For fixed parts, when there is a choice between words, the probability of the occurrence of words in that particular syntactic structure are tagged alongside." ></td>
	<td class="line oc" title="100:182	4.2 A ROUGE Based Approach ROUGE (Lin, 2004) is the standard automatic evaluation metric in the Summarization community." ></td>
	<td class="line o" title="101:182	It is derived from the BLEU (Papineni et al., 2001) score which is the evaluation metric used in the Machine Translation community." ></td>
	<td class="line o" title="102:182	The underlying idea in the metric is comparing the candidate and the reference sentences (or summaries) based on their token co-occurrence statistics." ></td>
	<td class="line x" title="103:182	For example, a unigram based measure would compare the vocabulary overlap between the candidate and reference sentences." ></td>
	<td class="line o" title="104:182	Thus, intuitively, we may use the ROUGE score as a measure for clustering the sentences." ></td>
	<td class="line p" title="105:182	Amongst the various ROUGE statistics, the most appealing is Weighted Longest Common Subsequence(WLCS)." ></td>
	<td class="line o" title="106:182	WLCS favors contiguous LCS which corresponds to the intuition of finding the common template." ></td>
	<td class="line p" title="107:182	We experimented with other ROUGE statistics but we got better and easily interpretable results using WLCS and so we chose it as the final metric." ></td>
	<td class="line x" title="108:182	In all the approaches the data was first preprocessed (baseNP and NE tagged) as described in the previous subsection." ></td>
	<td class="line o" title="109:182	In the following subsections, we describe the various clustering techniques that we tried using the ROUGE score followed by the alignment technique." ></td>
	<td class="line o" title="110:182	4.2.1 Clustering Unsupervised Clustering: As the ROUGE score defines a distance metric, we can use this score for doing unsupervised clustering." ></td>
	<td class="line x" title="111:182	We tried hierarchical clustering approaches but did not obtain good clusters, evaluated empirically." ></td>
	<td class="line x" title="112:182	In empirical evaluation, we manually looked at the output clusters and made a judgement call whether the candidate clusters are reasonably coherent and potentially correspond to templates." ></td>
	<td class="line x" title="113:182	The reason for the poor performance of the approach was the classical parameter estimation problem of determining a priori the number of clusters." ></td>
	<td class="line x" title="114:182	We could not find an elegant solution for the problem without losing the motivation of an automated approach." ></td>
	<td class="line x" title="115:182	Figure 7: Deterministic clustering based on Graph connectivity." ></td>
	<td class="line x" title="116:182	In the figure the squares with the same pattern belong to the same cluster." ></td>
	<td class="line x" title="117:182	Non-parametric Unsupervised Clustering: Since the unsupervised technique did not give good results, we experimented with a nonparametric clustering approach, namely, CrossAssociation(Chakrabarti et al., 2004)." ></td>
	<td class="line x" title="118:182	It is a non-parametric unsupervised clustering algorithm for similarity (boolean) matrices." ></td>
	<td class="line o" title="119:182	We obtain the similarity matrix in our domain by thresholding the ROUGE similarity score matrix." ></td>
	<td class="line n" title="120:182	This technique also did not give us good clusters, evaluated empirically." ></td>
	<td class="line x" title="121:182	The plausible reason for the poor performance seems to be that the technique is based on MDL (Minimum Description Length) principle." ></td>
	<td class="line x" title="122:182	Since in our domain we expect a large number of clusters with small membership along many singletons, MDL principle is not likely to perform well." ></td>
	<td class="line x" title="123:182	Deterministic Clustering: As the unsupervised techniques did not perform well, we tried deterministic clustering based on graph connectivity." ></td>
	<td class="line x" title="124:182	The underlying intuition is that all the sentences X1:::n that are similar to any other sentence Yi should be in the same cluster even though Xj and Xk may not be similar to each other." ></td>
	<td class="line x" title="125:182	Thus we find the connected components in the similarity matrix and label them as individual clusters.4 4This approach is similar to agglomerative single linkage clustering." ></td>
	<td class="line o" title="126:182	269 We created a similarity matrix by thresholding the ROUGE score." ></td>
	<td class="line n" title="127:182	In the event, the clusters obtained by this approach were also not good, evaluated empirically." ></td>
	<td class="line x" title="128:182	This led us to revisit the similarity function and tune it." ></td>
	<td class="line o" title="129:182	We factored the ROUGE-WLCS score, which is an F-measure score, into its component Precision and Recall scores and experimented with various combinations of using the Precision and Recall scores." ></td>
	<td class="line x" title="130:182	We finally chose a combined Precision and Recall measure (not f-measure) in which both the scores were independently thresholded." ></td>
	<td class="line x" title="131:182	The motivation for the measure is that in our domain we desire to have high precision matches." ></td>
	<td class="line x" title="132:182	Additionally we need to control the length of the sentences in the cluster for which we require a Recall threshold." ></td>
	<td class="line x" title="133:182	Fmeasure (which is the harmonic mean of Precision and Recall) does not give us the required individual control." ></td>
	<td class="line x" title="134:182	We set up our experiments such that while comparing two sentences the longer sentence is always treated as the reference and the shorter one as the candidate." ></td>
	<td class="line x" title="135:182	This helps us in interpreting the Precision/Recall measures better and thresholding them accordingly." ></td>
	<td class="line x" title="136:182	The approach gave us 149 clusters, which looked good on empirical evaluation." ></td>
	<td class="line x" title="137:182	We can argue that using this modified similarity function for previous unsupervised approaches could have given better results, but we did not reevaluate those approaches as our aim of getting a reasonable clustering approach is fulfilled with this simple scheme and tuning the unsupervised approaches can be interesting future work." ></td>
	<td class="line x" title="138:182	4.3 Alignment After obtaining the clusters using the Deterministic approach we needed to find out the template corresponding to each of the cluster." ></td>
	<td class="line x" title="139:182	Fairly intuitively we computed the Longest Common Subsequence(LCS) between the sentences in each cluster which we then claim to be the template corresponding to the cluster." ></td>
	<td class="line x" title="140:182	This resulted in a set of 149 templates, similar to the Predicate Argument based approach, as shown in figure 6." ></td>
	<td class="line x" title="141:182	5 Results 5.1 Evaluation Scheme Since there is no standard way to evaluate template extraction for summary creation, we adopted a mix of subjective and automatic measures for evaluating the templates extracted." ></td>
	<td class="line x" title="142:182	We define precision for this particular problem as: precision = number of domain relevant templatestotal number of extracted templates This is a subjective measure and we undertook a study involving three subjects who were accustomed to the language used in the corpus." ></td>
	<td class="line x" title="143:182	We asked the human subjects to mark each template as relevant or non-relevant to the weather forecast domain." ></td>
	<td class="line x" title="144:182	We also asked them to mark the template as grammatical or ungrammatical if it is non-relevant." ></td>
	<td class="line x" title="145:182	Our other metric for evaluation is automatic recall." ></td>
	<td class="line o" title="146:182	It is based on using the ROUGE-WLCS metric to determine a match between the preprocessed (baseNP and NE tagged) test corpora with the proposed set of correct templates, a set determined by taking an intersection of only the relevant templates marked by each judge." ></td>
	<td class="line o" title="147:182	For the ROUGE based method, the test corpus consists of 262 sentences, while for the predicate-argument based method it consists of a set of 263 propositions extracted from the 262 sentences using ASSERT followed by a filtering of invalid propositions (e.g. ones starting with a verb)." ></td>
	<td class="line o" title="148:182	Amongst different ROUGE scores (precision/recall/f-measure), we consider precision as the criterion for deciding a match and experimented with different thresholding values." ></td>
	<td class="line x" title="149:182	Main Verb Precision Main Verb Precision deepen 0.67 weaken 0.83 expect 0.76 lie 0.57 drift 0.93 continue 0.97 build 0.95 fill 0.80 cross 0.78 move 0.86 Table 1: Precision for top 10 most frequently occurring verbs 5.2 Results: Predicate-Argument Based Approach Table 1 shows the precision values for top 10 most frequently occurring verbs." ></td>
	<td class="line x" title="150:182	(Since a major proportion (> 90%) of the templates are covered by these verbs, we dont show all the precision values; it also helps to contain space.)" ></td>
	<td class="line x" title="151:182	The overall precision value achieved was 84.21%, the inter-rater Fleiss kappa measure (Fleiss, 1971) between the judges being 270  = 0:69, demonstrating substantial agreement." ></td>
	<td class="line x" title="152:182	The precision values are encouraging, and in most cases the reason for low precision is because of erroneous performance of the semantic role labeler system, which is corroborated by the percentage (47.47%) of ungrammatical templates among the irrelevant ones." ></td>
	<td class="line x" title="153:182	Results for the automated recall values are shown in Figure 8, where precision values are varied to observe the recall." ></td>
	<td class="line o" title="154:182	For 0.9 precision in ROUGEWLCS, the recall is 0.3 which shows that there is a 30% near exact coverage over propositions, while for 0.6 precision in ROUGE-WLCS, the recall is an encouraging 81%." ></td>
	<td class="line o" title="155:182	0  0.2  0.4  0.6  0.8  1  0.4 0.5 0.6 0.7 0.8 0.9 Recall Precision Threshold forMatching Test Sentences ROUGESRL Figure 8: Automated Recall based on ROUGEWLCS measure comparing the test corpora with the set of templates extracted by the PredicateArgument (SRL) and the ROUGE based method." ></td>
	<td class="line o" title="156:182	5.3 Results: ROUGE based approach Various precision and recall thresholds for ROUGE were considered for clustering." ></td>
	<td class="line x" title="157:182	We empirically settled on a recall threshold of 0.8 since this produces the set of clusters with optimum number of sentences." ></td>
	<td class="line x" title="158:182	The number of clusters and number of sentences in clusters at this recall values are shown in Figure 9 for various precision thresholds." ></td>
	<td class="line x" title="159:182	Precision was measured in the same way as the predicate argument approach and the value obtained was 76.3%, with Fleiss kappa measure of  = 0:79." ></td>
	<td class="line x" title="160:182	The percentage of ungrammatical templates among the irrelevant ones was 96.7%, strongly indicating that post processing the templates using a parser can, in future, give substantial improvement." ></td>
	<td class="line x" title="161:182	During error analysis, we observed simple grammatical errors in templates; first or last word being preposi 130  140  150  160  170  180  190  0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0  200  400  600  800  1000  1200  1400 No." ></td>
	<td class="line x" title="162:182	of Clusters No." ></td>
	<td class="line x" title="163:182	of Sentences in Clusters Precision Threshold No." ></td>
	<td class="line x" title="164:182	of ClustersNo." ></td>
	<td class="line x" title="165:182	of Sentences in Clusters Figure 9: Number of clusters and total number of sentences in clusters for various Precision Thresholds at Recall Threshold=0.8 tions." ></td>
	<td class="line x" title="166:182	So a fairly simple error recovery module that strips the leading and trailing prepositions was introduced." ></td>
	<td class="line x" title="167:182	20 templates out of the 149 were modified by the error recovery module and they were evaluated again by the three judges." ></td>
	<td class="line x" title="168:182	The precision obtained for the modified templates was 35%, with Fleiss kappa  = 1, boosting the overall precision to 80.98%." ></td>
	<td class="line x" title="169:182	The overall high precision is motivating as this is a fairly general approach that does not require any NLP resources." ></td>
	<td class="line x" title="170:182	Figure 8 shows the automated recall values for the templates and abstracted sentences from the held-out dataset." ></td>
	<td class="line x" title="171:182	For high precision points, the recall is low because there is not an exact match for most cases." ></td>
	<td class="line x" title="172:182	6 Conclusion and Future Work In this paper, we described two new approaches for template extraction for briefing generation." ></td>
	<td class="line x" title="173:182	For both approaches, high precision values indicate that meaningful templates are being extracted." ></td>
	<td class="line x" title="174:182	However, the recall values were moderate and they hint at possible improvements." ></td>
	<td class="line x" title="175:182	An interesting direction of future research is merging the two approaches and have one technique benefit from the other." ></td>
	<td class="line o" title="176:182	The approaches seem complementary as the ROUGE based technique does not use the structure of the sentence at all whereas the predicate-argument approach is heavily dependent on it." ></td>
	<td class="line o" title="177:182	Moreover, the predicate argument based approach gives general templates with one predicate while ROUGE based approach 271 can extract templates containing multiple verbs." ></td>
	<td class="line o" title="178:182	It would also be desirable to establish the generality of the techniques, by using other domains such as newswire, medical reports and others." ></td>
	<td class="line x" title="179:182	Acknowledgements We would like to express our gratitude to William Cohen and Noah Smith for their valuable suggestions and inputs during the course of this work." ></td>
	<td class="line x" title="180:182	We also thank the three anonymous reviewers for helpful suggestions." ></td>
	<td class="line x" title="181:182	This work was supported by DARPA grant NBCHD030010." ></td>
	<td class="line x" title="182:182	The content of the information in this publication does not necessarily reflect the position or the policy of the US Government, and no official endorsement should be inferred." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1063
Automatic Identification of Rhetorical Roles using Conditional Random Fields for Legal Document Summarization
Saravanan, M.;Ravindran, Balaraman;Raman, S.;"></td>
	<td class="line x" title="1:172	Automatic Identification of Rhetorical Roles using Conditional Random Fields for Legal Document Summarization  M. Saravanan  Department of CS & E IIT Madras, Chennai-36 msdess@yahoo.com B. Ravindran Department of CS & E IIT Madras, Chennai-36 ravi@cse.iitm.ac.in S. Raman Department of CS & E IIT Madras, Chennai-36 ramansubra@gmail.com  Abstract In this paper, we propose a machine learning approach to rhetorical role identification from legal documents." ></td>
	<td class="line x" title="2:172	In our approach, we annotate roles in sample documents with the help of legal experts and take them as training data." ></td>
	<td class="line x" title="3:172	Conditional random field model has been trained with the data to perform rhetorical role identification with reinforcement of rich feature sets." ></td>
	<td class="line x" title="4:172	The understanding of structure of a legal document and the application of mathematical model can brings out an effective summary in the final stage." ></td>
	<td class="line x" title="5:172	Other important new findings in this work include that the training of a model for one sub-domain can be extended to another sub-domains with very limited augmentation of feature sets." ></td>
	<td class="line x" title="6:172	Moreover, we can significantly improve extraction-based summarization results by modifying the ranking of sentences with the importance of specific roles." ></td>
	<td class="line x" title="7:172	1 Introduction With the availability of large number of colossal legal documents in electronic format, there is a rising need for effective information retrieval tools to assist in organizing, processing and retrieving this information and presenting them in a suitable user-friendly format." ></td>
	<td class="line x" title="8:172	To that end, text summarization is an important step for many of these larger information management goals." ></td>
	<td class="line x" title="9:172	In recent years, much attention has been focused on the problem of understanding the structure and textual units in legal judgments (Farzindar & Lapalme, 2004)." ></td>
	<td class="line x" title="10:172	In this case, performing automatic segmentation of a document to understand the rhetorical roles turns out to be an important research issue." ></td>
	<td class="line x" title="11:172	For instance, Farzindar (2004) proposed a text summarization method to manipulate factual and heuristic knowledge from legal documents." ></td>
	<td class="line x" title="12:172	Hachey and Grover (2005) explored machine learning approach to rhetorical status classification by performing fact extraction and sentence extraction for automatic summarization of texts in the legal domain." ></td>
	<td class="line x" title="13:172	They formalized the problem to extract most important units based on the identification of thematic structure of the document and determination of argumentative roles of the textual units in the judgment." ></td>
	<td class="line x" title="14:172	They mainly used linguistic features to identify the thematic structures." ></td>
	<td class="line x" title="15:172	In this paper, we discuss methods for automatic identification of rhetorical roles in legal judgments based on rules and on machine learning techniques." ></td>
	<td class="line x" title="16:172	Using manually annotated sample documents on three different legal sub-domains (rent control, income tax and sales tax), we train an undirected graphical model to segment the documents along different rhetorical structures." ></td>
	<td class="line x" title="17:172	To represent the documents for this work, we mainly used features like cue words, state transition, named entity, position and other local and global features." ></td>
	<td class="line x" title="18:172	The segmented texts with identified roles play a crucial part in re-ordering the ranking in the final extraction-based summary." ></td>
	<td class="line x" title="19:172	The important sentences are extracted based on the term distribution model given in [Saravanan et al, 2006]." ></td>
	<td class="line x" title="20:172	In   order to develop a generic approach to perform segmentation, we use a fixed set of seven rhetorical categories based on Bhatias (1993) genre analysis shown in Table 1." ></td>
	<td class="line x" title="21:172	Graphical Models are nowadays used in many text   processing  applications;   however  the  main 481  Rhetorical Roles Description  Identifying the case  (1) The sentences that are present in a judgment to identify the issues to be decided for a case." ></td>
	<td class="line x" title="22:172	Courts call them as Framing the issues." ></td>
	<td class="line x" title="23:172	Establishing facts of the case  (2) The facts that are relevant to the present proceedings/litigations that stand proved, disproved or unproved for proper applications of correct legal principle/law." ></td>
	<td class="line x" title="24:172	Arguing the case   (3) Application of legal principle/law advocated by contending parties to a given set of proved facts." ></td>
	<td class="line x" title="25:172	History of the case  (4) Chronology of events with factual details that led to the present case between parties named therein before the court on which the judgment is delivered." ></td>
	<td class="line x" title="26:172	Arguments (Analysis ) (5) The court discussion on the law that is applicable to the set of proved facts by weighing the arguments of contending parties with reference to the statute and precedents that are available." ></td>
	<td class="line x" title="27:172	Ratio decidendi    (6) (Ratio of the decision) Applying the correct law to a set of facts is the duty of any court." ></td>
	<td class="line x" title="28:172	The reason given for application of any legal principle/law to decide a case is called Ratio decidendi in legal parlance." ></td>
	<td class="line x" title="29:172	It can also be described as the central generic reference of text." ></td>
	<td class="line x" title="30:172	Final decision  (7) (Disposal) It is an ultimate decision or conclusion of the court following as a natural or logical outcome of ratio of the decision Table 1." ></td>
	<td class="line x" title="31:172	The current working version of the rhetorical annotation scheme for legal judgments." ></td>
	<td class="line x" title="32:172	focus has been performing Natural Language processing tasks on newspaper and research paper domains." ></td>
	<td class="line x" title="33:172	As a novel approach, we have tried and implemented the CRF model for role identification in legal domain." ></td>
	<td class="line x" title="34:172	In this regard, we have first implemented rule based approach and extend this method with additional features and a probabilistic model." ></td>
	<td class="line x" title="35:172	In another study, CRF is used as a tool to model the sequence labeling problem for summarization task (Shen at al., 2006)." ></td>
	<td class="line x" title="36:172	In our work, we are in the process of   developing a fully automatic summarization system for a legal domain on the basis of Laffertys (2001) segmentation task and Teufel & Moens (2004) gold standard approaches." ></td>
	<td class="line x" title="37:172	Legal judgments are different in characteristics compared with articles reporting scientific research papers and other simple domains related to the identification of basic structures of a document." ></td>
	<td class="line x" title="38:172	To perform a summarization methodology and find out important portions of a legal document is a complex problem (Moens, 2004)." ></td>
	<td class="line x" title="39:172	Even the skilled lawyers are facing difficulty in identifying the main       decision part of a law report." ></td>
	<td class="line x" title="40:172	The genre structure identified for legal judgment in our work plays a crucial role in identifying the main decision part in the way of breaking the document in anaphoric chains." ></td>
	<td class="line x" title="41:172	The sentence extraction task forms part of an automatic summarization system in the legal domain." ></td>
	<td class="line x" title="42:172	The main focus of this paper is information extraction task based on the identified roles and methods of structuring summaries which  has considered  being a  hot  research  topic  Most traditional rule learning algorithms are based on a divide-and-conquer strategy." ></td>
	<td class="line x" title="43:172	SLIPPER [Cohen, 1999] is one of the standard rule learning algorithms used for information retrieval task." ></td>
	<td class="line x" title="44:172	In SLIPPER, the ad hoc metrics used to guide the growing and pruning of rules are replaced with metrics based on the formal analysis of boosting algorithms." ></td>
	<td class="line x" title="45:172	For each instance, we need to check each and every rule in the rule set for a given sentence." ></td>
	<td class="line x" title="46:172	It takes more time for larger corpora   (Yeh et al., 2005)." ></td>
	<td class="line x" title="47:172	Now we will discuss the importance of identifying rules in the data collection by various methods available for rule learning in the next section." ></td>
	<td class="line x" title="48:172	2 Text Segmentation Algorithms We explain two approaches to text segmentation for identifying the rhetorical roles in legal judgments." ></td>
	<td class="line x" title="49:172	The focus of the first approach is on a rule-based method with novel rule sets which we fine-tuned for legal domains." ></td>
	<td class="line x" title="50:172	That is, we frame text segmentation as a rule learning problem." ></td>
	<td class="line x" title="51:172	The proposed rule-based method can be enhanced with additional features and a probabilistic model." ></td>
	<td class="line x" title="52:172	An undirected graphical model, Conditional Random Field (CRF) is used for this purpose." ></td>
	<td class="line x" title="53:172	It shows significant improvement over the rule-based method." ></td>
	<td class="line x" title="54:172	The explanation of these methods is given in the following sections." ></td>
	<td class="line x" title="55:172	2.1 Rule-based learning algorithms 482 compared to other rule learning algorithms even for  a  two-class  problem." ></td>
	<td class="line x" title="56:172	If  we  need to  consider more than two classes and to avoid overfitting of ensemble of rules, one has to think of grouping the rules in a rule set and some chaining mechanism has to be followed." ></td>
	<td class="line x" title="57:172	Another rule learning algorithm RuleFit (Friedman & Popescu, 2005) generates a small comprehensible rule set which is used in ensemble learning with larger margin." ></td>
	<td class="line x" title="58:172	In this case, overfitting may happen, if the rule set gets too large and thus some form of control has to be maintained." ></td>
	<td class="line x" title="59:172	Our main idea is to find a preferably small set of rules with high predictive accuracy and with marginal execution time." ></td>
	<td class="line x" title="60:172	We propose an alternative rule learning strategy that concentrates on classification of rules and chaining relation in each rhetorical role (Table 1) based on the human annotation schemes." ></td>
	<td class="line x" title="61:172	A chain relation is a technique used to identify cooccurrences of roles in legal judgments." ></td>
	<td class="line x" title="62:172	In our approach, rules are conjunctions of primitive conditions." ></td>
	<td class="line x" title="63:172	As used by the boosting algorithms, a rule set R can be any hypothesis that partitions the set of instance X into particular role categorization; the set of instances which satisfy any one of seven different set of categorized roles." ></td>
	<td class="line x" title="64:172	We start by generating rules that describe the original features found in the training set." ></td>
	<td class="line x" title="65:172	Each rule outputs 1 if its condition is met, 0 if it is not met." ></td>
	<td class="line x" title="66:172	Let us now define for a sample document X = (S1, S2,.,Sm) of size m, we assume that the set of rules R = {r   The CRF model-based retrieval system designed in this paper will depict the way a human can summarize a legal judgment by understanding the importance of roles and related contents." ></td>
	<td class="line x" title="67:172	Conditional Random Fields is one of the recently emerging graphical models which have been used for text segmentation problems and proved to be one of the best available frame works compared to other existing models (Lafferty, 2001)." ></td>
	<td class="line x" title="68:172	A judgment can be regarded as a sequence of sentences that can be segmented along the seven rhetorical roles where each segments is relatively coherent in content." ></td>
	<td class="line x" title="69:172	We use CRF as a tool to model the text segmentation problem." ></td>
	<td class="line x" title="70:172	CRFs are undirected graphical models used to specify the conditional probabilities of possible label sequences given an observation sequence." ></td>
	<td class="line x" title="71:172	Moreover, the conditional probabilities of label sequences can depend on arbitrary, non independent features of the observation sequence, since we are not forming the model to consider the distribution of those dependencies." ></td>
	<td class="line x" title="72:172	In a special case in which the output nodes of the graphical model are linked by edges in a linear chain, CRFs make a first-order Markov independence assumption with binary feature functions, and thus can be understood as conditionally-trained finite state   machines (FSMs) which are suitable for sequence labeling." ></td>
	<td class="line x" title="73:172	A linear chain CRF with parameters C = {C 1 ,r 2 ,} are applied to sample X, where each rule r i  : X  L  represents the mapping of sentences of X onto a rhetorical role and L = {L 1 ,L 2 ,,L 7 }." ></td>
	<td class="line x" title="74:172	Each L i  represents a rhetorical role from the fixed set shown in Table 1." ></td>
	<td class="line x" title="75:172	An outline of our method is given below." ></td>
	<td class="line x" title="76:172	Procedure Test (X)      {    Read test set            Read instances from sample X (instances  may  be             words,  N-grams or even full sentences)             Apply rules in R (with role categorization                        by maintaining chain relation)              For k = 1 to m sentences For i = 1, 2, ." ></td>
	<td class="line x" title="77:172	no. of instances in each sentence For j = 1 to 7      /* 7 identified roles */ If there exist a rule which satisfies then        X(i,j)  gets a value  1 Else    X(i,j) gets a value {1,0} based on chain relation  S(k) = L (argmax  (X(i,j)))  j  i     } 2.2 Conditional Random Fields and Features 1 ,C 2 ,} defines a  conditional probability for a label sequence l = l 1 ,l w  (e.g., Establishing facts of the case, Final decision, etc.) given an observed input sequence s = s 1 ,s W  to be  1           w m     P C (l | s) = -- exp[ C k  f k (l t-1 , l t . s, t)  ." ></td>
	<td class="line x" title="78:172	(1)       Z s t=1 k=1 where Z s  is the normalization factor that makes the probability of all state sequences sum to one,  f k (l t-1 , l t , s, t) is one of  m feature functions which is generally binary valued and C k  is a learned weight associated with feature function." ></td>
	<td class="line x" title="79:172	For example, a feature may have the value of 0 in most cases, but given the text points for consideration, it has the value 1 along the transition where l t-1  corresponds to a state with the label identifying the case, l t    corresponds to a state  with the label  history of the case,  and  f k  is  the feature  function  PHRASE= 483 points for consideration belongs to s at position t in the sequence." ></td>
	<td class="line x" title="80:172	Large positive values for C k  indicate a preference for such an event, while large negative values make the event unlikely and near zero for relatively uninformative features." ></td>
	<td class="line x" title="81:172	These weights are set to maximize the conditional log likelihood of labeled sequence in a training set D = {( s  State Transition features In CRFs, state transitions are also represented as features (Peng & McCullam, 2006)." ></td>
	<td class="line x" title="82:172	The feature function f t,  l t ) : t = 1,2,w), written as:           L C  (D) =   log P C (l i | s i )                                            i                 w m             =   (  C k  f k (l t-1 , l t . s, t)  log Zs i  )(2)                            i  t=1 k=1 The training state sequences are fully labeled and definite, the objective function is convex, and thus the model is guaranteed to find the optimal weight settings in terms of L C  (D)." ></td>
	<td class="line x" title="83:172	The probable labeling sequence for an input s i  can be efficiently calculated by dynamic programming using modified Viterbi algorithm." ></td>
	<td class="line x" title="84:172	These implementations of CRFs are done using newly developed java classes which also use a quasi-Newton method called L-BFGS to find these feature weights efficiently." ></td>
	<td class="line x" title="85:172	In addition to the following standard set of features, we also added other related features to reduce the complexity of legal domain." ></td>
	<td class="line x" title="86:172	Legal vocabulary features One of the simplest and most obvious set of features is decided using the basic vocabularies from a training data." ></td>
	<td class="line x" title="87:172	The words that appear with capitalizations, affixes, and in abbreviated texts are considered as important features." ></td>
	<td class="line x" title="88:172	Some of the phrases that include v. and act/section are the salient features for arguing the case and arguments categories." ></td>
	<td class="line x" title="89:172	We have gathered a corpus of legal judgments up to the year 2006 which were downloaded from www.keralawyer.com specific to the sub-domains of rent control, income tax and sales tax." ></td>
	<td class="line x" title="90:172	Using the manually annotated subset of the corpus (200 judgments) we have performed a number of preliminary experiments to determine which method would be appropriate for role identification." ></td>
	<td class="line x" title="91:172	The annotated corpus is available from iil.cs.iitm.ernet.in/datasets." ></td>
	<td class="line x" title="92:172	Even though, income tax and sales tax judgments are based on similar facts, the number of relevant legal sections / provisions are differ." ></td>
	<td class="line x" title="93:172	The details and structure of judgments related to rent control domain are not the same compared to income tax and sales tax domains." ></td>
	<td class="line x" title="94:172	Moreover, the roles like ratio decidendi and final decision occur many times spread over the full judgment in sales tax domain, which is comparatively different to other sub-domains." ></td>
	<td class="line x" title="95:172	We have implemented both the approaches on rent control domain successfully." ></td>
	<td class="line x" title="96:172	We found that the other sub-domains need specific add-on features which improve the result by an additional 20%." ></td>
	<td class="line x" title="97:172	Based on this, we have introduced additional features and new set of rules for the income tax and sales tax related judgments." ></td>
	<td class="line x" title="98:172	The modified rule set and additional features are smaller in number, but  create  a  good impact  on the  rhetorical status Indicator/cue phrases  The term cue phrase indicates the key phrases frequently used which are the indicators of common rhetorical roles of the sentences (e.g. phrases such as We agree with court, Question for consideration is, etc.,)." ></td>
	<td class="line x" title="99:172	In this study, we encoded this information and generated automatically explicit linguistic features." ></td>
	<td class="line x" title="100:172	Feature functions for the rules are set to 1 if they match words/phrases in the input sequence exactly." ></td>
	<td class="line x" title="101:172	Named entity recognition This type of recognition is not considered fully in summarizing scientific articles (Teufel & Moens, 2002)." ></td>
	<td class="line x" title="102:172	But in our work, we included few named entities like Supreme Court, Lower court etc., and generate binary-valued entity type features which take the value 0 or 1 indicating the presence or absence of a particular entity type in the sentences." ></td>
	<td class="line x" title="103:172	Local features and Layout features One of the main advantages of CRFs is that they easily afford the use of arbitrary features of the input." ></td>
	<td class="line x" title="104:172	One can encode abbreviated features; layout features such as position of paragraph beginning, as well as the sentences appearing with quotes, all in one framework." ></td>
	<td class="line x" title="105:172	k (l t-1 , l t . s, t) in Eq." ></td>
	<td class="line x" title="106:172	(1) is a general function over states and observations." ></td>
	<td class="line x" title="107:172	Different state transition features can be defined to form different Markov-order structures." ></td>
	<td class="line x" title="108:172	We define state transition features corresponding to appearance of years attached with Section and Act nos." ></td>
	<td class="line x" title="109:172	related to the labels arguing the case and arguments." ></td>
	<td class="line x" title="110:172	2.3 Experiments with role identification 484  Precision Recall F-measure  Rhetorical Roles Slipper  Rulebased CRF Slipper Rulebased CRF Slipper Rulebased CRF Identifying the case    0.641 0.742 0.846 0.512 0.703 0.768 0.569 0.722 0.853 Establishing the facts of the case 0.562 0.737 0.824 0.456 0.664 0.786 0.503 0.699 0.824 Arguing the case 0.436 0.654 0.824 0.408 0.654 0.786 0.422 0.654 0.805 History of the case 0.841 0.768 0.838 0.594 0.716 0.793 0.696 0.741 0.815 Arguments 0.543 0.692 0.760 0.313 0.702 0.816 0.397 0.697 0.787 Ratio of decidendi 0.574 0.821 0.874 0.480 0.857 0.903 0.523 0.839 0.888      Rent Control Domain  Final Decision 0.700 0.896 0.986 0.594 0.927 0.961 0.643 0.911 0.973 Micro-Average of F-measure   0.536 0.752 0.849 Precision Recall F-measure  Rhetorical Roles Slipper  Rulebased CRF Slipper Rulebased CRF Slipper Rulebased CRF Identifying the case 0.590 0.726 0.912 0.431 0.690 0.852 0.498 0.708 0.881 Establishing the facts of the case 0.597 0.711 0.864 0.512 0.659 0.813 0.551 0.684 0.838 Arguing the case 0.614 0.658 0.784 0.551 0.616 0.682 0.581 0.636 0.729 History of the case 0.437 0.729 0.812 0.418 0.724 0.762 0.427 0.726 0.786 Arguments 0.740 0.638 0.736 0.216 0.599 0.718 0.334 0.618 0.727 Ratio of decidendi 0.416 0.708 0.906 0.339 0.663 0.878 0.374 0.685 0.892      Income Tax Domain  Final Decision   0.382 0.752 0.938 0.375 0.733 0.802 0.378 0.742 0.865 Micro-Average of F-measure   0.449 0.686 0.817 Precision Recall F-measure  Rhetorical Roles Slipper  Rulebased CRF Slipper Rulebased CRF Slipper Rulebased CRF Identifying the case 0.539 0.675 0.842 0.398 0.610 0.782 0.458 0.641 0.811 Establishing the facts of the case 0.416 0.635 0.784 0.319 0.559 0.753 0.361 0.595 0.768 Arguing the case 0.476 0.718 0.821 0.343 0.636 0.747 0.399 0.675 0.782 History of the case 0.624 0.788 0.867 0.412 0.684 0.782 0.496 0.732 0.822 Arguments 0.500 0.638 0.736 0.438 0.614 0.692 0.467 0.626 0.713 Ratio of decidendi 0.456 0.646 0.792 0.318 0.553 0.828 0.375 0.596 0.810      Sales Tax Domain  Final Decision 0.300 0.614 0.818 0.281 0.582 0.786 0.290 0.598 0.802 Micro-Average of F-measure   0.407 0.637 0.787  classification in   the  sales  tax   and  income   tax domains." ></td>
	<td class="line x" title="111:172	It is common practice to consider human performances as an upper bound for most of the IR tasks, so in our evaluation, the performance of the system has been successfully tested by matching with human annotated documents." ></td>
	<td class="line x" title="112:172	Kappa (Siegal & Castellan, 1988) is an evaluation measure used in our work to compare the inter-agreement between sentences extracted by two human annotators for role identification in legal judgments." ></td>
	<td class="line x" title="113:172	The value (K=0.803) shows the good reliability of human annotated corpus." ></td>
	<td class="line x" title="114:172	The results given in Table 2 show that CRF-based and rule-based methods perform well for each role categories compared to SLIPPER method." ></td>
	<td class="line x" title="115:172	CRFbased method performs extremely well and paired t-test result indicates that it is significantly (p < .01) higher than the other two methods on rhetorical role identification for legal judgments belonging to  rent control, income tax and sales tax      Figure 1 shows that the distribution of the seven categories is very much skewed, with 60% of all sentences being classified as history of the case." ></td>
	<td class="line x" title="116:172	Basically it includes the   remaining contents of the   Table 2." ></td>
	<td class="line x" title="117:172	Precision, Recall and F-measure for seven rhetorical roles sub-domains." ></td>
	<td class="line x" title="118:172	In this experiment, we also made an effort to understand the annotation of relevance of seven rhetorical categories." ></td>
	<td class="line x" title="119:172	Figure 1." ></td>
	<td class="line x" title="120:172	Distribution of rhetorical roles (10 entire documents from rent control sub-domain)  1 12%% 2 9% 3 4% 4 60% 5 19% 6 5% 7 485 document other than the six categories." ></td>
	<td class="line x" title="121:172	In this case, we have calculated the distribution among 10 judgments related to rent control documents." ></td>
	<td class="line x" title="122:172	Figure 2 shows the rhetorical category distribution among the 10 different summaries from rent control domain." ></td>
	<td class="line x" title="123:172	This shows that the resulting category distribution is far more evenly distributed than the one covering all sentences in Figure 1." ></td>
	<td class="line x" title="124:172	Ratio of decidendi and final decision are the two most frequent categories in the sentences extracted from judgments." ></td>
	<td class="line x" title="125:172	The label numbers mentioned in the Figures denote the rhetorical roles which as defined in Table 1." ></td>
	<td class="line x" title="126:172	The automatic text summarization process starts with sending legal document to a preprocessing stage." ></td>
	<td class="line x" title="127:172	In this preprocessing stage, the document is to be divided into segments, sentences and tokens." ></td>
	<td class="line x" title="128:172	We have introduced some new feature identification techniques to explore paragraph alignments." ></td>
	<td class="line x" title="129:172	This process includes the understanding of abbreviated texts and section numbers and arguments which are very specific to the structure of legal documents." ></td>
	<td class="line x" title="130:172	The other useful statistical natural language processing tools, such as filtering out stop list words, stemming etc., are carried out in the preprocessing stage." ></td>
	<td class="line x" title="131:172	The resulting intelligible words are useful in the normalization of terms in the term distribution model (Saravanan et al., 2006)." ></td>
	<td class="line x" title="132:172	During the final stage, we have altered the ranks or removed some of the sentences from the final summary based on the structure discovered using CRF." ></td>
	<td class="line x" title="133:172	The summarization module architecture is shown in Figure 3." ></td>
	<td class="line x" title="134:172	1 8% 2 15% 3 12% 4 14% 5 16% 6 27% 7 8%  Figure 2." ></td>
	<td class="line x" title="135:172	Distribution of rhetorical roles (10 different summaries from rent control sub-domain)    The application of term distribution model brings out a good extract of sentences present in a legal document to generate a summary." ></td>
	<td class="line x" title="136:172	The sentences with labels identified during CRF implementation can be used with the term distribution model to give more significance to some of the sentences with specific roles." ></td>
	<td class="line x" title="137:172	Moreover, the structure details available in this stage are useful in improving the coherency and readability among the sentences present in the summary." ></td>
	<td class="line x" title="138:172	3 Legal Document Summarization Extraction of sentences in the generation of a summary at different percentage levels of text is one of the widely used methods in document summarization (Radev et al., 2002)." ></td>
	<td class="line x" title="139:172	For the legal domain, generating a summary from the original judgment is a complex problem." ></td>
	<td class="line x" title="140:172	Our approach to produce the summary is extraction-based method which identifies important elements present in a legal judgment." ></td>
	<td class="line x" title="141:172	The identification of the document structure using CRF-model categorizes the key ideas from the details of a legal judgment." ></td>
	<td class="line x" title="142:172	The genre structure has been applied to final summary to improve the readability and coherence." ></td>
	<td class="line x" title="143:172	In order to evaluate the effectiveness of our summarizer, we have applied four different measures to look for a match on the model summary generated by humans (head notes) from the text of the original judgments." ></td>
	<td class="line x" title="144:172	Extrinsic and intrinsic are the two different evaluation strategies available for text summarization (Sparck Jones & Gablier, 1996)." ></td>
	<td class="line x" title="145:172	Intrinsic measure shows the presence of source contents in the summary." ></td>
	<td class="line x" title="146:172	F-measure and MAP are two standard intrinsic measures used for the evaluation of our system-generated summary." ></td>
	<td class="line oc" title="147:172	We have also used ROUGE evaluation approach (Lin, 2004) which is based on n-gram co-occurrences between machine summaries and ideal human summaries." ></td>
	<td class="line x" title="148:172	3.1 Applying term distribution model     Legal Documents Segmented text with labels (CRF implementation)      Preprocessing Term distribution model Summary with ratio & final decision Figure 3." ></td>
	<td class="line x" title="149:172	Architectural view of summarization system." ></td>
	<td class="line o" title="150:172	3.2 Evaluation of  a summary 486 In this paper, we have applied ROUGE-1 and ROUGE-2 which are simple n-gram measures." ></td>
	<td class="line x" title="151:172	We compared our results with Microsoft, Mead Summarizer (Radev et al., 2003) and other two simple baselines: one which chooses 15% of words of the beginning of the judgment and second chooses last 10% of words of the judgment with human reference summaries." ></td>
	<td class="line x" title="152:172	Both the baselines defined in this study are standard baselines for newspaper and research domains." ></td>
	<td class="line x" title="153:172	The result shown in Table 3 highlights the better performances of our summarizer compared to other methods considered in this study." ></td>
	<td class="line x" title="154:172	We can see that the results of MEAD and WORD summaries are not at the expected level, while our summarizer is best in terms of all four evaluation measures." ></td>
	<td class="line x" title="155:172	Results are clearly indicated that our system performs significantly better than the other systems for legal judgments." ></td>
	<td class="line x" title="156:172	We would like to thank the legal fraternity for the assistance and guidance governs to us." ></td>
	<td class="line x" title="157:172	Especially we express our sincere gratitude to the advocates Mr. S.B.C. Karunakaran and Mr. K.N. Somasundaram for their domain advice and continuous guidance in understanding the structure of legal document and for hand annotated legal judgments." ></td>
	<td class="line x" title="158:172	Table 3." ></td>
	<td class="line o" title="159:172	MAP, F-measure and ROUGE scores." ></td>
	<td class="line x" title="160:172	4 Conclusion This paper describes a novel method for generating a summary for legal judgments with the help of undirected graphical models." ></td>
	<td class="line x" title="161:172	We observed that rhetorical role identification from legal documents is one of the primary tasks to understand the structure of the judgments." ></td>
	<td class="line x" title="162:172	CRF model performs much better than rule based and other rule learning method in segmenting the text for legal domains." ></td>
	<td class="line x" title="163:172	Our approach to summary extraction is based on the extended version of term weighting method." ></td>
	<td class="line x" title="164:172	With the identified roles, the important sentences generated in the probabilistic model will be reordered or suppressed in the final summary." ></td>
	<td class="line x" title="165:172	The evaluation results show that the summary generated by our summarizer is closer to the human generated head notes, compared to the other methods considered in this study." ></td>
	<td class="line x" title="166:172	Hence the legal community will get a better insight without reading a full judgment." ></td>
	<td class="line x" title="167:172	Moreover, our system-generated summary is more useful for lawyers to prepare the case history related to presently appearing cases." ></td>
	<td class="line x" title="168:172	Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen." ></td>
	<td class="line x" title="169:172	2007." ></td>
	<td class="line x" title="170:172	Document Summarization using Conditional Random Fields." ></td>
	<td class="line x" title="171:172	International Joint Conference on Artificial Intelligence, IJCAI 2007, Hyderabad, India, PP.2862-2867." ></td>
	<td class="line x" title="172:172	Acknowledgement" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1094
Can You Summarize This? Identifying Correlates of Input Difficulty for Multi-Document Summarization
Nenkova, Ani;Louis, Annie;"></td>
	<td class="line x" title="1:191	Proceedings of ACL-08: HLT, pages 825833, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:191	c2008 Association for Computational Linguistics Can you summarize this?" ></td>
	<td class="line x" title="3:191	Identifying correlates of input difficulty for generic multi-document summarization Ani Nenkova University of Pennsylvania Philadelphia, PA 19104, USA nenkova@seas.upenn.edu Annie Louis University of Pennsylvania Philadelphia, PA 19104, USA lannie@seas.upenn.edu Abstract Different summarization requirements could make the writing of a good summary more difficult, or easier." ></td>
	<td class="line x" title="4:191	Summary length and the characteristics of the input are such constraints influencing the quality of a potential summary." ></td>
	<td class="line x" title="5:191	In this paper we report the results of a quantitative analysis on data from large-scale evaluations of multi-document summarization, empirically confirming this hypothesis." ></td>
	<td class="line x" title="6:191	We further show that features measuring the cohesiveness of the input are highly correlated with eventual summary quality and that it is possible to use these as features to predict the difficulty of new, unseen, summarization inputs." ></td>
	<td class="line x" title="7:191	1 Introduction In certain situations even the best automatic summarizers or professional writers can find it hard to write a good summary of a set of articles." ></td>
	<td class="line x" title="8:191	If there is no clear topic shared across the input articles, or if they follow the development of the same event in time for a longer period, it could become difficult to decide what information is most representative and should be conveyed in a summary." ></td>
	<td class="line x" title="9:191	Similarly, length requirements could pre-determine summary qualitya short outline of a story might be confusing and unclear but a page long discussion might give an excellent overview of the same issue." ></td>
	<td class="line x" title="10:191	Even systems that perform well on average produce summaries of poor quality for some inputs." ></td>
	<td class="line x" title="11:191	For this reason, understanding what aspects of the input make it difficult for summarization becomes an interesting and important issue that has not been addressed in the summarization community untill now." ></td>
	<td class="line x" title="12:191	In information retrieval, for example, the variable system performance has been recognized as a research challenge and numerous studies on identifying query difficulty have been carried out (most recently (Cronen-Townsend et al., 2002; Yom-Tov et al., 2005; Carmel et al., 2006))." ></td>
	<td class="line x" title="13:191	In this paper we present results supporting the hypotheses that input topicality cohesiveness and summary length are among the factors that determine summary quality regardless of the choice of summarization strategy (Section 2)." ></td>
	<td class="line x" title="14:191	The data used for the analyses comes from the annual Document Understanding Conference (DUC) in which various summarization approaches are evaluated on common data, with new test sets provided each year." ></td>
	<td class="line x" title="15:191	In later sections we define a suite of features capturing aspects of the topicality cohesiveness of the input (Section 3) and relate these to system performance, identifying reliable correlates of input difficulty (Section 4)." ></td>
	<td class="line x" title="16:191	Finally, in Section 5, we demonstrate that the features can be used to build a classifier predicting summarization input difficulty with accuracy considerably above chance level." ></td>
	<td class="line x" title="17:191	2 Preliminary analysis and distinctions: DUC 2001 Generic multi-document summarization was featured as a task at the Document Understanding Conference (DUC) in four years, 2001 through 2004." ></td>
	<td class="line x" title="18:191	In our study we use the DUC 2001 multi-document task submissions as development data for in-depth analysis and feature selection." ></td>
	<td class="line x" title="19:191	There were 29 input sets and 12 automatic summarizers participating in the evaluation that year." ></td>
	<td class="line x" title="20:191	Summaries of different 825 lengths were produced by each system: 50, 100, 200 and 400 words." ></td>
	<td class="line x" title="21:191	Each summary was manually evaluated to determine the extent to which its content overlaped with that of a human model, giving a coverage score." ></td>
	<td class="line x" title="22:191	The content comparison was performed on a subsentence level and was based on elementary discourse units in the model summary.1 The coverage scores are taken as an indicator of difficultly of the input: systems achieve low coverage for difficult sets and higher coverage for easy sets." ></td>
	<td class="line x" title="23:191	Since we are interested in identifying characteristics of generally difficult inputs rather than in discovering what types of inputs might be difficult for one given system, we use the average system score per set as indicator of general difficulty." ></td>
	<td class="line x" title="24:191	2.1 Analysis of variance Before attempting to derive characteristics of inputs difficult for summarization, we first confirm that indeed expected performance is influenced by the input itself." ></td>
	<td class="line x" title="25:191	We performed analysis of variance for DUC 2001 data, with automatic system coverage score as the dependent variable, to gain some insight into the factors related to summarization difficulty." ></td>
	<td class="line x" title="26:191	The results of the ANOVA with input set, summarizer identity and summary length as factors, as well as the interaction between these, are shown in Table 1." ></td>
	<td class="line x" title="27:191	As expected, summarizer identity is a significant factor: some summarization strategies/systems are more effective than others and produce summaries with higher coverage score." ></td>
	<td class="line x" title="28:191	More interestingly, the input set and summary length factors are also highly significant and explain more of the variability in coverage scores than summarizer identity does, as indicated by the larger values of the F statistic." ></td>
	<td class="line x" title="29:191	Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row)." ></td>
	<td class="line x" title="30:191	The general trend we observe is that on average systems are better at producing summaries when more space is available." ></td>
	<td class="line pc" title="31:191	The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004)." ></td>
	<td class="line x" title="32:191	Type 50 100 200 400 Human 1.00 1.17 1.38 1.29 Automatic 0.50 0.55 0.70 0.76 Baseline 0.41 0.46 0.52 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words." ></td>
	<td class="line x" title="33:191	N = 50, 100, 200, and 400." ></td>
	<td class="line x" title="34:191	ferences are statistically significant2 only between 50-word and 200and 400-word summaries and between 100-word and 400-word summaries." ></td>
	<td class="line x" title="35:191	The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been paid to this fact in system development and no specific user studies are available to show what summary length might be most suitable for specific applications." ></td>
	<td class="line x" title="36:191	In later editions of the DUC conference, only summaries of 100 words were produced, focusing development efforts on one of the more demanding length restrictions." ></td>
	<td class="line x" title="37:191	The interaction between summary length and summarizer is small but significant (Table 1), with certain summarization strategies more successful at particular summary lengths than at others." ></td>
	<td class="line x" title="38:191	Improved performance as measured by increase in coverage scores is observed for human summarizers as well (shown in the first row of Table 2)." ></td>
	<td class="line x" title="39:191	Even the baseline systems (first n words of the most recent article in the input or first sentences from different input articles) show improvement when longer summaries are allowed (performance shown in the third row of the table)." ></td>
	<td class="line x" title="40:191	It is important to notice that the difference between automatic system and baseline performance increases as the summary length increasesthe difference between systems and baselines coverage scores is around 0.1 for the shorter 50and 100-word summaries but 0.2 for the longer summaries." ></td>
	<td class="line x" title="41:191	This fact has favorable implications for practical system developments because it indicates that in applications where somewhat longer summaries are appropriate, automatically produced summaries will be much more informative than a baseline summary." ></td>
	<td class="line x" title="42:191	2One-sided t-test, 95% level of significance." ></td>
	<td class="line x" title="43:191	826 Factor DF Sum of squares Expected mean squares F stat Pr(> F) input 28 150.702 5.382 59.4227 0 summarizer 11 34.316 3.120 34.4429 0 length 3 16.082 5.361 59.1852 0 input:summarizer 306 65.492 0.214 2.3630 0 input:length 84 36.276 0.432 4.7680 0 summarizer:length 33 6.810 0.206 2.2784 0 Table 1: Analysis of variance for coverage scores of automatic systems with input, summarizer, and length as factors." ></td>
	<td class="line x" title="44:191	Input The input set itself is a highly significant factor that influences the coverage scores that systems obtain: some inputs are handled by the systems better than others." ></td>
	<td class="line x" title="45:191	Moreover, the input interacts both with the summarizers and the summary length." ></td>
	<td class="line x" title="46:191	This is an important finding for several reasons." ></td>
	<td class="line x" title="47:191	First, in system evaluations such as DUC the inputs for summarization are manually selected by annotators." ></td>
	<td class="line x" title="48:191	There is no specific attempt to ensure that the inputs across different years have on average the same difficulty." ></td>
	<td class="line x" title="49:191	Simply assuming this to be the case could be misleading: it is possible in a given year to have easier input test set compared to a previous year." ></td>
	<td class="line x" title="50:191	Then system performance across years cannot be meaningfully compared, and higher system scores would not be indicative of system improvement between the evaluations." ></td>
	<td class="line x" title="51:191	Second, in summarization applications there is some control over the input for summarization." ></td>
	<td class="line x" title="52:191	For example, related documents that need to summarized could be split into smaller subsets that are more amenable to summarization or routed to an appropriate summarization system than can handle this kind of input using a different strategy, as done for instance in (McKeown et al., 2002)." ></td>
	<td class="line x" title="53:191	Because of these important implications we investigate input characteristics and define various features distinguishing easy inputs from difficult ones." ></td>
	<td class="line x" title="54:191	2.2 Difficulty for people and machines Before proceeding to the analysis of input difficulty in multi-document summarization, it is worth mentioning that our study is primarily motivated by system development needs and consequently the focus is on finding out what inputs are easy or difficult for automatic systems." ></td>
	<td class="line x" title="55:191	Different factors might make summarization difficult for people." ></td>
	<td class="line x" title="56:191	In order to see to what extent the notion of summarization input difsummary length correlation 50 0.50 100 0.57* 200 0.77** 400 0.70** Table 3: Pearson correlation between average human and system coverage scores on the DUC 2001 dataset." ></td>
	<td class="line x" title="57:191	Significance levels: *p < 0.05 and **p < 0.00001." ></td>
	<td class="line x" title="58:191	ficulty is shared between machines and people, we computed the correlation between the average system and average human coverage score at a given summary length for all DUC 2001 test sets (shown in Table 3)." ></td>
	<td class="line x" title="59:191	The correlation is highest for 200-word summaries, 0.77, which is also highly significant." ></td>
	<td class="line x" title="60:191	For shorter summaries the correlation between human and system performance is not significant." ></td>
	<td class="line x" title="61:191	In the remaining part of the paper we deal exclusively with difficulty as defined by system performance, which differs from difficulty for people summarizing the same material as evidenced by the correlations in Table 3." ></td>
	<td class="line x" title="62:191	We do not attempt to draw conclusions about any cognitively relevant factors involved in summarizing." ></td>
	<td class="line x" title="63:191	2.3 Type of summary and difficulty In DUC 2001, annotators prepared test sets from five possible predefined input categories:3." ></td>
	<td class="line x" title="64:191	Single event (3 sets) Documents describing a single event over a timeline (e.g. The Exxon Valdez oil spill)." ></td>
	<td class="line x" title="65:191	3Participants in the evaluation were aware of the different categories of input and indeed some groups developed systems that handled different types of input employing different strategies (McKeown et al., 2001)." ></td>
	<td class="line x" title="66:191	In later years, the idea of multistrategy summarization has been further explored by (Lacatusu et al., 2006) 827 Subject (6 sets) Documents discussing a single topic (e.g. Mad cow disease) Biographical (2 sets) All documents in the input provide information about the same person (e.g. Elizabeth Taylor) Multiple distinct events (12 sets) The documents discuss different events of the same type (e.g. different occasions of police misconduct)." ></td>
	<td class="line x" title="67:191	Opinion (6 sets) Each document describes a different perspective to a common topic (e.g. views of the senate, congress, public, lawyers etc on the decision by the senate to count illegal aliens in the 1990 census)." ></td>
	<td class="line x" title="68:191	Figure 1 shows the average system coverage score for the different input types." ></td>
	<td class="line x" title="69:191	The more topically cohesive input types such as biographical, single event and subject, which are more focused on a single entity or news item and narrower in scope, are easier for systems." ></td>
	<td class="line x" title="70:191	The average system coverage score for them is higher than for the non-cohesive sets such as multiple distinct events and opinion sets, regardless of summary length." ></td>
	<td class="line x" title="71:191	The difference is even more apparently clear when the scores are plotted after grouping input types into cohesive (biographical, single event and subject) and non-cohesive (multiple events and opinion)." ></td>
	<td class="line x" title="72:191	Such grouping also gives the necessary power to perform statistical test for significance, confirming the difference in coverage scores for the two groups." ></td>
	<td class="line x" title="73:191	This is not surprising: a summary of documents describing multiple distinct events of the same type is likely to require higher degree of generalization and abstraction." ></td>
	<td class="line x" title="74:191	Summarizing opinions would in addition be highly subjective." ></td>
	<td class="line x" title="75:191	A summary of a cohesive set meanwhile would contain facts directly from the input and it would be easier to determine which information is important." ></td>
	<td class="line x" title="76:191	The example human summaries for set D32 (single event) and set D19 (opinions) shown below give an idea of the potential difficulties automatic summarizers have to deal with." ></td>
	<td class="line x" title="77:191	set D32 On 24 March 1989, the oil tanker Exxon Valdez ran aground on a reef near Valdez, Alaska, spilling 8.4 million gallons of crude oil into Prince William Sound." ></td>
	<td class="line x" title="78:191	In two days, the oil spread over 100 miles with a heavy toll on wildlife." ></td>
	<td class="line x" title="79:191	Cleanup proceeded at a slow pace, and a plan for cleaning 364 miles of Alaskan coastline was released." ></td>
	<td class="line x" title="80:191	In June, the tanker was refloated." ></td>
	<td class="line x" title="81:191	By early 1990, only 5 to 9 percent of spilled oil was recovered." ></td>
	<td class="line x" title="82:191	A federal jury indicted Exxon on five criminal charges and the Valdez skipper was guilty of negligent discharge of oil." ></td>
	<td class="line x" title="83:191	set D19 Congress is debating whether or not to count illegal aliens in the 1990 census." ></td>
	<td class="line x" title="84:191	Congressional House seats are apportioned to the states and huge sums of federal money are allocated based on census population." ></td>
	<td class="line x" title="85:191	California, with an estimated half of all illegal aliens, will be greatly affected." ></td>
	<td class="line x" title="86:191	Those arguing for inclusion say that the Constitution does not mention citizens, but rather, instructs that House apportionment be based on the whole number of persons residing in the various states." ></td>
	<td class="line x" title="87:191	Those opposed say that the framers were unaware of this issue." ></td>
	<td class="line x" title="88:191	Illegal aliens did not exist in the U.S. until restrictive immigration laws were passed in 1875." ></td>
	<td class="line x" title="89:191	The manual set-type labels give an intuitive idea of what factors might be at play but it is desirable to devise more specific measures to predict difficulty." ></td>
	<td class="line x" title="90:191	Do such measures exist?" ></td>
	<td class="line x" title="91:191	Is there a way to automatically distinguish cohesive (easy) from non-cohesive (difficult) sets?" ></td>
	<td class="line x" title="92:191	In the next section we define a number of features that aim to capture the cohesiveness of an input set and show that some of them are indeed significantly related to set difficulty." ></td>
	<td class="line x" title="93:191	3 Features We implemented 14 features for our analysis of input set difficulty." ></td>
	<td class="line x" title="94:191	The working hypothesis is that cohesive sets with clear topics are easier to summarize and the features we define are designed to capture aspects of input cohesiveness." ></td>
	<td class="line x" title="95:191	Number of sentences in the input, calculated over all articles in the input set." ></td>
	<td class="line x" title="96:191	Shorter inputs should be easier as there will be less information loss between the summary and the original material." ></td>
	<td class="line x" title="97:191	Vocabulary size of the input set, equal to the number of unique words in the input." ></td>
	<td class="line x" title="98:191	Smaller vocabularies would be characteristic of easier sets." ></td>
	<td class="line x" title="99:191	Percentage of words used only once in the input." ></td>
	<td class="line x" title="100:191	The rationale behind this feature is that cohesive input sets contain news articles dealing with a clearly defined topic, so words will be reused across documents." ></td>
	<td class="line x" title="101:191	Sets that cover disparate events and opinions are likely to contain more words that appear in the input only once." ></td>
	<td class="line x" title="102:191	Type-token ratio is a measure of the lexical variation in an input set and is equal to the input vocabulary size divided by the number of words in the 828 Figure 1: Average system coverage scores for summaries in a category input." ></td>
	<td class="line x" title="103:191	A high type-token ratio indicates there is little (lexical) repetition in the input, a possible side-effect of non-cohesiveness." ></td>
	<td class="line x" title="104:191	Entropy of the input set." ></td>
	<td class="line x" title="105:191	Let X be a discrete random variable taking values from the finite set V = {w1,,wn} where V is the vocabulary of the input set and wi are the words that appear in the input." ></td>
	<td class="line x" title="106:191	The probability distribution p(w) = Pr(X = w) can be easily calculated using frequency counts from the input." ></td>
	<td class="line x" title="107:191	The entropy of the input set is equal to the entropy of X: H(X) =  i=nsummationdisplay i=1 p(wi)log2 p(wi) (1) Average, minimum and maximum cosine overlap between the news articles in the input." ></td>
	<td class="line x" title="108:191	Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn, 1958; Barzilay et al., 1999; Radev et al., 2004; Nenkova et al., 2006)." ></td>
	<td class="line x" title="109:191	The more similar the different documents in the input are to each other, the more likely there is repetition across documents at various granularities." ></td>
	<td class="line x" title="110:191	Cosine similarity between the document vector representations is probably the easiest and most commonly used among the various similarity measures." ></td>
	<td class="line x" title="111:191	We use tf*idf weights in the vector representations, with term frequency (tf) normalized by the total number of words in the document in order to remove bias resulting from high frequencies by virtue of higher document length alone." ></td>
	<td class="line x" title="112:191	The cosine similarity between two (document representation) vectors v1 and v2 is given by cos = v1.v2 ||v1||||v2||." ></td>
	<td class="line x" title="113:191	A value of 0 indicates that the vectors areorthogonal and dissimilar, a value of 1 indicates perfectly similar documents in terms of the words contained in them." ></td>
	<td class="line x" title="114:191	To compute the cosine overlap features, we find the pairwise cosine similarity between each two documents in an input set and compute their average." ></td>
	<td class="line x" title="115:191	The minimum and maximum overlap features are also computed as an indication of the overlap bounds." ></td>
	<td class="line x" title="116:191	We expect cohesive inputs to be composed of similar documents, hence the cosine overlaps in these sets of documents must be higher than those in non-cohesive inputs." ></td>
	<td class="line x" title="117:191	KL divergence Another measure of relatedness of the documents comprising an input set is the difference in word distributions in the input compared to the word distribution in a large collection of diverse texts." ></td>
	<td class="line x" title="118:191	If the input is found to be largely different from a generic collection, it is plausible to assume that the input is not a random collection of articles but rather is defined by a clear topic discussed within and across the articles." ></td>
	<td class="line x" title="119:191	It is reasonable to expect that the higher the divergence is, the easier it is to define what is important in the article and hence the easier it is to produce a good summary." ></td>
	<td class="line x" title="120:191	For computing the distribution of words in a general background corpus, we used all the inputs sets from DUC years 2001 to 2006." ></td>
	<td class="line x" title="121:191	The divergence measure we used is the Kullback Leibler divergence, or 829 relative entropy, between the input (I) and collection language models." ></td>
	<td class="line x" title="122:191	Let pinp(w) be the probability of the word w in the input and pcoll(w) be the probability of the word occurring in the large background collection." ></td>
	<td class="line x" title="123:191	Then the relative entropy between the input and the collection is given by KL divergence = summationdisplay wI pinp(w)log2 pinp(w)p coll(w) (2) Low KL divergence from a random background collection may be characteristic of highly noncohesive inputs consisting of unrelated documents." ></td>
	<td class="line x" title="124:191	Number of topic signature terms for the input set." ></td>
	<td class="line oc" title="125:191	The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007)." ></td>
	<td class="line x" title="126:191	Lin and Hovys idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer." ></td>
	<td class="line x" title="127:191	We will call this cluster T. Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection NT)." ></td>
	<td class="line x" title="128:191	Given T and NT, the likelihood ratio statistic (Dunning, 1994) is used to identify the topic signature terms." ></td>
	<td class="line x" title="129:191	The probabilistic model of the data allows for statistical inference in order to decide which terms t are associated with T more strongly than with NT than one would expect by chance." ></td>
	<td class="line x" title="130:191	More specifically, there are two possibilities for the distribution of a term t: either it is very indicative of the topic of cluster T, and appears more often in T than in documents from NT, or the term t is not topical and appears with equal frequency across both T and NT." ></td>
	<td class="line x" title="131:191	These two alternatives can be formally written as the following hypotheses: H1: P(t|T) = P(t|NT) = p (t is not a descriptive term for the input) H2: P(t|T) = p1 and P(t|NT) = p2 and p1 > p2 (t is a descriptive term) In order to compute the likelihood of each hypothesis given the collection of the background documents and the topic cluster, we view them as a sequence of words wi: w1w2 wN." ></td>
	<td class="line x" title="132:191	The occurrence of a given word t, wi = t, can thus be viewed a Bernoulli trial with probability p of success, with success occurring when wi = t and failure otherwise." ></td>
	<td class="line x" title="133:191	The probability of observing the term t appearing k times in N trials is given by the binomial distribution b(k,N,p) = parenleftBigg N k parenrightBigg pk(1  p)Nk (3) We can now compute  = Likelihood of the data given H1Likelihood of the data given H2 (4) which is equal to  = b(ct,N,p)b(c T,NT,p1)  b(cNT,NNT,p2) (5) The maximum likelihood estimates for the probabilities can be computed directly." ></td>
	<td class="line x" title="134:191	p = ctN , where ct is equal to the number of times term t appeared in the entire corpus T+NT, and N is the number of words in the entire corpus." ></td>
	<td class="line x" title="135:191	Similarly, p1 = cTNT , where cT is the number of times term t occurred in T and NT is the number of all words in T. p2 = cNTNNT , where cNT is the number of times term t occurred in NT and NNT is the total number of words in NT." ></td>
	<td class="line x" title="136:191	2log has a well-know distribution: 2." ></td>
	<td class="line x" title="137:191	Bigger values of 2log indicate that the likelihood of the data under H2 is higher, and the 2 distribution can be used to determine when it is significantly higher (2log exceeding 10 gives a significance level of 0.001 and is the cut-off we used)." ></td>
	<td class="line x" title="138:191	For terms for which the computed 2log is higher than 10, we can infer that they occur more often with the topic T than in a general corpus NT, and we can dub them topic signature terms." ></td>
	<td class="line x" title="139:191	Percentage of signature terms in vocabulary The number of signature terms gives the total count of topic signatures over all the documents in the input." ></td>
	<td class="line x" title="140:191	However, the number of documents in an input set and the size of the individual documents across different sets are not the same." ></td>
	<td class="line x" title="141:191	It is therefore possible that the mere count feature is biased to the length 830 and number of documents in the input set." ></td>
	<td class="line x" title="142:191	To account for this, we add the percentage of topic words in the vocabulary as a feature." ></td>
	<td class="line x" title="143:191	Average, minimum and maximum topic signature overlap between the documents in the input." ></td>
	<td class="line x" title="144:191	Cosine similarity measures the overlap between two documents based on all the words appearing in them." ></td>
	<td class="line x" title="145:191	A more refined document representation can be defined by assuming the document vectors contain only the topic signature words rather than all words." ></td>
	<td class="line x" title="146:191	A high overlap of topic words across two documents is indicative of shared topicality." ></td>
	<td class="line x" title="147:191	The average, minimum and maximum pairwise cosine overlap between the tf*idf weighted topic signature vectors of the two documents are used as features for predicting input cohesiveness." ></td>
	<td class="line x" title="148:191	If the overlap is large, then the topic is similar across the two documents and hence their combination will yield a cohesive input." ></td>
	<td class="line x" title="149:191	4 Feature selection Table 4 shows the results from a one-sided t-test comparing the values of the various features for the easy and difficult input set classes." ></td>
	<td class="line x" title="150:191	The comparisons are for summary length of 100 words because in later years only such summaries were evaluated." ></td>
	<td class="line x" title="151:191	The binary easy/difficult classes were assigned based on the average system coverage score for the given set, with half of the sets assigned to each class." ></td>
	<td class="line x" title="152:191	In addition to the t-tests we also calculated Pearsons correlation (shown in Table 5) between the features and the average system coverage score for each set." ></td>
	<td class="line x" title="153:191	In the correlation analysis the input sets are not classified into easy or difficult but rather the real valued coverage scores are used directly." ></td>
	<td class="line x" title="154:191	Overall, the features that were identified by the t-test as most descriptive of the differences between easy and difficult inputs were also the ones with higher correlations with real-valued coverage scores." ></td>
	<td class="line x" title="155:191	Our expectations in defining the features are confirmed by the correlation results." ></td>
	<td class="line x" title="156:191	For example, systems have low coverage scores for sets with highentropy vocabularies as indicated by the negative and high by absolute value correlation (-0.4256)." ></td>
	<td class="line x" title="157:191	Sets with high entropy are those in which there is little repetition within and across different articles, and for which it is subsequently difficult to deterfeature t-stat p-value KL divergence* -2.4725 0.01 % of sig." ></td>
	<td class="line x" title="158:191	terms in vocab* -2.0956 0.02 average cosine overlap* -2.1227 0.02 vocabulary size* 1.9378 0.03 set entropy* 2.0288 0.03 average sig." ></td>
	<td class="line x" title="159:191	term overlap* -1.8803 0.04 max cosine overlap -1.6968 0.05 max topic signature overlap -1.6380 0.06 number of sentences 1.4780 0.08 min topic signature overlap -0.9540 0.17 number of signature terms 0.8057 0.21 min cosine overlap -0.2654 0.39 % of words used only once 0.2497 0.40 type-token ratio 0.2343 0.41 Significant at a 95% confidence level(p < 0.05) Table 4: Comparison of non-cohesive (average system coverage score < median average system score) vs cohesive sets for summary length of 100 words mine what is the most important content." ></td>
	<td class="line x" title="160:191	On the other hand, sets characterized by bigger KL divergence are easierthere the distribution of words is skewed compared to a general collection of articles, with important topic words occurring more often." ></td>
	<td class="line x" title="161:191	Easy to summarize sets are characterized by low entropy, small vocabulary, high average cosine and average topic signature overlaps, high KL divergence and a high percentage of the vocabulary consists of topic signature terms." ></td>
	<td class="line x" title="162:191	5 Classification results We used the 192 sets from multi-document summarization DUC evaluations in 2002 (55 generic sets), 2003 (30 generic summary sets and 7 viewpoint sets) and 2004 (50 generic and 50 biography sets) to train and test a logistic regression classifier." ></td>
	<td class="line x" title="163:191	The sets from all years were pooled together and evenly divided into easy and difficult inputs based on the average system coverage score for each set." ></td>
	<td class="line x" title="164:191	Table 6 shows the results from 10-fold cross validation." ></td>
	<td class="line x" title="165:191	SIG is a classifier based on the six features identified as significant in distinguishing easy from difficult inputs based on a t-test comparison (Table 4)." ></td>
	<td class="line x" title="166:191	SIG+yt has two additional features: the year and the type of summarization input (generic, viewpoint and biographical)." ></td>
	<td class="line x" title="167:191	ALL is a classifier based on all 14 features defined in the previous section, and 831 feature correlation set entropy -0.4256 KL divergence 0.3663 vocabulary size -0.3610 % of sig." ></td>
	<td class="line x" title="168:191	terms in vocab 0.3277 average sig." ></td>
	<td class="line x" title="169:191	term overlap 0.2860 number of sentences -0.2511 max topic signature overlap 0.2416 average cosine overlap 0.2244 number of signature terms -0.1880 max cosine overlap 0.1337 min topic signature overlap 0.0401 min cosine overlap 0.0308 type-token ratio -0.0276 % of words used only once -0.0025 Table 5: Correlation between coverage score and feature values for the 29 DUC01 100-word summaries." ></td>
	<td class="line x" title="170:191	features accuracy P R F SIG 56.25% 0.553 0.600 0.576 SIG+yt 69.27% 0.696 0.674 0.684 ALL 61.45% 0.615 0.589 0.600 ALL+yt 65.10% 0.643 0.663 0.653 Table 6: Logistic regression classification results (accuracy, precision, recall and f-measure) for balanced data of 100-word summaries from DUC02 through DUC04." ></td>
	<td class="line x" title="171:191	ALL+yt also includes the year and task features." ></td>
	<td class="line x" title="172:191	Classification accuracy is considerably higher than the 50% random baseline." ></td>
	<td class="line x" title="173:191	Using all features yields better accuracy (61%) than using solely the 6 significant features (accuracy of 56%)." ></td>
	<td class="line x" title="174:191	In both cases, adding the year and task leads to extra 3% net improvement." ></td>
	<td class="line x" title="175:191	The best overall results are for the SIG+yt classifier with net improvement over the baseline equal to 20%." ></td>
	<td class="line x" title="176:191	At the same time, it should be taken into consideration that the amount of training data for our experiments is small: a total of 192 sets." ></td>
	<td class="line x" title="177:191	Despite this, the measures of input cohesiveness capture enough information to result in a classifier with above-baseline performance." ></td>
	<td class="line x" title="178:191	6 Conclusions We have addressed the question of what makes the writing of a summary for a multi-document input difficult." ></td>
	<td class="line x" title="179:191	Summary length is a significant factor, with all summarizers (people, machines and baselines) performing better at longer summary lengths." ></td>
	<td class="line x" title="180:191	An exploratory analysis of DUC 2001 indicated that systems produce better summaries for cohesive inputs dealing with a clear topic (single event, subject and biographical sets) while non-cohesive sets about multiple events and opposing opinions are consistently of lower quality." ></td>
	<td class="line x" title="181:191	We defined a number of features aimed at capturing input cohesiveness, ranging from simple features such as input length and size to more sophisticated measures such as input set entropy, KL divergence from a background corpus and topic signature terms based on log-likelihood ratio." ></td>
	<td class="line x" title="182:191	Generally, easy to summarize sets are characterized by low entropy, small vocabulary, high average cosine and average topic signature overlaps, high KL divergence and a high percentage of the vocabulary consists of topic signature terms." ></td>
	<td class="line x" title="183:191	Experiments with a logistic regression classifier based on the features further confirms that input cohesiveness is predictive of the difficulty it will pose to automatic summarizers." ></td>
	<td class="line x" title="184:191	Several important notes can be made." ></td>
	<td class="line x" title="185:191	First, it is important to develop strategies that can better handle non-cohesive inputs, reducing fluctuations in system performance." ></td>
	<td class="line x" title="186:191	Most current systems are developed with the expectation they can handle any input but this is evidently not the case and more attention should be paid to the issue." ></td>
	<td class="line x" title="187:191	Second, the interpretations of year to year evaluations can be affected." ></td>
	<td class="line x" title="188:191	As demonstrated, the properties of the input have a considerable influence on summarization quality." ></td>
	<td class="line x" title="189:191	If special care is not taken to ensure that the difficulty of inputs in different evaluations is kept more or less the same, results from the evaluations are not comparable and we cannot make general claims about progress and system improvements between evaluations." ></td>
	<td class="line x" title="190:191	Finally, the presented results are clearly just a beginning in understanding of summarization difficulty." ></td>
	<td class="line x" title="191:191	A more complete characterization of summarization input will be necessary in the future." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2003
Improving the Performance of the Random Walk Model for Answering Complex Questions
Chali, Yllias;Joty, Shafiq R.;"></td>
	<td class="line x" title="1:72	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 912, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:72	c2008 Association for Computational Linguistics Improving the Performance of the Random Walk Model for Answering Complex Questions Yllias Chali and Shafiq R. Joty University of Lethbridge 4401 University Drive Lethbridge, Alberta, Canada, T1K 3M4 {chali,jotys}@cs.uleth.ca Abstract We consider the problem of answering complex questions that require inferencing and synthesizing information from multiple documents and can be seen as a kind of topicoriented, informative multi-document summarization." ></td>
	<td class="line x" title="3:72	The stochastic, graph-based method for computing the relative importance of textual units (i.e. sentences) is very successful in generic summarization." ></td>
	<td class="line x" title="4:72	In this method, a sentence is encoded as a vector in which each component represents the occurrence frequency (TF*IDF) of a word." ></td>
	<td class="line x" title="5:72	However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information." ></td>
	<td class="line x" title="6:72	In this paper, we study the impact of syntactic and shallow semantic information in the graph-based method for answering complex questions." ></td>
	<td class="line x" title="7:72	1 Introduction After having made substantial headway in factoid and list questions, researchers have turned their attention to more complex information needs that cannot be answered by simply extracting named entities like persons, organizations, locations, dates, etc. Unlike informationally-simple factoid questions, complex questions often seek multiple different types of information simultaneously and do not presupposed that one single answer could meet all of its information needs." ></td>
	<td class="line x" title="8:72	For example, with complex questions like What are the causes of AIDS?, the wider focus of this question suggests that the submitter may not have a single or well-defined information need and therefore may be amenable to receiving additional supporting information that is relevant to some (as yet) undefined informational goal." ></td>
	<td class="line x" title="9:72	This type of questions require inferencing and synthesizing information from multiple documents." ></td>
	<td class="line x" title="10:72	In Natural Language Processing (NLP), this information synthesis can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information." ></td>
	<td class="line x" title="11:72	Recently, the graph-based method (LexRank) is applied successfully to generic, multi-document summarization (Erkan and Radev, 2004)." ></td>
	<td class="line x" title="12:72	A topicsensitive LexRank is proposed in (Otterbacher et al., 2005)." ></td>
	<td class="line x" title="13:72	In this method, a sentence is mapped to a vector in which each element represents the occurrence frequency (TF*IDF) of a word." ></td>
	<td class="line x" title="14:72	However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information thus cannot distinguish between The hero killed the villain and The villain killed the hero." ></td>
	<td class="line x" title="15:72	The task like answering complex questions that requires the use of more complex syntactic and semantics, the approaches with only TF*IDF are often inadequate to perform fine-level textual analysis." ></td>
	<td class="line x" title="16:72	In this paper, we extensively study the impact of syntactic and shallow semantic information in measuring similarity between the sentences in the random walk model for answering complex questions." ></td>
	<td class="line x" title="17:72	We argue that for this task, similarity measures based on syntactic and semantic information performs better and can be used to characterize the 9 relation between a question and a sentence (answer) in a more effective way than the traditional TF*IDF based similarity measures." ></td>
	<td class="line x" title="18:72	2 Graph-based Random Walk Model for Text Summarization In (Erkan and Radev, 2004), the concept of graphbased centrality is used to rank a set of sentences, in producing generic multi-document summaries." ></td>
	<td class="line x" title="19:72	A similarity graph is produced where each node represents a sentence in the collection and the edges between nodes measure the cosine similarity between the respective pair of sentences." ></td>
	<td class="line x" title="20:72	Each sentence is represented as a vector of term specific weights." ></td>
	<td class="line x" title="21:72	The term specific weights in the sentence vectors are products of term frequency (tf) and inverse document frequency (idf)." ></td>
	<td class="line x" title="22:72	The degree of a given node is an indication of how much important the sentence is. To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005)." ></td>
	<td class="line x" title="23:72	The score of a sentence is determined by a mixture model: p(s|q) = d rel(s|q)summationtext zC rel(z|q) + (1d)  summationdisplay vC sim(s,v)summationtext zC sim(z,v) p(v|q) (1) Where, p(s|q) is the score of a sentence s given a question q, is determined as the sum of its relevance to the question (i.e. rel(s|q)) and the similarity to other sentences in the collection (i.e. sim(s,v))." ></td>
	<td class="line x" title="24:72	The denominators in both terms are for normalization." ></td>
	<td class="line x" title="25:72	C is the set of all sentences in the collection." ></td>
	<td class="line x" title="26:72	The value of the parameter d which we call bias, is a trade-off between two terms in the equation and is set empirically." ></td>
	<td class="line x" title="27:72	We claim that for a complex task like answering complex questions where the relatedness between the query sentences and the document sentences is an important factor, the graph-based random walk model of ranking sentences would perform better if we could encode the syntactic and semantic information instead of just the bag of word (i.e. TF*IDF) information in calculating the similarity between sentences." ></td>
	<td class="line x" title="28:72	Thus, our mixture model for answering complex questions is: p(s|q) = dTREESIM(s,q) + (1d)  summationdisplay vC TREESIM(s,v)p(v|q) (2) Figure 1: Example of semantic trees Where TREESIM(s,q) is the normalized syntactic (and/or semantic) similarity between the query (q) and the document sentence (s) and C is the set of all sentences in the collection." ></td>
	<td class="line x" title="29:72	In cases where the query is composed of two or more sentences, we compute the similarity between the document sentence (s) and each of the query-sentences (qi) then we take the average of the scores." ></td>
	<td class="line x" title="30:72	3 Encoding Syntactic and Shallow Semantic Structures Encoding syntactic structure is easier and straight forward." ></td>
	<td class="line x" title="31:72	Given a sentence (or query), we first parse it into a syntactic tree using a syntactic parser (i.e. Charniak parser) and then we calculate the similarity between the two trees using the general tree kernel function (Section 4.1)." ></td>
	<td class="line x" title="32:72	Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003)." ></td>
	<td class="line x" title="33:72	For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like Figure 1 which we call Semantic Tree (ST)." ></td>
	<td class="line x" title="34:72	In the semantic tree, arguments are replaced with the most important wordoften referred to as the semantic head." ></td>
	<td class="line x" title="35:72	The sentences may contain one or more subordinate clauses." ></td>
	<td class="line x" title="36:72	For example the sentence, the Vatican, located wholly within Italy uses the Italian lira 10 Figure 2: Two STs composing a STN as their currency. gives the STs as in Figure 2." ></td>
	<td class="line x" title="37:72	As we can see in Figure 2(A), when an argument node corresponds to an entire subordinate clause, we label its leaf with ST , e.g. the leaf of ARG0." ></td>
	<td class="line x" title="38:72	Such ST node is actually the root of the subordinate clause in Figure 2(B)." ></td>
	<td class="line x" title="39:72	If taken separately, such STs do not express the whole meaning of the sentence, hence it is more accurate to define a single structure encoding the dependency between the two predicates as in Figure 2(C)." ></td>
	<td class="line x" title="40:72	We refer to this kind of nested STs as STNs." ></td>
	<td class="line x" title="41:72	4 Syntactic and Semantic Kernels for Text 4.1 Tree Kernels Once we build the trees (syntactic or semantic), our next task is to measure the similarity between the trees." ></td>
	<td class="line x" title="42:72	For this, every tree T is represented by an m dimensional vector v(T) = (v1(T),v2(T),vm(T)), where the i-th element vi(T) is the number of occurrences of the i-th tree fragment in tree T. The tree fragments of a tree are all of its sub-trees which include at least one production with the restriction that no production rules can be broken into incomplete parts." ></td>
	<td class="line x" title="43:72	Implicitly we enumerate all the possible tree fragments 1,2,,m. These fragments are the axis of this m-dimensional space." ></td>
	<td class="line x" title="44:72	Note that this could be done only implicitly, since the number m is extremely large." ></td>
	<td class="line x" title="45:72	Because of this, (Collins and Duffy, 2001) defines the tree kernel algorithm whose computational complexity does not depend on m. We followed the similar approach to compute the tree kernel between two syntactic trees." ></td>
	<td class="line x" title="46:72	4.2 Shallow Semantic Tree Kernel (SSTK) Note that, the tree kernel (TK) function defined in (Collins and Duffy, 2001) computes the number of common subtrees between two trees." ></td>
	<td class="line x" title="47:72	Such subtrees are subject to the constraint that their nodes are taken with all or none of the children they have in the original tree." ></td>
	<td class="line x" title="48:72	Though, this definition of subtrees makes the TK function appropriate for syntactic trees but at the same time makes it not well suited for the semantic trees (ST) defined in Section 3." ></td>
	<td class="line x" title="49:72	For instance, although the two STs of Figure 1 share most of the subtrees rooted in the ST node, the kernel defined above computes no match." ></td>
	<td class="line x" title="50:72	The critical aspect of the TK function is that the productions of two evaluated nodes have to be identical to allow the match of further descendants." ></td>
	<td class="line x" title="51:72	This means that common substructures cannot be composed by a node with only some of its children as an effective ST representation would require." ></td>
	<td class="line x" title="52:72	Moschitti et al.(2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a ST. We followed the similar approach to compute the SSTK." ></td>
	<td class="line x" title="54:72	5 Experiments 5.1 Evaluation Setup The Document Understanding Conference (DUC) series is run by the National Institute of Standards and Technology (NIST) to further progress in summarization and enable researchers to participate in large-scale experiments." ></td>
	<td class="line x" title="55:72	We used the DUC 2007 datasets for evaluation." ></td>
	<td class="line oc" title="56:72	We carried out automatic evaluation of our summaries using ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation." ></td>
	<td class="line o" title="57:72	It measures summary quality by counting overlapping units such as the n-gram (ROUGE-N), word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary." ></td>
	<td class="line o" title="58:72	ROUGE parameters were set as the same as DUC 2007 evaluation setup." ></td>
	<td class="line o" title="59:72	All the ROUGE measures were calculated by running ROUGE-1.5.5 with stemming but no removal of stopwords." ></td>
	<td class="line o" title="60:72	The ROUGE run-time parameters are: ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a 11 The purpose of our experiments is to study the impact of the syntactic and semantic representation for complex question answering task." ></td>
	<td class="line x" title="61:72	To accomplish this, we generate summaries for the topics of DUC 2007 by each of our four systems defined as below: (1) TF*IDF: system is the original topic-sensitive LexRank described in Section 2 that uses the similarity measures based on tf*idf." ></td>
	<td class="line x" title="62:72	(2) SYN: system measures the similarity between the sentences using the syntactic tree and the general tree kernel function defined in Section 4.1." ></td>
	<td class="line x" title="63:72	(3) SEM: system measures the similarity between the sentences using the shallow semantic tree and the shallow semantic tree kernel function defined in Section 4.2." ></td>
	<td class="line x" title="64:72	(4) SYNSEM: system measures the similarity between the sentences using both the syntactic and shallow semantic trees and their associated kernels." ></td>
	<td class="line x" title="65:72	For each sentence it measures the syntactic and semantic similarity with the query and takes the average of these measures." ></td>
	<td class="line x" title="66:72	5.2 Evaluation Results The comparison between the systems in terms of their F-scores is given in Table 1." ></td>
	<td class="line o" title="67:72	The SYN system improves the ROUGE-1, ROUGE-L and ROUGEW scores over the TF*IDF system by 2.84%, 0.53% and 2.14% respectively." ></td>
	<td class="line o" title="68:72	The SEM system improves the ROUGE-1, ROUGE-L, ROUGE-W, and ROUGE-SU scores over the TF*IDF system by 8.46%, 6.54%, 6.56%, and 11.68%, and over the SYN system by 5.46%, 5.98%, 4.33%, and 12.97% respectively." ></td>
	<td class="line o" title="69:72	The SYNSEM system improves the ROUGE-1, ROUGE-L, ROUGE-W, and ROUGESU scores over the TF*IDF system by 4.64%, 1.63%, 2.15%, and 4.06%, and over the SYN system by 1.74%, 1.09%, 0%, and 5.26% respectively." ></td>
	<td class="line o" title="70:72	The SEM system improves the ROUGE-1, ROUGEL, ROUGE-W, and ROUGE-SU scores over the SYNSEM system by 3.65%, 4.84%, 4.32%, and 7.33% respectively which indicates that including syntactic feature with the semantic feature degrades the performance." ></td>
	<td class="line o" title="71:72	6 Conclusion In this paper, we have introduced the syntactic and shallow semantic structures and discussed their imSystems ROUGE 1 ROUGE L ROUGE W ROUGE SU TF*IDF 0.359458 0.334882 0.124226 0.130603 SYN 0.369677 0.336673 0.126890 0.129109 SEM 0.389865 0.356792 0.132378 0.145859 SYNSEM 0.376126 0.340330 0.126894 0.135901 Table 1: ROUGE F-scores for different systems pacts in measuring the similarity between the sentences in the random walk framework for answering complex questions." ></td>
	<td class="line x" title="72:72	Our experiments suggest the following: (a) similarity measures based on the syntactic tree and/or shallow semantic tree outperforms the similarity measures based on the TF*IDF and (b) similarity measures based on the shallow semantic tree performs best for this problem." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2005
Extractive Summaries for Educational Science Content
Chica, Sebastian;Ahmad, Faisal;Martin, James H.;Sumner, Tamara;"></td>
	<td class="line x" title="1:89	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1720, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:89	c2008 Association for Computational Linguistics Extractive Summaries for Educational Science Content Sebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara Sumner Institute of Cognitive Science Department of Computer Science University of Colorado at Boulder sebastian.delachica, faisal.ahmad, james.martin, tamara.sumner@colorado.edu   Abstract This paper describes an extractive summarizer for educational science content called COGENT." ></td>
	<td class="line x" title="3:89	COGENT extends MEAD based on strategies elicited from an empirical study with domain and instructional experts." ></td>
	<td class="line x" title="4:89	COGENT implements a hybrid approach integrating both domain independent sentence scoring features and domain-aware features." ></td>
	<td class="line x" title="5:89	Initial evaluation results indicate that COGENT outperforms existing summarizers and generates summaries that closely resemble those generated by human experts." ></td>
	<td class="line x" title="6:89	1 Introduction Knowledge maps consist of nodes containing rich concept descriptions interconnected using a limited set of relationship types (Holley and Dansereau, 1984)." ></td>
	<td class="line x" title="7:89	Learning research indicates that knowledge maps may be useful for learners to understand the macro-level structure of an information space (O'Donnell et al., 2002)." ></td>
	<td class="line x" title="8:89	Knowledge maps have also emerged as an effective computational infrastructure to support the automated generation of conceptual browsers." ></td>
	<td class="line x" title="9:89	Such conceptual browsers appear to allow students to focus on the science content of large educational digital libraries (Sumner et al., 2003), such as the Digital Library for Earth System Education (DLESE.org)." ></td>
	<td class="line x" title="10:89	Knowledge maps have also shown promise as domain and student knowledge representations to support personalized learning interactions (de la Chica et al., 2008)." ></td>
	<td class="line x" title="11:89	In this paper we describe our progress towards the generation of science concept inventories as summaries of digital library collections." ></td>
	<td class="line x" title="12:89	Such inventories provide the basis for the construction of knowledge maps useful both as computational knowledge representations and as learning resources for presentation to the student." ></td>
	<td class="line x" title="13:89	2 Related Work Our work is informed by efforts to automate the acquisition of ontology concepts from text." ></td>
	<td class="line x" title="14:89	OntoLearn extracts candidate domain terms from texts using a syntactic parse and updates an existing ontology with the identified concepts and relationships (Navigli and Velardi, 2004)." ></td>
	<td class="line x" title="15:89	Knowledge Puzzle focuses on n-gram identification to produce a list of candidate terms pruned using information extraction techniques to derive the ontology (Zouaq et al., 2007)." ></td>
	<td class="line x" title="16:89	Lin and Pantel (2002) discover concepts using clustering by committee to group terms into conceptually related clusters." ></td>
	<td class="line x" title="17:89	These approaches produce ontologies of very fine granularity and therefore graphs that may not be suitable for presentation to a student." ></td>
	<td class="line x" title="18:89	Multi-document summarization (MDS) research also informs our work." ></td>
	<td class="line x" title="19:89	XDoX analyzes large document sets to extract important themes using n-gram scoring and clustering (Hardy et al., 2002)." ></td>
	<td class="line x" title="20:89	Topic representation and topic themes have also served as the basis for the exploration of promising MDS techniques (Harabagiu and Lacatusu, 2005)." ></td>
	<td class="line x" title="21:89	Finally, MEAD is a widely used MDS and evaluation platform (Radev et al., 2000)." ></td>
	<td class="line x" title="22:89	While all these systems have produced promising results in automated evaluations, none have directly targeted educational content collections." ></td>
	<td class="line x" title="23:89	17 3 Empirical Study We have conducted a study to capture how human experts processed digital library resources to create a domain knowledge map." ></td>
	<td class="line x" title="24:89	Four geology and instructional design experts selected 20 resources from DLESE to construct a knowledge map on earthquakes and plates tectonics for high school age learners." ></td>
	<td class="line x" title="25:89	The resulting knowledge map consists of 564 concepts and 578 relationships." ></td>
	<td class="line x" title="26:89	Figure 1." ></td>
	<td class="line x" title="27:89	Expert knowledge map excerpt The concepts include 7,846  words, or 5% of the resources." ></td>
	<td class="line x" title="28:89	Our experts relied on copying-andpasting (58%) and paraphrasing (37%) to create most concepts." ></td>
	<td class="line x" title="29:89	Only 5% of the concepts could not be traced directly to the original resources." ></td>
	<td class="line x" title="30:89	Relationship types were used in a Zipf-like distribution with the top 2 relationship types each accounting for more than 10% of all relationships: elaborations (19%) and examples (14%)." ></td>
	<td class="line x" title="31:89	Analysis by an independent instructional expert indicates that this knowledge map provides adequate coverage of nationally-recognized educational goals on earthquakes and plate tectonics for high school learners using the American Association for the Advancement of Science (AAAS) Benchmarks (Project 2061, 1993)." ></td>
	<td class="line x" title="32:89	Verbal protocol analysis shows that all experts used external sources to create the knowledge map, including their own expertise, other digital library resources, and the National Science Education Standards (NSES), a comprehensive collection of nationally-recognized science learning goals for K12 students (National Research Council, 1996)." ></td>
	<td class="line x" title="33:89	We have examined sentence extraction agreement between experts using the prevalenceadjusted bias-adjusted (PABA) kappa to account for prevalence of judgments and conflicting biases amongst experts (Byrt et al., 1993)." ></td>
	<td class="line x" title="34:89	The average PABA-kappa value of 0.62 indicates that experts substantially agree on sentence extraction from digital library resources." ></td>
	<td class="line x" title="35:89	This level of agreement suggests that these concepts may serve as the reference summary to evaluate our system." ></td>
	<td class="line x" title="36:89	4 Summarizer for Science Education We have implemented an extractive summarizer for educational science content, COGENT, based on MEAD version 3.11 (Radev et al., 2000)." ></td>
	<td class="line x" title="37:89	COGENT complements the default MEAD sentence scoring features with features based on findings from the empirical study." ></td>
	<td class="line x" title="38:89	COGENT represents a hybrid approach integrating bottom-up (hypertext and content word density) and top-down (educational standards and gazetteer) features." ></td>
	<td class="line x" title="39:89	We model how human experts used external information sources with the educational standards feature." ></td>
	<td class="line x" title="40:89	This feature leverages the text of the relevant AAAS Benchmarks and associated NSES." ></td>
	<td class="line x" title="41:89	Each sentence receives a score based on its TFIDF similarity to the textual contents of these learning goals and educational standards." ></td>
	<td class="line x" title="42:89	We have developed a feature that reflects the large number of examples extracted by the experts." ></td>
	<td class="line x" title="43:89	Earth science examples often refer to geographical locations and geological formations." ></td>
	<td class="line x" title="44:89	The gazetteer feature checks named entities from each sentence against the Alexandria Digital Library (ADL) Gazetteer (Hill, 2000)." ></td>
	<td class="line x" title="45:89	A gazetteer is a geo-referencing resource containing location and type information about place-names." ></td>
	<td class="line x" title="46:89	Each sentence receives a TFIDF score based on place-name term frequency and overall uniqueness in the gazetteer." ></td>
	<td class="line x" title="47:89	Our assumption is that geographical locations with more unique names may be more pedagogically relevant." ></td>
	<td class="line x" title="48:89	Based on the intuition that the HTML structure of a resource reflects relevancy, we have developed the hypertext feature." ></td>
	<td class="line x" title="49:89	This feature computes a sentence score directly proportional to the HTML heading level and inversely proportional to the relative paragraph number within a heading and to the relative sentence position within a paragraph." ></td>
	<td class="line x" title="50:89	18 To promote the extraction of sentences containing science concepts, we have developed the content word density feature." ></td>
	<td class="line x" title="51:89	This feature computes the ratio of content to function words in a sentence." ></td>
	<td class="line x" title="52:89	Function words are identified using a stopword list, and the feature only keeps sentences featuring more content words than function words." ></td>
	<td class="line x" title="53:89	We compute the final sentence score by adding the MEAD default feature scores (centroid and position) to the COGENT feature scores (educational standards, gazetteer, and hypertext)." ></td>
	<td class="line x" title="54:89	COGENT keeps sentences that pass the cut-off constraints, including the MEAD sentence length of 9 and COGENT content word density of 50%." ></td>
	<td class="line x" title="55:89	The default MEAD cosine re-ranker eliminates redundant sentences." ></td>
	<td class="line x" title="56:89	Since the experts used 5% of the total word count in the resources, we produce summaries of that same length." ></td>
	<td class="line x" title="57:89	5 Evaluation We have evaluated COGENT by processing the 20 digital library resources used in the empirical study and comparing the output against the concepts identified by the experts." ></td>
	<td class="line x" title="58:89	Three configurations are considered: Random, Default, and COGENT." ></td>
	<td class="line x" title="59:89	The Random summary uses MEAD to extract random sentences." ></td>
	<td class="line x" title="60:89	The Default summary uses the MEAD centroid, position and length default features." ></td>
	<td class="line x" title="61:89	Finally, the COGENT summary extends MEAD with the COGENT features." ></td>
	<td class="line oc" title="62:89	We use ROUGE (Lin, 2004) to assess summary quality using common n-gram counts and longest common subsequence (LCS) measures." ></td>
	<td class="line oc" title="63:89	We report on ROUGE-1 (unigrams), ROUGE-2 (bigrams), ROUGE W-1.2 (weighted LCS), and ROUGE-S* (skip bigrams) as they have been shown to correlate well with human judgments for longer multidocument summaries (Lin, 2004)." ></td>
	<td class="line x" title="64:89	Table 1 shows the results for recall (R), precision (P), and balanced f-measure (F)." ></td>
	<td class="line x" title="65:89	Random Default COGENT R 0.4855 0.4976 0.6073 P 0.5026 0.5688 0.6034 R-1 F 0.4939 0.5308 0.6054 R 0.0972 0.1321 0.1907 P 0.1006 0.1510 0.1895 R-2 F 0.0989 0.1409 0.1901 R 0.0929 0.0951 0.1185 P 0.1533 0.1733 0.1877 R-W-1.2 F 0.1157 0.1228 0.1453   Random Default COGENT R 0.2481 0.2620 0.3820 P 0.2657 0.3424 0.3772 R-S* F 0.2566 0.2969 0.3796 Table 1." ></td>
	<td class="line x" title="66:89	Quality evaluation results Table 1 indicates that COGENT consistently outperforms the Random and Default summaries." ></td>
	<td class="line x" title="67:89	These results indicate the promise of our approach to generate extractive summaries of educational science content." ></td>
	<td class="line x" title="68:89	Given our interest in generating a pedagogically effective domain knowledge map, we have also conducted a content-centric evaluation." ></td>
	<td class="line x" title="69:89	To characterize the COGENT summary contents, one of the authors manually constructed a summary corresponding to the best case output for an extractive summarizer." ></td>
	<td class="line x" title="70:89	This Best Case summary comprises all the sentences from the resources that align to all the concepts selected by the experts." ></td>
	<td class="line x" title="71:89	This summary comprises 621 sentences consisting of 13,116 words, or about a 9% word compression." ></td>
	<td class="line o" title="72:89	We use ROUGE-L to examine the union LCS between the reference and candidate summaries, thus capturing their linguistic surface structure similarity." ></td>
	<td class="line x" title="73:89	We also use MEAD to report on cosine similarity." ></td>
	<td class="line x" title="74:89	Table 2 shows the results for recall (R), precision (P), and balanced f-measure (F)." ></td>
	<td class="line x" title="75:89	Random (5%) Default (5%) COGENT (5%) Best Case (9%) R 0.4814 0.4919 0.6021 0.9669 P 0.4982 0.5623 0.5982 0.6256 R-L F 0.4897 0.5248 0.6001 0.7597 Cosine 0.5382 0.6748 0.8325 0.9323 Table 2." ></td>
	<td class="line o" title="76:89	Content evaluation results (word compression) The ROUGE-L scores consistently indicate that the COGENT summary may be closer to the reference in linguistic surface structure than either the Random or Default summaries." ></td>
	<td class="line o" title="77:89	Since the COGENT ROUGE-L recall score (R=0." ></td>
	<td class="line x" title="78:89	6021) is lower than the Best Case (R=0.9669), it is likely that COGENT may be extracting different sentences than those selected by the experts." ></td>
	<td class="line x" title="79:89	Based on the high cosine similarity with the reference (0.8325), we hypothesize that COGENT may be selecting sentences that cover very similar concepts to those selected by the experts, but expressed differently." ></td>
	<td class="line o" title="80:89	Given the difference in word compression for the Best Case summary, we have performed an 19 incremental analysis using the ROUGE-L measure shown in Figure 2." ></td>
	<td class="line x" title="81:89	ROUGE-L COGENT Evaluation 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 0 5 10 15 20 25 30 MEAD Word Percent Compression Recall Precision F-Measure Figure 2." ></td>
	<td class="line o" title="82:89	Incremental COGENT ROUGE-L analysis Figure 2 indicates that COGENT can match the Best Case recall (R=0.9669) by generating a longer summary." ></td>
	<td class="line x" title="83:89	For educational applications, lengthier summaries may be better suited for computational purposes, such as diagnosing student understanding, while shorter summaries may be more appropriate for display to the student." ></td>
	<td class="line x" title="84:89	6 Conclusions COGENT extends MEAD based on strategies elicited from an empirical study with domain and instructional experts." ></td>
	<td class="line x" title="85:89	Initial evaluation results indicate that COGENT holds promise for identifying important domain pedagogical concepts." ></td>
	<td class="line x" title="86:89	We are exploring portability to other science education domains and machine learning techniques to connect concepts into a knowledge map." ></td>
	<td class="line x" title="87:89	Automating the creation of inventories of pedagogically important concepts may represent an important step towards scalable intelligent tutoring systems." ></td>
	<td class="line x" title="88:89	Acknowledgements This research is funded in part by the National Science Foundation under NSF IIS/ALT Award 0537194." ></td>
	<td class="line x" title="89:89	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2051
Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries
Liu, Feifan;Liu, Yang;"></td>
	<td class="line x" title="1:108	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 201204, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:108	c2008 Association for Computational Linguistics Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries Feifan Liu, Yang Liu The University of Texas at Dallas Richardson, TX 75080, USA ffliu,yangl@hlt.utdallas.edu Abstract Automatic summarization evaluation is critical to the development of summarization systems." ></td>
	<td class="line x" title="3:108	While ROUGE has been shown to correlate well with human evaluation for content match in text summarization, there are many characteristics in multiparty meeting domain, which may pose potential problems to ROUGE." ></td>
	<td class="line o" title="4:108	In this paper, we carefully examine how well the ROUGE scores correlate with human evaluation for extractive meeting summarization." ></td>
	<td class="line x" title="5:108	Our experiments show that generally the correlation is rather low, but a significantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disfluencies and speaker information, especially when evaluating system-generated summaries." ></td>
	<td class="line x" title="6:108	1 Introduction Meeting summarization has drawn an increasing attention recently; therefore a study on the automatic evaluation metrics for this task is timely." ></td>
	<td class="line x" title="7:108	Automatic evaluation helps to advance system development and avoids the labor-intensive and potentially inconsistent human evaluation." ></td>
	<td class="line pc" title="8:108	ROUGE (Lin, 2004) has been widely used for summarization evaluation." ></td>
	<td class="line pc" title="9:108	In the news article domain, ROUGE scores have been shown to be generally highly correlated with human evaluation in content match (Lin, 2004)." ></td>
	<td class="line n" title="10:108	However, there are many differences between written texts (e.g., news wire) and spoken documents, especially in the meeting domain, for example, the presence of disfluencies and multiple speakers, and the lack of structure in spontaneous utterances." ></td>
	<td class="line o" title="11:108	The question of whether ROUGE is a good metric for meeting summarization is unclear." ></td>
	<td class="line n" title="12:108	(Murray et al., 2005) have reported that ROUGE-1 (unigram match) scores have low correlation with human evaluation in meetings." ></td>
	<td class="line o" title="13:108	In this paper we investigate the correlation between ROUGE and human evaluation of extractive meeting summaries and focus on two issues specific to the meeting domain: disfluencies and multiple speakers." ></td>
	<td class="line x" title="14:108	Both human and system generated summaries are used." ></td>
	<td class="line o" title="15:108	Our analysis shows that by integrating meeting characteristics into ROUGE settings, better correlation can be achieved between the ROUGE scores and human evaluation based on Spearmans rho in the meeting domain." ></td>
	<td class="line x" title="16:108	2 Related work Automaticsummarizationevaluationcanbebroadlyclassified into two categories (Jones and Galliers, 1996): intrinsic and extrinsic evaluation." ></td>
	<td class="line x" title="17:108	Intrinsic evaluation, such as relative utility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example,informativeness,redundancy,andcoherence)." ></td>
	<td class="line x" title="18:108	Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks." ></td>
	<td class="line x" title="19:108	In this study, we concentrate on the automatic intrinsic summarization evaluation." ></td>
	<td class="line x" title="20:108	It has been extensively studied in text summarization." ></td>
	<td class="line oc" title="21:108	Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006)." ></td>
	<td class="line x" title="22:108	With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics." ></td>
	<td class="line x" title="23:108	Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries." ></td>
	<td class="line o" title="24:108	However (Zhu and Penn, 2005) found a statistically significant difference between the ASR-inspired metrics and those taken from text summarization (e.g., RU, ROUGE) on a subset of the Switchboard data." ></td>
	<td class="line o" title="25:108	ROUGE has been used in meeting summarization evaluation (Murray et al., 2005; Galley, 2006), yet the question remained whether ROUGE is a good metric for the meeting domain." ></td>
	<td class="line p" title="26:108	(Murray et al., 2005) showed low correlation of ROUGE and human evaluation in meeting summarization evaluation; however, they 201 simply used ROUGE as is and did not take into account the meeting characteristics during evaluation." ></td>
	<td class="line o" title="27:108	In this paper, we ask the question of whether ROUGE correlates with human evaluation of extractive meeting summaries and whether we can modify ROUGE to account for the meeting style for a better correlation with human evaluation." ></td>
	<td class="line x" title="28:108	3 Experimental Setup 3.1 Data We used the ICSI meeting data (Janin et al., 2003) that contains naturally-occurring research meetings." ></td>
	<td class="line x" title="29:108	All the meetingshavebeentranscribedandannotatedwithdialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005)." ></td>
	<td class="line x" title="30:108	For this study, we used the same 6 test meetings as in (Murray et al., 2005; Galley, 2006)." ></td>
	<td class="line x" title="31:108	Each meeting already has 3 human summaries from 3 common annotators." ></td>
	<td class="line x" title="32:108	We recruited another 3 human subjects to generate 3 more human summaries, in order to create more data points for a reliable analysis." ></td>
	<td class="line x" title="33:108	The Kappa statistics for those 6 different annotators varies from 0.11 to 0.35 for different meetings." ></td>
	<td class="line x" title="34:108	The human summaries have different length, containing around 6.5% of the selected DAs and 13.5% of the words respectively." ></td>
	<td class="line x" title="35:108	We used four different system summaries for each of the 6 meetings: one based on the MMR method in MEAD (Carbonell and Goldstein, 1998; et al., 2003), the other three are the system output from (Galley, 2006; Murray et al., 2005; Xie and Liu, 2008)." ></td>
	<td class="line x" title="36:108	All the system generated summaries contain around 5% of the DAs and 16% of the words of the entire meeting." ></td>
	<td class="line o" title="37:108	Thus, intotalwehave36humansummariesand 24 system summaries on the 6 test meetings, on which the correlation between ROUGE and human evaluation is calculated and investigated." ></td>
	<td class="line o" title="38:108	All the experiments in this paper are based on human transcriptions, with a central interest on whether some characteristics of the meeting recordings affect the correlation between ROUGE and human evaluations, without the effect from speech recognition or automatic sentence segmentation errors." ></td>
	<td class="line oc" title="39:108	3.2 Automatic ROUGE Evaluation ROUGE(Lin, 2004)measuresthen-grammatchbetween system generated summaries and human summaries." ></td>
	<td class="line o" title="40:108	In most of this study, we used the same options in ROUGE as in the DUC summarization evaluation (NIST, 2007), and modify the input to ROUGE to account for the following two phenomena." ></td>
	<td class="line x" title="41:108	 Disfluencies Meetings contain spontaneous speech with many disfluencies, such as filled pauses (uh, um), discoursemarkers(e.g.,Imean,youknow),repetitions, corrections, and incomplete sentences." ></td>
	<td class="line x" title="42:108	There have been efforts on the study of the impact of disfluencies on summarization techniques (Liu et al., 2007; Zhu and Penn, 2006) and human readability (Jones et al., 2003)." ></td>
	<td class="line x" title="43:108	However, it is not clear whether disfluencies impact automatic evaluation of extractive meeting summarization." ></td>
	<td class="line x" title="44:108	Since we use extractive summarization, summary sentences may contain difluencies." ></td>
	<td class="line x" title="45:108	We hand annotated the transcripts for the 6 meetings and marked the disfluencies such that we can remove them to obtain cleaned up sentences for those selected summary sentences." ></td>
	<td class="line o" title="46:108	To study the impact of disfluencies, we run ROUGE using two different inputs: summaries based on the original transcription, and the summaries with disfluencies removed." ></td>
	<td class="line x" title="47:108	 Speaker information The existence of multiple speakers in meetings raises questions about the evaluation method." ></td>
	<td class="line x" title="48:108	(Galley, 2006) considered some location constrains in meeting summarization evaluation, which utilizes speaker information to some extent." ></td>
	<td class="line x" title="49:108	In this study weusethedatainseparatechannelsforeachspeaker and thus have the speaker information available for each sentence." ></td>
	<td class="line o" title="50:108	We associate the speaker ID with each word, treat them together as a new word in the input to ROUGE." ></td>
	<td class="line x" title="51:108	3.3 Human Evaluation Five human subjects (all undergraduate students in Computer Science) participated in human evaluation." ></td>
	<td class="line x" title="52:108	In total, there are 20 different summaries for each of the 6 test meetings: 6 human-generated, 4 system-generated, and their corresponding ones with disfluencies removed." ></td>
	<td class="line x" title="53:108	We assigned 4 summaries with different configurations to each human subject: human vs. system generated summaries, with or without disfluencies." ></td>
	<td class="line x" title="54:108	Each human evaluated 24 summaries in total, for the 6 test meetings." ></td>
	<td class="line x" title="55:108	For each summary, the human subjects were asked to rate the following statements using a scale of 1-5 according to the extent of their agreement with them." ></td>
	<td class="line x" title="56:108	 S1: The summary reflects the discussion flow in the meeting very well." ></td>
	<td class="line x" title="57:108	 S2: Almost all the important topic points of the meeting are represented." ></td>
	<td class="line x" title="58:108	 S3: Most of the sentences in the summary are relevant to the original meeting." ></td>
	<td class="line x" title="59:108	 S4: The information in the summary is not redundant." ></td>
	<td class="line x" title="60:108	 S5: Therelationshipbetweentheimportanceofeachtopic in the meeting and the amount of summary space given to that topic seems appropriate." ></td>
	<td class="line x" title="61:108	 S6: The relationship between the role of each speaker and the amount of summary speech selected for that speaker seems appropriate." ></td>
	<td class="line x" title="62:108	 S7: Some sentences in the summary convey the same meaning." ></td>
	<td class="line x" title="63:108	 S8: Some sentences are not necessary (e.g., in terms of importance) to be included in the summary." ></td>
	<td class="line x" title="64:108	 S9: The summary is helpful to someone who wants to know what are discussed in the meeting." ></td>
	<td class="line x" title="65:108	202 These statements are an extension of those used in (Murray et al., 2005) for human evaluation of meeting summaries." ></td>
	<td class="line x" title="66:108	The additional ones we added were designed to account for the discussion flow in the meetings." ></td>
	<td class="line x" title="67:108	Some of the statements above are used to measure similar aspects, but from different perspectives, such as S5 and S6, S4 and S7." ></td>
	<td class="line x" title="68:108	This may reduce some accidental noise in human evaluation." ></td>
	<td class="line x" title="69:108	We grouped these statements into 4 categories: Informative Structure (IS): S1, S5 and S6; Informative Coverage (IC): S2 and S9; Informative Relevance (IRV): S3 and S8; and Informative Redundancy (IRD): S4 and S7." ></td>
	<td class="line o" title="70:108	4 Results 4.1 Correlation between Human Evaluation and Original ROUGE Score Similar to (Murray et al., 2005), we also use Spearmans rank coefficient (rho) to investigate the correlation between ROUGE and human evaluation." ></td>
	<td class="line x" title="71:108	We have 36 human summaries and 24 system summaries for the 6 meetings in our study." ></td>
	<td class="line o" title="72:108	For each of the human summaries, the ROUGE scores are generated using the other 5 humansummariesasreferences." ></td>
	<td class="line o" title="73:108	Forsystemgeneratedsummaries, we calculate the ROUGE score using 5 human references, and then obtain the average from 6 such setups." ></td>
	<td class="line x" title="74:108	The correlation results are presented in Table 1." ></td>
	<td class="line x" title="75:108	In addition to the overall average for human evaluation (H AVG), we calculatedthe average score foreach evaluation category (see Section 3.3)." ></td>
	<td class="line x" title="76:108	For ROUGE evaluation, we chose the F-measure for R-1 (unigram) and R-SU4 (skip-bigram with maximum gap length of 4), which is based on our observation that other scores in ROUGE are always highly correlated (rho>0.9) to either of them for this task." ></td>
	<td class="line x" title="77:108	We compute the correlation separately for the human and system summaries in order to avoid the impact due to the inherent difference between the two different summaries." ></td>
	<td class="line o" title="78:108	Correlation on Human Summaries H AVG H IS H IC H IRV H IRD R-1 0.09 0.22 0.21 0.03 -0.20 R-SU4 0.18 0.33 0.38 0.04 -0.30 Correlation on System Summaries R-1 -0.07 -0.02 -0.17 -0.27 -0.02 R-SU4 0.08 0.05 0.01 -0.15 0.14 Table 1: Spearmans rho between human evaluation (H) and ROUGE (R) with basic setting." ></td>
	<td class="line x" title="79:108	We can see that R-SU4 obtains a higher correlation with human evaluation than R-1 on the whole, but still very low, which is consistent with the previous conclusion from (Murray et al., 2005)." ></td>
	<td class="line x" title="80:108	Among the four categories, better correlation is achieved for information structure (IS) and information coverage (IC) compared to the other two categories." ></td>
	<td class="line o" title="81:108	This is consistent with what ROUGE is designed for, recall oriented understudy gisting evaluation  we expect it to model IS and IC well by ngram and skip-bigram matching but not relevancy (IRV) and redundancy (IRD) effectively." ></td>
	<td class="line x" title="82:108	In addition, we found low correlation on system generated summaries, suggesting it is more challenging to evaluate those summaries both by humans and the automatic metrics." ></td>
	<td class="line o" title="83:108	4.2 Impacts of Disfluencies on Correlation Table 2 shows the correlation results between ROUGE (R-SU4) and human evaluation on the original and cleaned up summaries respectively." ></td>
	<td class="line n" title="84:108	For human summaries, after removing disfluencies, the correlation between ROUGE and human evaluation improves on the whole, but degrades on information structure (IS) and information coverage (IC) categories." ></td>
	<td class="line x" title="85:108	However, for system summaries, there is a significant gain of correlation on those two evaluation categories, even though no improvement on the overall average score." ></td>
	<td class="line x" title="86:108	Our hypothesis for this is that removing disfluencies helps remove the noise in the system generated summaries and make them more easily to be evaluated by human and machines." ></td>
	<td class="line x" title="87:108	In contrast, the human created summaries have better quality in terms of the information content and may not suffer as much from the disfluencies contained in the summary." ></td>
	<td class="line x" title="88:108	Correlation on Human Summaries H AVG H IS H IC H IRV H IRD Original 0.18 0.33 0.38 0.04 -0.30 Disfluencies 0.21 0.21 0.31 0.19 -0.16 removed Correlation on System Summaries Original 0.08 0.05 0.01 -0.15 0.14) Disfluencies 0.08 0.22 0.19 -0.02 -0.07 removed Table 2: Effect of disfluencies on the correlation between RSU4 and human evaluation." ></td>
	<td class="line o" title="89:108	4.3 Incorporating Speaker Information We further incorporated speaker information in ROUGE setting using the summaries with disfluencies removed." ></td>
	<td class="line x" title="90:108	Table 3 presents the resulting correlation values between ROUGE SU4 score and human evaluation." ></td>
	<td class="line x" title="91:108	For human summaries, addingspeakerinformationslightlydegraded the correlation, but it is still better compared to using the original transcripts (results in Table 1)." ></td>
	<td class="line x" title="92:108	For the systemsummaries,theoverallcorrelationissignificantlyimproved, with some significant improvement in the information redundancy (IRD) category." ></td>
	<td class="line p" title="93:108	This suggests that by leveraging speaker information, ROUGE can assign better credits or penalties to system generated summaries (same words from different speakers will not be counted as a match), and thus yield better correlation with human evaluation; whereas for human summaries, this may not happen often." ></td>
	<td class="line x" title="94:108	For similar sentences from different speakers, human annotators are more likely to agree with each 203 other in their selection compared to automatic summarization." ></td>
	<td class="line x" title="95:108	Correlation on Human Summaries Speaker Info." ></td>
	<td class="line x" title="96:108	H AVG H IS H IC H IRV H IRD NO 0.21 0.21 0.31 0.19 -0.16 YES 0.20 0.20 0.27 0.12 -0.09 Correlation on System Summaries NO 0.08 0.22 0.19 -0.02 -0.07 YES 0.14 0.20 0.16 0.02 0.21 Table 3: Effect of speaker information on the correlation between R-SU4 and human evaluation." ></td>
	<td class="line o" title="97:108	5 Conclusion and Future Work In this paper, we have made a first attempt to systematically investigate the correlation of automatic ROUGE scores with human evaluation for meeting summarization." ></td>
	<td class="line o" title="98:108	Adaptations on ROUGE setting based on meeting characteristics are proposed and evaluated using Spearmans rank coefficient." ></td>
	<td class="line n" title="99:108	Our experimental results show that in general the correlation between ROUGE scores and human evaluation is low, with ROUGE SU4 score showing better correlation than ROUGE-1 score." ></td>
	<td class="line p" title="100:108	There is significant improvement in correlation when disfluencies are removed and speaker information is leveraged, especiallyforevaluatingsystem-generatedsummaries." ></td>
	<td class="line o" title="101:108	In addition,weobservethatthecorrelationisaffecteddifferently by those factors for human summaries and systemgenerated summaries." ></td>
	<td class="line o" title="102:108	In our future work we will examine the correlation between each statement and ROUGE scores to better represent human evaluation results instead of using simply the average over all the statements." ></td>
	<td class="line x" title="103:108	Further studies are also needed using a larger data set." ></td>
	<td class="line x" title="104:108	Finally, we plan to investigate meeting summarization evaluation using speech recognition output." ></td>
	<td class="line x" title="105:108	Acknowledgments TheauthorsthankUniversityofEdinburghforprovidingtheannotated ICSI meeting corpus and Michel Galley for sharing his tool to process the annotated data." ></td>
	<td class="line x" title="106:108	We also thank Gabriel Murray and Michel Galley for letting us use their automatic summarization system output for this study." ></td>
	<td class="line x" title="107:108	This work is supported by NSF grant IIS-0714132." ></td>
	<td class="line x" title="108:108	Any opinions expressed in this work are those of the authors and do not necessarily reflect the views of NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2052
FastSum: Fast and Accurate Query-based Multi-document Summarization
Schilder, Frank;Kondadadi, Ravikumar;"></td>
	<td class="line x" title="1:86	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 205208, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:86	c2008 Association for Computational Linguistics FastSum: Fast and accurate query-based multi-document summarization Frank Schilder and Ravikumar Kondadadi Research & Development Thomson Corp. 610 Opperman Drive, Eagan, MN 55123, USA FirstName.LastName@Thomson.com Abstract Wepresentafastquery-basedmulti-document summarizer called FastSum based solely on word-frequency features of clusters, documents and topics." ></td>
	<td class="line x" title="3:86	Summary sentences are ranked by a regression SVM." ></td>
	<td class="line x" title="4:86	The summarizer does not use any expensive NLP techniques such as parsing, tagging of names or even part of speech information." ></td>
	<td class="line x" title="5:86	Still, the achieved accuracy is comparable to the best systems presented in recent academic competitions (i.e., Document Understanding Conference (DUC))." ></td>
	<td class="line x" title="6:86	Because of a detailed feature analysis using Least Angle Regression (LARS), FastSum can rely on a minimal set of featuresleading tofastprocessingtimes: 1250 news documents in 60 seconds." ></td>
	<td class="line x" title="7:86	1 Introduction In this paper, we propose a simple method for effectively generating query-based multi-document summaries without any complex processing steps." ></td>
	<td class="line x" title="8:86	It only involves sentence splitting, filtering candidate sentences and computing the word frequencies in the documents of a cluster, topic description and the topic title." ></td>
	<td class="line x" title="9:86	We use a machine learning technique called regression SVM, as proposed by (Li et al., 2007)." ></td>
	<td class="line x" title="10:86	For the feature selection we use a new model selection technique called Least Angle Regression (LARS) (Efron et al., 2004)." ></td>
	<td class="line x" title="11:86	Even though machine learning approaches dominated the field of summarization systems in recent DUC competitions, not much effort has been spent in finding simple but effective features." ></td>
	<td class="line x" title="12:86	Exceptions are the SumBasic system that achieves reasonable results with only one feature (i.e., word frequency in document clusters) (Nenkova and Vanderwende, 2005)." ></td>
	<td class="line x" title="13:86	Our approach goes beyond SumBasic by proposinganevenmorepowerfulfeaturethatproves to be the best predictor in all three recent DUC corpora." ></td>
	<td class="line x" title="14:86	In order to prove that our feature is more predictivethanotherfeaturesweprovidearigorousfeature analysis by employing LARS." ></td>
	<td class="line x" title="15:86	Scalability is normally not considered when different summarization systems are compared." ></td>
	<td class="line x" title="16:86	Processing time of more than several seconds per summary should be considered unacceptable, in particular, if you bear in mind that using such a system should help a user to process lots of data faster." ></td>
	<td class="line x" title="17:86	Our focus is on selecting the minimal set of features that are computationally less expensive than other features (i.e., full parse)." ></td>
	<td class="line x" title="18:86	Since FastSum can rely on a minimal set of features determined by LARS, it can process 1250 news documents in 60 seconds.1 A comparison test with the MEAD system2 showed that FastSum is more than 4 times faster." ></td>
	<td class="line x" title="19:86	2 System description We use a machine learning approach to rank all sentences in the topic cluster for summarizability." ></td>
	<td class="line x" title="20:86	We use some features from Microsofts PYTHY system (Toutonova et al., 2007), but added two new features, which turned out to be better predictors." ></td>
	<td class="line x" title="21:86	First, the pre-processing module carries out tokenization and sentence splitting." ></td>
	<td class="line x" title="22:86	We also created a sentence simplification component which is based 14-way/2.0GHz PIII Xeon 4096Mb Memory 2http://www.summarization.com/mead/ 205 on a few regular expressions to remove unimportant components of a sentence (e.g., As a matter of fact,)." ></td>
	<td class="line x" title="23:86	This processing step does not involve any syntactic parsing though." ></td>
	<td class="line x" title="24:86	For further processing, we ignore all sentences that do not have at least two exact word matches or at least three fuzzy matches with the topic description.3 Features are mainly based on word frequencies of words in the clusters, documents and topics." ></td>
	<td class="line x" title="25:86	A cluster contains 25 documents and is associated with a topic." ></td>
	<td class="line x" title="26:86	The topic contains a topic title and the topic descriptions." ></td>
	<td class="line x" title="27:86	The topic title is list of key words or phrases describing the topic." ></td>
	<td class="line x" title="28:86	The topic description contains the actual query or queries (e.g., Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999.)." ></td>
	<td class="line x" title="29:86	The features we used can be divided into two sets; word-based and sentence-based." ></td>
	<td class="line x" title="30:86	Word-based featuresarecomputedbasedontheprobabilityofwords for the different containers (i.e., cluster, document, topic title and description)." ></td>
	<td class="line x" title="31:86	At runtime, the different probabilities of all words in a candidate sentence are added up and normalized by length." ></td>
	<td class="line x" title="32:86	Sentence-based features include the length and position of the sentence in the document." ></td>
	<td class="line x" title="33:86	The starred features 1 and 4 are introduced by us, whereas the others can be found in earlier literature.4 *1 Topic title frequency (1): ratio of number of words ti in the sentence s that also appear in the topic title T to the total number of words t1|s| in the sentence s: summationtext|s| i=1 fT (ti) |s| , where fT = braceleftBigg 1 : ti  T 0 : otherwise 2 Topic description frequency (2): ratio of number of words ti in the sentence s that also appear in the topic description D to the total number of words t1|s| in the sentence s: summationtext|s| i=1 fD(ti) |s| , where fD = braceleftBigg 1 : ti  D 0 : otherwise 3 Content word frequency(3): the average content word probability pc(ti) of all content words 3Fuzzy matches are defined by the OVERLAP similarity (Bollegala et al., 2007) of at least 0.1." ></td>
	<td class="line x" title="34:86	4The numbers are used in the feature analysis, as in figure 2." ></td>
	<td class="line x" title="35:86	t1|s| in a sentence s. The content word probability is defined as pc(ti) = nN , where n is the number of times the word occurred in the cluster and N is the total number of words in the cluster: summationtext|s| i=1 pc(ti) |s| *4 Document frequency (4): the average document probability pd(ti) of all content words t1|s| in a sentence s. The document probability is defined as pd(ti) = dD, where d is the number of documents the word ti occurred in for a given cluster and D is the total number of documents in the cluster: summationtext|s| i=1 pd(ti) |s| The remaining features are Headline frequency (5), Sentence length (6), Sentence position (binary) (7), and Sentence position (real) (8) Eventually, each sentence is associated with a score which is a linear combination of the above mentioned feature values." ></td>
	<td class="line x" title="36:86	We ignore all sentences that do not have at least two exact word matches.5 In order to learn the feature weights, we trained a SVM on the previous years data using the same feature set." ></td>
	<td class="line x" title="37:86	We used a regression SVM." ></td>
	<td class="line x" title="38:86	In regression, the task is to estimate the functional dependence of a dependent variable on a set of independent variables." ></td>
	<td class="line x" title="39:86	In our case, the goal is to estimate the score of a sentence based on the given feature set." ></td>
	<td class="line x" title="40:86	In order to get training data, we computed the word overlap between the sentences from the document clusters and the sentences in DUC model summaries." ></td>
	<td class="line x" title="41:86	We associated the word overlap score to the corresponding sentence to generate the regression data." ></td>
	<td class="line x" title="42:86	As a last step, we use the pivoted QR decomposition to handle redundancy." ></td>
	<td class="line x" title="43:86	The basic idea is to avoid redundancybychangingtherelativeimportanceoftherest of the sentences based on the currently selected sentence." ></td>
	<td class="line x" title="44:86	The final summary is created from the ranked sentence list after the redundancy removal step." ></td>
	<td class="line x" title="45:86	3 Results We compared our system with the top performing systems in the last two DUC competitions." ></td>
	<td class="line oc" title="46:86	With our best performing features, we get ROUGE-2 (Lin, 2004) scores of 0.11 and 0.0925 on 2007 and 2006 5This threshold was derived experimentally with previous data." ></td>
	<td class="line x" title="47:86	206 IIIT MS LIP6 IDA PekingFastSumCataloniagen." ></td>
	<td class="line o" title="48:86	Baseline FastSum, 6 Top Systems and generic baseline for DUC 2007 ROUGE2 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 Figure 1: ROUGE-2 results including 95%-confidence intervals for the top 6 systems, FastSum and the generic baseline for DUC 2007 DUC data, respectively." ></td>
	<td class="line x" title="49:86	These scores correspond to rank 6th for DUC 2007 and the 2nd rank for DUC 2006." ></td>
	<td class="line x" title="50:86	Figure 1 shows a graphical comparison of our system with the top 6 systems in DUC 2007." ></td>
	<td class="line x" title="51:86	According to an ANOVA test carried out by the DUC organizers, these 6 systems are significant better than the remaining 26 participating systems." ></td>
	<td class="line x" title="52:86	Note that our system is better than the PYTHY system for 2006, if no sentence simplification was carried out (DUC 2006: 0.089 (without simplification); 0.096 (with simplification))." ></td>
	<td class="line x" title="53:86	Sentence simplification is a computationally expensive process, because it requires a syntactic parse." ></td>
	<td class="line x" title="54:86	We evaluated the performance of the FastSum algorithm using each of the features separately." ></td>
	<td class="line o" title="55:86	Table 1 shows the ROUGE score (recall) of the summaries generated when we used each of the features by themselves on 2006 and 2007 DUC data, trained on the data from the respective previous year." ></td>
	<td class="line x" title="56:86	Using only the Document frequency feature by itself leads to the second best system for DUC 2006 and to the tenth best system for DUC 2007." ></td>
	<td class="line x" title="57:86	This first simple analysis of features indicates that a more rigorous feature analysis would have benefits for building simpler models." ></td>
	<td class="line x" title="58:86	In addition, feature selection could be guided by the complexity of the features preferring those features that are computationally inexpensive." ></td>
	<td class="line o" title="59:86	Feature name 2007 2006 Title word frequency 0.096 0.0771 Topic word frequency 0.0996 0.0883 Content word frequency 0.1046 0.0839 Document frequency 0.1061 0.0903 Headline frequency 0.0938 0.0737 Sentence length 0.054 0.0438 Sentence position(binary) 0.0522 0.0484 Sentence position (real-valued) 0.0544 0.0458 Table 1: ROUGE-2 scores of individual features We chose a so-called model selection algorithm to find a minimal set of features." ></td>
	<td class="line x" title="60:86	This problem can be formulated as a shrinkage and selection method for linear regression." ></td>
	<td class="line x" title="61:86	The Least Angle Regression (LARS) (Efron et al., 2004) algorithm can be used for computing the least absolute shrinkage and selection operator (LASSO) (Tibshirani, 1996).At each stage in LARS, the feature that is most correlated with the response is added to the model." ></td>
	<td class="line x" title="62:86	The coefficient of the feature is set in the direction of the sign of the features correlation with the response." ></td>
	<td class="line x" title="63:86	We computed LARS on the DUC data sets from the last three years." ></td>
	<td class="line x" title="64:86	The graphical results for 2007 are shown in figure 2." ></td>
	<td class="line x" title="65:86	In a LARS graph, features are plotted on the x-axis and the corresponding coefficients are shown on y-axis." ></td>
	<td class="line x" title="66:86	The value on the xaxis is the ratio of norm of the coefficent vector to the maximal norm with no constraint." ></td>
	<td class="line x" title="67:86	The earlier a feature appears on the x-axis, the better it is. Table 2 summarizes the best four features we determined with LARS for the three available DUC data sets." ></td>
	<td class="line x" title="68:86	Year Top Features 2005 4 2 5 1 2006 4 3 2 1 2007 4 3 5 2 Table 2: The 4 top features for the DUC 2005, 2006 and 2007 data Table2 showsthatfeature 4, documentfrequency, is consistently the most important feature for all three data sets." ></td>
	<td class="line x" title="69:86	Content word frequency (3), on the other hand, comes in as second best feature for 2006 and 2007, but not for 2005." ></td>
	<td class="line x" title="70:86	For the 2005 data, the Topic description frequency is the second best feature." ></td>
	<td class="line x" title="71:86	This observation is reflected by our single fea207 * * * * ** * **** 0.0 0.2 0.4 0.6 0.8 1.0 0 2 4 6 8 2007 |beta|/max|beta| Standardized Coefficients * ** * **** * * ** * * *** * * * ** * * *** * *** * ** *** * LASSO 10 8 2 3 4 Figure 2: Graphical output of LARS analysis: Top features for 2007: 4 Document frequency, 3 Content word frequency, 5 Headline frequency, 2 Topic description frequency ture analysis for DUC 2006, as shown in table 1." ></td>
	<td class="line x" title="72:86	Similarly, Vanderwende et al.(2006) report that they gave the Topic description frequency a much higher weight than the Content word frequency." ></td>
	<td class="line x" title="74:86	Consequently, we have shown that our new feature Document frequency is consistently the best feature for all three past DUC corpora." ></td>
	<td class="line x" title="75:86	4 Conclusions We proposed a fast query-based multi-document summarizer called FastSum that produces state-ofthe-art summaries using a small set of predictors, two of those are proposed by us: document frequency and topic title frequency." ></td>
	<td class="line x" title="76:86	A feature analysis using least angle regression (LARS) indicated that the document frequency feature is the most useful feature consistently for the last three DUC data sets." ></td>
	<td class="line x" title="77:86	Using document frequency alone can produce competitive results for DUC 2006 and DUC 2007." ></td>
	<td class="line x" title="78:86	The two most useful feature that takes the topic description (i.e., the queries) into account is based on the number of words in the topic description and the topic title." ></td>
	<td class="line x" title="79:86	Using a limited feature set of the 5 best features generates summaries that are comparable to thetopsystemsoftheDUC2006and2007maintask and can be generated in real-time, since no computationally expensive features (e.g., parsing) are used." ></td>
	<td class="line x" title="80:86	From these findings, we draw the following conclusions." ></td>
	<td class="line x" title="81:86	Since a feature set mainly based on word frequencies can produce state-of-the-art summaries, we need to analyze further the current set-up for the query-basedmulti-documentsummarizationtask." ></td>
	<td class="line x" title="82:86	In particular, we need to ask the question whether the selection of relevant documents for the DUC topics is in any way biased." ></td>
	<td class="line x" title="83:86	For DUC, the document clusters for a topic containing relevant documents were always pre-selected by the assessors in preparation for DUC." ></td>
	<td class="line x" title="84:86	Our analysis suggests that simple word frequency computations of these clusters and the documents alone can produce reasonable summaries." ></td>
	<td class="line x" title="85:86	However, the human selecting the relevant documents may have already influenced the way summaries can automatically be generated." ></td>
	<td class="line x" title="86:86	Our system and systems such as SumBasic or SumFocus may just exploit the fact that relevant articles prescreened by humans contain a high density of good content words for summarization.6" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0127
Evaluation Understudy for Dialogue Coherence Models
Gandhe, Sudeep;Traum, David R.;"></td>
	<td class="line x" title="1:205	Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172181, Columbus, June 2008." ></td>
	<td class="line x" title="2:205	c2008 Association for Computational Linguistics An Evaluation Understudy for Dialogue Coherence Models Sudeep Gandhe and David Traum Institute for Creative Technologies University of Southern California 13274 Fiji way, Marina del Rey, CA, 90292 {gandhe,traum}@ict.usc.edu Abstract Evaluating a dialogue system is seen as a major challenge within the dialogue research community." ></td>
	<td class="line x" title="3:205	Due to the very nature of the task, most of the evaluation methods need a substantial amount of human involvement." ></td>
	<td class="line x" title="4:205	Following the tradition in machine translation, summarization and discourse coherence modeling, we introduce the the idea of evaluation understudy for dialogue coherence models." ></td>
	<td class="line x" title="5:205	Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models." ></td>
	<td class="line x" title="6:205	This paper reports findings about the reliability of the information ordering task as applied to dialogues." ></td>
	<td class="line x" title="7:205	We find that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence." ></td>
	<td class="line x" title="8:205	1 Introduction In computer science or any other research field, simply building a system that accomplishes a certain goal is not enough." ></td>
	<td class="line x" title="9:205	It needs to be thoroughly evaluated." ></td>
	<td class="line x" title="10:205	One might want to evaluate the system just to see to what degree the goal is being accomplished or to compare two or more systems with one another." ></td>
	<td class="line x" title="11:205	Evaluation can also lead to understanding the shortcomings of the system and the reasons for these." ></td>
	<td class="line x" title="12:205	Finally the evaluation results can be used as feedback in improving the system." ></td>
	<td class="line x" title="13:205	The best way to evaluate a novel algorithm or a model for a system that is designed to aid humans in processing natural language would be to employ it in a real system and allow users to interact with it." ></td>
	<td class="line x" title="14:205	The data collected by this process can then be used for evaluation." ></td>
	<td class="line x" title="15:205	Sometimes this data needs further analysis which may include annotations, collecting subjective judgments from humans, etc. Since human judgments tend to vary, we may need to employ multiple judges." ></td>
	<td class="line x" title="16:205	These are some of the reasons why evaluation is time consuming, costly and sometimes prohibitively expensive." ></td>
	<td class="line x" title="17:205	Furthermore, if the system being developed contains a machine learning component, the problem of costly evaluation becomes even more serious." ></td>
	<td class="line x" title="18:205	Machine learning components often optimize certain free parameters by using evaluation results on heldout data or by using n-fold cross-validation." ></td>
	<td class="line x" title="19:205	Evaluation results can also help with feature selection." ></td>
	<td class="line x" title="20:205	This need for repeated evaluation can forbid the use of data-driven machine learning components." ></td>
	<td class="line x" title="21:205	For these reasons, using an automatic evaluation measure as an understudy is quickly becoming a common practice in natural language processing tasks." ></td>
	<td class="line x" title="22:205	The general idea is to find an automatic evaluation metric that correlates very well with human judgments." ></td>
	<td class="line x" title="23:205	This allows developers to use the automatic metric as a stand-in for human evaluation." ></td>
	<td class="line p" title="24:205	Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated." ></td>
	<td class="line oc" title="25:205	e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization." ></td>
	<td class="line x" title="26:205	Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006)." ></td>
	<td class="line x" title="27:205	Lapata (2006) has proposed an au172 tomatic evaluation measure for the information ordering task." ></td>
	<td class="line x" title="28:205	We propose to use the same task as a testbed for dialogue coherence modeling." ></td>
	<td class="line x" title="29:205	We evaluate the reliability of the information ordering task as applied to dialogues and propose an evaluation understudy for dialogue coherence models." ></td>
	<td class="line x" title="30:205	In the next section, we look at related work in evaluation of dialogue systems." ></td>
	<td class="line x" title="31:205	Section 3 summarizes the information ordering task and Lapatas (2006) findings." ></td>
	<td class="line x" title="32:205	It is followed by the details of the experiments we carried out and our observations." ></td>
	<td class="line x" title="33:205	We conclude with a summary future work directions." ></td>
	<td class="line x" title="34:205	2 Related Work Most of the work on evaluating dialogue systems focuses on human-machine communication geared towards a specific task." ></td>
	<td class="line x" title="35:205	A variety of evaluation metrics can be reported for such task-oriented dialogue systems." ></td>
	<td class="line x" title="36:205	Dialogue systems can be judged based on the performance of their components like WER for ASR (Jurafsky and Martin, 2000), concept error rate or F-scores for NLU, understandability for speech synthesis etc. Usually the core component, the dialogue model which is responsible for keeping track of the dialogue progression and coming up with an appropriate response, is evaluated indirectly." ></td>
	<td class="line x" title="37:205	Different dialogue models can be compared with each other by keeping the rest of components fixed and then by comparing the dialogue systems as a whole." ></td>
	<td class="line x" title="38:205	Dialogue systems can report subjective measures such as user satisfaction scores and perceived task completion." ></td>
	<td class="line x" title="39:205	SASSI (Hone and Graham, 2000) prescribes a set of questions used for eliciting such subjective assessments." ></td>
	<td class="line x" title="40:205	The objective evaluation metrics can include dialogue efficiency and quality measures." ></td>
	<td class="line x" title="41:205	PARADISE (Walker et al., 2000) was an attempt at reducing the human involvement in evaluation." ></td>
	<td class="line x" title="42:205	It builds a predictive model for user satisfaction as a linear combination of some objective measures and perceived task completion." ></td>
	<td class="line x" title="43:205	Even then the system needs to train on the data gathered from user surveys and objective features retrieved from logs of dialogue runs." ></td>
	<td class="line x" title="44:205	It still needs to run the actual dialogue system and collect objective features and perceived task completeion to predict user satisfaction." ></td>
	<td class="line x" title="45:205	Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997)." ></td>
	<td class="line x" title="46:205	This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006)." ></td>
	<td class="line x" title="47:205	Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007)." ></td>
	<td class="line x" title="48:205	User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007)." ></td>
	<td class="line x" title="49:205	Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system." ></td>
	<td class="line x" title="50:205	(Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed." ></td>
	<td class="line x" title="51:205	Roque et al.(2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates." ></td>
	<td class="line x" title="53:205	Such measures make assumptions about the underlying dialogue model being used (e.g., form-based or information-state based etc.)." ></td>
	<td class="line x" title="54:205	We are more interested in evaluating types of dialogue systems that do not follow these task-based assumptions: systems designed to imitate humanhuman conversations." ></td>
	<td class="line x" title="55:205	Such dialogue systems can range from chatbots like Alice (Wallace, 2003), Eliza (Weizenbaum, 1966) to virtual humans used in simulation training (Traum et al., 2005)." ></td>
	<td class="line x" title="56:205	For such systems, the notion of task completion or efficiency is not well defined and task specific objective measures are hardly suitable." ></td>
	<td class="line x" title="57:205	Most evaluations report the subjective evaluations for appropriateness of responses." ></td>
	<td class="line x" title="58:205	Traum et." ></td>
	<td class="line x" title="59:205	al." ></td>
	<td class="line x" title="60:205	(2004) propose a coding scheme for response appropriateness and scoring functions for those categories." ></td>
	<td class="line x" title="61:205	Gandhe et." ></td>
	<td class="line x" title="62:205	al." ></td>
	<td class="line x" title="63:205	(2006) propose a scale for subjective assessment for appropriateness." ></td>
	<td class="line x" title="64:205	3 Information Ordering The information ordering task consists of choosing a presentation sequence for a set of information bearing elements." ></td>
	<td class="line x" title="65:205	This task is well suited for textto-text generation like in single or multi-document summarization (Barzilay et al., 2002)." ></td>
	<td class="line x" title="66:205	Recently there has been a lot of work in discourse coherence modeling (Lapata, 2003; Barzilay and Lapata, 2005; Soricut and Marcu, 2006) that has used 173 information ordering to test the coherence models." ></td>
	<td class="line x" title="67:205	The information-bearing elements here are sentences rather than high-level concepts." ></td>
	<td class="line x" title="68:205	This frees the models from having to depend on a hard to get training corpus which has been hand-authored for concepts." ></td>
	<td class="line x" title="69:205	Most of the dialogue models still work at the higher abstraction level of dialogue acts and intentions." ></td>
	<td class="line x" title="70:205	But with an increasing number of dialogue systems finding use in non-traditional applications such as simulation training, games, etc.; there is a need for dialogue models which do not depend on hand-authored corpora or rules." ></td>
	<td class="line x" title="71:205	Recently Gandhe and Traum (2007) proposed dialogue models that do not need annotations for dialogue-acts, semantics and hand-authored rules for information state updates or finite state machines." ></td>
	<td class="line x" title="72:205	Such dialogue models focus primarily on generating an appropriate coherent response given the dialogue history." ></td>
	<td class="line x" title="73:205	In certain cases the generation of a response can be reduced to selection from a set of available responses." ></td>
	<td class="line x" title="74:205	For such dialogue models, maintaining the information state can be considered as a secondary goal." ></td>
	<td class="line x" title="75:205	The element that is common to the information ordering task and the task of selecting next most appropriate response is the ability to express a preference for one sequence of dialogue turns over the other." ></td>
	<td class="line x" title="76:205	We propose to use the information ordering task to test dialogue coherence models." ></td>
	<td class="line x" title="77:205	Here the information bearing units will be dialogue turns.1 There are certain advantages offered by using information ordering as a task to evaluate dialogue coherence models." ></td>
	<td class="line x" title="78:205	First the task does not require a dialogue model to take part in conversations in an interactive manner." ></td>
	<td class="line x" title="79:205	This obviates the need for having real users engaging in the dialogue with the system." ></td>
	<td class="line x" title="80:205	Secondly, the task is agnostic about the underlying dialogue model." ></td>
	<td class="line x" title="81:205	It can be a data-driven statistical model or information-state based, form based or even a reinforcement learning system based on MDP or POMDP." ></td>
	<td class="line x" title="82:205	Third, there are simple objective measures available to evaluate the success of information ordering task." ></td>
	<td class="line x" title="83:205	Recently, Purandare and Litman (2008) have used 1These can also be at the utterance level, but for this paper we will use dialogue turns." ></td>
	<td class="line x" title="84:205	this task for modeling dialogue coherence." ></td>
	<td class="line x" title="85:205	But they only allow for a binary classification of sequences as either coherent or incoherent." ></td>
	<td class="line x" title="86:205	For comparing different dialogue coherence models, we need the ability for finer distinction between sequences of information being put together." ></td>
	<td class="line x" title="87:205	Lapata (2003) proposed Kendalls , a rank correlation measure, as one such candidate." ></td>
	<td class="line x" title="88:205	In a recent study they show that Kendalls  correlates well with human judgment (Lapata, 2006)." ></td>
	<td class="line x" title="89:205	They show that human judges can reliably provide coherence ratings for various permutations of text." ></td>
	<td class="line x" title="90:205	(Pearsons correlation for inter-rater agreement is 0.56) and that Kendalls  is a good indicator for human judgment (Pearsons correlation for Kendalls  with human judgment is 0.45 (p < 0.01))." ></td>
	<td class="line x" title="91:205	Before adapting the information ordering task for dialogues, certain questions need to be answered." ></td>
	<td class="line x" title="92:205	We need to validate that humans can reliably perform the task of information ordering and can judge the coherence for different sequences of dialogue turns." ></td>
	<td class="line x" title="93:205	We also need to find which objective measures (like Kendalls ) correlate well with human judgments." ></td>
	<td class="line x" title="94:205	4 Evaluating Information Ordering One of the advantages of using information ordering as a testbed is that there are objective measures available to evaluate the performance of information ordering task." ></td>
	<td class="line x" title="95:205	Kendalls  (Kendall, 1938), a rank correlation coefficient, is one such measure." ></td>
	<td class="line x" title="96:205	Given a reference sequence of length n, Kendalls  for an observed sequence can be defined as,  = # concordant pairs  # discordant pairs# total pairs Each pair of elements in the observed sequence is marked either as concordant appearing in the same order as in reference sequence or as discordant otherwise." ></td>
	<td class="line x" title="97:205	The total number of pairs is Cn2 = n(n1)/2." ></td>
	<td class="line x" title="98:205	 ranges from -1 to 1." ></td>
	<td class="line x" title="99:205	Another possible measure can be defined as the fraction of n-grams from reference sequence, that are preserved in the observed sequence." ></td>
	<td class="line x" title="100:205	bn = # n-grams preserved# total n-grams In this study we have used, b2, fraction of bigrams and b3, fraction of trigrams preserved from the reference sequence." ></td>
	<td class="line x" title="101:205	These values range from 0 to 1." ></td>
	<td class="line x" title="102:205	Table 1 gives examples of observed sequences and 174 Observed Sequence b2 b3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1.00 1.00 1.00 [8, 9, 0, 1, 2, 3, 4, 5, 6, 7] 0.89 0.75 0.29 [4, 1, 0, 3, 2, 5, 8, 7, 6, 9] 0.00 0.00 0.60 [6, 9, 8, 5, 4, 7, 0, 3, 2, 1] 0.00 0.00 -0.64 [2, 3, 0, 1, 4, 5, 8, 9, 6, 7] 0.56 0.00 0.64 Table 1: Examples of observed sequences and their respective b2, b3 &  values." ></td>
	<td class="line x" title="103:205	Here the reference sequence is [0,1,2,3,4,5,6,7,8,9]." ></td>
	<td class="line x" title="104:205	respective b2, b3 and  values." ></td>
	<td class="line x" title="105:205	Notice how  allows for long-distance relationships whereas b2, b3 are sensitive to local features only." ></td>
	<td class="line x" title="106:205	2 5 Experimental Setup For our experiments we used segments drawn from 9 dialogues." ></td>
	<td class="line x" title="107:205	These dialogues were two-party humanhuman dialogues." ></td>
	<td class="line x" title="108:205	To ensure applicability of our results over different types of dialogue, we chose these 9 dialogues from different sources." ></td>
	<td class="line x" title="109:205	Three of these were excerpts from role-play dialogues involving negotiations which were originally collected for a simulation training scenario (Traum et al., 2005)." ></td>
	<td class="line x" title="110:205	Three are from SRIs Amex Travel Agent data which are task-oriented dialogues about air travel planning (Bratt et al., 1995)." ></td>
	<td class="line x" title="111:205	The rest of the dialogues are scripts from popular television shows." ></td>
	<td class="line x" title="112:205	Fig 6 shows an example from the air-travel domain." ></td>
	<td class="line x" title="113:205	Each excerpt drawn was 10 turns long with turns strictly alternating between the two speakers." ></td>
	<td class="line x" title="114:205	Following the experimental design of (Lapata, 2006) we created random permutations for these dialogue segments." ></td>
	<td class="line x" title="115:205	We constrained our permutations so that the permutations always start with the same speaker as the original dialogue and turns strictly alternate between the speakers." ></td>
	<td class="line x" title="116:205	With these constraints there are still 5!5!" ></td>
	<td class="line x" title="117:205	= 14400 possible permutations per dialogue." ></td>
	<td class="line x" title="118:205	We selected 3 random permutations for each of the 9 dialogues." ></td>
	<td class="line x" title="119:205	In all, we have a total of 27 dialogue permutations." ></td>
	<td class="line x" title="120:205	They are arranged in 3 sets, each set containing a permutation for all 9 dialogues." ></td>
	<td class="line x" title="121:205	We ensured that not all permutations in a given set are particularly very good or very bad." ></td>
	<td class="line x" title="122:205	We used Kendalls  to balance the permutations across 2For more on the relationship between b2, b3 and  see row 3,4 of table 1 and figure 4." ></td>
	<td class="line x" title="123:205	the given set as well as across the given dialogue." ></td>
	<td class="line x" title="124:205	Unlike Lapata (2006) who chose to remove the pronouns and discourse connectives, we decided not do any pre-processing on the text like removing disfluencies or removing cohesive devices such as anaphora, ellipsis, discourse connectives, etc. One of the reason is such pre-processing if done manually defeats the purpose of removing humans from the evaluation procedure." ></td>
	<td class="line x" title="125:205	Moreover it is very difficult to remove certain cohesive devices such as discourse deixis without affecting the coherence level of the original dialogues." ></td>
	<td class="line x" title="126:205	6 Experiment 1 In our first experiment, we divided a total of 9 human judges among the 3 sets (3 judges per set)." ></td>
	<td class="line x" title="127:205	Each judge was presented with 9 dialogue permutations." ></td>
	<td class="line x" title="128:205	They were asked to assign a single coherence rating for each dialogue permutation." ></td>
	<td class="line x" title="129:205	The ratings were on a scale of 1 to 7, with 1 being very incoherent and 7 being perfectly coherent." ></td>
	<td class="line x" title="130:205	We did not provide any additional instructions or examples of scale as we wanted to capture the intuitive idea of coherence from our judges." ></td>
	<td class="line x" title="131:205	Within each set the dialogue permutations were presented in random order." ></td>
	<td class="line x" title="132:205	We compute the inter-rater agreement by using Pearsons correlation analysis." ></td>
	<td class="line x" title="133:205	We correlate the ratings given by each judge with the average ratings given by the judges who were assigned the same set." ></td>
	<td class="line x" title="134:205	For inter-rater agreement we report the average of 9 such correlations which is 0.73 (std dev = 0.07)." ></td>
	<td class="line x" title="135:205	Artstein and Poesio (2008) have argued that Krippendorffs  (Krippendorff, 2004) can be used for interrater agreement with interval scales like the one we have." ></td>
	<td class="line x" title="136:205	In our case for the three sets  values were 0.49, 0.58, 0.64." ></td>
	<td class="line x" title="137:205	These moderate values of alpha indicate that the task of judging coherence is indeed a difficult task, especially when detailed instructions or examples of scales are not given." ></td>
	<td class="line x" title="138:205	In order to assess whether Kendalls  can be used as an automatic measure of dialogue coherence, we perform a correlation analysis of  values against the average ratings by human judges." ></td>
	<td class="line x" title="139:205	The Pearsons correlation coefficient is 0.35 and it is statistically not significant (P=0.07)." ></td>
	<td class="line x" title="140:205	Fig 1(a) shows the relationship between coherence judgments and  values." ></td>
	<td class="line x" title="141:205	This experiment fails to support the suitability 175 (a) Kendalls  does not correlate well with human judgments for dialogue coherence." ></td>
	<td class="line x" title="142:205	(b) Fraction of bigram & trigram counts correlate well with human judgments for dialogue coherence." ></td>
	<td class="line x" title="143:205	Figure 1: Experiment 1 single coherence rating per permutation of Kendalls  as an evaluation understudy." ></td>
	<td class="line x" title="144:205	We also analyzed the correlation of human judgments against simple n-gram statistics, specifically (b2 +b3)/2." ></td>
	<td class="line x" title="145:205	Fig 1(b) shows the relationship between human judgments and the average of fraction of bigrams and fraction of trigrams that were preserved in the permutation." ></td>
	<td class="line x" title="146:205	The Pearsons correlation coefficient is 0.62 and it is statistically significant (P<0.01)." ></td>
	<td class="line x" title="147:205	7 Experiment 2 Since human judges found it relatively hard to assign a single rating to a dialogue permutation, we decided to repeat experiment 1 with some modifications." ></td>
	<td class="line x" title="148:205	In our second experiment we asked the judges to provide coherence ratings at every turn, based on the dialogue that preceded that turn." ></td>
	<td class="line x" title="149:205	The dialogue permutations were presented to the judges through a web interface in an incremental fashion turn by turn as they rated each turn for coherence (see Fig 5 in the appendix for the screenshot of this interface)." ></td>
	<td class="line x" title="150:205	We used a scale from 1 to 5 with 1 being completely incoherent and 5 as perfectly coherent." ></td>
	<td class="line x" title="151:205	3 A total of 11 judges participated in this experiment with the first set being judged by 5 judges and the remaining two sets by 3 judges each." ></td>
	<td class="line x" title="152:205	3We believe this is a less complex task than experiment 1 and hence a narrower scale is used." ></td>
	<td class="line x" title="153:205	For the rest of the analysis, we use the average coherence rating from all turns as a coherence rating for the dialogue permutation." ></td>
	<td class="line x" title="154:205	We performed the inter-rater agreement analysis as in experiment 1." ></td>
	<td class="line x" title="155:205	The average of 11 correlations is 0.83 (std dev = 0.09)." ></td>
	<td class="line x" title="156:205	Although the correlation has improved, Krippendorffs  values for the three sets are 0.49, 0.35, 0.63." ></td>
	<td class="line x" title="157:205	This shows that coherence rating is still a hard task even when judged turn by turn." ></td>
	<td class="line x" title="158:205	We assessed the relationship between the average coherence rating for dialogue permutations with Kendalls  (see Fig 2(a))." ></td>
	<td class="line x" title="159:205	The Pearsons correlation coefficient is 0.33 and is statistically not significant (P=0.09)." ></td>
	<td class="line x" title="160:205	Fig 2(b) shows high correlation of average coherence ratings with the fraction of bigrams and trigrams that were preserved in permutation." ></td>
	<td class="line x" title="161:205	The Pearsons correlation coefficient is 0.75 and is statistically significant (P<0.01)." ></td>
	<td class="line x" title="162:205	Results of both experiments suggest that, (b2 +b3)/2 correlates very well with human judgments and can be used for evaluating information ordering when applied to dialogues." ></td>
	<td class="line x" title="163:205	8 Experiment 3 We wanted to know whether information ordering as applied to dialogues is a valid task or not." ></td>
	<td class="line x" title="164:205	In this experiment we seek to establish a higher baseline for 176 (a) Kendalls  does not correlate well with human judgments for dialogue coherence." ></td>
	<td class="line x" title="165:205	(b) Fraction of bigram & trigram counts correlate well with human judgments for dialogue coherence." ></td>
	<td class="line x" title="166:205	Figure 2: Experiment 2 turn-by-turn coherence rating the task of information ordering in dialogues." ></td>
	<td class="line x" title="167:205	We presented the dialogue permutations to our human judges and asked them to reorder the turns so that the resulting order is as coherent as possible." ></td>
	<td class="line x" title="168:205	All 11 judges who participated in experiment 2 also participated in this experiment." ></td>
	<td class="line x" title="169:205	They were presented with a drag and drop interface over the web that allowed them to reorder the dialogue permutations." ></td>
	<td class="line x" title="170:205	The reordering was constrained to keep the first speaker of the reordering same as that of the original dialogue and the re-orderings must have strictly alternating turns." ></td>
	<td class="line x" title="171:205	We computed the Kendalls  and fraction of bigrams and trigrams (b2 +b3)/2 for these re-orderings." ></td>
	<td class="line x" title="172:205	There were a total of 11  9 = 99 reordered dialogue permutations." ></td>
	<td class="line x" title="173:205	Fig 3(a) and 3(b) shows the frequency distribution of  and (b2 +b3)/2 values respectively." ></td>
	<td class="line x" title="174:205	Humans achieve high values for the reordering task." ></td>
	<td class="line x" title="175:205	For Kendalls , the mean of the reordered dialogues is 0.82 (std dev = 0.25) and for (b2 +b3)/2, the mean is 0.71 (std dev = 0.28)." ></td>
	<td class="line x" title="176:205	These values establish an upper baseline for the information ordering task." ></td>
	<td class="line x" title="177:205	These can be compared against the random baseline." ></td>
	<td class="line x" title="178:205	For  random performance is 0.02 4 and 4Theoretically this should be zero." ></td>
	<td class="line x" title="179:205	The slight positive bias is the result of the constraints imposed on the re-orderings like only allowing the permutations that have the correct starting speaker." ></td>
	<td class="line x" title="180:205	for (b2 +b3)/2 it is 0.11." ></td>
	<td class="line x" title="181:205	5 9 Discussion Results show that (b2 +b3)/2 correlates well with human judgments for dialogue coherence better than Kendalls ." ></td>
	<td class="line x" title="182:205	 encodes long distance relationships in orderings where as (b2 +b3)/2 only looks at local context." ></td>
	<td class="line x" title="183:205	Fig 4 shows the relationship between these two measures." ></td>
	<td class="line x" title="184:205	Notice that most of the orderings have  values around zero (i.e. in the middle range for ), whereas majority of orderings will have a low value for (b2 +b3)/2." ></td>
	<td class="line x" title="185:205	 seems to overestimate the coherence even in the absence of immediate local coherence (See third entry in table 1)." ></td>
	<td class="line x" title="186:205	It seems that local context is more important for dialogues than for discourse, which may follow from the fact that dialogues are produced by two speakers who must react to each other, while discourse can be planned by one speaker from the beginning." ></td>
	<td class="line x" title="187:205	Traum and Allen (1994) point out that such social obligations to respond and address the contributions of the other should be an important factor in building dialogue systems." ></td>
	<td class="line x" title="188:205	The information ordering paradigm does not take into account the content of the information-bearing items, e.g. the fact that turns like yes, I agree, 5This value is calculated by considering all 14400 permutations as equally likely." ></td>
	<td class="line x" title="189:205	177 (a) Histogram of Kendalls  for reordered sequences (b) Histogram of fraction of bigrams & trigrams values for reordered sequences Figure 3: Experiment 3 upper baseline for information ordering task (human performance) okay perform the same function and should be treated as replaceable." ></td>
	<td class="line x" title="190:205	This may suggest a need to modify some of the objective measures to evaluate the information ordering specially for dialogue systems that involve more of such utterances." ></td>
	<td class="line x" title="191:205	Human judges can find the optimal sequences with relatively high frequency, at least for short dialogues." ></td>
	<td class="line x" title="192:205	It remains to be seen how this varies with longer dialogue lengths which may contain sub-dialogues that can be arranged independently of each other." ></td>
	<td class="line x" title="193:205	10 Conclusion & Future Work Evaluating dialogue systems has always been a major challenge in dialogue systems research." ></td>
	<td class="line x" title="194:205	The core component of dialogue systems, the dialogue model, has usually been only indirectly evaluated." ></td>
	<td class="line x" title="195:205	Such evaluations involve too much human effort and are a bottleneck for the use of data-driven machine learning models for dialogue coherence." ></td>
	<td class="line x" title="196:205	The information ordering task, widely used in discourse coherence modeling, can be adopted as a testbed for evaluating dialogue coherence models as well." ></td>
	<td class="line x" title="197:205	Here we have shown that simple n-gram statistics that are sensitive to local features correlate well with human judgments for coherence and can be used as an evaluation understudy for dialogue coherence models." ></td>
	<td class="line x" title="198:205	As with any evaluation understudy, one must be careful while using it as the correlation with human judgments is not perfect and may be inaccurate in some cases  it can not completely replace the need for full evaluation with human judges in all cases (see (Callison-Burch et al., 2006) for a critique of BLUE along these lines)." ></td>
	<td class="line x" title="199:205	In the future, we would like to perform more experiments with larger data sets and different types of dialogues." ></td>
	<td class="line x" title="200:205	It will also be interesting to see the role cohesive devices play in coherence ratings." ></td>
	<td class="line x" title="201:205	We would like to see if there are any other measures or certain modifications to the current ones that correlate better with human judgments." ></td>
	<td class="line x" title="202:205	We also plan to employ this evaluation metric as feedback in building dialogue coherence models as is done in machine translation (Och, 2003)." ></td>
	<td class="line x" title="203:205	Acknowledgments The effort described here has been sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM)." ></td>
	<td class="line x" title="204:205	Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred." ></td>
	<td class="line x" title="205:205	We would like to thank Radu Soricut, Ron Artstein, and the anonymous SIGdial reviewers for helpful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:197	Extractive vs. NLG-based Abstractive Summarization of Evaluative Text: The Effect of Corpus Controversiality Giuseppe Carenini and Jackie Chi Kit Cheung1 Department of Computer Science University of British Columbia Vancouver, B.C.  V6T 1Z4, Canada {carenini,cckitpw}@cs.ubc.ca Abstract Extractive summarization is the strategy of concatenating extracts taken from a corpus into a summary, while abstractive summarization involves paraphrasing the corpus using novel sentences." ></td>
	<td class="line x" title="2:197	We define a novel measure of corpus controversiality of opinions contained in evaluative text, and report the results of a user study comparing extractive and NLG-based abstractive summarization at different levels of controversiality." ></td>
	<td class="line x" title="3:197	While the abstractive summarizer performs better overall, the results suggest that the margin by which abstraction outperforms extraction is greater when controversiality is high, providing a context in which the need for generationbased methods is especially great." ></td>
	<td class="line x" title="4:197	1 Introduction There are two main approaches to the task of summarizationextraction and abstraction (Hahn and Mani, 2000)." ></td>
	<td class="line x" title="5:197	Extraction involves concatenating extracts taken from the corpus into a summary, whereas abstraction involves generating novel sentences from information extracted from the corpus." ></td>
	<td class="line x" title="6:197	It has been observed that in the context of multidocument summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al., 1999)." ></td>
	<td class="line x" title="7:197	However, there has been little work identifying specific factors which might affect the performance of each strategy in summarizing  evaluative  documents containing opinions and preferences, such as customer reviews or blogs." ></td>
	<td class="line x" title="8:197	This work aims to address this gap by exploring one dimension along which the effectiveness of the two paradigms could vary; namely, the controversiality of the opinions contained in the corpus." ></td>
	<td class="line x" title="9:197	In this paper, we make the following contributions." ></td>
	<td class="line x" title="10:197	Firstly, we define a measure of controversiality of opinions in the corpus based on information entropy." ></td>
	<td class="line x" title="11:197	Secondly, we run a user study to test the hypothesis that a controversial corpus has greater need of abstractive methods and consequently of NLG techniques." ></td>
	<td class="line x" title="12:197	Intuitively, extracting sentences from multiple users whose opinions are diverse and wide-ranging may not reflect the overall opinion, whereas it may be adequate contentwise if opinions are roughly the same across users." ></td>
	<td class="line x" title="13:197	As a secondary contribution, we propose a method for structuring text when summarizing controversial corpora." ></td>
	<td class="line x" title="14:197	This method is used in our study for generating abstractive summaries." ></td>
	<td class="line x" title="15:197	The results of the user study support our hypothesis that a NLG summarizer outperforms an extractive summarizer by more when the controversiality is high." ></td>
	<td class="line x" title="16:197	2 Related Work There has been little work comparing extractive and abstractive multi-document summarization." ></td>
	<td class="line x" title="17:197	A previous study on summarizing evaluative text (Carenini et." ></td>
	<td class="line x" title="18:197	al, 2006) showed that extraction and abstraction performed about equally well, though for different reasons." ></td>
	<td class="line x" title="19:197	The study, however, did not 1Authors are listed in alphabetical order." ></td>
	<td class="line x" title="20:197	33 look at the effect of the controversiality of the corpus on the relative performance of the two strategies." ></td>
	<td class="line x" title="21:197	To the best of our knowledge, the task of measuring the controversiality of opinions in a corpus has not been studied before." ></td>
	<td class="line x" title="22:197	Some well known measures are related to this task, including variance, information entropy, and measures of interrater reliability." ></td>
	<td class="line x" title="23:197	(e.g. Fleiss' Kappa (Fleiss, 1971), Krippendorff's Alpha (Krippendorff, 1980))." ></td>
	<td class="line x" title="24:197	However, these existing measures do not satisfy certain properties that a sound measure of controversiality should possess, prompting us to develop our own based on information entropy." ></td>
	<td class="line x" title="25:197	Summary evaluation is a challenging open research area." ></td>
	<td class="line x" title="26:197	Existing methods include soliciting human judgements, task-based approaches, and automatic approaches." ></td>
	<td class="line x" title="27:197	Task-based evaluation measures the effectiveness of a summarizer for its intended purpose." ></td>
	<td class="line x" title="28:197	(e.g.(McKeown et al., 2005)) This approach, however, is less applicable in this work because we are interested in evaluating specific properties of the summary such as the grammaticality and the content, which may be difficult to evaluate with an overall task-based approach." ></td>
	<td class="line x" title="30:197	Furthermore, the design of the task may intrinsically favour abstractive or extractive summarization." ></td>
	<td class="line x" title="31:197	As an extreme example, asking for a list of specific comments from users would clearly favour extractive summarization." ></td>
	<td class="line x" title="32:197	Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau, 2004), which takes into account the fact that human summaries with different content can be equally informative." ></td>
	<td class="line x" title="33:197	Multiple human summaries are taken to be models, and chunks of meaning known as Summary Content Units (SCU) are manually identified." ></td>
	<td class="line x" title="34:197	Peer summaries are evaluated based on how many SCUs they share with the model summaries, and the number of model summaries in which these SCUs are found." ></td>
	<td class="line x" title="35:197	Although this method has been tested in DUC 2006 and DUC 2005 (Passonneau et al., 2006), (Passonneau et al., 2005) in the domain of news articles, it has not been tested for evaluative text." ></td>
	<td class="line x" title="36:197	A pilot study that we conducted on a set of customer reviews on a product using the Pyramid method revealed several problems specific to the evaluative domain." ></td>
	<td class="line x" title="37:197	For example, summaries which misrepresented the polarity of the evaluations for a certain feature were not penalized, and human summaries sometimes produced contradictory statements about the distribution of the opinions." ></td>
	<td class="line x" title="38:197	In one case, one model summary claimed that a feature is positively rated, while another claimed the opposite, whereas the machine summary indicated that this feature drew mixed reviews." ></td>
	<td class="line x" title="39:197	Clearly, only one of these positions should be regarded as correct." ></td>
	<td class="line x" title="40:197	Further work is needed to resolve these problems." ></td>
	<td class="line oc" title="41:197	There are also automatic methods for summary evaluation, such as ROUGE (Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary  and  the  machine  summary." ></td>
	<td class="line x" title="42:197	While ROUGE scores have been shown to often correlate quite well with human judgements (Nenkova et al., 2007), they do not provide insights into the specific strengths and weaknesses of the summary." ></td>
	<td class="line x" title="43:197	The method of summarization evaluation used in this work is to ask users to complete a questionnaire about summaries that they are presented with." ></td>
	<td class="line x" title="44:197	The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et al., 2006)." ></td>
	<td class="line x" title="45:197	3 Representative Systems In our user study, we compare an abstractive and an extractive multi-document summarizer that are both developed specifically for the evaluative domain." ></td>
	<td class="line x" title="46:197	These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al., 2000)." ></td>
	<td class="line x" title="47:197	Both summarizers rely on information extraction from the corpus." ></td>
	<td class="line x" title="48:197	First, sentences with opinions need to be identified, along with the features of the entity that are evaluated, the strength, and polarity (positive or negative) of the evaluation." ></td>
	<td class="line x" title="49:197	For instance, in a corpus of customer reviews, the sentence Excellent picture quality on par with my Pioneer, Panasonic, and JVC players. contains an opinion on the feature picture quality of a DVD player, and is a very positive evaluation (+3 on a scale from -3 to +3)." ></td>
	<td class="line x" title="50:197	We rely on methods from previous work for these tasks (Hu and Liu, 2004)." ></td>
	<td class="line x" title="51:197	Once these features, called Crude Features (CFs), are extracted, they are mapped onto a taxonomy of User Defined Features (UDFs), so named because they can be defined by the user." ></td>
	<td class="line x" title="52:197	This mapping provides a better conceptual organization of the CFs 34 by grouping together semantically similar CFs, such as jpeg picture and jpeg slide show under the UDF JPEG." ></td>
	<td class="line x" title="53:197	For the purposes of our study, feature extraction, polarity/strength identification and the mapping from CFs to UDFs are not done automatically as in (Hu and Liu, 2004) and (Carenini et al, 2005)." ></td>
	<td class="line x" title="54:197	Instead, gold standard annotations by humans are used in order to focus on the effect of the summarization strategy." ></td>
	<td class="line x" title="55:197	3.1 Abstractive Summarizer: SEA The abstractive summarizer is the Summarizer of Evaluative Arguments (SEA), adapted from GEA, a system for generating evaluative text tailored to the user's preferences (Carenini and Moore, 2006)." ></td>
	<td class="line x" title="56:197	In SEA, units of content are organized by UDFs." ></td>
	<td class="line x" title="57:197	The importance of each UDF is based on the number and strength of evaluations of CFs mapped to this UDF, as well as the importance of its children UDFs." ></td>
	<td class="line x" title="58:197	Content selection consists of repeating the following two steps until the desired number of UDFs have been selected: (i) greedily selecting the most important UDF (ii) recalculating the measure of importance scores for the remaining UDFs." ></td>
	<td class="line x" title="59:197	The content structuring, microplanning, and realization stages of SEA are adapted from GEA." ></td>
	<td class="line x" title="60:197	Each selected UDF is realized in the final summary by one clause, generated from a template pattern based  on  the  number  and  distribution  of polarity/strength evaluations of the UDF." ></td>
	<td class="line x" title="61:197	For example, the UDF video output with an average polarity/strength of near -3 might be realized as several customers found the video output to be terrible. While experimenting with the SEA summarizer, we noticed that the document structuring of SEA summaries, which is adapted from GEA and is based on guidelines from argumentation theory (Carenini and Moore, 2000), sometimes sounded unnatural." ></td>
	<td class="line x" title="62:197	We found that controversially rated UDF features (roughly balanced positive and negative evaluations) were treated as contrasts to those which were uncontroversially rated (either mostly positive, or mostly negative evaluations)." ></td>
	<td class="line x" title="63:197	In SEA, contrast relations between features are realized by cue phrases signalling contrast such as however and although." ></td>
	<td class="line x" title="64:197	These cue phrases appear to signal a contrast that is too strong for the relation between controversial and uncontroversial features." ></td>
	<td class="line x" title="65:197	An example of a SEA summary suffering from this problem can be found in Figure 1." ></td>
	<td class="line x" title="66:197	To solve this problem, we devised an alternative content structure for controversial corpora, in which all controversial features appear first, followed by all positively and negatively evaluated features." ></td>
	<td class="line x" title="67:197	3.2 Extractive Summarizer: MEAD* The  extractive  approach  is  represented  by MEAD*, which is adapted from the open source summarization framework MEAD (Radev et al., 2000)." ></td>
	<td class="line x" title="68:197	After information extraction, MEAD* orders CFs by the number of sentences evaluating that CF, and selects a sentence from each CF until the word limit has been reached." ></td>
	<td class="line x" title="69:197	The sentence that is selected for each CF is the one with the highest sum of polarity/strength evaluations for any feature, so sentences that mention more CFs tend to be selected." ></td>
	<td class="line x" title="70:197	The selected sentences are then ordered according to the UDF hierarchy by a depthfirst traversal through the UDF tree so that more abstract features tend to precede more specific ones." ></td>
	<td class="line x" title="71:197	MEAD* does not have a special mechanism to deal with controversial features." ></td>
	<td class="line x" title="72:197	It is not clear how overall controversiality of a feature can be effectively expressed with extraction, as each sentence conveys a specific and unique opinion." ></td>
	<td class="line x" title="73:197	One could include two sentences of opposite polarity for each controversial feature." ></td>
	<td class="line x" title="74:197	However, in several cases that we considered, this produced extremely incoherent text that did not seem to convey the gist of the overall controversiality of the feature." ></td>
	<td class="line x" title="75:197	Customers had mixed opinions about the Apex AD2600." ></td>
	<td class="line x" title="76:197	Although several customers found the video output to be poor and some customers disliked the user interface, customers had mixed opinions about the range of compatible disc formats." ></td>
	<td class="line x" title="77:197	However, users did agree on some things." ></td>
	<td class="line x" title="78:197	Some users found the extra features to be very good even though customers had mixed opinions about the supplied universal remote control." ></td>
	<td class="line x" title="79:197	Figure 1: SEA summary of a controversial corpus with a document structuring problem." ></td>
	<td class="line x" title="80:197	Controversial and uncontroversial features are interwoven." ></td>
	<td class="line x" title="81:197	See Figure 3 for an example of a summary structured with our alternative strategy." ></td>
	<td class="line x" title="82:197	35 3.3 Links to the Corpus In common with the previous study on which this is based, both the SEA and MEAD* summaries contain clickable footnotes which are links back into an original user review, with a relevant sentence highlighted." ></td>
	<td class="line x" title="83:197	These footnotes serve to provide details for the abstractive SEA summarizer, and context for the sentences chosen by the extractive MEAD* summarizer." ></td>
	<td class="line x" title="84:197	They also aid the participants of the user study in checking the contents of the summary." ></td>
	<td class="line x" title="85:197	The sample sentences for SEA are selected by a method similar to the MEAD* sentence selection algorithm." ></td>
	<td class="line x" title="86:197	One of the questions in the questionnaire provided to users targets the effectiveness of the footnotes as an aid to the summary." ></td>
	<td class="line x" title="87:197	4 Measuring Controversiality The opinion sentences in the corpus are annotated with the CF that they evaluate as well as the strength, from 1 to 3, and polarity, positive or negative, of the evaluation." ></td>
	<td class="line x" title="88:197	It is natural then, to base a measure of controversiality on these annotations." ></td>
	<td class="line x" title="89:197	To measure the controversiality of a corpus, we first measure the controversiality of each of the features in the corpus." ></td>
	<td class="line x" title="90:197	We list two properties that a measure of feature controversiality should satisfy." ></td>
	<td class="line x" title="91:197	Strength-sensitivity: The measure should be sensitive to the strength of the evaluations." ></td>
	<td class="line x" title="92:197	e.g. Polarity/strength (P/S) evaluations of -2 and +2 should be less controversial than -3 and +3 Polarity-sensitivity: The measure should be sensitive the polarity of the evaluations." ></td>
	<td class="line x" title="93:197	e.g. P/S evaluations of -1 and +1 should be more controversial than +1 and +3." ></td>
	<td class="line x" title="94:197	The rationale for this property is that positive and negative evaluations are fundamentally different, and this distinction is more important than the difference in intensity." ></td>
	<td class="line x" title="95:197	Thus, though a numerical scale would suggest that -1 and +1 are as distant as +1 and +3, a suitable controversiality measure should not treat them so." ></td>
	<td class="line x" title="96:197	In addition, the overall measure of corpus controversiality should also satisfy the following two features." ></td>
	<td class="line x" title="97:197	CF-weighting: CFs should be weighted by the number of evaluations they contain when calculating the overall value of controversiality for the corpus." ></td>
	<td class="line x" title="98:197	CF-independence: The controversiality of individual CFs should not affect each other." ></td>
	<td class="line x" title="99:197	An alternative is to calculate controversiality by UDFs instead of CFs." ></td>
	<td class="line x" title="100:197	However, not all CFs mapped to the same UDF represent the same concept." ></td>
	<td class="line x" title="101:197	For example, the CFs picture clarity and color signal are both mapped to the UDF video output." ></td>
	<td class="line x" title="102:197	4.1 Existing Measures of Variability Since the problem of measuring the variability of a distribution has been well studied, we first examined existing metrics including variance, entropy, kappa, weighted kappa, Krippendorffs alpha, and information entropy." ></td>
	<td class="line x" title="103:197	Each of these, however, is problematic in their canonical form, leading us to devise a new metric based on information entropy which satisfies the above properties." ></td>
	<td class="line x" title="104:197	Existing metrics will now be examined in turn." ></td>
	<td class="line x" title="105:197	Variance: Variance does not satisfy polaritysensitivity, as the statistic only takes into account the difference of each data point to the mean, and the sign of the data point plays no role." ></td>
	<td class="line x" title="106:197	Information Entropy: The canonical form of information entropy does not satisfy strength or polarity sensitivity, because the measure considers the discrete values of the distribution to be an unordered set." ></td>
	<td class="line x" title="107:197	Measures of Inter-rater Reliability: Many measures exist to assess inter-rater agreement or disagreement, which is the task of measuring how similarly two or more judges rate one or more subjects beyond chance (dis)agreement." ></td>
	<td class="line x" title="108:197	Various versions of Kappa and Krippendorff's Alpha (Krippendorff, 1980), which have shown to be equivalent in their most generalized forms (Passonneau, 1997), can be modified to satisfy all the properties listed above." ></td>
	<td class="line x" title="109:197	However, there are important differences between the tasks of measuring controversiality and measuring inter-rater reliability." ></td>
	<td class="line x" title="110:197	Kappa and Krippendorff's Alpha correct for chance agreement between raters, which is appropriate in the context of inter-rater reliability calculations, because judges are asked to give their opinions on items that are given to them." ></td>
	<td class="line x" title="111:197	In contrast, expressions of opinion are volunteered by users, and users self-select the features they comment on." ></td>
	<td class="line x" title="112:197	Thus, it is reasonable to assume that they never randomly select an evaluation for a feature, and chance agreement does not exist." ></td>
	<td class="line x" title="113:197	36 4.2 Entropy-based Controversiality We define here our novel measure of controversiality, which is based on information entropy because it can be more easily adapted to measure controversiality." ></td>
	<td class="line x" title="114:197	As has been stated, entropy in its original form over the evaluations of a CF is not sensitive to strength or polarity." ></td>
	<td class="line x" title="115:197	To correct this, we first aggregate the positive and negative evaluations for each CF separately, and then calculate the entropy based on the resultant Bernoulli distribution." ></td>
	<td class="line x" title="116:197	Let ps(cfj) be the set of polarity/strength evaluations for  cfj." ></td>
	<td class="line x" title="117:197	Let the importance of a feature, imp(cfj), be the sum of the absolute values of the polarity/strength evaluations for cfj." ></td>
	<td class="line x" title="118:197	impcf j=  ps k pscf j psk Define: imp_ poscf j=  psk pscf j psk0 psk imp_negcf j=  psk pscf jpsk0 psk Now, calculate the entropy of the Bernoulli distribution corresponding to the importance of the two polarities to satisfy polarity-sensitivity." ></td>
	<td class="line x" title="119:197	That is, Bernoulli with parameter j=imp_ poscf j/impcf j H j=j log2j1jlog21j Next, we scale this score by the importance of the evaluations divided by the maximum possible importance for this number of evaluations to satisfy strength-sensitivity." ></td>
	<td class="line x" title="120:197	Since our scale is from -3 to +3, the maximum possible importance for a feature is three times the number of evaluations." ></td>
	<td class="line x" title="121:197	max_impcf j=3pscf j Then the controversiality of a feature is: controcf j=impcf jH jmax_impcf j The case corresponding to the highest possible feature controversiality, then, would be the bimodal case with equal numbers of evaluations on the extreme positive and negative bins (Figure 2)." ></td>
	<td class="line x" title="122:197	Note, however, that controversiality is not simply bimodality." ></td>
	<td class="line x" title="123:197	A unimodal normal-like distribution of evaluations centred on zero, for example, should intuitively be somewhat controversial, because there are equal numbers of positive and negative evaluations." ></td>
	<td class="line x" title="124:197	Our entropy-based feature controversiality measure is able to take this into account." ></td>
	<td class="line x" title="125:197	To calculate the controversiality of the corpus, a weighted average is taken over the CF controversiality scores, with the weight being equal to one less than the number of evaluations for that CF." ></td>
	<td class="line x" title="126:197	We subtract one to eliminate any CF where only one evaluation is made, as that CF has an entropy score of one by default before scaling by importance." ></td>
	<td class="line x" title="127:197	This procedure satisfies properties CFweighting and CF-independence." ></td>
	<td class="line x" title="128:197	wcf j=pscf j1 controcorpus=wcf jcontrocf jwcf j Although the annotations in this corpus range from -3 to +3, it would be easy to rescale opinion annotations of different corpora to apply this metric." ></td>
	<td class="line x" title="129:197	Note that empirically, this measure correlates highly with Kappa and Krippendorff's Alpha." ></td>
	<td class="line x" title="130:197	5 User Study Our main hypothesis that extractive summarization is outperformed even more in the case of controversial corpora was tested by a user study, which compared the results of MEAD* and the modified SEA." ></td>
	<td class="line x" title="131:197	First, ten subsets of 30 user reviews were selected from the corpus of 101 reviews of the Apex AD2600  DVD  player  from  amazon.com  by stochastic local search." ></td>
	<td class="line x" title="132:197	Five of these subsets are controversial, with controversiality scores between 0.83 and 0.88, and five of these are uncontroversial, with controversiality scores of 0." ></td>
	<td class="line x" title="133:197	A set of thirFigure 2: Sample feature controversiality scores for three different distributions of polarity/strength evaluations." ></td>
	<td class="line x" title="134:197	37 ty user reviews per subcorpus was needed to create a summary of sufficient length, which in our case was about 80 words in length." ></td>
	<td class="line x" title="135:197	Twenty university students were recruited and presented with two summaries of the same subcorpus, one generated from SEA and one from MEAD*." ></td>
	<td class="line x" title="136:197	We generated ten subcorpora in total, so each subcorpus was assigned to two participants." ></td>
	<td class="line x" title="137:197	One of these participants was shown the SEA summary first, and the other was shown the MEAD* summary first, to eliminate the order of presentation as a source of variation." ></td>
	<td class="line x" title="138:197	The participants were asked to take on the role of an employee of Apex, and told that they would have to write a summary for the quality assurance department of the company about the product in question." ></td>
	<td class="line x" title="139:197	The purpose of this was to prime them to look for information that should be included in a summary of this corpus." ></td>
	<td class="line x" title="140:197	They were given thirty minutes to read the reviews, and take notes." ></td>
	<td class="line x" title="141:197	They were then presented with a questionnaire on the summaries, consisting of ten Likert rating questions." ></td>
	<td class="line x" title="142:197	Five of these questions targeted the linguistic quality of the summary, based on linguistic well-formedness questions used at DUC 2005, one targeted the clickable footnotes linking to sample sentences in the summary (see section 3.3), and three evaluated the contents of the summary." ></td>
	<td class="line x" title="143:197	The three questions targeted Recall, Precision, and the general Accuracy of the summary contents respectively." ></td>
	<td class="line x" title="144:197	The tenth question asked for a general overall quality judgement of the summary." ></td>
	<td class="line x" title="145:197	After familiarizing themselves with the questionnaire, the participants were presented with the two summaries in sequence, and asked to fill out the questionnaire while reading the summary." ></td>
	<td class="line x" title="146:197	They were allowed to return to the original set of reviews during this time." ></td>
	<td class="line x" title="147:197	Lastly, they were given an additional questionnaire which asked them to compare the two summaries that they were shown." ></td>
	<td class="line x" title="148:197	Questions  in  the  questionnaire  not  found  in (Carenini et al., 2006) are attached in Appendix A. 6 Results 6.1 Quantitative Results We convert the Likert responses from a scale from Strongly Disagree to Strong Agree to a scale from 1 to 5, with 1 corresponding to Strongly Disagree, and 5 to Strongly Agree." ></td>
	<td class="line x" title="149:197	We group the ten questions into four categories: linguistic (questions 1 to 5), content (questions 6 to 8), footnote (question 9), and overall (question 10)." ></td>
	<td class="line x" title="150:197	See Table 1 for a breakdown of the responses for each question at each controversiality level." ></td>
	<td class="line x" title="151:197	For our analysis, we adopt a two-step approach that has been applied in Computational Linguistics (Di Eugenio et al., 2002) as well as in HCI (Hinckley et al., 1997)." ></td>
	<td class="line x" title="152:197	First, we perform a two-way Analysis of Variance (ANOVA) test using the average response of the questions in each category." ></td>
	<td class="line x" title="153:197	The two factors are controversiality of the corpus (high or low) as independent samples, and the summarizer (SEA or MEAD*) as repeated measures." ></td>
	<td class="line x" title="154:197	We repeat this procedure for the average of the ten questions, termed Macro below." ></td>
	<td class="line x" title="155:197	The p-values of these tests are summarized in Table 2." ></td>
	<td class="line x" title="156:197	The results of the ANOVA tests indicate that SEA significantly outperforms MEAD* in terms of linguistic and overall quality, as well as for all the questions combined." ></td>
	<td class="line x" title="157:197	It does not significantly outperform MEAD* by content, or in the amount that the included sample sentences linked to by the footnotes aid the summary." ></td>
	<td class="line x" title="158:197	No significant differences are found in the performance of the summaSEA Customers had mixed opinions about the Apex AD2600 1,2 possibly because users were divided on the range of compatible disc formats 3,4 and there was disagreement among the users about the video output 5,6." ></td>
	<td class="line x" title="159:197	However, users did agree on some things." ></td>
	<td class="line x" title="160:197	Some purchasers found the extra features 7 to be very good and some customers really liked the surround sound support 8 and thought the user interface 9 was poor." ></td>
	<td class="line x" title="161:197	MEAD* When we tried to hook up the first one , it was broken the motor would not eject discs or close the door . 1 The build quality feels solid , it does n't shake or whine while playing discs , and the picture and sound is top notch ( both dts and dd5.1 sound good ) . 2 The progressive scan option can be turned off easily by a button on the remote control which is one of the simplest and easiest remote controls i have ever seen or used . 3 It plays original dvds and cds and plays mp3s and jpegs . 4 Figure 3: Sample SEA and MEAD* summaries for a controversial corpus." ></td>
	<td class="line x" title="162:197	The numbers within the summaries are footnotes linking the summary to an original user review from the corpus." ></td>
	<td class="line x" title="163:197	38 rizers over the two levels of controversiality for any of the question sets . While the average differences in scores between the SEA and MEAD* summarizers are greater in the controversial case for the linguistic, content, and macro averages as well as the question on the overall quality, the p-values for interaction between the two factors in the two-way ANOVA test are not significant." ></td>
	<td class="line x" title="164:197	For the second step of the analysis, we use a one-tailed sign test (Siegel and Castellan, 1988) over the difference in performance of the summarizers at the two levels of controversiality for the questions in the questionnaire." ></td>
	<td class="line x" title="165:197	We encode + in the case where the difference  between  SEA  and MEAD* is greater for a question in the controversial setting,  if the difference is smaller, and we discard a question if the difference is the same (e.g. the Footnote question)." ></td>
	<td class="line x" title="166:197	Since the Overall question is likely correlated with the responses of the other questions, we did not include it in the test." ></td>
	<td class="line x" title="167:197	After discarding the Footnote question, the p-value over the remaining eight questions is 0.0352, which lends support to our hypothesis that the abstraction is better by more when the corpus is controversial." ></td>
	<td class="line x" title="168:197	We also analyze the users' summary preferences at the two levels of controversiality." ></td>
	<td class="line x" title="169:197	A strong preference for SEA is encoded as a 5, while a strong preference for MEAD* is encoded as a 1, with 3 being neutral." ></td>
	<td class="line x" title="170:197	Using a two-tailed unpaired two-sample t-test, we do not find a significant difference in the participants' summary preferences (p=0.6237)." ></td>
	<td class="line x" title="171:197	However, participants sometimes preferred summaries for reasons other than linguistic or content quality, or may base their judgement only on one aspect of the summary." ></td>
	<td class="line x" title="172:197	For instance, one participant rated SEA at least as well as MEAD* in all questions except Footnote, yet preferred MEAD* to SEA overall precisely because MEAD* was felt to have made better use of the footnotes than SEA." ></td>
	<td class="line x" title="173:197	6.2 Qualitative Results The qualitative comments that participants were asked to provide along with the Likert scores confirmed the observations that led us to formulate the initial hypothesis." ></td>
	<td class="line x" title="174:197	In the controversial subcorpora, participants generally agreed that the abstractive nature of SEA's generated text was an advantage." ></td>
	<td class="line x" title="175:197	For example, one participant lauded SEA for attempting to synthesize the reviews and said that it did reflect the mixed nature of the reviews, and covered some common complaints. The participant, however, said that SEA was somewhat misleading in that it understated the extent to which reviews were negative." ></td>
	<td class="line x" title="176:197	In particular, agreement was reported on Question Set Controversiality Summarizer Controversiality x Summarizer Linguistic 0.7226 <0.0001 0.2639 Content 0.9215 0.1906 0.2277 Footnote 0.2457 0.7805 1 Overall 0.6301 0.0115 0.2000 Macro 0.7127 0.0003 0.1655 Table 2: Two-way ANOVA p-values." ></td>
	<td class="line x" title="177:197	Table 1: Breakdown of average Likert question responses for each summary at the two levels of controversiality as well as the difference between SEA and MEAD*." ></td>
	<td class="line x" title="178:197	Controversial Uncontroversial SEA MEAD* (SEA  MEAD*)SEA MEAD* (SEA  MEAD*) Question Mean St. Dev." ></td>
	<td class="line x" title="179:197	Mean St. Dev." ></td>
	<td class="line x" title="180:197	Mean St. Dev." ></td>
	<td class="line x" title="181:197	Mean St. Dev." ></td>
	<td class="line x" title="182:197	Mean St. Dev." ></td>
	<td class="line x" title="183:197	Mean St. Dev." ></td>
	<td class="line x" title="184:197	Grammaticality 4.5 0.53 3.4 1.26 1.1 0.99 4.2 0.92 2.78 1.3 1.56 1.51 Non-redundancy 4.2 0.92 4 1.07 0.25 1.58 3.7 0.95 3.8 1.14 -0.1 1.45 Referential clarity 4.5 0.53 3.44 1.33 1 1.22 4.2 1.03 3.5 1.18 0.7 1.34 Focus 4.11 1.27 2.1 0.88 2.22 0.83 3.9 1.1 2.6 1.35 1.3 1.57 Structure and Coherence 4.1 0.99 1.9 0.99 2.2 1.14 3.8 1.4 2.3 1.06 1.5 1.9 Linguistic 4.29 0.87 2.91 1.35 1.39 1.34 3.96 1.07 3 1.29 0.98 1.63 Recall 2.8 1.32 1.8 1.23 1 1.33 2.5 1.27 2.5 1.43 0 1.89 Precision 3.9 1.1 2.7 1.64 1.2 1.23 3.5 1.27 3.3 0.95 0.2 1.93 Accuracy 3.4 0.97 3.3 1.57 0.1 1.2 3.1 1.52 3.2 1.03 -0.1 2.28 Content 3.37 1.19 2.6 1.57 0.77 1.3 3.03 1.38 3 1.17 0.03 1.97 Footnote 4 1.05 3.9 0.88 0.1 1.66 3.6 1.07 3.5 1.35 0.1 1.6 Overall 3.8 0.79 2.4 1.17 1.4 1.07 3.2 1.23 2.7 0.82 0.5 1.84 Macro  Footnote 3.92 1.06 2.75 1.41 1.17 1.32 3.57 1.26 2.97 1.2 0.61 1.81 Macro 3.93 1.05 2.87 1.4 1.06 1.39 3.57 1.24 3.02 1.22 0.56 1.79 39 some features where none existed, and problems with reliability were not mentioned. Participants disagreed on the information coverage of the MEAD* summary." ></td>
	<td class="line x" title="185:197	One participant said that MEAD* includes almost all the information about the Apex 2600 DVD player, while another said that it does not reflect all information from the customer reviews. In the uncontroversial subcorpora, more users criticized SEA for its inaccuracy in content selection." ></td>
	<td class="line x" title="186:197	One participant felt that SEA made generalizations that were not precise or accurate. Participants had specific comments about the features that SEA mentioned that they did not consider important." ></td>
	<td class="line x" title="187:197	For example, one comment was that Compatibility with CDs was not a general problem, nor were issues with the remote control, or video output (when it worked). MEAD* was criticized for being overly specific, but users praised MEAD* for being not at all redundant, and said that it included information I felt was important. 7 Conclusion and Future Work We have explored the controversiality of opinions in a corpus of evaluative text as an aspect which may determine how well abstractive and extractive summarization strategies perform." ></td>
	<td class="line x" title="188:197	We have presented a novel measure of controversiality, and reported on the results of a user study which suggest that abstraction by NLG outperforms extraction by a larger amount in more controversial corpora." ></td>
	<td class="line x" title="189:197	We have also presented a document structuring strategy for summarization of controversial corpora." ></td>
	<td class="line x" title="190:197	Our work has implications in practical decisions on summarization strategy choice; an extractive approach, which may be easier to implement because of its lack of requirement for natural language generation, may suffice if the controversiality of opinions in a corpus is sufficiently low." ></td>
	<td class="line x" title="191:197	A future approach to summarization of evaluative text might combine extraction and abstraction in order to combine the different strengths that each bring to the summary." ></td>
	<td class="line x" title="192:197	The controversiality of the corpus might be one factor determining the mix of abstraction and extraction in the summary." ></td>
	<td class="line x" title="193:197	The footnotes linking to sample sentences in the corpus in SEA are already one form of this combined approach." ></td>
	<td class="line x" title="194:197	Further work is needed to integrate this text into the summary itself, possibly in a modified form." ></td>
	<td class="line x" title="195:197	As a final note to our user study, further studies should be done with different corpora and summarization systems to increase the external validity of our results." ></td>
	<td class="line x" title="196:197	Acknowledgements We would like to thank Raymond T. Ng, Gabriel Murray and Lucas Rizoli for their helpful comments." ></td>
	<td class="line x" title="197:197	This work was partially funded by the Natural Sciences and Engineering Research Council of Canada." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:155	Automated Metrics That Agree With Human Judgements On Generated Output for an Embodied Conversational Agent Mary Ellen Foster Informatik VI: Robotics and Embedded Systems Technische Universitat Munchen Boltzmannstrae 3, D-85748 Garching bei Munchen, Germany foster@in.tum.de Abstract When evaluating a generation system, if a corpus of target outputs is available, a common and simple strategy is to compare the system output against the corpus contents." ></td>
	<td class="line x" title="2:155	However, cross-validation metrics that test whether the system makes exactly the same choices as the corpus on each item have recently been shown not to correlate well with human judgements of quality." ></td>
	<td class="line x" title="3:155	An alternative evaluation strategy is to compute intrinsic, task-specific properties of the generated output; this requires more domain-specific metrics, but can often produce a better assessment of the output." ></td>
	<td class="line x" title="4:155	In this paper, a range of metrics using both of these techniques are used to evaluate three methods for selecting the facial displays of an embodied conversational agent, and the predictions of the metrics are compared with human judgements of the same generated output." ></td>
	<td class="line x" title="5:155	The corpus-reproduction metrics show no relationship with the human judgements, while the intrinsic metrics that capture the number and variety of facial displays show a significant correlation with the preferences of the human users." ></td>
	<td class="line x" title="6:155	1 Introduction Evaluating the output of a generation system is known to be difficult: since generation is an openended task, the criteria for success can be difficult to define (cf.Mellish and Dale, 1998)." ></td>
	<td class="line x" title="8:155	In the current state of the art, there are two main strategies for evaluating the output of a generation system: the behaviour or preferences of humans in response to the output may be measured, or automated measures may be computed on the output itself." ></td>
	<td class="line x" title="9:155	A study involving human judges is the most complete and convincing evaluation of generated output." ></td>
	<td class="line x" title="10:155	However, such a study is not always practical, as recruiting sufficient subjects can be time-consuming and expensive." ></td>
	<td class="line x" title="11:155	So automated metrics are also used in addition toor instead ofhuman studies." ></td>
	<td class="line o" title="12:155	When automatically evaluating generated output, the goal is to find metrics that can easily be computed and that can also be shown to correlate with human judgements of quality." ></td>
	<td class="line oc" title="13:155	Such metrics have been introduced in other fields, including PARADISE (Walker et al., 1997) for spoken dialogue systems, BLEU (Papineni et al., 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation." ></td>
	<td class="line o" title="14:155	Many automated generation evaluations measure the similarity between the generated output and a corpus of gold-standard target outputs, often using measures such as precision and recall." ></td>
	<td class="line n" title="15:155	Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems." ></td>
	<td class="line x" title="16:155	One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al.(2007), [e]valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited. Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety." ></td>
	<td class="line x" title="18:155	1Although Callison-Burch et al.(2006) have recently called into question the utility of BLEU." ></td>
	<td class="line x" title="20:155	95 Automated metrics that take into account other properties than strict corpus similarity have also been used to evaluate the output of generation systems." ></td>
	<td class="line x" title="21:155	Walker (2005) describes several evaluations that used corpus data in a different way: each of the corpus examples was associated with some reward function (e.g., subjective user evaluation or task success), and machine-learning techniques such as reinforcement learning or boosting were then used to train the output planner." ></td>
	<td class="line x" title="22:155	Foster and White (2007) found that automated metrics based on factors other than corpus similarity (e.g., the amount of variation in the output) agreed better with user preferences than did the corpus-similarity scores." ></td>
	<td class="line x" title="23:155	Belz and Gatt (2008) compare the predictions of a range of measures, both intrinsic and extrinsic, that were used to evaluate the systems in a shared-task referringexpression generation challenge." ></td>
	<td class="line x" title="24:155	One main finding from this comparison was that there was no significant correlation between the intrinsic and extrinsic (task success) measures for this task." ></td>
	<td class="line x" title="25:155	All of the above studies considered only systems that generate text, but many of the same factors also apply to the generation of non-verbal behaviours for an embodied conversational agent (ECA) (Cassell et al., 2000)." ></td>
	<td class="line x" title="26:155	The behaviour of such an agent is normally based on recorded human behaviour, which can provide targets similar to those used in corpusbased evaluations of text-generation systems." ></td>
	<td class="line x" title="27:155	However, just as in text generation, a multimodal system that scores well on corpus similarity tends to produce highly repetitive non-verbal behaviours, so it is equally important to gather human judgements to accompany any automated evaluation." ></td>
	<td class="line x" title="28:155	This paper presents three corpus-driven methods of selecting facial displays for an embodied conversational agent and describes two studies comparing the output of the different methods." ></td>
	<td class="line x" title="29:155	All methods are based on annotated data drawn from a corpus of human facial displays, and each uses the corpus data in a different way." ></td>
	<td class="line x" title="30:155	The first evaluation study uses human judges to compare the output of the selection methods against one another, while the second study uses a range of automated metrics: several corpusreproduction measures, along with metrics based on intrinsic properties of the outputs." ></td>
	<td class="line x" title="31:155	The results of the two studies are compared using multiple regression, and the implications are discussed." ></td>
	<td class="line x" title="32:155	2 Corpus-based generation of facial displays for an ECA The experiments in this paper make use of the output components of the COMIC multimodal dialogue system (Foster et al., 2005), which adds a multimodal talking-head interface to a CAD-style system for redesigning bathrooms." ></td>
	<td class="line x" title="33:155	The studies focus on the task of selecting appropriate ECA head and eyebrow motions to accompany the turns in which the system describes and compares the options for tiling the room, as those are the parts of the output with the most interesting and varied content." ></td>
	<td class="line x" title="34:155	The implementations were based on a corpus of conversational facial displays derived from the behaviour of a single speaker reading approximately 450 scripted sentences generated by the COMIC output-generation system." ></td>
	<td class="line x" title="35:155	The OpenCCG syntactic derivation trees (White, 2006) for the sentences form the basis of the corpus." ></td>
	<td class="line x" title="36:155	The leaf nodes in these trees correspond to the individual words, while the internal nodes correspond to multi-word constituents." ></td>
	<td class="line x" title="37:155	Every node in each tree was initially labelled with all of the applicable contextual features produced by the output planner: the userpreference evaluation of the tile design being described (positive/negative/neutral), the information status (given/new) of each piece of information, and the predicted speech-synthesiser prosody." ></td>
	<td class="line x" title="38:155	The annotators then linked each facial display produced by the speaker to the node or span of nodes in the derivation tree covering the words temporally associated with the display." ></td>
	<td class="line x" title="39:155	Full details of this corpus are given in Foster (2007a)." ></td>
	<td class="line x" title="40:155	The most common display used by the speaker was a downward nod, while the user-preference evaluation had the single largest differential effect on the displays used." ></td>
	<td class="line x" title="41:155	When the speaker described features of the design that the user was expected to like, he was relatively more likely to turn to the right and to raise his eyebrows (Figure 1(a)); on features that the user was expected to dislike, on the other hand, there was a higher probability of left leaning, lowered eyebrows, and narrowed eyes (Figure 1(b))." ></td>
	<td class="line x" title="42:155	In a previous study, users were generally able to recognise these positive and negative displays when they were resynthesised on an embodied conversational agent (Foster, 2007b)." ></td>
	<td class="line x" title="43:155	96 (a) Positive (b) Negative Figure 1: Characteristic facial displays from the corpus Based on this corpus, three different strategies were implemented for selecting facial displays to accompany the synthesised speech: one strategy using only the three characteristic displays described above, along with two data-driven strategies drawing on the full corpus data." ></td>
	<td class="line x" title="44:155	All of the strategies use the same basic process to select the displays to accompany a sentence." ></td>
	<td class="line x" title="45:155	Beginning with the contextuallyannotated syntactic tree for the sentence, the system proceeds depth-first, selecting a face-display combination to accompany each node in turn." ></td>
	<td class="line x" title="46:155	The main difference among the strategies is the way that each selects the displays for a node as it is encountered." ></td>
	<td class="line x" title="47:155	The rule-based strategy includes displays only on derivation-tree nodes corresponding to specific tiledesign properties: that is, manufacturer and series names, colours, and decorative motifs." ></td>
	<td class="line x" title="48:155	The displays for such a node are entirely determined by the user-preference evaluation of the property being described, and are based on the corpus patterns described above: for every node associated with a positive evaluation, this strategy selects a right turn and brow raise; for a negative node, it selects a left turn, brow lower, and eye squint; while for all other design-property nodes, it chooses a downward nod." ></td>
	<td class="line x" title="49:155	While the rule-based strategy selects displays only on nodes describing tile-design features, the two data-driven strategies consider all nodes in the syntactic tree for a sentence as possible sites for a facial display." ></td>
	<td class="line x" title="50:155	To choose the displays for a given node, the system considers the set of displays that occurred on all nodes in the corpus with the same syntactic, semantic, and pragmatic context, and then chooses a display from this set in one of two ways." ></td>
	<td class="line x" title="51:155	The majority strategy selects the most common option in all cases, while the weighted strategy makes a stochastic choice among all of the options based on the relative frequency." ></td>
	<td class="line x" title="52:155	As a concrete example, consider a hypothetical context in which the speaker made no motion 80% of the time, a downward nod 15% of the time, and a downward nod with a brow raise the other 5% of the time." ></td>
	<td class="line x" title="53:155	For nodes with this context, the majority strategy would always choose no motion, while the weighted strategy would choose no motion with probability 0.8, a downward nod with probability 0.15, and a nod with a brow raise with probability 0.05." ></td>
	<td class="line x" title="54:155	Table 1 shows a sample sentence from the corpus, the original facial displays used by the speaker, and the displays selected by each of the strategies." ></td>
	<td class="line x" title="55:155	In the figure, nd=d indicates a downward nod, bw=u and bw=d a brow raise and lower, respectively, sq an eye squint, ln=l a left lean, and tn=r a right turn." ></td>
	<td class="line x" title="56:155	Most of the displays in these schedules are associated with leaf nodes in the derivation tree, and therefore with single words in the output." ></td>
	<td class="line x" title="57:155	However, both the left lean in the original schedule and the right turn in the weighted schedule are associated with internal nodes in the tree, and therefore cover more than one word in the surface string." ></td>
	<td class="line x" title="58:155	3 User-preference studies As a first comparison of the evaluation strategies, human judges were asked to compare videos based on the output of each of the generation strategies to one another and to resynthesised versions of the original displays from the corpus." ></td>
	<td class="line x" title="59:155	This section gives the details of a study in which the judges chose 97 Although its in the family style, the tiles are by Alessi." ></td>
	<td class="line x" title="60:155	Original nd=d nd=d nd=d nd=d nd=d,bw=u . . ." ></td>
	<td class="line x" title="61:155	ln=l . . ." ></td>
	<td class="line x" title="62:155	Rule-based ln=l,bw=d tn=r,bw=u Majority nd=d nd=d Weighted nd=d nd=d . . tn=r . . Table 1: Face-display schedules for a sample sentence Figure 2: RUTH talking head among the original displays and the output of the weighted and rule-based strategies." ></td>
	<td class="line x" title="63:155	At the end of the section, the results of this study are discussed together with the results of a similar previous study comparing the two data-driven strategies to each other; the full details of the earlier study are given in Foster and Oberlander (2007)." ></td>
	<td class="line x" title="64:155	3.1 Subjects Subjects were recruited for this experiment through the Language Experiments Portal,2 a website dedicated to online psycholinguistic experiments." ></td>
	<td class="line x" title="65:155	There were 36 subjects (20 female), 50% of whom identified themselves as native English speakers; most of the subjects were between 20 and 29 years old." ></td>
	<td class="line x" title="66:155	3.2 Materials The materials for this experiment were based on 18 randomly-selected sentences from the corpus." ></td>
	<td class="line x" title="67:155	For each sentence, face-display schedules were generated using both the rule-based and the weighted strategies." ></td>
	<td class="line x" title="68:155	The Festival speech synthesiser (Clark 2http://www.language-experiments.org/ et al., 2004) and the RUTH animated talking head (DeCarlo et al., 2004) (Figure 2) were used to create video clips of the two generated schedules for each sentence, along with a video clip showing the original facial displays annotated in the corpus." ></td>
	<td class="line x" title="69:155	3.3 Method Each subject saw a series of pairs of videos." ></td>
	<td class="line x" title="70:155	Both videos in a pair had identical spoken content, but the face-display schedules differed: each trial included two of rule-based, weighted, and original." ></td>
	<td class="line x" title="71:155	For each pair of videos, the subject was asked to select which of the two versions they preferred." ></td>
	<td class="line x" title="72:155	Subjects made each pairwise comparison between schedule types six timesthree times in each orderfor a total of 18 judgements." ></td>
	<td class="line x" title="73:155	All subjects saw the same set of sentences, in an individually randomly-selected order: the pairwise choices between schedule types were also allocated to items at random." ></td>
	<td class="line x" title="74:155	3.4 Results and analysis The overall pairwise preferences of the subjects in this study are shown in Figure 3(a)." ></td>
	<td class="line x" title="75:155	A c2 goodnessof-fit test can be used to evaluate the significance of the choices made on each individual comparison." ></td>
	<td class="line x" title="76:155	For the comparison between original and rule-based schedules, the preference is significant: c2(1;N = 216)= 4:17, p< 0:05." ></td>
	<td class="line x" title="77:155	The results are similar for the original vs. weighted comparison: c2(1;N = 215) = 4:47, p < 0:05." ></td>
	<td class="line x" title="78:155	However, the preferences for the weighted vs. rule-based comparison are not significant: c2(1;N = 217) = 2:44, p 0:12." ></td>
	<td class="line x" title="79:155	Figure 3(b) shows the results from a similar previous study (Foster and Oberlander, 2007) in which the subjects compared the two data-driven strategies to the original displays, using a design identical to that used in the current study with 54 subjects and 24 sentences." ></td>
	<td class="line x" title="80:155	The responses given by the subjects in this study also showed a signifi98 123 93 Original vs. Rule-based Weighted vs. Rule-based Original vs. Weighted 0255075100125 120 97 123 92 (a) Original, weighted, rule-based 295 153 Original vs. Majority Weighted vs. Majority Original vs. Weighted 050100150200250300 278 170 251 197 (b) Original, weighted, majority (Foster and Oberlander, 2007) Figure 3: Pairwise preferences from the user evaluations cant preference for the original schedules over the weighted ones (c2(1;N = 448) = 6:51, p < 0:05)." ></td>
	<td class="line x" title="81:155	Both the weighted and the original schedules were very strongly preferred over the majority schedules (c2(1;N = 448) = 45 and 26, respectively; p 0:0001)." ></td>
	<td class="line x" title="82:155	The original vs. weighted comparison was included in both studies (the rightmost pair of bars on the two graphs in Figure 3), and the response patterns across the two studies for this comparison did not differ significantly from each other: c2(1;N = 664) = 0:02, p 0:89." ></td>
	<td class="line x" title="83:155	3.5 Discussion Taken together, the results of these two studies suggest a rough preference ordering among the different strategies for generating facial displays." ></td>
	<td class="line x" title="84:155	In both studies, the judges significantly preferred the original displays from the corpus over any of the automatically-generated alternatives." ></td>
	<td class="line x" title="85:155	This suggests that, for this generation task, the data in the corpus can indeed be treated as a gold standard unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data." ></td>
	<td class="line x" title="86:155	The schedules generated by the majority strategy, on the other hand, were very obviously disliked by the judges in the Foster and Oberlander (2007) study." ></td>
	<td class="line x" title="87:155	The ranking between the rule-based and weighted schedules from the current study is less clear, although there was a tendency to prefer the latter." ></td>
	<td class="line x" title="88:155	4 Automated evaluation Since the subjects in the user-preference studies generally selected the corpus schedules over any of the alternatives, any automated metric for this task should favour output that resembles the examples in the corpus." ></td>
	<td class="line x" title="89:155	The most obvious form of corpus similarity is exact reproduction of the displays in the corpus, which suggests using metrics such as precision and recall that favour generation strategies whose output on every item is as close as possible to what was annotated in the corpus for that sentence." ></td>
	<td class="line x" title="90:155	In Section 4.1, several such corpus-reproduction metrics are described and their results presented." ></td>
	<td class="line x" title="91:155	For this type of open-ended generation task, though, it can be overly restrictive to allow only the displays that were annotated in the corpus for a sentence and to penalise any deviation." ></td>
	<td class="line x" title="92:155	Indeed, as mentioned in the introduction, a number of previous studies have found that the output of generation systems that score well on this type of metric is often disliked in practice by users." ></td>
	<td class="line x" title="93:155	Section 4.2 therefore presents several intrinsic metrics that aim to capture corpus similarity of a different type: rather than requiring the system to exactly reproduce the corpus on each sentence, these metrics instead favour strategies resulting in global behaviour that exhibits similar patterns to those found in the corpus, without necessarily agreeing exactly with the corpus on any specific sentence." ></td>
	<td class="line x" title="94:155	99 0.52 0.31 0.18 0.82 0.34 0.29 0.24 0.12 0.75 0.23 0.22 0.10 0.06 0.77 0.14 Precision Recall F Score Node Acc." ></td>
	<td class="line x" title="95:155	Beta 00.20.40.60.81 Majority Weighted Rule-based (a) Corpus-reproduction metrics 5.38 3.26 4.58 2.98 3.15 1.31 2.07 1.24 Tokens Types TTR 0123456 Original Weighted Majority Rule-based 0.640.69 0.480.68 00.20.40.60.81 (b) Intrinsic metrics Figure 4: Results of the automated evaluations 4.1 Corpus-reproduction metrics This first set of metrics compared the generated schedules against the original schedules annotated in the corpus, using 10-fold cross-validation." ></td>
	<td class="line x" title="96:155	The first three metrics that were tested are standard for this sort of corpus-comparison task: recall, precision, and F score." ></td>
	<td class="line x" title="97:155	Recall was computed as the proportion of the corpus displays for a sentence that were reproduced exactly in the generated output, while precision was the proportion of generated displays that had exact matches in the corpus; the F score for a sentence is then the harmonic mean of these two values, as usual." ></td>
	<td class="line x" title="98:155	The leftmost three columns in Table 2 show the precision, recall, and F score for the sample schedules in Table 1." ></td>
	<td class="line x" title="99:155	In addition to the above commonly-used metrics, two other corpus-reproduction metrics were also computed." ></td>
	<td class="line x" title="100:155	The first, node accuracy, represents the proportion of nodes in the derivation tree for a sentence where the proposed displays were correct, including those nodes where the system correctly selected no motiona baseline system that never proposes any motion scores 0.79 on this measure." ></td>
	<td class="line x" title="101:155	The fourth column of Table 2 shows the node-accuracy score for the sample sentences." ></td>
	<td class="line x" title="102:155	The final corpusreproduction metric compared the proposed displays to the annotated corpus displays using the b agreement measure (Artstein and Poesio, 2005)." ></td>
	<td class="line x" title="103:155	b is a weighted measure that permits different levels of P R F NAcc Tok Typ TTR Original     6 3 0.5 Rule-based 0 0 0 0.65 2 2 1 Majority 0.50 0.14 0.11 0.70 2 1 0.5 Weighted 0.67 0.29 0.20 0.74 3 2 0.67 Table 2: Automated evaluation of the sample schedules agreement when annotations overlap, and that can therefore capture a more fine-grained form of agreement than other measures such as k. Figure 4(a) shows the results for all of these corpus-reproduction measures, averaged across the sentences in the corpus; the results for the weighted and majority strategies are from Foster and Oberlander (2007)." ></td>
	<td class="line x" title="104:155	The majority strategy scored uniformly higher than the weighted strategy on all of these measuresparticularly on precisionwhile the weighted strategy in turn scored higher than the rule-based strategy on all measures except for node accuracy." ></td>
	<td class="line x" title="105:155	Using a Wilcoxon rank sum test with a Bonferroni correction for multiple comparisons, all of the differences among the strategies on precision, recall, F score, and node accuracy are significant at p < 0:001." ></td>
	<td class="line x" title="106:155	Significance cannot be assessed for the differences in b scores, as noted by Artstein and Poesio (2005), but the results are similar." ></td>
	<td class="line x" title="107:155	Also, the node accuracy score for the majority strategy is significantly better than the no-motion baseline of 0.79, while those for the weighted and rule-based strategies are worse (also all p < 0:001)." ></td>
	<td class="line x" title="108:155	100 As expectedand as noted by Foster and Oberlander (2007)all of the corpus-reproduction metrics strongly favoured the weighted strategy over the weighted strategy and generally penalised the rulebased strategy." ></td>
	<td class="line x" title="109:155	Since the majority strategy always chooses the most probable option, it is not surprising that it agrees more often with the corpus than do the other strategies, which deliberately select less frequent options; this led to its relatively high scores on the corpus-reproduction metrics." ></td>
	<td class="line x" title="110:155	It is also not surprising that the weighted strategy beat the rulebased strategy on most of these metrics, as the former selects from the most frequent options, while the latter uses the most marked options, which are not generally the most frequent." ></td>
	<td class="line x" title="111:155	4.2 Intrinsic metrics The metrics in the preceding section compared the displays selected for a sentence against the displays found in the corpus for that sentence." ></td>
	<td class="line x" title="112:155	This section describes other measures that are computed directly on the generated schedules, without any reference to the corpus data." ></td>
	<td class="line x" title="113:155	For each sentence, the following values were counted: the total number of face-display combinations (i.e., the number of display tokens), and the number of different combinations (types)." ></td>
	<td class="line x" title="114:155	In addition to being used as metrics themselves, these two counts were also used to compute a third value: the type/token ratio (TTR) (i.e., # types # tokens), which captures the diversity of the dis-plays selected for each sentence." ></td>
	<td class="line x" title="115:155	These intrinsic metrics were computed on each sentence produced in the cross-validation study from the preceding section and then averaged to produce the final results." ></td>
	<td class="line x" title="116:155	Since these metrics do not require the original corpus data for comparison, they were also computed on the original corpus schedules." ></td>
	<td class="line x" title="117:155	The rightmost columns in Table 2 show the intrinsic results for the sample schedules in Table 1." ></td>
	<td class="line x" title="118:155	The overall results for these metrics across the entire corpus are shown in Figure 4(b)." ></td>
	<td class="line x" title="119:155	The original corpus had both the most displays types and the most tokens; the values for weighted choice were a fairly close second, those for majority choice third, while the rule-based strategy scored lowest on both of these metrics." ></td>
	<td class="line x" title="120:155	Except for the difference between majority and rule-based on the facial-display types which is not significantall of the differences between schedule types on these two measures are significant at p < 0:001 on a Wilcoxon rank sum test with Bonferroni correction." ></td>
	<td class="line x" title="121:155	When it comes to the type/token ratio, the value for the majority-choice schedule is significantly lower than that for the other three schedule types (all p < 0:0001), while the value for weighted choice is somewhat higher than that for the original schedules (p < 0:01); no other differences are significant." ></td>
	<td class="line x" title="122:155	Since the original corpus schedules scored the highest on the user study, these metrics should be considered in the context of of how close the results are to those of the corpus." ></td>
	<td class="line x" title="123:155	Figure 4(b) shows that the weighted strategy is most similar to the corpus in both the number and the diversity of displays it selects, while the other two strategies have much lower diversity." ></td>
	<td class="line x" title="124:155	However, even though the rule-based strategy selects fewer displays all of the other strategies, its TTR is more similar to that of the corpus and the weighted strategy, while the majority strategy has a much lower TTR." ></td>
	<td class="line x" title="125:155	In fact, in the schedules generated by the majority-choice strategy, nearly 90% of the displays that were selected were downward nods." ></td>
	<td class="line x" title="126:155	5 Comparing the automated metrics with human preferences Qualitatively, the results of the corpus-reproduction metrics differ greatly from the preferences of the human judges." ></td>
	<td class="line x" title="127:155	The users generally liked the majority schedules the least, while all of these metrics scored this strategy the highest." ></td>
	<td class="line x" title="128:155	Among the intrinsic metrics, the type and token counts placed the weighted schedules closest to the corpus, while the majority and rule-based strategies were further away; this agrees with the human results for the two data-driven strategies, but not for the rule-based strategy." ></td>
	<td class="line x" title="129:155	On the other hand, the TTR indicated that the output of the rule-based and weighted schedules was similar to the schedules found in the corpus, while the majority-choice strategy produced sentences with TTRs more different from the corpus, generally agreeing with the human results." ></td>
	<td class="line x" title="130:155	To permit a more quantitative comparison between the predictions of the automated metrics and the judges preferences, the pairwise preferences from the user study were converted into a numeric value called the selection ratio." ></td>
	<td class="line x" title="131:155	The selection ratio 101 for an item (i.e., a sentence with a particular set of facial displays) was computed as the number of trials on which that item was selected, divided by the total number of trials on which that item was an option." ></td>
	<td class="line x" title="132:155	For example, an item that was always preferred over any of the alternatives on all trials would score 1.0 on this measure, while an item that was selected a quarter of the time would score 0.25." ></td>
	<td class="line x" title="133:155	The selection ratios of the items used in the human-preference studies ranged from 0.13 to 0.85." ></td>
	<td class="line x" title="134:155	As a concrete example, when the sentence in Table 1 was used in the Foster and Oberlander (2007) study, the selection ratios were 0.43 for the original version, 0.33 for the majority version, and 0.24 for the weighted version." ></td>
	<td class="line x" title="135:155	The relationship between the selection ratio and the full set of automated metrics from the preceding section was assessed through multiple linear regression." ></td>
	<td class="line x" title="136:155	An initial model including all of the automated metrics as predictor variables had an adjusted R2 value of 0.413." ></td>
	<td class="line x" title="137:155	Performing stepwise selection on this initial model resulted in a final model with two significant predictor variablesdisplay tokens and TTRand an adjusted R2 of 0.422." ></td>
	<td class="line x" title="138:155	The regression coefficients for both of these predictor variables are positive, with high significance (p < 0:001)." ></td>
	<td class="line x" title="139:155	While the R2 values indicate that neither the initial nor the final model fully explains the selection ratios from the user study, the details of the models themselves are relevant to the overall goal of finding automated metrics that agree with human preferences." ></td>
	<td class="line x" title="140:155	The results of the stepwise selection have backed up the qualitative intuition that none of the corpusreproduction metrics had any relationship to the users preferences, while the number and diversity of displays per sentence appear to have contributed much more strongly to the choices made by the human judges." ></td>
	<td class="line x" title="141:155	This adds to the growing body of evidence that intrinsic measures are the preferred option for evaluating the output of generation systems, particularly those that are designed to incorporate variation into their output, while measures based on strict corpus similarity are less likely to be useful." ></td>
	<td class="line x" title="142:155	6 Conclusions This paper has presented three methods for using corpus data to select facial displays for an embodied agent and shown the results from two studies comparing the output generated by these methods." ></td>
	<td class="line x" title="143:155	When human judges rated the output, they preferred the original displays from the corpus and strongly disliked the displays selected by a majority-choice strategy, with the weighted and rule-based strategies in between." ></td>
	<td class="line x" title="144:155	In the automated evaluation, the metrics that directly compared the generated output against the corpus data favoured the majority strategy and did not show any relationship with the user preferences." ></td>
	<td class="line x" title="145:155	On the other hand, the number of displays accompanying a sentence and the diversity of those displays both had a positive relationship with the rate at which users selected that display schedule." ></td>
	<td class="line x" title="146:155	These results confirm those of previous textgeneration evaluations and extend these results to the multimodal-generation case." ></td>
	<td class="line x" title="147:155	This adds to the body of evidence that, even though direct corpus reproduction is often the easiest factor to analyse automatically, it is rarely an accurate reflection of user reactions to generated output." ></td>
	<td class="line x" title="148:155	If a system performs well on this type of metric, its output tends to be constrained to a small space; for example, the majoritychoice strategy used in these studies nearly always selected a nodding expression." ></td>
	<td class="line x" title="149:155	For most generation tasks, output options beyond those in the corpus are often equally valid, and users seem to prefer a system that makes use of this wider space of variations." ></td>
	<td class="line x" title="150:155	This suggests that corpus-based generation systems should use strategies that retain the full range of variation, andperhaps most importantlythat metrics based on factors other than strict similarity are more likely to capture human preferences when evaluating generated output." ></td>
	<td class="line x" title="151:155	The user study described here was based only on the preferences of human judges." ></td>
	<td class="line x" title="152:155	In future, it would be informative to include more task-based measures such as task success and time taken, as user preferences do not always correlate with performance (Nielsen and Levy, 1994), to see if a different style of automated measure agrees better with the results of this sort of user study." ></td>
	<td class="line x" title="153:155	Acknowledgements This work was partly supported by the EU projects COMIC (IST-2001-32311) and JAST (FP6-003747IP)." ></td>
	<td class="line x" title="154:155	Thanks to Jon Oberlander, Jean Carletta, and the INLG reviewers for helpful feedback." ></td>
	<td class="line x" title="155:155	102" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:135	Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 3340 Manchester, August 2008 Mixed-Source Multi-Document Speech-to-Text Summarization Ricardo Ribeiro INESC ID Lisboa/ISCTE/IST Spoken Language Systems Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal rdmr@l2f.inesc-id.pt David Martins de Matos INESC ID Lisboa/IST Spoken Language Systems Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal david@l2f.inesc-id.pt Abstract Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disfluencies, or difficulties in the accurate identification of sentence boundaries." ></td>
	<td class="line x" title="2:135	We propose the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speechto-text summarization." ></td>
	<td class="line x" title="3:135	In this work, we explore the possibilities offered by phonetic information to select the background information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information." ></td>
	<td class="line x" title="4:135	Results show that summaries generated using this approach are considerably better than those produced by an up-to-date latent semantic analysis (LSA) summarization method and suggest that humans prefer summaries restricted to the information conveyed in the input source." ></td>
	<td class="line x" title="5:135	1 Introduction News have been the subject of summarization for a long time, demonstrating the importance of both the subject and the process." ></td>
	<td class="line x" title="6:135	Systems like NewsInEssence (Radev et al., 2005), Newsblaster (McKeown et al., 2002), or even Google c2008." ></td>
	<td class="line x" title="7:135	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="8:135	Some rights reserved." ></td>
	<td class="line x" title="9:135	News substantiate this relevance that is also supported by the spoken language scenario, where most speech summarization systems concentrate on broadcast news (McKeown et al., 2005)." ></td>
	<td class="line x" title="10:135	Nevertheless, although the pioneering efforts on summarization go back to the work of Luhn (1958) and Edmundson (1969), it is only after the renaissance of summarization as a research area of great activityfollowing up on the Dagstuhl Seminar (Endres-Niggemeyer et al., 1995)that the first multi-document news summarization system, SUMMONS (McKeown and Radev, 1995), makes its breakthrough (Radev et al., 2005; Sparck Jones, 2007)." ></td>
	<td class="line x" title="11:135	In what concerns speech summarization, the state of affairs is more problematic: news summarization systems appeared later and still focus only on single document summarization (McKeown et al., 2005)." ></td>
	<td class="line x" title="12:135	In fact, while text summarization has attained some degree of success (Hovy, 2003; McKeown et al., 2005; Sparck Jones, 2007) due to the considerable body of work, speech summarization still requires further research, both in speech and text analysis, in order to overcome the specific challenges of the task (McKeown et al., 2005; Furui, 2007)." ></td>
	<td class="line x" title="13:135	Issues like speech recognition errors, disfluencies, and difficulties in accurately identifying sentence boundaries must be taken into account when summarizing spoken language." ></td>
	<td class="line x" title="14:135	However, if on the one hand, recognition errors seem not to have a considerable impact on the summarization task (Murray et al., 2006; Murray et al., 2005), on the other hand, spoken language summarization systems often explore ways of minimizing that impact (Zechner and Waibel, 2000; Hori et al., 2003; Kikuchi et al., 2003)." ></td>
	<td class="line x" title="15:135	We argue that by including related solid background information from a different source less prone to this kind of errors (e.g., a textual source) 33 in the summarization process, we are able to reduce the influence of recognition errors on the resulting summary." ></td>
	<td class="line x" title="16:135	To support this argument, we developed a new approach to speech-to-text summarization that combines information from multiple information sources to produce a summary driven by the spoken language document to be summarized." ></td>
	<td class="line x" title="17:135	The idea mimics the natural human behavior, in which information acquired from different sources is used to build a better understanding of a given topic (Wan et al., 2007)." ></td>
	<td class="line x" title="18:135	Furthermore, we build on the conjecture that this background information is often used by humans to overcome perception difficulties." ></td>
	<td class="line x" title="19:135	In that sense, one of our goals is also to understand what is expected in a summary: a comprehensive, shorter, text that addresses the same subject of the input source to be summarized (possibly introducing new information); or a text restricted to the information conveyed in the input source." ></td>
	<td class="line x" title="20:135	This work explores the use of phonetic domain information to overcome speech recognition errors and disfluencies." ></td>
	<td class="line x" title="21:135	Instead of using the traditional output of the ASR module, we use the phonetic transliteration of the output and compare it to the phonetic transliteration of solid background information." ></td>
	<td class="line x" title="22:135	This enables the use of text, related to the input source, free from the common speech recognition issues, in further processing." ></td>
	<td class="line x" title="23:135	We use broadcast news as a case study and news stories from online newspapers provide the background information." ></td>
	<td class="line x" title="24:135	Media monitoring systems, used to transcribe and disseminate news, provide an adequate framework to test the proposed method." ></td>
	<td class="line x" title="25:135	This document is organized as follows: section 2 briefly introduces the related work; section 3 presents a characterization of the speech-to-text summarization problem and how we propose to address it; section 4 explicits our use of phonetic domain information, given the previously defined context; the next section describes the case study, including the experimental set up and results; conclusions close the document." ></td>
	<td class="line x" title="26:135	2 Related Work McKeown et al.(2005) depict spoken language summarization as a much harder task than text summarization." ></td>
	<td class="line x" title="28:135	In fact, the previously enumerated problems that make speech summarization such a difficult task constrain the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible (Christensen et al., 2003))." ></td>
	<td class="line x" title="29:135	On the other hand, speech offers possibilities like the use of prosody and speaker identification to ascertain relevant content." ></td>
	<td class="line x" title="30:135	Furui (2007) identifies three main approaches to speech summarization: sentence extractionbased methods, sentence compaction-based methods, and combinations of both." ></td>
	<td class="line x" title="31:135	Sentence extractive methods comprehend, essentially, methods like LSA (Gong and Liu, 2001), Maximal Marginal Relevance (Carbonell and Goldstein, 1998), and feature-based methods (Edmundson, 1969)." ></td>
	<td class="line x" title="32:135	Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and discourse features to summarize documents from domains like broadcast news or meetings (Maskey and Hirschberg, 2005; Murray et al., 2006; Ribeiro and de Matos, 2007)." ></td>
	<td class="line x" title="33:135	Even so, spoken language summarization is still quite distant from text summarization in what concerns the use of discourse features, and shallow approaches is what can be found in state-of-the-art work such as the one presented by Maskey and Hirschberg (2005) or Murray et al.(2006)." ></td>
	<td class="line x" title="35:135	Sentence compaction methods are based on word removal from the transcription, with recognition confidence scores playing a major role (Hori et al., 2003)." ></td>
	<td class="line x" title="36:135	A combination of these two types of methods was developed by Kikuchi et al.(2003), where summarization is performed in two steps: first, sentence extraction is done through feature combination; second, compaction is done by scoring the words in each sentence and then a dynamic programming technique is applied to select the words that will remain in the sentence to be included in the summary." ></td>
	<td class="line x" title="38:135	3 Problem Characterization Summarization can be seen as a reductive transformation  that, given an input source I, produces a summary S: S = (I), where len(S) < len(I) and inf (S) is as close as possible of inf (I); len() is the length of the given input and inf () is the information conveyed by its argument." ></td>
	<td class="line x" title="39:135	The problem is that in order to compute S, we are not using I, but I, a noisy representation of I. 34 Thus, we are computing S, which is a summary affected by the noise present in I: S = (I)." ></td>
	<td class="line x" title="40:135	This means that inf ( S)inf (S)inf (I), whereas len( S)len(S) < len(I)." ></td>
	<td class="line x" title="41:135	Our argument is that using a similar reductive transformation , where solid background information B is also given as input, it is possible to compute a summary S: S = (I,B), such that inf ( S)(inf ( S)inf (S))inf (I), with len( S)len( S)len(S) < len(I)." ></td>
	<td class="line x" title="42:135	As seen in section 2, the most common method to perform these transformations is by selecting sentences (or extracts) from the corresponding input sources." ></td>
	<td class="line x" title="43:135	Thus, let the input source representation I be composed by a sequence of extracts ei, I = e1,e2,,en and the background information be defined as a sequence of sentences B = s1,s2,,sm." ></td>
	<td class="line x" title="44:135	The proposed method consists of selecting sentences si form the background information B such that sim(si,ej) <0im0jn, with sim() being a similarity function and  an adequate threshold." ></td>
	<td class="line x" title="45:135	The difficulty lies in defining the function and the threshold." ></td>
	<td class="line x" title="46:135	4 Working in the phonetic domain The approach we introduce minimizes the effects of recognition errors through the selection, from previously determined background knowledge, of sentence-like units close to the ones of the news story transcription." ></td>
	<td class="line x" title="47:135	In order to select sentence-like units, while diminishing recognition problems, we compute the similarity between them at the phonetic level." ></td>
	<td class="line x" title="48:135	The estimation of the threshold is based on the distance, measured in the phonetic Feature Values Type vowel, consonant Vowel length short, long, diphthong, schwa Vowel height high, mid, low Vowel frontness front mid back Lip rounding yes, no Consonant type stop, fricative, affricative, nasal, liquid Place of articulation labial, alveolar, palatal, labio-dental, dental, velar Consonant voicing yes, no Table 1: Phone features." ></td>
	<td class="line x" title="49:135	domain, between the output of the ASR and its hand-corrected version." ></td>
	<td class="line x" title="50:135	The selection of sentences from the background information is based on the alignment cost of the phonetic transcriptions of sentences from the input source and sentence from the background information." ></td>
	<td class="line x" title="51:135	Sentences from the background information with alignment costs below the estimated threshold are selected to be used in summary generation." ></td>
	<td class="line x" title="52:135	4.1 Similarity Between Segments There are several ways to compute phonetic similarity." ></td>
	<td class="line x" title="53:135	Kessler (2005) states that phonetic distance can be seen as, among other things, differences between acoustic properties of the speech stream, differences in the articulatory positions during production, or as the perceptual distance between isolated sounds." ></td>
	<td class="line x" title="54:135	Choosing a way to calculate phonetic distance is a complex process." ></td>
	<td class="line x" title="55:135	The phone similarity function used in this process is based on a model of phone production, where the phone features correspond to the articulatory positions during production: the greater the matching between phone features, the smaller the distance between phones." ></td>
	<td class="line x" title="56:135	The phone features used are described in table 1." ></td>
	<td class="line x" title="57:135	The computation of the similarity between sentence-like units is based on the alignment of the phonetic transcriptions of the given segments." ></td>
	<td class="line x" title="58:135	The generation of the possible alignments and the selection of the best alignment is done through the use of Weighted Finite-State Transducers (WFSTs) (Mohri, 1997; Paulo and Oliveira, 2002)." ></td>
	<td class="line x" title="59:135	35 4.2 Threshold Estimation Process To estimate the threshold to be used in the sentence selection process, we use the algorithm presented in figure 1." ></td>
	<td class="line x" title="60:135	The procedure consists of comparing automatic transcriptions and their hand-corrected versions: the output is the average difference between the submitted inputs." ></td>
	<td class="line x" title="61:135	Phonetic transliteration Phonetic transliteration Sentence segmented ASR output Manual transcription Projection of the sentences of the ASR ouput over the manual transcription Sentence segmented Manual transcription Sentence-bysentence distance calculation Figure 1: Threshold estimation process." ></td>
	<td class="line x" title="62:135	The idea is that the phonetic distance between the automatic transcription and its hand-corrected version would be similar to the phonetic distance between the automatic transcription and the background information." ></td>
	<td class="line x" title="63:135	Even though this heuristic may appear naif, we believe it is adequate as a rough approach, considering the target material (broadcast news)." ></td>
	<td class="line x" title="64:135	5 A Case Study Using Broadcast News 5.1 Media Monitoring System SSNT (Amaral et al., 2007) is a system for selective dissemination of multimedia contents, working primarily with Portuguese broadcast news services." ></td>
	<td class="line x" title="65:135	The system is based on an ASR module, that generates the transcriptions used by the topic segmentation, topic indexing, and title&summarization modules." ></td>
	<td class="line x" title="66:135	User profiles enable the system to deliver e-mails containing relevant news stories." ></td>
	<td class="line x" title="67:135	These messages contain the name of the news service, a generated title, a summary, a link to the corresponding video segment, and a classification according to a thesaurus used by the broadcasting company." ></td>
	<td class="line x" title="68:135	Preceding the speech recognition module, an audio preprocessing module, based on Multi-layer Perceptrons, classifies the audio in accordance to several criteria: speech/non-speech, speaker segmentation and clustering, gender, and background conditions." ></td>
	<td class="line x" title="69:135	The ASR module, based on a hybrid speech recognition system that combines Hidden Markov Models with Multi-layer Perceptrons, with an average word error rate of 24% (Amaral et al., 2007), greatly influences the performance of the subsequent modules." ></td>
	<td class="line x" title="70:135	The topic segmentation and topic indexing modules were developed by Amaral and Trancoso (2004)." ></td>
	<td class="line x" title="71:135	Topic segmentation is based on clustering and groups transcribed segments into stories." ></td>
	<td class="line x" title="72:135	The algorithm relies on a heuristic derived from the structure of the news services: each story starts with a segment spoken by the anchor." ></td>
	<td class="line x" title="73:135	This module achieved an F-measure of 68% (Amaral et al., 2007)." ></td>
	<td class="line x" title="74:135	The main problem identified by the authors was boundary deletion: a problem which impacts the summarization task." ></td>
	<td class="line x" title="75:135	Topic indexing is based on a hierarchically organized thematic thesaurus provided by the broadcasting company." ></td>
	<td class="line x" title="76:135	The hierarchy has 22 thematic areas on the first level, for which the module achieved a correctness of 91.4% (Amaral et al., 2006; Amaral et al., 2007)." ></td>
	<td class="line x" title="77:135	Batista et al.(2007) inserted a module for recovering punctuation marks, based on maximum entropy models, after the ASR module." ></td>
	<td class="line x" title="79:135	The punctuation marks addressed were the full stop and comma, which provide the sentence units necessary for use in the title&summarization module." ></td>
	<td class="line x" title="80:135	This module achieved an F-measure of 56% and SER (Slot Error Rate, the measure commonly used to evaluate this kind of task) of 0.74." ></td>
	<td class="line x" title="81:135	Currently, the title&summarization module produces a summary composed by the first n sentences, as detected by the previous module, of each news story and a title (the first sentence)." ></td>
	<td class="line x" title="82:135	5.2 Corpora Two corpora were used in this experiment: a broadcast news corpus, the subject of our summarization efforts; and a written newspaper corpus, used to select the background information." ></td>
	<td class="line x" title="83:135	36 Corpus Stories SUs Tokens Duration train 184 2661 57063 5h test 26 627 7360 1h Table 2: Broadcast news corpus composition." ></td>
	<td class="line x" title="84:135	The broadcast news corpus is composed by 6 Portuguese news programs, and exists in two versions: an automatically processed one, and a handcorrected one." ></td>
	<td class="line x" title="85:135	Its composition (number of stories, number of sentence-like units (SUs), number of tokens, and duration) is detailed in table 2." ></td>
	<td class="line x" title="86:135	To estimate the threshold used for the selection of the background information, 5 news programs were used." ></td>
	<td class="line x" title="87:135	The last one was used for evaluation." ></td>
	<td class="line x" title="88:135	The written newspaper corpus consists of the online version a Portuguese newspaper, downloaded daily from the Internet." ></td>
	<td class="line x" title="89:135	In this experiment, three editions of the newspaper were used, corresponding to the day and the two previous days of the news program to be summarized." ></td>
	<td class="line x" title="90:135	The corpus is composed by 135 articles, 1418 sentence-like units, and 43102 tokens." ></td>
	<td class="line x" title="91:135	5.3 The Summarization Process The summarization process we implemented is characterized by the use of LSA to compute the relevance of the extracts (sentence-like units) of the given input source." ></td>
	<td class="line x" title="92:135	LSA is based on the singular vector decomposition (SVD) of the term-sentence frequency mn matrix, M. U is an mn matrix of left singular vectors;  is the nn diagonal matrix of singular values; and, V is the nn matrix of right singular vectors (only possible if mn): M = UVT The idea behind the method is that the decomposition captures the underlying topics of the document by means of co-occurrence of terms (the latent semantic analysis), and identifies the best representative sentence-like units of each topic." ></td>
	<td class="line x" title="93:135	Summary creation can be done by picking the best representatives of the most relevant topics according to a defined strategy." ></td>
	<td class="line x" title="94:135	For this summarization process, we implemented a module following the original ideas of Gong and Liu (2001) and the ones of Murray, Renals, and Carletta (2005) for solving dimensionality problems, and using, for matrix operations, the GNU Scientific Library1." ></td>
	<td class="line x" title="95:135	5.4 Experimental Results Our main objective was to understand if it is possible to select relevant information from background information that could improve the quality of speech-to-text summaries." ></td>
	<td class="line x" title="96:135	To assess the validity of this hypothesis, five different processes of generating a summary were considered." ></td>
	<td class="line x" title="97:135	To better analyze the influence of the background information, all automatic summarization methods are based on the up-to-date LSA method previously described: one taking as input only the news story to be summarized (Simple) and used as baseline; other taking as input only the selected background information (Background only); and, the last one, using both the news story and the background information (Background + News)." ></td>
	<td class="line x" title="98:135	The other two processes were human: extractive (using only the news story) and abstractive (understanding the news story and condensing it by means of paraphrase)." ></td>
	<td class="line x" title="99:135	Since the abstractive summaries had already been created, summary size was determined by their size (which means creating summaries using a compression rate of around 10% of the original size)." ></td>
	<td class="line x" title="100:135	As mentioned before, the whole summarization process begins with the selection of the background information." ></td>
	<td class="line x" title="101:135	Using the threshold estimated as described in section 4.2 and the method described in section 4.1 to compute similarity between sentence-like units, no background information was selected for 11 of the 26 news stories of the test corpus." ></td>
	<td class="line x" title="102:135	For the remaining 15 news stories, summaries were generated using the three automatic summarization strategies described before." ></td>
	<td class="line nc" title="103:135	In what concerns the evaluation process, although ROUGE (Lin, 2004) is the most common evaluation metric for the automatic evaluation of summarization, since our approach might introduce in the summary information that it is not present in the original input source, we found that a human evaluation was more adequate to assess the relevance of that additional information." ></td>
	<td class="line x" title="104:135	A perceptual evaluation is also adequate to assess the perceive quality of the summaries and a better indicator of the what is expected to be in a summary." ></td>
	<td class="line x" title="105:135	We asked an heterogeneous group of sixteen people to evaluate the summaries created for the 15 news stories for which background information 1http://www.gnu.org/software/gsl/ 37 0 20 40 60 80 100 120 Simple (News only) Background only Background + News Human Extractive Human Abstractive ns00 ns01 ns02 ns03 ns04 ns05 ns06 ns07 ns08 ns09 ns10 ns11 ns12 ns13 ns14 Figure 2: Overall results for each summary creation method (nsnn identifies a news story)." ></td>
	<td class="line x" title="106:135	was selected." ></td>
	<td class="line x" title="107:135	Each evaluator was given, for each story, the news story itself (without background information) and five summaries, corresponding to the five different methods presented before." ></td>
	<td class="line x" title="108:135	The evaluation procedure consisted in identifying the best summary and in the classification of each summary (15, 5 is better) according to its content and readability (which covers issues like grammaticality, existence of redundant information, or entity references (Nenkova, 2006))." ></td>
	<td class="line x" title="109:135	0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ns00 ns01 ns02 ns03 ns04 ns05 ns06 ns07 ns08 ns09 ns10 ns11 ns12 ns13 ns14 Simple (News only) Background only Background + News Human Extractive Human Abstractive Figure 3: Relative results for each news story (nsnnidentifies a news story; stack order is inverse of legend order)." ></td>
	<td class="line x" title="110:135	Surprisingly enough (see figures 2 and 3), in general, the extractive human summaries were preferred over the abstractive ones." ></td>
	<td class="line x" title="111:135	Moreover, the summaries generated automatically using background information (exclusively or not) were also selected as best summary (over the human created ones) a non-negligible number of times." ></td>
	<td class="line x" title="112:135	The poorest performance was attained, as expected, by the simple LSA summarizer, only preferred on two news stories for which all summaries were very similar." ></td>
	<td class="line x" title="113:135	The results of the two approaches using background information were very close, a result that can be explained by the fact the summaries generated by these two approaches were equal for 11 of the 15 news stories (in the remaining 4, the average distribution was 31.25% from the news story versus 68.75% from the background information)." ></td>
	<td class="line x" title="114:135	Figure 4 further discriminates the results in terms of content and readability." ></td>
	<td class="line x" title="115:135	0.00 0.50 1.00 1.50 2.00 2.50 3.00 3.50 4.00 4.50 5.00 Simple (News only) Background only Background + News Human Extractive Human Abstractive content readability Figure 4: Average of the content and readability scores for each summary creation method." ></td>
	<td class="line x" title="116:135	Regarding content, the results suggest that the choice of the best summary is highly correlated with its content, as the average content scores mimic the overall ones of figure 2." ></td>
	<td class="line x" title="117:135	In what concerns readability, the summaries generated using background information achieved the best results." ></td>
	<td class="line x" title="118:135	The reasons underlying these results are that the newspaper writing is naturally better planned than speech and that speech transcriptions are affected by the several problems described before (and the original motivation for the work), hence the idea of using them as background information." ></td>
	<td class="line x" title="119:135	However, what is odd is that the result obtained by the human abstractive summary creation method is worse than the ones obtained by automatic generation using background information, which could suffer from coherence and cohesion problems." ></td>
	<td class="line x" title="120:135	One possible explanation is that the human abstractive summaries tend to mix both informa38 tive and indicative styles of summary." ></td>
	<td class="line x" title="121:135	0.00 0.20 0.40 0.60 0.80 1.00 1.20 Simple (News only) Background only Background + News Human Extractive Human Abstractive content readability Figure 5: Standard deviation of the content and readability scores." ></td>
	<td class="line x" title="122:135	Figure 5 presents the standard deviation for content and readability scores: concerning content, automatically generated summaries using background information achieved the highest standard deviation scores (see also figure 6 for a sample story)." ></td>
	<td class="line x" title="123:135	That is in part supported by some commentaries made by the human evaluators on whether a summary should contain information that is not present in the input source." ></td>
	<td class="line x" title="124:135	This aspect and the obtained results, suggest that this issue should be further analyzed, possibly using an extrinsic evaluation setup." ></td>
	<td class="line x" title="125:135	On the other hand, readability standard deviation scores show that there is a considerable agreement in what concerns this criterion." ></td>
	<td class="line x" title="126:135	0.00 0.50 1.00 1.50 2.00 2.50 3.00 3.50 4.00 4.50 5.00 Simple (News only) Background only Background + News Human Extractive Human Abstractive Content (avg) Readability (avg) Content (stdev) Readability (stdev) Figure 6: Average and standard deviation of the content and readability scores for one news story." ></td>
	<td class="line x" title="127:135	6 Conclusions We present a new approach to speech summarization that goes in the direction of the integration of text and speech analysis, as suggested by McKeown et al.(2005)." ></td>
	<td class="line x" title="129:135	The main idea is the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech-to-text summarization." ></td>
	<td class="line x" title="130:135	In this work, we explore the possibilities offered by phonetic information to select the background information and conducted a perceptual evaluation to assess the relevance of the inclusion of that information." ></td>
	<td class="line x" title="131:135	The results obtained show that the human evaluators preferred human extractive summaries over human abstractive summaries." ></td>
	<td class="line x" title="132:135	Moreover, simple LSA summaries attained the poorest results both in terms of content and readability, while human extractive summaries achieved the best performance in what concerns content, and a considerably better performance than simple LSA in what concerns readability." ></td>
	<td class="line x" title="133:135	This suggests that it is sill relevant to pursue new methods for relevance estimation." ></td>
	<td class="line x" title="134:135	On the other hand, automatically generated summaries using background information were significantly better than simple LSA." ></td>
	<td class="line x" title="135:135	This indicates that background information is a viable way to increase the quality of automatic summarization systems." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:216	Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 4148 Manchester, August 2008 Evaluating automatically generated user-focused multi-document summaries for geo-referenced images Ahmet Aker Department of Computer Science University of Sheffield Sheffield, S1 4DP, UK A.Aker@dcs.shef.ac.uk Robert Gaizauskas Department of Computer Science University of Sheffield Sheffield, S1 4DP, UK R.Gaizauskas@dcs.shef.ac.uk Abstract Thispaperreportsaninitialstudythataims to assess the viability of a state-of-the-art multi-document summarizer for automatic captioning of geo-referenced images." ></td>
	<td class="line x" title="2:216	The automatic captioning procedure requires summarizing multiple web documents that contain information related to images location." ></td>
	<td class="line oc" title="3:216	We use SUMMA (Saggion and Gaizauskas, 2005) to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics (Lin, 2004) relative to human generated summaries." ></td>
	<td class="line x" title="4:216	Results show that, even though query-based summaries perform better than generic ones, they are still not selecting the information that human participants do." ></td>
	<td class="line x" title="5:216	In particular, the areas of interest that human summaries display (history, travel information, etc.) are not contained in the query-based summaries." ></td>
	<td class="line x" title="6:216	For our future work in automatic image captioning this result suggests that developing the query-based summarizer further and biasing it to account for user-specific requirements will prove worthwhile." ></td>
	<td class="line x" title="7:216	1 Introduction Retrieving textual information related to a location shown in an image has many potential applications." ></td>
	<td class="line x" title="8:216	It could help users gain quick access to the information they seek about a place of interest just by taking its picture." ></td>
	<td class="line x" title="9:216	Such textual informationcouldalso, forinstance, beusedbyajournalist c2008." ></td>
	<td class="line x" title="10:216	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="11:216	Some rights reserved." ></td>
	<td class="line x" title="12:216	whoisplanningtowriteanarticleaboutabuilding, or by a tourist who seeks further interesting places to visit nearby." ></td>
	<td class="line x" title="13:216	In this paper we aim to generate such textual information automatically by utilizing multi-document summarization techniques, where documents to be summarized are web documents that contain information related to the image content." ></td>
	<td class="line x" title="14:216	We focus on geo-referenced images, i.e. images tagged with coordinates (latitude and longitude) and compass information, that show things with fixed locations (e.g. buildings, mountains, etc.)." ></td>
	<td class="line x" title="15:216	Attempts towards automatic generation of image-related textual information or captions have been previously reported." ></td>
	<td class="line x" title="16:216	Deschacht and Moens (2007) and Mori et al.(2000) generate image captions automatically by analyzing image-related text from the immediate context of the image, i.e. existing image captions, surrounding text in HTML documents, text contained in the image, etc. The authors identify named entities and other noun phrases in the image-related text and assign these to the image as captions." ></td>
	<td class="line x" title="18:216	Other approaches create image captions by taking into consideration image features as well as image-related text (Westerveld, 2000; Barnard et al., 2003; Pan et al., 2004)." ></td>
	<td class="line x" title="19:216	These approaches can address all kinds of images, but focus mostly on images of people." ></td>
	<td class="line x" title="20:216	Theyanalyzeonlytheimmediatetextualcontextof the image on the web and are concerned with describing what is in the image only." ></td>
	<td class="line x" title="21:216	Consequently, background information about the objects in the image is not provided." ></td>
	<td class="line x" title="22:216	Our aim, however, is to have captions that inform users specific interests about a location, which clearly includes more than just image content description." ></td>
	<td class="line x" title="23:216	Multi-document summarization techniques offer the possibility to include image-related information from multiple 41 documents, however, the challenge lies in being able to summarize unrestricted web documents." ></td>
	<td class="line x" title="24:216	Various multi-document summarization tools have been developed: SUMMA (Saggion and Gaizauskas, 2005), MEAD (Radev et al., 2004), CLASSY (Conroy et al., 2005), CATS (Farzinder et al., 2005) and the system of Boros et al.(2001), to name just a few." ></td>
	<td class="line x" title="26:216	These systems generate either generic or query-based summaries or both." ></td>
	<td class="line x" title="27:216	Generic summaries address a broad readership whereas query-based summaries are preferred by specific groups of people aiming for quick knowledge gain about specific topics (Mani, 2001)." ></td>
	<td class="line x" title="28:216	SUMMA and MEAD generate both generic and query-based multi-document summaries." ></td>
	<td class="line x" title="29:216	Boros et al.(2001) create only generic summaries, whileCLASSYandCATScreateonlyquery-based summaries from multiple documents." ></td>
	<td class="line x" title="31:216	The performance of these tools has been reported for DUC tasks1." ></td>
	<td class="line x" title="32:216	As Sekine and Nobata (2003) note, although DUC tasks provide a common evaluation standard, they are restricted in topic and are somewhat idealized." ></td>
	<td class="line x" title="33:216	For our purposes the summarizer needs to create summaries from unrestricted web input, for which there are no previous performance reports." ></td>
	<td class="line x" title="34:216	For this reason we evaluate the performance of both a generic and a query-based summarizer and use SUMMA which provides both summarization modes." ></td>
	<td class="line x" title="35:216	We hypothesize that a query-based summarizer will better address the problem of creating summariestailoredtousersneeds." ></td>
	<td class="line x" title="36:216	Thisisbecause the query itself may contain important hints as to what the user is interested in." ></td>
	<td class="line x" title="37:216	A generic summarizer generates summaries based on the topics it observes from the documents and cannot take user specific input into consideration." ></td>
	<td class="line x" title="38:216	Using SUMMA, we generate both generic and query-based multidocument summaries of image-related documents obtained from the web." ></td>
	<td class="line x" title="39:216	In an online data collection procedure we presented a set of images with related web documents to human subjects and asked them to select from these documents the information that best describes the image." ></td>
	<td class="line x" title="40:216	Based on this user information we created model summaries against which we evaluated the automatically generated ones." ></td>
	<td class="line x" title="41:216	Section 2 in this paper describes how imagerelated documents were collected from the web." ></td>
	<td class="line x" title="42:216	In section 3 SUMMA is described in detail." ></td>
	<td class="line x" title="43:216	In 1http://www-nlpir.nist.gov/projects/duc/index.html section 4 we explain how the human image descriptions were collected." ></td>
	<td class="line x" title="44:216	Section 5 discusses the results, and section 6 concludes the paper and outlines directions for future work and improvements." ></td>
	<td class="line x" title="45:216	2 Web Document Collection For web document collection we used georeferenced images of locations in London such as Westminster Abbey, London Eye, etc. The images were taken with a digital SLR camera with a Geotagger plugged-in to its flash slot." ></td>
	<td class="line x" title="46:216	The Geotagger helped us to identify the location by means of coordinates of the position where the photographer stands, as well as the direction the camera is pointing (compass information)." ></td>
	<td class="line x" title="47:216	Based on the coordinates and compass information for each image, we carried out the following steps to collect related documents from the web:  identify a set of toponyms (terms that denote locations or associate names with locations, e.g. Westminster Abbey) that can be passed to a search engine as query terms for document search;  use a search engine to retrieve HTML documents to be summarized;  extract the pure text out of the HTML documents." ></td>
	<td class="line x" title="48:216	2.1 Toponym Collection In order to create the web queries a set of toponyms were collected semi-automatically." ></td>
	<td class="line x" title="49:216	We implemented an application (cf.Figure 1) that suggests a list of toponyms close to the photographers location." ></td>
	<td class="line x" title="51:216	The application uses Microsofts MapPoint2 service which allows users to query location-related information." ></td>
	<td class="line x" title="52:216	For example, a user can query for tourist attractions (interesting buildings, museums, art galleries etc.) close to a location that is identified by its address or its coordinates." ></td>
	<td class="line x" title="53:216	Based on the coordinates (latitude and longitude), important toponyms for a particular image can be queried from the MapPoint database." ></td>
	<td class="line x" title="54:216	In order to facilitate this, MapPoint returns a metric that measures the importance of each toponym." ></td>
	<td class="line x" title="55:216	A value close to zero means that the returned toponym is closer to the specified coordinates than a toponym with a higher value." ></td>
	<td class="line x" title="56:216	For instance for 2http://www.microsoft.com/mappoint/ 42 Figure 1: Image Toponym Collector: Westminster Abbey, Lat: 51.50024 Lon: -0.128138333: Direction: 137.1 the image of Westminster Abbey shown in the Image box of Figure 1 the following toponyms are collected: Queen Elizabeth II Conf." ></td>
	<td class="line x" title="57:216	Centre: 0.059 Parliament Square: 0.067 Westminster Abbey: 0.067 The photographers location is shown with a black dot on the first map in the Maps box of Figure 1." ></td>
	<td class="line x" title="58:216	The application suggests the toponyms shown in the Suggested Terms list." ></td>
	<td class="line x" title="59:216	Knowing the direction the photographer was facing helps us to select the correct toponyms from the list of suggested toponyms." ></td>
	<td class="line x" title="60:216	The current MapPoint implementation does not allow an arrow to be drawn on the map which would be the best indicationofthedirectionthephotographerisfacing." ></td>
	<td class="line x" title="61:216	To overcome this problem we create a second map (cf.Maps box of Figure 1) that shows another dot moved 50 meters in the compass direction." ></td>
	<td class="line x" title="63:216	By followingthedotfromthefirstmaptothesecondmap we can determine the direction the photographer is facing." ></td>
	<td class="line x" title="64:216	When the direction is known, it is certain that the image shows Westminster Abbey and not the Queen Elizabeth II Conf." ></td>
	<td class="line x" title="65:216	Centre or Parliament Square." ></td>
	<td class="line x" title="66:216	The Queen Elizabeth II Conf." ></td>
	<td class="line x" title="67:216	Centre is behind the photographer and Parliament Square is on the left hand side." ></td>
	<td class="line x" title="68:216	Consequently in this example the toponym Westminster Abbey is selected manually for the web search." ></td>
	<td class="line x" title="69:216	In order to avoid ambiguities, the city name and the country name (also generated by MapPoint) are added manually to the selected toponyms." ></td>
	<td class="line x" title="70:216	Hence, for Westminster Abbey, London and United Kingdom areaddedtothetoponym list." ></td>
	<td class="line x" title="71:216	Finally the terms in the toponym list are simply separated by a boolean AND operator to form the web query." ></td>
	<td class="line x" title="72:216	Then, the query is passed to the search engine as described in the next section." ></td>
	<td class="line x" title="73:216	2.2 Document Query and Text Extraction The web queries were passed to the Google Search engine and the 20 best search results were retrieved, from which only 11 were taken for the summarization process." ></td>
	<td class="line x" title="74:216	We ensure that these 20 search results are healthy hyperlinks, i.e. that the content of the hyperlink is accessible." ></td>
	<td class="line x" title="75:216	In addition to this, multiple hyperlinks belonging to the same domain are ignored as it is assumed that the content obtained from the same domain would be similar." ></td>
	<td class="line x" title="76:216	Each remaining search result is crawled to obtain its content." ></td>
	<td class="line x" title="77:216	The web-crawler downloads only the content of the document residing under the hyperlink, which was previously found as a search result, and does not follow any other hyperlinks within the document." ></td>
	<td class="line x" title="78:216	The content obtained by the web-crawler encapsulates an HTML structured document." ></td>
	<td class="line x" title="79:216	We further process this using an HTML parser3 to select the pure text, i.e. text consisting of sentences." ></td>
	<td class="line x" title="80:216	The HTML parser removes advertisements, menu items, tables, java scripts etc. from the HTMLdocumentsandkeepssentenceswhichcontainatleast4words." ></td>
	<td class="line x" title="81:216	Thisnumberwaschosenafter several experiments." ></td>
	<td class="line x" title="82:216	The resulting data is passed on to the multi-document summarizer which is described in the next section." ></td>
	<td class="line x" title="83:216	3 SUMMA SUMMA4 is a set of language and processing resources to create and evaluate summarization systems (single document, multi-document, multilingual)." ></td>
	<td class="line x" title="84:216	The components can be used within GATE5 to produce ready summarization applications." ></td>
	<td class="line x" title="85:216	SUMMA has been used in this work to create an extractive multi-document summarizer: both generic and query-based." ></td>
	<td class="line x" title="86:216	In the case of generic summarization SUMMA uses a single cluster approach to summarize n related documents which are given as input." ></td>
	<td class="line x" title="87:216	Using GATE, SUMMA first applies sentence detection and sentence tokenisation to the given documents." ></td>
	<td class="line x" title="88:216	Then each sentence in the documents is represented as a vector in a vector space model (Salton, 1988), where each vector position contains a term 3http://htmlparser.sourceforge.net/ 4http://www.dcs.shef.ac.uk/ saggion/summa/default.htm 5http://gate.ac.uk 43 (word) and a value which is a product of the term frequency in the document and the inverse documentfrequency(IDF),ameasurementoftheterms distribution over the set of documents (Salton and Buckley, 1988)." ></td>
	<td class="line x" title="89:216	Furthermore, SUMMA enhances the sentence vector representation with further features such as the sentence position in its document and the sentence similarity to the lead-part in its document." ></td>
	<td class="line x" title="90:216	Inadditiontocomputingthevectorrepresentation for all sentences in the document collection the centroid of this sentence representation is also computed." ></td>
	<td class="line x" title="91:216	In the sentence selection process, each sentence in the collection is ranked individually, and the top sentencesarechosentobuildupthefinalsummary." ></td>
	<td class="line x" title="92:216	The ranking of a sentence depends on its distance to the centroid, its absolute position in its document and its similarity to the lead-part of its document." ></td>
	<td class="line x" title="93:216	For calculating vector similarities, the cosine similarity measure is used (Salton and Lesk, 1968)." ></td>
	<td class="line x" title="94:216	In the case of the query-based approach, SUMMAaddsanadditionalfeaturetothesentence vector representation as computed for generic summarization." ></td>
	<td class="line x" title="95:216	For each sentence, cosine similarity to the given query is computed and added to the sentence vector representation." ></td>
	<td class="line x" title="96:216	Finally, the sentencesarescoredbysummingallfeaturesinthe vector space model according to the following formula: Sentencescore = nsummationdisplay i=1 featurei weighti Afterthescoringprocess,SUMMAstartsselecting sentences for summary generation." ></td>
	<td class="line x" title="97:216	In both generic and query-based summarization, the summary is constructed by first selecting the sentence that has the highest score, followed by the next sentence with the second highest score until the compression rate is reached." ></td>
	<td class="line x" title="98:216	However, before a sentence is selected a similarity metric for redundancy detection is applied to each sentence which decides whether a sentence is distinct enough from already selected sentences to be included in the summary or not." ></td>
	<td class="line x" title="99:216	SUMMA uses the following formula to compute the similarity between two sentences: NGramSim(S1,S2,n) = nsummationdisplay j=1 wj  grams(S1,j) intersectiontextgrams(S 2,j) grams(S1,j)uniontextgrams(S2,j) where n specifies maximum size of the n-grams to be considered, grams(SX, j) is the set of j-grams in sentence X and wj is the weight associated with j-gram similarity." ></td>
	<td class="line x" title="100:216	Two sentences are similar if NGramSim(S1, S2, n) > ." ></td>
	<td class="line x" title="101:216	In this work n is set to 4 and  to 0.1." ></td>
	<td class="line x" title="102:216	For j-gram similarity weights w1 = 0.1, w2 = 0.2, w3 = 0.3 and w4 = 0.4 are selected." ></td>
	<td class="line x" title="103:216	These values are coded in SUMMA as defaults." ></td>
	<td class="line x" title="104:216	Using SUMMA, generic and query-based summaries are generated for the image-related documents obtained from the web." ></td>
	<td class="line x" title="105:216	Each summary contains a maximum of 200 words." ></td>
	<td class="line x" title="106:216	The queries used inthequery-basedmodearetoponymscollectedas described in section 2.1." ></td>
	<td class="line x" title="107:216	4 Creating Model Summaries For evaluating automatically generated summaries as image captions, information that people associate with images is collected." ></td>
	<td class="line x" title="108:216	For this purpose, an online data collection procedure was set up." ></td>
	<td class="line x" title="109:216	Participants were provided with a set of 24 images." ></td>
	<td class="line x" title="110:216	Each image had a detailed map showing the location where it was taken, along with URLs to 11 related documents which were used for the automated summarization." ></td>
	<td class="line x" title="111:216	Figure 2 shows an example of an image and Table 2 contains the corresponding related information." ></td>
	<td class="line x" title="112:216	Each participant was asked to familiarize himor herself with the location of the image by analyzing the map and going through all 11 URLs." ></td>
	<td class="line x" title="113:216	Then each participant decided on up to 5 different pieces of information he/she would like to know if he/she sees the image or information about something he/she relates with the image." ></td>
	<td class="line x" title="114:216	The information we collected in this way is similar to information nuggets (Voorhees, 2003)." ></td>
	<td class="line x" title="115:216	Information nuggets are facts which help us assess automatic summaries by checkingwhether the summary contains the fact or not." ></td>
	<td class="line x" title="116:216	In addition to this, each participant was asked to collect the information only fromthegivendocuments,ignoringanyotherlinks in these documents." ></td>
	<td class="line x" title="117:216	Eleven students participated in this survey, simulating the scenario in which tourists look for information about an image of a popular sight." ></td>
	<td class="line x" title="118:216	The number of images annotated by each participant is shown in Table 1." ></td>
	<td class="line x" title="119:216	The participants selected the information from original HTML documents on the web and not from the documents which were preprocessed for themulti-documentsummarizationtask." ></td>
	<td class="line x" title="120:216	Wefound 44 Table 1: Number of images annotated by each particant User1 User2 User3 User4 User5 User6 User7 User8 User9 User10 User11 24 7 24 24 18 24 8 4 16 12 24 Figure 2: Example image Table 2: Information related to Figure 2 1." ></td>
	<td class="line x" title="121:216	Westminster Abbey is the place of the coronation, marriage and burial of British monarchs, except Edward V and Edward VIII since 1066 2." ></td>
	<td class="line x" title="122:216	the parish church of the Royal Family 3." ></td>
	<td class="line x" title="123:216	the centrepiece to the City of Westminster 4." ></td>
	<td class="line x" title="124:216	first church on the site is believed to have been constructed around the year 700 5." ></td>
	<td class="line x" title="125:216	The history and the monuments, crypts and memorials are not to be missed." ></td>
	<td class="line x" title="126:216	out that in some cases the participants selected information that did not occur in the preprocessed documents." ></td>
	<td class="line x" title="127:216	Toensurethattheinformationselected by the participants also occurs in the preprocessed documents, we retained only the information selected by the participants that could also be found in these documents, i.e. that was available to the summarizer." ></td>
	<td class="line x" title="128:216	Outof807nuggetsselectedbyparticipants 21 (2.6%) were not found in the documents available to the summarizer and were removed." ></td>
	<td class="line x" title="129:216	Furthermore, as the example above shows (cf.Table 2), not all the items of information selected by the participants were in form of full sentences." ></td>
	<td class="line x" title="131:216	They vary from phrases to whole sentences." ></td>
	<td class="line x" title="132:216	The participants were free to select any text unit from the documents that they related to the image content." ></td>
	<td class="line x" title="133:216	However, SUMMA works extractively and its summaries contain only sentences selected from the given input documents." ></td>
	<td class="line x" title="134:216	The user selected information was normalized to sentences in order to have comparable summaries for evaluation." ></td>
	<td class="line x" title="135:216	This was achieved by selecting the sentence(s) from the documents in which the participant-selected information was found and replacing the participant-selected phrases or clauses with the full sentence(s)." ></td>
	<td class="line x" title="136:216	In this way model summaries were obtained." ></td>
	<td class="line oc" title="137:216	5 Results The model summaries were compared against 24 summaries generated automatically using SUMMA by calculating ROUGE-1 to ROUGE4, ROUGE-L and ROUGE-W-1.2 recall metrics (Lin, 2004)." ></td>
	<td class="line o" title="138:216	For all these metrics ROUGE compares each automatically generated summary s pairwise to every model summary mi from the set of M model summaries and takes the maximum ROUGEScore value among all pairwise comparisons as the best ROUGEScore score: ROUGEScore = argmaxiROUGEScore(mi,s) ROUGE repeats this comparisonM times." ></td>
	<td class="line x" title="139:216	In each iteration it applies the Jackknife method and takes onemodelsummaryfromtheM modelsummaries away and compares the automatically generated summary s against the M  1 model summaries." ></td>
	<td class="line x" title="140:216	In each iteration one best ROUGEScore is calculated." ></td>
	<td class="line x" title="141:216	The final ROUGEScore is then the average of all best scores calculated in M iterations." ></td>
	<td class="line x" title="142:216	In this way each generic and query-based summary was compared with the corresponding model summaries." ></td>
	<td class="line x" title="143:216	The results are given in the first two columns of Table 3." ></td>
	<td class="line x" title="144:216	We also collected the common information all participants selected for a particular image and compared this to the corresponding query-based summary." ></td>
	<td class="line x" title="145:216	The common informationistheintersectionsetofthesetsofinformation eachoftheparticipantsselectedforaparticularimage." ></td>
	<td class="line x" title="146:216	The results for this comparison are shown in column QueryToCPOfModel of Table 3." ></td>
	<td class="line x" title="147:216	The model summaries were also compared against each other in order to assess the agreement between the participants." ></td>
	<td class="line x" title="148:216	To achieve this, the image information selected by each participant was compared against the rest." ></td>
	<td class="line x" title="149:216	The corresponding results are shown in column UserToUser of Table 4." ></td>
	<td class="line x" title="150:216	We applied the same pairwise comparison we used for our model summaries to the model summaries of task 5 in DUC 2004 in order to mea45 Table 3: Comparison: Automatically generated summaries against model summaries." ></td>
	<td class="line o" title="151:216	The column GenericToModel for example shows ROUGE results for generic summaries relative to model summaries." ></td>
	<td class="line x" title="152:216	CP stands for common part, i.e. common information selected by all participants." ></td>
	<td class="line x" title="153:216	Recall GenericToModel QueryToModel QueryToCPOfModel QueryToModelInDUC R-1 0.38293 0.39655 0.22084 0.3341 R-2 0.14760 0.17266 0.09894 0.0723 R-3 0.09286 0.11196 0.06222 0.0279 R-4 0.07450 0.09219 0.04971 0.0131 R-L 0.34437 0.35837 0.20913 0.3320 R-W-1.2 0.11821 0.12606 0.06350 0.1130 Table 4: Comparison: Model summaries against each other Recall UserToUser UserToUserInDUC R-1 0.42765 0.45407 R-2 0.30091 0.13820 R-3 0.26338 0.05870 R-4 0.24964 0.02950 R-L 0.40403 0.41594 R-W-1.2 0.15846 0.13973 sure the agreements between the participants on this standard task." ></td>
	<td class="line x" title="154:216	This gives us a benchmark relative to which we can assess how well users agree on what information should be related to images." ></td>
	<td class="line x" title="155:216	The results for this comparison are shown in column UserToUserInDUC of Table 4." ></td>
	<td class="line o" title="156:216	All ROUGE metrics except R-1 and R-L indicate higher agreement in human image-related summaries than in DUC document summaries." ></td>
	<td class="line o" title="157:216	The ROUGE metrics most indicative of agreement between human summaries are those that best capture words occurring in longer sequences of words immediately following each other (R-2, R-3, R-4 and R-W)." ></td>
	<td class="line x" title="158:216	If long word sequences are identical in two summaries it is more likely that they belong to the same sentence than if only single words are common, as captured by R-1, or sequences of words that do not immediately follow each other, ascapturedbyR-L.InR-Lgapsinwordsequences are ignored so that for instance A B C D G and A E B F C K D have the common sequence A B C D according to R-L." ></td>
	<td class="line x" title="159:216	R-W considers the gaps in words sequences so that this sequence would not be recognized as common." ></td>
	<td class="line x" title="160:216	Therefore the agreement on our image-related human summaries is substantially higher than agreement on DUC document human summaries." ></td>
	<td class="line x" title="161:216	The results in Table 3 support our hypothesis that query-based summaries will perform better thangenericonesonimage-relatedsummaries." ></td>
	<td class="line o" title="162:216	All ROUGE results of the query-based summaries are greater than the generic summary scores." ></td>
	<td class="line x" title="163:216	This reinforces our decision to focus on query-based summaries in order to create image-related summaries which also satisfy the users needs." ></td>
	<td class="line x" title="164:216	However, even though the query-based summaries are more appropriate for our purposes, they are not completely satisfactory." ></td>
	<td class="line x" title="165:216	The query-based summaries cover only 39% of the unigrams (ROUGE 1) in the model summaries and only 17% of the bigrams (ROUGE 2), while the model summaries have 42% agreement in unigrams and 30% agreement in bigrams (cf.column UserToUser in Table 4)." ></td>
	<td class="line o" title="167:216	The agreement between the query-based and model summaries gets lower for ROUGE-3 and ROUGE-4 indicating that the query-based summaries contain very little information in common with the participants results." ></td>
	<td class="line o" title="168:216	This indication is supported by the ROUGE-L (35%) and the low ROUGE-W (12%) agreement which are substantially lower compared to the UserToUser ROUGEL (40%) and ROUGE-W (15%) and the low ROUGE scores in column QueryToCPOfModel." ></td>
	<td class="line o" title="169:216	For comparison with automated summaries in a different domain, we include ROUGE scores of query based SUMMA used in DUC 2004 (Saggion and Gaizauskas, 2005) as shown in the last column of Table 3." ></td>
	<td class="line x" title="170:216	All scores are lower than our QueryToModel results which might be due to low agreement between human generated summaries for the DUC task (cf.UserToUserInDUC column in Table 4) or maybe because image captioning is an easier task." ></td>
	<td class="line x" title="172:216	The possibility that our summarization task is easier than DUC due to the summarizer having fewer documents to summarize or due to the documents being shorter than those in the DUC task can be excluded." ></td>
	<td class="line x" title="173:216	In the DUC task the multi-document clusters contain 10 documents on average while our summarizer works with 11 documents." ></td>
	<td class="line x" title="174:216	The mean length in documents in DUC 46 Table 5: Query-based summary for Westminster Abbey and information selected by participants Query-based summary Information selected by participants The City of London has St Pauls, but Westminster Abbey is the centrepiece to the City of Westminster." ></td>
	<td class="line x" title="175:216	Westminster Abbey should be at the top of any London travelers list." ></td>
	<td class="line x" title="176:216	Westminster Abbey, however, lacks the clear lines of a Rayonnant church, I loved Westminster Abbey on my trip to London." ></td>
	<td class="line x" title="177:216	Westminster Abbey was rebuilt after 1245 by Henry IIIs order, and in 1258 the remodeling of the east end of St. Pauls Cathedral began." ></td>
	<td class="line x" title="178:216	He was interred in Westminster Abbey." ></td>
	<td class="line x" title="179:216	From 1674 to 1678 he tuned the organ at Westminster Abbey and was employed there in 1675-76 to copy organ parts of anthems." ></td>
	<td class="line x" title="180:216	The architectural carving found at Westminster Abbey (mainly of the 1250s) has much of the daintiness of contemporary French work, although the drapery is still more like that of the early Chartres or Wells sculpture than that of the Joseph Master." ></td>
	<td class="line x" title="181:216	Nevertheless, Westminster Abbey is something to see if you have not seen it before." ></td>
	<td class="line x" title="182:216	I happened upon the Westminster Abbey on an outing to Parliament and Big Ben." ></td>
	<td class="line x" title="183:216	1.(3) Westminster Abbey is the place of the coronation, marriage and burial of British monarchs, except Edward V and Edward VIII since 1066." ></td>
	<td class="line x" title="184:216	2.(1) What is unknown, however is just how old it is. The first church on the site is believed to have been constructed around the year 700." ></td>
	<td class="line x" title="185:216	3.(2) Standing as it does between Westminster Abbey and the Houses of Parliament, and commonly called the parishchurchoftheHouseofCommons, StMargaretshas witnessed many important events in the life of this country." ></td>
	<td class="line x" title="186:216	4.(1) In addition, the Abbey is the parish church of the Royal Family, when in residence at Buckingham Palace." ></td>
	<td class="line x" title="187:216	5.(1) The history and the monuments, crypts and memorials are not to be missed." ></td>
	<td class="line x" title="188:216	6.(1) For almost one thousand years, Westminister Abbey has been the setting for much of Londons ceremonies such as Royal Weddings, Coronations, and Funeral Services." ></td>
	<td class="line x" title="189:216	7.(1) It is also where many visitors pay pilgrimage to The Tomb of the Unknown Soldier." ></td>
	<td class="line x" title="190:216	8.(1) The City of London has St Pauls, but Westminster Abbey is the centrepiece to the City of Westminster." ></td>
	<td class="line x" title="191:216	is 23 sentences while our documents have 44 sentences on average." ></td>
	<td class="line x" title="192:216	Table 5 shows an example query-based summary for the image of Westminster Abbey and the information participants selected for this particular image." ></td>
	<td class="line x" title="193:216	Jointly the participants have selected 8 different pieces of information as indicated by the bold numbers in the table." ></td>
	<td class="line x" title="194:216	The numbers in parentheses show the number of times that a particular information unit was selected." ></td>
	<td class="line x" title="195:216	By comparing the two sides it can be seen that the query-based summary does not cover most of the information from the list with the exception of item 2." ></td>
	<td class="line x" title="196:216	The item 2 is semantically related to the sentence in bold on the summary side as it addresses the year the abbey was built, but the information contained in the two descriptions is different." ></td>
	<td class="line x" title="197:216	Our results have confirmed our hypothesis that query-based summaries will better address the aim of this research, which is to get summaries tailored to users needs." ></td>
	<td class="line x" title="198:216	A generic summary does not take the user query into consideration and generatessummariesbasedonthetopicsitobserves." ></td>
	<td class="line x" title="199:216	For a set of documents containing mainly historical and little location-related information, a generic summary will probably contain a higher number of history-related than location-related sentences." ></td>
	<td class="line x" title="200:216	This might satisfy a group of people seeking historical information, however, it might not be interesting for a group who want to look for locationrelated information." ></td>
	<td class="line x" title="201:216	Therefore using a querybased multi-document summarizer is more appropriate for image-related summaries than a generic one." ></td>
	<td class="line x" title="202:216	However, the results of the query-based summaries show that even so they only cover a small partoftheinformationtheusersselect." ></td>
	<td class="line x" title="203:216	Onereason for this is that the query-based summarizer takes relevant sentences according to the query given to it and does not take into more general consideration the information likely to be relevant to the user." ></td>
	<td class="line x" title="204:216	However, we can assume that users will have shared interests in some of the information they would like to get about a particular type of object in an image (e.g. a bridge, church etc.)." ></td>
	<td class="line x" title="205:216	This assumption is supported by the high agreement betweenparticipantsperformancesinouronlinesurvey (cf.column UserToUser of Table 4)." ></td>
	<td class="line x" title="207:216	Therefore, one way to improve the performance of the query-based summarizer is to give the summarizer the information that users typically associate with a particular object type as input and bias the multi-document summarizer towards this information." ></td>
	<td class="line x" title="208:216	To do this we plan to build models of user preferences for different object types from the large number of existing image captions from web resources, which we believe will improve the quality of automatically generated captions." ></td>
	<td class="line x" title="209:216	6 Conclusion In this work we showed that query-based summarizers perform slightly better than generic summarizers on an image captioning task." ></td>
	<td class="line x" title="210:216	However, their output is not completely satisfactory when compared to what human participants indicated as important in our data collection study." ></td>
	<td class="line x" title="211:216	Our future work will concentrate on extending the query47 based summarizer to improve its performance in generating captions that match user expectations regarding specific image types." ></td>
	<td class="line x" title="212:216	This will include collectingalargenumberofexistingcaptionsfrom web sources and applying machine learning techniquesforbuildingmodelsofthekindsofinformation that people use for captioning." ></td>
	<td class="line x" title="213:216	Further work also needs to be carried out on improving the readability of the extractive caption summaries." ></td>
	<td class="line x" title="214:216	7 Acknowledgement ThisworkissupportedbytheEU-fundedTRIPOD project 6." ></td>
	<td class="line x" title="215:216	We would like to thank Horacio Saggion for his support with SUMMA." ></td>
	<td class="line x" title="216:216	We are also grateful to Emina Kurtic, Mark Sanderson, Mesude Bicak and Dilan Paranavithana for comments on the previous versions of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:174	Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 5865 Manchester, UK." ></td>
	<td class="line x" title="2:174	August 2008 Evaluation of Automatically Reformulated Questions in Question Series Richard Shaw, Ben Solway, Robert Gaizauskas and Mark A. Greenwood Department of Computer Science University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP UK {aca04rcs, aca04bs}@shef.ac.uk {r.gaizauskas, m.greenwood}@dcs.shef.ac.uk Abstract Having gold standards allows us to evaluate new methods and approaches against a common benchmark." ></td>
	<td class="line x" title="3:174	In this paper we describe a set of gold standard question reformulations and associated reformulation guidelines that we have created to support research into automatic interpretation of questions in TREC question series, where questions may refer anaphorically to the target of the series or to answers to previous questions." ></td>
	<td class="line x" title="4:174	We also assess various string comparison metrics for their utility as evaluation measures of the proximity of an automated systems reformulations to the gold standard." ></td>
	<td class="line x" title="5:174	Finally we show how we have used this approach to assess the question processing capability of our own QA system and to pinpoint areas for improvement." ></td>
	<td class="line x" title="6:174	1 Introduction The development of computational systems which can answer natural language questions using large text collections as knowledge sources is widely seen as both intellectually challenging and practically useful." ></td>
	<td class="line x" title="7:174	To stimulate research and development in this area the US National Institute of Standards and Technology (NIST) has organized a shared task evaluation as one track at the annual TExt Retrieval Conference (TREC) since 19991." ></td>
	<td class="line x" title="8:174	These evaluations began by considering factoidtype questions only (e.g. How many calories are c2008." ></td>
	<td class="line x" title="9:174	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:174	Some rights reserved." ></td>
	<td class="line x" title="11:174	1http://trec.nist.gov/ there in a Big Mac?)" ></td>
	<td class="line x" title="12:174	each of which was asked in isolation to any of the others." ></td>
	<td class="line x" title="13:174	However, in an effort to move the challenge towards a long term vision of interactive, dialogue-based question answering to support information analysts (Burger et al., 2002), the track introduced the notion of question targets and related question series in TREC2004 (Voorhees, 2005), and this approach to question presentation has remained central in each of the subsequent TRECs." ></td>
	<td class="line x" title="14:174	In this simulated task, questions are grouped into series where each series has a target of a definition associated with it (see Figure 1)." ></td>
	<td class="line x" title="15:174	Each question in the series asks for some information about the target and there is a final other question which is to be interpreted as Provide any other interesting details about the target that has not already been asked for explicitly." ></td>
	<td class="line x" title="16:174	In this way each series is a (limited) abstraction of an information dialogue in which the user is trying to define the target." ></td>
	<td class="line x" title="17:174	The target and earlier questions in a series provide the context for the current question. (Voorhees, 2005)." ></td>
	<td class="line x" title="18:174	One consequence of putting questions into series in this way is that questions may not make much sense when removed from the context their series provides." ></td>
	<td class="line x" title="19:174	For example, the question When was he born?" ></td>
	<td class="line x" title="20:174	cannot be sensibly interpreted without knowledge of the antecedent of he provided by the context (target or prior questions)." ></td>
	<td class="line x" title="21:174	Interpreting questions in question series, therefore, becomes a critical component within a QA systems." ></td>
	<td class="line x" title="22:174	Many QA systems have an initial document retrieval stage that takes the question and derives a query from it which is then passed to a search engine whose task is to retrieve candidate answering bearing documents for processing by the rest of the system." ></td>
	<td class="line x" title="23:174	Clearly a question such as When was he born?" ></td>
	<td class="line x" title="24:174	is unlikely to retrieve documents rele58 Target 136: Shiite Q136.1 Who was the first Imam of the Shiite sect of Islam?" ></td>
	<td class="line x" title="25:174	Q136.2 Where is his tomb?" ></td>
	<td class="line x" title="26:174	Q136.3 What was this persons relationship to the Prophet Mohammad?" ></td>
	<td class="line x" title="27:174	Q136.4 Who was the third Imam of Shiite Muslims?" ></td>
	<td class="line x" title="28:174	Q136.5 When did he die?" ></td>
	<td class="line x" title="29:174	Figure 1: An Example Question Series vant to answering a question about Kafkas date of birth if passed directly to a search engine." ></td>
	<td class="line x" title="30:174	This problem can be addressed in a naive way by simply appending the target to every question." ></td>
	<td class="line x" title="31:174	However, this has several disadvantages: (1) in some cases co-reference in a question series is to the answer of a previous question and not to the target, so blindly substituting the target is not appropriate; (2) some approaches to query formulation and to answer extraction from retrieved documents may require syntactically well-formed questions and may be able to take advantage of the extra information, such as syntactic dependencies, provided in a fully de-referenced, syntactically correct question." ></td>
	<td class="line x" title="32:174	Thus, it is helpful in general if systems can automatically interpret a question in context so as to resolve co-references appropriately, and indeed most TREC QA systems do this to at least a limited extent as part of their question pre-processing." ></td>
	<td class="line x" title="33:174	Ideally one would like a system to be able to reformulate a question as a human would if they were to reexpress the question so as to make it independent of the context of the preceding portion of the question series." ></td>
	<td class="line x" title="34:174	To support the development of such systems it would useful if there were a collection of gold standard reformulated questions against which systems outputs could be compared." ></td>
	<td class="line x" title="35:174	However, to the best of our knowledge no such resource exists." ></td>
	<td class="line x" title="36:174	In this paper we describe the creation of such a corpus of manually reformulated questions, measures we have investigated for comparing system generated reformulations against the gold standard, and experiments we have carried out comparing our TREC systems automatic question reformulator against the gold standard and insights we have obtained therefrom." ></td>
	<td class="line x" title="37:174	2 The Gold Standard Corpus Our aim was to take the questions in a TREC question series and re-express them as questions that would naturally be asked by a human asking them as a single, stand-alone question outside the context of the question series." ></td>
	<td class="line x" title="38:174	Our intuition was that most adult native speakers would agree on a small number of variant forms these reformulated questions would take." ></td>
	<td class="line x" title="39:174	We explored this intuition by having two persons iteratively reformulate some questions independently, compare results and evolve a small set of guidelines for the process." ></td>
	<td class="line x" title="40:174	2.1 Creating the Gold Standard Ten question sets were randomly selected from sets available at http://trec.nist.gov/ data/qa/t2007_qadata.html." ></td>
	<td class="line x" title="41:174	These were reformulated separately by two people and results compared." ></td>
	<td class="line x" title="42:174	From this an initial set of guidelines was drawn up." ></td>
	<td class="line x" title="43:174	Using these guidelines another 10 question sets from the TREC 2007 QA set were independently reformulated and then the guidelines refined." ></td>
	<td class="line x" title="44:174	At this point the reformulators outputs were sufficiently close to each other and the guidelines sufficiently stable that, given limited resources, it was decided reformulation could proceed singly." ></td>
	<td class="line x" title="45:174	Using the guidelines, therefore, a further 48 question sets from 2007 were reformulated, where this time each question set was only reformulated by a single person." ></td>
	<td class="line x" title="46:174	Each question set contained between 5 and 7 individual questions therefore around 406 questions were reformulated, creating one or more gold standard forms for each question." ></td>
	<td class="line x" title="47:174	In total there are approximately 448 individual reformulations, with a maximum number of 3 reformulations for any single question and a mean of 1.103 reformulations per question." ></td>
	<td class="line x" title="48:174	2.2 Guidelines Using the above method we derived a set of simple guidelines which anyone should be able to follow to create a set of reformulated questions." ></td>
	<td class="line x" title="49:174	Context independence and readability: The reformulation of questions should be understandable outside of the question series context." ></td>
	<td class="line x" title="50:174	The reformulation should be written as a native speaker would naturally express it; this means, for example, that stop words are included." ></td>
	<td class="line x" title="51:174	Example: How many people were killed 1991 59 eruption of Mount Pinatubo? vs How many people were killed in the 1991 eruption of Mount Pinatubo." ></td>
	<td class="line x" title="52:174	The latter is preferred as it more readable due to the inclusion of stop words in the." ></td>
	<td class="line x" title="53:174	Reformulate questions so as to maximise search results: Example: Who was William Shakespeare? vs Who was Shakespeare?." ></td>
	<td class="line x" title="54:174	William should be added to the phrase as it adds extra information which could allow more results to be found." ></td>
	<td class="line x" title="55:174	Target matches a sub-string of the question: If the target string matches a sub-string of the question the target string should substitute the entirety of the substring." ></td>
	<td class="line x" title="56:174	Stop-words should not be used when determining if strings and target match but should usually be substituted along with the rest of the target." ></td>
	<td class="line x" title="57:174	Example: Target: Sony Pictures Entertainment (SPE); Question: What U.S. company did Sony purchase to form SPE?; Gold Standard: What U.S. company did Sony purchase to form Sony Pictures Entertainment (SPE)? Rephrasing: A Question should not be unnecessarily rephrased." ></td>
	<td class="line x" title="58:174	Example: Target: Nissan Corp; Question: What was Nissan formerly known as?; What was Nissan Corp. formerly known as? is preferred over the other possible reformulation Nissan Corp. was formerly known as what?." ></td>
	<td class="line x" title="59:174	Previous Questions and Answers: Questions which include a reference to a previous question should be reformulated to include a PREVIOUS ANSWER variable." ></td>
	<td class="line x" title="60:174	Another reformulation should also be provided should a system know it needs the answer to the previous question but has not found one." ></td>
	<td class="line x" title="61:174	This should be a reformulation of the previous question within the current question." ></td>
	<td class="line x" title="62:174	Example: Target: Harriet Miers withdraws nomination to Supreme Court; Question: What criterion did this person cite in nominating Miers?; Gold Standard 1: What criterion did PREVIOUS ANSWER cite in nominating Harriet Miers?; Gold Standard 2: What criterion did this person who nominated Harriet Miers for the post cite in nominating Harriet Miers? Targets that contain brackets: Brackets in target should be dealt with in the following way." ></td>
	<td class="line x" title="63:174	The full target should be substituted into the question in the correct place as one of the Gold Standards." ></td>
	<td class="line x" title="64:174	The target without the bracketed word and with it should also be included in the Gold Standard." ></td>
	<td class="line x" title="65:174	Example: Target: Church of Jesus Christ of Latter-day Saints (Mormons); Question:Who founded the Church of Jesus Christ of Latter-day Saints?; Gold Standard 1: Who founded the Church of Jesus Christ of Latter-day Saints (Mormons)?; Gold Standard 2: Who founded the Church of Jesus Christ of Latter-day Saints?; Gold Standard 3 Who founded the Mormons? Stemming and Synonyms: Words should not be stemmed and synonyms should not be used unless they are found in the target or the current question series." ></td>
	<td class="line x" title="66:174	If they are found then both should be used in the Gold Standard." ></td>
	<td class="line x" title="67:174	Example: Target: Chunnel; Question:How long is the Chunnel?; Gold Standard: How long is the Chunnel?; Incorrect reformulation: How long is the Channel Tunnel? As the term Channel Tunnel is not referenced in this section or hard-coded into the QA engine it cannot be substituted for Chunnel, even though doing so may increase the probability of finding the correct answer." ></td>
	<td class="line x" title="68:174	It: The word it should be interpreted as referring to either the answer of the previous question of that set or if no answer available to the target itself." ></td>
	<td class="line x" title="69:174	Example:Target: 1980 Mount St. Helens eruption; Question: How many people died when it erupted?; Gold Standard: How many people died when Mt. St. Helens erupted in 1980? Pronouns (1): If the pronouns he or she are used within a question and the TARGET is of type Person then substitute the TARGET string for the pronoun." ></td>
	<td class="line x" title="70:174	If however the PREVIOUS ANSWER is of type Person then it should be substituted instead as in this case the natural interpretation of the pronoun is to the answer of the previous question." ></td>
	<td class="line x" title="71:174	Example: Target: Jay-Z; Question: When was he born?; Gold Standard: When was Jay-Z born? Pronouns (2): If the pronouns his/hers/their are used within a question and the TARGET is of type Person then substitute the TARGET string for the pronoun appending the string s to the end of the substitution." ></td>
	<td class="line x" title="72:174	If however the PREVIOUS ANSWER is of type Person then it should be substituted as the natural interpretation of the pronoun is to the answer of the previous question." ></td>
	<td class="line x" title="73:174	Example: Target: Jasper Fforde; Question: What year was his first book written?; Gold Standard: What year was Jasper Ffordes first book written? 60 3 Evaluation against the Gold Standard To assess how close a systems reformulation of a question in a questions series is to the gold standard requires a measure of proximity." ></td>
	<td class="line x" title="74:174	Whatever metric we adopt should have the property that reformulations that are closer to our gold standard reformulations get a higher score." ></td>
	<td class="line x" title="75:174	The closest possible score is achieved by getting an identical string to that of the gold standard." ></td>
	<td class="line x" title="76:174	Following conventional practice we will adopt a metric that gives us a value between 0 and 1, where 1 is highest (i.e. a score of 1 is achieved when the pre-processed reformulation and the gold standard are identical)." ></td>
	<td class="line x" title="77:174	Another requirement for the metric is that the ordering of the words in the reformulation is not as important as the content of the reformulation." ></td>
	<td class="line x" title="78:174	We assume this because one key use for reformulated questions in the retrieval of candidate answer bearing documents and the presence of key content terms in a reformulation can help to find answers when it is used as a query, regardless of their order Ordering does still need to be taken into account by the metric but it should alter the score less than the content words in the reformulation." ></td>
	<td class="line x" title="79:174	Related to this point, is that we would like reformulations that simply append the target onto the end of the original question to score more highly on average than the original questions on their own, since this is a default strategy followed by many systems that clearly helps in many cases." ></td>
	<td class="line x" title="80:174	These requirement can help to guide metric selection." ></td>
	<td class="line x" title="81:174	3.1 Choosing a metric There are many different systems which attempt to measure string similarity." ></td>
	<td class="line nc" title="82:174	We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task." ></td>
	<td class="line o" title="83:174	ROUGE and METEOR were developed to compare larger stretches of text  they are usually used to compare paragraphs rather than sentences." ></td>
	<td class="line n" title="84:174	We decided developing our own metric would be simpler than trying to adapt one of these existing tools." ></td>
	<td class="line x" title="85:174	To explore candidate similarity measures we created a program which would take as input a list of reformulations to be assessed and a list of gold standard reformulations and compare them to each other using a selection of different string comparison metrics." ></td>
	<td class="line x" title="86:174	To find out which of these metrics best scored reformulations in the way which we expected, we created a set of test reformulations to compare against the gold standard reformulations." ></td>
	<td class="line x" title="87:174	Three test data sets were created: one where the reformulation was simply the original question, one where the reformulation included the target appended to the end, and one where the reformualation was identical to the gold standard." ></td>
	<td class="line x" title="88:174	The idea here was that the without target question set should score less than the with target question set and the identical target question set should have a score of 1 (the highest possible score)." ></td>
	<td class="line x" title="89:174	We then had to choose a set of metrics to test and chose to use metrics from the SimMetrics library as it is an open source extensible library of string similarity and distance metrics 2." ></td>
	<td class="line x" title="90:174	3.2 Assessing Metrics After running the three input files against the metrics we could see that certain metrics gave a score which matched our requirements more closely than others." ></td>
	<td class="line x" title="91:174	Table 1 shows the metrics used and the mean scores across the data set for the different question sets." ></td>
	<td class="line x" title="92:174	A description of each of these metrics can be found in the SimMetrics library." ></td>
	<td class="line x" title="93:174	From these results we can see that certain metrics are not appropriate." ></td>
	<td class="line x" title="94:174	SmithWaterman, Jaro and JaroWinkler all do the opposite to what we require them to do in that they score a reformulation without the target higher than one with the target." ></td>
	<td class="line x" title="95:174	This could be due to over-emphasis on word ordering." ></td>
	<td class="line x" title="96:174	These metrics can therefore be discounted." ></td>
	<td class="line x" title="97:174	Levenshtein, NeedlemanWunch and QGramsDistance can also be discounted as the difference between With target and Without target is not large enough." ></td>
	<td class="line x" title="98:174	It would be difficult to measure improvements in the system if the difference is this small." ></td>
	<td class="line x" title="99:174	MongeElkan can also be discounted as overall its scores are too large and for this reason it would be difficult to measure improvements using it." ></td>
	<td class="line x" title="100:174	Of the five remaining metrics  DiceSimilarity, JaccardSimilarity, BlockDistance, EuclideanDistance and CosineSimilarity  we decided that we should discount EuclideanDistance as it had the smallest gap between with target and without target." ></td>
	<td class="line x" title="101:174	We now look at the other four metrics in more detail3: 2http://www.dcs.shef.ac.uk/sam/ simmetrics.html 3Refer to Manning and Schutze (2001) for more details on these algorithms." ></td>
	<td class="line x" title="102:174	61 Metric Without Target With Target Identical JaccardSim." ></td>
	<td class="line x" title="103:174	0.798 0.911 1.0 DiceSim." ></td>
	<td class="line x" title="104:174	0.872 0.948 1.0 CosineSim." ></td>
	<td class="line x" title="105:174	0.878 0.949 1.0 BlockDistance 0.869 0.941 1.0 EuclideanDistance 0.902 0.950 1.0 MongeElkan 0.922 0.993 1.0 Levenshtein 0.811 0.795 1.0 NeedlemanWunch 0.830 0.839 1.0 SmithWaterman 0.915 0.859 1.0 QGramsDistance 0.856 0.908 1.0 JaroWinkler 0.855 0.831 0.993 Jaro 0.644 0.589 0.984 Table 1: Mean scores across the data set for each of the different question sets." ></td>
	<td class="line x" title="106:174	3.2.1 Block Distance Block Distance metric is variously named block distance, L1 distance or city block distance." ></td>
	<td class="line x" title="107:174	It is a vector-based approach, where q and r are defined in n-dimensional vector space." ></td>
	<td class="line x" title="108:174	The L1 or block distance is calculated from summing the edge distances." ></td>
	<td class="line x" title="109:174	L1(q,r) = summationdisplay y | q(y)  r(y)| This can be described in two dimensions with discrete-valued vectors." ></td>
	<td class="line x" title="110:174	When we can picture the set of points within a grid, the distance value is simply the number of edges between points that must be traversed to get from q to r within the grid." ></td>
	<td class="line x" title="111:174	This is the same problem as getting from corner a to b in a rectilinear street map, hence the name city-block metric." ></td>
	<td class="line x" title="112:174	3.2.2 Dice Similarity This is based on Dice coefficient which is a term based similarity measure (0-1) whereby the similarity measure is defined as twice the number of terms common to compared entities divided by the total number of terms in both." ></td>
	<td class="line x" title="113:174	A coefficient result of 1 indicates identical vectors while a 0 indicates orthogonal vectors." ></td>
	<td class="line x" title="114:174	Dice Coefficient = 2  |S1  S2||S 1| + |S2| 3.2.3 Jaccard Similarity This is a token based vector space similarity measure like the cosine distance." ></td>
	<td class="line x" title="115:174	Jaccard Similarity uses word sets from the comparison instances to evaluate similarity." ></td>
	<td class="line x" title="116:174	The Jaccard measure penalizes a small number of shared entries (as a portion of all non-zero entries) more than the Dice coefficient." ></td>
	<td class="line x" title="117:174	Each instance is represented as a Jaccard vector similarity function." ></td>
	<td class="line x" title="118:174	The Jaccard similarity between two vectors X and Y is (X Y )/(|X||Y |(X Y )) where (X Y ) is the inner product of X and Y , and |X| = (X  X)1/2, i.e. the Euclidean norm of X. This can more easily be described as (|X  Y |)/(|X  Y |) 3.2.4 Cosine similarity This is a common vector based similarity measure similar to the Dice Coefficient." ></td>
	<td class="line x" title="119:174	The input string is transformed into vector space so that the Euclidean cosine rule can be used to determine similarity." ></td>
	<td class="line x" title="120:174	The cosine similarity is often paired with other approaches to limit the dimensionality of the problem." ></td>
	<td class="line x" title="121:174	For instance with simple strings a list of stopwords is used to reduce the dimensionality of the comparison." ></td>
	<td class="line x" title="122:174	In theory this problem has as many dimensions as terms exist." ></td>
	<td class="line x" title="123:174	cos(q,r) = summationtext y q(y)r(y)radicalBigsummationtext y q(y)2 summationtext y r(y)2 3.3 Using bigrams and trigrams All four of these measures appear to value the content of the strings higher than ordering which is what we want our metric to do." ></td>
	<td class="line x" title="124:174	However the scores are quite large, and as a result we considered refining the metrics to give scores that are not as close to 1." ></td>
	<td class="line x" title="125:174	To do this we decided to try and increase the importance of ordering by also taking into account shared bigrams and trigrams." ></td>
	<td class="line x" title="126:174	As we do not want ordering to be too important in our metric we introduced a weighting mechanism into the program to 62 Metric Without Target With Target Gap Dice 0.872 0.948 +0.076 Cosine 0.878 0.949 +0.071 Jaccard 0.798 0.911 +0.113 Block 0.869 0.941 +0.072 Table 2: Results for Unigram weighting Metric Without Target With Target Gap Dice 0.783 0.814 -3.6 Cosine 0.789 0.816 -3.5 Jaccard 0.698 0.748 -5.5 Block 0.782 0.811 -3.5 Table 3: U:1, B:1, T:0 allow us to used a weighted combination of shared unigrams, bigrams and trigrams." ></td>
	<td class="line x" title="127:174	The results for just unigram weighting is shown in Table 2." ></td>
	<td class="line x" title="128:174	We began by testing the metrics by introducing just bigrams to give us an idea of what effect they would have." ></td>
	<td class="line x" title="129:174	A weight ratio of U:1, B:1, T:0 was used (where U:unigram, B:bigram, T:trigram)." ></td>
	<td class="line x" title="130:174	The results are shown in Table 3." ></td>
	<td class="line x" title="131:174	The  Gap column is the increase in the difference between Without Target and With Target from the first test run which used only unigrams." ></td>
	<td class="line x" title="132:174	The introduction of bigrams decreases the gap between Without Target and With Target." ></td>
	<td class="line x" title="133:174	It also lowers the scores which is good as it is then easier to distinguish between perfect reformulations and reformulations which are close but not perfect." ></td>
	<td class="line x" title="134:174	This means that the introduction of bigrams is always going to decrease a systems ability to distinguish between Without Target and With Target." ></td>
	<td class="line x" title="135:174	We had to now find the lowest decrease in this gap whilst still lowering the score of the with target result." ></td>
	<td class="line x" title="136:174	From the results of the bigrams we expected that the introduction of trigrams would further decrease the gap (U : 1,B : 1,T : 1)." ></td>
	<td class="line x" title="137:174	The results proved Metric Without Target With Target Gap Dice 0.725 0.735 -6.4 Cosine 0.730 0.735 -6.3 Jaccard 0.639 0.663 -9.0 Block 0.724 0.733 -6.1 Table 4: U:1, B:1, T:1 Metric Without Target With Target Gap Dice 0.754 0.770 -4.8 Cosine 0.759 0.771 -4.9 Jaccard 0.664 0.694 -7.4 Block 0.753 0.767 -4.6 Table 5: U:1, B:2 Metric Without Target With Target Gap Dice 0.813 0.859 -2.4 Cosine 0.819 0.860 -2.4 Jaccard 0.731 0.802 -3.7 Block 0.811 0.854 -2.2 Table 6: U:2, B:1 this and are shown in Table 4." ></td>
	<td class="line x" title="138:174	The introduction of trigrams has caused the gaps to significantly drop." ></td>
	<td class="line x" title="139:174	It has also lowered the scores too much." ></td>
	<td class="line x" title="140:174	From this evidence we decided trigrams are not appropriate to use to refine these metrics." ></td>
	<td class="line x" title="141:174	We now had to try and find the best weighting of unigram to bigram that would lower the With Target score from 1.0 whilst still keeping the gap between Without Target and With Target high." ></td>
	<td class="line x" title="142:174	We would expect that further increasing the bigram weighting would further decrease the gap and the With Target score." ></td>
	<td class="line x" title="143:174	The results in Table 5 show this to be the case." ></td>
	<td class="line x" title="144:174	However this has decreased the gap too much." ></td>
	<td class="line x" title="145:174	The next step was to look at decreasing the weighting of the bigrams." ></td>
	<td class="line x" title="146:174	Table 6 shows that the gap has decreased slightly but the With Target score has decreased by around 10% on average." ></td>
	<td class="line x" title="147:174	The Jaccard score for this run is particularly good as it has a good gap and is not too close to 1.0." ></td>
	<td class="line x" title="148:174	The Without Target is also quite low which is what we want." ></td>
	<td class="line x" title="149:174	U : 2,B : 1 is currently the best weighting found with the best metric being Jaccard." ></td>
	<td class="line x" title="150:174	Further work in this area could be directed at further modifying these weightings using machine learning techniques to refine the weightings using linear regression." ></td>
	<td class="line x" title="151:174	4 Our system against the Metric Our current pre-processing system takes a question and its target and looks to replace pronouns like he, she and certain definite nominals with the target and also to replace parts of the target with the full target (Gaizauskas et al., 2005)." ></td>
	<td class="line x" title="152:174	Given our choice of metric we would hope that this strat63 Figure 2: Graph of Jaccard score distribution egy gets a better score than just adding the target on the end, as the ordering of the words is also taken into account by our pre-processing as it tries to achieve natural reformulations like those of our gold standard." ></td>
	<td class="line x" title="153:174	We would therefore expect that it achieves at least the same score as adding the target on the end, which is its default strategy when no co-reference can be determined, though of course incorrect coreference resolutions will have a negative effect." ></td>
	<td class="line x" title="154:174	One of the aims of creating the gold standard and a comparison metric was to quickly identify whether strategies such as ours are working and if not where not." ></td>
	<td class="line x" title="155:174	A subset of the gold standard was preprocessed by our system then compared against the results of doing no reformulation and of reformulating by simply appending the target." ></td>
	<td class="line x" title="156:174	Tables 7 and 8 shows how our system did in comparison." ></td>
	<td class="line x" title="157:174	Diff shows the difference between WithTarget and Our System." ></td>
	<td class="line x" title="158:174	Table 7 is results for weighting U : 1,B : 0,T : 0, Table 8 is results for U : 2,B : 1,T : 0." ></td>
	<td class="line x" title="159:174	Our system does do better than just adding the target on the end, and this difference is exaggerated (Table 8) when bigrams are taken into account, as expected since this weighting increases the metrics sensitivity to recognising our systems ability to put the target in the correct place." ></td>
	<td class="line x" title="160:174	Mean scores across a data set tell part of the story, but to gain more insight we need to examine the distribution of scores and then, in order to improve the system, we need to look at questions which have a low score and work out what has gone wrong." ></td>
	<td class="line x" title="161:174	Figure 2 shows the distribution of Jaccard scores across the test data set." ></td>
	<td class="line x" title="162:174	Looking at the scores from the data set using the U:2,B:1,T:0 weighting we find that the minimum Jaccard score was 0.44 and was for the following example: Metric Score Dice 0.574 Cosine 0.578 Jaccard 0.441 Block 0.574 Table 9: Finding Bad Reformulations Target: Hindenburg disaster; Question: How many of them were killed; Our System: How many of Hindenburg disaster were killed; Gold Standard: How many people were killed during the Hindenburg disaster." ></td>
	<td class="line x" title="163:174	The results of comparing our system with the gold standard for this question for all four metrics are shown in Table 9." ></td>
	<td class="line x" title="164:174	The problem here is that our system has wrongly replaced the term them with the target when in fact its antecedent was in the previous question in the series How many people were on board?." ></td>
	<td class="line x" title="165:174	Once again the low score has helped us to quickly identify a problem: the system is only interpreting pronouns as references to the target, which is clearly insufficient." ></td>
	<td class="line x" title="166:174	Furthermore should the preprocessing system be altered to address a problem like this the gold system and scoring software can be used for regression testing to ensure no previously correct reformulations have been lost." ></td>
	<td class="line x" title="167:174	Another example of a poor scoring reformulation is: Target: Hindenburg disaster; Question: What type of craft was the Hindenburg; Our System: What type of craft was the Hindenburg disaster; Gold Standard: What type of craft was the Hindenburg." ></td>
	<td class="line x" title="168:174	For this example Jaccard gave our system reformulation a score of 0.61." ></td>
	<td class="line x" title="169:174	The problem here is our system blindly expanded a substring of the target appearing in the question to the full target without recognizing that in this case the substring is not an abbreviated reference to the target (an event) but to an entity that figured in the event." ></td>
	<td class="line x" title="170:174	5 Conclusions and Future Work In this paper we have presented a Gold Standard for question reformulation and an associated set of guidelines which can be used to reformulate other questions in a similar fashion." ></td>
	<td class="line x" title="171:174	We then evaluated metrics which can be used to assess the effectiveness of the reformulations and validated the whole approach by showing how it could be used to help 64 Metric Without Target With Target Our System Diff Dice 0.776 0.901 0.931 +3.1 Cosine 0.786 0.904 0.936 +3.1 Jaccard 0.657 0.834 0.890 +5.5 Block 0.772 0.888 0.920 +4.2 Table 7: How our system compared, U:1,B:0,T:0 Metric Without Target With Target Our System Diff Dice 0.702 0.819 0.889 +8.7 Cosine 0.742 0.822 0.893 +9.2 Jaccard 0.616 0.738 0.839 +12.3 Block 0.732 0.812 0.884 +9.1 Table 8: How our system compared, U:2,B:1,T:0 improve the question pre-processing component of a QA system." ></td>
	<td class="line x" title="172:174	Further work will aim to expand the Gold Standard to at least 1000 questions, refining the guidelines as required." ></td>
	<td class="line x" title="173:174	The eventual goal is to incorporate the approach into an evaluation tool such that a developer would have a convenient way of evaluating any question reformulation strategy against a large gold standard." ></td>
	<td class="line x" title="174:174	Of course one also needs to develop methods for observing and measuring the effect of question reformulation within question pre-processing upon the performance of downstream components in the QA system, such as document retrieval." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:91	Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 5356 Manchester, August 2008 Concept-graph based Biomedical Automatic Summarization using Ontologies Laura Plaza Morales Alberto Daz Esteban Pablo Gervas Universidad Complutense de Madrid C/Profesor Jose Garca Santesmases, s/n, Madrid 28040, Spain lplazam@pas.ucm.es, albertodiaz@fdi.ucm.es,pgervas@sip.ucm.es Abstract One of the main problems in research on automatic summarization is the inaccurate semantic interpretation of the source." ></td>
	<td class="line x" title="2:91	Using specific domain knowledge can considerably alleviate the problem." ></td>
	<td class="line x" title="3:91	In this paper, we introduce an ontology-based extractive method for summarization." ></td>
	<td class="line x" title="4:91	It is based on mapping the text to concepts and representing the document and its sentences as graphs." ></td>
	<td class="line x" title="5:91	We have applied our approach to summarize biomedical literature, taking advantages of free resources as UMLS." ></td>
	<td class="line x" title="6:91	Preliminary empirical results are presented and pending problems are identified." ></td>
	<td class="line x" title="7:91	1 Introduction In recent years, the amount of electronic biomedical literature has increased explosively." ></td>
	<td class="line x" title="8:91	Physicians and researchers constantly have to consult up-to date information according to their needs, but the process is time-consuming." ></td>
	<td class="line x" title="9:91	In order to tackle this overload of information, text summarization can undoubtedly play a role." ></td>
	<td class="line x" title="10:91	Simultaneously, a big deal of resources, such as biomedical terminologies and ontologies, have emerged." ></td>
	<td class="line x" title="11:91	They can significantly benefit the development of NLP systems, and in particular, when used in automatic summarization, they can increase the quality of summaries." ></td>
	<td class="line x" title="12:91	In this paper, we present an ontology-based extractive method for the summarization of biomedical literature, based on mapping the text to concepts in UMLS and representing the document and its sentences as graphs." ></td>
	<td class="line x" title="13:91	To assess the importance c2008." ></td>
	<td class="line x" title="14:91	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="15:91	Some rights reserved." ></td>
	<td class="line x" title="16:91	of the sentences, we compute the centrality of their concepts in the document graph." ></td>
	<td class="line x" title="17:91	2 Previous Work Traditionally, automatic summarization methods have been classified in those which generate extracts and those which generate abstracts." ></td>
	<td class="line x" title="18:91	Although human summaries are typically abstracts, most of existing systems produce extracts." ></td>
	<td class="line x" title="19:91	Extractive methods build summaries on a superficial analysis of the source." ></td>
	<td class="line x" title="20:91	Early summarization systems are based on simple heuristic features, as the position of sentences in the document (Brandow et al., 1995), the frequency of the words they contain (Luhn, 1958; Edmundson, 1969), or the presence of certain cue words or indicative phrases (Edmundson, 1969)." ></td>
	<td class="line x" title="21:91	Some advanced approaches also employ machine learning techniques to determine the best set of attributes for extraction (Kupiec et al., 1995)." ></td>
	<td class="line x" title="22:91	Recently, several graph-based methods have been proposed to rank sentences for extraction." ></td>
	<td class="line x" title="23:91	LexRank (Erkan and Radev, 2004) is an example of a centroidbased method to multi-document summarization that assess sentence importance based on the concept of eigenvector centrality." ></td>
	<td class="line x" title="24:91	It represents the sentences in each document by its tf*idf vectors and computes sentence connectivity using the cosine similarity." ></td>
	<td class="line x" title="25:91	Even if results are promising, most of these approaches exhibit important deficiencies which are consequences of not capturing the semantic relations between terms (synonymy, hyperonymy, homonymy, and co-occurs and associatedwith relations)." ></td>
	<td class="line x" title="26:91	Wepresentanextractivemethodforsummarization which attempts to solve this deficiencies." ></td>
	<td class="line x" title="27:91	Unlike researches conducted by (Yoo et al., 2007; Erkan and Radev, 2004), which cluster sentences to identify shared topics in multiple documents, in this work we apply clustering to identify groups 53 of concepts closely related." ></td>
	<td class="line x" title="28:91	We hypothesize that each cluster represents a theme or topic in the document, and we evaluate three different heuristics to ranking sentences." ></td>
	<td class="line x" title="29:91	3 Biomedical Ontologies." ></td>
	<td class="line x" title="30:91	UMLS Biomedical ontologies organize domain concepts and knowledge in a system of hierarchical and associative relations." ></td>
	<td class="line x" title="31:91	One of the most widespread in NLP applications is UMLS1 (Unified Medical Language System)." ></td>
	<td class="line x" title="32:91	UMLS consists of three components: the Metathesaurus, a collection of concepts and terms from various vocabularies and their relationships; the Semantic Network, a set of categories and relations used to classify and relate the entries in the Metathesaurus; and the Specialist Lexicon, a database of lexicographic information for use in NLP." ></td>
	<td class="line x" title="33:91	In this work, we have selected UMLS for several reasons." ></td>
	<td class="line x" title="34:91	First, it provides a mapping structure between different terminologies, including MeSH or SNOMED, and thus allows to translate between them." ></td>
	<td class="line x" title="35:91	Secondly, it contains vocabularies in various languages, which allows to process multilingual information." ></td>
	<td class="line x" title="36:91	4 Summarization Method The method proposed consists of three steps." ></td>
	<td class="line x" title="37:91	Each step is discussed in detail below." ></td>
	<td class="line x" title="38:91	A preliminary systemhasbeenimplementedandtestedonseveral documents from the corpus developed by BioMed Central2." ></td>
	<td class="line x" title="39:91	As the preprocessing, text is split into sentences using GATE3, and generic words and high frequency terms are removed, as they are not useful in discriminating between relevant and irrelevant sentences." ></td>
	<td class="line x" title="40:91	4.1 Graph-based Document Representation This step consists in representing each document as a graph, where the vertices are the concepts in UMLS associated to the terms, and the edges indicate the relations between them." ></td>
	<td class="line x" title="41:91	Firstly, each sentenceismappedtotheUMLSMetathesaurususing MetaMap (Aronson, 2001)." ></td>
	<td class="line x" title="42:91	MetaMap allows to map terms to UMLS concepts, using n-grams for indexing in the ULMS Metathesaurus, and performing disambiguation to identify the correct 1NLM Unified Medical Language System (UMLS)." ></td>
	<td class="line x" title="43:91	URL: http://www.nlm.nih.gov/research/umls 2BioMed Central: http://www.biomedcentral.com/ 3GATE (Generic Architecture for Text Engineering): http://gate.ac.uk/ concept for a term." ></td>
	<td class="line x" title="44:91	Secondly, the UMLS concepts are extended with their hyperonyms." ></td>
	<td class="line x" title="45:91	Figure 1 shows the graph for sentence The goal of the trial was to assess cardiovascular mortality and morbidity for stroke, coronary heart disease and congestive heart failure, as an evidence-based guide for clinicians who treat hypertension. Next, each edge is assigned a weight, which is directly proportional to the deep in the hierarchy at which the concepts lies (Figure 1)." ></td>
	<td class="line x" title="46:91	That is to say, the more specific the concepts connected are, the more weight is assigned to them." ></td>
	<td class="line x" title="47:91	Expression (1) shows how these values are computed." ></td>
	<td class="line x" title="48:91	|  | |  | = || || (1) where  is the set of all the parents of a concept, including the concept, and  is the set of all the parents of its immediate higher-level concept, including the concept." ></td>
	<td class="line x" title="49:91	Finally, the sentence graphs are merged into a document graph, enriched with the associatedwith relations between the semantic types in UMLS corresponding to the concepts (Figure 1)." ></td>
	<td class="line x" title="50:91	Weights for the new edges are computed using expression (1)." ></td>
	<td class="line x" title="51:91	4.2 Concept Clustering and Theme Recognition The second step consists of clustering concepts in the document graph, using a degree-based method (Erkan and Radev, 2004)." ></td>
	<td class="line x" title="52:91	Each cluster is composed by a set of concepts that are closely related in meaning, and can be seen as a theme in the document." ></td>
	<td class="line x" title="53:91	The most central concepts in the cluster give the sufficient and necessary information related to its theme." ></td>
	<td class="line x" title="54:91	We hypothesize that the document graph is an instance of a scale-free network (Barabasi, 1999)." ></td>
	<td class="line x" title="55:91	Following (Yoo et al., 2007), we introduce the salience of vertices." ></td>
	<td class="line x" title="56:91	Mathematically, the salience of a vertex (vi) is calculated as follows." ></td>
	<td class="line x" title="57:91	salience(vi) = summationdisplay ej|vkejconecta(vi,vk) weight(ej) (2) Within the set of vertices, we select the n that present the higher salience and iteratively group them in Hub Vertex Sets (HVS)." ></td>
	<td class="line x" title="58:91	A HVS represents a group of vertices that are strongly related to each other." ></td>
	<td class="line x" title="59:91	The remaining vertices are 54 Figure 1: Sentence graph assigned to that cluster to which they are more connected." ></td>
	<td class="line x" title="60:91	Finally, we assign each sentence to a cluster." ></td>
	<td class="line x" title="61:91	To measure the similarity between a cluster and a sentence graph, we use a vote mechanism (Yoo et al., 2007)." ></td>
	<td class="line x" title="62:91	Each vertex (vk) of a sentence (Oj) gives to each cluster (Ci) a different number of votes (pi,j) depending on whether the vertex belongs to HVS or non-HVS (3)." ></td>
	<td class="line x" title="63:91	similarity(Ci,Oj) = summationdisplay vk|vkOj wk,j (3) where braceleftBigg wk,j=0sivknegationslashCi wk,j=1.0,sivkHVS(Ci) wk,j=0.5,sivknegationslashHVS(Ci) 4.3 Sentence Selection The last step consists of selecting significant sentences for the summary, based on the similarity between sentences and clusters." ></td>
	<td class="line x" title="64:91	We investigated three alternatives for this step." ></td>
	<td class="line x" title="65:91	 Heuristic 1: For each cluster, the top ni sentences are selected, where ni is proportional to its size." ></td>
	<td class="line x" title="66:91	 Heuristic 2: We accept the hypothesis that the cluster with more concepts represents the main theme in the document, and select the top N sentences from this cluster." ></td>
	<td class="line x" title="67:91	 Heuristic 3: We compute a single score for each sentence, as the sum of the votes assigned to each cluster adjusted to their sizes, andselecttheN sentenceswithhigherscores." ></td>
	<td class="line x" title="68:91	5 Results and Evaluation In order to evaluate the method, we analyze the summaries generated by the three heuristics over a document4 from the BioMed Central Corpus, using a compression rate of 20%." ></td>
	<td class="line x" title="69:91	Table 1 shows the sentences selected along with their scores." ></td>
	<td class="line x" title="70:91	Although results are not statistically significant, they show some aspects in which our method behaves satisfactorily." ></td>
	<td class="line x" title="71:91	Heuristics 1 and 3 extract sentence 0, and assign to it the higher score." ></td>
	<td class="line x" title="72:91	This supports the positional criterion of selecting the first sentence in the document, as the one that contains the most significant information." ></td>
	<td class="line x" title="73:91	Sentence 58 represents an example of sentence, situated at the end, which gathers the conclusions of the author." ></td>
	<td class="line x" title="74:91	In general, these sentences are highly informative." ></td>
	<td class="line x" title="75:91	Sentence 19, in turn, evidences how the method systematically gives preference to long sentences." ></td>
	<td class="line x" title="76:91	Moreover, while summaries by heuristics 1 and 3 have a lot of sentences in common (9 out of 12), heuristic 2 generates a summary considerably different and ignores important topics in the document." ></td>
	<td class="line x" title="77:91	Finally, we have compared these summaries with the authors abstract." ></td>
	<td class="line x" title="78:91	It can be observed that heuristics 1 and 3 cover all topics in the authors abstract (see sentences 0, 4, 15, 17, 19, 20 and 25)." ></td>
	<td class="line x" title="79:91	4BioMed Central: www.biomedcentral.com/content/ download/xml/cvm-2-6-254.xml 55 Sentences 0 4 19 58 7 28 25 20 21 8 43 15 Heuristic 1 99.0 20.0 19.0 18.5 17.0 16.5 16.0 15.5 15.5 13.5 13.5 12.0 Heuristic 2 19.0 16.5 15.5 12.5 12.0 10.5 9.0 9.0 7.5 7.0 7.0 7.0 Heuristic 3 98.8 18.7 17.9 16.3 15.3 14.5 13.4 13.0 13.0 12.7 12.7 12.2 Table 1: Results As far as heuristic 2 is concerned, it does not cover adequately the information in the abstract." ></td>
	<td class="line x" title="80:91	6 Conclusions and Future Work In this paper we introduce a method for summarizing biomedical literature." ></td>
	<td class="line x" title="81:91	We represent the document as an ontology-enriched scale-free graph, using UMLS concepts and relations." ></td>
	<td class="line x" title="82:91	This way we getaricherrepresentationthantheoneprovidedby a vector space model." ></td>
	<td class="line x" title="83:91	In section 5 we have evaluated several heuristics for sentence extraction." ></td>
	<td class="line x" title="84:91	We have determined that heuristic 2 does not cover all relevanttopicsandselectssentenceswithalowrelative significance." ></td>
	<td class="line x" title="85:91	Conversely, heuristics 1 and 3, present very similar results and cover all important topics." ></td>
	<td class="line x" title="86:91	Nonetheless, we have identified several problems and some possible improvements." ></td>
	<td class="line x" title="87:91	Firstly, as our method extracts whole sentences, long ones have higher probability to be selected, because they contain more concepts." ></td>
	<td class="line x" title="88:91	The alternative could betonormalise thesentencesscoresby thenumber of concepts." ></td>
	<td class="line x" title="89:91	Secondly, concepts associated with general semantic types in UMLS, as functional concept, temporal concept, entity and language, could be ignored, since they do not contribute to distinguish what sentences are significant." ></td>
	<td class="line oc" title="90:91	Finally, in order to formally evaluate the method and the different heuristics, a large-scale evaluation on the BioMed Corpus is under way, based on computing the ROUGE measures (Lin, 2004)." ></td>
	<td class="line x" title="91:91	Acknowledgements This research is funded by the Ministerio de Educacion y Ciencia (TIN2006-14433-C02-01), Universidad Complutense de Madrid and Direccion GeneraldeUniversidadeseInvestigaciondelaComunidad de Madrid (CCG07-UCM/TIC 2803)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1132
Finding Short Definitions of Terms on Web Pages
Lampouras, Gerasimos;Androutsopoulos, Ion;"></td>
	<td class="line x" title="1:255	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 12701279, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:255	c 2009 ACL and AFNLP Finding Short Definitions of Terms on Web Pages Gerasimos Lampouras and Ion Androutsopoulos+ Department of Informatics, Athens University of Economics and Business, Greece +Digital Curation Unit, Research Centre Athena, Athens, Greece Abstract We present a system that finds short definitions of terms on Web pages." ></td>
	<td class="line x" title="3:255	It employs a Maximum Entropy classifier, but it is trained on automatically generated examples; hence, it is in effect unsupervised." ></td>
	<td class="line x" title="4:255	We use ROUGE-W to generate training examples from encyclopedias and Web snippets, a method that outperforms an alternative centroid-based one." ></td>
	<td class="line x" title="5:255	After training, our system can be used to find definitions of terms that are not covered by encyclopedias." ></td>
	<td class="line x" title="6:255	The system outperforms a comparable publicly available system, as well as apreviouslypublishedformofoursystem." ></td>
	<td class="line x" title="7:255	1 Introduction Definitions of terms are among the most common types of information users search for on the Web." ></td>
	<td class="line x" title="8:255	In the TREC 2001 QA track (Voorhees, 2001), where the distribution of question types reflected that of real user logs, 27% of the questions were requests for definitions (e.g., What is gasohol?, Who was Duke Ellington?)." ></td>
	<td class="line x" title="9:255	Consequently, some Web search engines provide special facilities (e.g., Googles define: query prefix) that seek definitions of user-specified terms in online encyclopedias or glossaries; to save space, we call both encyclopedias." ></td>
	<td class="line x" title="10:255	There are, however, oftentermsthataretoorecent, tooold,orlesswidely used to be included in encyclopedias." ></td>
	<td class="line x" title="11:255	Their definitions may be present on other Web pages (e.g., newspaper articles), but they may be provided indirectly (e.g., He said that gasohol, a mixture of gasoline and ethanol, has been great for his business.) and they may be difficult to locate with generic search engines that may return dozens of pages containing, but not defining the terms." ></td>
	<td class="line x" title="12:255	We present a system to find short definitions of user-specified terms on Web pages." ></td>
	<td class="line x" title="13:255	It can be used as an add-on to generic search engines, when no definitions can be found in on-line encyclopedias." ></td>
	<td class="line x" title="14:255	The system first invokes a search engine using the (possibly multi-word) term whose definition is sought, the target term, as the query." ></td>
	<td class="line x" title="15:255	It then scans the top pages returned by the search engine to locate 250-character snippets with the target term at their centers; we call these snippets windows." ></td>
	<td class="line x" title="16:255	The windows are candidate definitions of the target term, and they are then classified as acceptable (positive class) or unacceptable (negative class) using supervised machine learning." ></td>
	<td class="line x" title="17:255	The system reports the windows for which it is most confidentthattheybelonginthepositiveclass." ></td>
	<td class="line x" title="18:255	Table1showsexamplesofshortdefinitionsfoundby our system." ></td>
	<td class="line x" title="19:255	In our experiments, we allow the system to return up to five windows per target term, and the systems response is counted as correct if any of the returned windows contains an acceptable short definition of the target." ></td>
	<td class="line x" title="20:255	This is similar to the treatment of definition questions in TREC 2000 and 2001 (Voorhees, 2000; Voorhees, 2001), but the answer is sought on the Web, not in a given document collection of a particular genre." ></td>
	<td class="line x" title="21:255	More recent TREC QA tracks required definition questions to be answered by lists of complementarytextsnippets, jointlyprovidingrequiredoroptional information nuggets (Voorhees, 2003)." ></td>
	<td class="line x" title="22:255	In contrast, we focus on locating single snippets that include self-contained short definitions." ></td>
	<td class="line x" title="23:255	Despite its simpler nature, we believe the task we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users seeking definitions of terms not covered by encyclopedias." ></td>
	<td class="line x" title="24:255	We also note that evaluating multi-snippet definitions can be problematic, because it is often difficult to agree which information nuggets should be treated as required, or even optional (Hildebrandt et al., 2004)." ></td>
	<td class="line x" title="25:255	In contrast, earlier experimental results we have reported (Androutsopoulos and Galanis, 2005) show strong inter-assessor agreement(K > 0.8)forsingle-snippetdefinitions(Eugenio and Glass, 2004)." ></td>
	<td class="line x" title="26:255	The task we address also differs from DUCs query focused summarization (Dang, 2005; Dang, 2006)." ></td>
	<td class="line x" title="27:255	Our queries are single terms, whereas DUC queries are longer topic 1270 Target term: Babesiosis () Babesiosis is a rare, severe and sometimes fatal tickborne disease caused by various types of Babesia, a microscopic parasite that infects red blood cells." ></td>
	<td class="line x" title="28:255	In New York state, the causative parasite is babesia microti." ></td>
	<td class="line x" title="29:255	Who gets Babesiosis?" ></td>
	<td class="line x" title="30:255	Babesiosis () Target term: anorexia nervosa () anorexia nervosa is an illness that usually occurs in teenagegirls,butitcanalsooccurinteenageboys,andadult women and men." ></td>
	<td class="line x" title="31:255	People with anorexia are obsessed with being thin." ></td>
	<td class="line x" title="32:255	They lose a lot of weight and are terrified of gaining weight." ></td>
	<td class="line x" title="33:255	The () Target term: Kinabalu () one hundred and thirty eight kilometers from Kota Kinabalu, the capital of the Malaysian state of Sabah, rises the majestic mount Kinabalu." ></td>
	<td class="line x" title="34:255	With its peak at 4,101 meters (and growing), mount Kinabalu is the highest mountain in south-east Asia." ></td>
	<td class="line x" title="35:255	This () Target term: Pythagoras () Pythagoras of Samos about 569 BC about 475 BC click the picture above to see eleven larger pictures Pythagoras was a Greek philosopher who made important developments in mathematics, astronomy, and the theory of music." ></td>
	<td class="line x" title="36:255	The theorem now known as () Target term: Sacajawea () Sacajawea was a Shoshone Indian princess." ></td>
	<td class="line x" title="37:255	The Shoshone lived from the rocky mountains to the plains." ></td>
	<td class="line x" title="38:255	They lived primarily on buffalo meat." ></td>
	<td class="line x" title="39:255	The shoshone traveled for many days searching for buffalo." ></td>
	<td class="line x" title="40:255	They hunted on horseback using the buffalo for food () Target term: tale of Genji () the tale of Genji This site aims to promote a wider understanding and appreciation of the tale of Genji the 11th century Japanese classic written by a Heian court lady knownasMurasakiShikibu." ></td>
	<td class="line x" title="41:255	Italsoservesasakindoftravel guide to the world () Target term: Jacques Lacan () who is Jacques Lacan?" ></td>
	<td class="line x" title="42:255	John Haber in New York city a primer for pre-post-structuralists Jacques Lacan is a Parisianpsychoanalystwhohasinfluencedliterarycriticism and feminism." ></td>
	<td class="line x" title="43:255	He began work in the 1950s, in the Freudian society there." ></td>
	<td class="line x" title="44:255	It was a () Table 1: Definitions found by our system." ></td>
	<td class="line x" title="45:255	descriptions, often entire paragraphs; furthermore, we do not attempt to compose coherent and cohesive summaries from several snippets." ></td>
	<td class="line x" title="46:255	The system we present is based on our earlier work (Miliaraki and Androutsopoulos, 2004), where an SVM classifier (Cristianini and ShaweTaylor,2000)wasusedtoseparateacceptablewindows from unacceptable ones; the SVM also returned confidence scores, which were used to rank the acceptable windows." ></td>
	<td class="line x" title="47:255	On datasets from the TREC 2000 and 2001 QA tracks, our earlier system clearly outperformed the methods of Joho and Sanderson (2000; 2001) and Prager et al.(2001; 2002), as reported in previous work (Miliaraki and Androutsopoulos, 2004)." ></td>
	<td class="line x" title="49:255	To train the SVM, however, thousands of training windows were required, eachtaggedasapositiveornegativeexample." ></td>
	<td class="line x" title="50:255	Obtaining large numbers of training windows is easy, but manually tagging them is very timeconsuming." ></td>
	<td class="line x" title="51:255	In the TREC 2000 and 2001 datasets, it was possible to tag the training windows automatically by using training target terms and accompanying regular expression patterns provided by the TREC organizers." ></td>
	<td class="line x" title="52:255	The regular expressions coveredalltheknownacceptabledefinitionsofthe correspondingtermsthatcanbeextractedfromthe datasets." ></td>
	<td class="line x" title="53:255	When the training windows, however, are obtained from the Web, it is impossible to construct manually regular expressions for all the possible phrasings of the acceptable definitions in the training windows." ></td>
	<td class="line x" title="54:255	In subsequent work (Androutsopoulos and Galanis, 2005), we developed ATTW (automatic taggingoftrainingwindows),atechniquethatproduces arbitrarily large collections of training windows from the Web with practically no manual effort, in effect making our overall system unsupervised." ></td>
	<td class="line x" title="55:255	ATTW uses training terms for which several encyclopedia definitions are available, and compares each Web training window (each window extracted from the pages the search engine returned for a training term) to the corresponding encyclopedia definitions." ></td>
	<td class="line x" title="56:255	Web training windows that are very similar (or dissimilar) to the corresponding encyclopedia definitions are tagged as positive (or negative) examples; if the similarity is neither too high nor too low, the window is not included in the classifiers training data." ></td>
	<td class="line x" title="57:255	Previously reported experiments (Androutsopoulos and Galanis, 2005) showed that ATTW leads to significantly better results, compared to training the classifier on all the available TREC windows, for which regular expressions are available, and then using it to classify Web windows." ></td>
	<td class="line x" title="58:255	Note that in ATTW the encyclopedia definitions are used only during training." ></td>
	<td class="line x" title="59:255	Once the classifier has been trained, it can be used to discover definitions on arbitrary Web pages." ></td>
	<td class="line x" title="60:255	In fact, during testing we discard windows originating from on-line encyclopedias, simulating the case where we seek definitions of terms not covered by encyclopedias; we also ignore windows from on-line encyclopedias during training." ></td>
	<td class="line x" title="61:255	Also, note that the classifier is trained on Web windows, not directly on encyclopedia definitions, which allows it to avoid relying excessively on phrasings that are common in encyclopedia definitions, but uncommon in more indirect definitions of arbitrary Web pages." ></td>
	<td class="line x" title="62:255	Fur1271 thermore, training the classifier directly on encyclopedia definitions would not provide negative examples." ></td>
	<td class="line x" title="63:255	In our previous work with ATTW (Androutsopoulos and Galanis, 2005) we used a measure constructed by ourselves to assess the similarity between Web windows and encyclopedia definitions." ></td>
	<td class="line oc" title="64:255	Here, we use the more established ROUGE-W measure (Lin, 2004) instead." ></td>
	<td class="line o" title="65:255	ROUGEW and other versions of ROUGE have been used in summarization to measure how close a machineauthored summary is to multiple human summaries of the same input." ></td>
	<td class="line o" title="66:255	We use ROUGE-W in a similar setting, to measure how close a training window is to multiple encyclopedia definitions of the same term." ></td>
	<td class="line o" title="67:255	A further difference from our previous work is that we also use ROUGE-W when computing the features of the windows to be classified." ></td>
	<td class="line x" title="68:255	Previously, the SVM relied, among others, on Boolean features indicating if the target term was preceded or followed in the window to be classified by a particular phrase indicating a definition (e.g., target, a kind of, such as target)." ></td>
	<td class="line o" title="69:255	The indicative phrases are selected automatically during training, but now the corresponding features are not Boolean; their values are the ROUGEW similarity scores between an indicative phrase and the context of the target term in the window." ></td>
	<td class="line x" title="70:255	This allows the system to soft-match the phrases tothewindows(e.g., encounteringtarget, another kind of, instead of target, a kind of).1 In our new system we also use a Maximum Entropy(MAXENT)classifier(Ratnaparkhi, 1997)instead of an SVM, because much faster implementations of the former are available.2 We present experimental results showing that our new system significantly outperforms our previously publishedone." ></td>
	<td class="line p" title="71:255	Theuseofthe MAXENT classifierbyitselfimprovedslightlyourresults, buttheimprovements come mostly from using ROUGE-W." ></td>
	<td class="line x" title="72:255	Apart from presenting an improved version of our system, the main contribution of this paper is a detailed experimental comparison of our new system against Cui et al.s (2004; 2005; 2006; 2007)." ></td>
	<td class="line p" title="73:255	The latter is particularly interesting, because it is well published, it includes both an alternative, centroid-based technique to automatically tag training examples and a soft-matching classifier, 1We also experimented with other similarity measures (e.g., edit distance) and ROUGE variants, but we obtained the best results with ROUGE-W." ></td>
	<td class="line x" title="74:255	2We use Stanfords classifier; see http://nlp.stanford.edu/." ></td>
	<td class="line x" title="75:255	and it is publicly available.3 We show that ATTW outperforms Cui et al.s centroid-based technique, and that our overall system is also clearly better than Cui et al.s in the task we address." ></td>
	<td class="line o" title="76:255	Section 2 discusses ATTW with ROUGE-W, Cui et al.s centroid-based method to tag training examples, and experiments showing that ATTW is better." ></td>
	<td class="line x" title="77:255	Section 3 describes our new overall system, the system of Cui et al., and the baselines." ></td>
	<td class="line x" title="78:255	Section 4 reports experimental results showing that our system is better than Cui et al.s, and better than our previously published system." ></td>
	<td class="line x" title="79:255	Section 5 discusses related work; and section 6 concludes." ></td>
	<td class="line x" title="80:255	2 Tagging training windows During both training and testing, for each target term we keep the r most highly ranked Web pages the search engine returns." ></td>
	<td class="line x" title="81:255	We then extract the first f windows of the target term from each page, since early occurrences of the target terms on pages are more likely to be definitions." ></td>
	<td class="line x" title="82:255	We, thus, obtain r  f windows per term.4 When testing, we return the k windows of the target term that the classifier is most certain they belong in the positive class." ></td>
	<td class="line x" title="83:255	In our experiments, r = 10, f = 5, k = 5." ></td>
	<td class="line x" title="84:255	During training, we train the classifier on the q  r  f windows we obtain for q training target terms; in our experiments, q ranged from 50 to 1500." ></td>
	<td class="line x" title="85:255	Training requires tagging first the training windows as positive or negative, possibly discarding windows that cannot be tagged automatically." ></td>
	<td class="line o" title="86:255	2.1 ATTW with ROUGE-W similarity To tag a training window w of a training term t with ATTW and ROUGE-W, we obtain a set Ct of definitions of t from encyclopedias.5 Stop-words, punctuation, and non-alphanumeric characters are removed from Ct and w, and a stemmer is applied; the testing windows undergo the same preprocessing.6 For each definition d  Ct, we find the longest common word subsequence of w and d. If w is the word sequence A,B,F,C,D,E 3See http://www.cuihang.com/software.html." ></td>
	<td class="line x" title="87:255	The software and a demo of our system, and the datasets we used are also freely available; see http://nlp.cs.aueb.gr/." ></td>
	<td class="line x" title="88:255	4We used Altavista in our experiments." ></td>
	<td class="line x" title="89:255	We remove HTML tags and retain only the plain text of the pages." ></td>
	<td class="line x" title="90:255	5The training terms were randomly selected from the index of http://www.encyclopedia.com/." ></td>
	<td class="line x" title="91:255	We used Googles define: to obtain definitions from other encyclopedias." ></td>
	<td class="line x" title="92:255	6We use the 100 most frequent words of the BNC corpus (http://www.natcorp.ox.ac.uk/) as the stop-list, and Porters stemmer (http://tartarus.org/martin/PorterStemmer/)." ></td>
	<td class="line x" title="93:255	1272 and d = A,B,E,C,G,D, the longest common subsequence is A,B,C,D." ></td>
	<td class="line x" title="94:255	The longest common subsequence is divided into consecutive matches, producing in our example A,B|C|D." ></td>
	<td class="line x" title="95:255	We then compute the following score (weighted longest common subsequence), where m is the number of consecutive matches, ki is the length of the i-th consecutive match, and f is a weighting function." ></td>
	<td class="line x" title="96:255	We use f(k) = ka, where a > 1 is a parameter we tune experimentally." ></td>
	<td class="line oc" title="97:255	WLCS(w,d) =summationtextmi=0 f(ki) We then compute the following quantities, where || is word length, and f1 is the inverse of f. P(w,d) = f1(WLCS(w,d)f(|w|) ) R(w,d) = f1(WLCS(w,d)f(|d|) ) F(w,d) = (1+2)R(w,d)P(w,d)R(w,d)+2P(w,d) In effect, P(w,d) examines how close the longest common substring is to w and R(w,d) how close it is to d. Following Lin (2004), we use  = 8, assigninggreaterimportancetoR(w,d)." ></td>
	<td class="line x" title="98:255	If R(w,d) is high, the longest common substring is very similar to d; then w (which also includes the longest common substring) intuitively contains almost all the information of d, i.e., all the information of a known acceptable definition (high recall)." ></td>
	<td class="line x" title="99:255	If P(w,d) is high, the longest common substring is very similar to w; then d (which also includes the longest common substring) contains almost all the information of w, i.e., w does not contain any (redundant) information not included in a known acceptable definition, something we care less for." ></td>
	<td class="line o" title="100:255	The ROUGE-W similarity sim(w,Ct) between w and Ct is the maximum F(w,d), for all d  Ct. Training windows with sim(w,Ct) > T+ are tagged as positive; if sim(w,Ct) < T, they are tagged as negative; and if T  sim(w,Ct)  T+, they are discarded." ></td>
	<td class="line x" title="101:255	We tune the thresholds T+ and T experimentally, as discussed below." ></td>
	<td class="line x" title="102:255	2.2 The centroid-based tagging approach This method is used in the system of Cui et al.(2004; 2005; 2006; 2007)." ></td>
	<td class="line x" title="104:255	For each training target term, we construct a centroid pseudo-text containing the words that co-occur most frequently with the target term." ></td>
	<td class="line x" title="105:255	We then compute the similarity between each training window and the centroid ofitstargetterm." ></td>
	<td class="line x" title="106:255	Ifitexceedsathreshold,thewindow is tagged as positive; Cui et al. produce only positive examples." ></td>
	<td class="line x" title="107:255	The centroid of a training target term t is constructed as follows." ></td>
	<td class="line x" title="108:255	For each worduints training windows, we compute the centrality score defined below, where SFt is the number of ts training windows, SFu is the number of us windows that can be extracted from the retained Web pages the search engine returned for t, SFtu is the number of windows on the same pages that contain both t and u, and idf(u) is the inverse document frequency of w.7 Centrality scores are pointwise mutual information with an extra idf (u) factor." ></td>
	<td class="line x" title="109:255	centrality(u) = log( SFtuSFt+SFu)  idf (u) The words u whose centrality scores exceed the mean by at least a standard deviation are added to the centroid of t. Before computing the centrality scores, stop-words, punctuation, and nonalphanumeric characters are removed, and a stemmer is applied, as in ATTW." ></td>
	<td class="line x" title="110:255	The similarities between training windows and centroids are then computed using cosine similarity, after turning the centroids and windows into binary vectors that show which words they contain." ></td>
	<td class="line x" title="111:255	2.3 Comparing the tagging approaches To evaluate the two methods that tag training windows, we selected randomly q = 200 target terms, different from those used for training and testing." ></td>
	<td class="line x" title="112:255	We collected the q  r  f = 200  10  5 windows from the corresponding Web pages, we selected randomly 400 from the collected 10,000 windows, and tagged them manually as positive or negative." ></td>
	<td class="line x" title="113:255	Figure 1 plots the positive precision of the two methods against their positive recall, and figure 2 shows negative precision against negative recall." ></td>
	<td class="line x" title="114:255	For different values of T+, we obtain a different point in figure 1; similarly for T and figure 2." ></td>
	<td class="line x" title="115:255	Positive precision is TP/(TP +FP), positive recall is TP/(TP + FN), and likewise for negative precision and recall; TP (true positives) are the positive training windows the method has correctly tagged as positive, FP are the negative windows the method has tagged as positives etc. Forveryhigh(strict)T+ values,themethodstag very few (or none) training windows as positive; hence, both TP and TP + FP approach (or become) zero; we take positive precision to be zero in that case." ></td>
	<td class="line x" title="116:255	Positive recall also approaches (or becomes) zero, which is why both positive recall and 7We obtained idf(u) from BNC." ></td>
	<td class="line x" title="117:255	Cui et al. use sentences instead of windows, reducing the risk of truncating definitions." ></td>
	<td class="line x" title="118:255	We used windows in all systems, to compare fairly." ></td>
	<td class="line x" title="119:255	1273 Figure 1: Results of generating positive examples." ></td>
	<td class="line x" title="120:255	Figure2: Resultsofgeneratingnegativeexamples." ></td>
	<td class="line x" title="121:255	precision reach zero in the left of figure 1." ></td>
	<td class="line x" title="122:255	Similar comments apply to figure 2, though both methods always tagged correctly at least a few training windows as negative, for the T values we tried; hence, negative precision was never zero." ></td>
	<td class="line x" title="123:255	Positive precision shows how certain we can be that training windows tagged as positive are indeed positive; whereas positive recall is the percentage of true positive examples that we manage to tag as such." ></td>
	<td class="line x" title="124:255	Figure 1 shows that when using ATTW, we need to settle for a low positive recall, i.e., miss out many positive examples, in order to obtain a reasonably high precision." ></td>
	<td class="line x" title="125:255	It also shows thatthecentroidmethodisclearlyworsewhentagging positive examples; its positive precision is almost always less than 0.3." ></td>
	<td class="line x" title="126:255	Figure 2 shows that both methods achieve high negative precision and recall; they manage to assign trustworthy negative labels without missing many negative examples." ></td>
	<td class="line o" title="127:255	However, ATTW is significantly better when tagging positive examples, as shown in figure 1; hence, it is better than the centroid method.8 8We tried different values of ROUGE-Ws a parameter in When using ATTW in practice, we need to select T+ and T." ></td>
	<td class="line x" title="128:255	We assign more importance to selecting a T+ (a point of ATTWs curve in figure 1) that yields high positive precision; the choice of T (point in figure 2) is less important, because ATTWs negative precision is always reasonably high." ></td>
	<td class="line x" title="129:255	Based on figure 1, we set T+ to 0.58, which corresponds to positive precision 0.66 and positive recall 0.16." ></td>
	<td class="line x" title="130:255	By tuning the two thresholds we can control the number of positively or negatively tagged examples we produce (and their ratio), and the number of examples we discard." ></td>
	<td class="line x" title="131:255	Having set T+, we set T to 0.30, a value that maintains the ratio of truly positive to truly negative windows of the 400 manually tagged windows (0.2 to 1), since this is approximately the ratio the classifier will confront during testing; we also experimented with a 1 to 1 ratio, but the results were worse." ></td>
	<td class="line x" title="132:255	This T value corresponds negative precision 0.70 and negative recall 0.02." ></td>
	<td class="line x" title="133:255	Thus, both positive and negative precision is approximately 0.7, which means that approximately 30% of the tags we assign to theexamplesareincorrect." ></td>
	<td class="line x" title="134:255	Ourexperiments,however, indicate that the classifier is able to generalize well over this noise." ></td>
	<td class="line x" title="135:255	3 Finding new definitions We now present our overall system, the system of Cui et al., and the baselines." ></td>
	<td class="line x" title="136:255	3.1 Our system Given a target term, our system extracts r  f = 10  5 windows from the pages returned by the search engine, and uses the MAXENT classifier to separate them into acceptable and unacceptable definitions.9 It then returns the k = 5 windows the classifier is most confident they are acceptable." ></td>
	<td class="line x" title="137:255	Theclassifieristrainedonwindowstaggedaspositive or negative using ATTW." ></td>
	<td class="line x" title="138:255	It views each window as a vector of the following features:10 SN: The ordinal number of the window on the pageitoriginatesfrom(e.g., secondwindowofthe target term from the beginning of the page)." ></td>
	<td class="line x" title="139:255	Early mentions of a term are more likely to define it." ></td>
	<td class="line x" title="140:255	RK: The ranking of the Web page the window originates from, as returned by the search engine." ></td>
	<td class="line x" title="141:255	the interval (1,2]." ></td>
	<td class="line x" title="142:255	We use a = 1.4, which was the value with the best results on the 400 windows." ></td>
	<td class="line x" title="143:255	We did not try a > 2, as the results were declining as a approached 2." ></td>
	<td class="line x" title="144:255	9We do not discuss MAXENT classifiers, since they are a well documented in the literature." ></td>
	<td class="line x" title="145:255	10SN andWC originate from Joho and Sanderson (2000)." ></td>
	<td class="line x" title="146:255	1274 WC: We create a simple centroid of the windows target term, much as in section 2.2." ></td>
	<td class="line x" title="147:255	The centroids wordsarechosenbasedontheirfrequencyinther f windows ofthe target term; the 20most frequent words are chosen." ></td>
	<td class="line x" title="148:255	WC is the percentage of the 20 words that appear in the vectors window." ></td>
	<td class="line x" title="149:255	Manual patterns: 13 Boolean features, each signaling if the window matches a different manually constructed lexical pattern (e.g., target, a/an/the, as in Tony Blair, the British prime minister)." ></td>
	<td class="line x" title="150:255	The patterns are those used by Joho and Sanderson (2000), and four more introduced in our previous work (Androutsopoulos and Galanis, 2005) and (Miliaraki and Androutsopoulos, 2004)." ></td>
	<td class="line x" title="151:255	They are intended to perform well across text genres." ></td>
	<td class="line x" title="152:255	Automatic patterns: m numeric features, each showing the degree to which the window matches a different automatically acquired lexical pattern." ></td>
	<td class="line x" title="153:255	Thepatternsarewordn-grams(n  {1,2,3})that must occur directly before or after the target term (e.g., target which is)." ></td>
	<td class="line x" title="154:255	The patterns are acquired as follows." ></td>
	<td class="line x" title="155:255	First, all the n-grams directly before or after any target term in the training windows are collected." ></td>
	<td class="line x" title="156:255	The n-grams that have been encountered at least 10 times are candidate patterns." ></td>
	<td class="line x" title="157:255	From those, the m patterns with the highest precision scores are retained, where precision is the number of positive training windows the pattern matchesoverthetotalnumberoftrainingwindows it matches; we use m = 300 in our experiments, based on the results of our previous work." ></td>
	<td class="line x" title="158:255	The automatically acquired patterns allow the system to detect definition contexts that are not captured by the manual patterns, including genre-specific contexts." ></td>
	<td class="line o" title="159:255	The value of each feature is the ROUGE-W score between a pattern and the left or right context of the target term in the window." ></td>
	<td class="line x" title="160:255	3.2 Cui et al.s system Given a target termt, Cui et al.(2004; 2005; 2006; 2007) initially locate sentences containing t in relevantdocuments." ></td>
	<td class="line x" title="162:255	Weusetherf = 105windows from the pages returned by the search engine, instead of sentences." ></td>
	<td class="line x" title="163:255	Cui et al. then construct the centroid of t, and compute the cosine similarity of each one of the r  f windows to the centroid, as in section 2.2." ></td>
	<td class="line x" title="164:255	The 10 windows that are closer to the centroid are considered candidate answers." ></td>
	<td class="line x" title="165:255	All candidate answers are then processed by a part-ofspeech (POS) tagger and a chunker." ></td>
	<td class="line x" title="166:255	The words of the centroid are replaced in all the candidate answers by their POS tags; the target term, noun phrases, forms of the verb to be, and articles are replaced by special tags (e.g., TARGET, NP), while adjectives and adverbs are removed." ></td>
	<td class="line x" title="167:255	The candidate answers are then cropped to L tokens to the left and right of the target term, producing twosubsequences(leftandright)percandidateanswer; we set L = 3, which is Cui et al.s default." ></td>
	<td class="line x" title="168:255	Cui et al. experimented with two approaches to rank the candidate answers, called Bigram Model and Profile Hidden Markov Model (PHMM)." ></td>
	<td class="line x" title="169:255	Both are learning components that produce soft patterns,though PHMM ismuchmorecomplicated." ></td>
	<td class="line x" title="170:255	In their earlier work, Cui et al.(2005) found the BigramModeltoperformbetterthan PHMM; inmore recentexperimentswithmoredata(Cui, 2006; Cui et al., 2007) they found PHMM to perform better, but the difference was not statistically significant." ></td>
	<td class="line x" title="172:255	Given these results and the complexity of PHMM, we experimented only with the Bigram Model." ></td>
	<td class="line x" title="173:255	In the Bigram Model, the left and right subsequences of each candidate answer are considered separately." ></td>
	<td class="line x" title="174:255	Below S1,,SL refer to the slots (word positions) of a (left or right) subsequence, and t1,,tL to the particular words in the slots." ></td>
	<td class="line x" title="175:255	For each subsequence S1 = t1,,SL = tL of a candidate answer, we first estimate: P(ti|Si) = |Si(ti)| + summationtext tprime |Si(ti)| +  N P(ti|ti1) = |Si(ti) Si1(ti1)||S i(ti)| P(ti|Si) is the probability that ti will appear in slotSi ofaleftorrightsubsequence(dependingon the subsequence considered) of an acceptable candidate answer." ></td>
	<td class="line x" title="176:255	P(ti|ti1) is the probability that ti will follow ti1 in a (left or right) subsequence of an acceptable candidate answer." ></td>
	<td class="line x" title="177:255	Cui et al. use only positive training examples, generated by the centroid-based approach of section 2.2." ></td>
	<td class="line x" title="178:255	|Si(ti)| is the number of times ti appeared in Si in the (left or right) subsequences of the training examples." ></td>
	<td class="line x" title="179:255	tprime ranges over all the words that occurred in Si in the training examples." ></td>
	<td class="line x" title="180:255	|Si(ti)  Si1(ti1)| is the numberoftimesti andti1 co-occurredinthecorresponding slots in the training examples." ></td>
	<td class="line x" title="181:255	N is the numberofdifferentwordsthatoccurredinthe(left or right) training subsequences, and is a constant set to 2, as in Cui et al.s experiments." ></td>
	<td class="line x" title="182:255	Following Cui et al., if ti is a POS or other special tag then the probabilities above are estimated by counting 1275 only the tags of the training examples." ></td>
	<td class="line x" title="183:255	Similarly, if ti is an actual word, only the actual words (not tags) of the training examples are considered." ></td>
	<td class="line x" title="184:255	The probability of each subsequence could then be estimated as: P(t1,,tL) = P(t1|S1)  Lproductdisplay i=2 (P(ti|ti1) + (1 ) P(ti|Si)) Instead, Cui et al. use the following scoring measure, which also accounts for the fact that some subsequences may have length l < L. They tune  by Expectation Maximization." ></td>
	<td class="line x" title="185:255	Pnorm(t1,,tL) = 1l  [logP(t1|S1) + Lsummationdisplay i=2 log(P(ti|ti1) + (1 ) P(ti|Si))] The overall score of a candidate answer is then: P = (1 ) Pnorm(left) + Pnorm(right) Again, Cui et al. tuneaby Expectation Maximization." ></td>
	<td class="line x" title="186:255	Instead, we tuned  and  by a grid search in [0,1]  [0,1], with step 0.1 for both parameters." ></td>
	<td class="line x" title="187:255	For the tuning, we trained Cui et al.s system on 2,000 randomly selected target terms, excluding terms used for other purposes." ></td>
	<td class="line x" title="188:255	We used 160 manually tagged windows to evaluate the systems performance with the different values of  and ; the 160 windows were selected randomly from the 10,000 windows of section 2.3, after excluding the 400 manually tagged windows of that section." ></td>
	<td class="line x" title="189:255	The resulting values for  and  were 0.7 and 0.6, respectively." ></td>
	<td class="line x" title="190:255	Apart from the modifications we mentioned,weuseCuietal.soriginalimplementation." ></td>
	<td class="line x" title="191:255	3.3 Baseline methods The first baseline selects the first window of each one of the five highest ranked Web pages, as returned by the search engine, and returns the five windows." ></td>
	<td class="line x" title="192:255	The second baseline returns five windows chosen randomly from the r  f = 10  5 available ones." ></td>
	<td class="line o" title="193:255	The third baseline (centroid baseline) creates a centroid of the r  f windows, as in section 2.2, and returns the five windows with the highest cosine similarity to the centroid.11 11We also reimplemented the definitions component of Chu-Carroll et al.(2004; 2005), but its performance was worse than our centroid baseline." ></td>
	<td class="line x" title="195:255	Figure 3: Correct responses, 5 answers/question." ></td>
	<td class="line x" title="196:255	4 Evaluation of systems We used q training target terms in the experiments of this section, with q ranging from 50 to 1500, and 200 testing terms, with no overlap between training and testing terms, and excluding terms that had been used for other purpose.12 We had to use testing terms for which encyclopedia definitions were also available, to judge the acceptability of the systems responses, since many terms are highly technical." ></td>
	<td class="line x" title="197:255	We discarded, however, windows extracted from encyclopedia pages when testing, simulating the case where the target terms are not covered by encyclopedias." ></td>
	<td class="line x" title="198:255	As already mentioned, for each target term we extract r  f = 10  5 windows (or fewer, if fewer are available) from the pages the search engine returns." ></td>
	<td class="line x" title="199:255	We then provide these windows to each of the systems, allowing them to return up to k = 5 windows, ordered by decreasing confidence." ></td>
	<td class="line x" title="200:255	If any of the k windows contains an acceptable short definition of the target term, as judged by a human evaluator, the systems response is counted as correct." ></td>
	<td class="line x" title="201:255	We also calculate the Mean Reciprocal Rank (MRR) of each systems responses, as in the TREC QA track: if the first acceptable definition of a response is in the j-th position (1  j  k), the responses score is 1/j; MRR is the mean of the responses scores, i.e., it rewards systems that return acceptable definitions higher in their responses." ></td>
	<td class="line x" title="202:255	Figures 3 and 4 show the results of our experimentsaspercentageofcorrectresponsesand MRR, respectively; the error bars of figure 3 correspond to 95% confidence intervals." ></td>
	<td class="line x" title="203:255	Our system clearly outperforms Cui et al.s, despite the fact that the 12The reader is reminded that all terms were selected randomly from the index of an on-line encyclopedia." ></td>
	<td class="line x" title="204:255	1276 Figure 4: MRR scores, 5 answers per question." ></td>
	<td class="line x" title="205:255	latter uses more linguistic resources (a POS tagger and a chunker)." ></td>
	<td class="line x" title="206:255	Both systems outperform the baselines, of which the centroid baseline is the best, and both systems perform better as the size of the training set increases." ></td>
	<td class="line x" title="207:255	The baselines contain no learning components; hence, their curves are flat." ></td>
	<td class="line x" title="208:255	We also show the results (Base-Attrs) of our system when the features that correspond to automatically acquired patterns are excluded." ></td>
	<td class="line x" title="209:255	Clearly, these patterns help our system achieve significantly better results; however, our system outperforms Cui et al.s even without them." ></td>
	<td class="line x" title="210:255	Without the automatic patterns, our system also shows signs of saturation as the training data increase." ></td>
	<td class="line x" title="211:255	Figures 5 and 6 show the performance of our new system against our previously published one (Androutsopoulos and Galanis, 2005); the new system clearly outperforms the old one." ></td>
	<td class="line p" title="212:255	Additional experiments we conducted with the old system replacing the SVM by the MAXENT classifier (without using ROUGE-W) indicate that the use of MAXENT by itself also improved slightly the results,butthedifferencesaretoominortoshow; the improvement is mostly due to the use of ROUGEW instead of our previous measure." ></td>
	<td class="line o" title="213:255	5 Related work Xu et al.(2004) use an information extraction engine to extract linguistic features from documents relevant to the target term." ></td>
	<td class="line x" title="215:255	The features are mostly phrases, such as appositives, and phrases expressing relations." ></td>
	<td class="line x" title="216:255	The features are then ranked by their type and similarity to a centroid, and the most highly ranked ones are returned." ></td>
	<td class="line x" title="217:255	Xu et al. seem to aim at generating multi-snippet definitions, unlike the single-snippet definitions we seek." ></td>
	<td class="line x" title="218:255	Blair-Goldensohn et al.(2003; 2004) extract sentences that may provide definitional informaFigure 5: Correct responses of our new and previous system, allowing 5 answers per question." ></td>
	<td class="line x" title="220:255	Figure 6: MRR of our new and previous system." ></td>
	<td class="line x" title="221:255	tion from documents retrieved for the target term; a decision tree learner and manually tagged training data are used." ></td>
	<td class="line x" title="222:255	The sentences are then matched against manually constructed patterns, which operate on syntax trees, to detect sentences expressing the target terms genus, species, or both (genus+species)." ></td>
	<td class="line x" title="223:255	The system composes its answer by placing first the genus+species sentence that is closer to the centroid of the extracted sentences." ></td>
	<td class="line x" title="224:255	The remaining sentences are ranked by their distance from the centroid, and the most highly ranked ones are clustered." ></td>
	<td class="line x" title="225:255	The system then selects iteratively the cluster that is closer to the centroid of the extracted sentences and the most recently used cluster." ></td>
	<td class="line x" title="226:255	The clusters most representative sentence, i.e., the sentence closest to the centroid of the clusters sentences, is added to the response." ></td>
	<td class="line x" title="227:255	Theiterationsstopwhenamaximumresponselengthisreached." ></td>
	<td class="line x" title="228:255	Multi-snippetdefinitions are generated." ></td>
	<td class="line x" title="229:255	Han et al.(2004; 2006) parse a definition question to locate the head word of the target term." ></td>
	<td class="line x" title="231:255	They also use a named entity recognizer to determine the target terms type (person, organization, 1277 etc.)." ></td>
	<td class="line x" title="232:255	They then extract from documents relevant to the target term sentences containing its head word, as well as sentences the extracted ones refer to (e.g., via pronouns)." ></td>
	<td class="line x" title="233:255	The resulting sentences are matched against manually constructed syntactic patterns to detect phrases conveying definitional information." ></td>
	<td class="line x" title="234:255	The resulting phrases are ranked by criteria like the degree to which the phrase contains words common in definitions of the target terms type, and the highest ranked phrases are included in a multi-snippet summary." ></td>
	<td class="line x" title="235:255	Other mechanisms discard phrases duplicating information." ></td>
	<td class="line x" title="236:255	Xu et al.(2005) aim to extract all the definitions in a document collection." ></td>
	<td class="line x" title="238:255	They parse the documents to detect base noun phrases (without embedded noun phrases)." ></td>
	<td class="line x" title="239:255	Base noun phrases are possible target terms; the paragraphs containing them are matched against manually constructed patterns that look for definitions." ></td>
	<td class="line x" title="240:255	An SVM then separates the remaining paragraphs into good, indifferent, and bad definitions." ></td>
	<td class="line x" title="241:255	Redundant paragraphs, identified by edit distance similarity, are removed." ></td>
	<td class="line x" title="242:255	6 Conclusions and future work We presented a freely available system that finds short definitions of user-specified terms on Web pages." ></td>
	<td class="line x" title="243:255	It employs a MAXENT classifier, which is trained on automatically generated examples; hence, the system is in effect unsupervised." ></td>
	<td class="line p" title="244:255	We use ROUGE-W to generate training examples from Web snippets and encyclopedias, a method that outperforms an alternative centroid-based one." ></td>
	<td class="line x" title="245:255	Once our system has been trained, it can find short definitions of terms that are not covered by encyclopedias." ></td>
	<td class="line x" title="246:255	Experiments show our system outperforms a comparable well-published system and a previously published form of our system." ></td>
	<td class="line x" title="247:255	Our system does not require linguistic processing tools, such as named entity recognizers, POS taggers, chunkers, parsers; hence, it can be easily used in languages where such tools are unavailable." ></td>
	<td class="line x" title="248:255	It could be improved by exploiting the HTML markup of Web pages and the Webs hyperlinks." ></td>
	<td class="line x" title="249:255	For example, the target term is sometimes written in italics in definitions, and some definitions are provided on pages (e.g., pop-up windows) that occurrences of the target term link to." ></td>
	<td class="line x" title="250:255	The work reported here was conducted in the context of project INDIGO, where an autonomous robotic guide for museum collections is being developed (Galanis et al., 2009)." ></td>
	<td class="line x" title="251:255	The guide engages the museums visitors in spoken dialogues, and it describes the exhibits the visitors select by generating spoken natural language descriptions from an ontology." ></td>
	<td class="line x" title="252:255	Among other requests, the visitors can ask follow up questions, and we have found that the most common kind of follow up questions are requests to define terms (e.g., names of persons, events, architectural terms, etc.) mentioned in the generated exhibit descriptions." ></td>
	<td class="line x" title="253:255	Some of these definition requests can be handled by generating new texts from the ontology, but some times the ontology contains no information for the target terms." ></td>
	<td class="line x" title="254:255	We are, thus, experimenting with the possibility of obtaining short definitions from the Web, using the system we presented." ></td>
	<td class="line x" title="255:255	Acknowledgements This work was carried out in INDIGO, an FP6 IST project funded by the European Union, with additional funding provided by the Greek General Secretariat of Research and Technology.13" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1048
Automatic Single-Document Key Fact Extraction from Newswire Articles
Kastner, Itamar;Monz, Christof;"></td>
	<td class="line x" title="1:218	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 415423, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:218	c2009 Association for Computational Linguistics Automatic Single-Document Key Fact Extraction from Newswire Articles Itamar Kastner Department of Computer Science Queen Mary, University of London, UK itk1@dcs.qmul.ac.uk Christof Monz ISLA, University of Amsterdam Amsterdam, The Netherlands christof@science.uva.nl Abstract This paper addresses the problem of extracting the most important facts from a news article." ></td>
	<td class="line x" title="3:218	Our approach uses syntactic, semantic, and general statistical features to identify the most important sentences in a document." ></td>
	<td class="line x" title="4:218	The importance of the individual features is estimated using generalized iterative scaling methods trained on an annotated newswire corpus." ></td>
	<td class="line o" title="5:218	The performance of our approach is evaluated against 300 unseen news articles and shows that use of these features results in statistically significant improvements over a provenly robust baseline, as measured using metrics such as precision, recall and ROUGE." ></td>
	<td class="line x" title="6:218	1 Introduction The increasing amount of information that is available to both professional users (such as journalists, financial analysts and intelligence analysts) and lay users has called for methods condensing information, in order to make the most important content stand out." ></td>
	<td class="line x" title="7:218	Several methods have been proposed over the last two decades, among which keyword extraction and summarization are the most prominent ones." ></td>
	<td class="line x" title="8:218	Keyword extraction aims to identify the most relevant words or phrases in a document, e.g., (Witten et al., 1999), while summarization aims to provide a short (commonly 100 words), coherent full-text summary of the document, e.g., (McKeown et al., 1999)." ></td>
	<td class="line x" title="9:218	Key fact extraction falls in between key word extraction and summarization." ></td>
	<td class="line x" title="10:218	Here, the challenge is to identify the most relevant facts in a document, but not necessarily in a coherent full-text form as is done in summarization." ></td>
	<td class="line x" title="11:218	Evidence of the usefulness of key fact extraction is CNNs web site which since 2006 has most of its news articles preceded by a list of story highlights, see Figure 1." ></td>
	<td class="line x" title="12:218	The advantage of the news highlights as opposed to full-text summaries is that they are much easier on the eye and are better suited for quick skimming." ></td>
	<td class="line x" title="13:218	So far, only CNN.com offers this service and we are interested in finding out to what extent it can be automated and thus applied to any newswire source." ></td>
	<td class="line x" title="14:218	Although these highlights could be easily generated by the respective journalists, many news organization shy away from introducing an additional manual stage into the workflow, where pushback times of minutes are considered unacceptable in an extremely competitive news business which competes in terms of seconds rather than minutes." ></td>
	<td class="line x" title="15:218	Automating highlight generation can help eliminate those delays." ></td>
	<td class="line x" title="16:218	Journalistic training emphasizes that news articles should contain the most important information in the beginning, while less important information, such as background or additional details, appears further down in the article." ></td>
	<td class="line x" title="17:218	This is also the main reason why most summarization systems applied to news articles do not outperform a simple baseline that just uses the first 100 words of an article (Svore et al., 2007; Nenkova, 2005)." ></td>
	<td class="line x" title="18:218	On the other hand, most of CNNs story highlights are not taken from the beginning of the articles." ></td>
	<td class="line x" title="19:218	In fact, more than 50% of the highlights stem from sentences that are not among the first 100 words of the articles." ></td>
	<td class="line x" title="20:218	This makes identifying story highlights a much more challenging task than single-document summarization in the news domain." ></td>
	<td class="line x" title="21:218	In order to automate story highlight identification we automatically extract syntactic, semantic, 415 Figure 1: CNN.com screen shot of a story excerpt with highlights." ></td>
	<td class="line x" title="22:218	and purely statistical features from the document." ></td>
	<td class="line x" title="23:218	The weights of the features are estimated using machine learning techniques, trained on an annotated corpus." ></td>
	<td class="line x" title="24:218	In this paper, we focus on identifying the relevant sentences in the news article from which the highlights were generated." ></td>
	<td class="line x" title="25:218	The system we have implemented is named AURUM: AUtomatic Retrieval of Unique information with Machine learning." ></td>
	<td class="line x" title="26:218	A full system would also contain a sentence compression step (Knight and Marcu, 2000), but since both steps are largely independent of each other, existing sentence compression or simplification techniques can be applied to the sentences identified by our approach." ></td>
	<td class="line x" title="27:218	The remainder of this paper is organized as follows: The next section describes the relevant work done to date in keyfact extraction and automatic summarization." ></td>
	<td class="line x" title="28:218	Section 3 lays out our features and explains how they were learned and estimated." ></td>
	<td class="line x" title="29:218	Section 4 presents the experimental setup and our results, and Section 5 concludes with a short discussion." ></td>
	<td class="line x" title="30:218	2 Related Work As mentioned above, the problem of identifying story highlight lies somewhere between keyword extraction and single-document summarization." ></td>
	<td class="line x" title="31:218	The KEA keyphrase extraction system (Witten et al., 1999) mainly relies on purely statistical features such as term frequencies, using the tf.idf measure from Information Retrieval,1 as well as on a terms position in the text." ></td>
	<td class="line x" title="32:218	In addition to tf.idf scores, Hulth (2004) uses part-of-speech tags and NP chunks and complements this with machine learning; the latter has been used to good results in similar cases (Turney, 2000; Neto et al., 2002)." ></td>
	<td class="line x" title="33:218	The B&C system (Barker and Cornacchia, 2000), also used linguistic methods to a very limited extent, identifying NP heads." ></td>
	<td class="line x" title="34:218	INFORMATIONFINDER (Krulwich and Burkey, 1996) requires user feedback to train the system, whereby a user notes whether a given document is of interest to them and specifies their own keywords which are then learned by the system." ></td>
	<td class="line x" title="35:218	Over the last few years, numerous singleas well as multi-document summarization approaches have been developed." ></td>
	<td class="line x" title="36:218	In this paper we will focus mainly on single-document summarization as it is more relevant to the issue we aim to address and traditionally proves harder to accomplish." ></td>
	<td class="line x" title="37:218	A good example of a powerful approach is a method named Maximum Marginal Relevance which extracts a sentence for the summary only if it is different than previously selected ones, thereby striving to reduce redundancy (Carbonell and Goldstein, 1998)." ></td>
	<td class="line o" title="38:218	More recently, the work of Svore et al.(2007) is closely related to our approach as it has also exploited the CNN Story Highlights, although their focus was on summarization and using ROUGE as an evaluation and training measure." ></td>
	<td class="line x" title="40:218	Their approach also heavily relies on additional data resources, mainly indexed Wikipedia articles and Microsoft Live query logs, which are not readily available." ></td>
	<td class="line x" title="41:218	Linguistic features are today used mostly in summarization systems, and include the standard features sentence length, n-gram frequency, sentence position, proper noun identification, similarity to title, tf.idf, and so-called bonus/stigma words (Neto et al., 2002; Leite et al., 2007; Pollock and Zamora, 1975; Goldstein et al., 1999)." ></td>
	<td class="line x" title="42:218	On the other hand, for most of these systems, simple statistical features and tf.idf still turn out to be the most important features." ></td>
	<td class="line x" title="43:218	Attempts to integrate discourse models have also been made (Thione et al., 2004), hand in hand with some of Marcus (1995) earlier work." ></td>
	<td class="line x" title="44:218	1tf(t,d) = frequency of term t in document d. idf(t,N) = inverse frequency of documents d containing term t in corpus N, log(|N||dt|) 416 Regarding syntax, it seems to be used mainly in sentence compression or trimming." ></td>
	<td class="line x" title="45:218	The algorithm used by Dorr et al.(2003) removes subordinate clauses, to name one example." ></td>
	<td class="line x" title="47:218	While our approach does not use syntactical features as such, it is worth noting these possible enhancements." ></td>
	<td class="line x" title="48:218	3 Approach In this section we describe which features were used and how the data was annotated to facilitate feature extraction and estimation." ></td>
	<td class="line x" title="49:218	3.1 Training Data In order to determine the features used for predicting which sentences are the sources for story highlights, we gathered statistics from 1,200 CNN newswire articles." ></td>
	<td class="line x" title="50:218	An additional 300 articles were set aside to serve as a test set later on." ></td>
	<td class="line x" title="51:218	The articles were taken from a wide range of topics: politics, business, sport, health, world affairs, weather, entertainment and technology." ></td>
	<td class="line x" title="52:218	Only articles with story highlights were considered." ></td>
	<td class="line x" title="53:218	For each article we extracted a number of ngram statistics, where n{1,2,3}." ></td>
	<td class="line x" title="54:218	n-gram score." ></td>
	<td class="line x" title="55:218	We observed the frequency and probability of unigrams, bigrams and trigrams appearing in both the article body and the highlights of a given story." ></td>
	<td class="line x" title="56:218	An important phrase (of length n  3) in the article would likely be used again in the highlights." ></td>
	<td class="line x" title="57:218	These phrases were ranked and scored according to the probability of their appearing in a given text and its highlights." ></td>
	<td class="line x" title="58:218	Triggerphrases." ></td>
	<td class="line x" title="59:218	These are phrases which cause adjacent words to appear in the highlights." ></td>
	<td class="line x" title="60:218	Over the entire set, such phrases become significant." ></td>
	<td class="line x" title="61:218	We specified a limit of 2 words to the left and 4 words to the right of a phrase." ></td>
	<td class="line x" title="62:218	For example, the word according caused other words in the same sentence to appear in the highlights nearly 25% of the time." ></td>
	<td class="line x" title="63:218	Consider the highlight/sentence pair in Table 1: highlight: 61 percent of those polled now say it was not worth invading Iraq, poll says Text: Now, 61 percent of those surveyed say it was not worth invading Iraq, according to the poll." ></td>
	<td class="line x" title="64:218	Table 1: Example highlight with source sentence." ></td>
	<td class="line x" title="65:218	The word according receives a score of 3 since {invading, Iraq, poll} are all in the highlight." ></td>
	<td class="line x" title="66:218	It should be noted that the trigram {invading Iraq according}would receive an identical score, since {not, worth, poll}are in the highlights as well." ></td>
	<td class="line x" title="67:218	Spawned phrases." ></td>
	<td class="line x" title="68:218	Conversely, spawned phrases occur frequently in the highlights and in close proximity to trigger phrases." ></td>
	<td class="line x" title="69:218	Continuing the example in Table 1,{invading, Iraq, poll, not, worth}are all considered to be spawned phrases." ></td>
	<td class="line x" title="70:218	Of course, simply using the identities of words neglects the issue of lexical paraphrasing, e.g., involving synonyms, which we address to some extent by using WordNet and other features described in this Section." ></td>
	<td class="line x" title="71:218	Table 2 gives an example involving paraphrasing." ></td>
	<td class="line x" title="72:218	highlight: Sources say men were planning to shoot soldiers at Army base Text: The federal government has charged five alleged Islamic radicals with plotting to kill U.S. soldiers at Fort Dix in New Jersey." ></td>
	<td class="line x" title="73:218	Table 2: An example of paraphrasing between a highlight and its source sentence." ></td>
	<td class="line x" title="74:218	Other approaches have tried to select linguistic features which could be useful (Chuang and Yang, 2000), but these gather them under one heading rather than treating them as separate features." ></td>
	<td class="line x" title="75:218	The identification of common verbs has been used both as a positive (Turney, 2000) and as a negative feature (Goldstein et al., 1999) in some systems, whereas we score such terms according to a scale." ></td>
	<td class="line x" title="76:218	Turney also uses a final adjective measure." ></td>
	<td class="line x" title="77:218	Use of a thesaurus has also shown to improve results in automatic summarization, even in multi-document environments (McKeown et al., 1999) and other languages such as Portuguese (Leite et al., 2007)." ></td>
	<td class="line x" title="78:218	3.2 Feature Selection By manually inspecting the training data, the linguistic features were selected." ></td>
	<td class="line x" title="79:218	AURUM has two types of features: sentence features, such as the position of the sentence or the existence of a negation word, receive the same value for the entire sentence." ></td>
	<td class="line x" title="80:218	On the other hand, word features are evaluated for each of the words in the sentence, normalized over the number of words in the sentence." ></td>
	<td class="line x" title="81:218	Our features resemble those suggested by previous works in keyphrase extraction and automatic summarization, but map more closely to the journalistic characteristics of the corpus, as explained in the following." ></td>
	<td class="line x" title="82:218	417 Figure 2: Positions of sentences from which highlights (HLs) were generated." ></td>
	<td class="line x" title="83:218	3.2.1 Sentence Features These are the features which apply once for each sentence." ></td>
	<td class="line x" title="84:218	Position of the sentence in the text." ></td>
	<td class="line x" title="85:218	Intuitively, facts of greater importance will be placed at the beginning of the text, and this is supported by the data, as can be seen in Figure 2." ></td>
	<td class="line x" title="86:218	Only half of the highlights stem from sentences in the first fifth of the article." ></td>
	<td class="line x" title="87:218	Nevertheless, selecting sentences from only the first few lines is not a sure-fire approach." ></td>
	<td class="line x" title="88:218	Table 3 presents an article in which none of the first four sentences were in the highlights." ></td>
	<td class="line x" title="89:218	While the baseline found no sentences, AURUMs performance was better." ></td>
	<td class="line x" title="90:218	The sentence positions score is defined as pi = 1(logi/logN), where i is the position of the sentence in the article and N the total number of sentences in the article." ></td>
	<td class="line x" title="91:218	Numbers or dates." ></td>
	<td class="line x" title="92:218	This is especially evident in news reports mentioning figures of casualties, opinion poll results, or financial news." ></td>
	<td class="line x" title="93:218	Source attribution." ></td>
	<td class="line x" title="94:218	Phrasings such as according to a source or officials say." ></td>
	<td class="line x" title="95:218	Negations." ></td>
	<td class="line x" title="96:218	Negations are often used for introducing new or contradictory information: Kelly is due in a Chicago courtroom Friday for yet another status hearing, but theres still no trial date in sight.2 We selected a number of typical negation phrases to this end." ></td>
	<td class="line x" title="97:218	Causal adverbs." ></td>
	<td class="line x" title="98:218	Manually compiled list of phrases, including in order to, hoping for and because." ></td>
	<td class="line x" title="99:218	2This sentence was included in the highlights Temporal adverbs." ></td>
	<td class="line x" title="100:218	Manually compiled list of phrases, such as after less than, for two weeks and Thursday." ></td>
	<td class="line x" title="101:218	Mention of the news agencys name." ></td>
	<td class="line x" title="102:218	Journalistic scoops and other exclusive nuggets of information often recall the agencys name, especially when there is an element of self-advertisement involved, as in ." ></td>
	<td class="line x" title="103:218	The debates are being held by CNN, WMUR and the New Hampshire Union Leader. It is interesting to note that an opposite approach has previously been taken (Goldstein et al., 1999), albeit involving a different corpus." ></td>
	<td class="line x" title="104:218	Story Highlights: Memorial Day marked by parades, cookouts, ceremonies AAA: 38 million Americans expected to travel at least 50 miles during weekend President Bush gives speech at Arlington National Cemetery  Gulf Coast once again packed with people celebrating holiday weekend First sentences of article: 1." ></td>
	<td class="line x" title="105:218	Veterans and active soldiers unfurled a 90-by100-foot U. S. flag as the nations top commander in the Middle East spoke to a Memorial Day crowd gathered in Central Park on Monday." ></td>
	<td class="line x" title="106:218	2." ></td>
	<td class="line x" title="107:218	Navy Adm. William Fallon, commander of U. S. Central Command, said America should remember those whom the holiday honors." ></td>
	<td class="line x" title="108:218	3." ></td>
	<td class="line x" title="109:218	Their sacrifice has enabled us to enjoy the things that we, I think in many cases, take for granted, Fallon said." ></td>
	<td class="line x" title="110:218	4." ></td>
	<td class="line x" title="111:218	Across the nation, flags snapped in the wind over decorated gravestones as relatives and friends paid tribute to their fallen soldiers." ></td>
	<td class="line x" title="112:218	Sentences the Highlights were derived from: 5." ></td>
	<td class="line x" title="113:218	Millions more kicked off summer with trips to beaches or their backyard grills." ></td>
	<td class="line x" title="114:218	6." ></td>
	<td class="line x" title="115:218	AAA estimated 38 million Americans would travel 50 miles or more during the weekend  up 1.7 percent from last year  even with gas averaging $3.20 a gallon for self-service regular." ></td>
	<td class="line x" title="116:218	7." ></td>
	<td class="line x" title="117:218	In the nations capital, thousands of motorcycles driven by military veterans and their loved ones roared through Washington to the Vietnam Veterans Memorial." ></td>
	<td class="line x" title="118:218	9." ></td>
	<td class="line x" title="119:218	President Bush spoke at nearby Arlington National Cemetery, honoring U. S. troops who have fought and died for freedom and expressing his resolve to succeed in the war in Iraq." ></td>
	<td class="line x" title="120:218	21." ></td>
	<td class="line x" title="121:218	Elsewhere, Alabamas Gulf Coast was once again packed with holiday-goers after the damage from hurricanes Ivan and Katrina in 2004 and 2005 kept the tourists away." ></td>
	<td class="line x" title="122:218	Table 3: Sentence selection outside the first four sentences (correctly identified sentence by AURUM in boldface)." ></td>
	<td class="line x" title="123:218	418 3.2.2 Word Features These features are tested on each word in the sentence." ></td>
	<td class="line x" title="124:218	Bonus words." ></td>
	<td class="line x" title="125:218	A list of phrases similar to sensational, badly, ironically, historic, identified from the training data." ></td>
	<td class="line x" title="126:218	This is akin to bonus/stigma words (Neto et al., 2002; Leite et al., 2007; Pollock and Zamora, 1975; Goldstein et al., 1999)." ></td>
	<td class="line x" title="127:218	Verb classes." ></td>
	<td class="line x" title="128:218	After exploring the training data we manually compiled two classes of verbs, each containing 15-20 inflected and uninflected lexemes, talkVerbs and actionVerbs." ></td>
	<td class="line x" title="129:218	talkVerbs include verbs such as{report, mention, accuse} and actionVerbs refer to verbs such as{provoke, spend, use}." ></td>
	<td class="line x" title="130:218	Both lists also contain the WordNet synonyms of each word in the list (Fellbaum, 1998)." ></td>
	<td class="line x" title="131:218	Proper nouns." ></td>
	<td class="line x" title="132:218	Proper nouns and other parts of speech were identified running Charniaks parser (Charniak, 2000) on the news articles." ></td>
	<td class="line x" title="133:218	3.2.3 Sentence Scoring The overall score of a sentence is computed as the weighted linear combination of the sentence and word scores." ></td>
	<td class="line x" title="134:218	The score (s) of sentence s is defined as follows: (s) = wposppos(s) + nsummationdisplay k=1 wkfk + |s|summationdisplay j=1 msummationdisplay k=1 wkgjk Each of the sentences s in the article was tested against the position feature ppos(s) and against each of the sentence featuresfk, see Section 3.2.1, where pos(s) returns the position of sentence s. Each word j of sentence s is tested against all applicable word features gjk, see Section 3.2.2." ></td>
	<td class="line x" title="135:218	A weight (wpos and wk) is associated with each feature." ></td>
	<td class="line x" title="136:218	How to estimate the weights is discussed next." ></td>
	<td class="line x" title="137:218	3.3 Parameter Estimation There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002)." ></td>
	<td class="line x" title="138:218	We opted for generalized iterative scaling as it is commonly used for other NLP tasks and off-the-shelf implementations exist." ></td>
	<td class="line x" title="139:218	Here we used YASMET.3 3A maximum entropy toolkit by Franz Josef Och, http: //www.fjoch.com/YASMET.html We used a development set of 240 news articles to train YASMET." ></td>
	<td class="line x" title="140:218	As YASMET is a supervised optimizer, we had to generate annotated data on which it was to be trained." ></td>
	<td class="line x" title="141:218	For each document in the development set, we labeled each sentence as to whether a story highlight was generated from it." ></td>
	<td class="line x" title="142:218	For instance, in the article presented in Figure 3, sentences 5, 6, 7, 9 and 21 were marked as highlight sources, whereas all other sentences in the document were not.4 When annotating, all sentences that were directly relevant to the highlights were marked, with preference given to those appearing earlier in the story or containing more precise information." ></td>
	<td class="line x" title="143:218	At this point it is worth noting that while the overlap between different editors is unknown, the highlights were originally written by a number of different people, ensuring enough variation in the data and helping to avoid over-fitting to a specific editor." ></td>
	<td class="line x" title="144:218	4 Experiments and Results The CNN corpus was divided into a training set and a development and test set." ></td>
	<td class="line x" title="145:218	As we had only 300 manually annotated news articles and we wanted to maximize the number of documents usable for parameter estimation, we applied crossfolding, which is commonly used for situations with limited data." ></td>
	<td class="line x" title="146:218	The dev/test set was randomly partitioned into five folds." ></td>
	<td class="line x" title="147:218	Four of the five folds were used as development data (i.e. for parameter estimation with YASMET), while the remaining fold was used for testing." ></td>
	<td class="line x" title="148:218	The procedure was repeated five times, each time with four folds used for development and a separate one for testing." ></td>
	<td class="line x" title="149:218	Cross-folding is safe to use as long as there are no dependencies between the folds, which is safe to assume here." ></td>
	<td class="line x" title="150:218	Some statistics on our training and development/test data can be found in Table 4." ></td>
	<td class="line x" title="151:218	Corpus subset Dev/Test Train Documents 300 1220 Avg." ></td>
	<td class="line x" title="152:218	sentences per article 33.26 31.02 Avg." ></td>
	<td class="line x" title="153:218	sentence length 20.62 20.50 Avg." ></td>
	<td class="line x" title="154:218	number of highlights 3.71 3.67 Avg." ></td>
	<td class="line x" title="155:218	number of highlight sources 4.32 Avg." ></td>
	<td class="line x" title="156:218	highlight length in words 10.26 10.28 Table 4: Characteristics of the evaluation corpus." ></td>
	<td class="line x" title="157:218	4The annotated data set is available at: http://www." ></td>
	<td class="line x" title="158:218	science.uva.nl/christof/data/hl/." ></td>
	<td class="line x" title="159:218	419 Most summarization evaluation campaigns, such as NISTs Document Understanding Conferences (DUC), impose a maximum length on summaries (e.g., 75 characters for the headline generation task or 100 words for the summarization task)." ></td>
	<td class="line x" title="160:218	When identifying sentences from which story highlights are generated, the situation is slightly different, as the number of story highlights is not fixed." ></td>
	<td class="line x" title="161:218	On the other hand, most stories have between three and four highlights, and on average between four and five sentences per story from which the highlights were generated." ></td>
	<td class="line x" title="162:218	This variation led to us to carry out two sets of experiments: In the first experiment (fixed), the number of highlight sources is fixed and our system always returns exactly four highlight sources." ></td>
	<td class="line x" title="163:218	In the second experiment (thresh), our system can return between three and six highlight sources, depending on whether a sentence score passes a given threshold." ></td>
	<td class="line x" title="164:218	The threshold  was used to allocate sentences si of article a to the highlight list HL by first finding the highest-scoring sentence for that article (sh)." ></td>
	<td class="line x" title="165:218	The threshold score was thus (sh) and sentences were judged accordingly." ></td>
	<td class="line x" title="166:218	The algorithm used is given in Figure 3." ></td>
	<td class="line x" title="167:218	initialize HL, sh sort si in s by (si) set sh = s0 for each sentence si in article a: if |HL|< 3 include si else if ((sh)(si))&&(|HL|5) include si else discard si return HL Figure 3: Procedure for selecting highlight sources." ></td>
	<td class="line x" title="168:218	All scores were compared to a baseline, which simply returns the first n sentences of a news article." ></td>
	<td class="line x" title="169:218	n = 4 in the fixed experiment." ></td>
	<td class="line x" title="170:218	For the thresh experiment, the baseline always selected the same number of sentences as AURUM-thresh, but from the beginning of the article." ></td>
	<td class="line x" title="171:218	Although this is a very simple baseline, it is worth reiterating that it is also a very competitive baseline, which most single-document summarization systems fail to beat due to the nature of news articles." ></td>
	<td class="line x" title="172:218	Since we are mainly interested in determining to what extent our system is able to correctly identify the highlight sources, we chose precision and recall as evaluation metrics." ></td>
	<td class="line x" title="173:218	Precision is the percentage of all returned highlight sources which are correct: Precision = |RT||R| where R is the set of returned highlight sources andT is the set of manually identified true sources in the test set." ></td>
	<td class="line x" title="174:218	Recall is defined as the percentage of all true highlight sources that have been correctly identified by the system: Recall = |RT||T| Precision and recall can be combined by using the F-measure, which is the harmonic mean of the two: F-measure = 2(precisionrecall)precision+recall Table 5 shows the results for both experiments (fixed and thresh) as an average over the folds." ></td>
	<td class="line x" title="175:218	To determine whether the observed differences between two approaches are statistically significant and not just caused by chance, we applied statistical significance testing." ></td>
	<td class="line x" title="176:218	As we did not want to make the assumption that the score differences are normally distributed, we used the bootstrap method, a powerful non-parametric inference test (Efron, 1979)." ></td>
	<td class="line x" title="177:218	Improvements at a confidence level of more than 95% are marked with ." ></td>
	<td class="line x" title="178:218	We can see that our approach consistently outperforms the baseline, and most of the improvementsin particular the F-measure scoresare statistically significant at the 0.95 level." ></td>
	<td class="line x" title="179:218	As to be expected, AURUM-fixed achieves higher precision gains, while AURUM-thresh achieves higher recall gains." ></td>
	<td class="line x" title="180:218	In addition, for 83.3 percent of the documents, our systems F-measure score is higher than or equal to that of the baseline." ></td>
	<td class="line x" title="181:218	Figure 4 shows how far down in the documents our system was able to correctly identify highlight sources." ></td>
	<td class="line x" title="182:218	Although the distribution is still heavily skewed towards extracting sentences from the beginning of the document, it is so to a lesser extent than just using positional information as a prior; see Figure 2." ></td>
	<td class="line x" title="183:218	In a third set of experiments we measured the n-gram overlap between the sentences we have identified as highlight sources and the actual story highlights in the ground truth." ></td>
	<td class="line x" title="184:218	To this end we use 420 System Recall Precision F-Measure Extracted Baseline-fixed 40.69 44.14 42.35 240 AURUM-fixed 41.88 (+2.96%) 45.40 (+2.85%) 43.57 (+2.88%) 240 Baseline-thresh 42.91 41.82 42.36 269 AURUM-thresh 44.49 (+3.73%) 43.30 (+3.53%) 43.88 (+3.59%) 269 Table 5: Evaluation scores for the four extraction systems." ></td>
	<td class="line o" title="185:218	System ROUGE-1 ROUGE-2 Baseline-fixed 47.73 15.98 AURUM-fixed 49.20 (+3.09%) 16.53 (+3.63%) Baseline-thresh 55.11 19.31 AURUM-thresh 56.73 (+2.96%) 19.66 (+1.87%) Table 6: ROUGE scores for AURUM-fixed, returning 4 sentences, and AURUM-thresh, returning between 3 and 6 sentences." ></td>
	<td class="line x" title="186:218	Figure 4: Position of correctly extracted sources by AURUM-thresh." ></td>
	<td class="line oc" title="187:218	ROUGE (Lin, 2004), a recall-oriented evaluation package for automatic summarization." ></td>
	<td class="line o" title="188:218	ROUGE operates essentially by comparing n-gram cooccurrences between a candidate summary and a number of reference summaries, and comparing that number in turn to the total number ofn-grams in the reference summaries: ROUGE-n = summationdisplay SReferences summationdisplay ngramnS Match(ngramn) summationdisplay SReferences summationdisplay ngramnS Count(ngramn) Where n is the length of the n-gram, with lengths of 1 and 2 words most commonly used in current evaluations." ></td>
	<td class="line n" title="189:218	ROUGE has become the standard tool for evaluating automatic summaries, though it is not the optimal system for this experiment." ></td>
	<td class="line n" title="190:218	This is due to the fact that it is geared towards a different taskas ours is not automatic summarization per seand that ROUGE works best judging between a number of candidate and model summaries." ></td>
	<td class="line o" title="191:218	The ROUGE scores are shown in Table 6." ></td>
	<td class="line x" title="192:218	Similar to the precision and recall scores, our approach consistently outperforms the baseline, with all but one difference being statistically significant." ></td>
	<td class="line o" title="193:218	Furthermore, in 76.2 percent of the documents, our systems ROUGE-1 score is higher than or equal to that of the baseline, and likewise for 85.2 percent of ROUGE-2 scores." ></td>
	<td class="line o" title="194:218	Our ROUGE scores and their improvements over the baseline are comparable to the results of Svore et al.(2007), who optimized their approach towards ROUGE and gained significant improvements from using third-party data resources, both of which our approach does not require.5 Table 7 shows the unique sentences extracted by every system, which are the number of sentences one system extracted correctly while the other did not; this is thus an intuitive measure of how much two systems differ." ></td>
	<td class="line x" title="196:218	Essentially, a system could simply pick the first two sentences of each article and might thus achieve higher precision scores, since it is less likely to return wrong sentences." ></td>
	<td class="line x" title="197:218	However, if the scores are similar but there is a difference in the number of unique sentences extracted, this means a system has gone beyond the first 4 sentences and extracted others from deeper down inside the text." ></td>
	<td class="line x" title="198:218	To get a better understanding of the importance of the individual features we examined the weights as determined by YASMET." ></td>
	<td class="line x" title="199:218	Table 8 contains example output from the development sets, with feature selection determined implicitly by the weights the MaxEnt model assigns, where non-discriminative features receive a low weight." ></td>
	<td class="line x" title="200:218	5Since the test data of (Svore et al., 2007) is not publicly available we were unable to carry out a more detailed comparison." ></td>
	<td class="line x" title="201:218	421 Clearly, sentence position is of highest importance, while trigram trigger phrases were quite important as well." ></td>
	<td class="line x" title="202:218	Simple bigrams continued to be a good indicator of data value, as is often the case." ></td>
	<td class="line x" title="203:218	Proper nouns proved to be a valuable pointer to new information, but mention of the news agencys name had less of an impact than originally thought." ></td>
	<td class="line x" title="204:218	Other particularly significant features included temporal adjectives, superlatives and all n-gram measures." ></td>
	<td class="line x" title="205:218	System Unique highlight sources Baseline AURUM-fixed 11.8 7.2 AURUM-thresh 14.2 7.6 Table 7: Unique recall scores for the systems." ></td>
	<td class="line x" title="206:218	Feature Weight Feature Weight Sentence pos." ></td>
	<td class="line x" title="207:218	10.23 Superlative 4.15 Proper noun 5.18 Temporal adj." ></td>
	<td class="line x" title="208:218	1.75 Trigger 3-gram 3.70 1-gram score 2.74 Spawn 2-gram 3.73 3-gram score 3.75 CNN mention 1.30 Trigger 2-gram 3.74 Table 8: Typical weights learned from the data." ></td>
	<td class="line x" title="209:218	5 Conclusions A system for extracting essential facts from a news article has been outlined here." ></td>
	<td class="line x" title="210:218	Finding the data nuggets deeper down is a cross between keyphrase extraction and automatic summarization, a task which requires more elaborate features and parameters." ></td>
	<td class="line x" title="211:218	Our approach emphasizes a wide variety of features, including many linguistic features." ></td>
	<td class="line x" title="212:218	These features range from the standard (n-gram frequency), through the essential (sentence position), to the semantic (spawned phrases, verb classes and types of adverbs)." ></td>
	<td class="line x" title="213:218	Our experimental results show that a combination of statistical and linguistic features can lead to competitive performance." ></td>
	<td class="line x" title="214:218	Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of (Svore et al., 2007), without requiring their third-party data resources." ></td>
	<td class="line x" title="215:218	On top of the statistically significant improvements of our approach over the baseline, we see value in the fact that it does not settle for sentences from the beginning of the articles." ></td>
	<td class="line x" title="216:218	Most single-document automatic summarization systems use other features, ranging from discourse structure to lexical chains." ></td>
	<td class="line x" title="217:218	Considering Marcus conclusion (2003) that different approaches should be combined in order to create a good summarization system (aided by machine learning), there seems to be room yet to use basic linguistic cues." ></td>
	<td class="line x" title="218:218	Seeing as how our linguistic featureswhich are predominantly semantic aid in this task, it is quite possible that further integration will aid in both automatic summarization and keyphrase extraction tasks." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1089
Text Summarization Model Based on Maximum Coverage Problem and its Variant
Takamura, Hiroya;Okumura, Manabu;"></td>
	<td class="line x" title="1:254	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 781789, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:254	c2009 Association for Computational Linguistics Text Summarization Model based on Maximum Coverage Problem and its Variant Hiroya Takamura and Manabu Okumura Precision and Intelligence Laboratory, Tokyo Institute of Technology 4259 Nagatsuta Midori-ku Yokohama, 226-8503 takamura@pi.titech.ac.jp oku@pi.titech.ac.jp Abstract We discuss text summarization in terms of maximum coverage problem and its variant." ></td>
	<td class="line x" title="3:254	Weexploresomedecodingalgorithms including the ones never used in this summarization formulation, such as a greedy algorithm with performance guarantee, a randomized algorithm, and a branch-andbound method." ></td>
	<td class="line x" title="4:254	On the basis of the results of comparative experiments, we also augment the summarization model so that it takesintoaccounttherelevancetothedocument cluster." ></td>
	<td class="line x" title="5:254	Through experiments, we showed that the augmented model is superior to the best-performing method of DUC04onROUGE-1withoutstopwords." ></td>
	<td class="line x" title="6:254	1 Introduction Automatic text summarization is one of the tasks that have long been studied in natural language processing." ></td>
	<td class="line x" title="7:254	This task is to create a summary, or a short and concise document that describes the content of a given set of documents (Mani, 2001)." ></td>
	<td class="line x" title="8:254	One well-known approach to text summarization is the extractive method, which selects some linguistic units (e.g., sentences) from given documents in order to generate a summary." ></td>
	<td class="line x" title="9:254	The extractive method has an advantage that the grammaticality is guaranteed at least at the level of the linguistic units." ></td>
	<td class="line x" title="10:254	Since the actual generation of linguistic expressions has not achieved the level of the practical use, we focus on the extractive method in this paper, especially the method based on the sentence extraction." ></td>
	<td class="line x" title="11:254	Most of the extractive summarization methods rely on sequentially solving binary classication problems of determining whether each sentence should be selected or not." ></td>
	<td class="line x" title="12:254	In such sequential methods, however, the viewpoint regarding whether the summary is good as a whole, is not taken into consideration, although a summary conveys information as a whole." ></td>
	<td class="line x" title="13:254	We represent text summarization as an optimization problem and attempt to globally solve the problem." ></td>
	<td class="line x" title="14:254	In particular, we represent text summarization as a maximum coverage problem with knapsack constraint (MCKP)." ></td>
	<td class="line x" title="15:254	One of the advantages of this representation is that MCKP can directly model whether each concept in the given documents is covered by the summary or not, and can dispense with rather counter-intuitive approachessuchasgivingpenaltytoeachpairoftwo similar sentences." ></td>
	<td class="line x" title="16:254	By formally apprehending the target problem, we can use a lot of knowledge and techniques developed in the combinatorial mathematics, and also analyse results more precisely." ></td>
	<td class="line x" title="17:254	In fact, on the basis of the results of the experiments, we augmented the summarization model." ></td>
	<td class="line x" title="18:254	The contributions of this paper are as follows." ></td>
	<td class="line x" title="19:254	Wearenotthersttorepresenttextsummarization as MCKP." ></td>
	<td class="line x" title="20:254	However, no researchers have exploited the decoding algorithms for solving MCKP in the summarization task." ></td>
	<td class="line x" title="21:254	We conduct comprehensive comparative experiments of those algorithms." ></td>
	<td class="line x" title="22:254	Specically, we test the greedy algorithm, the greedy algorithm with performance guarantee, the stack decoding, the linear relaxation problem with randomized decoding, and the branch-andbound method." ></td>
	<td class="line x" title="23:254	On the basis of the experimental results, we then propose an augmented model that takes into account the relevance to the document cluster." ></td>
	<td class="line x" title="24:254	We empirically show that the augmented model is superior to the best-performing method of DUC04 on ROUGE-1 without stopwords." ></td>
	<td class="line x" title="25:254	2 Related Work Carbonell and Goldstein (2000) used sequential sentence selection in combination with maximal marginal relevance (MMR), which gives penalty to sentences that are similar to the already selected sentences." ></td>
	<td class="line x" title="26:254	Schiffman et al.s method (2002) is also based on sequential sentence selection." ></td>
	<td class="line x" title="27:254	Radev et al.(2004), in their method MEAD, used a clustering technique to nd the centroid, that 781 is, the words with high relevance to the topic of the document cluster." ></td>
	<td class="line x" title="29:254	They used the centroid to rank sentences, together with the MMR-like redundancy score." ></td>
	<td class="line x" title="30:254	Both relevance and redundancy are taken into consideration, but no global viewpoint is given." ></td>
	<td class="line x" title="31:254	In CLASSY, which is the best-performing method in DUC04, Conroy et al.(2004) scored sentences with the sum of tf-idf scores of words." ></td>
	<td class="line x" title="33:254	They also incorporated sentence compression based on syntactic or heuristic rules." ></td>
	<td class="line x" title="34:254	McDonald (2007) formulated text summarization as a knapsack problem and obtained the global solution and its approximate solutions." ></td>
	<td class="line x" title="35:254	Its relation to our method will be discussed in Section 6.1." ></td>
	<td class="line x" title="36:254	Filatova and Hatzivassiloglou (2004) rst formulated text summarization as MCKP." ></td>
	<td class="line x" title="37:254	Their decoding method is a greedy one and will be empirically compared with other decoding methods in this paper." ></td>
	<td class="line x" title="38:254	Yih et al.(2007) used a slightlymodied stack decoding." ></td>
	<td class="line x" title="40:254	The optimization problem they solved was the MCKP with the last sentence truncation." ></td>
	<td class="line x" title="41:254	Their stack decoding is one of the decoding methods discussed in this paper." ></td>
	<td class="line x" title="42:254	Ye et al.(2007) is another example of coverage-based methods." ></td>
	<td class="line x" title="44:254	Shen et al.(2007) regarded summarization as a sequential labelling task and solved it with Conditional Random Fields." ></td>
	<td class="line x" title="46:254	Although the model is globally optimized in terms of likelihood, the coverage of concepts is not taken into account." ></td>
	<td class="line x" title="47:254	3 Modeling text summarization In this paper, we focus on the extractive summarization, which generates a summary by selecting linguistic units (e.g., sentences) in given documents." ></td>
	<td class="line x" title="48:254	There are two types of summarization tasks: single-document summarization and multidocument summarization." ></td>
	<td class="line x" title="49:254	While single-document summarization is to generate a summary from a single document, multi-document summarization istogenerateasummaryfrommultipledocuments regarding one topic." ></td>
	<td class="line x" title="50:254	Such a set of multiple documents is called a document cluster." ></td>
	<td class="line x" title="51:254	The method proposed in this paper is applicable to both tasks." ></td>
	<td class="line x" title="52:254	In both tasks, documents are split into several linguistic units D = {s1,,sjDj} in preprocessing." ></td>
	<td class="line x" title="53:254	We will select some linguistic units from D to generate a summary." ></td>
	<td class="line x" title="54:254	Among other linguistic units that can be used in the method, we use sentences so that the grammaticality at the sentence level is going to be guaranteed." ></td>
	<td class="line x" title="55:254	We introduce conceptual units (Filatova and Hatzivassiloglou, 2004), which compose the meaning of a sentence." ></td>
	<td class="line x" title="56:254	Sentence si is represented by a set of conceptual units {ei1,,eijsij}." ></td>
	<td class="line x" title="57:254	For example, the sentence The man bought a book and read it could be regarded as consisting of two conceptualunitsthemanboughtabookandthe man read the book." ></td>
	<td class="line x" title="58:254	It is not easy, however, to determine the appropriate granularity of conceptual units." ></td>
	<td class="line x" title="59:254	A simple way would be to regard the above sentence as consisting of four conceptual units man, book, buy, and read." ></td>
	<td class="line x" title="60:254	There is some work on the denition of conceptual units." ></td>
	<td class="line x" title="61:254	Hovy et al.(2006) proposed to use basic elements, which are dependency subtrees obtained by trimming dependency trees." ></td>
	<td class="line x" title="63:254	Although basic elements were proposed for evaluation of summaries, they can probably be used also for summary generation." ></td>
	<td class="line x" title="64:254	However, such novel units have not proved to be useful for summary generation." ></td>
	<td class="line x" title="65:254	Since we focus more on algorithms and models in this paper, we simply use words as conceptual units." ></td>
	<td class="line x" title="66:254	The goal of text summarization is to cover as many conceptual units as possible using only a small number of sentences." ></td>
	<td class="line x" title="67:254	In other words, the goal is to nd a subset S( D) that covers as many conceptual units as possible." ></td>
	<td class="line x" title="68:254	In the following, we introduce models for that purpose." ></td>
	<td class="line x" title="69:254	We thinkofthesituationthatthesummarylengthmust be at most K (cardinality constraint) and the summary length is measured by the number of words or bytes in the summary." ></td>
	<td class="line x" title="70:254	Let xi denote a variable which is 1 if sentence si is selected, otherwise 0, aij denote a constant which is 1 if sentence si contains word ej, otherwise 0." ></td>
	<td class="line x" title="71:254	We regard word ej as covered when at least one sentence containing ej is selected as part of the summary." ></td>
	<td class="line x" title="72:254	That is, word ej is covered if and only ifi aijxi  1." ></td>
	<td class="line x" title="73:254	Now our objective is to nd the binary assignment onxi with the best coverage such that the summary length is at most K: max." ></td>
	<td class="line x" title="74:254	|{j|i aijxi  1}| s.t. i cixi  K; i,xi  {0,1}, whereci is the cost of selecting si, i.e., the number of words or bytes in si." ></td>
	<td class="line x" title="75:254	For convenience, we rewrite the problem above: max." ></td>
	<td class="line x" title="76:254	j zj s.t. i cixi  K; j,i aijxi  zj; i,xi  {0,1}; j,zj  {0,1}, 782 where zj is 1 when ej is covered, 0 otherwise." ></td>
	<td class="line x" title="77:254	Notice that this new problem is equivalent to the previous one." ></td>
	<td class="line x" title="78:254	Since not all the words are equally important, we introduce weights wj on words ej." ></td>
	<td class="line x" title="79:254	Then the objective is restated as maximizing the weighted sum j wjzj such that the summary length is at most K. This problem is called maximum coverage problem with knapsack constraint (MCKP), which is an NP-hard problem (Khuller et al., 1999)." ></td>
	<td class="line x" title="80:254	We should note that MCKP is different from a knapsack problem." ></td>
	<td class="line x" title="81:254	MCKP merely has a constraint of knapsack form." ></td>
	<td class="line x" title="82:254	Filatova and Hatzivassiloglou (2004) pointed out that text summarization can be formalized by MCKP." ></td>
	<td class="line x" title="83:254	Theperformanceofthemethoddependsonhow to represent words and which words to use." ></td>
	<td class="line x" title="84:254	We represent words with their stems." ></td>
	<td class="line oc" title="85:254	We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004)." ></td>
	<td class="line x" title="86:254	The weights wj of words are also an important factor of good performance." ></td>
	<td class="line x" title="87:254	We tested two weighting schemes proposed by Yih et al.(2007)." ></td>
	<td class="line x" title="89:254	The rst one is interpolated weights, which are interpolated values of the generative word probabilityintheentiredocumentandthatinthebeginning part of the document (namely, the rst 100 words)." ></td>
	<td class="line x" title="90:254	Each probability is estimated with the maximum likelihood principle." ></td>
	<td class="line x" title="91:254	The second one is trained weights." ></td>
	<td class="line x" title="92:254	These values are estimated by the logistic regression trained on data instances, which are labeled 1 if the word appears in a summary in the training dataset, 0 otherwise." ></td>
	<td class="line x" title="93:254	The feature set for the logistic regression includes the frequency of the word in the document cluster and the position of the word instance and others." ></td>
	<td class="line x" title="94:254	4 Algorithms for solving MCKP We explain how to solve MCKP." ></td>
	<td class="line x" title="95:254	We rst explain the greedy algorithm applied to text summarization by Filatova and Hatzivassiloglou (2004)." ></td>
	<td class="line x" title="96:254	We then introduce a greedy algorithm with performance guarantee." ></td>
	<td class="line x" title="97:254	This algorithm has never been appliedtotextsummarization." ></td>
	<td class="line x" title="98:254	Wenextexplainthe stack decoding used by Yih et al.(2007)." ></td>
	<td class="line x" title="100:254	We then introduce an approximate method based on linear relaxation and a randomized algorithm, followed by the branch-and-bound method, which provides the exact solution." ></td>
	<td class="line x" title="101:254	Although the algorithms used in this paper themselves are not novel, this work is the rst to apply the greedy algorithm with performance guarantee, the randomized algorithm, and the branch-and-bound to solve the MCKP and automatically create a summary." ></td>
	<td class="line x" title="102:254	In addition, we conduct a comparative study on summarization algorithms including the above." ></td>
	<td class="line x" title="103:254	There are some other well-known methods for similar problems (e.g., the method of conditional probability (Hromkovic, 2003))." ></td>
	<td class="line x" title="104:254	A pipage approach (Ageev and Sviridenko, 2004) has been proposed for MCKP, but we do not use this algorithm, since it requires costly partial enumeration and solutions to many linear relaxation problems." ></td>
	<td class="line x" title="105:254	As in the previous section, D denotes the set of sentences {s1,,sjDj}, and S denotes a subset of D and thus represents a summary." ></td>
	<td class="line x" title="106:254	4.1 Greedy algorithm Filatova and Hatzivassiloglou (2004) used a greedy algorithm." ></td>
	<td class="line x" title="107:254	In this section, Wl denotes the sum of the weights of the words covered by sentence sl." ></td>
	<td class="line x" title="108:254	W0l denotes the sum of the weights of the words covered by sl, but not by current summary S. This algorithm sequentially selects sentence sl with the largest W0l." ></td>
	<td class="line x" title="109:254	Greedy Algorithm U  D, S  ` while U 6= ` si  argmaxsl2U W0l if ci +sl2S cl  K then insert si into S delete si in U end while output S. This algorithm has performance guarantee when the problem has a unit cost (i.e., when each sentence has the same length), but no performance guarantee for the general case where costs can have different values." ></td>
	<td class="line x" title="110:254	4.2 Greedy algorithm with performance guarantee We describe a greedy algorithm with performance guaranteeproposedbyKhulleretal.(1999),which proves to achieve an approximation factor of (1  1/e)/2 for MCKP." ></td>
	<td class="line x" title="111:254	This algorithm sequentially selects sentence sl with the largest ratio W0l/cl. After the sequential selection, the set of the selected sentences is compared with the single-sentence summary that has the largest value of the objective function." ></td>
	<td class="line x" title="112:254	The larger of the two is going to 783 be the output of this new greedy algorithm." ></td>
	<td class="line x" title="113:254	Here score(S) is j wjzj, the value of the objective function for summary S. Greedy Algorithm with Performance Guarantee U  D, S  ` while U 6= ` si  argmaxsl2U W0l/cl if ci +sl2S cl  K then insert si into S delete si in U end while st  argmaxsl Wl if score(S)  Wt, output S, otherwise, output {st}." ></td>
	<td class="line x" title="114:254	They also proposed an algorithm with a better performance guarantee, which is not used in this paper because it is costly due to its partial enumeration." ></td>
	<td class="line x" title="115:254	4.3 Stack decoding Stack decoding is a decoding method proposed by Jelinek (1969)." ></td>
	<td class="line x" title="116:254	This algorithm requires K priority queues, k-th of which is the queue for summaries of length k. The objective function value is used for the priority measure." ></td>
	<td class="line x" title="117:254	A new solution (summary) is generated by adding a sentence to a current solution in k-th queue and inserted into a succeeding queue.1 The pop operation in stack decoding pops the candidate summary with the least priority in the queue." ></td>
	<td class="line x" title="118:254	By restricting the size of eachqueuetoacertainconstantstacksize, wecan obtain an approximate solution within a practical computational time." ></td>
	<td class="line x" title="119:254	Stack Decoding for k = 0 to K 1 for each S  queues[k] for each sl  D insert sl into S insert S into queues[k + cl] pop if queue-size exceeds the stacksize end for end for end for return the best solution in queues[K] 4.4 Randomized algorithm Khuller et al.(2006) proposed a randomized algorithm (Hromkovic, 2003) for MCKP." ></td>
	<td class="line x" title="121:254	In this algorithm, a relaxation linear problem is generated by replacing the integer constraints xi  {0,1} 1We should be aware that stack in a strict data-structure sense is not used in the algorithm." ></td>
	<td class="line x" title="122:254	and zj  {0,1} with linear constraints xi  [0,1] and zj  [0,1]." ></td>
	<td class="line x" title="123:254	The optimal solution xi to the relaxation problem is regarded as the probability of sentence si being selected as a part of summary: xi = P(xi = 1)." ></td>
	<td class="line x" title="124:254	The algorithm randomly selects sentence si with probability xi, in order to generate a summary." ></td>
	<td class="line x" title="125:254	It has been proved that the expected length of each randomly-generated summary is upper-bounded by K, and the expected value of the objective function is at least the optimal value multiplied by (11/e) (Khuller et al., 2006)." ></td>
	<td class="line x" title="126:254	This random generation of a summary is iterated many times, and the summaries that are not longer than K are stored as candidate summaries." ></td>
	<td class="line x" title="127:254	Among those many candidate summaries, the one with the highest value of the objective function is going to be the output by this algorithm." ></td>
	<td class="line x" title="128:254	4.5 Branch-and-bound method The branch-and-bound method (Hromkovic, 2003) is an efcient method for nding the exact solutions to integer problems." ></td>
	<td class="line x" title="129:254	Since MCKP is an NP-hard problem, it cannot generally be solved in polynomial time under a reasonable assumption that NP6=P. However, if the size of the problem is limited, sometimes we can obtain the exact solution within a practical time by means of the branch-and-bound method." ></td>
	<td class="line oc" title="130:254	4.6 Weakly-constrained algorithms In evaluation with ROUGE (Lin, 2004), summaries are truncated to a target length K. Yih et al.(2007)usedastackdecodingwithaslightmodication, which allows the last sentence in a summary to be truncated to a target length." ></td>
	<td class="line x" title="131:254	Let us call this modied algorithm the weakly-constrained stack decoding." ></td>
	<td class="line x" title="132:254	The weakly-constrained stack decoding can be implemented simply by replacing queues[k +cl] with queues[min(k +cl,K)]." ></td>
	<td class="line x" title="133:254	We can also think of weakly-constrained versions of the greedy and randomized algorithms introduced before." ></td>
	<td class="line x" title="134:254	In this paper, we do not adopt weaklyconstrained algorithms, because although an advantage of the extractive summarization is the guaranteed grammaticality at the sentence level, thesummarieswithatruncatedsentencewillrelinquish this advantage." ></td>
	<td class="line x" title="135:254	We mentioned the weaklyconstrained algorithms in order to explain the relation between the proposed model and the model proposed by Yih et al.(2007)." ></td>
	<td class="line x" title="137:254	784 5 Experiments and Discussion 5.1 Experimental Setting We conducted experiments on the dataset of DUC04 (2004) with settings of task 2, which is a multi-document summarization task." ></td>
	<td class="line x" title="138:254	50 document clusters, each of which consists of 10 documents, are given." ></td>
	<td class="line x" title="139:254	One summary is to be generated for each cluster." ></td>
	<td class="line x" title="140:254	Following the most relevant previous method (Yih et al., 2007), we set the target length to 100 words." ></td>
	<td class="line x" title="141:254	DUC03 (2003) dataset was used as the training dataset for trained weights." ></td>
	<td class="line x" title="142:254	All the documents were segmented into sentences using a script distributed by DUC." ></td>
	<td class="line x" title="143:254	Words are stemmed by Porters stemmer (Porter, 1980)." ></td>
	<td class="line pc" title="144:254	ROUGE version 1.5.5 (Lin, 2004) was used for evaluation.2 Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation (Lin, 2004; Lin and Hovy, 2003)." ></td>
	<td class="line o" title="145:254	Wilcoxon signed rank test for paired samples with signicance level 0.05 was used for the signicance test of the difference in ROUGE1." ></td>
	<td class="line x" title="146:254	The simplex method and the branch-and-bound method implemented in GLPK (Makhorin, 2006) were used to solve respectively linear and integer programming problems." ></td>
	<td class="line x" title="147:254	The methods that are compared here are the greedy algorithm (greedy), the greedy algorithm with performance guarantee (g-greedy), the randomized algorithm (rand), the stack decoding (stack), and the branch-and-bound method (exact)." ></td>
	<td class="line x" title="148:254	5.2 Results The experimental results are shown in Tables 1 and 2." ></td>
	<td class="line o" title="149:254	The columns 1, 2, and SU4 in the tables respectively refer to ROUGE-1, ROUGE-2, and ROUGE-SU4." ></td>
	<td class="line x" title="150:254	In addition, rand100k refers to therandomizedalgorithmwith100,000randomlygenerated solution candidates, and stack30 refers to stack with the stacksize being 30." ></td>
	<td class="line x" title="151:254	The rightmost column (time) shows the average computational time required for generating a summary for a document cluster." ></td>
	<td class="line x" title="152:254	Both with interpolated (Table 1) and trained weights (Table 2), g-greedy signicantly outperformed greedy." ></td>
	<td class="line x" title="153:254	With interpolated weights, there was no signicant difference between exact and g-greedy, and between exact and stack30." ></td>
	<td class="line x" title="154:254	With trained weights, there was no signicant differ2With options -n 4 -m -2 4 -u -f A -p 0.5 -l 100 -t 0 -d -s." ></td>
	<td class="line o" title="155:254	Table 1: ROUGE of MCKP with interpolated weights." ></td>
	<td class="line o" title="156:254	Underlined ROUGE-1 scores are significantly different from the score of exact." ></td>
	<td class="line x" title="157:254	Computational time was measured in seconds." ></td>
	<td class="line o" title="158:254	ROUGE time 1 2 SU4 (sec) greedy 0.283 0.083 0.123 <0.01 g-greedy 0.294 0.080 0.121 0.01 rand100k 0.300 0.079 0.119 1.88 stack30 0.304 0.078 0.120 4.53 exact 0.305 0.081 0.121 4.04 Table 2: ROUGE of MCKP with trained weights." ></td>
	<td class="line o" title="159:254	Underlined ROUGE-1 scores are signicantly differentfromthescoreof exact." ></td>
	<td class="line x" title="160:254	Computationaltime was measured in seconds." ></td>
	<td class="line o" title="161:254	ROUGE time 1 2 SU4 (sec) greedy 0.283 0.080 0.121 < 0.01 g-greedy 0.310 0.077 0.118 0.01 rand100k 0.299 0.077 0.117 1.93 stack30 0.309 0.080 0.120 4.23 exact 0.307 0.078 0.119 4.56 ence between exact and the other algorithms except for greedy and rand100k." ></td>
	<td class="line o" title="162:254	The result suggests that approximate fast algorithms can yield results comparable to the exact method in terms of ROUGE-1 score." ></td>
	<td class="line x" title="163:254	We will later discuss the results in terms of objective function values and search errors in Table 4." ></td>
	<td class="line x" title="164:254	We should notice that stack outperformed exact with interpolated weights." ></td>
	<td class="line x" title="165:254	To examine this counter-intuitive point, we changed the stacksize of stack with interpolated weights (inter) and trained weights (train) from 10 to 100 and obtained Table 3." ></td>
	<td class="line o" title="166:254	This table shows that the ROUGE1 value does not increase as the stacksize does; ROUGE-1 for stack with interpolated weights does not change much with the stacksize, and the peak of ROUGE-1 for trained weights is at the stacksize of 20." ></td>
	<td class="line x" title="167:254	Since stack with a larger stacksize selects a solution from a larger number of solution candidates, this result is counter-intuitive in the sense that non-global decoding by stack has a favorable effect." ></td>
	<td class="line o" title="168:254	We also counted the number of the document clusters for which an approximate algorithm with interpolated weights yielded the same solution as 785 Table 3: ROUGE of stack with various stacksizes size 10 20 30 50 100 inter 0.304 0.304 0.304 0.304 0.303 train 0.308 0.310 0.309 0.308 0.307 Table 4: Search errors of MCKP with interpolated weights solution same search error ROUGE (=) =   greedy 0 1 35 14 g-greedy 0 5 26 19 rand100k 6 5 25 14 stack30 16 11 8 11 exact (same solution column in Table 4)." ></td>
	<td class="line o" title="169:254	If the approximate algorithm failed to yield the exact solution (search error column), we checked whether the search error made ROUGE score unchanged (= column), decreased ( column), or increased ( column) compared with ROUGE score of exact." ></td>
	<td class="line o" title="170:254	Table 4 shows that (i) stack30 is a better optimizer than other approximate algorithms, (ii) when the search error occurs, stack30 increases ROUGE-1 more often than it decreases ROUGE-1 compared with exact in spite of stack30s inaccurate solution, (iii) approximate algorithms sometimes achieved better ROUGE scores." ></td>
	<td class="line x" title="171:254	We observed similar phenomena for trained weights, though we skip the details due to space limitation." ></td>
	<td class="line x" title="172:254	These observations on stacksize and search errorssuggestthatthereexistsanothermaximization problem that is more suitable to summarization." ></td>
	<td class="line x" title="173:254	We should attempt to nd the more suitable maximization problem and solve it using some existing optimization and approximation techniques." ></td>
	<td class="line x" title="174:254	6 Augmentation of the model On the basis of the experimental results in the previous section, we augment our text summarization model." ></td>
	<td class="line x" title="175:254	We rst examine the current model more carefully." ></td>
	<td class="line x" title="176:254	As mentioned before, we used words as conceptual units because dening those units is hard and still under development by many researchers." ></td>
	<td class="line x" title="177:254	Suppose here that a more suitable unit has more detailed information, such as A did B to C." ></td>
	<td class="line x" title="178:254	Then the event A did D to E is a completely different unit from A did B to C." ></td>
	<td class="line x" title="179:254	However, when words are used as conceptual units, the two events have a redundant part A." ></td>
	<td class="line x" title="180:254	It can happen that a document is concise as a summary, but redundant on word level." ></td>
	<td class="line x" title="181:254	By being to some extent redundant on the word level, a summary can have sentences that are more relevant to the document cluster, as both of the sentences above are relevant to the document cluster if the document cluster is about A." ></td>
	<td class="line x" title="182:254	A summary with high cohesion and coherencewouldhaveredundancytosomeextent." ></td>
	<td class="line x" title="183:254	In thissection, wewillusethisconjecturetoaugment our model." ></td>
	<td class="line x" title="184:254	6.1 Augmented summarization model The objective function of MCKP consists of only one term that corresponds to coverage." ></td>
	<td class="line x" title="185:254	We add another term i(j wjaij)xi that corresponds to relevance to the topic of the document cluster." ></td>
	<td class="line x" title="186:254	We represent the relevance of sentence si by the sum of the weights of words in the sentence (j wjaij)." ></td>
	<td class="line x" title="187:254	We take the summation of the relevance values of the selected sentences: max." ></td>
	<td class="line x" title="188:254	(1)j wjzj + i(j wjaij)xi s.t. i cixi  K; j,i aijxi  zj; i,xi  {0,1}; j,zj  {0,1}, where  is a constant." ></td>
	<td class="line x" title="189:254	We call this model MCKPRel, because the relevance to the document cluster is taken into account." ></td>
	<td class="line x" title="190:254	We discuss the relation to the model proposed by McDonald (2007), whose objective function consists of a relevance term and a negative redundancy term." ></td>
	<td class="line x" title="191:254	We believe that MCKP-Rel is more intuitive and suitable for summarization, because coverage in McDonald (2007) is measured by subtracting the redundancy represented with the sum of similarities between two sentences, while MCKP-Rel focuses directly on coverage." ></td>
	<td class="line x" title="192:254	Suppose sentence s1 contains conceptual units A and B, s2 contains A, and s3 contains B. The proposed coverage-based methods can capture the fact that s1 has the same information as {s2,s3}, while similarity-based methods only learn that s1 is somewhat similar to each of s2 and s3." ></td>
	<td class="line o" title="193:254	We also empirically showed that our method outperforms McDonald (2007)s method in experiments on DUC02, where our method achieved 0.354 ROUGE-1 score with interpolated weights and 0.359 with trained weights when the optimal  is given, while McDonald (2007)s method yielded at most 0.348." ></td>
	<td class="line o" title="194:254	However, this very point can also 786 Table 5: ROUGE-1 of MCKP-Rel with interpolated weights." ></td>
	<td class="line x" title="195:254	The values in the parentheses are the corresponding values of  predicted using DUC03 as development data." ></td>
	<td class="line x" title="196:254	Underlined are the values that are signicantly different from the corresponding values of MCKP." ></td>
	<td class="line x" title="197:254	interpolated trained greedy 0.287 (0.1) 0.288 (0.8) g-greedy 0.307 (0.3) 0.320 (0.4) rand100k 0.310 (0.1) 0.316 (0.5) stack30 0.324 (0.1) 0.327 (0.3) exact 0.320 (0.3) 0.329 (0.5) exactopt 0.327 (0.2) 0.329 (0.5) be a drawback of our method, since our method premises that a sentence is represented as a set of conceptual units." ></td>
	<td class="line x" title="198:254	Similarity-based methods are free from such a premise." ></td>
	<td class="line x" title="199:254	Taking advantages of both models is left for future work." ></td>
	<td class="line x" title="200:254	The decoding algorithms introduced before are alsoapplicabletoMCKP-Rel, becauseMCKP-Rel can be reduced to MCKP by adding, for each sentence si, a dummy conceptual unit which exists only in si and has the weightj wjaij." ></td>
	<td class="line x" title="201:254	6.2 Experiments of the augmented model We ran greedy, g-greedy, rand100k, stack30 and exact to solve MCKP-Rel." ></td>
	<td class="line x" title="202:254	We experimented on DUC04 with the same experimental setting as the previous ones." ></td>
	<td class="line x" title="203:254	6.2.1 Experiments with the predicted  We determined the value of  for each method using DUC03 as development data." ></td>
	<td class="line o" title="204:254	Specically, we conducted experiments on DUC03 with different  ( {0.0,0.1,,1.0}) and simply selected the one with the highest ROUGE-1 value." ></td>
	<td class="line x" title="205:254	The results with these predicted  are shown in Table 5." ></td>
	<td class="line o" title="206:254	Only ROUGE-1 values are shown." ></td>
	<td class="line x" title="207:254	Method exactopt is exact with the optimal , and can be regarded as the upperbound of MCKP-Rel." ></td>
	<td class="line x" title="208:254	To evaluate the appropriateness of models without regard to search quality, we rst focused on exact and found that MCKP-Rel outperformed MCKP with exact." ></td>
	<td class="line x" title="209:254	This means that MCKP-Rel model is superior to MCKP model." ></td>
	<td class="line x" title="210:254	Among the algorithms, stack30 and exact performed well." ></td>
	<td class="line o" title="211:254	All methods except for greedy yielded signicantly better ROUGE values compared with the corresponding results in Tables 1 and 2." ></td>
	<td class="line o" title="212:254	Figures 1 and 2 show ROUGE-1 for different values of ." ></td>
	<td class="line x" title="213:254	The leftmost part ( = 0.0) corresponds to MCKP." ></td>
	<td class="line x" title="214:254	We can see from the gures, that MCKP-Rel at the best  always outperforms MCKP, and that MCKP-Rel tends to degrade for very large ." ></td>
	<td class="line x" title="215:254	This means that excessive weight on relevancehasanadversativeeffectonperformance and therefore the coverage is important." ></td>
	<td class="line o" title="216:254	0.28  0.29  0.3  0.31  0.32  0.33  0.34  0  0.2  0.4  0.6  0.8  1 ROUGE-1 lambda exactstack30 rand100kg-greedy greedy Figure 1: MCKP-Rel with interpolated weights  0.28  0.29  0.3  0.31  0.32  0.33  0.34  0  0.2  0.4  0.6  0.8  1 ROUGE-1 lambda exactstack30 rand100kg-greedy greedy Figure 2: MCKP-Rel with trained weights 6.2.2 Experiments with the optimal  In the experiments above, we found that  = 0.2 is the optimal value for exact with interpolated weights." ></td>
	<td class="line x" title="217:254	We suppose that this  gives the best model, and examined search errors as we did in Section 5.2." ></td>
	<td class="line o" title="218:254	We obtained Table 6, which shows that search errors in MCKP-Rel counterintuitively increase () ROUGE-1 score less often than MCKP did in Table 4." ></td>
	<td class="line x" title="219:254	This was the case also for trained weights." ></td>
	<td class="line x" title="220:254	This result suggests that MCKP-Rel is more suitable to text summarization than MCKP is. However, exact with trained weights at the optimal (= 0.4) in Figure 2 was outperformed by stack30." ></td>
	<td class="line x" title="221:254	It suggests that there is still room for future improvement in the model." ></td>
	<td class="line x" title="222:254	787 Table 6: Search errors of MCKP-Rel with interpolated weights ( = 0.2)." ></td>
	<td class="line o" title="223:254	solution same search error ROUGE (=) =   greedy 0 2 42 6 g-greedy 1 0 34 15 rand100k 3 6 33 8 stack30 14 13 14 10 6.2.3 Comparison with DUC results In Section 6.2.1, we empirically showed that the augmented model MCKP-Rel is better than MCKP, whose optimization problem is used also in one of the state-of-the-art methods by Yih et al.(2007)." ></td>
	<td class="line x" title="225:254	It would also be benecial to readers to directly compare our method with DUC results." ></td>
	<td class="line x" title="226:254	For that purpose, we conducted experiments with the cardinality constraint of DUC04, i.e., each summary should be 665 bytes long or shorter." ></td>
	<td class="line x" title="227:254	Other settings remained unchanged." ></td>
	<td class="line o" title="228:254	We compared the MCKP-Rel with peer65 (Conroy et al., 2004) of DUC04, which performed best in terms of ROUGE-1 in the competition." ></td>
	<td class="line o" title="229:254	Tables 7 and 8 are the ROUGE-1 scores, respectively evaluatedwithoutandwithstopwords." ></td>
	<td class="line x" title="230:254	Thelatteristhe ofcial evaluation measure of DUC04." ></td>
	<td class="line o" title="231:254	Table 7: ROUGE-1 of MCKP-Rel with byte constraints, evaluated without stopwords." ></td>
	<td class="line x" title="232:254	Underlined are the values signicantly different from peer65." ></td>
	<td class="line o" title="233:254	interpolated train greedy 0.289 (0.1) 0.284 (0.8) g-greedy 0.297 (0.4) 0.323 (0.3) rand100k 0.315 (0.2) 0.308 (0.4) stack30 0.324 (0.2) 0.323 (0.3) exact 0.325 (0.3) 0.326 (0.5) exactopt 0.325 (0.3) 0.329 (0.4) peer65 0.309 In Table 7, MCKP-Rel with stack30 and exact yielded signicantly better ROUGE-1 scores than peer65." ></td>
	<td class="line o" title="234:254	Although stack30 and exact yielded greater ROUGE-1 scores than peer65 also in Table 8, the difference was not signicant." ></td>
	<td class="line o" title="235:254	Only greedy was signicantly worse than peer65.3 One 3We actually succeeded in greatly improving the ROUGE-1 value of MCKP-Rel evaluated with stopwords by using all the words including stopwords as conceptual units." ></td>
	<td class="line o" title="236:254	However, we ignore those results in this paper, because it Table 8: ROUGE-1 of MCKP-Rel with byte constraints, evaluated with stopwords." ></td>
	<td class="line x" title="237:254	Underlined are the values signicantly different from peer65." ></td>
	<td class="line x" title="238:254	interpolated train greedy 0.374 (0.1) 0.377 (0.4) g-greedy 0.371 (0.0) 0.385 (0.2) rand100k 0.373 (0.2) 0.366 (0.3) stack30 0.384 (0.1) 0.386 (0.3) exact 0.383 (0.3) 0.384 (0.4) exactopt 0.385 (0.1) 0.384 (0.4) peer65 0.382 possibleexplanationonthedifferencebetweenTable 7 and Table 8 is that peer65 would probably be tuned to the evaluation with stopwords, since it is the ofcial setting of DUC04." ></td>
	<td class="line x" title="239:254	From these results, we can conclude that the MCKP-Rel is at least comparable to the bestperforming method, if we choose a powerful decoding method, such as stack and exact." ></td>
	<td class="line x" title="240:254	7 Conclusion We regarded text summarization as MCKP." ></td>
	<td class="line x" title="241:254	We applied some algorithms to solve the MCKP and conducted comparative experiments." ></td>
	<td class="line x" title="242:254	We conducted comparative experiments." ></td>
	<td class="line x" title="243:254	We also augmented our model to MCKP-Rel, which takes into consideration the relevance to the document cluster and performs well." ></td>
	<td class="line x" title="244:254	For future work, we will try other conceptual units such as basic elements (Hovy et al., 2006) proposed for summary evaluation." ></td>
	<td class="line x" title="245:254	We also plan to include compressed sentences into the set of candidate sentences to be selected as done by Yih et al.(2007)." ></td>
	<td class="line x" title="247:254	We also plan to design other decodingalgorithmsfortextsummarization(e.g., pipage approach (Ageev and Sviridenko, 2004))." ></td>
	<td class="line x" title="248:254	As discussed in Section 6.2, integration with similaritybased models is worth consideration." ></td>
	<td class="line x" title="249:254	We will incorporate techniques for arranging sentences into an appropriate order, while the current work concerns only selection." ></td>
	<td class="line x" title="250:254	Deshpande et al.(2007) proposed a selection and ordering technique, which is applicable only to the unit cost case such as selection and ordering of words for title generation." ></td>
	<td class="line x" title="252:254	We plan to rene their model so that it can be applied to general text summarization." ></td>
	<td class="line x" title="253:254	just trickily uses non-content words to increase the evaluation measure, disregarding the actual quality of summaries." ></td>
	<td class="line x" title="254:254	788" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1041
Exploring Content Models for Multi-Document Summarization
Haghighi, Aria;Vanderwende, Lucy;"></td>
	<td class="line x" title="1:207	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 362370, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:207	c 2009 Association for Computational Linguistics Exploring Content Models for Multi-Document Summarization Aria Haghighi UC Berkeley, CS Division aria42@cs.berkeley.edu Lucy Vanderwende Microsoft Research Lucy.Vanderwende@microsoft.com Abstract We present an exploration of generative probabilistic models for multi-document summarization." ></td>
	<td class="line x" title="3:207	Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representationofdocumentsetcontentandexhibiting ROUGE gains along the way." ></td>
	<td class="line x" title="4:207	Our final model, HIERSUM, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions." ></td>
	<td class="line o" title="5:207	At the task of producing generic DUC-style summaries, HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation stronglyoutperformsToutanovaetal.(2007)s state-of-the-art discriminative system." ></td>
	<td class="line x" title="6:207	We also explore HIERSUMs capacity to produce multiple topical summaries in order to facilitate content discovery and navigation." ></td>
	<td class="line x" title="7:207	1 Introduction Over the past several years, there has been much interest in the task of multi-document summarization." ></td>
	<td class="line x" title="8:207	In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary.1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 1In this work, we ignore the summary focus." ></td>
	<td class="line x" title="9:207	Here, the word topic will refer to elements of our statistical model rather than summary focus." ></td>
	<td class="line x" title="10:207	2Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007)." ></td>
	<td class="line x" title="11:207	Another strand of work (Barzilay and Lee, 2004; Daume III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content." ></td>
	<td class="line x" title="12:207	However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization." ></td>
	<td class="line x" title="13:207	In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2)." ></td>
	<td class="line x" title="14:207	We also contend that they provide convenient building blocks for adding more structure to a summarization model." ></td>
	<td class="line x" title="15:207	In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific subtopics within a document set." ></td>
	<td class="line x" title="16:207	The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics." ></td>
	<td class="line x" title="17:207	2 Experimental Setup The task we will consider is extractive multidocument summarization." ></td>
	<td class="line x" title="18:207	In this task we assume a document collection D consisting of documents D1,,Dn describing the same (or closely related) narrative (Lapata, 2003)." ></td>
	<td class="line x" title="19:207	362 set of events." ></td>
	<td class="line x" title="20:207	Our task will be to propose a summarySconsisting of sentences in D totaling at most L words.3 Here as in much extractive summarization, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1)." ></td>
	<td class="line x" title="21:207	The most prevalent example of this data setting is document clusters found on news aggregator sites." ></td>
	<td class="line x" title="22:207	2.1 Automated Evaluation For model development we will utilize the DUC 2006 evaluation set4 consisting of 50 document sets each with 25 documents; final evaluation will utilize the DUC 2007 evaluation set (section 5)." ></td>
	<td class="line oc" title="23:207	Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin, 2004) which representsrecallovervariousn-gramsstatisticsfrom asystem-generatedsummaryagainstasetofhumangenerated peer summaries.5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries." ></td>
	<td class="line x" title="24:207	In particular, we utilize R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams)6." ></td>
	<td class="line x" title="25:207	WepresentR-2withoutstopwordsinthe running text, but full development results are presented in table 1." ></td>
	<td class="line oc" title="26:207	Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004)." ></td>
	<td class="line x" title="27:207	In addition to presenting automated results, we also present a user evaluation in section 5.2." ></td>
	<td class="line x" title="28:207	3 Summarization Models We present a progression of models for multidocument summarization." ></td>
	<td class="line x" title="29:207	Inference details are given in section 4." ></td>
	<td class="line x" title="30:207	3.1 SumBasic The SUMBASIC algorithm, introduced in Nenkova and Vanderwende (2005), is a simple effective procedure for multi-document extractive summarization." ></td>
	<td class="line x" title="31:207	Its design is motivated by the observation that the relative frequency of a non-stop word in a document set is a good predictor of a word appearing in a human summary." ></td>
	<td class="line x" title="32:207	In SUMBASIC, each sentence 3For DUC summarization tasks, L is typically 250." ></td>
	<td class="line x" title="33:207	4http://www-nlpir.nist.gov/projects/duc/data.html 5All words from peer and proposed summaries are lowercased and stemmed." ></td>
	<td class="line x" title="34:207	6Bigrams formed by skipping at most two words." ></td>
	<td class="line x" title="35:207	S is assigned a score reflecting how many highfrequency words it contains, Score(S) = summationdisplay wS 1 |S|PD(w) (1) where PD() initially reflects the observed unigram probabilities obtained from the document collection D. A summary S is progressively built by adding the highest scoring sentence according to (1).7 In order to discourage redundancy, the words in the selected sentence are updated PnewD (w)  PoldD (w)2." ></td>
	<td class="line x" title="36:207	Sentences are selected in this manner until the summary word limit has been reached." ></td>
	<td class="line o" title="37:207	Despite its simplicity, SUMBASIC yields 5.3 R-2 without stop words on DUC 2006 (see table 1).8 By comparison, the highest-performing ROUGE system at the DUC 2006 evaluation, SUMFOCUS, was built on top of SUMBASIC and yielded a 6.0, which is not a statistically significant improvement (Vanderwende et al., 2007).9 Intuitively, SUMBASIC is trying to select a summary which has sentences where most words have high likelihood under the document set unigram distribution." ></td>
	<td class="line x" title="38:207	One conceptual problem with this objective is that it inherently favors repetition of frequent non-stop words despite the squaring update." ></td>
	<td class="line x" title="39:207	Ideally, asummarizationcriterionshouldbemorerecall oriented, penalizing summaries which omit moderately frequent document set words and quickly diminishing the reward for repeated use of word." ></td>
	<td class="line x" title="40:207	Another more subtle shortcoming is the use of the raw empirical unigram distribution to represent content significance." ></td>
	<td class="line x" title="41:207	For instance, there is no distinction between a word which occurs many times in the same document or the same number of times across several documents." ></td>
	<td class="line x" title="42:207	Intuitively, the latter word is more indicative of significant document set content." ></td>
	<td class="line x" title="43:207	3.2 KLSum The KLSUM algorithm introduces a criterion for selecting a summary S given document collection D, S = min S:words(S)L KL(PDbardblPS) (2) 7Note that sentence order is determined by the order in which sentences are selected according to (1)." ></td>
	<td class="line oc" title="44:207	8This result is presented as 0.053 with the official ROUGE scorer (Lin, 2004)." ></td>
	<td class="line x" title="45:207	Results here are scaled by 1,000." ></td>
	<td class="line o" title="46:207	9To be fair obtaining statistical significance in ROUGE scores is quite difficult." ></td>
	<td class="line x" title="47:207	363  B Z W  C  D  t Document Set Document Sentence Word Figure 1: Graphical model depiction of TOPICSUM model (see section 3.3)." ></td>
	<td class="line x" title="48:207	Note that many hyperparameter dependencies are omitted for compactness." ></td>
	<td class="line x" title="49:207	where PS is the empirical unigram distribution of the candidate summary S and KL(PbardblQ) represents the Kullback-Lieber (KL) divergence given bysummationtext wP(w)log P(w) Q(w)." ></td>
	<td class="line x" title="50:207	10 This quantity represents the divergence between the true distributionP (here the document set unigram distribution) and the approximating distribution Q (the summary distribution)." ></td>
	<td class="line x" title="51:207	This criterion casts summarization as finding a set of summary sentences which closely match the document set unigram distribution." ></td>
	<td class="line x" title="52:207	Lin et al.(2006) propose a related criterion for robust summarization evaluation, but to our knowledge this criteria has been unexplored in summarization systems." ></td>
	<td class="line x" title="54:207	We address optimizing equation (2) as well as summary sentence ordering in section 4." ></td>
	<td class="line x" title="55:207	KLSUM yields 6.0 R-2 without stop words, beating SUMBASIC butnotwithstatisticalsignificance." ></td>
	<td class="line x" title="56:207	It is worth noting however that KLSUMs performance matches SUMFOCUS (Vanderwende et al., 2007), the highest R-2 performing system at DUC 2006." ></td>
	<td class="line x" title="57:207	3.3 TopicSum As mentioned in section 3.2, the raw unigram distribution PD() may not best reflect the content of D for the purpose of summary extraction." ></td>
	<td class="line x" title="58:207	We propose TOPICSUM, which uses a simple LDA-like topic model (Blei et al., 2003) similar to Daume III and Marcu (2006) to estimate a content distribu10In order to ensure finite values of KL-divergence we smoothe PS() so that it has a small amount of mass on all document set words." ></td>
	<td class="line o" title="59:207	System ROUGE-stop ROUGEall R-1 R-2 R-SU4 R-1 R-2 R-SU4 SUMBASIC 29.6 5.3 8.6 36.1 7.1 12.3 KLSUM 30.6 6.0 8.9 38.9 8.3 13.7 TOPICSUM 31.7 6.3 9.1 39.2 8.4 13.6 HIERSUM 30.5 6.4 9.2 40.1 8.6 14.3 Table 1: ROUGE results on DUC2006 for models presented in section 3." ></td>
	<td class="line x" title="60:207	Results in bold represent results statistically significantly different from SUMBASIC in the appropriate metric." ></td>
	<td class="line x" title="61:207	tion for summary extraction.11 We extract summary sentences as before using the KLSUM criterion (see equation (2)), plugging in a learned content distribution in place of the raw unigram distribution." ></td>
	<td class="line x" title="62:207	First, we describe our topic model (see figure 1) which generates a collection of document sets." ></td>
	<td class="line x" title="63:207	We assume a fixed vocabularyV:12 1." ></td>
	<td class="line x" title="64:207	Draw a background vocabulary distributionB from DIRICHLET(V,B) shared across documentcollections13 representingthebackground distribution over vocabulary words." ></td>
	<td class="line x" title="65:207	This distribution is meant to flexibly model stop words which do not contribute content." ></td>
	<td class="line x" title="66:207	We will refer to this topic as BACKGROUND." ></td>
	<td class="line x" title="67:207	2." ></td>
	<td class="line x" title="68:207	For each document set D, we draw a content distribution C from DIRICHLET(V,C) representing the significant content of D that we wish to summarize." ></td>
	<td class="line x" title="69:207	We will refer to this topic as CONTENT." ></td>
	<td class="line x" title="70:207	3." ></td>
	<td class="line x" title="71:207	For each document D in D, we draw a document-specific vocabulary distribution D from DIRICHLET(V,D) representing words which are local to a single document, but do not appear across several documents." ></td>
	<td class="line x" title="72:207	We will refer to this topic as DOCSPECIFIC." ></td>
	<td class="line x" title="73:207	11Atopicmodelisaprobabilisticgenerativeprocessthatgenerates a collection of documents using a mixture of topic vocabulary distributions (Steyvers and Griffiths, 2007)." ></td>
	<td class="line x" title="74:207	Note this usage of topic is unrelated to the summary focus given for document collections; this information is ignored by our models." ></td>
	<td class="line x" title="75:207	12In contrast to previous models, stop words are not removed in pre-processing." ></td>
	<td class="line x" title="76:207	13DIRICHLET(V ,) represents the symmetric Dirichlet prior distribution over V each with a pseudo-count of ." ></td>
	<td class="line x" title="77:207	Concrete pseudo-count values will be given in section 4." ></td>
	<td class="line x" title="78:207	364 { star: 0.21, wars: 0.15, phantom: 0.10,  } General Content Topic  C 1 { $: 0.39, million: 0.15, record: 0.8,  } Specific Content Topic               'Financial'  C 1 { toys: 0.22, spend: 0.18, sell: 0.10,  } { fans: 0.16, line: 0.12, film: 0.09,  } Specific Content Topic               'Merchandise' Specific Content Topic               'Fans'  C 2  C 3 Document Set  C 0  C K  C 1 Z S  D Document Sentence Word Z W  T  G   B (a) Content Distributions (b) HIERSUM Graphical Model Figure 2: (a): Examples of general versus specific content distributions utilized by HIERSUM (see section 3.4)." ></td>
	<td class="line x" title="79:207	The general content distribution C0 will be used throughout a document collection and represents core concepts in a story." ></td>
	<td class="line x" title="80:207	The specific content distributions represent topical sub-stories with vocabulary tightly clustered together but consistently used across documents." ></td>
	<td class="line x" title="81:207	Quoted names of specific topics are given manually to facilitate interpretation." ></td>
	<td class="line x" title="82:207	(b) Graphical model depiction of the HIERSUM model (see section 3.4)." ></td>
	<td class="line x" title="83:207	Similar to the TOPICSUM model (see section 3.3) except for adding complexity in the content hierarchy as well as sentence-specific prior distributions between general and specific content topics (early sentences should have more general content words)." ></td>
	<td class="line x" title="84:207	Several dependencies are missing from this depiction; crucially, each sentences specific topicZS depends on the last sentencesZS." ></td>
	<td class="line x" title="85:207	4." ></td>
	<td class="line x" title="86:207	For each sentence S of each document D, draw a distribution T over topics (CONTENT,DOCSPECIFIC,BACKGROUND) from a Dirichlet prior with pseudo-counts (1.0,5.0,10.0).14 For each word position in the sentence, we draw a topic Z from T, and a word W from the topic distribution Z indicates." ></td>
	<td class="line x" title="87:207	Our intent is that C represents the core content of a document set." ></td>
	<td class="line x" title="88:207	Intuitively, C does not include words which are common amongst several document collections (modeled with the BACKGROUND topic), or words which dont appear across many documents (modeled with the DOCSPECIFIC topic)." ></td>
	<td class="line x" title="89:207	Also, because topics are tied together at the sentence level, words which frequently occur with other content words are more likely to be considered content words." ></td>
	<td class="line x" title="90:207	We ran our topic model over the DUC 2006 document collections and estimated the distribution C() for each document set.15 Then we extracted 14The different pseudo-counts reflect the intuition that most ofthewordsinadocumentcomefromthe BACKGROUND and DOCSPECIFIC topics." ></td>
	<td class="line x" title="91:207	15While possible to obtain the predictive posterior CONa summary using the KLSUM criterion with our estimatedC in place of the the raw unigram distribution." ></td>
	<td class="line x" title="92:207	Doing so yielded 6.3 R-2 without stop words (see TOPICSUM in table 1); while not a statistically significant improvement over KLSUM, it is our first model which outperforms SUMBASIC with statistical significance." ></td>
	<td class="line x" title="93:207	Daume III and Marcu (2006) explore a topic model similar to ours for query-focused multidocument summarization.16 Crucially however, DaumeIIIandMarcu(2006)selectedsentenceswith the highest expected number of CONTENT words.17 We found that in our model using this extraction criterion yielded 5.3 R-2 without stop words, significantly underperforming our TOPICSUM model." ></td>
	<td class="line x" title="94:207	One reason for this may be that Daume III and Marcu (2006)s criterion encourages selecting sentences which have words that are confidently generated by the CONTENT distribution, but not necessarily sentences which contain a plurality of its mass. TENT distribution by analytically integrating over C (Blei et al., 2003), doing so gave no benefit." ></td>
	<td class="line x" title="95:207	16Daume III and Marcu (2006) note their model could be used outside of query-focused summarization." ></td>
	<td class="line x" title="96:207	17This is phrased as selecting the sentence which has the highest posterior probability of emitting CONTENT topic words, but this is equivalent." ></td>
	<td class="line x" title="97:207	365 (a) HIERSUM output TheFrenchgovernment Saturdayannouncedseveralemergency measures to supportthe jobless people,includingsending anadditional500million franc(84millionU.S.dollars)unemploymentaid package." ></td>
	<td class="line x" title="98:207	Theunemploymentratein France droppedby 0.3percent to standat 12.4percent in November, saidthe MinistryofEmployment Tuesday." ></td>
	<td class="line x" title="99:207	(b) PYTHY output Severalhundredpeople tookpartinthedemonstrationheretodayagainst thepoliciesoftheworlds mostdevelopednations." ></td>
	<td class="line x" title="100:207	The12.5percentunemploymentrateis haunting the ChristmasseasoninFranceasmilitants andunionistsstagedseveralprotestsoverthepast weekagainstunemployment." ></td>
	<td class="line x" title="101:207	(c) Ref output Highunemploymentis Frances maineconomic problem,despiterecent improvements." ></td>
	<td class="line x" title="102:207	A top worryofFrenchpeople, it is a factoraffecting Franceshighsuiciderate." ></td>
	<td class="line x" title="103:207	Long-termunemployment causessocialexclusion and threatensFrances socialcohesion." ></td>
	<td class="line x" title="104:207	(d) Reference Unigram Coverage word Ref PYTHY HIERSUMunemployment 8 9 10 frances 6 1 4francs 4 0 1 high 4 1 2economic 2 0 1 french 2 1 3problem 2 0 1 benefits 2 0 0social 2 0 2 jobless 2 1 2 Table 2: Example summarization output for systems compared in section 5.2." ></td>
	<td class="line x" title="105:207	(a), (b), and (c) represent the first two sentences output from PYTHY, HIERSUM, and reference summary respectively." ></td>
	<td class="line x" title="106:207	In (d), we present the most frequent non-stop unigrams appearing in the reference summary and their counts in the PYTHY and HIERSUM summaries." ></td>
	<td class="line x" title="107:207	Note that many content words in the reference summary absent from PYTHYs proposal are present in HIERSUMs. 3.4 HIERSUM Previous sections have treated the content of a document set as a single (perhaps learned) unigram distribution." ></td>
	<td class="line x" title="108:207	However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consistingofseveraltopicalthemes, each with its own vocabulary and ordering preferences." ></td>
	<td class="line x" title="109:207	For concreteness consider the DUC 2006 documentcollectiondescribingtheopeningofStarWars: Episode 1 (see figure 2(a))." ></td>
	<td class="line x" title="110:207	While there are words which indicate the general content of this document collection (e.g. star, wars), there are several sub-stories with their own specific vocabulary." ></td>
	<td class="line x" title="111:207	For instance, several documents in this collection spend a paragraph or two talking about the financial aspect of the films opening and use a specific vocabulary there (e.g. $, million, record)." ></td>
	<td class="line x" title="112:207	A user may be interested in general content of a document collection or, depending on his or her interests, one or more of the sub-stories." ></td>
	<td class="line x" title="113:207	We choose to adapt our topic modeling approach to allow modeling this aspect of document set content." ></td>
	<td class="line x" title="114:207	Rather than drawing a single CONTENT distribution C for a document collection, we now draw a general content distribution C0 from DIRICHLET(V,G) as well as specific content distributions Ci for i = 1,,K each from DIRICHLET(V,S).18 Our intent is that C0 represents the 18We choose K=3 in our experiments, but one could flexibly general content of the document collection and each Ci represents specific sub-stories." ></td>
	<td class="line x" title="115:207	As with TOPICSUM, each sentence has a distribution T over topics (BACKGROUND,DOCSPECIFIC,CONTENT)." ></td>
	<td class="line x" title="116:207	When BACKGROUND or DOCSPECIFIC topics are chosen, the model works exactly as in TOPICSUM." ></td>
	<td class="line x" title="117:207	However when the CONTENT topic is drawn, we must decide whether to emit a general content word (from C0) or from one of the specific content distributions (from one ofCi fori = 1,,K)." ></td>
	<td class="line x" title="118:207	The generative story of TOPICSUM is altered as follows in this case:  General or Specific?" ></td>
	<td class="line x" title="119:207	We must first decide whether to use a general or specific content word." ></td>
	<td class="line x" title="120:207	EachsentencedrawsabinomialdistributionG determining whether a CONTENT word in the sentence will be drawn from the general or a specific topic distribution." ></td>
	<td class="line x" title="121:207	Reflecting the intuition that the earlier sentences in a document19 describe the general content of a story, we bias G to be drawn from BETA(5,2), preferring general content words, and every later sentence from BETA(1,2).20  What Specific Topic?" ></td>
	<td class="line x" title="122:207	If G decides we are choose K as Blei et al.(2004) does." ></td>
	<td class="line x" title="124:207	19In our experiments, the first 5 sentences." ></td>
	<td class="line x" title="125:207	20BETA(a,b) represents the beta prior over binomial random variables with a and b being pseudo-counts for the first and second outcomes respectively." ></td>
	<td class="line x" title="126:207	366 emitting a topic specific content word, we must decide which of C1,,CK to use." ></td>
	<td class="line x" title="127:207	In order to ensure tight lexical cohesion amongst the specific topics, we assume that each sentence draws a single specific topicZS used for every specific content word in that sentence." ></td>
	<td class="line x" title="128:207	Reflecting intuition that adjacent sentences are likely to share specific content vocabulary, we utilize a sticky HMM as in Barzilay and Lee (2004) over the each sentences ZS." ></td>
	<td class="line x" title="129:207	Concretely, ZS for the first sentence in a document is drawn uniformly from 1,,K, and each subsequent sentences ZS will be identical to the previous sentence with probability, and with probability 1  we select a successor topic from a learned transition distribution amongst 1,,K.21 Our intent is that the general content distribution C0 now prefers words which not only appear in many documents, but also words which appear consistently throughout a document rather than being concentrated in a small number of sentences." ></td>
	<td class="line x" title="130:207	Each specific content distribution Ci is meant to model topics which are used in several documents but tend to be used in concentrated locations." ></td>
	<td class="line x" title="131:207	HIERSUM can be used to extract several kinds of summaries." ></td>
	<td class="line x" title="132:207	It can extract a general summary by plugging C0 into the KLSUM criterion." ></td>
	<td class="line x" title="133:207	It can also produce topical summaries for the learned specific topics by extracting a summary over each Ci distribution; this might be appropriate for a user who wants to know more about a particular substory." ></td>
	<td class="line x" title="134:207	While we found the general content distribution (fromC0) to produce the best single summary, we experimented with utilizing topical summaries for other summarization tasks (see section 6.1)." ></td>
	<td class="line x" title="135:207	The resulting system, HIERSUM yielded 6.4 R-2 without stop words." ></td>
	<td class="line o" title="136:207	While not a statistically significant improvementinROUGEover TOPICSUM,wefoundthe summaries to be noticeably improved." ></td>
	<td class="line x" title="137:207	4 Inference and Model Details Since globally optimizing the KLSUM criterion in equation (equation (2)) is exponential in the total number of sentences in a document collection, we 21We choose  = 0.75 in our experiments." ></td>
	<td class="line x" title="138:207	opted instead for a simple approximation where sentences are greedily added to a summary so long as they decrease KL-divergence." ></td>
	<td class="line x" title="139:207	We attempted more complex inference procedures such as McDonald (2007), but these attempts only yielded negligible performance gains." ></td>
	<td class="line x" title="140:207	All summary sentence ordering was determined as follows: each sentence in the proposed summary was assigned a number in [0,1] reflecting its relative sentence position in its source document, and sorted by this quantity." ></td>
	<td class="line x" title="141:207	All topic models utilize Gibbs sampling for inference (Griffiths, 2002; Blei et al., 2004)." ></td>
	<td class="line x" title="142:207	In general for concentration parameters, the more specific a distribution is meant to be, the smaller its concentration parameter." ></td>
	<td class="line x" title="143:207	Accordingly for TOPICSUM, G = D = 1 and C = 0.1." ></td>
	<td class="line x" title="144:207	For HIERSUM we used G = 0.1 and S = 0.01." ></td>
	<td class="line o" title="145:207	These parameters wereminimallytuned(withoutreferencetoROUGE results) in order to ensure that all topic distribution behaved as intended." ></td>
	<td class="line x" title="146:207	5 Formal Experiments We present formal experiments on the DUC 2007 data main summarization task, proposing a general summary of at most 250 words22 which will be evaluated automatically and manually in order to simulate as much as possible the DUC evaluation environment.23 DUC 2007 consists of 45 document sets, each consisting of 25 documents and 4 human reference summaries." ></td>
	<td class="line x" title="147:207	We primarily evaluate the HIERSUM model, extracting a single summary from the general content distribution using the KLSUM criterion (see section 3.2)." ></td>
	<td class="line o" title="148:207	Although the differences in ROUGE between HIERSUM and TOPICSUM were minimal, we found HIERSUM summary quality to be stronger." ></td>
	<td class="line o" title="149:207	In order to provide a reference for ROUGE and manual evaluation results, we compare against PYTHY, a state-of-the-art supervised sentence extraction summarization system." ></td>
	<td class="line o" title="150:207	PYTHY uses humangenerated summaries in order to train a sentence ranking system which discriminatively maximizes 22Since the ROUGE evaluation metric is recall-oriented, it is always advantageous with respect to ROUGE to use all 250 words." ></td>
	<td class="line x" title="151:207	23AlthoughtheDUC2007mainsummarizationtaskprovides an indication of user intent through topic focus queries, we ignore this aspect of the data." ></td>
	<td class="line o" title="152:207	367 System ROUGE w/o stop ROUGE w/ stop R-1 R-2 R-SU4 R-1 R-2 R-SU4 HIERSUM unigram 34.6 7.3 10.4 43.1 9.7 15.3 HIERSUM bigram 33.8 9.3 11.6 42.4 11.8 16.7 PYTHY w/o simp 34.7 8.7 11.8 42.7 11.4 16.5 PYTHY w/ simp 35.7 8.9 12.1 42.6 11.9 16.8 Table 3: Formal ROUGE experiment results on DUC 2007 document set collection (see section 5.1)." ></td>
	<td class="line x" title="153:207	While HIERSUM unigram underperforms both PYTHY systems in statistical significance (for R-2 and RU-4 with and without stop words), HIERSUM bigrams performance is comparable and statistically no worse." ></td>
	<td class="line o" title="154:207	ROUGE scores." ></td>
	<td class="line x" title="155:207	PYTHY uses several features to rank sentences including several variations of the SUMBASIC score (see section 3.1)." ></td>
	<td class="line o" title="156:207	At DUC 2007, PYTHY wasrankedfirstoverallinautomaticROUGE evaluation and fifth in manual content judgments." ></td>
	<td class="line x" title="157:207	As PYTHY utilizes a sentence simplification component, which we do not, we also compare against PYTHY without sentence simplification." ></td>
	<td class="line o" title="158:207	5.1 ROUGE Evaluation ROUGEresultscomparingvariantsof HIERSUM and PYTHY are given in table 3." ></td>
	<td class="line x" title="159:207	The HIERSUM system as described in section 3.4 yields 7.3 R-2 without stop words, falling significantly short of the 8.7 that PYTHY without simplification yields." ></td>
	<td class="line x" title="160:207	Note that R-2 is a measure of bigram recall and HIERSUM does not represent bigrams whereas PYTHY includes several bigram and higher order n-gram statistics." ></td>
	<td class="line x" title="161:207	In order to put HIERSUM and PYTHY on equalfooting with respect to R-2, we instead ran HIERSUM with each sentence consisting of a bag of bigrams instead of unigrams.24 All the details of the model remain the same." ></td>
	<td class="line x" title="162:207	Once a general content distribution over bigrams has been determined by hierarchical topic modeling, the KLSUM criterion is used as before to extract a summary." ></td>
	<td class="line x" title="163:207	This system, labeled HIERSUM bigram in table 3, yields 9.3 R-2 without stop words, significantly outperforming HIERSUM unigram." ></td>
	<td class="line x" title="164:207	This model outperforms PYTHY with and without sentence simplification, but not with statistical significance." ></td>
	<td class="line o" title="165:207	We conclude that both PYTHY variants and HIERSUM bigram are comparable with respect to ROUGE performance." ></td>
	<td class="line x" title="166:207	24Note that by doing topic modeling in this way over bigrams, our model becomes degenerate as it can generate inconsistent bags of bigrams." ></td>
	<td class="line x" title="167:207	Future work may look at topic models over n-grams as suggested by Wang et al.(2007)." ></td>
	<td class="line x" title="169:207	Question PYTHY HIERSUM Overall 20 49 Non-Redundancy 21 48 Coherence 15 54 Focus 28 41 Table 4: Results of manual user evaluation (see section 5.2)." ></td>
	<td class="line x" title="170:207	15 participants expressed 69 pairwise preferences between HIERSUM and PYTHY." ></td>
	<td class="line x" title="171:207	For all attributes, HIERSUM outperforms PYTHY; all results are statistically significant as determined by pairwise t-test." ></td>
	<td class="line x" title="172:207	5.2 Manual Evaluation In order to obtain a more accurate measure of summary quality, we performed a simple user study." ></td>
	<td class="line x" title="173:207	For each document set in the DUC 2007 collection, a user was given a reference summary, a PYTHY summary, anda HIERSUM summary;25 notethattheoriginal documents in the set were not provided to the user, only a reference summary." ></td>
	<td class="line x" title="174:207	For this experiment we use the bigram variant of HIERSUM and compare it to PYTHY without simplification so both systems have the same set of possible output summaries." ></td>
	<td class="line x" title="175:207	The reference summary for each document set was selected according to highest R-2 without stop words against the remaining peer summaries." ></td>
	<td class="line x" title="176:207	Users were presented with 4 questions drawn from the DUC manual evaluation guidelines:26 (1) Overall quality: Which summary was better overall?" ></td>
	<td class="line x" title="177:207	(2) Non-Redundancy: Which summary was less redundant?" ></td>
	<td class="line x" title="178:207	(3) Coherence: Which summary was more coherent?" ></td>
	<td class="line x" title="179:207	(4) Focus: Which summary was more 25The system identifier was of course not visible to the user." ></td>
	<td class="line x" title="180:207	The order of automatic summaries was determined randomly." ></td>
	<td class="line x" title="181:207	26http://www-nlpir.nist.gov/projects/duc/duc2007/qualityquestions.txt 368 Figure 3: Using HIERSUM to organize content of document set into topics (see section 6.1)." ></td>
	<td class="line x" title="182:207	The sidebar gives key phrases salient in each of the specific content topics in HIERSUM (see section 3.4)." ></td>
	<td class="line x" title="183:207	When a topic is clicked in the right sidebar, the main frame displays an extractive topical summary with links into document set articles." ></td>
	<td class="line x" title="184:207	Ideally, a user could use this interface to quickly find content in a document collection that matches their interest." ></td>
	<td class="line x" title="185:207	focused in its content, not conveying irrelevant details?" ></td>
	<td class="line x" title="186:207	The study had 16 users and each was asked to compare five summary pairs, although some did fewer." ></td>
	<td class="line x" title="187:207	Atotalof69preferencesweresolicited." ></td>
	<td class="line x" title="188:207	Document collections presented to users were randomly selected from those evaluated fewest." ></td>
	<td class="line x" title="189:207	As seen in table 5.2, HIERSUM outperforms PYTHY under all questions." ></td>
	<td class="line x" title="190:207	All results are statistically significant as judged by a simple pairwise t-test with 95% confidence." ></td>
	<td class="line x" title="191:207	It is safe to conclude that users in this study strongly preferred the HIERSUM summaries over the PYTHY summaries." ></td>
	<td class="line x" title="192:207	6 Discussion While it is difficult to qualitatively compare one summarization system over another, we can broadly characterize HIERSUM summariescomparedtosome of the other systems discussed." ></td>
	<td class="line x" title="193:207	For example output from HIERSUM and PYTHY see table 2." ></td>
	<td class="line x" title="194:207	On the whole, HIERSUM summaries appear to be significantly less redundant than PYTHY and moderately less redundant than SUMBASIC." ></td>
	<td class="line o" title="195:207	The reason for this might be that PYTHY is discriminatively trained to maximize ROUGE which does not directly penalize redundancy." ></td>
	<td class="line x" title="196:207	Anothertendencyisfor HIERSUM toselect longer sentences typically chosen from an early sentence in a document." ></td>
	<td class="line x" title="197:207	As discussed in section 3.4, HIERSUM is biased to consider early sentences in documents have a higher proportion of general content words and so this tendency is to be expected." ></td>
	<td class="line x" title="198:207	6.1 Content Navigation A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible." ></td>
	<td class="line x" title="199:207	While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query." ></td>
	<td class="line x" title="200:207	As Leuski et al.(2003) and Branavan et al.(2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a users interests." ></td>
	<td class="line x" title="203:207	We may use HIERSUM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by C1,,CK (for an example see figure 3)." ></td>
	<td class="line x" title="204:207	While these topics are not adaptive to user interest, they typically reflect lexically coherent vocabularies." ></td>
	<td class="line x" title="205:207	Conclusion In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics." ></td>
	<td class="line x" title="206:207	Acknowledgements The authors would like to thank Bob Moore, Chris Brockett, Chris Quirk, and Kristina Toutanova for their useful discussions as well as the reviewers for their helpful comments." ></td>
	<td class="line x" title="207:207	369" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1066
Using Citations to Generate surveys of Scientific Paradigms
Mohammad, Saif;Dorr, Bonnie Jean;Egan, Melissa;Hassan, Ahmed;Muthukrishnan, Pradeep;Qazvinian, Vahed;Radev, Dragomir R.;Zajic, David;"></td>
	<td class="line x" title="1:220	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 584592, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:220	c 2009 Association for Computational Linguistics Using Citations to Generate Surveys of Scientific Paradigms Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, David Zajicdiamondmath Institute for Advanced Computer Studies and Computer Science, University of Maryland." ></td>
	<td class="line x" title="3:220	Human Language Technology Center of Excellence." ></td>
	<td class="line x" title="4:220	Center for Advanced Study of Languagediamondmath." ></td>
	<td class="line x" title="5:220	{saif,bonnie,mkegan,dmzajic}@umiacs.umd.edu Department of Electrical Engineering and Computer Science School of Information, University of Michigan." ></td>
	<td class="line x" title="6:220	{hassanam,mpradeep,vahed,radev}@umich.edu Abstract The number of research publications in various disciplines is growing exponentially." ></td>
	<td class="line x" title="7:220	Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material." ></td>
	<td class="line x" title="8:220	In this paper we present the first steps in producing an automatically generated, readily consumable, technical survey." ></td>
	<td class="line x" title="9:220	Specifically we explore the combination of citation information and summarization techniques." ></td>
	<td class="line x" title="10:220	Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role." ></td>
	<td class="line x" title="11:220	1 Introduction In todays rapidly expanding disciplines, scientists and scholars are constantly faced with the daunting task of keeping up with knowledge in their field." ></td>
	<td class="line x" title="12:220	In addition, the increasingly interconnected nature of real-world tasks often requires experts in one discipline to rapidly learn about other areas in a short amount of time." ></td>
	<td class="line x" title="13:220	Cross-disciplinary research requires scientists in areas such as linguistics, biology, and sociology to learn about computational approaches and applications, e.g., computational linguistics, biological modeling, social networks." ></td>
	<td class="line x" title="14:220	Authors of journal articles and books must write accurate surveys of previous work, ranging from short summaries of related research to in-depth historical notes." ></td>
	<td class="line x" title="15:220	Interdisciplinary review panels are often called upon to review proposals in a wide range of areas, some of which may be unfamiliar to panelists." ></td>
	<td class="line x" title="16:220	Thus, they must learn about a new discipline on the fly in order to relate their own expertise to the proposal." ></td>
	<td class="line x" title="17:220	Our goal is to effectively serve these needs by combining two currently available technologies: (1) bibliometric lexical link mining that exploits the structure of citations and relations among citations; and (2) summarization techniques that exploit the content of the material in both the citing and cited papers." ></td>
	<td class="line x" title="18:220	It is generally agreed upon that manually written abstracts are good summaries of individual papers." ></td>
	<td class="line x" title="19:220	More recently, Qazvinian and Radev (2008) argue that citation texts are useful in creating a summary of the important contributions of a research paper." ></td>
	<td class="line x" title="20:220	The citation text of a target paper is the set of sentences in other technical papers that explicitly refer to it (Elkiss et al., 2008a)." ></td>
	<td class="line x" title="21:220	However, Teufel (2005) argues that using citation text directly is not suitable for document summarization." ></td>
	<td class="line x" title="22:220	In this paper, we compare and contrast the usefulness of abstracts and of citation text in automatically generating a technical survey on a given topic from multiple research papers." ></td>
	<td class="line x" title="23:220	The next section provides the background for this work, including the primary features of a technical survey and also the types of input that are used in our study (full papers, abstracts, and citation texts)." ></td>
	<td class="line x" title="24:220	Following this, we describe related work and point out the advances of our work over previous work." ></td>
	<td class="line x" title="25:220	We then describe how citation texts are used as a new input for multidocument summarization to produce surveys of a given technical area." ></td>
	<td class="line o" title="26:220	We apply four different summarization techniques to data in the ACL Anthol584 ogy and evaluate our results using both automatic (ROUGE) and human-mediated (nugget-based pyramid) measures." ></td>
	<td class="line x" title="27:220	We observe that, as expected, abstracts are useful in survey creation, but, notably, we also conclude that citation texts have crucial surveyworthy information not present in (or at least, not easily extractable from) abstracts." ></td>
	<td class="line x" title="28:220	We further discover that abstracts are author-biased and thus complementary to the broader perspective inherent in citation texts; these differences enable the use of a range of different levels and types of information in the surveythe extent of which is subject to survey length restrictions (if any)." ></td>
	<td class="line x" title="29:220	2 Background Automatically creating technical surveys is significantly distinct from that of traditional multidocument summarization." ></td>
	<td class="line x" title="30:220	Below we describe primary characteristics of a technical survey and we present three types of input texts that we used for the production of surveys." ></td>
	<td class="line x" title="31:220	2.1 Technical Survey In the case of multi-document summarization, the goal is to produce a readable presentation of multiple documents, whereas in the case of technical survey creation, the goal is to convey the key features of a particular field, basic underpinnings of the field, early and late developments, important contributions and findings, contradicting positions that may reverse trends or start new sub-fields, and basic definitions and examples that enable rapid understanding of a field by non-experts." ></td>
	<td class="line x" title="32:220	A prototypical example of a technical survey is that of chapter notes, i.e., short (50500 word) descriptions of sub-areas found at the end of chapters of textbook, such as Jurafsky and Martin (2008)." ></td>
	<td class="line x" title="33:220	One might imagine producing such descriptions automatically, then hand-editing them and refining them for use in an actual textbook." ></td>
	<td class="line x" title="34:220	We conducted a human analysis of these chapter notes that revealed a set of conventions, an outline of which is provided here (with example sentences in italics): 1." ></td>
	<td class="line x" title="35:220	Introductory/opening statement: The earliest computational use of X was in Y, considered by many to be the foundational work in this area." ></td>
	<td class="line x" title="36:220	2." ></td>
	<td class="line x" title="37:220	Definitional follow up: X is def ined as Y. 3." ></td>
	<td class="line x" title="38:220	Elaboration of definition (e.g., with an example): Most early algorithms were based on Z. 4." ></td>
	<td class="line x" title="39:220	Deeper elaboration, e.g., pointing out issues with initial approaches: Unfortunately, this model seems to be wrong." ></td>
	<td class="line x" title="40:220	5." ></td>
	<td class="line x" title="41:220	Contrasting definition: Algorithms since then 6." ></td>
	<td class="line x" title="42:220	Introduction of additional specific instances / historical background with citations: Two classic approaches are described in Q. 7." ></td>
	<td class="line x" title="43:220	References to other summaries: R provides a comprehensive guide to the details behind X. The notion of text level categories or zoning of technical papersrelated to the survey components enumerated abovehas been investigated previously in the work of Nanba and Kan (2004b) and Teufel (2002)." ></td>
	<td class="line x" title="44:220	These earlier works focused on the analysis of scientific papers based on their rhetorical structure and on determining the portions of papers that contain new results, comparisons to earlier work, etc. The work described in this paper focuses on the synthesis of technical surveys based on knowledge gleaned from rhetorical structure not unlike that of the work of these earlier researchers, but perhaps guided by structural patterns along the lines of the conventions listed above." ></td>
	<td class="line x" title="45:220	Although our current approach to survey creation does not yet incorporate a fully pattern-based component, our ultimate objective is to apply these patterns to guide the creation and refinement of the final output." ></td>
	<td class="line x" title="46:220	As a first step toward this goal, we use citation texts (closest in structure to the patterns identified by convention 7 above) to pick out the most important content for survey creation." ></td>
	<td class="line x" title="47:220	2.2 Full papers, abstracts, and citation texts Published research on a particular topic can be summarized from two different kinds of sources: (1) where an author describes her own work and (2) where others describe an authors work (usually in relation to their own work)." ></td>
	<td class="line x" title="48:220	The authors description of her own work can be found in her paper." ></td>
	<td class="line x" title="49:220	How others perceive her work is spread across other papers that cite her work." ></td>
	<td class="line x" title="50:220	We will refer to the set of sentences that explicitly mention a target paper Y as the citation text of Y. 585 Traditionally, technical survey generation has been tackled by summarizing a set of research papers pertaining to the topic." ></td>
	<td class="line x" title="51:220	However, individual research papers usually come with manually-created summariestheir abstracts." ></td>
	<td class="line x" title="52:220	The abstract of a paper may have sentences that set the context, state the problem statement, mention how the problem is approached, and the bottom-line resultsall in 200 to 500 words." ></td>
	<td class="line x" title="53:220	Thus, using only the abstracts (instead of full papers) as input to a summarization system is worth exploring." ></td>
	<td class="line x" title="54:220	Whereas the abstract of a paper presents what the authors think to be the important contributions of a paper, the citation text of a paper captures what others in the field perceive as the contributions of the paper." ></td>
	<td class="line x" title="55:220	The two perspectives are expected to have some overlap in their content, but the citation text also contains additional information not found in abstracts (Elkiss et al., 2008a)." ></td>
	<td class="line x" title="56:220	For example, how a particular methodology (described in one paper) was combined with another (described in a different paper) to overcome some of the drawbacks of each." ></td>
	<td class="line x" title="57:220	A citation text is also an indicator of what contributions described in a paper were more influential over time." ></td>
	<td class="line x" title="58:220	Another distinguishing feature of citation texts in contrast to abstracts is that a citation text tends to have a certain amount of redundant information." ></td>
	<td class="line x" title="59:220	This is because multiple papers may describe the same contributions of a target paper." ></td>
	<td class="line x" title="60:220	This redundancy can be exploited to determine the important contributions of the target paper." ></td>
	<td class="line x" title="61:220	Our goal is to test the hypothesis that an effective technical survey will reflect information on research not only from the perspective of its authors but also from the perspective of others who use/commend/discredit/add to it." ></td>
	<td class="line x" title="62:220	Before describing our experiments with technical papers, abstracts, and citation texts, we first summarize relevant prior work that used these sources of information as input." ></td>
	<td class="line x" title="63:220	3 Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002)." ></td>
	<td class="line x" title="64:220	Bradshaw (2003) used citation texts to determine the content of articles and improve the results of a search engine." ></td>
	<td class="line x" title="65:220	Citation texts have also been used to create summaries of single scientific articles in Qazvinian and Radev (2008) and Mei and Zhai (2008)." ></td>
	<td class="line x" title="66:220	However, there is no previous work that uses the text of the citations to produce a multi-document survey of scientific articles." ></td>
	<td class="line x" title="67:220	Furthermore, there is no study contrasting the quality of surveys generated from citation summaries both automatically and manually producedto surveys generated from other forms of input such as the abstracts or full texts of the source articles." ></td>
	<td class="line x" title="68:220	Nanba and Okumura (1999) discuss citation categorization to support a system for writing a survey." ></td>
	<td class="line x" title="69:220	Nanba et al.(2004a) automatically categorize citation sentences into three groups using pre-defined phrase-based rules." ></td>
	<td class="line x" title="71:220	Based on this categorization a survey generation tool is introduced in Nanba et al.(2004b)." ></td>
	<td class="line x" title="73:220	They report that co-citation (where both papers are cited by many other papers) implies similarity by showing that the textual similarity of cocited papers is proportional to the proximity of their citations in the citing article." ></td>
	<td class="line x" title="74:220	Elkiss et al.(2008b) conducted several experiments on a set of 2,497 articles from the free PubMed Central (PMC) repository.1 Results from this experiment confirmed that the cohesion of a citation text of an article is consistently higher than the that of its abstract." ></td>
	<td class="line x" title="76:220	They also concluded that citation texts contain additional information are more focused than abstracts." ></td>
	<td class="line x" title="77:220	Nakov et al.(2004) use sentences surrounding citations to create training and testing data for semantic analysis, synonym set creation, database curation, document summarization, and information retrieval." ></td>
	<td class="line x" title="79:220	Kan et al.(2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents." ></td>
	<td class="line x" title="81:220	Kupiec et al.(1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization." ></td>
	<td class="line x" title="83:220	Siddharthan and Teufel (2007) describe a new reference task and show high human agreement as well as an improvement in the performance of argumentative zoning (Teufel, 2005)." ></td>
	<td class="line x" title="84:220	In argumentative zoninga rhetorical classification taskseven 1http://www.pubmedcentral.gov 586 classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the authors argument." ></td>
	<td class="line oc" title="85:220	Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full textscomparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Lin, 2004)." ></td>
	<td class="line x" title="86:220	4 Summarization systems We used four summarization systems for our survey-creation approach: Trimmer, LexRank, CLexRank, and C-RR." ></td>
	<td class="line x" title="87:220	Trimmer is a syntacticallymotivated parse-and-trim approach." ></td>
	<td class="line x" title="88:220	LexRank is a graph-based similarity approach." ></td>
	<td class="line x" title="89:220	C-LexRank and CRR use graph clustering (C stands for clustering)." ></td>
	<td class="line x" title="90:220	We describe each of these, in turn, below." ></td>
	<td class="line x" title="91:220	4.1 Trimmer Trimmer is a sentence-compression tool that extends the scope of an extractive summarization system by generating multiple alternative sentence compressions of the most important sentences in target documents (Zajic et al., 2007)." ></td>
	<td class="line x" title="92:220	Trimmer compressions are generated by applying linguistically-motivated rules to mask syntactic components of a parse of a source sentence." ></td>
	<td class="line x" title="93:220	The rules can be applied iteratively to compress sentences below a configurable length threshold, or can be applied in all combinations to generate the full space of compressions." ></td>
	<td class="line x" title="94:220	Trimmer can leverage the output of any constituency parser that uses the Penn Treebank conventions." ></td>
	<td class="line x" title="95:220	At present, the Stanford Parser (Klein and Manning, 2003) is used." ></td>
	<td class="line x" title="96:220	The set of compressions is ranked according to a set of features that may include metadata about the source sentences, details of the compression process that generated the compression, and externally calculated features of the compression." ></td>
	<td class="line x" title="97:220	Summaries are constructed from the highest scoring compressions, using the metadata and maximal marginal relevance (Carbonell and Goldstein, 1998) to avoid redundancy and over-representation of a single source." ></td>
	<td class="line x" title="98:220	4.2 LexRank We also used LexRank (Erkan and Radev, 2004), a state-of-the-art multidocument summarization system, to generate summaries." ></td>
	<td class="line x" title="99:220	LexRank first builds a graph of all the candidate sentences." ></td>
	<td class="line x" title="100:220	Two candidate sentences are connected with an edge if the similarity between them is above a threshold." ></td>
	<td class="line x" title="101:220	We used cosine as the similarity metric with a threshold of 0.15." ></td>
	<td class="line x" title="102:220	Once the network is built, the system finds the most central sentences by performing a random walk on the graph." ></td>
	<td class="line x" title="103:220	The salience of a node is recursively defined on the salience of adjacent nodes." ></td>
	<td class="line x" title="104:220	This is similar to the concept of prestige in social networks, where the prestige of a person is dependent on the prestige of the people he/she knows." ></td>
	<td class="line x" title="105:220	However, since random walk may get caught in cycles or in disconnected components, we reserve a low probability to jump to random nodes instead of neighbors (a technique suggested by Langville and Meyer (2006))." ></td>
	<td class="line x" title="106:220	Note also that unlike the original PageRank method, the graph of sentences is undirected." ></td>
	<td class="line x" title="107:220	This updated measure of sentence salience is called as LexRank." ></td>
	<td class="line x" title="108:220	The sentences with the highest LexRank scores form the summary." ></td>
	<td class="line x" title="109:220	4.3 Cluster Summarizers: C-LexRank, C-RR Two clustering methods proposed by Qazvinian and Radev (2008)C-RR and C-LexRankwere used to create summaries." ></td>
	<td class="line x" title="110:220	Both create a fully connected network in which nodes are sentences and edges are cosine similarities." ></td>
	<td class="line x" title="111:220	A cutoff value of 0.1 is applied to prune the graph and make a binary network." ></td>
	<td class="line x" title="112:220	The largest connected component of the network is then extracted and clustered." ></td>
	<td class="line x" title="113:220	Both of the mentioned summarizers cluster the network similarly but use different approaches to select sentences from different communities." ></td>
	<td class="line x" title="114:220	In CRR sentences are picked from different clusters in a round robin (RR) fashion." ></td>
	<td class="line x" title="115:220	C-LexRank first calculates LexRank within each cluster to find the most salient sentences of each community." ></td>
	<td class="line x" title="116:220	Then it picks the most salient sentence of each cluster, and then the second most salient and so forth until the summary length limit is reached." ></td>
	<td class="line x" title="117:220	587 Mostof workin QAandparaphrasingfocusedon foldingparaphrasingknowledgeintoquestionanalyzeror answer locatorRinaldiet al, 2003;Tomuro,2003.In addition,numberof researchershave builtsystemsto take reading comprehensionexaminationsdesignedto evaluatechildrens readinglevelsCharniaket al, 2000;Hirschmanet al, 1999;Nget al, 2000;Riloff andThelen,2000;Wanget al, 2000.so-called definition or  other questionsat recentTRECevaluationsVoorhees,2005serve as goodexamples.To betterfacilitateuser informationneeds,recenttrendsin QAresearchhave shiftedtowardscomplex, context-based,andinteractive questionansweringVoorhees,2001;Smallet al, 2003;Harabagiuet al, 2005.[Andso on.]" ></td>
	<td class="line x" title="118:220	Table 1: First few sentences of the QA citation texts survey generated by Trimmer." ></td>
	<td class="line x" title="119:220	5 Data The ACL Anthology is a collection of papers from the Computational Linguistics journal, and proceedings of ACL conferences and workshops." ></td>
	<td class="line x" title="120:220	It has almost 11,000 papers." ></td>
	<td class="line x" title="121:220	To produce the ACL Anthology Network (AAN), Joseph and Radev (2007) manually parsed the references before automatically compiling the network metadata, and generating citation and author collaboration networks." ></td>
	<td class="line x" title="122:220	The AAN includes all citation and collaboration data within the ACL papers, with the citation network consisting of 11,773 nodes and 38,765 directed edges." ></td>
	<td class="line x" title="123:220	Our evaluation experiments are on a set of papers in the research area of Question Answering (QA) and another set of papers on Dependency parsing (DP)." ></td>
	<td class="line x" title="124:220	The two sets of papers were compiled by selecting all the papers in AAN that had the words Question Answering and Dependency Parsing, respectively, in the title and the content." ></td>
	<td class="line x" title="125:220	There were 10 papers in the QA set and 16 papers in the DP set." ></td>
	<td class="line x" title="126:220	We also compiled the citation texts for the 10 QA papers and the citation texts for the 16 DP papers." ></td>
	<td class="line x" title="127:220	6 Experiments We automatically generated surveys for both QA and DP from three different types of documents: (1) full papers from the QA and DP setsQA and DP full papers (PA), (2) only the abstracts of the QA and DP papersQA and DP abstracts (AB), and (3) the citation texts corresponding to the QA and DP papersQA and DP citations texts (CT)." ></td>
	<td class="line x" title="128:220	We generated twenty four (4x3x2) surveys, each of length 250 words, by applying Trimmer, LexRank, C-LexRank and C-RR on the three data types (citation texts, abstracts, and full papers) for both QA and DP." ></td>
	<td class="line x" title="129:220	(Table 1 shows a fragment of one of the surveys automatically generated from QA citation texts.)" ></td>
	<td class="line x" title="130:220	We created six (3x2) additional 250word surveys by randomly choosing sentences from the citation texts, abstracts, and full papers of QA and DP." ></td>
	<td class="line x" title="131:220	We will refer to them as random surveys." ></td>
	<td class="line x" title="132:220	6.1 Evaluation Our goal was to determine if citation texts do indeed have useful information that one will want to put in a survey and if so, how much of this information is not available in the original papers and their abstracts." ></td>
	<td class="line o" title="133:220	For this we evaluated each of the automatically generated surveys using two separate approaches: nugget-based pyramid evaluation and ROUGE (described in the two subsections below)." ></td>
	<td class="line x" title="134:220	Two sets of gold standard data were manually created from the QA and DP citation texts and abstracts, respectively:2 (1) We asked two impartial judges to identify important nuggets of information worth including in a survey." ></td>
	<td class="line x" title="135:220	(2) We asked four fluent speakers of English to create 250-word surveys of the datasets." ></td>
	<td class="line x" title="136:220	Then we determined how well the different automatically generated surveys perform against these gold standards." ></td>
	<td class="line x" title="137:220	If the citation texts have only redundant information with respect to the abstracts and original papers, then the surveys of citation texts will not perform better than others." ></td>
	<td class="line oc" title="138:220	6.1.1 Nugget-Based Pyramid Evaluation For our first approach we used a nugget-based evaluation methodology (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Hildebrandt et al., 2004; Voorhees, 2003)." ></td>
	<td class="line x" title="139:220	We asked three impartial annotators (knowledgeable in NLP but not affiliated with the project) to review the citation texts and/or abstract sets for each of the papers in the QA and DP sets and manually extract prioritized lists 2Creating gold standard data from complete papers is fairly arduous, and was not pursued." ></td>
	<td class="line x" title="140:220	588 of 28 nuggets, or main contributions, supplied by each paper." ></td>
	<td class="line x" title="141:220	Each nugget was assigned a weight based on the frequency with which it was listed by annotators as well as the priority it was assigned in each case." ></td>
	<td class="line x" title="142:220	Our automatically generated surveys were then scored based on the number and weight of the nuggets that they covered." ></td>
	<td class="line x" title="143:220	This evaluation approach is similar to the one adopted by Qazvinian and Radev (2008), but adapted here for use in the multi-document case." ></td>
	<td class="line x" title="144:220	The annotators had two distinct tasks for the QA set, and one for the DP set: (1) extract nuggets for each of the 10 QA papers, based only on the citation texts for those papers; (2) extract nuggets for each of the 10 QA papers, based only on the abstracts of those papers; and (3) extract nuggets for each of the 16 DP papers, based only on the citation texts for those papers.3 We obtained a weight for each nugget by reversing its priority out of 8 (e.g., a nugget listed with priority 1 was assigned a weight of 8) and summing the weights over each listing of that nugget.4 To evaluate a given survey, we counted the number and weight of nuggets that it covered." ></td>
	<td class="line x" title="145:220	Nuggets were detected via the combined use of annotatorprovided regular expressions and careful human review." ></td>
	<td class="line x" title="146:220	Recall was calculated by dividing the combined weight of covered nuggets by the combined weight of all nuggets in the nugget set." ></td>
	<td class="line x" title="147:220	Precision was calculated by dividing the number of distinct nuggets covered in a survey by the number of sentences constituting that survey, with a cap of 1." ></td>
	<td class="line x" title="148:220	Fmeasure, the weighted harmonic mean of precision and recall, was calculated with a beta value of 3 in order to assign the greatest weight to recall." ></td>
	<td class="line x" title="149:220	Recall is favored because it rewards surveys that include highly weighted (important) facts, rather than just a 3We first experimented using only the QA set." ></td>
	<td class="line x" title="150:220	Then to show that the results apply to other datasets, we asked human annotators for gold standard data on the DP citation texts." ></td>
	<td class="line x" title="151:220	Additional experiments on DP abstracts were not pursued because this would have required additional human annotation effort to establish a point we had already made with the QA set, i.e., that abstracts are useful for survey creation." ></td>
	<td class="line x" title="152:220	4Results obtained with other weighting schemes that ignored priority ratings and multiple mentions of a nugget by a single annotator showed the same trends as the ones shown by the selected weighting scheme, but the latter was a stronger distinguisher among the four systems." ></td>
	<td class="line x" title="153:220	HumanPerformance:PyramidF-measureHuman1Human2Human3Human4Average Input:QAcitationsurveysQACTnuggets 0.524 0.711 0.468 0.695 0.599 QAABnuggets 0.495 0.606 0.423 0.608 0.533Input:QAabstractsurveys QACTnuggets 0.542 0.675 0.581 0.669 0.617QAABnuggets 0.646 0.841 0.673 0.790 0.738 Input:DPcitationsurveysDPCTnuggets 0.245 0.475 0.378 0.555 0.413 Table 2: Pyramid F-measure scores of human-created surveys of QA and DP data." ></td>
	<td class="line x" title="154:220	The surveys are evaluated using nuggets drawn from QA citation texts (QACT), QA abstracts (QAAB), and DP citation texts (DPCT)." ></td>
	<td class="line x" title="155:220	great number of facts." ></td>
	<td class="line x" title="156:220	Table 2 gives the F-measure values of the 250word surveys manually generated by humans." ></td>
	<td class="line x" title="157:220	The surveys were evaluated using the nuggets drawn from the QA citation texts, QA abstracts, and DP citation texts." ></td>
	<td class="line x" title="158:220	The average of their scores (listed in the rightmost column) may be considered a good score to aim for by the automatic summarization methods." ></td>
	<td class="line x" title="159:220	Table 3 gives the F-measure values of the surveys generated by the four automatic summarizers, evaluated using nuggets drawn from the QA citation texts, QA abstracts, and DP citation texts." ></td>
	<td class="line x" title="160:220	The table also includes results for the baseline random summaries." ></td>
	<td class="line x" title="161:220	When we used the nuggets from the abstracts set for evaluation, the surveys created from abstracts scored higher than the corresponding surveys created from citation texts and papers." ></td>
	<td class="line x" title="162:220	Further, the best surveys generated from citation texts outscored the best surveys generated from papers." ></td>
	<td class="line x" title="163:220	When we used the nuggets from citation sets for evaluation, the best automatic surveys generated from citation texts outperform those generated from abstracts and full papers." ></td>
	<td class="line x" title="164:220	All these pyramid results demonstrate that citation texts can contain useful information that is not available in the abstracts or the original papers, and that abstracts can contain useful information that is not available in the citation texts or full papers." ></td>
	<td class="line x" title="165:220	Among the various automatic summarizers, Trimmer performed best at this task, in two cases exceeding the average human performance." ></td>
	<td class="line x" title="166:220	Note also that the random summarizer outscored the automatic summarizers in cases where the nuggets were taken from a source different from that used to generate the survey." ></td>
	<td class="line x" title="167:220	However, one or two summarizers still tended to do well." ></td>
	<td class="line x" title="168:220	This indicates a difficulty in ex589 SystemPerformance:PyramidF-measure Random C-LexRank C-RR LexRank Trimmer Input:QAcitationsurveys QACTnuggets 0.321 0.434 0.268 0.295 0.616 QAABnuggets 0.305 0.388 0.349 0.320 0.543 Input:QAabstractsurveys QACTnuggets 0.452 0.383 0.480 0.441 0.404 QAABnuggets 0.623 0.484 0.574 0.606 0.622 Input:QAfullpapersurveys QACTnuggets 0.239 0.446 0.299 0.190 0.199 QAABnuggets 0.294 0.520 0.387 0.301 0.290 Input:DPcitationsurveys DPCTnuggets 0.219 0.231 0.170 0.372 0.136 Input:DPabstractsurveys DPCTnuggets 0.321 0.301 0.263 0.311 0.312 Input:DPfullpapersurveys DPCTnuggets 0.032 0.000 0.144 * 0.280 Table 3: Pyramid F-measure scores of automatic surveys of QA and DP data." ></td>
	<td class="line x" title="169:220	The surveys are evaluated using nuggets drawn from QA citation texts (QACT), QA abstracts (QAAB), and DP citation texts (DPCT)." ></td>
	<td class="line x" title="170:220	* LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences)." ></td>
	<td class="line x" title="171:220	HumanPerformance:ROUGE-2 human1human2human3human4average Input:QAcitationsurveys QACTrefs." ></td>
	<td class="line x" title="172:220	0.1807 0.1956 0.0756 0.2019 0.1635 QAABrefs." ></td>
	<td class="line x" title="173:220	0.1116 0.1399 0.0711 0.1576 0.1201 Input:QAabstractsurveys QACTrefs." ></td>
	<td class="line x" title="174:220	0.1315 0.1104 0.1216 0.1151 0.1197 QA-ABrefs." ></td>
	<td class="line x" title="175:220	0.2648 0.1977 0.1802 0.2544 0.2243 Input:DPcitationsurveys DPCTrefs." ></td>
	<td class="line o" title="176:220	0.1550 0.1259 0.1200 0.1654 0.1416 Table 4: ROUGE-2 scores obtained for each of the manually created surveys by using the other three as reference." ></td>
	<td class="line x" title="177:220	ROUGE-1 and ROUGE-L followed similar patterns." ></td>
	<td class="line x" title="178:220	tracting the overlapping survey-worthy information across the two sources." ></td>
	<td class="line oc" title="179:220	6.1.2 ROUGE evaluation Table 4 presents ROUGE scores (Lin, 2004) of each of human-generated 250-word surveys against each other." ></td>
	<td class="line x" title="180:220	The average (last column) is what the automatic surveys can aim for." ></td>
	<td class="line x" title="181:220	We then evaluated each of the random surveys and those generated by the four summarization systems against the references." ></td>
	<td class="line o" title="182:220	Table 5 lists ROUGE scores of surveys when the manually created 250-word survey of the QA citation texts, survey of the QA abstracts, and the survey of the DP citation texts, were used as gold standard." ></td>
	<td class="line o" title="183:220	When we use manually created citation text surveys as reference, then the surveys generated from citation texts obtained significantly better ROUGE scores than the surveys generated from abstracts and full papers (p < 0.05) [RESULT 1]." ></td>
	<td class="line x" title="184:220	This shows that crucial survey-worthy information present in citation texts is not available, or hard to extract, from abstracts and papers alone." ></td>
	<td class="line x" title="185:220	Further, the surveys generated from abstracts performed significantly better than those generated from the full papers (p < 0.05) [RESULT 2]." ></td>
	<td class="line x" title="186:220	This shows that abstracts and citation texts are generally denser in survey worthy information than full papers." ></td>
	<td class="line o" title="187:220	When we use manually created abstract surveys as reference, then the surveys generated from abstracts obtained significantly better ROUGE scores than the surveys generated from citation texts and full papers (p < 0.05) [RESULT 3]." ></td>
	<td class="line x" title="188:220	Further, and more importantly, the surveys generated from citation texts performed significantly better than those generated from the full papers (p < 0.05) [RESULT 4]." ></td>
	<td class="line x" title="189:220	Again, this shows that abstracts and citation texts are richer in survey-worthy information." ></td>
	<td class="line x" title="190:220	These results also show that abstracts of papers and citation texts have some overlapping information (RESULT 2 and RESULT 4), but they also have a significant amount of unique survey-worthy information (RESULT 1 and RESULT 3)." ></td>
	<td class="line x" title="191:220	Among the automatic summarizers, C-LexRank and LexRank perform best." ></td>
	<td class="line x" title="192:220	This is unlike the results found through the nugget-evaluation method, where Trimmer performed best." ></td>
	<td class="line o" title="193:220	This suggests that Trim590 SystemPerformance:ROUGE-2 Random C-LexRank C-RR LexRank Trimmer Input:QAcitationsurveys QACTrefs." ></td>
	<td class="line x" title="194:220	0.11561 0.17013 0.09522 0.13501 0.16984 QAABrefs." ></td>
	<td class="line x" title="195:220	0.08264 0.11653 0.07600 0.07013 0.10336 Input:QAabstractsurveys QACTrefs." ></td>
	<td class="line x" title="196:220	0.04516 0.05892 0.06149 0.05369 0.04114 QAABrefs." ></td>
	<td class="line x" title="197:220	0.12085 0.13634 0.12190 0.20311 0.13357 Input:QAfullpapersurveys QACTrefs." ></td>
	<td class="line x" title="198:220	0.03042 0.03606 0.03599 0.28244 0.03986 QAABrefs." ></td>
	<td class="line x" title="199:220	0.04621 0.05901 0.04976 0.10540 0.07505 Input:DPcitationsurveys DPCTrefs." ></td>
	<td class="line x" title="200:220	0.10690 0.13164 0.08748 0.04901 0.10052 Input:DPabstractsurveys DPCTrefs." ></td>
	<td class="line x" title="201:220	0.07027 0.07321 0.05318 0.20311 0.07176 Input:DPfullpapersurveys DPCTrefs." ></td>
	<td class="line o" title="202:220	0.03770 0.02511 0.03433 * 0.04554 Table 5: ROUGE-2 scores of automatic surveys of QA and DP data." ></td>
	<td class="line x" title="203:220	The surveys are evaluated by using human references created from QA citation texts (QACT), QA abstracts (QAAB), and DP citation texts (DPCT)." ></td>
	<td class="line x" title="204:220	These results are obtained after Jack-knifing the human references so that the values can be compared to those in Table 4." ></td>
	<td class="line x" title="205:220	* LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences)." ></td>
	<td class="line x" title="206:220	mer is better at identifying more useful nuggets of information, but C-LexRank and LexRank are better at producing unigrams and bigrams expected in a survey." ></td>
	<td class="line x" title="207:220	To some extent this may be due to the fact that Trimmer uses smaller (trimmed) fragments of source sentences in its summaries." ></td>
	<td class="line x" title="208:220	7 Conclusion In this paper, we investigated the usefulness of directly summarizing citation texts (sentences that cite other papers) in the automatic creation of technical surveys." ></td>
	<td class="line x" title="209:220	We generated surveys of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation texts using four state-of-the-art summarization systems (CLexRank, C-RR, LexRank, and Trimmer)." ></td>
	<td class="line o" title="210:220	We then used two separate approaches, nugget-based pyramid and ROUGE, to evaluate the surveys." ></td>
	<td class="line x" title="211:220	The results from both approaches and all four summarization systems show that both citation texts and abstracts have unique survey-worthy information." ></td>
	<td class="line x" title="212:220	These results also demonstrate that, unlike single document summarization (where citing sentences have been suggested to be inappropriate (Teufel et al., 2006)), multidocument summarization especially technical survey creationbenefits considerably from citation texts." ></td>
	<td class="line x" title="213:220	We next plan to generate surveys using both citation texts and abstracts together as input." ></td>
	<td class="line x" title="214:220	Given the overlapping content of abstracts and citation texts, discovered in the current study, it is clear that redundancy detection will be an integral component of this future work." ></td>
	<td class="line x" title="215:220	Creating readily consumable surveys is a hard task, especially when using only raw text and simple summarization techniques." ></td>
	<td class="line x" title="216:220	Therefore we intend to combine these summarization and bibliometric techniques with suitable visualization methods towards the creation of iterative technical survey toolssystems that present surveys and bibliometric links in a visually convenient manner and which incorporate user feedback to produce even better surveys." ></td>
	<td class="line x" title="217:220	Acknowledgments This work was supported, in part, by the National Science Foundation under Grant No." ></td>
	<td class="line x" title="218:220	IIS-0705832 (iOPENER: Information Organization for PENning Expositions on Research) and Grant No. 0534323 (Collaborative Research: BlogoCenter Infrastructure for Collecting, Mining and Accessing Blogs), in part, by the Human Language Technology Center of Excellence, and in part, by the Center for Advanced Study of Language (CASL)." ></td>
	<td class="line x" title="219:220	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors." ></td>
	<td class="line x" title="220:220	591" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1022
DEPEVAL(summ): Dependency-based Evaluation for Automatic Summaries
Owczarzak, Karolina;"></td>
	<td class="line x" title="1:180	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 190198, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:180	c2009 ACL and AFNLP DEPEVAL(summ): Dependency-based Evaluation for Automatic Summaries Karolina Owczarzak Information Access Division National Institute of Standards and Technology Gaithersburg, MD 20899 karolina.owczarzak@nist.gov Abstract This paper presents DEPEVAL(summ), a dependency-based metric for automatic evaluation of summaries." ></td>
	<td class="line x" title="3:180	Using a reranking parser and a Lexical-Functional Grammar (LFG) annotation, we produce a set of dependency triples for each summary." ></td>
	<td class="line x" title="4:180	The dependency set for each candidate summary is then automatically compared against dependencies generated from model summaries." ></td>
	<td class="line x" title="5:180	We examine a number of variations of the method, including the addition of WordNet, partial matching, or removing relation labels from the dependencies." ></td>
	<td class="line x" title="6:180	In a test on TAC 2008 and DUC 2007 data, DEPEVAL(summ) achieves comparable or higher correlations with human judgments than the popular evaluation metrics ROUGE and Basic Elements (BE)." ></td>
	<td class="line x" title="7:180	1 Introduction Evaluation is a crucial component in the area of automatic summarization; it is used both to rank multiple participant systems in shared summarization tasks, such as the Summarization track at Text Analysis Conference (TAC) 2008 and its Document Understanding Conference (DUC) predecessors, and to provide feedback to developers whose goal is to improve their summarization systems." ></td>
	<td class="line x" title="8:180	However, manual evaluation of a large number of documents necessary for a relatively unbiased view is often unfeasible, especially in the contexts where repeated evaluations are needed." ></td>
	<td class="line x" title="9:180	Therefore, there is a great need for reliable automatic metrics that can perform evaluation in a fast and consistent manner." ></td>
	<td class="line x" title="10:180	In this paper, we explore one such evaluation metric, DEPEVAL(summ), based on the comparison of Lexical-Functional Grammar (LFG) dependencies between a candidate summary and one or more model (reference) summaries." ></td>
	<td class="line x" title="11:180	The method is similar in nature to Basic Elements (Hovy et al., 2005), in that it extends beyond a simple string comparison of word sequences, reaching instead to a deeper linguistic analysis of the text." ></td>
	<td class="line x" title="12:180	Both methods use hand-written extraction rules to derive dependencies from constituent parses produced by widely available Penn II Treebank parsers." ></td>
	<td class="line o" title="13:180	The difference between DEPEVAL(summ) and BE is that in DEPEVAL(summ) the dependency extraction is accomplished through an LFG annotation of Cahill et al.(2004) applied to the output of the reranking parser of Charniak and Johnson (2005), whereas in BE (in the version presented here) dependencies are generated by the Minipar parser (Lin, 1995)." ></td>
	<td class="line nc" title="15:180	Despite relying on a the same concept, our approach outperforms BE in most comparisons, and it often achieves higher correlations with human judgments than the string-matching metric ROUGE (Lin, 2004)." ></td>
	<td class="line o" title="16:180	A more detailed description of BE and ROUGE is presented in Section 2, which also gives an account of manual evaluation methods employed at TAC 2008." ></td>
	<td class="line x" title="17:180	Section 3 gives a short introduction to the LFG annotation." ></td>
	<td class="line x" title="18:180	Section 4 describes in more detail DEPEVAL(summ) and its variants." ></td>
	<td class="line x" title="19:180	Section 5 presents the experiment in which we compared the perfomance of all three metrics on the TAC 2008 data (consisting of 5,952 100-words summaries) and on the DUC 2007 data (1,620 250-word summaries) and discusses the correlations these metrics achieve." ></td>
	<td class="line x" title="20:180	Finally, Section 6 presents conclusions and some directions for future work." ></td>
	<td class="line x" title="21:180	2 Current practice in summary evaluation In the first Text Analysis Conference (TAC 2008), as well as its predecessor, the Document Understanding Conference (DUC) series, the evaluation 190 of summarization tasks was conducted using both manual and automatic methods." ></td>
	<td class="line x" title="22:180	Since manual evaluation is still the undisputed gold standard, both at TAC and DUC there was much effort to evaluate manually as much data as possible." ></td>
	<td class="line x" title="23:180	2.1 Manual evaluation Manual assessment, performed by human judges, usually centers around two main aspects of summary quality: content and form." ></td>
	<td class="line x" title="24:180	Similarly to MachineTranslation,wherethesetwoaspectsarerepresented by the categories of Accuracy and Fluency, in automatic summarization evaluation performed at TAC and DUC they surface as (Content) Responsiveness and Readability." ></td>
	<td class="line x" title="25:180	In TAC 2008 (Dang and Owczarzak, 2008), however, Content Responsiveness was replaced by Overall Responsiveness, conflating these two dimensions and reflecting the overall quality of the summary: the degree to which a summary was responding to the information need contained in the topic statement, as well as its linguistic quality." ></td>
	<td class="line x" title="26:180	A separate Readability score was still provided, assessingthefluencyandstructureindependentlyofcontent,basedonsuchaspectsasgrammaticality,nonredundancy, referential clarity, focus, structure, and coherence." ></td>
	<td class="line x" title="27:180	Both Overall Responsiveness and Readability were evaluated according to a fivepoint scale, ranging from Very Poor to Very Good." ></td>
	<td class="line x" title="28:180	ContentwasevaluatedmanuallybyNISTassessors using the Pyramid framework (Passonneau et al., 2005)." ></td>
	<td class="line x" title="29:180	In the Pyramid evaluation, assessors first extract all possible information nuggets, or Summary Content Units (SCUs) from the four human-crafted model summaries on a given topic." ></td>
	<td class="line x" title="30:180	EachSCUisassignedaweightinproportiontothe number of model summaries in which it appears, on the assumption that information which appears in most or all human-produced model summaries is more essential to the topic." ></td>
	<td class="line x" title="31:180	Once all SCUs are harvested from the model summaries, assessors determine how many of these SCUs are present in each of the automatic peer summaries." ></td>
	<td class="line x" title="32:180	The final score for an automatic summary is its total SCUweightdividedbythemaximumSCUweight available to a summary of average length (where the average length is determined by the mean SCU count of the model summaries for this topic)." ></td>
	<td class="line x" title="33:180	All types of manual assessment are expensive and time-consuming, which is why it can be rarely provided for all submitted runs in shared tasks such as the TAC Summarization track." ></td>
	<td class="line x" title="34:180	It is also not a viable tool for system developers who ideally would like a fast, reliable, and above all automatic evaluation method that can be used to improve their systems." ></td>
	<td class="line x" title="35:180	The creation and testing of automatic evaluation methods is, therefore, an important research venue, and the goal is to produce automatic metrics that will correlate with manual assessment as closely as possible." ></td>
	<td class="line x" title="36:180	2.2 Automatic evaluation Automatic metrics, because of their relative speed, can be applied more widely than manual evaluation." ></td>
	<td class="line oc" title="37:180	In TAC 2008 Summarization track, all submitted runs were scored with the ROUGE (Lin, 2004) and Basic Elements (BE) metrics (Hovy et al., 2005)." ></td>
	<td class="line o" title="38:180	ROUGE is a collection of string-comparison techniques, based on matching n-grams between a candidate string and a reference string." ></td>
	<td class="line x" title="39:180	The string in question might be a single sentence (as in the case of translation), or a set of sentences (as in the case of summaries)." ></td>
	<td class="line x" title="40:180	The variations of ROUGE range from matching unigrams (i.e. single words) to matching four-grams, with or without lemmatization and stopwords, with the options of using different weights or skip-n-grams (i.e. matchingn-gramsdespiteinterveningwords)." ></td>
	<td class="line x" title="41:180	The two versions used in TAC 2008 evaluations were ROUGE-2 and ROUGE-SU4, where ROUGE-2 calculates the proportion of matching bigrams between the candidate summary and the reference summaries, and ROUGE-SU4 is a combination of unigram match and skip-bigram match with skip distance of 4 words." ></td>
	<td class="line x" title="42:180	BE, on the other hand, employs a certain degree of linguistic analysis in the assessment process,asitrestsoncomparingtheBasicElements between the candidate and the reference." ></td>
	<td class="line x" title="43:180	Basic Elements are syntactic in nature, and comprise the heads of major syntactic constituents in the text (noun, verb, adjective, etc.) and their modifiers in a dependency relation, expressed as a triple (head, modifier, relation type)." ></td>
	<td class="line x" title="44:180	First, the input text is parsed with a syntactic parser, then Basic Elements are extracted from the resulting parse, and the candidate BEs are matched against the reference BEs." ></td>
	<td class="line x" title="45:180	In TAC 2008 and DUC 2008 evaluations the BEs were extracted with Minipar (Lin, 1995)." ></td>
	<td class="line x" title="46:180	Since BE, contrary to ROUGE, does not 191 rely solely on the surface sequence of words to determine similarity between summaries, but delves intowhatcouldbecalledashallowsemanticstructure,comprisingthematicrolessuchassubjectand object, it is likely to notice identity of meaning where such identity is obscured by variations in word order." ></td>
	<td class="line x" title="47:180	In fact, when it comes to evaluation of automatic summaries, BE shows higher correlations with human judgments than ROUGE, although the difference is not large enough to be statistically significant." ></td>
	<td class="line x" title="48:180	In the TAC 2008 evaluations, BE-HM (a version of BE where the words are stemmed and the relation type is ignored) obtained a correlation of 0.911 with human assessment of overall responsiveness and 0.949 with the Pyramid score, whereas ROUGE-2 showed correlations of 0.894 and 0.946, respectively." ></td>
	<td class="line x" title="49:180	While using dependency information is an important step towards integrating linguistic knowledge into the evaluation process, there are many ways in which this could be approached." ></td>
	<td class="line x" title="50:180	Since this type of evaluation processes information in stages (constituent parser, dependency extraction, and the method of dependency matching between a candidate and a reference), there is potential for variance in performance among dependencybased evaluation metrics that use different components." ></td>
	<td class="line x" title="51:180	Therefore, it is interesting to compare our method, which relies on the Charniak-Johnson parser and the LFG annotation, with BE, which uses Minipar to parse the input and produce dependencies." ></td>
	<td class="line x" title="52:180	3 Lexical-Functional Grammar and the LFG parser The method discussed in this paper rests on the assumptions of Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) (LFG)." ></td>
	<td class="line x" title="53:180	In LFG sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure." ></td>
	<td class="line x" title="54:180	C-structure represents the word order of the surface string and the hierarchical organisation of phrases in terms of trees." ></td>
	<td class="line x" title="55:180	F-structures are recursive feature structures, representing abstract grammatical relations such as subject, object, oblique, adjunct, etc., approximating to predicateargument structure or simple logical forms." ></td>
	<td class="line x" title="56:180	Cstructure and f-structure are related by means of functional annotations in c-structure trees, which describe f-structures." ></td>
	<td class="line x" title="57:180	While c-structure is sensitive to surface rearrangement of constituents, f-structure abstracts away from (some of) the particulars of surface realization." ></td>
	<td class="line x" title="58:180	The sentences John resigned yesterday and Yesterday, John resigned will receive different tree representations, but identical f-structures." ></td>
	<td class="line x" title="59:180	The f-structure can also be described in terms of a flat set of triples, or dependencies." ></td>
	<td class="line x" title="60:180	In triples format, the f-structure for these two sentences is represented in 1." ></td>
	<td class="line x" title="61:180	(1) subject(resign,john) person(john,3) number(john,sg) tense(resign,past) adjunct(resign,yesterday) person(yesterday,3) number(yesterday,sg) Cahill et al.(2004), in their presentation of LFG parsing resources, distinguish 32 types of dependencies, divided into two major groups: a group of predicate-only dependencies and nonpredicate dependencies." ></td>
	<td class="line x" title="63:180	Predicate-only dependencies are those whose path ends in a predicatevalue pair, describing grammatical relations." ></td>
	<td class="line x" title="64:180	For instance, in the sentence John resigned yesterday, predicate-only dependencies would include: subject(resign, john) and adjunct(resign, yesterday), while non-predicate dependencies are person(john,3), number(john,sg), tense(resign,past), person(yesterday,3), num(yesterday,sg)." ></td>
	<td class="line x" title="65:180	Other predicate-only dependencies include: apposition, complement, open complement, coordination, determiner, object, second object, oblique, second oblique, oblique agent, possessive, quantifier, relative clause, topic, and relative clause pronoun." ></td>
	<td class="line x" title="66:180	The remaining non-predicate dependencies are: adjectival degree, coordination surface form, focus, complementizer forms: if, whether, and that, modal, verbal particle, participle, passive, pronoun surface form, and infinitival clause." ></td>
	<td class="line x" title="67:180	These 32 dependencies, produced by LFG annotation, and the overlap between the set of dependencies derived from the candidate summary and the reference summaries, form the basis of our evaluation method, which we present in Section 4." ></td>
	<td class="line x" title="68:180	First, a summary is parsed with the CharniakJohnson reranking parser (Charniak and Johnson, 2005) to obtain the phrase-structure tree." ></td>
	<td class="line x" title="69:180	Then, a sequence of scripts annotates the output, translating the relative phrase position into f-structural dependencies." ></td>
	<td class="line x" title="70:180	The treebank-based LFG annotation used in this paper and developed by Cahill et al.(2004) obtains high precision and recall rates." ></td>
	<td class="line x" title="72:180	As reported in Cahill et al.(2008), the version of 192 the LFG parser which applies the LFG annotation algorithm to the earlier Charniaks parser (Charniak, 2000) obtains an f-score of 86.97 on the Wall Street Journal Section 23 test set." ></td>
	<td class="line x" title="74:180	The LFG parser is robust as well, with coverage levels exceeding 99.9%, measured in terms of complete spanning parse." ></td>
	<td class="line x" title="75:180	4 Dependency-based evaluation Our dependency-based evaluation method, similarly to BE, compares two unordered sets of dependencies: one bag contains dependencies harvested from the candidate summary and the other contains dependencies from one or more reference summaries." ></td>
	<td class="line x" title="76:180	Overlap between the candidate bag and the reference bag is calculated in the form of precision, recall, and the f-measure (with precision and recall equally weighted)." ></td>
	<td class="line x" title="77:180	Since for ROUGE and BE the only reported score is recall, we present recall results here as well, calculated as in 2: (2) DEPEVAL(summ) Recall = |Dcand||Dref||Dref| where Dcand are the candidate dependencies andDref are the reference dependencies." ></td>
	<td class="line x" title="78:180	The dependency-based method using LFG annotation has been successfully employed in the evaluation of Machine Translation (MT)." ></td>
	<td class="line x" title="79:180	In Owczarzak (2008), the method achieves equal or higher correlations with human judgments than METEOR (Banerjee and Lavie, 2005), one of the best-performingautomaticMTevaluationmetrics." ></td>
	<td class="line x" title="80:180	However, it is not clear that the method can be applied without change to the task of assessing automatic summaries; after all, the two tasks of summarization and translation produce outputs that are different in nature." ></td>
	<td class="line x" title="81:180	In MT, the unit of text is a sentence; text is translated, and the translation evaluated, sentence by sentence." ></td>
	<td class="line x" title="82:180	In automatic summarization, the output unit is a summary with length varying depending on task, but which most often consists of at least several sentences." ></td>
	<td class="line x" title="83:180	This has bearing on the matching process: with several sentences on the candidate and reference side each, there is increased possibility of trivial matches, such as dependencies containing function words, which might inflate the summary score even in the absence of important content." ></td>
	<td class="line x" title="84:180	This is particularly likely if we were to employ partial matching for dependencies." ></td>
	<td class="line x" title="85:180	Partial matching (indicated in the result tables with the tag pm) splits each predicate dependency into two, replacing one or the other element with a variable, e.g. for the dependency subject(resign, John) we would obtain two partial dependencies subject(resign, x) and subject(x, John)." ></td>
	<td class="line x" title="86:180	This process helps circumvent some of the syntactic and lexical variation between a candidate and a reference, and it proved very useful in MT evaluation (Owczarzak, 2008)." ></td>
	<td class="line x" title="87:180	In summary evaluation, as will be shown in Section 5, it leads to higher correlations with human judgments only in the case of human-produced model summaries, because almost any variation between two model summaries is legal, i.e. either a paraphrase or another, but equally relevant, piece of information." ></td>
	<td class="line x" title="88:180	For automatic summaries, which are of relatively poor quality, partial matching lowers our methods ability to reflect human judgment, because it results in overly generous matching in situations where the examined information is neither a paraphrase nor relevant." ></td>
	<td class="line x" title="89:180	Similarly, evaluating a summary against the union of all references, as we do in the baseline version of our method, increases the pool of possible matches, but may also produce score inflation through matching repetitive information across models." ></td>
	<td class="line x" title="90:180	To deal with this, we produce a version of the score (marked in the result tables with the tag one) that counts only one hit for every dependency match, independent of how many instances of a given dependency are present in the comparison." ></td>
	<td class="line x" title="91:180	The use of WordNet1 module (Rennie, 2000) did not provide a great advantage (see results tagged with wn), and sometimes even lowered our correlations, especially in evaluation of automatic systems." ></td>
	<td class="line x" title="92:180	This makes sense if we take into consideration that WordNet lists all possible synonyms for all possible senses of a word, and so, given a great number of cross-sentence comparisons in multi-sentence summaries, there is an increased risk of spurious matches between words which, despite being potentially synonymous in certain contexts, are not equivalent in the text." ></td>
	<td class="line x" title="93:180	Another area of concern was the potential noise introduced by the parser and the annotation process." ></td>
	<td class="line x" title="94:180	Due to parsing errors, two otherwise equivalent expressions might be encoded as differing sets of dependencies." ></td>
	<td class="line x" title="95:180	In MT evaluation, the dependency-based method can alleviate parser 1http://wordnet.princeton.edu/ 193 noise by comparing n-best parses for the candidate andthereference(Owczarzaketal.,2007),butthis is not an efficient solution for comparing multisentence summaries." ></td>
	<td class="line x" title="96:180	We have therefore attempted to at least partially counteract this issue by removing relation labels from the dependencies (i.e. producing dependencies of the form (resign, John) instead of subject(resign, John)), which did provide someimprovement(seeresultstaggedwithnorel)." ></td>
	<td class="line x" title="97:180	Finally, we experimented with a predicate-only version of the evaluation, where only the predicate dependencies participate in the comparison, excluding dependencies that provide purely grammatical information such as person, tense, or number (tagged in the results table as pred)." ></td>
	<td class="line x" title="98:180	This move proved beneficial only in the case of system summaries, perhaps by decreasing the number of trivial matches, but decreased the methods correlation for model summaries, where such detailed information might be necessary to assess the degree of similarity between two human summaries." ></td>
	<td class="line x" title="99:180	5 Experimental results The first question we have to ask is: which of the manual evaluation categories do we want our metric to imitate?" ></td>
	<td class="line x" title="100:180	It is unlikely that a single automatic measure will be able to correctly reflect both Readability and Content Responsiveness, as form and content are separate qualities and need different measures." ></td>
	<td class="line x" title="101:180	Content seems to be the more important aspect, especially given that Readability can be partially derived from Responsiveness (a summary high in content cannot be very low in readability, although some very readable summaries can have little relevant content)." ></td>
	<td class="line x" title="102:180	Content Responsiveness was provided in DUC 2007 data, but not in TAC 2008, where the extrinsic Pyramid measure was used to evaluate content." ></td>
	<td class="line x" title="103:180	It is, in fact, preferable to compare our metric against thePyramidscoreratherthanContentResponsiveness, because both the Pyramid and our method aim to measure the degree of similarity between a candidate and a model, whereas Content Responsiveness is a direct assessment of whether the summarys content is adequate given a topic and a source text." ></td>
	<td class="line x" title="104:180	The Pyramid is, at the same time, a costly manual evaluation method, so an automatic metric that successfully emulates it would be a useful replacement." ></td>
	<td class="line x" title="105:180	Another question is whether we focus on system-level or summary-level evaluation." ></td>
	<td class="line x" title="106:180	The correlation values at the summary-level are generally much lower than on the system-level, which means the metrics are better at evaluating system performance than the quality of individual summaries." ></td>
	<td class="line x" title="107:180	System-level evaluations are essential to shared summarization tasks; summary-level assessment might be useful to developers who want to test the effect of particular improvements in their system." ></td>
	<td class="line x" title="108:180	Of course, the ideal evaluation metric would show high correlations with human judgment on both levels." ></td>
	<td class="line x" title="109:180	We used the data from the TAC 2008 and DUC 2007 Summarization tracks." ></td>
	<td class="line x" title="110:180	The first set comprised 58 system submissions and 4 humanproduced model summaries for each of the 96 subtopics (there were 48 topics, each of which required two summaries: a main and an update summary), as well as human-produced Overall Responsiveness and Pyramid scores for each summary." ></td>
	<td class="line x" title="111:180	The second set included 32 system submissions and 4 human models for each of the 45 topics." ></td>
	<td class="line x" title="112:180	For fair comparison of models and systems, we used jackknifing: while each model was evaluated against the remaining three models, each system summary was evaluated four times, each time against a different set of three models, and the four scores were averaged." ></td>
	<td class="line x" title="113:180	5.1 System-level correlations Table 1 presents system-level Pearsons correlations between the scores provided by our dependency-based metric DEPEVAL(summ), as well as the automatic metrics ROUGE-2, ROUGE-SU4, and BE-HM used in the TAC evaluation, and the manual Pyramid scores, which measured the content quality of the systems." ></td>
	<td class="line x" title="114:180	It also includes correlations with the manual Overall Responsiveness score, which reflected both content and linguistic quality." ></td>
	<td class="line x" title="115:180	Table 3 shows the correlations with Content Responsiveness for DUC 2007 data for ROUGE, BE, and those few select versions of DEPEVAL(summ) which achieve optimal results on TAC 2008 data (for a more detailed discussion of the selection see Section 6)." ></td>
	<td class="line x" title="116:180	The correlations are listed for the following versions of our method: pm partial matching for dependencies; wn WordNet; pred matching predicate-only dependencies; norel ignoring dependency relation label; one counting a match only once irrespective of how many instances of 194 TAC 2008 Pyramid Overall Responsiveness Metric models systems models systems DEPEVAL(summ): Variations base 0.653 0.931 0.883 0.862 pm 0.690 0.811 0.943 0.740 wn 0.687 0.929 0.888 0.860 pred 0.415 0.946 0.706 0.909 norel 0.676 0.929 0.880 0.861 one 0.585 0.958* 0.858 0.900 DEPEVAL(summ): Combinations pm wn 0.694 0.903 0.952* 0.839 pm pred 0.534 0.880 0.898 0.831 pm norel 0.722 0.907 0.936 0.835 pm one 0.611 0.950 0.876 0.895 wn pred 0.374 0.946 0.716 0.912 wn norel 0.405 0.941 0.752 0.905 wn one 0.611 0.952 0.856 0.897 pred norel 0.415 0.945 0.735 0.905 pred one 0.415 0.953 0.721 0.921* norel one 0.600 0.958* 0.863 0.900 pm wn pred 0.527 0.870 0.905 0.821 pm wn norel 0.738 0.897 0.931 0.826 pm wn one 0.634 0.936 0.887 0.881 pm pred norel 0.642 0.876 0.946 0.815 pm pred one 0.504 0.948 0.817 0.907 pm norel one 0.725 0.941 0.905 0.880 wn pred norel 0.433 0.944 0.764 0.906 wn pred one 0.385 0.950 0.722 0.919 wn norel one 0.632 0.954 0.872 0.896 pred norel one 0.452 0.955 0.756 0.919 pm wn pred norel 0.643 0.861 0.940 0.800 pm wn pred one 0.486 0.932 0.809 0.890 pm pred norel one 0.711 0.939 0.881 0.891 pm wn norel one 0.743* 0.930 0.902 0.870 wn pred norel one 0.467 0.950 0.767 0.918 pm wn pred norel one 0.712 0.927 0.887 0.880 Other metrics ROUGE-2 0.277 0.946 0.725 0.894 ROUGE-SU4 0.457 0.928 0.866 0.874 BE-HM 0.423 0.949 0.656 0.911 Table 1: System-level Pearsons correlation between automatic and manual evaluation metrics for TAC 2008 data." ></td>
	<td class="line x" title="117:180	a particular dependency are present in the candidate and reference." ></td>
	<td class="line x" title="118:180	For each of the metrics, including ROUGE and BE, we present the correlationsforrecall." ></td>
	<td class="line x" title="119:180	Thehighestresultineachcategory is marked by an asterisk." ></td>
	<td class="line x" title="120:180	The background gradient indicates whether DEPEVAL(summ) correlation is higher than all three competitors ROUGE2, ROUGE-SU4, andBE(darkestgrey), twoofthe three (medium grey), one of the three (light grey), or none (white)." ></td>
	<td class="line x" title="121:180	The 95% confidence intervals are not included here for reasons of space, but their comparison suggests that none of the system-level differences in correlation levels are large enough to be significant." ></td>
	<td class="line x" title="122:180	This is because the intervals themselves are very wide, due to relatively small number of summarizers (58 automatic and 8 human for TAC; 32 automatic and 10 human for DUC) involved in the comparison." ></td>
	<td class="line x" title="123:180	5.2 Summary-level correlations Tables 2 and 4 present the same correlations, but this time on the level of individual summaries." ></td>
	<td class="line x" title="124:180	As before, the highest level in each category is marked by an asterisk." ></td>
	<td class="line x" title="125:180	Contrary to system-level, here some correlations obtained by DEPEVAL(summ) are significantly higher than those achieved by the three competing metrics, ROUGE-2, ROUGE-SU4, and BE-HM, as determined by the confidence intervals." ></td>
	<td class="line x" title="126:180	The letters in parenthesis indicate that a given DEPEVAL(summ) variant is significantly better at correlating with human judgment than ROUGE-2 (= R2), ROUGE-SU4 (= R4), or BE-HM (= B)." ></td>
	<td class="line x" title="127:180	6 Discussion and future work It is obvious that none of the versions performs best across the board; their different characteristics might render them better suited either for models or for automatic systems, but not for both at the same time." ></td>
	<td class="line x" title="128:180	This can be explained if we understand that evaluating human gold standard summaries and automatically generated summaries of poor-to-medium quality is, in a way, not the same task." ></td>
	<td class="line x" title="129:180	Given that human models are by default well-formed and relevant, relaxing any restraints on matching between them (i.e. allowing partial dependencies, removing the relation label, or adding synonyms) serves, in effect, to accept as correct either (1) the same conceptual information expressed in different ways (where the difference might be real or introduced by faulty parsing), or (2) other information, yet still relevant to the topic." ></td>
	<td class="line x" title="130:180	Accepting information of the former type as correct will ratchet up the score for the summaryandthecorrelationwiththesummarysPyramid score, which measures identity of information across summaries." ></td>
	<td class="line x" title="131:180	Accepting the first and second type of information will raise the score and the correlation with Responsiveness, which measures relevance of information to the particular topic." ></td>
	<td class="line x" title="132:180	However, inevaluating system summaries such relaxation of matching constraints will result in accepting irrelevant and ungrammatical information ascorrect,drivinguptheDEPEVAL(summ)score, but lowering its correlation with both Pyramid and Responsiveness." ></td>
	<td class="line x" title="133:180	Insimplewords,itisokaytogive a model summary the benefit of doubt, and accept its content as correct even if it is not matching other model summaries exactly, but the same strategy applied to a system summary might cause mass over-estimation of the summarys quality." ></td>
	<td class="line x" title="134:180	This substantial difference in the nature of human-generated models and system-produced summaries has impact on all automatic means of evaluation, as long as we are limited to methods that operate on more shallow levels than a full 195 TAC 2008 Pyramid Overall Responsiveness Metric models systems models systems DEPEVAL(summ): Variations base 0.436 (B) 0.595 (R2,R4,B) 0.186 0.373 (R2,B) pm 0.467 (B) 0.584 (R2,B) 0.183 0.368 (B) wn 0.448 (B) 0.592 (R2,B) 0.192 0.376 (R2,R4,B) pred 0.344 0.543 (B) 0.170 0.327 norel 0.437 (B) 0.596* (R2,R4,B) 0.186 0.373 (R2,B) one 0.396 0.587 (R2,B) 0.171 0.376 (R2,R4,B) DEPEVAL(summ): Combinations pm wn 0.474 (B) 0.577 (R2,B) 0.194* 0.371 (R2,B) pm pred 0.407 0.537 (B) 0.153 0.337 pm norel 0.483 (R2,B) 0.584 (R2,B) 0.168 0.362 pm one 0.402 0.577 (R2,B) 0.167 0.384 (R2,R4,B) wn pred 0.352 0.537 (B) 0.182 0.328 wn norel 0.364 0.541 (B) 0.187 0.329 wn one 0.411 0.581 (R2,B) 0.182 0.384 (R2,R4,B) pred norel 0.351 0.547 (B) 0.169 0.327 pred one 0.325 0.542 (B) 0.171 0.347 norel one 0.403 0.589 (R2,B) 0.176 0.377 (R2,R4,B) pm wn pred 0.415 0.526 (B) 0.167 0.337 pm wn norel 0.488* (R2,R4,B) 0.576 (R2,B) 0.168 0.366 (B) pm wn one 0.417 0.563 (B) 0.179 0.389* (R2,R4.B) pm pred norel 0.433 (B) 0.538 (B) 0.124 0.333 pm pred one 0.357 0.545 (B) 0.151 0.381 (R2,R4,B) pm norel one 0.437 (B) 0.567 (R2,B) 0.174 0.369 (B) wn pred norel 0.353 0.541 (B) 0.180 0.324 wn pred one 0.328 0.535 (B) 0.179 0.346 wn norel one 0.416 0.584 (R2,B) 0.185 0.385 (R2,R4,B) pred norel one 0.336 0.549 (B) 0.169 0.351 pm wn pred norel 0.428 (B) 0.524 (B) 0.120 0.334 pm wn pred one 0.363 0.525 (B) 0.164 0.380 (R2,R4,B) pm pred norel one 0.420 (B) 0.533 (B) 0.154 0.375 (R2,R4,B) pm wn norel one 0.452 (B) 0.558 (B) 0.179 0.376 (R2,R4,B) wn pred norel one 0.338 0.544 (B) 0.178 0.349 pm wn pred norel one 0.427 (B) 0.522 (B) 0.153 0.379 (R2,R4,B) Other metrics ROUGE-2 0.307 0.527 0.098 0.323 ROUGE-SU4 0.318 0.557 0.153 0.327 BE-HM 0.239 0.456 0.135 0.317 Table2: Summary-levelPearsonscorrelationbetweenautomaticandmanual evaluation metrics for TAC 2008 data." ></td>
	<td class="line x" title="135:180	DUC 2007 Content Responsiveness Metric models systems DEPEVAL(summ) 0.7341 0.8429 DEPEVAL(summ) wn 0.7355 0.8354 DEPEVAL(summ) norel 0.7394 0.8277 DEPEVAL(summ) one 0.7507 0.8634 ROUGE-2 0.4077 0.8772 ROUGE-SU4 0.2533 0.8297 BE-HM 0.5471 0.8608 Table 3: System-level Pearsons correlation between automatic metrics and Content Responsiveness for DUC 2007 data." ></td>
	<td class="line x" title="136:180	For model summaries, only DEPEVAL correlations are significant (the 95% confidence interval does not include zero)." ></td>
	<td class="line x" title="137:180	None of the differences between metrics are significant at the 95% level." ></td>
	<td class="line x" title="138:180	DUC 2007 Content Responsiveness Metric models systems DEPEVAL(summ) 0.2059 0.4150 DEPEVAL(summ) wn 0.2081 0.4178 DEPEVAL(summ) norel 0.2119 0.4185 DEPEVAL(summ) one 0.1999 0.4101 ROUGE-2 0.1501 0.3875 ROUGE-SU4 0.1397 0.4264 BE-HM 0.1330 0.3722 Table 4: Summary-level Pearsons correlation between automatic metrics and Content Responsiveness for DUC 2007 data." ></td>
	<td class="line x" title="139:180	ROUGE-SU4 and BE correlations for model summaries are not statistically significant." ></td>
	<td class="line x" title="140:180	None of the differences between metrics are significant at the 95% level." ></td>
	<td class="line x" title="141:180	semantic and pragmatic analysis against humanlevel world knowledge." ></td>
	<td class="line x" title="142:180	The problem is twofold: first, our automatic metrics measure identity rather than quality." ></td>
	<td class="line x" title="143:180	Similarity of content between a candidate summary and one or more references is acting as a proxy measure for the quality of the candidate summary; yet, we cannot forget that the relation between these two features is not purely linear." ></td>
	<td class="line x" title="144:180	A candidate highly similar to the reference will be, necessarily, of good quality, but a candidate which is dissimilar from a reference is not necessarily of low quality (vide the case of parallel model summaries, which almost always contain some non-overlapping information)." ></td>
	<td class="line x" title="145:180	The second problem is the extent to which our metrics are able to distinguish content through the veil of differing forms." ></td>
	<td class="line x" title="146:180	Synonyms, paraphrases, or pragmatic features such as the choice of topic and focus render simple string-matching techniques ineffective, especially in the area of summarization where the evaluation happens on a supra-sentential level." ></td>
	<td class="line x" title="147:180	As a result, then, a lot of effort was put into developing metrics that can identify similar content despite non-similar form, which naturally led to the application of linguistically-oriented approaches that look beyond surface word order." ></td>
	<td class="line x" title="148:180	Essentially, though, we are using imperfect measures of similarity as an imperfect stand-in for quality, and the accumulated noise often causes a divergence in our metrics performance with model and system summaries." ></td>
	<td class="line x" title="149:180	Much like the inverse relation of precision and recall, changes and additions that improve a metrics correlation with human scores for model summaries often weaken the correlation for system summaries, and vice versa." ></td>
	<td class="line x" title="150:180	Admittedly, we could just ignore this problem and focus on increasing correlations for automatic summaries only; after all, the whole point of creating evaluation metrics is to score and rank the output of systems." ></td>
	<td class="line x" title="151:180	Such a perspective can be rather short-sighted, though, given that we expect continuous improvement from the summarization systems to, ideally, human levels, so the same issueswhichnowpreventhighcorrelationsformodels will start surfacing in evaluation of systemproduced summaries as well." ></td>
	<td class="line x" title="152:180	Using metrics that only perform reliably for low-quality summaries might prevent us from noticing when those summaries become better." ></td>
	<td class="line x" title="153:180	Our goal should be, therefore, to develop a metric which obtains high correlations in both categories, with the assumption that such a metric will be more reliable in evaluating summaries of varying quality." ></td>
	<td class="line x" title="154:180	196 Since there is no single winner among all 32 variants of DEPEVAL(summ) on TAC 2008 data, wemustdecidewhichofthecategoriesismostimportant to a successful automatic evaluation metric." ></td>
	<td class="line x" title="155:180	Correlations with Overall Responsiveness are in general lower than those with the Pyramid score (except in the case of system-level models)." ></td>
	<td class="line x" title="156:180	This makes sense, if we rememeber that Overall Responsiveness judges content as well as linguistic quality, which are two different dimensions and so a single automatic metric is unlikely to reflect it well, and that it judges content in terms of its relevance to topic, which is also beyond the reach of contemporary metrics which can at most judge content similarity to a model." ></td>
	<td class="line x" title="157:180	This means that the Pyramid score makes for a more relevant metric to emulate." ></td>
	<td class="line x" title="158:180	The last dilemma is whether we choose to focus on systemor summary-level correlations." ></td>
	<td class="line x" title="159:180	This ties in with the purpose which the evaluation metric should serve." ></td>
	<td class="line x" title="160:180	In comparisons of multiple systems, such as in TAC 2008, the value is placed in the correct ordering of these systems; while summary-level assessment can give us important feedback and insight during the system development stage." ></td>
	<td class="line x" title="161:180	The final choice among all DEPEVAL(summ) versions hinges on all of these factors: we should prefer a variant which correlates highly with the Pyramid score rather than with Responsiveness, which minimizes the gap between model and automatic peer correlations while retaining relatively high values for both, and which fulfills these requirements similarly well on both summaryand system-levels." ></td>
	<td class="line x" title="162:180	Three such variants are the baseline DEPEVAL(summ), the WordNet version DEPEVAL(summ) wn, and the version with removed relation labels DEPEVAL(summ) norel." ></td>
	<td class="line x" title="163:180	Both the baselineandnorelversionsachievesignificantimprovement over ROUGE and BE in correlations with the Pyramid score for automatic summaries, and over BE for models, on the summary level." ></td>
	<td class="line x" title="164:180	In fact, almost in all categories they achieve higher correlations than ROUGE and BE." ></td>
	<td class="line x" title="165:180	The only exceptions are the correlations with Pyramid for systems at the system-level, but there the results are close and none of the differences in that category are significant." ></td>
	<td class="line x" title="166:180	To balance this exception, DEPEVAL(summ) achieves much higher correlations withthePyramidscoresformodelsummariesthan either ROUGE or BE on the system level." ></td>
	<td class="line x" title="167:180	In order to see whether the DEPEVAL(summ) advantage holds for other data, we examined the most optimal versions (baseline, wn, norel, as well as one, which is the closest counterpart to label-free BE-HM) on data from DUC 2007." ></td>
	<td class="line x" title="168:180	Because only a portion of the DUC 2007 data was evaluated with Pyramid, we chose to look rather at the Content Responsiveness scores." ></td>
	<td class="line x" title="169:180	As can be seen in Tables 3 and 4, the same patterns hold: decided advantage over ROUGE/BE when it comes to model summaries (especially at system-level), comparable results for automatic summaries." ></td>
	<td class="line x" title="170:180	Since DUC 2007 data consisted of fewer summaries (1,620 vs 5,952 at TAC) and fewer submissions (32 vs 57 at TAC), some results did not reach statistical significance." ></td>
	<td class="line x" title="171:180	In Table 3, in the models category, only DEPEVAL(summ) correlations are significant." ></td>
	<td class="line x" title="172:180	In Table 4, in the model category, only DEPEVAL(summ) and ROUGE-2 correlations are significant." ></td>
	<td class="line x" title="173:180	Note also that these correlations with Content Responsiveness are generally lower than those with Pyramid in previous tables, but in the case of summary-level comparison higher than the correlations with Overall Responsiveness." ></td>
	<td class="line x" title="174:180	This is to be expected given our earlier discussion of the differences in what these metrics measure." ></td>
	<td class="line x" title="175:180	As mentioned before, the dependency-based evaluation can be approached from different angles, leading to differences in performance." ></td>
	<td class="line x" title="176:180	This is exemplified in our experiment, where DEPEVAL(summ) outperforms BE, even though both these metrics rest on the same general idea." ></td>
	<td class="line x" title="177:180	The new implementation of BE presented at the TAC 2008 workshop (Tratz and Hovy, 2008) introduces transformations for dependencies in order to increasethenumberofmatchesamongelementsthat are semantically similar yet differ in terms of syntactic structure and/or lexical choices, and adds WordNet for synonym matching." ></td>
	<td class="line x" title="178:180	Its core modules were updated as well: Minipar was replaced with the Charniak-Johnson reranking parser (Charniak and Johnson, 2005), Named Entity identification was added, and the BE extraction is conducted usingasetofTregexrules(LevyandAndrew, 2006)." ></td>
	<td class="line x" title="179:180	Since our method, presented in this paper, also uses the reranking parser, as well as WordNet, it would be interesting to compare both methods directly in terms of the performance of the dependency extraction procedure." ></td>
	<td class="line x" title="180:180	197" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1024
Automatically Generating Wikipedia Articles: A Structure-Aware Approach
Sauper, Christina;Barzilay, Regina;"></td>
	<td class="line x" title="1:261	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 208216, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:261	c2009 ACL and AFNLP Automatically Generating Wikipedia Articles: A Structure-Aware Approach Christina Sauper and Regina Barzilay Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology {csauper,regina}@csail.mit.edu Abstract In this paper, we investigate an approach for creating a comprehensive textual overview of a subject composed of informationdrawnfromtheInternet." ></td>
	<td class="line x" title="3:261	Weuse thehigh-levelstructureofhuman-authored texts to automatically induce a domainspecific template for the topic structure of a new overview." ></td>
	<td class="line x" title="4:261	The algorithmic innovation of our work is a method to learn topicspecific extractors for content selection jointly for the entire template." ></td>
	<td class="line x" title="5:261	We augment the standard perceptron algorithm with a global integer linear programming formulation to optimize both local fit of information into each topic and global coherence across the entire overview." ></td>
	<td class="line x" title="6:261	The results of our evaluation confirm the benefits of incorporating structural information into the content selection process." ></td>
	<td class="line x" title="7:261	1 Introduction Inthispaper, weconsiderthetaskofautomatically creating a multi-paragraph overview article that providesacomprehensivesummaryofasubjectof interest." ></td>
	<td class="line x" title="8:261	Examples of such overviews include actor biographies from IMDB and disease synopses from Wikipedia." ></td>
	<td class="line x" title="9:261	Producing these texts by hand is a labor-intensive task, especially when relevant information is scattered throughout a wide range of Internet sources." ></td>
	<td class="line x" title="10:261	Our goal is to automate this process." ></td>
	<td class="line x" title="11:261	We aim to create an overview of a subject  e.g., 3-M Syndrome  by intelligently combining relevant excerpts from across the Internet." ></td>
	<td class="line x" title="12:261	As a starting point, we can employ methodsdevelopedformulti-documentsummarization." ></td>
	<td class="line x" title="13:261	However, our task poses additional technical challenges with respect to content planning." ></td>
	<td class="line x" title="14:261	Generating a well-rounded overview article requires proactive strategies to gather relevant material, such as searching the Internet." ></td>
	<td class="line x" title="15:261	Moreover, the challenge of maintaining output readability is magnified when creating a longer document that discusses multiple topics." ></td>
	<td class="line x" title="16:261	In our approach, we explore how the highlevel structure of human-authored documents can be used to produce well-formed comprehensive overview articles." ></td>
	<td class="line x" title="17:261	We select relevant material for an article using a domain-specific automatically generated content template." ></td>
	<td class="line x" title="18:261	For example, a template for articles about diseases might contain diagnosis, causes, symptoms, and treatment." ></td>
	<td class="line x" title="19:261	Our system induces these templates by analyzing patterns in the structure of human-authored documents in the domain of interest." ></td>
	<td class="line x" title="20:261	Then, it produces anewarticlebyselectingcontentfromtheInternet for each part of this template." ></td>
	<td class="line x" title="21:261	An example of our systems output1 is shown in Figure 1." ></td>
	<td class="line x" title="22:261	The algorithmic innovation of our work is a method for learning topic-specific extractors for contentselectionjointlyacrosstheentiretemplate." ></td>
	<td class="line x" title="23:261	Learning a single topic-specific extractor can be easily achieved in a standard classification framework." ></td>
	<td class="line x" title="24:261	However, the choices for different topics in a template are mutually dependent; for example, in a multi-topic article, there is potential for redundancy across topics." ></td>
	<td class="line x" title="25:261	Simultaneously learning content selection for all topics enables us to explicitly model these inter-topic connections." ></td>
	<td class="line x" title="26:261	Weformulatethistaskasastructuredclassification problem." ></td>
	<td class="line x" title="27:261	We estimate the parameters of our model using the perceptron algorithm augmented with an integer linear programming (ILP) formulation, run over a training set of example articles in the given domain." ></td>
	<td class="line x" title="28:261	The key features of this structure-aware approach are twofold: 1ThissystemoutputwasaddedtoWikipediaathttp:// en.wikipedia.org/wiki/3-M syndrome on June 26, 2008." ></td>
	<td class="line x" title="29:261	The pages history provides examples of changes performedbyhumaneditorstoarticlescreatedbyoursystem." ></td>
	<td class="line x" title="30:261	208 Diagnosis No laboratories offering molecular genetic testing for prenatal diagnosis of 3-M syndrome are listed in the GeneTests Laboratory Directory." ></td>
	<td class="line x" title="31:261	However, prenatal testing may be available for families in which the disease-causing mutations have been identified in an affected family member in a research or clinical laboratory." ></td>
	<td class="line x" title="32:261	Causes Three M syndrome is thought to be inherited as an autosomal recessive genetic trait." ></td>
	<td class="line x" title="33:261	Human traits, including the classic genetic diseases, are the product of the interaction of two genes, one received from the father and one from the mother." ></td>
	<td class="line x" title="34:261	In recessive disorders, the condition does not occur unless an individual inherits the same defective gene for the same trait from each parent." ></td>
	<td class="line x" title="35:261	Symptoms Many of the symptoms and physical features associated with the disorder are apparent at birth (congenital)." ></td>
	<td class="line x" title="36:261	In some cases, individuals who carry a single copy of the disease gene (heterozygotes) may exhibit mild symptoms associated with Three M syndrome." ></td>
	<td class="line x" title="37:261	Treatment Genetic counseling will be of benefit for affected individuals and their families." ></td>
	<td class="line x" title="38:261	Family members of affected individuals should also receive regular clinical evaluations to detect any symptoms and physical characteristics that may be potentially associated with Three M syndrome or heterozygosity for the disorder." ></td>
	<td class="line x" title="39:261	Other treatment for Three M syndrome is symptomatic and supportive." ></td>
	<td class="line x" title="40:261	Figure 1: A fragment from the automatically created article for 3-M Syndrome." ></td>
	<td class="line x" title="41:261	 Automatic template creation: Templates are automatically induced from humanauthored documents." ></td>
	<td class="line x" title="42:261	This ensures that the overview article will have the breadth expected in a comprehensive summary, with content drawn from a wide variety of Internet sources." ></td>
	<td class="line x" title="43:261	 Joint parameter estimation for content selection: Parameters are learned jointly for all topics in the template." ></td>
	<td class="line x" title="44:261	This procedure optimizes both local relevance of information for each topic and global coherence across the entire article." ></td>
	<td class="line x" title="45:261	We evaluate our approach by creating articles in two domains: Actors and Diseases." ></td>
	<td class="line x" title="46:261	For a data set, we use Wikipedia, which contains articles similar to those we wish to produce in terms of length and breadth." ></td>
	<td class="line x" title="47:261	An advantage of this data set is that Wikipedia articles explicitly delineate topical sections, facilitating structural analysis." ></td>
	<td class="line x" title="48:261	The results of our evaluation confirm the benefits of structureaware content selection over approaches that do not explicitly model topical structure." ></td>
	<td class="line x" title="49:261	2 Related Work Concept-to-text generation and text-to-text generation take very different approaches to content selection." ></td>
	<td class="line x" title="50:261	In traditional concept-to-text generation, a content planner provides a detailed template for what information should be included in the output andhowthisinformationshouldbeorganized(Reiter and Dale, 2000)." ></td>
	<td class="line x" title="51:261	In text-to-text generation, such templates for information organization are not available; sentences are selected based on their salience properties (Mani and Maybury, 1999)." ></td>
	<td class="line x" title="52:261	While this strategy is robust and portable across domains, output summaries often suffer from coherence and coverage problems." ></td>
	<td class="line x" title="53:261	In between these two approaches is work on domain-specific text-to-text generation." ></td>
	<td class="line x" title="54:261	Instances of these tasks are biography generation in summarization and answering definition requests in question-answering." ></td>
	<td class="line x" title="55:261	In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain." ></td>
	<td class="line x" title="56:261	This characterization varies greatly in granularity." ></td>
	<td class="line x" title="57:261	For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsyetal.,2008),whileothersgobeyondbinary distinction by identifying atomic events  e.g., occupation and marital status  that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006)." ></td>
	<td class="line x" title="58:261	Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004)." ></td>
	<td class="line x" title="59:261	Our work is related to these approaches; however, content selection in our work is driven by domain-specific automatically induced templates." ></td>
	<td class="line x" title="60:261	As our experiments demonstrate, patterns observed in domain-specific training data provide sufficient constraints for topic organization, which is crucial for a comprehensive text." ></td>
	<td class="line x" title="61:261	Our work also relates to a large body of recent work that uses Wikipedia material." ></td>
	<td class="line x" title="62:261	Instances of this work include information extraction, ontology induction and resource acquisition (Wu and Weld, 2007; Biadsy et al., 2008; Nastase, 2008; Nastase and Strube, 2008)." ></td>
	<td class="line x" title="63:261	Our focus is on a different task  generation of new overview articles that follow the structure of Wikipedia articles." ></td>
	<td class="line x" title="64:261	209 3 Method The goal of our system is to produce a comprehensive overview article given a title  e.g., Cancer." ></td>
	<td class="line x" title="65:261	We assume that relevant information on the subject is available on the Internet but scattered among several pages interspersed with noise." ></td>
	<td class="line x" title="66:261	We are provided with a training corpus consisting of n documents d1 dn in the same domain  e.g., Diseases." ></td>
	<td class="line x" title="67:261	Each document di has a title and a set of delineated sections2 si1 sim." ></td>
	<td class="line x" title="68:261	The numberofsectionsmvariesbetweendocuments." ></td>
	<td class="line x" title="69:261	Each section sij also has a corresponding heading hij  e.g., Treatment." ></td>
	<td class="line x" title="70:261	Our overview article creation process consists of three parts." ></td>
	<td class="line x" title="71:261	First, a preprocessing step creates a template and searches for a number of candidate excerpts from the Internet." ></td>
	<td class="line x" title="72:261	Next, parameters must be trained for the content selection algorithm using our training data set." ></td>
	<td class="line x" title="73:261	Finally, a complete article may be created by combining a selection of candidate excerpts." ></td>
	<td class="line x" title="74:261	1." ></td>
	<td class="line x" title="75:261	Preprocessing (Section 3.1) Our preprocessing step leverages previous work in topic segmentation and query reformulation to prepare a template and a set of candidate excerpts for content selection." ></td>
	<td class="line x" title="76:261	Template generation must occur once per domain, whereas search occurs every time an article is generated in both learning and application." ></td>
	<td class="line x" title="77:261	(a) Template Induction To create a content template, we cluster all section headings hi1 him for all documents di." ></td>
	<td class="line x" title="78:261	Each cluster is labeled with the most common heading hij within the cluster." ></td>
	<td class="line x" title="79:261	The largest k clusters are selected to become topics t1 tk, which form the domain-specific content template." ></td>
	<td class="line x" title="80:261	(b) Search For each document that we wish to create, we retrieve from the Internet a set of r excerpts ej1 ejr for each topic tj from the template." ></td>
	<td class="line x" title="81:261	We define appropriate search queries using the requested document title and topics tj." ></td>
	<td class="line x" title="82:261	2." ></td>
	<td class="line x" title="83:261	Learning Content Selection (Section 3.2) For each topic tj, we learn the corresponding topic-specific parameters wj to determine the 2In data sets where such mark-up is not available, one can employ topical segmentation algorithms as an additional preprocessing step." ></td>
	<td class="line x" title="84:261	quality of a given excerpt." ></td>
	<td class="line x" title="85:261	Using the perceptron framework augmented with an ILP formulation for global optimization, the system is trained to select the best excerpt for each document di and each topic tj." ></td>
	<td class="line x" title="86:261	For training,weassumethebestexcerptistheoriginal human-authored text sij." ></td>
	<td class="line x" title="87:261	3." ></td>
	<td class="line x" title="88:261	Application (Section 3.2) Given the title of a requested document, we select several excerpts from the candidate vectors returned by the search procedure (1b) to create a comprehensive overview article." ></td>
	<td class="line x" title="89:261	We perform the decoding procedure jointly using learned parametersw1 wk and the same ILP formulation for global optimization as in training." ></td>
	<td class="line x" title="90:261	Theresultisanewdocumentwithk excerpts, one for each topic." ></td>
	<td class="line x" title="91:261	3.1 Preprocessing Template Induction A content template specifies the topical structure of documents in one domain." ></td>
	<td class="line x" title="92:261	For instance, the template for articles about actors consists of four topics t1 t4: biography, early life, career, and personal life." ></td>
	<td class="line x" title="93:261	Using this template to create the biography of a new actor will ensure that its information coverage is consistent with existing human-authored documents." ></td>
	<td class="line x" title="94:261	Weaimtoderivethesetemplatesbydiscovering commonpatternsintheorganizationofdocuments in a domain of interest." ></td>
	<td class="line x" title="95:261	There has been a sizable amount of research on structure induction ranging fromlinearsegmentation(Hearst, 1994)tocontent modeling (Barzilay and Lee, 2004)." ></td>
	<td class="line x" title="96:261	At the core of these methods is the assumption that fragments of text conveying similar information have similar word distribution patterns." ></td>
	<td class="line x" title="97:261	Therefore, often a simple segment clustering across domain texts can identifystrongpatternsincontentstructure(Barzilay and Elhadad, 2003)." ></td>
	<td class="line x" title="98:261	Clusters containing fragments from many documents are indicative of topics that are essential for a comprehensive summary." ></td>
	<td class="line x" title="99:261	Given the simplicity and robustness of this approach, we utilize it for template induction." ></td>
	<td class="line x" title="100:261	We cluster all section headings hi1 him from all documents di using a repeated bisectioning algorithm (Zhao et al., 2005)." ></td>
	<td class="line x" title="101:261	As a similarity function, we use cosine similarity weighted with TF*IDF." ></td>
	<td class="line x" title="102:261	We eliminate any clusters with low internal similarity (i.e., smaller than 0.5), as we assume these are miscellaneous clusters that will not yield unified topics." ></td>
	<td class="line x" title="103:261	210 We determine the average number of sections k over all documents in our training set, then select the k largest section clusters as topics." ></td>
	<td class="line x" title="104:261	We order these topics as t1 tk using a majority ordering algorithm (Cohen et al., 1998)." ></td>
	<td class="line x" title="105:261	This algorithm finds a total order among clusters that is consistent with a maximal number of pairwise relationships observed in our data set." ></td>
	<td class="line x" title="106:261	Each topic tj is identified by the most frequent heading found within the cluster  e.g., Causes." ></td>
	<td class="line x" title="107:261	This set of topics forms the content template for a domain." ></td>
	<td class="line x" title="108:261	Search To retrieve relevant excerpts, we must define appropriate search queries for each topic t1 tk." ></td>
	<td class="line x" title="109:261	Query reformulation is an active area of research (Agichtein et al., 2001)." ></td>
	<td class="line x" title="110:261	We have experimented with several of these methods for drawingsearchqueriesfromrepresentativewordsinthe body text of each topic; however, we find that the best performance is provided by deriving queries from a conjunction of the document title and topic  e.g., 3-M syndrome diagnosis." ></td>
	<td class="line x" title="111:261	Using these queries, we search using Yahoo!" ></td>
	<td class="line x" title="112:261	andretrievethefirsttenresultpagesforeachtopic." ></td>
	<td class="line x" title="113:261	From each of these pages, we extract all possible excerptsconsistingofchunksoftextbetweenstandardized boundary indicators (such as <p> tags)." ></td>
	<td class="line x" title="114:261	In our experiments, there are an average of 6 excerpts taken from each page." ></td>
	<td class="line x" title="115:261	For each topic tj of each document we wish to create, the total number of excerpts r found on the Internet may differ." ></td>
	<td class="line x" title="116:261	We label the excerpts ej1 ejr." ></td>
	<td class="line x" title="117:261	3.2 Selection Model Our selection model takes the content template t1 tk and the candidate excerpts ej1 ejr for each topic tj produced in the previous steps." ></td>
	<td class="line x" title="118:261	It then selects a series of k excerpts, one from each topic, to create a coherent summary." ></td>
	<td class="line x" title="119:261	One possible approach is to perform individual selections from each set of excerpts ej1 ejr and then combine the results." ></td>
	<td class="line x" title="120:261	This strategy is commonly used in multi-document summarization (Barzilay et al., 1999; Goldstein et al., 2000; Radev et al., 2000), where the combination step eliminates the redundancy across selected excerpts." ></td>
	<td class="line x" title="121:261	However, separating the two steps may not be optimal for this task  the balance between coverage and redundancy is harder to achieve when a multi-paragraph summary is generated." ></td>
	<td class="line x" title="122:261	In addition, a more discriminative selection strategy is needed when candidate excerpts are drawn directly from the web, as they may be contaminated with noise." ></td>
	<td class="line x" title="123:261	We propose a novel joint training algorithm that learns selection criteria for all the topics simultaneously." ></td>
	<td class="line x" title="124:261	This approach enables us to maximize both local fit and global coherence." ></td>
	<td class="line x" title="125:261	We implement this algorithm using the perceptron framework, as it can be easily modified for structured prediction while preserving convergence guarantees (Daume III and Marcu, 2005; Snyder and Barzilay, 2007)." ></td>
	<td class="line x" title="126:261	In this section, we first describe the structure and decoding procedure of our model." ></td>
	<td class="line x" title="127:261	We then present an algorithm to jointly learn the parameters of all topic models." ></td>
	<td class="line x" title="128:261	3.2.1 Model Structure The model inputs are as follows:  The title of the desired document  t1 tk  topics from the content template  ej1 ejr  candidate excerpts for each topic tj In addition, we define feature and parameter vectors:  (ejl)  feature vector for the lth candidate excerpt for topic tj  w1 wk  parameter vectors, one for each of the topics t1 tk Ourmodelconstructsanewarticlebyfollowing these two steps: Ranking First, we attempt to rank candidate excerpts based on how representative they are of each individual topic." ></td>
	<td class="line x" title="129:261	For each topic tj, we induce a ranking of the excerpts ej1 ejr by mapping each excerpt ejl to a score: scorej(ejl) = (ejl) wj Candidates for each topic are ranked from highest to lowest score." ></td>
	<td class="line x" title="130:261	After this procedure, the position l of excerpt ejl within the topic-specific candidate vector is the excerpts rank." ></td>
	<td class="line x" title="131:261	Optimizing the Global Objective To avoid redundancy between topics, we formulate an optimization problem using excerpt rankings to create the final article." ></td>
	<td class="line x" title="132:261	Given k topics, we would like to select one excerpt ejl for each topic tj, such that the rank is minimized; that is, scorej(ejl) is high." ></td>
	<td class="line x" title="133:261	To select the optimal excerpts, we employ integer linear programming (ILP)." ></td>
	<td class="line x" title="134:261	This framework is 211 commonly used in generation and summarization applications where the selection process is driven by multiple constraints (Marciniak and Strube, 2005; Clarke and Lapata, 2007)." ></td>
	<td class="line x" title="135:261	We represent excerpts included in the output using a set of indicator variables, xjl." ></td>
	<td class="line x" title="136:261	For each excerpt ejl, the corresponding indicator variable xjl = 1 if the excerpt is included in the final document, and xjl = 0 otherwise." ></td>
	<td class="line x" title="137:261	Our objective is to minimize the ranks of the excerpts selected for the final document: min ksummationdisplay j=1 rsummationdisplay l=1 l  xjl We augment this formulation with two types of constraints." ></td>
	<td class="line x" title="138:261	Exclusivity Constraints We want to ensure that exactly one indicator xjl is nonzero for each topic tj." ></td>
	<td class="line x" title="139:261	These constraints are formulated as follows: rsummationdisplay l=1 xjl = 1 j  {1k} Redundancy Constraints We also want to prevent redundancy across topics." ></td>
	<td class="line x" title="140:261	We define sim(ejl,ejprimelprime) as the cosine similarity between excerpts ejl from topic tj and ejprimelprime from topic tjprime." ></td>
	<td class="line x" title="141:261	We introduce constraints that ensure no pair of excerpts has similarity above 0.5: (xjl + xjprimelprime)  sim(ejl,ejprimelprime)  1 j,jprime = 1k l,lprime = 1r If excerpts ejl and ejprimelprime have cosine similarity sim(ejl,ejprimelprime) > 0.5, only one excerpt may be selected for the final document  i.e., either xjl or xjprimelprime may be 1, but not both." ></td>
	<td class="line x" title="142:261	Conversely, if sim(ejl,ejprimelprime)  0.5, both excerpts may be selected." ></td>
	<td class="line x" title="143:261	Solving the ILP Solving an integer linear program is NP-hard (Cormen et al., 1992); however, in practice there exist several strategies for solving certainILPsefficiently." ></td>
	<td class="line x" title="144:261	Inourstudy, weemployed lp solve,3 an efficient mixed integer programming solver which implements the Branch-and-Bound algorithm." ></td>
	<td class="line x" title="145:261	On a larger scale, there are several alternativesto approximatetheILPresults, such asa dynamicprogrammingapproximationtotheknapsack problem (McDonald, 2007)." ></td>
	<td class="line x" title="146:261	3http://lpsolve.sourceforge.net/5.5/ Feature Value UNI wordi count of word occurrences POS wordi first position of word in excerpt BI wordi wordi+1 count of bigram occurrences SENT count of all sentences EXCL count of exclamations QUES count of questions WORD count of all words NAME count of title mentions DATE count of dates PROP count of proper nouns PRON count of pronouns NUM count of numbers FIRST word1 1 FIRST word1 word2 1 SIMS count of similar excerpts Table 1: Features employed in the ranking model." ></td>
	<td class="line x" title="147:261	 Defined as the first unigram in the excerpt." ></td>
	<td class="line x" title="148:261	 Defined as the first bigram in the excerpt." ></td>
	<td class="line x" title="149:261	 Defined as excerpts with cosine similarity > 0.5 Features As shown in Table 1, most of the features we select in our model have been employed in previous work on summarization (Mani and Maybury, 1999)." ></td>
	<td class="line x" title="150:261	All features except the SIMS feature are defined for individual excerpts in isolation." ></td>
	<td class="line x" title="151:261	For each excerpt ejl, the value of the SIMS feature is the count of excerpts ejlprime in the same topic tj for which sim(ejl,ejlprime) > 0.5." ></td>
	<td class="line x" title="152:261	This feature quantifies the degree of repetition within a topic, oftenindicativeofanexcerptsaccuracyand relevance." ></td>
	<td class="line x" title="153:261	3.2.2 Model Training Generating Training Data For training, we are given n original documents d1 dn, a content template consisting of topics t1 tk, and a set of candidate excerpts eij1 eijr for each document di and topic tj." ></td>
	<td class="line x" title="154:261	For each section of each document, we add the gold excerpt sij to the corresponding vector of candidate excerpts eij1 eijr." ></td>
	<td class="line x" title="155:261	This excerpt represents the target for our training algorithm." ></td>
	<td class="line x" title="156:261	Note that the algorithm does not require annotated ranking data; only knowledge of this optimal excerpt is required." ></td>
	<td class="line x" title="157:261	However, if the excerpts provided in the training data have low quality, noise is introduced into the system." ></td>
	<td class="line x" title="158:261	Training Procedure Our algorithm is a modification of the perceptron ranking algorithm (Collins, 2002), which allows for joint learning across several ranking problems (Daume III and Marcu, 2005; Snyder and Barzilay, 2007)." ></td>
	<td class="line x" title="159:261	Pseudocode for this algorithm is provided in Figure 2." ></td>
	<td class="line x" title="160:261	First, we define Rank(eij1 eijr,wj), which 212 ranks all excerpts from the candidate excerpt vector eij1 eijr for document di and topic tj." ></td>
	<td class="line x" title="161:261	Excerpts are ordered by scorej(ejl) using the current parameter values." ></td>
	<td class="line x" title="162:261	We also define Optimize(eij1 eijr), which finds the optimal selection of excerpts (one per topic) given ranked lists of excerpts eij1 eijr for each document di and topic tj." ></td>
	<td class="line x" title="163:261	These functions follow the ranking and optimization procedures described in Section 3.2.1." ></td>
	<td class="line x" title="164:261	The algorithm maintains k parameter vectors w1 wk, one associated with each topic tj desired in the final article." ></td>
	<td class="line x" title="165:261	During initialization, all parameter vectors are set to zeros (line 2)." ></td>
	<td class="line x" title="166:261	To learn the optimal parameters, this algorithm iterates over the training set until the parameters converge or a maximum number of iterations is reached (line 3)." ></td>
	<td class="line x" title="167:261	For each document in the training set (line 4), the following steps occur: First, candidate excerpts for each topic are ranked (lines 5-6)." ></td>
	<td class="line x" title="168:261	Next, decoding through ILP optimization is performed over all ranked lists of candidate excerpts, selecting one excerpt for each topic (line 7)." ></td>
	<td class="line x" title="169:261	Finally, the parameters are updated in a joint fashion." ></td>
	<td class="line x" title="170:261	For each topic (line 8), if the selected excerpt is not similar enough to the gold excerpt (line 9), the parameters for that topic are updated using a standard perceptron update rule (line 10)." ></td>
	<td class="line x" title="171:261	When convergence is reached or the maximum iteration count is exceeded, the learned parameter values are returned (line 12)." ></td>
	<td class="line x" title="172:261	The use of ILP during each step of training sets this algorithm apart from previous work." ></td>
	<td class="line x" title="173:261	In prior research, ILP was used as a postprocessing step to remove redundancy and make other global decisions about parameters (McDonald, 2007; Marciniak and Strube, 2005; Clarke and Lapata, 2007)." ></td>
	<td class="line x" title="174:261	However, in our training, we intertwine the complete decoding procedure with the parameter updates." ></td>
	<td class="line x" title="175:261	Our joint learning approach finds per-topic parameter values that are maximally suited for the global decoding procedure for content selection." ></td>
	<td class="line x" title="176:261	4 Experimental Setup We evaluate our method by observing the quality of automatically created articles in different domains." ></td>
	<td class="line o" title="177:261	We compute the similarity of a large number of articles produced by our system and several baselines to the original human-authored articles using ROUGE, a standard metric for summary quality." ></td>
	<td class="line x" title="178:261	Inaddition, weperformananalysisofediInput: d1 dn: A set of n documents, each containing k sections si1 sik eij1 eijr: Sets of candidate excerpts for each topic tj and document di Define: Rank(eij1 eijr,wj): As described in Section 3.2.1: Calculates scorej(eijl) for all excerpts for document di and topic tj, using parameterswj." ></td>
	<td class="line x" title="179:261	Orders the list of excerpts by scorej(eijl) from highest to lowest." ></td>
	<td class="line x" title="180:261	Optimize(ei11 eikr): As described in Section 3.2.1: Finds the optimal selection of excerpts to form a final article, given ranked lists of excerpts for each topic t1 tk." ></td>
	<td class="line x" title="181:261	Returns a list of k excerpts, one for each topic." ></td>
	<td class="line x" title="182:261	(eijl): Returns the feature vector representing excerpt eijl Initialization: 1 For j = 1k 2 Set parameterswj = 0 Training: 3 Repeat until convergence or while iter < itermax: 4 For i = 1n 5 For j = 1k 6 Rank(eij1 eijr,wj) 7 x1 xk = Optimize(ei11 eikr) 8 For j = 1k 9 If sim(xj,sij) < 0.8 10 wj = wj + (sij)(xi) 11 iter = iter + 1 12 Return parametersw1 wk Figure 2: An algorithm for learning several ranking problems with a joint decoding mechanism." ></td>
	<td class="line x" title="183:261	tor reaction to system-produced articles submitted to Wikipedia." ></td>
	<td class="line x" title="184:261	Data For evaluation, we consider two domains: American Film Actors and Diseases." ></td>
	<td class="line o" title="185:261	These domains have been commonly used in prior work on summarization (Weischedel et al., 2004; Zhou et al., 2004; Filatova and Prager, 2005; DemnerFushman and Lin, 2007; Biadsy et al., 2008)." ></td>
	<td class="line x" title="186:261	Our text corpus consists of articles drawn from the corresponding categories in Wikipedia." ></td>
	<td class="line x" title="187:261	There are 2,150 articles in American Film Actors and 523 articles in Diseases." ></td>
	<td class="line x" title="188:261	For each domain, we randomly select 90% of articles for training and test on the remaining 10%." ></td>
	<td class="line x" title="189:261	Human-authored articles in both domains contain an average of four topics, and each topic contains an average of 193 words." ></td>
	<td class="line x" title="190:261	In order to model the real-world scenario where Wikipedia articles are not always available (as for new or specialized topics), we specifically exclude Wikipedia sources during our search pro213 Avg." ></td>
	<td class="line x" title="191:261	Excerpts Avg." ></td>
	<td class="line x" title="192:261	Sources Amer." ></td>
	<td class="line x" title="193:261	Film Actors Search 2.3 1 No Template 4 4.0 Disjoint 4 2.1 Full Model 4 3.4 Oracle 4.3 4.3 Diseases Search 3.1 1 No Template 4 2.5 Disjoint 4 3.0 Full Model 4 3.2 Oracle 5.8 3.9 Table 2: Average number of excerpts selected and sources used in article creation for test articles." ></td>
	<td class="line x" title="194:261	cedure (Section 3.1) for evaluation." ></td>
	<td class="line x" title="195:261	Baselines Our first baseline, Search, relies solely on search engine ranking for content selection." ></td>
	<td class="line x" title="196:261	Using the article title as a query  e.g., Bacillary Angiomatosis, this method selects the web pagethatisrankedfirstbythesearchengine." ></td>
	<td class="line x" title="197:261	From this page we select the first k paragraphs where k is defined in the same way as in our full model." ></td>
	<td class="line x" title="198:261	If there are less than k paragraphs on the page, all paragraphs are selected, but no other sources are used." ></td>
	<td class="line x" title="199:261	This yields a document of comparable size with the output of our system." ></td>
	<td class="line x" title="200:261	Despite its simplicity, this baseline is not naive: extracting material from a single document guarantees that the output is coherent, and a page highly ranked by a search engine may readily contain a comprehensive overview of the subject." ></td>
	<td class="line x" title="201:261	Our second baseline, No Template, does not use a template to specify desired topics; therefore, there are no constraints on content selection." ></td>
	<td class="line x" title="202:261	Instead, we follow a simplified form of previous work on biography creation, where a classifier is trained to distinguish biographical text (Zhou et al., 2004; Biadsy et al., 2008)." ></td>
	<td class="line x" title="203:261	In this case, we train a classifier to distinguish domain-specific text." ></td>
	<td class="line x" title="204:261	Positive training data is drawn from all topics in the given domain corpus." ></td>
	<td class="line x" title="205:261	To find negative training data, we perform the search procedure as in our full model (see Section 3.1) using only the article titles as search queries." ></td>
	<td class="line x" title="206:261	Any excerpts which have very low similarity to the original articles are used as negative examples." ></td>
	<td class="line x" title="207:261	During the decoding procedure, we use the same search procedure." ></td>
	<td class="line x" title="208:261	We then classify each excerpt as relevant or irrelevant and select the k non-redundant excerpts with the highest relevance confidence scores." ></td>
	<td class="line x" title="209:261	Our third baseline, Disjoint, uses the ranking perceptron framework as in our full system; however, rather than perform an optimization step during training and decoding, we simply select the highest-ranked excerpt for each topic." ></td>
	<td class="line x" title="210:261	This equates to standard linear classification for each section individually." ></td>
	<td class="line x" title="211:261	In addition to these baselines, we compare against an Oracle system." ></td>
	<td class="line x" title="212:261	For each topic present in the human-authored article, the Oracle selects the excerpt from our full models candidate excerpts with the highest cosine similarity to the human-authored text." ></td>
	<td class="line x" title="213:261	This excerpt is the optimal automatic selection from the results available, and thereforerepresentsanupperboundonourexcerpt selection task." ></td>
	<td class="line x" title="214:261	Some articles contain additional topicsbeyondthoseinthetemplate; inthesecases, the Oracle system produces a longer article than our algorithm." ></td>
	<td class="line x" title="215:261	Table 2 shows the average number of excerpts selectedandsourcesusedinarticlescreatedbyour full model and each baseline." ></td>
	<td class="line x" title="216:261	Automatic Evaluation To assess the quality of the resulting overview articles, we compare them with the original human-authored articles." ></td>
	<td class="line o" title="217:261	We use ROUGE, an evaluation metric employed at the Document Understanding Conferences (DUC), which assumes that proximity to human-authored text is an indicator of summary quality." ></td>
	<td class="line oc" title="218:261	We use the publicly available ROUGE toolkit (Lin, 2004)tocomputerecall, precision, andF-scorefor ROUGE-1." ></td>
	<td class="line x" title="219:261	WeusetheWilcoxonSignedRankTest to determine statistical significance." ></td>
	<td class="line x" title="220:261	Analysis of Human Edits In addition to our automatic evaluation, we perform a study of reactions to system-produced articles by the general public." ></td>
	<td class="line x" title="221:261	To achieve this goal, we insert automatically created articles4 into Wikipedia itself and examine the feedback of Wikipedia editors." ></td>
	<td class="line x" title="222:261	Selection of specific articles is constrained by the need to findtopicswhicharecurrentlyofstubstatusthat have enough information available on the Internet to construct a valid article." ></td>
	<td class="line x" title="223:261	After a period of time, we analyzed the edits made to the articles to determine the overall editor reaction." ></td>
	<td class="line x" title="224:261	We report results on 15 articles in the Diseases category5." ></td>
	<td class="line x" title="225:261	4In addition to the summary itself, we also include proper citations to the sources from which the material is extracted." ></td>
	<td class="line x" title="226:261	5We are continually submitting new articles; however, we report results on those that have at least a 6 month history at time of writing." ></td>
	<td class="line x" title="227:261	214 Recall Precision F-score Amer." ></td>
	<td class="line x" title="228:261	Film Actors Search 0.09 0.37 0.13  No Template 0.33 0.50 0.39  Disjoint 0.45 0.32 0.36  Full Model 0.46 0.40 0.41 Oracle 0.48 0.64 0.54  Diseases Search 0.31 0.37 0.32  No Template 0.32 0.27 0.28  Disjoint 0.33 0.40 0.35  Full Model 0.36 0.39 0.37 Oracle 0.59 0.37 0.44  Table 3: Results of ROUGE-1 evaluation." ></td>
	<td class="line x" title="229:261	 Significant with respect to our full model for p  0.05." ></td>
	<td class="line x" title="230:261	 Significant with respect to our full model for p  0.10." ></td>
	<td class="line x" title="231:261	Since Wikipedia is a live resource, we do not repeat this procedure for our baseline systems." ></td>
	<td class="line x" title="232:261	Adding articles from systems which have previously demonstrated poor quality would be improper, especially in Diseases." ></td>
	<td class="line x" title="233:261	Therefore, we present this analysis as an additional observation rather than a rigorous technical study." ></td>
	<td class="line x" title="234:261	5 Results AutomaticEvaluation Theresultsofthisevaluation are shown in Table 3." ></td>
	<td class="line x" title="235:261	Our full model outperforms all of the baselines." ></td>
	<td class="line x" title="236:261	By surpassing the Disjoint baseline, we demonstrate the benefits of joint classification." ></td>
	<td class="line x" title="237:261	Furthermore, the high performance of both our full model and the Disjoint baseline relative to the other baselines shows the importance of structure-aware content selection." ></td>
	<td class="line x" title="238:261	The Oracle system, which represents an upper bound on our systems capabilities, performs well." ></td>
	<td class="line x" title="239:261	The remaining baselines have different flaws: Articles produced by the No Template baseline tend to focus on a single topic extensively at the expense of breadth, because there are no constraints to ensure diverse topic selection." ></td>
	<td class="line x" title="240:261	On the other hand, performance of the Search baseline varies dramatically." ></td>
	<td class="line x" title="241:261	This is expected; this baseline relies heavily on both the search engine and individualwebpages." ></td>
	<td class="line x" title="242:261	Thesearchenginemustcorrectly rank relevant pages, and the web pages must provide the important material first." ></td>
	<td class="line x" title="243:261	AnalysisofHumanEdits Theresultsofourobservation of editing patterns are shown in Table 4." ></td>
	<td class="line x" title="244:261	These articles have resided on Wikipedia for a period of time ranging from 5-11 months." ></td>
	<td class="line x" title="245:261	All of them have been edited, and no articles were removed due to lack of quality." ></td>
	<td class="line x" title="246:261	Moreover, ten automatically created articles have been promoted Type Count Total articles 15 Promoted articles 10 Edit types Intra-wiki links 36 Formatting 25 Grammar 20 Minor topic edits 2 Major topic changes 1 Total edits 85 Table 4: Distribution of edits on Wikipedia." ></td>
	<td class="line x" title="247:261	by human editors from stubs to regular Wikipedia entries based on the quality and coverage of the material." ></td>
	<td class="line x" title="248:261	Information was removed in three cases for being irrelevant, one entire section and two smaller pieces." ></td>
	<td class="line x" title="249:261	The most common changes were small edits to formatting and introduction of links to other Wikipedia articles in the body text." ></td>
	<td class="line x" title="250:261	6 Conclusion In this paper, we investigated an approach for creating a multi-paragraph overview article by selecting relevant material from the web and organizing it into a single coherent text." ></td>
	<td class="line x" title="251:261	Our algorithm yields significant gains over a structure-agnostic approach." ></td>
	<td class="line x" title="252:261	Moreover, our results demonstrate the benefits of structured classification, which outperforms independently trained topical classifiers." ></td>
	<td class="line x" title="253:261	Overall, the results of our evaluation combined with our analysis of human edits confirm that the proposed method can effectively produce comprehensive overview articles." ></td>
	<td class="line x" title="254:261	Thisworkopensseveraldirectionsforfutureresearch." ></td>
	<td class="line x" title="255:261	Diseases and American Film Actors exhibit fairly consistent article structures, which are successfully captured by a simple template creation process." ></td>
	<td class="line x" title="256:261	However, with categories that exhibit structural variability, more sophisticated statistical approaches may be required to produce accurate templates." ></td>
	<td class="line x" title="257:261	Moreover, a promising direction is to consider hierarchical discourse formalisms such as RST (Mann and Thompson, 1988) to supplement our template-based approach." ></td>
	<td class="line x" title="258:261	Acknowledgments The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0835445, and grant IIS0835652) and NIH (grant V54LM008748)." ></td>
	<td class="line x" title="259:261	Thanks to Mike Collins, Julia Hirschberg, and members of the MIT NLP group for their helpful suggestions and comments." ></td>
	<td class="line x" title="260:261	Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations." ></td>
	<td class="line x" title="261:261	215" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1062
Summarizing multiple spoken documents: finding evidence from untranscribed audio
Zhu, Xiaodan;Penn, Gerald;Rudzicz, Frank;"></td>
	<td class="line x" title="1:212	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549557, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:212	c2009 ACL and AFNLP Summarizing multiple spoken documents: finding evidence from untranscribed audio Xiaodan Zhu, Gerald Penn and Frank Rudzicz University of Toronto 10 Kings College Rd., Toronto, M5S 3G4, ON, Canada {xzhu,gpenn,frank}@cs.toronto.edu Abstract This paper presents a model for summarizing multiple untranscribed spoken documents." ></td>
	<td class="line x" title="3:212	Without assuming the availability of transcripts, the model modifies a recently proposed unsupervised algorithm to detect re-occurring acoustic patterns in speech and uses them to estimate similarities between utterances, which are in turn used to identify salient utterances and remove redundancies." ></td>
	<td class="line x" title="4:212	This model is of interest due to its independence from spoken language transcription, an error-prone and resource-intensive process, its ability to integrate multiple sources of information on the same topic, and its novel use of acoustic patterns that extends previous work on low-level prosodic feature detection." ></td>
	<td class="line x" title="5:212	We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 3337% range without actually having to do the ASR, plus it better handles utterances with out-ofvocabulary words." ></td>
	<td class="line x" title="6:212	1 Introduction Summarizing spoken documents has been extensively studied over the past several years (Penn and Zhu, 2008; Maskey and Hirschberg, 2005; Murray et al., 2005; Christensen et al., 2004; Zechner, 2001)." ></td>
	<td class="line x" title="7:212	Conventionally called speech summarization, although speech connotes more than spoken documents themselves, it is motivated by the demand for better ways to navigate spoken content and the natural difficulty in doing so  speech is inherently more linear or sequential than text in its traditional delivery." ></td>
	<td class="line x" title="8:212	Previous research on speech summarization has addressed several important problems in this field (see Section 2.1)." ></td>
	<td class="line x" title="9:212	All of this work, however, has focused on single-document summarization and the integration of fairly simplistic acoustic features, inspired by work in descriptive linguistics." ></td>
	<td class="line x" title="10:212	The issues of navigating speech content are magnified when dealing with larger collections  multiple spoken documents on the same topic." ></td>
	<td class="line x" title="11:212	For example, when one is browsing news broadcasts covering the same events or call-centre recordings related to the same type of customer questions, content redundancy is a prominent issue." ></td>
	<td class="line x" title="12:212	Multi-document summarization on written documents has been studied for more than a decade (see Section 2.2)." ></td>
	<td class="line x" title="13:212	Unfortunately, no such effort has been made on audio documents yet." ></td>
	<td class="line x" title="14:212	An obvious way to summarize multiple spoken documents is to adopt the transcribe-andsummarize approach, in which automatic speech recognition (ASR) is first employed to acquire written transcripts." ></td>
	<td class="line x" title="15:212	Speech summarization is accordingly reduced to a text summarization task conducted on error-prone transcripts." ></td>
	<td class="line x" title="16:212	Such an approach, however, encounters several problems." ></td>
	<td class="line x" title="17:212	First, assuming the availability of ASR is not always valid for many languages other than English that one may want to summarize." ></td>
	<td class="line x" title="18:212	Even when it is, transcription quality is often an issue training ASR models requires collecting and annotating corpora on specific languages, dialects, or even different domains." ></td>
	<td class="line x" title="19:212	Although recognition errors do not significantly impair extractive summarizers (Christensen et al., 2004; Zhu and Penn, 2006), error-laden transcripts are not necessarily browseable if recognition errors are higher than certain thresholds (Munteanu et al., 2006)." ></td>
	<td class="line x" title="20:212	In such situations, audio summaries are an alternative when salient content can be identified directly from untranscribed audio." ></td>
	<td class="line x" title="21:212	Third, the underlying paradigm of most ASR models aims to solve a 549 classification problem, in which speech is segmented and classified into pre-existing categories (words)." ></td>
	<td class="line x" title="22:212	Words not in the predefined dictionary are certain to be misrecognized without exception." ></td>
	<td class="line x" title="23:212	This out-of-vocabulary (OOV) problem is unavoidable in the regular ASR framework, although it is more likely to happen on salient words such as named entities or domain-specific terms." ></td>
	<td class="line x" title="24:212	Our approach uses acoustic evidence from the untranscribed audio stream." ></td>
	<td class="line x" title="25:212	Consider text summarization first: many well-known models such as MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004) rely on the reoccurrence statistics of words." ></td>
	<td class="line x" title="26:212	That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved." ></td>
	<td class="line x" title="27:212	These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008)." ></td>
	<td class="line x" title="28:212	For spoken documents, such reoccurrence statistics are available directly from the speech signal." ></td>
	<td class="line x" title="29:212	In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008)." ></td>
	<td class="line x" title="30:212	This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007)." ></td>
	<td class="line x" title="31:212	Motivated by the work above, this paper explores the approach to summarizing multiple spoken documents directly over an untranscribed audio stream." ></td>
	<td class="line x" title="32:212	Such a model is of interest because of its independence from ASR." ></td>
	<td class="line x" title="33:212	It is directly applicable to audio recordings in languages or domains when ASR is not possible or transcription quality is low." ></td>
	<td class="line x" title="34:212	In principle, this approach is free from the OOV problem inherent to ASR." ></td>
	<td class="line x" title="35:212	The premise of this approach, however, is to reliably find reoccuring acoustic patterns in audio, which is challenging because of noise and pronunciation variance existing in the speech signal, as well as the difficulty of finding alignments with proper lengths corresponding to words well." ></td>
	<td class="line x" title="36:212	Therefore, our primary goal in this paper is to empirically determine the extent to which acoustic information alone can effectively replace conventional speech recognition with or without simple prosodic feature detection within the multi-document speech summarization task." ></td>
	<td class="line x" title="37:212	As shown below, a modification of the Park-Glass approach amounts to the efficacy of a 33-37% WER ASR engine in the domain of multiple spoken document summarization, and also has better treatment of OOV items." ></td>
	<td class="line x" title="38:212	ParkGlass similarity scores by themselves can attribute a high score to distorted paths that, in our context, ultimately leads to too many false-alarm alignments, even after applying the distortion threshold." ></td>
	<td class="line x" title="39:212	We introduce additional distortion penalty and subpath length constraints on their scoring to discourage this possibility." ></td>
	<td class="line x" title="40:212	2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document." ></td>
	<td class="line x" title="41:212	The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000)." ></td>
	<td class="line x" title="42:212	Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives." ></td>
	<td class="line x" title="43:212	Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates." ></td>
	<td class="line x" title="44:212	He et al.(2000) have shown that either playing back important audio-video segments or just highlighting the corresponding transcripts is significantly better than providing users with full transcripts, electronic slides, or both for browsing presentation recordings." ></td>
	<td class="line x" title="46:212	Given the limitations associated with ASR, it is no surprise that previous work (He et al., 1999; Maskey and Hirschberg, 2005; Murray et al., 2005; Zhu and Penn, 2006) has studied features available in audio." ></td>
	<td class="line x" title="47:212	The focus, however, is primarily limited to prosody." ></td>
	<td class="line x" title="48:212	The assumption is that prosodic effects such as stress can indicate salient information." ></td>
	<td class="line x" title="49:212	Since a direct modeling of complicated compound prosodic effects like stress is dif550 ficult, they have used basic features of prosody instead, such as pitch, energy, duration, and pauses." ></td>
	<td class="line x" title="50:212	The usefulness of prosody was found to be very limited by itself, if the effect of utterance length is not considered (Penn and Zhu, 2008)." ></td>
	<td class="line x" title="51:212	In multiplespoken-document summarization, it is unlikely that prosody will be more useful in predicating salience than in single document summarization." ></td>
	<td class="line x" title="52:212	Furthermore, prosody is also unlikely to be applicable to detecting or handling redundancy, which is prominent in the multiple-document setting." ></td>
	<td class="line x" title="53:212	All of the work above has been conducted on single-document summarization." ></td>
	<td class="line x" title="54:212	In this paper we are interested in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns." ></td>
	<td class="line x" title="55:212	2.2 Multiple-document summarization Multi-document summarization on written text has been studied for over a decade." ></td>
	<td class="line x" title="56:212	Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly." ></td>
	<td class="line x" title="57:212	This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998)." ></td>
	<td class="line x" title="58:212	Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004)." ></td>
	<td class="line x" title="59:212	Multi-document summarization has received intensive study at DUC." ></td>
	<td class="line x" title="60:212	1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet." ></td>
	<td class="line x" title="61:212	Abstractive approaches have been studied since the beginning." ></td>
	<td class="line x" title="62:212	A famous effort in this direction is the information fusion approach proposed in Barzilay et al.(1999)." ></td>
	<td class="line x" title="64:212	However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for the time being." ></td>
	<td class="line x" title="65:212	As in single-spoken-document summarization, this paper focuses on the extractive approach." ></td>
	<td class="line x" title="66:212	Among the extractive models, MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004), are possibly the most widely known." ></td>
	<td class="line x" title="67:212	Both of them are linear models that balance salience and redundancy." ></td>
	<td class="line x" title="68:212	Although in principle, these models allow for any estimates of salience and redundancy, they themselves calculate these scores with word reoccurrence statistics, e.g., tf.idf, and yield state-of-the-art performance." ></td>
	<td class="line x" title="69:212	MMR it1http://duc.nist.gov/ eratively selects sentences that are similar to the entire documents, but dissimilar to the previously selected sentences to avoid redundancy." ></td>
	<td class="line x" title="70:212	Its details will be revisited below." ></td>
	<td class="line x" title="71:212	MEAD uses a redundancy removal mechanism similar to MMR, but to decide the salience of a sentence to the whole topic, MEAD uses not only its similarity score but also sentence position, e.g., the first sentence of each new story is considered important." ></td>
	<td class="line x" title="72:212	Our work adopts the general framework of MMR and MEAD to study the effectiveness of the acoustic pattern evidence found in untranscribed audio." ></td>
	<td class="line x" title="73:212	3 An acoustics-based approach The acoustics-based summarization technique proposed in this paper consists of three consecutive components." ></td>
	<td class="line x" title="74:212	First, we detect acoustic patterns that recur between pairs of utterances in a set of documents that discuss a common topic." ></td>
	<td class="line x" title="75:212	The assumption here is that lemmata, words, or phrases that are shared between utterances are more likely to be acoustically similar." ></td>
	<td class="line x" title="76:212	The next step is to compute a relatedness score between each pair of utterances, given the matching patterns found in the first step." ></td>
	<td class="line x" title="77:212	This yields a symmetric relatedness matrix for the entire document set." ></td>
	<td class="line x" title="78:212	Finally, the relatedness matrix is incorporated into a general summarization model, where it is used for utterance selection." ></td>
	<td class="line x" title="79:212	3.1 Finding common acoustic patterns Our goal is to identify subsequences within acoustic sequences that appear highly similar to regions within other sequences, where each sequence consists of a progression of overlapping 20ms vectors (frames)." ></td>
	<td class="line x" title="80:212	In order to find those shared patterns, we apply a modification of the segmental dynamic time warping (SDTW) algorithm to pairs of audio sequences." ></td>
	<td class="line x" title="81:212	This method is similar to standard DTW, except that it computes multiple constrained alignments, each within predetermined bands of the similarity matrix (Park and Glass, 2008).2 SDTW has been successfully applied to problems such as topic boundary detection (Malioutov et al., 2007) and word detection (Park and Glass, 2006)." ></td>
	<td class="line x" title="82:212	An example application of SDTW is shown in Figure 1, which shows the results of two utterances from the TDT-4 English dataset: 2Park and Glass (2008) used Euclidean distance." ></td>
	<td class="line x" title="83:212	We used cosine distance instead, which was found to be better on our held-out dataset." ></td>
	<td class="line x" title="84:212	551 I: the explosion in aden harbor killed seventeen u.s. sailors and injured other thirty nine last month." ></td>
	<td class="line x" title="85:212	II: seventeen sailors were killed." ></td>
	<td class="line x" title="86:212	These two utterances share three words: killed, seventeen, and sailors, though in different orders." ></td>
	<td class="line x" title="87:212	The upper panel of Figure 1 shows a matrix of frame-level similarity scores between these two utterances where lighter grey represents higher similarity." ></td>
	<td class="line x" title="88:212	The lower panel shows the four most similar shared subpaths, three of which correspond to the common words, as determined by the approach detailed below." ></td>
	<td class="line x" title="89:212	Figure 1: Using segmental dynamic time warping to find matching acoustic patterns between two utterances." ></td>
	<td class="line x" title="90:212	Calculating MFCC The first step of SDTW is to represent each utterance as sequences of Mel-frequency cepstral coefficient (MFCC) vectors, a commonly used representation of the spectral characteristics of speech acoustics." ></td>
	<td class="line x" title="91:212	First, conventional short-time Fourier transforms are applied to overlapping 20ms Hamming windows of the speech amplitude signal." ></td>
	<td class="line x" title="92:212	The resulting spectral energy is then weighted by filters on the Mel-scale and converted to 39dimensional feature vectors, each consisting of 12 MFCCs, one normalized log-energy term, as well as the first and second derivatives of these 13 components over time." ></td>
	<td class="line x" title="93:212	The MFCC features used in the acoustics-based approach are the same as those used below in the ASR systems." ></td>
	<td class="line x" title="94:212	As in (Park and Glass, 2008), an additional whitening step is taken to normalize the variances on each of these 39 dimensions." ></td>
	<td class="line x" title="95:212	The similarities between frames are then estimated using cosine distance." ></td>
	<td class="line x" title="96:212	All similarity scores are then normalized to the range of [0,1], which yields similarity matrices exemplified in the upper panel of Figure 1." ></td>
	<td class="line x" title="97:212	Finding optimal paths For each similarity matrix obtained above, local alignments of matching patterns need to be found, as shown in the lower panel of Figure 1." ></td>
	<td class="line x" title="98:212	A single global DTW alignment is not adequate, since words or phrases held in common between utterances may occur in any order." ></td>
	<td class="line x" title="99:212	For example, in Figure 1 killed occurs before all other shared words in one document and after all of these in the other, so a single alignment path that monotonically seeks the lower right-hand corner of the similarity matrix could not possibly match all common words." ></td>
	<td class="line x" title="100:212	Instead, multiple DTWs are applied, each starting from different points on the left or top edges of the similarity matrix, and ending at different points on the bottom or right edges, respectively." ></td>
	<td class="line x" title="101:212	The width of this diagonal band is proportional to the estimated number of words per sequence." ></td>
	<td class="line x" title="102:212	Given an M-by-N matrix of frame-level similarity scores, the top-left corner is considered the origin, and the bottom-right corner represents an alignment of the last frames in each sequence." ></td>
	<td class="line x" title="103:212	For each of the multiple starting points p0 = (x0,y0) where either x0 = 0 or y0 = 0, but not necessarily both, we apply DTW to find paths P = p0,p1,,pK that maximize summationtext0 i K sim(pi), where sim(pi) is the cosine similarity score of point pi = (xi,yi) in the matrix." ></td>
	<td class="line x" title="104:212	Each point on the path, pi, is subject to the constraint |xi yi| < T, where T limits the distortion of the path, as we determine experimentally." ></td>
	<td class="line x" title="105:212	The ending points are pK = (xK,yK) with either xK = N or yK = M. For considerations of efficiency, the multiple DTW processes do not start from every point on the left or top edges." ></td>
	<td class="line x" title="106:212	Instead, they skip every T such starting points, which still guarantees that there will be no blind-spot in the matrices that are inaccessible to all DTW search paths." ></td>
	<td class="line x" title="107:212	Finding optimal subpaths After the multiple DTW paths are calculated, the optimal subpath on each is then detected in order to find the local alignments where the similarity is maximal, which is where we expect actual matched phrases to occur." ></td>
	<td class="line x" title="108:212	For a given path P = p0,p2,,pK, the optimal subpath is defined to be a continuous subpath, P = pm,pm+1,pn 552 that maximizes summationtext min sim(pi) nm+1 , 0  n  m  k,and m  n + 1  L. That is, the subpath is at least as long as L and has the maximal average similarity." ></td>
	<td class="line x" title="109:212	L is used to avoid short alignments that correspond to subword segments or short function words." ></td>
	<td class="line x" title="110:212	The value of L is determined on a development set." ></td>
	<td class="line x" title="111:212	The version of SDTW employed by (Malioutov et al., 2007) and Park and Glass (2008) employed an algorithm of complexity O(Klog(L)) from (Lin et al., 2002) to find subpaths." ></td>
	<td class="line x" title="112:212	Lin et al.(2002) have also proven that the length of the optimal subpath is between L and 2L1, inclusively." ></td>
	<td class="line x" title="114:212	Therefore, our version uses a very simple algorithm just search and find the maximum of average similarities among all possible subpaths with lengths between L and 2L  1." ></td>
	<td class="line x" title="115:212	Although the theoretical upper bound for this algorithm is O(KL), in practice we have found no significant increase in computation time compared with the O(Klog(L)) algorithmL is actually a constant for both Park and Glass (2008) and us, it is much smaller than K, and the O(Klog(L)) algorithm has (constant) overhead of calculating right-skew partitions." ></td>
	<td class="line x" title="116:212	In our implementation, since most of the time is spent on calculating the average similarity scores on candidate subpaths, all average scores are therefore pre-calculated incrementally and saved." ></td>
	<td class="line x" title="117:212	We have also parallelized the computation of similarities by topics over several computer clusters." ></td>
	<td class="line x" title="118:212	A detailed comparison of different parallelization techniques has been conducted by Gajjar et al.(2008)." ></td>
	<td class="line x" title="120:212	In addition, comparing time efficiency between the acoustics-based approach and ASRbased summarizers is interesting but not straightforward since a great deal of comparable programming optimization needs to be additionally considered in the present approach." ></td>
	<td class="line x" title="121:212	3.2 Estimating utterance-level similarity In the previous stage, we calculated frame-level similarities between utterance pairs and used these to find potential matching patterns between the utterances." ></td>
	<td class="line x" title="122:212	With this information, we estimate utterance-level similarities by estimating the numbers of true subpath alignments between two utterances, which are in turn determined by combining the following features associated with subpaths: Similarity of subpath We compute similarity features on each subpath." ></td>
	<td class="line x" title="123:212	We have obtained the average similarity score of each subpath as discussed in Section 3.1." ></td>
	<td class="line x" title="124:212	Based on this, we calculate relative similarity scores, which are computed by dividing the original similarity of a given subpath by the average similarity of its surrounding background." ></td>
	<td class="line x" title="125:212	The motivation for capturing the relative similarity is to punish subpaths that cannot distinguish themselves from their background, e.g., those found in a block of high-similarity regions caused by certain acoustic noise." ></td>
	<td class="line x" title="126:212	Distortion score Warped subpaths are less likely to correspond to valid matching patterns than straighter ones." ></td>
	<td class="line x" title="127:212	In addition to removing very distorted subpaths by applying a distortion threshold as in (Park and Glass, 2008), we also quantitatively measured the remaining ones." ></td>
	<td class="line x" title="128:212	We fit each of them with leastsquare linear regression and estimate the residue scores." ></td>
	<td class="line x" title="129:212	As discussed above, each point on a subpath satisfies |xi  yi| < T, so the residue cannot be bigger than T. We used this to normalize the distortion scores to the range of [0,1]." ></td>
	<td class="line x" title="130:212	Subpath length Given two subpaths with nearly identical average similarity scores, we suggest that the longer of the two is more likely to refer to content of interest that is shared between two speech utterances, e.g., named entities." ></td>
	<td class="line x" title="131:212	Longer subpaths may in this sense therefore be more useful in identifying similarities and redundancies within a speech summarization system." ></td>
	<td class="line x" title="132:212	As discussed above, since the length of a subpath len(P) has been proven to fall between L and 2L  1, i.e., L  len(P)  2L  1, given a parameter L, we normalize the path length to (len(P)  L)/L, corresponding to the range [0,1)." ></td>
	<td class="line x" title="133:212	The similarity scores of subpaths can vary widely over different spoken documents." ></td>
	<td class="line x" title="134:212	We do not use the raw similarity score of a subpath, but rather its rank." ></td>
	<td class="line x" title="135:212	For example, given an utterance pair, the top-1 subpath is more likely to be a true alignment than the rest, even if its distortion score may be higher." ></td>
	<td class="line x" title="136:212	The similarity ranks are combined with distortion scores and subpath lengths simply as follows." ></td>
	<td class="line x" title="137:212	We divide subpaths into the top 1, 3, 5, and 10 by their raw similarity scores." ></td>
	<td class="line x" title="138:212	For subpaths in each group, we check whether their distortion scores are below and lengths are above 553 some thresholds." ></td>
	<td class="line x" title="139:212	If they are, in any group, then the corresponding subpaths are selected as true alignments for the purposes of building utterancelevel similarity matrix." ></td>
	<td class="line x" title="140:212	The numbers of true alignments are used to measure the similarity between two utterances." ></td>
	<td class="line x" title="141:212	We therefore have 8 threshold parameters to estimate, and subpaths with similarity scores outside the top 10 are ignored." ></td>
	<td class="line x" title="142:212	The rank groups are checked one after another in a decision list." ></td>
	<td class="line x" title="143:212	Powells algorithm (Press et al., 2007) is used to find the optimal parameters that directly minimize summarization errors made by the acousticsbased model relative to utterances selected from manual transcripts." ></td>
	<td class="line x" title="144:212	3.3 Extractive summarization Once the similarity matrix between sentences in a topic is acquired, we can conduct extractive summarization by using the matrix to estimate both similarity and redundancy." ></td>
	<td class="line x" title="145:212	As discussed above, we take the general framework of MMR and MEAD, i.e., a linear model combining salience and redundancy." ></td>
	<td class="line x" title="146:212	In practice, we used MMR in our experiments, since the original MEAD considers also sentence positions 3 , which can always been added later as in (Penn and Zhu, 2008)." ></td>
	<td class="line x" title="147:212	To facilitate our discussion below, we briefly revisit MMR here." ></td>
	<td class="line x" title="148:212	MMR (Carbonell and Goldstein, 1998) iteratively augments the summary with utterances that are most similar to the document set under consideration, but most dissimilar to the previously selected utterances in that summary, as shown in the equation below." ></td>
	<td class="line x" title="149:212	Here, the sim1 term represents the similarity between a sentence and the document set it belongs to." ></td>
	<td class="line x" title="150:212	The assumption is that a sentence having a higher sim1 would better represent the content of the documents." ></td>
	<td class="line x" title="151:212	The sim2 term represents the similarity between a candidate sentence and sentences already in the summary." ></td>
	<td class="line x" title="152:212	It is used to control redundancy." ></td>
	<td class="line x" title="153:212	For the transcriptbased systems, the sim1 and sim2 scores in this paper are measured by the number of words shared between a sentence and a sentence/document set mentioned above, weighted by the idf scores of these words, which is similar to the calculation of sentence centroid values by Radev et al.(2004)." ></td>
	<td class="line x" title="155:212	3The usefulness of position varies significantly in different genres (Penn and Zhu, 2008)." ></td>
	<td class="line x" title="156:212	Even in the news domain, the style of broadcast news differs from written news, for example, the first sentence often serves to attract audiences (Christensen et al., 2004) and is hence less important as in written news." ></td>
	<td class="line x" title="157:212	Without consideration of position, MEAD is more similar to MMR." ></td>
	<td class="line x" title="158:212	Note that the acoustics-based approach estimates this by using the method discussed above in Section 3.2." ></td>
	<td class="line x" title="159:212	Nextsent = argmax tnr,j ( sim1(doc,tnr,j) (1 )maxtr,ksim2(tnr,j,tr,k)) 4 Experimental setup We use the TDT-4 dataset for our evaluation, which consists of annotated news broadcasts grouped into common topics." ></td>
	<td class="line x" title="160:212	Since our aim in this paper is to study the achievable performance of the audio-based model, we grouped together news stories by their news anchors for each topic." ></td>
	<td class="line x" title="161:212	Then we selected the largest 20 groups for our experiments." ></td>
	<td class="line x" title="162:212	Each of these contained between 5 and 20 articles." ></td>
	<td class="line x" title="163:212	We compare our acoustics-only approach against transcripts produced automatically from two ASR systems." ></td>
	<td class="line x" title="164:212	The first set of transcripts was obtained directly from the TDT-4 database." ></td>
	<td class="line x" title="165:212	These transcripts contain a word error rate of 12.6%, which is comparable to the best accuracies obtained in the literature on this data set." ></td>
	<td class="line x" title="166:212	We also run a custom ASR system designed to produce transcripts at various degrees of accuracy in order to simulate the type of performance one might expect given languages with sparser training corpora." ></td>
	<td class="line x" title="167:212	These custom acoustic models consist of context-dependent tri-phone units trained on HUB-4 broadcast news data by sequential Viterbi forced alignment." ></td>
	<td class="line x" title="168:212	During each round of forced alignment, the maximum likelihood linear regression (MLLR) transform is used on gender-dependent models to improve the alignment quality." ></td>
	<td class="line x" title="169:212	Language models are also trained on HUB-4 data." ></td>
	<td class="line x" title="170:212	Our aim in this paper is to study the achievable performance of the audio-based model." ></td>
	<td class="line x" title="171:212	Instead of evaluating the result against human generated summaries, we directly compare the performance against the summaries obtained by using manual transcripts, which we take as an upper bound to the audio-based systems performance." ></td>
	<td class="line x" title="172:212	This obviously does not preclude using the audio-based system together with other features such as utterance position, length, speakers roles, and most others used in the literature (Penn and Zhu, 2008)." ></td>
	<td class="line x" title="173:212	Here, we do not want our results to be affected by them with the hope of observing the difference accurately." ></td>
	<td class="line oc" title="174:212	As such, we quantify success based on ROUGE (Lin, 2004) scores." ></td>
	<td class="line x" title="175:212	Our goal is to evalu554 ate whether the relatedness of spoken documents can reasonably be gleaned solely from the surface acoustic information." ></td>
	<td class="line x" title="176:212	5 Experimental results We aim to empirically determine the extent to which acoustic information alone can effectively replace conventional speech recognition within the multi-document speech summarization task." ></td>
	<td class="line x" title="177:212	Since ASR performance can vary greatly as we discussed above, we compare our system against automatic transcripts having word error rates of 12.6%, 20.9%, 29.2%, and 35.5% on the same speech source." ></td>
	<td class="line x" title="178:212	We changed our language models by restricting the training data so as to obtain the worst WER and then interpolated the corresponding transcripts with the TDT-4 original automatic transcripts to obtain the rest." ></td>
	<td class="line o" title="179:212	Figure 2 shows ROUGE scores for our acoustics-only system, as depicted by horizontal lines, as well as those for the extractive summaries given automatic transcripts having different WERs, as depicted by points." ></td>
	<td class="line x" title="180:212	Dotted lines represent the 95% confidence intervals of the transcript-based models." ></td>
	<td class="line x" title="181:212	Figure 2 reveals that, typically, as the WERs of automatic transcripts increase to around 33%-37%, the difference between the transcript-based and the acoustics-based models is no longer significant." ></td>
	<td class="line x" title="182:212	These observations are consistent across summaries with different fixed lengths, namely 10%, 20%, and 30% of the lengths of the source documents for the top, middle, and bottom rows of Figure 2, respectively." ></td>
	<td class="line o" title="183:212	The consistency of this trend is shown across both ROUGE-2 and ROUGE-SU4, which are the official measures used in the DUC evaluation." ></td>
	<td class="line x" title="184:212	We also varied the MMR parameter  within a typical range of 0.41, which yielded the same observation." ></td>
	<td class="line x" title="185:212	Since the acoustics-based approach can be applied to any data domain and to any language in principle, this would be of special interest when those situations yield relatively high WER with conventional ASR." ></td>
	<td class="line o" title="186:212	Figure 2 also shows the ROUGE scores achievable by selecting utterances uniformly at random for extractive summarization, which are significantly lower than all other presented methods and corroborate the usefulness of acoustic information." ></td>
	<td class="line o" title="187:212	Although our acoustics-based method performs similarly to automatic transcripts with 33-37% WER, the errors observed are not the same, which 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=10% Rand=0.197 ROUGESU4 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=20%, Rand=0.340 ROUGESU4 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=30%, Rand=0.402 ROUGESU4 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=10%, Rand=0.176 ROUGE2 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=20%, Rand=0.324 ROUGE2 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=30%, Rand=0.389 ROUGE2 Word error rate Figure 2: ROUGE scores and 95% confidence intervals for the MMR-based extractive summaries produced from our acoustics-only approach (horizontal lines), and from ASR-generated transcripts having varying WER (points)." ></td>
	<td class="line x" title="188:212	The top, middle, and bottom rows of subfigures correspond to summaries whose lengths are fixed at 10%, 20%, and 30% the sizes of the source text, respectively." ></td>
	<td class="line x" title="189:212	 in MMR takes 1, 0.7, and 0.4 in these rows, respectively." ></td>
	<td class="line x" title="190:212	we attribute to fundamental differences between these two methods." ></td>
	<td class="line x" title="191:212	Table 1 presents the number of different utterances correctly selected by the acoustics-based and ASR-based methods across three categories, namely those sentences that are correctly selected by both methods, those appearing only in the acoustics-based summaries, and those appearing only in the ASR-based summaries." ></td>
	<td class="line x" title="192:212	These are shown for summaries having different proportional lengths relative to the source documents and at different WERs." ></td>
	<td class="line x" title="193:212	Again, correctness here means that the utterance is also selected when using a manual transcript, since that is our defined topline." ></td>
	<td class="line x" title="194:212	A manual analysis of the corpus shows that utterances correctly included in summaries by 555 Summ." ></td>
	<td class="line x" title="195:212	Both ASR Aco.length only only WER=12.6% 10% 85 37 8 20% 185 62 12 30% 297 87 20 WER=20.9% 10% 83 36 10 20% 178 65 19 30% 293 79 24 WER=29.2% 10% 77 34 16 20% 172 58 25 30% 286 64 31 WER=35.5% 10% 75 33 18 20% 164 54 33 30% 272 67 45 Table 1: Utterances correctly selected by both the ASR-based models and acoustics-based approach, or by either of them, under different WERs (12.6%, 20.9%, 29.2%, and 35.5%) and summary lengths (10%, 20%, and 30% utterances of the original documents) the acoustics-based method often contain out-ofvocabulary errors in the corresponding ASR transcripts." ></td>
	<td class="line x" title="196:212	For example, given the news topic of the bombing of the U.S. destroyer ship Cole in Yemen, the ASR-based method always mistook the word Cole, which was not in the vocabulary, for cold, khol, and called." ></td>
	<td class="line x" title="197:212	Although named entities and domain-specific terms are often highly relevant to the documents in which they are referenced, these types of words are often not included in ASR vocabularies, due to their relative global rarity." ></td>
	<td class="line x" title="198:212	Importantly, an unsupervised acoustics-based approach such as ours does not suffer from this fundamental discord." ></td>
	<td class="line x" title="199:212	At the very least, these findings suggest that ASR-based summarization systems augmented with our type of approach might be more robust against out-of-vocabulary errors." ></td>
	<td class="line x" title="200:212	It is, however, very encouraging that an acousticsbased approach can perform to within a typical WER range within non-broadcast-news domains, although those domains can likewise be more challenging for the acoustics-based approach." ></td>
	<td class="line x" title="201:212	Further experimentation is necessary." ></td>
	<td class="line x" title="202:212	It is also of scientific interest to be able to quantify this WER as an acoustics-only baseline for further research on ASR-based spoken document summarizers." ></td>
	<td class="line x" title="203:212	6 Conclusions and future work In text summarization, statistics based on word counts have traditionally served as the foundation of state-of-the-art models." ></td>
	<td class="line x" title="204:212	In this paper, the similarity of utterances is estimated directly from recurring acoustic patterns in untranscribed audio sequences." ></td>
	<td class="line x" title="205:212	These relatedness scores are then integrated into a maximum marginal relevance linear model to estimate the salience and redundancy of those utterance for extractive summarization." ></td>
	<td class="line x" title="206:212	Our empirical results show that the summarization performance given acoustic information alone is statistically indistinguishable from that of modern ASR on broadcast news in cases where the WER of the latter approaches 33%-37%." ></td>
	<td class="line x" title="207:212	This is an encouraging result in cases where summarization is required, but ASR is not available or speech recognition performance is degraded." ></td>
	<td class="line x" title="208:212	Additional analysis suggests that the acoustics-based approach is useful in overcoming situations where out-ofvocabulary error may be more prevalent, and we suggest that a hybrid approach of traditional ASR with acoustics-based pattern matching may be the most desirable future direction of research." ></td>
	<td class="line x" title="209:212	One limitation of the current analysis is that summaries are extracted only for collections of spoken documents from among similar speakers." ></td>
	<td class="line x" title="210:212	Namely, none of the topics under analysis consists of a mix of male and female speakers." ></td>
	<td class="line x" title="211:212	We are currently investigating supervised methods to learn joint probabilistic models relating the acoustics of groups of speakers in order to normalize acoustic similarity matrices (Toda et al., 2001)." ></td>
	<td class="line x" title="212:212	We suggest that if a stochastic transfer function between male and female voices can be estimated, then the somewhat disparate acoustics of these groups of speakers may be more easily compared." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1099
Comparing Objective and Subjective Measures of Usability in a Human-Robot Dialogue System
Foster, Mary Ellen;Giuliani, Manuel;Knoll, Alois;"></td>
	<td class="line x" title="1:159	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879887, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:159	c2009 ACL and AFNLP Comparing Objective and Subjective Measures of Usability in a Human-Robot Dialogue System Mary Ellen Foster and Manuel Giuliani and Alois Knoll Informatik VI: Robotics and Embedded Systems Technische Universitat Munchen Boltzmannstrae 3, 85748 Garching bei Munchen, Germany {foster,giuliani,knoll}@in.tum.de Abstract We present a human-robot dialogue system that enables a robot to work together with a human user to build wooden construction toys." ></td>
	<td class="line x" title="3:159	We then describe a study in which nave subjects interacted with this system under a range of conditions and then completed a user-satisfaction questionnaire." ></td>
	<td class="line x" title="4:159	The results of this study provide a wide range of subjective and objective measures of the quality of the interactions." ></td>
	<td class="line x" title="5:159	To assess which aspects of the interaction had the greatest impact on the users opinions of the system, we used a method based on the PARADISE evaluation framework (Walker et al., 1997) to derive a performance function from our data." ></td>
	<td class="line x" title="6:159	The major contributors to user satisfaction were the number of repetition requests (which had a negative effect on satisfaction), the dialogue length, and the users recall of the system instructions (both of which contributed positively)." ></td>
	<td class="line x" title="7:159	1 Introduction Evaluating the usability of a spoken language dialogue system generally requires a large-scale user study, which can be a time-consuming process both for the experimenters and for the experimental subjects." ></td>
	<td class="line x" title="8:159	In fact, it can be difficult even to define what the criteria are for evaluating such a system (cf.Novick, 1997)." ></td>
	<td class="line x" title="10:159	In recent years, techniques have been introduced that are designed to predict user satisfaction based on more easily measured properties of an interaction such as dialogue length and speech-recognition error rate." ></td>
	<td class="line x" title="11:159	The design of such performance methods for evaluating dialogue systems is still an area of open research." ></td>
	<td class="line x" title="12:159	The PARADISE framework (PARAdigm for DIalogue System Evaluation; Walker et al.(1997)) describes a method for using data to derive a performance function that predicts user-satisfaction scores from the results on other, more easily computed measures." ></td>
	<td class="line x" title="14:159	PARADISE uses stepwise multiple linear regression to model user satisfaction based on measures representing the performance dimensions of task success, dialogue quality, and dialogue efficiency, and has been applied to a wide range of systems (e.g., Walker et al., 2000; Litman and Pan, 2002; Moller et al., 2008)." ></td>
	<td class="line x" title="15:159	If the resulting performance function can be shown to predict user satisfaction as a function of other, more easily measured system properties, it will be widely applicable: in addition to making it possible to evaluate systems based on automatically available data from log files without the need for extensive experiments with users, for example, such a performance function can be used in an online, incremental manner to adapt system behaviour to avoid entering a state that is likely to reduce user satisfaction, or can be used as a reward function in a reinforcement-learning scenario (Walker, 2000)." ></td>
	<td class="line oc" title="16:159	Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU (Papineni et al., 2002) for machine translation and ROUGE (Lin, 2004) for summarisation, for example." ></td>
	<td class="line o" title="17:159	When employing any such metric, it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output." ></td>
	<td class="line x" title="18:159	If not, the risk arises that the automated measures do not capture the behaviour that is actually relevant for the human users of a system." ></td>
	<td class="line x" title="19:159	For example, Callison-Burch et al.(2006) presented a number of 879 counter-examples to the claim that BLEU agrees with human judgements." ></td>
	<td class="line x" title="21:159	Also, Foster (2008) examined a range of automated metrics for evaluation generated multimodal output and found that few agreed with the preferences expressed by human judges." ></td>
	<td class="line x" title="22:159	In this paper, we apply a PARADISE-style process to the results of a user study of a human-robot dialogue system." ></td>
	<td class="line x" title="23:159	We build models to predict the results on a set of subjective user-satisfaction measures, based on objective measures that were either gathered automatically from the system logs or derived from the video recordings of the interactions." ></td>
	<td class="line x" title="24:159	The results indicate that the most significant contributors to user satisfaction were the number of system turns in the dialogues, the users ability to recall the instructions given by the robot, and the number of times that the user had to ask for instructions to be repeated." ></td>
	<td class="line x" title="25:159	The former two measures were positively correlated with user satisfaction, while the latter had a negative impact on user satisfaction; however the correlation in all cases was relatively low." ></td>
	<td class="line x" title="26:159	At the end of the paper, we discuss possible reasons for these results and propose other measures that might have a larger effect on users judgements." ></td>
	<td class="line x" title="27:159	2 Task-Based Human-Robot Dialogue This study makes use of the JAST human-robot dialogue system (Rickert et al., 2007) which supports multimodal human-robot collaboration on a joint construction task." ></td>
	<td class="line x" title="28:159	The user and the robot work together to assemble wooden construction toys on a common workspace, coordinating their actions through speech, gestures, and facial displays." ></td>
	<td class="line x" title="29:159	The robot (Figure 1) consists of a pair of manipulator arms with grippers, mounted in a position to resemble human arms, and an animatronic talking head (van Breemen, 2005) capable of producing facial expressions, rigid head motion, and lip-synchronised synthesised speech." ></td>
	<td class="line x" title="30:159	The system can interact in English or German." ></td>
	<td class="line x" title="31:159	The robot is able to manipulate objects in the workspace and to perform simple assembly tasks." ></td>
	<td class="line x" title="32:159	In the system that was used in the current study, the robot instructs the user on building a particular compound object, explaining the necessary assembly steps and retrieving pieces as required, with the user performing the actual assembly actions." ></td>
	<td class="line x" title="33:159	To make joint action necessary for success in the assembly task, the workspace is divided into Figure 1: The JAST dialogue robot SYSTEM First we will build a windmill." ></td>
	<td class="line x" title="34:159	Okay?" ></td>
	<td class="line x" title="35:159	USER Okay." ></td>
	<td class="line x" title="36:159	SYSTEM To make a windmill, we must make a snowman." ></td>
	<td class="line x" title="37:159	SYSTEM [picking up and holding out red cube] To make a snowman, insert the green bolt through the end of this red cube and screw it into the blue cube." ></td>
	<td class="line x" title="38:159	USER [takes cube, performs action] Okay." ></td>
	<td class="line x" title="39:159	SYSTEM [picking up and holding out a small slat] To make a windmill, insert the yellow bolt through the middle of this short slat and the middle of another short slat and screw it into the snowman." ></td>
	<td class="line x" title="40:159	USER [takes slat, performs action] Okay." ></td>
	<td class="line x" title="41:159	SYSTEM Very good!" ></td>
	<td class="line x" title="42:159	Figure 2: Sample human-robot dialogue 880 (a) Windmill (b) Snowman (c) L Shape (d) Railway signal Figure 3: The four target objects used in the experiment two areasone belonging to the robot and one to the userso that the robot must hand over some pieces to the user." ></td>
	<td class="line x" title="43:159	Figure 2 shows a sample dialogue in which the system explains to the user how to build an object called a windmill, which has a sub-component called a snowman." ></td>
	<td class="line x" title="44:159	3 Experiment Design The human-robot system was evaluated via a user study in which subjects interacted with the complete system; all interactions were in German." ></td>
	<td class="line x" title="45:159	As a between-subjects factor, we manipulated two aspects of the generated output: the strategy used by the dialogue manager to explain a plan to the user, and the type of referring expressions produced by the system." ></td>
	<td class="line x" title="46:159	Foster et al.(2009) give the details of these factors and describes the effects of each individual manipulation." ></td>
	<td class="line x" title="48:159	In this paper, we concentrate on the relationships among the different factors that were measured during the study: the efficiency and quality of the dialogues, the users success at building the required objects and at learning the construction plans for new objects, and the users subjective reactions to the system." ></td>
	<td class="line x" title="49:159	3.1 Subjects 43 subjects (27 male) took part in this experiment; the results of one additional subject were discarded due to technical problems with the system." ></td>
	<td class="line x" title="50:159	The mean age of the subjects was 24.5, with a minimum of 14 and a maximum of 55." ></td>
	<td class="line x" title="51:159	Of the subjects who indicated an area of study, the two most common areas were Informatics (12 subjects) and Mathematics (10)." ></td>
	<td class="line x" title="52:159	On a scale of 15, subjects gave a mean assessment of their knowledge of computers at 3.4, of speech-recognition systems at 2.3, and of human-robot systems at 2.0." ></td>
	<td class="line x" title="53:159	The subjects were compensated for their participation in the experiment." ></td>
	<td class="line x" title="54:159	3.2 Scenario In this experiment, each subject built the same three objects in collaboration with the system, always in the same order." ></td>
	<td class="line x" title="55:159	The first target was a windmill (Figure 3a), which has a subcomponent called a snowman (Figure 3b)." ></td>
	<td class="line x" title="56:159	Once the windmill was completed, the system then walked the user through building an L shape (Figure 3c)." ></td>
	<td class="line x" title="57:159	Finally, the robot instructed the user to build a railway signal (Figure 3d), which combines an L shape with a snowman." ></td>
	<td class="line x" title="58:159	During the construction of the railway signal, the system asked the user if they remembered how to build a snowman and an L shape." ></td>
	<td class="line x" title="59:159	If the user did not remember, the system explained the building process again; if they did remember, the system simply told them to build another one." ></td>
	<td class="line x" title="60:159	3.3 Dependent Variables We gathered a wide range of dependent measures: objective measures derived from the system logs and video recordings, as well as subjective measures based on the users own ratings of their experience interacting with the system." ></td>
	<td class="line x" title="61:159	3.3.1 Objective Measures We collected a range of objective measures from the log files and videos of the interactions." ></td>
	<td class="line x" title="62:159	Like Litman and Pan (2002), we divided our objective measures into three categories based on those used in the PARADISE framework: dialogue efficiency, dialogue quality, and task success." ></td>
	<td class="line x" title="63:159	The dialogue efficiency measures concentrated on the timing of the interaction: the time taken to complete the three construction tasks, the number of system turns required for the complete interaction, and the mean time taken by the system to respond to the users requests." ></td>
	<td class="line x" title="64:159	We considered four measures of dialogue quality." ></td>
	<td class="line x" title="65:159	The first two measures looked specifically for signs of problems in the interaction, using data au881 tomatically extracted from the logs: the number of times that the user asked the system to repeat its instructions, and the number of times that the user failed to take an object that the robot attempted to hand over." ></td>
	<td class="line x" title="66:159	The other two dialogue quality measures were computed based on the video recordings: the number of times that the user looked at the robot, and the percentage of the total interaction that they spent looking at the robot." ></td>
	<td class="line x" title="67:159	We considered these gaze-based measures to be measures of dialogue quality since it has previously been shown that, in this sort of task-based interaction where there is a visually salient object, participants tend to look at their partner more often when there is a problem in the interaction (e.g., Argyle and Graham, 1976)." ></td>
	<td class="line x" title="68:159	The task success measures addressed user success in the two main tasks undertaken in these interactions: assembling the target objects following the robots instructions, and learning and remembering to make a snowman and an L shape." ></td>
	<td class="line x" title="69:159	We measured task success in two ways, corresponding to these two main tasks." ></td>
	<td class="line x" title="70:159	The users success in the overall assembly task was assessed by counting the proportion of target objects that were assembled as intended (i.e., as in Figure 3), which was judged based on the video recordings." ></td>
	<td class="line x" title="71:159	To test whether the subjects had learned how to build the sub-components that were required more than once (the snowman and the L shape), we recorded whether they said yes or no when they were asked if they remembered each of these components during the construction of the railway signal." ></td>
	<td class="line x" title="72:159	3.3.2 Subjective Measures In addition to the above objective measures, we also gathered a range of subjective measures." ></td>
	<td class="line x" title="73:159	Before the interaction, we asked subjects to rate their current level on a set of 22 emotions (Ortony et al., 1988) on a scale from 1 to 4; the subjects then rated their level on the same emotional scales again after the interaction." ></td>
	<td class="line x" title="74:159	After the interaction, the subjects also filled out a user-satisfaction questionnaire, which was based on that used in the user evaluation of the COMIC dialogue system (White et al., 2005), with modifications to address specific aspects of the current dialogue system and the experimental manipulations in this study." ></td>
	<td class="line x" title="75:159	There were 47 items in total, each of which requested that the user choose their level of agreement with a given statement on a five-point Likert scale." ></td>
	<td class="line x" title="76:159	The items were divided into the following categories: Mean (Stdev) Min Max Length (sec) 305.1 (54.0) 195.2 488.4 System turns 13.4 (1.73) 11 18 Response time (sec) 2.79 (1.13) 1.27 7.21 Table 1: Dialogue efficiency results Opinion of the robot as a partner 21 items addressing the ease with which subjects were able to interact with the robot Instruction quality 6 items specifically addressing the quality of the assembly instructions given by the robot Task success 11 items asking the user to rate how well they felt they performed on the various assembly tasks Feelings of the user 9 items asking users to rate their feelings while using the system At the end of the questionnaire, subjects were also invited to give free-form comments." ></td>
	<td class="line x" title="77:159	4 Results In this section, we present the results of each of the individual dependent measures; in the following section, we examine the relationship among the different types of measures." ></td>
	<td class="line x" title="78:159	These results are based on the data from 40 subjects: we excluded results from two subjects for whom the video data was not clear, and from one additional subject who appeared to be testing the system rather than making a serious effort to interact with it." ></td>
	<td class="line x" title="79:159	4.1 Objective Measures Dialogue efficiency The results on the dialogue efficiency measures are shown in Table 1." ></td>
	<td class="line x" title="80:159	The average subject took 305.1 secondsthat is, just over five minutesto build all three of the objects, and an average dialogue took 13 system turns to complete." ></td>
	<td class="line x" title="81:159	When a user made a request, the mean delay before the beginning of the system response was about three seconds, although for one user this time was more than twice as long." ></td>
	<td class="line x" title="82:159	This response delay resulted from two factors." ></td>
	<td class="line x" title="83:159	First, preparing long system utterances with several referring expressions (such as the third and fourth system turns in Figure 2) takes some time; second, if a user made a request during a system turn (i.e., a barge-in attempt), the system was not able to respond until the current turn was completed." ></td>
	<td class="line x" title="84:159	882 Mean (Stdev) Min Max Repetition requests 1.86 (1.79) 0 6 Failed hand-overs 1.07 (1.35) 0 6 Looks at the robot 23.55 (8.21) 14 50 Time looking at robot (%) 27 (8.6) 12 51 Table 2: Dialogue quality results These three measures of efficiency were correlated with each other: the correlation between length and turns was 0.38; between length and response time 0.47; and between turns and response time 0.19 (all p < 0.0001)." ></td>
	<td class="line x" title="85:159	Dialogue quality Table 2 shows the results for the dialogue quality measures: the two indications of problems, and the two measures of the frequency with which the subjects looked at the robots head." ></td>
	<td class="line x" title="86:159	On average, a subject asked for an instruction to be repeated nearly two times per interaction, while failed hand-overs occurred just over once per interaction; however, as can be seen from the standard-deviation values, these measures varied widely across the data." ></td>
	<td class="line x" title="87:159	In fact, 18 subjects never failed to take an object from the robot when it was offered, while one subject did so five times and one six times." ></td>
	<td class="line x" title="88:159	Similarly, 11 subjects never asked for any repetitions, while five subjects asked for repetitions five or more times.1 On average, the subjects in this study spent about a quarter of the interaction looking at the robot head, and changed their gaze to the robot 23.5 times over the course of the interaction." ></td>
	<td class="line x" title="89:159	Again, there was a wide range of results for both of these measures: 15 subjects looked at the robot fewer than 20 times during the interaction, 20 subjects looked at the robot between 20 to 30 times, while 5 subjects looked at the robot more than 30 times." ></td>
	<td class="line x" title="90:159	The two measures that count problems were mildly correlated with each other (R2 = 0.26, p < 0.001), as were the two measures of looking at the robot (R2 = 0.13, p < 0.05); there was no correlation between the two classes of measures." ></td>
	<td class="line x" title="91:159	Task success Table 3 shows the success rate for assembling each object in the sequence." ></td>
	<td class="line x" title="92:159	Objects in italics represent sub-components, as follows: the first snowman was constructed as part of the windmill, while the second formed part of the railway signal; the first L-shape was a goal in itself, 1The requested repetition rate was significantly affected by the description strategy used by the dialogue manager; see Foster et al.(2009) for details." ></td>
	<td class="line x" title="94:159	Object Rate Memory Snowman 0.76 Windmill 0.55 L shape 0.90 L shape 0.90 0.88 Snowman 0.86 0.70 Railway signal 0.71 Overall 0.72 0.79 Table 3: Task success results while the second was also part of the process of building the railway signal." ></td>
	<td class="line x" title="95:159	The Rate column indicates subjects overall success at building the relevant componentfor example, 55% of the subjects built the windmill correctly, while both of the L-shapes were built with 90% accuracy." ></td>
	<td class="line x" title="96:159	For the second occurrence of the snowman and the Lshape, the Memory column indicates the percentage of subjects who claimed to remember how to build it when asked." ></td>
	<td class="line x" title="97:159	The Overall row at the bottom indicates subjects overall success rate at building the three main target objects (windmill, L shape, railway signal): on average, a subject built about two of the three objects correctly." ></td>
	<td class="line x" title="98:159	The overall correct-assembly rate was correlated with the overall rate of remembering objects: R2 = 0.20, p < 0.005." ></td>
	<td class="line x" title="99:159	However, subjects who said that they did remember how to build a snowman or an L shape the second time around were no more likely to do it correctly than those who said that they did not remember." ></td>
	<td class="line x" title="100:159	4.2 Subjective Measures Two types of subjective measures were gathered during this study: responses on the usersatisfaction questionnaire, and self-assessment of emotions." ></td>
	<td class="line x" title="101:159	Table 4 shows the mean results for each category from the user-satisfaction questionnaire across all of the subjects, in all cases on a 5-point Likert scale." ></td>
	<td class="line x" title="102:159	The subjects in this study gave a generally positive assessment of their interactions with the systemwith a mean overall satisfaction score of 3.75and rated their perceived task success particularly highly, with a mean score of 4.1." ></td>
	<td class="line x" title="103:159	To analyse the emotional data, we averaged all of the subjects emotional self-ratings before and after the experiment, counting negative emotions on an inverse scale, and then computed the difference between the two means." ></td>
	<td class="line x" title="104:159	Table 5 shows the results from this analysis; note that this value was assessed on a 14 scale." ></td>
	<td class="line x" title="105:159	While the mean emotional 883 Question category Mean (Stdev) Robot as partner 3.63 (0.65) Instruction quality 3.69 (0.71) Task success 4.10 (0.68) Feelings 3.66 (0.61) Overall 3.75 (0.57) Table 4: User-satisfaction questionnaire results Mean (Stdev) Min Max Before the study 2.99 (0.32) 2.32 3.68 After the study 3.05 (0.32) 2.32 3.73 Change +0.06 (0.24) 0.55 +0.45 Table 5: Mean emotional assessments score across all of the subjects did not change over the course of the experiment, the ratings of individual subjects did show larger changes." ></td>
	<td class="line x" title="106:159	As shown in the final row of the table, one subjects mean rating decreased by 0.55 over the course of the interaction, while that of another subject increased by 0.45." ></td>
	<td class="line x" title="107:159	There was a slight correlation between the subjects description of their emotional state after the experiment and their responses to the questionnaire items asking for feelings about the interaction: R2 = 0.14, p < 0.01." ></td>
	<td class="line x" title="108:159	5 Building Performance Functions In the preceding section, we presented results on a number of objective and subjective measures, and also examined the correlation among measures of the same type." ></td>
	<td class="line x" title="109:159	The results on the objective measures varied widely across the subjects; also, the subjects generally rated their experience of using the system positively, but again with some variation." ></td>
	<td class="line x" title="110:159	In this section, we examine the relationship among measures of different types in order to determine which of the objective measures had the largest effect on users subjective reactions to the dialogue system." ></td>
	<td class="line x" title="111:159	To determine the relationship among the factors, we employed the procedure used in the PARADISE evaluation framework (Walker et al., 1997)." ></td>
	<td class="line x" title="112:159	The PARADISE model uses stepwise multiple linear regression to predict subjective user satisfaction based on measures representing the performance dimensions of task success, dialogue quality, and dialogue efficiency, resulting in a predictor function of the following form: Satisfaction = n i=1 wiN (mi) The mi terms represent the value of each measure, while the N function transforms each measure into a normal distribution using z-score normalisation." ></td>
	<td class="line x" title="113:159	Stepwise linear regression produces coefficients (wi) describing the relative contribution of each predictor to the user satisfaction." ></td>
	<td class="line x" title="114:159	If a predictor does not contribute significantly, its wi value is zero after the stepwise process." ></td>
	<td class="line x" title="115:159	Using stepwise linear regression, we computed a predictor function for each of the subjective measures that we gathered during our study: the mean score for each of the individual user-satisfaction categories (Table 4), the mean score across the whole questionnaire (the last line of Table 4), as well as the difference between the users emotional states before and after the study (the last line of Table 5)." ></td>
	<td class="line x" title="116:159	We included all of the objective measures from Section 4.1 as initial predictors." ></td>
	<td class="line x" title="117:159	The resulting predictor functions are shown in Table 6." ></td>
	<td class="line x" title="118:159	The following abbreviations are used for the factors that occur in the table: Rep for the number of repetition requests, Turns for the number of system turns, Len for the length of the dialogue, and Mem for the subjects memory for the components that were built twice." ></td>
	<td class="line x" title="119:159	The R2 column indicates the percentage of the variance that is explained by the performance function, while the Significance column gives significance values for each term in the function." ></td>
	<td class="line x" title="120:159	Although the R2 values for the predictor functions in Table 6 are generally quite low, indicating that the functions do not explain most of the variance in the data, the factors that remain after stepwise regression still provide an indication as to which of the objective measures had an effect on users opinions of the system." ></td>
	<td class="line x" title="121:159	In general, users who had longer interactions with the system (in terms of system turns) and who said that they remembered the robots instructions tended to give the system higher scores, while users who asked for more instructions to be repeated tended to give it lower scores; for the robot-as-partner questions, the length of the dialogue in seconds also made a slight negative contribution." ></td>
	<td class="line x" title="122:159	None of the other objective factors contributed significantly to any of the predictor functions." ></td>
	<td class="line x" title="123:159	6 Discussion That the factors included in Table 6 were the most significant contributors to user satisfaction is not surprising." ></td>
	<td class="line x" title="124:159	If a user asks for instructions to be re884 Measure Function R2 Significance Robot as partner 3.60+0.53N (Turns)0.39N (Rep)0.18N (Len) 0.12 Turns: p < 0.01, Rep: p < 0.05, Length: p0.17 Instruction quality 3.660.22N (Rep) 0.081 Rep: p < 0.05 Task success 4.07+0.20N (Mem) 0.058 Mem: p0.07 Feelings 3.63+0.34N (Turns)0.32N (Rep) 0.044 Turns: p0.06, Rep: p0.08 Overall 3.730.36N (Rep)+0.31N (Turns) 0.062 Rep: p < 0.05, Turns: p0.06 Emotion change 0.07+0.14N (Turns)+0.11N (Mem)0.090N (Rep) 0.20 Turns: p < 0.05, Mem: p < 0.01, Rep: p0.17 Table 6: Predictor functions peated, this is a clear indication of a problem in the dialogue; similarly, users who remembered the systems instructions were equally clearly having a relatively successful interaction." ></td>
	<td class="line x" title="125:159	In the current study, increased dialogue length had a positive contribution to user satisfaction; this contrasts with results such as those of Litman and Pan (2002), who found that increased dialogue length was associated with decreased user satisfaction." ></td>
	<td class="line x" title="126:159	We propose two possible explanations for this difference." ></td>
	<td class="line x" title="127:159	First, the system analysed by Litman and Pan (2002) was an information-seeking dialogue system, in which efficient access to the information is an important criterion." ></td>
	<td class="line x" title="128:159	The current system, on the other hand, has the goal of joint task execution, and pure efficiency is a less compelling measure of dialogue quality in this setting." ></td>
	<td class="line x" title="129:159	Second, it is possible that the sheer novelty factor of interacting with a fully-embodied humanoid robot affected peoples subjective responses to the system, so that subjects who had longer interactions also enjoyed the experience more." ></td>
	<td class="line x" title="130:159	Support for this explanation is provided by the fact that dialogue length was only a significant factor in the more subjective parts of the questionnaire, but did not have a significant impact on the users judgements about instruction quality or task success." ></td>
	<td class="line x" title="131:159	Other studies of human-robot dialogue systems have also had similar results: for example, the subjects in the study described by Sidner et al.(2005) who used a robot that moved while talking reported higher levels of engagement in the interaction, and also tended to have longer conversations with the robot." ></td>
	<td class="line x" title="133:159	While the predictor functions give useful insights into the relative contribution of the objective measures to the subjective user satisfaction, the R2 values are generally lower than those found in other PARADISE-style evaluations." ></td>
	<td class="line x" title="134:159	For example, Walker et al.(1998) reported an R2 value of 0.38, the values reported by Walker et al.(2000) on the training sets ranged from 0.39 to 0.56, Litman and Pan (2002) reported an R2 value of 0.71, while the R2 values reported by Moller et al.(2008) for linear regression models similar to those presented here were between 0.22 and 0.57." ></td>
	<td class="line x" title="138:159	The low R2 values from this analysis clearly suggest that, while the factors included in Table 6 did affect users opinionsparticularly their opinion of the robot as a partner and the change in their reported emotional statethe users subjective judgements were also affected by factors other than those captured by the objective measures considered here." ></td>
	<td class="line x" title="139:159	In most of the previous PARADISE-style studies, measures addressing the performance of the automated speech-recognition system and other input-processing components were included in the models." ></td>
	<td class="line x" title="140:159	For example, the factors listed by Moller et al.(2008) include several measures of word error rate and of parsing accuracy." ></td>
	<td class="line x" title="142:159	However, the scenario that was used in the current study required minimal speech input from the user (see Figure 2), so we did not include any such input-processing factors in our models." ></td>
	<td class="line x" title="143:159	Other objective factors that might be relevant for predicting user satisfaction in the current study include a range of non-verbal behaviour from the users." ></td>
	<td class="line x" title="144:159	For example, the users reaction time to instructions from the robot, the time the users need to adapt to the robots movements during handover actions (Huber et al., 2008), or the time taken for the actual assembly of the objects." ></td>
	<td class="line x" title="145:159	Also, other measures of the users gaze behaviour might be 885 useful: more global measures such as how often the users look at the robot arms or at the objects on the table, as well as more targeted measures examining factors such as the users gaze and other behaviour during and after different types of system outputs." ></td>
	<td class="line x" title="146:159	In future studies, we will also gather data on these additional non-verbal behaviours, and we expect to find higher correlations with subjective judgements." ></td>
	<td class="line x" title="147:159	7 Conclusions and Future Work We have presented the JAST human-robot dialogue system and described a user study in which the system instructed users to build a series of target objects out of wooden construction toys." ></td>
	<td class="line x" title="148:159	This study resulted in a range of objective and subjective measures, which were used to derive performance functions in the style of the PARADISE evaluation framework." ></td>
	<td class="line x" title="149:159	Three main factors were found to affect the users subjective ratings: longer dialogues and higher recall performance were associated with increased user satisfaction, while dialogues with more repetition requests tended to be associated with lower satisfaction scores." ></td>
	<td class="line x" title="150:159	The explained variance of the performance functions was generally low, suggesting that factors other than those measured in this study contributed to the user satisfaction scores; we have suggested several such factors." ></td>
	<td class="line x" title="151:159	The finding that longer dialogues were associated with higher user satisfaction disagrees with the results of many previous PARADISE-style evaluation studies." ></td>
	<td class="line x" title="152:159	However, it does confirm and extend the results of previous studies specifically addressing interactions between users and embodied agents: as in the previous studies, the users in this case seem to view the agent as a social entity with whom they enjoy having a conversation." ></td>
	<td class="line x" title="153:159	A newer version of the JAST system is currently under development and will shortly undergo a user evaluation." ></td>
	<td class="line x" title="154:159	This new system will support an extended set of interactions where both agents know the target assembly plan, and will will also incorporate enhanced components for vision, object recognition, and goal inference." ></td>
	<td class="line x" title="155:159	When evaluating this new system, we will include similar measures to those described here to enable the evaluations of the two systems to be compared." ></td>
	<td class="line x" title="156:159	We will also gather additional objective measures in order to measure their influence on the subjective results." ></td>
	<td class="line x" title="157:159	These additional measures will include those mentioned at the end of the preceding section, as well as measures targeted at the revised scenario and the updated system capabilitiesfor example, an additional dialogue quality measure will assess how often the goal-inference system was able to detect and correctly respond to an error by the user." ></td>
	<td class="line x" title="158:159	Acknowledgements This research was supported by the European Commission through the JAST2 (ISTFP6-003747-IP) and INDIGO3 (IST-FP6-045388) projects." ></td>
	<td class="line x" title="159:159	Thanks to Pawel Dacka for his help in running the experiment and analysing the data." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2025
Correlating Human and Automatic Evaluation of a German Surface Realiser
Cahill, Aoife;"></td>
	<td class="line x" title="1:89	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 97100, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:89	c 2009 ACL and AFNLP Correlating Human and Automatic Evaluation of a German Surface Realiser Aoife Cahill Institut fur Maschinelle Sprachverarbeitung (IMS) University of Stuttgart 70174 Stuttgart, Germany aoife.cahill@ims.uni-stuttgart.de Abstract We examine correlations between native speaker judgements on automatically generated German text against automatic evaluation metrics." ></td>
	<td class="line x" title="3:89	We look at a number of metrics from the MT and Summarisation communities and find that for a relative ranking task, most automatic metrics perform equally well and have fairly strong correlations to the human judgements." ></td>
	<td class="line x" title="4:89	In contrast, on a naturalness judgement task, the General Text Matcher (GTM) tool correlates best overall, although in general, correlation between the human judgements and the automatic metrics was quite weak." ></td>
	<td class="line x" title="5:89	1 Introduction During the development of a surface realisation system, it is important to be able to quickly and automatically evaluate its performance." ></td>
	<td class="line x" title="6:89	The evaluation of a string realisation system usually involves string comparisons between the output of the system and some gold standard set of strings." ></td>
	<td class="line x" title="7:89	Typically automatic metrics from the fields of Machine Translation (e.g. BLEU) or Summarisation (e.g. ROUGE) are used, but it is not clear how successful or even appropriate these are." ></td>
	<td class="line x" title="8:89	Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts)." ></td>
	<td class="line x" title="9:89	Their findings show that the NIST metric correlates best with the human judgements, and all automatic metrics favour systems that generate based on frequency." ></td>
	<td class="line x" title="10:89	They conclude that automatic evaluations should be accompanied by human evaluations where possible." ></td>
	<td class="line x" title="11:89	Stent et al.(2005) investigate a number of automatic evaluation methods for generation in terms of adequacy and fluency on automatically generated English paraphrases." ></td>
	<td class="line x" title="13:89	They find that the automatic metrics are reasonably good at measuring adequacy, but not good measures of fluency, i.e. syntactic correctness." ></td>
	<td class="line x" title="14:89	In this paper, we carry out experiments to correlate automatic evaluation of the output of a surface realisation ranking system for German against human judgements." ></td>
	<td class="line x" title="15:89	We particularly look at correlations at the individual sentence level." ></td>
	<td class="line x" title="16:89	2 Human Evaluation Experiments The data used in our experiments is the output of the Cahill et al.(2007) German realisation ranking system." ></td>
	<td class="line x" title="18:89	That system is couched within the Lexical Functional Grammar (LFG) grammatical framework." ></td>
	<td class="line x" title="19:89	LFG has two levels of representation, C(onstituent)-Structure which is a contextfree tree representation and F(unctional)-Structure which is a recursive attribute-value matrix capturing basic predicate-argument-adjunct relations." ></td>
	<td class="line x" title="20:89	Cahill et al.(2007) use a large-scale handcrafted grammar (Rohrer and Forst, 2006) to generate a number of (almost always) grammatical sentences given an input F-Structure." ></td>
	<td class="line x" title="22:89	They show that a linguistically-inspired log-linear ranking model outperforms a simple baseline tri-gram language model trained on the Huge German Corpus (HGC), a corpus of 200 million words of newspaper and other text." ></td>
	<td class="line x" title="23:89	Cahill and Forst (2009) describe a number of experiments where they collect judgements from native speakers about the three systems compared in Cahill et al.(2007): (i) the original corpus string, (ii) the string chosen by the language model, and (iii) the string chosen by the linguistically-inspired log-linear model.1 We only take the data from 2 of those experiments since the remaining experiments would not provide any 1In all cases, the three strings were different." ></td>
	<td class="line x" title="25:89	97 informative correlations." ></td>
	<td class="line x" title="26:89	In the first experiment that we consider (A), subjects are asked to rank on a scale from 13 (1 being the best, 3 being the worst) the output of the three systems (joint rankings were not permitted)." ></td>
	<td class="line x" title="27:89	In the second experiment (B), subjects were asked to rank on a scale from 15 (1 being the worst, 5 being the best) how natural sounding the string chosen by the log-linear model was." ></td>
	<td class="line x" title="28:89	The goal of experiment B was to determine whether the log-linear model was choosing good or bad alternatives to the original string." ></td>
	<td class="line x" title="29:89	Judgements on the data were collected from 24 native German speakers." ></td>
	<td class="line x" title="30:89	There were 44 items in Experiment A with an average sentence length of 14.4, and there were 52 items in Experiment B with an average sentence length of 12.1." ></td>
	<td class="line x" title="31:89	Each item was judged by each native speaker at least once." ></td>
	<td class="line x" title="32:89	3 Correlation with Automatic Metrics We examine the correlation between the human judgements and a number of automatic metrics: BLEU (Papineni et al., 2001) calculates the number of ngrams a solution shares with a reference, adjusted by a brevity penalty." ></td>
	<td class="line x" title="33:89	Usually the geometric mean for scores up to 4-gram are reported." ></td>
	<td class="line oc" title="34:89	ROUGE (Lin, 2004) is an evaluation metric designed to evaluate automatically generated summaries." ></td>
	<td class="line o" title="35:89	It comprises a number of string comparison methods including ngram matching and skip-ngrams." ></td>
	<td class="line o" title="36:89	We use the default ROUGE-L longest common subsequence f-score measure.2 GTM General Text Matching (Melamed et al., 2003) calculates word overlap between a reference and a solution, without double counting duplicate words." ></td>
	<td class="line o" title="37:89	It places less importance on word order than BLEU." ></td>
	<td class="line x" title="38:89	SED Levenshtein (String Edit) distance WER Word Error Rate TER Translation Error Rate (Snover et al., 2006) computes the number of insertions, deletions, substitutions and shifts needed to match a solution to a reference." ></td>
	<td class="line x" title="39:89	Most of these metrics come from the Machine Translation field, where the task is arguably significantly different." ></td>
	<td class="line x" title="40:89	In the evaluation of a surface realisation system (as opposed to a complete generation system), typically the choice of vocabulary is limited and often the task is closer to word reordering." ></td>
	<td class="line x" title="41:89	Many of the MT metrics have methods 2Preliminary experiments with the skip n-grams performed worse than the default parameters." ></td>
	<td class="line o" title="42:89	Experiment A Experiment B GOLD LM LL LL human A (rank 13) 1.4 2.55 2.05 human B (scale 15) 3.92 BLEU 1.0 0.67 0.72 0.79 ROUGE-L 1.0 0.85 0.78 0.85 GTM 1.0 0.55 0.60 0.74 SED 1.0 0.54 0.61 0.71 WER 0.0 48.04 39.88 28.83 TER 0.0 0.16 0.14 0.11 DEP 100 82.60 87.50 93.11 WDEP 1.0 0.70 0.82 0.90 Table 1: Average scores of each metric for Experiment A data Sentence Corpus corr p-value corr p-value BLEU -0.615 <0.001 -1 0.3333 ROUGE-L -0.644 <0.001 -0.5 1 GTM -0.643 <0.001 -1 0.3333 SED -0.628 <0.001 -1 0.3333 WER 0.623 <0.001 1 0.3333 TER 0.608 <0.001 1 0.3333 Table 2: Correlation between human judgements for experiment A (rank 13) and automatic metrics for attempting to account for different but equivalent translations of a given source word, typically by integrating a lexical resource such as WordNet." ></td>
	<td class="line x" title="43:89	Also, these metrics were mostly designed to evaluate English output, so it is not clear that they will be equally appropriate for other languages, especially freer word order ones, such as German." ></td>
	<td class="line x" title="44:89	The scores given by each metric for the data used in both experiments are presented in Table 1." ></td>
	<td class="line x" title="45:89	For the Experiment A data, we use the Spearman rank correlation coefficient to measure the correlation between the human judgements and the automatic scorers." ></td>
	<td class="line x" title="46:89	The results are presented in Table 2 for both the sentence and the corpus level correlations, we also present p-values for statistical significance." ></td>
	<td class="line x" title="47:89	Since we only have judgements on three systems, the corpus correlation is not that informative." ></td>
	<td class="line o" title="48:89	Interestingly, the ROUGE-L metric is the only one that does not rank the output of the three systems in the same order as the judges." ></td>
	<td class="line o" title="49:89	It ranks the strings chosen by the language model higher than the strings chosen by the log-linear model." ></td>
	<td class="line p" title="50:89	However, at the level of the individual sentence, the ROUGE-L metric correlates best with the human judgements." ></td>
	<td class="line o" title="51:89	The GTM metric correlates at about the same level, but in general there seems to be little difference between the metrics." ></td>
	<td class="line o" title="52:89	For the Experiment B data we use the Pearson correlation coefficient to measure the correlation between the human judgements and the automatic 98 Sentence Correlation P-Value BLEU 0.095 0.5048 ROUGE-L 0.207 0.1417 GTM 0.424 0.0017 SED 0.168 0.2344 WER -0.188 0.1817 TER -0.024 0.8646 Table 3: Correlation between human judgements for experiment B (naturalness scale 15) and automatic metrics metrics." ></td>
	<td class="line x" title="53:89	The results are given in Table 3." ></td>
	<td class="line x" title="54:89	Here we only look at the correlation at the individual sentence level, since we are looking at data from only one system." ></td>
	<td class="line x" title="55:89	For this data, the GTM metric clearly correlates most closely with the human judgements, and it is the only metric that has a statistically significant correlation." ></td>
	<td class="line x" title="56:89	BLEU and TER correlate particularly poorly, with correlation coefficients very close to zero." ></td>
	<td class="line x" title="57:89	3.1 Syntactic Metrics Recently, there has been a move towards more syntactic, rather than purely string based, evaluation of MT output and summarisation (Hovy et al., 2005; Owczarzak et al., 2008)." ></td>
	<td class="line x" title="58:89	The idea is to go beyond simple string comparisons and evaluate at a deeper linguistic level." ></td>
	<td class="line x" title="59:89	Since most of the work in this direction has only been carried out for English so far, we apply the idea rather than a specific tool to the data." ></td>
	<td class="line x" title="60:89	We parse the data from both experiments with a German dependency parser (Hall and Nivre, 2008) trained on the TIGER Treebank (with sentences 8000-10000 heldout for testing)." ></td>
	<td class="line x" title="61:89	This parser achieves 91.23% labelled accuracy on the 2000-sentence test set." ></td>
	<td class="line x" title="62:89	To calculate the correlation between the human judgements and the dependency parser, we parse the original strings as well as the strings chosen by the log-linear and language models." ></td>
	<td class="line x" title="63:89	The standard evaluation procedure relies on both strings being identical to calculate (un-)labelled dependency accuracy, and so we map the dependencies produced by the parser into sets of triples as used in the evaluation software of Crouch et al.(2002) where each dependency is represented as deprel(head,word) and each word is indexed with its position in the original string.3 We compare the parses for both experiments against 3This is a 1-1 mapping, and the indexing ensures that duplicate words in a sentence are not confused." ></td>
	<td class="line x" title="65:89	Experiment A Experiment B corr p-value corr p-value Dependencies -0.640 <0.001 0.186 0.1860 Unweighted Deps -0.657 <0.001 0.290 0.03686 Table 4: Correlation between dependency-based evaluation and human judgements the parses of the original strings." ></td>
	<td class="line x" title="66:89	We calculate both a weighted and unweighted dependency fscore, as given in Table 1." ></td>
	<td class="line x" title="67:89	The unweighted f-score is calculated by taking the average of the scores for each dependency type, while the weighted fscore weighs each average score by its frequency in the test corpus." ></td>
	<td class="line x" title="68:89	We calculate the Spearman and Pearson correlation coefficients as before; the results are given in Table 4." ></td>
	<td class="line x" title="69:89	The results show that the unweighted dependencies correlate more closely (and statistically significantly) with the human judgements than the weighted ones." ></td>
	<td class="line x" title="70:89	This suggests that the frequency of a dependency type does not matter as much as its overall correctness." ></td>
	<td class="line x" title="71:89	4 Discussion The large discrepancy between the absolute correlation coefficients for Experiment A and B can be explained by the fact that they are different tasks." ></td>
	<td class="line x" title="72:89	Experiment A ranks 3 strings relative to one another, while Experiment B measures the naturalness of the string." ></td>
	<td class="line x" title="73:89	We would expect automatic metrics to be better at the first task than the second, as it is easier to rank systems relative to each other than to give a system an absolute score." ></td>
	<td class="line x" title="74:89	Disappointingly, the correlation between the dependency parsing metric and the human judgements was no higher than the simple GTM stringbased metric (although it did outperform all other automatic metrics)." ></td>
	<td class="line n" title="75:89	This does not correspond to related work on English Summarisation evaluation (Owczarzak, 2009) which shows that a metric based on an automatically induced LFG parser for English achieves comparable or higher correlation with human judgements than ROUGE and Basic Elements (BE).4 Parsers of German typically do not achieve as high performance as their English counterparts, and further experiments including alternative parsers are needed to see if we can improve performance of this metric." ></td>
	<td class="line x" title="76:89	The data used in our experiments was almost always grammatically correct." ></td>
	<td class="line x" title="77:89	Therefore the task 4The GTM metric was not compared in that paper 99 of an evaluation system is to score more natural sounding strings higher than marked or unnatural ones." ></td>
	<td class="line x" title="78:89	In this respect, our findings mirror those of Stent et al.(2005) for English data, that the automatic metrics do not correlate well with human judges on syntactic correctness." ></td>
	<td class="line x" title="80:89	5 Conclusions We presented data that examined the correlation between native speaker judgements and automatic evaluation metrics on automatically generated German text." ></td>
	<td class="line p" title="81:89	We found that for our first experiment, all metrics were correlated to roughly the same degree (with ROUGE-L achieving the highest correlation at an individual sentence level and the GTM tool not far behind)." ></td>
	<td class="line o" title="82:89	At a corpus level all except ROUGE were in agreement with the human judgements." ></td>
	<td class="line x" title="83:89	In the second experiment, the General Text Matcher Tool had the strongest correlation." ></td>
	<td class="line x" title="84:89	We carried out an experiment to test whether a more sophisticated syntax-based evaluation metric performed better than the more simple string-based ones." ></td>
	<td class="line x" title="85:89	We found that while the unweighted dependency evaluation metric correlated with the human judgements more strongly than almost all metrics, it did not outperform the GTM tool." ></td>
	<td class="line x" title="86:89	The correlation between the human judgements and the automatic evaluation metrics was much higher for the relative ranking task than for the naturalness task." ></td>
	<td class="line x" title="87:89	Acknowledgments This work was funded by the Collaborative Research Centre (SFB 732) at the University of Stuttgart." ></td>
	<td class="line x" title="88:89	We would like to thank Martin Forst, Alex Fraser and the anonymous reviewers for their helpful feedback." ></td>
	<td class="line x" title="89:89	Furthermore, we would like to thank Johan Hall, Joakim Nivre and Yannick Versely for their help in retraining the MALT dependency parser with our data set." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2027
Query-Focused Summaries or Query-Biased Summaries?
Katragadda, Rahul;Varma, Vasudeva;"></td>
	<td class="line x" title="1:100	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 105108, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:100	c 2009 ACL and AFNLP Query-Focused Summaries or Query-Biased Summaries ? Rahul Katragadda Language Technologies Research Center IIIT Hyderabad rahul k@research.iiit.ac.in Vasudeva Varma Language Technologies Research Center IIIT Hyderabad vv@iiit.ac.in Abstract In the context of the Document Understanding Conferences, the task of Query-Focused Multi-DocumentSummarizationisintendedto improve agreement in content among humangenerated model summaries." ></td>
	<td class="line x" title="3:100	Query-focus also aids the automated summarizers in directing the summary at specific topics, which may result in better agreement with these model summaries." ></td>
	<td class="line x" title="4:100	However, while query focus correlates with performance, we show that highperforming automatic systems produce summaries with disproportionally higher query term density than human summarizers do." ></td>
	<td class="line x" title="5:100	Experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance." ></td>
	<td class="line x" title="6:100	1 Introduction The problem of automatically summarizing text documents has received a lot of attention since the early work by Luhn (Luhn, 1958)." ></td>
	<td class="line x" title="7:100	Most of the current automaticsummarizationsystemsrelyonasentenceextractive paradigm, where key sentences in the original text areselectedtoformthesummarybasedon theclues(or heuristics), or learning based approaches." ></td>
	<td class="line x" title="8:100	Common approaches for identifying key sentences include: training a binary classifier (Kupiec et al., 1995), training a Markov model or CRF (Conroy et al., 2004; Shen et al., 2007) or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (Toutanova et al., 2007)." ></td>
	<td class="line x" title="9:100	But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al.(Nenkova et al., 2006) explored the contribution of frequency based measures." ></td>
	<td class="line x" title="11:100	In this paper, we examine the role a query plays in automated multi-document summarization of newswire." ></td>
	<td class="line x" title="12:100	One of the issues studied since the inception of automatic summarization is that of human agreement: different people choose different content for their summaries (Rath et al., 1961; van Halteren and Teufel, 2003; Nenkova et al., 2007)." ></td>
	<td class="line x" title="13:100	Later, it was assumed (Dang, 2005) that having a question/query to provide focus would improve agreement between any two human-generated model summaries, as well as between a model summary and an automated summary." ></td>
	<td class="line x" title="14:100	Starting in 2005 until 2007, a query-focused multidocument summarization task was conducted as part of the annual Document Understanding Conference." ></td>
	<td class="line x" title="15:100	This task models a real-world complex question answering scenario, where systems need to synthesize from a set of 25 documents, a brief (250 words), well organized fluent answer to an information need." ></td>
	<td class="line x" title="16:100	Query-focused summarization is a topic of ongoing importance within the summarization and question answering communities." ></td>
	<td class="line x" title="17:100	Most of the work in this area has been conducted under the guise of query-focused multi-document summarization, descriptive question answering, or even complex question answering." ></td>
	<td class="line x" title="18:100	In this paper, based on structured empirical evaluations, we show that most of the systems participating in DUCs Query-Focused Multi-Document Summarization (QF-MDS) task have been query-biased in building extractive summaries." ></td>
	<td class="line x" title="19:100	Throughout our discussion, the term query-bias, with respect to a sentence, is precisely defined to mean that the sentence has at least one query term within it." ></td>
	<td class="line x" title="20:100	The term query-focus is less precisely defined, but is related to the cognitive task of focusing a summary on the query, which we assume humans do naturally." ></td>
	<td class="line x" title="21:100	In other words, the human generated model summaries are assumed to be queryfocused." ></td>
	<td class="line x" title="22:100	Here we first discuss query-biased content in Summary Content Units (SCUs) in Section 2 and then in Section 3 by building formal models on query-bias we discuss why/how automated systems are query-biased rather than being query-focused." ></td>
	<td class="line x" title="23:100	2 Query-biased content in Summary Content Units (SCUs) Summary content units, referred as SCUs hereafter, are semanticallymotivatedsubsententialunitsthatarevariable in length but not bigger than a sentential clause." ></td>
	<td class="line x" title="24:100	SCUs are constructed from annotation of a collection of human summaries on a given document collection." ></td>
	<td class="line x" title="25:100	They are identified by noting information that is repeated across summaries." ></td>
	<td class="line x" title="26:100	The repetition is as small as a modifier of a noun phrase or as large as a clause." ></td>
	<td class="line x" title="27:100	The evaluation method that is based on overlapping SCUs in human and automatic summaries is called the 105 Figure 1: SCU annotation of a source document." ></td>
	<td class="line x" title="28:100	pyramid method (Nenkova et al., 2007)." ></td>
	<td class="line x" title="29:100	The University of Ottawa has organized the pyramid annotation data such that for some of the sentences in the original document collection, a list of corresponding content units is known (Copeck et al., 2006)." ></td>
	<td class="line x" title="30:100	A sample of an SCU mapping from topic D0701A of the DUC 2007 QF-MDS corpus is shown in Figure 1." ></td>
	<td class="line x" title="31:100	Three sentences are seen in the figure among which two have been annotated with system IDs and SCU weights wherever applicable." ></td>
	<td class="line x" title="32:100	The first sentence has not been picked by any of the summarizers participating in Pyramid Evaluations, hence it is unknown if the sentence would have contributed to any SCU." ></td>
	<td class="line x" title="33:100	The second sentence was picked by 8 summarizers and that sentence contributed to an SCU of weight 3." ></td>
	<td class="line x" title="34:100	The third sentence in the example was picked by one summarizer, however, it did not contribute to any SCU." ></td>
	<td class="line x" title="35:100	This example shows all the three types of sentences available in the corpus: unknown samples, positive samples and negative samples." ></td>
	<td class="line x" title="36:100	Weextractedthepositiveandnegativesamplesinthe source documents from these annotations; types of second and third sentences shown in Figure 1." ></td>
	<td class="line x" title="37:100	A total of 14.8% sentences were annotated to be either positive or negative." ></td>
	<td class="line x" title="38:100	When we analyzed the positive set, we found that 84.63% sentences in this set were querybiased." ></td>
	<td class="line x" title="39:100	Also, on the negative sample set, we found that 69.12% sentences were query-biased." ></td>
	<td class="line x" title="40:100	That is, on an average, 76.67% of the sentences picked by any automated summarizer are query-biased." ></td>
	<td class="line x" title="41:100	On the other hand, for human summaries only 58% sentences were query-biased." ></td>
	<td class="line x" title="42:100	All the above numbers are based on the DUC 2007 dataset shown in boldface in Table 1 1." ></td>
	<td class="line x" title="43:100	There is one caveat: The annotated sentences come onlyfromthesummariesofsystemsthatparticipatedin the pyramid evaluations." ></td>
	<td class="line x" title="44:100	Since only 13 among a total 32 participating systems were evaluated using pyramid evaluations, the dataset is limited." ></td>
	<td class="line x" title="45:100	However, despite this small issue, it is very clear that at least those systemsthatparticipatedinpyramidevaluationshavebeen biased towards query-terms, or at least, they have been better at correctly identifying important sentences from the query-biased sentences than from query-unbiased sentences." ></td>
	<td class="line x" title="46:100	1We used DUC 2007 dataset for all experiments reported." ></td>
	<td class="line x" title="47:100	3 Formalizing query-bias Our search for a formal method to capture the relation between occurrence of query-biased sentences in the input and in summaries resulted in building binomial and multinomial model distributions." ></td>
	<td class="line x" title="48:100	The distributions estimated were then used to obtain the likelihood of a query-biasedsentencebeingemittedintoasummaryby each system." ></td>
	<td class="line x" title="49:100	For the DUC 2007 data, there were 45 summaries for each of the 32 systems (labeled 1-32) among which 2 were baselines (labeled 1 and 2), and 18 summaries from each of 10 human summarizers (labeled A-J)." ></td>
	<td class="line x" title="50:100	We computed the log-likelihood, log(L[summary;p(Ci)]), of all human and machine summaries from DUC07 query focused multi-document summarization task, based on both distributions described below (see Sections 3.1, 3.2)." ></td>
	<td class="line x" title="51:100	3.1 The binomial model Werepresentthesetofsentencesasabinomialdistribution over type of sentences." ></td>
	<td class="line x" title="52:100	Let C0 and C1 denote the sets of sentences without and with query-bias respectively." ></td>
	<td class="line x" title="53:100	Let p(Ci) be the probability of emitting a sentence from a specified set." ></td>
	<td class="line x" title="54:100	It is also obvious that querybiased sentences will be assigned lower emission probabilities, because the occurrence of query-biased sentences in the input is less likely." ></td>
	<td class="line x" title="55:100	On average each topic has 549 sentences, among which 196 contain a query term; which means only 35.6% sentences in the input were query-biased." ></td>
	<td class="line x" title="56:100	Hence, the likelihood function here denotes the likelihood of a summary to contain non query-biased sentences." ></td>
	<td class="line x" title="57:100	Humans and systems summaries must now constitute low likelihood to show that they rely on query-bias." ></td>
	<td class="line x" title="58:100	The likelihood of a summary then is : L[summary;p(Ci)] = N!n 0!n1!" ></td>
	<td class="line x" title="59:100	p(C0)n0p(C1)n1 (1) Where N is the number of sentences in the summary, and n0 + n1 = N; n0 and n1 are the cardinalities of C0 and C1 in the summary." ></td>
	<td class="line o" title="60:100	Table 2 shows various systems with their ranks based on ROUGE-2 and the average log-likelihood scores." ></td>
	<td class="line pc" title="61:100	The ROUGE (Lin, 2004) suite of metrics are n-gram overlap based metrics that have been shown to highly correlate with human evaluations on content responsiveness." ></td>
	<td class="line o" title="62:100	ROUGE-2 and ROUGE-SU4 are the official ROUGE metrics for evaluating query-focused multi-document summarization task since DUC 2005." ></td>
	<td class="line x" title="63:100	3.2 The multinomial model In the previous section (Section 3.1), we described the binomial model where we classified each sentence as being query-biased or not." ></td>
	<td class="line x" title="64:100	However, if we were to quantify the amount of query-bias in a sentence, we associate each sentence to one among k possible classes leading to a multinomial distribution." ></td>
	<td class="line x" title="65:100	Let Ci  106 Dataset total positive biasedpositive negative biasednegative %biasinpositive %biasinnegative DUC2005 24831 1480 1127 1912 1063 76.15 55.60 DUC2006 14747 1047 902 1407 908 86.15 71.64 DUC2007 12832 924 782 975 674 84.63 69.12 Table 1: Statistical information on counts of query-biased sentences." ></td>
	<td class="line o" title="66:100	ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2 1 31 -1.9842 0.06039 J -3.9465 0.13904 24 4 -5.8451 0.11793 C -2.1387 0.15055 E -3.9485 0.13850 9 12 -5.9049 0.10370 16 32 -2.2906 0.03813 10 28 -4.0723 0.07908 14 14 -5.9860 0.10277 27 30 -2.4012 0.06238 21 22 -4.2460 0.08989 5 23 -6.0464 0.08784 6 29 -2.5536 0.07135 G -4.3143 0.13390 4 3 -6.2347 0.11887 12 25 -2.9415 0.08505 25 27 -4.4542 0.08039 20 6 -6.3923 0.10879 I -3.0196 0.13621 B -4.4655 0.13992 29 2 -6.4076 0.12028 11 24 -3.0495 0.08678 19 26 -4.6785 0.08453 3 9 -7.1720 0.10660 28 16 -3.1932 0.09858 26 21 -4.7658 0.08989 8 11 -7.4125 0.10408 2 18 -3.2058 0.09382 23 7 -5.3418 0.10810 17 15 -7.4458 0.10212 D -3.2357 0.17528 30 10 -5.4039 0.10614 13 5 -7.7504 0.11172 H -3.4494 0.13001 7 8 -5.6291 0.10795 32 17 -8.0117 0.09750 A -3.6481 0.13254 18 19 -5.6397 0.09170 22 13 -8.9843 0.10329 F -3.8316 0.13395 15 1 -5.7938 0.12448 31 20 -9.0806 0.09126 Table 2: Rank, Averaged log-likelihood score based on binomial model, true ROUGE-2 score for the summaries of various systems in DUC07 query-focused multi-document summarization task." ></td>
	<td class="line o" title="67:100	ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2 1 31 -4.6770 0.06039 10 28 -8.5004 0.07908 5 23 -14.3259 0.08784 16 32 -4.7390 0.03813 G -9.5593 0.13390 9 12 -14.4732 0.10370 6 29 -5.4809 0.07135 E -9.6831 0.13850 22 13 -14.8557 0.10329 27 30 -5.5110 0.06238 26 21 -9.7163 0.08989 4 3 -14.9307 0.11887 I -6.7662 0.13621 J -9.8386 0.13904 18 19 -15.0114 0.09170 12 25 -6.8631 0.08505 19 26 -10.3226 0.08453 14 14 -15.4863 0.10277 2 18 -6.9363 0.09382 B -10.4152 0.13992 20 6 -15.8697 0.10879 C -7.2497 0.15055 25 27 -10.7693 0.08039 32 17 -15.9318 0.09750 H -7.6657 0.13001 29 2 -12.7595 0.12028 7 8 -15.9927 0.10795 11 24 -7.8048 0.08678 21 22 -13.1686 0.08989 17 15 -17.3737 0.10212 A -7.8690 0.13254 24 4 -13.2842 0.11793 8 11 -17.4454 0.10408 D -8.0266 0.17528 30 10 -13.3632 0.10614 31 20 -17.5615 0.09126 28 16 -8.0307 0.09858 23 7 -13.7781 0.10810 3 9 -19.0495 0.10660 F -8.2633 0.13395 15 1 -14.2832 0.12448 13 5 -19.3089 0.11172 Table 3: Rank, Averaged log-likelihood score based on multinomial model, true ROUGE-2 score for the summaries of various systems in DUC07 query-focused multi-document summarization task." ></td>
	<td class="line x" title="68:100	{C0,C1,C2,,Ck} denote the k levels of querybias." ></td>
	<td class="line x" title="69:100	Ci is the set of sentences, each having i query terms." ></td>
	<td class="line x" title="70:100	The number of sentences participating in each class varies highly, with C0 bagging a high percentage of sentences (64.4%) and the rest {C1,C2,,Ck} distributing among themselves the rest 35.6% sentences." ></td>
	<td class="line x" title="71:100	Since the distribution is highly-skewed, distinguishing systems based on log-likelihood scores using this model is easier and perhaps more accurate." ></td>
	<td class="line x" title="72:100	Like before, Humans and systems summaries must now constitute low likelihood to show that they rely on querybias.The likelihood of a summary then is : L[summary;p(Ci)] = N!n 0!n1!" ></td>
	<td class="line x" title="73:100	  nk!" ></td>
	<td class="line x" title="74:100	p(C0)n0p(C1)n1   p(Ck)nk (2) Where N is the number of sentences in the summary, and n0 + n1 +  + nk = N; n0, n1,,nk are respectively the cardinalities of C0, C1, ,Ck, in the summary." ></td>
	<td class="line o" title="75:100	Table 3 shows various systems with their ranks based on ROUGE-2 and the average loglikelihood scores." ></td>
	<td class="line o" title="76:100	3.3 Correlation of ROUGE and log-likelihood scores Tables 2 and 3 display log-likelihood scores of various systems in the descending order of log-likelihood scores along with their respective ROUGE-2 scores." ></td>
	<td class="line o" title="77:100	We computed the pearson correlation coefficient () of ROUGE-2 and log-likelihood and ROUGE-SU4 and log-likelihood." ></td>
	<td class="line x" title="78:100	This was computed for systems (ID: 132) (r1) and for humans (ID: A-J) (r2) separately, and for both distributions." ></td>
	<td class="line x" title="79:100	Forthebinomialmodel, r1 =-0.66and r2 =0.39was obtained." ></td>
	<td class="line o" title="80:100	This clearly indicates that there is a strong negative correlation between likelihood of occurrence of a non-query-term and ROUGE-2 score." ></td>
	<td class="line o" title="81:100	That is, a strongpositivecorrelationbetweenlikelihoodofoccur107 rence of a query-term and ROUGE-2 score." ></td>
	<td class="line o" title="82:100	Similarly, for human summarizers there is a weak negative correlation between likelihood of occurrence of a queryterm and ROUGE-2 score." ></td>
	<td class="line o" title="83:100	The same correlation analysis applies to ROUGE-SU4 scores: r1 = -0.66 and r2 = 0.38." ></td>
	<td class="line x" title="84:100	Similar analysis with the multinomial model have been reported in Tables 4 and 5." ></td>
	<td class="line o" title="85:100	Tables 4 and 5 show the correlation among ROUGE-2 and log-likelihood scores for systems2 and humans3." ></td>
	<td class="line o" title="86:100	 ROUGE-2 ROUGE-SU4 binomial -0.66 -0.66 multinomial -0.73 -0.73 Table 4: Correlation of ROUGE measures with loglikelihood scores for automated systems  ROUGE-2 ROUGE-SU4 binomial 0.39 0.38 multinomial 0.15 0.09 Table 5: Correlation of ROUGE measures with loglikelihood scores for humans 4 Conclusions and Discussion Our results underscore the differences between human and machine generated summaries." ></td>
	<td class="line x" title="87:100	Based on Summary Content Unit (SCU) level analysis of query-bias we argue that most systems are better at finding important sentences only from query-biased sentences." ></td>
	<td class="line x" title="88:100	More importantly, we show that on an average, 76.67% of the sentences picked by any automated summarizer are query-biased." ></td>
	<td class="line x" title="89:100	When asked to produce query-focused summaries, humans do not rely to the same extent on the repetition of query terms." ></td>
	<td class="line o" title="90:100	We further confirm based on the likelihood of emitting non query-biased sentence, that there is a strong (negative) correlation among systems likelihood score and ROUGE score, which suggests that systems are trying to improve performance based on ROUGE metrics by being biased towards the query terms." ></td>
	<td class="line x" title="91:100	On the other hand, humans do not rely on query-bias, though we do not have statistically significant evidence to suggest it." ></td>
	<td class="line x" title="92:100	We have also speculated that the multinomial model helps in better capturing the variance across the systems since it distinguishes among query-biased sentences by quantifying the amount of query-bias." ></td>
	<td class="line x" title="93:100	From our point of view, most of the extractive summarization algorithms are formalized based on a bagof-words query model." ></td>
	<td class="line x" title="94:100	The innovation with individual approaches has been in formulating the actual algorithm on top of the query model." ></td>
	<td class="line x" title="95:100	We speculate that 2All the results in Table 4 are statistically significant with p-value (p< 0.00004, N=32) 3None of the results in Table 5 are statistically significant with p-value (p> 0.265, N=10) the real difference in human summarizers and automated summarizers could be in the way a query (or relevance) is represented." ></td>
	<td class="line x" title="96:100	Traditional query models from IR literature have been used in summarization research thus far, and though some previous work (Amini and Usunier, 2007) tries to address this issue using contextual query expansion, new models to represent the query is perhaps one way to induce topic-focus on the summary." ></td>
	<td class="line x" title="97:100	IR-like query models, which are designed to handle short keyword queries, are perhaps not capable of handling an elaborate query in case of summarization." ></td>
	<td class="line x" title="98:100	Since the notion of query-focus is apparently missing in any or all of the algorithms, the future summarization algorithms must try to incorporate this while designing new algorithms." ></td>
	<td class="line x" title="99:100	Acknowledgements We thank Dr Charles L A Clarke at the University of Waterloo for his deep reviews and discussions on earlier versions of the paper." ></td>
	<td class="line x" title="100:100	We are also grateful to all the anonymous reviewers for their valuable comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2066
From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression?
Liu, Fei;Liu, Yang;"></td>
	<td class="line x" title="1:123	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 261264, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:123	c 2009 ACL and AFNLP From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression?" ></td>
	<td class="line x" title="3:123	Fei Liu and Yang Liu Computer Science Department The University of Texas at Dallas Richardson, TX 75080, USA {feiliu, yangl}@hlt.utdallas.edu Abstract Most previous studies on meeting summarization have focused on extractive summarization." ></td>
	<td class="line x" title="4:123	In this paper, we investigate if we can apply sentence compression to extractive summaries to generate abstractive summaries." ></td>
	<td class="line x" title="5:123	We use different compression algorithms, including integer linear programming with an additional step of filler phrase detection, a noisychannel approach using Markovization formulation of grammar rules, as well as human compressed sentences." ></td>
	<td class="line o" title="6:123	Our experiments on the ICSI meeting corpus show that when compared to the abstractive summaries, using sentence compression on the extractive summaries improves their ROUGE scores; however, the best performance is still quite low, suggesting the need of language generation for abstractive summarization." ></td>
	<td class="line x" title="7:123	1 Introduction Meeting summaries provide an efficient way for people to browse through the lengthy recordings." ></td>
	<td class="line x" title="8:123	Most currentresearchonmeetingsummarizationhasfocusedon extractive summarization, that is, it extracts important sentences(ordialogueacts)fromspeechtranscripts, either manual transcripts or automatic speech recognition (ASR) output." ></td>
	<td class="line x" title="9:123	Various approaches to extractive summarization have been evaluated recently." ></td>
	<td class="line x" title="10:123	Popular unsupervised approaches are maximum marginal relevance (MMR), latent semantic analysis (LSA) (Murray et al., 2005a), and integer programming (Gillick et al., 2009)." ></td>
	<td class="line x" title="11:123	Supervised methods include hidden Markov model (HMM), maximum entropy, conditional random fields (CRF), and support vector machines (SVM) (Galley, 2006; Buist et al., 2005; Xie et al., 2008; Maskey and Hirschberg, 2006)." ></td>
	<td class="line x" title="12:123	(Hori et al., 2003) used a word based speech summarization approach that utilized dynamic programming to obtain a set of words to maximize a summarization score." ></td>
	<td class="line x" title="13:123	Most of these summarization approaches aim for selecting the most informative sentences, while less attempt has been made to generate abstractive summaries, or compress the extracted sentences and merge them into a concise summary." ></td>
	<td class="line x" title="14:123	Simply concatenating extracted sentences may not comprise a good summary, especially for spoken documents, since speech transcripts often contain many disfluencies and are redundant." ></td>
	<td class="line x" title="15:123	The following example shows two extractive summary sentences (they are from the same speaker), and part of the abstractive summary that is related to these two extractive summary sentences." ></td>
	<td class="line x" title="16:123	This is an example from the ICSI meeting corpus (see Section 2.1 for more information on the data)." ></td>
	<td class="line x" title="17:123	Extractive summary sentences: Sent1: um we have to refine the tasks more and more which of course we havent done at all so far in order to avoid this rephrasing Sent2: and uh my suggestion is of course we we keep the wizard because i think she did a wonderful job Corresponding abstractive summary: the group decided to hire the wizard and continue with the refinement In this paper, our goal is to answer the question if we can perform sentence compression on an extractive summary to improve its readability and make it more like an abstractive summary." ></td>
	<td class="line x" title="18:123	Compressing sentences could be a first step toward our ultimate goal of creating an abstract for spoken documents." ></td>
	<td class="line x" title="19:123	Sentence compression has been widely studied in language processing." ></td>
	<td class="line x" title="20:123	(Knight and Marcu, 2002; Cohn and Lapata, 2009) learned rewriting rules that indicate which words should be dropped in a given context." ></td>
	<td class="line x" title="21:123	(Knight and Marcu, 2002; Turner and Charniak, 2005) applied the noisy-channel framework to predict the possibilities of translating a sentence to a shorter word sequence." ></td>
	<td class="line x" title="22:123	(Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous contextfree grammar (SCFG) deletion rules." ></td>
	<td class="line x" title="23:123	Unlike these approaches that need a training corpus, (Clarke and Lapata, 2008) encoded the language model and a variety of linguistic constraints as linear inequalities, and employedtheintegerprogrammingapproachtofindasubset of words that maximize an objective function." ></td>
	<td class="line x" title="24:123	Ourfocusinthispaperisnotonnewcompressionalgorithms, but rather on using compression to bridge the gap of extractive and abstractive summarization." ></td>
	<td class="line x" title="25:123	We use different automatic compression algorithms." ></td>
	<td class="line x" title="26:123	The first one is the integer programming (IP) framework, where we also introduce a filler phrase (FP) detection 261 module based on the Web resources." ></td>
	<td class="line x" title="27:123	The second one uses the SCFG that considers the grammaticality of the compressed sentences." ></td>
	<td class="line x" title="28:123	Finally, as a comparison, we also use human compression." ></td>
	<td class="line x" title="29:123	All of these compressed sentences are compared to abstractive summaries." ></td>
	<td class="line o" title="30:123	Our experiments using the ICSI meeting corpus show that compressing extractive summaries can improve human readability and the ROUGE scores against the reference abstractive summaries." ></td>
	<td class="line x" title="31:123	2 Sentence Compression of Extractive Summaries 2.1 Corpus We used the ICSI meeting corpus (Janin et al., 2003), which contains naturally occurring meetings, each about an hour long." ></td>
	<td class="line x" title="32:123	All the meetings have been transcribed and annotated with dialogue acts (DAs), topics, abstractive and extractive summaries (Shriberg et al., 2004; Murray et al., 2005b)." ></td>
	<td class="line x" title="33:123	In this study, we use the extractive and abstractive summaries of 6 meetings from this corpus." ></td>
	<td class="line x" title="34:123	These 6 meetings were chosen because they have been used previously in other related studies, such as summarization and keyword extraction (Murray et al., 2005a)." ></td>
	<td class="line x" title="35:123	On average, an extractive summary contains 76 sentences1 (1252 words), and an abstractive summary contains 5 sentences (111 words)." ></td>
	<td class="line x" title="36:123	2.2 Compression Approaches 2.2.1 Human Compression The data annotation was conducted via Amazon Mechanical Turk2." ></td>
	<td class="line x" title="37:123	Human annotators were asked to generate condensed version for each of the DAs in the extractive summaries." ></td>
	<td class="line x" title="38:123	The compression guideline is similar to (Clarke and Lapata, 2008)." ></td>
	<td class="line x" title="39:123	The annotators were asked to only remove words from the original sentence while preserving most of the important meanings, and make the compressed sentence as grammatical as possible." ></td>
	<td class="line x" title="40:123	The annotators can leave the sentence uncompressed if they think no words need to be deleted; however, they were not allowed to delete the entire sentence." ></td>
	<td class="line x" title="41:123	Since the meeting transcripts are not as readable as other text genres, we may need a better compression guideline for human annotators." ></td>
	<td class="line x" title="42:123	Currently we let the annotators make their own judgment what is an appropriate compression for a spoken sentence." ></td>
	<td class="line x" title="43:123	We split each extractive meeting summary sequentially into groups of 10 sentences, and asked 6 to 10 online workers to compress each group." ></td>
	<td class="line x" title="44:123	Then from these results, another human subject selected the best annotation for each sentence." ></td>
	<td class="line x" title="45:123	We also asked this human judge to select the 4-best compressions." ></td>
	<td class="line x" title="46:123	However, in this study, we only use the 1-best annotation result." ></td>
	<td class="line x" title="47:123	We would like to do more analysis on the 4-best results in the future." ></td>
	<td class="line x" title="48:123	1The extractive units are DAs." ></td>
	<td class="line x" title="49:123	We use DAs and sentences interchangeably in this paper when there is no ambiguity." ></td>
	<td class="line x" title="50:123	2http://www.mturk.com/mturk/welcome 2.2.2 Filler Phrase Detection We define filler phrases (FPs) as the combination of two or more words, which could be discourse markers (e.g., Imean, you know), editing terms, as well as some terms that are commonly used by human but without critical meaning, such as, for example, of course, and sort of." ></td>
	<td class="line x" title="51:123	Removing these fillers barely causes any information loss." ></td>
	<td class="line x" title="52:123	We propose to use web information to automatically generate a list of filler phrases and filter them out in compression." ></td>
	<td class="line x" title="53:123	For each extracted summary sentence of the 6 meetings,weuseitasaquerytoGoogleandexaminethetop N returned snippets (N is 400 in our experiments)." ></td>
	<td class="line x" title="54:123	The snippets may not contain all the words in a sentence query, but often contain frequently occurring phrases." ></td>
	<td class="line x" title="55:123	For example, of course can be found with high frequency in the snippets." ></td>
	<td class="line x" title="56:123	We collect all the phrases that appearinboththeextractedsummarysentencesandthe snippets with a frequency higher than three." ></td>
	<td class="line x" title="57:123	Then we calculate the inverse sentence frequency (ISF) for these phrases using the entire ICSI meeting corpus." ></td>
	<td class="line x" title="58:123	The ISF score of a phrase i is: isfi = NN i where N is the total number of sentences and Ni is the number of sentences containing this phrase." ></td>
	<td class="line x" title="59:123	Phrases with low ISF scores mean that they appear in many occasions and are not domainor topic-indicative." ></td>
	<td class="line x" title="60:123	These are the filler phrases we want to remove to compress a sentence." ></td>
	<td class="line x" title="61:123	The three phrases we found with the lowest ISF scores are you know, i mean and i think, consistent with our intuition." ></td>
	<td class="line x" title="62:123	We also noticed that not all the phrases with low ISF scores can be taken as FPs (we are would be a counter example)." ></td>
	<td class="line x" title="63:123	We therefore gave the ranked list of FPs (based on ISF values) to a human subject to select the proper ones." ></td>
	<td class="line x" title="64:123	The human annotator crossed out the phrases that may not be removable for sentence compression, and also generated simple rules to shorten some phrases (such as turning a little bit into a bit)." ></td>
	<td class="line x" title="65:123	This resulted in 50 final FPs and about a hundred simplification rules." ></td>
	<td class="line x" title="66:123	Examples of the final FPs are: you know, and I think, some of, I mean, so far, it seems like, more or less, of course, sort of, so forth, I guess, for example." ></td>
	<td class="line x" title="67:123	When using this list of FPs and rules for sentence compression, we also require that an FP candidate in the sentence is considered as a phrase in the returned snippets by the search engine, and its frequency in the snippets is higher than a pre-defined threshold." ></td>
	<td class="line x" title="68:123	2.2.3 Compression Using Integer Programming We employ the integer programming (IP) approach in the same way as (Clarke and Lapata, 2008)." ></td>
	<td class="line x" title="69:123	Given an utterance S = w1,w2,,wn, the IP approach forms a compression of this utterance only by dropping words and preserving the word sequence that maximizes an objective function, defined as the sum of the signifi262 cance scores of the consisting words and n-gram probabilities from a language model: max   nsummationtext i=1 yi  Sig(wi) + (1  )  n2summationtext i=0 n1summationtext j=i+1 nsummationtext k=j+1 xijk  P(wk|wi,wj) where yi and xijk are two binary variables: yi = 1 represents that word wi is in the compressed sentence; xijk = 1 represents that the sequence wi, wj , wk is in the compressed sentence." ></td>
	<td class="line x" title="70:123	A trade-off parameter  is used to balance the contribution from the significance scores for individual words and the language model scores." ></td>
	<td class="line x" title="71:123	Because of space limitation, we omitted the special sentence beginning and ending symbols in the formula above." ></td>
	<td class="line x" title="72:123	More details can be found in (Clarke and Lapata, 2008)." ></td>
	<td class="line x" title="73:123	We only used linear constraints defined on the variables, without any linguistic constraints." ></td>
	<td class="line x" title="74:123	We use the lp solve toolkit.3 The significance score for each word is its TF-IDF value (term frequency  inverse document frequency)." ></td>
	<td class="line x" title="75:123	We trained a language model using SRILM 4 on broadcast news data to generate the trigram probabilities." ></td>
	<td class="line x" title="76:123	We empirically set  as 0.7, which gives more weight to the word significance scores." ></td>
	<td class="line x" title="77:123	This IP compression method is applied to the sentences after filler phrases (FPs) are filtered out." ></td>
	<td class="line x" title="78:123	We refer to the output from this approach as FP + IP." ></td>
	<td class="line x" title="79:123	2.2.4 Compression Using Lexicalized Markov Grammars The last sentence compression method we use is the lexicalized Markov grammar-based approach (Galley and McKeown, 2007) with edit word detection (Charniak and Johnson, 2001)." ></td>
	<td class="line x" title="80:123	Two outputs were generated using this method with different compression rates (defined as the number of words preserved in the compression divided by the total number of words in the original sentence).5 We name them Markov (S1) and Markov (S2) respectively." ></td>
	<td class="line x" title="81:123	3 Experiments First we perform human evaluation for the compressed sentences." ></td>
	<td class="line x" title="82:123	Again we use the Amazon Mechanical Turk for the subjective evaluation process." ></td>
	<td class="line x" title="83:123	For each extractive summary sentence, we asked 10 human subjects to rate the compressed sentences from the three systems, as well as the human compression." ></td>
	<td class="line x" title="84:123	This evaluation was conducted on three meetings, containing 244 sentences in total." ></td>
	<td class="line x" title="85:123	Participants were asked to read the original sentence and assign scores to each of the compressed sentences for its informativeness and grammaticality respectively using a 1 to 5 scale." ></td>
	<td class="line x" title="86:123	An overall score is calculated as the average of the informativeness and grammaticality scores." ></td>
	<td class="line x" title="87:123	Results are shown in Table 1." ></td>
	<td class="line x" title="88:123	3http://www.geocities.com/lpsolve 4http://www.speech.sri.com/projects/srilm/ 5Thanks to Michel Galley to help generate these output." ></td>
	<td class="line oc" title="89:123	For a comparison, we also include the ROUGE-1 Fscores (Lin, 2004) of each system output against the human compressed sentences." ></td>
	<td class="line x" title="90:123	Approach Info." ></td>
	<td class="line x" title="91:123	Gram." ></td>
	<td class="line x" title="92:123	Overall R-1 F (%) Human 4.35 4.38 4.37 Markov (S1) 3.64 3.79 3.72 88.76 Markov (S2) 2.89 2.76 2.83 62.99 FP + IP 3.70 3.95 3.82 85.83 Table 1: Human evaluation results." ></td>
	<td class="line o" title="93:123	Also shown is the ROUGE-1 (unigram match) F-score of different systems compared to human compression." ></td>
	<td class="line x" title="94:123	We can see from the table that as expected, the human compression yields the best performance on both informativeness and grammaticality." ></td>
	<td class="line x" title="95:123	FP + IP and Markov (S1) approaches also achieve satisfying performance under both evaluation metrics." ></td>
	<td class="line x" title="96:123	The relatively low scores for Markov (S2) output are partly due to its low compression rate (see Table 2 for the length information)." ></td>
	<td class="line x" title="97:123	As an example, we show below the compressed sentences from human and systems for the first sentence in the example in Sec 1." ></td>
	<td class="line x" title="98:123	Human: we have to refine the tasks in order to avoid rephrasing Markov (S1): we have to refine the tasks more and more which we havent done in order to avoid this rephrasing Markov (S2): we have to refine the tasks which we havent done order to avoid this rephrasing FP + IP: we have to refine the tasks more and more which we havent done to avoid this rephrasing Since our goal is to answer the question if we can use sentence compression to generate abstractive summaries, we compare the compressed summaries, as well as the original extractive summaries, against the reference abstractive summaries." ></td>
	<td class="line o" title="99:123	The ROUGE-1 results along with the word compression ratio for each compression approach are shown in Table 2." ></td>
	<td class="line o" title="100:123	We can see that all of the compression algorithms yield better ROUGE score than the original extractive summaries." ></td>
	<td class="line x" title="101:123	Take Markov (S2) as an example." ></td>
	<td class="line x" title="102:123	The recall rate dropped only 8% (from the original 66% to 58%) when only 53% words in the extractive summaries are preserved." ></td>
	<td class="line x" title="103:123	This demonstrates that it is possible for the current sentence compression systems to greatly condense the extractive summaries while preserving the desirable information, and thus yield summaries that are more like abstractive summaries." ></td>
	<td class="line x" title="104:123	However, since theabstractivesummariesaremuchshorterthantheextractive summaries (even after compression), it is not surprising to see the low precision results as shown in Table 2." ></td>
	<td class="line o" title="105:123	We also observe some different patterns between the ROUGE scores and the human evaluation results in Table 1." ></td>
	<td class="line o" title="106:123	For example, Markov (S2) has the highest ROUGE result, but worse human evaluation score than other methods." ></td>
	<td class="line x" title="107:123	To evaluate the length impact and to further make 263 All Sent." ></td>
	<td class="line x" title="108:123	Top Sent." ></td>
	<td class="line o" title="109:123	Approach Word ratio (%) P(%) R(%) F(%) P(%) R(%) F(%) Original extractive summary 100 7.58 66.06 12.99 29.98 34.29 31.83 Human compression 65.58 10.43 63.00 16.95 34.35 37.39 35.79 Markov (S1) 67.67 10.15 61.98 16.41 34.24 36.88 35.46 Markov (S2) 53.28 11.90 58.14 18.37 32.23 34.96 33.49 FP + IP 76.38 9.11 59.85 14.78 31.82 35.62 33.57 Table 2: Compression ratio of different systems and ROUGE-1 scores compared to human abstractive summaries." ></td>
	<td class="line o" title="110:123	the extractive summaries more like abstractive summaries, we conduct an oracle experiment: we compute the ROUGE score for each of the extractive summary sentences (the original sentence or the compressed sentence) against the abstract, and select the sentences with the highest scores until the number of selected words is about the same as that in the abstract.6 The ROUGE results using these selected top sentences are shown in the right part of Table 2." ></td>
	<td class="line x" title="111:123	There is some difference using all the sentences vs. the top sentences regarding the ranking of different compression algorithms (comparing the two blocks in Table 2)." ></td>
	<td class="line x" title="112:123	From Table 2, we notice significant performance improvement when using the selected sentences to form a summary." ></td>
	<td class="line x" title="113:123	These results indicate that, it may be possible to convert extractive summaries to abstractive summaries." ></td>
	<td class="line x" title="114:123	On the other hand, this is an oracle result since wecomparetheextractivesummariestotheabstractfor sentence selection." ></td>
	<td class="line x" title="115:123	In the real scenario, we will need other methods to rank sentences." ></td>
	<td class="line o" title="116:123	Moreover, the current ROUGE score is not very high." ></td>
	<td class="line x" title="117:123	This suggests that there is a limit using extractive summarization and sentence compression to form abstractive summaries, and that sophisticated language generation is still needed." ></td>
	<td class="line x" title="118:123	4 Conclusion In this paper, we attempt to bridge the gap between extractive and abstractive summaries by performing sentence compression." ></td>
	<td class="line x" title="119:123	Several compression approaches are employed, including an integer programming based framework, wherewealsointroducedafillerphrasedetection module, the lexicalized Markov grammar-based approach, as well as human compression." ></td>
	<td class="line x" title="120:123	Results show that, while sentence compression provides a promising way of moving from extractive summaries toward abstracts, there is also a potential limit along this direction." ></td>
	<td class="line x" title="121:123	This study uses human annotated extractive summaries." ></td>
	<td class="line x" title="122:123	Inourfuturework, wewillevaluateusingautomatic extractive summaries." ></td>
	<td class="line x" title="123:123	Furthermore, we will explore the possibility of merging compressed extractive sentences to generate more unified summaries." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2083
Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering?
Chali, Yllias;Hasan, Sadid;Joty, Shafiq R.;"></td>
	<td class="line x" title="1:93	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 329332, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:93	c 2009 ACL and AFNLP Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering?" ></td>
	<td class="line x" title="3:93	Yllias Chali University of Lethbridge Lethbridge, AB, Canada chali@cs.uleth.ca Sadid A. Hasan University of Lethbridge Lethbridge, AB, Canada hasan@cs.uleth.ca Shafiq R. Joty University of British Columbia Vancouver, BC, Canada rjoty@cs.ubc.ca Abstract In this paper, we analyze the impact of different automatic annotation methods on the performance of supervised approaches to the complex question answering problem (defined in the DUC-2007 main task)." ></td>
	<td class="line x" title="4:93	Huge amount of annotated or labeled data is a prerequisite for supervised training." ></td>
	<td class="line x" title="5:93	The task of labeling can be accomplished either by humans or by computer programs." ></td>
	<td class="line x" title="6:93	When humans are employed, the whole process becomes time consuming and expensive." ></td>
	<td class="line x" title="7:93	So, in order to produce a large set of labeled data we prefer the automatic annotation strategy." ></td>
	<td class="line x" title="8:93	We apply five different automatic annotation techniques to produce labeled data using ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel (ESSK)." ></td>
	<td class="line x" title="9:93	The representative supervised methods we use are Support Vector Machines (SVM), Conditional Random Fields (CRF), Hidden Markov Models (HMM), and Maximum Entropy (MaxEnt)." ></td>
	<td class="line x" title="10:93	Evaluation results are presented to show the impact." ></td>
	<td class="line x" title="11:93	1 Introduction In this paper, we consider the complex question answering problem defined in the DUC-2007 main task1." ></td>
	<td class="line x" title="12:93	We focus on an extractive approach of summarization to answer complex questions where a subset of the sentences in the original documents are chosen." ></td>
	<td class="line x" title="13:93	For supervised learning methods, huge amount of annotated or labeled data sets are obviously required as a precondition." ></td>
	<td class="line x" title="14:93	The decision as to whether a sentence is important enough 1http://www-nlpir.nist.gov/projects/duc/duc2007/ to be annotated can be taken either by humans or by computer programs." ></td>
	<td class="line x" title="15:93	When humans are employed in the process, producing such a large labeled corpora becomes time consuming and expensive." ></td>
	<td class="line x" title="16:93	There comes the necessity of using automatic methods to align sentences with the intention to build extracts from abstracts." ></td>
	<td class="line o" title="17:93	In this paper, we use ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel (ESSK) to automatically label the corpora of sentences (DUC-2006 data) into extract summary or non-summary categories in correspondence with the document abstracts." ></td>
	<td class="line x" title="18:93	We feed these 5 types of labeled data into the learners of each of the supervised approaches: SVM, CRF, HMM, and MaxEnt." ></td>
	<td class="line x" title="19:93	Then we extensively investigate the performance of the classifiers to label unseen sentences (from 25 topics of DUC-2007 data set) as summary or non-summary sentence." ></td>
	<td class="line x" title="20:93	The experimental results clearly show the impact of different automatic annotation methods on the performance of the candidate supervised techniques." ></td>
	<td class="line oc" title="21:93	2 Automatic Annotation Schemes Using ROUGE Similarity Measures ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is an automatic tool to determine the quality of a summary using a collection of measures ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W and ROUGE-S which count the number of overlapping units such as n-gram, word-sequences, and word-pairs between the extract and the abstract summaries (Lin, 2004)." ></td>
	<td class="line o" title="22:93	We assume each individual document sentence as the extract summary and calculate its ROUGE similarity scores with the corresponding abstract summaries." ></td>
	<td class="line o" title="23:93	Thus an average ROUGE score is assigned to each sentence in the document." ></td>
	<td class="line x" title="24:93	We choose the top N sentences based on ROUGE scores to have the label 329 +1 (summary sentences) and the rest to have the label1 (non-summary sentences)." ></td>
	<td class="line x" title="25:93	Basic Element (BE) Overlap Measure We extract BEs, the head-modifier-relation triples for the sentences in the document collection using BE package 1.0 distributed by ISI 2." ></td>
	<td class="line x" title="26:93	The ranked list of BEs sorted according to their Likelihood Ratio (LR) scores contains important BEs at the top which may or may not be relevant to the abstract summary sentences." ></td>
	<td class="line x" title="27:93	We filter those BEs by checking possible matches with an abstract sentence word or a related word." ></td>
	<td class="line x" title="28:93	For each abstract sentence, we assign a score to every document sentence as the sum of its filtered BE scores divided by the number of BEs in the sentence." ></td>
	<td class="line x" title="29:93	Thus, every abstract sentence contributes to the BE score of each document sentence and we select the top N sentences based on average BE scores to have the label +1 and the rest to have the label1." ></td>
	<td class="line x" title="30:93	Syntactic Similarity Measure In order to calculate the syntactic similarity between the abstract sentence and the document sentence, we first parse the corresponding sentences into syntactic trees using Charniak parser 3 (Charniak, 1999) and then we calculate the similarity between the two trees using the tree kernel (Collins and Duffy, 2001)." ></td>
	<td class="line x" title="31:93	We convert each parenthesis representation generated by Charniak parser to its corresponding tree and give the trees as input to the tree kernel functions for measuring the syntactic similarity." ></td>
	<td class="line x" title="32:93	The tree kernel of two syntactic trees T1 and T2 is actually the inner product of the two m-dimensional vectors, v(T1) and v(T2): TK(T1,T2) = v(T1).v(T2) The TK (tree kernel) function gives the similarity score between the abstract sentence and the document sentence based on the syntactic structure." ></td>
	<td class="line x" title="33:93	Each abstract sentence contributes a score to the document sentences and the top N sentences are selected to be annotated as +1 and the rest as 1 based on the average of similarity scores." ></td>
	<td class="line x" title="34:93	Semantic Similarity Measure Shallow semantic representations, bearing a more compact information, can prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007)." ></td>
	<td class="line x" title="35:93	To experiment with semantic structures, we parse the corresponding 2BE website:http://www.isi.edu/ cyl/BE 3available at ftp://ftp.cs.brown.edu/pub/nlparser/ sentences semantically using a Semantic Role Labeling (SRL) system like ASSERT4." ></td>
	<td class="line x" title="36:93	ASSERT is an automatic statistical semantic role tagger, that can annotate naturally occuring text with semantic arguments." ></td>
	<td class="line x" title="37:93	We represent the annotated sentences using tree structures called semantic trees (ST)." ></td>
	<td class="line x" title="38:93	Thus, by calculating the similarity between STs, each document sentence gets a semantic similarity score corresponding to each abstract sentence and then the topN sentences are selected to be labeled as +1 and the rest as 1 on the basis of average similarity scores." ></td>
	<td class="line x" title="39:93	Extended String Subsequence Kernel (ESSK) Formally, ESSK is defined as follows (Hirao et al., 2004): Kessk(T,U) = dsummationdisplay m=1 summationdisplay tiT summationdisplay ujU Km(ti,uj) Km(ti,uj) = braceleftbigg val(t i,uj) if m = 1 Kprimem1(ti,uj)  val(ti,uj) Here, Kprimem(ti,uj) is defined below." ></td>
	<td class="line x" title="40:93	ti and uj are the nodes of T and U, respectively." ></td>
	<td class="line x" title="41:93	Each node includes a word and its disambiguated sense." ></td>
	<td class="line x" title="42:93	The function val(t,u) returns the number of attributes common to the given nodes t and u. Kprimem(ti,uj) = braceleftbigg 0 if j = 1 Kprimem(ti,uj1) +Kprimeprimem(ti,uj1) Here  is the decay parameter for the number of skipped words." ></td>
	<td class="line x" title="43:93	We choose  = 0.5 for this research." ></td>
	<td class="line x" title="44:93	Kprimeprimem(ti,uj) is defined as: Kprimeprimem(ti,uj) = braceleftbigg 0 if i = 1 Kprimeprimem(ti1,uj) +Km(ti1,uj) Finally, the similarity measure is defined after normalization as below: simessk(T,U) = Kessk(T,U)radicalbigK essk(T,T)Kessk(U,U) Indeed, this is the similarity score we assign to each document sentence for each abstract sentence and in the end, top N sentences are selected to be annotated as +1 and the rest as 1 based on average similarity scores." ></td>
	<td class="line x" title="45:93	3 Experiments Task Description The problem definition at DUC-2007 was: Given a complex question (topic description) and a collection of relevant documents, the task is to synthesize a fluent, wellorganized 250-word summary of the documents 4available at http://cemantix.org/assert 330 that answers the question(s) in the topic." ></td>
	<td class="line x" title="46:93	We consider this task and use the five automatic annotation methods to label each sentence of the 50 document sets of DUC-2006 to produce five different versions of training data for feeding the SVM, HMM, CRF and MaxEnt learners." ></td>
	<td class="line x" title="47:93	We choose the top 30% sentences (based on the scores assigned by an annotation scheme) of a document set to have the label +1 and the rest to have1." ></td>
	<td class="line x" title="48:93	Unlabeled sentences of 25 document sets of DUC-2007 data are used for the testing purpose." ></td>
	<td class="line x" title="49:93	Feature Space We represent each of the document-sentences as a vector of feature-values." ></td>
	<td class="line x" title="50:93	We extract several query-related features and some other important features from each sentence." ></td>
	<td class="line x" title="51:93	We use the features: n-gram overlap, Longest Common Subsequence (LCS), Weighted LCS (WLCS), skip-bigram, exact word overlap, synonym overlap, hypernym/hyponym overlap, gloss overlap, Basic Element (BE) overlap, syntactic tree similarity measure, position of sentences, length of sentences, Named Entity (NE), cue word match, and title match (Edmundson, 1969)." ></td>
	<td class="line o" title="52:93	Supervised Systems For SVM we use second order polynomial kernel for the ROUGE and ESSK labeled training." ></td>
	<td class="line x" title="53:93	For the BE, syntactic, and semantic labeled training third order polynomial kernel is used." ></td>
	<td class="line x" title="54:93	The use of kernel is based on the accuracy we achieved during training." ></td>
	<td class="line x" title="55:93	We apply 3-fold cross validation with randomized local-grid search for estimating the value of the trade-off parameter C. We try the value of C in 2i following heuristics, where i{5,4,,4,5} and set C as the best performed value 0.125 for second order polynomial kernel and default value is used for third order kernel." ></td>
	<td class="line x" title="56:93	We use SVMlight 5 package for training and testing in this research." ></td>
	<td class="line x" title="57:93	In case of HMM, we apply the Maximum Likelihood Estimation (MLE) technique by frequency counts with add-one smoothing to estimate the three HMM parameters: initial state probabilities, transition probabilities and emission probabilities." ></td>
	<td class="line x" title="58:93	We use Dr. Dekang Lins HMM package6 to generate the most probable label sequence given the model parameters and the observation sequence (unlabeled DUC-2007 test data)." ></td>
	<td class="line x" title="59:93	We use MALLET-0.4 NLP toolkit7 to implement the CRF." ></td>
	<td class="line x" title="60:93	We formu5http://svmlight.joachims.org/ 6http://www.cs.ualberta.ca/ lindek/hmm.htm 7http://mallet.cs.umass.edu/ late our problem in terms of MALLETs SimpleTagger class which is a command line interface to the MALLET CRF class." ></td>
	<td class="line x" title="61:93	We modify the SimpleTagger class in order to include the provision for producing corresponding posterior probabilities of the predicted labels which are used later for ranking sentences." ></td>
	<td class="line x" title="62:93	We build the MaxEnt system using Dr. Dekang Lins MaxEnt package8." ></td>
	<td class="line x" title="63:93	To define the exponential prior of the  values in MaxEnt models, an extra parameter  is used in the package during training." ></td>
	<td class="line x" title="64:93	We keep the value of  as default." ></td>
	<td class="line x" title="65:93	Sentence Selection The proportion of important sentences in the training data will differ from the one in the test data." ></td>
	<td class="line x" title="66:93	A simple strategy is to rank the sentences in a document, then select the top N sentences." ></td>
	<td class="line x" title="67:93	In SVM systems, we use the normalized distance from the hyperplane to each sample to rank the sentences." ></td>
	<td class="line x" title="68:93	Then, we choose N sentences until the summary length (250 words for DUC-2007) is reached." ></td>
	<td class="line x" title="69:93	For HMM systems, we use Maximal Marginal Relevance (MMR) based method to rank the sentences (Carbonell et al., 1997)." ></td>
	<td class="line x" title="70:93	In CRF systems, we generate posterior probabilities corresponding to each predicted label in the label sequence to measure the confidence of each sentence for summary inclusion." ></td>
	<td class="line x" title="71:93	Similarly for MaxEnt, the corresponding probability values of the predicted labels are used to rank the sentences." ></td>
	<td class="line x" title="72:93	Evaluation Results The multiple reference summaries given by DUC-2007 are used in the evaluation of our summary content." ></td>
	<td class="line oc" title="73:93	We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE (Lin, 2004)." ></td>
	<td class="line p" title="74:93	We report the three widely adopted important ROUGE metrics in the results: ROUGE-1 (unigram), ROUGE-2 (bigram) and ROUGE-SU (skip bi-gram)." ></td>
	<td class="line o" title="75:93	Figure 1 shows the ROUGE F-measures for SVM, HMM, CRF and MaxEnt systems." ></td>
	<td class="line o" title="76:93	The X-axis containing ROUGE, BE, Synt (Syntactic), Sem (Semantic), and ESSK stands for the annotation scheme used." ></td>
	<td class="line o" title="77:93	The Y-axis shows the ROUGE1 scores at the top, ROUGE-2 scores at the bottom and ROUGE-SU scores in the middle." ></td>
	<td class="line x" title="78:93	The supervised systems are distinguished by the line style used in the figure." ></td>
	<td class="line o" title="79:93	From the figure, we can see that the ESSK labeled SVM system is having the poorest ROUGE 1 score whereas the Sem labeled system performs 8http://www.cs.ualberta.ca/lindek/downloads.htm 331 Figure 1: ROUGE F-scores for different supervised systems best." ></td>
	<td class="line o" title="80:93	The other annotation methods impact is almost similar here in terms of ROUGE-1." ></td>
	<td class="line o" title="81:93	Analyzing ROUGE-2 scores, we find that the BE performs the best for SVM, on the other hand, Sem achieves top ROUGE-SU score." ></td>
	<td class="line o" title="82:93	As for the two measures Sem annotation is performing the best, we can typically conclude that Sem annotation is the most suitable method for the SVM system." ></td>
	<td class="line o" title="83:93	ESSK works as the best for HMM and Sem labeling performs the worst for all ROUGE scores." ></td>
	<td class="line o" title="84:93	Synt and BE labeled HMMs perform almost similar whereas ROUGE labeled system is pretty close to that of ESSK." ></td>
	<td class="line o" title="85:93	Again, we see that the CRF performs best with the ESSK annotated data in terms of ROUGE -1 and ROUGE-SU scores and Sem has the highest ROUGE-2 score." ></td>
	<td class="line o" title="86:93	But BE and Synt labeling work bad for CRF whereas the ROUGE labeling performs decently." ></td>
	<td class="line x" title="87:93	So, we can typically conclude that ESSK annotation is the best method for the CRF system." ></td>
	<td class="line x" title="88:93	Analyzing further, we find that ESSK works best for MaxEnt and BE labeling is the worst for all ROUGE scores." ></td>
	<td class="line o" title="89:93	We can also see that ROUGE, Synt and Sem labeled MaxEnt systems perform almost similar." ></td>
	<td class="line x" title="90:93	So, from this discussion we can come to a conclusion that SVM system performs best if the training data uses semantic annotation scheme and ESSK works best for HMM, CRF and MaxEnt systems." ></td>
	<td class="line x" title="91:93	4 Conclusion and Future Work In the work reported in this paper, we have performed an extensive experimental evaluation to show the impact of five automatic annotation methods on the performance of different supervised machine learning techniques in confronting the complex question answering problem." ></td>
	<td class="line x" title="92:93	Experimental results show that Sem annotation is the best for SVM whereas ESSK works well for HMM, CRF and MaxEnt systems." ></td>
	<td class="line x" title="93:93	In the near future, we plan to work on finding more sophisticated approaches to effective automatic labeling so that we can experiment with different supervised methods." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1607
Sentence Position revisited: A robust light-weight Update Summarization  baseline Algorithm
Katragadda, Rahul;Pingali, Prasad;Varma, Vasudeva;"></td>
	<td class="line x" title="1:162	Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 4652, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:162	c 2009 Association for Computational Linguistics Sentence Position revisited: A robust light-weight Update Summarization baseline Algorithm Rahul Katragadda rahul k@research.iiit.ac.in Prasad Pingali pvvpr@iiit.ac.in Language Technologies Research Center IIIT Hyderabad Vasudeva Varma vv@iiit.ac.in Abstract In this paper, we describe a sentence position based summarizer that is built based on a sentence position policy, created from the evaluation testbed of recent summarization tasks at Document Understanding Conferences (DUC)." ></td>
	<td class="line x" title="3:162	We show that the summarizer thus built is able to outperform most systems participating in task focused summarization evaluations at Text Analysis Conferences (TAC) 2008." ></td>
	<td class="line x" title="4:162	Our experiments also show that such a method would perform better at producingshortsummaries(upto100words)than longer summaries." ></td>
	<td class="line x" title="5:162	Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task." ></td>
	<td class="line x" title="6:162	1 Introduction Document summarization received a lot of attention since an early work by Luhn (1958)." ></td>
	<td class="line x" title="7:162	Statistical information derived from word frequency and distribution was used by the machine to compute a relative measure of significance, first for individual words and then for sentences." ></td>
	<td class="line x" title="8:162	Later, Edmundson (1969) introduced four clues for identifying significant words (topics) in a text." ></td>
	<td class="line x" title="9:162	Among them title and location are related to position methods, while the other two are presence of cue words and high frequency content words." ></td>
	<td class="line x" title="10:162	Edmundson assigned positive weights to sentences according to their ordinal position in the text, giving more weight to the first sentence in the first paragraph and last sentence in the last paragraph." ></td>
	<td class="line x" title="11:162	Position of a sentence in a document or the position of a word in a sentence give good clues towards importance of the sentence or word respectively." ></td>
	<td class="line x" title="12:162	Such features are called locational features, and a sentence position feature deals with presence of key sentences at specific locations in the text." ></td>
	<td class="line x" title="13:162	Sentence Position has been well studied in summarization research since its inception, early in Edmundsons work (1969)." ></td>
	<td class="line x" title="14:162	Earlier, Baxendale (1958) investigated a sample of 200 paragraphs to determine where the important words are most likely to be found." ></td>
	<td class="line x" title="15:162	He concluded that in 85% of the paragraphs, the first sentence was a topic sentence and in 7% of the paragraphs, the final one." ></td>
	<td class="line x" title="16:162	Recent advances in machine learning have been adaptedtosummarizationproblemthroughtheyears and locational features have been consistently used to identify salience of a sentence." ></td>
	<td class="line x" title="17:162	Some representative work in learning sentence extraction would include training a binary classifier (Kupiec et al., 1995), training a Markov model (Conroy et al., 2004), training a CRF (Shen et al., 2007), and learning pairwise-ranking of sentences (Toutanova et al., 2007)." ></td>
	<td class="line x" title="18:162	In recent years, at the Document Understanding Conferences (DUC1), Text Summarization research evolved through task focused evaluations ranging from generic single-document summarization to query-focused multi-document summarization (QFMDS)." ></td>
	<td class="line x" title="19:162	The QFMDS task models the realworld complex question answering task wherein, given a topic and a set of 25 relevant documents, the 1http://duc.nist.gov/ 46 task is to synthesize a fluent, well-organized 250word summary of the documents that answers the question(s) in the topic statement." ></td>
	<td class="line x" title="20:162	Recent focus in the community has been towards query-focused update-summarization task at DUC and the Text AnalysisConference(TAC2).Theupdatetaskwasto produce short (~100 words) multi-document update summaries of newswire articles under the assumption that the user has already read a set of earlier articles." ></td>
	<td class="line x" title="21:162	The purpose of each update summary will be to inform the reader of new information about a particular topic." ></td>
	<td class="line x" title="22:162	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="23:162	In Section 2, we describe a Sub-optimal Position Policy (SPP) based on Pyramid Annotated Data, then we derive a simple algorithm for summarization based on the SPP in Section 3, and show evaluation results." ></td>
	<td class="line x" title="24:162	Next, in Section 4, we explain the current baselines and evaluation for Multi-Document Summarization and finally in Section 5, we discuss the need for an older baseline in the current context of the short summary task of update summarization." ></td>
	<td class="line x" title="25:162	2 Sub-Optimal Sentence Position Policy Given a large text collection and a way to approximate the relevance for a reasonably large subset of sentences, we could identify significant positional attributes for the genre of the collection." ></td>
	<td class="line x" title="26:162	Our experiments are based on the work described in (Lin and Hovy, 1997), whose experiments using the ZiffDavis corpus gave great insights on the selective power of the position method." ></td>
	<td class="line x" title="27:162	2.1 Sentence Position Yield and Optimal Position Policy (OPP) LinandHovy(1997)provideanempiricalvalidation for the position hypothesis." ></td>
	<td class="line x" title="28:162	They describe a method of deriving an Optimal Position Policy for a collection of texts within a genre, as long as a small set of topic keywords is defined for each text." ></td>
	<td class="line x" title="29:162	They defined sentence yield (strength of relevance) of a sentence based on the mention of topic keywords in the sentence." ></td>
	<td class="line x" title="30:162	The positional yield is defined as the average sentence yield for that position in the document." ></td>
	<td class="line x" title="31:162	They 2http://www.nist.gov/tac/ computed the yield of each sentence position in each document by counting the number of different keywords contained in the respective sentence in each document, and averaging over all documents." ></td>
	<td class="line x" title="32:162	An Optimal Position Policy (OPP) is derived based on the decreasing values of positional yield." ></td>
	<td class="line x" title="33:162	Their experiments grounded on the assumption that abstract is an ideal representation of central topic(s) of a text." ></td>
	<td class="line x" title="34:162	For their evaluations, they used the abstract to compare whether the sentences found based on their Optimal Position Policy are indeed a good selection." ></td>
	<td class="line x" title="35:162	They used precision-recall measures to establish those findings." ></td>
	<td class="line x" title="36:162	At our disposal we had data from pyramid evaluations that provided sentences and their mapping to any content units in the gold standard summaries." ></td>
	<td class="line x" title="37:162	The annotations in the data provide a unique property that each sentence can derive for itself a score for relevance." ></td>
	<td class="line x" title="38:162	2.2 Documents There are a wide variety of document types across genre." ></td>
	<td class="line x" title="39:162	In our case of newswire collection we have identified two primary types of documents: small document and large document." ></td>
	<td class="line x" title="40:162	This distinction is made based on the total sentences in the document." ></td>
	<td class="line x" title="41:162	All documents that have the number of sentences above a threshold should be considered large." ></td>
	<td class="line x" title="42:162	We experimented on thresholds varying from 10 to 35 sentences and figured out that documents distribution into the two categories was acceptable when threshold-ed at 20 sentences." ></td>
	<td class="line x" title="43:162	This decision is also well supported by the fact that the last sentences of a document were more important than the others in the middle (Baxendale, 1958)." ></td>
	<td class="line x" title="44:162	Sentence Position Yield (SPY) is obtained separately for both types of documents." ></td>
	<td class="line x" title="45:162	For a small document, sentence positions have values from 1 through 20." ></td>
	<td class="line x" title="46:162	Meanwhile, for a large document we compute SPY for position 1 through 20, then the last 15sentenceslabeled136through150andany other sentence is labeled 100." ></td>
	<td class="line x" title="47:162	It can be seen in figure 3 thatsentencesthatdonotcomefromleadingortrailing part of large documents do not contribute much content to the summaries." ></td>
	<td class="line x" title="48:162	47 Figure 1: A sample mapping of SCU annotation to source document sentences." ></td>
	<td class="line x" title="49:162	An excerpt from mapping of topic D0701A of DUC 2007 QF-MDS task." ></td>
	<td class="line x" title="50:162	Figure 2: Sentence Position Yield for small documents." ></td>
	<td class="line x" title="51:162	2.3 Pyramid Data Summary content units, referred as SCUs hereafter, are semantically motivated, sub-sentential units that are variable in length but not bigger than a sentential clause." ></td>
	<td class="line x" title="52:162	SCUs emerge from annotation of a collection of human summaries for the same input." ></td>
	<td class="line x" title="53:162	They are identified by noting information that is repeated across summaries, whether the repetition is as small as a modifier of a noun phrase or as large as a clause." ></td>
	<td class="line x" title="54:162	The weight an SCU obtains is directly proportional to the number of reference summaries that support that piece of information." ></td>
	<td class="line x" title="55:162	The evaluation method that is based on overlapping SCUs in human and automatic summaries is described in the Pyramid method (Nenkova et al., 2007)." ></td>
	<td class="line x" title="56:162	The University of Ottawa has organized the pyramid annotation data such that for some of the sentences in the original document collection (those that were picked by systems participating in pyramidevaluation), alistofcorrespondingcontentunits is known (Copeck et al., 2006)." ></td>
	<td class="line x" title="57:162	We used this data to identify locations in a document from where most sentences were being picked, and which of those locations were being most content responsive to the query." ></td>
	<td class="line x" title="58:162	A sample of SCU mapping is shown in figure 1." ></td>
	<td class="line x" title="59:162	Three sentences are seen in the figure among which two have been annotated with system IDs and SCU weights wherever applicable." ></td>
	<td class="line x" title="60:162	The first sentence has not been picked by any of the summarizers participating in Pyramid Evaluations, hence it is unknown if the sentence would have contributed to any SCU." ></td>
	<td class="line x" title="61:162	The second sentence was picked by 8 summarizers and that sentence contributed to an SCU of weight 3." ></td>
	<td class="line x" title="62:162	The third sentence in the example was picked by one summarizer, however, it did not contribute to any SCU." ></td>
	<td class="line x" title="63:162	This example shows all the three types of sentences available in the corpus: unknown samples, positive samples and negative samples." ></td>
	<td class="line x" title="64:162	For each SCU, a weight is associated in pyramid annotations." ></td>
	<td class="line x" title="65:162	Thus a sentential score could be defined as sum of weights of all the contributing SCUs of the sentence." ></td>
	<td class="line x" title="66:162	For an unknown sample and a negative sample, sentential score is 0." ></td>
	<td class="line x" title="67:162	For example, in the second sentence in figure 1 the score is 3, contributedbyasingleSCU.Whilethesameforthefirst and third sentences is 0." ></td>
	<td class="line x" title="68:162	For each sentence position the sentential score is averaged over all documents, which we call Sentence Position Yield." ></td>
	<td class="line x" title="69:162	SPY for small and large documents is shown in figures 2 and 3." ></td>
	<td class="line x" title="70:162	Based on these values for various positions, a simple Position Pol48 Figure 3: Sentence Position Yield for large documents icy was framed as shown below." ></td>
	<td class="line x" title="71:162	A position policy is an ordered set consisting of elements in the order of most importance." ></td>
	<td class="line x" title="72:162	Within a subset, each sub-element is equally important and treated likewise." ></td>
	<td class="line x" title="73:162	{s1,S1,{s2,S2,s3},{S3,s4,s5,s6,s7,s8,s20}, {S4,s9}} In the above position policy, sentences from small documents and large documents are represented by si and Sj respectively." ></td>
	<td class="line x" title="74:162	The position policy described above provides an ordering of ranked sentence positions based on a very accurate relevance annotations on sentences." ></td>
	<td class="line x" title="75:162	However, there is a large subset of sentences that are not annotated with either positive or negative relevance judgment." ></td>
	<td class="line x" title="76:162	Hence, the policy derived is based on a high-precision low-recall corpus3 for sentence relevance." ></td>
	<td class="line x" title="77:162	If all the sentences were annotated with such judgements, the policy could have been different." ></td>
	<td class="line x" title="78:162	For this reason we call the above derived policy, a Sub-optimal Position Policy (SPP)." ></td>
	<td class="line x" title="79:162	3 SPP as an algorithm Thegoalofcreatinga positionpolicywastoidentify its effectiveness as a summarization algorithm." ></td>
	<td class="line x" title="80:162	The 3DUC 2005 and 2006 data has been used for learning the SPP." ></td>
	<td class="line x" title="81:162	In further experiments in section 3, DUC 2007 and TAC 2008 data have been used as test data." ></td>
	<td class="line x" title="82:162	above simple heuristic was easily incorporated as an algorithm based on simple scoring for each distinct set in the policy." ></td>
	<td class="line x" title="83:162	For instance, based on the policy above, all s1 get the highest weight followed by next best weight to all S1 and so on." ></td>
	<td class="line x" title="84:162	As it can be observed, only the first sentence of each document could end up comprising the summary." ></td>
	<td class="line x" title="85:162	This is okay, till we dont get redundant information in the summary." ></td>
	<td class="line x" title="86:162	Hence we also used a simple unigram match based redundancy measure that doesnt allow a sentence if it matches any of the already selected sentences in at least 40% of content words in it." ></td>
	<td class="line x" title="87:162	We also dis-allow sentences greater than 25 content words." ></td>
	<td class="line x" title="88:162	Weapplied theabove algorithmto generatemultidocument summaries for various tasks." ></td>
	<td class="line x" title="89:162	We have applied it to Query-Focused Multi-Document Summarization (QF-MDS) task of DUC 2007 and QueryFocused Update Summarization task of TAC 2008." ></td>
	<td class="line x" title="90:162	3.1 Query-Focused Multi-Document Summarization The query-focused multi-document summarization task at DUC models the real world complex question answering task." ></td>
	<td class="line x" title="91:162	Given a topic and a set of 25 relevant documents, this task is to synthesize a fluent, well-organized 250 word summary of the documents that answers the question(s) in the topic state49 ment/narration." ></td>
	<td class="line oc" title="92:162	The summaries from the above algorithm for the QF-MDS were evaluated based on ROUGE metrics (Lin, 2004)." ></td>
	<td class="line o" title="93:162	The average4 recall scores are reported for ROUGE-2 and ROUGE-SU4 in Table 1." ></td>
	<td class="line x" title="94:162	Also reported are the performance of the top performing system and the official baseline(s)." ></td>
	<td class="line x" title="95:162	This algorithm performed worse than most systems participating in the task that year and performed better5 thanonlythefirstxwordsbaselineand3othersystems." ></td>
	<td class="line o" title="96:162	system ROUGE-2 ROUGE-SU4 first x words baseline 0.06039 0.10507 generic baseline 0.09382 0.14641 SPP algorithm 0.06913 0.12492 system 15 (top system) 0.12448 0.17711 Table 1: ROUGE 2, SU4 Recall scores for two baselines, the SPP algorithm and a top performing system at Query-Focused Multi-Document Summarization task, DUC 2007." ></td>
	<td class="line x" title="97:162	3.2 Update Summarization Task The update summarization task is to produce short (~100 words) multi-document update summaries of newswire articles under the assumption that the user has already read a set of earlier articles." ></td>
	<td class="line x" title="98:162	The initial document set is called cluster A and the next set of articles are called cluster B. For cluster A, a queryfocused multi-document summary is expected." ></td>
	<td class="line x" title="99:162	The purpose of each update summary (summary of cluster B) will be to inform the reader of new information about a particular topic." ></td>
	<td class="line o" title="100:162	Summaries from the above algorithm for the Query Focused Update Summarization task were evaluated based on ROUGE metrics." ></td>
	<td class="line x" title="101:162	This algorithm performed surprisingly better at this task when compared to QF-MDS." ></td>
	<td class="line o" title="102:162	The rouge scores suggest that this algorithm is well above the median for cluster A and among the top 5 systems for cluster B. It must be noted that consistent performance across clusters (both A and B) shows the robustness of the SPP algorithm at the update summarization task." ></td>
	<td class="line x" title="103:162	Also, it is evident that such an algorithm is computationally simple and light-weight." ></td>
	<td class="line x" title="104:162	4Averaged over all the 45 topics of DUC 2007 dataset." ></td>
	<td class="line x" title="105:162	5Better in a statistical sense, based on 95% confidence intervals of the two systems evaluation based on ROUGE-2." ></td>
	<td class="line o" title="106:162	These surprisingly high scores on ROUGE metrics prompted us to evaluate the summaries based on PyramidEvaluation(Nenkovaetal.,2007)." ></td>
	<td class="line x" title="107:162	Pyramid evaluation provides a more semantic approach to evaluation of content based on SCUs as discussed in Section 2.3." ></td>
	<td class="line x" title="108:162	The average6 modified pyramid scores of cluster A and cluster B summaries is shown in Table 2, along with the average recall scores for ROUGE-2, ROUGE-SU4 scores." ></td>
	<td class="line x" title="109:162	The pyramid evaluation7 suggests that this algorithm performs better than all other automated systems at TAC 2008." ></td>
	<td class="line x" title="110:162	Table 3 shows the average performance (across clusters) of first x words baseline, SPP algorithm and two top performing systems (System ID=43 and ID=11)." ></td>
	<td class="line o" title="111:162	System 43 was adjudged best system based on ROUGE metrics, and system 11 was top performer based on pyramid evaluations at TAC 2008." ></td>
	<td class="line o" title="112:162	ROUGE-2 ROUGE-SU4 pyramid cluster A 0.08987 0.1213 0.3432 cluster B 0.09319 0.1283 0.3576 Table 2: Cluster wise ROUGE 2, SU4 Recall scores and modifiedPyramidScoresforSPPalgorithmattheUpdate Summarization task." ></td>
	<td class="line x" title="113:162	3.3 Discussion It is interesting to observe that the algorithm that performs very poorly at QF-MDS, does very well in the Update Summarization task." ></td>
	<td class="line x" title="114:162	A possible explanation for such behavior could be based on summary length." ></td>
	<td class="line x" title="115:162	For a 250 word summary in the QFMDS task, human summaries might provide a descriptive answer to the query that includes information nuggets accompanied by background information." ></td>
	<td class="line x" title="116:162	Indeed, ithasbeenearlierreportedthathumans appreciate receiving more information than just the answer to the query, whenever possible (Lin et al., 2003; Bosma, 2005)." ></td>
	<td class="line x" title="117:162	Whereas, in the case of Update Summarization task the summary length is only 100 words." ></td>
	<td class="line x" title="118:162	In such a short length humans need to trade-off between answer sentences and supporting sentences, and usually answers are preferred." ></td>
	<td class="line x" title="119:162	And since our method 6Averaged over all the 48 topics of TAC 2008 dataset." ></td>
	<td class="line x" title="120:162	7Pyramid Annotation were done by a volunteer who also volunteered for annotations during DUC 2007." ></td>
	<td class="line o" title="121:162	50 system ROUGE-2 ROUGE-SU4 pyramid first x words baseline 0.05896 0.09327 0.166 SPP algorithm 0.09153 0.1245 0.3504 System 43 (top in ROUGE) 0.10395 0.13646 0.289 System 11 (top in pyramid) 0.08858 0.12484 0.336 Table 3: Average ROUGE 2, SU4 Recall scores and modified Pyramid Scores for baseline, SPP algorithm and two top performing systems at TAC 2008." ></td>
	<td class="line x" title="122:162	identifies sentences that are known to be contributing towards the needed answers, it performs better at the shorter version of the task." ></td>
	<td class="line x" title="123:162	Another possible explanation is that as a shorter summary length is required, the task of choosing the most important information becomes more difficult and no approach works well consistently." ></td>
	<td class="line x" title="124:162	Also, it hasoftenbeennotedthatthisbaselineisindeedquite strong for this genre, due to the journalistic convention for putting the most important part of an article in the initial paragraphs." ></td>
	<td class="line x" title="125:162	4 Baselines in Summarization Tasks Over the years, as summarization research followed trends from generic single-document summarization, to generic multi-document summarization, to focused multi-document summarization there were two major baselines that stayed throughout the evaluations." ></td>
	<td class="line x" title="126:162	Those two baselines are: 1." ></td>
	<td class="line x" title="127:162	First N words of the document (or of the most recent document)." ></td>
	<td class="line x" title="128:162	2." ></td>
	<td class="line x" title="129:162	First sentence from each document in chronological order until the length requirement is reached." ></td>
	<td class="line x" title="130:162	The first baseline was in place ever since the first evaluation of generic single document summarization took place in DUC 2001." ></td>
	<td class="line x" title="131:162	For multi-document summarization, first N words of the most recent document (chronologically) was chosen as the baseline 1." ></td>
	<td class="line x" title="132:162	In the recent summarization evaluations at Text Analysis Conference (TAC 2008), where update summarization was evaluated; baseline 1 still persists." ></td>
	<td class="line x" title="133:162	Thisbaselineperformsprettypoorlyatcontent evaluations based on all manual and automatic metrics." ></td>
	<td class="line x" title="134:162	However, since it doesnt disturb the original flow and ordering of a document, linguistically these summaries are the best." ></td>
	<td class="line x" title="135:162	Indeed it outperforms alltheautomatedsystems basedonlinguisticquality evaluations." ></td>
	<td class="line x" title="136:162	The second baseline had been used occasionally with multi-document summarization from 2001 to 2004 with both generic multi-document summarization and focused multi-document summarization." ></td>
	<td class="line x" title="137:162	In 2001onlyonesystemsignificantlyoutperformedthe baseline 2 (Nenkova, 2005)." ></td>
	<td class="line x" title="138:162	In 2003 QF-MDS however, only one system outperformed the baseline 2 above, while in 2004 at the same task, no system significantly outperforms the baseline." ></td>
	<td class="line x" title="139:162	This baseline as can be seen, over the years has been pretty much untouched by systems based on content evaluation." ></td>
	<td class="line x" title="140:162	However, the linguistic aspects of summary quality would be compromised in such a summary." ></td>
	<td class="line x" title="141:162	Currently, for the Update Summarization task at TAC 2008, NISTs baseline is the baseline 1 (first x words baseline)." ></td>
	<td class="line x" title="142:162	And all systems (except one) perform better than the baseline in all forms of content evaluation." ></td>
	<td class="line x" title="143:162	Since the task is to generate 100 word summaries (short summaries), based on past experiences, there is no doubt that baseline 2 would perform well." ></td>
	<td class="line x" title="144:162	Itisinterestingtoobservethatbaseline2isaclose approximation to the SPP algorithm described in this paper." ></td>
	<td class="line x" title="145:162	There are two main differences that we draw between baseline 2 and SPP algorithm." ></td>
	<td class="line x" title="146:162	First, baseline 2 picks only the first sentence in each document, while SPP algorithm could pick other sentences in an order described by the position policy." ></td>
	<td class="line x" title="147:162	Second, baseline 2 puts no restriction on redundancy, thus due to journalistic conventions entire summary might be comprised of the same information nuggets, wasting the minimal real-estate available (~100 words)." ></td>
	<td class="line x" title="148:162	On the other hand, in our SPP algorithm we consider a simple unigram-overlap measure to identify redundant information in sentence pairs that avoids redundant nuggets in the final summary." ></td>
	<td class="line x" title="149:162	51 5 Discussion and Conclusion Baselines 1 and 2 mentioned above, could together act as a balancing mechanism to compare for linguistic quality and responsive content in the summary." ></td>
	<td class="line x" title="150:162	The availability of a stronger content responsive summary as a baseline would enable steady progress in the field." ></td>
	<td class="line x" title="151:162	While all the linguistically motivated systems would compare themselves with baseline 1, the summary content motivated systems would compare with the stronger baseline 2 and get better than it." ></td>
	<td class="line x" title="152:162	Over the years to come, the usage of baseline 1 doesnt help in understanding whether there has beensignificantimprovementinthefield." ></td>
	<td class="line x" title="153:162	Thisisbecause almost every simple algorithm beats the baseline performance." ></td>
	<td class="line x" title="154:162	Having a better baseline, like the one based on the position hypothesis, would raise the bar for systems participating in coming years, and tracking progress of the field over the years is easier.In this paper, we derived a method to identify a sub-optimal position policy based on pyramid annotation data, that were previously unavailable." ></td>
	<td class="line x" title="155:162	We also distinguish small and large documents to obtain the position policy." ></td>
	<td class="line x" title="156:162	We described the Sub-optimal Sentence Position Policy (SPP) based on pyramid annotation data and implemented the SPP as an algorithm to show that a position policy thus formed is a good representative of the genre and thus performs way above median performance." ></td>
	<td class="line x" title="157:162	We further describe the baselines used in summarization evaluation and discuss the need to bring back baseline 2 (or the SPP algorithm) as an official baseline for update summarization task." ></td>
	<td class="line x" title="158:162	Ultimately, as Lin and Hovy (1997) suggest, the position method can only take us certain distance." ></td>
	<td class="line x" title="159:162	It has a limited power of resolution (the sentence) and its limited method of identification (the position in a text)." ></td>
	<td class="line x" title="160:162	Which is why we intend to use it as a baseline." ></td>
	<td class="line x" title="161:162	Currently, as we can see the algorithm generates a generic summary, it doesnt consider the topic or query to generate a query-focused summary." ></td>
	<td class="line x" title="162:162	In future we plan to extend the SPP algorithm with some basic method for bringing in relevance." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1802
A Scalable Global Model for Summarization
Gillick, Dan;Favre, Benoit;"></td>
	<td class="line x" title="1:208	Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 1018, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:208	c 2009 Association for Computational Linguistics A Scalable Global Model for Summarization Dan Gillick1,2, Benoit Favre2 1 Computer Science Division, University of California Berkeley, USA 2 International Computer Science Institute, Berkeley, USA {dgillick,favre}@icsi.berkeley.edu Abstract We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization." ></td>
	<td class="line x" title="3:208	We compare our model, which operates at the subsentence or concept-level, to a sentencelevel model, previously solved with an ILP." ></td>
	<td class="line x" title="4:208	Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences." ></td>
	<td class="line x" title="5:208	We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously." ></td>
	<td class="line x" title="6:208	The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics." ></td>
	<td class="line x" title="7:208	1 Introduction Automatic summarization systems are typically extractive or abstractive." ></td>
	<td class="line x" title="8:208	Since abstraction is quite hard, the most successful systems tested at the Text Analysis Conference (TAC) and Document Understanding Conference (DUC)1, for example, are extractive." ></td>
	<td class="line x" title="9:208	In particular, sentence selection represents a reasonable trade-off between linguistic quality, guaranteed by longer textual units, and summary content, often improved with shorter units." ></td>
	<td class="line x" title="10:208	Whereas the majority of approaches employ a greedy search to find a set of sentences that is 1TAC is a continuation of DUC, which ran from 2001-2007." ></td>
	<td class="line x" title="11:208	both relevant and non-redundant (Goldstein et al., 2000; Nenkova and Vanderwende, 2005), some recent work focuses on improved search (McDonald, 2007; Yih et al., 2007)." ></td>
	<td class="line x" title="12:208	Among them, McDonald is the first to consider a non-approximated maximization of an objective function through Integer Linear Programming (ILP), which improves on a greedy search by 4-12%." ></td>
	<td class="line x" title="13:208	His formulation assumes that the quality of a summary is proportional to the sum of the relevance scores of the selected sentences, penalized by the sum of the redundancy scores of all pairs of selected sentences." ></td>
	<td class="line x" title="14:208	Under a maximum summary length constraint, this problem can be expressed as a quadratic knapsack (Gallo et al., 1980) and many methods are available to solve it (Pisinger et al., 2005)." ></td>
	<td class="line x" title="15:208	However, McDonald reports that the method is not scalable above 100 input sentences and discusses more practical approximations." ></td>
	<td class="line x" title="16:208	Still, an ILP formulation is appealing because it gives exact solutions and lends itself well to extensions through additional constraints." ></td>
	<td class="line x" title="17:208	Methods like McDonalds, including the wellknown Maximal Marginal Relevance (MMR) algorithm (Goldstein et al., 2000), are subject to another problem: Summary-level redundancy is not always well modeled by pairwise sentence-level redundancy." ></td>
	<td class="line x" title="18:208	Figure 1 shows an example where the combination of sentences (1) and (2) overlaps completely with sentence (3), a fact not captured by pairwise redundancy measures." ></td>
	<td class="line x" title="19:208	Redundancy, like content selection, is a global problem." ></td>
	<td class="line x" title="20:208	Here, we discuss a model for sentence selection with a globally optimal solution that also addresses redundancy globally." ></td>
	<td class="line x" title="21:208	We choose to represent infor10 (1) The cat is in the kitchen." ></td>
	<td class="line x" title="22:208	(2) The cat drinks the milk." ></td>
	<td class="line x" title="23:208	(3) The cat drinks the milk in the kitchen." ></td>
	<td class="line x" title="24:208	Figure 1: Example of sentences redundant as a group." ></td>
	<td class="line x" title="25:208	Their redundancy is only partially captured by sentencelevel pairwise measurement." ></td>
	<td class="line x" title="26:208	mation at a finer granularity than sentences, with concepts, and assume that the value of a summary is the sum of the values of the unique concepts it contains." ></td>
	<td class="line x" title="27:208	While the concepts we use in experiments are word n-grams, we use the generic term to emphasize that this is just one possible definition." ></td>
	<td class="line x" title="28:208	Only crediting each concept once serves as an implicit global constraint on redundancy." ></td>
	<td class="line x" title="29:208	We show how the resulting optimization problem can be mapped to an ILP that can be solved efficiently with standard software." ></td>
	<td class="line x" title="30:208	We begin by comparing our model to McDonalds (section 2) and detail the differences between the resulting ILP formulations (section 3), showing that ours can give competitive results (section 4) and offer better scalability2 (section 5)." ></td>
	<td class="line x" title="31:208	Next we demonstrate how our ILP formulation can be extended to include efficient parse-tree-based sentence compression (section 6)." ></td>
	<td class="line x" title="32:208	We review related work (section 7) and conclude with a discussion of potential improvements to the model (section 8)." ></td>
	<td class="line x" title="33:208	2 Models The model proposed by McDonald (2007) considers information and redundancy at the sentence level." ></td>
	<td class="line x" title="34:208	The score of a summary is defined as the sum of the relevance scores of the sentences it contains minus the sum of the redundancy scores of each pair of these sentences." ></td>
	<td class="line x" title="35:208	If si is an indicator for the presence of sentence i in the summary, Reli is its relevance, and Redij is its redundancy with sentence j, then a summary is scored according to: summationdisplay i Relisi  summationdisplay ij Redijsisj Generating a summary under this model involves maximizing this objective function, subject to a 2Strictly speaking, exact inference for the models discussed in this paper is NP-hard." ></td>
	<td class="line x" title="36:208	Thus we use the term scalable in a purely practical sense." ></td>
	<td class="line x" title="37:208	length constraint." ></td>
	<td class="line x" title="38:208	A variety of choices for Reli and Redij are possible, from simple word overlap metrics to the output of feature-based classifiers trained to perform information retrieval and textual entailment." ></td>
	<td class="line x" title="39:208	As an alternative, we consider information and redundancy at a sub-sentence, concept level, modeling the value of a summary as a function of the concepts it covers." ></td>
	<td class="line x" title="40:208	While McDonald uses an explicit redundancy term, we model redundancy implicitly: a summary only benefits from including each concept once." ></td>
	<td class="line x" title="41:208	With ci an indicator for the presence of concept i in the summary, and its weight wi, the objective function is: summationdisplay i wici We generate a summary by choosing a set of sentences that maximizes this objective function, subject to the usual length constraint." ></td>
	<td class="line x" title="42:208	In summing over concept weights, we assume that the value of including a concept is not effected by the presence of any other concept in the summary." ></td>
	<td class="line x" title="43:208	That is, concepts are assumed to be independent." ></td>
	<td class="line x" title="44:208	Choosing a suitable definition for concepts, and a mapping from the input documents to concept weights, is both important and difficult." ></td>
	<td class="line x" title="45:208	Concepts could be words, named entities, syntactic subtrees or semantic relations, for example." ></td>
	<td class="line x" title="46:208	While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone." ></td>
	<td class="line x" title="47:208	Any error in concept extraction can result in a biased objective function, leading to poor sentence selection." ></td>
	<td class="line x" title="48:208	3 Inference by ILP Each model presented above can be formalized as an Integer Linear Program, with a solution representing an optimal selection of sentences under the objective function, subject to a length constraint." ></td>
	<td class="line x" title="49:208	McDonald observes that the redundancy term makes for a quadratic objective function, which he coerces to a linear function by introducing additional variables sij that represent the presence of both sentence i and sentence j in the summary." ></td>
	<td class="line x" title="50:208	Additional constraints ensure the consistency between the sentence variables (si, sj) and the quadratic term (sij)." ></td>
	<td class="line x" title="51:208	With li the length of sentence i and L the length limit for 11 the whole summary, the resulting ILP is: Maximize: summationdisplay i Relisi  summationdisplay ij Redijsij Subject to: summationdisplay j ljsj  L sij  si sij  sj i,j si + sj sij  1 i,j si  {0,1} i sij  {0,1} i,j To express our concept-based model as an ILP, we maintain our notation from section 2, with ci an indicator for the presence of concept i in the summary and sj an indicator for the presence of sentence j in the summary." ></td>
	<td class="line x" title="52:208	We add Occij to indicate the occurrence of concept i in sentence j, resulting in a new ILP: Maximize: summationdisplay i wici Subject to: summationdisplay j ljsj  L sjOccij  ci, i,j (1)summationdisplay j sjOccij  ci i (2) ci  {0,1} i sj  {0,1} j Note that Occ, like Rel and Red, is a constant parameter." ></td>
	<td class="line x" title="53:208	The constraints formalized in equations (1) and (2) ensure the logical consistency of the solution: selecting a sentence necessitates selecting all the concepts it contains and selecting a concept is only possible if it is present in at least one selected sentence." ></td>
	<td class="line x" title="54:208	Constraint (1) also prevents the inclusion of concept-less sentences." ></td>
	<td class="line x" title="55:208	4 Performance Here we compare both models on a common summarization task." ></td>
	<td class="line x" title="56:208	The data is part of the Text Analysis Conference (TAC) multi-document summarization evaluation and involves generating 100-word summaries from 10 newswire documents, each on a given topic." ></td>
	<td class="line x" title="57:208	While the 2008 edition of TAC also includes an update taskadditional summaries assuming some prior knowledgewe focus only on the standard task." ></td>
	<td class="line x" title="58:208	This includes 48 topics, averaging 235 input sentences (ranging from 47 to 652)." ></td>
	<td class="line x" title="59:208	Since the mean sentence length is around 25 words, a typical summary consists of 4 sentences." ></td>
	<td class="line x" title="60:208	In order to facilitate comparison, we generate summaries from both models using a common pipeline: 1." ></td>
	<td class="line x" title="61:208	Clean input documents." ></td>
	<td class="line x" title="62:208	A simple set of rules removes headers and formatting markup." ></td>
	<td class="line x" title="63:208	2." ></td>
	<td class="line x" title="64:208	Split text into sentences." ></td>
	<td class="line x" title="65:208	We use the unsupervised Punkt system (Kiss and Strunk, 2006)." ></td>
	<td class="line x" title="66:208	3." ></td>
	<td class="line x" title="67:208	Prune sentences shorter than 5 words." ></td>
	<td class="line x" title="68:208	4." ></td>
	<td class="line x" title="69:208	Compute parameters needed by the models." ></td>
	<td class="line x" title="70:208	5." ></td>
	<td class="line x" title="71:208	Map to ILP format and solve." ></td>
	<td class="line x" title="72:208	We use an open source solver3." ></td>
	<td class="line x" title="73:208	6." ></td>
	<td class="line x" title="74:208	Order sentences picked by the ILP for inclusion in the summary." ></td>
	<td class="line x" title="75:208	The specifics of step 4 are described in detail in (McDonald, 2007) and (Gillick et al., 2008)." ></td>
	<td class="line x" title="76:208	McDonalds sentence relevance combines word-level cosine similarity with the source document and the inverse of its position (early sentences tend to be more important)." ></td>
	<td class="line x" title="77:208	Redundancy between a pair of sentences is their cosine similarity." ></td>
	<td class="line x" title="78:208	For sentence i in document D, Reli = cosine(i,D) + 1/pos(i,D) Redij = cosine(i,j) In our concept-based model, we use word bigrams, weighted by the number of input documents in which they appear." ></td>
	<td class="line x" title="79:208	While word bigrams stretch the notion of a concept a bit thin, they are easily extracted and matched (we use stemming to allow slightly more robust matching)." ></td>
	<td class="line x" title="80:208	Table 1 provides some justification for document frequency as a weighting function." ></td>
	<td class="line o" title="81:208	Note that bigrams gave consistently better performance than unigrams or trigrams for a variety of ROUGE measures." ></td>
	<td class="line o" title="82:208	Normalizing by document frequency measured over a generic set (TFIDF weighting) degraded ROUGE performance." ></td>
	<td class="line x" title="83:208	3gnu.org/software/glpk 12 Bigrams consisting of two stopwords are pruned, as are those appearing in fewer than three documents." ></td>
	<td class="line x" title="84:208	We largely ignore the sentence ordering problem, sorting the resulting sentences first by source document date, and then by position, so that the order of two originally adjacent sentences is preserved, for example." ></td>
	<td class="line x" title="85:208	Doc." ></td>
	<td class="line x" title="86:208	Freq." ></td>
	<td class="line x" title="87:208	(D) 1 2 3 4 5 6 In Gold Set 156 48 25 15 10 7 Not in Gold Set 5270 448 114 42 21 11 Relevant (P) 0.03 0.10 0.18 0.26 0.33 0.39 Table 1: There is a strong relationship between the document frequency of input bigrams and the fraction of those bigrams that appear in the human generated gold set: Let di be document frequency i and pi be the percent of input bigrams with di that are actually in the gold set." ></td>
	<td class="line x" title="88:208	Then the correlation (D,P) = 0.95 for DUC 2007 and 0.97 for DUC 2006." ></td>
	<td class="line x" title="89:208	Data here averaged over all problems in DUC 2007." ></td>
	<td class="line o" title="90:208	The summaries produced by the two systems have been evaluated automatically with ROUGE and manually with the Pyramid metric." ></td>
	<td class="line oc" title="91:208	In particular, ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin, 2004)." ></td>
	<td class="line x" title="92:208	The Pyramid score arises from a manual alignment of basic facts from the reference summaries, called Summary Content Units (SCUs), in a hypothesis summary (Nenkova and Passonneau, 2004)." ></td>
	<td class="line x" title="93:208	We used the SCUs provided by the TAC evaluation." ></td>
	<td class="line x" title="94:208	Table 2 compares these results, alongside a baseline that uses the first 100 words of the most recent document." ></td>
	<td class="line x" title="95:208	All the scores are significantly different, showing that according to both human and automatic content evaluation, the conceptbased model outperforms McDonalds sentencebased model, which in turn outperforms the baseline." ></td>
	<td class="line x" title="96:208	Of course, the relevance and redundancy functions used for McDonalds formulation in this experiment are rather primitive, and results would likely improve with better relevance features as used in many TAC systems." ></td>
	<td class="line o" title="97:208	Nonetheless, our system based on word bigram concepts, similarly primitive, performed at least as well as any in the TAC evaluation, according to two-tailed t-tests comparing ROUGE, Pyramid, and manually evaluated content responsiveness (Dang and Owczarzak, 2008) of our system and the highest scoring system in each category." ></td>
	<td class="line o" title="98:208	System ROUGE-2 Pyramid Baseline 0.058 0.186 McDonald 0.072 0.295 Concepts 0.110 0.345 Table 2: Scores for both systems and a baseline on TAC 2008 data (Set A) for ROUGE-2 and Pyramid evaluations." ></td>
	<td class="line x" title="99:208	5 Scalability McDonalds sentence-level formulation corresponds to a quadratic knapsack, and he shows his particular variant is NP-hard by reduction to 3-D matching." ></td>
	<td class="line x" title="100:208	The concept-level formulation is similar in spirit to the classical maximum coverage problem: Given a set of items X, a set of subsets S of X, and an integer k, the goal is to pick at most k subsets from S that maximizes the size of their union." ></td>
	<td class="line x" title="101:208	Maximum coverage is known to be NP-hard by reduction to the set cover problem (Hochbaum, 1996)." ></td>
	<td class="line x" title="102:208	Perhaps the simplest way to show that our formulation is NP-hard is by reduction to the knapsack problem (Karp, 1972)." ></td>
	<td class="line x" title="103:208	Consider the special case where sentences do not share any overlapping concepts." ></td>
	<td class="line x" title="104:208	Then, the value of each sentence to the summary is independent of every other sentence." ></td>
	<td class="line x" title="105:208	This is a knapsack problem: trying to maximize the value in a container of limited size." ></td>
	<td class="line x" title="106:208	Given a solver for our problem, we could solve all knapsack problem instances, so our problem must also be NP-hard." ></td>
	<td class="line x" title="107:208	With n input sentences and m concepts, both formulations generate a quadratic number of constraints." ></td>
	<td class="line x" title="108:208	However, McDonalds has O(n2) variables while ours has O(n + m)." ></td>
	<td class="line x" title="109:208	In practice, scalability is largely determined by the sparsity of the redundancy matrix Red and the sentence-concept matrix Occ." ></td>
	<td class="line x" title="110:208	Efficient solutions thus depend heavily on the choice of redundancy measure in McDonalds formulation and the choice of concepts in ours." ></td>
	<td class="line x" title="111:208	Pruning to reduce complexity involves removing lowrelevance sentences or ignoring low redundancy values in the former, and corresponds to removing lowweight concepts in the latter." ></td>
	<td class="line x" title="112:208	Note that pruning concepts may be more desirable: Pruned sentences are irretrievable, but pruned concepts may well appear in the selected sentences through co-occurrence." ></td>
	<td class="line x" title="113:208	Figure 2 compares ILP run-times for the two 13 formulations, using a set of 25 topics from DUC 2007, each of which have at least 500 input sentences." ></td>
	<td class="line x" title="114:208	These are very similar to the TAC 2008 topics, but more input documents are provided for each topic, which allowed us to extend the analysis to larger problems." ></td>
	<td class="line x" title="115:208	While the ILP solver finds optimal solutions efficiently for our concept-based formulation, run-time for McDonalds approach grows very rapidly." ></td>
	<td class="line x" title="116:208	The plot includes timing results for 250-word summaries as well, showing that our approach is fast even for much more complex problems: A rough estimate for the number of possible summaries hasparenleftbig5004 parenrightbig = 2.6109 for 100-word summaries and parenleftbig50010parenrightbig = 2.5  1020 for 250 words summaries." ></td>
	<td class="line x" title="117:208	While exact solutions are theoretically appealing, they are only useful in practice if fast approximations are inferior." ></td>
	<td class="line o" title="118:208	A greedy approximation of our objective function gives 10% lower ROUGE scores than the exact solution, a gap that separates the highest scoring systems from the middle of the pack in the TAC evaluation." ></td>
	<td class="line x" title="119:208	The greedy solution (linear in the number of sentences, assuming a constant summary length) marks an upper bound on speed and a lower bound on performance; The ILP solution marks an upper bound on performance but is subject to the perils of exponential scaling." ></td>
	<td class="line x" title="120:208	While we have not experimented with much larger documents, approximate methods will likely be valuable in bridging the performance gap for complex problems." ></td>
	<td class="line x" title="121:208	Preliminary experiments with local search methods are promising in this regard." ></td>
	<td class="line x" title="122:208	6 Extensions Here we describe how our ILP formulation can be extended with additional constraints to incorporate sentence compression." ></td>
	<td class="line x" title="123:208	In particular, we are interested in creating compressed alternatives for the original sentence by manipulating its parse tree (Knight and Marcu, 2000)." ></td>
	<td class="line x" title="124:208	This idea has been applied with some success to summarization (Turner and Charniak, 2005; Hovy et al., 2005; Nenkova, 2008) with the goal of removing irrelevant or redundant details, thus freeing space for more relevant information." ></td>
	<td class="line x" title="125:208	One way to achieve this end is to generate compressed candidates for each sentence, creating an expanded pool of input sentences, and emFigure 2: A comparison of ILP run-times (on an AMD 1.8Ghz desktop machine) of McDonalds sentence-based formulation and our concept-based formulation with an increasing number of input sentences." ></td>
	<td class="line x" title="126:208	ploy some redundancy removal on the final selection (Madnani et al., 2007)." ></td>
	<td class="line x" title="127:208	We adapt this approach to fit the ILP formulations so that the optimization procedure decides which compressed alternatives to pick." ></td>
	<td class="line x" title="128:208	Formally, each compression candidate belongs to a group gk corresponding to its original sentence." ></td>
	<td class="line x" title="129:208	We can then craft a constraint to ensure that at most one sentence can be selected from group gk, which also includes the original: summationdisplay igk si  1,gk Assuming that all the compressed candidates are themselves well-formed, meaningful sentences, we would expect this approach to generate higher quality summaries." ></td>
	<td class="line x" title="130:208	In general, however, compression algorithms can generate an exponential number of candidates." ></td>
	<td class="line x" title="131:208	Within McDonalds framework, this can increase the number of variables and constraints tremendously." ></td>
	<td class="line x" title="132:208	Thus, we seek a compact representation for compression in our concept framework." ></td>
	<td class="line x" title="133:208	Specifically, we assume that compression involves some combination of three basic operations on sentences: extraction, removal, and substitution." ></td>
	<td class="line x" title="134:208	In extraction, a sub-sentence (perhaps the content of a quotation) may be used independently, and the rest of the sentence is dropped." ></td>
	<td class="line x" title="135:208	In removal, a substring 14 is dropped (a temporal clause, for example) that preserves the grammaticality of the sentence." ></td>
	<td class="line x" title="136:208	In substitution, one substring is replaced by another (US replaces United States, for example)." ></td>
	<td class="line x" title="137:208	Arbitrary combinations of these operations are too general to be represented efficiently in an ILP." ></td>
	<td class="line x" title="138:208	In particular, we need to compute the length of a sentence and the concepts it covers for all compression candidates." ></td>
	<td class="line x" title="139:208	Thus, we insist that the operations can only affect non-overlapping spans of text, and end up with a tree representation of each sentence: Nodes correspond to compression operations and leaves map to the words." ></td>
	<td class="line x" title="140:208	Each node holds the length it contributes to the sentence recursively, as the sum of the lengths of its children." ></td>
	<td class="line x" title="141:208	Similarly, the concepts covered by a node are the union of the concepts covered by its children." ></td>
	<td class="line x" title="142:208	When a node is activated in the ILP, we consider that the text attached to it is present in the summary and update the length constraint and concept selection accordingly." ></td>
	<td class="line x" title="143:208	Figure 3 gives an example of this tree representation for a sentence from the TAC data, showing the derivations of some compressed candidates." ></td>
	<td class="line x" title="144:208	For a given sentence j, let Nj be the set of nodes in its compression tree, Ej  Nj be the set of nodes that can be extracted (used as independent sentences), Rj  Nj be the set of nodes that can be removed, and Sj  Nj be the set of substitution group nodes." ></td>
	<td class="line x" title="145:208	Let x and y be nodes from Nj; we create binary variables nx and ny to represent the inclusion of x or y in the summary." ></td>
	<td class="line x" title="146:208	Let x follows y denote the fact that x  Nj is a direct parent of y  Nj." ></td>
	<td class="line x" title="147:208	The constraints corresponding to the compression tree are: summationdisplay xEj nx  1 j (3) summationdisplay xfollowsy ny = nx x  Sj j (4) nx  ny (y follows xx / {Rj Sj}) j (5) nx  ny (y follows xx / {Ej Sj}) j (6) Eq." ></td>
	<td class="line x" title="148:208	(3) enforces that only one sub-sentence is extracted from the original sentence; eq." ></td>
	<td class="line x" title="149:208	(4) enforces that one child of a substitution group is selected if and only if the substitution node is selected; eq." ></td>
	<td class="line x" title="150:208	(5) ensures that a child node is selected when its parent is selected unless the child is removable (or a substitution group); eq." ></td>
	<td class="line x" title="151:208	(6) ensures that if a child node is selected, its parent is also selected unless the child is an extraction node (that can be used as a root)." ></td>
	<td class="line x" title="152:208	Each node is associated with the words and the concepts it contains directly (which are not contained by a child node) in order to compute the new length constraints and activate concepts in the objective function." ></td>
	<td class="line x" title="153:208	We set Occix to represent the occurrence of concept i in node x as a direct child." ></td>
	<td class="line x" title="154:208	Let lx be the length contributed to node x as direct children." ></td>
	<td class="line x" title="155:208	The resulting ILP for performing sentence compression jointly with sentence selection is: Maximize: summationdisplay i wici Subject to: summationdisplay j lxnx  L nxOccix  ci, i,xsummationdisplay x nxOccix  ci i idem constraints (3) to (6) ci  {0,1} i nx  {0,1} x While this framework can be used to implement a wide range of compression techniques, we choose to derive the compression tree from the sentences parse tree, extracted with the Berkeley parser (Petrov and Klein, 2007), and use a set of rules to label parse tree nodes with compression operations." ></td>
	<td class="line x" title="156:208	For example, declarative clauses containing a subject and a verb are labeled with the extract (E) operation; adverbial clauses and non-mandatory prepositional clauses are labeled with the remove (R) operation; Acronyms can be replaced by their full form by using substitution (S) operations and a primitive form of co-reference resolution is used to allow the substitution of noun phrases by their referent." ></td>
	<td class="line x" title="157:208	System R-2 Pyr." ></td>
	<td class="line x" title="158:208	LQ No comp." ></td>
	<td class="line x" title="159:208	0.110 0.345 2.479 Comp." ></td>
	<td class="line x" title="160:208	0.111 0.323 2.021 Table 3: Scores of the system with and without sentence compression included in the ILP (TAC08 Set A data)." ></td>
	<td class="line x" title="161:208	When implemented in the system presented in section 4, this approach gives a slight improvement 15 countries are  planing to hold the euro as part of their  reserves the magazine quoted  chief Wilm Disenberg as saying already (ECB | European Central Bank) foreign curency (1):E (2):E (6):R (7):R (8):R (4):R(3):S A number of (5):R Node Len." ></td>
	<td class="line x" title="162:208	Concepts (1):E 6 {the magazine, magazine quoted, chief Wilm, Wilm Disenberg} (2):E 7 {countries are, planning to, to hold, hold the, the euro} (3):S 0 {} (3a) 1 {ECB} (3b) 3 {European Central, Central Bank} (4):R 2 {as saying} (5):R 3 {a number, number of} (6):R 1 {} (7):R 5 {as part, part of, reserves} (8):R 2 {foreign currency}  Original: A number of Countries are already planning to hold the euro as part of their foreign currency reserves, the magazine quoted European Central Bank chief Wim Duisenberg as saying." ></td>
	<td class="line x" title="163:208	 [1,2,5,3a]: A number of countries are planning to hold the euro, the magazine quoted ECB chief Wim Duisenberg." ></td>
	<td class="line x" title="164:208	 [2,5,6,7,8]: A number of countries are already planning to hold the euro as part of their foreign currency reserves." ></td>
	<td class="line x" title="165:208	 [2,7,8]: Countries are planning to hold the euro as part of their foreign currency reserves." ></td>
	<td class="line x" title="166:208	 [2]: Countries are planning to hold the euro." ></td>
	<td class="line x" title="167:208	Figure 3: A compression tree for an example sentence." ></td>
	<td class="line x" title="168:208	E-nodes (diamonds) can be extracted and used as an independent sentences, R-nodes (circles) can be removed, and S-nodes (squares) contain substitution alternatives." ></td>
	<td class="line x" title="169:208	The table shows the word bigram concepts covered by each node and the length it contributes to the summary." ></td>
	<td class="line x" title="170:208	Examples of resulting compression candidates are given on the right side, with the list of nodes activated in their derivations." ></td>
	<td class="line o" title="171:208	in ROUGE-2 score (see Table 3), but a reduction in Pyramid score." ></td>
	<td class="line x" title="172:208	An analysis of the resulting summaries showed that the rules used for implementing sentence compression fail to ensure that all compression candidates are valid sentences, and about 60% of the summaries contain ungrammatical sentences." ></td>
	<td class="line x" title="173:208	This is confirmed by the linguistic quality4 score drop for this system." ></td>
	<td class="line x" title="174:208	The poor quality of the compressed sentences explains the reduction in Pyramid scores: Human judges tend to not give credit to ungrammatical sentences because they obscure the SCUs." ></td>
	<td class="line x" title="175:208	We have shown in this section how sentence compression can be implemented in a more scalable way under the concept-based model, but it remains to be shown that such a technique can improve summary quality." ></td>
	<td class="line x" title="176:208	7 Related work In addition to proposing an ILP for the sentencelevel model, McDonald (2007) discusses a kind of summary-level model: The score of a summary is 4As measured according to the TAC08 guidelines." ></td>
	<td class="line x" title="177:208	determined by its cosine similarity to the collection of input documents." ></td>
	<td class="line x" title="178:208	Though this idea is only implemented with approximate methods, it is similar in spirit to our concept-based model since it relies on weights for individual summary words rather than sentences." ></td>
	<td class="line x" title="179:208	Using a maximum coverage model for summarization is not new." ></td>
	<td class="line x" title="180:208	Filatova (2004) formalizes the idea, discussing its similarity to the classical NPhard problem, but in the end uses a greedy approximation to generate summaries." ></td>
	<td class="line x" title="181:208	More recently, Yih et al.(2007) employ a similar model and uses a stack decoder to improve on a greedy search." ></td>
	<td class="line x" title="183:208	Globally optimal summaries are also discussed by Liu (2006) and Jaoua Kallel (2004) who apply genetic algorithms for finding selections of sentences that maximize summary-level metrics." ></td>
	<td class="line x" title="184:208	Hassel (2006) uses hill climbing to build summaries that maximize a global information criterion based on random indexing." ></td>
	<td class="line x" title="185:208	The general idea of concept-level scoring for summarization is employed in the SumBasic system (Nenkova and Vanderwende, 2005), which chooses sentences greedily according to the sum of their word values (values are derived from fre16 quency)." ></td>
	<td class="line x" title="186:208	Conroy (2006) describes a bag-of-words model, with the goal of approximating the distribution of words from the input documents in the summary." ></td>
	<td class="line x" title="187:208	Others, like (Yih et al., 2007) train a model to learn the value of each word from a set of features including frequency and position." ></td>
	<td class="line x" title="188:208	Filatovas model is most theoretically similar to ours, though the concepts she chooses are events." ></td>
	<td class="line x" title="189:208	8 Conclusion and Future Work We have synthesized a number of ideas from the field of automatic summarization, including concept-level weighting, a maximum coverage model to minimize redundancy globally, and sentence compression derived from parse trees." ></td>
	<td class="line x" title="190:208	While an ILP formulation for summarization is not novel, ours provides reasonably scalable, efficient solutions for practical problems, including those in recent TAC and DUC evaluations." ></td>
	<td class="line x" title="191:208	We have also shown how it can be extended to perform sentence compression and sentence selection jointly." ></td>
	<td class="line o" title="192:208	In ROUGE and Pyramid evaluation, our system significantly outperformed McDonalds ILP system." ></td>
	<td class="line x" title="193:208	However, we would note that better design of sentence-level scoring would likely yield better results as suggested by the success of greedy sentencebased methods at the DUC and TAC conferences (see for instance (Toutanova et al., 2007))." ></td>
	<td class="line x" title="194:208	Still, the performance of our system, on par with the current state-of-the-art, is encouraging." ></td>
	<td class="line x" title="195:208	There are three principal directions for future work." ></td>
	<td class="line x" title="196:208	First, word bigram concepts are convenient, but semantically unappealing." ></td>
	<td class="line x" title="197:208	We plan to explore concepts derived from parse trees, where weights may be a function of frequency as well as hierarchical relationships." ></td>
	<td class="line x" title="198:208	Second, our current approach relies entirely on word frequency, a reasonable proxy for relevance, but likely inferior to learning weights from training data." ></td>
	<td class="line x" title="199:208	A number of systems have shown improvements by learning word values, though preliminary attempts to improve on our frequency heuristic by learning bigram values have not produced significant gains." ></td>
	<td class="line x" title="200:208	Better features may be necessary." ></td>
	<td class="line x" title="201:208	However, since the ILP gives optimal solutions so quickly, we are more interested in discriminative training where we learn weights for features that push the resulting summaries in the right direction, as opposed to the individual concept values." ></td>
	<td class="line x" title="202:208	Third, our rule-based sentence compression is more of a proof of concept, showing that joint compression and optimal selection is feasible." ></td>
	<td class="line x" title="203:208	Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008)." ></td>
	<td class="line x" title="204:208	The addition of compressed sentences tends to yield less coherent summaries, making sentence ordering more important." ></td>
	<td class="line x" title="205:208	We would like to add constraints on sentence ordering to the ILP formulation to address this issue." ></td>
	<td class="line x" title="206:208	Acknowledgments This work is supported by the Defense Advanced Research Projects Agency (DARPA) GALE project, under Contract No." ></td>
	<td class="line x" title="207:208	HR0011-06-C-0023." ></td>
	<td class="line x" title="208:208	Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2804
Optimization-based Content Selection for Opinion Summarization
Cheung, Jackie Chi Kit;Carenini, Giuseppe;Ng, Raymond T.;"></td>
	<td class="line x" title="1:204	Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 714, Suntec, Singapore, 6 August 2009." ></td>
	<td class="line x" title="2:204	c 2009 ACL and AFNLP Optimization-based Content Selection for Opinion Summarization Jackie Chi Kit Cheung Department of Computer Science University of Toronto Toronto, ON, M5S 3G4, Canada jcheung@cs.toronto.edu Giuseppe Carenini and Raymond T. Ng Department of Computer Science University of British Columbia Vancouver, BC, V6T 1Z4, Canada {carenini,rng}@cs.ubc.ca Abstract We introduce a content selection method for opinion summarization based on a well-studied, formal mathematical model, the p-median clustering problem from facility location theory." ></td>
	<td class="line x" title="3:204	Our method replaces a series of local, myopic steps to content selection with a global solution, and is designed to allow content and realization decisions to be naturally integrated." ></td>
	<td class="line x" title="4:204	We evaluate and compare our method against an existing heuristic-based method on content selection, using human selections as a gold standard." ></td>
	<td class="line x" title="5:204	We find that the algorithms perform similarly, suggesting that our content selection method is robust enough to support integration with other aspects of summarization." ></td>
	<td class="line x" title="6:204	1 Introduction It is now possible to find a large amount of information on peoples opinions on almost every subject online." ></td>
	<td class="line x" title="7:204	The ability to analyze such information is critical in complex, high-stakes decision making processes." ></td>
	<td class="line x" title="8:204	At the individual level, someone wishing to buy a laptop may read customer reviews from others who have purchased and used the product." ></td>
	<td class="line x" title="9:204	At the corporate level, customer feedback on a newly launched product may help to identify weaknesses and features that are in need of improvement (Dellarocas et al., 2004)." ></td>
	<td class="line x" title="10:204	Effective summarization systems are thus needed to convey peoples opinions to users." ></td>
	<td class="line x" title="11:204	A challenging problem in implementing this approach in a particular domain is to devise a content selection strategy that identifies what key information should be presented." ></td>
	<td class="line x" title="12:204	In general, content selection is a critical task at the core of both summarization and NLG and it represents a promising area for cross-fertilization." ></td>
	<td class="line x" title="13:204	Existing NLG systems tend to approach content selection by defining a heuristic based on several relevant factors, and maximizing this heuristic function." ></td>
	<td class="line x" title="14:204	ILEX (Intelligent Labelling Explorer) is a system for generating labels for sets of objects defined in a database, such as for museum artifacts (ODonnell et al., 2001)." ></td>
	<td class="line x" title="15:204	Its content selection strategy involves computing a heuristic relevance score for knowledge elements, and returning the items with the highest scores." ></td>
	<td class="line x" title="16:204	In GEA (Generator of Evaluative Arguments), evaluative arguments are generated to describe an entity as positive or negative (Carenini and Moore, 2006)." ></td>
	<td class="line x" title="17:204	An entity is decomposed into a hierarchy of features, and a relevance score is independently calculated for each feature, based on the preferences of the user and the value of that feature for the product." ></td>
	<td class="line x" title="18:204	Content selection involves selecting the most relevant features for the current user." ></td>
	<td class="line x" title="19:204	There is also work in sentiment analysis relying on optimization or clustering-based approaches." ></td>
	<td class="line x" title="20:204	Pang and Lee (2004) frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences." ></td>
	<td class="line x" title="21:204	They produce compressed versions of movie reviews using just the subjective sentences, which retain the polarity information of the review." ></td>
	<td class="line x" title="22:204	Gamon et al.(2005) use a heuristic approach to cluster sentences drawn from car reviews, grouping sentences that share common terms, especially those salient in the domain such as drive or handling." ></td>
	<td class="line x" title="24:204	The resulting clusters are displayed by a Treemap visualization." ></td>
	<td class="line x" title="25:204	Our work is most similar to the content selection method of the multimedia conversation system RIA (Responsive Information Architect) (Zhou and Aggarwal, 2004)." ></td>
	<td class="line x" title="26:204	In RIA, content selection involves selecting dimensions (such as price in the real estate domain) in response to a query such that the desirability of the dimensions selected for the query is maximized while respect7 ing time and space constraints." ></td>
	<td class="line x" title="27:204	The maximization of desirability is implemented as an optimization problem similar to a knapsack problem." ></td>
	<td class="line x" title="28:204	RIAs content selection method performs similarly to expert human designers, but the evaluation is limited in scale (two designers, each annotating two series of queries to the system), and no heuristic alternative is compared against it." ></td>
	<td class="line x" title="29:204	Our work also frames content selection as a formal optimization problem, but we apply this model to the domain of opinion summarization." ></td>
	<td class="line x" title="30:204	A key advantage of formulating a content selection strategy as a p-median optimization problem is that the resulting framework can be extended to select other characteristics of the summary at the same time as the information content, such as the realization strategy with which the content is expressed." ></td>
	<td class="line x" title="31:204	The p-median clustering works as a module separate from its interpretation as the solution to a content selection problem, so we can freely modify the conversion process from the selection problem to the clustering problem." ></td>
	<td class="line x" title="32:204	Work in NLG and summarization has shown that content and realization decisions (including media allocation) are often dependent on each other, which should be reflected in the summarization process." ></td>
	<td class="line x" title="33:204	For example, in multi-modal summarization, complex information can be more effectively conveyed by combining graphics and text (Tufte et al., 1998)." ></td>
	<td class="line x" title="34:204	While graphics can present large amounts of data compactly and support the discovery of trends and relationships, text is much more effective at explaining key points about the data." ></td>
	<td class="line x" title="35:204	In another case specific to opinion summarization, the controversiality of the opinions in a corpus was found to correlate with the type of text summary, with abstractive summarization being preferred when the controversiality is high (Carenini and Cheung, 2008)." ></td>
	<td class="line x" title="36:204	We first test whether our optimization-based approach can achieve reasonable performance on content selection alone." ></td>
	<td class="line x" title="37:204	As a contribution of this paper, we compare our optimization-based approach to a previously proposed heuristic method." ></td>
	<td class="line x" title="38:204	Because our approach replaces a set of myopic decisions with an extensively studied procedure (the p-median problem) that is able to find a global solution, we hypothesized our approach would produce better selections." ></td>
	<td class="line x" title="39:204	The results of our study indicate that our optimization-based content selection strategy performs about as well as the heuristic method." ></td>
	<td class="line x" title="40:204	These results suggest that our framework is robust enough for integrating other aspects of summarization with content selection." ></td>
	<td class="line x" title="41:204	2 Previous Heuristic Approach 2.1 Assumed Input Information We now define the expected input into the summarization process, then describe a previous greedy heuristic method." ></td>
	<td class="line x" title="42:204	The first phase of the summarization process is to extract opinions about an entity from free text or some other source, such as surveys." ></td>
	<td class="line x" title="43:204	and express the extracted information in a structured format for further processing." ></td>
	<td class="line x" title="44:204	We adopt the approach to opinion extraction described by Carenini et al.(2006), which we summarize here." ></td>
	<td class="line x" title="46:204	Given a corpus of documents expressing opinions about an entity, the system extracts a set of evaluations on aspects or features of the product." ></td>
	<td class="line x" title="47:204	An evaluation consists of a polarity, a score for the strength of the opinion, and the feature being evaluated." ></td>
	<td class="line x" title="48:204	The polarity expresses whether the opinion is positive or negative, and the strength expresses the degree of the sentiment, which is represented as an integer from 1 to 3." ></td>
	<td class="line x" title="49:204	Possible polarity/strength (P/S) scores are thus [-3,2,-1,+1,+2,+3], with +3 being the most positive evaluation, and -3 the most negative." ></td>
	<td class="line x" title="50:204	For example, using a DVD player as the entity, the comment Excellent picture qualityon par with my Pioneer, Panasonic, and JVC players. contains an opinion on the picture quality, and is a very positive evaluation (+3)." ></td>
	<td class="line x" title="51:204	The features and their associated opinions are organized into a hierarchy of user-defined features (UDFs), so named because they can be defined by a user according to the users needs or interests.1 The outcome of the process of opinion extraction and structuring is a UDF hierarchy in which each node is annotated with all the evaluations it received in the corpus (See Figure 1 for an example)." ></td>
	<td class="line x" title="52:204	2.2 Heuristic Content Selection Strategy Using the input information described above, content selection is framed as the process of selecting a subset of those features that are deemed more 1Actually, the system first extracts a set of surface-level crude features (CFs) on which opinions were expressed, using methods described by Hu and Liu (2004)." ></td>
	<td class="line x" title="53:204	Next, the CFs are mapped onto the UDFs using term similarity scores." ></td>
	<td class="line x" title="54:204	The process of mapping CFs to UDFs groups together semantically similar CFs and reduces redundancy." ></td>
	<td class="line x" title="55:204	Our study abstracts away from this mapping process, as well as the process of creating the UDF structure." ></td>
	<td class="line x" title="56:204	We leave the explanation of the details to the original papers." ></td>
	<td class="line x" title="57:204	8 Camera Lens [+1,+1,+3,2,+2] Digital Zoom Optical Zoom . . ." ></td>
	<td class="line x" title="58:204	Editing/Viewing [+1,+1] Viewfinder [-2,2,-1] . . ." ></td>
	<td class="line x" title="59:204	Flash [+1,+1,+3,+2,+2] . . ." ></td>
	<td class="line x" title="60:204	Image Image Type TIFF JPEG . . ." ></td>
	<td class="line x" title="61:204	Resolution Effective Pixels Aspect Ratio . . ." ></td>
	<td class="line x" title="62:204	Figure 1: Partial view of assumed input information (UDF hierarchy annotated with user evaluations) for a digital camera." ></td>
	<td class="line x" title="63:204	important and relevant to the user." ></td>
	<td class="line x" title="64:204	This is done using an importance measure defined on the available features (UDFs)." ></td>
	<td class="line x" title="65:204	This measure is calculated from the P/S scores of the evaluations associated to each UDF." ></td>
	<td class="line x" title="66:204	Let PS(u) be the set of P/S scores that UDF u receives." ></td>
	<td class="line x" title="67:204	Then, a measure of importance is defined as some function of the P/S scores." ></td>
	<td class="line x" title="68:204	Previous work considered only summing the squares of the scores." ></td>
	<td class="line x" title="69:204	In this work, we also consider summing the absolute value of the scores." ></td>
	<td class="line x" title="70:204	So, the importance measure is defined as dir moi(u) = summationdisplay psepsilon1PS(u) ps2 or summationdisplay psepsilon1PS(u) |ps| where the term direct means the importance is derived only from that feature and not from its descendant features." ></td>
	<td class="line x" title="71:204	The basic premises of these metrics are that a features importance should be proportional to the number of evaluations of that feature in the corpus, and that stronger evaluations should be given more weight." ></td>
	<td class="line x" title="72:204	The two versions implement the latter differently, using the sum of squares or the absolute values respectively." ></td>
	<td class="line x" title="73:204	Notice that each non-leaf node in the feature hierarchy effectively serves a dual purpose." ></td>
	<td class="line x" title="74:204	It is both a feature upon which a user might comment, as well as a category for grouping its sub-features." ></td>
	<td class="line x" title="75:204	Thus, a non-leaf node should be important if either its descendants are important or the node itself is important." ></td>
	<td class="line x" title="76:204	To this end, a total measure of importance moi(u) is defined as moi(u) =      dir moi(u) if CH(u) =  [dir moi(u) + (1)summationtext vCH(u)moi(v)] otherwise where CH(u) refers to the children of u in the hierarchy and  is some real parameter in the range [0.5,1] that adjusts the relative weights of the parent and children." ></td>
	<td class="line x" title="77:204	We found in our experimentation that the parameter setting does not substantially change the performance of the system, so we select the value 0.9 for , following previous work." ></td>
	<td class="line x" title="78:204	As a result, the total importance of a node is a combination of its direct importance and of the importance of its children." ></td>
	<td class="line x" title="79:204	The selection procedure proceeds as follows." ></td>
	<td class="line x" title="80:204	First, the most obvious simple greedy selection strategy was consideredsort the nodes in the UDF by the measure of importance and select the most important node until a desired number of features is included." ></td>
	<td class="line x" title="81:204	However, since a node derives part of its importance from its children, it is possible for a nodes importance to be dominated by one or more of its children." ></td>
	<td class="line x" title="82:204	Including both the child and parent node would be redundant because most of the information is contained in the child." ></td>
	<td class="line x" title="83:204	Thus, a dynamic greedy selection algorithm was devised in which the importance of each node was recalculated after each round of selection, with all previously selected nodes removed from the tree." ></td>
	<td class="line x" title="84:204	In this way, if a node that dominates its parents importance is selected, its parents importance will be reduced during later rounds of selection." ></td>
	<td class="line x" title="85:204	Notice, however, that this greedy selection consists of a series of myopic steps to decide which features to include in the summary next, based on what has been selected already and what remains to be selected at this step." ></td>
	<td class="line x" title="86:204	Although this series of local decisions may be locally optimal, it may result in a suboptimal choice of contents overall." ></td>
	<td class="line x" title="87:204	3 Clustering-Based Optimization Strategy To address the limitation of local optimality of this initial strategy, we explore if the content selection problem for opinion summarization can be naturally and effectively solved by a global optimization-based approach." ></td>
	<td class="line x" title="88:204	Our approach assumes the same input information as the previous approach, and we also use the direct measure 9 of importance defined above." ></td>
	<td class="line x" title="89:204	Our framework is UDF-based in the following senses." ></td>
	<td class="line x" title="90:204	First, a UDF is the basic unit of content that is selected for inclusion in the summary." ></td>
	<td class="line x" title="91:204	Also, the information content that needs to be covered by the summary is the sum of the information content in all of the UDFs in the UDF hierarchy." ></td>
	<td class="line x" title="92:204	To reduce content selection to a clustering problem, we need the following components." ></td>
	<td class="line x" title="93:204	First, we need a cost function to quantify how well a UDF (if selected) can express the information content in another UDF." ></td>
	<td class="line x" title="94:204	We call this measure the information coverage cost." ></td>
	<td class="line x" title="95:204	To define this cost function, we need to define the semantic relatedness between the selected content and the covered content, which is domain-dependent." ></td>
	<td class="line x" title="96:204	For example, we can rely on similarity metrics such as ones based on WordNet similarity scores (Fellbaum and others, 1998)." ></td>
	<td class="line x" title="97:204	In the consumer product domain in which we test our method, we use the UDF hierarchy of the entity being summarized." ></td>
	<td class="line x" title="98:204	Second, we need a clustering paradigm that defines the quality of a proposed clustering; that is, a way to globally quantify how well all the information content is represented by the set of UDFs that we select." ></td>
	<td class="line x" title="99:204	The clustering paradigm that we found to most naturally fit our task is the p-median problem (also known as the k-median problem), from facility location theory." ></td>
	<td class="line x" title="100:204	In its original interpretation, p-median is used to find optimal locations for opening facilities which provide services to customers, such that the cost of serving all of the customers with these facilities is minimized." ></td>
	<td class="line x" title="101:204	This matches our intuition that the quality of a summary of opinions depends on how well it represents all of the opinions to be summarized." ></td>
	<td class="line x" title="102:204	Formally, given a set F of m potential locations for facilities, a set U of n customers, a cost function d : F U Rfractur representing the cost of serving a customer uU with a facility f F, and a constant pm, an optimal solution to the p-median problem is a subsetS ofF, such that the expression summationdisplay uU min fS d(f,u) is minimized, and|S|= p. The subsetS is exactly the set of UDFs that we would include in the summary, and the parameter p can be set to determine the summary length." ></td>
	<td class="line x" title="103:204	Although solving the p-median problem is NPhard in general (Kariv and Hakimi, 1979), viable approximation methods do exist." ></td>
	<td class="line x" title="104:204	We use POPSTAR, an implementation of an approximate solution (Resende and Werneck, 2004) which has an average error rate of less than 0.4% on all the problem classes it was tested on in terms of the pmedian problem value." ></td>
	<td class="line x" title="105:204	As an independent test of the programs efficacy, we compare the programs output to solutions which we obtained by bruteforce search on 12 of the 36 datasets we worked with which are small enough such that an exact solution can be feasibly found." ></td>
	<td class="line x" title="106:204	POPSTAR returned the exact solution in all 12 instances." ></td>
	<td class="line x" title="107:204	We now reinterpret the p-median problem for summarization content selection by specifying the sets U, F, and the information coverage cost d in terms of properties of the summarization process." ></td>
	<td class="line x" title="108:204	We define the basic unit of the summarization process to be UDFs, so the sets U and F correspond to the set of UDFs describing the product." ></td>
	<td class="line x" title="109:204	The constant p is a parameter to the p-median problem, determining the summary size in terms of the number of features." ></td>
	<td class="line x" title="110:204	The cost function is d(u,v), where u is a UDF that is being considered for inclusion in the summary, and v is the UDF to be covered by u. To specify this cost, we need to consider both the total amount of information in v as well as the semantic relationship between the two features." ></td>
	<td class="line x" title="111:204	We use the importance measure defined earlier, based on the number and strength of evaluations of the covered feature to quantify the former." ></td>
	<td class="line x" title="112:204	The raw importance score is modified by multipliers which depend on the relationship between u and v. One is the semantic relatedness between the two features, which is modelled by the UDF tree hierarchy." ></td>
	<td class="line x" title="113:204	We hypothesize that it is easier for a more general feature to cover information about a more specific feature than the reverse, and that features that are not in a ancestor-descendant relationship cannot cover information about each other because of the tenuous semantic connection between them." ></td>
	<td class="line x" title="114:204	For example, knowing that a camera is well-liked in general provides stronger evidence that its durability is also well-liked than the reverse." ></td>
	<td class="line x" title="115:204	Based on these assumptions, we define a multiplier for the above measure of importance based on the UDF tree structure, T(u,v), as follows." ></td>
	<td class="line x" title="116:204	T(u,v) =   Tupk, if u is a descendant of v k, if u is an ancestor of v , otherwise k is the length of the path from u to v in the UDF 10 hierarchy." ></td>
	<td class="line x" title="117:204	Tup is a parameter specifying the relative difficulty of covering information in a feature that is an ancestor in the UDF hierarchy." ></td>
	<td class="line x" title="118:204	Mirroring our experience with the heuristic method, the value of the parameter does not affect performance very much." ></td>
	<td class="line x" title="119:204	In our experiments and the example to follow, we pick the values Tup = 3, meaning that covering information in an ancestor node is three times more difficult than covering information in a descendant node." ></td>
	<td class="line x" title="120:204	Another multiplier to the opinion domain is the distribution of evaluations of the features." ></td>
	<td class="line x" title="121:204	Coverage is expected to be less if the features are evaluated differently; for example, if users rated a camera well overall but the feature zoom poorly, a sentence about how well the camera is rated in general does not provide much evidence that the zoom is not well liked, and vice versa." ></td>
	<td class="line x" title="122:204	Since evaluations are labelled with P/S ratings in our data, it is natural to define this multiplier based on the distributions of ratings for the features." ></td>
	<td class="line x" title="123:204	Given these P/S ratings between -3 and +3, we first aggregate the positive and negative evaluations." ></td>
	<td class="line x" title="124:204	As before, we test both summing absolute values and squared values." ></td>
	<td class="line x" title="125:204	Define: imp pos(u) = summationdisplay psPS(u)ps>0 ps2 or |ps| imp neg(u) = summationdisplay psPS(u)ps<0 ps2 or |ps| Then, we calculate the parameter to the Bernoulli distribution corresponding to the ratio of the importance of the two polarities." ></td>
	<td class="line x" title="126:204	That is, Bernoulli with parameter (u) = imp pos(u)/(imp pos(u)+imp neg(u)) The distribution-based multiplier E(u,v) is the Jensen-Shannon divergence from Ber((u)) to Ber((v)), plus one for multiplicative identity when the divergence is zero." ></td>
	<td class="line x" title="127:204	E(u,v) = JS((u),(v)) + 1 The final formula for the information coverage cost is thus d(u,v) = dir moi(v)T(u,v)E(u,v) Consider the following example consisting of four-node UDF tree and importance scores." ></td>
	<td class="line x" title="128:204	i. Covered ii." ></td>
	<td class="line x" title="129:204	Solutions A B C D p Selected Val." ></td>
	<td class="line x" title="130:204	Co vering A 0 50 30 240 1 A 320 B 165 0  120 2 A,D 80 C 165  0  3 A,B,D 30 D 330 150  0 4 A,B,C,D 0 Table 1: i. Information coverage cost scores for the worked example." ></td>
	<td class="line x" title="131:204	Rows represent the covering feature, while columns represent the covered feature." ></td>
	<td class="line x" title="132:204	ii." ></td>
	<td class="line x" title="133:204	Optimal solution to p-median problem in the worked example at different numbers of features selected." ></td>
	<td class="line x" title="134:204	A dir moi(A) = 55 arrowsouthwestarrowsoutheast B C dir moi(B) = 50,dir moi(C) = 30  D dir moi(D) = 120 With parameter Tup = 3 and setting the distribution-based multiplier E to 1 to simplify calculations (or for example, if the features received the same distributions of evaluations), this tree yields the information coverage cost scores found in Table 1i." ></td>
	<td class="line x" title="135:204	Running p-median on these values produces the optimal results found in Table 1ii." ></td>
	<td class="line x" title="136:204	This method trades off selecting centrally located nodes near the root of the UDF tree and the importance of the individual nodes." ></td>
	<td class="line x" title="137:204	In this example, D is selected after the root node A even though D has a greater importance value." ></td>
	<td class="line x" title="138:204	4 Comparative Evaluation 4.1 Stochastic Data Generation In our experiments we wanted to compare the two content selection strategies (heuristic vs. p-median optimization) on datasets that were both realistic and diverse." ></td>
	<td class="line x" title="139:204	Despite the widespread adoption of user reviews in online websites, there is to our knowledge no publicly available corpus of customer reviews of sufficient size which is annotated with features arranged in a hierarchy." ></td>
	<td class="line x" title="140:204	While smallscale corpora do exist for a small number of products, the size of the corpora is too small to be representative of all possible distributions of evaluations and feature hierarchies of products, which limits our ability to draw any meaningful conclusion from the dataset.2 Thus, we stochastically 2Using a constructed dataset based on real data where no resources or agreed-upon evaluation methodology yet exists has been done in other NLP tasks such as topic boundary detection (Reynar, 1994) and local coherence modelling (Barzilay and Lapata, 2005)." ></td>
	<td class="line x" title="141:204	We are encouraged, however, that subsequent to our experiment, more resources for opinion anal11 mean std." ></td>
	<td class="line x" title="142:204	# Features 55.3889 8.5547 # Evaluated Features 21.6667 5.9722 # Children (depth 0) 11.3056 0.7753 # Children (depth 1 fertile) 5.5495 1.7724 Table 2: Statistics on the 36 generated data sets." ></td>
	<td class="line x" title="143:204	At depth 1, 134 of the 407 features in total across the trees were barren." ></td>
	<td class="line x" title="144:204	The generated tree hierarchies were quite flat, with a maximum depth of 2." ></td>
	<td class="line x" title="145:204	generated the data for the products to mimic real product feature hierarchies and evaluations." ></td>
	<td class="line x" title="146:204	We did this by gathering statistics from existing corpora of customer reviews about electronics products (Hu and Liu, 2004), which contain UDF hierarchies and evaluations that have been defined and annotated." ></td>
	<td class="line x" title="147:204	Using these statistics, we created distributions over the characteristics of the data, such as the number of nodes in a UDF hierarchy, and sampled from these distributions to generate new UDF hierarchies and evaluations." ></td>
	<td class="line x" title="148:204	In total, we generated 36 sets of data, which covered a realistic set of possible scenarios in term of feature hierarchy structures as well as in term of distribution of evaluations for each feature." ></td>
	<td class="line x" title="149:204	Table 2 presents some statistics on the generated data sets." ></td>
	<td class="line oc" title="150:204	4.2 Building a Human Performance Model We adopt the evaluation approach that a good content selection strategy should perform similarly to humans, which is the view taken by existing summarization evaluation schemes such as ROUGE (Lin, 2004) and the Pyramid method (Nenkova et al., 2007)." ></td>
	<td class="line x" title="151:204	For evaluating our content selection strategy, we conducted a user study asking human participants to perform a selection task to create gold standard selections." ></td>
	<td class="line x" title="152:204	Participants viewed and selected UDF features using a Treemap information visualization." ></td>
	<td class="line x" title="153:204	See Figure 2 for an example." ></td>
	<td class="line x" title="154:204	We recruited 25 university students or graduates, who were each presented with 19 to 20 of the cases we generated as described above." ></td>
	<td class="line x" title="155:204	Each case represented a different hypothetical product, which was represented by a UDF hierarchy, as well as P/S evaluations from -3 to +3." ></td>
	<td class="line x" title="156:204	These were displayed to the participants by a Treemap visualization (Shneiderman, 1992), which is able to give an overview of the feature hierarchy and the evaluations that each feature received." ></td>
	<td class="line x" title="157:204	Treemaps have been shown to be a generally successful tool for ysis such as a user review corpus by Constant et al.(2008) have been released, as an anonymous reviewer pointed out." ></td>
	<td class="line x" title="159:204	visualizing data in the customer review domain, even for novice users (Carenini et al., 2006)." ></td>
	<td class="line x" title="160:204	In a Treemap, the feature hierarchy is represented by nested rectangles, with parent features being larger rectangles, and children features being smaller rectangles contained within its parent rectangle." ></td>
	<td class="line x" title="161:204	The size of the rectangles depends on the number of evaluations that this feature received directly, as well as indirectly through its children features." ></td>
	<td class="line x" title="162:204	Each evaluation is also shown as a small rectangle, coloured according to its P/S rating, with -3 being bright red, and +3 being bright green." ></td>
	<td class="line x" title="163:204	Participants received 30 minutes of interactive training in using Treemaps, and were presented with a scenario in which they were told to take the role of a friend giving advice on the purchase of an electronics product based on existing customer reviews." ></td>
	<td class="line x" title="164:204	They were then shown 22 to 23 scenarios corresponding to different products and evaluations, and asked to select features which they think would be important to include in a summary to send to a friend." ></td>
	<td class="line x" title="165:204	We discarded the first three selections that participants made to allow them to become further accustomed to the visualization." ></td>
	<td class="line x" title="166:204	The number of features that participants were asked to select from each tree was 18% of the number of selectable features." ></td>
	<td class="line x" title="167:204	A feature is considered selectable if it appears in the Treemap visualization; that is, the feature receives at least one evaluation, or one of its descendant features does." ></td>
	<td class="line x" title="168:204	This proportion was the average proportion at which the selections made by the heuristic greedy strategy and p-median diverged the most when we were initially testing the algorithms." ></td>
	<td class="line x" title="169:204	Because each tree contained a different number of features, the actual number of features selected ranged from two to seven." ></td>
	<td class="line x" title="170:204	Features were given generic labels like Feature 34, so that participants cannot rely on preexisting knowledge about that Figure 2: A sample Treemap visualization of the customer review data sets shown to participants." ></td>
	<td class="line x" title="171:204	12 Selection method Cohens Kappa heuristic, squared moi 0.4839 heuristic, abs moi 0.4841 p-median, squared moi 0.4679 p-median, abs moi 0.4821 Table 3: Cohens kappa for heuristic greedy and p-median methods against human selections." ></td>
	<td class="line x" title="172:204	Two versions of the measure of importance were tested, one using squared P/S scores, the other using absolute values." ></td>
	<td class="line x" title="173:204	kind of product in their selections." ></td>
	<td class="line x" title="174:204	4.3 Evaluation Metrics Using this human gold standard, we can now compare the greedy heuristic and the p-median strategies." ></td>
	<td class="line x" title="175:204	We report the agreement between the human and machine selections in terms of kappa and a version of the Pyramid method." ></td>
	<td class="line x" title="176:204	The Pyramid method is a summarization evaluation scheme built upon the observation that human summaries can be equally informative despite being divergent in content (Nenkova et al., 2007)." ></td>
	<td class="line x" title="177:204	In the Pyramid method, Summary Content Units (SCUs) in a set of human-written model summaries are manually identified and annotated." ></td>
	<td class="line x" title="178:204	These SCUs are placed into a pyramid with different tiers, corresponding to the number of model (i.e. human) summaries in which each SCU appears." ></td>
	<td class="line x" title="179:204	A summary to be evaluated is similarly annotated by SCUs and is scored by the scores of its SCUs, which are the tier of the pyramid in which the SCU appears." ></td>
	<td class="line x" title="180:204	The Pyramid score is defined as the sum of the weights of the SCUs in the evaluated summary divided by the maximum score achievable with this number of SCUs, if we were to take SCUs starting from the highest tier of the pyramid." ></td>
	<td class="line x" title="181:204	Thus, a summary scores highly if its SCUs are found in many of the model summaries." ></td>
	<td class="line x" title="182:204	We use UDFs rather than text passages as SCUs, since UDFs are the basic units of content in our selections." ></td>
	<td class="line x" title="183:204	Moderate inter-annotator agreement between human feature selections shows that our data fits the assumption of the Pyramid method (i.e. diversity of human annotations); the Fleiss kappa (1971) scores for the human selections ranged from 0.2984 to 0.6151, with a mean of 0.4456 among all 33 sets which were evaluated." ></td>
	<td class="line x" title="184:204	A kappa value above 0.6 is generally taken to indicate substantial agreement (Landis and Koch, 1977)." ></td>
	<td class="line x" title="185:204	Figure 3: Pyramid scores for the two selection approaches at different numbers of features i. using the squared importance measure, ii." ></td>
	<td class="line x" title="186:204	using the absolute value importance measure." ></td>
	<td class="line x" title="187:204	4.4 Results The greedy heuristic method and p-median perform similarly at the number of features that the human participants were asked to select." ></td>
	<td class="line x" title="188:204	The difference is not statistically significant by a twotailed t-test." ></td>
	<td class="line x" title="189:204	Table 3 shows that using absolute values of P/S scores in the importance measure is better than using squares." ></td>
	<td class="line x" title="190:204	Squaring seems to give too much weight to extreme evaluations over more neutral evaluations." ></td>
	<td class="line x" title="191:204	P-median is particularly affected, which is not surprising as it uses the measure of importance both in the raw importance score and in the distribution-based multiplier." ></td>
	<td class="line x" title="192:204	The Pyramid method allows us to compare the algorithms at different numbers of features." ></td>
	<td class="line x" title="193:204	Figure 3 shows the average pyramid score for the two methods over the proportion of features that are selected." ></td>
	<td class="line x" title="194:204	Overall, both algorithms perform well, and reach a score of about 0.9 at 10% of features selected." ></td>
	<td class="line x" title="195:204	The heuristic method performs slightly better when the proportion is below 25%, but slightly worse above that proportion." ></td>
	<td class="line x" title="196:204	We consider several possible explanations for the surprising result that the heuristic greedy method and p-median methods perform similarly." ></td>
	<td class="line x" title="197:204	One possibility is that the approximate p-median solution we adopted (POPSTAR) is error-prone on this task, but this is unlikely as the approximate method has been rigorously tested both externally on much larger problems and internally on a subset of our data." ></td>
	<td class="line x" title="198:204	Another possibility is that the automatic methods have reached a ceiling in performance by these evaluation metrics." ></td>
	<td class="line x" title="199:204	Nevertheless, these results are encouraging in showing that our optimization-based method is a viable alternative to a heuristic strategy for content selection, and validate that incorporating other 13 summarization decisions into content selection is an option worth exploring." ></td>
	<td class="line x" title="200:204	5 Conclusions and Future Work We have proposed a formal optimization-based method for summarization content selection based on the p-median clustering paradigm, in which content selection is viewed as selecting clusters of related information." ></td>
	<td class="line x" title="201:204	We applied the framework to opinion summarization of customer reviews." ></td>
	<td class="line x" title="202:204	An experiment evaluating our p-median algorithm found that it performed about as well as a comparable existing heuristic approach designed for the opinion domain in terms of similarity to human selections." ></td>
	<td class="line x" title="203:204	These results suggest that the optimization-based approach is a good starting point for integration with other parts of the summarization/NLG process, which is a promising avenue of research." ></td>
	<td class="line x" title="204:204	6 Acknowledgements We would like to thank Lucas Rizoli, Gabriel Murray and the anonymous reviewers for their comments and suggestions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2806
Evaluation of Automatic Summaries: Metrics under Varying Data Conditions
Owczarzak, Karolina;Dang, Hoa Trang;"></td>
	<td class="line x" title="1:165	Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 2330, Suntec, Singapore, 6 August 2009." ></td>
	<td class="line x" title="2:165	c 2009 ACL and AFNLP Evaluation of automatic summaries: Metrics under varying data conditions Karolina Owczarzak and Hoa Trang Dang Information Access Division National Institute of Standards and Technology Gaithersburg, MD 20899 karolina.owczarzak@nist.gov hoa.dang@nist.gov Abstract In evaluation of automatic summaries, it is necessary to employ multiple topics and human-produced models in order for the assessment to be stable and reliable." ></td>
	<td class="line x" title="3:165	However, providing multiple topics and models is costly and time-consuming." ></td>
	<td class="line x" title="4:165	This paper examines the relation between the number of available models and topics and the correlations with human judgment obtained by automatic metrics ROUGE and BE, as well as the manual Pyramid method." ></td>
	<td class="line x" title="5:165	Testing all these methods on the same data set, taken from the TAC 2008 Summarization track, allows us to compare and contrast the methods under different conditions." ></td>
	<td class="line x" title="6:165	1 Introduction Appropriate evaluation of results is an important aspect of any research." ></td>
	<td class="line x" title="7:165	In areas such as automatic summarization, the problem is especially complex because of the inherent subjectivity in the task itself and its evaluation." ></td>
	<td class="line x" title="8:165	There is no single objective standard for a good quality summary; rather, its value depends on the summarys purpose, focus, and particular requirements of the reader (Sparck Jones, 2007)." ></td>
	<td class="line x" title="9:165	While the purpose and focus can be set as constant for a specific task, the variability of human judgment is more difficult to control." ></td>
	<td class="line x" title="10:165	Therefore, in attempts to produce stable evaluations, it has become standard to use multiple judges, not necessarily for parallel evaluation, but in such a way that each judge evaluates a different subset of the many summaries on which the final system assessment is based." ></td>
	<td class="line x" title="11:165	The incorporation of multiple points of view is also reflected in automatic evaluation, where it takes the form of employing multiple model summaries to which a candidate summary is compared." ></td>
	<td class="line x" title="12:165	Since these measures to neutralize judgment variation involve the production of multiple model summaries, as well as multiple topics, evaluation can become quite costly." ></td>
	<td class="line x" title="13:165	Therefore, it is interesting to examine how many models and topics are necessary to obtain a relatively stable evaluation, and whether this number is different for manual and automatic metrics." ></td>
	<td class="line x" title="14:165	In their examination of summary evaluations, van Halteren and Teufel (2003) suggest that it is necessary to use at least 30 to 40 model summaries for a stable evaluation; however, Harman and Over (2004) argue that a stable evaluation can be conducted even with a single model, as long as there is an adequate number of topics." ></td>
	<td class="line oc" title="15:165	This view is supported by Lin (2004a), who concludes that correlations to human judgments were increased by using multiple references but using single reference summary with enough number of samples was a valid alternative." ></td>
	<td class="line x" title="16:165	Interestingly, similar conclusions were also reached in the area of Machine Translation evaluation; in their experiments, Zhang and Vogel (2004) show that adding an additional reference translation compensates the effects of removing 1015% of the testing data, and state that, therefore, it seems more cost effective to have more test sentences but fewer reference translations." ></td>
	<td class="line x" title="17:165	In this paper, we look at how various metrics behave with respect to a variable number of topics and models used in the evaluation." ></td>
	<td class="line x" title="18:165	This lets us determine the stability of individual metrics, and helps to illuminate the trade-offs inherent in designing a good evaluation." ></td>
	<td class="line o" title="19:165	For our experiments, we used data from the Summarization track at the Text Analysis Conference (TAC) 2008, where participating systems were assessed on their summarization of 48 topics, and the automatic metrics ROUGE and BE, as well as the manual Pyramid evaluation method, had access to 4 human models." ></td>
	<td class="line x" title="20:165	TAC 2008 was the first task of the TAC/DUC (Document Understanding Conference) series in which the Pyramid method was used on all evaluateddata, makingitpossibletoconductafullcom23 parison among the manual and automatic methods." ></td>
	<td class="line o" title="21:165	Despite the lack of full Pyramid evaluation in DUC 2007, we look at the remaining metrics applied that year (ROUGE, BE, and Content Responsiveness), in order to see whether they confirm the insights gained from the TAC 2008 data." ></td>
	<td class="line x" title="22:165	2 Summary evaluation The main evaluation at TAC 2008 was performed manually, assessing the automatic candidate summaries with respect to Overall Responsiveness, Overall Readability, and content coverage according to the Pyramid framework (Nenkova and Passonneau, 2004; Passonneau et al., 2005)." ></td>
	<td class="line x" title="23:165	Task participantswereaskedtoproducetwosummariesfor each of the 48 topics; the first (initial summary) was a straightforward summary of 10 documents in response to a topic statement, which is a request for information about a subject or event; the second was an update summary, generated on the basisofanothersetof10documents, whichfollowed the first set in temporal order and described further developments in the given topic." ></td>
	<td class="line x" title="24:165	The idea behind the update summary was to avoid repeating all the information included in the first set of documents, on the assumption that the reader is familiar with that information already." ></td>
	<td class="line x" title="25:165	The participating teams submitted up to three runs each; however, only the first and second runs were evaluated manually due to limited resources." ></td>
	<td class="line x" title="26:165	For each summary under evaluation, assessors rated the summary from 1 (very poor) to 5 (very good) in terms of Overall Responsiveness, which measures how well the summary responds to the need for information expressed in the topic statement and whether its linguistic quality is adequate." ></td>
	<td class="line x" title="27:165	Linguistic qualities such as grammaticality, coreference, and focus were also evaluated as Overall Readability, also on the scale from 1 to 5." ></td>
	<td class="line x" title="28:165	Content coverage of each summary was evaluated using the Pyramid framework, where assessors create a list of information nuggets (called Summary Content Units, or SCUs) from the set of human-produced summaries on a given topic, then decide whether any of these nuggets are present in the candidate summary." ></td>
	<td class="line oc" title="29:165	All submitted runs were evaluated with the automatic metrics: ROUGE (Lin, 2004b), which calculates the proportion of n-grams shared between the candidate summary and the reference summaries, and Basic Elements (Hovy et al., 2005), which compares the candidate to the models in terms of head-modifier pairs." ></td>
	<td class="line x" title="30:165	2.1 Manual metrics Evaluating Overall Responsiveness and Overall Readability is a rather straightforward procedure, as most of the complex work is done in the mind of the human assessor." ></td>
	<td class="line x" title="31:165	Each candidate summary is given a single score, and the final score for the summarization system is the average of all its summary-level scores." ></td>
	<td class="line x" title="32:165	The only economic factor here is the number of topics, i.e. summaries per system, that need to be judged in order to neutralize both intraand inter-annotator variability and obtain a reliable assessment of the summarization system." ></td>
	<td class="line x" title="33:165	When it comes to the Pyramid method, which measures content coverage of candidate summaries, the need for multiple topics is accompanied by the need for multiple human model summaries." ></td>
	<td class="line x" title="34:165	First, independent human assessors producesummariesforeachtopic, guidedbythetopic statement." ></td>
	<td class="line x" title="35:165	Next, in the Pyramid creation stage, an assessor reads all human-produced summaries for a given topic and extracts all information nuggets, called Summary Content Units (SCUs), which are short, atomic statements of facts contained in the text." ></td>
	<td class="line x" title="36:165	Each SCU has a weight which is directly proportional to the number of model summaries in which it appears, on the assumption that the facts importance is reflected in how many human summarizers decide to include it as relevant in their summary." ></td>
	<td class="line x" title="37:165	Once all SCUs have been harvested from the model summaries, an assessor thenexamineseachcandidatesummarytoseehow many of the SCUs from the list it contains." ></td>
	<td class="line x" title="38:165	The final Pyramid score for a candidate summary is its total SCU weight divided by the maximum SCU weight available to a summary of average length (where the average length is determined by the mean SCU count of the model summaries for this topic)." ></td>
	<td class="line x" title="39:165	The final score for a summarization system is the average score of all its summaries." ></td>
	<td class="line x" title="40:165	In TAC 2008, the evaluation was conducted with 48 topics and 4 human models for each topic." ></td>
	<td class="line x" title="41:165	We examined to what extent the number of models and topics used in the evaluation can influence the Pyramid score and its stability." ></td>
	<td class="line o" title="42:165	The stability, similarly to the method employed by Voorhees and Buckley (2002) for Information Retrieval, is determined by how well a system ranking based on a small number of models/topics cor24 Models Pyramid ROUGE-2 ROUGE-SU4 BE 1 0.8839 0.8032 0.7842 0.7680 2 0.8943 0.8200 0.7957 0.7983 3 0.8974* 0.8258 0.7999* 0.8098 4 (bootstr) 0.8972* 0.8310 0.8023* 0.8152 4 (actual) 0.8997 0.8302 0.8033 0.8171 Table 1: Mean correlations of Responsiveness and other metrics using 1, 2, 3, or 4 models for TAC 2008 initial summaries." ></td>
	<td class="line x" title="43:165	Values in each row are significantly different from each other at 95% level." ></td>
	<td class="line o" title="44:165	Models Pyramid ROUGE-2 ROUGE-SU4 BE 1 0.9315 0.8861 0.8874 0.8716 2 0.9432 0.9013 0.8961 0.8978 3 0.9474* 0.9068* 0.8994 0.9076 4 (bootstr) 0.9481* 0.9079* 0.9023 0.9114 4 (actual) 0.9492 0.9103 0.9020 0.9132 Table 2: Mean correlations of Responsiveness and other metrics using 1, 2, 3, or 4 models for TAC 2008 update summaries." ></td>
	<td class="line o" title="45:165	Values in each row are significantly different from each other at 95% level except ROUGE-2 and ROUGE-SU4 in 1-model category." ></td>
	<td class="line o" title="46:165	Models ROUGE-2 ROUGE-SU4 BE 1 0.8789 0.8671 0.8553 2 0.8972 0.8803 0.8917 3 0.9036 0.8845 0.9048 4 (bootstr) 0.9082 0.8874 0.9107 4 (actual) 0.9077 0.8877 0.9123 Table 3: Mean correlations of 4-model Pyramid score and other metrics using 1, 2, 3, or 4 models for TAC 2008 initial summaries." ></td>
	<td class="line o" title="47:165	Values in each row are significantly different from each other at 95% level except ROUGE-2 and BE in 4-model category." ></td>
	<td class="line o" title="48:165	Models ROUGE-2 ROUGE-SU4 BE 1 0.9179 0.9110 0.9016 2 0.9336 0.9199 0.9284 3 0.9392 0.9233 0.9383 4 (bootstr) 0.9443 0.9277 0.9436 4 (actual) 0.9429 0.9263 0.9446 Table 4: Mean correlations of 4-model Pyramid score and other metrics using 1, 2, 3, or 4 models for TAC 2008 update summaries." ></td>
	<td class="line o" title="49:165	Values in each row are significantly different from each other at 95% level except ROUGE-2 and BE in 4-model category." ></td>
	<td class="line x" title="50:165	relates with the ranking based on another set of models/topics, where the two sets are randomly selected and mutually exclusive." ></td>
	<td class="line x" title="51:165	This methodology allows us to check the correlations based on up to half of the actual number of models/topics only(becauseofthenon-overlaprequirement), but it gives an indication of the general tendency." ></td>
	<td class="line x" title="52:165	We also look at the correlation between the Pyramid score and Overall Responsiveness." ></td>
	<td class="line x" title="53:165	We dont expect a perfect correlation between Pyramid and Responsiveness in the best of times, because Pyramid measures content identity between the candidate and the model, and Responsiveness measures content relevance to topic as well as linguistic quality." ></td>
	<td class="line x" title="54:165	However, the degree of variation between the two scores depending on the number of models/topics used for the Pyramid will give us a certain indication of the amount of information lost." ></td>
	<td class="line oc" title="55:165	2.2 Automatic metrics Similarly to the Pyramid method, ROUGE (Lin, 2004b) and Basic Elements (Hovy et al., 2005) require multiple topics and model summaries to produce optimal results." ></td>
	<td class="line o" title="56:165	ROUGE is a collection of automatic n-gram matching metrics, ranging from unigram to four-gram." ></td>
	<td class="line o" title="57:165	It also includes measurements of the longest common subsequence, weighted or unweighted, and the option to compare stemmed versions of words and omit stopwords." ></td>
	<td class="line x" title="58:165	There is also the possibility of accepting skip-n-grams, that is, counting n-grams as matching even if there are some intervening nonmatching words." ></td>
	<td class="line o" title="59:165	The skip-n-grams together with stemming are the only ways ROUGE can accomodate alternative forms of expression and match concepts even though they might differ in terms of their syntactic or lexical form." ></td>
	<td class="line o" title="60:165	These methods are necessarily limited, and so ROUGE relies on using multiple parallel model summaries which serve as a source of lexical/syntactic variation in the comparison process." ></td>
	<td class="line x" title="61:165	The fewer models there are, the less reliable the score." ></td>
	<td class="line oc" title="62:165	Our question here is not only what this relation looks like (as it was examined on the basis of Document Understanding Conference data in Lin (2004a)), but also how it compares to the reliability of other metrics." ></td>
	<td class="line x" title="63:165	Basic Elements (BE), on the other hand, goes beyond simple string matching and parses the syntactic structure of the candidate and model to obtain a set of head-modifier pairs for each, and then compares the sets." ></td>
	<td class="line x" title="64:165	A head-modifier pair consist of theheadofasyntacticunit(e.g. thenouninanoun phrase), and the word which modifes the head (i.e. a determiner in a noun phrase)." ></td>
	<td class="line x" title="65:165	It is also possible to include the name of the relation which connects them (i.e. subject, object, etc.)." ></td>
	<td class="line x" title="66:165	Since BEs reflect thematic relations in a sentence rather than surface word order, it should be possible to accommodate certain differences of expression that might appear between a candidate summary and a reference, especially as the words can be stemmed." ></td>
	<td class="line x" title="67:165	This could, in theory, allow us to use fewer models for the evaluation." ></td>
	<td class="line x" title="68:165	In practice, however, it fails to account for the total possible variety, and, what is more, 25 the additional step of parsing the text can introduce noise into the comparison." ></td>
	<td class="line o" title="69:165	TAC 2008 and DUC 2007 evaluations used ROUGE-2 and ROUGE-SU4, which refer to the recall of bigram and skip-bigram (with up to 4 intervening words) matches on stemmed words, respectively, as well as a BE score calculated on the basis of stemmed head-modifier pairs without relation labels." ></td>
	<td class="line x" title="70:165	Therefore, these are the versions we use in our comparisons." ></td>
	<td class="line x" title="71:165	3 Number of models Since Responsiveness score does not depend on the number of models, it serves as a reference against which we compare the remaining metrics, while we calculate their score with only 1, 2, 3, or all 4 models." ></td>
	<td class="line x" title="72:165	Given 48 topics in TAC 2008, and 4-model summaries for each topic, there are 448 possible combinations to derive the final score in the single-model category, so to keep the experiments simple we only selected 1000 random samples from that space." ></td>
	<td class="line x" title="73:165	For 1000 repetitions, each time we selected a random combination of model summaries (only one model out of 4 available per topic), against which we evaluated the candidate summaries." ></td>
	<td class="line x" title="74:165	Then, for each of the 1000 samples, we calculated the correlation between the resulting score and Responsiveness." ></td>
	<td class="line x" title="75:165	We then took the 1000 correlations produced in this manner, and computed their mean." ></td>
	<td class="line x" title="76:165	In the same way, we calculated the scores based on 2 and 3 model summaries, randomly selected from the 4 available for each topic." ></td>
	<td class="line x" title="77:165	The correlation means for all metrics and categories are given in Table 1 for initial summaries and Table 2 for update summaries." ></td>
	<td class="line x" title="78:165	We also ran a one-way analysis of variance (ANOVA) on these correlations to determine whether the correlation means were significantly different from each other." ></td>
	<td class="line x" title="79:165	For the 4-model category there was only one possible sample for each metric, so in order to perform ANOVA we bootstrapped this sample to produce 1000 samples." ></td>
	<td class="line x" title="80:165	The actual value of the 4-model correlation is given in the tables as 4 (actual), and the mean value of the bootstrapped 1000 correlations is given as 4 (bootstr)." ></td>
	<td class="line x" title="81:165	Values for initial summaries are significantly different from their counterparts for update summaries at the 95% level." ></td>
	<td class="line x" title="82:165	Pairwise testing of values for statistically significant differences is shown with symbols: in each column, the first value marked with a particular symbol is not significantlydifferentfromanysubsequentvaluemarked with the same symbol." ></td>
	<td class="line x" title="83:165	We also examined the correlations of the metrics with the 4-model Pyramid score." ></td>
	<td class="line x" title="84:165	Table 3 presents the correlation means for the initial summaries, and Table 4 shows the correlation means for the update summaries." ></td>
	<td class="line x" title="85:165	Since the Pyramid, contrary to Responsiveness, makes use of multiple model summaries, we examine its stability given a decreased number of models to rely on." ></td>
	<td class="line x" title="86:165	For this purpose, we correlated the Pyramid score based on randomly selected 2 models(halfofthemodelpool)foreachtopicwith the score based on the remaining 2 models, and repeated this 1000 times." ></td>
	<td class="line x" title="87:165	We also looked at the 1-model category, where the Pyramid score calculated on the basis of one model per topic was correlated with the Pyramid score calculated on the basis on another randomly selected model." ></td>
	<td class="line x" title="88:165	In bothcasewewitnessaveryhighmeancorrelation: 0.994 and 0.995 for the 2-model category, 0.982 and 0.985 for the 1-model category for TAC initial and update summaries, respectively." ></td>
	<td class="line x" title="89:165	As an illustration, Figure1 shows the variance of correlations for the initial summaries." ></td>
	<td class="line x" title="90:165	Figure 1: Correlations between Pyramid scores based on 1 or 2 model summaries for TAC 2008 initial summaries." ></td>
	<td class="line x" title="91:165	The variation in correlation levels between other metrics and Pyramid and Responsiveness, presented in Tables 34, is more visible in the graph form." ></td>
	<td class="line x" title="92:165	Figures 2-3 illustrate the mean correlation values for TAC 2008 initial summaries." ></td>
	<td class="line x" title="93:165	While all the metrics record the steepest increase in correlation values with the addition of the second model, adding the third and fourth model provides the metrics with smaller but steady improvement, with the exception of Pyramid-Responsiveness correlation in Figure 2." ></td>
	<td class="line n" title="94:165	The increase in correlation mean is most dramatic for BE, which in all cases starts as the lowest26 correlating metric in the single-model category, but by the 4-model point it outperforms one or both versions of ROUGE." ></td>
	<td class="line x" title="95:165	The Pyramid metric achieves significantly higher correlations than any other metric, independent of the number of models, which is perhaps unsurprising given that it is a manual evaluation method." ></td>
	<td class="line n" title="96:165	Of the two ROUGE versions, ROUGE-2 seems consistently a better predictor of both Responsiveness and the full 4model Pyramid score than ROUGE-SU4." ></td>
	<td class="line x" title="97:165	Figure 2: Responsiveness vs. other metrics with 1, 2, 3, or 4 models for TAC 2008 initial summaries." ></td>
	<td class="line x" title="98:165	Figure 3: 4-model Pyramid vs. other metrics with 1, 2, 3, or 4 models for TAC 2008 initial summaries." ></td>
	<td class="line x" title="99:165	Similar patterns appear in DUC 2007 data (Table 5), despite the fact that the Overall ResponsivenessofTAC2008isreplacedwithContentResponsiveness (ignoring linguistic quality), against which we calculate all the correlations." ></td>
	<td class="line x" title="100:165	Although the increase in correlation means from 1to 4models for the three automatic metrics is smaller than for TAC 2008, the clearest rise occurs with the addition of a second model, especially for BE, and the subsequent additions change little." ></td>
	<td class="line x" title="101:165	As in the case of initial summaries 2008, ROUGE-2 outperformstheremainingtwometricsindependently of the number of models." ></td>
	<td class="line x" title="102:165	However, most of the increases are too small to be significant." ></td>
	<td class="line x" title="103:165	This comparison suggests diminishing returns Models ROUGE-2 ROUGE-SU4 BE 1 0.8681 0.8254 0.8486 2 0.8747* 0.8291* 0.8577* 3 0.8766* 0.8299* 0.8599* 4 (bootstr) 0.8761* 0.8305* 0.8633 4 (actual) 0.8795 0.8301 0.8609 Table 5: Mean correlations of Content Responsiveness and other metrics using 1, 2, 3, or 4 models for DUC 2007 summaries." ></td>
	<td class="line x" title="104:165	Values in each row are significantly different from each other at 95% level." ></td>
	<td class="line x" title="105:165	with the addition of more models, as well as different reactions among the metrics to the presence orabsenceofadditionalmodels." ></td>
	<td class="line x" title="106:165	Whencorrelating with Responsiveness, the manual Pyramid metric benefits very little from the fourth model, but automatic BE benefits most from almost every addition." ></td>
	<td class="line o" title="107:165	ROUGE is situated somewhere between the two, noting small but often significant increases." ></td>
	<td class="line x" title="108:165	On the whole, the use of multiple models (at least two) seems supported, especially if we use automatic metrics in our evaluation." ></td>
	<td class="line x" title="109:165	4 Number of topics For the second set of experiments we kept all four models, but varied the number of topics which went into the final average system score." ></td>
	<td class="line x" title="110:165	To determine the stability of Responsiveness and Pyramid we looked at the correlations between the scores based on smaller sets of topics." ></td>
	<td class="line x" title="111:165	For 1000 repetitions, we calculated Pyramid/Responsiveness score based on a set of 1, 3, 6, 12, or 24 topics randomly chosen from the pool of 48, and compared the system ranking thus created with the ranking based on another, equally sized set, such that the setsdidnotcontaincommontopics." ></td>
	<td class="line x" title="112:165	Table6shows the mean correlation for each case." ></td>
	<td class="line x" title="113:165	Although such comparison was only possible up to 24 topics (half of the whole available topic pool), the numbers suggest that at the level of 48 topics both Responsiveness and Pyramid are stable enough to serve as reference for the automatic metrics." ></td>
	<td class="line x" title="114:165	Responsiveness Pyramid Topics Initial Update Initial Update 1 0.182 0.196 0.333 0.267 3 0.405 0.404 0.439 0.520 6 0.581 0.586 0.608 0.690 12 0.738 0.738 0.761 0.816 24 0.849 0.866 0.851 0.901 Table 6: Mean correlations between Responsiveness/Pyramid scores based on 1, 3, 6, 12, and 24 topic samples for TAC 2008 initial and update summaries." ></td>
	<td class="line o" title="115:165	In a process which mirrored that described in Section 3, we created 1000 random samples in each of the n-topics category: 1, 3, 6, 12, 24, 36, 27 Topics Pyramid ROUGE-2 ROUGE-SU4 BE 1 0.4219 0.4276 0.4375 0.3506 3 0.6204 0.5980 0.9016 0.5108 6 0.7274 0.6901 0.6836 0.6233 12 0.8159 0.7618 0.7456 0.7117 24 0.8679 0.8040 0.7809 0.7762 36 0.8890* 0.8208* 0.7951* 0.8017* 39 0.8927* 0.8231* 0.7967* 0.8063* 42 0.8954* 0.8258* 0.7958* 0.8102* 45 0.8977* 0.8274* 0.8008* 0.8132 48 (bootstr) 0.8972* 0.8302* 0.8046 0.8138 48 (actual) 0.8997 0.8302 0.8033 0.8171 Table 7: Mean correlations of 48 topic Responsiveness and other metrics using from 1 to 48 topics for TAC 2008 initial summaries." ></td>
	<td class="line o" title="116:165	Values in each row are significantly different from each other at 95% level except: ROUGE-2, ROUGE-SU4 and BE in 1-topic category, ROUGE-2 and ROUGE-SU4 in 3and 6-topic category." ></td>
	<td class="line o" title="117:165	Topics Pyramid ROUGE-2 ROUGE-SU4 BE 1 0.5005 0.4882 0.5609 0.4011 3 0.7053 0.6862 0.7340 0.6097 6 0.8080 0.7850 0.8114 0.7274 12 0.8812 0.8498 0.8596 0.8188 24 0.9250 0.8882 0.8859 0.8774 36 0.9408* 0.9023* 0.8960* 0.8999* 39 0.9433* 0.9045* 0.8973* 0.9037* 42 0.9455* 0.9061* 0.8987* 0.9068* 45 0.9474 0.9078* 0.8996* 0.9094 48 (bootstr) 0.9481 0.9101 0.9015* 0.9111 48 (actual) 0.9492 0.9103 0.9020 0.9132 Table 8: Mean correlations of 48 topic Responsiveness and other metrics using from 1 to 48 topics for TAC 2008 update summaries." ></td>
	<td class="line o" title="118:165	Values in each row are significantly different from each other at 95% level except: Pyramid and ROUGE-2 in 1topic category, Pyramid and ROUGE-SU4 in 6-topic category, ROUGE-2 and BE in 39-, 42-, and 48-topic category." ></td>
	<td class="line o" title="119:165	Topics ROUGE-2 ROUGE-SU4 BE 1 0.4693 0.4856 0.3888 3 0.6575 0.6684 0.5732 6 0.7577 0.7584 0.6960 12 0.8332 0.8245 0.7938 24 0.8805 0.8642 0.8684 36 0.8980* 0.8792* 0.8966* 39 0.9008* 0.8812* 0.9017* 42 0.9033* 0.8839* 0.9058 45 0.9052* 0.8853* 0.9093 48 (bootstr) 0.9074 0.8877 0.9107 48 (actual) 0.9077 0.8877 0.9123 Table 9: Mean correlations of 48 topic Pyramid score and other metrics using from 1 to 48 topics for TAC 2008 initial summaries." ></td>
	<td class="line o" title="120:165	Values in each row are significantly different from each other at 95% level except: ROUGE-2 and ROUGE-SU4 in the 6-topic category, ROUGE-2 and BE in 39and 48-topic category." ></td>
	<td class="line o" title="121:165	Topics ROUGE-2 ROUGE-SU4 BE 1 0.5026 0.5729 0.4094 3 0.7106 0.7532 0.6276 6 0.8130 0.8335 0.7512 12 0.8806 0.8834 0.8475 24 0.9196 0.9092 0.9063 36 0.9343* 0.9198* 0.9301* 39 0.9367* 0.9213* 0.9341* 42 0.9386* 0.9227* 0.9376* 45 0.9402* 0.9236* 0.9402 48 (bootstr) 0.9430 0.9280 0.9444 48 (actual) 0.9429 0.9263 0.9446 Table 10: Mean correlations of 48 topic Pyramid score and other metrics using from 1 to 48 topics for TAC 2008 update summaries." ></td>
	<td class="line o" title="122:165	Values in each row are significantly different from each other at 95% level except: ROUGE-2 and ROUGE-SU4 in 12-topic category, ROUGE-2 and BE in 45-topic category." ></td>
	<td class="line x" title="123:165	39, 42, or 45." ></td>
	<td class="line x" title="124:165	Within each of these categories, for a thousand repetitions, we calculated the score for automatic summarizers by averaging over n topics randomlyselectedfromthepoolof48topicsavailable in the evaluation." ></td>
	<td class="line x" title="125:165	Again, we examined the correlations between the metrics and the full 48topic Responsiveness and Pyramid." ></td>
	<td class="line x" title="126:165	As previously, we then used ANOVA to determine whether the correlation means differed significantly." ></td>
	<td class="line x" title="127:165	Because there was only one possible sample with all 48 topics for each metric, we bootstrapped this sample to provide 1000 new samples in the 48-topic category, in order to perfom the ANOVA comparison of variance." ></td>
	<td class="line x" title="128:165	Tables 7 and 8, as well as Figures 4 and 5, show the metrics changing correlations with Responsiveness." ></td>
	<td class="line x" title="129:165	Tables 9 and 10, and Figures 6 and 7, show the correlations with the 48topic Pyramid score." ></td>
	<td class="line x" title="130:165	Values for initial summaries are significantly different from their counterparts for update summaries at the 95% level." ></td>
	<td class="line x" title="131:165	In all cases, it becomes clear that the curves flatten out and the correlations stop increasing almost completely beyond the 36-topic mark." ></td>
	<td class="line x" title="132:165	This means that the scores for the automatic summarization systems based on 36 topics will be on average practically indistiguishable from the scores based on all 48 topics, showing that beyond a certain minimally necessary number of topics adding or removing a few (or even ten) topics will not influence the system scores much." ></td>
	<td class="line x" title="133:165	(However, we cannotconcludethatafurtherconsiderableincreasein the number of topics  well beyond 48  would not bring more improvement in the correlations, perhaps increasing the stable correlation window as well.)" ></td>
	<td class="line o" title="134:165	Topics ROUGE-2 ROUGE-SU4 BE 1 0.6157 0.6378 0.5756 3 0.7597 0.7511 0.7323 6 0.8168 0.7904 0.7957 12 0.8493 0.8123 0.8306 24 0.8690 0.8249* 0.8517* 36 0.8751* 0.8287* 0.8580* 39 0.8761* 0.8295* 0.8592 42 0.8768* 0.8299* 0.8602 45 (bootstr) 0.8761* 0.8305 0.8627 45 (actual) 0.8795 0.8301 0.8609 Table 11: Mean correlations of 45 topic Content Responsiveness and other metrics using from 1 to 45 topics for DUC 2007 summaries." ></td>
	<td class="line x" title="135:165	Values in each row are significantly different from each other at 95% level." ></td>
	<td class="line x" title="136:165	An interesting observation is that if we produce such limited-topic scores for the manual metrics, Responsiveness and Pyramid, and correlate them with their own full versions based on 28 Figure4: Responsivenessvs." ></td>
	<td class="line x" title="137:165	othermetricswith1to48topics for TAC 2008 initial summaries." ></td>
	<td class="line x" title="138:165	Figure5: Responsivenessvs." ></td>
	<td class="line x" title="139:165	othermetricswith1to48topics for TAC 2008 update summaries." ></td>
	<td class="line x" title="140:165	Figure 6: 48-topic Pyramid vs. other metrics with 1 to 48 topics for TAC 2008 initial summaries." ></td>
	<td class="line x" title="141:165	Figure 7: 48-topic Pyramid vs. other metrics with 1 to 48 topics for TAC 2008 update summaries." ></td>
	<td class="line x" title="142:165	all 48 topics, it appears that they are less stable than the automatic metrics, i.e. there is a larger gap between the worst and best correlations they achieve.1 The mean correlation between the full Responsiveness and that based on 1 topic is 0.443 and 0.448 for the initial and update summaries, respectively; for that based on 3 topics, 0.664 and 0.667." ></td>
	<td class="line x" title="143:165	Pyramid based on 1 topic achieves 0.467 for initial and 0.525 for update summaries; Pyramid based on 3 topics obtains 0.690 and 0.742, respectively." ></td>
	<td class="line o" title="144:165	Some of these values, especially for update summaries, are even lower than those obtained by ROUGE in the same category, despite the fact that 1and 3-topic Responsiveness or Pyramid is a proper subset of the 48-topic Responsiveness/Pyramid." ></td>
	<td class="line o" title="145:165	On the other hand, ROUGE achieves considerably worse correlations with Responsiveness than Pyramid when there are many topics available." ></td>
	<td class="line x" title="146:165	ROUGE-SU4 seems to be more stable than ROUGE-2; in all cases ROUGE-2 starts with lower correlations than ROUGE-SU4, but by the 12-topic mark its correlations increase 1For reasons of space, these values are not included in the tables, as they offer little insight besides what is mentioned here." ></td>
	<td class="line x" title="147:165	above it." ></td>
	<td class="line x" title="148:165	Additionally, despitebeinganautomaticmetric, BE seems to follow the same pattern as the manual metrics." ></td>
	<td class="line x" title="149:165	It is seriously affected by the decreasing number of topics; in fact, if the number of topics drops below 24, BE is the least reliable indicator of either Responsiveness or Pyramid." ></td>
	<td class="line x" title="150:165	However, by the 48-topic mark it rises to levels comparable with ROUGE-2." ></td>
	<td class="line x" title="151:165	As in the case of models, DUC 2007 data shows mostly the same pattern as TAC 2008." ></td>
	<td class="line x" title="152:165	Again, in this data set, the increase in the correlation mean with the addition of topics for each metric are smaller than for either initial or update summaries in TAC 2008, but the relative rate of increase remains the same: BE gains most from additional topics (+0.28 in DUC vs. +0.47 and +0.51 in TAC), ROUGE-SU4 again shows the smallest increase (+0.19 in DUC vs. +0.36 and +0.34 in TAC),whichmeansitisthemoststableofthemetrics across the variable number of topics.2 2The smaller total increase might be due to the smaller number of available topics (45 in DUC vs. 48 in TAC), but we have seen the same effect in Section 3 while discussing models, so it might just be an accidental property of a given data set." ></td>
	<td class="line x" title="153:165	29 5 Discussion and conclusions As the popularity of shared tasks increases, task organizers face an ever growing problem of providing an adequate evaluation to all participating teams." ></td>
	<td class="line x" title="154:165	Often, evaluation of multiple runs from the same team is required, as a way to foster research and development." ></td>
	<td class="line x" title="155:165	With more and more system submissions to judge, and the simultaneous need for multiple topics and models in order to provide a stable assessment, difficult decisions of cutting costs and effort might sometimes be necessary." ></td>
	<td class="line x" title="156:165	It would be useful then to know where such decisions will have the smallest negative impact, or at least, what might be the trade-offs inherent in such decisions." ></td>
	<td class="line x" title="157:165	From our experiments, it appears that manual metrics such as Pyramid gain less from the addition of more model summaries than the automatic metrics." ></td>
	<td class="line x" title="158:165	A Pyramid score based on any two models correlates very highly with the score based on any other two models." ></td>
	<td class="line x" title="159:165	For the automatic metrics, the largest gain is recorded with adding the second model; afterwards the returns diminish." ></td>
	<td class="line x" title="160:165	BE seems to be the most sensitive metric to changes in the number of models and topics; ROUGE-SU4, on the other hand, is the least sensitive to such changes and the most stable, but it does not obtain the highest correlations when many models and topics are available." ></td>
	<td class="line x" title="161:165	Whatever the number of models, manual Pyramid considerably outperforms automatic metrics, as can be expected, since human understanding is not hampered by the possible differences in surface expression between a candidate and a model." ></td>
	<td class="line x" title="162:165	But when it comes to decreased number of topics, the inherent variability of human judgment shows strongly, to the extent that, in extreme cases of very few topics, it might be more prudent to use ROUGE-SU4 than Pyramid or Responsiveness." ></td>
	<td class="line x" title="163:165	Lastly, we observe that, as with models, adding one or two topics to the evaluation plays a great role only if we have very few topics to start with." ></td>
	<td class="line x" title="164:165	Our experiments suggest that, as the number of topics available for evaluation increases, so does the number of additional topics necessary to make a difference in the system ranking produced by a metric." ></td>
	<td class="line x" title="165:165	It seems that in the case of evaluation based on 48 topics, as in the TAC Summarization track, it would be possible to decrease the number to about 36 without sacrificing much stability." ></td>
</tr></table>
</div
</body></html>
